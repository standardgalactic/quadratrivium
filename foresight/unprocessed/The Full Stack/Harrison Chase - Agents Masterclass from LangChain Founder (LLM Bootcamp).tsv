start	end	text
0	7200	So, I'll be talking about agents, and yeah, there are many things in the lane chain, but
7200	10240	I think agents are probably the most interesting one to talk about, so that's what I'll be
10240	11240	doing.
11240	15840	I'll cover kind of like, what are agents, why use agents, the typical implementation
15840	21120	of agents, talk about React, which is one of the first prompting strategies that really
21120	27080	accelerated and made reliable the use of agents, then I'll talk a bunch about challenges with
27080	31880	agents and challenges of getting them to work reliably, getting them to work in production.
31880	36680	I'll then touch a little bit on memory and segue that into more recent kind of like
36680	40960	papers and projects that do agentic things.
40960	46680	I'll probably skim over the initial slides because I think most people here are probably
46680	51400	familiar with the idea of agents, but the core idea of agents is using the language model
51400	56560	as a reasoning engine, so using it to determine kind of like what to do and how to interact
56560	61640	with the outside world, and this means that there is a non-deterministic kind of like sequence
61640	66920	of actions that'll be taken depending on the user input, so there's no kind of like hard
66920	73040	coded do A, then do B, then do C, rather the agent determines what actions to do depending
73040	78280	on the user input and depending on results of previous actions.
78280	81480	So why would you want to do this in the first place?
81480	86640	So one, there's this very tied to agents is the idea of tool usage and connecting it
86640	91720	to the outside world, and so connecting it to other sources of data or computation like
91720	95600	search, APIs, databases, these are all very useful to overcome.
95600	99360	Some of the limitations of language models, such as they don't know about your data, they
99360	105880	can't do math amazingly well, but the idea of tool usage isn't kind of like unique to
105880	110920	agents, you can still use tools, you can still connect LLMs to search engines without using
111000	113080	an agent, so why use an agent?
113080	117840	And I think some of the benefits of agents are that they're more flexible, they're more
117840	121160	powerful, they allow you to kind of like recover from errors better, they allow you to handle
121160	125200	kind of like multi-hop tasks, again with this idea of being the reasoning engine, and so
125200	131240	an example that I like to use here is thinking about like interacting with a SQL database.
131240	135720	You can do that in a sequence of predetermined steps, you can have kind of like a natural
135720	139960	language query, which you then use a language model to convert to a SQL query, which you
139960	145160	then pass, you then execute that, get back a result, pass that back to the language model,
145160	149840	ask it to synthesize it with respect to the original question, and get kind of this natural
149840	154080	language wrapper around a SQL database, very useful by itself, but there are a lot of edge
154080	157840	cases and things that can go wrong, so there could be an error in the SQL query, maybe
157840	164360	it hallucinates a table name or a field name, maybe it just writes incorrect SQL, and then
164360	168240	there are also queries that need multiple kind of like queries to be made under the
168240	176280	hood in order to answer, and so although kind of like a simple chain can handle maybe, I
176280	180280	don't know, 50, 80% of the use cases, you very quickly run into these like edge cases
180280	185920	where a more flexible framework like agents helps kind of like circumvent.
185920	190440	So the typical implementation of agents generally, and it's so early in this field that it kind
190440	193600	of feels a bit weird to be talking about a typical implementation because I'm sure we'll
193680	200320	see a bunch of different variants, but generally you get a user query, you use the LLM, that's
200320	207240	the agent to choose a tool to use, and also the input to that tool, you then do that,
207240	212000	you take that action, you get back an observation, and then you feed that back into the language
212000	216640	model and you kind of continue doing this until a stopping condition is met, and so there
216640	219880	can be different types of stopping conditions, probably the most common is the language model
219920	226600	itself realizes, hey, I'm done with this task, with this question that someone asked of me, I
226600	231160	should now return to the user, but there can be other more hard-coded rules, and so when we talk
231160	235840	about like reliability of agents, some of these can really help with that, so you know, if an
235840	240800	agent has done five different steps in a row and it hasn't reached a final answer, it might be nice
240800	245280	to have it just return something then. There are also certain tools that you can just like return
245360	250080	automatically, so basically the general idea is choose a tool to use, observe the output of that
250080	256920	tool, and kind of continue going. So how do you actually like, so that was pseudocode, now let's
256920	265200	talk about the actual kind of like ways to get this to do what we want, and the first and still
265200	272360	kind of like the main way of doing this, the main prompting strategy slash algorithm slash method
272360	278680	for doing this is react, which stands for reasoning and then acting, so RE from reasoning, ACT
278680	284200	from acting, and it's the papers, a great paper came out in I think October out of Princeton,
284200	291120	synergizing two different kind of like methods, and we can look at this example which is taken
291120	298640	from their paper and see why it's so effective. So this example comes from the hotspot QA data set,
298960	304240	which is basically a data set where it's asking questions over Wikipedia pages where there are
304240	309440	multi-hop usually kind of like two or three intermediate questions that need to be reasoned
309440	314600	about before answering. So we can see, so here there's this question aside from the Apple remote,
314600	321120	what other device can control the program Apple remote was originally designed to interact with,
321120	326640	and so the most basic prompting strategy is kind of just pass that into the language model and
326640	332040	get back an answer, and so we can see that in 1A standard, and it just returns a single answer,
332040	336480	and we can see that it's wrong. Another method that had emerged maybe like a month or so prior
336480	343480	was the idea of like chain of thought reasoning, and so this is usually associated with the let's
343480	350320	think step by step prefix to the response, and you can see in red that there's this like chain of
350320	356960	thought thing that's happening as it thinks through step by step, and it returns an answer
356960	363560	that's also incorrect in this case. This has been shown to kind of like get the agent or get the
363560	369760	language model to think a little bit better, so to speak. So it's yielding higher quality,
369760	376320	more reliable results. The issue is it still only knows kind of like what is actually present in
376320	380040	the data that the language model is trained on, and so there's another technique that came out
380080	385880	which is basically action only, where you give it access to different tools. In this case,
385880	392680	I think all the examples in this picture are of search, but I think in the paper it had search
392680	399000	and then look up, and so you can see here that the language model outputs kind of like search
399000	403600	Apple remote, it looks up Apple remote, it gets back an observation, it then does search front row,
404560	408920	can find that, so that's an instance of it kind of like recovering from an error,
408920	418560	and then it does search front row software, finds an answer, and then finishes. And you can see
418560	426320	here that the output that it gave, yes, kind of loses some of what it was actually supposed to
426320	431120	answer. So you've got this chain of thought reasoning which helps the language model think
431120	438000	about what to do, then you've got this action taking step that basically actually allows it to
438000	442640	plug into more kind of like sources of real data, what if you combine them, and that's the idea
442640	451040	of React, and that's the idea of a lot of the popular agent frameworks. Because again, agents use
451040	456280	a language model as a reasoning engine, so you want it to be really good at reasoning. So if
456280	460480	there are any prompting techniques you can use to improve its reasoning, you should probably do
460480	464320	those, and then a big part of it is also connecting into tools, and that's where action comes in.
464320	472000	And so you can see here, it arrives at kind of like the final answer. So that's the idea of
472000	477160	agents, React is still one of the more popular implementations for it, but what are some of the
477160	484160	current challenges? And there are a lot of challenges, like I think most agents are not
484160	490360	amazingly production ready at the moment, and these are some of the reasons why, and we can
491240	497320	walk through them in a bunch more detail. I'll also leave a lot of time at ends for the questions,
497320	502120	so I'd love to hear kind of like what you guys are observing as issues for getting agents to work
502120	507640	reliably. This is probably far from a complete list. So the most basic challenge with agents is
507640	514840	getting them to use tools in appropriate scenarios. And so in the React paper, they address this
514920	520560	challenge by bringing in the reasoning aspect of it, the chain of thought, style, prompting,
520560	527400	asking it to think. Kind of like common ways to do this include just like saying in the instructions,
527400	531720	you have access to these tools, you should use these to overcome some of your limitations,
531720	536520	and just basically instructing the language model. Tool descriptions are really, really
536520	541160	important for this. If you want the agent to use a particular tool, it should probably have
541160	547240	enough context to know that this tool is good at this, and that generally comes in the form of
547240	551160	like tool descriptions or some information about the tool that's passed into the prompt.
553080	557240	That can maybe not scale super well if you've got a lot of tools, because now you've got maybe
557240	562520	these more complex descriptions, you want to put them in the final prompt for the language model
562520	568120	to know what to do with them, but you can quickly run into kind of like context length issues.
568120	572520	And so that's where I think the idea of tool retrieval comes in handy. So you can have hundreds,
572520	577480	thousands of tools, you can do some retrieval step, and I think retrieval is another really
577480	580840	interesting topic that I'm not going to go into too much depth here. So for the sake of this,
580840	584680	we'll just say it's some embedding search lookup, although I think there's a lot more interesting
584680	589480	things to do there. You basically do the retrieval step, you get back five, ten, however many tools
589480	593400	that you think are most promising, you can then pass those to the prompt and kind of have the
593400	598280	language model take the final steps from there. Few shot examples I think can also be really
598280	603880	helpful, so you can use those to guide the language model in what to do. Again, I think the idea
603880	608520	of retrieval to find the most relevant few shot examples is particularly promising here. So if
608520	612760	you give it examples similar to the one it's trying to do, those help a lot better than random
612760	617320	examples. And then probably the most extreme version of this is fine tuning a model like
617320	624840	tool former to really help with tool selection. There's also a subtle second challenge which
624840	629240	is getting them not to use tools when they don't need to. So a big use case for agents is having
629240	634600	conversational style agents. One of the big problems that we've seen is oftentimes these types
634600	639320	of agents just want to use tools no matter what, even if they're having a conversation. And so
639320	644680	again, like the most basic thing you can do is probably put some information in the instructions,
644680	648920	some reminder in the prompt like, hey, you don't have to use a tool, you can respond to the user
648920	654920	if it seems like it's more of a conversation. That can get you so far. Another kind of like clever
654920	660120	hack that we've seen here is add another tool that explicitly just returns to the user. And then,
660120	665880	you know, they like to use tools, but they'll usually use that tool. So I thought that was a
665880	673240	pretty clever and interesting hack. A third challenge is the language models tell you what
673240	679720	tool to use and how to use it. But that's in the form of a string. And so you need to go from that
679720	684920	string into some code or something that can actually be run. And so that involves parsing kind
684920	690200	of like the output of the language model into this tool invocation. And so some tips and tricks
690200	693960	and hacks here are one like the more structured you ask for the response, the easier it is to
693960	699800	parse generally. So language models are pretty good at writing JSON. So we've kind of transitioned
699880	706600	a few of our agents to use that schema. Still doesn't always work, especially some of the
706600	715240	chat models like to add in a lot of kind of like language. So we've introduced kind of like this
715240	720120	concept of output parsers, which generically encapsulate all the logic that's needed to
720120	725160	parse this response. And we've tried to make that as modular as possible. So if you're seeing areas,
725160	730200	you can hopefully kind of like substitute that out very related to that. We also have a concept of
730200	737240	like output parsers that can retry and fix mistakes. So and I think there's there's some subtle,
738120	741800	there's some subtle differences here that I think are really cool. Basically, like, you know,
741800	748280	if you have misformatted schema, you can fix that explicitly by just passing it the output
748280	754120	and the error and saying fix this response. But if you have, if you have an output that just
754120	758520	forgets one of the fields, like it returns the action, but not the action input or something
758520	761800	like that, you need to provide more information here. So I think there's actually a lot of
761800	766040	subtlety in fixing some of these errors. But the basic idea is that you can try to parse it.
766920	771960	If you if it fails, you can then try to fix it. All this we currently encapsulate in this idea
771960	778760	of like output parsers. So the fourth challenge is getting them to remember previous steps that
778760	782920	were taken. The most basic thing, the thing that the React paper does is just keep a list of those
782920	790600	steps in memory. Again, that starts to run into some some context window issues, especially when
790600	795640	you're dealing with long running tasks. And so the thing that we've seen done here is again,
795640	799960	fetch previous steps with some retrieval method and put those into context. Usually we've actually
799960	806840	seen a combination of the two. So we've seen using the end most recent actions and observations
806840	814760	combined with the K most relevant actions and observations. Incorporating long observations
814760	819000	is another really interesting one. This actually comes up, or this came up a lot when we were
819000	824760	dealing with working with APIs, because APIs often return really big JSON blobs that are really big
824760	831240	and hard to put in context. So the most common thing that we've done here is just parse that in
831240	837000	some way. You can do really simple stuff like convert that blob to a string and put the first
837000	842120	like 1000 characters or something as the response. You can also do some more if you know that you're
842120	847960	working with a specific API, you can probably write some custom logic to kind of like take only the
847960	852440	relevant keys and put that if you want to make something general, you could also maybe do something
852440	857720	dynamically to like figure out what key like basically explore the JSON object and figure out
857720	863080	what keys to put in. That's a bit more exploratory, I would say. But the basic idea is, yeah, there
863080	869160	is this issue of, and so Zapier, I always have to think about how to pronounce it, but it's Zapier
869160	874440	makes you happier. So Zapier when they did this with their natural language API, not only did they
874440	880440	have something before the API that was like natural language to some API call, they also spent a lot
880440	884120	of time working on the output. And so the output is actually very specifically, I think it's like
884120	888440	under like 200 or 300 tokens or something like that. And they did that on purpose. They spent a
888440	892600	lot of time thinking about that. And so I think for tool usage, that is really important as well.
893800	899800	Another more kind of like exploratory way of doing this is also you could perhaps just store
899800	904680	the long output and then do retrieval on it when you're trying to think of like what next steps to
904680	913880	take. Agents can often go off track, especially in long running things. And so there's kind of
914040	917960	two methods that I've seen to kind of like keep them on track. One, you can just reiterate the
917960	925160	objective right before it makes its action. And why this works, I think we've seen that with,
925160	929080	at least with a lot of the current models, with instructions that are earlier in the prompt,
929080	932680	it might forget it by the time it gets to the end if it's a really long prompt. So putting it at the
932680	937400	end seems to help. And then another really interesting one that I'll talk about when I talk about some
937400	942200	of the more recent papers and stuff that have come out is this idea of separating explicitly a
942200	949320	planning and execution step and basically have one step that explicitly kind of thinks about,
949320	954360	these are kind of like all the objectives that I want to do at a high level. And then a second
954360	960280	step that says, okay, given this objective, given this one sub objective, now how do I do this one
960280	964840	sub objective and basically break it down even more in a hierarchical whole manner. And there's
964840	969720	a good example of that with baby AGI, which I'll talk about in a bit. And then another big issue
969720	975240	is evaluation of these things. I think evaluation of language models in general, very difficult
975240	981320	evaluation of applications built on top of language models. Also very difficult and agents are no
981320	987000	exception. I think there's the obvious kind of like evaluate whether it arrived at the correct
987000	992600	result in terms of in terms of getting metrics on evaluation. And so yeah, you know, if you're
992600	998360	asking the agent to produce some answer, that's like a natural language response. There's techniques
998360	1004040	you can do there. A lot of them in the flavor of asking a language model to score the expected
1004040	1008440	answer and the actual answer and come up with some grade and stuff like that, that applies to the
1008440	1012520	output of agents as well. But then there's also some agent specific ones that I think are really
1012520	1017400	interesting, mostly around evaluating these idea of like the agent trajectory or the intermediate
1017400	1025320	steps. And so where we'll actually have something coming out for this, someone opened a PR that I
1025320	1028600	need to get in. But basically, there's a lot of like little different things you can look at,
1028600	1034040	like did it take the correct action? Is the input to the action correct? Is it the correct number
1034040	1038680	of steps? And by this, you know, like sometimes you, and this is very related to the next one,
1038680	1042200	which is like the most efficient sequence of steps. And so there's a bunch of different things that
1042200	1047160	you can do to evaluate not only the final answer, but like is the agent getting there like efficiently,
1047160	1053320	correctly. And those are sometimes just as useful, if not more useful than evaluating the end result.
1054040	1058200	I'm trying to see what time it is, because I also want to leave lots of time for questions.
1058920	1062920	But I think I'm good. So memory, I think is really interesting as well. So we've obviously
1063640	1069160	tried it about like memory of remembering the AI to tool interactions. There's also like a more
1069160	1078040	basic idea of remembering the user to AI interactions. But I think the third type, which is showing up
1078040	1083880	in a lot of the recent papers on agents is this idea of like personalization of giving an agent
1083880	1089960	kind of like its own kind of like objective and own persona and stuff like that. The most obvious
1089960	1093480	way to do that is just like you encode it in the prompt. You say like, Hey, like, you know, this
1093480	1098600	is your job as an agent, you're supposed to do this, yada, yada, yada. But I think there's some
1098600	1104120	really cool work being done on how to kind of like evolve that over time and give agents a sense of
1104120	1109560	like this long term memory. And one of the papers in particular around generative agents, I think
1109560	1117080	does a really interesting job of diving into this. And I think when a lot of people, the reason this
1117080	1121240	is here in the agent section is I think when people think of agents, there's the obvious like
1121240	1125880	kind of like tool usage deciding what to do. But I think agents is also starting to take on this
1125880	1134760	concept of some kind of like more encapsulated kind of like program that that adapts over time
1134760	1141560	and memory is a big part of that. And so I think memories is there's a lot to explore here. So
1141560	1147640	that's why this is a bit of an outlier slide. Okay, I wanted to chat very quickly about four
1148280	1154600	projects that that came out in the past two, three weeks, specifically how they relate, build upon,
1154600	1161080	improve upon this side, the react style agent that has been around for a while. First up is auto
1161080	1172920	GPT, which I'm assuming most people have heard of. There we go. All right. So auto GPT, the one of
1172920	1178360	the main differences between this and the react style agents is just the objective of what it's
1178360	1183480	trying to solve auto GPT. A lot of the initial goals are like, you know, improve or increase my
1183480	1187640	Twitter following or something like that very kind of like open ended broad long running goals,
1187640	1190840	react on the other hand was designed and benchmarked on more kind of like
1192840	1199720	short lived kind of like really immediately quantifiable or more immediately quantifiable
1199720	1204520	goals. And so as a result, one of the things that auto GPT introduced is this idea of long
1204520	1209240	term memory between the agent and tools interactions and using a retriever vector store for that,
1209240	1214440	which becomes necessary because now you have this doing like 20 or 30 kind of like steps and it's
1214440	1218680	this really long running project. And so it's something that react just didn't need, but due
1218680	1225320	to the change in objectives, auto GPT kind of had to introduce baby AGI is another popular one.
1225320	1230120	It also has this idea of long term memory for the agent tool interactions. And this is the project
1230120	1233880	that introduced separate kind of like planning and execution steps, which I think is a really
1233880	1240840	interesting idea to improve upon some of the long running objectives. And so specifically,
1240840	1245080	it comes up with tasks, it then takes the first tasks, it then thinks about how to do that,
1245080	1250200	which usually involves actually baby AGI initially didn't have any tools. So it kind of just like
1250200	1255640	made stuff up. I think that I think they're giving it tools now so it can actually actually execute
1255640	1260120	those things. But the idea of separating the planning execution steps is I think that's a
1260120	1265720	really interesting idea that might help with some of the reliability and focus issues of longer term
1266440	1273800	agents. Camel is another paper that came out. The main novel thing here was they put two agents
1274440	1278200	in a simulation environment, which in this case, because it was just two was just a chat room
1278200	1284040	and had them interact with each other. And so the agents themselves, I think, were basically just
1284040	1288200	kind of like prompted language models. So I don't even think they were hooked up with tools. But
1288200	1293240	going back to this idea of kind of like memory and personalization, when people kind of like talk
1293240	1297720	about agents, that is part of what they're talking about. And so I think like the camel paper in my
1297720	1304920	mind, the main thing is this idea of simulation environment. There's maybe like two reasons
1304920	1310440	you might want to do this and have a simulation environment. One is kind of like practically
1310440	1315160	to maybe like evaluate an agent if you're kind of like testing out an agent and you want to see
1315160	1319000	how it's interacting. And for whatever reason, you don't want to test it out yourself. So you
1319000	1323560	put two of them and you kind of like make sure they don't go off the rails or something like that.
1324280	1330280	Another one is just kind of entertainment purposes. So there are a lot of examples of this by people.
1330280	1334280	I think there was one with like a VC and a founder and that had them chatting with each other and
1334280	1339400	kind of like solving stuff there. So this is a little bit entertainment, a little bit practical.
1340280	1345080	Generative agents was another paper that came out. I think this was maybe like a week and a
1345080	1350760	half ago, so very recent. It also had a simulation environment aspect. It was more complex. So I
1350760	1354840	think they had like 25 different agents and kind of like a Sims-like world interacting with each
1354840	1362600	other. So a much more complex environment setup. And then they also did some really cool stuff
1362680	1371720	around memory and reflection. So memory refers to basically remembering previous things that
1371720	1376200	happened in the world. So basically in the simulation environment, they had kind of like
1376200	1380840	things that happened. Then they had the agents decide what to do, take actions, observe kind of
1380840	1385320	like the results of those actions, observe more things that came in. And so all of this
1385320	1390120	is encapsulated in the idea of memory. And then you fetch things from this memory to inform kind
1390200	1398280	of like their actions in next time sequences. So there was three kind of like main components
1398280	1402120	to this memory retrieval thing. They had a time-weighted component which basically fetched
1402120	1408680	more recent memories. They had an important weighted piece which fetched more like important
1408680	1416920	information. So trivial things like I forget what I had for breakfast today, but I don't know what's
1416920	1421080	something that's really, but I remember meeting Charles way back when, right? So there's different
1421080	1424680	levels of importance there that get subscribed to events. And so you want to fetch events that are
1424680	1430040	kind of like more bigger in importance. And then they had the typical kind of like relevancy
1430040	1433720	weighted things. So depending on what situation you're in, you want to remember events that are
1433720	1438680	relevant for that. Then they also introduced a really interesting reflection step, which basically
1439800	1445080	after like, I think they, I think it was like 20 different steps or something happened, they would
1445080	1451000	reflect on those things and kind of like update different states of the world. And so I think this
1451000	1455880	is, I've been thinking about this a bit because I think this idea of like reflecting on recent
1455880	1461960	things and then updating state is maybe like a generalization that can be kind of like applied
1461960	1466920	to a bunch of different things. So some of the other memory types that we have in Lang chain
1466920	1472040	are we have like an entity memory type, which basically based on conversation kind of like
1472040	1476440	extracts relevant entities and then constructs some type of graph and updates that there's a more
1476440	1481400	general kind of like knowledge graph version of that as well. And then we also have kind of like a
1481400	1488120	summary conversation memory, which based on the conversation updates a running summary. So you
1488120	1491640	can get around some of the context window lengths. And so I think if you look at it sort of through
1491640	1496520	a certain angle, all of those kind of like relate to this idea of taking recent observations
1496600	1502120	and updating some state, whether that state is like a graph or just a piece of text or
1502120	1506680	anything like that. So there's also been some other papers recently that have incorporated
1506680	1510920	this idea of like reflection. I haven't had time to read those as carefully, but I think that's
1512040	1515160	yeah, I don't know, my personal take is I think that's really interesting and something to keep
1515160	1522040	an eye out for the future. And that's it. I have no idea what time it is because I can't see the
1522040	1535560	time, but I'm happy to take questions until Charles kicks me off.
