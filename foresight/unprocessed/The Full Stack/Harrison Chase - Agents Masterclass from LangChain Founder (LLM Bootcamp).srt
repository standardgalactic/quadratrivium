1
00:00:00,000 --> 00:00:07,200
So, I'll be talking about agents, and yeah, there are many things in the lane chain, but

2
00:00:07,200 --> 00:00:10,240
I think agents are probably the most interesting one to talk about, so that's what I'll be

3
00:00:10,240 --> 00:00:11,240
doing.

4
00:00:11,240 --> 00:00:15,840
I'll cover kind of like, what are agents, why use agents, the typical implementation

5
00:00:15,840 --> 00:00:21,120
of agents, talk about React, which is one of the first prompting strategies that really

6
00:00:21,120 --> 00:00:27,080
accelerated and made reliable the use of agents, then I'll talk a bunch about challenges with

7
00:00:27,080 --> 00:00:31,880
agents and challenges of getting them to work reliably, getting them to work in production.

8
00:00:31,880 --> 00:00:36,680
I'll then touch a little bit on memory and segue that into more recent kind of like

9
00:00:36,680 --> 00:00:40,960
papers and projects that do agentic things.

10
00:00:40,960 --> 00:00:46,680
I'll probably skim over the initial slides because I think most people here are probably

11
00:00:46,680 --> 00:00:51,400
familiar with the idea of agents, but the core idea of agents is using the language model

12
00:00:51,400 --> 00:00:56,560
as a reasoning engine, so using it to determine kind of like what to do and how to interact

13
00:00:56,560 --> 00:01:01,640
with the outside world, and this means that there is a non-deterministic kind of like sequence

14
00:01:01,640 --> 00:01:06,920
of actions that'll be taken depending on the user input, so there's no kind of like hard

15
00:01:06,920 --> 00:01:13,040
coded do A, then do B, then do C, rather the agent determines what actions to do depending

16
00:01:13,040 --> 00:01:18,280
on the user input and depending on results of previous actions.

17
00:01:18,280 --> 00:01:21,480
So why would you want to do this in the first place?

18
00:01:21,480 --> 00:01:26,640
So one, there's this very tied to agents is the idea of tool usage and connecting it

19
00:01:26,640 --> 00:01:31,720
to the outside world, and so connecting it to other sources of data or computation like

20
00:01:31,720 --> 00:01:35,600
search, APIs, databases, these are all very useful to overcome.

21
00:01:35,600 --> 00:01:39,360
Some of the limitations of language models, such as they don't know about your data, they

22
00:01:39,360 --> 00:01:45,880
can't do math amazingly well, but the idea of tool usage isn't kind of like unique to

23
00:01:45,880 --> 00:01:50,920
agents, you can still use tools, you can still connect LLMs to search engines without using

24
00:01:51,000 --> 00:01:53,080
an agent, so why use an agent?

25
00:01:53,080 --> 00:01:57,840
And I think some of the benefits of agents are that they're more flexible, they're more

26
00:01:57,840 --> 00:02:01,160
powerful, they allow you to kind of like recover from errors better, they allow you to handle

27
00:02:01,160 --> 00:02:05,200
kind of like multi-hop tasks, again with this idea of being the reasoning engine, and so

28
00:02:05,200 --> 00:02:11,240
an example that I like to use here is thinking about like interacting with a SQL database.

29
00:02:11,240 --> 00:02:15,720
You can do that in a sequence of predetermined steps, you can have kind of like a natural

30
00:02:15,720 --> 00:02:19,960
language query, which you then use a language model to convert to a SQL query, which you

31
00:02:19,960 --> 00:02:25,160
then pass, you then execute that, get back a result, pass that back to the language model,

32
00:02:25,160 --> 00:02:29,840
ask it to synthesize it with respect to the original question, and get kind of this natural

33
00:02:29,840 --> 00:02:34,080
language wrapper around a SQL database, very useful by itself, but there are a lot of edge

34
00:02:34,080 --> 00:02:37,840
cases and things that can go wrong, so there could be an error in the SQL query, maybe

35
00:02:37,840 --> 00:02:44,360
it hallucinates a table name or a field name, maybe it just writes incorrect SQL, and then

36
00:02:44,360 --> 00:02:48,240
there are also queries that need multiple kind of like queries to be made under the

37
00:02:48,240 --> 00:02:56,280
hood in order to answer, and so although kind of like a simple chain can handle maybe, I

38
00:02:56,280 --> 00:03:00,280
don't know, 50, 80% of the use cases, you very quickly run into these like edge cases

39
00:03:00,280 --> 00:03:05,920
where a more flexible framework like agents helps kind of like circumvent.

40
00:03:05,920 --> 00:03:10,440
So the typical implementation of agents generally, and it's so early in this field that it kind

41
00:03:10,440 --> 00:03:13,600
of feels a bit weird to be talking about a typical implementation because I'm sure we'll

42
00:03:13,680 --> 00:03:20,320
see a bunch of different variants, but generally you get a user query, you use the LLM, that's

43
00:03:20,320 --> 00:03:27,240
the agent to choose a tool to use, and also the input to that tool, you then do that,

44
00:03:27,240 --> 00:03:32,000
you take that action, you get back an observation, and then you feed that back into the language

45
00:03:32,000 --> 00:03:36,640
model and you kind of continue doing this until a stopping condition is met, and so there

46
00:03:36,640 --> 00:03:39,880
can be different types of stopping conditions, probably the most common is the language model

47
00:03:39,920 --> 00:03:46,600
itself realizes, hey, I'm done with this task, with this question that someone asked of me, I

48
00:03:46,600 --> 00:03:51,160
should now return to the user, but there can be other more hard-coded rules, and so when we talk

49
00:03:51,160 --> 00:03:55,840
about like reliability of agents, some of these can really help with that, so you know, if an

50
00:03:55,840 --> 00:04:00,800
agent has done five different steps in a row and it hasn't reached a final answer, it might be nice

51
00:04:00,800 --> 00:04:05,280
to have it just return something then. There are also certain tools that you can just like return

52
00:04:05,360 --> 00:04:10,080
automatically, so basically the general idea is choose a tool to use, observe the output of that

53
00:04:10,080 --> 00:04:16,920
tool, and kind of continue going. So how do you actually like, so that was pseudocode, now let's

54
00:04:16,920 --> 00:04:25,200
talk about the actual kind of like ways to get this to do what we want, and the first and still

55
00:04:25,200 --> 00:04:32,360
kind of like the main way of doing this, the main prompting strategy slash algorithm slash method

56
00:04:32,360 --> 00:04:38,680
for doing this is react, which stands for reasoning and then acting, so RE from reasoning, ACT

57
00:04:38,680 --> 00:04:44,200
from acting, and it's the papers, a great paper came out in I think October out of Princeton,

58
00:04:44,200 --> 00:04:51,120
synergizing two different kind of like methods, and we can look at this example which is taken

59
00:04:51,120 --> 00:04:58,640
from their paper and see why it's so effective. So this example comes from the hotspot QA data set,

60
00:04:58,960 --> 00:05:04,240
which is basically a data set where it's asking questions over Wikipedia pages where there are

61
00:05:04,240 --> 00:05:09,440
multi-hop usually kind of like two or three intermediate questions that need to be reasoned

62
00:05:09,440 --> 00:05:14,600
about before answering. So we can see, so here there's this question aside from the Apple remote,

63
00:05:14,600 --> 00:05:21,120
what other device can control the program Apple remote was originally designed to interact with,

64
00:05:21,120 --> 00:05:26,640
and so the most basic prompting strategy is kind of just pass that into the language model and

65
00:05:26,640 --> 00:05:32,040
get back an answer, and so we can see that in 1A standard, and it just returns a single answer,

66
00:05:32,040 --> 00:05:36,480
and we can see that it's wrong. Another method that had emerged maybe like a month or so prior

67
00:05:36,480 --> 00:05:43,480
was the idea of like chain of thought reasoning, and so this is usually associated with the let's

68
00:05:43,480 --> 00:05:50,320
think step by step prefix to the response, and you can see in red that there's this like chain of

69
00:05:50,320 --> 00:05:56,960
thought thing that's happening as it thinks through step by step, and it returns an answer

70
00:05:56,960 --> 00:06:03,560
that's also incorrect in this case. This has been shown to kind of like get the agent or get the

71
00:06:03,560 --> 00:06:09,760
language model to think a little bit better, so to speak. So it's yielding higher quality,

72
00:06:09,760 --> 00:06:16,320
more reliable results. The issue is it still only knows kind of like what is actually present in

73
00:06:16,320 --> 00:06:20,040
the data that the language model is trained on, and so there's another technique that came out

74
00:06:20,080 --> 00:06:25,880
which is basically action only, where you give it access to different tools. In this case,

75
00:06:25,880 --> 00:06:32,680
I think all the examples in this picture are of search, but I think in the paper it had search

76
00:06:32,680 --> 00:06:39,000
and then look up, and so you can see here that the language model outputs kind of like search

77
00:06:39,000 --> 00:06:43,600
Apple remote, it looks up Apple remote, it gets back an observation, it then does search front row,

78
00:06:44,560 --> 00:06:48,920
can find that, so that's an instance of it kind of like recovering from an error,

79
00:06:48,920 --> 00:06:58,560
and then it does search front row software, finds an answer, and then finishes. And you can see

80
00:06:58,560 --> 00:07:06,320
here that the output that it gave, yes, kind of loses some of what it was actually supposed to

81
00:07:06,320 --> 00:07:11,120
answer. So you've got this chain of thought reasoning which helps the language model think

82
00:07:11,120 --> 00:07:18,000
about what to do, then you've got this action taking step that basically actually allows it to

83
00:07:18,000 --> 00:07:22,640
plug into more kind of like sources of real data, what if you combine them, and that's the idea

84
00:07:22,640 --> 00:07:31,040
of React, and that's the idea of a lot of the popular agent frameworks. Because again, agents use

85
00:07:31,040 --> 00:07:36,280
a language model as a reasoning engine, so you want it to be really good at reasoning. So if

86
00:07:36,280 --> 00:07:40,480
there are any prompting techniques you can use to improve its reasoning, you should probably do

87
00:07:40,480 --> 00:07:44,320
those, and then a big part of it is also connecting into tools, and that's where action comes in.

88
00:07:44,320 --> 00:07:52,000
And so you can see here, it arrives at kind of like the final answer. So that's the idea of

89
00:07:52,000 --> 00:07:57,160
agents, React is still one of the more popular implementations for it, but what are some of the

90
00:07:57,160 --> 00:08:04,160
current challenges? And there are a lot of challenges, like I think most agents are not

91
00:08:04,160 --> 00:08:10,360
amazingly production ready at the moment, and these are some of the reasons why, and we can

92
00:08:11,240 --> 00:08:17,320
walk through them in a bunch more detail. I'll also leave a lot of time at ends for the questions,

93
00:08:17,320 --> 00:08:22,120
so I'd love to hear kind of like what you guys are observing as issues for getting agents to work

94
00:08:22,120 --> 00:08:27,640
reliably. This is probably far from a complete list. So the most basic challenge with agents is

95
00:08:27,640 --> 00:08:34,840
getting them to use tools in appropriate scenarios. And so in the React paper, they address this

96
00:08:34,920 --> 00:08:40,560
challenge by bringing in the reasoning aspect of it, the chain of thought, style, prompting,

97
00:08:40,560 --> 00:08:47,400
asking it to think. Kind of like common ways to do this include just like saying in the instructions,

98
00:08:47,400 --> 00:08:51,720
you have access to these tools, you should use these to overcome some of your limitations,

99
00:08:51,720 --> 00:08:56,520
and just basically instructing the language model. Tool descriptions are really, really

100
00:08:56,520 --> 00:09:01,160
important for this. If you want the agent to use a particular tool, it should probably have

101
00:09:01,160 --> 00:09:07,240
enough context to know that this tool is good at this, and that generally comes in the form of

102
00:09:07,240 --> 00:09:11,160
like tool descriptions or some information about the tool that's passed into the prompt.

103
00:09:13,080 --> 00:09:17,240
That can maybe not scale super well if you've got a lot of tools, because now you've got maybe

104
00:09:17,240 --> 00:09:22,520
these more complex descriptions, you want to put them in the final prompt for the language model

105
00:09:22,520 --> 00:09:28,120
to know what to do with them, but you can quickly run into kind of like context length issues.

106
00:09:28,120 --> 00:09:32,520
And so that's where I think the idea of tool retrieval comes in handy. So you can have hundreds,

107
00:09:32,520 --> 00:09:37,480
thousands of tools, you can do some retrieval step, and I think retrieval is another really

108
00:09:37,480 --> 00:09:40,840
interesting topic that I'm not going to go into too much depth here. So for the sake of this,

109
00:09:40,840 --> 00:09:44,680
we'll just say it's some embedding search lookup, although I think there's a lot more interesting

110
00:09:44,680 --> 00:09:49,480
things to do there. You basically do the retrieval step, you get back five, ten, however many tools

111
00:09:49,480 --> 00:09:53,400
that you think are most promising, you can then pass those to the prompt and kind of have the

112
00:09:53,400 --> 00:09:58,280
language model take the final steps from there. Few shot examples I think can also be really

113
00:09:58,280 --> 00:10:03,880
helpful, so you can use those to guide the language model in what to do. Again, I think the idea

114
00:10:03,880 --> 00:10:08,520
of retrieval to find the most relevant few shot examples is particularly promising here. So if

115
00:10:08,520 --> 00:10:12,760
you give it examples similar to the one it's trying to do, those help a lot better than random

116
00:10:12,760 --> 00:10:17,320
examples. And then probably the most extreme version of this is fine tuning a model like

117
00:10:17,320 --> 00:10:24,840
tool former to really help with tool selection. There's also a subtle second challenge which

118
00:10:24,840 --> 00:10:29,240
is getting them not to use tools when they don't need to. So a big use case for agents is having

119
00:10:29,240 --> 00:10:34,600
conversational style agents. One of the big problems that we've seen is oftentimes these types

120
00:10:34,600 --> 00:10:39,320
of agents just want to use tools no matter what, even if they're having a conversation. And so

121
00:10:39,320 --> 00:10:44,680
again, like the most basic thing you can do is probably put some information in the instructions,

122
00:10:44,680 --> 00:10:48,920
some reminder in the prompt like, hey, you don't have to use a tool, you can respond to the user

123
00:10:48,920 --> 00:10:54,920
if it seems like it's more of a conversation. That can get you so far. Another kind of like clever

124
00:10:54,920 --> 00:11:00,120
hack that we've seen here is add another tool that explicitly just returns to the user. And then,

125
00:11:00,120 --> 00:11:05,880
you know, they like to use tools, but they'll usually use that tool. So I thought that was a

126
00:11:05,880 --> 00:11:13,240
pretty clever and interesting hack. A third challenge is the language models tell you what

127
00:11:13,240 --> 00:11:19,720
tool to use and how to use it. But that's in the form of a string. And so you need to go from that

128
00:11:19,720 --> 00:11:24,920
string into some code or something that can actually be run. And so that involves parsing kind

129
00:11:24,920 --> 00:11:30,200
of like the output of the language model into this tool invocation. And so some tips and tricks

130
00:11:30,200 --> 00:11:33,960
and hacks here are one like the more structured you ask for the response, the easier it is to

131
00:11:33,960 --> 00:11:39,800
parse generally. So language models are pretty good at writing JSON. So we've kind of transitioned

132
00:11:39,880 --> 00:11:46,600
a few of our agents to use that schema. Still doesn't always work, especially some of the

133
00:11:46,600 --> 00:11:55,240
chat models like to add in a lot of kind of like language. So we've introduced kind of like this

134
00:11:55,240 --> 00:12:00,120
concept of output parsers, which generically encapsulate all the logic that's needed to

135
00:12:00,120 --> 00:12:05,160
parse this response. And we've tried to make that as modular as possible. So if you're seeing areas,

136
00:12:05,160 --> 00:12:10,200
you can hopefully kind of like substitute that out very related to that. We also have a concept of

137
00:12:10,200 --> 00:12:17,240
like output parsers that can retry and fix mistakes. So and I think there's there's some subtle,

138
00:12:18,120 --> 00:12:21,800
there's some subtle differences here that I think are really cool. Basically, like, you know,

139
00:12:21,800 --> 00:12:28,280
if you have misformatted schema, you can fix that explicitly by just passing it the output

140
00:12:28,280 --> 00:12:34,120
and the error and saying fix this response. But if you have, if you have an output that just

141
00:12:34,120 --> 00:12:38,520
forgets one of the fields, like it returns the action, but not the action input or something

142
00:12:38,520 --> 00:12:41,800
like that, you need to provide more information here. So I think there's actually a lot of

143
00:12:41,800 --> 00:12:46,040
subtlety in fixing some of these errors. But the basic idea is that you can try to parse it.

144
00:12:46,920 --> 00:12:51,960
If you if it fails, you can then try to fix it. All this we currently encapsulate in this idea

145
00:12:51,960 --> 00:12:58,760
of like output parsers. So the fourth challenge is getting them to remember previous steps that

146
00:12:58,760 --> 00:13:02,920
were taken. The most basic thing, the thing that the React paper does is just keep a list of those

147
00:13:02,920 --> 00:13:10,600
steps in memory. Again, that starts to run into some some context window issues, especially when

148
00:13:10,600 --> 00:13:15,640
you're dealing with long running tasks. And so the thing that we've seen done here is again,

149
00:13:15,640 --> 00:13:19,960
fetch previous steps with some retrieval method and put those into context. Usually we've actually

150
00:13:19,960 --> 00:13:26,840
seen a combination of the two. So we've seen using the end most recent actions and observations

151
00:13:26,840 --> 00:13:34,760
combined with the K most relevant actions and observations. Incorporating long observations

152
00:13:34,760 --> 00:13:39,000
is another really interesting one. This actually comes up, or this came up a lot when we were

153
00:13:39,000 --> 00:13:44,760
dealing with working with APIs, because APIs often return really big JSON blobs that are really big

154
00:13:44,760 --> 00:13:51,240
and hard to put in context. So the most common thing that we've done here is just parse that in

155
00:13:51,240 --> 00:13:57,000
some way. You can do really simple stuff like convert that blob to a string and put the first

156
00:13:57,000 --> 00:14:02,120
like 1000 characters or something as the response. You can also do some more if you know that you're

157
00:14:02,120 --> 00:14:07,960
working with a specific API, you can probably write some custom logic to kind of like take only the

158
00:14:07,960 --> 00:14:12,440
relevant keys and put that if you want to make something general, you could also maybe do something

159
00:14:12,440 --> 00:14:17,720
dynamically to like figure out what key like basically explore the JSON object and figure out

160
00:14:17,720 --> 00:14:23,080
what keys to put in. That's a bit more exploratory, I would say. But the basic idea is, yeah, there

161
00:14:23,080 --> 00:14:29,160
is this issue of, and so Zapier, I always have to think about how to pronounce it, but it's Zapier

162
00:14:29,160 --> 00:14:34,440
makes you happier. So Zapier when they did this with their natural language API, not only did they

163
00:14:34,440 --> 00:14:40,440
have something before the API that was like natural language to some API call, they also spent a lot

164
00:14:40,440 --> 00:14:44,120
of time working on the output. And so the output is actually very specifically, I think it's like

165
00:14:44,120 --> 00:14:48,440
under like 200 or 300 tokens or something like that. And they did that on purpose. They spent a

166
00:14:48,440 --> 00:14:52,600
lot of time thinking about that. And so I think for tool usage, that is really important as well.

167
00:14:53,800 --> 00:14:59,800
Another more kind of like exploratory way of doing this is also you could perhaps just store

168
00:14:59,800 --> 00:15:04,680
the long output and then do retrieval on it when you're trying to think of like what next steps to

169
00:15:04,680 --> 00:15:13,880
take. Agents can often go off track, especially in long running things. And so there's kind of

170
00:15:14,040 --> 00:15:17,960
two methods that I've seen to kind of like keep them on track. One, you can just reiterate the

171
00:15:17,960 --> 00:15:25,160
objective right before it makes its action. And why this works, I think we've seen that with,

172
00:15:25,160 --> 00:15:29,080
at least with a lot of the current models, with instructions that are earlier in the prompt,

173
00:15:29,080 --> 00:15:32,680
it might forget it by the time it gets to the end if it's a really long prompt. So putting it at the

174
00:15:32,680 --> 00:15:37,400
end seems to help. And then another really interesting one that I'll talk about when I talk about some

175
00:15:37,400 --> 00:15:42,200
of the more recent papers and stuff that have come out is this idea of separating explicitly a

176
00:15:42,200 --> 00:15:49,320
planning and execution step and basically have one step that explicitly kind of thinks about,

177
00:15:49,320 --> 00:15:54,360
these are kind of like all the objectives that I want to do at a high level. And then a second

178
00:15:54,360 --> 00:16:00,280
step that says, okay, given this objective, given this one sub objective, now how do I do this one

179
00:16:00,280 --> 00:16:04,840
sub objective and basically break it down even more in a hierarchical whole manner. And there's

180
00:16:04,840 --> 00:16:09,720
a good example of that with baby AGI, which I'll talk about in a bit. And then another big issue

181
00:16:09,720 --> 00:16:15,240
is evaluation of these things. I think evaluation of language models in general, very difficult

182
00:16:15,240 --> 00:16:21,320
evaluation of applications built on top of language models. Also very difficult and agents are no

183
00:16:21,320 --> 00:16:27,000
exception. I think there's the obvious kind of like evaluate whether it arrived at the correct

184
00:16:27,000 --> 00:16:32,600
result in terms of in terms of getting metrics on evaluation. And so yeah, you know, if you're

185
00:16:32,600 --> 00:16:38,360
asking the agent to produce some answer, that's like a natural language response. There's techniques

186
00:16:38,360 --> 00:16:44,040
you can do there. A lot of them in the flavor of asking a language model to score the expected

187
00:16:44,040 --> 00:16:48,440
answer and the actual answer and come up with some grade and stuff like that, that applies to the

188
00:16:48,440 --> 00:16:52,520
output of agents as well. But then there's also some agent specific ones that I think are really

189
00:16:52,520 --> 00:16:57,400
interesting, mostly around evaluating these idea of like the agent trajectory or the intermediate

190
00:16:57,400 --> 00:17:05,320
steps. And so where we'll actually have something coming out for this, someone opened a PR that I

191
00:17:05,320 --> 00:17:08,600
need to get in. But basically, there's a lot of like little different things you can look at,

192
00:17:08,600 --> 00:17:14,040
like did it take the correct action? Is the input to the action correct? Is it the correct number

193
00:17:14,040 --> 00:17:18,680
of steps? And by this, you know, like sometimes you, and this is very related to the next one,

194
00:17:18,680 --> 00:17:22,200
which is like the most efficient sequence of steps. And so there's a bunch of different things that

195
00:17:22,200 --> 00:17:27,160
you can do to evaluate not only the final answer, but like is the agent getting there like efficiently,

196
00:17:27,160 --> 00:17:33,320
correctly. And those are sometimes just as useful, if not more useful than evaluating the end result.

197
00:17:34,040 --> 00:17:38,200
I'm trying to see what time it is, because I also want to leave lots of time for questions.

198
00:17:38,920 --> 00:17:42,920
But I think I'm good. So memory, I think is really interesting as well. So we've obviously

199
00:17:43,640 --> 00:17:49,160
tried it about like memory of remembering the AI to tool interactions. There's also like a more

200
00:17:49,160 --> 00:17:58,040
basic idea of remembering the user to AI interactions. But I think the third type, which is showing up

201
00:17:58,040 --> 00:18:03,880
in a lot of the recent papers on agents is this idea of like personalization of giving an agent

202
00:18:03,880 --> 00:18:09,960
kind of like its own kind of like objective and own persona and stuff like that. The most obvious

203
00:18:09,960 --> 00:18:13,480
way to do that is just like you encode it in the prompt. You say like, Hey, like, you know, this

204
00:18:13,480 --> 00:18:18,600
is your job as an agent, you're supposed to do this, yada, yada, yada. But I think there's some

205
00:18:18,600 --> 00:18:24,120
really cool work being done on how to kind of like evolve that over time and give agents a sense of

206
00:18:24,120 --> 00:18:29,560
like this long term memory. And one of the papers in particular around generative agents, I think

207
00:18:29,560 --> 00:18:37,080
does a really interesting job of diving into this. And I think when a lot of people, the reason this

208
00:18:37,080 --> 00:18:41,240
is here in the agent section is I think when people think of agents, there's the obvious like

209
00:18:41,240 --> 00:18:45,880
kind of like tool usage deciding what to do. But I think agents is also starting to take on this

210
00:18:45,880 --> 00:18:54,760
concept of some kind of like more encapsulated kind of like program that that adapts over time

211
00:18:54,760 --> 00:19:01,560
and memory is a big part of that. And so I think memories is there's a lot to explore here. So

212
00:19:01,560 --> 00:19:07,640
that's why this is a bit of an outlier slide. Okay, I wanted to chat very quickly about four

213
00:19:08,280 --> 00:19:14,600
projects that that came out in the past two, three weeks, specifically how they relate, build upon,

214
00:19:14,600 --> 00:19:21,080
improve upon this side, the react style agent that has been around for a while. First up is auto

215
00:19:21,080 --> 00:19:32,920
GPT, which I'm assuming most people have heard of. There we go. All right. So auto GPT, the one of

216
00:19:32,920 --> 00:19:38,360
the main differences between this and the react style agents is just the objective of what it's

217
00:19:38,360 --> 00:19:43,480
trying to solve auto GPT. A lot of the initial goals are like, you know, improve or increase my

218
00:19:43,480 --> 00:19:47,640
Twitter following or something like that very kind of like open ended broad long running goals,

219
00:19:47,640 --> 00:19:50,840
react on the other hand was designed and benchmarked on more kind of like

220
00:19:52,840 --> 00:19:59,720
short lived kind of like really immediately quantifiable or more immediately quantifiable

221
00:19:59,720 --> 00:20:04,520
goals. And so as a result, one of the things that auto GPT introduced is this idea of long

222
00:20:04,520 --> 00:20:09,240
term memory between the agent and tools interactions and using a retriever vector store for that,

223
00:20:09,240 --> 00:20:14,440
which becomes necessary because now you have this doing like 20 or 30 kind of like steps and it's

224
00:20:14,440 --> 00:20:18,680
this really long running project. And so it's something that react just didn't need, but due

225
00:20:18,680 --> 00:20:25,320
to the change in objectives, auto GPT kind of had to introduce baby AGI is another popular one.

226
00:20:25,320 --> 00:20:30,120
It also has this idea of long term memory for the agent tool interactions. And this is the project

227
00:20:30,120 --> 00:20:33,880
that introduced separate kind of like planning and execution steps, which I think is a really

228
00:20:33,880 --> 00:20:40,840
interesting idea to improve upon some of the long running objectives. And so specifically,

229
00:20:40,840 --> 00:20:45,080
it comes up with tasks, it then takes the first tasks, it then thinks about how to do that,

230
00:20:45,080 --> 00:20:50,200
which usually involves actually baby AGI initially didn't have any tools. So it kind of just like

231
00:20:50,200 --> 00:20:55,640
made stuff up. I think that I think they're giving it tools now so it can actually actually execute

232
00:20:55,640 --> 00:21:00,120
those things. But the idea of separating the planning execution steps is I think that's a

233
00:21:00,120 --> 00:21:05,720
really interesting idea that might help with some of the reliability and focus issues of longer term

234
00:21:06,440 --> 00:21:13,800
agents. Camel is another paper that came out. The main novel thing here was they put two agents

235
00:21:14,440 --> 00:21:18,200
in a simulation environment, which in this case, because it was just two was just a chat room

236
00:21:18,200 --> 00:21:24,040
and had them interact with each other. And so the agents themselves, I think, were basically just

237
00:21:24,040 --> 00:21:28,200
kind of like prompted language models. So I don't even think they were hooked up with tools. But

238
00:21:28,200 --> 00:21:33,240
going back to this idea of kind of like memory and personalization, when people kind of like talk

239
00:21:33,240 --> 00:21:37,720
about agents, that is part of what they're talking about. And so I think like the camel paper in my

240
00:21:37,720 --> 00:21:44,920
mind, the main thing is this idea of simulation environment. There's maybe like two reasons

241
00:21:44,920 --> 00:21:50,440
you might want to do this and have a simulation environment. One is kind of like practically

242
00:21:50,440 --> 00:21:55,160
to maybe like evaluate an agent if you're kind of like testing out an agent and you want to see

243
00:21:55,160 --> 00:21:59,000
how it's interacting. And for whatever reason, you don't want to test it out yourself. So you

244
00:21:59,000 --> 00:22:03,560
put two of them and you kind of like make sure they don't go off the rails or something like that.

245
00:22:04,280 --> 00:22:10,280
Another one is just kind of entertainment purposes. So there are a lot of examples of this by people.

246
00:22:10,280 --> 00:22:14,280
I think there was one with like a VC and a founder and that had them chatting with each other and

247
00:22:14,280 --> 00:22:19,400
kind of like solving stuff there. So this is a little bit entertainment, a little bit practical.

248
00:22:20,280 --> 00:22:25,080
Generative agents was another paper that came out. I think this was maybe like a week and a

249
00:22:25,080 --> 00:22:30,760
half ago, so very recent. It also had a simulation environment aspect. It was more complex. So I

250
00:22:30,760 --> 00:22:34,840
think they had like 25 different agents and kind of like a Sims-like world interacting with each

251
00:22:34,840 --> 00:22:42,600
other. So a much more complex environment setup. And then they also did some really cool stuff

252
00:22:42,680 --> 00:22:51,720
around memory and reflection. So memory refers to basically remembering previous things that

253
00:22:51,720 --> 00:22:56,200
happened in the world. So basically in the simulation environment, they had kind of like

254
00:22:56,200 --> 00:23:00,840
things that happened. Then they had the agents decide what to do, take actions, observe kind of

255
00:23:00,840 --> 00:23:05,320
like the results of those actions, observe more things that came in. And so all of this

256
00:23:05,320 --> 00:23:10,120
is encapsulated in the idea of memory. And then you fetch things from this memory to inform kind

257
00:23:10,200 --> 00:23:18,280
of like their actions in next time sequences. So there was three kind of like main components

258
00:23:18,280 --> 00:23:22,120
to this memory retrieval thing. They had a time-weighted component which basically fetched

259
00:23:22,120 --> 00:23:28,680
more recent memories. They had an important weighted piece which fetched more like important

260
00:23:28,680 --> 00:23:36,920
information. So trivial things like I forget what I had for breakfast today, but I don't know what's

261
00:23:36,920 --> 00:23:41,080
something that's really, but I remember meeting Charles way back when, right? So there's different

262
00:23:41,080 --> 00:23:44,680
levels of importance there that get subscribed to events. And so you want to fetch events that are

263
00:23:44,680 --> 00:23:50,040
kind of like more bigger in importance. And then they had the typical kind of like relevancy

264
00:23:50,040 --> 00:23:53,720
weighted things. So depending on what situation you're in, you want to remember events that are

265
00:23:53,720 --> 00:23:58,680
relevant for that. Then they also introduced a really interesting reflection step, which basically

266
00:23:59,800 --> 00:24:05,080
after like, I think they, I think it was like 20 different steps or something happened, they would

267
00:24:05,080 --> 00:24:11,000
reflect on those things and kind of like update different states of the world. And so I think this

268
00:24:11,000 --> 00:24:15,880
is, I've been thinking about this a bit because I think this idea of like reflecting on recent

269
00:24:15,880 --> 00:24:21,960
things and then updating state is maybe like a generalization that can be kind of like applied

270
00:24:21,960 --> 00:24:26,920
to a bunch of different things. So some of the other memory types that we have in Lang chain

271
00:24:26,920 --> 00:24:32,040
are we have like an entity memory type, which basically based on conversation kind of like

272
00:24:32,040 --> 00:24:36,440
extracts relevant entities and then constructs some type of graph and updates that there's a more

273
00:24:36,440 --> 00:24:41,400
general kind of like knowledge graph version of that as well. And then we also have kind of like a

274
00:24:41,400 --> 00:24:48,120
summary conversation memory, which based on the conversation updates a running summary. So you

275
00:24:48,120 --> 00:24:51,640
can get around some of the context window lengths. And so I think if you look at it sort of through

276
00:24:51,640 --> 00:24:56,520
a certain angle, all of those kind of like relate to this idea of taking recent observations

277
00:24:56,600 --> 00:25:02,120
and updating some state, whether that state is like a graph or just a piece of text or

278
00:25:02,120 --> 00:25:06,680
anything like that. So there's also been some other papers recently that have incorporated

279
00:25:06,680 --> 00:25:10,920
this idea of like reflection. I haven't had time to read those as carefully, but I think that's

280
00:25:12,040 --> 00:25:15,160
yeah, I don't know, my personal take is I think that's really interesting and something to keep

281
00:25:15,160 --> 00:25:22,040
an eye out for the future. And that's it. I have no idea what time it is because I can't see the

282
00:25:22,040 --> 00:25:35,560
time, but I'm happy to take questions until Charles kicks me off.

