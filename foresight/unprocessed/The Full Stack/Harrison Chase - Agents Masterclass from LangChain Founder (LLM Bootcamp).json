{"text": " So, I'll be talking about agents, and yeah, there are many things in the lane chain, but I think agents are probably the most interesting one to talk about, so that's what I'll be doing. I'll cover kind of like, what are agents, why use agents, the typical implementation of agents, talk about React, which is one of the first prompting strategies that really accelerated and made reliable the use of agents, then I'll talk a bunch about challenges with agents and challenges of getting them to work reliably, getting them to work in production. I'll then touch a little bit on memory and segue that into more recent kind of like papers and projects that do agentic things. I'll probably skim over the initial slides because I think most people here are probably familiar with the idea of agents, but the core idea of agents is using the language model as a reasoning engine, so using it to determine kind of like what to do and how to interact with the outside world, and this means that there is a non-deterministic kind of like sequence of actions that'll be taken depending on the user input, so there's no kind of like hard coded do A, then do B, then do C, rather the agent determines what actions to do depending on the user input and depending on results of previous actions. So why would you want to do this in the first place? So one, there's this very tied to agents is the idea of tool usage and connecting it to the outside world, and so connecting it to other sources of data or computation like search, APIs, databases, these are all very useful to overcome. Some of the limitations of language models, such as they don't know about your data, they can't do math amazingly well, but the idea of tool usage isn't kind of like unique to agents, you can still use tools, you can still connect LLMs to search engines without using an agent, so why use an agent? And I think some of the benefits of agents are that they're more flexible, they're more powerful, they allow you to kind of like recover from errors better, they allow you to handle kind of like multi-hop tasks, again with this idea of being the reasoning engine, and so an example that I like to use here is thinking about like interacting with a SQL database. You can do that in a sequence of predetermined steps, you can have kind of like a natural language query, which you then use a language model to convert to a SQL query, which you then pass, you then execute that, get back a result, pass that back to the language model, ask it to synthesize it with respect to the original question, and get kind of this natural language wrapper around a SQL database, very useful by itself, but there are a lot of edge cases and things that can go wrong, so there could be an error in the SQL query, maybe it hallucinates a table name or a field name, maybe it just writes incorrect SQL, and then there are also queries that need multiple kind of like queries to be made under the hood in order to answer, and so although kind of like a simple chain can handle maybe, I don't know, 50, 80% of the use cases, you very quickly run into these like edge cases where a more flexible framework like agents helps kind of like circumvent. So the typical implementation of agents generally, and it's so early in this field that it kind of feels a bit weird to be talking about a typical implementation because I'm sure we'll see a bunch of different variants, but generally you get a user query, you use the LLM, that's the agent to choose a tool to use, and also the input to that tool, you then do that, you take that action, you get back an observation, and then you feed that back into the language model and you kind of continue doing this until a stopping condition is met, and so there can be different types of stopping conditions, probably the most common is the language model itself realizes, hey, I'm done with this task, with this question that someone asked of me, I should now return to the user, but there can be other more hard-coded rules, and so when we talk about like reliability of agents, some of these can really help with that, so you know, if an agent has done five different steps in a row and it hasn't reached a final answer, it might be nice to have it just return something then. There are also certain tools that you can just like return automatically, so basically the general idea is choose a tool to use, observe the output of that tool, and kind of continue going. So how do you actually like, so that was pseudocode, now let's talk about the actual kind of like ways to get this to do what we want, and the first and still kind of like the main way of doing this, the main prompting strategy slash algorithm slash method for doing this is react, which stands for reasoning and then acting, so RE from reasoning, ACT from acting, and it's the papers, a great paper came out in I think October out of Princeton, synergizing two different kind of like methods, and we can look at this example which is taken from their paper and see why it's so effective. So this example comes from the hotspot QA data set, which is basically a data set where it's asking questions over Wikipedia pages where there are multi-hop usually kind of like two or three intermediate questions that need to be reasoned about before answering. So we can see, so here there's this question aside from the Apple remote, what other device can control the program Apple remote was originally designed to interact with, and so the most basic prompting strategy is kind of just pass that into the language model and get back an answer, and so we can see that in 1A standard, and it just returns a single answer, and we can see that it's wrong. Another method that had emerged maybe like a month or so prior was the idea of like chain of thought reasoning, and so this is usually associated with the let's think step by step prefix to the response, and you can see in red that there's this like chain of thought thing that's happening as it thinks through step by step, and it returns an answer that's also incorrect in this case. This has been shown to kind of like get the agent or get the language model to think a little bit better, so to speak. So it's yielding higher quality, more reliable results. The issue is it still only knows kind of like what is actually present in the data that the language model is trained on, and so there's another technique that came out which is basically action only, where you give it access to different tools. In this case, I think all the examples in this picture are of search, but I think in the paper it had search and then look up, and so you can see here that the language model outputs kind of like search Apple remote, it looks up Apple remote, it gets back an observation, it then does search front row, can find that, so that's an instance of it kind of like recovering from an error, and then it does search front row software, finds an answer, and then finishes. And you can see here that the output that it gave, yes, kind of loses some of what it was actually supposed to answer. So you've got this chain of thought reasoning which helps the language model think about what to do, then you've got this action taking step that basically actually allows it to plug into more kind of like sources of real data, what if you combine them, and that's the idea of React, and that's the idea of a lot of the popular agent frameworks. Because again, agents use a language model as a reasoning engine, so you want it to be really good at reasoning. So if there are any prompting techniques you can use to improve its reasoning, you should probably do those, and then a big part of it is also connecting into tools, and that's where action comes in. And so you can see here, it arrives at kind of like the final answer. So that's the idea of agents, React is still one of the more popular implementations for it, but what are some of the current challenges? And there are a lot of challenges, like I think most agents are not amazingly production ready at the moment, and these are some of the reasons why, and we can walk through them in a bunch more detail. I'll also leave a lot of time at ends for the questions, so I'd love to hear kind of like what you guys are observing as issues for getting agents to work reliably. This is probably far from a complete list. So the most basic challenge with agents is getting them to use tools in appropriate scenarios. And so in the React paper, they address this challenge by bringing in the reasoning aspect of it, the chain of thought, style, prompting, asking it to think. Kind of like common ways to do this include just like saying in the instructions, you have access to these tools, you should use these to overcome some of your limitations, and just basically instructing the language model. Tool descriptions are really, really important for this. If you want the agent to use a particular tool, it should probably have enough context to know that this tool is good at this, and that generally comes in the form of like tool descriptions or some information about the tool that's passed into the prompt. That can maybe not scale super well if you've got a lot of tools, because now you've got maybe these more complex descriptions, you want to put them in the final prompt for the language model to know what to do with them, but you can quickly run into kind of like context length issues. And so that's where I think the idea of tool retrieval comes in handy. So you can have hundreds, thousands of tools, you can do some retrieval step, and I think retrieval is another really interesting topic that I'm not going to go into too much depth here. So for the sake of this, we'll just say it's some embedding search lookup, although I think there's a lot more interesting things to do there. You basically do the retrieval step, you get back five, ten, however many tools that you think are most promising, you can then pass those to the prompt and kind of have the language model take the final steps from there. Few shot examples I think can also be really helpful, so you can use those to guide the language model in what to do. Again, I think the idea of retrieval to find the most relevant few shot examples is particularly promising here. So if you give it examples similar to the one it's trying to do, those help a lot better than random examples. And then probably the most extreme version of this is fine tuning a model like tool former to really help with tool selection. There's also a subtle second challenge which is getting them not to use tools when they don't need to. So a big use case for agents is having conversational style agents. One of the big problems that we've seen is oftentimes these types of agents just want to use tools no matter what, even if they're having a conversation. And so again, like the most basic thing you can do is probably put some information in the instructions, some reminder in the prompt like, hey, you don't have to use a tool, you can respond to the user if it seems like it's more of a conversation. That can get you so far. Another kind of like clever hack that we've seen here is add another tool that explicitly just returns to the user. And then, you know, they like to use tools, but they'll usually use that tool. So I thought that was a pretty clever and interesting hack. A third challenge is the language models tell you what tool to use and how to use it. But that's in the form of a string. And so you need to go from that string into some code or something that can actually be run. And so that involves parsing kind of like the output of the language model into this tool invocation. And so some tips and tricks and hacks here are one like the more structured you ask for the response, the easier it is to parse generally. So language models are pretty good at writing JSON. So we've kind of transitioned a few of our agents to use that schema. Still doesn't always work, especially some of the chat models like to add in a lot of kind of like language. So we've introduced kind of like this concept of output parsers, which generically encapsulate all the logic that's needed to parse this response. And we've tried to make that as modular as possible. So if you're seeing areas, you can hopefully kind of like substitute that out very related to that. We also have a concept of like output parsers that can retry and fix mistakes. So and I think there's there's some subtle, there's some subtle differences here that I think are really cool. Basically, like, you know, if you have misformatted schema, you can fix that explicitly by just passing it the output and the error and saying fix this response. But if you have, if you have an output that just forgets one of the fields, like it returns the action, but not the action input or something like that, you need to provide more information here. So I think there's actually a lot of subtlety in fixing some of these errors. But the basic idea is that you can try to parse it. If you if it fails, you can then try to fix it. All this we currently encapsulate in this idea of like output parsers. So the fourth challenge is getting them to remember previous steps that were taken. The most basic thing, the thing that the React paper does is just keep a list of those steps in memory. Again, that starts to run into some some context window issues, especially when you're dealing with long running tasks. And so the thing that we've seen done here is again, fetch previous steps with some retrieval method and put those into context. Usually we've actually seen a combination of the two. So we've seen using the end most recent actions and observations combined with the K most relevant actions and observations. Incorporating long observations is another really interesting one. This actually comes up, or this came up a lot when we were dealing with working with APIs, because APIs often return really big JSON blobs that are really big and hard to put in context. So the most common thing that we've done here is just parse that in some way. You can do really simple stuff like convert that blob to a string and put the first like 1000 characters or something as the response. You can also do some more if you know that you're working with a specific API, you can probably write some custom logic to kind of like take only the relevant keys and put that if you want to make something general, you could also maybe do something dynamically to like figure out what key like basically explore the JSON object and figure out what keys to put in. That's a bit more exploratory, I would say. But the basic idea is, yeah, there is this issue of, and so Zapier, I always have to think about how to pronounce it, but it's Zapier makes you happier. So Zapier when they did this with their natural language API, not only did they have something before the API that was like natural language to some API call, they also spent a lot of time working on the output. And so the output is actually very specifically, I think it's like under like 200 or 300 tokens or something like that. And they did that on purpose. They spent a lot of time thinking about that. And so I think for tool usage, that is really important as well. Another more kind of like exploratory way of doing this is also you could perhaps just store the long output and then do retrieval on it when you're trying to think of like what next steps to take. Agents can often go off track, especially in long running things. And so there's kind of two methods that I've seen to kind of like keep them on track. One, you can just reiterate the objective right before it makes its action. And why this works, I think we've seen that with, at least with a lot of the current models, with instructions that are earlier in the prompt, it might forget it by the time it gets to the end if it's a really long prompt. So putting it at the end seems to help. And then another really interesting one that I'll talk about when I talk about some of the more recent papers and stuff that have come out is this idea of separating explicitly a planning and execution step and basically have one step that explicitly kind of thinks about, these are kind of like all the objectives that I want to do at a high level. And then a second step that says, okay, given this objective, given this one sub objective, now how do I do this one sub objective and basically break it down even more in a hierarchical whole manner. And there's a good example of that with baby AGI, which I'll talk about in a bit. And then another big issue is evaluation of these things. I think evaluation of language models in general, very difficult evaluation of applications built on top of language models. Also very difficult and agents are no exception. I think there's the obvious kind of like evaluate whether it arrived at the correct result in terms of in terms of getting metrics on evaluation. And so yeah, you know, if you're asking the agent to produce some answer, that's like a natural language response. There's techniques you can do there. A lot of them in the flavor of asking a language model to score the expected answer and the actual answer and come up with some grade and stuff like that, that applies to the output of agents as well. But then there's also some agent specific ones that I think are really interesting, mostly around evaluating these idea of like the agent trajectory or the intermediate steps. And so where we'll actually have something coming out for this, someone opened a PR that I need to get in. But basically, there's a lot of like little different things you can look at, like did it take the correct action? Is the input to the action correct? Is it the correct number of steps? And by this, you know, like sometimes you, and this is very related to the next one, which is like the most efficient sequence of steps. And so there's a bunch of different things that you can do to evaluate not only the final answer, but like is the agent getting there like efficiently, correctly. And those are sometimes just as useful, if not more useful than evaluating the end result. I'm trying to see what time it is, because I also want to leave lots of time for questions. But I think I'm good. So memory, I think is really interesting as well. So we've obviously tried it about like memory of remembering the AI to tool interactions. There's also like a more basic idea of remembering the user to AI interactions. But I think the third type, which is showing up in a lot of the recent papers on agents is this idea of like personalization of giving an agent kind of like its own kind of like objective and own persona and stuff like that. The most obvious way to do that is just like you encode it in the prompt. You say like, Hey, like, you know, this is your job as an agent, you're supposed to do this, yada, yada, yada. But I think there's some really cool work being done on how to kind of like evolve that over time and give agents a sense of like this long term memory. And one of the papers in particular around generative agents, I think does a really interesting job of diving into this. And I think when a lot of people, the reason this is here in the agent section is I think when people think of agents, there's the obvious like kind of like tool usage deciding what to do. But I think agents is also starting to take on this concept of some kind of like more encapsulated kind of like program that that adapts over time and memory is a big part of that. And so I think memories is there's a lot to explore here. So that's why this is a bit of an outlier slide. Okay, I wanted to chat very quickly about four projects that that came out in the past two, three weeks, specifically how they relate, build upon, improve upon this side, the react style agent that has been around for a while. First up is auto GPT, which I'm assuming most people have heard of. There we go. All right. So auto GPT, the one of the main differences between this and the react style agents is just the objective of what it's trying to solve auto GPT. A lot of the initial goals are like, you know, improve or increase my Twitter following or something like that very kind of like open ended broad long running goals, react on the other hand was designed and benchmarked on more kind of like short lived kind of like really immediately quantifiable or more immediately quantifiable goals. And so as a result, one of the things that auto GPT introduced is this idea of long term memory between the agent and tools interactions and using a retriever vector store for that, which becomes necessary because now you have this doing like 20 or 30 kind of like steps and it's this really long running project. And so it's something that react just didn't need, but due to the change in objectives, auto GPT kind of had to introduce baby AGI is another popular one. It also has this idea of long term memory for the agent tool interactions. And this is the project that introduced separate kind of like planning and execution steps, which I think is a really interesting idea to improve upon some of the long running objectives. And so specifically, it comes up with tasks, it then takes the first tasks, it then thinks about how to do that, which usually involves actually baby AGI initially didn't have any tools. So it kind of just like made stuff up. I think that I think they're giving it tools now so it can actually actually execute those things. But the idea of separating the planning execution steps is I think that's a really interesting idea that might help with some of the reliability and focus issues of longer term agents. Camel is another paper that came out. The main novel thing here was they put two agents in a simulation environment, which in this case, because it was just two was just a chat room and had them interact with each other. And so the agents themselves, I think, were basically just kind of like prompted language models. So I don't even think they were hooked up with tools. But going back to this idea of kind of like memory and personalization, when people kind of like talk about agents, that is part of what they're talking about. And so I think like the camel paper in my mind, the main thing is this idea of simulation environment. There's maybe like two reasons you might want to do this and have a simulation environment. One is kind of like practically to maybe like evaluate an agent if you're kind of like testing out an agent and you want to see how it's interacting. And for whatever reason, you don't want to test it out yourself. So you put two of them and you kind of like make sure they don't go off the rails or something like that. Another one is just kind of entertainment purposes. So there are a lot of examples of this by people. I think there was one with like a VC and a founder and that had them chatting with each other and kind of like solving stuff there. So this is a little bit entertainment, a little bit practical. Generative agents was another paper that came out. I think this was maybe like a week and a half ago, so very recent. It also had a simulation environment aspect. It was more complex. So I think they had like 25 different agents and kind of like a Sims-like world interacting with each other. So a much more complex environment setup. And then they also did some really cool stuff around memory and reflection. So memory refers to basically remembering previous things that happened in the world. So basically in the simulation environment, they had kind of like things that happened. Then they had the agents decide what to do, take actions, observe kind of like the results of those actions, observe more things that came in. And so all of this is encapsulated in the idea of memory. And then you fetch things from this memory to inform kind of like their actions in next time sequences. So there was three kind of like main components to this memory retrieval thing. They had a time-weighted component which basically fetched more recent memories. They had an important weighted piece which fetched more like important information. So trivial things like I forget what I had for breakfast today, but I don't know what's something that's really, but I remember meeting Charles way back when, right? So there's different levels of importance there that get subscribed to events. And so you want to fetch events that are kind of like more bigger in importance. And then they had the typical kind of like relevancy weighted things. So depending on what situation you're in, you want to remember events that are relevant for that. Then they also introduced a really interesting reflection step, which basically after like, I think they, I think it was like 20 different steps or something happened, they would reflect on those things and kind of like update different states of the world. And so I think this is, I've been thinking about this a bit because I think this idea of like reflecting on recent things and then updating state is maybe like a generalization that can be kind of like applied to a bunch of different things. So some of the other memory types that we have in Lang chain are we have like an entity memory type, which basically based on conversation kind of like extracts relevant entities and then constructs some type of graph and updates that there's a more general kind of like knowledge graph version of that as well. And then we also have kind of like a summary conversation memory, which based on the conversation updates a running summary. So you can get around some of the context window lengths. And so I think if you look at it sort of through a certain angle, all of those kind of like relate to this idea of taking recent observations and updating some state, whether that state is like a graph or just a piece of text or anything like that. So there's also been some other papers recently that have incorporated this idea of like reflection. I haven't had time to read those as carefully, but I think that's yeah, I don't know, my personal take is I think that's really interesting and something to keep an eye out for the future. And that's it. I have no idea what time it is because I can't see the time, but I'm happy to take questions until Charles kicks me off.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.2, "text": " So, I'll be talking about agents, and yeah, there are many things in the lane chain, but", "tokens": [50364, 407, 11, 286, 603, 312, 1417, 466, 12554, 11, 293, 1338, 11, 456, 366, 867, 721, 294, 264, 12705, 5021, 11, 457, 50724], "temperature": 0.0, "avg_logprob": -0.22148317950112478, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.06078365072607994}, {"id": 1, "seek": 0, "start": 7.2, "end": 10.24, "text": " I think agents are probably the most interesting one to talk about, so that's what I'll be", "tokens": [50724, 286, 519, 12554, 366, 1391, 264, 881, 1880, 472, 281, 751, 466, 11, 370, 300, 311, 437, 286, 603, 312, 50876], "temperature": 0.0, "avg_logprob": -0.22148317950112478, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.06078365072607994}, {"id": 2, "seek": 0, "start": 10.24, "end": 11.24, "text": " doing.", "tokens": [50876, 884, 13, 50926], "temperature": 0.0, "avg_logprob": -0.22148317950112478, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.06078365072607994}, {"id": 3, "seek": 0, "start": 11.24, "end": 15.84, "text": " I'll cover kind of like, what are agents, why use agents, the typical implementation", "tokens": [50926, 286, 603, 2060, 733, 295, 411, 11, 437, 366, 12554, 11, 983, 764, 12554, 11, 264, 7476, 11420, 51156], "temperature": 0.0, "avg_logprob": -0.22148317950112478, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.06078365072607994}, {"id": 4, "seek": 0, "start": 15.84, "end": 21.12, "text": " of agents, talk about React, which is one of the first prompting strategies that really", "tokens": [51156, 295, 12554, 11, 751, 466, 30644, 11, 597, 307, 472, 295, 264, 700, 12391, 278, 9029, 300, 534, 51420], "temperature": 0.0, "avg_logprob": -0.22148317950112478, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.06078365072607994}, {"id": 5, "seek": 0, "start": 21.12, "end": 27.080000000000002, "text": " accelerated and made reliable the use of agents, then I'll talk a bunch about challenges with", "tokens": [51420, 29763, 293, 1027, 12924, 264, 764, 295, 12554, 11, 550, 286, 603, 751, 257, 3840, 466, 4759, 365, 51718], "temperature": 0.0, "avg_logprob": -0.22148317950112478, "compression_ratio": 1.790513833992095, "no_speech_prob": 0.06078365072607994}, {"id": 6, "seek": 2708, "start": 27.08, "end": 31.88, "text": " agents and challenges of getting them to work reliably, getting them to work in production.", "tokens": [50364, 12554, 293, 4759, 295, 1242, 552, 281, 589, 49927, 11, 1242, 552, 281, 589, 294, 4265, 13, 50604], "temperature": 0.0, "avg_logprob": -0.15839139550133088, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.002048772992566228}, {"id": 7, "seek": 2708, "start": 31.88, "end": 36.68, "text": " I'll then touch a little bit on memory and segue that into more recent kind of like", "tokens": [50604, 286, 603, 550, 2557, 257, 707, 857, 322, 4675, 293, 33850, 300, 666, 544, 5162, 733, 295, 411, 50844], "temperature": 0.0, "avg_logprob": -0.15839139550133088, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.002048772992566228}, {"id": 8, "seek": 2708, "start": 36.68, "end": 40.96, "text": " papers and projects that do agentic things.", "tokens": [50844, 10577, 293, 4455, 300, 360, 9461, 299, 721, 13, 51058], "temperature": 0.0, "avg_logprob": -0.15839139550133088, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.002048772992566228}, {"id": 9, "seek": 2708, "start": 40.96, "end": 46.68, "text": " I'll probably skim over the initial slides because I think most people here are probably", "tokens": [51058, 286, 603, 1391, 1110, 332, 670, 264, 5883, 9788, 570, 286, 519, 881, 561, 510, 366, 1391, 51344], "temperature": 0.0, "avg_logprob": -0.15839139550133088, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.002048772992566228}, {"id": 10, "seek": 2708, "start": 46.68, "end": 51.4, "text": " familiar with the idea of agents, but the core idea of agents is using the language model", "tokens": [51344, 4963, 365, 264, 1558, 295, 12554, 11, 457, 264, 4965, 1558, 295, 12554, 307, 1228, 264, 2856, 2316, 51580], "temperature": 0.0, "avg_logprob": -0.15839139550133088, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.002048772992566228}, {"id": 11, "seek": 2708, "start": 51.4, "end": 56.56, "text": " as a reasoning engine, so using it to determine kind of like what to do and how to interact", "tokens": [51580, 382, 257, 21577, 2848, 11, 370, 1228, 309, 281, 6997, 733, 295, 411, 437, 281, 360, 293, 577, 281, 4648, 51838], "temperature": 0.0, "avg_logprob": -0.15839139550133088, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.002048772992566228}, {"id": 12, "seek": 5656, "start": 56.56, "end": 61.64, "text": " with the outside world, and this means that there is a non-deterministic kind of like sequence", "tokens": [50364, 365, 264, 2380, 1002, 11, 293, 341, 1355, 300, 456, 307, 257, 2107, 12, 49136, 259, 3142, 733, 295, 411, 8310, 50618], "temperature": 0.0, "avg_logprob": -0.1460327957615708, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.00035631636274047196}, {"id": 13, "seek": 5656, "start": 61.64, "end": 66.92, "text": " of actions that'll be taken depending on the user input, so there's no kind of like hard", "tokens": [50618, 295, 5909, 300, 603, 312, 2726, 5413, 322, 264, 4195, 4846, 11, 370, 456, 311, 572, 733, 295, 411, 1152, 50882], "temperature": 0.0, "avg_logprob": -0.1460327957615708, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.00035631636274047196}, {"id": 14, "seek": 5656, "start": 66.92, "end": 73.04, "text": " coded do A, then do B, then do C, rather the agent determines what actions to do depending", "tokens": [50882, 34874, 360, 316, 11, 550, 360, 363, 11, 550, 360, 383, 11, 2831, 264, 9461, 24799, 437, 5909, 281, 360, 5413, 51188], "temperature": 0.0, "avg_logprob": -0.1460327957615708, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.00035631636274047196}, {"id": 15, "seek": 5656, "start": 73.04, "end": 78.28, "text": " on the user input and depending on results of previous actions.", "tokens": [51188, 322, 264, 4195, 4846, 293, 5413, 322, 3542, 295, 3894, 5909, 13, 51450], "temperature": 0.0, "avg_logprob": -0.1460327957615708, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.00035631636274047196}, {"id": 16, "seek": 5656, "start": 78.28, "end": 81.48, "text": " So why would you want to do this in the first place?", "tokens": [51450, 407, 983, 576, 291, 528, 281, 360, 341, 294, 264, 700, 1081, 30, 51610], "temperature": 0.0, "avg_logprob": -0.1460327957615708, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.00035631636274047196}, {"id": 17, "seek": 8148, "start": 81.48, "end": 86.64, "text": " So one, there's this very tied to agents is the idea of tool usage and connecting it", "tokens": [50364, 407, 472, 11, 456, 311, 341, 588, 9601, 281, 12554, 307, 264, 1558, 295, 2290, 14924, 293, 11015, 309, 50622], "temperature": 0.0, "avg_logprob": -0.14492174474204458, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0007785098277963698}, {"id": 18, "seek": 8148, "start": 86.64, "end": 91.72, "text": " to the outside world, and so connecting it to other sources of data or computation like", "tokens": [50622, 281, 264, 2380, 1002, 11, 293, 370, 11015, 309, 281, 661, 7139, 295, 1412, 420, 24903, 411, 50876], "temperature": 0.0, "avg_logprob": -0.14492174474204458, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0007785098277963698}, {"id": 19, "seek": 8148, "start": 91.72, "end": 95.60000000000001, "text": " search, APIs, databases, these are all very useful to overcome.", "tokens": [50876, 3164, 11, 21445, 11, 22380, 11, 613, 366, 439, 588, 4420, 281, 10473, 13, 51070], "temperature": 0.0, "avg_logprob": -0.14492174474204458, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0007785098277963698}, {"id": 20, "seek": 8148, "start": 95.60000000000001, "end": 99.36, "text": " Some of the limitations of language models, such as they don't know about your data, they", "tokens": [51070, 2188, 295, 264, 15705, 295, 2856, 5245, 11, 1270, 382, 436, 500, 380, 458, 466, 428, 1412, 11, 436, 51258], "temperature": 0.0, "avg_logprob": -0.14492174474204458, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0007785098277963698}, {"id": 21, "seek": 8148, "start": 99.36, "end": 105.88000000000001, "text": " can't do math amazingly well, but the idea of tool usage isn't kind of like unique to", "tokens": [51258, 393, 380, 360, 5221, 31762, 731, 11, 457, 264, 1558, 295, 2290, 14924, 1943, 380, 733, 295, 411, 3845, 281, 51584], "temperature": 0.0, "avg_logprob": -0.14492174474204458, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0007785098277963698}, {"id": 22, "seek": 8148, "start": 105.88000000000001, "end": 110.92, "text": " agents, you can still use tools, you can still connect LLMs to search engines without using", "tokens": [51584, 12554, 11, 291, 393, 920, 764, 3873, 11, 291, 393, 920, 1745, 441, 43, 26386, 281, 3164, 12982, 1553, 1228, 51836], "temperature": 0.0, "avg_logprob": -0.14492174474204458, "compression_ratio": 1.8129496402877698, "no_speech_prob": 0.0007785098277963698}, {"id": 23, "seek": 11092, "start": 111.0, "end": 113.08, "text": " an agent, so why use an agent?", "tokens": [50368, 364, 9461, 11, 370, 983, 764, 364, 9461, 30, 50472], "temperature": 0.0, "avg_logprob": -0.15015362678690158, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.014482107944786549}, {"id": 24, "seek": 11092, "start": 113.08, "end": 117.84, "text": " And I think some of the benefits of agents are that they're more flexible, they're more", "tokens": [50472, 400, 286, 519, 512, 295, 264, 5311, 295, 12554, 366, 300, 436, 434, 544, 11358, 11, 436, 434, 544, 50710], "temperature": 0.0, "avg_logprob": -0.15015362678690158, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.014482107944786549}, {"id": 25, "seek": 11092, "start": 117.84, "end": 121.16, "text": " powerful, they allow you to kind of like recover from errors better, they allow you to handle", "tokens": [50710, 4005, 11, 436, 2089, 291, 281, 733, 295, 411, 8114, 490, 13603, 1101, 11, 436, 2089, 291, 281, 4813, 50876], "temperature": 0.0, "avg_logprob": -0.15015362678690158, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.014482107944786549}, {"id": 26, "seek": 11092, "start": 121.16, "end": 125.2, "text": " kind of like multi-hop tasks, again with this idea of being the reasoning engine, and so", "tokens": [50876, 733, 295, 411, 4825, 12, 9050, 9608, 11, 797, 365, 341, 1558, 295, 885, 264, 21577, 2848, 11, 293, 370, 51078], "temperature": 0.0, "avg_logprob": -0.15015362678690158, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.014482107944786549}, {"id": 27, "seek": 11092, "start": 125.2, "end": 131.24, "text": " an example that I like to use here is thinking about like interacting with a SQL database.", "tokens": [51078, 364, 1365, 300, 286, 411, 281, 764, 510, 307, 1953, 466, 411, 18017, 365, 257, 19200, 8149, 13, 51380], "temperature": 0.0, "avg_logprob": -0.15015362678690158, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.014482107944786549}, {"id": 28, "seek": 11092, "start": 131.24, "end": 135.72, "text": " You can do that in a sequence of predetermined steps, you can have kind of like a natural", "tokens": [51380, 509, 393, 360, 300, 294, 257, 8310, 295, 3852, 35344, 2001, 4439, 11, 291, 393, 362, 733, 295, 411, 257, 3303, 51604], "temperature": 0.0, "avg_logprob": -0.15015362678690158, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.014482107944786549}, {"id": 29, "seek": 11092, "start": 135.72, "end": 139.96, "text": " language query, which you then use a language model to convert to a SQL query, which you", "tokens": [51604, 2856, 14581, 11, 597, 291, 550, 764, 257, 2856, 2316, 281, 7620, 281, 257, 19200, 14581, 11, 597, 291, 51816], "temperature": 0.0, "avg_logprob": -0.15015362678690158, "compression_ratio": 1.8360128617363345, "no_speech_prob": 0.014482107944786549}, {"id": 30, "seek": 13996, "start": 139.96, "end": 145.16, "text": " then pass, you then execute that, get back a result, pass that back to the language model,", "tokens": [50364, 550, 1320, 11, 291, 550, 14483, 300, 11, 483, 646, 257, 1874, 11, 1320, 300, 646, 281, 264, 2856, 2316, 11, 50624], "temperature": 0.0, "avg_logprob": -0.11538176903357873, "compression_ratio": 1.7833333333333334, "no_speech_prob": 0.0002303851069882512}, {"id": 31, "seek": 13996, "start": 145.16, "end": 149.84, "text": " ask it to synthesize it with respect to the original question, and get kind of this natural", "tokens": [50624, 1029, 309, 281, 26617, 1125, 309, 365, 3104, 281, 264, 3380, 1168, 11, 293, 483, 733, 295, 341, 3303, 50858], "temperature": 0.0, "avg_logprob": -0.11538176903357873, "compression_ratio": 1.7833333333333334, "no_speech_prob": 0.0002303851069882512}, {"id": 32, "seek": 13996, "start": 149.84, "end": 154.08, "text": " language wrapper around a SQL database, very useful by itself, but there are a lot of edge", "tokens": [50858, 2856, 46906, 926, 257, 19200, 8149, 11, 588, 4420, 538, 2564, 11, 457, 456, 366, 257, 688, 295, 4691, 51070], "temperature": 0.0, "avg_logprob": -0.11538176903357873, "compression_ratio": 1.7833333333333334, "no_speech_prob": 0.0002303851069882512}, {"id": 33, "seek": 13996, "start": 154.08, "end": 157.84, "text": " cases and things that can go wrong, so there could be an error in the SQL query, maybe", "tokens": [51070, 3331, 293, 721, 300, 393, 352, 2085, 11, 370, 456, 727, 312, 364, 6713, 294, 264, 19200, 14581, 11, 1310, 51258], "temperature": 0.0, "avg_logprob": -0.11538176903357873, "compression_ratio": 1.7833333333333334, "no_speech_prob": 0.0002303851069882512}, {"id": 34, "seek": 13996, "start": 157.84, "end": 164.36, "text": " it hallucinates a table name or a field name, maybe it just writes incorrect SQL, and then", "tokens": [51258, 309, 35212, 259, 1024, 257, 3199, 1315, 420, 257, 2519, 1315, 11, 1310, 309, 445, 13657, 18424, 19200, 11, 293, 550, 51584], "temperature": 0.0, "avg_logprob": -0.11538176903357873, "compression_ratio": 1.7833333333333334, "no_speech_prob": 0.0002303851069882512}, {"id": 35, "seek": 13996, "start": 164.36, "end": 168.24, "text": " there are also queries that need multiple kind of like queries to be made under the", "tokens": [51584, 456, 366, 611, 24109, 300, 643, 3866, 733, 295, 411, 24109, 281, 312, 1027, 833, 264, 51778], "temperature": 0.0, "avg_logprob": -0.11538176903357873, "compression_ratio": 1.7833333333333334, "no_speech_prob": 0.0002303851069882512}, {"id": 36, "seek": 16824, "start": 168.24, "end": 176.28, "text": " hood in order to answer, and so although kind of like a simple chain can handle maybe, I", "tokens": [50364, 13376, 294, 1668, 281, 1867, 11, 293, 370, 4878, 733, 295, 411, 257, 2199, 5021, 393, 4813, 1310, 11, 286, 50766], "temperature": 0.0, "avg_logprob": -0.15371871221633185, "compression_ratio": 1.6377358490566039, "no_speech_prob": 0.04462121054530144}, {"id": 37, "seek": 16824, "start": 176.28, "end": 180.28, "text": " don't know, 50, 80% of the use cases, you very quickly run into these like edge cases", "tokens": [50766, 500, 380, 458, 11, 2625, 11, 4688, 4, 295, 264, 764, 3331, 11, 291, 588, 2661, 1190, 666, 613, 411, 4691, 3331, 50966], "temperature": 0.0, "avg_logprob": -0.15371871221633185, "compression_ratio": 1.6377358490566039, "no_speech_prob": 0.04462121054530144}, {"id": 38, "seek": 16824, "start": 180.28, "end": 185.92000000000002, "text": " where a more flexible framework like agents helps kind of like circumvent.", "tokens": [50966, 689, 257, 544, 11358, 8388, 411, 12554, 3665, 733, 295, 411, 7125, 2475, 13, 51248], "temperature": 0.0, "avg_logprob": -0.15371871221633185, "compression_ratio": 1.6377358490566039, "no_speech_prob": 0.04462121054530144}, {"id": 39, "seek": 16824, "start": 185.92000000000002, "end": 190.44, "text": " So the typical implementation of agents generally, and it's so early in this field that it kind", "tokens": [51248, 407, 264, 7476, 11420, 295, 12554, 5101, 11, 293, 309, 311, 370, 2440, 294, 341, 2519, 300, 309, 733, 51474], "temperature": 0.0, "avg_logprob": -0.15371871221633185, "compression_ratio": 1.6377358490566039, "no_speech_prob": 0.04462121054530144}, {"id": 40, "seek": 16824, "start": 190.44, "end": 193.60000000000002, "text": " of feels a bit weird to be talking about a typical implementation because I'm sure we'll", "tokens": [51474, 295, 3417, 257, 857, 3657, 281, 312, 1417, 466, 257, 7476, 11420, 570, 286, 478, 988, 321, 603, 51632], "temperature": 0.0, "avg_logprob": -0.15371871221633185, "compression_ratio": 1.6377358490566039, "no_speech_prob": 0.04462121054530144}, {"id": 41, "seek": 19360, "start": 193.68, "end": 200.32, "text": " see a bunch of different variants, but generally you get a user query, you use the LLM, that's", "tokens": [50368, 536, 257, 3840, 295, 819, 21669, 11, 457, 5101, 291, 483, 257, 4195, 14581, 11, 291, 764, 264, 441, 43, 44, 11, 300, 311, 50700], "temperature": 0.0, "avg_logprob": -0.13197184460503714, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.11574481427669525}, {"id": 42, "seek": 19360, "start": 200.32, "end": 207.24, "text": " the agent to choose a tool to use, and also the input to that tool, you then do that,", "tokens": [50700, 264, 9461, 281, 2826, 257, 2290, 281, 764, 11, 293, 611, 264, 4846, 281, 300, 2290, 11, 291, 550, 360, 300, 11, 51046], "temperature": 0.0, "avg_logprob": -0.13197184460503714, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.11574481427669525}, {"id": 43, "seek": 19360, "start": 207.24, "end": 212.0, "text": " you take that action, you get back an observation, and then you feed that back into the language", "tokens": [51046, 291, 747, 300, 3069, 11, 291, 483, 646, 364, 14816, 11, 293, 550, 291, 3154, 300, 646, 666, 264, 2856, 51284], "temperature": 0.0, "avg_logprob": -0.13197184460503714, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.11574481427669525}, {"id": 44, "seek": 19360, "start": 212.0, "end": 216.64, "text": " model and you kind of continue doing this until a stopping condition is met, and so there", "tokens": [51284, 2316, 293, 291, 733, 295, 2354, 884, 341, 1826, 257, 12767, 4188, 307, 1131, 11, 293, 370, 456, 51516], "temperature": 0.0, "avg_logprob": -0.13197184460503714, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.11574481427669525}, {"id": 45, "seek": 19360, "start": 216.64, "end": 219.88, "text": " can be different types of stopping conditions, probably the most common is the language model", "tokens": [51516, 393, 312, 819, 3467, 295, 12767, 4487, 11, 1391, 264, 881, 2689, 307, 264, 2856, 2316, 51678], "temperature": 0.0, "avg_logprob": -0.13197184460503714, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.11574481427669525}, {"id": 46, "seek": 21988, "start": 219.92, "end": 226.6, "text": " itself realizes, hey, I'm done with this task, with this question that someone asked of me, I", "tokens": [50366, 2564, 29316, 11, 4177, 11, 286, 478, 1096, 365, 341, 5633, 11, 365, 341, 1168, 300, 1580, 2351, 295, 385, 11, 286, 50700], "temperature": 0.0, "avg_logprob": -0.1488775889078776, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.038375042378902435}, {"id": 47, "seek": 21988, "start": 226.6, "end": 231.16, "text": " should now return to the user, but there can be other more hard-coded rules, and so when we talk", "tokens": [50700, 820, 586, 2736, 281, 264, 4195, 11, 457, 456, 393, 312, 661, 544, 1152, 12, 66, 12340, 4474, 11, 293, 370, 562, 321, 751, 50928], "temperature": 0.0, "avg_logprob": -0.1488775889078776, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.038375042378902435}, {"id": 48, "seek": 21988, "start": 231.16, "end": 235.84, "text": " about like reliability of agents, some of these can really help with that, so you know, if an", "tokens": [50928, 466, 411, 24550, 295, 12554, 11, 512, 295, 613, 393, 534, 854, 365, 300, 11, 370, 291, 458, 11, 498, 364, 51162], "temperature": 0.0, "avg_logprob": -0.1488775889078776, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.038375042378902435}, {"id": 49, "seek": 21988, "start": 235.84, "end": 240.8, "text": " agent has done five different steps in a row and it hasn't reached a final answer, it might be nice", "tokens": [51162, 9461, 575, 1096, 1732, 819, 4439, 294, 257, 5386, 293, 309, 6132, 380, 6488, 257, 2572, 1867, 11, 309, 1062, 312, 1481, 51410], "temperature": 0.0, "avg_logprob": -0.1488775889078776, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.038375042378902435}, {"id": 50, "seek": 21988, "start": 240.8, "end": 245.28, "text": " to have it just return something then. There are also certain tools that you can just like return", "tokens": [51410, 281, 362, 309, 445, 2736, 746, 550, 13, 821, 366, 611, 1629, 3873, 300, 291, 393, 445, 411, 2736, 51634], "temperature": 0.0, "avg_logprob": -0.1488775889078776, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.038375042378902435}, {"id": 51, "seek": 24528, "start": 245.36, "end": 250.08, "text": " automatically, so basically the general idea is choose a tool to use, observe the output of that", "tokens": [50368, 6772, 11, 370, 1936, 264, 2674, 1558, 307, 2826, 257, 2290, 281, 764, 11, 11441, 264, 5598, 295, 300, 50604], "temperature": 0.0, "avg_logprob": -0.14445201555887857, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.013624388724565506}, {"id": 52, "seek": 24528, "start": 250.08, "end": 256.92, "text": " tool, and kind of continue going. So how do you actually like, so that was pseudocode, now let's", "tokens": [50604, 2290, 11, 293, 733, 295, 2354, 516, 13, 407, 577, 360, 291, 767, 411, 11, 370, 300, 390, 25505, 532, 905, 1429, 11, 586, 718, 311, 50946], "temperature": 0.0, "avg_logprob": -0.14445201555887857, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.013624388724565506}, {"id": 53, "seek": 24528, "start": 256.92, "end": 265.2, "text": " talk about the actual kind of like ways to get this to do what we want, and the first and still", "tokens": [50946, 751, 466, 264, 3539, 733, 295, 411, 2098, 281, 483, 341, 281, 360, 437, 321, 528, 11, 293, 264, 700, 293, 920, 51360], "temperature": 0.0, "avg_logprob": -0.14445201555887857, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.013624388724565506}, {"id": 54, "seek": 24528, "start": 265.2, "end": 272.36, "text": " kind of like the main way of doing this, the main prompting strategy slash algorithm slash method", "tokens": [51360, 733, 295, 411, 264, 2135, 636, 295, 884, 341, 11, 264, 2135, 12391, 278, 5206, 17330, 9284, 17330, 3170, 51718], "temperature": 0.0, "avg_logprob": -0.14445201555887857, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.013624388724565506}, {"id": 55, "seek": 27236, "start": 272.36, "end": 278.68, "text": " for doing this is react, which stands for reasoning and then acting, so RE from reasoning, ACT", "tokens": [50364, 337, 884, 341, 307, 4515, 11, 597, 7382, 337, 21577, 293, 550, 6577, 11, 370, 10869, 490, 21577, 11, 40341, 50680], "temperature": 0.0, "avg_logprob": -0.20619702339172363, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0038803862407803535}, {"id": 56, "seek": 27236, "start": 278.68, "end": 284.2, "text": " from acting, and it's the papers, a great paper came out in I think October out of Princeton,", "tokens": [50680, 490, 6577, 11, 293, 309, 311, 264, 10577, 11, 257, 869, 3035, 1361, 484, 294, 286, 519, 7617, 484, 295, 36592, 11, 50956], "temperature": 0.0, "avg_logprob": -0.20619702339172363, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0038803862407803535}, {"id": 57, "seek": 27236, "start": 284.2, "end": 291.12, "text": " synergizing two different kind of like methods, and we can look at this example which is taken", "tokens": [50956, 33781, 70, 3319, 732, 819, 733, 295, 411, 7150, 11, 293, 321, 393, 574, 412, 341, 1365, 597, 307, 2726, 51302], "temperature": 0.0, "avg_logprob": -0.20619702339172363, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0038803862407803535}, {"id": 58, "seek": 27236, "start": 291.12, "end": 298.64, "text": " from their paper and see why it's so effective. So this example comes from the hotspot QA data set,", "tokens": [51302, 490, 641, 3035, 293, 536, 983, 309, 311, 370, 4942, 13, 407, 341, 1365, 1487, 490, 264, 36121, 17698, 1249, 32, 1412, 992, 11, 51678], "temperature": 0.0, "avg_logprob": -0.20619702339172363, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0038803862407803535}, {"id": 59, "seek": 29864, "start": 298.96, "end": 304.24, "text": " which is basically a data set where it's asking questions over Wikipedia pages where there are", "tokens": [50380, 597, 307, 1936, 257, 1412, 992, 689, 309, 311, 3365, 1651, 670, 28999, 7183, 689, 456, 366, 50644], "temperature": 0.0, "avg_logprob": -0.15964885822777608, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.003941452596336603}, {"id": 60, "seek": 29864, "start": 304.24, "end": 309.44, "text": " multi-hop usually kind of like two or three intermediate questions that need to be reasoned", "tokens": [50644, 4825, 12, 9050, 2673, 733, 295, 411, 732, 420, 1045, 19376, 1651, 300, 643, 281, 312, 1778, 292, 50904], "temperature": 0.0, "avg_logprob": -0.15964885822777608, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.003941452596336603}, {"id": 61, "seek": 29864, "start": 309.44, "end": 314.59999999999997, "text": " about before answering. So we can see, so here there's this question aside from the Apple remote,", "tokens": [50904, 466, 949, 13430, 13, 407, 321, 393, 536, 11, 370, 510, 456, 311, 341, 1168, 7359, 490, 264, 6373, 8607, 11, 51162], "temperature": 0.0, "avg_logprob": -0.15964885822777608, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.003941452596336603}, {"id": 62, "seek": 29864, "start": 314.59999999999997, "end": 321.12, "text": " what other device can control the program Apple remote was originally designed to interact with,", "tokens": [51162, 437, 661, 4302, 393, 1969, 264, 1461, 6373, 8607, 390, 7993, 4761, 281, 4648, 365, 11, 51488], "temperature": 0.0, "avg_logprob": -0.15964885822777608, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.003941452596336603}, {"id": 63, "seek": 29864, "start": 321.12, "end": 326.64, "text": " and so the most basic prompting strategy is kind of just pass that into the language model and", "tokens": [51488, 293, 370, 264, 881, 3875, 12391, 278, 5206, 307, 733, 295, 445, 1320, 300, 666, 264, 2856, 2316, 293, 51764], "temperature": 0.0, "avg_logprob": -0.15964885822777608, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.003941452596336603}, {"id": 64, "seek": 32664, "start": 326.64, "end": 332.03999999999996, "text": " get back an answer, and so we can see that in 1A standard, and it just returns a single answer,", "tokens": [50364, 483, 646, 364, 1867, 11, 293, 370, 321, 393, 536, 300, 294, 502, 32, 3832, 11, 293, 309, 445, 11247, 257, 2167, 1867, 11, 50634], "temperature": 0.0, "avg_logprob": -0.12971818078424513, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.0008555487729609013}, {"id": 65, "seek": 32664, "start": 332.03999999999996, "end": 336.47999999999996, "text": " and we can see that it's wrong. Another method that had emerged maybe like a month or so prior", "tokens": [50634, 293, 321, 393, 536, 300, 309, 311, 2085, 13, 3996, 3170, 300, 632, 20178, 1310, 411, 257, 1618, 420, 370, 4059, 50856], "temperature": 0.0, "avg_logprob": -0.12971818078424513, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.0008555487729609013}, {"id": 66, "seek": 32664, "start": 336.47999999999996, "end": 343.47999999999996, "text": " was the idea of like chain of thought reasoning, and so this is usually associated with the let's", "tokens": [50856, 390, 264, 1558, 295, 411, 5021, 295, 1194, 21577, 11, 293, 370, 341, 307, 2673, 6615, 365, 264, 718, 311, 51206], "temperature": 0.0, "avg_logprob": -0.12971818078424513, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.0008555487729609013}, {"id": 67, "seek": 32664, "start": 343.47999999999996, "end": 350.32, "text": " think step by step prefix to the response, and you can see in red that there's this like chain of", "tokens": [51206, 519, 1823, 538, 1823, 46969, 281, 264, 4134, 11, 293, 291, 393, 536, 294, 2182, 300, 456, 311, 341, 411, 5021, 295, 51548], "temperature": 0.0, "avg_logprob": -0.12971818078424513, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.0008555487729609013}, {"id": 68, "seek": 35032, "start": 350.32, "end": 356.96, "text": " thought thing that's happening as it thinks through step by step, and it returns an answer", "tokens": [50364, 1194, 551, 300, 311, 2737, 382, 309, 7309, 807, 1823, 538, 1823, 11, 293, 309, 11247, 364, 1867, 50696], "temperature": 0.0, "avg_logprob": -0.12114349571434227, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.013627520762383938}, {"id": 69, "seek": 35032, "start": 356.96, "end": 363.56, "text": " that's also incorrect in this case. This has been shown to kind of like get the agent or get the", "tokens": [50696, 300, 311, 611, 18424, 294, 341, 1389, 13, 639, 575, 668, 4898, 281, 733, 295, 411, 483, 264, 9461, 420, 483, 264, 51026], "temperature": 0.0, "avg_logprob": -0.12114349571434227, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.013627520762383938}, {"id": 70, "seek": 35032, "start": 363.56, "end": 369.76, "text": " language model to think a little bit better, so to speak. So it's yielding higher quality,", "tokens": [51026, 2856, 2316, 281, 519, 257, 707, 857, 1101, 11, 370, 281, 1710, 13, 407, 309, 311, 11257, 278, 2946, 3125, 11, 51336], "temperature": 0.0, "avg_logprob": -0.12114349571434227, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.013627520762383938}, {"id": 71, "seek": 35032, "start": 369.76, "end": 376.32, "text": " more reliable results. The issue is it still only knows kind of like what is actually present in", "tokens": [51336, 544, 12924, 3542, 13, 440, 2734, 307, 309, 920, 787, 3255, 733, 295, 411, 437, 307, 767, 1974, 294, 51664], "temperature": 0.0, "avg_logprob": -0.12114349571434227, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.013627520762383938}, {"id": 72, "seek": 35032, "start": 376.32, "end": 380.03999999999996, "text": " the data that the language model is trained on, and so there's another technique that came out", "tokens": [51664, 264, 1412, 300, 264, 2856, 2316, 307, 8895, 322, 11, 293, 370, 456, 311, 1071, 6532, 300, 1361, 484, 51850], "temperature": 0.0, "avg_logprob": -0.12114349571434227, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.013627520762383938}, {"id": 73, "seek": 38004, "start": 380.08000000000004, "end": 385.88, "text": " which is basically action only, where you give it access to different tools. In this case,", "tokens": [50366, 597, 307, 1936, 3069, 787, 11, 689, 291, 976, 309, 2105, 281, 819, 3873, 13, 682, 341, 1389, 11, 50656], "temperature": 0.0, "avg_logprob": -0.14709795039633047, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0006453786627389491}, {"id": 74, "seek": 38004, "start": 385.88, "end": 392.68, "text": " I think all the examples in this picture are of search, but I think in the paper it had search", "tokens": [50656, 286, 519, 439, 264, 5110, 294, 341, 3036, 366, 295, 3164, 11, 457, 286, 519, 294, 264, 3035, 309, 632, 3164, 50996], "temperature": 0.0, "avg_logprob": -0.14709795039633047, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0006453786627389491}, {"id": 75, "seek": 38004, "start": 392.68, "end": 399.0, "text": " and then look up, and so you can see here that the language model outputs kind of like search", "tokens": [50996, 293, 550, 574, 493, 11, 293, 370, 291, 393, 536, 510, 300, 264, 2856, 2316, 23930, 733, 295, 411, 3164, 51312], "temperature": 0.0, "avg_logprob": -0.14709795039633047, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0006453786627389491}, {"id": 76, "seek": 38004, "start": 399.0, "end": 403.6, "text": " Apple remote, it looks up Apple remote, it gets back an observation, it then does search front row,", "tokens": [51312, 6373, 8607, 11, 309, 1542, 493, 6373, 8607, 11, 309, 2170, 646, 364, 14816, 11, 309, 550, 775, 3164, 1868, 5386, 11, 51542], "temperature": 0.0, "avg_logprob": -0.14709795039633047, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0006453786627389491}, {"id": 77, "seek": 40360, "start": 404.56, "end": 408.92, "text": " can find that, so that's an instance of it kind of like recovering from an error,", "tokens": [50412, 393, 915, 300, 11, 370, 300, 311, 364, 5197, 295, 309, 733, 295, 411, 29180, 490, 364, 6713, 11, 50630], "temperature": 0.0, "avg_logprob": -0.17075799811970105, "compression_ratio": 1.65, "no_speech_prob": 0.0014087033923715353}, {"id": 78, "seek": 40360, "start": 408.92, "end": 418.56, "text": " and then it does search front row software, finds an answer, and then finishes. And you can see", "tokens": [50630, 293, 550, 309, 775, 3164, 1868, 5386, 4722, 11, 10704, 364, 1867, 11, 293, 550, 23615, 13, 400, 291, 393, 536, 51112], "temperature": 0.0, "avg_logprob": -0.17075799811970105, "compression_ratio": 1.65, "no_speech_prob": 0.0014087033923715353}, {"id": 79, "seek": 40360, "start": 418.56, "end": 426.32000000000005, "text": " here that the output that it gave, yes, kind of loses some of what it was actually supposed to", "tokens": [51112, 510, 300, 264, 5598, 300, 309, 2729, 11, 2086, 11, 733, 295, 18293, 512, 295, 437, 309, 390, 767, 3442, 281, 51500], "temperature": 0.0, "avg_logprob": -0.17075799811970105, "compression_ratio": 1.65, "no_speech_prob": 0.0014087033923715353}, {"id": 80, "seek": 40360, "start": 426.32000000000005, "end": 431.12, "text": " answer. So you've got this chain of thought reasoning which helps the language model think", "tokens": [51500, 1867, 13, 407, 291, 600, 658, 341, 5021, 295, 1194, 21577, 597, 3665, 264, 2856, 2316, 519, 51740], "temperature": 0.0, "avg_logprob": -0.17075799811970105, "compression_ratio": 1.65, "no_speech_prob": 0.0014087033923715353}, {"id": 81, "seek": 43112, "start": 431.12, "end": 438.0, "text": " about what to do, then you've got this action taking step that basically actually allows it to", "tokens": [50364, 466, 437, 281, 360, 11, 550, 291, 600, 658, 341, 3069, 1940, 1823, 300, 1936, 767, 4045, 309, 281, 50708], "temperature": 0.0, "avg_logprob": -0.11797566999468886, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.00106424477417022}, {"id": 82, "seek": 43112, "start": 438.0, "end": 442.64, "text": " plug into more kind of like sources of real data, what if you combine them, and that's the idea", "tokens": [50708, 5452, 666, 544, 733, 295, 411, 7139, 295, 957, 1412, 11, 437, 498, 291, 10432, 552, 11, 293, 300, 311, 264, 1558, 50940], "temperature": 0.0, "avg_logprob": -0.11797566999468886, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.00106424477417022}, {"id": 83, "seek": 43112, "start": 442.64, "end": 451.04, "text": " of React, and that's the idea of a lot of the popular agent frameworks. Because again, agents use", "tokens": [50940, 295, 30644, 11, 293, 300, 311, 264, 1558, 295, 257, 688, 295, 264, 3743, 9461, 29834, 13, 1436, 797, 11, 12554, 764, 51360], "temperature": 0.0, "avg_logprob": -0.11797566999468886, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.00106424477417022}, {"id": 84, "seek": 43112, "start": 451.04, "end": 456.28000000000003, "text": " a language model as a reasoning engine, so you want it to be really good at reasoning. So if", "tokens": [51360, 257, 2856, 2316, 382, 257, 21577, 2848, 11, 370, 291, 528, 309, 281, 312, 534, 665, 412, 21577, 13, 407, 498, 51622], "temperature": 0.0, "avg_logprob": -0.11797566999468886, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.00106424477417022}, {"id": 85, "seek": 43112, "start": 456.28000000000003, "end": 460.48, "text": " there are any prompting techniques you can use to improve its reasoning, you should probably do", "tokens": [51622, 456, 366, 604, 12391, 278, 7512, 291, 393, 764, 281, 3470, 1080, 21577, 11, 291, 820, 1391, 360, 51832], "temperature": 0.0, "avg_logprob": -0.11797566999468886, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.00106424477417022}, {"id": 86, "seek": 46048, "start": 460.48, "end": 464.32, "text": " those, and then a big part of it is also connecting into tools, and that's where action comes in.", "tokens": [50364, 729, 11, 293, 550, 257, 955, 644, 295, 309, 307, 611, 11015, 666, 3873, 11, 293, 300, 311, 689, 3069, 1487, 294, 13, 50556], "temperature": 0.0, "avg_logprob": -0.15835513739750304, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.00019705672457348555}, {"id": 87, "seek": 46048, "start": 464.32, "end": 472.0, "text": " And so you can see here, it arrives at kind of like the final answer. So that's the idea of", "tokens": [50556, 400, 370, 291, 393, 536, 510, 11, 309, 20116, 412, 733, 295, 411, 264, 2572, 1867, 13, 407, 300, 311, 264, 1558, 295, 50940], "temperature": 0.0, "avg_logprob": -0.15835513739750304, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.00019705672457348555}, {"id": 88, "seek": 46048, "start": 472.0, "end": 477.16, "text": " agents, React is still one of the more popular implementations for it, but what are some of the", "tokens": [50940, 12554, 11, 30644, 307, 920, 472, 295, 264, 544, 3743, 4445, 763, 337, 309, 11, 457, 437, 366, 512, 295, 264, 51198], "temperature": 0.0, "avg_logprob": -0.15835513739750304, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.00019705672457348555}, {"id": 89, "seek": 46048, "start": 477.16, "end": 484.16, "text": " current challenges? And there are a lot of challenges, like I think most agents are not", "tokens": [51198, 2190, 4759, 30, 400, 456, 366, 257, 688, 295, 4759, 11, 411, 286, 519, 881, 12554, 366, 406, 51548], "temperature": 0.0, "avg_logprob": -0.15835513739750304, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.00019705672457348555}, {"id": 90, "seek": 46048, "start": 484.16, "end": 490.36, "text": " amazingly production ready at the moment, and these are some of the reasons why, and we can", "tokens": [51548, 31762, 4265, 1919, 412, 264, 1623, 11, 293, 613, 366, 512, 295, 264, 4112, 983, 11, 293, 321, 393, 51858], "temperature": 0.0, "avg_logprob": -0.15835513739750304, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.00019705672457348555}, {"id": 91, "seek": 49036, "start": 491.24, "end": 497.32, "text": " walk through them in a bunch more detail. I'll also leave a lot of time at ends for the questions,", "tokens": [50408, 1792, 807, 552, 294, 257, 3840, 544, 2607, 13, 286, 603, 611, 1856, 257, 688, 295, 565, 412, 5314, 337, 264, 1651, 11, 50712], "temperature": 0.0, "avg_logprob": -0.1596036268317181, "compression_ratio": 1.556, "no_speech_prob": 0.0006254133186303079}, {"id": 92, "seek": 49036, "start": 497.32, "end": 502.12, "text": " so I'd love to hear kind of like what you guys are observing as issues for getting agents to work", "tokens": [50712, 370, 286, 1116, 959, 281, 1568, 733, 295, 411, 437, 291, 1074, 366, 22107, 382, 2663, 337, 1242, 12554, 281, 589, 50952], "temperature": 0.0, "avg_logprob": -0.1596036268317181, "compression_ratio": 1.556, "no_speech_prob": 0.0006254133186303079}, {"id": 93, "seek": 49036, "start": 502.12, "end": 507.64, "text": " reliably. This is probably far from a complete list. So the most basic challenge with agents is", "tokens": [50952, 49927, 13, 639, 307, 1391, 1400, 490, 257, 3566, 1329, 13, 407, 264, 881, 3875, 3430, 365, 12554, 307, 51228], "temperature": 0.0, "avg_logprob": -0.1596036268317181, "compression_ratio": 1.556, "no_speech_prob": 0.0006254133186303079}, {"id": 94, "seek": 49036, "start": 507.64, "end": 514.84, "text": " getting them to use tools in appropriate scenarios. And so in the React paper, they address this", "tokens": [51228, 1242, 552, 281, 764, 3873, 294, 6854, 15077, 13, 400, 370, 294, 264, 30644, 3035, 11, 436, 2985, 341, 51588], "temperature": 0.0, "avg_logprob": -0.1596036268317181, "compression_ratio": 1.556, "no_speech_prob": 0.0006254133186303079}, {"id": 95, "seek": 51484, "start": 514.9200000000001, "end": 520.5600000000001, "text": " challenge by bringing in the reasoning aspect of it, the chain of thought, style, prompting,", "tokens": [50368, 3430, 538, 5062, 294, 264, 21577, 4171, 295, 309, 11, 264, 5021, 295, 1194, 11, 3758, 11, 12391, 278, 11, 50650], "temperature": 0.0, "avg_logprob": -0.12354926154727028, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.009548324160277843}, {"id": 96, "seek": 51484, "start": 520.5600000000001, "end": 527.4, "text": " asking it to think. Kind of like common ways to do this include just like saying in the instructions,", "tokens": [50650, 3365, 309, 281, 519, 13, 9242, 295, 411, 2689, 2098, 281, 360, 341, 4090, 445, 411, 1566, 294, 264, 9415, 11, 50992], "temperature": 0.0, "avg_logprob": -0.12354926154727028, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.009548324160277843}, {"id": 97, "seek": 51484, "start": 527.4, "end": 531.72, "text": " you have access to these tools, you should use these to overcome some of your limitations,", "tokens": [50992, 291, 362, 2105, 281, 613, 3873, 11, 291, 820, 764, 613, 281, 10473, 512, 295, 428, 15705, 11, 51208], "temperature": 0.0, "avg_logprob": -0.12354926154727028, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.009548324160277843}, {"id": 98, "seek": 51484, "start": 531.72, "end": 536.52, "text": " and just basically instructing the language model. Tool descriptions are really, really", "tokens": [51208, 293, 445, 1936, 7232, 278, 264, 2856, 2316, 13, 15934, 24406, 366, 534, 11, 534, 51448], "temperature": 0.0, "avg_logprob": -0.12354926154727028, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.009548324160277843}, {"id": 99, "seek": 51484, "start": 536.52, "end": 541.1600000000001, "text": " important for this. If you want the agent to use a particular tool, it should probably have", "tokens": [51448, 1021, 337, 341, 13, 759, 291, 528, 264, 9461, 281, 764, 257, 1729, 2290, 11, 309, 820, 1391, 362, 51680], "temperature": 0.0, "avg_logprob": -0.12354926154727028, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.009548324160277843}, {"id": 100, "seek": 54116, "start": 541.16, "end": 547.24, "text": " enough context to know that this tool is good at this, and that generally comes in the form of", "tokens": [50364, 1547, 4319, 281, 458, 300, 341, 2290, 307, 665, 412, 341, 11, 293, 300, 5101, 1487, 294, 264, 1254, 295, 50668], "temperature": 0.0, "avg_logprob": -0.09662915151053612, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.001597001333720982}, {"id": 101, "seek": 54116, "start": 547.24, "end": 551.16, "text": " like tool descriptions or some information about the tool that's passed into the prompt.", "tokens": [50668, 411, 2290, 24406, 420, 512, 1589, 466, 264, 2290, 300, 311, 4678, 666, 264, 12391, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09662915151053612, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.001597001333720982}, {"id": 102, "seek": 54116, "start": 553.0799999999999, "end": 557.24, "text": " That can maybe not scale super well if you've got a lot of tools, because now you've got maybe", "tokens": [50960, 663, 393, 1310, 406, 4373, 1687, 731, 498, 291, 600, 658, 257, 688, 295, 3873, 11, 570, 586, 291, 600, 658, 1310, 51168], "temperature": 0.0, "avg_logprob": -0.09662915151053612, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.001597001333720982}, {"id": 103, "seek": 54116, "start": 557.24, "end": 562.52, "text": " these more complex descriptions, you want to put them in the final prompt for the language model", "tokens": [51168, 613, 544, 3997, 24406, 11, 291, 528, 281, 829, 552, 294, 264, 2572, 12391, 337, 264, 2856, 2316, 51432], "temperature": 0.0, "avg_logprob": -0.09662915151053612, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.001597001333720982}, {"id": 104, "seek": 54116, "start": 562.52, "end": 568.12, "text": " to know what to do with them, but you can quickly run into kind of like context length issues.", "tokens": [51432, 281, 458, 437, 281, 360, 365, 552, 11, 457, 291, 393, 2661, 1190, 666, 733, 295, 411, 4319, 4641, 2663, 13, 51712], "temperature": 0.0, "avg_logprob": -0.09662915151053612, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.001597001333720982}, {"id": 105, "seek": 56812, "start": 568.12, "end": 572.52, "text": " And so that's where I think the idea of tool retrieval comes in handy. So you can have hundreds,", "tokens": [50364, 400, 370, 300, 311, 689, 286, 519, 264, 1558, 295, 2290, 19817, 3337, 1487, 294, 13239, 13, 407, 291, 393, 362, 6779, 11, 50584], "temperature": 0.0, "avg_logprob": -0.08206039667129517, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.0006458857096731663}, {"id": 106, "seek": 56812, "start": 572.52, "end": 577.48, "text": " thousands of tools, you can do some retrieval step, and I think retrieval is another really", "tokens": [50584, 5383, 295, 3873, 11, 291, 393, 360, 512, 19817, 3337, 1823, 11, 293, 286, 519, 19817, 3337, 307, 1071, 534, 50832], "temperature": 0.0, "avg_logprob": -0.08206039667129517, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.0006458857096731663}, {"id": 107, "seek": 56812, "start": 577.48, "end": 580.84, "text": " interesting topic that I'm not going to go into too much depth here. So for the sake of this,", "tokens": [50832, 1880, 4829, 300, 286, 478, 406, 516, 281, 352, 666, 886, 709, 7161, 510, 13, 407, 337, 264, 9717, 295, 341, 11, 51000], "temperature": 0.0, "avg_logprob": -0.08206039667129517, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.0006458857096731663}, {"id": 108, "seek": 56812, "start": 580.84, "end": 584.68, "text": " we'll just say it's some embedding search lookup, although I think there's a lot more interesting", "tokens": [51000, 321, 603, 445, 584, 309, 311, 512, 12240, 3584, 3164, 574, 1010, 11, 4878, 286, 519, 456, 311, 257, 688, 544, 1880, 51192], "temperature": 0.0, "avg_logprob": -0.08206039667129517, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.0006458857096731663}, {"id": 109, "seek": 56812, "start": 584.68, "end": 589.48, "text": " things to do there. You basically do the retrieval step, you get back five, ten, however many tools", "tokens": [51192, 721, 281, 360, 456, 13, 509, 1936, 360, 264, 19817, 3337, 1823, 11, 291, 483, 646, 1732, 11, 2064, 11, 4461, 867, 3873, 51432], "temperature": 0.0, "avg_logprob": -0.08206039667129517, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.0006458857096731663}, {"id": 110, "seek": 56812, "start": 589.48, "end": 593.4, "text": " that you think are most promising, you can then pass those to the prompt and kind of have the", "tokens": [51432, 300, 291, 519, 366, 881, 20257, 11, 291, 393, 550, 1320, 729, 281, 264, 12391, 293, 733, 295, 362, 264, 51628], "temperature": 0.0, "avg_logprob": -0.08206039667129517, "compression_ratio": 1.8456591639871383, "no_speech_prob": 0.0006458857096731663}, {"id": 111, "seek": 59340, "start": 593.4, "end": 598.28, "text": " language model take the final steps from there. Few shot examples I think can also be really", "tokens": [50364, 2856, 2316, 747, 264, 2572, 4439, 490, 456, 13, 33468, 3347, 5110, 286, 519, 393, 611, 312, 534, 50608], "temperature": 0.0, "avg_logprob": -0.07121788689849573, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.000815796956885606}, {"id": 112, "seek": 59340, "start": 598.28, "end": 603.88, "text": " helpful, so you can use those to guide the language model in what to do. Again, I think the idea", "tokens": [50608, 4961, 11, 370, 291, 393, 764, 729, 281, 5934, 264, 2856, 2316, 294, 437, 281, 360, 13, 3764, 11, 286, 519, 264, 1558, 50888], "temperature": 0.0, "avg_logprob": -0.07121788689849573, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.000815796956885606}, {"id": 113, "seek": 59340, "start": 603.88, "end": 608.52, "text": " of retrieval to find the most relevant few shot examples is particularly promising here. So if", "tokens": [50888, 295, 19817, 3337, 281, 915, 264, 881, 7340, 1326, 3347, 5110, 307, 4098, 20257, 510, 13, 407, 498, 51120], "temperature": 0.0, "avg_logprob": -0.07121788689849573, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.000815796956885606}, {"id": 114, "seek": 59340, "start": 608.52, "end": 612.76, "text": " you give it examples similar to the one it's trying to do, those help a lot better than random", "tokens": [51120, 291, 976, 309, 5110, 2531, 281, 264, 472, 309, 311, 1382, 281, 360, 11, 729, 854, 257, 688, 1101, 813, 4974, 51332], "temperature": 0.0, "avg_logprob": -0.07121788689849573, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.000815796956885606}, {"id": 115, "seek": 59340, "start": 612.76, "end": 617.3199999999999, "text": " examples. And then probably the most extreme version of this is fine tuning a model like", "tokens": [51332, 5110, 13, 400, 550, 1391, 264, 881, 8084, 3037, 295, 341, 307, 2489, 15164, 257, 2316, 411, 51560], "temperature": 0.0, "avg_logprob": -0.07121788689849573, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.000815796956885606}, {"id": 116, "seek": 61732, "start": 617.32, "end": 624.84, "text": " tool former to really help with tool selection. There's also a subtle second challenge which", "tokens": [50364, 2290, 5819, 281, 534, 854, 365, 2290, 9450, 13, 821, 311, 611, 257, 13743, 1150, 3430, 597, 50740], "temperature": 0.0, "avg_logprob": -0.07375242926857688, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0032698549330234528}, {"id": 117, "seek": 61732, "start": 624.84, "end": 629.24, "text": " is getting them not to use tools when they don't need to. So a big use case for agents is having", "tokens": [50740, 307, 1242, 552, 406, 281, 764, 3873, 562, 436, 500, 380, 643, 281, 13, 407, 257, 955, 764, 1389, 337, 12554, 307, 1419, 50960], "temperature": 0.0, "avg_logprob": -0.07375242926857688, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0032698549330234528}, {"id": 118, "seek": 61732, "start": 629.24, "end": 634.6, "text": " conversational style agents. One of the big problems that we've seen is oftentimes these types", "tokens": [50960, 2615, 1478, 3758, 12554, 13, 1485, 295, 264, 955, 2740, 300, 321, 600, 1612, 307, 18349, 613, 3467, 51228], "temperature": 0.0, "avg_logprob": -0.07375242926857688, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0032698549330234528}, {"id": 119, "seek": 61732, "start": 634.6, "end": 639.32, "text": " of agents just want to use tools no matter what, even if they're having a conversation. And so", "tokens": [51228, 295, 12554, 445, 528, 281, 764, 3873, 572, 1871, 437, 11, 754, 498, 436, 434, 1419, 257, 3761, 13, 400, 370, 51464], "temperature": 0.0, "avg_logprob": -0.07375242926857688, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0032698549330234528}, {"id": 120, "seek": 61732, "start": 639.32, "end": 644.6800000000001, "text": " again, like the most basic thing you can do is probably put some information in the instructions,", "tokens": [51464, 797, 11, 411, 264, 881, 3875, 551, 291, 393, 360, 307, 1391, 829, 512, 1589, 294, 264, 9415, 11, 51732], "temperature": 0.0, "avg_logprob": -0.07375242926857688, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0032698549330234528}, {"id": 121, "seek": 64468, "start": 644.68, "end": 648.92, "text": " some reminder in the prompt like, hey, you don't have to use a tool, you can respond to the user", "tokens": [50364, 512, 13548, 294, 264, 12391, 411, 11, 4177, 11, 291, 500, 380, 362, 281, 764, 257, 2290, 11, 291, 393, 4196, 281, 264, 4195, 50576], "temperature": 0.0, "avg_logprob": -0.08528185291450564, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.00031477338052354753}, {"id": 122, "seek": 64468, "start": 648.92, "end": 654.92, "text": " if it seems like it's more of a conversation. That can get you so far. Another kind of like clever", "tokens": [50576, 498, 309, 2544, 411, 309, 311, 544, 295, 257, 3761, 13, 663, 393, 483, 291, 370, 1400, 13, 3996, 733, 295, 411, 13494, 50876], "temperature": 0.0, "avg_logprob": -0.08528185291450564, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.00031477338052354753}, {"id": 123, "seek": 64468, "start": 654.92, "end": 660.12, "text": " hack that we've seen here is add another tool that explicitly just returns to the user. And then,", "tokens": [50876, 10339, 300, 321, 600, 1612, 510, 307, 909, 1071, 2290, 300, 20803, 445, 11247, 281, 264, 4195, 13, 400, 550, 11, 51136], "temperature": 0.0, "avg_logprob": -0.08528185291450564, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.00031477338052354753}, {"id": 124, "seek": 64468, "start": 660.12, "end": 665.88, "text": " you know, they like to use tools, but they'll usually use that tool. So I thought that was a", "tokens": [51136, 291, 458, 11, 436, 411, 281, 764, 3873, 11, 457, 436, 603, 2673, 764, 300, 2290, 13, 407, 286, 1194, 300, 390, 257, 51424], "temperature": 0.0, "avg_logprob": -0.08528185291450564, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.00031477338052354753}, {"id": 125, "seek": 64468, "start": 665.88, "end": 673.24, "text": " pretty clever and interesting hack. A third challenge is the language models tell you what", "tokens": [51424, 1238, 13494, 293, 1880, 10339, 13, 316, 2636, 3430, 307, 264, 2856, 5245, 980, 291, 437, 51792], "temperature": 0.0, "avg_logprob": -0.08528185291450564, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.00031477338052354753}, {"id": 126, "seek": 67324, "start": 673.24, "end": 679.72, "text": " tool to use and how to use it. But that's in the form of a string. And so you need to go from that", "tokens": [50364, 2290, 281, 764, 293, 577, 281, 764, 309, 13, 583, 300, 311, 294, 264, 1254, 295, 257, 6798, 13, 400, 370, 291, 643, 281, 352, 490, 300, 50688], "temperature": 0.0, "avg_logprob": -0.08213932493813017, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0006257041241042316}, {"id": 127, "seek": 67324, "start": 679.72, "end": 684.92, "text": " string into some code or something that can actually be run. And so that involves parsing kind", "tokens": [50688, 6798, 666, 512, 3089, 420, 746, 300, 393, 767, 312, 1190, 13, 400, 370, 300, 11626, 21156, 278, 733, 50948], "temperature": 0.0, "avg_logprob": -0.08213932493813017, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0006257041241042316}, {"id": 128, "seek": 67324, "start": 684.92, "end": 690.2, "text": " of like the output of the language model into this tool invocation. And so some tips and tricks", "tokens": [50948, 295, 411, 264, 5598, 295, 264, 2856, 2316, 666, 341, 2290, 1048, 27943, 13, 400, 370, 512, 6082, 293, 11733, 51212], "temperature": 0.0, "avg_logprob": -0.08213932493813017, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0006257041241042316}, {"id": 129, "seek": 67324, "start": 690.2, "end": 693.96, "text": " and hacks here are one like the more structured you ask for the response, the easier it is to", "tokens": [51212, 293, 33617, 510, 366, 472, 411, 264, 544, 18519, 291, 1029, 337, 264, 4134, 11, 264, 3571, 309, 307, 281, 51400], "temperature": 0.0, "avg_logprob": -0.08213932493813017, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0006257041241042316}, {"id": 130, "seek": 67324, "start": 693.96, "end": 699.8, "text": " parse generally. So language models are pretty good at writing JSON. So we've kind of transitioned", "tokens": [51400, 48377, 5101, 13, 407, 2856, 5245, 366, 1238, 665, 412, 3579, 31828, 13, 407, 321, 600, 733, 295, 47346, 51692], "temperature": 0.0, "avg_logprob": -0.08213932493813017, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0006257041241042316}, {"id": 131, "seek": 69980, "start": 699.88, "end": 706.5999999999999, "text": " a few of our agents to use that schema. Still doesn't always work, especially some of the", "tokens": [50368, 257, 1326, 295, 527, 12554, 281, 764, 300, 34078, 13, 8291, 1177, 380, 1009, 589, 11, 2318, 512, 295, 264, 50704], "temperature": 0.0, "avg_logprob": -0.07990524845738564, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.00040420741424895823}, {"id": 132, "seek": 69980, "start": 706.5999999999999, "end": 715.24, "text": " chat models like to add in a lot of kind of like language. So we've introduced kind of like this", "tokens": [50704, 5081, 5245, 411, 281, 909, 294, 257, 688, 295, 733, 295, 411, 2856, 13, 407, 321, 600, 7268, 733, 295, 411, 341, 51136], "temperature": 0.0, "avg_logprob": -0.07990524845738564, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.00040420741424895823}, {"id": 133, "seek": 69980, "start": 715.24, "end": 720.12, "text": " concept of output parsers, which generically encapsulate all the logic that's needed to", "tokens": [51136, 3410, 295, 5598, 21156, 433, 11, 597, 1337, 984, 38745, 5256, 439, 264, 9952, 300, 311, 2978, 281, 51380], "temperature": 0.0, "avg_logprob": -0.07990524845738564, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.00040420741424895823}, {"id": 134, "seek": 69980, "start": 720.12, "end": 725.16, "text": " parse this response. And we've tried to make that as modular as possible. So if you're seeing areas,", "tokens": [51380, 48377, 341, 4134, 13, 400, 321, 600, 3031, 281, 652, 300, 382, 31111, 382, 1944, 13, 407, 498, 291, 434, 2577, 3179, 11, 51632], "temperature": 0.0, "avg_logprob": -0.07990524845738564, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.00040420741424895823}, {"id": 135, "seek": 72516, "start": 725.16, "end": 730.1999999999999, "text": " you can hopefully kind of like substitute that out very related to that. We also have a concept of", "tokens": [50364, 291, 393, 4696, 733, 295, 411, 15802, 300, 484, 588, 4077, 281, 300, 13, 492, 611, 362, 257, 3410, 295, 50616], "temperature": 0.0, "avg_logprob": -0.12342971304188603, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0007910267449915409}, {"id": 136, "seek": 72516, "start": 730.1999999999999, "end": 737.24, "text": " like output parsers that can retry and fix mistakes. So and I think there's there's some subtle,", "tokens": [50616, 411, 5598, 21156, 433, 300, 393, 1533, 627, 293, 3191, 8038, 13, 407, 293, 286, 519, 456, 311, 456, 311, 512, 13743, 11, 50968], "temperature": 0.0, "avg_logprob": -0.12342971304188603, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0007910267449915409}, {"id": 137, "seek": 72516, "start": 738.12, "end": 741.8, "text": " there's some subtle differences here that I think are really cool. Basically, like, you know,", "tokens": [51012, 456, 311, 512, 13743, 7300, 510, 300, 286, 519, 366, 534, 1627, 13, 8537, 11, 411, 11, 291, 458, 11, 51196], "temperature": 0.0, "avg_logprob": -0.12342971304188603, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0007910267449915409}, {"id": 138, "seek": 72516, "start": 741.8, "end": 748.28, "text": " if you have misformatted schema, you can fix that explicitly by just passing it the output", "tokens": [51196, 498, 291, 362, 3346, 837, 32509, 34078, 11, 291, 393, 3191, 300, 20803, 538, 445, 8437, 309, 264, 5598, 51520], "temperature": 0.0, "avg_logprob": -0.12342971304188603, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0007910267449915409}, {"id": 139, "seek": 72516, "start": 748.28, "end": 754.12, "text": " and the error and saying fix this response. But if you have, if you have an output that just", "tokens": [51520, 293, 264, 6713, 293, 1566, 3191, 341, 4134, 13, 583, 498, 291, 362, 11, 498, 291, 362, 364, 5598, 300, 445, 51812], "temperature": 0.0, "avg_logprob": -0.12342971304188603, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0007910267449915409}, {"id": 140, "seek": 75412, "start": 754.12, "end": 758.52, "text": " forgets one of the fields, like it returns the action, but not the action input or something", "tokens": [50364, 2870, 82, 472, 295, 264, 7909, 11, 411, 309, 11247, 264, 3069, 11, 457, 406, 264, 3069, 4846, 420, 746, 50584], "temperature": 0.0, "avg_logprob": -0.07675093923296247, "compression_ratio": 1.76875, "no_speech_prob": 0.0002096529642585665}, {"id": 141, "seek": 75412, "start": 758.52, "end": 761.8, "text": " like that, you need to provide more information here. So I think there's actually a lot of", "tokens": [50584, 411, 300, 11, 291, 643, 281, 2893, 544, 1589, 510, 13, 407, 286, 519, 456, 311, 767, 257, 688, 295, 50748], "temperature": 0.0, "avg_logprob": -0.07675093923296247, "compression_ratio": 1.76875, "no_speech_prob": 0.0002096529642585665}, {"id": 142, "seek": 75412, "start": 761.8, "end": 766.04, "text": " subtlety in fixing some of these errors. But the basic idea is that you can try to parse it.", "tokens": [50748, 7257, 75, 2210, 294, 19442, 512, 295, 613, 13603, 13, 583, 264, 3875, 1558, 307, 300, 291, 393, 853, 281, 48377, 309, 13, 50960], "temperature": 0.0, "avg_logprob": -0.07675093923296247, "compression_ratio": 1.76875, "no_speech_prob": 0.0002096529642585665}, {"id": 143, "seek": 75412, "start": 766.92, "end": 771.96, "text": " If you if it fails, you can then try to fix it. All this we currently encapsulate in this idea", "tokens": [51004, 759, 291, 498, 309, 18199, 11, 291, 393, 550, 853, 281, 3191, 309, 13, 1057, 341, 321, 4362, 38745, 5256, 294, 341, 1558, 51256], "temperature": 0.0, "avg_logprob": -0.07675093923296247, "compression_ratio": 1.76875, "no_speech_prob": 0.0002096529642585665}, {"id": 144, "seek": 75412, "start": 771.96, "end": 778.76, "text": " of like output parsers. So the fourth challenge is getting them to remember previous steps that", "tokens": [51256, 295, 411, 5598, 21156, 433, 13, 407, 264, 6409, 3430, 307, 1242, 552, 281, 1604, 3894, 4439, 300, 51596], "temperature": 0.0, "avg_logprob": -0.07675093923296247, "compression_ratio": 1.76875, "no_speech_prob": 0.0002096529642585665}, {"id": 145, "seek": 75412, "start": 778.76, "end": 782.92, "text": " were taken. The most basic thing, the thing that the React paper does is just keep a list of those", "tokens": [51596, 645, 2726, 13, 440, 881, 3875, 551, 11, 264, 551, 300, 264, 30644, 3035, 775, 307, 445, 1066, 257, 1329, 295, 729, 51804], "temperature": 0.0, "avg_logprob": -0.07675093923296247, "compression_ratio": 1.76875, "no_speech_prob": 0.0002096529642585665}, {"id": 146, "seek": 78292, "start": 782.92, "end": 790.5999999999999, "text": " steps in memory. Again, that starts to run into some some context window issues, especially when", "tokens": [50364, 4439, 294, 4675, 13, 3764, 11, 300, 3719, 281, 1190, 666, 512, 512, 4319, 4910, 2663, 11, 2318, 562, 50748], "temperature": 0.0, "avg_logprob": -0.08336832879603594, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0007086945115588605}, {"id": 147, "seek": 78292, "start": 790.5999999999999, "end": 795.64, "text": " you're dealing with long running tasks. And so the thing that we've seen done here is again,", "tokens": [50748, 291, 434, 6260, 365, 938, 2614, 9608, 13, 400, 370, 264, 551, 300, 321, 600, 1612, 1096, 510, 307, 797, 11, 51000], "temperature": 0.0, "avg_logprob": -0.08336832879603594, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0007086945115588605}, {"id": 148, "seek": 78292, "start": 795.64, "end": 799.9599999999999, "text": " fetch previous steps with some retrieval method and put those into context. Usually we've actually", "tokens": [51000, 23673, 3894, 4439, 365, 512, 19817, 3337, 3170, 293, 829, 729, 666, 4319, 13, 11419, 321, 600, 767, 51216], "temperature": 0.0, "avg_logprob": -0.08336832879603594, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0007086945115588605}, {"id": 149, "seek": 78292, "start": 799.9599999999999, "end": 806.8399999999999, "text": " seen a combination of the two. So we've seen using the end most recent actions and observations", "tokens": [51216, 1612, 257, 6562, 295, 264, 732, 13, 407, 321, 600, 1612, 1228, 264, 917, 881, 5162, 5909, 293, 18163, 51560], "temperature": 0.0, "avg_logprob": -0.08336832879603594, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0007086945115588605}, {"id": 150, "seek": 80684, "start": 806.84, "end": 814.76, "text": " combined with the K most relevant actions and observations. Incorporating long observations", "tokens": [50364, 9354, 365, 264, 591, 881, 7340, 5909, 293, 18163, 13, 39120, 2816, 990, 938, 18163, 50760], "temperature": 0.0, "avg_logprob": -0.0992436408996582, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0009543556370772421}, {"id": 151, "seek": 80684, "start": 814.76, "end": 819.0, "text": " is another really interesting one. This actually comes up, or this came up a lot when we were", "tokens": [50760, 307, 1071, 534, 1880, 472, 13, 639, 767, 1487, 493, 11, 420, 341, 1361, 493, 257, 688, 562, 321, 645, 50972], "temperature": 0.0, "avg_logprob": -0.0992436408996582, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0009543556370772421}, {"id": 152, "seek": 80684, "start": 819.0, "end": 824.76, "text": " dealing with working with APIs, because APIs often return really big JSON blobs that are really big", "tokens": [50972, 6260, 365, 1364, 365, 21445, 11, 570, 21445, 2049, 2736, 534, 955, 31828, 1749, 929, 300, 366, 534, 955, 51260], "temperature": 0.0, "avg_logprob": -0.0992436408996582, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0009543556370772421}, {"id": 153, "seek": 80684, "start": 824.76, "end": 831.24, "text": " and hard to put in context. So the most common thing that we've done here is just parse that in", "tokens": [51260, 293, 1152, 281, 829, 294, 4319, 13, 407, 264, 881, 2689, 551, 300, 321, 600, 1096, 510, 307, 445, 48377, 300, 294, 51584], "temperature": 0.0, "avg_logprob": -0.0992436408996582, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0009543556370772421}, {"id": 154, "seek": 83124, "start": 831.24, "end": 837.0, "text": " some way. You can do really simple stuff like convert that blob to a string and put the first", "tokens": [50364, 512, 636, 13, 509, 393, 360, 534, 2199, 1507, 411, 7620, 300, 46115, 281, 257, 6798, 293, 829, 264, 700, 50652], "temperature": 0.0, "avg_logprob": -0.07311062901108353, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.051044709980487823}, {"id": 155, "seek": 83124, "start": 837.0, "end": 842.12, "text": " like 1000 characters or something as the response. You can also do some more if you know that you're", "tokens": [50652, 411, 9714, 4342, 420, 746, 382, 264, 4134, 13, 509, 393, 611, 360, 512, 544, 498, 291, 458, 300, 291, 434, 50908], "temperature": 0.0, "avg_logprob": -0.07311062901108353, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.051044709980487823}, {"id": 156, "seek": 83124, "start": 842.12, "end": 847.96, "text": " working with a specific API, you can probably write some custom logic to kind of like take only the", "tokens": [50908, 1364, 365, 257, 2685, 9362, 11, 291, 393, 1391, 2464, 512, 2375, 9952, 281, 733, 295, 411, 747, 787, 264, 51200], "temperature": 0.0, "avg_logprob": -0.07311062901108353, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.051044709980487823}, {"id": 157, "seek": 83124, "start": 847.96, "end": 852.44, "text": " relevant keys and put that if you want to make something general, you could also maybe do something", "tokens": [51200, 7340, 9317, 293, 829, 300, 498, 291, 528, 281, 652, 746, 2674, 11, 291, 727, 611, 1310, 360, 746, 51424], "temperature": 0.0, "avg_logprob": -0.07311062901108353, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.051044709980487823}, {"id": 158, "seek": 83124, "start": 852.44, "end": 857.72, "text": " dynamically to like figure out what key like basically explore the JSON object and figure out", "tokens": [51424, 43492, 281, 411, 2573, 484, 437, 2141, 411, 1936, 6839, 264, 31828, 2657, 293, 2573, 484, 51688], "temperature": 0.0, "avg_logprob": -0.07311062901108353, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.051044709980487823}, {"id": 159, "seek": 85772, "start": 857.72, "end": 863.08, "text": " what keys to put in. That's a bit more exploratory, I would say. But the basic idea is, yeah, there", "tokens": [50364, 437, 9317, 281, 829, 294, 13, 663, 311, 257, 857, 544, 24765, 4745, 11, 286, 576, 584, 13, 583, 264, 3875, 1558, 307, 11, 1338, 11, 456, 50632], "temperature": 0.0, "avg_logprob": -0.08835033356674074, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.004753436427563429}, {"id": 160, "seek": 85772, "start": 863.08, "end": 869.1600000000001, "text": " is this issue of, and so Zapier, I always have to think about how to pronounce it, but it's Zapier", "tokens": [50632, 307, 341, 2734, 295, 11, 293, 370, 34018, 811, 11, 286, 1009, 362, 281, 519, 466, 577, 281, 19567, 309, 11, 457, 309, 311, 34018, 811, 50936], "temperature": 0.0, "avg_logprob": -0.08835033356674074, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.004753436427563429}, {"id": 161, "seek": 85772, "start": 869.1600000000001, "end": 874.44, "text": " makes you happier. So Zapier when they did this with their natural language API, not only did they", "tokens": [50936, 1669, 291, 20423, 13, 407, 34018, 811, 562, 436, 630, 341, 365, 641, 3303, 2856, 9362, 11, 406, 787, 630, 436, 51200], "temperature": 0.0, "avg_logprob": -0.08835033356674074, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.004753436427563429}, {"id": 162, "seek": 85772, "start": 874.44, "end": 880.44, "text": " have something before the API that was like natural language to some API call, they also spent a lot", "tokens": [51200, 362, 746, 949, 264, 9362, 300, 390, 411, 3303, 2856, 281, 512, 9362, 818, 11, 436, 611, 4418, 257, 688, 51500], "temperature": 0.0, "avg_logprob": -0.08835033356674074, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.004753436427563429}, {"id": 163, "seek": 85772, "start": 880.44, "end": 884.12, "text": " of time working on the output. And so the output is actually very specifically, I think it's like", "tokens": [51500, 295, 565, 1364, 322, 264, 5598, 13, 400, 370, 264, 5598, 307, 767, 588, 4682, 11, 286, 519, 309, 311, 411, 51684], "temperature": 0.0, "avg_logprob": -0.08835033356674074, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.004753436427563429}, {"id": 164, "seek": 88412, "start": 884.12, "end": 888.44, "text": " under like 200 or 300 tokens or something like that. And they did that on purpose. They spent a", "tokens": [50364, 833, 411, 2331, 420, 6641, 22667, 420, 746, 411, 300, 13, 400, 436, 630, 300, 322, 4334, 13, 814, 4418, 257, 50580], "temperature": 0.0, "avg_logprob": -0.08136596027602497, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.000579038925934583}, {"id": 165, "seek": 88412, "start": 888.44, "end": 892.6, "text": " lot of time thinking about that. And so I think for tool usage, that is really important as well.", "tokens": [50580, 688, 295, 565, 1953, 466, 300, 13, 400, 370, 286, 519, 337, 2290, 14924, 11, 300, 307, 534, 1021, 382, 731, 13, 50788], "temperature": 0.0, "avg_logprob": -0.08136596027602497, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.000579038925934583}, {"id": 166, "seek": 88412, "start": 893.8, "end": 899.8, "text": " Another more kind of like exploratory way of doing this is also you could perhaps just store", "tokens": [50848, 3996, 544, 733, 295, 411, 24765, 4745, 636, 295, 884, 341, 307, 611, 291, 727, 4317, 445, 3531, 51148], "temperature": 0.0, "avg_logprob": -0.08136596027602497, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.000579038925934583}, {"id": 167, "seek": 88412, "start": 899.8, "end": 904.68, "text": " the long output and then do retrieval on it when you're trying to think of like what next steps to", "tokens": [51148, 264, 938, 5598, 293, 550, 360, 19817, 3337, 322, 309, 562, 291, 434, 1382, 281, 519, 295, 411, 437, 958, 4439, 281, 51392], "temperature": 0.0, "avg_logprob": -0.08136596027602497, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.000579038925934583}, {"id": 168, "seek": 88412, "start": 904.68, "end": 913.88, "text": " take. Agents can often go off track, especially in long running things. And so there's kind of", "tokens": [51392, 747, 13, 2725, 791, 393, 2049, 352, 766, 2837, 11, 2318, 294, 938, 2614, 721, 13, 400, 370, 456, 311, 733, 295, 51852], "temperature": 0.0, "avg_logprob": -0.08136596027602497, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.000579038925934583}, {"id": 169, "seek": 91388, "start": 914.04, "end": 917.96, "text": " two methods that I've seen to kind of like keep them on track. One, you can just reiterate the", "tokens": [50372, 732, 7150, 300, 286, 600, 1612, 281, 733, 295, 411, 1066, 552, 322, 2837, 13, 1485, 11, 291, 393, 445, 33528, 264, 50568], "temperature": 0.0, "avg_logprob": -0.08009283202035086, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0015959974844008684}, {"id": 170, "seek": 91388, "start": 917.96, "end": 925.16, "text": " objective right before it makes its action. And why this works, I think we've seen that with,", "tokens": [50568, 10024, 558, 949, 309, 1669, 1080, 3069, 13, 400, 983, 341, 1985, 11, 286, 519, 321, 600, 1612, 300, 365, 11, 50928], "temperature": 0.0, "avg_logprob": -0.08009283202035086, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0015959974844008684}, {"id": 171, "seek": 91388, "start": 925.16, "end": 929.08, "text": " at least with a lot of the current models, with instructions that are earlier in the prompt,", "tokens": [50928, 412, 1935, 365, 257, 688, 295, 264, 2190, 5245, 11, 365, 9415, 300, 366, 3071, 294, 264, 12391, 11, 51124], "temperature": 0.0, "avg_logprob": -0.08009283202035086, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0015959974844008684}, {"id": 172, "seek": 91388, "start": 929.08, "end": 932.68, "text": " it might forget it by the time it gets to the end if it's a really long prompt. So putting it at the", "tokens": [51124, 309, 1062, 2870, 309, 538, 264, 565, 309, 2170, 281, 264, 917, 498, 309, 311, 257, 534, 938, 12391, 13, 407, 3372, 309, 412, 264, 51304], "temperature": 0.0, "avg_logprob": -0.08009283202035086, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0015959974844008684}, {"id": 173, "seek": 91388, "start": 932.68, "end": 937.4, "text": " end seems to help. And then another really interesting one that I'll talk about when I talk about some", "tokens": [51304, 917, 2544, 281, 854, 13, 400, 550, 1071, 534, 1880, 472, 300, 286, 603, 751, 466, 562, 286, 751, 466, 512, 51540], "temperature": 0.0, "avg_logprob": -0.08009283202035086, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0015959974844008684}, {"id": 174, "seek": 91388, "start": 937.4, "end": 942.2, "text": " of the more recent papers and stuff that have come out is this idea of separating explicitly a", "tokens": [51540, 295, 264, 544, 5162, 10577, 293, 1507, 300, 362, 808, 484, 307, 341, 1558, 295, 29279, 20803, 257, 51780], "temperature": 0.0, "avg_logprob": -0.08009283202035086, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0015959974844008684}, {"id": 175, "seek": 94220, "start": 942.2, "end": 949.32, "text": " planning and execution step and basically have one step that explicitly kind of thinks about,", "tokens": [50364, 5038, 293, 15058, 1823, 293, 1936, 362, 472, 1823, 300, 20803, 733, 295, 7309, 466, 11, 50720], "temperature": 0.0, "avg_logprob": -0.0945384091344373, "compression_ratio": 1.8604651162790697, "no_speech_prob": 0.00043015298433601856}, {"id": 176, "seek": 94220, "start": 949.32, "end": 954.36, "text": " these are kind of like all the objectives that I want to do at a high level. And then a second", "tokens": [50720, 613, 366, 733, 295, 411, 439, 264, 15961, 300, 286, 528, 281, 360, 412, 257, 1090, 1496, 13, 400, 550, 257, 1150, 50972], "temperature": 0.0, "avg_logprob": -0.0945384091344373, "compression_ratio": 1.8604651162790697, "no_speech_prob": 0.00043015298433601856}, {"id": 177, "seek": 94220, "start": 954.36, "end": 960.2800000000001, "text": " step that says, okay, given this objective, given this one sub objective, now how do I do this one", "tokens": [50972, 1823, 300, 1619, 11, 1392, 11, 2212, 341, 10024, 11, 2212, 341, 472, 1422, 10024, 11, 586, 577, 360, 286, 360, 341, 472, 51268], "temperature": 0.0, "avg_logprob": -0.0945384091344373, "compression_ratio": 1.8604651162790697, "no_speech_prob": 0.00043015298433601856}, {"id": 178, "seek": 94220, "start": 960.2800000000001, "end": 964.84, "text": " sub objective and basically break it down even more in a hierarchical whole manner. And there's", "tokens": [51268, 1422, 10024, 293, 1936, 1821, 309, 760, 754, 544, 294, 257, 35250, 804, 1379, 9060, 13, 400, 456, 311, 51496], "temperature": 0.0, "avg_logprob": -0.0945384091344373, "compression_ratio": 1.8604651162790697, "no_speech_prob": 0.00043015298433601856}, {"id": 179, "seek": 94220, "start": 964.84, "end": 969.72, "text": " a good example of that with baby AGI, which I'll talk about in a bit. And then another big issue", "tokens": [51496, 257, 665, 1365, 295, 300, 365, 3186, 316, 26252, 11, 597, 286, 603, 751, 466, 294, 257, 857, 13, 400, 550, 1071, 955, 2734, 51740], "temperature": 0.0, "avg_logprob": -0.0945384091344373, "compression_ratio": 1.8604651162790697, "no_speech_prob": 0.00043015298433601856}, {"id": 180, "seek": 96972, "start": 969.72, "end": 975.24, "text": " is evaluation of these things. I think evaluation of language models in general, very difficult", "tokens": [50364, 307, 13344, 295, 613, 721, 13, 286, 519, 13344, 295, 2856, 5245, 294, 2674, 11, 588, 2252, 50640], "temperature": 0.0, "avg_logprob": -0.08892587025960287, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.006285059731453657}, {"id": 181, "seek": 96972, "start": 975.24, "end": 981.32, "text": " evaluation of applications built on top of language models. Also very difficult and agents are no", "tokens": [50640, 13344, 295, 5821, 3094, 322, 1192, 295, 2856, 5245, 13, 2743, 588, 2252, 293, 12554, 366, 572, 50944], "temperature": 0.0, "avg_logprob": -0.08892587025960287, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.006285059731453657}, {"id": 182, "seek": 96972, "start": 981.32, "end": 987.0, "text": " exception. I think there's the obvious kind of like evaluate whether it arrived at the correct", "tokens": [50944, 11183, 13, 286, 519, 456, 311, 264, 6322, 733, 295, 411, 13059, 1968, 309, 6678, 412, 264, 3006, 51228], "temperature": 0.0, "avg_logprob": -0.08892587025960287, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.006285059731453657}, {"id": 183, "seek": 96972, "start": 987.0, "end": 992.6, "text": " result in terms of in terms of getting metrics on evaluation. And so yeah, you know, if you're", "tokens": [51228, 1874, 294, 2115, 295, 294, 2115, 295, 1242, 16367, 322, 13344, 13, 400, 370, 1338, 11, 291, 458, 11, 498, 291, 434, 51508], "temperature": 0.0, "avg_logprob": -0.08892587025960287, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.006285059731453657}, {"id": 184, "seek": 96972, "start": 992.6, "end": 998.36, "text": " asking the agent to produce some answer, that's like a natural language response. There's techniques", "tokens": [51508, 3365, 264, 9461, 281, 5258, 512, 1867, 11, 300, 311, 411, 257, 3303, 2856, 4134, 13, 821, 311, 7512, 51796], "temperature": 0.0, "avg_logprob": -0.08892587025960287, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.006285059731453657}, {"id": 185, "seek": 99836, "start": 998.36, "end": 1004.04, "text": " you can do there. A lot of them in the flavor of asking a language model to score the expected", "tokens": [50364, 291, 393, 360, 456, 13, 316, 688, 295, 552, 294, 264, 6813, 295, 3365, 257, 2856, 2316, 281, 6175, 264, 5176, 50648], "temperature": 0.0, "avg_logprob": -0.09338793623338051, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00027780429809354246}, {"id": 186, "seek": 99836, "start": 1004.04, "end": 1008.44, "text": " answer and the actual answer and come up with some grade and stuff like that, that applies to the", "tokens": [50648, 1867, 293, 264, 3539, 1867, 293, 808, 493, 365, 512, 7204, 293, 1507, 411, 300, 11, 300, 13165, 281, 264, 50868], "temperature": 0.0, "avg_logprob": -0.09338793623338051, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00027780429809354246}, {"id": 187, "seek": 99836, "start": 1008.44, "end": 1012.52, "text": " output of agents as well. But then there's also some agent specific ones that I think are really", "tokens": [50868, 5598, 295, 12554, 382, 731, 13, 583, 550, 456, 311, 611, 512, 9461, 2685, 2306, 300, 286, 519, 366, 534, 51072], "temperature": 0.0, "avg_logprob": -0.09338793623338051, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00027780429809354246}, {"id": 188, "seek": 99836, "start": 1012.52, "end": 1017.4, "text": " interesting, mostly around evaluating these idea of like the agent trajectory or the intermediate", "tokens": [51072, 1880, 11, 5240, 926, 27479, 613, 1558, 295, 411, 264, 9461, 21512, 420, 264, 19376, 51316], "temperature": 0.0, "avg_logprob": -0.09338793623338051, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00027780429809354246}, {"id": 189, "seek": 99836, "start": 1017.4, "end": 1025.32, "text": " steps. And so where we'll actually have something coming out for this, someone opened a PR that I", "tokens": [51316, 4439, 13, 400, 370, 689, 321, 603, 767, 362, 746, 1348, 484, 337, 341, 11, 1580, 5625, 257, 11568, 300, 286, 51712], "temperature": 0.0, "avg_logprob": -0.09338793623338051, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00027780429809354246}, {"id": 190, "seek": 102532, "start": 1025.32, "end": 1028.6, "text": " need to get in. But basically, there's a lot of like little different things you can look at,", "tokens": [50364, 643, 281, 483, 294, 13, 583, 1936, 11, 456, 311, 257, 688, 295, 411, 707, 819, 721, 291, 393, 574, 412, 11, 50528], "temperature": 0.0, "avg_logprob": -0.10300826354765556, "compression_ratio": 1.8853503184713376, "no_speech_prob": 0.0034804714377969503}, {"id": 191, "seek": 102532, "start": 1028.6, "end": 1034.04, "text": " like did it take the correct action? Is the input to the action correct? Is it the correct number", "tokens": [50528, 411, 630, 309, 747, 264, 3006, 3069, 30, 1119, 264, 4846, 281, 264, 3069, 3006, 30, 1119, 309, 264, 3006, 1230, 50800], "temperature": 0.0, "avg_logprob": -0.10300826354765556, "compression_ratio": 1.8853503184713376, "no_speech_prob": 0.0034804714377969503}, {"id": 192, "seek": 102532, "start": 1034.04, "end": 1038.6799999999998, "text": " of steps? And by this, you know, like sometimes you, and this is very related to the next one,", "tokens": [50800, 295, 4439, 30, 400, 538, 341, 11, 291, 458, 11, 411, 2171, 291, 11, 293, 341, 307, 588, 4077, 281, 264, 958, 472, 11, 51032], "temperature": 0.0, "avg_logprob": -0.10300826354765556, "compression_ratio": 1.8853503184713376, "no_speech_prob": 0.0034804714377969503}, {"id": 193, "seek": 102532, "start": 1038.6799999999998, "end": 1042.2, "text": " which is like the most efficient sequence of steps. And so there's a bunch of different things that", "tokens": [51032, 597, 307, 411, 264, 881, 7148, 8310, 295, 4439, 13, 400, 370, 456, 311, 257, 3840, 295, 819, 721, 300, 51208], "temperature": 0.0, "avg_logprob": -0.10300826354765556, "compression_ratio": 1.8853503184713376, "no_speech_prob": 0.0034804714377969503}, {"id": 194, "seek": 102532, "start": 1042.2, "end": 1047.1599999999999, "text": " you can do to evaluate not only the final answer, but like is the agent getting there like efficiently,", "tokens": [51208, 291, 393, 360, 281, 13059, 406, 787, 264, 2572, 1867, 11, 457, 411, 307, 264, 9461, 1242, 456, 411, 19621, 11, 51456], "temperature": 0.0, "avg_logprob": -0.10300826354765556, "compression_ratio": 1.8853503184713376, "no_speech_prob": 0.0034804714377969503}, {"id": 195, "seek": 102532, "start": 1047.1599999999999, "end": 1053.32, "text": " correctly. And those are sometimes just as useful, if not more useful than evaluating the end result.", "tokens": [51456, 8944, 13, 400, 729, 366, 2171, 445, 382, 4420, 11, 498, 406, 544, 4420, 813, 27479, 264, 917, 1874, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10300826354765556, "compression_ratio": 1.8853503184713376, "no_speech_prob": 0.0034804714377969503}, {"id": 196, "seek": 105332, "start": 1054.04, "end": 1058.2, "text": " I'm trying to see what time it is, because I also want to leave lots of time for questions.", "tokens": [50400, 286, 478, 1382, 281, 536, 437, 565, 309, 307, 11, 570, 286, 611, 528, 281, 1856, 3195, 295, 565, 337, 1651, 13, 50608], "temperature": 0.0, "avg_logprob": -0.12599386052882416, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0004042824439238757}, {"id": 197, "seek": 105332, "start": 1058.9199999999998, "end": 1062.9199999999998, "text": " But I think I'm good. So memory, I think is really interesting as well. So we've obviously", "tokens": [50644, 583, 286, 519, 286, 478, 665, 13, 407, 4675, 11, 286, 519, 307, 534, 1880, 382, 731, 13, 407, 321, 600, 2745, 50844], "temperature": 0.0, "avg_logprob": -0.12599386052882416, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0004042824439238757}, {"id": 198, "seek": 105332, "start": 1063.6399999999999, "end": 1069.1599999999999, "text": " tried it about like memory of remembering the AI to tool interactions. There's also like a more", "tokens": [50880, 3031, 309, 466, 411, 4675, 295, 20719, 264, 7318, 281, 2290, 13280, 13, 821, 311, 611, 411, 257, 544, 51156], "temperature": 0.0, "avg_logprob": -0.12599386052882416, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0004042824439238757}, {"id": 199, "seek": 105332, "start": 1069.1599999999999, "end": 1078.04, "text": " basic idea of remembering the user to AI interactions. But I think the third type, which is showing up", "tokens": [51156, 3875, 1558, 295, 20719, 264, 4195, 281, 7318, 13280, 13, 583, 286, 519, 264, 2636, 2010, 11, 597, 307, 4099, 493, 51600], "temperature": 0.0, "avg_logprob": -0.12599386052882416, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.0004042824439238757}, {"id": 200, "seek": 107804, "start": 1078.04, "end": 1083.8799999999999, "text": " in a lot of the recent papers on agents is this idea of like personalization of giving an agent", "tokens": [50364, 294, 257, 688, 295, 264, 5162, 10577, 322, 12554, 307, 341, 1558, 295, 411, 2973, 2144, 295, 2902, 364, 9461, 50656], "temperature": 0.0, "avg_logprob": -0.07296463870262915, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.016139959916472435}, {"id": 201, "seek": 107804, "start": 1083.8799999999999, "end": 1089.96, "text": " kind of like its own kind of like objective and own persona and stuff like that. The most obvious", "tokens": [50656, 733, 295, 411, 1080, 1065, 733, 295, 411, 10024, 293, 1065, 12184, 293, 1507, 411, 300, 13, 440, 881, 6322, 50960], "temperature": 0.0, "avg_logprob": -0.07296463870262915, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.016139959916472435}, {"id": 202, "seek": 107804, "start": 1089.96, "end": 1093.48, "text": " way to do that is just like you encode it in the prompt. You say like, Hey, like, you know, this", "tokens": [50960, 636, 281, 360, 300, 307, 445, 411, 291, 2058, 1429, 309, 294, 264, 12391, 13, 509, 584, 411, 11, 1911, 11, 411, 11, 291, 458, 11, 341, 51136], "temperature": 0.0, "avg_logprob": -0.07296463870262915, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.016139959916472435}, {"id": 203, "seek": 107804, "start": 1093.48, "end": 1098.6, "text": " is your job as an agent, you're supposed to do this, yada, yada, yada. But I think there's some", "tokens": [51136, 307, 428, 1691, 382, 364, 9461, 11, 291, 434, 3442, 281, 360, 341, 11, 288, 1538, 11, 288, 1538, 11, 288, 1538, 13, 583, 286, 519, 456, 311, 512, 51392], "temperature": 0.0, "avg_logprob": -0.07296463870262915, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.016139959916472435}, {"id": 204, "seek": 107804, "start": 1098.6, "end": 1104.12, "text": " really cool work being done on how to kind of like evolve that over time and give agents a sense of", "tokens": [51392, 534, 1627, 589, 885, 1096, 322, 577, 281, 733, 295, 411, 16693, 300, 670, 565, 293, 976, 12554, 257, 2020, 295, 51668], "temperature": 0.0, "avg_logprob": -0.07296463870262915, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.016139959916472435}, {"id": 205, "seek": 110412, "start": 1104.12, "end": 1109.56, "text": " like this long term memory. And one of the papers in particular around generative agents, I think", "tokens": [50364, 411, 341, 938, 1433, 4675, 13, 400, 472, 295, 264, 10577, 294, 1729, 926, 1337, 1166, 12554, 11, 286, 519, 50636], "temperature": 0.0, "avg_logprob": -0.09043868895499937, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0009388184989802539}, {"id": 206, "seek": 110412, "start": 1109.56, "end": 1117.08, "text": " does a really interesting job of diving into this. And I think when a lot of people, the reason this", "tokens": [50636, 775, 257, 534, 1880, 1691, 295, 20241, 666, 341, 13, 400, 286, 519, 562, 257, 688, 295, 561, 11, 264, 1778, 341, 51012], "temperature": 0.0, "avg_logprob": -0.09043868895499937, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0009388184989802539}, {"id": 207, "seek": 110412, "start": 1117.08, "end": 1121.2399999999998, "text": " is here in the agent section is I think when people think of agents, there's the obvious like", "tokens": [51012, 307, 510, 294, 264, 9461, 3541, 307, 286, 519, 562, 561, 519, 295, 12554, 11, 456, 311, 264, 6322, 411, 51220], "temperature": 0.0, "avg_logprob": -0.09043868895499937, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0009388184989802539}, {"id": 208, "seek": 110412, "start": 1121.2399999999998, "end": 1125.8799999999999, "text": " kind of like tool usage deciding what to do. But I think agents is also starting to take on this", "tokens": [51220, 733, 295, 411, 2290, 14924, 17990, 437, 281, 360, 13, 583, 286, 519, 12554, 307, 611, 2891, 281, 747, 322, 341, 51452], "temperature": 0.0, "avg_logprob": -0.09043868895499937, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0009388184989802539}, {"id": 209, "seek": 112588, "start": 1125.88, "end": 1134.7600000000002, "text": " concept of some kind of like more encapsulated kind of like program that that adapts over time", "tokens": [50364, 3410, 295, 512, 733, 295, 411, 544, 38745, 6987, 733, 295, 411, 1461, 300, 300, 23169, 1373, 670, 565, 50808], "temperature": 0.0, "avg_logprob": -0.09619728560300217, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.01769188977777958}, {"id": 210, "seek": 112588, "start": 1134.7600000000002, "end": 1141.5600000000002, "text": " and memory is a big part of that. And so I think memories is there's a lot to explore here. So", "tokens": [50808, 293, 4675, 307, 257, 955, 644, 295, 300, 13, 400, 370, 286, 519, 8495, 307, 456, 311, 257, 688, 281, 6839, 510, 13, 407, 51148], "temperature": 0.0, "avg_logprob": -0.09619728560300217, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.01769188977777958}, {"id": 211, "seek": 112588, "start": 1141.5600000000002, "end": 1147.64, "text": " that's why this is a bit of an outlier slide. Okay, I wanted to chat very quickly about four", "tokens": [51148, 300, 311, 983, 341, 307, 257, 857, 295, 364, 484, 2753, 4137, 13, 1033, 11, 286, 1415, 281, 5081, 588, 2661, 466, 1451, 51452], "temperature": 0.0, "avg_logprob": -0.09619728560300217, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.01769188977777958}, {"id": 212, "seek": 112588, "start": 1148.2800000000002, "end": 1154.6000000000001, "text": " projects that that came out in the past two, three weeks, specifically how they relate, build upon,", "tokens": [51484, 4455, 300, 300, 1361, 484, 294, 264, 1791, 732, 11, 1045, 3259, 11, 4682, 577, 436, 10961, 11, 1322, 3564, 11, 51800], "temperature": 0.0, "avg_logprob": -0.09619728560300217, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.01769188977777958}, {"id": 213, "seek": 115460, "start": 1154.6, "end": 1161.08, "text": " improve upon this side, the react style agent that has been around for a while. First up is auto", "tokens": [50364, 3470, 3564, 341, 1252, 11, 264, 4515, 3758, 9461, 300, 575, 668, 926, 337, 257, 1339, 13, 2386, 493, 307, 8399, 50688], "temperature": 0.0, "avg_logprob": -0.09743229866027832, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.0007548338617198169}, {"id": 214, "seek": 115460, "start": 1161.08, "end": 1172.9199999999998, "text": " GPT, which I'm assuming most people have heard of. There we go. All right. So auto GPT, the one of", "tokens": [50688, 26039, 51, 11, 597, 286, 478, 11926, 881, 561, 362, 2198, 295, 13, 821, 321, 352, 13, 1057, 558, 13, 407, 8399, 26039, 51, 11, 264, 472, 295, 51280], "temperature": 0.0, "avg_logprob": -0.09743229866027832, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.0007548338617198169}, {"id": 215, "seek": 115460, "start": 1172.9199999999998, "end": 1178.36, "text": " the main differences between this and the react style agents is just the objective of what it's", "tokens": [51280, 264, 2135, 7300, 1296, 341, 293, 264, 4515, 3758, 12554, 307, 445, 264, 10024, 295, 437, 309, 311, 51552], "temperature": 0.0, "avg_logprob": -0.09743229866027832, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.0007548338617198169}, {"id": 216, "seek": 115460, "start": 1178.36, "end": 1183.48, "text": " trying to solve auto GPT. A lot of the initial goals are like, you know, improve or increase my", "tokens": [51552, 1382, 281, 5039, 8399, 26039, 51, 13, 316, 688, 295, 264, 5883, 5493, 366, 411, 11, 291, 458, 11, 3470, 420, 3488, 452, 51808], "temperature": 0.0, "avg_logprob": -0.09743229866027832, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.0007548338617198169}, {"id": 217, "seek": 118348, "start": 1183.48, "end": 1187.64, "text": " Twitter following or something like that very kind of like open ended broad long running goals,", "tokens": [50364, 5794, 3480, 420, 746, 411, 300, 588, 733, 295, 411, 1269, 4590, 4152, 938, 2614, 5493, 11, 50572], "temperature": 0.0, "avg_logprob": -0.0822751084152533, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.0003459094150457531}, {"id": 218, "seek": 118348, "start": 1187.64, "end": 1190.84, "text": " react on the other hand was designed and benchmarked on more kind of like", "tokens": [50572, 4515, 322, 264, 661, 1011, 390, 4761, 293, 18927, 292, 322, 544, 733, 295, 411, 50732], "temperature": 0.0, "avg_logprob": -0.0822751084152533, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.0003459094150457531}, {"id": 219, "seek": 118348, "start": 1192.84, "end": 1199.72, "text": " short lived kind of like really immediately quantifiable or more immediately quantifiable", "tokens": [50832, 2099, 5152, 733, 295, 411, 534, 4258, 4426, 30876, 420, 544, 4258, 4426, 30876, 51176], "temperature": 0.0, "avg_logprob": -0.0822751084152533, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.0003459094150457531}, {"id": 220, "seek": 118348, "start": 1199.72, "end": 1204.52, "text": " goals. And so as a result, one of the things that auto GPT introduced is this idea of long", "tokens": [51176, 5493, 13, 400, 370, 382, 257, 1874, 11, 472, 295, 264, 721, 300, 8399, 26039, 51, 7268, 307, 341, 1558, 295, 938, 51416], "temperature": 0.0, "avg_logprob": -0.0822751084152533, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.0003459094150457531}, {"id": 221, "seek": 118348, "start": 1204.52, "end": 1209.24, "text": " term memory between the agent and tools interactions and using a retriever vector store for that,", "tokens": [51416, 1433, 4675, 1296, 264, 9461, 293, 3873, 13280, 293, 1228, 257, 19817, 331, 8062, 3531, 337, 300, 11, 51652], "temperature": 0.0, "avg_logprob": -0.0822751084152533, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.0003459094150457531}, {"id": 222, "seek": 120924, "start": 1209.24, "end": 1214.44, "text": " which becomes necessary because now you have this doing like 20 or 30 kind of like steps and it's", "tokens": [50364, 597, 3643, 4818, 570, 586, 291, 362, 341, 884, 411, 945, 420, 2217, 733, 295, 411, 4439, 293, 309, 311, 50624], "temperature": 0.0, "avg_logprob": -0.08579127852981155, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0002452895569149405}, {"id": 223, "seek": 120924, "start": 1214.44, "end": 1218.68, "text": " this really long running project. And so it's something that react just didn't need, but due", "tokens": [50624, 341, 534, 938, 2614, 1716, 13, 400, 370, 309, 311, 746, 300, 4515, 445, 994, 380, 643, 11, 457, 3462, 50836], "temperature": 0.0, "avg_logprob": -0.08579127852981155, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0002452895569149405}, {"id": 224, "seek": 120924, "start": 1218.68, "end": 1225.32, "text": " to the change in objectives, auto GPT kind of had to introduce baby AGI is another popular one.", "tokens": [50836, 281, 264, 1319, 294, 15961, 11, 8399, 26039, 51, 733, 295, 632, 281, 5366, 3186, 316, 26252, 307, 1071, 3743, 472, 13, 51168], "temperature": 0.0, "avg_logprob": -0.08579127852981155, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0002452895569149405}, {"id": 225, "seek": 120924, "start": 1225.32, "end": 1230.1200000000001, "text": " It also has this idea of long term memory for the agent tool interactions. And this is the project", "tokens": [51168, 467, 611, 575, 341, 1558, 295, 938, 1433, 4675, 337, 264, 9461, 2290, 13280, 13, 400, 341, 307, 264, 1716, 51408], "temperature": 0.0, "avg_logprob": -0.08579127852981155, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0002452895569149405}, {"id": 226, "seek": 120924, "start": 1230.1200000000001, "end": 1233.88, "text": " that introduced separate kind of like planning and execution steps, which I think is a really", "tokens": [51408, 300, 7268, 4994, 733, 295, 411, 5038, 293, 15058, 4439, 11, 597, 286, 519, 307, 257, 534, 51596], "temperature": 0.0, "avg_logprob": -0.08579127852981155, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0002452895569149405}, {"id": 227, "seek": 123388, "start": 1233.88, "end": 1240.8400000000001, "text": " interesting idea to improve upon some of the long running objectives. And so specifically,", "tokens": [50364, 1880, 1558, 281, 3470, 3564, 512, 295, 264, 938, 2614, 15961, 13, 400, 370, 4682, 11, 50712], "temperature": 0.0, "avg_logprob": -0.0985953392238792, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.00831015408039093}, {"id": 228, "seek": 123388, "start": 1240.8400000000001, "end": 1245.0800000000002, "text": " it comes up with tasks, it then takes the first tasks, it then thinks about how to do that,", "tokens": [50712, 309, 1487, 493, 365, 9608, 11, 309, 550, 2516, 264, 700, 9608, 11, 309, 550, 7309, 466, 577, 281, 360, 300, 11, 50924], "temperature": 0.0, "avg_logprob": -0.0985953392238792, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.00831015408039093}, {"id": 229, "seek": 123388, "start": 1245.0800000000002, "end": 1250.2, "text": " which usually involves actually baby AGI initially didn't have any tools. So it kind of just like", "tokens": [50924, 597, 2673, 11626, 767, 3186, 316, 26252, 9105, 994, 380, 362, 604, 3873, 13, 407, 309, 733, 295, 445, 411, 51180], "temperature": 0.0, "avg_logprob": -0.0985953392238792, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.00831015408039093}, {"id": 230, "seek": 123388, "start": 1250.2, "end": 1255.64, "text": " made stuff up. I think that I think they're giving it tools now so it can actually actually execute", "tokens": [51180, 1027, 1507, 493, 13, 286, 519, 300, 286, 519, 436, 434, 2902, 309, 3873, 586, 370, 309, 393, 767, 767, 14483, 51452], "temperature": 0.0, "avg_logprob": -0.0985953392238792, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.00831015408039093}, {"id": 231, "seek": 123388, "start": 1255.64, "end": 1260.1200000000001, "text": " those things. But the idea of separating the planning execution steps is I think that's a", "tokens": [51452, 729, 721, 13, 583, 264, 1558, 295, 29279, 264, 5038, 15058, 4439, 307, 286, 519, 300, 311, 257, 51676], "temperature": 0.0, "avg_logprob": -0.0985953392238792, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.00831015408039093}, {"id": 232, "seek": 126012, "start": 1260.12, "end": 1265.7199999999998, "text": " really interesting idea that might help with some of the reliability and focus issues of longer term", "tokens": [50364, 534, 1880, 1558, 300, 1062, 854, 365, 512, 295, 264, 24550, 293, 1879, 2663, 295, 2854, 1433, 50644], "temperature": 0.0, "avg_logprob": -0.08719848082946227, "compression_ratio": 1.6840277777777777, "no_speech_prob": 0.0005190849187783897}, {"id": 233, "seek": 126012, "start": 1266.4399999999998, "end": 1273.8, "text": " agents. Camel is another paper that came out. The main novel thing here was they put two agents", "tokens": [50680, 12554, 13, 6886, 338, 307, 1071, 3035, 300, 1361, 484, 13, 440, 2135, 7613, 551, 510, 390, 436, 829, 732, 12554, 51048], "temperature": 0.0, "avg_logprob": -0.08719848082946227, "compression_ratio": 1.6840277777777777, "no_speech_prob": 0.0005190849187783897}, {"id": 234, "seek": 126012, "start": 1274.4399999999998, "end": 1278.1999999999998, "text": " in a simulation environment, which in this case, because it was just two was just a chat room", "tokens": [51080, 294, 257, 16575, 2823, 11, 597, 294, 341, 1389, 11, 570, 309, 390, 445, 732, 390, 445, 257, 5081, 1808, 51268], "temperature": 0.0, "avg_logprob": -0.08719848082946227, "compression_ratio": 1.6840277777777777, "no_speech_prob": 0.0005190849187783897}, {"id": 235, "seek": 126012, "start": 1278.1999999999998, "end": 1284.04, "text": " and had them interact with each other. And so the agents themselves, I think, were basically just", "tokens": [51268, 293, 632, 552, 4648, 365, 1184, 661, 13, 400, 370, 264, 12554, 2969, 11, 286, 519, 11, 645, 1936, 445, 51560], "temperature": 0.0, "avg_logprob": -0.08719848082946227, "compression_ratio": 1.6840277777777777, "no_speech_prob": 0.0005190849187783897}, {"id": 236, "seek": 126012, "start": 1284.04, "end": 1288.1999999999998, "text": " kind of like prompted language models. So I don't even think they were hooked up with tools. But", "tokens": [51560, 733, 295, 411, 31042, 2856, 5245, 13, 407, 286, 500, 380, 754, 519, 436, 645, 20410, 493, 365, 3873, 13, 583, 51768], "temperature": 0.0, "avg_logprob": -0.08719848082946227, "compression_ratio": 1.6840277777777777, "no_speech_prob": 0.0005190849187783897}, {"id": 237, "seek": 128820, "start": 1288.2, "end": 1293.24, "text": " going back to this idea of kind of like memory and personalization, when people kind of like talk", "tokens": [50364, 516, 646, 281, 341, 1558, 295, 733, 295, 411, 4675, 293, 2973, 2144, 11, 562, 561, 733, 295, 411, 751, 50616], "temperature": 0.0, "avg_logprob": -0.07896315199988228, "compression_ratio": 1.8671875, "no_speech_prob": 0.00012726294517051429}, {"id": 238, "seek": 128820, "start": 1293.24, "end": 1297.72, "text": " about agents, that is part of what they're talking about. And so I think like the camel paper in my", "tokens": [50616, 466, 12554, 11, 300, 307, 644, 295, 437, 436, 434, 1417, 466, 13, 400, 370, 286, 519, 411, 264, 37755, 3035, 294, 452, 50840], "temperature": 0.0, "avg_logprob": -0.07896315199988228, "compression_ratio": 1.8671875, "no_speech_prob": 0.00012726294517051429}, {"id": 239, "seek": 128820, "start": 1297.72, "end": 1304.92, "text": " mind, the main thing is this idea of simulation environment. There's maybe like two reasons", "tokens": [50840, 1575, 11, 264, 2135, 551, 307, 341, 1558, 295, 16575, 2823, 13, 821, 311, 1310, 411, 732, 4112, 51200], "temperature": 0.0, "avg_logprob": -0.07896315199988228, "compression_ratio": 1.8671875, "no_speech_prob": 0.00012726294517051429}, {"id": 240, "seek": 128820, "start": 1304.92, "end": 1310.44, "text": " you might want to do this and have a simulation environment. One is kind of like practically", "tokens": [51200, 291, 1062, 528, 281, 360, 341, 293, 362, 257, 16575, 2823, 13, 1485, 307, 733, 295, 411, 15667, 51476], "temperature": 0.0, "avg_logprob": -0.07896315199988228, "compression_ratio": 1.8671875, "no_speech_prob": 0.00012726294517051429}, {"id": 241, "seek": 128820, "start": 1310.44, "end": 1315.16, "text": " to maybe like evaluate an agent if you're kind of like testing out an agent and you want to see", "tokens": [51476, 281, 1310, 411, 13059, 364, 9461, 498, 291, 434, 733, 295, 411, 4997, 484, 364, 9461, 293, 291, 528, 281, 536, 51712], "temperature": 0.0, "avg_logprob": -0.07896315199988228, "compression_ratio": 1.8671875, "no_speech_prob": 0.00012726294517051429}, {"id": 242, "seek": 131516, "start": 1315.16, "end": 1319.0, "text": " how it's interacting. And for whatever reason, you don't want to test it out yourself. So you", "tokens": [50364, 577, 309, 311, 18017, 13, 400, 337, 2035, 1778, 11, 291, 500, 380, 528, 281, 1500, 309, 484, 1803, 13, 407, 291, 50556], "temperature": 0.0, "avg_logprob": -0.08057339451894038, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.0010644096182659268}, {"id": 243, "seek": 131516, "start": 1319.0, "end": 1323.5600000000002, "text": " put two of them and you kind of like make sure they don't go off the rails or something like that.", "tokens": [50556, 829, 732, 295, 552, 293, 291, 733, 295, 411, 652, 988, 436, 500, 380, 352, 766, 264, 27649, 420, 746, 411, 300, 13, 50784], "temperature": 0.0, "avg_logprob": -0.08057339451894038, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.0010644096182659268}, {"id": 244, "seek": 131516, "start": 1324.28, "end": 1330.28, "text": " Another one is just kind of entertainment purposes. So there are a lot of examples of this by people.", "tokens": [50820, 3996, 472, 307, 445, 733, 295, 12393, 9932, 13, 407, 456, 366, 257, 688, 295, 5110, 295, 341, 538, 561, 13, 51120], "temperature": 0.0, "avg_logprob": -0.08057339451894038, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.0010644096182659268}, {"id": 245, "seek": 131516, "start": 1330.28, "end": 1334.28, "text": " I think there was one with like a VC and a founder and that had them chatting with each other and", "tokens": [51120, 286, 519, 456, 390, 472, 365, 411, 257, 41922, 293, 257, 14917, 293, 300, 632, 552, 24654, 365, 1184, 661, 293, 51320], "temperature": 0.0, "avg_logprob": -0.08057339451894038, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.0010644096182659268}, {"id": 246, "seek": 131516, "start": 1334.28, "end": 1339.4, "text": " kind of like solving stuff there. So this is a little bit entertainment, a little bit practical.", "tokens": [51320, 733, 295, 411, 12606, 1507, 456, 13, 407, 341, 307, 257, 707, 857, 12393, 11, 257, 707, 857, 8496, 13, 51576], "temperature": 0.0, "avg_logprob": -0.08057339451894038, "compression_ratio": 1.7977941176470589, "no_speech_prob": 0.0010644096182659268}, {"id": 247, "seek": 133940, "start": 1340.2800000000002, "end": 1345.0800000000002, "text": " Generative agents was another paper that came out. I think this was maybe like a week and a", "tokens": [50408, 15409, 1166, 12554, 390, 1071, 3035, 300, 1361, 484, 13, 286, 519, 341, 390, 1310, 411, 257, 1243, 293, 257, 50648], "temperature": 0.0, "avg_logprob": -0.07619855138990614, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.005297026131302118}, {"id": 248, "seek": 133940, "start": 1345.0800000000002, "end": 1350.76, "text": " half ago, so very recent. It also had a simulation environment aspect. It was more complex. So I", "tokens": [50648, 1922, 2057, 11, 370, 588, 5162, 13, 467, 611, 632, 257, 16575, 2823, 4171, 13, 467, 390, 544, 3997, 13, 407, 286, 50932], "temperature": 0.0, "avg_logprob": -0.07619855138990614, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.005297026131302118}, {"id": 249, "seek": 133940, "start": 1350.76, "end": 1354.8400000000001, "text": " think they had like 25 different agents and kind of like a Sims-like world interacting with each", "tokens": [50932, 519, 436, 632, 411, 3552, 819, 12554, 293, 733, 295, 411, 257, 33289, 12, 4092, 1002, 18017, 365, 1184, 51136], "temperature": 0.0, "avg_logprob": -0.07619855138990614, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.005297026131302118}, {"id": 250, "seek": 133940, "start": 1354.8400000000001, "end": 1362.6000000000001, "text": " other. So a much more complex environment setup. And then they also did some really cool stuff", "tokens": [51136, 661, 13, 407, 257, 709, 544, 3997, 2823, 8657, 13, 400, 550, 436, 611, 630, 512, 534, 1627, 1507, 51524], "temperature": 0.0, "avg_logprob": -0.07619855138990614, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.005297026131302118}, {"id": 251, "seek": 136260, "start": 1362.6799999999998, "end": 1371.7199999999998, "text": " around memory and reflection. So memory refers to basically remembering previous things that", "tokens": [50368, 926, 4675, 293, 12914, 13, 407, 4675, 14942, 281, 1936, 20719, 3894, 721, 300, 50820], "temperature": 0.0, "avg_logprob": -0.07692897553537406, "compression_ratio": 1.9170124481327802, "no_speech_prob": 0.03018363006412983}, {"id": 252, "seek": 136260, "start": 1371.7199999999998, "end": 1376.1999999999998, "text": " happened in the world. So basically in the simulation environment, they had kind of like", "tokens": [50820, 2011, 294, 264, 1002, 13, 407, 1936, 294, 264, 16575, 2823, 11, 436, 632, 733, 295, 411, 51044], "temperature": 0.0, "avg_logprob": -0.07692897553537406, "compression_ratio": 1.9170124481327802, "no_speech_prob": 0.03018363006412983}, {"id": 253, "seek": 136260, "start": 1376.1999999999998, "end": 1380.84, "text": " things that happened. Then they had the agents decide what to do, take actions, observe kind of", "tokens": [51044, 721, 300, 2011, 13, 1396, 436, 632, 264, 12554, 4536, 437, 281, 360, 11, 747, 5909, 11, 11441, 733, 295, 51276], "temperature": 0.0, "avg_logprob": -0.07692897553537406, "compression_ratio": 1.9170124481327802, "no_speech_prob": 0.03018363006412983}, {"id": 254, "seek": 136260, "start": 1380.84, "end": 1385.32, "text": " like the results of those actions, observe more things that came in. And so all of this", "tokens": [51276, 411, 264, 3542, 295, 729, 5909, 11, 11441, 544, 721, 300, 1361, 294, 13, 400, 370, 439, 295, 341, 51500], "temperature": 0.0, "avg_logprob": -0.07692897553537406, "compression_ratio": 1.9170124481327802, "no_speech_prob": 0.03018363006412983}, {"id": 255, "seek": 136260, "start": 1385.32, "end": 1390.12, "text": " is encapsulated in the idea of memory. And then you fetch things from this memory to inform kind", "tokens": [51500, 307, 38745, 6987, 294, 264, 1558, 295, 4675, 13, 400, 550, 291, 23673, 721, 490, 341, 4675, 281, 1356, 733, 51740], "temperature": 0.0, "avg_logprob": -0.07692897553537406, "compression_ratio": 1.9170124481327802, "no_speech_prob": 0.03018363006412983}, {"id": 256, "seek": 139012, "start": 1390.1999999999998, "end": 1398.28, "text": " of like their actions in next time sequences. So there was three kind of like main components", "tokens": [50368, 295, 411, 641, 5909, 294, 958, 565, 22978, 13, 407, 456, 390, 1045, 733, 295, 411, 2135, 6677, 50772], "temperature": 0.0, "avg_logprob": -0.1331922867718865, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0008821299416013062}, {"id": 257, "seek": 139012, "start": 1398.28, "end": 1402.12, "text": " to this memory retrieval thing. They had a time-weighted component which basically fetched", "tokens": [50772, 281, 341, 4675, 19817, 3337, 551, 13, 814, 632, 257, 565, 12, 12329, 292, 6542, 597, 1936, 23673, 292, 50964], "temperature": 0.0, "avg_logprob": -0.1331922867718865, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0008821299416013062}, {"id": 258, "seek": 139012, "start": 1402.12, "end": 1408.6799999999998, "text": " more recent memories. They had an important weighted piece which fetched more like important", "tokens": [50964, 544, 5162, 8495, 13, 814, 632, 364, 1021, 32807, 2522, 597, 23673, 292, 544, 411, 1021, 51292], "temperature": 0.0, "avg_logprob": -0.1331922867718865, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0008821299416013062}, {"id": 259, "seek": 139012, "start": 1408.6799999999998, "end": 1416.9199999999998, "text": " information. So trivial things like I forget what I had for breakfast today, but I don't know what's", "tokens": [51292, 1589, 13, 407, 26703, 721, 411, 286, 2870, 437, 286, 632, 337, 8201, 965, 11, 457, 286, 500, 380, 458, 437, 311, 51704], "temperature": 0.0, "avg_logprob": -0.1331922867718865, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0008821299416013062}, {"id": 260, "seek": 141692, "start": 1416.92, "end": 1421.0800000000002, "text": " something that's really, but I remember meeting Charles way back when, right? So there's different", "tokens": [50364, 746, 300, 311, 534, 11, 457, 286, 1604, 3440, 10523, 636, 646, 562, 11, 558, 30, 407, 456, 311, 819, 50572], "temperature": 0.0, "avg_logprob": -0.104376882314682, "compression_ratio": 1.8080495356037152, "no_speech_prob": 0.0038203815929591656}, {"id": 261, "seek": 141692, "start": 1421.0800000000002, "end": 1424.68, "text": " levels of importance there that get subscribed to events. And so you want to fetch events that are", "tokens": [50572, 4358, 295, 7379, 456, 300, 483, 16665, 281, 3931, 13, 400, 370, 291, 528, 281, 23673, 3931, 300, 366, 50752], "temperature": 0.0, "avg_logprob": -0.104376882314682, "compression_ratio": 1.8080495356037152, "no_speech_prob": 0.0038203815929591656}, {"id": 262, "seek": 141692, "start": 1424.68, "end": 1430.04, "text": " kind of like more bigger in importance. And then they had the typical kind of like relevancy", "tokens": [50752, 733, 295, 411, 544, 3801, 294, 7379, 13, 400, 550, 436, 632, 264, 7476, 733, 295, 411, 25916, 6717, 51020], "temperature": 0.0, "avg_logprob": -0.104376882314682, "compression_ratio": 1.8080495356037152, "no_speech_prob": 0.0038203815929591656}, {"id": 263, "seek": 141692, "start": 1430.04, "end": 1433.72, "text": " weighted things. So depending on what situation you're in, you want to remember events that are", "tokens": [51020, 32807, 721, 13, 407, 5413, 322, 437, 2590, 291, 434, 294, 11, 291, 528, 281, 1604, 3931, 300, 366, 51204], "temperature": 0.0, "avg_logprob": -0.104376882314682, "compression_ratio": 1.8080495356037152, "no_speech_prob": 0.0038203815929591656}, {"id": 264, "seek": 141692, "start": 1433.72, "end": 1438.68, "text": " relevant for that. Then they also introduced a really interesting reflection step, which basically", "tokens": [51204, 7340, 337, 300, 13, 1396, 436, 611, 7268, 257, 534, 1880, 12914, 1823, 11, 597, 1936, 51452], "temperature": 0.0, "avg_logprob": -0.104376882314682, "compression_ratio": 1.8080495356037152, "no_speech_prob": 0.0038203815929591656}, {"id": 265, "seek": 141692, "start": 1439.8000000000002, "end": 1445.0800000000002, "text": " after like, I think they, I think it was like 20 different steps or something happened, they would", "tokens": [51508, 934, 411, 11, 286, 519, 436, 11, 286, 519, 309, 390, 411, 945, 819, 4439, 420, 746, 2011, 11, 436, 576, 51772], "temperature": 0.0, "avg_logprob": -0.104376882314682, "compression_ratio": 1.8080495356037152, "no_speech_prob": 0.0038203815929591656}, {"id": 266, "seek": 144508, "start": 1445.08, "end": 1451.0, "text": " reflect on those things and kind of like update different states of the world. And so I think this", "tokens": [50364, 5031, 322, 729, 721, 293, 733, 295, 411, 5623, 819, 4368, 295, 264, 1002, 13, 400, 370, 286, 519, 341, 50660], "temperature": 0.0, "avg_logprob": -0.07689806679698909, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0007085460820235312}, {"id": 267, "seek": 144508, "start": 1451.0, "end": 1455.8799999999999, "text": " is, I've been thinking about this a bit because I think this idea of like reflecting on recent", "tokens": [50660, 307, 11, 286, 600, 668, 1953, 466, 341, 257, 857, 570, 286, 519, 341, 1558, 295, 411, 23543, 322, 5162, 50904], "temperature": 0.0, "avg_logprob": -0.07689806679698909, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0007085460820235312}, {"id": 268, "seek": 144508, "start": 1455.8799999999999, "end": 1461.96, "text": " things and then updating state is maybe like a generalization that can be kind of like applied", "tokens": [50904, 721, 293, 550, 25113, 1785, 307, 1310, 411, 257, 2674, 2144, 300, 393, 312, 733, 295, 411, 6456, 51208], "temperature": 0.0, "avg_logprob": -0.07689806679698909, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0007085460820235312}, {"id": 269, "seek": 144508, "start": 1461.96, "end": 1466.9199999999998, "text": " to a bunch of different things. So some of the other memory types that we have in Lang chain", "tokens": [51208, 281, 257, 3840, 295, 819, 721, 13, 407, 512, 295, 264, 661, 4675, 3467, 300, 321, 362, 294, 13313, 5021, 51456], "temperature": 0.0, "avg_logprob": -0.07689806679698909, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0007085460820235312}, {"id": 270, "seek": 144508, "start": 1466.9199999999998, "end": 1472.04, "text": " are we have like an entity memory type, which basically based on conversation kind of like", "tokens": [51456, 366, 321, 362, 411, 364, 13977, 4675, 2010, 11, 597, 1936, 2361, 322, 3761, 733, 295, 411, 51712], "temperature": 0.0, "avg_logprob": -0.07689806679698909, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.0007085460820235312}, {"id": 271, "seek": 147204, "start": 1472.04, "end": 1476.44, "text": " extracts relevant entities and then constructs some type of graph and updates that there's a more", "tokens": [50364, 8947, 82, 7340, 16667, 293, 550, 7690, 82, 512, 2010, 295, 4295, 293, 9205, 300, 456, 311, 257, 544, 50584], "temperature": 0.0, "avg_logprob": -0.0807893541124132, "compression_ratio": 1.7859778597785978, "no_speech_prob": 0.00039189704693853855}, {"id": 272, "seek": 147204, "start": 1476.44, "end": 1481.3999999999999, "text": " general kind of like knowledge graph version of that as well. And then we also have kind of like a", "tokens": [50584, 2674, 733, 295, 411, 3601, 4295, 3037, 295, 300, 382, 731, 13, 400, 550, 321, 611, 362, 733, 295, 411, 257, 50832], "temperature": 0.0, "avg_logprob": -0.0807893541124132, "compression_ratio": 1.7859778597785978, "no_speech_prob": 0.00039189704693853855}, {"id": 273, "seek": 147204, "start": 1481.3999999999999, "end": 1488.12, "text": " summary conversation memory, which based on the conversation updates a running summary. So you", "tokens": [50832, 12691, 3761, 4675, 11, 597, 2361, 322, 264, 3761, 9205, 257, 2614, 12691, 13, 407, 291, 51168], "temperature": 0.0, "avg_logprob": -0.0807893541124132, "compression_ratio": 1.7859778597785978, "no_speech_prob": 0.00039189704693853855}, {"id": 274, "seek": 147204, "start": 1488.12, "end": 1491.6399999999999, "text": " can get around some of the context window lengths. And so I think if you look at it sort of through", "tokens": [51168, 393, 483, 926, 512, 295, 264, 4319, 4910, 26329, 13, 400, 370, 286, 519, 498, 291, 574, 412, 309, 1333, 295, 807, 51344], "temperature": 0.0, "avg_logprob": -0.0807893541124132, "compression_ratio": 1.7859778597785978, "no_speech_prob": 0.00039189704693853855}, {"id": 275, "seek": 147204, "start": 1491.6399999999999, "end": 1496.52, "text": " a certain angle, all of those kind of like relate to this idea of taking recent observations", "tokens": [51344, 257, 1629, 5802, 11, 439, 295, 729, 733, 295, 411, 10961, 281, 341, 1558, 295, 1940, 5162, 18163, 51588], "temperature": 0.0, "avg_logprob": -0.0807893541124132, "compression_ratio": 1.7859778597785978, "no_speech_prob": 0.00039189704693853855}, {"id": 276, "seek": 149652, "start": 1496.6, "end": 1502.12, "text": " and updating some state, whether that state is like a graph or just a piece of text or", "tokens": [50368, 293, 25113, 512, 1785, 11, 1968, 300, 1785, 307, 411, 257, 4295, 420, 445, 257, 2522, 295, 2487, 420, 50644], "temperature": 0.0, "avg_logprob": -0.07914076180293642, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.006092740688472986}, {"id": 277, "seek": 149652, "start": 1502.12, "end": 1506.68, "text": " anything like that. So there's also been some other papers recently that have incorporated", "tokens": [50644, 1340, 411, 300, 13, 407, 456, 311, 611, 668, 512, 661, 10577, 3938, 300, 362, 21654, 50872], "temperature": 0.0, "avg_logprob": -0.07914076180293642, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.006092740688472986}, {"id": 278, "seek": 149652, "start": 1506.68, "end": 1510.92, "text": " this idea of like reflection. I haven't had time to read those as carefully, but I think that's", "tokens": [50872, 341, 1558, 295, 411, 12914, 13, 286, 2378, 380, 632, 565, 281, 1401, 729, 382, 7500, 11, 457, 286, 519, 300, 311, 51084], "temperature": 0.0, "avg_logprob": -0.07914076180293642, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.006092740688472986}, {"id": 279, "seek": 149652, "start": 1512.04, "end": 1515.16, "text": " yeah, I don't know, my personal take is I think that's really interesting and something to keep", "tokens": [51140, 1338, 11, 286, 500, 380, 458, 11, 452, 2973, 747, 307, 286, 519, 300, 311, 534, 1880, 293, 746, 281, 1066, 51296], "temperature": 0.0, "avg_logprob": -0.07914076180293642, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.006092740688472986}, {"id": 280, "seek": 149652, "start": 1515.16, "end": 1522.04, "text": " an eye out for the future. And that's it. I have no idea what time it is because I can't see the", "tokens": [51296, 364, 3313, 484, 337, 264, 2027, 13, 400, 300, 311, 309, 13, 286, 362, 572, 1558, 437, 565, 309, 307, 570, 286, 393, 380, 536, 264, 51640], "temperature": 0.0, "avg_logprob": -0.07914076180293642, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.006092740688472986}, {"id": 281, "seek": 152204, "start": 1522.04, "end": 1535.56, "text": " time, but I'm happy to take questions until Charles kicks me off.", "tokens": [50364, 565, 11, 457, 286, 478, 2055, 281, 747, 1651, 1826, 10523, 21293, 385, 766, 13, 51040], "temperature": 0.0, "avg_logprob": -0.30210063192579484, "compression_ratio": 0.9285714285714286, "no_speech_prob": 0.03498055413365364}], "language": "en"}