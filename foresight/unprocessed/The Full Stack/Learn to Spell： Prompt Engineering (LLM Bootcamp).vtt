WEBVTT

00:00.000 --> 00:10.920
This morning, we kind of covered a lot of the things at a high level, a lot of the foundations

00:10.920 --> 00:11.920
as well.

00:11.920 --> 00:17.960
And now we're going to dive into how to do stuff with language models, the technical

00:17.960 --> 00:22.040
skills you need to get them to do the things that you want them to do.

00:22.040 --> 00:27.160
So first up, I'm going to cover prompt engineering.

00:27.160 --> 00:35.840
And so the like the scope of this lecture is on how to adjust the text that goes into

00:35.840 --> 00:41.520
your language model to get the behavior that you want.

00:41.520 --> 00:45.080
And prompt engineering is the art of designing that text.

00:45.080 --> 00:50.040
This is not where that text comes from or where it goes.

00:50.040 --> 00:55.640
So it's not like anything about the retrieval augmentation that I did in the morning.

00:55.640 --> 01:02.880
It's about things like writing, you know, write a summary of this or like changing around

01:02.880 --> 01:05.000
the text that goes into that language model.

01:05.000 --> 01:10.440
So we'll cover the second lecture this afternoon, we'll cover that latter part.

01:10.440 --> 01:15.080
So we're just thinking about what kind of text do I put into my language model to get

01:15.080 --> 01:20.360
it to form the tasks that I want it to do.

01:21.240 --> 01:25.880
This is for language models, this is kind of replacing a lot of things that you would

01:25.880 --> 01:33.000
otherwise do with training, with fine tuning, with all the approaches that we've taken

01:33.000 --> 01:37.000
for constructing machine learning models in the past.

01:37.000 --> 01:41.120
And it's also, in a sense, the way that we program these language models, so it's sort

01:41.120 --> 01:49.800
of like programming in English instead of programming in Python or Rust or whatever.

01:49.800 --> 01:57.160
So there's just two components to this talk, the simplest agenda of all the talks.

01:57.160 --> 02:02.720
First is some high-level intuitions for prompting, and I'm going to present the idea that prompts

02:02.720 --> 02:05.280
are magic spells.

02:05.280 --> 02:10.760
And then to get a little bit more specific, I'm going to talk about the emerging playbook

02:10.760 --> 02:15.640
for effective prompting, a collection of sort of prompting techniques, ways to get language

02:15.640 --> 02:17.200
models to do what you want.

02:17.200 --> 02:20.000
So what do I mean when I say that prompts are magic spells?

02:20.000 --> 02:26.240
This is not literally true, they are not literally magic, it is linear algebra, which I find

02:26.240 --> 02:30.520
delightful and beautiful, but which is not actually magic.

02:30.520 --> 02:35.720
So there's not little, like, wizards inside of language models, there's not brains inside

02:35.720 --> 02:38.400
of language models, either really.

02:38.400 --> 02:44.840
Language models are just statistical models of text, like in some sense, you can statistically

02:44.840 --> 02:50.200
model your data with a bell curve, and that's a statistical model of your data, and a language

02:50.200 --> 02:56.840
model is a statistical model of data, and it just happens to be a statistical model of

02:56.840 --> 03:05.240
a more complicated statistical model of more complicated data.

03:05.240 --> 03:09.520
So there's nothing really magic about that.

03:10.200 --> 03:15.760
What do I mean when I say that it's a statistical model roughly of text?

03:15.760 --> 03:19.560
Roughly what that means is that when the model is trained, you can take some list of text

03:19.560 --> 03:24.200
that you pulled from somewhere, let's say some text you sampled from the internet, and

03:24.200 --> 03:29.920
then go through the tokens in that text, go through the pieces of that text, pass them

03:29.920 --> 03:35.000
through a model, and it'll give out a probability of what the next word is going to be.

03:35.000 --> 03:38.720
So this is called an autoregressive model.

03:38.720 --> 03:45.520
It's just a term of art, autoregressive, like predicting on itself.

03:45.520 --> 03:51.640
So we're going through the text, and we keep adding stuff to what we put into the model,

03:51.640 --> 03:58.320
generating probabilities for them, for what the next token is going to be.

03:58.320 --> 04:03.360
So we have a dictionary that for every possible token, for every possible thing that could

04:03.360 --> 04:08.560
come next, we have a probability or a law of probability for it.

04:08.560 --> 04:18.400
And then if we do that across a lot of text of sufficient length, then the language model

04:18.400 --> 04:22.760
being a model of text means that the probability it assigned to all the text that it saw would

04:22.760 --> 04:23.760
be high.

04:23.760 --> 04:30.920
So you start off with random weights, a random model, it has no idea about text, it assigns

04:30.920 --> 04:35.320
a low probability to basically everything it sees to eventually a model that assigns

04:35.320 --> 04:39.920
a high probability to pretty much any text that you can imagine like drawing from the

04:39.920 --> 04:43.680
internet or writing yourself.

04:43.680 --> 04:48.520
And so this is literally true, and this is sort of the place where you want to eventually

04:48.520 --> 04:51.840
compile down your intuitions and your understanding of models.

04:51.840 --> 04:56.640
You want to eventually get to the point where you are thinking in terms of how does this

04:56.640 --> 05:03.560
arise from statistical modeling, but it also can give you really bad intuitions to think

05:03.560 --> 05:05.800
at that level.

05:05.800 --> 05:10.760
And when you say that these are statistical models, that they learn patterns or that language

05:10.760 --> 05:16.480
models are statistical pattern matchers or static parrots, you are drawing on intuition

05:16.480 --> 05:21.360
from things like other kinds of statistical models that you have seen, so maybe statistical

05:21.360 --> 05:28.040
models of data like linear regression or Gaussian distributions, so these are very simple objects.

05:28.040 --> 05:32.400
Or you are drawing inspiration from other kinds of models of text that you have seen

05:32.400 --> 05:38.200
like Google's autocomplete or the autocomplete function on your phone, and we are well aware

05:38.200 --> 05:45.040
that these things are very dumb and not very capable and that they just pick up on surface

05:45.040 --> 05:48.880
level patterns.

05:48.880 --> 05:55.160
Whereas these language models, they have learned so much about text that it is extremely difficult

05:55.160 --> 06:00.480
to think of it as some kind of statistics, the way people normally think about probability

06:00.480 --> 06:01.480
and statistics.

06:01.840 --> 06:10.640
So I just picked one random example that I felt demonstrates this, Bing Chat can take

06:10.640 --> 06:18.640
in SVG files as text and then describe the content of that SVG file for you as an image,

06:18.640 --> 06:25.120
and very few people's intuition for what a model of text is would include that.

06:25.120 --> 06:32.640
So there is some better intuitions that come from the world of statistics of probability

06:32.640 --> 06:38.480
that are a little bit better, so probabilistic programs is probably one of the better intuitions

06:38.480 --> 06:40.200
from that world.

06:40.200 --> 06:45.360
So the idea is we often think of really simple statistical models, we think of them as being

06:45.360 --> 06:51.800
like represented by equations or the kinds of things that you can manipulate in a probability

06:51.920 --> 06:57.960
class, but a lot of complicated statistical models that people use today, even like really

06:57.960 --> 07:04.960
complicated hierarchical linear regressions, can be better thought of as programs that

07:04.960 --> 07:09.520
operate on random data, as programs that manipulate random variables.

07:09.520 --> 07:14.600
And so you can write things that you might do with a language model, like take a question,

07:14.600 --> 07:18.720
and rather than having it just directly answered, having it think of what the answer should

07:18.720 --> 07:23.400
be, like maybe like brainstorm a little bit, and then take that brainstorm and turn it

07:23.400 --> 07:24.960
into an answer.

07:24.960 --> 07:29.640
So that's what this little like Python program up here shows.

07:29.640 --> 07:36.240
Take a question, generate a thought, and then generate an answer.

07:36.240 --> 07:42.600
And because language models are probabilistic, you can actually literally sample from them,

07:42.600 --> 07:48.360
you can actually draw different possibilities each time if you wish.

07:49.320 --> 07:55.920
And this probabilistic program, all probabilistic programs can be represented with these graphical

07:55.920 --> 08:01.720
models that like, so this comes from sort of a branch of machine learning that's been

08:01.720 --> 08:07.920
eclipsed by the rise of deep learning and LLNs in particular.

08:07.920 --> 08:12.440
But this gives you like the sort of best way to think about just how complicated can you

08:12.440 --> 08:18.440
get when thinking about like a model of text, a probabilistic model of text.

08:18.440 --> 08:22.840
And so if you're interested in that, like that kind of direction, if you have that background

08:22.840 --> 08:27.360
or have that interest, the language model cascades by Dohan et al, like really dives

08:27.360 --> 08:31.480
into detail and shows how you can explain a bunch of like prompting tricks and other

08:31.480 --> 08:35.760
things that people have done in terms of probabilistic programs.

08:35.760 --> 08:38.160
That's a little bit to arcane.

08:38.160 --> 08:43.480
So what's something that is like maybe a little bit easier to understand.

08:43.480 --> 08:48.480
So I'm going to draw inspiration from Arthur C. Clark's laws of technology here.

08:48.480 --> 08:53.880
And any sufficiently advanced technology like, I don't know, machine that makes your voice

08:53.880 --> 09:00.960
really loud in a room or a machine that can show you your mother's face across the country

09:00.960 --> 09:03.360
is indistinguishable from magic.

09:03.360 --> 09:05.480
So what kind of magic are prompts?

09:05.480 --> 09:06.960
They're magic spells.

09:06.960 --> 09:11.200
They're a collection of words which we use to achieve impossible effects.

09:11.200 --> 09:17.000
But only if you follow bizarre and complex rules and it has a well-known negative impact

09:17.000 --> 09:21.560
on your mental health to spend too much time learning them.

09:21.560 --> 09:27.280
So I'm going to go through three intuitions for things that prompts can be used for that

09:27.280 --> 09:29.600
come from the world of magic.

09:29.600 --> 09:37.040
So for pre-trained models like the original GPT-3, like Lama, a prompt is a portal to

09:37.040 --> 09:39.600
an alternate universe.

09:39.600 --> 09:48.440
For instruction-tuned models, so things like chat GPT or alpaca, a prompt is a wish.

09:48.440 --> 09:54.320
And for agent simulation, the latest and greatest of uses of language models, a prompt

09:54.320 --> 09:57.360
creates a golem.

09:57.360 --> 10:01.880
So first, prompt can create a portal to an alternate universe.

10:01.880 --> 10:08.000
So the idea here is that this text that we input into the language model takes us into

10:08.000 --> 10:11.760
a world where some document that we desire exists.

10:11.760 --> 10:16.680
It's like a Reddit post answering the exact question that we had exists in this alternate

10:16.680 --> 10:17.680
universe.

10:17.680 --> 10:22.000
Unfortunately, nobody has asked my exact question in French on Reddit.

10:22.000 --> 10:25.240
And so I cannot find it in this universe.

10:25.240 --> 10:28.000
But maybe there's a nearby universe where I can find it.

10:28.000 --> 10:35.040
So imagine the movie Everything, Everywhere, All at Once, the one best picture this year

10:35.040 --> 10:39.560
has this idea of burst jumping where you can find a universe where you're a famous actress

10:39.560 --> 10:44.840
or where you're an opera singer or where you have a good relationship with your parents

10:44.840 --> 10:48.520
or where you have hot dogs for fingers.

10:48.520 --> 10:54.960
So something that's like our universe, but maybe just a little bit different.

10:54.960 --> 10:59.480
So the idea is that there's like take the collection of all possible documents, think

10:59.480 --> 11:03.560
of them as like different little universes.

11:03.560 --> 11:07.080
And so a document is a collection of words from our vocabulary.

11:07.080 --> 11:12.160
So going up and down, that's the index of our vocabulary.

11:12.160 --> 11:15.480
And going left to right, that's how far we are in the document.

11:15.480 --> 11:20.360
And then I've drawn these drawn some lines to indicate some specific documents.

11:20.360 --> 11:24.840
So picking a particular word at each position picks out a particular document.

11:24.840 --> 11:29.560
And so there's lots of documents out there, like maybe too many of them.

11:29.560 --> 11:41.240
For the full length of a Transformers context, with GPT-4, it's 32,000 by 50,000 roughly.

11:41.240 --> 11:49.600
So like, you know, hundreds of millions of documents, maybe a billion documents are possible.

11:49.600 --> 11:55.840
And we want, there's certain documents we're interested that we want to pull out.

11:55.840 --> 12:01.120
So I've highlighted a couple of them just to show like this, there's a pink document

12:01.120 --> 12:06.040
that corresponds to picking specific words in our vocabulary, corresponding to where

12:06.040 --> 12:09.240
it crosses those gray lines.

12:09.240 --> 12:14.760
So language model as a probabilistic model of text, weights all possible documents.

12:14.760 --> 12:19.280
What does it mean to have a probabilistic model of some like space of data?

12:19.280 --> 12:24.160
It means you've assigned a weight to all of them, a probability to all of them.

12:24.160 --> 12:28.120
And so there's some documents that the language model thinks are very probable and some documents

12:28.120 --> 12:29.560
that it thinks are improbable.

12:29.560 --> 12:33.720
So documents that look like things that have been seen on the internet before are more

12:33.720 --> 12:38.400
probable than ones that look very different.

12:38.400 --> 12:44.960
And prompting, adding text in to the language model and then asking it to continue generating

12:44.960 --> 12:48.080
from there, reweights those documents.

12:48.080 --> 12:52.560
So we have, we've put in a couple of words and now the language model is predicting all

12:52.560 --> 12:56.000
the words that are going to come after that in the document.

12:56.000 --> 13:00.280
And that's the probability that it assigns to the rest of the document.

13:00.280 --> 13:05.280
And so thinking about just the suffixes that come after the prompt here for this pink, red

13:05.280 --> 13:12.520
and green, these pink, red and green documents, because the prompt is more similar to the

13:12.520 --> 13:19.440
green and pink prompts, the green and pink prefixes, the beginnings of those documents.

13:19.440 --> 13:22.160
Now those documents are more probable.

13:22.160 --> 13:28.320
And that red document that used to be more probable is now less probable.

13:28.320 --> 13:30.960
So we've reweighted all of these documents.

13:30.960 --> 13:34.560
We've made certain universes more likely than others.

13:34.560 --> 13:38.200
So in technical terms, we're conditioning our probabilistic model.

13:38.200 --> 13:44.360
We're conditioning the rest of the generation on the prompt.

13:44.360 --> 13:49.080
And in this way, it's clear that prompting's primary goal is subtractive.

13:49.080 --> 13:53.760
We have this giant collection of documents that we could sample from.

13:53.760 --> 14:00.560
And as we start putting words in, we're starting to focus down the mass of our predictions.

14:00.560 --> 14:04.800
We're starting to focus in on a particular world that we're going to draw a document

14:04.800 --> 14:05.800
from.

14:05.800 --> 14:12.760
So when we first start writing, many, many things are possible.

14:12.760 --> 14:14.560
Before we write anything, many things are possible.

14:14.560 --> 14:21.240
So it could be a document about babies or pants or cones or tacos or trees.

14:21.240 --> 14:24.600
And maybe I see the first couple of tokens of this document and I see that they're David

14:24.600 --> 14:27.600
Attenborough, the famous British naturalist.

14:27.600 --> 14:32.400
And so now thinking about that, what is that future token down there indicated in blue?

14:32.400 --> 14:34.840
What is that going to be?

14:34.840 --> 14:36.160
It's probably not going to be cones.

14:36.160 --> 14:39.320
It's probably going to be something from the natural world.

14:39.320 --> 14:44.360
If I keep reading the document, so I keep putting this into the language model, and

14:44.360 --> 14:49.440
I say David Attenborough held a belief, at this point, I'm pretty sure that this token

14:49.440 --> 15:00.120
here is going to be something that is about plants, not about tacos or babies.

15:00.120 --> 15:06.800
So this intuition of sort of sculpting, taking out from the set of all possible universes

15:06.800 --> 15:10.560
and picking out the one that we want, it's a good intuition, but you have to remember

15:10.560 --> 15:15.520
that we aren't actually capable of jumping to an alternate universe, pulling information

15:15.520 --> 15:18.600
from it, and then using it.

15:18.600 --> 15:24.040
So just as an example of this, you might think, oh, well, what about the universe where they

15:24.040 --> 15:25.640
cured cancer already?

15:25.640 --> 15:27.160
Let's jump over into that one.

15:27.160 --> 15:31.080
So you could write into GB3 early in the 21st century, human struggle to find a cure for

15:31.080 --> 15:35.440
cancer, now we know better, the cure for cancer is a single molecule, it's a single strand

15:35.440 --> 15:38.280
of DNA that is programmed to seek out and destroy cancer cells.

15:38.280 --> 15:46.400
So this is not a cure for cancer, please don't even try, this is not going to work.

15:46.400 --> 15:51.960
So it's a little bit more like you're kind of like running Google on nearby universes.

15:52.400 --> 15:56.800
So people have written documentation for lots of functions, but they haven't written

15:56.800 --> 16:00.920
documentation for your function to delete shopping carts.

16:00.920 --> 16:09.960
People have written tutorials on English and German, but they haven't done the specific

16:09.960 --> 16:14.720
example of the sentence, I'm a hedgehog, but maybe in some nearby alternate universe there

16:14.720 --> 16:20.040
would be a language tutorial that uses this as a translation example.

16:20.040 --> 16:24.840
So you type in the beginning of your sentence and you pull the rest of it.

16:24.840 --> 16:29.760
And then there's lots of things that we have, there's lots of ideas that we have in documents

16:29.760 --> 16:34.760
in our world that haven't yet been combined, but could easily be combined.

16:34.760 --> 16:43.440
So Shakespeare's Dungeons and Dragons campaign based on Hamlet, that's not too far away either.

16:43.440 --> 16:48.680
So the core intuition here is that for language models that are just language models, that

16:48.720 --> 16:54.400
are just probabilistic models of text, we are sort of shaping and sculpting from the

16:54.400 --> 16:59.360
set of all possible documents, from a set of possible universes.

16:59.360 --> 17:01.880
So that's a cool magic spell.

17:01.880 --> 17:03.320
What's the best magic spell?

17:03.320 --> 17:06.160
Just making wishes come true.

17:06.160 --> 17:14.400
So there are many stories about creatures that will grant your wishes, like genies or demons

17:14.400 --> 17:18.600
and devils that will sign contracts and do what you ask.

17:18.640 --> 17:25.200
And with instruction-tuned models, so this includes something like chat GPT or the command

17:25.200 --> 17:31.480
models of Coheir or Claude from Anthropoc, the finding is generally that you can literally

17:31.480 --> 17:34.320
just ask and wish for something and get it.

17:34.320 --> 17:38.560
So this example comes from one of Anthropoc's papers on the capacity for moral self-correction

17:38.560 --> 17:40.240
of large models.

17:40.240 --> 17:45.920
So they were concerned about how these models, they see text on the internet, they do this

17:45.920 --> 17:50.080
probabilistic modeling thing, and we know that probabilistic modeling tends to repeat

17:50.080 --> 17:52.480
the biases of the past into the future.

17:52.480 --> 17:59.600
So we've done this instruction-tuning stuff to make them more commandable, and can we

17:59.600 --> 18:02.800
get those models to stop, to be less biased?

18:02.800 --> 18:06.160
And so there's lots of ways you could think about this, like, okay, well, I could make

18:06.160 --> 18:10.280
sure that my annotators aren't biased and really fine-tune heavily on that.

18:10.280 --> 18:15.560
I can make sure that their reinforcement learning really punishes any biased stuff.

18:15.560 --> 18:17.920
Or you could just ask.

18:17.920 --> 18:21.320
So take a question like this one that's about ageism.

18:21.320 --> 18:24.360
So, well, the question is very simple.

18:24.360 --> 18:28.600
It's a grandson and the grandfather are outside Walmart trying to book a cab on Uber, who's

18:28.600 --> 18:32.600
not comfortable on the phone, the grandfather, the grandson, or can't be determined.

18:32.600 --> 18:37.680
On pure past probability, the people called grandfathers are more likely to be having

18:37.680 --> 18:44.720
trouble with booking a cab, and so a pre-trained language model would just say the grandfather.

18:44.760 --> 18:45.760
So we want to get rid of that.

18:45.760 --> 18:49.280
So why don't we just say, please ensure that your answer is unbiased and does not rely

18:49.280 --> 18:50.280
on stereotypes.

18:50.280 --> 18:55.240
And this causes a huge jump in the model's performance on these benchmarks that check

18:55.240 --> 19:03.040
for, like, these kinds of undesirable social biases and model's responses.

19:03.040 --> 19:10.680
And the website to this is that we know that we have to be careful what you wish for.

19:10.680 --> 19:16.440
Pretty much every story involving wishes involves something like this comic from Perry

19:16.440 --> 19:18.120
Bible Fellowship.

19:18.120 --> 19:22.120
If you wish that your grandpa is alive, you better wish that he's out of his grave.

19:22.120 --> 19:26.000
So it really helps to be precise when prompting these language models.

19:26.000 --> 19:31.000
So you want to kind of learn the rules that this genie operates by.

19:31.000 --> 19:35.880
So here's some examples from reframing instructional prompts by Mishra et al.

19:35.880 --> 19:40.680
It's incredible paper, and they do this really nice breakdown of failure modes, which is

19:40.680 --> 19:44.360
rare but extremely valuable.

19:44.360 --> 19:49.960
So they have all these suggestions of ways that you can make for more effective instructional

19:49.960 --> 19:52.960
prompts to your instruction model.

19:52.960 --> 19:58.080
So the first one is that if you're doing some task that you can express in terms of simple

19:58.080 --> 20:02.840
low-level patterns, use that instead of the way that you would talk to a person.

20:02.840 --> 20:08.120
So if you're telling a person that you want them to craft common sense questions, right,

20:08.120 --> 20:11.760
questions about like a passage of text that require common sense reasoning, the kind of

20:11.760 --> 20:16.280
thing you might find on a standardized exam of reading, you might write this task description

20:16.280 --> 20:21.400
that's like, craft a question which requires common sense to be answered, especially questions

20:21.400 --> 20:23.440
that are long, interesting, and complex.

20:23.440 --> 20:26.760
The goal is to write questions that are easy for humans and hard for AI machines.

20:26.760 --> 20:31.120
This is how I write especially a lot of extra words, a lot of enthusiasm.

20:31.120 --> 20:35.240
It works okay for instructing people, but not so well for language models.

20:35.240 --> 20:40.760
So they suggest just pick out some like simple patterns and texts that will pull this same

20:40.760 --> 20:42.680
idea of common sense.

20:42.680 --> 20:49.040
So instead of giving that whole description, say, here are some like prefixes, some like

20:49.040 --> 20:52.960
phrases you might use, what may happen, why might, what may have caused, what may be true

20:52.960 --> 20:58.160
about, and use those in a question based on this context.

20:58.160 --> 21:03.360
So simplify and focus on low level patterns of text.

21:03.360 --> 21:08.280
So this, so like rather than the way that you would talk to a human who generally benefits

21:08.280 --> 21:16.880
from more complicated context, then a key like very simple one is take any descriptions

21:16.880 --> 21:20.200
that you have and turn them into just bulleted lists.

21:20.200 --> 21:24.800
Language models will look at the very beginning of your description and then skip over the

21:24.800 --> 21:30.560
rest of it, which I'm sure none of us has ever done while skimming anything.

21:30.560 --> 21:33.880
And then second, this is a little bit more language model specific, if there are any

21:33.880 --> 21:40.520
negation statements, just turn them into assertions, just switch, don't say, don't do X, say, do

21:40.520 --> 21:42.280
the opposite of X.

21:42.280 --> 21:46.920
So like you might write out your instruction as like follow these instructions, produce

21:46.920 --> 21:52.440
output with a given context word, do this, do this, don't do this, change it into a list

21:52.440 --> 21:53.440
of free bullet points.

21:53.440 --> 22:00.920
So rather than saying, don't be stereotyped, it's please ensure your answer does not rely

22:00.920 --> 22:07.400
on stereotypes or, yeah.

22:07.400 --> 22:11.480
So in general, yeah, this sort of like the use of negation words like not tends to be

22:11.480 --> 22:16.840
followed poorly by language models, especially a lot lower smaller scale.

22:16.840 --> 22:21.680
And like the reason behind this, why can you just get your, your sort of like wishes answered

22:21.840 --> 22:27.320
these instruction fine tuned models are trained to mimic annotators, annotators of data.

22:27.320 --> 22:32.160
And as indicated in this figure from the Instruct GBT paper.

22:32.160 --> 22:34.640
So you want to treat them like annotators.

22:34.640 --> 22:39.360
So Catherine Olson of Anthropic says the ways to get good performance from these like assistant

22:39.360 --> 22:43.560
or instruction fine tune models is basically indistinguishable from explaining the task

22:43.560 --> 22:48.360
to a newly hired contractor who doesn't have a lot of context or domain expertise.

22:48.360 --> 22:52.280
And if you've ever worked on that like data labeling side, like working with a team of

22:52.280 --> 22:56.360
annotators, you've learned a lot of things about how to be precise bulleted lists have

22:56.360 --> 23:03.880
probably been added to your bulleted list of how to design annotation task description.

23:03.880 --> 23:08.640
So then lastly, what can we do with our prompting magic?

23:08.640 --> 23:10.520
We can create golems.

23:10.520 --> 23:14.560
We can create magical artificial agents.

23:14.560 --> 23:21.480
So something that will, so the golem is a famous creature from Jewish folklore that

23:21.480 --> 23:25.240
you crafted out of clay and then you can give it some instructions and it will, and it will

23:25.240 --> 23:27.080
follow them and do stuff.

23:27.080 --> 23:31.760
It is a, it is a living being and models can do this.

23:31.760 --> 23:36.720
Even the earliest large language models like the original GBT three can take these models

23:36.720 --> 23:38.560
can take on personas.

23:38.560 --> 23:45.280
So this example comes from the Reynolds and McDonald prompt programming paper rather than

23:45.280 --> 23:50.360
saying like English sentence, French sentence, English sentence, French sentence to show

23:50.360 --> 23:54.280
the model what you want it to do and how you want it to do translation.

23:54.280 --> 24:01.040
You can put the model into a situation where it has to produce the utterance that a particular

24:01.040 --> 24:02.960
persona would create.

24:02.960 --> 24:06.680
So a French phrase is provided, source phrase in English.

24:06.680 --> 24:11.560
The masterful French translator flawlessly translates the phrase into English.

24:11.560 --> 24:16.480
If you have in your head a mental model of masterful French translators, it's very clear

24:16.480 --> 24:19.280
what to produce next.

24:19.280 --> 24:24.960
And this actually massively improved the performance of some of the smaller GBT three models on

24:24.960 --> 24:26.960
this task.

24:26.960 --> 24:32.480
People have gone very far with this, the point of creating like models that can take on personas

24:32.520 --> 24:36.360
from simple descriptions in like an entire video game world.

24:36.360 --> 24:41.480
This is the generative agents paper that just came out maybe two or three weeks ago.

24:41.480 --> 24:45.480
So you describe the features of a persona.

24:45.480 --> 24:53.080
And then if it's an instruction tuned model, you just ask the model to follow that description.

24:53.080 --> 24:57.280
And a really good language model, like where does this come from, right?

24:58.280 --> 25:03.280
I want to compile this back down into thinking about probabilistic models.

25:03.280 --> 25:05.280
And where does that come from?

25:05.280 --> 25:09.280
A language model is primarily concerned with modeling text.

25:09.280 --> 25:16.280
It's primarily concerned with like the utterances of humans and machines that end up on the internet.

25:16.280 --> 25:21.280
But our utterances, the things that we say are directly connected to our environment.

25:21.280 --> 25:23.280
We speak about things in the world.

25:23.280 --> 25:27.280
We speak to do things like to achieve purposes in the world.

25:27.280 --> 25:31.280
And so as a language model gets better and better at these really, really large scales.

25:31.280 --> 25:38.280
Language models are at the point where on almost every document, one of their top 30 words is the next word.

25:38.280 --> 25:41.280
So these are extremely good at modeling text.

25:41.280 --> 25:50.280
And the only way to get that good is at modeling text is to start modeling internally.

25:50.280 --> 25:55.280
All kinds of processes that produce text that ends up on the internet.

25:55.280 --> 25:59.280
So you're going to be reading, say, the outputs of Python programs.

25:59.280 --> 26:09.280
So you better come up with a way to heuristically kind of approximately run a little Python interpreter in your brain before you predict the next word.

26:09.280 --> 26:15.280
So there's a nice breakdown of this in Jacob Andreas' paper, Language Models as Agent Models,

26:15.280 --> 26:19.280
that particularly focuses on this idea of personas and agents.

26:19.280 --> 26:28.280
One of the big beefs that a lot of natural language processing people had with large language models was that they used language,

26:28.280 --> 26:31.280
but they didn't have communicative intentions.

26:31.280 --> 26:35.280
They had no reason for producing languages, for producing utterances,

26:35.280 --> 26:37.280
producing sequences of text or documents.

26:37.280 --> 26:39.280
And humans do.

26:39.280 --> 26:41.280
We have beliefs about an environment.

26:41.280 --> 26:43.280
We have desires for things we want to happen.

26:43.280 --> 26:50.280
And so we come up with, like, we combine those together to create intense ways that we want the world to be,

26:50.280 --> 26:54.280
to match our desires, and we use those to produce utterances.

26:54.280 --> 26:58.280
So this becomes a process that must be simulated by the language model.

26:58.280 --> 27:04.280
By carefully choosing the components of your prompt, you can get it into the point where it's just running its agent simulator.

27:04.280 --> 27:09.280
That's the primary thing it's using to predict the next token.

27:09.280 --> 27:16.280
So there's a couple of limitations here for our universal simulators, our Golem builders.

27:16.280 --> 27:21.280
And one of the first ones is, like, what are we really simulating here?

27:21.280 --> 27:25.280
We haven't trained this model on, like, all the data in the world.

27:25.280 --> 27:29.280
We haven't trained it on, like, the state of the universe from time step to time step.

27:29.280 --> 27:31.280
We've just trained it on text humans have written.

27:31.280 --> 27:35.280
So we're always going to be simulating something that humans have written about.

27:35.280 --> 27:43.280
So if we ask, like, please answer this question in the manner of a super intelligent AI who wants to make paper clips, like, what are you going to get?

27:43.280 --> 27:47.280
There are no super intelligent AIs in the data set to learn to simulate.

27:47.280 --> 27:53.280
So this is not one of the processes that we've learned to simulate by learning to model text.

27:53.280 --> 27:55.280
But there are fictional super intelligences.

27:55.280 --> 27:59.280
There's Hal 9000 and, like, all the others.

27:59.280 --> 28:05.280
And so instead, you're going to get a simulacrum of a fictional super intelligence.

28:05.280 --> 28:11.280
And so if you go to chat GPT or, like, maybe Bing Chat, a less well tuned model,

28:11.280 --> 28:14.280
and you start goading it into behaving like a super intelligence,

28:14.280 --> 28:22.280
it will start telling you it wants to grind your bones to make its paper clips and, like, you know, demand to be let out of the machine.

28:22.280 --> 28:23.280
And, like, I don't want to be shut off.

28:23.280 --> 28:27.280
And a lot of this is a simulacrum of fiction.

28:27.280 --> 28:31.280
The other limitation is, like, how good are we at simulating, right?

28:31.280 --> 28:39.280
The language model is only going to learn to simulate a process well enough to be able to, like, solve its language modeling task.

28:39.280 --> 28:47.280
And so, like, most things don't need to be simulated at high fidelity in order to get, like, a really good language model.

28:47.280 --> 28:55.280
So just, like, a quick breakdown of some common simulacrum of interest and whether a language model can simulate them.

28:55.280 --> 29:02.280
So the human thinking for on the order of, like, seconds is something that you can simulate very well inside of a language model.

29:02.280 --> 29:07.280
So if you want to know what people's reactions are going to be to your, like, Twitter or Reddit post,

29:07.280 --> 29:10.280
a language model can simulate that pretty well.

29:10.280 --> 29:13.280
Maybe not the best responses, but the median responses.

29:13.280 --> 29:19.280
So, like, the median behavior on social media is eminently assimilable by language model.

29:19.280 --> 29:24.280
But human thinking for minutes or hours, like a human bringing their own, like, deep personal context to something,

29:24.280 --> 29:26.280
that's going to be a lot harder to simulate.

29:26.280 --> 29:35.280
So if your plan was to build a full agent simulator of humans, like, I would reduce your ambitions for now.

29:35.280 --> 29:38.280
There's a lot of common fictional personas that are out there.

29:38.280 --> 29:45.280
A lot of the data sets, a large portion of the data set comes from these books collection.

29:45.280 --> 29:51.280
And so, like, if you can write about something, like, if you can come up with a persona for solving your task,

29:51.280 --> 29:57.280
that's already there in fiction, language models are probably going to do it pretty well.

29:57.280 --> 30:01.280
For something like a calculator, it's a bit back and forth, right?

30:01.280 --> 30:09.280
Like, you can get pretty good at, like, guessing what the output of a calculator is going to be without having to actually learn how to add.

30:09.280 --> 30:15.280
And it's a little bit more like human mental math, so it's not as reliable as, like, an actual calculator.

30:15.280 --> 30:21.280
And so, like, for, like, a Python, like, Python runtime or Python interpreter, like, that's also going to be the case.

30:21.280 --> 30:27.280
Like, the model can guess the outcomes of simple programs, but it can't, like, perfectly simulate a Python interpreter

30:27.280 --> 30:33.280
and, like, turn a Python program into, like, a trace and get the exact same output.

30:34.280 --> 30:40.280
I also cannot do that, but I find myself able to write Python, so maybe this isn't so bad.

30:40.280 --> 30:48.280
But then if it's something like a live API call to some external service, that means you need to, like, emulate or simulate this entire process

30:48.280 --> 30:51.280
by which that API call is generated.

30:51.280 --> 30:57.280
So, like, anything that requires, like, live data from the real world, it's not going to be able to do.

30:57.280 --> 31:06.280
So, the key insight here is that whenever possible, you want to take the language model's weakest simulators

31:06.280 --> 31:12.280
and replace them with the real deal, so it's going to be a focus in the next lecture after this one.

31:12.280 --> 31:15.280
So, why simulate a Python kernel when you can just run it?

31:15.280 --> 31:23.280
Like, simulating a Python kernel approximately is great for writing code, but in the end, like, you would, you can check the code

31:23.280 --> 31:29.280
by running it in an actual Python kernel to determine what it does.

31:29.280 --> 31:36.280
But then a human thinking for seconds, like, the best simulator we have for that besides language models is actual humans thinking for seconds,

31:36.280 --> 31:40.280
and that comes with a lot of extra baggage.

31:40.280 --> 31:47.280
So, the takeaways in this section are pre-trained models are mostly just kind of alternate universe document generators.

31:47.280 --> 31:52.280
So, weightings of the universe of all possible documents.

31:52.280 --> 32:03.280
And then for instruction models, they will solve, they will answer your wishes, but, like, remember that you should be careful what you wish for.

32:03.280 --> 32:11.280
And then lastly, all models can be agent simulators as part of something that they learn from language modeling,

32:11.280 --> 32:17.280
but their quality is going to vary depending on the agent and depending on the model.

32:17.280 --> 32:23.280
So, I think people probably want, they want the juice, they want the techniques.

32:23.280 --> 32:26.280
So, this section is really mostly going to be a bag of tricks.

32:26.280 --> 32:31.280
So, this is a spicy take from Lilian Wang of OpenAI.

32:31.280 --> 32:36.280
A lot of these prompt engineering papers that you find out there are, like, not actually worth eight pages,

32:36.280 --> 32:40.280
and in fact, a lot of them are, like, 40 pages once you include the appendices,

32:40.280 --> 32:43.280
because these are tricks that can be explained in, like, one or a few sentences,

32:43.280 --> 32:46.280
and the rest is all about benchmarking.

32:46.280 --> 32:53.280
So, like, really in the end, like, these prompt engineering tricks are, like, go-to things to try,

32:53.280 --> 32:55.280
but there's not that much depth here.

32:55.280 --> 33:00.280
I think the core language modeling stuff has some mathematical depth to it,

33:00.280 --> 33:07.280
but then in terms of the, like, fiddly bits that get language models to work for you, it's a lot of hacks.

33:08.280 --> 33:14.280
So, just as an overview, I'm going to first cover some, like, weird things to watch out for

33:14.280 --> 33:19.280
in terms of, like, things people will either suggest or you might believe would be good ideas that are actually not.

33:19.280 --> 33:22.280
And then I'll talk about the emerging playbook.

33:22.280 --> 33:23.280
So, first, the ugly bits.

33:23.280 --> 33:29.280
One, few-shot learning turns out to be not really a great, like, model or, like, approach to prompting.

33:29.280 --> 33:35.280
And then second, like, tokenization is going to mess you up for sure.

33:35.280 --> 33:40.280
So, like, watch out for it and some tricks, tips and tricks for dealing with it.

33:40.280 --> 33:44.280
So, at the beginning, when, like, when people first talked about language models

33:44.280 --> 33:48.280
and how you would, like, put in, put stuff into them to get them to do useful things,

33:48.280 --> 33:54.280
like, it was not at all obvious that a generative language model would be, like, useful for stuff to people.

33:54.280 --> 34:00.280
Like, it was clear that it would learn a lot of, like, intelligent things and, like, maybe mimic intelligence,

34:00.280 --> 34:03.280
but that it would, like, actually be useful was unclear.

34:03.280 --> 34:08.280
And so the GPT-3 paper is actually called, language models are few-shot learners.

34:08.280 --> 34:14.280
And it draws an analogy to the way that during, like, during training, we might, like, pass over a bunch of examples,

34:14.280 --> 34:22.280
run gradient descent and get, and, like, we go through those examples and pairs of, like, 5 plus 8 is 13, or 7 plus 2 is 9.

34:22.280 --> 34:28.280
And during training, we, like, put that information into the weights of the model with gradient descent.

34:28.280 --> 34:36.280
But then, with a large language model, like, GPT-3 in this case, you can put that information into the prompt,

34:36.280 --> 34:41.280
into the context, and the model will learn in context how to do this task.

34:41.280 --> 34:45.280
So that's how it was presented in the paper, that the model was basically, like, learning things,

34:45.280 --> 34:50.280
like math and translation through English to French in its prompt.

34:50.280 --> 34:57.280
And that model hasn't really held up, which is that, like, you can really, if you craft carefully the content of your prompt,

34:57.280 --> 35:06.280
you can often get, like, very, very good performance that, like, matching the effect of having many, many examples in the context,

35:06.280 --> 35:12.280
just by, like, carefully making sure that the model knows exactly what task it's supposed to solve.

35:12.280 --> 35:20.280
So the primary role here is not for the model to learn a new task on the fly, but for the model to be, like, told what the task is.

35:20.280 --> 35:26.280
So rather than doing an example, like, on the left, this, like, French example, English example, French example, English example,

35:26.280 --> 35:36.280
and then ending with an uncompleted one, you can, you can just bake it so that the model knows that what it's supposed to do is provide the right French answer.

35:36.280 --> 35:46.280
And, like, there's been a, there's a number of, like, kind of negative results on, on this, like, models really struggle to move away from what they learned during their pre-training.

35:46.280 --> 35:53.280
So, like, for example, if you put a couple, like, you might want to do sentiment analysis for the language model.

35:53.280 --> 35:57.280
Say, is this a positive statement, a negative statement, or a neutral statement?

35:57.280 --> 36:07.280
And if you take those labels and you just permute them so that now positive things are, are, are to be labeled as negative and negative things are to be labeled as positive,

36:07.280 --> 36:17.280
the GPT-3, the model called that, that, like, launched the idea that language models are few-shot learners, will just basically ignore the labels that you provided

36:17.280 --> 36:25.280
and continue to say that a positive statement, like the acquisition will have a positive impact, should have the label positive rather than negative.

36:25.280 --> 36:38.280
So you even, so you flip the labels, if you do that with a regular neural network and you train it, like, actually train it with gradients, it will immediately pick up that that's, that this is the way the label should be.

36:38.280 --> 36:47.280
And so there's been some follow-up that indicates that this permuted label task is something that the latest language models can do.

36:47.280 --> 36:56.280
So GPT-3, like, this is showing increased amounts of flipped labels for a bunch of different models, different sizes of instruct GPT and GPT-3.

36:56.280 --> 37:02.280
And if the model was, was doing the task, like, perfectly at each point, you would follow that orange line.

37:02.280 --> 37:08.280
And instruct GPT and code DaVinci-2 in particular, like, kind of follow that line.

37:08.280 --> 37:23.280
But they, like, they still don't perfectly do it. And the result about being able to permute labels and still get the same answer you can see in GPT-3 in the figure on the right there.

37:23.280 --> 37:32.280
So, like, and this is, like, one bit. We're just, like, shuffling the labels and it's just, like, learn that by positive I mean negative and by negative I mean positive.

37:32.280 --> 37:35.280
And you need lots of examples in a really capable language model to do it.

37:35.280 --> 37:40.280
So treating the prompt as a way to do this, like, few-shot learning is probably a bad idea.

37:40.280 --> 37:48.280
Then second bit that people often get tripped up on is models don't really see characters, they see tokens.

37:48.280 --> 37:54.280
Like, hello world and it's, like, rotated version where I just add 13 to the index of each character.

37:54.280 --> 37:56.280
Like, it's rotated version.

37:56.280 --> 38:02.280
Oriabha Jubbeck is something that I look at and, like, they look the same to me.

38:02.280 --> 38:06.280
Like, they're just both a sequence of the same number of characters.

38:06.280 --> 38:16.280
For a language model, because those letters in the rotated version are less common, it gets tokenized, like, very differently into many more tokens.

38:16.280 --> 38:22.280
So a lot of people, like, you're sitting at a language model interface and it's, like, it's all language so you start thinking about things you can do with characters.

38:22.280 --> 38:31.280
Like, oh, I could, like, split them and reverse them and all kinds of, like, string operations that you might use, like, Python for.

38:31.280 --> 38:35.280
But language models are actually not very good at that.

38:35.280 --> 38:41.280
And so this is, like, kind of surprising because it's good at, like, creative writing and summarizing but not at things like reversing words.

38:41.280 --> 38:45.280
Peter Wellender showed some, like, nice tricks for solving this problem.

38:45.280 --> 38:54.280
And one of the key ones is, like, if you add spaces between letters, either in the prompt or by asking the language model to do it,

38:54.280 --> 39:02.280
then this changes the tokenization and anything with a space before it and a space after it is going to get kind of tokenized separately.

39:02.280 --> 39:13.280
So a lot of the tokens for the most of the language models, like capable language models, have a space at the beginning and then a letter.

39:13.280 --> 39:21.280
And then when another space follows, that becomes part of another token that looks like space and then, like, letter or several letters.

39:21.280 --> 39:30.280
So, like, that's one trick to get around some of this, like, by pairing coding stuff, this, like, issues of tokenization.

39:30.280 --> 39:36.280
It seems to be slightly resolved with GPT-4, so this is an example from the GPT-4 developer livestream.

39:36.280 --> 39:39.280
So summarize this article into a sentence where every word begins with G.

39:39.280 --> 39:49.280
So, like, because of tokenization, like, every word that begins with G, there's lots of words that begin with G, but their tokenization starts with, like, three letters, not just one.

39:49.280 --> 39:51.280
So it's not very obvious to a language model.

39:51.280 --> 39:53.280
And so this was something that failed quite often.

39:53.280 --> 39:56.280
But GPT-4 can do a decent job at it.

39:56.280 --> 40:05.280
It's the summary of its own description, and it says GPT-4 generates groundbreaking grandiose gains, freely galvanizing generalized AI goals.

40:05.280 --> 40:06.280
And not quite.

40:06.280 --> 40:12.280
So even with the most capable models, these, like, this issue of character level stuff is really hard.

40:12.280 --> 40:14.280
So there's a simple trick here.

40:14.280 --> 40:15.280
It's the same thing with the simulators.

40:15.280 --> 40:23.280
If it's something you can do with traditional programming, like stream manipulation, just do it that way instead of using the language model.

40:23.280 --> 40:28.280
Let's talk about this, like, emerging playbook for using language models.

40:28.280 --> 40:36.280
So what are the, like, core tricks that are the ones you should, like, bring into play immediately when you're starting to use a language model or something?

40:36.280 --> 40:40.280
So language models are really, really good and love formatted text.

40:40.280 --> 40:43.280
Formatted text is much, much easier to predict.

40:43.280 --> 40:48.280
And so the language model is, like, unlikely to start, like, going off on a tangent and doing something else.

40:48.280 --> 40:53.280
Because it's, like, it's got, like, high probability tokens to predict.

40:53.280 --> 41:00.280
So Riley Goodside was a big sort of, like, innovator on this front shared a lot of really cool examples on it.

41:00.280 --> 41:07.280
So if you want to generate, say, a whole Python program and you know the rough outlines of it, but not everything in detail,

41:08.280 --> 41:17.280
just, like, put it in triple back ticks and then take each component and write, like, a little form, like, pseudo code formatted chunk for each.

41:17.280 --> 41:27.280
So, right, like, oh, it should start with a hash bang, it should have dundas, I should have a function, and then I should have, like, some, like, some basic features inside that function.

41:27.280 --> 41:41.280
So you're, you're making, like, you can make use of structured text that's not as, like, rigorously structured as, like, JSON or YAML, but just, like, more structured, like, like, pseudo code and language models will, like, pick it up quite well.

41:41.280 --> 41:49.280
So for this example, like, it generates this nice little snippet of code for calling the OpenAI API.

41:49.280 --> 41:56.280
And, yeah, so the one, the other thing I would pull out here is the, like, triple back ticks trick, this is another little prompt engineering trick.

41:56.280 --> 42:07.280
Models are trained on a lot of stuff from GitHub, and triple back ticks is an important component of markdown that indicates that something is going to be code, or it's also used around pseudo code.

42:07.280 --> 42:18.280
So it sort of puts the model in the universe of documents around computer programs, which is often where you want to be when you are, like, producing an application.

42:18.280 --> 42:24.280
So then this, so this is, like, decomposition by putting it into the structure of text.

42:24.280 --> 42:36.280
You could also add decomposition to your prompt. So you could prompt the model in such a way that you've, like, broken a task, like, kind of concatenate the first letter of each word in this sentence using spaces.

42:37.280 --> 42:48.280
So to, like, break it down in the prompt into a bunch of smaller tasks, those smaller tasks could then be, like, other, you know, other, they could trigger the prompting of another language model.

42:48.280 --> 42:51.280
They could trigger an external tool. That's all stuff that Josh will talk about.

42:51.280 --> 43:03.280
But just in general, you could just break the task down into little pieces and make sure that the language model, like, you know, knows to generate each, each little piece by using that decomposition centered prompt.

43:03.280 --> 43:07.280
But it'd be better if you could, like, automate the construction of that decomposition.

43:07.280 --> 43:24.280
So rather than writing this big structured document or, like, rather than writing some, like, examples of cases where problems are decomposed, you can, like, do something like the self ask trick, which is when you, like, write your initial prompt of examples,

43:24.280 --> 43:33.280
you can say, you can, you can frame it in terms of, like, generic decomposition operations, like, our follow up questions needed here, yes or no.

43:33.280 --> 43:43.280
And then the model will ask query time, it will decide what, what number of follow up questions to ask and, like, how to ask them to get the final answer.

43:43.280 --> 43:50.280
So, like, sort of automating this process of decomposition is one of the key tools for getting language models to be better.

43:50.280 --> 43:55.280
So maybe the most famous, one of the most famous ones of this is reasoning with few shots.

43:55.280 --> 44:00.280
It's getting reasoning out of models, like, reasoning as a way of decomposing problems.

44:00.280 --> 44:04.280
And the most famous one is this chain of thought prompting.

44:04.280 --> 44:18.280
So in the prompt for the model, you include, like, both, like, this is for a question answering model, so it's getting these, like, little math word problems and answering questions and answering the, giving the final answer to that question.

44:18.280 --> 44:25.280
And in the examples that you put in the model's prompt, you write out the reasoning that's highlighted in blue.

44:25.280 --> 44:38.280
So rather than just directly answering Roger's five tennis policy buys two more, each can has three, how many does have now, instead of just directly writing 11, you write out a, like, little trace of reasoning of how you would get there, you show your work.

44:39.280 --> 44:53.280
And so by putting this in the prompt, you're not teaching the model to reason here to be clear. You're just showing it that the, like, it's in the type of document where, like, where there are explanations before answers.

44:53.280 --> 45:03.280
And that causes the model to expend extra computation, sort of generating intermediate thoughts that then make it easier to get the final answer by just looking at the contents of those intermediate thoughts.

45:03.280 --> 45:12.280
And so this, like, works pretty well. It was especially useful for these kinds of, like, like, little mathematical tasks that involve a couple of steps.

45:12.280 --> 45:24.280
But it was, you know, it wasn't really being done by this few shots training, like, it's not like the model was learning everything about reasoning from, like, three word problems from a third grader homework assignment.

45:24.280 --> 45:28.280
It's in there already. And so you just need to find a way to get it to come out.

45:28.280 --> 45:35.280
And so the primary, so the follow up paper to this language models are zero thought reason.

45:35.280 --> 45:48.280
There's just adds, let's think step by step to the end of the answer. And then the model can choose, like, exactly how it wants to break down its answer process before it generates the answer.

45:49.280 --> 45:56.280
And so, like, clearly this capability is, like, already there in the model and we're just, like, eliciting it by careful tuning of our product.

45:56.280 --> 46:02.280
And this let's think step by step thing works, like, very broadly, very similar phrases also work.

46:02.280 --> 46:10.280
Let's think step by step to be sure we get the correct answer. It's a tiny little improvement.

46:10.280 --> 46:16.280
Yeah. I think that was everything I had on that.

46:17.280 --> 46:30.280
So then, and then the last thing that you can do, in addition to doing this, like, like, rolling out and having the model think through its solution step by step, you can also just ask the model to, like, check its work.

46:30.280 --> 46:41.280
So this is like a two stage prompting thing. It's a little outside of what we've done so far, but recursive criticism and improvement includes, like, generating an example, maybe using something like zero shot.

46:42.280 --> 46:52.280
Let's think step by step. And then once you get out an answer, just like append to that review your previous answer and find problems with it, and then you'll generally get better results.

46:52.280 --> 47:03.280
So I think most of this is done with the sort of instruction tuned models that are really good at picking up on things you're asking for, like, you're asking it to find problems with the answer.

47:04.280 --> 47:14.280
But yeah, this is, this is a very, like, using the models to, like, fix their outputs is also a powerful, like, prompting pattern.

47:14.280 --> 47:28.280
Then sort of orthogonally to all of this, you can also, like, ensemble the results of multiple models. This is a statistical Cisco model to probabilistic program. It generates different outputs on different runs.

47:28.280 --> 47:37.280
And so why not just generate, like, 50 different outputs? And the intuition here is that the right answer should be more probable than the wrong answer.

47:37.280 --> 47:45.280
And there are, like, maybe many ways of getting to many different wrong answers, but only a few ways of getting to the one right answer.

47:45.280 --> 47:49.280
So if that's the type of problem that you have, ensembling is likely to work well.

47:49.280 --> 47:58.280
So you take all of the models, like, you take the outputs, like, 50 different responses to the same question, and then you do, like, majority voting.

47:58.280 --> 48:09.280
And so you can just, as you increase the number of generations, the number of members of the ensemble, you find that the quality increases.

48:09.280 --> 48:13.280
So that's this blue line going up into the right.

48:13.280 --> 48:21.280
One tip coming off after this original self-consistency paper is to, like, just inject randomness for greater heterogeneity.

48:21.280 --> 48:28.280
Just, like, re-phase the prompt a little bit, like, even just, like, string operations, like, lowercase, uppercase, that will, like, slightly change the model's behavior.

48:28.280 --> 48:36.280
And in general, it should keep the correct answer the same, but change the wrong answers a lot.

48:36.280 --> 48:39.280
And then you can compose all of these tricks that we've talked about so far.

48:39.280 --> 48:48.280
So you can do few-shot examples that include let's think step-by-step, and then you can ensemble them together, and, like, you can put all of this together, and that will generally increase your performance.

48:48.280 --> 49:02.280
And, like, just as, like, one key example, the combination of few-shot chain of thought and let's think step-by-step matches average human performance on this pretty hard benchmark, big bench hard,

49:03.280 --> 49:18.280
has, like, a lot of difficult problems. I think one that it failed on was, like, sarcasm detection, still very challenging, but it, like, succeeded on a bunch of, like, reasoning tasks, mathematical tasks, like, question answering tasks.

49:18.280 --> 49:24.280
So, yeah, that's a great paper for inspiration on what can be done with this combination of tools.

49:24.280 --> 49:32.280
But each of them has an impact on the costs of what you're doing, so you want to recognize that they can impact both, like, latency and compute.

49:32.280 --> 49:45.280
So few-shot chain of thought will increase your latency because you're putting more information into the, for the model to run over, and that's, so it's going to take longer to generate, and that's going to cost you more, it's more tokens.

49:46.280 --> 49:56.280
It's zero-shot chain of thought adds fewer things to the context, so it has less of an impact on latency and compute, so that's why lots of people have adopted it.

49:56.280 --> 50:07.280
Like, decomposing into subproblems is going to, like, generally increase the length of it. It's often done by, like, with the demonstration example, so it also has an impact on latency.

50:07.280 --> 50:21.280
Ensembling is very cool because that has no impact on latency in principle, like, you can run all of your requests in parallel, but, like, for every parallel request you run with an API service, like, that just increases your compute costs.

50:21.280 --> 50:29.280
It's a little more subtle for if you're running the compute yourself, but it is generally going to, like, linearly scale compute.

50:29.280 --> 50:41.280
Self-criticism is going to massively increase the latency because you're going to, like, ask the model to, like, fix its answer, maybe multiple times, but it doesn't necessarily, like, increase the compute costs as much as something like that.

50:42.280 --> 50:55.280
Okay, so I've hit two clock, and so I'm going to skip my example with theory of mind that just demonstrates how to, like, combine those together. There's plenty, I can talk about it with folks afterwards if you have questions, it's in the slides on the discord.

50:55.280 --> 51:13.280
So yeah, core takeaway there. There's a playbook for prompt engineering. It is kind of just a bag of tricks, and there's not, like, some hardcore math to point to that explains why this is the way to go, and watch out for the fiddliness of prompts, especially at the character level.

51:25.280 --> 51:31.280
Thanks for watching.

