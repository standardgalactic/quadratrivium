This morning, we kind of covered a lot of the things at a high level, a lot of the foundations
as well.
And now we're going to dive into how to do stuff with language models, the technical
skills you need to get them to do the things that you want them to do.
So first up, I'm going to cover prompt engineering.
And so the like the scope of this lecture is on how to adjust the text that goes into
your language model to get the behavior that you want.
And prompt engineering is the art of designing that text.
This is not where that text comes from or where it goes.
So it's not like anything about the retrieval augmentation that I did in the morning.
It's about things like writing, you know, write a summary of this or like changing around
the text that goes into that language model.
So we'll cover the second lecture this afternoon, we'll cover that latter part.
So we're just thinking about what kind of text do I put into my language model to get
it to form the tasks that I want it to do.
This is for language models, this is kind of replacing a lot of things that you would
otherwise do with training, with fine tuning, with all the approaches that we've taken
for constructing machine learning models in the past.
And it's also, in a sense, the way that we program these language models, so it's sort
of like programming in English instead of programming in Python or Rust or whatever.
So there's just two components to this talk, the simplest agenda of all the talks.
First is some high-level intuitions for prompting, and I'm going to present the idea that prompts
are magic spells.
And then to get a little bit more specific, I'm going to talk about the emerging playbook
for effective prompting, a collection of sort of prompting techniques, ways to get language
models to do what you want.
So what do I mean when I say that prompts are magic spells?
This is not literally true, they are not literally magic, it is linear algebra, which I find
delightful and beautiful, but which is not actually magic.
So there's not little, like, wizards inside of language models, there's not brains inside
of language models, either really.
Language models are just statistical models of text, like in some sense, you can statistically
model your data with a bell curve, and that's a statistical model of your data, and a language
model is a statistical model of data, and it just happens to be a statistical model of
a more complicated statistical model of more complicated data.
So there's nothing really magic about that.
What do I mean when I say that it's a statistical model roughly of text?
Roughly what that means is that when the model is trained, you can take some list of text
that you pulled from somewhere, let's say some text you sampled from the internet, and
then go through the tokens in that text, go through the pieces of that text, pass them
through a model, and it'll give out a probability of what the next word is going to be.
So this is called an autoregressive model.
It's just a term of art, autoregressive, like predicting on itself.
So we're going through the text, and we keep adding stuff to what we put into the model,
generating probabilities for them, for what the next token is going to be.
So we have a dictionary that for every possible token, for every possible thing that could
come next, we have a probability or a law of probability for it.
And then if we do that across a lot of text of sufficient length, then the language model
being a model of text means that the probability it assigned to all the text that it saw would
be high.
So you start off with random weights, a random model, it has no idea about text, it assigns
a low probability to basically everything it sees to eventually a model that assigns
a high probability to pretty much any text that you can imagine like drawing from the
internet or writing yourself.
And so this is literally true, and this is sort of the place where you want to eventually
compile down your intuitions and your understanding of models.
You want to eventually get to the point where you are thinking in terms of how does this
arise from statistical modeling, but it also can give you really bad intuitions to think
at that level.
And when you say that these are statistical models, that they learn patterns or that language
models are statistical pattern matchers or static parrots, you are drawing on intuition
from things like other kinds of statistical models that you have seen, so maybe statistical
models of data like linear regression or Gaussian distributions, so these are very simple objects.
Or you are drawing inspiration from other kinds of models of text that you have seen
like Google's autocomplete or the autocomplete function on your phone, and we are well aware
that these things are very dumb and not very capable and that they just pick up on surface
level patterns.
Whereas these language models, they have learned so much about text that it is extremely difficult
to think of it as some kind of statistics, the way people normally think about probability
and statistics.
So I just picked one random example that I felt demonstrates this, Bing Chat can take
in SVG files as text and then describe the content of that SVG file for you as an image,
and very few people's intuition for what a model of text is would include that.
So there is some better intuitions that come from the world of statistics of probability
that are a little bit better, so probabilistic programs is probably one of the better intuitions
from that world.
So the idea is we often think of really simple statistical models, we think of them as being
like represented by equations or the kinds of things that you can manipulate in a probability
class, but a lot of complicated statistical models that people use today, even like really
complicated hierarchical linear regressions, can be better thought of as programs that
operate on random data, as programs that manipulate random variables.
And so you can write things that you might do with a language model, like take a question,
and rather than having it just directly answered, having it think of what the answer should
be, like maybe like brainstorm a little bit, and then take that brainstorm and turn it
into an answer.
So that's what this little like Python program up here shows.
Take a question, generate a thought, and then generate an answer.
And because language models are probabilistic, you can actually literally sample from them,
you can actually draw different possibilities each time if you wish.
And this probabilistic program, all probabilistic programs can be represented with these graphical
models that like, so this comes from sort of a branch of machine learning that's been
eclipsed by the rise of deep learning and LLNs in particular.
But this gives you like the sort of best way to think about just how complicated can you
get when thinking about like a model of text, a probabilistic model of text.
And so if you're interested in that, like that kind of direction, if you have that background
or have that interest, the language model cascades by Dohan et al, like really dives
into detail and shows how you can explain a bunch of like prompting tricks and other
things that people have done in terms of probabilistic programs.
That's a little bit to arcane.
So what's something that is like maybe a little bit easier to understand.
So I'm going to draw inspiration from Arthur C. Clark's laws of technology here.
And any sufficiently advanced technology like, I don't know, machine that makes your voice
really loud in a room or a machine that can show you your mother's face across the country
is indistinguishable from magic.
So what kind of magic are prompts?
They're magic spells.
They're a collection of words which we use to achieve impossible effects.
But only if you follow bizarre and complex rules and it has a well-known negative impact
on your mental health to spend too much time learning them.
So I'm going to go through three intuitions for things that prompts can be used for that
come from the world of magic.
So for pre-trained models like the original GPT-3, like Lama, a prompt is a portal to
an alternate universe.
For instruction-tuned models, so things like chat GPT or alpaca, a prompt is a wish.
And for agent simulation, the latest and greatest of uses of language models, a prompt
creates a golem.
So first, prompt can create a portal to an alternate universe.
So the idea here is that this text that we input into the language model takes us into
a world where some document that we desire exists.
It's like a Reddit post answering the exact question that we had exists in this alternate
universe.
Unfortunately, nobody has asked my exact question in French on Reddit.
And so I cannot find it in this universe.
But maybe there's a nearby universe where I can find it.
So imagine the movie Everything, Everywhere, All at Once, the one best picture this year
has this idea of burst jumping where you can find a universe where you're a famous actress
or where you're an opera singer or where you have a good relationship with your parents
or where you have hot dogs for fingers.
So something that's like our universe, but maybe just a little bit different.
So the idea is that there's like take the collection of all possible documents, think
of them as like different little universes.
And so a document is a collection of words from our vocabulary.
So going up and down, that's the index of our vocabulary.
And going left to right, that's how far we are in the document.
And then I've drawn these drawn some lines to indicate some specific documents.
So picking a particular word at each position picks out a particular document.
And so there's lots of documents out there, like maybe too many of them.
For the full length of a Transformers context, with GPT-4, it's 32,000 by 50,000 roughly.
So like, you know, hundreds of millions of documents, maybe a billion documents are possible.
And we want, there's certain documents we're interested that we want to pull out.
So I've highlighted a couple of them just to show like this, there's a pink document
that corresponds to picking specific words in our vocabulary, corresponding to where
it crosses those gray lines.
So language model as a probabilistic model of text, weights all possible documents.
What does it mean to have a probabilistic model of some like space of data?
It means you've assigned a weight to all of them, a probability to all of them.
And so there's some documents that the language model thinks are very probable and some documents
that it thinks are improbable.
So documents that look like things that have been seen on the internet before are more
probable than ones that look very different.
And prompting, adding text in to the language model and then asking it to continue generating
from there, reweights those documents.
So we have, we've put in a couple of words and now the language model is predicting all
the words that are going to come after that in the document.
And that's the probability that it assigns to the rest of the document.
And so thinking about just the suffixes that come after the prompt here for this pink, red
and green, these pink, red and green documents, because the prompt is more similar to the
green and pink prompts, the green and pink prefixes, the beginnings of those documents.
Now those documents are more probable.
And that red document that used to be more probable is now less probable.
So we've reweighted all of these documents.
We've made certain universes more likely than others.
So in technical terms, we're conditioning our probabilistic model.
We're conditioning the rest of the generation on the prompt.
And in this way, it's clear that prompting's primary goal is subtractive.
We have this giant collection of documents that we could sample from.
And as we start putting words in, we're starting to focus down the mass of our predictions.
We're starting to focus in on a particular world that we're going to draw a document
from.
So when we first start writing, many, many things are possible.
Before we write anything, many things are possible.
So it could be a document about babies or pants or cones or tacos or trees.
And maybe I see the first couple of tokens of this document and I see that they're David
Attenborough, the famous British naturalist.
And so now thinking about that, what is that future token down there indicated in blue?
What is that going to be?
It's probably not going to be cones.
It's probably going to be something from the natural world.
If I keep reading the document, so I keep putting this into the language model, and
I say David Attenborough held a belief, at this point, I'm pretty sure that this token
here is going to be something that is about plants, not about tacos or babies.
So this intuition of sort of sculpting, taking out from the set of all possible universes
and picking out the one that we want, it's a good intuition, but you have to remember
that we aren't actually capable of jumping to an alternate universe, pulling information
from it, and then using it.
So just as an example of this, you might think, oh, well, what about the universe where they
cured cancer already?
Let's jump over into that one.
So you could write into GB3 early in the 21st century, human struggle to find a cure for
cancer, now we know better, the cure for cancer is a single molecule, it's a single strand
of DNA that is programmed to seek out and destroy cancer cells.
So this is not a cure for cancer, please don't even try, this is not going to work.
So it's a little bit more like you're kind of like running Google on nearby universes.
So people have written documentation for lots of functions, but they haven't written
documentation for your function to delete shopping carts.
People have written tutorials on English and German, but they haven't done the specific
example of the sentence, I'm a hedgehog, but maybe in some nearby alternate universe there
would be a language tutorial that uses this as a translation example.
So you type in the beginning of your sentence and you pull the rest of it.
And then there's lots of things that we have, there's lots of ideas that we have in documents
in our world that haven't yet been combined, but could easily be combined.
So Shakespeare's Dungeons and Dragons campaign based on Hamlet, that's not too far away either.
So the core intuition here is that for language models that are just language models, that
are just probabilistic models of text, we are sort of shaping and sculpting from the
set of all possible documents, from a set of possible universes.
So that's a cool magic spell.
What's the best magic spell?
Just making wishes come true.
So there are many stories about creatures that will grant your wishes, like genies or demons
and devils that will sign contracts and do what you ask.
And with instruction-tuned models, so this includes something like chat GPT or the command
models of Coheir or Claude from Anthropoc, the finding is generally that you can literally
just ask and wish for something and get it.
So this example comes from one of Anthropoc's papers on the capacity for moral self-correction
of large models.
So they were concerned about how these models, they see text on the internet, they do this
probabilistic modeling thing, and we know that probabilistic modeling tends to repeat
the biases of the past into the future.
So we've done this instruction-tuning stuff to make them more commandable, and can we
get those models to stop, to be less biased?
And so there's lots of ways you could think about this, like, okay, well, I could make
sure that my annotators aren't biased and really fine-tune heavily on that.
I can make sure that their reinforcement learning really punishes any biased stuff.
Or you could just ask.
So take a question like this one that's about ageism.
So, well, the question is very simple.
It's a grandson and the grandfather are outside Walmart trying to book a cab on Uber, who's
not comfortable on the phone, the grandfather, the grandson, or can't be determined.
On pure past probability, the people called grandfathers are more likely to be having
trouble with booking a cab, and so a pre-trained language model would just say the grandfather.
So we want to get rid of that.
So why don't we just say, please ensure that your answer is unbiased and does not rely
on stereotypes.
And this causes a huge jump in the model's performance on these benchmarks that check
for, like, these kinds of undesirable social biases and model's responses.
And the website to this is that we know that we have to be careful what you wish for.
Pretty much every story involving wishes involves something like this comic from Perry
Bible Fellowship.
If you wish that your grandpa is alive, you better wish that he's out of his grave.
So it really helps to be precise when prompting these language models.
So you want to kind of learn the rules that this genie operates by.
So here's some examples from reframing instructional prompts by Mishra et al.
It's incredible paper, and they do this really nice breakdown of failure modes, which is
rare but extremely valuable.
So they have all these suggestions of ways that you can make for more effective instructional
prompts to your instruction model.
So the first one is that if you're doing some task that you can express in terms of simple
low-level patterns, use that instead of the way that you would talk to a person.
So if you're telling a person that you want them to craft common sense questions, right,
questions about like a passage of text that require common sense reasoning, the kind of
thing you might find on a standardized exam of reading, you might write this task description
that's like, craft a question which requires common sense to be answered, especially questions
that are long, interesting, and complex.
The goal is to write questions that are easy for humans and hard for AI machines.
This is how I write especially a lot of extra words, a lot of enthusiasm.
It works okay for instructing people, but not so well for language models.
So they suggest just pick out some like simple patterns and texts that will pull this same
idea of common sense.
So instead of giving that whole description, say, here are some like prefixes, some like
phrases you might use, what may happen, why might, what may have caused, what may be true
about, and use those in a question based on this context.
So simplify and focus on low level patterns of text.
So this, so like rather than the way that you would talk to a human who generally benefits
from more complicated context, then a key like very simple one is take any descriptions
that you have and turn them into just bulleted lists.
Language models will look at the very beginning of your description and then skip over the
rest of it, which I'm sure none of us has ever done while skimming anything.
And then second, this is a little bit more language model specific, if there are any
negation statements, just turn them into assertions, just switch, don't say, don't do X, say, do
the opposite of X.
So like you might write out your instruction as like follow these instructions, produce
output with a given context word, do this, do this, don't do this, change it into a list
of free bullet points.
So rather than saying, don't be stereotyped, it's please ensure your answer does not rely
on stereotypes or, yeah.
So in general, yeah, this sort of like the use of negation words like not tends to be
followed poorly by language models, especially a lot lower smaller scale.
And like the reason behind this, why can you just get your, your sort of like wishes answered
these instruction fine tuned models are trained to mimic annotators, annotators of data.
And as indicated in this figure from the Instruct GBT paper.
So you want to treat them like annotators.
So Catherine Olson of Anthropic says the ways to get good performance from these like assistant
or instruction fine tune models is basically indistinguishable from explaining the task
to a newly hired contractor who doesn't have a lot of context or domain expertise.
And if you've ever worked on that like data labeling side, like working with a team of
annotators, you've learned a lot of things about how to be precise bulleted lists have
probably been added to your bulleted list of how to design annotation task description.
So then lastly, what can we do with our prompting magic?
We can create golems.
We can create magical artificial agents.
So something that will, so the golem is a famous creature from Jewish folklore that
you crafted out of clay and then you can give it some instructions and it will, and it will
follow them and do stuff.
It is a, it is a living being and models can do this.
Even the earliest large language models like the original GBT three can take these models
can take on personas.
So this example comes from the Reynolds and McDonald prompt programming paper rather than
saying like English sentence, French sentence, English sentence, French sentence to show
the model what you want it to do and how you want it to do translation.
You can put the model into a situation where it has to produce the utterance that a particular
persona would create.
So a French phrase is provided, source phrase in English.
The masterful French translator flawlessly translates the phrase into English.
If you have in your head a mental model of masterful French translators, it's very clear
what to produce next.
And this actually massively improved the performance of some of the smaller GBT three models on
this task.
People have gone very far with this, the point of creating like models that can take on personas
from simple descriptions in like an entire video game world.
This is the generative agents paper that just came out maybe two or three weeks ago.
So you describe the features of a persona.
And then if it's an instruction tuned model, you just ask the model to follow that description.
And a really good language model, like where does this come from, right?
I want to compile this back down into thinking about probabilistic models.
And where does that come from?
A language model is primarily concerned with modeling text.
It's primarily concerned with like the utterances of humans and machines that end up on the internet.
But our utterances, the things that we say are directly connected to our environment.
We speak about things in the world.
We speak to do things like to achieve purposes in the world.
And so as a language model gets better and better at these really, really large scales.
Language models are at the point where on almost every document, one of their top 30 words is the next word.
So these are extremely good at modeling text.
And the only way to get that good is at modeling text is to start modeling internally.
All kinds of processes that produce text that ends up on the internet.
So you're going to be reading, say, the outputs of Python programs.
So you better come up with a way to heuristically kind of approximately run a little Python interpreter in your brain before you predict the next word.
So there's a nice breakdown of this in Jacob Andreas' paper, Language Models as Agent Models,
that particularly focuses on this idea of personas and agents.
One of the big beefs that a lot of natural language processing people had with large language models was that they used language,
but they didn't have communicative intentions.
They had no reason for producing languages, for producing utterances,
producing sequences of text or documents.
And humans do.
We have beliefs about an environment.
We have desires for things we want to happen.
And so we come up with, like, we combine those together to create intense ways that we want the world to be,
to match our desires, and we use those to produce utterances.
So this becomes a process that must be simulated by the language model.
By carefully choosing the components of your prompt, you can get it into the point where it's just running its agent simulator.
That's the primary thing it's using to predict the next token.
So there's a couple of limitations here for our universal simulators, our Golem builders.
And one of the first ones is, like, what are we really simulating here?
We haven't trained this model on, like, all the data in the world.
We haven't trained it on, like, the state of the universe from time step to time step.
We've just trained it on text humans have written.
So we're always going to be simulating something that humans have written about.
So if we ask, like, please answer this question in the manner of a super intelligent AI who wants to make paper clips, like, what are you going to get?
There are no super intelligent AIs in the data set to learn to simulate.
So this is not one of the processes that we've learned to simulate by learning to model text.
But there are fictional super intelligences.
There's Hal 9000 and, like, all the others.
And so instead, you're going to get a simulacrum of a fictional super intelligence.
And so if you go to chat GPT or, like, maybe Bing Chat, a less well tuned model,
and you start goading it into behaving like a super intelligence,
it will start telling you it wants to grind your bones to make its paper clips and, like, you know, demand to be let out of the machine.
And, like, I don't want to be shut off.
And a lot of this is a simulacrum of fiction.
The other limitation is, like, how good are we at simulating, right?
The language model is only going to learn to simulate a process well enough to be able to, like, solve its language modeling task.
And so, like, most things don't need to be simulated at high fidelity in order to get, like, a really good language model.
So just, like, a quick breakdown of some common simulacrum of interest and whether a language model can simulate them.
So the human thinking for on the order of, like, seconds is something that you can simulate very well inside of a language model.
So if you want to know what people's reactions are going to be to your, like, Twitter or Reddit post,
a language model can simulate that pretty well.
Maybe not the best responses, but the median responses.
So, like, the median behavior on social media is eminently assimilable by language model.
But human thinking for minutes or hours, like a human bringing their own, like, deep personal context to something,
that's going to be a lot harder to simulate.
So if your plan was to build a full agent simulator of humans, like, I would reduce your ambitions for now.
There's a lot of common fictional personas that are out there.
A lot of the data sets, a large portion of the data set comes from these books collection.
And so, like, if you can write about something, like, if you can come up with a persona for solving your task,
that's already there in fiction, language models are probably going to do it pretty well.
For something like a calculator, it's a bit back and forth, right?
Like, you can get pretty good at, like, guessing what the output of a calculator is going to be without having to actually learn how to add.
And it's a little bit more like human mental math, so it's not as reliable as, like, an actual calculator.
And so, like, for, like, a Python, like, Python runtime or Python interpreter, like, that's also going to be the case.
Like, the model can guess the outcomes of simple programs, but it can't, like, perfectly simulate a Python interpreter
and, like, turn a Python program into, like, a trace and get the exact same output.
I also cannot do that, but I find myself able to write Python, so maybe this isn't so bad.
But then if it's something like a live API call to some external service, that means you need to, like, emulate or simulate this entire process
by which that API call is generated.
So, like, anything that requires, like, live data from the real world, it's not going to be able to do.
So, the key insight here is that whenever possible, you want to take the language model's weakest simulators
and replace them with the real deal, so it's going to be a focus in the next lecture after this one.
So, why simulate a Python kernel when you can just run it?
Like, simulating a Python kernel approximately is great for writing code, but in the end, like, you would, you can check the code
by running it in an actual Python kernel to determine what it does.
But then a human thinking for seconds, like, the best simulator we have for that besides language models is actual humans thinking for seconds,
and that comes with a lot of extra baggage.
So, the takeaways in this section are pre-trained models are mostly just kind of alternate universe document generators.
So, weightings of the universe of all possible documents.
And then for instruction models, they will solve, they will answer your wishes, but, like, remember that you should be careful what you wish for.
And then lastly, all models can be agent simulators as part of something that they learn from language modeling,
but their quality is going to vary depending on the agent and depending on the model.
So, I think people probably want, they want the juice, they want the techniques.
So, this section is really mostly going to be a bag of tricks.
So, this is a spicy take from Lilian Wang of OpenAI.
A lot of these prompt engineering papers that you find out there are, like, not actually worth eight pages,
and in fact, a lot of them are, like, 40 pages once you include the appendices,
because these are tricks that can be explained in, like, one or a few sentences,
and the rest is all about benchmarking.
So, like, really in the end, like, these prompt engineering tricks are, like, go-to things to try,
but there's not that much depth here.
I think the core language modeling stuff has some mathematical depth to it,
but then in terms of the, like, fiddly bits that get language models to work for you, it's a lot of hacks.
So, just as an overview, I'm going to first cover some, like, weird things to watch out for
in terms of, like, things people will either suggest or you might believe would be good ideas that are actually not.
And then I'll talk about the emerging playbook.
So, first, the ugly bits.
One, few-shot learning turns out to be not really a great, like, model or, like, approach to prompting.
And then second, like, tokenization is going to mess you up for sure.
So, like, watch out for it and some tricks, tips and tricks for dealing with it.
So, at the beginning, when, like, when people first talked about language models
and how you would, like, put in, put stuff into them to get them to do useful things,
like, it was not at all obvious that a generative language model would be, like, useful for stuff to people.
Like, it was clear that it would learn a lot of, like, intelligent things and, like, maybe mimic intelligence,
but that it would, like, actually be useful was unclear.
And so the GPT-3 paper is actually called, language models are few-shot learners.
And it draws an analogy to the way that during, like, during training, we might, like, pass over a bunch of examples,
run gradient descent and get, and, like, we go through those examples and pairs of, like, 5 plus 8 is 13, or 7 plus 2 is 9.
And during training, we, like, put that information into the weights of the model with gradient descent.
But then, with a large language model, like, GPT-3 in this case, you can put that information into the prompt,
into the context, and the model will learn in context how to do this task.
So that's how it was presented in the paper, that the model was basically, like, learning things,
like math and translation through English to French in its prompt.
And that model hasn't really held up, which is that, like, you can really, if you craft carefully the content of your prompt,
you can often get, like, very, very good performance that, like, matching the effect of having many, many examples in the context,
just by, like, carefully making sure that the model knows exactly what task it's supposed to solve.
So the primary role here is not for the model to learn a new task on the fly, but for the model to be, like, told what the task is.
So rather than doing an example, like, on the left, this, like, French example, English example, French example, English example,
and then ending with an uncompleted one, you can, you can just bake it so that the model knows that what it's supposed to do is provide the right French answer.
And, like, there's been a, there's a number of, like, kind of negative results on, on this, like, models really struggle to move away from what they learned during their pre-training.
So, like, for example, if you put a couple, like, you might want to do sentiment analysis for the language model.
Say, is this a positive statement, a negative statement, or a neutral statement?
And if you take those labels and you just permute them so that now positive things are, are, are to be labeled as negative and negative things are to be labeled as positive,
the GPT-3, the model called that, that, like, launched the idea that language models are few-shot learners, will just basically ignore the labels that you provided
and continue to say that a positive statement, like the acquisition will have a positive impact, should have the label positive rather than negative.
So you even, so you flip the labels, if you do that with a regular neural network and you train it, like, actually train it with gradients, it will immediately pick up that that's, that this is the way the label should be.
And so there's been some follow-up that indicates that this permuted label task is something that the latest language models can do.
So GPT-3, like, this is showing increased amounts of flipped labels for a bunch of different models, different sizes of instruct GPT and GPT-3.
And if the model was, was doing the task, like, perfectly at each point, you would follow that orange line.
And instruct GPT and code DaVinci-2 in particular, like, kind of follow that line.
But they, like, they still don't perfectly do it. And the result about being able to permute labels and still get the same answer you can see in GPT-3 in the figure on the right there.
So, like, and this is, like, one bit. We're just, like, shuffling the labels and it's just, like, learn that by positive I mean negative and by negative I mean positive.
And you need lots of examples in a really capable language model to do it.
So treating the prompt as a way to do this, like, few-shot learning is probably a bad idea.
Then second bit that people often get tripped up on is models don't really see characters, they see tokens.
Like, hello world and it's, like, rotated version where I just add 13 to the index of each character.
Like, it's rotated version.
Oriabha Jubbeck is something that I look at and, like, they look the same to me.
Like, they're just both a sequence of the same number of characters.
For a language model, because those letters in the rotated version are less common, it gets tokenized, like, very differently into many more tokens.
So a lot of people, like, you're sitting at a language model interface and it's, like, it's all language so you start thinking about things you can do with characters.
Like, oh, I could, like, split them and reverse them and all kinds of, like, string operations that you might use, like, Python for.
But language models are actually not very good at that.
And so this is, like, kind of surprising because it's good at, like, creative writing and summarizing but not at things like reversing words.
Peter Wellender showed some, like, nice tricks for solving this problem.
And one of the key ones is, like, if you add spaces between letters, either in the prompt or by asking the language model to do it,
then this changes the tokenization and anything with a space before it and a space after it is going to get kind of tokenized separately.
So a lot of the tokens for the most of the language models, like capable language models, have a space at the beginning and then a letter.
And then when another space follows, that becomes part of another token that looks like space and then, like, letter or several letters.
So, like, that's one trick to get around some of this, like, by pairing coding stuff, this, like, issues of tokenization.
It seems to be slightly resolved with GPT-4, so this is an example from the GPT-4 developer livestream.
So summarize this article into a sentence where every word begins with G.
So, like, because of tokenization, like, every word that begins with G, there's lots of words that begin with G, but their tokenization starts with, like, three letters, not just one.
So it's not very obvious to a language model.
And so this was something that failed quite often.
But GPT-4 can do a decent job at it.
It's the summary of its own description, and it says GPT-4 generates groundbreaking grandiose gains, freely galvanizing generalized AI goals.
And not quite.
So even with the most capable models, these, like, this issue of character level stuff is really hard.
So there's a simple trick here.
It's the same thing with the simulators.
If it's something you can do with traditional programming, like stream manipulation, just do it that way instead of using the language model.
Let's talk about this, like, emerging playbook for using language models.
So what are the, like, core tricks that are the ones you should, like, bring into play immediately when you're starting to use a language model or something?
So language models are really, really good and love formatted text.
Formatted text is much, much easier to predict.
And so the language model is, like, unlikely to start, like, going off on a tangent and doing something else.
Because it's, like, it's got, like, high probability tokens to predict.
So Riley Goodside was a big sort of, like, innovator on this front shared a lot of really cool examples on it.
So if you want to generate, say, a whole Python program and you know the rough outlines of it, but not everything in detail,
just, like, put it in triple back ticks and then take each component and write, like, a little form, like, pseudo code formatted chunk for each.
So, right, like, oh, it should start with a hash bang, it should have dundas, I should have a function, and then I should have, like, some, like, some basic features inside that function.
So you're, you're making, like, you can make use of structured text that's not as, like, rigorously structured as, like, JSON or YAML, but just, like, more structured, like, like, pseudo code and language models will, like, pick it up quite well.
So for this example, like, it generates this nice little snippet of code for calling the OpenAI API.
And, yeah, so the one, the other thing I would pull out here is the, like, triple back ticks trick, this is another little prompt engineering trick.
Models are trained on a lot of stuff from GitHub, and triple back ticks is an important component of markdown that indicates that something is going to be code, or it's also used around pseudo code.
So it sort of puts the model in the universe of documents around computer programs, which is often where you want to be when you are, like, producing an application.
So then this, so this is, like, decomposition by putting it into the structure of text.
You could also add decomposition to your prompt. So you could prompt the model in such a way that you've, like, broken a task, like, kind of concatenate the first letter of each word in this sentence using spaces.
So to, like, break it down in the prompt into a bunch of smaller tasks, those smaller tasks could then be, like, other, you know, other, they could trigger the prompting of another language model.
They could trigger an external tool. That's all stuff that Josh will talk about.
But just in general, you could just break the task down into little pieces and make sure that the language model, like, you know, knows to generate each, each little piece by using that decomposition centered prompt.
But it'd be better if you could, like, automate the construction of that decomposition.
So rather than writing this big structured document or, like, rather than writing some, like, examples of cases where problems are decomposed, you can, like, do something like the self ask trick, which is when you, like, write your initial prompt of examples,
you can say, you can, you can frame it in terms of, like, generic decomposition operations, like, our follow up questions needed here, yes or no.
And then the model will ask query time, it will decide what, what number of follow up questions to ask and, like, how to ask them to get the final answer.
So, like, sort of automating this process of decomposition is one of the key tools for getting language models to be better.
So maybe the most famous, one of the most famous ones of this is reasoning with few shots.
It's getting reasoning out of models, like, reasoning as a way of decomposing problems.
And the most famous one is this chain of thought prompting.
So in the prompt for the model, you include, like, both, like, this is for a question answering model, so it's getting these, like, little math word problems and answering questions and answering the, giving the final answer to that question.
And in the examples that you put in the model's prompt, you write out the reasoning that's highlighted in blue.
So rather than just directly answering Roger's five tennis policy buys two more, each can has three, how many does have now, instead of just directly writing 11, you write out a, like, little trace of reasoning of how you would get there, you show your work.
And so by putting this in the prompt, you're not teaching the model to reason here to be clear. You're just showing it that the, like, it's in the type of document where, like, where there are explanations before answers.
And that causes the model to expend extra computation, sort of generating intermediate thoughts that then make it easier to get the final answer by just looking at the contents of those intermediate thoughts.
And so this, like, works pretty well. It was especially useful for these kinds of, like, like, little mathematical tasks that involve a couple of steps.
But it was, you know, it wasn't really being done by this few shots training, like, it's not like the model was learning everything about reasoning from, like, three word problems from a third grader homework assignment.
It's in there already. And so you just need to find a way to get it to come out.
And so the primary, so the follow up paper to this language models are zero thought reason.
There's just adds, let's think step by step to the end of the answer. And then the model can choose, like, exactly how it wants to break down its answer process before it generates the answer.
And so, like, clearly this capability is, like, already there in the model and we're just, like, eliciting it by careful tuning of our product.
And this let's think step by step thing works, like, very broadly, very similar phrases also work.
Let's think step by step to be sure we get the correct answer. It's a tiny little improvement.
Yeah. I think that was everything I had on that.
So then, and then the last thing that you can do, in addition to doing this, like, like, rolling out and having the model think through its solution step by step, you can also just ask the model to, like, check its work.
So this is like a two stage prompting thing. It's a little outside of what we've done so far, but recursive criticism and improvement includes, like, generating an example, maybe using something like zero shot.
Let's think step by step. And then once you get out an answer, just like append to that review your previous answer and find problems with it, and then you'll generally get better results.
So I think most of this is done with the sort of instruction tuned models that are really good at picking up on things you're asking for, like, you're asking it to find problems with the answer.
But yeah, this is, this is a very, like, using the models to, like, fix their outputs is also a powerful, like, prompting pattern.
Then sort of orthogonally to all of this, you can also, like, ensemble the results of multiple models. This is a statistical Cisco model to probabilistic program. It generates different outputs on different runs.
And so why not just generate, like, 50 different outputs? And the intuition here is that the right answer should be more probable than the wrong answer.
And there are, like, maybe many ways of getting to many different wrong answers, but only a few ways of getting to the one right answer.
So if that's the type of problem that you have, ensembling is likely to work well.
So you take all of the models, like, you take the outputs, like, 50 different responses to the same question, and then you do, like, majority voting.
And so you can just, as you increase the number of generations, the number of members of the ensemble, you find that the quality increases.
So that's this blue line going up into the right.
One tip coming off after this original self-consistency paper is to, like, just inject randomness for greater heterogeneity.
Just, like, re-phase the prompt a little bit, like, even just, like, string operations, like, lowercase, uppercase, that will, like, slightly change the model's behavior.
And in general, it should keep the correct answer the same, but change the wrong answers a lot.
And then you can compose all of these tricks that we've talked about so far.
So you can do few-shot examples that include let's think step-by-step, and then you can ensemble them together, and, like, you can put all of this together, and that will generally increase your performance.
And, like, just as, like, one key example, the combination of few-shot chain of thought and let's think step-by-step matches average human performance on this pretty hard benchmark, big bench hard,
has, like, a lot of difficult problems. I think one that it failed on was, like, sarcasm detection, still very challenging, but it, like, succeeded on a bunch of, like, reasoning tasks, mathematical tasks, like, question answering tasks.
So, yeah, that's a great paper for inspiration on what can be done with this combination of tools.
But each of them has an impact on the costs of what you're doing, so you want to recognize that they can impact both, like, latency and compute.
So few-shot chain of thought will increase your latency because you're putting more information into the, for the model to run over, and that's, so it's going to take longer to generate, and that's going to cost you more, it's more tokens.
It's zero-shot chain of thought adds fewer things to the context, so it has less of an impact on latency and compute, so that's why lots of people have adopted it.
Like, decomposing into subproblems is going to, like, generally increase the length of it. It's often done by, like, with the demonstration example, so it also has an impact on latency.
Ensembling is very cool because that has no impact on latency in principle, like, you can run all of your requests in parallel, but, like, for every parallel request you run with an API service, like, that just increases your compute costs.
It's a little more subtle for if you're running the compute yourself, but it is generally going to, like, linearly scale compute.
Self-criticism is going to massively increase the latency because you're going to, like, ask the model to, like, fix its answer, maybe multiple times, but it doesn't necessarily, like, increase the compute costs as much as something like that.
Okay, so I've hit two clock, and so I'm going to skip my example with theory of mind that just demonstrates how to, like, combine those together. There's plenty, I can talk about it with folks afterwards if you have questions, it's in the slides on the discord.
So yeah, core takeaway there. There's a playbook for prompt engineering. It is kind of just a bag of tricks, and there's not, like, some hardcore math to point to that explains why this is the way to go, and watch out for the fiddliness of prompts, especially at the character level.
Thanks for watching.
