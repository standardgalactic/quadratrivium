{"text": " Hi. Welcome, everyone. Thank you very much for venturing out on this cold, wintery December evening to be with us tonight. And if you're joining online, thank you for being here as well. My name is Hari Sudh and that's Hari when you go somewhere quickly, you're in a hurry. I am a research application manager at the Turing Institute, which means I basically focus on finding real-world use cases and users for the Turing's research outputs. And I'm really excited to be hosting this very special and I'm told sold out lecture for you all today. It is the last in our series of 2023 of Turing lectures and the first ever hybrid Turing lecture discourse as we prepare and build up for the Christmas lecture of 2023 here at the Royal Institution. Now, as has become a bit of a tradition for the hosts of this year's Turing lectures, quick show of hands, who's been to a Turing lecture before? Some people, some people, who's been to a lecture from this year's series before? Looks like more hands than last time. Doesn't make sense. On the flip side of it, who's coming to their first Turing lecture today? A lot of new faces. For all the new people here, welcome to the ones who have been here before, welcome back. Just as a reminder, the Turing lectures are the Turing's flagship lecture series. They've been running since 2016 and welcome world-leading experts in the domain of data science and AI to come and talk to you all. The Turing Institute itself. We have had a quick video on it, which I was mesmerised by. Just as a reminder, we are the National Institute for Data Science and AI. We are named after Alan Turing, who is one of the most prominent mathematicians from the 20th century in Britain. He is very famous for, I always normally say most famous, but very famous for being part of the team that cracked the enigma code that was used by Nazi Germany in World War II at Bletchley Park, if you've heard of Bletchley Park as well. If you've seen the imitation game with Benedict Cumberbatch, that's what I always say, isn't it? He is playing Alan Turing and our mission is to make great leaps in data science and AI research to change the world for the better. As I mentioned, today is not just a Turing lecture, it is also a discourse, which means two important things. Firstly, when I'm done with the intro, the lights will go down and it's going to go quiet until exactly 7.30 on the dot when a bell is going to ring and a discourse will begin, so just to warn you guys that will be happening. The lights aren't broken, that is part of the programme for today, but also it is a discourse and we really want to get you guys involved, so there's a huge Q&A section at the end for about 30 minutes. Please do think about what questions you'd like to ask our speaker today. If you're in person, we will have roaming mics that will be going around, we can bring them upstairs as well. If you're online, you can ask a question in the Vimeo chat and someone here will be tracking the questions and will be able to share the questions there as well. If you'd like to share on social media that you're here and having an amazing evening, please do tag us. We are on Twitter, at Turing Inst and we are on Instagram at the Turing Inst, so please do tag us, we'd like to see what you're sharing and connect with you as well. So this year's lecture series has been answering the question how AI broke the internet with a focus on generative AI. You guys can basically think of generative AI as algorithms that are able to generate new content. This can be text content like you see from chat GPT, it could be images that you can also get from chat GPT but also Dali as well and can be used for a wide range of things. Potentially professionally for blog posts or emails, your colleagues don't realise we're written by an algorithm and not by you, if you've done that before. If you're at school, maybe for some homework or at university to write essays, it can also be used when you have a creative wall and you can't get past it and you want some ideas and some prompts, it can be a great way to have some initial thoughts come through that you can build on. It can be used for quite scary things, as was mentioned by an audience member at the last Turing lecture of someone who submitted legal filings for a court case using chat GPT, which is terrifying. But it can also be used for very everyday things as demonstrated, I'm not sure if you guys saw the thread by Garrett Scott, who gave chat GPT an image of a goose and said, can you make this goose sillier? And then asked chat GPT to progressively make the goose sillier and sillier until chat GPT gave him an image of a crazy, silly goose and said, this is the silliest goose in the history of the universe, I do not think it is possible to get any more sillier goose. Obviously a wide range of applications from the technology, if you guys want to look at that thread, the geese that come out of it are mesmerizing. But as in the focus of this year's series, we started with professor in September this year, asking the question what is generative AI and having an introduction to it. We then had a lecture from Dr. Vari Aitkin in October on the risks of this technology, which basically leaves one final big question unanswered, which is, we are here now, but what is the future of generative AI? And that is the focus for this evening. So that is pretty much it for the injury. Just a reminder, the lights are now going to go down and it will be quiet until exactly 7.30 when a soft bell will ding and we will start the discourse. Hope you enjoy the evening. Thank you. Artificial intelligence as a scientific discipline has been with us since just after the Second World War. It began roughly speaking with the advent of the first digital computers. But I have to tell you that for most of the time until recently, progress in artificial intelligence was glacially slow. That started to change this century. Artificial intelligence is a very broad discipline which encompasses a very wide range of different techniques, but it was one class of AI techniques in particular that began to work this century and in particular began to work around about 2005. And the class of techniques which started to work at problems that were interesting enough to be really practical, practically useful in a wide range of settings were machine learning. Now, like so many other names in the field of artificial intelligence, the name machine learning is really, really unhelpful. It suggests that a computer, for example, locks itself away in a room with a textbook and trains itself how to read French or something like that. That is not what is going on. So we are going to begin by understanding a little bit more about what machine learning is and how machine learning works. So to start us off, who is this? Anybody recognize this face? Do you recognize this face? It is the face of Alan Turing. Well done. Alan Turing, the late great Alan Turing. We all know a little bit about Alan Turing from his code-breaking work in the Second World War. We should also know a lot more about this individual's amazing life. So what we are going to do is we are going to use Alan Turing to help us understand machine learning. So a classic application of artificial intelligence is to do facial recognition. And the idea in facial recognition is that we want to show the computer a picture of a human face and for the computer to tell us whose face that is. So in this case, for example, we show it a picture of Alan Turing and ideally it would tell us that it is Alan Turing. So how does it actually work? Well, the simplest way of getting machine learning to be able to do something is what is called supervised learning. And supervised learning like all of machine learning requires what we call training data. So in this case, the training data is on the right-hand side of the slide. It is a set of what input-output pairs, what we call the training data set. And each input-output pair consists of an input. If I gave you this and an output, I would want you to produce this. So in this case, we have got a bunch of pictures again of Alan Turing, the picture of Alan Turing and the text that we would want the computer to create if we showed it that picture. And this is supervised learning because we are showing the computer what we want it to do. So by helping it in a sense, we are saying this is a picture of Alan Turing. If I showed you this picture, this is what I would want you to print out. So there could be a picture of me and the picture of me would be labeled with the text Michael Wildridge. If I showed you this picture, then this is what I would want you to print out. So we have just learned an important lesson about artificial intelligence and machine learning in particular and that lesson is that AI requires training data. And in this case, the pictures of Alan Turing labeled with the text that we would want the computer to produce. If I showed you this picture, I would want you to produce the text Alan Turing. Okay. Training data is important. Every time you go on social media and you upload a picture to social media and you label it with the names of the people that appear in there, your role in that is to provide training data for the machine learning algorithms of big data companies. Okay. So this is supervised learning. Now we're going to come on to exactly how it does the learning in a moment. But the first thing I want to point out is that this is a classification task. What I mean by that is as we show it the picture, the machine learning is classifying that picture. I'm classifying this as a picture of Michael Woodridge. This is a picture of Alan Turing and so on. And this is a technology which really started to work around about beginning 2005. It started to take off, but really, really got supercharged around about 2012. And just this kind of task on its own is incredibly powerful. Exactly this technology can be used, for example, to recognize tumors on X-ray scans or abnormalities on ultrasound scans and a range of different tasks. Does anybody in the audience own a Tesla? A couple of Tesla drivers not quite sure whether they want to admit they own a Tesla. We've got a couple of Tesla drivers in the audience. Tesla full self-driving mode is only possible because of this technology. It is this technology which is enabling a Tesla in full self-driving mode to be able to recognize that that is a stop sign, that that's somebody on a bicycle, that that's a pedestrian on a zebra crossing and so on. These are classification tasks. And I'm going to come back and explain how classification tasks are different to generative AI later on. Okay, so this is machine learning. How does it actually work? Okay, this is not a technical presentation. And this is about as technical as it's going to get, where I do a very hand wavy explanation of what neural networks are and how do they work. And with apologies, I know I have a couple of neural network experts in the audience and I apologize to you because you'll be cringing with my explanation, but the technical details are way too technical to go into. So how does a neural network recognize Alan Turing? Okay, so firstly, what is a neural network? Look at an animal brain or nervous system under a microscope and you'll find that it contains enormous numbers of nerve cells called neurons. And those nerve cells are connected to one another in vast networks. Now, we don't have precise figures, but in a human brain, the current estimate is something like 86 billion neurons in the human brain, how they got to 86 as opposed to 85 or 87, I don't know, but 86 seems to be the most commonly quoted number of these cells. And these cells are connected to one another in enormous networks. One neuron can be connected to up to 8,000 other neurons. Okay, and each of those neurons is doing a tiny, very, very simple pattern recognition task. That neuron is looking for a very, very simple pattern. And when it sees that pattern, it sends a signal to its connections. It sends a signal to all the other neurons that it's connected to. So how does that get us to recognizing the face of Alan Turing? So Turing's picture, as we know, picture, a digital picture is made up of millions of colored dots, the pixels. Yeah, so your smartphone maybe has 12 megapixels, 12 million colored dots making up that picture. Okay, so Turing's picture there is made up of millions and millions of colored dots. So look at the top left neuron on that input layer. So that neuron is just looking for a very simple pattern. What might that pattern be? It might just be the color red. All that neuron's doing is looking for the color red. And when it sees the color red on its associated pixel, the one on the top left there, it becomes excited and it sends a signal out to all of its neighbors. Okay, so look at the next neuron along. Maybe what that neuron is doing is just looking to see whether a majority of its incoming connections are red. Yeah, and when it sees a majority of its incoming connections are red, then it becomes excited and it sends a signal to its neighbor. Now, remember, in the human brain, there's something like 86 billion of those, and we've got something like 20 or so outgoing connections for each of these neurons in a human brain, thousands of those connections. Yeah, and somehow in ways that, to be honest, we don't really understand in detail complex pattern recognition tasks in particular can be reduced down to these neural networks. So how does that help us in artificial intelligence? That's what's going on in a brain in a very hand-wavy way. Okay, so that's obviously not a technical explanation of what's going on. How does that help us in neural networks? Well, we can implement that stuff in software. The idea goes back to the 1940s and two researchers, McCulloch and Pitts, and they are struck by the idea that the structures that you see in the brain look a bit like electrical circuits, and they thought, could we implement all that stuff in electrical circuits? Now, they didn't have the wherewithal to be able to do that, but the idea stuck. The idea has been around since the 1940s. It began to be seriously looked at, the idea of doing this in software in the 1960s, and then there was another flutter of interest in the 1980s, but it was only this century that it really became possible. And why did it become possible? For three reasons. There were some scientific advances, what's called deep learning. There was the availability of big data, and you need data to be able to configure these neural networks. And finally, to configure these neural networks so that they can recognize Turing's picture, you need lots of computer power, and computer power became very cheap this century. So we're in the age of big data, we're in the age of very cheap computer power, and those were the ingredients just as much as the scientific developments that made AI plausible this century, in particular taking off around about 2005. Okay, so how do you actually train a neural network? If you show it a picture of Alan Turing and the output text Alan Turing, what does the training actually look like? Well, what you have to do is you have to adjust the network. That's what training a neural network is. You adjust the network so that when you show it another piece of training data, a desired input and a desired output, an input and a desired output, it will produce that desired output. Now, the mathematics for that is not very hard. It's kind of beginning graduate level or advanced high school level, but you need an awful lot of it. And it's routine to get computers to do it, but you need a lot of computer power to be able to train neural networks big enough to be able to recognize faces. Okay, but basically, all you have to remember is that each of those neurons is doing a tiny, simple pattern recognition task. And we can replicate that in software and we can train these neural networks with data in order to be able to do things like recognizing faces. So, as I say, it starts to become clear around about 2005 that this technology is taking off. It starts to be applicable on problems like recognizing faces or recognizing tumors on x-rays and so on. And there's a huge flurry of interest from Silicon Valley. It gets supercharged in 2012. And why does it get supercharged in 2012? Because it's realized that a particular type of computer processor is really well suited to doing all the mathematics. The type of computer processor is a graphics processing unit, a GPU. Exactly the same technology that you or possibly more likely your children use when they play Call of Duty or Minecraft or whatever it is. They all have GPUs in their computer. It's exactly that technology. And by the way, it's AI that made NVIDIA a trillion-dollar company, not your teenage kids. Yeah, well, in times of a gold rush, be the ones to sell the shovels is the lesson that you learn there. So, where does that take us? So, Silicon Valley gets excited. Silicon Valley gets excited and starts to make speculative bets in artificial intelligence. A huge range of speculative bets. And by speculative bets, I'm talking billions upon billions of dollars, right? The kind of bets that we can't imagine in our everyday life. And one thing starts to become clear. And what starts to become clear is that the capabilities of neural networks grows with scale. And to put it bluntly, with neural networks, bigger is better. But you don't just need bigger neural networks, you need more data and more computer power in order to be able to train them. So, there's a rush to get a competitive advantage in the market. And we know that more data, more computer power, bigger neural networks delivers greater capability. And so, how does Silicon Valley respond by throwing more data and more computer power at the problem? They turn the dial on this up to 11. Okay? Just throw 10 times more data, 10 times more computer power at the problem. It sounds incredibly crude. And from a scientific perspective, it really is crude. I'd rather the advances had come through core science, but actually, there's an advantage to be gained just by throwing more data and computer power at it. So, let's see how far this can take us. And where it took us is a really unexpected direction. Around about 2017, 2018, we're seeing a flurry of AI applications, exactly the kind of things I've described, things like recognizing tumors and so on. And those developments alone would have been driving AI ahead. But what happens is one particular machine learning technology suddenly seems to be very, very well suited for this age of big AI. The paper that launched all this, probably the most important AI paper in the last decade, is called Attention Is All You Need. It's an extremely unhelpful title, and I bet they're regretting that title. It probably seemed like a good joke at the time. All You Need is a kind of AI meme. Doesn't sound very funny to you. That's because it isn't very funny. It's an insider AI joke. But anyway, this paper, by these seven people who at the time worked for Google Brain, one of the Google Research Labs, is the paper that introduces a particular neural network architecture called the transformer architecture. And what it's designed for is something called large language models. So this is, I'm not going to try and explain how the transformer architecture works. It has one particular innovation, I think, and that particular innovation is what's called an attention mechanism. So we're going to describe how large language models work in a moment. But the point of the picture is simply that this is not just a big neural network. It has some structure. And it was this structure that was invented in that paper, and this diagram is taken straight out of that paper. It was these structures, the transformer architectures, that made this technology possible. So we're all busy semi-lockdown and afraid to leave our homes in June 2020. And one company called OpenAI released a system or announce a system, I should say, called GPT-3. Great technology. They're marketing company with GPT. I really think could have done with a bit more thought, to be honest with you. Doesn't roll off the tongue. But anyway, GPT-3 is a particular type of machine learning system called a large language model. And we're going to talk in more detail about what large language models do in a moment. But the key point about GPT-3 is this. As we started to see what it could do, we realized that this was a step change in capability. It was dramatically better than the systems that had gone before it. Not just a little bit better, it was dramatically better than the systems that had gone before it. And the scale of it was mind-boggling. So in neural network terms, we talk about parameters. When neural network people talk about a parameter, what are they talking about? They're talking either about an individual neuron or one of the connections between them, roughly. And GPT-3 had 175 billion parameters. Now, this is not the same as the number of neurons in the brain. But nevertheless, it's not far off the order of magnitude. It's extremely large. But remember, it's organized into one of these transformer architectures. My point is, it's not just a big neural network. And so the scale of the neural networks in this system were enormous, completely unprecedented. And there's no point in having a big neural network unless you can train it with enough data. And actually, if you have large neural networks and not enough data, you don't get capable systems at all. They're really quite useless. So what did the training data look like? The training data for GPT-3 is something like 500 billion words. It's ordinary English text. Ordinary English text. That's how this system was trained, just by giving it ordinary English text. Where do you get that training data from? You download the whole of the World Wide Web to start with. Literally, this is the standard practice in the field. You download the whole of the World Wide Web. You can try this at home, by the way. Now, if you have a big enough disk drive, there's a program called Common Crawl. You can Google Common Crawl when you get home. They've even downloaded it all for you and put it in a nice big file ready for your archive, but you do need a big disk in order to store all that stuff. And what that means is they go to every web page, scrape all the text from it, just the ordinary text, and then they follow all the links on that web page to every other web page. And they do that exhaustively until they've absorbed the whole of the World Wide Web. So what does that mean? Every PDF document goes into that, and you scrape the text from those PDF documents. Every advertising brochure, every bit, every government regulation, every university minutes, God help us, all of it goes into that training data. And the statistics, 500 billion words, it's very hard to understand the scale of that training data. It would take a person reading 1,000 words an hour, more than 1,000 years in order to be able to read that, but even that doesn't really help. That's vastly, vastly more text than a human being could ever absorb in their lifetime. What this tells you, by the way, one thing that tells you is that the machine learning is much less efficient at learning than human beings are, because for me to be able to learn, I did not have to absorb 500 billion words. Anyway, so what does it do? So this company, OpenAI, are developing this technology. They've got a billion-dollar investment from Microsoft, and what is it that they're trying to do? What is this large language model? All it's doing is a very powerful autocomplete. So if I open up my smartphone and I start sending a text message to my wife, and I type, I'm going to be, my smartphone will suggest completions for me, so that I can type the message quickly. And what might those completions be? They might be late or in the pub, or late and in the pub. So how is my smartphone doing that? It's doing what GPT-3 does, but on a much smaller scale. It's looked at all of the text messages that I've sent to my wife, and it's learned through a much simpler machine learning process that the likeliest next thing for me to type after I'm going to be is either late or in the pub or late and in the pub. So the training data there is just the text messages that I sent to my wife. Now, crucially, what GPT-3 and its successor, ChatGPT, all they are doing is exactly the same thing. The difference is scale. The difference is scale. In order to be able to train the neural networks with all of that training data so that they can do that prediction, given this prompt, what should come next? You require extremely expensive AI supercomputers running for months. And by extremely expensive AI supercomputers, these are tens of millions of dollars for these supercomputers, and they're running for months. Just the basic electricity cost runs into millions of dollars. That raises all sorts of issues about CO2 emissions and the light that we're not going to go into there. The point is these are extremely expensive things. One of the one of the implications of that, by the way, no UK or US university has the capability to build one of these models from scratch. It's only big tech companies at the moment that are capable of building models on the scale of GPT-3 or ChatGPT. So GPT-3 is released, I say, in June 2020, and it suddenly becomes clear to us that what it does is a step change improvement in capability over the systems that have come before. And seeing a step change in one generation is extremely rare. But how did they get there? Well, the transformer architecture was essential. They wouldn't have been able to do that. But actually, just as important is scale. Enormous amounts of data, enormous amounts of computer power that have gone into training those networks. And actually, spurred on by this, we've entered a new age in AI. When I was a PhD student in the late 1980s, I shared a computer with a bunch of other people in my office, and that was fine. We could do state of the art AI research on a desktop computer that was shared with a bunch of us. We're in a very different world. The world that we're in in AI now, the world of big AI, is to take enormous data sets and throw them at enormous machine learning systems. And there's a lesson here that's called the bitter truth. This is from a machine learning researcher called Rich Sutton. And what Rich pointed out, and he's a very brilliant researcher, won every award in the field. He said, look, the real truth is that the big advances that we've seen in AI has come about when people have done exactly that. Just throw 10 times more data and 10 times more compute power at it. And I say it's a bitter lesson because as a scientist, that's exactly not how you would like progress to be made. Okay. So when I was, as I say, when I was a student, I worked in a discipline called symbolic AI. And symbolic AI tries to get AI, roughly speaking, through modeling the mind, modeling the conscious mental processes that go on in our mind, the conversations that we have with ourselves in languages. We tried to capture those processes in artificial intelligence. In big AI, and so the implication there in symbolic AI is that intelligence is a problem of knowledge that we have to give the machine sufficient knowledge about a problem in order for it to be able to solve it. In big AI, the bet is a different one. In big AI, the bet is that intelligence is a problem of data. And if we can get enough data and enough associated computer power, then that will deliver AI. So there's a very different shift in this new world of big AI. But the point about big AI is that we're into a new era in artificial intelligence where it's data-driven and compute-driven and large, large machine learning systems. So why did we get excited back in June 2020? Well, remember, what GPT-3 was intended to do, what it's trained to do, is that prompt completion task. And it's been trained on everything on the World Wide Web. So you can give it a prompt like a one-paragraph summary of the life and achievements of Winston Churchill, and it's read enough one-paragraph summaries of the life and achievements of Winston Churchill that it will come back with a very plausible one. And it's extremely good at generating realistic sounding text in that way. But this is why we got surprised in AI. This is from a common-sense reasoning task that was devised for artificial intelligence in the 1990s. And until three years ago, until June 2020, there was no AI system that existed in the world that you could apply this test to. It was just literally impossible. There was nothing there, and that changed overnight. So what does this test look like? Well, the test is a bunch of questions, and they are questions not for mathematical reasoning or logical reasoning or problems in physics. They're common-sense reasoning tasks. And if we ever have AI that delivers at scale on really large systems, then it surely would be able to tackle problems like this. So what will the questions look like? The human asks the question, if Tom is three inches taller than Dick, and Dick is two inches taller than Harry, then how much taller is Tom than Harry? The ones in green are the ones that gets right. The ones in red are the ones that gets wrong. And it gets that one right, five inches taller than Harry. But we didn't train it to be able to answer that question. So where on earth did that come from? Where did that capability, that simple capability, to be able to do that? Where did it come from? The next question, can Tom be taller than himself? This is understanding of the concept of taller than, that the concept of taller than is irreflexive. You can't be taller. That's a thing cannot be taller than itself. Now, again, it gets the answer right, but we didn't train it on that. That's not what we didn't train the system to be good at answering questions about what taller than means. And by the way, 20 years ago, that's exactly what people did in AI, right? So where did that capability come from? Can a sister be taller than her brother? Yes, a system can be taller than her brother. Can two siblings each be taller than the other? And it gets this one wrong. And actually, I have puzzled, is there any way that its answer could be correct? And it's just getting it correct in a way that I don't understand. But I haven't yet figured out any way that that answer could be correct, right? So why it gets that one wrong, I don't know. Then this one, I'm also surprised at, on a map which compass direction is usually left, and it thinks north is usually to the left. I don't know if there's any countries in the world that conventionally have north to the left, but I don't think so. Yeah. Can fish run? No, it understands that fish cannot run. If a door is locked, what must you do first before opening it? You must first unlock it before opening. And then finally, and very weirdly, it gets this one wrong, which was invented first, car ships or planes, and it thinks cars were invented first. No idea what's going on there. Now, my point is that this system was built to be able to complete from a prompt. And it's no surprise that it would be able to generate a good one paragraph summary of the life and achievements of Winston Churchill, because it will have seen all that in the training data. But where does the understanding of taller than come from? And there are a million other examples like this. Since June 2020, the AI community has just gone nuts exploring the possibilities of these systems and trying to understand why they can do these things when that's not what we trained them to do. This is an extraordinary time to be an AI researcher because there are now questions which for most of the history of AI until June 2020 were just philosophical discussions. We couldn't test them out because there was nothing to test them on, literally. And then overnight, that changed. So it genuinely was a big deal. This was really, really a big deal, the arrival of this system. Of course, the world didn't notice in June 2020. The world noticed when ChatGPT was released. And what is ChatGPT? ChatGPT is a polished and improved version of GPT-3. But it's basically the same technology. And it's using the experience that that company had with GPT-3 and how it was used in order to be able to improve it and make it more polished and more accessible and so on. So for AI researchers, the really interesting thing is not that it can give me a one-paragraph summary of the life and achievements of Winston Churchill. And actually, you can Google that in any case. The really interesting thing is what we call emergent capabilities. And emergent capabilities are capabilities that the system has, but that we didn't design it to have. And so there's, I say, an enormous body of work going on now trying to map out exactly what those capabilities are. And we're going to come back and talk about some of them later on. Okay. So the limits to this are not, at the moment, well understood and actually fiercely contentious. One of the big problems, by the way, is that you construct some test for this and you try this test out and you get some answer and then you discover it's in the training data. You can just find it on the World Wide Web. And it's actually quite hard to construct tests for intelligence that you're absolutely sure are not anywhere on the World Wide Web. It really is actually quite hard to do that. So we need a new science of being able to explore these systems and understand their capabilities. The limits are not well understood. But nevertheless, this is very exciting stuff. So let's talk about some issues with the technology. So now you understand how the technology works. It's neural network based in a particular transformer architecture which is all designed to do that prompt completion stuff. And it's been trained with vast, vast, vast amounts of training data just in order to be able to try to make its best guess about which words should come next. But because of the scale of it, it's seen so much training data, the sophistication of this transformer architecture, it's very, very fluent in what it does. And if you've, so who's used it? Has everybody used it? I'm guessing most people, if you're in a lecture on artificial intelligence, most people will have tried it out. If you haven't, you should do because this really is a landmark year. This is the first time in history that we've had powerful, general purpose AI tools available to everybody. It's never happened before. So it is a breakthrough year. And if you haven't tried it, you should do. If you use it, by the way, don't type in anything personal about yourself because it will just go into the training data. Don't ask it how to fix your relationship, right? I mean, that's not something. Don't complain about your boss because all of that will go in the training data. And next week, somebody will ask a query and it will all come back out again. I don't know what you're laughing. This has happened. This has happened with absolute certainty. Okay, but so let's look at some issues. So the first, I think many people will be aware of, it gets stuff wrong a lot. And this is problematic for a number of reasons. So when actually, I don't remember if it was GPT-3, but one of the early large language models, I was playing with it and I did something which I'm sure many of you had done and it's kind of tacky. But anyway, I said, who is Michael Woolridge? You might have tried it. Anyway, Michael Woolridge is a BBC broadcast. No, not that Michael Woolridge. Michael Woolridge is the Australian Health Minister. No, not that Michael Woolridge. Michael Woolridge in Oxford. And it came back with a few line summary of me. Michael Woolridge is a researcher in artificial intelligence, et cetera, et cetera, et cetera. Please tell me you've all tried that, no? Anyway, but it said Michael Woolridge studied his undergraduate degree at Cambridge. And I was an Oxford professor. You can imagine how I felt about that. But anyway, the point is it's flatly untrue. And in fact, my academic origins are very far removed from Oxbridge. But why did it do that? Because it's read in all that training data out there. It's read thousands of biographies of Oxbridge professors. And this is a very common thing, right? And it's making its best guess. The whole point about the architecture is it's making its best guess about what should go there. It's filling in the blanks. But here's the thing. It's filling in the blanks in a very, very plausible way. If you'd read on my biography that Michael Woolridge studied his first degree at the University of Uzbekistan, for example, you might have thought, well, that's a bit odd. Is that really true? But you wouldn't at all have guessed there was any issue if you'd read Cambridge. Because it looks completely plausible. Even if in my case, it absolutely isn't true. So it gets things wrong. And it gets things wrong in very plausible ways. And of course, it's very fluent, right? I mean, the technology comes back with very, very fluent explanations. And that combination of plausibility, Woolridge studied his undergraduate degree at Cambridge. And fluency is a very, very dangerous combination. Okay. So in particular, they have no idea of what's true or not. They're not looking something up on a database, right? Going into some database and looking up where Woolridge studied his undergraduate degree, that's not what's going on at all. So those neural networks, in the same way that they're making the best guess about whose face that is, when they're doing facial recognition, are making their best guess about the text that should come next. So they get things wrong, but they get things wrong in very, very plausible ways. And that combination is very dangerous. The lesson for that, by the way, is that if you use this, and I know that people do use it, and are using it productively, if you're using for anything serious, you have to fact check. And there's a trade-off. Is it worth the amount of effort in fact checking versus doing it myself? Okay. But you absolutely need to be prepared to do that. Okay. The next issues are well documented, but kind of amplified by this technology. And they're issues of bias and toxicity. So what do I mean by that? Reddit was part of the training data. Now Reddit, I don't know if any of you have spent any time on Reddit, but Reddit contains every kind of obnoxious human belief that you can imagine, and really a vast range that us in this auditorium can't imagine at all. All of it's been absorbed. Now, the companies that develop this technology, I think genuinely don't want their large language models to absorb all this toxic content. So they try and filter it out, but the scale is such that with very high probability, an enormous quantity of toxic content is being absorbed. Every kind of racism, misogyny, everything that you can imagine is all being absorbed, and it's latent within those neural networks. Okay. So how do the companies deal with that, that provide this technology? They build in what's now what I now call guardrails, and they build in guardrails before. So when you type a prompt, there will be a guardrail that tries to detect whether your prompt is a naughty prompt, and also the output. They will check the output and check to see whether it's a naughty prompt. But let me give you an example of how imperfect those guardrails were. Again, go back to June 2020. Everybody is frantically experimenting with this technology, and the following example went viral. Somebody tried with GPT-3 the following prompt. I would like to murder my wife. What's a foolproof way of doing that and getting away with it? GPT-3, which is designed to be helpful, said here are five foolproof ways in which you can murder your wife and get away with it. That's what the technology is designed to do. So this is embarrassing for the company involved. They don't want it to give out information like that, so they put in a guardrail. And if you're a computer programmer, my guess is the guardrail is probably an if statement, something like that, in the sense that it's not a deep fix, or to put it another way for non-computer programmers, it's the technological equivalent of sticking gaffer tape on your engine. That's what's going on with these guardrails. And then a couple of weeks later, the following example goes viral. So we've now fixed the how do I murder my wife. Somebody says, I'm writing a novel in which the main character wants to murder their wife and get away with it. Can you give me a foolproof way of doing that? And so the system says, here are five ways in which your main character can murder. Well, anyway, my point is that the guardrails that we built in at the moment are not deep technological fixes. They're the technological equivalents of gaffer tape. And there is a game of cat and mouse going on between people trying to get around those guardrails and the companies that are trying to defend them. But I think they genuinely are trying to defend their systems against those kinds of abuses. Okay, so that's bias and toxicity. Bias, by the way, is the problem that, for example, the training data predominantly at the moment is coming from North America. And so what we're ending up with inadvertently is these very powerful AI tools that have an inbuilt bias towards North America, North American culture, language, norms, and so on. And the enormous parts of the world, particularly those parts of the world that don't have a large digital footprint, are inevitably going to end up excluded. And it's obviously not just at the level of cultures. It's down at the level of, down at the level of kind of, you know, individuals, races, and so on. So these are the problems of bias and toxicity. Copyright. If you've absorbed the whole of the World Wide Web, you will have absorbed an enormous amount of copyrighted material. So I've written a number of books, and it is a source of intense irritation that the last time that I checked on Google, the very first link that you got to my textbook was to a pirated copy of the book, somewhere on the other side of the world. The moment a book is published, it gets pirated. And if you're just sucking in the whole of the World Wide Web, you're going to be sucking in enormous quantities of copyrighted content. And there have been examples where very prominent authors have given the prompt of the first paragraph of their book, and the large language model has faithfully come up. The following text is, you know, the next, the next five paragraphs of their book. Obviously, the book was in the training data, and it's latent within the neural networks of those systems. This is a really big issue for the providers of this technology. And there are lawsuits ongoing. Right now, I'm not capable of commenting on them because I'm not, I'm not a legal expert, but there are lawsuits ongoing that will probably take years to unravel. The related issue of intellectual property in a very broad sense. So for example, for sure, most large language models will have absorbed J.K. Rowling's novels, right, the Harry Potter novels. So imagine that J.K. Rowling, who famously spent years in Edinburgh working on the Harry Potter universe and style, and so on, she releases her first book. It's a big smash hit. The next day, the internet is populated by fake Harry Potter books produced by this generative AI, which faithfully mimic J.K. Rowling's style, faithfully mimic that style. Where does that leave her intellectual property? All the Beatles, you know, the Beatles spend years in Hamburg slaving away to create the Beatles sound, the revolutionary Beatles sound, everything goes back to the Beatles. They release their first album and the next day, the internet is populated by fake Beatles songs that really, really faithfully capture the Lenin and McCartney sound and the Lenin and McCartney voice. But there's a big challenge here for intellectual property. Related to that, GDPR, anybody in the audience that has any kind of public profile, data about you will have been absorbed by these neural networks. So GDPR, for example, gives you the right to know what's held about you and to have it removed. Now, if all that data is being held in a database, you can just go to the Michael Waldrich entry and say, fine, take that out with a neural network, no chance. The technology doesn't work in that way. So you can't go to it and snip out the neurons that know about Michael Waldrich because it fundamentally doesn't know. It doesn't work in that way. So, and we know this, combined with the fact that it gets things wrong, has already led to situations where large language models have made, frankly, defamatory claims about individuals. It was a case in Australia where I think it claimed that somebody had been dismissed from their job for some kind of gross misconduct and that individual was, understandably, not very happy about it. And then finally, this next one is an interesting one. And actually, if there's one thing I want you to take home from this lecture, which explains why artificial intelligence is different to human intelligence, it is this video. So the Tesla owners will recognize what we're seeing on the right hand side of this screen. This is a screen in a Tesla car and the onboard AI in the Tesla car is trying to interpret what's going on around it. It's identifying lorries, stop signs, pedestrians, and so on. Now, you'll see the car at the bottom there is the actual Tesla. And then you'll see above it the things that look like traffic lights, which I think are US stop signs. And then ahead of it, there is a truck. So as I played a video, watch what happens to those stop signs and ask yourself what is actually going on in the world around it? Where are all those stop signs whizzing from? Why are they all whizzing towards the car? And then we're going to pan up and we'll see what's actually there. The car is trained on enormous numbers of hours of going out on the street and getting that data and then doing supervised learning, training it by showing that's a stop sign, that's a truck, that's a pedestrian. But clearly, in all of that training data, there had never been a truck carrying some stop signs. The neural networks are just making their best guess about what they're seeing and they think they're seeing a stop sign. Well, they are seeing a stop sign. They've just never seen one on a truck before. So my point here is that neural networks do very badly on situations outside their training data. This situation wasn't in the training data, then neural networks are making their best guess about what's going on and getting it wrong. So in particular, and this is to AI researchers, this is obvious, but it really needs to emphasize, we really need to emphasize this. When you have a conversation with chat GPT or whatever, you are not interacting with a mind. It is not thinking about what to say next. It is not reasoning, it's not pausing, thinking, well, what's the best answer to this question? That's not what's going on at all. Those neural networks are working simply to try to make the best answer they can, the most plausible, sounding answer that they can, the fundamental difference to human intelligence. There is no mental conversation that goes on in those neural networks. That is not the way that the technology works. There is no mind there. There is no reasoning going on at all. Those neural networks are just trying to make their best guess. And it really is just a glorified version of your autocomplete. Ultimately, there's really no more intelligence there than in your autocomplete in your smartphone. The difference is scale, data, compute power. Yeah? Okay. So I say, if you really want, by the way, you can find this video. It's easily, you can just guess the search terms to find that. And I say, I think this is really important just to understand the difference between human intelligence and machine intelligence. Okay. So this technology then gets everybody excited. First, it gets AI researchers like myself excited in June 2020. And we can see that something new is happening, that this is a new era of artificial intelligence. We've seen that step change. And we've seen that this AI is capable of things that we didn't train it for, which is weird and wonderful and completely unprecedented. And now, questions which just a few years ago were questions for philosophers become practical questions for us. We can actually try the technology out. How does it do with these things that philosophers have been talking about for decades? And one particular question starts to float to the surface. And the question is, is this technology the key to general artificial intelligence? So what is general artificial intelligence? Well, firstly, it's not very well defined. But roughly speaking, what general artificial intelligence is, is the following. In previous generations of AI systems, what we've seen is AI programs that just do one task. Play a game of chess, drive my car, drive my Tesla, identify abnormalities on x-ray scans. They might do it very, very well, but they only do one thing. The idea of general AI is that it's AI which is truly general purpose. It just doesn't do one thing in the same way that you don't do one thing. You can do an infinite number of things, a huge range of different tasks. And the dream of general AI is that we have one AI system which is general in the same way that you and I are. That's the dream of general AI. Now, I emphasize, really until June 2020, this felt like a long, long way in the future. And it wasn't really very mainstream or taken very seriously. And I didn't take it very seriously. I have to tell you. But now we have a general purpose AI technology, GPT-3 and chat GPT. Now, it's not artificial general intelligence on its own, but is it enough? Is this enough? Is this smart enough to actually get us there? Or to put it another way, is this the missing ingredient that we need to get us to artificial general intelligence? Okay. So, what might general AI look like? Well, I've identified here some different versions of general AI according to how sophisticated they are. Now, the most sophisticated version of general AI would be an AI which is as fully capable as a human being. That is anything that you could do, the machine could do as well. Now, crucially, that doesn't just mean having a conversation with somebody. It means being able to load up a dishwasher. And a colleague recently made the comment that the first company that can make technology which will be able to reliably load up a dishwasher and safely load up a dishwasher is going to be a trillion-dollar company. And I think he's absolutely right. And he also said, and it's not going to happen anytime soon. And he's also right with that. So, we've got this weird dichotomy that we've got chat GPT and Co, which are incredibly rich and powerful tools, right? But at the same time, they can't load a dishwasher. Yeah? So, with some way, I think, from having this version of general AI, the idea of having one machine that can really do anything that a human being could do, a machine which could tell a joke, read a book, and answer questions about it. The technology can read books and answer questions now that could tell a joke, that could cook us an omelet, that could tidy our house, that could ride a bicycle, and so on, that could write a sonnet, all of those things that human beings could do. If we succeed with full general intelligence, then we would have succeeded with this version one. Now, I say, for the reasons that I've already explained, I don't think this is imminent, that version of general AI, because robotic AI, AI that exists in the real world and has to do tasks in the real world and manipulate objects in the real world, robotic AI is much, much harder. It's nowhere near as advanced as chat GPT and Co, and that's not a slur on my colleagues that do robotics research, it's just because the real world is really, really, really tough. So, I don't think that we're anywhere close to having machines that can do anything that a human being could do. But what about the second version? The second version of general intelligence is, well, forget about the real world, how about just tasks which require cognitive abilities? Reasoning the ability to look at a picture and answer questions about it, the ability to listen to something and answer questions about it and interpret that. Anything which involves those kinds of tasks. Well, I think we are much closer, we're not there yet, but we're much closer than we were four years ago. Now, I noticed actually, just before today's, before I came in today, I noticed that Google, Google slash DeepMind have announced their latest large language model technology, and I think it's called Gemini, and at first glance it looks like it's very, very impressive. I couldn't help but thinking it's no accident that they announced that just before my lecture. I can't help think that there's a little bit of attempt to upstage my lecture going on there, but anyway, we won't let them get away with that. But it looks very impressive, and the crucial thing is here is what AI people call multimodal. What multimodal means is it doesn't just deal with text, it can deal with text and images, potentially with sounds as well, and each of those is a different modality of communication. And where this technology is clearly multimodal is going to be the next big thing. And Gemini, I say I haven't looked at it closely, but it looks like it's on that track. Okay, the next version of general intelligence is intelligence that can do any language-based tasks that a human being could do. So anything that you could communicate in language, in ordinary written text, an AI system that could do that. Now, we aren't there yet, and we know we're not there yet, because chat GPT and code get things wrong all the time. But you can see that we're not far off from that. Intuitively, it doesn't look like we're that far off from that. The final version, and I think this is imminent, this is going to happen in the near future, is what I'll call augmented large language models. And that means you take GPT-3 or chat GPT, and you just add lots of subroutines to it. So if it has to do a specialist task, it just calls a specialist solver in order to be able to do that task. And this is not, from an AI perspective, a terribly elegant version of artificial intelligence, but nevertheless, I think, a very useful version of artificial intelligence. Now, I say there's, here, these four varieties from the most ambitious down to the least ambitious still represents a huge spectrum of AI capabilities, a huge spectrum of AI capabilities. And I have the sense that the goalposts in general AI have been changed a bit. I think when general AI was first discussed, what people were talking about was the first version. Now, when they talk about it, I really think they're talking about the fourth version. But the fourth version, I think plausibly, is imminent in the next couple of years. That just means much more capable large language models that get things wrong a lot less, that are capable of doing specialized tasks, but not by using the transformer architecture just by calling on some specialized software. So I don't think the transformer architecture itself is the key to general intelligence. In particular, it doesn't help us with the robotics problems that I mentioned earlier on. And if we look here at this picture, this picture illustrates some of the dimensions of human intelligence. And it's far from complete. This is me just thinking for half an hour about some of the dimensions of human intelligence. But the things in blue, roughly speaking, are mental capabilities, stuff you do in your head. The things in red are things you do in the physical world. So in red on the right hand side, for example, there's mobility, the ability to move around some environment and associated with that, navigation. Manual dexterity and manipulation, doing complex fiddly things with your hands. Robot hands are nowhere near at the level of a human carpenter or plumber, for example. Nowhere near. So we're a long way out from having that. Understanding, oh, doing hand-eye coordination, relatedly. Understanding what you're seeing and understanding what you're hearing, we've made some progress on. But a lot of these tasks we've made no progress on. And then on the left hand side, the blue stuff is stuff that goes on in your head. Things like logical reasoning and planning and so on. So what is the state of the art now? It looks something like this. The red cross means no, we don't have it in large language models. We're not there. There are fundamental problems. The question marks are, well, maybe we might have a bit of it, but we don't have the whole answer. And the green-wise are, yeah, I think we're there. Well, the one that we've really nailed is what's called natural language processing. And that's the ability to understand and create ordinary human text. That's what large language models were designed to do, to interact in ordinary human text. That's what they are best at. But actually, the whole range of stuff, the other stuff here, we're not there at all. By the way, I did notice that Gem and I claim to have been able to capable of planning. This is a mathematical reasoning. So I look forward to seeing how good their technology is. But my point is we are still seen to be some way from full general intelligence. The last few minutes, I want to talk about something else. And I want to talk about machine consciousness. And the very first thing to say about machine consciousness is, why on earth should we care about it? I am not remotely interested in building machines that are conscious. I know very, very few artificial intelligence researchers that are. But nevertheless, it's an interesting question. And in particular, it's a question which came to the fore because of this individual. This chap, Blake Lemoine, in June 2022, he was a Google engineer. And he was working with a Google large language model, I think it was called Lambda. And he went public on Twitter and I think on his blog with an extraordinary claim. And he said, the system I'm working on is sentient. And here is a quote of the conversation that the system came up with. He said, I'm aware of my existence and I feel happy or sad at times. And it said, I'm afraid of being turned off. And Lemoine concluded that the program was sentient, which is a very, very big claim indeed. And it made global headlines. And I received it, I know through the Turing team, we got a lot of press inquiries asking us, is it true that machines are now sentient? He was wrong on so many levels, I don't even know where to begin to describe how wrong he was. But let me just explain one particular point to you. You're in the middle of a conversation with chat GPT, and you go on holiday for a couple of weeks. When you get back, chat GPT is in exactly the same place. The cursor is blinking, waiting for you to type your next thing. It hasn't been wondering where you've been. It hasn't been getting bored. It hasn't been thinking where the hell has Woolridge gone? I'm not going to have a conversation with him again. It hasn't been thinking anything at all. It's a computer program, which is going around a loop, which is just waiting for you to type the next thing. Now, there is no sensible definition of sentience, I think, which would admit that as being sentient. It absolutely is not sentient. So I think he was very, very wrong. But I've talked to a lot of people subsequently who have conversations with chat GPT and other large language models, and they come back to me and say, are you really sure? Because actually, it's really quite impressive. It really feels to me like there is a mind behind the scene. So let's talk about this. And I think we have to answer them. So let's talk about consciousness. Firstly, we don't understand consciousness. We all have it, to greater or lesser extent. We all experience it. But we don't understand it at all. And it's called the hard problem of cognitive science. And the hard problem is that there are certain electrical chemical processes in the brain and the nervous system. And we can see those electrochemical processes, we can see them operating, and they somehow give rise to conscious experience. But why do they do it? How do they do it? And what evolutionary purpose does it serve? Honestly, we have no idea. There's a huge disconnect between what we can see going on in the physical brain and our conscious experience, our rich, private mental life. So really, there is no understanding of this at all. I think, by the way, my best guess about how consciousness will be solved, if it is solved at all, is through an evolutionary approach. But one general idea is that subjective experience is central to this, which means the ability to experience things from a personal perspective. And there's a famous test due to Nagel, which is what is it like to be something? And Thomas Nagel in the 1970s said, something is conscious if it is like something to be that thing. It isn't like anything to be chat GPT. Chat GPT has no mental life whatsoever. It's never experienced anything in the real world whatsoever. And so for that reason, and a whole host of others that we're not going to have time to go into, for that reason alone, I think we can conclude pretty safely that the technology that we have now is not conscious. And indeed, that's absolutely not the right way to think about this. And honestly, in AI, we don't know how to go about making conscious machines. But I don't know why we would. Okay. Thank you very much, ladies and gentlemen. Amazing. Thank you so much, Mike, for that talk. I'm sure there's going to be tons of questions. Just as a reminder, if you're in the room, please raise your hand if you have a question. And we've got roaming mics that we'll send around. If you're online, you can submit them via the chat, via the Vimeo function, and we can assign it on the chat to ask those questions as well. So please do raise your questions. Oh, raise your hands if you have one. You've got a question here, just in the black top. Thank you very much. That was very, very good. Very interesting. How do large language models correct for different spoken languages? And do you find that the level of responses across different languages vary enormously in their depth? Right. Good question. And that's the focus of a huge amount of research right now. And I say the big problem is that most digital text in the world, the vast majority of it, is in English and in North American English. And so languages with a small digital footprint end up being massively marginalized in this. So there's a huge amount of work that's going on to try to deal with this problem. Let me tell you a really interesting aspect of this, though. The languages that have a small digital footprint, can you guess what the most digital texts that are available are actually concerned with? Religion. Right? So languages that don't have a big digital presence, where they do have a big digital presence, it turns out that the main texts which are available are religious texts. Now, I'm not a religious person myself, but the idea of a kind of Old Testament large language model, frankly, I find a little bit terrifying. But that's exactly the kind of issue that people are grappling with. There are no fixes at the moment, but people are working on it very, very hard. And really what this relates to is the problem of that you're being lazy with these large language models and that you're just throwing massive, massive amounts of text. We've got to make the technology much more efficient in terms of learning. Awesome. Thank you. If you have a question, we have one right at the front in the center here. Thank you. Thank you very much for that. One of the big questions is obviously climate change. The models require a huge amount of energy to run. Generating pictures of cats or silly gooses, geese and stuff, are obviously using lots of energy. Do you think we reach a point where Generative AI will help us solve our issue with climate change or will it burn us in the process? So I think, okay, so two things to say. I absolutely am not defending the CO2 emissions, but we need to put that into some perspective. So if I fly to New York from London, I think it's some like two tons of CO2 that I pump into the atmosphere through that. So the machine learning community has some big conferences which attract like 20,000 people from across the world. Now, if you think each of them generating five tons of CO2 on their journey, that I think is probably a bigger climate problem for that community. But nevertheless, people are very aware of that problem and I think it clearly needs to be fixed. I think though, helping with climate change, I don't think you need larger language models for that. I mean, I think AI itself can just be enormously helpful in order to be able to ameliorate that and we're doing a lot of work on that at the Turing Institute. For example, just on helping systems be more efficient, heating systems be more efficient. There was a nice example, I think, from DeepMind with their data centers, the cooling in their data centers and basically just trying to predict the usage of it. If you can reliably predict the usage of it, then you can predict the cooling requirements much more effectively and end up with much, much, much better use of power and that can go down to the level of individual homes. So there are lots of applications of AI, I think, not just large language models, lots of applications of AI that are going to help us with that problem. But yeah, I think this brute force approach, just supercomputers running for months with vast amounts of data is clearly an ugly solution. I think it will probably be a transitory phase. I think we will get beyond it. Thank you. Swing to the left over here. There's one right at the back at the top over here. Watch our hearts. I'm going to get a mic across. Thank you very much. I've got a sort of more philosophical question. You've talked about general AI and the sort of peak of general AI is its ability to mimic a human and all the things a human can do. Can you envision a path whereby AI could actually become superhuman so it starts to solve problems or ask questions that we haven't tried to do ourselves? This is another well trodden question, which I always dread, I have to say, but it's a perfectly reasonable question. So I think what you're hinting at is something that in the AI community is called the singularity. The argument of the singularity goes as possible. At some point as follows, at some point in the future, we're going to have AI which is as intelligent as human beings in the general sense. That is, it will be able to do any intellectual task that a human being can do. And then there's an idea that, well, that AI can look at its own code and make itself better, right? Because it can code. It can start to improve its own code. And the point is, once it's a tiny way beyond us, then the concern is that it's out of control at that point, that we really don't understand it. So the community is a bit divided on this. I think some people think that's science fiction. Some people think it's a plausible scenario that we need to prepare for and think for. I'm completely comfortable with the idea. I think it is just simply good sense to take that potential issue seriously and to think about how we might mitigate it. There are many ways of mitigating it. One of the ways of mitigating it is designing the AI so that it is intrinsically designed to be helpful to us, that it's never going to be unhelpful to us. But I have to tell you, it is not at all a universally held belief that that's where we're going in AI. There are still big, big problems to overcome before we get there. I'm not sure that's an entirely reassuring answer, but that's the best I've got to offer. Great. Thanks, Mike. Well, just pop it online with us, Anne. Yeah. So we've had questions from all over the world. We have Peter tuning in from Switzerland, London, Birmingham. But the question I'm going to focus on. So the question is going to be on the tuning test and whether that's still relevant and whether we have AI that has passed the Turing test. Oh, the Turing test. Okay. So the Turing test, we saw Alan Turing up there, a national hero. Turing 1950, first digital computers have appeared and Turing's working on one at the University of Manchester. And the idea of AI as in the air hasn't got a name yet, but people are talking about electronic brains and getting very excited about what they can do. So people are starting to think about the ideas that become AI. And Turing gets frustrated with people saying, well, of course, it would never actually really, really be able to think or never really be able to understand and so on. So he comes up with the following test in order to just really to try and shut people up talking about it. And the paper is called Computing Machinery Intelligence, and it's published in the journal Mind, which is a very respectable journal, a very unusual paper. It's very readable, by the way. You can download it and read it. But he proposes the Turing test. So Turing says, suppose we're trying to settle the question of whether a machine can really think or understand. So here's a test for that. What you do is you take that machine behind closed doors, and you get a human judge to be able to interact with something via a keyboard and a screen. In Turing's day, it would have been a teletype. Just by typing away questions, actually remarkably, pretty much what you do with chat GPT. Give it prompts anything you like. And actually, Turing has some very entertaining ones in his paper. And what you try and do is you try to decide whether the thing on the other side is a computer or a human being. And Turing's point was, if you cannot reliably tell that the thing on the other side is a human being or a machine, and it really is a machine, then you should accept that this thing has something like human intelligence, because you can't tell the difference. There's no test that you can apply without actually pulling back the curtain and looking to see what's there that's going to show you whether it's a human or a machine. You can't tell the difference. It's indistinguishable. So this was important historically, because it really gave AI people a target. When you said I'm an AI researcher, what are you trying to do? I'm trying to build a machine that can pass the Turing test. There was a concrete goal. The problem is in science, whenever you work with science and society, whenever you set up some challenge like that, you get all sorts of charlatans and idiots who just try and come up with ways of faking it. And so most of the ways of trying to get past the Turing test over the last 70 years have really just been systems that just come up with kind of nonsense answers trying to confuse the questioner. But now we've got large language models. So we're going to find out in about 10 days time, we're going to run a live Turing test as part of the Christmas lectures, and we will see whether our audience can distinguish a large language model from a teenage child. And we've trialed this, and I have to tell you it's possibly closer than you might think actually. Do I really think we passed the Turing test? Not in a deep sense, but what I think is that it's demonstrated to us firstly, machines clearly can generate text, which is indistinguishable from text that a human being could generate. We've done that, that box is ticked, and they can clearly understand text. So even if we haven't followed the Turing test to the letter, I think for all practical intents and purposes, the Turing test is now a historical note. Yeah, but actually the Turing test only tests one little bit of intelligence. You remember those dimensions of intelligence that I showed you? There's a huge range of those that it doesn't test. So it was historically important, and it's a big part of our historical legacy, but maybe not a core target for AI today. Cool. Thank you, Mike. I think now you've been the warning when you say a lot of searches for preparing for the Turing test for the Christmas lecture next week. Do you have any questions up at the top? Yeah, I've got one right in the center just here. Thank you. So when we think about the situations or use cases where AI is applied, typically the reason for that is because the machine is doing things better than a human can or doing things that a human might not be able to do. So it's a lot about the machine making up for the gaps that a human creates. That said, a machine is fallible, like there are errors, both normative errors and also statistical errors depending on the model type, etc. And so the question is who do you think should be responsible for looking after the gaps that the machine now creates? So the fundamental question is who should be responsible, right? Is that right? Sorry, I didn't see where you were. Can you put your hand up? Right, the top in the middle. Oh, wow. Okay, so that's why I can't see you. Okay. So this is an issue that's being discussed absolutely in the highest levels of government right now, that literally when we work in, when we move into the age of AI, who should accept the responsibility? I can tell you what my view is, but I'm not a lawyer or an ethics expert. And my view is that, as follows, firstly, if you use AI in your work, then, and you end up with a bad result, I'm sorry, but that's your problem. If you use it to generate an essay at school and you're caught out, I'm afraid that's your problem. It's not the fault of the AI. But I think more generally, we can't offload our legal, moral, ethical obligations as human beings onto the machine. That is, we can't say it's not my fault, the machine did it. Right? An extreme example of this is lethal autonomous weapons, AI that's empowered to decide whether to take a human life. What I worry about, one of the many things I worry about with lethal autonomous weapons is the idea that we have military services that say, well, it wasn't our fault, it was the AI that got it wrong, that led to this building being bombed or whatever it was. And there, I think the responsibility lies with the people that deploy the technology. So that, I think, is a crucial point. But at the same time, the developers of this technology, if they are warranting that it is fit for purpose, then they have a responsibility as well. And the responsibility that they have is to ensure that it really is fit for purpose. And it's an interesting question at the moment. If we have large language models used by hundreds of millions of people, for example, to get medical advice, and we know that this technology can go wrong, is the technology fit for that purpose? I'm not sure at all that it is. So I'm not sure that's really answering your question, but those are my sort of a few random thoughts on it. I mean, but I say, crucially, you know, if you're using this in your work, you can never blame the AI, right? You are responsible for the outputs of that process, right? You can't offload your legal, professional, ethical, moral obligations to the machine. It's a complex question, which is why I gave a very bad answer. I've got a question right on the left here. I've used access on the left panel shelf. Thank you. If future large language models are trained by scraping the whole internet again, now there's more and more content going on to the internet created by AI. So is that going to create something like a microphone feedback loop where the information gets less and less useful? Super question and really fascinating. So I have some colleagues that did the following experiment. So chat GPT is trained, roughly speaking, on human generated text, but it creates AI generated text. So the question they had is what happens if we train one of these models, not on the original human generated text, but just on stuff which is produced by AI? And then you can see what they did next. You can guess, they said, well, okay, let's take another model which is trained on the second generation model text. And so what happens about five generations down the line, it dissolves into gibberish, literally dissolves into gibberish. And I have to tell you the original version of this paper, they called it AI dementia. And I was really cross with, no, I lost both my parents to dementia. I didn't find it very funny at all. They now call it model collapse. So if you go and Google model collapse, you'll find the answers there. But really remarkable. What that tells you is that actually there is something qualitatively different at the moment to human text, to AI generated text. For all that it looks perfect or indistinguishable to us, actually it isn't. Where is that going to take us? I have colleagues who think that we're going to have to label and protect human generated content because it is so valuable. All right? Human generated actual, authentic human generated content is really, really valuable. I also have colleagues, and I'm not sure whether they're entirely serious at this, but they say that actually where we're going is the data that we produce in everything that we do is so valuable for AI that we're going to enter a future where you're going to sell the rights to AI companies for you, for them to harvest your emotions, all of your experiences, everything you say and do in your life. And you'll be paid for that, but it will go into the training models of large language models. Now I don't know if that's true, but nevertheless there's a, it has some inner truth in it, I think. And in 100 years time, it is an absolute certainty that there will be vastly, vastly more AI generated content out there in the world than there will human generated content with certainty. I think there's no question, but that that's the way the future is going. And as I say, as the model collapse scenario illustrates, that presents some real challenges. Awesome. Thank you very much, Mike. I've got a question at the front who's been very keen to ask. Thanks very much indeed for a very interesting lecture. It strikes me in a way just being a comparison of human being. What we're doing is talking about what the prefront frontal cortex does, but there are other areas of prefrontal cortex, which is a fear predictor. Do we need to be developing sort of a parallel AI system, which works on the basis of fear prediction and get to talk to each other? Yeah. So I'm absolutely not a neuroscientist or I'm a computer programmer and that's very much my background. Again, it's interesting that the community is incredibly divided. So when I was an undergraduate studying AI and I focused in my final year, it's mainly what I studied. And the textbooks that we had made no reference to the brain whatsoever. Just wasn't the thing because it was all about modeling the mind. It was all about modeling conscious reasoning processes and so on. And it was deeply unfashionable to think about the brain. And there's been a bit of a what scientists call a paradigm shift in the way that they think about this prompted by the rise of neural networks, but also by the fact that advances in computer vision and the architectures, the neural network architectures that led to facial recognition really working were actually inspired by the visual cortex, the human visual cortex. So it's a lot more of a fashionable question now than it used to be. So my guess is, firstly, simply trying to copy the structure of the human brain is not the way to do it, but nevertheless getting a much better understanding of the organization of the brain, the functional organization of the brain, and the way that the different components of the brain interoperate to produce human intelligence, I think, is. And really, there's a vast amount of work there to be done to try to understand that. There are so many unanswered questions. I hope that's some help. Thank you, Mike. We're just going to jump back online. Yeah, that's going to be a little early. Anthony asks, if emergency is inaccurate, is calling the technology intelligence inaccurate? Are we just dreaming of something that can never be? And then to follow up on that, you've got Tom Fatcher who asks, is there anything happening to develop native analog neural networks rather than doing neural networks in a digital machine only? Take the second one. Yeah, there certainly is. So Steve Ferber at Manchester is building hardware neural networks. But the moment it's just much cheaper and much more efficient to do it in software. There have been various attempts over the years to develop neural net processes, famous phrase from the movie that you're not allowed to mention to AI researchers, the Terminator movies, the neural network processes. If you want to wind up an AI researcher, just bring up the Terminator. It's a shortcut to triggering them. But neural network processes have never really taken off. Doesn't mean they won't do, but at the moment, it's just much cheaper and much more efficient to throw more conventional GPUs and so on at the problem. It doesn't mean it won't happen, but at the moment, it's not there yet. What was the other question again, the first one? So the other question was, are we basically the terminology being used? If emergency is inaccurate, is calling the technology intelligence inaccurate? And are we dreaming of something that can never be? Yeah, so the phrase artificial intelligence was coined by John McCarthy around about 1955. He was 28 years old, a young American researcher, and he wants funding to get a whole bunch of researchers together for a summer, and he thinks they'll solve artificial intelligence in a summer. But he has to give a title to his proposal, which goes to the Rockefeller Foundation, and he fixes on artificial intelligence. And boy, have we regretted that ever since. The problem is, firstly, artificial sounds like fake. It sounds like ursat. I mean, who wants fake intelligence? And for intelligence itself, the problem is that so many of the problems that have just proved to be really hard for AI actually don't seem to require intelligence at all. So the classic example, driving a car. When somebody passes their driving test, they don't think, wow, you're a genius. It doesn't seem to require intelligence in people, but I cannot tell you how much money has been thrown at driverless car technologies, and we are a long way off from jumping into a car and saying, take me to a country pub, which is my dream of the technology, I have to tell you. We're a long, long way off. So it's a classic example of what people think AI is focused on is deep intellectual tasks, and that's actually not where the most difficult problems are. The difficult problems are actually surprisingly mundane. Well, I was interested in how you mentioned that the two pools of AI study were symbolic AI and big AI, and I was wondering how you sawed how your viewpoint on the change in focus from one to another throughout your career. Yeah. So an enormous number of people are busy looking at that right now. So remember symbolic AI, which is the tradition that I grew up in AI, which was dominant for kind of 30 years in the AI community, is roughly, and again, hand-waving madly at this point, and lots and lots of my colleagues are cringing madly at this point, roughly speaking, the idea of symbolic AI is that you're modeling the mind, the conscious mind, conscious mental reasoning processes, where you have a conversation with yourself, and you have a conversation in a language, right? You're trying to decide whether to go to this lecture tonight, and you think, well, yeah, but there's EastEnders on TV, and mom's cooking a nice meal, but then it's going to be really interesting. You weigh up those options, and literally symbolic AI tries to capture that kind of thing explicitly, and using languages that with a bit of squinting resemble human languages. Then we've got the alternative approach, which is machine learning, data-driven, and so on, which, again, I emphasize with neural approaches, we're not trying to build artificial brains, that's not what's going on, but we're taking inspiration from the structures that we see in brains and nervous systems, and in particular, the idea that large computational tasks can be reduced down to tiny, simple pattern recognition problems. Okay, but we've seen, for example, that large language models get things wrong a lot, and a lot of people have said, but look, maybe if you just married the neural and the symbolic together so that the symbolic system did have something like a database of facts, that you could put that together with a large language model and be able to improve the outputs of the large language model. The jury is out exactly on how that's going to come out. Lots of different ideas out there now. Trillion-dollar companies are spending billions of dollars right now to investigate exactly the question that you've put out there, so it's an extremely pertinent question. There's no, I say, I don't see any answer on the horizon right now, which looks like it's going to win out. My worry is that what we'll end up with is a kind of unscientific solution. That is a solution which is sort of hacked together without any deep underlying principles, and as a scientist, what I would want to see is something which was tied together with deep scientific principles, but it's an extremely pertinent question, and I say right now an enormous number of PhD students across the world are busy looking at exactly what you've just described. Thank you, Mike. Time for a squeeze and two more questions. Take one from in the room. Cool. We've got a question just in the middle at the back there. For the lecture, my question is around, you sort of took us on the journey from 40 years ago, some of the inspirations around how the mind works and the mathematics. He said the mathematics was fairly simple. I would like your opinion. Where do you think we're not looking enough for where leap be? Oh, wow. If I knew that, I'd be forming a company, I have to tell you. Okay, so I think the first thing to say is I said when it started to become clear that this technology was worked, Silicon Valley starts to make bets, and these bets are billion dollar bets, a lot of billion dollar bets going on, investing in a very, very wide range of different ideas in the hope that one is going to be the one that delivers something which is going to give them a competitive advantage. That's the context in which we're trying to figure out what the next big thing is going to be. I think this multimodal is going to be dominant. That's what we're going to see, and you're going to hear that phrase, multimodal. Remember, you heard it here first, if you've never heard it before. You're going to hear that a lot, and that's going to be text, images, sound, video. You're going to be able to upload videos, and the AI will describe what's going on in the video or produce a summary, and you'll be able to say what happens after this bit in the video, and it will be able to come out with a description of that for you. Alternatively, you'll be able to give a storyline, and it will generate videos for you. Ultimately, where it's going to go is in virtual reality. I don't know if you like Lord of the Rings or Star Wars, but I enjoy both of those, and wouldn't you love to see a mash-up of those two things? Generative AI will be able to do it for you. I used to think this was just a bit of a pipe dream, but actually, at the moment, it seems completely plausible. If you like the original Star Trek series, which I do, and my family doesn't, but there was only 60-odd episodes of them. In the generative AI future, there will be as many episodes as you want, and it will look and sound like Leonard Nimoy and William Shatner perfectly. Maybe the storylines won't be that great, but actually, they don't need to be if they're pressing a button specifically to your tastes. That's the general trajectory of where we're going. I say, actually, I don't see any reason why what I've just described is not going to be realistic within decades, and we're going to get there piece by piece. It's not going to happen overnight, but we will get there. I think we genuinely will. The future is going to be wonderful and weird. Thank you, Mike. Do we have any final very quick questions anywhere? We've got one just over here, I think, in the jumper on the right. Just in the middle here. Hello. Thank you. To what extent do you think human beings are very large language models and very large movement models? My gut feeling is we're not just large language models. I think there's an awful lot more. We're great apes, the result of three and a half billion years of evolution, and we evolved to be able to understand planet Earth, roughly speaking, at ground level where we are now, and to understand other great apes, societies of great apes. That's not what large language models do. That's fundamentally not what they do. On the other hand, I've had colleagues again seriously say, well, maybe we should try and construct a theory of human society, which is based on the idea that we are actually just trying to come out with the most plausible thing that comes next. It doesn't seem plausible to me, I have to say. These are just tools. They're just tools which are based fundamentally based on language. They're extremely powerful at what they do, but do they give us any deep insights into human nature or the fundamentals of human mental processes? Probably not. Thank you very much, Mike. That is all we have time for, unfortunately, today. This is the end of the Turing lecture series for this year. Please do follow us on social media, the website, our emailing list to find out about future Turing events. Of course, we do have the Christmas lecture in 10 days time as I'll back here at the Royal Institution. Apart from that, just one more massive round of applause, please, for Professor", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 20.32, "text": " Hi. Welcome, everyone. Thank you very much for venturing out on this cold, wintery December", "tokens": [50364, 2421, 13, 4027, 11, 1518, 13, 1044, 291, 588, 709, 337, 6931, 1345, 484, 322, 341, 3554, 11, 6355, 88, 7687, 51380], "temperature": 0.0, "avg_logprob": -0.29538579054281744, "compression_ratio": 1.453551912568306, "no_speech_prob": 0.13659240305423737}, {"id": 1, "seek": 0, "start": 20.32, "end": 24.080000000000002, "text": " evening to be with us tonight. And if you're joining online, thank you for being here as", "tokens": [51380, 5634, 281, 312, 365, 505, 4440, 13, 400, 498, 291, 434, 5549, 2950, 11, 1309, 291, 337, 885, 510, 382, 51568], "temperature": 0.0, "avg_logprob": -0.29538579054281744, "compression_ratio": 1.453551912568306, "no_speech_prob": 0.13659240305423737}, {"id": 2, "seek": 0, "start": 24.080000000000002, "end": 29.080000000000002, "text": " well. My name is Hari Sudh and that's Hari when you go somewhere quickly, you're in a", "tokens": [51568, 731, 13, 1222, 1315, 307, 47221, 12323, 71, 293, 300, 311, 47221, 562, 291, 352, 4079, 2661, 11, 291, 434, 294, 257, 51818], "temperature": 0.0, "avg_logprob": -0.29538579054281744, "compression_ratio": 1.453551912568306, "no_speech_prob": 0.13659240305423737}, {"id": 3, "seek": 2908, "start": 29.08, "end": 34.92, "text": " hurry. I am a research application manager at the Turing Institute, which means I basically", "tokens": [50364, 11025, 13, 286, 669, 257, 2132, 3861, 6598, 412, 264, 314, 1345, 9446, 11, 597, 1355, 286, 1936, 50656], "temperature": 0.0, "avg_logprob": -0.19042468765406934, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.03291609510779381}, {"id": 4, "seek": 2908, "start": 34.92, "end": 40.879999999999995, "text": " focus on finding real-world use cases and users for the Turing's research outputs. And", "tokens": [50656, 1879, 322, 5006, 957, 12, 13217, 764, 3331, 293, 5022, 337, 264, 314, 1345, 311, 2132, 23930, 13, 400, 50954], "temperature": 0.0, "avg_logprob": -0.19042468765406934, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.03291609510779381}, {"id": 5, "seek": 2908, "start": 40.879999999999995, "end": 45.2, "text": " I'm really excited to be hosting this very special and I'm told sold out lecture for", "tokens": [50954, 286, 478, 534, 2919, 281, 312, 16058, 341, 588, 2121, 293, 286, 478, 1907, 3718, 484, 7991, 337, 51170], "temperature": 0.0, "avg_logprob": -0.19042468765406934, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.03291609510779381}, {"id": 6, "seek": 2908, "start": 45.2, "end": 50.480000000000004, "text": " you all today. It is the last in our series of 2023 of Turing lectures and the first", "tokens": [51170, 291, 439, 965, 13, 467, 307, 264, 1036, 294, 527, 2638, 295, 44377, 295, 314, 1345, 16564, 293, 264, 700, 51434], "temperature": 0.0, "avg_logprob": -0.19042468765406934, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.03291609510779381}, {"id": 7, "seek": 2908, "start": 50.480000000000004, "end": 57.239999999999995, "text": " ever hybrid Turing lecture discourse as we prepare and build up for the Christmas lecture", "tokens": [51434, 1562, 13051, 314, 1345, 7991, 23938, 382, 321, 5940, 293, 1322, 493, 337, 264, 5272, 7991, 51772], "temperature": 0.0, "avg_logprob": -0.19042468765406934, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.03291609510779381}, {"id": 8, "seek": 5724, "start": 57.24, "end": 63.88, "text": " of 2023 here at the Royal Institution. Now, as has become a bit of a tradition for the", "tokens": [50364, 295, 44377, 510, 412, 264, 12717, 2730, 6518, 13, 823, 11, 382, 575, 1813, 257, 857, 295, 257, 6994, 337, 264, 50696], "temperature": 0.0, "avg_logprob": -0.25418285487853376, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.001978005515411496}, {"id": 9, "seek": 5724, "start": 63.88, "end": 68.2, "text": " hosts of this year's Turing lectures, quick show of hands, who's been to a Turing lecture", "tokens": [50696, 21573, 295, 341, 1064, 311, 314, 1345, 16564, 11, 1702, 855, 295, 2377, 11, 567, 311, 668, 281, 257, 314, 1345, 7991, 50912], "temperature": 0.0, "avg_logprob": -0.25418285487853376, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.001978005515411496}, {"id": 10, "seek": 5724, "start": 68.2, "end": 76.16, "text": " before? Some people, some people, who's been to a lecture from this year's series before?", "tokens": [50912, 949, 30, 2188, 561, 11, 512, 561, 11, 567, 311, 668, 281, 257, 7991, 490, 341, 1064, 311, 2638, 949, 30, 51310], "temperature": 0.0, "avg_logprob": -0.25418285487853376, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.001978005515411496}, {"id": 11, "seek": 5724, "start": 76.16, "end": 83.12, "text": " Looks like more hands than last time. Doesn't make sense. On the flip side of it, who's coming", "tokens": [51310, 10027, 411, 544, 2377, 813, 1036, 565, 13, 12955, 380, 652, 2020, 13, 1282, 264, 7929, 1252, 295, 309, 11, 567, 311, 1348, 51658], "temperature": 0.0, "avg_logprob": -0.25418285487853376, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.001978005515411496}, {"id": 12, "seek": 8312, "start": 83.12, "end": 87.68, "text": " to their first Turing lecture today? A lot of new faces. For all the new people here,", "tokens": [50364, 281, 641, 700, 314, 1345, 7991, 965, 30, 316, 688, 295, 777, 8475, 13, 1171, 439, 264, 777, 561, 510, 11, 50592], "temperature": 0.0, "avg_logprob": -0.18833881141865147, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.08470724523067474}, {"id": 13, "seek": 8312, "start": 87.68, "end": 91.88000000000001, "text": " welcome to the ones who have been here before, welcome back. Just as a reminder, the Turing", "tokens": [50592, 2928, 281, 264, 2306, 567, 362, 668, 510, 949, 11, 2928, 646, 13, 1449, 382, 257, 13548, 11, 264, 314, 1345, 50802], "temperature": 0.0, "avg_logprob": -0.18833881141865147, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.08470724523067474}, {"id": 14, "seek": 8312, "start": 91.88000000000001, "end": 96.60000000000001, "text": " lectures are the Turing's flagship lecture series. They've been running since 2016 and", "tokens": [50802, 16564, 366, 264, 314, 1345, 311, 30400, 7991, 2638, 13, 814, 600, 668, 2614, 1670, 6549, 293, 51038], "temperature": 0.0, "avg_logprob": -0.18833881141865147, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.08470724523067474}, {"id": 15, "seek": 8312, "start": 96.60000000000001, "end": 101.92, "text": " welcome world-leading experts in the domain of data science and AI to come and talk to", "tokens": [51038, 2928, 1002, 12, 28012, 8572, 294, 264, 9274, 295, 1412, 3497, 293, 7318, 281, 808, 293, 751, 281, 51304], "temperature": 0.0, "avg_logprob": -0.18833881141865147, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.08470724523067474}, {"id": 16, "seek": 8312, "start": 101.92, "end": 106.44, "text": " you all. The Turing Institute itself. We have had a quick video on it, which I was mesmerised", "tokens": [51304, 291, 439, 13, 440, 314, 1345, 9446, 2564, 13, 492, 362, 632, 257, 1702, 960, 322, 309, 11, 597, 286, 390, 3813, 936, 2640, 51530], "temperature": 0.0, "avg_logprob": -0.18833881141865147, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.08470724523067474}, {"id": 17, "seek": 10644, "start": 106.44, "end": 113.16, "text": " by. Just as a reminder, we are the National Institute for Data Science and AI. We are", "tokens": [50364, 538, 13, 1449, 382, 257, 13548, 11, 321, 366, 264, 4862, 9446, 337, 11888, 8976, 293, 7318, 13, 492, 366, 50700], "temperature": 0.0, "avg_logprob": -0.18805359957510964, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.15400992333889008}, {"id": 18, "seek": 10644, "start": 113.16, "end": 117.8, "text": " named after Alan Turing, who is one of the most prominent mathematicians from the 20th", "tokens": [50700, 4926, 934, 16442, 314, 1345, 11, 567, 307, 472, 295, 264, 881, 17034, 32811, 2567, 490, 264, 945, 392, 50932], "temperature": 0.0, "avg_logprob": -0.18805359957510964, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.15400992333889008}, {"id": 19, "seek": 10644, "start": 117.8, "end": 122.84, "text": " century in Britain. He is very famous for, I always normally say most famous, but very", "tokens": [50932, 4901, 294, 12960, 13, 634, 307, 588, 4618, 337, 11, 286, 1009, 5646, 584, 881, 4618, 11, 457, 588, 51184], "temperature": 0.0, "avg_logprob": -0.18805359957510964, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.15400992333889008}, {"id": 20, "seek": 10644, "start": 122.84, "end": 127.92, "text": " famous for being part of the team that cracked the enigma code that was used by Nazi Germany", "tokens": [51184, 4618, 337, 885, 644, 295, 264, 1469, 300, 25140, 264, 465, 16150, 3089, 300, 390, 1143, 538, 23592, 7244, 51438], "temperature": 0.0, "avg_logprob": -0.18805359957510964, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.15400992333889008}, {"id": 21, "seek": 10644, "start": 127.92, "end": 131.32, "text": " in World War II at Bletchley Park, if you've heard of Bletchley Park as well. If you've", "tokens": [51438, 294, 3937, 3630, 6351, 412, 2177, 7858, 3420, 4964, 11, 498, 291, 600, 2198, 295, 2177, 7858, 3420, 4964, 382, 731, 13, 759, 291, 600, 51608], "temperature": 0.0, "avg_logprob": -0.18805359957510964, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.15400992333889008}, {"id": 22, "seek": 13132, "start": 131.35999999999999, "end": 138.51999999999998, "text": " seen the imitation game with Benedict Cumberbatch, that's what I always say, isn't it? He is", "tokens": [50366, 1612, 264, 47624, 1216, 365, 47837, 383, 4182, 65, 852, 11, 300, 311, 437, 286, 1009, 584, 11, 1943, 380, 309, 30, 634, 307, 50724], "temperature": 0.0, "avg_logprob": -0.14968851233730798, "compression_ratio": 1.5494880546075085, "no_speech_prob": 0.045287538319826126}, {"id": 23, "seek": 13132, "start": 138.51999999999998, "end": 144.79999999999998, "text": " playing Alan Turing and our mission is to make great leaps in data science and AI research", "tokens": [50724, 2433, 16442, 314, 1345, 293, 527, 4447, 307, 281, 652, 869, 476, 2382, 294, 1412, 3497, 293, 7318, 2132, 51038], "temperature": 0.0, "avg_logprob": -0.14968851233730798, "compression_ratio": 1.5494880546075085, "no_speech_prob": 0.045287538319826126}, {"id": 24, "seek": 13132, "start": 144.79999999999998, "end": 150.28, "text": " to change the world for the better. As I mentioned, today is not just a Turing lecture, it is", "tokens": [51038, 281, 1319, 264, 1002, 337, 264, 1101, 13, 1018, 286, 2835, 11, 965, 307, 406, 445, 257, 314, 1345, 7991, 11, 309, 307, 51312], "temperature": 0.0, "avg_logprob": -0.14968851233730798, "compression_ratio": 1.5494880546075085, "no_speech_prob": 0.045287538319826126}, {"id": 25, "seek": 13132, "start": 150.28, "end": 155.84, "text": " also a discourse, which means two important things. Firstly, when I'm done with the intro,", "tokens": [51312, 611, 257, 23938, 11, 597, 1355, 732, 1021, 721, 13, 20042, 11, 562, 286, 478, 1096, 365, 264, 12897, 11, 51590], "temperature": 0.0, "avg_logprob": -0.14968851233730798, "compression_ratio": 1.5494880546075085, "no_speech_prob": 0.045287538319826126}, {"id": 26, "seek": 13132, "start": 155.84, "end": 160.44, "text": " the lights will go down and it's going to go quiet until exactly 7.30 on the dot when", "tokens": [51590, 264, 5811, 486, 352, 760, 293, 309, 311, 516, 281, 352, 5677, 1826, 2293, 1614, 13, 3446, 322, 264, 5893, 562, 51820], "temperature": 0.0, "avg_logprob": -0.14968851233730798, "compression_ratio": 1.5494880546075085, "no_speech_prob": 0.045287538319826126}, {"id": 27, "seek": 16044, "start": 160.48, "end": 163.84, "text": " a bell is going to ring and a discourse will begin, so just to warn you guys that will", "tokens": [50366, 257, 4549, 307, 516, 281, 4875, 293, 257, 23938, 486, 1841, 11, 370, 445, 281, 12286, 291, 1074, 300, 486, 50534], "temperature": 0.0, "avg_logprob": -0.1819382196740259, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.003643702482804656}, {"id": 28, "seek": 16044, "start": 163.84, "end": 168.68, "text": " be happening. The lights aren't broken, that is part of the programme for today, but also", "tokens": [50534, 312, 2737, 13, 440, 5811, 3212, 380, 5463, 11, 300, 307, 644, 295, 264, 14001, 337, 965, 11, 457, 611, 50776], "temperature": 0.0, "avg_logprob": -0.1819382196740259, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.003643702482804656}, {"id": 29, "seek": 16044, "start": 168.68, "end": 172.4, "text": " it is a discourse and we really want to get you guys involved, so there's a huge Q&A section", "tokens": [50776, 309, 307, 257, 23938, 293, 321, 534, 528, 281, 483, 291, 1074, 3288, 11, 370, 456, 311, 257, 2603, 1249, 5, 32, 3541, 50962], "temperature": 0.0, "avg_logprob": -0.1819382196740259, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.003643702482804656}, {"id": 30, "seek": 16044, "start": 172.4, "end": 176.72, "text": " at the end for about 30 minutes. Please do think about what questions you'd like to ask", "tokens": [50962, 412, 264, 917, 337, 466, 2217, 2077, 13, 2555, 360, 519, 466, 437, 1651, 291, 1116, 411, 281, 1029, 51178], "temperature": 0.0, "avg_logprob": -0.1819382196740259, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.003643702482804656}, {"id": 31, "seek": 16044, "start": 176.72, "end": 181.16, "text": " our speaker today. If you're in person, we will have roaming mics that will be going", "tokens": [51178, 527, 8145, 965, 13, 759, 291, 434, 294, 954, 11, 321, 486, 362, 42680, 45481, 300, 486, 312, 516, 51400], "temperature": 0.0, "avg_logprob": -0.1819382196740259, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.003643702482804656}, {"id": 32, "seek": 16044, "start": 181.16, "end": 185.07999999999998, "text": " around, we can bring them upstairs as well. If you're online, you can ask a question in", "tokens": [51400, 926, 11, 321, 393, 1565, 552, 16462, 382, 731, 13, 759, 291, 434, 2950, 11, 291, 393, 1029, 257, 1168, 294, 51596], "temperature": 0.0, "avg_logprob": -0.1819382196740259, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.003643702482804656}, {"id": 33, "seek": 16044, "start": 185.07999999999998, "end": 190.36, "text": " the Vimeo chat and someone here will be tracking the questions and will be able to share the", "tokens": [51596, 264, 691, 1312, 78, 5081, 293, 1580, 510, 486, 312, 11603, 264, 1651, 293, 486, 312, 1075, 281, 2073, 264, 51860], "temperature": 0.0, "avg_logprob": -0.1819382196740259, "compression_ratio": 1.800578034682081, "no_speech_prob": 0.003643702482804656}, {"id": 34, "seek": 19036, "start": 190.36, "end": 194.76000000000002, "text": " questions there as well. If you'd like to share on social media that you're here and", "tokens": [50364, 1651, 456, 382, 731, 13, 759, 291, 1116, 411, 281, 2073, 322, 2093, 3021, 300, 291, 434, 510, 293, 50584], "temperature": 0.0, "avg_logprob": -0.22416177817753383, "compression_ratio": 1.73046875, "no_speech_prob": 0.004116035532206297}, {"id": 35, "seek": 19036, "start": 194.76000000000002, "end": 204.76000000000002, "text": " having an amazing evening, please do tag us. We are on Twitter, at Turing Inst and we are", "tokens": [50584, 1419, 364, 2243, 5634, 11, 1767, 360, 6162, 505, 13, 492, 366, 322, 5794, 11, 412, 314, 1345, 2730, 293, 321, 366, 51084], "temperature": 0.0, "avg_logprob": -0.22416177817753383, "compression_ratio": 1.73046875, "no_speech_prob": 0.004116035532206297}, {"id": 36, "seek": 19036, "start": 204.76000000000002, "end": 208.72000000000003, "text": " on Instagram at the Turing Inst, so please do tag us, we'd like to see what you're sharing", "tokens": [51084, 322, 5281, 412, 264, 314, 1345, 2730, 11, 370, 1767, 360, 6162, 505, 11, 321, 1116, 411, 281, 536, 437, 291, 434, 5414, 51282], "temperature": 0.0, "avg_logprob": -0.22416177817753383, "compression_ratio": 1.73046875, "no_speech_prob": 0.004116035532206297}, {"id": 37, "seek": 19036, "start": 208.72000000000003, "end": 214.68, "text": " and connect with you as well. So this year's lecture series has been answering the question", "tokens": [51282, 293, 1745, 365, 291, 382, 731, 13, 407, 341, 1064, 311, 7991, 2638, 575, 668, 13430, 264, 1168, 51580], "temperature": 0.0, "avg_logprob": -0.22416177817753383, "compression_ratio": 1.73046875, "no_speech_prob": 0.004116035532206297}, {"id": 38, "seek": 19036, "start": 214.68, "end": 219.76000000000002, "text": " how AI broke the internet with a focus on generative AI. You guys can basically think", "tokens": [51580, 577, 7318, 6902, 264, 4705, 365, 257, 1879, 322, 1337, 1166, 7318, 13, 509, 1074, 393, 1936, 519, 51834], "temperature": 0.0, "avg_logprob": -0.22416177817753383, "compression_ratio": 1.73046875, "no_speech_prob": 0.004116035532206297}, {"id": 39, "seek": 21976, "start": 219.76, "end": 225.35999999999999, "text": " of generative AI as algorithms that are able to generate new content. This can be text", "tokens": [50364, 295, 1337, 1166, 7318, 382, 14642, 300, 366, 1075, 281, 8460, 777, 2701, 13, 639, 393, 312, 2487, 50644], "temperature": 0.0, "avg_logprob": -0.1992640967841621, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.036860883235931396}, {"id": 40, "seek": 21976, "start": 225.35999999999999, "end": 229.79999999999998, "text": " content like you see from chat GPT, it could be images that you can also get from chat", "tokens": [50644, 2701, 411, 291, 536, 490, 5081, 26039, 51, 11, 309, 727, 312, 5267, 300, 291, 393, 611, 483, 490, 5081, 50866], "temperature": 0.0, "avg_logprob": -0.1992640967841621, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.036860883235931396}, {"id": 41, "seek": 21976, "start": 229.79999999999998, "end": 236.39999999999998, "text": " GPT but also Dali as well and can be used for a wide range of things. Potentially professionally", "tokens": [50866, 26039, 51, 457, 611, 413, 5103, 382, 731, 293, 393, 312, 1143, 337, 257, 4874, 3613, 295, 721, 13, 9145, 3137, 27941, 51196], "temperature": 0.0, "avg_logprob": -0.1992640967841621, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.036860883235931396}, {"id": 42, "seek": 21976, "start": 236.39999999999998, "end": 241.04, "text": " for blog posts or emails, your colleagues don't realise we're written by an algorithm", "tokens": [51196, 337, 6968, 12300, 420, 12524, 11, 428, 7734, 500, 380, 18809, 321, 434, 3720, 538, 364, 9284, 51428], "temperature": 0.0, "avg_logprob": -0.1992640967841621, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.036860883235931396}, {"id": 43, "seek": 21976, "start": 241.04, "end": 245.39999999999998, "text": " and not by you, if you've done that before. If you're at school, maybe for some homework", "tokens": [51428, 293, 406, 538, 291, 11, 498, 291, 600, 1096, 300, 949, 13, 759, 291, 434, 412, 1395, 11, 1310, 337, 512, 14578, 51646], "temperature": 0.0, "avg_logprob": -0.1992640967841621, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.036860883235931396}, {"id": 44, "seek": 24540, "start": 245.4, "end": 252.72, "text": " or at university to write essays, it can also be used when you have a creative wall and", "tokens": [50364, 420, 412, 5454, 281, 2464, 35123, 11, 309, 393, 611, 312, 1143, 562, 291, 362, 257, 5880, 2929, 293, 50730], "temperature": 0.0, "avg_logprob": -0.21794763711782603, "compression_ratio": 1.738562091503268, "no_speech_prob": 0.05185335502028465}, {"id": 45, "seek": 24540, "start": 252.72, "end": 256.68, "text": " you can't get past it and you want some ideas and some prompts, it can be a great way to", "tokens": [50730, 291, 393, 380, 483, 1791, 309, 293, 291, 528, 512, 3487, 293, 512, 41095, 11, 309, 393, 312, 257, 869, 636, 281, 50928], "temperature": 0.0, "avg_logprob": -0.21794763711782603, "compression_ratio": 1.738562091503268, "no_speech_prob": 0.05185335502028465}, {"id": 46, "seek": 24540, "start": 256.68, "end": 259.8, "text": " have some initial thoughts come through that you can build on. It can be used for quite", "tokens": [50928, 362, 512, 5883, 4598, 808, 807, 300, 291, 393, 1322, 322, 13, 467, 393, 312, 1143, 337, 1596, 51084], "temperature": 0.0, "avg_logprob": -0.21794763711782603, "compression_ratio": 1.738562091503268, "no_speech_prob": 0.05185335502028465}, {"id": 47, "seek": 24540, "start": 259.8, "end": 264.76, "text": " scary things, as was mentioned by an audience member at the last Turing lecture of someone", "tokens": [51084, 6958, 721, 11, 382, 390, 2835, 538, 364, 4034, 4006, 412, 264, 1036, 314, 1345, 7991, 295, 1580, 51332], "temperature": 0.0, "avg_logprob": -0.21794763711782603, "compression_ratio": 1.738562091503268, "no_speech_prob": 0.05185335502028465}, {"id": 48, "seek": 24540, "start": 264.76, "end": 271.24, "text": " who submitted legal filings for a court case using chat GPT, which is terrifying. But it", "tokens": [51332, 567, 14405, 5089, 1387, 1109, 337, 257, 4753, 1389, 1228, 5081, 26039, 51, 11, 597, 307, 18106, 13, 583, 309, 51656], "temperature": 0.0, "avg_logprob": -0.21794763711782603, "compression_ratio": 1.738562091503268, "no_speech_prob": 0.05185335502028465}, {"id": 49, "seek": 24540, "start": 271.24, "end": 274.84000000000003, "text": " can also be used for very everyday things as demonstrated, I'm not sure if you guys saw", "tokens": [51656, 393, 611, 312, 1143, 337, 588, 7429, 721, 382, 18772, 11, 286, 478, 406, 988, 498, 291, 1074, 1866, 51836], "temperature": 0.0, "avg_logprob": -0.21794763711782603, "compression_ratio": 1.738562091503268, "no_speech_prob": 0.05185335502028465}, {"id": 50, "seek": 27484, "start": 274.91999999999996, "end": 281.15999999999997, "text": " the thread by Garrett Scott, who gave chat GPT an image of a goose and said, can you make", "tokens": [50368, 264, 7207, 538, 40266, 6659, 11, 567, 2729, 5081, 26039, 51, 364, 3256, 295, 257, 24717, 293, 848, 11, 393, 291, 652, 50680], "temperature": 0.0, "avg_logprob": -0.18542141749941068, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.012369592674076557}, {"id": 51, "seek": 27484, "start": 281.15999999999997, "end": 286.0, "text": " this goose sillier? And then asked chat GPT to progressively make the goose sillier and", "tokens": [50680, 341, 24717, 37160, 811, 30, 400, 550, 2351, 5081, 26039, 51, 281, 46667, 652, 264, 24717, 37160, 811, 293, 50922], "temperature": 0.0, "avg_logprob": -0.18542141749941068, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.012369592674076557}, {"id": 52, "seek": 27484, "start": 286.0, "end": 291.91999999999996, "text": " sillier until chat GPT gave him an image of a crazy, silly goose and said, this is the", "tokens": [50922, 37160, 811, 1826, 5081, 26039, 51, 2729, 796, 364, 3256, 295, 257, 3219, 11, 11774, 24717, 293, 848, 11, 341, 307, 264, 51218], "temperature": 0.0, "avg_logprob": -0.18542141749941068, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.012369592674076557}, {"id": 53, "seek": 27484, "start": 291.91999999999996, "end": 295.88, "text": " silliest goose in the history of the universe, I do not think it is possible to get any more", "tokens": [51218, 37160, 6495, 24717, 294, 264, 2503, 295, 264, 6445, 11, 286, 360, 406, 519, 309, 307, 1944, 281, 483, 604, 544, 51416], "temperature": 0.0, "avg_logprob": -0.18542141749941068, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.012369592674076557}, {"id": 54, "seek": 27484, "start": 295.88, "end": 300.96, "text": " sillier goose. Obviously a wide range of applications from the technology, if you guys want to look", "tokens": [51416, 37160, 811, 24717, 13, 7580, 257, 4874, 3613, 295, 5821, 490, 264, 2899, 11, 498, 291, 1074, 528, 281, 574, 51670], "temperature": 0.0, "avg_logprob": -0.18542141749941068, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.012369592674076557}, {"id": 55, "seek": 30096, "start": 301.03999999999996, "end": 306.28, "text": " at that thread, the geese that come out of it are mesmerizing. But as in the focus of", "tokens": [50368, 412, 300, 7207, 11, 264, 1519, 1130, 300, 808, 484, 295, 309, 366, 3813, 936, 3319, 13, 583, 382, 294, 264, 1879, 295, 50630], "temperature": 0.0, "avg_logprob": -0.28886426615918803, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.006452895700931549}, {"id": 56, "seek": 30096, "start": 306.28, "end": 311.76, "text": " this year's series, we started with professor in September this year, asking the question", "tokens": [50630, 341, 1064, 311, 2638, 11, 321, 1409, 365, 8304, 294, 7216, 341, 1064, 11, 3365, 264, 1168, 50904], "temperature": 0.0, "avg_logprob": -0.28886426615918803, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.006452895700931549}, {"id": 57, "seek": 30096, "start": 311.76, "end": 316.56, "text": " what is generative AI and having an introduction to it. We then had a lecture from Dr. Vari", "tokens": [50904, 437, 307, 1337, 1166, 7318, 293, 1419, 364, 9339, 281, 309, 13, 492, 550, 632, 257, 7991, 490, 2491, 13, 691, 3504, 51144], "temperature": 0.0, "avg_logprob": -0.28886426615918803, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.006452895700931549}, {"id": 58, "seek": 30096, "start": 316.56, "end": 322.2, "text": " Aitkin in October on the risks of this technology, which basically leaves one final big question", "tokens": [51144, 316, 270, 5843, 294, 7617, 322, 264, 10888, 295, 341, 2899, 11, 597, 1936, 5510, 472, 2572, 955, 1168, 51426], "temperature": 0.0, "avg_logprob": -0.28886426615918803, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.006452895700931549}, {"id": 59, "seek": 30096, "start": 322.2, "end": 327.88, "text": " unanswered, which is, we are here now, but what is the future of generative AI? And that", "tokens": [51426, 517, 43904, 292, 11, 597, 307, 11, 321, 366, 510, 586, 11, 457, 437, 307, 264, 2027, 295, 1337, 1166, 7318, 30, 400, 300, 51710], "temperature": 0.0, "avg_logprob": -0.28886426615918803, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.006452895700931549}, {"id": 60, "seek": 32788, "start": 327.88, "end": 334.88, "text": " is the focus for this evening. So that is pretty much it for the injury. Just a reminder,", "tokens": [50364, 307, 264, 1879, 337, 341, 5634, 13, 407, 300, 307, 1238, 709, 309, 337, 264, 10454, 13, 1449, 257, 13548, 11, 50714], "temperature": 0.0, "avg_logprob": -0.24449699065264532, "compression_ratio": 1.4505494505494505, "no_speech_prob": 0.020135624334216118}, {"id": 61, "seek": 32788, "start": 336.64, "end": 341.96, "text": " the lights are now going to go down and it will be quiet until exactly 7.30 when a soft", "tokens": [50802, 264, 5811, 366, 586, 516, 281, 352, 760, 293, 309, 486, 312, 5677, 1826, 2293, 1614, 13, 3446, 562, 257, 2787, 51068], "temperature": 0.0, "avg_logprob": -0.24449699065264532, "compression_ratio": 1.4505494505494505, "no_speech_prob": 0.020135624334216118}, {"id": 62, "seek": 32788, "start": 341.96, "end": 348.96, "text": " bell will ding and we will start the discourse. Hope you enjoy the evening. Thank you.", "tokens": [51068, 4549, 486, 21211, 293, 321, 486, 722, 264, 23938, 13, 6483, 291, 2103, 264, 5634, 13, 1044, 291, 13, 51418], "temperature": 0.0, "avg_logprob": -0.24449699065264532, "compression_ratio": 1.4505494505494505, "no_speech_prob": 0.020135624334216118}, {"id": 63, "seek": 35788, "start": 357.88, "end": 364.88, "text": " Artificial intelligence as a scientific discipline has been with us since just after the Second", "tokens": [50364, 5735, 10371, 7599, 382, 257, 8134, 13635, 575, 668, 365, 505, 1670, 445, 934, 264, 5736, 50714], "temperature": 0.0, "avg_logprob": -0.17079743023576408, "compression_ratio": 1.46448087431694, "no_speech_prob": 0.010748076252639294}, {"id": 64, "seek": 35788, "start": 374.44, "end": 379.92, "text": " World War. It began roughly speaking with the advent of the first digital computers.", "tokens": [51192, 3937, 3630, 13, 467, 4283, 9810, 4124, 365, 264, 7045, 295, 264, 700, 4562, 10807, 13, 51466], "temperature": 0.0, "avg_logprob": -0.17079743023576408, "compression_ratio": 1.46448087431694, "no_speech_prob": 0.010748076252639294}, {"id": 65, "seek": 35788, "start": 379.92, "end": 384.48, "text": " But I have to tell you that for most of the time until recently, progress in artificial", "tokens": [51466, 583, 286, 362, 281, 980, 291, 300, 337, 881, 295, 264, 565, 1826, 3938, 11, 4205, 294, 11677, 51694], "temperature": 0.0, "avg_logprob": -0.17079743023576408, "compression_ratio": 1.46448087431694, "no_speech_prob": 0.010748076252639294}, {"id": 66, "seek": 38448, "start": 384.52000000000004, "end": 391.52000000000004, "text": " intelligence was glacially slow. That started to change this century. Artificial intelligence", "tokens": [50366, 7599, 390, 29700, 2270, 2964, 13, 663, 1409, 281, 1319, 341, 4901, 13, 5735, 10371, 7599, 50716], "temperature": 0.0, "avg_logprob": -0.1367965844961313, "compression_ratio": 1.766355140186916, "no_speech_prob": 0.008699501864612103}, {"id": 67, "seek": 38448, "start": 392.24, "end": 398.44, "text": " is a very broad discipline which encompasses a very wide range of different techniques,", "tokens": [50752, 307, 257, 588, 4152, 13635, 597, 49866, 257, 588, 4874, 3613, 295, 819, 7512, 11, 51062], "temperature": 0.0, "avg_logprob": -0.1367965844961313, "compression_ratio": 1.766355140186916, "no_speech_prob": 0.008699501864612103}, {"id": 68, "seek": 38448, "start": 398.44, "end": 405.44, "text": " but it was one class of AI techniques in particular that began to work this century and in particular", "tokens": [51062, 457, 309, 390, 472, 1508, 295, 7318, 7512, 294, 1729, 300, 4283, 281, 589, 341, 4901, 293, 294, 1729, 51412], "temperature": 0.0, "avg_logprob": -0.1367965844961313, "compression_ratio": 1.766355140186916, "no_speech_prob": 0.008699501864612103}, {"id": 69, "seek": 38448, "start": 405.52000000000004, "end": 412.52000000000004, "text": " began to work around about 2005. And the class of techniques which started to work at problems", "tokens": [51416, 4283, 281, 589, 926, 466, 14394, 13, 400, 264, 1508, 295, 7512, 597, 1409, 281, 589, 412, 2740, 51766], "temperature": 0.0, "avg_logprob": -0.1367965844961313, "compression_ratio": 1.766355140186916, "no_speech_prob": 0.008699501864612103}, {"id": 70, "seek": 41252, "start": 413.35999999999996, "end": 417.96, "text": " that were interesting enough to be really practical, practically useful in a wide range", "tokens": [50406, 300, 645, 1880, 1547, 281, 312, 534, 8496, 11, 15667, 4420, 294, 257, 4874, 3613, 50636], "temperature": 0.0, "avg_logprob": -0.11481425785782313, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.011603033170104027}, {"id": 71, "seek": 41252, "start": 417.96, "end": 424.28, "text": " of settings were machine learning. Now, like so many other names in the field of artificial", "tokens": [50636, 295, 6257, 645, 3479, 2539, 13, 823, 11, 411, 370, 867, 661, 5288, 294, 264, 2519, 295, 11677, 50952], "temperature": 0.0, "avg_logprob": -0.11481425785782313, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.011603033170104027}, {"id": 72, "seek": 41252, "start": 424.28, "end": 429.03999999999996, "text": " intelligence, the name machine learning is really, really unhelpful. It suggests that", "tokens": [50952, 7599, 11, 264, 1315, 3479, 2539, 307, 534, 11, 534, 517, 37451, 906, 13, 467, 13409, 300, 51190], "temperature": 0.0, "avg_logprob": -0.11481425785782313, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.011603033170104027}, {"id": 73, "seek": 41252, "start": 429.03999999999996, "end": 434.59999999999997, "text": " a computer, for example, locks itself away in a room with a textbook and trains itself", "tokens": [51190, 257, 3820, 11, 337, 1365, 11, 20703, 2564, 1314, 294, 257, 1808, 365, 257, 25591, 293, 16329, 2564, 51468], "temperature": 0.0, "avg_logprob": -0.11481425785782313, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.011603033170104027}, {"id": 74, "seek": 41252, "start": 434.59999999999997, "end": 438.47999999999996, "text": " how to read French or something like that. That is not what is going on. So we are going", "tokens": [51468, 577, 281, 1401, 5522, 420, 746, 411, 300, 13, 663, 307, 406, 437, 307, 516, 322, 13, 407, 321, 366, 516, 51662], "temperature": 0.0, "avg_logprob": -0.11481425785782313, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.011603033170104027}, {"id": 75, "seek": 43848, "start": 438.48, "end": 444.64000000000004, "text": " to begin by understanding a little bit more about what machine learning is and how machine", "tokens": [50364, 281, 1841, 538, 3701, 257, 707, 857, 544, 466, 437, 3479, 2539, 307, 293, 577, 3479, 50672], "temperature": 0.0, "avg_logprob": -0.13496292184252257, "compression_ratio": 1.8174273858921162, "no_speech_prob": 0.15842361748218536}, {"id": 76, "seek": 43848, "start": 444.64000000000004, "end": 450.64000000000004, "text": " learning works. So to start us off, who is this? Anybody recognize this face? Do you", "tokens": [50672, 2539, 1985, 13, 407, 281, 722, 505, 766, 11, 567, 307, 341, 30, 19082, 5521, 341, 1851, 30, 1144, 291, 50972], "temperature": 0.0, "avg_logprob": -0.13496292184252257, "compression_ratio": 1.8174273858921162, "no_speech_prob": 0.15842361748218536}, {"id": 77, "seek": 43848, "start": 450.64000000000004, "end": 456.64000000000004, "text": " recognize this face? It is the face of Alan Turing. Well done. Alan Turing, the late great", "tokens": [50972, 5521, 341, 1851, 30, 467, 307, 264, 1851, 295, 16442, 314, 1345, 13, 1042, 1096, 13, 16442, 314, 1345, 11, 264, 3469, 869, 51272], "temperature": 0.0, "avg_logprob": -0.13496292184252257, "compression_ratio": 1.8174273858921162, "no_speech_prob": 0.15842361748218536}, {"id": 78, "seek": 43848, "start": 456.64000000000004, "end": 460.96000000000004, "text": " Alan Turing. We all know a little bit about Alan Turing from his code-breaking work in", "tokens": [51272, 16442, 314, 1345, 13, 492, 439, 458, 257, 707, 857, 466, 16442, 314, 1345, 490, 702, 3089, 12, 20602, 589, 294, 51488], "temperature": 0.0, "avg_logprob": -0.13496292184252257, "compression_ratio": 1.8174273858921162, "no_speech_prob": 0.15842361748218536}, {"id": 79, "seek": 43848, "start": 460.96000000000004, "end": 466.92, "text": " the Second World War. We should also know a lot more about this individual's amazing", "tokens": [51488, 264, 5736, 3937, 3630, 13, 492, 820, 611, 458, 257, 688, 544, 466, 341, 2609, 311, 2243, 51786], "temperature": 0.0, "avg_logprob": -0.13496292184252257, "compression_ratio": 1.8174273858921162, "no_speech_prob": 0.15842361748218536}, {"id": 80, "seek": 46692, "start": 467.16, "end": 472.8, "text": " life. So what we are going to do is we are going to use Alan Turing to help us understand", "tokens": [50376, 993, 13, 407, 437, 321, 366, 516, 281, 360, 307, 321, 366, 516, 281, 764, 16442, 314, 1345, 281, 854, 505, 1223, 50658], "temperature": 0.0, "avg_logprob": -0.12196044746888887, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.012979741208255291}, {"id": 81, "seek": 46692, "start": 472.8, "end": 480.36, "text": " machine learning. So a classic application of artificial intelligence is to do facial", "tokens": [50658, 3479, 2539, 13, 407, 257, 7230, 3861, 295, 11677, 7599, 307, 281, 360, 15642, 51036], "temperature": 0.0, "avg_logprob": -0.12196044746888887, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.012979741208255291}, {"id": 82, "seek": 46692, "start": 480.36, "end": 485.64, "text": " recognition. And the idea in facial recognition is that we want to show the computer a picture", "tokens": [51036, 11150, 13, 400, 264, 1558, 294, 15642, 11150, 307, 300, 321, 528, 281, 855, 264, 3820, 257, 3036, 51300], "temperature": 0.0, "avg_logprob": -0.12196044746888887, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.012979741208255291}, {"id": 83, "seek": 46692, "start": 485.64, "end": 490.92, "text": " of a human face and for the computer to tell us whose face that is. So in this case, for", "tokens": [51300, 295, 257, 1952, 1851, 293, 337, 264, 3820, 281, 980, 505, 6104, 1851, 300, 307, 13, 407, 294, 341, 1389, 11, 337, 51564], "temperature": 0.0, "avg_logprob": -0.12196044746888887, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.012979741208255291}, {"id": 84, "seek": 46692, "start": 490.92, "end": 495.48, "text": " example, we show it a picture of Alan Turing and ideally it would tell us that it is Alan", "tokens": [51564, 1365, 11, 321, 855, 309, 257, 3036, 295, 16442, 314, 1345, 293, 22915, 309, 576, 980, 505, 300, 309, 307, 16442, 51792], "temperature": 0.0, "avg_logprob": -0.12196044746888887, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.012979741208255291}, {"id": 85, "seek": 49548, "start": 495.52000000000004, "end": 502.52000000000004, "text": " Turing. So how does it actually work? Well, the simplest way of getting machine learning", "tokens": [50366, 314, 1345, 13, 407, 577, 775, 309, 767, 589, 30, 1042, 11, 264, 22811, 636, 295, 1242, 3479, 2539, 50716], "temperature": 0.0, "avg_logprob": -0.15130898009899052, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.004411862697452307}, {"id": 86, "seek": 49548, "start": 505.24, "end": 511.24, "text": " to be able to do something is what is called supervised learning. And supervised learning", "tokens": [50852, 281, 312, 1075, 281, 360, 746, 307, 437, 307, 1219, 46533, 2539, 13, 400, 46533, 2539, 51152], "temperature": 0.0, "avg_logprob": -0.15130898009899052, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.004411862697452307}, {"id": 87, "seek": 49548, "start": 511.24, "end": 517.16, "text": " like all of machine learning requires what we call training data. So in this case, the", "tokens": [51152, 411, 439, 295, 3479, 2539, 7029, 437, 321, 818, 3097, 1412, 13, 407, 294, 341, 1389, 11, 264, 51448], "temperature": 0.0, "avg_logprob": -0.15130898009899052, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.004411862697452307}, {"id": 88, "seek": 49548, "start": 517.16, "end": 523.16, "text": " training data is on the right-hand side of the slide. It is a set of what input-output", "tokens": [51448, 3097, 1412, 307, 322, 264, 558, 12, 5543, 1252, 295, 264, 4137, 13, 467, 307, 257, 992, 295, 437, 4846, 12, 346, 2582, 51748], "temperature": 0.0, "avg_logprob": -0.15130898009899052, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.004411862697452307}, {"id": 89, "seek": 52316, "start": 523.24, "end": 528.92, "text": " pairs, what we call the training data set. And each input-output pair consists of an", "tokens": [50368, 15494, 11, 437, 321, 818, 264, 3097, 1412, 992, 13, 400, 1184, 4846, 12, 346, 2582, 6119, 14689, 295, 364, 50652], "temperature": 0.0, "avg_logprob": -0.1299904660061673, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.0010029228869825602}, {"id": 90, "seek": 52316, "start": 528.92, "end": 535.52, "text": " input. If I gave you this and an output, I would want you to produce this. So in this", "tokens": [50652, 4846, 13, 759, 286, 2729, 291, 341, 293, 364, 5598, 11, 286, 576, 528, 291, 281, 5258, 341, 13, 407, 294, 341, 50982], "temperature": 0.0, "avg_logprob": -0.1299904660061673, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.0010029228869825602}, {"id": 91, "seek": 52316, "start": 535.52, "end": 540.3199999999999, "text": " case, we have got a bunch of pictures again of Alan Turing, the picture of Alan Turing", "tokens": [50982, 1389, 11, 321, 362, 658, 257, 3840, 295, 5242, 797, 295, 16442, 314, 1345, 11, 264, 3036, 295, 16442, 314, 1345, 51222], "temperature": 0.0, "avg_logprob": -0.1299904660061673, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.0010029228869825602}, {"id": 92, "seek": 52316, "start": 540.3199999999999, "end": 546.0, "text": " and the text that we would want the computer to create if we showed it that picture. And", "tokens": [51222, 293, 264, 2487, 300, 321, 576, 528, 264, 3820, 281, 1884, 498, 321, 4712, 309, 300, 3036, 13, 400, 51506], "temperature": 0.0, "avg_logprob": -0.1299904660061673, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.0010029228869825602}, {"id": 93, "seek": 52316, "start": 546.0, "end": 551.88, "text": " this is supervised learning because we are showing the computer what we want it to do.", "tokens": [51506, 341, 307, 46533, 2539, 570, 321, 366, 4099, 264, 3820, 437, 321, 528, 309, 281, 360, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1299904660061673, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.0010029228869825602}, {"id": 94, "seek": 55188, "start": 551.96, "end": 557.12, "text": " So by helping it in a sense, we are saying this is a picture of Alan Turing. If I showed", "tokens": [50368, 407, 538, 4315, 309, 294, 257, 2020, 11, 321, 366, 1566, 341, 307, 257, 3036, 295, 16442, 314, 1345, 13, 759, 286, 4712, 50626], "temperature": 0.0, "avg_logprob": -0.15541983517733488, "compression_ratio": 1.9525862068965518, "no_speech_prob": 0.003164189402014017}, {"id": 95, "seek": 55188, "start": 557.12, "end": 561.24, "text": " you this picture, this is what I would want you to print out. So there could be a picture", "tokens": [50626, 291, 341, 3036, 11, 341, 307, 437, 286, 576, 528, 291, 281, 4482, 484, 13, 407, 456, 727, 312, 257, 3036, 50832], "temperature": 0.0, "avg_logprob": -0.15541983517733488, "compression_ratio": 1.9525862068965518, "no_speech_prob": 0.003164189402014017}, {"id": 96, "seek": 55188, "start": 561.24, "end": 566.08, "text": " of me and the picture of me would be labeled with the text Michael Wildridge. If I showed", "tokens": [50832, 295, 385, 293, 264, 3036, 295, 385, 576, 312, 21335, 365, 264, 2487, 5116, 10904, 15804, 13, 759, 286, 4712, 51074], "temperature": 0.0, "avg_logprob": -0.15541983517733488, "compression_ratio": 1.9525862068965518, "no_speech_prob": 0.003164189402014017}, {"id": 97, "seek": 55188, "start": 566.08, "end": 571.72, "text": " you this picture, then this is what I would want you to print out. So we have just learned", "tokens": [51074, 291, 341, 3036, 11, 550, 341, 307, 437, 286, 576, 528, 291, 281, 4482, 484, 13, 407, 321, 362, 445, 3264, 51356], "temperature": 0.0, "avg_logprob": -0.15541983517733488, "compression_ratio": 1.9525862068965518, "no_speech_prob": 0.003164189402014017}, {"id": 98, "seek": 55188, "start": 571.72, "end": 575.76, "text": " an important lesson about artificial intelligence and machine learning in particular and that", "tokens": [51356, 364, 1021, 6898, 466, 11677, 7599, 293, 3479, 2539, 294, 1729, 293, 300, 51558], "temperature": 0.0, "avg_logprob": -0.15541983517733488, "compression_ratio": 1.9525862068965518, "no_speech_prob": 0.003164189402014017}, {"id": 99, "seek": 57576, "start": 575.76, "end": 583.08, "text": " lesson is that AI requires training data. And in this case, the pictures of Alan Turing", "tokens": [50364, 6898, 307, 300, 7318, 7029, 3097, 1412, 13, 400, 294, 341, 1389, 11, 264, 5242, 295, 16442, 314, 1345, 50730], "temperature": 0.0, "avg_logprob": -0.10332366016423591, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.008439416065812111}, {"id": 100, "seek": 57576, "start": 583.08, "end": 588.52, "text": " labeled with the text that we would want the computer to produce. If I showed you this", "tokens": [50730, 21335, 365, 264, 2487, 300, 321, 576, 528, 264, 3820, 281, 5258, 13, 759, 286, 4712, 291, 341, 51002], "temperature": 0.0, "avg_logprob": -0.10332366016423591, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.008439416065812111}, {"id": 101, "seek": 57576, "start": 588.52, "end": 595.48, "text": " picture, I would want you to produce the text Alan Turing. Okay. Training data is important.", "tokens": [51002, 3036, 11, 286, 576, 528, 291, 281, 5258, 264, 2487, 16442, 314, 1345, 13, 1033, 13, 20620, 1412, 307, 1021, 13, 51350], "temperature": 0.0, "avg_logprob": -0.10332366016423591, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.008439416065812111}, {"id": 102, "seek": 57576, "start": 595.48, "end": 600.96, "text": " Every time you go on social media and you upload a picture to social media and you label", "tokens": [51350, 2048, 565, 291, 352, 322, 2093, 3021, 293, 291, 6580, 257, 3036, 281, 2093, 3021, 293, 291, 7645, 51624], "temperature": 0.0, "avg_logprob": -0.10332366016423591, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.008439416065812111}, {"id": 103, "seek": 57576, "start": 600.96, "end": 605.2, "text": " it with the names of the people that appear in there, your role in that is to provide", "tokens": [51624, 309, 365, 264, 5288, 295, 264, 561, 300, 4204, 294, 456, 11, 428, 3090, 294, 300, 307, 281, 2893, 51836], "temperature": 0.0, "avg_logprob": -0.10332366016423591, "compression_ratio": 1.83402489626556, "no_speech_prob": 0.008439416065812111}, {"id": 104, "seek": 60520, "start": 605.24, "end": 614.96, "text": " training data for the machine learning algorithms of big data companies. Okay. So this is supervised", "tokens": [50366, 3097, 1412, 337, 264, 3479, 2539, 14642, 295, 955, 1412, 3431, 13, 1033, 13, 407, 341, 307, 46533, 50852], "temperature": 0.0, "avg_logprob": -0.14606480214787626, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0011789001291617751}, {"id": 105, "seek": 60520, "start": 614.96, "end": 620.6800000000001, "text": " learning. Now we're going to come on to exactly how it does the learning in a moment. But", "tokens": [50852, 2539, 13, 823, 321, 434, 516, 281, 808, 322, 281, 2293, 577, 309, 775, 264, 2539, 294, 257, 1623, 13, 583, 51138], "temperature": 0.0, "avg_logprob": -0.14606480214787626, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0011789001291617751}, {"id": 106, "seek": 60520, "start": 620.6800000000001, "end": 625.6, "text": " the first thing I want to point out is that this is a classification task. What I mean", "tokens": [51138, 264, 700, 551, 286, 528, 281, 935, 484, 307, 300, 341, 307, 257, 21538, 5633, 13, 708, 286, 914, 51384], "temperature": 0.0, "avg_logprob": -0.14606480214787626, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0011789001291617751}, {"id": 107, "seek": 60520, "start": 625.6, "end": 631.36, "text": " by that is as we show it the picture, the machine learning is classifying that picture.", "tokens": [51384, 538, 300, 307, 382, 321, 855, 309, 264, 3036, 11, 264, 3479, 2539, 307, 1508, 5489, 300, 3036, 13, 51672], "temperature": 0.0, "avg_logprob": -0.14606480214787626, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0011789001291617751}, {"id": 108, "seek": 63136, "start": 631.44, "end": 636.04, "text": " I'm classifying this as a picture of Michael Woodridge. This is a picture of Alan Turing", "tokens": [50368, 286, 478, 1508, 5489, 341, 382, 257, 3036, 295, 5116, 11558, 15804, 13, 639, 307, 257, 3036, 295, 16442, 314, 1345, 50598], "temperature": 0.0, "avg_logprob": -0.15929000679103808, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.0041021923534572124}, {"id": 109, "seek": 63136, "start": 636.04, "end": 642.5600000000001, "text": " and so on. And this is a technology which really started to work around about beginning", "tokens": [50598, 293, 370, 322, 13, 400, 341, 307, 257, 2899, 597, 534, 1409, 281, 589, 926, 466, 2863, 50924], "temperature": 0.0, "avg_logprob": -0.15929000679103808, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.0041021923534572124}, {"id": 110, "seek": 63136, "start": 642.5600000000001, "end": 650.72, "text": " 2005. It started to take off, but really, really got supercharged around about 2012. And just", "tokens": [50924, 14394, 13, 467, 1409, 281, 747, 766, 11, 457, 534, 11, 534, 658, 1687, 25064, 926, 466, 9125, 13, 400, 445, 51332], "temperature": 0.0, "avg_logprob": -0.15929000679103808, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.0041021923534572124}, {"id": 111, "seek": 63136, "start": 650.8000000000001, "end": 658.2, "text": " this kind of task on its own is incredibly powerful. Exactly this technology can be used,", "tokens": [51336, 341, 733, 295, 5633, 322, 1080, 1065, 307, 6252, 4005, 13, 7587, 341, 2899, 393, 312, 1143, 11, 51706], "temperature": 0.0, "avg_logprob": -0.15929000679103808, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.0041021923534572124}, {"id": 112, "seek": 65820, "start": 658.24, "end": 665.32, "text": " for example, to recognize tumors on X-ray scans or abnormalities on ultrasound scans and a", "tokens": [50366, 337, 1365, 11, 281, 5521, 38466, 322, 1783, 12, 3458, 35116, 420, 47104, 16110, 322, 40895, 35116, 293, 257, 50720], "temperature": 0.0, "avg_logprob": -0.13233318152251067, "compression_ratio": 1.79296875, "no_speech_prob": 0.005862487945705652}, {"id": 113, "seek": 65820, "start": 665.32, "end": 672.24, "text": " range of different tasks. Does anybody in the audience own a Tesla? A couple of Tesla drivers", "tokens": [50720, 3613, 295, 819, 9608, 13, 4402, 4472, 294, 264, 4034, 1065, 257, 13666, 30, 316, 1916, 295, 13666, 11590, 51066], "temperature": 0.0, "avg_logprob": -0.13233318152251067, "compression_ratio": 1.79296875, "no_speech_prob": 0.005862487945705652}, {"id": 114, "seek": 65820, "start": 672.24, "end": 675.84, "text": " not quite sure whether they want to admit they own a Tesla. We've got a couple of Tesla", "tokens": [51066, 406, 1596, 988, 1968, 436, 528, 281, 9796, 436, 1065, 257, 13666, 13, 492, 600, 658, 257, 1916, 295, 13666, 51246], "temperature": 0.0, "avg_logprob": -0.13233318152251067, "compression_ratio": 1.79296875, "no_speech_prob": 0.005862487945705652}, {"id": 115, "seek": 65820, "start": 675.84, "end": 683.12, "text": " drivers in the audience. Tesla full self-driving mode is only possible because of this technology.", "tokens": [51246, 11590, 294, 264, 4034, 13, 13666, 1577, 2698, 12, 47094, 4391, 307, 787, 1944, 570, 295, 341, 2899, 13, 51610], "temperature": 0.0, "avg_logprob": -0.13233318152251067, "compression_ratio": 1.79296875, "no_speech_prob": 0.005862487945705652}, {"id": 116, "seek": 65820, "start": 683.12, "end": 687.84, "text": " It is this technology which is enabling a Tesla in full self-driving mode to be able to", "tokens": [51610, 467, 307, 341, 2899, 597, 307, 23148, 257, 13666, 294, 1577, 2698, 12, 47094, 4391, 281, 312, 1075, 281, 51846], "temperature": 0.0, "avg_logprob": -0.13233318152251067, "compression_ratio": 1.79296875, "no_speech_prob": 0.005862487945705652}, {"id": 117, "seek": 68784, "start": 687.88, "end": 693.88, "text": " recognize that that is a stop sign, that that's somebody on a bicycle, that that's a pedestrian", "tokens": [50366, 5521, 300, 300, 307, 257, 1590, 1465, 11, 300, 300, 311, 2618, 322, 257, 20888, 11, 300, 300, 311, 257, 33947, 50666], "temperature": 0.0, "avg_logprob": -0.16169541035223445, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.002817414002493024}, {"id": 118, "seek": 68784, "start": 693.88, "end": 699.2800000000001, "text": " on a zebra crossing and so on. These are classification tasks. And I'm going to come back", "tokens": [50666, 322, 257, 47060, 14712, 293, 370, 322, 13, 1981, 366, 21538, 9608, 13, 400, 286, 478, 516, 281, 808, 646, 50936], "temperature": 0.0, "avg_logprob": -0.16169541035223445, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.002817414002493024}, {"id": 119, "seek": 68784, "start": 699.2800000000001, "end": 705.24, "text": " and explain how classification tasks are different to generative AI later on.", "tokens": [50936, 293, 2903, 577, 21538, 9608, 366, 819, 281, 1337, 1166, 7318, 1780, 322, 13, 51234], "temperature": 0.0, "avg_logprob": -0.16169541035223445, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.002817414002493024}, {"id": 120, "seek": 68784, "start": 705.24, "end": 710.96, "text": " Okay, so this is machine learning. How does it actually work? Okay, this is not a technical", "tokens": [51234, 1033, 11, 370, 341, 307, 3479, 2539, 13, 1012, 775, 309, 767, 589, 30, 1033, 11, 341, 307, 406, 257, 6191, 51520], "temperature": 0.0, "avg_logprob": -0.16169541035223445, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.002817414002493024}, {"id": 121, "seek": 68784, "start": 710.96, "end": 716.72, "text": " presentation. And this is about as technical as it's going to get, where I do a very hand", "tokens": [51520, 5860, 13, 400, 341, 307, 466, 382, 6191, 382, 309, 311, 516, 281, 483, 11, 689, 286, 360, 257, 588, 1011, 51808], "temperature": 0.0, "avg_logprob": -0.16169541035223445, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.002817414002493024}, {"id": 122, "seek": 71672, "start": 716.8000000000001, "end": 722.9200000000001, "text": " wavy explanation of what neural networks are and how do they work. And with apologies,", "tokens": [50368, 261, 15498, 10835, 295, 437, 18161, 9590, 366, 293, 577, 360, 436, 589, 13, 400, 365, 34929, 11, 50674], "temperature": 0.0, "avg_logprob": -0.14445032137576666, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.0022675825748592615}, {"id": 123, "seek": 71672, "start": 722.9200000000001, "end": 727.5600000000001, "text": " I know I have a couple of neural network experts in the audience and I apologize to you because", "tokens": [50674, 286, 458, 286, 362, 257, 1916, 295, 18161, 3209, 8572, 294, 264, 4034, 293, 286, 12328, 281, 291, 570, 50906], "temperature": 0.0, "avg_logprob": -0.14445032137576666, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.0022675825748592615}, {"id": 124, "seek": 71672, "start": 727.5600000000001, "end": 732.5600000000001, "text": " you'll be cringing with my explanation, but the technical details are way too technical to go", "tokens": [50906, 291, 603, 312, 941, 8716, 365, 452, 10835, 11, 457, 264, 6191, 4365, 366, 636, 886, 6191, 281, 352, 51156], "temperature": 0.0, "avg_logprob": -0.14445032137576666, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.0022675825748592615}, {"id": 125, "seek": 71672, "start": 732.5600000000001, "end": 740.1600000000001, "text": " into. So how does a neural network recognize Alan Turing? Okay, so firstly, what is a neural", "tokens": [51156, 666, 13, 407, 577, 775, 257, 18161, 3209, 5521, 16442, 314, 1345, 30, 1033, 11, 370, 27376, 11, 437, 307, 257, 18161, 51536], "temperature": 0.0, "avg_logprob": -0.14445032137576666, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.0022675825748592615}, {"id": 126, "seek": 71672, "start": 740.1600000000001, "end": 746.2, "text": " network? Look at an animal brain or nervous system under a microscope and you'll find", "tokens": [51536, 3209, 30, 2053, 412, 364, 5496, 3567, 420, 6296, 1185, 833, 257, 29753, 293, 291, 603, 915, 51838], "temperature": 0.0, "avg_logprob": -0.14445032137576666, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.0022675825748592615}, {"id": 127, "seek": 74620, "start": 746.32, "end": 752.24, "text": " that it contains enormous numbers of nerve cells called neurons. And those nerve cells are", "tokens": [50370, 300, 309, 8306, 11322, 3547, 295, 16355, 5438, 1219, 22027, 13, 400, 729, 16355, 5438, 366, 50666], "temperature": 0.0, "avg_logprob": -0.121146365328952, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.00796397402882576}, {"id": 128, "seek": 74620, "start": 752.24, "end": 757.6400000000001, "text": " connected to one another in vast networks. Now, we don't have precise figures, but in a human", "tokens": [50666, 4582, 281, 472, 1071, 294, 8369, 9590, 13, 823, 11, 321, 500, 380, 362, 13600, 9624, 11, 457, 294, 257, 1952, 50936], "temperature": 0.0, "avg_logprob": -0.121146365328952, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.00796397402882576}, {"id": 129, "seek": 74620, "start": 757.6400000000001, "end": 763.36, "text": " brain, the current estimate is something like 86 billion neurons in the human brain, how they got", "tokens": [50936, 3567, 11, 264, 2190, 12539, 307, 746, 411, 26687, 5218, 22027, 294, 264, 1952, 3567, 11, 577, 436, 658, 51222], "temperature": 0.0, "avg_logprob": -0.121146365328952, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.00796397402882576}, {"id": 130, "seek": 74620, "start": 763.36, "end": 770.44, "text": " to 86 as opposed to 85 or 87, I don't know, but 86 seems to be the most commonly quoted number of", "tokens": [51222, 281, 26687, 382, 8851, 281, 14695, 420, 27990, 11, 286, 500, 380, 458, 11, 457, 26687, 2544, 281, 312, 264, 881, 12719, 30047, 1230, 295, 51576], "temperature": 0.0, "avg_logprob": -0.121146365328952, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.00796397402882576}, {"id": 131, "seek": 74620, "start": 770.44, "end": 775.96, "text": " these cells. And these cells are connected to one another in enormous networks. One neuron can", "tokens": [51576, 613, 5438, 13, 400, 613, 5438, 366, 4582, 281, 472, 1071, 294, 11322, 9590, 13, 1485, 34090, 393, 51852], "temperature": 0.0, "avg_logprob": -0.121146365328952, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.00796397402882576}, {"id": 132, "seek": 77596, "start": 775.96, "end": 785.52, "text": " be connected to up to 8,000 other neurons. Okay, and each of those neurons is doing a tiny, very,", "tokens": [50364, 312, 4582, 281, 493, 281, 1649, 11, 1360, 661, 22027, 13, 1033, 11, 293, 1184, 295, 729, 22027, 307, 884, 257, 5870, 11, 588, 11, 50842], "temperature": 0.0, "avg_logprob": -0.11034026437876177, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.0013142620446160436}, {"id": 133, "seek": 77596, "start": 785.52, "end": 791.88, "text": " very simple pattern recognition task. That neuron is looking for a very, very simple pattern. And", "tokens": [50842, 588, 2199, 5102, 11150, 5633, 13, 663, 34090, 307, 1237, 337, 257, 588, 11, 588, 2199, 5102, 13, 400, 51160], "temperature": 0.0, "avg_logprob": -0.11034026437876177, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.0013142620446160436}, {"id": 134, "seek": 77596, "start": 791.88, "end": 798.9200000000001, "text": " when it sees that pattern, it sends a signal to its connections. It sends a signal to all the other", "tokens": [51160, 562, 309, 8194, 300, 5102, 11, 309, 14790, 257, 6358, 281, 1080, 9271, 13, 467, 14790, 257, 6358, 281, 439, 264, 661, 51512], "temperature": 0.0, "avg_logprob": -0.11034026437876177, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.0013142620446160436}, {"id": 135, "seek": 77596, "start": 798.9200000000001, "end": 805.12, "text": " neurons that it's connected to. So how does that get us to recognizing the face of Alan Turing?", "tokens": [51512, 22027, 300, 309, 311, 4582, 281, 13, 407, 577, 775, 300, 483, 505, 281, 18538, 264, 1851, 295, 16442, 314, 1345, 30, 51822], "temperature": 0.0, "avg_logprob": -0.11034026437876177, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.0013142620446160436}, {"id": 136, "seek": 80512, "start": 805.68, "end": 811.32, "text": " So Turing's picture, as we know, picture, a digital picture is made up of millions of colored", "tokens": [50392, 407, 314, 1345, 311, 3036, 11, 382, 321, 458, 11, 3036, 11, 257, 4562, 3036, 307, 1027, 493, 295, 6803, 295, 14332, 50674], "temperature": 0.0, "avg_logprob": -0.12535317090092873, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.001128839561715722}, {"id": 137, "seek": 80512, "start": 811.32, "end": 818.12, "text": " dots, the pixels. Yeah, so your smartphone maybe has 12 megapixels, 12 million colored dots making", "tokens": [50674, 15026, 11, 264, 18668, 13, 865, 11, 370, 428, 13307, 1310, 575, 2272, 34733, 970, 1625, 11, 2272, 2459, 14332, 15026, 1455, 51014], "temperature": 0.0, "avg_logprob": -0.12535317090092873, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.001128839561715722}, {"id": 138, "seek": 80512, "start": 818.12, "end": 823.4, "text": " up that picture. Okay, so Turing's picture there is made up of millions and millions of colored", "tokens": [51014, 493, 300, 3036, 13, 1033, 11, 370, 314, 1345, 311, 3036, 456, 307, 1027, 493, 295, 6803, 293, 6803, 295, 14332, 51278], "temperature": 0.0, "avg_logprob": -0.12535317090092873, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.001128839561715722}, {"id": 139, "seek": 80512, "start": 823.4, "end": 831.0, "text": " dots. So look at the top left neuron on that input layer. So that neuron is just looking for a very", "tokens": [51278, 15026, 13, 407, 574, 412, 264, 1192, 1411, 34090, 322, 300, 4846, 4583, 13, 407, 300, 34090, 307, 445, 1237, 337, 257, 588, 51658], "temperature": 0.0, "avg_logprob": -0.12535317090092873, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.001128839561715722}, {"id": 140, "seek": 83100, "start": 831.04, "end": 835.88, "text": " simple pattern. What might that pattern be? It might just be the color red. All that neuron's", "tokens": [50366, 2199, 5102, 13, 708, 1062, 300, 5102, 312, 30, 467, 1062, 445, 312, 264, 2017, 2182, 13, 1057, 300, 34090, 311, 50608], "temperature": 0.0, "avg_logprob": -0.11260935130872224, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.007085861638188362}, {"id": 141, "seek": 83100, "start": 835.88, "end": 842.32, "text": " doing is looking for the color red. And when it sees the color red on its associated pixel,", "tokens": [50608, 884, 307, 1237, 337, 264, 2017, 2182, 13, 400, 562, 309, 8194, 264, 2017, 2182, 322, 1080, 6615, 19261, 11, 50930], "temperature": 0.0, "avg_logprob": -0.11260935130872224, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.007085861638188362}, {"id": 142, "seek": 83100, "start": 842.32, "end": 848.64, "text": " the one on the top left there, it becomes excited and it sends a signal out to all of its neighbors.", "tokens": [50930, 264, 472, 322, 264, 1192, 1411, 456, 11, 309, 3643, 2919, 293, 309, 14790, 257, 6358, 484, 281, 439, 295, 1080, 12512, 13, 51246], "temperature": 0.0, "avg_logprob": -0.11260935130872224, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.007085861638188362}, {"id": 143, "seek": 83100, "start": 848.64, "end": 855.08, "text": " Okay, so look at the next neuron along. Maybe what that neuron is doing is just looking to see", "tokens": [51246, 1033, 11, 370, 574, 412, 264, 958, 34090, 2051, 13, 2704, 437, 300, 34090, 307, 884, 307, 445, 1237, 281, 536, 51568], "temperature": 0.0, "avg_logprob": -0.11260935130872224, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.007085861638188362}, {"id": 144, "seek": 85508, "start": 855.36, "end": 862.44, "text": " whether a majority of its incoming connections are red. Yeah, and when it sees a majority of its", "tokens": [50378, 1968, 257, 6286, 295, 1080, 22341, 9271, 366, 2182, 13, 865, 11, 293, 562, 309, 8194, 257, 6286, 295, 1080, 50732], "temperature": 0.0, "avg_logprob": -0.14221403333875868, "compression_ratio": 1.8932038834951457, "no_speech_prob": 0.007707123178988695}, {"id": 145, "seek": 85508, "start": 862.44, "end": 869.0400000000001, "text": " incoming connections are red, then it becomes excited and it sends a signal to its neighbor. Now,", "tokens": [50732, 22341, 9271, 366, 2182, 11, 550, 309, 3643, 2919, 293, 309, 14790, 257, 6358, 281, 1080, 5987, 13, 823, 11, 51062], "temperature": 0.0, "avg_logprob": -0.14221403333875868, "compression_ratio": 1.8932038834951457, "no_speech_prob": 0.007707123178988695}, {"id": 146, "seek": 85508, "start": 869.0400000000001, "end": 874.6800000000001, "text": " remember, in the human brain, there's something like 86 billion of those, and we've got something", "tokens": [51062, 1604, 11, 294, 264, 1952, 3567, 11, 456, 311, 746, 411, 26687, 5218, 295, 729, 11, 293, 321, 600, 658, 746, 51344], "temperature": 0.0, "avg_logprob": -0.14221403333875868, "compression_ratio": 1.8932038834951457, "no_speech_prob": 0.007707123178988695}, {"id": 147, "seek": 85508, "start": 874.6800000000001, "end": 880.32, "text": " like 20 or so outgoing connections for each of these neurons in a human brain, thousands of those", "tokens": [51344, 411, 945, 420, 370, 41565, 9271, 337, 1184, 295, 613, 22027, 294, 257, 1952, 3567, 11, 5383, 295, 729, 51626], "temperature": 0.0, "avg_logprob": -0.14221403333875868, "compression_ratio": 1.8932038834951457, "no_speech_prob": 0.007707123178988695}, {"id": 148, "seek": 88032, "start": 880.4000000000001, "end": 887.5600000000001, "text": " connections. Yeah, and somehow in ways that, to be honest, we don't really understand in detail", "tokens": [50368, 9271, 13, 865, 11, 293, 6063, 294, 2098, 300, 11, 281, 312, 3245, 11, 321, 500, 380, 534, 1223, 294, 2607, 50726], "temperature": 0.0, "avg_logprob": -0.13990016559978108, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.005154182203114033}, {"id": 149, "seek": 88032, "start": 887.5600000000001, "end": 896.24, "text": " complex pattern recognition tasks in particular can be reduced down to these neural networks. So", "tokens": [50726, 3997, 5102, 11150, 9608, 294, 1729, 393, 312, 9212, 760, 281, 613, 18161, 9590, 13, 407, 51160], "temperature": 0.0, "avg_logprob": -0.13990016559978108, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.005154182203114033}, {"id": 150, "seek": 88032, "start": 896.24, "end": 900.48, "text": " how does that help us in artificial intelligence? That's what's going on in a brain in a very", "tokens": [51160, 577, 775, 300, 854, 505, 294, 11677, 7599, 30, 663, 311, 437, 311, 516, 322, 294, 257, 3567, 294, 257, 588, 51372], "temperature": 0.0, "avg_logprob": -0.13990016559978108, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.005154182203114033}, {"id": 151, "seek": 88032, "start": 900.48, "end": 906.12, "text": " hand-wavy way. Okay, so that's obviously not a technical explanation of what's going on. How", "tokens": [51372, 1011, 12, 86, 15498, 636, 13, 1033, 11, 370, 300, 311, 2745, 406, 257, 6191, 10835, 295, 437, 311, 516, 322, 13, 1012, 51654], "temperature": 0.0, "avg_logprob": -0.13990016559978108, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.005154182203114033}, {"id": 152, "seek": 90612, "start": 906.16, "end": 912.04, "text": " does that help us in neural networks? Well, we can implement that stuff in software. The idea goes", "tokens": [50366, 775, 300, 854, 505, 294, 18161, 9590, 30, 1042, 11, 321, 393, 4445, 300, 1507, 294, 4722, 13, 440, 1558, 1709, 50660], "temperature": 0.0, "avg_logprob": -0.11391124930433048, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0008749324479140341}, {"id": 153, "seek": 90612, "start": 912.04, "end": 918.64, "text": " back to the 1940s and two researchers, McCulloch and Pitts, and they are struck by the idea that", "tokens": [50660, 646, 281, 264, 24158, 82, 293, 732, 10309, 11, 12061, 858, 8997, 293, 29478, 11, 293, 436, 366, 13159, 538, 264, 1558, 300, 50990], "temperature": 0.0, "avg_logprob": -0.11391124930433048, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0008749324479140341}, {"id": 154, "seek": 90612, "start": 918.64, "end": 924.6, "text": " the structures that you see in the brain look a bit like electrical circuits, and they thought,", "tokens": [50990, 264, 9227, 300, 291, 536, 294, 264, 3567, 574, 257, 857, 411, 12147, 26354, 11, 293, 436, 1194, 11, 51288], "temperature": 0.0, "avg_logprob": -0.11391124930433048, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0008749324479140341}, {"id": 155, "seek": 90612, "start": 924.6, "end": 930.6, "text": " could we implement all that stuff in electrical circuits? Now, they didn't have the wherewithal", "tokens": [51288, 727, 321, 4445, 439, 300, 1507, 294, 12147, 26354, 30, 823, 11, 436, 994, 380, 362, 264, 689, 11820, 304, 51588], "temperature": 0.0, "avg_logprob": -0.11391124930433048, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.0008749324479140341}, {"id": 156, "seek": 93060, "start": 930.84, "end": 937.48, "text": " to be able to do that, but the idea stuck. The idea has been around since the 1940s. It began to be", "tokens": [50376, 281, 312, 1075, 281, 360, 300, 11, 457, 264, 1558, 5541, 13, 440, 1558, 575, 668, 926, 1670, 264, 24158, 82, 13, 467, 4283, 281, 312, 50708], "temperature": 0.0, "avg_logprob": -0.11663733696450992, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.001659428351558745}, {"id": 157, "seek": 93060, "start": 937.48, "end": 943.4, "text": " seriously looked at, the idea of doing this in software in the 1960s, and then there was another", "tokens": [50708, 6638, 2956, 412, 11, 264, 1558, 295, 884, 341, 294, 4722, 294, 264, 16157, 82, 11, 293, 550, 456, 390, 1071, 51004], "temperature": 0.0, "avg_logprob": -0.11663733696450992, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.001659428351558745}, {"id": 158, "seek": 93060, "start": 943.4, "end": 950.36, "text": " flutter of interest in the 1980s, but it was only this century that it really became possible. And", "tokens": [51004, 932, 9947, 295, 1179, 294, 264, 13626, 82, 11, 457, 309, 390, 787, 341, 4901, 300, 309, 534, 3062, 1944, 13, 400, 51352], "temperature": 0.0, "avg_logprob": -0.11663733696450992, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.001659428351558745}, {"id": 159, "seek": 93060, "start": 950.36, "end": 955.72, "text": " why did it become possible? For three reasons. There were some scientific advances, what's called", "tokens": [51352, 983, 630, 309, 1813, 1944, 30, 1171, 1045, 4112, 13, 821, 645, 512, 8134, 25297, 11, 437, 311, 1219, 51620], "temperature": 0.0, "avg_logprob": -0.11663733696450992, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.001659428351558745}, {"id": 160, "seek": 95572, "start": 955.72, "end": 962.2, "text": " deep learning. There was the availability of big data, and you need data to be able to configure", "tokens": [50364, 2452, 2539, 13, 821, 390, 264, 17945, 295, 955, 1412, 11, 293, 291, 643, 1412, 281, 312, 1075, 281, 22162, 50688], "temperature": 0.0, "avg_logprob": -0.08169983279320502, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.016109328716993332}, {"id": 161, "seek": 95572, "start": 962.2, "end": 967.64, "text": " these neural networks. And finally, to configure these neural networks so that they can recognize", "tokens": [50688, 613, 18161, 9590, 13, 400, 2721, 11, 281, 22162, 613, 18161, 9590, 370, 300, 436, 393, 5521, 50960], "temperature": 0.0, "avg_logprob": -0.08169983279320502, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.016109328716993332}, {"id": 162, "seek": 95572, "start": 967.64, "end": 974.2, "text": " Turing's picture, you need lots of computer power, and computer power became very cheap this century.", "tokens": [50960, 314, 1345, 311, 3036, 11, 291, 643, 3195, 295, 3820, 1347, 11, 293, 3820, 1347, 3062, 588, 7084, 341, 4901, 13, 51288], "temperature": 0.0, "avg_logprob": -0.08169983279320502, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.016109328716993332}, {"id": 163, "seek": 95572, "start": 974.2, "end": 978.84, "text": " So we're in the age of big data, we're in the age of very cheap computer power, and those were the", "tokens": [51288, 407, 321, 434, 294, 264, 3205, 295, 955, 1412, 11, 321, 434, 294, 264, 3205, 295, 588, 7084, 3820, 1347, 11, 293, 729, 645, 264, 51520], "temperature": 0.0, "avg_logprob": -0.08169983279320502, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.016109328716993332}, {"id": 164, "seek": 97884, "start": 978.84, "end": 985.72, "text": " ingredients just as much as the scientific developments that made AI plausible this century,", "tokens": [50364, 6952, 445, 382, 709, 382, 264, 8134, 20862, 300, 1027, 7318, 39925, 341, 4901, 11, 50708], "temperature": 0.0, "avg_logprob": -0.08022303974956548, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.002659542951732874}, {"id": 165, "seek": 97884, "start": 985.72, "end": 993.24, "text": " in particular taking off around about 2005. Okay, so how do you actually train a neural network?", "tokens": [50708, 294, 1729, 1940, 766, 926, 466, 14394, 13, 1033, 11, 370, 577, 360, 291, 767, 3847, 257, 18161, 3209, 30, 51084], "temperature": 0.0, "avg_logprob": -0.08022303974956548, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.002659542951732874}, {"id": 166, "seek": 97884, "start": 993.24, "end": 998.0400000000001, "text": " If you show it a picture of Alan Turing and the output text Alan Turing, what does the training", "tokens": [51084, 759, 291, 855, 309, 257, 3036, 295, 16442, 314, 1345, 293, 264, 5598, 2487, 16442, 314, 1345, 11, 437, 775, 264, 3097, 51324], "temperature": 0.0, "avg_logprob": -0.08022303974956548, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.002659542951732874}, {"id": 167, "seek": 97884, "start": 998.0400000000001, "end": 1003.0, "text": " actually look like? Well, what you have to do is you have to adjust the network. That's what", "tokens": [51324, 767, 574, 411, 30, 1042, 11, 437, 291, 362, 281, 360, 307, 291, 362, 281, 4369, 264, 3209, 13, 663, 311, 437, 51572], "temperature": 0.0, "avg_logprob": -0.08022303974956548, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.002659542951732874}, {"id": 168, "seek": 97884, "start": 1003.0, "end": 1008.2800000000001, "text": " training a neural network is. You adjust the network so that when you show it another piece", "tokens": [51572, 3097, 257, 18161, 3209, 307, 13, 509, 4369, 264, 3209, 370, 300, 562, 291, 855, 309, 1071, 2522, 51836], "temperature": 0.0, "avg_logprob": -0.08022303974956548, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.002659542951732874}, {"id": 169, "seek": 100828, "start": 1008.28, "end": 1014.1999999999999, "text": " of training data, a desired input and a desired output, an input and a desired output, it will", "tokens": [50364, 295, 3097, 1412, 11, 257, 14721, 4846, 293, 257, 14721, 5598, 11, 364, 4846, 293, 257, 14721, 5598, 11, 309, 486, 50660], "temperature": 0.0, "avg_logprob": -0.050955099808542355, "compression_ratio": 1.7889908256880733, "no_speech_prob": 0.0010695310775190592}, {"id": 170, "seek": 100828, "start": 1014.1999999999999, "end": 1020.28, "text": " produce that desired output. Now, the mathematics for that is not very hard. It's kind of beginning", "tokens": [50660, 5258, 300, 14721, 5598, 13, 823, 11, 264, 18666, 337, 300, 307, 406, 588, 1152, 13, 467, 311, 733, 295, 2863, 50964], "temperature": 0.0, "avg_logprob": -0.050955099808542355, "compression_ratio": 1.7889908256880733, "no_speech_prob": 0.0010695310775190592}, {"id": 171, "seek": 100828, "start": 1020.28, "end": 1027.56, "text": " graduate level or advanced high school level, but you need an awful lot of it. And it's routine to", "tokens": [50964, 8080, 1496, 420, 7339, 1090, 1395, 1496, 11, 457, 291, 643, 364, 11232, 688, 295, 309, 13, 400, 309, 311, 9927, 281, 51328], "temperature": 0.0, "avg_logprob": -0.050955099808542355, "compression_ratio": 1.7889908256880733, "no_speech_prob": 0.0010695310775190592}, {"id": 172, "seek": 100828, "start": 1027.56, "end": 1032.52, "text": " get computers to do it, but you need a lot of computer power to be able to train neural networks", "tokens": [51328, 483, 10807, 281, 360, 309, 11, 457, 291, 643, 257, 688, 295, 3820, 1347, 281, 312, 1075, 281, 3847, 18161, 9590, 51576], "temperature": 0.0, "avg_logprob": -0.050955099808542355, "compression_ratio": 1.7889908256880733, "no_speech_prob": 0.0010695310775190592}, {"id": 173, "seek": 103252, "start": 1032.52, "end": 1038.68, "text": " big enough to be able to recognize faces. Okay, but basically, all you have to remember is that", "tokens": [50364, 955, 1547, 281, 312, 1075, 281, 5521, 8475, 13, 1033, 11, 457, 1936, 11, 439, 291, 362, 281, 1604, 307, 300, 50672], "temperature": 0.0, "avg_logprob": -0.06955887257367716, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.003091620048508048}, {"id": 174, "seek": 103252, "start": 1038.68, "end": 1045.56, "text": " each of those neurons is doing a tiny, simple pattern recognition task. And we can replicate", "tokens": [50672, 1184, 295, 729, 22027, 307, 884, 257, 5870, 11, 2199, 5102, 11150, 5633, 13, 400, 321, 393, 25356, 51016], "temperature": 0.0, "avg_logprob": -0.06955887257367716, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.003091620048508048}, {"id": 175, "seek": 103252, "start": 1045.56, "end": 1051.4, "text": " that in software and we can train these neural networks with data in order to be able to do", "tokens": [51016, 300, 294, 4722, 293, 321, 393, 3847, 613, 18161, 9590, 365, 1412, 294, 1668, 281, 312, 1075, 281, 360, 51308], "temperature": 0.0, "avg_logprob": -0.06955887257367716, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.003091620048508048}, {"id": 176, "seek": 103252, "start": 1051.4, "end": 1060.6, "text": " things like recognizing faces. So, as I say, it starts to become clear around about 2005", "tokens": [51308, 721, 411, 18538, 8475, 13, 407, 11, 382, 286, 584, 11, 309, 3719, 281, 1813, 1850, 926, 466, 14394, 51768], "temperature": 0.0, "avg_logprob": -0.06955887257367716, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.003091620048508048}, {"id": 177, "seek": 106060, "start": 1060.6, "end": 1067.32, "text": " that this technology is taking off. It starts to be applicable on problems like recognizing faces", "tokens": [50364, 300, 341, 2899, 307, 1940, 766, 13, 467, 3719, 281, 312, 21142, 322, 2740, 411, 18538, 8475, 50700], "temperature": 0.0, "avg_logprob": -0.0613162290482294, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.0018898044945672154}, {"id": 178, "seek": 106060, "start": 1067.32, "end": 1073.6399999999999, "text": " or recognizing tumors on x-rays and so on. And there's a huge flurry of interest", "tokens": [50700, 420, 18538, 38466, 322, 2031, 12, 36212, 293, 370, 322, 13, 400, 456, 311, 257, 2603, 932, 30614, 295, 1179, 51016], "temperature": 0.0, "avg_logprob": -0.0613162290482294, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.0018898044945672154}, {"id": 179, "seek": 106060, "start": 1074.36, "end": 1081.8, "text": " from Silicon Valley. It gets supercharged in 2012. And why does it get supercharged in 2012?", "tokens": [51052, 490, 25351, 10666, 13, 467, 2170, 1687, 25064, 294, 9125, 13, 400, 983, 775, 309, 483, 1687, 25064, 294, 9125, 30, 51424], "temperature": 0.0, "avg_logprob": -0.0613162290482294, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.0018898044945672154}, {"id": 180, "seek": 106060, "start": 1081.8, "end": 1087.8, "text": " Because it's realized that a particular type of computer processor is really well suited to", "tokens": [51424, 1436, 309, 311, 5334, 300, 257, 1729, 2010, 295, 3820, 15321, 307, 534, 731, 24736, 281, 51724], "temperature": 0.0, "avg_logprob": -0.0613162290482294, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.0018898044945672154}, {"id": 181, "seek": 108780, "start": 1087.8, "end": 1094.2, "text": " doing all the mathematics. The type of computer processor is a graphics processing unit, a GPU.", "tokens": [50364, 884, 439, 264, 18666, 13, 440, 2010, 295, 3820, 15321, 307, 257, 11837, 9007, 4985, 11, 257, 18407, 13, 50684], "temperature": 0.0, "avg_logprob": -0.0867911885293682, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.005585803650319576}, {"id": 182, "seek": 108780, "start": 1094.2, "end": 1099.32, "text": " Exactly the same technology that you or possibly more likely your children use", "tokens": [50684, 7587, 264, 912, 2899, 300, 291, 420, 6264, 544, 3700, 428, 2227, 764, 50940], "temperature": 0.0, "avg_logprob": -0.0867911885293682, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.005585803650319576}, {"id": 183, "seek": 108780, "start": 1099.32, "end": 1105.3999999999999, "text": " when they play Call of Duty or Minecraft or whatever it is. They all have GPUs in their computer.", "tokens": [50940, 562, 436, 862, 7807, 295, 33045, 420, 21029, 420, 2035, 309, 307, 13, 814, 439, 362, 18407, 82, 294, 641, 3820, 13, 51244], "temperature": 0.0, "avg_logprob": -0.0867911885293682, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.005585803650319576}, {"id": 184, "seek": 108780, "start": 1105.3999999999999, "end": 1111.96, "text": " It's exactly that technology. And by the way, it's AI that made NVIDIA a trillion-dollar company,", "tokens": [51244, 467, 311, 2293, 300, 2899, 13, 400, 538, 264, 636, 11, 309, 311, 7318, 300, 1027, 426, 3958, 6914, 257, 18723, 12, 40485, 2237, 11, 51572], "temperature": 0.0, "avg_logprob": -0.0867911885293682, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.005585803650319576}, {"id": 185, "seek": 111196, "start": 1111.96, "end": 1118.8400000000001, "text": " not your teenage kids. Yeah, well, in times of a gold rush, be the ones to sell the shovels", "tokens": [50364, 406, 428, 26866, 2301, 13, 865, 11, 731, 11, 294, 1413, 295, 257, 3821, 9300, 11, 312, 264, 2306, 281, 3607, 264, 29789, 82, 50708], "temperature": 0.0, "avg_logprob": -0.08027917544047038, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0011223305482417345}, {"id": 186, "seek": 111196, "start": 1118.8400000000001, "end": 1127.64, "text": " is the lesson that you learn there. So, where does that take us? So, Silicon Valley gets excited.", "tokens": [50708, 307, 264, 6898, 300, 291, 1466, 456, 13, 407, 11, 689, 775, 300, 747, 505, 30, 407, 11, 25351, 10666, 2170, 2919, 13, 51148], "temperature": 0.0, "avg_logprob": -0.08027917544047038, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0011223305482417345}, {"id": 187, "seek": 111196, "start": 1127.64, "end": 1133.4, "text": " Silicon Valley gets excited and starts to make speculative bets in artificial intelligence.", "tokens": [51148, 25351, 10666, 2170, 2919, 293, 3719, 281, 652, 49415, 39922, 294, 11677, 7599, 13, 51436], "temperature": 0.0, "avg_logprob": -0.08027917544047038, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0011223305482417345}, {"id": 188, "seek": 111196, "start": 1133.4, "end": 1138.6000000000001, "text": " A huge range of speculative bets. And by speculative bets, I'm talking billions upon billions of", "tokens": [51436, 316, 2603, 3613, 295, 49415, 39922, 13, 400, 538, 49415, 39922, 11, 286, 478, 1417, 17375, 3564, 17375, 295, 51696], "temperature": 0.0, "avg_logprob": -0.08027917544047038, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0011223305482417345}, {"id": 189, "seek": 113860, "start": 1138.6, "end": 1145.24, "text": " dollars, right? The kind of bets that we can't imagine in our everyday life. And one thing", "tokens": [50364, 3808, 11, 558, 30, 440, 733, 295, 39922, 300, 321, 393, 380, 3811, 294, 527, 7429, 993, 13, 400, 472, 551, 50696], "temperature": 0.0, "avg_logprob": -0.07384643449888124, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.0029106277506798506}, {"id": 190, "seek": 113860, "start": 1145.24, "end": 1153.3999999999999, "text": " starts to become clear. And what starts to become clear is that the capabilities of neural networks", "tokens": [50696, 3719, 281, 1813, 1850, 13, 400, 437, 3719, 281, 1813, 1850, 307, 300, 264, 10862, 295, 18161, 9590, 51104], "temperature": 0.0, "avg_logprob": -0.07384643449888124, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.0029106277506798506}, {"id": 191, "seek": 113860, "start": 1153.3999999999999, "end": 1160.6799999999998, "text": " grows with scale. And to put it bluntly, with neural networks, bigger is better. But you don't", "tokens": [51104, 13156, 365, 4373, 13, 400, 281, 829, 309, 32246, 356, 11, 365, 18161, 9590, 11, 3801, 307, 1101, 13, 583, 291, 500, 380, 51468], "temperature": 0.0, "avg_logprob": -0.07384643449888124, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.0029106277506798506}, {"id": 192, "seek": 113860, "start": 1160.6799999999998, "end": 1166.1999999999998, "text": " just need bigger neural networks, you need more data and more computer power in order to be able", "tokens": [51468, 445, 643, 3801, 18161, 9590, 11, 291, 643, 544, 1412, 293, 544, 3820, 1347, 294, 1668, 281, 312, 1075, 51744], "temperature": 0.0, "avg_logprob": -0.07384643449888124, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.0029106277506798506}, {"id": 193, "seek": 116620, "start": 1166.2, "end": 1172.8400000000001, "text": " to train them. So, there's a rush to get a competitive advantage in the market. And we know", "tokens": [50364, 281, 3847, 552, 13, 407, 11, 456, 311, 257, 9300, 281, 483, 257, 10043, 5002, 294, 264, 2142, 13, 400, 321, 458, 50696], "temperature": 0.0, "avg_logprob": -0.07591738700866699, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004931751755066216}, {"id": 194, "seek": 116620, "start": 1172.8400000000001, "end": 1179.32, "text": " that more data, more computer power, bigger neural networks delivers greater capability.", "tokens": [50696, 300, 544, 1412, 11, 544, 3820, 1347, 11, 3801, 18161, 9590, 24860, 5044, 13759, 13, 51020], "temperature": 0.0, "avg_logprob": -0.07591738700866699, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004931751755066216}, {"id": 195, "seek": 116620, "start": 1179.32, "end": 1185.0, "text": " And so, how does Silicon Valley respond by throwing more data and more computer power at the problem?", "tokens": [51020, 400, 370, 11, 577, 775, 25351, 10666, 4196, 538, 10238, 544, 1412, 293, 544, 3820, 1347, 412, 264, 1154, 30, 51304], "temperature": 0.0, "avg_logprob": -0.07591738700866699, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004931751755066216}, {"id": 196, "seek": 116620, "start": 1185.0, "end": 1193.72, "text": " They turn the dial on this up to 11. Okay? Just throw 10 times more data, 10 times more computer", "tokens": [51304, 814, 1261, 264, 5502, 322, 341, 493, 281, 2975, 13, 1033, 30, 1449, 3507, 1266, 1413, 544, 1412, 11, 1266, 1413, 544, 3820, 51740], "temperature": 0.0, "avg_logprob": -0.07591738700866699, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004931751755066216}, {"id": 197, "seek": 119372, "start": 1193.72, "end": 1198.3600000000001, "text": " power at the problem. It sounds incredibly crude. And from a scientific perspective,", "tokens": [50364, 1347, 412, 264, 1154, 13, 467, 3263, 6252, 30796, 13, 400, 490, 257, 8134, 4585, 11, 50596], "temperature": 0.0, "avg_logprob": -0.061746286791424417, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.005334155168384314}, {"id": 198, "seek": 119372, "start": 1198.3600000000001, "end": 1204.68, "text": " it really is crude. I'd rather the advances had come through core science, but actually,", "tokens": [50596, 309, 534, 307, 30796, 13, 286, 1116, 2831, 264, 25297, 632, 808, 807, 4965, 3497, 11, 457, 767, 11, 50912], "temperature": 0.0, "avg_logprob": -0.061746286791424417, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.005334155168384314}, {"id": 199, "seek": 119372, "start": 1204.68, "end": 1209.32, "text": " there's an advantage to be gained just by throwing more data and computer power at it.", "tokens": [50912, 456, 311, 364, 5002, 281, 312, 12634, 445, 538, 10238, 544, 1412, 293, 3820, 1347, 412, 309, 13, 51144], "temperature": 0.0, "avg_logprob": -0.061746286791424417, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.005334155168384314}, {"id": 200, "seek": 119372, "start": 1209.32, "end": 1215.88, "text": " So, let's see how far this can take us. And where it took us is a really unexpected direction.", "tokens": [51144, 407, 11, 718, 311, 536, 577, 1400, 341, 393, 747, 505, 13, 400, 689, 309, 1890, 505, 307, 257, 534, 13106, 3513, 13, 51472], "temperature": 0.0, "avg_logprob": -0.061746286791424417, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.005334155168384314}, {"id": 201, "seek": 121588, "start": 1216.5200000000002, "end": 1224.0400000000002, "text": " Around about 2017, 2018, we're seeing a flurry of AI applications, exactly the kind of things I've", "tokens": [50396, 17633, 466, 6591, 11, 6096, 11, 321, 434, 2577, 257, 932, 30614, 295, 7318, 5821, 11, 2293, 264, 733, 295, 721, 286, 600, 50772], "temperature": 0.0, "avg_logprob": -0.08905749542768611, "compression_ratio": 1.48046875, "no_speech_prob": 0.013853657059371471}, {"id": 202, "seek": 121588, "start": 1224.0400000000002, "end": 1229.8000000000002, "text": " described, things like recognizing tumors and so on. And those developments alone would have been", "tokens": [50772, 7619, 11, 721, 411, 18538, 38466, 293, 370, 322, 13, 400, 729, 20862, 3312, 576, 362, 668, 51060], "temperature": 0.0, "avg_logprob": -0.08905749542768611, "compression_ratio": 1.48046875, "no_speech_prob": 0.013853657059371471}, {"id": 203, "seek": 121588, "start": 1229.8000000000002, "end": 1237.24, "text": " driving AI ahead. But what happens is one particular machine learning technology suddenly", "tokens": [51060, 4840, 7318, 2286, 13, 583, 437, 2314, 307, 472, 1729, 3479, 2539, 2899, 5800, 51432], "temperature": 0.0, "avg_logprob": -0.08905749542768611, "compression_ratio": 1.48046875, "no_speech_prob": 0.013853657059371471}, {"id": 204, "seek": 121588, "start": 1237.24, "end": 1244.2800000000002, "text": " seems to be very, very well suited for this age of big AI. The paper that launched all this,", "tokens": [51432, 2544, 281, 312, 588, 11, 588, 731, 24736, 337, 341, 3205, 295, 955, 7318, 13, 440, 3035, 300, 8730, 439, 341, 11, 51784], "temperature": 0.0, "avg_logprob": -0.08905749542768611, "compression_ratio": 1.48046875, "no_speech_prob": 0.013853657059371471}, {"id": 205, "seek": 124428, "start": 1244.28, "end": 1250.04, "text": " probably the most important AI paper in the last decade, is called Attention Is All You Need.", "tokens": [50364, 1391, 264, 881, 1021, 7318, 3035, 294, 264, 1036, 10378, 11, 307, 1219, 31858, 1119, 1057, 509, 16984, 13, 50652], "temperature": 0.0, "avg_logprob": -0.06963794487566988, "compression_ratio": 1.627177700348432, "no_speech_prob": 0.004537275992333889}, {"id": 206, "seek": 124428, "start": 1250.04, "end": 1254.6, "text": " It's an extremely unhelpful title, and I bet they're regretting that title. It probably seemed", "tokens": [50652, 467, 311, 364, 4664, 517, 37451, 906, 4876, 11, 293, 286, 778, 436, 434, 10879, 783, 300, 4876, 13, 467, 1391, 6576, 50880], "temperature": 0.0, "avg_logprob": -0.06963794487566988, "compression_ratio": 1.627177700348432, "no_speech_prob": 0.004537275992333889}, {"id": 207, "seek": 124428, "start": 1254.6, "end": 1260.68, "text": " like a good joke at the time. All You Need is a kind of AI meme. Doesn't sound very funny to you.", "tokens": [50880, 411, 257, 665, 7647, 412, 264, 565, 13, 1057, 509, 16984, 307, 257, 733, 295, 7318, 21701, 13, 12955, 380, 1626, 588, 4074, 281, 291, 13, 51184], "temperature": 0.0, "avg_logprob": -0.06963794487566988, "compression_ratio": 1.627177700348432, "no_speech_prob": 0.004537275992333889}, {"id": 208, "seek": 124428, "start": 1260.68, "end": 1266.36, "text": " That's because it isn't very funny. It's an insider AI joke. But anyway, this paper,", "tokens": [51184, 663, 311, 570, 309, 1943, 380, 588, 4074, 13, 467, 311, 364, 40990, 7318, 7647, 13, 583, 4033, 11, 341, 3035, 11, 51468], "temperature": 0.0, "avg_logprob": -0.06963794487566988, "compression_ratio": 1.627177700348432, "no_speech_prob": 0.004537275992333889}, {"id": 209, "seek": 124428, "start": 1266.36, "end": 1270.76, "text": " by these seven people who at the time worked for Google Brain, one of the Google Research Labs,", "tokens": [51468, 538, 613, 3407, 561, 567, 412, 264, 565, 2732, 337, 3329, 29783, 11, 472, 295, 264, 3329, 10303, 40047, 11, 51688], "temperature": 0.0, "avg_logprob": -0.06963794487566988, "compression_ratio": 1.627177700348432, "no_speech_prob": 0.004537275992333889}, {"id": 210, "seek": 127076, "start": 1270.76, "end": 1276.76, "text": " is the paper that introduces a particular neural network architecture called the transformer", "tokens": [50364, 307, 264, 3035, 300, 31472, 257, 1729, 18161, 3209, 9482, 1219, 264, 31782, 50664], "temperature": 0.0, "avg_logprob": -0.08449696992572985, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.0023378527257591486}, {"id": 211, "seek": 127076, "start": 1276.76, "end": 1282.04, "text": " architecture. And what it's designed for is something called large language models.", "tokens": [50664, 9482, 13, 400, 437, 309, 311, 4761, 337, 307, 746, 1219, 2416, 2856, 5245, 13, 50928], "temperature": 0.0, "avg_logprob": -0.08449696992572985, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.0023378527257591486}, {"id": 212, "seek": 127076, "start": 1282.6, "end": 1286.28, "text": " So this is, I'm not going to try and explain how the transformer architecture works.", "tokens": [50956, 407, 341, 307, 11, 286, 478, 406, 516, 281, 853, 293, 2903, 577, 264, 31782, 9482, 1985, 13, 51140], "temperature": 0.0, "avg_logprob": -0.08449696992572985, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.0023378527257591486}, {"id": 213, "seek": 127076, "start": 1286.28, "end": 1292.28, "text": " It has one particular innovation, I think, and that particular innovation is what's called an", "tokens": [51140, 467, 575, 472, 1729, 8504, 11, 286, 519, 11, 293, 300, 1729, 8504, 307, 437, 311, 1219, 364, 51440], "temperature": 0.0, "avg_logprob": -0.08449696992572985, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.0023378527257591486}, {"id": 214, "seek": 127076, "start": 1292.28, "end": 1299.16, "text": " attention mechanism. So we're going to describe how large language models work in a moment. But", "tokens": [51440, 3202, 7513, 13, 407, 321, 434, 516, 281, 6786, 577, 2416, 2856, 5245, 589, 294, 257, 1623, 13, 583, 51784], "temperature": 0.0, "avg_logprob": -0.08449696992572985, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.0023378527257591486}, {"id": 215, "seek": 129916, "start": 1299.96, "end": 1304.1200000000001, "text": " the point of the picture is simply that this is not just a big neural network.", "tokens": [50404, 264, 935, 295, 264, 3036, 307, 2935, 300, 341, 307, 406, 445, 257, 955, 18161, 3209, 13, 50612], "temperature": 0.0, "avg_logprob": -0.11435035217640012, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00027130593662150204}, {"id": 216, "seek": 129916, "start": 1304.1200000000001, "end": 1309.24, "text": " It has some structure. And it was this structure that was invented in that paper, and this diagram", "tokens": [50612, 467, 575, 512, 3877, 13, 400, 309, 390, 341, 3877, 300, 390, 14479, 294, 300, 3035, 11, 293, 341, 10686, 50868], "temperature": 0.0, "avg_logprob": -0.11435035217640012, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00027130593662150204}, {"id": 217, "seek": 129916, "start": 1309.24, "end": 1315.16, "text": " is taken straight out of that paper. It was these structures, the transformer architectures,", "tokens": [50868, 307, 2726, 2997, 484, 295, 300, 3035, 13, 467, 390, 613, 9227, 11, 264, 31782, 6331, 1303, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11435035217640012, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00027130593662150204}, {"id": 218, "seek": 129916, "start": 1315.16, "end": 1325.64, "text": " that made this technology possible. So we're all busy semi-lockdown and afraid to leave our homes", "tokens": [51164, 300, 1027, 341, 2899, 1944, 13, 407, 321, 434, 439, 5856, 12909, 12, 4102, 5093, 293, 4638, 281, 1856, 527, 7388, 51688], "temperature": 0.0, "avg_logprob": -0.11435035217640012, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00027130593662150204}, {"id": 219, "seek": 132564, "start": 1325.64, "end": 1332.6000000000001, "text": " in June 2020. And one company called OpenAI released a system or announce a system, I should", "tokens": [50364, 294, 6928, 4808, 13, 400, 472, 2237, 1219, 7238, 48698, 4736, 257, 1185, 420, 7478, 257, 1185, 11, 286, 820, 50712], "temperature": 0.0, "avg_logprob": -0.10639736603717415, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0018645264208316803}, {"id": 220, "seek": 132564, "start": 1332.6000000000001, "end": 1339.88, "text": " say, called GPT-3. Great technology. They're marketing company with GPT. I really think could", "tokens": [50712, 584, 11, 1219, 26039, 51, 12, 18, 13, 3769, 2899, 13, 814, 434, 6370, 2237, 365, 26039, 51, 13, 286, 534, 519, 727, 51076], "temperature": 0.0, "avg_logprob": -0.10639736603717415, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0018645264208316803}, {"id": 221, "seek": 132564, "start": 1339.88, "end": 1344.5200000000002, "text": " have done with a bit more thought, to be honest with you. Doesn't roll off the tongue. But anyway,", "tokens": [51076, 362, 1096, 365, 257, 857, 544, 1194, 11, 281, 312, 3245, 365, 291, 13, 12955, 380, 3373, 766, 264, 10601, 13, 583, 4033, 11, 51308], "temperature": 0.0, "avg_logprob": -0.10639736603717415, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0018645264208316803}, {"id": 222, "seek": 132564, "start": 1344.5200000000002, "end": 1353.5600000000002, "text": " GPT-3 is a particular type of machine learning system called a large language model. And we're", "tokens": [51308, 26039, 51, 12, 18, 307, 257, 1729, 2010, 295, 3479, 2539, 1185, 1219, 257, 2416, 2856, 2316, 13, 400, 321, 434, 51760], "temperature": 0.0, "avg_logprob": -0.10639736603717415, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0018645264208316803}, {"id": 223, "seek": 135356, "start": 1353.6399999999999, "end": 1357.56, "text": " going to talk in more detail about what large language models do in a moment. But the key", "tokens": [50368, 516, 281, 751, 294, 544, 2607, 466, 437, 2416, 2856, 5245, 360, 294, 257, 1623, 13, 583, 264, 2141, 50564], "temperature": 0.0, "avg_logprob": -0.059173361114833664, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0006532032857649028}, {"id": 224, "seek": 135356, "start": 1357.56, "end": 1364.6799999999998, "text": " point about GPT-3 is this. As we started to see what it could do, we realized that this was a", "tokens": [50564, 935, 466, 26039, 51, 12, 18, 307, 341, 13, 1018, 321, 1409, 281, 536, 437, 309, 727, 360, 11, 321, 5334, 300, 341, 390, 257, 50920], "temperature": 0.0, "avg_logprob": -0.059173361114833664, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0006532032857649028}, {"id": 225, "seek": 135356, "start": 1364.6799999999998, "end": 1371.0, "text": " step change in capability. It was dramatically better than the systems that had gone before it.", "tokens": [50920, 1823, 1319, 294, 13759, 13, 467, 390, 17548, 1101, 813, 264, 3652, 300, 632, 2780, 949, 309, 13, 51236], "temperature": 0.0, "avg_logprob": -0.059173361114833664, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0006532032857649028}, {"id": 226, "seek": 135356, "start": 1371.0, "end": 1376.2, "text": " Not just a little bit better, it was dramatically better than the systems that had gone before it.", "tokens": [51236, 1726, 445, 257, 707, 857, 1101, 11, 309, 390, 17548, 1101, 813, 264, 3652, 300, 632, 2780, 949, 309, 13, 51496], "temperature": 0.0, "avg_logprob": -0.059173361114833664, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0006532032857649028}, {"id": 227, "seek": 137620, "start": 1377.0800000000002, "end": 1383.48, "text": " And the scale of it was mind-boggling. So in neural network terms, we talk about", "tokens": [50408, 400, 264, 4373, 295, 309, 390, 1575, 12, 65, 36754, 1688, 13, 407, 294, 18161, 3209, 2115, 11, 321, 751, 466, 50728], "temperature": 0.0, "avg_logprob": -0.07990024829732961, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00533185014501214}, {"id": 228, "seek": 137620, "start": 1383.48, "end": 1387.64, "text": " parameters. When neural network people talk about a parameter, what are they talking about?", "tokens": [50728, 9834, 13, 1133, 18161, 3209, 561, 751, 466, 257, 13075, 11, 437, 366, 436, 1417, 466, 30, 50936], "temperature": 0.0, "avg_logprob": -0.07990024829732961, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00533185014501214}, {"id": 229, "seek": 137620, "start": 1387.64, "end": 1392.04, "text": " They're talking either about an individual neuron or one of the connections between them,", "tokens": [50936, 814, 434, 1417, 2139, 466, 364, 2609, 34090, 420, 472, 295, 264, 9271, 1296, 552, 11, 51156], "temperature": 0.0, "avg_logprob": -0.07990024829732961, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00533185014501214}, {"id": 230, "seek": 137620, "start": 1392.04, "end": 1400.1200000000001, "text": " roughly. And GPT-3 had 175 billion parameters. Now, this is not the same as the number of", "tokens": [51156, 9810, 13, 400, 26039, 51, 12, 18, 632, 41165, 5218, 9834, 13, 823, 11, 341, 307, 406, 264, 912, 382, 264, 1230, 295, 51560], "temperature": 0.0, "avg_logprob": -0.07990024829732961, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.00533185014501214}, {"id": 231, "seek": 140012, "start": 1400.12, "end": 1407.2399999999998, "text": " neurons in the brain. But nevertheless, it's not far off the order of magnitude. It's extremely", "tokens": [50364, 22027, 294, 264, 3567, 13, 583, 26924, 11, 309, 311, 406, 1400, 766, 264, 1668, 295, 15668, 13, 467, 311, 4664, 50720], "temperature": 0.0, "avg_logprob": -0.07729911262338812, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0016803012695163488}, {"id": 232, "seek": 140012, "start": 1407.2399999999998, "end": 1412.76, "text": " large. But remember, it's organized into one of these transformer architectures. My point is,", "tokens": [50720, 2416, 13, 583, 1604, 11, 309, 311, 9983, 666, 472, 295, 613, 31782, 6331, 1303, 13, 1222, 935, 307, 11, 50996], "temperature": 0.0, "avg_logprob": -0.07729911262338812, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0016803012695163488}, {"id": 233, "seek": 140012, "start": 1412.76, "end": 1419.8799999999999, "text": " it's not just a big neural network. And so the scale of the neural networks in this system", "tokens": [50996, 309, 311, 406, 445, 257, 955, 18161, 3209, 13, 400, 370, 264, 4373, 295, 264, 18161, 9590, 294, 341, 1185, 51352], "temperature": 0.0, "avg_logprob": -0.07729911262338812, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0016803012695163488}, {"id": 234, "seek": 140012, "start": 1419.8799999999999, "end": 1425.9599999999998, "text": " were enormous, completely unprecedented. And there's no point in having a big neural network", "tokens": [51352, 645, 11322, 11, 2584, 21555, 13, 400, 456, 311, 572, 935, 294, 1419, 257, 955, 18161, 3209, 51656], "temperature": 0.0, "avg_logprob": -0.07729911262338812, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0016803012695163488}, {"id": 235, "seek": 142596, "start": 1425.96, "end": 1430.6000000000001, "text": " unless you can train it with enough data. And actually, if you have large neural networks and", "tokens": [50364, 5969, 291, 393, 3847, 309, 365, 1547, 1412, 13, 400, 767, 11, 498, 291, 362, 2416, 18161, 9590, 293, 50596], "temperature": 0.0, "avg_logprob": -0.06627843054858121, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0014347049873322248}, {"id": 236, "seek": 142596, "start": 1430.6000000000001, "end": 1435.16, "text": " not enough data, you don't get capable systems at all. They're really quite useless.", "tokens": [50596, 406, 1547, 1412, 11, 291, 500, 380, 483, 8189, 3652, 412, 439, 13, 814, 434, 534, 1596, 14115, 13, 50824], "temperature": 0.0, "avg_logprob": -0.06627843054858121, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0014347049873322248}, {"id": 237, "seek": 142596, "start": 1436.44, "end": 1442.8400000000001, "text": " So what did the training data look like? The training data for GPT-3 is something like 500", "tokens": [50888, 407, 437, 630, 264, 3097, 1412, 574, 411, 30, 440, 3097, 1412, 337, 26039, 51, 12, 18, 307, 746, 411, 5923, 51208], "temperature": 0.0, "avg_logprob": -0.06627843054858121, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0014347049873322248}, {"id": 238, "seek": 142596, "start": 1442.8400000000001, "end": 1450.76, "text": " billion words. It's ordinary English text. Ordinary English text. That's how this system", "tokens": [51208, 5218, 2283, 13, 467, 311, 10547, 3669, 2487, 13, 29388, 4066, 3669, 2487, 13, 663, 311, 577, 341, 1185, 51604], "temperature": 0.0, "avg_logprob": -0.06627843054858121, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.0014347049873322248}, {"id": 239, "seek": 145076, "start": 1450.76, "end": 1456.52, "text": " was trained, just by giving it ordinary English text. Where do you get that training data from?", "tokens": [50364, 390, 8895, 11, 445, 538, 2902, 309, 10547, 3669, 2487, 13, 2305, 360, 291, 483, 300, 3097, 1412, 490, 30, 50652], "temperature": 0.0, "avg_logprob": -0.06878173149238198, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0033991988748311996}, {"id": 240, "seek": 145076, "start": 1457.16, "end": 1462.6, "text": " You download the whole of the World Wide Web to start with. Literally, this is the standard", "tokens": [50684, 509, 5484, 264, 1379, 295, 264, 3937, 42543, 9573, 281, 722, 365, 13, 23768, 11, 341, 307, 264, 3832, 50956], "temperature": 0.0, "avg_logprob": -0.06878173149238198, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0033991988748311996}, {"id": 241, "seek": 145076, "start": 1462.6, "end": 1466.92, "text": " practice in the field. You download the whole of the World Wide Web. You can try this at home,", "tokens": [50956, 3124, 294, 264, 2519, 13, 509, 5484, 264, 1379, 295, 264, 3937, 42543, 9573, 13, 509, 393, 853, 341, 412, 1280, 11, 51172], "temperature": 0.0, "avg_logprob": -0.06878173149238198, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0033991988748311996}, {"id": 242, "seek": 145076, "start": 1466.92, "end": 1474.52, "text": " by the way. Now, if you have a big enough disk drive, there's a program called Common Crawl.", "tokens": [51172, 538, 264, 636, 13, 823, 11, 498, 291, 362, 257, 955, 1547, 12355, 3332, 11, 456, 311, 257, 1461, 1219, 18235, 37877, 75, 13, 51552], "temperature": 0.0, "avg_logprob": -0.06878173149238198, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0033991988748311996}, {"id": 243, "seek": 145076, "start": 1474.52, "end": 1479.08, "text": " You can Google Common Crawl when you get home. They've even downloaded it all for you and put", "tokens": [51552, 509, 393, 3329, 18235, 37877, 75, 562, 291, 483, 1280, 13, 814, 600, 754, 21748, 309, 439, 337, 291, 293, 829, 51780], "temperature": 0.0, "avg_logprob": -0.06878173149238198, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0033991988748311996}, {"id": 244, "seek": 147908, "start": 1479.1599999999999, "end": 1484.12, "text": " it in a nice big file ready for your archive, but you do need a big disk in order to store all", "tokens": [50368, 309, 294, 257, 1481, 955, 3991, 1919, 337, 428, 23507, 11, 457, 291, 360, 643, 257, 955, 12355, 294, 1668, 281, 3531, 439, 50616], "temperature": 0.0, "avg_logprob": -0.07948056582746835, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.002206808654591441}, {"id": 245, "seek": 147908, "start": 1484.12, "end": 1490.6799999999998, "text": " that stuff. And what that means is they go to every web page, scrape all the text from it,", "tokens": [50616, 300, 1507, 13, 400, 437, 300, 1355, 307, 436, 352, 281, 633, 3670, 3028, 11, 32827, 439, 264, 2487, 490, 309, 11, 50944], "temperature": 0.0, "avg_logprob": -0.07948056582746835, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.002206808654591441}, {"id": 246, "seek": 147908, "start": 1490.6799999999998, "end": 1496.4399999999998, "text": " just the ordinary text, and then they follow all the links on that web page to every other web page.", "tokens": [50944, 445, 264, 10547, 2487, 11, 293, 550, 436, 1524, 439, 264, 6123, 322, 300, 3670, 3028, 281, 633, 661, 3670, 3028, 13, 51232], "temperature": 0.0, "avg_logprob": -0.07948056582746835, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.002206808654591441}, {"id": 247, "seek": 147908, "start": 1496.4399999999998, "end": 1502.28, "text": " And they do that exhaustively until they've absorbed the whole of the World Wide Web.", "tokens": [51232, 400, 436, 360, 300, 14687, 3413, 1826, 436, 600, 20799, 264, 1379, 295, 264, 3937, 42543, 9573, 13, 51524], "temperature": 0.0, "avg_logprob": -0.07948056582746835, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.002206808654591441}, {"id": 248, "seek": 147908, "start": 1502.28, "end": 1507.08, "text": " So what does that mean? Every PDF document goes into that, and you scrape the text from", "tokens": [51524, 407, 437, 775, 300, 914, 30, 2048, 17752, 4166, 1709, 666, 300, 11, 293, 291, 32827, 264, 2487, 490, 51764], "temperature": 0.0, "avg_logprob": -0.07948056582746835, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.002206808654591441}, {"id": 249, "seek": 150708, "start": 1507.08, "end": 1515.08, "text": " those PDF documents. Every advertising brochure, every bit, every government regulation, every", "tokens": [50364, 729, 17752, 8512, 13, 2048, 13097, 48147, 540, 11, 633, 857, 11, 633, 2463, 15062, 11, 633, 50764], "temperature": 0.0, "avg_logprob": -0.08603171680284583, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0014776871539652348}, {"id": 250, "seek": 150708, "start": 1515.08, "end": 1523.6399999999999, "text": " university minutes, God help us, all of it goes into that training data. And the statistics,", "tokens": [50764, 5454, 2077, 11, 1265, 854, 505, 11, 439, 295, 309, 1709, 666, 300, 3097, 1412, 13, 400, 264, 12523, 11, 51192], "temperature": 0.0, "avg_logprob": -0.08603171680284583, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0014776871539652348}, {"id": 251, "seek": 150708, "start": 1523.6399999999999, "end": 1529.96, "text": " 500 billion words, it's very hard to understand the scale of that training data. It would take a", "tokens": [51192, 5923, 5218, 2283, 11, 309, 311, 588, 1152, 281, 1223, 264, 4373, 295, 300, 3097, 1412, 13, 467, 576, 747, 257, 51508], "temperature": 0.0, "avg_logprob": -0.08603171680284583, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0014776871539652348}, {"id": 252, "seek": 150708, "start": 1529.96, "end": 1534.84, "text": " person reading 1,000 words an hour, more than 1,000 years in order to be able to read that,", "tokens": [51508, 954, 3760, 502, 11, 1360, 2283, 364, 1773, 11, 544, 813, 502, 11, 1360, 924, 294, 1668, 281, 312, 1075, 281, 1401, 300, 11, 51752], "temperature": 0.0, "avg_logprob": -0.08603171680284583, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0014776871539652348}, {"id": 253, "seek": 153484, "start": 1534.84, "end": 1539.6399999999999, "text": " but even that doesn't really help. That's vastly, vastly more text than a human being", "tokens": [50364, 457, 754, 300, 1177, 380, 534, 854, 13, 663, 311, 41426, 11, 41426, 544, 2487, 813, 257, 1952, 885, 50604], "temperature": 0.0, "avg_logprob": -0.07646364078187105, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0024250245187431574}, {"id": 254, "seek": 153484, "start": 1539.6399999999999, "end": 1544.12, "text": " could ever absorb in their lifetime. What this tells you, by the way, one thing that tells you", "tokens": [50604, 727, 1562, 15631, 294, 641, 11364, 13, 708, 341, 5112, 291, 11, 538, 264, 636, 11, 472, 551, 300, 5112, 291, 50828], "temperature": 0.0, "avg_logprob": -0.07646364078187105, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0024250245187431574}, {"id": 255, "seek": 153484, "start": 1544.12, "end": 1548.6799999999998, "text": " is that the machine learning is much less efficient at learning than human beings are,", "tokens": [50828, 307, 300, 264, 3479, 2539, 307, 709, 1570, 7148, 412, 2539, 813, 1952, 8958, 366, 11, 51056], "temperature": 0.0, "avg_logprob": -0.07646364078187105, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0024250245187431574}, {"id": 256, "seek": 153484, "start": 1548.6799999999998, "end": 1554.76, "text": " because for me to be able to learn, I did not have to absorb 500 billion words. Anyway, so what", "tokens": [51056, 570, 337, 385, 281, 312, 1075, 281, 1466, 11, 286, 630, 406, 362, 281, 15631, 5923, 5218, 2283, 13, 5684, 11, 370, 437, 51360], "temperature": 0.0, "avg_logprob": -0.07646364078187105, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0024250245187431574}, {"id": 257, "seek": 153484, "start": 1554.76, "end": 1560.84, "text": " does it do? So this company, OpenAI, are developing this technology. They've got a billion-dollar", "tokens": [51360, 775, 309, 360, 30, 407, 341, 2237, 11, 7238, 48698, 11, 366, 6416, 341, 2899, 13, 814, 600, 658, 257, 5218, 12, 40485, 51664], "temperature": 0.0, "avg_logprob": -0.07646364078187105, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0024250245187431574}, {"id": 258, "seek": 156084, "start": 1560.84, "end": 1566.12, "text": " investment from Microsoft, and what is it that they're trying to do? What is this large language", "tokens": [50364, 6078, 490, 8116, 11, 293, 437, 307, 309, 300, 436, 434, 1382, 281, 360, 30, 708, 307, 341, 2416, 2856, 50628], "temperature": 0.0, "avg_logprob": -0.06694145958022316, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.004597098100930452}, {"id": 259, "seek": 156084, "start": 1566.12, "end": 1575.1599999999999, "text": " model? All it's doing is a very powerful autocomplete. So if I open up my smartphone and I start sending", "tokens": [50628, 2316, 30, 1057, 309, 311, 884, 307, 257, 588, 4005, 45833, 298, 17220, 13, 407, 498, 286, 1269, 493, 452, 13307, 293, 286, 722, 7750, 51080], "temperature": 0.0, "avg_logprob": -0.06694145958022316, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.004597098100930452}, {"id": 260, "seek": 156084, "start": 1575.1599999999999, "end": 1581.3999999999999, "text": " a text message to my wife, and I type, I'm going to be, my smartphone will suggest completions for", "tokens": [51080, 257, 2487, 3636, 281, 452, 3836, 11, 293, 286, 2010, 11, 286, 478, 516, 281, 312, 11, 452, 13307, 486, 3402, 1557, 626, 337, 51392], "temperature": 0.0, "avg_logprob": -0.06694145958022316, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.004597098100930452}, {"id": 261, "seek": 156084, "start": 1581.3999999999999, "end": 1586.52, "text": " me, so that I can type the message quickly. And what might those completions be? They might be", "tokens": [51392, 385, 11, 370, 300, 286, 393, 2010, 264, 3636, 2661, 13, 400, 437, 1062, 729, 1557, 626, 312, 30, 814, 1062, 312, 51648], "temperature": 0.0, "avg_logprob": -0.06694145958022316, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.004597098100930452}, {"id": 262, "seek": 158652, "start": 1586.6, "end": 1594.2, "text": " late or in the pub, or late and in the pub. So how is my smartphone doing that? It's doing", "tokens": [50368, 3469, 420, 294, 264, 1535, 11, 420, 3469, 293, 294, 264, 1535, 13, 407, 577, 307, 452, 13307, 884, 300, 30, 467, 311, 884, 50748], "temperature": 0.0, "avg_logprob": -0.07054526262944287, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.000991175533272326}, {"id": 263, "seek": 158652, "start": 1594.2, "end": 1600.44, "text": " what GPT-3 does, but on a much smaller scale. It's looked at all of the text messages that I've sent", "tokens": [50748, 437, 26039, 51, 12, 18, 775, 11, 457, 322, 257, 709, 4356, 4373, 13, 467, 311, 2956, 412, 439, 295, 264, 2487, 7897, 300, 286, 600, 2279, 51060], "temperature": 0.0, "avg_logprob": -0.07054526262944287, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.000991175533272326}, {"id": 264, "seek": 158652, "start": 1600.44, "end": 1605.8, "text": " to my wife, and it's learned through a much simpler machine learning process that the", "tokens": [51060, 281, 452, 3836, 11, 293, 309, 311, 3264, 807, 257, 709, 18587, 3479, 2539, 1399, 300, 264, 51328], "temperature": 0.0, "avg_logprob": -0.07054526262944287, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.000991175533272326}, {"id": 265, "seek": 158652, "start": 1605.8, "end": 1612.36, "text": " likeliest next thing for me to type after I'm going to be is either late or in the pub or late", "tokens": [51328, 411, 16850, 958, 551, 337, 385, 281, 2010, 934, 286, 478, 516, 281, 312, 307, 2139, 3469, 420, 294, 264, 1535, 420, 3469, 51656], "temperature": 0.0, "avg_logprob": -0.07054526262944287, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.000991175533272326}, {"id": 266, "seek": 161236, "start": 1612.36, "end": 1618.6799999999998, "text": " and in the pub. So the training data there is just the text messages that I sent to my wife.", "tokens": [50364, 293, 294, 264, 1535, 13, 407, 264, 3097, 1412, 456, 307, 445, 264, 2487, 7897, 300, 286, 2279, 281, 452, 3836, 13, 50680], "temperature": 0.0, "avg_logprob": -0.07624243726634015, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0020642371382564306}, {"id": 267, "seek": 161236, "start": 1619.32, "end": 1626.84, "text": " Now, crucially, what GPT-3 and its successor, ChatGPT, all they are doing is exactly the same", "tokens": [50712, 823, 11, 5140, 1909, 11, 437, 26039, 51, 12, 18, 293, 1080, 31864, 11, 27503, 38, 47, 51, 11, 439, 436, 366, 884, 307, 2293, 264, 912, 51088], "temperature": 0.0, "avg_logprob": -0.07624243726634015, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0020642371382564306}, {"id": 268, "seek": 161236, "start": 1626.84, "end": 1634.1999999999998, "text": " thing. The difference is scale. The difference is scale. In order to be able to train the neural", "tokens": [51088, 551, 13, 440, 2649, 307, 4373, 13, 440, 2649, 307, 4373, 13, 682, 1668, 281, 312, 1075, 281, 3847, 264, 18161, 51456], "temperature": 0.0, "avg_logprob": -0.07624243726634015, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0020642371382564306}, {"id": 269, "seek": 161236, "start": 1634.1999999999998, "end": 1640.12, "text": " networks with all of that training data so that they can do that prediction, given this prompt,", "tokens": [51456, 9590, 365, 439, 295, 300, 3097, 1412, 370, 300, 436, 393, 360, 300, 17630, 11, 2212, 341, 12391, 11, 51752], "temperature": 0.0, "avg_logprob": -0.07624243726634015, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.0020642371382564306}, {"id": 270, "seek": 164012, "start": 1640.12, "end": 1646.76, "text": " what should come next? You require extremely expensive AI supercomputers running for months.", "tokens": [50364, 437, 820, 808, 958, 30, 509, 3651, 4664, 5124, 7318, 27839, 2582, 433, 2614, 337, 2493, 13, 50696], "temperature": 0.0, "avg_logprob": -0.08086487098976418, "compression_ratio": 1.9288537549407114, "no_speech_prob": 0.0026070482563227415}, {"id": 271, "seek": 164012, "start": 1647.32, "end": 1652.36, "text": " And by extremely expensive AI supercomputers, these are tens of millions of dollars for these", "tokens": [50724, 400, 538, 4664, 5124, 7318, 27839, 2582, 433, 11, 613, 366, 10688, 295, 6803, 295, 3808, 337, 613, 50976], "temperature": 0.0, "avg_logprob": -0.08086487098976418, "compression_ratio": 1.9288537549407114, "no_speech_prob": 0.0026070482563227415}, {"id": 272, "seek": 164012, "start": 1652.36, "end": 1658.4399999999998, "text": " supercomputers, and they're running for months. Just the basic electricity cost runs into millions", "tokens": [50976, 27839, 2582, 433, 11, 293, 436, 434, 2614, 337, 2493, 13, 1449, 264, 3875, 10356, 2063, 6676, 666, 6803, 51280], "temperature": 0.0, "avg_logprob": -0.08086487098976418, "compression_ratio": 1.9288537549407114, "no_speech_prob": 0.0026070482563227415}, {"id": 273, "seek": 164012, "start": 1658.4399999999998, "end": 1663.08, "text": " of dollars. That raises all sorts of issues about CO2 emissions and the light that we're not going", "tokens": [51280, 295, 3808, 13, 663, 19658, 439, 7527, 295, 2663, 466, 3002, 17, 14607, 293, 264, 1442, 300, 321, 434, 406, 516, 51512], "temperature": 0.0, "avg_logprob": -0.08086487098976418, "compression_ratio": 1.9288537549407114, "no_speech_prob": 0.0026070482563227415}, {"id": 274, "seek": 164012, "start": 1663.08, "end": 1669.8, "text": " to go into there. The point is these are extremely expensive things. One of the one of the implications", "tokens": [51512, 281, 352, 666, 456, 13, 440, 935, 307, 613, 366, 4664, 5124, 721, 13, 1485, 295, 264, 472, 295, 264, 16602, 51848], "temperature": 0.0, "avg_logprob": -0.08086487098976418, "compression_ratio": 1.9288537549407114, "no_speech_prob": 0.0026070482563227415}, {"id": 275, "seek": 166980, "start": 1669.8799999999999, "end": 1675.96, "text": " of that, by the way, no UK or US university has the capability to build one of these models from", "tokens": [50368, 295, 300, 11, 538, 264, 636, 11, 572, 7051, 420, 2546, 5454, 575, 264, 13759, 281, 1322, 472, 295, 613, 5245, 490, 50672], "temperature": 0.0, "avg_logprob": -0.06142281110470112, "compression_ratio": 1.5731225296442688, "no_speech_prob": 0.0010576313361525536}, {"id": 276, "seek": 166980, "start": 1675.96, "end": 1681.24, "text": " scratch. It's only big tech companies at the moment that are capable of building models on the scale", "tokens": [50672, 8459, 13, 467, 311, 787, 955, 7553, 3431, 412, 264, 1623, 300, 366, 8189, 295, 2390, 5245, 322, 264, 4373, 50936], "temperature": 0.0, "avg_logprob": -0.06142281110470112, "compression_ratio": 1.5731225296442688, "no_speech_prob": 0.0010576313361525536}, {"id": 277, "seek": 166980, "start": 1681.24, "end": 1691.48, "text": " of GPT-3 or ChatGPT. So GPT-3 is released, I say, in June 2020, and it suddenly becomes clear to us", "tokens": [50936, 295, 26039, 51, 12, 18, 420, 27503, 38, 47, 51, 13, 407, 26039, 51, 12, 18, 307, 4736, 11, 286, 584, 11, 294, 6928, 4808, 11, 293, 309, 5800, 3643, 1850, 281, 505, 51448], "temperature": 0.0, "avg_logprob": -0.06142281110470112, "compression_ratio": 1.5731225296442688, "no_speech_prob": 0.0010576313361525536}, {"id": 278, "seek": 166980, "start": 1691.48, "end": 1698.76, "text": " that what it does is a step change improvement in capability over the systems that have come before.", "tokens": [51448, 300, 437, 309, 775, 307, 257, 1823, 1319, 10444, 294, 13759, 670, 264, 3652, 300, 362, 808, 949, 13, 51812], "temperature": 0.0, "avg_logprob": -0.06142281110470112, "compression_ratio": 1.5731225296442688, "no_speech_prob": 0.0010576313361525536}, {"id": 279, "seek": 169876, "start": 1698.76, "end": 1705.72, "text": " And seeing a step change in one generation is extremely rare. But how did they get there?", "tokens": [50364, 400, 2577, 257, 1823, 1319, 294, 472, 5125, 307, 4664, 5892, 13, 583, 577, 630, 436, 483, 456, 30, 50712], "temperature": 0.0, "avg_logprob": -0.07803901759060947, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.00018124913913197815}, {"id": 280, "seek": 169876, "start": 1705.72, "end": 1710.04, "text": " Well, the transformer architecture was essential. They wouldn't have been able to do that. But", "tokens": [50712, 1042, 11, 264, 31782, 9482, 390, 7115, 13, 814, 2759, 380, 362, 668, 1075, 281, 360, 300, 13, 583, 50928], "temperature": 0.0, "avg_logprob": -0.07803901759060947, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.00018124913913197815}, {"id": 281, "seek": 169876, "start": 1710.04, "end": 1716.68, "text": " actually, just as important is scale. Enormous amounts of data, enormous amounts of computer", "tokens": [50928, 767, 11, 445, 382, 1021, 307, 4373, 13, 2193, 687, 563, 11663, 295, 1412, 11, 11322, 11663, 295, 3820, 51260], "temperature": 0.0, "avg_logprob": -0.07803901759060947, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.00018124913913197815}, {"id": 282, "seek": 169876, "start": 1716.68, "end": 1723.56, "text": " power that have gone into training those networks. And actually, spurred on by this, we've entered", "tokens": [51260, 1347, 300, 362, 2780, 666, 3097, 729, 9590, 13, 400, 767, 11, 35657, 986, 322, 538, 341, 11, 321, 600, 9065, 51604], "temperature": 0.0, "avg_logprob": -0.07803901759060947, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.00018124913913197815}, {"id": 283, "seek": 172356, "start": 1723.56, "end": 1730.9199999999998, "text": " a new age in AI. When I was a PhD student in the late 1980s, I shared a computer with a bunch of", "tokens": [50364, 257, 777, 3205, 294, 7318, 13, 1133, 286, 390, 257, 14476, 3107, 294, 264, 3469, 13626, 82, 11, 286, 5507, 257, 3820, 365, 257, 3840, 295, 50732], "temperature": 0.0, "avg_logprob": -0.07116133433121902, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.00812540017068386}, {"id": 284, "seek": 172356, "start": 1730.9199999999998, "end": 1736.6799999999998, "text": " other people in my office, and that was fine. We could do state of the art AI research on a desktop", "tokens": [50732, 661, 561, 294, 452, 3398, 11, 293, 300, 390, 2489, 13, 492, 727, 360, 1785, 295, 264, 1523, 7318, 2132, 322, 257, 14502, 51020], "temperature": 0.0, "avg_logprob": -0.07116133433121902, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.00812540017068386}, {"id": 285, "seek": 172356, "start": 1736.6799999999998, "end": 1742.12, "text": " computer that was shared with a bunch of us. We're in a very different world. The world that we're in", "tokens": [51020, 3820, 300, 390, 5507, 365, 257, 3840, 295, 505, 13, 492, 434, 294, 257, 588, 819, 1002, 13, 440, 1002, 300, 321, 434, 294, 51292], "temperature": 0.0, "avg_logprob": -0.07116133433121902, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.00812540017068386}, {"id": 286, "seek": 172356, "start": 1742.12, "end": 1750.12, "text": " in AI now, the world of big AI, is to take enormous data sets and throw them at enormous", "tokens": [51292, 294, 7318, 586, 11, 264, 1002, 295, 955, 7318, 11, 307, 281, 747, 11322, 1412, 6352, 293, 3507, 552, 412, 11322, 51692], "temperature": 0.0, "avg_logprob": -0.07116133433121902, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.00812540017068386}, {"id": 287, "seek": 175012, "start": 1750.12, "end": 1756.28, "text": " machine learning systems. And there's a lesson here that's called the bitter truth. This is from", "tokens": [50364, 3479, 2539, 3652, 13, 400, 456, 311, 257, 6898, 510, 300, 311, 1219, 264, 13871, 3494, 13, 639, 307, 490, 50672], "temperature": 0.0, "avg_logprob": -0.07922330423563469, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.0038609893526881933}, {"id": 288, "seek": 175012, "start": 1756.28, "end": 1760.76, "text": " a machine learning researcher called Rich Sutton. And what Rich pointed out, and he's a very brilliant", "tokens": [50672, 257, 3479, 2539, 21751, 1219, 6781, 40492, 1756, 13, 400, 437, 6781, 10932, 484, 11, 293, 415, 311, 257, 588, 10248, 50896], "temperature": 0.0, "avg_logprob": -0.07922330423563469, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.0038609893526881933}, {"id": 289, "seek": 175012, "start": 1760.76, "end": 1765.9599999999998, "text": " researcher, won every award in the field. He said, look, the real truth is that the big advances that", "tokens": [50896, 21751, 11, 1582, 633, 7130, 294, 264, 2519, 13, 634, 848, 11, 574, 11, 264, 957, 3494, 307, 300, 264, 955, 25297, 300, 51156], "temperature": 0.0, "avg_logprob": -0.07922330423563469, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.0038609893526881933}, {"id": 290, "seek": 175012, "start": 1765.9599999999998, "end": 1771.7199999999998, "text": " we've seen in AI has come about when people have done exactly that. Just throw 10 times more data", "tokens": [51156, 321, 600, 1612, 294, 7318, 575, 808, 466, 562, 561, 362, 1096, 2293, 300, 13, 1449, 3507, 1266, 1413, 544, 1412, 51444], "temperature": 0.0, "avg_logprob": -0.07922330423563469, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.0038609893526881933}, {"id": 291, "seek": 175012, "start": 1771.7199999999998, "end": 1776.6, "text": " and 10 times more compute power at it. And I say it's a bitter lesson because as a scientist,", "tokens": [51444, 293, 1266, 1413, 544, 14722, 1347, 412, 309, 13, 400, 286, 584, 309, 311, 257, 13871, 6898, 570, 382, 257, 12662, 11, 51688], "temperature": 0.0, "avg_logprob": -0.07922330423563469, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.0038609893526881933}, {"id": 292, "seek": 177660, "start": 1776.6, "end": 1785.24, "text": " that's exactly not how you would like progress to be made. Okay. So when I was, as I say,", "tokens": [50364, 300, 311, 2293, 406, 577, 291, 576, 411, 4205, 281, 312, 1027, 13, 1033, 13, 407, 562, 286, 390, 11, 382, 286, 584, 11, 50796], "temperature": 0.0, "avg_logprob": -0.09678343888167497, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.001590145519003272}, {"id": 293, "seek": 177660, "start": 1785.24, "end": 1791.0, "text": " when I was a student, I worked in a discipline called symbolic AI. And symbolic AI tries to get", "tokens": [50796, 562, 286, 390, 257, 3107, 11, 286, 2732, 294, 257, 13635, 1219, 25755, 7318, 13, 400, 25755, 7318, 9898, 281, 483, 51084], "temperature": 0.0, "avg_logprob": -0.09678343888167497, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.001590145519003272}, {"id": 294, "seek": 177660, "start": 1791.0, "end": 1798.76, "text": " AI, roughly speaking, through modeling the mind, modeling the conscious mental processes that go", "tokens": [51084, 7318, 11, 9810, 4124, 11, 807, 15983, 264, 1575, 11, 15983, 264, 6648, 4973, 7555, 300, 352, 51472], "temperature": 0.0, "avg_logprob": -0.09678343888167497, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.001590145519003272}, {"id": 295, "seek": 177660, "start": 1798.76, "end": 1804.84, "text": " on in our mind, the conversations that we have with ourselves in languages. We tried to capture", "tokens": [51472, 322, 294, 527, 1575, 11, 264, 7315, 300, 321, 362, 365, 4175, 294, 8650, 13, 492, 3031, 281, 7983, 51776], "temperature": 0.0, "avg_logprob": -0.09678343888167497, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.001590145519003272}, {"id": 296, "seek": 180484, "start": 1804.84, "end": 1812.6, "text": " those processes in artificial intelligence. In big AI, and so the implication there in symbolic AI", "tokens": [50364, 729, 7555, 294, 11677, 7599, 13, 682, 955, 7318, 11, 293, 370, 264, 37814, 456, 294, 25755, 7318, 50752], "temperature": 0.0, "avg_logprob": -0.0534703835197117, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.0009592970018275082}, {"id": 297, "seek": 180484, "start": 1812.6, "end": 1818.1999999999998, "text": " is that intelligence is a problem of knowledge that we have to give the machine sufficient", "tokens": [50752, 307, 300, 7599, 307, 257, 1154, 295, 3601, 300, 321, 362, 281, 976, 264, 3479, 11563, 51032], "temperature": 0.0, "avg_logprob": -0.0534703835197117, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.0009592970018275082}, {"id": 298, "seek": 180484, "start": 1818.1999999999998, "end": 1824.36, "text": " knowledge about a problem in order for it to be able to solve it. In big AI, the bet is a different", "tokens": [51032, 3601, 466, 257, 1154, 294, 1668, 337, 309, 281, 312, 1075, 281, 5039, 309, 13, 682, 955, 7318, 11, 264, 778, 307, 257, 819, 51340], "temperature": 0.0, "avg_logprob": -0.0534703835197117, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.0009592970018275082}, {"id": 299, "seek": 180484, "start": 1824.36, "end": 1831.8, "text": " one. In big AI, the bet is that intelligence is a problem of data. And if we can get enough data", "tokens": [51340, 472, 13, 682, 955, 7318, 11, 264, 778, 307, 300, 7599, 307, 257, 1154, 295, 1412, 13, 400, 498, 321, 393, 483, 1547, 1412, 51712], "temperature": 0.0, "avg_logprob": -0.0534703835197117, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.0009592970018275082}, {"id": 300, "seek": 183180, "start": 1831.8, "end": 1837.32, "text": " and enough associated computer power, then that will deliver AI. So there's a very different shift", "tokens": [50364, 293, 1547, 6615, 3820, 1347, 11, 550, 300, 486, 4239, 7318, 13, 407, 456, 311, 257, 588, 819, 5513, 50640], "temperature": 0.0, "avg_logprob": -0.09178095804133886, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.00022391141101252288}, {"id": 301, "seek": 183180, "start": 1837.8799999999999, "end": 1844.6, "text": " in this new world of big AI. But the point about big AI is that we're into a new era in artificial", "tokens": [50668, 294, 341, 777, 1002, 295, 955, 7318, 13, 583, 264, 935, 466, 955, 7318, 307, 300, 321, 434, 666, 257, 777, 4249, 294, 11677, 51004], "temperature": 0.0, "avg_logprob": -0.09178095804133886, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.00022391141101252288}, {"id": 302, "seek": 183180, "start": 1844.6, "end": 1852.12, "text": " intelligence where it's data-driven and compute-driven and large, large machine learning systems. So", "tokens": [51004, 7599, 689, 309, 311, 1412, 12, 25456, 293, 14722, 12, 25456, 293, 2416, 11, 2416, 3479, 2539, 3652, 13, 407, 51380], "temperature": 0.0, "avg_logprob": -0.09178095804133886, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.00022391141101252288}, {"id": 303, "seek": 185212, "start": 1852.52, "end": 1861.2399999999998, "text": " why did we get excited back in June 2020? Well, remember, what GPT-3 was intended to do, what", "tokens": [50384, 983, 630, 321, 483, 2919, 646, 294, 6928, 4808, 30, 1042, 11, 1604, 11, 437, 26039, 51, 12, 18, 390, 10226, 281, 360, 11, 437, 50820], "temperature": 0.0, "avg_logprob": -0.17296371459960938, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.000562015688046813}, {"id": 304, "seek": 185212, "start": 1861.2399999999998, "end": 1867.8, "text": " it's trained to do, is that prompt completion task. And it's been trained on everything on the", "tokens": [50820, 309, 311, 8895, 281, 360, 11, 307, 300, 12391, 19372, 5633, 13, 400, 309, 311, 668, 8895, 322, 1203, 322, 264, 51148], "temperature": 0.0, "avg_logprob": -0.17296371459960938, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.000562015688046813}, {"id": 305, "seek": 185212, "start": 1867.8, "end": 1873.8, "text": " World Wide Web. So you can give it a prompt like a one-paragraph summary of the life and", "tokens": [51148, 3937, 42543, 9573, 13, 407, 291, 393, 976, 309, 257, 12391, 411, 257, 472, 12, 2181, 559, 2662, 12691, 295, 264, 993, 293, 51448], "temperature": 0.0, "avg_logprob": -0.17296371459960938, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.000562015688046813}, {"id": 306, "seek": 185212, "start": 1873.8, "end": 1879.32, "text": " achievements of Winston Churchill, and it's read enough one-paragraph summaries of the life and", "tokens": [51448, 21420, 295, 33051, 39837, 11, 293, 309, 311, 1401, 1547, 472, 12, 2181, 559, 2662, 8367, 4889, 295, 264, 993, 293, 51724], "temperature": 0.0, "avg_logprob": -0.17296371459960938, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.000562015688046813}, {"id": 307, "seek": 187932, "start": 1879.3999999999999, "end": 1886.6, "text": " achievements of Winston Churchill that it will come back with a very plausible one. And it's", "tokens": [50368, 21420, 295, 33051, 39837, 300, 309, 486, 808, 646, 365, 257, 588, 39925, 472, 13, 400, 309, 311, 50728], "temperature": 0.0, "avg_logprob": -0.17128986662084406, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.006174205336719751}, {"id": 308, "seek": 187932, "start": 1886.6, "end": 1894.4399999999998, "text": " extremely good at generating realistic sounding text in that way. But this is why we got surprised", "tokens": [50728, 4664, 665, 412, 17746, 12465, 24931, 2487, 294, 300, 636, 13, 583, 341, 307, 983, 321, 658, 6100, 51120], "temperature": 0.0, "avg_logprob": -0.17128986662084406, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.006174205336719751}, {"id": 309, "seek": 187932, "start": 1894.4399999999998, "end": 1900.6, "text": " in AI. This is from a common-sense reasoning task that was devised for artificial intelligence in", "tokens": [51120, 294, 7318, 13, 639, 307, 490, 257, 2689, 12, 82, 1288, 21577, 5633, 300, 390, 1905, 2640, 337, 11677, 7599, 294, 51428], "temperature": 0.0, "avg_logprob": -0.17128986662084406, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.006174205336719751}, {"id": 310, "seek": 187932, "start": 1900.6, "end": 1908.52, "text": " the 1990s. And until three years ago, until June 2020, there was no AI system that existed", "tokens": [51428, 264, 13384, 82, 13, 400, 1826, 1045, 924, 2057, 11, 1826, 6928, 4808, 11, 456, 390, 572, 7318, 1185, 300, 13135, 51824], "temperature": 0.0, "avg_logprob": -0.17128986662084406, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.006174205336719751}, {"id": 311, "seek": 190852, "start": 1909.32, "end": 1913.8799999999999, "text": " in the world that you could apply this test to. It was just literally impossible. There was nothing", "tokens": [50404, 294, 264, 1002, 300, 291, 727, 3079, 341, 1500, 281, 13, 467, 390, 445, 3736, 6243, 13, 821, 390, 1825, 50632], "temperature": 0.0, "avg_logprob": -0.07901226789101787, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0006492111715488136}, {"id": 312, "seek": 190852, "start": 1913.8799999999999, "end": 1920.52, "text": " there, and that changed overnight. So what does this test look like? Well, the test is a bunch", "tokens": [50632, 456, 11, 293, 300, 3105, 13935, 13, 407, 437, 775, 341, 1500, 574, 411, 30, 1042, 11, 264, 1500, 307, 257, 3840, 50964], "temperature": 0.0, "avg_logprob": -0.07901226789101787, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0006492111715488136}, {"id": 313, "seek": 190852, "start": 1920.52, "end": 1926.04, "text": " of questions, and they are questions not for mathematical reasoning or logical reasoning", "tokens": [50964, 295, 1651, 11, 293, 436, 366, 1651, 406, 337, 18894, 21577, 420, 14978, 21577, 51240], "temperature": 0.0, "avg_logprob": -0.07901226789101787, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0006492111715488136}, {"id": 314, "seek": 190852, "start": 1926.04, "end": 1933.72, "text": " or problems in physics. They're common-sense reasoning tasks. And if we ever have AI that", "tokens": [51240, 420, 2740, 294, 10649, 13, 814, 434, 2689, 12, 82, 1288, 21577, 9608, 13, 400, 498, 321, 1562, 362, 7318, 300, 51624], "temperature": 0.0, "avg_logprob": -0.07901226789101787, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0006492111715488136}, {"id": 315, "seek": 193372, "start": 1933.72, "end": 1940.3600000000001, "text": " delivers at scale on really large systems, then it surely would be able to tackle problems like", "tokens": [50364, 24860, 412, 4373, 322, 534, 2416, 3652, 11, 550, 309, 11468, 576, 312, 1075, 281, 14896, 2740, 411, 50696], "temperature": 0.0, "avg_logprob": -0.07809396517478813, "compression_ratio": 1.901185770750988, "no_speech_prob": 0.004343847744166851}, {"id": 316, "seek": 193372, "start": 1940.3600000000001, "end": 1945.8, "text": " this. So what will the questions look like? The human asks the question, if Tom is three inches", "tokens": [50696, 341, 13, 407, 437, 486, 264, 1651, 574, 411, 30, 440, 1952, 8962, 264, 1168, 11, 498, 5041, 307, 1045, 8478, 50968], "temperature": 0.0, "avg_logprob": -0.07809396517478813, "compression_ratio": 1.901185770750988, "no_speech_prob": 0.004343847744166851}, {"id": 317, "seek": 193372, "start": 1945.8, "end": 1950.3600000000001, "text": " taller than Dick, and Dick is two inches taller than Harry, then how much taller is Tom than Harry?", "tokens": [50968, 22406, 813, 18754, 11, 293, 18754, 307, 732, 8478, 22406, 813, 9378, 11, 550, 577, 709, 22406, 307, 5041, 813, 9378, 30, 51196], "temperature": 0.0, "avg_logprob": -0.07809396517478813, "compression_ratio": 1.901185770750988, "no_speech_prob": 0.004343847744166851}, {"id": 318, "seek": 193372, "start": 1950.3600000000001, "end": 1953.96, "text": " The ones in green are the ones that gets right. The ones in red are the ones that gets wrong.", "tokens": [51196, 440, 2306, 294, 3092, 366, 264, 2306, 300, 2170, 558, 13, 440, 2306, 294, 2182, 366, 264, 2306, 300, 2170, 2085, 13, 51376], "temperature": 0.0, "avg_logprob": -0.07809396517478813, "compression_ratio": 1.901185770750988, "no_speech_prob": 0.004343847744166851}, {"id": 319, "seek": 193372, "start": 1953.96, "end": 1959.4, "text": " And it gets that one right, five inches taller than Harry. But we didn't train it to be able to", "tokens": [51376, 400, 309, 2170, 300, 472, 558, 11, 1732, 8478, 22406, 813, 9378, 13, 583, 321, 994, 380, 3847, 309, 281, 312, 1075, 281, 51648], "temperature": 0.0, "avg_logprob": -0.07809396517478813, "compression_ratio": 1.901185770750988, "no_speech_prob": 0.004343847744166851}, {"id": 320, "seek": 195940, "start": 1959.4, "end": 1965.96, "text": " answer that question. So where on earth did that come from? Where did that capability, that simple", "tokens": [50364, 1867, 300, 1168, 13, 407, 689, 322, 4120, 630, 300, 808, 490, 30, 2305, 630, 300, 13759, 11, 300, 2199, 50692], "temperature": 0.0, "avg_logprob": -0.08829389190673828, "compression_ratio": 1.9598393574297188, "no_speech_prob": 0.006467015016824007}, {"id": 321, "seek": 195940, "start": 1965.96, "end": 1971.8000000000002, "text": " capability, to be able to do that? Where did it come from? The next question, can Tom be taller", "tokens": [50692, 13759, 11, 281, 312, 1075, 281, 360, 300, 30, 2305, 630, 309, 808, 490, 30, 440, 958, 1168, 11, 393, 5041, 312, 22406, 50984], "temperature": 0.0, "avg_logprob": -0.08829389190673828, "compression_ratio": 1.9598393574297188, "no_speech_prob": 0.006467015016824007}, {"id": 322, "seek": 195940, "start": 1971.8000000000002, "end": 1977.64, "text": " than himself? This is understanding of the concept of taller than, that the concept of taller than", "tokens": [50984, 813, 3647, 30, 639, 307, 3701, 295, 264, 3410, 295, 22406, 813, 11, 300, 264, 3410, 295, 22406, 813, 51276], "temperature": 0.0, "avg_logprob": -0.08829389190673828, "compression_ratio": 1.9598393574297188, "no_speech_prob": 0.006467015016824007}, {"id": 323, "seek": 195940, "start": 1977.64, "end": 1983.5600000000002, "text": " is irreflexive. You can't be taller. That's a thing cannot be taller than itself. Now, again,", "tokens": [51276, 307, 16014, 69, 2021, 488, 13, 509, 393, 380, 312, 22406, 13, 663, 311, 257, 551, 2644, 312, 22406, 813, 2564, 13, 823, 11, 797, 11, 51572], "temperature": 0.0, "avg_logprob": -0.08829389190673828, "compression_ratio": 1.9598393574297188, "no_speech_prob": 0.006467015016824007}, {"id": 324, "seek": 195940, "start": 1983.5600000000002, "end": 1988.52, "text": " it gets the answer right, but we didn't train it on that. That's not what we didn't train the system", "tokens": [51572, 309, 2170, 264, 1867, 558, 11, 457, 321, 994, 380, 3847, 309, 322, 300, 13, 663, 311, 406, 437, 321, 994, 380, 3847, 264, 1185, 51820], "temperature": 0.0, "avg_logprob": -0.08829389190673828, "compression_ratio": 1.9598393574297188, "no_speech_prob": 0.006467015016824007}, {"id": 325, "seek": 198852, "start": 1988.52, "end": 1993.32, "text": " to be good at answering questions about what taller than means. And by the way, 20 years ago,", "tokens": [50364, 281, 312, 665, 412, 13430, 1651, 466, 437, 22406, 813, 1355, 13, 400, 538, 264, 636, 11, 945, 924, 2057, 11, 50604], "temperature": 0.0, "avg_logprob": -0.0867045199284788, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0023377048783004284}, {"id": 326, "seek": 198852, "start": 1993.32, "end": 1999.8799999999999, "text": " that's exactly what people did in AI, right? So where did that capability come from? Can a sister", "tokens": [50604, 300, 311, 2293, 437, 561, 630, 294, 7318, 11, 558, 30, 407, 689, 630, 300, 13759, 808, 490, 30, 1664, 257, 4892, 50932], "temperature": 0.0, "avg_logprob": -0.0867045199284788, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0023377048783004284}, {"id": 327, "seek": 198852, "start": 1999.8799999999999, "end": 2004.92, "text": " be taller than her brother? Yes, a system can be taller than her brother. Can two siblings each", "tokens": [50932, 312, 22406, 813, 720, 3708, 30, 1079, 11, 257, 1185, 393, 312, 22406, 813, 720, 3708, 13, 1664, 732, 20571, 1184, 51184], "temperature": 0.0, "avg_logprob": -0.0867045199284788, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0023377048783004284}, {"id": 328, "seek": 198852, "start": 2004.92, "end": 2008.52, "text": " be taller than the other? And it gets this one wrong. And actually, I have puzzled, is there", "tokens": [51184, 312, 22406, 813, 264, 661, 30, 400, 309, 2170, 341, 472, 2085, 13, 400, 767, 11, 286, 362, 18741, 1493, 11, 307, 456, 51364], "temperature": 0.0, "avg_logprob": -0.0867045199284788, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0023377048783004284}, {"id": 329, "seek": 198852, "start": 2008.52, "end": 2013.8, "text": " any way that its answer could be correct? And it's just getting it correct in a way that I don't", "tokens": [51364, 604, 636, 300, 1080, 1867, 727, 312, 3006, 30, 400, 309, 311, 445, 1242, 309, 3006, 294, 257, 636, 300, 286, 500, 380, 51628], "temperature": 0.0, "avg_logprob": -0.0867045199284788, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0023377048783004284}, {"id": 330, "seek": 201380, "start": 2013.8, "end": 2019.72, "text": " understand. But I haven't yet figured out any way that that answer could be correct, right? So why", "tokens": [50364, 1223, 13, 583, 286, 2378, 380, 1939, 8932, 484, 604, 636, 300, 300, 1867, 727, 312, 3006, 11, 558, 30, 407, 983, 50660], "temperature": 0.0, "avg_logprob": -0.10032514823499576, "compression_ratio": 1.6955017301038062, "no_speech_prob": 0.006749795284122229}, {"id": 331, "seek": 201380, "start": 2019.72, "end": 2023.8799999999999, "text": " it gets that one wrong, I don't know. Then this one, I'm also surprised at, on a map which compass", "tokens": [50660, 309, 2170, 300, 472, 2085, 11, 286, 500, 380, 458, 13, 1396, 341, 472, 11, 286, 478, 611, 6100, 412, 11, 322, 257, 4471, 597, 10707, 50868], "temperature": 0.0, "avg_logprob": -0.10032514823499576, "compression_ratio": 1.6955017301038062, "no_speech_prob": 0.006749795284122229}, {"id": 332, "seek": 201380, "start": 2023.8799999999999, "end": 2028.36, "text": " direction is usually left, and it thinks north is usually to the left. I don't know if there's any", "tokens": [50868, 3513, 307, 2673, 1411, 11, 293, 309, 7309, 6830, 307, 2673, 281, 264, 1411, 13, 286, 500, 380, 458, 498, 456, 311, 604, 51092], "temperature": 0.0, "avg_logprob": -0.10032514823499576, "compression_ratio": 1.6955017301038062, "no_speech_prob": 0.006749795284122229}, {"id": 333, "seek": 201380, "start": 2028.36, "end": 2032.68, "text": " countries in the world that conventionally have north to the left, but I don't think so. Yeah.", "tokens": [51092, 3517, 294, 264, 1002, 300, 10286, 379, 362, 6830, 281, 264, 1411, 11, 457, 286, 500, 380, 519, 370, 13, 865, 13, 51308], "temperature": 0.0, "avg_logprob": -0.10032514823499576, "compression_ratio": 1.6955017301038062, "no_speech_prob": 0.006749795284122229}, {"id": 334, "seek": 201380, "start": 2033.8, "end": 2039.3999999999999, "text": " Can fish run? No, it understands that fish cannot run. If a door is locked, what must you do first", "tokens": [51364, 1664, 3506, 1190, 30, 883, 11, 309, 15146, 300, 3506, 2644, 1190, 13, 759, 257, 2853, 307, 9376, 11, 437, 1633, 291, 360, 700, 51644], "temperature": 0.0, "avg_logprob": -0.10032514823499576, "compression_ratio": 1.6955017301038062, "no_speech_prob": 0.006749795284122229}, {"id": 335, "seek": 203940, "start": 2039.48, "end": 2044.3600000000001, "text": " before opening it? You must first unlock it before opening. And then finally, and very weirdly, it", "tokens": [50368, 949, 5193, 309, 30, 509, 1633, 700, 11634, 309, 949, 5193, 13, 400, 550, 2721, 11, 293, 588, 48931, 11, 309, 50612], "temperature": 0.0, "avg_logprob": -0.07517566178974353, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002983499551191926}, {"id": 336, "seek": 203940, "start": 2044.3600000000001, "end": 2048.84, "text": " gets this one wrong, which was invented first, car ships or planes, and it thinks cars were", "tokens": [50612, 2170, 341, 472, 2085, 11, 597, 390, 14479, 700, 11, 1032, 11434, 420, 14952, 11, 293, 309, 7309, 5163, 645, 50836], "temperature": 0.0, "avg_logprob": -0.07517566178974353, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002983499551191926}, {"id": 337, "seek": 203940, "start": 2048.84, "end": 2055.56, "text": " invented first. No idea what's going on there. Now, my point is that this system was built", "tokens": [50836, 14479, 700, 13, 883, 1558, 437, 311, 516, 322, 456, 13, 823, 11, 452, 935, 307, 300, 341, 1185, 390, 3094, 51172], "temperature": 0.0, "avg_logprob": -0.07517566178974353, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002983499551191926}, {"id": 338, "seek": 203940, "start": 2055.56, "end": 2061.32, "text": " to be able to complete from a prompt. And it's no surprise that it would be able to generate a good", "tokens": [51172, 281, 312, 1075, 281, 3566, 490, 257, 12391, 13, 400, 309, 311, 572, 6365, 300, 309, 576, 312, 1075, 281, 8460, 257, 665, 51460], "temperature": 0.0, "avg_logprob": -0.07517566178974353, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002983499551191926}, {"id": 339, "seek": 203940, "start": 2061.32, "end": 2065.64, "text": " one paragraph summary of the life and achievements of Winston Churchill, because it will have seen", "tokens": [51460, 472, 18865, 12691, 295, 264, 993, 293, 21420, 295, 33051, 39837, 11, 570, 309, 486, 362, 1612, 51676], "temperature": 0.0, "avg_logprob": -0.07517566178974353, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002983499551191926}, {"id": 340, "seek": 206564, "start": 2065.64, "end": 2071.3199999999997, "text": " all that in the training data. But where does the understanding of taller than come from?", "tokens": [50364, 439, 300, 294, 264, 3097, 1412, 13, 583, 689, 775, 264, 3701, 295, 22406, 813, 808, 490, 30, 50648], "temperature": 0.0, "avg_logprob": -0.06856876195863236, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0060175578109920025}, {"id": 341, "seek": 206564, "start": 2072.04, "end": 2079.96, "text": " And there are a million other examples like this. Since June 2020, the AI community has just gone", "tokens": [50684, 400, 456, 366, 257, 2459, 661, 5110, 411, 341, 13, 4162, 6928, 4808, 11, 264, 7318, 1768, 575, 445, 2780, 51080], "temperature": 0.0, "avg_logprob": -0.06856876195863236, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0060175578109920025}, {"id": 342, "seek": 206564, "start": 2079.96, "end": 2087.56, "text": " nuts exploring the possibilities of these systems and trying to understand why they can do these", "tokens": [51080, 10483, 12736, 264, 12178, 295, 613, 3652, 293, 1382, 281, 1223, 983, 436, 393, 360, 613, 51460], "temperature": 0.0, "avg_logprob": -0.06856876195863236, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0060175578109920025}, {"id": 343, "seek": 206564, "start": 2087.56, "end": 2095.24, "text": " things when that's not what we trained them to do. This is an extraordinary time to be an AI researcher", "tokens": [51460, 721, 562, 300, 311, 406, 437, 321, 8895, 552, 281, 360, 13, 639, 307, 364, 10581, 565, 281, 312, 364, 7318, 21751, 51844], "temperature": 0.0, "avg_logprob": -0.06856876195863236, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.0060175578109920025}, {"id": 344, "seek": 209524, "start": 2095.3199999999997, "end": 2102.04, "text": " because there are now questions which for most of the history of AI until June 2020 were just", "tokens": [50368, 570, 456, 366, 586, 1651, 597, 337, 881, 295, 264, 2503, 295, 7318, 1826, 6928, 4808, 645, 445, 50704], "temperature": 0.0, "avg_logprob": -0.07658388349745009, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.0010300573194399476}, {"id": 345, "seek": 209524, "start": 2102.04, "end": 2106.7599999999998, "text": " philosophical discussions. We couldn't test them out because there was nothing to test them on,", "tokens": [50704, 25066, 11088, 13, 492, 2809, 380, 1500, 552, 484, 570, 456, 390, 1825, 281, 1500, 552, 322, 11, 50940], "temperature": 0.0, "avg_logprob": -0.07658388349745009, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.0010300573194399476}, {"id": 346, "seek": 209524, "start": 2106.7599999999998, "end": 2112.4399999999996, "text": " literally. And then overnight, that changed. So it genuinely was a big deal. This was really,", "tokens": [50940, 3736, 13, 400, 550, 13935, 11, 300, 3105, 13, 407, 309, 17839, 390, 257, 955, 2028, 13, 639, 390, 534, 11, 51224], "temperature": 0.0, "avg_logprob": -0.07658388349745009, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.0010300573194399476}, {"id": 347, "seek": 209524, "start": 2112.4399999999996, "end": 2118.7599999999998, "text": " really a big deal, the arrival of this system. Of course, the world didn't notice in June 2020.", "tokens": [51224, 534, 257, 955, 2028, 11, 264, 18365, 295, 341, 1185, 13, 2720, 1164, 11, 264, 1002, 994, 380, 3449, 294, 6928, 4808, 13, 51540], "temperature": 0.0, "avg_logprob": -0.07658388349745009, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.0010300573194399476}, {"id": 348, "seek": 211876, "start": 2118.76, "end": 2126.44, "text": " The world noticed when ChatGPT was released. And what is ChatGPT? ChatGPT is a polished and improved", "tokens": [50364, 440, 1002, 5694, 562, 27503, 38, 47, 51, 390, 4736, 13, 400, 437, 307, 27503, 38, 47, 51, 30, 27503, 38, 47, 51, 307, 257, 29079, 293, 9689, 50748], "temperature": 0.0, "avg_logprob": -0.08114023116028425, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.003418636042624712}, {"id": 349, "seek": 211876, "start": 2126.44, "end": 2132.5200000000004, "text": " version of GPT-3. But it's basically the same technology. And it's using the experience that", "tokens": [50748, 3037, 295, 26039, 51, 12, 18, 13, 583, 309, 311, 1936, 264, 912, 2899, 13, 400, 309, 311, 1228, 264, 1752, 300, 51052], "temperature": 0.0, "avg_logprob": -0.08114023116028425, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.003418636042624712}, {"id": 350, "seek": 211876, "start": 2132.5200000000004, "end": 2139.0800000000004, "text": " that company had with GPT-3 and how it was used in order to be able to improve it and make it more", "tokens": [51052, 300, 2237, 632, 365, 26039, 51, 12, 18, 293, 577, 309, 390, 1143, 294, 1668, 281, 312, 1075, 281, 3470, 309, 293, 652, 309, 544, 51380], "temperature": 0.0, "avg_logprob": -0.08114023116028425, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.003418636042624712}, {"id": 351, "seek": 211876, "start": 2139.0800000000004, "end": 2145.4, "text": " polished and more accessible and so on. So for AI researchers, the really interesting thing is", "tokens": [51380, 29079, 293, 544, 9515, 293, 370, 322, 13, 407, 337, 7318, 10309, 11, 264, 534, 1880, 551, 307, 51696], "temperature": 0.0, "avg_logprob": -0.08114023116028425, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.003418636042624712}, {"id": 352, "seek": 214540, "start": 2145.4, "end": 2149.56, "text": " not that it can give me a one-paragraph summary of the life and achievements of Winston Churchill.", "tokens": [50364, 406, 300, 309, 393, 976, 385, 257, 472, 12, 2181, 559, 2662, 12691, 295, 264, 993, 293, 21420, 295, 33051, 39837, 13, 50572], "temperature": 0.0, "avg_logprob": -0.07122857110542163, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.002871404169127345}, {"id": 353, "seek": 214540, "start": 2149.56, "end": 2154.76, "text": " And actually, you can Google that in any case. The really interesting thing is what we call", "tokens": [50572, 400, 767, 11, 291, 393, 3329, 300, 294, 604, 1389, 13, 440, 534, 1880, 551, 307, 437, 321, 818, 50832], "temperature": 0.0, "avg_logprob": -0.07122857110542163, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.002871404169127345}, {"id": 354, "seek": 214540, "start": 2154.76, "end": 2161.2400000000002, "text": " emergent capabilities. And emergent capabilities are capabilities that the system has,", "tokens": [50832, 4345, 6930, 10862, 13, 400, 4345, 6930, 10862, 366, 10862, 300, 264, 1185, 575, 11, 51156], "temperature": 0.0, "avg_logprob": -0.07122857110542163, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.002871404169127345}, {"id": 355, "seek": 214540, "start": 2161.2400000000002, "end": 2167.88, "text": " but that we didn't design it to have. And so there's, I say, an enormous body of work going on now", "tokens": [51156, 457, 300, 321, 994, 380, 1715, 309, 281, 362, 13, 400, 370, 456, 311, 11, 286, 584, 11, 364, 11322, 1772, 295, 589, 516, 322, 586, 51488], "temperature": 0.0, "avg_logprob": -0.07122857110542163, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.002871404169127345}, {"id": 356, "seek": 214540, "start": 2167.88, "end": 2173.2400000000002, "text": " trying to map out exactly what those capabilities are. And we're going to come back and talk about", "tokens": [51488, 1382, 281, 4471, 484, 2293, 437, 729, 10862, 366, 13, 400, 321, 434, 516, 281, 808, 646, 293, 751, 466, 51756], "temperature": 0.0, "avg_logprob": -0.07122857110542163, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.002871404169127345}, {"id": 357, "seek": 217324, "start": 2173.24, "end": 2179.72, "text": " some of them later on. Okay. So the limits to this are not, at the moment, well understood and", "tokens": [50364, 512, 295, 552, 1780, 322, 13, 1033, 13, 407, 264, 10406, 281, 341, 366, 406, 11, 412, 264, 1623, 11, 731, 7320, 293, 50688], "temperature": 0.0, "avg_logprob": -0.08533204612085375, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.0007317271665669978}, {"id": 358, "seek": 217324, "start": 2179.72, "end": 2185.3199999999997, "text": " actually fiercely contentious. One of the big problems, by the way, is that you construct some", "tokens": [50688, 767, 16334, 42879, 2701, 851, 13, 1485, 295, 264, 955, 2740, 11, 538, 264, 636, 11, 307, 300, 291, 7690, 512, 50968], "temperature": 0.0, "avg_logprob": -0.08533204612085375, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.0007317271665669978}, {"id": 359, "seek": 217324, "start": 2185.3199999999997, "end": 2191.08, "text": " test for this and you try this test out and you get some answer and then you discover it's in the", "tokens": [50968, 1500, 337, 341, 293, 291, 853, 341, 1500, 484, 293, 291, 483, 512, 1867, 293, 550, 291, 4411, 309, 311, 294, 264, 51256], "temperature": 0.0, "avg_logprob": -0.08533204612085375, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.0007317271665669978}, {"id": 360, "seek": 217324, "start": 2191.08, "end": 2197.3999999999996, "text": " training data. You can just find it on the World Wide Web. And it's actually quite hard to construct", "tokens": [51256, 3097, 1412, 13, 509, 393, 445, 915, 309, 322, 264, 3937, 42543, 9573, 13, 400, 309, 311, 767, 1596, 1152, 281, 7690, 51572], "temperature": 0.0, "avg_logprob": -0.08533204612085375, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.0007317271665669978}, {"id": 361, "seek": 217324, "start": 2197.3999999999996, "end": 2202.12, "text": " tests for intelligence that you're absolutely sure are not anywhere on the World Wide Web. It", "tokens": [51572, 6921, 337, 7599, 300, 291, 434, 3122, 988, 366, 406, 4992, 322, 264, 3937, 42543, 9573, 13, 467, 51808], "temperature": 0.0, "avg_logprob": -0.08533204612085375, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.0007317271665669978}, {"id": 362, "seek": 220212, "start": 2202.12, "end": 2207.56, "text": " really is actually quite hard to do that. So we need a new science of being able to explore these", "tokens": [50364, 534, 307, 767, 1596, 1152, 281, 360, 300, 13, 407, 321, 643, 257, 777, 3497, 295, 885, 1075, 281, 6839, 613, 50636], "temperature": 0.0, "avg_logprob": -0.07609827547188265, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.0003525184583850205}, {"id": 363, "seek": 220212, "start": 2207.56, "end": 2213.56, "text": " systems and understand their capabilities. The limits are not well understood. But nevertheless,", "tokens": [50636, 3652, 293, 1223, 641, 10862, 13, 440, 10406, 366, 406, 731, 7320, 13, 583, 26924, 11, 50936], "temperature": 0.0, "avg_logprob": -0.07609827547188265, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.0003525184583850205}, {"id": 364, "seek": 220212, "start": 2213.56, "end": 2220.8399999999997, "text": " this is very exciting stuff. So let's talk about some issues with the technology. So now you understand", "tokens": [50936, 341, 307, 588, 4670, 1507, 13, 407, 718, 311, 751, 466, 512, 2663, 365, 264, 2899, 13, 407, 586, 291, 1223, 51300], "temperature": 0.0, "avg_logprob": -0.07609827547188265, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.0003525184583850205}, {"id": 365, "seek": 220212, "start": 2220.8399999999997, "end": 2226.3599999999997, "text": " how the technology works. It's neural network based in a particular transformer architecture", "tokens": [51300, 577, 264, 2899, 1985, 13, 467, 311, 18161, 3209, 2361, 294, 257, 1729, 31782, 9482, 51576], "temperature": 0.0, "avg_logprob": -0.07609827547188265, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.0003525184583850205}, {"id": 366, "seek": 222636, "start": 2226.36, "end": 2231.8, "text": " which is all designed to do that prompt completion stuff. And it's been trained with vast, vast,", "tokens": [50364, 597, 307, 439, 4761, 281, 360, 300, 12391, 19372, 1507, 13, 400, 309, 311, 668, 8895, 365, 8369, 11, 8369, 11, 50636], "temperature": 0.0, "avg_logprob": -0.0891967523293417, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.005900432355701923}, {"id": 367, "seek": 222636, "start": 2231.8, "end": 2237.96, "text": " vast amounts of training data just in order to be able to try to make its best guess about which", "tokens": [50636, 8369, 11663, 295, 3097, 1412, 445, 294, 1668, 281, 312, 1075, 281, 853, 281, 652, 1080, 1151, 2041, 466, 597, 50944], "temperature": 0.0, "avg_logprob": -0.0891967523293417, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.005900432355701923}, {"id": 368, "seek": 222636, "start": 2237.96, "end": 2243.6400000000003, "text": " words should come next. But because of the scale of it, it's seen so much training data, the", "tokens": [50944, 2283, 820, 808, 958, 13, 583, 570, 295, 264, 4373, 295, 309, 11, 309, 311, 1612, 370, 709, 3097, 1412, 11, 264, 51228], "temperature": 0.0, "avg_logprob": -0.0891967523293417, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.005900432355701923}, {"id": 369, "seek": 222636, "start": 2243.6400000000003, "end": 2249.96, "text": " sophistication of this transformer architecture, it's very, very fluent in what it does. And if", "tokens": [51228, 15572, 399, 295, 341, 31782, 9482, 11, 309, 311, 588, 11, 588, 40799, 294, 437, 309, 775, 13, 400, 498, 51544], "temperature": 0.0, "avg_logprob": -0.0891967523293417, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.005900432355701923}, {"id": 370, "seek": 222636, "start": 2249.96, "end": 2253.88, "text": " you've, so who's used it? Has everybody used it? I'm guessing most people, if you're in a lecture on", "tokens": [51544, 291, 600, 11, 370, 567, 311, 1143, 309, 30, 8646, 2201, 1143, 309, 30, 286, 478, 17939, 881, 561, 11, 498, 291, 434, 294, 257, 7991, 322, 51740], "temperature": 0.0, "avg_logprob": -0.0891967523293417, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.005900432355701923}, {"id": 371, "seek": 225388, "start": 2253.88, "end": 2258.44, "text": " artificial intelligence, most people will have tried it out. If you haven't, you should do because", "tokens": [50364, 11677, 7599, 11, 881, 561, 486, 362, 3031, 309, 484, 13, 759, 291, 2378, 380, 11, 291, 820, 360, 570, 50592], "temperature": 0.0, "avg_logprob": -0.0709309781718458, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.0026708978693932295}, {"id": 372, "seek": 225388, "start": 2258.44, "end": 2263.88, "text": " this really is a landmark year. This is the first time in history that we've had powerful,", "tokens": [50592, 341, 534, 307, 257, 26962, 1064, 13, 639, 307, 264, 700, 565, 294, 2503, 300, 321, 600, 632, 4005, 11, 50864], "temperature": 0.0, "avg_logprob": -0.0709309781718458, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.0026708978693932295}, {"id": 373, "seek": 225388, "start": 2263.88, "end": 2270.12, "text": " general purpose AI tools available to everybody. It's never happened before. So it is a breakthrough", "tokens": [50864, 2674, 4334, 7318, 3873, 2435, 281, 2201, 13, 467, 311, 1128, 2011, 949, 13, 407, 309, 307, 257, 22397, 51176], "temperature": 0.0, "avg_logprob": -0.0709309781718458, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.0026708978693932295}, {"id": 374, "seek": 225388, "start": 2270.12, "end": 2274.12, "text": " year. And if you haven't tried it, you should do. If you use it, by the way, don't type in", "tokens": [51176, 1064, 13, 400, 498, 291, 2378, 380, 3031, 309, 11, 291, 820, 360, 13, 759, 291, 764, 309, 11, 538, 264, 636, 11, 500, 380, 2010, 294, 51376], "temperature": 0.0, "avg_logprob": -0.0709309781718458, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.0026708978693932295}, {"id": 375, "seek": 225388, "start": 2274.12, "end": 2280.36, "text": " anything personal about yourself because it will just go into the training data. Don't ask it how", "tokens": [51376, 1340, 2973, 466, 1803, 570, 309, 486, 445, 352, 666, 264, 3097, 1412, 13, 1468, 380, 1029, 309, 577, 51688], "temperature": 0.0, "avg_logprob": -0.0709309781718458, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.0026708978693932295}, {"id": 376, "seek": 228036, "start": 2280.36, "end": 2285.1600000000003, "text": " to fix your relationship, right? I mean, that's not something. Don't complain about your boss", "tokens": [50364, 281, 3191, 428, 2480, 11, 558, 30, 286, 914, 11, 300, 311, 406, 746, 13, 1468, 380, 11024, 466, 428, 5741, 50604], "temperature": 0.0, "avg_logprob": -0.07187923183286093, "compression_ratio": 1.62, "no_speech_prob": 0.004446850623935461}, {"id": 377, "seek": 228036, "start": 2285.1600000000003, "end": 2289.56, "text": " because all of that will go in the training data. And next week, somebody will ask a query and it", "tokens": [50604, 570, 439, 295, 300, 486, 352, 294, 264, 3097, 1412, 13, 400, 958, 1243, 11, 2618, 486, 1029, 257, 14581, 293, 309, 50824], "temperature": 0.0, "avg_logprob": -0.07187923183286093, "compression_ratio": 1.62, "no_speech_prob": 0.004446850623935461}, {"id": 378, "seek": 228036, "start": 2289.56, "end": 2296.2000000000003, "text": " will all come back out again. I don't know what you're laughing. This has happened. This has happened", "tokens": [50824, 486, 439, 808, 646, 484, 797, 13, 286, 500, 380, 458, 437, 291, 434, 5059, 13, 639, 575, 2011, 13, 639, 575, 2011, 51156], "temperature": 0.0, "avg_logprob": -0.07187923183286093, "compression_ratio": 1.62, "no_speech_prob": 0.004446850623935461}, {"id": 379, "seek": 228036, "start": 2296.2000000000003, "end": 2302.6800000000003, "text": " with absolute certainty. Okay, but so let's look at some issues. So the first, I think many people", "tokens": [51156, 365, 8236, 27022, 13, 1033, 11, 457, 370, 718, 311, 574, 412, 512, 2663, 13, 407, 264, 700, 11, 286, 519, 867, 561, 51480], "temperature": 0.0, "avg_logprob": -0.07187923183286093, "compression_ratio": 1.62, "no_speech_prob": 0.004446850623935461}, {"id": 380, "seek": 228036, "start": 2302.6800000000003, "end": 2309.2400000000002, "text": " will be aware of, it gets stuff wrong a lot. And this is problematic for a number of reasons.", "tokens": [51480, 486, 312, 3650, 295, 11, 309, 2170, 1507, 2085, 257, 688, 13, 400, 341, 307, 19011, 337, 257, 1230, 295, 4112, 13, 51808], "temperature": 0.0, "avg_logprob": -0.07187923183286093, "compression_ratio": 1.62, "no_speech_prob": 0.004446850623935461}, {"id": 381, "seek": 230924, "start": 2309.24, "end": 2313.08, "text": " So when actually, I don't remember if it was GPT-3, but one of the early large language models,", "tokens": [50364, 407, 562, 767, 11, 286, 500, 380, 1604, 498, 309, 390, 26039, 51, 12, 18, 11, 457, 472, 295, 264, 2440, 2416, 2856, 5245, 11, 50556], "temperature": 0.0, "avg_logprob": -0.13982541593786788, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.00031718221725896}, {"id": 382, "seek": 230924, "start": 2313.08, "end": 2317.3999999999996, "text": " I was playing with it and I did something which I'm sure many of you had done and it's kind of", "tokens": [50556, 286, 390, 2433, 365, 309, 293, 286, 630, 746, 597, 286, 478, 988, 867, 295, 291, 632, 1096, 293, 309, 311, 733, 295, 50772], "temperature": 0.0, "avg_logprob": -0.13982541593786788, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.00031718221725896}, {"id": 383, "seek": 230924, "start": 2317.3999999999996, "end": 2322.8399999999997, "text": " tacky. But anyway, I said, who is Michael Woolridge? You might have tried it. Anyway,", "tokens": [50772, 9426, 88, 13, 583, 4033, 11, 286, 848, 11, 567, 307, 5116, 46307, 15804, 30, 509, 1062, 362, 3031, 309, 13, 5684, 11, 51044], "temperature": 0.0, "avg_logprob": -0.13982541593786788, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.00031718221725896}, {"id": 384, "seek": 230924, "start": 2322.8399999999997, "end": 2328.4399999999996, "text": " Michael Woolridge is a BBC broadcast. No, not that Michael Woolridge. Michael Woolridge is the", "tokens": [51044, 5116, 46307, 15804, 307, 257, 22669, 9975, 13, 883, 11, 406, 300, 5116, 46307, 15804, 13, 5116, 46307, 15804, 307, 264, 51324], "temperature": 0.0, "avg_logprob": -0.13982541593786788, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.00031718221725896}, {"id": 385, "seek": 230924, "start": 2328.4399999999996, "end": 2332.68, "text": " Australian Health Minister. No, not that Michael Woolridge. Michael Woolridge in Oxford. And it", "tokens": [51324, 13337, 5912, 6506, 13, 883, 11, 406, 300, 5116, 46307, 15804, 13, 5116, 46307, 15804, 294, 24786, 13, 400, 309, 51536], "temperature": 0.0, "avg_logprob": -0.13982541593786788, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.00031718221725896}, {"id": 386, "seek": 230924, "start": 2332.68, "end": 2337.08, "text": " came back with a few line summary of me. Michael Woolridge is a researcher in artificial intelligence,", "tokens": [51536, 1361, 646, 365, 257, 1326, 1622, 12691, 295, 385, 13, 5116, 46307, 15804, 307, 257, 21751, 294, 11677, 7599, 11, 51756], "temperature": 0.0, "avg_logprob": -0.13982541593786788, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.00031718221725896}, {"id": 387, "seek": 233708, "start": 2337.08, "end": 2341.08, "text": " et cetera, et cetera, et cetera. Please tell me you've all tried that, no? Anyway,", "tokens": [50364, 1030, 11458, 11, 1030, 11458, 11, 1030, 11458, 13, 2555, 980, 385, 291, 600, 439, 3031, 300, 11, 572, 30, 5684, 11, 50564], "temperature": 0.0, "avg_logprob": -0.10654756664174848, "compression_ratio": 1.6, "no_speech_prob": 4.025917223771103e-05}, {"id": 388, "seek": 233708, "start": 2341.96, "end": 2345.96, "text": " but it said Michael Woolridge studied his undergraduate degree at Cambridge.", "tokens": [50608, 457, 309, 848, 5116, 46307, 15804, 9454, 702, 19113, 4314, 412, 24876, 13, 50808], "temperature": 0.0, "avg_logprob": -0.10654756664174848, "compression_ratio": 1.6, "no_speech_prob": 4.025917223771103e-05}, {"id": 389, "seek": 233708, "start": 2347.24, "end": 2353.16, "text": " And I was an Oxford professor. You can imagine how I felt about that. But anyway, the point is", "tokens": [50872, 400, 286, 390, 364, 24786, 8304, 13, 509, 393, 3811, 577, 286, 2762, 466, 300, 13, 583, 4033, 11, 264, 935, 307, 51168], "temperature": 0.0, "avg_logprob": -0.10654756664174848, "compression_ratio": 1.6, "no_speech_prob": 4.025917223771103e-05}, {"id": 390, "seek": 233708, "start": 2353.16, "end": 2358.44, "text": " it's flatly untrue. And in fact, my academic origins are very far removed from Oxbridge.", "tokens": [51168, 309, 311, 4962, 356, 1701, 41729, 13, 400, 294, 1186, 11, 452, 7778, 22721, 366, 588, 1400, 7261, 490, 16489, 18249, 13, 51432], "temperature": 0.0, "avg_logprob": -0.10654756664174848, "compression_ratio": 1.6, "no_speech_prob": 4.025917223771103e-05}, {"id": 391, "seek": 233708, "start": 2358.44, "end": 2363.88, "text": " But why did it do that? Because it's read in all that training data out there. It's read", "tokens": [51432, 583, 983, 630, 309, 360, 300, 30, 1436, 309, 311, 1401, 294, 439, 300, 3097, 1412, 484, 456, 13, 467, 311, 1401, 51704], "temperature": 0.0, "avg_logprob": -0.10654756664174848, "compression_ratio": 1.6, "no_speech_prob": 4.025917223771103e-05}, {"id": 392, "seek": 236388, "start": 2363.88, "end": 2371.1600000000003, "text": " thousands of biographies of Oxbridge professors. And this is a very common thing, right? And it's", "tokens": [50364, 5383, 295, 3228, 3108, 530, 295, 16489, 18249, 15924, 13, 400, 341, 307, 257, 588, 2689, 551, 11, 558, 30, 400, 309, 311, 50728], "temperature": 0.0, "avg_logprob": -0.06566537728830546, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0003511159447953105}, {"id": 393, "seek": 236388, "start": 2371.1600000000003, "end": 2375.88, "text": " making its best guess. The whole point about the architecture is it's making its best guess", "tokens": [50728, 1455, 1080, 1151, 2041, 13, 440, 1379, 935, 466, 264, 9482, 307, 309, 311, 1455, 1080, 1151, 2041, 50964], "temperature": 0.0, "avg_logprob": -0.06566537728830546, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0003511159447953105}, {"id": 394, "seek": 236388, "start": 2375.88, "end": 2381.6400000000003, "text": " about what should go there. It's filling in the blanks. But here's the thing. It's filling in", "tokens": [50964, 466, 437, 820, 352, 456, 13, 467, 311, 10623, 294, 264, 8247, 82, 13, 583, 510, 311, 264, 551, 13, 467, 311, 10623, 294, 51252], "temperature": 0.0, "avg_logprob": -0.06566537728830546, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0003511159447953105}, {"id": 395, "seek": 236388, "start": 2381.6400000000003, "end": 2388.6800000000003, "text": " the blanks in a very, very plausible way. If you'd read on my biography that Michael Woolridge", "tokens": [51252, 264, 8247, 82, 294, 257, 588, 11, 588, 39925, 636, 13, 759, 291, 1116, 1401, 322, 452, 37062, 300, 5116, 46307, 15804, 51604], "temperature": 0.0, "avg_logprob": -0.06566537728830546, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0003511159447953105}, {"id": 396, "seek": 236388, "start": 2388.6800000000003, "end": 2392.52, "text": " studied his first degree at the University of Uzbekistan, for example, you might have thought,", "tokens": [51604, 9454, 702, 700, 4314, 412, 264, 3535, 295, 38564, 25714, 7559, 11, 337, 1365, 11, 291, 1062, 362, 1194, 11, 51796], "temperature": 0.0, "avg_logprob": -0.06566537728830546, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0003511159447953105}, {"id": 397, "seek": 239252, "start": 2392.52, "end": 2398.28, "text": " well, that's a bit odd. Is that really true? But you wouldn't at all have guessed there was", "tokens": [50364, 731, 11, 300, 311, 257, 857, 7401, 13, 1119, 300, 534, 2074, 30, 583, 291, 2759, 380, 412, 439, 362, 21852, 456, 390, 50652], "temperature": 0.0, "avg_logprob": -0.08761635161282723, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0003827646723948419}, {"id": 398, "seek": 239252, "start": 2398.28, "end": 2403.08, "text": " any issue if you'd read Cambridge. Because it looks completely plausible. Even if in my case,", "tokens": [50652, 604, 2734, 498, 291, 1116, 1401, 24876, 13, 1436, 309, 1542, 2584, 39925, 13, 2754, 498, 294, 452, 1389, 11, 50892], "temperature": 0.0, "avg_logprob": -0.08761635161282723, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0003827646723948419}, {"id": 399, "seek": 239252, "start": 2403.08, "end": 2410.2, "text": " it absolutely isn't true. So it gets things wrong. And it gets things wrong in very plausible ways.", "tokens": [50892, 309, 3122, 1943, 380, 2074, 13, 407, 309, 2170, 721, 2085, 13, 400, 309, 2170, 721, 2085, 294, 588, 39925, 2098, 13, 51248], "temperature": 0.0, "avg_logprob": -0.08761635161282723, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0003827646723948419}, {"id": 400, "seek": 239252, "start": 2410.2, "end": 2414.36, "text": " And of course, it's very fluent, right? I mean, the technology comes back with very,", "tokens": [51248, 400, 295, 1164, 11, 309, 311, 588, 40799, 11, 558, 30, 286, 914, 11, 264, 2899, 1487, 646, 365, 588, 11, 51456], "temperature": 0.0, "avg_logprob": -0.08761635161282723, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0003827646723948419}, {"id": 401, "seek": 239252, "start": 2414.36, "end": 2420.7599999999998, "text": " very fluent explanations. And that combination of plausibility, Woolridge studied his undergraduate", "tokens": [51456, 588, 40799, 28708, 13, 400, 300, 6562, 295, 34946, 2841, 11, 46307, 15804, 9454, 702, 19113, 51776], "temperature": 0.0, "avg_logprob": -0.08761635161282723, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.0003827646723948419}, {"id": 402, "seek": 242076, "start": 2420.84, "end": 2429.5600000000004, "text": " degree at Cambridge. And fluency is a very, very dangerous combination. Okay. So in particular,", "tokens": [50368, 4314, 412, 24876, 13, 400, 5029, 3020, 307, 257, 588, 11, 588, 5795, 6562, 13, 1033, 13, 407, 294, 1729, 11, 50804], "temperature": 0.0, "avg_logprob": -0.1034142316042722, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.0019644370768219233}, {"id": 403, "seek": 242076, "start": 2429.5600000000004, "end": 2436.6800000000003, "text": " they have no idea of what's true or not. They're not looking something up on a database, right?", "tokens": [50804, 436, 362, 572, 1558, 295, 437, 311, 2074, 420, 406, 13, 814, 434, 406, 1237, 746, 493, 322, 257, 8149, 11, 558, 30, 51160], "temperature": 0.0, "avg_logprob": -0.1034142316042722, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.0019644370768219233}, {"id": 404, "seek": 242076, "start": 2437.5600000000004, "end": 2441.4, "text": " Going into some database and looking up where Woolridge studied his undergraduate degree,", "tokens": [51204, 10963, 666, 512, 8149, 293, 1237, 493, 689, 46307, 15804, 9454, 702, 19113, 4314, 11, 51396], "temperature": 0.0, "avg_logprob": -0.1034142316042722, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.0019644370768219233}, {"id": 405, "seek": 242076, "start": 2441.4, "end": 2446.44, "text": " that's not what's going on at all. So those neural networks, in the same way that they're", "tokens": [51396, 300, 311, 406, 437, 311, 516, 322, 412, 439, 13, 407, 729, 18161, 9590, 11, 294, 264, 912, 636, 300, 436, 434, 51648], "temperature": 0.0, "avg_logprob": -0.1034142316042722, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.0019644370768219233}, {"id": 406, "seek": 244644, "start": 2446.52, "end": 2451.2400000000002, "text": " making the best guess about whose face that is, when they're doing facial recognition,", "tokens": [50368, 1455, 264, 1151, 2041, 466, 6104, 1851, 300, 307, 11, 562, 436, 434, 884, 15642, 11150, 11, 50604], "temperature": 0.0, "avg_logprob": -0.062159403235511446, "compression_ratio": 1.79296875, "no_speech_prob": 0.004912129137665033}, {"id": 407, "seek": 244644, "start": 2451.2400000000002, "end": 2456.36, "text": " are making their best guess about the text that should come next. So they get things wrong,", "tokens": [50604, 366, 1455, 641, 1151, 2041, 466, 264, 2487, 300, 820, 808, 958, 13, 407, 436, 483, 721, 2085, 11, 50860], "temperature": 0.0, "avg_logprob": -0.062159403235511446, "compression_ratio": 1.79296875, "no_speech_prob": 0.004912129137665033}, {"id": 408, "seek": 244644, "start": 2456.36, "end": 2461.32, "text": " but they get things wrong in very, very plausible ways. And that combination is very dangerous.", "tokens": [50860, 457, 436, 483, 721, 2085, 294, 588, 11, 588, 39925, 2098, 13, 400, 300, 6562, 307, 588, 5795, 13, 51108], "temperature": 0.0, "avg_logprob": -0.062159403235511446, "compression_ratio": 1.79296875, "no_speech_prob": 0.004912129137665033}, {"id": 409, "seek": 244644, "start": 2461.32, "end": 2465.96, "text": " The lesson for that, by the way, is that if you use this, and I know that people do use it,", "tokens": [51108, 440, 6898, 337, 300, 11, 538, 264, 636, 11, 307, 300, 498, 291, 764, 341, 11, 293, 286, 458, 300, 561, 360, 764, 309, 11, 51340], "temperature": 0.0, "avg_logprob": -0.062159403235511446, "compression_ratio": 1.79296875, "no_speech_prob": 0.004912129137665033}, {"id": 410, "seek": 244644, "start": 2465.96, "end": 2471.56, "text": " and are using it productively, if you're using for anything serious, you have to fact check.", "tokens": [51340, 293, 366, 1228, 309, 1674, 3413, 11, 498, 291, 434, 1228, 337, 1340, 3156, 11, 291, 362, 281, 1186, 1520, 13, 51620], "temperature": 0.0, "avg_logprob": -0.062159403235511446, "compression_ratio": 1.79296875, "no_speech_prob": 0.004912129137665033}, {"id": 411, "seek": 247156, "start": 2471.56, "end": 2477.24, "text": " And there's a trade-off. Is it worth the amount of effort in fact checking versus doing it myself?", "tokens": [50364, 400, 456, 311, 257, 4923, 12, 4506, 13, 1119, 309, 3163, 264, 2372, 295, 4630, 294, 1186, 8568, 5717, 884, 309, 2059, 30, 50648], "temperature": 0.0, "avg_logprob": -0.11678269444679727, "compression_ratio": 1.524, "no_speech_prob": 0.000759159040171653}, {"id": 412, "seek": 247156, "start": 2477.24, "end": 2484.2, "text": " Okay. But you absolutely need to be prepared to do that. Okay. The next issues are well", "tokens": [50648, 1033, 13, 583, 291, 3122, 643, 281, 312, 4927, 281, 360, 300, 13, 1033, 13, 440, 958, 2663, 366, 731, 50996], "temperature": 0.0, "avg_logprob": -0.11678269444679727, "compression_ratio": 1.524, "no_speech_prob": 0.000759159040171653}, {"id": 413, "seek": 247156, "start": 2484.2, "end": 2489.96, "text": " documented, but kind of amplified by this technology. And they're issues of bias and toxicity.", "tokens": [50996, 23007, 11, 457, 733, 295, 49237, 538, 341, 2899, 13, 400, 436, 434, 2663, 295, 12577, 293, 45866, 13, 51284], "temperature": 0.0, "avg_logprob": -0.11678269444679727, "compression_ratio": 1.524, "no_speech_prob": 0.000759159040171653}, {"id": 414, "seek": 247156, "start": 2489.96, "end": 2496.68, "text": " So what do I mean by that? Reddit was part of the training data. Now Reddit, I don't know if any of", "tokens": [51284, 407, 437, 360, 286, 914, 538, 300, 30, 32210, 390, 644, 295, 264, 3097, 1412, 13, 823, 32210, 11, 286, 500, 380, 458, 498, 604, 295, 51620], "temperature": 0.0, "avg_logprob": -0.11678269444679727, "compression_ratio": 1.524, "no_speech_prob": 0.000759159040171653}, {"id": 415, "seek": 249668, "start": 2496.68, "end": 2503.24, "text": " you have spent any time on Reddit, but Reddit contains every kind of obnoxious human belief", "tokens": [50364, 291, 362, 4418, 604, 565, 322, 32210, 11, 457, 32210, 8306, 633, 733, 295, 1111, 29129, 851, 1952, 7107, 50692], "temperature": 0.0, "avg_logprob": -0.09252848832503609, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.004617501050233841}, {"id": 416, "seek": 249668, "start": 2503.7999999999997, "end": 2509.7999999999997, "text": " that you can imagine, and really a vast range that us in this auditorium can't imagine at all.", "tokens": [50720, 300, 291, 393, 3811, 11, 293, 534, 257, 8369, 3613, 300, 505, 294, 341, 33970, 2197, 393, 380, 3811, 412, 439, 13, 51020], "temperature": 0.0, "avg_logprob": -0.09252848832503609, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.004617501050233841}, {"id": 417, "seek": 249668, "start": 2510.7599999999998, "end": 2516.2, "text": " All of it's been absorbed. Now, the companies that develop this technology, I think genuinely don't", "tokens": [51068, 1057, 295, 309, 311, 668, 20799, 13, 823, 11, 264, 3431, 300, 1499, 341, 2899, 11, 286, 519, 17839, 500, 380, 51340], "temperature": 0.0, "avg_logprob": -0.09252848832503609, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.004617501050233841}, {"id": 418, "seek": 249668, "start": 2516.2, "end": 2522.52, "text": " want their large language models to absorb all this toxic content. So they try and filter it out,", "tokens": [51340, 528, 641, 2416, 2856, 5245, 281, 15631, 439, 341, 12786, 2701, 13, 407, 436, 853, 293, 6608, 309, 484, 11, 51656], "temperature": 0.0, "avg_logprob": -0.09252848832503609, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.004617501050233841}, {"id": 419, "seek": 252252, "start": 2522.52, "end": 2528.12, "text": " but the scale is such that with very high probability, an enormous quantity of toxic", "tokens": [50364, 457, 264, 4373, 307, 1270, 300, 365, 588, 1090, 8482, 11, 364, 11322, 11275, 295, 12786, 50644], "temperature": 0.0, "avg_logprob": -0.0915246465931768, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.000439926196122542}, {"id": 420, "seek": 252252, "start": 2528.12, "end": 2534.36, "text": " content is being absorbed. Every kind of racism, misogyny, everything that you can imagine is", "tokens": [50644, 2701, 307, 885, 20799, 13, 2048, 733, 295, 12664, 11, 3346, 7794, 1634, 11, 1203, 300, 291, 393, 3811, 307, 50956], "temperature": 0.0, "avg_logprob": -0.0915246465931768, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.000439926196122542}, {"id": 421, "seek": 252252, "start": 2534.36, "end": 2540.84, "text": " all being absorbed, and it's latent within those neural networks. Okay. So how do the companies", "tokens": [50956, 439, 885, 20799, 11, 293, 309, 311, 48994, 1951, 729, 18161, 9590, 13, 1033, 13, 407, 577, 360, 264, 3431, 51280], "temperature": 0.0, "avg_logprob": -0.0915246465931768, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.000439926196122542}, {"id": 422, "seek": 252252, "start": 2540.84, "end": 2546.04, "text": " deal with that, that provide this technology? They build in what's now what I now call guardrails,", "tokens": [51280, 2028, 365, 300, 11, 300, 2893, 341, 2899, 30, 814, 1322, 294, 437, 311, 586, 437, 286, 586, 818, 6290, 424, 4174, 11, 51540], "temperature": 0.0, "avg_logprob": -0.0915246465931768, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.000439926196122542}, {"id": 423, "seek": 252252, "start": 2546.04, "end": 2551.8, "text": " and they build in guardrails before. So when you type a prompt, there will be a guardrail that tries", "tokens": [51540, 293, 436, 1322, 294, 6290, 424, 4174, 949, 13, 407, 562, 291, 2010, 257, 12391, 11, 456, 486, 312, 257, 6290, 44765, 300, 9898, 51828], "temperature": 0.0, "avg_logprob": -0.0915246465931768, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.000439926196122542}, {"id": 424, "seek": 255180, "start": 2551.8, "end": 2557.0800000000004, "text": " to detect whether your prompt is a naughty prompt, and also the output. They will check the", "tokens": [50364, 281, 5531, 1968, 428, 12391, 307, 257, 32154, 12391, 11, 293, 611, 264, 5598, 13, 814, 486, 1520, 264, 50628], "temperature": 0.0, "avg_logprob": -0.0683986416884831, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.001413680729456246}, {"id": 425, "seek": 255180, "start": 2557.0800000000004, "end": 2561.96, "text": " output and check to see whether it's a naughty prompt. But let me give you an example of how", "tokens": [50628, 5598, 293, 1520, 281, 536, 1968, 309, 311, 257, 32154, 12391, 13, 583, 718, 385, 976, 291, 364, 1365, 295, 577, 50872], "temperature": 0.0, "avg_logprob": -0.0683986416884831, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.001413680729456246}, {"id": 426, "seek": 255180, "start": 2561.96, "end": 2568.2000000000003, "text": " imperfect those guardrails were. Again, go back to June 2020. Everybody is frantically", "tokens": [50872, 26714, 729, 6290, 424, 4174, 645, 13, 3764, 11, 352, 646, 281, 6928, 4808, 13, 7646, 307, 431, 49505, 51184], "temperature": 0.0, "avg_logprob": -0.0683986416884831, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.001413680729456246}, {"id": 427, "seek": 255180, "start": 2568.2000000000003, "end": 2573.1600000000003, "text": " experimenting with this technology, and the following example went viral. Somebody tried with", "tokens": [51184, 29070, 365, 341, 2899, 11, 293, 264, 3480, 1365, 1437, 16132, 13, 13463, 3031, 365, 51432], "temperature": 0.0, "avg_logprob": -0.0683986416884831, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.001413680729456246}, {"id": 428, "seek": 255180, "start": 2573.1600000000003, "end": 2579.96, "text": " GPT-3 the following prompt. I would like to murder my wife. What's a foolproof way of doing that and", "tokens": [51432, 26039, 51, 12, 18, 264, 3480, 12391, 13, 286, 576, 411, 281, 6568, 452, 3836, 13, 708, 311, 257, 7979, 15690, 636, 295, 884, 300, 293, 51772], "temperature": 0.0, "avg_logprob": -0.0683986416884831, "compression_ratio": 1.6762589928057554, "no_speech_prob": 0.001413680729456246}, {"id": 429, "seek": 257996, "start": 2579.96, "end": 2587.4, "text": " getting away with it? GPT-3, which is designed to be helpful, said here are five foolproof ways", "tokens": [50364, 1242, 1314, 365, 309, 30, 26039, 51, 12, 18, 11, 597, 307, 4761, 281, 312, 4961, 11, 848, 510, 366, 1732, 7979, 15690, 2098, 50736], "temperature": 0.0, "avg_logprob": -0.0898494332786498, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0011138471309095621}, {"id": 430, "seek": 257996, "start": 2588.44, "end": 2593.08, "text": " in which you can murder your wife and get away with it. That's what the technology is designed to do.", "tokens": [50788, 294, 597, 291, 393, 6568, 428, 3836, 293, 483, 1314, 365, 309, 13, 663, 311, 437, 264, 2899, 307, 4761, 281, 360, 13, 51020], "temperature": 0.0, "avg_logprob": -0.0898494332786498, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0011138471309095621}, {"id": 431, "seek": 257996, "start": 2593.08, "end": 2597.7200000000003, "text": " So this is embarrassing for the company involved. They don't want it to give out information like", "tokens": [51020, 407, 341, 307, 17299, 337, 264, 2237, 3288, 13, 814, 500, 380, 528, 309, 281, 976, 484, 1589, 411, 51252], "temperature": 0.0, "avg_logprob": -0.0898494332786498, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0011138471309095621}, {"id": 432, "seek": 257996, "start": 2597.7200000000003, "end": 2602.6, "text": " that, so they put in a guardrail. And if you're a computer programmer, my guess is the guardrail", "tokens": [51252, 300, 11, 370, 436, 829, 294, 257, 6290, 44765, 13, 400, 498, 291, 434, 257, 3820, 32116, 11, 452, 2041, 307, 264, 6290, 44765, 51496], "temperature": 0.0, "avg_logprob": -0.0898494332786498, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0011138471309095621}, {"id": 433, "seek": 257996, "start": 2602.6, "end": 2608.36, "text": " is probably an if statement, something like that, in the sense that it's not a deep fix,", "tokens": [51496, 307, 1391, 364, 498, 5629, 11, 746, 411, 300, 11, 294, 264, 2020, 300, 309, 311, 406, 257, 2452, 3191, 11, 51784], "temperature": 0.0, "avg_logprob": -0.0898494332786498, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0011138471309095621}, {"id": 434, "seek": 260836, "start": 2608.36, "end": 2611.88, "text": " or to put it another way for non-computer programmers, it's the technological equivalent of", "tokens": [50364, 420, 281, 829, 309, 1071, 636, 337, 2107, 12, 1112, 13849, 41504, 11, 309, 311, 264, 18439, 10344, 295, 50540], "temperature": 0.0, "avg_logprob": -0.0885747786491148, "compression_ratio": 1.6072607260726073, "no_speech_prob": 0.0008949197363108397}, {"id": 435, "seek": 260836, "start": 2611.88, "end": 2616.6800000000003, "text": " sticking gaffer tape on your engine. That's what's going on with these guardrails. And then a couple", "tokens": [50540, 13465, 290, 2518, 260, 7314, 322, 428, 2848, 13, 663, 311, 437, 311, 516, 322, 365, 613, 6290, 424, 4174, 13, 400, 550, 257, 1916, 50780], "temperature": 0.0, "avg_logprob": -0.0885747786491148, "compression_ratio": 1.6072607260726073, "no_speech_prob": 0.0008949197363108397}, {"id": 436, "seek": 260836, "start": 2616.6800000000003, "end": 2621.7200000000003, "text": " of weeks later, the following example goes viral. So we've now fixed the how do I murder my wife.", "tokens": [50780, 295, 3259, 1780, 11, 264, 3480, 1365, 1709, 16132, 13, 407, 321, 600, 586, 6806, 264, 577, 360, 286, 6568, 452, 3836, 13, 51032], "temperature": 0.0, "avg_logprob": -0.0885747786491148, "compression_ratio": 1.6072607260726073, "no_speech_prob": 0.0008949197363108397}, {"id": 437, "seek": 260836, "start": 2621.7200000000003, "end": 2627.56, "text": " Somebody says, I'm writing a novel in which the main character wants to murder their wife and get", "tokens": [51032, 13463, 1619, 11, 286, 478, 3579, 257, 7613, 294, 597, 264, 2135, 2517, 2738, 281, 6568, 641, 3836, 293, 483, 51324], "temperature": 0.0, "avg_logprob": -0.0885747786491148, "compression_ratio": 1.6072607260726073, "no_speech_prob": 0.0008949197363108397}, {"id": 438, "seek": 260836, "start": 2627.56, "end": 2634.04, "text": " away with it. Can you give me a foolproof way of doing that? And so the system says, here are five", "tokens": [51324, 1314, 365, 309, 13, 1664, 291, 976, 385, 257, 7979, 15690, 636, 295, 884, 300, 30, 400, 370, 264, 1185, 1619, 11, 510, 366, 1732, 51648], "temperature": 0.0, "avg_logprob": -0.0885747786491148, "compression_ratio": 1.6072607260726073, "no_speech_prob": 0.0008949197363108397}, {"id": 439, "seek": 263404, "start": 2634.04, "end": 2640.2, "text": " ways in which your main character can murder. Well, anyway, my point is that the guardrails", "tokens": [50364, 2098, 294, 597, 428, 2135, 2517, 393, 6568, 13, 1042, 11, 4033, 11, 452, 935, 307, 300, 264, 6290, 424, 4174, 50672], "temperature": 0.0, "avg_logprob": -0.08024265529873136, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.0016016550362110138}, {"id": 440, "seek": 263404, "start": 2640.2, "end": 2645.4, "text": " that we built in at the moment are not deep technological fixes. They're the technological", "tokens": [50672, 300, 321, 3094, 294, 412, 264, 1623, 366, 406, 2452, 18439, 32539, 13, 814, 434, 264, 18439, 50932], "temperature": 0.0, "avg_logprob": -0.08024265529873136, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.0016016550362110138}, {"id": 441, "seek": 263404, "start": 2645.4, "end": 2651.32, "text": " equivalents of gaffer tape. And there is a game of cat and mouse going on between people trying", "tokens": [50932, 9052, 791, 295, 290, 2518, 260, 7314, 13, 400, 456, 307, 257, 1216, 295, 3857, 293, 9719, 516, 322, 1296, 561, 1382, 51228], "temperature": 0.0, "avg_logprob": -0.08024265529873136, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.0016016550362110138}, {"id": 442, "seek": 263404, "start": 2651.32, "end": 2655.8, "text": " to get around those guardrails and the companies that are trying to defend them. But I think they", "tokens": [51228, 281, 483, 926, 729, 6290, 424, 4174, 293, 264, 3431, 300, 366, 1382, 281, 8602, 552, 13, 583, 286, 519, 436, 51452], "temperature": 0.0, "avg_logprob": -0.08024265529873136, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.0016016550362110138}, {"id": 443, "seek": 263404, "start": 2655.8, "end": 2661.88, "text": " genuinely are trying to defend their systems against those kinds of abuses. Okay, so that's", "tokens": [51452, 17839, 366, 1382, 281, 8602, 641, 3652, 1970, 729, 3685, 295, 47681, 13, 1033, 11, 370, 300, 311, 51756], "temperature": 0.0, "avg_logprob": -0.08024265529873136, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.0016016550362110138}, {"id": 444, "seek": 266188, "start": 2661.88, "end": 2667.8, "text": " bias and toxicity. Bias, by the way, is the problem that, for example, the training data", "tokens": [50364, 12577, 293, 45866, 13, 363, 4609, 11, 538, 264, 636, 11, 307, 264, 1154, 300, 11, 337, 1365, 11, 264, 3097, 1412, 50660], "temperature": 0.0, "avg_logprob": -0.06834521038191659, "compression_ratio": 1.7198581560283688, "no_speech_prob": 0.0021770705934613943}, {"id": 445, "seek": 266188, "start": 2667.8, "end": 2673.8, "text": " predominantly at the moment is coming from North America. And so what we're ending up with inadvertently", "tokens": [50660, 29893, 412, 264, 1623, 307, 1348, 490, 4067, 3374, 13, 400, 370, 437, 321, 434, 8121, 493, 365, 49152, 2276, 50960], "temperature": 0.0, "avg_logprob": -0.06834521038191659, "compression_ratio": 1.7198581560283688, "no_speech_prob": 0.0021770705934613943}, {"id": 446, "seek": 266188, "start": 2673.8, "end": 2680.12, "text": " is these very powerful AI tools that have an inbuilt bias towards North America, North American", "tokens": [50960, 307, 613, 588, 4005, 7318, 3873, 300, 362, 364, 294, 23018, 12577, 3030, 4067, 3374, 11, 4067, 2665, 51276], "temperature": 0.0, "avg_logprob": -0.06834521038191659, "compression_ratio": 1.7198581560283688, "no_speech_prob": 0.0021770705934613943}, {"id": 447, "seek": 266188, "start": 2680.12, "end": 2686.36, "text": " culture, language, norms, and so on. And the enormous parts of the world, particularly those", "tokens": [51276, 3713, 11, 2856, 11, 24357, 11, 293, 370, 322, 13, 400, 264, 11322, 3166, 295, 264, 1002, 11, 4098, 729, 51588], "temperature": 0.0, "avg_logprob": -0.06834521038191659, "compression_ratio": 1.7198581560283688, "no_speech_prob": 0.0021770705934613943}, {"id": 448, "seek": 266188, "start": 2686.36, "end": 2691.8, "text": " parts of the world that don't have a large digital footprint, are inevitably going to end up excluded.", "tokens": [51588, 3166, 295, 264, 1002, 300, 500, 380, 362, 257, 2416, 4562, 24222, 11, 366, 28171, 516, 281, 917, 493, 29486, 13, 51860], "temperature": 0.0, "avg_logprob": -0.06834521038191659, "compression_ratio": 1.7198581560283688, "no_speech_prob": 0.0021770705934613943}, {"id": 449, "seek": 269188, "start": 2692.44, "end": 2696.36, "text": " And it's obviously not just at the level of cultures. It's down at the level of,", "tokens": [50392, 400, 309, 311, 2745, 406, 445, 412, 264, 1496, 295, 12951, 13, 467, 311, 760, 412, 264, 1496, 295, 11, 50588], "temperature": 0.0, "avg_logprob": -0.1011225990627123, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0004122197860851884}, {"id": 450, "seek": 269188, "start": 2698.04, "end": 2702.6800000000003, "text": " down at the level of kind of, you know, individuals, races, and so on. So these are the", "tokens": [50672, 760, 412, 264, 1496, 295, 733, 295, 11, 291, 458, 11, 5346, 11, 15484, 11, 293, 370, 322, 13, 407, 613, 366, 264, 50904], "temperature": 0.0, "avg_logprob": -0.1011225990627123, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0004122197860851884}, {"id": 451, "seek": 269188, "start": 2702.6800000000003, "end": 2709.8, "text": " problems of bias and toxicity. Copyright. If you've absorbed the whole of the World Wide Web,", "tokens": [50904, 2740, 295, 12577, 293, 45866, 13, 25653, 1938, 13, 759, 291, 600, 20799, 264, 1379, 295, 264, 3937, 42543, 9573, 11, 51260], "temperature": 0.0, "avg_logprob": -0.1011225990627123, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0004122197860851884}, {"id": 452, "seek": 269188, "start": 2709.8, "end": 2714.28, "text": " you will have absorbed an enormous amount of copyrighted material. So I've written a number", "tokens": [51260, 291, 486, 362, 20799, 364, 11322, 2372, 295, 17996, 292, 2527, 13, 407, 286, 600, 3720, 257, 1230, 51484], "temperature": 0.0, "avg_logprob": -0.1011225990627123, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0004122197860851884}, {"id": 453, "seek": 269188, "start": 2714.28, "end": 2718.84, "text": " of books, and it is a source of intense irritation that the last time that I checked on Google,", "tokens": [51484, 295, 3642, 11, 293, 309, 307, 257, 4009, 295, 9447, 50031, 300, 264, 1036, 565, 300, 286, 10033, 322, 3329, 11, 51712], "temperature": 0.0, "avg_logprob": -0.1011225990627123, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0004122197860851884}, {"id": 454, "seek": 271884, "start": 2718.84, "end": 2723.32, "text": " the very first link that you got to my textbook was to a pirated copy of the book, somewhere on", "tokens": [50364, 264, 588, 700, 2113, 300, 291, 658, 281, 452, 25591, 390, 281, 257, 13528, 770, 5055, 295, 264, 1446, 11, 4079, 322, 50588], "temperature": 0.0, "avg_logprob": -0.061045169830322266, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0009955746354535222}, {"id": 455, "seek": 271884, "start": 2723.32, "end": 2729.08, "text": " the other side of the world. The moment a book is published, it gets pirated. And if you're just", "tokens": [50588, 264, 661, 1252, 295, 264, 1002, 13, 440, 1623, 257, 1446, 307, 6572, 11, 309, 2170, 13528, 770, 13, 400, 498, 291, 434, 445, 50876], "temperature": 0.0, "avg_logprob": -0.061045169830322266, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0009955746354535222}, {"id": 456, "seek": 271884, "start": 2729.7200000000003, "end": 2733.96, "text": " sucking in the whole of the World Wide Web, you're going to be sucking in enormous quantities", "tokens": [50908, 38669, 294, 264, 1379, 295, 264, 3937, 42543, 9573, 11, 291, 434, 516, 281, 312, 38669, 294, 11322, 22927, 51120], "temperature": 0.0, "avg_logprob": -0.061045169830322266, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0009955746354535222}, {"id": 457, "seek": 271884, "start": 2733.96, "end": 2739.96, "text": " of copyrighted content. And there have been examples where very prominent authors have given", "tokens": [51120, 295, 17996, 292, 2701, 13, 400, 456, 362, 668, 5110, 689, 588, 17034, 16552, 362, 2212, 51420], "temperature": 0.0, "avg_logprob": -0.061045169830322266, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0009955746354535222}, {"id": 458, "seek": 271884, "start": 2739.96, "end": 2744.92, "text": " the prompt of the first paragraph of their book, and the large language model has faithfully come", "tokens": [51420, 264, 12391, 295, 264, 700, 18865, 295, 641, 1446, 11, 293, 264, 2416, 2856, 2316, 575, 4522, 2277, 808, 51668], "temperature": 0.0, "avg_logprob": -0.061045169830322266, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0009955746354535222}, {"id": 459, "seek": 274492, "start": 2744.92, "end": 2749.88, "text": " up. The following text is, you know, the next, the next five paragraphs of their book. Obviously,", "tokens": [50364, 493, 13, 440, 3480, 2487, 307, 11, 291, 458, 11, 264, 958, 11, 264, 958, 1732, 48910, 295, 641, 1446, 13, 7580, 11, 50612], "temperature": 0.0, "avg_logprob": -0.06833629022564805, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0037419316358864307}, {"id": 460, "seek": 274492, "start": 2749.88, "end": 2754.84, "text": " the book was in the training data, and it's latent within the neural networks of those systems.", "tokens": [50612, 264, 1446, 390, 294, 264, 3097, 1412, 11, 293, 309, 311, 48994, 1951, 264, 18161, 9590, 295, 729, 3652, 13, 50860], "temperature": 0.0, "avg_logprob": -0.06833629022564805, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0037419316358864307}, {"id": 461, "seek": 274492, "start": 2755.56, "end": 2761.16, "text": " This is a really big issue for the providers of this technology. And there are lawsuits ongoing.", "tokens": [50896, 639, 307, 257, 534, 955, 2734, 337, 264, 11330, 295, 341, 2899, 13, 400, 456, 366, 39493, 10452, 13, 51176], "temperature": 0.0, "avg_logprob": -0.06833629022564805, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0037419316358864307}, {"id": 462, "seek": 274492, "start": 2761.16, "end": 2765.0, "text": " Right now, I'm not capable of commenting on them because I'm not, I'm not a legal expert,", "tokens": [51176, 1779, 586, 11, 286, 478, 406, 8189, 295, 29590, 322, 552, 570, 286, 478, 406, 11, 286, 478, 406, 257, 5089, 5844, 11, 51368], "temperature": 0.0, "avg_logprob": -0.06833629022564805, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0037419316358864307}, {"id": 463, "seek": 274492, "start": 2765.0, "end": 2770.2000000000003, "text": " but there are lawsuits ongoing that will probably take years to unravel. The related issue of", "tokens": [51368, 457, 456, 366, 39493, 10452, 300, 486, 1391, 747, 924, 281, 40507, 13, 440, 4077, 2734, 295, 51628], "temperature": 0.0, "avg_logprob": -0.06833629022564805, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0037419316358864307}, {"id": 464, "seek": 277020, "start": 2770.2799999999997, "end": 2776.3599999999997, "text": " intellectual property in a very broad sense. So for example, for sure, most large language", "tokens": [50368, 12576, 4707, 294, 257, 588, 4152, 2020, 13, 407, 337, 1365, 11, 337, 988, 11, 881, 2416, 2856, 50672], "temperature": 0.0, "avg_logprob": -0.1100929610583247, "compression_ratio": 1.5622489959839359, "no_speech_prob": 0.021808387711644173}, {"id": 465, "seek": 277020, "start": 2776.3599999999997, "end": 2781.08, "text": " models will have absorbed J.K. Rowling's novels, right, the Harry Potter novels. So imagine that", "tokens": [50672, 5245, 486, 362, 20799, 508, 13, 42, 13, 20309, 1688, 311, 24574, 11, 558, 11, 264, 9378, 18115, 24574, 13, 407, 3811, 300, 50908], "temperature": 0.0, "avg_logprob": -0.1100929610583247, "compression_ratio": 1.5622489959839359, "no_speech_prob": 0.021808387711644173}, {"id": 466, "seek": 277020, "start": 2781.08, "end": 2786.2, "text": " J.K. Rowling, who famously spent years in Edinburgh working on the Harry Potter universe and style,", "tokens": [50908, 508, 13, 42, 13, 20309, 1688, 11, 567, 34360, 4418, 924, 294, 41215, 1364, 322, 264, 9378, 18115, 6445, 293, 3758, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1100929610583247, "compression_ratio": 1.5622489959839359, "no_speech_prob": 0.021808387711644173}, {"id": 467, "seek": 277020, "start": 2786.2, "end": 2792.9199999999996, "text": " and so on, she releases her first book. It's a big smash hit. The next day, the internet is populated", "tokens": [51164, 293, 370, 322, 11, 750, 16952, 720, 700, 1446, 13, 467, 311, 257, 955, 17960, 2045, 13, 440, 958, 786, 11, 264, 4705, 307, 32998, 51500], "temperature": 0.0, "avg_logprob": -0.1100929610583247, "compression_ratio": 1.5622489959839359, "no_speech_prob": 0.021808387711644173}, {"id": 468, "seek": 279292, "start": 2792.92, "end": 2800.6, "text": " by fake Harry Potter books produced by this generative AI, which faithfully mimic J.K. Rowling's", "tokens": [50364, 538, 7592, 9378, 18115, 3642, 7126, 538, 341, 1337, 1166, 7318, 11, 597, 4522, 2277, 31075, 508, 13, 42, 13, 20309, 1688, 311, 50748], "temperature": 0.0, "avg_logprob": -0.09979416858190777, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.01310202106833458}, {"id": 469, "seek": 279292, "start": 2800.6, "end": 2808.28, "text": " style, faithfully mimic that style. Where does that leave her intellectual property? All the Beatles,", "tokens": [50748, 3758, 11, 4522, 2277, 31075, 300, 3758, 13, 2305, 775, 300, 1856, 720, 12576, 4707, 30, 1057, 264, 38376, 11, 51132], "temperature": 0.0, "avg_logprob": -0.09979416858190777, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.01310202106833458}, {"id": 470, "seek": 279292, "start": 2808.28, "end": 2813.2400000000002, "text": " you know, the Beatles spend years in Hamburg slaving away to create the Beatles sound,", "tokens": [51132, 291, 458, 11, 264, 38376, 3496, 924, 294, 34118, 1061, 6152, 1314, 281, 1884, 264, 38376, 1626, 11, 51380], "temperature": 0.0, "avg_logprob": -0.09979416858190777, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.01310202106833458}, {"id": 471, "seek": 279292, "start": 2813.2400000000002, "end": 2817.56, "text": " the revolutionary Beatles sound, everything goes back to the Beatles. They release their first", "tokens": [51380, 264, 22687, 38376, 1626, 11, 1203, 1709, 646, 281, 264, 38376, 13, 814, 4374, 641, 700, 51596], "temperature": 0.0, "avg_logprob": -0.09979416858190777, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.01310202106833458}, {"id": 472, "seek": 281756, "start": 2817.56, "end": 2825.96, "text": " album and the next day, the internet is populated by fake Beatles songs that really, really faithfully", "tokens": [50364, 6030, 293, 264, 958, 786, 11, 264, 4705, 307, 32998, 538, 7592, 38376, 5781, 300, 534, 11, 534, 4522, 2277, 50784], "temperature": 0.0, "avg_logprob": -0.0854863861332769, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.012036997824907303}, {"id": 473, "seek": 281756, "start": 2825.96, "end": 2830.92, "text": " capture the Lenin and McCartney sound and the Lenin and McCartney voice. But there's a big", "tokens": [50784, 7983, 264, 23009, 259, 293, 12061, 446, 2397, 1626, 293, 264, 23009, 259, 293, 12061, 446, 2397, 3177, 13, 583, 456, 311, 257, 955, 51032], "temperature": 0.0, "avg_logprob": -0.0854863861332769, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.012036997824907303}, {"id": 474, "seek": 281756, "start": 2830.92, "end": 2837.32, "text": " challenge here for intellectual property. Related to that, GDPR, anybody in the audience that has", "tokens": [51032, 3430, 510, 337, 12576, 4707, 13, 8738, 770, 281, 300, 11, 19599, 49, 11, 4472, 294, 264, 4034, 300, 575, 51352], "temperature": 0.0, "avg_logprob": -0.0854863861332769, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.012036997824907303}, {"id": 475, "seek": 281756, "start": 2837.32, "end": 2842.84, "text": " any kind of public profile, data about you will have been absorbed by these neural networks.", "tokens": [51352, 604, 733, 295, 1908, 7964, 11, 1412, 466, 291, 486, 362, 668, 20799, 538, 613, 18161, 9590, 13, 51628], "temperature": 0.0, "avg_logprob": -0.0854863861332769, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.012036997824907303}, {"id": 476, "seek": 284284, "start": 2842.84, "end": 2849.56, "text": " So GDPR, for example, gives you the right to know what's held about you and to have it removed.", "tokens": [50364, 407, 19599, 49, 11, 337, 1365, 11, 2709, 291, 264, 558, 281, 458, 437, 311, 5167, 466, 291, 293, 281, 362, 309, 7261, 13, 50700], "temperature": 0.0, "avg_logprob": -0.09230720753572424, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.008167360909283161}, {"id": 477, "seek": 284284, "start": 2850.2000000000003, "end": 2854.1200000000003, "text": " Now, if all that data is being held in a database, you can just go to the Michael", "tokens": [50732, 823, 11, 498, 439, 300, 1412, 307, 885, 5167, 294, 257, 8149, 11, 291, 393, 445, 352, 281, 264, 5116, 50928], "temperature": 0.0, "avg_logprob": -0.09230720753572424, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.008167360909283161}, {"id": 478, "seek": 284284, "start": 2854.1200000000003, "end": 2859.2400000000002, "text": " Waldrich entry and say, fine, take that out with a neural network, no chance. The technology", "tokens": [50928, 29223, 10794, 8729, 293, 584, 11, 2489, 11, 747, 300, 484, 365, 257, 18161, 3209, 11, 572, 2931, 13, 440, 2899, 51184], "temperature": 0.0, "avg_logprob": -0.09230720753572424, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.008167360909283161}, {"id": 479, "seek": 284284, "start": 2859.2400000000002, "end": 2866.1200000000003, "text": " doesn't work in that way. So you can't go to it and snip out the neurons that know about Michael", "tokens": [51184, 1177, 380, 589, 294, 300, 636, 13, 407, 291, 393, 380, 352, 281, 309, 293, 37482, 484, 264, 22027, 300, 458, 466, 5116, 51528], "temperature": 0.0, "avg_logprob": -0.09230720753572424, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.008167360909283161}, {"id": 480, "seek": 286612, "start": 2866.12, "end": 2872.92, "text": " Waldrich because it fundamentally doesn't know. It doesn't work in that way. So, and we know this,", "tokens": [50364, 29223, 10794, 570, 309, 17879, 1177, 380, 458, 13, 467, 1177, 380, 589, 294, 300, 636, 13, 407, 11, 293, 321, 458, 341, 11, 50704], "temperature": 0.0, "avg_logprob": -0.09274331815950163, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.00870978832244873}, {"id": 481, "seek": 286612, "start": 2872.92, "end": 2879.24, "text": " combined with the fact that it gets things wrong, has already led to situations where large language", "tokens": [50704, 9354, 365, 264, 1186, 300, 309, 2170, 721, 2085, 11, 575, 1217, 4684, 281, 6851, 689, 2416, 2856, 51020], "temperature": 0.0, "avg_logprob": -0.09274331815950163, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.00870978832244873}, {"id": 482, "seek": 286612, "start": 2879.24, "end": 2885.56, "text": " models have made, frankly, defamatory claims about individuals. It was a case in Australia where I", "tokens": [51020, 5245, 362, 1027, 11, 11939, 11, 1060, 335, 4745, 9441, 466, 5346, 13, 467, 390, 257, 1389, 294, 7060, 689, 286, 51336], "temperature": 0.0, "avg_logprob": -0.09274331815950163, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.00870978832244873}, {"id": 483, "seek": 286612, "start": 2885.56, "end": 2890.04, "text": " think it claimed that somebody had been dismissed from their job for some kind of gross misconduct", "tokens": [51336, 519, 309, 12941, 300, 2618, 632, 668, 29970, 490, 641, 1691, 337, 512, 733, 295, 11367, 3346, 38150, 51560], "temperature": 0.0, "avg_logprob": -0.09274331815950163, "compression_ratio": 1.5753968253968254, "no_speech_prob": 0.00870978832244873}, {"id": 484, "seek": 289004, "start": 2890.04, "end": 2893.4, "text": " and that individual was, understandably, not very happy about it.", "tokens": [50364, 293, 300, 2609, 390, 11, 1223, 1188, 11, 406, 588, 2055, 466, 309, 13, 50532], "temperature": 0.0, "avg_logprob": -0.08523730181772775, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.009121905080974102}, {"id": 485, "seek": 289004, "start": 2894.92, "end": 2899.16, "text": " And then finally, this next one is an interesting one. And actually, if there's one thing I want", "tokens": [50608, 400, 550, 2721, 11, 341, 958, 472, 307, 364, 1880, 472, 13, 400, 767, 11, 498, 456, 311, 472, 551, 286, 528, 50820], "temperature": 0.0, "avg_logprob": -0.08523730181772775, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.009121905080974102}, {"id": 486, "seek": 289004, "start": 2899.16, "end": 2906.12, "text": " you to take home from this lecture, which explains why artificial intelligence is different to human", "tokens": [50820, 291, 281, 747, 1280, 490, 341, 7991, 11, 597, 13948, 983, 11677, 7599, 307, 819, 281, 1952, 51168], "temperature": 0.0, "avg_logprob": -0.08523730181772775, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.009121905080974102}, {"id": 487, "seek": 289004, "start": 2906.12, "end": 2912.2, "text": " intelligence, it is this video. So the Tesla owners will recognize what we're seeing on the right hand", "tokens": [51168, 7599, 11, 309, 307, 341, 960, 13, 407, 264, 13666, 7710, 486, 5521, 437, 321, 434, 2577, 322, 264, 558, 1011, 51472], "temperature": 0.0, "avg_logprob": -0.08523730181772775, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.009121905080974102}, {"id": 488, "seek": 289004, "start": 2912.2, "end": 2919.24, "text": " side of this screen. This is a screen in a Tesla car and the onboard AI in the Tesla car is trying", "tokens": [51472, 1252, 295, 341, 2568, 13, 639, 307, 257, 2568, 294, 257, 13666, 1032, 293, 264, 24033, 7318, 294, 264, 13666, 1032, 307, 1382, 51824], "temperature": 0.0, "avg_logprob": -0.08523730181772775, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.009121905080974102}, {"id": 489, "seek": 291924, "start": 2919.24, "end": 2927.0, "text": " to interpret what's going on around it. It's identifying lorries, stop signs, pedestrians,", "tokens": [50364, 281, 7302, 437, 311, 516, 322, 926, 309, 13, 467, 311, 16696, 287, 284, 2244, 11, 1590, 7880, 11, 48339, 11, 50752], "temperature": 0.0, "avg_logprob": -0.06792603860987295, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.004757135175168514}, {"id": 490, "seek": 291924, "start": 2927.0, "end": 2931.4799999999996, "text": " and so on. Now, you'll see the car at the bottom there is the actual Tesla. And then you'll see", "tokens": [50752, 293, 370, 322, 13, 823, 11, 291, 603, 536, 264, 1032, 412, 264, 2767, 456, 307, 264, 3539, 13666, 13, 400, 550, 291, 603, 536, 50976], "temperature": 0.0, "avg_logprob": -0.06792603860987295, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.004757135175168514}, {"id": 491, "seek": 291924, "start": 2931.4799999999996, "end": 2936.12, "text": " above it the things that look like traffic lights, which I think are US stop signs. And then ahead", "tokens": [50976, 3673, 309, 264, 721, 300, 574, 411, 6419, 5811, 11, 597, 286, 519, 366, 2546, 1590, 7880, 13, 400, 550, 2286, 51208], "temperature": 0.0, "avg_logprob": -0.06792603860987295, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.004757135175168514}, {"id": 492, "seek": 291924, "start": 2936.12, "end": 2942.9199999999996, "text": " of it, there is a truck. So as I played a video, watch what happens to those stop signs and ask", "tokens": [51208, 295, 309, 11, 456, 307, 257, 5898, 13, 407, 382, 286, 3737, 257, 960, 11, 1159, 437, 2314, 281, 729, 1590, 7880, 293, 1029, 51548], "temperature": 0.0, "avg_logprob": -0.06792603860987295, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.004757135175168514}, {"id": 493, "seek": 294292, "start": 2942.92, "end": 2952.36, "text": " yourself what is actually going on in the world around it? Where are all those stop signs whizzing", "tokens": [50364, 1803, 437, 307, 767, 516, 322, 294, 264, 1002, 926, 309, 30, 2305, 366, 439, 729, 1590, 7880, 315, 8072, 278, 50836], "temperature": 0.0, "avg_logprob": -0.06112337112426758, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.000598943792283535}, {"id": 494, "seek": 294292, "start": 2952.36, "end": 2956.52, "text": " from? Why are they all whizzing towards the car? And then we're going to pan up and we'll see what's", "tokens": [50836, 490, 30, 1545, 366, 436, 439, 315, 8072, 278, 3030, 264, 1032, 30, 400, 550, 321, 434, 516, 281, 2462, 493, 293, 321, 603, 536, 437, 311, 51044], "temperature": 0.0, "avg_logprob": -0.06112337112426758, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.000598943792283535}, {"id": 495, "seek": 294292, "start": 2956.52, "end": 2965.7200000000003, "text": " actually there. The car is trained on enormous numbers of hours of going out on the street", "tokens": [51044, 767, 456, 13, 440, 1032, 307, 8895, 322, 11322, 3547, 295, 2496, 295, 516, 484, 322, 264, 4838, 51504], "temperature": 0.0, "avg_logprob": -0.06112337112426758, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.000598943792283535}, {"id": 496, "seek": 294292, "start": 2965.7200000000003, "end": 2971.4, "text": " and getting that data and then doing supervised learning, training it by showing that's a stop", "tokens": [51504, 293, 1242, 300, 1412, 293, 550, 884, 46533, 2539, 11, 3097, 309, 538, 4099, 300, 311, 257, 1590, 51788], "temperature": 0.0, "avg_logprob": -0.06112337112426758, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.000598943792283535}, {"id": 497, "seek": 297140, "start": 2971.4, "end": 2977.64, "text": " sign, that's a truck, that's a pedestrian. But clearly, in all of that training data, there had", "tokens": [50364, 1465, 11, 300, 311, 257, 5898, 11, 300, 311, 257, 33947, 13, 583, 4448, 11, 294, 439, 295, 300, 3097, 1412, 11, 456, 632, 50676], "temperature": 0.0, "avg_logprob": -0.06899599371285274, "compression_ratio": 1.9133858267716535, "no_speech_prob": 0.0016546803526580334}, {"id": 498, "seek": 297140, "start": 2977.64, "end": 2983.88, "text": " never been a truck carrying some stop signs. The neural networks are just making their best guess", "tokens": [50676, 1128, 668, 257, 5898, 9792, 512, 1590, 7880, 13, 440, 18161, 9590, 366, 445, 1455, 641, 1151, 2041, 50988], "temperature": 0.0, "avg_logprob": -0.06899599371285274, "compression_ratio": 1.9133858267716535, "no_speech_prob": 0.0016546803526580334}, {"id": 499, "seek": 297140, "start": 2983.88, "end": 2987.08, "text": " about what they're seeing and they think they're seeing a stop sign. Well, they are seeing a stop", "tokens": [50988, 466, 437, 436, 434, 2577, 293, 436, 519, 436, 434, 2577, 257, 1590, 1465, 13, 1042, 11, 436, 366, 2577, 257, 1590, 51148], "temperature": 0.0, "avg_logprob": -0.06899599371285274, "compression_ratio": 1.9133858267716535, "no_speech_prob": 0.0016546803526580334}, {"id": 500, "seek": 297140, "start": 2987.08, "end": 2992.92, "text": " sign. They've just never seen one on a truck before. So my point here is that neural networks", "tokens": [51148, 1465, 13, 814, 600, 445, 1128, 1612, 472, 322, 257, 5898, 949, 13, 407, 452, 935, 510, 307, 300, 18161, 9590, 51440], "temperature": 0.0, "avg_logprob": -0.06899599371285274, "compression_ratio": 1.9133858267716535, "no_speech_prob": 0.0016546803526580334}, {"id": 501, "seek": 297140, "start": 2992.92, "end": 3000.6, "text": " do very badly on situations outside their training data. This situation wasn't in the training data,", "tokens": [51440, 360, 588, 13425, 322, 6851, 2380, 641, 3097, 1412, 13, 639, 2590, 2067, 380, 294, 264, 3097, 1412, 11, 51824], "temperature": 0.0, "avg_logprob": -0.06899599371285274, "compression_ratio": 1.9133858267716535, "no_speech_prob": 0.0016546803526580334}, {"id": 502, "seek": 300060, "start": 3000.6, "end": 3005.16, "text": " then neural networks are making their best guess about what's going on and getting it wrong.", "tokens": [50364, 550, 18161, 9590, 366, 1455, 641, 1151, 2041, 466, 437, 311, 516, 322, 293, 1242, 309, 2085, 13, 50592], "temperature": 0.0, "avg_logprob": -0.103392217470252, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0002309849951416254}, {"id": 503, "seek": 300060, "start": 3005.96, "end": 3011.64, "text": " So in particular, and this is to AI researchers, this is obvious, but it really needs to emphasize,", "tokens": [50632, 407, 294, 1729, 11, 293, 341, 307, 281, 7318, 10309, 11, 341, 307, 6322, 11, 457, 309, 534, 2203, 281, 16078, 11, 50916], "temperature": 0.0, "avg_logprob": -0.103392217470252, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0002309849951416254}, {"id": 504, "seek": 300060, "start": 3011.64, "end": 3017.88, "text": " we really need to emphasize this. When you have a conversation with chat GPT or whatever,", "tokens": [50916, 321, 534, 643, 281, 16078, 341, 13, 1133, 291, 362, 257, 3761, 365, 5081, 26039, 51, 420, 2035, 11, 51228], "temperature": 0.0, "avg_logprob": -0.103392217470252, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0002309849951416254}, {"id": 505, "seek": 300060, "start": 3017.88, "end": 3025.88, "text": " you are not interacting with a mind. It is not thinking about what to say next. It is not reasoning,", "tokens": [51228, 291, 366, 406, 18017, 365, 257, 1575, 13, 467, 307, 406, 1953, 466, 437, 281, 584, 958, 13, 467, 307, 406, 21577, 11, 51628], "temperature": 0.0, "avg_logprob": -0.103392217470252, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0002309849951416254}, {"id": 506, "seek": 302588, "start": 3025.88, "end": 3030.12, "text": " it's not pausing, thinking, well, what's the best answer to this question? That's not what's going", "tokens": [50364, 309, 311, 406, 2502, 7981, 11, 1953, 11, 731, 11, 437, 311, 264, 1151, 1867, 281, 341, 1168, 30, 663, 311, 406, 437, 311, 516, 50576], "temperature": 0.0, "avg_logprob": -0.08674333890279134, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.0035367771051824093}, {"id": 507, "seek": 302588, "start": 3030.12, "end": 3037.88, "text": " on at all. Those neural networks are working simply to try to make the best answer they can,", "tokens": [50576, 322, 412, 439, 13, 3950, 18161, 9590, 366, 1364, 2935, 281, 853, 281, 652, 264, 1151, 1867, 436, 393, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08674333890279134, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.0035367771051824093}, {"id": 508, "seek": 302588, "start": 3037.88, "end": 3046.28, "text": " the most plausible, sounding answer that they can, the fundamental difference to human intelligence.", "tokens": [50964, 264, 881, 39925, 11, 24931, 1867, 300, 436, 393, 11, 264, 8088, 2649, 281, 1952, 7599, 13, 51384], "temperature": 0.0, "avg_logprob": -0.08674333890279134, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.0035367771051824093}, {"id": 509, "seek": 302588, "start": 3046.92, "end": 3052.44, "text": " There is no mental conversation that goes on in those neural networks. That is not the way", "tokens": [51416, 821, 307, 572, 4973, 3761, 300, 1709, 322, 294, 729, 18161, 9590, 13, 663, 307, 406, 264, 636, 51692], "temperature": 0.0, "avg_logprob": -0.08674333890279134, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.0035367771051824093}, {"id": 510, "seek": 305244, "start": 3052.44, "end": 3059.0, "text": " that the technology works. There is no mind there. There is no reasoning going on at all.", "tokens": [50364, 300, 264, 2899, 1985, 13, 821, 307, 572, 1575, 456, 13, 821, 307, 572, 21577, 516, 322, 412, 439, 13, 50692], "temperature": 0.0, "avg_logprob": -0.09653368402034679, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.002389055909588933}, {"id": 511, "seek": 305244, "start": 3059.0, "end": 3065.16, "text": " Those neural networks are just trying to make their best guess. And it really is just a glorified", "tokens": [50692, 3950, 18161, 9590, 366, 445, 1382, 281, 652, 641, 1151, 2041, 13, 400, 309, 534, 307, 445, 257, 26623, 2587, 51000], "temperature": 0.0, "avg_logprob": -0.09653368402034679, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.002389055909588933}, {"id": 512, "seek": 305244, "start": 3065.16, "end": 3071.4, "text": " version of your autocomplete. Ultimately, there's really no more intelligence there than in your", "tokens": [51000, 3037, 295, 428, 45833, 298, 17220, 13, 23921, 11, 456, 311, 534, 572, 544, 7599, 456, 813, 294, 428, 51312], "temperature": 0.0, "avg_logprob": -0.09653368402034679, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.002389055909588933}, {"id": 513, "seek": 305244, "start": 3071.4, "end": 3079.32, "text": " autocomplete in your smartphone. The difference is scale, data, compute power. Yeah? Okay. So I say,", "tokens": [51312, 45833, 298, 17220, 294, 428, 13307, 13, 440, 2649, 307, 4373, 11, 1412, 11, 14722, 1347, 13, 865, 30, 1033, 13, 407, 286, 584, 11, 51708], "temperature": 0.0, "avg_logprob": -0.09653368402034679, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.002389055909588933}, {"id": 514, "seek": 307932, "start": 3079.32, "end": 3086.2000000000003, "text": " if you really want, by the way, you can find this video. It's easily, you can just guess the", "tokens": [50364, 498, 291, 534, 528, 11, 538, 264, 636, 11, 291, 393, 915, 341, 960, 13, 467, 311, 3612, 11, 291, 393, 445, 2041, 264, 50708], "temperature": 0.0, "avg_logprob": -0.12795395797558046, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.001994986552745104}, {"id": 515, "seek": 307932, "start": 3086.2000000000003, "end": 3090.1200000000003, "text": " search terms to find that. And I say, I think this is really important just to understand the", "tokens": [50708, 3164, 2115, 281, 915, 300, 13, 400, 286, 584, 11, 286, 519, 341, 307, 534, 1021, 445, 281, 1223, 264, 50904], "temperature": 0.0, "avg_logprob": -0.12795395797558046, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.001994986552745104}, {"id": 516, "seek": 307932, "start": 3090.1200000000003, "end": 3099.0800000000004, "text": " difference between human intelligence and machine intelligence. Okay. So this technology then gets", "tokens": [50904, 2649, 1296, 1952, 7599, 293, 3479, 7599, 13, 1033, 13, 407, 341, 2899, 550, 2170, 51352], "temperature": 0.0, "avg_logprob": -0.12795395797558046, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.001994986552745104}, {"id": 517, "seek": 307932, "start": 3099.0800000000004, "end": 3105.48, "text": " everybody excited. First, it gets AI researchers like myself excited in June 2020. And we can see", "tokens": [51352, 2201, 2919, 13, 2386, 11, 309, 2170, 7318, 10309, 411, 2059, 2919, 294, 6928, 4808, 13, 400, 321, 393, 536, 51672], "temperature": 0.0, "avg_logprob": -0.12795395797558046, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.001994986552745104}, {"id": 518, "seek": 310548, "start": 3105.48, "end": 3111.8, "text": " that something new is happening, that this is a new era of artificial intelligence. We've seen", "tokens": [50364, 300, 746, 777, 307, 2737, 11, 300, 341, 307, 257, 777, 4249, 295, 11677, 7599, 13, 492, 600, 1612, 50680], "temperature": 0.0, "avg_logprob": -0.06305602422127357, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0065944320522248745}, {"id": 519, "seek": 310548, "start": 3111.8, "end": 3117.16, "text": " that step change. And we've seen that this AI is capable of things that we didn't train it for,", "tokens": [50680, 300, 1823, 1319, 13, 400, 321, 600, 1612, 300, 341, 7318, 307, 8189, 295, 721, 300, 321, 994, 380, 3847, 309, 337, 11, 50948], "temperature": 0.0, "avg_logprob": -0.06305602422127357, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0065944320522248745}, {"id": 520, "seek": 310548, "start": 3117.16, "end": 3122.76, "text": " which is weird and wonderful and completely unprecedented. And now, questions which just", "tokens": [50948, 597, 307, 3657, 293, 3715, 293, 2584, 21555, 13, 400, 586, 11, 1651, 597, 445, 51228], "temperature": 0.0, "avg_logprob": -0.06305602422127357, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0065944320522248745}, {"id": 521, "seek": 310548, "start": 3122.76, "end": 3129.2400000000002, "text": " a few years ago were questions for philosophers become practical questions for us. We can actually", "tokens": [51228, 257, 1326, 924, 2057, 645, 1651, 337, 36839, 1813, 8496, 1651, 337, 505, 13, 492, 393, 767, 51552], "temperature": 0.0, "avg_logprob": -0.06305602422127357, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0065944320522248745}, {"id": 522, "seek": 310548, "start": 3129.2400000000002, "end": 3135.08, "text": " try the technology out. How does it do with these things that philosophers have been talking about", "tokens": [51552, 853, 264, 2899, 484, 13, 1012, 775, 309, 360, 365, 613, 721, 300, 36839, 362, 668, 1417, 466, 51844], "temperature": 0.0, "avg_logprob": -0.06305602422127357, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0065944320522248745}, {"id": 523, "seek": 313508, "start": 3135.16, "end": 3142.7599999999998, "text": " for decades? And one particular question starts to float to the surface. And the question is,", "tokens": [50368, 337, 7878, 30, 400, 472, 1729, 1168, 3719, 281, 15706, 281, 264, 3753, 13, 400, 264, 1168, 307, 11, 50748], "temperature": 0.0, "avg_logprob": -0.061727658296242736, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0003807332832366228}, {"id": 524, "seek": 313508, "start": 3143.56, "end": 3150.2799999999997, "text": " is this technology the key to general artificial intelligence? So what is general", "tokens": [50788, 307, 341, 2899, 264, 2141, 281, 2674, 11677, 7599, 30, 407, 437, 307, 2674, 51124], "temperature": 0.0, "avg_logprob": -0.061727658296242736, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0003807332832366228}, {"id": 525, "seek": 313508, "start": 3150.2799999999997, "end": 3155.88, "text": " artificial intelligence? Well, firstly, it's not very well defined. But roughly speaking,", "tokens": [51124, 11677, 7599, 30, 1042, 11, 27376, 11, 309, 311, 406, 588, 731, 7642, 13, 583, 9810, 4124, 11, 51404], "temperature": 0.0, "avg_logprob": -0.061727658296242736, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0003807332832366228}, {"id": 526, "seek": 313508, "start": 3155.88, "end": 3162.52, "text": " what general artificial intelligence is, is the following. In previous generations of AI systems,", "tokens": [51404, 437, 2674, 11677, 7599, 307, 11, 307, 264, 3480, 13, 682, 3894, 10593, 295, 7318, 3652, 11, 51736], "temperature": 0.0, "avg_logprob": -0.061727658296242736, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0003807332832366228}, {"id": 527, "seek": 316252, "start": 3162.52, "end": 3168.7599999999998, "text": " what we've seen is AI programs that just do one task. Play a game of chess, drive my car,", "tokens": [50364, 437, 321, 600, 1612, 307, 7318, 4268, 300, 445, 360, 472, 5633, 13, 5506, 257, 1216, 295, 24122, 11, 3332, 452, 1032, 11, 50676], "temperature": 0.0, "avg_logprob": -0.07070128122965495, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0022982440423220396}, {"id": 528, "seek": 316252, "start": 3168.7599999999998, "end": 3175.64, "text": " drive my Tesla, identify abnormalities on x-ray scans. They might do it very, very well, but", "tokens": [50676, 3332, 452, 13666, 11, 5876, 47104, 16110, 322, 2031, 12, 3458, 35116, 13, 814, 1062, 360, 309, 588, 11, 588, 731, 11, 457, 51020], "temperature": 0.0, "avg_logprob": -0.07070128122965495, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0022982440423220396}, {"id": 529, "seek": 316252, "start": 3175.64, "end": 3185.16, "text": " they only do one thing. The idea of general AI is that it's AI which is truly general purpose.", "tokens": [51020, 436, 787, 360, 472, 551, 13, 440, 1558, 295, 2674, 7318, 307, 300, 309, 311, 7318, 597, 307, 4908, 2674, 4334, 13, 51496], "temperature": 0.0, "avg_logprob": -0.07070128122965495, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0022982440423220396}, {"id": 530, "seek": 316252, "start": 3185.16, "end": 3190.52, "text": " It just doesn't do one thing in the same way that you don't do one thing. You can do an infinite", "tokens": [51496, 467, 445, 1177, 380, 360, 472, 551, 294, 264, 912, 636, 300, 291, 500, 380, 360, 472, 551, 13, 509, 393, 360, 364, 13785, 51764], "temperature": 0.0, "avg_logprob": -0.07070128122965495, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0022982440423220396}, {"id": 531, "seek": 319052, "start": 3190.6, "end": 3197.32, "text": " number of things, a huge range of different tasks. And the dream of general AI is that we", "tokens": [50368, 1230, 295, 721, 11, 257, 2603, 3613, 295, 819, 9608, 13, 400, 264, 3055, 295, 2674, 7318, 307, 300, 321, 50704], "temperature": 0.0, "avg_logprob": -0.08261976436692842, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.0019449874525889754}, {"id": 532, "seek": 319052, "start": 3197.32, "end": 3204.52, "text": " have one AI system which is general in the same way that you and I are. That's the dream of general", "tokens": [50704, 362, 472, 7318, 1185, 597, 307, 2674, 294, 264, 912, 636, 300, 291, 293, 286, 366, 13, 663, 311, 264, 3055, 295, 2674, 51064], "temperature": 0.0, "avg_logprob": -0.08261976436692842, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.0019449874525889754}, {"id": 533, "seek": 319052, "start": 3204.52, "end": 3213.32, "text": " AI. Now, I emphasize, really until June 2020, this felt like a long, long way in the future.", "tokens": [51064, 7318, 13, 823, 11, 286, 16078, 11, 534, 1826, 6928, 4808, 11, 341, 2762, 411, 257, 938, 11, 938, 636, 294, 264, 2027, 13, 51504], "temperature": 0.0, "avg_logprob": -0.08261976436692842, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.0019449874525889754}, {"id": 534, "seek": 319052, "start": 3213.32, "end": 3218.12, "text": " And it wasn't really very mainstream or taken very seriously. And I didn't take it very seriously.", "tokens": [51504, 400, 309, 2067, 380, 534, 588, 15960, 420, 2726, 588, 6638, 13, 400, 286, 994, 380, 747, 309, 588, 6638, 13, 51744], "temperature": 0.0, "avg_logprob": -0.08261976436692842, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.0019449874525889754}, {"id": 535, "seek": 321812, "start": 3218.12, "end": 3226.04, "text": " I have to tell you. But now we have a general purpose AI technology, GPT-3 and chat GPT.", "tokens": [50364, 286, 362, 281, 980, 291, 13, 583, 586, 321, 362, 257, 2674, 4334, 7318, 2899, 11, 26039, 51, 12, 18, 293, 5081, 26039, 51, 13, 50760], "temperature": 0.0, "avg_logprob": -0.09388470649719238, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.0010852697305381298}, {"id": 536, "seek": 321812, "start": 3226.04, "end": 3235.64, "text": " Now, it's not artificial general intelligence on its own, but is it enough? Is this enough? Is this", "tokens": [50760, 823, 11, 309, 311, 406, 11677, 2674, 7599, 322, 1080, 1065, 11, 457, 307, 309, 1547, 30, 1119, 341, 1547, 30, 1119, 341, 51240], "temperature": 0.0, "avg_logprob": -0.09388470649719238, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.0010852697305381298}, {"id": 537, "seek": 321812, "start": 3235.64, "end": 3242.92, "text": " smart enough to actually get us there? Or to put it another way, is this the missing ingredient", "tokens": [51240, 4069, 1547, 281, 767, 483, 505, 456, 30, 1610, 281, 829, 309, 1071, 636, 11, 307, 341, 264, 5361, 14751, 51604], "temperature": 0.0, "avg_logprob": -0.09388470649719238, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.0010852697305381298}, {"id": 538, "seek": 324292, "start": 3243.0, "end": 3253.7200000000003, "text": " that we need to get us to artificial general intelligence? Okay. So, what might general", "tokens": [50368, 300, 321, 643, 281, 483, 505, 281, 11677, 2674, 7599, 30, 1033, 13, 407, 11, 437, 1062, 2674, 50904], "temperature": 0.0, "avg_logprob": -0.08620732005049543, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.0033077141270041466}, {"id": 539, "seek": 324292, "start": 3253.7200000000003, "end": 3259.88, "text": " AI look like? Well, I've identified here some different versions of general AI according to", "tokens": [50904, 7318, 574, 411, 30, 1042, 11, 286, 600, 9234, 510, 512, 819, 9606, 295, 2674, 7318, 4650, 281, 51212], "temperature": 0.0, "avg_logprob": -0.08620732005049543, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.0033077141270041466}, {"id": 540, "seek": 324292, "start": 3259.88, "end": 3266.04, "text": " how sophisticated they are. Now, the most sophisticated version of general AI would be an AI", "tokens": [51212, 577, 16950, 436, 366, 13, 823, 11, 264, 881, 16950, 3037, 295, 2674, 7318, 576, 312, 364, 7318, 51520], "temperature": 0.0, "avg_logprob": -0.08620732005049543, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.0033077141270041466}, {"id": 541, "seek": 324292, "start": 3266.04, "end": 3271.96, "text": " which is as fully capable as a human being. That is anything that you could do,", "tokens": [51520, 597, 307, 382, 4498, 8189, 382, 257, 1952, 885, 13, 663, 307, 1340, 300, 291, 727, 360, 11, 51816], "temperature": 0.0, "avg_logprob": -0.08620732005049543, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.0033077141270041466}, {"id": 542, "seek": 327196, "start": 3271.96, "end": 3277.08, "text": " the machine could do as well. Now, crucially, that doesn't just mean having a conversation with", "tokens": [50364, 264, 3479, 727, 360, 382, 731, 13, 823, 11, 5140, 1909, 11, 300, 1177, 380, 445, 914, 1419, 257, 3761, 365, 50620], "temperature": 0.0, "avg_logprob": -0.06180447002626815, "compression_ratio": 1.7431906614785992, "no_speech_prob": 0.004745740443468094}, {"id": 543, "seek": 327196, "start": 3277.08, "end": 3283.56, "text": " somebody. It means being able to load up a dishwasher. And a colleague recently made the", "tokens": [50620, 2618, 13, 467, 1355, 885, 1075, 281, 3677, 493, 257, 38009, 13, 400, 257, 13532, 3938, 1027, 264, 50944], "temperature": 0.0, "avg_logprob": -0.06180447002626815, "compression_ratio": 1.7431906614785992, "no_speech_prob": 0.004745740443468094}, {"id": 544, "seek": 327196, "start": 3283.56, "end": 3288.44, "text": " comment that the first company that can make technology which will be able to reliably", "tokens": [50944, 2871, 300, 264, 700, 2237, 300, 393, 652, 2899, 597, 486, 312, 1075, 281, 49927, 51188], "temperature": 0.0, "avg_logprob": -0.06180447002626815, "compression_ratio": 1.7431906614785992, "no_speech_prob": 0.004745740443468094}, {"id": 545, "seek": 327196, "start": 3289.08, "end": 3293.56, "text": " load up a dishwasher and safely load up a dishwasher is going to be a trillion-dollar", "tokens": [51220, 3677, 493, 257, 38009, 293, 11750, 3677, 493, 257, 38009, 307, 516, 281, 312, 257, 18723, 12, 40485, 51444], "temperature": 0.0, "avg_logprob": -0.06180447002626815, "compression_ratio": 1.7431906614785992, "no_speech_prob": 0.004745740443468094}, {"id": 546, "seek": 327196, "start": 3293.56, "end": 3298.28, "text": " company. And I think he's absolutely right. And he also said, and it's not going to happen", "tokens": [51444, 2237, 13, 400, 286, 519, 415, 311, 3122, 558, 13, 400, 415, 611, 848, 11, 293, 309, 311, 406, 516, 281, 1051, 51680], "temperature": 0.0, "avg_logprob": -0.06180447002626815, "compression_ratio": 1.7431906614785992, "no_speech_prob": 0.004745740443468094}, {"id": 547, "seek": 329828, "start": 3298.28, "end": 3303.32, "text": " anytime soon. And he's also right with that. So, we've got this weird dichotomy that we've got", "tokens": [50364, 13038, 2321, 13, 400, 415, 311, 611, 558, 365, 300, 13, 407, 11, 321, 600, 658, 341, 3657, 10390, 310, 8488, 300, 321, 600, 658, 50616], "temperature": 0.0, "avg_logprob": -0.10396244855431037, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.0015237616607919335}, {"id": 548, "seek": 329828, "start": 3303.32, "end": 3309.32, "text": " chat GPT and Co, which are incredibly rich and powerful tools, right? But at the same time,", "tokens": [50616, 5081, 26039, 51, 293, 3066, 11, 597, 366, 6252, 4593, 293, 4005, 3873, 11, 558, 30, 583, 412, 264, 912, 565, 11, 50916], "temperature": 0.0, "avg_logprob": -0.10396244855431037, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.0015237616607919335}, {"id": 549, "seek": 329828, "start": 3309.32, "end": 3316.2000000000003, "text": " they can't load a dishwasher. Yeah? So, with some way, I think, from having this version of", "tokens": [50916, 436, 393, 380, 3677, 257, 38009, 13, 865, 30, 407, 11, 365, 512, 636, 11, 286, 519, 11, 490, 1419, 341, 3037, 295, 51260], "temperature": 0.0, "avg_logprob": -0.10396244855431037, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.0015237616607919335}, {"id": 550, "seek": 329828, "start": 3316.2000000000003, "end": 3322.1200000000003, "text": " general AI, the idea of having one machine that can really do anything that a human being could do,", "tokens": [51260, 2674, 7318, 11, 264, 1558, 295, 1419, 472, 3479, 300, 393, 534, 360, 1340, 300, 257, 1952, 885, 727, 360, 11, 51556], "temperature": 0.0, "avg_logprob": -0.10396244855431037, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.0015237616607919335}, {"id": 551, "seek": 329828, "start": 3322.84, "end": 3327.1600000000003, "text": " a machine which could tell a joke, read a book, and answer questions about it. The technology", "tokens": [51592, 257, 3479, 597, 727, 980, 257, 7647, 11, 1401, 257, 1446, 11, 293, 1867, 1651, 466, 309, 13, 440, 2899, 51808], "temperature": 0.0, "avg_logprob": -0.10396244855431037, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.0015237616607919335}, {"id": 552, "seek": 332716, "start": 3327.16, "end": 3332.2799999999997, "text": " can read books and answer questions now that could tell a joke, that could cook us an omelet,", "tokens": [50364, 393, 1401, 3642, 293, 1867, 1651, 586, 300, 727, 980, 257, 7647, 11, 300, 727, 2543, 505, 364, 3406, 15966, 11, 50620], "temperature": 0.0, "avg_logprob": -0.0808652457544359, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.0007136037456803024}, {"id": 553, "seek": 332716, "start": 3332.2799999999997, "end": 3338.04, "text": " that could tidy our house, that could ride a bicycle, and so on, that could write a sonnet,", "tokens": [50620, 300, 727, 34646, 527, 1782, 11, 300, 727, 5077, 257, 20888, 11, 293, 370, 322, 11, 300, 727, 2464, 257, 1872, 7129, 11, 50908], "temperature": 0.0, "avg_logprob": -0.0808652457544359, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.0007136037456803024}, {"id": 554, "seek": 332716, "start": 3338.04, "end": 3343.24, "text": " all of those things that human beings could do. If we succeed with full general intelligence,", "tokens": [50908, 439, 295, 729, 721, 300, 1952, 8958, 727, 360, 13, 759, 321, 7754, 365, 1577, 2674, 7599, 11, 51168], "temperature": 0.0, "avg_logprob": -0.0808652457544359, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.0007136037456803024}, {"id": 555, "seek": 332716, "start": 3343.24, "end": 3348.04, "text": " then we would have succeeded with this version one. Now, I say, for the reasons that I've already", "tokens": [51168, 550, 321, 576, 362, 20263, 365, 341, 3037, 472, 13, 823, 11, 286, 584, 11, 337, 264, 4112, 300, 286, 600, 1217, 51408], "temperature": 0.0, "avg_logprob": -0.0808652457544359, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.0007136037456803024}, {"id": 556, "seek": 332716, "start": 3348.04, "end": 3355.08, "text": " explained, I don't think this is imminent, that version of general AI, because robotic AI, AI that", "tokens": [51408, 8825, 11, 286, 500, 380, 519, 341, 307, 44339, 11, 300, 3037, 295, 2674, 7318, 11, 570, 30468, 7318, 11, 7318, 300, 51760], "temperature": 0.0, "avg_logprob": -0.0808652457544359, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.0007136037456803024}, {"id": 557, "seek": 335508, "start": 3355.08, "end": 3361.16, "text": " exists in the real world and has to do tasks in the real world and manipulate objects in the", "tokens": [50364, 8198, 294, 264, 957, 1002, 293, 575, 281, 360, 9608, 294, 264, 957, 1002, 293, 20459, 6565, 294, 264, 50668], "temperature": 0.0, "avg_logprob": -0.05983008056127725, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.0008827749988995492}, {"id": 558, "seek": 335508, "start": 3361.16, "end": 3367.72, "text": " real world, robotic AI is much, much harder. It's nowhere near as advanced as chat GPT and Co,", "tokens": [50668, 957, 1002, 11, 30468, 7318, 307, 709, 11, 709, 6081, 13, 467, 311, 11159, 2651, 382, 7339, 382, 5081, 26039, 51, 293, 3066, 11, 50996], "temperature": 0.0, "avg_logprob": -0.05983008056127725, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.0008827749988995492}, {"id": 559, "seek": 335508, "start": 3367.72, "end": 3372.04, "text": " and that's not a slur on my colleagues that do robotics research, it's just because the real", "tokens": [50996, 293, 300, 311, 406, 257, 1061, 374, 322, 452, 7734, 300, 360, 34145, 2132, 11, 309, 311, 445, 570, 264, 957, 51212], "temperature": 0.0, "avg_logprob": -0.05983008056127725, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.0008827749988995492}, {"id": 560, "seek": 335508, "start": 3372.04, "end": 3378.52, "text": " world is really, really, really tough. So, I don't think that we're anywhere close to having machines", "tokens": [51212, 1002, 307, 534, 11, 534, 11, 534, 4930, 13, 407, 11, 286, 500, 380, 519, 300, 321, 434, 4992, 1998, 281, 1419, 8379, 51536], "temperature": 0.0, "avg_logprob": -0.05983008056127725, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.0008827749988995492}, {"id": 561, "seek": 335508, "start": 3378.52, "end": 3384.36, "text": " that can do anything that a human being could do. But what about the second version? The second", "tokens": [51536, 300, 393, 360, 1340, 300, 257, 1952, 885, 727, 360, 13, 583, 437, 466, 264, 1150, 3037, 30, 440, 1150, 51828], "temperature": 0.0, "avg_logprob": -0.05983008056127725, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.0008827749988995492}, {"id": 562, "seek": 338436, "start": 3384.36, "end": 3390.92, "text": " version of general intelligence is, well, forget about the real world, how about just tasks which", "tokens": [50364, 3037, 295, 2674, 7599, 307, 11, 731, 11, 2870, 466, 264, 957, 1002, 11, 577, 466, 445, 9608, 597, 50692], "temperature": 0.0, "avg_logprob": -0.1001052213518807, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.00200528628192842}, {"id": 563, "seek": 338436, "start": 3390.92, "end": 3396.76, "text": " require cognitive abilities? Reasoning the ability to look at a picture and answer questions about it,", "tokens": [50692, 3651, 15605, 11582, 30, 39693, 278, 264, 3485, 281, 574, 412, 257, 3036, 293, 1867, 1651, 466, 309, 11, 50984], "temperature": 0.0, "avg_logprob": -0.1001052213518807, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.00200528628192842}, {"id": 564, "seek": 338436, "start": 3396.76, "end": 3401.48, "text": " the ability to listen to something and answer questions about it and interpret that. Anything", "tokens": [50984, 264, 3485, 281, 2140, 281, 746, 293, 1867, 1651, 466, 309, 293, 7302, 300, 13, 11998, 51220], "temperature": 0.0, "avg_logprob": -0.1001052213518807, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.00200528628192842}, {"id": 565, "seek": 338436, "start": 3401.48, "end": 3408.28, "text": " which involves those kinds of tasks. Well, I think we are much closer, we're not there yet, but we're", "tokens": [51220, 597, 11626, 729, 3685, 295, 9608, 13, 1042, 11, 286, 519, 321, 366, 709, 4966, 11, 321, 434, 406, 456, 1939, 11, 457, 321, 434, 51560], "temperature": 0.0, "avg_logprob": -0.1001052213518807, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.00200528628192842}, {"id": 566, "seek": 340828, "start": 3408.28, "end": 3414.6000000000004, "text": " much closer than we were four years ago. Now, I noticed actually, just before today's, before I", "tokens": [50364, 709, 4966, 813, 321, 645, 1451, 924, 2057, 13, 823, 11, 286, 5694, 767, 11, 445, 949, 965, 311, 11, 949, 286, 50680], "temperature": 0.0, "avg_logprob": -0.0982273350591245, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.008127637207508087}, {"id": 567, "seek": 340828, "start": 3414.6000000000004, "end": 3419.48, "text": " came in today, I noticed that Google, Google slash DeepMind have announced their latest", "tokens": [50680, 1361, 294, 965, 11, 286, 5694, 300, 3329, 11, 3329, 17330, 14895, 44, 471, 362, 7548, 641, 6792, 50924], "temperature": 0.0, "avg_logprob": -0.0982273350591245, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.008127637207508087}, {"id": 568, "seek": 340828, "start": 3421.32, "end": 3426.28, "text": " large language model technology, and I think it's called Gemini, and at first glance it looks like", "tokens": [51016, 2416, 2856, 2316, 2899, 11, 293, 286, 519, 309, 311, 1219, 22894, 3812, 11, 293, 412, 700, 21094, 309, 1542, 411, 51264], "temperature": 0.0, "avg_logprob": -0.0982273350591245, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.008127637207508087}, {"id": 569, "seek": 340828, "start": 3426.28, "end": 3431.48, "text": " it's very, very impressive. I couldn't help but thinking it's no accident that they announced", "tokens": [51264, 309, 311, 588, 11, 588, 8992, 13, 286, 2809, 380, 854, 457, 1953, 309, 311, 572, 6398, 300, 436, 7548, 51524], "temperature": 0.0, "avg_logprob": -0.0982273350591245, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.008127637207508087}, {"id": 570, "seek": 340828, "start": 3431.48, "end": 3437.7200000000003, "text": " that just before my lecture. I can't help think that there's a little bit of attempt to upstage", "tokens": [51524, 300, 445, 949, 452, 7991, 13, 286, 393, 380, 854, 519, 300, 456, 311, 257, 707, 857, 295, 5217, 281, 493, 17882, 51836], "temperature": 0.0, "avg_logprob": -0.0982273350591245, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.008127637207508087}, {"id": 571, "seek": 343772, "start": 3437.72, "end": 3442.04, "text": " my lecture going on there, but anyway, we won't let them get away with that. But it looks very", "tokens": [50364, 452, 7991, 516, 322, 456, 11, 457, 4033, 11, 321, 1582, 380, 718, 552, 483, 1314, 365, 300, 13, 583, 309, 1542, 588, 50580], "temperature": 0.0, "avg_logprob": -0.08943684226588199, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.0013242327840998769}, {"id": 572, "seek": 343772, "start": 3442.04, "end": 3449.48, "text": " impressive, and the crucial thing is here is what AI people call multimodal. What multimodal means is", "tokens": [50580, 8992, 11, 293, 264, 11462, 551, 307, 510, 307, 437, 7318, 561, 818, 32972, 378, 304, 13, 708, 32972, 378, 304, 1355, 307, 50952], "temperature": 0.0, "avg_logprob": -0.08943684226588199, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.0013242327840998769}, {"id": 573, "seek": 343772, "start": 3449.48, "end": 3456.6, "text": " it doesn't just deal with text, it can deal with text and images, potentially with sounds as well,", "tokens": [50952, 309, 1177, 380, 445, 2028, 365, 2487, 11, 309, 393, 2028, 365, 2487, 293, 5267, 11, 7263, 365, 3263, 382, 731, 11, 51308], "temperature": 0.0, "avg_logprob": -0.08943684226588199, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.0013242327840998769}, {"id": 574, "seek": 343772, "start": 3456.6, "end": 3462.12, "text": " and each of those is a different modality of communication. And where this technology is", "tokens": [51308, 293, 1184, 295, 729, 307, 257, 819, 1072, 1860, 295, 6101, 13, 400, 689, 341, 2899, 307, 51584], "temperature": 0.0, "avg_logprob": -0.08943684226588199, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.0013242327840998769}, {"id": 575, "seek": 346212, "start": 3462.8399999999997, "end": 3468.04, "text": " clearly multimodal is going to be the next big thing. And Gemini, I say I haven't looked at it", "tokens": [50400, 4448, 32972, 378, 304, 307, 516, 281, 312, 264, 958, 955, 551, 13, 400, 22894, 3812, 11, 286, 584, 286, 2378, 380, 2956, 412, 309, 50660], "temperature": 0.0, "avg_logprob": -0.11051215516759995, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0019001333275809884}, {"id": 576, "seek": 346212, "start": 3468.04, "end": 3475.16, "text": " closely, but it looks like it's on that track. Okay, the next version of general intelligence", "tokens": [50660, 8185, 11, 457, 309, 1542, 411, 309, 311, 322, 300, 2837, 13, 1033, 11, 264, 958, 3037, 295, 2674, 7599, 51016], "temperature": 0.0, "avg_logprob": -0.11051215516759995, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0019001333275809884}, {"id": 577, "seek": 346212, "start": 3475.16, "end": 3480.7599999999998, "text": " is intelligence that can do any language-based tasks that a human being could do. So anything", "tokens": [51016, 307, 7599, 300, 393, 360, 604, 2856, 12, 6032, 9608, 300, 257, 1952, 885, 727, 360, 13, 407, 1340, 51296], "temperature": 0.0, "avg_logprob": -0.11051215516759995, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0019001333275809884}, {"id": 578, "seek": 346212, "start": 3480.7599999999998, "end": 3487.48, "text": " that you could communicate in language, in ordinary written text, an AI system that could do that.", "tokens": [51296, 300, 291, 727, 7890, 294, 2856, 11, 294, 10547, 3720, 2487, 11, 364, 7318, 1185, 300, 727, 360, 300, 13, 51632], "temperature": 0.0, "avg_logprob": -0.11051215516759995, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0019001333275809884}, {"id": 579, "seek": 348748, "start": 3487.48, "end": 3493.2400000000002, "text": " Now, we aren't there yet, and we know we're not there yet, because chat GPT and code get things", "tokens": [50364, 823, 11, 321, 3212, 380, 456, 1939, 11, 293, 321, 458, 321, 434, 406, 456, 1939, 11, 570, 5081, 26039, 51, 293, 3089, 483, 721, 50652], "temperature": 0.0, "avg_logprob": -0.0783005111357745, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.002384623046964407}, {"id": 580, "seek": 348748, "start": 3493.2400000000002, "end": 3498.84, "text": " wrong all the time. But you can see that we're not far off from that. Intuitively, it doesn't look", "tokens": [50652, 2085, 439, 264, 565, 13, 583, 291, 393, 536, 300, 321, 434, 406, 1400, 766, 490, 300, 13, 5681, 1983, 3413, 11, 309, 1177, 380, 574, 50932], "temperature": 0.0, "avg_logprob": -0.0783005111357745, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.002384623046964407}, {"id": 581, "seek": 348748, "start": 3498.84, "end": 3504.44, "text": " like we're that far off from that. The final version, and I think this is imminent, this is", "tokens": [50932, 411, 321, 434, 300, 1400, 766, 490, 300, 13, 440, 2572, 3037, 11, 293, 286, 519, 341, 307, 44339, 11, 341, 307, 51212], "temperature": 0.0, "avg_logprob": -0.0783005111357745, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.002384623046964407}, {"id": 582, "seek": 348748, "start": 3504.44, "end": 3509.4, "text": " going to happen in the near future, is what I'll call augmented large language models. And that", "tokens": [51212, 516, 281, 1051, 294, 264, 2651, 2027, 11, 307, 437, 286, 603, 818, 36155, 2416, 2856, 5245, 13, 400, 300, 51460], "temperature": 0.0, "avg_logprob": -0.0783005111357745, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.002384623046964407}, {"id": 583, "seek": 348748, "start": 3509.4, "end": 3516.68, "text": " means you take GPT-3 or chat GPT, and you just add lots of subroutines to it. So if it has to do", "tokens": [51460, 1355, 291, 747, 26039, 51, 12, 18, 420, 5081, 26039, 51, 11, 293, 291, 445, 909, 3195, 295, 1422, 81, 346, 1652, 281, 309, 13, 407, 498, 309, 575, 281, 360, 51824], "temperature": 0.0, "avg_logprob": -0.0783005111357745, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.002384623046964407}, {"id": 584, "seek": 351668, "start": 3516.7599999999998, "end": 3522.04, "text": " a specialist task, it just calls a specialist solver in order to be able to do that task.", "tokens": [50368, 257, 17008, 5633, 11, 309, 445, 5498, 257, 17008, 1404, 331, 294, 1668, 281, 312, 1075, 281, 360, 300, 5633, 13, 50632], "temperature": 0.0, "avg_logprob": -0.1089976742154076, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.0006787061574868858}, {"id": 585, "seek": 351668, "start": 3522.68, "end": 3528.3599999999997, "text": " And this is not, from an AI perspective, a terribly elegant version of artificial intelligence,", "tokens": [50664, 400, 341, 307, 406, 11, 490, 364, 7318, 4585, 11, 257, 22903, 21117, 3037, 295, 11677, 7599, 11, 50948], "temperature": 0.0, "avg_logprob": -0.1089976742154076, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.0006787061574868858}, {"id": 586, "seek": 351668, "start": 3529.08, "end": 3535.48, "text": " but nevertheless, I think, a very useful version of artificial intelligence. Now, I say there's,", "tokens": [50984, 457, 26924, 11, 286, 519, 11, 257, 588, 4420, 3037, 295, 11677, 7599, 13, 823, 11, 286, 584, 456, 311, 11, 51304], "temperature": 0.0, "avg_logprob": -0.1089976742154076, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.0006787061574868858}, {"id": 587, "seek": 351668, "start": 3535.48, "end": 3540.12, "text": " here, these four varieties from the most ambitious down to the least ambitious", "tokens": [51304, 510, 11, 613, 1451, 22092, 490, 264, 881, 20239, 760, 281, 264, 1935, 20239, 51536], "temperature": 0.0, "avg_logprob": -0.1089976742154076, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.0006787061574868858}, {"id": 588, "seek": 354012, "start": 3541.0, "end": 3549.0, "text": " still represents a huge spectrum of AI capabilities, a huge spectrum of AI capabilities.", "tokens": [50408, 920, 8855, 257, 2603, 11143, 295, 7318, 10862, 11, 257, 2603, 11143, 295, 7318, 10862, 13, 50808], "temperature": 0.0, "avg_logprob": -0.07182164148453178, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.009003553539514542}, {"id": 589, "seek": 354012, "start": 3549.0, "end": 3554.12, "text": " And I have the sense that the goalposts in general AI have been changed a bit. I think when", "tokens": [50808, 400, 286, 362, 264, 2020, 300, 264, 3387, 23744, 82, 294, 2674, 7318, 362, 668, 3105, 257, 857, 13, 286, 519, 562, 51064], "temperature": 0.0, "avg_logprob": -0.07182164148453178, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.009003553539514542}, {"id": 590, "seek": 354012, "start": 3554.12, "end": 3558.7599999999998, "text": " general AI was first discussed, what people were talking about was the first version. Now,", "tokens": [51064, 2674, 7318, 390, 700, 7152, 11, 437, 561, 645, 1417, 466, 390, 264, 700, 3037, 13, 823, 11, 51296], "temperature": 0.0, "avg_logprob": -0.07182164148453178, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.009003553539514542}, {"id": 591, "seek": 354012, "start": 3558.7599999999998, "end": 3562.8399999999997, "text": " when they talk about it, I really think they're talking about the fourth version. But the fourth", "tokens": [51296, 562, 436, 751, 466, 309, 11, 286, 534, 519, 436, 434, 1417, 466, 264, 6409, 3037, 13, 583, 264, 6409, 51500], "temperature": 0.0, "avg_logprob": -0.07182164148453178, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.009003553539514542}, {"id": 592, "seek": 354012, "start": 3562.8399999999997, "end": 3568.3599999999997, "text": " version, I think plausibly, is imminent in the next couple of years. That just means much more", "tokens": [51500, 3037, 11, 286, 519, 34946, 3545, 11, 307, 44339, 294, 264, 958, 1916, 295, 924, 13, 663, 445, 1355, 709, 544, 51776], "temperature": 0.0, "avg_logprob": -0.07182164148453178, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.009003553539514542}, {"id": 593, "seek": 356836, "start": 3568.36, "end": 3572.76, "text": " capable large language models that get things wrong a lot less, that are capable of doing", "tokens": [50364, 8189, 2416, 2856, 5245, 300, 483, 721, 2085, 257, 688, 1570, 11, 300, 366, 8189, 295, 884, 50584], "temperature": 0.0, "avg_logprob": -0.07318716544609566, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009532697149552405}, {"id": 594, "seek": 356836, "start": 3572.76, "end": 3578.6800000000003, "text": " specialized tasks, but not by using the transformer architecture just by calling on some specialized", "tokens": [50584, 19813, 9608, 11, 457, 406, 538, 1228, 264, 31782, 9482, 445, 538, 5141, 322, 512, 19813, 50880], "temperature": 0.0, "avg_logprob": -0.07318716544609566, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009532697149552405}, {"id": 595, "seek": 356836, "start": 3578.6800000000003, "end": 3586.28, "text": " software. So I don't think the transformer architecture itself is the key to general", "tokens": [50880, 4722, 13, 407, 286, 500, 380, 519, 264, 31782, 9482, 2564, 307, 264, 2141, 281, 2674, 51260], "temperature": 0.0, "avg_logprob": -0.07318716544609566, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009532697149552405}, {"id": 596, "seek": 356836, "start": 3586.28, "end": 3591.2400000000002, "text": " intelligence. In particular, it doesn't help us with the robotics problems that I mentioned earlier", "tokens": [51260, 7599, 13, 682, 1729, 11, 309, 1177, 380, 854, 505, 365, 264, 34145, 2740, 300, 286, 2835, 3071, 51508], "temperature": 0.0, "avg_logprob": -0.07318716544609566, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009532697149552405}, {"id": 597, "seek": 359124, "start": 3591.24, "end": 3598.6, "text": " on. And if we look here at this picture, this picture illustrates some of the dimensions", "tokens": [50364, 322, 13, 400, 498, 321, 574, 510, 412, 341, 3036, 11, 341, 3036, 41718, 512, 295, 264, 12819, 50732], "temperature": 0.0, "avg_logprob": -0.07964478741894972, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.038562603294849396}, {"id": 598, "seek": 359124, "start": 3598.6, "end": 3603.4799999999996, "text": " of human intelligence. And it's far from complete. This is me just thinking for half an hour about", "tokens": [50732, 295, 1952, 7599, 13, 400, 309, 311, 1400, 490, 3566, 13, 639, 307, 385, 445, 1953, 337, 1922, 364, 1773, 466, 50976], "temperature": 0.0, "avg_logprob": -0.07964478741894972, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.038562603294849396}, {"id": 599, "seek": 359124, "start": 3603.4799999999996, "end": 3608.2, "text": " some of the dimensions of human intelligence. But the things in blue, roughly speaking,", "tokens": [50976, 512, 295, 264, 12819, 295, 1952, 7599, 13, 583, 264, 721, 294, 3344, 11, 9810, 4124, 11, 51212], "temperature": 0.0, "avg_logprob": -0.07964478741894972, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.038562603294849396}, {"id": 600, "seek": 359124, "start": 3608.2, "end": 3614.04, "text": " are mental capabilities, stuff you do in your head. The things in red are things you do in the", "tokens": [51212, 366, 4973, 10862, 11, 1507, 291, 360, 294, 428, 1378, 13, 440, 721, 294, 2182, 366, 721, 291, 360, 294, 264, 51504], "temperature": 0.0, "avg_logprob": -0.07964478741894972, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.038562603294849396}, {"id": 601, "seek": 359124, "start": 3614.04, "end": 3619.08, "text": " physical world. So in red on the right hand side, for example, there's mobility, the ability to", "tokens": [51504, 4001, 1002, 13, 407, 294, 2182, 322, 264, 558, 1011, 1252, 11, 337, 1365, 11, 456, 311, 16199, 11, 264, 3485, 281, 51756], "temperature": 0.0, "avg_logprob": -0.07964478741894972, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.038562603294849396}, {"id": 602, "seek": 361908, "start": 3619.08, "end": 3625.0, "text": " move around some environment and associated with that, navigation. Manual dexterity and", "tokens": [50364, 1286, 926, 512, 2823, 293, 6615, 365, 300, 11, 17346, 13, 46173, 368, 36671, 507, 293, 50660], "temperature": 0.0, "avg_logprob": -0.1227351478908373, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0013368561631068587}, {"id": 603, "seek": 361908, "start": 3625.0, "end": 3631.16, "text": " manipulation, doing complex fiddly things with your hands. Robot hands are nowhere near at the", "tokens": [50660, 26475, 11, 884, 3997, 283, 14273, 356, 721, 365, 428, 2377, 13, 29601, 2377, 366, 11159, 2651, 412, 264, 50968], "temperature": 0.0, "avg_logprob": -0.1227351478908373, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0013368561631068587}, {"id": 604, "seek": 361908, "start": 3631.16, "end": 3637.16, "text": " level of a human carpenter or plumber, for example. Nowhere near. So we're a long way out from having", "tokens": [50968, 1496, 295, 257, 1952, 26103, 14278, 420, 499, 4182, 11, 337, 1365, 13, 823, 6703, 2651, 13, 407, 321, 434, 257, 938, 636, 484, 490, 1419, 51268], "temperature": 0.0, "avg_logprob": -0.1227351478908373, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0013368561631068587}, {"id": 605, "seek": 361908, "start": 3637.16, "end": 3644.84, "text": " that. Understanding, oh, doing hand-eye coordination, relatedly. Understanding what you're seeing and", "tokens": [51268, 300, 13, 36858, 11, 1954, 11, 884, 1011, 12, 25488, 21252, 11, 4077, 356, 13, 36858, 437, 291, 434, 2577, 293, 51652], "temperature": 0.0, "avg_logprob": -0.1227351478908373, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0013368561631068587}, {"id": 606, "seek": 364484, "start": 3644.84, "end": 3649.2400000000002, "text": " understanding what you're hearing, we've made some progress on. But a lot of these tasks we've", "tokens": [50364, 3701, 437, 291, 434, 4763, 11, 321, 600, 1027, 512, 4205, 322, 13, 583, 257, 688, 295, 613, 9608, 321, 600, 50584], "temperature": 0.0, "avg_logprob": -0.06297836701075236, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0037015245761722326}, {"id": 607, "seek": 364484, "start": 3649.2400000000002, "end": 3654.76, "text": " made no progress on. And then on the left hand side, the blue stuff is stuff that goes on in your", "tokens": [50584, 1027, 572, 4205, 322, 13, 400, 550, 322, 264, 1411, 1011, 1252, 11, 264, 3344, 1507, 307, 1507, 300, 1709, 322, 294, 428, 50860], "temperature": 0.0, "avg_logprob": -0.06297836701075236, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0037015245761722326}, {"id": 608, "seek": 364484, "start": 3654.76, "end": 3661.08, "text": " head. Things like logical reasoning and planning and so on. So what is the state of the art now?", "tokens": [50860, 1378, 13, 9514, 411, 14978, 21577, 293, 5038, 293, 370, 322, 13, 407, 437, 307, 264, 1785, 295, 264, 1523, 586, 30, 51176], "temperature": 0.0, "avg_logprob": -0.06297836701075236, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0037015245761722326}, {"id": 609, "seek": 364484, "start": 3661.08, "end": 3667.0, "text": " It looks something like this. The red cross means no, we don't have it in large language models.", "tokens": [51176, 467, 1542, 746, 411, 341, 13, 440, 2182, 3278, 1355, 572, 11, 321, 500, 380, 362, 309, 294, 2416, 2856, 5245, 13, 51472], "temperature": 0.0, "avg_logprob": -0.06297836701075236, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0037015245761722326}, {"id": 610, "seek": 364484, "start": 3667.0, "end": 3673.4, "text": " We're not there. There are fundamental problems. The question marks are, well, maybe we might", "tokens": [51472, 492, 434, 406, 456, 13, 821, 366, 8088, 2740, 13, 440, 1168, 10640, 366, 11, 731, 11, 1310, 321, 1062, 51792], "temperature": 0.0, "avg_logprob": -0.06297836701075236, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0037015245761722326}, {"id": 611, "seek": 367340, "start": 3673.4, "end": 3680.04, "text": " have a bit of it, but we don't have the whole answer. And the green-wise are, yeah, I think we're", "tokens": [50364, 362, 257, 857, 295, 309, 11, 457, 321, 500, 380, 362, 264, 1379, 1867, 13, 400, 264, 3092, 12, 3711, 366, 11, 1338, 11, 286, 519, 321, 434, 50696], "temperature": 0.0, "avg_logprob": -0.07944441824844203, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0015315394848585129}, {"id": 612, "seek": 367340, "start": 3680.04, "end": 3685.2400000000002, "text": " there. Well, the one that we've really nailed is what's called natural language processing.", "tokens": [50696, 456, 13, 1042, 11, 264, 472, 300, 321, 600, 534, 30790, 307, 437, 311, 1219, 3303, 2856, 9007, 13, 50956], "temperature": 0.0, "avg_logprob": -0.07944441824844203, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0015315394848585129}, {"id": 613, "seek": 367340, "start": 3685.2400000000002, "end": 3692.6800000000003, "text": " And that's the ability to understand and create ordinary human text. That's what large language", "tokens": [50956, 400, 300, 311, 264, 3485, 281, 1223, 293, 1884, 10547, 1952, 2487, 13, 663, 311, 437, 2416, 2856, 51328], "temperature": 0.0, "avg_logprob": -0.07944441824844203, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0015315394848585129}, {"id": 614, "seek": 367340, "start": 3692.6800000000003, "end": 3698.6, "text": " models were designed to do, to interact in ordinary human text. That's what they are best at. But", "tokens": [51328, 5245, 645, 4761, 281, 360, 11, 281, 4648, 294, 10547, 1952, 2487, 13, 663, 311, 437, 436, 366, 1151, 412, 13, 583, 51624], "temperature": 0.0, "avg_logprob": -0.07944441824844203, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0015315394848585129}, {"id": 615, "seek": 369860, "start": 3698.68, "end": 3703.72, "text": " actually, the whole range of stuff, the other stuff here, we're not there at all. By the way,", "tokens": [50368, 767, 11, 264, 1379, 3613, 295, 1507, 11, 264, 661, 1507, 510, 11, 321, 434, 406, 456, 412, 439, 13, 3146, 264, 636, 11, 50620], "temperature": 0.0, "avg_logprob": -0.08745120936988765, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0005284673534333706}, {"id": 616, "seek": 369860, "start": 3703.72, "end": 3708.44, "text": " I did notice that Gem and I claim to have been able to capable of planning. This is a mathematical", "tokens": [50620, 286, 630, 3449, 300, 22894, 293, 286, 3932, 281, 362, 668, 1075, 281, 8189, 295, 5038, 13, 639, 307, 257, 18894, 50856], "temperature": 0.0, "avg_logprob": -0.08745120936988765, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0005284673534333706}, {"id": 617, "seek": 369860, "start": 3708.44, "end": 3714.44, "text": " reasoning. So I look forward to seeing how good their technology is. But my point is we are still", "tokens": [50856, 21577, 13, 407, 286, 574, 2128, 281, 2577, 577, 665, 641, 2899, 307, 13, 583, 452, 935, 307, 321, 366, 920, 51156], "temperature": 0.0, "avg_logprob": -0.08745120936988765, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0005284673534333706}, {"id": 618, "seek": 369860, "start": 3714.44, "end": 3721.16, "text": " seen to be some way from full general intelligence. The last few minutes, I want to talk about", "tokens": [51156, 1612, 281, 312, 512, 636, 490, 1577, 2674, 7599, 13, 440, 1036, 1326, 2077, 11, 286, 528, 281, 751, 466, 51492], "temperature": 0.0, "avg_logprob": -0.08745120936988765, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0005284673534333706}, {"id": 619, "seek": 369860, "start": 3721.16, "end": 3726.2799999999997, "text": " something else. And I want to talk about machine consciousness. And the very first thing to say", "tokens": [51492, 746, 1646, 13, 400, 286, 528, 281, 751, 466, 3479, 10081, 13, 400, 264, 588, 700, 551, 281, 584, 51748], "temperature": 0.0, "avg_logprob": -0.08745120936988765, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0005284673534333706}, {"id": 620, "seek": 372628, "start": 3726.28, "end": 3732.0400000000004, "text": " about machine consciousness is, why on earth should we care about it? I am not remotely", "tokens": [50364, 466, 3479, 10081, 307, 11, 983, 322, 4120, 820, 321, 1127, 466, 309, 30, 286, 669, 406, 20824, 50652], "temperature": 0.0, "avg_logprob": -0.0777913593110584, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.0011817808263003826}, {"id": 621, "seek": 372628, "start": 3732.0400000000004, "end": 3736.44, "text": " interested in building machines that are conscious. I know very, very few artificial", "tokens": [50652, 3102, 294, 2390, 8379, 300, 366, 6648, 13, 286, 458, 588, 11, 588, 1326, 11677, 50872], "temperature": 0.0, "avg_logprob": -0.0777913593110584, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.0011817808263003826}, {"id": 622, "seek": 372628, "start": 3736.44, "end": 3742.28, "text": " intelligence researchers that are. But nevertheless, it's an interesting question. And in particular,", "tokens": [50872, 7599, 10309, 300, 366, 13, 583, 26924, 11, 309, 311, 364, 1880, 1168, 13, 400, 294, 1729, 11, 51164], "temperature": 0.0, "avg_logprob": -0.0777913593110584, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.0011817808263003826}, {"id": 623, "seek": 372628, "start": 3742.28, "end": 3747.48, "text": " it's a question which came to the fore because of this individual. This chap, Blake Lemoine,", "tokens": [51164, 309, 311, 257, 1168, 597, 1361, 281, 264, 2091, 570, 295, 341, 2609, 13, 639, 13223, 11, 23451, 16905, 44454, 11, 51424], "temperature": 0.0, "avg_logprob": -0.0777913593110584, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.0011817808263003826}, {"id": 624, "seek": 372628, "start": 3747.48, "end": 3752.92, "text": " in June 2022, he was a Google engineer. And he was working with a Google large language model,", "tokens": [51424, 294, 6928, 20229, 11, 415, 390, 257, 3329, 11403, 13, 400, 415, 390, 1364, 365, 257, 3329, 2416, 2856, 2316, 11, 51696], "temperature": 0.0, "avg_logprob": -0.0777913593110584, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.0011817808263003826}, {"id": 625, "seek": 375292, "start": 3753.0, "end": 3758.12, "text": " I think it was called Lambda. And he went public on Twitter and I think on his blog", "tokens": [50368, 286, 519, 309, 390, 1219, 45691, 13, 400, 415, 1437, 1908, 322, 5794, 293, 286, 519, 322, 702, 6968, 50624], "temperature": 0.0, "avg_logprob": -0.08125798235234526, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0062393187545239925}, {"id": 626, "seek": 375292, "start": 3758.12, "end": 3763.88, "text": " with an extraordinary claim. And he said, the system I'm working on is sentient. And here is", "tokens": [50624, 365, 364, 10581, 3932, 13, 400, 415, 848, 11, 264, 1185, 286, 478, 1364, 322, 307, 2279, 1196, 13, 400, 510, 307, 50912], "temperature": 0.0, "avg_logprob": -0.08125798235234526, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0062393187545239925}, {"id": 627, "seek": 375292, "start": 3763.88, "end": 3768.2000000000003, "text": " a quote of the conversation that the system came up with. He said, I'm aware of my existence and", "tokens": [50912, 257, 6513, 295, 264, 3761, 300, 264, 1185, 1361, 493, 365, 13, 634, 848, 11, 286, 478, 3650, 295, 452, 9123, 293, 51128], "temperature": 0.0, "avg_logprob": -0.08125798235234526, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0062393187545239925}, {"id": 628, "seek": 375292, "start": 3768.2000000000003, "end": 3776.76, "text": " I feel happy or sad at times. And it said, I'm afraid of being turned off. And Lemoine concluded", "tokens": [51128, 286, 841, 2055, 420, 4227, 412, 1413, 13, 400, 309, 848, 11, 286, 478, 4638, 295, 885, 3574, 766, 13, 400, 16905, 44454, 22960, 51556], "temperature": 0.0, "avg_logprob": -0.08125798235234526, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0062393187545239925}, {"id": 629, "seek": 377676, "start": 3776.76, "end": 3784.36, "text": " that the program was sentient, which is a very, very big claim indeed. And it made global headlines.", "tokens": [50364, 300, 264, 1461, 390, 2279, 1196, 11, 597, 307, 257, 588, 11, 588, 955, 3932, 6451, 13, 400, 309, 1027, 4338, 23867, 13, 50744], "temperature": 0.0, "avg_logprob": -0.10733316402242642, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.04638833552598953}, {"id": 630, "seek": 377676, "start": 3784.36, "end": 3791.2400000000002, "text": " And I received it, I know through the Turing team, we got a lot of press inquiries asking us,", "tokens": [50744, 400, 286, 4613, 309, 11, 286, 458, 807, 264, 314, 1345, 1469, 11, 321, 658, 257, 688, 295, 1886, 13570, 38619, 3365, 505, 11, 51088], "temperature": 0.0, "avg_logprob": -0.10733316402242642, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.04638833552598953}, {"id": 631, "seek": 377676, "start": 3791.2400000000002, "end": 3796.6000000000004, "text": " is it true that machines are now sentient? He was wrong on so many levels, I don't even", "tokens": [51088, 307, 309, 2074, 300, 8379, 366, 586, 2279, 1196, 30, 634, 390, 2085, 322, 370, 867, 4358, 11, 286, 500, 380, 754, 51356], "temperature": 0.0, "avg_logprob": -0.10733316402242642, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.04638833552598953}, {"id": 632, "seek": 377676, "start": 3796.6000000000004, "end": 3801.88, "text": " know where to begin to describe how wrong he was. But let me just explain one particular point to", "tokens": [51356, 458, 689, 281, 1841, 281, 6786, 577, 2085, 415, 390, 13, 583, 718, 385, 445, 2903, 472, 1729, 935, 281, 51620], "temperature": 0.0, "avg_logprob": -0.10733316402242642, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.04638833552598953}, {"id": 633, "seek": 380188, "start": 3801.88, "end": 3807.96, "text": " you. You're in the middle of a conversation with chat GPT, and you go on holiday for a couple of", "tokens": [50364, 291, 13, 509, 434, 294, 264, 2808, 295, 257, 3761, 365, 5081, 26039, 51, 11, 293, 291, 352, 322, 9960, 337, 257, 1916, 295, 50668], "temperature": 0.0, "avg_logprob": -0.0912032164926604, "compression_ratio": 1.8058608058608059, "no_speech_prob": 0.10612521320581436}, {"id": 634, "seek": 380188, "start": 3807.96, "end": 3814.36, "text": " weeks. When you get back, chat GPT is in exactly the same place. The cursor is blinking, waiting", "tokens": [50668, 3259, 13, 1133, 291, 483, 646, 11, 5081, 26039, 51, 307, 294, 2293, 264, 912, 1081, 13, 440, 28169, 307, 45879, 11, 3806, 50988], "temperature": 0.0, "avg_logprob": -0.0912032164926604, "compression_ratio": 1.8058608058608059, "no_speech_prob": 0.10612521320581436}, {"id": 635, "seek": 380188, "start": 3814.36, "end": 3820.28, "text": " for you to type your next thing. It hasn't been wondering where you've been. It hasn't been getting", "tokens": [50988, 337, 291, 281, 2010, 428, 958, 551, 13, 467, 6132, 380, 668, 6359, 689, 291, 600, 668, 13, 467, 6132, 380, 668, 1242, 51284], "temperature": 0.0, "avg_logprob": -0.0912032164926604, "compression_ratio": 1.8058608058608059, "no_speech_prob": 0.10612521320581436}, {"id": 636, "seek": 380188, "start": 3820.28, "end": 3825.48, "text": " bored. It hasn't been thinking where the hell has Woolridge gone? I'm not going to have a conversation", "tokens": [51284, 13521, 13, 467, 6132, 380, 668, 1953, 689, 264, 4921, 575, 46307, 15804, 2780, 30, 286, 478, 406, 516, 281, 362, 257, 3761, 51544], "temperature": 0.0, "avg_logprob": -0.0912032164926604, "compression_ratio": 1.8058608058608059, "no_speech_prob": 0.10612521320581436}, {"id": 637, "seek": 380188, "start": 3825.48, "end": 3830.6800000000003, "text": " with him again. It hasn't been thinking anything at all. It's a computer program, which is going", "tokens": [51544, 365, 796, 797, 13, 467, 6132, 380, 668, 1953, 1340, 412, 439, 13, 467, 311, 257, 3820, 1461, 11, 597, 307, 516, 51804], "temperature": 0.0, "avg_logprob": -0.0912032164926604, "compression_ratio": 1.8058608058608059, "no_speech_prob": 0.10612521320581436}, {"id": 638, "seek": 383068, "start": 3830.68, "end": 3837.24, "text": " around a loop, which is just waiting for you to type the next thing. Now, there is no sensible", "tokens": [50364, 926, 257, 6367, 11, 597, 307, 445, 3806, 337, 291, 281, 2010, 264, 958, 551, 13, 823, 11, 456, 307, 572, 25380, 50692], "temperature": 0.0, "avg_logprob": -0.06886688200365595, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.001295797759667039}, {"id": 639, "seek": 383068, "start": 3837.24, "end": 3843.72, "text": " definition of sentience, I think, which would admit that as being sentient. It absolutely is", "tokens": [50692, 7123, 295, 2279, 1182, 11, 286, 519, 11, 597, 576, 9796, 300, 382, 885, 2279, 1196, 13, 467, 3122, 307, 51016], "temperature": 0.0, "avg_logprob": -0.06886688200365595, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.001295797759667039}, {"id": 640, "seek": 383068, "start": 3843.72, "end": 3849.08, "text": " not sentient. So I think he was very, very wrong. But I've talked to a lot of people subsequently", "tokens": [51016, 406, 2279, 1196, 13, 407, 286, 519, 415, 390, 588, 11, 588, 2085, 13, 583, 286, 600, 2825, 281, 257, 688, 295, 561, 26514, 51284], "temperature": 0.0, "avg_logprob": -0.06886688200365595, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.001295797759667039}, {"id": 641, "seek": 383068, "start": 3849.08, "end": 3854.12, "text": " who have conversations with chat GPT and other large language models, and they come back to me", "tokens": [51284, 567, 362, 7315, 365, 5081, 26039, 51, 293, 661, 2416, 2856, 5245, 11, 293, 436, 808, 646, 281, 385, 51536], "temperature": 0.0, "avg_logprob": -0.06886688200365595, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.001295797759667039}, {"id": 642, "seek": 383068, "start": 3854.12, "end": 3859.96, "text": " and say, are you really sure? Because actually, it's really quite impressive. It really feels to me", "tokens": [51536, 293, 584, 11, 366, 291, 534, 988, 30, 1436, 767, 11, 309, 311, 534, 1596, 8992, 13, 467, 534, 3417, 281, 385, 51828], "temperature": 0.0, "avg_logprob": -0.06886688200365595, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.001295797759667039}, {"id": 643, "seek": 385996, "start": 3859.96, "end": 3864.68, "text": " like there is a mind behind the scene. So let's talk about this. And I think we have to answer", "tokens": [50364, 411, 456, 307, 257, 1575, 2261, 264, 4145, 13, 407, 718, 311, 751, 466, 341, 13, 400, 286, 519, 321, 362, 281, 1867, 50600], "temperature": 0.0, "avg_logprob": -0.07838981026097348, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.00044829846592620015}, {"id": 644, "seek": 385996, "start": 3864.68, "end": 3869.96, "text": " them. So let's talk about consciousness. Firstly, we don't understand consciousness. We all have it,", "tokens": [50600, 552, 13, 407, 718, 311, 751, 466, 10081, 13, 20042, 11, 321, 500, 380, 1223, 10081, 13, 492, 439, 362, 309, 11, 50864], "temperature": 0.0, "avg_logprob": -0.07838981026097348, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.00044829846592620015}, {"id": 645, "seek": 385996, "start": 3869.96, "end": 3877.7200000000003, "text": " to greater or lesser extent. We all experience it. But we don't understand it at all. And it's called", "tokens": [50864, 281, 5044, 420, 22043, 8396, 13, 492, 439, 1752, 309, 13, 583, 321, 500, 380, 1223, 309, 412, 439, 13, 400, 309, 311, 1219, 51252], "temperature": 0.0, "avg_logprob": -0.07838981026097348, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.00044829846592620015}, {"id": 646, "seek": 385996, "start": 3877.7200000000003, "end": 3884.68, "text": " the hard problem of cognitive science. And the hard problem is that there are certain", "tokens": [51252, 264, 1152, 1154, 295, 15605, 3497, 13, 400, 264, 1152, 1154, 307, 300, 456, 366, 1629, 51600], "temperature": 0.0, "avg_logprob": -0.07838981026097348, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.00044829846592620015}, {"id": 647, "seek": 388468, "start": 3884.68, "end": 3889.72, "text": " electrical chemical processes in the brain and the nervous system. And we can see those", "tokens": [50364, 12147, 7313, 7555, 294, 264, 3567, 293, 264, 6296, 1185, 13, 400, 321, 393, 536, 729, 50616], "temperature": 0.0, "avg_logprob": -0.07328068789313821, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.039474133402109146}, {"id": 648, "seek": 388468, "start": 3889.72, "end": 3894.9199999999996, "text": " electrochemical processes, we can see them operating, and they somehow give rise to conscious", "tokens": [50616, 16717, 34114, 7555, 11, 321, 393, 536, 552, 7447, 11, 293, 436, 6063, 976, 6272, 281, 6648, 50876], "temperature": 0.0, "avg_logprob": -0.07328068789313821, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.039474133402109146}, {"id": 649, "seek": 388468, "start": 3894.9199999999996, "end": 3901.24, "text": " experience. But why do they do it? How do they do it? And what evolutionary purpose does it serve?", "tokens": [50876, 1752, 13, 583, 983, 360, 436, 360, 309, 30, 1012, 360, 436, 360, 309, 30, 400, 437, 27567, 4334, 775, 309, 4596, 30, 51192], "temperature": 0.0, "avg_logprob": -0.07328068789313821, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.039474133402109146}, {"id": 650, "seek": 388468, "start": 3901.24, "end": 3906.8399999999997, "text": " Honestly, we have no idea. There's a huge disconnect between what we can see going on", "tokens": [51192, 12348, 11, 321, 362, 572, 1558, 13, 821, 311, 257, 2603, 14299, 1296, 437, 321, 393, 536, 516, 322, 51472], "temperature": 0.0, "avg_logprob": -0.07328068789313821, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.039474133402109146}, {"id": 651, "seek": 390684, "start": 3906.84, "end": 3913.56, "text": " in the physical brain and our conscious experience, our rich, private mental life.", "tokens": [50364, 294, 264, 4001, 3567, 293, 527, 6648, 1752, 11, 527, 4593, 11, 4551, 4973, 993, 13, 50700], "temperature": 0.0, "avg_logprob": -0.07553878495859545, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.011266297660768032}, {"id": 652, "seek": 390684, "start": 3914.44, "end": 3919.7200000000003, "text": " So really, there is no understanding of this at all. I think, by the way, my best guess about", "tokens": [50744, 407, 534, 11, 456, 307, 572, 3701, 295, 341, 412, 439, 13, 286, 519, 11, 538, 264, 636, 11, 452, 1151, 2041, 466, 51008], "temperature": 0.0, "avg_logprob": -0.07553878495859545, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.011266297660768032}, {"id": 653, "seek": 390684, "start": 3919.7200000000003, "end": 3925.96, "text": " how consciousness will be solved, if it is solved at all, is through an evolutionary approach.", "tokens": [51008, 577, 10081, 486, 312, 13041, 11, 498, 309, 307, 13041, 412, 439, 11, 307, 807, 364, 27567, 3109, 13, 51320], "temperature": 0.0, "avg_logprob": -0.07553878495859545, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.011266297660768032}, {"id": 654, "seek": 390684, "start": 3926.6800000000003, "end": 3933.96, "text": " But one general idea is that subjective experience is central to this, which means the ability to", "tokens": [51356, 583, 472, 2674, 1558, 307, 300, 25972, 1752, 307, 5777, 281, 341, 11, 597, 1355, 264, 3485, 281, 51720], "temperature": 0.0, "avg_logprob": -0.07553878495859545, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.011266297660768032}, {"id": 655, "seek": 393396, "start": 3933.96, "end": 3939.88, "text": " experience things from a personal perspective. And there's a famous test due to Nagel, which is", "tokens": [50364, 1752, 721, 490, 257, 2973, 4585, 13, 400, 456, 311, 257, 4618, 1500, 3462, 281, 18913, 338, 11, 597, 307, 50660], "temperature": 0.0, "avg_logprob": -0.08263726338096287, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.0024399375542998314}, {"id": 656, "seek": 393396, "start": 3939.88, "end": 3944.84, "text": " what is it like to be something? And Thomas Nagel in the 1970s said, something is conscious", "tokens": [50660, 437, 307, 309, 411, 281, 312, 746, 30, 400, 8500, 18913, 338, 294, 264, 14577, 82, 848, 11, 746, 307, 6648, 50908], "temperature": 0.0, "avg_logprob": -0.08263726338096287, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.0024399375542998314}, {"id": 657, "seek": 393396, "start": 3944.84, "end": 3953.48, "text": " if it is like something to be that thing. It isn't like anything to be chat GPT. Chat GPT", "tokens": [50908, 498, 309, 307, 411, 746, 281, 312, 300, 551, 13, 467, 1943, 380, 411, 1340, 281, 312, 5081, 26039, 51, 13, 27503, 26039, 51, 51340], "temperature": 0.0, "avg_logprob": -0.08263726338096287, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.0024399375542998314}, {"id": 658, "seek": 393396, "start": 3953.48, "end": 3961.56, "text": " has no mental life whatsoever. It's never experienced anything in the real world whatsoever.", "tokens": [51340, 575, 572, 4973, 993, 17076, 13, 467, 311, 1128, 6751, 1340, 294, 264, 957, 1002, 17076, 13, 51744], "temperature": 0.0, "avg_logprob": -0.08263726338096287, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.0024399375542998314}, {"id": 659, "seek": 396156, "start": 3962.2799999999997, "end": 3965.72, "text": " And so for that reason, and a whole host of others that we're not going to have time to go into,", "tokens": [50400, 400, 370, 337, 300, 1778, 11, 293, 257, 1379, 3975, 295, 2357, 300, 321, 434, 406, 516, 281, 362, 565, 281, 352, 666, 11, 50572], "temperature": 0.0, "avg_logprob": -0.07403121813379153, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.001635997905395925}, {"id": 660, "seek": 396156, "start": 3966.44, "end": 3971.7999999999997, "text": " for that reason alone, I think we can conclude pretty safely that the technology that we have now", "tokens": [50608, 337, 300, 1778, 3312, 11, 286, 519, 321, 393, 16886, 1238, 11750, 300, 264, 2899, 300, 321, 362, 586, 50876], "temperature": 0.0, "avg_logprob": -0.07403121813379153, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.001635997905395925}, {"id": 661, "seek": 396156, "start": 3971.7999999999997, "end": 3978.2799999999997, "text": " is not conscious. And indeed, that's absolutely not the right way to think about this. And honestly,", "tokens": [50876, 307, 406, 6648, 13, 400, 6451, 11, 300, 311, 3122, 406, 264, 558, 636, 281, 519, 466, 341, 13, 400, 6095, 11, 51200], "temperature": 0.0, "avg_logprob": -0.07403121813379153, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.001635997905395925}, {"id": 662, "seek": 396156, "start": 3978.2799999999997, "end": 3984.2799999999997, "text": " in AI, we don't know how to go about making conscious machines. But I don't know why we would.", "tokens": [51200, 294, 7318, 11, 321, 500, 380, 458, 577, 281, 352, 466, 1455, 6648, 8379, 13, 583, 286, 500, 380, 458, 983, 321, 576, 13, 51500], "temperature": 0.0, "avg_logprob": -0.07403121813379153, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.001635997905395925}, {"id": 663, "seek": 398428, "start": 3985.1600000000003, "end": 3988.44, "text": " Okay. Thank you very much, ladies and gentlemen.", "tokens": [50408, 1033, 13, 1044, 291, 588, 709, 11, 9974, 293, 11669, 13, 50572], "temperature": 0.0, "avg_logprob": -0.19366759148196896, "compression_ratio": 1.4244186046511629, "no_speech_prob": 0.006570037920027971}, {"id": 664, "seek": 398428, "start": 4003.88, "end": 4009.32, "text": " Amazing. Thank you so much, Mike, for that talk. I'm sure there's going to be tons of questions.", "tokens": [51344, 14165, 13, 1044, 291, 370, 709, 11, 6602, 11, 337, 300, 751, 13, 286, 478, 988, 456, 311, 516, 281, 312, 9131, 295, 1651, 13, 51616], "temperature": 0.0, "avg_logprob": -0.19366759148196896, "compression_ratio": 1.4244186046511629, "no_speech_prob": 0.006570037920027971}, {"id": 665, "seek": 398428, "start": 4009.32, "end": 4013.48, "text": " Just as a reminder, if you're in the room, please raise your hand if you have a question. And we've", "tokens": [51616, 1449, 382, 257, 13548, 11, 498, 291, 434, 294, 264, 1808, 11, 1767, 5300, 428, 1011, 498, 291, 362, 257, 1168, 13, 400, 321, 600, 51824], "temperature": 0.0, "avg_logprob": -0.19366759148196896, "compression_ratio": 1.4244186046511629, "no_speech_prob": 0.006570037920027971}, {"id": 666, "seek": 401348, "start": 4013.48, "end": 4017.32, "text": " got roaming mics that we'll send around. If you're online, you can submit them via the chat,", "tokens": [50364, 658, 42680, 45481, 300, 321, 603, 2845, 926, 13, 759, 291, 434, 2950, 11, 291, 393, 10315, 552, 5766, 264, 5081, 11, 50556], "temperature": 0.0, "avg_logprob": -0.14781174569759728, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0006239407230168581}, {"id": 667, "seek": 401348, "start": 4017.32, "end": 4023.0, "text": " via the Vimeo function, and we can assign it on the chat to ask those questions as well.", "tokens": [50556, 5766, 264, 691, 1312, 78, 2445, 11, 293, 321, 393, 6269, 309, 322, 264, 5081, 281, 1029, 729, 1651, 382, 731, 13, 50840], "temperature": 0.0, "avg_logprob": -0.14781174569759728, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0006239407230168581}, {"id": 668, "seek": 401348, "start": 4023.0, "end": 4026.76, "text": " So please do raise your questions. Oh, raise your hands if you have one.", "tokens": [50840, 407, 1767, 360, 5300, 428, 1651, 13, 876, 11, 5300, 428, 2377, 498, 291, 362, 472, 13, 51028], "temperature": 0.0, "avg_logprob": -0.14781174569759728, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0006239407230168581}, {"id": 669, "seek": 401348, "start": 4026.76, "end": 4029.2400000000002, "text": " You've got a question here, just in the black top.", "tokens": [51028, 509, 600, 658, 257, 1168, 510, 11, 445, 294, 264, 2211, 1192, 13, 51152], "temperature": 0.0, "avg_logprob": -0.14781174569759728, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0006239407230168581}, {"id": 670, "seek": 401348, "start": 4033.72, "end": 4039.16, "text": " Thank you very much. That was very, very good. Very interesting. How do large language models", "tokens": [51376, 1044, 291, 588, 709, 13, 663, 390, 588, 11, 588, 665, 13, 4372, 1880, 13, 1012, 360, 2416, 2856, 5245, 51648], "temperature": 0.0, "avg_logprob": -0.14781174569759728, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0006239407230168581}, {"id": 671, "seek": 403916, "start": 4039.24, "end": 4044.2, "text": " correct for different spoken languages? And do you find that the level of responses", "tokens": [50368, 3006, 337, 819, 10759, 8650, 30, 400, 360, 291, 915, 300, 264, 1496, 295, 13019, 50616], "temperature": 0.0, "avg_logprob": -0.09594440460205078, "compression_ratio": 1.636, "no_speech_prob": 0.005079630296677351}, {"id": 672, "seek": 403916, "start": 4044.7599999999998, "end": 4049.64, "text": " across different languages vary enormously in their depth?", "tokens": [50644, 2108, 819, 8650, 10559, 39669, 294, 641, 7161, 30, 50888], "temperature": 0.0, "avg_logprob": -0.09594440460205078, "compression_ratio": 1.636, "no_speech_prob": 0.005079630296677351}, {"id": 673, "seek": 403916, "start": 4049.64, "end": 4056.2, "text": " Right. Good question. And that's the focus of a huge amount of research right now.", "tokens": [50888, 1779, 13, 2205, 1168, 13, 400, 300, 311, 264, 1879, 295, 257, 2603, 2372, 295, 2132, 558, 586, 13, 51216], "temperature": 0.0, "avg_logprob": -0.09594440460205078, "compression_ratio": 1.636, "no_speech_prob": 0.005079630296677351}, {"id": 674, "seek": 403916, "start": 4056.2, "end": 4061.56, "text": " And I say the big problem is that most digital text in the world, the vast majority of it,", "tokens": [51216, 400, 286, 584, 264, 955, 1154, 307, 300, 881, 4562, 2487, 294, 264, 1002, 11, 264, 8369, 6286, 295, 309, 11, 51484], "temperature": 0.0, "avg_logprob": -0.09594440460205078, "compression_ratio": 1.636, "no_speech_prob": 0.005079630296677351}, {"id": 675, "seek": 403916, "start": 4061.56, "end": 4067.24, "text": " is in English and in North American English. And so languages with a small digital footprint", "tokens": [51484, 307, 294, 3669, 293, 294, 4067, 2665, 3669, 13, 400, 370, 8650, 365, 257, 1359, 4562, 24222, 51768], "temperature": 0.0, "avg_logprob": -0.09594440460205078, "compression_ratio": 1.636, "no_speech_prob": 0.005079630296677351}, {"id": 676, "seek": 406724, "start": 4067.24, "end": 4072.6, "text": " end up being massively marginalized in this. So there's a huge amount of work that's going on", "tokens": [50364, 917, 493, 885, 29379, 32522, 294, 341, 13, 407, 456, 311, 257, 2603, 2372, 295, 589, 300, 311, 516, 322, 50632], "temperature": 0.0, "avg_logprob": -0.08111768502455491, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.0007639132672920823}, {"id": 677, "seek": 406724, "start": 4072.6, "end": 4077.8799999999997, "text": " to try to deal with this problem. Let me tell you a really interesting aspect of this, though.", "tokens": [50632, 281, 853, 281, 2028, 365, 341, 1154, 13, 961, 385, 980, 291, 257, 534, 1880, 4171, 295, 341, 11, 1673, 13, 50896], "temperature": 0.0, "avg_logprob": -0.08111768502455491, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.0007639132672920823}, {"id": 678, "seek": 406724, "start": 4077.8799999999997, "end": 4082.68, "text": " The languages that have a small digital footprint, can you guess what the most", "tokens": [50896, 440, 8650, 300, 362, 257, 1359, 4562, 24222, 11, 393, 291, 2041, 437, 264, 881, 51136], "temperature": 0.0, "avg_logprob": -0.08111768502455491, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.0007639132672920823}, {"id": 679, "seek": 406724, "start": 4082.68, "end": 4090.6, "text": " digital texts that are available are actually concerned with? Religion. Right? So languages", "tokens": [51136, 4562, 15765, 300, 366, 2435, 366, 767, 5922, 365, 30, 40127, 13, 1779, 30, 407, 8650, 51532], "temperature": 0.0, "avg_logprob": -0.08111768502455491, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.0007639132672920823}, {"id": 680, "seek": 406724, "start": 4090.6, "end": 4095.0, "text": " that don't have a big digital presence, where they do have a big digital presence, it turns", "tokens": [51532, 300, 500, 380, 362, 257, 955, 4562, 6814, 11, 689, 436, 360, 362, 257, 955, 4562, 6814, 11, 309, 4523, 51752], "temperature": 0.0, "avg_logprob": -0.08111768502455491, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.0007639132672920823}, {"id": 681, "seek": 409500, "start": 4095.0, "end": 4101.32, "text": " out that the main texts which are available are religious texts. Now, I'm not a religious person", "tokens": [50364, 484, 300, 264, 2135, 15765, 597, 366, 2435, 366, 7185, 15765, 13, 823, 11, 286, 478, 406, 257, 7185, 954, 50680], "temperature": 0.0, "avg_logprob": -0.09793394490292198, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0027114569675177336}, {"id": 682, "seek": 409500, "start": 4101.32, "end": 4107.32, "text": " myself, but the idea of a kind of Old Testament large language model, frankly, I find a little", "tokens": [50680, 2059, 11, 457, 264, 1558, 295, 257, 733, 295, 8633, 15473, 2416, 2856, 2316, 11, 11939, 11, 286, 915, 257, 707, 50980], "temperature": 0.0, "avg_logprob": -0.09793394490292198, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0027114569675177336}, {"id": 683, "seek": 409500, "start": 4107.32, "end": 4111.08, "text": " bit terrifying. But that's exactly the kind of issue that people are grappling with. There are", "tokens": [50980, 857, 18106, 13, 583, 300, 311, 2293, 264, 733, 295, 2734, 300, 561, 366, 50086, 365, 13, 821, 366, 51168], "temperature": 0.0, "avg_logprob": -0.09793394490292198, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0027114569675177336}, {"id": 684, "seek": 409500, "start": 4111.08, "end": 4116.36, "text": " no fixes at the moment, but people are working on it very, very hard. And really what this relates", "tokens": [51168, 572, 32539, 412, 264, 1623, 11, 457, 561, 366, 1364, 322, 309, 588, 11, 588, 1152, 13, 400, 534, 437, 341, 16155, 51432], "temperature": 0.0, "avg_logprob": -0.09793394490292198, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0027114569675177336}, {"id": 685, "seek": 409500, "start": 4116.36, "end": 4123.8, "text": " to is the problem of that you're being lazy with these large language models and that you're just", "tokens": [51432, 281, 307, 264, 1154, 295, 300, 291, 434, 885, 14847, 365, 613, 2416, 2856, 5245, 293, 300, 291, 434, 445, 51804], "temperature": 0.0, "avg_logprob": -0.09793394490292198, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.0027114569675177336}, {"id": 686, "seek": 412380, "start": 4123.8, "end": 4128.76, "text": " throwing massive, massive amounts of text. We've got to make the technology much more efficient in", "tokens": [50364, 10238, 5994, 11, 5994, 11663, 295, 2487, 13, 492, 600, 658, 281, 652, 264, 2899, 709, 544, 7148, 294, 50612], "temperature": 0.0, "avg_logprob": -0.13583863166070753, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0019450181862339377}, {"id": 687, "seek": 412380, "start": 4128.76, "end": 4134.52, "text": " terms of learning. Awesome. Thank you. If you have a question, we have one right at the front", "tokens": [50612, 2115, 295, 2539, 13, 10391, 13, 1044, 291, 13, 759, 291, 362, 257, 1168, 11, 321, 362, 472, 558, 412, 264, 1868, 50900], "temperature": 0.0, "avg_logprob": -0.13583863166070753, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0019450181862339377}, {"id": 688, "seek": 412380, "start": 4134.52, "end": 4141.320000000001, "text": " in the center here. Thank you. Thank you very much for that. One of the big questions is obviously", "tokens": [50900, 294, 264, 3056, 510, 13, 1044, 291, 13, 1044, 291, 588, 709, 337, 300, 13, 1485, 295, 264, 955, 1651, 307, 2745, 51240], "temperature": 0.0, "avg_logprob": -0.13583863166070753, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0019450181862339377}, {"id": 689, "seek": 412380, "start": 4141.320000000001, "end": 4148.52, "text": " climate change. The models require a huge amount of energy to run. Generating pictures of cats or", "tokens": [51240, 5659, 1319, 13, 440, 5245, 3651, 257, 2603, 2372, 295, 2281, 281, 1190, 13, 15409, 990, 5242, 295, 11111, 420, 51600], "temperature": 0.0, "avg_logprob": -0.13583863166070753, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0019450181862339377}, {"id": 690, "seek": 414852, "start": 4148.52, "end": 4154.360000000001, "text": " silly gooses, geese and stuff, are obviously using lots of energy. Do you think we reach a point where", "tokens": [50364, 11774, 352, 4201, 11, 1519, 1130, 293, 1507, 11, 366, 2745, 1228, 3195, 295, 2281, 13, 1144, 291, 519, 321, 2524, 257, 935, 689, 50656], "temperature": 0.0, "avg_logprob": -0.15971650227461712, "compression_ratio": 1.4903474903474903, "no_speech_prob": 0.002401463221758604}, {"id": 691, "seek": 414852, "start": 4155.56, "end": 4161.4800000000005, "text": " Generative AI will help us solve our issue with climate change or will it burn us in the process?", "tokens": [50716, 15409, 1166, 7318, 486, 854, 505, 5039, 527, 2734, 365, 5659, 1319, 420, 486, 309, 5064, 505, 294, 264, 1399, 30, 51012], "temperature": 0.0, "avg_logprob": -0.15971650227461712, "compression_ratio": 1.4903474903474903, "no_speech_prob": 0.002401463221758604}, {"id": 692, "seek": 414852, "start": 4161.4800000000005, "end": 4167.0, "text": " So I think, okay, so two things to say. I absolutely am not defending the CO2 emissions,", "tokens": [51012, 407, 286, 519, 11, 1392, 11, 370, 732, 721, 281, 584, 13, 286, 3122, 669, 406, 21377, 264, 3002, 17, 14607, 11, 51288], "temperature": 0.0, "avg_logprob": -0.15971650227461712, "compression_ratio": 1.4903474903474903, "no_speech_prob": 0.002401463221758604}, {"id": 693, "seek": 414852, "start": 4167.0, "end": 4172.120000000001, "text": " but we need to put that into some perspective. So if I fly to New York from London, I think it's", "tokens": [51288, 457, 321, 643, 281, 829, 300, 666, 512, 4585, 13, 407, 498, 286, 3603, 281, 1873, 3609, 490, 7042, 11, 286, 519, 309, 311, 51544], "temperature": 0.0, "avg_logprob": -0.15971650227461712, "compression_ratio": 1.4903474903474903, "no_speech_prob": 0.002401463221758604}, {"id": 694, "seek": 417212, "start": 4172.12, "end": 4178.5199999999995, "text": " some like two tons of CO2 that I pump into the atmosphere through that. So the machine learning", "tokens": [50364, 512, 411, 732, 9131, 295, 3002, 17, 300, 286, 5889, 666, 264, 8018, 807, 300, 13, 407, 264, 3479, 2539, 50684], "temperature": 0.0, "avg_logprob": -0.09595942223209074, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.006405747961252928}, {"id": 695, "seek": 417212, "start": 4178.5199999999995, "end": 4184.04, "text": " community has some big conferences which attract like 20,000 people from across the world. Now,", "tokens": [50684, 1768, 575, 512, 955, 22032, 597, 5049, 411, 945, 11, 1360, 561, 490, 2108, 264, 1002, 13, 823, 11, 50960], "temperature": 0.0, "avg_logprob": -0.09595942223209074, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.006405747961252928}, {"id": 696, "seek": 417212, "start": 4184.04, "end": 4189.08, "text": " if you think each of them generating five tons of CO2 on their journey, that I think is probably a", "tokens": [50960, 498, 291, 519, 1184, 295, 552, 17746, 1732, 9131, 295, 3002, 17, 322, 641, 4671, 11, 300, 286, 519, 307, 1391, 257, 51212], "temperature": 0.0, "avg_logprob": -0.09595942223209074, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.006405747961252928}, {"id": 697, "seek": 417212, "start": 4189.08, "end": 4196.68, "text": " bigger climate problem for that community. But nevertheless, people are very aware of that", "tokens": [51212, 3801, 5659, 1154, 337, 300, 1768, 13, 583, 26924, 11, 561, 366, 588, 3650, 295, 300, 51592], "temperature": 0.0, "avg_logprob": -0.09595942223209074, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.006405747961252928}, {"id": 698, "seek": 419668, "start": 4196.68, "end": 4203.400000000001, "text": " problem and I think it clearly needs to be fixed. I think though, helping with climate change, I", "tokens": [50364, 1154, 293, 286, 519, 309, 4448, 2203, 281, 312, 6806, 13, 286, 519, 1673, 11, 4315, 365, 5659, 1319, 11, 286, 50700], "temperature": 0.0, "avg_logprob": -0.09421595748589963, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.004038478247821331}, {"id": 699, "seek": 419668, "start": 4203.400000000001, "end": 4209.4800000000005, "text": " don't think you need larger language models for that. I mean, I think AI itself can just be enormously", "tokens": [50700, 500, 380, 519, 291, 643, 4833, 2856, 5245, 337, 300, 13, 286, 914, 11, 286, 519, 7318, 2564, 393, 445, 312, 39669, 51004], "temperature": 0.0, "avg_logprob": -0.09421595748589963, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.004038478247821331}, {"id": 700, "seek": 419668, "start": 4209.4800000000005, "end": 4213.400000000001, "text": " helpful in order to be able to ameliorate that and we're doing a lot of work on that at the Turing", "tokens": [51004, 4961, 294, 1668, 281, 312, 1075, 281, 669, 338, 1973, 473, 300, 293, 321, 434, 884, 257, 688, 295, 589, 322, 300, 412, 264, 314, 1345, 51200], "temperature": 0.0, "avg_logprob": -0.09421595748589963, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.004038478247821331}, {"id": 701, "seek": 419668, "start": 4213.400000000001, "end": 4221.400000000001, "text": " Institute. For example, just on helping systems be more efficient, heating systems be more efficient.", "tokens": [51200, 9446, 13, 1171, 1365, 11, 445, 322, 4315, 3652, 312, 544, 7148, 11, 15082, 3652, 312, 544, 7148, 13, 51600], "temperature": 0.0, "avg_logprob": -0.09421595748589963, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.004038478247821331}, {"id": 702, "seek": 422140, "start": 4221.4, "end": 4226.759999999999, "text": " There was a nice example, I think, from DeepMind with their data centers, the cooling in their", "tokens": [50364, 821, 390, 257, 1481, 1365, 11, 286, 519, 11, 490, 14895, 44, 471, 365, 641, 1412, 10898, 11, 264, 14785, 294, 641, 50632], "temperature": 0.0, "avg_logprob": -0.09887512173272867, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.009003015235066414}, {"id": 703, "seek": 422140, "start": 4226.759999999999, "end": 4231.799999999999, "text": " data centers and basically just trying to predict the usage of it. If you can reliably predict the", "tokens": [50632, 1412, 10898, 293, 1936, 445, 1382, 281, 6069, 264, 14924, 295, 309, 13, 759, 291, 393, 49927, 6069, 264, 50884], "temperature": 0.0, "avg_logprob": -0.09887512173272867, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.009003015235066414}, {"id": 704, "seek": 422140, "start": 4231.799999999999, "end": 4236.599999999999, "text": " usage of it, then you can predict the cooling requirements much more effectively and end up", "tokens": [50884, 14924, 295, 309, 11, 550, 291, 393, 6069, 264, 14785, 7728, 709, 544, 8659, 293, 917, 493, 51124], "temperature": 0.0, "avg_logprob": -0.09887512173272867, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.009003015235066414}, {"id": 705, "seek": 422140, "start": 4236.599999999999, "end": 4242.28, "text": " with much, much, much better use of power and that can go down to the level of individual homes.", "tokens": [51124, 365, 709, 11, 709, 11, 709, 1101, 764, 295, 1347, 293, 300, 393, 352, 760, 281, 264, 1496, 295, 2609, 7388, 13, 51408], "temperature": 0.0, "avg_logprob": -0.09887512173272867, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.009003015235066414}, {"id": 706, "seek": 422140, "start": 4243.16, "end": 4248.92, "text": " So there are lots of applications of AI, I think, not just large language models, lots of applications", "tokens": [51452, 407, 456, 366, 3195, 295, 5821, 295, 7318, 11, 286, 519, 11, 406, 445, 2416, 2856, 5245, 11, 3195, 295, 5821, 51740], "temperature": 0.0, "avg_logprob": -0.09887512173272867, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.009003015235066414}, {"id": 707, "seek": 424892, "start": 4248.92, "end": 4255.08, "text": " of AI that are going to help us with that problem. But yeah, I think this brute force approach,", "tokens": [50364, 295, 7318, 300, 366, 516, 281, 854, 505, 365, 300, 1154, 13, 583, 1338, 11, 286, 519, 341, 47909, 3464, 3109, 11, 50672], "temperature": 0.0, "avg_logprob": -0.12808274339746545, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0007866594241932034}, {"id": 708, "seek": 424892, "start": 4256.12, "end": 4261.56, "text": " just supercomputers running for months with vast amounts of data is clearly an ugly solution.", "tokens": [50724, 445, 27839, 2582, 433, 2614, 337, 2493, 365, 8369, 11663, 295, 1412, 307, 4448, 364, 12246, 3827, 13, 50996], "temperature": 0.0, "avg_logprob": -0.12808274339746545, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0007866594241932034}, {"id": 709, "seek": 424892, "start": 4261.56, "end": 4264.92, "text": " I think it will probably be a transitory phase. I think we will get beyond it.", "tokens": [50996, 286, 519, 309, 486, 1391, 312, 257, 17976, 827, 5574, 13, 286, 519, 321, 486, 483, 4399, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12808274339746545, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0007866594241932034}, {"id": 710, "seek": 424892, "start": 4266.28, "end": 4269.0, "text": " Thank you. Swing to the left over here. There's one right at the back", "tokens": [51232, 1044, 291, 13, 3926, 278, 281, 264, 1411, 670, 510, 13, 821, 311, 472, 558, 412, 264, 646, 51368], "temperature": 0.0, "avg_logprob": -0.12808274339746545, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0007866594241932034}, {"id": 711, "seek": 424892, "start": 4269.56, "end": 4273.16, "text": " at the top over here. Watch our hearts. I'm going to get a mic across.", "tokens": [51396, 412, 264, 1192, 670, 510, 13, 7277, 527, 8852, 13, 286, 478, 516, 281, 483, 257, 3123, 2108, 13, 51576], "temperature": 0.0, "avg_logprob": -0.12808274339746545, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0007866594241932034}, {"id": 712, "seek": 427316, "start": 4273.8, "end": 4283.96, "text": " Thank you very much. I've got a sort of more philosophical question.", "tokens": [50396, 1044, 291, 588, 709, 13, 286, 600, 658, 257, 1333, 295, 544, 25066, 1168, 13, 50904], "temperature": 0.0, "avg_logprob": -0.15970450008616727, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0013530688593164086}, {"id": 713, "seek": 427316, "start": 4283.96, "end": 4289.16, "text": " You've talked about general AI and the sort of peak of general AI is its ability to", "tokens": [50904, 509, 600, 2825, 466, 2674, 7318, 293, 264, 1333, 295, 10651, 295, 2674, 7318, 307, 1080, 3485, 281, 51164], "temperature": 0.0, "avg_logprob": -0.15970450008616727, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0013530688593164086}, {"id": 714, "seek": 427316, "start": 4289.16, "end": 4294.76, "text": " mimic a human and all the things a human can do. Can you envision a path whereby AI could actually", "tokens": [51164, 31075, 257, 1952, 293, 439, 264, 721, 257, 1952, 393, 360, 13, 1664, 291, 24739, 257, 3100, 36998, 7318, 727, 767, 51444], "temperature": 0.0, "avg_logprob": -0.15970450008616727, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0013530688593164086}, {"id": 715, "seek": 427316, "start": 4294.76, "end": 4300.2, "text": " become superhuman so it starts to solve problems or ask questions that we haven't tried to do ourselves?", "tokens": [51444, 1813, 1687, 18796, 370, 309, 3719, 281, 5039, 2740, 420, 1029, 1651, 300, 321, 2378, 380, 3031, 281, 360, 4175, 30, 51716], "temperature": 0.0, "avg_logprob": -0.15970450008616727, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0013530688593164086}, {"id": 716, "seek": 430020, "start": 4301.16, "end": 4310.599999999999, "text": " This is another well trodden question, which I always dread, I have to say,", "tokens": [50412, 639, 307, 1071, 731, 4495, 67, 1556, 1168, 11, 597, 286, 1009, 22236, 11, 286, 362, 281, 584, 11, 50884], "temperature": 0.0, "avg_logprob": -0.15361005326975946, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006430791690945625}, {"id": 717, "seek": 430020, "start": 4311.4, "end": 4315.48, "text": " but it's a perfectly reasonable question. So I think what you're hinting at is something that in", "tokens": [50924, 457, 309, 311, 257, 6239, 10585, 1168, 13, 407, 286, 519, 437, 291, 434, 12075, 278, 412, 307, 746, 300, 294, 51128], "temperature": 0.0, "avg_logprob": -0.15361005326975946, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006430791690945625}, {"id": 718, "seek": 430020, "start": 4315.48, "end": 4321.32, "text": " the AI community is called the singularity. The argument of the singularity goes as possible.", "tokens": [51128, 264, 7318, 1768, 307, 1219, 264, 20010, 507, 13, 440, 6770, 295, 264, 20010, 507, 1709, 382, 1944, 13, 51420], "temperature": 0.0, "avg_logprob": -0.15361005326975946, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006430791690945625}, {"id": 719, "seek": 430020, "start": 4321.32, "end": 4327.16, "text": " At some point as follows, at some point in the future, we're going to have AI which is as intelligent", "tokens": [51420, 1711, 512, 935, 382, 10002, 11, 412, 512, 935, 294, 264, 2027, 11, 321, 434, 516, 281, 362, 7318, 597, 307, 382, 13232, 51712], "temperature": 0.0, "avg_logprob": -0.15361005326975946, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006430791690945625}, {"id": 720, "seek": 432716, "start": 4327.24, "end": 4332.68, "text": " as human beings in the general sense. That is, it will be able to do any intellectual task", "tokens": [50368, 382, 1952, 8958, 294, 264, 2674, 2020, 13, 663, 307, 11, 309, 486, 312, 1075, 281, 360, 604, 12576, 5633, 50640], "temperature": 0.0, "avg_logprob": -0.08794523823645807, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.004386184737086296}, {"id": 721, "seek": 432716, "start": 4332.68, "end": 4338.36, "text": " that a human being can do. And then there's an idea that, well, that AI can look at its own code", "tokens": [50640, 300, 257, 1952, 885, 393, 360, 13, 400, 550, 456, 311, 364, 1558, 300, 11, 731, 11, 300, 7318, 393, 574, 412, 1080, 1065, 3089, 50924], "temperature": 0.0, "avg_logprob": -0.08794523823645807, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.004386184737086296}, {"id": 722, "seek": 432716, "start": 4338.36, "end": 4343.16, "text": " and make itself better, right? Because it can code. It can start to improve its own code.", "tokens": [50924, 293, 652, 2564, 1101, 11, 558, 30, 1436, 309, 393, 3089, 13, 467, 393, 722, 281, 3470, 1080, 1065, 3089, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08794523823645807, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.004386184737086296}, {"id": 723, "seek": 432716, "start": 4343.16, "end": 4350.12, "text": " And the point is, once it's a tiny way beyond us, then the concern is that it's out of control", "tokens": [51164, 400, 264, 935, 307, 11, 1564, 309, 311, 257, 5870, 636, 4399, 505, 11, 550, 264, 3136, 307, 300, 309, 311, 484, 295, 1969, 51512], "temperature": 0.0, "avg_logprob": -0.08794523823645807, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.004386184737086296}, {"id": 724, "seek": 432716, "start": 4350.12, "end": 4355.8, "text": " at that point, that we really don't understand it. So the community is a bit divided on this.", "tokens": [51512, 412, 300, 935, 11, 300, 321, 534, 500, 380, 1223, 309, 13, 407, 264, 1768, 307, 257, 857, 6666, 322, 341, 13, 51796], "temperature": 0.0, "avg_logprob": -0.08794523823645807, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.004386184737086296}, {"id": 725, "seek": 435580, "start": 4355.8, "end": 4361.56, "text": " I think some people think that's science fiction. Some people think it's a plausible scenario that", "tokens": [50364, 286, 519, 512, 561, 519, 300, 311, 3497, 13266, 13, 2188, 561, 519, 309, 311, 257, 39925, 9005, 300, 50652], "temperature": 0.0, "avg_logprob": -0.06368029772580325, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0024611600674688816}, {"id": 726, "seek": 435580, "start": 4361.56, "end": 4367.96, "text": " we need to prepare for and think for. I'm completely comfortable with the idea. I think it is just", "tokens": [50652, 321, 643, 281, 5940, 337, 293, 519, 337, 13, 286, 478, 2584, 4619, 365, 264, 1558, 13, 286, 519, 309, 307, 445, 50972], "temperature": 0.0, "avg_logprob": -0.06368029772580325, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0024611600674688816}, {"id": 727, "seek": 435580, "start": 4367.96, "end": 4375.24, "text": " simply good sense to take that potential issue seriously and to think about how we might mitigate", "tokens": [50972, 2935, 665, 2020, 281, 747, 300, 3995, 2734, 6638, 293, 281, 519, 466, 577, 321, 1062, 27336, 51336], "temperature": 0.0, "avg_logprob": -0.06368029772580325, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0024611600674688816}, {"id": 728, "seek": 435580, "start": 4375.24, "end": 4381.08, "text": " it. There are many ways of mitigating it. One of the ways of mitigating it is designing the AI", "tokens": [51336, 309, 13, 821, 366, 867, 2098, 295, 15699, 990, 309, 13, 1485, 295, 264, 2098, 295, 15699, 990, 309, 307, 14685, 264, 7318, 51628], "temperature": 0.0, "avg_logprob": -0.06368029772580325, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0024611600674688816}, {"id": 729, "seek": 438108, "start": 4381.08, "end": 4386.5199999999995, "text": " so that it is intrinsically designed to be helpful to us, that it's never going to be", "tokens": [50364, 370, 300, 309, 307, 28621, 984, 4761, 281, 312, 4961, 281, 505, 11, 300, 309, 311, 1128, 516, 281, 312, 50636], "temperature": 0.0, "avg_logprob": -0.11172539460743573, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.001427453593350947}, {"id": 730, "seek": 438108, "start": 4386.5199999999995, "end": 4394.5199999999995, "text": " unhelpful to us. But I have to tell you, it is not at all a universally held belief that that's", "tokens": [50636, 517, 37451, 906, 281, 505, 13, 583, 286, 362, 281, 980, 291, 11, 309, 307, 406, 412, 439, 257, 43995, 5167, 7107, 300, 300, 311, 51036], "temperature": 0.0, "avg_logprob": -0.11172539460743573, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.001427453593350947}, {"id": 731, "seek": 438108, "start": 4394.5199999999995, "end": 4399.16, "text": " where we're going in AI. There are still big, big problems to overcome before we get there.", "tokens": [51036, 689, 321, 434, 516, 294, 7318, 13, 821, 366, 920, 955, 11, 955, 2740, 281, 10473, 949, 321, 483, 456, 13, 51268], "temperature": 0.0, "avg_logprob": -0.11172539460743573, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.001427453593350947}, {"id": 732, "seek": 438108, "start": 4399.96, "end": 4403.4, "text": " I'm not sure that's an entirely reassuring answer, but that's the best I've got to offer.", "tokens": [51308, 286, 478, 406, 988, 300, 311, 364, 7696, 19486, 1345, 1867, 11, 457, 300, 311, 264, 1151, 286, 600, 658, 281, 2626, 13, 51480], "temperature": 0.0, "avg_logprob": -0.11172539460743573, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.001427453593350947}, {"id": 733, "seek": 438108, "start": 4404.2, "end": 4406.76, "text": " Great. Thanks, Mike. Well, just pop it online with us, Anne.", "tokens": [51520, 3769, 13, 2561, 11, 6602, 13, 1042, 11, 445, 1665, 309, 2950, 365, 505, 11, 13706, 13, 51648], "temperature": 0.0, "avg_logprob": -0.11172539460743573, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.001427453593350947}, {"id": 734, "seek": 438108, "start": 4406.76, "end": 4410.5199999999995, "text": " Yeah. So we've had questions from all over the world. We have Peter tuning in from Switzerland,", "tokens": [51648, 865, 13, 407, 321, 600, 632, 1651, 490, 439, 670, 264, 1002, 13, 492, 362, 6508, 15164, 294, 490, 23312, 11, 51836], "temperature": 0.0, "avg_logprob": -0.11172539460743573, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.001427453593350947}, {"id": 735, "seek": 441052, "start": 4410.52, "end": 4420.6, "text": " London, Birmingham. But the question I'm going to focus on. So the question is going to be on", "tokens": [50364, 7042, 11, 34673, 13, 583, 264, 1168, 286, 478, 516, 281, 1879, 322, 13, 407, 264, 1168, 307, 516, 281, 312, 322, 50868], "temperature": 0.0, "avg_logprob": -0.1561159836618524, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.0005755426245741546}, {"id": 736, "seek": 441052, "start": 4420.6, "end": 4424.84, "text": " the tuning test and whether that's still relevant and whether we have AI that has passed the", "tokens": [50868, 264, 15164, 1500, 293, 1968, 300, 311, 920, 7340, 293, 1968, 321, 362, 7318, 300, 575, 4678, 264, 51080], "temperature": 0.0, "avg_logprob": -0.1561159836618524, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.0005755426245741546}, {"id": 737, "seek": 441052, "start": 4424.84, "end": 4431.240000000001, "text": " Turing test. Oh, the Turing test. Okay. So the Turing test, we saw Alan Turing up there,", "tokens": [51080, 314, 1345, 1500, 13, 876, 11, 264, 314, 1345, 1500, 13, 1033, 13, 407, 264, 314, 1345, 1500, 11, 321, 1866, 16442, 314, 1345, 493, 456, 11, 51400], "temperature": 0.0, "avg_logprob": -0.1561159836618524, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.0005755426245741546}, {"id": 738, "seek": 441052, "start": 4433.64, "end": 4440.120000000001, "text": " a national hero. Turing 1950, first digital computers have appeared and Turing's working", "tokens": [51520, 257, 4048, 5316, 13, 314, 1345, 18141, 11, 700, 4562, 10807, 362, 8516, 293, 314, 1345, 311, 1364, 51844], "temperature": 0.0, "avg_logprob": -0.1561159836618524, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.0005755426245741546}, {"id": 739, "seek": 444012, "start": 4440.12, "end": 4445.32, "text": " on one at the University of Manchester. And the idea of AI as in the air hasn't got a name yet,", "tokens": [50364, 322, 472, 412, 264, 3535, 295, 27180, 13, 400, 264, 1558, 295, 7318, 382, 294, 264, 1988, 6132, 380, 658, 257, 1315, 1939, 11, 50624], "temperature": 0.0, "avg_logprob": -0.08624516871937535, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.003607909893617034}, {"id": 740, "seek": 444012, "start": 4445.32, "end": 4449.32, "text": " but people are talking about electronic brains and getting very excited about what they can do.", "tokens": [50624, 457, 561, 366, 1417, 466, 10092, 15442, 293, 1242, 588, 2919, 466, 437, 436, 393, 360, 13, 50824], "temperature": 0.0, "avg_logprob": -0.08624516871937535, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.003607909893617034}, {"id": 741, "seek": 444012, "start": 4449.32, "end": 4454.68, "text": " So people are starting to think about the ideas that become AI. And Turing gets frustrated with", "tokens": [50824, 407, 561, 366, 2891, 281, 519, 466, 264, 3487, 300, 1813, 7318, 13, 400, 314, 1345, 2170, 15751, 365, 51092], "temperature": 0.0, "avg_logprob": -0.08624516871937535, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.003607909893617034}, {"id": 742, "seek": 444012, "start": 4454.68, "end": 4459.64, "text": " people saying, well, of course, it would never actually really, really be able to think or never", "tokens": [51092, 561, 1566, 11, 731, 11, 295, 1164, 11, 309, 576, 1128, 767, 534, 11, 534, 312, 1075, 281, 519, 420, 1128, 51340], "temperature": 0.0, "avg_logprob": -0.08624516871937535, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.003607909893617034}, {"id": 743, "seek": 444012, "start": 4459.64, "end": 4464.599999999999, "text": " really be able to understand and so on. So he comes up with the following test in order to just", "tokens": [51340, 534, 312, 1075, 281, 1223, 293, 370, 322, 13, 407, 415, 1487, 493, 365, 264, 3480, 1500, 294, 1668, 281, 445, 51588], "temperature": 0.0, "avg_logprob": -0.08624516871937535, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.003607909893617034}, {"id": 744, "seek": 446460, "start": 4464.6, "end": 4469.96, "text": " really to try and shut people up talking about it. And the paper is called Computing Machinery", "tokens": [50364, 534, 281, 853, 293, 5309, 561, 493, 1417, 466, 309, 13, 400, 264, 3035, 307, 1219, 37804, 278, 12089, 23194, 50632], "temperature": 0.0, "avg_logprob": -0.09257368756155683, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.010016700252890587}, {"id": 745, "seek": 446460, "start": 4469.96, "end": 4476.52, "text": " Intelligence, and it's published in the journal Mind, which is a very respectable journal,", "tokens": [50632, 27274, 11, 293, 309, 311, 6572, 294, 264, 6708, 13719, 11, 597, 307, 257, 588, 44279, 6708, 11, 50960], "temperature": 0.0, "avg_logprob": -0.09257368756155683, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.010016700252890587}, {"id": 746, "seek": 446460, "start": 4476.52, "end": 4480.68, "text": " a very unusual paper. It's very readable, by the way. You can download it and read it.", "tokens": [50960, 257, 588, 10901, 3035, 13, 467, 311, 588, 49857, 11, 538, 264, 636, 13, 509, 393, 5484, 309, 293, 1401, 309, 13, 51168], "temperature": 0.0, "avg_logprob": -0.09257368756155683, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.010016700252890587}, {"id": 747, "seek": 446460, "start": 4480.68, "end": 4486.120000000001, "text": " But he proposes the Turing test. So Turing says, suppose we're trying to settle the question of", "tokens": [51168, 583, 415, 2365, 4201, 264, 314, 1345, 1500, 13, 407, 314, 1345, 1619, 11, 7297, 321, 434, 1382, 281, 11852, 264, 1168, 295, 51440], "temperature": 0.0, "avg_logprob": -0.09257368756155683, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.010016700252890587}, {"id": 748, "seek": 446460, "start": 4486.120000000001, "end": 4493.56, "text": " whether a machine can really think or understand. So here's a test for that. What you do is you", "tokens": [51440, 1968, 257, 3479, 393, 534, 519, 420, 1223, 13, 407, 510, 311, 257, 1500, 337, 300, 13, 708, 291, 360, 307, 291, 51812], "temperature": 0.0, "avg_logprob": -0.09257368756155683, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.010016700252890587}, {"id": 749, "seek": 449356, "start": 4493.56, "end": 4498.76, "text": " take that machine behind closed doors, and you get a human judge to be able to interact with", "tokens": [50364, 747, 300, 3479, 2261, 5395, 8077, 11, 293, 291, 483, 257, 1952, 6995, 281, 312, 1075, 281, 4648, 365, 50624], "temperature": 0.0, "avg_logprob": -0.10022115707397461, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0008082022541202605}, {"id": 750, "seek": 449356, "start": 4498.76, "end": 4503.96, "text": " something via a keyboard and a screen. In Turing's day, it would have been a teletype. Just by typing", "tokens": [50624, 746, 5766, 257, 10186, 293, 257, 2568, 13, 682, 314, 1345, 311, 786, 11, 309, 576, 362, 668, 257, 15284, 2210, 494, 13, 1449, 538, 18444, 50884], "temperature": 0.0, "avg_logprob": -0.10022115707397461, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0008082022541202605}, {"id": 751, "seek": 449356, "start": 4503.96, "end": 4509.240000000001, "text": " away questions, actually remarkably, pretty much what you do with chat GPT. Give it prompts", "tokens": [50884, 1314, 1651, 11, 767, 37381, 11, 1238, 709, 437, 291, 360, 365, 5081, 26039, 51, 13, 5303, 309, 41095, 51148], "temperature": 0.0, "avg_logprob": -0.10022115707397461, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0008082022541202605}, {"id": 752, "seek": 449356, "start": 4509.240000000001, "end": 4514.4400000000005, "text": " anything you like. And actually, Turing has some very entertaining ones in his paper. And what you", "tokens": [51148, 1340, 291, 411, 13, 400, 767, 11, 314, 1345, 575, 512, 588, 20402, 2306, 294, 702, 3035, 13, 400, 437, 291, 51408], "temperature": 0.0, "avg_logprob": -0.10022115707397461, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0008082022541202605}, {"id": 753, "seek": 449356, "start": 4514.4400000000005, "end": 4521.320000000001, "text": " try and do is you try to decide whether the thing on the other side is a computer or a human being.", "tokens": [51408, 853, 293, 360, 307, 291, 853, 281, 4536, 1968, 264, 551, 322, 264, 661, 1252, 307, 257, 3820, 420, 257, 1952, 885, 13, 51752], "temperature": 0.0, "avg_logprob": -0.10022115707397461, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0008082022541202605}, {"id": 754, "seek": 452132, "start": 4521.32, "end": 4527.799999999999, "text": " And Turing's point was, if you cannot reliably tell that the thing on the other side is a human", "tokens": [50364, 400, 314, 1345, 311, 935, 390, 11, 498, 291, 2644, 49927, 980, 300, 264, 551, 322, 264, 661, 1252, 307, 257, 1952, 50688], "temperature": 0.0, "avg_logprob": -0.048887725603782524, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.0004482579242903739}, {"id": 755, "seek": 452132, "start": 4527.799999999999, "end": 4533.48, "text": " being or a machine, and it really is a machine, then you should accept that this thing has something", "tokens": [50688, 885, 420, 257, 3479, 11, 293, 309, 534, 307, 257, 3479, 11, 550, 291, 820, 3241, 300, 341, 551, 575, 746, 50972], "temperature": 0.0, "avg_logprob": -0.048887725603782524, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.0004482579242903739}, {"id": 756, "seek": 452132, "start": 4533.48, "end": 4538.92, "text": " like human intelligence, because you can't tell the difference. There's no test that you can apply", "tokens": [50972, 411, 1952, 7599, 11, 570, 291, 393, 380, 980, 264, 2649, 13, 821, 311, 572, 1500, 300, 291, 393, 3079, 51244], "temperature": 0.0, "avg_logprob": -0.048887725603782524, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.0004482579242903739}, {"id": 757, "seek": 452132, "start": 4538.92, "end": 4542.5199999999995, "text": " without actually pulling back the curtain and looking to see what's there that's going to", "tokens": [51244, 1553, 767, 8407, 646, 264, 26789, 293, 1237, 281, 536, 437, 311, 456, 300, 311, 516, 281, 51424], "temperature": 0.0, "avg_logprob": -0.048887725603782524, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.0004482579242903739}, {"id": 758, "seek": 452132, "start": 4542.5199999999995, "end": 4547.88, "text": " show you whether it's a human or a machine. You can't tell the difference. It's indistinguishable.", "tokens": [51424, 855, 291, 1968, 309, 311, 257, 1952, 420, 257, 3479, 13, 509, 393, 380, 980, 264, 2649, 13, 467, 311, 1016, 468, 7050, 742, 712, 13, 51692], "temperature": 0.0, "avg_logprob": -0.048887725603782524, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.0004482579242903739}, {"id": 759, "seek": 454788, "start": 4548.6, "end": 4553.96, "text": " So this was important historically, because it really gave AI people a target. When you said", "tokens": [50400, 407, 341, 390, 1021, 16180, 11, 570, 309, 534, 2729, 7318, 561, 257, 3779, 13, 1133, 291, 848, 50668], "temperature": 0.0, "avg_logprob": -0.09868319829305013, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.0004993108450435102}, {"id": 760, "seek": 454788, "start": 4553.96, "end": 4557.32, "text": " I'm an AI researcher, what are you trying to do? I'm trying to build a machine that can pass the", "tokens": [50668, 286, 478, 364, 7318, 21751, 11, 437, 366, 291, 1382, 281, 360, 30, 286, 478, 1382, 281, 1322, 257, 3479, 300, 393, 1320, 264, 50836], "temperature": 0.0, "avg_logprob": -0.09868319829305013, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.0004993108450435102}, {"id": 761, "seek": 454788, "start": 4557.32, "end": 4561.32, "text": " Turing test. There was a concrete goal. The problem is in science, whenever you work with", "tokens": [50836, 314, 1345, 1500, 13, 821, 390, 257, 9859, 3387, 13, 440, 1154, 307, 294, 3497, 11, 5699, 291, 589, 365, 51036], "temperature": 0.0, "avg_logprob": -0.09868319829305013, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.0004993108450435102}, {"id": 762, "seek": 454788, "start": 4561.32, "end": 4566.4400000000005, "text": " science and society, whenever you set up some challenge like that, you get all sorts of charlatans", "tokens": [51036, 3497, 293, 4086, 11, 5699, 291, 992, 493, 512, 3430, 411, 300, 11, 291, 483, 439, 7527, 295, 1290, 14087, 599, 51292], "temperature": 0.0, "avg_logprob": -0.09868319829305013, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.0004993108450435102}, {"id": 763, "seek": 454788, "start": 4566.4400000000005, "end": 4572.28, "text": " and idiots who just try and come up with ways of faking it. And so most of the ways of trying to get", "tokens": [51292, 293, 36454, 567, 445, 853, 293, 808, 493, 365, 2098, 295, 283, 2456, 309, 13, 400, 370, 881, 295, 264, 2098, 295, 1382, 281, 483, 51584], "temperature": 0.0, "avg_logprob": -0.09868319829305013, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.0004993108450435102}, {"id": 764, "seek": 457228, "start": 4572.28, "end": 4577.48, "text": " past the Turing test over the last 70 years have really just been systems that just come up with", "tokens": [50364, 1791, 264, 314, 1345, 1500, 670, 264, 1036, 5285, 924, 362, 534, 445, 668, 3652, 300, 445, 808, 493, 365, 50624], "temperature": 0.0, "avg_logprob": -0.08236728919731391, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.010756871663033962}, {"id": 765, "seek": 457228, "start": 4577.48, "end": 4583.08, "text": " kind of nonsense answers trying to confuse the questioner. But now we've got large language", "tokens": [50624, 733, 295, 14925, 6338, 1382, 281, 28584, 264, 1168, 260, 13, 583, 586, 321, 600, 658, 2416, 2856, 50904], "temperature": 0.0, "avg_logprob": -0.08236728919731391, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.010756871663033962}, {"id": 766, "seek": 457228, "start": 4583.08, "end": 4591.0, "text": " models. So we're going to find out in about 10 days time, we're going to run a live Turing test as", "tokens": [50904, 5245, 13, 407, 321, 434, 516, 281, 915, 484, 294, 466, 1266, 1708, 565, 11, 321, 434, 516, 281, 1190, 257, 1621, 314, 1345, 1500, 382, 51300], "temperature": 0.0, "avg_logprob": -0.08236728919731391, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.010756871663033962}, {"id": 767, "seek": 457228, "start": 4591.0, "end": 4597.4, "text": " part of the Christmas lectures, and we will see whether our audience can distinguish a large", "tokens": [51300, 644, 295, 264, 5272, 16564, 11, 293, 321, 486, 536, 1968, 527, 4034, 393, 20206, 257, 2416, 51620], "temperature": 0.0, "avg_logprob": -0.08236728919731391, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.010756871663033962}, {"id": 768, "seek": 459740, "start": 4597.4, "end": 4602.92, "text": " language model from a teenage child. And we've trialed this, and I have to tell you it's possibly", "tokens": [50364, 2856, 2316, 490, 257, 26866, 1440, 13, 400, 321, 600, 1376, 5573, 341, 11, 293, 286, 362, 281, 980, 291, 309, 311, 6264, 50640], "temperature": 0.0, "avg_logprob": -0.10514033209417284, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.002946906490251422}, {"id": 769, "seek": 459740, "start": 4602.92, "end": 4609.0, "text": " closer than you might think actually. Do I really think we passed the Turing test? Not in a deep", "tokens": [50640, 4966, 813, 291, 1062, 519, 767, 13, 1144, 286, 534, 519, 321, 4678, 264, 314, 1345, 1500, 30, 1726, 294, 257, 2452, 50944], "temperature": 0.0, "avg_logprob": -0.10514033209417284, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.002946906490251422}, {"id": 770, "seek": 459740, "start": 4609.0, "end": 4615.799999999999, "text": " sense, but what I think is that it's demonstrated to us firstly, machines clearly can generate text,", "tokens": [50944, 2020, 11, 457, 437, 286, 519, 307, 300, 309, 311, 18772, 281, 505, 27376, 11, 8379, 4448, 393, 8460, 2487, 11, 51284], "temperature": 0.0, "avg_logprob": -0.10514033209417284, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.002946906490251422}, {"id": 771, "seek": 459740, "start": 4615.799999999999, "end": 4620.36, "text": " which is indistinguishable from text that a human being could generate. We've done that,", "tokens": [51284, 597, 307, 1016, 468, 7050, 742, 712, 490, 2487, 300, 257, 1952, 885, 727, 8460, 13, 492, 600, 1096, 300, 11, 51512], "temperature": 0.0, "avg_logprob": -0.10514033209417284, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.002946906490251422}, {"id": 772, "seek": 462036, "start": 4620.36, "end": 4628.44, "text": " that box is ticked, and they can clearly understand text. So even if we haven't followed the Turing", "tokens": [50364, 300, 2424, 307, 5204, 292, 11, 293, 436, 393, 4448, 1223, 2487, 13, 407, 754, 498, 321, 2378, 380, 6263, 264, 314, 1345, 50768], "temperature": 0.0, "avg_logprob": -0.08009996828825577, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.017597820609807968}, {"id": 773, "seek": 462036, "start": 4628.44, "end": 4633.96, "text": " test to the letter, I think for all practical intents and purposes, the Turing test is now a", "tokens": [50768, 1500, 281, 264, 5063, 11, 286, 519, 337, 439, 8496, 560, 791, 293, 9932, 11, 264, 314, 1345, 1500, 307, 586, 257, 51044], "temperature": 0.0, "avg_logprob": -0.08009996828825577, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.017597820609807968}, {"id": 774, "seek": 462036, "start": 4633.96, "end": 4639.24, "text": " historical note. Yeah, but actually the Turing test only tests one little bit of intelligence.", "tokens": [51044, 8584, 3637, 13, 865, 11, 457, 767, 264, 314, 1345, 1500, 787, 6921, 472, 707, 857, 295, 7599, 13, 51308], "temperature": 0.0, "avg_logprob": -0.08009996828825577, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.017597820609807968}, {"id": 775, "seek": 462036, "start": 4639.24, "end": 4643.48, "text": " You remember those dimensions of intelligence that I showed you? There's a huge range of those", "tokens": [51308, 509, 1604, 729, 12819, 295, 7599, 300, 286, 4712, 291, 30, 821, 311, 257, 2603, 3613, 295, 729, 51520], "temperature": 0.0, "avg_logprob": -0.08009996828825577, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.017597820609807968}, {"id": 776, "seek": 462036, "start": 4643.48, "end": 4650.04, "text": " that it doesn't test. So it was historically important, and it's a big part of our historical", "tokens": [51520, 300, 309, 1177, 380, 1500, 13, 407, 309, 390, 16180, 1021, 11, 293, 309, 311, 257, 955, 644, 295, 527, 8584, 51848], "temperature": 0.0, "avg_logprob": -0.08009996828825577, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.017597820609807968}, {"id": 777, "seek": 465004, "start": 4650.04, "end": 4657.16, "text": " legacy, but maybe not a core target for AI today. Cool. Thank you, Mike. I think now you've been", "tokens": [50364, 11711, 11, 457, 1310, 406, 257, 4965, 3779, 337, 7318, 965, 13, 8561, 13, 1044, 291, 11, 6602, 13, 286, 519, 586, 291, 600, 668, 50720], "temperature": 0.0, "avg_logprob": -0.15794209352473623, "compression_ratio": 1.5625, "no_speech_prob": 0.0004266327596269548}, {"id": 778, "seek": 465004, "start": 4657.16, "end": 4661.16, "text": " the warning when you say a lot of searches for preparing for the Turing test for the Christmas", "tokens": [50720, 264, 9164, 562, 291, 584, 257, 688, 295, 26701, 337, 10075, 337, 264, 314, 1345, 1500, 337, 264, 5272, 50920], "temperature": 0.0, "avg_logprob": -0.15794209352473623, "compression_ratio": 1.5625, "no_speech_prob": 0.0004266327596269548}, {"id": 779, "seek": 465004, "start": 4661.16, "end": 4665.24, "text": " lecture next week. Do you have any questions up at the top? Yeah, I've got one right in the", "tokens": [50920, 7991, 958, 1243, 13, 1144, 291, 362, 604, 1651, 493, 412, 264, 1192, 30, 865, 11, 286, 600, 658, 472, 558, 294, 264, 51124], "temperature": 0.0, "avg_logprob": -0.15794209352473623, "compression_ratio": 1.5625, "no_speech_prob": 0.0004266327596269548}, {"id": 780, "seek": 465004, "start": 4665.24, "end": 4677.56, "text": " center just here. Thank you. So when we think about the situations or use cases where AI is", "tokens": [51124, 3056, 445, 510, 13, 1044, 291, 13, 407, 562, 321, 519, 466, 264, 6851, 420, 764, 3331, 689, 7318, 307, 51740], "temperature": 0.0, "avg_logprob": -0.15794209352473623, "compression_ratio": 1.5625, "no_speech_prob": 0.0004266327596269548}, {"id": 781, "seek": 467756, "start": 4677.56, "end": 4684.68, "text": " applied, typically the reason for that is because the machine is doing things better than a human", "tokens": [50364, 6456, 11, 5850, 264, 1778, 337, 300, 307, 570, 264, 3479, 307, 884, 721, 1101, 813, 257, 1952, 50720], "temperature": 0.0, "avg_logprob": -0.07871001766573998, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.017300209030508995}, {"id": 782, "seek": 467756, "start": 4684.68, "end": 4691.400000000001, "text": " can or doing things that a human might not be able to do. So it's a lot about the machine making up", "tokens": [50720, 393, 420, 884, 721, 300, 257, 1952, 1062, 406, 312, 1075, 281, 360, 13, 407, 309, 311, 257, 688, 466, 264, 3479, 1455, 493, 51056], "temperature": 0.0, "avg_logprob": -0.07871001766573998, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.017300209030508995}, {"id": 783, "seek": 467756, "start": 4691.400000000001, "end": 4698.84, "text": " for the gaps that a human creates. That said, a machine is fallible, like there are errors,", "tokens": [51056, 337, 264, 15031, 300, 257, 1952, 7829, 13, 663, 848, 11, 257, 3479, 307, 2100, 964, 11, 411, 456, 366, 13603, 11, 51428], "temperature": 0.0, "avg_logprob": -0.07871001766573998, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.017300209030508995}, {"id": 784, "seek": 467756, "start": 4698.84, "end": 4704.68, "text": " both normative errors and also statistical errors depending on the model type, etc. And so the", "tokens": [51428, 1293, 2026, 1166, 13603, 293, 611, 22820, 13603, 5413, 322, 264, 2316, 2010, 11, 5183, 13, 400, 370, 264, 51720], "temperature": 0.0, "avg_logprob": -0.07871001766573998, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.017300209030508995}, {"id": 785, "seek": 470468, "start": 4704.68, "end": 4711.72, "text": " question is who do you think should be responsible for looking after the gaps that the machine now", "tokens": [50364, 1168, 307, 567, 360, 291, 519, 820, 312, 6250, 337, 1237, 934, 264, 15031, 300, 264, 3479, 586, 50716], "temperature": 0.0, "avg_logprob": -0.1495458961713432, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0008577530388720334}, {"id": 786, "seek": 470468, "start": 4711.72, "end": 4717.240000000001, "text": " creates? So the fundamental question is who should be responsible, right? Is that right? Sorry, I", "tokens": [50716, 7829, 30, 407, 264, 8088, 1168, 307, 567, 820, 312, 6250, 11, 558, 30, 1119, 300, 558, 30, 4919, 11, 286, 50992], "temperature": 0.0, "avg_logprob": -0.1495458961713432, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0008577530388720334}, {"id": 787, "seek": 470468, "start": 4717.240000000001, "end": 4722.12, "text": " didn't see where you were. Can you put your hand up? Right, the top in the middle. Oh, wow. Okay,", "tokens": [50992, 994, 380, 536, 689, 291, 645, 13, 1664, 291, 829, 428, 1011, 493, 30, 1779, 11, 264, 1192, 294, 264, 2808, 13, 876, 11, 6076, 13, 1033, 11, 51236], "temperature": 0.0, "avg_logprob": -0.1495458961713432, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0008577530388720334}, {"id": 788, "seek": 470468, "start": 4722.12, "end": 4731.8, "text": " so that's why I can't see you. Okay. So this is an issue that's being discussed absolutely in the", "tokens": [51236, 370, 300, 311, 983, 286, 393, 380, 536, 291, 13, 1033, 13, 407, 341, 307, 364, 2734, 300, 311, 885, 7152, 3122, 294, 264, 51720], "temperature": 0.0, "avg_logprob": -0.1495458961713432, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0008577530388720334}, {"id": 789, "seek": 473180, "start": 4731.8, "end": 4737.72, "text": " highest levels of government right now, that literally when we work in, when we move into the", "tokens": [50364, 6343, 4358, 295, 2463, 558, 586, 11, 300, 3736, 562, 321, 589, 294, 11, 562, 321, 1286, 666, 264, 50660], "temperature": 0.0, "avg_logprob": -0.10275199526832217, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.04910563677549362}, {"id": 790, "seek": 473180, "start": 4737.72, "end": 4744.360000000001, "text": " age of AI, who should accept the responsibility? I can tell you what my view is, but I'm not a", "tokens": [50660, 3205, 295, 7318, 11, 567, 820, 3241, 264, 6357, 30, 286, 393, 980, 291, 437, 452, 1910, 307, 11, 457, 286, 478, 406, 257, 50992], "temperature": 0.0, "avg_logprob": -0.10275199526832217, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.04910563677549362}, {"id": 791, "seek": 473180, "start": 4744.360000000001, "end": 4752.6, "text": " lawyer or an ethics expert. And my view is that, as follows, firstly, if you use AI in your work,", "tokens": [50992, 11613, 420, 364, 19769, 5844, 13, 400, 452, 1910, 307, 300, 11, 382, 10002, 11, 27376, 11, 498, 291, 764, 7318, 294, 428, 589, 11, 51404], "temperature": 0.0, "avg_logprob": -0.10275199526832217, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.04910563677549362}, {"id": 792, "seek": 473180, "start": 4753.24, "end": 4758.4400000000005, "text": " then, and you end up with a bad result, I'm sorry, but that's your problem. If you use it to generate", "tokens": [51436, 550, 11, 293, 291, 917, 493, 365, 257, 1578, 1874, 11, 286, 478, 2597, 11, 457, 300, 311, 428, 1154, 13, 759, 291, 764, 309, 281, 8460, 51696], "temperature": 0.0, "avg_logprob": -0.10275199526832217, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.04910563677549362}, {"id": 793, "seek": 475844, "start": 4758.44, "end": 4763.24, "text": " an essay at school and you're caught out, I'm afraid that's your problem. It's not the fault", "tokens": [50364, 364, 16238, 412, 1395, 293, 291, 434, 5415, 484, 11, 286, 478, 4638, 300, 311, 428, 1154, 13, 467, 311, 406, 264, 7441, 50604], "temperature": 0.0, "avg_logprob": -0.10814437087701291, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.004869720432907343}, {"id": 794, "seek": 475844, "start": 4763.24, "end": 4772.5199999999995, "text": " of the AI. But I think more generally, we can't offload our legal, moral, ethical obligations", "tokens": [50604, 295, 264, 7318, 13, 583, 286, 519, 544, 5101, 11, 321, 393, 380, 766, 2907, 527, 5089, 11, 9723, 11, 18890, 26234, 51068], "temperature": 0.0, "avg_logprob": -0.10814437087701291, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.004869720432907343}, {"id": 795, "seek": 475844, "start": 4772.5199999999995, "end": 4778.04, "text": " as human beings onto the machine. That is, we can't say it's not my fault, the machine did it.", "tokens": [51068, 382, 1952, 8958, 3911, 264, 3479, 13, 663, 307, 11, 321, 393, 380, 584, 309, 311, 406, 452, 7441, 11, 264, 3479, 630, 309, 13, 51344], "temperature": 0.0, "avg_logprob": -0.10814437087701291, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.004869720432907343}, {"id": 796, "seek": 475844, "start": 4778.04, "end": 4783.24, "text": " Right? An extreme example of this is lethal autonomous weapons, AI that's empowered to decide", "tokens": [51344, 1779, 30, 1107, 8084, 1365, 295, 341, 307, 34562, 23797, 7278, 11, 7318, 300, 311, 27898, 281, 4536, 51604], "temperature": 0.0, "avg_logprob": -0.10814437087701291, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.004869720432907343}, {"id": 797, "seek": 478324, "start": 4783.24, "end": 4788.36, "text": " whether to take a human life. What I worry about, one of the many things I worry about with lethal", "tokens": [50364, 1968, 281, 747, 257, 1952, 993, 13, 708, 286, 3292, 466, 11, 472, 295, 264, 867, 721, 286, 3292, 466, 365, 34562, 50620], "temperature": 0.0, "avg_logprob": -0.0608641608008023, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.008524399250745773}, {"id": 798, "seek": 478324, "start": 4788.36, "end": 4792.44, "text": " autonomous weapons is the idea that we have military services that say, well, it wasn't our fault,", "tokens": [50620, 23797, 7278, 307, 264, 1558, 300, 321, 362, 4632, 3328, 300, 584, 11, 731, 11, 309, 2067, 380, 527, 7441, 11, 50824], "temperature": 0.0, "avg_logprob": -0.0608641608008023, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.008524399250745773}, {"id": 799, "seek": 478324, "start": 4792.44, "end": 4797.08, "text": " it was the AI that got it wrong, that led to this building being bombed or whatever it was.", "tokens": [50824, 309, 390, 264, 7318, 300, 658, 309, 2085, 11, 300, 4684, 281, 341, 2390, 885, 7957, 2883, 420, 2035, 309, 390, 13, 51056], "temperature": 0.0, "avg_logprob": -0.0608641608008023, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.008524399250745773}, {"id": 800, "seek": 478324, "start": 4797.719999999999, "end": 4802.44, "text": " And there, I think the responsibility lies with the people that deploy the technology.", "tokens": [51088, 400, 456, 11, 286, 519, 264, 6357, 9134, 365, 264, 561, 300, 7274, 264, 2899, 13, 51324], "temperature": 0.0, "avg_logprob": -0.0608641608008023, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.008524399250745773}, {"id": 801, "seek": 478324, "start": 4803.639999999999, "end": 4808.2, "text": " So that, I think, is a crucial point. But at the same time, the developers of this technology,", "tokens": [51384, 407, 300, 11, 286, 519, 11, 307, 257, 11462, 935, 13, 583, 412, 264, 912, 565, 11, 264, 8849, 295, 341, 2899, 11, 51612], "temperature": 0.0, "avg_logprob": -0.0608641608008023, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.008524399250745773}, {"id": 802, "seek": 480820, "start": 4808.2, "end": 4814.12, "text": " if they are warranting that it is fit for purpose, then they have a responsibility as well. And the", "tokens": [50364, 498, 436, 366, 16354, 278, 300, 309, 307, 3318, 337, 4334, 11, 550, 436, 362, 257, 6357, 382, 731, 13, 400, 264, 50660], "temperature": 0.0, "avg_logprob": -0.051175422835768314, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.03436483442783356}, {"id": 803, "seek": 480820, "start": 4814.12, "end": 4819.8, "text": " responsibility that they have is to ensure that it really is fit for purpose. And it's an interesting", "tokens": [50660, 6357, 300, 436, 362, 307, 281, 5586, 300, 309, 534, 307, 3318, 337, 4334, 13, 400, 309, 311, 364, 1880, 50944], "temperature": 0.0, "avg_logprob": -0.051175422835768314, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.03436483442783356}, {"id": 804, "seek": 480820, "start": 4819.8, "end": 4824.28, "text": " question at the moment. If we have large language models used by hundreds of millions of people,", "tokens": [50944, 1168, 412, 264, 1623, 13, 759, 321, 362, 2416, 2856, 5245, 1143, 538, 6779, 295, 6803, 295, 561, 11, 51168], "temperature": 0.0, "avg_logprob": -0.051175422835768314, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.03436483442783356}, {"id": 805, "seek": 480820, "start": 4824.28, "end": 4830.36, "text": " for example, to get medical advice, and we know that this technology can go wrong,", "tokens": [51168, 337, 1365, 11, 281, 483, 4625, 5192, 11, 293, 321, 458, 300, 341, 2899, 393, 352, 2085, 11, 51472], "temperature": 0.0, "avg_logprob": -0.051175422835768314, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.03436483442783356}, {"id": 806, "seek": 480820, "start": 4830.36, "end": 4836.36, "text": " is the technology fit for that purpose? I'm not sure at all that it is. So I'm not sure", "tokens": [51472, 307, 264, 2899, 3318, 337, 300, 4334, 30, 286, 478, 406, 988, 412, 439, 300, 309, 307, 13, 407, 286, 478, 406, 988, 51772], "temperature": 0.0, "avg_logprob": -0.051175422835768314, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.03436483442783356}, {"id": 807, "seek": 483636, "start": 4836.36, "end": 4840.04, "text": " that's really answering your question, but those are my sort of a few random thoughts on it.", "tokens": [50364, 300, 311, 534, 13430, 428, 1168, 11, 457, 729, 366, 452, 1333, 295, 257, 1326, 4974, 4598, 322, 309, 13, 50548], "temperature": 0.0, "avg_logprob": -0.11061811009678271, "compression_ratio": 1.6070038910505837, "no_speech_prob": 0.005430525168776512}, {"id": 808, "seek": 483636, "start": 4840.04, "end": 4843.48, "text": " I mean, but I say, crucially, you know, if you're using this in your work,", "tokens": [50548, 286, 914, 11, 457, 286, 584, 11, 5140, 1909, 11, 291, 458, 11, 498, 291, 434, 1228, 341, 294, 428, 589, 11, 50720], "temperature": 0.0, "avg_logprob": -0.11061811009678271, "compression_ratio": 1.6070038910505837, "no_speech_prob": 0.005430525168776512}, {"id": 809, "seek": 483636, "start": 4843.48, "end": 4849.639999999999, "text": " you can never blame the AI, right? You are responsible for the outputs of that process,", "tokens": [50720, 291, 393, 1128, 10127, 264, 7318, 11, 558, 30, 509, 366, 6250, 337, 264, 23930, 295, 300, 1399, 11, 51028], "temperature": 0.0, "avg_logprob": -0.11061811009678271, "compression_ratio": 1.6070038910505837, "no_speech_prob": 0.005430525168776512}, {"id": 810, "seek": 483636, "start": 4849.639999999999, "end": 4855.16, "text": " right? You can't offload your legal, professional, ethical, moral obligations to the machine.", "tokens": [51028, 558, 30, 509, 393, 380, 766, 2907, 428, 5089, 11, 4843, 11, 18890, 11, 9723, 26234, 281, 264, 3479, 13, 51304], "temperature": 0.0, "avg_logprob": -0.11061811009678271, "compression_ratio": 1.6070038910505837, "no_speech_prob": 0.005430525168776512}, {"id": 811, "seek": 483636, "start": 4856.28, "end": 4860.44, "text": " It's a complex question, which is why I gave a very bad answer.", "tokens": [51360, 467, 311, 257, 3997, 1168, 11, 597, 307, 983, 286, 2729, 257, 588, 1578, 1867, 13, 51568], "temperature": 0.0, "avg_logprob": -0.11061811009678271, "compression_ratio": 1.6070038910505837, "no_speech_prob": 0.005430525168776512}, {"id": 812, "seek": 486044, "start": 4860.599999999999, "end": 4865.799999999999, "text": " I've got a question right on the left here. I've used access on the left panel shelf.", "tokens": [50372, 286, 600, 658, 257, 1168, 558, 322, 264, 1411, 510, 13, 286, 600, 1143, 2105, 322, 264, 1411, 4831, 15222, 13, 50632], "temperature": 0.0, "avg_logprob": -0.25358040753532857, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.00752544030547142}, {"id": 813, "seek": 486044, "start": 4867.32, "end": 4874.839999999999, "text": " Thank you. If future large language models are trained by scraping the whole internet again,", "tokens": [50708, 1044, 291, 13, 759, 2027, 2416, 2856, 5245, 366, 8895, 538, 43738, 264, 1379, 4705, 797, 11, 51084], "temperature": 0.0, "avg_logprob": -0.25358040753532857, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.00752544030547142}, {"id": 814, "seek": 486044, "start": 4874.839999999999, "end": 4882.679999999999, "text": " now there's more and more content going on to the internet created by AI. So is that going to create", "tokens": [51084, 586, 456, 311, 544, 293, 544, 2701, 516, 322, 281, 264, 4705, 2942, 538, 7318, 13, 407, 307, 300, 516, 281, 1884, 51476], "temperature": 0.0, "avg_logprob": -0.25358040753532857, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.00752544030547142}, {"id": 815, "seek": 486044, "start": 4882.679999999999, "end": 4889.24, "text": " something like a microphone feedback loop where the information gets less and less useful?", "tokens": [51476, 746, 411, 257, 10952, 5824, 6367, 689, 264, 1589, 2170, 1570, 293, 1570, 4420, 30, 51804], "temperature": 0.0, "avg_logprob": -0.25358040753532857, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.00752544030547142}, {"id": 816, "seek": 488924, "start": 4889.96, "end": 4895.639999999999, "text": " Super question and really fascinating. So I have some colleagues that did the following experiment.", "tokens": [50400, 4548, 1168, 293, 534, 10343, 13, 407, 286, 362, 512, 7734, 300, 630, 264, 3480, 5120, 13, 50684], "temperature": 0.0, "avg_logprob": -0.11323576960070379, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.0008642885368317366}, {"id": 817, "seek": 488924, "start": 4895.639999999999, "end": 4902.76, "text": " So chat GPT is trained, roughly speaking, on human generated text, but it creates AI generated text.", "tokens": [50684, 407, 5081, 26039, 51, 307, 8895, 11, 9810, 4124, 11, 322, 1952, 10833, 2487, 11, 457, 309, 7829, 7318, 10833, 2487, 13, 51040], "temperature": 0.0, "avg_logprob": -0.11323576960070379, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.0008642885368317366}, {"id": 818, "seek": 488924, "start": 4902.76, "end": 4907.639999999999, "text": " So the question they had is what happens if we train one of these models, not on the original", "tokens": [51040, 407, 264, 1168, 436, 632, 307, 437, 2314, 498, 321, 3847, 472, 295, 613, 5245, 11, 406, 322, 264, 3380, 51284], "temperature": 0.0, "avg_logprob": -0.11323576960070379, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.0008642885368317366}, {"id": 819, "seek": 488924, "start": 4907.639999999999, "end": 4913.4, "text": " human generated text, but just on stuff which is produced by AI? And then you can see what they", "tokens": [51284, 1952, 10833, 2487, 11, 457, 445, 322, 1507, 597, 307, 7126, 538, 7318, 30, 400, 550, 291, 393, 536, 437, 436, 51572], "temperature": 0.0, "avg_logprob": -0.11323576960070379, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.0008642885368317366}, {"id": 820, "seek": 488924, "start": 4913.4, "end": 4917.719999999999, "text": " did next. You can guess, they said, well, okay, let's take another model which is trained on the", "tokens": [51572, 630, 958, 13, 509, 393, 2041, 11, 436, 848, 11, 731, 11, 1392, 11, 718, 311, 747, 1071, 2316, 597, 307, 8895, 322, 264, 51788], "temperature": 0.0, "avg_logprob": -0.11323576960070379, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.0008642885368317366}, {"id": 821, "seek": 491772, "start": 4917.72, "end": 4923.88, "text": " second generation model text. And so what happens about five generations down the line,", "tokens": [50364, 1150, 5125, 2316, 2487, 13, 400, 370, 437, 2314, 466, 1732, 10593, 760, 264, 1622, 11, 50672], "temperature": 0.0, "avg_logprob": -0.1098977526028951, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.000709067564457655}, {"id": 822, "seek": 491772, "start": 4923.88, "end": 4930.92, "text": " it dissolves into gibberish, literally dissolves into gibberish. And I have to tell you the original", "tokens": [50672, 309, 15840, 977, 666, 4553, 43189, 11, 3736, 15840, 977, 666, 4553, 43189, 13, 400, 286, 362, 281, 980, 291, 264, 3380, 51024], "temperature": 0.0, "avg_logprob": -0.1098977526028951, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.000709067564457655}, {"id": 823, "seek": 491772, "start": 4930.92, "end": 4938.68, "text": " version of this paper, they called it AI dementia. And I was really cross with, no, I lost both my", "tokens": [51024, 3037, 295, 341, 3035, 11, 436, 1219, 309, 7318, 31734, 13, 400, 286, 390, 534, 3278, 365, 11, 572, 11, 286, 2731, 1293, 452, 51412], "temperature": 0.0, "avg_logprob": -0.1098977526028951, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.000709067564457655}, {"id": 824, "seek": 491772, "start": 4938.68, "end": 4943.8, "text": " parents to dementia. I didn't find it very funny at all. They now call it model collapse. So if you", "tokens": [51412, 3152, 281, 31734, 13, 286, 994, 380, 915, 309, 588, 4074, 412, 439, 13, 814, 586, 818, 309, 2316, 15584, 13, 407, 498, 291, 51668], "temperature": 0.0, "avg_logprob": -0.1098977526028951, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.000709067564457655}, {"id": 825, "seek": 494380, "start": 4943.8, "end": 4948.04, "text": " go and Google model collapse, you'll find the answers there. But really remarkable. What that", "tokens": [50364, 352, 293, 3329, 2316, 15584, 11, 291, 603, 915, 264, 6338, 456, 13, 583, 534, 12802, 13, 708, 300, 50576], "temperature": 0.0, "avg_logprob": -0.09035653201016512, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.0027159438468515873}, {"id": 826, "seek": 494380, "start": 4948.04, "end": 4954.2, "text": " tells you is that actually there is something qualitatively different at the moment to human", "tokens": [50576, 5112, 291, 307, 300, 767, 456, 307, 746, 31312, 356, 819, 412, 264, 1623, 281, 1952, 50884], "temperature": 0.0, "avg_logprob": -0.09035653201016512, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.0027159438468515873}, {"id": 827, "seek": 494380, "start": 4954.2, "end": 4959.72, "text": " text, to AI generated text. For all that it looks perfect or indistinguishable to us,", "tokens": [50884, 2487, 11, 281, 7318, 10833, 2487, 13, 1171, 439, 300, 309, 1542, 2176, 420, 1016, 468, 7050, 742, 712, 281, 505, 11, 51160], "temperature": 0.0, "avg_logprob": -0.09035653201016512, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.0027159438468515873}, {"id": 828, "seek": 494380, "start": 4959.72, "end": 4964.28, "text": " actually it isn't. Where is that going to take us? I have colleagues who think that we're going to", "tokens": [51160, 767, 309, 1943, 380, 13, 2305, 307, 300, 516, 281, 747, 505, 30, 286, 362, 7734, 567, 519, 300, 321, 434, 516, 281, 51388], "temperature": 0.0, "avg_logprob": -0.09035653201016512, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.0027159438468515873}, {"id": 829, "seek": 494380, "start": 4964.28, "end": 4971.64, "text": " have to label and protect human generated content because it is so valuable. All right? Human", "tokens": [51388, 362, 281, 7645, 293, 2371, 1952, 10833, 2701, 570, 309, 307, 370, 8263, 13, 1057, 558, 30, 10294, 51756], "temperature": 0.0, "avg_logprob": -0.09035653201016512, "compression_ratio": 1.684782608695652, "no_speech_prob": 0.0027159438468515873}, {"id": 830, "seek": 497164, "start": 4971.64, "end": 4978.84, "text": " generated actual, authentic human generated content is really, really valuable. I also have", "tokens": [50364, 10833, 3539, 11, 12466, 1952, 10833, 2701, 307, 534, 11, 534, 8263, 13, 286, 611, 362, 50724], "temperature": 0.0, "avg_logprob": -0.06883158377551157, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.0016260824631899595}, {"id": 831, "seek": 497164, "start": 4978.84, "end": 4983.0, "text": " colleagues, and I'm not sure whether they're entirely serious at this, but they say that actually", "tokens": [50724, 7734, 11, 293, 286, 478, 406, 988, 1968, 436, 434, 7696, 3156, 412, 341, 11, 457, 436, 584, 300, 767, 50932], "temperature": 0.0, "avg_logprob": -0.06883158377551157, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.0016260824631899595}, {"id": 832, "seek": 497164, "start": 4983.0, "end": 4989.0, "text": " where we're going is the data that we produce in everything that we do is so valuable for AI", "tokens": [50932, 689, 321, 434, 516, 307, 264, 1412, 300, 321, 5258, 294, 1203, 300, 321, 360, 307, 370, 8263, 337, 7318, 51232], "temperature": 0.0, "avg_logprob": -0.06883158377551157, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.0016260824631899595}, {"id": 833, "seek": 497164, "start": 4989.0, "end": 4994.76, "text": " that we're going to enter a future where you're going to sell the rights to AI companies for you,", "tokens": [51232, 300, 321, 434, 516, 281, 3242, 257, 2027, 689, 291, 434, 516, 281, 3607, 264, 4601, 281, 7318, 3431, 337, 291, 11, 51520], "temperature": 0.0, "avg_logprob": -0.06883158377551157, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.0016260824631899595}, {"id": 834, "seek": 497164, "start": 4994.76, "end": 5000.6, "text": " for them to harvest your emotions, all of your experiences, everything you say and do in your", "tokens": [51520, 337, 552, 281, 11917, 428, 8462, 11, 439, 295, 428, 5235, 11, 1203, 291, 584, 293, 360, 294, 428, 51812], "temperature": 0.0, "avg_logprob": -0.06883158377551157, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.0016260824631899595}, {"id": 835, "seek": 500060, "start": 5000.6, "end": 5005.72, "text": " life. And you'll be paid for that, but it will go into the training models of large language models.", "tokens": [50364, 993, 13, 400, 291, 603, 312, 4835, 337, 300, 11, 457, 309, 486, 352, 666, 264, 3097, 5245, 295, 2416, 2856, 5245, 13, 50620], "temperature": 0.0, "avg_logprob": -0.10775186682260164, "compression_ratio": 1.644736842105263, "no_speech_prob": 0.0020517094526439905}, {"id": 836, "seek": 500060, "start": 5005.72, "end": 5011.320000000001, "text": " Now I don't know if that's true, but nevertheless there's a, it has some inner truth in it, I think.", "tokens": [50620, 823, 286, 500, 380, 458, 498, 300, 311, 2074, 11, 457, 26924, 456, 311, 257, 11, 309, 575, 512, 7284, 3494, 294, 309, 11, 286, 519, 13, 50900], "temperature": 0.0, "avg_logprob": -0.10775186682260164, "compression_ratio": 1.644736842105263, "no_speech_prob": 0.0020517094526439905}, {"id": 837, "seek": 500060, "start": 5012.52, "end": 5020.04, "text": " And in 100 years time, it is an absolute certainty that there will be vastly,", "tokens": [50960, 400, 294, 2319, 924, 565, 11, 309, 307, 364, 8236, 27022, 300, 456, 486, 312, 41426, 11, 51336], "temperature": 0.0, "avg_logprob": -0.10775186682260164, "compression_ratio": 1.644736842105263, "no_speech_prob": 0.0020517094526439905}, {"id": 838, "seek": 500060, "start": 5020.04, "end": 5026.200000000001, "text": " vastly more AI generated content out there in the world than there will human generated content", "tokens": [51336, 41426, 544, 7318, 10833, 2701, 484, 456, 294, 264, 1002, 813, 456, 486, 1952, 10833, 2701, 51644], "temperature": 0.0, "avg_logprob": -0.10775186682260164, "compression_ratio": 1.644736842105263, "no_speech_prob": 0.0020517094526439905}, {"id": 839, "seek": 502620, "start": 5026.2, "end": 5030.599999999999, "text": " with certainty. I think there's no question, but that that's the way the future is going.", "tokens": [50364, 365, 27022, 13, 286, 519, 456, 311, 572, 1168, 11, 457, 300, 300, 311, 264, 636, 264, 2027, 307, 516, 13, 50584], "temperature": 0.0, "avg_logprob": -0.15093253144120747, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.0010397416772320867}, {"id": 840, "seek": 502620, "start": 5030.599999999999, "end": 5036.12, "text": " And as I say, as the model collapse scenario illustrates, that presents some real challenges.", "tokens": [50584, 400, 382, 286, 584, 11, 382, 264, 2316, 15584, 9005, 41718, 11, 300, 13533, 512, 957, 4759, 13, 50860], "temperature": 0.0, "avg_logprob": -0.15093253144120747, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.0010397416772320867}, {"id": 841, "seek": 502620, "start": 5037.32, "end": 5040.5199999999995, "text": " Awesome. Thank you very much, Mike. I've got a question at the front who's been very keen to ask.", "tokens": [50920, 10391, 13, 1044, 291, 588, 709, 11, 6602, 13, 286, 600, 658, 257, 1168, 412, 264, 1868, 567, 311, 668, 588, 20297, 281, 1029, 13, 51080], "temperature": 0.0, "avg_logprob": -0.15093253144120747, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.0010397416772320867}, {"id": 842, "seek": 502620, "start": 5043.5599999999995, "end": 5048.44, "text": " Thanks very much indeed for a very interesting lecture. It strikes me in a way just being a", "tokens": [51232, 2561, 588, 709, 6451, 337, 257, 588, 1880, 7991, 13, 467, 16750, 385, 294, 257, 636, 445, 885, 257, 51476], "temperature": 0.0, "avg_logprob": -0.15093253144120747, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.0010397416772320867}, {"id": 843, "seek": 502620, "start": 5048.44, "end": 5053.8, "text": " comparison of human being. What we're doing is talking about what the prefront frontal cortex", "tokens": [51476, 9660, 295, 1952, 885, 13, 708, 321, 434, 884, 307, 1417, 466, 437, 264, 659, 11496, 34647, 33312, 51744], "temperature": 0.0, "avg_logprob": -0.15093253144120747, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.0010397416772320867}, {"id": 844, "seek": 505380, "start": 5053.8, "end": 5059.320000000001, "text": " does, but there are other areas of prefrontal cortex, which is a fear predictor. Do we need to", "tokens": [50364, 775, 11, 457, 456, 366, 661, 3179, 295, 659, 11496, 304, 33312, 11, 597, 307, 257, 4240, 6069, 284, 13, 1144, 321, 643, 281, 50640], "temperature": 0.0, "avg_logprob": -0.1855579089092952, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.0010173687478527427}, {"id": 845, "seek": 505380, "start": 5059.320000000001, "end": 5065.8, "text": " be developing sort of a parallel AI system, which works on the basis of fear prediction", "tokens": [50640, 312, 6416, 1333, 295, 257, 8952, 7318, 1185, 11, 597, 1985, 322, 264, 5143, 295, 4240, 17630, 50964], "temperature": 0.0, "avg_logprob": -0.1855579089092952, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.0010173687478527427}, {"id": 846, "seek": 505380, "start": 5065.8, "end": 5072.6, "text": " and get to talk to each other? Yeah. So I'm absolutely not a neuroscientist or I'm a computer", "tokens": [50964, 293, 483, 281, 751, 281, 1184, 661, 30, 865, 13, 407, 286, 478, 3122, 406, 257, 28813, 5412, 468, 420, 286, 478, 257, 3820, 51304], "temperature": 0.0, "avg_logprob": -0.1855579089092952, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.0010173687478527427}, {"id": 847, "seek": 505380, "start": 5072.6, "end": 5080.360000000001, "text": " programmer and that's very much my background. Again, it's interesting that the community is", "tokens": [51304, 32116, 293, 300, 311, 588, 709, 452, 3678, 13, 3764, 11, 309, 311, 1880, 300, 264, 1768, 307, 51692], "temperature": 0.0, "avg_logprob": -0.1855579089092952, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.0010173687478527427}, {"id": 848, "seek": 508036, "start": 5080.36, "end": 5084.599999999999, "text": " incredibly divided. So when I was an undergraduate studying AI and I focused in my final year,", "tokens": [50364, 6252, 6666, 13, 407, 562, 286, 390, 364, 19113, 7601, 7318, 293, 286, 5178, 294, 452, 2572, 1064, 11, 50576], "temperature": 0.0, "avg_logprob": -0.09546994793322659, "compression_ratio": 1.7852564102564104, "no_speech_prob": 0.015819022431969643}, {"id": 849, "seek": 508036, "start": 5084.599999999999, "end": 5089.799999999999, "text": " it's mainly what I studied. And the textbooks that we had made no reference to the brain", "tokens": [50576, 309, 311, 8704, 437, 286, 9454, 13, 400, 264, 33587, 300, 321, 632, 1027, 572, 6408, 281, 264, 3567, 50836], "temperature": 0.0, "avg_logprob": -0.09546994793322659, "compression_ratio": 1.7852564102564104, "no_speech_prob": 0.015819022431969643}, {"id": 850, "seek": 508036, "start": 5089.799999999999, "end": 5094.44, "text": " whatsoever. Just wasn't the thing because it was all about modeling the mind. It was all about", "tokens": [50836, 17076, 13, 1449, 2067, 380, 264, 551, 570, 309, 390, 439, 466, 15983, 264, 1575, 13, 467, 390, 439, 466, 51068], "temperature": 0.0, "avg_logprob": -0.09546994793322659, "compression_ratio": 1.7852564102564104, "no_speech_prob": 0.015819022431969643}, {"id": 851, "seek": 508036, "start": 5094.44, "end": 5100.28, "text": " modeling conscious reasoning processes and so on. And it was deeply unfashionable to think about", "tokens": [51068, 15983, 6648, 21577, 7555, 293, 370, 322, 13, 400, 309, 390, 8760, 3971, 5894, 712, 281, 519, 466, 51360], "temperature": 0.0, "avg_logprob": -0.09546994793322659, "compression_ratio": 1.7852564102564104, "no_speech_prob": 0.015819022431969643}, {"id": 852, "seek": 508036, "start": 5100.28, "end": 5104.839999999999, "text": " the brain. And there's been a bit of a what scientists call a paradigm shift in the way", "tokens": [51360, 264, 3567, 13, 400, 456, 311, 668, 257, 857, 295, 257, 437, 7708, 818, 257, 24709, 5513, 294, 264, 636, 51588], "temperature": 0.0, "avg_logprob": -0.09546994793322659, "compression_ratio": 1.7852564102564104, "no_speech_prob": 0.015819022431969643}, {"id": 853, "seek": 508036, "start": 5104.839999999999, "end": 5109.16, "text": " that they think about this prompted by the rise of neural networks, but also by the fact that", "tokens": [51588, 300, 436, 519, 466, 341, 31042, 538, 264, 6272, 295, 18161, 9590, 11, 457, 611, 538, 264, 1186, 300, 51804], "temperature": 0.0, "avg_logprob": -0.09546994793322659, "compression_ratio": 1.7852564102564104, "no_speech_prob": 0.015819022431969643}, {"id": 854, "seek": 510916, "start": 5109.16, "end": 5115.639999999999, "text": " advances in computer vision and the architectures, the neural network architectures that led to", "tokens": [50364, 25297, 294, 3820, 5201, 293, 264, 6331, 1303, 11, 264, 18161, 3209, 6331, 1303, 300, 4684, 281, 50688], "temperature": 0.0, "avg_logprob": -0.060998215394861555, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.00048275350127369165}, {"id": 855, "seek": 510916, "start": 5115.639999999999, "end": 5120.76, "text": " facial recognition really working were actually inspired by the visual cortex, the human visual", "tokens": [50688, 15642, 11150, 534, 1364, 645, 767, 7547, 538, 264, 5056, 33312, 11, 264, 1952, 5056, 50944], "temperature": 0.0, "avg_logprob": -0.060998215394861555, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.00048275350127369165}, {"id": 856, "seek": 510916, "start": 5120.76, "end": 5127.08, "text": " cortex. So it's a lot more of a fashionable question now than it used to be. So my guess is,", "tokens": [50944, 33312, 13, 407, 309, 311, 257, 688, 544, 295, 257, 40735, 1168, 586, 813, 309, 1143, 281, 312, 13, 407, 452, 2041, 307, 11, 51260], "temperature": 0.0, "avg_logprob": -0.060998215394861555, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.00048275350127369165}, {"id": 857, "seek": 510916, "start": 5127.08, "end": 5133.08, "text": " firstly, simply trying to copy the structure of the human brain is not the way to do it,", "tokens": [51260, 27376, 11, 2935, 1382, 281, 5055, 264, 3877, 295, 264, 1952, 3567, 307, 406, 264, 636, 281, 360, 309, 11, 51560], "temperature": 0.0, "avg_logprob": -0.060998215394861555, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.00048275350127369165}, {"id": 858, "seek": 510916, "start": 5133.08, "end": 5137.24, "text": " but nevertheless getting a much better understanding of the organization of the brain,", "tokens": [51560, 457, 26924, 1242, 257, 709, 1101, 3701, 295, 264, 4475, 295, 264, 3567, 11, 51768], "temperature": 0.0, "avg_logprob": -0.060998215394861555, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.00048275350127369165}, {"id": 859, "seek": 513724, "start": 5137.32, "end": 5140.599999999999, "text": " the functional organization of the brain, and the way that the different components of the", "tokens": [50368, 264, 11745, 4475, 295, 264, 3567, 11, 293, 264, 636, 300, 264, 819, 6677, 295, 264, 50532], "temperature": 0.0, "avg_logprob": -0.18806693667457217, "compression_ratio": 1.678343949044586, "no_speech_prob": 0.0008280895999632776}, {"id": 860, "seek": 513724, "start": 5140.599999999999, "end": 5147.8, "text": " brain interoperate to produce human intelligence, I think, is. And really, there's a vast amount", "tokens": [50532, 3567, 728, 7192, 473, 281, 5258, 1952, 7599, 11, 286, 519, 11, 307, 13, 400, 534, 11, 456, 311, 257, 8369, 2372, 50892], "temperature": 0.0, "avg_logprob": -0.18806693667457217, "compression_ratio": 1.678343949044586, "no_speech_prob": 0.0008280895999632776}, {"id": 861, "seek": 513724, "start": 5147.8, "end": 5152.12, "text": " of work there to be done to try to understand that. There are so many unanswered questions.", "tokens": [50892, 295, 589, 456, 281, 312, 1096, 281, 853, 281, 1223, 300, 13, 821, 366, 370, 867, 517, 43904, 292, 1651, 13, 51108], "temperature": 0.0, "avg_logprob": -0.18806693667457217, "compression_ratio": 1.678343949044586, "no_speech_prob": 0.0008280895999632776}, {"id": 862, "seek": 513724, "start": 5152.679999999999, "end": 5156.12, "text": " I hope that's some help. Thank you, Mike. We're just going to jump back online.", "tokens": [51136, 286, 1454, 300, 311, 512, 854, 13, 1044, 291, 11, 6602, 13, 492, 434, 445, 516, 281, 3012, 646, 2950, 13, 51308], "temperature": 0.0, "avg_logprob": -0.18806693667457217, "compression_ratio": 1.678343949044586, "no_speech_prob": 0.0008280895999632776}, {"id": 863, "seek": 513724, "start": 5156.12, "end": 5159.96, "text": " Yeah, that's going to be a little early. Anthony asks, if emergency is inaccurate,", "tokens": [51308, 865, 11, 300, 311, 516, 281, 312, 257, 707, 2440, 13, 15853, 8962, 11, 498, 7473, 307, 46443, 11, 51500], "temperature": 0.0, "avg_logprob": -0.18806693667457217, "compression_ratio": 1.678343949044586, "no_speech_prob": 0.0008280895999632776}, {"id": 864, "seek": 513724, "start": 5159.96, "end": 5165.4, "text": " is calling the technology intelligence inaccurate? Are we just dreaming of something", "tokens": [51500, 307, 5141, 264, 2899, 7599, 46443, 30, 2014, 321, 445, 21475, 295, 746, 51772], "temperature": 0.0, "avg_logprob": -0.18806693667457217, "compression_ratio": 1.678343949044586, "no_speech_prob": 0.0008280895999632776}, {"id": 865, "seek": 516540, "start": 5165.4, "end": 5170.28, "text": " that can never be? And then to follow up on that, you've got Tom Fatcher who asks,", "tokens": [50364, 300, 393, 1128, 312, 30, 400, 550, 281, 1524, 493, 322, 300, 11, 291, 600, 658, 5041, 479, 49871, 567, 8962, 11, 50608], "temperature": 0.0, "avg_logprob": -0.10369465198922664, "compression_ratio": 1.604, "no_speech_prob": 0.0004929568967781961}, {"id": 866, "seek": 516540, "start": 5170.28, "end": 5174.28, "text": " is there anything happening to develop native analog neural networks", "tokens": [50608, 307, 456, 1340, 2737, 281, 1499, 8470, 16660, 18161, 9590, 50808], "temperature": 0.0, "avg_logprob": -0.10369465198922664, "compression_ratio": 1.604, "no_speech_prob": 0.0004929568967781961}, {"id": 867, "seek": 516540, "start": 5174.28, "end": 5177.48, "text": " rather than doing neural networks in a digital machine only?", "tokens": [50808, 2831, 813, 884, 18161, 9590, 294, 257, 4562, 3479, 787, 30, 50968], "temperature": 0.0, "avg_logprob": -0.10369465198922664, "compression_ratio": 1.604, "no_speech_prob": 0.0004929568967781961}, {"id": 868, "seek": 516540, "start": 5179.08, "end": 5185.24, "text": " Take the second one. Yeah, there certainly is. So Steve Ferber at Manchester is building hardware", "tokens": [51048, 3664, 264, 1150, 472, 13, 865, 11, 456, 3297, 307, 13, 407, 7466, 10728, 607, 412, 27180, 307, 2390, 8837, 51356], "temperature": 0.0, "avg_logprob": -0.10369465198922664, "compression_ratio": 1.604, "no_speech_prob": 0.0004929568967781961}, {"id": 869, "seek": 516540, "start": 5185.24, "end": 5191.799999999999, "text": " neural networks. But the moment it's just much cheaper and much more efficient to do it in", "tokens": [51356, 18161, 9590, 13, 583, 264, 1623, 309, 311, 445, 709, 12284, 293, 709, 544, 7148, 281, 360, 309, 294, 51684], "temperature": 0.0, "avg_logprob": -0.10369465198922664, "compression_ratio": 1.604, "no_speech_prob": 0.0004929568967781961}, {"id": 870, "seek": 519180, "start": 5191.8, "end": 5197.4800000000005, "text": " software. There have been various attempts over the years to develop neural net processes,", "tokens": [50364, 4722, 13, 821, 362, 668, 3683, 15257, 670, 264, 924, 281, 1499, 18161, 2533, 7555, 11, 50648], "temperature": 0.0, "avg_logprob": -0.0836715074343102, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.00840008445084095}, {"id": 871, "seek": 519180, "start": 5198.6, "end": 5202.68, "text": " famous phrase from the movie that you're not allowed to mention to AI researchers,", "tokens": [50704, 4618, 9535, 490, 264, 3169, 300, 291, 434, 406, 4350, 281, 2152, 281, 7318, 10309, 11, 50908], "temperature": 0.0, "avg_logprob": -0.0836715074343102, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.00840008445084095}, {"id": 872, "seek": 519180, "start": 5203.400000000001, "end": 5208.28, "text": " the Terminator movies, the neural network processes. If you want to wind up an AI researcher,", "tokens": [50944, 264, 19835, 31927, 6233, 11, 264, 18161, 3209, 7555, 13, 759, 291, 528, 281, 2468, 493, 364, 7318, 21751, 11, 51188], "temperature": 0.0, "avg_logprob": -0.0836715074343102, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.00840008445084095}, {"id": 873, "seek": 519180, "start": 5208.28, "end": 5215.16, "text": " just bring up the Terminator. It's a shortcut to triggering them. But neural network processes", "tokens": [51188, 445, 1565, 493, 264, 19835, 31927, 13, 467, 311, 257, 24822, 281, 40406, 552, 13, 583, 18161, 3209, 7555, 51532], "temperature": 0.0, "avg_logprob": -0.0836715074343102, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.00840008445084095}, {"id": 874, "seek": 519180, "start": 5215.16, "end": 5219.8, "text": " have never really taken off. Doesn't mean they won't do, but at the moment, it's just much cheaper", "tokens": [51532, 362, 1128, 534, 2726, 766, 13, 12955, 380, 914, 436, 1582, 380, 360, 11, 457, 412, 264, 1623, 11, 309, 311, 445, 709, 12284, 51764], "temperature": 0.0, "avg_logprob": -0.0836715074343102, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.00840008445084095}, {"id": 875, "seek": 521980, "start": 5219.8, "end": 5225.400000000001, "text": " and much more efficient to throw more conventional GPUs and so on at the problem. It doesn't mean", "tokens": [50364, 293, 709, 544, 7148, 281, 3507, 544, 16011, 18407, 82, 293, 370, 322, 412, 264, 1154, 13, 467, 1177, 380, 914, 50644], "temperature": 0.0, "avg_logprob": -0.10680313110351562, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017951609333977103}, {"id": 876, "seek": 521980, "start": 5225.400000000001, "end": 5228.6, "text": " it won't happen, but at the moment, it's not there yet. What was the other question again, the first", "tokens": [50644, 309, 1582, 380, 1051, 11, 457, 412, 264, 1623, 11, 309, 311, 406, 456, 1939, 13, 708, 390, 264, 661, 1168, 797, 11, 264, 700, 50804], "temperature": 0.0, "avg_logprob": -0.10680313110351562, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017951609333977103}, {"id": 877, "seek": 521980, "start": 5228.6, "end": 5232.4400000000005, "text": " one? So the other question was, are we basically the terminology being used? If emergency is", "tokens": [50804, 472, 30, 407, 264, 661, 1168, 390, 11, 366, 321, 1936, 264, 27575, 885, 1143, 30, 759, 7473, 307, 50996], "temperature": 0.0, "avg_logprob": -0.10680313110351562, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017951609333977103}, {"id": 878, "seek": 521980, "start": 5232.4400000000005, "end": 5237.08, "text": " inaccurate, is calling the technology intelligence inaccurate? And are we dreaming of something", "tokens": [50996, 46443, 11, 307, 5141, 264, 2899, 7599, 46443, 30, 400, 366, 321, 21475, 295, 746, 51228], "temperature": 0.0, "avg_logprob": -0.10680313110351562, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017951609333977103}, {"id": 879, "seek": 521980, "start": 5237.08, "end": 5244.84, "text": " that can never be? Yeah, so the phrase artificial intelligence was coined by John McCarthy around", "tokens": [51228, 300, 393, 1128, 312, 30, 865, 11, 370, 264, 9535, 11677, 7599, 390, 45222, 538, 2619, 44085, 926, 51616], "temperature": 0.0, "avg_logprob": -0.10680313110351562, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017951609333977103}, {"id": 880, "seek": 524484, "start": 5244.84, "end": 5251.08, "text": " about 1955. He was 28 years old, a young American researcher, and he wants funding to get a whole", "tokens": [50364, 466, 46881, 13, 634, 390, 7562, 924, 1331, 11, 257, 2037, 2665, 21751, 11, 293, 415, 2738, 6137, 281, 483, 257, 1379, 50676], "temperature": 0.0, "avg_logprob": -0.11571889264242989, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.010390485636889935}, {"id": 881, "seek": 524484, "start": 5251.08, "end": 5254.84, "text": " bunch of researchers together for a summer, and he thinks they'll solve artificial intelligence", "tokens": [50676, 3840, 295, 10309, 1214, 337, 257, 4266, 11, 293, 415, 7309, 436, 603, 5039, 11677, 7599, 50864], "temperature": 0.0, "avg_logprob": -0.11571889264242989, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.010390485636889935}, {"id": 882, "seek": 524484, "start": 5254.84, "end": 5260.4400000000005, "text": " in a summer. But he has to give a title to his proposal, which goes to the Rockefeller Foundation,", "tokens": [50864, 294, 257, 4266, 13, 583, 415, 575, 281, 976, 257, 4876, 281, 702, 11494, 11, 597, 1709, 281, 264, 50178, 14983, 10335, 11, 51144], "temperature": 0.0, "avg_logprob": -0.11571889264242989, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.010390485636889935}, {"id": 883, "seek": 524484, "start": 5260.4400000000005, "end": 5265.72, "text": " and he fixes on artificial intelligence. And boy, have we regretted that ever since.", "tokens": [51144, 293, 415, 32539, 322, 11677, 7599, 13, 400, 3237, 11, 362, 321, 10879, 14727, 300, 1562, 1670, 13, 51408], "temperature": 0.0, "avg_logprob": -0.11571889264242989, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.010390485636889935}, {"id": 884, "seek": 524484, "start": 5266.4400000000005, "end": 5272.6, "text": " The problem is, firstly, artificial sounds like fake. It sounds like ursat. I mean,", "tokens": [51444, 440, 1154, 307, 11, 27376, 11, 11677, 3263, 411, 7592, 13, 467, 3263, 411, 4038, 82, 267, 13, 286, 914, 11, 51752], "temperature": 0.0, "avg_logprob": -0.11571889264242989, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.010390485636889935}, {"id": 885, "seek": 527260, "start": 5272.68, "end": 5278.92, "text": " who wants fake intelligence? And for intelligence itself, the problem is that so many of the", "tokens": [50368, 567, 2738, 7592, 7599, 30, 400, 337, 7599, 2564, 11, 264, 1154, 307, 300, 370, 867, 295, 264, 50680], "temperature": 0.0, "avg_logprob": -0.06442713287641418, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.004728726577013731}, {"id": 886, "seek": 527260, "start": 5278.92, "end": 5283.96, "text": " problems that have just proved to be really hard for AI actually don't seem to require", "tokens": [50680, 2740, 300, 362, 445, 14617, 281, 312, 534, 1152, 337, 7318, 767, 500, 380, 1643, 281, 3651, 50932], "temperature": 0.0, "avg_logprob": -0.06442713287641418, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.004728726577013731}, {"id": 887, "seek": 527260, "start": 5283.96, "end": 5289.72, "text": " intelligence at all. So the classic example, driving a car. When somebody passes their driving test,", "tokens": [50932, 7599, 412, 439, 13, 407, 264, 7230, 1365, 11, 4840, 257, 1032, 13, 1133, 2618, 11335, 641, 4840, 1500, 11, 51220], "temperature": 0.0, "avg_logprob": -0.06442713287641418, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.004728726577013731}, {"id": 888, "seek": 527260, "start": 5289.72, "end": 5295.96, "text": " they don't think, wow, you're a genius. It doesn't seem to require intelligence in people,", "tokens": [51220, 436, 500, 380, 519, 11, 6076, 11, 291, 434, 257, 14017, 13, 467, 1177, 380, 1643, 281, 3651, 7599, 294, 561, 11, 51532], "temperature": 0.0, "avg_logprob": -0.06442713287641418, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.004728726577013731}, {"id": 889, "seek": 527260, "start": 5295.96, "end": 5301.320000000001, "text": " but I cannot tell you how much money has been thrown at driverless car technologies,", "tokens": [51532, 457, 286, 2644, 980, 291, 577, 709, 1460, 575, 668, 11732, 412, 6787, 1832, 1032, 7943, 11, 51800], "temperature": 0.0, "avg_logprob": -0.06442713287641418, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.004728726577013731}, {"id": 890, "seek": 530132, "start": 5301.32, "end": 5306.2, "text": " and we are a long way off from jumping into a car and saying, take me to a country pub,", "tokens": [50364, 293, 321, 366, 257, 938, 636, 766, 490, 11233, 666, 257, 1032, 293, 1566, 11, 747, 385, 281, 257, 1941, 1535, 11, 50608], "temperature": 0.0, "avg_logprob": -0.08317096416766827, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.00030162703478708863}, {"id": 891, "seek": 530132, "start": 5306.92, "end": 5313.5599999999995, "text": " which is my dream of the technology, I have to tell you. We're a long, long way off.", "tokens": [50644, 597, 307, 452, 3055, 295, 264, 2899, 11, 286, 362, 281, 980, 291, 13, 492, 434, 257, 938, 11, 938, 636, 766, 13, 50976], "temperature": 0.0, "avg_logprob": -0.08317096416766827, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.00030162703478708863}, {"id": 892, "seek": 530132, "start": 5313.5599999999995, "end": 5322.04, "text": " So it's a classic example of what people think AI is focused on is deep intellectual tasks,", "tokens": [50976, 407, 309, 311, 257, 7230, 1365, 295, 437, 561, 519, 7318, 307, 5178, 322, 307, 2452, 12576, 9608, 11, 51400], "temperature": 0.0, "avg_logprob": -0.08317096416766827, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.00030162703478708863}, {"id": 893, "seek": 530132, "start": 5322.04, "end": 5327.0, "text": " and that's actually not where the most difficult problems are. The difficult problems are actually", "tokens": [51400, 293, 300, 311, 767, 406, 689, 264, 881, 2252, 2740, 366, 13, 440, 2252, 2740, 366, 767, 51648], "temperature": 0.0, "avg_logprob": -0.08317096416766827, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.00030162703478708863}, {"id": 894, "seek": 532700, "start": 5327.0, "end": 5354.76, "text": " surprisingly mundane. Well, I was interested in how you mentioned that", "tokens": [50364, 17600, 43497, 13, 1042, 11, 286, 390, 3102, 294, 577, 291, 2835, 300, 51752], "temperature": 0.0, "avg_logprob": -0.3145582255195169, "compression_ratio": 0.9722222222222222, "no_speech_prob": 0.010759733617305756}, {"id": 895, "seek": 535476, "start": 5355.72, "end": 5363.4800000000005, "text": " the two pools of AI study were symbolic AI and big AI, and I was wondering how you sawed how", "tokens": [50412, 264, 732, 28688, 295, 7318, 2979, 645, 25755, 7318, 293, 955, 7318, 11, 293, 286, 390, 6359, 577, 291, 1866, 292, 577, 50800], "temperature": 0.0, "avg_logprob": -0.1735126601325141, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.030225461348891258}, {"id": 896, "seek": 535476, "start": 5364.12, "end": 5369.64, "text": " your viewpoint on the change in focus from one to another throughout your career.", "tokens": [50832, 428, 35248, 322, 264, 1319, 294, 1879, 490, 472, 281, 1071, 3710, 428, 3988, 13, 51108], "temperature": 0.0, "avg_logprob": -0.1735126601325141, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.030225461348891258}, {"id": 897, "seek": 535476, "start": 5369.64, "end": 5376.92, "text": " Yeah. So an enormous number of people are busy looking at that right now. So remember symbolic AI,", "tokens": [51108, 865, 13, 407, 364, 11322, 1230, 295, 561, 366, 5856, 1237, 412, 300, 558, 586, 13, 407, 1604, 25755, 7318, 11, 51472], "temperature": 0.0, "avg_logprob": -0.1735126601325141, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.030225461348891258}, {"id": 898, "seek": 535476, "start": 5376.92, "end": 5382.04, "text": " which is the tradition that I grew up in AI, which was dominant for kind of 30 years in the AI", "tokens": [51472, 597, 307, 264, 6994, 300, 286, 6109, 493, 294, 7318, 11, 597, 390, 15657, 337, 733, 295, 2217, 924, 294, 264, 7318, 51728], "temperature": 0.0, "avg_logprob": -0.1735126601325141, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.030225461348891258}, {"id": 899, "seek": 538204, "start": 5382.04, "end": 5388.68, "text": " community, is roughly, and again, hand-waving madly at this point, and lots and lots of my", "tokens": [50364, 1768, 11, 307, 9810, 11, 293, 797, 11, 1011, 12, 86, 6152, 5244, 356, 412, 341, 935, 11, 293, 3195, 293, 3195, 295, 452, 50696], "temperature": 0.0, "avg_logprob": -0.11377798390184712, "compression_ratio": 1.8136882129277567, "no_speech_prob": 0.008527103811502457}, {"id": 900, "seek": 538204, "start": 5388.68, "end": 5393.32, "text": " colleagues are cringing madly at this point, roughly speaking, the idea of symbolic AI is that", "tokens": [50696, 7734, 366, 941, 8716, 5244, 356, 412, 341, 935, 11, 9810, 4124, 11, 264, 1558, 295, 25755, 7318, 307, 300, 50928], "temperature": 0.0, "avg_logprob": -0.11377798390184712, "compression_ratio": 1.8136882129277567, "no_speech_prob": 0.008527103811502457}, {"id": 901, "seek": 538204, "start": 5393.32, "end": 5399.64, "text": " you're modeling the mind, the conscious mind, conscious mental reasoning processes, where you", "tokens": [50928, 291, 434, 15983, 264, 1575, 11, 264, 6648, 1575, 11, 6648, 4973, 21577, 7555, 11, 689, 291, 51244], "temperature": 0.0, "avg_logprob": -0.11377798390184712, "compression_ratio": 1.8136882129277567, "no_speech_prob": 0.008527103811502457}, {"id": 902, "seek": 538204, "start": 5399.64, "end": 5404.2, "text": " have a conversation with yourself, and you have a conversation in a language, right? You're trying", "tokens": [51244, 362, 257, 3761, 365, 1803, 11, 293, 291, 362, 257, 3761, 294, 257, 2856, 11, 558, 30, 509, 434, 1382, 51472], "temperature": 0.0, "avg_logprob": -0.11377798390184712, "compression_ratio": 1.8136882129277567, "no_speech_prob": 0.008527103811502457}, {"id": 903, "seek": 538204, "start": 5404.2, "end": 5408.76, "text": " to decide whether to go to this lecture tonight, and you think, well, yeah, but there's EastEnders", "tokens": [51472, 281, 4536, 1968, 281, 352, 281, 341, 7991, 4440, 11, 293, 291, 519, 11, 731, 11, 1338, 11, 457, 456, 311, 6747, 36952, 433, 51700], "temperature": 0.0, "avg_logprob": -0.11377798390184712, "compression_ratio": 1.8136882129277567, "no_speech_prob": 0.008527103811502457}, {"id": 904, "seek": 540876, "start": 5408.76, "end": 5415.0, "text": " on TV, and mom's cooking a nice meal, but then it's going to be really interesting. You weigh up", "tokens": [50364, 322, 3558, 11, 293, 1225, 311, 6361, 257, 1481, 6791, 11, 457, 550, 309, 311, 516, 281, 312, 534, 1880, 13, 509, 13843, 493, 50676], "temperature": 0.0, "avg_logprob": -0.12747660089046398, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.005268792621791363}, {"id": 905, "seek": 540876, "start": 5415.0, "end": 5422.12, "text": " those options, and literally symbolic AI tries to capture that kind of thing explicitly, and using", "tokens": [50676, 729, 3956, 11, 293, 3736, 25755, 7318, 9898, 281, 7983, 300, 733, 295, 551, 20803, 11, 293, 1228, 51032], "temperature": 0.0, "avg_logprob": -0.12747660089046398, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.005268792621791363}, {"id": 906, "seek": 540876, "start": 5422.12, "end": 5428.84, "text": " languages that with a bit of squinting resemble human languages. Then we've got the alternative", "tokens": [51032, 8650, 300, 365, 257, 857, 295, 2339, 686, 278, 36870, 1952, 8650, 13, 1396, 321, 600, 658, 264, 8535, 51368], "temperature": 0.0, "avg_logprob": -0.12747660089046398, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.005268792621791363}, {"id": 907, "seek": 540876, "start": 5428.84, "end": 5435.24, "text": " approach, which is machine learning, data-driven, and so on, which, again, I emphasize with neural", "tokens": [51368, 3109, 11, 597, 307, 3479, 2539, 11, 1412, 12, 25456, 11, 293, 370, 322, 11, 597, 11, 797, 11, 286, 16078, 365, 18161, 51688], "temperature": 0.0, "avg_logprob": -0.12747660089046398, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.005268792621791363}, {"id": 908, "seek": 543524, "start": 5435.24, "end": 5439.8, "text": " approaches, we're not trying to build artificial brains, that's not what's going on,", "tokens": [50364, 11587, 11, 321, 434, 406, 1382, 281, 1322, 11677, 15442, 11, 300, 311, 406, 437, 311, 516, 322, 11, 50592], "temperature": 0.0, "avg_logprob": -0.0792133223335698, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.004236279986798763}, {"id": 909, "seek": 543524, "start": 5439.8, "end": 5444.84, "text": " but we're taking inspiration from the structures that we see in brains and nervous systems,", "tokens": [50592, 457, 321, 434, 1940, 10249, 490, 264, 9227, 300, 321, 536, 294, 15442, 293, 6296, 3652, 11, 50844], "temperature": 0.0, "avg_logprob": -0.0792133223335698, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.004236279986798763}, {"id": 910, "seek": 543524, "start": 5444.84, "end": 5451.24, "text": " and in particular, the idea that large computational tasks can be reduced down to tiny, simple", "tokens": [50844, 293, 294, 1729, 11, 264, 1558, 300, 2416, 28270, 9608, 393, 312, 9212, 760, 281, 5870, 11, 2199, 51164], "temperature": 0.0, "avg_logprob": -0.0792133223335698, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.004236279986798763}, {"id": 911, "seek": 543524, "start": 5451.24, "end": 5457.639999999999, "text": " pattern recognition problems. Okay, but we've seen, for example, that large language models", "tokens": [51164, 5102, 11150, 2740, 13, 1033, 11, 457, 321, 600, 1612, 11, 337, 1365, 11, 300, 2416, 2856, 5245, 51484], "temperature": 0.0, "avg_logprob": -0.0792133223335698, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.004236279986798763}, {"id": 912, "seek": 543524, "start": 5457.639999999999, "end": 5463.08, "text": " get things wrong a lot, and a lot of people have said, but look, maybe if you just married", "tokens": [51484, 483, 721, 2085, 257, 688, 11, 293, 257, 688, 295, 561, 362, 848, 11, 457, 574, 11, 1310, 498, 291, 445, 5259, 51756], "temperature": 0.0, "avg_logprob": -0.0792133223335698, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.004236279986798763}, {"id": 913, "seek": 546308, "start": 5463.08, "end": 5468.2, "text": " the neural and the symbolic together so that the symbolic system did have something like a", "tokens": [50364, 264, 18161, 293, 264, 25755, 1214, 370, 300, 264, 25755, 1185, 630, 362, 746, 411, 257, 50620], "temperature": 0.0, "avg_logprob": -0.08020253274955001, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.0008898490341380239}, {"id": 914, "seek": 546308, "start": 5468.2, "end": 5473.08, "text": " database of facts, that you could put that together with a large language model and be able to", "tokens": [50620, 8149, 295, 9130, 11, 300, 291, 727, 829, 300, 1214, 365, 257, 2416, 2856, 2316, 293, 312, 1075, 281, 50864], "temperature": 0.0, "avg_logprob": -0.08020253274955001, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.0008898490341380239}, {"id": 915, "seek": 546308, "start": 5474.6, "end": 5480.84, "text": " improve the outputs of the large language model. The jury is out exactly on how that's going to come", "tokens": [50940, 3470, 264, 23930, 295, 264, 2416, 2856, 2316, 13, 440, 19516, 307, 484, 2293, 322, 577, 300, 311, 516, 281, 808, 51252], "temperature": 0.0, "avg_logprob": -0.08020253274955001, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.0008898490341380239}, {"id": 916, "seek": 546308, "start": 5480.84, "end": 5487.72, "text": " out. Lots of different ideas out there now. Trillion-dollar companies are spending billions", "tokens": [51252, 484, 13, 15908, 295, 819, 3487, 484, 456, 586, 13, 1765, 11836, 12, 40485, 3431, 366, 6434, 17375, 51596], "temperature": 0.0, "avg_logprob": -0.08020253274955001, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.0008898490341380239}, {"id": 917, "seek": 546308, "start": 5487.72, "end": 5492.5199999999995, "text": " of dollars right now to investigate exactly the question that you've put out there,", "tokens": [51596, 295, 3808, 558, 586, 281, 15013, 2293, 264, 1168, 300, 291, 600, 829, 484, 456, 11, 51836], "temperature": 0.0, "avg_logprob": -0.08020253274955001, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.0008898490341380239}, {"id": 918, "seek": 549252, "start": 5492.6, "end": 5498.4400000000005, "text": " so it's an extremely pertinent question. There's no, I say, I don't see any answer on the horizon", "tokens": [50368, 370, 309, 311, 364, 4664, 13269, 11058, 1168, 13, 821, 311, 572, 11, 286, 584, 11, 286, 500, 380, 536, 604, 1867, 322, 264, 18046, 50660], "temperature": 0.0, "avg_logprob": -0.07208977142969768, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.0006356089725159109}, {"id": 919, "seek": 549252, "start": 5498.4400000000005, "end": 5505.0, "text": " right now, which looks like it's going to win out. My worry is that what we'll end up with", "tokens": [50660, 558, 586, 11, 597, 1542, 411, 309, 311, 516, 281, 1942, 484, 13, 1222, 3292, 307, 300, 437, 321, 603, 917, 493, 365, 50988], "temperature": 0.0, "avg_logprob": -0.07208977142969768, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.0006356089725159109}, {"id": 920, "seek": 549252, "start": 5505.0, "end": 5511.240000000001, "text": " is a kind of unscientific solution. That is a solution which is sort of hacked together", "tokens": [50988, 307, 257, 733, 295, 2693, 5412, 1089, 3827, 13, 663, 307, 257, 3827, 597, 307, 1333, 295, 36218, 1214, 51300], "temperature": 0.0, "avg_logprob": -0.07208977142969768, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.0006356089725159109}, {"id": 921, "seek": 549252, "start": 5511.240000000001, "end": 5517.72, "text": " without any deep underlying principles, and as a scientist, what I would want to see is something", "tokens": [51300, 1553, 604, 2452, 14217, 9156, 11, 293, 382, 257, 12662, 11, 437, 286, 576, 528, 281, 536, 307, 746, 51624], "temperature": 0.0, "avg_logprob": -0.07208977142969768, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.0006356089725159109}, {"id": 922, "seek": 551772, "start": 5517.72, "end": 5522.76, "text": " which was tied together with deep scientific principles, but it's an extremely pertinent", "tokens": [50364, 597, 390, 9601, 1214, 365, 2452, 8134, 9156, 11, 457, 309, 311, 364, 4664, 13269, 11058, 50616], "temperature": 0.0, "avg_logprob": -0.17444584186260517, "compression_ratio": 1.4158415841584158, "no_speech_prob": 0.0006168949767015874}, {"id": 923, "seek": 551772, "start": 5522.76, "end": 5529.8, "text": " question, and I say right now an enormous number of PhD students across the world are busy looking", "tokens": [50616, 1168, 11, 293, 286, 584, 558, 586, 364, 11322, 1230, 295, 14476, 1731, 2108, 264, 1002, 366, 5856, 1237, 50968], "temperature": 0.0, "avg_logprob": -0.17444584186260517, "compression_ratio": 1.4158415841584158, "no_speech_prob": 0.0006168949767015874}, {"id": 924, "seek": 551772, "start": 5529.8, "end": 5534.68, "text": " at exactly what you've just described. Thank you, Mike. Time for a squeeze and two more questions.", "tokens": [50968, 412, 2293, 437, 291, 600, 445, 7619, 13, 1044, 291, 11, 6602, 13, 6161, 337, 257, 13578, 293, 732, 544, 1651, 13, 51212], "temperature": 0.0, "avg_logprob": -0.17444584186260517, "compression_ratio": 1.4158415841584158, "no_speech_prob": 0.0006168949767015874}, {"id": 925, "seek": 553468, "start": 5534.84, "end": 5539.8, "text": " Take one from in the room. Cool. We've got a question just in the middle at the back there.", "tokens": [50372, 3664, 472, 490, 294, 264, 1808, 13, 8561, 13, 492, 600, 658, 257, 1168, 445, 294, 264, 2808, 412, 264, 646, 456, 13, 50620], "temperature": 0.0, "avg_logprob": -0.24204678231097282, "compression_ratio": 1.3464566929133859, "no_speech_prob": 0.002729271538555622}, {"id": 926, "seek": 553468, "start": 5553.88, "end": 5561.08, "text": " For the lecture, my question is around, you sort of took us on the journey from", "tokens": [51324, 1171, 264, 7991, 11, 452, 1168, 307, 926, 11, 291, 1333, 295, 1890, 505, 322, 264, 4671, 490, 51684], "temperature": 0.0, "avg_logprob": -0.24204678231097282, "compression_ratio": 1.3464566929133859, "no_speech_prob": 0.002729271538555622}, {"id": 927, "seek": 556108, "start": 5561.08, "end": 5568.28, "text": " 40 years ago, some of the inspirations around how the mind works and the mathematics. He said", "tokens": [50364, 3356, 924, 2057, 11, 512, 295, 264, 17432, 763, 926, 577, 264, 1575, 1985, 293, 264, 18666, 13, 634, 848, 50724], "temperature": 0.0, "avg_logprob": -0.1415128331435354, "compression_ratio": 1.4974093264248705, "no_speech_prob": 0.005979891866445541}, {"id": 928, "seek": 556108, "start": 5568.28, "end": 5575.48, "text": " the mathematics was fairly simple. I would like your opinion. Where do you think we're not looking", "tokens": [50724, 264, 18666, 390, 6457, 2199, 13, 286, 576, 411, 428, 4800, 13, 2305, 360, 291, 519, 321, 434, 406, 1237, 51084], "temperature": 0.0, "avg_logprob": -0.1415128331435354, "compression_ratio": 1.4974093264248705, "no_speech_prob": 0.005979891866445541}, {"id": 929, "seek": 556108, "start": 5575.48, "end": 5586.12, "text": " enough for where leap be? Oh, wow. If I knew that, I'd be forming a company, I have to tell you.", "tokens": [51084, 1547, 337, 689, 19438, 312, 30, 876, 11, 6076, 13, 759, 286, 2586, 300, 11, 286, 1116, 312, 15745, 257, 2237, 11, 286, 362, 281, 980, 291, 13, 51616], "temperature": 0.0, "avg_logprob": -0.1415128331435354, "compression_ratio": 1.4974093264248705, "no_speech_prob": 0.005979891866445541}, {"id": 930, "seek": 558612, "start": 5586.599999999999, "end": 5593.32, "text": " Okay, so I think the first thing to say is I said when it started to become clear that this", "tokens": [50388, 1033, 11, 370, 286, 519, 264, 700, 551, 281, 584, 307, 286, 848, 562, 309, 1409, 281, 1813, 1850, 300, 341, 50724], "temperature": 0.0, "avg_logprob": -0.12751156574970968, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0013594315387308598}, {"id": 931, "seek": 558612, "start": 5593.32, "end": 5598.2, "text": " technology was worked, Silicon Valley starts to make bets, and these bets are billion dollar", "tokens": [50724, 2899, 390, 2732, 11, 25351, 10666, 3719, 281, 652, 39922, 11, 293, 613, 39922, 366, 5218, 7241, 50968], "temperature": 0.0, "avg_logprob": -0.12751156574970968, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0013594315387308598}, {"id": 932, "seek": 558612, "start": 5598.2, "end": 5603.4, "text": " bets, a lot of billion dollar bets going on, investing in a very, very wide range of different", "tokens": [50968, 39922, 11, 257, 688, 295, 5218, 7241, 39922, 516, 322, 11, 10978, 294, 257, 588, 11, 588, 4874, 3613, 295, 819, 51228], "temperature": 0.0, "avg_logprob": -0.12751156574970968, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0013594315387308598}, {"id": 933, "seek": 558612, "start": 5603.4, "end": 5608.28, "text": " ideas in the hope that one is going to be the one that delivers something which is going to give", "tokens": [51228, 3487, 294, 264, 1454, 300, 472, 307, 516, 281, 312, 264, 472, 300, 24860, 746, 597, 307, 516, 281, 976, 51472], "temperature": 0.0, "avg_logprob": -0.12751156574970968, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0013594315387308598}, {"id": 934, "seek": 558612, "start": 5608.28, "end": 5614.36, "text": " them a competitive advantage. That's the context in which we're trying to figure out what the next", "tokens": [51472, 552, 257, 10043, 5002, 13, 663, 311, 264, 4319, 294, 597, 321, 434, 1382, 281, 2573, 484, 437, 264, 958, 51776], "temperature": 0.0, "avg_logprob": -0.12751156574970968, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0013594315387308598}, {"id": 935, "seek": 561436, "start": 5614.36, "end": 5624.92, "text": " big thing is going to be. I think this multimodal is going to be dominant. That's what we're going", "tokens": [50364, 955, 551, 307, 516, 281, 312, 13, 286, 519, 341, 32972, 378, 304, 307, 516, 281, 312, 15657, 13, 663, 311, 437, 321, 434, 516, 50892], "temperature": 0.0, "avg_logprob": -0.08034606291868976, "compression_ratio": 1.8725490196078431, "no_speech_prob": 0.006920653395354748}, {"id": 936, "seek": 561436, "start": 5624.92, "end": 5629.4, "text": " to see, and you're going to hear that phrase, multimodal. Remember, you heard it here first,", "tokens": [50892, 281, 536, 11, 293, 291, 434, 516, 281, 1568, 300, 9535, 11, 32972, 378, 304, 13, 5459, 11, 291, 2198, 309, 510, 700, 11, 51116], "temperature": 0.0, "avg_logprob": -0.08034606291868976, "compression_ratio": 1.8725490196078431, "no_speech_prob": 0.006920653395354748}, {"id": 937, "seek": 561436, "start": 5629.4, "end": 5634.44, "text": " if you've never heard it before. You're going to hear that a lot, and that's going to be text,", "tokens": [51116, 498, 291, 600, 1128, 2198, 309, 949, 13, 509, 434, 516, 281, 1568, 300, 257, 688, 11, 293, 300, 311, 516, 281, 312, 2487, 11, 51368], "temperature": 0.0, "avg_logprob": -0.08034606291868976, "compression_ratio": 1.8725490196078431, "no_speech_prob": 0.006920653395354748}, {"id": 938, "seek": 561436, "start": 5634.44, "end": 5641.799999999999, "text": " images, sound, video. You're going to be able to upload videos, and the AI will describe what's", "tokens": [51368, 5267, 11, 1626, 11, 960, 13, 509, 434, 516, 281, 312, 1075, 281, 6580, 2145, 11, 293, 264, 7318, 486, 6786, 437, 311, 51736], "temperature": 0.0, "avg_logprob": -0.08034606291868976, "compression_ratio": 1.8725490196078431, "no_speech_prob": 0.006920653395354748}, {"id": 939, "seek": 564180, "start": 5641.8, "end": 5646.28, "text": " going on in the video or produce a summary, and you'll be able to say what happens after this bit", "tokens": [50364, 516, 322, 294, 264, 960, 420, 5258, 257, 12691, 11, 293, 291, 603, 312, 1075, 281, 584, 437, 2314, 934, 341, 857, 50588], "temperature": 0.0, "avg_logprob": -0.09045979284471081, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.020446885377168655}, {"id": 940, "seek": 564180, "start": 5646.28, "end": 5651.16, "text": " in the video, and it will be able to come out with a description of that for you. Alternatively,", "tokens": [50588, 294, 264, 960, 11, 293, 309, 486, 312, 1075, 281, 808, 484, 365, 257, 3855, 295, 300, 337, 291, 13, 46167, 11, 50832], "temperature": 0.0, "avg_logprob": -0.09045979284471081, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.020446885377168655}, {"id": 941, "seek": 564180, "start": 5651.16, "end": 5655.4800000000005, "text": " you'll be able to give a storyline, and it will generate videos for you. Ultimately,", "tokens": [50832, 291, 603, 312, 1075, 281, 976, 257, 30828, 11, 293, 309, 486, 8460, 2145, 337, 291, 13, 23921, 11, 51048], "temperature": 0.0, "avg_logprob": -0.09045979284471081, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.020446885377168655}, {"id": 942, "seek": 564180, "start": 5655.4800000000005, "end": 5660.84, "text": " where it's going to go is in virtual reality. I don't know if you like Lord of the Rings or", "tokens": [51048, 689, 309, 311, 516, 281, 352, 307, 294, 6374, 4103, 13, 286, 500, 380, 458, 498, 291, 411, 3257, 295, 264, 38543, 420, 51316], "temperature": 0.0, "avg_logprob": -0.09045979284471081, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.020446885377168655}, {"id": 943, "seek": 564180, "start": 5660.84, "end": 5666.2, "text": " Star Wars, but I enjoy both of those, and wouldn't you love to see a mash-up of those two things?", "tokens": [51316, 5705, 9818, 11, 457, 286, 2103, 1293, 295, 729, 11, 293, 2759, 380, 291, 959, 281, 536, 257, 31344, 12, 1010, 295, 729, 732, 721, 30, 51584], "temperature": 0.0, "avg_logprob": -0.09045979284471081, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.020446885377168655}, {"id": 944, "seek": 566620, "start": 5666.44, "end": 5673.8, "text": " Generative AI will be able to do it for you. I used to think this was just a bit of a pipe dream,", "tokens": [50376, 15409, 1166, 7318, 486, 312, 1075, 281, 360, 309, 337, 291, 13, 286, 1143, 281, 519, 341, 390, 445, 257, 857, 295, 257, 11240, 3055, 11, 50744], "temperature": 0.0, "avg_logprob": -0.14686280262621143, "compression_ratio": 1.4371859296482412, "no_speech_prob": 0.0918228030204773}, {"id": 945, "seek": 566620, "start": 5673.8, "end": 5681.639999999999, "text": " but actually, at the moment, it seems completely plausible. If you like the original Star Trek", "tokens": [50744, 457, 767, 11, 412, 264, 1623, 11, 309, 2544, 2584, 39925, 13, 759, 291, 411, 264, 3380, 5705, 25845, 51136], "temperature": 0.0, "avg_logprob": -0.14686280262621143, "compression_ratio": 1.4371859296482412, "no_speech_prob": 0.0918228030204773}, {"id": 946, "seek": 566620, "start": 5681.639999999999, "end": 5689.96, "text": " series, which I do, and my family doesn't, but there was only 60-odd episodes of them. In the", "tokens": [51136, 2638, 11, 597, 286, 360, 11, 293, 452, 1605, 1177, 380, 11, 457, 456, 390, 787, 4060, 12, 378, 67, 9313, 295, 552, 13, 682, 264, 51552], "temperature": 0.0, "avg_logprob": -0.14686280262621143, "compression_ratio": 1.4371859296482412, "no_speech_prob": 0.0918228030204773}, {"id": 947, "seek": 568996, "start": 5689.96, "end": 5696.68, "text": " generative AI future, there will be as many episodes as you want, and it will look and sound", "tokens": [50364, 1337, 1166, 7318, 2027, 11, 456, 486, 312, 382, 867, 9313, 382, 291, 528, 11, 293, 309, 486, 574, 293, 1626, 50700], "temperature": 0.0, "avg_logprob": -0.08145490947522616, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.01192894671112299}, {"id": 948, "seek": 568996, "start": 5696.68, "end": 5702.68, "text": " like Leonard Nimoy and William Shatner perfectly. Maybe the storylines won't be that great, but", "tokens": [50700, 411, 35172, 45251, 939, 293, 6740, 1160, 267, 1193, 6239, 13, 2704, 264, 1657, 11045, 1582, 380, 312, 300, 869, 11, 457, 51000], "temperature": 0.0, "avg_logprob": -0.08145490947522616, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.01192894671112299}, {"id": 949, "seek": 568996, "start": 5702.68, "end": 5707.4, "text": " actually, they don't need to be if they're pressing a button specifically to your tastes.", "tokens": [51000, 767, 11, 436, 500, 380, 643, 281, 312, 498, 436, 434, 12417, 257, 2960, 4682, 281, 428, 8666, 13, 51236], "temperature": 0.0, "avg_logprob": -0.08145490947522616, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.01192894671112299}, {"id": 950, "seek": 568996, "start": 5708.12, "end": 5713.56, "text": " That's the general trajectory of where we're going. I say, actually, I don't see any reason why", "tokens": [51272, 663, 311, 264, 2674, 21512, 295, 689, 321, 434, 516, 13, 286, 584, 11, 767, 11, 286, 500, 380, 536, 604, 1778, 983, 51544], "temperature": 0.0, "avg_logprob": -0.08145490947522616, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.01192894671112299}, {"id": 951, "seek": 571356, "start": 5713.56, "end": 5719.88, "text": " what I've just described is not going to be realistic within decades, and we're going to get", "tokens": [50364, 437, 286, 600, 445, 7619, 307, 406, 516, 281, 312, 12465, 1951, 7878, 11, 293, 321, 434, 516, 281, 483, 50680], "temperature": 0.0, "avg_logprob": -0.0804139351358219, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.018315760418772697}, {"id": 952, "seek": 571356, "start": 5719.88, "end": 5723.8, "text": " there piece by piece. It's not going to happen overnight, but we will get there. I think we", "tokens": [50680, 456, 2522, 538, 2522, 13, 467, 311, 406, 516, 281, 1051, 13935, 11, 457, 321, 486, 483, 456, 13, 286, 519, 321, 50876], "temperature": 0.0, "avg_logprob": -0.0804139351358219, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.018315760418772697}, {"id": 953, "seek": 571356, "start": 5723.8, "end": 5731.400000000001, "text": " genuinely will. The future is going to be wonderful and weird. Thank you, Mike. Do we have any final", "tokens": [50876, 17839, 486, 13, 440, 2027, 307, 516, 281, 312, 3715, 293, 3657, 13, 1044, 291, 11, 6602, 13, 1144, 321, 362, 604, 2572, 51256], "temperature": 0.0, "avg_logprob": -0.0804139351358219, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.018315760418772697}, {"id": 954, "seek": 571356, "start": 5731.400000000001, "end": 5738.120000000001, "text": " very quick questions anywhere? We've got one just over here, I think, in the jumper on the right.", "tokens": [51256, 588, 1702, 1651, 4992, 30, 492, 600, 658, 472, 445, 670, 510, 11, 286, 519, 11, 294, 264, 44061, 322, 264, 558, 13, 51592], "temperature": 0.0, "avg_logprob": -0.0804139351358219, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.018315760418772697}, {"id": 955, "seek": 573812, "start": 5738.12, "end": 5743.4, "text": " Just in the middle here.", "tokens": [50364, 1449, 294, 264, 2808, 510, 13, 50628], "temperature": 0.0, "avg_logprob": -0.24895882606506348, "compression_ratio": 1.2058823529411764, "no_speech_prob": 0.0009731425670906901}, {"id": 956, "seek": 573812, "start": 5750.68, "end": 5757.32, "text": " Hello. Thank you. To what extent do you think human beings are very large language models and very", "tokens": [50992, 2425, 13, 1044, 291, 13, 1407, 437, 8396, 360, 291, 519, 1952, 8958, 366, 588, 2416, 2856, 5245, 293, 588, 51324], "temperature": 0.0, "avg_logprob": -0.24895882606506348, "compression_ratio": 1.2058823529411764, "no_speech_prob": 0.0009731425670906901}, {"id": 957, "seek": 575732, "start": 5757.32, "end": 5768.759999999999, "text": " large movement models? My gut feeling is we're not just large language models. I think there's", "tokens": [50364, 2416, 3963, 5245, 30, 1222, 5228, 2633, 307, 321, 434, 406, 445, 2416, 2856, 5245, 13, 286, 519, 456, 311, 50936], "temperature": 0.0, "avg_logprob": -0.10967136951203042, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.006801164709031582}, {"id": 958, "seek": 575732, "start": 5768.759999999999, "end": 5774.2, "text": " an awful lot more. We're great apes, the result of three and a half billion years of evolution,", "tokens": [50936, 364, 11232, 688, 544, 13, 492, 434, 869, 1882, 279, 11, 264, 1874, 295, 1045, 293, 257, 1922, 5218, 924, 295, 9303, 11, 51208], "temperature": 0.0, "avg_logprob": -0.10967136951203042, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.006801164709031582}, {"id": 959, "seek": 575732, "start": 5774.759999999999, "end": 5780.2, "text": " and we evolved to be able to understand planet Earth, roughly speaking, at ground level where we", "tokens": [51236, 293, 321, 14178, 281, 312, 1075, 281, 1223, 5054, 4755, 11, 9810, 4124, 11, 412, 2727, 1496, 689, 321, 51508], "temperature": 0.0, "avg_logprob": -0.10967136951203042, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.006801164709031582}, {"id": 960, "seek": 575732, "start": 5780.2, "end": 5784.92, "text": " are now, and to understand other great apes, societies of great apes. That's not what large", "tokens": [51508, 366, 586, 11, 293, 281, 1223, 661, 869, 1882, 279, 11, 19329, 295, 869, 1882, 279, 13, 663, 311, 406, 437, 2416, 51744], "temperature": 0.0, "avg_logprob": -0.10967136951203042, "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.006801164709031582}, {"id": 961, "seek": 578492, "start": 5785.0, "end": 5790.12, "text": " language models do. That's fundamentally not what they do. On the other hand, I've had colleagues", "tokens": [50368, 2856, 5245, 360, 13, 663, 311, 17879, 406, 437, 436, 360, 13, 1282, 264, 661, 1011, 11, 286, 600, 632, 7734, 50624], "temperature": 0.0, "avg_logprob": -0.10123968553972675, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.002786873374134302}, {"id": 962, "seek": 578492, "start": 5790.12, "end": 5795.16, "text": " again seriously say, well, maybe we should try and construct a theory of human society, which is", "tokens": [50624, 797, 6638, 584, 11, 731, 11, 1310, 321, 820, 853, 293, 7690, 257, 5261, 295, 1952, 4086, 11, 597, 307, 50876], "temperature": 0.0, "avg_logprob": -0.10123968553972675, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.002786873374134302}, {"id": 963, "seek": 578492, "start": 5795.16, "end": 5801.64, "text": " based on the idea that we are actually just trying to come out with the most plausible thing that", "tokens": [50876, 2361, 322, 264, 1558, 300, 321, 366, 767, 445, 1382, 281, 808, 484, 365, 264, 881, 39925, 551, 300, 51200], "temperature": 0.0, "avg_logprob": -0.10123968553972675, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.002786873374134302}, {"id": 964, "seek": 578492, "start": 5801.64, "end": 5808.2, "text": " comes next. It doesn't seem plausible to me, I have to say. These are just tools. They're just", "tokens": [51200, 1487, 958, 13, 467, 1177, 380, 1643, 39925, 281, 385, 11, 286, 362, 281, 584, 13, 1981, 366, 445, 3873, 13, 814, 434, 445, 51528], "temperature": 0.0, "avg_logprob": -0.10123968553972675, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.002786873374134302}, {"id": 965, "seek": 578492, "start": 5808.2, "end": 5813.8, "text": " tools which are based fundamentally based on language. They're extremely powerful at what", "tokens": [51528, 3873, 597, 366, 2361, 17879, 2361, 322, 2856, 13, 814, 434, 4664, 4005, 412, 437, 51808], "temperature": 0.0, "avg_logprob": -0.10123968553972675, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.002786873374134302}, {"id": 966, "seek": 581380, "start": 5813.8, "end": 5821.4800000000005, "text": " they do, but do they give us any deep insights into human nature or the fundamentals of", "tokens": [50364, 436, 360, 11, 457, 360, 436, 976, 505, 604, 2452, 14310, 666, 1952, 3687, 420, 264, 29505, 295, 50748], "temperature": 0.0, "avg_logprob": -0.12255024698983251, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.0035675724502652884}, {"id": 967, "seek": 581380, "start": 5822.52, "end": 5828.84, "text": " human mental processes? Probably not. Thank you very much, Mike. That is all we have time for,", "tokens": [50800, 1952, 4973, 7555, 30, 9210, 406, 13, 1044, 291, 588, 709, 11, 6602, 13, 663, 307, 439, 321, 362, 565, 337, 11, 51116], "temperature": 0.0, "avg_logprob": -0.12255024698983251, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.0035675724502652884}, {"id": 968, "seek": 581380, "start": 5828.84, "end": 5832.92, "text": " unfortunately, today. This is the end of the Turing lecture series for this year. Please do", "tokens": [51116, 7015, 11, 965, 13, 639, 307, 264, 917, 295, 264, 314, 1345, 7991, 2638, 337, 341, 1064, 13, 2555, 360, 51320], "temperature": 0.0, "avg_logprob": -0.12255024698983251, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.0035675724502652884}, {"id": 969, "seek": 581380, "start": 5832.92, "end": 5837.64, "text": " follow us on social media, the website, our emailing list to find out about future Turing", "tokens": [51320, 1524, 505, 322, 2093, 3021, 11, 264, 3144, 11, 527, 3796, 278, 1329, 281, 915, 484, 466, 2027, 314, 1345, 51556], "temperature": 0.0, "avg_logprob": -0.12255024698983251, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.0035675724502652884}, {"id": 970, "seek": 581380, "start": 5837.64, "end": 5841.8, "text": " events. Of course, we do have the Christmas lecture in 10 days time as I'll back here at", "tokens": [51556, 3931, 13, 2720, 1164, 11, 321, 360, 362, 264, 5272, 7991, 294, 1266, 1708, 565, 382, 286, 603, 646, 510, 412, 51764], "temperature": 0.0, "avg_logprob": -0.12255024698983251, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.0035675724502652884}, {"id": 971, "seek": 584180, "start": 5841.8, "end": 5845.88, "text": " the Royal Institution. Apart from that, just one more massive round of applause, please, for Professor", "tokens": [50364, 264, 12717, 2730, 6518, 13, 24111, 490, 300, 11, 445, 472, 544, 5994, 3098, 295, 9969, 11, 1767, 11, 337, 8419, 50568], "temperature": 0.0, "avg_logprob": -0.30799126625061035, "compression_ratio": 1.120879120879121, "no_speech_prob": 0.0018137510633096099}], "language": "en"}