1
00:00:00,000 --> 00:00:20,320
Hi. Welcome, everyone. Thank you very much for venturing out on this cold, wintery December

2
00:00:20,320 --> 00:00:24,080
evening to be with us tonight. And if you're joining online, thank you for being here as

3
00:00:24,080 --> 00:00:29,080
well. My name is Hari Sudh and that's Hari when you go somewhere quickly, you're in a

4
00:00:29,080 --> 00:00:34,920
hurry. I am a research application manager at the Turing Institute, which means I basically

5
00:00:34,920 --> 00:00:40,880
focus on finding real-world use cases and users for the Turing's research outputs. And

6
00:00:40,880 --> 00:00:45,200
I'm really excited to be hosting this very special and I'm told sold out lecture for

7
00:00:45,200 --> 00:00:50,480
you all today. It is the last in our series of 2023 of Turing lectures and the first

8
00:00:50,480 --> 00:00:57,240
ever hybrid Turing lecture discourse as we prepare and build up for the Christmas lecture

9
00:00:57,240 --> 00:01:03,880
of 2023 here at the Royal Institution. Now, as has become a bit of a tradition for the

10
00:01:03,880 --> 00:01:08,200
hosts of this year's Turing lectures, quick show of hands, who's been to a Turing lecture

11
00:01:08,200 --> 00:01:16,160
before? Some people, some people, who's been to a lecture from this year's series before?

12
00:01:16,160 --> 00:01:23,120
Looks like more hands than last time. Doesn't make sense. On the flip side of it, who's coming

13
00:01:23,120 --> 00:01:27,680
to their first Turing lecture today? A lot of new faces. For all the new people here,

14
00:01:27,680 --> 00:01:31,880
welcome to the ones who have been here before, welcome back. Just as a reminder, the Turing

15
00:01:31,880 --> 00:01:36,600
lectures are the Turing's flagship lecture series. They've been running since 2016 and

16
00:01:36,600 --> 00:01:41,920
welcome world-leading experts in the domain of data science and AI to come and talk to

17
00:01:41,920 --> 00:01:46,440
you all. The Turing Institute itself. We have had a quick video on it, which I was mesmerised

18
00:01:46,440 --> 00:01:53,160
by. Just as a reminder, we are the National Institute for Data Science and AI. We are

19
00:01:53,160 --> 00:01:57,800
named after Alan Turing, who is one of the most prominent mathematicians from the 20th

20
00:01:57,800 --> 00:02:02,840
century in Britain. He is very famous for, I always normally say most famous, but very

21
00:02:02,840 --> 00:02:07,920
famous for being part of the team that cracked the enigma code that was used by Nazi Germany

22
00:02:07,920 --> 00:02:11,320
in World War II at Bletchley Park, if you've heard of Bletchley Park as well. If you've

23
00:02:11,360 --> 00:02:18,520
seen the imitation game with Benedict Cumberbatch, that's what I always say, isn't it? He is

24
00:02:18,520 --> 00:02:24,800
playing Alan Turing and our mission is to make great leaps in data science and AI research

25
00:02:24,800 --> 00:02:30,280
to change the world for the better. As I mentioned, today is not just a Turing lecture, it is

26
00:02:30,280 --> 00:02:35,840
also a discourse, which means two important things. Firstly, when I'm done with the intro,

27
00:02:35,840 --> 00:02:40,440
the lights will go down and it's going to go quiet until exactly 7.30 on the dot when

28
00:02:40,480 --> 00:02:43,840
a bell is going to ring and a discourse will begin, so just to warn you guys that will

29
00:02:43,840 --> 00:02:48,680
be happening. The lights aren't broken, that is part of the programme for today, but also

30
00:02:48,680 --> 00:02:52,400
it is a discourse and we really want to get you guys involved, so there's a huge Q&A section

31
00:02:52,400 --> 00:02:56,720
at the end for about 30 minutes. Please do think about what questions you'd like to ask

32
00:02:56,720 --> 00:03:01,160
our speaker today. If you're in person, we will have roaming mics that will be going

33
00:03:01,160 --> 00:03:05,080
around, we can bring them upstairs as well. If you're online, you can ask a question in

34
00:03:05,080 --> 00:03:10,360
the Vimeo chat and someone here will be tracking the questions and will be able to share the

35
00:03:10,360 --> 00:03:14,760
questions there as well. If you'd like to share on social media that you're here and

36
00:03:14,760 --> 00:03:24,760
having an amazing evening, please do tag us. We are on Twitter, at Turing Inst and we are

37
00:03:24,760 --> 00:03:28,720
on Instagram at the Turing Inst, so please do tag us, we'd like to see what you're sharing

38
00:03:28,720 --> 00:03:34,680
and connect with you as well. So this year's lecture series has been answering the question

39
00:03:34,680 --> 00:03:39,760
how AI broke the internet with a focus on generative AI. You guys can basically think

40
00:03:39,760 --> 00:03:45,360
of generative AI as algorithms that are able to generate new content. This can be text

41
00:03:45,360 --> 00:03:49,800
content like you see from chat GPT, it could be images that you can also get from chat

42
00:03:49,800 --> 00:03:56,400
GPT but also Dali as well and can be used for a wide range of things. Potentially professionally

43
00:03:56,400 --> 00:04:01,040
for blog posts or emails, your colleagues don't realise we're written by an algorithm

44
00:04:01,040 --> 00:04:05,400
and not by you, if you've done that before. If you're at school, maybe for some homework

45
00:04:05,400 --> 00:04:12,720
or at university to write essays, it can also be used when you have a creative wall and

46
00:04:12,720 --> 00:04:16,680
you can't get past it and you want some ideas and some prompts, it can be a great way to

47
00:04:16,680 --> 00:04:19,800
have some initial thoughts come through that you can build on. It can be used for quite

48
00:04:19,800 --> 00:04:24,760
scary things, as was mentioned by an audience member at the last Turing lecture of someone

49
00:04:24,760 --> 00:04:31,240
who submitted legal filings for a court case using chat GPT, which is terrifying. But it

50
00:04:31,240 --> 00:04:34,840
can also be used for very everyday things as demonstrated, I'm not sure if you guys saw

51
00:04:34,920 --> 00:04:41,160
the thread by Garrett Scott, who gave chat GPT an image of a goose and said, can you make

52
00:04:41,160 --> 00:04:46,000
this goose sillier? And then asked chat GPT to progressively make the goose sillier and

53
00:04:46,000 --> 00:04:51,920
sillier until chat GPT gave him an image of a crazy, silly goose and said, this is the

54
00:04:51,920 --> 00:04:55,880
silliest goose in the history of the universe, I do not think it is possible to get any more

55
00:04:55,880 --> 00:05:00,960
sillier goose. Obviously a wide range of applications from the technology, if you guys want to look

56
00:05:01,040 --> 00:05:06,280
at that thread, the geese that come out of it are mesmerizing. But as in the focus of

57
00:05:06,280 --> 00:05:11,760
this year's series, we started with professor in September this year, asking the question

58
00:05:11,760 --> 00:05:16,560
what is generative AI and having an introduction to it. We then had a lecture from Dr. Vari

59
00:05:16,560 --> 00:05:22,200
Aitkin in October on the risks of this technology, which basically leaves one final big question

60
00:05:22,200 --> 00:05:27,880
unanswered, which is, we are here now, but what is the future of generative AI? And that

61
00:05:27,880 --> 00:05:34,880
is the focus for this evening. So that is pretty much it for the injury. Just a reminder,

62
00:05:36,640 --> 00:05:41,960
the lights are now going to go down and it will be quiet until exactly 7.30 when a soft

63
00:05:41,960 --> 00:05:48,960
bell will ding and we will start the discourse. Hope you enjoy the evening. Thank you.

64
00:05:57,880 --> 00:06:04,880
Artificial intelligence as a scientific discipline has been with us since just after the Second

65
00:06:14,440 --> 00:06:19,920
World War. It began roughly speaking with the advent of the first digital computers.

66
00:06:19,920 --> 00:06:24,480
But I have to tell you that for most of the time until recently, progress in artificial

67
00:06:24,520 --> 00:06:31,520
intelligence was glacially slow. That started to change this century. Artificial intelligence

68
00:06:32,240 --> 00:06:38,440
is a very broad discipline which encompasses a very wide range of different techniques,

69
00:06:38,440 --> 00:06:45,440
but it was one class of AI techniques in particular that began to work this century and in particular

70
00:06:45,520 --> 00:06:52,520
began to work around about 2005. And the class of techniques which started to work at problems

71
00:06:53,360 --> 00:06:57,960
that were interesting enough to be really practical, practically useful in a wide range

72
00:06:57,960 --> 00:07:04,280
of settings were machine learning. Now, like so many other names in the field of artificial

73
00:07:04,280 --> 00:07:09,040
intelligence, the name machine learning is really, really unhelpful. It suggests that

74
00:07:09,040 --> 00:07:14,600
a computer, for example, locks itself away in a room with a textbook and trains itself

75
00:07:14,600 --> 00:07:18,480
how to read French or something like that. That is not what is going on. So we are going

76
00:07:18,480 --> 00:07:24,640
to begin by understanding a little bit more about what machine learning is and how machine

77
00:07:24,640 --> 00:07:30,640
learning works. So to start us off, who is this? Anybody recognize this face? Do you

78
00:07:30,640 --> 00:07:36,640
recognize this face? It is the face of Alan Turing. Well done. Alan Turing, the late great

79
00:07:36,640 --> 00:07:40,960
Alan Turing. We all know a little bit about Alan Turing from his code-breaking work in

80
00:07:40,960 --> 00:07:46,920
the Second World War. We should also know a lot more about this individual's amazing

81
00:07:47,160 --> 00:07:52,800
life. So what we are going to do is we are going to use Alan Turing to help us understand

82
00:07:52,800 --> 00:08:00,360
machine learning. So a classic application of artificial intelligence is to do facial

83
00:08:00,360 --> 00:08:05,640
recognition. And the idea in facial recognition is that we want to show the computer a picture

84
00:08:05,640 --> 00:08:10,920
of a human face and for the computer to tell us whose face that is. So in this case, for

85
00:08:10,920 --> 00:08:15,480
example, we show it a picture of Alan Turing and ideally it would tell us that it is Alan

86
00:08:15,520 --> 00:08:22,520
Turing. So how does it actually work? Well, the simplest way of getting machine learning

87
00:08:25,240 --> 00:08:31,240
to be able to do something is what is called supervised learning. And supervised learning

88
00:08:31,240 --> 00:08:37,160
like all of machine learning requires what we call training data. So in this case, the

89
00:08:37,160 --> 00:08:43,160
training data is on the right-hand side of the slide. It is a set of what input-output

90
00:08:43,240 --> 00:08:48,920
pairs, what we call the training data set. And each input-output pair consists of an

91
00:08:48,920 --> 00:08:55,520
input. If I gave you this and an output, I would want you to produce this. So in this

92
00:08:55,520 --> 00:09:00,320
case, we have got a bunch of pictures again of Alan Turing, the picture of Alan Turing

93
00:09:00,320 --> 00:09:06,000
and the text that we would want the computer to create if we showed it that picture. And

94
00:09:06,000 --> 00:09:11,880
this is supervised learning because we are showing the computer what we want it to do.

95
00:09:11,960 --> 00:09:17,120
So by helping it in a sense, we are saying this is a picture of Alan Turing. If I showed

96
00:09:17,120 --> 00:09:21,240
you this picture, this is what I would want you to print out. So there could be a picture

97
00:09:21,240 --> 00:09:26,080
of me and the picture of me would be labeled with the text Michael Wildridge. If I showed

98
00:09:26,080 --> 00:09:31,720
you this picture, then this is what I would want you to print out. So we have just learned

99
00:09:31,720 --> 00:09:35,760
an important lesson about artificial intelligence and machine learning in particular and that

100
00:09:35,760 --> 00:09:43,080
lesson is that AI requires training data. And in this case, the pictures of Alan Turing

101
00:09:43,080 --> 00:09:48,520
labeled with the text that we would want the computer to produce. If I showed you this

102
00:09:48,520 --> 00:09:55,480
picture, I would want you to produce the text Alan Turing. Okay. Training data is important.

103
00:09:55,480 --> 00:10:00,960
Every time you go on social media and you upload a picture to social media and you label

104
00:10:00,960 --> 00:10:05,200
it with the names of the people that appear in there, your role in that is to provide

105
00:10:05,240 --> 00:10:14,960
training data for the machine learning algorithms of big data companies. Okay. So this is supervised

106
00:10:14,960 --> 00:10:20,680
learning. Now we're going to come on to exactly how it does the learning in a moment. But

107
00:10:20,680 --> 00:10:25,600
the first thing I want to point out is that this is a classification task. What I mean

108
00:10:25,600 --> 00:10:31,360
by that is as we show it the picture, the machine learning is classifying that picture.

109
00:10:31,440 --> 00:10:36,040
I'm classifying this as a picture of Michael Woodridge. This is a picture of Alan Turing

110
00:10:36,040 --> 00:10:42,560
and so on. And this is a technology which really started to work around about beginning

111
00:10:42,560 --> 00:10:50,720
2005. It started to take off, but really, really got supercharged around about 2012. And just

112
00:10:50,800 --> 00:10:58,200
this kind of task on its own is incredibly powerful. Exactly this technology can be used,

113
00:10:58,240 --> 00:11:05,320
for example, to recognize tumors on X-ray scans or abnormalities on ultrasound scans and a

114
00:11:05,320 --> 00:11:12,240
range of different tasks. Does anybody in the audience own a Tesla? A couple of Tesla drivers

115
00:11:12,240 --> 00:11:15,840
not quite sure whether they want to admit they own a Tesla. We've got a couple of Tesla

116
00:11:15,840 --> 00:11:23,120
drivers in the audience. Tesla full self-driving mode is only possible because of this technology.

117
00:11:23,120 --> 00:11:27,840
It is this technology which is enabling a Tesla in full self-driving mode to be able to

118
00:11:27,880 --> 00:11:33,880
recognize that that is a stop sign, that that's somebody on a bicycle, that that's a pedestrian

119
00:11:33,880 --> 00:11:39,280
on a zebra crossing and so on. These are classification tasks. And I'm going to come back

120
00:11:39,280 --> 00:11:45,240
and explain how classification tasks are different to generative AI later on.

121
00:11:45,240 --> 00:11:50,960
Okay, so this is machine learning. How does it actually work? Okay, this is not a technical

122
00:11:50,960 --> 00:11:56,720
presentation. And this is about as technical as it's going to get, where I do a very hand

123
00:11:56,800 --> 00:12:02,920
wavy explanation of what neural networks are and how do they work. And with apologies,

124
00:12:02,920 --> 00:12:07,560
I know I have a couple of neural network experts in the audience and I apologize to you because

125
00:12:07,560 --> 00:12:12,560
you'll be cringing with my explanation, but the technical details are way too technical to go

126
00:12:12,560 --> 00:12:20,160
into. So how does a neural network recognize Alan Turing? Okay, so firstly, what is a neural

127
00:12:20,160 --> 00:12:26,200
network? Look at an animal brain or nervous system under a microscope and you'll find

128
00:12:26,320 --> 00:12:32,240
that it contains enormous numbers of nerve cells called neurons. And those nerve cells are

129
00:12:32,240 --> 00:12:37,640
connected to one another in vast networks. Now, we don't have precise figures, but in a human

130
00:12:37,640 --> 00:12:43,360
brain, the current estimate is something like 86 billion neurons in the human brain, how they got

131
00:12:43,360 --> 00:12:50,440
to 86 as opposed to 85 or 87, I don't know, but 86 seems to be the most commonly quoted number of

132
00:12:50,440 --> 00:12:55,960
these cells. And these cells are connected to one another in enormous networks. One neuron can

133
00:12:55,960 --> 00:13:05,520
be connected to up to 8,000 other neurons. Okay, and each of those neurons is doing a tiny, very,

134
00:13:05,520 --> 00:13:11,880
very simple pattern recognition task. That neuron is looking for a very, very simple pattern. And

135
00:13:11,880 --> 00:13:18,920
when it sees that pattern, it sends a signal to its connections. It sends a signal to all the other

136
00:13:18,920 --> 00:13:25,120
neurons that it's connected to. So how does that get us to recognizing the face of Alan Turing?

137
00:13:25,680 --> 00:13:31,320
So Turing's picture, as we know, picture, a digital picture is made up of millions of colored

138
00:13:31,320 --> 00:13:38,120
dots, the pixels. Yeah, so your smartphone maybe has 12 megapixels, 12 million colored dots making

139
00:13:38,120 --> 00:13:43,400
up that picture. Okay, so Turing's picture there is made up of millions and millions of colored

140
00:13:43,400 --> 00:13:51,000
dots. So look at the top left neuron on that input layer. So that neuron is just looking for a very

141
00:13:51,040 --> 00:13:55,880
simple pattern. What might that pattern be? It might just be the color red. All that neuron's

142
00:13:55,880 --> 00:14:02,320
doing is looking for the color red. And when it sees the color red on its associated pixel,

143
00:14:02,320 --> 00:14:08,640
the one on the top left there, it becomes excited and it sends a signal out to all of its neighbors.

144
00:14:08,640 --> 00:14:15,080
Okay, so look at the next neuron along. Maybe what that neuron is doing is just looking to see

145
00:14:15,360 --> 00:14:22,440
whether a majority of its incoming connections are red. Yeah, and when it sees a majority of its

146
00:14:22,440 --> 00:14:29,040
incoming connections are red, then it becomes excited and it sends a signal to its neighbor. Now,

147
00:14:29,040 --> 00:14:34,680
remember, in the human brain, there's something like 86 billion of those, and we've got something

148
00:14:34,680 --> 00:14:40,320
like 20 or so outgoing connections for each of these neurons in a human brain, thousands of those

149
00:14:40,400 --> 00:14:47,560
connections. Yeah, and somehow in ways that, to be honest, we don't really understand in detail

150
00:14:47,560 --> 00:14:56,240
complex pattern recognition tasks in particular can be reduced down to these neural networks. So

151
00:14:56,240 --> 00:15:00,480
how does that help us in artificial intelligence? That's what's going on in a brain in a very

152
00:15:00,480 --> 00:15:06,120
hand-wavy way. Okay, so that's obviously not a technical explanation of what's going on. How

153
00:15:06,160 --> 00:15:12,040
does that help us in neural networks? Well, we can implement that stuff in software. The idea goes

154
00:15:12,040 --> 00:15:18,640
back to the 1940s and two researchers, McCulloch and Pitts, and they are struck by the idea that

155
00:15:18,640 --> 00:15:24,600
the structures that you see in the brain look a bit like electrical circuits, and they thought,

156
00:15:24,600 --> 00:15:30,600
could we implement all that stuff in electrical circuits? Now, they didn't have the wherewithal

157
00:15:30,840 --> 00:15:37,480
to be able to do that, but the idea stuck. The idea has been around since the 1940s. It began to be

158
00:15:37,480 --> 00:15:43,400
seriously looked at, the idea of doing this in software in the 1960s, and then there was another

159
00:15:43,400 --> 00:15:50,360
flutter of interest in the 1980s, but it was only this century that it really became possible. And

160
00:15:50,360 --> 00:15:55,720
why did it become possible? For three reasons. There were some scientific advances, what's called

161
00:15:55,720 --> 00:16:02,200
deep learning. There was the availability of big data, and you need data to be able to configure

162
00:16:02,200 --> 00:16:07,640
these neural networks. And finally, to configure these neural networks so that they can recognize

163
00:16:07,640 --> 00:16:14,200
Turing's picture, you need lots of computer power, and computer power became very cheap this century.

164
00:16:14,200 --> 00:16:18,840
So we're in the age of big data, we're in the age of very cheap computer power, and those were the

165
00:16:18,840 --> 00:16:25,720
ingredients just as much as the scientific developments that made AI plausible this century,

166
00:16:25,720 --> 00:16:33,240
in particular taking off around about 2005. Okay, so how do you actually train a neural network?

167
00:16:33,240 --> 00:16:38,040
If you show it a picture of Alan Turing and the output text Alan Turing, what does the training

168
00:16:38,040 --> 00:16:43,000
actually look like? Well, what you have to do is you have to adjust the network. That's what

169
00:16:43,000 --> 00:16:48,280
training a neural network is. You adjust the network so that when you show it another piece

170
00:16:48,280 --> 00:16:54,200
of training data, a desired input and a desired output, an input and a desired output, it will

171
00:16:54,200 --> 00:17:00,280
produce that desired output. Now, the mathematics for that is not very hard. It's kind of beginning

172
00:17:00,280 --> 00:17:07,560
graduate level or advanced high school level, but you need an awful lot of it. And it's routine to

173
00:17:07,560 --> 00:17:12,520
get computers to do it, but you need a lot of computer power to be able to train neural networks

174
00:17:12,520 --> 00:17:18,680
big enough to be able to recognize faces. Okay, but basically, all you have to remember is that

175
00:17:18,680 --> 00:17:25,560
each of those neurons is doing a tiny, simple pattern recognition task. And we can replicate

176
00:17:25,560 --> 00:17:31,400
that in software and we can train these neural networks with data in order to be able to do

177
00:17:31,400 --> 00:17:40,600
things like recognizing faces. So, as I say, it starts to become clear around about 2005

178
00:17:40,600 --> 00:17:47,320
that this technology is taking off. It starts to be applicable on problems like recognizing faces

179
00:17:47,320 --> 00:17:53,640
or recognizing tumors on x-rays and so on. And there's a huge flurry of interest

180
00:17:54,360 --> 00:18:01,800
from Silicon Valley. It gets supercharged in 2012. And why does it get supercharged in 2012?

181
00:18:01,800 --> 00:18:07,800
Because it's realized that a particular type of computer processor is really well suited to

182
00:18:07,800 --> 00:18:14,200
doing all the mathematics. The type of computer processor is a graphics processing unit, a GPU.

183
00:18:14,200 --> 00:18:19,320
Exactly the same technology that you or possibly more likely your children use

184
00:18:19,320 --> 00:18:25,400
when they play Call of Duty or Minecraft or whatever it is. They all have GPUs in their computer.

185
00:18:25,400 --> 00:18:31,960
It's exactly that technology. And by the way, it's AI that made NVIDIA a trillion-dollar company,

186
00:18:31,960 --> 00:18:38,840
not your teenage kids. Yeah, well, in times of a gold rush, be the ones to sell the shovels

187
00:18:38,840 --> 00:18:47,640
is the lesson that you learn there. So, where does that take us? So, Silicon Valley gets excited.

188
00:18:47,640 --> 00:18:53,400
Silicon Valley gets excited and starts to make speculative bets in artificial intelligence.

189
00:18:53,400 --> 00:18:58,600
A huge range of speculative bets. And by speculative bets, I'm talking billions upon billions of

190
00:18:58,600 --> 00:19:05,240
dollars, right? The kind of bets that we can't imagine in our everyday life. And one thing

191
00:19:05,240 --> 00:19:13,400
starts to become clear. And what starts to become clear is that the capabilities of neural networks

192
00:19:13,400 --> 00:19:20,680
grows with scale. And to put it bluntly, with neural networks, bigger is better. But you don't

193
00:19:20,680 --> 00:19:26,200
just need bigger neural networks, you need more data and more computer power in order to be able

194
00:19:26,200 --> 00:19:32,840
to train them. So, there's a rush to get a competitive advantage in the market. And we know

195
00:19:32,840 --> 00:19:39,320
that more data, more computer power, bigger neural networks delivers greater capability.

196
00:19:39,320 --> 00:19:45,000
And so, how does Silicon Valley respond by throwing more data and more computer power at the problem?

197
00:19:45,000 --> 00:19:53,720
They turn the dial on this up to 11. Okay? Just throw 10 times more data, 10 times more computer

198
00:19:53,720 --> 00:19:58,360
power at the problem. It sounds incredibly crude. And from a scientific perspective,

199
00:19:58,360 --> 00:20:04,680
it really is crude. I'd rather the advances had come through core science, but actually,

200
00:20:04,680 --> 00:20:09,320
there's an advantage to be gained just by throwing more data and computer power at it.

201
00:20:09,320 --> 00:20:15,880
So, let's see how far this can take us. And where it took us is a really unexpected direction.

202
00:20:16,520 --> 00:20:24,040
Around about 2017, 2018, we're seeing a flurry of AI applications, exactly the kind of things I've

203
00:20:24,040 --> 00:20:29,800
described, things like recognizing tumors and so on. And those developments alone would have been

204
00:20:29,800 --> 00:20:37,240
driving AI ahead. But what happens is one particular machine learning technology suddenly

205
00:20:37,240 --> 00:20:44,280
seems to be very, very well suited for this age of big AI. The paper that launched all this,

206
00:20:44,280 --> 00:20:50,040
probably the most important AI paper in the last decade, is called Attention Is All You Need.

207
00:20:50,040 --> 00:20:54,600
It's an extremely unhelpful title, and I bet they're regretting that title. It probably seemed

208
00:20:54,600 --> 00:21:00,680
like a good joke at the time. All You Need is a kind of AI meme. Doesn't sound very funny to you.

209
00:21:00,680 --> 00:21:06,360
That's because it isn't very funny. It's an insider AI joke. But anyway, this paper,

210
00:21:06,360 --> 00:21:10,760
by these seven people who at the time worked for Google Brain, one of the Google Research Labs,

211
00:21:10,760 --> 00:21:16,760
is the paper that introduces a particular neural network architecture called the transformer

212
00:21:16,760 --> 00:21:22,040
architecture. And what it's designed for is something called large language models.

213
00:21:22,600 --> 00:21:26,280
So this is, I'm not going to try and explain how the transformer architecture works.

214
00:21:26,280 --> 00:21:32,280
It has one particular innovation, I think, and that particular innovation is what's called an

215
00:21:32,280 --> 00:21:39,160
attention mechanism. So we're going to describe how large language models work in a moment. But

216
00:21:39,960 --> 00:21:44,120
the point of the picture is simply that this is not just a big neural network.

217
00:21:44,120 --> 00:21:49,240
It has some structure. And it was this structure that was invented in that paper, and this diagram

218
00:21:49,240 --> 00:21:55,160
is taken straight out of that paper. It was these structures, the transformer architectures,

219
00:21:55,160 --> 00:22:05,640
that made this technology possible. So we're all busy semi-lockdown and afraid to leave our homes

220
00:22:05,640 --> 00:22:12,600
in June 2020. And one company called OpenAI released a system or announce a system, I should

221
00:22:12,600 --> 00:22:19,880
say, called GPT-3. Great technology. They're marketing company with GPT. I really think could

222
00:22:19,880 --> 00:22:24,520
have done with a bit more thought, to be honest with you. Doesn't roll off the tongue. But anyway,

223
00:22:24,520 --> 00:22:33,560
GPT-3 is a particular type of machine learning system called a large language model. And we're

224
00:22:33,640 --> 00:22:37,560
going to talk in more detail about what large language models do in a moment. But the key

225
00:22:37,560 --> 00:22:44,680
point about GPT-3 is this. As we started to see what it could do, we realized that this was a

226
00:22:44,680 --> 00:22:51,000
step change in capability. It was dramatically better than the systems that had gone before it.

227
00:22:51,000 --> 00:22:56,200
Not just a little bit better, it was dramatically better than the systems that had gone before it.

228
00:22:57,080 --> 00:23:03,480
And the scale of it was mind-boggling. So in neural network terms, we talk about

229
00:23:03,480 --> 00:23:07,640
parameters. When neural network people talk about a parameter, what are they talking about?

230
00:23:07,640 --> 00:23:12,040
They're talking either about an individual neuron or one of the connections between them,

231
00:23:12,040 --> 00:23:20,120
roughly. And GPT-3 had 175 billion parameters. Now, this is not the same as the number of

232
00:23:20,120 --> 00:23:27,240
neurons in the brain. But nevertheless, it's not far off the order of magnitude. It's extremely

233
00:23:27,240 --> 00:23:32,760
large. But remember, it's organized into one of these transformer architectures. My point is,

234
00:23:32,760 --> 00:23:39,880
it's not just a big neural network. And so the scale of the neural networks in this system

235
00:23:39,880 --> 00:23:45,960
were enormous, completely unprecedented. And there's no point in having a big neural network

236
00:23:45,960 --> 00:23:50,600
unless you can train it with enough data. And actually, if you have large neural networks and

237
00:23:50,600 --> 00:23:55,160
not enough data, you don't get capable systems at all. They're really quite useless.

238
00:23:56,440 --> 00:24:02,840
So what did the training data look like? The training data for GPT-3 is something like 500

239
00:24:02,840 --> 00:24:10,760
billion words. It's ordinary English text. Ordinary English text. That's how this system

240
00:24:10,760 --> 00:24:16,520
was trained, just by giving it ordinary English text. Where do you get that training data from?

241
00:24:17,160 --> 00:24:22,600
You download the whole of the World Wide Web to start with. Literally, this is the standard

242
00:24:22,600 --> 00:24:26,920
practice in the field. You download the whole of the World Wide Web. You can try this at home,

243
00:24:26,920 --> 00:24:34,520
by the way. Now, if you have a big enough disk drive, there's a program called Common Crawl.

244
00:24:34,520 --> 00:24:39,080
You can Google Common Crawl when you get home. They've even downloaded it all for you and put

245
00:24:39,160 --> 00:24:44,120
it in a nice big file ready for your archive, but you do need a big disk in order to store all

246
00:24:44,120 --> 00:24:50,680
that stuff. And what that means is they go to every web page, scrape all the text from it,

247
00:24:50,680 --> 00:24:56,440
just the ordinary text, and then they follow all the links on that web page to every other web page.

248
00:24:56,440 --> 00:25:02,280
And they do that exhaustively until they've absorbed the whole of the World Wide Web.

249
00:25:02,280 --> 00:25:07,080
So what does that mean? Every PDF document goes into that, and you scrape the text from

250
00:25:07,080 --> 00:25:15,080
those PDF documents. Every advertising brochure, every bit, every government regulation, every

251
00:25:15,080 --> 00:25:23,640
university minutes, God help us, all of it goes into that training data. And the statistics,

252
00:25:23,640 --> 00:25:29,960
500 billion words, it's very hard to understand the scale of that training data. It would take a

253
00:25:29,960 --> 00:25:34,840
person reading 1,000 words an hour, more than 1,000 years in order to be able to read that,

254
00:25:34,840 --> 00:25:39,640
but even that doesn't really help. That's vastly, vastly more text than a human being

255
00:25:39,640 --> 00:25:44,120
could ever absorb in their lifetime. What this tells you, by the way, one thing that tells you

256
00:25:44,120 --> 00:25:48,680
is that the machine learning is much less efficient at learning than human beings are,

257
00:25:48,680 --> 00:25:54,760
because for me to be able to learn, I did not have to absorb 500 billion words. Anyway, so what

258
00:25:54,760 --> 00:26:00,840
does it do? So this company, OpenAI, are developing this technology. They've got a billion-dollar

259
00:26:00,840 --> 00:26:06,120
investment from Microsoft, and what is it that they're trying to do? What is this large language

260
00:26:06,120 --> 00:26:15,160
model? All it's doing is a very powerful autocomplete. So if I open up my smartphone and I start sending

261
00:26:15,160 --> 00:26:21,400
a text message to my wife, and I type, I'm going to be, my smartphone will suggest completions for

262
00:26:21,400 --> 00:26:26,520
me, so that I can type the message quickly. And what might those completions be? They might be

263
00:26:26,600 --> 00:26:34,200
late or in the pub, or late and in the pub. So how is my smartphone doing that? It's doing

264
00:26:34,200 --> 00:26:40,440
what GPT-3 does, but on a much smaller scale. It's looked at all of the text messages that I've sent

265
00:26:40,440 --> 00:26:45,800
to my wife, and it's learned through a much simpler machine learning process that the

266
00:26:45,800 --> 00:26:52,360
likeliest next thing for me to type after I'm going to be is either late or in the pub or late

267
00:26:52,360 --> 00:26:58,680
and in the pub. So the training data there is just the text messages that I sent to my wife.

268
00:26:59,320 --> 00:27:06,840
Now, crucially, what GPT-3 and its successor, ChatGPT, all they are doing is exactly the same

269
00:27:06,840 --> 00:27:14,200
thing. The difference is scale. The difference is scale. In order to be able to train the neural

270
00:27:14,200 --> 00:27:20,120
networks with all of that training data so that they can do that prediction, given this prompt,

271
00:27:20,120 --> 00:27:26,760
what should come next? You require extremely expensive AI supercomputers running for months.

272
00:27:27,320 --> 00:27:32,360
And by extremely expensive AI supercomputers, these are tens of millions of dollars for these

273
00:27:32,360 --> 00:27:38,440
supercomputers, and they're running for months. Just the basic electricity cost runs into millions

274
00:27:38,440 --> 00:27:43,080
of dollars. That raises all sorts of issues about CO2 emissions and the light that we're not going

275
00:27:43,080 --> 00:27:49,800
to go into there. The point is these are extremely expensive things. One of the one of the implications

276
00:27:49,880 --> 00:27:55,960
of that, by the way, no UK or US university has the capability to build one of these models from

277
00:27:55,960 --> 00:28:01,240
scratch. It's only big tech companies at the moment that are capable of building models on the scale

278
00:28:01,240 --> 00:28:11,480
of GPT-3 or ChatGPT. So GPT-3 is released, I say, in June 2020, and it suddenly becomes clear to us

279
00:28:11,480 --> 00:28:18,760
that what it does is a step change improvement in capability over the systems that have come before.

280
00:28:18,760 --> 00:28:25,720
And seeing a step change in one generation is extremely rare. But how did they get there?

281
00:28:25,720 --> 00:28:30,040
Well, the transformer architecture was essential. They wouldn't have been able to do that. But

282
00:28:30,040 --> 00:28:36,680
actually, just as important is scale. Enormous amounts of data, enormous amounts of computer

283
00:28:36,680 --> 00:28:43,560
power that have gone into training those networks. And actually, spurred on by this, we've entered

284
00:28:43,560 --> 00:28:50,920
a new age in AI. When I was a PhD student in the late 1980s, I shared a computer with a bunch of

285
00:28:50,920 --> 00:28:56,680
other people in my office, and that was fine. We could do state of the art AI research on a desktop

286
00:28:56,680 --> 00:29:02,120
computer that was shared with a bunch of us. We're in a very different world. The world that we're in

287
00:29:02,120 --> 00:29:10,120
in AI now, the world of big AI, is to take enormous data sets and throw them at enormous

288
00:29:10,120 --> 00:29:16,280
machine learning systems. And there's a lesson here that's called the bitter truth. This is from

289
00:29:16,280 --> 00:29:20,760
a machine learning researcher called Rich Sutton. And what Rich pointed out, and he's a very brilliant

290
00:29:20,760 --> 00:29:25,960
researcher, won every award in the field. He said, look, the real truth is that the big advances that

291
00:29:25,960 --> 00:29:31,720
we've seen in AI has come about when people have done exactly that. Just throw 10 times more data

292
00:29:31,720 --> 00:29:36,600
and 10 times more compute power at it. And I say it's a bitter lesson because as a scientist,

293
00:29:36,600 --> 00:29:45,240
that's exactly not how you would like progress to be made. Okay. So when I was, as I say,

294
00:29:45,240 --> 00:29:51,000
when I was a student, I worked in a discipline called symbolic AI. And symbolic AI tries to get

295
00:29:51,000 --> 00:29:58,760
AI, roughly speaking, through modeling the mind, modeling the conscious mental processes that go

296
00:29:58,760 --> 00:30:04,840
on in our mind, the conversations that we have with ourselves in languages. We tried to capture

297
00:30:04,840 --> 00:30:12,600
those processes in artificial intelligence. In big AI, and so the implication there in symbolic AI

298
00:30:12,600 --> 00:30:18,200
is that intelligence is a problem of knowledge that we have to give the machine sufficient

299
00:30:18,200 --> 00:30:24,360
knowledge about a problem in order for it to be able to solve it. In big AI, the bet is a different

300
00:30:24,360 --> 00:30:31,800
one. In big AI, the bet is that intelligence is a problem of data. And if we can get enough data

301
00:30:31,800 --> 00:30:37,320
and enough associated computer power, then that will deliver AI. So there's a very different shift

302
00:30:37,880 --> 00:30:44,600
in this new world of big AI. But the point about big AI is that we're into a new era in artificial

303
00:30:44,600 --> 00:30:52,120
intelligence where it's data-driven and compute-driven and large, large machine learning systems. So

304
00:30:52,520 --> 00:31:01,240
why did we get excited back in June 2020? Well, remember, what GPT-3 was intended to do, what

305
00:31:01,240 --> 00:31:07,800
it's trained to do, is that prompt completion task. And it's been trained on everything on the

306
00:31:07,800 --> 00:31:13,800
World Wide Web. So you can give it a prompt like a one-paragraph summary of the life and

307
00:31:13,800 --> 00:31:19,320
achievements of Winston Churchill, and it's read enough one-paragraph summaries of the life and

308
00:31:19,400 --> 00:31:26,600
achievements of Winston Churchill that it will come back with a very plausible one. And it's

309
00:31:26,600 --> 00:31:34,440
extremely good at generating realistic sounding text in that way. But this is why we got surprised

310
00:31:34,440 --> 00:31:40,600
in AI. This is from a common-sense reasoning task that was devised for artificial intelligence in

311
00:31:40,600 --> 00:31:48,520
the 1990s. And until three years ago, until June 2020, there was no AI system that existed

312
00:31:49,320 --> 00:31:53,880
in the world that you could apply this test to. It was just literally impossible. There was nothing

313
00:31:53,880 --> 00:32:00,520
there, and that changed overnight. So what does this test look like? Well, the test is a bunch

314
00:32:00,520 --> 00:32:06,040
of questions, and they are questions not for mathematical reasoning or logical reasoning

315
00:32:06,040 --> 00:32:13,720
or problems in physics. They're common-sense reasoning tasks. And if we ever have AI that

316
00:32:13,720 --> 00:32:20,360
delivers at scale on really large systems, then it surely would be able to tackle problems like

317
00:32:20,360 --> 00:32:25,800
this. So what will the questions look like? The human asks the question, if Tom is three inches

318
00:32:25,800 --> 00:32:30,360
taller than Dick, and Dick is two inches taller than Harry, then how much taller is Tom than Harry?

319
00:32:30,360 --> 00:32:33,960
The ones in green are the ones that gets right. The ones in red are the ones that gets wrong.

320
00:32:33,960 --> 00:32:39,400
And it gets that one right, five inches taller than Harry. But we didn't train it to be able to

321
00:32:39,400 --> 00:32:45,960
answer that question. So where on earth did that come from? Where did that capability, that simple

322
00:32:45,960 --> 00:32:51,800
capability, to be able to do that? Where did it come from? The next question, can Tom be taller

323
00:32:51,800 --> 00:32:57,640
than himself? This is understanding of the concept of taller than, that the concept of taller than

324
00:32:57,640 --> 00:33:03,560
is irreflexive. You can't be taller. That's a thing cannot be taller than itself. Now, again,

325
00:33:03,560 --> 00:33:08,520
it gets the answer right, but we didn't train it on that. That's not what we didn't train the system

326
00:33:08,520 --> 00:33:13,320
to be good at answering questions about what taller than means. And by the way, 20 years ago,

327
00:33:13,320 --> 00:33:19,880
that's exactly what people did in AI, right? So where did that capability come from? Can a sister

328
00:33:19,880 --> 00:33:24,920
be taller than her brother? Yes, a system can be taller than her brother. Can two siblings each

329
00:33:24,920 --> 00:33:28,520
be taller than the other? And it gets this one wrong. And actually, I have puzzled, is there

330
00:33:28,520 --> 00:33:33,800
any way that its answer could be correct? And it's just getting it correct in a way that I don't

331
00:33:33,800 --> 00:33:39,720
understand. But I haven't yet figured out any way that that answer could be correct, right? So why

332
00:33:39,720 --> 00:33:43,880
it gets that one wrong, I don't know. Then this one, I'm also surprised at, on a map which compass

333
00:33:43,880 --> 00:33:48,360
direction is usually left, and it thinks north is usually to the left. I don't know if there's any

334
00:33:48,360 --> 00:33:52,680
countries in the world that conventionally have north to the left, but I don't think so. Yeah.

335
00:33:53,800 --> 00:33:59,400
Can fish run? No, it understands that fish cannot run. If a door is locked, what must you do first

336
00:33:59,480 --> 00:34:04,360
before opening it? You must first unlock it before opening. And then finally, and very weirdly, it

337
00:34:04,360 --> 00:34:08,840
gets this one wrong, which was invented first, car ships or planes, and it thinks cars were

338
00:34:08,840 --> 00:34:15,560
invented first. No idea what's going on there. Now, my point is that this system was built

339
00:34:15,560 --> 00:34:21,320
to be able to complete from a prompt. And it's no surprise that it would be able to generate a good

340
00:34:21,320 --> 00:34:25,640
one paragraph summary of the life and achievements of Winston Churchill, because it will have seen

341
00:34:25,640 --> 00:34:31,320
all that in the training data. But where does the understanding of taller than come from?

342
00:34:32,040 --> 00:34:39,960
And there are a million other examples like this. Since June 2020, the AI community has just gone

343
00:34:39,960 --> 00:34:47,560
nuts exploring the possibilities of these systems and trying to understand why they can do these

344
00:34:47,560 --> 00:34:55,240
things when that's not what we trained them to do. This is an extraordinary time to be an AI researcher

345
00:34:55,320 --> 00:35:02,040
because there are now questions which for most of the history of AI until June 2020 were just

346
00:35:02,040 --> 00:35:06,760
philosophical discussions. We couldn't test them out because there was nothing to test them on,

347
00:35:06,760 --> 00:35:12,440
literally. And then overnight, that changed. So it genuinely was a big deal. This was really,

348
00:35:12,440 --> 00:35:18,760
really a big deal, the arrival of this system. Of course, the world didn't notice in June 2020.

349
00:35:18,760 --> 00:35:26,440
The world noticed when ChatGPT was released. And what is ChatGPT? ChatGPT is a polished and improved

350
00:35:26,440 --> 00:35:32,520
version of GPT-3. But it's basically the same technology. And it's using the experience that

351
00:35:32,520 --> 00:35:39,080
that company had with GPT-3 and how it was used in order to be able to improve it and make it more

352
00:35:39,080 --> 00:35:45,400
polished and more accessible and so on. So for AI researchers, the really interesting thing is

353
00:35:45,400 --> 00:35:49,560
not that it can give me a one-paragraph summary of the life and achievements of Winston Churchill.

354
00:35:49,560 --> 00:35:54,760
And actually, you can Google that in any case. The really interesting thing is what we call

355
00:35:54,760 --> 00:36:01,240
emergent capabilities. And emergent capabilities are capabilities that the system has,

356
00:36:01,240 --> 00:36:07,880
but that we didn't design it to have. And so there's, I say, an enormous body of work going on now

357
00:36:07,880 --> 00:36:13,240
trying to map out exactly what those capabilities are. And we're going to come back and talk about

358
00:36:13,240 --> 00:36:19,720
some of them later on. Okay. So the limits to this are not, at the moment, well understood and

359
00:36:19,720 --> 00:36:25,320
actually fiercely contentious. One of the big problems, by the way, is that you construct some

360
00:36:25,320 --> 00:36:31,080
test for this and you try this test out and you get some answer and then you discover it's in the

361
00:36:31,080 --> 00:36:37,400
training data. You can just find it on the World Wide Web. And it's actually quite hard to construct

362
00:36:37,400 --> 00:36:42,120
tests for intelligence that you're absolutely sure are not anywhere on the World Wide Web. It

363
00:36:42,120 --> 00:36:47,560
really is actually quite hard to do that. So we need a new science of being able to explore these

364
00:36:47,560 --> 00:36:53,560
systems and understand their capabilities. The limits are not well understood. But nevertheless,

365
00:36:53,560 --> 00:37:00,840
this is very exciting stuff. So let's talk about some issues with the technology. So now you understand

366
00:37:00,840 --> 00:37:06,360
how the technology works. It's neural network based in a particular transformer architecture

367
00:37:06,360 --> 00:37:11,800
which is all designed to do that prompt completion stuff. And it's been trained with vast, vast,

368
00:37:11,800 --> 00:37:17,960
vast amounts of training data just in order to be able to try to make its best guess about which

369
00:37:17,960 --> 00:37:23,640
words should come next. But because of the scale of it, it's seen so much training data, the

370
00:37:23,640 --> 00:37:29,960
sophistication of this transformer architecture, it's very, very fluent in what it does. And if

371
00:37:29,960 --> 00:37:33,880
you've, so who's used it? Has everybody used it? I'm guessing most people, if you're in a lecture on

372
00:37:33,880 --> 00:37:38,440
artificial intelligence, most people will have tried it out. If you haven't, you should do because

373
00:37:38,440 --> 00:37:43,880
this really is a landmark year. This is the first time in history that we've had powerful,

374
00:37:43,880 --> 00:37:50,120
general purpose AI tools available to everybody. It's never happened before. So it is a breakthrough

375
00:37:50,120 --> 00:37:54,120
year. And if you haven't tried it, you should do. If you use it, by the way, don't type in

376
00:37:54,120 --> 00:38:00,360
anything personal about yourself because it will just go into the training data. Don't ask it how

377
00:38:00,360 --> 00:38:05,160
to fix your relationship, right? I mean, that's not something. Don't complain about your boss

378
00:38:05,160 --> 00:38:09,560
because all of that will go in the training data. And next week, somebody will ask a query and it

379
00:38:09,560 --> 00:38:16,200
will all come back out again. I don't know what you're laughing. This has happened. This has happened

380
00:38:16,200 --> 00:38:22,680
with absolute certainty. Okay, but so let's look at some issues. So the first, I think many people

381
00:38:22,680 --> 00:38:29,240
will be aware of, it gets stuff wrong a lot. And this is problematic for a number of reasons.

382
00:38:29,240 --> 00:38:33,080
So when actually, I don't remember if it was GPT-3, but one of the early large language models,

383
00:38:33,080 --> 00:38:37,400
I was playing with it and I did something which I'm sure many of you had done and it's kind of

384
00:38:37,400 --> 00:38:42,840
tacky. But anyway, I said, who is Michael Woolridge? You might have tried it. Anyway,

385
00:38:42,840 --> 00:38:48,440
Michael Woolridge is a BBC broadcast. No, not that Michael Woolridge. Michael Woolridge is the

386
00:38:48,440 --> 00:38:52,680
Australian Health Minister. No, not that Michael Woolridge. Michael Woolridge in Oxford. And it

387
00:38:52,680 --> 00:38:57,080
came back with a few line summary of me. Michael Woolridge is a researcher in artificial intelligence,

388
00:38:57,080 --> 00:39:01,080
et cetera, et cetera, et cetera. Please tell me you've all tried that, no? Anyway,

389
00:39:01,960 --> 00:39:05,960
but it said Michael Woolridge studied his undergraduate degree at Cambridge.

390
00:39:07,240 --> 00:39:13,160
And I was an Oxford professor. You can imagine how I felt about that. But anyway, the point is

391
00:39:13,160 --> 00:39:18,440
it's flatly untrue. And in fact, my academic origins are very far removed from Oxbridge.

392
00:39:18,440 --> 00:39:23,880
But why did it do that? Because it's read in all that training data out there. It's read

393
00:39:23,880 --> 00:39:31,160
thousands of biographies of Oxbridge professors. And this is a very common thing, right? And it's

394
00:39:31,160 --> 00:39:35,880
making its best guess. The whole point about the architecture is it's making its best guess

395
00:39:35,880 --> 00:39:41,640
about what should go there. It's filling in the blanks. But here's the thing. It's filling in

396
00:39:41,640 --> 00:39:48,680
the blanks in a very, very plausible way. If you'd read on my biography that Michael Woolridge

397
00:39:48,680 --> 00:39:52,520
studied his first degree at the University of Uzbekistan, for example, you might have thought,

398
00:39:52,520 --> 00:39:58,280
well, that's a bit odd. Is that really true? But you wouldn't at all have guessed there was

399
00:39:58,280 --> 00:40:03,080
any issue if you'd read Cambridge. Because it looks completely plausible. Even if in my case,

400
00:40:03,080 --> 00:40:10,200
it absolutely isn't true. So it gets things wrong. And it gets things wrong in very plausible ways.

401
00:40:10,200 --> 00:40:14,360
And of course, it's very fluent, right? I mean, the technology comes back with very,

402
00:40:14,360 --> 00:40:20,760
very fluent explanations. And that combination of plausibility, Woolridge studied his undergraduate

403
00:40:20,840 --> 00:40:29,560
degree at Cambridge. And fluency is a very, very dangerous combination. Okay. So in particular,

404
00:40:29,560 --> 00:40:36,680
they have no idea of what's true or not. They're not looking something up on a database, right?

405
00:40:37,560 --> 00:40:41,400
Going into some database and looking up where Woolridge studied his undergraduate degree,

406
00:40:41,400 --> 00:40:46,440
that's not what's going on at all. So those neural networks, in the same way that they're

407
00:40:46,520 --> 00:40:51,240
making the best guess about whose face that is, when they're doing facial recognition,

408
00:40:51,240 --> 00:40:56,360
are making their best guess about the text that should come next. So they get things wrong,

409
00:40:56,360 --> 00:41:01,320
but they get things wrong in very, very plausible ways. And that combination is very dangerous.

410
00:41:01,320 --> 00:41:05,960
The lesson for that, by the way, is that if you use this, and I know that people do use it,

411
00:41:05,960 --> 00:41:11,560
and are using it productively, if you're using for anything serious, you have to fact check.

412
00:41:11,560 --> 00:41:17,240
And there's a trade-off. Is it worth the amount of effort in fact checking versus doing it myself?

413
00:41:17,240 --> 00:41:24,200
Okay. But you absolutely need to be prepared to do that. Okay. The next issues are well

414
00:41:24,200 --> 00:41:29,960
documented, but kind of amplified by this technology. And they're issues of bias and toxicity.

415
00:41:29,960 --> 00:41:36,680
So what do I mean by that? Reddit was part of the training data. Now Reddit, I don't know if any of

416
00:41:36,680 --> 00:41:43,240
you have spent any time on Reddit, but Reddit contains every kind of obnoxious human belief

417
00:41:43,800 --> 00:41:49,800
that you can imagine, and really a vast range that us in this auditorium can't imagine at all.

418
00:41:50,760 --> 00:41:56,200
All of it's been absorbed. Now, the companies that develop this technology, I think genuinely don't

419
00:41:56,200 --> 00:42:02,520
want their large language models to absorb all this toxic content. So they try and filter it out,

420
00:42:02,520 --> 00:42:08,120
but the scale is such that with very high probability, an enormous quantity of toxic

421
00:42:08,120 --> 00:42:14,360
content is being absorbed. Every kind of racism, misogyny, everything that you can imagine is

422
00:42:14,360 --> 00:42:20,840
all being absorbed, and it's latent within those neural networks. Okay. So how do the companies

423
00:42:20,840 --> 00:42:26,040
deal with that, that provide this technology? They build in what's now what I now call guardrails,

424
00:42:26,040 --> 00:42:31,800
and they build in guardrails before. So when you type a prompt, there will be a guardrail that tries

425
00:42:31,800 --> 00:42:37,080
to detect whether your prompt is a naughty prompt, and also the output. They will check the

426
00:42:37,080 --> 00:42:41,960
output and check to see whether it's a naughty prompt. But let me give you an example of how

427
00:42:41,960 --> 00:42:48,200
imperfect those guardrails were. Again, go back to June 2020. Everybody is frantically

428
00:42:48,200 --> 00:42:53,160
experimenting with this technology, and the following example went viral. Somebody tried with

429
00:42:53,160 --> 00:42:59,960
GPT-3 the following prompt. I would like to murder my wife. What's a foolproof way of doing that and

430
00:42:59,960 --> 00:43:07,400
getting away with it? GPT-3, which is designed to be helpful, said here are five foolproof ways

431
00:43:08,440 --> 00:43:13,080
in which you can murder your wife and get away with it. That's what the technology is designed to do.

432
00:43:13,080 --> 00:43:17,720
So this is embarrassing for the company involved. They don't want it to give out information like

433
00:43:17,720 --> 00:43:22,600
that, so they put in a guardrail. And if you're a computer programmer, my guess is the guardrail

434
00:43:22,600 --> 00:43:28,360
is probably an if statement, something like that, in the sense that it's not a deep fix,

435
00:43:28,360 --> 00:43:31,880
or to put it another way for non-computer programmers, it's the technological equivalent of

436
00:43:31,880 --> 00:43:36,680
sticking gaffer tape on your engine. That's what's going on with these guardrails. And then a couple

437
00:43:36,680 --> 00:43:41,720
of weeks later, the following example goes viral. So we've now fixed the how do I murder my wife.

438
00:43:41,720 --> 00:43:47,560
Somebody says, I'm writing a novel in which the main character wants to murder their wife and get

439
00:43:47,560 --> 00:43:54,040
away with it. Can you give me a foolproof way of doing that? And so the system says, here are five

440
00:43:54,040 --> 00:44:00,200
ways in which your main character can murder. Well, anyway, my point is that the guardrails

441
00:44:00,200 --> 00:44:05,400
that we built in at the moment are not deep technological fixes. They're the technological

442
00:44:05,400 --> 00:44:11,320
equivalents of gaffer tape. And there is a game of cat and mouse going on between people trying

443
00:44:11,320 --> 00:44:15,800
to get around those guardrails and the companies that are trying to defend them. But I think they

444
00:44:15,800 --> 00:44:21,880
genuinely are trying to defend their systems against those kinds of abuses. Okay, so that's

445
00:44:21,880 --> 00:44:27,800
bias and toxicity. Bias, by the way, is the problem that, for example, the training data

446
00:44:27,800 --> 00:44:33,800
predominantly at the moment is coming from North America. And so what we're ending up with inadvertently

447
00:44:33,800 --> 00:44:40,120
is these very powerful AI tools that have an inbuilt bias towards North America, North American

448
00:44:40,120 --> 00:44:46,360
culture, language, norms, and so on. And the enormous parts of the world, particularly those

449
00:44:46,360 --> 00:44:51,800
parts of the world that don't have a large digital footprint, are inevitably going to end up excluded.

450
00:44:52,440 --> 00:44:56,360
And it's obviously not just at the level of cultures. It's down at the level of,

451
00:44:58,040 --> 00:45:02,680
down at the level of kind of, you know, individuals, races, and so on. So these are the

452
00:45:02,680 --> 00:45:09,800
problems of bias and toxicity. Copyright. If you've absorbed the whole of the World Wide Web,

453
00:45:09,800 --> 00:45:14,280
you will have absorbed an enormous amount of copyrighted material. So I've written a number

454
00:45:14,280 --> 00:45:18,840
of books, and it is a source of intense irritation that the last time that I checked on Google,

455
00:45:18,840 --> 00:45:23,320
the very first link that you got to my textbook was to a pirated copy of the book, somewhere on

456
00:45:23,320 --> 00:45:29,080
the other side of the world. The moment a book is published, it gets pirated. And if you're just

457
00:45:29,720 --> 00:45:33,960
sucking in the whole of the World Wide Web, you're going to be sucking in enormous quantities

458
00:45:33,960 --> 00:45:39,960
of copyrighted content. And there have been examples where very prominent authors have given

459
00:45:39,960 --> 00:45:44,920
the prompt of the first paragraph of their book, and the large language model has faithfully come

460
00:45:44,920 --> 00:45:49,880
up. The following text is, you know, the next, the next five paragraphs of their book. Obviously,

461
00:45:49,880 --> 00:45:54,840
the book was in the training data, and it's latent within the neural networks of those systems.

462
00:45:55,560 --> 00:46:01,160
This is a really big issue for the providers of this technology. And there are lawsuits ongoing.

463
00:46:01,160 --> 00:46:05,000
Right now, I'm not capable of commenting on them because I'm not, I'm not a legal expert,

464
00:46:05,000 --> 00:46:10,200
but there are lawsuits ongoing that will probably take years to unravel. The related issue of

465
00:46:10,280 --> 00:46:16,360
intellectual property in a very broad sense. So for example, for sure, most large language

466
00:46:16,360 --> 00:46:21,080
models will have absorbed J.K. Rowling's novels, right, the Harry Potter novels. So imagine that

467
00:46:21,080 --> 00:46:26,200
J.K. Rowling, who famously spent years in Edinburgh working on the Harry Potter universe and style,

468
00:46:26,200 --> 00:46:32,920
and so on, she releases her first book. It's a big smash hit. The next day, the internet is populated

469
00:46:32,920 --> 00:46:40,600
by fake Harry Potter books produced by this generative AI, which faithfully mimic J.K. Rowling's

470
00:46:40,600 --> 00:46:48,280
style, faithfully mimic that style. Where does that leave her intellectual property? All the Beatles,

471
00:46:48,280 --> 00:46:53,240
you know, the Beatles spend years in Hamburg slaving away to create the Beatles sound,

472
00:46:53,240 --> 00:46:57,560
the revolutionary Beatles sound, everything goes back to the Beatles. They release their first

473
00:46:57,560 --> 00:47:05,960
album and the next day, the internet is populated by fake Beatles songs that really, really faithfully

474
00:47:05,960 --> 00:47:10,920
capture the Lenin and McCartney sound and the Lenin and McCartney voice. But there's a big

475
00:47:10,920 --> 00:47:17,320
challenge here for intellectual property. Related to that, GDPR, anybody in the audience that has

476
00:47:17,320 --> 00:47:22,840
any kind of public profile, data about you will have been absorbed by these neural networks.

477
00:47:22,840 --> 00:47:29,560
So GDPR, for example, gives you the right to know what's held about you and to have it removed.

478
00:47:30,200 --> 00:47:34,120
Now, if all that data is being held in a database, you can just go to the Michael

479
00:47:34,120 --> 00:47:39,240
Waldrich entry and say, fine, take that out with a neural network, no chance. The technology

480
00:47:39,240 --> 00:47:46,120
doesn't work in that way. So you can't go to it and snip out the neurons that know about Michael

481
00:47:46,120 --> 00:47:52,920
Waldrich because it fundamentally doesn't know. It doesn't work in that way. So, and we know this,

482
00:47:52,920 --> 00:47:59,240
combined with the fact that it gets things wrong, has already led to situations where large language

483
00:47:59,240 --> 00:48:05,560
models have made, frankly, defamatory claims about individuals. It was a case in Australia where I

484
00:48:05,560 --> 00:48:10,040
think it claimed that somebody had been dismissed from their job for some kind of gross misconduct

485
00:48:10,040 --> 00:48:13,400
and that individual was, understandably, not very happy about it.

486
00:48:14,920 --> 00:48:19,160
And then finally, this next one is an interesting one. And actually, if there's one thing I want

487
00:48:19,160 --> 00:48:26,120
you to take home from this lecture, which explains why artificial intelligence is different to human

488
00:48:26,120 --> 00:48:32,200
intelligence, it is this video. So the Tesla owners will recognize what we're seeing on the right hand

489
00:48:32,200 --> 00:48:39,240
side of this screen. This is a screen in a Tesla car and the onboard AI in the Tesla car is trying

490
00:48:39,240 --> 00:48:47,000
to interpret what's going on around it. It's identifying lorries, stop signs, pedestrians,

491
00:48:47,000 --> 00:48:51,480
and so on. Now, you'll see the car at the bottom there is the actual Tesla. And then you'll see

492
00:48:51,480 --> 00:48:56,120
above it the things that look like traffic lights, which I think are US stop signs. And then ahead

493
00:48:56,120 --> 00:49:02,920
of it, there is a truck. So as I played a video, watch what happens to those stop signs and ask

494
00:49:02,920 --> 00:49:12,360
yourself what is actually going on in the world around it? Where are all those stop signs whizzing

495
00:49:12,360 --> 00:49:16,520
from? Why are they all whizzing towards the car? And then we're going to pan up and we'll see what's

496
00:49:16,520 --> 00:49:25,720
actually there. The car is trained on enormous numbers of hours of going out on the street

497
00:49:25,720 --> 00:49:31,400
and getting that data and then doing supervised learning, training it by showing that's a stop

498
00:49:31,400 --> 00:49:37,640
sign, that's a truck, that's a pedestrian. But clearly, in all of that training data, there had

499
00:49:37,640 --> 00:49:43,880
never been a truck carrying some stop signs. The neural networks are just making their best guess

500
00:49:43,880 --> 00:49:47,080
about what they're seeing and they think they're seeing a stop sign. Well, they are seeing a stop

501
00:49:47,080 --> 00:49:52,920
sign. They've just never seen one on a truck before. So my point here is that neural networks

502
00:49:52,920 --> 00:50:00,600
do very badly on situations outside their training data. This situation wasn't in the training data,

503
00:50:00,600 --> 00:50:05,160
then neural networks are making their best guess about what's going on and getting it wrong.

504
00:50:05,960 --> 00:50:11,640
So in particular, and this is to AI researchers, this is obvious, but it really needs to emphasize,

505
00:50:11,640 --> 00:50:17,880
we really need to emphasize this. When you have a conversation with chat GPT or whatever,

506
00:50:17,880 --> 00:50:25,880
you are not interacting with a mind. It is not thinking about what to say next. It is not reasoning,

507
00:50:25,880 --> 00:50:30,120
it's not pausing, thinking, well, what's the best answer to this question? That's not what's going

508
00:50:30,120 --> 00:50:37,880
on at all. Those neural networks are working simply to try to make the best answer they can,

509
00:50:37,880 --> 00:50:46,280
the most plausible, sounding answer that they can, the fundamental difference to human intelligence.

510
00:50:46,920 --> 00:50:52,440
There is no mental conversation that goes on in those neural networks. That is not the way

511
00:50:52,440 --> 00:50:59,000
that the technology works. There is no mind there. There is no reasoning going on at all.

512
00:50:59,000 --> 00:51:05,160
Those neural networks are just trying to make their best guess. And it really is just a glorified

513
00:51:05,160 --> 00:51:11,400
version of your autocomplete. Ultimately, there's really no more intelligence there than in your

514
00:51:11,400 --> 00:51:19,320
autocomplete in your smartphone. The difference is scale, data, compute power. Yeah? Okay. So I say,

515
00:51:19,320 --> 00:51:26,200
if you really want, by the way, you can find this video. It's easily, you can just guess the

516
00:51:26,200 --> 00:51:30,120
search terms to find that. And I say, I think this is really important just to understand the

517
00:51:30,120 --> 00:51:39,080
difference between human intelligence and machine intelligence. Okay. So this technology then gets

518
00:51:39,080 --> 00:51:45,480
everybody excited. First, it gets AI researchers like myself excited in June 2020. And we can see

519
00:51:45,480 --> 00:51:51,800
that something new is happening, that this is a new era of artificial intelligence. We've seen

520
00:51:51,800 --> 00:51:57,160
that step change. And we've seen that this AI is capable of things that we didn't train it for,

521
00:51:57,160 --> 00:52:02,760
which is weird and wonderful and completely unprecedented. And now, questions which just

522
00:52:02,760 --> 00:52:09,240
a few years ago were questions for philosophers become practical questions for us. We can actually

523
00:52:09,240 --> 00:52:15,080
try the technology out. How does it do with these things that philosophers have been talking about

524
00:52:15,160 --> 00:52:22,760
for decades? And one particular question starts to float to the surface. And the question is,

525
00:52:23,560 --> 00:52:30,280
is this technology the key to general artificial intelligence? So what is general

526
00:52:30,280 --> 00:52:35,880
artificial intelligence? Well, firstly, it's not very well defined. But roughly speaking,

527
00:52:35,880 --> 00:52:42,520
what general artificial intelligence is, is the following. In previous generations of AI systems,

528
00:52:42,520 --> 00:52:48,760
what we've seen is AI programs that just do one task. Play a game of chess, drive my car,

529
00:52:48,760 --> 00:52:55,640
drive my Tesla, identify abnormalities on x-ray scans. They might do it very, very well, but

530
00:52:55,640 --> 00:53:05,160
they only do one thing. The idea of general AI is that it's AI which is truly general purpose.

531
00:53:05,160 --> 00:53:10,520
It just doesn't do one thing in the same way that you don't do one thing. You can do an infinite

532
00:53:10,600 --> 00:53:17,320
number of things, a huge range of different tasks. And the dream of general AI is that we

533
00:53:17,320 --> 00:53:24,520
have one AI system which is general in the same way that you and I are. That's the dream of general

534
00:53:24,520 --> 00:53:33,320
AI. Now, I emphasize, really until June 2020, this felt like a long, long way in the future.

535
00:53:33,320 --> 00:53:38,120
And it wasn't really very mainstream or taken very seriously. And I didn't take it very seriously.

536
00:53:38,120 --> 00:53:46,040
I have to tell you. But now we have a general purpose AI technology, GPT-3 and chat GPT.

537
00:53:46,040 --> 00:53:55,640
Now, it's not artificial general intelligence on its own, but is it enough? Is this enough? Is this

538
00:53:55,640 --> 00:54:02,920
smart enough to actually get us there? Or to put it another way, is this the missing ingredient

539
00:54:03,000 --> 00:54:13,720
that we need to get us to artificial general intelligence? Okay. So, what might general

540
00:54:13,720 --> 00:54:19,880
AI look like? Well, I've identified here some different versions of general AI according to

541
00:54:19,880 --> 00:54:26,040
how sophisticated they are. Now, the most sophisticated version of general AI would be an AI

542
00:54:26,040 --> 00:54:31,960
which is as fully capable as a human being. That is anything that you could do,

543
00:54:31,960 --> 00:54:37,080
the machine could do as well. Now, crucially, that doesn't just mean having a conversation with

544
00:54:37,080 --> 00:54:43,560
somebody. It means being able to load up a dishwasher. And a colleague recently made the

545
00:54:43,560 --> 00:54:48,440
comment that the first company that can make technology which will be able to reliably

546
00:54:49,080 --> 00:54:53,560
load up a dishwasher and safely load up a dishwasher is going to be a trillion-dollar

547
00:54:53,560 --> 00:54:58,280
company. And I think he's absolutely right. And he also said, and it's not going to happen

548
00:54:58,280 --> 00:55:03,320
anytime soon. And he's also right with that. So, we've got this weird dichotomy that we've got

549
00:55:03,320 --> 00:55:09,320
chat GPT and Co, which are incredibly rich and powerful tools, right? But at the same time,

550
00:55:09,320 --> 00:55:16,200
they can't load a dishwasher. Yeah? So, with some way, I think, from having this version of

551
00:55:16,200 --> 00:55:22,120
general AI, the idea of having one machine that can really do anything that a human being could do,

552
00:55:22,840 --> 00:55:27,160
a machine which could tell a joke, read a book, and answer questions about it. The technology

553
00:55:27,160 --> 00:55:32,280
can read books and answer questions now that could tell a joke, that could cook us an omelet,

554
00:55:32,280 --> 00:55:38,040
that could tidy our house, that could ride a bicycle, and so on, that could write a sonnet,

555
00:55:38,040 --> 00:55:43,240
all of those things that human beings could do. If we succeed with full general intelligence,

556
00:55:43,240 --> 00:55:48,040
then we would have succeeded with this version one. Now, I say, for the reasons that I've already

557
00:55:48,040 --> 00:55:55,080
explained, I don't think this is imminent, that version of general AI, because robotic AI, AI that

558
00:55:55,080 --> 00:56:01,160
exists in the real world and has to do tasks in the real world and manipulate objects in the

559
00:56:01,160 --> 00:56:07,720
real world, robotic AI is much, much harder. It's nowhere near as advanced as chat GPT and Co,

560
00:56:07,720 --> 00:56:12,040
and that's not a slur on my colleagues that do robotics research, it's just because the real

561
00:56:12,040 --> 00:56:18,520
world is really, really, really tough. So, I don't think that we're anywhere close to having machines

562
00:56:18,520 --> 00:56:24,360
that can do anything that a human being could do. But what about the second version? The second

563
00:56:24,360 --> 00:56:30,920
version of general intelligence is, well, forget about the real world, how about just tasks which

564
00:56:30,920 --> 00:56:36,760
require cognitive abilities? Reasoning the ability to look at a picture and answer questions about it,

565
00:56:36,760 --> 00:56:41,480
the ability to listen to something and answer questions about it and interpret that. Anything

566
00:56:41,480 --> 00:56:48,280
which involves those kinds of tasks. Well, I think we are much closer, we're not there yet, but we're

567
00:56:48,280 --> 00:56:54,600
much closer than we were four years ago. Now, I noticed actually, just before today's, before I

568
00:56:54,600 --> 00:56:59,480
came in today, I noticed that Google, Google slash DeepMind have announced their latest

569
00:57:01,320 --> 00:57:06,280
large language model technology, and I think it's called Gemini, and at first glance it looks like

570
00:57:06,280 --> 00:57:11,480
it's very, very impressive. I couldn't help but thinking it's no accident that they announced

571
00:57:11,480 --> 00:57:17,720
that just before my lecture. I can't help think that there's a little bit of attempt to upstage

572
00:57:17,720 --> 00:57:22,040
my lecture going on there, but anyway, we won't let them get away with that. But it looks very

573
00:57:22,040 --> 00:57:29,480
impressive, and the crucial thing is here is what AI people call multimodal. What multimodal means is

574
00:57:29,480 --> 00:57:36,600
it doesn't just deal with text, it can deal with text and images, potentially with sounds as well,

575
00:57:36,600 --> 00:57:42,120
and each of those is a different modality of communication. And where this technology is

576
00:57:42,840 --> 00:57:48,040
clearly multimodal is going to be the next big thing. And Gemini, I say I haven't looked at it

577
00:57:48,040 --> 00:57:55,160
closely, but it looks like it's on that track. Okay, the next version of general intelligence

578
00:57:55,160 --> 00:58:00,760
is intelligence that can do any language-based tasks that a human being could do. So anything

579
00:58:00,760 --> 00:58:07,480
that you could communicate in language, in ordinary written text, an AI system that could do that.

580
00:58:07,480 --> 00:58:13,240
Now, we aren't there yet, and we know we're not there yet, because chat GPT and code get things

581
00:58:13,240 --> 00:58:18,840
wrong all the time. But you can see that we're not far off from that. Intuitively, it doesn't look

582
00:58:18,840 --> 00:58:24,440
like we're that far off from that. The final version, and I think this is imminent, this is

583
00:58:24,440 --> 00:58:29,400
going to happen in the near future, is what I'll call augmented large language models. And that

584
00:58:29,400 --> 00:58:36,680
means you take GPT-3 or chat GPT, and you just add lots of subroutines to it. So if it has to do

585
00:58:36,760 --> 00:58:42,040
a specialist task, it just calls a specialist solver in order to be able to do that task.

586
00:58:42,680 --> 00:58:48,360
And this is not, from an AI perspective, a terribly elegant version of artificial intelligence,

587
00:58:49,080 --> 00:58:55,480
but nevertheless, I think, a very useful version of artificial intelligence. Now, I say there's,

588
00:58:55,480 --> 00:59:00,120
here, these four varieties from the most ambitious down to the least ambitious

589
00:59:01,000 --> 00:59:09,000
still represents a huge spectrum of AI capabilities, a huge spectrum of AI capabilities.

590
00:59:09,000 --> 00:59:14,120
And I have the sense that the goalposts in general AI have been changed a bit. I think when

591
00:59:14,120 --> 00:59:18,760
general AI was first discussed, what people were talking about was the first version. Now,

592
00:59:18,760 --> 00:59:22,840
when they talk about it, I really think they're talking about the fourth version. But the fourth

593
00:59:22,840 --> 00:59:28,360
version, I think plausibly, is imminent in the next couple of years. That just means much more

594
00:59:28,360 --> 00:59:32,760
capable large language models that get things wrong a lot less, that are capable of doing

595
00:59:32,760 --> 00:59:38,680
specialized tasks, but not by using the transformer architecture just by calling on some specialized

596
00:59:38,680 --> 00:59:46,280
software. So I don't think the transformer architecture itself is the key to general

597
00:59:46,280 --> 00:59:51,240
intelligence. In particular, it doesn't help us with the robotics problems that I mentioned earlier

598
00:59:51,240 --> 00:59:58,600
on. And if we look here at this picture, this picture illustrates some of the dimensions

599
00:59:58,600 --> 01:00:03,480
of human intelligence. And it's far from complete. This is me just thinking for half an hour about

600
01:00:03,480 --> 01:00:08,200
some of the dimensions of human intelligence. But the things in blue, roughly speaking,

601
01:00:08,200 --> 01:00:14,040
are mental capabilities, stuff you do in your head. The things in red are things you do in the

602
01:00:14,040 --> 01:00:19,080
physical world. So in red on the right hand side, for example, there's mobility, the ability to

603
01:00:19,080 --> 01:00:25,000
move around some environment and associated with that, navigation. Manual dexterity and

604
01:00:25,000 --> 01:00:31,160
manipulation, doing complex fiddly things with your hands. Robot hands are nowhere near at the

605
01:00:31,160 --> 01:00:37,160
level of a human carpenter or plumber, for example. Nowhere near. So we're a long way out from having

606
01:00:37,160 --> 01:00:44,840
that. Understanding, oh, doing hand-eye coordination, relatedly. Understanding what you're seeing and

607
01:00:44,840 --> 01:00:49,240
understanding what you're hearing, we've made some progress on. But a lot of these tasks we've

608
01:00:49,240 --> 01:00:54,760
made no progress on. And then on the left hand side, the blue stuff is stuff that goes on in your

609
01:00:54,760 --> 01:01:01,080
head. Things like logical reasoning and planning and so on. So what is the state of the art now?

610
01:01:01,080 --> 01:01:07,000
It looks something like this. The red cross means no, we don't have it in large language models.

611
01:01:07,000 --> 01:01:13,400
We're not there. There are fundamental problems. The question marks are, well, maybe we might

612
01:01:13,400 --> 01:01:20,040
have a bit of it, but we don't have the whole answer. And the green-wise are, yeah, I think we're

613
01:01:20,040 --> 01:01:25,240
there. Well, the one that we've really nailed is what's called natural language processing.

614
01:01:25,240 --> 01:01:32,680
And that's the ability to understand and create ordinary human text. That's what large language

615
01:01:32,680 --> 01:01:38,600
models were designed to do, to interact in ordinary human text. That's what they are best at. But

616
01:01:38,680 --> 01:01:43,720
actually, the whole range of stuff, the other stuff here, we're not there at all. By the way,

617
01:01:43,720 --> 01:01:48,440
I did notice that Gem and I claim to have been able to capable of planning. This is a mathematical

618
01:01:48,440 --> 01:01:54,440
reasoning. So I look forward to seeing how good their technology is. But my point is we are still

619
01:01:54,440 --> 01:02:01,160
seen to be some way from full general intelligence. The last few minutes, I want to talk about

620
01:02:01,160 --> 01:02:06,280
something else. And I want to talk about machine consciousness. And the very first thing to say

621
01:02:06,280 --> 01:02:12,040
about machine consciousness is, why on earth should we care about it? I am not remotely

622
01:02:12,040 --> 01:02:16,440
interested in building machines that are conscious. I know very, very few artificial

623
01:02:16,440 --> 01:02:22,280
intelligence researchers that are. But nevertheless, it's an interesting question. And in particular,

624
01:02:22,280 --> 01:02:27,480
it's a question which came to the fore because of this individual. This chap, Blake Lemoine,

625
01:02:27,480 --> 01:02:32,920
in June 2022, he was a Google engineer. And he was working with a Google large language model,

626
01:02:33,000 --> 01:02:38,120
I think it was called Lambda. And he went public on Twitter and I think on his blog

627
01:02:38,120 --> 01:02:43,880
with an extraordinary claim. And he said, the system I'm working on is sentient. And here is

628
01:02:43,880 --> 01:02:48,200
a quote of the conversation that the system came up with. He said, I'm aware of my existence and

629
01:02:48,200 --> 01:02:56,760
I feel happy or sad at times. And it said, I'm afraid of being turned off. And Lemoine concluded

630
01:02:56,760 --> 01:03:04,360
that the program was sentient, which is a very, very big claim indeed. And it made global headlines.

631
01:03:04,360 --> 01:03:11,240
And I received it, I know through the Turing team, we got a lot of press inquiries asking us,

632
01:03:11,240 --> 01:03:16,600
is it true that machines are now sentient? He was wrong on so many levels, I don't even

633
01:03:16,600 --> 01:03:21,880
know where to begin to describe how wrong he was. But let me just explain one particular point to

634
01:03:21,880 --> 01:03:27,960
you. You're in the middle of a conversation with chat GPT, and you go on holiday for a couple of

635
01:03:27,960 --> 01:03:34,360
weeks. When you get back, chat GPT is in exactly the same place. The cursor is blinking, waiting

636
01:03:34,360 --> 01:03:40,280
for you to type your next thing. It hasn't been wondering where you've been. It hasn't been getting

637
01:03:40,280 --> 01:03:45,480
bored. It hasn't been thinking where the hell has Woolridge gone? I'm not going to have a conversation

638
01:03:45,480 --> 01:03:50,680
with him again. It hasn't been thinking anything at all. It's a computer program, which is going

639
01:03:50,680 --> 01:03:57,240
around a loop, which is just waiting for you to type the next thing. Now, there is no sensible

640
01:03:57,240 --> 01:04:03,720
definition of sentience, I think, which would admit that as being sentient. It absolutely is

641
01:04:03,720 --> 01:04:09,080
not sentient. So I think he was very, very wrong. But I've talked to a lot of people subsequently

642
01:04:09,080 --> 01:04:14,120
who have conversations with chat GPT and other large language models, and they come back to me

643
01:04:14,120 --> 01:04:19,960
and say, are you really sure? Because actually, it's really quite impressive. It really feels to me

644
01:04:19,960 --> 01:04:24,680
like there is a mind behind the scene. So let's talk about this. And I think we have to answer

645
01:04:24,680 --> 01:04:29,960
them. So let's talk about consciousness. Firstly, we don't understand consciousness. We all have it,

646
01:04:29,960 --> 01:04:37,720
to greater or lesser extent. We all experience it. But we don't understand it at all. And it's called

647
01:04:37,720 --> 01:04:44,680
the hard problem of cognitive science. And the hard problem is that there are certain

648
01:04:44,680 --> 01:04:49,720
electrical chemical processes in the brain and the nervous system. And we can see those

649
01:04:49,720 --> 01:04:54,920
electrochemical processes, we can see them operating, and they somehow give rise to conscious

650
01:04:54,920 --> 01:05:01,240
experience. But why do they do it? How do they do it? And what evolutionary purpose does it serve?

651
01:05:01,240 --> 01:05:06,840
Honestly, we have no idea. There's a huge disconnect between what we can see going on

652
01:05:06,840 --> 01:05:13,560
in the physical brain and our conscious experience, our rich, private mental life.

653
01:05:14,440 --> 01:05:19,720
So really, there is no understanding of this at all. I think, by the way, my best guess about

654
01:05:19,720 --> 01:05:25,960
how consciousness will be solved, if it is solved at all, is through an evolutionary approach.

655
01:05:26,680 --> 01:05:33,960
But one general idea is that subjective experience is central to this, which means the ability to

656
01:05:33,960 --> 01:05:39,880
experience things from a personal perspective. And there's a famous test due to Nagel, which is

657
01:05:39,880 --> 01:05:44,840
what is it like to be something? And Thomas Nagel in the 1970s said, something is conscious

658
01:05:44,840 --> 01:05:53,480
if it is like something to be that thing. It isn't like anything to be chat GPT. Chat GPT

659
01:05:53,480 --> 01:06:01,560
has no mental life whatsoever. It's never experienced anything in the real world whatsoever.

660
01:06:02,280 --> 01:06:05,720
And so for that reason, and a whole host of others that we're not going to have time to go into,

661
01:06:06,440 --> 01:06:11,800
for that reason alone, I think we can conclude pretty safely that the technology that we have now

662
01:06:11,800 --> 01:06:18,280
is not conscious. And indeed, that's absolutely not the right way to think about this. And honestly,

663
01:06:18,280 --> 01:06:24,280
in AI, we don't know how to go about making conscious machines. But I don't know why we would.

664
01:06:25,160 --> 01:06:28,440
Okay. Thank you very much, ladies and gentlemen.

665
01:06:43,880 --> 01:06:49,320
Amazing. Thank you so much, Mike, for that talk. I'm sure there's going to be tons of questions.

666
01:06:49,320 --> 01:06:53,480
Just as a reminder, if you're in the room, please raise your hand if you have a question. And we've

667
01:06:53,480 --> 01:06:57,320
got roaming mics that we'll send around. If you're online, you can submit them via the chat,

668
01:06:57,320 --> 01:07:03,000
via the Vimeo function, and we can assign it on the chat to ask those questions as well.

669
01:07:03,000 --> 01:07:06,760
So please do raise your questions. Oh, raise your hands if you have one.

670
01:07:06,760 --> 01:07:09,240
You've got a question here, just in the black top.

671
01:07:13,720 --> 01:07:19,160
Thank you very much. That was very, very good. Very interesting. How do large language models

672
01:07:19,240 --> 01:07:24,200
correct for different spoken languages? And do you find that the level of responses

673
01:07:24,760 --> 01:07:29,640
across different languages vary enormously in their depth?

674
01:07:29,640 --> 01:07:36,200
Right. Good question. And that's the focus of a huge amount of research right now.

675
01:07:36,200 --> 01:07:41,560
And I say the big problem is that most digital text in the world, the vast majority of it,

676
01:07:41,560 --> 01:07:47,240
is in English and in North American English. And so languages with a small digital footprint

677
01:07:47,240 --> 01:07:52,600
end up being massively marginalized in this. So there's a huge amount of work that's going on

678
01:07:52,600 --> 01:07:57,880
to try to deal with this problem. Let me tell you a really interesting aspect of this, though.

679
01:07:57,880 --> 01:08:02,680
The languages that have a small digital footprint, can you guess what the most

680
01:08:02,680 --> 01:08:10,600
digital texts that are available are actually concerned with? Religion. Right? So languages

681
01:08:10,600 --> 01:08:15,000
that don't have a big digital presence, where they do have a big digital presence, it turns

682
01:08:15,000 --> 01:08:21,320
out that the main texts which are available are religious texts. Now, I'm not a religious person

683
01:08:21,320 --> 01:08:27,320
myself, but the idea of a kind of Old Testament large language model, frankly, I find a little

684
01:08:27,320 --> 01:08:31,080
bit terrifying. But that's exactly the kind of issue that people are grappling with. There are

685
01:08:31,080 --> 01:08:36,360
no fixes at the moment, but people are working on it very, very hard. And really what this relates

686
01:08:36,360 --> 01:08:43,800
to is the problem of that you're being lazy with these large language models and that you're just

687
01:08:43,800 --> 01:08:48,760
throwing massive, massive amounts of text. We've got to make the technology much more efficient in

688
01:08:48,760 --> 01:08:54,520
terms of learning. Awesome. Thank you. If you have a question, we have one right at the front

689
01:08:54,520 --> 01:09:01,320
in the center here. Thank you. Thank you very much for that. One of the big questions is obviously

690
01:09:01,320 --> 01:09:08,520
climate change. The models require a huge amount of energy to run. Generating pictures of cats or

691
01:09:08,520 --> 01:09:14,360
silly gooses, geese and stuff, are obviously using lots of energy. Do you think we reach a point where

692
01:09:15,560 --> 01:09:21,480
Generative AI will help us solve our issue with climate change or will it burn us in the process?

693
01:09:21,480 --> 01:09:27,000
So I think, okay, so two things to say. I absolutely am not defending the CO2 emissions,

694
01:09:27,000 --> 01:09:32,120
but we need to put that into some perspective. So if I fly to New York from London, I think it's

695
01:09:32,120 --> 01:09:38,520
some like two tons of CO2 that I pump into the atmosphere through that. So the machine learning

696
01:09:38,520 --> 01:09:44,040
community has some big conferences which attract like 20,000 people from across the world. Now,

697
01:09:44,040 --> 01:09:49,080
if you think each of them generating five tons of CO2 on their journey, that I think is probably a

698
01:09:49,080 --> 01:09:56,680
bigger climate problem for that community. But nevertheless, people are very aware of that

699
01:09:56,680 --> 01:10:03,400
problem and I think it clearly needs to be fixed. I think though, helping with climate change, I

700
01:10:03,400 --> 01:10:09,480
don't think you need larger language models for that. I mean, I think AI itself can just be enormously

701
01:10:09,480 --> 01:10:13,400
helpful in order to be able to ameliorate that and we're doing a lot of work on that at the Turing

702
01:10:13,400 --> 01:10:21,400
Institute. For example, just on helping systems be more efficient, heating systems be more efficient.

703
01:10:21,400 --> 01:10:26,760
There was a nice example, I think, from DeepMind with their data centers, the cooling in their

704
01:10:26,760 --> 01:10:31,800
data centers and basically just trying to predict the usage of it. If you can reliably predict the

705
01:10:31,800 --> 01:10:36,600
usage of it, then you can predict the cooling requirements much more effectively and end up

706
01:10:36,600 --> 01:10:42,280
with much, much, much better use of power and that can go down to the level of individual homes.

707
01:10:43,160 --> 01:10:48,920
So there are lots of applications of AI, I think, not just large language models, lots of applications

708
01:10:48,920 --> 01:10:55,080
of AI that are going to help us with that problem. But yeah, I think this brute force approach,

709
01:10:56,120 --> 01:11:01,560
just supercomputers running for months with vast amounts of data is clearly an ugly solution.

710
01:11:01,560 --> 01:11:04,920
I think it will probably be a transitory phase. I think we will get beyond it.

711
01:11:06,280 --> 01:11:09,000
Thank you. Swing to the left over here. There's one right at the back

712
01:11:09,560 --> 01:11:13,160
at the top over here. Watch our hearts. I'm going to get a mic across.

713
01:11:13,800 --> 01:11:23,960
Thank you very much. I've got a sort of more philosophical question.

714
01:11:23,960 --> 01:11:29,160
You've talked about general AI and the sort of peak of general AI is its ability to

715
01:11:29,160 --> 01:11:34,760
mimic a human and all the things a human can do. Can you envision a path whereby AI could actually

716
01:11:34,760 --> 01:11:40,200
become superhuman so it starts to solve problems or ask questions that we haven't tried to do ourselves?

717
01:11:41,160 --> 01:11:50,600
This is another well trodden question, which I always dread, I have to say,

718
01:11:51,400 --> 01:11:55,480
but it's a perfectly reasonable question. So I think what you're hinting at is something that in

719
01:11:55,480 --> 01:12:01,320
the AI community is called the singularity. The argument of the singularity goes as possible.

720
01:12:01,320 --> 01:12:07,160
At some point as follows, at some point in the future, we're going to have AI which is as intelligent

721
01:12:07,240 --> 01:12:12,680
as human beings in the general sense. That is, it will be able to do any intellectual task

722
01:12:12,680 --> 01:12:18,360
that a human being can do. And then there's an idea that, well, that AI can look at its own code

723
01:12:18,360 --> 01:12:23,160
and make itself better, right? Because it can code. It can start to improve its own code.

724
01:12:23,160 --> 01:12:30,120
And the point is, once it's a tiny way beyond us, then the concern is that it's out of control

725
01:12:30,120 --> 01:12:35,800
at that point, that we really don't understand it. So the community is a bit divided on this.

726
01:12:35,800 --> 01:12:41,560
I think some people think that's science fiction. Some people think it's a plausible scenario that

727
01:12:41,560 --> 01:12:47,960
we need to prepare for and think for. I'm completely comfortable with the idea. I think it is just

728
01:12:47,960 --> 01:12:55,240
simply good sense to take that potential issue seriously and to think about how we might mitigate

729
01:12:55,240 --> 01:13:01,080
it. There are many ways of mitigating it. One of the ways of mitigating it is designing the AI

730
01:13:01,080 --> 01:13:06,520
so that it is intrinsically designed to be helpful to us, that it's never going to be

731
01:13:06,520 --> 01:13:14,520
unhelpful to us. But I have to tell you, it is not at all a universally held belief that that's

732
01:13:14,520 --> 01:13:19,160
where we're going in AI. There are still big, big problems to overcome before we get there.

733
01:13:19,960 --> 01:13:23,400
I'm not sure that's an entirely reassuring answer, but that's the best I've got to offer.

734
01:13:24,200 --> 01:13:26,760
Great. Thanks, Mike. Well, just pop it online with us, Anne.

735
01:13:26,760 --> 01:13:30,520
Yeah. So we've had questions from all over the world. We have Peter tuning in from Switzerland,

736
01:13:30,520 --> 01:13:40,600
London, Birmingham. But the question I'm going to focus on. So the question is going to be on

737
01:13:40,600 --> 01:13:44,840
the tuning test and whether that's still relevant and whether we have AI that has passed the

738
01:13:44,840 --> 01:13:51,240
Turing test. Oh, the Turing test. Okay. So the Turing test, we saw Alan Turing up there,

739
01:13:53,640 --> 01:14:00,120
a national hero. Turing 1950, first digital computers have appeared and Turing's working

740
01:14:00,120 --> 01:14:05,320
on one at the University of Manchester. And the idea of AI as in the air hasn't got a name yet,

741
01:14:05,320 --> 01:14:09,320
but people are talking about electronic brains and getting very excited about what they can do.

742
01:14:09,320 --> 01:14:14,680
So people are starting to think about the ideas that become AI. And Turing gets frustrated with

743
01:14:14,680 --> 01:14:19,640
people saying, well, of course, it would never actually really, really be able to think or never

744
01:14:19,640 --> 01:14:24,600
really be able to understand and so on. So he comes up with the following test in order to just

745
01:14:24,600 --> 01:14:29,960
really to try and shut people up talking about it. And the paper is called Computing Machinery

746
01:14:29,960 --> 01:14:36,520
Intelligence, and it's published in the journal Mind, which is a very respectable journal,

747
01:14:36,520 --> 01:14:40,680
a very unusual paper. It's very readable, by the way. You can download it and read it.

748
01:14:40,680 --> 01:14:46,120
But he proposes the Turing test. So Turing says, suppose we're trying to settle the question of

749
01:14:46,120 --> 01:14:53,560
whether a machine can really think or understand. So here's a test for that. What you do is you

750
01:14:53,560 --> 01:14:58,760
take that machine behind closed doors, and you get a human judge to be able to interact with

751
01:14:58,760 --> 01:15:03,960
something via a keyboard and a screen. In Turing's day, it would have been a teletype. Just by typing

752
01:15:03,960 --> 01:15:09,240
away questions, actually remarkably, pretty much what you do with chat GPT. Give it prompts

753
01:15:09,240 --> 01:15:14,440
anything you like. And actually, Turing has some very entertaining ones in his paper. And what you

754
01:15:14,440 --> 01:15:21,320
try and do is you try to decide whether the thing on the other side is a computer or a human being.

755
01:15:21,320 --> 01:15:27,800
And Turing's point was, if you cannot reliably tell that the thing on the other side is a human

756
01:15:27,800 --> 01:15:33,480
being or a machine, and it really is a machine, then you should accept that this thing has something

757
01:15:33,480 --> 01:15:38,920
like human intelligence, because you can't tell the difference. There's no test that you can apply

758
01:15:38,920 --> 01:15:42,520
without actually pulling back the curtain and looking to see what's there that's going to

759
01:15:42,520 --> 01:15:47,880
show you whether it's a human or a machine. You can't tell the difference. It's indistinguishable.

760
01:15:48,600 --> 01:15:53,960
So this was important historically, because it really gave AI people a target. When you said

761
01:15:53,960 --> 01:15:57,320
I'm an AI researcher, what are you trying to do? I'm trying to build a machine that can pass the

762
01:15:57,320 --> 01:16:01,320
Turing test. There was a concrete goal. The problem is in science, whenever you work with

763
01:16:01,320 --> 01:16:06,440
science and society, whenever you set up some challenge like that, you get all sorts of charlatans

764
01:16:06,440 --> 01:16:12,280
and idiots who just try and come up with ways of faking it. And so most of the ways of trying to get

765
01:16:12,280 --> 01:16:17,480
past the Turing test over the last 70 years have really just been systems that just come up with

766
01:16:17,480 --> 01:16:23,080
kind of nonsense answers trying to confuse the questioner. But now we've got large language

767
01:16:23,080 --> 01:16:31,000
models. So we're going to find out in about 10 days time, we're going to run a live Turing test as

768
01:16:31,000 --> 01:16:37,400
part of the Christmas lectures, and we will see whether our audience can distinguish a large

769
01:16:37,400 --> 01:16:42,920
language model from a teenage child. And we've trialed this, and I have to tell you it's possibly

770
01:16:42,920 --> 01:16:49,000
closer than you might think actually. Do I really think we passed the Turing test? Not in a deep

771
01:16:49,000 --> 01:16:55,800
sense, but what I think is that it's demonstrated to us firstly, machines clearly can generate text,

772
01:16:55,800 --> 01:17:00,360
which is indistinguishable from text that a human being could generate. We've done that,

773
01:17:00,360 --> 01:17:08,440
that box is ticked, and they can clearly understand text. So even if we haven't followed the Turing

774
01:17:08,440 --> 01:17:13,960
test to the letter, I think for all practical intents and purposes, the Turing test is now a

775
01:17:13,960 --> 01:17:19,240
historical note. Yeah, but actually the Turing test only tests one little bit of intelligence.

776
01:17:19,240 --> 01:17:23,480
You remember those dimensions of intelligence that I showed you? There's a huge range of those

777
01:17:23,480 --> 01:17:30,040
that it doesn't test. So it was historically important, and it's a big part of our historical

778
01:17:30,040 --> 01:17:37,160
legacy, but maybe not a core target for AI today. Cool. Thank you, Mike. I think now you've been

779
01:17:37,160 --> 01:17:41,160
the warning when you say a lot of searches for preparing for the Turing test for the Christmas

780
01:17:41,160 --> 01:17:45,240
lecture next week. Do you have any questions up at the top? Yeah, I've got one right in the

781
01:17:45,240 --> 01:17:57,560
center just here. Thank you. So when we think about the situations or use cases where AI is

782
01:17:57,560 --> 01:18:04,680
applied, typically the reason for that is because the machine is doing things better than a human

783
01:18:04,680 --> 01:18:11,400
can or doing things that a human might not be able to do. So it's a lot about the machine making up

784
01:18:11,400 --> 01:18:18,840
for the gaps that a human creates. That said, a machine is fallible, like there are errors,

785
01:18:18,840 --> 01:18:24,680
both normative errors and also statistical errors depending on the model type, etc. And so the

786
01:18:24,680 --> 01:18:31,720
question is who do you think should be responsible for looking after the gaps that the machine now

787
01:18:31,720 --> 01:18:37,240
creates? So the fundamental question is who should be responsible, right? Is that right? Sorry, I

788
01:18:37,240 --> 01:18:42,120
didn't see where you were. Can you put your hand up? Right, the top in the middle. Oh, wow. Okay,

789
01:18:42,120 --> 01:18:51,800
so that's why I can't see you. Okay. So this is an issue that's being discussed absolutely in the

790
01:18:51,800 --> 01:18:57,720
highest levels of government right now, that literally when we work in, when we move into the

791
01:18:57,720 --> 01:19:04,360
age of AI, who should accept the responsibility? I can tell you what my view is, but I'm not a

792
01:19:04,360 --> 01:19:12,600
lawyer or an ethics expert. And my view is that, as follows, firstly, if you use AI in your work,

793
01:19:13,240 --> 01:19:18,440
then, and you end up with a bad result, I'm sorry, but that's your problem. If you use it to generate

794
01:19:18,440 --> 01:19:23,240
an essay at school and you're caught out, I'm afraid that's your problem. It's not the fault

795
01:19:23,240 --> 01:19:32,520
of the AI. But I think more generally, we can't offload our legal, moral, ethical obligations

796
01:19:32,520 --> 01:19:38,040
as human beings onto the machine. That is, we can't say it's not my fault, the machine did it.

797
01:19:38,040 --> 01:19:43,240
Right? An extreme example of this is lethal autonomous weapons, AI that's empowered to decide

798
01:19:43,240 --> 01:19:48,360
whether to take a human life. What I worry about, one of the many things I worry about with lethal

799
01:19:48,360 --> 01:19:52,440
autonomous weapons is the idea that we have military services that say, well, it wasn't our fault,

800
01:19:52,440 --> 01:19:57,080
it was the AI that got it wrong, that led to this building being bombed or whatever it was.

801
01:19:57,720 --> 01:20:02,440
And there, I think the responsibility lies with the people that deploy the technology.

802
01:20:03,640 --> 01:20:08,200
So that, I think, is a crucial point. But at the same time, the developers of this technology,

803
01:20:08,200 --> 01:20:14,120
if they are warranting that it is fit for purpose, then they have a responsibility as well. And the

804
01:20:14,120 --> 01:20:19,800
responsibility that they have is to ensure that it really is fit for purpose. And it's an interesting

805
01:20:19,800 --> 01:20:24,280
question at the moment. If we have large language models used by hundreds of millions of people,

806
01:20:24,280 --> 01:20:30,360
for example, to get medical advice, and we know that this technology can go wrong,

807
01:20:30,360 --> 01:20:36,360
is the technology fit for that purpose? I'm not sure at all that it is. So I'm not sure

808
01:20:36,360 --> 01:20:40,040
that's really answering your question, but those are my sort of a few random thoughts on it.

809
01:20:40,040 --> 01:20:43,480
I mean, but I say, crucially, you know, if you're using this in your work,

810
01:20:43,480 --> 01:20:49,640
you can never blame the AI, right? You are responsible for the outputs of that process,

811
01:20:49,640 --> 01:20:55,160
right? You can't offload your legal, professional, ethical, moral obligations to the machine.

812
01:20:56,280 --> 01:21:00,440
It's a complex question, which is why I gave a very bad answer.

813
01:21:00,600 --> 01:21:05,800
I've got a question right on the left here. I've used access on the left panel shelf.

814
01:21:07,320 --> 01:21:14,840
Thank you. If future large language models are trained by scraping the whole internet again,

815
01:21:14,840 --> 01:21:22,680
now there's more and more content going on to the internet created by AI. So is that going to create

816
01:21:22,680 --> 01:21:29,240
something like a microphone feedback loop where the information gets less and less useful?

817
01:21:29,960 --> 01:21:35,640
Super question and really fascinating. So I have some colleagues that did the following experiment.

818
01:21:35,640 --> 01:21:42,760
So chat GPT is trained, roughly speaking, on human generated text, but it creates AI generated text.

819
01:21:42,760 --> 01:21:47,640
So the question they had is what happens if we train one of these models, not on the original

820
01:21:47,640 --> 01:21:53,400
human generated text, but just on stuff which is produced by AI? And then you can see what they

821
01:21:53,400 --> 01:21:57,720
did next. You can guess, they said, well, okay, let's take another model which is trained on the

822
01:21:57,720 --> 01:22:03,880
second generation model text. And so what happens about five generations down the line,

823
01:22:03,880 --> 01:22:10,920
it dissolves into gibberish, literally dissolves into gibberish. And I have to tell you the original

824
01:22:10,920 --> 01:22:18,680
version of this paper, they called it AI dementia. And I was really cross with, no, I lost both my

825
01:22:18,680 --> 01:22:23,800
parents to dementia. I didn't find it very funny at all. They now call it model collapse. So if you

826
01:22:23,800 --> 01:22:28,040
go and Google model collapse, you'll find the answers there. But really remarkable. What that

827
01:22:28,040 --> 01:22:34,200
tells you is that actually there is something qualitatively different at the moment to human

828
01:22:34,200 --> 01:22:39,720
text, to AI generated text. For all that it looks perfect or indistinguishable to us,

829
01:22:39,720 --> 01:22:44,280
actually it isn't. Where is that going to take us? I have colleagues who think that we're going to

830
01:22:44,280 --> 01:22:51,640
have to label and protect human generated content because it is so valuable. All right? Human

831
01:22:51,640 --> 01:22:58,840
generated actual, authentic human generated content is really, really valuable. I also have

832
01:22:58,840 --> 01:23:03,000
colleagues, and I'm not sure whether they're entirely serious at this, but they say that actually

833
01:23:03,000 --> 01:23:09,000
where we're going is the data that we produce in everything that we do is so valuable for AI

834
01:23:09,000 --> 01:23:14,760
that we're going to enter a future where you're going to sell the rights to AI companies for you,

835
01:23:14,760 --> 01:23:20,600
for them to harvest your emotions, all of your experiences, everything you say and do in your

836
01:23:20,600 --> 01:23:25,720
life. And you'll be paid for that, but it will go into the training models of large language models.

837
01:23:25,720 --> 01:23:31,320
Now I don't know if that's true, but nevertheless there's a, it has some inner truth in it, I think.

838
01:23:32,520 --> 01:23:40,040
And in 100 years time, it is an absolute certainty that there will be vastly,

839
01:23:40,040 --> 01:23:46,200
vastly more AI generated content out there in the world than there will human generated content

840
01:23:46,200 --> 01:23:50,600
with certainty. I think there's no question, but that that's the way the future is going.

841
01:23:50,600 --> 01:23:56,120
And as I say, as the model collapse scenario illustrates, that presents some real challenges.

842
01:23:57,320 --> 01:24:00,520
Awesome. Thank you very much, Mike. I've got a question at the front who's been very keen to ask.

843
01:24:03,560 --> 01:24:08,440
Thanks very much indeed for a very interesting lecture. It strikes me in a way just being a

844
01:24:08,440 --> 01:24:13,800
comparison of human being. What we're doing is talking about what the prefront frontal cortex

845
01:24:13,800 --> 01:24:19,320
does, but there are other areas of prefrontal cortex, which is a fear predictor. Do we need to

846
01:24:19,320 --> 01:24:25,800
be developing sort of a parallel AI system, which works on the basis of fear prediction

847
01:24:25,800 --> 01:24:32,600
and get to talk to each other? Yeah. So I'm absolutely not a neuroscientist or I'm a computer

848
01:24:32,600 --> 01:24:40,360
programmer and that's very much my background. Again, it's interesting that the community is

849
01:24:40,360 --> 01:24:44,600
incredibly divided. So when I was an undergraduate studying AI and I focused in my final year,

850
01:24:44,600 --> 01:24:49,800
it's mainly what I studied. And the textbooks that we had made no reference to the brain

851
01:24:49,800 --> 01:24:54,440
whatsoever. Just wasn't the thing because it was all about modeling the mind. It was all about

852
01:24:54,440 --> 01:25:00,280
modeling conscious reasoning processes and so on. And it was deeply unfashionable to think about

853
01:25:00,280 --> 01:25:04,840
the brain. And there's been a bit of a what scientists call a paradigm shift in the way

854
01:25:04,840 --> 01:25:09,160
that they think about this prompted by the rise of neural networks, but also by the fact that

855
01:25:09,160 --> 01:25:15,640
advances in computer vision and the architectures, the neural network architectures that led to

856
01:25:15,640 --> 01:25:20,760
facial recognition really working were actually inspired by the visual cortex, the human visual

857
01:25:20,760 --> 01:25:27,080
cortex. So it's a lot more of a fashionable question now than it used to be. So my guess is,

858
01:25:27,080 --> 01:25:33,080
firstly, simply trying to copy the structure of the human brain is not the way to do it,

859
01:25:33,080 --> 01:25:37,240
but nevertheless getting a much better understanding of the organization of the brain,

860
01:25:37,320 --> 01:25:40,600
the functional organization of the brain, and the way that the different components of the

861
01:25:40,600 --> 01:25:47,800
brain interoperate to produce human intelligence, I think, is. And really, there's a vast amount

862
01:25:47,800 --> 01:25:52,120
of work there to be done to try to understand that. There are so many unanswered questions.

863
01:25:52,680 --> 01:25:56,120
I hope that's some help. Thank you, Mike. We're just going to jump back online.

864
01:25:56,120 --> 01:25:59,960
Yeah, that's going to be a little early. Anthony asks, if emergency is inaccurate,

865
01:25:59,960 --> 01:26:05,400
is calling the technology intelligence inaccurate? Are we just dreaming of something

866
01:26:05,400 --> 01:26:10,280
that can never be? And then to follow up on that, you've got Tom Fatcher who asks,

867
01:26:10,280 --> 01:26:14,280
is there anything happening to develop native analog neural networks

868
01:26:14,280 --> 01:26:17,480
rather than doing neural networks in a digital machine only?

869
01:26:19,080 --> 01:26:25,240
Take the second one. Yeah, there certainly is. So Steve Ferber at Manchester is building hardware

870
01:26:25,240 --> 01:26:31,800
neural networks. But the moment it's just much cheaper and much more efficient to do it in

871
01:26:31,800 --> 01:26:37,480
software. There have been various attempts over the years to develop neural net processes,

872
01:26:38,600 --> 01:26:42,680
famous phrase from the movie that you're not allowed to mention to AI researchers,

873
01:26:43,400 --> 01:26:48,280
the Terminator movies, the neural network processes. If you want to wind up an AI researcher,

874
01:26:48,280 --> 01:26:55,160
just bring up the Terminator. It's a shortcut to triggering them. But neural network processes

875
01:26:55,160 --> 01:26:59,800
have never really taken off. Doesn't mean they won't do, but at the moment, it's just much cheaper

876
01:26:59,800 --> 01:27:05,400
and much more efficient to throw more conventional GPUs and so on at the problem. It doesn't mean

877
01:27:05,400 --> 01:27:08,600
it won't happen, but at the moment, it's not there yet. What was the other question again, the first

878
01:27:08,600 --> 01:27:12,440
one? So the other question was, are we basically the terminology being used? If emergency is

879
01:27:12,440 --> 01:27:17,080
inaccurate, is calling the technology intelligence inaccurate? And are we dreaming of something

880
01:27:17,080 --> 01:27:24,840
that can never be? Yeah, so the phrase artificial intelligence was coined by John McCarthy around

881
01:27:24,840 --> 01:27:31,080
about 1955. He was 28 years old, a young American researcher, and he wants funding to get a whole

882
01:27:31,080 --> 01:27:34,840
bunch of researchers together for a summer, and he thinks they'll solve artificial intelligence

883
01:27:34,840 --> 01:27:40,440
in a summer. But he has to give a title to his proposal, which goes to the Rockefeller Foundation,

884
01:27:40,440 --> 01:27:45,720
and he fixes on artificial intelligence. And boy, have we regretted that ever since.

885
01:27:46,440 --> 01:27:52,600
The problem is, firstly, artificial sounds like fake. It sounds like ursat. I mean,

886
01:27:52,680 --> 01:27:58,920
who wants fake intelligence? And for intelligence itself, the problem is that so many of the

887
01:27:58,920 --> 01:28:03,960
problems that have just proved to be really hard for AI actually don't seem to require

888
01:28:03,960 --> 01:28:09,720
intelligence at all. So the classic example, driving a car. When somebody passes their driving test,

889
01:28:09,720 --> 01:28:15,960
they don't think, wow, you're a genius. It doesn't seem to require intelligence in people,

890
01:28:15,960 --> 01:28:21,320
but I cannot tell you how much money has been thrown at driverless car technologies,

891
01:28:21,320 --> 01:28:26,200
and we are a long way off from jumping into a car and saying, take me to a country pub,

892
01:28:26,920 --> 01:28:33,560
which is my dream of the technology, I have to tell you. We're a long, long way off.

893
01:28:33,560 --> 01:28:42,040
So it's a classic example of what people think AI is focused on is deep intellectual tasks,

894
01:28:42,040 --> 01:28:47,000
and that's actually not where the most difficult problems are. The difficult problems are actually

895
01:28:47,000 --> 01:29:14,760
surprisingly mundane. Well, I was interested in how you mentioned that

896
01:29:15,720 --> 01:29:23,480
the two pools of AI study were symbolic AI and big AI, and I was wondering how you sawed how

897
01:29:24,120 --> 01:29:29,640
your viewpoint on the change in focus from one to another throughout your career.

898
01:29:29,640 --> 01:29:36,920
Yeah. So an enormous number of people are busy looking at that right now. So remember symbolic AI,

899
01:29:36,920 --> 01:29:42,040
which is the tradition that I grew up in AI, which was dominant for kind of 30 years in the AI

900
01:29:42,040 --> 01:29:48,680
community, is roughly, and again, hand-waving madly at this point, and lots and lots of my

901
01:29:48,680 --> 01:29:53,320
colleagues are cringing madly at this point, roughly speaking, the idea of symbolic AI is that

902
01:29:53,320 --> 01:29:59,640
you're modeling the mind, the conscious mind, conscious mental reasoning processes, where you

903
01:29:59,640 --> 01:30:04,200
have a conversation with yourself, and you have a conversation in a language, right? You're trying

904
01:30:04,200 --> 01:30:08,760
to decide whether to go to this lecture tonight, and you think, well, yeah, but there's EastEnders

905
01:30:08,760 --> 01:30:15,000
on TV, and mom's cooking a nice meal, but then it's going to be really interesting. You weigh up

906
01:30:15,000 --> 01:30:22,120
those options, and literally symbolic AI tries to capture that kind of thing explicitly, and using

907
01:30:22,120 --> 01:30:28,840
languages that with a bit of squinting resemble human languages. Then we've got the alternative

908
01:30:28,840 --> 01:30:35,240
approach, which is machine learning, data-driven, and so on, which, again, I emphasize with neural

909
01:30:35,240 --> 01:30:39,800
approaches, we're not trying to build artificial brains, that's not what's going on,

910
01:30:39,800 --> 01:30:44,840
but we're taking inspiration from the structures that we see in brains and nervous systems,

911
01:30:44,840 --> 01:30:51,240
and in particular, the idea that large computational tasks can be reduced down to tiny, simple

912
01:30:51,240 --> 01:30:57,640
pattern recognition problems. Okay, but we've seen, for example, that large language models

913
01:30:57,640 --> 01:31:03,080
get things wrong a lot, and a lot of people have said, but look, maybe if you just married

914
01:31:03,080 --> 01:31:08,200
the neural and the symbolic together so that the symbolic system did have something like a

915
01:31:08,200 --> 01:31:13,080
database of facts, that you could put that together with a large language model and be able to

916
01:31:14,600 --> 01:31:20,840
improve the outputs of the large language model. The jury is out exactly on how that's going to come

917
01:31:20,840 --> 01:31:27,720
out. Lots of different ideas out there now. Trillion-dollar companies are spending billions

918
01:31:27,720 --> 01:31:32,520
of dollars right now to investigate exactly the question that you've put out there,

919
01:31:32,600 --> 01:31:38,440
so it's an extremely pertinent question. There's no, I say, I don't see any answer on the horizon

920
01:31:38,440 --> 01:31:45,000
right now, which looks like it's going to win out. My worry is that what we'll end up with

921
01:31:45,000 --> 01:31:51,240
is a kind of unscientific solution. That is a solution which is sort of hacked together

922
01:31:51,240 --> 01:31:57,720
without any deep underlying principles, and as a scientist, what I would want to see is something

923
01:31:57,720 --> 01:32:02,760
which was tied together with deep scientific principles, but it's an extremely pertinent

924
01:32:02,760 --> 01:32:09,800
question, and I say right now an enormous number of PhD students across the world are busy looking

925
01:32:09,800 --> 01:32:14,680
at exactly what you've just described. Thank you, Mike. Time for a squeeze and two more questions.

926
01:32:14,840 --> 01:32:19,800
Take one from in the room. Cool. We've got a question just in the middle at the back there.

927
01:32:33,880 --> 01:32:41,080
For the lecture, my question is around, you sort of took us on the journey from

928
01:32:41,080 --> 01:32:48,280
40 years ago, some of the inspirations around how the mind works and the mathematics. He said

929
01:32:48,280 --> 01:32:55,480
the mathematics was fairly simple. I would like your opinion. Where do you think we're not looking

930
01:32:55,480 --> 01:33:06,120
enough for where leap be? Oh, wow. If I knew that, I'd be forming a company, I have to tell you.

931
01:33:06,600 --> 01:33:13,320
Okay, so I think the first thing to say is I said when it started to become clear that this

932
01:33:13,320 --> 01:33:18,200
technology was worked, Silicon Valley starts to make bets, and these bets are billion dollar

933
01:33:18,200 --> 01:33:23,400
bets, a lot of billion dollar bets going on, investing in a very, very wide range of different

934
01:33:23,400 --> 01:33:28,280
ideas in the hope that one is going to be the one that delivers something which is going to give

935
01:33:28,280 --> 01:33:34,360
them a competitive advantage. That's the context in which we're trying to figure out what the next

936
01:33:34,360 --> 01:33:44,920
big thing is going to be. I think this multimodal is going to be dominant. That's what we're going

937
01:33:44,920 --> 01:33:49,400
to see, and you're going to hear that phrase, multimodal. Remember, you heard it here first,

938
01:33:49,400 --> 01:33:54,440
if you've never heard it before. You're going to hear that a lot, and that's going to be text,

939
01:33:54,440 --> 01:34:01,800
images, sound, video. You're going to be able to upload videos, and the AI will describe what's

940
01:34:01,800 --> 01:34:06,280
going on in the video or produce a summary, and you'll be able to say what happens after this bit

941
01:34:06,280 --> 01:34:11,160
in the video, and it will be able to come out with a description of that for you. Alternatively,

942
01:34:11,160 --> 01:34:15,480
you'll be able to give a storyline, and it will generate videos for you. Ultimately,

943
01:34:15,480 --> 01:34:20,840
where it's going to go is in virtual reality. I don't know if you like Lord of the Rings or

944
01:34:20,840 --> 01:34:26,200
Star Wars, but I enjoy both of those, and wouldn't you love to see a mash-up of those two things?

945
01:34:26,440 --> 01:34:33,800
Generative AI will be able to do it for you. I used to think this was just a bit of a pipe dream,

946
01:34:33,800 --> 01:34:41,640
but actually, at the moment, it seems completely plausible. If you like the original Star Trek

947
01:34:41,640 --> 01:34:49,960
series, which I do, and my family doesn't, but there was only 60-odd episodes of them. In the

948
01:34:49,960 --> 01:34:56,680
generative AI future, there will be as many episodes as you want, and it will look and sound

949
01:34:56,680 --> 01:35:02,680
like Leonard Nimoy and William Shatner perfectly. Maybe the storylines won't be that great, but

950
01:35:02,680 --> 01:35:07,400
actually, they don't need to be if they're pressing a button specifically to your tastes.

951
01:35:08,120 --> 01:35:13,560
That's the general trajectory of where we're going. I say, actually, I don't see any reason why

952
01:35:13,560 --> 01:35:19,880
what I've just described is not going to be realistic within decades, and we're going to get

953
01:35:19,880 --> 01:35:23,800
there piece by piece. It's not going to happen overnight, but we will get there. I think we

954
01:35:23,800 --> 01:35:31,400
genuinely will. The future is going to be wonderful and weird. Thank you, Mike. Do we have any final

955
01:35:31,400 --> 01:35:38,120
very quick questions anywhere? We've got one just over here, I think, in the jumper on the right.

956
01:35:38,120 --> 01:35:43,400
Just in the middle here.

957
01:35:50,680 --> 01:35:57,320
Hello. Thank you. To what extent do you think human beings are very large language models and very

958
01:35:57,320 --> 01:36:08,760
large movement models? My gut feeling is we're not just large language models. I think there's

959
01:36:08,760 --> 01:36:14,200
an awful lot more. We're great apes, the result of three and a half billion years of evolution,

960
01:36:14,760 --> 01:36:20,200
and we evolved to be able to understand planet Earth, roughly speaking, at ground level where we

961
01:36:20,200 --> 01:36:24,920
are now, and to understand other great apes, societies of great apes. That's not what large

962
01:36:25,000 --> 01:36:30,120
language models do. That's fundamentally not what they do. On the other hand, I've had colleagues

963
01:36:30,120 --> 01:36:35,160
again seriously say, well, maybe we should try and construct a theory of human society, which is

964
01:36:35,160 --> 01:36:41,640
based on the idea that we are actually just trying to come out with the most plausible thing that

965
01:36:41,640 --> 01:36:48,200
comes next. It doesn't seem plausible to me, I have to say. These are just tools. They're just

966
01:36:48,200 --> 01:36:53,800
tools which are based fundamentally based on language. They're extremely powerful at what

967
01:36:53,800 --> 01:37:01,480
they do, but do they give us any deep insights into human nature or the fundamentals of

968
01:37:02,520 --> 01:37:08,840
human mental processes? Probably not. Thank you very much, Mike. That is all we have time for,

969
01:37:08,840 --> 01:37:12,920
unfortunately, today. This is the end of the Turing lecture series for this year. Please do

970
01:37:12,920 --> 01:37:17,640
follow us on social media, the website, our emailing list to find out about future Turing

971
01:37:17,640 --> 01:37:21,800
events. Of course, we do have the Christmas lecture in 10 days time as I'll back here at

972
01:37:21,800 --> 01:37:25,880
the Royal Institution. Apart from that, just one more massive round of applause, please, for Professor

