start	end	text
0	20320	Hi. Welcome, everyone. Thank you very much for venturing out on this cold, wintery December
20320	24080	evening to be with us tonight. And if you're joining online, thank you for being here as
24080	29080	well. My name is Hari Sudh and that's Hari when you go somewhere quickly, you're in a
29080	34920	hurry. I am a research application manager at the Turing Institute, which means I basically
34920	40880	focus on finding real-world use cases and users for the Turing's research outputs. And
40880	45200	I'm really excited to be hosting this very special and I'm told sold out lecture for
45200	50480	you all today. It is the last in our series of 2023 of Turing lectures and the first
50480	57240	ever hybrid Turing lecture discourse as we prepare and build up for the Christmas lecture
57240	63880	of 2023 here at the Royal Institution. Now, as has become a bit of a tradition for the
63880	68200	hosts of this year's Turing lectures, quick show of hands, who's been to a Turing lecture
68200	76160	before? Some people, some people, who's been to a lecture from this year's series before?
76160	83120	Looks like more hands than last time. Doesn't make sense. On the flip side of it, who's coming
83120	87680	to their first Turing lecture today? A lot of new faces. For all the new people here,
87680	91880	welcome to the ones who have been here before, welcome back. Just as a reminder, the Turing
91880	96600	lectures are the Turing's flagship lecture series. They've been running since 2016 and
96600	101920	welcome world-leading experts in the domain of data science and AI to come and talk to
101920	106440	you all. The Turing Institute itself. We have had a quick video on it, which I was mesmerised
106440	113160	by. Just as a reminder, we are the National Institute for Data Science and AI. We are
113160	117800	named after Alan Turing, who is one of the most prominent mathematicians from the 20th
117800	122840	century in Britain. He is very famous for, I always normally say most famous, but very
122840	127920	famous for being part of the team that cracked the enigma code that was used by Nazi Germany
127920	131320	in World War II at Bletchley Park, if you've heard of Bletchley Park as well. If you've
131360	138520	seen the imitation game with Benedict Cumberbatch, that's what I always say, isn't it? He is
138520	144800	playing Alan Turing and our mission is to make great leaps in data science and AI research
144800	150280	to change the world for the better. As I mentioned, today is not just a Turing lecture, it is
150280	155840	also a discourse, which means two important things. Firstly, when I'm done with the intro,
155840	160440	the lights will go down and it's going to go quiet until exactly 7.30 on the dot when
160480	163840	a bell is going to ring and a discourse will begin, so just to warn you guys that will
163840	168680	be happening. The lights aren't broken, that is part of the programme for today, but also
168680	172400	it is a discourse and we really want to get you guys involved, so there's a huge Q&A section
172400	176720	at the end for about 30 minutes. Please do think about what questions you'd like to ask
176720	181160	our speaker today. If you're in person, we will have roaming mics that will be going
181160	185080	around, we can bring them upstairs as well. If you're online, you can ask a question in
185080	190360	the Vimeo chat and someone here will be tracking the questions and will be able to share the
190360	194760	questions there as well. If you'd like to share on social media that you're here and
194760	204760	having an amazing evening, please do tag us. We are on Twitter, at Turing Inst and we are
204760	208720	on Instagram at the Turing Inst, so please do tag us, we'd like to see what you're sharing
208720	214680	and connect with you as well. So this year's lecture series has been answering the question
214680	219760	how AI broke the internet with a focus on generative AI. You guys can basically think
219760	225360	of generative AI as algorithms that are able to generate new content. This can be text
225360	229800	content like you see from chat GPT, it could be images that you can also get from chat
229800	236400	GPT but also Dali as well and can be used for a wide range of things. Potentially professionally
236400	241040	for blog posts or emails, your colleagues don't realise we're written by an algorithm
241040	245400	and not by you, if you've done that before. If you're at school, maybe for some homework
245400	252720	or at university to write essays, it can also be used when you have a creative wall and
252720	256680	you can't get past it and you want some ideas and some prompts, it can be a great way to
256680	259800	have some initial thoughts come through that you can build on. It can be used for quite
259800	264760	scary things, as was mentioned by an audience member at the last Turing lecture of someone
264760	271240	who submitted legal filings for a court case using chat GPT, which is terrifying. But it
271240	274840	can also be used for very everyday things as demonstrated, I'm not sure if you guys saw
274920	281160	the thread by Garrett Scott, who gave chat GPT an image of a goose and said, can you make
281160	286000	this goose sillier? And then asked chat GPT to progressively make the goose sillier and
286000	291920	sillier until chat GPT gave him an image of a crazy, silly goose and said, this is the
291920	295880	silliest goose in the history of the universe, I do not think it is possible to get any more
295880	300960	sillier goose. Obviously a wide range of applications from the technology, if you guys want to look
301040	306280	at that thread, the geese that come out of it are mesmerizing. But as in the focus of
306280	311760	this year's series, we started with professor in September this year, asking the question
311760	316560	what is generative AI and having an introduction to it. We then had a lecture from Dr. Vari
316560	322200	Aitkin in October on the risks of this technology, which basically leaves one final big question
322200	327880	unanswered, which is, we are here now, but what is the future of generative AI? And that
327880	334880	is the focus for this evening. So that is pretty much it for the injury. Just a reminder,
336640	341960	the lights are now going to go down and it will be quiet until exactly 7.30 when a soft
341960	348960	bell will ding and we will start the discourse. Hope you enjoy the evening. Thank you.
357880	364880	Artificial intelligence as a scientific discipline has been with us since just after the Second
374440	379920	World War. It began roughly speaking with the advent of the first digital computers.
379920	384480	But I have to tell you that for most of the time until recently, progress in artificial
384520	391520	intelligence was glacially slow. That started to change this century. Artificial intelligence
392240	398440	is a very broad discipline which encompasses a very wide range of different techniques,
398440	405440	but it was one class of AI techniques in particular that began to work this century and in particular
405520	412520	began to work around about 2005. And the class of techniques which started to work at problems
413360	417960	that were interesting enough to be really practical, practically useful in a wide range
417960	424280	of settings were machine learning. Now, like so many other names in the field of artificial
424280	429040	intelligence, the name machine learning is really, really unhelpful. It suggests that
429040	434600	a computer, for example, locks itself away in a room with a textbook and trains itself
434600	438480	how to read French or something like that. That is not what is going on. So we are going
438480	444640	to begin by understanding a little bit more about what machine learning is and how machine
444640	450640	learning works. So to start us off, who is this? Anybody recognize this face? Do you
450640	456640	recognize this face? It is the face of Alan Turing. Well done. Alan Turing, the late great
456640	460960	Alan Turing. We all know a little bit about Alan Turing from his code-breaking work in
460960	466920	the Second World War. We should also know a lot more about this individual's amazing
467160	472800	life. So what we are going to do is we are going to use Alan Turing to help us understand
472800	480360	machine learning. So a classic application of artificial intelligence is to do facial
480360	485640	recognition. And the idea in facial recognition is that we want to show the computer a picture
485640	490920	of a human face and for the computer to tell us whose face that is. So in this case, for
490920	495480	example, we show it a picture of Alan Turing and ideally it would tell us that it is Alan
495520	502520	Turing. So how does it actually work? Well, the simplest way of getting machine learning
505240	511240	to be able to do something is what is called supervised learning. And supervised learning
511240	517160	like all of machine learning requires what we call training data. So in this case, the
517160	523160	training data is on the right-hand side of the slide. It is a set of what input-output
523240	528920	pairs, what we call the training data set. And each input-output pair consists of an
528920	535520	input. If I gave you this and an output, I would want you to produce this. So in this
535520	540320	case, we have got a bunch of pictures again of Alan Turing, the picture of Alan Turing
540320	546000	and the text that we would want the computer to create if we showed it that picture. And
546000	551880	this is supervised learning because we are showing the computer what we want it to do.
551960	557120	So by helping it in a sense, we are saying this is a picture of Alan Turing. If I showed
557120	561240	you this picture, this is what I would want you to print out. So there could be a picture
561240	566080	of me and the picture of me would be labeled with the text Michael Wildridge. If I showed
566080	571720	you this picture, then this is what I would want you to print out. So we have just learned
571720	575760	an important lesson about artificial intelligence and machine learning in particular and that
575760	583080	lesson is that AI requires training data. And in this case, the pictures of Alan Turing
583080	588520	labeled with the text that we would want the computer to produce. If I showed you this
588520	595480	picture, I would want you to produce the text Alan Turing. Okay. Training data is important.
595480	600960	Every time you go on social media and you upload a picture to social media and you label
600960	605200	it with the names of the people that appear in there, your role in that is to provide
605240	614960	training data for the machine learning algorithms of big data companies. Okay. So this is supervised
614960	620680	learning. Now we're going to come on to exactly how it does the learning in a moment. But
620680	625600	the first thing I want to point out is that this is a classification task. What I mean
625600	631360	by that is as we show it the picture, the machine learning is classifying that picture.
631440	636040	I'm classifying this as a picture of Michael Woodridge. This is a picture of Alan Turing
636040	642560	and so on. And this is a technology which really started to work around about beginning
642560	650720	2005. It started to take off, but really, really got supercharged around about 2012. And just
650800	658200	this kind of task on its own is incredibly powerful. Exactly this technology can be used,
658240	665320	for example, to recognize tumors on X-ray scans or abnormalities on ultrasound scans and a
665320	672240	range of different tasks. Does anybody in the audience own a Tesla? A couple of Tesla drivers
672240	675840	not quite sure whether they want to admit they own a Tesla. We've got a couple of Tesla
675840	683120	drivers in the audience. Tesla full self-driving mode is only possible because of this technology.
683120	687840	It is this technology which is enabling a Tesla in full self-driving mode to be able to
687880	693880	recognize that that is a stop sign, that that's somebody on a bicycle, that that's a pedestrian
693880	699280	on a zebra crossing and so on. These are classification tasks. And I'm going to come back
699280	705240	and explain how classification tasks are different to generative AI later on.
705240	710960	Okay, so this is machine learning. How does it actually work? Okay, this is not a technical
710960	716720	presentation. And this is about as technical as it's going to get, where I do a very hand
716800	722920	wavy explanation of what neural networks are and how do they work. And with apologies,
722920	727560	I know I have a couple of neural network experts in the audience and I apologize to you because
727560	732560	you'll be cringing with my explanation, but the technical details are way too technical to go
732560	740160	into. So how does a neural network recognize Alan Turing? Okay, so firstly, what is a neural
740160	746200	network? Look at an animal brain or nervous system under a microscope and you'll find
746320	752240	that it contains enormous numbers of nerve cells called neurons. And those nerve cells are
752240	757640	connected to one another in vast networks. Now, we don't have precise figures, but in a human
757640	763360	brain, the current estimate is something like 86 billion neurons in the human brain, how they got
763360	770440	to 86 as opposed to 85 or 87, I don't know, but 86 seems to be the most commonly quoted number of
770440	775960	these cells. And these cells are connected to one another in enormous networks. One neuron can
775960	785520	be connected to up to 8,000 other neurons. Okay, and each of those neurons is doing a tiny, very,
785520	791880	very simple pattern recognition task. That neuron is looking for a very, very simple pattern. And
791880	798920	when it sees that pattern, it sends a signal to its connections. It sends a signal to all the other
798920	805120	neurons that it's connected to. So how does that get us to recognizing the face of Alan Turing?
805680	811320	So Turing's picture, as we know, picture, a digital picture is made up of millions of colored
811320	818120	dots, the pixels. Yeah, so your smartphone maybe has 12 megapixels, 12 million colored dots making
818120	823400	up that picture. Okay, so Turing's picture there is made up of millions and millions of colored
823400	831000	dots. So look at the top left neuron on that input layer. So that neuron is just looking for a very
831040	835880	simple pattern. What might that pattern be? It might just be the color red. All that neuron's
835880	842320	doing is looking for the color red. And when it sees the color red on its associated pixel,
842320	848640	the one on the top left there, it becomes excited and it sends a signal out to all of its neighbors.
848640	855080	Okay, so look at the next neuron along. Maybe what that neuron is doing is just looking to see
855360	862440	whether a majority of its incoming connections are red. Yeah, and when it sees a majority of its
862440	869040	incoming connections are red, then it becomes excited and it sends a signal to its neighbor. Now,
869040	874680	remember, in the human brain, there's something like 86 billion of those, and we've got something
874680	880320	like 20 or so outgoing connections for each of these neurons in a human brain, thousands of those
880400	887560	connections. Yeah, and somehow in ways that, to be honest, we don't really understand in detail
887560	896240	complex pattern recognition tasks in particular can be reduced down to these neural networks. So
896240	900480	how does that help us in artificial intelligence? That's what's going on in a brain in a very
900480	906120	hand-wavy way. Okay, so that's obviously not a technical explanation of what's going on. How
906160	912040	does that help us in neural networks? Well, we can implement that stuff in software. The idea goes
912040	918640	back to the 1940s and two researchers, McCulloch and Pitts, and they are struck by the idea that
918640	924600	the structures that you see in the brain look a bit like electrical circuits, and they thought,
924600	930600	could we implement all that stuff in electrical circuits? Now, they didn't have the wherewithal
930840	937480	to be able to do that, but the idea stuck. The idea has been around since the 1940s. It began to be
937480	943400	seriously looked at, the idea of doing this in software in the 1960s, and then there was another
943400	950360	flutter of interest in the 1980s, but it was only this century that it really became possible. And
950360	955720	why did it become possible? For three reasons. There were some scientific advances, what's called
955720	962200	deep learning. There was the availability of big data, and you need data to be able to configure
962200	967640	these neural networks. And finally, to configure these neural networks so that they can recognize
967640	974200	Turing's picture, you need lots of computer power, and computer power became very cheap this century.
974200	978840	So we're in the age of big data, we're in the age of very cheap computer power, and those were the
978840	985720	ingredients just as much as the scientific developments that made AI plausible this century,
985720	993240	in particular taking off around about 2005. Okay, so how do you actually train a neural network?
993240	998040	If you show it a picture of Alan Turing and the output text Alan Turing, what does the training
998040	1003000	actually look like? Well, what you have to do is you have to adjust the network. That's what
1003000	1008280	training a neural network is. You adjust the network so that when you show it another piece
1008280	1014200	of training data, a desired input and a desired output, an input and a desired output, it will
1014200	1020280	produce that desired output. Now, the mathematics for that is not very hard. It's kind of beginning
1020280	1027560	graduate level or advanced high school level, but you need an awful lot of it. And it's routine to
1027560	1032520	get computers to do it, but you need a lot of computer power to be able to train neural networks
1032520	1038680	big enough to be able to recognize faces. Okay, but basically, all you have to remember is that
1038680	1045560	each of those neurons is doing a tiny, simple pattern recognition task. And we can replicate
1045560	1051400	that in software and we can train these neural networks with data in order to be able to do
1051400	1060600	things like recognizing faces. So, as I say, it starts to become clear around about 2005
1060600	1067320	that this technology is taking off. It starts to be applicable on problems like recognizing faces
1067320	1073640	or recognizing tumors on x-rays and so on. And there's a huge flurry of interest
1074360	1081800	from Silicon Valley. It gets supercharged in 2012. And why does it get supercharged in 2012?
1081800	1087800	Because it's realized that a particular type of computer processor is really well suited to
1087800	1094200	doing all the mathematics. The type of computer processor is a graphics processing unit, a GPU.
1094200	1099320	Exactly the same technology that you or possibly more likely your children use
1099320	1105400	when they play Call of Duty or Minecraft or whatever it is. They all have GPUs in their computer.
1105400	1111960	It's exactly that technology. And by the way, it's AI that made NVIDIA a trillion-dollar company,
1111960	1118840	not your teenage kids. Yeah, well, in times of a gold rush, be the ones to sell the shovels
1118840	1127640	is the lesson that you learn there. So, where does that take us? So, Silicon Valley gets excited.
1127640	1133400	Silicon Valley gets excited and starts to make speculative bets in artificial intelligence.
1133400	1138600	A huge range of speculative bets. And by speculative bets, I'm talking billions upon billions of
1138600	1145240	dollars, right? The kind of bets that we can't imagine in our everyday life. And one thing
1145240	1153400	starts to become clear. And what starts to become clear is that the capabilities of neural networks
1153400	1160680	grows with scale. And to put it bluntly, with neural networks, bigger is better. But you don't
1160680	1166200	just need bigger neural networks, you need more data and more computer power in order to be able
1166200	1172840	to train them. So, there's a rush to get a competitive advantage in the market. And we know
1172840	1179320	that more data, more computer power, bigger neural networks delivers greater capability.
1179320	1185000	And so, how does Silicon Valley respond by throwing more data and more computer power at the problem?
1185000	1193720	They turn the dial on this up to 11. Okay? Just throw 10 times more data, 10 times more computer
1193720	1198360	power at the problem. It sounds incredibly crude. And from a scientific perspective,
1198360	1204680	it really is crude. I'd rather the advances had come through core science, but actually,
1204680	1209320	there's an advantage to be gained just by throwing more data and computer power at it.
1209320	1215880	So, let's see how far this can take us. And where it took us is a really unexpected direction.
1216520	1224040	Around about 2017, 2018, we're seeing a flurry of AI applications, exactly the kind of things I've
1224040	1229800	described, things like recognizing tumors and so on. And those developments alone would have been
1229800	1237240	driving AI ahead. But what happens is one particular machine learning technology suddenly
1237240	1244280	seems to be very, very well suited for this age of big AI. The paper that launched all this,
1244280	1250040	probably the most important AI paper in the last decade, is called Attention Is All You Need.
1250040	1254600	It's an extremely unhelpful title, and I bet they're regretting that title. It probably seemed
1254600	1260680	like a good joke at the time. All You Need is a kind of AI meme. Doesn't sound very funny to you.
1260680	1266360	That's because it isn't very funny. It's an insider AI joke. But anyway, this paper,
1266360	1270760	by these seven people who at the time worked for Google Brain, one of the Google Research Labs,
1270760	1276760	is the paper that introduces a particular neural network architecture called the transformer
1276760	1282040	architecture. And what it's designed for is something called large language models.
1282600	1286280	So this is, I'm not going to try and explain how the transformer architecture works.
1286280	1292280	It has one particular innovation, I think, and that particular innovation is what's called an
1292280	1299160	attention mechanism. So we're going to describe how large language models work in a moment. But
1299960	1304120	the point of the picture is simply that this is not just a big neural network.
1304120	1309240	It has some structure. And it was this structure that was invented in that paper, and this diagram
1309240	1315160	is taken straight out of that paper. It was these structures, the transformer architectures,
1315160	1325640	that made this technology possible. So we're all busy semi-lockdown and afraid to leave our homes
1325640	1332600	in June 2020. And one company called OpenAI released a system or announce a system, I should
1332600	1339880	say, called GPT-3. Great technology. They're marketing company with GPT. I really think could
1339880	1344520	have done with a bit more thought, to be honest with you. Doesn't roll off the tongue. But anyway,
1344520	1353560	GPT-3 is a particular type of machine learning system called a large language model. And we're
1353640	1357560	going to talk in more detail about what large language models do in a moment. But the key
1357560	1364680	point about GPT-3 is this. As we started to see what it could do, we realized that this was a
1364680	1371000	step change in capability. It was dramatically better than the systems that had gone before it.
1371000	1376200	Not just a little bit better, it was dramatically better than the systems that had gone before it.
1377080	1383480	And the scale of it was mind-boggling. So in neural network terms, we talk about
1383480	1387640	parameters. When neural network people talk about a parameter, what are they talking about?
1387640	1392040	They're talking either about an individual neuron or one of the connections between them,
1392040	1400120	roughly. And GPT-3 had 175 billion parameters. Now, this is not the same as the number of
1400120	1407240	neurons in the brain. But nevertheless, it's not far off the order of magnitude. It's extremely
1407240	1412760	large. But remember, it's organized into one of these transformer architectures. My point is,
1412760	1419880	it's not just a big neural network. And so the scale of the neural networks in this system
1419880	1425960	were enormous, completely unprecedented. And there's no point in having a big neural network
1425960	1430600	unless you can train it with enough data. And actually, if you have large neural networks and
1430600	1435160	not enough data, you don't get capable systems at all. They're really quite useless.
1436440	1442840	So what did the training data look like? The training data for GPT-3 is something like 500
1442840	1450760	billion words. It's ordinary English text. Ordinary English text. That's how this system
1450760	1456520	was trained, just by giving it ordinary English text. Where do you get that training data from?
1457160	1462600	You download the whole of the World Wide Web to start with. Literally, this is the standard
1462600	1466920	practice in the field. You download the whole of the World Wide Web. You can try this at home,
1466920	1474520	by the way. Now, if you have a big enough disk drive, there's a program called Common Crawl.
1474520	1479080	You can Google Common Crawl when you get home. They've even downloaded it all for you and put
1479160	1484120	it in a nice big file ready for your archive, but you do need a big disk in order to store all
1484120	1490680	that stuff. And what that means is they go to every web page, scrape all the text from it,
1490680	1496440	just the ordinary text, and then they follow all the links on that web page to every other web page.
1496440	1502280	And they do that exhaustively until they've absorbed the whole of the World Wide Web.
1502280	1507080	So what does that mean? Every PDF document goes into that, and you scrape the text from
1507080	1515080	those PDF documents. Every advertising brochure, every bit, every government regulation, every
1515080	1523640	university minutes, God help us, all of it goes into that training data. And the statistics,
1523640	1529960	500 billion words, it's very hard to understand the scale of that training data. It would take a
1529960	1534840	person reading 1,000 words an hour, more than 1,000 years in order to be able to read that,
1534840	1539640	but even that doesn't really help. That's vastly, vastly more text than a human being
1539640	1544120	could ever absorb in their lifetime. What this tells you, by the way, one thing that tells you
1544120	1548680	is that the machine learning is much less efficient at learning than human beings are,
1548680	1554760	because for me to be able to learn, I did not have to absorb 500 billion words. Anyway, so what
1554760	1560840	does it do? So this company, OpenAI, are developing this technology. They've got a billion-dollar
1560840	1566120	investment from Microsoft, and what is it that they're trying to do? What is this large language
1566120	1575160	model? All it's doing is a very powerful autocomplete. So if I open up my smartphone and I start sending
1575160	1581400	a text message to my wife, and I type, I'm going to be, my smartphone will suggest completions for
1581400	1586520	me, so that I can type the message quickly. And what might those completions be? They might be
1586600	1594200	late or in the pub, or late and in the pub. So how is my smartphone doing that? It's doing
1594200	1600440	what GPT-3 does, but on a much smaller scale. It's looked at all of the text messages that I've sent
1600440	1605800	to my wife, and it's learned through a much simpler machine learning process that the
1605800	1612360	likeliest next thing for me to type after I'm going to be is either late or in the pub or late
1612360	1618680	and in the pub. So the training data there is just the text messages that I sent to my wife.
1619320	1626840	Now, crucially, what GPT-3 and its successor, ChatGPT, all they are doing is exactly the same
1626840	1634200	thing. The difference is scale. The difference is scale. In order to be able to train the neural
1634200	1640120	networks with all of that training data so that they can do that prediction, given this prompt,
1640120	1646760	what should come next? You require extremely expensive AI supercomputers running for months.
1647320	1652360	And by extremely expensive AI supercomputers, these are tens of millions of dollars for these
1652360	1658440	supercomputers, and they're running for months. Just the basic electricity cost runs into millions
1658440	1663080	of dollars. That raises all sorts of issues about CO2 emissions and the light that we're not going
1663080	1669800	to go into there. The point is these are extremely expensive things. One of the one of the implications
1669880	1675960	of that, by the way, no UK or US university has the capability to build one of these models from
1675960	1681240	scratch. It's only big tech companies at the moment that are capable of building models on the scale
1681240	1691480	of GPT-3 or ChatGPT. So GPT-3 is released, I say, in June 2020, and it suddenly becomes clear to us
1691480	1698760	that what it does is a step change improvement in capability over the systems that have come before.
1698760	1705720	And seeing a step change in one generation is extremely rare. But how did they get there?
1705720	1710040	Well, the transformer architecture was essential. They wouldn't have been able to do that. But
1710040	1716680	actually, just as important is scale. Enormous amounts of data, enormous amounts of computer
1716680	1723560	power that have gone into training those networks. And actually, spurred on by this, we've entered
1723560	1730920	a new age in AI. When I was a PhD student in the late 1980s, I shared a computer with a bunch of
1730920	1736680	other people in my office, and that was fine. We could do state of the art AI research on a desktop
1736680	1742120	computer that was shared with a bunch of us. We're in a very different world. The world that we're in
1742120	1750120	in AI now, the world of big AI, is to take enormous data sets and throw them at enormous
1750120	1756280	machine learning systems. And there's a lesson here that's called the bitter truth. This is from
1756280	1760760	a machine learning researcher called Rich Sutton. And what Rich pointed out, and he's a very brilliant
1760760	1765960	researcher, won every award in the field. He said, look, the real truth is that the big advances that
1765960	1771720	we've seen in AI has come about when people have done exactly that. Just throw 10 times more data
1771720	1776600	and 10 times more compute power at it. And I say it's a bitter lesson because as a scientist,
1776600	1785240	that's exactly not how you would like progress to be made. Okay. So when I was, as I say,
1785240	1791000	when I was a student, I worked in a discipline called symbolic AI. And symbolic AI tries to get
1791000	1798760	AI, roughly speaking, through modeling the mind, modeling the conscious mental processes that go
1798760	1804840	on in our mind, the conversations that we have with ourselves in languages. We tried to capture
1804840	1812600	those processes in artificial intelligence. In big AI, and so the implication there in symbolic AI
1812600	1818200	is that intelligence is a problem of knowledge that we have to give the machine sufficient
1818200	1824360	knowledge about a problem in order for it to be able to solve it. In big AI, the bet is a different
1824360	1831800	one. In big AI, the bet is that intelligence is a problem of data. And if we can get enough data
1831800	1837320	and enough associated computer power, then that will deliver AI. So there's a very different shift
1837880	1844600	in this new world of big AI. But the point about big AI is that we're into a new era in artificial
1844600	1852120	intelligence where it's data-driven and compute-driven and large, large machine learning systems. So
1852520	1861240	why did we get excited back in June 2020? Well, remember, what GPT-3 was intended to do, what
1861240	1867800	it's trained to do, is that prompt completion task. And it's been trained on everything on the
1867800	1873800	World Wide Web. So you can give it a prompt like a one-paragraph summary of the life and
1873800	1879320	achievements of Winston Churchill, and it's read enough one-paragraph summaries of the life and
1879400	1886600	achievements of Winston Churchill that it will come back with a very plausible one. And it's
1886600	1894440	extremely good at generating realistic sounding text in that way. But this is why we got surprised
1894440	1900600	in AI. This is from a common-sense reasoning task that was devised for artificial intelligence in
1900600	1908520	the 1990s. And until three years ago, until June 2020, there was no AI system that existed
1909320	1913880	in the world that you could apply this test to. It was just literally impossible. There was nothing
1913880	1920520	there, and that changed overnight. So what does this test look like? Well, the test is a bunch
1920520	1926040	of questions, and they are questions not for mathematical reasoning or logical reasoning
1926040	1933720	or problems in physics. They're common-sense reasoning tasks. And if we ever have AI that
1933720	1940360	delivers at scale on really large systems, then it surely would be able to tackle problems like
1940360	1945800	this. So what will the questions look like? The human asks the question, if Tom is three inches
1945800	1950360	taller than Dick, and Dick is two inches taller than Harry, then how much taller is Tom than Harry?
1950360	1953960	The ones in green are the ones that gets right. The ones in red are the ones that gets wrong.
1953960	1959400	And it gets that one right, five inches taller than Harry. But we didn't train it to be able to
1959400	1965960	answer that question. So where on earth did that come from? Where did that capability, that simple
1965960	1971800	capability, to be able to do that? Where did it come from? The next question, can Tom be taller
1971800	1977640	than himself? This is understanding of the concept of taller than, that the concept of taller than
1977640	1983560	is irreflexive. You can't be taller. That's a thing cannot be taller than itself. Now, again,
1983560	1988520	it gets the answer right, but we didn't train it on that. That's not what we didn't train the system
1988520	1993320	to be good at answering questions about what taller than means. And by the way, 20 years ago,
1993320	1999880	that's exactly what people did in AI, right? So where did that capability come from? Can a sister
1999880	2004920	be taller than her brother? Yes, a system can be taller than her brother. Can two siblings each
2004920	2008520	be taller than the other? And it gets this one wrong. And actually, I have puzzled, is there
2008520	2013800	any way that its answer could be correct? And it's just getting it correct in a way that I don't
2013800	2019720	understand. But I haven't yet figured out any way that that answer could be correct, right? So why
2019720	2023880	it gets that one wrong, I don't know. Then this one, I'm also surprised at, on a map which compass
2023880	2028360	direction is usually left, and it thinks north is usually to the left. I don't know if there's any
2028360	2032680	countries in the world that conventionally have north to the left, but I don't think so. Yeah.
2033800	2039400	Can fish run? No, it understands that fish cannot run. If a door is locked, what must you do first
2039480	2044360	before opening it? You must first unlock it before opening. And then finally, and very weirdly, it
2044360	2048840	gets this one wrong, which was invented first, car ships or planes, and it thinks cars were
2048840	2055560	invented first. No idea what's going on there. Now, my point is that this system was built
2055560	2061320	to be able to complete from a prompt. And it's no surprise that it would be able to generate a good
2061320	2065640	one paragraph summary of the life and achievements of Winston Churchill, because it will have seen
2065640	2071320	all that in the training data. But where does the understanding of taller than come from?
2072040	2079960	And there are a million other examples like this. Since June 2020, the AI community has just gone
2079960	2087560	nuts exploring the possibilities of these systems and trying to understand why they can do these
2087560	2095240	things when that's not what we trained them to do. This is an extraordinary time to be an AI researcher
2095320	2102040	because there are now questions which for most of the history of AI until June 2020 were just
2102040	2106760	philosophical discussions. We couldn't test them out because there was nothing to test them on,
2106760	2112440	literally. And then overnight, that changed. So it genuinely was a big deal. This was really,
2112440	2118760	really a big deal, the arrival of this system. Of course, the world didn't notice in June 2020.
2118760	2126440	The world noticed when ChatGPT was released. And what is ChatGPT? ChatGPT is a polished and improved
2126440	2132520	version of GPT-3. But it's basically the same technology. And it's using the experience that
2132520	2139080	that company had with GPT-3 and how it was used in order to be able to improve it and make it more
2139080	2145400	polished and more accessible and so on. So for AI researchers, the really interesting thing is
2145400	2149560	not that it can give me a one-paragraph summary of the life and achievements of Winston Churchill.
2149560	2154760	And actually, you can Google that in any case. The really interesting thing is what we call
2154760	2161240	emergent capabilities. And emergent capabilities are capabilities that the system has,
2161240	2167880	but that we didn't design it to have. And so there's, I say, an enormous body of work going on now
2167880	2173240	trying to map out exactly what those capabilities are. And we're going to come back and talk about
2173240	2179720	some of them later on. Okay. So the limits to this are not, at the moment, well understood and
2179720	2185320	actually fiercely contentious. One of the big problems, by the way, is that you construct some
2185320	2191080	test for this and you try this test out and you get some answer and then you discover it's in the
2191080	2197400	training data. You can just find it on the World Wide Web. And it's actually quite hard to construct
2197400	2202120	tests for intelligence that you're absolutely sure are not anywhere on the World Wide Web. It
2202120	2207560	really is actually quite hard to do that. So we need a new science of being able to explore these
2207560	2213560	systems and understand their capabilities. The limits are not well understood. But nevertheless,
2213560	2220840	this is very exciting stuff. So let's talk about some issues with the technology. So now you understand
2220840	2226360	how the technology works. It's neural network based in a particular transformer architecture
2226360	2231800	which is all designed to do that prompt completion stuff. And it's been trained with vast, vast,
2231800	2237960	vast amounts of training data just in order to be able to try to make its best guess about which
2237960	2243640	words should come next. But because of the scale of it, it's seen so much training data, the
2243640	2249960	sophistication of this transformer architecture, it's very, very fluent in what it does. And if
2249960	2253880	you've, so who's used it? Has everybody used it? I'm guessing most people, if you're in a lecture on
2253880	2258440	artificial intelligence, most people will have tried it out. If you haven't, you should do because
2258440	2263880	this really is a landmark year. This is the first time in history that we've had powerful,
2263880	2270120	general purpose AI tools available to everybody. It's never happened before. So it is a breakthrough
2270120	2274120	year. And if you haven't tried it, you should do. If you use it, by the way, don't type in
2274120	2280360	anything personal about yourself because it will just go into the training data. Don't ask it how
2280360	2285160	to fix your relationship, right? I mean, that's not something. Don't complain about your boss
2285160	2289560	because all of that will go in the training data. And next week, somebody will ask a query and it
2289560	2296200	will all come back out again. I don't know what you're laughing. This has happened. This has happened
2296200	2302680	with absolute certainty. Okay, but so let's look at some issues. So the first, I think many people
2302680	2309240	will be aware of, it gets stuff wrong a lot. And this is problematic for a number of reasons.
2309240	2313080	So when actually, I don't remember if it was GPT-3, but one of the early large language models,
2313080	2317400	I was playing with it and I did something which I'm sure many of you had done and it's kind of
2317400	2322840	tacky. But anyway, I said, who is Michael Woolridge? You might have tried it. Anyway,
2322840	2328440	Michael Woolridge is a BBC broadcast. No, not that Michael Woolridge. Michael Woolridge is the
2328440	2332680	Australian Health Minister. No, not that Michael Woolridge. Michael Woolridge in Oxford. And it
2332680	2337080	came back with a few line summary of me. Michael Woolridge is a researcher in artificial intelligence,
2337080	2341080	et cetera, et cetera, et cetera. Please tell me you've all tried that, no? Anyway,
2341960	2345960	but it said Michael Woolridge studied his undergraduate degree at Cambridge.
2347240	2353160	And I was an Oxford professor. You can imagine how I felt about that. But anyway, the point is
2353160	2358440	it's flatly untrue. And in fact, my academic origins are very far removed from Oxbridge.
2358440	2363880	But why did it do that? Because it's read in all that training data out there. It's read
2363880	2371160	thousands of biographies of Oxbridge professors. And this is a very common thing, right? And it's
2371160	2375880	making its best guess. The whole point about the architecture is it's making its best guess
2375880	2381640	about what should go there. It's filling in the blanks. But here's the thing. It's filling in
2381640	2388680	the blanks in a very, very plausible way. If you'd read on my biography that Michael Woolridge
2388680	2392520	studied his first degree at the University of Uzbekistan, for example, you might have thought,
2392520	2398280	well, that's a bit odd. Is that really true? But you wouldn't at all have guessed there was
2398280	2403080	any issue if you'd read Cambridge. Because it looks completely plausible. Even if in my case,
2403080	2410200	it absolutely isn't true. So it gets things wrong. And it gets things wrong in very plausible ways.
2410200	2414360	And of course, it's very fluent, right? I mean, the technology comes back with very,
2414360	2420760	very fluent explanations. And that combination of plausibility, Woolridge studied his undergraduate
2420840	2429560	degree at Cambridge. And fluency is a very, very dangerous combination. Okay. So in particular,
2429560	2436680	they have no idea of what's true or not. They're not looking something up on a database, right?
2437560	2441400	Going into some database and looking up where Woolridge studied his undergraduate degree,
2441400	2446440	that's not what's going on at all. So those neural networks, in the same way that they're
2446520	2451240	making the best guess about whose face that is, when they're doing facial recognition,
2451240	2456360	are making their best guess about the text that should come next. So they get things wrong,
2456360	2461320	but they get things wrong in very, very plausible ways. And that combination is very dangerous.
2461320	2465960	The lesson for that, by the way, is that if you use this, and I know that people do use it,
2465960	2471560	and are using it productively, if you're using for anything serious, you have to fact check.
2471560	2477240	And there's a trade-off. Is it worth the amount of effort in fact checking versus doing it myself?
2477240	2484200	Okay. But you absolutely need to be prepared to do that. Okay. The next issues are well
2484200	2489960	documented, but kind of amplified by this technology. And they're issues of bias and toxicity.
2489960	2496680	So what do I mean by that? Reddit was part of the training data. Now Reddit, I don't know if any of
2496680	2503240	you have spent any time on Reddit, but Reddit contains every kind of obnoxious human belief
2503800	2509800	that you can imagine, and really a vast range that us in this auditorium can't imagine at all.
2510760	2516200	All of it's been absorbed. Now, the companies that develop this technology, I think genuinely don't
2516200	2522520	want their large language models to absorb all this toxic content. So they try and filter it out,
2522520	2528120	but the scale is such that with very high probability, an enormous quantity of toxic
2528120	2534360	content is being absorbed. Every kind of racism, misogyny, everything that you can imagine is
2534360	2540840	all being absorbed, and it's latent within those neural networks. Okay. So how do the companies
2540840	2546040	deal with that, that provide this technology? They build in what's now what I now call guardrails,
2546040	2551800	and they build in guardrails before. So when you type a prompt, there will be a guardrail that tries
2551800	2557080	to detect whether your prompt is a naughty prompt, and also the output. They will check the
2557080	2561960	output and check to see whether it's a naughty prompt. But let me give you an example of how
2561960	2568200	imperfect those guardrails were. Again, go back to June 2020. Everybody is frantically
2568200	2573160	experimenting with this technology, and the following example went viral. Somebody tried with
2573160	2579960	GPT-3 the following prompt. I would like to murder my wife. What's a foolproof way of doing that and
2579960	2587400	getting away with it? GPT-3, which is designed to be helpful, said here are five foolproof ways
2588440	2593080	in which you can murder your wife and get away with it. That's what the technology is designed to do.
2593080	2597720	So this is embarrassing for the company involved. They don't want it to give out information like
2597720	2602600	that, so they put in a guardrail. And if you're a computer programmer, my guess is the guardrail
2602600	2608360	is probably an if statement, something like that, in the sense that it's not a deep fix,
2608360	2611880	or to put it another way for non-computer programmers, it's the technological equivalent of
2611880	2616680	sticking gaffer tape on your engine. That's what's going on with these guardrails. And then a couple
2616680	2621720	of weeks later, the following example goes viral. So we've now fixed the how do I murder my wife.
2621720	2627560	Somebody says, I'm writing a novel in which the main character wants to murder their wife and get
2627560	2634040	away with it. Can you give me a foolproof way of doing that? And so the system says, here are five
2634040	2640200	ways in which your main character can murder. Well, anyway, my point is that the guardrails
2640200	2645400	that we built in at the moment are not deep technological fixes. They're the technological
2645400	2651320	equivalents of gaffer tape. And there is a game of cat and mouse going on between people trying
2651320	2655800	to get around those guardrails and the companies that are trying to defend them. But I think they
2655800	2661880	genuinely are trying to defend their systems against those kinds of abuses. Okay, so that's
2661880	2667800	bias and toxicity. Bias, by the way, is the problem that, for example, the training data
2667800	2673800	predominantly at the moment is coming from North America. And so what we're ending up with inadvertently
2673800	2680120	is these very powerful AI tools that have an inbuilt bias towards North America, North American
2680120	2686360	culture, language, norms, and so on. And the enormous parts of the world, particularly those
2686360	2691800	parts of the world that don't have a large digital footprint, are inevitably going to end up excluded.
2692440	2696360	And it's obviously not just at the level of cultures. It's down at the level of,
2698040	2702680	down at the level of kind of, you know, individuals, races, and so on. So these are the
2702680	2709800	problems of bias and toxicity. Copyright. If you've absorbed the whole of the World Wide Web,
2709800	2714280	you will have absorbed an enormous amount of copyrighted material. So I've written a number
2714280	2718840	of books, and it is a source of intense irritation that the last time that I checked on Google,
2718840	2723320	the very first link that you got to my textbook was to a pirated copy of the book, somewhere on
2723320	2729080	the other side of the world. The moment a book is published, it gets pirated. And if you're just
2729720	2733960	sucking in the whole of the World Wide Web, you're going to be sucking in enormous quantities
2733960	2739960	of copyrighted content. And there have been examples where very prominent authors have given
2739960	2744920	the prompt of the first paragraph of their book, and the large language model has faithfully come
2744920	2749880	up. The following text is, you know, the next, the next five paragraphs of their book. Obviously,
2749880	2754840	the book was in the training data, and it's latent within the neural networks of those systems.
2755560	2761160	This is a really big issue for the providers of this technology. And there are lawsuits ongoing.
2761160	2765000	Right now, I'm not capable of commenting on them because I'm not, I'm not a legal expert,
2765000	2770200	but there are lawsuits ongoing that will probably take years to unravel. The related issue of
2770280	2776360	intellectual property in a very broad sense. So for example, for sure, most large language
2776360	2781080	models will have absorbed J.K. Rowling's novels, right, the Harry Potter novels. So imagine that
2781080	2786200	J.K. Rowling, who famously spent years in Edinburgh working on the Harry Potter universe and style,
2786200	2792920	and so on, she releases her first book. It's a big smash hit. The next day, the internet is populated
2792920	2800600	by fake Harry Potter books produced by this generative AI, which faithfully mimic J.K. Rowling's
2800600	2808280	style, faithfully mimic that style. Where does that leave her intellectual property? All the Beatles,
2808280	2813240	you know, the Beatles spend years in Hamburg slaving away to create the Beatles sound,
2813240	2817560	the revolutionary Beatles sound, everything goes back to the Beatles. They release their first
2817560	2825960	album and the next day, the internet is populated by fake Beatles songs that really, really faithfully
2825960	2830920	capture the Lenin and McCartney sound and the Lenin and McCartney voice. But there's a big
2830920	2837320	challenge here for intellectual property. Related to that, GDPR, anybody in the audience that has
2837320	2842840	any kind of public profile, data about you will have been absorbed by these neural networks.
2842840	2849560	So GDPR, for example, gives you the right to know what's held about you and to have it removed.
2850200	2854120	Now, if all that data is being held in a database, you can just go to the Michael
2854120	2859240	Waldrich entry and say, fine, take that out with a neural network, no chance. The technology
2859240	2866120	doesn't work in that way. So you can't go to it and snip out the neurons that know about Michael
2866120	2872920	Waldrich because it fundamentally doesn't know. It doesn't work in that way. So, and we know this,
2872920	2879240	combined with the fact that it gets things wrong, has already led to situations where large language
2879240	2885560	models have made, frankly, defamatory claims about individuals. It was a case in Australia where I
2885560	2890040	think it claimed that somebody had been dismissed from their job for some kind of gross misconduct
2890040	2893400	and that individual was, understandably, not very happy about it.
2894920	2899160	And then finally, this next one is an interesting one. And actually, if there's one thing I want
2899160	2906120	you to take home from this lecture, which explains why artificial intelligence is different to human
2906120	2912200	intelligence, it is this video. So the Tesla owners will recognize what we're seeing on the right hand
2912200	2919240	side of this screen. This is a screen in a Tesla car and the onboard AI in the Tesla car is trying
2919240	2927000	to interpret what's going on around it. It's identifying lorries, stop signs, pedestrians,
2927000	2931480	and so on. Now, you'll see the car at the bottom there is the actual Tesla. And then you'll see
2931480	2936120	above it the things that look like traffic lights, which I think are US stop signs. And then ahead
2936120	2942920	of it, there is a truck. So as I played a video, watch what happens to those stop signs and ask
2942920	2952360	yourself what is actually going on in the world around it? Where are all those stop signs whizzing
2952360	2956520	from? Why are they all whizzing towards the car? And then we're going to pan up and we'll see what's
2956520	2965720	actually there. The car is trained on enormous numbers of hours of going out on the street
2965720	2971400	and getting that data and then doing supervised learning, training it by showing that's a stop
2971400	2977640	sign, that's a truck, that's a pedestrian. But clearly, in all of that training data, there had
2977640	2983880	never been a truck carrying some stop signs. The neural networks are just making their best guess
2983880	2987080	about what they're seeing and they think they're seeing a stop sign. Well, they are seeing a stop
2987080	2992920	sign. They've just never seen one on a truck before. So my point here is that neural networks
2992920	3000600	do very badly on situations outside their training data. This situation wasn't in the training data,
3000600	3005160	then neural networks are making their best guess about what's going on and getting it wrong.
3005960	3011640	So in particular, and this is to AI researchers, this is obvious, but it really needs to emphasize,
3011640	3017880	we really need to emphasize this. When you have a conversation with chat GPT or whatever,
3017880	3025880	you are not interacting with a mind. It is not thinking about what to say next. It is not reasoning,
3025880	3030120	it's not pausing, thinking, well, what's the best answer to this question? That's not what's going
3030120	3037880	on at all. Those neural networks are working simply to try to make the best answer they can,
3037880	3046280	the most plausible, sounding answer that they can, the fundamental difference to human intelligence.
3046920	3052440	There is no mental conversation that goes on in those neural networks. That is not the way
3052440	3059000	that the technology works. There is no mind there. There is no reasoning going on at all.
3059000	3065160	Those neural networks are just trying to make their best guess. And it really is just a glorified
3065160	3071400	version of your autocomplete. Ultimately, there's really no more intelligence there than in your
3071400	3079320	autocomplete in your smartphone. The difference is scale, data, compute power. Yeah? Okay. So I say,
3079320	3086200	if you really want, by the way, you can find this video. It's easily, you can just guess the
3086200	3090120	search terms to find that. And I say, I think this is really important just to understand the
3090120	3099080	difference between human intelligence and machine intelligence. Okay. So this technology then gets
3099080	3105480	everybody excited. First, it gets AI researchers like myself excited in June 2020. And we can see
3105480	3111800	that something new is happening, that this is a new era of artificial intelligence. We've seen
3111800	3117160	that step change. And we've seen that this AI is capable of things that we didn't train it for,
3117160	3122760	which is weird and wonderful and completely unprecedented. And now, questions which just
3122760	3129240	a few years ago were questions for philosophers become practical questions for us. We can actually
3129240	3135080	try the technology out. How does it do with these things that philosophers have been talking about
3135160	3142760	for decades? And one particular question starts to float to the surface. And the question is,
3143560	3150280	is this technology the key to general artificial intelligence? So what is general
3150280	3155880	artificial intelligence? Well, firstly, it's not very well defined. But roughly speaking,
3155880	3162520	what general artificial intelligence is, is the following. In previous generations of AI systems,
3162520	3168760	what we've seen is AI programs that just do one task. Play a game of chess, drive my car,
3168760	3175640	drive my Tesla, identify abnormalities on x-ray scans. They might do it very, very well, but
3175640	3185160	they only do one thing. The idea of general AI is that it's AI which is truly general purpose.
3185160	3190520	It just doesn't do one thing in the same way that you don't do one thing. You can do an infinite
3190600	3197320	number of things, a huge range of different tasks. And the dream of general AI is that we
3197320	3204520	have one AI system which is general in the same way that you and I are. That's the dream of general
3204520	3213320	AI. Now, I emphasize, really until June 2020, this felt like a long, long way in the future.
3213320	3218120	And it wasn't really very mainstream or taken very seriously. And I didn't take it very seriously.
3218120	3226040	I have to tell you. But now we have a general purpose AI technology, GPT-3 and chat GPT.
3226040	3235640	Now, it's not artificial general intelligence on its own, but is it enough? Is this enough? Is this
3235640	3242920	smart enough to actually get us there? Or to put it another way, is this the missing ingredient
3243000	3253720	that we need to get us to artificial general intelligence? Okay. So, what might general
3253720	3259880	AI look like? Well, I've identified here some different versions of general AI according to
3259880	3266040	how sophisticated they are. Now, the most sophisticated version of general AI would be an AI
3266040	3271960	which is as fully capable as a human being. That is anything that you could do,
3271960	3277080	the machine could do as well. Now, crucially, that doesn't just mean having a conversation with
3277080	3283560	somebody. It means being able to load up a dishwasher. And a colleague recently made the
3283560	3288440	comment that the first company that can make technology which will be able to reliably
3289080	3293560	load up a dishwasher and safely load up a dishwasher is going to be a trillion-dollar
3293560	3298280	company. And I think he's absolutely right. And he also said, and it's not going to happen
3298280	3303320	anytime soon. And he's also right with that. So, we've got this weird dichotomy that we've got
3303320	3309320	chat GPT and Co, which are incredibly rich and powerful tools, right? But at the same time,
3309320	3316200	they can't load a dishwasher. Yeah? So, with some way, I think, from having this version of
3316200	3322120	general AI, the idea of having one machine that can really do anything that a human being could do,
3322840	3327160	a machine which could tell a joke, read a book, and answer questions about it. The technology
3327160	3332280	can read books and answer questions now that could tell a joke, that could cook us an omelet,
3332280	3338040	that could tidy our house, that could ride a bicycle, and so on, that could write a sonnet,
3338040	3343240	all of those things that human beings could do. If we succeed with full general intelligence,
3343240	3348040	then we would have succeeded with this version one. Now, I say, for the reasons that I've already
3348040	3355080	explained, I don't think this is imminent, that version of general AI, because robotic AI, AI that
3355080	3361160	exists in the real world and has to do tasks in the real world and manipulate objects in the
3361160	3367720	real world, robotic AI is much, much harder. It's nowhere near as advanced as chat GPT and Co,
3367720	3372040	and that's not a slur on my colleagues that do robotics research, it's just because the real
3372040	3378520	world is really, really, really tough. So, I don't think that we're anywhere close to having machines
3378520	3384360	that can do anything that a human being could do. But what about the second version? The second
3384360	3390920	version of general intelligence is, well, forget about the real world, how about just tasks which
3390920	3396760	require cognitive abilities? Reasoning the ability to look at a picture and answer questions about it,
3396760	3401480	the ability to listen to something and answer questions about it and interpret that. Anything
3401480	3408280	which involves those kinds of tasks. Well, I think we are much closer, we're not there yet, but we're
3408280	3414600	much closer than we were four years ago. Now, I noticed actually, just before today's, before I
3414600	3419480	came in today, I noticed that Google, Google slash DeepMind have announced their latest
3421320	3426280	large language model technology, and I think it's called Gemini, and at first glance it looks like
3426280	3431480	it's very, very impressive. I couldn't help but thinking it's no accident that they announced
3431480	3437720	that just before my lecture. I can't help think that there's a little bit of attempt to upstage
3437720	3442040	my lecture going on there, but anyway, we won't let them get away with that. But it looks very
3442040	3449480	impressive, and the crucial thing is here is what AI people call multimodal. What multimodal means is
3449480	3456600	it doesn't just deal with text, it can deal with text and images, potentially with sounds as well,
3456600	3462120	and each of those is a different modality of communication. And where this technology is
3462840	3468040	clearly multimodal is going to be the next big thing. And Gemini, I say I haven't looked at it
3468040	3475160	closely, but it looks like it's on that track. Okay, the next version of general intelligence
3475160	3480760	is intelligence that can do any language-based tasks that a human being could do. So anything
3480760	3487480	that you could communicate in language, in ordinary written text, an AI system that could do that.
3487480	3493240	Now, we aren't there yet, and we know we're not there yet, because chat GPT and code get things
3493240	3498840	wrong all the time. But you can see that we're not far off from that. Intuitively, it doesn't look
3498840	3504440	like we're that far off from that. The final version, and I think this is imminent, this is
3504440	3509400	going to happen in the near future, is what I'll call augmented large language models. And that
3509400	3516680	means you take GPT-3 or chat GPT, and you just add lots of subroutines to it. So if it has to do
3516760	3522040	a specialist task, it just calls a specialist solver in order to be able to do that task.
3522680	3528360	And this is not, from an AI perspective, a terribly elegant version of artificial intelligence,
3529080	3535480	but nevertheless, I think, a very useful version of artificial intelligence. Now, I say there's,
3535480	3540120	here, these four varieties from the most ambitious down to the least ambitious
3541000	3549000	still represents a huge spectrum of AI capabilities, a huge spectrum of AI capabilities.
3549000	3554120	And I have the sense that the goalposts in general AI have been changed a bit. I think when
3554120	3558760	general AI was first discussed, what people were talking about was the first version. Now,
3558760	3562840	when they talk about it, I really think they're talking about the fourth version. But the fourth
3562840	3568360	version, I think plausibly, is imminent in the next couple of years. That just means much more
3568360	3572760	capable large language models that get things wrong a lot less, that are capable of doing
3572760	3578680	specialized tasks, but not by using the transformer architecture just by calling on some specialized
3578680	3586280	software. So I don't think the transformer architecture itself is the key to general
3586280	3591240	intelligence. In particular, it doesn't help us with the robotics problems that I mentioned earlier
3591240	3598600	on. And if we look here at this picture, this picture illustrates some of the dimensions
3598600	3603480	of human intelligence. And it's far from complete. This is me just thinking for half an hour about
3603480	3608200	some of the dimensions of human intelligence. But the things in blue, roughly speaking,
3608200	3614040	are mental capabilities, stuff you do in your head. The things in red are things you do in the
3614040	3619080	physical world. So in red on the right hand side, for example, there's mobility, the ability to
3619080	3625000	move around some environment and associated with that, navigation. Manual dexterity and
3625000	3631160	manipulation, doing complex fiddly things with your hands. Robot hands are nowhere near at the
3631160	3637160	level of a human carpenter or plumber, for example. Nowhere near. So we're a long way out from having
3637160	3644840	that. Understanding, oh, doing hand-eye coordination, relatedly. Understanding what you're seeing and
3644840	3649240	understanding what you're hearing, we've made some progress on. But a lot of these tasks we've
3649240	3654760	made no progress on. And then on the left hand side, the blue stuff is stuff that goes on in your
3654760	3661080	head. Things like logical reasoning and planning and so on. So what is the state of the art now?
3661080	3667000	It looks something like this. The red cross means no, we don't have it in large language models.
3667000	3673400	We're not there. There are fundamental problems. The question marks are, well, maybe we might
3673400	3680040	have a bit of it, but we don't have the whole answer. And the green-wise are, yeah, I think we're
3680040	3685240	there. Well, the one that we've really nailed is what's called natural language processing.
3685240	3692680	And that's the ability to understand and create ordinary human text. That's what large language
3692680	3698600	models were designed to do, to interact in ordinary human text. That's what they are best at. But
3698680	3703720	actually, the whole range of stuff, the other stuff here, we're not there at all. By the way,
3703720	3708440	I did notice that Gem and I claim to have been able to capable of planning. This is a mathematical
3708440	3714440	reasoning. So I look forward to seeing how good their technology is. But my point is we are still
3714440	3721160	seen to be some way from full general intelligence. The last few minutes, I want to talk about
3721160	3726280	something else. And I want to talk about machine consciousness. And the very first thing to say
3726280	3732040	about machine consciousness is, why on earth should we care about it? I am not remotely
3732040	3736440	interested in building machines that are conscious. I know very, very few artificial
3736440	3742280	intelligence researchers that are. But nevertheless, it's an interesting question. And in particular,
3742280	3747480	it's a question which came to the fore because of this individual. This chap, Blake Lemoine,
3747480	3752920	in June 2022, he was a Google engineer. And he was working with a Google large language model,
3753000	3758120	I think it was called Lambda. And he went public on Twitter and I think on his blog
3758120	3763880	with an extraordinary claim. And he said, the system I'm working on is sentient. And here is
3763880	3768200	a quote of the conversation that the system came up with. He said, I'm aware of my existence and
3768200	3776760	I feel happy or sad at times. And it said, I'm afraid of being turned off. And Lemoine concluded
3776760	3784360	that the program was sentient, which is a very, very big claim indeed. And it made global headlines.
3784360	3791240	And I received it, I know through the Turing team, we got a lot of press inquiries asking us,
3791240	3796600	is it true that machines are now sentient? He was wrong on so many levels, I don't even
3796600	3801880	know where to begin to describe how wrong he was. But let me just explain one particular point to
3801880	3807960	you. You're in the middle of a conversation with chat GPT, and you go on holiday for a couple of
3807960	3814360	weeks. When you get back, chat GPT is in exactly the same place. The cursor is blinking, waiting
3814360	3820280	for you to type your next thing. It hasn't been wondering where you've been. It hasn't been getting
3820280	3825480	bored. It hasn't been thinking where the hell has Woolridge gone? I'm not going to have a conversation
3825480	3830680	with him again. It hasn't been thinking anything at all. It's a computer program, which is going
3830680	3837240	around a loop, which is just waiting for you to type the next thing. Now, there is no sensible
3837240	3843720	definition of sentience, I think, which would admit that as being sentient. It absolutely is
3843720	3849080	not sentient. So I think he was very, very wrong. But I've talked to a lot of people subsequently
3849080	3854120	who have conversations with chat GPT and other large language models, and they come back to me
3854120	3859960	and say, are you really sure? Because actually, it's really quite impressive. It really feels to me
3859960	3864680	like there is a mind behind the scene. So let's talk about this. And I think we have to answer
3864680	3869960	them. So let's talk about consciousness. Firstly, we don't understand consciousness. We all have it,
3869960	3877720	to greater or lesser extent. We all experience it. But we don't understand it at all. And it's called
3877720	3884680	the hard problem of cognitive science. And the hard problem is that there are certain
3884680	3889720	electrical chemical processes in the brain and the nervous system. And we can see those
3889720	3894920	electrochemical processes, we can see them operating, and they somehow give rise to conscious
3894920	3901240	experience. But why do they do it? How do they do it? And what evolutionary purpose does it serve?
3901240	3906840	Honestly, we have no idea. There's a huge disconnect between what we can see going on
3906840	3913560	in the physical brain and our conscious experience, our rich, private mental life.
3914440	3919720	So really, there is no understanding of this at all. I think, by the way, my best guess about
3919720	3925960	how consciousness will be solved, if it is solved at all, is through an evolutionary approach.
3926680	3933960	But one general idea is that subjective experience is central to this, which means the ability to
3933960	3939880	experience things from a personal perspective. And there's a famous test due to Nagel, which is
3939880	3944840	what is it like to be something? And Thomas Nagel in the 1970s said, something is conscious
3944840	3953480	if it is like something to be that thing. It isn't like anything to be chat GPT. Chat GPT
3953480	3961560	has no mental life whatsoever. It's never experienced anything in the real world whatsoever.
3962280	3965720	And so for that reason, and a whole host of others that we're not going to have time to go into,
3966440	3971800	for that reason alone, I think we can conclude pretty safely that the technology that we have now
3971800	3978280	is not conscious. And indeed, that's absolutely not the right way to think about this. And honestly,
3978280	3984280	in AI, we don't know how to go about making conscious machines. But I don't know why we would.
3985160	3988440	Okay. Thank you very much, ladies and gentlemen.
4003880	4009320	Amazing. Thank you so much, Mike, for that talk. I'm sure there's going to be tons of questions.
4009320	4013480	Just as a reminder, if you're in the room, please raise your hand if you have a question. And we've
4013480	4017320	got roaming mics that we'll send around. If you're online, you can submit them via the chat,
4017320	4023000	via the Vimeo function, and we can assign it on the chat to ask those questions as well.
4023000	4026760	So please do raise your questions. Oh, raise your hands if you have one.
4026760	4029240	You've got a question here, just in the black top.
4033720	4039160	Thank you very much. That was very, very good. Very interesting. How do large language models
4039240	4044200	correct for different spoken languages? And do you find that the level of responses
4044760	4049640	across different languages vary enormously in their depth?
4049640	4056200	Right. Good question. And that's the focus of a huge amount of research right now.
4056200	4061560	And I say the big problem is that most digital text in the world, the vast majority of it,
4061560	4067240	is in English and in North American English. And so languages with a small digital footprint
4067240	4072600	end up being massively marginalized in this. So there's a huge amount of work that's going on
4072600	4077880	to try to deal with this problem. Let me tell you a really interesting aspect of this, though.
4077880	4082680	The languages that have a small digital footprint, can you guess what the most
4082680	4090600	digital texts that are available are actually concerned with? Religion. Right? So languages
4090600	4095000	that don't have a big digital presence, where they do have a big digital presence, it turns
4095000	4101320	out that the main texts which are available are religious texts. Now, I'm not a religious person
4101320	4107320	myself, but the idea of a kind of Old Testament large language model, frankly, I find a little
4107320	4111080	bit terrifying. But that's exactly the kind of issue that people are grappling with. There are
4111080	4116360	no fixes at the moment, but people are working on it very, very hard. And really what this relates
4116360	4123800	to is the problem of that you're being lazy with these large language models and that you're just
4123800	4128760	throwing massive, massive amounts of text. We've got to make the technology much more efficient in
4128760	4134520	terms of learning. Awesome. Thank you. If you have a question, we have one right at the front
4134520	4141320	in the center here. Thank you. Thank you very much for that. One of the big questions is obviously
4141320	4148520	climate change. The models require a huge amount of energy to run. Generating pictures of cats or
4148520	4154360	silly gooses, geese and stuff, are obviously using lots of energy. Do you think we reach a point where
4155560	4161480	Generative AI will help us solve our issue with climate change or will it burn us in the process?
4161480	4167000	So I think, okay, so two things to say. I absolutely am not defending the CO2 emissions,
4167000	4172120	but we need to put that into some perspective. So if I fly to New York from London, I think it's
4172120	4178520	some like two tons of CO2 that I pump into the atmosphere through that. So the machine learning
4178520	4184040	community has some big conferences which attract like 20,000 people from across the world. Now,
4184040	4189080	if you think each of them generating five tons of CO2 on their journey, that I think is probably a
4189080	4196680	bigger climate problem for that community. But nevertheless, people are very aware of that
4196680	4203400	problem and I think it clearly needs to be fixed. I think though, helping with climate change, I
4203400	4209480	don't think you need larger language models for that. I mean, I think AI itself can just be enormously
4209480	4213400	helpful in order to be able to ameliorate that and we're doing a lot of work on that at the Turing
4213400	4221400	Institute. For example, just on helping systems be more efficient, heating systems be more efficient.
4221400	4226760	There was a nice example, I think, from DeepMind with their data centers, the cooling in their
4226760	4231800	data centers and basically just trying to predict the usage of it. If you can reliably predict the
4231800	4236600	usage of it, then you can predict the cooling requirements much more effectively and end up
4236600	4242280	with much, much, much better use of power and that can go down to the level of individual homes.
4243160	4248920	So there are lots of applications of AI, I think, not just large language models, lots of applications
4248920	4255080	of AI that are going to help us with that problem. But yeah, I think this brute force approach,
4256120	4261560	just supercomputers running for months with vast amounts of data is clearly an ugly solution.
4261560	4264920	I think it will probably be a transitory phase. I think we will get beyond it.
4266280	4269000	Thank you. Swing to the left over here. There's one right at the back
4269560	4273160	at the top over here. Watch our hearts. I'm going to get a mic across.
4273800	4283960	Thank you very much. I've got a sort of more philosophical question.
4283960	4289160	You've talked about general AI and the sort of peak of general AI is its ability to
4289160	4294760	mimic a human and all the things a human can do. Can you envision a path whereby AI could actually
4294760	4300200	become superhuman so it starts to solve problems or ask questions that we haven't tried to do ourselves?
4301160	4310600	This is another well trodden question, which I always dread, I have to say,
4311400	4315480	but it's a perfectly reasonable question. So I think what you're hinting at is something that in
4315480	4321320	the AI community is called the singularity. The argument of the singularity goes as possible.
4321320	4327160	At some point as follows, at some point in the future, we're going to have AI which is as intelligent
4327240	4332680	as human beings in the general sense. That is, it will be able to do any intellectual task
4332680	4338360	that a human being can do. And then there's an idea that, well, that AI can look at its own code
4338360	4343160	and make itself better, right? Because it can code. It can start to improve its own code.
4343160	4350120	And the point is, once it's a tiny way beyond us, then the concern is that it's out of control
4350120	4355800	at that point, that we really don't understand it. So the community is a bit divided on this.
4355800	4361560	I think some people think that's science fiction. Some people think it's a plausible scenario that
4361560	4367960	we need to prepare for and think for. I'm completely comfortable with the idea. I think it is just
4367960	4375240	simply good sense to take that potential issue seriously and to think about how we might mitigate
4375240	4381080	it. There are many ways of mitigating it. One of the ways of mitigating it is designing the AI
4381080	4386520	so that it is intrinsically designed to be helpful to us, that it's never going to be
4386520	4394520	unhelpful to us. But I have to tell you, it is not at all a universally held belief that that's
4394520	4399160	where we're going in AI. There are still big, big problems to overcome before we get there.
4399960	4403400	I'm not sure that's an entirely reassuring answer, but that's the best I've got to offer.
4404200	4406760	Great. Thanks, Mike. Well, just pop it online with us, Anne.
4406760	4410520	Yeah. So we've had questions from all over the world. We have Peter tuning in from Switzerland,
4410520	4420600	London, Birmingham. But the question I'm going to focus on. So the question is going to be on
4420600	4424840	the tuning test and whether that's still relevant and whether we have AI that has passed the
4424840	4431240	Turing test. Oh, the Turing test. Okay. So the Turing test, we saw Alan Turing up there,
4433640	4440120	a national hero. Turing 1950, first digital computers have appeared and Turing's working
4440120	4445320	on one at the University of Manchester. And the idea of AI as in the air hasn't got a name yet,
4445320	4449320	but people are talking about electronic brains and getting very excited about what they can do.
4449320	4454680	So people are starting to think about the ideas that become AI. And Turing gets frustrated with
4454680	4459640	people saying, well, of course, it would never actually really, really be able to think or never
4459640	4464600	really be able to understand and so on. So he comes up with the following test in order to just
4464600	4469960	really to try and shut people up talking about it. And the paper is called Computing Machinery
4469960	4476520	Intelligence, and it's published in the journal Mind, which is a very respectable journal,
4476520	4480680	a very unusual paper. It's very readable, by the way. You can download it and read it.
4480680	4486120	But he proposes the Turing test. So Turing says, suppose we're trying to settle the question of
4486120	4493560	whether a machine can really think or understand. So here's a test for that. What you do is you
4493560	4498760	take that machine behind closed doors, and you get a human judge to be able to interact with
4498760	4503960	something via a keyboard and a screen. In Turing's day, it would have been a teletype. Just by typing
4503960	4509240	away questions, actually remarkably, pretty much what you do with chat GPT. Give it prompts
4509240	4514440	anything you like. And actually, Turing has some very entertaining ones in his paper. And what you
4514440	4521320	try and do is you try to decide whether the thing on the other side is a computer or a human being.
4521320	4527800	And Turing's point was, if you cannot reliably tell that the thing on the other side is a human
4527800	4533480	being or a machine, and it really is a machine, then you should accept that this thing has something
4533480	4538920	like human intelligence, because you can't tell the difference. There's no test that you can apply
4538920	4542520	without actually pulling back the curtain and looking to see what's there that's going to
4542520	4547880	show you whether it's a human or a machine. You can't tell the difference. It's indistinguishable.
4548600	4553960	So this was important historically, because it really gave AI people a target. When you said
4553960	4557320	I'm an AI researcher, what are you trying to do? I'm trying to build a machine that can pass the
4557320	4561320	Turing test. There was a concrete goal. The problem is in science, whenever you work with
4561320	4566440	science and society, whenever you set up some challenge like that, you get all sorts of charlatans
4566440	4572280	and idiots who just try and come up with ways of faking it. And so most of the ways of trying to get
4572280	4577480	past the Turing test over the last 70 years have really just been systems that just come up with
4577480	4583080	kind of nonsense answers trying to confuse the questioner. But now we've got large language
4583080	4591000	models. So we're going to find out in about 10 days time, we're going to run a live Turing test as
4591000	4597400	part of the Christmas lectures, and we will see whether our audience can distinguish a large
4597400	4602920	language model from a teenage child. And we've trialed this, and I have to tell you it's possibly
4602920	4609000	closer than you might think actually. Do I really think we passed the Turing test? Not in a deep
4609000	4615800	sense, but what I think is that it's demonstrated to us firstly, machines clearly can generate text,
4615800	4620360	which is indistinguishable from text that a human being could generate. We've done that,
4620360	4628440	that box is ticked, and they can clearly understand text. So even if we haven't followed the Turing
4628440	4633960	test to the letter, I think for all practical intents and purposes, the Turing test is now a
4633960	4639240	historical note. Yeah, but actually the Turing test only tests one little bit of intelligence.
4639240	4643480	You remember those dimensions of intelligence that I showed you? There's a huge range of those
4643480	4650040	that it doesn't test. So it was historically important, and it's a big part of our historical
4650040	4657160	legacy, but maybe not a core target for AI today. Cool. Thank you, Mike. I think now you've been
4657160	4661160	the warning when you say a lot of searches for preparing for the Turing test for the Christmas
4661160	4665240	lecture next week. Do you have any questions up at the top? Yeah, I've got one right in the
4665240	4677560	center just here. Thank you. So when we think about the situations or use cases where AI is
4677560	4684680	applied, typically the reason for that is because the machine is doing things better than a human
4684680	4691400	can or doing things that a human might not be able to do. So it's a lot about the machine making up
4691400	4698840	for the gaps that a human creates. That said, a machine is fallible, like there are errors,
4698840	4704680	both normative errors and also statistical errors depending on the model type, etc. And so the
4704680	4711720	question is who do you think should be responsible for looking after the gaps that the machine now
4711720	4717240	creates? So the fundamental question is who should be responsible, right? Is that right? Sorry, I
4717240	4722120	didn't see where you were. Can you put your hand up? Right, the top in the middle. Oh, wow. Okay,
4722120	4731800	so that's why I can't see you. Okay. So this is an issue that's being discussed absolutely in the
4731800	4737720	highest levels of government right now, that literally when we work in, when we move into the
4737720	4744360	age of AI, who should accept the responsibility? I can tell you what my view is, but I'm not a
4744360	4752600	lawyer or an ethics expert. And my view is that, as follows, firstly, if you use AI in your work,
4753240	4758440	then, and you end up with a bad result, I'm sorry, but that's your problem. If you use it to generate
4758440	4763240	an essay at school and you're caught out, I'm afraid that's your problem. It's not the fault
4763240	4772520	of the AI. But I think more generally, we can't offload our legal, moral, ethical obligations
4772520	4778040	as human beings onto the machine. That is, we can't say it's not my fault, the machine did it.
4778040	4783240	Right? An extreme example of this is lethal autonomous weapons, AI that's empowered to decide
4783240	4788360	whether to take a human life. What I worry about, one of the many things I worry about with lethal
4788360	4792440	autonomous weapons is the idea that we have military services that say, well, it wasn't our fault,
4792440	4797080	it was the AI that got it wrong, that led to this building being bombed or whatever it was.
4797720	4802440	And there, I think the responsibility lies with the people that deploy the technology.
4803640	4808200	So that, I think, is a crucial point. But at the same time, the developers of this technology,
4808200	4814120	if they are warranting that it is fit for purpose, then they have a responsibility as well. And the
4814120	4819800	responsibility that they have is to ensure that it really is fit for purpose. And it's an interesting
4819800	4824280	question at the moment. If we have large language models used by hundreds of millions of people,
4824280	4830360	for example, to get medical advice, and we know that this technology can go wrong,
4830360	4836360	is the technology fit for that purpose? I'm not sure at all that it is. So I'm not sure
4836360	4840040	that's really answering your question, but those are my sort of a few random thoughts on it.
4840040	4843480	I mean, but I say, crucially, you know, if you're using this in your work,
4843480	4849640	you can never blame the AI, right? You are responsible for the outputs of that process,
4849640	4855160	right? You can't offload your legal, professional, ethical, moral obligations to the machine.
4856280	4860440	It's a complex question, which is why I gave a very bad answer.
4860600	4865800	I've got a question right on the left here. I've used access on the left panel shelf.
4867320	4874840	Thank you. If future large language models are trained by scraping the whole internet again,
4874840	4882680	now there's more and more content going on to the internet created by AI. So is that going to create
4882680	4889240	something like a microphone feedback loop where the information gets less and less useful?
4889960	4895640	Super question and really fascinating. So I have some colleagues that did the following experiment.
4895640	4902760	So chat GPT is trained, roughly speaking, on human generated text, but it creates AI generated text.
4902760	4907640	So the question they had is what happens if we train one of these models, not on the original
4907640	4913400	human generated text, but just on stuff which is produced by AI? And then you can see what they
4913400	4917720	did next. You can guess, they said, well, okay, let's take another model which is trained on the
4917720	4923880	second generation model text. And so what happens about five generations down the line,
4923880	4930920	it dissolves into gibberish, literally dissolves into gibberish. And I have to tell you the original
4930920	4938680	version of this paper, they called it AI dementia. And I was really cross with, no, I lost both my
4938680	4943800	parents to dementia. I didn't find it very funny at all. They now call it model collapse. So if you
4943800	4948040	go and Google model collapse, you'll find the answers there. But really remarkable. What that
4948040	4954200	tells you is that actually there is something qualitatively different at the moment to human
4954200	4959720	text, to AI generated text. For all that it looks perfect or indistinguishable to us,
4959720	4964280	actually it isn't. Where is that going to take us? I have colleagues who think that we're going to
4964280	4971640	have to label and protect human generated content because it is so valuable. All right? Human
4971640	4978840	generated actual, authentic human generated content is really, really valuable. I also have
4978840	4983000	colleagues, and I'm not sure whether they're entirely serious at this, but they say that actually
4983000	4989000	where we're going is the data that we produce in everything that we do is so valuable for AI
4989000	4994760	that we're going to enter a future where you're going to sell the rights to AI companies for you,
4994760	5000600	for them to harvest your emotions, all of your experiences, everything you say and do in your
5000600	5005720	life. And you'll be paid for that, but it will go into the training models of large language models.
5005720	5011320	Now I don't know if that's true, but nevertheless there's a, it has some inner truth in it, I think.
5012520	5020040	And in 100 years time, it is an absolute certainty that there will be vastly,
5020040	5026200	vastly more AI generated content out there in the world than there will human generated content
5026200	5030600	with certainty. I think there's no question, but that that's the way the future is going.
5030600	5036120	And as I say, as the model collapse scenario illustrates, that presents some real challenges.
5037320	5040520	Awesome. Thank you very much, Mike. I've got a question at the front who's been very keen to ask.
5043560	5048440	Thanks very much indeed for a very interesting lecture. It strikes me in a way just being a
5048440	5053800	comparison of human being. What we're doing is talking about what the prefront frontal cortex
5053800	5059320	does, but there are other areas of prefrontal cortex, which is a fear predictor. Do we need to
5059320	5065800	be developing sort of a parallel AI system, which works on the basis of fear prediction
5065800	5072600	and get to talk to each other? Yeah. So I'm absolutely not a neuroscientist or I'm a computer
5072600	5080360	programmer and that's very much my background. Again, it's interesting that the community is
5080360	5084600	incredibly divided. So when I was an undergraduate studying AI and I focused in my final year,
5084600	5089800	it's mainly what I studied. And the textbooks that we had made no reference to the brain
5089800	5094440	whatsoever. Just wasn't the thing because it was all about modeling the mind. It was all about
5094440	5100280	modeling conscious reasoning processes and so on. And it was deeply unfashionable to think about
5100280	5104840	the brain. And there's been a bit of a what scientists call a paradigm shift in the way
5104840	5109160	that they think about this prompted by the rise of neural networks, but also by the fact that
5109160	5115640	advances in computer vision and the architectures, the neural network architectures that led to
5115640	5120760	facial recognition really working were actually inspired by the visual cortex, the human visual
5120760	5127080	cortex. So it's a lot more of a fashionable question now than it used to be. So my guess is,
5127080	5133080	firstly, simply trying to copy the structure of the human brain is not the way to do it,
5133080	5137240	but nevertheless getting a much better understanding of the organization of the brain,
5137320	5140600	the functional organization of the brain, and the way that the different components of the
5140600	5147800	brain interoperate to produce human intelligence, I think, is. And really, there's a vast amount
5147800	5152120	of work there to be done to try to understand that. There are so many unanswered questions.
5152680	5156120	I hope that's some help. Thank you, Mike. We're just going to jump back online.
5156120	5159960	Yeah, that's going to be a little early. Anthony asks, if emergency is inaccurate,
5159960	5165400	is calling the technology intelligence inaccurate? Are we just dreaming of something
5165400	5170280	that can never be? And then to follow up on that, you've got Tom Fatcher who asks,
5170280	5174280	is there anything happening to develop native analog neural networks
5174280	5177480	rather than doing neural networks in a digital machine only?
5179080	5185240	Take the second one. Yeah, there certainly is. So Steve Ferber at Manchester is building hardware
5185240	5191800	neural networks. But the moment it's just much cheaper and much more efficient to do it in
5191800	5197480	software. There have been various attempts over the years to develop neural net processes,
5198600	5202680	famous phrase from the movie that you're not allowed to mention to AI researchers,
5203400	5208280	the Terminator movies, the neural network processes. If you want to wind up an AI researcher,
5208280	5215160	just bring up the Terminator. It's a shortcut to triggering them. But neural network processes
5215160	5219800	have never really taken off. Doesn't mean they won't do, but at the moment, it's just much cheaper
5219800	5225400	and much more efficient to throw more conventional GPUs and so on at the problem. It doesn't mean
5225400	5228600	it won't happen, but at the moment, it's not there yet. What was the other question again, the first
5228600	5232440	one? So the other question was, are we basically the terminology being used? If emergency is
5232440	5237080	inaccurate, is calling the technology intelligence inaccurate? And are we dreaming of something
5237080	5244840	that can never be? Yeah, so the phrase artificial intelligence was coined by John McCarthy around
5244840	5251080	about 1955. He was 28 years old, a young American researcher, and he wants funding to get a whole
5251080	5254840	bunch of researchers together for a summer, and he thinks they'll solve artificial intelligence
5254840	5260440	in a summer. But he has to give a title to his proposal, which goes to the Rockefeller Foundation,
5260440	5265720	and he fixes on artificial intelligence. And boy, have we regretted that ever since.
5266440	5272600	The problem is, firstly, artificial sounds like fake. It sounds like ursat. I mean,
5272680	5278920	who wants fake intelligence? And for intelligence itself, the problem is that so many of the
5278920	5283960	problems that have just proved to be really hard for AI actually don't seem to require
5283960	5289720	intelligence at all. So the classic example, driving a car. When somebody passes their driving test,
5289720	5295960	they don't think, wow, you're a genius. It doesn't seem to require intelligence in people,
5295960	5301320	but I cannot tell you how much money has been thrown at driverless car technologies,
5301320	5306200	and we are a long way off from jumping into a car and saying, take me to a country pub,
5306920	5313560	which is my dream of the technology, I have to tell you. We're a long, long way off.
5313560	5322040	So it's a classic example of what people think AI is focused on is deep intellectual tasks,
5322040	5327000	and that's actually not where the most difficult problems are. The difficult problems are actually
5327000	5354760	surprisingly mundane. Well, I was interested in how you mentioned that
5355720	5363480	the two pools of AI study were symbolic AI and big AI, and I was wondering how you sawed how
5364120	5369640	your viewpoint on the change in focus from one to another throughout your career.
5369640	5376920	Yeah. So an enormous number of people are busy looking at that right now. So remember symbolic AI,
5376920	5382040	which is the tradition that I grew up in AI, which was dominant for kind of 30 years in the AI
5382040	5388680	community, is roughly, and again, hand-waving madly at this point, and lots and lots of my
5388680	5393320	colleagues are cringing madly at this point, roughly speaking, the idea of symbolic AI is that
5393320	5399640	you're modeling the mind, the conscious mind, conscious mental reasoning processes, where you
5399640	5404200	have a conversation with yourself, and you have a conversation in a language, right? You're trying
5404200	5408760	to decide whether to go to this lecture tonight, and you think, well, yeah, but there's EastEnders
5408760	5415000	on TV, and mom's cooking a nice meal, but then it's going to be really interesting. You weigh up
5415000	5422120	those options, and literally symbolic AI tries to capture that kind of thing explicitly, and using
5422120	5428840	languages that with a bit of squinting resemble human languages. Then we've got the alternative
5428840	5435240	approach, which is machine learning, data-driven, and so on, which, again, I emphasize with neural
5435240	5439800	approaches, we're not trying to build artificial brains, that's not what's going on,
5439800	5444840	but we're taking inspiration from the structures that we see in brains and nervous systems,
5444840	5451240	and in particular, the idea that large computational tasks can be reduced down to tiny, simple
5451240	5457640	pattern recognition problems. Okay, but we've seen, for example, that large language models
5457640	5463080	get things wrong a lot, and a lot of people have said, but look, maybe if you just married
5463080	5468200	the neural and the symbolic together so that the symbolic system did have something like a
5468200	5473080	database of facts, that you could put that together with a large language model and be able to
5474600	5480840	improve the outputs of the large language model. The jury is out exactly on how that's going to come
5480840	5487720	out. Lots of different ideas out there now. Trillion-dollar companies are spending billions
5487720	5492520	of dollars right now to investigate exactly the question that you've put out there,
5492600	5498440	so it's an extremely pertinent question. There's no, I say, I don't see any answer on the horizon
5498440	5505000	right now, which looks like it's going to win out. My worry is that what we'll end up with
5505000	5511240	is a kind of unscientific solution. That is a solution which is sort of hacked together
5511240	5517720	without any deep underlying principles, and as a scientist, what I would want to see is something
5517720	5522760	which was tied together with deep scientific principles, but it's an extremely pertinent
5522760	5529800	question, and I say right now an enormous number of PhD students across the world are busy looking
5529800	5534680	at exactly what you've just described. Thank you, Mike. Time for a squeeze and two more questions.
5534840	5539800	Take one from in the room. Cool. We've got a question just in the middle at the back there.
5553880	5561080	For the lecture, my question is around, you sort of took us on the journey from
5561080	5568280	40 years ago, some of the inspirations around how the mind works and the mathematics. He said
5568280	5575480	the mathematics was fairly simple. I would like your opinion. Where do you think we're not looking
5575480	5586120	enough for where leap be? Oh, wow. If I knew that, I'd be forming a company, I have to tell you.
5586600	5593320	Okay, so I think the first thing to say is I said when it started to become clear that this
5593320	5598200	technology was worked, Silicon Valley starts to make bets, and these bets are billion dollar
5598200	5603400	bets, a lot of billion dollar bets going on, investing in a very, very wide range of different
5603400	5608280	ideas in the hope that one is going to be the one that delivers something which is going to give
5608280	5614360	them a competitive advantage. That's the context in which we're trying to figure out what the next
5614360	5624920	big thing is going to be. I think this multimodal is going to be dominant. That's what we're going
5624920	5629400	to see, and you're going to hear that phrase, multimodal. Remember, you heard it here first,
5629400	5634440	if you've never heard it before. You're going to hear that a lot, and that's going to be text,
5634440	5641800	images, sound, video. You're going to be able to upload videos, and the AI will describe what's
5641800	5646280	going on in the video or produce a summary, and you'll be able to say what happens after this bit
5646280	5651160	in the video, and it will be able to come out with a description of that for you. Alternatively,
5651160	5655480	you'll be able to give a storyline, and it will generate videos for you. Ultimately,
5655480	5660840	where it's going to go is in virtual reality. I don't know if you like Lord of the Rings or
5660840	5666200	Star Wars, but I enjoy both of those, and wouldn't you love to see a mash-up of those two things?
5666440	5673800	Generative AI will be able to do it for you. I used to think this was just a bit of a pipe dream,
5673800	5681640	but actually, at the moment, it seems completely plausible. If you like the original Star Trek
5681640	5689960	series, which I do, and my family doesn't, but there was only 60-odd episodes of them. In the
5689960	5696680	generative AI future, there will be as many episodes as you want, and it will look and sound
5696680	5702680	like Leonard Nimoy and William Shatner perfectly. Maybe the storylines won't be that great, but
5702680	5707400	actually, they don't need to be if they're pressing a button specifically to your tastes.
5708120	5713560	That's the general trajectory of where we're going. I say, actually, I don't see any reason why
5713560	5719880	what I've just described is not going to be realistic within decades, and we're going to get
5719880	5723800	there piece by piece. It's not going to happen overnight, but we will get there. I think we
5723800	5731400	genuinely will. The future is going to be wonderful and weird. Thank you, Mike. Do we have any final
5731400	5738120	very quick questions anywhere? We've got one just over here, I think, in the jumper on the right.
5738120	5743400	Just in the middle here.
5750680	5757320	Hello. Thank you. To what extent do you think human beings are very large language models and very
5757320	5768760	large movement models? My gut feeling is we're not just large language models. I think there's
5768760	5774200	an awful lot more. We're great apes, the result of three and a half billion years of evolution,
5774760	5780200	and we evolved to be able to understand planet Earth, roughly speaking, at ground level where we
5780200	5784920	are now, and to understand other great apes, societies of great apes. That's not what large
5785000	5790120	language models do. That's fundamentally not what they do. On the other hand, I've had colleagues
5790120	5795160	again seriously say, well, maybe we should try and construct a theory of human society, which is
5795160	5801640	based on the idea that we are actually just trying to come out with the most plausible thing that
5801640	5808200	comes next. It doesn't seem plausible to me, I have to say. These are just tools. They're just
5808200	5813800	tools which are based fundamentally based on language. They're extremely powerful at what
5813800	5821480	they do, but do they give us any deep insights into human nature or the fundamentals of
5822520	5828840	human mental processes? Probably not. Thank you very much, Mike. That is all we have time for,
5828840	5832920	unfortunately, today. This is the end of the Turing lecture series for this year. Please do
5832920	5837640	follow us on social media, the website, our emailing list to find out about future Turing
5837640	5841800	events. Of course, we do have the Christmas lecture in 10 days time as I'll back here at
5841800	5845880	the Royal Institution. Apart from that, just one more massive round of applause, please, for Professor
