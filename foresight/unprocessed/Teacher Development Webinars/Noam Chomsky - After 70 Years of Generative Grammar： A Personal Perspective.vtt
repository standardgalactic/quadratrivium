WEBVTT

00:00.000 --> 00:07.000
Yes, you can start.

00:07.000 --> 00:10.000
What did you say?

00:10.000 --> 00:16.000
Good to go.

00:16.000 --> 00:27.000
Hello, everyone. Welcome to our session today. This is a very special session because of our guest, but also because TESOL Tampico Talks is collaborating

00:27.000 --> 00:37.000
with teacher development webinars. My friend Amunalai is here. My name is Jorge Torsal Masan, and we want to welcome you to this unique session.

00:37.000 --> 00:46.000
Amunalai is going to introduce himself, and he's also going to introduce our special guest. I mean, he needs no introduction, but he's going to go for the protocol.

00:46.000 --> 00:51.000
Amunalai, over to you.

00:51.000 --> 01:02.000
Bismillah ar-Rahman ar-Rahim. As-salamu alaikum. My name is Zaman al-Dawthand, and I just asked, you know, with the couplet of Shah Latif, the famous poet of Sindh.

01:02.000 --> 01:03.000
He says,

01:03.000 --> 01:30.000
It translates as those worshippers who search the ocean and find different jewels from the depths. Latif says they are the ones who have found profound ones.

01:30.000 --> 01:41.000
So it's my singular honor, a player, and a privilege to introduce Professor Noam Chomsky, considered the founder of a modern linguistics.

01:41.000 --> 01:50.000
Noam Chomsky is one of the most cited scholars in modern history. Among his groundbreaking books are Syndicate Structures, Language in Mind,

01:51.000 --> 02:04.000
aspects of the theory of syntax and the minimalist program, each of which has met distinct contributions to the development of the field. He has received numerous evas, including the Kyoto Prize in basic sciences, the

02:04.000 --> 02:19.000
Helmholtz Medal and the Ben Franklin Medal in Computer and Cognitive Science. Chomsky introduced the Chomsky Hierarchy Generative Grammar and the concept of a universal grammar which underlies all human speech and is best in the innate

02:19.000 --> 02:33.000
structure of the mind brain. Chomsky has not only transformed the field of linguistics, his work has influenced a field such as cognitive science, philosophy, psychology, computer science, mathematics,

02:34.000 --> 02:56.000
childhood education, and anthropology. Professor Chomsky is one of the most influential public intellectuals in the world. He has written more than 100 books and his website is www.chomsky.com. Certainly right said by the New York Times magazine, the most important intellectual alive.

02:56.000 --> 03:02.000
So ladies and gentlemen, Professor Noam Chomsky for you.

03:04.000 --> 03:08.000
Over to you sir.

03:08.000 --> 03:37.000
Over to you sir.

03:37.000 --> 03:43.000
Thank you for the technical problems.

03:43.000 --> 03:48.000
Times brief there's a lot I would like to cover.

03:48.000 --> 04:00.000
So I'll have to keep to a sketchy rather informal account we can extend things in the question period.

04:00.000 --> 04:19.000
The first term goal of theoretical inquiry is explanation, not just description. Description is hard enough descriptive inquiries seek to show that this is the way things are.

04:19.000 --> 04:35.000
The explanation seeks to show why things are this way, and not some other way. So much more ambitious endeavor, but it is the goal to keep in mind.

04:35.000 --> 04:56.000
In the case of language inquiry proceeds at two levels. One level is concerned with in languages. The second and higher level is concerned with the general faculty of language.

04:56.000 --> 05:20.000
And the other to it as FL faculty of language. FL is the innate endowment that enables a language to be acquired and used. The theory of FL is called universal grammar, you G traditional term it's adapted to a new context.

05:20.000 --> 05:47.000
At the first level the study of individual languages inquiry seeks an adequate grammar of the language, the description of the properties of the language, an adequate grammar must at the very least provide a listing technically a recursive enumeration of the grammatical sentences.

05:47.000 --> 06:11.000
Beyond that, it must yield what has been called the basic property of language. Each language is a digital infinity of hierarchically structured expressions, each of which determine determines perhaps constitutes a thought.

06:11.000 --> 06:20.000
Each of which can optionally be externalized in some sensory motor medium.

06:20.000 --> 06:44.000
A bold thesis would be the generation of thought by the internal language is entirely separated from externalization, which would be an ancillary process. I'll actually assume this return to pretty strong reasons to suppose that it's correct.

06:44.000 --> 07:05.000
In this conception, the internal language consists of the compositional rules that satisfy the basic property, along with what's called formal semantics more accurately term logical syntax.

07:06.000 --> 07:32.000
Important matter that I'll put aside here. The system for generating thought, we can call I language, it's internal individual in the technical sense, intentional with an S meaning we're interested in the actual system coded in the brain, not just one that yields the same output.

07:32.000 --> 07:48.000
Well, understood in this way, the study of language conforms to a long tradition from classical Greece, classical India up through the 19th century.

07:48.000 --> 08:07.000
Throughout the core of the tradition, regarded language as fundamentally a system of thought as audible thought in the phrase of the 19th century linguist William Dwight Whitney.

08:07.000 --> 08:26.000
We know that the restriction of sound is too narrow. This tradition was swept aside in the 20th century by behaviorist and structuralist currents, and it was forgotten. It's still very little known.

08:27.000 --> 08:43.000
Modern approaches in the 20th century take language to be at its root, a system of communication, rather than a system of thought system that perhaps evolved from animal communication.

08:43.000 --> 09:02.000
Recent work seems to me to indicate quite clearly that the tradition was on the right track and should be carried forward. That's a controversial view today, but I think it's justified, and I'll adopt it here.

09:02.000 --> 09:24.000
No returning to explanation. The task arises only at the higher level. In query into FL, the faculty of language seems that any human infant can acquire any language with comparable facility.

09:24.000 --> 09:42.000
If so, FL is a shared human property. There's also by now substantial evidence that FL is unique to humans, that its fundamental properties have no analog in the animal world.

09:42.000 --> 10:10.000
If so, FL is a true species property shared by humans, unique to human essentials. FL must enable any human infant to acquire any language to condition on UG that has been called explanatory adequacy since the 1950s.

10:11.000 --> 10:36.000
Sometimes it's called Plato's problem. It was raised in a certain form by Plato, as it was formulated by Bertrand Russell. It's the problem of how we can know so much, given so little evidence in technical terms of linguistics, it's the problem of poverty of stimulus.

10:36.000 --> 11:02.000
In the early days of the generative enterprise in mid 20th century, it was recognized that the poverty of stimulus problem was quite serious, and that standard formulas within behaviorist frameworks will get nowhere.

11:02.000 --> 11:25.000
There are contemporary variants with supercomputers and astronomical amounts of data, but they do no better. These early conclusions were established more firmly as serious experimental inquiry was undertaken in later years into language acquisition.

11:26.000 --> 11:48.000
It was found that the problem of poverty of stimulus is far more severe than had been assumed. By now it's been quite solidly established that the knowledge of language of a two to three year old child goes far beyond what they exhibit in behavior,

11:48.000 --> 12:16.000
and also that the gap between data available and knowledge attained is vast greatly deepens Plato's problem. These results seem to suggest that the faculty of language must be rich enough to yield complex knowledge from impoverished data, very rich therefore.

12:17.000 --> 12:39.000
Well, at the same time, FL must somehow accommodate the wide variety of languages poses a problem that problem deepens return to another condition that FL must satisfy, sometimes called Darwin's problem.

12:40.000 --> 13:01.000
How can FL have evolved under the conditions of human evolution, though evidence is fragmentary. There are some plausible conclusions about these conditions. Modern humans appeared about two to 300,000 years ago.

13:02.000 --> 13:12.000
There's no genomic evidence that the small group of humans began to separate not long after they appeared.

13:13.000 --> 13:20.000
All share FL, which was presumably already in place.

13:20.000 --> 13:32.000
Prior to the appearance of modern humans, there's virtually no indication of significant symbolic activity in the archaeological record.

13:32.000 --> 13:45.000
Not long after their appearance, there's very rich evidence. Note that these numbers are barely a flick of an eye in evolutionary time.

13:45.000 --> 13:56.000
Well, all of this suggests that FL appeared in a narrow window of time, pretty much along with modern humans.

13:56.000 --> 14:07.000
And there's quite strong evidence that it hasn't changed since. Well, if all that's the case, we expect FL to be very simple.

14:07.000 --> 14:17.000
So we therefore face a conundrum. FL must be rich enough to handle Plato's problem.

14:17.000 --> 14:33.000
At the same time, simple enough to deal with Darwin's problem and the apparent wide diversity of languages all determined by FL lurks in the background.

14:33.000 --> 14:41.000
These are basic conditions that be satisfied by UG.

14:41.000 --> 14:57.000
The conditions appear to be contradictory and much of the search for explanatory theory in past years has been guided by this apparent contradiction.

14:57.000 --> 15:08.000
For the first time, I think we can now begin to see how the conundrum might be resolved. So let's turn to that.

15:08.000 --> 15:24.000
Let's begin with the problem of diversity. That problem would be greatly reduced if diversity of language is sequestered in a particular component of the overall system.

15:24.000 --> 15:36.000
The natural place to look is in externalization, excluding the language operations that yield the thought system.

15:36.000 --> 15:50.000
That move has initial plausibility for eye language, the POS, the repository of stimulus problems are overwhelming, generally insurmountable.

15:50.000 --> 15:57.000
For externalization, at least there's some direct evidence what's heard or seen.

15:57.000 --> 16:16.000
Though this hasn't been firmly established, research is tending towards showing that the variety, complexity, and easy mutability of language is indeed sequestered in externalization,

16:16.000 --> 16:32.000
leaving the system of generative generation of thought relatively fixed, be completely fixed, we might someday discover that would vindicate an ancient tradition.

16:32.000 --> 16:54.000
If this turns out to be true for language, it wouldn't be very surprising. Externalization, some sensory motor medium, is not strictly speaking language, rather it's a combination of the internal system for generation of thought

16:54.000 --> 17:06.000
and sensory motor systems that have nothing to do with language. They were in place long before language appeared, haven't changed since.

17:06.000 --> 17:26.000
So we might expect that mapping the internal language system into completely unrelated sensory motor systems would be a complex task, solvable in many ways, easily subject to at least superficial change.

17:26.000 --> 17:41.000
And so it seems to be. Well, if this is basically correct, then the poverty of stimulus problems reduce largely to externalization.

17:41.000 --> 17:55.000
These topics have been the subject of extensive research since the principles and parameters framework was formulated 40 years ago.

17:55.000 --> 18:08.000
That was the first approach that offered at least a potential framework for accounting for acquisition of language in a feasible manner.

18:08.000 --> 18:24.000
And very important work, much of it by the late Ken Hale and his students, which is shown that languages that appear to be radically different are actually cast to the same mold at deeper levels.

18:24.000 --> 18:39.000
One high point in this research was Mark Baker's work on showing that on developing a hierarchy of parameters.

18:39.000 --> 19:04.000
The major step forward was later taken by Ian Roberts, who's shown that very simple algorithms based on elementary cognitive principles can feasibly zero in on choice of parameters tested this approach with thousands of languages of great typological

19:04.000 --> 19:20.000
variety. I think we can fairly say that the problem of externalization systems is fairly well in hand, at least in principle, partly in practice.

19:20.000 --> 19:37.000
And if externalization is the primary, maybe the only locus of variation, a large part of the conundrum is on its way to resolution, namely variability and learnability.

19:37.000 --> 20:00.000
Well, to proceed beyond, let's turn to some concrete examples. I'll begin with the most fundamental property of human language, one that is quite surprising in many ways, and has rich consequences that are not always sufficiently appreciated.

20:00.000 --> 20:15.000
This is the property called structure dependence. Let's briefly review some standard cases. So begin with the operation of construal.

20:15.000 --> 20:31.000
For example, finding out what verb phrase an advert modifies to take the sentence, the man who fixed the car carefully packed his tools. It's actually ambiguous.

20:31.000 --> 20:47.000
Could mean he fixed the car carefully, or he carefully packed his tools. Now, place the adverb in the front, carefully, the man who fixed the car, packed his tools.

20:47.000 --> 21:02.000
It's unambiguous, can only mean he carefully packed his tools. Actually, that poses a puzzle. The advert in initial position has to find a verb phrase to modify.

21:02.000 --> 21:22.000
But in doing so, we ignore the simplest computation, find the closest verb phrase, which would be fix the car. We reflexively ignore that and choose the more remote verb phrase, packed his tools.

21:22.000 --> 21:28.000
It's clear if you think about the structure of the sentence.

21:28.000 --> 21:47.000
The phrase packed his tools is actually closer to the advert, then fixed his car, fixed the car in the abstract structure. And that's what we attend to, though we never hear it is actually more involved.

21:47.000 --> 22:06.000
But that's enough to bring out the basic puzzle. We ignore the simple computation on linear order of words. We reflexively carry out a computation on strict structure. Let's take a second example.

22:06.000 --> 22:18.000
Anaphors terms that don't refer in themselves but find to and find an earlier antecedent that determines what they refer to.

22:18.000 --> 22:43.000
Well, the simplest algorithm is to seek the closest eligible antecedent as in sentences like the boys expect the girls to like each other means the girls like each other, not the boys. Sometimes, however, that fails as in sentences like the friends of the boys like each other.

22:43.000 --> 23:00.000
Here it's the friends who like each other, not the boys, the closest antecedent. So once again, we ignore the simplest algorithm, which relies on what we hear words in linear order.

23:00.000 --> 23:20.000
Now we use an algorithm that applies to abstract structures, and it's quite complex. First we have to identify the subject noun phrase, then we identify by the core element within the noun phrase, choose that as the antecedent.

23:20.000 --> 23:23.000
That's what we do reflexively.

23:23.000 --> 23:41.000
Final example, take verbal agreement sentence like the bombing of the city cities is a crime, not are a crime. The bombings of the city are a crime, not is a crime.

23:41.000 --> 24:04.000
So again, we reflexively ignore the simplest operation, adjacency, and again carry out an operation that looks quite complex construct the abstract structure, subject predicate, locate the head of that structure.

24:04.000 --> 24:07.000
That's what we do reflexively.

24:08.000 --> 24:18.000
Well, these observations generalized structure dependence holds of all constructions in all languages.

24:18.000 --> 24:42.000
What that means is that from infancy and on through life, we reflexively ignore 100% of what we hear words in linear order. We attend only to what we never hear abstract structures generated by the mind and operations on these structures.

24:42.000 --> 24:56.000
Furthermore, experimental work has shown that the principle is known to children as early as they can be tested two years old or less.

24:56.000 --> 25:10.000
Notice that this curious property is restricted to I language, the generation of thought principle doesn't hold for externalization.

25:10.000 --> 25:26.000
Well, the only plausible conclusion is that I language and externalization are distinct systems for I language, computation ignores what is externalized.

25:26.000 --> 25:53.000
Furthermore, all of this must be part of the innately fixed faculty of language property of Eugene conclusion further supports the thesis that I language which is language proper is a system of generation of thought, as was assumed in the millennia old tradition.

25:53.000 --> 26:11.000
Prior to the behaviorist structuralist revision in the 20th century. Well, it's plain why languages use linear order for externalization is required by the speech organs, which cannot produce structures.

26:11.000 --> 26:22.000
In fact, sign language, which is the resources of visual space available, does not adhere strictly to linearization.

26:22.000 --> 26:44.000
We recall that the sensory motor organs used for externalization are unrelated to language and their nature and evolution. So it's not surprising that I think which pure language ignores the properties that are imposed by these non linguistic organs.

26:44.000 --> 27:05.000
Well, with this in mind, we can turn to the why questions. Why does Eugene for structured abundance. The optimal answer would be that this follows from the simplest forms of Eugene, using the simplest operation for generating expressions.

27:05.000 --> 27:22.000
The simplest operation is unbounded binary set formation. It's the basis for what's called merge in the literature, and in fact, structure dependence does follow directly from merge.

27:22.000 --> 27:38.000
So to put it metaphorically, when mother nature created language, she found the simplest possible solution, which is incidentally, the way evolution works.

27:38.000 --> 27:57.000
Similarly, all of science so it appears to driving intuition of science since Galileo spinoso broadly verified that Einstein called it the miracle creed that guides all serious inquiry.

27:57.000 --> 28:20.000
In terms of language, it's called the strong thesis. Reliance on merge as the soul structure building operation has many other crucial empirical consequences won't be able to go into it in the time available but there's fair amount in the literature.

28:20.000 --> 28:38.000
Reliance on merge as the soul structure building operation yields the core of the basic property, generating an infinite array of hierarchically structured expressions.

28:38.000 --> 28:54.000
I can only add that the primitive elements of the system are elementary concepts, elaborated by merge these become phrases, we can translate straightforwardly to event semantics.

28:54.000 --> 29:19.000
And then tactic exponents of participants in events program along these lines, if it can be fully carried out with complete to generate the theory of generation of thought should add that the evolutionary origin of the most elementary concepts that have to serve as primitives is a complete

29:19.000 --> 29:24.000
mystery, and will probably remain so.

29:24.000 --> 29:36.000
There are no analogs in the animal world, no record of their development. Well, there are two logical possibilities for merge of X and Y.

29:36.000 --> 29:48.000
The two are distinct, or one is part of the other. Technically, it's called a term of the other. The two options are called external and internal merge.

29:48.000 --> 30:06.000
External merge yields combination, as in read books, internal merge yields displacement, as in which books did he read, where which books is understood to be the object of read.

30:06.000 --> 30:23.000
The strong minimalist thesis should not permit an operation of deletion, which would allow far richer systems in externalization deletion does take place within narrow conditions.

30:23.000 --> 30:34.000
But that does not apply to I language, which we may therefore assume has no deletion operations that has consequences.

30:34.000 --> 30:53.000
Let's take again simple cases like which books did you read the I language representation is which books did you read which books, which is interpreted directly as for which books X, you read the books X.

30:53.000 --> 31:11.000
The merge automatically yields the phenomenon of displacement with reconstruction. It's a topic that's been extensively studied. It falls out directly from the optimal system of generation.

31:11.000 --> 31:36.000
This structure dependence does what reaches our mind in this case is which books, did you read which books, but which reaches the ear and reaches consciousness is which books did you read externalization removes all but one occurrence, which has to remain to indicate that the

31:36.000 --> 31:57.000
operation took place. That's a general economic issue, radically reducing both mental computation and execution of articulatory emotions, actually the result of these simple operations to maximize computational

31:57.000 --> 32:22.000
efficiency yield problems for perception and parsing, what are called filler gap problems. The parser finds a WH phrase, like which books, and it has to find a gap that fills it for interpretation, turns out to be quite complex, one of the major problems in parsing.

32:22.000 --> 32:44.000
The problem would be radically reduced if the gap were filled, but Mother Nature didn't care about that when designing language. She insisted on finding the most elegant solution, even though it poses difficulties for language use quite serious ones.

32:45.000 --> 32:59.000
A sentence has become more complex. This is one of many cases in which computational efficiency conflicts with communicative efficiency.

32:59.000 --> 33:13.000
And in all cases, communicative efficiency is sacrificed. It's more evidence for the traditional conception of languages, fundamentally a system of thought.

33:14.000 --> 33:36.000
All of this is consistent with how evolution proceeds quite generally. Schematically, we can distinguish three stages. First, some disruption takes place, maybe a mutation or drift or gene transfer, or

33:36.000 --> 34:03.000
a bacterium accidentally swallowing another microorganism. That's the breakthrough that led to complex cells. That's why we're not all bacteria. The second stage is reconstruction. Nature reconstructs the new entity in the simplest way, observing the miracle creed, paying no attention to how the new entity might be used.

34:03.000 --> 34:25.000
The third stage is winnowing the outcomes that reproduce more effectively prevail. That's natural selection. Well, that suggests a reasonable scenario for language evolution. First, some minor rewiring of the brain took place.

34:26.000 --> 34:43.000
Yielded the new property of recursive enumeration. Next step is reconstruction. Nature devises the simplest, most elegant way to organize the new system. Strong minimalist thesis.

34:44.000 --> 35:01.000
The semantic theory becomes available, possibly with more primitive origins. Computational procedures are given part of natural law. The winnowing stage is never reached, possibly because of lack of time.

35:02.000 --> 35:14.000
Possibly because the optimal system is so delicately designed that it's either all or none. So we're approaching an interesting conclusion.

35:15.000 --> 35:34.000
The internal language might be perfect on a common human possession. That's the strong minimalist thesis proposed 25 years ago, was not then regarded as realistic, rather as a long term goal that might guide research.

35:34.000 --> 35:54.000
Recent work does suggest something more audacious. Thesis might indeed be true as evolutionary theory, leases to suspect, along with science generally, and as empirical inquiry increasingly suggests.

35:54.000 --> 36:05.000
Well, there's a lot more to talk about. Many new, many other possibilities, but I don't think there's time to talk about it here.

36:05.000 --> 36:20.000
The crucial point is that the internal language seems to keep to the strong minimalist thesis throughout, with no departure necessary, even for pretty complex cases.

36:20.000 --> 36:32.000
Well, remains to show how far we can extend such reasoning. Needless to say, that's an extremely challenging task.

36:32.000 --> 36:48.000
Nothing like it has ever risen in the study of language and thought. There are promising early steps far beyond what could have been imagined just a few years ago.

36:49.000 --> 37:06.000
Final remark, strong minimalist thesis has several functions. One is a disciplinary function, sharply restricts the options for description, and thus deepens explanation.

37:06.000 --> 37:35.000
It also has an enabling function. It provides options for what I language might be, for what kinds of subsystems might exist. An interesting question just coming into focus is to explore whether I languages make use of all the possibilities that are enabled by the strong minimalist thesis,

37:35.000 --> 37:56.000
which may be more than just a guideline for inquiry, as assumed in the past, but may actually express fundamental truth about the nature of language and thought, the most distinctive possessions of this strange species of ours.

37:56.000 --> 38:06.000
Thank you.

38:06.000 --> 38:12.000
Thank you, Mr Chomsky. Amala, do we have any questions? Did you select some?

38:12.000 --> 38:30.000
Yeah, I see this question. To what extent does the UG remain available to second language accusations? If you can see the chat, Mr Chomsky.

38:30.000 --> 38:52.000
Okay, I see questions here. First one I see is, what's your take on views of data linguistics that focus on the data instead of UG and internalization, and also criticize the syntactic theory as well?

38:52.000 --> 39:04.000
What's your take on Annie Chomsky in School of Thought? Well, I can't comment on all of them. There's plenty of criticism. You have to pick them up and look at them.

39:04.000 --> 39:23.000
As far as data linguistics is concerned, it's fine. You can collect a lot of data and can look at it, can find surfed properties of it. It doesn't tell you very much, just as in any other field. You look at a lot of data, you don't find much.

39:23.000 --> 39:42.000
If you want to study the laws of motion in physics, you don't collect data about leaves blowing in the wind. What you do is careful experimentation, including thought experiments, to focus on the crucial principles.

39:42.000 --> 40:10.000
Now, right now, data linguistics is very popular. There are studies that you've seen many of them in the press, a lot of excitement, GPT-3 and others. They take astronomical amounts of data, maybe 50 terabytes of data, have thousands, maybe a trillion parameters,

40:11.000 --> 40:31.000
supercomputers running on them. And with all of that data, they can find surfaced properties of the expressions in some vast amount of data. You string things together like this, it looks more or less like language.

40:31.000 --> 40:51.000
It tells you absolutely nothing, zero. The proof of that is that the same systems work exactly as well for impossible languages. So for languages, say that violate structure dependence, which are impossible for humans, they work just fine.

40:51.000 --> 41:07.000
It's very much as if I were to go to a physics conference and say, I have a terrific new theory. It accounts for all the particles that have been discovered, even the ones that are possible and haven't been discovered.

41:07.000 --> 41:26.000
And it's so simple that I can express it in two words, anything goes, sorry, I don't get the Nobel Prize, because the same theory accounts for everything that's not a possible particle. So it tells you precisely nothing.

41:27.000 --> 41:48.000
All of these things that are achieving vast excitement and the press basically do nothing. I mean, there are approaches based on the same techniques, deep learning, that do do something.

41:49.000 --> 42:07.000
So for example, the Google translate is based on these techniques and it's useful. There's been some success in handling protein folding by these methods, but these data linguistics approaches are absolutely worthless.

42:08.000 --> 42:13.000
They don't achieve anything useful. They don't.

42:13.000 --> 42:32.000
And they tell you nothing about language by definition. So basically it's a way to use up a lot of the energy in California and to develop public relations for the Silicon Valley, but it's doing nothing else.

42:33.000 --> 42:41.000
Well, the next to what extent does UG remain available to second language acquisition.

42:41.000 --> 42:57.000
And secondly, is it true that movement from the specifier VP the specifier of TP is not motivated by the chest case checking property nominative case.

42:57.000 --> 43:04.000
Well, first, to what extent does UG remain available to second language acquisition.

43:04.000 --> 43:12.000
There is a good deal of research on that by people working on second language.

43:12.000 --> 43:20.000
My own view is that decisive conclusions have not been reached.

43:20.000 --> 43:36.000
There's, and it may actually vary among people remember there's a lot of individual variation and variation as to how the second languages are acquired, a second language that's acquired through immersion.

43:36.000 --> 43:49.000
The best way if possible is going to be acquired very differently than one that's acquired by reading a grammar or going to a class.

43:49.000 --> 43:57.000
And it's possible that UG functions in the first case, much more effectively than it does in the second.

43:57.000 --> 44:04.000
These are basically open research questions, non trivial ones for teachers.

44:04.000 --> 44:13.000
Is it true that movement from the specifier VP, the specifier TP is not motivated by the case checking principle.

44:13.000 --> 44:22.000
Well, again, that's an internal technical question I my own view is basically that it's not.

44:22.000 --> 44:29.000
There's no reason why the nominative case couldn't be assigned to the specifier of VP.

44:29.000 --> 44:51.000
There are other reasons for this having to do with the, what's called EPP, the and the and ECP the empty category principle which are very, which are tied together and they're very their properties of certain languages, their

44:51.000 --> 45:03.000
properties of languages that don't automatically delete the freely delete an empty subject. So Spanish and English are quite different this way.

45:03.000 --> 45:18.000
Spanish, you don't have to pronounce the surface subject and spec TP in English you do English is one of a relatively small number of languages where you have to pronounce it.

45:18.000 --> 45:35.000
Spanish is one of the much larger category of so called no subject languages in which you don't. So for Spanish, the movement is not obligatory. It's optional for other reasons for semantic reasons.

45:36.000 --> 45:49.000
And I think when we look at all these things together they really have to do with what's called labeling theory. It's been discussed for the last 10 years or so I don't have time to go into the details.

45:49.000 --> 46:06.000
Well, how does generative grammar differ from descriptive grammar depends what you mean by descriptive grammar. If you mean what was meant by it and structuralist linguistics.

46:07.000 --> 46:22.000
Descriptive grammar. What I studied when I was a grad student under graduate 80 75 years ago was regarded as a taxonomic science. That's what it was called.

46:22.000 --> 46:38.000
There are procedures of analysis which you use that find the units of a language and the way in which they are organized relative to one another. That was descriptive grammar.

46:38.000 --> 46:51.000
If there's anything new it's just by what's called analogy. I don't think any of that can be sustained. A generative grammar does something quite different.

46:51.000 --> 47:07.000
It tries to tell you what all the possible structures are for the entire language. It's quite different from a taxonomic grammar. Furthermore, the elements that enter into it can't be acquired by procedures.

47:07.000 --> 47:25.000
That was shown years ago. Now, from another point of view, the generative grammar is a descriptive grammar, but a very different kind from the descriptive grammars of structural linguistics.

47:25.000 --> 47:43.000
What do you see as the future of linguistics in the future six field in Pakistan? Well, controversial subject question, of course, my own feeling is pretty much what I tried to describe.

47:43.000 --> 48:04.000
I think there's a future for linguistics as quite a new field, a field that for the first time is able to provide genuine explanations for the basic phenomenon of language, instead of describing them, which is hard enough.

48:04.000 --> 48:23.000
There's a couple of examples. There are many more that can be carried through. It's a major breakthrough for the future. You can't predict the future of science, so we don't know. But that's my personal opinion.

48:23.000 --> 48:44.000
We just see the future differently. What about the future in Pakistan? Well, simple answer to that depends on Pakistani linguists, other linguists who are interested in studying or do other native languages of Pakistan.

48:44.000 --> 48:59.000
I don't know if you're interested, but for the future of the linguistic field as a general field, it'll depend on developments within Pakistan, in Pakistani universities and research centers.

48:59.000 --> 49:12.000
Many things changed in the last 70 years. What new parameters must we take into consideration for effective language teaching?

49:12.000 --> 49:38.000
A few years ago there were no parameters. The assumption in structural linguistics explicitly stated over and over was that any languages can vary virtually without limits and each individual language must have to be studied in its own terms

49:38.000 --> 49:54.000
with no assumptions about other languages. That was in the mid-1950s the basic doctrine that was sometimes called the Boazian doctrine, rightly or wrongly.

49:54.000 --> 50:12.000
Well, that's very far from true. I should say that something similar was believed in biology at about the same time. It was assumed that organisms vary virtually without limit.

50:12.000 --> 50:24.000
Each one has to be studied on its own. In biology that's now known to be completely false. In fact, there are very narrow constraints on possible organisms.

50:24.000 --> 50:44.000
So narrow that some have even speculated that there's a universal genome and the apparent variety of organisms is just superficial working out of some of the possibilities determined by the fixed form for organisms.

50:44.000 --> 51:11.000
I think something along those lines has happened with regard to our concept knowledge of language. About 40 years ago there was an innovation in linguistic theory suggesting that the basic theory in our minds has fixed unchanging principles

51:11.000 --> 51:25.000
like some of those they discussed and parametric variation, a set of parameters. So that would mean that language acquisition is kind of like a question answering game.

51:25.000 --> 51:45.000
An infant asks for each parameter, is it set this way or that way? Is it set as an old subject language like Spanish or like a language that requires an explicit subject like English? Yes or no question can be answered in a very small amount of data.

51:45.000 --> 52:00.000
Does the language have verb proceeding object like English and Spanish or does it have verb following object like Japanese and many others? Again, answer Belenna, small number of evidence.

52:00.000 --> 52:19.000
We now have recent work of the kind I mentioned like Ian Roberts, which shows how the infant can work through the system of parameters very efficiently to fix on the exact language.

52:19.000 --> 52:43.000
I want to look at what look like the plausible set of parameters. The best work I know is by Ian Roberts and by Giuseppe Lungabardi, teaches at York University in England, has done extensive work on the variety of possible parameters and what they tell us about the history of linguistics.

52:43.000 --> 53:01.000
That's a new topic that he's innovated and it tells us quite a lot. You can get much deeper knowledge of the relationships among languages at a much deeper level if you look at shared borders than in the standard way.

53:01.000 --> 53:16.000
So that's a source you can look in. How does UG theory play into surviving languages like Romanian, maintaining its Latin roots along with neighbouring Slavic languages?

53:16.000 --> 53:35.000
You can say the same about English, about 60% of the vocabularies romance, though a lot of the grammar is Germanic. Say the same about French. French has a lot of Germanic properties, but it's a romance language.

53:35.000 --> 53:53.000
So it has a critic movement like the romance languages, but it's like English and German in requiring an overt subject. Well, UG is exactly what studied by UG.

53:53.000 --> 54:10.000
So how are the range of parameters determined for particular languages? They all seem to share the same principles at an underlying level. In fact, the same is true of languages.

54:10.000 --> 54:29.000
Amazon or Papua and Guinea tribes that haven't had other contact for tens of thousands of years. When they're studied carefully, they seem to have the same underlying principles, different parametric choices.

54:29.000 --> 54:48.000
There is a phenomenon of called Sprachwende languages commonly accept some features of neighbouring languages, even if they're related, not closely related, which is what happened with Romanian.

54:48.000 --> 55:04.000
It picked up some of the properties of neighbouring Slavic languages in these geographic areas of which involve interaction among people. In the case of English it's pretty straightforward.

55:04.000 --> 55:26.000
The Norman conquest in 1066 turned English into a mixture of romance and Germanic with a lot of romance vocabulary. In fact, the majority, but Germanic Germanical structure.

55:27.000 --> 55:42.000
Given the natural, the recent advances in natural language processing and AI, what prospects do you see for SLA?

55:42.000 --> 55:58.000
As I mentioned, the advances in natural language processing and AI, well, they may be useful for some purposes. Just tell us absolutely nothing about language.

55:58.000 --> 56:27.000
If they happen to be useful for some purpose, say Google Translate, Life Translation, Transcription, SLA, that's fine. Use whatever is useful. But if you're interested in the nature of language, the nature of cognition, human cognitive processes, this work just tells you basically nothing for the reasons that I mentioned.

56:29.000 --> 56:46.000
Does the human brain contain a limited set of constraints for organizing language? Undoubtedly it does. We don't know very much about it because we don't know very much about the human brain.

56:46.000 --> 57:12.000
It's a very hard topic to study. The brain altogether is a hard topic to study, even for tiny organisms. So if you take an ant, which has a brain the size of a tiny, a minute brain, you need a microscope to see it.

57:12.000 --> 57:34.000
We have no idea how it carries out highly complex computations that humans can't carry out so the desert ants in my backyard can navigate in a way that a human can't. They use computations that are inaccessible to us.

57:34.000 --> 57:52.000
We have to duplicate it with complicated instruments. Even the ant brain is very hard to understand. Human brain is much harder because we cannot do experiments, the kind of experiments that immediately come to mind.

57:53.000 --> 58:10.000
You can't do them for ethical reasons. So we happen to know a fair amount about the human visual system, but that's from experiments with some monkeys, which rightly or wrongly we've allowed ourselves to carry on.

58:11.000 --> 58:25.000
And they have about the same visual system they do. But you can't do that for language because there's no other organism. No other organism has even the rudiments of human language. So there's nobody to study.

58:25.000 --> 58:43.000
And you can't study the human brain for ethical reasons. So it's a very difficult topic. Nevertheless, there are some achievements. One of the most important ones, in fact, has to do with structure dependence, what I mentioned, the property I mentioned.

58:44.000 --> 59:04.000
There is research created by Andrea Moro, fine linguist in Italy, Milan research group, which was able to show that here's the paradigm they used.

59:05.000 --> 59:23.000
They took subjects, say, whose native language was German, and they gave them two kinds of invented languages. One invented language was based on an existing language that they didn't know, maybe Italian.

59:23.000 --> 59:40.000
Another database was very simple language, which used principles that you don't have in language, like linear order. So a language in which negation is the third word of a sentence.

59:40.000 --> 59:59.000
It's very simple to work out. Well, it turns out, when the subjects were given an inventive language based on an actual language, the norm, the language areas of the brain language, dedicated areas of the brain function normally.

59:59.000 --> 01:00:13.000
When they were given an invented language that violated structure dependence, even with trivial algorithms, there was diffuse activity, the brain, the language areas were not activated.

01:00:14.000 --> 01:00:33.000
Well, that tells you that the brain, it tells you something about what we expect to be true, that the brain is structured in such a way as to be available for language the way it is.

01:00:33.000 --> 01:00:51.000
It sets conditions on what language must be. One of them are the conditions that conform to the fundamental principle of structure dependence. There are a few other things like this, which are quite interesting, but it's a hard topic.

01:00:51.000 --> 01:00:59.000
What are my thoughts on systematic functional grammar?

01:00:59.000 --> 01:01:13.000
I think it tells us much about the questions that I've been discussing here. It may be useful as indicated in the question by studying the wider context in which language is used.

01:01:13.000 --> 01:01:26.000
So there is a broad context in which language is used, which is not studied by the generative grammar which investigates sentence grammar.

01:01:26.000 --> 01:01:36.000
It's a perfectly sensible topic and maybe functional grammar can provide some help with that. It does all to the good.

01:01:36.000 --> 01:01:58.000
So if I say something like, John went to the library, he bought a book, John went to the bookstore, he bought a book.

01:01:58.000 --> 01:02:16.000
If you think about what the language tells you, doesn't tell you what the word he refers to. That discourse is perfectly possible if John went to the store and Bill bought a book. Doesn't tell you that the book was bought at the store.

01:02:17.000 --> 01:02:31.000
It's perfectly the sentence that are perfectly possible if John went to the bookstore, Bill bought a book somewhere else. That's perfectly possible. Language doesn't tell you anything about that.

01:02:31.000 --> 01:02:48.000
There are conditions of normal discourse that make it likely that in John went to the store, he bought a book, makes it likely that he is John and that he got the book in the store.

01:02:48.000 --> 01:03:00.000
That's plausible because of the nature of human functioning systems and maybe functional grammar can tell you something about such things.

01:03:00.000 --> 01:03:07.000
I frankly think there's not going to be a lot to say about it. These are quite complex and diverse.

01:03:07.000 --> 01:03:14.000
Well, that's the end of the questions that I see here.

01:03:14.000 --> 01:03:19.000
Oh, I see a couple of others.

01:03:19.000 --> 01:03:28.000
Let me say Chomsky, if you have the time to answer them, that's okay. Or if you want to stop right now, it's up to you.

01:03:28.000 --> 01:03:34.000
Sorry, I couldn't hear that.

01:03:34.000 --> 01:03:48.000
I'm asking you about the questions. If you want to go ahead or you want to stop here, we know you have some other things to do and we had only one hour for this webinar.

01:03:48.000 --> 01:03:53.000
Should I go on with some further questions?

01:03:53.000 --> 01:03:54.000
If you want to.

01:03:54.000 --> 01:03:57.000
Is that what you're suggesting? Yeah, okay.

01:03:58.000 --> 01:04:04.000
How may we better connect UG with contemporary teaching?

01:04:04.000 --> 01:04:10.000
It's kind of like asking if you're, suppose you're a swimming coach.

01:04:10.000 --> 01:04:22.000
It's a good idea to understand something about physiology and use your knowledge of physiology and improving your teaching of swimming.

01:04:22.000 --> 01:04:29.000
If you're teaching language, it's useful to know something about language. That's UG.

01:04:29.000 --> 01:04:35.000
A skilled teacher will figure out how to use their knowledge of language.

01:04:35.000 --> 01:04:45.000
UG in teaching just as a good swimming coach will use their knowledge of physiology and teaching swimming.

01:04:45.000 --> 01:04:51.000
There's no formula for this. That's what good teaching is about.

01:04:51.000 --> 01:05:02.000
As a generative grammar is concerned, you have this claim that teaching grammar should be excluded in classroom construction.

01:05:02.000 --> 01:05:12.000
It's been criticized by some linguists long said that teaching should not be excluded, but that of communication should be foregrounded.

01:05:12.000 --> 01:05:21.000
Do you think this claim of excluding teaching grammar and classroom instruction is still valid, especially in today's time?

01:05:21.000 --> 01:05:28.000
It depends what your goals are.

01:05:28.000 --> 01:05:37.000
If you want the student to understand the language that they're acquiring, it's perfectly useful.

01:05:37.000 --> 01:05:42.000
It's very useful to teach something about the nature of the language.

01:05:42.000 --> 01:05:55.000
If you're teaching just by immersion the best way, so you send somebody to Italy to study to learn Italian, they'll just pick it up by listening to it.

01:05:55.000 --> 01:06:02.000
That's not teaching. That's immersion.

01:06:02.000 --> 01:06:07.000
You try to make it as much like a child as you can, but there's no...

01:06:07.000 --> 01:06:12.000
There can't be an answer to this. It depends what you're trying to achieve.

01:06:12.000 --> 01:06:21.000
If you're trying to achieve communication skills, but you don't care whether the person understands the language, you can foreground communication.

01:06:21.000 --> 01:06:26.000
If you care about whether the person understands the language, you'll teach grammar.

01:06:26.000 --> 01:06:30.000
That's just the nature of the language.

01:06:30.000 --> 01:06:34.000
Why did I choose grammar?

01:06:34.000 --> 01:06:37.000
Grammar is a funny word.

01:06:37.000 --> 01:06:44.000
Grammar, in the technical sense, grammar just means the nature of language.

01:06:44.000 --> 01:06:48.000
What I've been talking about is grammar.

01:06:48.000 --> 01:06:53.000
What's the nature of our faculty of language and thought?

01:06:53.000 --> 01:06:56.000
Because we're intimately connected.

01:06:56.000 --> 01:07:01.000
So why did I choose to study language and thought? Good reason, I think.

01:07:01.000 --> 01:07:09.000
These are the fundamental properties that distinguish humans from the rest of the organic world.

01:07:09.000 --> 01:07:14.000
So if we want to understand what kind of creatures we are,

01:07:14.000 --> 01:07:20.000
if we want to know thyself as the Delphic Oracle advised us,

01:07:20.000 --> 01:07:24.000
the obvious thing to look at is language and thought.

01:07:24.000 --> 01:07:30.000
It's been assumed for several millennia. I think it's a wise choice.

01:07:30.000 --> 01:07:36.000
I chose it for the same reason.

01:07:36.000 --> 01:07:46.000
Well, the others that I see here have to do with

01:07:46.000 --> 01:07:50.000
finding

01:07:50.000 --> 01:07:57.000
those exposing learners to the cell environment for more than three years,

01:07:57.000 --> 01:08:01.000
make them acquire, develop a native like language.

01:08:01.000 --> 01:08:08.000
A child, yes, a child will pick up a second language in almost no time.

01:08:08.000 --> 01:08:13.000
Just to give you an example from personal experience.

01:08:13.000 --> 01:08:20.000
My family went to, I was teaching in Italy about 40 years ago.

01:08:20.000 --> 01:08:23.000
So my whole family came along.

01:08:23.000 --> 01:08:30.000
My older kids were studying, 20, 25, were studying Italian.

01:08:30.000 --> 01:08:32.000
They really wanted to learn it.

01:08:32.000 --> 01:08:35.000
We had a 10-year-old who didn't want to learn it.

01:08:35.000 --> 01:08:40.000
He didn't want to come with us. He said he was not refused to learn Italian.

01:08:40.000 --> 01:08:47.000
Well, after a month, the 10-year-old, if the phone rang, the 10-year-old had to answer it.

01:08:47.000 --> 01:08:55.000
Despite himself, he just acquired Italian by being in school where everyone was talking Italian.

01:08:55.000 --> 01:08:58.000
Children just absorb it like a sponge.

01:08:58.000 --> 01:09:05.000
The older kids were working on it, just my wife and I were going to make it harder for adults.

01:09:05.000 --> 01:09:14.000
So the answer is depends on age, depends on motivation, depends on all kinds of things.

01:09:14.000 --> 01:09:21.000
But you can live in a foreign culture for all your life and not know the language.

01:09:21.000 --> 01:09:23.000
I know people like that.

01:09:23.000 --> 01:09:29.000
Do you find the need for an eclectic approach to communicative competence?

01:09:29.000 --> 01:09:34.000
Do we need to apply all kinds of competence time to test the speech?

01:09:34.000 --> 01:09:39.000
Currently, researchers use only four or five or single competence.

01:09:39.000 --> 01:09:49.000
Well, I really don't know. It seems to me, if you want to broaden the approach to communicative competence, whatever that is,

01:09:49.000 --> 01:09:54.000
use whatever you find valuable to increase it.

01:09:54.000 --> 01:09:57.000
I don't think there are any formulas.

01:09:57.000 --> 01:10:05.000
Why do two children of same parents start speaking languages at different ages?

01:10:05.000 --> 01:10:12.000
Well, there's slight individual variation, but not very much.

01:10:12.000 --> 01:10:25.000
Unless there's some very unusual circumstances, two children in the same family will start speaking at about the same age, maybe a couple of months different.

01:10:25.000 --> 01:10:30.000
UG has nothing to say about this. It's a matter of maturation.

01:10:30.000 --> 01:10:38.000
The maturation of innate competence, it's like acquiring the ability to walk.

01:10:38.000 --> 01:10:44.000
You find children are slightly different. One child may be walking at 12 months.

01:10:44.000 --> 01:10:53.000
Another might start walking at 14 months, but it's just a matter of how the organism matures.

01:10:53.000 --> 01:10:58.000
Maybe the main core of UG in just three words.

01:10:58.000 --> 01:11:06.000
I don't know if I can do it in three words, but it's the theory of the nature of language.

01:11:06.000 --> 01:11:17.000
We talked about explanation and description. What about the metacognitive aspect of learning?

01:11:17.000 --> 01:11:23.000
I don't know what can be said about that, not familiar with it.

01:11:23.000 --> 01:11:35.000
I think I'll probably have to stop here. I don't see any simple questions.

01:11:36.000 --> 01:11:44.000
Yes, Mr Chomsky. Thank you very much. We really appreciate you being here with us.

01:11:44.000 --> 01:11:50.000
Thank you very much on behalf of T-Soul Tampico Talks and Teachers Development Webinars.

01:11:50.000 --> 01:11:52.000
Thank you.

01:11:52.000 --> 01:11:55.000
You want to say something before Mr Chomsky leaves?

01:11:55.000 --> 01:12:02.000
Yes, Professor Chomsky, it has been a really fulfilling experience having you at Teacher Development Webinars.

01:12:02.000 --> 01:12:06.000
Thanks to T-Soul Tampico Talks for organizing this wonderful talk.

01:12:06.000 --> 01:12:17.000
As a student of linguists and now I am a scholar, I have been reading your work and seeing you as a father figure.

01:12:17.000 --> 01:12:20.000
Having you is like a dream come true.

01:12:20.000 --> 01:12:26.000
I really can't express my happiness and humbleness.

01:12:26.000 --> 01:12:33.000
I much appreciate your time and experience in sharing all this.

01:12:33.000 --> 01:12:39.000
You celebrated your birthday a few days ago.

01:12:39.000 --> 01:12:46.000
Happy belated birthday to you again. Long live.

01:12:47.000 --> 01:12:49.000
Thank you, Amnala.

01:12:49.000 --> 01:12:55.000
Thank you, everyone. We hope to see you again in our next events, our next webinars.

01:12:55.000 --> 01:13:03.000
Yes, I would like to also like to acknowledge Master English Training for sponsoring a Zoom account.

01:13:03.000 --> 01:13:09.000
The other ones, the supporters have come to us in time of need.

01:13:09.000 --> 01:13:18.000
Thanks to Master English Training and Fatima, who deals with all these things.

01:13:18.000 --> 01:13:24.000
Thanks very much for your time and support and appreciation for Teacher Development Webinars.

01:13:24.000 --> 01:13:30.000
Thank you to T-Soul Tampico Talks.

01:13:30.000 --> 01:13:35.000
Thank you, Amnala. Thank you, everyone. Thank you, Mr Chomsky. Bye-bye. See you next time.

01:13:39.000 --> 01:13:42.000
Thank you, Amnala.

