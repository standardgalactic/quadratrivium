start	end	text
0	10000	All right, I'm so excited to be here with you, Ray.
10000	16680	It's great to be here, great to see everybody together, beautiful audience.
16680	20800	So my favorite thing in that introduction of you is that you have been working in AI
20800	26780	longer than any other human alive, which means if you live forever and will get to that,
26780	29000	you will always have that distinction.
29000	32800	I think that's right.
32800	36480	Marvin Minsky was actually my mentor.
36480	41560	If he were alive today, he would actually be more than 61 years.
41560	43360	We're going to bring him back also.
43360	46360	So maybe you'll not sure how we'll count the distinction then.
51360	53040	All right, so we're going to fix the audio.
53040	55560	But this is what we're going to do with this conversation.
55600	59120	I'm going to start out asking Ray some questions about where we are today.
59120	60520	We'll do that for a few minutes.
60520	66120	Then we'll get into what has to happen to reach the singularity, so the next 20 years.
66120	68640	Then we'll get into discussion about what the singularity is,
68640	70360	what it means, how it would change our lives.
70360	72600	And then at the end, we'll talk a little bit about how,
72600	75560	if we believe this vision of the future, what it means for us today.
75560	77280	Ask your questions, they'll come in.
77280	80040	I'll ask them as they go in the different sections of the conversation.
80040	81280	But let's get cracking.
81320	82120	Can you hear me?
87160	88000	You can't hear Ray.
91000	92840	Well, this will be recorded.
92840	94320	You guys are going to all live forever.
94320	95520	There'll be plenty of time.
96560	98280	It will be fine.
98280	99440	I'm just going to get started.
99440	100920	I assume the audio will get worked out.
100920	102840	They do a fabulous job here at South by.
103840	106040	I think they should be able to hear me and you.
109280	110760	All right, we got this over on the right.
111280	116640	Audio engineers, are we good to go?
118640	119640	We're good to go.
119640	122600	All right, first question, Ray.
123880	127200	So, you've been working in AI for 61 years.
127200	128640	Wait, can you hear me?
133760	136520	So everybody in the front can hear you, but nobody in the back can hear you.
136520	137560	Can you hear me now?
141480	142960	All right, I'll speak louder.
145160	146800	First question.
146800	149960	So you've been living in the revolution for a long time.
149960	155080	You've made lots of predictions, many of which have been remarkably accurate.
155080	159880	We've all been living in a remarkable two year transformation
159880	162560	with large language models a year and a half.
162560	166720	What has surprised you about the innovations in large language models
166720	168440	and what has happened recently?
168440	172480	Well, I did finish this book a year ago
172480	175200	and didn't really cover a large language model.
175200	190760	So I delayed the book to cover that, but I was expecting that to happen
190760	192520	like a couple of years later.
192520	198640	I mean, I made a prediction in 1999 that it would happen by 2029.
199960	207120	And we're not quite there yet, but it looks like it's maybe a year or two ahead of schedule.
211560	213880	So that was maybe a bit of a surprise.
213880	218920	Wait, you predicted back in 1999 that a computer would pass the Turing test in 2029.
218920	223240	Are you revising that to something more closer to today?
226240	230120	No, I'm still saying 2029.
232720	237840	The definition of the Turing test is not precise.
237840	242760	We're going to have people claiming that the Turing test has been solved
242760	246280	and people are saying that GPT-4 actually passes it.
246280	249880	Some people, so it's going to be like maybe two or three years
249880	253920	where people start claiming and then they continue to claim
253920	256120	and finally everybody will accept it.
256120	258440	So it's not like it happens in one day.
258440	261480	But you have a very specific definition of the Turing test.
261480	263400	When do you think we'll pass that definition?
265360	268200	Well, the Turing test is actually not that significant,
268200	278080	because that means that a computer will pass for a human being.
278080	283160	And what's much more important is AGI, Automatic General Intelligence,
283160	286480	which means it can emulate any human being.
286480	292440	So you have one computer and it can do everything that any human being can do.
292440	296880	And that's also 2029, it all happens at the same time.
296880	297960	But nobody can do that.
297960	302440	I mean, just take an average large language model today.
302440	307840	You can ask it anything and it will answer you pretty convincingly.
307840	310240	No human being can do all of that.
310240	311440	And it does it very quickly.
311440	315480	It will write a very nice essay in 15 seconds.
315480	319200	And then you can ask it again and it will write another essay.
319200	323280	And no human being can actually perform at that level.
323280	326560	Right, so you have to dumb it down to actually have a convincing Turing test.
326560	328840	So having a Turing test, you have to dumb it down.
328840	331120	Yeah, let me ask the first question from the audience,
331120	334640	since I think it's quite relevant to where we are, which is Brian Daniel.
334640	337080	Is the Kurzweil curve still accurate?
337080	337600	Say again?
337600	340640	Is the Kurzweil curve still accurate?
340640	343720	Yes, in fact, can they see that?
343720	345120	Let's pull the slides up, first slide.
348680	352160	So this is an 80 year track record.
352160	354560	This is an exponential growth.
354560	358880	A straight line on this curve means exponential curvature.
361240	365800	If it was sort of exponential, but not quite, it would curve.
365800	367480	This is actually a straight line.
368760	380200	It started out with a computer that did 0.0000007 calculations per second per
380200	381600	constant dollar.
381600	383760	That's the lower left hand corner.
383760	388320	At the upper right hand corner, it's 65 billion calculations per second for
388320	390040	the same amount of money.
391760	395840	So that's why large language models have only been feasible for two years.
395840	399560	We actually had large language models before that, but it didn't work very well.
401840	404400	And this is an exponential curve.
404400	408400	Technology moves in an exponential curve.
408400	419000	We see that, for example, having renewable energy come from the sun and
419000	421720	wind, that's actually an exponential curve.
423640	425560	It's increased, it's gone.
426720	430760	We've decreased the price by 99.7%.
432280	437200	We've multiplied the amount of energy coming from solar energy a million fold.
438520	444520	So this kind of curve really directs all kinds of technology.
447600	451760	And this is the reason that we're making progress.
451760	456160	I mean, we knew how to do large language models years ago, but
456160	457760	we're dependent on this curve.
460160	461160	And it's pretty amazing.
461160	467920	It started out increasing relay speeds, then vacuum tubes, then integrated circuits.
467920	471640	And each year, it makes the same amount of progress approximately
473640	476080	regardless of where you are on this curve.
476080	479960	We just added the last point, and it's, again,
481320	485600	we basically multiply this by two every 1.4 years.
488320	492480	And this is the reason that computers are exciting, but
492480	494840	it actually affects every type of technology.
495840	498680	And we just added the last point like two weeks ago.
500800	503440	All right, so let me ask you a question.
503440	506720	You wrote a book about how to build a mind.
506720	509800	You have a lot about how the human mind is constructed.
509800	512840	A lot of the progress in AI, AI systems are being built and
512840	514520	what we understand about neural networks, right?
514520	519280	So clearly our understanding of this helps with AI.
519280	522840	In the last two years, by watching these large language models,
522840	525400	have we learned anything new about our brains?
525400	528800	Are we learning about the inside of our skulls as we do this?
528800	531200	It really has to do with the amount of connections.
532480	536840	The brain is actually organized fairly differently.
536840	540040	The things near the eye, for example, deal with vision.
542480	546000	And we have different ways of implementing different parts of the brain that
546000	547520	remember different things.
547520	549960	We actually don't need that.
549960	553640	In large language model, all the connections are the same.
553640	556840	We have to get the connections up to a certain point.
556840	559560	If it approximately matches what the brain does,
559560	565040	which is about a trillion connections, it will perform kind of like the brain.
565040	567720	We're kind of almost at that point.
567720	574480	Wait, so if the GPT-4 is 400 billion, the next ones will be a trillion or more.
574480	576480	So the construction of these models,
576480	579320	they are more efficient in their construction than our brains are.
581040	584680	We make them to be as efficient as possible, but
584680	587600	it doesn't really matter how they're organized.
587600	593240	And we can actually create certain software that will actually expand
593240	597440	the amount of connections more for the same amount of computation.
600000	603800	But it really has to do with how many connections
606480	611200	our particular computer is responsible for.
611200	618360	So as we approach AGI, we're not looking for a new understanding of how to make
618360	619480	these machines more efficient.
619480	622520	The transformer architecture was clearly very important.
622520	625840	We can really just get there with more connections.
625840	628880	But the software and the learning is also important.
628880	631720	I mean, you could have a trillion connections, but
631720	635760	if you didn't have something to learn from, it wouldn't be very effective.
635760	639600	So we actually have to be able to collect all this data.
639600	641960	So we do it on the web and so on.
641960	647400	I mean, we've been collecting stuff on the web for several decades.
648840	657680	That's really what we're depending on to be able to train these large language
657680	659000	models.
659000	662720	And we shouldn't actually call them large language models,
662720	665400	because they deal with much more than language.
665400	669360	I mean, it's language, but you can add pictures.
669360	677400	You can add things that affect disease that have nothing to do with language.
677400	686600	In fact, we're using now simulated biology to be able to simulate
686600	694200	different ways to affect disease that have nothing to do with language.
694200	697040	So they really should be called large event models.
698640	702440	Do you think there's anything that happens inside of our brains that cannot
702440	704480	be captured by computation and by math?
705680	707720	No, I mean, what would that be?
707720	712520	I mean, okay, quick poll of the audience.
712520	714720	Raise your hand if you think there's something in your brain that cannot be
714720	717840	captured by computation or math, like a soul.
719240	721960	All right, so convince them that they're wrong, right?
721960	728120	I mean, consciousness is very important, but it's actually not scientific.
728120	732160	There's no way I could slide somebody in and the light will go on.
732160	733240	This one's conscious.
733240	734240	No, this one's not.
736000	741160	It's not scientific, but it's actually extremely important.
743600	746680	And another question, why am I me?
746680	748880	How come what happens to me?
748880	752640	I'm conscious of, and I'm not conscious of what happens to you.
754640	760040	These are deeply mysterious things, but it's really not conscious.
760040	764080	So Marvin Minsky, who was my mentor for 50 years, he said,
764080	767320	it's not scientific and therefore we shouldn't bother with it.
767320	771360	And any discussion of consciousness he would kind of dismiss.
772520	778040	But he actually did, his reaction to people was totally dependent on whether he
778040	780000	felt they were conscious or not.
780000	783600	So he actually did use that.
783600	788160	But it's not something that we're ignoring because there's no way to tell
788160	789760	whether something's conscious.
791760	795360	And that's not just something that we don't know and we'll discover.
795360	798760	There's really no way to tell whether or not something's conscious.
798760	800960	What do you mean, this is not conscious?
800960	803040	And the gentleman sitting right there is conscious.
803040	803720	I'm pretty confident.
803720	804720	Well, how do you prove that?
804720	814400	I mean, we kind of agree with humans, that humans are conscious.
814400	816280	Some humans are conscious, not all humans.
816280	821960	But how about animals?
821960	823720	We have a big disagreement.
823720	829920	Some people say animals are not conscious and other people think animals are conscious.
829920	832920	Maybe some animals are conscious and others are not.
832920	835400	There's no way to prove that.
835400	839320	Let's, okay, I want to run down this consciousness question.
839320	843080	But before we do that, I want to make sure I understood your previous answer correctly.
843080	849600	So the feeling I get of being in love or the feeling,
849600	856280	any emotion that I get could eventually be represented in math in a large language model.
856280	861520	Yeah, I mean certainly the behavior, the feelings that you have if you're with
861520	868560	somebody that you love is definitely dependent on what the connections do.
868560	872440	You can tell whether or not that's happening.
872440	880040	All right, and back to, is everybody here convinced?
880040	880960	Not entirely.
880960	881720	All right, we'll close it up.
881720	885760	So you don't think that it's worth trying to define consciousness.
885760	889960	I mean, you spend a fair amount in your book giving different arguments about what consciousness means.
889960	896240	But it seems like you're arguing on stage that we shouldn't try to define it.
896240	898960	There's no way to actually prove it.
898960	901280	I mean, we have certain agreements.
901280	902760	I agree that all of you are conscious.
902760	904480	You actually made it into this room.
904480	912840	So that's a pretty good indication that you're conscious, but that's not a proof.
912840	918320	And there may be human beings that don't seem quite conscious at the time.
918320	921760	Are they conscious or not, and animals?
921760	926280	I mean, I think elephants and whales are conscious, but not everybody agrees with that.
926280	932720	So at what point can we then essentially, how long will it be until we can essentially
932720	940120	download the entire contents of your brain and express it through some kind of a machine?
940120	945760	That's actually an important question because we're going to talk about longevity.
945760	950480	We're going to get to a point where we have longevity escape velocity, and it's not that
950480	951480	far away.
951480	955200	I think if you're diligent, you'll be able to achieve that by 2029.
955200	960560	That's only five or six years from now.
960560	966680	And so right now you go through a year, use up a year of your longevity, but you get back
966680	970120	from scientific progress right now about four months.
970120	973720	But that scientific progress is on an exponential curve.
973720	979080	It's going to speed up every year, and by 2029, if you're diligent, you'll use up a
979080	983960	year of your longevity with the year passing, but you'll get back a full year.
983960	988800	And past 2029, you'll get back more than a year, so you'll actually go backwards in
988800	989800	time.
989800	998880	Now, that's not a guarantee of infinite life because you could have a 10-year-old and you
998880	1005800	could compute his longevity as many, many decades, and he could die tomorrow.
1005800	1010320	But what's important about actually capturing everything in your brain, we can't do that
1010320	1015720	today, and we won't be able to do that in five years, but you will be able to do that
1015720	1020000	by the singularity, which is 2045.
1020000	1023360	And so at that point, you can actually go inside the brain and capture everything in
1023360	1024360	there.
1025040	1032080	Now, your thinking is going to be a combination of the amount you get from computation, which
1032080	1037600	will add to your thinking, and that's automatically captured.
1037600	1046960	I mean, right now, anything that you have in a computer is automatically captured today,
1046960	1051680	and the kind of additional thinking we'll have by adding to our brain, that will be
1051680	1062400	captured, but the connections that we have in the brain that we start with, we'll still
1062400	1064800	have that.
1064800	1067600	That's not captured today, but that will be captured in 2045.
1067600	1072720	We'll be able to go inside the brain and capture that as well, and therefore, we'll
1072720	1078200	actually capture the entire brain, which will be backed up.
1078240	1084440	So even if you get wiped out, you walk into a bomb and it explodes, we can actually recreate
1084440	1088520	everything that was in your brain by 2045.
1088520	1091960	That's one of the implications of the singularity.
1091960	1103560	Now, that doesn't absolutely guarantee because, I mean, the world could blow up and all the
1104520	1113680	computer, all the things that computers could blow up, and so you wouldn't be able to recreate
1113680	1115560	that.
1115560	1120800	We never actually get to a point where we absolutely guarantee that you live forever,
1120800	1128840	but most of the things that right now would upset capturing that will be overcome by that
1128840	1129840	time.
1129840	1133080	There's a lot there, right?
1133120	1135400	Let's start with escape velocity.
1135400	1139800	So do you think that anybody in this audience, in their current biological body, will live
1139800	1141680	to be 500 years old?
1141680	1144520	You're using me?
1144520	1145520	Yeah.
1145520	1146520	Absolutely.
1146520	1147520	And who?
1147520	1152960	I mean, if you're going to be alive in five years, and I imagine all of you will be alive
1152960	1153960	in five years.
1153960	1154960	No, five.
1154960	1155960	Oh, okay.
1155960	1160720	If they're alive for five years, they will likely live to be 500 years old.
1160720	1164920	If they're diligent, and I think the people in this audience will be diligent.
1164920	1165920	Wow.
1165920	1166920	All right.
1166920	1169520	Well, you can drink whatever you want as long as you don't get run over tonight because
1169520	1171560	you don't have to worry about decline.
1171560	1173000	All right.
1173000	1174560	So let me ask a question.
1174560	1177840	I want to get, we're going to spend a lot of time on what the singularity is, what it
1177840	1180400	means, and what it'll be like, but I want to ask some questions that will lead us up
1180400	1181400	there.
1181400	1184600	So I'm going to take this question from Mark Sternberg and modify it slightly.
1184600	1190000	In the timeframe, AI will be able to do, or sufficiently sophisticated computers in your
1190000	1193320	argument, can do everything that the human brain can do.
1193320	1199880	What will they not be able to do in the next 10 years?
1199880	1207280	Well, one thing has to do with being creative.
1207280	1211880	Some people, they'll be able to do everything a human can do, but they're not going to be
1211880	1214640	able to create new knowledge.
1214640	1222160	That's actually wrong because we can simulate, for example, biology, and the Moderna vaccine,
1222160	1223660	for example.
1223660	1227560	We didn't do it the usual way, which is somebody sits down and thinks, well, I think this might
1227560	1233480	work, and then they try it out, and it takes years to try it out on multiple people, and
1233480	1236960	it's one person's idea about what might work.
1236960	1241720	They actually listed everything that might work, and there was actually several billion
1241720	1247640	different mRNA sequences, and they said, let's try them all, and they tried every single
1247640	1252440	one by simulating biology, and that took two days.
1252440	1257120	So one weekend, they tried out several billion different possibilities, and then they picked
1257120	1267320	the one that turned out to be the best, and that actually was the Moderna vaccine up until
1267320	1269320	today.
1270320	1272880	Now, they did actually test it on humans.
1272880	1280120	We'll be able to overcome that as well because we'll be able to test using simulated biology
1280120	1281120	as well.
1281120	1282760	They actually decided to test it.
1282760	1286400	It's a little bit hard to give up testing on humans.
1286400	1291880	We will do that, so you can actually try out every single one, pick the best one, and then
1291880	1298200	you can try out that by testing on a million simulated humans and do that in a few days
1298200	1299200	as well.
1299320	1304320	And that's actually the future of how we're going to create medications for diseases, and
1304320	1311720	there's lots of things going on now with cancer and other diseases that are using that.
1311720	1314320	So that's a whole new method.
1314320	1316440	This is actually starting now.
1316440	1318360	Started right here with the Moderna vaccine.
1318360	1326920	We did another cure for a mental disease.
1326960	1330560	It's actually now in stage three trials.
1330560	1335000	That's going to be how we create medications from now on.
1335000	1336000	What are the frontiers?
1336000	1337000	What can we not do?
1337000	1342920	So that's where a computer is being creative, and it's not just actually trying something
1342920	1345360	that occurs to it.
1345360	1349040	It makes a list of everything that's possible and tries it all.
1349040	1355040	Is that creativity or is that just brute force with maximum capability?
1355040	1359640	It's much better than any other form of creativity.
1359640	1363720	And yes, it's creative because you're trying out every single possibility, and you're doing
1363720	1367640	it very quickly, and you come up with something that we didn't have before.
1367640	1370640	I mean, what else would creativity be?
1370640	1371640	All right.
1371640	1373680	So we're going to cross the frontier of creativity.
1373680	1375440	What will we not cross?
1375440	1378000	What are the challenges that will be outstanding the next 10 years?
1378000	1382600	Well, we don't know everything, and we haven't gone through this process.
1382600	1389520	It does require some creativity to imagine what might work, and we have to also be able
1389520	1395520	to simulate it in a biochemical simulator.
1395520	1402480	So we actually have to figure that out, and we'll be using people for a while to do that.
1402480	1404760	So we don't know everything.
1404760	1408800	I mean, to be able to do everything a human being can do is one thing, but there's so
1408800	1416400	much we don't know that we want to find out, and that requires creativity.
1416400	1424240	That will require some kind of human creativity working with machines.
1424240	1425240	All right.
1425240	1428400	Let's go back to what's going to happen to get us to the singularity.
1428400	1431880	So clearly, we have the chart that you showed on the power of compute.
1431880	1436960	It's been very steady, moving straight up on a logarithmic scale on a straight line.
1436960	1441840	There are a couple of other elements that you think are necessary to get to the singularity.
1441840	1446720	One is the rise of nanobots, and the other is the rise of brain-machine interfaces.
1446720	1450760	And both of those have gone more slowly than AI.
1450760	1452440	So convince the audience that.
1452440	1460000	Well, it would be slow, because any time you affect the human body, a lot of people are
1460000	1463640	going to be concerned about it.
1463640	1471560	If we do something with computers, we have a new algorithm, or we increase the speed
1471560	1479720	of it, nobody really is concerned about it.
1479720	1481520	You can do that.
1481520	1484840	Nobody cares about any dangers in it.
1484840	1487760	I mean, that's the reality.
1487760	1489560	There's some dangers that people care about.
1489560	1490560	Yes.
1490560	1492200	But it goes very, very quickly.
1492480	1495840	There's one of the reasons it goes so fast.
1495840	1501960	But if you're affecting the body, we have all kinds of concerns that it might have affected
1501960	1502960	negatively.
1502960	1505440	And so we want to actually try it on people.
1505440	1512080	But the reason brain-machine interfaces haven't moved in an exponential curve isn't just
1512080	1517240	because, you know, lots of people are concerned about the risk to humans.
1517680	1522120	I mean, as you explain in the book, they just don't work as well as they could.
1526000	1530560	If we could try things out without having to test it, it would go a lot faster.
1530560	1532640	I mean, that's the reason it goes slowly.
1535240	1545800	But there's some thought now that we can actually figure out what's
1545800	1550880	going on inside the brain and put things into the brain without actually going inside
1550880	1551880	the brain.
1551880	1554720	We wouldn't need something like brain link.
1554720	1561160	We could just, I mean, there's some tests where we can actually tell what's going on
1561160	1565600	in the brain without actually putting something inside the brain.
1565600	1570800	And that might actually be a way to do this much more quickly.
1570800	1575520	But your prediction about the singularity depends, maybe I'm reading it wrong, not just
1575520	1580000	on the continued exponential growth of compute, but on solving this particular problem, too,
1580000	1581000	right?
1585000	1592160	Yes, because we want to increase the amount of intelligence that humans can command, so
1592160	1598560	we have to be able to marry the best computers with our actual brain.
1598560	1599800	And why do we have to do that?
1599800	1602760	Because like right now, here I have my phone.
1602760	1604520	In some ways, this augments my intelligence.
1604520	1605520	It's wonderful.
1605520	1606520	But it's very slow.
1606520	1611880	I mean, if I ask you a question, you're going to have to type it in or speak it and it takes
1611880	1612880	a while.
1612880	1618320	I mean, I ask a question and then people fool around with their computer, it might take
1618320	1620960	15 seconds or 30 seconds.
1620960	1624960	It's not like it just goes right into your brain.
1624960	1626920	I mean, these are very useful.
1626920	1627920	These are brain extenders.
1628320	1632520	We didn't have these a little while ago.
1632520	1637160	Generally in my talks, I ask people who here has their phone.
1637160	1644440	I'll bet here, maybe there's one or two people, but everybody here has their phone.
1644440	1646520	That wasn't two, five years ago.
1646520	1649440	Definitely wasn't two, 10 years ago.
1649440	1655120	And it is a brain extender, but it does have some speed problems.
1655120	1658400	So we want to increase that speed.
1658400	1663280	A question could just come up where we're talking and the computer would instantly tell you
1663280	1669080	what the answer is without you having to fool around with an external device.
1669080	1672840	And that's almost feasible today.
1672840	1677640	And something like that would be helpful to do this.
1677640	1684200	But could you not get a lot of the good that you talk about if we just kept the problem
1684240	1686360	with connecting our brains to the machines.
1686360	1691240	Suddenly you're in this whole world, complicated privacy issues where stuff is being injected
1691240	1693600	in my brain, stuff in my brain is going elsewhere.
1693600	1697680	Like you're opening up a whole host of ethical, moral existential problems.
1697680	1701560	Can't you just make the phones a lot better?
1701560	1707280	Well, that's the idea that we can do that without having to go inside your brain, but
1707280	1713280	to be able to tell what's going on in your brain externally without going inside the
1713280	1717800	brain with some kind of device.
1717800	1719480	All right, well let's keep moving into the future.
1719480	1722160	So we're moving into the future with exponential growth in the computer.
1722160	1727080	We solve a way of ideally figuring out how to communicate directly with your brain to
1727080	1728320	speed things up.
1728320	1732560	Explain why nanobots are essential to your vision of where we're going.
1732560	1737040	Well if you really want to tell what's going on inside the brain, you've got to be able
1737080	1743080	to go at the level of the particles in the brain so we can actually tell what they're
1743080	1750080	doing and that's feasible.
1750080	1755000	We can't actually do it, but we can show that it's feasible.
1755000	1760400	And that's one possibility.
1760400	1765400	We're actually hoping that you could do this without actually affecting the brain at all.
1765760	1771320	Okay, all right, so we're pushing ahead.
1771320	1775200	We've got nanobots that run around inside of our brain, they're understanding ahead,
1775200	1778320	they're extracting thoughts, they're inputting thoughts.
1778320	1782160	Let's go to this nice question, which fits in lovely, from Luis Condreva.
1782160	1787160	What are the five main ethical questions that we will face as that happens?
1787160	1793160	Um, is four enough?
1793920	1796920	Four is fine.
1796920	1802920	There might even be six Ray, but you can give us four.
1807920	1814920	I mean we're going to have a lot more power if we can actually, with our own brain, control
1814920	1821920	computers, does that give people too much power?
1824160	1831160	Also, I mean right now we talk about having a certain amount of value based on your talent.
1839520	1846520	This will give talent to people who otherwise don't have talent, and talent won't be as
1846520	1853520	important, because you'll be able to gain talent just by merging with the right kind
1854080	1859080	of large language model, or whatever we call them.
1859080	1866080	And it also seems kind of arbitrary why we would give more power to somebody who has
1866800	1873800	more talent, because they didn't create that talent, it just happened to happen.
1874800	1881800	And, what everybody says, we should give somebody who has talent in an area more power.
1886160	1893160	This way you'd be able to gain talent, just as in the matrix you could learn to fly an
1893160	1900160	air helicopter, just by downloading the right software, as opposed to spending a lot of
1900160	1907160	time doing that. Is that fair, or unfair? I mean I think that would fall into the ethical
1914040	1921040	challenge area. And it's not like we get to the end of this, and say okay this is finally
1921040	1926040	what the singularity is all about, and people can do certain things, and they can't do other
1926040	1931040	things, but it's over. We'll never get to that point. I mean this curve is going to
1931040	1938040	continue, the other curve is going to continue indefinitely. And we've actually shown, for
1940560	1947560	example, with nanotechnology we can create a computer, and we can create a computer,
1951960	1958960	where one liter computer would actually match the amount of power that all human beings
1959280	1966280	today have, like 10 to the 10th persons, would all fit into one liter computer. Does that
1972320	1979320	create ethical problems? So I mean a lot of the implications kind of run against what
1980320	1987320	we've been assuming about human beings. Wait, on the talent question, which is super
1987320	1994320	interesting, do you feel like everybody, when we get to 2040, will have equal capacities?
1996680	2002400	I think it will be more different, because we'll have different interests. You might
2002400	2008200	be into some fantastic type of music, and I might be into some kind of literature or
2008240	2015240	something else. We're going to have different interests, and so we'll excel at certain
2016320	2022280	things depending on what your interests are. So it's not like we all have the same amount
2022280	2027800	of power, but we'll all have fantastic power compared to what we have today.
2027800	2030680	And if you're in Texas, where there are no regulations, you'll probably get it first
2030680	2032080	instead of you in Massachusetts.
2032080	2033280	Exactly, yeah.
2034280	2038040	Let me ask you another ethical question while we're on this one. So about a few minutes
2038040	2043440	ago you mentioned the capacity to replicate someone's brain and bring them back. So let's
2043440	2049320	say I do that with my father, passed away six years ago, sadly. I bring him back, and
2049320	2055240	I'm able to create a mind and a body just like my father's. It's exact, perfect replica,
2055240	2061480	all those thoughts. What happens to all the bills that he owed when he died? Because that's
2061520	2064640	a lot of money, and a lot of bill collectors call me. Do we have to pay those off, or are
2064640	2066440	we good?
2066440	2073440	Well, we're doing something like that with my daughter, and you can read about this in
2076080	2082840	her book, and it's also in my book. We collected everything my father had written. He died
2082880	2089880	when I was 22, so he's been dead for more than 50 years. And we fed that into a large
2093520	2100520	language model, and basically answered the question of all the things he ever wrote,
2101880	2108000	what best answers this question? And then you could put any question you want, and then
2108040	2112840	you could talk to it. You'd say something, you then go through everything he ever had
2112840	2119200	written and find the best answer that he actually wrote to that question. And it actually was
2119200	2125200	a lot of like talking to him. You could ask him what he liked about music. He was a musician.
2125200	2132200	He actually liked Brahms the best. And it was very much like talking to him. And I
2133200	2140200	reported on this in my book, and Amy talks about this in her book. And Amy actually asked
2142560	2148760	the question, could I fall in love with this person, even though I've never met him? And
2148760	2153160	she does a pretty good job. I mean, you really do fall in love with this character that she
2153160	2160160	creates, even though she never met him.
2162920	2168400	So we can actually, with today's technology, do something where you can actually emulate
2168400	2175400	somebody else. And I think as we get further on, we can actually do that more and more
2175400	2182040	responsibly and more and more that really would match that person, and actually emulate
2182040	2185280	the way he would move and so on. It's a tone of voice.
2185280	2188880	Well, you know, my dad, he loved Brahms too, particularly those piano trios. So if we can
2188920	2193920	solve the back taxes problem, we'll get my dad and your dad's bots. Hang out. It would
2193920	2194920	be great.
2194920	2197920	Well, yeah. That would be cool.
2197920	2198920	All right.
2198920	2204920	All right. We've got 20 minutes left. I want to get to the thing that I most want to understand,
2204920	2207880	because by the way, this book is wonderful. I think you guys are all going to get signed
2207880	2212280	copies of it when it comes out. It's truly remarkable, as are all of Ray's books. Whether
2212280	2215400	you agree or disagree, they'll definitely make you think more.
2215440	2220600	One of the things that I don't think you do in this book is describe what a day will
2220600	2227600	be like in 2045 when we're all much more intelligent. So it's 2045. We're all million
2227760	2232760	times as intelligent. I wake up. Do I have breakfast or do I not have breakfast?
2232760	2239760	Well, the answer to that question is kind of the same as it is now. But first of all,
2246400	2253400	the reason it's called a singularity is because we don't really fully understand that question.
2256040	2262640	Singularity is borrowed from physics. Singularity in physics is where you have a black hole
2262640	2266600	and no light can escape. And so you can't actually tell what's going on inside the black
2266600	2272960	hole. And so we call it a singularity, a physical singularity. So this is a historical
2273000	2280000	singularity where we're borrowing that term from physics and call it a singularity. Because
2280120	2284560	we can't really answer the question. If we actually multiply our intelligence a million
2284560	2290680	fold, what's that like? It's a little bit like asking a mouse, gee, what would it be
2290680	2297520	like if you had the amount of intelligence of this person? The mouse wouldn't really
2297640	2304640	even understand the question. It does have intelligence. It has a fair amount of intelligence.
2304880	2310400	But it couldn't understand that question. It couldn't articulate an answer. That's a
2310400	2317000	little bit what it would be like for us to take the next step in intelligence by adding
2317000	2319680	all the intelligence that the singularity would provide.
2319680	2321240	Wait, I just want to make sure I understand.
2321240	2328240	But I'll give you one answer. I said if you're diligent, you'll achieve longevity escape
2329280	2336280	velocity in five or six years. And if we want to actually emulate everything that's going
2336280	2343280	on inside the brain, let's go out a few more years. Let's say the 2040, 2045. Now there's
2354800	2360800	a lot, you talk to a person, they've got all the connections that they had originally
2360800	2367800	plus all the seditional connections that we add through having them access computers and
2369640	2376640	that becomes part of their thinking. So can you, suppose a person like blows up or something
2379480	2386480	happens to their mind, you definitely can recreate everything that's of a computer origin
2387480	2393800	because we do that now. Any time we create anything with a computer, it's backed up.
2393800	2400880	So if the computer goes away, you've got the backup and you can recreate it. It says okay,
2400880	2407880	but what about the thinking in their normal brain that's not done with computers? We don't
2408240	2415240	have some ways of backing that up. When we get to the singularity with 2045, we'll be
2416240	2422040	able to back that up as well because we'll be able to figure out, we'll have some ways
2422040	2429040	of actually figuring out what's going on in that sort of non-mechanical brain. So we'll
2429640	2436640	be able to back up both their normal brain as well as the computer addition. And I believe
2442880	2445680	that's feasible by 2045.
2445680	2448280	In your vision of it?
2448280	2453920	So you can back up their entire brain. Now that doesn't guarantee, the whole world could
2454000	2460000	blow up and lose all the data centers. So it's not absolute guarantee.
2460000	2467000	Yeah, I'll be ashamed. What I don't understand is will we even be fully distinct people? If
2467000	2473160	we're sharing memories and we're all uploading our brains to the cloud and we're getting
2473160	2480000	all this information coming back directly into our neocortex, are we still distinct?
2480800	2487800	Yes, but we could also find new ways of communicating. So the computers that extend my brain, the
2492480	2498840	right computers that extend your brain, we could create something that's like a hybrid
2498840	2504280	or not. And it would be up to our own decision as to whether or not to do that. So there'll
2504280	2507880	be some new ways of communicating.
2507880	2511560	Let me ask another question about this. This is what, when I was reading the book, this
2511560	2516800	is where I kept getting stuck. You are extremely optimistic. You're optimistic about where
2516800	2521600	we are today. You're optimistic that technology has been a massive force for good. You're
2521600	2526880	optimistic that it will continue to be a massive force for good. Yet there is a lot of uncertainty
2526880	2528560	in the future you were describing.
2528560	2535560	Well, first of all, I'm not necessarily optimistic. The things that can go wrong are
2538160	2545160	we had things that can go wrong before we had computers. When I was a child, atomic
2547600	2554600	weapons were created and people were very worried about an atomic war. We would actually
2556560	2562360	get under our desk and put our hands behind our head to protect us against an atomic war.
2563320	2570320	It seemed to work, actually. It was still here. But if you had asked people, we had actually
2571000	2578000	two weapons that went off in anger and killed a lot of people within a week. And if you
2578400	2582200	had asked people, what's the chance that we're going to go another 80 years and this will
2582200	2589200	never happen again, nobody would say that that was true. But it has happened. That doesn't
2590200	2597200	mean it's not going to happen next week. But anyway, that's a great danger. And I think
2598800	2605800	that's a much greater danger than computers are. Yes, they're dangers, but the computers
2605920	2612920	will also be more intelligent to avoid kinds of dangers. Yes, it's some bad people in the
2613920	2620920	world, but I mean, go back 80, 90 years, we had 100 million people die in Asia and Europe
2624920	2631920	from World War II. We don't have wars like that anymore. We could. We certainly have the
2634000	2641000	atomic weapons to do that. And you could also imagine computers could be involved with that.
2643800	2650800	But if you actually look, and this goes right through one piece. First of all, if you look
2651040	2658040	at my lineage of computers going from tiny fraction of one calculation to 65 billion,
2660280	2667280	that's a 20 quadrillion fold increase that we've achieved in 80 years. And look at this,
2667280	2674280	the U.S. personal income, this is done in constant dollars, so this has nothing to do
2674360	2681360	with inflation. And this is the average income in the United States. It went, it's multiplied
2686880	2693880	by about 100 fold. And we live far more successfully than we used to. And I think that's a great
2697520	2702520	question. And we live far more successfully. People say, oh, things were great 100 years
2702520	2708520	ago, they weren't. And you can look at this chart. I've got 50 charts in the book that
2708520	2714640	show the kind of progress we've made. A number of people that live in dire poverty has gone
2714640	2720640	down dramatically. We actually did a poll where they asked people, people that live in poverty
2721080	2727580	as they've gone up or down, 80 percent said it's gone up. But the reality is it's actually
2727580	2734580	fallen by 50 percent in the last 20 years. So what we think about the past is really
2744840	2750240	the opposite of what's happened. Things have gotten far better than they have. And computers
2750320	2754520	are going to make things even better. I mean, just the kind of things you can do now with
2754520	2757600	a large language model, it didn't exist two years ago.
2757600	2764360	Do you ever worry that take it as a given, if yours have made things better, take it
2764360	2768360	as a given, that personal income will keep going up? Do you ever worry it's just coming
2768360	2772840	too quickly and it'll be better if maybe the slope of the Kurzweil curve was a little
2772840	2773320	less steep?
2773320	2779840	That's a big difference in the past. I mean, talk about what effect did the railroad have.
2780440	2787440	I mean, lots of jobs were lost or even the cotton genie that happened 200 years ago. And
2787520	2793400	people were quite happy making money with the cotton genie and suddenly that was gone
2793400	2798440	and machines were doing that. And people say, well, wait till this gets going, all jobs
2798440	2805440	will be lost. And that's actually what was said at that time. But actually, income went
2805440	2812440	up, more and more people worked. We created, and if you say, well, what are they going
2813080	2819280	to do? You couldn't answer that question because it was in industries that nobody had a clue
2819280	2826280	of, like for example, all of electronics. So things are getting better even if jobs
2826720	2833720	are lost. Now, you can certainly point to jobs, like take computer programming. Google
2839760	2846760	has, I don't know, 60,000 people that program computers and lots of other companies do.
2847560	2853760	At some point, that's not going to be a feasible job. They can already code, lots of language
2853800	2860800	models can write code, not quite the way an expert programmer can do. But how long is
2860800	2867800	that going to take? It's measured in years, not in decades. Nonetheless, I believe that
2870320	2877320	things will get better because we wipe out jobs, but we create other ways of having an
2878320	2885320	income. And if you actually point to something, let's say this machine, and this is being
2887440	2893320	worked on, can wash dishes. You just have a bunch of dishes that will pick the ones that
2893320	2900320	have to go in the dishwasher and clean everything else up, and that will wash dishes for you.
2901320	2907320	Would we want that not to happen? Would we say, well, this is kind of upsetting things,
2907320	2914320	let's get rid of it. It's not going to happen, and no one would advocate that. So we'll
2916600	2922600	find things to do, we'll have other methods of distributing money, and it will be, it
2922600	2927600	will continue these kinds of curves that we've seen already.
2928360	2934360	This is kind of remarkable that we got large language models before we got robotic dishwashers.
2934360	2941360	You have grandchildren. What would you tell a young person? They buy in, they agree, or
2942440	2948240	how would you tell them to best prepare themselves for what will be, if you're correct, a remarkably
2948240	2951240	different future?
2951240	2957520	I'd be less concerned about what will make money, and much more concerned about what
2957560	2964560	turns them on. They love video games, so they should learn about that. They should read
2967720	2973720	literature that turns them on. Some of those literature in the future will be created by
2973720	2980720	computers. And find out what in the world has a positive effect on their mental being.
2980720	2987720	And if you know that your child or your grandchild, this gets to one of the questions that is asked
2991680	2997480	on the screen here, if you know that someone is going to live for hundreds of years, as
2997480	3001680	you predict, how does that affect the way, certainly it means they shouldn't retire
3001680	3006440	at 65, but what else does it change about the way they should think about their lives?
3006840	3013840	Well, I talk to people and say, well, I wouldn't want to live past 100, or maybe they're a
3013840	3020840	little more ambitious to say, I don't want to live past 110. But if you actually look,
3022680	3029200	when people decide they've had enough and they don't want to live anymore, that never,
3029200	3035520	ever happens unless these people are in some kind of dire pain, in physical pain or emotional
3035880	3042880	pain or spiritual pain or whatever, and they just cannot bear to be alive anymore, nobody
3043120	3050120	takes their lives other than that. And if we can actually overcome many kinds of physical
3051920	3058920	problems, cancers wiped out and so on, which I expect to happen, people will be even that
3059920	3066920	much more happy to live and they'll want to continue to experience tomorrow, and tomorrow
3069200	3076200	is going to be better and better, these kinds of progress is not going to go away. So people
3077320	3084320	will want to live, unless they're in dire pain, but that's what the whole sort of medical
3084720	3091720	profession is about, which is going to be greatly amplified by tomorrow's computers.
3091720	3095720	Let me ask you a great question that has popped on the screen. This is from Colin McCabe.
3095720	3101520	AI is a black box. Nobody knows how it was built. How do you show that AI is trustworthy
3101520	3106020	to users who want to trust it, adopt it, and accept it, particularly if you're going to
3106020	3113020	upload it directly into your brain? Well, it's not sure that nobody knows how they work.
3113520	3119520	Right. Most people who are using a large language model don't know what data sense went into
3119520	3123520	it. They're things that happen in the transformer layer that even the architects don't understand.
3123520	3130020	Right. But we're going to learn more and more about that, and in fact how computers work
3130020	3137020	will be, I think, a very common type of talent that people want to gain. And ultimately we'll
3138020	3144520	have more trust of computers. I mean, large language models aren't perfect, and you can
3144520	3151520	ask the question and it can give you something that's incorrect. I mean, we've seen that
3153160	3160160	just recently. The reason we have these computers give you incorrect information is it doesn't
3160160	3166160	have the information to begin with, and it actually doesn't know what it doesn't know.
3166160	3173160	And that's actually something we're working on so that it knows, well, I don't know that.
3173160	3179160	That's actually very good if it can actually say that, because right now it will find the
3179160	3186160	best thing it knows. And if it's never trained on that information and there's nothing in
3187160	3194160	there that tells you, it'll just give you the best guess, which could be very incorrect.
3194160	3200160	And we're actually learning to be able to figure out when it knows and when it doesn't know.
3200160	3207160	But ultimately we'll have pretty good confidence when it knows and when it doesn't know, and
3208160	3214160	we can actually rely on what it says.
3214160	3219160	So your answer to the question is, A, we will understand more, and B, they'll be much more
3219160	3223160	trustworthy, so it won't be as risky to not understand them.
3223160	3224160	Right.
3224160	3230160	You spent your life making predictions, some of which like the Turing test, you've held
3230160	3234160	on to them and been remarkably accurate. As you move from an overwhelming optimist to
3234160	3237160	a pessimist, what is a prediction?
3237160	3242160	Well, my books have always had a chapter on how these things can go wrong.
3242160	3249160	Tell me a prediction that you are chewing over right now, but you're not sure whether you
3249160	3255160	want to make it or whether you don't want to make it.
3255160	3262160	I mean, there's well-known dangers in nanotechnology. If someone were to create nanotechnology,
3263160	3270160	that replicates, well-known as if it replicates everything into paper clips, turn the entire
3273160	3278160	world into paper clips, that would not be positive.
3278160	3279160	No.
3279160	3281160	Unless you're staples.
3281160	3288160	And that's feasible. Take somebody who's...
3289160	3296160	a little bit mental to do that, but it could be done.
3296160	3303160	And we actually will have something that actually avoids that.
3303160	3312160	So we'll have something that can detect that this is actually turning everything into paper
3312160	3317160	clips and destroy it before it does that.
3317160	3325160	But I mean, I have a chapter in this new book, the Singularity is Nearer, that talks about
3325160	3327160	the kinds of things that could happen.
3327160	3331160	Oh, the most remarkable part of this book is he does exactly the mathematical calculations
3331160	3335160	on how long it would take nanobots to turn the world into grey goo and how long it would
3335160	3338160	take the blue goo to stop the grey goo. It's remarkable.
3338160	3341160	The book will be out soon. You definitely need to read until the end.
3341160	3345160	But this leads to a... maybe... let me try and answer it.
3345160	3350160	The question I asked before is what should young people think about and be working on?
3350160	3352160	You said their passions and what turns them on.
3352160	3360160	Shouldn't they be thinking through how to design and architect these future systems
3360160	3363160	so they are less likely to turn us into grey goo or paper clips?
3363160	3366160	I don't know if everybody wants to work on that.
3366160	3370160	But folks in this room, technologically minded, you guys should all be working on not turning
3370160	3372160	us into grey goo, right?
3372160	3374160	Yes, that would be on the list.
3374160	3380160	But then that leads to another question, which is what will the role of humans be in thinking
3380160	3384160	through that problem when they're only a millionth or a billionth or a trillionth as intelligent
3384160	3388160	as machines?
3388160	3389160	Say that again?
3389160	3393160	So we're going to have these really hard problems to solve.
3393160	3399160	Right now, we are, along with our machines, we can be extremely intelligent.
3399160	3404160	But 10 years from now, 15 years from now, there will be machines that will be so much more intelligent
3404160	3405160	than us.
3405160	3409160	What will our role... what will the role of humans be in trying to solve these problems?
3409160	3412160	Well, first of all, I see those as extensions of humans.
3412160	3415160	We wouldn't have them if we didn't have humans to begin with.
3415160	3418160	And humans have a brain that can think these things through.
3418160	3421160	And we have this thumb.
3421160	3423160	It's not really very much appreciated.
3423160	3428160	But like whales and elephants actually have a larger brain than we have.
3428160	3435160	And they can probably think deeper thoughts, but they don't have a thumb until they don't create technology.
3435160	3442160	A monkey can create... it actually has a thumb, but it's actually down an inch or so,
3442160	3444160	and therefore it really can't grab very well.
3444160	3450160	So it can create a little bit of technology, but the technology it creates cannot create other technology.
3450160	3456160	So the fact that we have a thumb means we can create integrated circuits
3456160	3463160	that can become a large language model that comes from the human brain.
3463160	3470160	And it's actually trained with everything that we've ever thought.
3470160	3479160	Anything that human beings have thought has been documented and it can go into these large language models.
3479160	3482160	And everybody can work on these things.
3482160	3486160	And it's not true only certain wealthy people will have it.
3486160	3490160	I mean, how many people here have phones?
3490160	3494160	If it's not 100%, it's like 99.9%.
3494160	3500160	And you don't have to be kind of from a wealthy group.
3500160	3505160	I mean, I see people who are homeless who have their own phone.
3505160	3509160	It's not that expensive.
3509160	3515160	And so that represents the distribution of these capabilities.
3515160	3519160	It's not something you have to be fabulously wealthy to afford.
3519160	3524160	So you think that we're heading into a future where we're going to live much longer and will be much more equal?
3524160	3530160	We think we're heading into a society where we'll live much longer, be wealthier, but also much more equality.
3530160	3533160	Yes, absolutely. And we've seen that already.
3533160	3541160	Well, we're at time, but Ray and I will be back in 21, 24, 22, 24, and 23, 24.
3541160	3546160	So thank you for coming today. Thank you so much. He is an American treasure. Thank you.
