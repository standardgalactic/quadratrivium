WEBVTT

00:00.000 --> 00:10.000
All right, I'm so excited to be here with you, Ray.

00:10.000 --> 00:16.680
It's great to be here, great to see everybody together, beautiful audience.

00:16.680 --> 00:20.800
So my favorite thing in that introduction of you is that you have been working in AI

00:20.800 --> 00:26.780
longer than any other human alive, which means if you live forever and will get to that,

00:26.780 --> 00:29.000
you will always have that distinction.

00:29.000 --> 00:32.800
I think that's right.

00:32.800 --> 00:36.480
Marvin Minsky was actually my mentor.

00:36.480 --> 00:41.560
If he were alive today, he would actually be more than 61 years.

00:41.560 --> 00:43.360
We're going to bring him back also.

00:43.360 --> 00:46.360
So maybe you'll not sure how we'll count the distinction then.

00:51.360 --> 00:53.040
All right, so we're going to fix the audio.

00:53.040 --> 00:55.560
But this is what we're going to do with this conversation.

00:55.600 --> 00:59.120
I'm going to start out asking Ray some questions about where we are today.

00:59.120 --> 01:00.520
We'll do that for a few minutes.

01:00.520 --> 01:06.120
Then we'll get into what has to happen to reach the singularity, so the next 20 years.

01:06.120 --> 01:08.640
Then we'll get into discussion about what the singularity is,

01:08.640 --> 01:10.360
what it means, how it would change our lives.

01:10.360 --> 01:12.600
And then at the end, we'll talk a little bit about how,

01:12.600 --> 01:15.560
if we believe this vision of the future, what it means for us today.

01:15.560 --> 01:17.280
Ask your questions, they'll come in.

01:17.280 --> 01:20.040
I'll ask them as they go in the different sections of the conversation.

01:20.040 --> 01:21.280
But let's get cracking.

01:21.320 --> 01:22.120
Can you hear me?

01:27.160 --> 01:28.000
You can't hear Ray.

01:31.000 --> 01:32.840
Well, this will be recorded.

01:32.840 --> 01:34.320
You guys are going to all live forever.

01:34.320 --> 01:35.520
There'll be plenty of time.

01:36.560 --> 01:38.280
It will be fine.

01:38.280 --> 01:39.440
I'm just going to get started.

01:39.440 --> 01:40.920
I assume the audio will get worked out.

01:40.920 --> 01:42.840
They do a fabulous job here at South by.

01:43.840 --> 01:46.040
I think they should be able to hear me and you.

01:49.280 --> 01:50.760
All right, we got this over on the right.

01:51.280 --> 01:56.640
Audio engineers, are we good to go?

01:58.640 --> 01:59.640
We're good to go.

01:59.640 --> 02:02.600
All right, first question, Ray.

02:03.880 --> 02:07.200
So, you've been working in AI for 61 years.

02:07.200 --> 02:08.640
Wait, can you hear me?

02:13.760 --> 02:16.520
So everybody in the front can hear you, but nobody in the back can hear you.

02:16.520 --> 02:17.560
Can you hear me now?

02:21.480 --> 02:22.960
All right, I'll speak louder.

02:25.160 --> 02:26.800
First question.

02:26.800 --> 02:29.960
So you've been living in the revolution for a long time.

02:29.960 --> 02:35.080
You've made lots of predictions, many of which have been remarkably accurate.

02:35.080 --> 02:39.880
We've all been living in a remarkable two year transformation

02:39.880 --> 02:42.560
with large language models a year and a half.

02:42.560 --> 02:46.720
What has surprised you about the innovations in large language models

02:46.720 --> 02:48.440
and what has happened recently?

02:48.440 --> 02:52.480
Well, I did finish this book a year ago

02:52.480 --> 02:55.200
and didn't really cover a large language model.

02:55.200 --> 03:10.760
So I delayed the book to cover that, but I was expecting that to happen

03:10.760 --> 03:12.520
like a couple of years later.

03:12.520 --> 03:18.640
I mean, I made a prediction in 1999 that it would happen by 2029.

03:19.960 --> 03:27.120
And we're not quite there yet, but it looks like it's maybe a year or two ahead of schedule.

03:31.560 --> 03:33.880
So that was maybe a bit of a surprise.

03:33.880 --> 03:38.920
Wait, you predicted back in 1999 that a computer would pass the Turing test in 2029.

03:38.920 --> 03:43.240
Are you revising that to something more closer to today?

03:46.240 --> 03:50.120
No, I'm still saying 2029.

03:52.720 --> 03:57.840
The definition of the Turing test is not precise.

03:57.840 --> 04:02.760
We're going to have people claiming that the Turing test has been solved

04:02.760 --> 04:06.280
and people are saying that GPT-4 actually passes it.

04:06.280 --> 04:09.880
Some people, so it's going to be like maybe two or three years

04:09.880 --> 04:13.920
where people start claiming and then they continue to claim

04:13.920 --> 04:16.120
and finally everybody will accept it.

04:16.120 --> 04:18.440
So it's not like it happens in one day.

04:18.440 --> 04:21.480
But you have a very specific definition of the Turing test.

04:21.480 --> 04:23.400
When do you think we'll pass that definition?

04:25.360 --> 04:28.200
Well, the Turing test is actually not that significant,

04:28.200 --> 04:38.080
because that means that a computer will pass for a human being.

04:38.080 --> 04:43.160
And what's much more important is AGI, Automatic General Intelligence,

04:43.160 --> 04:46.480
which means it can emulate any human being.

04:46.480 --> 04:52.440
So you have one computer and it can do everything that any human being can do.

04:52.440 --> 04:56.880
And that's also 2029, it all happens at the same time.

04:56.880 --> 04:57.960
But nobody can do that.

04:57.960 --> 05:02.440
I mean, just take an average large language model today.

05:02.440 --> 05:07.840
You can ask it anything and it will answer you pretty convincingly.

05:07.840 --> 05:10.240
No human being can do all of that.

05:10.240 --> 05:11.440
And it does it very quickly.

05:11.440 --> 05:15.480
It will write a very nice essay in 15 seconds.

05:15.480 --> 05:19.200
And then you can ask it again and it will write another essay.

05:19.200 --> 05:23.280
And no human being can actually perform at that level.

05:23.280 --> 05:26.560
Right, so you have to dumb it down to actually have a convincing Turing test.

05:26.560 --> 05:28.840
So having a Turing test, you have to dumb it down.

05:28.840 --> 05:31.120
Yeah, let me ask the first question from the audience,

05:31.120 --> 05:34.640
since I think it's quite relevant to where we are, which is Brian Daniel.

05:34.640 --> 05:37.080
Is the Kurzweil curve still accurate?

05:37.080 --> 05:37.600
Say again?

05:37.600 --> 05:40.640
Is the Kurzweil curve still accurate?

05:40.640 --> 05:43.720
Yes, in fact, can they see that?

05:43.720 --> 05:45.120
Let's pull the slides up, first slide.

05:48.680 --> 05:52.160
So this is an 80 year track record.

05:52.160 --> 05:54.560
This is an exponential growth.

05:54.560 --> 05:58.880
A straight line on this curve means exponential curvature.

06:01.240 --> 06:05.800
If it was sort of exponential, but not quite, it would curve.

06:05.800 --> 06:07.480
This is actually a straight line.

06:08.760 --> 06:20.200
It started out with a computer that did 0.0000007 calculations per second per

06:20.200 --> 06:21.600
constant dollar.

06:21.600 --> 06:23.760
That's the lower left hand corner.

06:23.760 --> 06:28.320
At the upper right hand corner, it's 65 billion calculations per second for

06:28.320 --> 06:30.040
the same amount of money.

06:31.760 --> 06:35.840
So that's why large language models have only been feasible for two years.

06:35.840 --> 06:39.560
We actually had large language models before that, but it didn't work very well.

06:41.840 --> 06:44.400
And this is an exponential curve.

06:44.400 --> 06:48.400
Technology moves in an exponential curve.

06:48.400 --> 06:59.000
We see that, for example, having renewable energy come from the sun and

06:59.000 --> 07:01.720
wind, that's actually an exponential curve.

07:03.640 --> 07:05.560
It's increased, it's gone.

07:06.720 --> 07:10.760
We've decreased the price by 99.7%.

07:12.280 --> 07:17.200
We've multiplied the amount of energy coming from solar energy a million fold.

07:18.520 --> 07:24.520
So this kind of curve really directs all kinds of technology.

07:27.600 --> 07:31.760
And this is the reason that we're making progress.

07:31.760 --> 07:36.160
I mean, we knew how to do large language models years ago, but

07:36.160 --> 07:37.760
we're dependent on this curve.

07:40.160 --> 07:41.160
And it's pretty amazing.

07:41.160 --> 07:47.920
It started out increasing relay speeds, then vacuum tubes, then integrated circuits.

07:47.920 --> 07:51.640
And each year, it makes the same amount of progress approximately

07:53.640 --> 07:56.080
regardless of where you are on this curve.

07:56.080 --> 07:59.960
We just added the last point, and it's, again,

08:01.320 --> 08:05.600
we basically multiply this by two every 1.4 years.

08:08.320 --> 08:12.480
And this is the reason that computers are exciting, but

08:12.480 --> 08:14.840
it actually affects every type of technology.

08:15.840 --> 08:18.680
And we just added the last point like two weeks ago.

08:20.800 --> 08:23.440
All right, so let me ask you a question.

08:23.440 --> 08:26.720
You wrote a book about how to build a mind.

08:26.720 --> 08:29.800
You have a lot about how the human mind is constructed.

08:29.800 --> 08:32.840
A lot of the progress in AI, AI systems are being built and

08:32.840 --> 08:34.520
what we understand about neural networks, right?

08:34.520 --> 08:39.280
So clearly our understanding of this helps with AI.

08:39.280 --> 08:42.840
In the last two years, by watching these large language models,

08:42.840 --> 08:45.400
have we learned anything new about our brains?

08:45.400 --> 08:48.800
Are we learning about the inside of our skulls as we do this?

08:48.800 --> 08:51.200
It really has to do with the amount of connections.

08:52.480 --> 08:56.840
The brain is actually organized fairly differently.

08:56.840 --> 09:00.040
The things near the eye, for example, deal with vision.

09:02.480 --> 09:06.000
And we have different ways of implementing different parts of the brain that

09:06.000 --> 09:07.520
remember different things.

09:07.520 --> 09:09.960
We actually don't need that.

09:09.960 --> 09:13.640
In large language model, all the connections are the same.

09:13.640 --> 09:16.840
We have to get the connections up to a certain point.

09:16.840 --> 09:19.560
If it approximately matches what the brain does,

09:19.560 --> 09:25.040
which is about a trillion connections, it will perform kind of like the brain.

09:25.040 --> 09:27.720
We're kind of almost at that point.

09:27.720 --> 09:34.480
Wait, so if the GPT-4 is 400 billion, the next ones will be a trillion or more.

09:34.480 --> 09:36.480
So the construction of these models,

09:36.480 --> 09:39.320
they are more efficient in their construction than our brains are.

09:41.040 --> 09:44.680
We make them to be as efficient as possible, but

09:44.680 --> 09:47.600
it doesn't really matter how they're organized.

09:47.600 --> 09:53.240
And we can actually create certain software that will actually expand

09:53.240 --> 09:57.440
the amount of connections more for the same amount of computation.

10:00.000 --> 10:03.800
But it really has to do with how many connections

10:06.480 --> 10:11.200
our particular computer is responsible for.

10:11.200 --> 10:18.360
So as we approach AGI, we're not looking for a new understanding of how to make

10:18.360 --> 10:19.480
these machines more efficient.

10:19.480 --> 10:22.520
The transformer architecture was clearly very important.

10:22.520 --> 10:25.840
We can really just get there with more connections.

10:25.840 --> 10:28.880
But the software and the learning is also important.

10:28.880 --> 10:31.720
I mean, you could have a trillion connections, but

10:31.720 --> 10:35.760
if you didn't have something to learn from, it wouldn't be very effective.

10:35.760 --> 10:39.600
So we actually have to be able to collect all this data.

10:39.600 --> 10:41.960
So we do it on the web and so on.

10:41.960 --> 10:47.400
I mean, we've been collecting stuff on the web for several decades.

10:48.840 --> 10:57.680
That's really what we're depending on to be able to train these large language

10:57.680 --> 10:59.000
models.

10:59.000 --> 11:02.720
And we shouldn't actually call them large language models,

11:02.720 --> 11:05.400
because they deal with much more than language.

11:05.400 --> 11:09.360
I mean, it's language, but you can add pictures.

11:09.360 --> 11:17.400
You can add things that affect disease that have nothing to do with language.

11:17.400 --> 11:26.600
In fact, we're using now simulated biology to be able to simulate

11:26.600 --> 11:34.200
different ways to affect disease that have nothing to do with language.

11:34.200 --> 11:37.040
So they really should be called large event models.

11:38.640 --> 11:42.440
Do you think there's anything that happens inside of our brains that cannot

11:42.440 --> 11:44.480
be captured by computation and by math?

11:45.680 --> 11:47.720
No, I mean, what would that be?

11:47.720 --> 11:52.520
I mean, okay, quick poll of the audience.

11:52.520 --> 11:54.720
Raise your hand if you think there's something in your brain that cannot be

11:54.720 --> 11:57.840
captured by computation or math, like a soul.

11:59.240 --> 12:01.960
All right, so convince them that they're wrong, right?

12:01.960 --> 12:08.120
I mean, consciousness is very important, but it's actually not scientific.

12:08.120 --> 12:12.160
There's no way I could slide somebody in and the light will go on.

12:12.160 --> 12:13.240
This one's conscious.

12:13.240 --> 12:14.240
No, this one's not.

12:16.000 --> 12:21.160
It's not scientific, but it's actually extremely important.

12:23.600 --> 12:26.680
And another question, why am I me?

12:26.680 --> 12:28.880
How come what happens to me?

12:28.880 --> 12:32.640
I'm conscious of, and I'm not conscious of what happens to you.

12:34.640 --> 12:40.040
These are deeply mysterious things, but it's really not conscious.

12:40.040 --> 12:44.080
So Marvin Minsky, who was my mentor for 50 years, he said,

12:44.080 --> 12:47.320
it's not scientific and therefore we shouldn't bother with it.

12:47.320 --> 12:51.360
And any discussion of consciousness he would kind of dismiss.

12:52.520 --> 12:58.040
But he actually did, his reaction to people was totally dependent on whether he

12:58.040 --> 13:00.000
felt they were conscious or not.

13:00.000 --> 13:03.600
So he actually did use that.

13:03.600 --> 13:08.160
But it's not something that we're ignoring because there's no way to tell

13:08.160 --> 13:09.760
whether something's conscious.

13:11.760 --> 13:15.360
And that's not just something that we don't know and we'll discover.

13:15.360 --> 13:18.760
There's really no way to tell whether or not something's conscious.

13:18.760 --> 13:20.960
What do you mean, this is not conscious?

13:20.960 --> 13:23.040
And the gentleman sitting right there is conscious.

13:23.040 --> 13:23.720
I'm pretty confident.

13:23.720 --> 13:24.720
Well, how do you prove that?

13:24.720 --> 13:34.400
I mean, we kind of agree with humans, that humans are conscious.

13:34.400 --> 13:36.280
Some humans are conscious, not all humans.

13:36.280 --> 13:41.960
But how about animals?

13:41.960 --> 13:43.720
We have a big disagreement.

13:43.720 --> 13:49.920
Some people say animals are not conscious and other people think animals are conscious.

13:49.920 --> 13:52.920
Maybe some animals are conscious and others are not.

13:52.920 --> 13:55.400
There's no way to prove that.

13:55.400 --> 13:59.320
Let's, okay, I want to run down this consciousness question.

13:59.320 --> 14:03.080
But before we do that, I want to make sure I understood your previous answer correctly.

14:03.080 --> 14:09.600
So the feeling I get of being in love or the feeling,

14:09.600 --> 14:16.280
any emotion that I get could eventually be represented in math in a large language model.

14:16.280 --> 14:21.520
Yeah, I mean certainly the behavior, the feelings that you have if you're with

14:21.520 --> 14:28.560
somebody that you love is definitely dependent on what the connections do.

14:28.560 --> 14:32.440
You can tell whether or not that's happening.

14:32.440 --> 14:40.040
All right, and back to, is everybody here convinced?

14:40.040 --> 14:40.960
Not entirely.

14:40.960 --> 14:41.720
All right, we'll close it up.

14:41.720 --> 14:45.760
So you don't think that it's worth trying to define consciousness.

14:45.760 --> 14:49.960
I mean, you spend a fair amount in your book giving different arguments about what consciousness means.

14:49.960 --> 14:56.240
But it seems like you're arguing on stage that we shouldn't try to define it.

14:56.240 --> 14:58.960
There's no way to actually prove it.

14:58.960 --> 15:01.280
I mean, we have certain agreements.

15:01.280 --> 15:02.760
I agree that all of you are conscious.

15:02.760 --> 15:04.480
You actually made it into this room.

15:04.480 --> 15:12.840
So that's a pretty good indication that you're conscious, but that's not a proof.

15:12.840 --> 15:18.320
And there may be human beings that don't seem quite conscious at the time.

15:18.320 --> 15:21.760
Are they conscious or not, and animals?

15:21.760 --> 15:26.280
I mean, I think elephants and whales are conscious, but not everybody agrees with that.

15:26.280 --> 15:32.720
So at what point can we then essentially, how long will it be until we can essentially

15:32.720 --> 15:40.120
download the entire contents of your brain and express it through some kind of a machine?

15:40.120 --> 15:45.760
That's actually an important question because we're going to talk about longevity.

15:45.760 --> 15:50.480
We're going to get to a point where we have longevity escape velocity, and it's not that

15:50.480 --> 15:51.480
far away.

15:51.480 --> 15:55.200
I think if you're diligent, you'll be able to achieve that by 2029.

15:55.200 --> 16:00.560
That's only five or six years from now.

16:00.560 --> 16:06.680
And so right now you go through a year, use up a year of your longevity, but you get back

16:06.680 --> 16:10.120
from scientific progress right now about four months.

16:10.120 --> 16:13.720
But that scientific progress is on an exponential curve.

16:13.720 --> 16:19.080
It's going to speed up every year, and by 2029, if you're diligent, you'll use up a

16:19.080 --> 16:23.960
year of your longevity with the year passing, but you'll get back a full year.

16:23.960 --> 16:28.800
And past 2029, you'll get back more than a year, so you'll actually go backwards in

16:28.800 --> 16:29.800
time.

16:29.800 --> 16:38.880
Now, that's not a guarantee of infinite life because you could have a 10-year-old and you

16:38.880 --> 16:45.800
could compute his longevity as many, many decades, and he could die tomorrow.

16:45.800 --> 16:50.320
But what's important about actually capturing everything in your brain, we can't do that

16:50.320 --> 16:55.720
today, and we won't be able to do that in five years, but you will be able to do that

16:55.720 --> 17:00.000
by the singularity, which is 2045.

17:00.000 --> 17:03.360
And so at that point, you can actually go inside the brain and capture everything in

17:03.360 --> 17:04.360
there.

17:05.040 --> 17:12.080
Now, your thinking is going to be a combination of the amount you get from computation, which

17:12.080 --> 17:17.600
will add to your thinking, and that's automatically captured.

17:17.600 --> 17:26.960
I mean, right now, anything that you have in a computer is automatically captured today,

17:26.960 --> 17:31.680
and the kind of additional thinking we'll have by adding to our brain, that will be

17:31.680 --> 17:42.400
captured, but the connections that we have in the brain that we start with, we'll still

17:42.400 --> 17:44.800
have that.

17:44.800 --> 17:47.600
That's not captured today, but that will be captured in 2045.

17:47.600 --> 17:52.720
We'll be able to go inside the brain and capture that as well, and therefore, we'll

17:52.720 --> 17:58.200
actually capture the entire brain, which will be backed up.

17:58.240 --> 18:04.440
So even if you get wiped out, you walk into a bomb and it explodes, we can actually recreate

18:04.440 --> 18:08.520
everything that was in your brain by 2045.

18:08.520 --> 18:11.960
That's one of the implications of the singularity.

18:11.960 --> 18:23.560
Now, that doesn't absolutely guarantee because, I mean, the world could blow up and all the

18:24.520 --> 18:33.680
computer, all the things that computers could blow up, and so you wouldn't be able to recreate

18:33.680 --> 18:35.560
that.

18:35.560 --> 18:40.800
We never actually get to a point where we absolutely guarantee that you live forever,

18:40.800 --> 18:48.840
but most of the things that right now would upset capturing that will be overcome by that

18:48.840 --> 18:49.840
time.

18:49.840 --> 18:53.080
There's a lot there, right?

18:53.120 --> 18:55.400
Let's start with escape velocity.

18:55.400 --> 18:59.800
So do you think that anybody in this audience, in their current biological body, will live

18:59.800 --> 19:01.680
to be 500 years old?

19:01.680 --> 19:04.520
You're using me?

19:04.520 --> 19:05.520
Yeah.

19:05.520 --> 19:06.520
Absolutely.

19:06.520 --> 19:07.520
And who?

19:07.520 --> 19:12.960
I mean, if you're going to be alive in five years, and I imagine all of you will be alive

19:12.960 --> 19:13.960
in five years.

19:13.960 --> 19:14.960
No, five.

19:14.960 --> 19:15.960
Oh, okay.

19:15.960 --> 19:20.720
If they're alive for five years, they will likely live to be 500 years old.

19:20.720 --> 19:24.920
If they're diligent, and I think the people in this audience will be diligent.

19:24.920 --> 19:25.920
Wow.

19:25.920 --> 19:26.920
All right.

19:26.920 --> 19:29.520
Well, you can drink whatever you want as long as you don't get run over tonight because

19:29.520 --> 19:31.560
you don't have to worry about decline.

19:31.560 --> 19:33.000
All right.

19:33.000 --> 19:34.560
So let me ask a question.

19:34.560 --> 19:37.840
I want to get, we're going to spend a lot of time on what the singularity is, what it

19:37.840 --> 19:40.400
means, and what it'll be like, but I want to ask some questions that will lead us up

19:40.400 --> 19:41.400
there.

19:41.400 --> 19:44.600
So I'm going to take this question from Mark Sternberg and modify it slightly.

19:44.600 --> 19:50.000
In the timeframe, AI will be able to do, or sufficiently sophisticated computers in your

19:50.000 --> 19:53.320
argument, can do everything that the human brain can do.

19:53.320 --> 19:59.880
What will they not be able to do in the next 10 years?

19:59.880 --> 20:07.280
Well, one thing has to do with being creative.

20:07.280 --> 20:11.880
Some people, they'll be able to do everything a human can do, but they're not going to be

20:11.880 --> 20:14.640
able to create new knowledge.

20:14.640 --> 20:22.160
That's actually wrong because we can simulate, for example, biology, and the Moderna vaccine,

20:22.160 --> 20:23.660
for example.

20:23.660 --> 20:27.560
We didn't do it the usual way, which is somebody sits down and thinks, well, I think this might

20:27.560 --> 20:33.480
work, and then they try it out, and it takes years to try it out on multiple people, and

20:33.480 --> 20:36.960
it's one person's idea about what might work.

20:36.960 --> 20:41.720
They actually listed everything that might work, and there was actually several billion

20:41.720 --> 20:47.640
different mRNA sequences, and they said, let's try them all, and they tried every single

20:47.640 --> 20:52.440
one by simulating biology, and that took two days.

20:52.440 --> 20:57.120
So one weekend, they tried out several billion different possibilities, and then they picked

20:57.120 --> 21:07.320
the one that turned out to be the best, and that actually was the Moderna vaccine up until

21:07.320 --> 21:09.320
today.

21:10.320 --> 21:12.880
Now, they did actually test it on humans.

21:12.880 --> 21:20.120
We'll be able to overcome that as well because we'll be able to test using simulated biology

21:20.120 --> 21:21.120
as well.

21:21.120 --> 21:22.760
They actually decided to test it.

21:22.760 --> 21:26.400
It's a little bit hard to give up testing on humans.

21:26.400 --> 21:31.880
We will do that, so you can actually try out every single one, pick the best one, and then

21:31.880 --> 21:38.200
you can try out that by testing on a million simulated humans and do that in a few days

21:38.200 --> 21:39.200
as well.

21:39.320 --> 21:44.320
And that's actually the future of how we're going to create medications for diseases, and

21:44.320 --> 21:51.720
there's lots of things going on now with cancer and other diseases that are using that.

21:51.720 --> 21:54.320
So that's a whole new method.

21:54.320 --> 21:56.440
This is actually starting now.

21:56.440 --> 21:58.360
Started right here with the Moderna vaccine.

21:58.360 --> 22:06.920
We did another cure for a mental disease.

22:06.960 --> 22:10.560
It's actually now in stage three trials.

22:10.560 --> 22:15.000
That's going to be how we create medications from now on.

22:15.000 --> 22:16.000
What are the frontiers?

22:16.000 --> 22:17.000
What can we not do?

22:17.000 --> 22:22.920
So that's where a computer is being creative, and it's not just actually trying something

22:22.920 --> 22:25.360
that occurs to it.

22:25.360 --> 22:29.040
It makes a list of everything that's possible and tries it all.

22:29.040 --> 22:35.040
Is that creativity or is that just brute force with maximum capability?

22:35.040 --> 22:39.640
It's much better than any other form of creativity.

22:39.640 --> 22:43.720
And yes, it's creative because you're trying out every single possibility, and you're doing

22:43.720 --> 22:47.640
it very quickly, and you come up with something that we didn't have before.

22:47.640 --> 22:50.640
I mean, what else would creativity be?

22:50.640 --> 22:51.640
All right.

22:51.640 --> 22:53.680
So we're going to cross the frontier of creativity.

22:53.680 --> 22:55.440
What will we not cross?

22:55.440 --> 22:58.000
What are the challenges that will be outstanding the next 10 years?

22:58.000 --> 23:02.600
Well, we don't know everything, and we haven't gone through this process.

23:02.600 --> 23:09.520
It does require some creativity to imagine what might work, and we have to also be able

23:09.520 --> 23:15.520
to simulate it in a biochemical simulator.

23:15.520 --> 23:22.480
So we actually have to figure that out, and we'll be using people for a while to do that.

23:22.480 --> 23:24.760
So we don't know everything.

23:24.760 --> 23:28.800
I mean, to be able to do everything a human being can do is one thing, but there's so

23:28.800 --> 23:36.400
much we don't know that we want to find out, and that requires creativity.

23:36.400 --> 23:44.240
That will require some kind of human creativity working with machines.

23:44.240 --> 23:45.240
All right.

23:45.240 --> 23:48.400
Let's go back to what's going to happen to get us to the singularity.

23:48.400 --> 23:51.880
So clearly, we have the chart that you showed on the power of compute.

23:51.880 --> 23:56.960
It's been very steady, moving straight up on a logarithmic scale on a straight line.

23:56.960 --> 24:01.840
There are a couple of other elements that you think are necessary to get to the singularity.

24:01.840 --> 24:06.720
One is the rise of nanobots, and the other is the rise of brain-machine interfaces.

24:06.720 --> 24:10.760
And both of those have gone more slowly than AI.

24:10.760 --> 24:12.440
So convince the audience that.

24:12.440 --> 24:20.000
Well, it would be slow, because any time you affect the human body, a lot of people are

24:20.000 --> 24:23.640
going to be concerned about it.

24:23.640 --> 24:31.560
If we do something with computers, we have a new algorithm, or we increase the speed

24:31.560 --> 24:39.720
of it, nobody really is concerned about it.

24:39.720 --> 24:41.520
You can do that.

24:41.520 --> 24:44.840
Nobody cares about any dangers in it.

24:44.840 --> 24:47.760
I mean, that's the reality.

24:47.760 --> 24:49.560
There's some dangers that people care about.

24:49.560 --> 24:50.560
Yes.

24:50.560 --> 24:52.200
But it goes very, very quickly.

24:52.480 --> 24:55.840
There's one of the reasons it goes so fast.

24:55.840 --> 25:01.960
But if you're affecting the body, we have all kinds of concerns that it might have affected

25:01.960 --> 25:02.960
negatively.

25:02.960 --> 25:05.440
And so we want to actually try it on people.

25:05.440 --> 25:12.080
But the reason brain-machine interfaces haven't moved in an exponential curve isn't just

25:12.080 --> 25:17.240
because, you know, lots of people are concerned about the risk to humans.

25:17.680 --> 25:22.120
I mean, as you explain in the book, they just don't work as well as they could.

25:26.000 --> 25:30.560
If we could try things out without having to test it, it would go a lot faster.

25:30.560 --> 25:32.640
I mean, that's the reason it goes slowly.

25:35.240 --> 25:45.800
But there's some thought now that we can actually figure out what's

25:45.800 --> 25:50.880
going on inside the brain and put things into the brain without actually going inside

25:50.880 --> 25:51.880
the brain.

25:51.880 --> 25:54.720
We wouldn't need something like brain link.

25:54.720 --> 26:01.160
We could just, I mean, there's some tests where we can actually tell what's going on

26:01.160 --> 26:05.600
in the brain without actually putting something inside the brain.

26:05.600 --> 26:10.800
And that might actually be a way to do this much more quickly.

26:10.800 --> 26:15.520
But your prediction about the singularity depends, maybe I'm reading it wrong, not just

26:15.520 --> 26:20.000
on the continued exponential growth of compute, but on solving this particular problem, too,

26:20.000 --> 26:21.000
right?

26:25.000 --> 26:32.160
Yes, because we want to increase the amount of intelligence that humans can command, so

26:32.160 --> 26:38.560
we have to be able to marry the best computers with our actual brain.

26:38.560 --> 26:39.800
And why do we have to do that?

26:39.800 --> 26:42.760
Because like right now, here I have my phone.

26:42.760 --> 26:44.520
In some ways, this augments my intelligence.

26:44.520 --> 26:45.520
It's wonderful.

26:45.520 --> 26:46.520
But it's very slow.

26:46.520 --> 26:51.880
I mean, if I ask you a question, you're going to have to type it in or speak it and it takes

26:51.880 --> 26:52.880
a while.

26:52.880 --> 26:58.320
I mean, I ask a question and then people fool around with their computer, it might take

26:58.320 --> 27:00.960
15 seconds or 30 seconds.

27:00.960 --> 27:04.960
It's not like it just goes right into your brain.

27:04.960 --> 27:06.920
I mean, these are very useful.

27:06.920 --> 27:07.920
These are brain extenders.

27:08.320 --> 27:12.520
We didn't have these a little while ago.

27:12.520 --> 27:17.160
Generally in my talks, I ask people who here has their phone.

27:17.160 --> 27:24.440
I'll bet here, maybe there's one or two people, but everybody here has their phone.

27:24.440 --> 27:26.520
That wasn't two, five years ago.

27:26.520 --> 27:29.440
Definitely wasn't two, 10 years ago.

27:29.440 --> 27:35.120
And it is a brain extender, but it does have some speed problems.

27:35.120 --> 27:38.400
So we want to increase that speed.

27:38.400 --> 27:43.280
A question could just come up where we're talking and the computer would instantly tell you

27:43.280 --> 27:49.080
what the answer is without you having to fool around with an external device.

27:49.080 --> 27:52.840
And that's almost feasible today.

27:52.840 --> 27:57.640
And something like that would be helpful to do this.

27:57.640 --> 28:04.200
But could you not get a lot of the good that you talk about if we just kept the problem

28:04.240 --> 28:06.360
with connecting our brains to the machines.

28:06.360 --> 28:11.240
Suddenly you're in this whole world, complicated privacy issues where stuff is being injected

28:11.240 --> 28:13.600
in my brain, stuff in my brain is going elsewhere.

28:13.600 --> 28:17.680
Like you're opening up a whole host of ethical, moral existential problems.

28:17.680 --> 28:21.560
Can't you just make the phones a lot better?

28:21.560 --> 28:27.280
Well, that's the idea that we can do that without having to go inside your brain, but

28:27.280 --> 28:33.280
to be able to tell what's going on in your brain externally without going inside the

28:33.280 --> 28:37.800
brain with some kind of device.

28:37.800 --> 28:39.480
All right, well let's keep moving into the future.

28:39.480 --> 28:42.160
So we're moving into the future with exponential growth in the computer.

28:42.160 --> 28:47.080
We solve a way of ideally figuring out how to communicate directly with your brain to

28:47.080 --> 28:48.320
speed things up.

28:48.320 --> 28:52.560
Explain why nanobots are essential to your vision of where we're going.

28:52.560 --> 28:57.040
Well if you really want to tell what's going on inside the brain, you've got to be able

28:57.080 --> 29:03.080
to go at the level of the particles in the brain so we can actually tell what they're

29:03.080 --> 29:10.080
doing and that's feasible.

29:10.080 --> 29:15.000
We can't actually do it, but we can show that it's feasible.

29:15.000 --> 29:20.400
And that's one possibility.

29:20.400 --> 29:25.400
We're actually hoping that you could do this without actually affecting the brain at all.

29:25.760 --> 29:31.320
Okay, all right, so we're pushing ahead.

29:31.320 --> 29:35.200
We've got nanobots that run around inside of our brain, they're understanding ahead,

29:35.200 --> 29:38.320
they're extracting thoughts, they're inputting thoughts.

29:38.320 --> 29:42.160
Let's go to this nice question, which fits in lovely, from Luis Condreva.

29:42.160 --> 29:47.160
What are the five main ethical questions that we will face as that happens?

29:47.160 --> 29:53.160
Um, is four enough?

29:53.920 --> 29:56.920
Four is fine.

29:56.920 --> 30:02.920
There might even be six Ray, but you can give us four.

30:07.920 --> 30:14.920
I mean we're going to have a lot more power if we can actually, with our own brain, control

30:14.920 --> 30:21.920
computers, does that give people too much power?

30:24.160 --> 30:31.160
Also, I mean right now we talk about having a certain amount of value based on your talent.

30:39.520 --> 30:46.520
This will give talent to people who otherwise don't have talent, and talent won't be as

30:46.520 --> 30:53.520
important, because you'll be able to gain talent just by merging with the right kind

30:54.080 --> 30:59.080
of large language model, or whatever we call them.

30:59.080 --> 31:06.080
And it also seems kind of arbitrary why we would give more power to somebody who has

31:06.800 --> 31:13.800
more talent, because they didn't create that talent, it just happened to happen.

31:14.800 --> 31:21.800
And, what everybody says, we should give somebody who has talent in an area more power.

31:26.160 --> 31:33.160
This way you'd be able to gain talent, just as in the matrix you could learn to fly an

31:33.160 --> 31:40.160
air helicopter, just by downloading the right software, as opposed to spending a lot of

31:40.160 --> 31:47.160
time doing that. Is that fair, or unfair? I mean I think that would fall into the ethical

31:54.040 --> 32:01.040
challenge area. And it's not like we get to the end of this, and say okay this is finally

32:01.040 --> 32:06.040
what the singularity is all about, and people can do certain things, and they can't do other

32:06.040 --> 32:11.040
things, but it's over. We'll never get to that point. I mean this curve is going to

32:11.040 --> 32:18.040
continue, the other curve is going to continue indefinitely. And we've actually shown, for

32:20.560 --> 32:27.560
example, with nanotechnology we can create a computer, and we can create a computer,

32:31.960 --> 32:38.960
where one liter computer would actually match the amount of power that all human beings

32:39.280 --> 32:46.280
today have, like 10 to the 10th persons, would all fit into one liter computer. Does that

32:52.320 --> 32:59.320
create ethical problems? So I mean a lot of the implications kind of run against what

33:00.320 --> 33:07.320
we've been assuming about human beings. Wait, on the talent question, which is super

33:07.320 --> 33:14.320
interesting, do you feel like everybody, when we get to 2040, will have equal capacities?

33:16.680 --> 33:22.400
I think it will be more different, because we'll have different interests. You might

33:22.400 --> 33:28.200
be into some fantastic type of music, and I might be into some kind of literature or

33:28.240 --> 33:35.240
something else. We're going to have different interests, and so we'll excel at certain

33:36.320 --> 33:42.280
things depending on what your interests are. So it's not like we all have the same amount

33:42.280 --> 33:47.800
of power, but we'll all have fantastic power compared to what we have today.

33:47.800 --> 33:50.680
And if you're in Texas, where there are no regulations, you'll probably get it first

33:50.680 --> 33:52.080
instead of you in Massachusetts.

33:52.080 --> 33:53.280
Exactly, yeah.

33:54.280 --> 33:58.040
Let me ask you another ethical question while we're on this one. So about a few minutes

33:58.040 --> 34:03.440
ago you mentioned the capacity to replicate someone's brain and bring them back. So let's

34:03.440 --> 34:09.320
say I do that with my father, passed away six years ago, sadly. I bring him back, and

34:09.320 --> 34:15.240
I'm able to create a mind and a body just like my father's. It's exact, perfect replica,

34:15.240 --> 34:21.480
all those thoughts. What happens to all the bills that he owed when he died? Because that's

34:21.520 --> 34:24.640
a lot of money, and a lot of bill collectors call me. Do we have to pay those off, or are

34:24.640 --> 34:26.440
we good?

34:26.440 --> 34:33.440
Well, we're doing something like that with my daughter, and you can read about this in

34:36.080 --> 34:42.840
her book, and it's also in my book. We collected everything my father had written. He died

34:42.880 --> 34:49.880
when I was 22, so he's been dead for more than 50 years. And we fed that into a large

34:53.520 --> 35:00.520
language model, and basically answered the question of all the things he ever wrote,

35:01.880 --> 35:08.000
what best answers this question? And then you could put any question you want, and then

35:08.040 --> 35:12.840
you could talk to it. You'd say something, you then go through everything he ever had

35:12.840 --> 35:19.200
written and find the best answer that he actually wrote to that question. And it actually was

35:19.200 --> 35:25.200
a lot of like talking to him. You could ask him what he liked about music. He was a musician.

35:25.200 --> 35:32.200
He actually liked Brahms the best. And it was very much like talking to him. And I

35:33.200 --> 35:40.200
reported on this in my book, and Amy talks about this in her book. And Amy actually asked

35:42.560 --> 35:48.760
the question, could I fall in love with this person, even though I've never met him? And

35:48.760 --> 35:53.160
she does a pretty good job. I mean, you really do fall in love with this character that she

35:53.160 --> 36:00.160
creates, even though she never met him.

36:02.920 --> 36:08.400
So we can actually, with today's technology, do something where you can actually emulate

36:08.400 --> 36:15.400
somebody else. And I think as we get further on, we can actually do that more and more

36:15.400 --> 36:22.040
responsibly and more and more that really would match that person, and actually emulate

36:22.040 --> 36:25.280
the way he would move and so on. It's a tone of voice.

36:25.280 --> 36:28.880
Well, you know, my dad, he loved Brahms too, particularly those piano trios. So if we can

36:28.920 --> 36:33.920
solve the back taxes problem, we'll get my dad and your dad's bots. Hang out. It would

36:33.920 --> 36:34.920
be great.

36:34.920 --> 36:37.920
Well, yeah. That would be cool.

36:37.920 --> 36:38.920
All right.

36:38.920 --> 36:44.920
All right. We've got 20 minutes left. I want to get to the thing that I most want to understand,

36:44.920 --> 36:47.880
because by the way, this book is wonderful. I think you guys are all going to get signed

36:47.880 --> 36:52.280
copies of it when it comes out. It's truly remarkable, as are all of Ray's books. Whether

36:52.280 --> 36:55.400
you agree or disagree, they'll definitely make you think more.

36:55.440 --> 37:00.600
One of the things that I don't think you do in this book is describe what a day will

37:00.600 --> 37:07.600
be like in 2045 when we're all much more intelligent. So it's 2045. We're all million

37:07.760 --> 37:12.760
times as intelligent. I wake up. Do I have breakfast or do I not have breakfast?

37:12.760 --> 37:19.760
Well, the answer to that question is kind of the same as it is now. But first of all,

37:26.400 --> 37:33.400
the reason it's called a singularity is because we don't really fully understand that question.

37:36.040 --> 37:42.640
Singularity is borrowed from physics. Singularity in physics is where you have a black hole

37:42.640 --> 37:46.600
and no light can escape. And so you can't actually tell what's going on inside the black

37:46.600 --> 37:52.960
hole. And so we call it a singularity, a physical singularity. So this is a historical

37:53.000 --> 38:00.000
singularity where we're borrowing that term from physics and call it a singularity. Because

38:00.120 --> 38:04.560
we can't really answer the question. If we actually multiply our intelligence a million

38:04.560 --> 38:10.680
fold, what's that like? It's a little bit like asking a mouse, gee, what would it be

38:10.680 --> 38:17.520
like if you had the amount of intelligence of this person? The mouse wouldn't really

38:17.640 --> 38:24.640
even understand the question. It does have intelligence. It has a fair amount of intelligence.

38:24.880 --> 38:30.400
But it couldn't understand that question. It couldn't articulate an answer. That's a

38:30.400 --> 38:37.000
little bit what it would be like for us to take the next step in intelligence by adding

38:37.000 --> 38:39.680
all the intelligence that the singularity would provide.

38:39.680 --> 38:41.240
Wait, I just want to make sure I understand.

38:41.240 --> 38:48.240
But I'll give you one answer. I said if you're diligent, you'll achieve longevity escape

38:49.280 --> 38:56.280
velocity in five or six years. And if we want to actually emulate everything that's going

38:56.280 --> 39:03.280
on inside the brain, let's go out a few more years. Let's say the 2040, 2045. Now there's

39:14.800 --> 39:20.800
a lot, you talk to a person, they've got all the connections that they had originally

39:20.800 --> 39:27.800
plus all the seditional connections that we add through having them access computers and

39:29.640 --> 39:36.640
that becomes part of their thinking. So can you, suppose a person like blows up or something

39:39.480 --> 39:46.480
happens to their mind, you definitely can recreate everything that's of a computer origin

39:47.480 --> 39:53.800
because we do that now. Any time we create anything with a computer, it's backed up.

39:53.800 --> 40:00.880
So if the computer goes away, you've got the backup and you can recreate it. It says okay,

40:00.880 --> 40:07.880
but what about the thinking in their normal brain that's not done with computers? We don't

40:08.240 --> 40:15.240
have some ways of backing that up. When we get to the singularity with 2045, we'll be

40:16.240 --> 40:22.040
able to back that up as well because we'll be able to figure out, we'll have some ways

40:22.040 --> 40:29.040
of actually figuring out what's going on in that sort of non-mechanical brain. So we'll

40:29.640 --> 40:36.640
be able to back up both their normal brain as well as the computer addition. And I believe

40:42.880 --> 40:45.680
that's feasible by 2045.

40:45.680 --> 40:48.280
In your vision of it?

40:48.280 --> 40:53.920
So you can back up their entire brain. Now that doesn't guarantee, the whole world could

40:54.000 --> 41:00.000
blow up and lose all the data centers. So it's not absolute guarantee.

41:00.000 --> 41:07.000
Yeah, I'll be ashamed. What I don't understand is will we even be fully distinct people? If

41:07.000 --> 41:13.160
we're sharing memories and we're all uploading our brains to the cloud and we're getting

41:13.160 --> 41:20.000
all this information coming back directly into our neocortex, are we still distinct?

41:20.800 --> 41:27.800
Yes, but we could also find new ways of communicating. So the computers that extend my brain, the

41:32.480 --> 41:38.840
right computers that extend your brain, we could create something that's like a hybrid

41:38.840 --> 41:44.280
or not. And it would be up to our own decision as to whether or not to do that. So there'll

41:44.280 --> 41:47.880
be some new ways of communicating.

41:47.880 --> 41:51.560
Let me ask another question about this. This is what, when I was reading the book, this

41:51.560 --> 41:56.800
is where I kept getting stuck. You are extremely optimistic. You're optimistic about where

41:56.800 --> 42:01.600
we are today. You're optimistic that technology has been a massive force for good. You're

42:01.600 --> 42:06.880
optimistic that it will continue to be a massive force for good. Yet there is a lot of uncertainty

42:06.880 --> 42:08.560
in the future you were describing.

42:08.560 --> 42:15.560
Well, first of all, I'm not necessarily optimistic. The things that can go wrong are

42:18.160 --> 42:25.160
we had things that can go wrong before we had computers. When I was a child, atomic

42:27.600 --> 42:34.600
weapons were created and people were very worried about an atomic war. We would actually

42:36.560 --> 42:42.360
get under our desk and put our hands behind our head to protect us against an atomic war.

42:43.320 --> 42:50.320
It seemed to work, actually. It was still here. But if you had asked people, we had actually

42:51.000 --> 42:58.000
two weapons that went off in anger and killed a lot of people within a week. And if you

42:58.400 --> 43:02.200
had asked people, what's the chance that we're going to go another 80 years and this will

43:02.200 --> 43:09.200
never happen again, nobody would say that that was true. But it has happened. That doesn't

43:10.200 --> 43:17.200
mean it's not going to happen next week. But anyway, that's a great danger. And I think

43:18.800 --> 43:25.800
that's a much greater danger than computers are. Yes, they're dangers, but the computers

43:25.920 --> 43:32.920
will also be more intelligent to avoid kinds of dangers. Yes, it's some bad people in the

43:33.920 --> 43:40.920
world, but I mean, go back 80, 90 years, we had 100 million people die in Asia and Europe

43:44.920 --> 43:51.920
from World War II. We don't have wars like that anymore. We could. We certainly have the

43:54.000 --> 44:01.000
atomic weapons to do that. And you could also imagine computers could be involved with that.

44:03.800 --> 44:10.800
But if you actually look, and this goes right through one piece. First of all, if you look

44:11.040 --> 44:18.040
at my lineage of computers going from tiny fraction of one calculation to 65 billion,

44:20.280 --> 44:27.280
that's a 20 quadrillion fold increase that we've achieved in 80 years. And look at this,

44:27.280 --> 44:34.280
the U.S. personal income, this is done in constant dollars, so this has nothing to do

44:34.360 --> 44:41.360
with inflation. And this is the average income in the United States. It went, it's multiplied

44:46.880 --> 44:53.880
by about 100 fold. And we live far more successfully than we used to. And I think that's a great

44:57.520 --> 45:02.520
question. And we live far more successfully. People say, oh, things were great 100 years

45:02.520 --> 45:08.520
ago, they weren't. And you can look at this chart. I've got 50 charts in the book that

45:08.520 --> 45:14.640
show the kind of progress we've made. A number of people that live in dire poverty has gone

45:14.640 --> 45:20.640
down dramatically. We actually did a poll where they asked people, people that live in poverty

45:21.080 --> 45:27.580
as they've gone up or down, 80 percent said it's gone up. But the reality is it's actually

45:27.580 --> 45:34.580
fallen by 50 percent in the last 20 years. So what we think about the past is really

45:44.840 --> 45:50.240
the opposite of what's happened. Things have gotten far better than they have. And computers

45:50.320 --> 45:54.520
are going to make things even better. I mean, just the kind of things you can do now with

45:54.520 --> 45:57.600
a large language model, it didn't exist two years ago.

45:57.600 --> 46:04.360
Do you ever worry that take it as a given, if yours have made things better, take it

46:04.360 --> 46:08.360
as a given, that personal income will keep going up? Do you ever worry it's just coming

46:08.360 --> 46:12.840
too quickly and it'll be better if maybe the slope of the Kurzweil curve was a little

46:12.840 --> 46:13.320
less steep?

46:13.320 --> 46:19.840
That's a big difference in the past. I mean, talk about what effect did the railroad have.

46:20.440 --> 46:27.440
I mean, lots of jobs were lost or even the cotton genie that happened 200 years ago. And

46:27.520 --> 46:33.400
people were quite happy making money with the cotton genie and suddenly that was gone

46:33.400 --> 46:38.440
and machines were doing that. And people say, well, wait till this gets going, all jobs

46:38.440 --> 46:45.440
will be lost. And that's actually what was said at that time. But actually, income went

46:45.440 --> 46:52.440
up, more and more people worked. We created, and if you say, well, what are they going

46:53.080 --> 46:59.280
to do? You couldn't answer that question because it was in industries that nobody had a clue

46:59.280 --> 47:06.280
of, like for example, all of electronics. So things are getting better even if jobs

47:06.720 --> 47:13.720
are lost. Now, you can certainly point to jobs, like take computer programming. Google

47:19.760 --> 47:26.760
has, I don't know, 60,000 people that program computers and lots of other companies do.

47:27.560 --> 47:33.760
At some point, that's not going to be a feasible job. They can already code, lots of language

47:33.800 --> 47:40.800
models can write code, not quite the way an expert programmer can do. But how long is

47:40.800 --> 47:47.800
that going to take? It's measured in years, not in decades. Nonetheless, I believe that

47:50.320 --> 47:57.320
things will get better because we wipe out jobs, but we create other ways of having an

47:58.320 --> 48:05.320
income. And if you actually point to something, let's say this machine, and this is being

48:07.440 --> 48:13.320
worked on, can wash dishes. You just have a bunch of dishes that will pick the ones that

48:13.320 --> 48:20.320
have to go in the dishwasher and clean everything else up, and that will wash dishes for you.

48:21.320 --> 48:27.320
Would we want that not to happen? Would we say, well, this is kind of upsetting things,

48:27.320 --> 48:34.320
let's get rid of it. It's not going to happen, and no one would advocate that. So we'll

48:36.600 --> 48:42.600
find things to do, we'll have other methods of distributing money, and it will be, it

48:42.600 --> 48:47.600
will continue these kinds of curves that we've seen already.

48:48.360 --> 48:54.360
This is kind of remarkable that we got large language models before we got robotic dishwashers.

48:54.360 --> 49:01.360
You have grandchildren. What would you tell a young person? They buy in, they agree, or

49:02.440 --> 49:08.240
how would you tell them to best prepare themselves for what will be, if you're correct, a remarkably

49:08.240 --> 49:11.240
different future?

49:11.240 --> 49:17.520
I'd be less concerned about what will make money, and much more concerned about what

49:17.560 --> 49:24.560
turns them on. They love video games, so they should learn about that. They should read

49:27.720 --> 49:33.720
literature that turns them on. Some of those literature in the future will be created by

49:33.720 --> 49:40.720
computers. And find out what in the world has a positive effect on their mental being.

49:40.720 --> 49:47.720
And if you know that your child or your grandchild, this gets to one of the questions that is asked

49:51.680 --> 49:57.480
on the screen here, if you know that someone is going to live for hundreds of years, as

49:57.480 --> 50:01.680
you predict, how does that affect the way, certainly it means they shouldn't retire

50:01.680 --> 50:06.440
at 65, but what else does it change about the way they should think about their lives?

50:06.840 --> 50:13.840
Well, I talk to people and say, well, I wouldn't want to live past 100, or maybe they're a

50:13.840 --> 50:20.840
little more ambitious to say, I don't want to live past 110. But if you actually look,

50:22.680 --> 50:29.200
when people decide they've had enough and they don't want to live anymore, that never,

50:29.200 --> 50:35.520
ever happens unless these people are in some kind of dire pain, in physical pain or emotional

50:35.880 --> 50:42.880
pain or spiritual pain or whatever, and they just cannot bear to be alive anymore, nobody

50:43.120 --> 50:50.120
takes their lives other than that. And if we can actually overcome many kinds of physical

50:51.920 --> 50:58.920
problems, cancers wiped out and so on, which I expect to happen, people will be even that

50:59.920 --> 51:06.920
much more happy to live and they'll want to continue to experience tomorrow, and tomorrow

51:09.200 --> 51:16.200
is going to be better and better, these kinds of progress is not going to go away. So people

51:17.320 --> 51:24.320
will want to live, unless they're in dire pain, but that's what the whole sort of medical

51:24.720 --> 51:31.720
profession is about, which is going to be greatly amplified by tomorrow's computers.

51:31.720 --> 51:35.720
Let me ask you a great question that has popped on the screen. This is from Colin McCabe.

51:35.720 --> 51:41.520
AI is a black box. Nobody knows how it was built. How do you show that AI is trustworthy

51:41.520 --> 51:46.020
to users who want to trust it, adopt it, and accept it, particularly if you're going to

51:46.020 --> 51:53.020
upload it directly into your brain? Well, it's not sure that nobody knows how they work.

51:53.520 --> 51:59.520
Right. Most people who are using a large language model don't know what data sense went into

51:59.520 --> 52:03.520
it. They're things that happen in the transformer layer that even the architects don't understand.

52:03.520 --> 52:10.020
Right. But we're going to learn more and more about that, and in fact how computers work

52:10.020 --> 52:17.020
will be, I think, a very common type of talent that people want to gain. And ultimately we'll

52:18.020 --> 52:24.520
have more trust of computers. I mean, large language models aren't perfect, and you can

52:24.520 --> 52:31.520
ask the question and it can give you something that's incorrect. I mean, we've seen that

52:33.160 --> 52:40.160
just recently. The reason we have these computers give you incorrect information is it doesn't

52:40.160 --> 52:46.160
have the information to begin with, and it actually doesn't know what it doesn't know.

52:46.160 --> 52:53.160
And that's actually something we're working on so that it knows, well, I don't know that.

52:53.160 --> 52:59.160
That's actually very good if it can actually say that, because right now it will find the

52:59.160 --> 53:06.160
best thing it knows. And if it's never trained on that information and there's nothing in

53:07.160 --> 53:14.160
there that tells you, it'll just give you the best guess, which could be very incorrect.

53:14.160 --> 53:20.160
And we're actually learning to be able to figure out when it knows and when it doesn't know.

53:20.160 --> 53:27.160
But ultimately we'll have pretty good confidence when it knows and when it doesn't know, and

53:28.160 --> 53:34.160
we can actually rely on what it says.

53:34.160 --> 53:39.160
So your answer to the question is, A, we will understand more, and B, they'll be much more

53:39.160 --> 53:43.160
trustworthy, so it won't be as risky to not understand them.

53:43.160 --> 53:44.160
Right.

53:44.160 --> 53:50.160
You spent your life making predictions, some of which like the Turing test, you've held

53:50.160 --> 53:54.160
on to them and been remarkably accurate. As you move from an overwhelming optimist to

53:54.160 --> 53:57.160
a pessimist, what is a prediction?

53:57.160 --> 54:02.160
Well, my books have always had a chapter on how these things can go wrong.

54:02.160 --> 54:09.160
Tell me a prediction that you are chewing over right now, but you're not sure whether you

54:09.160 --> 54:15.160
want to make it or whether you don't want to make it.

54:15.160 --> 54:22.160
I mean, there's well-known dangers in nanotechnology. If someone were to create nanotechnology,

54:23.160 --> 54:30.160
that replicates, well-known as if it replicates everything into paper clips, turn the entire

54:33.160 --> 54:38.160
world into paper clips, that would not be positive.

54:38.160 --> 54:39.160
No.

54:39.160 --> 54:41.160
Unless you're staples.

54:41.160 --> 54:48.160
And that's feasible. Take somebody who's...

54:49.160 --> 54:56.160
a little bit mental to do that, but it could be done.

54:56.160 --> 55:03.160
And we actually will have something that actually avoids that.

55:03.160 --> 55:12.160
So we'll have something that can detect that this is actually turning everything into paper

55:12.160 --> 55:17.160
clips and destroy it before it does that.

55:17.160 --> 55:25.160
But I mean, I have a chapter in this new book, the Singularity is Nearer, that talks about

55:25.160 --> 55:27.160
the kinds of things that could happen.

55:27.160 --> 55:31.160
Oh, the most remarkable part of this book is he does exactly the mathematical calculations

55:31.160 --> 55:35.160
on how long it would take nanobots to turn the world into grey goo and how long it would

55:35.160 --> 55:38.160
take the blue goo to stop the grey goo. It's remarkable.

55:38.160 --> 55:41.160
The book will be out soon. You definitely need to read until the end.

55:41.160 --> 55:45.160
But this leads to a... maybe... let me try and answer it.

55:45.160 --> 55:50.160
The question I asked before is what should young people think about and be working on?

55:50.160 --> 55:52.160
You said their passions and what turns them on.

55:52.160 --> 56:00.160
Shouldn't they be thinking through how to design and architect these future systems

56:00.160 --> 56:03.160
so they are less likely to turn us into grey goo or paper clips?

56:03.160 --> 56:06.160
I don't know if everybody wants to work on that.

56:06.160 --> 56:10.160
But folks in this room, technologically minded, you guys should all be working on not turning

56:10.160 --> 56:12.160
us into grey goo, right?

56:12.160 --> 56:14.160
Yes, that would be on the list.

56:14.160 --> 56:20.160
But then that leads to another question, which is what will the role of humans be in thinking

56:20.160 --> 56:24.160
through that problem when they're only a millionth or a billionth or a trillionth as intelligent

56:24.160 --> 56:28.160
as machines?

56:28.160 --> 56:29.160
Say that again?

56:29.160 --> 56:33.160
So we're going to have these really hard problems to solve.

56:33.160 --> 56:39.160
Right now, we are, along with our machines, we can be extremely intelligent.

56:39.160 --> 56:44.160
But 10 years from now, 15 years from now, there will be machines that will be so much more intelligent

56:44.160 --> 56:45.160
than us.

56:45.160 --> 56:49.160
What will our role... what will the role of humans be in trying to solve these problems?

56:49.160 --> 56:52.160
Well, first of all, I see those as extensions of humans.

56:52.160 --> 56:55.160
We wouldn't have them if we didn't have humans to begin with.

56:55.160 --> 56:58.160
And humans have a brain that can think these things through.

56:58.160 --> 57:01.160
And we have this thumb.

57:01.160 --> 57:03.160
It's not really very much appreciated.

57:03.160 --> 57:08.160
But like whales and elephants actually have a larger brain than we have.

57:08.160 --> 57:15.160
And they can probably think deeper thoughts, but they don't have a thumb until they don't create technology.

57:15.160 --> 57:22.160
A monkey can create... it actually has a thumb, but it's actually down an inch or so,

57:22.160 --> 57:24.160
and therefore it really can't grab very well.

57:24.160 --> 57:30.160
So it can create a little bit of technology, but the technology it creates cannot create other technology.

57:30.160 --> 57:36.160
So the fact that we have a thumb means we can create integrated circuits

57:36.160 --> 57:43.160
that can become a large language model that comes from the human brain.

57:43.160 --> 57:50.160
And it's actually trained with everything that we've ever thought.

57:50.160 --> 57:59.160
Anything that human beings have thought has been documented and it can go into these large language models.

57:59.160 --> 58:02.160
And everybody can work on these things.

58:02.160 --> 58:06.160
And it's not true only certain wealthy people will have it.

58:06.160 --> 58:10.160
I mean, how many people here have phones?

58:10.160 --> 58:14.160
If it's not 100%, it's like 99.9%.

58:14.160 --> 58:20.160
And you don't have to be kind of from a wealthy group.

58:20.160 --> 58:25.160
I mean, I see people who are homeless who have their own phone.

58:25.160 --> 58:29.160
It's not that expensive.

58:29.160 --> 58:35.160
And so that represents the distribution of these capabilities.

58:35.160 --> 58:39.160
It's not something you have to be fabulously wealthy to afford.

58:39.160 --> 58:44.160
So you think that we're heading into a future where we're going to live much longer and will be much more equal?

58:44.160 --> 58:50.160
We think we're heading into a society where we'll live much longer, be wealthier, but also much more equality.

58:50.160 --> 58:53.160
Yes, absolutely. And we've seen that already.

58:53.160 --> 59:01.160
Well, we're at time, but Ray and I will be back in 21, 24, 22, 24, and 23, 24.

59:01.160 --> 59:06.160
So thank you for coming today. Thank you so much. He is an American treasure. Thank you.

