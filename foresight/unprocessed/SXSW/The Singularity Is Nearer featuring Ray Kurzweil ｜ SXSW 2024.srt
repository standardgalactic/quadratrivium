1
00:00:00,000 --> 00:00:10,000
All right, I'm so excited to be here with you, Ray.

2
00:00:10,000 --> 00:00:16,680
It's great to be here, great to see everybody together, beautiful audience.

3
00:00:16,680 --> 00:00:20,800
So my favorite thing in that introduction of you is that you have been working in AI

4
00:00:20,800 --> 00:00:26,780
longer than any other human alive, which means if you live forever and will get to that,

5
00:00:26,780 --> 00:00:29,000
you will always have that distinction.

6
00:00:29,000 --> 00:00:32,800
I think that's right.

7
00:00:32,800 --> 00:00:36,480
Marvin Minsky was actually my mentor.

8
00:00:36,480 --> 00:00:41,560
If he were alive today, he would actually be more than 61 years.

9
00:00:41,560 --> 00:00:43,360
We're going to bring him back also.

10
00:00:43,360 --> 00:00:46,360
So maybe you'll not sure how we'll count the distinction then.

11
00:00:51,360 --> 00:00:53,040
All right, so we're going to fix the audio.

12
00:00:53,040 --> 00:00:55,560
But this is what we're going to do with this conversation.

13
00:00:55,600 --> 00:00:59,120
I'm going to start out asking Ray some questions about where we are today.

14
00:00:59,120 --> 00:01:00,520
We'll do that for a few minutes.

15
00:01:00,520 --> 00:01:06,120
Then we'll get into what has to happen to reach the singularity, so the next 20 years.

16
00:01:06,120 --> 00:01:08,640
Then we'll get into discussion about what the singularity is,

17
00:01:08,640 --> 00:01:10,360
what it means, how it would change our lives.

18
00:01:10,360 --> 00:01:12,600
And then at the end, we'll talk a little bit about how,

19
00:01:12,600 --> 00:01:15,560
if we believe this vision of the future, what it means for us today.

20
00:01:15,560 --> 00:01:17,280
Ask your questions, they'll come in.

21
00:01:17,280 --> 00:01:20,040
I'll ask them as they go in the different sections of the conversation.

22
00:01:20,040 --> 00:01:21,280
But let's get cracking.

23
00:01:21,320 --> 00:01:22,120
Can you hear me?

24
00:01:27,160 --> 00:01:28,000
You can't hear Ray.

25
00:01:31,000 --> 00:01:32,840
Well, this will be recorded.

26
00:01:32,840 --> 00:01:34,320
You guys are going to all live forever.

27
00:01:34,320 --> 00:01:35,520
There'll be plenty of time.

28
00:01:36,560 --> 00:01:38,280
It will be fine.

29
00:01:38,280 --> 00:01:39,440
I'm just going to get started.

30
00:01:39,440 --> 00:01:40,920
I assume the audio will get worked out.

31
00:01:40,920 --> 00:01:42,840
They do a fabulous job here at South by.

32
00:01:43,840 --> 00:01:46,040
I think they should be able to hear me and you.

33
00:01:49,280 --> 00:01:50,760
All right, we got this over on the right.

34
00:01:51,280 --> 00:01:56,640
Audio engineers, are we good to go?

35
00:01:58,640 --> 00:01:59,640
We're good to go.

36
00:01:59,640 --> 00:02:02,600
All right, first question, Ray.

37
00:02:03,880 --> 00:02:07,200
So, you've been working in AI for 61 years.

38
00:02:07,200 --> 00:02:08,640
Wait, can you hear me?

39
00:02:13,760 --> 00:02:16,520
So everybody in the front can hear you, but nobody in the back can hear you.

40
00:02:16,520 --> 00:02:17,560
Can you hear me now?

41
00:02:21,480 --> 00:02:22,960
All right, I'll speak louder.

42
00:02:25,160 --> 00:02:26,800
First question.

43
00:02:26,800 --> 00:02:29,960
So you've been living in the revolution for a long time.

44
00:02:29,960 --> 00:02:35,080
You've made lots of predictions, many of which have been remarkably accurate.

45
00:02:35,080 --> 00:02:39,880
We've all been living in a remarkable two year transformation

46
00:02:39,880 --> 00:02:42,560
with large language models a year and a half.

47
00:02:42,560 --> 00:02:46,720
What has surprised you about the innovations in large language models

48
00:02:46,720 --> 00:02:48,440
and what has happened recently?

49
00:02:48,440 --> 00:02:52,480
Well, I did finish this book a year ago

50
00:02:52,480 --> 00:02:55,200
and didn't really cover a large language model.

51
00:02:55,200 --> 00:03:10,760
So I delayed the book to cover that, but I was expecting that to happen

52
00:03:10,760 --> 00:03:12,520
like a couple of years later.

53
00:03:12,520 --> 00:03:18,640
I mean, I made a prediction in 1999 that it would happen by 2029.

54
00:03:19,960 --> 00:03:27,120
And we're not quite there yet, but it looks like it's maybe a year or two ahead of schedule.

55
00:03:31,560 --> 00:03:33,880
So that was maybe a bit of a surprise.

56
00:03:33,880 --> 00:03:38,920
Wait, you predicted back in 1999 that a computer would pass the Turing test in 2029.

57
00:03:38,920 --> 00:03:43,240
Are you revising that to something more closer to today?

58
00:03:46,240 --> 00:03:50,120
No, I'm still saying 2029.

59
00:03:52,720 --> 00:03:57,840
The definition of the Turing test is not precise.

60
00:03:57,840 --> 00:04:02,760
We're going to have people claiming that the Turing test has been solved

61
00:04:02,760 --> 00:04:06,280
and people are saying that GPT-4 actually passes it.

62
00:04:06,280 --> 00:04:09,880
Some people, so it's going to be like maybe two or three years

63
00:04:09,880 --> 00:04:13,920
where people start claiming and then they continue to claim

64
00:04:13,920 --> 00:04:16,120
and finally everybody will accept it.

65
00:04:16,120 --> 00:04:18,440
So it's not like it happens in one day.

66
00:04:18,440 --> 00:04:21,480
But you have a very specific definition of the Turing test.

67
00:04:21,480 --> 00:04:23,400
When do you think we'll pass that definition?

68
00:04:25,360 --> 00:04:28,200
Well, the Turing test is actually not that significant,

69
00:04:28,200 --> 00:04:38,080
because that means that a computer will pass for a human being.

70
00:04:38,080 --> 00:04:43,160
And what's much more important is AGI, Automatic General Intelligence,

71
00:04:43,160 --> 00:04:46,480
which means it can emulate any human being.

72
00:04:46,480 --> 00:04:52,440
So you have one computer and it can do everything that any human being can do.

73
00:04:52,440 --> 00:04:56,880
And that's also 2029, it all happens at the same time.

74
00:04:56,880 --> 00:04:57,960
But nobody can do that.

75
00:04:57,960 --> 00:05:02,440
I mean, just take an average large language model today.

76
00:05:02,440 --> 00:05:07,840
You can ask it anything and it will answer you pretty convincingly.

77
00:05:07,840 --> 00:05:10,240
No human being can do all of that.

78
00:05:10,240 --> 00:05:11,440
And it does it very quickly.

79
00:05:11,440 --> 00:05:15,480
It will write a very nice essay in 15 seconds.

80
00:05:15,480 --> 00:05:19,200
And then you can ask it again and it will write another essay.

81
00:05:19,200 --> 00:05:23,280
And no human being can actually perform at that level.

82
00:05:23,280 --> 00:05:26,560
Right, so you have to dumb it down to actually have a convincing Turing test.

83
00:05:26,560 --> 00:05:28,840
So having a Turing test, you have to dumb it down.

84
00:05:28,840 --> 00:05:31,120
Yeah, let me ask the first question from the audience,

85
00:05:31,120 --> 00:05:34,640
since I think it's quite relevant to where we are, which is Brian Daniel.

86
00:05:34,640 --> 00:05:37,080
Is the Kurzweil curve still accurate?

87
00:05:37,080 --> 00:05:37,600
Say again?

88
00:05:37,600 --> 00:05:40,640
Is the Kurzweil curve still accurate?

89
00:05:40,640 --> 00:05:43,720
Yes, in fact, can they see that?

90
00:05:43,720 --> 00:05:45,120
Let's pull the slides up, first slide.

91
00:05:48,680 --> 00:05:52,160
So this is an 80 year track record.

92
00:05:52,160 --> 00:05:54,560
This is an exponential growth.

93
00:05:54,560 --> 00:05:58,880
A straight line on this curve means exponential curvature.

94
00:06:01,240 --> 00:06:05,800
If it was sort of exponential, but not quite, it would curve.

95
00:06:05,800 --> 00:06:07,480
This is actually a straight line.

96
00:06:08,760 --> 00:06:20,200
It started out with a computer that did 0.0000007 calculations per second per

97
00:06:20,200 --> 00:06:21,600
constant dollar.

98
00:06:21,600 --> 00:06:23,760
That's the lower left hand corner.

99
00:06:23,760 --> 00:06:28,320
At the upper right hand corner, it's 65 billion calculations per second for

100
00:06:28,320 --> 00:06:30,040
the same amount of money.

101
00:06:31,760 --> 00:06:35,840
So that's why large language models have only been feasible for two years.

102
00:06:35,840 --> 00:06:39,560
We actually had large language models before that, but it didn't work very well.

103
00:06:41,840 --> 00:06:44,400
And this is an exponential curve.

104
00:06:44,400 --> 00:06:48,400
Technology moves in an exponential curve.

105
00:06:48,400 --> 00:06:59,000
We see that, for example, having renewable energy come from the sun and

106
00:06:59,000 --> 00:07:01,720
wind, that's actually an exponential curve.

107
00:07:03,640 --> 00:07:05,560
It's increased, it's gone.

108
00:07:06,720 --> 00:07:10,760
We've decreased the price by 99.7%.

109
00:07:12,280 --> 00:07:17,200
We've multiplied the amount of energy coming from solar energy a million fold.

110
00:07:18,520 --> 00:07:24,520
So this kind of curve really directs all kinds of technology.

111
00:07:27,600 --> 00:07:31,760
And this is the reason that we're making progress.

112
00:07:31,760 --> 00:07:36,160
I mean, we knew how to do large language models years ago, but

113
00:07:36,160 --> 00:07:37,760
we're dependent on this curve.

114
00:07:40,160 --> 00:07:41,160
And it's pretty amazing.

115
00:07:41,160 --> 00:07:47,920
It started out increasing relay speeds, then vacuum tubes, then integrated circuits.

116
00:07:47,920 --> 00:07:51,640
And each year, it makes the same amount of progress approximately

117
00:07:53,640 --> 00:07:56,080
regardless of where you are on this curve.

118
00:07:56,080 --> 00:07:59,960
We just added the last point, and it's, again,

119
00:08:01,320 --> 00:08:05,600
we basically multiply this by two every 1.4 years.

120
00:08:08,320 --> 00:08:12,480
And this is the reason that computers are exciting, but

121
00:08:12,480 --> 00:08:14,840
it actually affects every type of technology.

122
00:08:15,840 --> 00:08:18,680
And we just added the last point like two weeks ago.

123
00:08:20,800 --> 00:08:23,440
All right, so let me ask you a question.

124
00:08:23,440 --> 00:08:26,720
You wrote a book about how to build a mind.

125
00:08:26,720 --> 00:08:29,800
You have a lot about how the human mind is constructed.

126
00:08:29,800 --> 00:08:32,840
A lot of the progress in AI, AI systems are being built and

127
00:08:32,840 --> 00:08:34,520
what we understand about neural networks, right?

128
00:08:34,520 --> 00:08:39,280
So clearly our understanding of this helps with AI.

129
00:08:39,280 --> 00:08:42,840
In the last two years, by watching these large language models,

130
00:08:42,840 --> 00:08:45,400
have we learned anything new about our brains?

131
00:08:45,400 --> 00:08:48,800
Are we learning about the inside of our skulls as we do this?

132
00:08:48,800 --> 00:08:51,200
It really has to do with the amount of connections.

133
00:08:52,480 --> 00:08:56,840
The brain is actually organized fairly differently.

134
00:08:56,840 --> 00:09:00,040
The things near the eye, for example, deal with vision.

135
00:09:02,480 --> 00:09:06,000
And we have different ways of implementing different parts of the brain that

136
00:09:06,000 --> 00:09:07,520
remember different things.

137
00:09:07,520 --> 00:09:09,960
We actually don't need that.

138
00:09:09,960 --> 00:09:13,640
In large language model, all the connections are the same.

139
00:09:13,640 --> 00:09:16,840
We have to get the connections up to a certain point.

140
00:09:16,840 --> 00:09:19,560
If it approximately matches what the brain does,

141
00:09:19,560 --> 00:09:25,040
which is about a trillion connections, it will perform kind of like the brain.

142
00:09:25,040 --> 00:09:27,720
We're kind of almost at that point.

143
00:09:27,720 --> 00:09:34,480
Wait, so if the GPT-4 is 400 billion, the next ones will be a trillion or more.

144
00:09:34,480 --> 00:09:36,480
So the construction of these models,

145
00:09:36,480 --> 00:09:39,320
they are more efficient in their construction than our brains are.

146
00:09:41,040 --> 00:09:44,680
We make them to be as efficient as possible, but

147
00:09:44,680 --> 00:09:47,600
it doesn't really matter how they're organized.

148
00:09:47,600 --> 00:09:53,240
And we can actually create certain software that will actually expand

149
00:09:53,240 --> 00:09:57,440
the amount of connections more for the same amount of computation.

150
00:10:00,000 --> 00:10:03,800
But it really has to do with how many connections

151
00:10:06,480 --> 00:10:11,200
our particular computer is responsible for.

152
00:10:11,200 --> 00:10:18,360
So as we approach AGI, we're not looking for a new understanding of how to make

153
00:10:18,360 --> 00:10:19,480
these machines more efficient.

154
00:10:19,480 --> 00:10:22,520
The transformer architecture was clearly very important.

155
00:10:22,520 --> 00:10:25,840
We can really just get there with more connections.

156
00:10:25,840 --> 00:10:28,880
But the software and the learning is also important.

157
00:10:28,880 --> 00:10:31,720
I mean, you could have a trillion connections, but

158
00:10:31,720 --> 00:10:35,760
if you didn't have something to learn from, it wouldn't be very effective.

159
00:10:35,760 --> 00:10:39,600
So we actually have to be able to collect all this data.

160
00:10:39,600 --> 00:10:41,960
So we do it on the web and so on.

161
00:10:41,960 --> 00:10:47,400
I mean, we've been collecting stuff on the web for several decades.

162
00:10:48,840 --> 00:10:57,680
That's really what we're depending on to be able to train these large language

163
00:10:57,680 --> 00:10:59,000
models.

164
00:10:59,000 --> 00:11:02,720
And we shouldn't actually call them large language models,

165
00:11:02,720 --> 00:11:05,400
because they deal with much more than language.

166
00:11:05,400 --> 00:11:09,360
I mean, it's language, but you can add pictures.

167
00:11:09,360 --> 00:11:17,400
You can add things that affect disease that have nothing to do with language.

168
00:11:17,400 --> 00:11:26,600
In fact, we're using now simulated biology to be able to simulate

169
00:11:26,600 --> 00:11:34,200
different ways to affect disease that have nothing to do with language.

170
00:11:34,200 --> 00:11:37,040
So they really should be called large event models.

171
00:11:38,640 --> 00:11:42,440
Do you think there's anything that happens inside of our brains that cannot

172
00:11:42,440 --> 00:11:44,480
be captured by computation and by math?

173
00:11:45,680 --> 00:11:47,720
No, I mean, what would that be?

174
00:11:47,720 --> 00:11:52,520
I mean, okay, quick poll of the audience.

175
00:11:52,520 --> 00:11:54,720
Raise your hand if you think there's something in your brain that cannot be

176
00:11:54,720 --> 00:11:57,840
captured by computation or math, like a soul.

177
00:11:59,240 --> 00:12:01,960
All right, so convince them that they're wrong, right?

178
00:12:01,960 --> 00:12:08,120
I mean, consciousness is very important, but it's actually not scientific.

179
00:12:08,120 --> 00:12:12,160
There's no way I could slide somebody in and the light will go on.

180
00:12:12,160 --> 00:12:13,240
This one's conscious.

181
00:12:13,240 --> 00:12:14,240
No, this one's not.

182
00:12:16,000 --> 00:12:21,160
It's not scientific, but it's actually extremely important.

183
00:12:23,600 --> 00:12:26,680
And another question, why am I me?

184
00:12:26,680 --> 00:12:28,880
How come what happens to me?

185
00:12:28,880 --> 00:12:32,640
I'm conscious of, and I'm not conscious of what happens to you.

186
00:12:34,640 --> 00:12:40,040
These are deeply mysterious things, but it's really not conscious.

187
00:12:40,040 --> 00:12:44,080
So Marvin Minsky, who was my mentor for 50 years, he said,

188
00:12:44,080 --> 00:12:47,320
it's not scientific and therefore we shouldn't bother with it.

189
00:12:47,320 --> 00:12:51,360
And any discussion of consciousness he would kind of dismiss.

190
00:12:52,520 --> 00:12:58,040
But he actually did, his reaction to people was totally dependent on whether he

191
00:12:58,040 --> 00:13:00,000
felt they were conscious or not.

192
00:13:00,000 --> 00:13:03,600
So he actually did use that.

193
00:13:03,600 --> 00:13:08,160
But it's not something that we're ignoring because there's no way to tell

194
00:13:08,160 --> 00:13:09,760
whether something's conscious.

195
00:13:11,760 --> 00:13:15,360
And that's not just something that we don't know and we'll discover.

196
00:13:15,360 --> 00:13:18,760
There's really no way to tell whether or not something's conscious.

197
00:13:18,760 --> 00:13:20,960
What do you mean, this is not conscious?

198
00:13:20,960 --> 00:13:23,040
And the gentleman sitting right there is conscious.

199
00:13:23,040 --> 00:13:23,720
I'm pretty confident.

200
00:13:23,720 --> 00:13:24,720
Well, how do you prove that?

201
00:13:24,720 --> 00:13:34,400
I mean, we kind of agree with humans, that humans are conscious.

202
00:13:34,400 --> 00:13:36,280
Some humans are conscious, not all humans.

203
00:13:36,280 --> 00:13:41,960
But how about animals?

204
00:13:41,960 --> 00:13:43,720
We have a big disagreement.

205
00:13:43,720 --> 00:13:49,920
Some people say animals are not conscious and other people think animals are conscious.

206
00:13:49,920 --> 00:13:52,920
Maybe some animals are conscious and others are not.

207
00:13:52,920 --> 00:13:55,400
There's no way to prove that.

208
00:13:55,400 --> 00:13:59,320
Let's, okay, I want to run down this consciousness question.

209
00:13:59,320 --> 00:14:03,080
But before we do that, I want to make sure I understood your previous answer correctly.

210
00:14:03,080 --> 00:14:09,600
So the feeling I get of being in love or the feeling,

211
00:14:09,600 --> 00:14:16,280
any emotion that I get could eventually be represented in math in a large language model.

212
00:14:16,280 --> 00:14:21,520
Yeah, I mean certainly the behavior, the feelings that you have if you're with

213
00:14:21,520 --> 00:14:28,560
somebody that you love is definitely dependent on what the connections do.

214
00:14:28,560 --> 00:14:32,440
You can tell whether or not that's happening.

215
00:14:32,440 --> 00:14:40,040
All right, and back to, is everybody here convinced?

216
00:14:40,040 --> 00:14:40,960
Not entirely.

217
00:14:40,960 --> 00:14:41,720
All right, we'll close it up.

218
00:14:41,720 --> 00:14:45,760
So you don't think that it's worth trying to define consciousness.

219
00:14:45,760 --> 00:14:49,960
I mean, you spend a fair amount in your book giving different arguments about what consciousness means.

220
00:14:49,960 --> 00:14:56,240
But it seems like you're arguing on stage that we shouldn't try to define it.

221
00:14:56,240 --> 00:14:58,960
There's no way to actually prove it.

222
00:14:58,960 --> 00:15:01,280
I mean, we have certain agreements.

223
00:15:01,280 --> 00:15:02,760
I agree that all of you are conscious.

224
00:15:02,760 --> 00:15:04,480
You actually made it into this room.

225
00:15:04,480 --> 00:15:12,840
So that's a pretty good indication that you're conscious, but that's not a proof.

226
00:15:12,840 --> 00:15:18,320
And there may be human beings that don't seem quite conscious at the time.

227
00:15:18,320 --> 00:15:21,760
Are they conscious or not, and animals?

228
00:15:21,760 --> 00:15:26,280
I mean, I think elephants and whales are conscious, but not everybody agrees with that.

229
00:15:26,280 --> 00:15:32,720
So at what point can we then essentially, how long will it be until we can essentially

230
00:15:32,720 --> 00:15:40,120
download the entire contents of your brain and express it through some kind of a machine?

231
00:15:40,120 --> 00:15:45,760
That's actually an important question because we're going to talk about longevity.

232
00:15:45,760 --> 00:15:50,480
We're going to get to a point where we have longevity escape velocity, and it's not that

233
00:15:50,480 --> 00:15:51,480
far away.

234
00:15:51,480 --> 00:15:55,200
I think if you're diligent, you'll be able to achieve that by 2029.

235
00:15:55,200 --> 00:16:00,560
That's only five or six years from now.

236
00:16:00,560 --> 00:16:06,680
And so right now you go through a year, use up a year of your longevity, but you get back

237
00:16:06,680 --> 00:16:10,120
from scientific progress right now about four months.

238
00:16:10,120 --> 00:16:13,720
But that scientific progress is on an exponential curve.

239
00:16:13,720 --> 00:16:19,080
It's going to speed up every year, and by 2029, if you're diligent, you'll use up a

240
00:16:19,080 --> 00:16:23,960
year of your longevity with the year passing, but you'll get back a full year.

241
00:16:23,960 --> 00:16:28,800
And past 2029, you'll get back more than a year, so you'll actually go backwards in

242
00:16:28,800 --> 00:16:29,800
time.

243
00:16:29,800 --> 00:16:38,880
Now, that's not a guarantee of infinite life because you could have a 10-year-old and you

244
00:16:38,880 --> 00:16:45,800
could compute his longevity as many, many decades, and he could die tomorrow.

245
00:16:45,800 --> 00:16:50,320
But what's important about actually capturing everything in your brain, we can't do that

246
00:16:50,320 --> 00:16:55,720
today, and we won't be able to do that in five years, but you will be able to do that

247
00:16:55,720 --> 00:17:00,000
by the singularity, which is 2045.

248
00:17:00,000 --> 00:17:03,360
And so at that point, you can actually go inside the brain and capture everything in

249
00:17:03,360 --> 00:17:04,360
there.

250
00:17:05,040 --> 00:17:12,080
Now, your thinking is going to be a combination of the amount you get from computation, which

251
00:17:12,080 --> 00:17:17,600
will add to your thinking, and that's automatically captured.

252
00:17:17,600 --> 00:17:26,960
I mean, right now, anything that you have in a computer is automatically captured today,

253
00:17:26,960 --> 00:17:31,680
and the kind of additional thinking we'll have by adding to our brain, that will be

254
00:17:31,680 --> 00:17:42,400
captured, but the connections that we have in the brain that we start with, we'll still

255
00:17:42,400 --> 00:17:44,800
have that.

256
00:17:44,800 --> 00:17:47,600
That's not captured today, but that will be captured in 2045.

257
00:17:47,600 --> 00:17:52,720
We'll be able to go inside the brain and capture that as well, and therefore, we'll

258
00:17:52,720 --> 00:17:58,200
actually capture the entire brain, which will be backed up.

259
00:17:58,240 --> 00:18:04,440
So even if you get wiped out, you walk into a bomb and it explodes, we can actually recreate

260
00:18:04,440 --> 00:18:08,520
everything that was in your brain by 2045.

261
00:18:08,520 --> 00:18:11,960
That's one of the implications of the singularity.

262
00:18:11,960 --> 00:18:23,560
Now, that doesn't absolutely guarantee because, I mean, the world could blow up and all the

263
00:18:24,520 --> 00:18:33,680
computer, all the things that computers could blow up, and so you wouldn't be able to recreate

264
00:18:33,680 --> 00:18:35,560
that.

265
00:18:35,560 --> 00:18:40,800
We never actually get to a point where we absolutely guarantee that you live forever,

266
00:18:40,800 --> 00:18:48,840
but most of the things that right now would upset capturing that will be overcome by that

267
00:18:48,840 --> 00:18:49,840
time.

268
00:18:49,840 --> 00:18:53,080
There's a lot there, right?

269
00:18:53,120 --> 00:18:55,400
Let's start with escape velocity.

270
00:18:55,400 --> 00:18:59,800
So do you think that anybody in this audience, in their current biological body, will live

271
00:18:59,800 --> 00:19:01,680
to be 500 years old?

272
00:19:01,680 --> 00:19:04,520
You're using me?

273
00:19:04,520 --> 00:19:05,520
Yeah.

274
00:19:05,520 --> 00:19:06,520
Absolutely.

275
00:19:06,520 --> 00:19:07,520
And who?

276
00:19:07,520 --> 00:19:12,960
I mean, if you're going to be alive in five years, and I imagine all of you will be alive

277
00:19:12,960 --> 00:19:13,960
in five years.

278
00:19:13,960 --> 00:19:14,960
No, five.

279
00:19:14,960 --> 00:19:15,960
Oh, okay.

280
00:19:15,960 --> 00:19:20,720
If they're alive for five years, they will likely live to be 500 years old.

281
00:19:20,720 --> 00:19:24,920
If they're diligent, and I think the people in this audience will be diligent.

282
00:19:24,920 --> 00:19:25,920
Wow.

283
00:19:25,920 --> 00:19:26,920
All right.

284
00:19:26,920 --> 00:19:29,520
Well, you can drink whatever you want as long as you don't get run over tonight because

285
00:19:29,520 --> 00:19:31,560
you don't have to worry about decline.

286
00:19:31,560 --> 00:19:33,000
All right.

287
00:19:33,000 --> 00:19:34,560
So let me ask a question.

288
00:19:34,560 --> 00:19:37,840
I want to get, we're going to spend a lot of time on what the singularity is, what it

289
00:19:37,840 --> 00:19:40,400
means, and what it'll be like, but I want to ask some questions that will lead us up

290
00:19:40,400 --> 00:19:41,400
there.

291
00:19:41,400 --> 00:19:44,600
So I'm going to take this question from Mark Sternberg and modify it slightly.

292
00:19:44,600 --> 00:19:50,000
In the timeframe, AI will be able to do, or sufficiently sophisticated computers in your

293
00:19:50,000 --> 00:19:53,320
argument, can do everything that the human brain can do.

294
00:19:53,320 --> 00:19:59,880
What will they not be able to do in the next 10 years?

295
00:19:59,880 --> 00:20:07,280
Well, one thing has to do with being creative.

296
00:20:07,280 --> 00:20:11,880
Some people, they'll be able to do everything a human can do, but they're not going to be

297
00:20:11,880 --> 00:20:14,640
able to create new knowledge.

298
00:20:14,640 --> 00:20:22,160
That's actually wrong because we can simulate, for example, biology, and the Moderna vaccine,

299
00:20:22,160 --> 00:20:23,660
for example.

300
00:20:23,660 --> 00:20:27,560
We didn't do it the usual way, which is somebody sits down and thinks, well, I think this might

301
00:20:27,560 --> 00:20:33,480
work, and then they try it out, and it takes years to try it out on multiple people, and

302
00:20:33,480 --> 00:20:36,960
it's one person's idea about what might work.

303
00:20:36,960 --> 00:20:41,720
They actually listed everything that might work, and there was actually several billion

304
00:20:41,720 --> 00:20:47,640
different mRNA sequences, and they said, let's try them all, and they tried every single

305
00:20:47,640 --> 00:20:52,440
one by simulating biology, and that took two days.

306
00:20:52,440 --> 00:20:57,120
So one weekend, they tried out several billion different possibilities, and then they picked

307
00:20:57,120 --> 00:21:07,320
the one that turned out to be the best, and that actually was the Moderna vaccine up until

308
00:21:07,320 --> 00:21:09,320
today.

309
00:21:10,320 --> 00:21:12,880
Now, they did actually test it on humans.

310
00:21:12,880 --> 00:21:20,120
We'll be able to overcome that as well because we'll be able to test using simulated biology

311
00:21:20,120 --> 00:21:21,120
as well.

312
00:21:21,120 --> 00:21:22,760
They actually decided to test it.

313
00:21:22,760 --> 00:21:26,400
It's a little bit hard to give up testing on humans.

314
00:21:26,400 --> 00:21:31,880
We will do that, so you can actually try out every single one, pick the best one, and then

315
00:21:31,880 --> 00:21:38,200
you can try out that by testing on a million simulated humans and do that in a few days

316
00:21:38,200 --> 00:21:39,200
as well.

317
00:21:39,320 --> 00:21:44,320
And that's actually the future of how we're going to create medications for diseases, and

318
00:21:44,320 --> 00:21:51,720
there's lots of things going on now with cancer and other diseases that are using that.

319
00:21:51,720 --> 00:21:54,320
So that's a whole new method.

320
00:21:54,320 --> 00:21:56,440
This is actually starting now.

321
00:21:56,440 --> 00:21:58,360
Started right here with the Moderna vaccine.

322
00:21:58,360 --> 00:22:06,920
We did another cure for a mental disease.

323
00:22:06,960 --> 00:22:10,560
It's actually now in stage three trials.

324
00:22:10,560 --> 00:22:15,000
That's going to be how we create medications from now on.

325
00:22:15,000 --> 00:22:16,000
What are the frontiers?

326
00:22:16,000 --> 00:22:17,000
What can we not do?

327
00:22:17,000 --> 00:22:22,920
So that's where a computer is being creative, and it's not just actually trying something

328
00:22:22,920 --> 00:22:25,360
that occurs to it.

329
00:22:25,360 --> 00:22:29,040
It makes a list of everything that's possible and tries it all.

330
00:22:29,040 --> 00:22:35,040
Is that creativity or is that just brute force with maximum capability?

331
00:22:35,040 --> 00:22:39,640
It's much better than any other form of creativity.

332
00:22:39,640 --> 00:22:43,720
And yes, it's creative because you're trying out every single possibility, and you're doing

333
00:22:43,720 --> 00:22:47,640
it very quickly, and you come up with something that we didn't have before.

334
00:22:47,640 --> 00:22:50,640
I mean, what else would creativity be?

335
00:22:50,640 --> 00:22:51,640
All right.

336
00:22:51,640 --> 00:22:53,680
So we're going to cross the frontier of creativity.

337
00:22:53,680 --> 00:22:55,440
What will we not cross?

338
00:22:55,440 --> 00:22:58,000
What are the challenges that will be outstanding the next 10 years?

339
00:22:58,000 --> 00:23:02,600
Well, we don't know everything, and we haven't gone through this process.

340
00:23:02,600 --> 00:23:09,520
It does require some creativity to imagine what might work, and we have to also be able

341
00:23:09,520 --> 00:23:15,520
to simulate it in a biochemical simulator.

342
00:23:15,520 --> 00:23:22,480
So we actually have to figure that out, and we'll be using people for a while to do that.

343
00:23:22,480 --> 00:23:24,760
So we don't know everything.

344
00:23:24,760 --> 00:23:28,800
I mean, to be able to do everything a human being can do is one thing, but there's so

345
00:23:28,800 --> 00:23:36,400
much we don't know that we want to find out, and that requires creativity.

346
00:23:36,400 --> 00:23:44,240
That will require some kind of human creativity working with machines.

347
00:23:44,240 --> 00:23:45,240
All right.

348
00:23:45,240 --> 00:23:48,400
Let's go back to what's going to happen to get us to the singularity.

349
00:23:48,400 --> 00:23:51,880
So clearly, we have the chart that you showed on the power of compute.

350
00:23:51,880 --> 00:23:56,960
It's been very steady, moving straight up on a logarithmic scale on a straight line.

351
00:23:56,960 --> 00:24:01,840
There are a couple of other elements that you think are necessary to get to the singularity.

352
00:24:01,840 --> 00:24:06,720
One is the rise of nanobots, and the other is the rise of brain-machine interfaces.

353
00:24:06,720 --> 00:24:10,760
And both of those have gone more slowly than AI.

354
00:24:10,760 --> 00:24:12,440
So convince the audience that.

355
00:24:12,440 --> 00:24:20,000
Well, it would be slow, because any time you affect the human body, a lot of people are

356
00:24:20,000 --> 00:24:23,640
going to be concerned about it.

357
00:24:23,640 --> 00:24:31,560
If we do something with computers, we have a new algorithm, or we increase the speed

358
00:24:31,560 --> 00:24:39,720
of it, nobody really is concerned about it.

359
00:24:39,720 --> 00:24:41,520
You can do that.

360
00:24:41,520 --> 00:24:44,840
Nobody cares about any dangers in it.

361
00:24:44,840 --> 00:24:47,760
I mean, that's the reality.

362
00:24:47,760 --> 00:24:49,560
There's some dangers that people care about.

363
00:24:49,560 --> 00:24:50,560
Yes.

364
00:24:50,560 --> 00:24:52,200
But it goes very, very quickly.

365
00:24:52,480 --> 00:24:55,840
There's one of the reasons it goes so fast.

366
00:24:55,840 --> 00:25:01,960
But if you're affecting the body, we have all kinds of concerns that it might have affected

367
00:25:01,960 --> 00:25:02,960
negatively.

368
00:25:02,960 --> 00:25:05,440
And so we want to actually try it on people.

369
00:25:05,440 --> 00:25:12,080
But the reason brain-machine interfaces haven't moved in an exponential curve isn't just

370
00:25:12,080 --> 00:25:17,240
because, you know, lots of people are concerned about the risk to humans.

371
00:25:17,680 --> 00:25:22,120
I mean, as you explain in the book, they just don't work as well as they could.

372
00:25:26,000 --> 00:25:30,560
If we could try things out without having to test it, it would go a lot faster.

373
00:25:30,560 --> 00:25:32,640
I mean, that's the reason it goes slowly.

374
00:25:35,240 --> 00:25:45,800
But there's some thought now that we can actually figure out what's

375
00:25:45,800 --> 00:25:50,880
going on inside the brain and put things into the brain without actually going inside

376
00:25:50,880 --> 00:25:51,880
the brain.

377
00:25:51,880 --> 00:25:54,720
We wouldn't need something like brain link.

378
00:25:54,720 --> 00:26:01,160
We could just, I mean, there's some tests where we can actually tell what's going on

379
00:26:01,160 --> 00:26:05,600
in the brain without actually putting something inside the brain.

380
00:26:05,600 --> 00:26:10,800
And that might actually be a way to do this much more quickly.

381
00:26:10,800 --> 00:26:15,520
But your prediction about the singularity depends, maybe I'm reading it wrong, not just

382
00:26:15,520 --> 00:26:20,000
on the continued exponential growth of compute, but on solving this particular problem, too,

383
00:26:20,000 --> 00:26:21,000
right?

384
00:26:25,000 --> 00:26:32,160
Yes, because we want to increase the amount of intelligence that humans can command, so

385
00:26:32,160 --> 00:26:38,560
we have to be able to marry the best computers with our actual brain.

386
00:26:38,560 --> 00:26:39,800
And why do we have to do that?

387
00:26:39,800 --> 00:26:42,760
Because like right now, here I have my phone.

388
00:26:42,760 --> 00:26:44,520
In some ways, this augments my intelligence.

389
00:26:44,520 --> 00:26:45,520
It's wonderful.

390
00:26:45,520 --> 00:26:46,520
But it's very slow.

391
00:26:46,520 --> 00:26:51,880
I mean, if I ask you a question, you're going to have to type it in or speak it and it takes

392
00:26:51,880 --> 00:26:52,880
a while.

393
00:26:52,880 --> 00:26:58,320
I mean, I ask a question and then people fool around with their computer, it might take

394
00:26:58,320 --> 00:27:00,960
15 seconds or 30 seconds.

395
00:27:00,960 --> 00:27:04,960
It's not like it just goes right into your brain.

396
00:27:04,960 --> 00:27:06,920
I mean, these are very useful.

397
00:27:06,920 --> 00:27:07,920
These are brain extenders.

398
00:27:08,320 --> 00:27:12,520
We didn't have these a little while ago.

399
00:27:12,520 --> 00:27:17,160
Generally in my talks, I ask people who here has their phone.

400
00:27:17,160 --> 00:27:24,440
I'll bet here, maybe there's one or two people, but everybody here has their phone.

401
00:27:24,440 --> 00:27:26,520
That wasn't two, five years ago.

402
00:27:26,520 --> 00:27:29,440
Definitely wasn't two, 10 years ago.

403
00:27:29,440 --> 00:27:35,120
And it is a brain extender, but it does have some speed problems.

404
00:27:35,120 --> 00:27:38,400
So we want to increase that speed.

405
00:27:38,400 --> 00:27:43,280
A question could just come up where we're talking and the computer would instantly tell you

406
00:27:43,280 --> 00:27:49,080
what the answer is without you having to fool around with an external device.

407
00:27:49,080 --> 00:27:52,840
And that's almost feasible today.

408
00:27:52,840 --> 00:27:57,640
And something like that would be helpful to do this.

409
00:27:57,640 --> 00:28:04,200
But could you not get a lot of the good that you talk about if we just kept the problem

410
00:28:04,240 --> 00:28:06,360
with connecting our brains to the machines.

411
00:28:06,360 --> 00:28:11,240
Suddenly you're in this whole world, complicated privacy issues where stuff is being injected

412
00:28:11,240 --> 00:28:13,600
in my brain, stuff in my brain is going elsewhere.

413
00:28:13,600 --> 00:28:17,680
Like you're opening up a whole host of ethical, moral existential problems.

414
00:28:17,680 --> 00:28:21,560
Can't you just make the phones a lot better?

415
00:28:21,560 --> 00:28:27,280
Well, that's the idea that we can do that without having to go inside your brain, but

416
00:28:27,280 --> 00:28:33,280
to be able to tell what's going on in your brain externally without going inside the

417
00:28:33,280 --> 00:28:37,800
brain with some kind of device.

418
00:28:37,800 --> 00:28:39,480
All right, well let's keep moving into the future.

419
00:28:39,480 --> 00:28:42,160
So we're moving into the future with exponential growth in the computer.

420
00:28:42,160 --> 00:28:47,080
We solve a way of ideally figuring out how to communicate directly with your brain to

421
00:28:47,080 --> 00:28:48,320
speed things up.

422
00:28:48,320 --> 00:28:52,560
Explain why nanobots are essential to your vision of where we're going.

423
00:28:52,560 --> 00:28:57,040
Well if you really want to tell what's going on inside the brain, you've got to be able

424
00:28:57,080 --> 00:29:03,080
to go at the level of the particles in the brain so we can actually tell what they're

425
00:29:03,080 --> 00:29:10,080
doing and that's feasible.

426
00:29:10,080 --> 00:29:15,000
We can't actually do it, but we can show that it's feasible.

427
00:29:15,000 --> 00:29:20,400
And that's one possibility.

428
00:29:20,400 --> 00:29:25,400
We're actually hoping that you could do this without actually affecting the brain at all.

429
00:29:25,760 --> 00:29:31,320
Okay, all right, so we're pushing ahead.

430
00:29:31,320 --> 00:29:35,200
We've got nanobots that run around inside of our brain, they're understanding ahead,

431
00:29:35,200 --> 00:29:38,320
they're extracting thoughts, they're inputting thoughts.

432
00:29:38,320 --> 00:29:42,160
Let's go to this nice question, which fits in lovely, from Luis Condreva.

433
00:29:42,160 --> 00:29:47,160
What are the five main ethical questions that we will face as that happens?

434
00:29:47,160 --> 00:29:53,160
Um, is four enough?

435
00:29:53,920 --> 00:29:56,920
Four is fine.

436
00:29:56,920 --> 00:30:02,920
There might even be six Ray, but you can give us four.

437
00:30:07,920 --> 00:30:14,920
I mean we're going to have a lot more power if we can actually, with our own brain, control

438
00:30:14,920 --> 00:30:21,920
computers, does that give people too much power?

439
00:30:24,160 --> 00:30:31,160
Also, I mean right now we talk about having a certain amount of value based on your talent.

440
00:30:39,520 --> 00:30:46,520
This will give talent to people who otherwise don't have talent, and talent won't be as

441
00:30:46,520 --> 00:30:53,520
important, because you'll be able to gain talent just by merging with the right kind

442
00:30:54,080 --> 00:30:59,080
of large language model, or whatever we call them.

443
00:30:59,080 --> 00:31:06,080
And it also seems kind of arbitrary why we would give more power to somebody who has

444
00:31:06,800 --> 00:31:13,800
more talent, because they didn't create that talent, it just happened to happen.

445
00:31:14,800 --> 00:31:21,800
And, what everybody says, we should give somebody who has talent in an area more power.

446
00:31:26,160 --> 00:31:33,160
This way you'd be able to gain talent, just as in the matrix you could learn to fly an

447
00:31:33,160 --> 00:31:40,160
air helicopter, just by downloading the right software, as opposed to spending a lot of

448
00:31:40,160 --> 00:31:47,160
time doing that. Is that fair, or unfair? I mean I think that would fall into the ethical

449
00:31:54,040 --> 00:32:01,040
challenge area. And it's not like we get to the end of this, and say okay this is finally

450
00:32:01,040 --> 00:32:06,040
what the singularity is all about, and people can do certain things, and they can't do other

451
00:32:06,040 --> 00:32:11,040
things, but it's over. We'll never get to that point. I mean this curve is going to

452
00:32:11,040 --> 00:32:18,040
continue, the other curve is going to continue indefinitely. And we've actually shown, for

453
00:32:20,560 --> 00:32:27,560
example, with nanotechnology we can create a computer, and we can create a computer,

454
00:32:31,960 --> 00:32:38,960
where one liter computer would actually match the amount of power that all human beings

455
00:32:39,280 --> 00:32:46,280
today have, like 10 to the 10th persons, would all fit into one liter computer. Does that

456
00:32:52,320 --> 00:32:59,320
create ethical problems? So I mean a lot of the implications kind of run against what

457
00:33:00,320 --> 00:33:07,320
we've been assuming about human beings. Wait, on the talent question, which is super

458
00:33:07,320 --> 00:33:14,320
interesting, do you feel like everybody, when we get to 2040, will have equal capacities?

459
00:33:16,680 --> 00:33:22,400
I think it will be more different, because we'll have different interests. You might

460
00:33:22,400 --> 00:33:28,200
be into some fantastic type of music, and I might be into some kind of literature or

461
00:33:28,240 --> 00:33:35,240
something else. We're going to have different interests, and so we'll excel at certain

462
00:33:36,320 --> 00:33:42,280
things depending on what your interests are. So it's not like we all have the same amount

463
00:33:42,280 --> 00:33:47,800
of power, but we'll all have fantastic power compared to what we have today.

464
00:33:47,800 --> 00:33:50,680
And if you're in Texas, where there are no regulations, you'll probably get it first

465
00:33:50,680 --> 00:33:52,080
instead of you in Massachusetts.

466
00:33:52,080 --> 00:33:53,280
Exactly, yeah.

467
00:33:54,280 --> 00:33:58,040
Let me ask you another ethical question while we're on this one. So about a few minutes

468
00:33:58,040 --> 00:34:03,440
ago you mentioned the capacity to replicate someone's brain and bring them back. So let's

469
00:34:03,440 --> 00:34:09,320
say I do that with my father, passed away six years ago, sadly. I bring him back, and

470
00:34:09,320 --> 00:34:15,240
I'm able to create a mind and a body just like my father's. It's exact, perfect replica,

471
00:34:15,240 --> 00:34:21,480
all those thoughts. What happens to all the bills that he owed when he died? Because that's

472
00:34:21,520 --> 00:34:24,640
a lot of money, and a lot of bill collectors call me. Do we have to pay those off, or are

473
00:34:24,640 --> 00:34:26,440
we good?

474
00:34:26,440 --> 00:34:33,440
Well, we're doing something like that with my daughter, and you can read about this in

475
00:34:36,080 --> 00:34:42,840
her book, and it's also in my book. We collected everything my father had written. He died

476
00:34:42,880 --> 00:34:49,880
when I was 22, so he's been dead for more than 50 years. And we fed that into a large

477
00:34:53,520 --> 00:35:00,520
language model, and basically answered the question of all the things he ever wrote,

478
00:35:01,880 --> 00:35:08,000
what best answers this question? And then you could put any question you want, and then

479
00:35:08,040 --> 00:35:12,840
you could talk to it. You'd say something, you then go through everything he ever had

480
00:35:12,840 --> 00:35:19,200
written and find the best answer that he actually wrote to that question. And it actually was

481
00:35:19,200 --> 00:35:25,200
a lot of like talking to him. You could ask him what he liked about music. He was a musician.

482
00:35:25,200 --> 00:35:32,200
He actually liked Brahms the best. And it was very much like talking to him. And I

483
00:35:33,200 --> 00:35:40,200
reported on this in my book, and Amy talks about this in her book. And Amy actually asked

484
00:35:42,560 --> 00:35:48,760
the question, could I fall in love with this person, even though I've never met him? And

485
00:35:48,760 --> 00:35:53,160
she does a pretty good job. I mean, you really do fall in love with this character that she

486
00:35:53,160 --> 00:36:00,160
creates, even though she never met him.

487
00:36:02,920 --> 00:36:08,400
So we can actually, with today's technology, do something where you can actually emulate

488
00:36:08,400 --> 00:36:15,400
somebody else. And I think as we get further on, we can actually do that more and more

489
00:36:15,400 --> 00:36:22,040
responsibly and more and more that really would match that person, and actually emulate

490
00:36:22,040 --> 00:36:25,280
the way he would move and so on. It's a tone of voice.

491
00:36:25,280 --> 00:36:28,880
Well, you know, my dad, he loved Brahms too, particularly those piano trios. So if we can

492
00:36:28,920 --> 00:36:33,920
solve the back taxes problem, we'll get my dad and your dad's bots. Hang out. It would

493
00:36:33,920 --> 00:36:34,920
be great.

494
00:36:34,920 --> 00:36:37,920
Well, yeah. That would be cool.

495
00:36:37,920 --> 00:36:38,920
All right.

496
00:36:38,920 --> 00:36:44,920
All right. We've got 20 minutes left. I want to get to the thing that I most want to understand,

497
00:36:44,920 --> 00:36:47,880
because by the way, this book is wonderful. I think you guys are all going to get signed

498
00:36:47,880 --> 00:36:52,280
copies of it when it comes out. It's truly remarkable, as are all of Ray's books. Whether

499
00:36:52,280 --> 00:36:55,400
you agree or disagree, they'll definitely make you think more.

500
00:36:55,440 --> 00:37:00,600
One of the things that I don't think you do in this book is describe what a day will

501
00:37:00,600 --> 00:37:07,600
be like in 2045 when we're all much more intelligent. So it's 2045. We're all million

502
00:37:07,760 --> 00:37:12,760
times as intelligent. I wake up. Do I have breakfast or do I not have breakfast?

503
00:37:12,760 --> 00:37:19,760
Well, the answer to that question is kind of the same as it is now. But first of all,

504
00:37:26,400 --> 00:37:33,400
the reason it's called a singularity is because we don't really fully understand that question.

505
00:37:36,040 --> 00:37:42,640
Singularity is borrowed from physics. Singularity in physics is where you have a black hole

506
00:37:42,640 --> 00:37:46,600
and no light can escape. And so you can't actually tell what's going on inside the black

507
00:37:46,600 --> 00:37:52,960
hole. And so we call it a singularity, a physical singularity. So this is a historical

508
00:37:53,000 --> 00:38:00,000
singularity where we're borrowing that term from physics and call it a singularity. Because

509
00:38:00,120 --> 00:38:04,560
we can't really answer the question. If we actually multiply our intelligence a million

510
00:38:04,560 --> 00:38:10,680
fold, what's that like? It's a little bit like asking a mouse, gee, what would it be

511
00:38:10,680 --> 00:38:17,520
like if you had the amount of intelligence of this person? The mouse wouldn't really

512
00:38:17,640 --> 00:38:24,640
even understand the question. It does have intelligence. It has a fair amount of intelligence.

513
00:38:24,880 --> 00:38:30,400
But it couldn't understand that question. It couldn't articulate an answer. That's a

514
00:38:30,400 --> 00:38:37,000
little bit what it would be like for us to take the next step in intelligence by adding

515
00:38:37,000 --> 00:38:39,680
all the intelligence that the singularity would provide.

516
00:38:39,680 --> 00:38:41,240
Wait, I just want to make sure I understand.

517
00:38:41,240 --> 00:38:48,240
But I'll give you one answer. I said if you're diligent, you'll achieve longevity escape

518
00:38:49,280 --> 00:38:56,280
velocity in five or six years. And if we want to actually emulate everything that's going

519
00:38:56,280 --> 00:39:03,280
on inside the brain, let's go out a few more years. Let's say the 2040, 2045. Now there's

520
00:39:14,800 --> 00:39:20,800
a lot, you talk to a person, they've got all the connections that they had originally

521
00:39:20,800 --> 00:39:27,800
plus all the seditional connections that we add through having them access computers and

522
00:39:29,640 --> 00:39:36,640
that becomes part of their thinking. So can you, suppose a person like blows up or something

523
00:39:39,480 --> 00:39:46,480
happens to their mind, you definitely can recreate everything that's of a computer origin

524
00:39:47,480 --> 00:39:53,800
because we do that now. Any time we create anything with a computer, it's backed up.

525
00:39:53,800 --> 00:40:00,880
So if the computer goes away, you've got the backup and you can recreate it. It says okay,

526
00:40:00,880 --> 00:40:07,880
but what about the thinking in their normal brain that's not done with computers? We don't

527
00:40:08,240 --> 00:40:15,240
have some ways of backing that up. When we get to the singularity with 2045, we'll be

528
00:40:16,240 --> 00:40:22,040
able to back that up as well because we'll be able to figure out, we'll have some ways

529
00:40:22,040 --> 00:40:29,040
of actually figuring out what's going on in that sort of non-mechanical brain. So we'll

530
00:40:29,640 --> 00:40:36,640
be able to back up both their normal brain as well as the computer addition. And I believe

531
00:40:42,880 --> 00:40:45,680
that's feasible by 2045.

532
00:40:45,680 --> 00:40:48,280
In your vision of it?

533
00:40:48,280 --> 00:40:53,920
So you can back up their entire brain. Now that doesn't guarantee, the whole world could

534
00:40:54,000 --> 00:41:00,000
blow up and lose all the data centers. So it's not absolute guarantee.

535
00:41:00,000 --> 00:41:07,000
Yeah, I'll be ashamed. What I don't understand is will we even be fully distinct people? If

536
00:41:07,000 --> 00:41:13,160
we're sharing memories and we're all uploading our brains to the cloud and we're getting

537
00:41:13,160 --> 00:41:20,000
all this information coming back directly into our neocortex, are we still distinct?

538
00:41:20,800 --> 00:41:27,800
Yes, but we could also find new ways of communicating. So the computers that extend my brain, the

539
00:41:32,480 --> 00:41:38,840
right computers that extend your brain, we could create something that's like a hybrid

540
00:41:38,840 --> 00:41:44,280
or not. And it would be up to our own decision as to whether or not to do that. So there'll

541
00:41:44,280 --> 00:41:47,880
be some new ways of communicating.

542
00:41:47,880 --> 00:41:51,560
Let me ask another question about this. This is what, when I was reading the book, this

543
00:41:51,560 --> 00:41:56,800
is where I kept getting stuck. You are extremely optimistic. You're optimistic about where

544
00:41:56,800 --> 00:42:01,600
we are today. You're optimistic that technology has been a massive force for good. You're

545
00:42:01,600 --> 00:42:06,880
optimistic that it will continue to be a massive force for good. Yet there is a lot of uncertainty

546
00:42:06,880 --> 00:42:08,560
in the future you were describing.

547
00:42:08,560 --> 00:42:15,560
Well, first of all, I'm not necessarily optimistic. The things that can go wrong are

548
00:42:18,160 --> 00:42:25,160
we had things that can go wrong before we had computers. When I was a child, atomic

549
00:42:27,600 --> 00:42:34,600
weapons were created and people were very worried about an atomic war. We would actually

550
00:42:36,560 --> 00:42:42,360
get under our desk and put our hands behind our head to protect us against an atomic war.

551
00:42:43,320 --> 00:42:50,320
It seemed to work, actually. It was still here. But if you had asked people, we had actually

552
00:42:51,000 --> 00:42:58,000
two weapons that went off in anger and killed a lot of people within a week. And if you

553
00:42:58,400 --> 00:43:02,200
had asked people, what's the chance that we're going to go another 80 years and this will

554
00:43:02,200 --> 00:43:09,200
never happen again, nobody would say that that was true. But it has happened. That doesn't

555
00:43:10,200 --> 00:43:17,200
mean it's not going to happen next week. But anyway, that's a great danger. And I think

556
00:43:18,800 --> 00:43:25,800
that's a much greater danger than computers are. Yes, they're dangers, but the computers

557
00:43:25,920 --> 00:43:32,920
will also be more intelligent to avoid kinds of dangers. Yes, it's some bad people in the

558
00:43:33,920 --> 00:43:40,920
world, but I mean, go back 80, 90 years, we had 100 million people die in Asia and Europe

559
00:43:44,920 --> 00:43:51,920
from World War II. We don't have wars like that anymore. We could. We certainly have the

560
00:43:54,000 --> 00:44:01,000
atomic weapons to do that. And you could also imagine computers could be involved with that.

561
00:44:03,800 --> 00:44:10,800
But if you actually look, and this goes right through one piece. First of all, if you look

562
00:44:11,040 --> 00:44:18,040
at my lineage of computers going from tiny fraction of one calculation to 65 billion,

563
00:44:20,280 --> 00:44:27,280
that's a 20 quadrillion fold increase that we've achieved in 80 years. And look at this,

564
00:44:27,280 --> 00:44:34,280
the U.S. personal income, this is done in constant dollars, so this has nothing to do

565
00:44:34,360 --> 00:44:41,360
with inflation. And this is the average income in the United States. It went, it's multiplied

566
00:44:46,880 --> 00:44:53,880
by about 100 fold. And we live far more successfully than we used to. And I think that's a great

567
00:44:57,520 --> 00:45:02,520
question. And we live far more successfully. People say, oh, things were great 100 years

568
00:45:02,520 --> 00:45:08,520
ago, they weren't. And you can look at this chart. I've got 50 charts in the book that

569
00:45:08,520 --> 00:45:14,640
show the kind of progress we've made. A number of people that live in dire poverty has gone

570
00:45:14,640 --> 00:45:20,640
down dramatically. We actually did a poll where they asked people, people that live in poverty

571
00:45:21,080 --> 00:45:27,580
as they've gone up or down, 80 percent said it's gone up. But the reality is it's actually

572
00:45:27,580 --> 00:45:34,580
fallen by 50 percent in the last 20 years. So what we think about the past is really

573
00:45:44,840 --> 00:45:50,240
the opposite of what's happened. Things have gotten far better than they have. And computers

574
00:45:50,320 --> 00:45:54,520
are going to make things even better. I mean, just the kind of things you can do now with

575
00:45:54,520 --> 00:45:57,600
a large language model, it didn't exist two years ago.

576
00:45:57,600 --> 00:46:04,360
Do you ever worry that take it as a given, if yours have made things better, take it

577
00:46:04,360 --> 00:46:08,360
as a given, that personal income will keep going up? Do you ever worry it's just coming

578
00:46:08,360 --> 00:46:12,840
too quickly and it'll be better if maybe the slope of the Kurzweil curve was a little

579
00:46:12,840 --> 00:46:13,320
less steep?

580
00:46:13,320 --> 00:46:19,840
That's a big difference in the past. I mean, talk about what effect did the railroad have.

581
00:46:20,440 --> 00:46:27,440
I mean, lots of jobs were lost or even the cotton genie that happened 200 years ago. And

582
00:46:27,520 --> 00:46:33,400
people were quite happy making money with the cotton genie and suddenly that was gone

583
00:46:33,400 --> 00:46:38,440
and machines were doing that. And people say, well, wait till this gets going, all jobs

584
00:46:38,440 --> 00:46:45,440
will be lost. And that's actually what was said at that time. But actually, income went

585
00:46:45,440 --> 00:46:52,440
up, more and more people worked. We created, and if you say, well, what are they going

586
00:46:53,080 --> 00:46:59,280
to do? You couldn't answer that question because it was in industries that nobody had a clue

587
00:46:59,280 --> 00:47:06,280
of, like for example, all of electronics. So things are getting better even if jobs

588
00:47:06,720 --> 00:47:13,720
are lost. Now, you can certainly point to jobs, like take computer programming. Google

589
00:47:19,760 --> 00:47:26,760
has, I don't know, 60,000 people that program computers and lots of other companies do.

590
00:47:27,560 --> 00:47:33,760
At some point, that's not going to be a feasible job. They can already code, lots of language

591
00:47:33,800 --> 00:47:40,800
models can write code, not quite the way an expert programmer can do. But how long is

592
00:47:40,800 --> 00:47:47,800
that going to take? It's measured in years, not in decades. Nonetheless, I believe that

593
00:47:50,320 --> 00:47:57,320
things will get better because we wipe out jobs, but we create other ways of having an

594
00:47:58,320 --> 00:48:05,320
income. And if you actually point to something, let's say this machine, and this is being

595
00:48:07,440 --> 00:48:13,320
worked on, can wash dishes. You just have a bunch of dishes that will pick the ones that

596
00:48:13,320 --> 00:48:20,320
have to go in the dishwasher and clean everything else up, and that will wash dishes for you.

597
00:48:21,320 --> 00:48:27,320
Would we want that not to happen? Would we say, well, this is kind of upsetting things,

598
00:48:27,320 --> 00:48:34,320
let's get rid of it. It's not going to happen, and no one would advocate that. So we'll

599
00:48:36,600 --> 00:48:42,600
find things to do, we'll have other methods of distributing money, and it will be, it

600
00:48:42,600 --> 00:48:47,600
will continue these kinds of curves that we've seen already.

601
00:48:48,360 --> 00:48:54,360
This is kind of remarkable that we got large language models before we got robotic dishwashers.

602
00:48:54,360 --> 00:49:01,360
You have grandchildren. What would you tell a young person? They buy in, they agree, or

603
00:49:02,440 --> 00:49:08,240
how would you tell them to best prepare themselves for what will be, if you're correct, a remarkably

604
00:49:08,240 --> 00:49:11,240
different future?

605
00:49:11,240 --> 00:49:17,520
I'd be less concerned about what will make money, and much more concerned about what

606
00:49:17,560 --> 00:49:24,560
turns them on. They love video games, so they should learn about that. They should read

607
00:49:27,720 --> 00:49:33,720
literature that turns them on. Some of those literature in the future will be created by

608
00:49:33,720 --> 00:49:40,720
computers. And find out what in the world has a positive effect on their mental being.

609
00:49:40,720 --> 00:49:47,720
And if you know that your child or your grandchild, this gets to one of the questions that is asked

610
00:49:51,680 --> 00:49:57,480
on the screen here, if you know that someone is going to live for hundreds of years, as

611
00:49:57,480 --> 00:50:01,680
you predict, how does that affect the way, certainly it means they shouldn't retire

612
00:50:01,680 --> 00:50:06,440
at 65, but what else does it change about the way they should think about their lives?

613
00:50:06,840 --> 00:50:13,840
Well, I talk to people and say, well, I wouldn't want to live past 100, or maybe they're a

614
00:50:13,840 --> 00:50:20,840
little more ambitious to say, I don't want to live past 110. But if you actually look,

615
00:50:22,680 --> 00:50:29,200
when people decide they've had enough and they don't want to live anymore, that never,

616
00:50:29,200 --> 00:50:35,520
ever happens unless these people are in some kind of dire pain, in physical pain or emotional

617
00:50:35,880 --> 00:50:42,880
pain or spiritual pain or whatever, and they just cannot bear to be alive anymore, nobody

618
00:50:43,120 --> 00:50:50,120
takes their lives other than that. And if we can actually overcome many kinds of physical

619
00:50:51,920 --> 00:50:58,920
problems, cancers wiped out and so on, which I expect to happen, people will be even that

620
00:50:59,920 --> 00:51:06,920
much more happy to live and they'll want to continue to experience tomorrow, and tomorrow

621
00:51:09,200 --> 00:51:16,200
is going to be better and better, these kinds of progress is not going to go away. So people

622
00:51:17,320 --> 00:51:24,320
will want to live, unless they're in dire pain, but that's what the whole sort of medical

623
00:51:24,720 --> 00:51:31,720
profession is about, which is going to be greatly amplified by tomorrow's computers.

624
00:51:31,720 --> 00:51:35,720
Let me ask you a great question that has popped on the screen. This is from Colin McCabe.

625
00:51:35,720 --> 00:51:41,520
AI is a black box. Nobody knows how it was built. How do you show that AI is trustworthy

626
00:51:41,520 --> 00:51:46,020
to users who want to trust it, adopt it, and accept it, particularly if you're going to

627
00:51:46,020 --> 00:51:53,020
upload it directly into your brain? Well, it's not sure that nobody knows how they work.

628
00:51:53,520 --> 00:51:59,520
Right. Most people who are using a large language model don't know what data sense went into

629
00:51:59,520 --> 00:52:03,520
it. They're things that happen in the transformer layer that even the architects don't understand.

630
00:52:03,520 --> 00:52:10,020
Right. But we're going to learn more and more about that, and in fact how computers work

631
00:52:10,020 --> 00:52:17,020
will be, I think, a very common type of talent that people want to gain. And ultimately we'll

632
00:52:18,020 --> 00:52:24,520
have more trust of computers. I mean, large language models aren't perfect, and you can

633
00:52:24,520 --> 00:52:31,520
ask the question and it can give you something that's incorrect. I mean, we've seen that

634
00:52:33,160 --> 00:52:40,160
just recently. The reason we have these computers give you incorrect information is it doesn't

635
00:52:40,160 --> 00:52:46,160
have the information to begin with, and it actually doesn't know what it doesn't know.

636
00:52:46,160 --> 00:52:53,160
And that's actually something we're working on so that it knows, well, I don't know that.

637
00:52:53,160 --> 00:52:59,160
That's actually very good if it can actually say that, because right now it will find the

638
00:52:59,160 --> 00:53:06,160
best thing it knows. And if it's never trained on that information and there's nothing in

639
00:53:07,160 --> 00:53:14,160
there that tells you, it'll just give you the best guess, which could be very incorrect.

640
00:53:14,160 --> 00:53:20,160
And we're actually learning to be able to figure out when it knows and when it doesn't know.

641
00:53:20,160 --> 00:53:27,160
But ultimately we'll have pretty good confidence when it knows and when it doesn't know, and

642
00:53:28,160 --> 00:53:34,160
we can actually rely on what it says.

643
00:53:34,160 --> 00:53:39,160
So your answer to the question is, A, we will understand more, and B, they'll be much more

644
00:53:39,160 --> 00:53:43,160
trustworthy, so it won't be as risky to not understand them.

645
00:53:43,160 --> 00:53:44,160
Right.

646
00:53:44,160 --> 00:53:50,160
You spent your life making predictions, some of which like the Turing test, you've held

647
00:53:50,160 --> 00:53:54,160
on to them and been remarkably accurate. As you move from an overwhelming optimist to

648
00:53:54,160 --> 00:53:57,160
a pessimist, what is a prediction?

649
00:53:57,160 --> 00:54:02,160
Well, my books have always had a chapter on how these things can go wrong.

650
00:54:02,160 --> 00:54:09,160
Tell me a prediction that you are chewing over right now, but you're not sure whether you

651
00:54:09,160 --> 00:54:15,160
want to make it or whether you don't want to make it.

652
00:54:15,160 --> 00:54:22,160
I mean, there's well-known dangers in nanotechnology. If someone were to create nanotechnology,

653
00:54:23,160 --> 00:54:30,160
that replicates, well-known as if it replicates everything into paper clips, turn the entire

654
00:54:33,160 --> 00:54:38,160
world into paper clips, that would not be positive.

655
00:54:38,160 --> 00:54:39,160
No.

656
00:54:39,160 --> 00:54:41,160
Unless you're staples.

657
00:54:41,160 --> 00:54:48,160
And that's feasible. Take somebody who's...

658
00:54:49,160 --> 00:54:56,160
a little bit mental to do that, but it could be done.

659
00:54:56,160 --> 00:55:03,160
And we actually will have something that actually avoids that.

660
00:55:03,160 --> 00:55:12,160
So we'll have something that can detect that this is actually turning everything into paper

661
00:55:12,160 --> 00:55:17,160
clips and destroy it before it does that.

662
00:55:17,160 --> 00:55:25,160
But I mean, I have a chapter in this new book, the Singularity is Nearer, that talks about

663
00:55:25,160 --> 00:55:27,160
the kinds of things that could happen.

664
00:55:27,160 --> 00:55:31,160
Oh, the most remarkable part of this book is he does exactly the mathematical calculations

665
00:55:31,160 --> 00:55:35,160
on how long it would take nanobots to turn the world into grey goo and how long it would

666
00:55:35,160 --> 00:55:38,160
take the blue goo to stop the grey goo. It's remarkable.

667
00:55:38,160 --> 00:55:41,160
The book will be out soon. You definitely need to read until the end.

668
00:55:41,160 --> 00:55:45,160
But this leads to a... maybe... let me try and answer it.

669
00:55:45,160 --> 00:55:50,160
The question I asked before is what should young people think about and be working on?

670
00:55:50,160 --> 00:55:52,160
You said their passions and what turns them on.

671
00:55:52,160 --> 00:56:00,160
Shouldn't they be thinking through how to design and architect these future systems

672
00:56:00,160 --> 00:56:03,160
so they are less likely to turn us into grey goo or paper clips?

673
00:56:03,160 --> 00:56:06,160
I don't know if everybody wants to work on that.

674
00:56:06,160 --> 00:56:10,160
But folks in this room, technologically minded, you guys should all be working on not turning

675
00:56:10,160 --> 00:56:12,160
us into grey goo, right?

676
00:56:12,160 --> 00:56:14,160
Yes, that would be on the list.

677
00:56:14,160 --> 00:56:20,160
But then that leads to another question, which is what will the role of humans be in thinking

678
00:56:20,160 --> 00:56:24,160
through that problem when they're only a millionth or a billionth or a trillionth as intelligent

679
00:56:24,160 --> 00:56:28,160
as machines?

680
00:56:28,160 --> 00:56:29,160
Say that again?

681
00:56:29,160 --> 00:56:33,160
So we're going to have these really hard problems to solve.

682
00:56:33,160 --> 00:56:39,160
Right now, we are, along with our machines, we can be extremely intelligent.

683
00:56:39,160 --> 00:56:44,160
But 10 years from now, 15 years from now, there will be machines that will be so much more intelligent

684
00:56:44,160 --> 00:56:45,160
than us.

685
00:56:45,160 --> 00:56:49,160
What will our role... what will the role of humans be in trying to solve these problems?

686
00:56:49,160 --> 00:56:52,160
Well, first of all, I see those as extensions of humans.

687
00:56:52,160 --> 00:56:55,160
We wouldn't have them if we didn't have humans to begin with.

688
00:56:55,160 --> 00:56:58,160
And humans have a brain that can think these things through.

689
00:56:58,160 --> 00:57:01,160
And we have this thumb.

690
00:57:01,160 --> 00:57:03,160
It's not really very much appreciated.

691
00:57:03,160 --> 00:57:08,160
But like whales and elephants actually have a larger brain than we have.

692
00:57:08,160 --> 00:57:15,160
And they can probably think deeper thoughts, but they don't have a thumb until they don't create technology.

693
00:57:15,160 --> 00:57:22,160
A monkey can create... it actually has a thumb, but it's actually down an inch or so,

694
00:57:22,160 --> 00:57:24,160
and therefore it really can't grab very well.

695
00:57:24,160 --> 00:57:30,160
So it can create a little bit of technology, but the technology it creates cannot create other technology.

696
00:57:30,160 --> 00:57:36,160
So the fact that we have a thumb means we can create integrated circuits

697
00:57:36,160 --> 00:57:43,160
that can become a large language model that comes from the human brain.

698
00:57:43,160 --> 00:57:50,160
And it's actually trained with everything that we've ever thought.

699
00:57:50,160 --> 00:57:59,160
Anything that human beings have thought has been documented and it can go into these large language models.

700
00:57:59,160 --> 00:58:02,160
And everybody can work on these things.

701
00:58:02,160 --> 00:58:06,160
And it's not true only certain wealthy people will have it.

702
00:58:06,160 --> 00:58:10,160
I mean, how many people here have phones?

703
00:58:10,160 --> 00:58:14,160
If it's not 100%, it's like 99.9%.

704
00:58:14,160 --> 00:58:20,160
And you don't have to be kind of from a wealthy group.

705
00:58:20,160 --> 00:58:25,160
I mean, I see people who are homeless who have their own phone.

706
00:58:25,160 --> 00:58:29,160
It's not that expensive.

707
00:58:29,160 --> 00:58:35,160
And so that represents the distribution of these capabilities.

708
00:58:35,160 --> 00:58:39,160
It's not something you have to be fabulously wealthy to afford.

709
00:58:39,160 --> 00:58:44,160
So you think that we're heading into a future where we're going to live much longer and will be much more equal?

710
00:58:44,160 --> 00:58:50,160
We think we're heading into a society where we'll live much longer, be wealthier, but also much more equality.

711
00:58:50,160 --> 00:58:53,160
Yes, absolutely. And we've seen that already.

712
00:58:53,160 --> 00:59:01,160
Well, we're at time, but Ray and I will be back in 21, 24, 22, 24, and 23, 24.

713
00:59:01,160 --> 00:59:06,160
So thank you for coming today. Thank you so much. He is an American treasure. Thank you.

