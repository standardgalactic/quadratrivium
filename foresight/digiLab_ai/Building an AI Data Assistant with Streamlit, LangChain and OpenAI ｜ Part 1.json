{"text": " Imagine accelerating your machine learning projects with an AI assistant that will save you hours and hours of work. Welcome to Digilab Academy. You go to destination for in-depth courses and resources to help you master the world of data science and AI. I'm Anna and in this series I'm going to show you how to build your own AI assistant using StreamLid and LangChain. I'll walk you through the entire process from selling the required libraries to solve a machine learning project using AI. Make sure that you take the full reading tutorial that accompanies this video in the Digilab Academy website. You'll find the link in the video description below, plus you can download the final project and also the dataset that we will be using. Now let's break down what to expect in this video. In this project, StreamLid is the foundation for the user interface, allowing you to upload CSV files, visualize data and interact with the AI assistant. The beauty of StreamLid is its simplicity. Even if you have limited web development experience, you can create dynamic and interactive data applications. While StreamLid is beginner-friendly, it also offers customization options for those who want to create more sophisticated applications. Finally, StreamLid makes it easy to deploy your web apps to the cloud or share them with others, further enhancing its utility for collaborative data projects. We'll start by structuring their StreamLid app with titles, headings, subheadings, captions and text formatting. Next, we'll implement dividers to segment your content, making it easier for users to navigate through your AI assistant. Uploading CSV files is essential for this data-driven app. We'll cover how to implement this. To enhance user interaction, we'll create a dynamic sidebar and expanders, and we will discuss how to display graphs for data visualization. We'll also explore text input, which allow users to interact with your AI assistant through text. Caching is a powerful technique for optimizing performance. We'll explore this topic and show how you implement it effectively. Understanding session state and implementing non-statefold buttons are a key to creating responsive applications. We'll cover this in detail. Now, let's shift our focus to LangChain. LangChain is a framework that can be used to build conversational AI systems that can understand and respond to user queries. Its main components are models, agents, tools, prompt templates, chains, memory and indexes. The project integrates OpenAI's GPT-3 3.5 Turbo Large Language Model. LLMs are integrated into the StreamLid application, allowing you to have a dynamic real-time interactions with the AI assistant. You can ask questions, seek explanations and receive immediate responses. We'll start by setting the OpenAI key. Then we will explore how to load and use OpenAI large language models to generate information, open up a world of possibilities for your AI assistant. With the Pandas Agent, you can answer specific predefined questions about your data frame or any variable of your choice. I'll show you how to set this up. You can also enable your AI assistant to answer specific questions chosen by the user. What you learn here can be applied to your own data analysis problems. For example, you might want to create an assistant to help you explore sales data or customer reviews. Why we'll focus on one specific use case, you can easily adapt these techniques to your unique needs. So without further ado, let's start building our AI assistant. Hit that like button and don't forget to subscribe for more data science adventures. Let's get started. Okay, so before we start building our AI assistant, there are two things that we need to do. First is to set the API key and the second is to run Streamlet. To set the API key, you need an API key from OpenAI and you need to boot VAL into a different script in your same directory. What I've done is I created this API key file and there what we need to do is store our key. So here you would put your key as a string, but in this file called API key. The reason that I'm not sharing is because this key is a secret key and it shouldn't be shared. So let's remove that and just remember that that goes to your API key script. The second is to run Streamlet. For that, what I want you to do is to open the terminal, go to your directory and what I want you to do is to run Streamlet using this command. So that is Streamlet run and then the name of your API script where you're going to write the code for the AI assistant and just press enter. And what's going to happen is that a new window is going to pop up in your browser and there's where we're going to see your app. Now we're going to import the packages that are going to provide the necessary functionality for our project. The packages that we're going to import are OS, API key, long chain, dot M, Streamlet and pandas. So let's write that down. Okay, I'm going to close this player so we can see the code better. So as you can see, we have imported the recurrent libraries. So the OS library provides a way of using operating system dependent functionality. Then we have API key in order to load or API key correctly. Then Streamlet that is the heart of our project really then behind this library fundamental for data manipulation and visualization. This we have the package long chain, which is specific to our project and that incorporates open AI language models that is going to allow us to interact with the assistant. And then we have dot M, very important package to make sure that sensitive information such as the API key is securely stored. Okay, so now we're going to start building the user interface a little and I'm going to show you how to add titles, subtitles, headings and other stuff that is going to make the user experience a little bit more engaging. So first of all, we want our assistant to have a title and also we want to have a welcoming message. Okay, so let's write that. So in extremely in order to write a title, we do sd.title and then inside of the brackets, we write the title that we want to add to our app. So I'm going to call a assistant for data science and I'm actually going to capitalize that and let's add a header now. So in order to add headers and striplet is just sd.hether. And we're going to add, for example, exploratory data analysis part. If you want to write a subheader is just sd.subheader. So let's say, for example, solution. Okay, as you can see, nothing appears on the app and in order to see what we're reading, then we need to click rerun. So let's do that and let's see what happens. So this is running. And you can see now the title appears and we have here the header and this header as well. As I said, we want to write a welcoming message. So for that, we're going to just use plain text in order to do that in striplet, we use sd.write. So let's write for now just welcoming message and see what happens when we rerun. So we rerun and it appears here. So I'm going to write something along the lines of, hello, I'm your assistant and I'm here to assist you with your machine learning projects. So let's write something like that. So, okay, so let's see what happens when we run. Okay, so now our placeholder for the welcoming message has been populated and now we have hello, I'm your assistant and I'm here to help you with your data science projects. I'm going to move the header and this header after the welcoming message to enhance user experience. Something that we can do is add a sidebar on the side of the application and to do that we can use with sd sidebar. So I think something interesting that we can put in the sidebar is an explanation of what the user can expect from the app. For that, what I'm going to do is to write a text. So using sd.write that will appeal in the sidebar. So it needs to be within the with statement. So let's write that. So your data science adventure begins with the CSV file. And now I'm going to add some extra text. Okay, and I'm going to add that to make sure that this is actually a string. Okay, let's rerun. Okay, so as you can see, this has appeared here. Let's click and this is our sidebar with our text in the sidebar giving a little bit of explanation of what the user can expect from the app. We have your data science adventure begins with the CSV file. You may already know that every exciting data science journey starts with a data set. That's why I would love for you to upload a CSV file. Once we have your data in hand, we'll dive into understanding it and have some fun exploring it. Then we'll work together to shape your business challenge into a data science framework. And I'll introduce you to the coolest machine learning modules and we'll use them to tackle your problem. Sounds fun, right? This looks alright like this, but we can make it a little bit better and a little bit more visually appealing. So what we're going to do is use some text formatting to make this look a little bit more organized. So first of all, let's put the first sentence into a title and then the rest is going to be the text. But instead of using titles and write, we're going to use write and caption. So caption is used more footnotes to add the text to images and things like that. But just because I like the formatting of this particular way of writing text, I'm just going to use it for this. So let's do that. I'm going to leave that in write. And then what I'm going to do is I'm going to write st.caption and then add this in brackets. So let's run and see what is happening. Brilliant. So now we have a small title at the beginning and then text. I'm going to write this here. That is better. Perfect. Something that we can do is add bold text and italics. So I'm going to show you how to do that. So if we write here an asterisk and here another asterisk at the end over string and here we add double asterisk. When we run, we're going to see that the first sentence is now going to be italics and the rest is going to be bold. So that means that as you can see, if we want to write text in bold, we use double asterisk and then we want to write text in italics. We write just one asterisk. Okay, let's continue talking about formatting and the appearance of our app. I think it would be cool if we could add a little line here and we can achieve that by using dividers. So in StripLid, in order to add dividers, we can use manst.divider. So just as this, if we run, we will see that this line has appeared here. And finally, something that I would like to add is just to say that I made this app. So I'm going to use caption. Okay, and if we run, we're going to see that that appears here. I think it would look better if we could make that centered. Luckily in StripLid, we can use HTML in order to further format our text. And that is what I'm going to do now in order to make this text move to the center. And we need to add another parameter, which is this one. The default is set to false, but in order to make the HTML work, we need to set that to true. So let's do that. Okay, and when that is done, we cannot rerun this. And there you can see that it has been centered. There are other things that we can add to our application. Something that is quite interesting is expanders. So let's see how to do that with st.expander. So inside of the expander, we're just going to write some placeholder text. And let's run and see what happens. So as you can see, this has appeared here. So the user can click and then it can decide, you know, they want to look at that information or not. So then that we can also add our emojis just to make it a little bit more fun. So I'm just going to add a couple there. Okay, let's rerun and see what happens. So now the emojis have appeared on the screen. Okay, so now I'm going to show you how to add a button and how to manage what is called session state. So in order to add a button in striplet, I'm going to add it just before the header is as easy as this. So, well, it's with st.button, but because we want to trigger an action, we're going to add an if statement. So if st.button, let's get started. So when the user clicks the button, then this is going to happen. I'm going to move that there. And in fact, let me just move all of this after the sidebar. Okay, okay, so let's rerun. And we can see that now this button appears here when we click. Then we have the header and the subheader. So let's continue building our app and I'll show you in a second why session state is so important. As we said in the explanation, your data science adventure begins with a CSV file. So now I'm going to show you how to integrate a CSV uploader into your application. Streamlin provides a convenient function called st.fileuploader for adding file uploads in your application. So it's as simple as adding the following. So after the button, we are going to add the following. And then we need to specify the type of file that we want the user to upload. So in this case, it's going to be a CSV file. Sorry, this isn't equal. Right, let's rerun. Okay, so now you can see that that appears there. Let's try it out. Okay, so this is the data set that I'm going to use is a Twitter stock market data. I'm going to upload this and this appears here. Great. But we will like this to appear after we click the let's get started button. So we're just going to move this and let's rerun. So now we click this, how there's appear here. And then we have the file uploader. We click, we select our file, but boom, it disappeared. Okay, so it turns out that patterns are an unstable and that means that buttons return only momentarily during the page load immediately after their click. And then they refer to false. So in order to work around this is streaming allowed you to use session state, which is essential for maintaining information and interactions between different sections of your application. So I'm going to show you how to implement this with the button that we have in our application. So first of all, what we need to do is to initialize the key in the session state. So following the notation that we've used for the statement, I'm going to create this function called collect. And finally, what we need to do is to modify your button. And then under this if statement, then we can add the CSB uploader. So let's rerun, see what happens. Okay, so now we click, disappears, browse file, choose our file, we upload and nothing disappears. So we have fixed this problem using session state. Okay, so now that the users can upload their CSB files, it's time to convert the uploaded file into a pandas data frame, which is the standard data structure for data manipulation and analysis in Python. So for that, what we're going to do is after the user uploads the file, what we're going to say is if the user CSB file actually exists, we're going to transform that into a data frame. Also, what we're going to do is to say the low memory to false, just because the default is to actually optimize memory, but just in case the file is really large, we're just going to set that to false. Also, what we're going to do is to ensure that the file pointer is at the start of the file just in case. So let's write all that down. So that ensures that the file pointer is at the start of the file. And then we're going to transform this into a data frame. Okay, and now the data frame is now ready for analysis and exploration. Our AI assistant relies on large language models to provide natural language understanding and generate responses. In this section, what we're going to cover is how to load and initialize the LN model for your string like application. So first of all, what we're going to do is we're going to create an instance of the LN model and also we're going to set the temperature parameter to zero. What does that mean? Well, the temperature, what it does in controls the randomness of the model. So the higher the temperature, the more creative your model is going to be. So for this particular project, we're going to let the temperature be low in order to make the responses a little bit more deterministic. So let's write that down. So we create an instance of the model. We're going to call it LLM. And then we're going to set the temperature to zero. So first thing we're going to use our model for is to generate some information and we're going to add that to the sidebar. So I think it will be interesting if we could add some information about what is the steps of the EDR and we add that into the sidebar. And if the user wants to look at it because it finds it useful, just need to click and the information will expand. So let's do that. So I'm going to move our LLM model first here. And then what we're going to do is in the expander, what are the steps of EDA? And what is going to right now is the steps of the EDA, but we're not going to manually type it down. The LLM is going to give us this information. So in order to do that, what we're going to do is we're going to say, OK, LLM. And then we're going to ask the same, what are the steps of EDA? So let's rerun. OK, so very important. We need to add the open AI key. So let's do that very quickly. So we're going to add it here after we have imported the required packages. And also something that we need to do is to load the dot environment just to make sure that the variables are correctly read. Let's do that. OK, so what we're going to do, and you'll see why later, is we're going to move this at the very end. And we're going to add another with SD dot site bar. Oh, and I saw another error. So that should be like that. Right, let's rerun. We're going to browse the file, upload. It's running. And let's check the sidebar. So we have the expander here. And if we click, we can see that the LLM has generated the steps of the EDA. In this section, what we're going to do is we're going to cover how to create a pandas agent and enable it to analyze and provide insights about the data. So first of all, what we need to do is to create an instance of the pandas agent bypassing our LLM model and the data frame that we want to analyze. So in order to do that, what we're going to do is. So here we have pandas agent. And then we use the create pandas data frame agent, and we're going to pass the LLM. And we are also going to pass the data frame, and we're going to set the reverse to true. So the default is false, and that's just to see the train of thought of the agent. So first thing we're going to use our pandas agent for is to answer specific questions about the data. So for example, let's create a question that is, what is the meaning of the colons? What we're going to do is to create a variable that is going to be called colons meaning, for example, that is going to be the response over pandas agent. So for that, we need to say, okay, pandas agent dot run, and then we're going to pass the question. Okay, so now we want to see the output in our application. So we write ST dot write, and we're going to write their colons meaning. Let's run. Cool. So the columns represent the date, opening price, highest price, lowest price, closing price, adjust, and adjust closing price and volume of the stock. So our pandas agent is able to answer this predefined question that we have passed through it. So what we're going to do now is to create a function with certain predefined questions. So when we run these functions, all these questions will be answered by the pandas agent. So the type of questions that we're going to ask are very general EDA questions. For example, how do the rows of the data set look like? As I said, what are the meaning of the columns? How many missing values do we have? Are any duplicate values a little bit about data summarization and even a little bit of feature engineering? We're going to ask the pandas agent if there are any new features that it would be interesting to create. So let's write down this function. So we're going to leave the pandas agent outside, and then I'll just remove this for now. And we'll just create a function that's going to be function agent. So we're going to use the pandas agent to generate all the answers of these questions. Okay, so having written a function down, let's call the function and then let's run the app and let's see what happens. Okay, we rerun. Okay, so this has appeared here. So we can see how the first rows of our data set look like. Then we have the meaning of the columns. We have the missing values saying that there are no duplicates. Information about the data summarization, a correlation between variables and also what are the features that we could maybe create. And also it's talking about potential layers. Okay, now imagine that the user is interested in a very specific variable. And we want the user to select the variable for further study. Well, we can do this using text input. This is a feature that Streamlick has in where the user can actually write text in the application. Okay, so user question is going to be a text input. And we're going to ask what variable are you interested in? We're going to run and see what happens. And as you can see, this is running again and you would agree with me that it would be better if the information stays and it doesn't run again. Okay, so now what variable are you interested in? Well, we haven't done anything with this, but let's say volume, enter, and again, everything runs again. So how do we fix this? So every single time we are entering something, it doesn't run again. So we can sort this issue using caching in Streamlick. So caching is an important feature that allows you to store and reuse the results of computationally expensive functions, which improves the performance and responsiveness of our AI assistant. So we're going to use the cache decorator to make sure that the functions are not run and run again. And we avoid the app to run everything again. So one thing appears after another. So let me show you how to do this. And we're going to create a couple more functions and organize our script a little bit more. So first of all, what we're going to do is we are going to move our function agent and our pandas agent earlier in the script. So there after the element model, so what we're going to do is to add functions of the main script. So here we're going to add this function agent and then I'm going to move the pandas agent early on as well. So we are importing the libraries. We are setting the open AI key. And then what we're going to do is we're going to put there after the open AI key, we are going to set the title and welcoming message. And so let's write here title and then welcoming message. Okay. And after we've done that, we are going to put here the explanation site bar. So let's grab that. So this is this part here. So we're just going to move that just necessary so things work fine and what we cash the functions. So yes, as I said, here is where the explanation site bar is going to be. And then we are going to move the button after this. So this is all about the button that we really don't want. So we don't want the header and this header there. We will use it later for the sections that we are created. So we are going to move all this. Yes, after the sidebar. So yes, we have the explanation sidebar and then we are initializing the key in the session state. Adding the button, we are uploading the file. And when we upload the file and we convert it into a data frame, then is when the game starts, right? Then is when everything is built and generated. So that means that we are actually going to move this in to this F statement. And then or pandas agent is going to go here and the same for the functions. And then we'll move this as well. So we have the pandas agent there. This is something that we already have so we can remove this. And first function that I'm going to create is after initializing the LLM model, we are going to create a function for creating the steps of the EDA. So that is going to be steps A and then here is just going to be this bit, right? So the LLM is going to generate that and that is not going to run again because we're going to decorate that with the cache data decorator. So let me show you how to do that. So here we are going to add this and this is going to be called steps EDA. And then that is what we are going to return. Excellent. So this will be computed and that's it and will be computed again. Amazing. Then we have the pandas agent and then the function that is involving the pandas agent. So let's put here function sidebar and then this is a function main. Excellent. So all this is running and then it's time to add a header. So first thing that we're doing is exploratory data analysis, right? So that will be a really good header. And then we're going to move this sidebar that involves the LLMs there. But now we're just going to write steps EDA. So what this function returns. Great. And then we are running the function agent. And then we have the user question. And let's add a subheader here. So let's say general information about the dataset. And then here, let's say variable of study. Okay. And finally, we're going to hash the function agent. So let's do that. Cool. So let's rerun and see what happens. Okay. So running function agent, this header and the subheader, the function agent is running. Okay. So now hopefully when we write volume here and we press enter, nothing will runs again. Brilliant. So we have sort out this issue of things running again and again by using caching in streamlight. Okay. So coming back to this variable of study, what we're going to do is to create a function that does some exploratory data analysis, but is specific to this variable. And I think the first thing that will be interesting to show is a graph of this variable. Streamlight has a really good visualization properties. So we're going to make use of that to create a line chart. So we are going to add a function there. So first we hash the function and then we're going to call this a function. Okay. So first of all, we're going to use the line chart from streamlight. And that is ST dot line chart. What we're going to pass is our data frame and then the user question variables. So did I call it a user question? Okay. So user question. And variable. We're going to call it. So if we actually let's do this, if we actually run this, let's see what happens. Okay. I forgot that. Okay. Excellent. So you can see now I line chart of the variable volume. So now I'm going to use the pandas agent in the same way I did before to ask a specific questions about this variable. But now instead of just writing questions, I write questions and I'll pass the user question variable. So we can answer specific questions about this specific variable. So for example, imagine that what we want is to know the summary of the statistics, right, of the user question variables. So we're going to ask a summary, summary statistics. So, so for example, pandas agent dot run, and it's going to be gave me a summary of the statistics off. And then we have the user question variable. And then we're going to use ST dot write to write this summary statistics. And we're going to add older things, like for example, checking for normality, checking for liars, checking for trends and add all this to our function. Okay. So let's rerun and let's see what happens. Okay. So this has changed. So it's running again. And great is given a summary of statistics of this variable is saying that the variable is not normal, but is skew to the right. And there is an outlier present. Great. And there is a cyclic pattern of increasing volume in the summer months. And there are five missing values of volume in order to avoid issues in case that user enters an empty string, and all there is no response. And then we continue writing things because that would definitely create a problem. What we're going to do is to write a couple of lines of code to sort that issue. So we're going to say that the user question variable is not non and also if it's different that an empty string, then in that case is when we're going to run or function. And then we will continue right in a or code after that. So it's not non. All right. And then for the next section, we're going to add a header that is going to be further study. So imagine now that the user wants to add specific questions that are not pretty fine in our app. So what we can do now is to create another text input in where the user can enter whatever question they want. And we're going to use this pandas agent to answer this specific question. So so first of all, if the question variable exists, then we're going to say user question data frame. So we're going to say here, is there anything else that you would like to know about the data frame? Okay, so that was our user question data frame. And now again, if I'm just going to copy and paste this. So if the question is not known, and also if it's not empty, and also actually not in empty string, or if they say, okay, I actually don't want to know anything else, I'm very happy with the information I've had so far. Imagine that that happens. Then what we're going to do is we're going to run a function that we're going to create in a second. So this function is going to be called a function question and data frame. And in this function, what we're going to do is to ask the pandas agent to answer this specific question. So again, and then we're going to create this function called function question. Okay, so what is going to happen here is this extra information is going to be stored in this variable. And we're going to use the pandas agent.run to answer specifically this user question data frame. And then we're going to add a return and I'm going to add a st.write in order to actually write the answer to this question. Cool. And then we have called this here. If the user is actually in the user question data frame is actually no and or no, then we're just going to add an empty string. Let's rerun. Okay, again, the colon. Let's rerun. Okay, so is there any strong core relation between some of the variables? Let's see what happens. Okay, so there is a stronger relation between open, high, low, close and add close but not between volume in the other variables. Does the variable close have many peaks? And let's see what it says. Yes, the variable close has a peak of 77 point something. Cool. Amazing. Okay, so that is all for this first video of the series, but what is next? You can continue your data exploration by asking more questions, selecting different variables and seeking additional insights from our AI assistant. In the next video of this series, we will continue building our assistant. It will be able to help you in converting your business challenge into a data science framework, over in guidance on model selection, providing predictions and more. We will introduce the concepts of chains and tools and we will be exploring other agents. Be sure to download the completed project and sample data on the Digilab Academy website, where you can also find more resources and courses on data science and AI. You can also find the written tutorial linked in the description. Don't forget to like and subscribe to be updated with the latest tutorials and upcoming content. Thank you for joining me in this journey and I can't wait to see you in the next part. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.6000000000000005, "text": " Imagine accelerating your machine learning projects with an AI assistant that will save you hours and hours of work.", "tokens": [50364, 11739, 34391, 428, 3479, 2539, 4455, 365, 364, 7318, 10994, 300, 486, 3155, 291, 2496, 293, 2496, 295, 589, 13, 50644], "temperature": 0.0, "avg_logprob": -0.21277552579356507, "compression_ratio": 1.7118055555555556, "no_speech_prob": 0.00042378250509500504}, {"id": 1, "seek": 0, "start": 9.700000000000001, "end": 11.200000000000001, "text": " Welcome to Digilab Academy.", "tokens": [50849, 4027, 281, 10976, 388, 455, 11735, 13, 50924], "temperature": 0.0, "avg_logprob": -0.21277552579356507, "compression_ratio": 1.7118055555555556, "no_speech_prob": 0.00042378250509500504}, {"id": 2, "seek": 0, "start": 11.200000000000001, "end": 16.9, "text": " You go to destination for in-depth courses and resources to help you master the world of data science and AI.", "tokens": [50924, 509, 352, 281, 12236, 337, 294, 12, 25478, 7712, 293, 3593, 281, 854, 291, 4505, 264, 1002, 295, 1412, 3497, 293, 7318, 13, 51209], "temperature": 0.0, "avg_logprob": -0.21277552579356507, "compression_ratio": 1.7118055555555556, "no_speech_prob": 0.00042378250509500504}, {"id": 3, "seek": 0, "start": 16.9, "end": 23.1, "text": " I'm Anna and in this series I'm going to show you how to build your own AI assistant using StreamLid and LangChain.", "tokens": [51209, 286, 478, 12899, 293, 294, 341, 2638, 286, 478, 516, 281, 855, 291, 577, 281, 1322, 428, 1065, 7318, 10994, 1228, 24904, 43, 327, 293, 13313, 6546, 491, 13, 51519], "temperature": 0.0, "avg_logprob": -0.21277552579356507, "compression_ratio": 1.7118055555555556, "no_speech_prob": 0.00042378250509500504}, {"id": 4, "seek": 0, "start": 23.1, "end": 29.0, "text": " I'll walk you through the entire process from selling the required libraries to solve a machine learning project using AI.", "tokens": [51519, 286, 603, 1792, 291, 807, 264, 2302, 1399, 490, 6511, 264, 4739, 15148, 281, 5039, 257, 3479, 2539, 1716, 1228, 7318, 13, 51814], "temperature": 0.0, "avg_logprob": -0.21277552579356507, "compression_ratio": 1.7118055555555556, "no_speech_prob": 0.00042378250509500504}, {"id": 5, "seek": 2900, "start": 29.0, "end": 33.7, "text": " Make sure that you take the full reading tutorial that accompanies this video in the Digilab Academy website.", "tokens": [50364, 4387, 988, 300, 291, 747, 264, 1577, 3760, 7073, 300, 19307, 530, 341, 960, 294, 264, 10976, 388, 455, 11735, 3144, 13, 50599], "temperature": 0.0, "avg_logprob": -0.09904437594943577, "compression_ratio": 1.7236467236467237, "no_speech_prob": 0.006795240566134453}, {"id": 6, "seek": 2900, "start": 33.7, "end": 39.8, "text": " You'll find the link in the video description below, plus you can download the final project and also the dataset that we will be using.", "tokens": [50599, 509, 603, 915, 264, 2113, 294, 264, 960, 3855, 2507, 11, 1804, 291, 393, 5484, 264, 2572, 1716, 293, 611, 264, 28872, 300, 321, 486, 312, 1228, 13, 50904], "temperature": 0.0, "avg_logprob": -0.09904437594943577, "compression_ratio": 1.7236467236467237, "no_speech_prob": 0.006795240566134453}, {"id": 7, "seek": 2900, "start": 39.8, "end": 42.5, "text": " Now let's break down what to expect in this video.", "tokens": [50904, 823, 718, 311, 1821, 760, 437, 281, 2066, 294, 341, 960, 13, 51039], "temperature": 0.0, "avg_logprob": -0.09904437594943577, "compression_ratio": 1.7236467236467237, "no_speech_prob": 0.006795240566134453}, {"id": 8, "seek": 2900, "start": 42.5, "end": 50.0, "text": " In this project, StreamLid is the foundation for the user interface, allowing you to upload CSV files, visualize data and interact with the AI assistant.", "tokens": [51039, 682, 341, 1716, 11, 24904, 43, 327, 307, 264, 7030, 337, 264, 4195, 9226, 11, 8293, 291, 281, 6580, 48814, 7098, 11, 23273, 1412, 293, 4648, 365, 264, 7318, 10994, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09904437594943577, "compression_ratio": 1.7236467236467237, "no_speech_prob": 0.006795240566134453}, {"id": 9, "seek": 2900, "start": 50.0, "end": 51.8, "text": " The beauty of StreamLid is its simplicity.", "tokens": [51414, 440, 6643, 295, 24904, 43, 327, 307, 1080, 25632, 13, 51504], "temperature": 0.0, "avg_logprob": -0.09904437594943577, "compression_ratio": 1.7236467236467237, "no_speech_prob": 0.006795240566134453}, {"id": 10, "seek": 2900, "start": 51.8, "end": 57.2, "text": " Even if you have limited web development experience, you can create dynamic and interactive data applications.", "tokens": [51504, 2754, 498, 291, 362, 5567, 3670, 3250, 1752, 11, 291, 393, 1884, 8546, 293, 15141, 1412, 5821, 13, 51774], "temperature": 0.0, "avg_logprob": -0.09904437594943577, "compression_ratio": 1.7236467236467237, "no_speech_prob": 0.006795240566134453}, {"id": 11, "seek": 5720, "start": 57.2, "end": 63.400000000000006, "text": " While StreamLid is beginner-friendly, it also offers customization options for those who want to create more sophisticated applications.", "tokens": [50364, 3987, 24904, 43, 327, 307, 22080, 12, 22864, 11, 309, 611, 7736, 39387, 3956, 337, 729, 567, 528, 281, 1884, 544, 16950, 5821, 13, 50674], "temperature": 0.0, "avg_logprob": -0.06425201996513034, "compression_ratio": 1.6439628482972137, "no_speech_prob": 0.0005274673458188772}, {"id": 12, "seek": 5720, "start": 63.400000000000006, "end": 71.5, "text": " Finally, StreamLid makes it easy to deploy your web apps to the cloud or share them with others, further enhancing its utility for collaborative data projects.", "tokens": [50674, 6288, 11, 24904, 43, 327, 1669, 309, 1858, 281, 7274, 428, 3670, 7733, 281, 264, 4588, 420, 2073, 552, 365, 2357, 11, 3052, 36579, 1080, 14877, 337, 16555, 1412, 4455, 13, 51079], "temperature": 0.0, "avg_logprob": -0.06425201996513034, "compression_ratio": 1.6439628482972137, "no_speech_prob": 0.0005274673458188772}, {"id": 13, "seek": 5720, "start": 71.5, "end": 76.9, "text": " We'll start by structuring their StreamLid app with titles, headings, subheadings, captions and text formatting.", "tokens": [51079, 492, 603, 722, 538, 6594, 1345, 641, 24904, 43, 327, 724, 365, 12992, 11, 1378, 1109, 11, 1422, 1934, 1109, 11, 44832, 293, 2487, 39366, 13, 51349], "temperature": 0.0, "avg_logprob": -0.06425201996513034, "compression_ratio": 1.6439628482972137, "no_speech_prob": 0.0005274673458188772}, {"id": 14, "seek": 5720, "start": 76.9, "end": 83.0, "text": " Next, we'll implement dividers to segment your content, making it easier for users to navigate through your AI assistant.", "tokens": [51349, 3087, 11, 321, 603, 4445, 4996, 433, 281, 9469, 428, 2701, 11, 1455, 309, 3571, 337, 5022, 281, 12350, 807, 428, 7318, 10994, 13, 51654], "temperature": 0.0, "avg_logprob": -0.06425201996513034, "compression_ratio": 1.6439628482972137, "no_speech_prob": 0.0005274673458188772}, {"id": 15, "seek": 8300, "start": 83.0, "end": 86.4, "text": " Uploading CSV files is essential for this data-driven app.", "tokens": [50364, 624, 21132, 8166, 48814, 7098, 307, 7115, 337, 341, 1412, 12, 25456, 724, 13, 50534], "temperature": 0.0, "avg_logprob": -0.08289491562616258, "compression_ratio": 1.6932153392330382, "no_speech_prob": 0.0008969209156930447}, {"id": 16, "seek": 8300, "start": 86.4, "end": 88.1, "text": " We'll cover how to implement this.", "tokens": [50534, 492, 603, 2060, 577, 281, 4445, 341, 13, 50619], "temperature": 0.0, "avg_logprob": -0.08289491562616258, "compression_ratio": 1.6932153392330382, "no_speech_prob": 0.0008969209156930447}, {"id": 17, "seek": 8300, "start": 88.1, "end": 95.5, "text": " To enhance user interaction, we'll create a dynamic sidebar and expanders, and we will discuss how to display graphs for data visualization.", "tokens": [50619, 1407, 11985, 4195, 9285, 11, 321, 603, 1884, 257, 8546, 1252, 5356, 293, 5268, 433, 11, 293, 321, 486, 2248, 577, 281, 4674, 24877, 337, 1412, 25801, 13, 50989], "temperature": 0.0, "avg_logprob": -0.08289491562616258, "compression_ratio": 1.6932153392330382, "no_speech_prob": 0.0008969209156930447}, {"id": 18, "seek": 8300, "start": 95.5, "end": 100.5, "text": " We'll also explore text input, which allow users to interact with your AI assistant through text.", "tokens": [50989, 492, 603, 611, 6839, 2487, 4846, 11, 597, 2089, 5022, 281, 4648, 365, 428, 7318, 10994, 807, 2487, 13, 51239], "temperature": 0.0, "avg_logprob": -0.08289491562616258, "compression_ratio": 1.6932153392330382, "no_speech_prob": 0.0008969209156930447}, {"id": 19, "seek": 8300, "start": 100.5, "end": 103.1, "text": " Caching is a powerful technique for optimizing performance.", "tokens": [51239, 383, 2834, 307, 257, 4005, 6532, 337, 40425, 3389, 13, 51369], "temperature": 0.0, "avg_logprob": -0.08289491562616258, "compression_ratio": 1.6932153392330382, "no_speech_prob": 0.0008969209156930447}, {"id": 20, "seek": 8300, "start": 103.1, "end": 106.0, "text": " We'll explore this topic and show how you implement it effectively.", "tokens": [51369, 492, 603, 6839, 341, 4829, 293, 855, 577, 291, 4445, 309, 8659, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08289491562616258, "compression_ratio": 1.6932153392330382, "no_speech_prob": 0.0008969209156930447}, {"id": 21, "seek": 8300, "start": 106.0, "end": 111.5, "text": " Understanding session state and implementing non-statefold buttons are a key to creating responsive applications.", "tokens": [51514, 36858, 5481, 1785, 293, 18114, 2107, 12, 15406, 18353, 9905, 366, 257, 2141, 281, 4084, 21826, 5821, 13, 51789], "temperature": 0.0, "avg_logprob": -0.08289491562616258, "compression_ratio": 1.6932153392330382, "no_speech_prob": 0.0008969209156930447}, {"id": 22, "seek": 11150, "start": 111.5, "end": 112.9, "text": " We'll cover this in detail.", "tokens": [50364, 492, 603, 2060, 341, 294, 2607, 13, 50434], "temperature": 0.0, "avg_logprob": -0.0950464048693257, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.010486920364201069}, {"id": 23, "seek": 11150, "start": 112.9, "end": 115.0, "text": " Now, let's shift our focus to LangChain.", "tokens": [50434, 823, 11, 718, 311, 5513, 527, 1879, 281, 13313, 6546, 491, 13, 50539], "temperature": 0.0, "avg_logprob": -0.0950464048693257, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.010486920364201069}, {"id": 24, "seek": 11150, "start": 115.0, "end": 121.8, "text": " LangChain is a framework that can be used to build conversational AI systems that can understand and respond to user queries.", "tokens": [50539, 13313, 6546, 491, 307, 257, 8388, 300, 393, 312, 1143, 281, 1322, 2615, 1478, 7318, 3652, 300, 393, 1223, 293, 4196, 281, 4195, 24109, 13, 50879], "temperature": 0.0, "avg_logprob": -0.0950464048693257, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.010486920364201069}, {"id": 25, "seek": 11150, "start": 121.8, "end": 127.7, "text": " Its main components are models, agents, tools, prompt templates, chains, memory and indexes.", "tokens": [50879, 6953, 2135, 6677, 366, 5245, 11, 12554, 11, 3873, 11, 12391, 21165, 11, 12626, 11, 4675, 293, 8186, 279, 13, 51174], "temperature": 0.0, "avg_logprob": -0.0950464048693257, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.010486920364201069}, {"id": 26, "seek": 11150, "start": 127.7, "end": 132.7, "text": " The project integrates OpenAI's GPT-3 3.5 Turbo Large Language Model.", "tokens": [51174, 440, 1716, 3572, 1024, 7238, 48698, 311, 26039, 51, 12, 18, 805, 13, 20, 35848, 33092, 24445, 17105, 13, 51424], "temperature": 0.0, "avg_logprob": -0.0950464048693257, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.010486920364201069}, {"id": 27, "seek": 11150, "start": 132.7, "end": 139.7, "text": " LLMs are integrated into the StreamLid application, allowing you to have a dynamic real-time interactions with the AI assistant.", "tokens": [51424, 441, 43, 26386, 366, 10919, 666, 264, 24904, 43, 327, 3861, 11, 8293, 291, 281, 362, 257, 8546, 957, 12, 3766, 13280, 365, 264, 7318, 10994, 13, 51774], "temperature": 0.0, "avg_logprob": -0.0950464048693257, "compression_ratio": 1.5576923076923077, "no_speech_prob": 0.010486920364201069}, {"id": 28, "seek": 13970, "start": 139.7, "end": 144.1, "text": " You can ask questions, seek explanations and receive immediate responses.", "tokens": [50364, 509, 393, 1029, 1651, 11, 8075, 28708, 293, 4774, 11629, 13019, 13, 50584], "temperature": 0.0, "avg_logprob": -0.1031631967295771, "compression_ratio": 1.7094594594594594, "no_speech_prob": 0.0011335164308547974}, {"id": 29, "seek": 13970, "start": 144.1, "end": 146.29999999999998, "text": " We'll start by setting the OpenAI key.", "tokens": [50584, 492, 603, 722, 538, 3287, 264, 7238, 48698, 2141, 13, 50694], "temperature": 0.0, "avg_logprob": -0.1031631967295771, "compression_ratio": 1.7094594594594594, "no_speech_prob": 0.0011335164308547974}, {"id": 30, "seek": 13970, "start": 146.29999999999998, "end": 155.2, "text": " Then we will explore how to load and use OpenAI large language models to generate information, open up a world of possibilities for your AI assistant.", "tokens": [50694, 1396, 321, 486, 6839, 577, 281, 3677, 293, 764, 7238, 48698, 2416, 2856, 5245, 281, 8460, 1589, 11, 1269, 493, 257, 1002, 295, 12178, 337, 428, 7318, 10994, 13, 51139], "temperature": 0.0, "avg_logprob": -0.1031631967295771, "compression_ratio": 1.7094594594594594, "no_speech_prob": 0.0011335164308547974}, {"id": 31, "seek": 13970, "start": 155.2, "end": 161.5, "text": " With the Pandas Agent, you can answer specific predefined questions about your data frame or any variable of your choice.", "tokens": [51139, 2022, 264, 16995, 296, 27174, 11, 291, 393, 1867, 2685, 659, 37716, 1651, 466, 428, 1412, 3920, 420, 604, 7006, 295, 428, 3922, 13, 51454], "temperature": 0.0, "avg_logprob": -0.1031631967295771, "compression_ratio": 1.7094594594594594, "no_speech_prob": 0.0011335164308547974}, {"id": 32, "seek": 13970, "start": 161.5, "end": 163.0, "text": " I'll show you how to set this up.", "tokens": [51454, 286, 603, 855, 291, 577, 281, 992, 341, 493, 13, 51529], "temperature": 0.0, "avg_logprob": -0.1031631967295771, "compression_ratio": 1.7094594594594594, "no_speech_prob": 0.0011335164308547974}, {"id": 33, "seek": 13970, "start": 163.0, "end": 167.7, "text": " You can also enable your AI assistant to answer specific questions chosen by the user.", "tokens": [51529, 509, 393, 611, 9528, 428, 7318, 10994, 281, 1867, 2685, 1651, 8614, 538, 264, 4195, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1031631967295771, "compression_ratio": 1.7094594594594594, "no_speech_prob": 0.0011335164308547974}, {"id": 34, "seek": 16770, "start": 167.7, "end": 171.6, "text": " What you learn here can be applied to your own data analysis problems.", "tokens": [50364, 708, 291, 1466, 510, 393, 312, 6456, 281, 428, 1065, 1412, 5215, 2740, 13, 50559], "temperature": 0.0, "avg_logprob": -0.11338428386206766, "compression_ratio": 1.589928057553957, "no_speech_prob": 0.004006882663816214}, {"id": 35, "seek": 16770, "start": 171.6, "end": 177.0, "text": " For example, you might want to create an assistant to help you explore sales data or customer reviews.", "tokens": [50559, 1171, 1365, 11, 291, 1062, 528, 281, 1884, 364, 10994, 281, 854, 291, 6839, 5763, 1412, 420, 5474, 10229, 13, 50829], "temperature": 0.0, "avg_logprob": -0.11338428386206766, "compression_ratio": 1.589928057553957, "no_speech_prob": 0.004006882663816214}, {"id": 36, "seek": 16770, "start": 177.0, "end": 182.5, "text": " Why we'll focus on one specific use case, you can easily adapt these techniques to your unique needs.", "tokens": [50829, 1545, 321, 603, 1879, 322, 472, 2685, 764, 1389, 11, 291, 393, 3612, 6231, 613, 7512, 281, 428, 3845, 2203, 13, 51104], "temperature": 0.0, "avg_logprob": -0.11338428386206766, "compression_ratio": 1.589928057553957, "no_speech_prob": 0.004006882663816214}, {"id": 37, "seek": 16770, "start": 182.5, "end": 185.6, "text": " So without further ado, let's start building our AI assistant.", "tokens": [51104, 407, 1553, 3052, 22450, 11, 718, 311, 722, 2390, 527, 7318, 10994, 13, 51259], "temperature": 0.0, "avg_logprob": -0.11338428386206766, "compression_ratio": 1.589928057553957, "no_speech_prob": 0.004006882663816214}, {"id": 38, "seek": 16770, "start": 185.6, "end": 189.5, "text": " Hit that like button and don't forget to subscribe for more data science adventures.", "tokens": [51259, 9217, 300, 411, 2960, 293, 500, 380, 2870, 281, 3022, 337, 544, 1412, 3497, 20905, 13, 51454], "temperature": 0.0, "avg_logprob": -0.11338428386206766, "compression_ratio": 1.589928057553957, "no_speech_prob": 0.004006882663816214}, {"id": 39, "seek": 16770, "start": 189.5, "end": 190.39999999999998, "text": " Let's get started.", "tokens": [51454, 961, 311, 483, 1409, 13, 51499], "temperature": 0.0, "avg_logprob": -0.11338428386206766, "compression_ratio": 1.589928057553957, "no_speech_prob": 0.004006882663816214}, {"id": 40, "seek": 19040, "start": 191.0, "end": 199.8, "text": " Okay, so before we start building our AI assistant, there are two things that we need to do.", "tokens": [50394, 1033, 11, 370, 949, 321, 722, 2390, 527, 7318, 10994, 11, 456, 366, 732, 721, 300, 321, 643, 281, 360, 13, 50834], "temperature": 0.0, "avg_logprob": -0.23727984110514322, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.07136406004428864}, {"id": 41, "seek": 19040, "start": 199.8, "end": 203.3, "text": " First is to set the API key and the second is to run Streamlet.", "tokens": [50834, 2386, 307, 281, 992, 264, 9362, 2141, 293, 264, 1150, 307, 281, 1190, 24904, 2631, 13, 51009], "temperature": 0.0, "avg_logprob": -0.23727984110514322, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.07136406004428864}, {"id": 42, "seek": 19040, "start": 203.3, "end": 213.1, "text": " To set the API key, you need an API key from OpenAI and you need to boot VAL into a different script in your same directory.", "tokens": [51009, 1407, 992, 264, 9362, 2141, 11, 291, 643, 364, 9362, 2141, 490, 7238, 48698, 293, 291, 643, 281, 11450, 691, 3427, 666, 257, 819, 5755, 294, 428, 912, 21120, 13, 51499], "temperature": 0.0, "avg_logprob": -0.23727984110514322, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.07136406004428864}, {"id": 43, "seek": 21310, "start": 213.2, "end": 222.5, "text": " What I've done is I created this API key file and there what we need to do is store our key.", "tokens": [50369, 708, 286, 600, 1096, 307, 286, 2942, 341, 9362, 2141, 3991, 293, 456, 437, 321, 643, 281, 360, 307, 3531, 527, 2141, 13, 50834], "temperature": 0.0, "avg_logprob": -0.10212072435316148, "compression_ratio": 1.6570048309178744, "no_speech_prob": 0.13649645447731018}, {"id": 44, "seek": 21310, "start": 222.5, "end": 228.79999999999998, "text": " So here you would put your key as a string, but in this file called API key.", "tokens": [50834, 407, 510, 291, 576, 829, 428, 2141, 382, 257, 6798, 11, 457, 294, 341, 3991, 1219, 9362, 2141, 13, 51149], "temperature": 0.0, "avg_logprob": -0.10212072435316148, "compression_ratio": 1.6570048309178744, "no_speech_prob": 0.13649645447731018}, {"id": 45, "seek": 21310, "start": 228.79999999999998, "end": 235.5, "text": " The reason that I'm not sharing is because this key is a secret key and it shouldn't be shared.", "tokens": [51149, 440, 1778, 300, 286, 478, 406, 5414, 307, 570, 341, 2141, 307, 257, 4054, 2141, 293, 309, 4659, 380, 312, 5507, 13, 51484], "temperature": 0.0, "avg_logprob": -0.10212072435316148, "compression_ratio": 1.6570048309178744, "no_speech_prob": 0.13649645447731018}, {"id": 46, "seek": 21310, "start": 235.5, "end": 241.4, "text": " So let's remove that and just remember that that goes to your API key script.", "tokens": [51484, 407, 718, 311, 4159, 300, 293, 445, 1604, 300, 300, 1709, 281, 428, 9362, 2141, 5755, 13, 51779], "temperature": 0.0, "avg_logprob": -0.10212072435316148, "compression_ratio": 1.6570048309178744, "no_speech_prob": 0.13649645447731018}, {"id": 47, "seek": 24140, "start": 241.4, "end": 243.20000000000002, "text": " The second is to run Streamlet.", "tokens": [50364, 440, 1150, 307, 281, 1190, 24904, 2631, 13, 50454], "temperature": 0.0, "avg_logprob": -0.15121542889138925, "compression_ratio": 1.8471074380165289, "no_speech_prob": 0.002756427740678191}, {"id": 48, "seek": 24140, "start": 243.20000000000002, "end": 249.4, "text": " For that, what I want you to do is to open the terminal, go to your directory and what I want you to do is to run Streamlet using this command.", "tokens": [50454, 1171, 300, 11, 437, 286, 528, 291, 281, 360, 307, 281, 1269, 264, 14709, 11, 352, 281, 428, 21120, 293, 437, 286, 528, 291, 281, 360, 307, 281, 1190, 24904, 2631, 1228, 341, 5622, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15121542889138925, "compression_ratio": 1.8471074380165289, "no_speech_prob": 0.002756427740678191}, {"id": 49, "seek": 24140, "start": 249.4, "end": 258.2, "text": " So that is Streamlet run and then the name of your API script where you're going to write the code for the AI assistant and just press enter.", "tokens": [50764, 407, 300, 307, 24904, 2631, 1190, 293, 550, 264, 1315, 295, 428, 9362, 5755, 689, 291, 434, 516, 281, 2464, 264, 3089, 337, 264, 7318, 10994, 293, 445, 1886, 3242, 13, 51204], "temperature": 0.0, "avg_logprob": -0.15121542889138925, "compression_ratio": 1.8471074380165289, "no_speech_prob": 0.002756427740678191}, {"id": 50, "seek": 24140, "start": 258.2, "end": 264.9, "text": " And what's going to happen is that a new window is going to pop up in your browser and there's where we're going to see your app.", "tokens": [51204, 400, 437, 311, 516, 281, 1051, 307, 300, 257, 777, 4910, 307, 516, 281, 1665, 493, 294, 428, 11185, 293, 456, 311, 689, 321, 434, 516, 281, 536, 428, 724, 13, 51539], "temperature": 0.0, "avg_logprob": -0.15121542889138925, "compression_ratio": 1.8471074380165289, "no_speech_prob": 0.002756427740678191}, {"id": 51, "seek": 26490, "start": 265.9, "end": 273.9, "text": " Now we're going to import the packages that are going to provide the necessary functionality for our project.", "tokens": [50414, 823, 321, 434, 516, 281, 974, 264, 17401, 300, 366, 516, 281, 2893, 264, 4818, 14980, 337, 527, 1716, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23321608246349898, "compression_ratio": 1.522875816993464, "no_speech_prob": 0.029306737706065178}, {"id": 52, "seek": 26490, "start": 273.9, "end": 282.59999999999997, "text": " The packages that we're going to import are OS, API key, long chain, dot M, Streamlet and pandas.", "tokens": [50814, 440, 17401, 300, 321, 434, 516, 281, 974, 366, 12731, 11, 9362, 2141, 11, 938, 5021, 11, 5893, 376, 11, 24904, 2631, 293, 4565, 296, 13, 51249], "temperature": 0.0, "avg_logprob": -0.23321608246349898, "compression_ratio": 1.522875816993464, "no_speech_prob": 0.029306737706065178}, {"id": 53, "seek": 26490, "start": 282.59999999999997, "end": 285.0, "text": " So let's write that down.", "tokens": [51249, 407, 718, 311, 2464, 300, 760, 13, 51369], "temperature": 0.0, "avg_logprob": -0.23321608246349898, "compression_ratio": 1.522875816993464, "no_speech_prob": 0.029306737706065178}, {"id": 54, "seek": 28500, "start": 285.0, "end": 295.6, "text": " Okay, I'm going to close this player so we can see the code better.", "tokens": [50364, 1033, 11, 286, 478, 516, 281, 1998, 341, 4256, 370, 321, 393, 536, 264, 3089, 1101, 13, 50894], "temperature": 0.0, "avg_logprob": -0.20529362775277402, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.005219541490077972}, {"id": 55, "seek": 28500, "start": 295.6, "end": 299.1, "text": " So as you can see, we have imported the recurrent libraries.", "tokens": [50894, 407, 382, 291, 393, 536, 11, 321, 362, 25524, 264, 18680, 1753, 15148, 13, 51069], "temperature": 0.0, "avg_logprob": -0.20529362775277402, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.005219541490077972}, {"id": 56, "seek": 28500, "start": 299.1, "end": 303.6, "text": " So the OS library provides a way of using operating system dependent functionality.", "tokens": [51069, 407, 264, 12731, 6405, 6417, 257, 636, 295, 1228, 7447, 1185, 12334, 14980, 13, 51294], "temperature": 0.0, "avg_logprob": -0.20529362775277402, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.005219541490077972}, {"id": 57, "seek": 28500, "start": 303.6, "end": 308.1, "text": " Then we have API key in order to load or API key correctly.", "tokens": [51294, 1396, 321, 362, 9362, 2141, 294, 1668, 281, 3677, 420, 9362, 2141, 8944, 13, 51519], "temperature": 0.0, "avg_logprob": -0.20529362775277402, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.005219541490077972}, {"id": 58, "seek": 30810, "start": 308.20000000000005, "end": 316.1, "text": " Then Streamlet that is the heart of our project really then behind this library fundamental for data manipulation and visualization.", "tokens": [50369, 1396, 24904, 2631, 300, 307, 264, 1917, 295, 527, 1716, 534, 550, 2261, 341, 6405, 8088, 337, 1412, 26475, 293, 25801, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2081463035495802, "compression_ratio": 1.6339622641509435, "no_speech_prob": 0.005298839416354895}, {"id": 59, "seek": 30810, "start": 316.1, "end": 325.5, "text": " This we have the package long chain, which is specific to our project and that incorporates open AI language models that is going to allow us to interact with the assistant.", "tokens": [50764, 639, 321, 362, 264, 7372, 938, 5021, 11, 597, 307, 2685, 281, 527, 1716, 293, 300, 50193, 1269, 7318, 2856, 5245, 300, 307, 516, 281, 2089, 505, 281, 4648, 365, 264, 10994, 13, 51234], "temperature": 0.0, "avg_logprob": -0.2081463035495802, "compression_ratio": 1.6339622641509435, "no_speech_prob": 0.005298839416354895}, {"id": 60, "seek": 30810, "start": 325.5, "end": 335.5, "text": " And then we have dot M, very important package to make sure that sensitive information such as the API key is securely stored.", "tokens": [51234, 400, 550, 321, 362, 5893, 376, 11, 588, 1021, 7372, 281, 652, 988, 300, 9477, 1589, 1270, 382, 264, 9362, 2141, 307, 38348, 12187, 13, 51734], "temperature": 0.0, "avg_logprob": -0.2081463035495802, "compression_ratio": 1.6339622641509435, "no_speech_prob": 0.005298839416354895}, {"id": 61, "seek": 33810, "start": 339.1, "end": 351.1, "text": " Okay, so now we're going to start building the user interface a little and I'm going to show you how to add titles, subtitles, headings and other stuff that is going to make the user experience a little bit more engaging.", "tokens": [50414, 1033, 11, 370, 586, 321, 434, 516, 281, 722, 2390, 264, 4195, 9226, 257, 707, 293, 286, 478, 516, 281, 855, 291, 577, 281, 909, 12992, 11, 42045, 11, 1378, 1109, 293, 661, 1507, 300, 307, 516, 281, 652, 264, 4195, 1752, 257, 707, 857, 544, 11268, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1502363270726697, "compression_ratio": 1.7024390243902439, "no_speech_prob": 0.00460758525878191}, {"id": 62, "seek": 33810, "start": 351.1, "end": 357.8, "text": " So first of all, we want our assistant to have a title and also we want to have a welcoming message.", "tokens": [51014, 407, 700, 295, 439, 11, 321, 528, 527, 10994, 281, 362, 257, 4876, 293, 611, 321, 528, 281, 362, 257, 17378, 3636, 13, 51349], "temperature": 0.0, "avg_logprob": -0.1502363270726697, "compression_ratio": 1.7024390243902439, "no_speech_prob": 0.00460758525878191}, {"id": 63, "seek": 33810, "start": 357.8, "end": 359.90000000000003, "text": " Okay, so let's write that.", "tokens": [51349, 1033, 11, 370, 718, 311, 2464, 300, 13, 51454], "temperature": 0.0, "avg_logprob": -0.1502363270726697, "compression_ratio": 1.7024390243902439, "no_speech_prob": 0.00460758525878191}, {"id": 64, "seek": 35990, "start": 359.9, "end": 370.7, "text": " So in extremely in order to write a title, we do sd.title and then inside of the brackets, we write the title that we want to add to our app.", "tokens": [50364, 407, 294, 4664, 294, 1668, 281, 2464, 257, 4876, 11, 321, 360, 262, 67, 13, 27689, 306, 293, 550, 1854, 295, 264, 26179, 11, 321, 2464, 264, 4876, 300, 321, 528, 281, 909, 281, 527, 724, 13, 50904], "temperature": 0.0, "avg_logprob": -0.2574526968966709, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.01064885500818491}, {"id": 65, "seek": 35990, "start": 370.7, "end": 380.7, "text": " So I'm going to call a assistant for data science and I'm actually going to capitalize that and let's add a header now.", "tokens": [50904, 407, 286, 478, 516, 281, 818, 257, 10994, 337, 1412, 3497, 293, 286, 478, 767, 516, 281, 48114, 300, 293, 718, 311, 909, 257, 23117, 586, 13, 51404], "temperature": 0.0, "avg_logprob": -0.2574526968966709, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.01064885500818491}, {"id": 66, "seek": 35990, "start": 380.7, "end": 386.2, "text": " So in order to add headers and striplet is just sd.hether.", "tokens": [51404, 407, 294, 1668, 281, 909, 45101, 293, 12828, 2631, 307, 445, 262, 67, 13, 71, 1666, 13, 51679], "temperature": 0.0, "avg_logprob": -0.2574526968966709, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.01064885500818491}, {"id": 67, "seek": 38620, "start": 386.2, "end": 392.8, "text": " And we're going to add, for example, exploratory data analysis part.", "tokens": [50364, 400, 321, 434, 516, 281, 909, 11, 337, 1365, 11, 24765, 4745, 1412, 5215, 644, 13, 50694], "temperature": 0.0, "avg_logprob": -0.1333329628924934, "compression_ratio": 1.6069651741293531, "no_speech_prob": 0.011329952627420425}, {"id": 68, "seek": 38620, "start": 392.8, "end": 397.3, "text": " If you want to write a subheader is just sd.subheader.", "tokens": [50694, 759, 291, 528, 281, 2464, 257, 1422, 1934, 260, 307, 445, 262, 67, 13, 30131, 1934, 260, 13, 50919], "temperature": 0.0, "avg_logprob": -0.1333329628924934, "compression_ratio": 1.6069651741293531, "no_speech_prob": 0.011329952627420425}, {"id": 69, "seek": 38620, "start": 397.3, "end": 401.2, "text": " So let's say, for example, solution.", "tokens": [50919, 407, 718, 311, 584, 11, 337, 1365, 11, 3827, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1333329628924934, "compression_ratio": 1.6069651741293531, "no_speech_prob": 0.011329952627420425}, {"id": 70, "seek": 38620, "start": 401.2, "end": 409.3, "text": " Okay, as you can see, nothing appears on the app and in order to see what we're reading, then we need to click rerun.", "tokens": [51114, 1033, 11, 382, 291, 393, 536, 11, 1825, 7038, 322, 264, 724, 293, 294, 1668, 281, 536, 437, 321, 434, 3760, 11, 550, 321, 643, 281, 2052, 43819, 409, 13, 51519], "temperature": 0.0, "avg_logprob": -0.1333329628924934, "compression_ratio": 1.6069651741293531, "no_speech_prob": 0.011329952627420425}, {"id": 71, "seek": 38620, "start": 409.3, "end": 411.7, "text": " So let's do that and let's see what happens.", "tokens": [51519, 407, 718, 311, 360, 300, 293, 718, 311, 536, 437, 2314, 13, 51639], "temperature": 0.0, "avg_logprob": -0.1333329628924934, "compression_ratio": 1.6069651741293531, "no_speech_prob": 0.011329952627420425}, {"id": 72, "seek": 41170, "start": 411.7, "end": 413.8, "text": " So this is running.", "tokens": [50364, 407, 341, 307, 2614, 13, 50469], "temperature": 0.0, "avg_logprob": -0.17088780721028646, "compression_ratio": 1.5297619047619047, "no_speech_prob": 0.0006164898513816297}, {"id": 73, "seek": 41170, "start": 413.8, "end": 424.7, "text": " And you can see now the title appears and we have here the header and this header as well.", "tokens": [50469, 400, 291, 393, 536, 586, 264, 4876, 7038, 293, 321, 362, 510, 264, 23117, 293, 341, 23117, 382, 731, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17088780721028646, "compression_ratio": 1.5297619047619047, "no_speech_prob": 0.0006164898513816297}, {"id": 74, "seek": 41170, "start": 424.7, "end": 427.0, "text": " As I said, we want to write a welcoming message.", "tokens": [51014, 1018, 286, 848, 11, 321, 528, 281, 2464, 257, 17378, 3636, 13, 51129], "temperature": 0.0, "avg_logprob": -0.17088780721028646, "compression_ratio": 1.5297619047619047, "no_speech_prob": 0.0006164898513816297}, {"id": 75, "seek": 41170, "start": 427.0, "end": 433.7, "text": " So for that, we're going to just use plain text in order to do that in striplet, we use sd.write.", "tokens": [51129, 407, 337, 300, 11, 321, 434, 516, 281, 445, 764, 11121, 2487, 294, 1668, 281, 360, 300, 294, 12828, 2631, 11, 321, 764, 262, 67, 13, 21561, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17088780721028646, "compression_ratio": 1.5297619047619047, "no_speech_prob": 0.0006164898513816297}, {"id": 76, "seek": 43370, "start": 433.7, "end": 442.0, "text": " So let's write for now just welcoming message and see what happens when we rerun.", "tokens": [50364, 407, 718, 311, 2464, 337, 586, 445, 17378, 3636, 293, 536, 437, 2314, 562, 321, 43819, 409, 13, 50779], "temperature": 0.0, "avg_logprob": -0.12912830553556742, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.013213615864515305}, {"id": 77, "seek": 43370, "start": 442.0, "end": 445.3, "text": " So we rerun and it appears here.", "tokens": [50779, 407, 321, 43819, 409, 293, 309, 7038, 510, 13, 50944], "temperature": 0.0, "avg_logprob": -0.12912830553556742, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.013213615864515305}, {"id": 78, "seek": 43370, "start": 445.3, "end": 454.8, "text": " So I'm going to write something along the lines of, hello, I'm your assistant and I'm here to assist you with your machine learning projects.", "tokens": [50944, 407, 286, 478, 516, 281, 2464, 746, 2051, 264, 3876, 295, 11, 7751, 11, 286, 478, 428, 10994, 293, 286, 478, 510, 281, 4255, 291, 365, 428, 3479, 2539, 4455, 13, 51419], "temperature": 0.0, "avg_logprob": -0.12912830553556742, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.013213615864515305}, {"id": 79, "seek": 43370, "start": 454.8, "end": 457.9, "text": " So let's write something like that.", "tokens": [51419, 407, 718, 311, 2464, 746, 411, 300, 13, 51574], "temperature": 0.0, "avg_logprob": -0.12912830553556742, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.013213615864515305}, {"id": 80, "seek": 45790, "start": 457.9, "end": 467.0, "text": " So, okay, so let's see what happens when we run.", "tokens": [50364, 407, 11, 1392, 11, 370, 718, 311, 536, 437, 2314, 562, 321, 1190, 13, 50819], "temperature": 0.0, "avg_logprob": -0.18008086555882505, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0027558798901736736}, {"id": 81, "seek": 45790, "start": 467.0, "end": 482.7, "text": " Okay, so now our placeholder for the welcoming message has been populated and now we have hello, I'm your assistant and I'm here to help you with your data science projects.", "tokens": [50819, 1033, 11, 370, 586, 527, 1081, 20480, 337, 264, 17378, 3636, 575, 668, 32998, 293, 586, 321, 362, 7751, 11, 286, 478, 428, 10994, 293, 286, 478, 510, 281, 854, 291, 365, 428, 1412, 3497, 4455, 13, 51604], "temperature": 0.0, "avg_logprob": -0.18008086555882505, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0027558798901736736}, {"id": 82, "seek": 48270, "start": 482.7, "end": 488.09999999999997, "text": " I'm going to move the header and this header after the welcoming message to enhance user experience.", "tokens": [50364, 286, 478, 516, 281, 1286, 264, 23117, 293, 341, 23117, 934, 264, 17378, 3636, 281, 11985, 4195, 1752, 13, 50634], "temperature": 0.0, "avg_logprob": -0.11516143336440578, "compression_ratio": 1.853080568720379, "no_speech_prob": 0.08492683619260788}, {"id": 83, "seek": 48270, "start": 488.09999999999997, "end": 496.2, "text": " Something that we can do is add a sidebar on the side of the application and to do that we can use with sd sidebar.", "tokens": [50634, 6595, 300, 321, 393, 360, 307, 909, 257, 1252, 5356, 322, 264, 1252, 295, 264, 3861, 293, 281, 360, 300, 321, 393, 764, 365, 262, 67, 1252, 5356, 13, 51039], "temperature": 0.0, "avg_logprob": -0.11516143336440578, "compression_ratio": 1.853080568720379, "no_speech_prob": 0.08492683619260788}, {"id": 84, "seek": 48270, "start": 496.2, "end": 503.09999999999997, "text": " So I think something interesting that we can put in the sidebar is an explanation of what the user can expect from the app.", "tokens": [51039, 407, 286, 519, 746, 1880, 300, 321, 393, 829, 294, 264, 1252, 5356, 307, 364, 10835, 295, 437, 264, 4195, 393, 2066, 490, 264, 724, 13, 51384], "temperature": 0.0, "avg_logprob": -0.11516143336440578, "compression_ratio": 1.853080568720379, "no_speech_prob": 0.08492683619260788}, {"id": 85, "seek": 48270, "start": 503.09999999999997, "end": 505.8, "text": " For that, what I'm going to do is to write a text.", "tokens": [51384, 1171, 300, 11, 437, 286, 478, 516, 281, 360, 307, 281, 2464, 257, 2487, 13, 51519], "temperature": 0.0, "avg_logprob": -0.11516143336440578, "compression_ratio": 1.853080568720379, "no_speech_prob": 0.08492683619260788}, {"id": 86, "seek": 50580, "start": 505.8, "end": 509.90000000000003, "text": " So using sd.write that will appeal in the sidebar.", "tokens": [50364, 407, 1228, 262, 67, 13, 21561, 300, 486, 13668, 294, 264, 1252, 5356, 13, 50569], "temperature": 0.0, "avg_logprob": -0.2006768435239792, "compression_ratio": 1.423841059602649, "no_speech_prob": 0.06094282492995262}, {"id": 87, "seek": 50580, "start": 509.90000000000003, "end": 513.5, "text": " So it needs to be within the with statement.", "tokens": [50569, 407, 309, 2203, 281, 312, 1951, 264, 365, 5629, 13, 50749], "temperature": 0.0, "avg_logprob": -0.2006768435239792, "compression_ratio": 1.423841059602649, "no_speech_prob": 0.06094282492995262}, {"id": 88, "seek": 50580, "start": 513.5, "end": 520.0, "text": " So let's write that.", "tokens": [50749, 407, 718, 311, 2464, 300, 13, 51074], "temperature": 0.0, "avg_logprob": -0.2006768435239792, "compression_ratio": 1.423841059602649, "no_speech_prob": 0.06094282492995262}, {"id": 89, "seek": 50580, "start": 520.0, "end": 524.1, "text": " So your data science adventure begins with the CSV file.", "tokens": [51074, 407, 428, 1412, 3497, 9868, 7338, 365, 264, 48814, 3991, 13, 51279], "temperature": 0.0, "avg_logprob": -0.2006768435239792, "compression_ratio": 1.423841059602649, "no_speech_prob": 0.06094282492995262}, {"id": 90, "seek": 50580, "start": 524.1, "end": 531.7, "text": " And now I'm going to add some extra text.", "tokens": [51279, 400, 586, 286, 478, 516, 281, 909, 512, 2857, 2487, 13, 51659], "temperature": 0.0, "avg_logprob": -0.2006768435239792, "compression_ratio": 1.423841059602649, "no_speech_prob": 0.06094282492995262}, {"id": 91, "seek": 53170, "start": 531.7, "end": 537.4000000000001, "text": " Okay, and I'm going to add that to make sure that this is actually a string.", "tokens": [50364, 1033, 11, 293, 286, 478, 516, 281, 909, 300, 281, 652, 988, 300, 341, 307, 767, 257, 6798, 13, 50649], "temperature": 0.0, "avg_logprob": -0.12055959954725957, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.020638924092054367}, {"id": 92, "seek": 53170, "start": 537.4000000000001, "end": 540.2, "text": " Okay, let's rerun.", "tokens": [50649, 1033, 11, 718, 311, 43819, 409, 13, 50789], "temperature": 0.0, "avg_logprob": -0.12055959954725957, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.020638924092054367}, {"id": 93, "seek": 53170, "start": 540.2, "end": 542.7, "text": " Okay, so as you can see, this has appeared here.", "tokens": [50789, 1033, 11, 370, 382, 291, 393, 536, 11, 341, 575, 8516, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12055959954725957, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.020638924092054367}, {"id": 94, "seek": 53170, "start": 542.7, "end": 553.0, "text": " Let's click and this is our sidebar with our text in the sidebar giving a little bit of explanation of what the user can expect from the app.", "tokens": [50914, 961, 311, 2052, 293, 341, 307, 527, 1252, 5356, 365, 527, 2487, 294, 264, 1252, 5356, 2902, 257, 707, 857, 295, 10835, 295, 437, 264, 4195, 393, 2066, 490, 264, 724, 13, 51429], "temperature": 0.0, "avg_logprob": -0.12055959954725957, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.020638924092054367}, {"id": 95, "seek": 53170, "start": 553.0, "end": 556.8000000000001, "text": " We have your data science adventure begins with the CSV file.", "tokens": [51429, 492, 362, 428, 1412, 3497, 9868, 7338, 365, 264, 48814, 3991, 13, 51619], "temperature": 0.0, "avg_logprob": -0.12055959954725957, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.020638924092054367}, {"id": 96, "seek": 53170, "start": 556.8000000000001, "end": 561.3000000000001, "text": " You may already know that every exciting data science journey starts with a data set.", "tokens": [51619, 509, 815, 1217, 458, 300, 633, 4670, 1412, 3497, 4671, 3719, 365, 257, 1412, 992, 13, 51844], "temperature": 0.0, "avg_logprob": -0.12055959954725957, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.020638924092054367}, {"id": 97, "seek": 56130, "start": 561.3, "end": 563.6999999999999, "text": " That's why I would love for you to upload a CSV file.", "tokens": [50364, 663, 311, 983, 286, 576, 959, 337, 291, 281, 6580, 257, 48814, 3991, 13, 50484], "temperature": 0.0, "avg_logprob": -0.09493005484865423, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.0014778024051338434}, {"id": 98, "seek": 56130, "start": 563.6999999999999, "end": 569.0999999999999, "text": " Once we have your data in hand, we'll dive into understanding it and have some fun exploring it.", "tokens": [50484, 3443, 321, 362, 428, 1412, 294, 1011, 11, 321, 603, 9192, 666, 3701, 309, 293, 362, 512, 1019, 12736, 309, 13, 50754], "temperature": 0.0, "avg_logprob": -0.09493005484865423, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.0014778024051338434}, {"id": 99, "seek": 56130, "start": 569.0999999999999, "end": 573.1999999999999, "text": " Then we'll work together to shape your business challenge into a data science framework.", "tokens": [50754, 1396, 321, 603, 589, 1214, 281, 3909, 428, 1606, 3430, 666, 257, 1412, 3497, 8388, 13, 50959], "temperature": 0.0, "avg_logprob": -0.09493005484865423, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.0014778024051338434}, {"id": 100, "seek": 56130, "start": 573.1999999999999, "end": 577.9, "text": " And I'll introduce you to the coolest machine learning modules and we'll use them to tackle your problem.", "tokens": [50959, 400, 286, 603, 5366, 291, 281, 264, 22013, 3479, 2539, 16679, 293, 321, 603, 764, 552, 281, 14896, 428, 1154, 13, 51194], "temperature": 0.0, "avg_logprob": -0.09493005484865423, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.0014778024051338434}, {"id": 101, "seek": 56130, "start": 577.9, "end": 584.3, "text": " Sounds fun, right?", "tokens": [51194, 14576, 1019, 11, 558, 30, 51514], "temperature": 0.0, "avg_logprob": -0.09493005484865423, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.0014778024051338434}, {"id": 102, "seek": 56130, "start": 584.3, "end": 589.8, "text": " This looks alright like this, but we can make it a little bit better and a little bit more visually appealing.", "tokens": [51514, 639, 1542, 5845, 411, 341, 11, 457, 321, 393, 652, 309, 257, 707, 857, 1101, 293, 257, 707, 857, 544, 19622, 23842, 13, 51789], "temperature": 0.0, "avg_logprob": -0.09493005484865423, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.0014778024051338434}, {"id": 103, "seek": 58980, "start": 589.8, "end": 598.0999999999999, "text": " So what we're going to do is use some text formatting to make this look a little bit more organized.", "tokens": [50364, 407, 437, 321, 434, 516, 281, 360, 307, 764, 512, 2487, 39366, 281, 652, 341, 574, 257, 707, 857, 544, 9983, 13, 50779], "temperature": 0.0, "avg_logprob": -0.10331440997380083, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.02550005540251732}, {"id": 104, "seek": 58980, "start": 598.0999999999999, "end": 606.9, "text": " So first of all, let's put the first sentence into a title and then the rest is going to be the text.", "tokens": [50779, 407, 700, 295, 439, 11, 718, 311, 829, 264, 700, 8174, 666, 257, 4876, 293, 550, 264, 1472, 307, 516, 281, 312, 264, 2487, 13, 51219], "temperature": 0.0, "avg_logprob": -0.10331440997380083, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.02550005540251732}, {"id": 105, "seek": 58980, "start": 606.9, "end": 612.5999999999999, "text": " But instead of using titles and write, we're going to use write and caption.", "tokens": [51219, 583, 2602, 295, 1228, 12992, 293, 2464, 11, 321, 434, 516, 281, 764, 2464, 293, 31974, 13, 51504], "temperature": 0.0, "avg_logprob": -0.10331440997380083, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.02550005540251732}, {"id": 106, "seek": 58980, "start": 612.5999999999999, "end": 617.8, "text": " So caption is used more footnotes to add the text to images and things like that.", "tokens": [51504, 407, 31974, 307, 1143, 544, 2671, 2247, 279, 281, 909, 264, 2487, 281, 5267, 293, 721, 411, 300, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10331440997380083, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.02550005540251732}, {"id": 107, "seek": 61780, "start": 617.8, "end": 625.6999999999999, "text": " But just because I like the formatting of this particular way of writing text, I'm just going to use it for this.", "tokens": [50364, 583, 445, 570, 286, 411, 264, 39366, 295, 341, 1729, 636, 295, 3579, 2487, 11, 286, 478, 445, 516, 281, 764, 309, 337, 341, 13, 50759], "temperature": 0.0, "avg_logprob": -0.122802734375, "compression_ratio": 1.7283236994219653, "no_speech_prob": 0.0203233789652586}, {"id": 108, "seek": 61780, "start": 625.6999999999999, "end": 627.3, "text": " So let's do that.", "tokens": [50759, 407, 718, 311, 360, 300, 13, 50839], "temperature": 0.0, "avg_logprob": -0.122802734375, "compression_ratio": 1.7283236994219653, "no_speech_prob": 0.0203233789652586}, {"id": 109, "seek": 61780, "start": 627.3, "end": 630.9, "text": " I'm going to leave that in write.", "tokens": [50839, 286, 478, 516, 281, 1856, 300, 294, 2464, 13, 51019], "temperature": 0.0, "avg_logprob": -0.122802734375, "compression_ratio": 1.7283236994219653, "no_speech_prob": 0.0203233789652586}, {"id": 110, "seek": 61780, "start": 630.9, "end": 642.8, "text": " And then what I'm going to do is I'm going to write st.caption and then add this in brackets.", "tokens": [51019, 400, 550, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 2464, 342, 13, 496, 1695, 293, 550, 909, 341, 294, 26179, 13, 51614], "temperature": 0.0, "avg_logprob": -0.122802734375, "compression_ratio": 1.7283236994219653, "no_speech_prob": 0.0203233789652586}, {"id": 111, "seek": 61780, "start": 642.8, "end": 647.0999999999999, "text": " So let's run and see what is happening.", "tokens": [51614, 407, 718, 311, 1190, 293, 536, 437, 307, 2737, 13, 51829], "temperature": 0.0, "avg_logprob": -0.122802734375, "compression_ratio": 1.7283236994219653, "no_speech_prob": 0.0203233789652586}, {"id": 112, "seek": 64710, "start": 647.1, "end": 653.1, "text": " Brilliant. So now we have a small title at the beginning and then text.", "tokens": [50364, 34007, 13, 407, 586, 321, 362, 257, 1359, 4876, 412, 264, 2863, 293, 550, 2487, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13773228465646936, "compression_ratio": 1.4701986754966887, "no_speech_prob": 0.0016995267942547798}, {"id": 113, "seek": 64710, "start": 653.1, "end": 658.5, "text": " I'm going to write this here.", "tokens": [50664, 286, 478, 516, 281, 2464, 341, 510, 13, 50934], "temperature": 0.0, "avg_logprob": -0.13773228465646936, "compression_ratio": 1.4701986754966887, "no_speech_prob": 0.0016995267942547798}, {"id": 114, "seek": 64710, "start": 658.5, "end": 662.4, "text": " That is better.", "tokens": [50934, 663, 307, 1101, 13, 51129], "temperature": 0.0, "avg_logprob": -0.13773228465646936, "compression_ratio": 1.4701986754966887, "no_speech_prob": 0.0016995267942547798}, {"id": 115, "seek": 64710, "start": 662.4, "end": 668.3000000000001, "text": " Perfect.", "tokens": [51129, 10246, 13, 51424], "temperature": 0.0, "avg_logprob": -0.13773228465646936, "compression_ratio": 1.4701986754966887, "no_speech_prob": 0.0016995267942547798}, {"id": 116, "seek": 64710, "start": 668.3000000000001, "end": 671.6, "text": " Something that we can do is add bold text and italics.", "tokens": [51424, 6595, 300, 321, 393, 360, 307, 909, 11928, 2487, 293, 22366, 1167, 13, 51589], "temperature": 0.0, "avg_logprob": -0.13773228465646936, "compression_ratio": 1.4701986754966887, "no_speech_prob": 0.0016995267942547798}, {"id": 117, "seek": 64710, "start": 671.6, "end": 673.6, "text": " So I'm going to show you how to do that.", "tokens": [51589, 407, 286, 478, 516, 281, 855, 291, 577, 281, 360, 300, 13, 51689], "temperature": 0.0, "avg_logprob": -0.13773228465646936, "compression_ratio": 1.4701986754966887, "no_speech_prob": 0.0016995267942547798}, {"id": 118, "seek": 67360, "start": 673.6, "end": 685.7, "text": " So if we write here an asterisk and here another asterisk at the end over string and here we add double asterisk.", "tokens": [50364, 407, 498, 321, 2464, 510, 364, 257, 3120, 7797, 293, 510, 1071, 257, 3120, 7797, 412, 264, 917, 670, 6798, 293, 510, 321, 909, 3834, 257, 3120, 7797, 13, 50969], "temperature": 0.0, "avg_logprob": -0.11754464631033416, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.002053820062428713}, {"id": 119, "seek": 67360, "start": 685.7, "end": 694.0, "text": " When we run, we're going to see that the first sentence is now going to be italics and the rest is going to be bold.", "tokens": [50969, 1133, 321, 1190, 11, 321, 434, 516, 281, 536, 300, 264, 700, 8174, 307, 586, 516, 281, 312, 22366, 1167, 293, 264, 1472, 307, 516, 281, 312, 11928, 13, 51384], "temperature": 0.0, "avg_logprob": -0.11754464631033416, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.002053820062428713}, {"id": 120, "seek": 67360, "start": 694.0, "end": 703.0, "text": " So that means that as you can see, if we want to write text in bold, we use double asterisk and then we want to write text in italics.", "tokens": [51384, 407, 300, 1355, 300, 382, 291, 393, 536, 11, 498, 321, 528, 281, 2464, 2487, 294, 11928, 11, 321, 764, 3834, 257, 3120, 7797, 293, 550, 321, 528, 281, 2464, 2487, 294, 22366, 1167, 13, 51834], "temperature": 0.0, "avg_logprob": -0.11754464631033416, "compression_ratio": 1.972972972972973, "no_speech_prob": 0.002053820062428713}, {"id": 121, "seek": 70300, "start": 703.0, "end": 710.5, "text": " We write just one asterisk.", "tokens": [50364, 492, 2464, 445, 472, 257, 3120, 7797, 13, 50739], "temperature": 0.0, "avg_logprob": -0.15695835631570698, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.002711440436542034}, {"id": 122, "seek": 70300, "start": 710.5, "end": 714.3, "text": " Okay, let's continue talking about formatting and the appearance of our app.", "tokens": [50739, 1033, 11, 718, 311, 2354, 1417, 466, 39366, 293, 264, 8967, 295, 527, 724, 13, 50929], "temperature": 0.0, "avg_logprob": -0.15695835631570698, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.002711440436542034}, {"id": 123, "seek": 70300, "start": 714.3, "end": 720.4, "text": " I think it would be cool if we could add a little line here and we can achieve that by using dividers.", "tokens": [50929, 286, 519, 309, 576, 312, 1627, 498, 321, 727, 909, 257, 707, 1622, 510, 293, 321, 393, 4584, 300, 538, 1228, 4996, 433, 13, 51234], "temperature": 0.0, "avg_logprob": -0.15695835631570698, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.002711440436542034}, {"id": 124, "seek": 70300, "start": 720.4, "end": 726.4, "text": " So in StripLid, in order to add dividers, we can use manst.divider.", "tokens": [51234, 407, 294, 745, 8400, 43, 327, 11, 294, 1668, 281, 909, 4996, 433, 11, 321, 393, 764, 587, 372, 13, 67, 592, 1438, 13, 51534], "temperature": 0.0, "avg_logprob": -0.15695835631570698, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.002711440436542034}, {"id": 125, "seek": 72640, "start": 726.4, "end": 737.3, "text": " So just as this, if we run, we will see that this line has appeared here.", "tokens": [50364, 407, 445, 382, 341, 11, 498, 321, 1190, 11, 321, 486, 536, 300, 341, 1622, 575, 8516, 510, 13, 50909], "temperature": 0.0, "avg_logprob": -0.15881179462779652, "compression_ratio": 1.375, "no_speech_prob": 0.11429579555988312}, {"id": 126, "seek": 72640, "start": 737.3, "end": 744.3, "text": " And finally, something that I would like to add is just to say that I made this app.", "tokens": [50909, 400, 2721, 11, 746, 300, 286, 576, 411, 281, 909, 307, 445, 281, 584, 300, 286, 1027, 341, 724, 13, 51259], "temperature": 0.0, "avg_logprob": -0.15881179462779652, "compression_ratio": 1.375, "no_speech_prob": 0.11429579555988312}, {"id": 127, "seek": 72640, "start": 744.3, "end": 752.8, "text": " So I'm going to use caption.", "tokens": [51259, 407, 286, 478, 516, 281, 764, 31974, 13, 51684], "temperature": 0.0, "avg_logprob": -0.15881179462779652, "compression_ratio": 1.375, "no_speech_prob": 0.11429579555988312}, {"id": 128, "seek": 75280, "start": 752.8, "end": 762.1999999999999, "text": " Okay, and if we run, we're going to see that that appears here.", "tokens": [50364, 1033, 11, 293, 498, 321, 1190, 11, 321, 434, 516, 281, 536, 300, 300, 7038, 510, 13, 50834], "temperature": 0.0, "avg_logprob": -0.18106303637540794, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.003221893683075905}, {"id": 129, "seek": 75280, "start": 762.1999999999999, "end": 765.4, "text": " I think it would look better if we could make that centered.", "tokens": [50834, 286, 519, 309, 576, 574, 1101, 498, 321, 727, 652, 300, 18988, 13, 50994], "temperature": 0.0, "avg_logprob": -0.18106303637540794, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.003221893683075905}, {"id": 130, "seek": 75280, "start": 765.4, "end": 770.9, "text": " Luckily in StripLid, we can use HTML in order to further format our text.", "tokens": [50994, 19726, 294, 745, 8400, 43, 327, 11, 321, 393, 764, 17995, 294, 1668, 281, 3052, 7877, 527, 2487, 13, 51269], "temperature": 0.0, "avg_logprob": -0.18106303637540794, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.003221893683075905}, {"id": 131, "seek": 75280, "start": 770.9, "end": 775.9, "text": " And that is what I'm going to do now in order to make this text move to the center.", "tokens": [51269, 400, 300, 307, 437, 286, 478, 516, 281, 360, 586, 294, 1668, 281, 652, 341, 2487, 1286, 281, 264, 3056, 13, 51519], "temperature": 0.0, "avg_logprob": -0.18106303637540794, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.003221893683075905}, {"id": 132, "seek": 78280, "start": 783.8, "end": 789.8, "text": " And we need to add another parameter, which is this one.", "tokens": [50414, 400, 321, 643, 281, 909, 1071, 13075, 11, 597, 307, 341, 472, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1906657934188843, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.005139154382050037}, {"id": 133, "seek": 78280, "start": 789.8, "end": 794.8, "text": " The default is set to false, but in order to make the HTML work, we need to set that to true.", "tokens": [50714, 440, 7576, 307, 992, 281, 7908, 11, 457, 294, 1668, 281, 652, 264, 17995, 589, 11, 321, 643, 281, 992, 300, 281, 2074, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1906657934188843, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.005139154382050037}, {"id": 134, "seek": 78280, "start": 794.8, "end": 795.8, "text": " So let's do that.", "tokens": [50964, 407, 718, 311, 360, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1906657934188843, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.005139154382050037}, {"id": 135, "seek": 78280, "start": 795.8, "end": 802.8, "text": " Okay, and when that is done, we cannot rerun this.", "tokens": [51014, 1033, 11, 293, 562, 300, 307, 1096, 11, 321, 2644, 43819, 409, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1906657934188843, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.005139154382050037}, {"id": 136, "seek": 78280, "start": 802.8, "end": 810.8, "text": " And there you can see that it has been centered.", "tokens": [51364, 400, 456, 291, 393, 536, 300, 309, 575, 668, 18988, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1906657934188843, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.005139154382050037}, {"id": 137, "seek": 81080, "start": 810.8, "end": 812.8, "text": " There are other things that we can add to our application.", "tokens": [50364, 821, 366, 661, 721, 300, 321, 393, 909, 281, 527, 3861, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11800418502983005, "compression_ratio": 1.5846153846153845, "no_speech_prob": 0.009550662711262703}, {"id": 138, "seek": 81080, "start": 812.8, "end": 815.8, "text": " Something that is quite interesting is expanders.", "tokens": [50464, 6595, 300, 307, 1596, 1880, 307, 5268, 433, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11800418502983005, "compression_ratio": 1.5846153846153845, "no_speech_prob": 0.009550662711262703}, {"id": 139, "seek": 81080, "start": 815.8, "end": 824.8, "text": " So let's see how to do that with st.expander.", "tokens": [50614, 407, 718, 311, 536, 577, 281, 360, 300, 365, 342, 13, 15952, 4483, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11800418502983005, "compression_ratio": 1.5846153846153845, "no_speech_prob": 0.009550662711262703}, {"id": 140, "seek": 81080, "start": 824.8, "end": 829.8, "text": " So inside of the expander, we're just going to write some placeholder text.", "tokens": [51064, 407, 1854, 295, 264, 1278, 4483, 11, 321, 434, 445, 516, 281, 2464, 512, 1081, 20480, 2487, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11800418502983005, "compression_ratio": 1.5846153846153845, "no_speech_prob": 0.009550662711262703}, {"id": 141, "seek": 81080, "start": 829.8, "end": 832.8, "text": " And let's run and see what happens.", "tokens": [51314, 400, 718, 311, 1190, 293, 536, 437, 2314, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11800418502983005, "compression_ratio": 1.5846153846153845, "no_speech_prob": 0.009550662711262703}, {"id": 142, "seek": 81080, "start": 832.8, "end": 835.8, "text": " So as you can see, this has appeared here.", "tokens": [51464, 407, 382, 291, 393, 536, 11, 341, 575, 8516, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11800418502983005, "compression_ratio": 1.5846153846153845, "no_speech_prob": 0.009550662711262703}, {"id": 143, "seek": 83580, "start": 835.8, "end": 841.8, "text": " So the user can click and then it can decide, you know, they want to look at that information or not.", "tokens": [50364, 407, 264, 4195, 393, 2052, 293, 550, 309, 393, 4536, 11, 291, 458, 11, 436, 528, 281, 574, 412, 300, 1589, 420, 406, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09489047527313232, "compression_ratio": 1.4914285714285713, "no_speech_prob": 0.0138399014249444}, {"id": 144, "seek": 83580, "start": 841.8, "end": 845.8, "text": " So then that we can also add our emojis just to make it a little bit more fun.", "tokens": [50664, 407, 550, 300, 321, 393, 611, 909, 527, 19611, 40371, 445, 281, 652, 309, 257, 707, 857, 544, 1019, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09489047527313232, "compression_ratio": 1.4914285714285713, "no_speech_prob": 0.0138399014249444}, {"id": 145, "seek": 83580, "start": 845.8, "end": 858.8, "text": " So I'm just going to add a couple there.", "tokens": [50864, 407, 286, 478, 445, 516, 281, 909, 257, 1916, 456, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09489047527313232, "compression_ratio": 1.4914285714285713, "no_speech_prob": 0.0138399014249444}, {"id": 146, "seek": 83580, "start": 858.8, "end": 861.8, "text": " Okay, let's rerun and see what happens.", "tokens": [51514, 1033, 11, 718, 311, 43819, 409, 293, 536, 437, 2314, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09489047527313232, "compression_ratio": 1.4914285714285713, "no_speech_prob": 0.0138399014249444}, {"id": 147, "seek": 86180, "start": 861.8, "end": 868.8, "text": " So now the emojis have appeared on the screen.", "tokens": [50364, 407, 586, 264, 19611, 40371, 362, 8516, 322, 264, 2568, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11626073226187993, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.02594471350312233}, {"id": 148, "seek": 86180, "start": 868.8, "end": 873.8, "text": " Okay, so now I'm going to show you how to add a button and how to manage what is called session state.", "tokens": [50714, 1033, 11, 370, 586, 286, 478, 516, 281, 855, 291, 577, 281, 909, 257, 2960, 293, 577, 281, 3067, 437, 307, 1219, 5481, 1785, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11626073226187993, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.02594471350312233}, {"id": 149, "seek": 86180, "start": 873.8, "end": 882.8, "text": " So in order to add a button in striplet, I'm going to add it just before the header is as easy as this.", "tokens": [50964, 407, 294, 1668, 281, 909, 257, 2960, 294, 12828, 2631, 11, 286, 478, 516, 281, 909, 309, 445, 949, 264, 23117, 307, 382, 1858, 382, 341, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11626073226187993, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.02594471350312233}, {"id": 150, "seek": 86180, "start": 882.8, "end": 890.8, "text": " So, well, it's with st.button, but because we want to trigger an action, we're going to add an if statement.", "tokens": [51414, 407, 11, 731, 11, 309, 311, 365, 342, 13, 5955, 1756, 11, 457, 570, 321, 528, 281, 7875, 364, 3069, 11, 321, 434, 516, 281, 909, 364, 498, 5629, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11626073226187993, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.02594471350312233}, {"id": 151, "seek": 89080, "start": 890.8, "end": 898.8, "text": " So if st.button, let's get started.", "tokens": [50364, 407, 498, 342, 13, 5955, 1756, 11, 718, 311, 483, 1409, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1228551239263816, "compression_ratio": 1.4222222222222223, "no_speech_prob": 0.04671074450016022}, {"id": 152, "seek": 89080, "start": 898.8, "end": 904.8, "text": " So when the user clicks the button, then this is going to happen.", "tokens": [50764, 407, 562, 264, 4195, 18521, 264, 2960, 11, 550, 341, 307, 516, 281, 1051, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1228551239263816, "compression_ratio": 1.4222222222222223, "no_speech_prob": 0.04671074450016022}, {"id": 153, "seek": 89080, "start": 904.8, "end": 907.8, "text": " I'm going to move that there.", "tokens": [51064, 286, 478, 516, 281, 1286, 300, 456, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1228551239263816, "compression_ratio": 1.4222222222222223, "no_speech_prob": 0.04671074450016022}, {"id": 154, "seek": 89080, "start": 907.8, "end": 915.8, "text": " And in fact, let me just move all of this after the sidebar.", "tokens": [51214, 400, 294, 1186, 11, 718, 385, 445, 1286, 439, 295, 341, 934, 264, 1252, 5356, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1228551239263816, "compression_ratio": 1.4222222222222223, "no_speech_prob": 0.04671074450016022}, {"id": 155, "seek": 91580, "start": 915.8, "end": 920.8, "text": " Okay, okay, so let's rerun.", "tokens": [50364, 1033, 11, 1392, 11, 370, 718, 311, 43819, 409, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09840003056312675, "compression_ratio": 1.4355828220858895, "no_speech_prob": 0.010487938299775124}, {"id": 156, "seek": 91580, "start": 920.8, "end": 926.8, "text": " And we can see that now this button appears here when we click.", "tokens": [50614, 400, 321, 393, 536, 300, 586, 341, 2960, 7038, 510, 562, 321, 2052, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09840003056312675, "compression_ratio": 1.4355828220858895, "no_speech_prob": 0.010487938299775124}, {"id": 157, "seek": 91580, "start": 926.8, "end": 929.8, "text": " Then we have the header and the subheader.", "tokens": [50914, 1396, 321, 362, 264, 23117, 293, 264, 1422, 1934, 260, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09840003056312675, "compression_ratio": 1.4355828220858895, "no_speech_prob": 0.010487938299775124}, {"id": 158, "seek": 91580, "start": 929.8, "end": 940.8, "text": " So let's continue building our app and I'll show you in a second why session state is so important.", "tokens": [51064, 407, 718, 311, 2354, 2390, 527, 724, 293, 286, 603, 855, 291, 294, 257, 1150, 983, 5481, 1785, 307, 370, 1021, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09840003056312675, "compression_ratio": 1.4355828220858895, "no_speech_prob": 0.010487938299775124}, {"id": 159, "seek": 94080, "start": 940.8, "end": 945.8, "text": " As we said in the explanation, your data science adventure begins with a CSV file.", "tokens": [50364, 1018, 321, 848, 294, 264, 10835, 11, 428, 1412, 3497, 9868, 7338, 365, 257, 48814, 3991, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09424219849289105, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.07801530510187149}, {"id": 160, "seek": 94080, "start": 945.8, "end": 950.8, "text": " So now I'm going to show you how to integrate a CSV uploader into your application.", "tokens": [50614, 407, 586, 286, 478, 516, 281, 855, 291, 577, 281, 13365, 257, 48814, 6580, 260, 666, 428, 3861, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09424219849289105, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.07801530510187149}, {"id": 161, "seek": 94080, "start": 950.8, "end": 957.8, "text": " Streamlin provides a convenient function called st.fileuploader for adding file uploads in your application.", "tokens": [50864, 24904, 5045, 6417, 257, 10851, 2445, 1219, 342, 13, 69, 794, 1010, 2907, 260, 337, 5127, 3991, 48611, 294, 428, 3861, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09424219849289105, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.07801530510187149}, {"id": 162, "seek": 94080, "start": 957.8, "end": 960.8, "text": " So it's as simple as adding the following.", "tokens": [51214, 407, 309, 311, 382, 2199, 382, 5127, 264, 3480, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09424219849289105, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.07801530510187149}, {"id": 163, "seek": 94080, "start": 960.8, "end": 968.8, "text": " So after the button, we are going to add the following.", "tokens": [51364, 407, 934, 264, 2960, 11, 321, 366, 516, 281, 909, 264, 3480, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09424219849289105, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.07801530510187149}, {"id": 164, "seek": 96880, "start": 968.8, "end": 973.8, "text": " And then we need to specify the type of file that we want the user to upload.", "tokens": [50364, 400, 550, 321, 643, 281, 16500, 264, 2010, 295, 3991, 300, 321, 528, 264, 4195, 281, 6580, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0846429201635984, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008445056155323982}, {"id": 165, "seek": 96880, "start": 973.8, "end": 979.8, "text": " So in this case, it's going to be a CSV file.", "tokens": [50614, 407, 294, 341, 1389, 11, 309, 311, 516, 281, 312, 257, 48814, 3991, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0846429201635984, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008445056155323982}, {"id": 166, "seek": 96880, "start": 979.8, "end": 981.8, "text": " Sorry, this isn't equal.", "tokens": [50914, 4919, 11, 341, 1943, 380, 2681, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0846429201635984, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008445056155323982}, {"id": 167, "seek": 96880, "start": 981.8, "end": 984.8, "text": " Right, let's rerun.", "tokens": [51014, 1779, 11, 718, 311, 43819, 409, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0846429201635984, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008445056155323982}, {"id": 168, "seek": 96880, "start": 984.8, "end": 989.8, "text": " Okay, so now you can see that that appears there.", "tokens": [51164, 1033, 11, 370, 586, 291, 393, 536, 300, 300, 7038, 456, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0846429201635984, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008445056155323982}, {"id": 169, "seek": 96880, "start": 989.8, "end": 991.8, "text": " Let's try it out.", "tokens": [51414, 961, 311, 853, 309, 484, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0846429201635984, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008445056155323982}, {"id": 170, "seek": 96880, "start": 991.8, "end": 996.8, "text": " Okay, so this is the data set that I'm going to use is a Twitter stock market data.", "tokens": [51514, 1033, 11, 370, 341, 307, 264, 1412, 992, 300, 286, 478, 516, 281, 764, 307, 257, 5794, 4127, 2142, 1412, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0846429201635984, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.008445056155323982}, {"id": 171, "seek": 99680, "start": 996.8, "end": 1002.8, "text": " I'm going to upload this and this appears here.", "tokens": [50364, 286, 478, 516, 281, 6580, 341, 293, 341, 7038, 510, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1740786636931987, "compression_ratio": 1.639751552795031, "no_speech_prob": 0.018259616568684578}, {"id": 172, "seek": 99680, "start": 1002.8, "end": 1003.8, "text": " Great.", "tokens": [50664, 3769, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1740786636931987, "compression_ratio": 1.639751552795031, "no_speech_prob": 0.018259616568684578}, {"id": 173, "seek": 99680, "start": 1003.8, "end": 1009.8, "text": " But we will like this to appear after we click the let's get started button.", "tokens": [50714, 583, 321, 486, 411, 341, 281, 4204, 934, 321, 2052, 264, 718, 311, 483, 1409, 2960, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1740786636931987, "compression_ratio": 1.639751552795031, "no_speech_prob": 0.018259616568684578}, {"id": 174, "seek": 99680, "start": 1009.8, "end": 1015.8, "text": " So we're just going to move this and let's rerun.", "tokens": [51014, 407, 321, 434, 445, 516, 281, 1286, 341, 293, 718, 311, 43819, 409, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1740786636931987, "compression_ratio": 1.639751552795031, "no_speech_prob": 0.018259616568684578}, {"id": 175, "seek": 99680, "start": 1015.8, "end": 1020.8, "text": " So now we click this, how there's appear here.", "tokens": [51314, 407, 586, 321, 2052, 341, 11, 577, 456, 311, 4204, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1740786636931987, "compression_ratio": 1.639751552795031, "no_speech_prob": 0.018259616568684578}, {"id": 176, "seek": 99680, "start": 1020.8, "end": 1022.8, "text": " And then we have the file uploader.", "tokens": [51564, 400, 550, 321, 362, 264, 3991, 6580, 260, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1740786636931987, "compression_ratio": 1.639751552795031, "no_speech_prob": 0.018259616568684578}, {"id": 177, "seek": 102280, "start": 1022.8, "end": 1028.8, "text": " We click, we select our file, but boom, it disappeared.", "tokens": [50364, 492, 2052, 11, 321, 3048, 527, 3991, 11, 457, 9351, 11, 309, 13954, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16948894701505962, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.05579478293657303}, {"id": 178, "seek": 102280, "start": 1028.8, "end": 1033.8, "text": " Okay, so it turns out that patterns are an unstable and that means that buttons return", "tokens": [50664, 1033, 11, 370, 309, 4523, 484, 300, 8294, 366, 364, 23742, 293, 300, 1355, 300, 9905, 2736, 50914], "temperature": 0.0, "avg_logprob": -0.16948894701505962, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.05579478293657303}, {"id": 179, "seek": 102280, "start": 1033.8, "end": 1037.8, "text": " only momentarily during the page load immediately after their click.", "tokens": [50914, 787, 1623, 3289, 1830, 264, 3028, 3677, 4258, 934, 641, 2052, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16948894701505962, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.05579478293657303}, {"id": 180, "seek": 102280, "start": 1037.8, "end": 1040.8, "text": " And then they refer to false.", "tokens": [51114, 400, 550, 436, 2864, 281, 7908, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16948894701505962, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.05579478293657303}, {"id": 181, "seek": 102280, "start": 1040.8, "end": 1044.8, "text": " So in order to work around this is streaming allowed you to use session state,", "tokens": [51264, 407, 294, 1668, 281, 589, 926, 341, 307, 11791, 4350, 291, 281, 764, 5481, 1785, 11, 51464], "temperature": 0.0, "avg_logprob": -0.16948894701505962, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.05579478293657303}, {"id": 182, "seek": 102280, "start": 1044.8, "end": 1050.8, "text": " which is essential for maintaining information and interactions between different sections of your application.", "tokens": [51464, 597, 307, 7115, 337, 14916, 1589, 293, 13280, 1296, 819, 10863, 295, 428, 3861, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16948894701505962, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.05579478293657303}, {"id": 183, "seek": 105080, "start": 1050.8, "end": 1055.8, "text": " So I'm going to show you how to implement this with the button that we have in our application.", "tokens": [50364, 407, 286, 478, 516, 281, 855, 291, 577, 281, 4445, 341, 365, 264, 2960, 300, 321, 362, 294, 527, 3861, 13, 50614], "temperature": 0.0, "avg_logprob": -0.05333430568377177, "compression_ratio": 1.3587786259541985, "no_speech_prob": 0.0010483295191079378}, {"id": 184, "seek": 105080, "start": 1055.8, "end": 1070.8, "text": " So first of all, what we need to do is to initialize the key in the session state.", "tokens": [50614, 407, 700, 295, 439, 11, 437, 321, 643, 281, 360, 307, 281, 5883, 1125, 264, 2141, 294, 264, 5481, 1785, 13, 51364], "temperature": 0.0, "avg_logprob": -0.05333430568377177, "compression_ratio": 1.3587786259541985, "no_speech_prob": 0.0010483295191079378}, {"id": 185, "seek": 107080, "start": 1070.8, "end": 1085.8, "text": " So following the notation that we've used for the statement, I'm going to create this function called collect.", "tokens": [50364, 407, 3480, 264, 24657, 300, 321, 600, 1143, 337, 264, 5629, 11, 286, 478, 516, 281, 1884, 341, 2445, 1219, 2500, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1692505799807035, "compression_ratio": 1.1956521739130435, "no_speech_prob": 0.007396973669528961}, {"id": 186, "seek": 108580, "start": 1085.8, "end": 1103.8, "text": " And finally, what we need to do is to modify your button.", "tokens": [50364, 400, 2721, 11, 437, 321, 643, 281, 360, 307, 281, 16927, 428, 2960, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1617823230976961, "compression_ratio": 1.282258064516129, "no_speech_prob": 0.01628413051366806}, {"id": 187, "seek": 108580, "start": 1103.8, "end": 1109.8, "text": " And then under this if statement, then we can add the CSB uploader.", "tokens": [51264, 400, 550, 833, 341, 498, 5629, 11, 550, 321, 393, 909, 264, 9460, 33, 6580, 260, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1617823230976961, "compression_ratio": 1.282258064516129, "no_speech_prob": 0.01628413051366806}, {"id": 188, "seek": 108580, "start": 1109.8, "end": 1112.8, "text": " So let's rerun, see what happens.", "tokens": [51564, 407, 718, 311, 43819, 409, 11, 536, 437, 2314, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1617823230976961, "compression_ratio": 1.282258064516129, "no_speech_prob": 0.01628413051366806}, {"id": 189, "seek": 111280, "start": 1112.8, "end": 1122.8, "text": " Okay, so now we click, disappears, browse file, choose our file, we upload and nothing disappears.", "tokens": [50364, 1033, 11, 370, 586, 321, 2052, 11, 25527, 11, 31442, 3991, 11, 2826, 527, 3991, 11, 321, 6580, 293, 1825, 25527, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1224429635440602, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.02368580922484398}, {"id": 190, "seek": 111280, "start": 1122.8, "end": 1129.8, "text": " So we have fixed this problem using session state.", "tokens": [50864, 407, 321, 362, 6806, 341, 1154, 1228, 5481, 1785, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1224429635440602, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.02368580922484398}, {"id": 191, "seek": 111280, "start": 1129.8, "end": 1136.8, "text": " Okay, so now that the users can upload their CSB files, it's time to convert the uploaded file into a pandas data frame,", "tokens": [51214, 1033, 11, 370, 586, 300, 264, 5022, 393, 6580, 641, 9460, 33, 7098, 11, 309, 311, 565, 281, 7620, 264, 17135, 3991, 666, 257, 4565, 296, 1412, 3920, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1224429635440602, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.02368580922484398}, {"id": 192, "seek": 111280, "start": 1136.8, "end": 1141.8, "text": " which is the standard data structure for data manipulation and analysis in Python.", "tokens": [51564, 597, 307, 264, 3832, 1412, 3877, 337, 1412, 26475, 293, 5215, 294, 15329, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1224429635440602, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.02368580922484398}, {"id": 193, "seek": 114180, "start": 1141.8, "end": 1153.8, "text": " So for that, what we're going to do is after the user uploads the file, what we're going to say is if the user CSB file actually exists,", "tokens": [50364, 407, 337, 300, 11, 437, 321, 434, 516, 281, 360, 307, 934, 264, 4195, 48611, 264, 3991, 11, 437, 321, 434, 516, 281, 584, 307, 498, 264, 4195, 9460, 33, 3991, 767, 8198, 11, 50964], "temperature": 0.0, "avg_logprob": -0.10102264553892847, "compression_ratio": 1.94, "no_speech_prob": 0.03619358688592911}, {"id": 194, "seek": 114180, "start": 1153.8, "end": 1157.8, "text": " we're going to transform that into a data frame.", "tokens": [50964, 321, 434, 516, 281, 4088, 300, 666, 257, 1412, 3920, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10102264553892847, "compression_ratio": 1.94, "no_speech_prob": 0.03619358688592911}, {"id": 195, "seek": 114180, "start": 1157.8, "end": 1167.8, "text": " Also, what we're going to do is to say the low memory to false, just because the default is to actually optimize memory,", "tokens": [51164, 2743, 11, 437, 321, 434, 516, 281, 360, 307, 281, 584, 264, 2295, 4675, 281, 7908, 11, 445, 570, 264, 7576, 307, 281, 767, 19719, 4675, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10102264553892847, "compression_ratio": 1.94, "no_speech_prob": 0.03619358688592911}, {"id": 196, "seek": 114180, "start": 1167.8, "end": 1170.8, "text": " but just in case the file is really large, we're just going to set that to false.", "tokens": [51664, 457, 445, 294, 1389, 264, 3991, 307, 534, 2416, 11, 321, 434, 445, 516, 281, 992, 300, 281, 7908, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10102264553892847, "compression_ratio": 1.94, "no_speech_prob": 0.03619358688592911}, {"id": 197, "seek": 117080, "start": 1170.8, "end": 1176.8, "text": " Also, what we're going to do is to ensure that the file pointer is at the start of the file just in case.", "tokens": [50364, 2743, 11, 437, 321, 434, 516, 281, 360, 307, 281, 5586, 300, 264, 3991, 23918, 307, 412, 264, 722, 295, 264, 3991, 445, 294, 1389, 13, 50664], "temperature": 0.0, "avg_logprob": -0.050137215190463595, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.028001654893159866}, {"id": 198, "seek": 117080, "start": 1176.8, "end": 1186.8, "text": " So let's write all that down.", "tokens": [50664, 407, 718, 311, 2464, 439, 300, 760, 13, 51164], "temperature": 0.0, "avg_logprob": -0.050137215190463595, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.028001654893159866}, {"id": 199, "seek": 117080, "start": 1186.8, "end": 1190.8, "text": " So that ensures that the file pointer is at the start of the file.", "tokens": [51164, 407, 300, 28111, 300, 264, 3991, 23918, 307, 412, 264, 722, 295, 264, 3991, 13, 51364], "temperature": 0.0, "avg_logprob": -0.050137215190463595, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.028001654893159866}, {"id": 200, "seek": 117080, "start": 1190.8, "end": 1199.8, "text": " And then we're going to transform this into a data frame.", "tokens": [51364, 400, 550, 321, 434, 516, 281, 4088, 341, 666, 257, 1412, 3920, 13, 51814], "temperature": 0.0, "avg_logprob": -0.050137215190463595, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.028001654893159866}, {"id": 201, "seek": 119980, "start": 1199.8, "end": 1209.8, "text": " Okay, and now the data frame is now ready for analysis and exploration.", "tokens": [50364, 1033, 11, 293, 586, 264, 1412, 3920, 307, 586, 1919, 337, 5215, 293, 16197, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1433155714575924, "compression_ratio": 1.5073891625615763, "no_speech_prob": 0.004003747366368771}, {"id": 202, "seek": 119980, "start": 1209.8, "end": 1217.8, "text": " Our AI assistant relies on large language models to provide natural language understanding and generate responses.", "tokens": [50864, 2621, 7318, 10994, 30910, 322, 2416, 2856, 5245, 281, 2893, 3303, 2856, 3701, 293, 8460, 13019, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1433155714575924, "compression_ratio": 1.5073891625615763, "no_speech_prob": 0.004003747366368771}, {"id": 203, "seek": 119980, "start": 1217.8, "end": 1224.8, "text": " In this section, what we're going to cover is how to load and initialize the LN model for your string like application.", "tokens": [51264, 682, 341, 3541, 11, 437, 321, 434, 516, 281, 2060, 307, 577, 281, 3677, 293, 5883, 1125, 264, 441, 45, 2316, 337, 428, 6798, 411, 3861, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1433155714575924, "compression_ratio": 1.5073891625615763, "no_speech_prob": 0.004003747366368771}, {"id": 204, "seek": 122480, "start": 1224.8, "end": 1233.8, "text": " So first of all, what we're going to do is we're going to create an instance of the LN model and also we're going to set the temperature parameter to zero.", "tokens": [50364, 407, 700, 295, 439, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1884, 364, 5197, 295, 264, 441, 45, 2316, 293, 611, 321, 434, 516, 281, 992, 264, 4292, 13075, 281, 4018, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05671014785766602, "compression_ratio": 1.9218106995884774, "no_speech_prob": 0.12080639600753784}, {"id": 205, "seek": 122480, "start": 1233.8, "end": 1235.8, "text": " What does that mean?", "tokens": [50814, 708, 775, 300, 914, 30, 50914], "temperature": 0.0, "avg_logprob": -0.05671014785766602, "compression_ratio": 1.9218106995884774, "no_speech_prob": 0.12080639600753784}, {"id": 206, "seek": 122480, "start": 1235.8, "end": 1240.8, "text": " Well, the temperature, what it does in controls the randomness of the model.", "tokens": [50914, 1042, 11, 264, 4292, 11, 437, 309, 775, 294, 9003, 264, 4974, 1287, 295, 264, 2316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.05671014785766602, "compression_ratio": 1.9218106995884774, "no_speech_prob": 0.12080639600753784}, {"id": 207, "seek": 122480, "start": 1240.8, "end": 1244.8, "text": " So the higher the temperature, the more creative your model is going to be.", "tokens": [51164, 407, 264, 2946, 264, 4292, 11, 264, 544, 5880, 428, 2316, 307, 516, 281, 312, 13, 51364], "temperature": 0.0, "avg_logprob": -0.05671014785766602, "compression_ratio": 1.9218106995884774, "no_speech_prob": 0.12080639600753784}, {"id": 208, "seek": 122480, "start": 1244.8, "end": 1252.8, "text": " So for this particular project, we're going to let the temperature be low in order to make the responses a little bit more deterministic.", "tokens": [51364, 407, 337, 341, 1729, 1716, 11, 321, 434, 516, 281, 718, 264, 4292, 312, 2295, 294, 1668, 281, 652, 264, 13019, 257, 707, 857, 544, 15957, 3142, 13, 51764], "temperature": 0.0, "avg_logprob": -0.05671014785766602, "compression_ratio": 1.9218106995884774, "no_speech_prob": 0.12080639600753784}, {"id": 209, "seek": 125280, "start": 1252.8, "end": 1254.8, "text": " So let's write that down.", "tokens": [50364, 407, 718, 311, 2464, 300, 760, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09247223536173503, "compression_ratio": 1.6303030303030304, "no_speech_prob": 0.010326909832656384}, {"id": 210, "seek": 125280, "start": 1254.8, "end": 1257.8, "text": " So we create an instance of the model.", "tokens": [50464, 407, 321, 1884, 364, 5197, 295, 264, 2316, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09247223536173503, "compression_ratio": 1.6303030303030304, "no_speech_prob": 0.010326909832656384}, {"id": 211, "seek": 125280, "start": 1257.8, "end": 1260.8, "text": " We're going to call it LLM.", "tokens": [50614, 492, 434, 516, 281, 818, 309, 441, 43, 44, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09247223536173503, "compression_ratio": 1.6303030303030304, "no_speech_prob": 0.010326909832656384}, {"id": 212, "seek": 125280, "start": 1260.8, "end": 1274.8, "text": " And then we're going to set the temperature to zero.", "tokens": [50764, 400, 550, 321, 434, 516, 281, 992, 264, 4292, 281, 4018, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09247223536173503, "compression_ratio": 1.6303030303030304, "no_speech_prob": 0.010326909832656384}, {"id": 213, "seek": 125280, "start": 1274.8, "end": 1281.8, "text": " So first thing we're going to use our model for is to generate some information and we're going to add that to the sidebar.", "tokens": [51464, 407, 700, 551, 321, 434, 516, 281, 764, 527, 2316, 337, 307, 281, 8460, 512, 1589, 293, 321, 434, 516, 281, 909, 300, 281, 264, 1252, 5356, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09247223536173503, "compression_ratio": 1.6303030303030304, "no_speech_prob": 0.010326909832656384}, {"id": 214, "seek": 128180, "start": 1281.8, "end": 1288.8, "text": " So I think it will be interesting if we could add some information about what is the steps of the EDR and we add that into the sidebar.", "tokens": [50364, 407, 286, 519, 309, 486, 312, 1880, 498, 321, 727, 909, 512, 1589, 466, 437, 307, 264, 4439, 295, 264, 462, 9301, 293, 321, 909, 300, 666, 264, 1252, 5356, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11088371276855469, "compression_ratio": 1.5339805825242718, "no_speech_prob": 0.0032210934441536665}, {"id": 215, "seek": 128180, "start": 1288.8, "end": 1295.8, "text": " And if the user wants to look at it because it finds it useful, just need to click and the information will expand.", "tokens": [50714, 400, 498, 264, 4195, 2738, 281, 574, 412, 309, 570, 309, 10704, 309, 4420, 11, 445, 643, 281, 2052, 293, 264, 1589, 486, 5268, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11088371276855469, "compression_ratio": 1.5339805825242718, "no_speech_prob": 0.0032210934441536665}, {"id": 216, "seek": 128180, "start": 1295.8, "end": 1297.8, "text": " So let's do that.", "tokens": [51064, 407, 718, 311, 360, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11088371276855469, "compression_ratio": 1.5339805825242718, "no_speech_prob": 0.0032210934441536665}, {"id": 217, "seek": 128180, "start": 1297.8, "end": 1308.8, "text": " So I'm going to move our LLM model first here.", "tokens": [51164, 407, 286, 478, 516, 281, 1286, 527, 441, 43, 44, 2316, 700, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11088371276855469, "compression_ratio": 1.5339805825242718, "no_speech_prob": 0.0032210934441536665}, {"id": 218, "seek": 130880, "start": 1308.8, "end": 1318.8, "text": " And then what we're going to do is in the expander, what are the steps of EDA?", "tokens": [50364, 400, 550, 437, 321, 434, 516, 281, 360, 307, 294, 264, 1278, 4483, 11, 437, 366, 264, 4439, 295, 462, 7509, 30, 50864], "temperature": 0.0, "avg_logprob": -0.07050658265749614, "compression_ratio": 1.7732558139534884, "no_speech_prob": 0.028419533744454384}, {"id": 219, "seek": 130880, "start": 1318.8, "end": 1325.8, "text": " And what is going to right now is the steps of the EDA, but we're not going to manually type it down.", "tokens": [50864, 400, 437, 307, 516, 281, 558, 586, 307, 264, 4439, 295, 264, 462, 7509, 11, 457, 321, 434, 406, 516, 281, 16945, 2010, 309, 760, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07050658265749614, "compression_ratio": 1.7732558139534884, "no_speech_prob": 0.028419533744454384}, {"id": 220, "seek": 130880, "start": 1325.8, "end": 1328.8, "text": " The LLM is going to give us this information.", "tokens": [51214, 440, 441, 43, 44, 307, 516, 281, 976, 505, 341, 1589, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07050658265749614, "compression_ratio": 1.7732558139534884, "no_speech_prob": 0.028419533744454384}, {"id": 221, "seek": 130880, "start": 1328.8, "end": 1334.8, "text": " So in order to do that, what we're going to do is we're going to say, OK, LLM.", "tokens": [51364, 407, 294, 1668, 281, 360, 300, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 584, 11, 2264, 11, 441, 43, 44, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07050658265749614, "compression_ratio": 1.7732558139534884, "no_speech_prob": 0.028419533744454384}, {"id": 222, "seek": 133480, "start": 1334.8, "end": 1347.8, "text": " And then we're going to ask the same, what are the steps of EDA?", "tokens": [50364, 400, 550, 321, 434, 516, 281, 1029, 264, 912, 11, 437, 366, 264, 4439, 295, 462, 7509, 30, 51014], "temperature": 0.0, "avg_logprob": -0.10652310161267296, "compression_ratio": 1.2671755725190839, "no_speech_prob": 0.037311919033527374}, {"id": 223, "seek": 133480, "start": 1347.8, "end": 1350.8, "text": " So let's rerun.", "tokens": [51014, 407, 718, 311, 43819, 409, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10652310161267296, "compression_ratio": 1.2671755725190839, "no_speech_prob": 0.037311919033527374}, {"id": 224, "seek": 133480, "start": 1350.8, "end": 1353.8, "text": " OK, so very important.", "tokens": [51164, 2264, 11, 370, 588, 1021, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10652310161267296, "compression_ratio": 1.2671755725190839, "no_speech_prob": 0.037311919033527374}, {"id": 225, "seek": 133480, "start": 1353.8, "end": 1356.8, "text": " We need to add the open AI key.", "tokens": [51314, 492, 643, 281, 909, 264, 1269, 7318, 2141, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10652310161267296, "compression_ratio": 1.2671755725190839, "no_speech_prob": 0.037311919033527374}, {"id": 226, "seek": 133480, "start": 1356.8, "end": 1359.8, "text": " So let's do that very quickly.", "tokens": [51464, 407, 718, 311, 360, 300, 588, 2661, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10652310161267296, "compression_ratio": 1.2671755725190839, "no_speech_prob": 0.037311919033527374}, {"id": 227, "seek": 135980, "start": 1359.8, "end": 1365.8, "text": " So we're going to add it here after we have imported the required packages.", "tokens": [50364, 407, 321, 434, 516, 281, 909, 309, 510, 934, 321, 362, 25524, 264, 4739, 17401, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07712828671490704, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.06889213621616364}, {"id": 228, "seek": 135980, "start": 1365.8, "end": 1372.8, "text": " And also something that we need to do is to load the dot environment just to make sure that the variables are correctly read.", "tokens": [50664, 400, 611, 746, 300, 321, 643, 281, 360, 307, 281, 3677, 264, 5893, 2823, 445, 281, 652, 988, 300, 264, 9102, 366, 8944, 1401, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07712828671490704, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.06889213621616364}, {"id": 229, "seek": 135980, "start": 1372.8, "end": 1383.8, "text": " Let's do that.", "tokens": [51014, 961, 311, 360, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07712828671490704, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.06889213621616364}, {"id": 230, "seek": 138380, "start": 1383.8, "end": 1396.8, "text": " OK, so what we're going to do, and you'll see why later, is we're going to move this at the very end.", "tokens": [50364, 2264, 11, 370, 437, 321, 434, 516, 281, 360, 11, 293, 291, 603, 536, 983, 1780, 11, 307, 321, 434, 516, 281, 1286, 341, 412, 264, 588, 917, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15198848122044614, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.015298260375857353}, {"id": 231, "seek": 138380, "start": 1396.8, "end": 1403.8, "text": " And we're going to add another with SD dot site bar.", "tokens": [51014, 400, 321, 434, 516, 281, 909, 1071, 365, 14638, 5893, 3621, 2159, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15198848122044614, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.015298260375857353}, {"id": 232, "seek": 138380, "start": 1403.8, "end": 1406.8, "text": " Oh, and I saw another error.", "tokens": [51364, 876, 11, 293, 286, 1866, 1071, 6713, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15198848122044614, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.015298260375857353}, {"id": 233, "seek": 138380, "start": 1406.8, "end": 1408.8, "text": " So that should be like that.", "tokens": [51514, 407, 300, 820, 312, 411, 300, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15198848122044614, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.015298260375857353}, {"id": 234, "seek": 138380, "start": 1408.8, "end": 1409.8, "text": " Right, let's rerun.", "tokens": [51614, 1779, 11, 718, 311, 43819, 409, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15198848122044614, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.015298260375857353}, {"id": 235, "seek": 140980, "start": 1409.8, "end": 1414.8, "text": " We're going to browse the file, upload.", "tokens": [50364, 492, 434, 516, 281, 31442, 264, 3991, 11, 6580, 13, 50614], "temperature": 0.0, "avg_logprob": -0.15286791324615479, "compression_ratio": 1.35, "no_speech_prob": 0.05337907746434212}, {"id": 236, "seek": 140980, "start": 1414.8, "end": 1417.8, "text": " It's running.", "tokens": [50614, 467, 311, 2614, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15286791324615479, "compression_ratio": 1.35, "no_speech_prob": 0.05337907746434212}, {"id": 237, "seek": 140980, "start": 1417.8, "end": 1419.8, "text": " And let's check the sidebar.", "tokens": [50764, 400, 718, 311, 1520, 264, 1252, 5356, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15286791324615479, "compression_ratio": 1.35, "no_speech_prob": 0.05337907746434212}, {"id": 238, "seek": 140980, "start": 1419.8, "end": 1421.8, "text": " So we have the expander here.", "tokens": [50864, 407, 321, 362, 264, 1278, 4483, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15286791324615479, "compression_ratio": 1.35, "no_speech_prob": 0.05337907746434212}, {"id": 239, "seek": 140980, "start": 1421.8, "end": 1430.8, "text": " And if we click, we can see that the LLM has generated the steps of the EDA.", "tokens": [50964, 400, 498, 321, 2052, 11, 321, 393, 536, 300, 264, 441, 43, 44, 575, 10833, 264, 4439, 295, 264, 462, 7509, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15286791324615479, "compression_ratio": 1.35, "no_speech_prob": 0.05337907746434212}, {"id": 240, "seek": 143080, "start": 1430.8, "end": 1439.8, "text": " In this section, what we're going to do is we're going to cover how to create a pandas agent and enable it to analyze and provide insights about the data.", "tokens": [50364, 682, 341, 3541, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 2060, 577, 281, 1884, 257, 4565, 296, 9461, 293, 9528, 309, 281, 12477, 293, 2893, 14310, 466, 264, 1412, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07721142015959087, "compression_ratio": 1.8534031413612566, "no_speech_prob": 0.06362105906009674}, {"id": 241, "seek": 143080, "start": 1439.8, "end": 1448.8, "text": " So first of all, what we need to do is to create an instance of the pandas agent bypassing our LLM model and the data frame that we want to analyze.", "tokens": [50814, 407, 700, 295, 439, 11, 437, 321, 643, 281, 360, 307, 281, 1884, 364, 5197, 295, 264, 4565, 296, 9461, 24996, 278, 527, 441, 43, 44, 2316, 293, 264, 1412, 3920, 300, 321, 528, 281, 12477, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07721142015959087, "compression_ratio": 1.8534031413612566, "no_speech_prob": 0.06362105906009674}, {"id": 242, "seek": 143080, "start": 1448.8, "end": 1455.8, "text": " So in order to do that, what we're going to do is.", "tokens": [51264, 407, 294, 1668, 281, 360, 300, 11, 437, 321, 434, 516, 281, 360, 307, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07721142015959087, "compression_ratio": 1.8534031413612566, "no_speech_prob": 0.06362105906009674}, {"id": 243, "seek": 145580, "start": 1455.8, "end": 1465.8, "text": " So here we have pandas agent.", "tokens": [50364, 407, 510, 321, 362, 4565, 296, 9461, 13, 50864], "temperature": 0.0, "avg_logprob": -0.20140954314685258, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.016619142144918442}, {"id": 244, "seek": 145580, "start": 1465.8, "end": 1470.8, "text": " And then we use the create pandas data frame agent, and we're going to pass the LLM.", "tokens": [50864, 400, 550, 321, 764, 264, 1884, 4565, 296, 1412, 3920, 9461, 11, 293, 321, 434, 516, 281, 1320, 264, 441, 43, 44, 13, 51114], "temperature": 0.0, "avg_logprob": -0.20140954314685258, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.016619142144918442}, {"id": 245, "seek": 145580, "start": 1470.8, "end": 1477.8, "text": " And we are also going to pass the data frame, and we're going to set the reverse to true.", "tokens": [51114, 400, 321, 366, 611, 516, 281, 1320, 264, 1412, 3920, 11, 293, 321, 434, 516, 281, 992, 264, 9943, 281, 2074, 13, 51464], "temperature": 0.0, "avg_logprob": -0.20140954314685258, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.016619142144918442}, {"id": 246, "seek": 147780, "start": 1477.8, "end": 1488.8, "text": " So the default is false, and that's just to see the train of thought of the agent.", "tokens": [50364, 407, 264, 7576, 307, 7908, 11, 293, 300, 311, 445, 281, 536, 264, 3847, 295, 1194, 295, 264, 9461, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14867460223990428, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.07349923998117447}, {"id": 247, "seek": 147780, "start": 1488.8, "end": 1494.8, "text": " So first thing we're going to use our pandas agent for is to answer specific questions about the data.", "tokens": [50914, 407, 700, 551, 321, 434, 516, 281, 764, 527, 4565, 296, 9461, 337, 307, 281, 1867, 2685, 1651, 466, 264, 1412, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14867460223990428, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.07349923998117447}, {"id": 248, "seek": 147780, "start": 1494.8, "end": 1502.8, "text": " So for example, let's create a question that is, what is the meaning of the colons?", "tokens": [51214, 407, 337, 1365, 11, 718, 311, 1884, 257, 1168, 300, 307, 11, 437, 307, 264, 3620, 295, 264, 1173, 892, 30, 51614], "temperature": 0.0, "avg_logprob": -0.14867460223990428, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.07349923998117447}, {"id": 249, "seek": 150280, "start": 1502.8, "end": 1513.8, "text": " What we're going to do is to create a variable that is going to be called colons meaning, for example, that is going to be the response over pandas agent.", "tokens": [50364, 708, 321, 434, 516, 281, 360, 307, 281, 1884, 257, 7006, 300, 307, 516, 281, 312, 1219, 1173, 892, 3620, 11, 337, 1365, 11, 300, 307, 516, 281, 312, 264, 4134, 670, 4565, 296, 9461, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10136800653794233, "compression_ratio": 1.6827956989247312, "no_speech_prob": 0.06263620406389236}, {"id": 250, "seek": 150280, "start": 1513.8, "end": 1521.8, "text": " So for that, we need to say, okay, pandas agent dot run, and then we're going to pass the question.", "tokens": [50914, 407, 337, 300, 11, 321, 643, 281, 584, 11, 1392, 11, 4565, 296, 9461, 5893, 1190, 11, 293, 550, 321, 434, 516, 281, 1320, 264, 1168, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10136800653794233, "compression_ratio": 1.6827956989247312, "no_speech_prob": 0.06263620406389236}, {"id": 251, "seek": 150280, "start": 1521.8, "end": 1526.8, "text": " Okay, so now we want to see the output in our application.", "tokens": [51314, 1033, 11, 370, 586, 321, 528, 281, 536, 264, 5598, 294, 527, 3861, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10136800653794233, "compression_ratio": 1.6827956989247312, "no_speech_prob": 0.06263620406389236}, {"id": 252, "seek": 152680, "start": 1526.8, "end": 1535.8, "text": " So we write ST dot write, and we're going to write their colons meaning.", "tokens": [50364, 407, 321, 2464, 4904, 5893, 2464, 11, 293, 321, 434, 516, 281, 2464, 641, 1173, 892, 3620, 13, 50814], "temperature": 0.0, "avg_logprob": -0.19436639639047476, "compression_ratio": 1.6394557823129252, "no_speech_prob": 0.05829126387834549}, {"id": 253, "seek": 152680, "start": 1535.8, "end": 1540.8, "text": " Let's run.", "tokens": [50814, 961, 311, 1190, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19436639639047476, "compression_ratio": 1.6394557823129252, "no_speech_prob": 0.05829126387834549}, {"id": 254, "seek": 152680, "start": 1540.8, "end": 1541.8, "text": " Cool.", "tokens": [51064, 8561, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19436639639047476, "compression_ratio": 1.6394557823129252, "no_speech_prob": 0.05829126387834549}, {"id": 255, "seek": 152680, "start": 1541.8, "end": 1551.8, "text": " So the columns represent the date, opening price, highest price, lowest price, closing price, adjust, and adjust closing price and volume of the stock.", "tokens": [51114, 407, 264, 13766, 2906, 264, 4002, 11, 5193, 3218, 11, 6343, 3218, 11, 12437, 3218, 11, 10377, 3218, 11, 4369, 11, 293, 4369, 10377, 3218, 293, 5523, 295, 264, 4127, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19436639639047476, "compression_ratio": 1.6394557823129252, "no_speech_prob": 0.05829126387834549}, {"id": 256, "seek": 155180, "start": 1551.8, "end": 1559.8, "text": " So our pandas agent is able to answer this predefined question that we have passed through it.", "tokens": [50364, 407, 527, 4565, 296, 9461, 307, 1075, 281, 1867, 341, 659, 37716, 1168, 300, 321, 362, 4678, 807, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05736402023670285, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.25290447473526}, {"id": 257, "seek": 155180, "start": 1559.8, "end": 1563.8, "text": " So what we're going to do now is to create a function with certain predefined questions.", "tokens": [50764, 407, 437, 321, 434, 516, 281, 360, 586, 307, 281, 1884, 257, 2445, 365, 1629, 659, 37716, 1651, 13, 50964], "temperature": 0.0, "avg_logprob": -0.05736402023670285, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.25290447473526}, {"id": 258, "seek": 155180, "start": 1563.8, "end": 1569.8, "text": " So when we run these functions, all these questions will be answered by the pandas agent.", "tokens": [50964, 407, 562, 321, 1190, 613, 6828, 11, 439, 613, 1651, 486, 312, 10103, 538, 264, 4565, 296, 9461, 13, 51264], "temperature": 0.0, "avg_logprob": -0.05736402023670285, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.25290447473526}, {"id": 259, "seek": 155180, "start": 1569.8, "end": 1575.8, "text": " So the type of questions that we're going to ask are very general EDA questions.", "tokens": [51264, 407, 264, 2010, 295, 1651, 300, 321, 434, 516, 281, 1029, 366, 588, 2674, 462, 7509, 1651, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05736402023670285, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.25290447473526}, {"id": 260, "seek": 157580, "start": 1575.8, "end": 1583.8, "text": " For example, how do the rows of the data set look like?", "tokens": [50364, 1171, 1365, 11, 577, 360, 264, 13241, 295, 264, 1412, 992, 574, 411, 30, 50764], "temperature": 0.0, "avg_logprob": -0.1550634503364563, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.40220993757247925}, {"id": 261, "seek": 157580, "start": 1583.8, "end": 1585.8, "text": " As I said, what are the meaning of the columns?", "tokens": [50764, 1018, 286, 848, 11, 437, 366, 264, 3620, 295, 264, 13766, 30, 50864], "temperature": 0.0, "avg_logprob": -0.1550634503364563, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.40220993757247925}, {"id": 262, "seek": 157580, "start": 1585.8, "end": 1588.8, "text": " How many missing values do we have?", "tokens": [50864, 1012, 867, 5361, 4190, 360, 321, 362, 30, 51014], "temperature": 0.0, "avg_logprob": -0.1550634503364563, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.40220993757247925}, {"id": 263, "seek": 157580, "start": 1588.8, "end": 1598.8, "text": " Are any duplicate values a little bit about data summarization and even a little bit of feature engineering?", "tokens": [51014, 2014, 604, 23976, 4190, 257, 707, 857, 466, 1412, 14611, 2144, 293, 754, 257, 707, 857, 295, 4111, 7043, 30, 51514], "temperature": 0.0, "avg_logprob": -0.1550634503364563, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.40220993757247925}, {"id": 264, "seek": 159880, "start": 1598.8, "end": 1608.8, "text": " We're going to ask the pandas agent if there are any new features that it would be interesting to create.", "tokens": [50364, 492, 434, 516, 281, 1029, 264, 4565, 296, 9461, 498, 456, 366, 604, 777, 4122, 300, 309, 576, 312, 1880, 281, 1884, 13, 50864], "temperature": 0.0, "avg_logprob": -0.127290774614383, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.2655167579650879}, {"id": 265, "seek": 159880, "start": 1608.8, "end": 1612.8, "text": " So let's write down this function.", "tokens": [50864, 407, 718, 311, 2464, 760, 341, 2445, 13, 51064], "temperature": 0.0, "avg_logprob": -0.127290774614383, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.2655167579650879}, {"id": 266, "seek": 159880, "start": 1612.8, "end": 1618.8, "text": " So we're going to leave the pandas agent outside, and then I'll just remove this for now.", "tokens": [51064, 407, 321, 434, 516, 281, 1856, 264, 4565, 296, 9461, 2380, 11, 293, 550, 286, 603, 445, 4159, 341, 337, 586, 13, 51364], "temperature": 0.0, "avg_logprob": -0.127290774614383, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.2655167579650879}, {"id": 267, "seek": 159880, "start": 1618.8, "end": 1626.8, "text": " And we'll just create a function that's going to be function agent.", "tokens": [51364, 400, 321, 603, 445, 1884, 257, 2445, 300, 311, 516, 281, 312, 2445, 9461, 13, 51764], "temperature": 0.0, "avg_logprob": -0.127290774614383, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.2655167579650879}, {"id": 268, "seek": 162680, "start": 1626.8, "end": 1637.8, "text": " So we're going to use the pandas agent to generate all the answers of these questions.", "tokens": [50364, 407, 321, 434, 516, 281, 764, 264, 4565, 296, 9461, 281, 8460, 439, 264, 6338, 295, 613, 1651, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1654324297045098, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.024403385818004608}, {"id": 269, "seek": 162680, "start": 1637.8, "end": 1650.8, "text": " Okay, so having written a function down, let's call the function and then let's run the app and let's see what happens.", "tokens": [50914, 1033, 11, 370, 1419, 3720, 257, 2445, 760, 11, 718, 311, 818, 264, 2445, 293, 550, 718, 311, 1190, 264, 724, 293, 718, 311, 536, 437, 2314, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1654324297045098, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.024403385818004608}, {"id": 270, "seek": 162680, "start": 1650.8, "end": 1655.8, "text": " Okay, we rerun.", "tokens": [51564, 1033, 11, 321, 43819, 409, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1654324297045098, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.024403385818004608}, {"id": 271, "seek": 165580, "start": 1655.8, "end": 1657.8, "text": " Okay, so this has appeared here.", "tokens": [50364, 1033, 11, 370, 341, 575, 8516, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14458049138387044, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.06368529796600342}, {"id": 272, "seek": 165580, "start": 1657.8, "end": 1662.8, "text": " So we can see how the first rows of our data set look like.", "tokens": [50464, 407, 321, 393, 536, 577, 264, 700, 13241, 295, 527, 1412, 992, 574, 411, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14458049138387044, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.06368529796600342}, {"id": 273, "seek": 165580, "start": 1662.8, "end": 1665.8, "text": " Then we have the meaning of the columns.", "tokens": [50714, 1396, 321, 362, 264, 3620, 295, 264, 13766, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14458049138387044, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.06368529796600342}, {"id": 274, "seek": 165580, "start": 1665.8, "end": 1669.8, "text": " We have the missing values saying that there are no duplicates.", "tokens": [50864, 492, 362, 264, 5361, 4190, 1566, 300, 456, 366, 572, 17154, 1024, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14458049138387044, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.06368529796600342}, {"id": 275, "seek": 165580, "start": 1669.8, "end": 1679.8, "text": " Information about the data summarization, a correlation between variables and also what are the features that we could maybe create.", "tokens": [51064, 15357, 466, 264, 1412, 14611, 2144, 11, 257, 20009, 1296, 9102, 293, 611, 437, 366, 264, 4122, 300, 321, 727, 1310, 1884, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14458049138387044, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.06368529796600342}, {"id": 276, "seek": 165580, "start": 1679.8, "end": 1684.8, "text": " And also it's talking about potential layers.", "tokens": [51564, 400, 611, 309, 311, 1417, 466, 3995, 7914, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14458049138387044, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.06368529796600342}, {"id": 277, "seek": 168580, "start": 1685.8, "end": 1693.8, "text": " Okay, now imagine that the user is interested in a very specific variable.", "tokens": [50364, 1033, 11, 586, 3811, 300, 264, 4195, 307, 3102, 294, 257, 588, 2685, 7006, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14162201812301856, "compression_ratio": 1.5248618784530388, "no_speech_prob": 0.003171429270878434}, {"id": 278, "seek": 168580, "start": 1693.8, "end": 1697.8, "text": " And we want the user to select the variable for further study.", "tokens": [50764, 400, 321, 528, 264, 4195, 281, 3048, 264, 7006, 337, 3052, 2979, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14162201812301856, "compression_ratio": 1.5248618784530388, "no_speech_prob": 0.003171429270878434}, {"id": 279, "seek": 168580, "start": 1697.8, "end": 1701.8, "text": " Well, we can do this using text input.", "tokens": [50964, 1042, 11, 321, 393, 360, 341, 1228, 2487, 4846, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14162201812301856, "compression_ratio": 1.5248618784530388, "no_speech_prob": 0.003171429270878434}, {"id": 280, "seek": 168580, "start": 1701.8, "end": 1707.8, "text": " This is a feature that Streamlick has in where the user can actually write text in the application.", "tokens": [51164, 639, 307, 257, 4111, 300, 24904, 75, 618, 575, 294, 689, 264, 4195, 393, 767, 2464, 2487, 294, 264, 3861, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14162201812301856, "compression_ratio": 1.5248618784530388, "no_speech_prob": 0.003171429270878434}, {"id": 281, "seek": 170780, "start": 1707.8, "end": 1715.8, "text": " Okay, so user question is going to be a text input.", "tokens": [50364, 1033, 11, 370, 4195, 1168, 307, 516, 281, 312, 257, 2487, 4846, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10643910139034955, "compression_ratio": 1.5797872340425532, "no_speech_prob": 0.025892650708556175}, {"id": 282, "seek": 170780, "start": 1715.8, "end": 1722.8, "text": " And we're going to ask what variable are you interested in?", "tokens": [50764, 400, 321, 434, 516, 281, 1029, 437, 7006, 366, 291, 3102, 294, 30, 51114], "temperature": 0.0, "avg_logprob": -0.10643910139034955, "compression_ratio": 1.5797872340425532, "no_speech_prob": 0.025892650708556175}, {"id": 283, "seek": 170780, "start": 1722.8, "end": 1726.8, "text": " We're going to run and see what happens.", "tokens": [51114, 492, 434, 516, 281, 1190, 293, 536, 437, 2314, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10643910139034955, "compression_ratio": 1.5797872340425532, "no_speech_prob": 0.025892650708556175}, {"id": 284, "seek": 170780, "start": 1726.8, "end": 1736.8, "text": " And as you can see, this is running again and you would agree with me that it would be better if the information stays and it doesn't run again.", "tokens": [51314, 400, 382, 291, 393, 536, 11, 341, 307, 2614, 797, 293, 291, 576, 3986, 365, 385, 300, 309, 576, 312, 1101, 498, 264, 1589, 10834, 293, 309, 1177, 380, 1190, 797, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10643910139034955, "compression_ratio": 1.5797872340425532, "no_speech_prob": 0.025892650708556175}, {"id": 285, "seek": 173680, "start": 1737.8, "end": 1740.8, "text": " Okay, so now what variable are you interested in?", "tokens": [50414, 1033, 11, 370, 586, 437, 7006, 366, 291, 3102, 294, 30, 50564], "temperature": 0.0, "avg_logprob": -0.15665031881893382, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.005219624377787113}, {"id": 286, "seek": 173680, "start": 1740.8, "end": 1750.8, "text": " Well, we haven't done anything with this, but let's say volume, enter, and again, everything runs again.", "tokens": [50564, 1042, 11, 321, 2378, 380, 1096, 1340, 365, 341, 11, 457, 718, 311, 584, 5523, 11, 3242, 11, 293, 797, 11, 1203, 6676, 797, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15665031881893382, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.005219624377787113}, {"id": 287, "seek": 173680, "start": 1750.8, "end": 1753.8, "text": " So how do we fix this?", "tokens": [51064, 407, 577, 360, 321, 3191, 341, 30, 51214], "temperature": 0.0, "avg_logprob": -0.15665031881893382, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.005219624377787113}, {"id": 288, "seek": 173680, "start": 1753.8, "end": 1765.8, "text": " So every single time we are entering something, it doesn't run again.", "tokens": [51214, 407, 633, 2167, 565, 321, 366, 11104, 746, 11, 309, 1177, 380, 1190, 797, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15665031881893382, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.005219624377787113}, {"id": 289, "seek": 176580, "start": 1765.8, "end": 1768.8, "text": " So we can sort this issue using caching in Streamlick.", "tokens": [50364, 407, 321, 393, 1333, 341, 2734, 1228, 269, 2834, 294, 24904, 75, 618, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08468415690403358, "compression_ratio": 1.72, "no_speech_prob": 0.011499893851578236}, {"id": 290, "seek": 176580, "start": 1768.8, "end": 1775.8, "text": " So caching is an important feature that allows you to store and reuse the results of computationally expensive functions,", "tokens": [50514, 407, 269, 2834, 307, 364, 1021, 4111, 300, 4045, 291, 281, 3531, 293, 26225, 264, 3542, 295, 24903, 379, 5124, 6828, 11, 50864], "temperature": 0.0, "avg_logprob": -0.08468415690403358, "compression_ratio": 1.72, "no_speech_prob": 0.011499893851578236}, {"id": 291, "seek": 176580, "start": 1775.8, "end": 1780.8, "text": " which improves the performance and responsiveness of our AI assistant.", "tokens": [50864, 597, 24771, 264, 3389, 293, 2914, 8477, 295, 527, 7318, 10994, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08468415690403358, "compression_ratio": 1.72, "no_speech_prob": 0.011499893851578236}, {"id": 292, "seek": 176580, "start": 1780.8, "end": 1787.8, "text": " So we're going to use the cache decorator to make sure that the functions are not run and run again.", "tokens": [51114, 407, 321, 434, 516, 281, 764, 264, 19459, 7919, 1639, 281, 652, 988, 300, 264, 6828, 366, 406, 1190, 293, 1190, 797, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08468415690403358, "compression_ratio": 1.72, "no_speech_prob": 0.011499893851578236}, {"id": 293, "seek": 176580, "start": 1787.8, "end": 1790.8, "text": " And we avoid the app to run everything again.", "tokens": [51464, 400, 321, 5042, 264, 724, 281, 1190, 1203, 797, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08468415690403358, "compression_ratio": 1.72, "no_speech_prob": 0.011499893851578236}, {"id": 294, "seek": 176580, "start": 1790.8, "end": 1793.8, "text": " So one thing appears after another.", "tokens": [51614, 407, 472, 551, 7038, 934, 1071, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08468415690403358, "compression_ratio": 1.72, "no_speech_prob": 0.011499893851578236}, {"id": 295, "seek": 179380, "start": 1793.8, "end": 1796.8, "text": " So let me show you how to do this.", "tokens": [50364, 407, 718, 385, 855, 291, 577, 281, 360, 341, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11764831253976533, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.0156504325568676}, {"id": 296, "seek": 179380, "start": 1796.8, "end": 1803.8, "text": " And we're going to create a couple more functions and organize our script a little bit more.", "tokens": [50514, 400, 321, 434, 516, 281, 1884, 257, 1916, 544, 6828, 293, 13859, 527, 5755, 257, 707, 857, 544, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11764831253976533, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.0156504325568676}, {"id": 297, "seek": 179380, "start": 1803.8, "end": 1819.8, "text": " So first of all, what we're going to do is we are going to move our function agent and our pandas agent earlier in the script.", "tokens": [50864, 407, 700, 295, 439, 11, 437, 321, 434, 516, 281, 360, 307, 321, 366, 516, 281, 1286, 527, 2445, 9461, 293, 527, 4565, 296, 9461, 3071, 294, 264, 5755, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11764831253976533, "compression_ratio": 1.5974842767295598, "no_speech_prob": 0.0156504325568676}, {"id": 298, "seek": 182380, "start": 1823.8, "end": 1834.8, "text": " So there after the element model, so what we're going to do is to add functions of the main script.", "tokens": [50364, 407, 456, 934, 264, 4478, 2316, 11, 370, 437, 321, 434, 516, 281, 360, 307, 281, 909, 6828, 295, 264, 2135, 5755, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13817324204878373, "compression_ratio": 1.5407407407407407, "no_speech_prob": 0.014272613450884819}, {"id": 299, "seek": 182380, "start": 1834.8, "end": 1844.8, "text": " So here we're going to add this function agent and then I'm going to move the pandas agent early on as well.", "tokens": [50914, 407, 510, 321, 434, 516, 281, 909, 341, 2445, 9461, 293, 550, 286, 478, 516, 281, 1286, 264, 4565, 296, 9461, 2440, 322, 382, 731, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13817324204878373, "compression_ratio": 1.5407407407407407, "no_speech_prob": 0.014272613450884819}, {"id": 300, "seek": 184480, "start": 1844.8, "end": 1850.8, "text": " So we are importing the libraries.", "tokens": [50364, 407, 321, 366, 43866, 264, 15148, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12008339062071684, "compression_ratio": 1.5905511811023623, "no_speech_prob": 0.043953754007816315}, {"id": 301, "seek": 184480, "start": 1850.8, "end": 1854.8, "text": " We are setting the open AI key.", "tokens": [50664, 492, 366, 3287, 264, 1269, 7318, 2141, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12008339062071684, "compression_ratio": 1.5905511811023623, "no_speech_prob": 0.043953754007816315}, {"id": 302, "seek": 184480, "start": 1854.8, "end": 1865.8, "text": " And then what we're going to do is we're going to put there after the open AI key,", "tokens": [50864, 400, 550, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 829, 456, 934, 264, 1269, 7318, 2141, 11, 51414], "temperature": 0.0, "avg_logprob": -0.12008339062071684, "compression_ratio": 1.5905511811023623, "no_speech_prob": 0.043953754007816315}, {"id": 303, "seek": 184480, "start": 1865.8, "end": 1871.8, "text": " we are going to set the title and welcoming message.", "tokens": [51414, 321, 366, 516, 281, 992, 264, 4876, 293, 17378, 3636, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12008339062071684, "compression_ratio": 1.5905511811023623, "no_speech_prob": 0.043953754007816315}, {"id": 304, "seek": 187180, "start": 1871.8, "end": 1880.8, "text": " And so let's write here title and then welcoming message.", "tokens": [50364, 400, 370, 718, 311, 2464, 510, 4876, 293, 550, 17378, 3636, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15337722084738992, "compression_ratio": 1.3722627737226278, "no_speech_prob": 0.016147958114743233}, {"id": 305, "seek": 187180, "start": 1880.8, "end": 1890.8, "text": " Okay. And after we've done that, we are going to put here the explanation site bar.", "tokens": [50814, 1033, 13, 400, 934, 321, 600, 1096, 300, 11, 321, 366, 516, 281, 829, 510, 264, 10835, 3621, 2159, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15337722084738992, "compression_ratio": 1.3722627737226278, "no_speech_prob": 0.016147958114743233}, {"id": 306, "seek": 187180, "start": 1890.8, "end": 1894.8, "text": " So let's grab that.", "tokens": [51314, 407, 718, 311, 4444, 300, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15337722084738992, "compression_ratio": 1.3722627737226278, "no_speech_prob": 0.016147958114743233}, {"id": 307, "seek": 187180, "start": 1894.8, "end": 1897.8, "text": " So this is this part here.", "tokens": [51514, 407, 341, 307, 341, 644, 510, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15337722084738992, "compression_ratio": 1.3722627737226278, "no_speech_prob": 0.016147958114743233}, {"id": 308, "seek": 189780, "start": 1897.8, "end": 1907.8, "text": " So we're just going to move that just necessary so things work fine and what we cash the functions.", "tokens": [50364, 407, 321, 434, 445, 516, 281, 1286, 300, 445, 4818, 370, 721, 589, 2489, 293, 437, 321, 6388, 264, 6828, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1367574691772461, "compression_ratio": 1.4966887417218544, "no_speech_prob": 0.004447667393833399}, {"id": 309, "seek": 189780, "start": 1907.8, "end": 1917.8, "text": " So yes, as I said, here is where the explanation site bar is going to be.", "tokens": [50864, 407, 2086, 11, 382, 286, 848, 11, 510, 307, 689, 264, 10835, 3621, 2159, 307, 516, 281, 312, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1367574691772461, "compression_ratio": 1.4966887417218544, "no_speech_prob": 0.004447667393833399}, {"id": 310, "seek": 189780, "start": 1917.8, "end": 1925.8, "text": " And then we are going to move the button after this.", "tokens": [51364, 400, 550, 321, 366, 516, 281, 1286, 264, 2960, 934, 341, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1367574691772461, "compression_ratio": 1.4966887417218544, "no_speech_prob": 0.004447667393833399}, {"id": 311, "seek": 192580, "start": 1925.8, "end": 1934.8, "text": " So this is all about the button that we really don't want.", "tokens": [50364, 407, 341, 307, 439, 466, 264, 2960, 300, 321, 534, 500, 380, 528, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12733433986532278, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.02126775123178959}, {"id": 312, "seek": 192580, "start": 1934.8, "end": 1938.8, "text": " So we don't want the header and this header there.", "tokens": [50814, 407, 321, 500, 380, 528, 264, 23117, 293, 341, 23117, 456, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12733433986532278, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.02126775123178959}, {"id": 313, "seek": 192580, "start": 1938.8, "end": 1942.8, "text": " We will use it later for the sections that we are created.", "tokens": [51014, 492, 486, 764, 309, 1780, 337, 264, 10863, 300, 321, 366, 2942, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12733433986532278, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.02126775123178959}, {"id": 314, "seek": 192580, "start": 1942.8, "end": 1947.8, "text": " So we are going to move all this.", "tokens": [51214, 407, 321, 366, 516, 281, 1286, 439, 341, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12733433986532278, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.02126775123178959}, {"id": 315, "seek": 194780, "start": 1948.8, "end": 1955.8, "text": " Yes, after the sidebar.", "tokens": [50414, 1079, 11, 934, 264, 1252, 5356, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1782829761505127, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.03203330561518669}, {"id": 316, "seek": 194780, "start": 1955.8, "end": 1962.8, "text": " So yes, we have the explanation sidebar and then we are initializing the key in the session state.", "tokens": [50764, 407, 2086, 11, 321, 362, 264, 10835, 1252, 5356, 293, 550, 321, 366, 5883, 3319, 264, 2141, 294, 264, 5481, 1785, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1782829761505127, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.03203330561518669}, {"id": 317, "seek": 194780, "start": 1962.8, "end": 1966.8, "text": " Adding the button, we are uploading the file.", "tokens": [51114, 31204, 264, 2960, 11, 321, 366, 27301, 264, 3991, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1782829761505127, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.03203330561518669}, {"id": 318, "seek": 194780, "start": 1966.8, "end": 1973.8, "text": " And when we upload the file and we convert it into a data frame,", "tokens": [51314, 400, 562, 321, 6580, 264, 3991, 293, 321, 7620, 309, 666, 257, 1412, 3920, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1782829761505127, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.03203330561518669}, {"id": 319, "seek": 197380, "start": 1973.8, "end": 1975.8, "text": " then is when the game starts, right?", "tokens": [50364, 550, 307, 562, 264, 1216, 3719, 11, 558, 30, 50464], "temperature": 0.0, "avg_logprob": -0.13579295402349428, "compression_ratio": 1.35, "no_speech_prob": 0.13447022438049316}, {"id": 320, "seek": 197380, "start": 1975.8, "end": 1981.8, "text": " Then is when everything is built and generated.", "tokens": [50464, 1396, 307, 562, 1203, 307, 3094, 293, 10833, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13579295402349428, "compression_ratio": 1.35, "no_speech_prob": 0.13447022438049316}, {"id": 321, "seek": 197380, "start": 1981.8, "end": 1994.8, "text": " So that means that we are actually going to move this in to this F statement.", "tokens": [50764, 407, 300, 1355, 300, 321, 366, 767, 516, 281, 1286, 341, 294, 281, 341, 479, 5629, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13579295402349428, "compression_ratio": 1.35, "no_speech_prob": 0.13447022438049316}, {"id": 322, "seek": 199480, "start": 1994.8, "end": 2007.8, "text": " And then or pandas agent is going to go here and the same for the functions.", "tokens": [50364, 400, 550, 420, 4565, 296, 9461, 307, 516, 281, 352, 510, 293, 264, 912, 337, 264, 6828, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14271598751262082, "compression_ratio": 1.5220588235294117, "no_speech_prob": 0.3839282691478729}, {"id": 323, "seek": 199480, "start": 2007.8, "end": 2012.8, "text": " And then we'll move this as well.", "tokens": [51014, 400, 550, 321, 603, 1286, 341, 382, 731, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14271598751262082, "compression_ratio": 1.5220588235294117, "no_speech_prob": 0.3839282691478729}, {"id": 324, "seek": 199480, "start": 2012.8, "end": 2014.8, "text": " So we have the pandas agent there.", "tokens": [51264, 407, 321, 362, 264, 4565, 296, 9461, 456, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14271598751262082, "compression_ratio": 1.5220588235294117, "no_speech_prob": 0.3839282691478729}, {"id": 325, "seek": 199480, "start": 2014.8, "end": 2018.8, "text": " This is something that we already have so we can remove this.", "tokens": [51364, 639, 307, 746, 300, 321, 1217, 362, 370, 321, 393, 4159, 341, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14271598751262082, "compression_ratio": 1.5220588235294117, "no_speech_prob": 0.3839282691478729}, {"id": 326, "seek": 201880, "start": 2019.8, "end": 2027.8, "text": " And first function that I'm going to create is after initializing the LLM model,", "tokens": [50414, 400, 700, 2445, 300, 286, 478, 516, 281, 1884, 307, 934, 5883, 3319, 264, 441, 43, 44, 2316, 11, 50814], "temperature": 0.0, "avg_logprob": -0.19047977030277252, "compression_ratio": 1.5608108108108107, "no_speech_prob": 0.12382052838802338}, {"id": 327, "seek": 201880, "start": 2027.8, "end": 2033.8, "text": " we are going to create a function for creating the steps of the EDA.", "tokens": [50814, 321, 366, 516, 281, 1884, 257, 2445, 337, 4084, 264, 4439, 295, 264, 462, 7509, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19047977030277252, "compression_ratio": 1.5608108108108107, "no_speech_prob": 0.12382052838802338}, {"id": 328, "seek": 201880, "start": 2033.8, "end": 2045.8, "text": " So that is going to be steps A and then here is just going to be this bit, right?", "tokens": [51114, 407, 300, 307, 516, 281, 312, 4439, 316, 293, 550, 510, 307, 445, 516, 281, 312, 341, 857, 11, 558, 30, 51714], "temperature": 0.0, "avg_logprob": -0.19047977030277252, "compression_ratio": 1.5608108108108107, "no_speech_prob": 0.12382052838802338}, {"id": 329, "seek": 204580, "start": 2046.8, "end": 2051.8, "text": " So the LLM is going to generate that and that is not going to run again", "tokens": [50414, 407, 264, 441, 43, 44, 307, 516, 281, 8460, 300, 293, 300, 307, 406, 516, 281, 1190, 797, 50664], "temperature": 0.0, "avg_logprob": -0.0951461683620106, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.00831510592252016}, {"id": 330, "seek": 204580, "start": 2051.8, "end": 2055.8, "text": " because we're going to decorate that with the cache data decorator.", "tokens": [50664, 570, 321, 434, 516, 281, 24229, 300, 365, 264, 19459, 1412, 7919, 1639, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0951461683620106, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.00831510592252016}, {"id": 331, "seek": 204580, "start": 2055.8, "end": 2057.8, "text": " So let me show you how to do that.", "tokens": [50864, 407, 718, 385, 855, 291, 577, 281, 360, 300, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0951461683620106, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.00831510592252016}, {"id": 332, "seek": 204580, "start": 2057.8, "end": 2066.8, "text": " So here we are going to add this and this is going to be called steps EDA.", "tokens": [50964, 407, 510, 321, 366, 516, 281, 909, 341, 293, 341, 307, 516, 281, 312, 1219, 4439, 462, 7509, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0951461683620106, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.00831510592252016}, {"id": 333, "seek": 204580, "start": 2066.8, "end": 2070.8, "text": " And then that is what we are going to return.", "tokens": [51414, 400, 550, 300, 307, 437, 321, 366, 516, 281, 2736, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0951461683620106, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.00831510592252016}, {"id": 334, "seek": 204580, "start": 2070.8, "end": 2071.8, "text": " Excellent.", "tokens": [51614, 16723, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0951461683620106, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.00831510592252016}, {"id": 335, "seek": 207180, "start": 2071.8, "end": 2076.8, "text": " So this will be computed and that's it and will be computed again.", "tokens": [50364, 407, 341, 486, 312, 40610, 293, 300, 311, 309, 293, 486, 312, 40610, 797, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1555802749864983, "compression_ratio": 1.6875, "no_speech_prob": 0.010326830670237541}, {"id": 336, "seek": 207180, "start": 2076.8, "end": 2077.8, "text": " Amazing.", "tokens": [50614, 14165, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1555802749864983, "compression_ratio": 1.6875, "no_speech_prob": 0.010326830670237541}, {"id": 337, "seek": 207180, "start": 2077.8, "end": 2084.8, "text": " Then we have the pandas agent and then the function that is involving the pandas agent.", "tokens": [50664, 1396, 321, 362, 264, 4565, 296, 9461, 293, 550, 264, 2445, 300, 307, 17030, 264, 4565, 296, 9461, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1555802749864983, "compression_ratio": 1.6875, "no_speech_prob": 0.010326830670237541}, {"id": 338, "seek": 207180, "start": 2084.8, "end": 2094.8, "text": " So let's put here function sidebar and then this is a function main.", "tokens": [51014, 407, 718, 311, 829, 510, 2445, 1252, 5356, 293, 550, 341, 307, 257, 2445, 2135, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1555802749864983, "compression_ratio": 1.6875, "no_speech_prob": 0.010326830670237541}, {"id": 339, "seek": 207180, "start": 2094.8, "end": 2095.8, "text": " Excellent.", "tokens": [51514, 16723, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1555802749864983, "compression_ratio": 1.6875, "no_speech_prob": 0.010326830670237541}, {"id": 340, "seek": 209580, "start": 2096.8, "end": 2105.8, "text": " So all this is running and then it's time to add a header.", "tokens": [50414, 407, 439, 341, 307, 2614, 293, 550, 309, 311, 565, 281, 909, 257, 23117, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18367205274865983, "compression_ratio": 1.3306451612903225, "no_speech_prob": 0.04141683131456375}, {"id": 341, "seek": 209580, "start": 2110.8, "end": 2115.8, "text": " So first thing that we're doing is exploratory data analysis, right?", "tokens": [51114, 407, 700, 551, 300, 321, 434, 884, 307, 24765, 4745, 1412, 5215, 11, 558, 30, 51364], "temperature": 0.0, "avg_logprob": -0.18367205274865983, "compression_ratio": 1.3306451612903225, "no_speech_prob": 0.04141683131456375}, {"id": 342, "seek": 209580, "start": 2115.8, "end": 2117.8, "text": " So that will be a really good header.", "tokens": [51364, 407, 300, 486, 312, 257, 534, 665, 23117, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18367205274865983, "compression_ratio": 1.3306451612903225, "no_speech_prob": 0.04141683131456375}, {"id": 343, "seek": 211780, "start": 2118.8, "end": 2129.8, "text": " And then we're going to move this sidebar that involves the LLMs there.", "tokens": [50414, 400, 550, 321, 434, 516, 281, 1286, 341, 1252, 5356, 300, 11626, 264, 441, 43, 26386, 456, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17000896647825078, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.02479448914527893}, {"id": 344, "seek": 211780, "start": 2129.8, "end": 2134.8, "text": " But now we're just going to write steps EDA.", "tokens": [50964, 583, 586, 321, 434, 445, 516, 281, 2464, 4439, 462, 7509, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17000896647825078, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.02479448914527893}, {"id": 345, "seek": 211780, "start": 2134.8, "end": 2138.8, "text": " So what this function returns.", "tokens": [51214, 407, 437, 341, 2445, 11247, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17000896647825078, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.02479448914527893}, {"id": 346, "seek": 211780, "start": 2138.8, "end": 2139.8, "text": " Great.", "tokens": [51414, 3769, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17000896647825078, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.02479448914527893}, {"id": 347, "seek": 211780, "start": 2139.8, "end": 2144.8, "text": " And then we are running the function agent.", "tokens": [51464, 400, 550, 321, 366, 2614, 264, 2445, 9461, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17000896647825078, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.02479448914527893}, {"id": 348, "seek": 214480, "start": 2144.8, "end": 2151.8, "text": " And then we have the user question.", "tokens": [50364, 400, 550, 321, 362, 264, 4195, 1168, 13, 50714], "temperature": 0.0, "avg_logprob": -0.191636946466234, "compression_ratio": 1.202020202020202, "no_speech_prob": 0.0028426297940313816}, {"id": 349, "seek": 214480, "start": 2151.8, "end": 2157.8, "text": " And let's add a subheader here.", "tokens": [50714, 400, 718, 311, 909, 257, 1422, 1934, 260, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.191636946466234, "compression_ratio": 1.202020202020202, "no_speech_prob": 0.0028426297940313816}, {"id": 350, "seek": 214480, "start": 2157.8, "end": 2166.8, "text": " So let's say general information about the dataset.", "tokens": [51014, 407, 718, 311, 584, 2674, 1589, 466, 264, 28872, 13, 51464], "temperature": 0.0, "avg_logprob": -0.191636946466234, "compression_ratio": 1.202020202020202, "no_speech_prob": 0.0028426297940313816}, {"id": 351, "seek": 216680, "start": 2167.8, "end": 2176.8, "text": " And then here, let's say variable of study.", "tokens": [50414, 400, 550, 510, 11, 718, 311, 584, 7006, 295, 2979, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1507171881003458, "compression_ratio": 1.310077519379845, "no_speech_prob": 0.0034818865824490786}, {"id": 352, "seek": 216680, "start": 2176.8, "end": 2177.8, "text": " Okay.", "tokens": [50864, 1033, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1507171881003458, "compression_ratio": 1.310077519379845, "no_speech_prob": 0.0034818865824490786}, {"id": 353, "seek": 216680, "start": 2177.8, "end": 2180.8, "text": " And finally, we're going to hash the function agent.", "tokens": [50914, 400, 2721, 11, 321, 434, 516, 281, 22019, 264, 2445, 9461, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1507171881003458, "compression_ratio": 1.310077519379845, "no_speech_prob": 0.0034818865824490786}, {"id": 354, "seek": 216680, "start": 2180.8, "end": 2184.8, "text": " So let's do that.", "tokens": [51064, 407, 718, 311, 360, 300, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1507171881003458, "compression_ratio": 1.310077519379845, "no_speech_prob": 0.0034818865824490786}, {"id": 355, "seek": 216680, "start": 2184.8, "end": 2185.8, "text": " Cool.", "tokens": [51264, 8561, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1507171881003458, "compression_ratio": 1.310077519379845, "no_speech_prob": 0.0034818865824490786}, {"id": 356, "seek": 216680, "start": 2185.8, "end": 2190.8, "text": " So let's rerun and see what happens.", "tokens": [51314, 407, 718, 311, 43819, 409, 293, 536, 437, 2314, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1507171881003458, "compression_ratio": 1.310077519379845, "no_speech_prob": 0.0034818865824490786}, {"id": 357, "seek": 216680, "start": 2190.8, "end": 2191.8, "text": " Okay.", "tokens": [51564, 1033, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1507171881003458, "compression_ratio": 1.310077519379845, "no_speech_prob": 0.0034818865824490786}, {"id": 358, "seek": 219180, "start": 2191.8, "end": 2202.8, "text": " So running function agent, this header and the subheader, the function agent is running.", "tokens": [50364, 407, 2614, 2445, 9461, 11, 341, 23117, 293, 264, 1422, 1934, 260, 11, 264, 2445, 9461, 307, 2614, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20470435946595436, "compression_ratio": 1.4191176470588236, "no_speech_prob": 0.0013669206527993083}, {"id": 359, "seek": 219180, "start": 2202.8, "end": 2203.8, "text": " Okay.", "tokens": [50914, 1033, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20470435946595436, "compression_ratio": 1.4191176470588236, "no_speech_prob": 0.0013669206527993083}, {"id": 360, "seek": 219180, "start": 2203.8, "end": 2210.8, "text": " So now hopefully when we write volume here and we press enter, nothing will runs again.", "tokens": [50964, 407, 586, 4696, 562, 321, 2464, 5523, 510, 293, 321, 1886, 3242, 11, 1825, 486, 6676, 797, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20470435946595436, "compression_ratio": 1.4191176470588236, "no_speech_prob": 0.0013669206527993083}, {"id": 361, "seek": 219180, "start": 2210.8, "end": 2211.8, "text": " Brilliant.", "tokens": [51314, 34007, 13, 51364], "temperature": 0.0, "avg_logprob": -0.20470435946595436, "compression_ratio": 1.4191176470588236, "no_speech_prob": 0.0013669206527993083}, {"id": 362, "seek": 221180, "start": 2211.8, "end": 2224.8, "text": " So we have sort out this issue of things running again and again by using caching in streamlight.", "tokens": [50364, 407, 321, 362, 1333, 484, 341, 2734, 295, 721, 2614, 797, 293, 797, 538, 1228, 269, 2834, 294, 4309, 2764, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1283432315377628, "compression_ratio": 1.5227272727272727, "no_speech_prob": 0.016887947916984558}, {"id": 363, "seek": 221180, "start": 2224.8, "end": 2225.8, "text": " Okay.", "tokens": [51014, 1033, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1283432315377628, "compression_ratio": 1.5227272727272727, "no_speech_prob": 0.016887947916984558}, {"id": 364, "seek": 221180, "start": 2225.8, "end": 2229.8, "text": " So coming back to this variable of study, what we're going to do is to create a function", "tokens": [51064, 407, 1348, 646, 281, 341, 7006, 295, 2979, 11, 437, 321, 434, 516, 281, 360, 307, 281, 1884, 257, 2445, 51264], "temperature": 0.0, "avg_logprob": -0.1283432315377628, "compression_ratio": 1.5227272727272727, "no_speech_prob": 0.016887947916984558}, {"id": 365, "seek": 221180, "start": 2229.8, "end": 2234.8, "text": " that does some exploratory data analysis, but is specific to this variable.", "tokens": [51264, 300, 775, 512, 24765, 4745, 1412, 5215, 11, 457, 307, 2685, 281, 341, 7006, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1283432315377628, "compression_ratio": 1.5227272727272727, "no_speech_prob": 0.016887947916984558}, {"id": 366, "seek": 223480, "start": 2234.8, "end": 2242.8, "text": " And I think the first thing that will be interesting to show is a graph of this variable.", "tokens": [50364, 400, 286, 519, 264, 700, 551, 300, 486, 312, 1880, 281, 855, 307, 257, 4295, 295, 341, 7006, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12041229425474655, "compression_ratio": 1.694300518134715, "no_speech_prob": 0.1292201429605484}, {"id": 367, "seek": 223480, "start": 2242.8, "end": 2244.8, "text": " Streamlight has a really good visualization properties.", "tokens": [50764, 24904, 2764, 575, 257, 534, 665, 25801, 7221, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12041229425474655, "compression_ratio": 1.694300518134715, "no_speech_prob": 0.1292201429605484}, {"id": 368, "seek": 223480, "start": 2244.8, "end": 2248.8, "text": " So we're going to make use of that to create a line chart.", "tokens": [50864, 407, 321, 434, 516, 281, 652, 764, 295, 300, 281, 1884, 257, 1622, 6927, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12041229425474655, "compression_ratio": 1.694300518134715, "no_speech_prob": 0.1292201429605484}, {"id": 369, "seek": 223480, "start": 2248.8, "end": 2252.8, "text": " So we are going to add a function there.", "tokens": [51064, 407, 321, 366, 516, 281, 909, 257, 2445, 456, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12041229425474655, "compression_ratio": 1.694300518134715, "no_speech_prob": 0.1292201429605484}, {"id": 370, "seek": 223480, "start": 2252.8, "end": 2260.8, "text": " So first we hash the function and then we're going to call this a function.", "tokens": [51264, 407, 700, 321, 22019, 264, 2445, 293, 550, 321, 434, 516, 281, 818, 341, 257, 2445, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12041229425474655, "compression_ratio": 1.694300518134715, "no_speech_prob": 0.1292201429605484}, {"id": 371, "seek": 223480, "start": 2260.8, "end": 2261.8, "text": " Okay.", "tokens": [51664, 1033, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12041229425474655, "compression_ratio": 1.694300518134715, "no_speech_prob": 0.1292201429605484}, {"id": 372, "seek": 226180, "start": 2261.8, "end": 2266.8, "text": " So first of all, we're going to use the line chart from streamlight.", "tokens": [50364, 407, 700, 295, 439, 11, 321, 434, 516, 281, 764, 264, 1622, 6927, 490, 4309, 2764, 13, 50614], "temperature": 0.0, "avg_logprob": -0.23958324378644916, "compression_ratio": 1.576158940397351, "no_speech_prob": 0.007230239920318127}, {"id": 373, "seek": 226180, "start": 2266.8, "end": 2270.8, "text": " And that is ST dot line chart.", "tokens": [50614, 400, 300, 307, 4904, 5893, 1622, 6927, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23958324378644916, "compression_ratio": 1.576158940397351, "no_speech_prob": 0.007230239920318127}, {"id": 374, "seek": 226180, "start": 2270.8, "end": 2275.8, "text": " What we're going to pass is our data frame and then the user question variables.", "tokens": [50814, 708, 321, 434, 516, 281, 1320, 307, 527, 1412, 3920, 293, 550, 264, 4195, 1168, 9102, 13, 51064], "temperature": 0.0, "avg_logprob": -0.23958324378644916, "compression_ratio": 1.576158940397351, "no_speech_prob": 0.007230239920318127}, {"id": 375, "seek": 226180, "start": 2275.8, "end": 2282.8, "text": " So did I call it a user question?", "tokens": [51064, 407, 630, 286, 818, 309, 257, 4195, 1168, 30, 51414], "temperature": 0.0, "avg_logprob": -0.23958324378644916, "compression_ratio": 1.576158940397351, "no_speech_prob": 0.007230239920318127}, {"id": 376, "seek": 226180, "start": 2282.8, "end": 2283.8, "text": " Okay.", "tokens": [51414, 1033, 13, 51464], "temperature": 0.0, "avg_logprob": -0.23958324378644916, "compression_ratio": 1.576158940397351, "no_speech_prob": 0.007230239920318127}, {"id": 377, "seek": 226180, "start": 2283.8, "end": 2284.8, "text": " So user question.", "tokens": [51464, 407, 4195, 1168, 13, 51514], "temperature": 0.0, "avg_logprob": -0.23958324378644916, "compression_ratio": 1.576158940397351, "no_speech_prob": 0.007230239920318127}, {"id": 378, "seek": 228480, "start": 2284.8, "end": 2285.8, "text": " And variable.", "tokens": [50364, 400, 7006, 13, 50414], "temperature": 0.0, "avg_logprob": -0.33022645803598255, "compression_ratio": 1.2659574468085106, "no_speech_prob": 0.2362116128206253}, {"id": 379, "seek": 228480, "start": 2285.8, "end": 2290.8, "text": " We're going to call it.", "tokens": [50414, 492, 434, 516, 281, 818, 309, 13, 50664], "temperature": 0.0, "avg_logprob": -0.33022645803598255, "compression_ratio": 1.2659574468085106, "no_speech_prob": 0.2362116128206253}, {"id": 380, "seek": 228480, "start": 2290.8, "end": 2298.8, "text": " So if we actually let's do this, if we actually run this, let's see what happens.", "tokens": [50664, 407, 498, 321, 767, 718, 311, 360, 341, 11, 498, 321, 767, 1190, 341, 11, 718, 311, 536, 437, 2314, 13, 51064], "temperature": 0.0, "avg_logprob": -0.33022645803598255, "compression_ratio": 1.2659574468085106, "no_speech_prob": 0.2362116128206253}, {"id": 381, "seek": 229880, "start": 2298.8, "end": 2309.8, "text": " Okay.", "tokens": [50364, 1033, 13, 50914], "temperature": 0.0, "avg_logprob": -0.21034906678280588, "compression_ratio": 1.3308823529411764, "no_speech_prob": 0.006485371384769678}, {"id": 382, "seek": 229880, "start": 2309.8, "end": 2314.8, "text": " I forgot that.", "tokens": [50914, 286, 5298, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.21034906678280588, "compression_ratio": 1.3308823529411764, "no_speech_prob": 0.006485371384769678}, {"id": 383, "seek": 229880, "start": 2314.8, "end": 2315.8, "text": " Okay.", "tokens": [51164, 1033, 13, 51214], "temperature": 0.0, "avg_logprob": -0.21034906678280588, "compression_ratio": 1.3308823529411764, "no_speech_prob": 0.006485371384769678}, {"id": 384, "seek": 229880, "start": 2315.8, "end": 2316.8, "text": " Excellent.", "tokens": [51214, 16723, 13, 51264], "temperature": 0.0, "avg_logprob": -0.21034906678280588, "compression_ratio": 1.3308823529411764, "no_speech_prob": 0.006485371384769678}, {"id": 385, "seek": 229880, "start": 2316.8, "end": 2321.8, "text": " So you can see now I line chart of the variable volume.", "tokens": [51264, 407, 291, 393, 536, 586, 286, 1622, 6927, 295, 264, 7006, 5523, 13, 51514], "temperature": 0.0, "avg_logprob": -0.21034906678280588, "compression_ratio": 1.3308823529411764, "no_speech_prob": 0.006485371384769678}, {"id": 386, "seek": 229880, "start": 2321.8, "end": 2327.8, "text": " So now I'm going to use the pandas agent in the same way I did before to ask a specific", "tokens": [51514, 407, 586, 286, 478, 516, 281, 764, 264, 4565, 296, 9461, 294, 264, 912, 636, 286, 630, 949, 281, 1029, 257, 2685, 51814], "temperature": 0.0, "avg_logprob": -0.21034906678280588, "compression_ratio": 1.3308823529411764, "no_speech_prob": 0.006485371384769678}, {"id": 387, "seek": 232780, "start": 2327.8, "end": 2329.8, "text": " questions about this variable.", "tokens": [50364, 1651, 466, 341, 7006, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12981518946195902, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.012034378945827484}, {"id": 388, "seek": 232780, "start": 2329.8, "end": 2336.8, "text": " But now instead of just writing questions, I write questions and I'll pass the user question", "tokens": [50464, 583, 586, 2602, 295, 445, 3579, 1651, 11, 286, 2464, 1651, 293, 286, 603, 1320, 264, 4195, 1168, 50814], "temperature": 0.0, "avg_logprob": -0.12981518946195902, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.012034378945827484}, {"id": 389, "seek": 232780, "start": 2336.8, "end": 2337.8, "text": " variable.", "tokens": [50814, 7006, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12981518946195902, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.012034378945827484}, {"id": 390, "seek": 232780, "start": 2337.8, "end": 2342.8, "text": " So we can answer specific questions about this specific variable.", "tokens": [50864, 407, 321, 393, 1867, 2685, 1651, 466, 341, 2685, 7006, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12981518946195902, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.012034378945827484}, {"id": 391, "seek": 232780, "start": 2342.8, "end": 2352.8, "text": " So for example, imagine that what we want is to know the summary of the statistics, right,", "tokens": [51114, 407, 337, 1365, 11, 3811, 300, 437, 321, 528, 307, 281, 458, 264, 12691, 295, 264, 12523, 11, 558, 11, 51614], "temperature": 0.0, "avg_logprob": -0.12981518946195902, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.012034378945827484}, {"id": 392, "seek": 232780, "start": 2352.8, "end": 2354.8, "text": " of the user question variables.", "tokens": [51614, 295, 264, 4195, 1168, 9102, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12981518946195902, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.012034378945827484}, {"id": 393, "seek": 235480, "start": 2354.8, "end": 2362.8, "text": " So we're going to ask a summary, summary statistics.", "tokens": [50364, 407, 321, 434, 516, 281, 1029, 257, 12691, 11, 12691, 12523, 13, 50764], "temperature": 0.0, "avg_logprob": -0.25338572547549293, "compression_ratio": 1.3461538461538463, "no_speech_prob": 0.1642516404390335}, {"id": 394, "seek": 235480, "start": 2362.8, "end": 2377.8, "text": " So, so for example, pandas agent dot run, and it's going to be gave me a summary of the", "tokens": [50764, 407, 11, 370, 337, 1365, 11, 4565, 296, 9461, 5893, 1190, 11, 293, 309, 311, 516, 281, 312, 2729, 385, 257, 12691, 295, 264, 51514], "temperature": 0.0, "avg_logprob": -0.25338572547549293, "compression_ratio": 1.3461538461538463, "no_speech_prob": 0.1642516404390335}, {"id": 395, "seek": 237780, "start": 2378.8, "end": 2380.8, "text": " statistics off.", "tokens": [50414, 12523, 766, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2192957694070381, "compression_ratio": 1.5594405594405594, "no_speech_prob": 0.14793825149536133}, {"id": 396, "seek": 237780, "start": 2380.8, "end": 2387.8, "text": " And then we have the user question variable.", "tokens": [50514, 400, 550, 321, 362, 264, 4195, 1168, 7006, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2192957694070381, "compression_ratio": 1.5594405594405594, "no_speech_prob": 0.14793825149536133}, {"id": 397, "seek": 237780, "start": 2387.8, "end": 2395.8, "text": " And then we're going to use ST dot write to write this summary statistics.", "tokens": [50864, 400, 550, 321, 434, 516, 281, 764, 4904, 5893, 2464, 281, 2464, 341, 12691, 12523, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2192957694070381, "compression_ratio": 1.5594405594405594, "no_speech_prob": 0.14793825149536133}, {"id": 398, "seek": 237780, "start": 2395.8, "end": 2402.8, "text": " And we're going to add older things, like for example, checking for normality, checking", "tokens": [51264, 400, 321, 434, 516, 281, 909, 4906, 721, 11, 411, 337, 1365, 11, 8568, 337, 2026, 1860, 11, 8568, 51614], "temperature": 0.0, "avg_logprob": -0.2192957694070381, "compression_ratio": 1.5594405594405594, "no_speech_prob": 0.14793825149536133}, {"id": 399, "seek": 240280, "start": 2402.8, "end": 2415.8, "text": " for liars, checking for trends and add all this to our function.", "tokens": [50364, 337, 375, 685, 11, 8568, 337, 13892, 293, 909, 439, 341, 281, 527, 2445, 13, 51014], "temperature": 0.0, "avg_logprob": -0.20523363835102804, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.0043976930901408195}, {"id": 400, "seek": 240280, "start": 2415.8, "end": 2416.8, "text": " Okay.", "tokens": [51014, 1033, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20523363835102804, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.0043976930901408195}, {"id": 401, "seek": 240280, "start": 2416.8, "end": 2423.8, "text": " So let's rerun and let's see what happens.", "tokens": [51064, 407, 718, 311, 43819, 409, 293, 718, 311, 536, 437, 2314, 13, 51414], "temperature": 0.0, "avg_logprob": -0.20523363835102804, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.0043976930901408195}, {"id": 402, "seek": 240280, "start": 2423.8, "end": 2424.8, "text": " Okay.", "tokens": [51414, 1033, 13, 51464], "temperature": 0.0, "avg_logprob": -0.20523363835102804, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.0043976930901408195}, {"id": 403, "seek": 240280, "start": 2424.8, "end": 2425.8, "text": " So this has changed.", "tokens": [51464, 407, 341, 575, 3105, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20523363835102804, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.0043976930901408195}, {"id": 404, "seek": 240280, "start": 2425.8, "end": 2426.8, "text": " So it's running again.", "tokens": [51514, 407, 309, 311, 2614, 797, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20523363835102804, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.0043976930901408195}, {"id": 405, "seek": 240280, "start": 2426.8, "end": 2431.8, "text": " And great is given a summary of statistics of this variable is saying that the variable", "tokens": [51564, 400, 869, 307, 2212, 257, 12691, 295, 12523, 295, 341, 7006, 307, 1566, 300, 264, 7006, 51814], "temperature": 0.0, "avg_logprob": -0.20523363835102804, "compression_ratio": 1.5120481927710843, "no_speech_prob": 0.0043976930901408195}, {"id": 406, "seek": 243180, "start": 2431.8, "end": 2434.8, "text": " is not normal, but is skew to the right.", "tokens": [50364, 307, 406, 2710, 11, 457, 307, 8756, 86, 281, 264, 558, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12833809852600098, "compression_ratio": 1.7104072398190044, "no_speech_prob": 0.008055554702877998}, {"id": 407, "seek": 243180, "start": 2434.8, "end": 2437.8, "text": " And there is an outlier present.", "tokens": [50514, 400, 456, 307, 364, 484, 2753, 1974, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12833809852600098, "compression_ratio": 1.7104072398190044, "no_speech_prob": 0.008055554702877998}, {"id": 408, "seek": 243180, "start": 2437.8, "end": 2438.8, "text": " Great.", "tokens": [50664, 3769, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12833809852600098, "compression_ratio": 1.7104072398190044, "no_speech_prob": 0.008055554702877998}, {"id": 409, "seek": 243180, "start": 2438.8, "end": 2441.8, "text": " And there is a cyclic pattern of increasing volume in the summer months.", "tokens": [50714, 400, 456, 307, 257, 38154, 1050, 5102, 295, 5662, 5523, 294, 264, 4266, 2493, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12833809852600098, "compression_ratio": 1.7104072398190044, "no_speech_prob": 0.008055554702877998}, {"id": 410, "seek": 243180, "start": 2441.8, "end": 2447.8, "text": " And there are five missing values of volume in order to avoid issues in case that user", "tokens": [50864, 400, 456, 366, 1732, 5361, 4190, 295, 5523, 294, 1668, 281, 5042, 2663, 294, 1389, 300, 4195, 51164], "temperature": 0.0, "avg_logprob": -0.12833809852600098, "compression_ratio": 1.7104072398190044, "no_speech_prob": 0.008055554702877998}, {"id": 411, "seek": 243180, "start": 2447.8, "end": 2453.8, "text": " enters an empty string, and all there is no response.", "tokens": [51164, 18780, 364, 6707, 6798, 11, 293, 439, 456, 307, 572, 4134, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12833809852600098, "compression_ratio": 1.7104072398190044, "no_speech_prob": 0.008055554702877998}, {"id": 412, "seek": 243180, "start": 2453.8, "end": 2457.8, "text": " And then we continue writing things because that would definitely create a problem.", "tokens": [51464, 400, 550, 321, 2354, 3579, 721, 570, 300, 576, 2138, 1884, 257, 1154, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12833809852600098, "compression_ratio": 1.7104072398190044, "no_speech_prob": 0.008055554702877998}, {"id": 413, "seek": 245780, "start": 2457.8, "end": 2463.8, "text": " What we're going to do is to write a couple of lines of code to sort that issue.", "tokens": [50364, 708, 321, 434, 516, 281, 360, 307, 281, 2464, 257, 1916, 295, 3876, 295, 3089, 281, 1333, 300, 2734, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15202313038840223, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.09881485253572464}, {"id": 414, "seek": 245780, "start": 2463.8, "end": 2471.8, "text": " So we're going to say that the user question variable is not non and also if it's different", "tokens": [50664, 407, 321, 434, 516, 281, 584, 300, 264, 4195, 1168, 7006, 307, 406, 2107, 293, 611, 498, 309, 311, 819, 51064], "temperature": 0.0, "avg_logprob": -0.15202313038840223, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.09881485253572464}, {"id": 415, "seek": 245780, "start": 2471.8, "end": 2477.8, "text": " that an empty string, then in that case is when we're going to run or function.", "tokens": [51064, 300, 364, 6707, 6798, 11, 550, 294, 300, 1389, 307, 562, 321, 434, 516, 281, 1190, 420, 2445, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15202313038840223, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.09881485253572464}, {"id": 416, "seek": 247780, "start": 2478.8, "end": 2483.8, "text": " And then we will continue right in a or code after that.", "tokens": [50414, 400, 550, 321, 486, 2354, 558, 294, 257, 420, 3089, 934, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2665375641414097, "compression_ratio": 1.390625, "no_speech_prob": 0.18223990499973297}, {"id": 417, "seek": 247780, "start": 2483.8, "end": 2494.8, "text": " So it's not non.", "tokens": [50664, 407, 309, 311, 406, 2107, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2665375641414097, "compression_ratio": 1.390625, "no_speech_prob": 0.18223990499973297}, {"id": 418, "seek": 247780, "start": 2494.8, "end": 2495.8, "text": " All right.", "tokens": [51214, 1057, 558, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2665375641414097, "compression_ratio": 1.390625, "no_speech_prob": 0.18223990499973297}, {"id": 419, "seek": 247780, "start": 2495.8, "end": 2502.8, "text": " And then for the next section, we're going to add a header that is going to be further", "tokens": [51264, 400, 550, 337, 264, 958, 3541, 11, 321, 434, 516, 281, 909, 257, 23117, 300, 307, 516, 281, 312, 3052, 51614], "temperature": 0.0, "avg_logprob": -0.2665375641414097, "compression_ratio": 1.390625, "no_speech_prob": 0.18223990499973297}, {"id": 420, "seek": 247780, "start": 2502.8, "end": 2505.8, "text": " study.", "tokens": [51614, 2979, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2665375641414097, "compression_ratio": 1.390625, "no_speech_prob": 0.18223990499973297}, {"id": 421, "seek": 250580, "start": 2505.8, "end": 2512.8, "text": " So imagine now that the user wants to add specific questions that are not pretty fine", "tokens": [50364, 407, 3811, 586, 300, 264, 4195, 2738, 281, 909, 2685, 1651, 300, 366, 406, 1238, 2489, 50714], "temperature": 0.0, "avg_logprob": -0.137107446160115, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005271073896437883}, {"id": 422, "seek": 250580, "start": 2512.8, "end": 2518.8, "text": " in our app.", "tokens": [50714, 294, 527, 724, 13, 51014], "temperature": 0.0, "avg_logprob": -0.137107446160115, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005271073896437883}, {"id": 423, "seek": 250580, "start": 2518.8, "end": 2524.8, "text": " So what we can do now is to create another text input in where the user can enter whatever", "tokens": [51014, 407, 437, 321, 393, 360, 586, 307, 281, 1884, 1071, 2487, 4846, 294, 689, 264, 4195, 393, 3242, 2035, 51314], "temperature": 0.0, "avg_logprob": -0.137107446160115, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005271073896437883}, {"id": 424, "seek": 250580, "start": 2524.8, "end": 2525.8, "text": " question they want.", "tokens": [51314, 1168, 436, 528, 13, 51364], "temperature": 0.0, "avg_logprob": -0.137107446160115, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005271073896437883}, {"id": 425, "seek": 250580, "start": 2525.8, "end": 2530.8, "text": " And we're going to use this pandas agent to answer this specific question.", "tokens": [51364, 400, 321, 434, 516, 281, 764, 341, 4565, 296, 9461, 281, 1867, 341, 2685, 1168, 13, 51614], "temperature": 0.0, "avg_logprob": -0.137107446160115, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.0005271073896437883}, {"id": 426, "seek": 253080, "start": 2530.8, "end": 2538.8, "text": " So so first of all, if the question variable exists, then we're going to say user question", "tokens": [50364, 407, 370, 700, 295, 439, 11, 498, 264, 1168, 7006, 8198, 11, 550, 321, 434, 516, 281, 584, 4195, 1168, 50764], "temperature": 0.0, "avg_logprob": -0.09728020290995754, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.01880643330514431}, {"id": 427, "seek": 253080, "start": 2538.8, "end": 2543.8, "text": " data frame.", "tokens": [50764, 1412, 3920, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09728020290995754, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.01880643330514431}, {"id": 428, "seek": 253080, "start": 2543.8, "end": 2546.8, "text": " So we're going to say here, is there anything else that you would like to know about the", "tokens": [51014, 407, 321, 434, 516, 281, 584, 510, 11, 307, 456, 1340, 1646, 300, 291, 576, 411, 281, 458, 466, 264, 51164], "temperature": 0.0, "avg_logprob": -0.09728020290995754, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.01880643330514431}, {"id": 429, "seek": 253080, "start": 2546.8, "end": 2550.8, "text": " data frame?", "tokens": [51164, 1412, 3920, 30, 51364], "temperature": 0.0, "avg_logprob": -0.09728020290995754, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.01880643330514431}, {"id": 430, "seek": 253080, "start": 2550.8, "end": 2554.8, "text": " Okay, so that was our user question data frame.", "tokens": [51364, 1033, 11, 370, 300, 390, 527, 4195, 1168, 1412, 3920, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09728020290995754, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.01880643330514431}, {"id": 431, "seek": 253080, "start": 2554.8, "end": 2559.8, "text": " And now again, if I'm just going to copy and paste this.", "tokens": [51564, 400, 586, 797, 11, 498, 286, 478, 445, 516, 281, 5055, 293, 9163, 341, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09728020290995754, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.01880643330514431}, {"id": 432, "seek": 255980, "start": 2559.8, "end": 2573.8, "text": " So if the question is not known, and also if it's not empty, and also actually not in", "tokens": [50364, 407, 498, 264, 1168, 307, 406, 2570, 11, 293, 611, 498, 309, 311, 406, 6707, 11, 293, 611, 767, 406, 294, 51064], "temperature": 0.0, "avg_logprob": -0.1367629881828062, "compression_ratio": 1.5103448275862068, "no_speech_prob": 0.00897769071161747}, {"id": 433, "seek": 255980, "start": 2573.8, "end": 2581.8, "text": " empty string, or if they say, okay, I actually don't want to know anything else, I'm very", "tokens": [51064, 6707, 6798, 11, 420, 498, 436, 584, 11, 1392, 11, 286, 767, 500, 380, 528, 281, 458, 1340, 1646, 11, 286, 478, 588, 51464], "temperature": 0.0, "avg_logprob": -0.1367629881828062, "compression_ratio": 1.5103448275862068, "no_speech_prob": 0.00897769071161747}, {"id": 434, "seek": 255980, "start": 2581.8, "end": 2584.8, "text": " happy with the information I've had so far.", "tokens": [51464, 2055, 365, 264, 1589, 286, 600, 632, 370, 1400, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1367629881828062, "compression_ratio": 1.5103448275862068, "no_speech_prob": 0.00897769071161747}, {"id": 435, "seek": 258480, "start": 2585.8, "end": 2588.8, "text": " Imagine that that happens.", "tokens": [50414, 11739, 300, 300, 2314, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09061284237597363, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.10793603211641312}, {"id": 436, "seek": 258480, "start": 2588.8, "end": 2595.8, "text": " Then what we're going to do is we're going to run a function that we're going to create", "tokens": [50564, 1396, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1190, 257, 2445, 300, 321, 434, 516, 281, 1884, 50914], "temperature": 0.0, "avg_logprob": -0.09061284237597363, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.10793603211641312}, {"id": 437, "seek": 258480, "start": 2595.8, "end": 2596.8, "text": " in a second.", "tokens": [50914, 294, 257, 1150, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09061284237597363, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.10793603211641312}, {"id": 438, "seek": 258480, "start": 2596.8, "end": 2603.8, "text": " So this function is going to be called a function question and data frame.", "tokens": [50964, 407, 341, 2445, 307, 516, 281, 312, 1219, 257, 2445, 1168, 293, 1412, 3920, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09061284237597363, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.10793603211641312}, {"id": 439, "seek": 258480, "start": 2603.8, "end": 2612.8, "text": " And in this function, what we're going to do is to ask the pandas agent to answer this", "tokens": [51314, 400, 294, 341, 2445, 11, 437, 321, 434, 516, 281, 360, 307, 281, 1029, 264, 4565, 296, 9461, 281, 1867, 341, 51764], "temperature": 0.0, "avg_logprob": -0.09061284237597363, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.10793603211641312}, {"id": 440, "seek": 258480, "start": 2612.8, "end": 2613.8, "text": " specific question.", "tokens": [51764, 2685, 1168, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09061284237597363, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.10793603211641312}, {"id": 441, "seek": 261380, "start": 2613.8, "end": 2626.8, "text": " So again, and then we're going to create this function called function question.", "tokens": [50364, 407, 797, 11, 293, 550, 321, 434, 516, 281, 1884, 341, 2445, 1219, 2445, 1168, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11790030177046613, "compression_ratio": 1.4083333333333334, "no_speech_prob": 0.0022869533859193325}, {"id": 442, "seek": 261380, "start": 2626.8, "end": 2639.8, "text": " Okay, so what is going to happen here is this extra information is going to be stored in", "tokens": [51014, 1033, 11, 370, 437, 307, 516, 281, 1051, 510, 307, 341, 2857, 1589, 307, 516, 281, 312, 12187, 294, 51664], "temperature": 0.0, "avg_logprob": -0.11790030177046613, "compression_ratio": 1.4083333333333334, "no_speech_prob": 0.0022869533859193325}, {"id": 443, "seek": 263980, "start": 2639.8, "end": 2640.8, "text": " this variable.", "tokens": [50364, 341, 7006, 13, 50414], "temperature": 0.0, "avg_logprob": -0.17090285164969307, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.10644328594207764}, {"id": 444, "seek": 263980, "start": 2640.8, "end": 2650.8, "text": " And we're going to use the pandas agent.run to answer specifically this user question", "tokens": [50414, 400, 321, 434, 516, 281, 764, 264, 4565, 296, 9461, 13, 12997, 281, 1867, 4682, 341, 4195, 1168, 50914], "temperature": 0.0, "avg_logprob": -0.17090285164969307, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.10644328594207764}, {"id": 445, "seek": 263980, "start": 2650.8, "end": 2651.8, "text": " data frame.", "tokens": [50914, 1412, 3920, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17090285164969307, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.10644328594207764}, {"id": 446, "seek": 263980, "start": 2651.8, "end": 2659.8, "text": " And then we're going to add a return and I'm going to add a st.write in order to actually", "tokens": [50964, 400, 550, 321, 434, 516, 281, 909, 257, 2736, 293, 286, 478, 516, 281, 909, 257, 342, 13, 21561, 294, 1668, 281, 767, 51364], "temperature": 0.0, "avg_logprob": -0.17090285164969307, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.10644328594207764}, {"id": 447, "seek": 263980, "start": 2659.8, "end": 2663.8, "text": " write the answer to this question.", "tokens": [51364, 2464, 264, 1867, 281, 341, 1168, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17090285164969307, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.10644328594207764}, {"id": 448, "seek": 263980, "start": 2663.8, "end": 2664.8, "text": " Cool.", "tokens": [51564, 8561, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17090285164969307, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.10644328594207764}, {"id": 449, "seek": 266480, "start": 2664.8, "end": 2672.8, "text": " And then we have called this here.", "tokens": [50364, 400, 550, 321, 362, 1219, 341, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16262123801491477, "compression_ratio": 1.3296703296703296, "no_speech_prob": 0.027942348271608353}, {"id": 450, "seek": 266480, "start": 2672.8, "end": 2686.8, "text": " If the user is actually in the user question data frame is actually no and or no, then", "tokens": [50764, 759, 264, 4195, 307, 767, 294, 264, 4195, 1168, 1412, 3920, 307, 767, 572, 293, 420, 572, 11, 550, 51464], "temperature": 0.0, "avg_logprob": -0.16262123801491477, "compression_ratio": 1.3296703296703296, "no_speech_prob": 0.027942348271608353}, {"id": 451, "seek": 268680, "start": 2686.8, "end": 2695.8, "text": " we're just going to add an empty string.", "tokens": [50364, 321, 434, 445, 516, 281, 909, 364, 6707, 6798, 13, 50814], "temperature": 0.0, "avg_logprob": -0.19343173826063, "compression_ratio": 1.125, "no_speech_prob": 0.20390145480632782}, {"id": 452, "seek": 268680, "start": 2695.8, "end": 2698.8, "text": " Let's rerun.", "tokens": [50814, 961, 311, 43819, 409, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19343173826063, "compression_ratio": 1.125, "no_speech_prob": 0.20390145480632782}, {"id": 453, "seek": 268680, "start": 2698.8, "end": 2703.8, "text": " Okay, again, the colon.", "tokens": [50964, 1033, 11, 797, 11, 264, 8255, 13, 51214], "temperature": 0.0, "avg_logprob": -0.19343173826063, "compression_ratio": 1.125, "no_speech_prob": 0.20390145480632782}, {"id": 454, "seek": 268680, "start": 2703.8, "end": 2706.8, "text": " Let's rerun.", "tokens": [51214, 961, 311, 43819, 409, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19343173826063, "compression_ratio": 1.125, "no_speech_prob": 0.20390145480632782}, {"id": 455, "seek": 270680, "start": 2707.8, "end": 2717.8, "text": " Okay, so is there any strong core relation between some of the variables?", "tokens": [50414, 1033, 11, 370, 307, 456, 604, 2068, 4965, 9721, 1296, 512, 295, 264, 9102, 30, 50914], "temperature": 0.0, "avg_logprob": -0.1871793746948242, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.05254292115569115}, {"id": 456, "seek": 270680, "start": 2717.8, "end": 2721.8, "text": " Let's see what happens.", "tokens": [50914, 961, 311, 536, 437, 2314, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1871793746948242, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.05254292115569115}, {"id": 457, "seek": 270680, "start": 2721.8, "end": 2728.8, "text": " Okay, so there is a stronger relation between open, high, low, close and add close but not", "tokens": [51114, 1033, 11, 370, 456, 307, 257, 7249, 9721, 1296, 1269, 11, 1090, 11, 2295, 11, 1998, 293, 909, 1998, 457, 406, 51464], "temperature": 0.0, "avg_logprob": -0.1871793746948242, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.05254292115569115}, {"id": 458, "seek": 272880, "start": 2728.8, "end": 2738.8, "text": " between volume in the other variables.", "tokens": [50364, 1296, 5523, 294, 264, 661, 9102, 13, 50864], "temperature": 0.0, "avg_logprob": -0.154190875865795, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.0655868873000145}, {"id": 459, "seek": 272880, "start": 2738.8, "end": 2745.8, "text": " Does the variable close have many peaks?", "tokens": [50864, 4402, 264, 7006, 1998, 362, 867, 26897, 30, 51214], "temperature": 0.0, "avg_logprob": -0.154190875865795, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.0655868873000145}, {"id": 460, "seek": 272880, "start": 2745.8, "end": 2748.8, "text": " And let's see what it says.", "tokens": [51214, 400, 718, 311, 536, 437, 309, 1619, 13, 51364], "temperature": 0.0, "avg_logprob": -0.154190875865795, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.0655868873000145}, {"id": 461, "seek": 272880, "start": 2748.8, "end": 2752.8, "text": " Yes, the variable close has a peak of 77 point something.", "tokens": [51364, 1079, 11, 264, 7006, 1998, 575, 257, 10651, 295, 25546, 935, 746, 13, 51564], "temperature": 0.0, "avg_logprob": -0.154190875865795, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.0655868873000145}, {"id": 462, "seek": 272880, "start": 2752.8, "end": 2753.8, "text": " Cool.", "tokens": [51564, 8561, 13, 51614], "temperature": 0.0, "avg_logprob": -0.154190875865795, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.0655868873000145}, {"id": 463, "seek": 272880, "start": 2753.8, "end": 2754.8, "text": " Amazing.", "tokens": [51614, 14165, 13, 51664], "temperature": 0.0, "avg_logprob": -0.154190875865795, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.0655868873000145}, {"id": 464, "seek": 275480, "start": 2754.8, "end": 2759.8, "text": " Okay, so that is all for this first video of the series, but what is next?", "tokens": [50364, 1033, 11, 370, 300, 307, 439, 337, 341, 700, 960, 295, 264, 2638, 11, 457, 437, 307, 958, 30, 50614], "temperature": 0.0, "avg_logprob": -0.10753646691640219, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.060910217463970184}, {"id": 465, "seek": 275480, "start": 2759.8, "end": 2764.8, "text": " You can continue your data exploration by asking more questions, selecting different variables", "tokens": [50614, 509, 393, 2354, 428, 1412, 16197, 538, 3365, 544, 1651, 11, 18182, 819, 9102, 50864], "temperature": 0.0, "avg_logprob": -0.10753646691640219, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.060910217463970184}, {"id": 466, "seek": 275480, "start": 2764.8, "end": 2767.8, "text": " and seeking additional insights from our AI assistant.", "tokens": [50864, 293, 11670, 4497, 14310, 490, 527, 7318, 10994, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10753646691640219, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.060910217463970184}, {"id": 467, "seek": 275480, "start": 2767.8, "end": 2770.8, "text": " In the next video of this series, we will continue building our assistant.", "tokens": [51014, 682, 264, 958, 960, 295, 341, 2638, 11, 321, 486, 2354, 2390, 527, 10994, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10753646691640219, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.060910217463970184}, {"id": 468, "seek": 275480, "start": 2770.8, "end": 2775.8, "text": " It will be able to help you in converting your business challenge into a data science framework,", "tokens": [51164, 467, 486, 312, 1075, 281, 854, 291, 294, 29942, 428, 1606, 3430, 666, 257, 1412, 3497, 8388, 11, 51414], "temperature": 0.0, "avg_logprob": -0.10753646691640219, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.060910217463970184}, {"id": 469, "seek": 275480, "start": 2775.8, "end": 2778.8, "text": " over in guidance on model selection, providing predictions and more.", "tokens": [51414, 670, 294, 10056, 322, 2316, 9450, 11, 6530, 21264, 293, 544, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10753646691640219, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.060910217463970184}, {"id": 470, "seek": 275480, "start": 2778.8, "end": 2783.8, "text": " We will introduce the concepts of chains and tools and we will be exploring other agents.", "tokens": [51564, 492, 486, 5366, 264, 10392, 295, 12626, 293, 3873, 293, 321, 486, 312, 12736, 661, 12554, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10753646691640219, "compression_ratio": 1.7563291139240507, "no_speech_prob": 0.060910217463970184}, {"id": 471, "seek": 278380, "start": 2783.8, "end": 2787.8, "text": " Be sure to download the completed project and sample data on the Digilab Academy website,", "tokens": [50364, 879, 988, 281, 5484, 264, 7365, 1716, 293, 6889, 1412, 322, 264, 10976, 388, 455, 11735, 3144, 11, 50564], "temperature": 0.0, "avg_logprob": -0.10176759777647076, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.20916198194026947}, {"id": 472, "seek": 278380, "start": 2787.8, "end": 2791.8, "text": " where you can also find more resources and courses on data science and AI.", "tokens": [50564, 689, 291, 393, 611, 915, 544, 3593, 293, 7712, 322, 1412, 3497, 293, 7318, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10176759777647076, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.20916198194026947}, {"id": 473, "seek": 278380, "start": 2791.8, "end": 2794.8, "text": " You can also find the written tutorial linked in the description.", "tokens": [50764, 509, 393, 611, 915, 264, 3720, 7073, 9408, 294, 264, 3855, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10176759777647076, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.20916198194026947}, {"id": 474, "seek": 278380, "start": 2794.8, "end": 2798.8, "text": " Don't forget to like and subscribe to be updated with the latest tutorials and upcoming content.", "tokens": [50914, 1468, 380, 2870, 281, 411, 293, 3022, 281, 312, 10588, 365, 264, 6792, 17616, 293, 11500, 2701, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10176759777647076, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.20916198194026947}, {"id": 475, "seek": 278380, "start": 2798.8, "end": 2802.8, "text": " Thank you for joining me in this journey and I can't wait to see you in the next part.", "tokens": [51114, 1044, 291, 337, 5549, 385, 294, 341, 4671, 293, 286, 393, 380, 1699, 281, 536, 291, 294, 264, 958, 644, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10176759777647076, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.20916198194026947}, {"id": 476, "seek": 278380, "start": 2802.8, "end": 2803.8, "text": " Bye.", "tokens": [51314, 4621, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10176759777647076, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.20916198194026947}], "language": "en"}