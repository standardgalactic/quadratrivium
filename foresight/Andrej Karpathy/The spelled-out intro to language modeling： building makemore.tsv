start	end	text
0	5360	Hi everyone, hope you're well and next up what I'd like to do is I'd like to build out make more
5880	10580	Like micrograd before it make more is a repository that I have on my github web page
11040	14000	You can look at it, but just like with micrograd
14000	17640	I'm going to build it out step by step and I'm going to spell everything out
17640	19640	So we're going to build it out slowly and together
19980	21980	Now what is make more?
22020	27020	Make more as the name suggests makes more of things that you give it
27380	32080	So here's an example names.txt is an example data set to make more and
32600	37100	When you look at names.txt you'll find that it's a very large data set of names
37940	39940	so
40060	41660	Here's lots of different types of names
41660	42160	in fact
42160	47200	I believe there are 32,000 names that I've sort of found randomly on the government website and
48020	53720	If you train make more on this data set it will learn to make more of things like this
54600	61680	And in particular in this case that will mean more things that sound name like but are actually unique names
61680	64840	And maybe if you have a baby and you're trying to assign name
64840	68760	Maybe you're looking for a cool new sounding unique name make more might help you
69360	75260	So here are some example generations from the neural network once we train it on our data set
76040	80280	So here's some example unique names that it will generate don't tell
81280	83280	I rot
83440	89160	Zendy and so on and so all these are sound name like but they're not of course names
90440	94280	So under the hood make more is a character level language model
94520	100920	So what that means is that is treating every single line here as an example and within each example
100920	104600	It's treating them all as sequences of individual characters
104800	110360	So R E E S E is this example and that's the sequence of characters
110360	116600	And that's the level on which we are building out make more and what it means to be a character level language model
116600	122600	Then is that it's just sort of modeling those sequences of characters and it knows how to predict the next character in the sequence
123360	126200	Now we're actually going to implement a large number of
126520	131880	Character level language models in terms of the neural networks that are involved in predicting the next character in a sequence
132040	135000	So very simple by Graham and back of word models
135360	139460	Multilevel perceptrons recurrent neural networks all the way to modern
139800	147400	Transformers in fact a transformer that we will build will be basically the equivalent transformer to GPT2 if you have heard of GPT
147840	149120	So that's kind of a big deal
149120	153580	It's a modern network and by the end of the series you will actually understand how that works
154080	159000	On the level of characters now to give you a sense of the extensions here
159480	166760	After characters we will probably spend some time on the word level so that we can generate documents of words not just little you know segments of characters
167320	170360	But we can generate entire large much larger documents
170360	174040	And then we're probably going to go into images and image text
174640	179240	Networks such as Dali stable diffusion and so on but for now we have to start
179800	182680	Here character level language modeling. Let's go
183200	186600	So like before we are starting with a completely blank GPTN notebook page
186760	190840	The first thing is I would like to basically load up the data set names.txt
191520	194480	So we're going to open up names.txt for reading and
195400	198760	We're going to read in everything into a massive string and
199680	203880	Then because it's a massive string we'd only like the individual words and put them in the list
204240	210840	So let's call split lines on that string to get all of our words as a Python list of strings
211800	215080	so basically we can look at for example the first 10 words and
215800	222280	We have that it's a list of Emma Olivia Eva and so on and if we look at
223720	226200	The top of the page here that is indeed what we see
227080	228200	um
228200	229640	So that's good
229640	231640	This list actually makes me feel that
232280	234280	This is probably sorted by frequency
235640	241560	But okay, so these are the words now we'd like to actually like learn a little bit more about this data set
241800	245400	Let's look at the total number of words. We expect this to be roughly 32 000
246520	248520	And then what is the for example shortest word?
249160	251000	so min of
251000	253000	line of each word for w in words
253640	256440	So the shortest word will be length
257160	258280	two
258280	262280	And max of land w for w in words. So the longest word will be
263160	264680	15 characters
264680	266840	So let's now think through our very first language model
267400	272600	As I mentioned a character level language model is predicting the next character in a sequence given
273080	275720	Already some concrete sequence of characters before it
276600	280280	Now what we have to realize here is that every single word here like Isabella
280840	284840	Is actually quite a few examples packed in to that single word
285560	290600	Because what is a an existence of a word like Isabella in the data set telling us really it's saying that
291160	295000	The character i is a very likely character to come first
295560	297560	in a sequence of a name
298600	300920	The character s is likely to come
301800	303800	after i
304360	306840	The character a is likely to come after is
307640	313640	The character b is very likely to come after isa and someone all the way to a following Isabella
314440	318360	And then there's one more example actually packed in here and that is that
319080	321080	After there's Isabella
321400	323400	The word is very likely to end
323800	328840	So that's one more sort of explicit piece of information that we have here that we have to be careful with
329640	333320	And so there's a lot packed into a single individual word in terms of the
333800	337480	Statistical structure of what's likely to follow in these character sequences
338120	343960	And then of course we don't have just an individual word. We actually have 32 000 of these and so there's a lot of structure here to model
344920	349960	Now in beginning what I'd like to start with is I'd like to start with building a bi-gram language model
350200	355960	Now in a bi-gram language model, we're always working with just two characters at a time
356760	363000	So we're only looking at one character that we are given and we're trying to predict the next character in the sequence
363960	365160	so
365160	371400	What characters are likely to follow are what characters are likely to follow a and so on and we're just modeling that kind of a little
371640	372920	local structure
372920	376520	And we're forgetting the fact that we may have a lot more information
376760	379800	We're always just looking at the previous character to predict the next one
380200	383480	So it's a very simple and weak language model, but I think it's a great place to start
384120	390360	So now let's begin by looking at these bi-grams in our data set and what they look like and these bi-grams again are just two characters in a row
390920	392920	so for w in words
393160	395560	each w here is an individual word a string
396360	398360	we want to iterate uh for
399640	401640	We want to iterate this word
401720	403720	with consecutive characters
403720	406360	So two characters at a time sliding it through the word
406840	410920	Now a interesting nice way cute way to do this in python, by the way
411320	415800	Is doing something like this for character one character two in zip of
416440	418440	w and w at one
420040	421800	one column
421800	422920	print
422920	424680	character one character two
424680	425720	And let's not do all the words
425720	429400	Let's just do the first three words and i'm going to show you in a second how this works
429960	434120	But for now basically as an example, let's just do the very first word alone emma
435400	440280	You see how we have a emma and this will just print em, mm, ma
441000	447880	And the reason this works is because w is the string emma w at one column is the string mma
448680	453000	And zip takes two iterators and it pairs them up
453480	456840	And then creates an iterator over the tuples of their consecutive entries
457400	462840	And if any one of these lists is shorter than the other then it will just uh halt and return
463720	469320	So basically, that's why we return em, mm, mm, ma
469960	477240	But then because this iterator second one here runs out of elements zip just ends and that's why we only get these tuples
477720	479720	So pretty cute
479720	482600	So these are the consecutive elements in the first word
483080	486760	Now we have to be careful because we actually have more information here than just these three
487320	494760	Examples as I mentioned we know that e is the is very likely to come first and we know that a in this case is coming last
495640	498680	So one way to do this is basically we're going to create
499480	501480	special array here
501560	503080	characters
503080	507000	And um, we're going to hallucinate a special start token here
508440	509880	I'm going to
509880	511880	call it like special start
512600	514600	So this is a list of one element
514600	516600	plus
516600	517800	w
517800	520360	And then plus a special end character
521320	525320	And the reason i'm wrapping the list of w here is because w is a string mma
525960	529720	List of w will just have the individual characters in the list
530840	536920	And then doing this again now, but not iterating over w's but over the characters
538200	540200	Will give us something like this
540200	545880	So e is likely so this is a bygram of the start character and e and this is a bygram of the
546440	548440	a and the special end character
549080	551240	And now we can look at for example what this looks like for
552040	554040	olivia or heva
554520	556520	And indeed we can actually
556520	560040	Potentially do this for the entire data set, but we won't print that that's going to be too much
560680	564120	But these are the individual character bygrams and we can print them
565000	569480	Now in order to learn the statistics about which characters are likely to follow other characters
569800	573800	The simplest way in the bygram language models is to simply do it by counting
574440	579720	So we're basically just going to count how often any one of these combinations occurs in the training set
580280	581800	In these words
581800	586840	So we're going to need some kind of a dictionary that's going to maintain some counts for every one of these bygrams
587320	589320	So let's use a dictionary b
589800	592360	And this will map these bygrams
592680	595080	So bygram is a tuple of character one character two
596200	598200	And then b at bygram
598760	600760	Will be b dot get of bygram
601080	603720	Which is basically the same as b at bygram
604680	610600	But in the case that bygram is not in the dictionary b. We would like to by default return a zero
611880	613080	Plus one
613080	617560	So this will basically add up all the bygrams and count how often they occur
618280	620280	Let's get rid of printing
620280	622440	or rather
622440	625960	Let's keep the printing and let's just inspect what b is in this case
626920	632280	And we see that many bygrams occur just a single time this one allegedly occurred three times
633000	635000	So a was an ending character three times
635480	640600	And that's true for all of these words all of emma olivia and eva and with a
641640	644040	Uh, so that's why this occurred three times
645720	648520	Um now let's do it for all the words
651240	653240	Oops, I should not have printed
653960	655960	I meant to erase that
656760	658760	Let's kill this
658760	660600	Let's just run
660600	663160	And now b will have the statistics of the entire data set
664120	667560	So these are the counts across all the words of the individual bygrams
668360	671880	And we could for example look at some of the most common ones and least common ones
672760	674760	Um, this kind of grows in python
674760	678840	But the way to do this the simplest way I like is we just use b dot items
679560	681560	b dot items returns
681800	683800	the tuples of
684360	690040	Key value in this case the keys are the character bygrams and the values are the counts
690920	693400	And so then what we want to do is we want to do um
695720	697720	Sort it of this
698440	700440	But by default sort is on the first
701160	703160	um
703560	705560	On the first item of a tuple
705560	709880	But we want to sort by the values which are the second element of a tuple that is the key value
710680	712680	So we want to use the key
713080	715080	equals lambda
715320	717320	That takes the key value
717640	723640	And returns the key value at the one not at zero but at one which is the count
723960	725960	So we want to sort by the count
727160	729160	Of these elements
730440	732440	And actually we want it to go backwards
732680	737560	So here what we have is the bygram q and r occurs only a single time
738520	742200	dz occurred only a single time and when we sort this the other way around
743400	745880	We're going to see the most likely bygrams
746360	753880	So we see that n was very often an ending character many many times and apparently n almost always follows an a
754440	756440	And that's a very likely combination as well
757160	758760	Um
758760	759960	So
759960	763640	This is kind of the individual counts that we achieve over the entire data set
765000	767880	Now it's actually going to be significantly more convenient for us to
768440	772760	Keep this information in a two-dimensional array instead of a python dictionary
773720	778840	so we're going to store this information in a 2d array and
780120	784760	The rows are going to be the first character of the bygram and the columns are going to be the second character
785080	791960	And each entry in the two-dimensional array will tell us how often that first character follows the second character in the data set
792680	798280	So in particular the array representation that we're going to use or the library is that of pytorch
798840	801800	And pytorch is a deep learning neural network framework
802280	808940	But part of it is also this torch dot tensor, uh, which allows us to create multi-dimensional arrays and manipulate them very efficiently
809880	813400	So let's import pytorch, which you can do by import torch
814760	816760	And then we can create arrays
817560	819800	So let's create an array of zeros
820680	826200	And we give it a um size of this array. Let's create a three by five array as an example
827080	828600	and
828600	830600	This is a three by five array of zeros
831560	836440	And by default you'll notice a dot d type, which is short for data type is float 32
836600	838840	So these are single precision floating point numbers
839480	845000	Because we are going to represent counts. Let's actually use d type as torch dot in 32
846040	847880	So these are
847880	849880	32 bit integers
850120	853640	So now you see that we have integer data inside this tensor
854600	856600	Now tensors allow us to really, um
857400	860120	Manipulate all the individual entries and do it very efficiently
860760	863160	So for example, if we want to change this bit
863720	867240	We have to index into the tensor and in particular here
867640	870120	This is the first row and the
870840	878120	Um, because it's zero indexed. So this is row index one and column index zero one two three
878840	882360	So a at one comma three we can set that to one
883720	885720	And then a we'll have a one over there
887160	891480	We can of course also do things like this. So now a will be two over there
892600	893880	Or three
893880	896440	And also we can for example say a zero zero is five
897320	899480	And then a we'll have a five over here
900200	902760	So that's how we can index into the arrays
903320	905800	Now, of course the array that we are interested in is much much bigger
906280	909400	So for our purposes, we have 26 letters of the alphabet
909960	913320	And then we have two special characters s and e
914120	918600	So, uh, we want 26 plus two or 28 by 28 array
919320	923000	And let's call it the capital n because it's going to represent sort of the counts
924520	926520	Let me erase this stuff
926760	929720	So that's the array that starts at zeros 28 by 28
930440	932440	And now let's copy paste this
933720	936120	Here but instead of having a dictionary b
937000	939560	Which we're going to erase we now have an n
941000	944920	Now the problem here is that we have these characters which are strings, but we have to now
945880	947880	basically index into a
948760	954440	Array and we have to index using integers. So we need some kind of a lookup table from characters to integers
955320	957320	So let's construct such a character array
958040	962120	And the way we're going to do this is we're going to take all the words which is a list of strings
962840	968120	We're going to concatenate all of it into a massive string. So this is just simply the entire dataset as a single string
969320	973960	We're going to pass this to the set constructor which takes this massive string
974440	978280	And throws out duplicates because sets do not allow duplicates
978920	982840	So set of this will just be the set of all the lowercase characters
983080	986280	And there should be a total of 26 of them
988680	990680	And now we actually don't want a set we want a list
992680	996680	But we don't want a list sorted in some weird arbitrary way. We want it to be sorted
997640	999640	from a to z
999880	1001880	So sorted list
1001880	1003880	So those are our characters
1005640	1011880	Now what we want is this lookup table as I mentioned. So let's create a special s2i. I will call it
1013480	1017480	s is string or character and this will be an s2i mapping
1018840	1020200	for
1020200	1023080	Is in enumerate of these characters
1024360	1028280	So enumerate basically gives us this iterator over the integer
1028760	1034360	index and the actual element of the list and then we are mapping the character to the integer
1035240	1036760	So s2i
1036760	1041560	Is a mapping from a to 0 b to 1 etc all the way from z to 25
1043800	1049080	And that's going to be useful here, but we actually also have to specifically set that s will be 26
1049800	1051800	And s2i at e
1052120	1054520	Will be 27 right because z was 25
1056040	1059400	So those are the lookups and now we can come here and we can map
1059960	1062200	Both character 1 and character 2 to their integers
1062840	1064840	So this will be s2i character 1
1065320	1068120	And ix2 will be s2i of character 2
1069480	1071480	And now we should be able to
1072120	1076760	Do this line, but using our array. So n at ix1 ix2
1077320	1082120	This is the two-dimensional array indexing. I've shown you before and honestly just plus equals 1
1083000	1085000	Because everything starts at zero
1086200	1088200	So this should work
1088920	1092200	And give us a large 28 by 28 array
1093080	1095960	Of all these counts. So if we print n
1096920	1099480	This is the array, but of course it looks ugly
1099800	1104120	So let's erase this ugly mess and let's try to visualize it a bit more nicer
1104840	1107880	So for that we're going to use a library called mathplotlib
1108840	1114360	So mathplotlib allows us to create figures. So we can do things like pltim show of the count array
1116120	1118280	So this is the 20 by 28 array
1119000	1123240	And this is a structure, but even this I would say is still pretty ugly
1123880	1128520	So we're going to try to create a much nicer visualization of it and I wrote a bunch of code for that
1129720	1131720	The first thing we're going to need is
1131960	1133880	We're going to need to invert
1133880	1139000	This array here this dictionary. So s2i is a mapping from s to i
1139800	1142840	And in i2s, we're going to reverse this dictionary
1143080	1145960	So it rid of all the items and just reverse that array
1146600	1151640	So i2s maps inversely from 0 to a 1 to b etc
1152600	1154200	So we'll need that
1154200	1158040	And then here's the code that I came up with to try to make this a little bit nicer
1160440	1162040	To create a figure
1162040	1163560	We plot n
1164360	1169240	And then we do and then we visualize a bunch of things later. Let me just run it so you get a sense of what this is
1171960	1174200	Okay, so you see here that we have
1175320	1177160	the array spaced out
1177240	1181720	And every one of these is basically like b follows g zero times
1182360	1184360	b follows h 41 times
1185240	1187240	So a follows j 175 times
1187960	1192360	And so what you can see that i'm doing here is first i show that entire array
1192920	1195720	And then I iterate over all the individual little cells here
1196760	1198760	And I create a character string here
1199320	1204200	Which is the inverse mapping i2s of the integer i and the integer j
1204600	1207480	So those are the bigrams in a character representation
1208600	1215160	And then I plot just the bigram text and then I plot the number of times that this bigram occurs
1216040	1222280	Now the reason that there's a dot item here is because when you index into these arrays, these are torch tensors
1223000	1225320	You see that we still get a tensor back
1226040	1231080	So the type of this thing you think it would be just an integer 149, but it's actually a torch dot tensor
1231960	1237080	And so if you do dot item, then it will pop out that individual integer
1238440	1240440	So it will just be 149
1240680	1244280	So that's what's happening there. And these are just some options to make it look nice
1245240	1247240	So what is the structure of this array?
1249160	1253320	We have all these counts and we see that some of them occur often and some of them do not occur often
1253960	1257880	Now if you scrutinize this carefully, you will notice that we're not actually being very clever
1258600	1260600	That's because when you come over here
1260600	1264120	You'll notice that for example, we have an entire row of completely zeros
1264680	1266680	And that's because the end character
1267000	1273400	Is never possibly going to be the first character of a bigram because we're always placing these end tokens all at the end of a bigram
1274360	1278520	Similarly, we have entire column zeros here because the s
1279640	1287080	Character will never possibly be the second element of a bigram because we always start with s and we end with e and we only have the words in between
1287640	1291240	So we have an entire column of zeros an entire row of zeros
1291720	1293720	And in this little two by two matrix here as well
1294040	1297880	The only one that can possibly happen is if s directly follows e
1298600	1302520	That can be non-zero if we have a word that has no letters
1303080	1306840	So in that case, there's no letters in the word. It's an empty word and we just have s follows e
1307560	1309560	But the other ones are just not possible
1310120	1315000	And so we're basically wasting space and not only that but the s and the e are getting very crowded here
1315560	1321720	I was using these brackets because there's convention and natural language processing to use these kinds of brackets to denote special
1322040	1323240	tokens
1323240	1325240	But we're going to use something else
1325240	1327480	So let's fix all this and make it prettier
1328200	1332200	We're not actually going to have two special tokens. We're only going to have one special token
1333000	1337560	So we're going to have n by n array of 27 by set 27 instead
1338840	1342920	Instead of having two we will just have one and I will call it a dot
1343640	1345640	Okay
1347320	1349320	Let me swing this over here
1350440	1355640	Now one more thing that I would like to do is I would actually like to make this special character half position zero
1356200	1361000	And I would like to offset all the other letters off. I find that a little bit more pleasing
1361640	1363640	um, so
1364680	1368760	We need a plus one here so that the first character which is a will start at one
1368760	1374760	So s to i will now be a starts at one and dot is zero
1375880	1381880	And uh i2s, of course, we're not changing this because i2s just creates a reverse mapping and this will work fine
1382200	1385000	So one is a two is b zero is dot
1386520	1388520	So we've reversed that here
1389080	1391240	We have a dot and a dot
1392920	1395880	This should work fine make sure I started zeros
1396680	1404360	Count and then here we don't go up to 28 we go up to 27 and this should just work
1410840	1415720	Okay, so we see that dot dot never happened. It's at zero because we don't have empty words
1416520	1419640	Then this row here now is just very simply the
1420760	1423800	Counts for all the first letters. So
1424680	1429080	G j starts a word h starts a word i starts a word etc
1429480	1431480	And then these are all the ending
1431880	1433000	characters
1433000	1436280	And in between we have the structure of what characters follow each other
1437000	1439960	So this is the counts array of our entire
1440600	1441640	Uh dataset
1441640	1447000	So this array actually has all of the information necessary for us to actually sample from this bigram
1447560	1449560	character level language model
1449720	1450760	and
1450840	1455320	Roughly speaking what we're going to do is we're just going to start following these probabilities and these counts
1455640	1458200	And we're going to start sampling from the from model
1458840	1462920	So in the beginning, of course, um, we start with the dot the start token
1463800	1469400	Dot so to sample the first character of a name. We're looking at this row here
1470520	1478600	So we see that we have the counts and those counts externally are telling us how often any one of these characters is to start a word
1479560	1483640	So if we take this n and we grab the first row
1484760	1486200	We can do that by
1486200	1492680	using just indexing a zero and then using this notation column for the rest of that row
1493720	1495720	so n zero column
1496520	1498520	Is indexing into the zero?
1498840	1501080	Row and then it's grabbing all the columns
1501960	1504520	And so this will give us a one-dimensional array
1505240	1507640	Of the first row. So zero four four ten
1508360	1512040	You know zero four four ten one three oh six one five four two
1512520	1515880	Etc. Just the first row the shape of this is
1516520	1518520	27 it's just the row of 27
1519880	1523160	And the other way that you can do this also is you just you don't actually give this
1523720	1526920	You just grab the zero row like this. This is equal
1528120	1529960	Now these are the counts
1529960	1534200	And now what we'd like to do is we'd like to basically um sample from this
1535000	1538120	Since these are the raw counts, we actually have to convert this to probabilities
1539160	1541560	So we create a probability vector
1542920	1544920	So we'll take n of zero
1545000	1547480	And we'll actually convert this to float
1548280	1550040	first
1550040	1552120	Okay, so these integers are converted to float
1552840	1558040	floating point numbers and the reason we're creating floats is because we're about to normalize these counts
1558840	1562440	So to create a probability distribution here, we want to divide
1562680	1566440	We basically want to do p p p divide p that sum
1569640	1573400	And now we get a vector of smaller numbers and these are now probabilities
1573720	1578200	So of course because we divided by the sum the sum of p now is one
1578760	1581000	So this is a nice proper probability distribution
1581080	1585480	It sums to one and this is giving us the probability for any single character to be the first
1586040	1588040	character of a word
1588040	1592120	So now we can try to sample from this distribution to sample from these distributions
1592120	1595160	We're going to use torsion multinomial, which I've pulled up here
1596200	1598440	So torsion multinomial returns a
1600520	1606280	Samples from the multinomial probability distribution, which is a complicated way of saying you give me probabilities
1606360	1608920	And I will give you integers which are sampled
1609480	1611480	According to the probability distribution
1611560	1614680	So this is the signature of the method and to make everything deterministic
1614760	1617960	We're going to use a generator object in pi torch
1619320	1622600	So this makes everything deterministic so when you run this on your computer
1622600	1626280	You're going to the exact get the exact same results that i'm getting here on my computer
1627320	1629320	So let me show you how this works
1632760	1637320	Here's the deterministic way of creating a torch generator object
1638280	1640520	Seeding it with some number that we can agree on
1641240	1644200	So that seeds a generator gets gives us an object g
1644840	1649000	And then we can pass that g to a function that creates
1650200	1654520	Here random numbers torch.rand creates random numbers three of them
1655240	1658680	And it's using this generator object to as a source of randomness
1660440	1661960	So
1661960	1663960	Without normalizing it
1664280	1666280	I can just print
1666520	1672440	This is sort of like numbers between zero and one that are random according to this thing and whenever I run it again
1673080	1677720	I'm always going to get the same result because I keep using the same generator object, which i'm seeding here
1678760	1680760	And then if I divide
1681720	1686360	To normalize i'm going to get a nice probability distribution of just three elements
1687480	1692360	And then we can use torsion multinomial to draw samples from it. So this is what that looks like
1693080	1697480	Torsion multinomial will take the torch tensor
1698360	1700360	of probability distributions
1701000	1703240	Then we can ask for a number of samples like say 20
1704600	1708120	Replacement equals true means that when we draw an element
1708840	1715160	We will we can draw it and then we can put it back into the list of eligible indices to draw again
1715880	1720760	And we have to specify replacement as true because by default for some reason it's false
1721560	1723000	And I think
1723000	1725000	You know, it's just something to be careful with
1725720	1730920	And the generator is passed in here. So we are going to always get deterministic results the same results
1731240	1733240	So if I run these two
1733800	1736280	We're going to get a bunch of samples from this distribution
1737160	1743240	Now you'll notice here that the probability for the first element in this tensor is 60
1744520	1749400	So in these 20 samples, we'd expect 60 of them to be zero
1749400	1753160	We'd expect 30 percent of them to be one
1754280	1756760	And because the the element index two
1757480	1759480	Has only 10 probability
1759480	1764440	Very few of these samples should be two and indeed we only have a small number of twos
1765320	1767320	And we can sample as many as we like
1769000	1771000	And the more we sample the more
1771080	1774680	These numbers should roughly have the distribution here
1775640	1779080	So we should have lots of zeros half as many
1781560	1783560	Once and we should have
1784120	1786120	three times s few
1786200	1789240	Oh, sorry s few ones and three times s few
1790440	1791560	twos
1791560	1795240	So you see that we have very few twos. We have some ones and most of them are zero
1795720	1797880	So that's what torsion multinomial is doing
1798840	1800840	for us here
1801080	1803080	We are interested in this row. We've created this
1805640	1808040	P here and now we can sample from it
1809640	1811640	So if we use the same seed
1812840	1816200	And then we sample from this distribution, let's just get one sample
1818200	1820600	Then we see that the sample is say 13
1822760	1824760	So this will be the index
1825240	1828280	And let's you see how it's a tensor that wraps 13
1828760	1832280	We again have to use dot item to pop out that integer
1832920	1835720	And now index would be just the number 13
1837400	1844520	And of course the um, we can do we can map the i2s of ix to figure out exactly which character
1845080	1847080	We're sampling here. We're sampling m
1848040	1852360	So we're saying that the first character is m in our generation
1853080	1855080	And just looking at the row here
1855160	1859240	m was drawn and you we can see that m actually starts a large number of words
1860120	1865480	m started 2,500 words out of 32,000 words. So almost
1866280	1871480	A bit less than 10 of the words start with m. So this is actually fairly likely character to draw
1875240	1879240	So that would be the first character of our word and now we can continue to sample more characters
1879720	1882040	Because now we know that m started
1882920	1884760	m is already sampled
1884760	1890120	So now to draw the next character, we will come back here and we will look for the row
1891000	1893480	That starts with m. So you see m
1894600	1896600	And we have a row here
1896680	1898680	so we see that m dot is
1899560	1903720	516 ma is this many mb is this many etc
1903800	1908040	So these are the counts for the next row and that's the next character that we are going to now generate
1908600	1913560	So I think we are ready to actually just write out the loop because I think you're starting to get a sense of how this is going to go
1914520	1916280	The um
1916280	1920600	We always begin at index zero because that's the start token
1922360	1924360	And then while true
1924920	1927720	We're going to grab the row corresponding to index
1928440	1930440	That we're currently on so that's p
1931160	1933560	So that's n array at ix
1934440	1936440	converted to float is rp
1939160	1942680	Then we normalize the speed to sum to one
1944280	1947480	I accidentally ran the infinite loop
1948120	1950120	We normalize p to sum to one
1950680	1952680	Then we need this generator object
1953800	1958040	And we're going to initialize up here and we're going to draw a single sample from this distribution
1960840	1964680	And then this is going to tell us what index is going to be next
1966600	1971480	If the index sampled is zero, then that's now the nth token
1971560	1973560	So we will break
1975400	1979320	Otherwise we are going to print s2i of ix
1982280	1984280	i2s of ix
1985320	1989080	And uh, that's pretty much it. We're just uh, this should work
1990360	1992120	Okay more
1992120	1999240	So that's the that's the name that we've sampled we started with m the next step was o then r and then dot
2001480	2004040	And this dot we printed here as well
2004840	2006440	so
2006440	2008440	Let's now do this a few times
2008600	2010040	um
2010040	2012040	So let's actually create an
2013480	2015480	Out list here
2017160	2021800	And instead of printing we're going to append so out that append this character
2024440	2030840	And then here let's just print it at the end. So let's just join up all the outs and we're just going to print more
2031480	2034360	Okay, now we're always getting the same result because of the generator
2035160	2039320	So if we want to do this a few times we can go for high and range
2040120	2042120	10 we can sample 10 names
2042520	2044520	And we can just do that 10 times
2045720	2047720	And these are the names that we're getting out
2048520	2050520	Let's do 20
2054280	2056280	I'll be honest with you, this doesn't look right
2056520	2059720	So I started a few minutes to convince myself that it actually is right
2060520	2066920	The reason these samples are so terrible is that by gram language model is actually looks just like really terrible
2067800	2069800	We can generate a few more here
2070040	2075000	And you can see that they're kind of like their name like a little bit like keanu iraily etc
2075560	2077560	But they're just like totally messed up
2078600	2082600	And I mean the reason that this is so bad like we're generating h as a name
2083000	2086040	But you have to think through it from the model's eyes
2086440	2091640	It doesn't know that this h is the very first h. All it knows is that h was previously
2092120	2095160	And now how likely is h the last character?
2095640	2099240	Well, it's somewhat likely and so it just makes it last character
2099240	2103320	It doesn't know that there were other things before it or there were not other things before it
2103880	2107400	And so that's why it's generating all these like nonsense names
2108120	2110120	in other ways to do this is
2111960	2115400	To convince yourself that this is actually doing something reasonable even though it's so terrible
2116120	2117720	is
2117720	2121800	These little p's here are 27 right like 27
2123160	2125160	So how about if we did something like this?
2126280	2128280	Instead of p having any structure whatsoever
2128920	2132360	How about if p was just a torch dot ones?
2134920	2136920	Of 27
2137240	2141080	By default, this is a float 32. So this is fine divide 27
2141720	2148600	So what I'm doing here is this is the uniform distribution, which will make everything equally likely
2149880	2153240	And we can sample from that. So let's see if that does any better
2154200	2160120	Okay, so it's this is what you have from a model that is completely untrained where everything is equally likely
2160600	2166360	So it's obviously garbage and then if we have a trained model, which is trained on just by grams
2167160	2172360	This is what we get. So you can see that it is more name like it is actually working. It's just
2174120	2176200	By gram is so terrible and we have to do better
2176600	2179560	Now next I would like to fix an inefficiency that we have going on here
2180280	2185640	Because what we're doing here is we're always fetching a row of n from the counts matrix up ahead
2186520	2187960	And then we're always doing the same things
2187960	2192440	We're converting to float and we're dividing and we're doing this every single iteration of this loop
2192680	2196840	And we just keep renormalizing these rows over and over again and it's extremely inefficient and wasteful
2197320	2201240	So what I'd like to do is I'd like to actually prepare a matrix capital p
2201640	2203640	That will just have the probabilities in it
2203880	2207800	So in other words is going to be the same as the capital n matrix here of counts
2207960	2210920	But every single row will have the row of probabilities
2211560	2215560	That is normalized to 1 indicating the probability distribution for the next character
2216040	2218040	Given the character before it
2218520	2220520	As defined by which row we're in
2221480	2224600	So basically what we'd like to do is we'd like to just do it up front here
2225000	2227160	And then we would like to just use that row here
2228120	2232120	So here we would like to just do p equals p of ix instead
2232920	2234920	okay
2234920	2236920	The other reason I want to do this is not just for efficiency
2237000	2240680	But also I would like us to practice these n-dimensional tensors
2241000	2246280	And I'd like us to practice their manipulation and especially something that's called broadcasting that we'll go into in a second
2246920	2250200	We're actually going to have to become very good at these tensor manipulations
2250520	2254840	Because if we're going to build out all the way to transformers, we're going to be doing some pretty complicated
2255480	2260120	array operations for efficiency, and we need to really understand that and be very good at it
2262200	2265400	So intuitively what we want to do is we first want to grab the floating point
2265960	2267960	copy of n
2268200	2270200	And I'm mimicking the line here basically
2270920	2275080	And then we want to divide all the rows so that they sum to 1
2275720	2278840	So we'd like to do something like this p divide p dot sum
2280600	2284360	But now we have to be careful because p dot sum actually
2285400	2287400	produces a sum
2288040	2292520	Sorry p equals n dot float copy p dot sum produces a
2295080	2297640	Summs up all of the counts of this entire matrix n
2298280	2300760	And gives us a single number of just the summation of everything
2301240	2307080	So that's not the way we want to divide we want to simultaneously and in parallel divide all the rows
2307640	2309640	by their respective sums
2310600	2314760	So what we have to do now is we have to go into documentation for torch dot sum
2315800	2318600	And we can scroll down here to a definition that is relevant to us
2318680	2323000	Which is where we don't only provide an input array that we want to sum
2323320	2326440	But we also provide the dimension along which we want to sum
2327240	2329240	And in particular we want to sum up
2329880	2331880	Over rows, right
2332360	2336840	Now one more argument that I want you to pay attention to here is the keep them is false
2337800	2339720	If keep them is true
2339720	2344520	Then the output tensor is of the same size as input except of course the dimension along which you summed
2344760	2346760	Which will become just one
2347320	2350360	But if you pass in keep them as false
2352040	2354040	Then this dimension is squeezed out
2354360	2358600	And so torch dot sum not only does the sum and collapses dimension to be of size one
2358840	2363320	But in addition it does what's called a squeeze where it squeezes out it squeezes out that dimension
2363480	2369400	So basically what we want here is we instead want to do p dot sum of sum axis
2370760	2374200	And in particular notice that p dot shape is 27 by 27
2375560	2381480	So when we sum up across axis zero, then we would be taking the zero dimension and we would be summing across it
2382600	2384600	So when keep them is true
2385000	2388440	Then this thing will not only give us the counts across
2389000	2390120	um
2390120	2391960	along the columns
2391960	2396520	But notice that basically the shape of this is one by 27. We just get a row vector
2397400	2400680	And the reason we get a row vector here again is because we pass in zero dimension
2400920	2403960	So this zero dimension becomes one and we've done a sum
2404840	2407320	And we get a row and so basically we've done the sum
2408120	2409400	this way
2409400	2412280	Vertically and arrived at just a single one by 27
2413000	2415000	vector of counts
2415320	2417320	What happens when you take out keep them
2417720	2425800	Is that we just get 27 so it squeezes out that dimension and we just get a one dimensional vector of size 27
2428600	2430600	Now we don't actually want
2431320	2436280	One by 27 row vector because that gives us the counts or the sums across
2437560	2439480	the columns
2439480	2442200	We actually want to sum the other way along dimension one
2442680	2447000	And you'll see that the shape of this is 27 by one. So it's a column vector
2447320	2449320	It's a 27 by one
2449960	2451960	vector of counts
2452840	2459400	Okay, and that's because what's happened here is that we're going horizontally and this 27 by 27 matrix becomes a
2460040	2462040	27 by one array
2463480	2465480	Now you'll notice by the way that um
2466200	2468200	the actual numbers
2468200	2470200	Of these counts are identical
2470600	2474600	And that's because this special array of counts here comes from bi-gram statistics
2474840	2477160	And actually it just so happens by chance
2477720	2482440	Or because of the way this array is constructed that this sums along the columns or along the rows
2483080	2485080	Horizontally or vertically is identical
2486120	2490520	But actually what we want to do in this case is we want to sum across the uh rows
2491320	2492280	horizontally
2492280	2495800	So what we want here is speed at some of one would keep them true
2497320	2501720	27 by one column vector and now what we want to do is we want to divide by that
2504680	2507640	Now we have to be careful here again. Is it possible to take
2508840	2512360	What's a um p dot shape you see here is 27 by 27?
2512680	2519560	Is it possible to take a 27 by 27 array and divide it by what is a 27 by one array?
2521320	2523320	Is that an operation that you can do?
2523880	2527800	And whether or not you can perform this operation is determined by what's called broadcasting rules
2528280	2531080	So if you just search broadcasting semantics in torch
2532040	2537480	You'll notice that there's a special definition for what's called broadcasting that uh for whether or not
2538280	2539480	these two
2539480	2542920	Arrays can be combined in a binary operation like division
2543880	2547880	So the first condition is each tensor has at least one dimension, which is the case for us
2548520	2551800	And then when iterating over the dimension sizes starting at the trailing dimension
2552360	2556760	The dimension sizes must either be equal one of them is one or one of them does not exist
2556920	2563640	Okay, so let's do that. We need to align the two arrays and their shapes
2563960	2567320	Which is very easy because both of these shapes have two elements. So they're aligned
2568040	2571480	Then we iterate over from the from the right and going to the left
2572360	2577400	Each dimension must be either equal one of them is a one or one of them does not exist
2577880	2581320	So in this case, they're not equal, but one of them is a one. So this is fine
2581880	2585240	And then this dimension they're both equal. So this is fine
2585880	2591000	So all the dimensions are fine and therefore the this operation is broadcastable
2591880	2593880	So that means that this operation is allowed
2594520	2599160	And what is it that these arrays do when you divide 27 by 27 by 27 by 1?
2599800	2604840	What it does is that it takes this dimension one and it stretches it out it copies it
2605880	2607320	to match
2607320	2609000	27 here in this case
2609000	2615640	So in our case it takes this column vector, which is 27 by 1 and it copies it 27 times
2616680	2617800	to make
2617800	2623480	These both be 27 by 27 internally you can think of it that way and so it copies those counts
2624200	2626200	And then it does an element wise division
2627320	2633640	Which is what we want because these counts we want to divide by them on every single one of these columns in this matrix
2634760	2637080	So this actually we expect will normalize
2637720	2639720	every single row
2639720	2644760	And we can check that this is true by taking the first row for example and taking it some
2645320	2647320	We expect this to be one
2648200	2650200	Because it's now normalized
2650280	2652280	And then we expect this now
2652680	2658440	Because if we actually correctly normalize all the rows we expect to get the exact same result here. So let's run this
2659160	2661160	It's the exact same result
2661320	2664680	So this is correct. So now I would like to scare you a little bit
2665400	2669800	You actually have to like I basically encourage you very strongly to read through broadcasting semantics
2670440	2673800	And I encourage you to treat this with respect and it's not something to play
2674280	2677960	Fast and loose with it's something to really respect really understand and look up
2678040	2683400	Maybe some tutorials for broadcasting and practice it and be careful with it because you can very quickly run it to box
2683720	2685720	Let me show you what I mean
2687160	2689800	You see how here we have p. That's some of one keep them this true
2690520	2692520	The shape of this is 27 by 1
2692920	2697880	Let me take out this line just so we have the n and then we can see the counts
2698360	2701640	We can see that this is a all the counts across all the
2702520	2703480	rows
2703480	2705640	And it's a 27 by 1 column vector, right?
2707080	2712600	Now suppose that I tried to do the following but I erase keep them this true here
2713880	2716760	What does that do if keep them is not true? It's false
2717080	2722360	Then remember according to documentation it gets rid of this dimension one. It squeezes it out
2722920	2726040	So basically we just get all the same counts the same result
2726520	2730840	Except the shape of it is not 27 by 1. It is just 27 the one disappears
2731720	2733720	But all the counts are the same
2734200	2737320	So you'd think that this divide that
2737960	2739960	would uh would work
2740040	2745960	First of all, can we even uh write this and will it is it even is it even expected to run? Is it broadcastable?
2746280	2748280	Let's determine if this result is broadcastable
2749160	2751160	p dot summit one is shape
2751560	2755640	Is 27 this is 27 by 27 so 27 by 27
2757720	2759720	Broadcasting into 27
2760280	2761720	so now
2761720	2765720	rules of broadcasting number one align all the dimensions on the right done
2766280	2769400	Now iteration over all the dimensions starting from the right going to the left
2770200	2772200	All the dimensions must either be equal
2773000	2777160	One of them must be one or one of them does not exist. So here they are all equal
2777720	2779720	Here the dimension does not exist
2779960	2783480	So internally what broadcasting will do is it will create a one here
2784280	2785880	and then
2785880	2791080	We see that one of them is a one and this will get copied and this will run this will broadcast
2792440	2795320	Okay, so you'd expect this to work
2797320	2799320	Because we we are um
2799880	2801320	um
2801320	2805720	This broadcasts and this we can divide this now if I run this you'd expect it to work but
2806600	2807960	It doesn't
2807960	2811800	Uh, you actually get garbage you get a wrong result because this is actually a bug
2812440	2814440	This keep them for equals true
2817240	2819240	Makes it work
2820600	2822600	This is a bug
2822840	2828200	In both cases we are doing the correct counts. We are summing up across the rows
2829320	2832120	But keep them is saving us and making it work. So in this case
2832760	2837960	I'd like you to encourage you to potentially like pause this video at this point and try to think about why this is buggy
2838360	2840360	And why the keep them was necessary here
2842200	2843080	Okay
2843080	2848760	So the reason to do for this is I'm trying to hint it here when I was sort of giving you a bit of a hint on how this works
2849480	2851480	this 27 vector
2852200	2854200	Internally inside the broadcasting
2854200	2856200	This becomes a one by 27
2856520	2859000	And one by 27 is a row vector, right?
2859640	2862600	And now we are dividing 27 by 27 by 1 by 27
2863160	2868520	And torch will replicate this dimension. So basically, uh, it will take
2869720	2874440	It will take this, uh row vector and it will copy it vertically now
2875400	2879320	27 times so the 27 by 27 lines exactly an element wise divides
2880360	2882920	And so basically what's happening here is um
2884360	2887560	We're actually normalizing the columns instead of normalizing the rows
2889400	2895640	So you can check that what's happening here is that p at zero, which is the first row of p that sum
2896280	2898280	Is not one it's seven
2898600	2901640	It is the first column as an example that sums to one
2903640	2909320	So to summarize where does the issue come from the issue comes from the silent adding of a dimension here
2909720	2915560	Because in broadcasting rules you align on the right and go from right to left and if dimension doesn't exist you create it
2916200	2917880	So that's where the problem happens
2917880	2919320	We still did the counts correctly
2919320	2925160	We did the counts across the rows and we got the the counts on the right here as a column vector
2925560	2930840	But because the keep things was true this this uh, this dimension was discarded and now we just have a vector of 27
2931480	2936360	And because of broadcasting the way it works this vector of 27 suddenly becomes a row vector
2937000	2939000	And then this row vector gets replicated
2939000	2943320	Vertically and at every single point we are dividing by the by the count
2944280	2946280	Uh in the opposite direction
2947400	2953400	So, uh, so this thing just uh, doesn't work. You this needs to be keep them's equals true in this case
2954200	2955560	so then
2955560	2958440	Uh, then we have that p at zero is normalized
2959880	2963160	And conversely the first column you'd expect to potentially not be normalized
2964600	2966600	And this is what makes it work
2967720	2972920	So pretty subtle and uh, hopefully this helps to scare you that you should
2973000	2975640	Have a respect for broadcasting be careful. Check your work
2976440	2980920	And uh, understand how it works under the hood and make sure that it's broadcasting in the direction that you like
2981240	2987800	Otherwise, you're going to introduce very subtle bugs very hard to find bugs and uh, just be careful one more note to an efficiency
2988280	2993800	We don't want to be doing this here because uh, this creates a completely new tensor that we store into p
2994360	2996920	We prefer to use in place operations if possible
2997880	3001400	So this would be an in-place operation has the potential to be faster
3001800	3005560	It doesn't create new memory under the hood and then let's erase this
3006200	3007960	We don't need it
3007960	3009960	and let's also
3011000	3014040	Um, just do fewer just so i'm not wasting space
3014680	3016520	Okay, so we're actually in a pretty good spot now
3017000	3021240	We trained a bi-gram language model and we trained it really just by counting
3021720	3026760	Uh, how frequently any pairing occurs and then normalizing so that we get a nice probability
3027880	3030600	So really these elements of this array p
3031160	3036120	Are really the parameters of our bi-gram language model giving us and summarizing the statistics of these bi-grams
3036920	3041080	So we trained a model and then we know how to sample from a model. We just iteratively
3042040	3045960	Sampled the next character and uh feed it in each time and get a next character
3047000	3050600	Now what i'd like to do is i'd like to somehow evaluate the quality of this model
3051080	3056600	We'd like to somehow summarize the quality of this model into a single number. How good is it at predicting?
3057480	3059000	the training set
3059000	3062920	And as an example so in the training set we can evaluate now the training
3063400	3066280	Loss and this training loss is telling us about
3066920	3070680	Sort of the quality of this model in a single number just like we saw in micrograd
3071960	3075160	So let's try to think through the quality of the model and how we would evaluate it
3077080	3079880	Basically what we're going to do is we're going to copy paste this code
3080680	3082680	That we previously used for counting
3083000	3084200	Okay
3084200	3087240	And let me just print these bi-grams first. We're going to use f strings
3087880	3091480	And i'm going to print character one followed by character two. These are the bi-grams
3091960	3094680	And then I don't want to do it for all the words. Let's just do first three words
3095960	3098840	So here we have emma olivia and ava bi-grams
3100200	3107320	Now what we'd like to do is we'd like to basically look at the probability that the model assigns to every one of these bi-grams
3108120	3110600	So in other words, we can look at the probability, which is
3111160	3113160	summarized in the matrix p
3113160	3115160	of ix1, ix2
3116120	3119000	And then we can print it here as probability
3120680	3123720	And because these probabilities are way too large, let me percent
3124840	3126840	our column 0.4f
3126840	3128840	to like truncate it a bit
3129160	3134520	So what do we have here, right? We're looking at the probabilities that the model assigns to every one of these bi-grams in the data set
3135240	3138200	And so we can see some of them are four percent, three percent, etc
3138600	3140680	Just to have a measuring stick in our mind, by the way
3141640	3148360	We have 27 possible characters or tokens and if everything was equally likely, then you'd expect all these probabilities
3148920	3150600	to be
3150600	3152440	four percent roughly
3152440	3157160	So anything above four percent means that we've learned something useful from these bi-gram statistics
3157560	3161240	And you see that roughly some of these are four percent, but some of them are as high as 40 percent
3161880	3163880	35 percent and so on
3163880	3169400	So you see that the model actually assigned a pretty high probability to whatever's in the training set and so that that's a good thing
3170120	3172120	Um, basically if you have a very good model
3172440	3177560	You'd expect that these probabilities should be near one because that means that uh, your model is correctly predicting
3177640	3181400	What's going to come next especially on the training set where you where you train your model
3182840	3190120	So now we'd like to think about how can we summarize these probabilities into a single number that measures the quality of this model
3191720	3196600	Now when you look at the literature into maximum likelihood estimation and statistical modeling and so on
3197240	3200760	You'll see that what's typically used here is something called the likelihood
3201480	3204680	And the likelihood is the product of all of these probabilities
3205800	3208840	And so the product of all of these probabilities is the likelihood
3209240	3214040	And it's really telling us about the probability of the entire data set assigned
3215080	3218840	Assigned by the model that we've trained and that is a measure of quality
3219400	3222600	So the product of these should be as high as possible
3223160	3228280	When you are training the model and when you have a good model your product your product of these probabilities should be very high
3229400	3230360	um
3230360	3233960	Now because the product of these probabilities is an unwieldy thing to work with
3234280	3239080	You can see that all of them are between zero and one. So your product of these probabilities will be a very tiny number
3240120	3240920	um
3240920	3246680	So for convenience what people work with usually is not the likelihood, but they work with what's called the log likelihood
3247880	3249000	So
3249000	3252040	The product of these is the likelihood to get the log likelihood
3252360	3254360	We just have to take the log of the probability
3255000	3258600	And so the log of the probability here. I have the log of x from zero to one
3259800	3263800	The log is a you see here monotonic transformation of the probability
3264680	3266680	Where if you pass in one
3267240	3268840	You get zero
3268840	3271640	So probability one gets your log probability of zero
3272280	3274280	And then as you go lower and lower probability
3274520	3279320	The log will grow more and more negative until all the way to negative infinity at zero
3281880	3286440	So here we have a log prob, which is really just a torche dot log of probability
3286840	3289400	Let's print it out to get a sense of what that looks like
3290040	3291640	log prob
3291640	3293640	also 0.4 f
3294840	3296680	Okay
3296680	3301240	So as you can see when we plug in numbers that are very close some of our higher numbers
3301320	3308120	We get closer and closer to zero and then if we plug in very bad probabilities, we get more and more negative number. That's bad
3309560	3310920	so
3310920	3314760	And the reason we work with this is for large extent convenience, right?
3315240	3320520	Because we have mathematically that if you have some product a times b times c of all these probabilities, right?
3321160	3324600	The likelihood is the product of all these probabilities
3325400	3327400	then the log
3327400	3330280	Of these is just log of a plus
3330840	3332840	log of b
3333800	3336760	Plus log of c if you remember your logs from your
3337480	3339480	High school or undergrad and so on
3339800	3341560	so we have that basically
3341560	3347640	The likelihood is the product of probabilities. The log likelihood is just the sum of the logs of the individual probabilities
3348840	3350040	so
3350040	3352040	log likelihood
3352760	3354680	Starts at zero
3354680	3358280	And then log likelihood here we can just accumulate simply
3360440	3362440	And then the end we can print this
3365400	3367400	Print the log likelihood
3369640	3371640	F strings
3371800	3373800	Maybe you're familiar with this
3373880	3376380	So log likelihood is negative 38
3379960	3381320	Okay
3381320	3382600	now
3382600	3384600	We actually want um
3385240	3389320	So how high can log likelihood get it can go to zero
3389800	3394760	So when all the probabilities are one log likelihood will be zero and then when all the probabilities are lower
3394840	3396840	This will grow more and more negative
3397480	3404200	Now we don't actually like this because what we'd like is a loss function and a loss function has the semantics that low
3404680	3407720	Is good because we're trying to minimize the loss
3408200	3413480	So we actually need to invert this and that's what gives us something called the negative log likelihood
3414360	3416040	um
3416040	3419560	Negative log likelihood is just negative of the log likelihood
3423880	3427720	These are f strings by the way if you'd like to look this up negative log likelihood equals
3429320	3431480	So negative log likelihood now is just negative of it
3432040	3436120	and so the negative log likelihood is a very nice loss function because
3437640	3439640	The lowest it can get is zero
3439800	3443880	And the higher it is the worse off the predictions are that you're making
3444680	3448600	And then one more modification to this that sometimes people do is that for convenience
3449160	3453480	They actually like to normalize by they like to make it an average instead of a sum
3454440	3456440	and so uh here
3457080	3459080	Let's just keep some counts as well
3459320	3463480	So n plus equals one starts at zero and then here
3464600	3466940	We can have sort of like a normalized log likelihood
3470440	3472440	If we just normalize it by the count
3472440	3478280	Then we will sort of get the average log likelihood. So this would be usually our loss function here
3478840	3480840	Is put this we would this is what we would use
3482280	3486280	So our loss function for the training set assigned by the model is 2.4
3486440	3488440	That's the quality of this model
3488520	3492360	And the lower it is the better off we are and the higher it is the worse off we are
3493320	3501160	And the job of our you know training is to find the parameters that minimize the negative log likelihood loss
3502840	3507400	And that would be like a high quality model. Okay, so to summarize I actually wrote it out here
3508040	3514920	So our goal is to maximize likelihood, which is the product of all the probabilities assigned by the model
3515560	3519320	And we want to maximize this likelihood with respect to the model parameters
3519720	3525720	And in our case the model parameters here are defined in the table these numbers the probabilities are
3526520	3529320	The model parameters is sort of in our brygm language model so far
3530120	3534520	But you have to keep in mind that here we are storing everything in a table format the probabilities
3534760	3539720	But what's coming up as a brief preview is that these numbers will not be kept explicitly
3540120	3542520	But these numbers will be calculated by a neural network
3543080	3544520	So that's coming up
3544520	3547640	And we want to change and tune the parameters of these neural networks
3548040	3552360	We want to change these parameters to maximize the likelihood the product of the probabilities
3553400	3559080	Now maximizing the likelihood is equivalent to maximizing the log likelihood because log is a monotonic function
3559880	3561880	Here's the graph of log
3562040	3566040	And basically all it is doing is it's just scaling your
3566680	3568760	You can look at it as just a scaling of the loss function
3569400	3576440	And so the optimization problem here and here are actually equivalent because this is just scaling you can look at it that way
3577000	3579720	And so these are two identical optimization problems
3581160	3585320	Um maximizing the log likelihood is equivalent to minimizing the negative log likelihood
3586200	3592200	And then in practice people actually minimize the average negative log likelihood to get numbers like 2.4
3593000	3598920	And then this summarizes the quality of your model and we'd like to minimize it and make it as small as possible
3599640	3601720	And the lowest it can get is zero
3602360	3604200	and the lower it is
3604200	3609080	The better off your model is because it's assigning it's assigning high probabilities to your data
3609560	3614360	Now let's estimate the probability over the entire training set just to make sure that we get something around 2.4
3614920	3616920	Let's run this over the entire oops
3617320	3619320	Let's take out the print statement as well
3620680	3622920	Okay 2.45 or the entire training set
3624520	3629080	Now what I'd like to show you is that you can actually evaluate the probability for any word that you want like for example
3629320	3634040	If we just test a single word andre and bring back the print statement
3635880	3639640	Then you see that andre is actually kind of like an unlikely word or like on average
3640760	3648280	We take three log probability to represent it and roughly that's because ej apparently is very uncommon as an example
3650040	3652040	Now think through this
3653800	3657800	When I take andre and I append q and I test the probability of it andreq
3659800	3662200	We actually get um infinity
3663000	3668680	And that's because jq has a zero percent probability according to our model. So the log likelihood
3669320	3673720	So the log of zero will be negative infinity. We get infinite loss
3674520	3678840	So this is kind of undesirable right because we plugged in a string that could be like a somewhat reasonable name
3679160	3684920	But basically what this is saying is that this model is exactly zero percent likely to uh to predict this
3685480	3689080	Name and our loss is infinity on this example
3689720	3692040	And really what the reason for that is that j
3692920	3694920	is followed by q
3695480	3697080	zero times
3697080	3701560	Where's q jq is zero and so jq is uh zero percent likely
3702200	3705880	So it's actually kind of gross and people don't like this too much to fix this
3705880	3710200	There's a very simple fix that people like to do to sort of like smooth out your model a little bit
3710280	3712120	It's called model smoothing
3712120	3715560	And roughly what's happening is that we will eight we will add some fake accounts
3716280	3719720	So imagine adding a count of one to everything
3721000	3723000	So we add a count of one
3723400	3724600	like this
3724600	3726600	And then we recalculate the probabilities
3727800	3732280	And that's model smoothing and you can add as much as you like you can add five and that will give you a smoother model
3732920	3734760	and the more you add here
3734760	3738600	The more uniform model you're gonna have and the less you add
3739560	3741720	The more peaked model you are gonna have of course
3742280	3744280	So one is like a pretty decent
3744360	3749640	Count to add and that will ensure that there will be no zeros in our probability matrix p
3750840	3755800	And so this will of course change the generations a little bit in this case. It didn't buy it in principle. It could
3756520	3759480	But what that's going to do now is that nothing will be infinity
3760040	3761160	unlikely
3761160	3762280	So now
3762280	3767240	Our model will predict some other probability and we see that jq now has a very small probability
3767640	3772840	So the model still finds it very surprising that this was a word or a by-gram, but we don't get negative infinity
3773320	3776680	So it's kind of like a nice fix that people like to apply sometimes and it's called model smoothing
3777080	3779080	Okay, so we've now trained a respectable
3779400	3783160	by-gram character level language model and we saw that we both
3784120	3787320	Sort of trained the model by looking at the counts of all the by-grams
3787800	3790680	And normalizing the rows to get probability distributions
3791480	3797880	We saw that we can also then use those parameters of this model to perform sampling of new words
3799480	3801800	So we sample new names according to those distributions
3802200	3804920	And we also saw that we can evaluate the quality of this model
3805400	3807800	And the quality of this model is summarized in a single number
3808040	3812600	Which is the negative log likelihood and the lower this number is the better the model is
3813320	3818920	Because it is giving high probabilities to the actual next characters and all the by-grams in our training set
3819640	3821640	So that's all well and good
3822040	3826040	But we've arrived at this model explicitly by doing something that felt sensible
3826120	3830120	We were just performing counts and then we were normalizing those counts
3831000	3833720	Now what I would like to do is I would like to take an alternative approach
3834040	3836200	We will end up in a very very similar position
3836440	3843400	But the approach will look very different because I would like to cast the problem of by-gram character level language modeling into the neural network framework
3844200	3849960	And in neural network framework, we're going to approach things slightly differently, but again end up in a very similar spot
3850200	3851960	I'll go into that later
3851960	3856840	Now our neural network is going to be a still a by-gram character level language model
3857240	3859720	So it receives a single character as an input
3860360	3863320	Then there's neural network with some weights or some parameters w
3864200	3868920	And it's going to output the probability distribution over the next character in a sequence
3869080	3874600	It's going to make guesses as to what is likely to follow this character that was input to the model
3875960	3882520	And then in addition to that we're going to be able to evaluate any setting of the parameters of the neural net because we have the loss function
3883720	3888760	The negative log likelihood. So we're going to take a look at its probability distributions and we're going to use the labels
3890120	3894120	Which are basically just the identity of the next character in that by-gram the second character
3894680	3897880	So knowing what the second character actually comes next in the by-gram
3898200	3903240	Allows us to then look at what how high of probability the model assigns to that character
3903880	3906120	And then we of course want the probability to be very high
3907000	3909720	And that is another way of saying that the loss is low
3910840	3915000	So we're going to use gradient based optimization then to tune the parameters of this network
3915400	3918120	Because we have the loss function and we're going to minimize it
3918360	3923560	So we're going to tune the weights so that the neural net is correctly predicting the probabilities for the next character
3924360	3929480	So let's get started. The first thing I want to do is I want to compile the training set of this neural network, right?
3929560	3932120	So create the training set
3933080	3935080	of all the by-grams
3936360	3938360	Okay, and
3939400	3942760	Here I'm going to copy paste this code
3943720	3945720	Because this code iterates over all the by-grams
3947400	3952680	So here we start with the words we iterate over all the by-grams and previously as you recall we did the counts
3953000	3956040	But now we're not going to do counts. We're just creating a training set
3956920	3959720	Now this training set will be made up of two lists
3962120	3964120	We have the
3964760	3966280	inputs
3966280	3968600	And the targets the the labels
3969480	3972440	And these by-grams will denote x y those are the characters, right?
3973160	3976920	And so we're given the first character of the by-gram and then we're trying to predict the next one
3977720	3982520	Both of these are going to be integers. So here we'll take x's that append is just
3983560	3985960	x1 y's that append ix2
3987640	3989320	And then here
3989320	3995720	We actually don't want lists of integers. We will create uh tensors out of these. So x's is torched dot tensor
3996520	3999880	x's and y's is torched dot tensor of y's
4001480	4006360	And then we don't actually want to take all the words just yet because I want everything to be manageable
4007000	4009240	So let's just do the first word which is emma
4011240	4013640	And then it's clear what these x's and y's would be
4015400	4017400	Here let me print
4017880	4020440	Character one character two just so you see what's going on here
4021560	4028280	So the by-grams of these characters is dot e e m m m a dot
4028760	4033800	So this single word as I mentioned has one two three four five examples for our neural network
4034680	4036680	There are five separate examples in emma
4037560	4042120	And those examples are summarized here when the input to the neural neural network is integer zero
4043160	4047160	The desired label is integer five which corresponds to e
4047960	4054440	When the input to the neural network is five, we want its weights to be arranged so that 13 gets a very high probability
4055080	4058520	When 13 is put in we want 13 to have a high probability
4059160	4062520	When 13 is put in we also want one to have a high probability
4063480	4066920	When one is input we want zero to have a very high probability
4067400	4070600	So there are five separate input examples to a neural net
4071400	4073400	in this data set
4075000	4080440	I wanted to add a tangent of a note of caution to be careful with a lot of the apis of some of these frameworks
4081400	4087160	You saw me silently use torch dot tensor with a lowercase t and the output looked right
4087800	4091240	But you should be aware that there's actually two ways of constructing a tensor
4091720	4097880	There's a torch dot lowercase tensor and there's also a torch dot capital tensor class, which you can also construct
4098680	4102040	So you can actually call both you can also do torch dot capital tensor
4102920	4104920	And you get an x as in y as well
4105400	4107400	So that's not confusing at all
4108920	4112440	There are threads on what is the difference between these two and um
4113400	4120120	Unfortunately, the docs are just like not clear on the difference and when you look at the the docs of lowercase tensor construct tensor
4120200	4122520	With no autograd history by copying data
4123640	4125640	It's just like it doesn't
4125640	4131000	It doesn't make sense. So the actual difference as far as I can tell is explained eventually in this random thread that you can google
4131640	4133640	And really it comes down to I believe
4135080	4137080	That um, where is this?
4138520	4143720	Torch dot tensor in first the d type the data type automatically while torch dot tensor just returns a float tensor
4144360	4146680	I would recommend stick to torch dot lowercase tensor
4147800	4149640	so um
4149640	4156760	Indeed we see that when I construct this with a capital t the data type here of x's is float 32
4158120	4160120	But torch dot lowercase tensor
4161080	4165080	You see how it's now x dot d type is now integer
4166760	4168200	So, um
4168200	4173400	It's advised that you use lowercase t and you can read more about it if you like in some of these threads
4174040	4176040	but basically
4176600	4181560	I'm pointing out some of these things because because I want to caution you and I want you to read get used to reading a
4181560	4187800	lot of documentation and reading through a lot of uh q and a's and threads like this and um
4188280	4192280	You know some of this stuff is unfortunately not easy and not very well documented and you have to be careful out there
4192680	4196440	What we want here is integers because that's what makes sense
4197080	4198040	um
4198040	4199480	and so
4199480	4205560	Lowercase tensor is what we are using. Okay. Now. We want to think through how we're going to feed in these examples into a neural network
4206200	4208200	Now it's not quite as straightforward as
4209080	4214360	Plugging it in because these examples right now are integers. So there's like a 0 5 or 13
4214600	4219080	It gives us the index of the character and you can't just plug an integer index into a neural net
4219960	4226520	these neural nets, uh, right are sort of made up of these neurons and uh, these neurons have weights
4226840	4232200	And as you saw in micrograd these weights act multiplicatively on the inputs w x plus b
4232520	4236360	There's 10 hs and so on and so it doesn't really make sense to make an input neuron
4236440	4240840	Take on integer values that you feed in and then multiply on with weights
4241720	4246280	So instead a common way of encoding integers is what's called one hot encoding
4247000	4253240	In one hot encoding, uh, we take an integer like 13 and we create a vector that is all zeros
4253640	4260040	Except for the 13th dimension which we turn to a one and then that vector can feed into a neural net
4261000	4265480	Now conveniently, uh, PyTorch actually has something called the one hot
4267880	4272440	Function inside torch and in functional it takes a tensor made up of integers
4275000	4277400	Long is a is a is an integer
4279160	4286280	And it also takes a number of classes, um, which is how large you want your tensor your vector to be
4286520	4293320	So here let's import torch dot and in that functional sf. This is a common way of importing it
4294120	4296120	And then let's do f dot one hot
4296680	4299240	And we feed in the integers that we want to encode
4300040	4302920	So we can actually feed in the entire array of x's
4304040	4306920	And we can tell it that num classes is 27
4307720	4313080	So it doesn't have to try to guess it it may have guessed that it's only 13 and would give us an incorrect result
4313480	4319480	So this is the one hot. Let's call this x ink for x encoded
4322040	4325720	And then we see that x encoded that shape is 5 by 27
4327080	4328440	and uh
4328440	4331240	We can also visualize it plt.im show of x ink
4332360	4334680	To make it a little bit more clear because this is a little messy
4335480	4338200	So we see that we've encoded all the five examples
4338920	4341320	Into vectors we have five examples
4341400	4345560	So we have five rows and each row here is now an example into a neural net
4346280	4351000	And we see that the appropriate bit is turned on as a one and everything else is zero
4351960	4353560	so um
4353560	4357800	Here for example, the zero bit is turned on the fifth bit is turned on
4358360	4363640	Thirteenth bits are turned on for both of these examples and the first bit here is turned on
4364680	4366680	So that's how we can encode
4367160	4369160	integers into vectors
4369320	4374200	And then these vectors can feed in to neural nets one more issue to be careful with here by the way is
4375160	4378680	Let's look at the data type of encoding. We always want to be careful with data types
4379400	4384920	What would you expect x encodings data type to be when we're plugging numbers into neural nets?
4384920	4389960	We don't want them to be integers. We want them to be floating point numbers that can take on various values
4390440	4393560	But the d type here is actually 64 bit integer
4394280	4399160	And the reason for that I suspect is that one hot received a 64 bit integer here
4399640	4401640	And it returned the same data type
4401880	4407880	And when you look at the signature of one hot it doesn't even take a d type a desired data type of the output tensor
4408440	4413960	And so we can't in a lot of functions in torch we'd be able to do something like d type equals torch dot float 32
4414440	4417320	Which is what we want, but one hot does not support that
4417960	4421400	So instead we're going to want to cast this to float like this
4422120	4424120	So that these
4424840	4426440	Everything is the same
4426440	4431320	Everything looks the same but the d type is float 32 and floats can feed into
4432280	4435000	Neural nets. So now let's construct our first neuron
4436040	4439000	This neuron will look at these input vectors
4440040	4447720	And as you remember from micrograd these neurons basically perform a very simple function wx plus b where wx is a dot product
4448280	4449560	right
4449640	4454600	So we can achieve the same thing here. Let's first define the weights of this neuron
4454600	4458040	Basically, what are the initial weights at initialization for this neuron?
4458760	4460760	Let's initialize them with torch dot random
4461640	4463320	torch dot random
4463320	4464600	is
4464600	4466600	Fills a tensor with random numbers
4467160	4469160	drawn from a normal distribution
4469240	4471240	And a normal distribution has
4471960	4478280	A probability density function like this and so most of the numbers drawn from this distribution will be around zero
4479080	4485240	But some of them will be as high as almost three and so on and very few numbers will be above three in magnitude
4486360	4489400	So we need to take a size as an input here
4490440	4493160	And i'm going to use size as to be 27 by one
4494680	4501160	So 27 by one and then let's visualize w. So w is a column vector of 27 numbers
4503080	4507400	And uh, these weights are then multiplied by the inputs
4508680	4514120	So now to perform this multiplication, we can take x encoding and we can multiply it with w
4515000	4518200	This is a matrix multiplication operator in pytorch
4519960	4522920	And the output of this operation is five by one
4523640	4525640	The reason it's five by five is the following
4525880	4531720	We took x encoding, which is five by 27 and we multiplied it by 27 by one
4533560	4535720	And in matrix multiplication
4536520	4543320	You see that the output will become five by one because these 27 will multiply and add
4544840	4547960	So basically what we're seeing here out of this operation
4548760	4550760	Is we are seeing the five
4550760	4552040	um
4552040	4553640	activations
4553640	4555640	of this neuron
4556360	4560200	On these five inputs and we've evaluated all of them in parallel
4560440	4563080	We didn't feed in just a single input to the single neuron
4563400	4567400	We fed in simultaneously all the five inputs into the same neuron
4568120	4570520	And in parallel pytorch has evaluated
4571160	4575160	The wx plus b but here is just wx. There's no bias
4575880	4578760	It has valued w w times x for all of them
4579400	4585240	Uh independently now instead of a single neuron though, I would like to have 27 neurons and I'll show you in the second
4585240	4587240	Why I'm on 27 neurons?
4587560	4591880	So instead of having just a one here, which is indicating this presence of one single neuron
4592520	4594520	We can use 27
4594680	4596920	And then when w is 27 by 27
4598360	4604760	This will in parallel evaluate all the 27 neurons on all the five inputs
4606520	4609000	Giving us a much better much much bigger result
4609480	4613320	So now what we've done is five by 27 multiplied 27 by 27
4614120	4616620	And the output of this is now five by 27
4617800	4619800	So we can see that the shape of this
4621880	4623880	Is five by 27
4623880	4626520	So what is every element here telling us right?
4627160	4630760	It's telling us for every one of 27 neurons that we created
4633240	4638680	What is the firing rate of those neurons on every one of those five examples
4639640	4640840	so
4640840	4644200	The element for example 3 comma 13
4645400	4648680	Is giving us the firing rate of the 13th neuron
4649320	4651320	looking at the third input
4651880	4655240	And the way this was achieved is by a dot product
4656280	4658280	between the third input
4658920	4660920	and the 13th column
4661560	4663560	of this w matrix here
4664840	4669560	Okay, so using matrix multiplication. We can very efficiently evaluate
4670840	4674040	The dot product between lots of input examples in a batch
4675000	4680360	And lots of neurons where all of those neurons have weights in the columns of those w's
4681080	4683720	And in matrix multiplication, we're just doing those dot products and
4684520	4689240	In parallel just to show you that this is the case. We can take x-ank and we can take the third
4690520	4692200	row
4692200	4695080	And we can take the w and take its 13th column
4697400	4699800	And then we can do x-ank at 3
4701720	4704440	Element wise multiply with w at 13
4706760	4708920	And sum that up does w x plus b
4709640	4713320	Well, there's no plus b. It's just w x dot product. And that's
4714120	4715320	this number
4715320	4719320	So you see that this is just being done efficiently by the matrix multiplication
4719640	4725400	operation for all the input examples and for all the output neurons of this first layer
4726040	4732920	Okay, so we fed our 27 dimensional inputs into a first layer of a neural net that has 27 neurons, right?
4732920	4739400	So we have 27 inputs and now we have 27 neurons these neurons perform w times x
4739720	4743000	They don't have a bias and they don't have a non-linearity like 10 h
4743160	4745880	We're going to leave them to be a linear layer
4746600	4750840	In addition to that we're not going to have any other layers. This is going to be it. It's just going to be
4751480	4755240	The dumbest smallest simplest neural net, which is just a single linear layer
4756520	4760280	And now I'd like to explain what I want those 27 outputs to be
4761240	4764200	Intuitively what we're trying to produce here for every single input example
4764520	4768760	Is we're trying to produce some kind of a probability distribution for the next character in a sequence
4769320	4771320	And there's 27 of them
4771480	4775320	But we have to come up with like precise semantics for exactly how we're going to interpret
4775800	4778760	These 27 numbers that these neurons take on
4779720	4781160	Now intuitively
4781160	4784440	You see here that these numbers are negative and some of them are positive, etc
4785160	4789240	And that's because these are coming out of a neural net layer initialized with these
4791240	4793080	normal distribution
4793080	4794280	parameters
4794280	4796680	But what we want is we want something like we had here
4797240	4802920	Like each row here told us the counts and then we normalize the counts to get probabilities
4803480	4805640	And we want something similar to come out of a neural net
4806440	4809240	But what we just have right now is just some negative and positive numbers
4810600	4814520	Now we want those numbers to somehow represent the probabilities for the next character
4815320	4819240	But you see that probabilities they they have a special structure. They um
4819960	4825080	They're positive numbers and they sum to 1 and so that doesn't just come out of a neural net
4825800	4830120	And then they can't be counts because these counts are positive
4830680	4835880	And counts are integers. So counts are also not really a good thing to output from a neural net
4836680	4841400	So instead what the neural net is going to output and how we are going to interpret the um
4842120	4847640	The 27 numbers is that these 27 numbers are giving us log counts
4848520	4849720	basically
4849720	4850440	um
4850440	4855240	So instead of giving us counts directly like in this table, they're giving us log counts
4856040	4860440	And to get the counts, we're going to take the log counts and we're going to exponentiate them
4861400	4863000	now
4863000	4864200	exponentiation
4864200	4866200	takes the following form
4867160	4868920	It takes numbers
4868920	4872280	That are negative or they are positive. It takes the entire real line
4872840	4877720	And then if you plug in negative numbers, you're going to get e to the x which is
4878520	4880520	always below 1
4880600	4882600	So you're getting numbers lower than 1
4883480	4887720	And if you plug in numbers greater than zero, you're getting numbers greater than one
4888360	4890360	all the way growing to the infinity
4890840	4892840	And this here grows to zero
4893320	4896680	So basically we're going to take these numbers
4897800	4899800	here
4900360	4902200	and
4903240	4905640	Instead of them being positive and negative and all over the place
4906040	4911560	We're going to interpret them as log counts and then we're going to element wise exponentiate these numbers
4912840	4915400	Exponentiating them now gives us something like this
4916440	4919000	And you see that these numbers now because they went through an exponent
4919320	4923720	All the negative numbers turned into numbers below one like 0.338
4924200	4929400	And all the positive numbers originally turned into even more positive numbers sort of greater than one
4930120	4932120	um, so like for example
4932520	4934520	7
4934520	4937000	Is some positive number over here?
4938440	4940440	That is greater than zero
4941080	4942200	But
4942200	4944200	Exponentiated outputs here
4944840	4950440	Basically give us something that we can use and interpret as the equivalent of counts originally
4950920	4954840	So you see these counts here 112 7 51 1 etc
4956360	4958360	The neural net is kind of now predicting
4959160	4960360	Uh
4960360	4961560	counts
4961560	4966200	And these counts are positive numbers. They can never be below zero. So that makes sense
4966760	4969080	And they can now take on various values
4969720	4972600	Depending on the settings of w
4974200	4976200	So let me break this down
4976200	4979080	We're going to interpret these to be the log counts
4981240	4984360	In other words for this that is often used is so called logits
4985160	4987640	These are logits log counts
4988600	4990600	Then these will be sort of the counts
4991320	4993320	Logits exponentiated
4993400	4997160	And this is equivalent to the n matrix sort of the n
4998120	5000760	Array that we used previously. Remember, this was the n
5001640	5007000	This is the the array of counts and each row here are the counts for the
5007800	5010360	For the um next character sort of
5012680	5016760	So those are the counts and now the probabilities are just the counts
5017320	5019320	um normalized
5019720	5021720	and so um
5021720	5025400	I'm not going to find the same but basically I'm not going to scroll all over the place
5026040	5029400	We've already done this. We want two counts that sum
5030120	5033880	Along the first dimension and we want to keep them. It's true
5034760	5041320	We've went over this and this is how we normalized the rows of our counts matrix to get our probabilities
5042120	5044120	Props
5044840	5046840	So now these are the probabilities
5047880	5052120	And these are the counts that we have currently and now when I show the probabilities
5053720	5057800	You see that um every row here, of course
5059480	5061480	Will sum to one
5061480	5063160	Because they're normalized
5063160	5065160	And the shape of this
5065320	5067320	Is five by 27
5067480	5071240	And so really what we've achieved is for every one of our five examples
5071720	5074280	We now have a row that came out of a neural net
5075160	5079960	And because of the transformations here, we made sure that this output of this neural net now
5080360	5083000	Are probabilities or we can interpret to be probabilities
5084120	5085320	so
5085320	5090120	Our wx here gave us logits and then we interpret those to be log counts
5090760	5093320	We exponentiate to get something that looks like counts
5094040	5096760	And then we normalize those counts to get a probability distribution
5097480	5099720	And all of these are differentiable operations
5100360	5102680	So what we've done now is we are taking inputs
5103240	5106120	We have differentiable operations that we can back propagate through
5106920	5108920	And we're getting out probability distributions
5109880	5113880	So um for example for the zeroth example that fed in
5115160	5117160	Right, which was um
5117160	5119640	The zeroth example here was a one half vector of zero
5120600	5122600	and um
5122600	5125320	It basically corresponded to feeding in
5125480	5126600	Uh
5126600	5127800	This example here
5127800	5133640	So we're feeding in a dot into a neural net and the way we fed the dot into a neural net is that we first got its index
5134360	5136360	Then we one hot encoded it
5136600	5139400	Then it went into the neural net and out came
5140600	5142600	This distribution of probabilities
5143400	5145400	And its shape
5146440	5153320	Is 27 there's 27 numbers and we're going to interpret this as the neural net's assignment for how likely
5154040	5156040	every one of these characters
5156600	5158840	The 27 characters are to come next
5159800	5161800	And as we tune the weights w
5162440	5166360	We're going to be of course getting different probabilities out for any character that you input
5167240	5170360	And so now the question is just can we optimize and find a good w?
5171080	5173960	Such that the probabilities coming out are pretty good
5174520	5176840	And the way we measure pretty good is by the loss function
5177160	5180840	Okay, so I organized everything into a single summary so that hopefully it's a bit more clear
5181240	5183800	So it starts here with an input data set
5184440	5191320	We have some inputs to the neural net and we have some labels for the correct next character in a sequence and these are integers
5192840	5197720	Here i'm using uh torch generators now so that you see the same numbers that I see
5198600	5200600	and i'm generating um
5200920	5205560	27 neurons weights and each neuron here receives 27 inputs
5206280	5214360	Then here we're going to plug in all the input examples x's into a neural net. So here this is a forward pass
5215720	5219560	First we have to encode all of the inputs into one half representations
5220440	5228680	So we have 27 classes. We pass in these integers and x ink becomes a array that is 5 by 27
5229720	5231720	zeros except for a few ones
5232280	5235800	We then multiply this in the first layer of a neural net to get logits
5236920	5242920	Exponentiate the logits to get fake counts sort of and normalize these counts to get probabilities
5244440	5248520	So the these last two lines by the way here are called the softmax
5249800	5251800	Uh, which I pulled up here
5252040	5258280	Softmax is a very often used layer in a neural net that takes these z's which are logits
5258920	5260920	Exponentiates them
5261000	5264040	And uh divides and normalizes it's a way of taking
5264680	5266680	outputs of a neural net layer and these
5267240	5269240	These outputs can be positive or negative
5269800	5274520	And it outputs probability distributions. It outputs something that is always
5275240	5277880	sums to one and are positive numbers just like probabilities
5278600	5281640	Um, so this is kind of like a normalization function if you want to think of it that way
5282120	5285400	And you can put it on top of any other linear layer inside a neural net
5285640	5291720	And it basically makes a neural net output probabilities. That's very often used and we used it as well here
5293400	5296840	So this is the forward pass and that's how we made a neural net output probability
5297960	5299480	now
5299480	5301480	you'll notice that um
5303000	5306600	All of these this entire forward pass is made up of differentiable
5307320	5312600	Layers everything here we can back propagate through and we saw some of the back propagation in micrograd
5313240	5315240	This is just
5315320	5317320	Multiplication and addition all that's happening here
5317320	5320120	Just multiply and then add and we know how to back propagate through them
5320920	5322920	Exponentiations we know how to back propagate through
5323880	5329480	And then here we are summing and sum is is easily back propagated well as well
5330120	5333800	And division as well. So everything here is differentiable operation
5334600	5336600	And we can back propagate through
5337560	5341000	Now we achieve these probabilities which are 5 by 27
5341640	5345480	For every single example, we have a vector of probabilities that sum to 1
5346360	5349640	And then here I wrote a bunch of stuff to sort of like break down
5350280	5354680	The examples so we have five examples making up emma, right?
5356360	5358920	And there are five bigrams inside emma
5359000	5366920	So by gram example a by gram example one is that e is the beginning character right after dot
5368360	5373240	And the indexes for these are zero and five. So then we feed in a zero
5374120	5379880	That's the input to the neural net we get probabilities from the neural net that are 27 numbers
5381320	5386760	And then the label is five because e actually comes after dot. So that's the label
5387720	5389240	And then
5389240	5393640	We use this label five to index into the probability distribution here
5394360	5400520	So this index five here is zero one two three four five. It's this number here
5401240	5403240	Which is here
5404040	5408040	So that's basically the probability assigned by the neural net to the actual correct character
5408760	5414760	You see that the network currently thinks that this next character that e following dot is only 1% likely
5415320	5417000	Which is of course not very good, right?
5417000	5422120	Because this actually is a training example and the network thinks that this is currently very very unlikely
5422440	5426680	But that's just because we didn't get very lucky in generating a good setting of w
5427080	5431080	So right now this network thinks this is unlikely and 0.01 is not a good outcome
5431960	5433960	So the log likelihood then
5434360	5438760	Is very negative and the negative log likelihood is very positive
5439400	5441400	And so four is a very high
5441880	5443160	negative log likelihood
5443160	5445160	And that means we're going to have a high loss
5445320	5449720	Because what is the loss the loss is just the average negative log likelihood
5451640	5453640	So the second character is em
5453640	5458840	And you see here that also the network thought that m following e is very unlikely 1%
5461000	5463640	The for m following m it thought it was 2%
5464280	5467480	And for a following m it actually thought it was 7% likely
5467880	5474440	So just by chance this one actually has a pretty good probability and therefore a pretty low negative log likelihood
5475320	5477720	And finally here it thought this was 1% likely
5478360	5484200	So overall our average negative log likelihood, which is the loss the total loss that summarizes
5484760	5489480	Basically the how well this network currently works at least on this one word not on the full data
5489560	5495960	So just the one word is 3.76 which is actually very fairly high loss. This is not a very good setting of w's
5496840	5498600	Now here's what we can do
5498600	5500600	We're currently getting 3.76
5501320	5505240	We can actually come here and we can change our w we can resample it
5505640	5507960	So let me just add one to have a different seed
5508760	5511800	And then we get a different w and then we can rerun this
5512840	5517800	And with this different seed with this different setting of w's we now get 3.37
5518600	5523960	So this is a much better w right and that and it's better because the probabilities just happen to come out
5524760	5527880	Higher for the for the characters that actually are next
5528840	5532680	And so you can imagine actually just resampling this, you know, we can try as two
5534360	5535560	So
5535560	5539320	Okay, this was not very good. Let's try one more. We can try three
5540920	5544040	Okay, this was terrible setting because we have a very high loss
5544760	5546120	so
5546120	5548120	Anyway, I'm going to erase this
5548680	5554920	What I'm doing here, which is just guess and check of randomly assigning parameters and seeing if the network is good
5555480	5558760	That is a amateur hour. That's not how you optimize a neural net
5559080	5563480	The way you optimize your neural net is you start with some random guess and we're going to commit to this one
5563560	5565160	Even though it's not very good
5565160	5567320	But now the big deal is we have a loss function
5568360	5570040	So this loss
5570040	5572040	Is made up only of differentiable
5572600	5574280	operations
5574280	5577080	And we can minimize the loss by tuning
5577640	5583720	W's by computing the gradients of the loss with respect to these w matrices
5585080	5591240	And so then we can tune w to minimize the loss and find a good setting of w using gradient based optimization
5591640	5596440	So let's see how that will work now things are actually going to look almost identical to what we had with micrograd
5597080	5601720	So here I pulled up the lecture from micrograd the notebook
5602040	5603880	It's from this repository
5603880	5607960	And when I scroll all the way to the end where we left off with micrograd, we had something very very similar
5608600	5613560	We had a number of input examples in this case. We had four input examples inside x's
5614200	5616200	And we had their targets
5616280	5617720	desired targets
5617720	5623400	Just like here we have our x's now, but we have five of them and they're now integers instead of vectors
5624120	5630600	But we're going to convert our integers to vectors except our vectors will be 27 large instead of three large
5630840	5637080	And then here what we did is first we did a forward pass where we ran a neural net on all of the inputs
5638360	5640360	to get predictions
5640360	5644440	Our neural net at the time this n of x was a multilayer perceptron
5645160	5649400	Our neural net is going to look different because our neural net is just a single layer
5650520	5652840	Single linear layer followed by a softmax
5653800	5655800	So that's our neural net
5655880	5658040	And the loss here was the mean squared error
5658360	5662600	So we simply subtracted the prediction from the ground truth and squared it and summed it all up
5662920	5668200	And that was the loss and loss was the single number that summarized the quality of the neural net
5668520	5674840	And when loss is low like almost zero that means the neural net is um predicting correctly
5676280	5681480	So we had a single number that uh that summarized the uh the performance of the neural net
5682040	5685400	And everything here was differentiable and was stored in massive compute graph
5686760	5689160	And then we iterated over all the parameters
5689160	5693240	We made sure that the gradients are set to zero and we called loss.backward
5694120	5699800	And loss.backward initiated back propagation at the final output node of loss, right? So
5700840	5703320	Yeah, I remember these expressions. We had loss all the way at the end
5703560	5705800	We start back propagation and we went all the way back
5706360	5710120	And we made sure that we populated all the parameters dot grad
5710760	5713960	So dot grad started at zero but back propagation filled it in
5714520	5720760	And then in the update we iterated over all the parameters and we simply did a parameter update where every single
5721480	5726440	element of our parameters was nudged in the opposite direction of the gradient
5727560	5730840	And so we're going to do the exact same thing here
5731720	5733720	So i'm going to pull this up
5734440	5736440	On the side here
5737400	5741560	So that we have it available and we're actually going to do the exact same thing
5742120	5745720	So this was the forward pass. So we're we did this
5746920	5748920	And probes is our y-pred
5748920	5752040	So now we have to evaluate the loss, but we're not using the mean squared error
5752360	5757640	We're using the negative log likelihood because we are doing classification. We're not doing regression as it's called
5759080	5761080	So here we want to calculate loss
5762440	5766280	Now the way we calculate it is is just this average negative log likelihood
5767160	5769160	Now this probes here
5770680	5772680	Has a shape of five by 27
5773400	5779400	And so to get all the we basically want to pluck out the probabilities at the correct indices here
5780040	5783880	So in particular because the labels are stored here in the array y's
5784520	5790280	Basically what we're after is for the first example, we're looking at probability of five right at index five
5790920	5792760	for the second example
5792760	5795720	at the the second row or row index one
5796200	5799320	We are interested in the probability assigned to index 13
5800280	5802520	At the second example, we also have 13
5803480	5805880	At the third row, we want one
5807400	5810920	And at the last row, which is four we want zero
5811240	5817160	So these are the probabilities we're interested in right and you can see that they're not amazing as we saw above
5818680	5823480	So these are the probabilities we want but we want like a more efficient way to access these probabilities
5824040	5826600	Um, not just listing them out in a tuple like this
5827080	5833480	So it turns out that the way to do this in pi torch one of the ways at least is we can basically pass in all of these
5836760	5841240	Sorry about that all of these um integers in vectors
5842120	5846600	So the these ones you see how they're just zero one two three four
5847160	5852120	We can actually create that using mp not mp. Sorry torch dot a range of five
5852840	5854440	zero one two three four
5854440	5857400	So we can index here with torch dot a range of five
5858360	5860360	And here we index with y's
5861160	5863160	And you see that that gives us
5863240	5865240	exactly these numbers
5869000	5873400	So that plugs out the probabilities of that the neural network assigns to the
5874040	5876040	correct next character
5876360	5880280	Now we take those probabilities and we don't we actually look at the log probability
5880600	5882600	So we want to dot log
5883560	5886520	And then we want to just average that up
5886680	5892520	So take the mean of all of that and then it's the negative average log likelihood. That is the loss
5894280	5896280	So the loss here is
5896920	5900360	3.7 something and you see that this loss 3.76
5900760	5905800	3.76 is exactly as we've obtained before but this is a vectorized form of that expression
5906520	5908680	So we get the same loss
5909480	5913400	And the same loss we can consider sort of as part of this forward pass
5914040	5916040	And we've achieved here now loss
5916360	5919400	Okay, so we made our way all the way to loss. We've defined the forward pass
5920120	5925240	We forwarded the network and the loss now. We're ready to do the backward pass. So backward pass
5928040	5931800	We want to first make sure that all the gradients are reset. So they're at zero
5932440	5935880	Now in pi torch, you can set the gradients to be zero
5935880	5939880	But you can also just set it to none and setting it to none is more efficient
5940200	5945240	And pi torch will interpret none as like a lack of a gradient and it's the same as zeros
5945800	5948440	So this is a way to set to zero the gradient
5950440	5952440	And now we do lost up backward
5954680	5958280	Before we do lost up backward, we need one more thing if you remember from micrograd
5958920	5960920	pi torch actually requires
5961320	5963880	That we pass in requires grad is true
5964840	5966840	Uh, so that we tell
5967000	5972760	Pi torch that we are interested in calculating gradients for this leaf tensor by default. This is false
5973480	5975480	So let me recalculate with that
5975880	5978360	And then set to none and lost up backward
5980680	5983480	Now something magical happened when lost up backward was run
5984440	5988520	Because pi torch just like micrograd when we did the forward pass here
5989080	5994200	It keeps track of all the operations under the hood. It builds a full computational graph
5994760	5999800	Just like the graphs we've produced in micrograd those graphs exist inside pi torch
6000760	6004440	And so it knows all the dependencies and all the mathematical operations of everything
6005000	6008760	And when you then calculate the loss we can call a dot backward on it
6009560	6013000	And dot backward then fills in the gradients of
6013640	6017240	All the intermediates all the way back to w's
6017800	6021480	Which are the parameters of our neural net. So now we can do w dot grad
6022360	6024920	And we see that it has structure there's stuff inside it
6029160	6032120	And these gradients every single element here
6033400	6036200	So w dot shape is 27 by 27
6036840	6039800	W grads shape is the same 27 by 27
6040680	6042520	And every element of w dot grad
6043160	6044440	is telling us
6044520	6048040	The influence of that weight on the loss function
6048760	6051080	So for example this number all the way here
6051960	6054760	If this element the zero zero element of w
6055560	6062280	Because the gradient is positive it's telling us that this has a positive influence on the loss slightly nudging
6063160	6064440	w
6064440	6066440	slightly taking w zero zero
6067000	6069640	And adding a small h to it
6070360	6074680	Would increase the loss mildly because this gradient is positive
6075640	6077640	Some of these gradients are also negative
6078600	6083000	So that's telling us about the gradient information and we can use this gradient information
6083400	6087800	To update the weights of this neural network. So let's not do the update
6088280	6090360	It's going to be very similar to what we had in micrograd
6090760	6094600	We need no loop over all the parameters because we only have one parameter
6095320	6100360	Tensor and that is w. So we simply do w dot data plus equals
6101400	6105240	The we can actually copy this almost exactly negative zero point one times
6106120	6108120	w dot grad
6108200	6109480	um
6109480	6112760	And that would be the update to the tensor
6114520	6116520	So that updates the tensor
6118760	6123480	And because the tensor is updated we would expect that now the loss should decrease
6124360	6127480	So here if I print loss
6129480	6131160	That item
6131160	6133160	It was 3.76 right
6133160	6137800	So we've updated the w here. So if I recalculate forward pass
6139000	6142920	Loss now should be slightly lower. So 3.76 goes to
6143560	6145560	3.74
6145800	6150120	And then we can again set to set grad to none and backward
6150840	6152600	update
6152600	6154600	And now the parameters changed again
6154920	6159960	So if we recalculate the forward pass, we expect a lower loss again 3.72
6162200	6165640	Okay, and this is again doing the we're now doing reading the set
6168520	6175000	And when we achieve a low loss that will mean that the network is assigning high probabilities to the correct next characters
6175240	6178600	Okay, so I rearranged everything and I put it all together from scratch
6179400	6182280	So here is where we construct our data set of bigrams
6183240	6186040	You see that we are still iterating only over the first word emma
6186920	6188920	I'm going to change that in a second
6189080	6196120	I added a number that counts the number of elements in axis so that we explicitly see that number of examples is five
6196920	6199640	Because currently we're just working with emma. There's five bigrams there
6200600	6203240	And here I added a loop of exactly what we had before
6203720	6208360	So we had 10 iterations of gradient descent of forward pass backward pass and an update
6209000	6212120	And so running these two cells initialization and gradient descent
6212840	6214840	Gives us some improvement
6215400	6217400	on the last function
6218200	6220200	But now I want to use all the words
6221720	6225640	And there's not five but 228,000 bigrams now
6226680	6230440	However, this should require no modification whatsoever. Everything should just run
6230760	6237160	Because all the code we wrote doesn't care if there's five bigrams or 228,000 bigrams and with everything we should just work
6237320	6238440	So
6238440	6240440	You see that this will just run
6240440	6243800	But now we are optimizing over the entire training set of all the bigrams
6244680	6249960	And you see now that we are decreasing very slightly. So actually we can probably afford the larger learning rate
6252360	6254360	Can probably afford even larger learning rate
6260680	6267560	Even 50 seems to work on this very very simple example, right? So let me re-initialize and let's run 100 iterations
6269240	6271240	See what happens
6273000	6275000	Okay
6276280	6278280	We seem to be
6279080	6282200	Coming up to some pretty good losses here 2.47
6282840	6284600	Let me run 100 more
6284600	6286840	What is the number that we expect by the way in the loss?
6287240	6290680	We expect to get something around what we had originally actually
6292040	6295320	So all the way back if you remember in the beginning of this video when we
6295960	6297400	optimized
6297400	6298840	Just by counting
6298840	6300840	Our loss was roughly 2.47
6301560	6303560	After we added smoothing
6303560	6305800	But before smoothing we had roughly 2.45
6306600	6308360	likely it
6308360	6309720	Sorry loss
6309720	6313320	And so that's actually roughly the vicinity of what we expect to achieve
6313800	6318600	But before we achieved it by counting and here we are achieving the roughly the same result
6318760	6320760	But with gradient based optimization
6321000	6325640	So we come to about 2.46 2.45, etc
6326280	6329720	And that makes sense because fundamentally we're not taking any additional information
6329880	6333080	We're still just taking in the previous character and trying to predict the next one
6333640	6337080	But instead of doing it explicitly by counting and normalizing
6338200	6342360	We are doing it with gradient based learning and it just so happens that the explicit approach
6342680	6348120	Happens to very well optimize the loss function without any need for gradient based optimization
6348520	6352600	Because the setup for bi-gram language models are is so straightforward. It's so simple
6352920	6357720	We can just afford to estimate those probabilities directly and maintain them in a table
6358920	6362200	But the gradient based approach is significantly more flexible
6362920	6365320	so we've actually gained a lot because
6366680	6368680	What we can do now is
6369240	6372280	We can expand this approach and complexify the neural net
6372840	6377400	So currently we're just taking a single character and feeding into a neural net and the neural is extremely simple
6377800	6380040	But we're about to iterate on this substantially
6380440	6387080	We're going to be taking multiple previous characters and we're going to be feeding them into increasingly more complex neural nets
6387480	6391400	But fundamentally out the output of the neural net will always just be logits
6392680	6395160	And those logits will go through the exact same transformation
6395560	6397560	We're going to take them through a softmax
6397960	6403080	Calculate the loss function and the negative log likelihood and do gradient based optimization
6403640	6409000	And so actually as we complexify the neural nets and work all the way up to transformers
6409800	6411800	None of this will really fundamentally change
6411960	6415000	None of this will fundamentally change the only thing that will change is
6415640	6422360	The way we do the forward pass or we've taken some previous characters and calculate logits for the next character in a sequence
6422920	6424920	that will become more complex
6425160	6428200	And that will use the same machinery to optimize it
6428920	6430680	and
6430680	6432680	It's not obvious how we would have extended
6433160	6434920	This bygram approach
6434920	6437160	Into the case where there are many more
6437480	6443800	Characters at the input because eventually these tables would get way too large because there's way too many combinations
6444200	6446200	Of what previous characters
6446200	6447880	could be
6447880	6451720	If you only have one previous character we can just keep everything in a table the counts
6452040	6457080	But if you have the last 10 characters that are in but we can't actually keep everything in the table anymore
6457400	6462600	So this is fundamentally an unscalable approach and the neural network approach is significantly more scalable
6463080	6468280	And it's something that actually we can improve on over time. So that's where we will be digging next
6468520	6470520	I wanted to point out two more things
6471160	6472280	number one
6472280	6474280	I want you to notice that this
6475080	6476760	x-ank here
6476760	6482280	This is made up of one-hot vectors and then those one-hot vectors are multiplied by this w matrix
6483240	6487880	And we think of this as a multiple neurons being forwarded in a fully connected manner
6488680	6490840	But actually what's happening here is that for example
6491960	6497000	If you have a one-hot vector here that has a one at say the fifth dimension
6497720	6500200	Then because of the way the matrix multiplication works
6501160	6506680	Multiplying that one-hot vector with w actually ends up plucking out the fifth row of w
6507640	6510520	Lot logits would become just the fifth row of w
6511320	6514440	And that's because of the way the matrix multiplication works
6517000	6519240	So that's actually what ends up happening
6520120	6524920	So but that's actually exactly what happened before because remember all the way up here
6525560	6527480	We have a bi-gram
6527560	6534200	We took the first character and then that first character indexed into a row of this array here
6534920	6538120	And that row gave us the probability distribution for the next character
6538600	6541800	So the first character was used as a lookup into a
6543640	6545640	Matrix here to get the probability distribution
6546280	6550120	Well, that's actually exactly what's happening here because we're taking the index
6550600	6552920	We're encoding it as one-hot and multiplying it by w
6553480	6555880	So logits literally becomes the
6558040	6562360	The appropriate row of w and that gets just as before
6563160	6566840	Exponentiated to create the counts and then normalized and becomes probability
6567480	6570280	So this w here is literally
6571400	6573800	The same as this array here
6575080	6581160	But w remember is the log counts not the counts. So it's more precise to say that w
6581400	6585400	Exponentiated w.exp is this array
6586200	6588680	But this array was filled in by counting
6589320	6591320	and by basically
6591960	6595640	Populating the counts of bi-grams whereas in the gradient based framework
6595880	6598760	We initialize it randomly and then we let the loss
6599400	6602440	Guide us to arrive at the exact same array
6603240	6605240	So this array exactly here
6605800	6606760	is
6606840	6611400	Basically the array w at the end of optimization except we arrived at it
6612280	6614280	piece by piece by following the loss
6615000	6619880	And that's why we also obtained the same loss function at the end and the second note is if I come here
6620520	6625480	remember the smoothing where we added fake counts to our counts in order to
6626040	6630280	Smooth out and make more uniform the distributions of these probabilities
6631000	6633640	And that prevented us from assigning zero probability to
6634360	6636360	Um to any one bi-gram
6637240	6639560	Now if I increase the count here
6640280	6642280	What's happening to the probability?
6642840	6647240	As I increase the count probability becomes more and more uniform
6647960	6651480	Right because these counts go only up to like 900 or whatever
6651480	6654600	So if I'm adding plus a million to every single number here
6655160	6658840	You can see how uh the row and its probability then when we divide
6659080	6663960	Is just going to become more and more close to exactly even probability uniform distribution
6665160	6669560	It turns out that the gradient based framework has an equivalent to smoothing
6670760	6672760	In particular
6673160	6675160	Think through these w's here
6675880	6677880	Which we initialized randomly
6678520	6681320	We could also think about initializing w's to be zero
6682120	6684120	If all the entries of w are zero
6685960	6688120	Then you'll see that logits will become all zero
6688840	6691160	And then exponentiating those logits becomes all one
6692120	6694840	And then the probabilities turn out to be exactly uniform
6695720	6700040	So basically when w's are all equal to each other or say especially zero
6701240	6703480	Then the probabilities come out completely uniform
6704440	6705480	So
6705480	6708360	Trying to incentivize w to be near zero
6709240	6711240	Is basically equivalent
6711240	6718040	To label smoothing and the more you incentivize that in a loss function the more smooth distribution you're going to achieve
6718920	6721320	So this brings us to something that's called regularization
6721960	6727720	Where we can actually augment the loss function to have a small component that we call a regularization loss
6729000	6733720	In particular what we're going to do is we can take w and we can for example square all of its entries
6734680	6736680	And then we can um oops
6737800	6739000	Sorry about that
6739000	6741400	We can take all the entries of w and we can sum them
6743640	6747160	And because we're squaring uh, there will be no signs anymore
6747160	6748360	Um
6748360	6750920	negatives and positives all get squashed to be positive numbers
6751480	6756600	And then the way this works is you achieve zero loss if w is exactly or zero
6757160	6760280	But if w has non-zero numbers you accumulate loss
6761160	6764200	And so we can actually take this and we can add it on here
6764920	6767800	So we can do something like loss plus
6768920	6770440	w square
6770440	6771960	dot sum
6771960	6776360	Or let's actually instead of some let's take a mean because otherwise the sum gets too large
6777480	6779480	So mean is like a little bit more manageable
6781320	6787960	And then we have a regularization loss here like say 0.01 times or something like that you can choose the regularization strength
6789320	6791320	And then we can just optimize this
6792120	6797880	And now this optimization actually has two components not only is it trying to make all the probabilities work out
6798280	6803400	But in addition to that there's an additional component that simultaneously tries to make all w's be zero
6803880	6810040	Because if w's are non-zero you feel a loss and so minimizing this the only way to achieve that is for w to be zero
6810680	6817320	And so you can think of this as adding like a spring force or like a gravity force that that pushes w to be zero
6817800	6820920	So w wants to be zero and the probabilities want to be uniform
6821400	6826680	But they also simultaneously want to match up your your probabilities as indicated by the data
6827480	6830200	And so the strength of this regularization
6830760	6832760	is exactly controlling
6832760	6834440	the amount of counts
6834440	6836440	that you add here
6837240	6839240	Adding a lot more counts
6839400	6840840	here
6840840	6842840	corresponds to
6842840	6844680	Increasing this number
6844680	6849080	Because the more you increase it the more this part of the loss function dominates this part
6849560	6854680	And the more these these weights will be unable to grow because as they grow
6855480	6857480	They accumulate way too much loss
6858440	6860440	And so if this is strong enough
6861240	6865640	Then we are not able to overcome the force of this loss and we will never
6866760	6868760	And basically everything will be uniform predictions
6869400	6872440	So I thought that's kind of cool. Okay, and lastly before we wrap up
6873160	6876040	I wanted to show you how you would sample from this neural net model
6876920	6879800	And I copy pasted the sampling code from before
6880760	6883640	Where remember that we sampled five times
6884280	6889640	And all we did is we started zero we grabbed the current ix row of p
6890440	6892440	And that was our probability row
6892440	6898120	From which we sampled the next index and just accumulated that and break when zero
6898920	6901880	And running this gave us these results
6903880	6907880	I still have the p in memory. So this is fine now
6909160	6913480	The speed doesn't come from the row of p instead. It comes from this neural net
6915000	6917000	First we take ix
6917080	6919960	And we encode it into a one-hot row
6920440	6922440	of xank
6922440	6924440	This xank multiplies our w
6925160	6929880	Which really just plugs out the row of w corresponding to ix really that's what's happening
6930440	6934200	And that gets our logits and then we normalize those logits
6934920	6940680	Exponentiate to get counts and then normalize to get the distribution and then we can sample from the distribution
6941320	6943320	So if I run this
6945240	6949880	Kind of anti-climatic or climatic depending how you look at it, but we get the exact same result
6951160	6952280	Um
6952280	6956360	And that's because this is in the identical model. Not only does it achieve the same loss
6957000	6958120	but
6958120	6963800	As I mentioned, these are identical models and this w is the log counts of what we've estimated before
6964200	6969080	But we came to this answer in a very different way and it's got a very different interpretation
6969400	6973560	But fundamentally, this is basically the same model and gives the same samples here. And so
6974840	6977480	That's kind of cool. Okay, so we've actually covered a lot of ground
6978040	6981240	We introduced the bi-gram character level language model
6982040	6987640	We saw how we can train the model how we can sample from the model and how we can evaluate the quality of the model
6987880	6989880	Using the negative log likelihood loss
6990200	6995560	And then we actually trained the model in two completely different ways that actually get the same result and the same model
6996280	7000840	In the first way, we just counted up the frequency of all the bi-grams and normalized
7001480	7007160	In the second way, we used the uh negative log likelihood loss as a guide
7007720	7009720	To optimizing the counts matrix
7010840	7015480	Or the counts array so that the loss is minimized in the in a gradient based framework
7015800	7017800	and we saw that both of them give the same result
7018440	7020360	and
7020360	7021400	That's it
7021400	7024440	Now the second one of these the gradient based framework is much more flexible
7024920	7027400	And right now our neural network is super simple
7027560	7033400	We're taking a single previous character and we're taking it through a single linear layer to calculate the logits
7034120	7039720	This is about to complexify. So in the follow-up videos, we're going to be taking more and more of these characters
7040520	7042440	And we're going to be feeding them into a neural net
7042920	7047080	But this neural net will still output the exact same thing. The neural net will output logits
7048040	7052760	And these logits will still be normalized in the exact same way and all the loss and everything else and the gradient
7052840	7057720	Gradient based framework everything stays identical. It's just that this neural net will now
7058140	7060140	Complexify all the way to transformers
7060140	7064300	So that's going to be pretty awesome and i'm looking forward to it for now. Bye
