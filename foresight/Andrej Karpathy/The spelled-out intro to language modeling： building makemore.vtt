WEBVTT

00:00.000 --> 00:05.360
Hi everyone, hope you're well and next up what I'd like to do is I'd like to build out make more

00:05.880 --> 00:10.580
Like micrograd before it make more is a repository that I have on my github web page

00:11.040 --> 00:14.000
You can look at it, but just like with micrograd

00:14.000 --> 00:17.640
I'm going to build it out step by step and I'm going to spell everything out

00:17.640 --> 00:19.640
So we're going to build it out slowly and together

00:19.980 --> 00:21.980
Now what is make more?

00:22.020 --> 00:27.020
Make more as the name suggests makes more of things that you give it

00:27.380 --> 00:32.080
So here's an example names.txt is an example data set to make more and

00:32.600 --> 00:37.100
When you look at names.txt you'll find that it's a very large data set of names

00:37.940 --> 00:39.940
so

00:40.060 --> 00:41.660
Here's lots of different types of names

00:41.660 --> 00:42.160
in fact

00:42.160 --> 00:47.200
I believe there are 32,000 names that I've sort of found randomly on the government website and

00:48.020 --> 00:53.720
If you train make more on this data set it will learn to make more of things like this

00:54.600 --> 01:01.680
And in particular in this case that will mean more things that sound name like but are actually unique names

01:01.680 --> 01:04.840
And maybe if you have a baby and you're trying to assign name

01:04.840 --> 01:08.760
Maybe you're looking for a cool new sounding unique name make more might help you

01:09.360 --> 01:15.260
So here are some example generations from the neural network once we train it on our data set

01:16.040 --> 01:20.280
So here's some example unique names that it will generate don't tell

01:21.280 --> 01:23.280
I rot

01:23.440 --> 01:29.160
Zendy and so on and so all these are sound name like but they're not of course names

01:30.440 --> 01:34.280
So under the hood make more is a character level language model

01:34.520 --> 01:40.920
So what that means is that is treating every single line here as an example and within each example

01:40.920 --> 01:44.600
It's treating them all as sequences of individual characters

01:44.800 --> 01:50.360
So R E E S E is this example and that's the sequence of characters

01:50.360 --> 01:56.600
And that's the level on which we are building out make more and what it means to be a character level language model

01:56.600 --> 02:02.600
Then is that it's just sort of modeling those sequences of characters and it knows how to predict the next character in the sequence

02:03.360 --> 02:06.200
Now we're actually going to implement a large number of

02:06.520 --> 02:11.880
Character level language models in terms of the neural networks that are involved in predicting the next character in a sequence

02:12.040 --> 02:15.000
So very simple by Graham and back of word models

02:15.360 --> 02:19.460
Multilevel perceptrons recurrent neural networks all the way to modern

02:19.800 --> 02:27.400
Transformers in fact a transformer that we will build will be basically the equivalent transformer to GPT2 if you have heard of GPT

02:27.840 --> 02:29.120
So that's kind of a big deal

02:29.120 --> 02:33.580
It's a modern network and by the end of the series you will actually understand how that works

02:34.080 --> 02:39.000
On the level of characters now to give you a sense of the extensions here

02:39.480 --> 02:46.760
After characters we will probably spend some time on the word level so that we can generate documents of words not just little you know segments of characters

02:47.320 --> 02:50.360
But we can generate entire large much larger documents

02:50.360 --> 02:54.040
And then we're probably going to go into images and image text

02:54.640 --> 02:59.240
Networks such as Dali stable diffusion and so on but for now we have to start

02:59.800 --> 03:02.680
Here character level language modeling. Let's go

03:03.200 --> 03:06.600
So like before we are starting with a completely blank GPTN notebook page

03:06.760 --> 03:10.840
The first thing is I would like to basically load up the data set names.txt

03:11.520 --> 03:14.480
So we're going to open up names.txt for reading and

03:15.400 --> 03:18.760
We're going to read in everything into a massive string and

03:19.680 --> 03:23.880
Then because it's a massive string we'd only like the individual words and put them in the list

03:24.240 --> 03:30.840
So let's call split lines on that string to get all of our words as a Python list of strings

03:31.800 --> 03:35.080
so basically we can look at for example the first 10 words and

03:35.800 --> 03:42.280
We have that it's a list of Emma Olivia Eva and so on and if we look at

03:43.720 --> 03:46.200
The top of the page here that is indeed what we see

03:47.080 --> 03:48.200
um

03:48.200 --> 03:49.640
So that's good

03:49.640 --> 03:51.640
This list actually makes me feel that

03:52.280 --> 03:54.280
This is probably sorted by frequency

03:55.640 --> 04:01.560
But okay, so these are the words now we'd like to actually like learn a little bit more about this data set

04:01.800 --> 04:05.400
Let's look at the total number of words. We expect this to be roughly 32 000

04:06.520 --> 04:08.520
And then what is the for example shortest word?

04:09.160 --> 04:11.000
so min of

04:11.000 --> 04:13.000
line of each word for w in words

04:13.640 --> 04:16.440
So the shortest word will be length

04:17.160 --> 04:18.280
two

04:18.280 --> 04:22.280
And max of land w for w in words. So the longest word will be

04:23.160 --> 04:24.680
15 characters

04:24.680 --> 04:26.840
So let's now think through our very first language model

04:27.400 --> 04:32.600
As I mentioned a character level language model is predicting the next character in a sequence given

04:33.080 --> 04:35.720
Already some concrete sequence of characters before it

04:36.600 --> 04:40.280
Now what we have to realize here is that every single word here like Isabella

04:40.840 --> 04:44.840
Is actually quite a few examples packed in to that single word

04:45.560 --> 04:50.600
Because what is a an existence of a word like Isabella in the data set telling us really it's saying that

04:51.160 --> 04:55.000
The character i is a very likely character to come first

04:55.560 --> 04:57.560
in a sequence of a name

04:58.600 --> 05:00.920
The character s is likely to come

05:01.800 --> 05:03.800
after i

05:04.360 --> 05:06.840
The character a is likely to come after is

05:07.640 --> 05:13.640
The character b is very likely to come after isa and someone all the way to a following Isabella

05:14.440 --> 05:18.360
And then there's one more example actually packed in here and that is that

05:19.080 --> 05:21.080
After there's Isabella

05:21.400 --> 05:23.400
The word is very likely to end

05:23.800 --> 05:28.840
So that's one more sort of explicit piece of information that we have here that we have to be careful with

05:29.640 --> 05:33.320
And so there's a lot packed into a single individual word in terms of the

05:33.800 --> 05:37.480
Statistical structure of what's likely to follow in these character sequences

05:38.120 --> 05:43.960
And then of course we don't have just an individual word. We actually have 32 000 of these and so there's a lot of structure here to model

05:44.920 --> 05:49.960
Now in beginning what I'd like to start with is I'd like to start with building a bi-gram language model

05:50.200 --> 05:55.960
Now in a bi-gram language model, we're always working with just two characters at a time

05:56.760 --> 06:03.000
So we're only looking at one character that we are given and we're trying to predict the next character in the sequence

06:03.960 --> 06:05.160
so

06:05.160 --> 06:11.400
What characters are likely to follow are what characters are likely to follow a and so on and we're just modeling that kind of a little

06:11.640 --> 06:12.920
local structure

06:12.920 --> 06:16.520
And we're forgetting the fact that we may have a lot more information

06:16.760 --> 06:19.800
We're always just looking at the previous character to predict the next one

06:20.200 --> 06:23.480
So it's a very simple and weak language model, but I think it's a great place to start

06:24.120 --> 06:30.360
So now let's begin by looking at these bi-grams in our data set and what they look like and these bi-grams again are just two characters in a row

06:30.920 --> 06:32.920
so for w in words

06:33.160 --> 06:35.560
each w here is an individual word a string

06:36.360 --> 06:38.360
we want to iterate uh for

06:39.640 --> 06:41.640
We want to iterate this word

06:41.720 --> 06:43.720
with consecutive characters

06:43.720 --> 06:46.360
So two characters at a time sliding it through the word

06:46.840 --> 06:50.920
Now a interesting nice way cute way to do this in python, by the way

06:51.320 --> 06:55.800
Is doing something like this for character one character two in zip of

06:56.440 --> 06:58.440
w and w at one

07:00.040 --> 07:01.800
one column

07:01.800 --> 07:02.920
print

07:02.920 --> 07:04.680
character one character two

07:04.680 --> 07:05.720
And let's not do all the words

07:05.720 --> 07:09.400
Let's just do the first three words and i'm going to show you in a second how this works

07:09.960 --> 07:14.120
But for now basically as an example, let's just do the very first word alone emma

07:15.400 --> 07:20.280
You see how we have a emma and this will just print em, mm, ma

07:21.000 --> 07:27.880
And the reason this works is because w is the string emma w at one column is the string mma

07:28.680 --> 07:33.000
And zip takes two iterators and it pairs them up

07:33.480 --> 07:36.840
And then creates an iterator over the tuples of their consecutive entries

07:37.400 --> 07:42.840
And if any one of these lists is shorter than the other then it will just uh halt and return

07:43.720 --> 07:49.320
So basically, that's why we return em, mm, mm, ma

07:49.960 --> 07:57.240
But then because this iterator second one here runs out of elements zip just ends and that's why we only get these tuples

07:57.720 --> 07:59.720
So pretty cute

07:59.720 --> 08:02.600
So these are the consecutive elements in the first word

08:03.080 --> 08:06.760
Now we have to be careful because we actually have more information here than just these three

08:07.320 --> 08:14.760
Examples as I mentioned we know that e is the is very likely to come first and we know that a in this case is coming last

08:15.640 --> 08:18.680
So one way to do this is basically we're going to create

08:19.480 --> 08:21.480
special array here

08:21.560 --> 08:23.080
characters

08:23.080 --> 08:27.000
And um, we're going to hallucinate a special start token here

08:28.440 --> 08:29.880
I'm going to

08:29.880 --> 08:31.880
call it like special start

08:32.600 --> 08:34.600
So this is a list of one element

08:34.600 --> 08:36.600
plus

08:36.600 --> 08:37.800
w

08:37.800 --> 08:40.360
And then plus a special end character

08:41.320 --> 08:45.320
And the reason i'm wrapping the list of w here is because w is a string mma

08:45.960 --> 08:49.720
List of w will just have the individual characters in the list

08:50.840 --> 08:56.920
And then doing this again now, but not iterating over w's but over the characters

08:58.200 --> 09:00.200
Will give us something like this

09:00.200 --> 09:05.880
So e is likely so this is a bygram of the start character and e and this is a bygram of the

09:06.440 --> 09:08.440
a and the special end character

09:09.080 --> 09:11.240
And now we can look at for example what this looks like for

09:12.040 --> 09:14.040
olivia or heva

09:14.520 --> 09:16.520
And indeed we can actually

09:16.520 --> 09:20.040
Potentially do this for the entire data set, but we won't print that that's going to be too much

09:20.680 --> 09:24.120
But these are the individual character bygrams and we can print them

09:25.000 --> 09:29.480
Now in order to learn the statistics about which characters are likely to follow other characters

09:29.800 --> 09:33.800
The simplest way in the bygram language models is to simply do it by counting

09:34.440 --> 09:39.720
So we're basically just going to count how often any one of these combinations occurs in the training set

09:40.280 --> 09:41.800
In these words

09:41.800 --> 09:46.840
So we're going to need some kind of a dictionary that's going to maintain some counts for every one of these bygrams

09:47.320 --> 09:49.320
So let's use a dictionary b

09:49.800 --> 09:52.360
And this will map these bygrams

09:52.680 --> 09:55.080
So bygram is a tuple of character one character two

09:56.200 --> 09:58.200
And then b at bygram

09:58.760 --> 10:00.760
Will be b dot get of bygram

10:01.080 --> 10:03.720
Which is basically the same as b at bygram

10:04.680 --> 10:10.600
But in the case that bygram is not in the dictionary b. We would like to by default return a zero

10:11.880 --> 10:13.080
Plus one

10:13.080 --> 10:17.560
So this will basically add up all the bygrams and count how often they occur

10:18.280 --> 10:20.280
Let's get rid of printing

10:20.280 --> 10:22.440
or rather

10:22.440 --> 10:25.960
Let's keep the printing and let's just inspect what b is in this case

10:26.920 --> 10:32.280
And we see that many bygrams occur just a single time this one allegedly occurred three times

10:33.000 --> 10:35.000
So a was an ending character three times

10:35.480 --> 10:40.600
And that's true for all of these words all of emma olivia and eva and with a

10:41.640 --> 10:44.040
Uh, so that's why this occurred three times

10:45.720 --> 10:48.520
Um now let's do it for all the words

10:51.240 --> 10:53.240
Oops, I should not have printed

10:53.960 --> 10:55.960
I meant to erase that

10:56.760 --> 10:58.760
Let's kill this

10:58.760 --> 11:00.600
Let's just run

11:00.600 --> 11:03.160
And now b will have the statistics of the entire data set

11:04.120 --> 11:07.560
So these are the counts across all the words of the individual bygrams

11:08.360 --> 11:11.880
And we could for example look at some of the most common ones and least common ones

11:12.760 --> 11:14.760
Um, this kind of grows in python

11:14.760 --> 11:18.840
But the way to do this the simplest way I like is we just use b dot items

11:19.560 --> 11:21.560
b dot items returns

11:21.800 --> 11:23.800
the tuples of

11:24.360 --> 11:30.040
Key value in this case the keys are the character bygrams and the values are the counts

11:30.920 --> 11:33.400
And so then what we want to do is we want to do um

11:35.720 --> 11:37.720
Sort it of this

11:38.440 --> 11:40.440
But by default sort is on the first

11:41.160 --> 11:43.160
um

11:43.560 --> 11:45.560
On the first item of a tuple

11:45.560 --> 11:49.880
But we want to sort by the values which are the second element of a tuple that is the key value

11:50.680 --> 11:52.680
So we want to use the key

11:53.080 --> 11:55.080
equals lambda

11:55.320 --> 11:57.320
That takes the key value

11:57.640 --> 12:03.640
And returns the key value at the one not at zero but at one which is the count

12:03.960 --> 12:05.960
So we want to sort by the count

12:07.160 --> 12:09.160
Of these elements

12:10.440 --> 12:12.440
And actually we want it to go backwards

12:12.680 --> 12:17.560
So here what we have is the bygram q and r occurs only a single time

12:18.520 --> 12:22.200
dz occurred only a single time and when we sort this the other way around

12:23.400 --> 12:25.880
We're going to see the most likely bygrams

12:26.360 --> 12:33.880
So we see that n was very often an ending character many many times and apparently n almost always follows an a

12:34.440 --> 12:36.440
And that's a very likely combination as well

12:37.160 --> 12:38.760
Um

12:38.760 --> 12:39.960
So

12:39.960 --> 12:43.640
This is kind of the individual counts that we achieve over the entire data set

12:45.000 --> 12:47.880
Now it's actually going to be significantly more convenient for us to

12:48.440 --> 12:52.760
Keep this information in a two-dimensional array instead of a python dictionary

12:53.720 --> 12:58.840
so we're going to store this information in a 2d array and

13:00.120 --> 13:04.760
The rows are going to be the first character of the bygram and the columns are going to be the second character

13:05.080 --> 13:11.960
And each entry in the two-dimensional array will tell us how often that first character follows the second character in the data set

13:12.680 --> 13:18.280
So in particular the array representation that we're going to use or the library is that of pytorch

13:18.840 --> 13:21.800
And pytorch is a deep learning neural network framework

13:22.280 --> 13:28.940
But part of it is also this torch dot tensor, uh, which allows us to create multi-dimensional arrays and manipulate them very efficiently

13:29.880 --> 13:33.400
So let's import pytorch, which you can do by import torch

13:34.760 --> 13:36.760
And then we can create arrays

13:37.560 --> 13:39.800
So let's create an array of zeros

13:40.680 --> 13:46.200
And we give it a um size of this array. Let's create a three by five array as an example

13:47.080 --> 13:48.600
and

13:48.600 --> 13:50.600
This is a three by five array of zeros

13:51.560 --> 13:56.440
And by default you'll notice a dot d type, which is short for data type is float 32

13:56.600 --> 13:58.840
So these are single precision floating point numbers

13:59.480 --> 14:05.000
Because we are going to represent counts. Let's actually use d type as torch dot in 32

14:06.040 --> 14:07.880
So these are

14:07.880 --> 14:09.880
32 bit integers

14:10.120 --> 14:13.640
So now you see that we have integer data inside this tensor

14:14.600 --> 14:16.600
Now tensors allow us to really, um

14:17.400 --> 14:20.120
Manipulate all the individual entries and do it very efficiently

14:20.760 --> 14:23.160
So for example, if we want to change this bit

14:23.720 --> 14:27.240
We have to index into the tensor and in particular here

14:27.640 --> 14:30.120
This is the first row and the

14:30.840 --> 14:38.120
Um, because it's zero indexed. So this is row index one and column index zero one two three

14:38.840 --> 14:42.360
So a at one comma three we can set that to one

14:43.720 --> 14:45.720
And then a we'll have a one over there

14:47.160 --> 14:51.480
We can of course also do things like this. So now a will be two over there

14:52.600 --> 14:53.880
Or three

14:53.880 --> 14:56.440
And also we can for example say a zero zero is five

14:57.320 --> 14:59.480
And then a we'll have a five over here

15:00.200 --> 15:02.760
So that's how we can index into the arrays

15:03.320 --> 15:05.800
Now, of course the array that we are interested in is much much bigger

15:06.280 --> 15:09.400
So for our purposes, we have 26 letters of the alphabet

15:09.960 --> 15:13.320
And then we have two special characters s and e

15:14.120 --> 15:18.600
So, uh, we want 26 plus two or 28 by 28 array

15:19.320 --> 15:23.000
And let's call it the capital n because it's going to represent sort of the counts

15:24.520 --> 15:26.520
Let me erase this stuff

15:26.760 --> 15:29.720
So that's the array that starts at zeros 28 by 28

15:30.440 --> 15:32.440
And now let's copy paste this

15:33.720 --> 15:36.120
Here but instead of having a dictionary b

15:37.000 --> 15:39.560
Which we're going to erase we now have an n

15:41.000 --> 15:44.920
Now the problem here is that we have these characters which are strings, but we have to now

15:45.880 --> 15:47.880
basically index into a

15:48.760 --> 15:54.440
Array and we have to index using integers. So we need some kind of a lookup table from characters to integers

15:55.320 --> 15:57.320
So let's construct such a character array

15:58.040 --> 16:02.120
And the way we're going to do this is we're going to take all the words which is a list of strings

16:02.840 --> 16:08.120
We're going to concatenate all of it into a massive string. So this is just simply the entire dataset as a single string

16:09.320 --> 16:13.960
We're going to pass this to the set constructor which takes this massive string

16:14.440 --> 16:18.280
And throws out duplicates because sets do not allow duplicates

16:18.920 --> 16:22.840
So set of this will just be the set of all the lowercase characters

16:23.080 --> 16:26.280
And there should be a total of 26 of them

16:28.680 --> 16:30.680
And now we actually don't want a set we want a list

16:32.680 --> 16:36.680
But we don't want a list sorted in some weird arbitrary way. We want it to be sorted

16:37.640 --> 16:39.640
from a to z

16:39.880 --> 16:41.880
So sorted list

16:41.880 --> 16:43.880
So those are our characters

16:45.640 --> 16:51.880
Now what we want is this lookup table as I mentioned. So let's create a special s2i. I will call it

16:53.480 --> 16:57.480
s is string or character and this will be an s2i mapping

16:58.840 --> 17:00.200
for

17:00.200 --> 17:03.080
Is in enumerate of these characters

17:04.360 --> 17:08.280
So enumerate basically gives us this iterator over the integer

17:08.760 --> 17:14.360
index and the actual element of the list and then we are mapping the character to the integer

17:15.240 --> 17:16.760
So s2i

17:16.760 --> 17:21.560
Is a mapping from a to 0 b to 1 etc all the way from z to 25

17:23.800 --> 17:29.080
And that's going to be useful here, but we actually also have to specifically set that s will be 26

17:29.800 --> 17:31.800
And s2i at e

17:32.120 --> 17:34.520
Will be 27 right because z was 25

17:36.040 --> 17:39.400
So those are the lookups and now we can come here and we can map

17:39.960 --> 17:42.200
Both character 1 and character 2 to their integers

17:42.840 --> 17:44.840
So this will be s2i character 1

17:45.320 --> 17:48.120
And ix2 will be s2i of character 2

17:49.480 --> 17:51.480
And now we should be able to

17:52.120 --> 17:56.760
Do this line, but using our array. So n at ix1 ix2

17:57.320 --> 18:02.120
This is the two-dimensional array indexing. I've shown you before and honestly just plus equals 1

18:03.000 --> 18:05.000
Because everything starts at zero

18:06.200 --> 18:08.200
So this should work

18:08.920 --> 18:12.200
And give us a large 28 by 28 array

18:13.080 --> 18:15.960
Of all these counts. So if we print n

18:16.920 --> 18:19.480
This is the array, but of course it looks ugly

18:19.800 --> 18:24.120
So let's erase this ugly mess and let's try to visualize it a bit more nicer

18:24.840 --> 18:27.880
So for that we're going to use a library called mathplotlib

18:28.840 --> 18:34.360
So mathplotlib allows us to create figures. So we can do things like pltim show of the count array

18:36.120 --> 18:38.280
So this is the 20 by 28 array

18:39.000 --> 18:43.240
And this is a structure, but even this I would say is still pretty ugly

18:43.880 --> 18:48.520
So we're going to try to create a much nicer visualization of it and I wrote a bunch of code for that

18:49.720 --> 18:51.720
The first thing we're going to need is

18:51.960 --> 18:53.880
We're going to need to invert

18:53.880 --> 18:59.000
This array here this dictionary. So s2i is a mapping from s to i

18:59.800 --> 19:02.840
And in i2s, we're going to reverse this dictionary

19:03.080 --> 19:05.960
So it rid of all the items and just reverse that array

19:06.600 --> 19:11.640
So i2s maps inversely from 0 to a 1 to b etc

19:12.600 --> 19:14.200
So we'll need that

19:14.200 --> 19:18.040
And then here's the code that I came up with to try to make this a little bit nicer

19:20.440 --> 19:22.040
To create a figure

19:22.040 --> 19:23.560
We plot n

19:24.360 --> 19:29.240
And then we do and then we visualize a bunch of things later. Let me just run it so you get a sense of what this is

19:31.960 --> 19:34.200
Okay, so you see here that we have

19:35.320 --> 19:37.160
the array spaced out

19:37.240 --> 19:41.720
And every one of these is basically like b follows g zero times

19:42.360 --> 19:44.360
b follows h 41 times

19:45.240 --> 19:47.240
So a follows j 175 times

19:47.960 --> 19:52.360
And so what you can see that i'm doing here is first i show that entire array

19:52.920 --> 19:55.720
And then I iterate over all the individual little cells here

19:56.760 --> 19:58.760
And I create a character string here

19:59.320 --> 20:04.200
Which is the inverse mapping i2s of the integer i and the integer j

20:04.600 --> 20:07.480
So those are the bigrams in a character representation

20:08.600 --> 20:15.160
And then I plot just the bigram text and then I plot the number of times that this bigram occurs

20:16.040 --> 20:22.280
Now the reason that there's a dot item here is because when you index into these arrays, these are torch tensors

20:23.000 --> 20:25.320
You see that we still get a tensor back

20:26.040 --> 20:31.080
So the type of this thing you think it would be just an integer 149, but it's actually a torch dot tensor

20:31.960 --> 20:37.080
And so if you do dot item, then it will pop out that individual integer

20:38.440 --> 20:40.440
So it will just be 149

20:40.680 --> 20:44.280
So that's what's happening there. And these are just some options to make it look nice

20:45.240 --> 20:47.240
So what is the structure of this array?

20:49.160 --> 20:53.320
We have all these counts and we see that some of them occur often and some of them do not occur often

20:53.960 --> 20:57.880
Now if you scrutinize this carefully, you will notice that we're not actually being very clever

20:58.600 --> 21:00.600
That's because when you come over here

21:00.600 --> 21:04.120
You'll notice that for example, we have an entire row of completely zeros

21:04.680 --> 21:06.680
And that's because the end character

21:07.000 --> 21:13.400
Is never possibly going to be the first character of a bigram because we're always placing these end tokens all at the end of a bigram

21:14.360 --> 21:18.520
Similarly, we have entire column zeros here because the s

21:19.640 --> 21:27.080
Character will never possibly be the second element of a bigram because we always start with s and we end with e and we only have the words in between

21:27.640 --> 21:31.240
So we have an entire column of zeros an entire row of zeros

21:31.720 --> 21:33.720
And in this little two by two matrix here as well

21:34.040 --> 21:37.880
The only one that can possibly happen is if s directly follows e

21:38.600 --> 21:42.520
That can be non-zero if we have a word that has no letters

21:43.080 --> 21:46.840
So in that case, there's no letters in the word. It's an empty word and we just have s follows e

21:47.560 --> 21:49.560
But the other ones are just not possible

21:50.120 --> 21:55.000
And so we're basically wasting space and not only that but the s and the e are getting very crowded here

21:55.560 --> 22:01.720
I was using these brackets because there's convention and natural language processing to use these kinds of brackets to denote special

22:02.040 --> 22:03.240
tokens

22:03.240 --> 22:05.240
But we're going to use something else

22:05.240 --> 22:07.480
So let's fix all this and make it prettier

22:08.200 --> 22:12.200
We're not actually going to have two special tokens. We're only going to have one special token

22:13.000 --> 22:17.560
So we're going to have n by n array of 27 by set 27 instead

22:18.840 --> 22:22.920
Instead of having two we will just have one and I will call it a dot

22:23.640 --> 22:25.640
Okay

22:27.320 --> 22:29.320
Let me swing this over here

22:30.440 --> 22:35.640
Now one more thing that I would like to do is I would actually like to make this special character half position zero

22:36.200 --> 22:41.000
And I would like to offset all the other letters off. I find that a little bit more pleasing

22:41.640 --> 22:43.640
um, so

22:44.680 --> 22:48.760
We need a plus one here so that the first character which is a will start at one

22:48.760 --> 22:54.760
So s to i will now be a starts at one and dot is zero

22:55.880 --> 23:01.880
And uh i2s, of course, we're not changing this because i2s just creates a reverse mapping and this will work fine

23:02.200 --> 23:05.000
So one is a two is b zero is dot

23:06.520 --> 23:08.520
So we've reversed that here

23:09.080 --> 23:11.240
We have a dot and a dot

23:12.920 --> 23:15.880
This should work fine make sure I started zeros

23:16.680 --> 23:24.360
Count and then here we don't go up to 28 we go up to 27 and this should just work

23:30.840 --> 23:35.720
Okay, so we see that dot dot never happened. It's at zero because we don't have empty words

23:36.520 --> 23:39.640
Then this row here now is just very simply the

23:40.760 --> 23:43.800
Counts for all the first letters. So

23:44.680 --> 23:49.080
G j starts a word h starts a word i starts a word etc

23:49.480 --> 23:51.480
And then these are all the ending

23:51.880 --> 23:53.000
characters

23:53.000 --> 23:56.280
And in between we have the structure of what characters follow each other

23:57.000 --> 23:59.960
So this is the counts array of our entire

24:00.600 --> 24:01.640
Uh dataset

24:01.640 --> 24:07.000
So this array actually has all of the information necessary for us to actually sample from this bigram

24:07.560 --> 24:09.560
character level language model

24:09.720 --> 24:10.760
and

24:10.840 --> 24:15.320
Roughly speaking what we're going to do is we're just going to start following these probabilities and these counts

24:15.640 --> 24:18.200
And we're going to start sampling from the from model

24:18.840 --> 24:22.920
So in the beginning, of course, um, we start with the dot the start token

24:23.800 --> 24:29.400
Dot so to sample the first character of a name. We're looking at this row here

24:30.520 --> 24:38.600
So we see that we have the counts and those counts externally are telling us how often any one of these characters is to start a word

24:39.560 --> 24:43.640
So if we take this n and we grab the first row

24:44.760 --> 24:46.200
We can do that by

24:46.200 --> 24:52.680
using just indexing a zero and then using this notation column for the rest of that row

24:53.720 --> 24:55.720
so n zero column

24:56.520 --> 24:58.520
Is indexing into the zero?

24:58.840 --> 25:01.080
Row and then it's grabbing all the columns

25:01.960 --> 25:04.520
And so this will give us a one-dimensional array

25:05.240 --> 25:07.640
Of the first row. So zero four four ten

25:08.360 --> 25:12.040
You know zero four four ten one three oh six one five four two

25:12.520 --> 25:15.880
Etc. Just the first row the shape of this is

25:16.520 --> 25:18.520
27 it's just the row of 27

25:19.880 --> 25:23.160
And the other way that you can do this also is you just you don't actually give this

25:23.720 --> 25:26.920
You just grab the zero row like this. This is equal

25:28.120 --> 25:29.960
Now these are the counts

25:29.960 --> 25:34.200
And now what we'd like to do is we'd like to basically um sample from this

25:35.000 --> 25:38.120
Since these are the raw counts, we actually have to convert this to probabilities

25:39.160 --> 25:41.560
So we create a probability vector

25:42.920 --> 25:44.920
So we'll take n of zero

25:45.000 --> 25:47.480
And we'll actually convert this to float

25:48.280 --> 25:50.040
first

25:50.040 --> 25:52.120
Okay, so these integers are converted to float

25:52.840 --> 25:58.040
floating point numbers and the reason we're creating floats is because we're about to normalize these counts

25:58.840 --> 26:02.440
So to create a probability distribution here, we want to divide

26:02.680 --> 26:06.440
We basically want to do p p p divide p that sum

26:09.640 --> 26:13.400
And now we get a vector of smaller numbers and these are now probabilities

26:13.720 --> 26:18.200
So of course because we divided by the sum the sum of p now is one

26:18.760 --> 26:21.000
So this is a nice proper probability distribution

26:21.080 --> 26:25.480
It sums to one and this is giving us the probability for any single character to be the first

26:26.040 --> 26:28.040
character of a word

26:28.040 --> 26:32.120
So now we can try to sample from this distribution to sample from these distributions

26:32.120 --> 26:35.160
We're going to use torsion multinomial, which I've pulled up here

26:36.200 --> 26:38.440
So torsion multinomial returns a

26:40.520 --> 26:46.280
Samples from the multinomial probability distribution, which is a complicated way of saying you give me probabilities

26:46.360 --> 26:48.920
And I will give you integers which are sampled

26:49.480 --> 26:51.480
According to the probability distribution

26:51.560 --> 26:54.680
So this is the signature of the method and to make everything deterministic

26:54.760 --> 26:57.960
We're going to use a generator object in pi torch

26:59.320 --> 27:02.600
So this makes everything deterministic so when you run this on your computer

27:02.600 --> 27:06.280
You're going to the exact get the exact same results that i'm getting here on my computer

27:07.320 --> 27:09.320
So let me show you how this works

27:12.760 --> 27:17.320
Here's the deterministic way of creating a torch generator object

27:18.280 --> 27:20.520
Seeding it with some number that we can agree on

27:21.240 --> 27:24.200
So that seeds a generator gets gives us an object g

27:24.840 --> 27:29.000
And then we can pass that g to a function that creates

27:30.200 --> 27:34.520
Here random numbers torch.rand creates random numbers three of them

27:35.240 --> 27:38.680
And it's using this generator object to as a source of randomness

27:40.440 --> 27:41.960
So

27:41.960 --> 27:43.960
Without normalizing it

27:44.280 --> 27:46.280
I can just print

27:46.520 --> 27:52.440
This is sort of like numbers between zero and one that are random according to this thing and whenever I run it again

27:53.080 --> 27:57.720
I'm always going to get the same result because I keep using the same generator object, which i'm seeding here

27:58.760 --> 28:00.760
And then if I divide

28:01.720 --> 28:06.360
To normalize i'm going to get a nice probability distribution of just three elements

28:07.480 --> 28:12.360
And then we can use torsion multinomial to draw samples from it. So this is what that looks like

28:13.080 --> 28:17.480
Torsion multinomial will take the torch tensor

28:18.360 --> 28:20.360
of probability distributions

28:21.000 --> 28:23.240
Then we can ask for a number of samples like say 20

28:24.600 --> 28:28.120
Replacement equals true means that when we draw an element

28:28.840 --> 28:35.160
We will we can draw it and then we can put it back into the list of eligible indices to draw again

28:35.880 --> 28:40.760
And we have to specify replacement as true because by default for some reason it's false

28:41.560 --> 28:43.000
And I think

28:43.000 --> 28:45.000
You know, it's just something to be careful with

28:45.720 --> 28:50.920
And the generator is passed in here. So we are going to always get deterministic results the same results

28:51.240 --> 28:53.240
So if I run these two

28:53.800 --> 28:56.280
We're going to get a bunch of samples from this distribution

28:57.160 --> 29:03.240
Now you'll notice here that the probability for the first element in this tensor is 60

29:04.520 --> 29:09.400
So in these 20 samples, we'd expect 60 of them to be zero

29:09.400 --> 29:13.160
We'd expect 30 percent of them to be one

29:14.280 --> 29:16.760
And because the the element index two

29:17.480 --> 29:19.480
Has only 10 probability

29:19.480 --> 29:24.440
Very few of these samples should be two and indeed we only have a small number of twos

29:25.320 --> 29:27.320
And we can sample as many as we like

29:29.000 --> 29:31.000
And the more we sample the more

29:31.080 --> 29:34.680
These numbers should roughly have the distribution here

29:35.640 --> 29:39.080
So we should have lots of zeros half as many

29:41.560 --> 29:43.560
Once and we should have

29:44.120 --> 29:46.120
three times s few

29:46.200 --> 29:49.240
Oh, sorry s few ones and three times s few

29:50.440 --> 29:51.560
twos

29:51.560 --> 29:55.240
So you see that we have very few twos. We have some ones and most of them are zero

29:55.720 --> 29:57.880
So that's what torsion multinomial is doing

29:58.840 --> 30:00.840
for us here

30:01.080 --> 30:03.080
We are interested in this row. We've created this

30:05.640 --> 30:08.040
P here and now we can sample from it

30:09.640 --> 30:11.640
So if we use the same seed

30:12.840 --> 30:16.200
And then we sample from this distribution, let's just get one sample

30:18.200 --> 30:20.600
Then we see that the sample is say 13

30:22.760 --> 30:24.760
So this will be the index

30:25.240 --> 30:28.280
And let's you see how it's a tensor that wraps 13

30:28.760 --> 30:32.280
We again have to use dot item to pop out that integer

30:32.920 --> 30:35.720
And now index would be just the number 13

30:37.400 --> 30:44.520
And of course the um, we can do we can map the i2s of ix to figure out exactly which character

30:45.080 --> 30:47.080
We're sampling here. We're sampling m

30:48.040 --> 30:52.360
So we're saying that the first character is m in our generation

30:53.080 --> 30:55.080
And just looking at the row here

30:55.160 --> 30:59.240
m was drawn and you we can see that m actually starts a large number of words

31:00.120 --> 31:05.480
m started 2,500 words out of 32,000 words. So almost

31:06.280 --> 31:11.480
A bit less than 10 of the words start with m. So this is actually fairly likely character to draw

31:15.240 --> 31:19.240
So that would be the first character of our word and now we can continue to sample more characters

31:19.720 --> 31:22.040
Because now we know that m started

31:22.920 --> 31:24.760
m is already sampled

31:24.760 --> 31:30.120
So now to draw the next character, we will come back here and we will look for the row

31:31.000 --> 31:33.480
That starts with m. So you see m

31:34.600 --> 31:36.600
And we have a row here

31:36.680 --> 31:38.680
so we see that m dot is

31:39.560 --> 31:43.720
516 ma is this many mb is this many etc

31:43.800 --> 31:48.040
So these are the counts for the next row and that's the next character that we are going to now generate

31:48.600 --> 31:53.560
So I think we are ready to actually just write out the loop because I think you're starting to get a sense of how this is going to go

31:54.520 --> 31:56.280
The um

31:56.280 --> 32:00.600
We always begin at index zero because that's the start token

32:02.360 --> 32:04.360
And then while true

32:04.920 --> 32:07.720
We're going to grab the row corresponding to index

32:08.440 --> 32:10.440
That we're currently on so that's p

32:11.160 --> 32:13.560
So that's n array at ix

32:14.440 --> 32:16.440
converted to float is rp

32:19.160 --> 32:22.680
Then we normalize the speed to sum to one

32:24.280 --> 32:27.480
I accidentally ran the infinite loop

32:28.120 --> 32:30.120
We normalize p to sum to one

32:30.680 --> 32:32.680
Then we need this generator object

32:33.800 --> 32:38.040
And we're going to initialize up here and we're going to draw a single sample from this distribution

32:40.840 --> 32:44.680
And then this is going to tell us what index is going to be next

32:46.600 --> 32:51.480
If the index sampled is zero, then that's now the nth token

32:51.560 --> 32:53.560
So we will break

32:55.400 --> 32:59.320
Otherwise we are going to print s2i of ix

33:02.280 --> 33:04.280
i2s of ix

33:05.320 --> 33:09.080
And uh, that's pretty much it. We're just uh, this should work

33:10.360 --> 33:12.120
Okay more

33:12.120 --> 33:19.240
So that's the that's the name that we've sampled we started with m the next step was o then r and then dot

33:21.480 --> 33:24.040
And this dot we printed here as well

33:24.840 --> 33:26.440
so

33:26.440 --> 33:28.440
Let's now do this a few times

33:28.600 --> 33:30.040
um

33:30.040 --> 33:32.040
So let's actually create an

33:33.480 --> 33:35.480
Out list here

33:37.160 --> 33:41.800
And instead of printing we're going to append so out that append this character

33:44.440 --> 33:50.840
And then here let's just print it at the end. So let's just join up all the outs and we're just going to print more

33:51.480 --> 33:54.360
Okay, now we're always getting the same result because of the generator

33:55.160 --> 33:59.320
So if we want to do this a few times we can go for high and range

34:00.120 --> 34:02.120
10 we can sample 10 names

34:02.520 --> 34:04.520
And we can just do that 10 times

34:05.720 --> 34:07.720
And these are the names that we're getting out

34:08.520 --> 34:10.520
Let's do 20

34:14.280 --> 34:16.280
I'll be honest with you, this doesn't look right

34:16.520 --> 34:19.720
So I started a few minutes to convince myself that it actually is right

34:20.520 --> 34:26.920
The reason these samples are so terrible is that by gram language model is actually looks just like really terrible

34:27.800 --> 34:29.800
We can generate a few more here

34:30.040 --> 34:35.000
And you can see that they're kind of like their name like a little bit like keanu iraily etc

34:35.560 --> 34:37.560
But they're just like totally messed up

34:38.600 --> 34:42.600
And I mean the reason that this is so bad like we're generating h as a name

34:43.000 --> 34:46.040
But you have to think through it from the model's eyes

34:46.440 --> 34:51.640
It doesn't know that this h is the very first h. All it knows is that h was previously

34:52.120 --> 34:55.160
And now how likely is h the last character?

34:55.640 --> 34:59.240
Well, it's somewhat likely and so it just makes it last character

34:59.240 --> 35:03.320
It doesn't know that there were other things before it or there were not other things before it

35:03.880 --> 35:07.400
And so that's why it's generating all these like nonsense names

35:08.120 --> 35:10.120
in other ways to do this is

35:11.960 --> 35:15.400
To convince yourself that this is actually doing something reasonable even though it's so terrible

35:16.120 --> 35:17.720
is

35:17.720 --> 35:21.800
These little p's here are 27 right like 27

35:23.160 --> 35:25.160
So how about if we did something like this?

35:26.280 --> 35:28.280
Instead of p having any structure whatsoever

35:28.920 --> 35:32.360
How about if p was just a torch dot ones?

35:34.920 --> 35:36.920
Of 27

35:37.240 --> 35:41.080
By default, this is a float 32. So this is fine divide 27

35:41.720 --> 35:48.600
So what I'm doing here is this is the uniform distribution, which will make everything equally likely

35:49.880 --> 35:53.240
And we can sample from that. So let's see if that does any better

35:54.200 --> 36:00.120
Okay, so it's this is what you have from a model that is completely untrained where everything is equally likely

36:00.600 --> 36:06.360
So it's obviously garbage and then if we have a trained model, which is trained on just by grams

36:07.160 --> 36:12.360
This is what we get. So you can see that it is more name like it is actually working. It's just

36:14.120 --> 36:16.200
By gram is so terrible and we have to do better

36:16.600 --> 36:19.560
Now next I would like to fix an inefficiency that we have going on here

36:20.280 --> 36:25.640
Because what we're doing here is we're always fetching a row of n from the counts matrix up ahead

36:26.520 --> 36:27.960
And then we're always doing the same things

36:27.960 --> 36:32.440
We're converting to float and we're dividing and we're doing this every single iteration of this loop

36:32.680 --> 36:36.840
And we just keep renormalizing these rows over and over again and it's extremely inefficient and wasteful

36:37.320 --> 36:41.240
So what I'd like to do is I'd like to actually prepare a matrix capital p

36:41.640 --> 36:43.640
That will just have the probabilities in it

36:43.880 --> 36:47.800
So in other words is going to be the same as the capital n matrix here of counts

36:47.960 --> 36:50.920
But every single row will have the row of probabilities

36:51.560 --> 36:55.560
That is normalized to 1 indicating the probability distribution for the next character

36:56.040 --> 36:58.040
Given the character before it

36:58.520 --> 37:00.520
As defined by which row we're in

37:01.480 --> 37:04.600
So basically what we'd like to do is we'd like to just do it up front here

37:05.000 --> 37:07.160
And then we would like to just use that row here

37:08.120 --> 37:12.120
So here we would like to just do p equals p of ix instead

37:12.920 --> 37:14.920
okay

37:14.920 --> 37:16.920
The other reason I want to do this is not just for efficiency

37:17.000 --> 37:20.680
But also I would like us to practice these n-dimensional tensors

37:21.000 --> 37:26.280
And I'd like us to practice their manipulation and especially something that's called broadcasting that we'll go into in a second

37:26.920 --> 37:30.200
We're actually going to have to become very good at these tensor manipulations

37:30.520 --> 37:34.840
Because if we're going to build out all the way to transformers, we're going to be doing some pretty complicated

37:35.480 --> 37:40.120
array operations for efficiency, and we need to really understand that and be very good at it

37:42.200 --> 37:45.400
So intuitively what we want to do is we first want to grab the floating point

37:45.960 --> 37:47.960
copy of n

37:48.200 --> 37:50.200
And I'm mimicking the line here basically

37:50.920 --> 37:55.080
And then we want to divide all the rows so that they sum to 1

37:55.720 --> 37:58.840
So we'd like to do something like this p divide p dot sum

38:00.600 --> 38:04.360
But now we have to be careful because p dot sum actually

38:05.400 --> 38:07.400
produces a sum

38:08.040 --> 38:12.520
Sorry p equals n dot float copy p dot sum produces a

38:15.080 --> 38:17.640
Summs up all of the counts of this entire matrix n

38:18.280 --> 38:20.760
And gives us a single number of just the summation of everything

38:21.240 --> 38:27.080
So that's not the way we want to divide we want to simultaneously and in parallel divide all the rows

38:27.640 --> 38:29.640
by their respective sums

38:30.600 --> 38:34.760
So what we have to do now is we have to go into documentation for torch dot sum

38:35.800 --> 38:38.600
And we can scroll down here to a definition that is relevant to us

38:38.680 --> 38:43.000
Which is where we don't only provide an input array that we want to sum

38:43.320 --> 38:46.440
But we also provide the dimension along which we want to sum

38:47.240 --> 38:49.240
And in particular we want to sum up

38:49.880 --> 38:51.880
Over rows, right

38:52.360 --> 38:56.840
Now one more argument that I want you to pay attention to here is the keep them is false

38:57.800 --> 38:59.720
If keep them is true

38:59.720 --> 39:04.520
Then the output tensor is of the same size as input except of course the dimension along which you summed

39:04.760 --> 39:06.760
Which will become just one

39:07.320 --> 39:10.360
But if you pass in keep them as false

39:12.040 --> 39:14.040
Then this dimension is squeezed out

39:14.360 --> 39:18.600
And so torch dot sum not only does the sum and collapses dimension to be of size one

39:18.840 --> 39:23.320
But in addition it does what's called a squeeze where it squeezes out it squeezes out that dimension

39:23.480 --> 39:29.400
So basically what we want here is we instead want to do p dot sum of sum axis

39:30.760 --> 39:34.200
And in particular notice that p dot shape is 27 by 27

39:35.560 --> 39:41.480
So when we sum up across axis zero, then we would be taking the zero dimension and we would be summing across it

39:42.600 --> 39:44.600
So when keep them is true

39:45.000 --> 39:48.440
Then this thing will not only give us the counts across

39:49.000 --> 39:50.120
um

39:50.120 --> 39:51.960
along the columns

39:51.960 --> 39:56.520
But notice that basically the shape of this is one by 27. We just get a row vector

39:57.400 --> 40:00.680
And the reason we get a row vector here again is because we pass in zero dimension

40:00.920 --> 40:03.960
So this zero dimension becomes one and we've done a sum

40:04.840 --> 40:07.320
And we get a row and so basically we've done the sum

40:08.120 --> 40:09.400
this way

40:09.400 --> 40:12.280
Vertically and arrived at just a single one by 27

40:13.000 --> 40:15.000
vector of counts

40:15.320 --> 40:17.320
What happens when you take out keep them

40:17.720 --> 40:25.800
Is that we just get 27 so it squeezes out that dimension and we just get a one dimensional vector of size 27

40:28.600 --> 40:30.600
Now we don't actually want

40:31.320 --> 40:36.280
One by 27 row vector because that gives us the counts or the sums across

40:37.560 --> 40:39.480
the columns

40:39.480 --> 40:42.200
We actually want to sum the other way along dimension one

40:42.680 --> 40:47.000
And you'll see that the shape of this is 27 by one. So it's a column vector

40:47.320 --> 40:49.320
It's a 27 by one

40:49.960 --> 40:51.960
vector of counts

40:52.840 --> 40:59.400
Okay, and that's because what's happened here is that we're going horizontally and this 27 by 27 matrix becomes a

41:00.040 --> 41:02.040
27 by one array

41:03.480 --> 41:05.480
Now you'll notice by the way that um

41:06.200 --> 41:08.200
the actual numbers

41:08.200 --> 41:10.200
Of these counts are identical

41:10.600 --> 41:14.600
And that's because this special array of counts here comes from bi-gram statistics

41:14.840 --> 41:17.160
And actually it just so happens by chance

41:17.720 --> 41:22.440
Or because of the way this array is constructed that this sums along the columns or along the rows

41:23.080 --> 41:25.080
Horizontally or vertically is identical

41:26.120 --> 41:30.520
But actually what we want to do in this case is we want to sum across the uh rows

41:31.320 --> 41:32.280
horizontally

41:32.280 --> 41:35.800
So what we want here is speed at some of one would keep them true

41:37.320 --> 41:41.720
27 by one column vector and now what we want to do is we want to divide by that

41:44.680 --> 41:47.640
Now we have to be careful here again. Is it possible to take

41:48.840 --> 41:52.360
What's a um p dot shape you see here is 27 by 27?

41:52.680 --> 41:59.560
Is it possible to take a 27 by 27 array and divide it by what is a 27 by one array?

42:01.320 --> 42:03.320
Is that an operation that you can do?

42:03.880 --> 42:07.800
And whether or not you can perform this operation is determined by what's called broadcasting rules

42:08.280 --> 42:11.080
So if you just search broadcasting semantics in torch

42:12.040 --> 42:17.480
You'll notice that there's a special definition for what's called broadcasting that uh for whether or not

42:18.280 --> 42:19.480
these two

42:19.480 --> 42:22.920
Arrays can be combined in a binary operation like division

42:23.880 --> 42:27.880
So the first condition is each tensor has at least one dimension, which is the case for us

42:28.520 --> 42:31.800
And then when iterating over the dimension sizes starting at the trailing dimension

42:32.360 --> 42:36.760
The dimension sizes must either be equal one of them is one or one of them does not exist

42:36.920 --> 42:43.640
Okay, so let's do that. We need to align the two arrays and their shapes

42:43.960 --> 42:47.320
Which is very easy because both of these shapes have two elements. So they're aligned

42:48.040 --> 42:51.480
Then we iterate over from the from the right and going to the left

42:52.360 --> 42:57.400
Each dimension must be either equal one of them is a one or one of them does not exist

42:57.880 --> 43:01.320
So in this case, they're not equal, but one of them is a one. So this is fine

43:01.880 --> 43:05.240
And then this dimension they're both equal. So this is fine

43:05.880 --> 43:11.000
So all the dimensions are fine and therefore the this operation is broadcastable

43:11.880 --> 43:13.880
So that means that this operation is allowed

43:14.520 --> 43:19.160
And what is it that these arrays do when you divide 27 by 27 by 27 by 1?

43:19.800 --> 43:24.840
What it does is that it takes this dimension one and it stretches it out it copies it

43:25.880 --> 43:27.320
to match

43:27.320 --> 43:29.000
27 here in this case

43:29.000 --> 43:35.640
So in our case it takes this column vector, which is 27 by 1 and it copies it 27 times

43:36.680 --> 43:37.800
to make

43:37.800 --> 43:43.480
These both be 27 by 27 internally you can think of it that way and so it copies those counts

43:44.200 --> 43:46.200
And then it does an element wise division

43:47.320 --> 43:53.640
Which is what we want because these counts we want to divide by them on every single one of these columns in this matrix

43:54.760 --> 43:57.080
So this actually we expect will normalize

43:57.720 --> 43:59.720
every single row

43:59.720 --> 44:04.760
And we can check that this is true by taking the first row for example and taking it some

44:05.320 --> 44:07.320
We expect this to be one

44:08.200 --> 44:10.200
Because it's now normalized

44:10.280 --> 44:12.280
And then we expect this now

44:12.680 --> 44:18.440
Because if we actually correctly normalize all the rows we expect to get the exact same result here. So let's run this

44:19.160 --> 44:21.160
It's the exact same result

44:21.320 --> 44:24.680
So this is correct. So now I would like to scare you a little bit

44:25.400 --> 44:29.800
You actually have to like I basically encourage you very strongly to read through broadcasting semantics

44:30.440 --> 44:33.800
And I encourage you to treat this with respect and it's not something to play

44:34.280 --> 44:37.960
Fast and loose with it's something to really respect really understand and look up

44:38.040 --> 44:43.400
Maybe some tutorials for broadcasting and practice it and be careful with it because you can very quickly run it to box

44:43.720 --> 44:45.720
Let me show you what I mean

44:47.160 --> 44:49.800
You see how here we have p. That's some of one keep them this true

44:50.520 --> 44:52.520
The shape of this is 27 by 1

44:52.920 --> 44:57.880
Let me take out this line just so we have the n and then we can see the counts

44:58.360 --> 45:01.640
We can see that this is a all the counts across all the

45:02.520 --> 45:03.480
rows

45:03.480 --> 45:05.640
And it's a 27 by 1 column vector, right?

45:07.080 --> 45:12.600
Now suppose that I tried to do the following but I erase keep them this true here

45:13.880 --> 45:16.760
What does that do if keep them is not true? It's false

45:17.080 --> 45:22.360
Then remember according to documentation it gets rid of this dimension one. It squeezes it out

45:22.920 --> 45:26.040
So basically we just get all the same counts the same result

45:26.520 --> 45:30.840
Except the shape of it is not 27 by 1. It is just 27 the one disappears

45:31.720 --> 45:33.720
But all the counts are the same

45:34.200 --> 45:37.320
So you'd think that this divide that

45:37.960 --> 45:39.960
would uh would work

45:40.040 --> 45:45.960
First of all, can we even uh write this and will it is it even is it even expected to run? Is it broadcastable?

45:46.280 --> 45:48.280
Let's determine if this result is broadcastable

45:49.160 --> 45:51.160
p dot summit one is shape

45:51.560 --> 45:55.640
Is 27 this is 27 by 27 so 27 by 27

45:57.720 --> 45:59.720
Broadcasting into 27

46:00.280 --> 46:01.720
so now

46:01.720 --> 46:05.720
rules of broadcasting number one align all the dimensions on the right done

46:06.280 --> 46:09.400
Now iteration over all the dimensions starting from the right going to the left

46:10.200 --> 46:12.200
All the dimensions must either be equal

46:13.000 --> 46:17.160
One of them must be one or one of them does not exist. So here they are all equal

46:17.720 --> 46:19.720
Here the dimension does not exist

46:19.960 --> 46:23.480
So internally what broadcasting will do is it will create a one here

46:24.280 --> 46:25.880
and then

46:25.880 --> 46:31.080
We see that one of them is a one and this will get copied and this will run this will broadcast

46:32.440 --> 46:35.320
Okay, so you'd expect this to work

46:37.320 --> 46:39.320
Because we we are um

46:39.880 --> 46:41.320
um

46:41.320 --> 46:45.720
This broadcasts and this we can divide this now if I run this you'd expect it to work but

46:46.600 --> 46:47.960
It doesn't

46:47.960 --> 46:51.800
Uh, you actually get garbage you get a wrong result because this is actually a bug

46:52.440 --> 46:54.440
This keep them for equals true

46:57.240 --> 46:59.240
Makes it work

47:00.600 --> 47:02.600
This is a bug

47:02.840 --> 47:08.200
In both cases we are doing the correct counts. We are summing up across the rows

47:09.320 --> 47:12.120
But keep them is saving us and making it work. So in this case

47:12.760 --> 47:17.960
I'd like you to encourage you to potentially like pause this video at this point and try to think about why this is buggy

47:18.360 --> 47:20.360
And why the keep them was necessary here

47:22.200 --> 47:23.080
Okay

47:23.080 --> 47:28.760
So the reason to do for this is I'm trying to hint it here when I was sort of giving you a bit of a hint on how this works

47:29.480 --> 47:31.480
this 27 vector

47:32.200 --> 47:34.200
Internally inside the broadcasting

47:34.200 --> 47:36.200
This becomes a one by 27

47:36.520 --> 47:39.000
And one by 27 is a row vector, right?

47:39.640 --> 47:42.600
And now we are dividing 27 by 27 by 1 by 27

47:43.160 --> 47:48.520
And torch will replicate this dimension. So basically, uh, it will take

47:49.720 --> 47:54.440
It will take this, uh row vector and it will copy it vertically now

47:55.400 --> 47:59.320
27 times so the 27 by 27 lines exactly an element wise divides

48:00.360 --> 48:02.920
And so basically what's happening here is um

48:04.360 --> 48:07.560
We're actually normalizing the columns instead of normalizing the rows

48:09.400 --> 48:15.640
So you can check that what's happening here is that p at zero, which is the first row of p that sum

48:16.280 --> 48:18.280
Is not one it's seven

48:18.600 --> 48:21.640
It is the first column as an example that sums to one

48:23.640 --> 48:29.320
So to summarize where does the issue come from the issue comes from the silent adding of a dimension here

48:29.720 --> 48:35.560
Because in broadcasting rules you align on the right and go from right to left and if dimension doesn't exist you create it

48:36.200 --> 48:37.880
So that's where the problem happens

48:37.880 --> 48:39.320
We still did the counts correctly

48:39.320 --> 48:45.160
We did the counts across the rows and we got the the counts on the right here as a column vector

48:45.560 --> 48:50.840
But because the keep things was true this this uh, this dimension was discarded and now we just have a vector of 27

48:51.480 --> 48:56.360
And because of broadcasting the way it works this vector of 27 suddenly becomes a row vector

48:57.000 --> 48:59.000
And then this row vector gets replicated

48:59.000 --> 49:03.320
Vertically and at every single point we are dividing by the by the count

49:04.280 --> 49:06.280
Uh in the opposite direction

49:07.400 --> 49:13.400
So, uh, so this thing just uh, doesn't work. You this needs to be keep them's equals true in this case

49:14.200 --> 49:15.560
so then

49:15.560 --> 49:18.440
Uh, then we have that p at zero is normalized

49:19.880 --> 49:23.160
And conversely the first column you'd expect to potentially not be normalized

49:24.600 --> 49:26.600
And this is what makes it work

49:27.720 --> 49:32.920
So pretty subtle and uh, hopefully this helps to scare you that you should

49:33.000 --> 49:35.640
Have a respect for broadcasting be careful. Check your work

49:36.440 --> 49:40.920
And uh, understand how it works under the hood and make sure that it's broadcasting in the direction that you like

49:41.240 --> 49:47.800
Otherwise, you're going to introduce very subtle bugs very hard to find bugs and uh, just be careful one more note to an efficiency

49:48.280 --> 49:53.800
We don't want to be doing this here because uh, this creates a completely new tensor that we store into p

49:54.360 --> 49:56.920
We prefer to use in place operations if possible

49:57.880 --> 50:01.400
So this would be an in-place operation has the potential to be faster

50:01.800 --> 50:05.560
It doesn't create new memory under the hood and then let's erase this

50:06.200 --> 50:07.960
We don't need it

50:07.960 --> 50:09.960
and let's also

50:11.000 --> 50:14.040
Um, just do fewer just so i'm not wasting space

50:14.680 --> 50:16.520
Okay, so we're actually in a pretty good spot now

50:17.000 --> 50:21.240
We trained a bi-gram language model and we trained it really just by counting

50:21.720 --> 50:26.760
Uh, how frequently any pairing occurs and then normalizing so that we get a nice probability

50:27.880 --> 50:30.600
So really these elements of this array p

50:31.160 --> 50:36.120
Are really the parameters of our bi-gram language model giving us and summarizing the statistics of these bi-grams

50:36.920 --> 50:41.080
So we trained a model and then we know how to sample from a model. We just iteratively

50:42.040 --> 50:45.960
Sampled the next character and uh feed it in each time and get a next character

50:47.000 --> 50:50.600
Now what i'd like to do is i'd like to somehow evaluate the quality of this model

50:51.080 --> 50:56.600
We'd like to somehow summarize the quality of this model into a single number. How good is it at predicting?

50:57.480 --> 50:59.000
the training set

50:59.000 --> 51:02.920
And as an example so in the training set we can evaluate now the training

51:03.400 --> 51:06.280
Loss and this training loss is telling us about

51:06.920 --> 51:10.680
Sort of the quality of this model in a single number just like we saw in micrograd

51:11.960 --> 51:15.160
So let's try to think through the quality of the model and how we would evaluate it

51:17.080 --> 51:19.880
Basically what we're going to do is we're going to copy paste this code

51:20.680 --> 51:22.680
That we previously used for counting

51:23.000 --> 51:24.200
Okay

51:24.200 --> 51:27.240
And let me just print these bi-grams first. We're going to use f strings

51:27.880 --> 51:31.480
And i'm going to print character one followed by character two. These are the bi-grams

51:31.960 --> 51:34.680
And then I don't want to do it for all the words. Let's just do first three words

51:35.960 --> 51:38.840
So here we have emma olivia and ava bi-grams

51:40.200 --> 51:47.320
Now what we'd like to do is we'd like to basically look at the probability that the model assigns to every one of these bi-grams

51:48.120 --> 51:50.600
So in other words, we can look at the probability, which is

51:51.160 --> 51:53.160
summarized in the matrix p

51:53.160 --> 51:55.160
of ix1, ix2

51:56.120 --> 51:59.000
And then we can print it here as probability

52:00.680 --> 52:03.720
And because these probabilities are way too large, let me percent

52:04.840 --> 52:06.840
our column 0.4f

52:06.840 --> 52:08.840
to like truncate it a bit

52:09.160 --> 52:14.520
So what do we have here, right? We're looking at the probabilities that the model assigns to every one of these bi-grams in the data set

52:15.240 --> 52:18.200
And so we can see some of them are four percent, three percent, etc

52:18.600 --> 52:20.680
Just to have a measuring stick in our mind, by the way

52:21.640 --> 52:28.360
We have 27 possible characters or tokens and if everything was equally likely, then you'd expect all these probabilities

52:28.920 --> 52:30.600
to be

52:30.600 --> 52:32.440
four percent roughly

52:32.440 --> 52:37.160
So anything above four percent means that we've learned something useful from these bi-gram statistics

52:37.560 --> 52:41.240
And you see that roughly some of these are four percent, but some of them are as high as 40 percent

52:41.880 --> 52:43.880
35 percent and so on

52:43.880 --> 52:49.400
So you see that the model actually assigned a pretty high probability to whatever's in the training set and so that that's a good thing

52:50.120 --> 52:52.120
Um, basically if you have a very good model

52:52.440 --> 52:57.560
You'd expect that these probabilities should be near one because that means that uh, your model is correctly predicting

52:57.640 --> 53:01.400
What's going to come next especially on the training set where you where you train your model

53:02.840 --> 53:10.120
So now we'd like to think about how can we summarize these probabilities into a single number that measures the quality of this model

53:11.720 --> 53:16.600
Now when you look at the literature into maximum likelihood estimation and statistical modeling and so on

53:17.240 --> 53:20.760
You'll see that what's typically used here is something called the likelihood

53:21.480 --> 53:24.680
And the likelihood is the product of all of these probabilities

53:25.800 --> 53:28.840
And so the product of all of these probabilities is the likelihood

53:29.240 --> 53:34.040
And it's really telling us about the probability of the entire data set assigned

53:35.080 --> 53:38.840
Assigned by the model that we've trained and that is a measure of quality

53:39.400 --> 53:42.600
So the product of these should be as high as possible

53:43.160 --> 53:48.280
When you are training the model and when you have a good model your product your product of these probabilities should be very high

53:49.400 --> 53:50.360
um

53:50.360 --> 53:53.960
Now because the product of these probabilities is an unwieldy thing to work with

53:54.280 --> 53:59.080
You can see that all of them are between zero and one. So your product of these probabilities will be a very tiny number

54:00.120 --> 54:00.920
um

54:00.920 --> 54:06.680
So for convenience what people work with usually is not the likelihood, but they work with what's called the log likelihood

54:07.880 --> 54:09.000
So

54:09.000 --> 54:12.040
The product of these is the likelihood to get the log likelihood

54:12.360 --> 54:14.360
We just have to take the log of the probability

54:15.000 --> 54:18.600
And so the log of the probability here. I have the log of x from zero to one

54:19.800 --> 54:23.800
The log is a you see here monotonic transformation of the probability

54:24.680 --> 54:26.680
Where if you pass in one

54:27.240 --> 54:28.840
You get zero

54:28.840 --> 54:31.640
So probability one gets your log probability of zero

54:32.280 --> 54:34.280
And then as you go lower and lower probability

54:34.520 --> 54:39.320
The log will grow more and more negative until all the way to negative infinity at zero

54:41.880 --> 54:46.440
So here we have a log prob, which is really just a torche dot log of probability

54:46.840 --> 54:49.400
Let's print it out to get a sense of what that looks like

54:50.040 --> 54:51.640
log prob

54:51.640 --> 54:53.640
also 0.4 f

54:54.840 --> 54:56.680
Okay

54:56.680 --> 55:01.240
So as you can see when we plug in numbers that are very close some of our higher numbers

55:01.320 --> 55:08.120
We get closer and closer to zero and then if we plug in very bad probabilities, we get more and more negative number. That's bad

55:09.560 --> 55:10.920
so

55:10.920 --> 55:14.760
And the reason we work with this is for large extent convenience, right?

55:15.240 --> 55:20.520
Because we have mathematically that if you have some product a times b times c of all these probabilities, right?

55:21.160 --> 55:24.600
The likelihood is the product of all these probabilities

55:25.400 --> 55:27.400
then the log

55:27.400 --> 55:30.280
Of these is just log of a plus

55:30.840 --> 55:32.840
log of b

55:33.800 --> 55:36.760
Plus log of c if you remember your logs from your

55:37.480 --> 55:39.480
High school or undergrad and so on

55:39.800 --> 55:41.560
so we have that basically

55:41.560 --> 55:47.640
The likelihood is the product of probabilities. The log likelihood is just the sum of the logs of the individual probabilities

55:48.840 --> 55:50.040
so

55:50.040 --> 55:52.040
log likelihood

55:52.760 --> 55:54.680
Starts at zero

55:54.680 --> 55:58.280
And then log likelihood here we can just accumulate simply

56:00.440 --> 56:02.440
And then the end we can print this

56:05.400 --> 56:07.400
Print the log likelihood

56:09.640 --> 56:11.640
F strings

56:11.800 --> 56:13.800
Maybe you're familiar with this

56:13.880 --> 56:16.380
So log likelihood is negative 38

56:19.960 --> 56:21.320
Okay

56:21.320 --> 56:22.600
now

56:22.600 --> 56:24.600
We actually want um

56:25.240 --> 56:29.320
So how high can log likelihood get it can go to zero

56:29.800 --> 56:34.760
So when all the probabilities are one log likelihood will be zero and then when all the probabilities are lower

56:34.840 --> 56:36.840
This will grow more and more negative

56:37.480 --> 56:44.200
Now we don't actually like this because what we'd like is a loss function and a loss function has the semantics that low

56:44.680 --> 56:47.720
Is good because we're trying to minimize the loss

56:48.200 --> 56:53.480
So we actually need to invert this and that's what gives us something called the negative log likelihood

56:54.360 --> 56:56.040
um

56:56.040 --> 56:59.560
Negative log likelihood is just negative of the log likelihood

57:03.880 --> 57:07.720
These are f strings by the way if you'd like to look this up negative log likelihood equals

57:09.320 --> 57:11.480
So negative log likelihood now is just negative of it

57:12.040 --> 57:16.120
and so the negative log likelihood is a very nice loss function because

57:17.640 --> 57:19.640
The lowest it can get is zero

57:19.800 --> 57:23.880
And the higher it is the worse off the predictions are that you're making

57:24.680 --> 57:28.600
And then one more modification to this that sometimes people do is that for convenience

57:29.160 --> 57:33.480
They actually like to normalize by they like to make it an average instead of a sum

57:34.440 --> 57:36.440
and so uh here

57:37.080 --> 57:39.080
Let's just keep some counts as well

57:39.320 --> 57:43.480
So n plus equals one starts at zero and then here

57:44.600 --> 57:46.940
We can have sort of like a normalized log likelihood

57:50.440 --> 57:52.440
If we just normalize it by the count

57:52.440 --> 57:58.280
Then we will sort of get the average log likelihood. So this would be usually our loss function here

57:58.840 --> 58:00.840
Is put this we would this is what we would use

58:02.280 --> 58:06.280
So our loss function for the training set assigned by the model is 2.4

58:06.440 --> 58:08.440
That's the quality of this model

58:08.520 --> 58:12.360
And the lower it is the better off we are and the higher it is the worse off we are

58:13.320 --> 58:21.160
And the job of our you know training is to find the parameters that minimize the negative log likelihood loss

58:22.840 --> 58:27.400
And that would be like a high quality model. Okay, so to summarize I actually wrote it out here

58:28.040 --> 58:34.920
So our goal is to maximize likelihood, which is the product of all the probabilities assigned by the model

58:35.560 --> 58:39.320
And we want to maximize this likelihood with respect to the model parameters

58:39.720 --> 58:45.720
And in our case the model parameters here are defined in the table these numbers the probabilities are

58:46.520 --> 58:49.320
The model parameters is sort of in our brygm language model so far

58:50.120 --> 58:54.520
But you have to keep in mind that here we are storing everything in a table format the probabilities

58:54.760 --> 58:59.720
But what's coming up as a brief preview is that these numbers will not be kept explicitly

59:00.120 --> 59:02.520
But these numbers will be calculated by a neural network

59:03.080 --> 59:04.520
So that's coming up

59:04.520 --> 59:07.640
And we want to change and tune the parameters of these neural networks

59:08.040 --> 59:12.360
We want to change these parameters to maximize the likelihood the product of the probabilities

59:13.400 --> 59:19.080
Now maximizing the likelihood is equivalent to maximizing the log likelihood because log is a monotonic function

59:19.880 --> 59:21.880
Here's the graph of log

59:22.040 --> 59:26.040
And basically all it is doing is it's just scaling your

59:26.680 --> 59:28.760
You can look at it as just a scaling of the loss function

59:29.400 --> 59:36.440
And so the optimization problem here and here are actually equivalent because this is just scaling you can look at it that way

59:37.000 --> 59:39.720
And so these are two identical optimization problems

59:41.160 --> 59:45.320
Um maximizing the log likelihood is equivalent to minimizing the negative log likelihood

59:46.200 --> 59:52.200
And then in practice people actually minimize the average negative log likelihood to get numbers like 2.4

59:53.000 --> 59:58.920
And then this summarizes the quality of your model and we'd like to minimize it and make it as small as possible

59:59.640 --> 01:00:01.720
And the lowest it can get is zero

01:00:02.360 --> 01:00:04.200
and the lower it is

01:00:04.200 --> 01:00:09.080
The better off your model is because it's assigning it's assigning high probabilities to your data

01:00:09.560 --> 01:00:14.360
Now let's estimate the probability over the entire training set just to make sure that we get something around 2.4

01:00:14.920 --> 01:00:16.920
Let's run this over the entire oops

01:00:17.320 --> 01:00:19.320
Let's take out the print statement as well

01:00:20.680 --> 01:00:22.920
Okay 2.45 or the entire training set

01:00:24.520 --> 01:00:29.080
Now what I'd like to show you is that you can actually evaluate the probability for any word that you want like for example

01:00:29.320 --> 01:00:34.040
If we just test a single word andre and bring back the print statement

01:00:35.880 --> 01:00:39.640
Then you see that andre is actually kind of like an unlikely word or like on average

01:00:40.760 --> 01:00:48.280
We take three log probability to represent it and roughly that's because ej apparently is very uncommon as an example

01:00:50.040 --> 01:00:52.040
Now think through this

01:00:53.800 --> 01:00:57.800
When I take andre and I append q and I test the probability of it andreq

01:00:59.800 --> 01:01:02.200
We actually get um infinity

01:01:03.000 --> 01:01:08.680
And that's because jq has a zero percent probability according to our model. So the log likelihood

01:01:09.320 --> 01:01:13.720
So the log of zero will be negative infinity. We get infinite loss

01:01:14.520 --> 01:01:18.840
So this is kind of undesirable right because we plugged in a string that could be like a somewhat reasonable name

01:01:19.160 --> 01:01:24.920
But basically what this is saying is that this model is exactly zero percent likely to uh to predict this

01:01:25.480 --> 01:01:29.080
Name and our loss is infinity on this example

01:01:29.720 --> 01:01:32.040
And really what the reason for that is that j

01:01:32.920 --> 01:01:34.920
is followed by q

01:01:35.480 --> 01:01:37.080
zero times

01:01:37.080 --> 01:01:41.560
Where's q jq is zero and so jq is uh zero percent likely

01:01:42.200 --> 01:01:45.880
So it's actually kind of gross and people don't like this too much to fix this

01:01:45.880 --> 01:01:50.200
There's a very simple fix that people like to do to sort of like smooth out your model a little bit

01:01:50.280 --> 01:01:52.120
It's called model smoothing

01:01:52.120 --> 01:01:55.560
And roughly what's happening is that we will eight we will add some fake accounts

01:01:56.280 --> 01:01:59.720
So imagine adding a count of one to everything

01:02:01.000 --> 01:02:03.000
So we add a count of one

01:02:03.400 --> 01:02:04.600
like this

01:02:04.600 --> 01:02:06.600
And then we recalculate the probabilities

01:02:07.800 --> 01:02:12.280
And that's model smoothing and you can add as much as you like you can add five and that will give you a smoother model

01:02:12.920 --> 01:02:14.760
and the more you add here

01:02:14.760 --> 01:02:18.600
The more uniform model you're gonna have and the less you add

01:02:19.560 --> 01:02:21.720
The more peaked model you are gonna have of course

01:02:22.280 --> 01:02:24.280
So one is like a pretty decent

01:02:24.360 --> 01:02:29.640
Count to add and that will ensure that there will be no zeros in our probability matrix p

01:02:30.840 --> 01:02:35.800
And so this will of course change the generations a little bit in this case. It didn't buy it in principle. It could

01:02:36.520 --> 01:02:39.480
But what that's going to do now is that nothing will be infinity

01:02:40.040 --> 01:02:41.160
unlikely

01:02:41.160 --> 01:02:42.280
So now

01:02:42.280 --> 01:02:47.240
Our model will predict some other probability and we see that jq now has a very small probability

01:02:47.640 --> 01:02:52.840
So the model still finds it very surprising that this was a word or a by-gram, but we don't get negative infinity

01:02:53.320 --> 01:02:56.680
So it's kind of like a nice fix that people like to apply sometimes and it's called model smoothing

01:02:57.080 --> 01:02:59.080
Okay, so we've now trained a respectable

01:02:59.400 --> 01:03:03.160
by-gram character level language model and we saw that we both

01:03:04.120 --> 01:03:07.320
Sort of trained the model by looking at the counts of all the by-grams

01:03:07.800 --> 01:03:10.680
And normalizing the rows to get probability distributions

01:03:11.480 --> 01:03:17.880
We saw that we can also then use those parameters of this model to perform sampling of new words

01:03:19.480 --> 01:03:21.800
So we sample new names according to those distributions

01:03:22.200 --> 01:03:24.920
And we also saw that we can evaluate the quality of this model

01:03:25.400 --> 01:03:27.800
And the quality of this model is summarized in a single number

01:03:28.040 --> 01:03:32.600
Which is the negative log likelihood and the lower this number is the better the model is

01:03:33.320 --> 01:03:38.920
Because it is giving high probabilities to the actual next characters and all the by-grams in our training set

01:03:39.640 --> 01:03:41.640
So that's all well and good

01:03:42.040 --> 01:03:46.040
But we've arrived at this model explicitly by doing something that felt sensible

01:03:46.120 --> 01:03:50.120
We were just performing counts and then we were normalizing those counts

01:03:51.000 --> 01:03:53.720
Now what I would like to do is I would like to take an alternative approach

01:03:54.040 --> 01:03:56.200
We will end up in a very very similar position

01:03:56.440 --> 01:04:03.400
But the approach will look very different because I would like to cast the problem of by-gram character level language modeling into the neural network framework

01:04:04.200 --> 01:04:09.960
And in neural network framework, we're going to approach things slightly differently, but again end up in a very similar spot

01:04:10.200 --> 01:04:11.960
I'll go into that later

01:04:11.960 --> 01:04:16.840
Now our neural network is going to be a still a by-gram character level language model

01:04:17.240 --> 01:04:19.720
So it receives a single character as an input

01:04:20.360 --> 01:04:23.320
Then there's neural network with some weights or some parameters w

01:04:24.200 --> 01:04:28.920
And it's going to output the probability distribution over the next character in a sequence

01:04:29.080 --> 01:04:34.600
It's going to make guesses as to what is likely to follow this character that was input to the model

01:04:35.960 --> 01:04:42.520
And then in addition to that we're going to be able to evaluate any setting of the parameters of the neural net because we have the loss function

01:04:43.720 --> 01:04:48.760
The negative log likelihood. So we're going to take a look at its probability distributions and we're going to use the labels

01:04:50.120 --> 01:04:54.120
Which are basically just the identity of the next character in that by-gram the second character

01:04:54.680 --> 01:04:57.880
So knowing what the second character actually comes next in the by-gram

01:04:58.200 --> 01:05:03.240
Allows us to then look at what how high of probability the model assigns to that character

01:05:03.880 --> 01:05:06.120
And then we of course want the probability to be very high

01:05:07.000 --> 01:05:09.720
And that is another way of saying that the loss is low

01:05:10.840 --> 01:05:15.000
So we're going to use gradient based optimization then to tune the parameters of this network

01:05:15.400 --> 01:05:18.120
Because we have the loss function and we're going to minimize it

01:05:18.360 --> 01:05:23.560
So we're going to tune the weights so that the neural net is correctly predicting the probabilities for the next character

01:05:24.360 --> 01:05:29.480
So let's get started. The first thing I want to do is I want to compile the training set of this neural network, right?

01:05:29.560 --> 01:05:32.120
So create the training set

01:05:33.080 --> 01:05:35.080
of all the by-grams

01:05:36.360 --> 01:05:38.360
Okay, and

01:05:39.400 --> 01:05:42.760
Here I'm going to copy paste this code

01:05:43.720 --> 01:05:45.720
Because this code iterates over all the by-grams

01:05:47.400 --> 01:05:52.680
So here we start with the words we iterate over all the by-grams and previously as you recall we did the counts

01:05:53.000 --> 01:05:56.040
But now we're not going to do counts. We're just creating a training set

01:05:56.920 --> 01:05:59.720
Now this training set will be made up of two lists

01:06:02.120 --> 01:06:04.120
We have the

01:06:04.760 --> 01:06:06.280
inputs

01:06:06.280 --> 01:06:08.600
And the targets the the labels

01:06:09.480 --> 01:06:12.440
And these by-grams will denote x y those are the characters, right?

01:06:13.160 --> 01:06:16.920
And so we're given the first character of the by-gram and then we're trying to predict the next one

01:06:17.720 --> 01:06:22.520
Both of these are going to be integers. So here we'll take x's that append is just

01:06:23.560 --> 01:06:25.960
x1 y's that append ix2

01:06:27.640 --> 01:06:29.320
And then here

01:06:29.320 --> 01:06:35.720
We actually don't want lists of integers. We will create uh tensors out of these. So x's is torched dot tensor

01:06:36.520 --> 01:06:39.880
x's and y's is torched dot tensor of y's

01:06:41.480 --> 01:06:46.360
And then we don't actually want to take all the words just yet because I want everything to be manageable

01:06:47.000 --> 01:06:49.240
So let's just do the first word which is emma

01:06:51.240 --> 01:06:53.640
And then it's clear what these x's and y's would be

01:06:55.400 --> 01:06:57.400
Here let me print

01:06:57.880 --> 01:07:00.440
Character one character two just so you see what's going on here

01:07:01.560 --> 01:07:08.280
So the by-grams of these characters is dot e e m m m a dot

01:07:08.760 --> 01:07:13.800
So this single word as I mentioned has one two three four five examples for our neural network

01:07:14.680 --> 01:07:16.680
There are five separate examples in emma

01:07:17.560 --> 01:07:22.120
And those examples are summarized here when the input to the neural neural network is integer zero

01:07:23.160 --> 01:07:27.160
The desired label is integer five which corresponds to e

01:07:27.960 --> 01:07:34.440
When the input to the neural network is five, we want its weights to be arranged so that 13 gets a very high probability

01:07:35.080 --> 01:07:38.520
When 13 is put in we want 13 to have a high probability

01:07:39.160 --> 01:07:42.520
When 13 is put in we also want one to have a high probability

01:07:43.480 --> 01:07:46.920
When one is input we want zero to have a very high probability

01:07:47.400 --> 01:07:50.600
So there are five separate input examples to a neural net

01:07:51.400 --> 01:07:53.400
in this data set

01:07:55.000 --> 01:08:00.440
I wanted to add a tangent of a note of caution to be careful with a lot of the apis of some of these frameworks

01:08:01.400 --> 01:08:07.160
You saw me silently use torch dot tensor with a lowercase t and the output looked right

01:08:07.800 --> 01:08:11.240
But you should be aware that there's actually two ways of constructing a tensor

01:08:11.720 --> 01:08:17.880
There's a torch dot lowercase tensor and there's also a torch dot capital tensor class, which you can also construct

01:08:18.680 --> 01:08:22.040
So you can actually call both you can also do torch dot capital tensor

01:08:22.920 --> 01:08:24.920
And you get an x as in y as well

01:08:25.400 --> 01:08:27.400
So that's not confusing at all

01:08:28.920 --> 01:08:32.440
There are threads on what is the difference between these two and um

01:08:33.400 --> 01:08:40.120
Unfortunately, the docs are just like not clear on the difference and when you look at the the docs of lowercase tensor construct tensor

01:08:40.200 --> 01:08:42.520
With no autograd history by copying data

01:08:43.640 --> 01:08:45.640
It's just like it doesn't

01:08:45.640 --> 01:08:51.000
It doesn't make sense. So the actual difference as far as I can tell is explained eventually in this random thread that you can google

01:08:51.640 --> 01:08:53.640
And really it comes down to I believe

01:08:55.080 --> 01:08:57.080
That um, where is this?

01:08:58.520 --> 01:09:03.720
Torch dot tensor in first the d type the data type automatically while torch dot tensor just returns a float tensor

01:09:04.360 --> 01:09:06.680
I would recommend stick to torch dot lowercase tensor

01:09:07.800 --> 01:09:09.640
so um

01:09:09.640 --> 01:09:16.760
Indeed we see that when I construct this with a capital t the data type here of x's is float 32

01:09:18.120 --> 01:09:20.120
But torch dot lowercase tensor

01:09:21.080 --> 01:09:25.080
You see how it's now x dot d type is now integer

01:09:26.760 --> 01:09:28.200
So, um

01:09:28.200 --> 01:09:33.400
It's advised that you use lowercase t and you can read more about it if you like in some of these threads

01:09:34.040 --> 01:09:36.040
but basically

01:09:36.600 --> 01:09:41.560
I'm pointing out some of these things because because I want to caution you and I want you to read get used to reading a

01:09:41.560 --> 01:09:47.800
lot of documentation and reading through a lot of uh q and a's and threads like this and um

01:09:48.280 --> 01:09:52.280
You know some of this stuff is unfortunately not easy and not very well documented and you have to be careful out there

01:09:52.680 --> 01:09:56.440
What we want here is integers because that's what makes sense

01:09:57.080 --> 01:09:58.040
um

01:09:58.040 --> 01:09:59.480
and so

01:09:59.480 --> 01:10:05.560
Lowercase tensor is what we are using. Okay. Now. We want to think through how we're going to feed in these examples into a neural network

01:10:06.200 --> 01:10:08.200
Now it's not quite as straightforward as

01:10:09.080 --> 01:10:14.360
Plugging it in because these examples right now are integers. So there's like a 0 5 or 13

01:10:14.600 --> 01:10:19.080
It gives us the index of the character and you can't just plug an integer index into a neural net

01:10:19.960 --> 01:10:26.520
these neural nets, uh, right are sort of made up of these neurons and uh, these neurons have weights

01:10:26.840 --> 01:10:32.200
And as you saw in micrograd these weights act multiplicatively on the inputs w x plus b

01:10:32.520 --> 01:10:36.360
There's 10 hs and so on and so it doesn't really make sense to make an input neuron

01:10:36.440 --> 01:10:40.840
Take on integer values that you feed in and then multiply on with weights

01:10:41.720 --> 01:10:46.280
So instead a common way of encoding integers is what's called one hot encoding

01:10:47.000 --> 01:10:53.240
In one hot encoding, uh, we take an integer like 13 and we create a vector that is all zeros

01:10:53.640 --> 01:11:00.040
Except for the 13th dimension which we turn to a one and then that vector can feed into a neural net

01:11:01.000 --> 01:11:05.480
Now conveniently, uh, PyTorch actually has something called the one hot

01:11:07.880 --> 01:11:12.440
Function inside torch and in functional it takes a tensor made up of integers

01:11:15.000 --> 01:11:17.400
Long is a is a is an integer

01:11:19.160 --> 01:11:26.280
And it also takes a number of classes, um, which is how large you want your tensor your vector to be

01:11:26.520 --> 01:11:33.320
So here let's import torch dot and in that functional sf. This is a common way of importing it

01:11:34.120 --> 01:11:36.120
And then let's do f dot one hot

01:11:36.680 --> 01:11:39.240
And we feed in the integers that we want to encode

01:11:40.040 --> 01:11:42.920
So we can actually feed in the entire array of x's

01:11:44.040 --> 01:11:46.920
And we can tell it that num classes is 27

01:11:47.720 --> 01:11:53.080
So it doesn't have to try to guess it it may have guessed that it's only 13 and would give us an incorrect result

01:11:53.480 --> 01:11:59.480
So this is the one hot. Let's call this x ink for x encoded

01:12:02.040 --> 01:12:05.720
And then we see that x encoded that shape is 5 by 27

01:12:07.080 --> 01:12:08.440
and uh

01:12:08.440 --> 01:12:11.240
We can also visualize it plt.im show of x ink

01:12:12.360 --> 01:12:14.680
To make it a little bit more clear because this is a little messy

01:12:15.480 --> 01:12:18.200
So we see that we've encoded all the five examples

01:12:18.920 --> 01:12:21.320
Into vectors we have five examples

01:12:21.400 --> 01:12:25.560
So we have five rows and each row here is now an example into a neural net

01:12:26.280 --> 01:12:31.000
And we see that the appropriate bit is turned on as a one and everything else is zero

01:12:31.960 --> 01:12:33.560
so um

01:12:33.560 --> 01:12:37.800
Here for example, the zero bit is turned on the fifth bit is turned on

01:12:38.360 --> 01:12:43.640
Thirteenth bits are turned on for both of these examples and the first bit here is turned on

01:12:44.680 --> 01:12:46.680
So that's how we can encode

01:12:47.160 --> 01:12:49.160
integers into vectors

01:12:49.320 --> 01:12:54.200
And then these vectors can feed in to neural nets one more issue to be careful with here by the way is

01:12:55.160 --> 01:12:58.680
Let's look at the data type of encoding. We always want to be careful with data types

01:12:59.400 --> 01:13:04.920
What would you expect x encodings data type to be when we're plugging numbers into neural nets?

01:13:04.920 --> 01:13:09.960
We don't want them to be integers. We want them to be floating point numbers that can take on various values

01:13:10.440 --> 01:13:13.560
But the d type here is actually 64 bit integer

01:13:14.280 --> 01:13:19.160
And the reason for that I suspect is that one hot received a 64 bit integer here

01:13:19.640 --> 01:13:21.640
And it returned the same data type

01:13:21.880 --> 01:13:27.880
And when you look at the signature of one hot it doesn't even take a d type a desired data type of the output tensor

01:13:28.440 --> 01:13:33.960
And so we can't in a lot of functions in torch we'd be able to do something like d type equals torch dot float 32

01:13:34.440 --> 01:13:37.320
Which is what we want, but one hot does not support that

01:13:37.960 --> 01:13:41.400
So instead we're going to want to cast this to float like this

01:13:42.120 --> 01:13:44.120
So that these

01:13:44.840 --> 01:13:46.440
Everything is the same

01:13:46.440 --> 01:13:51.320
Everything looks the same but the d type is float 32 and floats can feed into

01:13:52.280 --> 01:13:55.000
Neural nets. So now let's construct our first neuron

01:13:56.040 --> 01:13:59.000
This neuron will look at these input vectors

01:14:00.040 --> 01:14:07.720
And as you remember from micrograd these neurons basically perform a very simple function wx plus b where wx is a dot product

01:14:08.280 --> 01:14:09.560
right

01:14:09.640 --> 01:14:14.600
So we can achieve the same thing here. Let's first define the weights of this neuron

01:14:14.600 --> 01:14:18.040
Basically, what are the initial weights at initialization for this neuron?

01:14:18.760 --> 01:14:20.760
Let's initialize them with torch dot random

01:14:21.640 --> 01:14:23.320
torch dot random

01:14:23.320 --> 01:14:24.600
is

01:14:24.600 --> 01:14:26.600
Fills a tensor with random numbers

01:14:27.160 --> 01:14:29.160
drawn from a normal distribution

01:14:29.240 --> 01:14:31.240
And a normal distribution has

01:14:31.960 --> 01:14:38.280
A probability density function like this and so most of the numbers drawn from this distribution will be around zero

01:14:39.080 --> 01:14:45.240
But some of them will be as high as almost three and so on and very few numbers will be above three in magnitude

01:14:46.360 --> 01:14:49.400
So we need to take a size as an input here

01:14:50.440 --> 01:14:53.160
And i'm going to use size as to be 27 by one

01:14:54.680 --> 01:15:01.160
So 27 by one and then let's visualize w. So w is a column vector of 27 numbers

01:15:03.080 --> 01:15:07.400
And uh, these weights are then multiplied by the inputs

01:15:08.680 --> 01:15:14.120
So now to perform this multiplication, we can take x encoding and we can multiply it with w

01:15:15.000 --> 01:15:18.200
This is a matrix multiplication operator in pytorch

01:15:19.960 --> 01:15:22.920
And the output of this operation is five by one

01:15:23.640 --> 01:15:25.640
The reason it's five by five is the following

01:15:25.880 --> 01:15:31.720
We took x encoding, which is five by 27 and we multiplied it by 27 by one

01:15:33.560 --> 01:15:35.720
And in matrix multiplication

01:15:36.520 --> 01:15:43.320
You see that the output will become five by one because these 27 will multiply and add

01:15:44.840 --> 01:15:47.960
So basically what we're seeing here out of this operation

01:15:48.760 --> 01:15:50.760
Is we are seeing the five

01:15:50.760 --> 01:15:52.040
um

01:15:52.040 --> 01:15:53.640
activations

01:15:53.640 --> 01:15:55.640
of this neuron

01:15:56.360 --> 01:16:00.200
On these five inputs and we've evaluated all of them in parallel

01:16:00.440 --> 01:16:03.080
We didn't feed in just a single input to the single neuron

01:16:03.400 --> 01:16:07.400
We fed in simultaneously all the five inputs into the same neuron

01:16:08.120 --> 01:16:10.520
And in parallel pytorch has evaluated

01:16:11.160 --> 01:16:15.160
The wx plus b but here is just wx. There's no bias

01:16:15.880 --> 01:16:18.760
It has valued w w times x for all of them

01:16:19.400 --> 01:16:25.240
Uh independently now instead of a single neuron though, I would like to have 27 neurons and I'll show you in the second

01:16:25.240 --> 01:16:27.240
Why I'm on 27 neurons?

01:16:27.560 --> 01:16:31.880
So instead of having just a one here, which is indicating this presence of one single neuron

01:16:32.520 --> 01:16:34.520
We can use 27

01:16:34.680 --> 01:16:36.920
And then when w is 27 by 27

01:16:38.360 --> 01:16:44.760
This will in parallel evaluate all the 27 neurons on all the five inputs

01:16:46.520 --> 01:16:49.000
Giving us a much better much much bigger result

01:16:49.480 --> 01:16:53.320
So now what we've done is five by 27 multiplied 27 by 27

01:16:54.120 --> 01:16:56.620
And the output of this is now five by 27

01:16:57.800 --> 01:16:59.800
So we can see that the shape of this

01:17:01.880 --> 01:17:03.880
Is five by 27

01:17:03.880 --> 01:17:06.520
So what is every element here telling us right?

01:17:07.160 --> 01:17:10.760
It's telling us for every one of 27 neurons that we created

01:17:13.240 --> 01:17:18.680
What is the firing rate of those neurons on every one of those five examples

01:17:19.640 --> 01:17:20.840
so

01:17:20.840 --> 01:17:24.200
The element for example 3 comma 13

01:17:25.400 --> 01:17:28.680
Is giving us the firing rate of the 13th neuron

01:17:29.320 --> 01:17:31.320
looking at the third input

01:17:31.880 --> 01:17:35.240
And the way this was achieved is by a dot product

01:17:36.280 --> 01:17:38.280
between the third input

01:17:38.920 --> 01:17:40.920
and the 13th column

01:17:41.560 --> 01:17:43.560
of this w matrix here

01:17:44.840 --> 01:17:49.560
Okay, so using matrix multiplication. We can very efficiently evaluate

01:17:50.840 --> 01:17:54.040
The dot product between lots of input examples in a batch

01:17:55.000 --> 01:18:00.360
And lots of neurons where all of those neurons have weights in the columns of those w's

01:18:01.080 --> 01:18:03.720
And in matrix multiplication, we're just doing those dot products and

01:18:04.520 --> 01:18:09.240
In parallel just to show you that this is the case. We can take x-ank and we can take the third

01:18:10.520 --> 01:18:12.200
row

01:18:12.200 --> 01:18:15.080
And we can take the w and take its 13th column

01:18:17.400 --> 01:18:19.800
And then we can do x-ank at 3

01:18:21.720 --> 01:18:24.440
Element wise multiply with w at 13

01:18:26.760 --> 01:18:28.920
And sum that up does w x plus b

01:18:29.640 --> 01:18:33.320
Well, there's no plus b. It's just w x dot product. And that's

01:18:34.120 --> 01:18:35.320
this number

01:18:35.320 --> 01:18:39.320
So you see that this is just being done efficiently by the matrix multiplication

01:18:39.640 --> 01:18:45.400
operation for all the input examples and for all the output neurons of this first layer

01:18:46.040 --> 01:18:52.920
Okay, so we fed our 27 dimensional inputs into a first layer of a neural net that has 27 neurons, right?

01:18:52.920 --> 01:18:59.400
So we have 27 inputs and now we have 27 neurons these neurons perform w times x

01:18:59.720 --> 01:19:03.000
They don't have a bias and they don't have a non-linearity like 10 h

01:19:03.160 --> 01:19:05.880
We're going to leave them to be a linear layer

01:19:06.600 --> 01:19:10.840
In addition to that we're not going to have any other layers. This is going to be it. It's just going to be

01:19:11.480 --> 01:19:15.240
The dumbest smallest simplest neural net, which is just a single linear layer

01:19:16.520 --> 01:19:20.280
And now I'd like to explain what I want those 27 outputs to be

01:19:21.240 --> 01:19:24.200
Intuitively what we're trying to produce here for every single input example

01:19:24.520 --> 01:19:28.760
Is we're trying to produce some kind of a probability distribution for the next character in a sequence

01:19:29.320 --> 01:19:31.320
And there's 27 of them

01:19:31.480 --> 01:19:35.320
But we have to come up with like precise semantics for exactly how we're going to interpret

01:19:35.800 --> 01:19:38.760
These 27 numbers that these neurons take on

01:19:39.720 --> 01:19:41.160
Now intuitively

01:19:41.160 --> 01:19:44.440
You see here that these numbers are negative and some of them are positive, etc

01:19:45.160 --> 01:19:49.240
And that's because these are coming out of a neural net layer initialized with these

01:19:51.240 --> 01:19:53.080
normal distribution

01:19:53.080 --> 01:19:54.280
parameters

01:19:54.280 --> 01:19:56.680
But what we want is we want something like we had here

01:19:57.240 --> 01:20:02.920
Like each row here told us the counts and then we normalize the counts to get probabilities

01:20:03.480 --> 01:20:05.640
And we want something similar to come out of a neural net

01:20:06.440 --> 01:20:09.240
But what we just have right now is just some negative and positive numbers

01:20:10.600 --> 01:20:14.520
Now we want those numbers to somehow represent the probabilities for the next character

01:20:15.320 --> 01:20:19.240
But you see that probabilities they they have a special structure. They um

01:20:19.960 --> 01:20:25.080
They're positive numbers and they sum to 1 and so that doesn't just come out of a neural net

01:20:25.800 --> 01:20:30.120
And then they can't be counts because these counts are positive

01:20:30.680 --> 01:20:35.880
And counts are integers. So counts are also not really a good thing to output from a neural net

01:20:36.680 --> 01:20:41.400
So instead what the neural net is going to output and how we are going to interpret the um

01:20:42.120 --> 01:20:47.640
The 27 numbers is that these 27 numbers are giving us log counts

01:20:48.520 --> 01:20:49.720
basically

01:20:49.720 --> 01:20:50.440
um

01:20:50.440 --> 01:20:55.240
So instead of giving us counts directly like in this table, they're giving us log counts

01:20:56.040 --> 01:21:00.440
And to get the counts, we're going to take the log counts and we're going to exponentiate them

01:21:01.400 --> 01:21:03.000
now

01:21:03.000 --> 01:21:04.200
exponentiation

01:21:04.200 --> 01:21:06.200
takes the following form

01:21:07.160 --> 01:21:08.920
It takes numbers

01:21:08.920 --> 01:21:12.280
That are negative or they are positive. It takes the entire real line

01:21:12.840 --> 01:21:17.720
And then if you plug in negative numbers, you're going to get e to the x which is

01:21:18.520 --> 01:21:20.520
always below 1

01:21:20.600 --> 01:21:22.600
So you're getting numbers lower than 1

01:21:23.480 --> 01:21:27.720
And if you plug in numbers greater than zero, you're getting numbers greater than one

01:21:28.360 --> 01:21:30.360
all the way growing to the infinity

01:21:30.840 --> 01:21:32.840
And this here grows to zero

01:21:33.320 --> 01:21:36.680
So basically we're going to take these numbers

01:21:37.800 --> 01:21:39.800
here

01:21:40.360 --> 01:21:42.200
and

01:21:43.240 --> 01:21:45.640
Instead of them being positive and negative and all over the place

01:21:46.040 --> 01:21:51.560
We're going to interpret them as log counts and then we're going to element wise exponentiate these numbers

01:21:52.840 --> 01:21:55.400
Exponentiating them now gives us something like this

01:21:56.440 --> 01:21:59.000
And you see that these numbers now because they went through an exponent

01:21:59.320 --> 01:22:03.720
All the negative numbers turned into numbers below one like 0.338

01:22:04.200 --> 01:22:09.400
And all the positive numbers originally turned into even more positive numbers sort of greater than one

01:22:10.120 --> 01:22:12.120
um, so like for example

01:22:12.520 --> 01:22:14.520
7

01:22:14.520 --> 01:22:17.000
Is some positive number over here?

01:22:18.440 --> 01:22:20.440
That is greater than zero

01:22:21.080 --> 01:22:22.200
But

01:22:22.200 --> 01:22:24.200
Exponentiated outputs here

01:22:24.840 --> 01:22:30.440
Basically give us something that we can use and interpret as the equivalent of counts originally

01:22:30.920 --> 01:22:34.840
So you see these counts here 112 7 51 1 etc

01:22:36.360 --> 01:22:38.360
The neural net is kind of now predicting

01:22:39.160 --> 01:22:40.360
Uh

01:22:40.360 --> 01:22:41.560
counts

01:22:41.560 --> 01:22:46.200
And these counts are positive numbers. They can never be below zero. So that makes sense

01:22:46.760 --> 01:22:49.080
And they can now take on various values

01:22:49.720 --> 01:22:52.600
Depending on the settings of w

01:22:54.200 --> 01:22:56.200
So let me break this down

01:22:56.200 --> 01:22:59.080
We're going to interpret these to be the log counts

01:23:01.240 --> 01:23:04.360
In other words for this that is often used is so called logits

01:23:05.160 --> 01:23:07.640
These are logits log counts

01:23:08.600 --> 01:23:10.600
Then these will be sort of the counts

01:23:11.320 --> 01:23:13.320
Logits exponentiated

01:23:13.400 --> 01:23:17.160
And this is equivalent to the n matrix sort of the n

01:23:18.120 --> 01:23:20.760
Array that we used previously. Remember, this was the n

01:23:21.640 --> 01:23:27.000
This is the the array of counts and each row here are the counts for the

01:23:27.800 --> 01:23:30.360
For the um next character sort of

01:23:32.680 --> 01:23:36.760
So those are the counts and now the probabilities are just the counts

01:23:37.320 --> 01:23:39.320
um normalized

01:23:39.720 --> 01:23:41.720
and so um

01:23:41.720 --> 01:23:45.400
I'm not going to find the same but basically I'm not going to scroll all over the place

01:23:46.040 --> 01:23:49.400
We've already done this. We want two counts that sum

01:23:50.120 --> 01:23:53.880
Along the first dimension and we want to keep them. It's true

01:23:54.760 --> 01:24:01.320
We've went over this and this is how we normalized the rows of our counts matrix to get our probabilities

01:24:02.120 --> 01:24:04.120
Props

01:24:04.840 --> 01:24:06.840
So now these are the probabilities

01:24:07.880 --> 01:24:12.120
And these are the counts that we have currently and now when I show the probabilities

01:24:13.720 --> 01:24:17.800
You see that um every row here, of course

01:24:19.480 --> 01:24:21.480
Will sum to one

01:24:21.480 --> 01:24:23.160
Because they're normalized

01:24:23.160 --> 01:24:25.160
And the shape of this

01:24:25.320 --> 01:24:27.320
Is five by 27

01:24:27.480 --> 01:24:31.240
And so really what we've achieved is for every one of our five examples

01:24:31.720 --> 01:24:34.280
We now have a row that came out of a neural net

01:24:35.160 --> 01:24:39.960
And because of the transformations here, we made sure that this output of this neural net now

01:24:40.360 --> 01:24:43.000
Are probabilities or we can interpret to be probabilities

01:24:44.120 --> 01:24:45.320
so

01:24:45.320 --> 01:24:50.120
Our wx here gave us logits and then we interpret those to be log counts

01:24:50.760 --> 01:24:53.320
We exponentiate to get something that looks like counts

01:24:54.040 --> 01:24:56.760
And then we normalize those counts to get a probability distribution

01:24:57.480 --> 01:24:59.720
And all of these are differentiable operations

01:25:00.360 --> 01:25:02.680
So what we've done now is we are taking inputs

01:25:03.240 --> 01:25:06.120
We have differentiable operations that we can back propagate through

01:25:06.920 --> 01:25:08.920
And we're getting out probability distributions

01:25:09.880 --> 01:25:13.880
So um for example for the zeroth example that fed in

01:25:15.160 --> 01:25:17.160
Right, which was um

01:25:17.160 --> 01:25:19.640
The zeroth example here was a one half vector of zero

01:25:20.600 --> 01:25:22.600
and um

01:25:22.600 --> 01:25:25.320
It basically corresponded to feeding in

01:25:25.480 --> 01:25:26.600
Uh

01:25:26.600 --> 01:25:27.800
This example here

01:25:27.800 --> 01:25:33.640
So we're feeding in a dot into a neural net and the way we fed the dot into a neural net is that we first got its index

01:25:34.360 --> 01:25:36.360
Then we one hot encoded it

01:25:36.600 --> 01:25:39.400
Then it went into the neural net and out came

01:25:40.600 --> 01:25:42.600
This distribution of probabilities

01:25:43.400 --> 01:25:45.400
And its shape

01:25:46.440 --> 01:25:53.320
Is 27 there's 27 numbers and we're going to interpret this as the neural net's assignment for how likely

01:25:54.040 --> 01:25:56.040
every one of these characters

01:25:56.600 --> 01:25:58.840
The 27 characters are to come next

01:25:59.800 --> 01:26:01.800
And as we tune the weights w

01:26:02.440 --> 01:26:06.360
We're going to be of course getting different probabilities out for any character that you input

01:26:07.240 --> 01:26:10.360
And so now the question is just can we optimize and find a good w?

01:26:11.080 --> 01:26:13.960
Such that the probabilities coming out are pretty good

01:26:14.520 --> 01:26:16.840
And the way we measure pretty good is by the loss function

01:26:17.160 --> 01:26:20.840
Okay, so I organized everything into a single summary so that hopefully it's a bit more clear

01:26:21.240 --> 01:26:23.800
So it starts here with an input data set

01:26:24.440 --> 01:26:31.320
We have some inputs to the neural net and we have some labels for the correct next character in a sequence and these are integers

01:26:32.840 --> 01:26:37.720
Here i'm using uh torch generators now so that you see the same numbers that I see

01:26:38.600 --> 01:26:40.600
and i'm generating um

01:26:40.920 --> 01:26:45.560
27 neurons weights and each neuron here receives 27 inputs

01:26:46.280 --> 01:26:54.360
Then here we're going to plug in all the input examples x's into a neural net. So here this is a forward pass

01:26:55.720 --> 01:26:59.560
First we have to encode all of the inputs into one half representations

01:27:00.440 --> 01:27:08.680
So we have 27 classes. We pass in these integers and x ink becomes a array that is 5 by 27

01:27:09.720 --> 01:27:11.720
zeros except for a few ones

01:27:12.280 --> 01:27:15.800
We then multiply this in the first layer of a neural net to get logits

01:27:16.920 --> 01:27:22.920
Exponentiate the logits to get fake counts sort of and normalize these counts to get probabilities

01:27:24.440 --> 01:27:28.520
So the these last two lines by the way here are called the softmax

01:27:29.800 --> 01:27:31.800
Uh, which I pulled up here

01:27:32.040 --> 01:27:38.280
Softmax is a very often used layer in a neural net that takes these z's which are logits

01:27:38.920 --> 01:27:40.920
Exponentiates them

01:27:41.000 --> 01:27:44.040
And uh divides and normalizes it's a way of taking

01:27:44.680 --> 01:27:46.680
outputs of a neural net layer and these

01:27:47.240 --> 01:27:49.240
These outputs can be positive or negative

01:27:49.800 --> 01:27:54.520
And it outputs probability distributions. It outputs something that is always

01:27:55.240 --> 01:27:57.880
sums to one and are positive numbers just like probabilities

01:27:58.600 --> 01:28:01.640
Um, so this is kind of like a normalization function if you want to think of it that way

01:28:02.120 --> 01:28:05.400
And you can put it on top of any other linear layer inside a neural net

01:28:05.640 --> 01:28:11.720
And it basically makes a neural net output probabilities. That's very often used and we used it as well here

01:28:13.400 --> 01:28:16.840
So this is the forward pass and that's how we made a neural net output probability

01:28:17.960 --> 01:28:19.480
now

01:28:19.480 --> 01:28:21.480
you'll notice that um

01:28:23.000 --> 01:28:26.600
All of these this entire forward pass is made up of differentiable

01:28:27.320 --> 01:28:32.600
Layers everything here we can back propagate through and we saw some of the back propagation in micrograd

01:28:33.240 --> 01:28:35.240
This is just

01:28:35.320 --> 01:28:37.320
Multiplication and addition all that's happening here

01:28:37.320 --> 01:28:40.120
Just multiply and then add and we know how to back propagate through them

01:28:40.920 --> 01:28:42.920
Exponentiations we know how to back propagate through

01:28:43.880 --> 01:28:49.480
And then here we are summing and sum is is easily back propagated well as well

01:28:50.120 --> 01:28:53.800
And division as well. So everything here is differentiable operation

01:28:54.600 --> 01:28:56.600
And we can back propagate through

01:28:57.560 --> 01:29:01.000
Now we achieve these probabilities which are 5 by 27

01:29:01.640 --> 01:29:05.480
For every single example, we have a vector of probabilities that sum to 1

01:29:06.360 --> 01:29:09.640
And then here I wrote a bunch of stuff to sort of like break down

01:29:10.280 --> 01:29:14.680
The examples so we have five examples making up emma, right?

01:29:16.360 --> 01:29:18.920
And there are five bigrams inside emma

01:29:19.000 --> 01:29:26.920
So by gram example a by gram example one is that e is the beginning character right after dot

01:29:28.360 --> 01:29:33.240
And the indexes for these are zero and five. So then we feed in a zero

01:29:34.120 --> 01:29:39.880
That's the input to the neural net we get probabilities from the neural net that are 27 numbers

01:29:41.320 --> 01:29:46.760
And then the label is five because e actually comes after dot. So that's the label

01:29:47.720 --> 01:29:49.240
And then

01:29:49.240 --> 01:29:53.640
We use this label five to index into the probability distribution here

01:29:54.360 --> 01:30:00.520
So this index five here is zero one two three four five. It's this number here

01:30:01.240 --> 01:30:03.240
Which is here

01:30:04.040 --> 01:30:08.040
So that's basically the probability assigned by the neural net to the actual correct character

01:30:08.760 --> 01:30:14.760
You see that the network currently thinks that this next character that e following dot is only 1% likely

01:30:15.320 --> 01:30:17.000
Which is of course not very good, right?

01:30:17.000 --> 01:30:22.120
Because this actually is a training example and the network thinks that this is currently very very unlikely

01:30:22.440 --> 01:30:26.680
But that's just because we didn't get very lucky in generating a good setting of w

01:30:27.080 --> 01:30:31.080
So right now this network thinks this is unlikely and 0.01 is not a good outcome

01:30:31.960 --> 01:30:33.960
So the log likelihood then

01:30:34.360 --> 01:30:38.760
Is very negative and the negative log likelihood is very positive

01:30:39.400 --> 01:30:41.400
And so four is a very high

01:30:41.880 --> 01:30:43.160
negative log likelihood

01:30:43.160 --> 01:30:45.160
And that means we're going to have a high loss

01:30:45.320 --> 01:30:49.720
Because what is the loss the loss is just the average negative log likelihood

01:30:51.640 --> 01:30:53.640
So the second character is em

01:30:53.640 --> 01:30:58.840
And you see here that also the network thought that m following e is very unlikely 1%

01:31:01.000 --> 01:31:03.640
The for m following m it thought it was 2%

01:31:04.280 --> 01:31:07.480
And for a following m it actually thought it was 7% likely

01:31:07.880 --> 01:31:14.440
So just by chance this one actually has a pretty good probability and therefore a pretty low negative log likelihood

01:31:15.320 --> 01:31:17.720
And finally here it thought this was 1% likely

01:31:18.360 --> 01:31:24.200
So overall our average negative log likelihood, which is the loss the total loss that summarizes

01:31:24.760 --> 01:31:29.480
Basically the how well this network currently works at least on this one word not on the full data

01:31:29.560 --> 01:31:35.960
So just the one word is 3.76 which is actually very fairly high loss. This is not a very good setting of w's

01:31:36.840 --> 01:31:38.600
Now here's what we can do

01:31:38.600 --> 01:31:40.600
We're currently getting 3.76

01:31:41.320 --> 01:31:45.240
We can actually come here and we can change our w we can resample it

01:31:45.640 --> 01:31:47.960
So let me just add one to have a different seed

01:31:48.760 --> 01:31:51.800
And then we get a different w and then we can rerun this

01:31:52.840 --> 01:31:57.800
And with this different seed with this different setting of w's we now get 3.37

01:31:58.600 --> 01:32:03.960
So this is a much better w right and that and it's better because the probabilities just happen to come out

01:32:04.760 --> 01:32:07.880
Higher for the for the characters that actually are next

01:32:08.840 --> 01:32:12.680
And so you can imagine actually just resampling this, you know, we can try as two

01:32:14.360 --> 01:32:15.560
So

01:32:15.560 --> 01:32:19.320
Okay, this was not very good. Let's try one more. We can try three

01:32:20.920 --> 01:32:24.040
Okay, this was terrible setting because we have a very high loss

01:32:24.760 --> 01:32:26.120
so

01:32:26.120 --> 01:32:28.120
Anyway, I'm going to erase this

01:32:28.680 --> 01:32:34.920
What I'm doing here, which is just guess and check of randomly assigning parameters and seeing if the network is good

01:32:35.480 --> 01:32:38.760
That is a amateur hour. That's not how you optimize a neural net

01:32:39.080 --> 01:32:43.480
The way you optimize your neural net is you start with some random guess and we're going to commit to this one

01:32:43.560 --> 01:32:45.160
Even though it's not very good

01:32:45.160 --> 01:32:47.320
But now the big deal is we have a loss function

01:32:48.360 --> 01:32:50.040
So this loss

01:32:50.040 --> 01:32:52.040
Is made up only of differentiable

01:32:52.600 --> 01:32:54.280
operations

01:32:54.280 --> 01:32:57.080
And we can minimize the loss by tuning

01:32:57.640 --> 01:33:03.720
W's by computing the gradients of the loss with respect to these w matrices

01:33:05.080 --> 01:33:11.240
And so then we can tune w to minimize the loss and find a good setting of w using gradient based optimization

01:33:11.640 --> 01:33:16.440
So let's see how that will work now things are actually going to look almost identical to what we had with micrograd

01:33:17.080 --> 01:33:21.720
So here I pulled up the lecture from micrograd the notebook

01:33:22.040 --> 01:33:23.880
It's from this repository

01:33:23.880 --> 01:33:27.960
And when I scroll all the way to the end where we left off with micrograd, we had something very very similar

01:33:28.600 --> 01:33:33.560
We had a number of input examples in this case. We had four input examples inside x's

01:33:34.200 --> 01:33:36.200
And we had their targets

01:33:36.280 --> 01:33:37.720
desired targets

01:33:37.720 --> 01:33:43.400
Just like here we have our x's now, but we have five of them and they're now integers instead of vectors

01:33:44.120 --> 01:33:50.600
But we're going to convert our integers to vectors except our vectors will be 27 large instead of three large

01:33:50.840 --> 01:33:57.080
And then here what we did is first we did a forward pass where we ran a neural net on all of the inputs

01:33:58.360 --> 01:34:00.360
to get predictions

01:34:00.360 --> 01:34:04.440
Our neural net at the time this n of x was a multilayer perceptron

01:34:05.160 --> 01:34:09.400
Our neural net is going to look different because our neural net is just a single layer

01:34:10.520 --> 01:34:12.840
Single linear layer followed by a softmax

01:34:13.800 --> 01:34:15.800
So that's our neural net

01:34:15.880 --> 01:34:18.040
And the loss here was the mean squared error

01:34:18.360 --> 01:34:22.600
So we simply subtracted the prediction from the ground truth and squared it and summed it all up

01:34:22.920 --> 01:34:28.200
And that was the loss and loss was the single number that summarized the quality of the neural net

01:34:28.520 --> 01:34:34.840
And when loss is low like almost zero that means the neural net is um predicting correctly

01:34:36.280 --> 01:34:41.480
So we had a single number that uh that summarized the uh the performance of the neural net

01:34:42.040 --> 01:34:45.400
And everything here was differentiable and was stored in massive compute graph

01:34:46.760 --> 01:34:49.160
And then we iterated over all the parameters

01:34:49.160 --> 01:34:53.240
We made sure that the gradients are set to zero and we called loss.backward

01:34:54.120 --> 01:34:59.800
And loss.backward initiated back propagation at the final output node of loss, right? So

01:35:00.840 --> 01:35:03.320
Yeah, I remember these expressions. We had loss all the way at the end

01:35:03.560 --> 01:35:05.800
We start back propagation and we went all the way back

01:35:06.360 --> 01:35:10.120
And we made sure that we populated all the parameters dot grad

01:35:10.760 --> 01:35:13.960
So dot grad started at zero but back propagation filled it in

01:35:14.520 --> 01:35:20.760
And then in the update we iterated over all the parameters and we simply did a parameter update where every single

01:35:21.480 --> 01:35:26.440
element of our parameters was nudged in the opposite direction of the gradient

01:35:27.560 --> 01:35:30.840
And so we're going to do the exact same thing here

01:35:31.720 --> 01:35:33.720
So i'm going to pull this up

01:35:34.440 --> 01:35:36.440
On the side here

01:35:37.400 --> 01:35:41.560
So that we have it available and we're actually going to do the exact same thing

01:35:42.120 --> 01:35:45.720
So this was the forward pass. So we're we did this

01:35:46.920 --> 01:35:48.920
And probes is our y-pred

01:35:48.920 --> 01:35:52.040
So now we have to evaluate the loss, but we're not using the mean squared error

01:35:52.360 --> 01:35:57.640
We're using the negative log likelihood because we are doing classification. We're not doing regression as it's called

01:35:59.080 --> 01:36:01.080
So here we want to calculate loss

01:36:02.440 --> 01:36:06.280
Now the way we calculate it is is just this average negative log likelihood

01:36:07.160 --> 01:36:09.160
Now this probes here

01:36:10.680 --> 01:36:12.680
Has a shape of five by 27

01:36:13.400 --> 01:36:19.400
And so to get all the we basically want to pluck out the probabilities at the correct indices here

01:36:20.040 --> 01:36:23.880
So in particular because the labels are stored here in the array y's

01:36:24.520 --> 01:36:30.280
Basically what we're after is for the first example, we're looking at probability of five right at index five

01:36:30.920 --> 01:36:32.760
for the second example

01:36:32.760 --> 01:36:35.720
at the the second row or row index one

01:36:36.200 --> 01:36:39.320
We are interested in the probability assigned to index 13

01:36:40.280 --> 01:36:42.520
At the second example, we also have 13

01:36:43.480 --> 01:36:45.880
At the third row, we want one

01:36:47.400 --> 01:36:50.920
And at the last row, which is four we want zero

01:36:51.240 --> 01:36:57.160
So these are the probabilities we're interested in right and you can see that they're not amazing as we saw above

01:36:58.680 --> 01:37:03.480
So these are the probabilities we want but we want like a more efficient way to access these probabilities

01:37:04.040 --> 01:37:06.600
Um, not just listing them out in a tuple like this

01:37:07.080 --> 01:37:13.480
So it turns out that the way to do this in pi torch one of the ways at least is we can basically pass in all of these

01:37:16.760 --> 01:37:21.240
Sorry about that all of these um integers in vectors

01:37:22.120 --> 01:37:26.600
So the these ones you see how they're just zero one two three four

01:37:27.160 --> 01:37:32.120
We can actually create that using mp not mp. Sorry torch dot a range of five

01:37:32.840 --> 01:37:34.440
zero one two three four

01:37:34.440 --> 01:37:37.400
So we can index here with torch dot a range of five

01:37:38.360 --> 01:37:40.360
And here we index with y's

01:37:41.160 --> 01:37:43.160
And you see that that gives us

01:37:43.240 --> 01:37:45.240
exactly these numbers

01:37:49.000 --> 01:37:53.400
So that plugs out the probabilities of that the neural network assigns to the

01:37:54.040 --> 01:37:56.040
correct next character

01:37:56.360 --> 01:38:00.280
Now we take those probabilities and we don't we actually look at the log probability

01:38:00.600 --> 01:38:02.600
So we want to dot log

01:38:03.560 --> 01:38:06.520
And then we want to just average that up

01:38:06.680 --> 01:38:12.520
So take the mean of all of that and then it's the negative average log likelihood. That is the loss

01:38:14.280 --> 01:38:16.280
So the loss here is

01:38:16.920 --> 01:38:20.360
3.7 something and you see that this loss 3.76

01:38:20.760 --> 01:38:25.800
3.76 is exactly as we've obtained before but this is a vectorized form of that expression

01:38:26.520 --> 01:38:28.680
So we get the same loss

01:38:29.480 --> 01:38:33.400
And the same loss we can consider sort of as part of this forward pass

01:38:34.040 --> 01:38:36.040
And we've achieved here now loss

01:38:36.360 --> 01:38:39.400
Okay, so we made our way all the way to loss. We've defined the forward pass

01:38:40.120 --> 01:38:45.240
We forwarded the network and the loss now. We're ready to do the backward pass. So backward pass

01:38:48.040 --> 01:38:51.800
We want to first make sure that all the gradients are reset. So they're at zero

01:38:52.440 --> 01:38:55.880
Now in pi torch, you can set the gradients to be zero

01:38:55.880 --> 01:38:59.880
But you can also just set it to none and setting it to none is more efficient

01:39:00.200 --> 01:39:05.240
And pi torch will interpret none as like a lack of a gradient and it's the same as zeros

01:39:05.800 --> 01:39:08.440
So this is a way to set to zero the gradient

01:39:10.440 --> 01:39:12.440
And now we do lost up backward

01:39:14.680 --> 01:39:18.280
Before we do lost up backward, we need one more thing if you remember from micrograd

01:39:18.920 --> 01:39:20.920
pi torch actually requires

01:39:21.320 --> 01:39:23.880
That we pass in requires grad is true

01:39:24.840 --> 01:39:26.840
Uh, so that we tell

01:39:27.000 --> 01:39:32.760
Pi torch that we are interested in calculating gradients for this leaf tensor by default. This is false

01:39:33.480 --> 01:39:35.480
So let me recalculate with that

01:39:35.880 --> 01:39:38.360
And then set to none and lost up backward

01:39:40.680 --> 01:39:43.480
Now something magical happened when lost up backward was run

01:39:44.440 --> 01:39:48.520
Because pi torch just like micrograd when we did the forward pass here

01:39:49.080 --> 01:39:54.200
It keeps track of all the operations under the hood. It builds a full computational graph

01:39:54.760 --> 01:39:59.800
Just like the graphs we've produced in micrograd those graphs exist inside pi torch

01:40:00.760 --> 01:40:04.440
And so it knows all the dependencies and all the mathematical operations of everything

01:40:05.000 --> 01:40:08.760
And when you then calculate the loss we can call a dot backward on it

01:40:09.560 --> 01:40:13.000
And dot backward then fills in the gradients of

01:40:13.640 --> 01:40:17.240
All the intermediates all the way back to w's

01:40:17.800 --> 01:40:21.480
Which are the parameters of our neural net. So now we can do w dot grad

01:40:22.360 --> 01:40:24.920
And we see that it has structure there's stuff inside it

01:40:29.160 --> 01:40:32.120
And these gradients every single element here

01:40:33.400 --> 01:40:36.200
So w dot shape is 27 by 27

01:40:36.840 --> 01:40:39.800
W grads shape is the same 27 by 27

01:40:40.680 --> 01:40:42.520
And every element of w dot grad

01:40:43.160 --> 01:40:44.440
is telling us

01:40:44.520 --> 01:40:48.040
The influence of that weight on the loss function

01:40:48.760 --> 01:40:51.080
So for example this number all the way here

01:40:51.960 --> 01:40:54.760
If this element the zero zero element of w

01:40:55.560 --> 01:41:02.280
Because the gradient is positive it's telling us that this has a positive influence on the loss slightly nudging

01:41:03.160 --> 01:41:04.440
w

01:41:04.440 --> 01:41:06.440
slightly taking w zero zero

01:41:07.000 --> 01:41:09.640
And adding a small h to it

01:41:10.360 --> 01:41:14.680
Would increase the loss mildly because this gradient is positive

01:41:15.640 --> 01:41:17.640
Some of these gradients are also negative

01:41:18.600 --> 01:41:23.000
So that's telling us about the gradient information and we can use this gradient information

01:41:23.400 --> 01:41:27.800
To update the weights of this neural network. So let's not do the update

01:41:28.280 --> 01:41:30.360
It's going to be very similar to what we had in micrograd

01:41:30.760 --> 01:41:34.600
We need no loop over all the parameters because we only have one parameter

01:41:35.320 --> 01:41:40.360
Tensor and that is w. So we simply do w dot data plus equals

01:41:41.400 --> 01:41:45.240
The we can actually copy this almost exactly negative zero point one times

01:41:46.120 --> 01:41:48.120
w dot grad

01:41:48.200 --> 01:41:49.480
um

01:41:49.480 --> 01:41:52.760
And that would be the update to the tensor

01:41:54.520 --> 01:41:56.520
So that updates the tensor

01:41:58.760 --> 01:42:03.480
And because the tensor is updated we would expect that now the loss should decrease

01:42:04.360 --> 01:42:07.480
So here if I print loss

01:42:09.480 --> 01:42:11.160
That item

01:42:11.160 --> 01:42:13.160
It was 3.76 right

01:42:13.160 --> 01:42:17.800
So we've updated the w here. So if I recalculate forward pass

01:42:19.000 --> 01:42:22.920
Loss now should be slightly lower. So 3.76 goes to

01:42:23.560 --> 01:42:25.560
3.74

01:42:25.800 --> 01:42:30.120
And then we can again set to set grad to none and backward

01:42:30.840 --> 01:42:32.600
update

01:42:32.600 --> 01:42:34.600
And now the parameters changed again

01:42:34.920 --> 01:42:39.960
So if we recalculate the forward pass, we expect a lower loss again 3.72

01:42:42.200 --> 01:42:45.640
Okay, and this is again doing the we're now doing reading the set

01:42:48.520 --> 01:42:55.000
And when we achieve a low loss that will mean that the network is assigning high probabilities to the correct next characters

01:42:55.240 --> 01:42:58.600
Okay, so I rearranged everything and I put it all together from scratch

01:42:59.400 --> 01:43:02.280
So here is where we construct our data set of bigrams

01:43:03.240 --> 01:43:06.040
You see that we are still iterating only over the first word emma

01:43:06.920 --> 01:43:08.920
I'm going to change that in a second

01:43:09.080 --> 01:43:16.120
I added a number that counts the number of elements in axis so that we explicitly see that number of examples is five

01:43:16.920 --> 01:43:19.640
Because currently we're just working with emma. There's five bigrams there

01:43:20.600 --> 01:43:23.240
And here I added a loop of exactly what we had before

01:43:23.720 --> 01:43:28.360
So we had 10 iterations of gradient descent of forward pass backward pass and an update

01:43:29.000 --> 01:43:32.120
And so running these two cells initialization and gradient descent

01:43:32.840 --> 01:43:34.840
Gives us some improvement

01:43:35.400 --> 01:43:37.400
on the last function

01:43:38.200 --> 01:43:40.200
But now I want to use all the words

01:43:41.720 --> 01:43:45.640
And there's not five but 228,000 bigrams now

01:43:46.680 --> 01:43:50.440
However, this should require no modification whatsoever. Everything should just run

01:43:50.760 --> 01:43:57.160
Because all the code we wrote doesn't care if there's five bigrams or 228,000 bigrams and with everything we should just work

01:43:57.320 --> 01:43:58.440
So

01:43:58.440 --> 01:44:00.440
You see that this will just run

01:44:00.440 --> 01:44:03.800
But now we are optimizing over the entire training set of all the bigrams

01:44:04.680 --> 01:44:09.960
And you see now that we are decreasing very slightly. So actually we can probably afford the larger learning rate

01:44:12.360 --> 01:44:14.360
Can probably afford even larger learning rate

01:44:20.680 --> 01:44:27.560
Even 50 seems to work on this very very simple example, right? So let me re-initialize and let's run 100 iterations

01:44:29.240 --> 01:44:31.240
See what happens

01:44:33.000 --> 01:44:35.000
Okay

01:44:36.280 --> 01:44:38.280
We seem to be

01:44:39.080 --> 01:44:42.200
Coming up to some pretty good losses here 2.47

01:44:42.840 --> 01:44:44.600
Let me run 100 more

01:44:44.600 --> 01:44:46.840
What is the number that we expect by the way in the loss?

01:44:47.240 --> 01:44:50.680
We expect to get something around what we had originally actually

01:44:52.040 --> 01:44:55.320
So all the way back if you remember in the beginning of this video when we

01:44:55.960 --> 01:44:57.400
optimized

01:44:57.400 --> 01:44:58.840
Just by counting

01:44:58.840 --> 01:45:00.840
Our loss was roughly 2.47

01:45:01.560 --> 01:45:03.560
After we added smoothing

01:45:03.560 --> 01:45:05.800
But before smoothing we had roughly 2.45

01:45:06.600 --> 01:45:08.360
likely it

01:45:08.360 --> 01:45:09.720
Sorry loss

01:45:09.720 --> 01:45:13.320
And so that's actually roughly the vicinity of what we expect to achieve

01:45:13.800 --> 01:45:18.600
But before we achieved it by counting and here we are achieving the roughly the same result

01:45:18.760 --> 01:45:20.760
But with gradient based optimization

01:45:21.000 --> 01:45:25.640
So we come to about 2.46 2.45, etc

01:45:26.280 --> 01:45:29.720
And that makes sense because fundamentally we're not taking any additional information

01:45:29.880 --> 01:45:33.080
We're still just taking in the previous character and trying to predict the next one

01:45:33.640 --> 01:45:37.080
But instead of doing it explicitly by counting and normalizing

01:45:38.200 --> 01:45:42.360
We are doing it with gradient based learning and it just so happens that the explicit approach

01:45:42.680 --> 01:45:48.120
Happens to very well optimize the loss function without any need for gradient based optimization

01:45:48.520 --> 01:45:52.600
Because the setup for bi-gram language models are is so straightforward. It's so simple

01:45:52.920 --> 01:45:57.720
We can just afford to estimate those probabilities directly and maintain them in a table

01:45:58.920 --> 01:46:02.200
But the gradient based approach is significantly more flexible

01:46:02.920 --> 01:46:05.320
so we've actually gained a lot because

01:46:06.680 --> 01:46:08.680
What we can do now is

01:46:09.240 --> 01:46:12.280
We can expand this approach and complexify the neural net

01:46:12.840 --> 01:46:17.400
So currently we're just taking a single character and feeding into a neural net and the neural is extremely simple

01:46:17.800 --> 01:46:20.040
But we're about to iterate on this substantially

01:46:20.440 --> 01:46:27.080
We're going to be taking multiple previous characters and we're going to be feeding them into increasingly more complex neural nets

01:46:27.480 --> 01:46:31.400
But fundamentally out the output of the neural net will always just be logits

01:46:32.680 --> 01:46:35.160
And those logits will go through the exact same transformation

01:46:35.560 --> 01:46:37.560
We're going to take them through a softmax

01:46:37.960 --> 01:46:43.080
Calculate the loss function and the negative log likelihood and do gradient based optimization

01:46:43.640 --> 01:46:49.000
And so actually as we complexify the neural nets and work all the way up to transformers

01:46:49.800 --> 01:46:51.800
None of this will really fundamentally change

01:46:51.960 --> 01:46:55.000
None of this will fundamentally change the only thing that will change is

01:46:55.640 --> 01:47:02.360
The way we do the forward pass or we've taken some previous characters and calculate logits for the next character in a sequence

01:47:02.920 --> 01:47:04.920
that will become more complex

01:47:05.160 --> 01:47:08.200
And that will use the same machinery to optimize it

01:47:08.920 --> 01:47:10.680
and

01:47:10.680 --> 01:47:12.680
It's not obvious how we would have extended

01:47:13.160 --> 01:47:14.920
This bygram approach

01:47:14.920 --> 01:47:17.160
Into the case where there are many more

01:47:17.480 --> 01:47:23.800
Characters at the input because eventually these tables would get way too large because there's way too many combinations

01:47:24.200 --> 01:47:26.200
Of what previous characters

01:47:26.200 --> 01:47:27.880
could be

01:47:27.880 --> 01:47:31.720
If you only have one previous character we can just keep everything in a table the counts

01:47:32.040 --> 01:47:37.080
But if you have the last 10 characters that are in but we can't actually keep everything in the table anymore

01:47:37.400 --> 01:47:42.600
So this is fundamentally an unscalable approach and the neural network approach is significantly more scalable

01:47:43.080 --> 01:47:48.280
And it's something that actually we can improve on over time. So that's where we will be digging next

01:47:48.520 --> 01:47:50.520
I wanted to point out two more things

01:47:51.160 --> 01:47:52.280
number one

01:47:52.280 --> 01:47:54.280
I want you to notice that this

01:47:55.080 --> 01:47:56.760
x-ank here

01:47:56.760 --> 01:48:02.280
This is made up of one-hot vectors and then those one-hot vectors are multiplied by this w matrix

01:48:03.240 --> 01:48:07.880
And we think of this as a multiple neurons being forwarded in a fully connected manner

01:48:08.680 --> 01:48:10.840
But actually what's happening here is that for example

01:48:11.960 --> 01:48:17.000
If you have a one-hot vector here that has a one at say the fifth dimension

01:48:17.720 --> 01:48:20.200
Then because of the way the matrix multiplication works

01:48:21.160 --> 01:48:26.680
Multiplying that one-hot vector with w actually ends up plucking out the fifth row of w

01:48:27.640 --> 01:48:30.520
Lot logits would become just the fifth row of w

01:48:31.320 --> 01:48:34.440
And that's because of the way the matrix multiplication works

01:48:37.000 --> 01:48:39.240
So that's actually what ends up happening

01:48:40.120 --> 01:48:44.920
So but that's actually exactly what happened before because remember all the way up here

01:48:45.560 --> 01:48:47.480
We have a bi-gram

01:48:47.560 --> 01:48:54.200
We took the first character and then that first character indexed into a row of this array here

01:48:54.920 --> 01:48:58.120
And that row gave us the probability distribution for the next character

01:48:58.600 --> 01:49:01.800
So the first character was used as a lookup into a

01:49:03.640 --> 01:49:05.640
Matrix here to get the probability distribution

01:49:06.280 --> 01:49:10.120
Well, that's actually exactly what's happening here because we're taking the index

01:49:10.600 --> 01:49:12.920
We're encoding it as one-hot and multiplying it by w

01:49:13.480 --> 01:49:15.880
So logits literally becomes the

01:49:18.040 --> 01:49:22.360
The appropriate row of w and that gets just as before

01:49:23.160 --> 01:49:26.840
Exponentiated to create the counts and then normalized and becomes probability

01:49:27.480 --> 01:49:30.280
So this w here is literally

01:49:31.400 --> 01:49:33.800
The same as this array here

01:49:35.080 --> 01:49:41.160
But w remember is the log counts not the counts. So it's more precise to say that w

01:49:41.400 --> 01:49:45.400
Exponentiated w.exp is this array

01:49:46.200 --> 01:49:48.680
But this array was filled in by counting

01:49:49.320 --> 01:49:51.320
and by basically

01:49:51.960 --> 01:49:55.640
Populating the counts of bi-grams whereas in the gradient based framework

01:49:55.880 --> 01:49:58.760
We initialize it randomly and then we let the loss

01:49:59.400 --> 01:50:02.440
Guide us to arrive at the exact same array

01:50:03.240 --> 01:50:05.240
So this array exactly here

01:50:05.800 --> 01:50:06.760
is

01:50:06.840 --> 01:50:11.400
Basically the array w at the end of optimization except we arrived at it

01:50:12.280 --> 01:50:14.280
piece by piece by following the loss

01:50:15.000 --> 01:50:19.880
And that's why we also obtained the same loss function at the end and the second note is if I come here

01:50:20.520 --> 01:50:25.480
remember the smoothing where we added fake counts to our counts in order to

01:50:26.040 --> 01:50:30.280
Smooth out and make more uniform the distributions of these probabilities

01:50:31.000 --> 01:50:33.640
And that prevented us from assigning zero probability to

01:50:34.360 --> 01:50:36.360
Um to any one bi-gram

01:50:37.240 --> 01:50:39.560
Now if I increase the count here

01:50:40.280 --> 01:50:42.280
What's happening to the probability?

01:50:42.840 --> 01:50:47.240
As I increase the count probability becomes more and more uniform

01:50:47.960 --> 01:50:51.480
Right because these counts go only up to like 900 or whatever

01:50:51.480 --> 01:50:54.600
So if I'm adding plus a million to every single number here

01:50:55.160 --> 01:50:58.840
You can see how uh the row and its probability then when we divide

01:50:59.080 --> 01:51:03.960
Is just going to become more and more close to exactly even probability uniform distribution

01:51:05.160 --> 01:51:09.560
It turns out that the gradient based framework has an equivalent to smoothing

01:51:10.760 --> 01:51:12.760
In particular

01:51:13.160 --> 01:51:15.160
Think through these w's here

01:51:15.880 --> 01:51:17.880
Which we initialized randomly

01:51:18.520 --> 01:51:21.320
We could also think about initializing w's to be zero

01:51:22.120 --> 01:51:24.120
If all the entries of w are zero

01:51:25.960 --> 01:51:28.120
Then you'll see that logits will become all zero

01:51:28.840 --> 01:51:31.160
And then exponentiating those logits becomes all one

01:51:32.120 --> 01:51:34.840
And then the probabilities turn out to be exactly uniform

01:51:35.720 --> 01:51:40.040
So basically when w's are all equal to each other or say especially zero

01:51:41.240 --> 01:51:43.480
Then the probabilities come out completely uniform

01:51:44.440 --> 01:51:45.480
So

01:51:45.480 --> 01:51:48.360
Trying to incentivize w to be near zero

01:51:49.240 --> 01:51:51.240
Is basically equivalent

01:51:51.240 --> 01:51:58.040
To label smoothing and the more you incentivize that in a loss function the more smooth distribution you're going to achieve

01:51:58.920 --> 01:52:01.320
So this brings us to something that's called regularization

01:52:01.960 --> 01:52:07.720
Where we can actually augment the loss function to have a small component that we call a regularization loss

01:52:09.000 --> 01:52:13.720
In particular what we're going to do is we can take w and we can for example square all of its entries

01:52:14.680 --> 01:52:16.680
And then we can um oops

01:52:17.800 --> 01:52:19.000
Sorry about that

01:52:19.000 --> 01:52:21.400
We can take all the entries of w and we can sum them

01:52:23.640 --> 01:52:27.160
And because we're squaring uh, there will be no signs anymore

01:52:27.160 --> 01:52:28.360
Um

01:52:28.360 --> 01:52:30.920
negatives and positives all get squashed to be positive numbers

01:52:31.480 --> 01:52:36.600
And then the way this works is you achieve zero loss if w is exactly or zero

01:52:37.160 --> 01:52:40.280
But if w has non-zero numbers you accumulate loss

01:52:41.160 --> 01:52:44.200
And so we can actually take this and we can add it on here

01:52:44.920 --> 01:52:47.800
So we can do something like loss plus

01:52:48.920 --> 01:52:50.440
w square

01:52:50.440 --> 01:52:51.960
dot sum

01:52:51.960 --> 01:52:56.360
Or let's actually instead of some let's take a mean because otherwise the sum gets too large

01:52:57.480 --> 01:52:59.480
So mean is like a little bit more manageable

01:53:01.320 --> 01:53:07.960
And then we have a regularization loss here like say 0.01 times or something like that you can choose the regularization strength

01:53:09.320 --> 01:53:11.320
And then we can just optimize this

01:53:12.120 --> 01:53:17.880
And now this optimization actually has two components not only is it trying to make all the probabilities work out

01:53:18.280 --> 01:53:23.400
But in addition to that there's an additional component that simultaneously tries to make all w's be zero

01:53:23.880 --> 01:53:30.040
Because if w's are non-zero you feel a loss and so minimizing this the only way to achieve that is for w to be zero

01:53:30.680 --> 01:53:37.320
And so you can think of this as adding like a spring force or like a gravity force that that pushes w to be zero

01:53:37.800 --> 01:53:40.920
So w wants to be zero and the probabilities want to be uniform

01:53:41.400 --> 01:53:46.680
But they also simultaneously want to match up your your probabilities as indicated by the data

01:53:47.480 --> 01:53:50.200
And so the strength of this regularization

01:53:50.760 --> 01:53:52.760
is exactly controlling

01:53:52.760 --> 01:53:54.440
the amount of counts

01:53:54.440 --> 01:53:56.440
that you add here

01:53:57.240 --> 01:53:59.240
Adding a lot more counts

01:53:59.400 --> 01:54:00.840
here

01:54:00.840 --> 01:54:02.840
corresponds to

01:54:02.840 --> 01:54:04.680
Increasing this number

01:54:04.680 --> 01:54:09.080
Because the more you increase it the more this part of the loss function dominates this part

01:54:09.560 --> 01:54:14.680
And the more these these weights will be unable to grow because as they grow

01:54:15.480 --> 01:54:17.480
They accumulate way too much loss

01:54:18.440 --> 01:54:20.440
And so if this is strong enough

01:54:21.240 --> 01:54:25.640
Then we are not able to overcome the force of this loss and we will never

01:54:26.760 --> 01:54:28.760
And basically everything will be uniform predictions

01:54:29.400 --> 01:54:32.440
So I thought that's kind of cool. Okay, and lastly before we wrap up

01:54:33.160 --> 01:54:36.040
I wanted to show you how you would sample from this neural net model

01:54:36.920 --> 01:54:39.800
And I copy pasted the sampling code from before

01:54:40.760 --> 01:54:43.640
Where remember that we sampled five times

01:54:44.280 --> 01:54:49.640
And all we did is we started zero we grabbed the current ix row of p

01:54:50.440 --> 01:54:52.440
And that was our probability row

01:54:52.440 --> 01:54:58.120
From which we sampled the next index and just accumulated that and break when zero

01:54:58.920 --> 01:55:01.880
And running this gave us these results

01:55:03.880 --> 01:55:07.880
I still have the p in memory. So this is fine now

01:55:09.160 --> 01:55:13.480
The speed doesn't come from the row of p instead. It comes from this neural net

01:55:15.000 --> 01:55:17.000
First we take ix

01:55:17.080 --> 01:55:19.960
And we encode it into a one-hot row

01:55:20.440 --> 01:55:22.440
of xank

01:55:22.440 --> 01:55:24.440
This xank multiplies our w

01:55:25.160 --> 01:55:29.880
Which really just plugs out the row of w corresponding to ix really that's what's happening

01:55:30.440 --> 01:55:34.200
And that gets our logits and then we normalize those logits

01:55:34.920 --> 01:55:40.680
Exponentiate to get counts and then normalize to get the distribution and then we can sample from the distribution

01:55:41.320 --> 01:55:43.320
So if I run this

01:55:45.240 --> 01:55:49.880
Kind of anti-climatic or climatic depending how you look at it, but we get the exact same result

01:55:51.160 --> 01:55:52.280
Um

01:55:52.280 --> 01:55:56.360
And that's because this is in the identical model. Not only does it achieve the same loss

01:55:57.000 --> 01:55:58.120
but

01:55:58.120 --> 01:56:03.800
As I mentioned, these are identical models and this w is the log counts of what we've estimated before

01:56:04.200 --> 01:56:09.080
But we came to this answer in a very different way and it's got a very different interpretation

01:56:09.400 --> 01:56:13.560
But fundamentally, this is basically the same model and gives the same samples here. And so

01:56:14.840 --> 01:56:17.480
That's kind of cool. Okay, so we've actually covered a lot of ground

01:56:18.040 --> 01:56:21.240
We introduced the bi-gram character level language model

01:56:22.040 --> 01:56:27.640
We saw how we can train the model how we can sample from the model and how we can evaluate the quality of the model

01:56:27.880 --> 01:56:29.880
Using the negative log likelihood loss

01:56:30.200 --> 01:56:35.560
And then we actually trained the model in two completely different ways that actually get the same result and the same model

01:56:36.280 --> 01:56:40.840
In the first way, we just counted up the frequency of all the bi-grams and normalized

01:56:41.480 --> 01:56:47.160
In the second way, we used the uh negative log likelihood loss as a guide

01:56:47.720 --> 01:56:49.720
To optimizing the counts matrix

01:56:50.840 --> 01:56:55.480
Or the counts array so that the loss is minimized in the in a gradient based framework

01:56:55.800 --> 01:56:57.800
and we saw that both of them give the same result

01:56:58.440 --> 01:57:00.360
and

01:57:00.360 --> 01:57:01.400
That's it

01:57:01.400 --> 01:57:04.440
Now the second one of these the gradient based framework is much more flexible

01:57:04.920 --> 01:57:07.400
And right now our neural network is super simple

01:57:07.560 --> 01:57:13.400
We're taking a single previous character and we're taking it through a single linear layer to calculate the logits

01:57:14.120 --> 01:57:19.720
This is about to complexify. So in the follow-up videos, we're going to be taking more and more of these characters

01:57:20.520 --> 01:57:22.440
And we're going to be feeding them into a neural net

01:57:22.920 --> 01:57:27.080
But this neural net will still output the exact same thing. The neural net will output logits

01:57:28.040 --> 01:57:32.760
And these logits will still be normalized in the exact same way and all the loss and everything else and the gradient

01:57:32.840 --> 01:57:37.720
Gradient based framework everything stays identical. It's just that this neural net will now

01:57:38.140 --> 01:57:40.140
Complexify all the way to transformers

01:57:40.140 --> 01:57:44.300
So that's going to be pretty awesome and i'm looking forward to it for now. Bye

