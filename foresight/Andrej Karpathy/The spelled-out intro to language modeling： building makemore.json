{"text": " Hi everyone, hope you're well and next up what I'd like to do is I'd like to build out make more Like micrograd before it make more is a repository that I have on my github web page You can look at it, but just like with micrograd I'm going to build it out step by step and I'm going to spell everything out So we're going to build it out slowly and together Now what is make more? Make more as the name suggests makes more of things that you give it So here's an example names.txt is an example data set to make more and When you look at names.txt you'll find that it's a very large data set of names so Here's lots of different types of names in fact I believe there are 32,000 names that I've sort of found randomly on the government website and If you train make more on this data set it will learn to make more of things like this And in particular in this case that will mean more things that sound name like but are actually unique names And maybe if you have a baby and you're trying to assign name Maybe you're looking for a cool new sounding unique name make more might help you So here are some example generations from the neural network once we train it on our data set So here's some example unique names that it will generate don't tell I rot Zendy and so on and so all these are sound name like but they're not of course names So under the hood make more is a character level language model So what that means is that is treating every single line here as an example and within each example It's treating them all as sequences of individual characters So R E E S E is this example and that's the sequence of characters And that's the level on which we are building out make more and what it means to be a character level language model Then is that it's just sort of modeling those sequences of characters and it knows how to predict the next character in the sequence Now we're actually going to implement a large number of Character level language models in terms of the neural networks that are involved in predicting the next character in a sequence So very simple by Graham and back of word models Multilevel perceptrons recurrent neural networks all the way to modern Transformers in fact a transformer that we will build will be basically the equivalent transformer to GPT2 if you have heard of GPT So that's kind of a big deal It's a modern network and by the end of the series you will actually understand how that works On the level of characters now to give you a sense of the extensions here After characters we will probably spend some time on the word level so that we can generate documents of words not just little you know segments of characters But we can generate entire large much larger documents And then we're probably going to go into images and image text Networks such as Dali stable diffusion and so on but for now we have to start Here character level language modeling. Let's go So like before we are starting with a completely blank GPTN notebook page The first thing is I would like to basically load up the data set names.txt So we're going to open up names.txt for reading and We're going to read in everything into a massive string and Then because it's a massive string we'd only like the individual words and put them in the list So let's call split lines on that string to get all of our words as a Python list of strings so basically we can look at for example the first 10 words and We have that it's a list of Emma Olivia Eva and so on and if we look at The top of the page here that is indeed what we see um So that's good This list actually makes me feel that This is probably sorted by frequency But okay, so these are the words now we'd like to actually like learn a little bit more about this data set Let's look at the total number of words. We expect this to be roughly 32 000 And then what is the for example shortest word? so min of line of each word for w in words So the shortest word will be length two And max of land w for w in words. So the longest word will be 15 characters So let's now think through our very first language model As I mentioned a character level language model is predicting the next character in a sequence given Already some concrete sequence of characters before it Now what we have to realize here is that every single word here like Isabella Is actually quite a few examples packed in to that single word Because what is a an existence of a word like Isabella in the data set telling us really it's saying that The character i is a very likely character to come first in a sequence of a name The character s is likely to come after i The character a is likely to come after is The character b is very likely to come after isa and someone all the way to a following Isabella And then there's one more example actually packed in here and that is that After there's Isabella The word is very likely to end So that's one more sort of explicit piece of information that we have here that we have to be careful with And so there's a lot packed into a single individual word in terms of the Statistical structure of what's likely to follow in these character sequences And then of course we don't have just an individual word. We actually have 32 000 of these and so there's a lot of structure here to model Now in beginning what I'd like to start with is I'd like to start with building a bi-gram language model Now in a bi-gram language model, we're always working with just two characters at a time So we're only looking at one character that we are given and we're trying to predict the next character in the sequence so What characters are likely to follow are what characters are likely to follow a and so on and we're just modeling that kind of a little local structure And we're forgetting the fact that we may have a lot more information We're always just looking at the previous character to predict the next one So it's a very simple and weak language model, but I think it's a great place to start So now let's begin by looking at these bi-grams in our data set and what they look like and these bi-grams again are just two characters in a row so for w in words each w here is an individual word a string we want to iterate uh for We want to iterate this word with consecutive characters So two characters at a time sliding it through the word Now a interesting nice way cute way to do this in python, by the way Is doing something like this for character one character two in zip of w and w at one one column print character one character two And let's not do all the words Let's just do the first three words and i'm going to show you in a second how this works But for now basically as an example, let's just do the very first word alone emma You see how we have a emma and this will just print em, mm, ma And the reason this works is because w is the string emma w at one column is the string mma And zip takes two iterators and it pairs them up And then creates an iterator over the tuples of their consecutive entries And if any one of these lists is shorter than the other then it will just uh halt and return So basically, that's why we return em, mm, mm, ma But then because this iterator second one here runs out of elements zip just ends and that's why we only get these tuples So pretty cute So these are the consecutive elements in the first word Now we have to be careful because we actually have more information here than just these three Examples as I mentioned we know that e is the is very likely to come first and we know that a in this case is coming last So one way to do this is basically we're going to create special array here characters And um, we're going to hallucinate a special start token here I'm going to call it like special start So this is a list of one element plus w And then plus a special end character And the reason i'm wrapping the list of w here is because w is a string mma List of w will just have the individual characters in the list And then doing this again now, but not iterating over w's but over the characters Will give us something like this So e is likely so this is a bygram of the start character and e and this is a bygram of the a and the special end character And now we can look at for example what this looks like for olivia or heva And indeed we can actually Potentially do this for the entire data set, but we won't print that that's going to be too much But these are the individual character bygrams and we can print them Now in order to learn the statistics about which characters are likely to follow other characters The simplest way in the bygram language models is to simply do it by counting So we're basically just going to count how often any one of these combinations occurs in the training set In these words So we're going to need some kind of a dictionary that's going to maintain some counts for every one of these bygrams So let's use a dictionary b And this will map these bygrams So bygram is a tuple of character one character two And then b at bygram Will be b dot get of bygram Which is basically the same as b at bygram But in the case that bygram is not in the dictionary b. We would like to by default return a zero Plus one So this will basically add up all the bygrams and count how often they occur Let's get rid of printing or rather Let's keep the printing and let's just inspect what b is in this case And we see that many bygrams occur just a single time this one allegedly occurred three times So a was an ending character three times And that's true for all of these words all of emma olivia and eva and with a Uh, so that's why this occurred three times Um now let's do it for all the words Oops, I should not have printed I meant to erase that Let's kill this Let's just run And now b will have the statistics of the entire data set So these are the counts across all the words of the individual bygrams And we could for example look at some of the most common ones and least common ones Um, this kind of grows in python But the way to do this the simplest way I like is we just use b dot items b dot items returns the tuples of Key value in this case the keys are the character bygrams and the values are the counts And so then what we want to do is we want to do um Sort it of this But by default sort is on the first um On the first item of a tuple But we want to sort by the values which are the second element of a tuple that is the key value So we want to use the key equals lambda That takes the key value And returns the key value at the one not at zero but at one which is the count So we want to sort by the count Of these elements And actually we want it to go backwards So here what we have is the bygram q and r occurs only a single time dz occurred only a single time and when we sort this the other way around We're going to see the most likely bygrams So we see that n was very often an ending character many many times and apparently n almost always follows an a And that's a very likely combination as well Um So This is kind of the individual counts that we achieve over the entire data set Now it's actually going to be significantly more convenient for us to Keep this information in a two-dimensional array instead of a python dictionary so we're going to store this information in a 2d array and The rows are going to be the first character of the bygram and the columns are going to be the second character And each entry in the two-dimensional array will tell us how often that first character follows the second character in the data set So in particular the array representation that we're going to use or the library is that of pytorch And pytorch is a deep learning neural network framework But part of it is also this torch dot tensor, uh, which allows us to create multi-dimensional arrays and manipulate them very efficiently So let's import pytorch, which you can do by import torch And then we can create arrays So let's create an array of zeros And we give it a um size of this array. Let's create a three by five array as an example and This is a three by five array of zeros And by default you'll notice a dot d type, which is short for data type is float 32 So these are single precision floating point numbers Because we are going to represent counts. Let's actually use d type as torch dot in 32 So these are 32 bit integers So now you see that we have integer data inside this tensor Now tensors allow us to really, um Manipulate all the individual entries and do it very efficiently So for example, if we want to change this bit We have to index into the tensor and in particular here This is the first row and the Um, because it's zero indexed. So this is row index one and column index zero one two three So a at one comma three we can set that to one And then a we'll have a one over there We can of course also do things like this. So now a will be two over there Or three And also we can for example say a zero zero is five And then a we'll have a five over here So that's how we can index into the arrays Now, of course the array that we are interested in is much much bigger So for our purposes, we have 26 letters of the alphabet And then we have two special characters s and e So, uh, we want 26 plus two or 28 by 28 array And let's call it the capital n because it's going to represent sort of the counts Let me erase this stuff So that's the array that starts at zeros 28 by 28 And now let's copy paste this Here but instead of having a dictionary b Which we're going to erase we now have an n Now the problem here is that we have these characters which are strings, but we have to now basically index into a Array and we have to index using integers. So we need some kind of a lookup table from characters to integers So let's construct such a character array And the way we're going to do this is we're going to take all the words which is a list of strings We're going to concatenate all of it into a massive string. So this is just simply the entire dataset as a single string We're going to pass this to the set constructor which takes this massive string And throws out duplicates because sets do not allow duplicates So set of this will just be the set of all the lowercase characters And there should be a total of 26 of them And now we actually don't want a set we want a list But we don't want a list sorted in some weird arbitrary way. We want it to be sorted from a to z So sorted list So those are our characters Now what we want is this lookup table as I mentioned. So let's create a special s2i. I will call it s is string or character and this will be an s2i mapping for Is in enumerate of these characters So enumerate basically gives us this iterator over the integer index and the actual element of the list and then we are mapping the character to the integer So s2i Is a mapping from a to 0 b to 1 etc all the way from z to 25 And that's going to be useful here, but we actually also have to specifically set that s will be 26 And s2i at e Will be 27 right because z was 25 So those are the lookups and now we can come here and we can map Both character 1 and character 2 to their integers So this will be s2i character 1 And ix2 will be s2i of character 2 And now we should be able to Do this line, but using our array. So n at ix1 ix2 This is the two-dimensional array indexing. I've shown you before and honestly just plus equals 1 Because everything starts at zero So this should work And give us a large 28 by 28 array Of all these counts. So if we print n This is the array, but of course it looks ugly So let's erase this ugly mess and let's try to visualize it a bit more nicer So for that we're going to use a library called mathplotlib So mathplotlib allows us to create figures. So we can do things like pltim show of the count array So this is the 20 by 28 array And this is a structure, but even this I would say is still pretty ugly So we're going to try to create a much nicer visualization of it and I wrote a bunch of code for that The first thing we're going to need is We're going to need to invert This array here this dictionary. So s2i is a mapping from s to i And in i2s, we're going to reverse this dictionary So it rid of all the items and just reverse that array So i2s maps inversely from 0 to a 1 to b etc So we'll need that And then here's the code that I came up with to try to make this a little bit nicer To create a figure We plot n And then we do and then we visualize a bunch of things later. Let me just run it so you get a sense of what this is Okay, so you see here that we have the array spaced out And every one of these is basically like b follows g zero times b follows h 41 times So a follows j 175 times And so what you can see that i'm doing here is first i show that entire array And then I iterate over all the individual little cells here And I create a character string here Which is the inverse mapping i2s of the integer i and the integer j So those are the bigrams in a character representation And then I plot just the bigram text and then I plot the number of times that this bigram occurs Now the reason that there's a dot item here is because when you index into these arrays, these are torch tensors You see that we still get a tensor back So the type of this thing you think it would be just an integer 149, but it's actually a torch dot tensor And so if you do dot item, then it will pop out that individual integer So it will just be 149 So that's what's happening there. And these are just some options to make it look nice So what is the structure of this array? We have all these counts and we see that some of them occur often and some of them do not occur often Now if you scrutinize this carefully, you will notice that we're not actually being very clever That's because when you come over here You'll notice that for example, we have an entire row of completely zeros And that's because the end character Is never possibly going to be the first character of a bigram because we're always placing these end tokens all at the end of a bigram Similarly, we have entire column zeros here because the s Character will never possibly be the second element of a bigram because we always start with s and we end with e and we only have the words in between So we have an entire column of zeros an entire row of zeros And in this little two by two matrix here as well The only one that can possibly happen is if s directly follows e That can be non-zero if we have a word that has no letters So in that case, there's no letters in the word. It's an empty word and we just have s follows e But the other ones are just not possible And so we're basically wasting space and not only that but the s and the e are getting very crowded here I was using these brackets because there's convention and natural language processing to use these kinds of brackets to denote special tokens But we're going to use something else So let's fix all this and make it prettier We're not actually going to have two special tokens. We're only going to have one special token So we're going to have n by n array of 27 by set 27 instead Instead of having two we will just have one and I will call it a dot Okay Let me swing this over here Now one more thing that I would like to do is I would actually like to make this special character half position zero And I would like to offset all the other letters off. I find that a little bit more pleasing um, so We need a plus one here so that the first character which is a will start at one So s to i will now be a starts at one and dot is zero And uh i2s, of course, we're not changing this because i2s just creates a reverse mapping and this will work fine So one is a two is b zero is dot So we've reversed that here We have a dot and a dot This should work fine make sure I started zeros Count and then here we don't go up to 28 we go up to 27 and this should just work Okay, so we see that dot dot never happened. It's at zero because we don't have empty words Then this row here now is just very simply the Counts for all the first letters. So G j starts a word h starts a word i starts a word etc And then these are all the ending characters And in between we have the structure of what characters follow each other So this is the counts array of our entire Uh dataset So this array actually has all of the information necessary for us to actually sample from this bigram character level language model and Roughly speaking what we're going to do is we're just going to start following these probabilities and these counts And we're going to start sampling from the from model So in the beginning, of course, um, we start with the dot the start token Dot so to sample the first character of a name. We're looking at this row here So we see that we have the counts and those counts externally are telling us how often any one of these characters is to start a word So if we take this n and we grab the first row We can do that by using just indexing a zero and then using this notation column for the rest of that row so n zero column Is indexing into the zero? Row and then it's grabbing all the columns And so this will give us a one-dimensional array Of the first row. So zero four four ten You know zero four four ten one three oh six one five four two Etc. Just the first row the shape of this is 27 it's just the row of 27 And the other way that you can do this also is you just you don't actually give this You just grab the zero row like this. This is equal Now these are the counts And now what we'd like to do is we'd like to basically um sample from this Since these are the raw counts, we actually have to convert this to probabilities So we create a probability vector So we'll take n of zero And we'll actually convert this to float first Okay, so these integers are converted to float floating point numbers and the reason we're creating floats is because we're about to normalize these counts So to create a probability distribution here, we want to divide We basically want to do p p p divide p that sum And now we get a vector of smaller numbers and these are now probabilities So of course because we divided by the sum the sum of p now is one So this is a nice proper probability distribution It sums to one and this is giving us the probability for any single character to be the first character of a word So now we can try to sample from this distribution to sample from these distributions We're going to use torsion multinomial, which I've pulled up here So torsion multinomial returns a Samples from the multinomial probability distribution, which is a complicated way of saying you give me probabilities And I will give you integers which are sampled According to the probability distribution So this is the signature of the method and to make everything deterministic We're going to use a generator object in pi torch So this makes everything deterministic so when you run this on your computer You're going to the exact get the exact same results that i'm getting here on my computer So let me show you how this works Here's the deterministic way of creating a torch generator object Seeding it with some number that we can agree on So that seeds a generator gets gives us an object g And then we can pass that g to a function that creates Here random numbers torch.rand creates random numbers three of them And it's using this generator object to as a source of randomness So Without normalizing it I can just print This is sort of like numbers between zero and one that are random according to this thing and whenever I run it again I'm always going to get the same result because I keep using the same generator object, which i'm seeding here And then if I divide To normalize i'm going to get a nice probability distribution of just three elements And then we can use torsion multinomial to draw samples from it. So this is what that looks like Torsion multinomial will take the torch tensor of probability distributions Then we can ask for a number of samples like say 20 Replacement equals true means that when we draw an element We will we can draw it and then we can put it back into the list of eligible indices to draw again And we have to specify replacement as true because by default for some reason it's false And I think You know, it's just something to be careful with And the generator is passed in here. So we are going to always get deterministic results the same results So if I run these two We're going to get a bunch of samples from this distribution Now you'll notice here that the probability for the first element in this tensor is 60 So in these 20 samples, we'd expect 60 of them to be zero We'd expect 30 percent of them to be one And because the the element index two Has only 10 probability Very few of these samples should be two and indeed we only have a small number of twos And we can sample as many as we like And the more we sample the more These numbers should roughly have the distribution here So we should have lots of zeros half as many Once and we should have three times s few Oh, sorry s few ones and three times s few twos So you see that we have very few twos. We have some ones and most of them are zero So that's what torsion multinomial is doing for us here We are interested in this row. We've created this P here and now we can sample from it So if we use the same seed And then we sample from this distribution, let's just get one sample Then we see that the sample is say 13 So this will be the index And let's you see how it's a tensor that wraps 13 We again have to use dot item to pop out that integer And now index would be just the number 13 And of course the um, we can do we can map the i2s of ix to figure out exactly which character We're sampling here. We're sampling m So we're saying that the first character is m in our generation And just looking at the row here m was drawn and you we can see that m actually starts a large number of words m started 2,500 words out of 32,000 words. So almost A bit less than 10 of the words start with m. So this is actually fairly likely character to draw So that would be the first character of our word and now we can continue to sample more characters Because now we know that m started m is already sampled So now to draw the next character, we will come back here and we will look for the row That starts with m. So you see m And we have a row here so we see that m dot is 516 ma is this many mb is this many etc So these are the counts for the next row and that's the next character that we are going to now generate So I think we are ready to actually just write out the loop because I think you're starting to get a sense of how this is going to go The um We always begin at index zero because that's the start token And then while true We're going to grab the row corresponding to index That we're currently on so that's p So that's n array at ix converted to float is rp Then we normalize the speed to sum to one I accidentally ran the infinite loop We normalize p to sum to one Then we need this generator object And we're going to initialize up here and we're going to draw a single sample from this distribution And then this is going to tell us what index is going to be next If the index sampled is zero, then that's now the nth token So we will break Otherwise we are going to print s2i of ix i2s of ix And uh, that's pretty much it. We're just uh, this should work Okay more So that's the that's the name that we've sampled we started with m the next step was o then r and then dot And this dot we printed here as well so Let's now do this a few times um So let's actually create an Out list here And instead of printing we're going to append so out that append this character And then here let's just print it at the end. So let's just join up all the outs and we're just going to print more Okay, now we're always getting the same result because of the generator So if we want to do this a few times we can go for high and range 10 we can sample 10 names And we can just do that 10 times And these are the names that we're getting out Let's do 20 I'll be honest with you, this doesn't look right So I started a few minutes to convince myself that it actually is right The reason these samples are so terrible is that by gram language model is actually looks just like really terrible We can generate a few more here And you can see that they're kind of like their name like a little bit like keanu iraily etc But they're just like totally messed up And I mean the reason that this is so bad like we're generating h as a name But you have to think through it from the model's eyes It doesn't know that this h is the very first h. All it knows is that h was previously And now how likely is h the last character? Well, it's somewhat likely and so it just makes it last character It doesn't know that there were other things before it or there were not other things before it And so that's why it's generating all these like nonsense names in other ways to do this is To convince yourself that this is actually doing something reasonable even though it's so terrible is These little p's here are 27 right like 27 So how about if we did something like this? Instead of p having any structure whatsoever How about if p was just a torch dot ones? Of 27 By default, this is a float 32. So this is fine divide 27 So what I'm doing here is this is the uniform distribution, which will make everything equally likely And we can sample from that. So let's see if that does any better Okay, so it's this is what you have from a model that is completely untrained where everything is equally likely So it's obviously garbage and then if we have a trained model, which is trained on just by grams This is what we get. So you can see that it is more name like it is actually working. It's just By gram is so terrible and we have to do better Now next I would like to fix an inefficiency that we have going on here Because what we're doing here is we're always fetching a row of n from the counts matrix up ahead And then we're always doing the same things We're converting to float and we're dividing and we're doing this every single iteration of this loop And we just keep renormalizing these rows over and over again and it's extremely inefficient and wasteful So what I'd like to do is I'd like to actually prepare a matrix capital p That will just have the probabilities in it So in other words is going to be the same as the capital n matrix here of counts But every single row will have the row of probabilities That is normalized to 1 indicating the probability distribution for the next character Given the character before it As defined by which row we're in So basically what we'd like to do is we'd like to just do it up front here And then we would like to just use that row here So here we would like to just do p equals p of ix instead okay The other reason I want to do this is not just for efficiency But also I would like us to practice these n-dimensional tensors And I'd like us to practice their manipulation and especially something that's called broadcasting that we'll go into in a second We're actually going to have to become very good at these tensor manipulations Because if we're going to build out all the way to transformers, we're going to be doing some pretty complicated array operations for efficiency, and we need to really understand that and be very good at it So intuitively what we want to do is we first want to grab the floating point copy of n And I'm mimicking the line here basically And then we want to divide all the rows so that they sum to 1 So we'd like to do something like this p divide p dot sum But now we have to be careful because p dot sum actually produces a sum Sorry p equals n dot float copy p dot sum produces a Summs up all of the counts of this entire matrix n And gives us a single number of just the summation of everything So that's not the way we want to divide we want to simultaneously and in parallel divide all the rows by their respective sums So what we have to do now is we have to go into documentation for torch dot sum And we can scroll down here to a definition that is relevant to us Which is where we don't only provide an input array that we want to sum But we also provide the dimension along which we want to sum And in particular we want to sum up Over rows, right Now one more argument that I want you to pay attention to here is the keep them is false If keep them is true Then the output tensor is of the same size as input except of course the dimension along which you summed Which will become just one But if you pass in keep them as false Then this dimension is squeezed out And so torch dot sum not only does the sum and collapses dimension to be of size one But in addition it does what's called a squeeze where it squeezes out it squeezes out that dimension So basically what we want here is we instead want to do p dot sum of sum axis And in particular notice that p dot shape is 27 by 27 So when we sum up across axis zero, then we would be taking the zero dimension and we would be summing across it So when keep them is true Then this thing will not only give us the counts across um along the columns But notice that basically the shape of this is one by 27. We just get a row vector And the reason we get a row vector here again is because we pass in zero dimension So this zero dimension becomes one and we've done a sum And we get a row and so basically we've done the sum this way Vertically and arrived at just a single one by 27 vector of counts What happens when you take out keep them Is that we just get 27 so it squeezes out that dimension and we just get a one dimensional vector of size 27 Now we don't actually want One by 27 row vector because that gives us the counts or the sums across the columns We actually want to sum the other way along dimension one And you'll see that the shape of this is 27 by one. So it's a column vector It's a 27 by one vector of counts Okay, and that's because what's happened here is that we're going horizontally and this 27 by 27 matrix becomes a 27 by one array Now you'll notice by the way that um the actual numbers Of these counts are identical And that's because this special array of counts here comes from bi-gram statistics And actually it just so happens by chance Or because of the way this array is constructed that this sums along the columns or along the rows Horizontally or vertically is identical But actually what we want to do in this case is we want to sum across the uh rows horizontally So what we want here is speed at some of one would keep them true 27 by one column vector and now what we want to do is we want to divide by that Now we have to be careful here again. Is it possible to take What's a um p dot shape you see here is 27 by 27? Is it possible to take a 27 by 27 array and divide it by what is a 27 by one array? Is that an operation that you can do? And whether or not you can perform this operation is determined by what's called broadcasting rules So if you just search broadcasting semantics in torch You'll notice that there's a special definition for what's called broadcasting that uh for whether or not these two Arrays can be combined in a binary operation like division So the first condition is each tensor has at least one dimension, which is the case for us And then when iterating over the dimension sizes starting at the trailing dimension The dimension sizes must either be equal one of them is one or one of them does not exist Okay, so let's do that. We need to align the two arrays and their shapes Which is very easy because both of these shapes have two elements. So they're aligned Then we iterate over from the from the right and going to the left Each dimension must be either equal one of them is a one or one of them does not exist So in this case, they're not equal, but one of them is a one. So this is fine And then this dimension they're both equal. So this is fine So all the dimensions are fine and therefore the this operation is broadcastable So that means that this operation is allowed And what is it that these arrays do when you divide 27 by 27 by 27 by 1? What it does is that it takes this dimension one and it stretches it out it copies it to match 27 here in this case So in our case it takes this column vector, which is 27 by 1 and it copies it 27 times to make These both be 27 by 27 internally you can think of it that way and so it copies those counts And then it does an element wise division Which is what we want because these counts we want to divide by them on every single one of these columns in this matrix So this actually we expect will normalize every single row And we can check that this is true by taking the first row for example and taking it some We expect this to be one Because it's now normalized And then we expect this now Because if we actually correctly normalize all the rows we expect to get the exact same result here. So let's run this It's the exact same result So this is correct. So now I would like to scare you a little bit You actually have to like I basically encourage you very strongly to read through broadcasting semantics And I encourage you to treat this with respect and it's not something to play Fast and loose with it's something to really respect really understand and look up Maybe some tutorials for broadcasting and practice it and be careful with it because you can very quickly run it to box Let me show you what I mean You see how here we have p. That's some of one keep them this true The shape of this is 27 by 1 Let me take out this line just so we have the n and then we can see the counts We can see that this is a all the counts across all the rows And it's a 27 by 1 column vector, right? Now suppose that I tried to do the following but I erase keep them this true here What does that do if keep them is not true? It's false Then remember according to documentation it gets rid of this dimension one. It squeezes it out So basically we just get all the same counts the same result Except the shape of it is not 27 by 1. It is just 27 the one disappears But all the counts are the same So you'd think that this divide that would uh would work First of all, can we even uh write this and will it is it even is it even expected to run? Is it broadcastable? Let's determine if this result is broadcastable p dot summit one is shape Is 27 this is 27 by 27 so 27 by 27 Broadcasting into 27 so now rules of broadcasting number one align all the dimensions on the right done Now iteration over all the dimensions starting from the right going to the left All the dimensions must either be equal One of them must be one or one of them does not exist. So here they are all equal Here the dimension does not exist So internally what broadcasting will do is it will create a one here and then We see that one of them is a one and this will get copied and this will run this will broadcast Okay, so you'd expect this to work Because we we are um um This broadcasts and this we can divide this now if I run this you'd expect it to work but It doesn't Uh, you actually get garbage you get a wrong result because this is actually a bug This keep them for equals true Makes it work This is a bug In both cases we are doing the correct counts. We are summing up across the rows But keep them is saving us and making it work. So in this case I'd like you to encourage you to potentially like pause this video at this point and try to think about why this is buggy And why the keep them was necessary here Okay So the reason to do for this is I'm trying to hint it here when I was sort of giving you a bit of a hint on how this works this 27 vector Internally inside the broadcasting This becomes a one by 27 And one by 27 is a row vector, right? And now we are dividing 27 by 27 by 1 by 27 And torch will replicate this dimension. So basically, uh, it will take It will take this, uh row vector and it will copy it vertically now 27 times so the 27 by 27 lines exactly an element wise divides And so basically what's happening here is um We're actually normalizing the columns instead of normalizing the rows So you can check that what's happening here is that p at zero, which is the first row of p that sum Is not one it's seven It is the first column as an example that sums to one So to summarize where does the issue come from the issue comes from the silent adding of a dimension here Because in broadcasting rules you align on the right and go from right to left and if dimension doesn't exist you create it So that's where the problem happens We still did the counts correctly We did the counts across the rows and we got the the counts on the right here as a column vector But because the keep things was true this this uh, this dimension was discarded and now we just have a vector of 27 And because of broadcasting the way it works this vector of 27 suddenly becomes a row vector And then this row vector gets replicated Vertically and at every single point we are dividing by the by the count Uh in the opposite direction So, uh, so this thing just uh, doesn't work. You this needs to be keep them's equals true in this case so then Uh, then we have that p at zero is normalized And conversely the first column you'd expect to potentially not be normalized And this is what makes it work So pretty subtle and uh, hopefully this helps to scare you that you should Have a respect for broadcasting be careful. Check your work And uh, understand how it works under the hood and make sure that it's broadcasting in the direction that you like Otherwise, you're going to introduce very subtle bugs very hard to find bugs and uh, just be careful one more note to an efficiency We don't want to be doing this here because uh, this creates a completely new tensor that we store into p We prefer to use in place operations if possible So this would be an in-place operation has the potential to be faster It doesn't create new memory under the hood and then let's erase this We don't need it and let's also Um, just do fewer just so i'm not wasting space Okay, so we're actually in a pretty good spot now We trained a bi-gram language model and we trained it really just by counting Uh, how frequently any pairing occurs and then normalizing so that we get a nice probability So really these elements of this array p Are really the parameters of our bi-gram language model giving us and summarizing the statistics of these bi-grams So we trained a model and then we know how to sample from a model. We just iteratively Sampled the next character and uh feed it in each time and get a next character Now what i'd like to do is i'd like to somehow evaluate the quality of this model We'd like to somehow summarize the quality of this model into a single number. How good is it at predicting? the training set And as an example so in the training set we can evaluate now the training Loss and this training loss is telling us about Sort of the quality of this model in a single number just like we saw in micrograd So let's try to think through the quality of the model and how we would evaluate it Basically what we're going to do is we're going to copy paste this code That we previously used for counting Okay And let me just print these bi-grams first. We're going to use f strings And i'm going to print character one followed by character two. These are the bi-grams And then I don't want to do it for all the words. Let's just do first three words So here we have emma olivia and ava bi-grams Now what we'd like to do is we'd like to basically look at the probability that the model assigns to every one of these bi-grams So in other words, we can look at the probability, which is summarized in the matrix p of ix1, ix2 And then we can print it here as probability And because these probabilities are way too large, let me percent our column 0.4f to like truncate it a bit So what do we have here, right? We're looking at the probabilities that the model assigns to every one of these bi-grams in the data set And so we can see some of them are four percent, three percent, etc Just to have a measuring stick in our mind, by the way We have 27 possible characters or tokens and if everything was equally likely, then you'd expect all these probabilities to be four percent roughly So anything above four percent means that we've learned something useful from these bi-gram statistics And you see that roughly some of these are four percent, but some of them are as high as 40 percent 35 percent and so on So you see that the model actually assigned a pretty high probability to whatever's in the training set and so that that's a good thing Um, basically if you have a very good model You'd expect that these probabilities should be near one because that means that uh, your model is correctly predicting What's going to come next especially on the training set where you where you train your model So now we'd like to think about how can we summarize these probabilities into a single number that measures the quality of this model Now when you look at the literature into maximum likelihood estimation and statistical modeling and so on You'll see that what's typically used here is something called the likelihood And the likelihood is the product of all of these probabilities And so the product of all of these probabilities is the likelihood And it's really telling us about the probability of the entire data set assigned Assigned by the model that we've trained and that is a measure of quality So the product of these should be as high as possible When you are training the model and when you have a good model your product your product of these probabilities should be very high um Now because the product of these probabilities is an unwieldy thing to work with You can see that all of them are between zero and one. So your product of these probabilities will be a very tiny number um So for convenience what people work with usually is not the likelihood, but they work with what's called the log likelihood So The product of these is the likelihood to get the log likelihood We just have to take the log of the probability And so the log of the probability here. I have the log of x from zero to one The log is a you see here monotonic transformation of the probability Where if you pass in one You get zero So probability one gets your log probability of zero And then as you go lower and lower probability The log will grow more and more negative until all the way to negative infinity at zero So here we have a log prob, which is really just a torche dot log of probability Let's print it out to get a sense of what that looks like log prob also 0.4 f Okay So as you can see when we plug in numbers that are very close some of our higher numbers We get closer and closer to zero and then if we plug in very bad probabilities, we get more and more negative number. That's bad so And the reason we work with this is for large extent convenience, right? Because we have mathematically that if you have some product a times b times c of all these probabilities, right? The likelihood is the product of all these probabilities then the log Of these is just log of a plus log of b Plus log of c if you remember your logs from your High school or undergrad and so on so we have that basically The likelihood is the product of probabilities. The log likelihood is just the sum of the logs of the individual probabilities so log likelihood Starts at zero And then log likelihood here we can just accumulate simply And then the end we can print this Print the log likelihood F strings Maybe you're familiar with this So log likelihood is negative 38 Okay now We actually want um So how high can log likelihood get it can go to zero So when all the probabilities are one log likelihood will be zero and then when all the probabilities are lower This will grow more and more negative Now we don't actually like this because what we'd like is a loss function and a loss function has the semantics that low Is good because we're trying to minimize the loss So we actually need to invert this and that's what gives us something called the negative log likelihood um Negative log likelihood is just negative of the log likelihood These are f strings by the way if you'd like to look this up negative log likelihood equals So negative log likelihood now is just negative of it and so the negative log likelihood is a very nice loss function because The lowest it can get is zero And the higher it is the worse off the predictions are that you're making And then one more modification to this that sometimes people do is that for convenience They actually like to normalize by they like to make it an average instead of a sum and so uh here Let's just keep some counts as well So n plus equals one starts at zero and then here We can have sort of like a normalized log likelihood If we just normalize it by the count Then we will sort of get the average log likelihood. So this would be usually our loss function here Is put this we would this is what we would use So our loss function for the training set assigned by the model is 2.4 That's the quality of this model And the lower it is the better off we are and the higher it is the worse off we are And the job of our you know training is to find the parameters that minimize the negative log likelihood loss And that would be like a high quality model. Okay, so to summarize I actually wrote it out here So our goal is to maximize likelihood, which is the product of all the probabilities assigned by the model And we want to maximize this likelihood with respect to the model parameters And in our case the model parameters here are defined in the table these numbers the probabilities are The model parameters is sort of in our brygm language model so far But you have to keep in mind that here we are storing everything in a table format the probabilities But what's coming up as a brief preview is that these numbers will not be kept explicitly But these numbers will be calculated by a neural network So that's coming up And we want to change and tune the parameters of these neural networks We want to change these parameters to maximize the likelihood the product of the probabilities Now maximizing the likelihood is equivalent to maximizing the log likelihood because log is a monotonic function Here's the graph of log And basically all it is doing is it's just scaling your You can look at it as just a scaling of the loss function And so the optimization problem here and here are actually equivalent because this is just scaling you can look at it that way And so these are two identical optimization problems Um maximizing the log likelihood is equivalent to minimizing the negative log likelihood And then in practice people actually minimize the average negative log likelihood to get numbers like 2.4 And then this summarizes the quality of your model and we'd like to minimize it and make it as small as possible And the lowest it can get is zero and the lower it is The better off your model is because it's assigning it's assigning high probabilities to your data Now let's estimate the probability over the entire training set just to make sure that we get something around 2.4 Let's run this over the entire oops Let's take out the print statement as well Okay 2.45 or the entire training set Now what I'd like to show you is that you can actually evaluate the probability for any word that you want like for example If we just test a single word andre and bring back the print statement Then you see that andre is actually kind of like an unlikely word or like on average We take three log probability to represent it and roughly that's because ej apparently is very uncommon as an example Now think through this When I take andre and I append q and I test the probability of it andreq We actually get um infinity And that's because jq has a zero percent probability according to our model. So the log likelihood So the log of zero will be negative infinity. We get infinite loss So this is kind of undesirable right because we plugged in a string that could be like a somewhat reasonable name But basically what this is saying is that this model is exactly zero percent likely to uh to predict this Name and our loss is infinity on this example And really what the reason for that is that j is followed by q zero times Where's q jq is zero and so jq is uh zero percent likely So it's actually kind of gross and people don't like this too much to fix this There's a very simple fix that people like to do to sort of like smooth out your model a little bit It's called model smoothing And roughly what's happening is that we will eight we will add some fake accounts So imagine adding a count of one to everything So we add a count of one like this And then we recalculate the probabilities And that's model smoothing and you can add as much as you like you can add five and that will give you a smoother model and the more you add here The more uniform model you're gonna have and the less you add The more peaked model you are gonna have of course So one is like a pretty decent Count to add and that will ensure that there will be no zeros in our probability matrix p And so this will of course change the generations a little bit in this case. It didn't buy it in principle. It could But what that's going to do now is that nothing will be infinity unlikely So now Our model will predict some other probability and we see that jq now has a very small probability So the model still finds it very surprising that this was a word or a by-gram, but we don't get negative infinity So it's kind of like a nice fix that people like to apply sometimes and it's called model smoothing Okay, so we've now trained a respectable by-gram character level language model and we saw that we both Sort of trained the model by looking at the counts of all the by-grams And normalizing the rows to get probability distributions We saw that we can also then use those parameters of this model to perform sampling of new words So we sample new names according to those distributions And we also saw that we can evaluate the quality of this model And the quality of this model is summarized in a single number Which is the negative log likelihood and the lower this number is the better the model is Because it is giving high probabilities to the actual next characters and all the by-grams in our training set So that's all well and good But we've arrived at this model explicitly by doing something that felt sensible We were just performing counts and then we were normalizing those counts Now what I would like to do is I would like to take an alternative approach We will end up in a very very similar position But the approach will look very different because I would like to cast the problem of by-gram character level language modeling into the neural network framework And in neural network framework, we're going to approach things slightly differently, but again end up in a very similar spot I'll go into that later Now our neural network is going to be a still a by-gram character level language model So it receives a single character as an input Then there's neural network with some weights or some parameters w And it's going to output the probability distribution over the next character in a sequence It's going to make guesses as to what is likely to follow this character that was input to the model And then in addition to that we're going to be able to evaluate any setting of the parameters of the neural net because we have the loss function The negative log likelihood. So we're going to take a look at its probability distributions and we're going to use the labels Which are basically just the identity of the next character in that by-gram the second character So knowing what the second character actually comes next in the by-gram Allows us to then look at what how high of probability the model assigns to that character And then we of course want the probability to be very high And that is another way of saying that the loss is low So we're going to use gradient based optimization then to tune the parameters of this network Because we have the loss function and we're going to minimize it So we're going to tune the weights so that the neural net is correctly predicting the probabilities for the next character So let's get started. The first thing I want to do is I want to compile the training set of this neural network, right? So create the training set of all the by-grams Okay, and Here I'm going to copy paste this code Because this code iterates over all the by-grams So here we start with the words we iterate over all the by-grams and previously as you recall we did the counts But now we're not going to do counts. We're just creating a training set Now this training set will be made up of two lists We have the inputs And the targets the the labels And these by-grams will denote x y those are the characters, right? And so we're given the first character of the by-gram and then we're trying to predict the next one Both of these are going to be integers. So here we'll take x's that append is just x1 y's that append ix2 And then here We actually don't want lists of integers. We will create uh tensors out of these. So x's is torched dot tensor x's and y's is torched dot tensor of y's And then we don't actually want to take all the words just yet because I want everything to be manageable So let's just do the first word which is emma And then it's clear what these x's and y's would be Here let me print Character one character two just so you see what's going on here So the by-grams of these characters is dot e e m m m a dot So this single word as I mentioned has one two three four five examples for our neural network There are five separate examples in emma And those examples are summarized here when the input to the neural neural network is integer zero The desired label is integer five which corresponds to e When the input to the neural network is five, we want its weights to be arranged so that 13 gets a very high probability When 13 is put in we want 13 to have a high probability When 13 is put in we also want one to have a high probability When one is input we want zero to have a very high probability So there are five separate input examples to a neural net in this data set I wanted to add a tangent of a note of caution to be careful with a lot of the apis of some of these frameworks You saw me silently use torch dot tensor with a lowercase t and the output looked right But you should be aware that there's actually two ways of constructing a tensor There's a torch dot lowercase tensor and there's also a torch dot capital tensor class, which you can also construct So you can actually call both you can also do torch dot capital tensor And you get an x as in y as well So that's not confusing at all There are threads on what is the difference between these two and um Unfortunately, the docs are just like not clear on the difference and when you look at the the docs of lowercase tensor construct tensor With no autograd history by copying data It's just like it doesn't It doesn't make sense. So the actual difference as far as I can tell is explained eventually in this random thread that you can google And really it comes down to I believe That um, where is this? Torch dot tensor in first the d type the data type automatically while torch dot tensor just returns a float tensor I would recommend stick to torch dot lowercase tensor so um Indeed we see that when I construct this with a capital t the data type here of x's is float 32 But torch dot lowercase tensor You see how it's now x dot d type is now integer So, um It's advised that you use lowercase t and you can read more about it if you like in some of these threads but basically I'm pointing out some of these things because because I want to caution you and I want you to read get used to reading a lot of documentation and reading through a lot of uh q and a's and threads like this and um You know some of this stuff is unfortunately not easy and not very well documented and you have to be careful out there What we want here is integers because that's what makes sense um and so Lowercase tensor is what we are using. Okay. Now. We want to think through how we're going to feed in these examples into a neural network Now it's not quite as straightforward as Plugging it in because these examples right now are integers. So there's like a 0 5 or 13 It gives us the index of the character and you can't just plug an integer index into a neural net these neural nets, uh, right are sort of made up of these neurons and uh, these neurons have weights And as you saw in micrograd these weights act multiplicatively on the inputs w x plus b There's 10 hs and so on and so it doesn't really make sense to make an input neuron Take on integer values that you feed in and then multiply on with weights So instead a common way of encoding integers is what's called one hot encoding In one hot encoding, uh, we take an integer like 13 and we create a vector that is all zeros Except for the 13th dimension which we turn to a one and then that vector can feed into a neural net Now conveniently, uh, PyTorch actually has something called the one hot Function inside torch and in functional it takes a tensor made up of integers Long is a is a is an integer And it also takes a number of classes, um, which is how large you want your tensor your vector to be So here let's import torch dot and in that functional sf. This is a common way of importing it And then let's do f dot one hot And we feed in the integers that we want to encode So we can actually feed in the entire array of x's And we can tell it that num classes is 27 So it doesn't have to try to guess it it may have guessed that it's only 13 and would give us an incorrect result So this is the one hot. Let's call this x ink for x encoded And then we see that x encoded that shape is 5 by 27 and uh We can also visualize it plt.im show of x ink To make it a little bit more clear because this is a little messy So we see that we've encoded all the five examples Into vectors we have five examples So we have five rows and each row here is now an example into a neural net And we see that the appropriate bit is turned on as a one and everything else is zero so um Here for example, the zero bit is turned on the fifth bit is turned on Thirteenth bits are turned on for both of these examples and the first bit here is turned on So that's how we can encode integers into vectors And then these vectors can feed in to neural nets one more issue to be careful with here by the way is Let's look at the data type of encoding. We always want to be careful with data types What would you expect x encodings data type to be when we're plugging numbers into neural nets? We don't want them to be integers. We want them to be floating point numbers that can take on various values But the d type here is actually 64 bit integer And the reason for that I suspect is that one hot received a 64 bit integer here And it returned the same data type And when you look at the signature of one hot it doesn't even take a d type a desired data type of the output tensor And so we can't in a lot of functions in torch we'd be able to do something like d type equals torch dot float 32 Which is what we want, but one hot does not support that So instead we're going to want to cast this to float like this So that these Everything is the same Everything looks the same but the d type is float 32 and floats can feed into Neural nets. So now let's construct our first neuron This neuron will look at these input vectors And as you remember from micrograd these neurons basically perform a very simple function wx plus b where wx is a dot product right So we can achieve the same thing here. Let's first define the weights of this neuron Basically, what are the initial weights at initialization for this neuron? Let's initialize them with torch dot random torch dot random is Fills a tensor with random numbers drawn from a normal distribution And a normal distribution has A probability density function like this and so most of the numbers drawn from this distribution will be around zero But some of them will be as high as almost three and so on and very few numbers will be above three in magnitude So we need to take a size as an input here And i'm going to use size as to be 27 by one So 27 by one and then let's visualize w. So w is a column vector of 27 numbers And uh, these weights are then multiplied by the inputs So now to perform this multiplication, we can take x encoding and we can multiply it with w This is a matrix multiplication operator in pytorch And the output of this operation is five by one The reason it's five by five is the following We took x encoding, which is five by 27 and we multiplied it by 27 by one And in matrix multiplication You see that the output will become five by one because these 27 will multiply and add So basically what we're seeing here out of this operation Is we are seeing the five um activations of this neuron On these five inputs and we've evaluated all of them in parallel We didn't feed in just a single input to the single neuron We fed in simultaneously all the five inputs into the same neuron And in parallel pytorch has evaluated The wx plus b but here is just wx. There's no bias It has valued w w times x for all of them Uh independently now instead of a single neuron though, I would like to have 27 neurons and I'll show you in the second Why I'm on 27 neurons? So instead of having just a one here, which is indicating this presence of one single neuron We can use 27 And then when w is 27 by 27 This will in parallel evaluate all the 27 neurons on all the five inputs Giving us a much better much much bigger result So now what we've done is five by 27 multiplied 27 by 27 And the output of this is now five by 27 So we can see that the shape of this Is five by 27 So what is every element here telling us right? It's telling us for every one of 27 neurons that we created What is the firing rate of those neurons on every one of those five examples so The element for example 3 comma 13 Is giving us the firing rate of the 13th neuron looking at the third input And the way this was achieved is by a dot product between the third input and the 13th column of this w matrix here Okay, so using matrix multiplication. We can very efficiently evaluate The dot product between lots of input examples in a batch And lots of neurons where all of those neurons have weights in the columns of those w's And in matrix multiplication, we're just doing those dot products and In parallel just to show you that this is the case. We can take x-ank and we can take the third row And we can take the w and take its 13th column And then we can do x-ank at 3 Element wise multiply with w at 13 And sum that up does w x plus b Well, there's no plus b. It's just w x dot product. And that's this number So you see that this is just being done efficiently by the matrix multiplication operation for all the input examples and for all the output neurons of this first layer Okay, so we fed our 27 dimensional inputs into a first layer of a neural net that has 27 neurons, right? So we have 27 inputs and now we have 27 neurons these neurons perform w times x They don't have a bias and they don't have a non-linearity like 10 h We're going to leave them to be a linear layer In addition to that we're not going to have any other layers. This is going to be it. It's just going to be The dumbest smallest simplest neural net, which is just a single linear layer And now I'd like to explain what I want those 27 outputs to be Intuitively what we're trying to produce here for every single input example Is we're trying to produce some kind of a probability distribution for the next character in a sequence And there's 27 of them But we have to come up with like precise semantics for exactly how we're going to interpret These 27 numbers that these neurons take on Now intuitively You see here that these numbers are negative and some of them are positive, etc And that's because these are coming out of a neural net layer initialized with these normal distribution parameters But what we want is we want something like we had here Like each row here told us the counts and then we normalize the counts to get probabilities And we want something similar to come out of a neural net But what we just have right now is just some negative and positive numbers Now we want those numbers to somehow represent the probabilities for the next character But you see that probabilities they they have a special structure. They um They're positive numbers and they sum to 1 and so that doesn't just come out of a neural net And then they can't be counts because these counts are positive And counts are integers. So counts are also not really a good thing to output from a neural net So instead what the neural net is going to output and how we are going to interpret the um The 27 numbers is that these 27 numbers are giving us log counts basically um So instead of giving us counts directly like in this table, they're giving us log counts And to get the counts, we're going to take the log counts and we're going to exponentiate them now exponentiation takes the following form It takes numbers That are negative or they are positive. It takes the entire real line And then if you plug in negative numbers, you're going to get e to the x which is always below 1 So you're getting numbers lower than 1 And if you plug in numbers greater than zero, you're getting numbers greater than one all the way growing to the infinity And this here grows to zero So basically we're going to take these numbers here and Instead of them being positive and negative and all over the place We're going to interpret them as log counts and then we're going to element wise exponentiate these numbers Exponentiating them now gives us something like this And you see that these numbers now because they went through an exponent All the negative numbers turned into numbers below one like 0.338 And all the positive numbers originally turned into even more positive numbers sort of greater than one um, so like for example 7 Is some positive number over here? That is greater than zero But Exponentiated outputs here Basically give us something that we can use and interpret as the equivalent of counts originally So you see these counts here 112 7 51 1 etc The neural net is kind of now predicting Uh counts And these counts are positive numbers. They can never be below zero. So that makes sense And they can now take on various values Depending on the settings of w So let me break this down We're going to interpret these to be the log counts In other words for this that is often used is so called logits These are logits log counts Then these will be sort of the counts Logits exponentiated And this is equivalent to the n matrix sort of the n Array that we used previously. Remember, this was the n This is the the array of counts and each row here are the counts for the For the um next character sort of So those are the counts and now the probabilities are just the counts um normalized and so um I'm not going to find the same but basically I'm not going to scroll all over the place We've already done this. We want two counts that sum Along the first dimension and we want to keep them. It's true We've went over this and this is how we normalized the rows of our counts matrix to get our probabilities Props So now these are the probabilities And these are the counts that we have currently and now when I show the probabilities You see that um every row here, of course Will sum to one Because they're normalized And the shape of this Is five by 27 And so really what we've achieved is for every one of our five examples We now have a row that came out of a neural net And because of the transformations here, we made sure that this output of this neural net now Are probabilities or we can interpret to be probabilities so Our wx here gave us logits and then we interpret those to be log counts We exponentiate to get something that looks like counts And then we normalize those counts to get a probability distribution And all of these are differentiable operations So what we've done now is we are taking inputs We have differentiable operations that we can back propagate through And we're getting out probability distributions So um for example for the zeroth example that fed in Right, which was um The zeroth example here was a one half vector of zero and um It basically corresponded to feeding in Uh This example here So we're feeding in a dot into a neural net and the way we fed the dot into a neural net is that we first got its index Then we one hot encoded it Then it went into the neural net and out came This distribution of probabilities And its shape Is 27 there's 27 numbers and we're going to interpret this as the neural net's assignment for how likely every one of these characters The 27 characters are to come next And as we tune the weights w We're going to be of course getting different probabilities out for any character that you input And so now the question is just can we optimize and find a good w? Such that the probabilities coming out are pretty good And the way we measure pretty good is by the loss function Okay, so I organized everything into a single summary so that hopefully it's a bit more clear So it starts here with an input data set We have some inputs to the neural net and we have some labels for the correct next character in a sequence and these are integers Here i'm using uh torch generators now so that you see the same numbers that I see and i'm generating um 27 neurons weights and each neuron here receives 27 inputs Then here we're going to plug in all the input examples x's into a neural net. So here this is a forward pass First we have to encode all of the inputs into one half representations So we have 27 classes. We pass in these integers and x ink becomes a array that is 5 by 27 zeros except for a few ones We then multiply this in the first layer of a neural net to get logits Exponentiate the logits to get fake counts sort of and normalize these counts to get probabilities So the these last two lines by the way here are called the softmax Uh, which I pulled up here Softmax is a very often used layer in a neural net that takes these z's which are logits Exponentiates them And uh divides and normalizes it's a way of taking outputs of a neural net layer and these These outputs can be positive or negative And it outputs probability distributions. It outputs something that is always sums to one and are positive numbers just like probabilities Um, so this is kind of like a normalization function if you want to think of it that way And you can put it on top of any other linear layer inside a neural net And it basically makes a neural net output probabilities. That's very often used and we used it as well here So this is the forward pass and that's how we made a neural net output probability now you'll notice that um All of these this entire forward pass is made up of differentiable Layers everything here we can back propagate through and we saw some of the back propagation in micrograd This is just Multiplication and addition all that's happening here Just multiply and then add and we know how to back propagate through them Exponentiations we know how to back propagate through And then here we are summing and sum is is easily back propagated well as well And division as well. So everything here is differentiable operation And we can back propagate through Now we achieve these probabilities which are 5 by 27 For every single example, we have a vector of probabilities that sum to 1 And then here I wrote a bunch of stuff to sort of like break down The examples so we have five examples making up emma, right? And there are five bigrams inside emma So by gram example a by gram example one is that e is the beginning character right after dot And the indexes for these are zero and five. So then we feed in a zero That's the input to the neural net we get probabilities from the neural net that are 27 numbers And then the label is five because e actually comes after dot. So that's the label And then We use this label five to index into the probability distribution here So this index five here is zero one two three four five. It's this number here Which is here So that's basically the probability assigned by the neural net to the actual correct character You see that the network currently thinks that this next character that e following dot is only 1% likely Which is of course not very good, right? Because this actually is a training example and the network thinks that this is currently very very unlikely But that's just because we didn't get very lucky in generating a good setting of w So right now this network thinks this is unlikely and 0.01 is not a good outcome So the log likelihood then Is very negative and the negative log likelihood is very positive And so four is a very high negative log likelihood And that means we're going to have a high loss Because what is the loss the loss is just the average negative log likelihood So the second character is em And you see here that also the network thought that m following e is very unlikely 1% The for m following m it thought it was 2% And for a following m it actually thought it was 7% likely So just by chance this one actually has a pretty good probability and therefore a pretty low negative log likelihood And finally here it thought this was 1% likely So overall our average negative log likelihood, which is the loss the total loss that summarizes Basically the how well this network currently works at least on this one word not on the full data So just the one word is 3.76 which is actually very fairly high loss. This is not a very good setting of w's Now here's what we can do We're currently getting 3.76 We can actually come here and we can change our w we can resample it So let me just add one to have a different seed And then we get a different w and then we can rerun this And with this different seed with this different setting of w's we now get 3.37 So this is a much better w right and that and it's better because the probabilities just happen to come out Higher for the for the characters that actually are next And so you can imagine actually just resampling this, you know, we can try as two So Okay, this was not very good. Let's try one more. We can try three Okay, this was terrible setting because we have a very high loss so Anyway, I'm going to erase this What I'm doing here, which is just guess and check of randomly assigning parameters and seeing if the network is good That is a amateur hour. That's not how you optimize a neural net The way you optimize your neural net is you start with some random guess and we're going to commit to this one Even though it's not very good But now the big deal is we have a loss function So this loss Is made up only of differentiable operations And we can minimize the loss by tuning W's by computing the gradients of the loss with respect to these w matrices And so then we can tune w to minimize the loss and find a good setting of w using gradient based optimization So let's see how that will work now things are actually going to look almost identical to what we had with micrograd So here I pulled up the lecture from micrograd the notebook It's from this repository And when I scroll all the way to the end where we left off with micrograd, we had something very very similar We had a number of input examples in this case. We had four input examples inside x's And we had their targets desired targets Just like here we have our x's now, but we have five of them and they're now integers instead of vectors But we're going to convert our integers to vectors except our vectors will be 27 large instead of three large And then here what we did is first we did a forward pass where we ran a neural net on all of the inputs to get predictions Our neural net at the time this n of x was a multilayer perceptron Our neural net is going to look different because our neural net is just a single layer Single linear layer followed by a softmax So that's our neural net And the loss here was the mean squared error So we simply subtracted the prediction from the ground truth and squared it and summed it all up And that was the loss and loss was the single number that summarized the quality of the neural net And when loss is low like almost zero that means the neural net is um predicting correctly So we had a single number that uh that summarized the uh the performance of the neural net And everything here was differentiable and was stored in massive compute graph And then we iterated over all the parameters We made sure that the gradients are set to zero and we called loss.backward And loss.backward initiated back propagation at the final output node of loss, right? So Yeah, I remember these expressions. We had loss all the way at the end We start back propagation and we went all the way back And we made sure that we populated all the parameters dot grad So dot grad started at zero but back propagation filled it in And then in the update we iterated over all the parameters and we simply did a parameter update where every single element of our parameters was nudged in the opposite direction of the gradient And so we're going to do the exact same thing here So i'm going to pull this up On the side here So that we have it available and we're actually going to do the exact same thing So this was the forward pass. So we're we did this And probes is our y-pred So now we have to evaluate the loss, but we're not using the mean squared error We're using the negative log likelihood because we are doing classification. We're not doing regression as it's called So here we want to calculate loss Now the way we calculate it is is just this average negative log likelihood Now this probes here Has a shape of five by 27 And so to get all the we basically want to pluck out the probabilities at the correct indices here So in particular because the labels are stored here in the array y's Basically what we're after is for the first example, we're looking at probability of five right at index five for the second example at the the second row or row index one We are interested in the probability assigned to index 13 At the second example, we also have 13 At the third row, we want one And at the last row, which is four we want zero So these are the probabilities we're interested in right and you can see that they're not amazing as we saw above So these are the probabilities we want but we want like a more efficient way to access these probabilities Um, not just listing them out in a tuple like this So it turns out that the way to do this in pi torch one of the ways at least is we can basically pass in all of these Sorry about that all of these um integers in vectors So the these ones you see how they're just zero one two three four We can actually create that using mp not mp. Sorry torch dot a range of five zero one two three four So we can index here with torch dot a range of five And here we index with y's And you see that that gives us exactly these numbers So that plugs out the probabilities of that the neural network assigns to the correct next character Now we take those probabilities and we don't we actually look at the log probability So we want to dot log And then we want to just average that up So take the mean of all of that and then it's the negative average log likelihood. That is the loss So the loss here is 3.7 something and you see that this loss 3.76 3.76 is exactly as we've obtained before but this is a vectorized form of that expression So we get the same loss And the same loss we can consider sort of as part of this forward pass And we've achieved here now loss Okay, so we made our way all the way to loss. We've defined the forward pass We forwarded the network and the loss now. We're ready to do the backward pass. So backward pass We want to first make sure that all the gradients are reset. So they're at zero Now in pi torch, you can set the gradients to be zero But you can also just set it to none and setting it to none is more efficient And pi torch will interpret none as like a lack of a gradient and it's the same as zeros So this is a way to set to zero the gradient And now we do lost up backward Before we do lost up backward, we need one more thing if you remember from micrograd pi torch actually requires That we pass in requires grad is true Uh, so that we tell Pi torch that we are interested in calculating gradients for this leaf tensor by default. This is false So let me recalculate with that And then set to none and lost up backward Now something magical happened when lost up backward was run Because pi torch just like micrograd when we did the forward pass here It keeps track of all the operations under the hood. It builds a full computational graph Just like the graphs we've produced in micrograd those graphs exist inside pi torch And so it knows all the dependencies and all the mathematical operations of everything And when you then calculate the loss we can call a dot backward on it And dot backward then fills in the gradients of All the intermediates all the way back to w's Which are the parameters of our neural net. So now we can do w dot grad And we see that it has structure there's stuff inside it And these gradients every single element here So w dot shape is 27 by 27 W grads shape is the same 27 by 27 And every element of w dot grad is telling us The influence of that weight on the loss function So for example this number all the way here If this element the zero zero element of w Because the gradient is positive it's telling us that this has a positive influence on the loss slightly nudging w slightly taking w zero zero And adding a small h to it Would increase the loss mildly because this gradient is positive Some of these gradients are also negative So that's telling us about the gradient information and we can use this gradient information To update the weights of this neural network. So let's not do the update It's going to be very similar to what we had in micrograd We need no loop over all the parameters because we only have one parameter Tensor and that is w. So we simply do w dot data plus equals The we can actually copy this almost exactly negative zero point one times w dot grad um And that would be the update to the tensor So that updates the tensor And because the tensor is updated we would expect that now the loss should decrease So here if I print loss That item It was 3.76 right So we've updated the w here. So if I recalculate forward pass Loss now should be slightly lower. So 3.76 goes to 3.74 And then we can again set to set grad to none and backward update And now the parameters changed again So if we recalculate the forward pass, we expect a lower loss again 3.72 Okay, and this is again doing the we're now doing reading the set And when we achieve a low loss that will mean that the network is assigning high probabilities to the correct next characters Okay, so I rearranged everything and I put it all together from scratch So here is where we construct our data set of bigrams You see that we are still iterating only over the first word emma I'm going to change that in a second I added a number that counts the number of elements in axis so that we explicitly see that number of examples is five Because currently we're just working with emma. There's five bigrams there And here I added a loop of exactly what we had before So we had 10 iterations of gradient descent of forward pass backward pass and an update And so running these two cells initialization and gradient descent Gives us some improvement on the last function But now I want to use all the words And there's not five but 228,000 bigrams now However, this should require no modification whatsoever. Everything should just run Because all the code we wrote doesn't care if there's five bigrams or 228,000 bigrams and with everything we should just work So You see that this will just run But now we are optimizing over the entire training set of all the bigrams And you see now that we are decreasing very slightly. So actually we can probably afford the larger learning rate Can probably afford even larger learning rate Even 50 seems to work on this very very simple example, right? So let me re-initialize and let's run 100 iterations See what happens Okay We seem to be Coming up to some pretty good losses here 2.47 Let me run 100 more What is the number that we expect by the way in the loss? We expect to get something around what we had originally actually So all the way back if you remember in the beginning of this video when we optimized Just by counting Our loss was roughly 2.47 After we added smoothing But before smoothing we had roughly 2.45 likely it Sorry loss And so that's actually roughly the vicinity of what we expect to achieve But before we achieved it by counting and here we are achieving the roughly the same result But with gradient based optimization So we come to about 2.46 2.45, etc And that makes sense because fundamentally we're not taking any additional information We're still just taking in the previous character and trying to predict the next one But instead of doing it explicitly by counting and normalizing We are doing it with gradient based learning and it just so happens that the explicit approach Happens to very well optimize the loss function without any need for gradient based optimization Because the setup for bi-gram language models are is so straightforward. It's so simple We can just afford to estimate those probabilities directly and maintain them in a table But the gradient based approach is significantly more flexible so we've actually gained a lot because What we can do now is We can expand this approach and complexify the neural net So currently we're just taking a single character and feeding into a neural net and the neural is extremely simple But we're about to iterate on this substantially We're going to be taking multiple previous characters and we're going to be feeding them into increasingly more complex neural nets But fundamentally out the output of the neural net will always just be logits And those logits will go through the exact same transformation We're going to take them through a softmax Calculate the loss function and the negative log likelihood and do gradient based optimization And so actually as we complexify the neural nets and work all the way up to transformers None of this will really fundamentally change None of this will fundamentally change the only thing that will change is The way we do the forward pass or we've taken some previous characters and calculate logits for the next character in a sequence that will become more complex And that will use the same machinery to optimize it and It's not obvious how we would have extended This bygram approach Into the case where there are many more Characters at the input because eventually these tables would get way too large because there's way too many combinations Of what previous characters could be If you only have one previous character we can just keep everything in a table the counts But if you have the last 10 characters that are in but we can't actually keep everything in the table anymore So this is fundamentally an unscalable approach and the neural network approach is significantly more scalable And it's something that actually we can improve on over time. So that's where we will be digging next I wanted to point out two more things number one I want you to notice that this x-ank here This is made up of one-hot vectors and then those one-hot vectors are multiplied by this w matrix And we think of this as a multiple neurons being forwarded in a fully connected manner But actually what's happening here is that for example If you have a one-hot vector here that has a one at say the fifth dimension Then because of the way the matrix multiplication works Multiplying that one-hot vector with w actually ends up plucking out the fifth row of w Lot logits would become just the fifth row of w And that's because of the way the matrix multiplication works So that's actually what ends up happening So but that's actually exactly what happened before because remember all the way up here We have a bi-gram We took the first character and then that first character indexed into a row of this array here And that row gave us the probability distribution for the next character So the first character was used as a lookup into a Matrix here to get the probability distribution Well, that's actually exactly what's happening here because we're taking the index We're encoding it as one-hot and multiplying it by w So logits literally becomes the The appropriate row of w and that gets just as before Exponentiated to create the counts and then normalized and becomes probability So this w here is literally The same as this array here But w remember is the log counts not the counts. So it's more precise to say that w Exponentiated w.exp is this array But this array was filled in by counting and by basically Populating the counts of bi-grams whereas in the gradient based framework We initialize it randomly and then we let the loss Guide us to arrive at the exact same array So this array exactly here is Basically the array w at the end of optimization except we arrived at it piece by piece by following the loss And that's why we also obtained the same loss function at the end and the second note is if I come here remember the smoothing where we added fake counts to our counts in order to Smooth out and make more uniform the distributions of these probabilities And that prevented us from assigning zero probability to Um to any one bi-gram Now if I increase the count here What's happening to the probability? As I increase the count probability becomes more and more uniform Right because these counts go only up to like 900 or whatever So if I'm adding plus a million to every single number here You can see how uh the row and its probability then when we divide Is just going to become more and more close to exactly even probability uniform distribution It turns out that the gradient based framework has an equivalent to smoothing In particular Think through these w's here Which we initialized randomly We could also think about initializing w's to be zero If all the entries of w are zero Then you'll see that logits will become all zero And then exponentiating those logits becomes all one And then the probabilities turn out to be exactly uniform So basically when w's are all equal to each other or say especially zero Then the probabilities come out completely uniform So Trying to incentivize w to be near zero Is basically equivalent To label smoothing and the more you incentivize that in a loss function the more smooth distribution you're going to achieve So this brings us to something that's called regularization Where we can actually augment the loss function to have a small component that we call a regularization loss In particular what we're going to do is we can take w and we can for example square all of its entries And then we can um oops Sorry about that We can take all the entries of w and we can sum them And because we're squaring uh, there will be no signs anymore Um negatives and positives all get squashed to be positive numbers And then the way this works is you achieve zero loss if w is exactly or zero But if w has non-zero numbers you accumulate loss And so we can actually take this and we can add it on here So we can do something like loss plus w square dot sum Or let's actually instead of some let's take a mean because otherwise the sum gets too large So mean is like a little bit more manageable And then we have a regularization loss here like say 0.01 times or something like that you can choose the regularization strength And then we can just optimize this And now this optimization actually has two components not only is it trying to make all the probabilities work out But in addition to that there's an additional component that simultaneously tries to make all w's be zero Because if w's are non-zero you feel a loss and so minimizing this the only way to achieve that is for w to be zero And so you can think of this as adding like a spring force or like a gravity force that that pushes w to be zero So w wants to be zero and the probabilities want to be uniform But they also simultaneously want to match up your your probabilities as indicated by the data And so the strength of this regularization is exactly controlling the amount of counts that you add here Adding a lot more counts here corresponds to Increasing this number Because the more you increase it the more this part of the loss function dominates this part And the more these these weights will be unable to grow because as they grow They accumulate way too much loss And so if this is strong enough Then we are not able to overcome the force of this loss and we will never And basically everything will be uniform predictions So I thought that's kind of cool. Okay, and lastly before we wrap up I wanted to show you how you would sample from this neural net model And I copy pasted the sampling code from before Where remember that we sampled five times And all we did is we started zero we grabbed the current ix row of p And that was our probability row From which we sampled the next index and just accumulated that and break when zero And running this gave us these results I still have the p in memory. So this is fine now The speed doesn't come from the row of p instead. It comes from this neural net First we take ix And we encode it into a one-hot row of xank This xank multiplies our w Which really just plugs out the row of w corresponding to ix really that's what's happening And that gets our logits and then we normalize those logits Exponentiate to get counts and then normalize to get the distribution and then we can sample from the distribution So if I run this Kind of anti-climatic or climatic depending how you look at it, but we get the exact same result Um And that's because this is in the identical model. Not only does it achieve the same loss but As I mentioned, these are identical models and this w is the log counts of what we've estimated before But we came to this answer in a very different way and it's got a very different interpretation But fundamentally, this is basically the same model and gives the same samples here. And so That's kind of cool. Okay, so we've actually covered a lot of ground We introduced the bi-gram character level language model We saw how we can train the model how we can sample from the model and how we can evaluate the quality of the model Using the negative log likelihood loss And then we actually trained the model in two completely different ways that actually get the same result and the same model In the first way, we just counted up the frequency of all the bi-grams and normalized In the second way, we used the uh negative log likelihood loss as a guide To optimizing the counts matrix Or the counts array so that the loss is minimized in the in a gradient based framework and we saw that both of them give the same result and That's it Now the second one of these the gradient based framework is much more flexible And right now our neural network is super simple We're taking a single previous character and we're taking it through a single linear layer to calculate the logits This is about to complexify. So in the follow-up videos, we're going to be taking more and more of these characters And we're going to be feeding them into a neural net But this neural net will still output the exact same thing. The neural net will output logits And these logits will still be normalized in the exact same way and all the loss and everything else and the gradient Gradient based framework everything stays identical. It's just that this neural net will now Complexify all the way to transformers So that's going to be pretty awesome and i'm looking forward to it for now. Bye", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.36, "text": " Hi everyone, hope you're well and next up what I'd like to do is I'd like to build out make more", "tokens": [50364, 2421, 1518, 11, 1454, 291, 434, 731, 293, 958, 493, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 1322, 484, 652, 544, 50632], "temperature": 0.0, "avg_logprob": -0.19242363288754322, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.06841865926980972}, {"id": 1, "seek": 0, "start": 5.88, "end": 10.58, "text": " Like micrograd before it make more is a repository that I have on my github web page", "tokens": [50658, 1743, 4532, 7165, 949, 309, 652, 544, 307, 257, 25841, 300, 286, 362, 322, 452, 290, 355, 836, 3670, 3028, 50893], "temperature": 0.0, "avg_logprob": -0.19242363288754322, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.06841865926980972}, {"id": 2, "seek": 0, "start": 11.040000000000001, "end": 14.0, "text": " You can look at it, but just like with micrograd", "tokens": [50916, 509, 393, 574, 412, 309, 11, 457, 445, 411, 365, 4532, 7165, 51064], "temperature": 0.0, "avg_logprob": -0.19242363288754322, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.06841865926980972}, {"id": 3, "seek": 0, "start": 14.0, "end": 17.64, "text": " I'm going to build it out step by step and I'm going to spell everything out", "tokens": [51064, 286, 478, 516, 281, 1322, 309, 484, 1823, 538, 1823, 293, 286, 478, 516, 281, 9827, 1203, 484, 51246], "temperature": 0.0, "avg_logprob": -0.19242363288754322, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.06841865926980972}, {"id": 4, "seek": 0, "start": 17.64, "end": 19.64, "text": " So we're going to build it out slowly and together", "tokens": [51246, 407, 321, 434, 516, 281, 1322, 309, 484, 5692, 293, 1214, 51346], "temperature": 0.0, "avg_logprob": -0.19242363288754322, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.06841865926980972}, {"id": 5, "seek": 0, "start": 19.98, "end": 21.98, "text": " Now what is make more?", "tokens": [51363, 823, 437, 307, 652, 544, 30, 51463], "temperature": 0.0, "avg_logprob": -0.19242363288754322, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.06841865926980972}, {"id": 6, "seek": 0, "start": 22.02, "end": 27.02, "text": " Make more as the name suggests makes more of things that you give it", "tokens": [51465, 4387, 544, 382, 264, 1315, 13409, 1669, 544, 295, 721, 300, 291, 976, 309, 51715], "temperature": 0.0, "avg_logprob": -0.19242363288754322, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.06841865926980972}, {"id": 7, "seek": 2702, "start": 27.38, "end": 32.08, "text": " So here's an example names.txt is an example data set to make more and", "tokens": [50382, 407, 510, 311, 364, 1365, 5288, 13, 83, 734, 307, 364, 1365, 1412, 992, 281, 652, 544, 293, 50617], "temperature": 0.0, "avg_logprob": -0.2270905994233631, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.010163072496652603}, {"id": 8, "seek": 2702, "start": 32.6, "end": 37.1, "text": " When you look at names.txt you'll find that it's a very large data set of names", "tokens": [50643, 1133, 291, 574, 412, 5288, 13, 83, 734, 291, 603, 915, 300, 309, 311, 257, 588, 2416, 1412, 992, 295, 5288, 50868], "temperature": 0.0, "avg_logprob": -0.2270905994233631, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.010163072496652603}, {"id": 9, "seek": 2702, "start": 37.94, "end": 39.94, "text": " so", "tokens": [50910, 370, 51010], "temperature": 0.0, "avg_logprob": -0.2270905994233631, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.010163072496652603}, {"id": 10, "seek": 2702, "start": 40.06, "end": 41.66, "text": " Here's lots of different types of names", "tokens": [51016, 1692, 311, 3195, 295, 819, 3467, 295, 5288, 51096], "temperature": 0.0, "avg_logprob": -0.2270905994233631, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.010163072496652603}, {"id": 11, "seek": 2702, "start": 41.66, "end": 42.16, "text": " in fact", "tokens": [51096, 294, 1186, 51121], "temperature": 0.0, "avg_logprob": -0.2270905994233631, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.010163072496652603}, {"id": 12, "seek": 2702, "start": 42.16, "end": 47.2, "text": " I believe there are 32,000 names that I've sort of found randomly on the government website and", "tokens": [51121, 286, 1697, 456, 366, 8858, 11, 1360, 5288, 300, 286, 600, 1333, 295, 1352, 16979, 322, 264, 2463, 3144, 293, 51373], "temperature": 0.0, "avg_logprob": -0.2270905994233631, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.010163072496652603}, {"id": 13, "seek": 2702, "start": 48.019999999999996, "end": 53.72, "text": " If you train make more on this data set it will learn to make more of things like this", "tokens": [51414, 759, 291, 3847, 652, 544, 322, 341, 1412, 992, 309, 486, 1466, 281, 652, 544, 295, 721, 411, 341, 51699], "temperature": 0.0, "avg_logprob": -0.2270905994233631, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.010163072496652603}, {"id": 14, "seek": 5372, "start": 54.6, "end": 61.68, "text": " And in particular in this case that will mean more things that sound name like but are actually unique names", "tokens": [50408, 400, 294, 1729, 294, 341, 1389, 300, 486, 914, 544, 721, 300, 1626, 1315, 411, 457, 366, 767, 3845, 5288, 50762], "temperature": 0.0, "avg_logprob": -0.20903224133430642, "compression_ratio": 1.751054852320675, "no_speech_prob": 0.004467869643121958}, {"id": 15, "seek": 5372, "start": 61.68, "end": 64.84, "text": " And maybe if you have a baby and you're trying to assign name", "tokens": [50762, 400, 1310, 498, 291, 362, 257, 3186, 293, 291, 434, 1382, 281, 6269, 1315, 50920], "temperature": 0.0, "avg_logprob": -0.20903224133430642, "compression_ratio": 1.751054852320675, "no_speech_prob": 0.004467869643121958}, {"id": 16, "seek": 5372, "start": 64.84, "end": 68.76, "text": " Maybe you're looking for a cool new sounding unique name make more might help you", "tokens": [50920, 2704, 291, 434, 1237, 337, 257, 1627, 777, 24931, 3845, 1315, 652, 544, 1062, 854, 291, 51116], "temperature": 0.0, "avg_logprob": -0.20903224133430642, "compression_ratio": 1.751054852320675, "no_speech_prob": 0.004467869643121958}, {"id": 17, "seek": 5372, "start": 69.36, "end": 75.25999999999999, "text": " So here are some example generations from the neural network once we train it on our data set", "tokens": [51146, 407, 510, 366, 512, 1365, 10593, 490, 264, 18161, 3209, 1564, 321, 3847, 309, 322, 527, 1412, 992, 51441], "temperature": 0.0, "avg_logprob": -0.20903224133430642, "compression_ratio": 1.751054852320675, "no_speech_prob": 0.004467869643121958}, {"id": 18, "seek": 5372, "start": 76.03999999999999, "end": 80.28, "text": " So here's some example unique names that it will generate don't tell", "tokens": [51480, 407, 510, 311, 512, 1365, 3845, 5288, 300, 309, 486, 8460, 500, 380, 980, 51692], "temperature": 0.0, "avg_logprob": -0.20903224133430642, "compression_ratio": 1.751054852320675, "no_speech_prob": 0.004467869643121958}, {"id": 19, "seek": 8028, "start": 81.28, "end": 83.28, "text": " I rot", "tokens": [50414, 286, 4297, 50514], "temperature": 0.0, "avg_logprob": -0.2574056243896484, "compression_ratio": 1.649214659685864, "no_speech_prob": 0.003648685524240136}, {"id": 20, "seek": 8028, "start": 83.44, "end": 89.16, "text": " Zendy and so on and so all these are sound name like but they're not of course names", "tokens": [50522, 1176, 18642, 293, 370, 322, 293, 370, 439, 613, 366, 1626, 1315, 411, 457, 436, 434, 406, 295, 1164, 5288, 50808], "temperature": 0.0, "avg_logprob": -0.2574056243896484, "compression_ratio": 1.649214659685864, "no_speech_prob": 0.003648685524240136}, {"id": 21, "seek": 8028, "start": 90.44, "end": 94.28, "text": " So under the hood make more is a character level language model", "tokens": [50872, 407, 833, 264, 13376, 652, 544, 307, 257, 2517, 1496, 2856, 2316, 51064], "temperature": 0.0, "avg_logprob": -0.2574056243896484, "compression_ratio": 1.649214659685864, "no_speech_prob": 0.003648685524240136}, {"id": 22, "seek": 8028, "start": 94.52, "end": 100.92, "text": " So what that means is that is treating every single line here as an example and within each example", "tokens": [51076, 407, 437, 300, 1355, 307, 300, 307, 15083, 633, 2167, 1622, 510, 382, 364, 1365, 293, 1951, 1184, 1365, 51396], "temperature": 0.0, "avg_logprob": -0.2574056243896484, "compression_ratio": 1.649214659685864, "no_speech_prob": 0.003648685524240136}, {"id": 23, "seek": 8028, "start": 100.92, "end": 104.6, "text": " It's treating them all as sequences of individual characters", "tokens": [51396, 467, 311, 15083, 552, 439, 382, 22978, 295, 2609, 4342, 51580], "temperature": 0.0, "avg_logprob": -0.2574056243896484, "compression_ratio": 1.649214659685864, "no_speech_prob": 0.003648685524240136}, {"id": 24, "seek": 10460, "start": 104.8, "end": 110.36, "text": " So R E E S E is this example and that's the sequence of characters", "tokens": [50374, 407, 497, 462, 462, 318, 462, 307, 341, 1365, 293, 300, 311, 264, 8310, 295, 4342, 50652], "temperature": 0.0, "avg_logprob": -0.1884568065678308, "compression_ratio": 1.9724409448818898, "no_speech_prob": 0.01665288396179676}, {"id": 25, "seek": 10460, "start": 110.36, "end": 116.6, "text": " And that's the level on which we are building out make more and what it means to be a character level language model", "tokens": [50652, 400, 300, 311, 264, 1496, 322, 597, 321, 366, 2390, 484, 652, 544, 293, 437, 309, 1355, 281, 312, 257, 2517, 1496, 2856, 2316, 50964], "temperature": 0.0, "avg_logprob": -0.1884568065678308, "compression_ratio": 1.9724409448818898, "no_speech_prob": 0.01665288396179676}, {"id": 26, "seek": 10460, "start": 116.6, "end": 122.6, "text": " Then is that it's just sort of modeling those sequences of characters and it knows how to predict the next character in the sequence", "tokens": [50964, 1396, 307, 300, 309, 311, 445, 1333, 295, 15983, 729, 22978, 295, 4342, 293, 309, 3255, 577, 281, 6069, 264, 958, 2517, 294, 264, 8310, 51264], "temperature": 0.0, "avg_logprob": -0.1884568065678308, "compression_ratio": 1.9724409448818898, "no_speech_prob": 0.01665288396179676}, {"id": 27, "seek": 10460, "start": 123.36, "end": 126.19999999999999, "text": " Now we're actually going to implement a large number of", "tokens": [51302, 823, 321, 434, 767, 516, 281, 4445, 257, 2416, 1230, 295, 51444], "temperature": 0.0, "avg_logprob": -0.1884568065678308, "compression_ratio": 1.9724409448818898, "no_speech_prob": 0.01665288396179676}, {"id": 28, "seek": 10460, "start": 126.52, "end": 131.88, "text": " Character level language models in terms of the neural networks that are involved in predicting the next character in a sequence", "tokens": [51460, 36786, 1496, 2856, 5245, 294, 2115, 295, 264, 18161, 9590, 300, 366, 3288, 294, 32884, 264, 958, 2517, 294, 257, 8310, 51728], "temperature": 0.0, "avg_logprob": -0.1884568065678308, "compression_ratio": 1.9724409448818898, "no_speech_prob": 0.01665288396179676}, {"id": 29, "seek": 13188, "start": 132.04, "end": 135.0, "text": " So very simple by Graham and back of word models", "tokens": [50372, 407, 588, 2199, 538, 22691, 293, 646, 295, 1349, 5245, 50520], "temperature": 0.0, "avg_logprob": -0.2328388178459952, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.010011238045990467}, {"id": 30, "seek": 13188, "start": 135.35999999999999, "end": 139.46, "text": " Multilevel perceptrons recurrent neural networks all the way to modern", "tokens": [50538, 14665, 794, 779, 43276, 13270, 18680, 1753, 18161, 9590, 439, 264, 636, 281, 4363, 50743], "temperature": 0.0, "avg_logprob": -0.2328388178459952, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.010011238045990467}, {"id": 31, "seek": 13188, "start": 139.79999999999998, "end": 147.4, "text": " Transformers in fact a transformer that we will build will be basically the equivalent transformer to GPT2 if you have heard of GPT", "tokens": [50760, 27938, 433, 294, 1186, 257, 31782, 300, 321, 486, 1322, 486, 312, 1936, 264, 10344, 31782, 281, 26039, 51, 17, 498, 291, 362, 2198, 295, 26039, 51, 51140], "temperature": 0.0, "avg_logprob": -0.2328388178459952, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.010011238045990467}, {"id": 32, "seek": 13188, "start": 147.84, "end": 149.12, "text": " So that's kind of a big deal", "tokens": [51162, 407, 300, 311, 733, 295, 257, 955, 2028, 51226], "temperature": 0.0, "avg_logprob": -0.2328388178459952, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.010011238045990467}, {"id": 33, "seek": 13188, "start": 149.12, "end": 153.57999999999998, "text": " It's a modern network and by the end of the series you will actually understand how that works", "tokens": [51226, 467, 311, 257, 4363, 3209, 293, 538, 264, 917, 295, 264, 2638, 291, 486, 767, 1223, 577, 300, 1985, 51449], "temperature": 0.0, "avg_logprob": -0.2328388178459952, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.010011238045990467}, {"id": 34, "seek": 13188, "start": 154.07999999999998, "end": 159.0, "text": " On the level of characters now to give you a sense of the extensions here", "tokens": [51474, 1282, 264, 1496, 295, 4342, 586, 281, 976, 291, 257, 2020, 295, 264, 25129, 510, 51720], "temperature": 0.0, "avg_logprob": -0.2328388178459952, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.010011238045990467}, {"id": 35, "seek": 15900, "start": 159.48, "end": 166.76, "text": " After characters we will probably spend some time on the word level so that we can generate documents of words not just little you know segments of characters", "tokens": [50388, 2381, 4342, 321, 486, 1391, 3496, 512, 565, 322, 264, 1349, 1496, 370, 300, 321, 393, 8460, 8512, 295, 2283, 406, 445, 707, 291, 458, 19904, 295, 4342, 50752], "temperature": 0.0, "avg_logprob": -0.21697933268996905, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.020014043897390366}, {"id": 36, "seek": 15900, "start": 167.32, "end": 170.36, "text": " But we can generate entire large much larger documents", "tokens": [50780, 583, 321, 393, 8460, 2302, 2416, 709, 4833, 8512, 50932], "temperature": 0.0, "avg_logprob": -0.21697933268996905, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.020014043897390366}, {"id": 37, "seek": 15900, "start": 170.36, "end": 174.04, "text": " And then we're probably going to go into images and image text", "tokens": [50932, 400, 550, 321, 434, 1391, 516, 281, 352, 666, 5267, 293, 3256, 2487, 51116], "temperature": 0.0, "avg_logprob": -0.21697933268996905, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.020014043897390366}, {"id": 38, "seek": 15900, "start": 174.64, "end": 179.24, "text": " Networks such as Dali stable diffusion and so on but for now we have to start", "tokens": [51146, 12640, 82, 1270, 382, 413, 5103, 8351, 25242, 293, 370, 322, 457, 337, 586, 321, 362, 281, 722, 51376], "temperature": 0.0, "avg_logprob": -0.21697933268996905, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.020014043897390366}, {"id": 39, "seek": 15900, "start": 179.8, "end": 182.68, "text": " Here character level language modeling. Let's go", "tokens": [51404, 1692, 2517, 1496, 2856, 15983, 13, 961, 311, 352, 51548], "temperature": 0.0, "avg_logprob": -0.21697933268996905, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.020014043897390366}, {"id": 40, "seek": 15900, "start": 183.2, "end": 186.6, "text": " So like before we are starting with a completely blank GPTN notebook page", "tokens": [51574, 407, 411, 949, 321, 366, 2891, 365, 257, 2584, 8247, 26039, 51, 45, 21060, 3028, 51744], "temperature": 0.0, "avg_logprob": -0.21697933268996905, "compression_ratio": 1.6914893617021276, "no_speech_prob": 0.020014043897390366}, {"id": 41, "seek": 18660, "start": 186.76, "end": 190.84, "text": " The first thing is I would like to basically load up the data set names.txt", "tokens": [50372, 440, 700, 551, 307, 286, 576, 411, 281, 1936, 3677, 493, 264, 1412, 992, 5288, 13, 83, 734, 50576], "temperature": 0.0, "avg_logprob": -0.21836895602090017, "compression_ratio": 1.8065843621399178, "no_speech_prob": 0.023681538179516792}, {"id": 42, "seek": 18660, "start": 191.51999999999998, "end": 194.48, "text": " So we're going to open up names.txt for reading and", "tokens": [50610, 407, 321, 434, 516, 281, 1269, 493, 5288, 13, 83, 734, 337, 3760, 293, 50758], "temperature": 0.0, "avg_logprob": -0.21836895602090017, "compression_ratio": 1.8065843621399178, "no_speech_prob": 0.023681538179516792}, {"id": 43, "seek": 18660, "start": 195.4, "end": 198.76, "text": " We're going to read in everything into a massive string and", "tokens": [50804, 492, 434, 516, 281, 1401, 294, 1203, 666, 257, 5994, 6798, 293, 50972], "temperature": 0.0, "avg_logprob": -0.21836895602090017, "compression_ratio": 1.8065843621399178, "no_speech_prob": 0.023681538179516792}, {"id": 44, "seek": 18660, "start": 199.68, "end": 203.88, "text": " Then because it's a massive string we'd only like the individual words and put them in the list", "tokens": [51018, 1396, 570, 309, 311, 257, 5994, 6798, 321, 1116, 787, 411, 264, 2609, 2283, 293, 829, 552, 294, 264, 1329, 51228], "temperature": 0.0, "avg_logprob": -0.21836895602090017, "compression_ratio": 1.8065843621399178, "no_speech_prob": 0.023681538179516792}, {"id": 45, "seek": 18660, "start": 204.24, "end": 210.84, "text": " So let's call split lines on that string to get all of our words as a Python list of strings", "tokens": [51246, 407, 718, 311, 818, 7472, 3876, 322, 300, 6798, 281, 483, 439, 295, 527, 2283, 382, 257, 15329, 1329, 295, 13985, 51576], "temperature": 0.0, "avg_logprob": -0.21836895602090017, "compression_ratio": 1.8065843621399178, "no_speech_prob": 0.023681538179516792}, {"id": 46, "seek": 18660, "start": 211.79999999999998, "end": 215.07999999999998, "text": " so basically we can look at for example the first 10 words and", "tokens": [51624, 370, 1936, 321, 393, 574, 412, 337, 1365, 264, 700, 1266, 2283, 293, 51788], "temperature": 0.0, "avg_logprob": -0.21836895602090017, "compression_ratio": 1.8065843621399178, "no_speech_prob": 0.023681538179516792}, {"id": 47, "seek": 21508, "start": 215.8, "end": 222.28, "text": " We have that it's a list of Emma Olivia Eva and so on and if we look at", "tokens": [50400, 492, 362, 300, 309, 311, 257, 1329, 295, 17124, 26023, 29377, 293, 370, 322, 293, 498, 321, 574, 412, 50724], "temperature": 0.0, "avg_logprob": -0.14601653881287308, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.002115496201440692}, {"id": 48, "seek": 21508, "start": 223.72000000000003, "end": 226.20000000000002, "text": " The top of the page here that is indeed what we see", "tokens": [50796, 440, 1192, 295, 264, 3028, 510, 300, 307, 6451, 437, 321, 536, 50920], "temperature": 0.0, "avg_logprob": -0.14601653881287308, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.002115496201440692}, {"id": 49, "seek": 21508, "start": 227.08, "end": 228.20000000000002, "text": " um", "tokens": [50964, 1105, 51020], "temperature": 0.0, "avg_logprob": -0.14601653881287308, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.002115496201440692}, {"id": 50, "seek": 21508, "start": 228.20000000000002, "end": 229.64000000000001, "text": " So that's good", "tokens": [51020, 407, 300, 311, 665, 51092], "temperature": 0.0, "avg_logprob": -0.14601653881287308, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.002115496201440692}, {"id": 51, "seek": 21508, "start": 229.64000000000001, "end": 231.64000000000001, "text": " This list actually makes me feel that", "tokens": [51092, 639, 1329, 767, 1669, 385, 841, 300, 51192], "temperature": 0.0, "avg_logprob": -0.14601653881287308, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.002115496201440692}, {"id": 52, "seek": 21508, "start": 232.28, "end": 234.28, "text": " This is probably sorted by frequency", "tokens": [51224, 639, 307, 1391, 25462, 538, 7893, 51324], "temperature": 0.0, "avg_logprob": -0.14601653881287308, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.002115496201440692}, {"id": 53, "seek": 21508, "start": 235.64000000000001, "end": 241.56, "text": " But okay, so these are the words now we'd like to actually like learn a little bit more about this data set", "tokens": [51392, 583, 1392, 11, 370, 613, 366, 264, 2283, 586, 321, 1116, 411, 281, 767, 411, 1466, 257, 707, 857, 544, 466, 341, 1412, 992, 51688], "temperature": 0.0, "avg_logprob": -0.14601653881287308, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.002115496201440692}, {"id": 54, "seek": 24156, "start": 241.8, "end": 245.4, "text": " Let's look at the total number of words. We expect this to be roughly 32 000", "tokens": [50376, 961, 311, 574, 412, 264, 3217, 1230, 295, 2283, 13, 492, 2066, 341, 281, 312, 9810, 8858, 13711, 50556], "temperature": 0.0, "avg_logprob": -0.1794400413831075, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.007936383597552776}, {"id": 55, "seek": 24156, "start": 246.52, "end": 248.52, "text": " And then what is the for example shortest word?", "tokens": [50612, 400, 550, 437, 307, 264, 337, 1365, 31875, 1349, 30, 50712], "temperature": 0.0, "avg_logprob": -0.1794400413831075, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.007936383597552776}, {"id": 56, "seek": 24156, "start": 249.16, "end": 251.0, "text": " so min of", "tokens": [50744, 370, 923, 295, 50836], "temperature": 0.0, "avg_logprob": -0.1794400413831075, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.007936383597552776}, {"id": 57, "seek": 24156, "start": 251.0, "end": 253.0, "text": " line of each word for w in words", "tokens": [50836, 1622, 295, 1184, 1349, 337, 261, 294, 2283, 50936], "temperature": 0.0, "avg_logprob": -0.1794400413831075, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.007936383597552776}, {"id": 58, "seek": 24156, "start": 253.64000000000001, "end": 256.44, "text": " So the shortest word will be length", "tokens": [50968, 407, 264, 31875, 1349, 486, 312, 4641, 51108], "temperature": 0.0, "avg_logprob": -0.1794400413831075, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.007936383597552776}, {"id": 59, "seek": 24156, "start": 257.16, "end": 258.28, "text": " two", "tokens": [51144, 732, 51200], "temperature": 0.0, "avg_logprob": -0.1794400413831075, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.007936383597552776}, {"id": 60, "seek": 24156, "start": 258.28, "end": 262.28, "text": " And max of land w for w in words. So the longest word will be", "tokens": [51200, 400, 11469, 295, 2117, 261, 337, 261, 294, 2283, 13, 407, 264, 15438, 1349, 486, 312, 51400], "temperature": 0.0, "avg_logprob": -0.1794400413831075, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.007936383597552776}, {"id": 61, "seek": 24156, "start": 263.16, "end": 264.68, "text": " 15 characters", "tokens": [51444, 2119, 4342, 51520], "temperature": 0.0, "avg_logprob": -0.1794400413831075, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.007936383597552776}, {"id": 62, "seek": 24156, "start": 264.68, "end": 266.84000000000003, "text": " So let's now think through our very first language model", "tokens": [51520, 407, 718, 311, 586, 519, 807, 527, 588, 700, 2856, 2316, 51628], "temperature": 0.0, "avg_logprob": -0.1794400413831075, "compression_ratio": 1.6504854368932038, "no_speech_prob": 0.007936383597552776}, {"id": 63, "seek": 26684, "start": 267.4, "end": 272.59999999999997, "text": " As I mentioned a character level language model is predicting the next character in a sequence given", "tokens": [50392, 1018, 286, 2835, 257, 2517, 1496, 2856, 2316, 307, 32884, 264, 958, 2517, 294, 257, 8310, 2212, 50652], "temperature": 0.0, "avg_logprob": -0.13578445132416073, "compression_ratio": 1.8, "no_speech_prob": 0.0072312066331505775}, {"id": 64, "seek": 26684, "start": 273.08, "end": 275.71999999999997, "text": " Already some concrete sequence of characters before it", "tokens": [50676, 23741, 512, 9859, 8310, 295, 4342, 949, 309, 50808], "temperature": 0.0, "avg_logprob": -0.13578445132416073, "compression_ratio": 1.8, "no_speech_prob": 0.0072312066331505775}, {"id": 65, "seek": 26684, "start": 276.59999999999997, "end": 280.28, "text": " Now what we have to realize here is that every single word here like Isabella", "tokens": [50852, 823, 437, 321, 362, 281, 4325, 510, 307, 300, 633, 2167, 1349, 510, 411, 35686, 9885, 51036], "temperature": 0.0, "avg_logprob": -0.13578445132416073, "compression_ratio": 1.8, "no_speech_prob": 0.0072312066331505775}, {"id": 66, "seek": 26684, "start": 280.84, "end": 284.84, "text": " Is actually quite a few examples packed in to that single word", "tokens": [51064, 1119, 767, 1596, 257, 1326, 5110, 13265, 294, 281, 300, 2167, 1349, 51264], "temperature": 0.0, "avg_logprob": -0.13578445132416073, "compression_ratio": 1.8, "no_speech_prob": 0.0072312066331505775}, {"id": 67, "seek": 26684, "start": 285.55999999999995, "end": 290.59999999999997, "text": " Because what is a an existence of a word like Isabella in the data set telling us really it's saying that", "tokens": [51300, 1436, 437, 307, 257, 364, 9123, 295, 257, 1349, 411, 35686, 9885, 294, 264, 1412, 992, 3585, 505, 534, 309, 311, 1566, 300, 51552], "temperature": 0.0, "avg_logprob": -0.13578445132416073, "compression_ratio": 1.8, "no_speech_prob": 0.0072312066331505775}, {"id": 68, "seek": 26684, "start": 291.15999999999997, "end": 295.0, "text": " The character i is a very likely character to come first", "tokens": [51580, 440, 2517, 741, 307, 257, 588, 3700, 2517, 281, 808, 700, 51772], "temperature": 0.0, "avg_logprob": -0.13578445132416073, "compression_ratio": 1.8, "no_speech_prob": 0.0072312066331505775}, {"id": 69, "seek": 29500, "start": 295.56, "end": 297.56, "text": " in a sequence of a name", "tokens": [50392, 294, 257, 8310, 295, 257, 1315, 50492], "temperature": 0.0, "avg_logprob": -0.12443679173787435, "compression_ratio": 1.9763313609467457, "no_speech_prob": 0.005729001946747303}, {"id": 70, "seek": 29500, "start": 298.6, "end": 300.92, "text": " The character s is likely to come", "tokens": [50544, 440, 2517, 262, 307, 3700, 281, 808, 50660], "temperature": 0.0, "avg_logprob": -0.12443679173787435, "compression_ratio": 1.9763313609467457, "no_speech_prob": 0.005729001946747303}, {"id": 71, "seek": 29500, "start": 301.8, "end": 303.8, "text": " after i", "tokens": [50704, 934, 741, 50804], "temperature": 0.0, "avg_logprob": -0.12443679173787435, "compression_ratio": 1.9763313609467457, "no_speech_prob": 0.005729001946747303}, {"id": 72, "seek": 29500, "start": 304.36, "end": 306.84, "text": " The character a is likely to come after is", "tokens": [50832, 440, 2517, 257, 307, 3700, 281, 808, 934, 307, 50956], "temperature": 0.0, "avg_logprob": -0.12443679173787435, "compression_ratio": 1.9763313609467457, "no_speech_prob": 0.005729001946747303}, {"id": 73, "seek": 29500, "start": 307.64, "end": 313.64, "text": " The character b is very likely to come after isa and someone all the way to a following Isabella", "tokens": [50996, 440, 2517, 272, 307, 588, 3700, 281, 808, 934, 307, 64, 293, 1580, 439, 264, 636, 281, 257, 3480, 35686, 9885, 51296], "temperature": 0.0, "avg_logprob": -0.12443679173787435, "compression_ratio": 1.9763313609467457, "no_speech_prob": 0.005729001946747303}, {"id": 74, "seek": 29500, "start": 314.44, "end": 318.36, "text": " And then there's one more example actually packed in here and that is that", "tokens": [51336, 400, 550, 456, 311, 472, 544, 1365, 767, 13265, 294, 510, 293, 300, 307, 300, 51532], "temperature": 0.0, "avg_logprob": -0.12443679173787435, "compression_ratio": 1.9763313609467457, "no_speech_prob": 0.005729001946747303}, {"id": 75, "seek": 29500, "start": 319.08, "end": 321.08, "text": " After there's Isabella", "tokens": [51568, 2381, 456, 311, 35686, 9885, 51668], "temperature": 0.0, "avg_logprob": -0.12443679173787435, "compression_ratio": 1.9763313609467457, "no_speech_prob": 0.005729001946747303}, {"id": 76, "seek": 29500, "start": 321.4, "end": 323.4, "text": " The word is very likely to end", "tokens": [51684, 440, 1349, 307, 588, 3700, 281, 917, 51784], "temperature": 0.0, "avg_logprob": -0.12443679173787435, "compression_ratio": 1.9763313609467457, "no_speech_prob": 0.005729001946747303}, {"id": 77, "seek": 32340, "start": 323.79999999999995, "end": 328.84, "text": " So that's one more sort of explicit piece of information that we have here that we have to be careful with", "tokens": [50384, 407, 300, 311, 472, 544, 1333, 295, 13691, 2522, 295, 1589, 300, 321, 362, 510, 300, 321, 362, 281, 312, 5026, 365, 50636], "temperature": 0.0, "avg_logprob": -0.10238565832881605, "compression_ratio": 1.792857142857143, "no_speech_prob": 0.001896521425805986}, {"id": 78, "seek": 32340, "start": 329.64, "end": 333.32, "text": " And so there's a lot packed into a single individual word in terms of the", "tokens": [50676, 400, 370, 456, 311, 257, 688, 13265, 666, 257, 2167, 2609, 1349, 294, 2115, 295, 264, 50860], "temperature": 0.0, "avg_logprob": -0.10238565832881605, "compression_ratio": 1.792857142857143, "no_speech_prob": 0.001896521425805986}, {"id": 79, "seek": 32340, "start": 333.79999999999995, "end": 337.47999999999996, "text": " Statistical structure of what's likely to follow in these character sequences", "tokens": [50884, 16249, 42686, 3877, 295, 437, 311, 3700, 281, 1524, 294, 613, 2517, 22978, 51068], "temperature": 0.0, "avg_logprob": -0.10238565832881605, "compression_ratio": 1.792857142857143, "no_speech_prob": 0.001896521425805986}, {"id": 80, "seek": 32340, "start": 338.12, "end": 343.96, "text": " And then of course we don't have just an individual word. We actually have 32 000 of these and so there's a lot of structure here to model", "tokens": [51100, 400, 550, 295, 1164, 321, 500, 380, 362, 445, 364, 2609, 1349, 13, 492, 767, 362, 8858, 13711, 295, 613, 293, 370, 456, 311, 257, 688, 295, 3877, 510, 281, 2316, 51392], "temperature": 0.0, "avg_logprob": -0.10238565832881605, "compression_ratio": 1.792857142857143, "no_speech_prob": 0.001896521425805986}, {"id": 81, "seek": 32340, "start": 344.91999999999996, "end": 349.96, "text": " Now in beginning what I'd like to start with is I'd like to start with building a bi-gram language model", "tokens": [51440, 823, 294, 2863, 437, 286, 1116, 411, 281, 722, 365, 307, 286, 1116, 411, 281, 722, 365, 2390, 257, 3228, 12, 1342, 2856, 2316, 51692], "temperature": 0.0, "avg_logprob": -0.10238565832881605, "compression_ratio": 1.792857142857143, "no_speech_prob": 0.001896521425805986}, {"id": 82, "seek": 34996, "start": 350.2, "end": 355.96, "text": " Now in a bi-gram language model, we're always working with just two characters at a time", "tokens": [50376, 823, 294, 257, 3228, 12, 1342, 2856, 2316, 11, 321, 434, 1009, 1364, 365, 445, 732, 4342, 412, 257, 565, 50664], "temperature": 0.0, "avg_logprob": -0.1387172212787703, "compression_ratio": 1.8663793103448276, "no_speech_prob": 0.006288317032158375}, {"id": 83, "seek": 34996, "start": 356.76, "end": 363.0, "text": " So we're only looking at one character that we are given and we're trying to predict the next character in the sequence", "tokens": [50704, 407, 321, 434, 787, 1237, 412, 472, 2517, 300, 321, 366, 2212, 293, 321, 434, 1382, 281, 6069, 264, 958, 2517, 294, 264, 8310, 51016], "temperature": 0.0, "avg_logprob": -0.1387172212787703, "compression_ratio": 1.8663793103448276, "no_speech_prob": 0.006288317032158375}, {"id": 84, "seek": 34996, "start": 363.96, "end": 365.15999999999997, "text": " so", "tokens": [51064, 370, 51124], "temperature": 0.0, "avg_logprob": -0.1387172212787703, "compression_ratio": 1.8663793103448276, "no_speech_prob": 0.006288317032158375}, {"id": 85, "seek": 34996, "start": 365.15999999999997, "end": 371.4, "text": " What characters are likely to follow are what characters are likely to follow a and so on and we're just modeling that kind of a little", "tokens": [51124, 708, 4342, 366, 3700, 281, 1524, 366, 437, 4342, 366, 3700, 281, 1524, 257, 293, 370, 322, 293, 321, 434, 445, 15983, 300, 733, 295, 257, 707, 51436], "temperature": 0.0, "avg_logprob": -0.1387172212787703, "compression_ratio": 1.8663793103448276, "no_speech_prob": 0.006288317032158375}, {"id": 86, "seek": 34996, "start": 371.64, "end": 372.91999999999996, "text": " local structure", "tokens": [51448, 2654, 3877, 51512], "temperature": 0.0, "avg_logprob": -0.1387172212787703, "compression_ratio": 1.8663793103448276, "no_speech_prob": 0.006288317032158375}, {"id": 87, "seek": 34996, "start": 372.91999999999996, "end": 376.52, "text": " And we're forgetting the fact that we may have a lot more information", "tokens": [51512, 400, 321, 434, 25428, 264, 1186, 300, 321, 815, 362, 257, 688, 544, 1589, 51692], "temperature": 0.0, "avg_logprob": -0.1387172212787703, "compression_ratio": 1.8663793103448276, "no_speech_prob": 0.006288317032158375}, {"id": 88, "seek": 37652, "start": 376.76, "end": 379.79999999999995, "text": " We're always just looking at the previous character to predict the next one", "tokens": [50376, 492, 434, 1009, 445, 1237, 412, 264, 3894, 2517, 281, 6069, 264, 958, 472, 50528], "temperature": 0.0, "avg_logprob": -0.12282467696626308, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.012621365487575531}, {"id": 89, "seek": 37652, "start": 380.2, "end": 383.47999999999996, "text": " So it's a very simple and weak language model, but I think it's a great place to start", "tokens": [50548, 407, 309, 311, 257, 588, 2199, 293, 5336, 2856, 2316, 11, 457, 286, 519, 309, 311, 257, 869, 1081, 281, 722, 50712], "temperature": 0.0, "avg_logprob": -0.12282467696626308, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.012621365487575531}, {"id": 90, "seek": 37652, "start": 384.12, "end": 390.35999999999996, "text": " So now let's begin by looking at these bi-grams in our data set and what they look like and these bi-grams again are just two characters in a row", "tokens": [50744, 407, 586, 718, 311, 1841, 538, 1237, 412, 613, 3228, 12, 1342, 82, 294, 527, 1412, 992, 293, 437, 436, 574, 411, 293, 613, 3228, 12, 1342, 82, 797, 366, 445, 732, 4342, 294, 257, 5386, 51056], "temperature": 0.0, "avg_logprob": -0.12282467696626308, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.012621365487575531}, {"id": 91, "seek": 37652, "start": 390.91999999999996, "end": 392.91999999999996, "text": " so for w in words", "tokens": [51084, 370, 337, 261, 294, 2283, 51184], "temperature": 0.0, "avg_logprob": -0.12282467696626308, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.012621365487575531}, {"id": 92, "seek": 37652, "start": 393.15999999999997, "end": 395.56, "text": " each w here is an individual word a string", "tokens": [51196, 1184, 261, 510, 307, 364, 2609, 1349, 257, 6798, 51316], "temperature": 0.0, "avg_logprob": -0.12282467696626308, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.012621365487575531}, {"id": 93, "seek": 37652, "start": 396.35999999999996, "end": 398.35999999999996, "text": " we want to iterate uh for", "tokens": [51356, 321, 528, 281, 44497, 2232, 337, 51456], "temperature": 0.0, "avg_logprob": -0.12282467696626308, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.012621365487575531}, {"id": 94, "seek": 37652, "start": 399.64, "end": 401.64, "text": " We want to iterate this word", "tokens": [51520, 492, 528, 281, 44497, 341, 1349, 51620], "temperature": 0.0, "avg_logprob": -0.12282467696626308, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.012621365487575531}, {"id": 95, "seek": 37652, "start": 401.71999999999997, "end": 403.71999999999997, "text": " with consecutive characters", "tokens": [51624, 365, 30497, 4342, 51724], "temperature": 0.0, "avg_logprob": -0.12282467696626308, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.012621365487575531}, {"id": 96, "seek": 40372, "start": 403.72, "end": 406.36, "text": " So two characters at a time sliding it through the word", "tokens": [50364, 407, 732, 4342, 412, 257, 565, 21169, 309, 807, 264, 1349, 50496], "temperature": 0.0, "avg_logprob": -0.15481887477459294, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0026726361829787493}, {"id": 97, "seek": 40372, "start": 406.84000000000003, "end": 410.92, "text": " Now a interesting nice way cute way to do this in python, by the way", "tokens": [50520, 823, 257, 1880, 1481, 636, 4052, 636, 281, 360, 341, 294, 38797, 11, 538, 264, 636, 50724], "temperature": 0.0, "avg_logprob": -0.15481887477459294, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0026726361829787493}, {"id": 98, "seek": 40372, "start": 411.32000000000005, "end": 415.8, "text": " Is doing something like this for character one character two in zip of", "tokens": [50744, 1119, 884, 746, 411, 341, 337, 2517, 472, 2517, 732, 294, 20730, 295, 50968], "temperature": 0.0, "avg_logprob": -0.15481887477459294, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0026726361829787493}, {"id": 99, "seek": 40372, "start": 416.44000000000005, "end": 418.44000000000005, "text": " w and w at one", "tokens": [51000, 261, 293, 261, 412, 472, 51100], "temperature": 0.0, "avg_logprob": -0.15481887477459294, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0026726361829787493}, {"id": 100, "seek": 40372, "start": 420.04, "end": 421.8, "text": " one column", "tokens": [51180, 472, 7738, 51268], "temperature": 0.0, "avg_logprob": -0.15481887477459294, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0026726361829787493}, {"id": 101, "seek": 40372, "start": 421.8, "end": 422.92, "text": " print", "tokens": [51268, 4482, 51324], "temperature": 0.0, "avg_logprob": -0.15481887477459294, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0026726361829787493}, {"id": 102, "seek": 40372, "start": 422.92, "end": 424.68, "text": " character one character two", "tokens": [51324, 2517, 472, 2517, 732, 51412], "temperature": 0.0, "avg_logprob": -0.15481887477459294, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0026726361829787493}, {"id": 103, "seek": 40372, "start": 424.68, "end": 425.72, "text": " And let's not do all the words", "tokens": [51412, 400, 718, 311, 406, 360, 439, 264, 2283, 51464], "temperature": 0.0, "avg_logprob": -0.15481887477459294, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0026726361829787493}, {"id": 104, "seek": 40372, "start": 425.72, "end": 429.40000000000003, "text": " Let's just do the first three words and i'm going to show you in a second how this works", "tokens": [51464, 961, 311, 445, 360, 264, 700, 1045, 2283, 293, 741, 478, 516, 281, 855, 291, 294, 257, 1150, 577, 341, 1985, 51648], "temperature": 0.0, "avg_logprob": -0.15481887477459294, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0026726361829787493}, {"id": 105, "seek": 42940, "start": 429.96, "end": 434.12, "text": " But for now basically as an example, let's just do the very first word alone emma", "tokens": [50392, 583, 337, 586, 1936, 382, 364, 1365, 11, 718, 311, 445, 360, 264, 588, 700, 1349, 3312, 846, 1696, 50600], "temperature": 0.0, "avg_logprob": -0.16714970270792642, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.013017100282013416}, {"id": 106, "seek": 42940, "start": 435.4, "end": 440.28, "text": " You see how we have a emma and this will just print em, mm, ma", "tokens": [50664, 509, 536, 577, 321, 362, 257, 846, 1696, 293, 341, 486, 445, 4482, 846, 11, 11169, 11, 463, 50908], "temperature": 0.0, "avg_logprob": -0.16714970270792642, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.013017100282013416}, {"id": 107, "seek": 42940, "start": 441.0, "end": 447.88, "text": " And the reason this works is because w is the string emma w at one column is the string mma", "tokens": [50944, 400, 264, 1778, 341, 1985, 307, 570, 261, 307, 264, 6798, 846, 1696, 261, 412, 472, 7738, 307, 264, 6798, 275, 1696, 51288], "temperature": 0.0, "avg_logprob": -0.16714970270792642, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.013017100282013416}, {"id": 108, "seek": 42940, "start": 448.67999999999995, "end": 453.0, "text": " And zip takes two iterators and it pairs them up", "tokens": [51328, 400, 20730, 2516, 732, 17138, 3391, 293, 309, 15494, 552, 493, 51544], "temperature": 0.0, "avg_logprob": -0.16714970270792642, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.013017100282013416}, {"id": 109, "seek": 42940, "start": 453.47999999999996, "end": 456.84, "text": " And then creates an iterator over the tuples of their consecutive entries", "tokens": [51568, 400, 550, 7829, 364, 17138, 1639, 670, 264, 2604, 2622, 295, 641, 30497, 23041, 51736], "temperature": 0.0, "avg_logprob": -0.16714970270792642, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.013017100282013416}, {"id": 110, "seek": 45684, "start": 457.4, "end": 462.84, "text": " And if any one of these lists is shorter than the other then it will just uh halt and return", "tokens": [50392, 400, 498, 604, 472, 295, 613, 14511, 307, 11639, 813, 264, 661, 550, 309, 486, 445, 2232, 12479, 293, 2736, 50664], "temperature": 0.0, "avg_logprob": -0.15673286970271622, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.005819195881485939}, {"id": 111, "seek": 45684, "start": 463.71999999999997, "end": 469.32, "text": " So basically, that's why we return em, mm, mm, ma", "tokens": [50708, 407, 1936, 11, 300, 311, 983, 321, 2736, 846, 11, 11169, 11, 11169, 11, 463, 50988], "temperature": 0.0, "avg_logprob": -0.15673286970271622, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.005819195881485939}, {"id": 112, "seek": 45684, "start": 469.96, "end": 477.23999999999995, "text": " But then because this iterator second one here runs out of elements zip just ends and that's why we only get these tuples", "tokens": [51020, 583, 550, 570, 341, 17138, 1639, 1150, 472, 510, 6676, 484, 295, 4959, 20730, 445, 5314, 293, 300, 311, 983, 321, 787, 483, 613, 2604, 2622, 51384], "temperature": 0.0, "avg_logprob": -0.15673286970271622, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.005819195881485939}, {"id": 113, "seek": 45684, "start": 477.71999999999997, "end": 479.71999999999997, "text": " So pretty cute", "tokens": [51408, 407, 1238, 4052, 51508], "temperature": 0.0, "avg_logprob": -0.15673286970271622, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.005819195881485939}, {"id": 114, "seek": 45684, "start": 479.71999999999997, "end": 482.59999999999997, "text": " So these are the consecutive elements in the first word", "tokens": [51508, 407, 613, 366, 264, 30497, 4959, 294, 264, 700, 1349, 51652], "temperature": 0.0, "avg_logprob": -0.15673286970271622, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.005819195881485939}, {"id": 115, "seek": 48260, "start": 483.08000000000004, "end": 486.76000000000005, "text": " Now we have to be careful because we actually have more information here than just these three", "tokens": [50388, 823, 321, 362, 281, 312, 5026, 570, 321, 767, 362, 544, 1589, 510, 813, 445, 613, 1045, 50572], "temperature": 0.0, "avg_logprob": -0.12602399127318129, "compression_ratio": 1.738197424892704, "no_speech_prob": 0.022967353463172913}, {"id": 116, "seek": 48260, "start": 487.32000000000005, "end": 494.76000000000005, "text": " Examples as I mentioned we know that e is the is very likely to come first and we know that a in this case is coming last", "tokens": [50600, 48591, 382, 286, 2835, 321, 458, 300, 308, 307, 264, 307, 588, 3700, 281, 808, 700, 293, 321, 458, 300, 257, 294, 341, 1389, 307, 1348, 1036, 50972], "temperature": 0.0, "avg_logprob": -0.12602399127318129, "compression_ratio": 1.738197424892704, "no_speech_prob": 0.022967353463172913}, {"id": 117, "seek": 48260, "start": 495.64000000000004, "end": 498.68, "text": " So one way to do this is basically we're going to create", "tokens": [51016, 407, 472, 636, 281, 360, 341, 307, 1936, 321, 434, 516, 281, 1884, 51168], "temperature": 0.0, "avg_logprob": -0.12602399127318129, "compression_ratio": 1.738197424892704, "no_speech_prob": 0.022967353463172913}, {"id": 118, "seek": 48260, "start": 499.48, "end": 501.48, "text": " special array here", "tokens": [51208, 2121, 10225, 510, 51308], "temperature": 0.0, "avg_logprob": -0.12602399127318129, "compression_ratio": 1.738197424892704, "no_speech_prob": 0.022967353463172913}, {"id": 119, "seek": 48260, "start": 501.56, "end": 503.08000000000004, "text": " characters", "tokens": [51312, 4342, 51388], "temperature": 0.0, "avg_logprob": -0.12602399127318129, "compression_ratio": 1.738197424892704, "no_speech_prob": 0.022967353463172913}, {"id": 120, "seek": 48260, "start": 503.08000000000004, "end": 507.0, "text": " And um, we're going to hallucinate a special start token here", "tokens": [51388, 400, 1105, 11, 321, 434, 516, 281, 35212, 13923, 257, 2121, 722, 14862, 510, 51584], "temperature": 0.0, "avg_logprob": -0.12602399127318129, "compression_ratio": 1.738197424892704, "no_speech_prob": 0.022967353463172913}, {"id": 121, "seek": 48260, "start": 508.44, "end": 509.88, "text": " I'm going to", "tokens": [51656, 286, 478, 516, 281, 51728], "temperature": 0.0, "avg_logprob": -0.12602399127318129, "compression_ratio": 1.738197424892704, "no_speech_prob": 0.022967353463172913}, {"id": 122, "seek": 48260, "start": 509.88, "end": 511.88, "text": " call it like special start", "tokens": [51728, 818, 309, 411, 2121, 722, 51828], "temperature": 0.0, "avg_logprob": -0.12602399127318129, "compression_ratio": 1.738197424892704, "no_speech_prob": 0.022967353463172913}, {"id": 123, "seek": 51260, "start": 512.6, "end": 514.6, "text": " So this is a list of one element", "tokens": [50364, 407, 341, 307, 257, 1329, 295, 472, 4478, 50464], "temperature": 0.0, "avg_logprob": -0.10395110448201497, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.0004108351422473788}, {"id": 124, "seek": 51260, "start": 514.6, "end": 516.6, "text": " plus", "tokens": [50464, 1804, 50564], "temperature": 0.0, "avg_logprob": -0.10395110448201497, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.0004108351422473788}, {"id": 125, "seek": 51260, "start": 516.6, "end": 517.8000000000001, "text": " w", "tokens": [50564, 261, 50624], "temperature": 0.0, "avg_logprob": -0.10395110448201497, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.0004108351422473788}, {"id": 126, "seek": 51260, "start": 517.8000000000001, "end": 520.36, "text": " And then plus a special end character", "tokens": [50624, 400, 550, 1804, 257, 2121, 917, 2517, 50752], "temperature": 0.0, "avg_logprob": -0.10395110448201497, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.0004108351422473788}, {"id": 127, "seek": 51260, "start": 521.32, "end": 525.32, "text": " And the reason i'm wrapping the list of w here is because w is a string mma", "tokens": [50800, 400, 264, 1778, 741, 478, 21993, 264, 1329, 295, 261, 510, 307, 570, 261, 307, 257, 6798, 275, 1696, 51000], "temperature": 0.0, "avg_logprob": -0.10395110448201497, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.0004108351422473788}, {"id": 128, "seek": 51260, "start": 525.96, "end": 529.72, "text": " List of w will just have the individual characters in the list", "tokens": [51032, 17668, 295, 261, 486, 445, 362, 264, 2609, 4342, 294, 264, 1329, 51220], "temperature": 0.0, "avg_logprob": -0.10395110448201497, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.0004108351422473788}, {"id": 129, "seek": 51260, "start": 530.84, "end": 536.9200000000001, "text": " And then doing this again now, but not iterating over w's but over the characters", "tokens": [51276, 400, 550, 884, 341, 797, 586, 11, 457, 406, 17138, 990, 670, 261, 311, 457, 670, 264, 4342, 51580], "temperature": 0.0, "avg_logprob": -0.10395110448201497, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.0004108351422473788}, {"id": 130, "seek": 51260, "start": 538.2, "end": 540.2, "text": " Will give us something like this", "tokens": [51644, 3099, 976, 505, 746, 411, 341, 51744], "temperature": 0.0, "avg_logprob": -0.10395110448201497, "compression_ratio": 1.7150259067357514, "no_speech_prob": 0.0004108351422473788}, {"id": 131, "seek": 54020, "start": 540.2, "end": 545.88, "text": " So e is likely so this is a bygram of the start character and e and this is a bygram of the", "tokens": [50364, 407, 308, 307, 3700, 370, 341, 307, 257, 538, 1342, 295, 264, 722, 2517, 293, 308, 293, 341, 307, 257, 538, 1342, 295, 264, 50648], "temperature": 0.0, "avg_logprob": -0.13077452874952747, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.0011334982700645924}, {"id": 132, "seek": 54020, "start": 546.44, "end": 548.44, "text": " a and the special end character", "tokens": [50676, 257, 293, 264, 2121, 917, 2517, 50776], "temperature": 0.0, "avg_logprob": -0.13077452874952747, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.0011334982700645924}, {"id": 133, "seek": 54020, "start": 549.08, "end": 551.24, "text": " And now we can look at for example what this looks like for", "tokens": [50808, 400, 586, 321, 393, 574, 412, 337, 1365, 437, 341, 1542, 411, 337, 50916], "temperature": 0.0, "avg_logprob": -0.13077452874952747, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.0011334982700645924}, {"id": 134, "seek": 54020, "start": 552.0400000000001, "end": 554.0400000000001, "text": " olivia or heva", "tokens": [50956, 2545, 18503, 420, 415, 2757, 51056], "temperature": 0.0, "avg_logprob": -0.13077452874952747, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.0011334982700645924}, {"id": 135, "seek": 54020, "start": 554.5200000000001, "end": 556.5200000000001, "text": " And indeed we can actually", "tokens": [51080, 400, 6451, 321, 393, 767, 51180], "temperature": 0.0, "avg_logprob": -0.13077452874952747, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.0011334982700645924}, {"id": 136, "seek": 54020, "start": 556.5200000000001, "end": 560.0400000000001, "text": " Potentially do this for the entire data set, but we won't print that that's going to be too much", "tokens": [51180, 9145, 3137, 360, 341, 337, 264, 2302, 1412, 992, 11, 457, 321, 1582, 380, 4482, 300, 300, 311, 516, 281, 312, 886, 709, 51356], "temperature": 0.0, "avg_logprob": -0.13077452874952747, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.0011334982700645924}, {"id": 137, "seek": 54020, "start": 560.6800000000001, "end": 564.12, "text": " But these are the individual character bygrams and we can print them", "tokens": [51388, 583, 613, 366, 264, 2609, 2517, 538, 1342, 82, 293, 321, 393, 4482, 552, 51560], "temperature": 0.0, "avg_logprob": -0.13077452874952747, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.0011334982700645924}, {"id": 138, "seek": 54020, "start": 565.0, "end": 569.48, "text": " Now in order to learn the statistics about which characters are likely to follow other characters", "tokens": [51604, 823, 294, 1668, 281, 1466, 264, 12523, 466, 597, 4342, 366, 3700, 281, 1524, 661, 4342, 51828], "temperature": 0.0, "avg_logprob": -0.13077452874952747, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.0011334982700645924}, {"id": 139, "seek": 56948, "start": 569.8000000000001, "end": 573.8000000000001, "text": " The simplest way in the bygram language models is to simply do it by counting", "tokens": [50380, 440, 22811, 636, 294, 264, 538, 1342, 2856, 5245, 307, 281, 2935, 360, 309, 538, 13251, 50580], "temperature": 0.0, "avg_logprob": -0.07848318691911368, "compression_ratio": 1.8744769874476988, "no_speech_prob": 0.003649546531960368}, {"id": 140, "seek": 56948, "start": 574.44, "end": 579.72, "text": " So we're basically just going to count how often any one of these combinations occurs in the training set", "tokens": [50612, 407, 321, 434, 1936, 445, 516, 281, 1207, 577, 2049, 604, 472, 295, 613, 21267, 11843, 294, 264, 3097, 992, 50876], "temperature": 0.0, "avg_logprob": -0.07848318691911368, "compression_ratio": 1.8744769874476988, "no_speech_prob": 0.003649546531960368}, {"id": 141, "seek": 56948, "start": 580.28, "end": 581.8000000000001, "text": " In these words", "tokens": [50904, 682, 613, 2283, 50980], "temperature": 0.0, "avg_logprob": -0.07848318691911368, "compression_ratio": 1.8744769874476988, "no_speech_prob": 0.003649546531960368}, {"id": 142, "seek": 56948, "start": 581.8000000000001, "end": 586.84, "text": " So we're going to need some kind of a dictionary that's going to maintain some counts for every one of these bygrams", "tokens": [50980, 407, 321, 434, 516, 281, 643, 512, 733, 295, 257, 25890, 300, 311, 516, 281, 6909, 512, 14893, 337, 633, 472, 295, 613, 538, 1342, 82, 51232], "temperature": 0.0, "avg_logprob": -0.07848318691911368, "compression_ratio": 1.8744769874476988, "no_speech_prob": 0.003649546531960368}, {"id": 143, "seek": 56948, "start": 587.32, "end": 589.32, "text": " So let's use a dictionary b", "tokens": [51256, 407, 718, 311, 764, 257, 25890, 272, 51356], "temperature": 0.0, "avg_logprob": -0.07848318691911368, "compression_ratio": 1.8744769874476988, "no_speech_prob": 0.003649546531960368}, {"id": 144, "seek": 56948, "start": 589.8000000000001, "end": 592.36, "text": " And this will map these bygrams", "tokens": [51380, 400, 341, 486, 4471, 613, 538, 1342, 82, 51508], "temperature": 0.0, "avg_logprob": -0.07848318691911368, "compression_ratio": 1.8744769874476988, "no_speech_prob": 0.003649546531960368}, {"id": 145, "seek": 56948, "start": 592.6800000000001, "end": 595.08, "text": " So bygram is a tuple of character one character two", "tokens": [51524, 407, 538, 1342, 307, 257, 2604, 781, 295, 2517, 472, 2517, 732, 51644], "temperature": 0.0, "avg_logprob": -0.07848318691911368, "compression_ratio": 1.8744769874476988, "no_speech_prob": 0.003649546531960368}, {"id": 146, "seek": 56948, "start": 596.2, "end": 598.2, "text": " And then b at bygram", "tokens": [51700, 400, 550, 272, 412, 538, 1342, 51800], "temperature": 0.0, "avg_logprob": -0.07848318691911368, "compression_ratio": 1.8744769874476988, "no_speech_prob": 0.003649546531960368}, {"id": 147, "seek": 59820, "start": 598.76, "end": 600.76, "text": " Will be b dot get of bygram", "tokens": [50392, 3099, 312, 272, 5893, 483, 295, 538, 1342, 50492], "temperature": 0.0, "avg_logprob": -0.08838165507597082, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003483146894723177}, {"id": 148, "seek": 59820, "start": 601.08, "end": 603.72, "text": " Which is basically the same as b at bygram", "tokens": [50508, 3013, 307, 1936, 264, 912, 382, 272, 412, 538, 1342, 50640], "temperature": 0.0, "avg_logprob": -0.08838165507597082, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003483146894723177}, {"id": 149, "seek": 59820, "start": 604.6800000000001, "end": 610.6, "text": " But in the case that bygram is not in the dictionary b. We would like to by default return a zero", "tokens": [50688, 583, 294, 264, 1389, 300, 538, 1342, 307, 406, 294, 264, 25890, 272, 13, 492, 576, 411, 281, 538, 7576, 2736, 257, 4018, 50984], "temperature": 0.0, "avg_logprob": -0.08838165507597082, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003483146894723177}, {"id": 150, "seek": 59820, "start": 611.88, "end": 613.08, "text": " Plus one", "tokens": [51048, 7721, 472, 51108], "temperature": 0.0, "avg_logprob": -0.08838165507597082, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003483146894723177}, {"id": 151, "seek": 59820, "start": 613.08, "end": 617.5600000000001, "text": " So this will basically add up all the bygrams and count how often they occur", "tokens": [51108, 407, 341, 486, 1936, 909, 493, 439, 264, 538, 1342, 82, 293, 1207, 577, 2049, 436, 5160, 51332], "temperature": 0.0, "avg_logprob": -0.08838165507597082, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003483146894723177}, {"id": 152, "seek": 59820, "start": 618.2800000000001, "end": 620.2800000000001, "text": " Let's get rid of printing", "tokens": [51368, 961, 311, 483, 3973, 295, 14699, 51468], "temperature": 0.0, "avg_logprob": -0.08838165507597082, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003483146894723177}, {"id": 153, "seek": 59820, "start": 620.2800000000001, "end": 622.44, "text": " or rather", "tokens": [51468, 420, 2831, 51576], "temperature": 0.0, "avg_logprob": -0.08838165507597082, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003483146894723177}, {"id": 154, "seek": 59820, "start": 622.44, "end": 625.96, "text": " Let's keep the printing and let's just inspect what b is in this case", "tokens": [51576, 961, 311, 1066, 264, 14699, 293, 718, 311, 445, 15018, 437, 272, 307, 294, 341, 1389, 51752], "temperature": 0.0, "avg_logprob": -0.08838165507597082, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003483146894723177}, {"id": 155, "seek": 62596, "start": 626.9200000000001, "end": 632.2800000000001, "text": " And we see that many bygrams occur just a single time this one allegedly occurred three times", "tokens": [50412, 400, 321, 536, 300, 867, 538, 1342, 82, 5160, 445, 257, 2167, 565, 341, 472, 26794, 11068, 1045, 1413, 50680], "temperature": 0.0, "avg_logprob": -0.17451495267032238, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005468536168336868}, {"id": 156, "seek": 62596, "start": 633.0, "end": 635.0, "text": " So a was an ending character three times", "tokens": [50716, 407, 257, 390, 364, 8121, 2517, 1045, 1413, 50816], "temperature": 0.0, "avg_logprob": -0.17451495267032238, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005468536168336868}, {"id": 157, "seek": 62596, "start": 635.48, "end": 640.6, "text": " And that's true for all of these words all of emma olivia and eva and with a", "tokens": [50840, 400, 300, 311, 2074, 337, 439, 295, 613, 2283, 439, 295, 846, 1696, 2545, 18503, 293, 1073, 64, 293, 365, 257, 51096], "temperature": 0.0, "avg_logprob": -0.17451495267032238, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005468536168336868}, {"id": 158, "seek": 62596, "start": 641.64, "end": 644.0400000000001, "text": " Uh, so that's why this occurred three times", "tokens": [51148, 4019, 11, 370, 300, 311, 983, 341, 11068, 1045, 1413, 51268], "temperature": 0.0, "avg_logprob": -0.17451495267032238, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005468536168336868}, {"id": 159, "seek": 62596, "start": 645.72, "end": 648.52, "text": " Um now let's do it for all the words", "tokens": [51352, 3301, 586, 718, 311, 360, 309, 337, 439, 264, 2283, 51492], "temperature": 0.0, "avg_logprob": -0.17451495267032238, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005468536168336868}, {"id": 160, "seek": 62596, "start": 651.24, "end": 653.24, "text": " Oops, I should not have printed", "tokens": [51628, 21726, 11, 286, 820, 406, 362, 13567, 51728], "temperature": 0.0, "avg_logprob": -0.17451495267032238, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005468536168336868}, {"id": 161, "seek": 65324, "start": 653.96, "end": 655.96, "text": " I meant to erase that", "tokens": [50400, 286, 4140, 281, 23525, 300, 50500], "temperature": 0.0, "avg_logprob": -0.13513980525554997, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.003272805130109191}, {"id": 162, "seek": 65324, "start": 656.76, "end": 658.76, "text": " Let's kill this", "tokens": [50540, 961, 311, 1961, 341, 50640], "temperature": 0.0, "avg_logprob": -0.13513980525554997, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.003272805130109191}, {"id": 163, "seek": 65324, "start": 658.76, "end": 660.6, "text": " Let's just run", "tokens": [50640, 961, 311, 445, 1190, 50732], "temperature": 0.0, "avg_logprob": -0.13513980525554997, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.003272805130109191}, {"id": 164, "seek": 65324, "start": 660.6, "end": 663.16, "text": " And now b will have the statistics of the entire data set", "tokens": [50732, 400, 586, 272, 486, 362, 264, 12523, 295, 264, 2302, 1412, 992, 50860], "temperature": 0.0, "avg_logprob": -0.13513980525554997, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.003272805130109191}, {"id": 165, "seek": 65324, "start": 664.12, "end": 667.5600000000001, "text": " So these are the counts across all the words of the individual bygrams", "tokens": [50908, 407, 613, 366, 264, 14893, 2108, 439, 264, 2283, 295, 264, 2609, 538, 1342, 82, 51080], "temperature": 0.0, "avg_logprob": -0.13513980525554997, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.003272805130109191}, {"id": 166, "seek": 65324, "start": 668.36, "end": 671.88, "text": " And we could for example look at some of the most common ones and least common ones", "tokens": [51120, 400, 321, 727, 337, 1365, 574, 412, 512, 295, 264, 881, 2689, 2306, 293, 1935, 2689, 2306, 51296], "temperature": 0.0, "avg_logprob": -0.13513980525554997, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.003272805130109191}, {"id": 167, "seek": 65324, "start": 672.76, "end": 674.76, "text": " Um, this kind of grows in python", "tokens": [51340, 3301, 11, 341, 733, 295, 13156, 294, 38797, 51440], "temperature": 0.0, "avg_logprob": -0.13513980525554997, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.003272805130109191}, {"id": 168, "seek": 65324, "start": 674.76, "end": 678.84, "text": " But the way to do this the simplest way I like is we just use b dot items", "tokens": [51440, 583, 264, 636, 281, 360, 341, 264, 22811, 636, 286, 411, 307, 321, 445, 764, 272, 5893, 4754, 51644], "temperature": 0.0, "avg_logprob": -0.13513980525554997, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.003272805130109191}, {"id": 169, "seek": 67884, "start": 679.5600000000001, "end": 681.5600000000001, "text": " b dot items returns", "tokens": [50400, 272, 5893, 4754, 11247, 50500], "temperature": 0.0, "avg_logprob": -0.122333824634552, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.007694569882005453}, {"id": 170, "seek": 67884, "start": 681.8000000000001, "end": 683.8000000000001, "text": " the tuples of", "tokens": [50512, 264, 2604, 2622, 295, 50612], "temperature": 0.0, "avg_logprob": -0.122333824634552, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.007694569882005453}, {"id": 171, "seek": 67884, "start": 684.36, "end": 690.0400000000001, "text": " Key value in this case the keys are the character bygrams and the values are the counts", "tokens": [50640, 12759, 2158, 294, 341, 1389, 264, 9317, 366, 264, 2517, 538, 1342, 82, 293, 264, 4190, 366, 264, 14893, 50924], "temperature": 0.0, "avg_logprob": -0.122333824634552, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.007694569882005453}, {"id": 172, "seek": 67884, "start": 690.9200000000001, "end": 693.4, "text": " And so then what we want to do is we want to do um", "tokens": [50968, 400, 370, 550, 437, 321, 528, 281, 360, 307, 321, 528, 281, 360, 1105, 51092], "temperature": 0.0, "avg_logprob": -0.122333824634552, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.007694569882005453}, {"id": 173, "seek": 67884, "start": 695.72, "end": 697.72, "text": " Sort it of this", "tokens": [51208, 26149, 309, 295, 341, 51308], "temperature": 0.0, "avg_logprob": -0.122333824634552, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.007694569882005453}, {"id": 174, "seek": 67884, "start": 698.44, "end": 700.44, "text": " But by default sort is on the first", "tokens": [51344, 583, 538, 7576, 1333, 307, 322, 264, 700, 51444], "temperature": 0.0, "avg_logprob": -0.122333824634552, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.007694569882005453}, {"id": 175, "seek": 67884, "start": 701.1600000000001, "end": 703.1600000000001, "text": " um", "tokens": [51480, 1105, 51580], "temperature": 0.0, "avg_logprob": -0.122333824634552, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.007694569882005453}, {"id": 176, "seek": 67884, "start": 703.5600000000001, "end": 705.5600000000001, "text": " On the first item of a tuple", "tokens": [51600, 1282, 264, 700, 3174, 295, 257, 2604, 781, 51700], "temperature": 0.0, "avg_logprob": -0.122333824634552, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.007694569882005453}, {"id": 177, "seek": 70556, "start": 705.56, "end": 709.88, "text": " But we want to sort by the values which are the second element of a tuple that is the key value", "tokens": [50364, 583, 321, 528, 281, 1333, 538, 264, 4190, 597, 366, 264, 1150, 4478, 295, 257, 2604, 781, 300, 307, 264, 2141, 2158, 50580], "temperature": 0.0, "avg_logprob": -0.10279950990781679, "compression_ratio": 1.9127906976744187, "no_speech_prob": 0.0021823253482580185}, {"id": 178, "seek": 70556, "start": 710.68, "end": 712.68, "text": " So we want to use the key", "tokens": [50620, 407, 321, 528, 281, 764, 264, 2141, 50720], "temperature": 0.0, "avg_logprob": -0.10279950990781679, "compression_ratio": 1.9127906976744187, "no_speech_prob": 0.0021823253482580185}, {"id": 179, "seek": 70556, "start": 713.0799999999999, "end": 715.0799999999999, "text": " equals lambda", "tokens": [50740, 6915, 13607, 50840], "temperature": 0.0, "avg_logprob": -0.10279950990781679, "compression_ratio": 1.9127906976744187, "no_speech_prob": 0.0021823253482580185}, {"id": 180, "seek": 70556, "start": 715.3199999999999, "end": 717.3199999999999, "text": " That takes the key value", "tokens": [50852, 663, 2516, 264, 2141, 2158, 50952], "temperature": 0.0, "avg_logprob": -0.10279950990781679, "compression_ratio": 1.9127906976744187, "no_speech_prob": 0.0021823253482580185}, {"id": 181, "seek": 70556, "start": 717.64, "end": 723.64, "text": " And returns the key value at the one not at zero but at one which is the count", "tokens": [50968, 400, 11247, 264, 2141, 2158, 412, 264, 472, 406, 412, 4018, 457, 412, 472, 597, 307, 264, 1207, 51268], "temperature": 0.0, "avg_logprob": -0.10279950990781679, "compression_ratio": 1.9127906976744187, "no_speech_prob": 0.0021823253482580185}, {"id": 182, "seek": 70556, "start": 723.9599999999999, "end": 725.9599999999999, "text": " So we want to sort by the count", "tokens": [51284, 407, 321, 528, 281, 1333, 538, 264, 1207, 51384], "temperature": 0.0, "avg_logprob": -0.10279950990781679, "compression_ratio": 1.9127906976744187, "no_speech_prob": 0.0021823253482580185}, {"id": 183, "seek": 70556, "start": 727.16, "end": 729.16, "text": " Of these elements", "tokens": [51444, 2720, 613, 4959, 51544], "temperature": 0.0, "avg_logprob": -0.10279950990781679, "compression_ratio": 1.9127906976744187, "no_speech_prob": 0.0021823253482580185}, {"id": 184, "seek": 70556, "start": 730.4399999999999, "end": 732.4399999999999, "text": " And actually we want it to go backwards", "tokens": [51608, 400, 767, 321, 528, 309, 281, 352, 12204, 51708], "temperature": 0.0, "avg_logprob": -0.10279950990781679, "compression_ratio": 1.9127906976744187, "no_speech_prob": 0.0021823253482580185}, {"id": 185, "seek": 73244, "start": 732.6800000000001, "end": 737.5600000000001, "text": " So here what we have is the bygram q and r occurs only a single time", "tokens": [50376, 407, 510, 437, 321, 362, 307, 264, 538, 1342, 9505, 293, 367, 11843, 787, 257, 2167, 565, 50620], "temperature": 0.0, "avg_logprob": -0.12568229368363304, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.003482674015685916}, {"id": 186, "seek": 73244, "start": 738.5200000000001, "end": 742.2, "text": " dz occurred only a single time and when we sort this the other way around", "tokens": [50668, 274, 89, 11068, 787, 257, 2167, 565, 293, 562, 321, 1333, 341, 264, 661, 636, 926, 50852], "temperature": 0.0, "avg_logprob": -0.12568229368363304, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.003482674015685916}, {"id": 187, "seek": 73244, "start": 743.4000000000001, "end": 745.8800000000001, "text": " We're going to see the most likely bygrams", "tokens": [50912, 492, 434, 516, 281, 536, 264, 881, 3700, 538, 1342, 82, 51036], "temperature": 0.0, "avg_logprob": -0.12568229368363304, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.003482674015685916}, {"id": 188, "seek": 73244, "start": 746.36, "end": 753.8800000000001, "text": " So we see that n was very often an ending character many many times and apparently n almost always follows an a", "tokens": [51060, 407, 321, 536, 300, 297, 390, 588, 2049, 364, 8121, 2517, 867, 867, 1413, 293, 7970, 297, 1920, 1009, 10002, 364, 257, 51436], "temperature": 0.0, "avg_logprob": -0.12568229368363304, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.003482674015685916}, {"id": 189, "seek": 73244, "start": 754.44, "end": 756.44, "text": " And that's a very likely combination as well", "tokens": [51464, 400, 300, 311, 257, 588, 3700, 6562, 382, 731, 51564], "temperature": 0.0, "avg_logprob": -0.12568229368363304, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.003482674015685916}, {"id": 190, "seek": 75644, "start": 757.1600000000001, "end": 758.7600000000001, "text": " Um", "tokens": [50400, 3301, 50480], "temperature": 0.0, "avg_logprob": -0.12786375616014617, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.0056409393437206745}, {"id": 191, "seek": 75644, "start": 758.7600000000001, "end": 759.96, "text": " So", "tokens": [50480, 407, 50540], "temperature": 0.0, "avg_logprob": -0.12786375616014617, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.0056409393437206745}, {"id": 192, "seek": 75644, "start": 759.96, "end": 763.6400000000001, "text": " This is kind of the individual counts that we achieve over the entire data set", "tokens": [50540, 639, 307, 733, 295, 264, 2609, 14893, 300, 321, 4584, 670, 264, 2302, 1412, 992, 50724], "temperature": 0.0, "avg_logprob": -0.12786375616014617, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.0056409393437206745}, {"id": 193, "seek": 75644, "start": 765.0, "end": 767.8800000000001, "text": " Now it's actually going to be significantly more convenient for us to", "tokens": [50792, 823, 309, 311, 767, 516, 281, 312, 10591, 544, 10851, 337, 505, 281, 50936], "temperature": 0.0, "avg_logprob": -0.12786375616014617, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.0056409393437206745}, {"id": 194, "seek": 75644, "start": 768.44, "end": 772.7600000000001, "text": " Keep this information in a two-dimensional array instead of a python dictionary", "tokens": [50964, 5527, 341, 1589, 294, 257, 732, 12, 18759, 10225, 2602, 295, 257, 38797, 25890, 51180], "temperature": 0.0, "avg_logprob": -0.12786375616014617, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.0056409393437206745}, {"id": 195, "seek": 75644, "start": 773.72, "end": 778.84, "text": " so we're going to store this information in a 2d array and", "tokens": [51228, 370, 321, 434, 516, 281, 3531, 341, 1589, 294, 257, 568, 67, 10225, 293, 51484], "temperature": 0.0, "avg_logprob": -0.12786375616014617, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.0056409393437206745}, {"id": 196, "seek": 75644, "start": 780.12, "end": 784.7600000000001, "text": " The rows are going to be the first character of the bygram and the columns are going to be the second character", "tokens": [51548, 440, 13241, 366, 516, 281, 312, 264, 700, 2517, 295, 264, 538, 1342, 293, 264, 13766, 366, 516, 281, 312, 264, 1150, 2517, 51780], "temperature": 0.0, "avg_logprob": -0.12786375616014617, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.0056409393437206745}, {"id": 197, "seek": 78476, "start": 785.08, "end": 791.96, "text": " And each entry in the two-dimensional array will tell us how often that first character follows the second character in the data set", "tokens": [50380, 400, 1184, 8729, 294, 264, 732, 12, 18759, 10225, 486, 980, 505, 577, 2049, 300, 700, 2517, 10002, 264, 1150, 2517, 294, 264, 1412, 992, 50724], "temperature": 0.0, "avg_logprob": -0.1224999341097745, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0019265066366642714}, {"id": 198, "seek": 78476, "start": 792.68, "end": 798.28, "text": " So in particular the array representation that we're going to use or the library is that of pytorch", "tokens": [50760, 407, 294, 1729, 264, 10225, 10290, 300, 321, 434, 516, 281, 764, 420, 264, 6405, 307, 300, 295, 25878, 284, 339, 51040], "temperature": 0.0, "avg_logprob": -0.1224999341097745, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0019265066366642714}, {"id": 199, "seek": 78476, "start": 798.84, "end": 801.8, "text": " And pytorch is a deep learning neural network framework", "tokens": [51068, 400, 25878, 284, 339, 307, 257, 2452, 2539, 18161, 3209, 8388, 51216], "temperature": 0.0, "avg_logprob": -0.1224999341097745, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0019265066366642714}, {"id": 200, "seek": 78476, "start": 802.28, "end": 808.9399999999999, "text": " But part of it is also this torch dot tensor, uh, which allows us to create multi-dimensional arrays and manipulate them very efficiently", "tokens": [51240, 583, 644, 295, 309, 307, 611, 341, 27822, 5893, 40863, 11, 2232, 11, 597, 4045, 505, 281, 1884, 4825, 12, 18759, 41011, 293, 20459, 552, 588, 19621, 51573], "temperature": 0.0, "avg_logprob": -0.1224999341097745, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0019265066366642714}, {"id": 201, "seek": 78476, "start": 809.88, "end": 813.4, "text": " So let's import pytorch, which you can do by import torch", "tokens": [51620, 407, 718, 311, 974, 25878, 284, 339, 11, 597, 291, 393, 360, 538, 974, 27822, 51796], "temperature": 0.0, "avg_logprob": -0.1224999341097745, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0019265066366642714}, {"id": 202, "seek": 81476, "start": 814.76, "end": 816.76, "text": " And then we can create arrays", "tokens": [50364, 400, 550, 321, 393, 1884, 41011, 50464], "temperature": 0.0, "avg_logprob": -0.13547240363226998, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.0012842047726735473}, {"id": 203, "seek": 81476, "start": 817.56, "end": 819.8, "text": " So let's create an array of zeros", "tokens": [50504, 407, 718, 311, 1884, 364, 10225, 295, 35193, 50616], "temperature": 0.0, "avg_logprob": -0.13547240363226998, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.0012842047726735473}, {"id": 204, "seek": 81476, "start": 820.68, "end": 826.2, "text": " And we give it a um size of this array. Let's create a three by five array as an example", "tokens": [50660, 400, 321, 976, 309, 257, 1105, 2744, 295, 341, 10225, 13, 961, 311, 1884, 257, 1045, 538, 1732, 10225, 382, 364, 1365, 50936], "temperature": 0.0, "avg_logprob": -0.13547240363226998, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.0012842047726735473}, {"id": 205, "seek": 81476, "start": 827.08, "end": 828.6, "text": " and", "tokens": [50980, 293, 51056], "temperature": 0.0, "avg_logprob": -0.13547240363226998, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.0012842047726735473}, {"id": 206, "seek": 81476, "start": 828.6, "end": 830.6, "text": " This is a three by five array of zeros", "tokens": [51056, 639, 307, 257, 1045, 538, 1732, 10225, 295, 35193, 51156], "temperature": 0.0, "avg_logprob": -0.13547240363226998, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.0012842047726735473}, {"id": 207, "seek": 81476, "start": 831.56, "end": 836.4399999999999, "text": " And by default you'll notice a dot d type, which is short for data type is float 32", "tokens": [51204, 400, 538, 7576, 291, 603, 3449, 257, 5893, 274, 2010, 11, 597, 307, 2099, 337, 1412, 2010, 307, 15706, 8858, 51448], "temperature": 0.0, "avg_logprob": -0.13547240363226998, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.0012842047726735473}, {"id": 208, "seek": 81476, "start": 836.6, "end": 838.84, "text": " So these are single precision floating point numbers", "tokens": [51456, 407, 613, 366, 2167, 18356, 12607, 935, 3547, 51568], "temperature": 0.0, "avg_logprob": -0.13547240363226998, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.0012842047726735473}, {"id": 209, "seek": 83884, "start": 839.48, "end": 845.0, "text": " Because we are going to represent counts. Let's actually use d type as torch dot in 32", "tokens": [50396, 1436, 321, 366, 516, 281, 2906, 14893, 13, 961, 311, 767, 764, 274, 2010, 382, 27822, 5893, 294, 8858, 50672], "temperature": 0.0, "avg_logprob": -0.1283164121666733, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.004198435693979263}, {"id": 210, "seek": 83884, "start": 846.0400000000001, "end": 847.88, "text": " So these are", "tokens": [50724, 407, 613, 366, 50816], "temperature": 0.0, "avg_logprob": -0.1283164121666733, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.004198435693979263}, {"id": 211, "seek": 83884, "start": 847.88, "end": 849.88, "text": " 32 bit integers", "tokens": [50816, 8858, 857, 41674, 50916], "temperature": 0.0, "avg_logprob": -0.1283164121666733, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.004198435693979263}, {"id": 212, "seek": 83884, "start": 850.12, "end": 853.64, "text": " So now you see that we have integer data inside this tensor", "tokens": [50928, 407, 586, 291, 536, 300, 321, 362, 24922, 1412, 1854, 341, 40863, 51104], "temperature": 0.0, "avg_logprob": -0.1283164121666733, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.004198435693979263}, {"id": 213, "seek": 83884, "start": 854.6, "end": 856.6, "text": " Now tensors allow us to really, um", "tokens": [51152, 823, 10688, 830, 2089, 505, 281, 534, 11, 1105, 51252], "temperature": 0.0, "avg_logprob": -0.1283164121666733, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.004198435693979263}, {"id": 214, "seek": 83884, "start": 857.4, "end": 860.12, "text": " Manipulate all the individual entries and do it very efficiently", "tokens": [51292, 2458, 647, 5256, 439, 264, 2609, 23041, 293, 360, 309, 588, 19621, 51428], "temperature": 0.0, "avg_logprob": -0.1283164121666733, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.004198435693979263}, {"id": 215, "seek": 83884, "start": 860.76, "end": 863.1600000000001, "text": " So for example, if we want to change this bit", "tokens": [51460, 407, 337, 1365, 11, 498, 321, 528, 281, 1319, 341, 857, 51580], "temperature": 0.0, "avg_logprob": -0.1283164121666733, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.004198435693979263}, {"id": 216, "seek": 83884, "start": 863.72, "end": 867.24, "text": " We have to index into the tensor and in particular here", "tokens": [51608, 492, 362, 281, 8186, 666, 264, 40863, 293, 294, 1729, 510, 51784], "temperature": 0.0, "avg_logprob": -0.1283164121666733, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.004198435693979263}, {"id": 217, "seek": 86724, "start": 867.64, "end": 870.12, "text": " This is the first row and the", "tokens": [50384, 639, 307, 264, 700, 5386, 293, 264, 50508], "temperature": 0.0, "avg_logprob": -0.11995112166112783, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.007576156873255968}, {"id": 218, "seek": 86724, "start": 870.84, "end": 878.12, "text": " Um, because it's zero indexed. So this is row index one and column index zero one two three", "tokens": [50544, 3301, 11, 570, 309, 311, 4018, 8186, 292, 13, 407, 341, 307, 5386, 8186, 472, 293, 7738, 8186, 4018, 472, 732, 1045, 50908], "temperature": 0.0, "avg_logprob": -0.11995112166112783, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.007576156873255968}, {"id": 219, "seek": 86724, "start": 878.84, "end": 882.36, "text": " So a at one comma three we can set that to one", "tokens": [50944, 407, 257, 412, 472, 22117, 1045, 321, 393, 992, 300, 281, 472, 51120], "temperature": 0.0, "avg_logprob": -0.11995112166112783, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.007576156873255968}, {"id": 220, "seek": 86724, "start": 883.72, "end": 885.72, "text": " And then a we'll have a one over there", "tokens": [51188, 400, 550, 257, 321, 603, 362, 257, 472, 670, 456, 51288], "temperature": 0.0, "avg_logprob": -0.11995112166112783, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.007576156873255968}, {"id": 221, "seek": 86724, "start": 887.16, "end": 891.48, "text": " We can of course also do things like this. So now a will be two over there", "tokens": [51360, 492, 393, 295, 1164, 611, 360, 721, 411, 341, 13, 407, 586, 257, 486, 312, 732, 670, 456, 51576], "temperature": 0.0, "avg_logprob": -0.11995112166112783, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.007576156873255968}, {"id": 222, "seek": 86724, "start": 892.6, "end": 893.88, "text": " Or three", "tokens": [51632, 1610, 1045, 51696], "temperature": 0.0, "avg_logprob": -0.11995112166112783, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.007576156873255968}, {"id": 223, "seek": 86724, "start": 893.88, "end": 896.44, "text": " And also we can for example say a zero zero is five", "tokens": [51696, 400, 611, 321, 393, 337, 1365, 584, 257, 4018, 4018, 307, 1732, 51824], "temperature": 0.0, "avg_logprob": -0.11995112166112783, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.007576156873255968}, {"id": 224, "seek": 89724, "start": 897.32, "end": 899.48, "text": " And then a we'll have a five over here", "tokens": [50368, 400, 550, 257, 321, 603, 362, 257, 1732, 670, 510, 50476], "temperature": 0.0, "avg_logprob": -0.09612745898110527, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.0009398676338605583}, {"id": 225, "seek": 89724, "start": 900.2, "end": 902.76, "text": " So that's how we can index into the arrays", "tokens": [50512, 407, 300, 311, 577, 321, 393, 8186, 666, 264, 41011, 50640], "temperature": 0.0, "avg_logprob": -0.09612745898110527, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.0009398676338605583}, {"id": 226, "seek": 89724, "start": 903.32, "end": 905.8, "text": " Now, of course the array that we are interested in is much much bigger", "tokens": [50668, 823, 11, 295, 1164, 264, 10225, 300, 321, 366, 3102, 294, 307, 709, 709, 3801, 50792], "temperature": 0.0, "avg_logprob": -0.09612745898110527, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.0009398676338605583}, {"id": 227, "seek": 89724, "start": 906.28, "end": 909.4, "text": " So for our purposes, we have 26 letters of the alphabet", "tokens": [50816, 407, 337, 527, 9932, 11, 321, 362, 7551, 7825, 295, 264, 23339, 50972], "temperature": 0.0, "avg_logprob": -0.09612745898110527, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.0009398676338605583}, {"id": 228, "seek": 89724, "start": 909.96, "end": 913.32, "text": " And then we have two special characters s and e", "tokens": [51000, 400, 550, 321, 362, 732, 2121, 4342, 262, 293, 308, 51168], "temperature": 0.0, "avg_logprob": -0.09612745898110527, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.0009398676338605583}, {"id": 229, "seek": 89724, "start": 914.12, "end": 918.6, "text": " So, uh, we want 26 plus two or 28 by 28 array", "tokens": [51208, 407, 11, 2232, 11, 321, 528, 7551, 1804, 732, 420, 7562, 538, 7562, 10225, 51432], "temperature": 0.0, "avg_logprob": -0.09612745898110527, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.0009398676338605583}, {"id": 230, "seek": 89724, "start": 919.32, "end": 923.0, "text": " And let's call it the capital n because it's going to represent sort of the counts", "tokens": [51468, 400, 718, 311, 818, 309, 264, 4238, 297, 570, 309, 311, 516, 281, 2906, 1333, 295, 264, 14893, 51652], "temperature": 0.0, "avg_logprob": -0.09612745898110527, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.0009398676338605583}, {"id": 231, "seek": 89724, "start": 924.52, "end": 926.52, "text": " Let me erase this stuff", "tokens": [51728, 961, 385, 23525, 341, 1507, 51828], "temperature": 0.0, "avg_logprob": -0.09612745898110527, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.0009398676338605583}, {"id": 232, "seek": 92652, "start": 926.76, "end": 929.72, "text": " So that's the array that starts at zeros 28 by 28", "tokens": [50376, 407, 300, 311, 264, 10225, 300, 3719, 412, 35193, 7562, 538, 7562, 50524], "temperature": 0.0, "avg_logprob": -0.10928856500304572, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.00025713694049045444}, {"id": 233, "seek": 92652, "start": 930.4399999999999, "end": 932.4399999999999, "text": " And now let's copy paste this", "tokens": [50560, 400, 586, 718, 311, 5055, 9163, 341, 50660], "temperature": 0.0, "avg_logprob": -0.10928856500304572, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.00025713694049045444}, {"id": 234, "seek": 92652, "start": 933.72, "end": 936.12, "text": " Here but instead of having a dictionary b", "tokens": [50724, 1692, 457, 2602, 295, 1419, 257, 25890, 272, 50844], "temperature": 0.0, "avg_logprob": -0.10928856500304572, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.00025713694049045444}, {"id": 235, "seek": 92652, "start": 937.0, "end": 939.56, "text": " Which we're going to erase we now have an n", "tokens": [50888, 3013, 321, 434, 516, 281, 23525, 321, 586, 362, 364, 297, 51016], "temperature": 0.0, "avg_logprob": -0.10928856500304572, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.00025713694049045444}, {"id": 236, "seek": 92652, "start": 941.0, "end": 944.92, "text": " Now the problem here is that we have these characters which are strings, but we have to now", "tokens": [51088, 823, 264, 1154, 510, 307, 300, 321, 362, 613, 4342, 597, 366, 13985, 11, 457, 321, 362, 281, 586, 51284], "temperature": 0.0, "avg_logprob": -0.10928856500304572, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.00025713694049045444}, {"id": 237, "seek": 92652, "start": 945.88, "end": 947.88, "text": " basically index into a", "tokens": [51332, 1936, 8186, 666, 257, 51432], "temperature": 0.0, "avg_logprob": -0.10928856500304572, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.00025713694049045444}, {"id": 238, "seek": 92652, "start": 948.76, "end": 954.4399999999999, "text": " Array and we have to index using integers. So we need some kind of a lookup table from characters to integers", "tokens": [51476, 1587, 3458, 293, 321, 362, 281, 8186, 1228, 41674, 13, 407, 321, 643, 512, 733, 295, 257, 574, 1010, 3199, 490, 4342, 281, 41674, 51760], "temperature": 0.0, "avg_logprob": -0.10928856500304572, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.00025713694049045444}, {"id": 239, "seek": 95444, "start": 955.32, "end": 957.32, "text": " So let's construct such a character array", "tokens": [50408, 407, 718, 311, 7690, 1270, 257, 2517, 10225, 50508], "temperature": 0.0, "avg_logprob": -0.07693482274594514, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.0019265443552285433}, {"id": 240, "seek": 95444, "start": 958.0400000000001, "end": 962.12, "text": " And the way we're going to do this is we're going to take all the words which is a list of strings", "tokens": [50544, 400, 264, 636, 321, 434, 516, 281, 360, 341, 307, 321, 434, 516, 281, 747, 439, 264, 2283, 597, 307, 257, 1329, 295, 13985, 50748], "temperature": 0.0, "avg_logprob": -0.07693482274594514, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.0019265443552285433}, {"id": 241, "seek": 95444, "start": 962.84, "end": 968.12, "text": " We're going to concatenate all of it into a massive string. So this is just simply the entire dataset as a single string", "tokens": [50784, 492, 434, 516, 281, 1588, 7186, 473, 439, 295, 309, 666, 257, 5994, 6798, 13, 407, 341, 307, 445, 2935, 264, 2302, 28872, 382, 257, 2167, 6798, 51048], "temperature": 0.0, "avg_logprob": -0.07693482274594514, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.0019265443552285433}, {"id": 242, "seek": 95444, "start": 969.32, "end": 973.96, "text": " We're going to pass this to the set constructor which takes this massive string", "tokens": [51108, 492, 434, 516, 281, 1320, 341, 281, 264, 992, 47479, 597, 2516, 341, 5994, 6798, 51340], "temperature": 0.0, "avg_logprob": -0.07693482274594514, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.0019265443552285433}, {"id": 243, "seek": 95444, "start": 974.44, "end": 978.2800000000001, "text": " And throws out duplicates because sets do not allow duplicates", "tokens": [51364, 400, 19251, 484, 17154, 1024, 570, 6352, 360, 406, 2089, 17154, 1024, 51556], "temperature": 0.0, "avg_logprob": -0.07693482274594514, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.0019265443552285433}, {"id": 244, "seek": 95444, "start": 978.9200000000001, "end": 982.84, "text": " So set of this will just be the set of all the lowercase characters", "tokens": [51588, 407, 992, 295, 341, 486, 445, 312, 264, 992, 295, 439, 264, 3126, 9765, 4342, 51784], "temperature": 0.0, "avg_logprob": -0.07693482274594514, "compression_ratio": 1.9666666666666666, "no_speech_prob": 0.0019265443552285433}, {"id": 245, "seek": 98284, "start": 983.08, "end": 986.2800000000001, "text": " And there should be a total of 26 of them", "tokens": [50376, 400, 456, 820, 312, 257, 3217, 295, 7551, 295, 552, 50536], "temperature": 0.0, "avg_logprob": -0.1161404442541378, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0010161862010136247}, {"id": 246, "seek": 98284, "start": 988.6800000000001, "end": 990.6800000000001, "text": " And now we actually don't want a set we want a list", "tokens": [50656, 400, 586, 321, 767, 500, 380, 528, 257, 992, 321, 528, 257, 1329, 50756], "temperature": 0.0, "avg_logprob": -0.1161404442541378, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0010161862010136247}, {"id": 247, "seek": 98284, "start": 992.6800000000001, "end": 996.6800000000001, "text": " But we don't want a list sorted in some weird arbitrary way. We want it to be sorted", "tokens": [50856, 583, 321, 500, 380, 528, 257, 1329, 25462, 294, 512, 3657, 23211, 636, 13, 492, 528, 309, 281, 312, 25462, 51056], "temperature": 0.0, "avg_logprob": -0.1161404442541378, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0010161862010136247}, {"id": 248, "seek": 98284, "start": 997.64, "end": 999.64, "text": " from a to z", "tokens": [51104, 490, 257, 281, 710, 51204], "temperature": 0.0, "avg_logprob": -0.1161404442541378, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0010161862010136247}, {"id": 249, "seek": 98284, "start": 999.88, "end": 1001.88, "text": " So sorted list", "tokens": [51216, 407, 25462, 1329, 51316], "temperature": 0.0, "avg_logprob": -0.1161404442541378, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0010161862010136247}, {"id": 250, "seek": 98284, "start": 1001.88, "end": 1003.88, "text": " So those are our characters", "tokens": [51316, 407, 729, 366, 527, 4342, 51416], "temperature": 0.0, "avg_logprob": -0.1161404442541378, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0010161862010136247}, {"id": 251, "seek": 98284, "start": 1005.64, "end": 1011.88, "text": " Now what we want is this lookup table as I mentioned. So let's create a special s2i. I will call it", "tokens": [51504, 823, 437, 321, 528, 307, 341, 574, 1010, 3199, 382, 286, 2835, 13, 407, 718, 311, 1884, 257, 2121, 262, 17, 72, 13, 286, 486, 818, 309, 51816], "temperature": 0.0, "avg_logprob": -0.1161404442541378, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0010161862010136247}, {"id": 252, "seek": 101284, "start": 1013.48, "end": 1017.48, "text": " s is string or character and this will be an s2i mapping", "tokens": [50396, 262, 307, 6798, 420, 2517, 293, 341, 486, 312, 364, 262, 17, 72, 18350, 50596], "temperature": 0.0, "avg_logprob": -0.1640268053327288, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0016227512387558818}, {"id": 253, "seek": 101284, "start": 1018.84, "end": 1020.2, "text": " for", "tokens": [50664, 337, 50732], "temperature": 0.0, "avg_logprob": -0.1640268053327288, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0016227512387558818}, {"id": 254, "seek": 101284, "start": 1020.2, "end": 1023.08, "text": " Is in enumerate of these characters", "tokens": [50732, 1119, 294, 465, 15583, 473, 295, 613, 4342, 50876], "temperature": 0.0, "avg_logprob": -0.1640268053327288, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0016227512387558818}, {"id": 255, "seek": 101284, "start": 1024.3600000000001, "end": 1028.28, "text": " So enumerate basically gives us this iterator over the integer", "tokens": [50940, 407, 465, 15583, 473, 1936, 2709, 505, 341, 17138, 1639, 670, 264, 24922, 51136], "temperature": 0.0, "avg_logprob": -0.1640268053327288, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0016227512387558818}, {"id": 256, "seek": 101284, "start": 1028.76, "end": 1034.3600000000001, "text": " index and the actual element of the list and then we are mapping the character to the integer", "tokens": [51160, 8186, 293, 264, 3539, 4478, 295, 264, 1329, 293, 550, 321, 366, 18350, 264, 2517, 281, 264, 24922, 51440], "temperature": 0.0, "avg_logprob": -0.1640268053327288, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0016227512387558818}, {"id": 257, "seek": 101284, "start": 1035.24, "end": 1036.76, "text": " So s2i", "tokens": [51484, 407, 262, 17, 72, 51560], "temperature": 0.0, "avg_logprob": -0.1640268053327288, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0016227512387558818}, {"id": 258, "seek": 101284, "start": 1036.76, "end": 1041.56, "text": " Is a mapping from a to 0 b to 1 etc all the way from z to 25", "tokens": [51560, 1119, 257, 18350, 490, 257, 281, 1958, 272, 281, 502, 5183, 439, 264, 636, 490, 710, 281, 3552, 51800], "temperature": 0.0, "avg_logprob": -0.1640268053327288, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0016227512387558818}, {"id": 259, "seek": 104284, "start": 1043.8, "end": 1049.08, "text": " And that's going to be useful here, but we actually also have to specifically set that s will be 26", "tokens": [50412, 400, 300, 311, 516, 281, 312, 4420, 510, 11, 457, 321, 767, 611, 362, 281, 4682, 992, 300, 262, 486, 312, 7551, 50676], "temperature": 0.0, "avg_logprob": -0.13418173567156924, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.001727357623167336}, {"id": 260, "seek": 104284, "start": 1049.8, "end": 1051.8, "text": " And s2i at e", "tokens": [50712, 400, 262, 17, 72, 412, 308, 50812], "temperature": 0.0, "avg_logprob": -0.13418173567156924, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.001727357623167336}, {"id": 261, "seek": 104284, "start": 1052.12, "end": 1054.52, "text": " Will be 27 right because z was 25", "tokens": [50828, 3099, 312, 7634, 558, 570, 710, 390, 3552, 50948], "temperature": 0.0, "avg_logprob": -0.13418173567156924, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.001727357623167336}, {"id": 262, "seek": 104284, "start": 1056.04, "end": 1059.3999999999999, "text": " So those are the lookups and now we can come here and we can map", "tokens": [51024, 407, 729, 366, 264, 574, 7528, 293, 586, 321, 393, 808, 510, 293, 321, 393, 4471, 51192], "temperature": 0.0, "avg_logprob": -0.13418173567156924, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.001727357623167336}, {"id": 263, "seek": 104284, "start": 1059.9599999999998, "end": 1062.1999999999998, "text": " Both character 1 and character 2 to their integers", "tokens": [51220, 6767, 2517, 502, 293, 2517, 568, 281, 641, 41674, 51332], "temperature": 0.0, "avg_logprob": -0.13418173567156924, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.001727357623167336}, {"id": 264, "seek": 104284, "start": 1062.84, "end": 1064.84, "text": " So this will be s2i character 1", "tokens": [51364, 407, 341, 486, 312, 262, 17, 72, 2517, 502, 51464], "temperature": 0.0, "avg_logprob": -0.13418173567156924, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.001727357623167336}, {"id": 265, "seek": 104284, "start": 1065.32, "end": 1068.12, "text": " And ix2 will be s2i of character 2", "tokens": [51488, 400, 741, 87, 17, 486, 312, 262, 17, 72, 295, 2517, 568, 51628], "temperature": 0.0, "avg_logprob": -0.13418173567156924, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.001727357623167336}, {"id": 266, "seek": 104284, "start": 1069.48, "end": 1071.48, "text": " And now we should be able to", "tokens": [51696, 400, 586, 321, 820, 312, 1075, 281, 51796], "temperature": 0.0, "avg_logprob": -0.13418173567156924, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.001727357623167336}, {"id": 267, "seek": 107148, "start": 1072.1200000000001, "end": 1076.76, "text": " Do this line, but using our array. So n at ix1 ix2", "tokens": [50396, 1144, 341, 1622, 11, 457, 1228, 527, 10225, 13, 407, 297, 412, 741, 87, 16, 741, 87, 17, 50628], "temperature": 0.0, "avg_logprob": -0.16657399542537737, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.010167663916945457}, {"id": 268, "seek": 107148, "start": 1077.32, "end": 1082.1200000000001, "text": " This is the two-dimensional array indexing. I've shown you before and honestly just plus equals 1", "tokens": [50656, 639, 307, 264, 732, 12, 18759, 10225, 8186, 278, 13, 286, 600, 4898, 291, 949, 293, 6095, 445, 1804, 6915, 502, 50896], "temperature": 0.0, "avg_logprob": -0.16657399542537737, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.010167663916945457}, {"id": 269, "seek": 107148, "start": 1083.0, "end": 1085.0, "text": " Because everything starts at zero", "tokens": [50940, 1436, 1203, 3719, 412, 4018, 51040], "temperature": 0.0, "avg_logprob": -0.16657399542537737, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.010167663916945457}, {"id": 270, "seek": 107148, "start": 1086.2, "end": 1088.2, "text": " So this should work", "tokens": [51100, 407, 341, 820, 589, 51200], "temperature": 0.0, "avg_logprob": -0.16657399542537737, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.010167663916945457}, {"id": 271, "seek": 107148, "start": 1088.92, "end": 1092.2, "text": " And give us a large 28 by 28 array", "tokens": [51236, 400, 976, 505, 257, 2416, 7562, 538, 7562, 10225, 51400], "temperature": 0.0, "avg_logprob": -0.16657399542537737, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.010167663916945457}, {"id": 272, "seek": 107148, "start": 1093.08, "end": 1095.96, "text": " Of all these counts. So if we print n", "tokens": [51444, 2720, 439, 613, 14893, 13, 407, 498, 321, 4482, 297, 51588], "temperature": 0.0, "avg_logprob": -0.16657399542537737, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.010167663916945457}, {"id": 273, "seek": 109596, "start": 1096.92, "end": 1099.48, "text": " This is the array, but of course it looks ugly", "tokens": [50412, 639, 307, 264, 10225, 11, 457, 295, 1164, 309, 1542, 12246, 50540], "temperature": 0.0, "avg_logprob": -0.12931537628173828, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.01590299792587757}, {"id": 274, "seek": 109596, "start": 1099.8, "end": 1104.1200000000001, "text": " So let's erase this ugly mess and let's try to visualize it a bit more nicer", "tokens": [50556, 407, 718, 311, 23525, 341, 12246, 2082, 293, 718, 311, 853, 281, 23273, 309, 257, 857, 544, 22842, 50772], "temperature": 0.0, "avg_logprob": -0.12931537628173828, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.01590299792587757}, {"id": 275, "seek": 109596, "start": 1104.8400000000001, "end": 1107.88, "text": " So for that we're going to use a library called mathplotlib", "tokens": [50808, 407, 337, 300, 321, 434, 516, 281, 764, 257, 6405, 1219, 5221, 564, 310, 38270, 50960], "temperature": 0.0, "avg_logprob": -0.12931537628173828, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.01590299792587757}, {"id": 276, "seek": 109596, "start": 1108.8400000000001, "end": 1114.3600000000001, "text": " So mathplotlib allows us to create figures. So we can do things like pltim show of the count array", "tokens": [51008, 407, 5221, 564, 310, 38270, 4045, 505, 281, 1884, 9624, 13, 407, 321, 393, 360, 721, 411, 499, 83, 332, 855, 295, 264, 1207, 10225, 51284], "temperature": 0.0, "avg_logprob": -0.12931537628173828, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.01590299792587757}, {"id": 277, "seek": 109596, "start": 1116.1200000000001, "end": 1118.28, "text": " So this is the 20 by 28 array", "tokens": [51372, 407, 341, 307, 264, 945, 538, 7562, 10225, 51480], "temperature": 0.0, "avg_logprob": -0.12931537628173828, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.01590299792587757}, {"id": 278, "seek": 109596, "start": 1119.0, "end": 1123.24, "text": " And this is a structure, but even this I would say is still pretty ugly", "tokens": [51516, 400, 341, 307, 257, 3877, 11, 457, 754, 341, 286, 576, 584, 307, 920, 1238, 12246, 51728], "temperature": 0.0, "avg_logprob": -0.12931537628173828, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.01590299792587757}, {"id": 279, "seek": 112324, "start": 1123.88, "end": 1128.52, "text": " So we're going to try to create a much nicer visualization of it and I wrote a bunch of code for that", "tokens": [50396, 407, 321, 434, 516, 281, 853, 281, 1884, 257, 709, 22842, 25801, 295, 309, 293, 286, 4114, 257, 3840, 295, 3089, 337, 300, 50628], "temperature": 0.0, "avg_logprob": -0.13272114310945785, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.005219417158514261}, {"id": 280, "seek": 112324, "start": 1129.72, "end": 1131.72, "text": " The first thing we're going to need is", "tokens": [50688, 440, 700, 551, 321, 434, 516, 281, 643, 307, 50788], "temperature": 0.0, "avg_logprob": -0.13272114310945785, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.005219417158514261}, {"id": 281, "seek": 112324, "start": 1131.96, "end": 1133.88, "text": " We're going to need to invert", "tokens": [50800, 492, 434, 516, 281, 643, 281, 33966, 50896], "temperature": 0.0, "avg_logprob": -0.13272114310945785, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.005219417158514261}, {"id": 282, "seek": 112324, "start": 1133.88, "end": 1139.0, "text": " This array here this dictionary. So s2i is a mapping from s to i", "tokens": [50896, 639, 10225, 510, 341, 25890, 13, 407, 262, 17, 72, 307, 257, 18350, 490, 262, 281, 741, 51152], "temperature": 0.0, "avg_logprob": -0.13272114310945785, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.005219417158514261}, {"id": 283, "seek": 112324, "start": 1139.8, "end": 1142.84, "text": " And in i2s, we're going to reverse this dictionary", "tokens": [51192, 400, 294, 741, 17, 82, 11, 321, 434, 516, 281, 9943, 341, 25890, 51344], "temperature": 0.0, "avg_logprob": -0.13272114310945785, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.005219417158514261}, {"id": 284, "seek": 112324, "start": 1143.08, "end": 1145.96, "text": " So it rid of all the items and just reverse that array", "tokens": [51356, 407, 309, 3973, 295, 439, 264, 4754, 293, 445, 9943, 300, 10225, 51500], "temperature": 0.0, "avg_logprob": -0.13272114310945785, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.005219417158514261}, {"id": 285, "seek": 112324, "start": 1146.6, "end": 1151.64, "text": " So i2s maps inversely from 0 to a 1 to b etc", "tokens": [51532, 407, 741, 17, 82, 11317, 21378, 736, 490, 1958, 281, 257, 502, 281, 272, 5183, 51784], "temperature": 0.0, "avg_logprob": -0.13272114310945785, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.005219417158514261}, {"id": 286, "seek": 115164, "start": 1152.6000000000001, "end": 1154.2, "text": " So we'll need that", "tokens": [50412, 407, 321, 603, 643, 300, 50492], "temperature": 0.0, "avg_logprob": -0.1401483557197485, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.0019266613526269794}, {"id": 287, "seek": 115164, "start": 1154.2, "end": 1158.0400000000002, "text": " And then here's the code that I came up with to try to make this a little bit nicer", "tokens": [50492, 400, 550, 510, 311, 264, 3089, 300, 286, 1361, 493, 365, 281, 853, 281, 652, 341, 257, 707, 857, 22842, 50684], "temperature": 0.0, "avg_logprob": -0.1401483557197485, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.0019266613526269794}, {"id": 288, "seek": 115164, "start": 1160.44, "end": 1162.0400000000002, "text": " To create a figure", "tokens": [50804, 1407, 1884, 257, 2573, 50884], "temperature": 0.0, "avg_logprob": -0.1401483557197485, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.0019266613526269794}, {"id": 289, "seek": 115164, "start": 1162.0400000000002, "end": 1163.5600000000002, "text": " We plot n", "tokens": [50884, 492, 7542, 297, 50960], "temperature": 0.0, "avg_logprob": -0.1401483557197485, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.0019266613526269794}, {"id": 290, "seek": 115164, "start": 1164.3600000000001, "end": 1169.24, "text": " And then we do and then we visualize a bunch of things later. Let me just run it so you get a sense of what this is", "tokens": [51000, 400, 550, 321, 360, 293, 550, 321, 23273, 257, 3840, 295, 721, 1780, 13, 961, 385, 445, 1190, 309, 370, 291, 483, 257, 2020, 295, 437, 341, 307, 51244], "temperature": 0.0, "avg_logprob": -0.1401483557197485, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.0019266613526269794}, {"id": 291, "seek": 115164, "start": 1171.96, "end": 1174.2, "text": " Okay, so you see here that we have", "tokens": [51380, 1033, 11, 370, 291, 536, 510, 300, 321, 362, 51492], "temperature": 0.0, "avg_logprob": -0.1401483557197485, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.0019266613526269794}, {"id": 292, "seek": 115164, "start": 1175.3200000000002, "end": 1177.16, "text": " the array spaced out", "tokens": [51548, 264, 10225, 43766, 484, 51640], "temperature": 0.0, "avg_logprob": -0.1401483557197485, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.0019266613526269794}, {"id": 293, "seek": 117716, "start": 1177.24, "end": 1181.72, "text": " And every one of these is basically like b follows g zero times", "tokens": [50368, 400, 633, 472, 295, 613, 307, 1936, 411, 272, 10002, 290, 4018, 1413, 50592], "temperature": 0.0, "avg_logprob": -0.0943641766257908, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.003123333677649498}, {"id": 294, "seek": 117716, "start": 1182.3600000000001, "end": 1184.3600000000001, "text": " b follows h 41 times", "tokens": [50624, 272, 10002, 276, 18173, 1413, 50724], "temperature": 0.0, "avg_logprob": -0.0943641766257908, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.003123333677649498}, {"id": 295, "seek": 117716, "start": 1185.24, "end": 1187.24, "text": " So a follows j 175 times", "tokens": [50768, 407, 257, 10002, 361, 41165, 1413, 50868], "temperature": 0.0, "avg_logprob": -0.0943641766257908, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.003123333677649498}, {"id": 296, "seek": 117716, "start": 1187.96, "end": 1192.3600000000001, "text": " And so what you can see that i'm doing here is first i show that entire array", "tokens": [50904, 400, 370, 437, 291, 393, 536, 300, 741, 478, 884, 510, 307, 700, 741, 855, 300, 2302, 10225, 51124], "temperature": 0.0, "avg_logprob": -0.0943641766257908, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.003123333677649498}, {"id": 297, "seek": 117716, "start": 1192.92, "end": 1195.72, "text": " And then I iterate over all the individual little cells here", "tokens": [51152, 400, 550, 286, 44497, 670, 439, 264, 2609, 707, 5438, 510, 51292], "temperature": 0.0, "avg_logprob": -0.0943641766257908, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.003123333677649498}, {"id": 298, "seek": 117716, "start": 1196.76, "end": 1198.76, "text": " And I create a character string here", "tokens": [51344, 400, 286, 1884, 257, 2517, 6798, 510, 51444], "temperature": 0.0, "avg_logprob": -0.0943641766257908, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.003123333677649498}, {"id": 299, "seek": 117716, "start": 1199.3200000000002, "end": 1204.2, "text": " Which is the inverse mapping i2s of the integer i and the integer j", "tokens": [51472, 3013, 307, 264, 17340, 18350, 741, 17, 82, 295, 264, 24922, 741, 293, 264, 24922, 361, 51716], "temperature": 0.0, "avg_logprob": -0.0943641766257908, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.003123333677649498}, {"id": 300, "seek": 120420, "start": 1204.6000000000001, "end": 1207.48, "text": " So those are the bigrams in a character representation", "tokens": [50384, 407, 729, 366, 264, 955, 2356, 82, 294, 257, 2517, 10290, 50528], "temperature": 0.0, "avg_logprob": -0.1314395700843589, "compression_ratio": 1.6872427983539096, "no_speech_prob": 0.011685365810990334}, {"id": 301, "seek": 120420, "start": 1208.6000000000001, "end": 1215.16, "text": " And then I plot just the bigram text and then I plot the number of times that this bigram occurs", "tokens": [50584, 400, 550, 286, 7542, 445, 264, 955, 2356, 2487, 293, 550, 286, 7542, 264, 1230, 295, 1413, 300, 341, 955, 2356, 11843, 50912], "temperature": 0.0, "avg_logprob": -0.1314395700843589, "compression_ratio": 1.6872427983539096, "no_speech_prob": 0.011685365810990334}, {"id": 302, "seek": 120420, "start": 1216.04, "end": 1222.28, "text": " Now the reason that there's a dot item here is because when you index into these arrays, these are torch tensors", "tokens": [50956, 823, 264, 1778, 300, 456, 311, 257, 5893, 3174, 510, 307, 570, 562, 291, 8186, 666, 613, 41011, 11, 613, 366, 27822, 10688, 830, 51268], "temperature": 0.0, "avg_logprob": -0.1314395700843589, "compression_ratio": 1.6872427983539096, "no_speech_prob": 0.011685365810990334}, {"id": 303, "seek": 120420, "start": 1223.0, "end": 1225.32, "text": " You see that we still get a tensor back", "tokens": [51304, 509, 536, 300, 321, 920, 483, 257, 40863, 646, 51420], "temperature": 0.0, "avg_logprob": -0.1314395700843589, "compression_ratio": 1.6872427983539096, "no_speech_prob": 0.011685365810990334}, {"id": 304, "seek": 120420, "start": 1226.04, "end": 1231.0800000000002, "text": " So the type of this thing you think it would be just an integer 149, but it's actually a torch dot tensor", "tokens": [51456, 407, 264, 2010, 295, 341, 551, 291, 519, 309, 576, 312, 445, 364, 24922, 3499, 24, 11, 457, 309, 311, 767, 257, 27822, 5893, 40863, 51708], "temperature": 0.0, "avg_logprob": -0.1314395700843589, "compression_ratio": 1.6872427983539096, "no_speech_prob": 0.011685365810990334}, {"id": 305, "seek": 123108, "start": 1231.96, "end": 1237.08, "text": " And so if you do dot item, then it will pop out that individual integer", "tokens": [50408, 400, 370, 498, 291, 360, 5893, 3174, 11, 550, 309, 486, 1665, 484, 300, 2609, 24922, 50664], "temperature": 0.0, "avg_logprob": -0.07750435410258925, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0019569506403058767}, {"id": 306, "seek": 123108, "start": 1238.4399999999998, "end": 1240.4399999999998, "text": " So it will just be 149", "tokens": [50732, 407, 309, 486, 445, 312, 3499, 24, 50832], "temperature": 0.0, "avg_logprob": -0.07750435410258925, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0019569506403058767}, {"id": 307, "seek": 123108, "start": 1240.6799999999998, "end": 1244.28, "text": " So that's what's happening there. And these are just some options to make it look nice", "tokens": [50844, 407, 300, 311, 437, 311, 2737, 456, 13, 400, 613, 366, 445, 512, 3956, 281, 652, 309, 574, 1481, 51024], "temperature": 0.0, "avg_logprob": -0.07750435410258925, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0019569506403058767}, {"id": 308, "seek": 123108, "start": 1245.24, "end": 1247.24, "text": " So what is the structure of this array?", "tokens": [51072, 407, 437, 307, 264, 3877, 295, 341, 10225, 30, 51172], "temperature": 0.0, "avg_logprob": -0.07750435410258925, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0019569506403058767}, {"id": 309, "seek": 123108, "start": 1249.1599999999999, "end": 1253.32, "text": " We have all these counts and we see that some of them occur often and some of them do not occur often", "tokens": [51268, 492, 362, 439, 613, 14893, 293, 321, 536, 300, 512, 295, 552, 5160, 2049, 293, 512, 295, 552, 360, 406, 5160, 2049, 51476], "temperature": 0.0, "avg_logprob": -0.07750435410258925, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0019569506403058767}, {"id": 310, "seek": 123108, "start": 1253.96, "end": 1257.8799999999999, "text": " Now if you scrutinize this carefully, you will notice that we're not actually being very clever", "tokens": [51508, 823, 498, 291, 28949, 259, 1125, 341, 7500, 11, 291, 486, 3449, 300, 321, 434, 406, 767, 885, 588, 13494, 51704], "temperature": 0.0, "avg_logprob": -0.07750435410258925, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0019569506403058767}, {"id": 311, "seek": 125788, "start": 1258.6000000000001, "end": 1260.6000000000001, "text": " That's because when you come over here", "tokens": [50400, 663, 311, 570, 562, 291, 808, 670, 510, 50500], "temperature": 0.0, "avg_logprob": -0.10710119993790336, "compression_ratio": 1.92578125, "no_speech_prob": 0.015661148354411125}, {"id": 312, "seek": 125788, "start": 1260.6000000000001, "end": 1264.1200000000001, "text": " You'll notice that for example, we have an entire row of completely zeros", "tokens": [50500, 509, 603, 3449, 300, 337, 1365, 11, 321, 362, 364, 2302, 5386, 295, 2584, 35193, 50676], "temperature": 0.0, "avg_logprob": -0.10710119993790336, "compression_ratio": 1.92578125, "no_speech_prob": 0.015661148354411125}, {"id": 313, "seek": 125788, "start": 1264.68, "end": 1266.68, "text": " And that's because the end character", "tokens": [50704, 400, 300, 311, 570, 264, 917, 2517, 50804], "temperature": 0.0, "avg_logprob": -0.10710119993790336, "compression_ratio": 1.92578125, "no_speech_prob": 0.015661148354411125}, {"id": 314, "seek": 125788, "start": 1267.0, "end": 1273.4, "text": " Is never possibly going to be the first character of a bigram because we're always placing these end tokens all at the end of a bigram", "tokens": [50820, 1119, 1128, 6264, 516, 281, 312, 264, 700, 2517, 295, 257, 955, 2356, 570, 321, 434, 1009, 17221, 613, 917, 22667, 439, 412, 264, 917, 295, 257, 955, 2356, 51140], "temperature": 0.0, "avg_logprob": -0.10710119993790336, "compression_ratio": 1.92578125, "no_speech_prob": 0.015661148354411125}, {"id": 315, "seek": 125788, "start": 1274.3600000000001, "end": 1278.5200000000002, "text": " Similarly, we have entire column zeros here because the s", "tokens": [51188, 13157, 11, 321, 362, 2302, 7738, 35193, 510, 570, 264, 262, 51396], "temperature": 0.0, "avg_logprob": -0.10710119993790336, "compression_ratio": 1.92578125, "no_speech_prob": 0.015661148354411125}, {"id": 316, "seek": 125788, "start": 1279.64, "end": 1287.0800000000002, "text": " Character will never possibly be the second element of a bigram because we always start with s and we end with e and we only have the words in between", "tokens": [51452, 36786, 486, 1128, 6264, 312, 264, 1150, 4478, 295, 257, 955, 2356, 570, 321, 1009, 722, 365, 262, 293, 321, 917, 365, 308, 293, 321, 787, 362, 264, 2283, 294, 1296, 51824], "temperature": 0.0, "avg_logprob": -0.10710119993790336, "compression_ratio": 1.92578125, "no_speech_prob": 0.015661148354411125}, {"id": 317, "seek": 128708, "start": 1287.6399999999999, "end": 1291.24, "text": " So we have an entire column of zeros an entire row of zeros", "tokens": [50392, 407, 321, 362, 364, 2302, 7738, 295, 35193, 364, 2302, 5386, 295, 35193, 50572], "temperature": 0.0, "avg_logprob": -0.06374091636843798, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034830868244171143}, {"id": 318, "seek": 128708, "start": 1291.72, "end": 1293.72, "text": " And in this little two by two matrix here as well", "tokens": [50596, 400, 294, 341, 707, 732, 538, 732, 8141, 510, 382, 731, 50696], "temperature": 0.0, "avg_logprob": -0.06374091636843798, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034830868244171143}, {"id": 319, "seek": 128708, "start": 1294.04, "end": 1297.8799999999999, "text": " The only one that can possibly happen is if s directly follows e", "tokens": [50712, 440, 787, 472, 300, 393, 6264, 1051, 307, 498, 262, 3838, 10002, 308, 50904], "temperature": 0.0, "avg_logprob": -0.06374091636843798, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034830868244171143}, {"id": 320, "seek": 128708, "start": 1298.6, "end": 1302.52, "text": " That can be non-zero if we have a word that has no letters", "tokens": [50940, 663, 393, 312, 2107, 12, 32226, 498, 321, 362, 257, 1349, 300, 575, 572, 7825, 51136], "temperature": 0.0, "avg_logprob": -0.06374091636843798, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034830868244171143}, {"id": 321, "seek": 128708, "start": 1303.08, "end": 1306.84, "text": " So in that case, there's no letters in the word. It's an empty word and we just have s follows e", "tokens": [51164, 407, 294, 300, 1389, 11, 456, 311, 572, 7825, 294, 264, 1349, 13, 467, 311, 364, 6707, 1349, 293, 321, 445, 362, 262, 10002, 308, 51352], "temperature": 0.0, "avg_logprob": -0.06374091636843798, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034830868244171143}, {"id": 322, "seek": 128708, "start": 1307.56, "end": 1309.56, "text": " But the other ones are just not possible", "tokens": [51388, 583, 264, 661, 2306, 366, 445, 406, 1944, 51488], "temperature": 0.0, "avg_logprob": -0.06374091636843798, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034830868244171143}, {"id": 323, "seek": 128708, "start": 1310.12, "end": 1315.0, "text": " And so we're basically wasting space and not only that but the s and the e are getting very crowded here", "tokens": [51516, 400, 370, 321, 434, 1936, 20457, 1901, 293, 406, 787, 300, 457, 264, 262, 293, 264, 308, 366, 1242, 588, 21634, 510, 51760], "temperature": 0.0, "avg_logprob": -0.06374091636843798, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.0034830868244171143}, {"id": 324, "seek": 131500, "start": 1315.56, "end": 1321.72, "text": " I was using these brackets because there's convention and natural language processing to use these kinds of brackets to denote special", "tokens": [50392, 286, 390, 1228, 613, 26179, 570, 456, 311, 10286, 293, 3303, 2856, 9007, 281, 764, 613, 3685, 295, 26179, 281, 45708, 2121, 50700], "temperature": 0.0, "avg_logprob": -0.07214571139134399, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.023682326078414917}, {"id": 325, "seek": 131500, "start": 1322.04, "end": 1323.24, "text": " tokens", "tokens": [50716, 22667, 50776], "temperature": 0.0, "avg_logprob": -0.07214571139134399, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.023682326078414917}, {"id": 326, "seek": 131500, "start": 1323.24, "end": 1325.24, "text": " But we're going to use something else", "tokens": [50776, 583, 321, 434, 516, 281, 764, 746, 1646, 50876], "temperature": 0.0, "avg_logprob": -0.07214571139134399, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.023682326078414917}, {"id": 327, "seek": 131500, "start": 1325.24, "end": 1327.48, "text": " So let's fix all this and make it prettier", "tokens": [50876, 407, 718, 311, 3191, 439, 341, 293, 652, 309, 36825, 50988], "temperature": 0.0, "avg_logprob": -0.07214571139134399, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.023682326078414917}, {"id": 328, "seek": 131500, "start": 1328.2, "end": 1332.2, "text": " We're not actually going to have two special tokens. We're only going to have one special token", "tokens": [51024, 492, 434, 406, 767, 516, 281, 362, 732, 2121, 22667, 13, 492, 434, 787, 516, 281, 362, 472, 2121, 14862, 51224], "temperature": 0.0, "avg_logprob": -0.07214571139134399, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.023682326078414917}, {"id": 329, "seek": 131500, "start": 1333.0, "end": 1337.56, "text": " So we're going to have n by n array of 27 by set 27 instead", "tokens": [51264, 407, 321, 434, 516, 281, 362, 297, 538, 297, 10225, 295, 7634, 538, 992, 7634, 2602, 51492], "temperature": 0.0, "avg_logprob": -0.07214571139134399, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.023682326078414917}, {"id": 330, "seek": 131500, "start": 1338.84, "end": 1342.92, "text": " Instead of having two we will just have one and I will call it a dot", "tokens": [51556, 7156, 295, 1419, 732, 321, 486, 445, 362, 472, 293, 286, 486, 818, 309, 257, 5893, 51760], "temperature": 0.0, "avg_logprob": -0.07214571139134399, "compression_ratio": 1.8702928870292888, "no_speech_prob": 0.023682326078414917}, {"id": 331, "seek": 134292, "start": 1343.64, "end": 1345.64, "text": " Okay", "tokens": [50400, 1033, 50500], "temperature": 0.0, "avg_logprob": -0.1300621144911822, "compression_ratio": 1.6386138613861385, "no_speech_prob": 0.004133573267608881}, {"id": 332, "seek": 134292, "start": 1347.3200000000002, "end": 1349.3200000000002, "text": " Let me swing this over here", "tokens": [50584, 961, 385, 11173, 341, 670, 510, 50684], "temperature": 0.0, "avg_logprob": -0.1300621144911822, "compression_ratio": 1.6386138613861385, "no_speech_prob": 0.004133573267608881}, {"id": 333, "seek": 134292, "start": 1350.44, "end": 1355.64, "text": " Now one more thing that I would like to do is I would actually like to make this special character half position zero", "tokens": [50740, 823, 472, 544, 551, 300, 286, 576, 411, 281, 360, 307, 286, 576, 767, 411, 281, 652, 341, 2121, 2517, 1922, 2535, 4018, 51000], "temperature": 0.0, "avg_logprob": -0.1300621144911822, "compression_ratio": 1.6386138613861385, "no_speech_prob": 0.004133573267608881}, {"id": 334, "seek": 134292, "start": 1356.2, "end": 1361.0, "text": " And I would like to offset all the other letters off. I find that a little bit more pleasing", "tokens": [51028, 400, 286, 576, 411, 281, 18687, 439, 264, 661, 7825, 766, 13, 286, 915, 300, 257, 707, 857, 544, 32798, 51268], "temperature": 0.0, "avg_logprob": -0.1300621144911822, "compression_ratio": 1.6386138613861385, "no_speech_prob": 0.004133573267608881}, {"id": 335, "seek": 134292, "start": 1361.64, "end": 1363.64, "text": " um, so", "tokens": [51300, 1105, 11, 370, 51400], "temperature": 0.0, "avg_logprob": -0.1300621144911822, "compression_ratio": 1.6386138613861385, "no_speech_prob": 0.004133573267608881}, {"id": 336, "seek": 134292, "start": 1364.68, "end": 1368.76, "text": " We need a plus one here so that the first character which is a will start at one", "tokens": [51452, 492, 643, 257, 1804, 472, 510, 370, 300, 264, 700, 2517, 597, 307, 257, 486, 722, 412, 472, 51656], "temperature": 0.0, "avg_logprob": -0.1300621144911822, "compression_ratio": 1.6386138613861385, "no_speech_prob": 0.004133573267608881}, {"id": 337, "seek": 136876, "start": 1368.76, "end": 1374.76, "text": " So s to i will now be a starts at one and dot is zero", "tokens": [50364, 407, 262, 281, 741, 486, 586, 312, 257, 3719, 412, 472, 293, 5893, 307, 4018, 50664], "temperature": 0.0, "avg_logprob": -0.1735818734329738, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0053845178335905075}, {"id": 338, "seek": 136876, "start": 1375.8799999999999, "end": 1381.8799999999999, "text": " And uh i2s, of course, we're not changing this because i2s just creates a reverse mapping and this will work fine", "tokens": [50720, 400, 2232, 741, 17, 82, 11, 295, 1164, 11, 321, 434, 406, 4473, 341, 570, 741, 17, 82, 445, 7829, 257, 9943, 18350, 293, 341, 486, 589, 2489, 51020], "temperature": 0.0, "avg_logprob": -0.1735818734329738, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0053845178335905075}, {"id": 339, "seek": 136876, "start": 1382.2, "end": 1385.0, "text": " So one is a two is b zero is dot", "tokens": [51036, 407, 472, 307, 257, 732, 307, 272, 4018, 307, 5893, 51176], "temperature": 0.0, "avg_logprob": -0.1735818734329738, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0053845178335905075}, {"id": 340, "seek": 136876, "start": 1386.52, "end": 1388.52, "text": " So we've reversed that here", "tokens": [51252, 407, 321, 600, 30563, 300, 510, 51352], "temperature": 0.0, "avg_logprob": -0.1735818734329738, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0053845178335905075}, {"id": 341, "seek": 136876, "start": 1389.08, "end": 1391.24, "text": " We have a dot and a dot", "tokens": [51380, 492, 362, 257, 5893, 293, 257, 5893, 51488], "temperature": 0.0, "avg_logprob": -0.1735818734329738, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0053845178335905075}, {"id": 342, "seek": 136876, "start": 1392.92, "end": 1395.8799999999999, "text": " This should work fine make sure I started zeros", "tokens": [51572, 639, 820, 589, 2489, 652, 988, 286, 1409, 35193, 51720], "temperature": 0.0, "avg_logprob": -0.1735818734329738, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0053845178335905075}, {"id": 343, "seek": 139588, "start": 1396.68, "end": 1404.3600000000001, "text": " Count and then here we don't go up to 28 we go up to 27 and this should just work", "tokens": [50404, 5247, 293, 550, 510, 321, 500, 380, 352, 493, 281, 7562, 321, 352, 493, 281, 7634, 293, 341, 820, 445, 589, 50788], "temperature": 0.0, "avg_logprob": -0.1793750392066108, "compression_ratio": 1.4770114942528736, "no_speech_prob": 0.004069674294441938}, {"id": 344, "seek": 139588, "start": 1410.8400000000001, "end": 1415.72, "text": " Okay, so we see that dot dot never happened. It's at zero because we don't have empty words", "tokens": [51112, 1033, 11, 370, 321, 536, 300, 5893, 5893, 1128, 2011, 13, 467, 311, 412, 4018, 570, 321, 500, 380, 362, 6707, 2283, 51356], "temperature": 0.0, "avg_logprob": -0.1793750392066108, "compression_ratio": 1.4770114942528736, "no_speech_prob": 0.004069674294441938}, {"id": 345, "seek": 139588, "start": 1416.5200000000002, "end": 1419.64, "text": " Then this row here now is just very simply the", "tokens": [51396, 1396, 341, 5386, 510, 586, 307, 445, 588, 2935, 264, 51552], "temperature": 0.0, "avg_logprob": -0.1793750392066108, "compression_ratio": 1.4770114942528736, "no_speech_prob": 0.004069674294441938}, {"id": 346, "seek": 139588, "start": 1420.7600000000002, "end": 1423.8000000000002, "text": " Counts for all the first letters. So", "tokens": [51608, 5247, 82, 337, 439, 264, 700, 7825, 13, 407, 51760], "temperature": 0.0, "avg_logprob": -0.1793750392066108, "compression_ratio": 1.4770114942528736, "no_speech_prob": 0.004069674294441938}, {"id": 347, "seek": 142380, "start": 1424.68, "end": 1429.08, "text": " G j starts a word h starts a word i starts a word etc", "tokens": [50408, 460, 361, 3719, 257, 1349, 276, 3719, 257, 1349, 741, 3719, 257, 1349, 5183, 50628], "temperature": 0.0, "avg_logprob": -0.20211556752522786, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.0004955178592354059}, {"id": 348, "seek": 142380, "start": 1429.48, "end": 1431.48, "text": " And then these are all the ending", "tokens": [50648, 400, 550, 613, 366, 439, 264, 8121, 50748], "temperature": 0.0, "avg_logprob": -0.20211556752522786, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.0004955178592354059}, {"id": 349, "seek": 142380, "start": 1431.8799999999999, "end": 1433.0, "text": " characters", "tokens": [50768, 4342, 50824], "temperature": 0.0, "avg_logprob": -0.20211556752522786, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.0004955178592354059}, {"id": 350, "seek": 142380, "start": 1433.0, "end": 1436.28, "text": " And in between we have the structure of what characters follow each other", "tokens": [50824, 400, 294, 1296, 321, 362, 264, 3877, 295, 437, 4342, 1524, 1184, 661, 50988], "temperature": 0.0, "avg_logprob": -0.20211556752522786, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.0004955178592354059}, {"id": 351, "seek": 142380, "start": 1437.0, "end": 1439.96, "text": " So this is the counts array of our entire", "tokens": [51024, 407, 341, 307, 264, 14893, 10225, 295, 527, 2302, 51172], "temperature": 0.0, "avg_logprob": -0.20211556752522786, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.0004955178592354059}, {"id": 352, "seek": 142380, "start": 1440.6, "end": 1441.6399999999999, "text": " Uh dataset", "tokens": [51204, 4019, 28872, 51256], "temperature": 0.0, "avg_logprob": -0.20211556752522786, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.0004955178592354059}, {"id": 353, "seek": 142380, "start": 1441.6399999999999, "end": 1447.0, "text": " So this array actually has all of the information necessary for us to actually sample from this bigram", "tokens": [51256, 407, 341, 10225, 767, 575, 439, 295, 264, 1589, 4818, 337, 505, 281, 767, 6889, 490, 341, 955, 2356, 51524], "temperature": 0.0, "avg_logprob": -0.20211556752522786, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.0004955178592354059}, {"id": 354, "seek": 142380, "start": 1447.56, "end": 1449.56, "text": " character level language model", "tokens": [51552, 2517, 1496, 2856, 2316, 51652], "temperature": 0.0, "avg_logprob": -0.20211556752522786, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.0004955178592354059}, {"id": 355, "seek": 142380, "start": 1449.72, "end": 1450.76, "text": " and", "tokens": [51660, 293, 51712], "temperature": 0.0, "avg_logprob": -0.20211556752522786, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.0004955178592354059}, {"id": 356, "seek": 145076, "start": 1450.84, "end": 1455.32, "text": " Roughly speaking what we're going to do is we're just going to start following these probabilities and these counts", "tokens": [50368, 42791, 356, 4124, 437, 321, 434, 516, 281, 360, 307, 321, 434, 445, 516, 281, 722, 3480, 613, 33783, 293, 613, 14893, 50592], "temperature": 0.0, "avg_logprob": -0.09471520671138058, "compression_ratio": 1.8167330677290836, "no_speech_prob": 0.0010483546648174524}, {"id": 357, "seek": 145076, "start": 1455.64, "end": 1458.2, "text": " And we're going to start sampling from the from model", "tokens": [50608, 400, 321, 434, 516, 281, 722, 21179, 490, 264, 490, 2316, 50736], "temperature": 0.0, "avg_logprob": -0.09471520671138058, "compression_ratio": 1.8167330677290836, "no_speech_prob": 0.0010483546648174524}, {"id": 358, "seek": 145076, "start": 1458.84, "end": 1462.92, "text": " So in the beginning, of course, um, we start with the dot the start token", "tokens": [50768, 407, 294, 264, 2863, 11, 295, 1164, 11, 1105, 11, 321, 722, 365, 264, 5893, 264, 722, 14862, 50972], "temperature": 0.0, "avg_logprob": -0.09471520671138058, "compression_ratio": 1.8167330677290836, "no_speech_prob": 0.0010483546648174524}, {"id": 359, "seek": 145076, "start": 1463.8, "end": 1469.4, "text": " Dot so to sample the first character of a name. We're looking at this row here", "tokens": [51016, 38753, 370, 281, 6889, 264, 700, 2517, 295, 257, 1315, 13, 492, 434, 1237, 412, 341, 5386, 510, 51296], "temperature": 0.0, "avg_logprob": -0.09471520671138058, "compression_ratio": 1.8167330677290836, "no_speech_prob": 0.0010483546648174524}, {"id": 360, "seek": 145076, "start": 1470.52, "end": 1478.6, "text": " So we see that we have the counts and those counts externally are telling us how often any one of these characters is to start a word", "tokens": [51352, 407, 321, 536, 300, 321, 362, 264, 14893, 293, 729, 14893, 40899, 366, 3585, 505, 577, 2049, 604, 472, 295, 613, 4342, 307, 281, 722, 257, 1349, 51756], "temperature": 0.0, "avg_logprob": -0.09471520671138058, "compression_ratio": 1.8167330677290836, "no_speech_prob": 0.0010483546648174524}, {"id": 361, "seek": 147860, "start": 1479.56, "end": 1483.6399999999999, "text": " So if we take this n and we grab the first row", "tokens": [50412, 407, 498, 321, 747, 341, 297, 293, 321, 4444, 264, 700, 5386, 50616], "temperature": 0.0, "avg_logprob": -0.16529442401642494, "compression_ratio": 1.6649746192893402, "no_speech_prob": 0.005384329240769148}, {"id": 362, "seek": 147860, "start": 1484.76, "end": 1486.1999999999998, "text": " We can do that by", "tokens": [50672, 492, 393, 360, 300, 538, 50744], "temperature": 0.0, "avg_logprob": -0.16529442401642494, "compression_ratio": 1.6649746192893402, "no_speech_prob": 0.005384329240769148}, {"id": 363, "seek": 147860, "start": 1486.1999999999998, "end": 1492.6799999999998, "text": " using just indexing a zero and then using this notation column for the rest of that row", "tokens": [50744, 1228, 445, 8186, 278, 257, 4018, 293, 550, 1228, 341, 24657, 7738, 337, 264, 1472, 295, 300, 5386, 51068], "temperature": 0.0, "avg_logprob": -0.16529442401642494, "compression_ratio": 1.6649746192893402, "no_speech_prob": 0.005384329240769148}, {"id": 364, "seek": 147860, "start": 1493.7199999999998, "end": 1495.7199999999998, "text": " so n zero column", "tokens": [51120, 370, 297, 4018, 7738, 51220], "temperature": 0.0, "avg_logprob": -0.16529442401642494, "compression_ratio": 1.6649746192893402, "no_speech_prob": 0.005384329240769148}, {"id": 365, "seek": 147860, "start": 1496.52, "end": 1498.52, "text": " Is indexing into the zero?", "tokens": [51260, 1119, 8186, 278, 666, 264, 4018, 30, 51360], "temperature": 0.0, "avg_logprob": -0.16529442401642494, "compression_ratio": 1.6649746192893402, "no_speech_prob": 0.005384329240769148}, {"id": 366, "seek": 147860, "start": 1498.84, "end": 1501.08, "text": " Row and then it's grabbing all the columns", "tokens": [51376, 20309, 293, 550, 309, 311, 23771, 439, 264, 13766, 51488], "temperature": 0.0, "avg_logprob": -0.16529442401642494, "compression_ratio": 1.6649746192893402, "no_speech_prob": 0.005384329240769148}, {"id": 367, "seek": 147860, "start": 1501.9599999999998, "end": 1504.52, "text": " And so this will give us a one-dimensional array", "tokens": [51532, 400, 370, 341, 486, 976, 505, 257, 472, 12, 18759, 10225, 51660], "temperature": 0.0, "avg_logprob": -0.16529442401642494, "compression_ratio": 1.6649746192893402, "no_speech_prob": 0.005384329240769148}, {"id": 368, "seek": 147860, "start": 1505.24, "end": 1507.6399999999999, "text": " Of the first row. So zero four four ten", "tokens": [51696, 2720, 264, 700, 5386, 13, 407, 4018, 1451, 1451, 2064, 51816], "temperature": 0.0, "avg_logprob": -0.16529442401642494, "compression_ratio": 1.6649746192893402, "no_speech_prob": 0.005384329240769148}, {"id": 369, "seek": 150764, "start": 1508.3600000000001, "end": 1512.0400000000002, "text": " You know zero four four ten one three oh six one five four two", "tokens": [50400, 509, 458, 4018, 1451, 1451, 2064, 472, 1045, 1954, 2309, 472, 1732, 1451, 732, 50584], "temperature": 0.0, "avg_logprob": -0.19023409256568322, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0020828156266361475}, {"id": 370, "seek": 150764, "start": 1512.5200000000002, "end": 1515.88, "text": " Etc. Just the first row the shape of this is", "tokens": [50608, 3790, 66, 13, 1449, 264, 700, 5386, 264, 3909, 295, 341, 307, 50776], "temperature": 0.0, "avg_logprob": -0.19023409256568322, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0020828156266361475}, {"id": 371, "seek": 150764, "start": 1516.5200000000002, "end": 1518.5200000000002, "text": " 27 it's just the row of 27", "tokens": [50808, 7634, 309, 311, 445, 264, 5386, 295, 7634, 50908], "temperature": 0.0, "avg_logprob": -0.19023409256568322, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0020828156266361475}, {"id": 372, "seek": 150764, "start": 1519.88, "end": 1523.16, "text": " And the other way that you can do this also is you just you don't actually give this", "tokens": [50976, 400, 264, 661, 636, 300, 291, 393, 360, 341, 611, 307, 291, 445, 291, 500, 380, 767, 976, 341, 51140], "temperature": 0.0, "avg_logprob": -0.19023409256568322, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0020828156266361475}, {"id": 373, "seek": 150764, "start": 1523.72, "end": 1526.92, "text": " You just grab the zero row like this. This is equal", "tokens": [51168, 509, 445, 4444, 264, 4018, 5386, 411, 341, 13, 639, 307, 2681, 51328], "temperature": 0.0, "avg_logprob": -0.19023409256568322, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0020828156266361475}, {"id": 374, "seek": 150764, "start": 1528.1200000000001, "end": 1529.96, "text": " Now these are the counts", "tokens": [51388, 823, 613, 366, 264, 14893, 51480], "temperature": 0.0, "avg_logprob": -0.19023409256568322, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0020828156266361475}, {"id": 375, "seek": 150764, "start": 1529.96, "end": 1534.2, "text": " And now what we'd like to do is we'd like to basically um sample from this", "tokens": [51480, 400, 586, 437, 321, 1116, 411, 281, 360, 307, 321, 1116, 411, 281, 1936, 1105, 6889, 490, 341, 51692], "temperature": 0.0, "avg_logprob": -0.19023409256568322, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0020828156266361475}, {"id": 376, "seek": 153420, "start": 1535.0, "end": 1538.1200000000001, "text": " Since these are the raw counts, we actually have to convert this to probabilities", "tokens": [50404, 4162, 613, 366, 264, 8936, 14893, 11, 321, 767, 362, 281, 7620, 341, 281, 33783, 50560], "temperature": 0.0, "avg_logprob": -0.1309280494848887, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.001987701514735818}, {"id": 377, "seek": 153420, "start": 1539.16, "end": 1541.56, "text": " So we create a probability vector", "tokens": [50612, 407, 321, 1884, 257, 8482, 8062, 50732], "temperature": 0.0, "avg_logprob": -0.1309280494848887, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.001987701514735818}, {"id": 378, "seek": 153420, "start": 1542.92, "end": 1544.92, "text": " So we'll take n of zero", "tokens": [50800, 407, 321, 603, 747, 297, 295, 4018, 50900], "temperature": 0.0, "avg_logprob": -0.1309280494848887, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.001987701514735818}, {"id": 379, "seek": 153420, "start": 1545.0, "end": 1547.48, "text": " And we'll actually convert this to float", "tokens": [50904, 400, 321, 603, 767, 7620, 341, 281, 15706, 51028], "temperature": 0.0, "avg_logprob": -0.1309280494848887, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.001987701514735818}, {"id": 380, "seek": 153420, "start": 1548.28, "end": 1550.04, "text": " first", "tokens": [51068, 700, 51156], "temperature": 0.0, "avg_logprob": -0.1309280494848887, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.001987701514735818}, {"id": 381, "seek": 153420, "start": 1550.04, "end": 1552.1200000000001, "text": " Okay, so these integers are converted to float", "tokens": [51156, 1033, 11, 370, 613, 41674, 366, 16424, 281, 15706, 51260], "temperature": 0.0, "avg_logprob": -0.1309280494848887, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.001987701514735818}, {"id": 382, "seek": 153420, "start": 1552.8400000000001, "end": 1558.04, "text": " floating point numbers and the reason we're creating floats is because we're about to normalize these counts", "tokens": [51296, 12607, 935, 3547, 293, 264, 1778, 321, 434, 4084, 37878, 307, 570, 321, 434, 466, 281, 2710, 1125, 613, 14893, 51556], "temperature": 0.0, "avg_logprob": -0.1309280494848887, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.001987701514735818}, {"id": 383, "seek": 153420, "start": 1558.8400000000001, "end": 1562.44, "text": " So to create a probability distribution here, we want to divide", "tokens": [51596, 407, 281, 1884, 257, 8482, 7316, 510, 11, 321, 528, 281, 9845, 51776], "temperature": 0.0, "avg_logprob": -0.1309280494848887, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.001987701514735818}, {"id": 384, "seek": 156244, "start": 1562.68, "end": 1566.44, "text": " We basically want to do p p p divide p that sum", "tokens": [50376, 492, 1936, 528, 281, 360, 280, 280, 280, 9845, 280, 300, 2408, 50564], "temperature": 0.0, "avg_logprob": -0.13018456820783944, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0010162192629650235}, {"id": 385, "seek": 156244, "start": 1569.64, "end": 1573.4, "text": " And now we get a vector of smaller numbers and these are now probabilities", "tokens": [50724, 400, 586, 321, 483, 257, 8062, 295, 4356, 3547, 293, 613, 366, 586, 33783, 50912], "temperature": 0.0, "avg_logprob": -0.13018456820783944, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0010162192629650235}, {"id": 386, "seek": 156244, "start": 1573.72, "end": 1578.2, "text": " So of course because we divided by the sum the sum of p now is one", "tokens": [50928, 407, 295, 1164, 570, 321, 6666, 538, 264, 2408, 264, 2408, 295, 280, 586, 307, 472, 51152], "temperature": 0.0, "avg_logprob": -0.13018456820783944, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0010162192629650235}, {"id": 387, "seek": 156244, "start": 1578.76, "end": 1581.0, "text": " So this is a nice proper probability distribution", "tokens": [51180, 407, 341, 307, 257, 1481, 2296, 8482, 7316, 51292], "temperature": 0.0, "avg_logprob": -0.13018456820783944, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0010162192629650235}, {"id": 388, "seek": 156244, "start": 1581.0800000000002, "end": 1585.48, "text": " It sums to one and this is giving us the probability for any single character to be the first", "tokens": [51296, 467, 34499, 281, 472, 293, 341, 307, 2902, 505, 264, 8482, 337, 604, 2167, 2517, 281, 312, 264, 700, 51516], "temperature": 0.0, "avg_logprob": -0.13018456820783944, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0010162192629650235}, {"id": 389, "seek": 156244, "start": 1586.04, "end": 1588.04, "text": " character of a word", "tokens": [51544, 2517, 295, 257, 1349, 51644], "temperature": 0.0, "avg_logprob": -0.13018456820783944, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0010162192629650235}, {"id": 390, "seek": 158804, "start": 1588.04, "end": 1592.12, "text": " So now we can try to sample from this distribution to sample from these distributions", "tokens": [50364, 407, 586, 321, 393, 853, 281, 6889, 490, 341, 7316, 281, 6889, 490, 613, 37870, 50568], "temperature": 0.0, "avg_logprob": -0.13978355902212639, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0004238670808263123}, {"id": 391, "seek": 158804, "start": 1592.12, "end": 1595.1599999999999, "text": " We're going to use torsion multinomial, which I've pulled up here", "tokens": [50568, 492, 434, 516, 281, 764, 3930, 82, 313, 2120, 259, 47429, 11, 597, 286, 600, 7373, 493, 510, 50720], "temperature": 0.0, "avg_logprob": -0.13978355902212639, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0004238670808263123}, {"id": 392, "seek": 158804, "start": 1596.2, "end": 1598.44, "text": " So torsion multinomial returns a", "tokens": [50772, 407, 3930, 82, 313, 2120, 259, 47429, 11247, 257, 50884], "temperature": 0.0, "avg_logprob": -0.13978355902212639, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0004238670808263123}, {"id": 393, "seek": 158804, "start": 1600.52, "end": 1606.28, "text": " Samples from the multinomial probability distribution, which is a complicated way of saying you give me probabilities", "tokens": [50988, 4832, 2622, 490, 264, 2120, 259, 47429, 8482, 7316, 11, 597, 307, 257, 6179, 636, 295, 1566, 291, 976, 385, 33783, 51276], "temperature": 0.0, "avg_logprob": -0.13978355902212639, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0004238670808263123}, {"id": 394, "seek": 158804, "start": 1606.36, "end": 1608.92, "text": " And I will give you integers which are sampled", "tokens": [51280, 400, 286, 486, 976, 291, 41674, 597, 366, 3247, 15551, 51408], "temperature": 0.0, "avg_logprob": -0.13978355902212639, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0004238670808263123}, {"id": 395, "seek": 158804, "start": 1609.48, "end": 1611.48, "text": " According to the probability distribution", "tokens": [51436, 7328, 281, 264, 8482, 7316, 51536], "temperature": 0.0, "avg_logprob": -0.13978355902212639, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0004238670808263123}, {"id": 396, "seek": 158804, "start": 1611.56, "end": 1614.68, "text": " So this is the signature of the method and to make everything deterministic", "tokens": [51540, 407, 341, 307, 264, 13397, 295, 264, 3170, 293, 281, 652, 1203, 15957, 3142, 51696], "temperature": 0.0, "avg_logprob": -0.13978355902212639, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0004238670808263123}, {"id": 397, "seek": 161468, "start": 1614.76, "end": 1617.96, "text": " We're going to use a generator object in pi torch", "tokens": [50368, 492, 434, 516, 281, 764, 257, 19265, 2657, 294, 3895, 27822, 50528], "temperature": 0.0, "avg_logprob": -0.13610938425814167, "compression_ratio": 1.7464114832535884, "no_speech_prob": 0.00638752244412899}, {"id": 398, "seek": 161468, "start": 1619.3200000000002, "end": 1622.6000000000001, "text": " So this makes everything deterministic so when you run this on your computer", "tokens": [50596, 407, 341, 1669, 1203, 15957, 3142, 370, 562, 291, 1190, 341, 322, 428, 3820, 50760], "temperature": 0.0, "avg_logprob": -0.13610938425814167, "compression_ratio": 1.7464114832535884, "no_speech_prob": 0.00638752244412899}, {"id": 399, "seek": 161468, "start": 1622.6000000000001, "end": 1626.28, "text": " You're going to the exact get the exact same results that i'm getting here on my computer", "tokens": [50760, 509, 434, 516, 281, 264, 1900, 483, 264, 1900, 912, 3542, 300, 741, 478, 1242, 510, 322, 452, 3820, 50944], "temperature": 0.0, "avg_logprob": -0.13610938425814167, "compression_ratio": 1.7464114832535884, "no_speech_prob": 0.00638752244412899}, {"id": 400, "seek": 161468, "start": 1627.3200000000002, "end": 1629.3200000000002, "text": " So let me show you how this works", "tokens": [50996, 407, 718, 385, 855, 291, 577, 341, 1985, 51096], "temperature": 0.0, "avg_logprob": -0.13610938425814167, "compression_ratio": 1.7464114832535884, "no_speech_prob": 0.00638752244412899}, {"id": 401, "seek": 161468, "start": 1632.76, "end": 1637.3200000000002, "text": " Here's the deterministic way of creating a torch generator object", "tokens": [51268, 1692, 311, 264, 15957, 3142, 636, 295, 4084, 257, 27822, 19265, 2657, 51496], "temperature": 0.0, "avg_logprob": -0.13610938425814167, "compression_ratio": 1.7464114832535884, "no_speech_prob": 0.00638752244412899}, {"id": 402, "seek": 161468, "start": 1638.28, "end": 1640.52, "text": " Seeding it with some number that we can agree on", "tokens": [51544, 1100, 9794, 309, 365, 512, 1230, 300, 321, 393, 3986, 322, 51656], "temperature": 0.0, "avg_logprob": -0.13610938425814167, "compression_ratio": 1.7464114832535884, "no_speech_prob": 0.00638752244412899}, {"id": 403, "seek": 164052, "start": 1641.24, "end": 1644.2, "text": " So that seeds a generator gets gives us an object g", "tokens": [50400, 407, 300, 9203, 257, 19265, 2170, 2709, 505, 364, 2657, 290, 50548], "temperature": 0.0, "avg_logprob": -0.15645342904168205, "compression_ratio": 1.5988700564971752, "no_speech_prob": 0.006692157592624426}, {"id": 404, "seek": 164052, "start": 1644.84, "end": 1649.0, "text": " And then we can pass that g to a function that creates", "tokens": [50580, 400, 550, 321, 393, 1320, 300, 290, 281, 257, 2445, 300, 7829, 50788], "temperature": 0.0, "avg_logprob": -0.15645342904168205, "compression_ratio": 1.5988700564971752, "no_speech_prob": 0.006692157592624426}, {"id": 405, "seek": 164052, "start": 1650.2, "end": 1654.52, "text": " Here random numbers torch.rand creates random numbers three of them", "tokens": [50848, 1692, 4974, 3547, 27822, 13, 3699, 7829, 4974, 3547, 1045, 295, 552, 51064], "temperature": 0.0, "avg_logprob": -0.15645342904168205, "compression_ratio": 1.5988700564971752, "no_speech_prob": 0.006692157592624426}, {"id": 406, "seek": 164052, "start": 1655.24, "end": 1658.68, "text": " And it's using this generator object to as a source of randomness", "tokens": [51100, 400, 309, 311, 1228, 341, 19265, 2657, 281, 382, 257, 4009, 295, 4974, 1287, 51272], "temperature": 0.0, "avg_logprob": -0.15645342904168205, "compression_ratio": 1.5988700564971752, "no_speech_prob": 0.006692157592624426}, {"id": 407, "seek": 164052, "start": 1660.44, "end": 1661.96, "text": " So", "tokens": [51360, 407, 51436], "temperature": 0.0, "avg_logprob": -0.15645342904168205, "compression_ratio": 1.5988700564971752, "no_speech_prob": 0.006692157592624426}, {"id": 408, "seek": 164052, "start": 1661.96, "end": 1663.96, "text": " Without normalizing it", "tokens": [51436, 9129, 2710, 3319, 309, 51536], "temperature": 0.0, "avg_logprob": -0.15645342904168205, "compression_ratio": 1.5988700564971752, "no_speech_prob": 0.006692157592624426}, {"id": 409, "seek": 164052, "start": 1664.28, "end": 1666.28, "text": " I can just print", "tokens": [51552, 286, 393, 445, 4482, 51652], "temperature": 0.0, "avg_logprob": -0.15645342904168205, "compression_ratio": 1.5988700564971752, "no_speech_prob": 0.006692157592624426}, {"id": 410, "seek": 166628, "start": 1666.52, "end": 1672.44, "text": " This is sort of like numbers between zero and one that are random according to this thing and whenever I run it again", "tokens": [50376, 639, 307, 1333, 295, 411, 3547, 1296, 4018, 293, 472, 300, 366, 4974, 4650, 281, 341, 551, 293, 5699, 286, 1190, 309, 797, 50672], "temperature": 0.0, "avg_logprob": -0.11006947664114144, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.002590747084468603}, {"id": 411, "seek": 166628, "start": 1673.08, "end": 1677.72, "text": " I'm always going to get the same result because I keep using the same generator object, which i'm seeding here", "tokens": [50704, 286, 478, 1009, 516, 281, 483, 264, 912, 1874, 570, 286, 1066, 1228, 264, 912, 19265, 2657, 11, 597, 741, 478, 8871, 278, 510, 50936], "temperature": 0.0, "avg_logprob": -0.11006947664114144, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.002590747084468603}, {"id": 412, "seek": 166628, "start": 1678.76, "end": 1680.76, "text": " And then if I divide", "tokens": [50988, 400, 550, 498, 286, 9845, 51088], "temperature": 0.0, "avg_logprob": -0.11006947664114144, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.002590747084468603}, {"id": 413, "seek": 166628, "start": 1681.72, "end": 1686.36, "text": " To normalize i'm going to get a nice probability distribution of just three elements", "tokens": [51136, 1407, 2710, 1125, 741, 478, 516, 281, 483, 257, 1481, 8482, 7316, 295, 445, 1045, 4959, 51368], "temperature": 0.0, "avg_logprob": -0.11006947664114144, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.002590747084468603}, {"id": 414, "seek": 166628, "start": 1687.48, "end": 1692.36, "text": " And then we can use torsion multinomial to draw samples from it. So this is what that looks like", "tokens": [51424, 400, 550, 321, 393, 764, 3930, 82, 313, 2120, 259, 47429, 281, 2642, 10938, 490, 309, 13, 407, 341, 307, 437, 300, 1542, 411, 51668], "temperature": 0.0, "avg_logprob": -0.11006947664114144, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.002590747084468603}, {"id": 415, "seek": 169236, "start": 1693.08, "end": 1697.4799999999998, "text": " Torsion multinomial will take the torch tensor", "tokens": [50400, 7160, 82, 313, 2120, 259, 47429, 486, 747, 264, 27822, 40863, 50620], "temperature": 0.0, "avg_logprob": -0.15284780926174588, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.006588388700038195}, {"id": 416, "seek": 169236, "start": 1698.36, "end": 1700.36, "text": " of probability distributions", "tokens": [50664, 295, 8482, 37870, 50764], "temperature": 0.0, "avg_logprob": -0.15284780926174588, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.006588388700038195}, {"id": 417, "seek": 169236, "start": 1701.0, "end": 1703.24, "text": " Then we can ask for a number of samples like say 20", "tokens": [50796, 1396, 321, 393, 1029, 337, 257, 1230, 295, 10938, 411, 584, 945, 50908], "temperature": 0.0, "avg_logprob": -0.15284780926174588, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.006588388700038195}, {"id": 418, "seek": 169236, "start": 1704.6, "end": 1708.12, "text": " Replacement equals true means that when we draw an element", "tokens": [50976, 47762, 7621, 6915, 2074, 1355, 300, 562, 321, 2642, 364, 4478, 51152], "temperature": 0.0, "avg_logprob": -0.15284780926174588, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.006588388700038195}, {"id": 419, "seek": 169236, "start": 1708.84, "end": 1715.1599999999999, "text": " We will we can draw it and then we can put it back into the list of eligible indices to draw again", "tokens": [51188, 492, 486, 321, 393, 2642, 309, 293, 550, 321, 393, 829, 309, 646, 666, 264, 1329, 295, 14728, 43840, 281, 2642, 797, 51504], "temperature": 0.0, "avg_logprob": -0.15284780926174588, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.006588388700038195}, {"id": 420, "seek": 169236, "start": 1715.8799999999999, "end": 1720.76, "text": " And we have to specify replacement as true because by default for some reason it's false", "tokens": [51540, 400, 321, 362, 281, 16500, 14419, 382, 2074, 570, 538, 7576, 337, 512, 1778, 309, 311, 7908, 51784], "temperature": 0.0, "avg_logprob": -0.15284780926174588, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.006588388700038195}, {"id": 421, "seek": 172076, "start": 1721.56, "end": 1723.0, "text": " And I think", "tokens": [50404, 400, 286, 519, 50476], "temperature": 0.0, "avg_logprob": -0.10226260081376179, "compression_ratio": 1.5887096774193548, "no_speech_prob": 0.0021487511694431305}, {"id": 422, "seek": 172076, "start": 1723.0, "end": 1725.0, "text": " You know, it's just something to be careful with", "tokens": [50476, 509, 458, 11, 309, 311, 445, 746, 281, 312, 5026, 365, 50576], "temperature": 0.0, "avg_logprob": -0.10226260081376179, "compression_ratio": 1.5887096774193548, "no_speech_prob": 0.0021487511694431305}, {"id": 423, "seek": 172076, "start": 1725.72, "end": 1730.92, "text": " And the generator is passed in here. So we are going to always get deterministic results the same results", "tokens": [50612, 400, 264, 19265, 307, 4678, 294, 510, 13, 407, 321, 366, 516, 281, 1009, 483, 15957, 3142, 3542, 264, 912, 3542, 50872], "temperature": 0.0, "avg_logprob": -0.10226260081376179, "compression_ratio": 1.5887096774193548, "no_speech_prob": 0.0021487511694431305}, {"id": 424, "seek": 172076, "start": 1731.24, "end": 1733.24, "text": " So if I run these two", "tokens": [50888, 407, 498, 286, 1190, 613, 732, 50988], "temperature": 0.0, "avg_logprob": -0.10226260081376179, "compression_ratio": 1.5887096774193548, "no_speech_prob": 0.0021487511694431305}, {"id": 425, "seek": 172076, "start": 1733.8, "end": 1736.28, "text": " We're going to get a bunch of samples from this distribution", "tokens": [51016, 492, 434, 516, 281, 483, 257, 3840, 295, 10938, 490, 341, 7316, 51140], "temperature": 0.0, "avg_logprob": -0.10226260081376179, "compression_ratio": 1.5887096774193548, "no_speech_prob": 0.0021487511694431305}, {"id": 426, "seek": 172076, "start": 1737.16, "end": 1743.24, "text": " Now you'll notice here that the probability for the first element in this tensor is 60", "tokens": [51184, 823, 291, 603, 3449, 510, 300, 264, 8482, 337, 264, 700, 4478, 294, 341, 40863, 307, 4060, 51488], "temperature": 0.0, "avg_logprob": -0.10226260081376179, "compression_ratio": 1.5887096774193548, "no_speech_prob": 0.0021487511694431305}, {"id": 427, "seek": 172076, "start": 1744.52, "end": 1749.4, "text": " So in these 20 samples, we'd expect 60 of them to be zero", "tokens": [51552, 407, 294, 613, 945, 10938, 11, 321, 1116, 2066, 4060, 295, 552, 281, 312, 4018, 51796], "temperature": 0.0, "avg_logprob": -0.10226260081376179, "compression_ratio": 1.5887096774193548, "no_speech_prob": 0.0021487511694431305}, {"id": 428, "seek": 174940, "start": 1749.4, "end": 1753.16, "text": " We'd expect 30 percent of them to be one", "tokens": [50364, 492, 1116, 2066, 2217, 3043, 295, 552, 281, 312, 472, 50552], "temperature": 0.0, "avg_logprob": -0.16587749719619752, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.0014324708608910441}, {"id": 429, "seek": 174940, "start": 1754.2800000000002, "end": 1756.76, "text": " And because the the element index two", "tokens": [50608, 400, 570, 264, 264, 4478, 8186, 732, 50732], "temperature": 0.0, "avg_logprob": -0.16587749719619752, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.0014324708608910441}, {"id": 430, "seek": 174940, "start": 1757.48, "end": 1759.48, "text": " Has only 10 probability", "tokens": [50768, 8646, 787, 1266, 8482, 50868], "temperature": 0.0, "avg_logprob": -0.16587749719619752, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.0014324708608910441}, {"id": 431, "seek": 174940, "start": 1759.48, "end": 1764.44, "text": " Very few of these samples should be two and indeed we only have a small number of twos", "tokens": [50868, 4372, 1326, 295, 613, 10938, 820, 312, 732, 293, 6451, 321, 787, 362, 257, 1359, 1230, 295, 683, 329, 51116], "temperature": 0.0, "avg_logprob": -0.16587749719619752, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.0014324708608910441}, {"id": 432, "seek": 174940, "start": 1765.3200000000002, "end": 1767.3200000000002, "text": " And we can sample as many as we like", "tokens": [51160, 400, 321, 393, 6889, 382, 867, 382, 321, 411, 51260], "temperature": 0.0, "avg_logprob": -0.16587749719619752, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.0014324708608910441}, {"id": 433, "seek": 174940, "start": 1769.0, "end": 1771.0, "text": " And the more we sample the more", "tokens": [51344, 400, 264, 544, 321, 6889, 264, 544, 51444], "temperature": 0.0, "avg_logprob": -0.16587749719619752, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.0014324708608910441}, {"id": 434, "seek": 174940, "start": 1771.0800000000002, "end": 1774.68, "text": " These numbers should roughly have the distribution here", "tokens": [51448, 1981, 3547, 820, 9810, 362, 264, 7316, 510, 51628], "temperature": 0.0, "avg_logprob": -0.16587749719619752, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.0014324708608910441}, {"id": 435, "seek": 177468, "start": 1775.64, "end": 1779.0800000000002, "text": " So we should have lots of zeros half as many", "tokens": [50412, 407, 321, 820, 362, 3195, 295, 35193, 1922, 382, 867, 50584], "temperature": 0.0, "avg_logprob": -0.2253325131474709, "compression_ratio": 1.708994708994709, "no_speech_prob": 0.004681065212935209}, {"id": 436, "seek": 177468, "start": 1781.5600000000002, "end": 1783.5600000000002, "text": " Once and we should have", "tokens": [50708, 3443, 293, 321, 820, 362, 50808], "temperature": 0.0, "avg_logprob": -0.2253325131474709, "compression_ratio": 1.708994708994709, "no_speech_prob": 0.004681065212935209}, {"id": 437, "seek": 177468, "start": 1784.1200000000001, "end": 1786.1200000000001, "text": " three times s few", "tokens": [50836, 1045, 1413, 262, 1326, 50936], "temperature": 0.0, "avg_logprob": -0.2253325131474709, "compression_ratio": 1.708994708994709, "no_speech_prob": 0.004681065212935209}, {"id": 438, "seek": 177468, "start": 1786.2, "end": 1789.24, "text": " Oh, sorry s few ones and three times s few", "tokens": [50940, 876, 11, 2597, 262, 1326, 2306, 293, 1045, 1413, 262, 1326, 51092], "temperature": 0.0, "avg_logprob": -0.2253325131474709, "compression_ratio": 1.708994708994709, "no_speech_prob": 0.004681065212935209}, {"id": 439, "seek": 177468, "start": 1790.44, "end": 1791.5600000000002, "text": " twos", "tokens": [51152, 683, 329, 51208], "temperature": 0.0, "avg_logprob": -0.2253325131474709, "compression_ratio": 1.708994708994709, "no_speech_prob": 0.004681065212935209}, {"id": 440, "seek": 177468, "start": 1791.5600000000002, "end": 1795.24, "text": " So you see that we have very few twos. We have some ones and most of them are zero", "tokens": [51208, 407, 291, 536, 300, 321, 362, 588, 1326, 683, 329, 13, 492, 362, 512, 2306, 293, 881, 295, 552, 366, 4018, 51392], "temperature": 0.0, "avg_logprob": -0.2253325131474709, "compression_ratio": 1.708994708994709, "no_speech_prob": 0.004681065212935209}, {"id": 441, "seek": 177468, "start": 1795.72, "end": 1797.88, "text": " So that's what torsion multinomial is doing", "tokens": [51416, 407, 300, 311, 437, 3930, 82, 313, 2120, 259, 47429, 307, 884, 51524], "temperature": 0.0, "avg_logprob": -0.2253325131474709, "compression_ratio": 1.708994708994709, "no_speech_prob": 0.004681065212935209}, {"id": 442, "seek": 177468, "start": 1798.8400000000001, "end": 1800.8400000000001, "text": " for us here", "tokens": [51572, 337, 505, 510, 51672], "temperature": 0.0, "avg_logprob": -0.2253325131474709, "compression_ratio": 1.708994708994709, "no_speech_prob": 0.004681065212935209}, {"id": 443, "seek": 177468, "start": 1801.0800000000002, "end": 1803.0800000000002, "text": " We are interested in this row. We've created this", "tokens": [51684, 492, 366, 3102, 294, 341, 5386, 13, 492, 600, 2942, 341, 51784], "temperature": 0.0, "avg_logprob": -0.2253325131474709, "compression_ratio": 1.708994708994709, "no_speech_prob": 0.004681065212935209}, {"id": 444, "seek": 180468, "start": 1805.64, "end": 1808.04, "text": " P here and now we can sample from it", "tokens": [50412, 430, 510, 293, 586, 321, 393, 6889, 490, 309, 50532], "temperature": 0.0, "avg_logprob": -0.10907479219658431, "compression_ratio": 1.6042780748663101, "no_speech_prob": 0.005059979390352964}, {"id": 445, "seek": 180468, "start": 1809.64, "end": 1811.64, "text": " So if we use the same seed", "tokens": [50612, 407, 498, 321, 764, 264, 912, 8871, 50712], "temperature": 0.0, "avg_logprob": -0.10907479219658431, "compression_ratio": 1.6042780748663101, "no_speech_prob": 0.005059979390352964}, {"id": 446, "seek": 180468, "start": 1812.8400000000001, "end": 1816.2, "text": " And then we sample from this distribution, let's just get one sample", "tokens": [50772, 400, 550, 321, 6889, 490, 341, 7316, 11, 718, 311, 445, 483, 472, 6889, 50940], "temperature": 0.0, "avg_logprob": -0.10907479219658431, "compression_ratio": 1.6042780748663101, "no_speech_prob": 0.005059979390352964}, {"id": 447, "seek": 180468, "start": 1818.2, "end": 1820.6000000000001, "text": " Then we see that the sample is say 13", "tokens": [51040, 1396, 321, 536, 300, 264, 6889, 307, 584, 3705, 51160], "temperature": 0.0, "avg_logprob": -0.10907479219658431, "compression_ratio": 1.6042780748663101, "no_speech_prob": 0.005059979390352964}, {"id": 448, "seek": 180468, "start": 1822.76, "end": 1824.76, "text": " So this will be the index", "tokens": [51268, 407, 341, 486, 312, 264, 8186, 51368], "temperature": 0.0, "avg_logprob": -0.10907479219658431, "compression_ratio": 1.6042780748663101, "no_speech_prob": 0.005059979390352964}, {"id": 449, "seek": 180468, "start": 1825.24, "end": 1828.28, "text": " And let's you see how it's a tensor that wraps 13", "tokens": [51392, 400, 718, 311, 291, 536, 577, 309, 311, 257, 40863, 300, 25831, 3705, 51544], "temperature": 0.0, "avg_logprob": -0.10907479219658431, "compression_ratio": 1.6042780748663101, "no_speech_prob": 0.005059979390352964}, {"id": 450, "seek": 180468, "start": 1828.76, "end": 1832.28, "text": " We again have to use dot item to pop out that integer", "tokens": [51568, 492, 797, 362, 281, 764, 5893, 3174, 281, 1665, 484, 300, 24922, 51744], "temperature": 0.0, "avg_logprob": -0.10907479219658431, "compression_ratio": 1.6042780748663101, "no_speech_prob": 0.005059979390352964}, {"id": 451, "seek": 183228, "start": 1832.92, "end": 1835.72, "text": " And now index would be just the number 13", "tokens": [50396, 400, 586, 8186, 576, 312, 445, 264, 1230, 3705, 50536], "temperature": 0.0, "avg_logprob": -0.15030985690177756, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.005729195661842823}, {"id": 452, "seek": 183228, "start": 1837.3999999999999, "end": 1844.52, "text": " And of course the um, we can do we can map the i2s of ix to figure out exactly which character", "tokens": [50620, 400, 295, 1164, 264, 1105, 11, 321, 393, 360, 321, 393, 4471, 264, 741, 17, 82, 295, 741, 87, 281, 2573, 484, 2293, 597, 2517, 50976], "temperature": 0.0, "avg_logprob": -0.15030985690177756, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.005729195661842823}, {"id": 453, "seek": 183228, "start": 1845.08, "end": 1847.08, "text": " We're sampling here. We're sampling m", "tokens": [51004, 492, 434, 21179, 510, 13, 492, 434, 21179, 275, 51104], "temperature": 0.0, "avg_logprob": -0.15030985690177756, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.005729195661842823}, {"id": 454, "seek": 183228, "start": 1848.04, "end": 1852.36, "text": " So we're saying that the first character is m in our generation", "tokens": [51152, 407, 321, 434, 1566, 300, 264, 700, 2517, 307, 275, 294, 527, 5125, 51368], "temperature": 0.0, "avg_logprob": -0.15030985690177756, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.005729195661842823}, {"id": 455, "seek": 183228, "start": 1853.08, "end": 1855.08, "text": " And just looking at the row here", "tokens": [51404, 400, 445, 1237, 412, 264, 5386, 510, 51504], "temperature": 0.0, "avg_logprob": -0.15030985690177756, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.005729195661842823}, {"id": 456, "seek": 183228, "start": 1855.16, "end": 1859.24, "text": " m was drawn and you we can see that m actually starts a large number of words", "tokens": [51508, 275, 390, 10117, 293, 291, 321, 393, 536, 300, 275, 767, 3719, 257, 2416, 1230, 295, 2283, 51712], "temperature": 0.0, "avg_logprob": -0.15030985690177756, "compression_ratio": 1.6308411214953271, "no_speech_prob": 0.005729195661842823}, {"id": 457, "seek": 185924, "start": 1860.1200000000001, "end": 1865.48, "text": " m started 2,500 words out of 32,000 words. So almost", "tokens": [50408, 275, 1409, 568, 11, 7526, 2283, 484, 295, 8858, 11, 1360, 2283, 13, 407, 1920, 50676], "temperature": 0.0, "avg_logprob": -0.17583385902115062, "compression_ratio": 1.622340425531915, "no_speech_prob": 0.004069723188877106}, {"id": 458, "seek": 185924, "start": 1866.28, "end": 1871.48, "text": " A bit less than 10 of the words start with m. So this is actually fairly likely character to draw", "tokens": [50716, 316, 857, 1570, 813, 1266, 295, 264, 2283, 722, 365, 275, 13, 407, 341, 307, 767, 6457, 3700, 2517, 281, 2642, 50976], "temperature": 0.0, "avg_logprob": -0.17583385902115062, "compression_ratio": 1.622340425531915, "no_speech_prob": 0.004069723188877106}, {"id": 459, "seek": 185924, "start": 1875.24, "end": 1879.24, "text": " So that would be the first character of our word and now we can continue to sample more characters", "tokens": [51164, 407, 300, 576, 312, 264, 700, 2517, 295, 527, 1349, 293, 586, 321, 393, 2354, 281, 6889, 544, 4342, 51364], "temperature": 0.0, "avg_logprob": -0.17583385902115062, "compression_ratio": 1.622340425531915, "no_speech_prob": 0.004069723188877106}, {"id": 460, "seek": 185924, "start": 1879.72, "end": 1882.04, "text": " Because now we know that m started", "tokens": [51388, 1436, 586, 321, 458, 300, 275, 1409, 51504], "temperature": 0.0, "avg_logprob": -0.17583385902115062, "compression_ratio": 1.622340425531915, "no_speech_prob": 0.004069723188877106}, {"id": 461, "seek": 185924, "start": 1882.92, "end": 1884.76, "text": " m is already sampled", "tokens": [51548, 275, 307, 1217, 3247, 15551, 51640], "temperature": 0.0, "avg_logprob": -0.17583385902115062, "compression_ratio": 1.622340425531915, "no_speech_prob": 0.004069723188877106}, {"id": 462, "seek": 188476, "start": 1884.76, "end": 1890.12, "text": " So now to draw the next character, we will come back here and we will look for the row", "tokens": [50364, 407, 586, 281, 2642, 264, 958, 2517, 11, 321, 486, 808, 646, 510, 293, 321, 486, 574, 337, 264, 5386, 50632], "temperature": 0.0, "avg_logprob": -0.11043418821741323, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0017544886795803905}, {"id": 463, "seek": 188476, "start": 1891.0, "end": 1893.48, "text": " That starts with m. So you see m", "tokens": [50676, 663, 3719, 365, 275, 13, 407, 291, 536, 275, 50800], "temperature": 0.0, "avg_logprob": -0.11043418821741323, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0017544886795803905}, {"id": 464, "seek": 188476, "start": 1894.6, "end": 1896.6, "text": " And we have a row here", "tokens": [50856, 400, 321, 362, 257, 5386, 510, 50956], "temperature": 0.0, "avg_logprob": -0.11043418821741323, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0017544886795803905}, {"id": 465, "seek": 188476, "start": 1896.68, "end": 1898.68, "text": " so we see that m dot is", "tokens": [50960, 370, 321, 536, 300, 275, 5893, 307, 51060], "temperature": 0.0, "avg_logprob": -0.11043418821741323, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0017544886795803905}, {"id": 466, "seek": 188476, "start": 1899.56, "end": 1903.72, "text": " 516 ma is this many mb is this many etc", "tokens": [51104, 1025, 6866, 463, 307, 341, 867, 275, 65, 307, 341, 867, 5183, 51312], "temperature": 0.0, "avg_logprob": -0.11043418821741323, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0017544886795803905}, {"id": 467, "seek": 188476, "start": 1903.8, "end": 1908.04, "text": " So these are the counts for the next row and that's the next character that we are going to now generate", "tokens": [51316, 407, 613, 366, 264, 14893, 337, 264, 958, 5386, 293, 300, 311, 264, 958, 2517, 300, 321, 366, 516, 281, 586, 8460, 51528], "temperature": 0.0, "avg_logprob": -0.11043418821741323, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0017544886795803905}, {"id": 468, "seek": 188476, "start": 1908.6, "end": 1913.56, "text": " So I think we are ready to actually just write out the loop because I think you're starting to get a sense of how this is going to go", "tokens": [51556, 407, 286, 519, 321, 366, 1919, 281, 767, 445, 2464, 484, 264, 6367, 570, 286, 519, 291, 434, 2891, 281, 483, 257, 2020, 295, 577, 341, 307, 516, 281, 352, 51804], "temperature": 0.0, "avg_logprob": -0.11043418821741323, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0017544886795803905}, {"id": 469, "seek": 191356, "start": 1914.52, "end": 1916.28, "text": " The um", "tokens": [50412, 440, 1105, 50500], "temperature": 0.0, "avg_logprob": -0.13889140349168044, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0027147557120770216}, {"id": 470, "seek": 191356, "start": 1916.28, "end": 1920.6, "text": " We always begin at index zero because that's the start token", "tokens": [50500, 492, 1009, 1841, 412, 8186, 4018, 570, 300, 311, 264, 722, 14862, 50716], "temperature": 0.0, "avg_logprob": -0.13889140349168044, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0027147557120770216}, {"id": 471, "seek": 191356, "start": 1922.36, "end": 1924.36, "text": " And then while true", "tokens": [50804, 400, 550, 1339, 2074, 50904], "temperature": 0.0, "avg_logprob": -0.13889140349168044, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0027147557120770216}, {"id": 472, "seek": 191356, "start": 1924.9199999999998, "end": 1927.72, "text": " We're going to grab the row corresponding to index", "tokens": [50932, 492, 434, 516, 281, 4444, 264, 5386, 11760, 281, 8186, 51072], "temperature": 0.0, "avg_logprob": -0.13889140349168044, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0027147557120770216}, {"id": 473, "seek": 191356, "start": 1928.44, "end": 1930.44, "text": " That we're currently on so that's p", "tokens": [51108, 663, 321, 434, 4362, 322, 370, 300, 311, 280, 51208], "temperature": 0.0, "avg_logprob": -0.13889140349168044, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0027147557120770216}, {"id": 474, "seek": 191356, "start": 1931.1599999999999, "end": 1933.56, "text": " So that's n array at ix", "tokens": [51244, 407, 300, 311, 297, 10225, 412, 741, 87, 51364], "temperature": 0.0, "avg_logprob": -0.13889140349168044, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0027147557120770216}, {"id": 475, "seek": 191356, "start": 1934.44, "end": 1936.44, "text": " converted to float is rp", "tokens": [51408, 16424, 281, 15706, 307, 367, 79, 51508], "temperature": 0.0, "avg_logprob": -0.13889140349168044, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0027147557120770216}, {"id": 476, "seek": 191356, "start": 1939.1599999999999, "end": 1942.6799999999998, "text": " Then we normalize the speed to sum to one", "tokens": [51644, 1396, 321, 2710, 1125, 264, 3073, 281, 2408, 281, 472, 51820], "temperature": 0.0, "avg_logprob": -0.13889140349168044, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0027147557120770216}, {"id": 477, "seek": 194356, "start": 1944.28, "end": 1947.48, "text": " I accidentally ran the infinite loop", "tokens": [50400, 286, 15715, 5872, 264, 13785, 6367, 50560], "temperature": 0.0, "avg_logprob": -0.1265874796135481, "compression_ratio": 1.6548223350253808, "no_speech_prob": 0.0011334755690768361}, {"id": 478, "seek": 194356, "start": 1948.12, "end": 1950.12, "text": " We normalize p to sum to one", "tokens": [50592, 492, 2710, 1125, 280, 281, 2408, 281, 472, 50692], "temperature": 0.0, "avg_logprob": -0.1265874796135481, "compression_ratio": 1.6548223350253808, "no_speech_prob": 0.0011334755690768361}, {"id": 479, "seek": 194356, "start": 1950.6799999999998, "end": 1952.6799999999998, "text": " Then we need this generator object", "tokens": [50720, 1396, 321, 643, 341, 19265, 2657, 50820], "temperature": 0.0, "avg_logprob": -0.1265874796135481, "compression_ratio": 1.6548223350253808, "no_speech_prob": 0.0011334755690768361}, {"id": 480, "seek": 194356, "start": 1953.8, "end": 1958.04, "text": " And we're going to initialize up here and we're going to draw a single sample from this distribution", "tokens": [50876, 400, 321, 434, 516, 281, 5883, 1125, 493, 510, 293, 321, 434, 516, 281, 2642, 257, 2167, 6889, 490, 341, 7316, 51088], "temperature": 0.0, "avg_logprob": -0.1265874796135481, "compression_ratio": 1.6548223350253808, "no_speech_prob": 0.0011334755690768361}, {"id": 481, "seek": 194356, "start": 1960.84, "end": 1964.6799999999998, "text": " And then this is going to tell us what index is going to be next", "tokens": [51228, 400, 550, 341, 307, 516, 281, 980, 505, 437, 8186, 307, 516, 281, 312, 958, 51420], "temperature": 0.0, "avg_logprob": -0.1265874796135481, "compression_ratio": 1.6548223350253808, "no_speech_prob": 0.0011334755690768361}, {"id": 482, "seek": 194356, "start": 1966.6, "end": 1971.48, "text": " If the index sampled is zero, then that's now the nth token", "tokens": [51516, 759, 264, 8186, 3247, 15551, 307, 4018, 11, 550, 300, 311, 586, 264, 297, 392, 14862, 51760], "temperature": 0.0, "avg_logprob": -0.1265874796135481, "compression_ratio": 1.6548223350253808, "no_speech_prob": 0.0011334755690768361}, {"id": 483, "seek": 197148, "start": 1971.56, "end": 1973.56, "text": " So we will break", "tokens": [50368, 407, 321, 486, 1821, 50468], "temperature": 0.0, "avg_logprob": -0.18180276140754606, "compression_ratio": 1.503030303030303, "no_speech_prob": 0.003172515658661723}, {"id": 484, "seek": 197148, "start": 1975.4, "end": 1979.32, "text": " Otherwise we are going to print s2i of ix", "tokens": [50560, 10328, 321, 366, 516, 281, 4482, 262, 17, 72, 295, 741, 87, 50756], "temperature": 0.0, "avg_logprob": -0.18180276140754606, "compression_ratio": 1.503030303030303, "no_speech_prob": 0.003172515658661723}, {"id": 485, "seek": 197148, "start": 1982.28, "end": 1984.28, "text": " i2s of ix", "tokens": [50904, 741, 17, 82, 295, 741, 87, 51004], "temperature": 0.0, "avg_logprob": -0.18180276140754606, "compression_ratio": 1.503030303030303, "no_speech_prob": 0.003172515658661723}, {"id": 486, "seek": 197148, "start": 1985.32, "end": 1989.08, "text": " And uh, that's pretty much it. We're just uh, this should work", "tokens": [51056, 400, 2232, 11, 300, 311, 1238, 709, 309, 13, 492, 434, 445, 2232, 11, 341, 820, 589, 51244], "temperature": 0.0, "avg_logprob": -0.18180276140754606, "compression_ratio": 1.503030303030303, "no_speech_prob": 0.003172515658661723}, {"id": 487, "seek": 197148, "start": 1990.3600000000001, "end": 1992.1200000000001, "text": " Okay more", "tokens": [51308, 1033, 544, 51396], "temperature": 0.0, "avg_logprob": -0.18180276140754606, "compression_ratio": 1.503030303030303, "no_speech_prob": 0.003172515658661723}, {"id": 488, "seek": 197148, "start": 1992.1200000000001, "end": 1999.24, "text": " So that's the that's the name that we've sampled we started with m the next step was o then r and then dot", "tokens": [51396, 407, 300, 311, 264, 300, 311, 264, 1315, 300, 321, 600, 3247, 15551, 321, 1409, 365, 275, 264, 958, 1823, 390, 277, 550, 367, 293, 550, 5893, 51752], "temperature": 0.0, "avg_logprob": -0.18180276140754606, "compression_ratio": 1.503030303030303, "no_speech_prob": 0.003172515658661723}, {"id": 489, "seek": 200148, "start": 2001.48, "end": 2004.04, "text": " And this dot we printed here as well", "tokens": [50364, 400, 341, 5893, 321, 13567, 510, 382, 731, 50492], "temperature": 0.0, "avg_logprob": -0.10229382731697777, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.001700397813692689}, {"id": 490, "seek": 200148, "start": 2004.84, "end": 2006.44, "text": " so", "tokens": [50532, 370, 50612], "temperature": 0.0, "avg_logprob": -0.10229382731697777, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.001700397813692689}, {"id": 491, "seek": 200148, "start": 2006.44, "end": 2008.44, "text": " Let's now do this a few times", "tokens": [50612, 961, 311, 586, 360, 341, 257, 1326, 1413, 50712], "temperature": 0.0, "avg_logprob": -0.10229382731697777, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.001700397813692689}, {"id": 492, "seek": 200148, "start": 2008.6, "end": 2010.04, "text": " um", "tokens": [50720, 1105, 50792], "temperature": 0.0, "avg_logprob": -0.10229382731697777, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.001700397813692689}, {"id": 493, "seek": 200148, "start": 2010.04, "end": 2012.04, "text": " So let's actually create an", "tokens": [50792, 407, 718, 311, 767, 1884, 364, 50892], "temperature": 0.0, "avg_logprob": -0.10229382731697777, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.001700397813692689}, {"id": 494, "seek": 200148, "start": 2013.48, "end": 2015.48, "text": " Out list here", "tokens": [50964, 5925, 1329, 510, 51064], "temperature": 0.0, "avg_logprob": -0.10229382731697777, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.001700397813692689}, {"id": 495, "seek": 200148, "start": 2017.16, "end": 2021.8, "text": " And instead of printing we're going to append so out that append this character", "tokens": [51148, 400, 2602, 295, 14699, 321, 434, 516, 281, 34116, 370, 484, 300, 34116, 341, 2517, 51380], "temperature": 0.0, "avg_logprob": -0.10229382731697777, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.001700397813692689}, {"id": 496, "seek": 200148, "start": 2024.44, "end": 2030.84, "text": " And then here let's just print it at the end. So let's just join up all the outs and we're just going to print more", "tokens": [51512, 400, 550, 510, 718, 311, 445, 4482, 309, 412, 264, 917, 13, 407, 718, 311, 445, 3917, 493, 439, 264, 14758, 293, 321, 434, 445, 516, 281, 4482, 544, 51832], "temperature": 0.0, "avg_logprob": -0.10229382731697777, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.001700397813692689}, {"id": 497, "seek": 203148, "start": 2031.48, "end": 2034.3600000000001, "text": " Okay, now we're always getting the same result because of the generator", "tokens": [50364, 1033, 11, 586, 321, 434, 1009, 1242, 264, 912, 1874, 570, 295, 264, 19265, 50508], "temperature": 0.0, "avg_logprob": -0.11773424241149309, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0008558559929952025}, {"id": 498, "seek": 203148, "start": 2035.16, "end": 2039.32, "text": " So if we want to do this a few times we can go for high and range", "tokens": [50548, 407, 498, 321, 528, 281, 360, 341, 257, 1326, 1413, 321, 393, 352, 337, 1090, 293, 3613, 50756], "temperature": 0.0, "avg_logprob": -0.11773424241149309, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0008558559929952025}, {"id": 499, "seek": 203148, "start": 2040.1200000000001, "end": 2042.1200000000001, "text": " 10 we can sample 10 names", "tokens": [50796, 1266, 321, 393, 6889, 1266, 5288, 50896], "temperature": 0.0, "avg_logprob": -0.11773424241149309, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0008558559929952025}, {"id": 500, "seek": 203148, "start": 2042.52, "end": 2044.52, "text": " And we can just do that 10 times", "tokens": [50916, 400, 321, 393, 445, 360, 300, 1266, 1413, 51016], "temperature": 0.0, "avg_logprob": -0.11773424241149309, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0008558559929952025}, {"id": 501, "seek": 203148, "start": 2045.72, "end": 2047.72, "text": " And these are the names that we're getting out", "tokens": [51076, 400, 613, 366, 264, 5288, 300, 321, 434, 1242, 484, 51176], "temperature": 0.0, "avg_logprob": -0.11773424241149309, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0008558559929952025}, {"id": 502, "seek": 203148, "start": 2048.52, "end": 2050.52, "text": " Let's do 20", "tokens": [51216, 961, 311, 360, 945, 51316], "temperature": 0.0, "avg_logprob": -0.11773424241149309, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0008558559929952025}, {"id": 503, "seek": 203148, "start": 2054.28, "end": 2056.28, "text": " I'll be honest with you, this doesn't look right", "tokens": [51504, 286, 603, 312, 3245, 365, 291, 11, 341, 1177, 380, 574, 558, 51604], "temperature": 0.0, "avg_logprob": -0.11773424241149309, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0008558559929952025}, {"id": 504, "seek": 203148, "start": 2056.52, "end": 2059.72, "text": " So I started a few minutes to convince myself that it actually is right", "tokens": [51616, 407, 286, 1409, 257, 1326, 2077, 281, 13447, 2059, 300, 309, 767, 307, 558, 51776], "temperature": 0.0, "avg_logprob": -0.11773424241149309, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0008558559929952025}, {"id": 505, "seek": 205972, "start": 2060.52, "end": 2066.9199999999996, "text": " The reason these samples are so terrible is that by gram language model is actually looks just like really terrible", "tokens": [50404, 440, 1778, 613, 10938, 366, 370, 6237, 307, 300, 538, 21353, 2856, 2316, 307, 767, 1542, 445, 411, 534, 6237, 50724], "temperature": 0.0, "avg_logprob": -0.12527256385952817, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.007814322598278522}, {"id": 506, "seek": 205972, "start": 2067.7999999999997, "end": 2069.7999999999997, "text": " We can generate a few more here", "tokens": [50768, 492, 393, 8460, 257, 1326, 544, 510, 50868], "temperature": 0.0, "avg_logprob": -0.12527256385952817, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.007814322598278522}, {"id": 507, "seek": 205972, "start": 2070.04, "end": 2075.0, "text": " And you can see that they're kind of like their name like a little bit like keanu iraily etc", "tokens": [50880, 400, 291, 393, 536, 300, 436, 434, 733, 295, 411, 641, 1315, 411, 257, 707, 857, 411, 803, 48136, 3418, 64, 953, 5183, 51128], "temperature": 0.0, "avg_logprob": -0.12527256385952817, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.007814322598278522}, {"id": 508, "seek": 205972, "start": 2075.56, "end": 2077.56, "text": " But they're just like totally messed up", "tokens": [51156, 583, 436, 434, 445, 411, 3879, 16507, 493, 51256], "temperature": 0.0, "avg_logprob": -0.12527256385952817, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.007814322598278522}, {"id": 509, "seek": 205972, "start": 2078.6, "end": 2082.6, "text": " And I mean the reason that this is so bad like we're generating h as a name", "tokens": [51308, 400, 286, 914, 264, 1778, 300, 341, 307, 370, 1578, 411, 321, 434, 17746, 276, 382, 257, 1315, 51508], "temperature": 0.0, "avg_logprob": -0.12527256385952817, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.007814322598278522}, {"id": 510, "seek": 205972, "start": 2083.0, "end": 2086.04, "text": " But you have to think through it from the model's eyes", "tokens": [51528, 583, 291, 362, 281, 519, 807, 309, 490, 264, 2316, 311, 2575, 51680], "temperature": 0.0, "avg_logprob": -0.12527256385952817, "compression_ratio": 1.7415254237288136, "no_speech_prob": 0.007814322598278522}, {"id": 511, "seek": 208604, "start": 2086.44, "end": 2091.64, "text": " It doesn't know that this h is the very first h. All it knows is that h was previously", "tokens": [50384, 467, 1177, 380, 458, 300, 341, 276, 307, 264, 588, 700, 276, 13, 1057, 309, 3255, 307, 300, 276, 390, 8046, 50644], "temperature": 0.0, "avg_logprob": -0.12684594859247622, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.04270875081419945}, {"id": 512, "seek": 208604, "start": 2092.12, "end": 2095.16, "text": " And now how likely is h the last character?", "tokens": [50668, 400, 586, 577, 3700, 307, 276, 264, 1036, 2517, 30, 50820], "temperature": 0.0, "avg_logprob": -0.12684594859247622, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.04270875081419945}, {"id": 513, "seek": 208604, "start": 2095.64, "end": 2099.24, "text": " Well, it's somewhat likely and so it just makes it last character", "tokens": [50844, 1042, 11, 309, 311, 8344, 3700, 293, 370, 309, 445, 1669, 309, 1036, 2517, 51024], "temperature": 0.0, "avg_logprob": -0.12684594859247622, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.04270875081419945}, {"id": 514, "seek": 208604, "start": 2099.24, "end": 2103.32, "text": " It doesn't know that there were other things before it or there were not other things before it", "tokens": [51024, 467, 1177, 380, 458, 300, 456, 645, 661, 721, 949, 309, 420, 456, 645, 406, 661, 721, 949, 309, 51228], "temperature": 0.0, "avg_logprob": -0.12684594859247622, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.04270875081419945}, {"id": 515, "seek": 208604, "start": 2103.88, "end": 2107.4, "text": " And so that's why it's generating all these like nonsense names", "tokens": [51256, 400, 370, 300, 311, 983, 309, 311, 17746, 439, 613, 411, 14925, 5288, 51432], "temperature": 0.0, "avg_logprob": -0.12684594859247622, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.04270875081419945}, {"id": 516, "seek": 208604, "start": 2108.12, "end": 2110.12, "text": " in other ways to do this is", "tokens": [51468, 294, 661, 2098, 281, 360, 341, 307, 51568], "temperature": 0.0, "avg_logprob": -0.12684594859247622, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.04270875081419945}, {"id": 517, "seek": 208604, "start": 2111.96, "end": 2115.4, "text": " To convince yourself that this is actually doing something reasonable even though it's so terrible", "tokens": [51660, 1407, 13447, 1803, 300, 341, 307, 767, 884, 746, 10585, 754, 1673, 309, 311, 370, 6237, 51832], "temperature": 0.0, "avg_logprob": -0.12684594859247622, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.04270875081419945}, {"id": 518, "seek": 211604, "start": 2116.12, "end": 2117.72, "text": " is", "tokens": [50368, 307, 50448], "temperature": 0.0, "avg_logprob": -0.15903005335066053, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.004198565147817135}, {"id": 519, "seek": 211604, "start": 2117.72, "end": 2121.8, "text": " These little p's here are 27 right like 27", "tokens": [50448, 1981, 707, 280, 311, 510, 366, 7634, 558, 411, 7634, 50652], "temperature": 0.0, "avg_logprob": -0.15903005335066053, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.004198565147817135}, {"id": 520, "seek": 211604, "start": 2123.16, "end": 2125.16, "text": " So how about if we did something like this?", "tokens": [50720, 407, 577, 466, 498, 321, 630, 746, 411, 341, 30, 50820], "temperature": 0.0, "avg_logprob": -0.15903005335066053, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.004198565147817135}, {"id": 521, "seek": 211604, "start": 2126.2799999999997, "end": 2128.2799999999997, "text": " Instead of p having any structure whatsoever", "tokens": [50876, 7156, 295, 280, 1419, 604, 3877, 17076, 50976], "temperature": 0.0, "avg_logprob": -0.15903005335066053, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.004198565147817135}, {"id": 522, "seek": 211604, "start": 2128.92, "end": 2132.36, "text": " How about if p was just a torch dot ones?", "tokens": [51008, 1012, 466, 498, 280, 390, 445, 257, 27822, 5893, 2306, 30, 51180], "temperature": 0.0, "avg_logprob": -0.15903005335066053, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.004198565147817135}, {"id": 523, "seek": 211604, "start": 2134.92, "end": 2136.92, "text": " Of 27", "tokens": [51308, 2720, 7634, 51408], "temperature": 0.0, "avg_logprob": -0.15903005335066053, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.004198565147817135}, {"id": 524, "seek": 211604, "start": 2137.24, "end": 2141.08, "text": " By default, this is a float 32. So this is fine divide 27", "tokens": [51424, 3146, 7576, 11, 341, 307, 257, 15706, 8858, 13, 407, 341, 307, 2489, 9845, 7634, 51616], "temperature": 0.0, "avg_logprob": -0.15903005335066053, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.004198565147817135}, {"id": 525, "seek": 214108, "start": 2141.72, "end": 2148.6, "text": " So what I'm doing here is this is the uniform distribution, which will make everything equally likely", "tokens": [50396, 407, 437, 286, 478, 884, 510, 307, 341, 307, 264, 9452, 7316, 11, 597, 486, 652, 1203, 12309, 3700, 50740], "temperature": 0.0, "avg_logprob": -0.13919501358203673, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.008576931431889534}, {"id": 526, "seek": 214108, "start": 2149.88, "end": 2153.24, "text": " And we can sample from that. So let's see if that does any better", "tokens": [50804, 400, 321, 393, 6889, 490, 300, 13, 407, 718, 311, 536, 498, 300, 775, 604, 1101, 50972], "temperature": 0.0, "avg_logprob": -0.13919501358203673, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.008576931431889534}, {"id": 527, "seek": 214108, "start": 2154.2, "end": 2160.12, "text": " Okay, so it's this is what you have from a model that is completely untrained where everything is equally likely", "tokens": [51020, 1033, 11, 370, 309, 311, 341, 307, 437, 291, 362, 490, 257, 2316, 300, 307, 2584, 1701, 31774, 689, 1203, 307, 12309, 3700, 51316], "temperature": 0.0, "avg_logprob": -0.13919501358203673, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.008576931431889534}, {"id": 528, "seek": 214108, "start": 2160.6, "end": 2166.36, "text": " So it's obviously garbage and then if we have a trained model, which is trained on just by grams", "tokens": [51340, 407, 309, 311, 2745, 14150, 293, 550, 498, 321, 362, 257, 8895, 2316, 11, 597, 307, 8895, 322, 445, 538, 11899, 51628], "temperature": 0.0, "avg_logprob": -0.13919501358203673, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.008576931431889534}, {"id": 529, "seek": 216636, "start": 2167.1600000000003, "end": 2172.36, "text": " This is what we get. So you can see that it is more name like it is actually working. It's just", "tokens": [50404, 639, 307, 437, 321, 483, 13, 407, 291, 393, 536, 300, 309, 307, 544, 1315, 411, 309, 307, 767, 1364, 13, 467, 311, 445, 50664], "temperature": 0.0, "avg_logprob": -0.09795235765391383, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.006289456970989704}, {"id": 530, "seek": 216636, "start": 2174.1200000000003, "end": 2176.2000000000003, "text": " By gram is so terrible and we have to do better", "tokens": [50752, 3146, 21353, 307, 370, 6237, 293, 321, 362, 281, 360, 1101, 50856], "temperature": 0.0, "avg_logprob": -0.09795235765391383, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.006289456970989704}, {"id": 531, "seek": 216636, "start": 2176.6, "end": 2179.56, "text": " Now next I would like to fix an inefficiency that we have going on here", "tokens": [50876, 823, 958, 286, 576, 411, 281, 3191, 364, 7167, 49086, 300, 321, 362, 516, 322, 510, 51024], "temperature": 0.0, "avg_logprob": -0.09795235765391383, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.006289456970989704}, {"id": 532, "seek": 216636, "start": 2180.28, "end": 2185.6400000000003, "text": " Because what we're doing here is we're always fetching a row of n from the counts matrix up ahead", "tokens": [51060, 1436, 437, 321, 434, 884, 510, 307, 321, 434, 1009, 23673, 278, 257, 5386, 295, 297, 490, 264, 14893, 8141, 493, 2286, 51328], "temperature": 0.0, "avg_logprob": -0.09795235765391383, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.006289456970989704}, {"id": 533, "seek": 216636, "start": 2186.52, "end": 2187.96, "text": " And then we're always doing the same things", "tokens": [51372, 400, 550, 321, 434, 1009, 884, 264, 912, 721, 51444], "temperature": 0.0, "avg_logprob": -0.09795235765391383, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.006289456970989704}, {"id": 534, "seek": 216636, "start": 2187.96, "end": 2192.44, "text": " We're converting to float and we're dividing and we're doing this every single iteration of this loop", "tokens": [51444, 492, 434, 29942, 281, 15706, 293, 321, 434, 26764, 293, 321, 434, 884, 341, 633, 2167, 24784, 295, 341, 6367, 51668], "temperature": 0.0, "avg_logprob": -0.09795235765391383, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.006289456970989704}, {"id": 535, "seek": 219244, "start": 2192.68, "end": 2196.84, "text": " And we just keep renormalizing these rows over and over again and it's extremely inefficient and wasteful", "tokens": [50376, 400, 321, 445, 1066, 8124, 24440, 3319, 613, 13241, 670, 293, 670, 797, 293, 309, 311, 4664, 43495, 293, 5964, 906, 50584], "temperature": 0.0, "avg_logprob": -0.09056897524024259, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.00912424921989441}, {"id": 536, "seek": 219244, "start": 2197.32, "end": 2201.2400000000002, "text": " So what I'd like to do is I'd like to actually prepare a matrix capital p", "tokens": [50608, 407, 437, 286, 1116, 411, 281, 360, 307, 286, 1116, 411, 281, 767, 5940, 257, 8141, 4238, 280, 50804], "temperature": 0.0, "avg_logprob": -0.09056897524024259, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.00912424921989441}, {"id": 537, "seek": 219244, "start": 2201.64, "end": 2203.64, "text": " That will just have the probabilities in it", "tokens": [50824, 663, 486, 445, 362, 264, 33783, 294, 309, 50924], "temperature": 0.0, "avg_logprob": -0.09056897524024259, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.00912424921989441}, {"id": 538, "seek": 219244, "start": 2203.88, "end": 2207.8, "text": " So in other words is going to be the same as the capital n matrix here of counts", "tokens": [50936, 407, 294, 661, 2283, 307, 516, 281, 312, 264, 912, 382, 264, 4238, 297, 8141, 510, 295, 14893, 51132], "temperature": 0.0, "avg_logprob": -0.09056897524024259, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.00912424921989441}, {"id": 539, "seek": 219244, "start": 2207.96, "end": 2210.92, "text": " But every single row will have the row of probabilities", "tokens": [51140, 583, 633, 2167, 5386, 486, 362, 264, 5386, 295, 33783, 51288], "temperature": 0.0, "avg_logprob": -0.09056897524024259, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.00912424921989441}, {"id": 540, "seek": 219244, "start": 2211.56, "end": 2215.56, "text": " That is normalized to 1 indicating the probability distribution for the next character", "tokens": [51320, 663, 307, 48704, 281, 502, 25604, 264, 8482, 7316, 337, 264, 958, 2517, 51520], "temperature": 0.0, "avg_logprob": -0.09056897524024259, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.00912424921989441}, {"id": 541, "seek": 219244, "start": 2216.04, "end": 2218.04, "text": " Given the character before it", "tokens": [51544, 18600, 264, 2517, 949, 309, 51644], "temperature": 0.0, "avg_logprob": -0.09056897524024259, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.00912424921989441}, {"id": 542, "seek": 219244, "start": 2218.52, "end": 2220.52, "text": " As defined by which row we're in", "tokens": [51668, 1018, 7642, 538, 597, 5386, 321, 434, 294, 51768], "temperature": 0.0, "avg_logprob": -0.09056897524024259, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.00912424921989441}, {"id": 543, "seek": 222052, "start": 2221.48, "end": 2224.6, "text": " So basically what we'd like to do is we'd like to just do it up front here", "tokens": [50412, 407, 1936, 437, 321, 1116, 411, 281, 360, 307, 321, 1116, 411, 281, 445, 360, 309, 493, 1868, 510, 50568], "temperature": 0.0, "avg_logprob": -0.1080843164015186, "compression_ratio": 1.8776978417266188, "no_speech_prob": 0.0015976420836523175}, {"id": 544, "seek": 222052, "start": 2225.0, "end": 2227.16, "text": " And then we would like to just use that row here", "tokens": [50588, 400, 550, 321, 576, 411, 281, 445, 764, 300, 5386, 510, 50696], "temperature": 0.0, "avg_logprob": -0.1080843164015186, "compression_ratio": 1.8776978417266188, "no_speech_prob": 0.0015976420836523175}, {"id": 545, "seek": 222052, "start": 2228.12, "end": 2232.12, "text": " So here we would like to just do p equals p of ix instead", "tokens": [50744, 407, 510, 321, 576, 411, 281, 445, 360, 280, 6915, 280, 295, 741, 87, 2602, 50944], "temperature": 0.0, "avg_logprob": -0.1080843164015186, "compression_ratio": 1.8776978417266188, "no_speech_prob": 0.0015976420836523175}, {"id": 546, "seek": 222052, "start": 2232.92, "end": 2234.92, "text": " okay", "tokens": [50984, 1392, 51084], "temperature": 0.0, "avg_logprob": -0.1080843164015186, "compression_ratio": 1.8776978417266188, "no_speech_prob": 0.0015976420836523175}, {"id": 547, "seek": 222052, "start": 2234.92, "end": 2236.92, "text": " The other reason I want to do this is not just for efficiency", "tokens": [51084, 440, 661, 1778, 286, 528, 281, 360, 341, 307, 406, 445, 337, 10493, 51184], "temperature": 0.0, "avg_logprob": -0.1080843164015186, "compression_ratio": 1.8776978417266188, "no_speech_prob": 0.0015976420836523175}, {"id": 548, "seek": 222052, "start": 2237.0, "end": 2240.68, "text": " But also I would like us to practice these n-dimensional tensors", "tokens": [51188, 583, 611, 286, 576, 411, 505, 281, 3124, 613, 297, 12, 18759, 10688, 830, 51372], "temperature": 0.0, "avg_logprob": -0.1080843164015186, "compression_ratio": 1.8776978417266188, "no_speech_prob": 0.0015976420836523175}, {"id": 549, "seek": 222052, "start": 2241.0, "end": 2246.28, "text": " And I'd like us to practice their manipulation and especially something that's called broadcasting that we'll go into in a second", "tokens": [51388, 400, 286, 1116, 411, 505, 281, 3124, 641, 26475, 293, 2318, 746, 300, 311, 1219, 30024, 300, 321, 603, 352, 666, 294, 257, 1150, 51652], "temperature": 0.0, "avg_logprob": -0.1080843164015186, "compression_ratio": 1.8776978417266188, "no_speech_prob": 0.0015976420836523175}, {"id": 550, "seek": 222052, "start": 2246.92, "end": 2250.2, "text": " We're actually going to have to become very good at these tensor manipulations", "tokens": [51684, 492, 434, 767, 516, 281, 362, 281, 1813, 588, 665, 412, 613, 40863, 9258, 4136, 51848], "temperature": 0.0, "avg_logprob": -0.1080843164015186, "compression_ratio": 1.8776978417266188, "no_speech_prob": 0.0015976420836523175}, {"id": 551, "seek": 225052, "start": 2250.52, "end": 2254.84, "text": " Because if we're going to build out all the way to transformers, we're going to be doing some pretty complicated", "tokens": [50364, 1436, 498, 321, 434, 516, 281, 1322, 484, 439, 264, 636, 281, 4088, 433, 11, 321, 434, 516, 281, 312, 884, 512, 1238, 6179, 50580], "temperature": 0.0, "avg_logprob": -0.09157758010061164, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.0006877923151478171}, {"id": 552, "seek": 225052, "start": 2255.48, "end": 2260.12, "text": " array operations for efficiency, and we need to really understand that and be very good at it", "tokens": [50612, 10225, 7705, 337, 10493, 11, 293, 321, 643, 281, 534, 1223, 300, 293, 312, 588, 665, 412, 309, 50844], "temperature": 0.0, "avg_logprob": -0.09157758010061164, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.0006877923151478171}, {"id": 553, "seek": 225052, "start": 2262.2, "end": 2265.4, "text": " So intuitively what we want to do is we first want to grab the floating point", "tokens": [50948, 407, 46506, 437, 321, 528, 281, 360, 307, 321, 700, 528, 281, 4444, 264, 12607, 935, 51108], "temperature": 0.0, "avg_logprob": -0.09157758010061164, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.0006877923151478171}, {"id": 554, "seek": 225052, "start": 2265.96, "end": 2267.96, "text": " copy of n", "tokens": [51136, 5055, 295, 297, 51236], "temperature": 0.0, "avg_logprob": -0.09157758010061164, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.0006877923151478171}, {"id": 555, "seek": 225052, "start": 2268.2, "end": 2270.2, "text": " And I'm mimicking the line here basically", "tokens": [51248, 400, 286, 478, 12247, 10401, 264, 1622, 510, 1936, 51348], "temperature": 0.0, "avg_logprob": -0.09157758010061164, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.0006877923151478171}, {"id": 556, "seek": 225052, "start": 2270.92, "end": 2275.08, "text": " And then we want to divide all the rows so that they sum to 1", "tokens": [51384, 400, 550, 321, 528, 281, 9845, 439, 264, 13241, 370, 300, 436, 2408, 281, 502, 51592], "temperature": 0.0, "avg_logprob": -0.09157758010061164, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.0006877923151478171}, {"id": 557, "seek": 225052, "start": 2275.72, "end": 2278.84, "text": " So we'd like to do something like this p divide p dot sum", "tokens": [51624, 407, 321, 1116, 411, 281, 360, 746, 411, 341, 280, 9845, 280, 5893, 2408, 51780], "temperature": 0.0, "avg_logprob": -0.09157758010061164, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.0006877923151478171}, {"id": 558, "seek": 228052, "start": 2280.6, "end": 2284.36, "text": " But now we have to be careful because p dot sum actually", "tokens": [50368, 583, 586, 321, 362, 281, 312, 5026, 570, 280, 5893, 2408, 767, 50556], "temperature": 0.0, "avg_logprob": -0.1337531687139155, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0009398823021911085}, {"id": 559, "seek": 228052, "start": 2285.4, "end": 2287.4, "text": " produces a sum", "tokens": [50608, 14725, 257, 2408, 50708], "temperature": 0.0, "avg_logprob": -0.1337531687139155, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0009398823021911085}, {"id": 560, "seek": 228052, "start": 2288.04, "end": 2292.52, "text": " Sorry p equals n dot float copy p dot sum produces a", "tokens": [50740, 4919, 280, 6915, 297, 5893, 15706, 5055, 280, 5893, 2408, 14725, 257, 50964], "temperature": 0.0, "avg_logprob": -0.1337531687139155, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0009398823021911085}, {"id": 561, "seek": 228052, "start": 2295.08, "end": 2297.64, "text": " Summs up all of the counts of this entire matrix n", "tokens": [51092, 8626, 2592, 493, 439, 295, 264, 14893, 295, 341, 2302, 8141, 297, 51220], "temperature": 0.0, "avg_logprob": -0.1337531687139155, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0009398823021911085}, {"id": 562, "seek": 228052, "start": 2298.28, "end": 2300.7599999999998, "text": " And gives us a single number of just the summation of everything", "tokens": [51252, 400, 2709, 505, 257, 2167, 1230, 295, 445, 264, 28811, 295, 1203, 51376], "temperature": 0.0, "avg_logprob": -0.1337531687139155, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0009398823021911085}, {"id": 563, "seek": 228052, "start": 2301.24, "end": 2307.08, "text": " So that's not the way we want to divide we want to simultaneously and in parallel divide all the rows", "tokens": [51400, 407, 300, 311, 406, 264, 636, 321, 528, 281, 9845, 321, 528, 281, 16561, 293, 294, 8952, 9845, 439, 264, 13241, 51692], "temperature": 0.0, "avg_logprob": -0.1337531687139155, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0009398823021911085}, {"id": 564, "seek": 228052, "start": 2307.64, "end": 2309.64, "text": " by their respective sums", "tokens": [51720, 538, 641, 23649, 34499, 51820], "temperature": 0.0, "avg_logprob": -0.1337531687139155, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0009398823021911085}, {"id": 565, "seek": 231052, "start": 2310.6, "end": 2314.7599999999998, "text": " So what we have to do now is we have to go into documentation for torch dot sum", "tokens": [50368, 407, 437, 321, 362, 281, 360, 586, 307, 321, 362, 281, 352, 666, 14333, 337, 27822, 5893, 2408, 50576], "temperature": 0.0, "avg_logprob": -0.10070710761524806, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.0007672077044844627}, {"id": 566, "seek": 231052, "start": 2315.8, "end": 2318.6, "text": " And we can scroll down here to a definition that is relevant to us", "tokens": [50628, 400, 321, 393, 11369, 760, 510, 281, 257, 7123, 300, 307, 7340, 281, 505, 50768], "temperature": 0.0, "avg_logprob": -0.10070710761524806, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.0007672077044844627}, {"id": 567, "seek": 231052, "start": 2318.68, "end": 2323.0, "text": " Which is where we don't only provide an input array that we want to sum", "tokens": [50772, 3013, 307, 689, 321, 500, 380, 787, 2893, 364, 4846, 10225, 300, 321, 528, 281, 2408, 50988], "temperature": 0.0, "avg_logprob": -0.10070710761524806, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.0007672077044844627}, {"id": 568, "seek": 231052, "start": 2323.32, "end": 2326.44, "text": " But we also provide the dimension along which we want to sum", "tokens": [51004, 583, 321, 611, 2893, 264, 10139, 2051, 597, 321, 528, 281, 2408, 51160], "temperature": 0.0, "avg_logprob": -0.10070710761524806, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.0007672077044844627}, {"id": 569, "seek": 231052, "start": 2327.24, "end": 2329.24, "text": " And in particular we want to sum up", "tokens": [51200, 400, 294, 1729, 321, 528, 281, 2408, 493, 51300], "temperature": 0.0, "avg_logprob": -0.10070710761524806, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.0007672077044844627}, {"id": 570, "seek": 231052, "start": 2329.88, "end": 2331.88, "text": " Over rows, right", "tokens": [51332, 4886, 13241, 11, 558, 51432], "temperature": 0.0, "avg_logprob": -0.10070710761524806, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.0007672077044844627}, {"id": 571, "seek": 231052, "start": 2332.36, "end": 2336.84, "text": " Now one more argument that I want you to pay attention to here is the keep them is false", "tokens": [51456, 823, 472, 544, 6770, 300, 286, 528, 291, 281, 1689, 3202, 281, 510, 307, 264, 1066, 552, 307, 7908, 51680], "temperature": 0.0, "avg_logprob": -0.10070710761524806, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.0007672077044844627}, {"id": 572, "seek": 233684, "start": 2337.8, "end": 2339.7200000000003, "text": " If keep them is true", "tokens": [50412, 759, 1066, 552, 307, 2074, 50508], "temperature": 0.0, "avg_logprob": -0.10332093380465365, "compression_ratio": 1.8355555555555556, "no_speech_prob": 0.006289047654718161}, {"id": 573, "seek": 233684, "start": 2339.7200000000003, "end": 2344.52, "text": " Then the output tensor is of the same size as input except of course the dimension along which you summed", "tokens": [50508, 1396, 264, 5598, 40863, 307, 295, 264, 912, 2744, 382, 4846, 3993, 295, 1164, 264, 10139, 2051, 597, 291, 2408, 1912, 50748], "temperature": 0.0, "avg_logprob": -0.10332093380465365, "compression_ratio": 1.8355555555555556, "no_speech_prob": 0.006289047654718161}, {"id": 574, "seek": 233684, "start": 2344.76, "end": 2346.76, "text": " Which will become just one", "tokens": [50760, 3013, 486, 1813, 445, 472, 50860], "temperature": 0.0, "avg_logprob": -0.10332093380465365, "compression_ratio": 1.8355555555555556, "no_speech_prob": 0.006289047654718161}, {"id": 575, "seek": 233684, "start": 2347.32, "end": 2350.36, "text": " But if you pass in keep them as false", "tokens": [50888, 583, 498, 291, 1320, 294, 1066, 552, 382, 7908, 51040], "temperature": 0.0, "avg_logprob": -0.10332093380465365, "compression_ratio": 1.8355555555555556, "no_speech_prob": 0.006289047654718161}, {"id": 576, "seek": 233684, "start": 2352.04, "end": 2354.04, "text": " Then this dimension is squeezed out", "tokens": [51124, 1396, 341, 10139, 307, 39470, 484, 51224], "temperature": 0.0, "avg_logprob": -0.10332093380465365, "compression_ratio": 1.8355555555555556, "no_speech_prob": 0.006289047654718161}, {"id": 577, "seek": 233684, "start": 2354.36, "end": 2358.6000000000004, "text": " And so torch dot sum not only does the sum and collapses dimension to be of size one", "tokens": [51240, 400, 370, 27822, 5893, 2408, 406, 787, 775, 264, 2408, 293, 48765, 10139, 281, 312, 295, 2744, 472, 51452], "temperature": 0.0, "avg_logprob": -0.10332093380465365, "compression_ratio": 1.8355555555555556, "no_speech_prob": 0.006289047654718161}, {"id": 578, "seek": 233684, "start": 2358.84, "end": 2363.32, "text": " But in addition it does what's called a squeeze where it squeezes out it squeezes out that dimension", "tokens": [51464, 583, 294, 4500, 309, 775, 437, 311, 1219, 257, 13578, 689, 309, 22390, 279, 484, 309, 22390, 279, 484, 300, 10139, 51688], "temperature": 0.0, "avg_logprob": -0.10332093380465365, "compression_ratio": 1.8355555555555556, "no_speech_prob": 0.006289047654718161}, {"id": 579, "seek": 236332, "start": 2363.48, "end": 2369.4, "text": " So basically what we want here is we instead want to do p dot sum of sum axis", "tokens": [50372, 407, 1936, 437, 321, 528, 510, 307, 321, 2602, 528, 281, 360, 280, 5893, 2408, 295, 2408, 10298, 50668], "temperature": 0.0, "avg_logprob": -0.16066883957904318, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.005468593444675207}, {"id": 580, "seek": 236332, "start": 2370.76, "end": 2374.2000000000003, "text": " And in particular notice that p dot shape is 27 by 27", "tokens": [50736, 400, 294, 1729, 3449, 300, 280, 5893, 3909, 307, 7634, 538, 7634, 50908], "temperature": 0.0, "avg_logprob": -0.16066883957904318, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.005468593444675207}, {"id": 581, "seek": 236332, "start": 2375.56, "end": 2381.48, "text": " So when we sum up across axis zero, then we would be taking the zero dimension and we would be summing across it", "tokens": [50976, 407, 562, 321, 2408, 493, 2108, 10298, 4018, 11, 550, 321, 576, 312, 1940, 264, 4018, 10139, 293, 321, 576, 312, 2408, 2810, 2108, 309, 51272], "temperature": 0.0, "avg_logprob": -0.16066883957904318, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.005468593444675207}, {"id": 582, "seek": 236332, "start": 2382.6000000000004, "end": 2384.6000000000004, "text": " So when keep them is true", "tokens": [51328, 407, 562, 1066, 552, 307, 2074, 51428], "temperature": 0.0, "avg_logprob": -0.16066883957904318, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.005468593444675207}, {"id": 583, "seek": 236332, "start": 2385.0, "end": 2388.44, "text": " Then this thing will not only give us the counts across", "tokens": [51448, 1396, 341, 551, 486, 406, 787, 976, 505, 264, 14893, 2108, 51620], "temperature": 0.0, "avg_logprob": -0.16066883957904318, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.005468593444675207}, {"id": 584, "seek": 236332, "start": 2389.0, "end": 2390.1200000000003, "text": " um", "tokens": [51648, 1105, 51704], "temperature": 0.0, "avg_logprob": -0.16066883957904318, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.005468593444675207}, {"id": 585, "seek": 236332, "start": 2390.1200000000003, "end": 2391.96, "text": " along the columns", "tokens": [51704, 2051, 264, 13766, 51796], "temperature": 0.0, "avg_logprob": -0.16066883957904318, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.005468593444675207}, {"id": 586, "seek": 239196, "start": 2391.96, "end": 2396.52, "text": " But notice that basically the shape of this is one by 27. We just get a row vector", "tokens": [50364, 583, 3449, 300, 1936, 264, 3909, 295, 341, 307, 472, 538, 7634, 13, 492, 445, 483, 257, 5386, 8062, 50592], "temperature": 0.0, "avg_logprob": -0.09109918353627029, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0011694170534610748}, {"id": 587, "seek": 239196, "start": 2397.4, "end": 2400.68, "text": " And the reason we get a row vector here again is because we pass in zero dimension", "tokens": [50636, 400, 264, 1778, 321, 483, 257, 5386, 8062, 510, 797, 307, 570, 321, 1320, 294, 4018, 10139, 50800], "temperature": 0.0, "avg_logprob": -0.09109918353627029, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0011694170534610748}, {"id": 588, "seek": 239196, "start": 2400.92, "end": 2403.96, "text": " So this zero dimension becomes one and we've done a sum", "tokens": [50812, 407, 341, 4018, 10139, 3643, 472, 293, 321, 600, 1096, 257, 2408, 50964], "temperature": 0.0, "avg_logprob": -0.09109918353627029, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0011694170534610748}, {"id": 589, "seek": 239196, "start": 2404.84, "end": 2407.32, "text": " And we get a row and so basically we've done the sum", "tokens": [51008, 400, 321, 483, 257, 5386, 293, 370, 1936, 321, 600, 1096, 264, 2408, 51132], "temperature": 0.0, "avg_logprob": -0.09109918353627029, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0011694170534610748}, {"id": 590, "seek": 239196, "start": 2408.12, "end": 2409.4, "text": " this way", "tokens": [51172, 341, 636, 51236], "temperature": 0.0, "avg_logprob": -0.09109918353627029, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0011694170534610748}, {"id": 591, "seek": 239196, "start": 2409.4, "end": 2412.28, "text": " Vertically and arrived at just a single one by 27", "tokens": [51236, 21044, 984, 293, 6678, 412, 445, 257, 2167, 472, 538, 7634, 51380], "temperature": 0.0, "avg_logprob": -0.09109918353627029, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0011694170534610748}, {"id": 592, "seek": 239196, "start": 2413.0, "end": 2415.0, "text": " vector of counts", "tokens": [51416, 8062, 295, 14893, 51516], "temperature": 0.0, "avg_logprob": -0.09109918353627029, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0011694170534610748}, {"id": 593, "seek": 239196, "start": 2415.32, "end": 2417.32, "text": " What happens when you take out keep them", "tokens": [51532, 708, 2314, 562, 291, 747, 484, 1066, 552, 51632], "temperature": 0.0, "avg_logprob": -0.09109918353627029, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0011694170534610748}, {"id": 594, "seek": 241732, "start": 2417.7200000000003, "end": 2425.8, "text": " Is that we just get 27 so it squeezes out that dimension and we just get a one dimensional vector of size 27", "tokens": [50384, 1119, 300, 321, 445, 483, 7634, 370, 309, 22390, 279, 484, 300, 10139, 293, 321, 445, 483, 257, 472, 18795, 8062, 295, 2744, 7634, 50788], "temperature": 0.0, "avg_logprob": -0.13025396996802027, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.003706896211951971}, {"id": 595, "seek": 241732, "start": 2428.6000000000004, "end": 2430.6000000000004, "text": " Now we don't actually want", "tokens": [50928, 823, 321, 500, 380, 767, 528, 51028], "temperature": 0.0, "avg_logprob": -0.13025396996802027, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.003706896211951971}, {"id": 596, "seek": 241732, "start": 2431.32, "end": 2436.28, "text": " One by 27 row vector because that gives us the counts or the sums across", "tokens": [51064, 1485, 538, 7634, 5386, 8062, 570, 300, 2709, 505, 264, 14893, 420, 264, 34499, 2108, 51312], "temperature": 0.0, "avg_logprob": -0.13025396996802027, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.003706896211951971}, {"id": 597, "seek": 241732, "start": 2437.56, "end": 2439.48, "text": " the columns", "tokens": [51376, 264, 13766, 51472], "temperature": 0.0, "avg_logprob": -0.13025396996802027, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.003706896211951971}, {"id": 598, "seek": 241732, "start": 2439.48, "end": 2442.2000000000003, "text": " We actually want to sum the other way along dimension one", "tokens": [51472, 492, 767, 528, 281, 2408, 264, 661, 636, 2051, 10139, 472, 51608], "temperature": 0.0, "avg_logprob": -0.13025396996802027, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.003706896211951971}, {"id": 599, "seek": 241732, "start": 2442.6800000000003, "end": 2447.0, "text": " And you'll see that the shape of this is 27 by one. So it's a column vector", "tokens": [51632, 400, 291, 603, 536, 300, 264, 3909, 295, 341, 307, 7634, 538, 472, 13, 407, 309, 311, 257, 7738, 8062, 51848], "temperature": 0.0, "avg_logprob": -0.13025396996802027, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.003706896211951971}, {"id": 600, "seek": 244732, "start": 2447.32, "end": 2449.32, "text": " It's a 27 by one", "tokens": [50364, 467, 311, 257, 7634, 538, 472, 50464], "temperature": 0.0, "avg_logprob": -0.1552872982892123, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0009696873603388667}, {"id": 601, "seek": 244732, "start": 2449.96, "end": 2451.96, "text": " vector of counts", "tokens": [50496, 8062, 295, 14893, 50596], "temperature": 0.0, "avg_logprob": -0.1552872982892123, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0009696873603388667}, {"id": 602, "seek": 244732, "start": 2452.84, "end": 2459.4, "text": " Okay, and that's because what's happened here is that we're going horizontally and this 27 by 27 matrix becomes a", "tokens": [50640, 1033, 11, 293, 300, 311, 570, 437, 311, 2011, 510, 307, 300, 321, 434, 516, 33796, 293, 341, 7634, 538, 7634, 8141, 3643, 257, 50968], "temperature": 0.0, "avg_logprob": -0.1552872982892123, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0009696873603388667}, {"id": 603, "seek": 244732, "start": 2460.04, "end": 2462.04, "text": " 27 by one array", "tokens": [51000, 7634, 538, 472, 10225, 51100], "temperature": 0.0, "avg_logprob": -0.1552872982892123, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0009696873603388667}, {"id": 604, "seek": 244732, "start": 2463.48, "end": 2465.48, "text": " Now you'll notice by the way that um", "tokens": [51172, 823, 291, 603, 3449, 538, 264, 636, 300, 1105, 51272], "temperature": 0.0, "avg_logprob": -0.1552872982892123, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0009696873603388667}, {"id": 605, "seek": 244732, "start": 2466.2000000000003, "end": 2468.2000000000003, "text": " the actual numbers", "tokens": [51308, 264, 3539, 3547, 51408], "temperature": 0.0, "avg_logprob": -0.1552872982892123, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0009696873603388667}, {"id": 606, "seek": 244732, "start": 2468.2000000000003, "end": 2470.2000000000003, "text": " Of these counts are identical", "tokens": [51408, 2720, 613, 14893, 366, 14800, 51508], "temperature": 0.0, "avg_logprob": -0.1552872982892123, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0009696873603388667}, {"id": 607, "seek": 244732, "start": 2470.6000000000004, "end": 2474.6000000000004, "text": " And that's because this special array of counts here comes from bi-gram statistics", "tokens": [51528, 400, 300, 311, 570, 341, 2121, 10225, 295, 14893, 510, 1487, 490, 3228, 12, 1342, 12523, 51728], "temperature": 0.0, "avg_logprob": -0.1552872982892123, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0009696873603388667}, {"id": 608, "seek": 247460, "start": 2474.8399999999997, "end": 2477.16, "text": " And actually it just so happens by chance", "tokens": [50376, 400, 767, 309, 445, 370, 2314, 538, 2931, 50492], "temperature": 0.0, "avg_logprob": -0.13892737638603136, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.0020188584458082914}, {"id": 609, "seek": 247460, "start": 2477.72, "end": 2482.44, "text": " Or because of the way this array is constructed that this sums along the columns or along the rows", "tokens": [50520, 1610, 570, 295, 264, 636, 341, 10225, 307, 17083, 300, 341, 34499, 2051, 264, 13766, 420, 2051, 264, 13241, 50756], "temperature": 0.0, "avg_logprob": -0.13892737638603136, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.0020188584458082914}, {"id": 610, "seek": 247460, "start": 2483.08, "end": 2485.08, "text": " Horizontally or vertically is identical", "tokens": [50788, 42141, 896, 379, 420, 28450, 307, 14800, 50888], "temperature": 0.0, "avg_logprob": -0.13892737638603136, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.0020188584458082914}, {"id": 611, "seek": 247460, "start": 2486.12, "end": 2490.52, "text": " But actually what we want to do in this case is we want to sum across the uh rows", "tokens": [50940, 583, 767, 437, 321, 528, 281, 360, 294, 341, 1389, 307, 321, 528, 281, 2408, 2108, 264, 2232, 13241, 51160], "temperature": 0.0, "avg_logprob": -0.13892737638603136, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.0020188584458082914}, {"id": 612, "seek": 247460, "start": 2491.3199999999997, "end": 2492.2799999999997, "text": " horizontally", "tokens": [51200, 33796, 51248], "temperature": 0.0, "avg_logprob": -0.13892737638603136, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.0020188584458082914}, {"id": 613, "seek": 247460, "start": 2492.2799999999997, "end": 2495.7999999999997, "text": " So what we want here is speed at some of one would keep them true", "tokens": [51248, 407, 437, 321, 528, 510, 307, 3073, 412, 512, 295, 472, 576, 1066, 552, 2074, 51424], "temperature": 0.0, "avg_logprob": -0.13892737638603136, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.0020188584458082914}, {"id": 614, "seek": 247460, "start": 2497.3199999999997, "end": 2501.72, "text": " 27 by one column vector and now what we want to do is we want to divide by that", "tokens": [51500, 7634, 538, 472, 7738, 8062, 293, 586, 437, 321, 528, 281, 360, 307, 321, 528, 281, 9845, 538, 300, 51720], "temperature": 0.0, "avg_logprob": -0.13892737638603136, "compression_ratio": 1.854625550660793, "no_speech_prob": 0.0020188584458082914}, {"id": 615, "seek": 250460, "start": 2504.68, "end": 2507.64, "text": " Now we have to be careful here again. Is it possible to take", "tokens": [50368, 823, 321, 362, 281, 312, 5026, 510, 797, 13, 1119, 309, 1944, 281, 747, 50516], "temperature": 0.0, "avg_logprob": -0.09804514139005453, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.001304427394643426}, {"id": 616, "seek": 250460, "start": 2508.8399999999997, "end": 2512.36, "text": " What's a um p dot shape you see here is 27 by 27?", "tokens": [50576, 708, 311, 257, 1105, 280, 5893, 3909, 291, 536, 510, 307, 7634, 538, 7634, 30, 50752], "temperature": 0.0, "avg_logprob": -0.09804514139005453, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.001304427394643426}, {"id": 617, "seek": 250460, "start": 2512.68, "end": 2519.56, "text": " Is it possible to take a 27 by 27 array and divide it by what is a 27 by one array?", "tokens": [50768, 1119, 309, 1944, 281, 747, 257, 7634, 538, 7634, 10225, 293, 9845, 309, 538, 437, 307, 257, 7634, 538, 472, 10225, 30, 51112], "temperature": 0.0, "avg_logprob": -0.09804514139005453, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.001304427394643426}, {"id": 618, "seek": 250460, "start": 2521.3199999999997, "end": 2523.3199999999997, "text": " Is that an operation that you can do?", "tokens": [51200, 1119, 300, 364, 6916, 300, 291, 393, 360, 30, 51300], "temperature": 0.0, "avg_logprob": -0.09804514139005453, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.001304427394643426}, {"id": 619, "seek": 250460, "start": 2523.88, "end": 2527.7999999999997, "text": " And whether or not you can perform this operation is determined by what's called broadcasting rules", "tokens": [51328, 400, 1968, 420, 406, 291, 393, 2042, 341, 6916, 307, 9540, 538, 437, 311, 1219, 30024, 4474, 51524], "temperature": 0.0, "avg_logprob": -0.09804514139005453, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.001304427394643426}, {"id": 620, "seek": 250460, "start": 2528.2799999999997, "end": 2531.08, "text": " So if you just search broadcasting semantics in torch", "tokens": [51548, 407, 498, 291, 445, 3164, 30024, 4361, 45298, 294, 27822, 51688], "temperature": 0.0, "avg_logprob": -0.09804514139005453, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.001304427394643426}, {"id": 621, "seek": 253108, "start": 2532.04, "end": 2537.48, "text": " You'll notice that there's a special definition for what's called broadcasting that uh for whether or not", "tokens": [50412, 509, 603, 3449, 300, 456, 311, 257, 2121, 7123, 337, 437, 311, 1219, 30024, 300, 2232, 337, 1968, 420, 406, 50684], "temperature": 0.0, "avg_logprob": -0.10921754837036132, "compression_ratio": 1.7630522088353413, "no_speech_prob": 0.002216813387349248}, {"id": 622, "seek": 253108, "start": 2538.2799999999997, "end": 2539.48, "text": " these two", "tokens": [50724, 613, 732, 50784], "temperature": 0.0, "avg_logprob": -0.10921754837036132, "compression_ratio": 1.7630522088353413, "no_speech_prob": 0.002216813387349248}, {"id": 623, "seek": 253108, "start": 2539.48, "end": 2542.92, "text": " Arrays can be combined in a binary operation like division", "tokens": [50784, 1587, 36212, 393, 312, 9354, 294, 257, 17434, 6916, 411, 10044, 50956], "temperature": 0.0, "avg_logprob": -0.10921754837036132, "compression_ratio": 1.7630522088353413, "no_speech_prob": 0.002216813387349248}, {"id": 624, "seek": 253108, "start": 2543.88, "end": 2547.88, "text": " So the first condition is each tensor has at least one dimension, which is the case for us", "tokens": [51004, 407, 264, 700, 4188, 307, 1184, 40863, 575, 412, 1935, 472, 10139, 11, 597, 307, 264, 1389, 337, 505, 51204], "temperature": 0.0, "avg_logprob": -0.10921754837036132, "compression_ratio": 1.7630522088353413, "no_speech_prob": 0.002216813387349248}, {"id": 625, "seek": 253108, "start": 2548.52, "end": 2551.7999999999997, "text": " And then when iterating over the dimension sizes starting at the trailing dimension", "tokens": [51236, 400, 550, 562, 17138, 990, 670, 264, 10139, 11602, 2891, 412, 264, 944, 4883, 10139, 51400], "temperature": 0.0, "avg_logprob": -0.10921754837036132, "compression_ratio": 1.7630522088353413, "no_speech_prob": 0.002216813387349248}, {"id": 626, "seek": 253108, "start": 2552.36, "end": 2556.7599999999998, "text": " The dimension sizes must either be equal one of them is one or one of them does not exist", "tokens": [51428, 440, 10139, 11602, 1633, 2139, 312, 2681, 472, 295, 552, 307, 472, 420, 472, 295, 552, 775, 406, 2514, 51648], "temperature": 0.0, "avg_logprob": -0.10921754837036132, "compression_ratio": 1.7630522088353413, "no_speech_prob": 0.002216813387349248}, {"id": 627, "seek": 255676, "start": 2556.92, "end": 2563.6400000000003, "text": " Okay, so let's do that. We need to align the two arrays and their shapes", "tokens": [50372, 1033, 11, 370, 718, 311, 360, 300, 13, 492, 643, 281, 7975, 264, 732, 41011, 293, 641, 10854, 50708], "temperature": 0.0, "avg_logprob": -0.12685810806404832, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.008984611369669437}, {"id": 628, "seek": 255676, "start": 2563.96, "end": 2567.32, "text": " Which is very easy because both of these shapes have two elements. So they're aligned", "tokens": [50724, 3013, 307, 588, 1858, 570, 1293, 295, 613, 10854, 362, 732, 4959, 13, 407, 436, 434, 17962, 50892], "temperature": 0.0, "avg_logprob": -0.12685810806404832, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.008984611369669437}, {"id": 629, "seek": 255676, "start": 2568.0400000000004, "end": 2571.48, "text": " Then we iterate over from the from the right and going to the left", "tokens": [50928, 1396, 321, 44497, 670, 490, 264, 490, 264, 558, 293, 516, 281, 264, 1411, 51100], "temperature": 0.0, "avg_logprob": -0.12685810806404832, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.008984611369669437}, {"id": 630, "seek": 255676, "start": 2572.36, "end": 2577.4, "text": " Each dimension must be either equal one of them is a one or one of them does not exist", "tokens": [51144, 6947, 10139, 1633, 312, 2139, 2681, 472, 295, 552, 307, 257, 472, 420, 472, 295, 552, 775, 406, 2514, 51396], "temperature": 0.0, "avg_logprob": -0.12685810806404832, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.008984611369669437}, {"id": 631, "seek": 255676, "start": 2577.88, "end": 2581.32, "text": " So in this case, they're not equal, but one of them is a one. So this is fine", "tokens": [51420, 407, 294, 341, 1389, 11, 436, 434, 406, 2681, 11, 457, 472, 295, 552, 307, 257, 472, 13, 407, 341, 307, 2489, 51592], "temperature": 0.0, "avg_logprob": -0.12685810806404832, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.008984611369669437}, {"id": 632, "seek": 255676, "start": 2581.88, "end": 2585.2400000000002, "text": " And then this dimension they're both equal. So this is fine", "tokens": [51620, 400, 550, 341, 10139, 436, 434, 1293, 2681, 13, 407, 341, 307, 2489, 51788], "temperature": 0.0, "avg_logprob": -0.12685810806404832, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.008984611369669437}, {"id": 633, "seek": 258524, "start": 2585.8799999999997, "end": 2591.0, "text": " So all the dimensions are fine and therefore the this operation is broadcastable", "tokens": [50396, 407, 439, 264, 12819, 366, 2489, 293, 4412, 264, 341, 6916, 307, 9975, 712, 50652], "temperature": 0.0, "avg_logprob": -0.11069372892379761, "compression_ratio": 1.7348066298342542, "no_speech_prob": 0.00359327532351017}, {"id": 634, "seek": 258524, "start": 2591.8799999999997, "end": 2593.8799999999997, "text": " So that means that this operation is allowed", "tokens": [50696, 407, 300, 1355, 300, 341, 6916, 307, 4350, 50796], "temperature": 0.0, "avg_logprob": -0.11069372892379761, "compression_ratio": 1.7348066298342542, "no_speech_prob": 0.00359327532351017}, {"id": 635, "seek": 258524, "start": 2594.52, "end": 2599.16, "text": " And what is it that these arrays do when you divide 27 by 27 by 27 by 1?", "tokens": [50828, 400, 437, 307, 309, 300, 613, 41011, 360, 562, 291, 9845, 7634, 538, 7634, 538, 7634, 538, 502, 30, 51060], "temperature": 0.0, "avg_logprob": -0.11069372892379761, "compression_ratio": 1.7348066298342542, "no_speech_prob": 0.00359327532351017}, {"id": 636, "seek": 258524, "start": 2599.7999999999997, "end": 2604.8399999999997, "text": " What it does is that it takes this dimension one and it stretches it out it copies it", "tokens": [51092, 708, 309, 775, 307, 300, 309, 2516, 341, 10139, 472, 293, 309, 29058, 309, 484, 309, 14341, 309, 51344], "temperature": 0.0, "avg_logprob": -0.11069372892379761, "compression_ratio": 1.7348066298342542, "no_speech_prob": 0.00359327532351017}, {"id": 637, "seek": 258524, "start": 2605.8799999999997, "end": 2607.3199999999997, "text": " to match", "tokens": [51396, 281, 2995, 51468], "temperature": 0.0, "avg_logprob": -0.11069372892379761, "compression_ratio": 1.7348066298342542, "no_speech_prob": 0.00359327532351017}, {"id": 638, "seek": 258524, "start": 2607.3199999999997, "end": 2609.0, "text": " 27 here in this case", "tokens": [51468, 7634, 510, 294, 341, 1389, 51552], "temperature": 0.0, "avg_logprob": -0.11069372892379761, "compression_ratio": 1.7348066298342542, "no_speech_prob": 0.00359327532351017}, {"id": 639, "seek": 260900, "start": 2609.0, "end": 2615.64, "text": " So in our case it takes this column vector, which is 27 by 1 and it copies it 27 times", "tokens": [50364, 407, 294, 527, 1389, 309, 2516, 341, 7738, 8062, 11, 597, 307, 7634, 538, 502, 293, 309, 14341, 309, 7634, 1413, 50696], "temperature": 0.0, "avg_logprob": -0.09387365321523135, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0013457861496135592}, {"id": 640, "seek": 260900, "start": 2616.68, "end": 2617.8, "text": " to make", "tokens": [50748, 281, 652, 50804], "temperature": 0.0, "avg_logprob": -0.09387365321523135, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0013457861496135592}, {"id": 641, "seek": 260900, "start": 2617.8, "end": 2623.48, "text": " These both be 27 by 27 internally you can think of it that way and so it copies those counts", "tokens": [50804, 1981, 1293, 312, 7634, 538, 7634, 19501, 291, 393, 519, 295, 309, 300, 636, 293, 370, 309, 14341, 729, 14893, 51088], "temperature": 0.0, "avg_logprob": -0.09387365321523135, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0013457861496135592}, {"id": 642, "seek": 260900, "start": 2624.2, "end": 2626.2, "text": " And then it does an element wise division", "tokens": [51124, 400, 550, 309, 775, 364, 4478, 10829, 10044, 51224], "temperature": 0.0, "avg_logprob": -0.09387365321523135, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0013457861496135592}, {"id": 643, "seek": 260900, "start": 2627.32, "end": 2633.64, "text": " Which is what we want because these counts we want to divide by them on every single one of these columns in this matrix", "tokens": [51280, 3013, 307, 437, 321, 528, 570, 613, 14893, 321, 528, 281, 9845, 538, 552, 322, 633, 2167, 472, 295, 613, 13766, 294, 341, 8141, 51596], "temperature": 0.0, "avg_logprob": -0.09387365321523135, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0013457861496135592}, {"id": 644, "seek": 260900, "start": 2634.76, "end": 2637.08, "text": " So this actually we expect will normalize", "tokens": [51652, 407, 341, 767, 321, 2066, 486, 2710, 1125, 51768], "temperature": 0.0, "avg_logprob": -0.09387365321523135, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0013457861496135592}, {"id": 645, "seek": 263708, "start": 2637.72, "end": 2639.72, "text": " every single row", "tokens": [50396, 633, 2167, 5386, 50496], "temperature": 0.0, "avg_logprob": -0.10937458735245925, "compression_ratio": 1.78125, "no_speech_prob": 0.008186997845768929}, {"id": 646, "seek": 263708, "start": 2639.72, "end": 2644.7599999999998, "text": " And we can check that this is true by taking the first row for example and taking it some", "tokens": [50496, 400, 321, 393, 1520, 300, 341, 307, 2074, 538, 1940, 264, 700, 5386, 337, 1365, 293, 1940, 309, 512, 50748], "temperature": 0.0, "avg_logprob": -0.10937458735245925, "compression_ratio": 1.78125, "no_speech_prob": 0.008186997845768929}, {"id": 647, "seek": 263708, "start": 2645.3199999999997, "end": 2647.3199999999997, "text": " We expect this to be one", "tokens": [50776, 492, 2066, 341, 281, 312, 472, 50876], "temperature": 0.0, "avg_logprob": -0.10937458735245925, "compression_ratio": 1.78125, "no_speech_prob": 0.008186997845768929}, {"id": 648, "seek": 263708, "start": 2648.2, "end": 2650.2, "text": " Because it's now normalized", "tokens": [50920, 1436, 309, 311, 586, 48704, 51020], "temperature": 0.0, "avg_logprob": -0.10937458735245925, "compression_ratio": 1.78125, "no_speech_prob": 0.008186997845768929}, {"id": 649, "seek": 263708, "start": 2650.2799999999997, "end": 2652.2799999999997, "text": " And then we expect this now", "tokens": [51024, 400, 550, 321, 2066, 341, 586, 51124], "temperature": 0.0, "avg_logprob": -0.10937458735245925, "compression_ratio": 1.78125, "no_speech_prob": 0.008186997845768929}, {"id": 650, "seek": 263708, "start": 2652.68, "end": 2658.44, "text": " Because if we actually correctly normalize all the rows we expect to get the exact same result here. So let's run this", "tokens": [51144, 1436, 498, 321, 767, 8944, 2710, 1125, 439, 264, 13241, 321, 2066, 281, 483, 264, 1900, 912, 1874, 510, 13, 407, 718, 311, 1190, 341, 51432], "temperature": 0.0, "avg_logprob": -0.10937458735245925, "compression_ratio": 1.78125, "no_speech_prob": 0.008186997845768929}, {"id": 651, "seek": 263708, "start": 2659.16, "end": 2661.16, "text": " It's the exact same result", "tokens": [51468, 467, 311, 264, 1900, 912, 1874, 51568], "temperature": 0.0, "avg_logprob": -0.10937458735245925, "compression_ratio": 1.78125, "no_speech_prob": 0.008186997845768929}, {"id": 652, "seek": 263708, "start": 2661.3199999999997, "end": 2664.68, "text": " So this is correct. So now I would like to scare you a little bit", "tokens": [51576, 407, 341, 307, 3006, 13, 407, 586, 286, 576, 411, 281, 17185, 291, 257, 707, 857, 51744], "temperature": 0.0, "avg_logprob": -0.10937458735245925, "compression_ratio": 1.78125, "no_speech_prob": 0.008186997845768929}, {"id": 653, "seek": 266468, "start": 2665.3999999999996, "end": 2669.7999999999997, "text": " You actually have to like I basically encourage you very strongly to read through broadcasting semantics", "tokens": [50400, 509, 767, 362, 281, 411, 286, 1936, 5373, 291, 588, 10613, 281, 1401, 807, 30024, 4361, 45298, 50620], "temperature": 0.0, "avg_logprob": -0.10295661829285703, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.01690981723368168}, {"id": 654, "seek": 266468, "start": 2670.44, "end": 2673.7999999999997, "text": " And I encourage you to treat this with respect and it's not something to play", "tokens": [50652, 400, 286, 5373, 291, 281, 2387, 341, 365, 3104, 293, 309, 311, 406, 746, 281, 862, 50820], "temperature": 0.0, "avg_logprob": -0.10295661829285703, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.01690981723368168}, {"id": 655, "seek": 266468, "start": 2674.2799999999997, "end": 2677.96, "text": " Fast and loose with it's something to really respect really understand and look up", "tokens": [50844, 15968, 293, 9612, 365, 309, 311, 746, 281, 534, 3104, 534, 1223, 293, 574, 493, 51028], "temperature": 0.0, "avg_logprob": -0.10295661829285703, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.01690981723368168}, {"id": 656, "seek": 266468, "start": 2678.04, "end": 2683.3999999999996, "text": " Maybe some tutorials for broadcasting and practice it and be careful with it because you can very quickly run it to box", "tokens": [51032, 2704, 512, 17616, 337, 30024, 293, 3124, 309, 293, 312, 5026, 365, 309, 570, 291, 393, 588, 2661, 1190, 309, 281, 2424, 51300], "temperature": 0.0, "avg_logprob": -0.10295661829285703, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.01690981723368168}, {"id": 657, "seek": 266468, "start": 2683.72, "end": 2685.72, "text": " Let me show you what I mean", "tokens": [51316, 961, 385, 855, 291, 437, 286, 914, 51416], "temperature": 0.0, "avg_logprob": -0.10295661829285703, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.01690981723368168}, {"id": 658, "seek": 266468, "start": 2687.16, "end": 2689.7999999999997, "text": " You see how here we have p. That's some of one keep them this true", "tokens": [51488, 509, 536, 577, 510, 321, 362, 280, 13, 663, 311, 512, 295, 472, 1066, 552, 341, 2074, 51620], "temperature": 0.0, "avg_logprob": -0.10295661829285703, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.01690981723368168}, {"id": 659, "seek": 266468, "start": 2690.52, "end": 2692.52, "text": " The shape of this is 27 by 1", "tokens": [51656, 440, 3909, 295, 341, 307, 7634, 538, 502, 51756], "temperature": 0.0, "avg_logprob": -0.10295661829285703, "compression_ratio": 1.7551724137931035, "no_speech_prob": 0.01690981723368168}, {"id": 660, "seek": 269252, "start": 2692.92, "end": 2697.88, "text": " Let me take out this line just so we have the n and then we can see the counts", "tokens": [50384, 961, 385, 747, 484, 341, 1622, 445, 370, 321, 362, 264, 297, 293, 550, 321, 393, 536, 264, 14893, 50632], "temperature": 0.0, "avg_logprob": -0.1511379556918363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.004006918054074049}, {"id": 661, "seek": 269252, "start": 2698.36, "end": 2701.64, "text": " We can see that this is a all the counts across all the", "tokens": [50656, 492, 393, 536, 300, 341, 307, 257, 439, 264, 14893, 2108, 439, 264, 50820], "temperature": 0.0, "avg_logprob": -0.1511379556918363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.004006918054074049}, {"id": 662, "seek": 269252, "start": 2702.52, "end": 2703.48, "text": " rows", "tokens": [50864, 13241, 50912], "temperature": 0.0, "avg_logprob": -0.1511379556918363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.004006918054074049}, {"id": 663, "seek": 269252, "start": 2703.48, "end": 2705.64, "text": " And it's a 27 by 1 column vector, right?", "tokens": [50912, 400, 309, 311, 257, 7634, 538, 502, 7738, 8062, 11, 558, 30, 51020], "temperature": 0.0, "avg_logprob": -0.1511379556918363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.004006918054074049}, {"id": 664, "seek": 269252, "start": 2707.08, "end": 2712.6, "text": " Now suppose that I tried to do the following but I erase keep them this true here", "tokens": [51092, 823, 7297, 300, 286, 3031, 281, 360, 264, 3480, 457, 286, 23525, 1066, 552, 341, 2074, 510, 51368], "temperature": 0.0, "avg_logprob": -0.1511379556918363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.004006918054074049}, {"id": 665, "seek": 269252, "start": 2713.88, "end": 2716.7599999999998, "text": " What does that do if keep them is not true? It's false", "tokens": [51432, 708, 775, 300, 360, 498, 1066, 552, 307, 406, 2074, 30, 467, 311, 7908, 51576], "temperature": 0.0, "avg_logprob": -0.1511379556918363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.004006918054074049}, {"id": 666, "seek": 269252, "start": 2717.08, "end": 2722.36, "text": " Then remember according to documentation it gets rid of this dimension one. It squeezes it out", "tokens": [51592, 1396, 1604, 4650, 281, 14333, 309, 2170, 3973, 295, 341, 10139, 472, 13, 467, 22390, 279, 309, 484, 51856], "temperature": 0.0, "avg_logprob": -0.1511379556918363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.004006918054074049}, {"id": 667, "seek": 272252, "start": 2722.92, "end": 2726.04, "text": " So basically we just get all the same counts the same result", "tokens": [50384, 407, 1936, 321, 445, 483, 439, 264, 912, 14893, 264, 912, 1874, 50540], "temperature": 0.0, "avg_logprob": -0.14197344346479937, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0017544702859595418}, {"id": 668, "seek": 272252, "start": 2726.52, "end": 2730.84, "text": " Except the shape of it is not 27 by 1. It is just 27 the one disappears", "tokens": [50564, 16192, 264, 3909, 295, 309, 307, 406, 7634, 538, 502, 13, 467, 307, 445, 7634, 264, 472, 25527, 50780], "temperature": 0.0, "avg_logprob": -0.14197344346479937, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0017544702859595418}, {"id": 669, "seek": 272252, "start": 2731.72, "end": 2733.72, "text": " But all the counts are the same", "tokens": [50824, 583, 439, 264, 14893, 366, 264, 912, 50924], "temperature": 0.0, "avg_logprob": -0.14197344346479937, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0017544702859595418}, {"id": 670, "seek": 272252, "start": 2734.2, "end": 2737.32, "text": " So you'd think that this divide that", "tokens": [50948, 407, 291, 1116, 519, 300, 341, 9845, 300, 51104], "temperature": 0.0, "avg_logprob": -0.14197344346479937, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0017544702859595418}, {"id": 671, "seek": 272252, "start": 2737.96, "end": 2739.96, "text": " would uh would work", "tokens": [51136, 576, 2232, 576, 589, 51236], "temperature": 0.0, "avg_logprob": -0.14197344346479937, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0017544702859595418}, {"id": 672, "seek": 272252, "start": 2740.04, "end": 2745.96, "text": " First of all, can we even uh write this and will it is it even is it even expected to run? Is it broadcastable?", "tokens": [51240, 2386, 295, 439, 11, 393, 321, 754, 2232, 2464, 341, 293, 486, 309, 307, 309, 754, 307, 309, 754, 5176, 281, 1190, 30, 1119, 309, 9975, 712, 30, 51536], "temperature": 0.0, "avg_logprob": -0.14197344346479937, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0017544702859595418}, {"id": 673, "seek": 272252, "start": 2746.28, "end": 2748.28, "text": " Let's determine if this result is broadcastable", "tokens": [51552, 961, 311, 6997, 498, 341, 1874, 307, 9975, 712, 51652], "temperature": 0.0, "avg_logprob": -0.14197344346479937, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0017544702859595418}, {"id": 674, "seek": 272252, "start": 2749.16, "end": 2751.16, "text": " p dot summit one is shape", "tokens": [51696, 280, 5893, 21564, 472, 307, 3909, 51796], "temperature": 0.0, "avg_logprob": -0.14197344346479937, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0017544702859595418}, {"id": 675, "seek": 275116, "start": 2751.56, "end": 2755.64, "text": " Is 27 this is 27 by 27 so 27 by 27", "tokens": [50384, 1119, 7634, 341, 307, 7634, 538, 7634, 370, 7634, 538, 7634, 50588], "temperature": 0.0, "avg_logprob": -0.1508709480022562, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0049049705266952515}, {"id": 676, "seek": 275116, "start": 2757.72, "end": 2759.72, "text": " Broadcasting into 27", "tokens": [50692, 14074, 48860, 666, 7634, 50792], "temperature": 0.0, "avg_logprob": -0.1508709480022562, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0049049705266952515}, {"id": 677, "seek": 275116, "start": 2760.2799999999997, "end": 2761.72, "text": " so now", "tokens": [50820, 370, 586, 50892], "temperature": 0.0, "avg_logprob": -0.1508709480022562, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0049049705266952515}, {"id": 678, "seek": 275116, "start": 2761.72, "end": 2765.72, "text": " rules of broadcasting number one align all the dimensions on the right done", "tokens": [50892, 4474, 295, 30024, 1230, 472, 7975, 439, 264, 12819, 322, 264, 558, 1096, 51092], "temperature": 0.0, "avg_logprob": -0.1508709480022562, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0049049705266952515}, {"id": 679, "seek": 275116, "start": 2766.2799999999997, "end": 2769.3999999999996, "text": " Now iteration over all the dimensions starting from the right going to the left", "tokens": [51120, 823, 24784, 670, 439, 264, 12819, 2891, 490, 264, 558, 516, 281, 264, 1411, 51276], "temperature": 0.0, "avg_logprob": -0.1508709480022562, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0049049705266952515}, {"id": 680, "seek": 275116, "start": 2770.2, "end": 2772.2, "text": " All the dimensions must either be equal", "tokens": [51316, 1057, 264, 12819, 1633, 2139, 312, 2681, 51416], "temperature": 0.0, "avg_logprob": -0.1508709480022562, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0049049705266952515}, {"id": 681, "seek": 275116, "start": 2773.0, "end": 2777.16, "text": " One of them must be one or one of them does not exist. So here they are all equal", "tokens": [51456, 1485, 295, 552, 1633, 312, 472, 420, 472, 295, 552, 775, 406, 2514, 13, 407, 510, 436, 366, 439, 2681, 51664], "temperature": 0.0, "avg_logprob": -0.1508709480022562, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0049049705266952515}, {"id": 682, "seek": 277716, "start": 2777.72, "end": 2779.72, "text": " Here the dimension does not exist", "tokens": [50392, 1692, 264, 10139, 775, 406, 2514, 50492], "temperature": 0.0, "avg_logprob": -0.12309678395589192, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.007344967685639858}, {"id": 683, "seek": 277716, "start": 2779.96, "end": 2783.48, "text": " So internally what broadcasting will do is it will create a one here", "tokens": [50504, 407, 19501, 437, 30024, 486, 360, 307, 309, 486, 1884, 257, 472, 510, 50680], "temperature": 0.0, "avg_logprob": -0.12309678395589192, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.007344967685639858}, {"id": 684, "seek": 277716, "start": 2784.2799999999997, "end": 2785.8799999999997, "text": " and then", "tokens": [50720, 293, 550, 50800], "temperature": 0.0, "avg_logprob": -0.12309678395589192, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.007344967685639858}, {"id": 685, "seek": 277716, "start": 2785.8799999999997, "end": 2791.08, "text": " We see that one of them is a one and this will get copied and this will run this will broadcast", "tokens": [50800, 492, 536, 300, 472, 295, 552, 307, 257, 472, 293, 341, 486, 483, 25365, 293, 341, 486, 1190, 341, 486, 9975, 51060], "temperature": 0.0, "avg_logprob": -0.12309678395589192, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.007344967685639858}, {"id": 686, "seek": 277716, "start": 2792.44, "end": 2795.3199999999997, "text": " Okay, so you'd expect this to work", "tokens": [51128, 1033, 11, 370, 291, 1116, 2066, 341, 281, 589, 51272], "temperature": 0.0, "avg_logprob": -0.12309678395589192, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.007344967685639858}, {"id": 687, "seek": 277716, "start": 2797.3199999999997, "end": 2799.3199999999997, "text": " Because we we are um", "tokens": [51372, 1436, 321, 321, 366, 1105, 51472], "temperature": 0.0, "avg_logprob": -0.12309678395589192, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.007344967685639858}, {"id": 688, "seek": 277716, "start": 2799.8799999999997, "end": 2801.3199999999997, "text": " um", "tokens": [51500, 1105, 51572], "temperature": 0.0, "avg_logprob": -0.12309678395589192, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.007344967685639858}, {"id": 689, "seek": 277716, "start": 2801.3199999999997, "end": 2805.72, "text": " This broadcasts and this we can divide this now if I run this you'd expect it to work but", "tokens": [51572, 639, 9975, 82, 293, 341, 321, 393, 9845, 341, 586, 498, 286, 1190, 341, 291, 1116, 2066, 309, 281, 589, 457, 51792], "temperature": 0.0, "avg_logprob": -0.12309678395589192, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.007344967685639858}, {"id": 690, "seek": 280572, "start": 2806.6, "end": 2807.9599999999996, "text": " It doesn't", "tokens": [50408, 467, 1177, 380, 50476], "temperature": 0.0, "avg_logprob": -0.14933069740853658, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0023595080710947514}, {"id": 691, "seek": 280572, "start": 2807.9599999999996, "end": 2811.7999999999997, "text": " Uh, you actually get garbage you get a wrong result because this is actually a bug", "tokens": [50476, 4019, 11, 291, 767, 483, 14150, 291, 483, 257, 2085, 1874, 570, 341, 307, 767, 257, 7426, 50668], "temperature": 0.0, "avg_logprob": -0.14933069740853658, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0023595080710947514}, {"id": 692, "seek": 280572, "start": 2812.4399999999996, "end": 2814.4399999999996, "text": " This keep them for equals true", "tokens": [50700, 639, 1066, 552, 337, 6915, 2074, 50800], "temperature": 0.0, "avg_logprob": -0.14933069740853658, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0023595080710947514}, {"id": 693, "seek": 280572, "start": 2817.24, "end": 2819.24, "text": " Makes it work", "tokens": [50940, 25245, 309, 589, 51040], "temperature": 0.0, "avg_logprob": -0.14933069740853658, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0023595080710947514}, {"id": 694, "seek": 280572, "start": 2820.6, "end": 2822.6, "text": " This is a bug", "tokens": [51108, 639, 307, 257, 7426, 51208], "temperature": 0.0, "avg_logprob": -0.14933069740853658, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0023595080710947514}, {"id": 695, "seek": 280572, "start": 2822.8399999999997, "end": 2828.2, "text": " In both cases we are doing the correct counts. We are summing up across the rows", "tokens": [51220, 682, 1293, 3331, 321, 366, 884, 264, 3006, 14893, 13, 492, 366, 2408, 2810, 493, 2108, 264, 13241, 51488], "temperature": 0.0, "avg_logprob": -0.14933069740853658, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0023595080710947514}, {"id": 696, "seek": 280572, "start": 2829.3199999999997, "end": 2832.12, "text": " But keep them is saving us and making it work. So in this case", "tokens": [51544, 583, 1066, 552, 307, 6816, 505, 293, 1455, 309, 589, 13, 407, 294, 341, 1389, 51684], "temperature": 0.0, "avg_logprob": -0.14933069740853658, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0023595080710947514}, {"id": 697, "seek": 283212, "start": 2832.7599999999998, "end": 2837.96, "text": " I'd like you to encourage you to potentially like pause this video at this point and try to think about why this is buggy", "tokens": [50396, 286, 1116, 411, 291, 281, 5373, 291, 281, 7263, 411, 10465, 341, 960, 412, 341, 935, 293, 853, 281, 519, 466, 983, 341, 307, 7426, 1480, 50656], "temperature": 0.0, "avg_logprob": -0.11978733867680261, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.008576082065701485}, {"id": 698, "seek": 283212, "start": 2838.3599999999997, "end": 2840.3599999999997, "text": " And why the keep them was necessary here", "tokens": [50676, 400, 983, 264, 1066, 552, 390, 4818, 510, 50776], "temperature": 0.0, "avg_logprob": -0.11978733867680261, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.008576082065701485}, {"id": 699, "seek": 283212, "start": 2842.2, "end": 2843.08, "text": " Okay", "tokens": [50868, 1033, 50912], "temperature": 0.0, "avg_logprob": -0.11978733867680261, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.008576082065701485}, {"id": 700, "seek": 283212, "start": 2843.08, "end": 2848.7599999999998, "text": " So the reason to do for this is I'm trying to hint it here when I was sort of giving you a bit of a hint on how this works", "tokens": [50912, 407, 264, 1778, 281, 360, 337, 341, 307, 286, 478, 1382, 281, 12075, 309, 510, 562, 286, 390, 1333, 295, 2902, 291, 257, 857, 295, 257, 12075, 322, 577, 341, 1985, 51196], "temperature": 0.0, "avg_logprob": -0.11978733867680261, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.008576082065701485}, {"id": 701, "seek": 283212, "start": 2849.48, "end": 2851.48, "text": " this 27 vector", "tokens": [51232, 341, 7634, 8062, 51332], "temperature": 0.0, "avg_logprob": -0.11978733867680261, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.008576082065701485}, {"id": 702, "seek": 283212, "start": 2852.2, "end": 2854.2, "text": " Internally inside the broadcasting", "tokens": [51368, 4844, 379, 1854, 264, 30024, 51468], "temperature": 0.0, "avg_logprob": -0.11978733867680261, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.008576082065701485}, {"id": 703, "seek": 283212, "start": 2854.2, "end": 2856.2, "text": " This becomes a one by 27", "tokens": [51468, 639, 3643, 257, 472, 538, 7634, 51568], "temperature": 0.0, "avg_logprob": -0.11978733867680261, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.008576082065701485}, {"id": 704, "seek": 283212, "start": 2856.52, "end": 2859.0, "text": " And one by 27 is a row vector, right?", "tokens": [51584, 400, 472, 538, 7634, 307, 257, 5386, 8062, 11, 558, 30, 51708], "temperature": 0.0, "avg_logprob": -0.11978733867680261, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.008576082065701485}, {"id": 705, "seek": 285900, "start": 2859.64, "end": 2862.6, "text": " And now we are dividing 27 by 27 by 1 by 27", "tokens": [50396, 400, 586, 321, 366, 26764, 7634, 538, 7634, 538, 502, 538, 7634, 50544], "temperature": 0.0, "avg_logprob": -0.16152582168579102, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.002511195372790098}, {"id": 706, "seek": 285900, "start": 2863.16, "end": 2868.52, "text": " And torch will replicate this dimension. So basically, uh, it will take", "tokens": [50572, 400, 27822, 486, 25356, 341, 10139, 13, 407, 1936, 11, 2232, 11, 309, 486, 747, 50840], "temperature": 0.0, "avg_logprob": -0.16152582168579102, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.002511195372790098}, {"id": 707, "seek": 285900, "start": 2869.72, "end": 2874.44, "text": " It will take this, uh row vector and it will copy it vertically now", "tokens": [50900, 467, 486, 747, 341, 11, 2232, 5386, 8062, 293, 309, 486, 5055, 309, 28450, 586, 51136], "temperature": 0.0, "avg_logprob": -0.16152582168579102, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.002511195372790098}, {"id": 708, "seek": 285900, "start": 2875.4, "end": 2879.32, "text": " 27 times so the 27 by 27 lines exactly an element wise divides", "tokens": [51184, 7634, 1413, 370, 264, 7634, 538, 7634, 3876, 2293, 364, 4478, 10829, 41347, 51380], "temperature": 0.0, "avg_logprob": -0.16152582168579102, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.002511195372790098}, {"id": 709, "seek": 285900, "start": 2880.36, "end": 2882.92, "text": " And so basically what's happening here is um", "tokens": [51432, 400, 370, 1936, 437, 311, 2737, 510, 307, 1105, 51560], "temperature": 0.0, "avg_logprob": -0.16152582168579102, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.002511195372790098}, {"id": 710, "seek": 285900, "start": 2884.36, "end": 2887.56, "text": " We're actually normalizing the columns instead of normalizing the rows", "tokens": [51632, 492, 434, 767, 2710, 3319, 264, 13766, 2602, 295, 2710, 3319, 264, 13241, 51792], "temperature": 0.0, "avg_logprob": -0.16152582168579102, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.002511195372790098}, {"id": 711, "seek": 288900, "start": 2889.4, "end": 2895.64, "text": " So you can check that what's happening here is that p at zero, which is the first row of p that sum", "tokens": [50384, 407, 291, 393, 1520, 300, 437, 311, 2737, 510, 307, 300, 280, 412, 4018, 11, 597, 307, 264, 700, 5386, 295, 280, 300, 2408, 50696], "temperature": 0.0, "avg_logprob": -0.09679660991746553, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.001867440645582974}, {"id": 712, "seek": 288900, "start": 2896.28, "end": 2898.28, "text": " Is not one it's seven", "tokens": [50728, 1119, 406, 472, 309, 311, 3407, 50828], "temperature": 0.0, "avg_logprob": -0.09679660991746553, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.001867440645582974}, {"id": 713, "seek": 288900, "start": 2898.6, "end": 2901.64, "text": " It is the first column as an example that sums to one", "tokens": [50844, 467, 307, 264, 700, 7738, 382, 364, 1365, 300, 34499, 281, 472, 50996], "temperature": 0.0, "avg_logprob": -0.09679660991746553, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.001867440645582974}, {"id": 714, "seek": 288900, "start": 2903.64, "end": 2909.32, "text": " So to summarize where does the issue come from the issue comes from the silent adding of a dimension here", "tokens": [51096, 407, 281, 20858, 689, 775, 264, 2734, 808, 490, 264, 2734, 1487, 490, 264, 12784, 5127, 295, 257, 10139, 510, 51380], "temperature": 0.0, "avg_logprob": -0.09679660991746553, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.001867440645582974}, {"id": 715, "seek": 288900, "start": 2909.72, "end": 2915.56, "text": " Because in broadcasting rules you align on the right and go from right to left and if dimension doesn't exist you create it", "tokens": [51400, 1436, 294, 30024, 4474, 291, 7975, 322, 264, 558, 293, 352, 490, 558, 281, 1411, 293, 498, 10139, 1177, 380, 2514, 291, 1884, 309, 51692], "temperature": 0.0, "avg_logprob": -0.09679660991746553, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.001867440645582974}, {"id": 716, "seek": 291556, "start": 2916.2, "end": 2917.88, "text": " So that's where the problem happens", "tokens": [50396, 407, 300, 311, 689, 264, 1154, 2314, 50480], "temperature": 0.0, "avg_logprob": -0.14612621543681728, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.005059645511209965}, {"id": 717, "seek": 291556, "start": 2917.88, "end": 2919.32, "text": " We still did the counts correctly", "tokens": [50480, 492, 920, 630, 264, 14893, 8944, 50552], "temperature": 0.0, "avg_logprob": -0.14612621543681728, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.005059645511209965}, {"id": 718, "seek": 291556, "start": 2919.32, "end": 2925.16, "text": " We did the counts across the rows and we got the the counts on the right here as a column vector", "tokens": [50552, 492, 630, 264, 14893, 2108, 264, 13241, 293, 321, 658, 264, 264, 14893, 322, 264, 558, 510, 382, 257, 7738, 8062, 50844], "temperature": 0.0, "avg_logprob": -0.14612621543681728, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.005059645511209965}, {"id": 719, "seek": 291556, "start": 2925.56, "end": 2930.84, "text": " But because the keep things was true this this uh, this dimension was discarded and now we just have a vector of 27", "tokens": [50864, 583, 570, 264, 1066, 721, 390, 2074, 341, 341, 2232, 11, 341, 10139, 390, 45469, 293, 586, 321, 445, 362, 257, 8062, 295, 7634, 51128], "temperature": 0.0, "avg_logprob": -0.14612621543681728, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.005059645511209965}, {"id": 720, "seek": 291556, "start": 2931.48, "end": 2936.36, "text": " And because of broadcasting the way it works this vector of 27 suddenly becomes a row vector", "tokens": [51160, 400, 570, 295, 30024, 264, 636, 309, 1985, 341, 8062, 295, 7634, 5800, 3643, 257, 5386, 8062, 51404], "temperature": 0.0, "avg_logprob": -0.14612621543681728, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.005059645511209965}, {"id": 721, "seek": 291556, "start": 2937.0, "end": 2939.0, "text": " And then this row vector gets replicated", "tokens": [51436, 400, 550, 341, 5386, 8062, 2170, 46365, 51536], "temperature": 0.0, "avg_logprob": -0.14612621543681728, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.005059645511209965}, {"id": 722, "seek": 291556, "start": 2939.0, "end": 2943.32, "text": " Vertically and at every single point we are dividing by the by the count", "tokens": [51536, 21044, 984, 293, 412, 633, 2167, 935, 321, 366, 26764, 538, 264, 538, 264, 1207, 51752], "temperature": 0.0, "avg_logprob": -0.14612621543681728, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.005059645511209965}, {"id": 723, "seek": 294332, "start": 2944.28, "end": 2946.28, "text": " Uh in the opposite direction", "tokens": [50412, 4019, 294, 264, 6182, 3513, 50512], "temperature": 0.0, "avg_logprob": -0.2000248309263249, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.003272820496931672}, {"id": 724, "seek": 294332, "start": 2947.4, "end": 2953.4, "text": " So, uh, so this thing just uh, doesn't work. You this needs to be keep them's equals true in this case", "tokens": [50568, 407, 11, 2232, 11, 370, 341, 551, 445, 2232, 11, 1177, 380, 589, 13, 509, 341, 2203, 281, 312, 1066, 552, 311, 6915, 2074, 294, 341, 1389, 50868], "temperature": 0.0, "avg_logprob": -0.2000248309263249, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.003272820496931672}, {"id": 725, "seek": 294332, "start": 2954.2000000000003, "end": 2955.56, "text": " so then", "tokens": [50908, 370, 550, 50976], "temperature": 0.0, "avg_logprob": -0.2000248309263249, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.003272820496931672}, {"id": 726, "seek": 294332, "start": 2955.56, "end": 2958.44, "text": " Uh, then we have that p at zero is normalized", "tokens": [50976, 4019, 11, 550, 321, 362, 300, 280, 412, 4018, 307, 48704, 51120], "temperature": 0.0, "avg_logprob": -0.2000248309263249, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.003272820496931672}, {"id": 727, "seek": 294332, "start": 2959.88, "end": 2963.1600000000003, "text": " And conversely the first column you'd expect to potentially not be normalized", "tokens": [51192, 400, 2615, 736, 264, 700, 7738, 291, 1116, 2066, 281, 7263, 406, 312, 48704, 51356], "temperature": 0.0, "avg_logprob": -0.2000248309263249, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.003272820496931672}, {"id": 728, "seek": 294332, "start": 2964.6000000000004, "end": 2966.6000000000004, "text": " And this is what makes it work", "tokens": [51428, 400, 341, 307, 437, 1669, 309, 589, 51528], "temperature": 0.0, "avg_logprob": -0.2000248309263249, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.003272820496931672}, {"id": 729, "seek": 294332, "start": 2967.7200000000003, "end": 2972.92, "text": " So pretty subtle and uh, hopefully this helps to scare you that you should", "tokens": [51584, 407, 1238, 13743, 293, 2232, 11, 4696, 341, 3665, 281, 17185, 291, 300, 291, 820, 51844], "temperature": 0.0, "avg_logprob": -0.2000248309263249, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.003272820496931672}, {"id": 730, "seek": 297292, "start": 2973.0, "end": 2975.64, "text": " Have a respect for broadcasting be careful. Check your work", "tokens": [50368, 3560, 257, 3104, 337, 30024, 312, 5026, 13, 6881, 428, 589, 50500], "temperature": 0.0, "avg_logprob": -0.14298156556629, "compression_ratio": 1.6824817518248176, "no_speech_prob": 0.005219249986112118}, {"id": 731, "seek": 297292, "start": 2976.44, "end": 2980.92, "text": " And uh, understand how it works under the hood and make sure that it's broadcasting in the direction that you like", "tokens": [50540, 400, 2232, 11, 1223, 577, 309, 1985, 833, 264, 13376, 293, 652, 988, 300, 309, 311, 30024, 294, 264, 3513, 300, 291, 411, 50764], "temperature": 0.0, "avg_logprob": -0.14298156556629, "compression_ratio": 1.6824817518248176, "no_speech_prob": 0.005219249986112118}, {"id": 732, "seek": 297292, "start": 2981.2400000000002, "end": 2987.8, "text": " Otherwise, you're going to introduce very subtle bugs very hard to find bugs and uh, just be careful one more note to an efficiency", "tokens": [50780, 10328, 11, 291, 434, 516, 281, 5366, 588, 13743, 15120, 588, 1152, 281, 915, 15120, 293, 2232, 11, 445, 312, 5026, 472, 544, 3637, 281, 364, 10493, 51108], "temperature": 0.0, "avg_logprob": -0.14298156556629, "compression_ratio": 1.6824817518248176, "no_speech_prob": 0.005219249986112118}, {"id": 733, "seek": 297292, "start": 2988.28, "end": 2993.8, "text": " We don't want to be doing this here because uh, this creates a completely new tensor that we store into p", "tokens": [51132, 492, 500, 380, 528, 281, 312, 884, 341, 510, 570, 2232, 11, 341, 7829, 257, 2584, 777, 40863, 300, 321, 3531, 666, 280, 51408], "temperature": 0.0, "avg_logprob": -0.14298156556629, "compression_ratio": 1.6824817518248176, "no_speech_prob": 0.005219249986112118}, {"id": 734, "seek": 297292, "start": 2994.36, "end": 2996.92, "text": " We prefer to use in place operations if possible", "tokens": [51436, 492, 4382, 281, 764, 294, 1081, 7705, 498, 1944, 51564], "temperature": 0.0, "avg_logprob": -0.14298156556629, "compression_ratio": 1.6824817518248176, "no_speech_prob": 0.005219249986112118}, {"id": 735, "seek": 299692, "start": 2997.88, "end": 3001.4, "text": " So this would be an in-place operation has the potential to be faster", "tokens": [50412, 407, 341, 576, 312, 364, 294, 12, 6742, 6916, 575, 264, 3995, 281, 312, 4663, 50588], "temperature": 0.0, "avg_logprob": -0.16007838332862184, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.018262382596731186}, {"id": 736, "seek": 299692, "start": 3001.8, "end": 3005.56, "text": " It doesn't create new memory under the hood and then let's erase this", "tokens": [50608, 467, 1177, 380, 1884, 777, 4675, 833, 264, 13376, 293, 550, 718, 311, 23525, 341, 50796], "temperature": 0.0, "avg_logprob": -0.16007838332862184, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.018262382596731186}, {"id": 737, "seek": 299692, "start": 3006.2000000000003, "end": 3007.96, "text": " We don't need it", "tokens": [50828, 492, 500, 380, 643, 309, 50916], "temperature": 0.0, "avg_logprob": -0.16007838332862184, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.018262382596731186}, {"id": 738, "seek": 299692, "start": 3007.96, "end": 3009.96, "text": " and let's also", "tokens": [50916, 293, 718, 311, 611, 51016], "temperature": 0.0, "avg_logprob": -0.16007838332862184, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.018262382596731186}, {"id": 739, "seek": 299692, "start": 3011.0, "end": 3014.04, "text": " Um, just do fewer just so i'm not wasting space", "tokens": [51068, 3301, 11, 445, 360, 13366, 445, 370, 741, 478, 406, 20457, 1901, 51220], "temperature": 0.0, "avg_logprob": -0.16007838332862184, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.018262382596731186}, {"id": 740, "seek": 299692, "start": 3014.6800000000003, "end": 3016.52, "text": " Okay, so we're actually in a pretty good spot now", "tokens": [51252, 1033, 11, 370, 321, 434, 767, 294, 257, 1238, 665, 4008, 586, 51344], "temperature": 0.0, "avg_logprob": -0.16007838332862184, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.018262382596731186}, {"id": 741, "seek": 299692, "start": 3017.0, "end": 3021.2400000000002, "text": " We trained a bi-gram language model and we trained it really just by counting", "tokens": [51368, 492, 8895, 257, 3228, 12, 1342, 2856, 2316, 293, 321, 8895, 309, 534, 445, 538, 13251, 51580], "temperature": 0.0, "avg_logprob": -0.16007838332862184, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.018262382596731186}, {"id": 742, "seek": 299692, "start": 3021.7200000000003, "end": 3026.76, "text": " Uh, how frequently any pairing occurs and then normalizing so that we get a nice probability", "tokens": [51604, 4019, 11, 577, 10374, 604, 32735, 11843, 293, 550, 2710, 3319, 370, 300, 321, 483, 257, 1481, 8482, 51856], "temperature": 0.0, "avg_logprob": -0.16007838332862184, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.018262382596731186}, {"id": 743, "seek": 302692, "start": 3027.88, "end": 3030.6, "text": " So really these elements of this array p", "tokens": [50412, 407, 534, 613, 4959, 295, 341, 10225, 280, 50548], "temperature": 0.0, "avg_logprob": -0.11534932351881458, "compression_ratio": 1.858695652173913, "no_speech_prob": 0.005383945070207119}, {"id": 744, "seek": 302692, "start": 3031.16, "end": 3036.12, "text": " Are really the parameters of our bi-gram language model giving us and summarizing the statistics of these bi-grams", "tokens": [50576, 2014, 534, 264, 9834, 295, 527, 3228, 12, 1342, 2856, 2316, 2902, 505, 293, 14611, 3319, 264, 12523, 295, 613, 3228, 12, 1342, 82, 50824], "temperature": 0.0, "avg_logprob": -0.11534932351881458, "compression_ratio": 1.858695652173913, "no_speech_prob": 0.005383945070207119}, {"id": 745, "seek": 302692, "start": 3036.92, "end": 3041.08, "text": " So we trained a model and then we know how to sample from a model. We just iteratively", "tokens": [50864, 407, 321, 8895, 257, 2316, 293, 550, 321, 458, 577, 281, 6889, 490, 257, 2316, 13, 492, 445, 17138, 19020, 51072], "temperature": 0.0, "avg_logprob": -0.11534932351881458, "compression_ratio": 1.858695652173913, "no_speech_prob": 0.005383945070207119}, {"id": 746, "seek": 302692, "start": 3042.04, "end": 3045.96, "text": " Sampled the next character and uh feed it in each time and get a next character", "tokens": [51120, 4832, 15551, 264, 958, 2517, 293, 2232, 3154, 309, 294, 1184, 565, 293, 483, 257, 958, 2517, 51316], "temperature": 0.0, "avg_logprob": -0.11534932351881458, "compression_ratio": 1.858695652173913, "no_speech_prob": 0.005383945070207119}, {"id": 747, "seek": 302692, "start": 3047.0, "end": 3050.6, "text": " Now what i'd like to do is i'd like to somehow evaluate the quality of this model", "tokens": [51368, 823, 437, 741, 1116, 411, 281, 360, 307, 741, 1116, 411, 281, 6063, 13059, 264, 3125, 295, 341, 2316, 51548], "temperature": 0.0, "avg_logprob": -0.11534932351881458, "compression_ratio": 1.858695652173913, "no_speech_prob": 0.005383945070207119}, {"id": 748, "seek": 302692, "start": 3051.08, "end": 3056.6, "text": " We'd like to somehow summarize the quality of this model into a single number. How good is it at predicting?", "tokens": [51572, 492, 1116, 411, 281, 6063, 20858, 264, 3125, 295, 341, 2316, 666, 257, 2167, 1230, 13, 1012, 665, 307, 309, 412, 32884, 30, 51848], "temperature": 0.0, "avg_logprob": -0.11534932351881458, "compression_ratio": 1.858695652173913, "no_speech_prob": 0.005383945070207119}, {"id": 749, "seek": 305692, "start": 3057.48, "end": 3059.0, "text": " the training set", "tokens": [50392, 264, 3097, 992, 50468], "temperature": 0.0, "avg_logprob": -0.11193108331589471, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0005527414614334702}, {"id": 750, "seek": 305692, "start": 3059.0, "end": 3062.92, "text": " And as an example so in the training set we can evaluate now the training", "tokens": [50468, 400, 382, 364, 1365, 370, 294, 264, 3097, 992, 321, 393, 13059, 586, 264, 3097, 50664], "temperature": 0.0, "avg_logprob": -0.11193108331589471, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0005527414614334702}, {"id": 751, "seek": 305692, "start": 3063.4, "end": 3066.28, "text": " Loss and this training loss is telling us about", "tokens": [50688, 441, 772, 293, 341, 3097, 4470, 307, 3585, 505, 466, 50832], "temperature": 0.0, "avg_logprob": -0.11193108331589471, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0005527414614334702}, {"id": 752, "seek": 305692, "start": 3066.92, "end": 3070.6800000000003, "text": " Sort of the quality of this model in a single number just like we saw in micrograd", "tokens": [50864, 26149, 295, 264, 3125, 295, 341, 2316, 294, 257, 2167, 1230, 445, 411, 321, 1866, 294, 4532, 7165, 51052], "temperature": 0.0, "avg_logprob": -0.11193108331589471, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0005527414614334702}, {"id": 753, "seek": 305692, "start": 3071.96, "end": 3075.16, "text": " So let's try to think through the quality of the model and how we would evaluate it", "tokens": [51116, 407, 718, 311, 853, 281, 519, 807, 264, 3125, 295, 264, 2316, 293, 577, 321, 576, 13059, 309, 51276], "temperature": 0.0, "avg_logprob": -0.11193108331589471, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0005527414614334702}, {"id": 754, "seek": 305692, "start": 3077.08, "end": 3079.88, "text": " Basically what we're going to do is we're going to copy paste this code", "tokens": [51372, 8537, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 5055, 9163, 341, 3089, 51512], "temperature": 0.0, "avg_logprob": -0.11193108331589471, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0005527414614334702}, {"id": 755, "seek": 305692, "start": 3080.6800000000003, "end": 3082.6800000000003, "text": " That we previously used for counting", "tokens": [51552, 663, 321, 8046, 1143, 337, 13251, 51652], "temperature": 0.0, "avg_logprob": -0.11193108331589471, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0005527414614334702}, {"id": 756, "seek": 305692, "start": 3083.0, "end": 3084.2000000000003, "text": " Okay", "tokens": [51668, 1033, 51728], "temperature": 0.0, "avg_logprob": -0.11193108331589471, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0005527414614334702}, {"id": 757, "seek": 308420, "start": 3084.2, "end": 3087.24, "text": " And let me just print these bi-grams first. We're going to use f strings", "tokens": [50364, 400, 718, 385, 445, 4482, 613, 3228, 12, 1342, 82, 700, 13, 492, 434, 516, 281, 764, 283, 13985, 50516], "temperature": 0.0, "avg_logprob": -0.13161389256866884, "compression_ratio": 1.8057553956834533, "no_speech_prob": 0.0011334650916978717}, {"id": 758, "seek": 308420, "start": 3087.8799999999997, "end": 3091.48, "text": " And i'm going to print character one followed by character two. These are the bi-grams", "tokens": [50548, 400, 741, 478, 516, 281, 4482, 2517, 472, 6263, 538, 2517, 732, 13, 1981, 366, 264, 3228, 12, 1342, 82, 50728], "temperature": 0.0, "avg_logprob": -0.13161389256866884, "compression_ratio": 1.8057553956834533, "no_speech_prob": 0.0011334650916978717}, {"id": 759, "seek": 308420, "start": 3091.96, "end": 3094.68, "text": " And then I don't want to do it for all the words. Let's just do first three words", "tokens": [50752, 400, 550, 286, 500, 380, 528, 281, 360, 309, 337, 439, 264, 2283, 13, 961, 311, 445, 360, 700, 1045, 2283, 50888], "temperature": 0.0, "avg_logprob": -0.13161389256866884, "compression_ratio": 1.8057553956834533, "no_speech_prob": 0.0011334650916978717}, {"id": 760, "seek": 308420, "start": 3095.96, "end": 3098.8399999999997, "text": " So here we have emma olivia and ava bi-grams", "tokens": [50952, 407, 510, 321, 362, 846, 1696, 2545, 18503, 293, 1305, 64, 3228, 12, 1342, 82, 51096], "temperature": 0.0, "avg_logprob": -0.13161389256866884, "compression_ratio": 1.8057553956834533, "no_speech_prob": 0.0011334650916978717}, {"id": 761, "seek": 308420, "start": 3100.2, "end": 3107.3199999999997, "text": " Now what we'd like to do is we'd like to basically look at the probability that the model assigns to every one of these bi-grams", "tokens": [51164, 823, 437, 321, 1116, 411, 281, 360, 307, 321, 1116, 411, 281, 1936, 574, 412, 264, 8482, 300, 264, 2316, 6269, 82, 281, 633, 472, 295, 613, 3228, 12, 1342, 82, 51520], "temperature": 0.0, "avg_logprob": -0.13161389256866884, "compression_ratio": 1.8057553956834533, "no_speech_prob": 0.0011334650916978717}, {"id": 762, "seek": 308420, "start": 3108.12, "end": 3110.6, "text": " So in other words, we can look at the probability, which is", "tokens": [51560, 407, 294, 661, 2283, 11, 321, 393, 574, 412, 264, 8482, 11, 597, 307, 51684], "temperature": 0.0, "avg_logprob": -0.13161389256866884, "compression_ratio": 1.8057553956834533, "no_speech_prob": 0.0011334650916978717}, {"id": 763, "seek": 308420, "start": 3111.16, "end": 3113.16, "text": " summarized in the matrix p", "tokens": [51712, 14611, 1602, 294, 264, 8141, 280, 51812], "temperature": 0.0, "avg_logprob": -0.13161389256866884, "compression_ratio": 1.8057553956834533, "no_speech_prob": 0.0011334650916978717}, {"id": 764, "seek": 311316, "start": 3113.16, "end": 3115.16, "text": " of ix1, ix2", "tokens": [50364, 295, 741, 87, 16, 11, 741, 87, 17, 50464], "temperature": 0.0, "avg_logprob": -0.16065111467915197, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.000417304050642997}, {"id": 765, "seek": 311316, "start": 3116.12, "end": 3119.0, "text": " And then we can print it here as probability", "tokens": [50512, 400, 550, 321, 393, 4482, 309, 510, 382, 8482, 50656], "temperature": 0.0, "avg_logprob": -0.16065111467915197, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.000417304050642997}, {"id": 766, "seek": 311316, "start": 3120.68, "end": 3123.72, "text": " And because these probabilities are way too large, let me percent", "tokens": [50740, 400, 570, 613, 33783, 366, 636, 886, 2416, 11, 718, 385, 3043, 50892], "temperature": 0.0, "avg_logprob": -0.16065111467915197, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.000417304050642997}, {"id": 767, "seek": 311316, "start": 3124.8399999999997, "end": 3126.8399999999997, "text": " our column 0.4f", "tokens": [50948, 527, 7738, 1958, 13, 19, 69, 51048], "temperature": 0.0, "avg_logprob": -0.16065111467915197, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.000417304050642997}, {"id": 768, "seek": 311316, "start": 3126.8399999999997, "end": 3128.8399999999997, "text": " to like truncate it a bit", "tokens": [51048, 281, 411, 504, 409, 66, 473, 309, 257, 857, 51148], "temperature": 0.0, "avg_logprob": -0.16065111467915197, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.000417304050642997}, {"id": 769, "seek": 311316, "start": 3129.16, "end": 3134.52, "text": " So what do we have here, right? We're looking at the probabilities that the model assigns to every one of these bi-grams in the data set", "tokens": [51164, 407, 437, 360, 321, 362, 510, 11, 558, 30, 492, 434, 1237, 412, 264, 33783, 300, 264, 2316, 6269, 82, 281, 633, 472, 295, 613, 3228, 12, 1342, 82, 294, 264, 1412, 992, 51432], "temperature": 0.0, "avg_logprob": -0.16065111467915197, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.000417304050642997}, {"id": 770, "seek": 311316, "start": 3135.24, "end": 3138.2, "text": " And so we can see some of them are four percent, three percent, etc", "tokens": [51468, 400, 370, 321, 393, 536, 512, 295, 552, 366, 1451, 3043, 11, 1045, 3043, 11, 5183, 51616], "temperature": 0.0, "avg_logprob": -0.16065111467915197, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.000417304050642997}, {"id": 771, "seek": 311316, "start": 3138.6, "end": 3140.68, "text": " Just to have a measuring stick in our mind, by the way", "tokens": [51636, 1449, 281, 362, 257, 13389, 2897, 294, 527, 1575, 11, 538, 264, 636, 51740], "temperature": 0.0, "avg_logprob": -0.16065111467915197, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.000417304050642997}, {"id": 772, "seek": 314068, "start": 3141.64, "end": 3148.3599999999997, "text": " We have 27 possible characters or tokens and if everything was equally likely, then you'd expect all these probabilities", "tokens": [50412, 492, 362, 7634, 1944, 4342, 420, 22667, 293, 498, 1203, 390, 12309, 3700, 11, 550, 291, 1116, 2066, 439, 613, 33783, 50748], "temperature": 0.0, "avg_logprob": -0.13349649362396776, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.0025505912490189075}, {"id": 773, "seek": 314068, "start": 3148.9199999999996, "end": 3150.6, "text": " to be", "tokens": [50776, 281, 312, 50860], "temperature": 0.0, "avg_logprob": -0.13349649362396776, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.0025505912490189075}, {"id": 774, "seek": 314068, "start": 3150.6, "end": 3152.44, "text": " four percent roughly", "tokens": [50860, 1451, 3043, 9810, 50952], "temperature": 0.0, "avg_logprob": -0.13349649362396776, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.0025505912490189075}, {"id": 775, "seek": 314068, "start": 3152.44, "end": 3157.16, "text": " So anything above four percent means that we've learned something useful from these bi-gram statistics", "tokens": [50952, 407, 1340, 3673, 1451, 3043, 1355, 300, 321, 600, 3264, 746, 4420, 490, 613, 3228, 12, 1342, 12523, 51188], "temperature": 0.0, "avg_logprob": -0.13349649362396776, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.0025505912490189075}, {"id": 776, "seek": 314068, "start": 3157.56, "end": 3161.24, "text": " And you see that roughly some of these are four percent, but some of them are as high as 40 percent", "tokens": [51208, 400, 291, 536, 300, 9810, 512, 295, 613, 366, 1451, 3043, 11, 457, 512, 295, 552, 366, 382, 1090, 382, 3356, 3043, 51392], "temperature": 0.0, "avg_logprob": -0.13349649362396776, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.0025505912490189075}, {"id": 777, "seek": 314068, "start": 3161.8799999999997, "end": 3163.8799999999997, "text": " 35 percent and so on", "tokens": [51424, 6976, 3043, 293, 370, 322, 51524], "temperature": 0.0, "avg_logprob": -0.13349649362396776, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.0025505912490189075}, {"id": 778, "seek": 314068, "start": 3163.8799999999997, "end": 3169.3999999999996, "text": " So you see that the model actually assigned a pretty high probability to whatever's in the training set and so that that's a good thing", "tokens": [51524, 407, 291, 536, 300, 264, 2316, 767, 13279, 257, 1238, 1090, 8482, 281, 2035, 311, 294, 264, 3097, 992, 293, 370, 300, 300, 311, 257, 665, 551, 51800], "temperature": 0.0, "avg_logprob": -0.13349649362396776, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.0025505912490189075}, {"id": 779, "seek": 316940, "start": 3170.12, "end": 3172.12, "text": " Um, basically if you have a very good model", "tokens": [50400, 3301, 11, 1936, 498, 291, 362, 257, 588, 665, 2316, 50500], "temperature": 0.0, "avg_logprob": -0.10633510293312443, "compression_ratio": 1.756183745583039, "no_speech_prob": 0.001454971032217145}, {"id": 780, "seek": 316940, "start": 3172.44, "end": 3177.56, "text": " You'd expect that these probabilities should be near one because that means that uh, your model is correctly predicting", "tokens": [50516, 509, 1116, 2066, 300, 613, 33783, 820, 312, 2651, 472, 570, 300, 1355, 300, 2232, 11, 428, 2316, 307, 8944, 32884, 50772], "temperature": 0.0, "avg_logprob": -0.10633510293312443, "compression_ratio": 1.756183745583039, "no_speech_prob": 0.001454971032217145}, {"id": 781, "seek": 316940, "start": 3177.64, "end": 3181.4, "text": " What's going to come next especially on the training set where you where you train your model", "tokens": [50776, 708, 311, 516, 281, 808, 958, 2318, 322, 264, 3097, 992, 689, 291, 689, 291, 3847, 428, 2316, 50964], "temperature": 0.0, "avg_logprob": -0.10633510293312443, "compression_ratio": 1.756183745583039, "no_speech_prob": 0.001454971032217145}, {"id": 782, "seek": 316940, "start": 3182.84, "end": 3190.12, "text": " So now we'd like to think about how can we summarize these probabilities into a single number that measures the quality of this model", "tokens": [51036, 407, 586, 321, 1116, 411, 281, 519, 466, 577, 393, 321, 20858, 613, 33783, 666, 257, 2167, 1230, 300, 8000, 264, 3125, 295, 341, 2316, 51400], "temperature": 0.0, "avg_logprob": -0.10633510293312443, "compression_ratio": 1.756183745583039, "no_speech_prob": 0.001454971032217145}, {"id": 783, "seek": 316940, "start": 3191.7200000000003, "end": 3196.6, "text": " Now when you look at the literature into maximum likelihood estimation and statistical modeling and so on", "tokens": [51480, 823, 562, 291, 574, 412, 264, 10394, 666, 6674, 22119, 35701, 293, 22820, 15983, 293, 370, 322, 51724], "temperature": 0.0, "avg_logprob": -0.10633510293312443, "compression_ratio": 1.756183745583039, "no_speech_prob": 0.001454971032217145}, {"id": 784, "seek": 319660, "start": 3197.24, "end": 3200.7599999999998, "text": " You'll see that what's typically used here is something called the likelihood", "tokens": [50396, 509, 603, 536, 300, 437, 311, 5850, 1143, 510, 307, 746, 1219, 264, 22119, 50572], "temperature": 0.0, "avg_logprob": -0.10659762351743636, "compression_ratio": 2.0048076923076925, "no_speech_prob": 0.0037069348618388176}, {"id": 785, "seek": 319660, "start": 3201.48, "end": 3204.68, "text": " And the likelihood is the product of all of these probabilities", "tokens": [50608, 400, 264, 22119, 307, 264, 1674, 295, 439, 295, 613, 33783, 50768], "temperature": 0.0, "avg_logprob": -0.10659762351743636, "compression_ratio": 2.0048076923076925, "no_speech_prob": 0.0037069348618388176}, {"id": 786, "seek": 319660, "start": 3205.7999999999997, "end": 3208.8399999999997, "text": " And so the product of all of these probabilities is the likelihood", "tokens": [50824, 400, 370, 264, 1674, 295, 439, 295, 613, 33783, 307, 264, 22119, 50976], "temperature": 0.0, "avg_logprob": -0.10659762351743636, "compression_ratio": 2.0048076923076925, "no_speech_prob": 0.0037069348618388176}, {"id": 787, "seek": 319660, "start": 3209.24, "end": 3214.04, "text": " And it's really telling us about the probability of the entire data set assigned", "tokens": [50996, 400, 309, 311, 534, 3585, 505, 466, 264, 8482, 295, 264, 2302, 1412, 992, 13279, 51236], "temperature": 0.0, "avg_logprob": -0.10659762351743636, "compression_ratio": 2.0048076923076925, "no_speech_prob": 0.0037069348618388176}, {"id": 788, "seek": 319660, "start": 3215.08, "end": 3218.8399999999997, "text": " Assigned by the model that we've trained and that is a measure of quality", "tokens": [51288, 6281, 16690, 538, 264, 2316, 300, 321, 600, 8895, 293, 300, 307, 257, 3481, 295, 3125, 51476], "temperature": 0.0, "avg_logprob": -0.10659762351743636, "compression_ratio": 2.0048076923076925, "no_speech_prob": 0.0037069348618388176}, {"id": 789, "seek": 319660, "start": 3219.4, "end": 3222.6, "text": " So the product of these should be as high as possible", "tokens": [51504, 407, 264, 1674, 295, 613, 820, 312, 382, 1090, 382, 1944, 51664], "temperature": 0.0, "avg_logprob": -0.10659762351743636, "compression_ratio": 2.0048076923076925, "no_speech_prob": 0.0037069348618388176}, {"id": 790, "seek": 322260, "start": 3223.16, "end": 3228.2799999999997, "text": " When you are training the model and when you have a good model your product your product of these probabilities should be very high", "tokens": [50392, 1133, 291, 366, 3097, 264, 2316, 293, 562, 291, 362, 257, 665, 2316, 428, 1674, 428, 1674, 295, 613, 33783, 820, 312, 588, 1090, 50648], "temperature": 0.0, "avg_logprob": -0.11132574527063102, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0015486006159335375}, {"id": 791, "seek": 322260, "start": 3229.4, "end": 3230.36, "text": " um", "tokens": [50704, 1105, 50752], "temperature": 0.0, "avg_logprob": -0.11132574527063102, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0015486006159335375}, {"id": 792, "seek": 322260, "start": 3230.36, "end": 3233.96, "text": " Now because the product of these probabilities is an unwieldy thing to work with", "tokens": [50752, 823, 570, 264, 1674, 295, 613, 33783, 307, 364, 14853, 1789, 88, 551, 281, 589, 365, 50932], "temperature": 0.0, "avg_logprob": -0.11132574527063102, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0015486006159335375}, {"id": 793, "seek": 322260, "start": 3234.2799999999997, "end": 3239.08, "text": " You can see that all of them are between zero and one. So your product of these probabilities will be a very tiny number", "tokens": [50948, 509, 393, 536, 300, 439, 295, 552, 366, 1296, 4018, 293, 472, 13, 407, 428, 1674, 295, 613, 33783, 486, 312, 257, 588, 5870, 1230, 51188], "temperature": 0.0, "avg_logprob": -0.11132574527063102, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0015486006159335375}, {"id": 794, "seek": 322260, "start": 3240.12, "end": 3240.92, "text": " um", "tokens": [51240, 1105, 51280], "temperature": 0.0, "avg_logprob": -0.11132574527063102, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0015486006159335375}, {"id": 795, "seek": 322260, "start": 3240.92, "end": 3246.68, "text": " So for convenience what people work with usually is not the likelihood, but they work with what's called the log likelihood", "tokens": [51280, 407, 337, 19283, 437, 561, 589, 365, 2673, 307, 406, 264, 22119, 11, 457, 436, 589, 365, 437, 311, 1219, 264, 3565, 22119, 51568], "temperature": 0.0, "avg_logprob": -0.11132574527063102, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0015486006159335375}, {"id": 796, "seek": 322260, "start": 3247.88, "end": 3249.0, "text": " So", "tokens": [51628, 407, 51684], "temperature": 0.0, "avg_logprob": -0.11132574527063102, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.0015486006159335375}, {"id": 797, "seek": 324900, "start": 3249.0, "end": 3252.04, "text": " The product of these is the likelihood to get the log likelihood", "tokens": [50364, 440, 1674, 295, 613, 307, 264, 22119, 281, 483, 264, 3565, 22119, 50516], "temperature": 0.0, "avg_logprob": -0.09754148483276368, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.0017820547800511122}, {"id": 798, "seek": 324900, "start": 3252.36, "end": 3254.36, "text": " We just have to take the log of the probability", "tokens": [50532, 492, 445, 362, 281, 747, 264, 3565, 295, 264, 8482, 50632], "temperature": 0.0, "avg_logprob": -0.09754148483276368, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.0017820547800511122}, {"id": 799, "seek": 324900, "start": 3255.0, "end": 3258.6, "text": " And so the log of the probability here. I have the log of x from zero to one", "tokens": [50664, 400, 370, 264, 3565, 295, 264, 8482, 510, 13, 286, 362, 264, 3565, 295, 2031, 490, 4018, 281, 472, 50844], "temperature": 0.0, "avg_logprob": -0.09754148483276368, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.0017820547800511122}, {"id": 800, "seek": 324900, "start": 3259.8, "end": 3263.8, "text": " The log is a you see here monotonic transformation of the probability", "tokens": [50904, 440, 3565, 307, 257, 291, 536, 510, 1108, 310, 11630, 9887, 295, 264, 8482, 51104], "temperature": 0.0, "avg_logprob": -0.09754148483276368, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.0017820547800511122}, {"id": 801, "seek": 324900, "start": 3264.68, "end": 3266.68, "text": " Where if you pass in one", "tokens": [51148, 2305, 498, 291, 1320, 294, 472, 51248], "temperature": 0.0, "avg_logprob": -0.09754148483276368, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.0017820547800511122}, {"id": 802, "seek": 324900, "start": 3267.24, "end": 3268.84, "text": " You get zero", "tokens": [51276, 509, 483, 4018, 51356], "temperature": 0.0, "avg_logprob": -0.09754148483276368, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.0017820547800511122}, {"id": 803, "seek": 324900, "start": 3268.84, "end": 3271.64, "text": " So probability one gets your log probability of zero", "tokens": [51356, 407, 8482, 472, 2170, 428, 3565, 8482, 295, 4018, 51496], "temperature": 0.0, "avg_logprob": -0.09754148483276368, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.0017820547800511122}, {"id": 804, "seek": 324900, "start": 3272.28, "end": 3274.28, "text": " And then as you go lower and lower probability", "tokens": [51528, 400, 550, 382, 291, 352, 3126, 293, 3126, 8482, 51628], "temperature": 0.0, "avg_logprob": -0.09754148483276368, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.0017820547800511122}, {"id": 805, "seek": 327428, "start": 3274.52, "end": 3279.32, "text": " The log will grow more and more negative until all the way to negative infinity at zero", "tokens": [50376, 440, 3565, 486, 1852, 544, 293, 544, 3671, 1826, 439, 264, 636, 281, 3671, 13202, 412, 4018, 50616], "temperature": 0.0, "avg_logprob": -0.12946487754903813, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.0035929325968027115}, {"id": 806, "seek": 327428, "start": 3281.88, "end": 3286.44, "text": " So here we have a log prob, which is really just a torche dot log of probability", "tokens": [50744, 407, 510, 321, 362, 257, 3565, 1239, 11, 597, 307, 534, 445, 257, 3930, 1876, 5893, 3565, 295, 8482, 50972], "temperature": 0.0, "avg_logprob": -0.12946487754903813, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.0035929325968027115}, {"id": 807, "seek": 327428, "start": 3286.84, "end": 3289.4, "text": " Let's print it out to get a sense of what that looks like", "tokens": [50992, 961, 311, 4482, 309, 484, 281, 483, 257, 2020, 295, 437, 300, 1542, 411, 51120], "temperature": 0.0, "avg_logprob": -0.12946487754903813, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.0035929325968027115}, {"id": 808, "seek": 327428, "start": 3290.0400000000004, "end": 3291.6400000000003, "text": " log prob", "tokens": [51152, 3565, 1239, 51232], "temperature": 0.0, "avg_logprob": -0.12946487754903813, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.0035929325968027115}, {"id": 809, "seek": 327428, "start": 3291.6400000000003, "end": 3293.6400000000003, "text": " also 0.4 f", "tokens": [51232, 611, 1958, 13, 19, 283, 51332], "temperature": 0.0, "avg_logprob": -0.12946487754903813, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.0035929325968027115}, {"id": 810, "seek": 327428, "start": 3294.84, "end": 3296.6800000000003, "text": " Okay", "tokens": [51392, 1033, 51484], "temperature": 0.0, "avg_logprob": -0.12946487754903813, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.0035929325968027115}, {"id": 811, "seek": 327428, "start": 3296.6800000000003, "end": 3301.2400000000002, "text": " So as you can see when we plug in numbers that are very close some of our higher numbers", "tokens": [51484, 407, 382, 291, 393, 536, 562, 321, 5452, 294, 3547, 300, 366, 588, 1998, 512, 295, 527, 2946, 3547, 51712], "temperature": 0.0, "avg_logprob": -0.12946487754903813, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.0035929325968027115}, {"id": 812, "seek": 330124, "start": 3301.3199999999997, "end": 3308.12, "text": " We get closer and closer to zero and then if we plug in very bad probabilities, we get more and more negative number. That's bad", "tokens": [50368, 492, 483, 4966, 293, 4966, 281, 4018, 293, 550, 498, 321, 5452, 294, 588, 1578, 33783, 11, 321, 483, 544, 293, 544, 3671, 1230, 13, 663, 311, 1578, 50708], "temperature": 0.0, "avg_logprob": -0.11585049535713944, "compression_ratio": 1.7385892116182573, "no_speech_prob": 0.0014778657350689173}, {"id": 813, "seek": 330124, "start": 3309.56, "end": 3310.9199999999996, "text": " so", "tokens": [50780, 370, 50848], "temperature": 0.0, "avg_logprob": -0.11585049535713944, "compression_ratio": 1.7385892116182573, "no_speech_prob": 0.0014778657350689173}, {"id": 814, "seek": 330124, "start": 3310.9199999999996, "end": 3314.7599999999998, "text": " And the reason we work with this is for large extent convenience, right?", "tokens": [50848, 400, 264, 1778, 321, 589, 365, 341, 307, 337, 2416, 8396, 19283, 11, 558, 30, 51040], "temperature": 0.0, "avg_logprob": -0.11585049535713944, "compression_ratio": 1.7385892116182573, "no_speech_prob": 0.0014778657350689173}, {"id": 815, "seek": 330124, "start": 3315.24, "end": 3320.52, "text": " Because we have mathematically that if you have some product a times b times c of all these probabilities, right?", "tokens": [51064, 1436, 321, 362, 44003, 300, 498, 291, 362, 512, 1674, 257, 1413, 272, 1413, 269, 295, 439, 613, 33783, 11, 558, 30, 51328], "temperature": 0.0, "avg_logprob": -0.11585049535713944, "compression_ratio": 1.7385892116182573, "no_speech_prob": 0.0014778657350689173}, {"id": 816, "seek": 330124, "start": 3321.16, "end": 3324.6, "text": " The likelihood is the product of all these probabilities", "tokens": [51360, 440, 22119, 307, 264, 1674, 295, 439, 613, 33783, 51532], "temperature": 0.0, "avg_logprob": -0.11585049535713944, "compression_ratio": 1.7385892116182573, "no_speech_prob": 0.0014778657350689173}, {"id": 817, "seek": 330124, "start": 3325.3999999999996, "end": 3327.3999999999996, "text": " then the log", "tokens": [51572, 550, 264, 3565, 51672], "temperature": 0.0, "avg_logprob": -0.11585049535713944, "compression_ratio": 1.7385892116182573, "no_speech_prob": 0.0014778657350689173}, {"id": 818, "seek": 330124, "start": 3327.3999999999996, "end": 3330.2799999999997, "text": " Of these is just log of a plus", "tokens": [51672, 2720, 613, 307, 445, 3565, 295, 257, 1804, 51816], "temperature": 0.0, "avg_logprob": -0.11585049535713944, "compression_ratio": 1.7385892116182573, "no_speech_prob": 0.0014778657350689173}, {"id": 819, "seek": 333028, "start": 3330.84, "end": 3332.84, "text": " log of b", "tokens": [50392, 3565, 295, 272, 50492], "temperature": 0.0, "avg_logprob": -0.1258570466722761, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0013884373474866152}, {"id": 820, "seek": 333028, "start": 3333.8, "end": 3336.76, "text": " Plus log of c if you remember your logs from your", "tokens": [50540, 7721, 3565, 295, 269, 498, 291, 1604, 428, 20820, 490, 428, 50688], "temperature": 0.0, "avg_logprob": -0.1258570466722761, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0013884373474866152}, {"id": 821, "seek": 333028, "start": 3337.48, "end": 3339.48, "text": " High school or undergrad and so on", "tokens": [50724, 5229, 1395, 420, 14295, 293, 370, 322, 50824], "temperature": 0.0, "avg_logprob": -0.1258570466722761, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0013884373474866152}, {"id": 822, "seek": 333028, "start": 3339.8, "end": 3341.5600000000004, "text": " so we have that basically", "tokens": [50840, 370, 321, 362, 300, 1936, 50928], "temperature": 0.0, "avg_logprob": -0.1258570466722761, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0013884373474866152}, {"id": 823, "seek": 333028, "start": 3341.5600000000004, "end": 3347.6400000000003, "text": " The likelihood is the product of probabilities. The log likelihood is just the sum of the logs of the individual probabilities", "tokens": [50928, 440, 22119, 307, 264, 1674, 295, 33783, 13, 440, 3565, 22119, 307, 445, 264, 2408, 295, 264, 20820, 295, 264, 2609, 33783, 51232], "temperature": 0.0, "avg_logprob": -0.1258570466722761, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0013884373474866152}, {"id": 824, "seek": 333028, "start": 3348.84, "end": 3350.0400000000004, "text": " so", "tokens": [51292, 370, 51352], "temperature": 0.0, "avg_logprob": -0.1258570466722761, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0013884373474866152}, {"id": 825, "seek": 333028, "start": 3350.0400000000004, "end": 3352.0400000000004, "text": " log likelihood", "tokens": [51352, 3565, 22119, 51452], "temperature": 0.0, "avg_logprob": -0.1258570466722761, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0013884373474866152}, {"id": 826, "seek": 333028, "start": 3352.76, "end": 3354.6800000000003, "text": " Starts at zero", "tokens": [51488, 6481, 82, 412, 4018, 51584], "temperature": 0.0, "avg_logprob": -0.1258570466722761, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0013884373474866152}, {"id": 827, "seek": 333028, "start": 3354.6800000000003, "end": 3358.28, "text": " And then log likelihood here we can just accumulate simply", "tokens": [51584, 400, 550, 3565, 22119, 510, 321, 393, 445, 33384, 2935, 51764], "temperature": 0.0, "avg_logprob": -0.1258570466722761, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0013884373474866152}, {"id": 828, "seek": 336028, "start": 3360.44, "end": 3362.44, "text": " And then the end we can print this", "tokens": [50372, 400, 550, 264, 917, 321, 393, 4482, 341, 50472], "temperature": 0.0, "avg_logprob": -0.14954800605773927, "compression_ratio": 1.304, "no_speech_prob": 0.0006361748091876507}, {"id": 829, "seek": 336028, "start": 3365.4, "end": 3367.4, "text": " Print the log likelihood", "tokens": [50620, 34439, 264, 3565, 22119, 50720], "temperature": 0.0, "avg_logprob": -0.14954800605773927, "compression_ratio": 1.304, "no_speech_prob": 0.0006361748091876507}, {"id": 830, "seek": 336028, "start": 3369.6400000000003, "end": 3371.6400000000003, "text": " F strings", "tokens": [50832, 479, 13985, 50932], "temperature": 0.0, "avg_logprob": -0.14954800605773927, "compression_ratio": 1.304, "no_speech_prob": 0.0006361748091876507}, {"id": 831, "seek": 336028, "start": 3371.8, "end": 3373.8, "text": " Maybe you're familiar with this", "tokens": [50940, 2704, 291, 434, 4963, 365, 341, 51040], "temperature": 0.0, "avg_logprob": -0.14954800605773927, "compression_ratio": 1.304, "no_speech_prob": 0.0006361748091876507}, {"id": 832, "seek": 336028, "start": 3373.88, "end": 3376.38, "text": " So log likelihood is negative 38", "tokens": [51044, 407, 3565, 22119, 307, 3671, 12843, 51169], "temperature": 0.0, "avg_logprob": -0.14954800605773927, "compression_ratio": 1.304, "no_speech_prob": 0.0006361748091876507}, {"id": 833, "seek": 336028, "start": 3379.96, "end": 3381.32, "text": " Okay", "tokens": [51348, 1033, 51416], "temperature": 0.0, "avg_logprob": -0.14954800605773927, "compression_ratio": 1.304, "no_speech_prob": 0.0006361748091876507}, {"id": 834, "seek": 336028, "start": 3381.32, "end": 3382.6000000000004, "text": " now", "tokens": [51416, 586, 51480], "temperature": 0.0, "avg_logprob": -0.14954800605773927, "compression_ratio": 1.304, "no_speech_prob": 0.0006361748091876507}, {"id": 835, "seek": 336028, "start": 3382.6000000000004, "end": 3384.6000000000004, "text": " We actually want um", "tokens": [51480, 492, 767, 528, 1105, 51580], "temperature": 0.0, "avg_logprob": -0.14954800605773927, "compression_ratio": 1.304, "no_speech_prob": 0.0006361748091876507}, {"id": 836, "seek": 338460, "start": 3385.24, "end": 3389.3199999999997, "text": " So how high can log likelihood get it can go to zero", "tokens": [50396, 407, 577, 1090, 393, 3565, 22119, 483, 309, 393, 352, 281, 4018, 50600], "temperature": 0.0, "avg_logprob": -0.08586274252997504, "compression_ratio": 1.9352226720647774, "no_speech_prob": 0.0035376958549022675}, {"id": 837, "seek": 338460, "start": 3389.7999999999997, "end": 3394.7599999999998, "text": " So when all the probabilities are one log likelihood will be zero and then when all the probabilities are lower", "tokens": [50624, 407, 562, 439, 264, 33783, 366, 472, 3565, 22119, 486, 312, 4018, 293, 550, 562, 439, 264, 33783, 366, 3126, 50872], "temperature": 0.0, "avg_logprob": -0.08586274252997504, "compression_ratio": 1.9352226720647774, "no_speech_prob": 0.0035376958549022675}, {"id": 838, "seek": 338460, "start": 3394.8399999999997, "end": 3396.8399999999997, "text": " This will grow more and more negative", "tokens": [50876, 639, 486, 1852, 544, 293, 544, 3671, 50976], "temperature": 0.0, "avg_logprob": -0.08586274252997504, "compression_ratio": 1.9352226720647774, "no_speech_prob": 0.0035376958549022675}, {"id": 839, "seek": 338460, "start": 3397.48, "end": 3404.2, "text": " Now we don't actually like this because what we'd like is a loss function and a loss function has the semantics that low", "tokens": [51008, 823, 321, 500, 380, 767, 411, 341, 570, 437, 321, 1116, 411, 307, 257, 4470, 2445, 293, 257, 4470, 2445, 575, 264, 4361, 45298, 300, 2295, 51344], "temperature": 0.0, "avg_logprob": -0.08586274252997504, "compression_ratio": 1.9352226720647774, "no_speech_prob": 0.0035376958549022675}, {"id": 840, "seek": 338460, "start": 3404.68, "end": 3407.72, "text": " Is good because we're trying to minimize the loss", "tokens": [51368, 1119, 665, 570, 321, 434, 1382, 281, 17522, 264, 4470, 51520], "temperature": 0.0, "avg_logprob": -0.08586274252997504, "compression_ratio": 1.9352226720647774, "no_speech_prob": 0.0035376958549022675}, {"id": 841, "seek": 338460, "start": 3408.2, "end": 3413.48, "text": " So we actually need to invert this and that's what gives us something called the negative log likelihood", "tokens": [51544, 407, 321, 767, 643, 281, 33966, 341, 293, 300, 311, 437, 2709, 505, 746, 1219, 264, 3671, 3565, 22119, 51808], "temperature": 0.0, "avg_logprob": -0.08586274252997504, "compression_ratio": 1.9352226720647774, "no_speech_prob": 0.0035376958549022675}, {"id": 842, "seek": 341348, "start": 3414.36, "end": 3416.04, "text": " um", "tokens": [50408, 1105, 50492], "temperature": 0.0, "avg_logprob": -0.17139896186622414, "compression_ratio": 1.9685534591194969, "no_speech_prob": 0.0015977723523974419}, {"id": 843, "seek": 341348, "start": 3416.04, "end": 3419.56, "text": " Negative log likelihood is just negative of the log likelihood", "tokens": [50492, 43230, 3565, 22119, 307, 445, 3671, 295, 264, 3565, 22119, 50668], "temperature": 0.0, "avg_logprob": -0.17139896186622414, "compression_ratio": 1.9685534591194969, "no_speech_prob": 0.0015977723523974419}, {"id": 844, "seek": 341348, "start": 3423.88, "end": 3427.72, "text": " These are f strings by the way if you'd like to look this up negative log likelihood equals", "tokens": [50884, 1981, 366, 283, 13985, 538, 264, 636, 498, 291, 1116, 411, 281, 574, 341, 493, 3671, 3565, 22119, 6915, 51076], "temperature": 0.0, "avg_logprob": -0.17139896186622414, "compression_ratio": 1.9685534591194969, "no_speech_prob": 0.0015977723523974419}, {"id": 845, "seek": 341348, "start": 3429.32, "end": 3431.48, "text": " So negative log likelihood now is just negative of it", "tokens": [51156, 407, 3671, 3565, 22119, 586, 307, 445, 3671, 295, 309, 51264], "temperature": 0.0, "avg_logprob": -0.17139896186622414, "compression_ratio": 1.9685534591194969, "no_speech_prob": 0.0015977723523974419}, {"id": 846, "seek": 341348, "start": 3432.04, "end": 3436.12, "text": " and so the negative log likelihood is a very nice loss function because", "tokens": [51292, 293, 370, 264, 3671, 3565, 22119, 307, 257, 588, 1481, 4470, 2445, 570, 51496], "temperature": 0.0, "avg_logprob": -0.17139896186622414, "compression_ratio": 1.9685534591194969, "no_speech_prob": 0.0015977723523974419}, {"id": 847, "seek": 341348, "start": 3437.64, "end": 3439.64, "text": " The lowest it can get is zero", "tokens": [51572, 440, 12437, 309, 393, 483, 307, 4018, 51672], "temperature": 0.0, "avg_logprob": -0.17139896186622414, "compression_ratio": 1.9685534591194969, "no_speech_prob": 0.0015977723523974419}, {"id": 848, "seek": 343964, "start": 3439.7999999999997, "end": 3443.8799999999997, "text": " And the higher it is the worse off the predictions are that you're making", "tokens": [50372, 400, 264, 2946, 309, 307, 264, 5324, 766, 264, 21264, 366, 300, 291, 434, 1455, 50576], "temperature": 0.0, "avg_logprob": -0.10479424397150676, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.003123106900602579}, {"id": 849, "seek": 343964, "start": 3444.68, "end": 3448.6, "text": " And then one more modification to this that sometimes people do is that for convenience", "tokens": [50616, 400, 550, 472, 544, 26747, 281, 341, 300, 2171, 561, 360, 307, 300, 337, 19283, 50812], "temperature": 0.0, "avg_logprob": -0.10479424397150676, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.003123106900602579}, {"id": 850, "seek": 343964, "start": 3449.16, "end": 3453.48, "text": " They actually like to normalize by they like to make it an average instead of a sum", "tokens": [50840, 814, 767, 411, 281, 2710, 1125, 538, 436, 411, 281, 652, 309, 364, 4274, 2602, 295, 257, 2408, 51056], "temperature": 0.0, "avg_logprob": -0.10479424397150676, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.003123106900602579}, {"id": 851, "seek": 343964, "start": 3454.44, "end": 3456.44, "text": " and so uh here", "tokens": [51104, 293, 370, 2232, 510, 51204], "temperature": 0.0, "avg_logprob": -0.10479424397150676, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.003123106900602579}, {"id": 852, "seek": 343964, "start": 3457.08, "end": 3459.08, "text": " Let's just keep some counts as well", "tokens": [51236, 961, 311, 445, 1066, 512, 14893, 382, 731, 51336], "temperature": 0.0, "avg_logprob": -0.10479424397150676, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.003123106900602579}, {"id": 853, "seek": 343964, "start": 3459.3199999999997, "end": 3463.48, "text": " So n plus equals one starts at zero and then here", "tokens": [51348, 407, 297, 1804, 6915, 472, 3719, 412, 4018, 293, 550, 510, 51556], "temperature": 0.0, "avg_logprob": -0.10479424397150676, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.003123106900602579}, {"id": 854, "seek": 343964, "start": 3464.6, "end": 3466.94, "text": " We can have sort of like a normalized log likelihood", "tokens": [51612, 492, 393, 362, 1333, 295, 411, 257, 48704, 3565, 22119, 51729], "temperature": 0.0, "avg_logprob": -0.10479424397150676, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.003123106900602579}, {"id": 855, "seek": 346964, "start": 3470.44, "end": 3472.44, "text": " If we just normalize it by the count", "tokens": [50404, 759, 321, 445, 2710, 1125, 309, 538, 264, 1207, 50504], "temperature": 0.0, "avg_logprob": -0.10537393314322245, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.000855865073390305}, {"id": 856, "seek": 346964, "start": 3472.44, "end": 3478.2799999999997, "text": " Then we will sort of get the average log likelihood. So this would be usually our loss function here", "tokens": [50504, 1396, 321, 486, 1333, 295, 483, 264, 4274, 3565, 22119, 13, 407, 341, 576, 312, 2673, 527, 4470, 2445, 510, 50796], "temperature": 0.0, "avg_logprob": -0.10537393314322245, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.000855865073390305}, {"id": 857, "seek": 346964, "start": 3478.8399999999997, "end": 3480.8399999999997, "text": " Is put this we would this is what we would use", "tokens": [50824, 1119, 829, 341, 321, 576, 341, 307, 437, 321, 576, 764, 50924], "temperature": 0.0, "avg_logprob": -0.10537393314322245, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.000855865073390305}, {"id": 858, "seek": 346964, "start": 3482.2799999999997, "end": 3486.2799999999997, "text": " So our loss function for the training set assigned by the model is 2.4", "tokens": [50996, 407, 527, 4470, 2445, 337, 264, 3097, 992, 13279, 538, 264, 2316, 307, 568, 13, 19, 51196], "temperature": 0.0, "avg_logprob": -0.10537393314322245, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.000855865073390305}, {"id": 859, "seek": 346964, "start": 3486.44, "end": 3488.44, "text": " That's the quality of this model", "tokens": [51204, 663, 311, 264, 3125, 295, 341, 2316, 51304], "temperature": 0.0, "avg_logprob": -0.10537393314322245, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.000855865073390305}, {"id": 860, "seek": 346964, "start": 3488.52, "end": 3492.3599999999997, "text": " And the lower it is the better off we are and the higher it is the worse off we are", "tokens": [51308, 400, 264, 3126, 309, 307, 264, 1101, 766, 321, 366, 293, 264, 2946, 309, 307, 264, 5324, 766, 321, 366, 51500], "temperature": 0.0, "avg_logprob": -0.10537393314322245, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.000855865073390305}, {"id": 861, "seek": 349236, "start": 3493.32, "end": 3501.1600000000003, "text": " And the job of our you know training is to find the parameters that minimize the negative log likelihood loss", "tokens": [50412, 400, 264, 1691, 295, 527, 291, 458, 3097, 307, 281, 915, 264, 9834, 300, 17522, 264, 3671, 3565, 22119, 4470, 50804], "temperature": 0.0, "avg_logprob": -0.1283715111868722, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.03904429450631142}, {"id": 862, "seek": 349236, "start": 3502.84, "end": 3507.4, "text": " And that would be like a high quality model. Okay, so to summarize I actually wrote it out here", "tokens": [50888, 400, 300, 576, 312, 411, 257, 1090, 3125, 2316, 13, 1033, 11, 370, 281, 20858, 286, 767, 4114, 309, 484, 510, 51116], "temperature": 0.0, "avg_logprob": -0.1283715111868722, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.03904429450631142}, {"id": 863, "seek": 349236, "start": 3508.04, "end": 3514.92, "text": " So our goal is to maximize likelihood, which is the product of all the probabilities assigned by the model", "tokens": [51148, 407, 527, 3387, 307, 281, 19874, 22119, 11, 597, 307, 264, 1674, 295, 439, 264, 33783, 13279, 538, 264, 2316, 51492], "temperature": 0.0, "avg_logprob": -0.1283715111868722, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.03904429450631142}, {"id": 864, "seek": 349236, "start": 3515.56, "end": 3519.32, "text": " And we want to maximize this likelihood with respect to the model parameters", "tokens": [51524, 400, 321, 528, 281, 19874, 341, 22119, 365, 3104, 281, 264, 2316, 9834, 51712], "temperature": 0.0, "avg_logprob": -0.1283715111868722, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.03904429450631142}, {"id": 865, "seek": 351932, "start": 3519.7200000000003, "end": 3525.7200000000003, "text": " And in our case the model parameters here are defined in the table these numbers the probabilities are", "tokens": [50384, 400, 294, 527, 1389, 264, 2316, 9834, 510, 366, 7642, 294, 264, 3199, 613, 3547, 264, 33783, 366, 50684], "temperature": 0.0, "avg_logprob": -0.1298387444537619, "compression_ratio": 1.9538461538461538, "no_speech_prob": 0.006192345637828112}, {"id": 866, "seek": 351932, "start": 3526.52, "end": 3529.32, "text": " The model parameters is sort of in our brygm language model so far", "tokens": [50724, 440, 2316, 9834, 307, 1333, 295, 294, 527, 272, 627, 70, 76, 2856, 2316, 370, 1400, 50864], "temperature": 0.0, "avg_logprob": -0.1298387444537619, "compression_ratio": 1.9538461538461538, "no_speech_prob": 0.006192345637828112}, {"id": 867, "seek": 351932, "start": 3530.1200000000003, "end": 3534.52, "text": " But you have to keep in mind that here we are storing everything in a table format the probabilities", "tokens": [50904, 583, 291, 362, 281, 1066, 294, 1575, 300, 510, 321, 366, 26085, 1203, 294, 257, 3199, 7877, 264, 33783, 51124], "temperature": 0.0, "avg_logprob": -0.1298387444537619, "compression_ratio": 1.9538461538461538, "no_speech_prob": 0.006192345637828112}, {"id": 868, "seek": 351932, "start": 3534.76, "end": 3539.7200000000003, "text": " But what's coming up as a brief preview is that these numbers will not be kept explicitly", "tokens": [51136, 583, 437, 311, 1348, 493, 382, 257, 5353, 14281, 307, 300, 613, 3547, 486, 406, 312, 4305, 20803, 51384], "temperature": 0.0, "avg_logprob": -0.1298387444537619, "compression_ratio": 1.9538461538461538, "no_speech_prob": 0.006192345637828112}, {"id": 869, "seek": 351932, "start": 3540.1200000000003, "end": 3542.52, "text": " But these numbers will be calculated by a neural network", "tokens": [51404, 583, 613, 3547, 486, 312, 15598, 538, 257, 18161, 3209, 51524], "temperature": 0.0, "avg_logprob": -0.1298387444537619, "compression_ratio": 1.9538461538461538, "no_speech_prob": 0.006192345637828112}, {"id": 870, "seek": 351932, "start": 3543.0800000000004, "end": 3544.52, "text": " So that's coming up", "tokens": [51552, 407, 300, 311, 1348, 493, 51624], "temperature": 0.0, "avg_logprob": -0.1298387444537619, "compression_ratio": 1.9538461538461538, "no_speech_prob": 0.006192345637828112}, {"id": 871, "seek": 351932, "start": 3544.52, "end": 3547.6400000000003, "text": " And we want to change and tune the parameters of these neural networks", "tokens": [51624, 400, 321, 528, 281, 1319, 293, 10864, 264, 9834, 295, 613, 18161, 9590, 51780], "temperature": 0.0, "avg_logprob": -0.1298387444537619, "compression_ratio": 1.9538461538461538, "no_speech_prob": 0.006192345637828112}, {"id": 872, "seek": 354764, "start": 3548.04, "end": 3552.3599999999997, "text": " We want to change these parameters to maximize the likelihood the product of the probabilities", "tokens": [50384, 492, 528, 281, 1319, 613, 9834, 281, 19874, 264, 22119, 264, 1674, 295, 264, 33783, 50600], "temperature": 0.0, "avg_logprob": -0.08164215087890625, "compression_ratio": 1.9831932773109244, "no_speech_prob": 0.0029807756654918194}, {"id": 873, "seek": 354764, "start": 3553.4, "end": 3559.08, "text": " Now maximizing the likelihood is equivalent to maximizing the log likelihood because log is a monotonic function", "tokens": [50652, 823, 5138, 3319, 264, 22119, 307, 10344, 281, 5138, 3319, 264, 3565, 22119, 570, 3565, 307, 257, 1108, 310, 11630, 2445, 50936], "temperature": 0.0, "avg_logprob": -0.08164215087890625, "compression_ratio": 1.9831932773109244, "no_speech_prob": 0.0029807756654918194}, {"id": 874, "seek": 354764, "start": 3559.8799999999997, "end": 3561.8799999999997, "text": " Here's the graph of log", "tokens": [50976, 1692, 311, 264, 4295, 295, 3565, 51076], "temperature": 0.0, "avg_logprob": -0.08164215087890625, "compression_ratio": 1.9831932773109244, "no_speech_prob": 0.0029807756654918194}, {"id": 875, "seek": 354764, "start": 3562.04, "end": 3566.04, "text": " And basically all it is doing is it's just scaling your", "tokens": [51084, 400, 1936, 439, 309, 307, 884, 307, 309, 311, 445, 21589, 428, 51284], "temperature": 0.0, "avg_logprob": -0.08164215087890625, "compression_ratio": 1.9831932773109244, "no_speech_prob": 0.0029807756654918194}, {"id": 876, "seek": 354764, "start": 3566.68, "end": 3568.7599999999998, "text": " You can look at it as just a scaling of the loss function", "tokens": [51316, 509, 393, 574, 412, 309, 382, 445, 257, 21589, 295, 264, 4470, 2445, 51420], "temperature": 0.0, "avg_logprob": -0.08164215087890625, "compression_ratio": 1.9831932773109244, "no_speech_prob": 0.0029807756654918194}, {"id": 877, "seek": 354764, "start": 3569.4, "end": 3576.44, "text": " And so the optimization problem here and here are actually equivalent because this is just scaling you can look at it that way", "tokens": [51452, 400, 370, 264, 19618, 1154, 510, 293, 510, 366, 767, 10344, 570, 341, 307, 445, 21589, 291, 393, 574, 412, 309, 300, 636, 51804], "temperature": 0.0, "avg_logprob": -0.08164215087890625, "compression_ratio": 1.9831932773109244, "no_speech_prob": 0.0029807756654918194}, {"id": 878, "seek": 357644, "start": 3577.0, "end": 3579.7200000000003, "text": " And so these are two identical optimization problems", "tokens": [50392, 400, 370, 613, 366, 732, 14800, 19618, 2740, 50528], "temperature": 0.0, "avg_logprob": -0.12319900678551715, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.0018967690411955118}, {"id": 879, "seek": 357644, "start": 3581.16, "end": 3585.32, "text": " Um maximizing the log likelihood is equivalent to minimizing the negative log likelihood", "tokens": [50600, 3301, 5138, 3319, 264, 3565, 22119, 307, 10344, 281, 46608, 264, 3671, 3565, 22119, 50808], "temperature": 0.0, "avg_logprob": -0.12319900678551715, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.0018967690411955118}, {"id": 880, "seek": 357644, "start": 3586.2000000000003, "end": 3592.2000000000003, "text": " And then in practice people actually minimize the average negative log likelihood to get numbers like 2.4", "tokens": [50852, 400, 550, 294, 3124, 561, 767, 17522, 264, 4274, 3671, 3565, 22119, 281, 483, 3547, 411, 568, 13, 19, 51152], "temperature": 0.0, "avg_logprob": -0.12319900678551715, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.0018967690411955118}, {"id": 881, "seek": 357644, "start": 3593.0, "end": 3598.92, "text": " And then this summarizes the quality of your model and we'd like to minimize it and make it as small as possible", "tokens": [51192, 400, 550, 341, 14611, 5660, 264, 3125, 295, 428, 2316, 293, 321, 1116, 411, 281, 17522, 309, 293, 652, 309, 382, 1359, 382, 1944, 51488], "temperature": 0.0, "avg_logprob": -0.12319900678551715, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.0018967690411955118}, {"id": 882, "seek": 357644, "start": 3599.64, "end": 3601.7200000000003, "text": " And the lowest it can get is zero", "tokens": [51524, 400, 264, 12437, 309, 393, 483, 307, 4018, 51628], "temperature": 0.0, "avg_logprob": -0.12319900678551715, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.0018967690411955118}, {"id": 883, "seek": 357644, "start": 3602.36, "end": 3604.2000000000003, "text": " and the lower it is", "tokens": [51660, 293, 264, 3126, 309, 307, 51752], "temperature": 0.0, "avg_logprob": -0.12319900678551715, "compression_ratio": 1.8078602620087336, "no_speech_prob": 0.0018967690411955118}, {"id": 884, "seek": 360420, "start": 3604.2, "end": 3609.08, "text": " The better off your model is because it's assigning it's assigning high probabilities to your data", "tokens": [50364, 440, 1101, 766, 428, 2316, 307, 570, 309, 311, 49602, 309, 311, 49602, 1090, 33783, 281, 428, 1412, 50608], "temperature": 0.0, "avg_logprob": -0.09745151591750811, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.00263146567158401}, {"id": 885, "seek": 360420, "start": 3609.56, "end": 3614.3599999999997, "text": " Now let's estimate the probability over the entire training set just to make sure that we get something around 2.4", "tokens": [50632, 823, 718, 311, 12539, 264, 8482, 670, 264, 2302, 3097, 992, 445, 281, 652, 988, 300, 321, 483, 746, 926, 568, 13, 19, 50872], "temperature": 0.0, "avg_logprob": -0.09745151591750811, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.00263146567158401}, {"id": 886, "seek": 360420, "start": 3614.9199999999996, "end": 3616.9199999999996, "text": " Let's run this over the entire oops", "tokens": [50900, 961, 311, 1190, 341, 670, 264, 2302, 34166, 51000], "temperature": 0.0, "avg_logprob": -0.09745151591750811, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.00263146567158401}, {"id": 887, "seek": 360420, "start": 3617.3199999999997, "end": 3619.3199999999997, "text": " Let's take out the print statement as well", "tokens": [51020, 961, 311, 747, 484, 264, 4482, 5629, 382, 731, 51120], "temperature": 0.0, "avg_logprob": -0.09745151591750811, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.00263146567158401}, {"id": 888, "seek": 360420, "start": 3620.68, "end": 3622.9199999999996, "text": " Okay 2.45 or the entire training set", "tokens": [51188, 1033, 568, 13, 8465, 420, 264, 2302, 3097, 992, 51300], "temperature": 0.0, "avg_logprob": -0.09745151591750811, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.00263146567158401}, {"id": 889, "seek": 360420, "start": 3624.52, "end": 3629.08, "text": " Now what I'd like to show you is that you can actually evaluate the probability for any word that you want like for example", "tokens": [51380, 823, 437, 286, 1116, 411, 281, 855, 291, 307, 300, 291, 393, 767, 13059, 264, 8482, 337, 604, 1349, 300, 291, 528, 411, 337, 1365, 51608], "temperature": 0.0, "avg_logprob": -0.09745151591750811, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.00263146567158401}, {"id": 890, "seek": 362908, "start": 3629.3199999999997, "end": 3634.04, "text": " If we just test a single word andre and bring back the print statement", "tokens": [50376, 759, 321, 445, 1500, 257, 2167, 1349, 293, 265, 293, 1565, 646, 264, 4482, 5629, 50612], "temperature": 0.0, "avg_logprob": -0.18869009982334095, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.008061359636485577}, {"id": 891, "seek": 362908, "start": 3635.88, "end": 3639.64, "text": " Then you see that andre is actually kind of like an unlikely word or like on average", "tokens": [50704, 1396, 291, 536, 300, 293, 265, 307, 767, 733, 295, 411, 364, 17518, 1349, 420, 411, 322, 4274, 50892], "temperature": 0.0, "avg_logprob": -0.18869009982334095, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.008061359636485577}, {"id": 892, "seek": 362908, "start": 3640.7599999999998, "end": 3648.2799999999997, "text": " We take three log probability to represent it and roughly that's because ej apparently is very uncommon as an example", "tokens": [50948, 492, 747, 1045, 3565, 8482, 281, 2906, 309, 293, 9810, 300, 311, 570, 308, 73, 7970, 307, 588, 29289, 382, 364, 1365, 51324], "temperature": 0.0, "avg_logprob": -0.18869009982334095, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.008061359636485577}, {"id": 893, "seek": 362908, "start": 3650.04, "end": 3652.04, "text": " Now think through this", "tokens": [51412, 823, 519, 807, 341, 51512], "temperature": 0.0, "avg_logprob": -0.18869009982334095, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.008061359636485577}, {"id": 894, "seek": 362908, "start": 3653.7999999999997, "end": 3657.7999999999997, "text": " When I take andre and I append q and I test the probability of it andreq", "tokens": [51600, 1133, 286, 747, 293, 265, 293, 286, 34116, 9505, 293, 286, 1500, 264, 8482, 295, 309, 293, 265, 80, 51800], "temperature": 0.0, "avg_logprob": -0.18869009982334095, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.008061359636485577}, {"id": 895, "seek": 365908, "start": 3659.7999999999997, "end": 3662.2, "text": " We actually get um infinity", "tokens": [50400, 492, 767, 483, 1105, 13202, 50520], "temperature": 0.0, "avg_logprob": -0.1429694391066028, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.0007793335826136172}, {"id": 896, "seek": 365908, "start": 3663.0, "end": 3668.68, "text": " And that's because jq has a zero percent probability according to our model. So the log likelihood", "tokens": [50560, 400, 300, 311, 570, 361, 80, 575, 257, 4018, 3043, 8482, 4650, 281, 527, 2316, 13, 407, 264, 3565, 22119, 50844], "temperature": 0.0, "avg_logprob": -0.1429694391066028, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.0007793335826136172}, {"id": 897, "seek": 365908, "start": 3669.3199999999997, "end": 3673.72, "text": " So the log of zero will be negative infinity. We get infinite loss", "tokens": [50876, 407, 264, 3565, 295, 4018, 486, 312, 3671, 13202, 13, 492, 483, 13785, 4470, 51096], "temperature": 0.0, "avg_logprob": -0.1429694391066028, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.0007793335826136172}, {"id": 898, "seek": 365908, "start": 3674.52, "end": 3678.84, "text": " So this is kind of undesirable right because we plugged in a string that could be like a somewhat reasonable name", "tokens": [51136, 407, 341, 307, 733, 295, 45667, 21493, 558, 570, 321, 25679, 294, 257, 6798, 300, 727, 312, 411, 257, 8344, 10585, 1315, 51352], "temperature": 0.0, "avg_logprob": -0.1429694391066028, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.0007793335826136172}, {"id": 899, "seek": 365908, "start": 3679.16, "end": 3684.92, "text": " But basically what this is saying is that this model is exactly zero percent likely to uh to predict this", "tokens": [51368, 583, 1936, 437, 341, 307, 1566, 307, 300, 341, 2316, 307, 2293, 4018, 3043, 3700, 281, 2232, 281, 6069, 341, 51656], "temperature": 0.0, "avg_logprob": -0.1429694391066028, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.0007793335826136172}, {"id": 900, "seek": 368492, "start": 3685.48, "end": 3689.08, "text": " Name and our loss is infinity on this example", "tokens": [50392, 13866, 293, 527, 4470, 307, 13202, 322, 341, 1365, 50572], "temperature": 0.0, "avg_logprob": -0.1502152478919839, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.01032728049904108}, {"id": 901, "seek": 368492, "start": 3689.7200000000003, "end": 3692.04, "text": " And really what the reason for that is that j", "tokens": [50604, 400, 534, 437, 264, 1778, 337, 300, 307, 300, 361, 50720], "temperature": 0.0, "avg_logprob": -0.1502152478919839, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.01032728049904108}, {"id": 902, "seek": 368492, "start": 3692.92, "end": 3694.92, "text": " is followed by q", "tokens": [50764, 307, 6263, 538, 9505, 50864], "temperature": 0.0, "avg_logprob": -0.1502152478919839, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.01032728049904108}, {"id": 903, "seek": 368492, "start": 3695.48, "end": 3697.08, "text": " zero times", "tokens": [50892, 4018, 1413, 50972], "temperature": 0.0, "avg_logprob": -0.1502152478919839, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.01032728049904108}, {"id": 904, "seek": 368492, "start": 3697.08, "end": 3701.56, "text": " Where's q jq is zero and so jq is uh zero percent likely", "tokens": [50972, 2305, 311, 9505, 361, 80, 307, 4018, 293, 370, 361, 80, 307, 2232, 4018, 3043, 3700, 51196], "temperature": 0.0, "avg_logprob": -0.1502152478919839, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.01032728049904108}, {"id": 905, "seek": 368492, "start": 3702.2000000000003, "end": 3705.88, "text": " So it's actually kind of gross and people don't like this too much to fix this", "tokens": [51228, 407, 309, 311, 767, 733, 295, 11367, 293, 561, 500, 380, 411, 341, 886, 709, 281, 3191, 341, 51412], "temperature": 0.0, "avg_logprob": -0.1502152478919839, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.01032728049904108}, {"id": 906, "seek": 368492, "start": 3705.88, "end": 3710.2000000000003, "text": " There's a very simple fix that people like to do to sort of like smooth out your model a little bit", "tokens": [51412, 821, 311, 257, 588, 2199, 3191, 300, 561, 411, 281, 360, 281, 1333, 295, 411, 5508, 484, 428, 2316, 257, 707, 857, 51628], "temperature": 0.0, "avg_logprob": -0.1502152478919839, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.01032728049904108}, {"id": 907, "seek": 368492, "start": 3710.28, "end": 3712.12, "text": " It's called model smoothing", "tokens": [51632, 467, 311, 1219, 2316, 899, 6259, 571, 51724], "temperature": 0.0, "avg_logprob": -0.1502152478919839, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.01032728049904108}, {"id": 908, "seek": 371212, "start": 3712.12, "end": 3715.56, "text": " And roughly what's happening is that we will eight we will add some fake accounts", "tokens": [50364, 400, 9810, 437, 311, 2737, 307, 300, 321, 486, 3180, 321, 486, 909, 512, 7592, 9402, 50536], "temperature": 0.0, "avg_logprob": -0.12635314023053204, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0018385817529633641}, {"id": 909, "seek": 371212, "start": 3716.2799999999997, "end": 3719.72, "text": " So imagine adding a count of one to everything", "tokens": [50572, 407, 3811, 5127, 257, 1207, 295, 472, 281, 1203, 50744], "temperature": 0.0, "avg_logprob": -0.12635314023053204, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0018385817529633641}, {"id": 910, "seek": 371212, "start": 3721.0, "end": 3723.0, "text": " So we add a count of one", "tokens": [50808, 407, 321, 909, 257, 1207, 295, 472, 50908], "temperature": 0.0, "avg_logprob": -0.12635314023053204, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0018385817529633641}, {"id": 911, "seek": 371212, "start": 3723.4, "end": 3724.6, "text": " like this", "tokens": [50928, 411, 341, 50988], "temperature": 0.0, "avg_logprob": -0.12635314023053204, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0018385817529633641}, {"id": 912, "seek": 371212, "start": 3724.6, "end": 3726.6, "text": " And then we recalculate the probabilities", "tokens": [50988, 400, 550, 321, 850, 304, 2444, 473, 264, 33783, 51088], "temperature": 0.0, "avg_logprob": -0.12635314023053204, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0018385817529633641}, {"id": 913, "seek": 371212, "start": 3727.7999999999997, "end": 3732.2799999999997, "text": " And that's model smoothing and you can add as much as you like you can add five and that will give you a smoother model", "tokens": [51148, 400, 300, 311, 2316, 899, 6259, 571, 293, 291, 393, 909, 382, 709, 382, 291, 411, 291, 393, 909, 1732, 293, 300, 486, 976, 291, 257, 28640, 2316, 51372], "temperature": 0.0, "avg_logprob": -0.12635314023053204, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0018385817529633641}, {"id": 914, "seek": 371212, "start": 3732.92, "end": 3734.7599999999998, "text": " and the more you add here", "tokens": [51404, 293, 264, 544, 291, 909, 510, 51496], "temperature": 0.0, "avg_logprob": -0.12635314023053204, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0018385817529633641}, {"id": 915, "seek": 371212, "start": 3734.7599999999998, "end": 3738.6, "text": " The more uniform model you're gonna have and the less you add", "tokens": [51496, 440, 544, 9452, 2316, 291, 434, 799, 362, 293, 264, 1570, 291, 909, 51688], "temperature": 0.0, "avg_logprob": -0.12635314023053204, "compression_ratio": 1.8772727272727272, "no_speech_prob": 0.0018385817529633641}, {"id": 916, "seek": 373860, "start": 3739.56, "end": 3741.72, "text": " The more peaked model you are gonna have of course", "tokens": [50412, 440, 544, 520, 7301, 2316, 291, 366, 799, 362, 295, 1164, 50520], "temperature": 0.0, "avg_logprob": -0.14264445469297213, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.006003113929182291}, {"id": 917, "seek": 373860, "start": 3742.2799999999997, "end": 3744.2799999999997, "text": " So one is like a pretty decent", "tokens": [50548, 407, 472, 307, 411, 257, 1238, 8681, 50648], "temperature": 0.0, "avg_logprob": -0.14264445469297213, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.006003113929182291}, {"id": 918, "seek": 373860, "start": 3744.36, "end": 3749.64, "text": " Count to add and that will ensure that there will be no zeros in our probability matrix p", "tokens": [50652, 5247, 281, 909, 293, 300, 486, 5586, 300, 456, 486, 312, 572, 35193, 294, 527, 8482, 8141, 280, 50916], "temperature": 0.0, "avg_logprob": -0.14264445469297213, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.006003113929182291}, {"id": 919, "seek": 373860, "start": 3750.8399999999997, "end": 3755.7999999999997, "text": " And so this will of course change the generations a little bit in this case. It didn't buy it in principle. It could", "tokens": [50976, 400, 370, 341, 486, 295, 1164, 1319, 264, 10593, 257, 707, 857, 294, 341, 1389, 13, 467, 994, 380, 2256, 309, 294, 8665, 13, 467, 727, 51224], "temperature": 0.0, "avg_logprob": -0.14264445469297213, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.006003113929182291}, {"id": 920, "seek": 373860, "start": 3756.52, "end": 3759.48, "text": " But what that's going to do now is that nothing will be infinity", "tokens": [51260, 583, 437, 300, 311, 516, 281, 360, 586, 307, 300, 1825, 486, 312, 13202, 51408], "temperature": 0.0, "avg_logprob": -0.14264445469297213, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.006003113929182291}, {"id": 921, "seek": 373860, "start": 3760.04, "end": 3761.16, "text": " unlikely", "tokens": [51436, 17518, 51492], "temperature": 0.0, "avg_logprob": -0.14264445469297213, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.006003113929182291}, {"id": 922, "seek": 373860, "start": 3761.16, "end": 3762.2799999999997, "text": " So now", "tokens": [51492, 407, 586, 51548], "temperature": 0.0, "avg_logprob": -0.14264445469297213, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.006003113929182291}, {"id": 923, "seek": 373860, "start": 3762.2799999999997, "end": 3767.24, "text": " Our model will predict some other probability and we see that jq now has a very small probability", "tokens": [51548, 2621, 2316, 486, 6069, 512, 661, 8482, 293, 321, 536, 300, 361, 80, 586, 575, 257, 588, 1359, 8482, 51796], "temperature": 0.0, "avg_logprob": -0.14264445469297213, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.006003113929182291}, {"id": 924, "seek": 376724, "start": 3767.64, "end": 3772.8399999999997, "text": " So the model still finds it very surprising that this was a word or a by-gram, but we don't get negative infinity", "tokens": [50384, 407, 264, 2316, 920, 10704, 309, 588, 8830, 300, 341, 390, 257, 1349, 420, 257, 538, 12, 1342, 11, 457, 321, 500, 380, 483, 3671, 13202, 50644], "temperature": 0.0, "avg_logprob": -0.13616517213013796, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005138977896422148}, {"id": 925, "seek": 376724, "start": 3773.3199999999997, "end": 3776.68, "text": " So it's kind of like a nice fix that people like to apply sometimes and it's called model smoothing", "tokens": [50668, 407, 309, 311, 733, 295, 411, 257, 1481, 3191, 300, 561, 411, 281, 3079, 2171, 293, 309, 311, 1219, 2316, 899, 6259, 571, 50836], "temperature": 0.0, "avg_logprob": -0.13616517213013796, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005138977896422148}, {"id": 926, "seek": 376724, "start": 3777.08, "end": 3779.08, "text": " Okay, so we've now trained a respectable", "tokens": [50856, 1033, 11, 370, 321, 600, 586, 8895, 257, 44279, 50956], "temperature": 0.0, "avg_logprob": -0.13616517213013796, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005138977896422148}, {"id": 927, "seek": 376724, "start": 3779.3999999999996, "end": 3783.16, "text": " by-gram character level language model and we saw that we both", "tokens": [50972, 538, 12, 1342, 2517, 1496, 2856, 2316, 293, 321, 1866, 300, 321, 1293, 51160], "temperature": 0.0, "avg_logprob": -0.13616517213013796, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005138977896422148}, {"id": 928, "seek": 376724, "start": 3784.12, "end": 3787.3199999999997, "text": " Sort of trained the model by looking at the counts of all the by-grams", "tokens": [51208, 26149, 295, 8895, 264, 2316, 538, 1237, 412, 264, 14893, 295, 439, 264, 538, 12, 1342, 82, 51368], "temperature": 0.0, "avg_logprob": -0.13616517213013796, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005138977896422148}, {"id": 929, "seek": 376724, "start": 3787.7999999999997, "end": 3790.68, "text": " And normalizing the rows to get probability distributions", "tokens": [51392, 400, 2710, 3319, 264, 13241, 281, 483, 8482, 37870, 51536], "temperature": 0.0, "avg_logprob": -0.13616517213013796, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005138977896422148}, {"id": 930, "seek": 379068, "start": 3791.48, "end": 3797.8799999999997, "text": " We saw that we can also then use those parameters of this model to perform sampling of new words", "tokens": [50404, 492, 1866, 300, 321, 393, 611, 550, 764, 729, 9834, 295, 341, 2316, 281, 2042, 21179, 295, 777, 2283, 50724], "temperature": 0.0, "avg_logprob": -0.056840620308278876, "compression_ratio": 1.8352490421455938, "no_speech_prob": 0.02332782745361328}, {"id": 931, "seek": 379068, "start": 3799.48, "end": 3801.7999999999997, "text": " So we sample new names according to those distributions", "tokens": [50804, 407, 321, 6889, 777, 5288, 4650, 281, 729, 37870, 50920], "temperature": 0.0, "avg_logprob": -0.056840620308278876, "compression_ratio": 1.8352490421455938, "no_speech_prob": 0.02332782745361328}, {"id": 932, "seek": 379068, "start": 3802.2, "end": 3804.9199999999996, "text": " And we also saw that we can evaluate the quality of this model", "tokens": [50940, 400, 321, 611, 1866, 300, 321, 393, 13059, 264, 3125, 295, 341, 2316, 51076], "temperature": 0.0, "avg_logprob": -0.056840620308278876, "compression_ratio": 1.8352490421455938, "no_speech_prob": 0.02332782745361328}, {"id": 933, "seek": 379068, "start": 3805.3999999999996, "end": 3807.7999999999997, "text": " And the quality of this model is summarized in a single number", "tokens": [51100, 400, 264, 3125, 295, 341, 2316, 307, 14611, 1602, 294, 257, 2167, 1230, 51220], "temperature": 0.0, "avg_logprob": -0.056840620308278876, "compression_ratio": 1.8352490421455938, "no_speech_prob": 0.02332782745361328}, {"id": 934, "seek": 379068, "start": 3808.04, "end": 3812.6, "text": " Which is the negative log likelihood and the lower this number is the better the model is", "tokens": [51232, 3013, 307, 264, 3671, 3565, 22119, 293, 264, 3126, 341, 1230, 307, 264, 1101, 264, 2316, 307, 51460], "temperature": 0.0, "avg_logprob": -0.056840620308278876, "compression_ratio": 1.8352490421455938, "no_speech_prob": 0.02332782745361328}, {"id": 935, "seek": 379068, "start": 3813.3199999999997, "end": 3818.9199999999996, "text": " Because it is giving high probabilities to the actual next characters and all the by-grams in our training set", "tokens": [51496, 1436, 309, 307, 2902, 1090, 33783, 281, 264, 3539, 958, 4342, 293, 439, 264, 538, 12, 1342, 82, 294, 527, 3097, 992, 51776], "temperature": 0.0, "avg_logprob": -0.056840620308278876, "compression_ratio": 1.8352490421455938, "no_speech_prob": 0.02332782745361328}, {"id": 936, "seek": 381892, "start": 3819.64, "end": 3821.64, "text": " So that's all well and good", "tokens": [50400, 407, 300, 311, 439, 731, 293, 665, 50500], "temperature": 0.0, "avg_logprob": -0.10728121738807828, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.001477847807109356}, {"id": 937, "seek": 381892, "start": 3822.04, "end": 3826.04, "text": " But we've arrived at this model explicitly by doing something that felt sensible", "tokens": [50520, 583, 321, 600, 6678, 412, 341, 2316, 20803, 538, 884, 746, 300, 2762, 25380, 50720], "temperature": 0.0, "avg_logprob": -0.10728121738807828, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.001477847807109356}, {"id": 938, "seek": 381892, "start": 3826.12, "end": 3830.12, "text": " We were just performing counts and then we were normalizing those counts", "tokens": [50724, 492, 645, 445, 10205, 14893, 293, 550, 321, 645, 2710, 3319, 729, 14893, 50924], "temperature": 0.0, "avg_logprob": -0.10728121738807828, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.001477847807109356}, {"id": 939, "seek": 381892, "start": 3831.0, "end": 3833.7200000000003, "text": " Now what I would like to do is I would like to take an alternative approach", "tokens": [50968, 823, 437, 286, 576, 411, 281, 360, 307, 286, 576, 411, 281, 747, 364, 8535, 3109, 51104], "temperature": 0.0, "avg_logprob": -0.10728121738807828, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.001477847807109356}, {"id": 940, "seek": 381892, "start": 3834.04, "end": 3836.2000000000003, "text": " We will end up in a very very similar position", "tokens": [51120, 492, 486, 917, 493, 294, 257, 588, 588, 2531, 2535, 51228], "temperature": 0.0, "avg_logprob": -0.10728121738807828, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.001477847807109356}, {"id": 941, "seek": 381892, "start": 3836.44, "end": 3843.4, "text": " But the approach will look very different because I would like to cast the problem of by-gram character level language modeling into the neural network framework", "tokens": [51240, 583, 264, 3109, 486, 574, 588, 819, 570, 286, 576, 411, 281, 4193, 264, 1154, 295, 538, 12, 1342, 2517, 1496, 2856, 15983, 666, 264, 18161, 3209, 8388, 51588], "temperature": 0.0, "avg_logprob": -0.10728121738807828, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.001477847807109356}, {"id": 942, "seek": 384340, "start": 3844.2000000000003, "end": 3849.96, "text": " And in neural network framework, we're going to approach things slightly differently, but again end up in a very similar spot", "tokens": [50404, 400, 294, 18161, 3209, 8388, 11, 321, 434, 516, 281, 3109, 721, 4748, 7614, 11, 457, 797, 917, 493, 294, 257, 588, 2531, 4008, 50692], "temperature": 0.0, "avg_logprob": -0.09290436060741694, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.03256755322217941}, {"id": 943, "seek": 384340, "start": 3850.2000000000003, "end": 3851.96, "text": " I'll go into that later", "tokens": [50704, 286, 603, 352, 666, 300, 1780, 50792], "temperature": 0.0, "avg_logprob": -0.09290436060741694, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.03256755322217941}, {"id": 944, "seek": 384340, "start": 3851.96, "end": 3856.84, "text": " Now our neural network is going to be a still a by-gram character level language model", "tokens": [50792, 823, 527, 18161, 3209, 307, 516, 281, 312, 257, 920, 257, 538, 12, 1342, 2517, 1496, 2856, 2316, 51036], "temperature": 0.0, "avg_logprob": -0.09290436060741694, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.03256755322217941}, {"id": 945, "seek": 384340, "start": 3857.2400000000002, "end": 3859.7200000000003, "text": " So it receives a single character as an input", "tokens": [51056, 407, 309, 20717, 257, 2167, 2517, 382, 364, 4846, 51180], "temperature": 0.0, "avg_logprob": -0.09290436060741694, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.03256755322217941}, {"id": 946, "seek": 384340, "start": 3860.36, "end": 3863.32, "text": " Then there's neural network with some weights or some parameters w", "tokens": [51212, 1396, 456, 311, 18161, 3209, 365, 512, 17443, 420, 512, 9834, 261, 51360], "temperature": 0.0, "avg_logprob": -0.09290436060741694, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.03256755322217941}, {"id": 947, "seek": 384340, "start": 3864.2000000000003, "end": 3868.92, "text": " And it's going to output the probability distribution over the next character in a sequence", "tokens": [51404, 400, 309, 311, 516, 281, 5598, 264, 8482, 7316, 670, 264, 958, 2517, 294, 257, 8310, 51640], "temperature": 0.0, "avg_logprob": -0.09290436060741694, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.03256755322217941}, {"id": 948, "seek": 386892, "start": 3869.08, "end": 3874.6, "text": " It's going to make guesses as to what is likely to follow this character that was input to the model", "tokens": [50372, 467, 311, 516, 281, 652, 42703, 382, 281, 437, 307, 3700, 281, 1524, 341, 2517, 300, 390, 4846, 281, 264, 2316, 50648], "temperature": 0.0, "avg_logprob": -0.07490965000634055, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.010816991329193115}, {"id": 949, "seek": 386892, "start": 3875.96, "end": 3882.52, "text": " And then in addition to that we're going to be able to evaluate any setting of the parameters of the neural net because we have the loss function", "tokens": [50716, 400, 550, 294, 4500, 281, 300, 321, 434, 516, 281, 312, 1075, 281, 13059, 604, 3287, 295, 264, 9834, 295, 264, 18161, 2533, 570, 321, 362, 264, 4470, 2445, 51044], "temperature": 0.0, "avg_logprob": -0.07490965000634055, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.010816991329193115}, {"id": 950, "seek": 386892, "start": 3883.7200000000003, "end": 3888.76, "text": " The negative log likelihood. So we're going to take a look at its probability distributions and we're going to use the labels", "tokens": [51104, 440, 3671, 3565, 22119, 13, 407, 321, 434, 516, 281, 747, 257, 574, 412, 1080, 8482, 37870, 293, 321, 434, 516, 281, 764, 264, 16949, 51356], "temperature": 0.0, "avg_logprob": -0.07490965000634055, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.010816991329193115}, {"id": 951, "seek": 386892, "start": 3890.12, "end": 3894.12, "text": " Which are basically just the identity of the next character in that by-gram the second character", "tokens": [51424, 3013, 366, 1936, 445, 264, 6575, 295, 264, 958, 2517, 294, 300, 538, 12, 1342, 264, 1150, 2517, 51624], "temperature": 0.0, "avg_logprob": -0.07490965000634055, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.010816991329193115}, {"id": 952, "seek": 389412, "start": 3894.68, "end": 3897.88, "text": " So knowing what the second character actually comes next in the by-gram", "tokens": [50392, 407, 5276, 437, 264, 1150, 2517, 767, 1487, 958, 294, 264, 538, 12, 1342, 50552], "temperature": 0.0, "avg_logprob": -0.07372571576026178, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0028893742710351944}, {"id": 953, "seek": 389412, "start": 3898.2, "end": 3903.24, "text": " Allows us to then look at what how high of probability the model assigns to that character", "tokens": [50568, 1057, 1509, 505, 281, 550, 574, 412, 437, 577, 1090, 295, 8482, 264, 2316, 6269, 82, 281, 300, 2517, 50820], "temperature": 0.0, "avg_logprob": -0.07372571576026178, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0028893742710351944}, {"id": 954, "seek": 389412, "start": 3903.88, "end": 3906.12, "text": " And then we of course want the probability to be very high", "tokens": [50852, 400, 550, 321, 295, 1164, 528, 264, 8482, 281, 312, 588, 1090, 50964], "temperature": 0.0, "avg_logprob": -0.07372571576026178, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0028893742710351944}, {"id": 955, "seek": 389412, "start": 3907.0, "end": 3909.72, "text": " And that is another way of saying that the loss is low", "tokens": [51008, 400, 300, 307, 1071, 636, 295, 1566, 300, 264, 4470, 307, 2295, 51144], "temperature": 0.0, "avg_logprob": -0.07372571576026178, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0028893742710351944}, {"id": 956, "seek": 389412, "start": 3910.8399999999997, "end": 3915.0, "text": " So we're going to use gradient based optimization then to tune the parameters of this network", "tokens": [51200, 407, 321, 434, 516, 281, 764, 16235, 2361, 19618, 550, 281, 10864, 264, 9834, 295, 341, 3209, 51408], "temperature": 0.0, "avg_logprob": -0.07372571576026178, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0028893742710351944}, {"id": 957, "seek": 389412, "start": 3915.4, "end": 3918.12, "text": " Because we have the loss function and we're going to minimize it", "tokens": [51428, 1436, 321, 362, 264, 4470, 2445, 293, 321, 434, 516, 281, 17522, 309, 51564], "temperature": 0.0, "avg_logprob": -0.07372571576026178, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0028893742710351944}, {"id": 958, "seek": 389412, "start": 3918.3599999999997, "end": 3923.56, "text": " So we're going to tune the weights so that the neural net is correctly predicting the probabilities for the next character", "tokens": [51576, 407, 321, 434, 516, 281, 10864, 264, 17443, 370, 300, 264, 18161, 2533, 307, 8944, 32884, 264, 33783, 337, 264, 958, 2517, 51836], "temperature": 0.0, "avg_logprob": -0.07372571576026178, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0028893742710351944}, {"id": 959, "seek": 392412, "start": 3924.3599999999997, "end": 3929.48, "text": " So let's get started. The first thing I want to do is I want to compile the training set of this neural network, right?", "tokens": [50376, 407, 718, 311, 483, 1409, 13, 440, 700, 551, 286, 528, 281, 360, 307, 286, 528, 281, 31413, 264, 3097, 992, 295, 341, 18161, 3209, 11, 558, 30, 50632], "temperature": 0.0, "avg_logprob": -0.09521078382219587, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0017005748813971877}, {"id": 960, "seek": 392412, "start": 3929.56, "end": 3932.12, "text": " So create the training set", "tokens": [50636, 407, 1884, 264, 3097, 992, 50764], "temperature": 0.0, "avg_logprob": -0.09521078382219587, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0017005748813971877}, {"id": 961, "seek": 392412, "start": 3933.08, "end": 3935.08, "text": " of all the by-grams", "tokens": [50812, 295, 439, 264, 538, 12, 1342, 82, 50912], "temperature": 0.0, "avg_logprob": -0.09521078382219587, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0017005748813971877}, {"id": 962, "seek": 392412, "start": 3936.3599999999997, "end": 3938.3599999999997, "text": " Okay, and", "tokens": [50976, 1033, 11, 293, 51076], "temperature": 0.0, "avg_logprob": -0.09521078382219587, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0017005748813971877}, {"id": 963, "seek": 392412, "start": 3939.4, "end": 3942.7599999999998, "text": " Here I'm going to copy paste this code", "tokens": [51128, 1692, 286, 478, 516, 281, 5055, 9163, 341, 3089, 51296], "temperature": 0.0, "avg_logprob": -0.09521078382219587, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0017005748813971877}, {"id": 964, "seek": 392412, "start": 3943.72, "end": 3945.72, "text": " Because this code iterates over all the by-grams", "tokens": [51344, 1436, 341, 3089, 17138, 1024, 670, 439, 264, 538, 12, 1342, 82, 51444], "temperature": 0.0, "avg_logprob": -0.09521078382219587, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0017005748813971877}, {"id": 965, "seek": 392412, "start": 3947.4, "end": 3952.68, "text": " So here we start with the words we iterate over all the by-grams and previously as you recall we did the counts", "tokens": [51528, 407, 510, 321, 722, 365, 264, 2283, 321, 44497, 670, 439, 264, 538, 12, 1342, 82, 293, 8046, 382, 291, 9901, 321, 630, 264, 14893, 51792], "temperature": 0.0, "avg_logprob": -0.09521078382219587, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0017005748813971877}, {"id": 966, "seek": 395268, "start": 3953.0, "end": 3956.04, "text": " But now we're not going to do counts. We're just creating a training set", "tokens": [50380, 583, 586, 321, 434, 406, 516, 281, 360, 14893, 13, 492, 434, 445, 4084, 257, 3097, 992, 50532], "temperature": 0.0, "avg_logprob": -0.08516591660519864, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.0022516704630106688}, {"id": 967, "seek": 395268, "start": 3956.9199999999996, "end": 3959.72, "text": " Now this training set will be made up of two lists", "tokens": [50576, 823, 341, 3097, 992, 486, 312, 1027, 493, 295, 732, 14511, 50716], "temperature": 0.0, "avg_logprob": -0.08516591660519864, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.0022516704630106688}, {"id": 968, "seek": 395268, "start": 3962.12, "end": 3964.12, "text": " We have the", "tokens": [50836, 492, 362, 264, 50936], "temperature": 0.0, "avg_logprob": -0.08516591660519864, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.0022516704630106688}, {"id": 969, "seek": 395268, "start": 3964.7599999999998, "end": 3966.2799999999997, "text": " inputs", "tokens": [50968, 15743, 51044], "temperature": 0.0, "avg_logprob": -0.08516591660519864, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.0022516704630106688}, {"id": 970, "seek": 395268, "start": 3966.2799999999997, "end": 3968.6, "text": " And the targets the the labels", "tokens": [51044, 400, 264, 12911, 264, 264, 16949, 51160], "temperature": 0.0, "avg_logprob": -0.08516591660519864, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.0022516704630106688}, {"id": 971, "seek": 395268, "start": 3969.48, "end": 3972.44, "text": " And these by-grams will denote x y those are the characters, right?", "tokens": [51204, 400, 613, 538, 12, 1342, 82, 486, 45708, 2031, 288, 729, 366, 264, 4342, 11, 558, 30, 51352], "temperature": 0.0, "avg_logprob": -0.08516591660519864, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.0022516704630106688}, {"id": 972, "seek": 395268, "start": 3973.16, "end": 3976.9199999999996, "text": " And so we're given the first character of the by-gram and then we're trying to predict the next one", "tokens": [51388, 400, 370, 321, 434, 2212, 264, 700, 2517, 295, 264, 538, 12, 1342, 293, 550, 321, 434, 1382, 281, 6069, 264, 958, 472, 51576], "temperature": 0.0, "avg_logprob": -0.08516591660519864, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.0022516704630106688}, {"id": 973, "seek": 397692, "start": 3977.7200000000003, "end": 3982.52, "text": " Both of these are going to be integers. So here we'll take x's that append is just", "tokens": [50404, 6767, 295, 613, 366, 516, 281, 312, 41674, 13, 407, 510, 321, 603, 747, 2031, 311, 300, 34116, 307, 445, 50644], "temperature": 0.0, "avg_logprob": -0.16433456708800118, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.015187254175543785}, {"id": 974, "seek": 397692, "start": 3983.56, "end": 3985.96, "text": " x1 y's that append ix2", "tokens": [50696, 2031, 16, 288, 311, 300, 34116, 741, 87, 17, 50816], "temperature": 0.0, "avg_logprob": -0.16433456708800118, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.015187254175543785}, {"id": 975, "seek": 397692, "start": 3987.64, "end": 3989.32, "text": " And then here", "tokens": [50900, 400, 550, 510, 50984], "temperature": 0.0, "avg_logprob": -0.16433456708800118, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.015187254175543785}, {"id": 976, "seek": 397692, "start": 3989.32, "end": 3995.7200000000003, "text": " We actually don't want lists of integers. We will create uh tensors out of these. So x's is torched dot tensor", "tokens": [50984, 492, 767, 500, 380, 528, 14511, 295, 41674, 13, 492, 486, 1884, 2232, 10688, 830, 484, 295, 613, 13, 407, 2031, 311, 307, 3930, 19318, 5893, 40863, 51304], "temperature": 0.0, "avg_logprob": -0.16433456708800118, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.015187254175543785}, {"id": 977, "seek": 397692, "start": 3996.52, "end": 3999.88, "text": " x's and y's is torched dot tensor of y's", "tokens": [51344, 2031, 311, 293, 288, 311, 307, 3930, 19318, 5893, 40863, 295, 288, 311, 51512], "temperature": 0.0, "avg_logprob": -0.16433456708800118, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.015187254175543785}, {"id": 978, "seek": 397692, "start": 4001.48, "end": 4006.36, "text": " And then we don't actually want to take all the words just yet because I want everything to be manageable", "tokens": [51592, 400, 550, 321, 500, 380, 767, 528, 281, 747, 439, 264, 2283, 445, 1939, 570, 286, 528, 1203, 281, 312, 38798, 51836], "temperature": 0.0, "avg_logprob": -0.16433456708800118, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.015187254175543785}, {"id": 979, "seek": 400692, "start": 4007.0, "end": 4009.2400000000002, "text": " So let's just do the first word which is emma", "tokens": [50368, 407, 718, 311, 445, 360, 264, 700, 1349, 597, 307, 846, 1696, 50480], "temperature": 0.0, "avg_logprob": -0.14376471119542275, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.00039202856714837253}, {"id": 980, "seek": 400692, "start": 4011.2400000000002, "end": 4013.64, "text": " And then it's clear what these x's and y's would be", "tokens": [50580, 400, 550, 309, 311, 1850, 437, 613, 2031, 311, 293, 288, 311, 576, 312, 50700], "temperature": 0.0, "avg_logprob": -0.14376471119542275, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.00039202856714837253}, {"id": 981, "seek": 400692, "start": 4015.4, "end": 4017.4, "text": " Here let me print", "tokens": [50788, 1692, 718, 385, 4482, 50888], "temperature": 0.0, "avg_logprob": -0.14376471119542275, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.00039202856714837253}, {"id": 982, "seek": 400692, "start": 4017.88, "end": 4020.44, "text": " Character one character two just so you see what's going on here", "tokens": [50912, 36786, 472, 2517, 732, 445, 370, 291, 536, 437, 311, 516, 322, 510, 51040], "temperature": 0.0, "avg_logprob": -0.14376471119542275, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.00039202856714837253}, {"id": 983, "seek": 400692, "start": 4021.56, "end": 4028.28, "text": " So the by-grams of these characters is dot e e m m m a dot", "tokens": [51096, 407, 264, 538, 12, 1342, 82, 295, 613, 4342, 307, 5893, 308, 308, 275, 275, 275, 257, 5893, 51432], "temperature": 0.0, "avg_logprob": -0.14376471119542275, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.00039202856714837253}, {"id": 984, "seek": 400692, "start": 4028.76, "end": 4033.8, "text": " So this single word as I mentioned has one two three four five examples for our neural network", "tokens": [51456, 407, 341, 2167, 1349, 382, 286, 2835, 575, 472, 732, 1045, 1451, 1732, 5110, 337, 527, 18161, 3209, 51708], "temperature": 0.0, "avg_logprob": -0.14376471119542275, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.00039202856714837253}, {"id": 985, "seek": 403380, "start": 4034.6800000000003, "end": 4036.6800000000003, "text": " There are five separate examples in emma", "tokens": [50408, 821, 366, 1732, 4994, 5110, 294, 846, 1696, 50508], "temperature": 0.0, "avg_logprob": -0.1280786476883234, "compression_ratio": 2.0232558139534884, "no_speech_prob": 0.005300919059664011}, {"id": 986, "seek": 403380, "start": 4037.5600000000004, "end": 4042.1200000000003, "text": " And those examples are summarized here when the input to the neural neural network is integer zero", "tokens": [50552, 400, 729, 5110, 366, 14611, 1602, 510, 562, 264, 4846, 281, 264, 18161, 18161, 3209, 307, 24922, 4018, 50780], "temperature": 0.0, "avg_logprob": -0.1280786476883234, "compression_ratio": 2.0232558139534884, "no_speech_prob": 0.005300919059664011}, {"id": 987, "seek": 403380, "start": 4043.1600000000003, "end": 4047.1600000000003, "text": " The desired label is integer five which corresponds to e", "tokens": [50832, 440, 14721, 7645, 307, 24922, 1732, 597, 23249, 281, 308, 51032], "temperature": 0.0, "avg_logprob": -0.1280786476883234, "compression_ratio": 2.0232558139534884, "no_speech_prob": 0.005300919059664011}, {"id": 988, "seek": 403380, "start": 4047.96, "end": 4054.44, "text": " When the input to the neural network is five, we want its weights to be arranged so that 13 gets a very high probability", "tokens": [51072, 1133, 264, 4846, 281, 264, 18161, 3209, 307, 1732, 11, 321, 528, 1080, 17443, 281, 312, 18721, 370, 300, 3705, 2170, 257, 588, 1090, 8482, 51396], "temperature": 0.0, "avg_logprob": -0.1280786476883234, "compression_ratio": 2.0232558139534884, "no_speech_prob": 0.005300919059664011}, {"id": 989, "seek": 403380, "start": 4055.0800000000004, "end": 4058.52, "text": " When 13 is put in we want 13 to have a high probability", "tokens": [51428, 1133, 3705, 307, 829, 294, 321, 528, 3705, 281, 362, 257, 1090, 8482, 51600], "temperature": 0.0, "avg_logprob": -0.1280786476883234, "compression_ratio": 2.0232558139534884, "no_speech_prob": 0.005300919059664011}, {"id": 990, "seek": 403380, "start": 4059.1600000000003, "end": 4062.52, "text": " When 13 is put in we also want one to have a high probability", "tokens": [51632, 1133, 3705, 307, 829, 294, 321, 611, 528, 472, 281, 362, 257, 1090, 8482, 51800], "temperature": 0.0, "avg_logprob": -0.1280786476883234, "compression_ratio": 2.0232558139534884, "no_speech_prob": 0.005300919059664011}, {"id": 991, "seek": 406252, "start": 4063.48, "end": 4066.92, "text": " When one is input we want zero to have a very high probability", "tokens": [50412, 1133, 472, 307, 4846, 321, 528, 4018, 281, 362, 257, 588, 1090, 8482, 50584], "temperature": 0.0, "avg_logprob": -0.10927751748868735, "compression_ratio": 1.62890625, "no_speech_prob": 0.00047282534069381654}, {"id": 992, "seek": 406252, "start": 4067.4, "end": 4070.6, "text": " So there are five separate input examples to a neural net", "tokens": [50608, 407, 456, 366, 1732, 4994, 4846, 5110, 281, 257, 18161, 2533, 50768], "temperature": 0.0, "avg_logprob": -0.10927751748868735, "compression_ratio": 1.62890625, "no_speech_prob": 0.00047282534069381654}, {"id": 993, "seek": 406252, "start": 4071.4, "end": 4073.4, "text": " in this data set", "tokens": [50808, 294, 341, 1412, 992, 50908], "temperature": 0.0, "avg_logprob": -0.10927751748868735, "compression_ratio": 1.62890625, "no_speech_prob": 0.00047282534069381654}, {"id": 994, "seek": 406252, "start": 4075.0, "end": 4080.44, "text": " I wanted to add a tangent of a note of caution to be careful with a lot of the apis of some of these frameworks", "tokens": [50988, 286, 1415, 281, 909, 257, 27747, 295, 257, 3637, 295, 23585, 281, 312, 5026, 365, 257, 688, 295, 264, 1882, 271, 295, 512, 295, 613, 29834, 51260], "temperature": 0.0, "avg_logprob": -0.10927751748868735, "compression_ratio": 1.62890625, "no_speech_prob": 0.00047282534069381654}, {"id": 995, "seek": 406252, "start": 4081.4, "end": 4087.16, "text": " You saw me silently use torch dot tensor with a lowercase t and the output looked right", "tokens": [51308, 509, 1866, 385, 40087, 764, 27822, 5893, 40863, 365, 257, 3126, 9765, 256, 293, 264, 5598, 2956, 558, 51596], "temperature": 0.0, "avg_logprob": -0.10927751748868735, "compression_ratio": 1.62890625, "no_speech_prob": 0.00047282534069381654}, {"id": 996, "seek": 406252, "start": 4087.8, "end": 4091.24, "text": " But you should be aware that there's actually two ways of constructing a tensor", "tokens": [51628, 583, 291, 820, 312, 3650, 300, 456, 311, 767, 732, 2098, 295, 39969, 257, 40863, 51800], "temperature": 0.0, "avg_logprob": -0.10927751748868735, "compression_ratio": 1.62890625, "no_speech_prob": 0.00047282534069381654}, {"id": 997, "seek": 409124, "start": 4091.72, "end": 4097.88, "text": " There's a torch dot lowercase tensor and there's also a torch dot capital tensor class, which you can also construct", "tokens": [50388, 821, 311, 257, 27822, 5893, 3126, 9765, 40863, 293, 456, 311, 611, 257, 27822, 5893, 4238, 40863, 1508, 11, 597, 291, 393, 611, 7690, 50696], "temperature": 0.0, "avg_logprob": -0.12999495753535517, "compression_ratio": 1.952991452991453, "no_speech_prob": 0.00460915919393301}, {"id": 998, "seek": 409124, "start": 4098.679999999999, "end": 4102.04, "text": " So you can actually call both you can also do torch dot capital tensor", "tokens": [50736, 407, 291, 393, 767, 818, 1293, 291, 393, 611, 360, 27822, 5893, 4238, 40863, 50904], "temperature": 0.0, "avg_logprob": -0.12999495753535517, "compression_ratio": 1.952991452991453, "no_speech_prob": 0.00460915919393301}, {"id": 999, "seek": 409124, "start": 4102.92, "end": 4104.92, "text": " And you get an x as in y as well", "tokens": [50948, 400, 291, 483, 364, 2031, 382, 294, 288, 382, 731, 51048], "temperature": 0.0, "avg_logprob": -0.12999495753535517, "compression_ratio": 1.952991452991453, "no_speech_prob": 0.00460915919393301}, {"id": 1000, "seek": 409124, "start": 4105.4, "end": 4107.4, "text": " So that's not confusing at all", "tokens": [51072, 407, 300, 311, 406, 13181, 412, 439, 51172], "temperature": 0.0, "avg_logprob": -0.12999495753535517, "compression_ratio": 1.952991452991453, "no_speech_prob": 0.00460915919393301}, {"id": 1001, "seek": 409124, "start": 4108.92, "end": 4112.44, "text": " There are threads on what is the difference between these two and um", "tokens": [51248, 821, 366, 19314, 322, 437, 307, 264, 2649, 1296, 613, 732, 293, 1105, 51424], "temperature": 0.0, "avg_logprob": -0.12999495753535517, "compression_ratio": 1.952991452991453, "no_speech_prob": 0.00460915919393301}, {"id": 1002, "seek": 409124, "start": 4113.4, "end": 4120.12, "text": " Unfortunately, the docs are just like not clear on the difference and when you look at the the docs of lowercase tensor construct tensor", "tokens": [51472, 8590, 11, 264, 45623, 366, 445, 411, 406, 1850, 322, 264, 2649, 293, 562, 291, 574, 412, 264, 264, 45623, 295, 3126, 9765, 40863, 7690, 40863, 51808], "temperature": 0.0, "avg_logprob": -0.12999495753535517, "compression_ratio": 1.952991452991453, "no_speech_prob": 0.00460915919393301}, {"id": 1003, "seek": 412012, "start": 4120.2, "end": 4122.5199999999995, "text": " With no autograd history by copying data", "tokens": [50368, 2022, 572, 1476, 664, 6206, 2503, 538, 27976, 1412, 50484], "temperature": 0.0, "avg_logprob": -0.15536656813188032, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.001956854946911335}, {"id": 1004, "seek": 412012, "start": 4123.64, "end": 4125.64, "text": " It's just like it doesn't", "tokens": [50540, 467, 311, 445, 411, 309, 1177, 380, 50640], "temperature": 0.0, "avg_logprob": -0.15536656813188032, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.001956854946911335}, {"id": 1005, "seek": 412012, "start": 4125.64, "end": 4131.0, "text": " It doesn't make sense. So the actual difference as far as I can tell is explained eventually in this random thread that you can google", "tokens": [50640, 467, 1177, 380, 652, 2020, 13, 407, 264, 3539, 2649, 382, 1400, 382, 286, 393, 980, 307, 8825, 4728, 294, 341, 4974, 7207, 300, 291, 393, 20742, 50908], "temperature": 0.0, "avg_logprob": -0.15536656813188032, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.001956854946911335}, {"id": 1006, "seek": 412012, "start": 4131.64, "end": 4133.64, "text": " And really it comes down to I believe", "tokens": [50940, 400, 534, 309, 1487, 760, 281, 286, 1697, 51040], "temperature": 0.0, "avg_logprob": -0.15536656813188032, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.001956854946911335}, {"id": 1007, "seek": 412012, "start": 4135.08, "end": 4137.08, "text": " That um, where is this?", "tokens": [51112, 663, 1105, 11, 689, 307, 341, 30, 51212], "temperature": 0.0, "avg_logprob": -0.15536656813188032, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.001956854946911335}, {"id": 1008, "seek": 412012, "start": 4138.5199999999995, "end": 4143.72, "text": " Torch dot tensor in first the d type the data type automatically while torch dot tensor just returns a float tensor", "tokens": [51284, 7160, 339, 5893, 40863, 294, 700, 264, 274, 2010, 264, 1412, 2010, 6772, 1339, 27822, 5893, 40863, 445, 11247, 257, 15706, 40863, 51544], "temperature": 0.0, "avg_logprob": -0.15536656813188032, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.001956854946911335}, {"id": 1009, "seek": 412012, "start": 4144.36, "end": 4146.68, "text": " I would recommend stick to torch dot lowercase tensor", "tokens": [51576, 286, 576, 2748, 2897, 281, 27822, 5893, 3126, 9765, 40863, 51692], "temperature": 0.0, "avg_logprob": -0.15536656813188032, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.001956854946911335}, {"id": 1010, "seek": 412012, "start": 4147.8, "end": 4149.64, "text": " so um", "tokens": [51748, 370, 1105, 51840], "temperature": 0.0, "avg_logprob": -0.15536656813188032, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.001956854946911335}, {"id": 1011, "seek": 414964, "start": 4149.64, "end": 4156.76, "text": " Indeed we see that when I construct this with a capital t the data type here of x's is float 32", "tokens": [50364, 15061, 321, 536, 300, 562, 286, 7690, 341, 365, 257, 4238, 256, 264, 1412, 2010, 510, 295, 2031, 311, 307, 15706, 8858, 50720], "temperature": 0.0, "avg_logprob": -0.14352235339936756, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.0005614363471977413}, {"id": 1012, "seek": 414964, "start": 4158.12, "end": 4160.12, "text": " But torch dot lowercase tensor", "tokens": [50788, 583, 27822, 5893, 3126, 9765, 40863, 50888], "temperature": 0.0, "avg_logprob": -0.14352235339936756, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.0005614363471977413}, {"id": 1013, "seek": 414964, "start": 4161.08, "end": 4165.08, "text": " You see how it's now x dot d type is now integer", "tokens": [50936, 509, 536, 577, 309, 311, 586, 2031, 5893, 274, 2010, 307, 586, 24922, 51136], "temperature": 0.0, "avg_logprob": -0.14352235339936756, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.0005614363471977413}, {"id": 1014, "seek": 414964, "start": 4166.76, "end": 4168.200000000001, "text": " So, um", "tokens": [51220, 407, 11, 1105, 51292], "temperature": 0.0, "avg_logprob": -0.14352235339936756, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.0005614363471977413}, {"id": 1015, "seek": 414964, "start": 4168.200000000001, "end": 4173.400000000001, "text": " It's advised that you use lowercase t and you can read more about it if you like in some of these threads", "tokens": [51292, 467, 311, 26269, 300, 291, 764, 3126, 9765, 256, 293, 291, 393, 1401, 544, 466, 309, 498, 291, 411, 294, 512, 295, 613, 19314, 51552], "temperature": 0.0, "avg_logprob": -0.14352235339936756, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.0005614363471977413}, {"id": 1016, "seek": 414964, "start": 4174.04, "end": 4176.04, "text": " but basically", "tokens": [51584, 457, 1936, 51684], "temperature": 0.0, "avg_logprob": -0.14352235339936756, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.0005614363471977413}, {"id": 1017, "seek": 417604, "start": 4176.6, "end": 4181.56, "text": " I'm pointing out some of these things because because I want to caution you and I want you to read get used to reading a", "tokens": [50392, 286, 478, 12166, 484, 512, 295, 613, 721, 570, 570, 286, 528, 281, 23585, 291, 293, 286, 528, 291, 281, 1401, 483, 1143, 281, 3760, 257, 50640], "temperature": 0.0, "avg_logprob": -0.12896800407996545, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.00394488126039505}, {"id": 1018, "seek": 417604, "start": 4181.56, "end": 4187.8, "text": " lot of documentation and reading through a lot of uh q and a's and threads like this and um", "tokens": [50640, 688, 295, 14333, 293, 3760, 807, 257, 688, 295, 2232, 9505, 293, 257, 311, 293, 19314, 411, 341, 293, 1105, 50952], "temperature": 0.0, "avg_logprob": -0.12896800407996545, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.00394488126039505}, {"id": 1019, "seek": 417604, "start": 4188.28, "end": 4192.28, "text": " You know some of this stuff is unfortunately not easy and not very well documented and you have to be careful out there", "tokens": [50976, 509, 458, 512, 295, 341, 1507, 307, 7015, 406, 1858, 293, 406, 588, 731, 23007, 293, 291, 362, 281, 312, 5026, 484, 456, 51176], "temperature": 0.0, "avg_logprob": -0.12896800407996545, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.00394488126039505}, {"id": 1020, "seek": 417604, "start": 4192.68, "end": 4196.44, "text": " What we want here is integers because that's what makes sense", "tokens": [51196, 708, 321, 528, 510, 307, 41674, 570, 300, 311, 437, 1669, 2020, 51384], "temperature": 0.0, "avg_logprob": -0.12896800407996545, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.00394488126039505}, {"id": 1021, "seek": 417604, "start": 4197.08, "end": 4198.04, "text": " um", "tokens": [51416, 1105, 51464], "temperature": 0.0, "avg_logprob": -0.12896800407996545, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.00394488126039505}, {"id": 1022, "seek": 417604, "start": 4198.04, "end": 4199.48, "text": " and so", "tokens": [51464, 293, 370, 51536], "temperature": 0.0, "avg_logprob": -0.12896800407996545, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.00394488126039505}, {"id": 1023, "seek": 417604, "start": 4199.48, "end": 4205.56, "text": " Lowercase tensor is what we are using. Okay. Now. We want to think through how we're going to feed in these examples into a neural network", "tokens": [51536, 25523, 9765, 40863, 307, 437, 321, 366, 1228, 13, 1033, 13, 823, 13, 492, 528, 281, 519, 807, 577, 321, 434, 516, 281, 3154, 294, 613, 5110, 666, 257, 18161, 3209, 51840], "temperature": 0.0, "avg_logprob": -0.12896800407996545, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.00394488126039505}, {"id": 1024, "seek": 420604, "start": 4206.2, "end": 4208.2, "text": " Now it's not quite as straightforward as", "tokens": [50372, 823, 309, 311, 406, 1596, 382, 15325, 382, 50472], "temperature": 0.0, "avg_logprob": -0.12574762511021884, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.001206454006023705}, {"id": 1025, "seek": 420604, "start": 4209.08, "end": 4214.36, "text": " Plugging it in because these examples right now are integers. So there's like a 0 5 or 13", "tokens": [50516, 40740, 3249, 309, 294, 570, 613, 5110, 558, 586, 366, 41674, 13, 407, 456, 311, 411, 257, 1958, 1025, 420, 3705, 50780], "temperature": 0.0, "avg_logprob": -0.12574762511021884, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.001206454006023705}, {"id": 1026, "seek": 420604, "start": 4214.6, "end": 4219.08, "text": " It gives us the index of the character and you can't just plug an integer index into a neural net", "tokens": [50792, 467, 2709, 505, 264, 8186, 295, 264, 2517, 293, 291, 393, 380, 445, 5452, 364, 24922, 8186, 666, 257, 18161, 2533, 51016], "temperature": 0.0, "avg_logprob": -0.12574762511021884, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.001206454006023705}, {"id": 1027, "seek": 420604, "start": 4219.96, "end": 4226.5199999999995, "text": " these neural nets, uh, right are sort of made up of these neurons and uh, these neurons have weights", "tokens": [51060, 613, 18161, 36170, 11, 2232, 11, 558, 366, 1333, 295, 1027, 493, 295, 613, 22027, 293, 2232, 11, 613, 22027, 362, 17443, 51388], "temperature": 0.0, "avg_logprob": -0.12574762511021884, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.001206454006023705}, {"id": 1028, "seek": 420604, "start": 4226.84, "end": 4232.2, "text": " And as you saw in micrograd these weights act multiplicatively on the inputs w x plus b", "tokens": [51404, 400, 382, 291, 1866, 294, 4532, 7165, 613, 17443, 605, 17596, 19020, 322, 264, 15743, 261, 2031, 1804, 272, 51672], "temperature": 0.0, "avg_logprob": -0.12574762511021884, "compression_ratio": 1.6814516129032258, "no_speech_prob": 0.001206454006023705}, {"id": 1029, "seek": 423220, "start": 4232.5199999999995, "end": 4236.36, "text": " There's 10 hs and so on and so it doesn't really make sense to make an input neuron", "tokens": [50380, 821, 311, 1266, 276, 82, 293, 370, 322, 293, 370, 309, 1177, 380, 534, 652, 2020, 281, 652, 364, 4846, 34090, 50572], "temperature": 0.0, "avg_logprob": -0.12830700064605138, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.009858689270913601}, {"id": 1030, "seek": 423220, "start": 4236.44, "end": 4240.84, "text": " Take on integer values that you feed in and then multiply on with weights", "tokens": [50576, 3664, 322, 24922, 4190, 300, 291, 3154, 294, 293, 550, 12972, 322, 365, 17443, 50796], "temperature": 0.0, "avg_logprob": -0.12830700064605138, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.009858689270913601}, {"id": 1031, "seek": 423220, "start": 4241.72, "end": 4246.28, "text": " So instead a common way of encoding integers is what's called one hot encoding", "tokens": [50840, 407, 2602, 257, 2689, 636, 295, 43430, 41674, 307, 437, 311, 1219, 472, 2368, 43430, 51068], "temperature": 0.0, "avg_logprob": -0.12830700064605138, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.009858689270913601}, {"id": 1032, "seek": 423220, "start": 4247.0, "end": 4253.24, "text": " In one hot encoding, uh, we take an integer like 13 and we create a vector that is all zeros", "tokens": [51104, 682, 472, 2368, 43430, 11, 2232, 11, 321, 747, 364, 24922, 411, 3705, 293, 321, 1884, 257, 8062, 300, 307, 439, 35193, 51416], "temperature": 0.0, "avg_logprob": -0.12830700064605138, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.009858689270913601}, {"id": 1033, "seek": 423220, "start": 4253.639999999999, "end": 4260.04, "text": " Except for the 13th dimension which we turn to a one and then that vector can feed into a neural net", "tokens": [51436, 16192, 337, 264, 3705, 392, 10139, 597, 321, 1261, 281, 257, 472, 293, 550, 300, 8062, 393, 3154, 666, 257, 18161, 2533, 51756], "temperature": 0.0, "avg_logprob": -0.12830700064605138, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.009858689270913601}, {"id": 1034, "seek": 426004, "start": 4261.0, "end": 4265.48, "text": " Now conveniently, uh, PyTorch actually has something called the one hot", "tokens": [50412, 823, 44375, 11, 2232, 11, 9953, 51, 284, 339, 767, 575, 746, 1219, 264, 472, 2368, 50636], "temperature": 0.0, "avg_logprob": -0.21011189619700113, "compression_ratio": 1.55, "no_speech_prob": 0.0017006383277475834}, {"id": 1035, "seek": 426004, "start": 4267.88, "end": 4272.44, "text": " Function inside torch and in functional it takes a tensor made up of integers", "tokens": [50756, 11166, 882, 1854, 27822, 293, 294, 11745, 309, 2516, 257, 40863, 1027, 493, 295, 41674, 50984], "temperature": 0.0, "avg_logprob": -0.21011189619700113, "compression_ratio": 1.55, "no_speech_prob": 0.0017006383277475834}, {"id": 1036, "seek": 426004, "start": 4275.0, "end": 4277.4, "text": " Long is a is a is an integer", "tokens": [51112, 8282, 307, 257, 307, 257, 307, 364, 24922, 51232], "temperature": 0.0, "avg_logprob": -0.21011189619700113, "compression_ratio": 1.55, "no_speech_prob": 0.0017006383277475834}, {"id": 1037, "seek": 426004, "start": 4279.16, "end": 4286.28, "text": " And it also takes a number of classes, um, which is how large you want your tensor your vector to be", "tokens": [51320, 400, 309, 611, 2516, 257, 1230, 295, 5359, 11, 1105, 11, 597, 307, 577, 2416, 291, 528, 428, 40863, 428, 8062, 281, 312, 51676], "temperature": 0.0, "avg_logprob": -0.21011189619700113, "compression_ratio": 1.55, "no_speech_prob": 0.0017006383277475834}, {"id": 1038, "seek": 428628, "start": 4286.5199999999995, "end": 4293.32, "text": " So here let's import torch dot and in that functional sf. This is a common way of importing it", "tokens": [50376, 407, 510, 718, 311, 974, 27822, 5893, 293, 294, 300, 11745, 47095, 13, 639, 307, 257, 2689, 636, 295, 43866, 309, 50716], "temperature": 0.0, "avg_logprob": -0.12845164078932542, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.004538137000054121}, {"id": 1039, "seek": 428628, "start": 4294.12, "end": 4296.12, "text": " And then let's do f dot one hot", "tokens": [50756, 400, 550, 718, 311, 360, 283, 5893, 472, 2368, 50856], "temperature": 0.0, "avg_logprob": -0.12845164078932542, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.004538137000054121}, {"id": 1040, "seek": 428628, "start": 4296.679999999999, "end": 4299.24, "text": " And we feed in the integers that we want to encode", "tokens": [50884, 400, 321, 3154, 294, 264, 41674, 300, 321, 528, 281, 2058, 1429, 51012], "temperature": 0.0, "avg_logprob": -0.12845164078932542, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.004538137000054121}, {"id": 1041, "seek": 428628, "start": 4300.04, "end": 4302.92, "text": " So we can actually feed in the entire array of x's", "tokens": [51052, 407, 321, 393, 767, 3154, 294, 264, 2302, 10225, 295, 2031, 311, 51196], "temperature": 0.0, "avg_logprob": -0.12845164078932542, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.004538137000054121}, {"id": 1042, "seek": 428628, "start": 4304.04, "end": 4306.92, "text": " And we can tell it that num classes is 27", "tokens": [51252, 400, 321, 393, 980, 309, 300, 1031, 5359, 307, 7634, 51396], "temperature": 0.0, "avg_logprob": -0.12845164078932542, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.004538137000054121}, {"id": 1043, "seek": 428628, "start": 4307.719999999999, "end": 4313.08, "text": " So it doesn't have to try to guess it it may have guessed that it's only 13 and would give us an incorrect result", "tokens": [51436, 407, 309, 1177, 380, 362, 281, 853, 281, 2041, 309, 309, 815, 362, 21852, 300, 309, 311, 787, 3705, 293, 576, 976, 505, 364, 18424, 1874, 51704], "temperature": 0.0, "avg_logprob": -0.12845164078932542, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.004538137000054121}, {"id": 1044, "seek": 431308, "start": 4313.48, "end": 4319.48, "text": " So this is the one hot. Let's call this x ink for x encoded", "tokens": [50384, 407, 341, 307, 264, 472, 2368, 13, 961, 311, 818, 341, 2031, 11276, 337, 2031, 2058, 12340, 50684], "temperature": 0.0, "avg_logprob": -0.15822223175403682, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.0009399167029187083}, {"id": 1045, "seek": 431308, "start": 4322.04, "end": 4325.72, "text": " And then we see that x encoded that shape is 5 by 27", "tokens": [50812, 400, 550, 321, 536, 300, 2031, 2058, 12340, 300, 3909, 307, 1025, 538, 7634, 50996], "temperature": 0.0, "avg_logprob": -0.15822223175403682, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.0009399167029187083}, {"id": 1046, "seek": 431308, "start": 4327.08, "end": 4328.44, "text": " and uh", "tokens": [51064, 293, 2232, 51132], "temperature": 0.0, "avg_logprob": -0.15822223175403682, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.0009399167029187083}, {"id": 1047, "seek": 431308, "start": 4328.44, "end": 4331.24, "text": " We can also visualize it plt.im show of x ink", "tokens": [51132, 492, 393, 611, 23273, 309, 499, 83, 13, 332, 855, 295, 2031, 11276, 51272], "temperature": 0.0, "avg_logprob": -0.15822223175403682, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.0009399167029187083}, {"id": 1048, "seek": 431308, "start": 4332.36, "end": 4334.68, "text": " To make it a little bit more clear because this is a little messy", "tokens": [51328, 1407, 652, 309, 257, 707, 857, 544, 1850, 570, 341, 307, 257, 707, 16191, 51444], "temperature": 0.0, "avg_logprob": -0.15822223175403682, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.0009399167029187083}, {"id": 1049, "seek": 431308, "start": 4335.48, "end": 4338.2, "text": " So we see that we've encoded all the five examples", "tokens": [51484, 407, 321, 536, 300, 321, 600, 2058, 12340, 439, 264, 1732, 5110, 51620], "temperature": 0.0, "avg_logprob": -0.15822223175403682, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.0009399167029187083}, {"id": 1050, "seek": 433820, "start": 4338.92, "end": 4341.32, "text": " Into vectors we have five examples", "tokens": [50400, 23373, 18875, 321, 362, 1732, 5110, 50520], "temperature": 0.0, "avg_logprob": -0.12652288825766553, "compression_ratio": 1.8803827751196172, "no_speech_prob": 0.02128126099705696}, {"id": 1051, "seek": 433820, "start": 4341.4, "end": 4345.5599999999995, "text": " So we have five rows and each row here is now an example into a neural net", "tokens": [50524, 407, 321, 362, 1732, 13241, 293, 1184, 5386, 510, 307, 586, 364, 1365, 666, 257, 18161, 2533, 50732], "temperature": 0.0, "avg_logprob": -0.12652288825766553, "compression_ratio": 1.8803827751196172, "no_speech_prob": 0.02128126099705696}, {"id": 1052, "seek": 433820, "start": 4346.28, "end": 4351.0, "text": " And we see that the appropriate bit is turned on as a one and everything else is zero", "tokens": [50768, 400, 321, 536, 300, 264, 6854, 857, 307, 3574, 322, 382, 257, 472, 293, 1203, 1646, 307, 4018, 51004], "temperature": 0.0, "avg_logprob": -0.12652288825766553, "compression_ratio": 1.8803827751196172, "no_speech_prob": 0.02128126099705696}, {"id": 1053, "seek": 433820, "start": 4351.96, "end": 4353.5599999999995, "text": " so um", "tokens": [51052, 370, 1105, 51132], "temperature": 0.0, "avg_logprob": -0.12652288825766553, "compression_ratio": 1.8803827751196172, "no_speech_prob": 0.02128126099705696}, {"id": 1054, "seek": 433820, "start": 4353.5599999999995, "end": 4357.8, "text": " Here for example, the zero bit is turned on the fifth bit is turned on", "tokens": [51132, 1692, 337, 1365, 11, 264, 4018, 857, 307, 3574, 322, 264, 9266, 857, 307, 3574, 322, 51344], "temperature": 0.0, "avg_logprob": -0.12652288825766553, "compression_ratio": 1.8803827751196172, "no_speech_prob": 0.02128126099705696}, {"id": 1055, "seek": 433820, "start": 4358.36, "end": 4363.639999999999, "text": " Thirteenth bits are turned on for both of these examples and the first bit here is turned on", "tokens": [51372, 334, 347, 46897, 9239, 366, 3574, 322, 337, 1293, 295, 613, 5110, 293, 264, 700, 857, 510, 307, 3574, 322, 51636], "temperature": 0.0, "avg_logprob": -0.12652288825766553, "compression_ratio": 1.8803827751196172, "no_speech_prob": 0.02128126099705696}, {"id": 1056, "seek": 433820, "start": 4364.679999999999, "end": 4366.679999999999, "text": " So that's how we can encode", "tokens": [51688, 407, 300, 311, 577, 321, 393, 2058, 1429, 51788], "temperature": 0.0, "avg_logprob": -0.12652288825766553, "compression_ratio": 1.8803827751196172, "no_speech_prob": 0.02128126099705696}, {"id": 1057, "seek": 436668, "start": 4367.16, "end": 4369.16, "text": " integers into vectors", "tokens": [50388, 41674, 666, 18875, 50488], "temperature": 0.0, "avg_logprob": -0.1154447112764631, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.006096833385527134}, {"id": 1058, "seek": 436668, "start": 4369.320000000001, "end": 4374.200000000001, "text": " And then these vectors can feed in to neural nets one more issue to be careful with here by the way is", "tokens": [50496, 400, 550, 613, 18875, 393, 3154, 294, 281, 18161, 36170, 472, 544, 2734, 281, 312, 5026, 365, 510, 538, 264, 636, 307, 50740], "temperature": 0.0, "avg_logprob": -0.1154447112764631, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.006096833385527134}, {"id": 1059, "seek": 436668, "start": 4375.16, "end": 4378.68, "text": " Let's look at the data type of encoding. We always want to be careful with data types", "tokens": [50788, 961, 311, 574, 412, 264, 1412, 2010, 295, 43430, 13, 492, 1009, 528, 281, 312, 5026, 365, 1412, 3467, 50964], "temperature": 0.0, "avg_logprob": -0.1154447112764631, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.006096833385527134}, {"id": 1060, "seek": 436668, "start": 4379.400000000001, "end": 4384.92, "text": " What would you expect x encodings data type to be when we're plugging numbers into neural nets?", "tokens": [51000, 708, 576, 291, 2066, 2031, 2058, 378, 1109, 1412, 2010, 281, 312, 562, 321, 434, 42975, 3547, 666, 18161, 36170, 30, 51276], "temperature": 0.0, "avg_logprob": -0.1154447112764631, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.006096833385527134}, {"id": 1061, "seek": 436668, "start": 4384.92, "end": 4389.96, "text": " We don't want them to be integers. We want them to be floating point numbers that can take on various values", "tokens": [51276, 492, 500, 380, 528, 552, 281, 312, 41674, 13, 492, 528, 552, 281, 312, 12607, 935, 3547, 300, 393, 747, 322, 3683, 4190, 51528], "temperature": 0.0, "avg_logprob": -0.1154447112764631, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.006096833385527134}, {"id": 1062, "seek": 436668, "start": 4390.4400000000005, "end": 4393.56, "text": " But the d type here is actually 64 bit integer", "tokens": [51552, 583, 264, 274, 2010, 510, 307, 767, 12145, 857, 24922, 51708], "temperature": 0.0, "avg_logprob": -0.1154447112764631, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.006096833385527134}, {"id": 1063, "seek": 439356, "start": 4394.280000000001, "end": 4399.160000000001, "text": " And the reason for that I suspect is that one hot received a 64 bit integer here", "tokens": [50400, 400, 264, 1778, 337, 300, 286, 9091, 307, 300, 472, 2368, 4613, 257, 12145, 857, 24922, 510, 50644], "temperature": 0.0, "avg_logprob": -0.09162583593594825, "compression_ratio": 1.706959706959707, "no_speech_prob": 0.0038842579815536737}, {"id": 1064, "seek": 439356, "start": 4399.64, "end": 4401.64, "text": " And it returned the same data type", "tokens": [50668, 400, 309, 8752, 264, 912, 1412, 2010, 50768], "temperature": 0.0, "avg_logprob": -0.09162583593594825, "compression_ratio": 1.706959706959707, "no_speech_prob": 0.0038842579815536737}, {"id": 1065, "seek": 439356, "start": 4401.88, "end": 4407.88, "text": " And when you look at the signature of one hot it doesn't even take a d type a desired data type of the output tensor", "tokens": [50780, 400, 562, 291, 574, 412, 264, 13397, 295, 472, 2368, 309, 1177, 380, 754, 747, 257, 274, 2010, 257, 14721, 1412, 2010, 295, 264, 5598, 40863, 51080], "temperature": 0.0, "avg_logprob": -0.09162583593594825, "compression_ratio": 1.706959706959707, "no_speech_prob": 0.0038842579815536737}, {"id": 1066, "seek": 439356, "start": 4408.4400000000005, "end": 4413.96, "text": " And so we can't in a lot of functions in torch we'd be able to do something like d type equals torch dot float 32", "tokens": [51108, 400, 370, 321, 393, 380, 294, 257, 688, 295, 6828, 294, 27822, 321, 1116, 312, 1075, 281, 360, 746, 411, 274, 2010, 6915, 27822, 5893, 15706, 8858, 51384], "temperature": 0.0, "avg_logprob": -0.09162583593594825, "compression_ratio": 1.706959706959707, "no_speech_prob": 0.0038842579815536737}, {"id": 1067, "seek": 439356, "start": 4414.4400000000005, "end": 4417.320000000001, "text": " Which is what we want, but one hot does not support that", "tokens": [51408, 3013, 307, 437, 321, 528, 11, 457, 472, 2368, 775, 406, 1406, 300, 51552], "temperature": 0.0, "avg_logprob": -0.09162583593594825, "compression_ratio": 1.706959706959707, "no_speech_prob": 0.0038842579815536737}, {"id": 1068, "seek": 439356, "start": 4417.96, "end": 4421.400000000001, "text": " So instead we're going to want to cast this to float like this", "tokens": [51584, 407, 2602, 321, 434, 516, 281, 528, 281, 4193, 341, 281, 15706, 411, 341, 51756], "temperature": 0.0, "avg_logprob": -0.09162583593594825, "compression_ratio": 1.706959706959707, "no_speech_prob": 0.0038842579815536737}, {"id": 1069, "seek": 442140, "start": 4422.12, "end": 4424.12, "text": " So that these", "tokens": [50400, 407, 300, 613, 50500], "temperature": 0.0, "avg_logprob": -0.1680018513701683, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.0014778680633753538}, {"id": 1070, "seek": 442140, "start": 4424.839999999999, "end": 4426.44, "text": " Everything is the same", "tokens": [50536, 5471, 307, 264, 912, 50616], "temperature": 0.0, "avg_logprob": -0.1680018513701683, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.0014778680633753538}, {"id": 1071, "seek": 442140, "start": 4426.44, "end": 4431.32, "text": " Everything looks the same but the d type is float 32 and floats can feed into", "tokens": [50616, 5471, 1542, 264, 912, 457, 264, 274, 2010, 307, 15706, 8858, 293, 37878, 393, 3154, 666, 50860], "temperature": 0.0, "avg_logprob": -0.1680018513701683, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.0014778680633753538}, {"id": 1072, "seek": 442140, "start": 4432.28, "end": 4435.0, "text": " Neural nets. So now let's construct our first neuron", "tokens": [50908, 1734, 1807, 36170, 13, 407, 586, 718, 311, 7690, 527, 700, 34090, 51044], "temperature": 0.0, "avg_logprob": -0.1680018513701683, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.0014778680633753538}, {"id": 1073, "seek": 442140, "start": 4436.04, "end": 4439.0, "text": " This neuron will look at these input vectors", "tokens": [51096, 639, 34090, 486, 574, 412, 613, 4846, 18875, 51244], "temperature": 0.0, "avg_logprob": -0.1680018513701683, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.0014778680633753538}, {"id": 1074, "seek": 442140, "start": 4440.04, "end": 4447.719999999999, "text": " And as you remember from micrograd these neurons basically perform a very simple function wx plus b where wx is a dot product", "tokens": [51296, 400, 382, 291, 1604, 490, 4532, 7165, 613, 22027, 1936, 2042, 257, 588, 2199, 2445, 261, 87, 1804, 272, 689, 261, 87, 307, 257, 5893, 1674, 51680], "temperature": 0.0, "avg_logprob": -0.1680018513701683, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.0014778680633753538}, {"id": 1075, "seek": 442140, "start": 4448.28, "end": 4449.5599999999995, "text": " right", "tokens": [51708, 558, 51772], "temperature": 0.0, "avg_logprob": -0.1680018513701683, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.0014778680633753538}, {"id": 1076, "seek": 444956, "start": 4449.64, "end": 4454.6, "text": " So we can achieve the same thing here. Let's first define the weights of this neuron", "tokens": [50368, 407, 321, 393, 4584, 264, 912, 551, 510, 13, 961, 311, 700, 6964, 264, 17443, 295, 341, 34090, 50616], "temperature": 0.0, "avg_logprob": -0.10850023755840227, "compression_ratio": 1.8638297872340426, "no_speech_prob": 0.0004511779989115894}, {"id": 1077, "seek": 444956, "start": 4454.6, "end": 4458.04, "text": " Basically, what are the initial weights at initialization for this neuron?", "tokens": [50616, 8537, 11, 437, 366, 264, 5883, 17443, 412, 5883, 2144, 337, 341, 34090, 30, 50788], "temperature": 0.0, "avg_logprob": -0.10850023755840227, "compression_ratio": 1.8638297872340426, "no_speech_prob": 0.0004511779989115894}, {"id": 1078, "seek": 444956, "start": 4458.76, "end": 4460.76, "text": " Let's initialize them with torch dot random", "tokens": [50824, 961, 311, 5883, 1125, 552, 365, 27822, 5893, 4974, 50924], "temperature": 0.0, "avg_logprob": -0.10850023755840227, "compression_ratio": 1.8638297872340426, "no_speech_prob": 0.0004511779989115894}, {"id": 1079, "seek": 444956, "start": 4461.64, "end": 4463.320000000001, "text": " torch dot random", "tokens": [50968, 27822, 5893, 4974, 51052], "temperature": 0.0, "avg_logprob": -0.10850023755840227, "compression_ratio": 1.8638297872340426, "no_speech_prob": 0.0004511779989115894}, {"id": 1080, "seek": 444956, "start": 4463.320000000001, "end": 4464.6, "text": " is", "tokens": [51052, 307, 51116], "temperature": 0.0, "avg_logprob": -0.10850023755840227, "compression_ratio": 1.8638297872340426, "no_speech_prob": 0.0004511779989115894}, {"id": 1081, "seek": 444956, "start": 4464.6, "end": 4466.6, "text": " Fills a tensor with random numbers", "tokens": [51116, 479, 2565, 257, 40863, 365, 4974, 3547, 51216], "temperature": 0.0, "avg_logprob": -0.10850023755840227, "compression_ratio": 1.8638297872340426, "no_speech_prob": 0.0004511779989115894}, {"id": 1082, "seek": 444956, "start": 4467.160000000001, "end": 4469.160000000001, "text": " drawn from a normal distribution", "tokens": [51244, 10117, 490, 257, 2710, 7316, 51344], "temperature": 0.0, "avg_logprob": -0.10850023755840227, "compression_ratio": 1.8638297872340426, "no_speech_prob": 0.0004511779989115894}, {"id": 1083, "seek": 444956, "start": 4469.240000000001, "end": 4471.240000000001, "text": " And a normal distribution has", "tokens": [51348, 400, 257, 2710, 7316, 575, 51448], "temperature": 0.0, "avg_logprob": -0.10850023755840227, "compression_ratio": 1.8638297872340426, "no_speech_prob": 0.0004511779989115894}, {"id": 1084, "seek": 444956, "start": 4471.96, "end": 4478.280000000001, "text": " A probability density function like this and so most of the numbers drawn from this distribution will be around zero", "tokens": [51484, 316, 8482, 10305, 2445, 411, 341, 293, 370, 881, 295, 264, 3547, 10117, 490, 341, 7316, 486, 312, 926, 4018, 51800], "temperature": 0.0, "avg_logprob": -0.10850023755840227, "compression_ratio": 1.8638297872340426, "no_speech_prob": 0.0004511779989115894}, {"id": 1085, "seek": 447828, "start": 4479.08, "end": 4485.24, "text": " But some of them will be as high as almost three and so on and very few numbers will be above three in magnitude", "tokens": [50404, 583, 512, 295, 552, 486, 312, 382, 1090, 382, 1920, 1045, 293, 370, 322, 293, 588, 1326, 3547, 486, 312, 3673, 1045, 294, 15668, 50712], "temperature": 0.0, "avg_logprob": -0.12193592919243706, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0011877763317897916}, {"id": 1086, "seek": 447828, "start": 4486.36, "end": 4489.4, "text": " So we need to take a size as an input here", "tokens": [50768, 407, 321, 643, 281, 747, 257, 2744, 382, 364, 4846, 510, 50920], "temperature": 0.0, "avg_logprob": -0.12193592919243706, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0011877763317897916}, {"id": 1087, "seek": 447828, "start": 4490.44, "end": 4493.16, "text": " And i'm going to use size as to be 27 by one", "tokens": [50972, 400, 741, 478, 516, 281, 764, 2744, 382, 281, 312, 7634, 538, 472, 51108], "temperature": 0.0, "avg_logprob": -0.12193592919243706, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0011877763317897916}, {"id": 1088, "seek": 447828, "start": 4494.679999999999, "end": 4501.16, "text": " So 27 by one and then let's visualize w. So w is a column vector of 27 numbers", "tokens": [51184, 407, 7634, 538, 472, 293, 550, 718, 311, 23273, 261, 13, 407, 261, 307, 257, 7738, 8062, 295, 7634, 3547, 51508], "temperature": 0.0, "avg_logprob": -0.12193592919243706, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0011877763317897916}, {"id": 1089, "seek": 447828, "start": 4503.08, "end": 4507.4, "text": " And uh, these weights are then multiplied by the inputs", "tokens": [51604, 400, 2232, 11, 613, 17443, 366, 550, 17207, 538, 264, 15743, 51820], "temperature": 0.0, "avg_logprob": -0.12193592919243706, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.0011877763317897916}, {"id": 1090, "seek": 450828, "start": 4508.679999999999, "end": 4514.12, "text": " So now to perform this multiplication, we can take x encoding and we can multiply it with w", "tokens": [50384, 407, 586, 281, 2042, 341, 27290, 11, 321, 393, 747, 2031, 43430, 293, 321, 393, 12972, 309, 365, 261, 50656], "temperature": 0.0, "avg_logprob": -0.11377726162181181, "compression_ratio": 1.8085106382978724, "no_speech_prob": 0.0007321261218748987}, {"id": 1091, "seek": 450828, "start": 4515.0, "end": 4518.2, "text": " This is a matrix multiplication operator in pytorch", "tokens": [50700, 639, 307, 257, 8141, 27290, 12973, 294, 25878, 284, 339, 50860], "temperature": 0.0, "avg_logprob": -0.11377726162181181, "compression_ratio": 1.8085106382978724, "no_speech_prob": 0.0007321261218748987}, {"id": 1092, "seek": 450828, "start": 4519.96, "end": 4522.92, "text": " And the output of this operation is five by one", "tokens": [50948, 400, 264, 5598, 295, 341, 6916, 307, 1732, 538, 472, 51096], "temperature": 0.0, "avg_logprob": -0.11377726162181181, "compression_ratio": 1.8085106382978724, "no_speech_prob": 0.0007321261218748987}, {"id": 1093, "seek": 450828, "start": 4523.639999999999, "end": 4525.639999999999, "text": " The reason it's five by five is the following", "tokens": [51132, 440, 1778, 309, 311, 1732, 538, 1732, 307, 264, 3480, 51232], "temperature": 0.0, "avg_logprob": -0.11377726162181181, "compression_ratio": 1.8085106382978724, "no_speech_prob": 0.0007321261218748987}, {"id": 1094, "seek": 450828, "start": 4525.88, "end": 4531.719999999999, "text": " We took x encoding, which is five by 27 and we multiplied it by 27 by one", "tokens": [51244, 492, 1890, 2031, 43430, 11, 597, 307, 1732, 538, 7634, 293, 321, 17207, 309, 538, 7634, 538, 472, 51536], "temperature": 0.0, "avg_logprob": -0.11377726162181181, "compression_ratio": 1.8085106382978724, "no_speech_prob": 0.0007321261218748987}, {"id": 1095, "seek": 450828, "start": 4533.5599999999995, "end": 4535.719999999999, "text": " And in matrix multiplication", "tokens": [51628, 400, 294, 8141, 27290, 51736], "temperature": 0.0, "avg_logprob": -0.11377726162181181, "compression_ratio": 1.8085106382978724, "no_speech_prob": 0.0007321261218748987}, {"id": 1096, "seek": 453572, "start": 4536.52, "end": 4543.320000000001, "text": " You see that the output will become five by one because these 27 will multiply and add", "tokens": [50404, 509, 536, 300, 264, 5598, 486, 1813, 1732, 538, 472, 570, 613, 7634, 486, 12972, 293, 909, 50744], "temperature": 0.0, "avg_logprob": -0.14282600084940592, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0027147983200848103}, {"id": 1097, "seek": 453572, "start": 4544.84, "end": 4547.96, "text": " So basically what we're seeing here out of this operation", "tokens": [50820, 407, 1936, 437, 321, 434, 2577, 510, 484, 295, 341, 6916, 50976], "temperature": 0.0, "avg_logprob": -0.14282600084940592, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0027147983200848103}, {"id": 1098, "seek": 453572, "start": 4548.76, "end": 4550.76, "text": " Is we are seeing the five", "tokens": [51016, 1119, 321, 366, 2577, 264, 1732, 51116], "temperature": 0.0, "avg_logprob": -0.14282600084940592, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0027147983200848103}, {"id": 1099, "seek": 453572, "start": 4550.76, "end": 4552.04, "text": " um", "tokens": [51116, 1105, 51180], "temperature": 0.0, "avg_logprob": -0.14282600084940592, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0027147983200848103}, {"id": 1100, "seek": 453572, "start": 4552.04, "end": 4553.64, "text": " activations", "tokens": [51180, 2430, 763, 51260], "temperature": 0.0, "avg_logprob": -0.14282600084940592, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0027147983200848103}, {"id": 1101, "seek": 453572, "start": 4553.64, "end": 4555.64, "text": " of this neuron", "tokens": [51260, 295, 341, 34090, 51360], "temperature": 0.0, "avg_logprob": -0.14282600084940592, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0027147983200848103}, {"id": 1102, "seek": 453572, "start": 4556.360000000001, "end": 4560.2, "text": " On these five inputs and we've evaluated all of them in parallel", "tokens": [51396, 1282, 613, 1732, 15743, 293, 321, 600, 25509, 439, 295, 552, 294, 8952, 51588], "temperature": 0.0, "avg_logprob": -0.14282600084940592, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0027147983200848103}, {"id": 1103, "seek": 453572, "start": 4560.4400000000005, "end": 4563.08, "text": " We didn't feed in just a single input to the single neuron", "tokens": [51600, 492, 994, 380, 3154, 294, 445, 257, 2167, 4846, 281, 264, 2167, 34090, 51732], "temperature": 0.0, "avg_logprob": -0.14282600084940592, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0027147983200848103}, {"id": 1104, "seek": 456308, "start": 4563.4, "end": 4567.4, "text": " We fed in simultaneously all the five inputs into the same neuron", "tokens": [50380, 492, 4636, 294, 16561, 439, 264, 1732, 15743, 666, 264, 912, 34090, 50580], "temperature": 0.0, "avg_logprob": -0.1601554566780023, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.004133312031626701}, {"id": 1105, "seek": 456308, "start": 4568.12, "end": 4570.5199999999995, "text": " And in parallel pytorch has evaluated", "tokens": [50616, 400, 294, 8952, 25878, 284, 339, 575, 25509, 50736], "temperature": 0.0, "avg_logprob": -0.1601554566780023, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.004133312031626701}, {"id": 1106, "seek": 456308, "start": 4571.16, "end": 4575.16, "text": " The wx plus b but here is just wx. There's no bias", "tokens": [50768, 440, 261, 87, 1804, 272, 457, 510, 307, 445, 261, 87, 13, 821, 311, 572, 12577, 50968], "temperature": 0.0, "avg_logprob": -0.1601554566780023, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.004133312031626701}, {"id": 1107, "seek": 456308, "start": 4575.88, "end": 4578.76, "text": " It has valued w w times x for all of them", "tokens": [51004, 467, 575, 22608, 261, 261, 1413, 2031, 337, 439, 295, 552, 51148], "temperature": 0.0, "avg_logprob": -0.1601554566780023, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.004133312031626701}, {"id": 1108, "seek": 456308, "start": 4579.4, "end": 4585.24, "text": " Uh independently now instead of a single neuron though, I would like to have 27 neurons and I'll show you in the second", "tokens": [51180, 4019, 21761, 586, 2602, 295, 257, 2167, 34090, 1673, 11, 286, 576, 411, 281, 362, 7634, 22027, 293, 286, 603, 855, 291, 294, 264, 1150, 51472], "temperature": 0.0, "avg_logprob": -0.1601554566780023, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.004133312031626701}, {"id": 1109, "seek": 456308, "start": 4585.24, "end": 4587.24, "text": " Why I'm on 27 neurons?", "tokens": [51472, 1545, 286, 478, 322, 7634, 22027, 30, 51572], "temperature": 0.0, "avg_logprob": -0.1601554566780023, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.004133312031626701}, {"id": 1110, "seek": 456308, "start": 4587.5599999999995, "end": 4591.88, "text": " So instead of having just a one here, which is indicating this presence of one single neuron", "tokens": [51588, 407, 2602, 295, 1419, 445, 257, 472, 510, 11, 597, 307, 25604, 341, 6814, 295, 472, 2167, 34090, 51804], "temperature": 0.0, "avg_logprob": -0.1601554566780023, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.004133312031626701}, {"id": 1111, "seek": 459188, "start": 4592.52, "end": 4594.52, "text": " We can use 27", "tokens": [50396, 492, 393, 764, 7634, 50496], "temperature": 0.0, "avg_logprob": -0.08560050251972244, "compression_ratio": 1.6408839779005524, "no_speech_prob": 0.0017273230478167534}, {"id": 1112, "seek": 459188, "start": 4594.68, "end": 4596.92, "text": " And then when w is 27 by 27", "tokens": [50504, 400, 550, 562, 261, 307, 7634, 538, 7634, 50616], "temperature": 0.0, "avg_logprob": -0.08560050251972244, "compression_ratio": 1.6408839779005524, "no_speech_prob": 0.0017273230478167534}, {"id": 1113, "seek": 459188, "start": 4598.36, "end": 4604.76, "text": " This will in parallel evaluate all the 27 neurons on all the five inputs", "tokens": [50688, 639, 486, 294, 8952, 13059, 439, 264, 7634, 22027, 322, 439, 264, 1732, 15743, 51008], "temperature": 0.0, "avg_logprob": -0.08560050251972244, "compression_ratio": 1.6408839779005524, "no_speech_prob": 0.0017273230478167534}, {"id": 1114, "seek": 459188, "start": 4606.52, "end": 4609.0, "text": " Giving us a much better much much bigger result", "tokens": [51096, 28983, 505, 257, 709, 1101, 709, 709, 3801, 1874, 51220], "temperature": 0.0, "avg_logprob": -0.08560050251972244, "compression_ratio": 1.6408839779005524, "no_speech_prob": 0.0017273230478167534}, {"id": 1115, "seek": 459188, "start": 4609.4800000000005, "end": 4613.32, "text": " So now what we've done is five by 27 multiplied 27 by 27", "tokens": [51244, 407, 586, 437, 321, 600, 1096, 307, 1732, 538, 7634, 17207, 7634, 538, 7634, 51436], "temperature": 0.0, "avg_logprob": -0.08560050251972244, "compression_ratio": 1.6408839779005524, "no_speech_prob": 0.0017273230478167534}, {"id": 1116, "seek": 459188, "start": 4614.12, "end": 4616.62, "text": " And the output of this is now five by 27", "tokens": [51476, 400, 264, 5598, 295, 341, 307, 586, 1732, 538, 7634, 51601], "temperature": 0.0, "avg_logprob": -0.08560050251972244, "compression_ratio": 1.6408839779005524, "no_speech_prob": 0.0017273230478167534}, {"id": 1117, "seek": 459188, "start": 4617.8, "end": 4619.8, "text": " So we can see that the shape of this", "tokens": [51660, 407, 321, 393, 536, 300, 264, 3909, 295, 341, 51760], "temperature": 0.0, "avg_logprob": -0.08560050251972244, "compression_ratio": 1.6408839779005524, "no_speech_prob": 0.0017273230478167534}, {"id": 1118, "seek": 462188, "start": 4621.88, "end": 4623.88, "text": " Is five by 27", "tokens": [50364, 1119, 1732, 538, 7634, 50464], "temperature": 0.0, "avg_logprob": -0.11694132563579514, "compression_ratio": 1.7670454545454546, "no_speech_prob": 0.0021824249997735023}, {"id": 1119, "seek": 462188, "start": 4623.88, "end": 4626.52, "text": " So what is every element here telling us right?", "tokens": [50464, 407, 437, 307, 633, 4478, 510, 3585, 505, 558, 30, 50596], "temperature": 0.0, "avg_logprob": -0.11694132563579514, "compression_ratio": 1.7670454545454546, "no_speech_prob": 0.0021824249997735023}, {"id": 1120, "seek": 462188, "start": 4627.16, "end": 4630.76, "text": " It's telling us for every one of 27 neurons that we created", "tokens": [50628, 467, 311, 3585, 505, 337, 633, 472, 295, 7634, 22027, 300, 321, 2942, 50808], "temperature": 0.0, "avg_logprob": -0.11694132563579514, "compression_ratio": 1.7670454545454546, "no_speech_prob": 0.0021824249997735023}, {"id": 1121, "seek": 462188, "start": 4633.24, "end": 4638.68, "text": " What is the firing rate of those neurons on every one of those five examples", "tokens": [50932, 708, 307, 264, 16045, 3314, 295, 729, 22027, 322, 633, 472, 295, 729, 1732, 5110, 51204], "temperature": 0.0, "avg_logprob": -0.11694132563579514, "compression_ratio": 1.7670454545454546, "no_speech_prob": 0.0021824249997735023}, {"id": 1122, "seek": 462188, "start": 4639.64, "end": 4640.84, "text": " so", "tokens": [51252, 370, 51312], "temperature": 0.0, "avg_logprob": -0.11694132563579514, "compression_ratio": 1.7670454545454546, "no_speech_prob": 0.0021824249997735023}, {"id": 1123, "seek": 462188, "start": 4640.84, "end": 4644.2, "text": " The element for example 3 comma 13", "tokens": [51312, 440, 4478, 337, 1365, 805, 22117, 3705, 51480], "temperature": 0.0, "avg_logprob": -0.11694132563579514, "compression_ratio": 1.7670454545454546, "no_speech_prob": 0.0021824249997735023}, {"id": 1124, "seek": 462188, "start": 4645.400000000001, "end": 4648.68, "text": " Is giving us the firing rate of the 13th neuron", "tokens": [51540, 1119, 2902, 505, 264, 16045, 3314, 295, 264, 3705, 392, 34090, 51704], "temperature": 0.0, "avg_logprob": -0.11694132563579514, "compression_ratio": 1.7670454545454546, "no_speech_prob": 0.0021824249997735023}, {"id": 1125, "seek": 462188, "start": 4649.32, "end": 4651.32, "text": " looking at the third input", "tokens": [51736, 1237, 412, 264, 2636, 4846, 51836], "temperature": 0.0, "avg_logprob": -0.11694132563579514, "compression_ratio": 1.7670454545454546, "no_speech_prob": 0.0021824249997735023}, {"id": 1126, "seek": 465188, "start": 4651.88, "end": 4655.24, "text": " And the way this was achieved is by a dot product", "tokens": [50364, 400, 264, 636, 341, 390, 11042, 307, 538, 257, 5893, 1674, 50532], "temperature": 0.0, "avg_logprob": -0.10810474070107065, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.00034596904879435897}, {"id": 1127, "seek": 465188, "start": 4656.28, "end": 4658.28, "text": " between the third input", "tokens": [50584, 1296, 264, 2636, 4846, 50684], "temperature": 0.0, "avg_logprob": -0.10810474070107065, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.00034596904879435897}, {"id": 1128, "seek": 465188, "start": 4658.92, "end": 4660.92, "text": " and the 13th column", "tokens": [50716, 293, 264, 3705, 392, 7738, 50816], "temperature": 0.0, "avg_logprob": -0.10810474070107065, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.00034596904879435897}, {"id": 1129, "seek": 465188, "start": 4661.56, "end": 4663.56, "text": " of this w matrix here", "tokens": [50848, 295, 341, 261, 8141, 510, 50948], "temperature": 0.0, "avg_logprob": -0.10810474070107065, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.00034596904879435897}, {"id": 1130, "seek": 465188, "start": 4664.84, "end": 4669.56, "text": " Okay, so using matrix multiplication. We can very efficiently evaluate", "tokens": [51012, 1033, 11, 370, 1228, 8141, 27290, 13, 492, 393, 588, 19621, 13059, 51248], "temperature": 0.0, "avg_logprob": -0.10810474070107065, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.00034596904879435897}, {"id": 1131, "seek": 465188, "start": 4670.84, "end": 4674.04, "text": " The dot product between lots of input examples in a batch", "tokens": [51312, 440, 5893, 1674, 1296, 3195, 295, 4846, 5110, 294, 257, 15245, 51472], "temperature": 0.0, "avg_logprob": -0.10810474070107065, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.00034596904879435897}, {"id": 1132, "seek": 465188, "start": 4675.0, "end": 4680.36, "text": " And lots of neurons where all of those neurons have weights in the columns of those w's", "tokens": [51520, 400, 3195, 295, 22027, 689, 439, 295, 729, 22027, 362, 17443, 294, 264, 13766, 295, 729, 261, 311, 51788], "temperature": 0.0, "avg_logprob": -0.10810474070107065, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.00034596904879435897}, {"id": 1133, "seek": 468036, "start": 4681.08, "end": 4683.719999999999, "text": " And in matrix multiplication, we're just doing those dot products and", "tokens": [50400, 400, 294, 8141, 27290, 11, 321, 434, 445, 884, 729, 5893, 3383, 293, 50532], "temperature": 0.0, "avg_logprob": -0.22092365181964377, "compression_ratio": 1.5888324873096447, "no_speech_prob": 0.0014324552612379193}, {"id": 1134, "seek": 468036, "start": 4684.5199999999995, "end": 4689.24, "text": " In parallel just to show you that this is the case. We can take x-ank and we can take the third", "tokens": [50572, 682, 8952, 445, 281, 855, 291, 300, 341, 307, 264, 1389, 13, 492, 393, 747, 2031, 12, 657, 293, 321, 393, 747, 264, 2636, 50808], "temperature": 0.0, "avg_logprob": -0.22092365181964377, "compression_ratio": 1.5888324873096447, "no_speech_prob": 0.0014324552612379193}, {"id": 1135, "seek": 468036, "start": 4690.5199999999995, "end": 4692.2, "text": " row", "tokens": [50872, 5386, 50956], "temperature": 0.0, "avg_logprob": -0.22092365181964377, "compression_ratio": 1.5888324873096447, "no_speech_prob": 0.0014324552612379193}, {"id": 1136, "seek": 468036, "start": 4692.2, "end": 4695.08, "text": " And we can take the w and take its 13th column", "tokens": [50956, 400, 321, 393, 747, 264, 261, 293, 747, 1080, 3705, 392, 7738, 51100], "temperature": 0.0, "avg_logprob": -0.22092365181964377, "compression_ratio": 1.5888324873096447, "no_speech_prob": 0.0014324552612379193}, {"id": 1137, "seek": 468036, "start": 4697.4, "end": 4699.799999999999, "text": " And then we can do x-ank at 3", "tokens": [51216, 400, 550, 321, 393, 360, 2031, 12, 657, 412, 805, 51336], "temperature": 0.0, "avg_logprob": -0.22092365181964377, "compression_ratio": 1.5888324873096447, "no_speech_prob": 0.0014324552612379193}, {"id": 1138, "seek": 468036, "start": 4701.719999999999, "end": 4704.44, "text": " Element wise multiply with w at 13", "tokens": [51432, 20900, 10829, 12972, 365, 261, 412, 3705, 51568], "temperature": 0.0, "avg_logprob": -0.22092365181964377, "compression_ratio": 1.5888324873096447, "no_speech_prob": 0.0014324552612379193}, {"id": 1139, "seek": 468036, "start": 4706.759999999999, "end": 4708.92, "text": " And sum that up does w x plus b", "tokens": [51684, 400, 2408, 300, 493, 775, 261, 2031, 1804, 272, 51792], "temperature": 0.0, "avg_logprob": -0.22092365181964377, "compression_ratio": 1.5888324873096447, "no_speech_prob": 0.0014324552612379193}, {"id": 1140, "seek": 470892, "start": 4709.64, "end": 4713.32, "text": " Well, there's no plus b. It's just w x dot product. And that's", "tokens": [50400, 1042, 11, 456, 311, 572, 1804, 272, 13, 467, 311, 445, 261, 2031, 5893, 1674, 13, 400, 300, 311, 50584], "temperature": 0.0, "avg_logprob": -0.11887135998956089, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.002322836546227336}, {"id": 1141, "seek": 470892, "start": 4714.12, "end": 4715.32, "text": " this number", "tokens": [50624, 341, 1230, 50684], "temperature": 0.0, "avg_logprob": -0.11887135998956089, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.002322836546227336}, {"id": 1142, "seek": 470892, "start": 4715.32, "end": 4719.32, "text": " So you see that this is just being done efficiently by the matrix multiplication", "tokens": [50684, 407, 291, 536, 300, 341, 307, 445, 885, 1096, 19621, 538, 264, 8141, 27290, 50884], "temperature": 0.0, "avg_logprob": -0.11887135998956089, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.002322836546227336}, {"id": 1143, "seek": 470892, "start": 4719.64, "end": 4725.4, "text": " operation for all the input examples and for all the output neurons of this first layer", "tokens": [50900, 6916, 337, 439, 264, 4846, 5110, 293, 337, 439, 264, 5598, 22027, 295, 341, 700, 4583, 51188], "temperature": 0.0, "avg_logprob": -0.11887135998956089, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.002322836546227336}, {"id": 1144, "seek": 470892, "start": 4726.04, "end": 4732.92, "text": " Okay, so we fed our 27 dimensional inputs into a first layer of a neural net that has 27 neurons, right?", "tokens": [51220, 1033, 11, 370, 321, 4636, 527, 7634, 18795, 15743, 666, 257, 700, 4583, 295, 257, 18161, 2533, 300, 575, 7634, 22027, 11, 558, 30, 51564], "temperature": 0.0, "avg_logprob": -0.11887135998956089, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.002322836546227336}, {"id": 1145, "seek": 473292, "start": 4732.92, "end": 4739.4, "text": " So we have 27 inputs and now we have 27 neurons these neurons perform w times x", "tokens": [50364, 407, 321, 362, 7634, 15743, 293, 586, 321, 362, 7634, 22027, 613, 22027, 2042, 261, 1413, 2031, 50688], "temperature": 0.0, "avg_logprob": -0.11141039153276863, "compression_ratio": 1.783132530120482, "no_speech_prob": 0.013019534759223461}, {"id": 1146, "seek": 473292, "start": 4739.72, "end": 4743.0, "text": " They don't have a bias and they don't have a non-linearity like 10 h", "tokens": [50704, 814, 500, 380, 362, 257, 12577, 293, 436, 500, 380, 362, 257, 2107, 12, 1889, 17409, 411, 1266, 276, 50868], "temperature": 0.0, "avg_logprob": -0.11141039153276863, "compression_ratio": 1.783132530120482, "no_speech_prob": 0.013019534759223461}, {"id": 1147, "seek": 473292, "start": 4743.16, "end": 4745.88, "text": " We're going to leave them to be a linear layer", "tokens": [50876, 492, 434, 516, 281, 1856, 552, 281, 312, 257, 8213, 4583, 51012], "temperature": 0.0, "avg_logprob": -0.11141039153276863, "compression_ratio": 1.783132530120482, "no_speech_prob": 0.013019534759223461}, {"id": 1148, "seek": 473292, "start": 4746.6, "end": 4750.84, "text": " In addition to that we're not going to have any other layers. This is going to be it. It's just going to be", "tokens": [51048, 682, 4500, 281, 300, 321, 434, 406, 516, 281, 362, 604, 661, 7914, 13, 639, 307, 516, 281, 312, 309, 13, 467, 311, 445, 516, 281, 312, 51260], "temperature": 0.0, "avg_logprob": -0.11141039153276863, "compression_ratio": 1.783132530120482, "no_speech_prob": 0.013019534759223461}, {"id": 1149, "seek": 473292, "start": 4751.4800000000005, "end": 4755.24, "text": " The dumbest smallest simplest neural net, which is just a single linear layer", "tokens": [51292, 440, 10316, 377, 16998, 22811, 18161, 2533, 11, 597, 307, 445, 257, 2167, 8213, 4583, 51480], "temperature": 0.0, "avg_logprob": -0.11141039153276863, "compression_ratio": 1.783132530120482, "no_speech_prob": 0.013019534759223461}, {"id": 1150, "seek": 473292, "start": 4756.52, "end": 4760.28, "text": " And now I'd like to explain what I want those 27 outputs to be", "tokens": [51544, 400, 586, 286, 1116, 411, 281, 2903, 437, 286, 528, 729, 7634, 23930, 281, 312, 51732], "temperature": 0.0, "avg_logprob": -0.11141039153276863, "compression_ratio": 1.783132530120482, "no_speech_prob": 0.013019534759223461}, {"id": 1151, "seek": 476028, "start": 4761.24, "end": 4764.2, "text": " Intuitively what we're trying to produce here for every single input example", "tokens": [50412, 5681, 1983, 3413, 437, 321, 434, 1382, 281, 5258, 510, 337, 633, 2167, 4846, 1365, 50560], "temperature": 0.0, "avg_logprob": -0.08798121604598871, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.002287093782797456}, {"id": 1152, "seek": 476028, "start": 4764.5199999999995, "end": 4768.759999999999, "text": " Is we're trying to produce some kind of a probability distribution for the next character in a sequence", "tokens": [50576, 1119, 321, 434, 1382, 281, 5258, 512, 733, 295, 257, 8482, 7316, 337, 264, 958, 2517, 294, 257, 8310, 50788], "temperature": 0.0, "avg_logprob": -0.08798121604598871, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.002287093782797456}, {"id": 1153, "seek": 476028, "start": 4769.32, "end": 4771.32, "text": " And there's 27 of them", "tokens": [50816, 400, 456, 311, 7634, 295, 552, 50916], "temperature": 0.0, "avg_logprob": -0.08798121604598871, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.002287093782797456}, {"id": 1154, "seek": 476028, "start": 4771.48, "end": 4775.32, "text": " But we have to come up with like precise semantics for exactly how we're going to interpret", "tokens": [50924, 583, 321, 362, 281, 808, 493, 365, 411, 13600, 4361, 45298, 337, 2293, 577, 321, 434, 516, 281, 7302, 51116], "temperature": 0.0, "avg_logprob": -0.08798121604598871, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.002287093782797456}, {"id": 1155, "seek": 476028, "start": 4775.8, "end": 4778.759999999999, "text": " These 27 numbers that these neurons take on", "tokens": [51140, 1981, 7634, 3547, 300, 613, 22027, 747, 322, 51288], "temperature": 0.0, "avg_logprob": -0.08798121604598871, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.002287093782797456}, {"id": 1156, "seek": 476028, "start": 4779.719999999999, "end": 4781.16, "text": " Now intuitively", "tokens": [51336, 823, 46506, 51408], "temperature": 0.0, "avg_logprob": -0.08798121604598871, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.002287093782797456}, {"id": 1157, "seek": 476028, "start": 4781.16, "end": 4784.44, "text": " You see here that these numbers are negative and some of them are positive, etc", "tokens": [51408, 509, 536, 510, 300, 613, 3547, 366, 3671, 293, 512, 295, 552, 366, 3353, 11, 5183, 51572], "temperature": 0.0, "avg_logprob": -0.08798121604598871, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.002287093782797456}, {"id": 1158, "seek": 476028, "start": 4785.16, "end": 4789.24, "text": " And that's because these are coming out of a neural net layer initialized with these", "tokens": [51608, 400, 300, 311, 570, 613, 366, 1348, 484, 295, 257, 18161, 2533, 4583, 5883, 1602, 365, 613, 51812], "temperature": 0.0, "avg_logprob": -0.08798121604598871, "compression_ratio": 1.7747440273037542, "no_speech_prob": 0.002287093782797456}, {"id": 1159, "seek": 479028, "start": 4791.24, "end": 4793.08, "text": " normal distribution", "tokens": [50412, 2710, 7316, 50504], "temperature": 0.0, "avg_logprob": -0.12706461406889416, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.0014549181796610355}, {"id": 1160, "seek": 479028, "start": 4793.08, "end": 4794.28, "text": " parameters", "tokens": [50504, 9834, 50564], "temperature": 0.0, "avg_logprob": -0.12706461406889416, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.0014549181796610355}, {"id": 1161, "seek": 479028, "start": 4794.28, "end": 4796.679999999999, "text": " But what we want is we want something like we had here", "tokens": [50564, 583, 437, 321, 528, 307, 321, 528, 746, 411, 321, 632, 510, 50684], "temperature": 0.0, "avg_logprob": -0.12706461406889416, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.0014549181796610355}, {"id": 1162, "seek": 479028, "start": 4797.24, "end": 4802.92, "text": " Like each row here told us the counts and then we normalize the counts to get probabilities", "tokens": [50712, 1743, 1184, 5386, 510, 1907, 505, 264, 14893, 293, 550, 321, 2710, 1125, 264, 14893, 281, 483, 33783, 50996], "temperature": 0.0, "avg_logprob": -0.12706461406889416, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.0014549181796610355}, {"id": 1163, "seek": 479028, "start": 4803.48, "end": 4805.639999999999, "text": " And we want something similar to come out of a neural net", "tokens": [51024, 400, 321, 528, 746, 2531, 281, 808, 484, 295, 257, 18161, 2533, 51132], "temperature": 0.0, "avg_logprob": -0.12706461406889416, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.0014549181796610355}, {"id": 1164, "seek": 479028, "start": 4806.44, "end": 4809.24, "text": " But what we just have right now is just some negative and positive numbers", "tokens": [51172, 583, 437, 321, 445, 362, 558, 586, 307, 445, 512, 3671, 293, 3353, 3547, 51312], "temperature": 0.0, "avg_logprob": -0.12706461406889416, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.0014549181796610355}, {"id": 1165, "seek": 479028, "start": 4810.599999999999, "end": 4814.5199999999995, "text": " Now we want those numbers to somehow represent the probabilities for the next character", "tokens": [51380, 823, 321, 528, 729, 3547, 281, 6063, 2906, 264, 33783, 337, 264, 958, 2517, 51576], "temperature": 0.0, "avg_logprob": -0.12706461406889416, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.0014549181796610355}, {"id": 1166, "seek": 479028, "start": 4815.32, "end": 4819.24, "text": " But you see that probabilities they they have a special structure. They um", "tokens": [51616, 583, 291, 536, 300, 33783, 436, 436, 362, 257, 2121, 3877, 13, 814, 1105, 51812], "temperature": 0.0, "avg_logprob": -0.12706461406889416, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.0014549181796610355}, {"id": 1167, "seek": 481924, "start": 4819.96, "end": 4825.08, "text": " They're positive numbers and they sum to 1 and so that doesn't just come out of a neural net", "tokens": [50400, 814, 434, 3353, 3547, 293, 436, 2408, 281, 502, 293, 370, 300, 1177, 380, 445, 808, 484, 295, 257, 18161, 2533, 50656], "temperature": 0.0, "avg_logprob": -0.09493787509878886, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.0008295521256513894}, {"id": 1168, "seek": 481924, "start": 4825.8, "end": 4830.12, "text": " And then they can't be counts because these counts are positive", "tokens": [50692, 400, 550, 436, 393, 380, 312, 14893, 570, 613, 14893, 366, 3353, 50908], "temperature": 0.0, "avg_logprob": -0.09493787509878886, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.0008295521256513894}, {"id": 1169, "seek": 481924, "start": 4830.679999999999, "end": 4835.88, "text": " And counts are integers. So counts are also not really a good thing to output from a neural net", "tokens": [50936, 400, 14893, 366, 41674, 13, 407, 14893, 366, 611, 406, 534, 257, 665, 551, 281, 5598, 490, 257, 18161, 2533, 51196], "temperature": 0.0, "avg_logprob": -0.09493787509878886, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.0008295521256513894}, {"id": 1170, "seek": 481924, "start": 4836.679999999999, "end": 4841.4, "text": " So instead what the neural net is going to output and how we are going to interpret the um", "tokens": [51236, 407, 2602, 437, 264, 18161, 2533, 307, 516, 281, 5598, 293, 577, 321, 366, 516, 281, 7302, 264, 1105, 51472], "temperature": 0.0, "avg_logprob": -0.09493787509878886, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.0008295521256513894}, {"id": 1171, "seek": 481924, "start": 4842.12, "end": 4847.639999999999, "text": " The 27 numbers is that these 27 numbers are giving us log counts", "tokens": [51508, 440, 7634, 3547, 307, 300, 613, 7634, 3547, 366, 2902, 505, 3565, 14893, 51784], "temperature": 0.0, "avg_logprob": -0.09493787509878886, "compression_ratio": 1.9154929577464788, "no_speech_prob": 0.0008295521256513894}, {"id": 1172, "seek": 484764, "start": 4848.52, "end": 4849.72, "text": " basically", "tokens": [50408, 1936, 50468], "temperature": 0.0, "avg_logprob": -0.15875229724617893, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.00113349047023803}, {"id": 1173, "seek": 484764, "start": 4849.72, "end": 4850.4400000000005, "text": " um", "tokens": [50468, 1105, 50504], "temperature": 0.0, "avg_logprob": -0.15875229724617893, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.00113349047023803}, {"id": 1174, "seek": 484764, "start": 4850.4400000000005, "end": 4855.240000000001, "text": " So instead of giving us counts directly like in this table, they're giving us log counts", "tokens": [50504, 407, 2602, 295, 2902, 505, 14893, 3838, 411, 294, 341, 3199, 11, 436, 434, 2902, 505, 3565, 14893, 50744], "temperature": 0.0, "avg_logprob": -0.15875229724617893, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.00113349047023803}, {"id": 1175, "seek": 484764, "start": 4856.04, "end": 4860.4400000000005, "text": " And to get the counts, we're going to take the log counts and we're going to exponentiate them", "tokens": [50784, 400, 281, 483, 264, 14893, 11, 321, 434, 516, 281, 747, 264, 3565, 14893, 293, 321, 434, 516, 281, 37871, 13024, 552, 51004], "temperature": 0.0, "avg_logprob": -0.15875229724617893, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.00113349047023803}, {"id": 1176, "seek": 484764, "start": 4861.400000000001, "end": 4863.0, "text": " now", "tokens": [51052, 586, 51132], "temperature": 0.0, "avg_logprob": -0.15875229724617893, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.00113349047023803}, {"id": 1177, "seek": 484764, "start": 4863.0, "end": 4864.200000000001, "text": " exponentiation", "tokens": [51132, 37871, 6642, 51192], "temperature": 0.0, "avg_logprob": -0.15875229724617893, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.00113349047023803}, {"id": 1178, "seek": 484764, "start": 4864.200000000001, "end": 4866.200000000001, "text": " takes the following form", "tokens": [51192, 2516, 264, 3480, 1254, 51292], "temperature": 0.0, "avg_logprob": -0.15875229724617893, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.00113349047023803}, {"id": 1179, "seek": 484764, "start": 4867.160000000001, "end": 4868.92, "text": " It takes numbers", "tokens": [51340, 467, 2516, 3547, 51428], "temperature": 0.0, "avg_logprob": -0.15875229724617893, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.00113349047023803}, {"id": 1180, "seek": 484764, "start": 4868.92, "end": 4872.280000000001, "text": " That are negative or they are positive. It takes the entire real line", "tokens": [51428, 663, 366, 3671, 420, 436, 366, 3353, 13, 467, 2516, 264, 2302, 957, 1622, 51596], "temperature": 0.0, "avg_logprob": -0.15875229724617893, "compression_ratio": 1.7393617021276595, "no_speech_prob": 0.00113349047023803}, {"id": 1181, "seek": 487228, "start": 4872.84, "end": 4877.719999999999, "text": " And then if you plug in negative numbers, you're going to get e to the x which is", "tokens": [50392, 400, 550, 498, 291, 5452, 294, 3671, 3547, 11, 291, 434, 516, 281, 483, 308, 281, 264, 2031, 597, 307, 50636], "temperature": 0.0, "avg_logprob": -0.13568965157309731, "compression_ratio": 1.8633879781420766, "no_speech_prob": 0.029307378455996513}, {"id": 1182, "seek": 487228, "start": 4878.5199999999995, "end": 4880.5199999999995, "text": " always below 1", "tokens": [50676, 1009, 2507, 502, 50776], "temperature": 0.0, "avg_logprob": -0.13568965157309731, "compression_ratio": 1.8633879781420766, "no_speech_prob": 0.029307378455996513}, {"id": 1183, "seek": 487228, "start": 4880.599999999999, "end": 4882.599999999999, "text": " So you're getting numbers lower than 1", "tokens": [50780, 407, 291, 434, 1242, 3547, 3126, 813, 502, 50880], "temperature": 0.0, "avg_logprob": -0.13568965157309731, "compression_ratio": 1.8633879781420766, "no_speech_prob": 0.029307378455996513}, {"id": 1184, "seek": 487228, "start": 4883.48, "end": 4887.719999999999, "text": " And if you plug in numbers greater than zero, you're getting numbers greater than one", "tokens": [50924, 400, 498, 291, 5452, 294, 3547, 5044, 813, 4018, 11, 291, 434, 1242, 3547, 5044, 813, 472, 51136], "temperature": 0.0, "avg_logprob": -0.13568965157309731, "compression_ratio": 1.8633879781420766, "no_speech_prob": 0.029307378455996513}, {"id": 1185, "seek": 487228, "start": 4888.36, "end": 4890.36, "text": " all the way growing to the infinity", "tokens": [51168, 439, 264, 636, 4194, 281, 264, 13202, 51268], "temperature": 0.0, "avg_logprob": -0.13568965157309731, "compression_ratio": 1.8633879781420766, "no_speech_prob": 0.029307378455996513}, {"id": 1186, "seek": 487228, "start": 4890.84, "end": 4892.84, "text": " And this here grows to zero", "tokens": [51292, 400, 341, 510, 13156, 281, 4018, 51392], "temperature": 0.0, "avg_logprob": -0.13568965157309731, "compression_ratio": 1.8633879781420766, "no_speech_prob": 0.029307378455996513}, {"id": 1187, "seek": 487228, "start": 4893.32, "end": 4896.679999999999, "text": " So basically we're going to take these numbers", "tokens": [51416, 407, 1936, 321, 434, 516, 281, 747, 613, 3547, 51584], "temperature": 0.0, "avg_logprob": -0.13568965157309731, "compression_ratio": 1.8633879781420766, "no_speech_prob": 0.029307378455996513}, {"id": 1188, "seek": 487228, "start": 4897.8, "end": 4899.8, "text": " here", "tokens": [51640, 510, 51740], "temperature": 0.0, "avg_logprob": -0.13568965157309731, "compression_ratio": 1.8633879781420766, "no_speech_prob": 0.029307378455996513}, {"id": 1189, "seek": 487228, "start": 4900.36, "end": 4902.2, "text": " and", "tokens": [51768, 293, 51860], "temperature": 0.0, "avg_logprob": -0.13568965157309731, "compression_ratio": 1.8633879781420766, "no_speech_prob": 0.029307378455996513}, {"id": 1190, "seek": 490228, "start": 4903.24, "end": 4905.639999999999, "text": " Instead of them being positive and negative and all over the place", "tokens": [50412, 7156, 295, 552, 885, 3353, 293, 3671, 293, 439, 670, 264, 1081, 50532], "temperature": 0.0, "avg_logprob": -0.1528157750400928, "compression_ratio": 1.9073359073359073, "no_speech_prob": 0.000335332821123302}, {"id": 1191, "seek": 490228, "start": 4906.04, "end": 4911.5599999999995, "text": " We're going to interpret them as log counts and then we're going to element wise exponentiate these numbers", "tokens": [50552, 492, 434, 516, 281, 7302, 552, 382, 3565, 14893, 293, 550, 321, 434, 516, 281, 4478, 10829, 37871, 13024, 613, 3547, 50828], "temperature": 0.0, "avg_logprob": -0.1528157750400928, "compression_ratio": 1.9073359073359073, "no_speech_prob": 0.000335332821123302}, {"id": 1192, "seek": 490228, "start": 4912.84, "end": 4915.4, "text": " Exponentiating them now gives us something like this", "tokens": [50892, 21391, 266, 23012, 990, 552, 586, 2709, 505, 746, 411, 341, 51020], "temperature": 0.0, "avg_logprob": -0.1528157750400928, "compression_ratio": 1.9073359073359073, "no_speech_prob": 0.000335332821123302}, {"id": 1193, "seek": 490228, "start": 4916.44, "end": 4919.0, "text": " And you see that these numbers now because they went through an exponent", "tokens": [51072, 400, 291, 536, 300, 613, 3547, 586, 570, 436, 1437, 807, 364, 37871, 51200], "temperature": 0.0, "avg_logprob": -0.1528157750400928, "compression_ratio": 1.9073359073359073, "no_speech_prob": 0.000335332821123302}, {"id": 1194, "seek": 490228, "start": 4919.32, "end": 4923.719999999999, "text": " All the negative numbers turned into numbers below one like 0.338", "tokens": [51216, 1057, 264, 3671, 3547, 3574, 666, 3547, 2507, 472, 411, 1958, 13, 10191, 23, 51436], "temperature": 0.0, "avg_logprob": -0.1528157750400928, "compression_ratio": 1.9073359073359073, "no_speech_prob": 0.000335332821123302}, {"id": 1195, "seek": 490228, "start": 4924.2, "end": 4929.4, "text": " And all the positive numbers originally turned into even more positive numbers sort of greater than one", "tokens": [51460, 400, 439, 264, 3353, 3547, 7993, 3574, 666, 754, 544, 3353, 3547, 1333, 295, 5044, 813, 472, 51720], "temperature": 0.0, "avg_logprob": -0.1528157750400928, "compression_ratio": 1.9073359073359073, "no_speech_prob": 0.000335332821123302}, {"id": 1196, "seek": 490228, "start": 4930.12, "end": 4932.12, "text": " um, so like for example", "tokens": [51756, 1105, 11, 370, 411, 337, 1365, 51856], "temperature": 0.0, "avg_logprob": -0.1528157750400928, "compression_ratio": 1.9073359073359073, "no_speech_prob": 0.000335332821123302}, {"id": 1197, "seek": 493228, "start": 4932.5199999999995, "end": 4934.5199999999995, "text": " 7", "tokens": [50376, 1614, 50476], "temperature": 0.0, "avg_logprob": -0.211368299510381, "compression_ratio": 1.4397905759162304, "no_speech_prob": 0.00036824363633058965}, {"id": 1198, "seek": 493228, "start": 4934.5199999999995, "end": 4937.0, "text": " Is some positive number over here?", "tokens": [50476, 1119, 512, 3353, 1230, 670, 510, 30, 50600], "temperature": 0.0, "avg_logprob": -0.211368299510381, "compression_ratio": 1.4397905759162304, "no_speech_prob": 0.00036824363633058965}, {"id": 1199, "seek": 493228, "start": 4938.44, "end": 4940.44, "text": " That is greater than zero", "tokens": [50672, 663, 307, 5044, 813, 4018, 50772], "temperature": 0.0, "avg_logprob": -0.211368299510381, "compression_ratio": 1.4397905759162304, "no_speech_prob": 0.00036824363633058965}, {"id": 1200, "seek": 493228, "start": 4941.08, "end": 4942.2, "text": " But", "tokens": [50804, 583, 50860], "temperature": 0.0, "avg_logprob": -0.211368299510381, "compression_ratio": 1.4397905759162304, "no_speech_prob": 0.00036824363633058965}, {"id": 1201, "seek": 493228, "start": 4942.2, "end": 4944.2, "text": " Exponentiated outputs here", "tokens": [50860, 21391, 266, 23012, 770, 23930, 510, 50960], "temperature": 0.0, "avg_logprob": -0.211368299510381, "compression_ratio": 1.4397905759162304, "no_speech_prob": 0.00036824363633058965}, {"id": 1202, "seek": 493228, "start": 4944.84, "end": 4950.44, "text": " Basically give us something that we can use and interpret as the equivalent of counts originally", "tokens": [50992, 8537, 976, 505, 746, 300, 321, 393, 764, 293, 7302, 382, 264, 10344, 295, 14893, 7993, 51272], "temperature": 0.0, "avg_logprob": -0.211368299510381, "compression_ratio": 1.4397905759162304, "no_speech_prob": 0.00036824363633058965}, {"id": 1203, "seek": 493228, "start": 4950.92, "end": 4954.84, "text": " So you see these counts here 112 7 51 1 etc", "tokens": [51296, 407, 291, 536, 613, 14893, 510, 45835, 1614, 18485, 502, 5183, 51492], "temperature": 0.0, "avg_logprob": -0.211368299510381, "compression_ratio": 1.4397905759162304, "no_speech_prob": 0.00036824363633058965}, {"id": 1204, "seek": 493228, "start": 4956.36, "end": 4958.36, "text": " The neural net is kind of now predicting", "tokens": [51568, 440, 18161, 2533, 307, 733, 295, 586, 32884, 51668], "temperature": 0.0, "avg_logprob": -0.211368299510381, "compression_ratio": 1.4397905759162304, "no_speech_prob": 0.00036824363633058965}, {"id": 1205, "seek": 495836, "start": 4959.16, "end": 4960.36, "text": " Uh", "tokens": [50404, 4019, 50464], "temperature": 0.0, "avg_logprob": -0.16271466149224176, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.00288939056918025}, {"id": 1206, "seek": 495836, "start": 4960.36, "end": 4961.5599999999995, "text": " counts", "tokens": [50464, 14893, 50524], "temperature": 0.0, "avg_logprob": -0.16271466149224176, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.00288939056918025}, {"id": 1207, "seek": 495836, "start": 4961.5599999999995, "end": 4966.2, "text": " And these counts are positive numbers. They can never be below zero. So that makes sense", "tokens": [50524, 400, 613, 14893, 366, 3353, 3547, 13, 814, 393, 1128, 312, 2507, 4018, 13, 407, 300, 1669, 2020, 50756], "temperature": 0.0, "avg_logprob": -0.16271466149224176, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.00288939056918025}, {"id": 1208, "seek": 495836, "start": 4966.759999999999, "end": 4969.08, "text": " And they can now take on various values", "tokens": [50784, 400, 436, 393, 586, 747, 322, 3683, 4190, 50900], "temperature": 0.0, "avg_logprob": -0.16271466149224176, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.00288939056918025}, {"id": 1209, "seek": 495836, "start": 4969.719999999999, "end": 4972.599999999999, "text": " Depending on the settings of w", "tokens": [50932, 22539, 322, 264, 6257, 295, 261, 51076], "temperature": 0.0, "avg_logprob": -0.16271466149224176, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.00288939056918025}, {"id": 1210, "seek": 495836, "start": 4974.2, "end": 4976.2, "text": " So let me break this down", "tokens": [51156, 407, 718, 385, 1821, 341, 760, 51256], "temperature": 0.0, "avg_logprob": -0.16271466149224176, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.00288939056918025}, {"id": 1211, "seek": 495836, "start": 4976.2, "end": 4979.08, "text": " We're going to interpret these to be the log counts", "tokens": [51256, 492, 434, 516, 281, 7302, 613, 281, 312, 264, 3565, 14893, 51400], "temperature": 0.0, "avg_logprob": -0.16271466149224176, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.00288939056918025}, {"id": 1212, "seek": 495836, "start": 4981.24, "end": 4984.36, "text": " In other words for this that is often used is so called logits", "tokens": [51508, 682, 661, 2283, 337, 341, 300, 307, 2049, 1143, 307, 370, 1219, 3565, 1208, 51664], "temperature": 0.0, "avg_logprob": -0.16271466149224176, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.00288939056918025}, {"id": 1213, "seek": 495836, "start": 4985.16, "end": 4987.639999999999, "text": " These are logits log counts", "tokens": [51704, 1981, 366, 3565, 1208, 3565, 14893, 51828], "temperature": 0.0, "avg_logprob": -0.16271466149224176, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.00288939056918025}, {"id": 1214, "seek": 498836, "start": 4988.599999999999, "end": 4990.599999999999, "text": " Then these will be sort of the counts", "tokens": [50376, 1396, 613, 486, 312, 1333, 295, 264, 14893, 50476], "temperature": 0.0, "avg_logprob": -0.1636731253729926, "compression_ratio": 1.72, "no_speech_prob": 0.0004305242619011551}, {"id": 1215, "seek": 498836, "start": 4991.32, "end": 4993.32, "text": " Logits exponentiated", "tokens": [50512, 10824, 1208, 37871, 72, 770, 50612], "temperature": 0.0, "avg_logprob": -0.1636731253729926, "compression_ratio": 1.72, "no_speech_prob": 0.0004305242619011551}, {"id": 1216, "seek": 498836, "start": 4993.4, "end": 4997.16, "text": " And this is equivalent to the n matrix sort of the n", "tokens": [50616, 400, 341, 307, 10344, 281, 264, 297, 8141, 1333, 295, 264, 297, 50804], "temperature": 0.0, "avg_logprob": -0.1636731253729926, "compression_ratio": 1.72, "no_speech_prob": 0.0004305242619011551}, {"id": 1217, "seek": 498836, "start": 4998.12, "end": 5000.759999999999, "text": " Array that we used previously. Remember, this was the n", "tokens": [50852, 1587, 3458, 300, 321, 1143, 8046, 13, 5459, 11, 341, 390, 264, 297, 50984], "temperature": 0.0, "avg_logprob": -0.1636731253729926, "compression_ratio": 1.72, "no_speech_prob": 0.0004305242619011551}, {"id": 1218, "seek": 498836, "start": 5001.639999999999, "end": 5007.0, "text": " This is the the array of counts and each row here are the counts for the", "tokens": [51028, 639, 307, 264, 264, 10225, 295, 14893, 293, 1184, 5386, 510, 366, 264, 14893, 337, 264, 51296], "temperature": 0.0, "avg_logprob": -0.1636731253729926, "compression_ratio": 1.72, "no_speech_prob": 0.0004305242619011551}, {"id": 1219, "seek": 498836, "start": 5007.799999999999, "end": 5010.36, "text": " For the um next character sort of", "tokens": [51336, 1171, 264, 1105, 958, 2517, 1333, 295, 51464], "temperature": 0.0, "avg_logprob": -0.1636731253729926, "compression_ratio": 1.72, "no_speech_prob": 0.0004305242619011551}, {"id": 1220, "seek": 498836, "start": 5012.679999999999, "end": 5016.759999999999, "text": " So those are the counts and now the probabilities are just the counts", "tokens": [51580, 407, 729, 366, 264, 14893, 293, 586, 264, 33783, 366, 445, 264, 14893, 51784], "temperature": 0.0, "avg_logprob": -0.1636731253729926, "compression_ratio": 1.72, "no_speech_prob": 0.0004305242619011551}, {"id": 1221, "seek": 501676, "start": 5017.320000000001, "end": 5019.320000000001, "text": " um normalized", "tokens": [50392, 1105, 48704, 50492], "temperature": 0.0, "avg_logprob": -0.17261285560075626, "compression_ratio": 1.66, "no_speech_prob": 0.0037651187740266323}, {"id": 1222, "seek": 501676, "start": 5019.72, "end": 5021.72, "text": " and so um", "tokens": [50512, 293, 370, 1105, 50612], "temperature": 0.0, "avg_logprob": -0.17261285560075626, "compression_ratio": 1.66, "no_speech_prob": 0.0037651187740266323}, {"id": 1223, "seek": 501676, "start": 5021.72, "end": 5025.400000000001, "text": " I'm not going to find the same but basically I'm not going to scroll all over the place", "tokens": [50612, 286, 478, 406, 516, 281, 915, 264, 912, 457, 1936, 286, 478, 406, 516, 281, 11369, 439, 670, 264, 1081, 50796], "temperature": 0.0, "avg_logprob": -0.17261285560075626, "compression_ratio": 1.66, "no_speech_prob": 0.0037651187740266323}, {"id": 1224, "seek": 501676, "start": 5026.04, "end": 5029.400000000001, "text": " We've already done this. We want two counts that sum", "tokens": [50828, 492, 600, 1217, 1096, 341, 13, 492, 528, 732, 14893, 300, 2408, 50996], "temperature": 0.0, "avg_logprob": -0.17261285560075626, "compression_ratio": 1.66, "no_speech_prob": 0.0037651187740266323}, {"id": 1225, "seek": 501676, "start": 5030.12, "end": 5033.88, "text": " Along the first dimension and we want to keep them. It's true", "tokens": [51032, 17457, 264, 700, 10139, 293, 321, 528, 281, 1066, 552, 13, 467, 311, 2074, 51220], "temperature": 0.0, "avg_logprob": -0.17261285560075626, "compression_ratio": 1.66, "no_speech_prob": 0.0037651187740266323}, {"id": 1226, "seek": 501676, "start": 5034.76, "end": 5041.320000000001, "text": " We've went over this and this is how we normalized the rows of our counts matrix to get our probabilities", "tokens": [51264, 492, 600, 1437, 670, 341, 293, 341, 307, 577, 321, 48704, 264, 13241, 295, 527, 14893, 8141, 281, 483, 527, 33783, 51592], "temperature": 0.0, "avg_logprob": -0.17261285560075626, "compression_ratio": 1.66, "no_speech_prob": 0.0037651187740266323}, {"id": 1227, "seek": 504132, "start": 5042.12, "end": 5044.12, "text": " Props", "tokens": [50404, 21944, 82, 50504], "temperature": 0.0, "avg_logprob": -0.19640753925710486, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012448111083358526}, {"id": 1228, "seek": 504132, "start": 5044.84, "end": 5046.84, "text": " So now these are the probabilities", "tokens": [50540, 407, 586, 613, 366, 264, 33783, 50640], "temperature": 0.0, "avg_logprob": -0.19640753925710486, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012448111083358526}, {"id": 1229, "seek": 504132, "start": 5047.88, "end": 5052.12, "text": " And these are the counts that we have currently and now when I show the probabilities", "tokens": [50692, 400, 613, 366, 264, 14893, 300, 321, 362, 4362, 293, 586, 562, 286, 855, 264, 33783, 50904], "temperature": 0.0, "avg_logprob": -0.19640753925710486, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012448111083358526}, {"id": 1230, "seek": 504132, "start": 5053.719999999999, "end": 5057.799999999999, "text": " You see that um every row here, of course", "tokens": [50984, 509, 536, 300, 1105, 633, 5386, 510, 11, 295, 1164, 51188], "temperature": 0.0, "avg_logprob": -0.19640753925710486, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012448111083358526}, {"id": 1231, "seek": 504132, "start": 5059.48, "end": 5061.48, "text": " Will sum to one", "tokens": [51272, 3099, 2408, 281, 472, 51372], "temperature": 0.0, "avg_logprob": -0.19640753925710486, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012448111083358526}, {"id": 1232, "seek": 504132, "start": 5061.48, "end": 5063.16, "text": " Because they're normalized", "tokens": [51372, 1436, 436, 434, 48704, 51456], "temperature": 0.0, "avg_logprob": -0.19640753925710486, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012448111083358526}, {"id": 1233, "seek": 504132, "start": 5063.16, "end": 5065.16, "text": " And the shape of this", "tokens": [51456, 400, 264, 3909, 295, 341, 51556], "temperature": 0.0, "avg_logprob": -0.19640753925710486, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012448111083358526}, {"id": 1234, "seek": 504132, "start": 5065.32, "end": 5067.32, "text": " Is five by 27", "tokens": [51564, 1119, 1732, 538, 7634, 51664], "temperature": 0.0, "avg_logprob": -0.19640753925710486, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.0012448111083358526}, {"id": 1235, "seek": 506732, "start": 5067.48, "end": 5071.24, "text": " And so really what we've achieved is for every one of our five examples", "tokens": [50372, 400, 370, 534, 437, 321, 600, 11042, 307, 337, 633, 472, 295, 527, 1732, 5110, 50560], "temperature": 0.0, "avg_logprob": -0.097558390986812, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0007096108165569603}, {"id": 1236, "seek": 506732, "start": 5071.719999999999, "end": 5074.28, "text": " We now have a row that came out of a neural net", "tokens": [50584, 492, 586, 362, 257, 5386, 300, 1361, 484, 295, 257, 18161, 2533, 50712], "temperature": 0.0, "avg_logprob": -0.097558390986812, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0007096108165569603}, {"id": 1237, "seek": 506732, "start": 5075.16, "end": 5079.96, "text": " And because of the transformations here, we made sure that this output of this neural net now", "tokens": [50756, 400, 570, 295, 264, 34852, 510, 11, 321, 1027, 988, 300, 341, 5598, 295, 341, 18161, 2533, 586, 50996], "temperature": 0.0, "avg_logprob": -0.097558390986812, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0007096108165569603}, {"id": 1238, "seek": 506732, "start": 5080.36, "end": 5083.0, "text": " Are probabilities or we can interpret to be probabilities", "tokens": [51016, 2014, 33783, 420, 321, 393, 7302, 281, 312, 33783, 51148], "temperature": 0.0, "avg_logprob": -0.097558390986812, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0007096108165569603}, {"id": 1239, "seek": 506732, "start": 5084.12, "end": 5085.32, "text": " so", "tokens": [51204, 370, 51264], "temperature": 0.0, "avg_logprob": -0.097558390986812, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0007096108165569603}, {"id": 1240, "seek": 506732, "start": 5085.32, "end": 5090.12, "text": " Our wx here gave us logits and then we interpret those to be log counts", "tokens": [51264, 2621, 261, 87, 510, 2729, 505, 3565, 1208, 293, 550, 321, 7302, 729, 281, 312, 3565, 14893, 51504], "temperature": 0.0, "avg_logprob": -0.097558390986812, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0007096108165569603}, {"id": 1241, "seek": 506732, "start": 5090.759999999999, "end": 5093.32, "text": " We exponentiate to get something that looks like counts", "tokens": [51536, 492, 37871, 13024, 281, 483, 746, 300, 1542, 411, 14893, 51664], "temperature": 0.0, "avg_logprob": -0.097558390986812, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0007096108165569603}, {"id": 1242, "seek": 506732, "start": 5094.04, "end": 5096.759999999999, "text": " And then we normalize those counts to get a probability distribution", "tokens": [51700, 400, 550, 321, 2710, 1125, 729, 14893, 281, 483, 257, 8482, 7316, 51836], "temperature": 0.0, "avg_logprob": -0.097558390986812, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0007096108165569603}, {"id": 1243, "seek": 509732, "start": 5097.48, "end": 5099.719999999999, "text": " And all of these are differentiable operations", "tokens": [50372, 400, 439, 295, 613, 366, 819, 9364, 7705, 50484], "temperature": 0.0, "avg_logprob": -0.15592412446674547, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0006563213537447155}, {"id": 1244, "seek": 509732, "start": 5100.36, "end": 5102.679999999999, "text": " So what we've done now is we are taking inputs", "tokens": [50516, 407, 437, 321, 600, 1096, 586, 307, 321, 366, 1940, 15743, 50632], "temperature": 0.0, "avg_logprob": -0.15592412446674547, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0006563213537447155}, {"id": 1245, "seek": 509732, "start": 5103.24, "end": 5106.12, "text": " We have differentiable operations that we can back propagate through", "tokens": [50660, 492, 362, 819, 9364, 7705, 300, 321, 393, 646, 48256, 807, 50804], "temperature": 0.0, "avg_logprob": -0.15592412446674547, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0006563213537447155}, {"id": 1246, "seek": 509732, "start": 5106.92, "end": 5108.92, "text": " And we're getting out probability distributions", "tokens": [50844, 400, 321, 434, 1242, 484, 8482, 37870, 50944], "temperature": 0.0, "avg_logprob": -0.15592412446674547, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0006563213537447155}, {"id": 1247, "seek": 509732, "start": 5109.88, "end": 5113.88, "text": " So um for example for the zeroth example that fed in", "tokens": [50992, 407, 1105, 337, 1365, 337, 264, 44746, 900, 1365, 300, 4636, 294, 51192], "temperature": 0.0, "avg_logprob": -0.15592412446674547, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0006563213537447155}, {"id": 1248, "seek": 509732, "start": 5115.16, "end": 5117.16, "text": " Right, which was um", "tokens": [51256, 1779, 11, 597, 390, 1105, 51356], "temperature": 0.0, "avg_logprob": -0.15592412446674547, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0006563213537447155}, {"id": 1249, "seek": 509732, "start": 5117.16, "end": 5119.639999999999, "text": " The zeroth example here was a one half vector of zero", "tokens": [51356, 440, 44746, 900, 1365, 510, 390, 257, 472, 1922, 8062, 295, 4018, 51480], "temperature": 0.0, "avg_logprob": -0.15592412446674547, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0006563213537447155}, {"id": 1250, "seek": 509732, "start": 5120.599999999999, "end": 5122.599999999999, "text": " and um", "tokens": [51528, 293, 1105, 51628], "temperature": 0.0, "avg_logprob": -0.15592412446674547, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0006563213537447155}, {"id": 1251, "seek": 509732, "start": 5122.599999999999, "end": 5125.32, "text": " It basically corresponded to feeding in", "tokens": [51628, 467, 1936, 6805, 292, 281, 12919, 294, 51764], "temperature": 0.0, "avg_logprob": -0.15592412446674547, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0006563213537447155}, {"id": 1252, "seek": 512532, "start": 5125.48, "end": 5126.599999999999, "text": " Uh", "tokens": [50372, 4019, 50428], "temperature": 0.0, "avg_logprob": -0.10992666618111208, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.0020828587003052235}, {"id": 1253, "seek": 512532, "start": 5126.599999999999, "end": 5127.799999999999, "text": " This example here", "tokens": [50428, 639, 1365, 510, 50488], "temperature": 0.0, "avg_logprob": -0.10992666618111208, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.0020828587003052235}, {"id": 1254, "seek": 512532, "start": 5127.799999999999, "end": 5133.639999999999, "text": " So we're feeding in a dot into a neural net and the way we fed the dot into a neural net is that we first got its index", "tokens": [50488, 407, 321, 434, 12919, 294, 257, 5893, 666, 257, 18161, 2533, 293, 264, 636, 321, 4636, 264, 5893, 666, 257, 18161, 2533, 307, 300, 321, 700, 658, 1080, 8186, 50780], "temperature": 0.0, "avg_logprob": -0.10992666618111208, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.0020828587003052235}, {"id": 1255, "seek": 512532, "start": 5134.36, "end": 5136.36, "text": " Then we one hot encoded it", "tokens": [50816, 1396, 321, 472, 2368, 2058, 12340, 309, 50916], "temperature": 0.0, "avg_logprob": -0.10992666618111208, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.0020828587003052235}, {"id": 1256, "seek": 512532, "start": 5136.599999999999, "end": 5139.4, "text": " Then it went into the neural net and out came", "tokens": [50928, 1396, 309, 1437, 666, 264, 18161, 2533, 293, 484, 1361, 51068], "temperature": 0.0, "avg_logprob": -0.10992666618111208, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.0020828587003052235}, {"id": 1257, "seek": 512532, "start": 5140.599999999999, "end": 5142.599999999999, "text": " This distribution of probabilities", "tokens": [51128, 639, 7316, 295, 33783, 51228], "temperature": 0.0, "avg_logprob": -0.10992666618111208, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.0020828587003052235}, {"id": 1258, "seek": 512532, "start": 5143.4, "end": 5145.4, "text": " And its shape", "tokens": [51268, 400, 1080, 3909, 51368], "temperature": 0.0, "avg_logprob": -0.10992666618111208, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.0020828587003052235}, {"id": 1259, "seek": 512532, "start": 5146.44, "end": 5153.32, "text": " Is 27 there's 27 numbers and we're going to interpret this as the neural net's assignment for how likely", "tokens": [51420, 1119, 7634, 456, 311, 7634, 3547, 293, 321, 434, 516, 281, 7302, 341, 382, 264, 18161, 2533, 311, 15187, 337, 577, 3700, 51764], "temperature": 0.0, "avg_logprob": -0.10992666618111208, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.0020828587003052235}, {"id": 1260, "seek": 515332, "start": 5154.04, "end": 5156.04, "text": " every one of these characters", "tokens": [50400, 633, 472, 295, 613, 4342, 50500], "temperature": 0.0, "avg_logprob": -0.07605090574784712, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0044678994454443455}, {"id": 1261, "seek": 515332, "start": 5156.599999999999, "end": 5158.84, "text": " The 27 characters are to come next", "tokens": [50528, 440, 7634, 4342, 366, 281, 808, 958, 50640], "temperature": 0.0, "avg_logprob": -0.07605090574784712, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0044678994454443455}, {"id": 1262, "seek": 515332, "start": 5159.799999999999, "end": 5161.799999999999, "text": " And as we tune the weights w", "tokens": [50688, 400, 382, 321, 10864, 264, 17443, 261, 50788], "temperature": 0.0, "avg_logprob": -0.07605090574784712, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0044678994454443455}, {"id": 1263, "seek": 515332, "start": 5162.44, "end": 5166.36, "text": " We're going to be of course getting different probabilities out for any character that you input", "tokens": [50820, 492, 434, 516, 281, 312, 295, 1164, 1242, 819, 33783, 484, 337, 604, 2517, 300, 291, 4846, 51016], "temperature": 0.0, "avg_logprob": -0.07605090574784712, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0044678994454443455}, {"id": 1264, "seek": 515332, "start": 5167.24, "end": 5170.36, "text": " And so now the question is just can we optimize and find a good w?", "tokens": [51060, 400, 370, 586, 264, 1168, 307, 445, 393, 321, 19719, 293, 915, 257, 665, 261, 30, 51216], "temperature": 0.0, "avg_logprob": -0.07605090574784712, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0044678994454443455}, {"id": 1265, "seek": 515332, "start": 5171.08, "end": 5173.96, "text": " Such that the probabilities coming out are pretty good", "tokens": [51252, 9653, 300, 264, 33783, 1348, 484, 366, 1238, 665, 51396], "temperature": 0.0, "avg_logprob": -0.07605090574784712, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0044678994454443455}, {"id": 1266, "seek": 515332, "start": 5174.5199999999995, "end": 5176.84, "text": " And the way we measure pretty good is by the loss function", "tokens": [51424, 400, 264, 636, 321, 3481, 1238, 665, 307, 538, 264, 4470, 2445, 51540], "temperature": 0.0, "avg_logprob": -0.07605090574784712, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0044678994454443455}, {"id": 1267, "seek": 515332, "start": 5177.16, "end": 5180.84, "text": " Okay, so I organized everything into a single summary so that hopefully it's a bit more clear", "tokens": [51556, 1033, 11, 370, 286, 9983, 1203, 666, 257, 2167, 12691, 370, 300, 4696, 309, 311, 257, 857, 544, 1850, 51740], "temperature": 0.0, "avg_logprob": -0.07605090574784712, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0044678994454443455}, {"id": 1268, "seek": 518084, "start": 5181.24, "end": 5183.8, "text": " So it starts here with an input data set", "tokens": [50384, 407, 309, 3719, 510, 365, 364, 4846, 1412, 992, 50512], "temperature": 0.0, "avg_logprob": -0.1432740054553068, "compression_ratio": 1.645320197044335, "no_speech_prob": 0.0015246368711814284}, {"id": 1269, "seek": 518084, "start": 5184.4400000000005, "end": 5191.32, "text": " We have some inputs to the neural net and we have some labels for the correct next character in a sequence and these are integers", "tokens": [50544, 492, 362, 512, 15743, 281, 264, 18161, 2533, 293, 321, 362, 512, 16949, 337, 264, 3006, 958, 2517, 294, 257, 8310, 293, 613, 366, 41674, 50888], "temperature": 0.0, "avg_logprob": -0.1432740054553068, "compression_ratio": 1.645320197044335, "no_speech_prob": 0.0015246368711814284}, {"id": 1270, "seek": 518084, "start": 5192.84, "end": 5197.72, "text": " Here i'm using uh torch generators now so that you see the same numbers that I see", "tokens": [50964, 1692, 741, 478, 1228, 2232, 27822, 38662, 586, 370, 300, 291, 536, 264, 912, 3547, 300, 286, 536, 51208], "temperature": 0.0, "avg_logprob": -0.1432740054553068, "compression_ratio": 1.645320197044335, "no_speech_prob": 0.0015246368711814284}, {"id": 1271, "seek": 518084, "start": 5198.6, "end": 5200.6, "text": " and i'm generating um", "tokens": [51252, 293, 741, 478, 17746, 1105, 51352], "temperature": 0.0, "avg_logprob": -0.1432740054553068, "compression_ratio": 1.645320197044335, "no_speech_prob": 0.0015246368711814284}, {"id": 1272, "seek": 518084, "start": 5200.92, "end": 5205.56, "text": " 27 neurons weights and each neuron here receives 27 inputs", "tokens": [51368, 7634, 22027, 17443, 293, 1184, 34090, 510, 20717, 7634, 15743, 51600], "temperature": 0.0, "avg_logprob": -0.1432740054553068, "compression_ratio": 1.645320197044335, "no_speech_prob": 0.0015246368711814284}, {"id": 1273, "seek": 520556, "start": 5206.280000000001, "end": 5214.360000000001, "text": " Then here we're going to plug in all the input examples x's into a neural net. So here this is a forward pass", "tokens": [50400, 1396, 510, 321, 434, 516, 281, 5452, 294, 439, 264, 4846, 5110, 2031, 311, 666, 257, 18161, 2533, 13, 407, 510, 341, 307, 257, 2128, 1320, 50804], "temperature": 0.0, "avg_logprob": -0.17056801380255285, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0021825365256518126}, {"id": 1274, "seek": 520556, "start": 5215.72, "end": 5219.56, "text": " First we have to encode all of the inputs into one half representations", "tokens": [50872, 2386, 321, 362, 281, 2058, 1429, 439, 295, 264, 15743, 666, 472, 1922, 33358, 51064], "temperature": 0.0, "avg_logprob": -0.17056801380255285, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0021825365256518126}, {"id": 1275, "seek": 520556, "start": 5220.4400000000005, "end": 5228.68, "text": " So we have 27 classes. We pass in these integers and x ink becomes a array that is 5 by 27", "tokens": [51108, 407, 321, 362, 7634, 5359, 13, 492, 1320, 294, 613, 41674, 293, 2031, 11276, 3643, 257, 10225, 300, 307, 1025, 538, 7634, 51520], "temperature": 0.0, "avg_logprob": -0.17056801380255285, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0021825365256518126}, {"id": 1276, "seek": 520556, "start": 5229.72, "end": 5231.72, "text": " zeros except for a few ones", "tokens": [51572, 35193, 3993, 337, 257, 1326, 2306, 51672], "temperature": 0.0, "avg_logprob": -0.17056801380255285, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0021825365256518126}, {"id": 1277, "seek": 523172, "start": 5232.280000000001, "end": 5235.8, "text": " We then multiply this in the first layer of a neural net to get logits", "tokens": [50392, 492, 550, 12972, 341, 294, 264, 700, 4583, 295, 257, 18161, 2533, 281, 483, 3565, 1208, 50568], "temperature": 0.0, "avg_logprob": -0.16862637063731317, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.002980543067678809}, {"id": 1278, "seek": 523172, "start": 5236.92, "end": 5242.92, "text": " Exponentiate the logits to get fake counts sort of and normalize these counts to get probabilities", "tokens": [50624, 21391, 266, 23012, 473, 264, 3565, 1208, 281, 483, 7592, 14893, 1333, 295, 293, 2710, 1125, 613, 14893, 281, 483, 33783, 50924], "temperature": 0.0, "avg_logprob": -0.16862637063731317, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.002980543067678809}, {"id": 1279, "seek": 523172, "start": 5244.4400000000005, "end": 5248.52, "text": " So the these last two lines by the way here are called the softmax", "tokens": [51000, 407, 264, 613, 1036, 732, 3876, 538, 264, 636, 510, 366, 1219, 264, 2787, 41167, 51204], "temperature": 0.0, "avg_logprob": -0.16862637063731317, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.002980543067678809}, {"id": 1280, "seek": 523172, "start": 5249.8, "end": 5251.8, "text": " Uh, which I pulled up here", "tokens": [51268, 4019, 11, 597, 286, 7373, 493, 510, 51368], "temperature": 0.0, "avg_logprob": -0.16862637063731317, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.002980543067678809}, {"id": 1281, "seek": 523172, "start": 5252.04, "end": 5258.280000000001, "text": " Softmax is a very often used layer in a neural net that takes these z's which are logits", "tokens": [51380, 16985, 41167, 307, 257, 588, 2049, 1143, 4583, 294, 257, 18161, 2533, 300, 2516, 613, 710, 311, 597, 366, 3565, 1208, 51692], "temperature": 0.0, "avg_logprob": -0.16862637063731317, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.002980543067678809}, {"id": 1282, "seek": 525828, "start": 5258.92, "end": 5260.92, "text": " Exponentiates them", "tokens": [50396, 21391, 266, 23012, 1024, 552, 50496], "temperature": 0.0, "avg_logprob": -0.16345013609719933, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.005059855990111828}, {"id": 1283, "seek": 525828, "start": 5261.0, "end": 5264.04, "text": " And uh divides and normalizes it's a way of taking", "tokens": [50500, 400, 2232, 41347, 293, 2710, 5660, 309, 311, 257, 636, 295, 1940, 50652], "temperature": 0.0, "avg_logprob": -0.16345013609719933, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.005059855990111828}, {"id": 1284, "seek": 525828, "start": 5264.679999999999, "end": 5266.679999999999, "text": " outputs of a neural net layer and these", "tokens": [50684, 23930, 295, 257, 18161, 2533, 4583, 293, 613, 50784], "temperature": 0.0, "avg_logprob": -0.16345013609719933, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.005059855990111828}, {"id": 1285, "seek": 525828, "start": 5267.24, "end": 5269.24, "text": " These outputs can be positive or negative", "tokens": [50812, 1981, 23930, 393, 312, 3353, 420, 3671, 50912], "temperature": 0.0, "avg_logprob": -0.16345013609719933, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.005059855990111828}, {"id": 1286, "seek": 525828, "start": 5269.8, "end": 5274.5199999999995, "text": " And it outputs probability distributions. It outputs something that is always", "tokens": [50940, 400, 309, 23930, 8482, 37870, 13, 467, 23930, 746, 300, 307, 1009, 51176], "temperature": 0.0, "avg_logprob": -0.16345013609719933, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.005059855990111828}, {"id": 1287, "seek": 525828, "start": 5275.24, "end": 5277.88, "text": " sums to one and are positive numbers just like probabilities", "tokens": [51212, 34499, 281, 472, 293, 366, 3353, 3547, 445, 411, 33783, 51344], "temperature": 0.0, "avg_logprob": -0.16345013609719933, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.005059855990111828}, {"id": 1288, "seek": 525828, "start": 5278.599999999999, "end": 5281.639999999999, "text": " Um, so this is kind of like a normalization function if you want to think of it that way", "tokens": [51380, 3301, 11, 370, 341, 307, 733, 295, 411, 257, 2710, 2144, 2445, 498, 291, 528, 281, 519, 295, 309, 300, 636, 51532], "temperature": 0.0, "avg_logprob": -0.16345013609719933, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.005059855990111828}, {"id": 1289, "seek": 525828, "start": 5282.12, "end": 5285.4, "text": " And you can put it on top of any other linear layer inside a neural net", "tokens": [51556, 400, 291, 393, 829, 309, 322, 1192, 295, 604, 661, 8213, 4583, 1854, 257, 18161, 2533, 51720], "temperature": 0.0, "avg_logprob": -0.16345013609719933, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.005059855990111828}, {"id": 1290, "seek": 528540, "start": 5285.639999999999, "end": 5291.719999999999, "text": " And it basically makes a neural net output probabilities. That's very often used and we used it as well here", "tokens": [50376, 400, 309, 1936, 1669, 257, 18161, 2533, 5598, 33783, 13, 663, 311, 588, 2049, 1143, 293, 321, 1143, 309, 382, 731, 510, 50680], "temperature": 0.0, "avg_logprob": -0.09588364932848059, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0017006393754854798}, {"id": 1291, "seek": 528540, "start": 5293.4, "end": 5296.839999999999, "text": " So this is the forward pass and that's how we made a neural net output probability", "tokens": [50764, 407, 341, 307, 264, 2128, 1320, 293, 300, 311, 577, 321, 1027, 257, 18161, 2533, 5598, 8482, 50936], "temperature": 0.0, "avg_logprob": -0.09588364932848059, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0017006393754854798}, {"id": 1292, "seek": 528540, "start": 5297.96, "end": 5299.48, "text": " now", "tokens": [50992, 586, 51068], "temperature": 0.0, "avg_logprob": -0.09588364932848059, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0017006393754854798}, {"id": 1293, "seek": 528540, "start": 5299.48, "end": 5301.48, "text": " you'll notice that um", "tokens": [51068, 291, 603, 3449, 300, 1105, 51168], "temperature": 0.0, "avg_logprob": -0.09588364932848059, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0017006393754854798}, {"id": 1294, "seek": 528540, "start": 5303.0, "end": 5306.599999999999, "text": " All of these this entire forward pass is made up of differentiable", "tokens": [51244, 1057, 295, 613, 341, 2302, 2128, 1320, 307, 1027, 493, 295, 819, 9364, 51424], "temperature": 0.0, "avg_logprob": -0.09588364932848059, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0017006393754854798}, {"id": 1295, "seek": 528540, "start": 5307.32, "end": 5312.599999999999, "text": " Layers everything here we can back propagate through and we saw some of the back propagation in micrograd", "tokens": [51460, 20084, 433, 1203, 510, 321, 393, 646, 48256, 807, 293, 321, 1866, 512, 295, 264, 646, 38377, 294, 4532, 7165, 51724], "temperature": 0.0, "avg_logprob": -0.09588364932848059, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0017006393754854798}, {"id": 1296, "seek": 531260, "start": 5313.240000000001, "end": 5315.240000000001, "text": " This is just", "tokens": [50396, 639, 307, 445, 50496], "temperature": 0.0, "avg_logprob": -0.18425218794080947, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.0037069569807499647}, {"id": 1297, "seek": 531260, "start": 5315.320000000001, "end": 5317.320000000001, "text": " Multiplication and addition all that's happening here", "tokens": [50500, 29238, 4770, 399, 293, 4500, 439, 300, 311, 2737, 510, 50600], "temperature": 0.0, "avg_logprob": -0.18425218794080947, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.0037069569807499647}, {"id": 1298, "seek": 531260, "start": 5317.320000000001, "end": 5320.120000000001, "text": " Just multiply and then add and we know how to back propagate through them", "tokens": [50600, 1449, 12972, 293, 550, 909, 293, 321, 458, 577, 281, 646, 48256, 807, 552, 50740], "temperature": 0.0, "avg_logprob": -0.18425218794080947, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.0037069569807499647}, {"id": 1299, "seek": 531260, "start": 5320.92, "end": 5322.92, "text": " Exponentiations we know how to back propagate through", "tokens": [50780, 21391, 266, 23012, 763, 321, 458, 577, 281, 646, 48256, 807, 50880], "temperature": 0.0, "avg_logprob": -0.18425218794080947, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.0037069569807499647}, {"id": 1300, "seek": 531260, "start": 5323.88, "end": 5329.4800000000005, "text": " And then here we are summing and sum is is easily back propagated well as well", "tokens": [50928, 400, 550, 510, 321, 366, 2408, 2810, 293, 2408, 307, 307, 3612, 646, 12425, 770, 731, 382, 731, 51208], "temperature": 0.0, "avg_logprob": -0.18425218794080947, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.0037069569807499647}, {"id": 1301, "seek": 531260, "start": 5330.120000000001, "end": 5333.8, "text": " And division as well. So everything here is differentiable operation", "tokens": [51240, 400, 10044, 382, 731, 13, 407, 1203, 510, 307, 819, 9364, 6916, 51424], "temperature": 0.0, "avg_logprob": -0.18425218794080947, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.0037069569807499647}, {"id": 1302, "seek": 531260, "start": 5334.6, "end": 5336.6, "text": " And we can back propagate through", "tokens": [51464, 400, 321, 393, 646, 48256, 807, 51564], "temperature": 0.0, "avg_logprob": -0.18425218794080947, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.0037069569807499647}, {"id": 1303, "seek": 533660, "start": 5337.56, "end": 5341.0, "text": " Now we achieve these probabilities which are 5 by 27", "tokens": [50412, 823, 321, 4584, 613, 33783, 597, 366, 1025, 538, 7634, 50584], "temperature": 0.0, "avg_logprob": -0.17829601388228566, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0029346856754273176}, {"id": 1304, "seek": 533660, "start": 5341.64, "end": 5345.4800000000005, "text": " For every single example, we have a vector of probabilities that sum to 1", "tokens": [50616, 1171, 633, 2167, 1365, 11, 321, 362, 257, 8062, 295, 33783, 300, 2408, 281, 502, 50808], "temperature": 0.0, "avg_logprob": -0.17829601388228566, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0029346856754273176}, {"id": 1305, "seek": 533660, "start": 5346.360000000001, "end": 5349.64, "text": " And then here I wrote a bunch of stuff to sort of like break down", "tokens": [50852, 400, 550, 510, 286, 4114, 257, 3840, 295, 1507, 281, 1333, 295, 411, 1821, 760, 51016], "temperature": 0.0, "avg_logprob": -0.17829601388228566, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0029346856754273176}, {"id": 1306, "seek": 533660, "start": 5350.280000000001, "end": 5354.68, "text": " The examples so we have five examples making up emma, right?", "tokens": [51048, 440, 5110, 370, 321, 362, 1732, 5110, 1455, 493, 846, 1696, 11, 558, 30, 51268], "temperature": 0.0, "avg_logprob": -0.17829601388228566, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0029346856754273176}, {"id": 1307, "seek": 533660, "start": 5356.360000000001, "end": 5358.92, "text": " And there are five bigrams inside emma", "tokens": [51352, 400, 456, 366, 1732, 955, 2356, 82, 1854, 846, 1696, 51480], "temperature": 0.0, "avg_logprob": -0.17829601388228566, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.0029346856754273176}, {"id": 1308, "seek": 535892, "start": 5359.0, "end": 5366.92, "text": " So by gram example a by gram example one is that e is the beginning character right after dot", "tokens": [50368, 407, 538, 21353, 1365, 257, 538, 21353, 1365, 472, 307, 300, 308, 307, 264, 2863, 2517, 558, 934, 5893, 50764], "temperature": 0.0, "avg_logprob": -0.14444302377246676, "compression_ratio": 1.715, "no_speech_prob": 0.010986188426613808}, {"id": 1309, "seek": 535892, "start": 5368.36, "end": 5373.24, "text": " And the indexes for these are zero and five. So then we feed in a zero", "tokens": [50836, 400, 264, 8186, 279, 337, 613, 366, 4018, 293, 1732, 13, 407, 550, 321, 3154, 294, 257, 4018, 51080], "temperature": 0.0, "avg_logprob": -0.14444302377246676, "compression_ratio": 1.715, "no_speech_prob": 0.010986188426613808}, {"id": 1310, "seek": 535892, "start": 5374.12, "end": 5379.88, "text": " That's the input to the neural net we get probabilities from the neural net that are 27 numbers", "tokens": [51124, 663, 311, 264, 4846, 281, 264, 18161, 2533, 321, 483, 33783, 490, 264, 18161, 2533, 300, 366, 7634, 3547, 51412], "temperature": 0.0, "avg_logprob": -0.14444302377246676, "compression_ratio": 1.715, "no_speech_prob": 0.010986188426613808}, {"id": 1311, "seek": 535892, "start": 5381.32, "end": 5386.76, "text": " And then the label is five because e actually comes after dot. So that's the label", "tokens": [51484, 400, 550, 264, 7645, 307, 1732, 570, 308, 767, 1487, 934, 5893, 13, 407, 300, 311, 264, 7645, 51756], "temperature": 0.0, "avg_logprob": -0.14444302377246676, "compression_ratio": 1.715, "no_speech_prob": 0.010986188426613808}, {"id": 1312, "seek": 538676, "start": 5387.72, "end": 5389.24, "text": " And then", "tokens": [50412, 400, 550, 50488], "temperature": 0.0, "avg_logprob": -0.10819040186264936, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0012842488940805197}, {"id": 1313, "seek": 538676, "start": 5389.24, "end": 5393.64, "text": " We use this label five to index into the probability distribution here", "tokens": [50488, 492, 764, 341, 7645, 1732, 281, 8186, 666, 264, 8482, 7316, 510, 50708], "temperature": 0.0, "avg_logprob": -0.10819040186264936, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0012842488940805197}, {"id": 1314, "seek": 538676, "start": 5394.360000000001, "end": 5400.52, "text": " So this index five here is zero one two three four five. It's this number here", "tokens": [50744, 407, 341, 8186, 1732, 510, 307, 4018, 472, 732, 1045, 1451, 1732, 13, 467, 311, 341, 1230, 510, 51052], "temperature": 0.0, "avg_logprob": -0.10819040186264936, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0012842488940805197}, {"id": 1315, "seek": 538676, "start": 5401.24, "end": 5403.24, "text": " Which is here", "tokens": [51088, 3013, 307, 510, 51188], "temperature": 0.0, "avg_logprob": -0.10819040186264936, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0012842488940805197}, {"id": 1316, "seek": 538676, "start": 5404.04, "end": 5408.04, "text": " So that's basically the probability assigned by the neural net to the actual correct character", "tokens": [51228, 407, 300, 311, 1936, 264, 8482, 13279, 538, 264, 18161, 2533, 281, 264, 3539, 3006, 2517, 51428], "temperature": 0.0, "avg_logprob": -0.10819040186264936, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0012842488940805197}, {"id": 1317, "seek": 538676, "start": 5408.76, "end": 5414.76, "text": " You see that the network currently thinks that this next character that e following dot is only 1% likely", "tokens": [51464, 509, 536, 300, 264, 3209, 4362, 7309, 300, 341, 958, 2517, 300, 308, 3480, 5893, 307, 787, 502, 4, 3700, 51764], "temperature": 0.0, "avg_logprob": -0.10819040186264936, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0012842488940805197}, {"id": 1318, "seek": 541476, "start": 5415.320000000001, "end": 5417.0, "text": " Which is of course not very good, right?", "tokens": [50392, 3013, 307, 295, 1164, 406, 588, 665, 11, 558, 30, 50476], "temperature": 0.0, "avg_logprob": -0.11665350325564121, "compression_ratio": 1.7652173913043478, "no_speech_prob": 0.009266241453588009}, {"id": 1319, "seek": 541476, "start": 5417.0, "end": 5422.12, "text": " Because this actually is a training example and the network thinks that this is currently very very unlikely", "tokens": [50476, 1436, 341, 767, 307, 257, 3097, 1365, 293, 264, 3209, 7309, 300, 341, 307, 4362, 588, 588, 17518, 50732], "temperature": 0.0, "avg_logprob": -0.11665350325564121, "compression_ratio": 1.7652173913043478, "no_speech_prob": 0.009266241453588009}, {"id": 1320, "seek": 541476, "start": 5422.4400000000005, "end": 5426.68, "text": " But that's just because we didn't get very lucky in generating a good setting of w", "tokens": [50748, 583, 300, 311, 445, 570, 321, 994, 380, 483, 588, 6356, 294, 17746, 257, 665, 3287, 295, 261, 50960], "temperature": 0.0, "avg_logprob": -0.11665350325564121, "compression_ratio": 1.7652173913043478, "no_speech_prob": 0.009266241453588009}, {"id": 1321, "seek": 541476, "start": 5427.08, "end": 5431.08, "text": " So right now this network thinks this is unlikely and 0.01 is not a good outcome", "tokens": [50980, 407, 558, 586, 341, 3209, 7309, 341, 307, 17518, 293, 1958, 13, 10607, 307, 406, 257, 665, 9700, 51180], "temperature": 0.0, "avg_logprob": -0.11665350325564121, "compression_ratio": 1.7652173913043478, "no_speech_prob": 0.009266241453588009}, {"id": 1322, "seek": 541476, "start": 5431.96, "end": 5433.96, "text": " So the log likelihood then", "tokens": [51224, 407, 264, 3565, 22119, 550, 51324], "temperature": 0.0, "avg_logprob": -0.11665350325564121, "compression_ratio": 1.7652173913043478, "no_speech_prob": 0.009266241453588009}, {"id": 1323, "seek": 541476, "start": 5434.360000000001, "end": 5438.76, "text": " Is very negative and the negative log likelihood is very positive", "tokens": [51344, 1119, 588, 3671, 293, 264, 3671, 3565, 22119, 307, 588, 3353, 51564], "temperature": 0.0, "avg_logprob": -0.11665350325564121, "compression_ratio": 1.7652173913043478, "no_speech_prob": 0.009266241453588009}, {"id": 1324, "seek": 543876, "start": 5439.400000000001, "end": 5441.400000000001, "text": " And so four is a very high", "tokens": [50396, 400, 370, 1451, 307, 257, 588, 1090, 50496], "temperature": 0.0, "avg_logprob": -0.11581935504875561, "compression_ratio": 1.8450704225352113, "no_speech_prob": 0.0030277089681476355}, {"id": 1325, "seek": 543876, "start": 5441.88, "end": 5443.16, "text": " negative log likelihood", "tokens": [50520, 3671, 3565, 22119, 50584], "temperature": 0.0, "avg_logprob": -0.11581935504875561, "compression_ratio": 1.8450704225352113, "no_speech_prob": 0.0030277089681476355}, {"id": 1326, "seek": 543876, "start": 5443.16, "end": 5445.16, "text": " And that means we're going to have a high loss", "tokens": [50584, 400, 300, 1355, 321, 434, 516, 281, 362, 257, 1090, 4470, 50684], "temperature": 0.0, "avg_logprob": -0.11581935504875561, "compression_ratio": 1.8450704225352113, "no_speech_prob": 0.0030277089681476355}, {"id": 1327, "seek": 543876, "start": 5445.320000000001, "end": 5449.72, "text": " Because what is the loss the loss is just the average negative log likelihood", "tokens": [50692, 1436, 437, 307, 264, 4470, 264, 4470, 307, 445, 264, 4274, 3671, 3565, 22119, 50912], "temperature": 0.0, "avg_logprob": -0.11581935504875561, "compression_ratio": 1.8450704225352113, "no_speech_prob": 0.0030277089681476355}, {"id": 1328, "seek": 543876, "start": 5451.64, "end": 5453.64, "text": " So the second character is em", "tokens": [51008, 407, 264, 1150, 2517, 307, 846, 51108], "temperature": 0.0, "avg_logprob": -0.11581935504875561, "compression_ratio": 1.8450704225352113, "no_speech_prob": 0.0030277089681476355}, {"id": 1329, "seek": 543876, "start": 5453.64, "end": 5458.84, "text": " And you see here that also the network thought that m following e is very unlikely 1%", "tokens": [51108, 400, 291, 536, 510, 300, 611, 264, 3209, 1194, 300, 275, 3480, 308, 307, 588, 17518, 502, 4, 51368], "temperature": 0.0, "avg_logprob": -0.11581935504875561, "compression_ratio": 1.8450704225352113, "no_speech_prob": 0.0030277089681476355}, {"id": 1330, "seek": 543876, "start": 5461.0, "end": 5463.64, "text": " The for m following m it thought it was 2%", "tokens": [51476, 440, 337, 275, 3480, 275, 309, 1194, 309, 390, 568, 4, 51608], "temperature": 0.0, "avg_logprob": -0.11581935504875561, "compression_ratio": 1.8450704225352113, "no_speech_prob": 0.0030277089681476355}, {"id": 1331, "seek": 543876, "start": 5464.280000000001, "end": 5467.4800000000005, "text": " And for a following m it actually thought it was 7% likely", "tokens": [51640, 400, 337, 257, 3480, 275, 309, 767, 1194, 309, 390, 1614, 4, 3700, 51800], "temperature": 0.0, "avg_logprob": -0.11581935504875561, "compression_ratio": 1.8450704225352113, "no_speech_prob": 0.0030277089681476355}, {"id": 1332, "seek": 546748, "start": 5467.879999999999, "end": 5474.44, "text": " So just by chance this one actually has a pretty good probability and therefore a pretty low negative log likelihood", "tokens": [50384, 407, 445, 538, 2931, 341, 472, 767, 575, 257, 1238, 665, 8482, 293, 4412, 257, 1238, 2295, 3671, 3565, 22119, 50712], "temperature": 0.0, "avg_logprob": -0.12302679385779039, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.0025112873408943415}, {"id": 1333, "seek": 546748, "start": 5475.32, "end": 5477.719999999999, "text": " And finally here it thought this was 1% likely", "tokens": [50756, 400, 2721, 510, 309, 1194, 341, 390, 502, 4, 3700, 50876], "temperature": 0.0, "avg_logprob": -0.12302679385779039, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.0025112873408943415}, {"id": 1334, "seek": 546748, "start": 5478.36, "end": 5484.2, "text": " So overall our average negative log likelihood, which is the loss the total loss that summarizes", "tokens": [50908, 407, 4787, 527, 4274, 3671, 3565, 22119, 11, 597, 307, 264, 4470, 264, 3217, 4470, 300, 14611, 5660, 51200], "temperature": 0.0, "avg_logprob": -0.12302679385779039, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.0025112873408943415}, {"id": 1335, "seek": 546748, "start": 5484.759999999999, "end": 5489.48, "text": " Basically the how well this network currently works at least on this one word not on the full data", "tokens": [51228, 8537, 264, 577, 731, 341, 3209, 4362, 1985, 412, 1935, 322, 341, 472, 1349, 406, 322, 264, 1577, 1412, 51464], "temperature": 0.0, "avg_logprob": -0.12302679385779039, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.0025112873408943415}, {"id": 1336, "seek": 546748, "start": 5489.5599999999995, "end": 5495.959999999999, "text": " So just the one word is 3.76 which is actually very fairly high loss. This is not a very good setting of w's", "tokens": [51468, 407, 445, 264, 472, 1349, 307, 805, 13, 25026, 597, 307, 767, 588, 6457, 1090, 4470, 13, 639, 307, 406, 257, 588, 665, 3287, 295, 261, 311, 51788], "temperature": 0.0, "avg_logprob": -0.12302679385779039, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.0025112873408943415}, {"id": 1337, "seek": 549596, "start": 5496.84, "end": 5498.6, "text": " Now here's what we can do", "tokens": [50408, 823, 510, 311, 437, 321, 393, 360, 50496], "temperature": 0.0, "avg_logprob": -0.08008240801947457, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.027165815234184265}, {"id": 1338, "seek": 549596, "start": 5498.6, "end": 5500.6, "text": " We're currently getting 3.76", "tokens": [50496, 492, 434, 4362, 1242, 805, 13, 25026, 50596], "temperature": 0.0, "avg_logprob": -0.08008240801947457, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.027165815234184265}, {"id": 1339, "seek": 549596, "start": 5501.32, "end": 5505.24, "text": " We can actually come here and we can change our w we can resample it", "tokens": [50632, 492, 393, 767, 808, 510, 293, 321, 393, 1319, 527, 261, 321, 393, 725, 335, 781, 309, 50828], "temperature": 0.0, "avg_logprob": -0.08008240801947457, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.027165815234184265}, {"id": 1340, "seek": 549596, "start": 5505.64, "end": 5507.96, "text": " So let me just add one to have a different seed", "tokens": [50848, 407, 718, 385, 445, 909, 472, 281, 362, 257, 819, 8871, 50964], "temperature": 0.0, "avg_logprob": -0.08008240801947457, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.027165815234184265}, {"id": 1341, "seek": 549596, "start": 5508.76, "end": 5511.8, "text": " And then we get a different w and then we can rerun this", "tokens": [51004, 400, 550, 321, 483, 257, 819, 261, 293, 550, 321, 393, 43819, 409, 341, 51156], "temperature": 0.0, "avg_logprob": -0.08008240801947457, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.027165815234184265}, {"id": 1342, "seek": 549596, "start": 5512.84, "end": 5517.8, "text": " And with this different seed with this different setting of w's we now get 3.37", "tokens": [51208, 400, 365, 341, 819, 8871, 365, 341, 819, 3287, 295, 261, 311, 321, 586, 483, 805, 13, 12851, 51456], "temperature": 0.0, "avg_logprob": -0.08008240801947457, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.027165815234184265}, {"id": 1343, "seek": 549596, "start": 5518.6, "end": 5523.96, "text": " So this is a much better w right and that and it's better because the probabilities just happen to come out", "tokens": [51496, 407, 341, 307, 257, 709, 1101, 261, 558, 293, 300, 293, 309, 311, 1101, 570, 264, 33783, 445, 1051, 281, 808, 484, 51764], "temperature": 0.0, "avg_logprob": -0.08008240801947457, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.027165815234184265}, {"id": 1344, "seek": 552396, "start": 5524.76, "end": 5527.88, "text": " Higher for the for the characters that actually are next", "tokens": [50404, 31997, 337, 264, 337, 264, 4342, 300, 767, 366, 958, 50560], "temperature": 0.0, "avg_logprob": -0.12450786020563936, "compression_ratio": 1.5477386934673367, "no_speech_prob": 0.002631605602800846}, {"id": 1345, "seek": 552396, "start": 5528.84, "end": 5532.68, "text": " And so you can imagine actually just resampling this, you know, we can try as two", "tokens": [50608, 400, 370, 291, 393, 3811, 767, 445, 725, 335, 11970, 341, 11, 291, 458, 11, 321, 393, 853, 382, 732, 50800], "temperature": 0.0, "avg_logprob": -0.12450786020563936, "compression_ratio": 1.5477386934673367, "no_speech_prob": 0.002631605602800846}, {"id": 1346, "seek": 552396, "start": 5534.36, "end": 5535.56, "text": " So", "tokens": [50884, 407, 50944], "temperature": 0.0, "avg_logprob": -0.12450786020563936, "compression_ratio": 1.5477386934673367, "no_speech_prob": 0.002631605602800846}, {"id": 1347, "seek": 552396, "start": 5535.56, "end": 5539.32, "text": " Okay, this was not very good. Let's try one more. We can try three", "tokens": [50944, 1033, 11, 341, 390, 406, 588, 665, 13, 961, 311, 853, 472, 544, 13, 492, 393, 853, 1045, 51132], "temperature": 0.0, "avg_logprob": -0.12450786020563936, "compression_ratio": 1.5477386934673367, "no_speech_prob": 0.002631605602800846}, {"id": 1348, "seek": 552396, "start": 5540.92, "end": 5544.04, "text": " Okay, this was terrible setting because we have a very high loss", "tokens": [51212, 1033, 11, 341, 390, 6237, 3287, 570, 321, 362, 257, 588, 1090, 4470, 51368], "temperature": 0.0, "avg_logprob": -0.12450786020563936, "compression_ratio": 1.5477386934673367, "no_speech_prob": 0.002631605602800846}, {"id": 1349, "seek": 552396, "start": 5544.76, "end": 5546.12, "text": " so", "tokens": [51404, 370, 51472], "temperature": 0.0, "avg_logprob": -0.12450786020563936, "compression_ratio": 1.5477386934673367, "no_speech_prob": 0.002631605602800846}, {"id": 1350, "seek": 552396, "start": 5546.12, "end": 5548.12, "text": " Anyway, I'm going to erase this", "tokens": [51472, 5684, 11, 286, 478, 516, 281, 23525, 341, 51572], "temperature": 0.0, "avg_logprob": -0.12450786020563936, "compression_ratio": 1.5477386934673367, "no_speech_prob": 0.002631605602800846}, {"id": 1351, "seek": 554812, "start": 5548.68, "end": 5554.92, "text": " What I'm doing here, which is just guess and check of randomly assigning parameters and seeing if the network is good", "tokens": [50392, 708, 286, 478, 884, 510, 11, 597, 307, 445, 2041, 293, 1520, 295, 16979, 49602, 9834, 293, 2577, 498, 264, 3209, 307, 665, 50704], "temperature": 0.0, "avg_logprob": -0.16534385034593485, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0017821117071434855}, {"id": 1352, "seek": 554812, "start": 5555.48, "end": 5558.76, "text": " That is a amateur hour. That's not how you optimize a neural net", "tokens": [50732, 663, 307, 257, 29339, 1773, 13, 663, 311, 406, 577, 291, 19719, 257, 18161, 2533, 50896], "temperature": 0.0, "avg_logprob": -0.16534385034593485, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0017821117071434855}, {"id": 1353, "seek": 554812, "start": 5559.08, "end": 5563.48, "text": " The way you optimize your neural net is you start with some random guess and we're going to commit to this one", "tokens": [50912, 440, 636, 291, 19719, 428, 18161, 2533, 307, 291, 722, 365, 512, 4974, 2041, 293, 321, 434, 516, 281, 5599, 281, 341, 472, 51132], "temperature": 0.0, "avg_logprob": -0.16534385034593485, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0017821117071434855}, {"id": 1354, "seek": 554812, "start": 5563.5599999999995, "end": 5565.16, "text": " Even though it's not very good", "tokens": [51136, 2754, 1673, 309, 311, 406, 588, 665, 51216], "temperature": 0.0, "avg_logprob": -0.16534385034593485, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0017821117071434855}, {"id": 1355, "seek": 554812, "start": 5565.16, "end": 5567.32, "text": " But now the big deal is we have a loss function", "tokens": [51216, 583, 586, 264, 955, 2028, 307, 321, 362, 257, 4470, 2445, 51324], "temperature": 0.0, "avg_logprob": -0.16534385034593485, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0017821117071434855}, {"id": 1356, "seek": 554812, "start": 5568.36, "end": 5570.04, "text": " So this loss", "tokens": [51376, 407, 341, 4470, 51460], "temperature": 0.0, "avg_logprob": -0.16534385034593485, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0017821117071434855}, {"id": 1357, "seek": 554812, "start": 5570.04, "end": 5572.04, "text": " Is made up only of differentiable", "tokens": [51460, 1119, 1027, 493, 787, 295, 819, 9364, 51560], "temperature": 0.0, "avg_logprob": -0.16534385034593485, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0017821117071434855}, {"id": 1358, "seek": 554812, "start": 5572.599999999999, "end": 5574.28, "text": " operations", "tokens": [51588, 7705, 51672], "temperature": 0.0, "avg_logprob": -0.16534385034593485, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0017821117071434855}, {"id": 1359, "seek": 554812, "start": 5574.28, "end": 5577.08, "text": " And we can minimize the loss by tuning", "tokens": [51672, 400, 321, 393, 17522, 264, 4470, 538, 15164, 51812], "temperature": 0.0, "avg_logprob": -0.16534385034593485, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0017821117071434855}, {"id": 1360, "seek": 557708, "start": 5577.64, "end": 5583.72, "text": " W's by computing the gradients of the loss with respect to these w matrices", "tokens": [50392, 343, 311, 538, 15866, 264, 2771, 2448, 295, 264, 4470, 365, 3104, 281, 613, 261, 32284, 50696], "temperature": 0.0, "avg_logprob": -0.13607110033978473, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0013044269289821386}, {"id": 1361, "seek": 557708, "start": 5585.08, "end": 5591.24, "text": " And so then we can tune w to minimize the loss and find a good setting of w using gradient based optimization", "tokens": [50764, 400, 370, 550, 321, 393, 10864, 261, 281, 17522, 264, 4470, 293, 915, 257, 665, 3287, 295, 261, 1228, 16235, 2361, 19618, 51072], "temperature": 0.0, "avg_logprob": -0.13607110033978473, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0013044269289821386}, {"id": 1362, "seek": 557708, "start": 5591.64, "end": 5596.44, "text": " So let's see how that will work now things are actually going to look almost identical to what we had with micrograd", "tokens": [51092, 407, 718, 311, 536, 577, 300, 486, 589, 586, 721, 366, 767, 516, 281, 574, 1920, 14800, 281, 437, 321, 632, 365, 4532, 7165, 51332], "temperature": 0.0, "avg_logprob": -0.13607110033978473, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0013044269289821386}, {"id": 1363, "seek": 557708, "start": 5597.08, "end": 5601.72, "text": " So here I pulled up the lecture from micrograd the notebook", "tokens": [51364, 407, 510, 286, 7373, 493, 264, 7991, 490, 4532, 7165, 264, 21060, 51596], "temperature": 0.0, "avg_logprob": -0.13607110033978473, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0013044269289821386}, {"id": 1364, "seek": 557708, "start": 5602.04, "end": 5603.88, "text": " It's from this repository", "tokens": [51612, 467, 311, 490, 341, 25841, 51704], "temperature": 0.0, "avg_logprob": -0.13607110033978473, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.0013044269289821386}, {"id": 1365, "seek": 560388, "start": 5603.88, "end": 5607.96, "text": " And when I scroll all the way to the end where we left off with micrograd, we had something very very similar", "tokens": [50364, 400, 562, 286, 11369, 439, 264, 636, 281, 264, 917, 689, 321, 1411, 766, 365, 4532, 7165, 11, 321, 632, 746, 588, 588, 2531, 50568], "temperature": 0.0, "avg_logprob": -0.11805300756331978, "compression_ratio": 1.76171875, "no_speech_prob": 0.004609089810401201}, {"id": 1366, "seek": 560388, "start": 5608.6, "end": 5613.56, "text": " We had a number of input examples in this case. We had four input examples inside x's", "tokens": [50600, 492, 632, 257, 1230, 295, 4846, 5110, 294, 341, 1389, 13, 492, 632, 1451, 4846, 5110, 1854, 2031, 311, 50848], "temperature": 0.0, "avg_logprob": -0.11805300756331978, "compression_ratio": 1.76171875, "no_speech_prob": 0.004609089810401201}, {"id": 1367, "seek": 560388, "start": 5614.2, "end": 5616.2, "text": " And we had their targets", "tokens": [50880, 400, 321, 632, 641, 12911, 50980], "temperature": 0.0, "avg_logprob": -0.11805300756331978, "compression_ratio": 1.76171875, "no_speech_prob": 0.004609089810401201}, {"id": 1368, "seek": 560388, "start": 5616.28, "end": 5617.72, "text": " desired targets", "tokens": [50984, 14721, 12911, 51056], "temperature": 0.0, "avg_logprob": -0.11805300756331978, "compression_ratio": 1.76171875, "no_speech_prob": 0.004609089810401201}, {"id": 1369, "seek": 560388, "start": 5617.72, "end": 5623.400000000001, "text": " Just like here we have our x's now, but we have five of them and they're now integers instead of vectors", "tokens": [51056, 1449, 411, 510, 321, 362, 527, 2031, 311, 586, 11, 457, 321, 362, 1732, 295, 552, 293, 436, 434, 586, 41674, 2602, 295, 18875, 51340], "temperature": 0.0, "avg_logprob": -0.11805300756331978, "compression_ratio": 1.76171875, "no_speech_prob": 0.004609089810401201}, {"id": 1370, "seek": 560388, "start": 5624.12, "end": 5630.6, "text": " But we're going to convert our integers to vectors except our vectors will be 27 large instead of three large", "tokens": [51376, 583, 321, 434, 516, 281, 7620, 527, 41674, 281, 18875, 3993, 527, 18875, 486, 312, 7634, 2416, 2602, 295, 1045, 2416, 51700], "temperature": 0.0, "avg_logprob": -0.11805300756331978, "compression_ratio": 1.76171875, "no_speech_prob": 0.004609089810401201}, {"id": 1371, "seek": 563060, "start": 5630.84, "end": 5637.08, "text": " And then here what we did is first we did a forward pass where we ran a neural net on all of the inputs", "tokens": [50376, 400, 550, 510, 437, 321, 630, 307, 700, 321, 630, 257, 2128, 1320, 689, 321, 5872, 257, 18161, 2533, 322, 439, 295, 264, 15743, 50688], "temperature": 0.0, "avg_logprob": -0.13478763451736964, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.001264379359781742}, {"id": 1372, "seek": 563060, "start": 5638.360000000001, "end": 5640.360000000001, "text": " to get predictions", "tokens": [50752, 281, 483, 21264, 50852], "temperature": 0.0, "avg_logprob": -0.13478763451736964, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.001264379359781742}, {"id": 1373, "seek": 563060, "start": 5640.360000000001, "end": 5644.4400000000005, "text": " Our neural net at the time this n of x was a multilayer perceptron", "tokens": [50852, 2621, 18161, 2533, 412, 264, 565, 341, 297, 295, 2031, 390, 257, 2120, 388, 11167, 43276, 2044, 51056], "temperature": 0.0, "avg_logprob": -0.13478763451736964, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.001264379359781742}, {"id": 1374, "seek": 563060, "start": 5645.160000000001, "end": 5649.400000000001, "text": " Our neural net is going to look different because our neural net is just a single layer", "tokens": [51092, 2621, 18161, 2533, 307, 516, 281, 574, 819, 570, 527, 18161, 2533, 307, 445, 257, 2167, 4583, 51304], "temperature": 0.0, "avg_logprob": -0.13478763451736964, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.001264379359781742}, {"id": 1375, "seek": 563060, "start": 5650.52, "end": 5652.84, "text": " Single linear layer followed by a softmax", "tokens": [51360, 31248, 8213, 4583, 6263, 538, 257, 2787, 41167, 51476], "temperature": 0.0, "avg_logprob": -0.13478763451736964, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.001264379359781742}, {"id": 1376, "seek": 563060, "start": 5653.8, "end": 5655.8, "text": " So that's our neural net", "tokens": [51524, 407, 300, 311, 527, 18161, 2533, 51624], "temperature": 0.0, "avg_logprob": -0.13478763451736964, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.001264379359781742}, {"id": 1377, "seek": 565580, "start": 5655.88, "end": 5658.04, "text": " And the loss here was the mean squared error", "tokens": [50368, 400, 264, 4470, 510, 390, 264, 914, 8889, 6713, 50476], "temperature": 0.0, "avg_logprob": -0.0973769608918611, "compression_ratio": 2.0201612903225805, "no_speech_prob": 0.0020188784692436457}, {"id": 1378, "seek": 565580, "start": 5658.360000000001, "end": 5662.6, "text": " So we simply subtracted the prediction from the ground truth and squared it and summed it all up", "tokens": [50492, 407, 321, 2935, 16390, 292, 264, 17630, 490, 264, 2727, 3494, 293, 8889, 309, 293, 2408, 1912, 309, 439, 493, 50704], "temperature": 0.0, "avg_logprob": -0.0973769608918611, "compression_ratio": 2.0201612903225805, "no_speech_prob": 0.0020188784692436457}, {"id": 1379, "seek": 565580, "start": 5662.92, "end": 5668.2, "text": " And that was the loss and loss was the single number that summarized the quality of the neural net", "tokens": [50720, 400, 300, 390, 264, 4470, 293, 4470, 390, 264, 2167, 1230, 300, 14611, 1602, 264, 3125, 295, 264, 18161, 2533, 50984], "temperature": 0.0, "avg_logprob": -0.0973769608918611, "compression_ratio": 2.0201612903225805, "no_speech_prob": 0.0020188784692436457}, {"id": 1380, "seek": 565580, "start": 5668.52, "end": 5674.84, "text": " And when loss is low like almost zero that means the neural net is um predicting correctly", "tokens": [51000, 400, 562, 4470, 307, 2295, 411, 1920, 4018, 300, 1355, 264, 18161, 2533, 307, 1105, 32884, 8944, 51316], "temperature": 0.0, "avg_logprob": -0.0973769608918611, "compression_ratio": 2.0201612903225805, "no_speech_prob": 0.0020188784692436457}, {"id": 1381, "seek": 565580, "start": 5676.28, "end": 5681.4800000000005, "text": " So we had a single number that uh that summarized the uh the performance of the neural net", "tokens": [51388, 407, 321, 632, 257, 2167, 1230, 300, 2232, 300, 14611, 1602, 264, 2232, 264, 3389, 295, 264, 18161, 2533, 51648], "temperature": 0.0, "avg_logprob": -0.0973769608918611, "compression_ratio": 2.0201612903225805, "no_speech_prob": 0.0020188784692436457}, {"id": 1382, "seek": 565580, "start": 5682.04, "end": 5685.400000000001, "text": " And everything here was differentiable and was stored in massive compute graph", "tokens": [51676, 400, 1203, 510, 390, 819, 9364, 293, 390, 12187, 294, 5994, 14722, 4295, 51844], "temperature": 0.0, "avg_logprob": -0.0973769608918611, "compression_ratio": 2.0201612903225805, "no_speech_prob": 0.0020188784692436457}, {"id": 1383, "seek": 568580, "start": 5686.76, "end": 5689.16, "text": " And then we iterated over all the parameters", "tokens": [50412, 400, 550, 321, 17138, 770, 670, 439, 264, 9834, 50532], "temperature": 0.0, "avg_logprob": -0.12726284759213227, "compression_ratio": 1.8511627906976744, "no_speech_prob": 0.0003981995105277747}, {"id": 1384, "seek": 568580, "start": 5689.16, "end": 5693.24, "text": " We made sure that the gradients are set to zero and we called loss.backward", "tokens": [50532, 492, 1027, 988, 300, 264, 2771, 2448, 366, 992, 281, 4018, 293, 321, 1219, 4470, 13, 3207, 1007, 50736], "temperature": 0.0, "avg_logprob": -0.12726284759213227, "compression_ratio": 1.8511627906976744, "no_speech_prob": 0.0003981995105277747}, {"id": 1385, "seek": 568580, "start": 5694.12, "end": 5699.8, "text": " And loss.backward initiated back propagation at the final output node of loss, right? So", "tokens": [50780, 400, 4470, 13, 3207, 1007, 28578, 646, 38377, 412, 264, 2572, 5598, 9984, 295, 4470, 11, 558, 30, 407, 51064], "temperature": 0.0, "avg_logprob": -0.12726284759213227, "compression_ratio": 1.8511627906976744, "no_speech_prob": 0.0003981995105277747}, {"id": 1386, "seek": 568580, "start": 5700.84, "end": 5703.320000000001, "text": " Yeah, I remember these expressions. We had loss all the way at the end", "tokens": [51116, 865, 11, 286, 1604, 613, 15277, 13, 492, 632, 4470, 439, 264, 636, 412, 264, 917, 51240], "temperature": 0.0, "avg_logprob": -0.12726284759213227, "compression_ratio": 1.8511627906976744, "no_speech_prob": 0.0003981995105277747}, {"id": 1387, "seek": 568580, "start": 5703.56, "end": 5705.8, "text": " We start back propagation and we went all the way back", "tokens": [51252, 492, 722, 646, 38377, 293, 321, 1437, 439, 264, 636, 646, 51364], "temperature": 0.0, "avg_logprob": -0.12726284759213227, "compression_ratio": 1.8511627906976744, "no_speech_prob": 0.0003981995105277747}, {"id": 1388, "seek": 568580, "start": 5706.360000000001, "end": 5710.12, "text": " And we made sure that we populated all the parameters dot grad", "tokens": [51392, 400, 321, 1027, 988, 300, 321, 32998, 439, 264, 9834, 5893, 2771, 51580], "temperature": 0.0, "avg_logprob": -0.12726284759213227, "compression_ratio": 1.8511627906976744, "no_speech_prob": 0.0003981995105277747}, {"id": 1389, "seek": 571012, "start": 5710.76, "end": 5713.96, "text": " So dot grad started at zero but back propagation filled it in", "tokens": [50396, 407, 5893, 2771, 1409, 412, 4018, 457, 646, 38377, 6412, 309, 294, 50556], "temperature": 0.0, "avg_logprob": -0.10861340234445971, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.010488341562449932}, {"id": 1390, "seek": 571012, "start": 5714.5199999999995, "end": 5720.76, "text": " And then in the update we iterated over all the parameters and we simply did a parameter update where every single", "tokens": [50584, 400, 550, 294, 264, 5623, 321, 17138, 770, 670, 439, 264, 9834, 293, 321, 2935, 630, 257, 13075, 5623, 689, 633, 2167, 50896], "temperature": 0.0, "avg_logprob": -0.10861340234445971, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.010488341562449932}, {"id": 1391, "seek": 571012, "start": 5721.48, "end": 5726.44, "text": " element of our parameters was nudged in the opposite direction of the gradient", "tokens": [50932, 4478, 295, 527, 9834, 390, 40045, 3004, 294, 264, 6182, 3513, 295, 264, 16235, 51180], "temperature": 0.0, "avg_logprob": -0.10861340234445971, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.010488341562449932}, {"id": 1392, "seek": 571012, "start": 5727.5599999999995, "end": 5730.84, "text": " And so we're going to do the exact same thing here", "tokens": [51236, 400, 370, 321, 434, 516, 281, 360, 264, 1900, 912, 551, 510, 51400], "temperature": 0.0, "avg_logprob": -0.10861340234445971, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.010488341562449932}, {"id": 1393, "seek": 571012, "start": 5731.72, "end": 5733.72, "text": " So i'm going to pull this up", "tokens": [51444, 407, 741, 478, 516, 281, 2235, 341, 493, 51544], "temperature": 0.0, "avg_logprob": -0.10861340234445971, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.010488341562449932}, {"id": 1394, "seek": 571012, "start": 5734.44, "end": 5736.44, "text": " On the side here", "tokens": [51580, 1282, 264, 1252, 510, 51680], "temperature": 0.0, "avg_logprob": -0.10861340234445971, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.010488341562449932}, {"id": 1395, "seek": 573644, "start": 5737.4, "end": 5741.5599999999995, "text": " So that we have it available and we're actually going to do the exact same thing", "tokens": [50412, 407, 300, 321, 362, 309, 2435, 293, 321, 434, 767, 516, 281, 360, 264, 1900, 912, 551, 50620], "temperature": 0.0, "avg_logprob": -0.13453394848367442, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0007672571227885783}, {"id": 1396, "seek": 573644, "start": 5742.12, "end": 5745.719999999999, "text": " So this was the forward pass. So we're we did this", "tokens": [50648, 407, 341, 390, 264, 2128, 1320, 13, 407, 321, 434, 321, 630, 341, 50828], "temperature": 0.0, "avg_logprob": -0.13453394848367442, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0007672571227885783}, {"id": 1397, "seek": 573644, "start": 5746.919999999999, "end": 5748.919999999999, "text": " And probes is our y-pred", "tokens": [50888, 400, 1239, 279, 307, 527, 288, 12, 79, 986, 50988], "temperature": 0.0, "avg_logprob": -0.13453394848367442, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0007672571227885783}, {"id": 1398, "seek": 573644, "start": 5748.919999999999, "end": 5752.04, "text": " So now we have to evaluate the loss, but we're not using the mean squared error", "tokens": [50988, 407, 586, 321, 362, 281, 13059, 264, 4470, 11, 457, 321, 434, 406, 1228, 264, 914, 8889, 6713, 51144], "temperature": 0.0, "avg_logprob": -0.13453394848367442, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0007672571227885783}, {"id": 1399, "seek": 573644, "start": 5752.36, "end": 5757.639999999999, "text": " We're using the negative log likelihood because we are doing classification. We're not doing regression as it's called", "tokens": [51160, 492, 434, 1228, 264, 3671, 3565, 22119, 570, 321, 366, 884, 21538, 13, 492, 434, 406, 884, 24590, 382, 309, 311, 1219, 51424], "temperature": 0.0, "avg_logprob": -0.13453394848367442, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0007672571227885783}, {"id": 1400, "seek": 573644, "start": 5759.08, "end": 5761.08, "text": " So here we want to calculate loss", "tokens": [51496, 407, 510, 321, 528, 281, 8873, 4470, 51596], "temperature": 0.0, "avg_logprob": -0.13453394848367442, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0007672571227885783}, {"id": 1401, "seek": 573644, "start": 5762.44, "end": 5766.28, "text": " Now the way we calculate it is is just this average negative log likelihood", "tokens": [51664, 823, 264, 636, 321, 8873, 309, 307, 307, 445, 341, 4274, 3671, 3565, 22119, 51856], "temperature": 0.0, "avg_logprob": -0.13453394848367442, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0007672571227885783}, {"id": 1402, "seek": 576644, "start": 5767.16, "end": 5769.16, "text": " Now this probes here", "tokens": [50400, 823, 341, 1239, 279, 510, 50500], "temperature": 0.0, "avg_logprob": -0.12354577581087749, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0006361603154800832}, {"id": 1403, "seek": 576644, "start": 5770.679999999999, "end": 5772.679999999999, "text": " Has a shape of five by 27", "tokens": [50576, 8646, 257, 3909, 295, 1732, 538, 7634, 50676], "temperature": 0.0, "avg_logprob": -0.12354577581087749, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0006361603154800832}, {"id": 1404, "seek": 576644, "start": 5773.4, "end": 5779.4, "text": " And so to get all the we basically want to pluck out the probabilities at the correct indices here", "tokens": [50712, 400, 370, 281, 483, 439, 264, 321, 1936, 528, 281, 41514, 484, 264, 33783, 412, 264, 3006, 43840, 510, 51012], "temperature": 0.0, "avg_logprob": -0.12354577581087749, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0006361603154800832}, {"id": 1405, "seek": 576644, "start": 5780.04, "end": 5783.879999999999, "text": " So in particular because the labels are stored here in the array y's", "tokens": [51044, 407, 294, 1729, 570, 264, 16949, 366, 12187, 510, 294, 264, 10225, 288, 311, 51236], "temperature": 0.0, "avg_logprob": -0.12354577581087749, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0006361603154800832}, {"id": 1406, "seek": 576644, "start": 5784.5199999999995, "end": 5790.28, "text": " Basically what we're after is for the first example, we're looking at probability of five right at index five", "tokens": [51268, 8537, 437, 321, 434, 934, 307, 337, 264, 700, 1365, 11, 321, 434, 1237, 412, 8482, 295, 1732, 558, 412, 8186, 1732, 51556], "temperature": 0.0, "avg_logprob": -0.12354577581087749, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0006361603154800832}, {"id": 1407, "seek": 576644, "start": 5790.919999999999, "end": 5792.759999999999, "text": " for the second example", "tokens": [51588, 337, 264, 1150, 1365, 51680], "temperature": 0.0, "avg_logprob": -0.12354577581087749, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0006361603154800832}, {"id": 1408, "seek": 576644, "start": 5792.759999999999, "end": 5795.719999999999, "text": " at the the second row or row index one", "tokens": [51680, 412, 264, 264, 1150, 5386, 420, 5386, 8186, 472, 51828], "temperature": 0.0, "avg_logprob": -0.12354577581087749, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0006361603154800832}, {"id": 1409, "seek": 579572, "start": 5796.2, "end": 5799.320000000001, "text": " We are interested in the probability assigned to index 13", "tokens": [50388, 492, 366, 3102, 294, 264, 8482, 13279, 281, 8186, 3705, 50544], "temperature": 0.0, "avg_logprob": -0.12819146608051502, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.0009849605849012733}, {"id": 1410, "seek": 579572, "start": 5800.280000000001, "end": 5802.52, "text": " At the second example, we also have 13", "tokens": [50592, 1711, 264, 1150, 1365, 11, 321, 611, 362, 3705, 50704], "temperature": 0.0, "avg_logprob": -0.12819146608051502, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.0009849605849012733}, {"id": 1411, "seek": 579572, "start": 5803.4800000000005, "end": 5805.88, "text": " At the third row, we want one", "tokens": [50752, 1711, 264, 2636, 5386, 11, 321, 528, 472, 50872], "temperature": 0.0, "avg_logprob": -0.12819146608051502, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.0009849605849012733}, {"id": 1412, "seek": 579572, "start": 5807.400000000001, "end": 5810.92, "text": " And at the last row, which is four we want zero", "tokens": [50948, 400, 412, 264, 1036, 5386, 11, 597, 307, 1451, 321, 528, 4018, 51124], "temperature": 0.0, "avg_logprob": -0.12819146608051502, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.0009849605849012733}, {"id": 1413, "seek": 579572, "start": 5811.240000000001, "end": 5817.16, "text": " So these are the probabilities we're interested in right and you can see that they're not amazing as we saw above", "tokens": [51140, 407, 613, 366, 264, 33783, 321, 434, 3102, 294, 558, 293, 291, 393, 536, 300, 436, 434, 406, 2243, 382, 321, 1866, 3673, 51436], "temperature": 0.0, "avg_logprob": -0.12819146608051502, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.0009849605849012733}, {"id": 1414, "seek": 579572, "start": 5818.68, "end": 5823.4800000000005, "text": " So these are the probabilities we want but we want like a more efficient way to access these probabilities", "tokens": [51512, 407, 613, 366, 264, 33783, 321, 528, 457, 321, 528, 411, 257, 544, 7148, 636, 281, 2105, 613, 33783, 51752], "temperature": 0.0, "avg_logprob": -0.12819146608051502, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.0009849605849012733}, {"id": 1415, "seek": 582348, "start": 5824.04, "end": 5826.599999999999, "text": " Um, not just listing them out in a tuple like this", "tokens": [50392, 3301, 11, 406, 445, 22161, 552, 484, 294, 257, 2604, 781, 411, 341, 50520], "temperature": 0.0, "avg_logprob": -0.1576374093281854, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.007576666306704283}, {"id": 1416, "seek": 582348, "start": 5827.08, "end": 5833.48, "text": " So it turns out that the way to do this in pi torch one of the ways at least is we can basically pass in all of these", "tokens": [50544, 407, 309, 4523, 484, 300, 264, 636, 281, 360, 341, 294, 3895, 27822, 472, 295, 264, 2098, 412, 1935, 307, 321, 393, 1936, 1320, 294, 439, 295, 613, 50864], "temperature": 0.0, "avg_logprob": -0.1576374093281854, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.007576666306704283}, {"id": 1417, "seek": 582348, "start": 5836.759999999999, "end": 5841.24, "text": " Sorry about that all of these um integers in vectors", "tokens": [51028, 4919, 466, 300, 439, 295, 613, 1105, 41674, 294, 18875, 51252], "temperature": 0.0, "avg_logprob": -0.1576374093281854, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.007576666306704283}, {"id": 1418, "seek": 582348, "start": 5842.12, "end": 5846.599999999999, "text": " So the these ones you see how they're just zero one two three four", "tokens": [51296, 407, 264, 613, 2306, 291, 536, 577, 436, 434, 445, 4018, 472, 732, 1045, 1451, 51520], "temperature": 0.0, "avg_logprob": -0.1576374093281854, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.007576666306704283}, {"id": 1419, "seek": 582348, "start": 5847.16, "end": 5852.12, "text": " We can actually create that using mp not mp. Sorry torch dot a range of five", "tokens": [51548, 492, 393, 767, 1884, 300, 1228, 275, 79, 406, 275, 79, 13, 4919, 27822, 5893, 257, 3613, 295, 1732, 51796], "temperature": 0.0, "avg_logprob": -0.1576374093281854, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.007576666306704283}, {"id": 1420, "seek": 585212, "start": 5852.84, "end": 5854.44, "text": " zero one two three four", "tokens": [50400, 4018, 472, 732, 1045, 1451, 50480], "temperature": 0.0, "avg_logprob": -0.113892910092376, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0007096499903127551}, {"id": 1421, "seek": 585212, "start": 5854.44, "end": 5857.4, "text": " So we can index here with torch dot a range of five", "tokens": [50480, 407, 321, 393, 8186, 510, 365, 27822, 5893, 257, 3613, 295, 1732, 50628], "temperature": 0.0, "avg_logprob": -0.113892910092376, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0007096499903127551}, {"id": 1422, "seek": 585212, "start": 5858.36, "end": 5860.36, "text": " And here we index with y's", "tokens": [50676, 400, 510, 321, 8186, 365, 288, 311, 50776], "temperature": 0.0, "avg_logprob": -0.113892910092376, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0007096499903127551}, {"id": 1423, "seek": 585212, "start": 5861.16, "end": 5863.16, "text": " And you see that that gives us", "tokens": [50816, 400, 291, 536, 300, 300, 2709, 505, 50916], "temperature": 0.0, "avg_logprob": -0.113892910092376, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0007096499903127551}, {"id": 1424, "seek": 585212, "start": 5863.24, "end": 5865.24, "text": " exactly these numbers", "tokens": [50920, 2293, 613, 3547, 51020], "temperature": 0.0, "avg_logprob": -0.113892910092376, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0007096499903127551}, {"id": 1425, "seek": 585212, "start": 5869.0, "end": 5873.4, "text": " So that plugs out the probabilities of that the neural network assigns to the", "tokens": [51208, 407, 300, 33899, 484, 264, 33783, 295, 300, 264, 18161, 3209, 6269, 82, 281, 264, 51428], "temperature": 0.0, "avg_logprob": -0.113892910092376, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0007096499903127551}, {"id": 1426, "seek": 585212, "start": 5874.04, "end": 5876.04, "text": " correct next character", "tokens": [51460, 3006, 958, 2517, 51560], "temperature": 0.0, "avg_logprob": -0.113892910092376, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0007096499903127551}, {"id": 1427, "seek": 585212, "start": 5876.36, "end": 5880.28, "text": " Now we take those probabilities and we don't we actually look at the log probability", "tokens": [51576, 823, 321, 747, 729, 33783, 293, 321, 500, 380, 321, 767, 574, 412, 264, 3565, 8482, 51772], "temperature": 0.0, "avg_logprob": -0.113892910092376, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0007096499903127551}, {"id": 1428, "seek": 588028, "start": 5880.599999999999, "end": 5882.599999999999, "text": " So we want to dot log", "tokens": [50380, 407, 321, 528, 281, 5893, 3565, 50480], "temperature": 0.0, "avg_logprob": -0.11849303098069024, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00376509758643806}, {"id": 1429, "seek": 588028, "start": 5883.5599999999995, "end": 5886.5199999999995, "text": " And then we want to just average that up", "tokens": [50528, 400, 550, 321, 528, 281, 445, 4274, 300, 493, 50676], "temperature": 0.0, "avg_logprob": -0.11849303098069024, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00376509758643806}, {"id": 1430, "seek": 588028, "start": 5886.679999999999, "end": 5892.5199999999995, "text": " So take the mean of all of that and then it's the negative average log likelihood. That is the loss", "tokens": [50684, 407, 747, 264, 914, 295, 439, 295, 300, 293, 550, 309, 311, 264, 3671, 4274, 3565, 22119, 13, 663, 307, 264, 4470, 50976], "temperature": 0.0, "avg_logprob": -0.11849303098069024, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00376509758643806}, {"id": 1431, "seek": 588028, "start": 5894.28, "end": 5896.28, "text": " So the loss here is", "tokens": [51064, 407, 264, 4470, 510, 307, 51164], "temperature": 0.0, "avg_logprob": -0.11849303098069024, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00376509758643806}, {"id": 1432, "seek": 588028, "start": 5896.92, "end": 5900.36, "text": " 3.7 something and you see that this loss 3.76", "tokens": [51196, 805, 13, 22, 746, 293, 291, 536, 300, 341, 4470, 805, 13, 25026, 51368], "temperature": 0.0, "avg_logprob": -0.11849303098069024, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00376509758643806}, {"id": 1433, "seek": 588028, "start": 5900.759999999999, "end": 5905.8, "text": " 3.76 is exactly as we've obtained before but this is a vectorized form of that expression", "tokens": [51388, 805, 13, 25026, 307, 2293, 382, 321, 600, 14879, 949, 457, 341, 307, 257, 8062, 1602, 1254, 295, 300, 6114, 51640], "temperature": 0.0, "avg_logprob": -0.11849303098069024, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00376509758643806}, {"id": 1434, "seek": 588028, "start": 5906.5199999999995, "end": 5908.679999999999, "text": " So we get the same loss", "tokens": [51676, 407, 321, 483, 264, 912, 4470, 51784], "temperature": 0.0, "avg_logprob": -0.11849303098069024, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00376509758643806}, {"id": 1435, "seek": 590868, "start": 5909.4800000000005, "end": 5913.400000000001, "text": " And the same loss we can consider sort of as part of this forward pass", "tokens": [50404, 400, 264, 912, 4470, 321, 393, 1949, 1333, 295, 382, 644, 295, 341, 2128, 1320, 50600], "temperature": 0.0, "avg_logprob": -0.10528440475463867, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.001501142280176282}, {"id": 1436, "seek": 590868, "start": 5914.04, "end": 5916.04, "text": " And we've achieved here now loss", "tokens": [50632, 400, 321, 600, 11042, 510, 586, 4470, 50732], "temperature": 0.0, "avg_logprob": -0.10528440475463867, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.001501142280176282}, {"id": 1437, "seek": 590868, "start": 5916.360000000001, "end": 5919.400000000001, "text": " Okay, so we made our way all the way to loss. We've defined the forward pass", "tokens": [50748, 1033, 11, 370, 321, 1027, 527, 636, 439, 264, 636, 281, 4470, 13, 492, 600, 7642, 264, 2128, 1320, 50900], "temperature": 0.0, "avg_logprob": -0.10528440475463867, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.001501142280176282}, {"id": 1438, "seek": 590868, "start": 5920.12, "end": 5925.240000000001, "text": " We forwarded the network and the loss now. We're ready to do the backward pass. So backward pass", "tokens": [50936, 492, 2128, 292, 264, 3209, 293, 264, 4470, 586, 13, 492, 434, 1919, 281, 360, 264, 23897, 1320, 13, 407, 23897, 1320, 51192], "temperature": 0.0, "avg_logprob": -0.10528440475463867, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.001501142280176282}, {"id": 1439, "seek": 590868, "start": 5928.04, "end": 5931.8, "text": " We want to first make sure that all the gradients are reset. So they're at zero", "tokens": [51332, 492, 528, 281, 700, 652, 988, 300, 439, 264, 2771, 2448, 366, 14322, 13, 407, 436, 434, 412, 4018, 51520], "temperature": 0.0, "avg_logprob": -0.10528440475463867, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.001501142280176282}, {"id": 1440, "seek": 590868, "start": 5932.4400000000005, "end": 5935.88, "text": " Now in pi torch, you can set the gradients to be zero", "tokens": [51552, 823, 294, 3895, 27822, 11, 291, 393, 992, 264, 2771, 2448, 281, 312, 4018, 51724], "temperature": 0.0, "avg_logprob": -0.10528440475463867, "compression_ratio": 1.794759825327511, "no_speech_prob": 0.001501142280176282}, {"id": 1441, "seek": 593588, "start": 5935.88, "end": 5939.88, "text": " But you can also just set it to none and setting it to none is more efficient", "tokens": [50364, 583, 291, 393, 611, 445, 992, 309, 281, 6022, 293, 3287, 309, 281, 6022, 307, 544, 7148, 50564], "temperature": 0.0, "avg_logprob": -0.12101039319935411, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.002287127310410142}, {"id": 1442, "seek": 593588, "start": 5940.2, "end": 5945.24, "text": " And pi torch will interpret none as like a lack of a gradient and it's the same as zeros", "tokens": [50580, 400, 3895, 27822, 486, 7302, 6022, 382, 411, 257, 5011, 295, 257, 16235, 293, 309, 311, 264, 912, 382, 35193, 50832], "temperature": 0.0, "avg_logprob": -0.12101039319935411, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.002287127310410142}, {"id": 1443, "seek": 593588, "start": 5945.8, "end": 5948.4400000000005, "text": " So this is a way to set to zero the gradient", "tokens": [50860, 407, 341, 307, 257, 636, 281, 992, 281, 4018, 264, 16235, 50992], "temperature": 0.0, "avg_logprob": -0.12101039319935411, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.002287127310410142}, {"id": 1444, "seek": 593588, "start": 5950.4400000000005, "end": 5952.4400000000005, "text": " And now we do lost up backward", "tokens": [51092, 400, 586, 321, 360, 2731, 493, 23897, 51192], "temperature": 0.0, "avg_logprob": -0.12101039319935411, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.002287127310410142}, {"id": 1445, "seek": 593588, "start": 5954.68, "end": 5958.28, "text": " Before we do lost up backward, we need one more thing if you remember from micrograd", "tokens": [51304, 4546, 321, 360, 2731, 493, 23897, 11, 321, 643, 472, 544, 551, 498, 291, 1604, 490, 4532, 7165, 51484], "temperature": 0.0, "avg_logprob": -0.12101039319935411, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.002287127310410142}, {"id": 1446, "seek": 593588, "start": 5958.92, "end": 5960.92, "text": " pi torch actually requires", "tokens": [51516, 3895, 27822, 767, 7029, 51616], "temperature": 0.0, "avg_logprob": -0.12101039319935411, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.002287127310410142}, {"id": 1447, "seek": 593588, "start": 5961.32, "end": 5963.88, "text": " That we pass in requires grad is true", "tokens": [51636, 663, 321, 1320, 294, 7029, 2771, 307, 2074, 51764], "temperature": 0.0, "avg_logprob": -0.12101039319935411, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.002287127310410142}, {"id": 1448, "seek": 596388, "start": 5964.84, "end": 5966.84, "text": " Uh, so that we tell", "tokens": [50412, 4019, 11, 370, 300, 321, 980, 50512], "temperature": 0.0, "avg_logprob": -0.15572237386936094, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.0012447984190657735}, {"id": 1449, "seek": 596388, "start": 5967.0, "end": 5972.76, "text": " Pi torch that we are interested in calculating gradients for this leaf tensor by default. This is false", "tokens": [50520, 17741, 27822, 300, 321, 366, 3102, 294, 28258, 2771, 2448, 337, 341, 10871, 40863, 538, 7576, 13, 639, 307, 7908, 50808], "temperature": 0.0, "avg_logprob": -0.15572237386936094, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.0012447984190657735}, {"id": 1450, "seek": 596388, "start": 5973.4800000000005, "end": 5975.4800000000005, "text": " So let me recalculate with that", "tokens": [50844, 407, 718, 385, 850, 304, 2444, 473, 365, 300, 50944], "temperature": 0.0, "avg_logprob": -0.15572237386936094, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.0012447984190657735}, {"id": 1451, "seek": 596388, "start": 5975.88, "end": 5978.36, "text": " And then set to none and lost up backward", "tokens": [50964, 400, 550, 992, 281, 6022, 293, 2731, 493, 23897, 51088], "temperature": 0.0, "avg_logprob": -0.15572237386936094, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.0012447984190657735}, {"id": 1452, "seek": 596388, "start": 5980.68, "end": 5983.4800000000005, "text": " Now something magical happened when lost up backward was run", "tokens": [51204, 823, 746, 12066, 2011, 562, 2731, 493, 23897, 390, 1190, 51344], "temperature": 0.0, "avg_logprob": -0.15572237386936094, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.0012447984190657735}, {"id": 1453, "seek": 596388, "start": 5984.4400000000005, "end": 5988.52, "text": " Because pi torch just like micrograd when we did the forward pass here", "tokens": [51392, 1436, 3895, 27822, 445, 411, 4532, 7165, 562, 321, 630, 264, 2128, 1320, 510, 51596], "temperature": 0.0, "avg_logprob": -0.15572237386936094, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.0012447984190657735}, {"id": 1454, "seek": 598852, "start": 5989.080000000001, "end": 5994.200000000001, "text": " It keeps track of all the operations under the hood. It builds a full computational graph", "tokens": [50392, 467, 5965, 2837, 295, 439, 264, 7705, 833, 264, 13376, 13, 467, 15182, 257, 1577, 28270, 4295, 50648], "temperature": 0.0, "avg_logprob": -0.10596223221611731, "compression_ratio": 1.7520661157024793, "no_speech_prob": 0.028002185747027397}, {"id": 1455, "seek": 598852, "start": 5994.76, "end": 5999.8, "text": " Just like the graphs we've produced in micrograd those graphs exist inside pi torch", "tokens": [50676, 1449, 411, 264, 24877, 321, 600, 7126, 294, 4532, 7165, 729, 24877, 2514, 1854, 3895, 27822, 50928], "temperature": 0.0, "avg_logprob": -0.10596223221611731, "compression_ratio": 1.7520661157024793, "no_speech_prob": 0.028002185747027397}, {"id": 1456, "seek": 598852, "start": 6000.76, "end": 6004.4400000000005, "text": " And so it knows all the dependencies and all the mathematical operations of everything", "tokens": [50976, 400, 370, 309, 3255, 439, 264, 36606, 293, 439, 264, 18894, 7705, 295, 1203, 51160], "temperature": 0.0, "avg_logprob": -0.10596223221611731, "compression_ratio": 1.7520661157024793, "no_speech_prob": 0.028002185747027397}, {"id": 1457, "seek": 598852, "start": 6005.0, "end": 6008.76, "text": " And when you then calculate the loss we can call a dot backward on it", "tokens": [51188, 400, 562, 291, 550, 8873, 264, 4470, 321, 393, 818, 257, 5893, 23897, 322, 309, 51376], "temperature": 0.0, "avg_logprob": -0.10596223221611731, "compression_ratio": 1.7520661157024793, "no_speech_prob": 0.028002185747027397}, {"id": 1458, "seek": 598852, "start": 6009.56, "end": 6013.0, "text": " And dot backward then fills in the gradients of", "tokens": [51416, 400, 5893, 23897, 550, 22498, 294, 264, 2771, 2448, 295, 51588], "temperature": 0.0, "avg_logprob": -0.10596223221611731, "compression_ratio": 1.7520661157024793, "no_speech_prob": 0.028002185747027397}, {"id": 1459, "seek": 598852, "start": 6013.64, "end": 6017.240000000001, "text": " All the intermediates all the way back to w's", "tokens": [51620, 1057, 264, 15184, 1024, 439, 264, 636, 646, 281, 261, 311, 51800], "temperature": 0.0, "avg_logprob": -0.10596223221611731, "compression_ratio": 1.7520661157024793, "no_speech_prob": 0.028002185747027397}, {"id": 1460, "seek": 601724, "start": 6017.8, "end": 6021.48, "text": " Which are the parameters of our neural net. So now we can do w dot grad", "tokens": [50392, 3013, 366, 264, 9834, 295, 527, 18161, 2533, 13, 407, 586, 321, 393, 360, 261, 5893, 2771, 50576], "temperature": 0.0, "avg_logprob": -0.13193333001784335, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0031724111177027225}, {"id": 1461, "seek": 601724, "start": 6022.36, "end": 6024.92, "text": " And we see that it has structure there's stuff inside it", "tokens": [50620, 400, 321, 536, 300, 309, 575, 3877, 456, 311, 1507, 1854, 309, 50748], "temperature": 0.0, "avg_logprob": -0.13193333001784335, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0031724111177027225}, {"id": 1462, "seek": 601724, "start": 6029.16, "end": 6032.12, "text": " And these gradients every single element here", "tokens": [50960, 400, 613, 2771, 2448, 633, 2167, 4478, 510, 51108], "temperature": 0.0, "avg_logprob": -0.13193333001784335, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0031724111177027225}, {"id": 1463, "seek": 601724, "start": 6033.4, "end": 6036.2, "text": " So w dot shape is 27 by 27", "tokens": [51172, 407, 261, 5893, 3909, 307, 7634, 538, 7634, 51312], "temperature": 0.0, "avg_logprob": -0.13193333001784335, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0031724111177027225}, {"id": 1464, "seek": 601724, "start": 6036.84, "end": 6039.8, "text": " W grads shape is the same 27 by 27", "tokens": [51344, 343, 2771, 82, 3909, 307, 264, 912, 7634, 538, 7634, 51492], "temperature": 0.0, "avg_logprob": -0.13193333001784335, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0031724111177027225}, {"id": 1465, "seek": 601724, "start": 6040.679999999999, "end": 6042.5199999999995, "text": " And every element of w dot grad", "tokens": [51536, 400, 633, 4478, 295, 261, 5893, 2771, 51628], "temperature": 0.0, "avg_logprob": -0.13193333001784335, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0031724111177027225}, {"id": 1466, "seek": 601724, "start": 6043.16, "end": 6044.44, "text": " is telling us", "tokens": [51660, 307, 3585, 505, 51724], "temperature": 0.0, "avg_logprob": -0.13193333001784335, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0031724111177027225}, {"id": 1467, "seek": 604444, "start": 6044.5199999999995, "end": 6048.04, "text": " The influence of that weight on the loss function", "tokens": [50368, 440, 6503, 295, 300, 3364, 322, 264, 4470, 2445, 50544], "temperature": 0.0, "avg_logprob": -0.1406245417409129, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.01150574628263712}, {"id": 1468, "seek": 604444, "start": 6048.759999999999, "end": 6051.08, "text": " So for example this number all the way here", "tokens": [50580, 407, 337, 1365, 341, 1230, 439, 264, 636, 510, 50696], "temperature": 0.0, "avg_logprob": -0.1406245417409129, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.01150574628263712}, {"id": 1469, "seek": 604444, "start": 6051.96, "end": 6054.759999999999, "text": " If this element the zero zero element of w", "tokens": [50740, 759, 341, 4478, 264, 4018, 4018, 4478, 295, 261, 50880], "temperature": 0.0, "avg_logprob": -0.1406245417409129, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.01150574628263712}, {"id": 1470, "seek": 604444, "start": 6055.5599999999995, "end": 6062.28, "text": " Because the gradient is positive it's telling us that this has a positive influence on the loss slightly nudging", "tokens": [50920, 1436, 264, 16235, 307, 3353, 309, 311, 3585, 505, 300, 341, 575, 257, 3353, 6503, 322, 264, 4470, 4748, 40045, 3249, 51256], "temperature": 0.0, "avg_logprob": -0.1406245417409129, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.01150574628263712}, {"id": 1471, "seek": 604444, "start": 6063.16, "end": 6064.44, "text": " w", "tokens": [51300, 261, 51364], "temperature": 0.0, "avg_logprob": -0.1406245417409129, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.01150574628263712}, {"id": 1472, "seek": 604444, "start": 6064.44, "end": 6066.44, "text": " slightly taking w zero zero", "tokens": [51364, 4748, 1940, 261, 4018, 4018, 51464], "temperature": 0.0, "avg_logprob": -0.1406245417409129, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.01150574628263712}, {"id": 1473, "seek": 604444, "start": 6067.0, "end": 6069.639999999999, "text": " And adding a small h to it", "tokens": [51492, 400, 5127, 257, 1359, 276, 281, 309, 51624], "temperature": 0.0, "avg_logprob": -0.1406245417409129, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.01150574628263712}, {"id": 1474, "seek": 606964, "start": 6070.360000000001, "end": 6074.68, "text": " Would increase the loss mildly because this gradient is positive", "tokens": [50400, 6068, 3488, 264, 4470, 15154, 356, 570, 341, 16235, 307, 3353, 50616], "temperature": 0.0, "avg_logprob": -0.11392403674382036, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.006003070157021284}, {"id": 1475, "seek": 606964, "start": 6075.64, "end": 6077.64, "text": " Some of these gradients are also negative", "tokens": [50664, 2188, 295, 613, 2771, 2448, 366, 611, 3671, 50764], "temperature": 0.0, "avg_logprob": -0.11392403674382036, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.006003070157021284}, {"id": 1476, "seek": 606964, "start": 6078.6, "end": 6083.0, "text": " So that's telling us about the gradient information and we can use this gradient information", "tokens": [50812, 407, 300, 311, 3585, 505, 466, 264, 16235, 1589, 293, 321, 393, 764, 341, 16235, 1589, 51032], "temperature": 0.0, "avg_logprob": -0.11392403674382036, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.006003070157021284}, {"id": 1477, "seek": 606964, "start": 6083.400000000001, "end": 6087.8, "text": " To update the weights of this neural network. So let's not do the update", "tokens": [51052, 1407, 5623, 264, 17443, 295, 341, 18161, 3209, 13, 407, 718, 311, 406, 360, 264, 5623, 51272], "temperature": 0.0, "avg_logprob": -0.11392403674382036, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.006003070157021284}, {"id": 1478, "seek": 606964, "start": 6088.280000000001, "end": 6090.360000000001, "text": " It's going to be very similar to what we had in micrograd", "tokens": [51296, 467, 311, 516, 281, 312, 588, 2531, 281, 437, 321, 632, 294, 4532, 7165, 51400], "temperature": 0.0, "avg_logprob": -0.11392403674382036, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.006003070157021284}, {"id": 1479, "seek": 606964, "start": 6090.76, "end": 6094.6, "text": " We need no loop over all the parameters because we only have one parameter", "tokens": [51420, 492, 643, 572, 6367, 670, 439, 264, 9834, 570, 321, 787, 362, 472, 13075, 51612], "temperature": 0.0, "avg_logprob": -0.11392403674382036, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.006003070157021284}, {"id": 1480, "seek": 609460, "start": 6095.320000000001, "end": 6100.360000000001, "text": " Tensor and that is w. So we simply do w dot data plus equals", "tokens": [50400, 34306, 293, 300, 307, 261, 13, 407, 321, 2935, 360, 261, 5893, 1412, 1804, 6915, 50652], "temperature": 0.0, "avg_logprob": -0.15526775261024375, "compression_ratio": 1.6740331491712708, "no_speech_prob": 0.027165817096829414}, {"id": 1481, "seek": 609460, "start": 6101.400000000001, "end": 6105.240000000001, "text": " The we can actually copy this almost exactly negative zero point one times", "tokens": [50704, 440, 321, 393, 767, 5055, 341, 1920, 2293, 3671, 4018, 935, 472, 1413, 50896], "temperature": 0.0, "avg_logprob": -0.15526775261024375, "compression_ratio": 1.6740331491712708, "no_speech_prob": 0.027165817096829414}, {"id": 1482, "seek": 609460, "start": 6106.120000000001, "end": 6108.120000000001, "text": " w dot grad", "tokens": [50940, 261, 5893, 2771, 51040], "temperature": 0.0, "avg_logprob": -0.15526775261024375, "compression_ratio": 1.6740331491712708, "no_speech_prob": 0.027165817096829414}, {"id": 1483, "seek": 609460, "start": 6108.200000000001, "end": 6109.4800000000005, "text": " um", "tokens": [51044, 1105, 51108], "temperature": 0.0, "avg_logprob": -0.15526775261024375, "compression_ratio": 1.6740331491712708, "no_speech_prob": 0.027165817096829414}, {"id": 1484, "seek": 609460, "start": 6109.4800000000005, "end": 6112.76, "text": " And that would be the update to the tensor", "tokens": [51108, 400, 300, 576, 312, 264, 5623, 281, 264, 40863, 51272], "temperature": 0.0, "avg_logprob": -0.15526775261024375, "compression_ratio": 1.6740331491712708, "no_speech_prob": 0.027165817096829414}, {"id": 1485, "seek": 609460, "start": 6114.52, "end": 6116.52, "text": " So that updates the tensor", "tokens": [51360, 407, 300, 9205, 264, 40863, 51460], "temperature": 0.0, "avg_logprob": -0.15526775261024375, "compression_ratio": 1.6740331491712708, "no_speech_prob": 0.027165817096829414}, {"id": 1486, "seek": 609460, "start": 6118.76, "end": 6123.4800000000005, "text": " And because the tensor is updated we would expect that now the loss should decrease", "tokens": [51572, 400, 570, 264, 40863, 307, 10588, 321, 576, 2066, 300, 586, 264, 4470, 820, 11514, 51808], "temperature": 0.0, "avg_logprob": -0.15526775261024375, "compression_ratio": 1.6740331491712708, "no_speech_prob": 0.027165817096829414}, {"id": 1487, "seek": 612348, "start": 6124.36, "end": 6127.48, "text": " So here if I print loss", "tokens": [50408, 407, 510, 498, 286, 4482, 4470, 50564], "temperature": 0.0, "avg_logprob": -0.13694723844528198, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0030751789454370737}, {"id": 1488, "seek": 612348, "start": 6129.48, "end": 6131.16, "text": " That item", "tokens": [50664, 663, 3174, 50748], "temperature": 0.0, "avg_logprob": -0.13694723844528198, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0030751789454370737}, {"id": 1489, "seek": 612348, "start": 6131.16, "end": 6133.16, "text": " It was 3.76 right", "tokens": [50748, 467, 390, 805, 13, 25026, 558, 50848], "temperature": 0.0, "avg_logprob": -0.13694723844528198, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0030751789454370737}, {"id": 1490, "seek": 612348, "start": 6133.16, "end": 6137.799999999999, "text": " So we've updated the w here. So if I recalculate forward pass", "tokens": [50848, 407, 321, 600, 10588, 264, 261, 510, 13, 407, 498, 286, 850, 304, 2444, 473, 2128, 1320, 51080], "temperature": 0.0, "avg_logprob": -0.13694723844528198, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0030751789454370737}, {"id": 1491, "seek": 612348, "start": 6139.0, "end": 6142.919999999999, "text": " Loss now should be slightly lower. So 3.76 goes to", "tokens": [51140, 441, 772, 586, 820, 312, 4748, 3126, 13, 407, 805, 13, 25026, 1709, 281, 51336], "temperature": 0.0, "avg_logprob": -0.13694723844528198, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0030751789454370737}, {"id": 1492, "seek": 612348, "start": 6143.5599999999995, "end": 6145.5599999999995, "text": " 3.74", "tokens": [51368, 805, 13, 34026, 51468], "temperature": 0.0, "avg_logprob": -0.13694723844528198, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0030751789454370737}, {"id": 1493, "seek": 612348, "start": 6145.799999999999, "end": 6150.12, "text": " And then we can again set to set grad to none and backward", "tokens": [51480, 400, 550, 321, 393, 797, 992, 281, 992, 2771, 281, 6022, 293, 23897, 51696], "temperature": 0.0, "avg_logprob": -0.13694723844528198, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0030751789454370737}, {"id": 1494, "seek": 612348, "start": 6150.839999999999, "end": 6152.599999999999, "text": " update", "tokens": [51732, 5623, 51820], "temperature": 0.0, "avg_logprob": -0.13694723844528198, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0030751789454370737}, {"id": 1495, "seek": 615260, "start": 6152.6, "end": 6154.6, "text": " And now the parameters changed again", "tokens": [50364, 400, 586, 264, 9834, 3105, 797, 50464], "temperature": 0.0, "avg_logprob": -0.10739525689019097, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.00034597795456647873}, {"id": 1496, "seek": 615260, "start": 6154.92, "end": 6159.96, "text": " So if we recalculate the forward pass, we expect a lower loss again 3.72", "tokens": [50480, 407, 498, 321, 850, 304, 2444, 473, 264, 2128, 1320, 11, 321, 2066, 257, 3126, 4470, 797, 805, 13, 28890, 50732], "temperature": 0.0, "avg_logprob": -0.10739525689019097, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.00034597795456647873}, {"id": 1497, "seek": 615260, "start": 6162.200000000001, "end": 6165.64, "text": " Okay, and this is again doing the we're now doing reading the set", "tokens": [50844, 1033, 11, 293, 341, 307, 797, 884, 264, 321, 434, 586, 884, 3760, 264, 992, 51016], "temperature": 0.0, "avg_logprob": -0.10739525689019097, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.00034597795456647873}, {"id": 1498, "seek": 615260, "start": 6168.52, "end": 6175.0, "text": " And when we achieve a low loss that will mean that the network is assigning high probabilities to the correct next characters", "tokens": [51160, 400, 562, 321, 4584, 257, 2295, 4470, 300, 486, 914, 300, 264, 3209, 307, 49602, 1090, 33783, 281, 264, 3006, 958, 4342, 51484], "temperature": 0.0, "avg_logprob": -0.10739525689019097, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.00034597795456647873}, {"id": 1499, "seek": 615260, "start": 6175.240000000001, "end": 6178.6, "text": " Okay, so I rearranged everything and I put it all together from scratch", "tokens": [51496, 1033, 11, 370, 286, 29875, 10296, 1203, 293, 286, 829, 309, 439, 1214, 490, 8459, 51664], "temperature": 0.0, "avg_logprob": -0.10739525689019097, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.00034597795456647873}, {"id": 1500, "seek": 617860, "start": 6179.400000000001, "end": 6182.280000000001, "text": " So here is where we construct our data set of bigrams", "tokens": [50404, 407, 510, 307, 689, 321, 7690, 527, 1412, 992, 295, 955, 2356, 82, 50548], "temperature": 0.0, "avg_logprob": -0.12475007251628394, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0023229827638715506}, {"id": 1501, "seek": 617860, "start": 6183.240000000001, "end": 6186.04, "text": " You see that we are still iterating only over the first word emma", "tokens": [50596, 509, 536, 300, 321, 366, 920, 17138, 990, 787, 670, 264, 700, 1349, 846, 1696, 50736], "temperature": 0.0, "avg_logprob": -0.12475007251628394, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0023229827638715506}, {"id": 1502, "seek": 617860, "start": 6186.92, "end": 6188.92, "text": " I'm going to change that in a second", "tokens": [50780, 286, 478, 516, 281, 1319, 300, 294, 257, 1150, 50880], "temperature": 0.0, "avg_logprob": -0.12475007251628394, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0023229827638715506}, {"id": 1503, "seek": 617860, "start": 6189.08, "end": 6196.120000000001, "text": " I added a number that counts the number of elements in axis so that we explicitly see that number of examples is five", "tokens": [50888, 286, 3869, 257, 1230, 300, 14893, 264, 1230, 295, 4959, 294, 10298, 370, 300, 321, 20803, 536, 300, 1230, 295, 5110, 307, 1732, 51240], "temperature": 0.0, "avg_logprob": -0.12475007251628394, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0023229827638715506}, {"id": 1504, "seek": 617860, "start": 6196.92, "end": 6199.64, "text": " Because currently we're just working with emma. There's five bigrams there", "tokens": [51280, 1436, 4362, 321, 434, 445, 1364, 365, 846, 1696, 13, 821, 311, 1732, 955, 2356, 82, 456, 51416], "temperature": 0.0, "avg_logprob": -0.12475007251628394, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0023229827638715506}, {"id": 1505, "seek": 617860, "start": 6200.6, "end": 6203.240000000001, "text": " And here I added a loop of exactly what we had before", "tokens": [51464, 400, 510, 286, 3869, 257, 6367, 295, 2293, 437, 321, 632, 949, 51596], "temperature": 0.0, "avg_logprob": -0.12475007251628394, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.0023229827638715506}, {"id": 1506, "seek": 620324, "start": 6203.719999999999, "end": 6208.36, "text": " So we had 10 iterations of gradient descent of forward pass backward pass and an update", "tokens": [50388, 407, 321, 632, 1266, 36540, 295, 16235, 23475, 295, 2128, 1320, 23897, 1320, 293, 364, 5623, 50620], "temperature": 0.0, "avg_logprob": -0.12842194600538773, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.01798393949866295}, {"id": 1507, "seek": 620324, "start": 6209.0, "end": 6212.12, "text": " And so running these two cells initialization and gradient descent", "tokens": [50652, 400, 370, 2614, 613, 732, 5438, 5883, 2144, 293, 16235, 23475, 50808], "temperature": 0.0, "avg_logprob": -0.12842194600538773, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.01798393949866295}, {"id": 1508, "seek": 620324, "start": 6212.84, "end": 6214.84, "text": " Gives us some improvement", "tokens": [50844, 460, 1539, 505, 512, 10444, 50944], "temperature": 0.0, "avg_logprob": -0.12842194600538773, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.01798393949866295}, {"id": 1509, "seek": 620324, "start": 6215.4, "end": 6217.4, "text": " on the last function", "tokens": [50972, 322, 264, 1036, 2445, 51072], "temperature": 0.0, "avg_logprob": -0.12842194600538773, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.01798393949866295}, {"id": 1510, "seek": 620324, "start": 6218.2, "end": 6220.2, "text": " But now I want to use all the words", "tokens": [51112, 583, 586, 286, 528, 281, 764, 439, 264, 2283, 51212], "temperature": 0.0, "avg_logprob": -0.12842194600538773, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.01798393949866295}, {"id": 1511, "seek": 620324, "start": 6221.719999999999, "end": 6225.639999999999, "text": " And there's not five but 228,000 bigrams now", "tokens": [51288, 400, 456, 311, 406, 1732, 457, 5853, 23, 11, 1360, 955, 2356, 82, 586, 51484], "temperature": 0.0, "avg_logprob": -0.12842194600538773, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.01798393949866295}, {"id": 1512, "seek": 620324, "start": 6226.679999999999, "end": 6230.44, "text": " However, this should require no modification whatsoever. Everything should just run", "tokens": [51536, 2908, 11, 341, 820, 3651, 572, 26747, 17076, 13, 5471, 820, 445, 1190, 51724], "temperature": 0.0, "avg_logprob": -0.12842194600538773, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.01798393949866295}, {"id": 1513, "seek": 623044, "start": 6230.759999999999, "end": 6237.16, "text": " Because all the code we wrote doesn't care if there's five bigrams or 228,000 bigrams and with everything we should just work", "tokens": [50380, 1436, 439, 264, 3089, 321, 4114, 1177, 380, 1127, 498, 456, 311, 1732, 955, 2356, 82, 420, 5853, 23, 11, 1360, 955, 2356, 82, 293, 365, 1203, 321, 820, 445, 589, 50700], "temperature": 0.0, "avg_logprob": -0.0991526444753011, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.0071206423453986645}, {"id": 1514, "seek": 623044, "start": 6237.32, "end": 6238.44, "text": " So", "tokens": [50708, 407, 50764], "temperature": 0.0, "avg_logprob": -0.0991526444753011, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.0071206423453986645}, {"id": 1515, "seek": 623044, "start": 6238.44, "end": 6240.44, "text": " You see that this will just run", "tokens": [50764, 509, 536, 300, 341, 486, 445, 1190, 50864], "temperature": 0.0, "avg_logprob": -0.0991526444753011, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.0071206423453986645}, {"id": 1516, "seek": 623044, "start": 6240.44, "end": 6243.799999999999, "text": " But now we are optimizing over the entire training set of all the bigrams", "tokens": [50864, 583, 586, 321, 366, 40425, 670, 264, 2302, 3097, 992, 295, 439, 264, 955, 2356, 82, 51032], "temperature": 0.0, "avg_logprob": -0.0991526444753011, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.0071206423453986645}, {"id": 1517, "seek": 623044, "start": 6244.679999999999, "end": 6249.96, "text": " And you see now that we are decreasing very slightly. So actually we can probably afford the larger learning rate", "tokens": [51076, 400, 291, 536, 586, 300, 321, 366, 23223, 588, 4748, 13, 407, 767, 321, 393, 1391, 6157, 264, 4833, 2539, 3314, 51340], "temperature": 0.0, "avg_logprob": -0.0991526444753011, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.0071206423453986645}, {"id": 1518, "seek": 623044, "start": 6252.36, "end": 6254.36, "text": " Can probably afford even larger learning rate", "tokens": [51460, 1664, 1391, 6157, 754, 4833, 2539, 3314, 51560], "temperature": 0.0, "avg_logprob": -0.0991526444753011, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.0071206423453986645}, {"id": 1519, "seek": 626044, "start": 6260.679999999999, "end": 6267.5599999999995, "text": " Even 50 seems to work on this very very simple example, right? So let me re-initialize and let's run 100 iterations", "tokens": [50376, 2754, 2625, 2544, 281, 589, 322, 341, 588, 588, 2199, 1365, 11, 558, 30, 407, 718, 385, 319, 12, 259, 270, 831, 1125, 293, 718, 311, 1190, 2319, 36540, 50720], "temperature": 0.0, "avg_logprob": -0.12122245007250683, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.01281889621168375}, {"id": 1520, "seek": 626044, "start": 6269.24, "end": 6271.24, "text": " See what happens", "tokens": [50804, 3008, 437, 2314, 50904], "temperature": 0.0, "avg_logprob": -0.12122245007250683, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.01281889621168375}, {"id": 1521, "seek": 626044, "start": 6273.0, "end": 6275.0, "text": " Okay", "tokens": [50992, 1033, 51092], "temperature": 0.0, "avg_logprob": -0.12122245007250683, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.01281889621168375}, {"id": 1522, "seek": 626044, "start": 6276.28, "end": 6278.28, "text": " We seem to be", "tokens": [51156, 492, 1643, 281, 312, 51256], "temperature": 0.0, "avg_logprob": -0.12122245007250683, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.01281889621168375}, {"id": 1523, "seek": 626044, "start": 6279.08, "end": 6282.2, "text": " Coming up to some pretty good losses here 2.47", "tokens": [51296, 12473, 493, 281, 512, 1238, 665, 15352, 510, 568, 13, 14060, 51452], "temperature": 0.0, "avg_logprob": -0.12122245007250683, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.01281889621168375}, {"id": 1524, "seek": 626044, "start": 6282.839999999999, "end": 6284.599999999999, "text": " Let me run 100 more", "tokens": [51484, 961, 385, 1190, 2319, 544, 51572], "temperature": 0.0, "avg_logprob": -0.12122245007250683, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.01281889621168375}, {"id": 1525, "seek": 626044, "start": 6284.599999999999, "end": 6286.839999999999, "text": " What is the number that we expect by the way in the loss?", "tokens": [51572, 708, 307, 264, 1230, 300, 321, 2066, 538, 264, 636, 294, 264, 4470, 30, 51684], "temperature": 0.0, "avg_logprob": -0.12122245007250683, "compression_ratio": 1.422680412371134, "no_speech_prob": 0.01281889621168375}, {"id": 1526, "seek": 628684, "start": 6287.24, "end": 6290.68, "text": " We expect to get something around what we had originally actually", "tokens": [50384, 492, 2066, 281, 483, 746, 926, 437, 321, 632, 7993, 767, 50556], "temperature": 0.0, "avg_logprob": -0.1358240529110557, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0075761703774333}, {"id": 1527, "seek": 628684, "start": 6292.04, "end": 6295.32, "text": " So all the way back if you remember in the beginning of this video when we", "tokens": [50624, 407, 439, 264, 636, 646, 498, 291, 1604, 294, 264, 2863, 295, 341, 960, 562, 321, 50788], "temperature": 0.0, "avg_logprob": -0.1358240529110557, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0075761703774333}, {"id": 1528, "seek": 628684, "start": 6295.96, "end": 6297.400000000001, "text": " optimized", "tokens": [50820, 26941, 50892], "temperature": 0.0, "avg_logprob": -0.1358240529110557, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0075761703774333}, {"id": 1529, "seek": 628684, "start": 6297.400000000001, "end": 6298.84, "text": " Just by counting", "tokens": [50892, 1449, 538, 13251, 50964], "temperature": 0.0, "avg_logprob": -0.1358240529110557, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0075761703774333}, {"id": 1530, "seek": 628684, "start": 6298.84, "end": 6300.84, "text": " Our loss was roughly 2.47", "tokens": [50964, 2621, 4470, 390, 9810, 568, 13, 14060, 51064], "temperature": 0.0, "avg_logprob": -0.1358240529110557, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0075761703774333}, {"id": 1531, "seek": 628684, "start": 6301.56, "end": 6303.56, "text": " After we added smoothing", "tokens": [51100, 2381, 321, 3869, 899, 6259, 571, 51200], "temperature": 0.0, "avg_logprob": -0.1358240529110557, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0075761703774333}, {"id": 1532, "seek": 628684, "start": 6303.56, "end": 6305.8, "text": " But before smoothing we had roughly 2.45", "tokens": [51200, 583, 949, 899, 6259, 571, 321, 632, 9810, 568, 13, 8465, 51312], "temperature": 0.0, "avg_logprob": -0.1358240529110557, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0075761703774333}, {"id": 1533, "seek": 628684, "start": 6306.6, "end": 6308.360000000001, "text": " likely it", "tokens": [51352, 3700, 309, 51440], "temperature": 0.0, "avg_logprob": -0.1358240529110557, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0075761703774333}, {"id": 1534, "seek": 628684, "start": 6308.360000000001, "end": 6309.72, "text": " Sorry loss", "tokens": [51440, 4919, 4470, 51508], "temperature": 0.0, "avg_logprob": -0.1358240529110557, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0075761703774333}, {"id": 1535, "seek": 628684, "start": 6309.72, "end": 6313.32, "text": " And so that's actually roughly the vicinity of what we expect to achieve", "tokens": [51508, 400, 370, 300, 311, 767, 9810, 264, 42387, 295, 437, 321, 2066, 281, 4584, 51688], "temperature": 0.0, "avg_logprob": -0.1358240529110557, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0075761703774333}, {"id": 1536, "seek": 631332, "start": 6313.799999999999, "end": 6318.599999999999, "text": " But before we achieved it by counting and here we are achieving the roughly the same result", "tokens": [50388, 583, 949, 321, 11042, 309, 538, 13251, 293, 510, 321, 366, 19626, 264, 9810, 264, 912, 1874, 50628], "temperature": 0.0, "avg_logprob": -0.10147065586513943, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.003765031462535262}, {"id": 1537, "seek": 631332, "start": 6318.759999999999, "end": 6320.759999999999, "text": " But with gradient based optimization", "tokens": [50636, 583, 365, 16235, 2361, 19618, 50736], "temperature": 0.0, "avg_logprob": -0.10147065586513943, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.003765031462535262}, {"id": 1538, "seek": 631332, "start": 6321.0, "end": 6325.639999999999, "text": " So we come to about 2.46 2.45, etc", "tokens": [50748, 407, 321, 808, 281, 466, 568, 13, 16169, 568, 13, 8465, 11, 5183, 50980], "temperature": 0.0, "avg_logprob": -0.10147065586513943, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.003765031462535262}, {"id": 1539, "seek": 631332, "start": 6326.28, "end": 6329.719999999999, "text": " And that makes sense because fundamentally we're not taking any additional information", "tokens": [51012, 400, 300, 1669, 2020, 570, 17879, 321, 434, 406, 1940, 604, 4497, 1589, 51184], "temperature": 0.0, "avg_logprob": -0.10147065586513943, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.003765031462535262}, {"id": 1540, "seek": 631332, "start": 6329.88, "end": 6333.08, "text": " We're still just taking in the previous character and trying to predict the next one", "tokens": [51192, 492, 434, 920, 445, 1940, 294, 264, 3894, 2517, 293, 1382, 281, 6069, 264, 958, 472, 51352], "temperature": 0.0, "avg_logprob": -0.10147065586513943, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.003765031462535262}, {"id": 1541, "seek": 631332, "start": 6333.639999999999, "end": 6337.08, "text": " But instead of doing it explicitly by counting and normalizing", "tokens": [51380, 583, 2602, 295, 884, 309, 20803, 538, 13251, 293, 2710, 3319, 51552], "temperature": 0.0, "avg_logprob": -0.10147065586513943, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.003765031462535262}, {"id": 1542, "seek": 631332, "start": 6338.2, "end": 6342.36, "text": " We are doing it with gradient based learning and it just so happens that the explicit approach", "tokens": [51608, 492, 366, 884, 309, 365, 16235, 2361, 2539, 293, 309, 445, 370, 2314, 300, 264, 13691, 3109, 51816], "temperature": 0.0, "avg_logprob": -0.10147065586513943, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.003765031462535262}, {"id": 1543, "seek": 634236, "start": 6342.679999999999, "end": 6348.12, "text": " Happens to very well optimize the loss function without any need for gradient based optimization", "tokens": [50380, 7412, 694, 281, 588, 731, 19719, 264, 4470, 2445, 1553, 604, 643, 337, 16235, 2361, 19618, 50652], "temperature": 0.0, "avg_logprob": -0.13016641971676848, "compression_ratio": 1.588, "no_speech_prob": 0.005819099023938179}, {"id": 1544, "seek": 634236, "start": 6348.5199999999995, "end": 6352.599999999999, "text": " Because the setup for bi-gram language models are is so straightforward. It's so simple", "tokens": [50672, 1436, 264, 8657, 337, 3228, 12, 1342, 2856, 5245, 366, 307, 370, 15325, 13, 467, 311, 370, 2199, 50876], "temperature": 0.0, "avg_logprob": -0.13016641971676848, "compression_ratio": 1.588, "no_speech_prob": 0.005819099023938179}, {"id": 1545, "seek": 634236, "start": 6352.92, "end": 6357.719999999999, "text": " We can just afford to estimate those probabilities directly and maintain them in a table", "tokens": [50892, 492, 393, 445, 6157, 281, 12539, 729, 33783, 3838, 293, 6909, 552, 294, 257, 3199, 51132], "temperature": 0.0, "avg_logprob": -0.13016641971676848, "compression_ratio": 1.588, "no_speech_prob": 0.005819099023938179}, {"id": 1546, "seek": 634236, "start": 6358.92, "end": 6362.2, "text": " But the gradient based approach is significantly more flexible", "tokens": [51192, 583, 264, 16235, 2361, 3109, 307, 10591, 544, 11358, 51356], "temperature": 0.0, "avg_logprob": -0.13016641971676848, "compression_ratio": 1.588, "no_speech_prob": 0.005819099023938179}, {"id": 1547, "seek": 634236, "start": 6362.92, "end": 6365.32, "text": " so we've actually gained a lot because", "tokens": [51392, 370, 321, 600, 767, 12634, 257, 688, 570, 51512], "temperature": 0.0, "avg_logprob": -0.13016641971676848, "compression_ratio": 1.588, "no_speech_prob": 0.005819099023938179}, {"id": 1548, "seek": 634236, "start": 6366.679999999999, "end": 6368.679999999999, "text": " What we can do now is", "tokens": [51580, 708, 321, 393, 360, 586, 307, 51680], "temperature": 0.0, "avg_logprob": -0.13016641971676848, "compression_ratio": 1.588, "no_speech_prob": 0.005819099023938179}, {"id": 1549, "seek": 636868, "start": 6369.240000000001, "end": 6372.280000000001, "text": " We can expand this approach and complexify the neural net", "tokens": [50392, 492, 393, 5268, 341, 3109, 293, 3997, 2505, 264, 18161, 2533, 50544], "temperature": 0.0, "avg_logprob": -0.1060501213731437, "compression_ratio": 1.9316546762589928, "no_speech_prob": 0.009266427718102932}, {"id": 1550, "seek": 636868, "start": 6372.84, "end": 6377.400000000001, "text": " So currently we're just taking a single character and feeding into a neural net and the neural is extremely simple", "tokens": [50572, 407, 4362, 321, 434, 445, 1940, 257, 2167, 2517, 293, 12919, 666, 257, 18161, 2533, 293, 264, 18161, 307, 4664, 2199, 50800], "temperature": 0.0, "avg_logprob": -0.1060501213731437, "compression_ratio": 1.9316546762589928, "no_speech_prob": 0.009266427718102932}, {"id": 1551, "seek": 636868, "start": 6377.8, "end": 6380.04, "text": " But we're about to iterate on this substantially", "tokens": [50820, 583, 321, 434, 466, 281, 44497, 322, 341, 30797, 50932], "temperature": 0.0, "avg_logprob": -0.1060501213731437, "compression_ratio": 1.9316546762589928, "no_speech_prob": 0.009266427718102932}, {"id": 1552, "seek": 636868, "start": 6380.4400000000005, "end": 6387.08, "text": " We're going to be taking multiple previous characters and we're going to be feeding them into increasingly more complex neural nets", "tokens": [50952, 492, 434, 516, 281, 312, 1940, 3866, 3894, 4342, 293, 321, 434, 516, 281, 312, 12919, 552, 666, 12980, 544, 3997, 18161, 36170, 51284], "temperature": 0.0, "avg_logprob": -0.1060501213731437, "compression_ratio": 1.9316546762589928, "no_speech_prob": 0.009266427718102932}, {"id": 1553, "seek": 636868, "start": 6387.4800000000005, "end": 6391.400000000001, "text": " But fundamentally out the output of the neural net will always just be logits", "tokens": [51304, 583, 17879, 484, 264, 5598, 295, 264, 18161, 2533, 486, 1009, 445, 312, 3565, 1208, 51500], "temperature": 0.0, "avg_logprob": -0.1060501213731437, "compression_ratio": 1.9316546762589928, "no_speech_prob": 0.009266427718102932}, {"id": 1554, "seek": 636868, "start": 6392.68, "end": 6395.16, "text": " And those logits will go through the exact same transformation", "tokens": [51564, 400, 729, 3565, 1208, 486, 352, 807, 264, 1900, 912, 9887, 51688], "temperature": 0.0, "avg_logprob": -0.1060501213731437, "compression_ratio": 1.9316546762589928, "no_speech_prob": 0.009266427718102932}, {"id": 1555, "seek": 636868, "start": 6395.56, "end": 6397.56, "text": " We're going to take them through a softmax", "tokens": [51708, 492, 434, 516, 281, 747, 552, 807, 257, 2787, 41167, 51808], "temperature": 0.0, "avg_logprob": -0.1060501213731437, "compression_ratio": 1.9316546762589928, "no_speech_prob": 0.009266427718102932}, {"id": 1556, "seek": 639756, "start": 6397.96, "end": 6403.080000000001, "text": " Calculate the loss function and the negative log likelihood and do gradient based optimization", "tokens": [50384, 3511, 2444, 473, 264, 4470, 2445, 293, 264, 3671, 3565, 22119, 293, 360, 16235, 2361, 19618, 50640], "temperature": 0.0, "avg_logprob": -0.14040436407532356, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0027147189248353243}, {"id": 1557, "seek": 639756, "start": 6403.64, "end": 6409.0, "text": " And so actually as we complexify the neural nets and work all the way up to transformers", "tokens": [50668, 400, 370, 767, 382, 321, 3997, 2505, 264, 18161, 36170, 293, 589, 439, 264, 636, 493, 281, 4088, 433, 50936], "temperature": 0.0, "avg_logprob": -0.14040436407532356, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0027147189248353243}, {"id": 1558, "seek": 639756, "start": 6409.8, "end": 6411.8, "text": " None of this will really fundamentally change", "tokens": [50976, 14492, 295, 341, 486, 534, 17879, 1319, 51076], "temperature": 0.0, "avg_logprob": -0.14040436407532356, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0027147189248353243}, {"id": 1559, "seek": 639756, "start": 6411.96, "end": 6415.0, "text": " None of this will fundamentally change the only thing that will change is", "tokens": [51084, 14492, 295, 341, 486, 17879, 1319, 264, 787, 551, 300, 486, 1319, 307, 51236], "temperature": 0.0, "avg_logprob": -0.14040436407532356, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0027147189248353243}, {"id": 1560, "seek": 639756, "start": 6415.64, "end": 6422.360000000001, "text": " The way we do the forward pass or we've taken some previous characters and calculate logits for the next character in a sequence", "tokens": [51268, 440, 636, 321, 360, 264, 2128, 1320, 420, 321, 600, 2726, 512, 3894, 4342, 293, 8873, 3565, 1208, 337, 264, 958, 2517, 294, 257, 8310, 51604], "temperature": 0.0, "avg_logprob": -0.14040436407532356, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0027147189248353243}, {"id": 1561, "seek": 639756, "start": 6422.92, "end": 6424.92, "text": " that will become more complex", "tokens": [51632, 300, 486, 1813, 544, 3997, 51732], "temperature": 0.0, "avg_logprob": -0.14040436407532356, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0027147189248353243}, {"id": 1562, "seek": 642492, "start": 6425.16, "end": 6428.2, "text": " And that will use the same machinery to optimize it", "tokens": [50376, 400, 300, 486, 764, 264, 912, 27302, 281, 19719, 309, 50528], "temperature": 0.0, "avg_logprob": -0.15565667693147955, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.005138786043971777}, {"id": 1563, "seek": 642492, "start": 6428.92, "end": 6430.68, "text": " and", "tokens": [50564, 293, 50652], "temperature": 0.0, "avg_logprob": -0.15565667693147955, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.005138786043971777}, {"id": 1564, "seek": 642492, "start": 6430.68, "end": 6432.68, "text": " It's not obvious how we would have extended", "tokens": [50652, 467, 311, 406, 6322, 577, 321, 576, 362, 10913, 50752], "temperature": 0.0, "avg_logprob": -0.15565667693147955, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.005138786043971777}, {"id": 1565, "seek": 642492, "start": 6433.16, "end": 6434.92, "text": " This bygram approach", "tokens": [50776, 639, 538, 1342, 3109, 50864], "temperature": 0.0, "avg_logprob": -0.15565667693147955, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.005138786043971777}, {"id": 1566, "seek": 642492, "start": 6434.92, "end": 6437.16, "text": " Into the case where there are many more", "tokens": [50864, 23373, 264, 1389, 689, 456, 366, 867, 544, 50976], "temperature": 0.0, "avg_logprob": -0.15565667693147955, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.005138786043971777}, {"id": 1567, "seek": 642492, "start": 6437.4800000000005, "end": 6443.8, "text": " Characters at the input because eventually these tables would get way too large because there's way too many combinations", "tokens": [50992, 4327, 326, 1559, 412, 264, 4846, 570, 4728, 613, 8020, 576, 483, 636, 886, 2416, 570, 456, 311, 636, 886, 867, 21267, 51308], "temperature": 0.0, "avg_logprob": -0.15565667693147955, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.005138786043971777}, {"id": 1568, "seek": 642492, "start": 6444.2, "end": 6446.2, "text": " Of what previous characters", "tokens": [51328, 2720, 437, 3894, 4342, 51428], "temperature": 0.0, "avg_logprob": -0.15565667693147955, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.005138786043971777}, {"id": 1569, "seek": 642492, "start": 6446.2, "end": 6447.88, "text": " could be", "tokens": [51428, 727, 312, 51512], "temperature": 0.0, "avg_logprob": -0.15565667693147955, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.005138786043971777}, {"id": 1570, "seek": 642492, "start": 6447.88, "end": 6451.72, "text": " If you only have one previous character we can just keep everything in a table the counts", "tokens": [51512, 759, 291, 787, 362, 472, 3894, 2517, 321, 393, 445, 1066, 1203, 294, 257, 3199, 264, 14893, 51704], "temperature": 0.0, "avg_logprob": -0.15565667693147955, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.005138786043971777}, {"id": 1571, "seek": 645172, "start": 6452.04, "end": 6457.08, "text": " But if you have the last 10 characters that are in but we can't actually keep everything in the table anymore", "tokens": [50380, 583, 498, 291, 362, 264, 1036, 1266, 4342, 300, 366, 294, 457, 321, 393, 380, 767, 1066, 1203, 294, 264, 3199, 3602, 50632], "temperature": 0.0, "avg_logprob": -0.1055082937683722, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.004006985574960709}, {"id": 1572, "seek": 645172, "start": 6457.400000000001, "end": 6462.6, "text": " So this is fundamentally an unscalable approach and the neural network approach is significantly more scalable", "tokens": [50648, 407, 341, 307, 17879, 364, 2693, 9895, 712, 3109, 293, 264, 18161, 3209, 3109, 307, 10591, 544, 38481, 50908], "temperature": 0.0, "avg_logprob": -0.1055082937683722, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.004006985574960709}, {"id": 1573, "seek": 645172, "start": 6463.08, "end": 6468.280000000001, "text": " And it's something that actually we can improve on over time. So that's where we will be digging next", "tokens": [50932, 400, 309, 311, 746, 300, 767, 321, 393, 3470, 322, 670, 565, 13, 407, 300, 311, 689, 321, 486, 312, 17343, 958, 51192], "temperature": 0.0, "avg_logprob": -0.1055082937683722, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.004006985574960709}, {"id": 1574, "seek": 645172, "start": 6468.52, "end": 6470.52, "text": " I wanted to point out two more things", "tokens": [51204, 286, 1415, 281, 935, 484, 732, 544, 721, 51304], "temperature": 0.0, "avg_logprob": -0.1055082937683722, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.004006985574960709}, {"id": 1575, "seek": 645172, "start": 6471.16, "end": 6472.280000000001, "text": " number one", "tokens": [51336, 1230, 472, 51392], "temperature": 0.0, "avg_logprob": -0.1055082937683722, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.004006985574960709}, {"id": 1576, "seek": 645172, "start": 6472.280000000001, "end": 6474.280000000001, "text": " I want you to notice that this", "tokens": [51392, 286, 528, 291, 281, 3449, 300, 341, 51492], "temperature": 0.0, "avg_logprob": -0.1055082937683722, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.004006985574960709}, {"id": 1577, "seek": 645172, "start": 6475.08, "end": 6476.76, "text": " x-ank here", "tokens": [51532, 2031, 12, 657, 510, 51616], "temperature": 0.0, "avg_logprob": -0.1055082937683722, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.004006985574960709}, {"id": 1578, "seek": 647676, "start": 6476.76, "end": 6482.280000000001, "text": " This is made up of one-hot vectors and then those one-hot vectors are multiplied by this w matrix", "tokens": [50364, 639, 307, 1027, 493, 295, 472, 12, 12194, 18875, 293, 550, 729, 472, 12, 12194, 18875, 366, 17207, 538, 341, 261, 8141, 50640], "temperature": 0.0, "avg_logprob": -0.1156486923044378, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.04022180289030075}, {"id": 1579, "seek": 647676, "start": 6483.24, "end": 6487.88, "text": " And we think of this as a multiple neurons being forwarded in a fully connected manner", "tokens": [50688, 400, 321, 519, 295, 341, 382, 257, 3866, 22027, 885, 2128, 292, 294, 257, 4498, 4582, 9060, 50920], "temperature": 0.0, "avg_logprob": -0.1156486923044378, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.04022180289030075}, {"id": 1580, "seek": 647676, "start": 6488.68, "end": 6490.84, "text": " But actually what's happening here is that for example", "tokens": [50960, 583, 767, 437, 311, 2737, 510, 307, 300, 337, 1365, 51068], "temperature": 0.0, "avg_logprob": -0.1156486923044378, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.04022180289030075}, {"id": 1581, "seek": 647676, "start": 6491.96, "end": 6497.0, "text": " If you have a one-hot vector here that has a one at say the fifth dimension", "tokens": [51124, 759, 291, 362, 257, 472, 12, 12194, 8062, 510, 300, 575, 257, 472, 412, 584, 264, 9266, 10139, 51376], "temperature": 0.0, "avg_logprob": -0.1156486923044378, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.04022180289030075}, {"id": 1582, "seek": 647676, "start": 6497.72, "end": 6500.2, "text": " Then because of the way the matrix multiplication works", "tokens": [51412, 1396, 570, 295, 264, 636, 264, 8141, 27290, 1985, 51536], "temperature": 0.0, "avg_logprob": -0.1156486923044378, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.04022180289030075}, {"id": 1583, "seek": 650020, "start": 6501.16, "end": 6506.679999999999, "text": " Multiplying that one-hot vector with w actually ends up plucking out the fifth row of w", "tokens": [50412, 31150, 7310, 300, 472, 12, 12194, 8062, 365, 261, 767, 5314, 493, 499, 33260, 484, 264, 9266, 5386, 295, 261, 50688], "temperature": 0.0, "avg_logprob": -0.11787679867866711, "compression_ratio": 1.7634408602150538, "no_speech_prob": 0.010986044071614742}, {"id": 1584, "seek": 650020, "start": 6507.639999999999, "end": 6510.5199999999995, "text": " Lot logits would become just the fifth row of w", "tokens": [50736, 20131, 3565, 1208, 576, 1813, 445, 264, 9266, 5386, 295, 261, 50880], "temperature": 0.0, "avg_logprob": -0.11787679867866711, "compression_ratio": 1.7634408602150538, "no_speech_prob": 0.010986044071614742}, {"id": 1585, "seek": 650020, "start": 6511.32, "end": 6514.44, "text": " And that's because of the way the matrix multiplication works", "tokens": [50920, 400, 300, 311, 570, 295, 264, 636, 264, 8141, 27290, 1985, 51076], "temperature": 0.0, "avg_logprob": -0.11787679867866711, "compression_ratio": 1.7634408602150538, "no_speech_prob": 0.010986044071614742}, {"id": 1586, "seek": 650020, "start": 6517.0, "end": 6519.24, "text": " So that's actually what ends up happening", "tokens": [51204, 407, 300, 311, 767, 437, 5314, 493, 2737, 51316], "temperature": 0.0, "avg_logprob": -0.11787679867866711, "compression_ratio": 1.7634408602150538, "no_speech_prob": 0.010986044071614742}, {"id": 1587, "seek": 650020, "start": 6520.12, "end": 6524.92, "text": " So but that's actually exactly what happened before because remember all the way up here", "tokens": [51360, 407, 457, 300, 311, 767, 2293, 437, 2011, 949, 570, 1604, 439, 264, 636, 493, 510, 51600], "temperature": 0.0, "avg_logprob": -0.11787679867866711, "compression_ratio": 1.7634408602150538, "no_speech_prob": 0.010986044071614742}, {"id": 1588, "seek": 652492, "start": 6525.56, "end": 6527.4800000000005, "text": " We have a bi-gram", "tokens": [50396, 492, 362, 257, 3228, 12, 1342, 50492], "temperature": 0.0, "avg_logprob": -0.13085609851497235, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.003172507742419839}, {"id": 1589, "seek": 652492, "start": 6527.56, "end": 6534.2, "text": " We took the first character and then that first character indexed into a row of this array here", "tokens": [50496, 492, 1890, 264, 700, 2517, 293, 550, 300, 700, 2517, 8186, 292, 666, 257, 5386, 295, 341, 10225, 510, 50828], "temperature": 0.0, "avg_logprob": -0.13085609851497235, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.003172507742419839}, {"id": 1590, "seek": 652492, "start": 6534.92, "end": 6538.12, "text": " And that row gave us the probability distribution for the next character", "tokens": [50864, 400, 300, 5386, 2729, 505, 264, 8482, 7316, 337, 264, 958, 2517, 51024], "temperature": 0.0, "avg_logprob": -0.13085609851497235, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.003172507742419839}, {"id": 1591, "seek": 652492, "start": 6538.6, "end": 6541.8, "text": " So the first character was used as a lookup into a", "tokens": [51048, 407, 264, 700, 2517, 390, 1143, 382, 257, 574, 1010, 666, 257, 51208], "temperature": 0.0, "avg_logprob": -0.13085609851497235, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.003172507742419839}, {"id": 1592, "seek": 652492, "start": 6543.64, "end": 6545.64, "text": " Matrix here to get the probability distribution", "tokens": [51300, 36274, 510, 281, 483, 264, 8482, 7316, 51400], "temperature": 0.0, "avg_logprob": -0.13085609851497235, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.003172507742419839}, {"id": 1593, "seek": 652492, "start": 6546.28, "end": 6550.12, "text": " Well, that's actually exactly what's happening here because we're taking the index", "tokens": [51432, 1042, 11, 300, 311, 767, 2293, 437, 311, 2737, 510, 570, 321, 434, 1940, 264, 8186, 51624], "temperature": 0.0, "avg_logprob": -0.13085609851497235, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.003172507742419839}, {"id": 1594, "seek": 652492, "start": 6550.6, "end": 6552.92, "text": " We're encoding it as one-hot and multiplying it by w", "tokens": [51648, 492, 434, 43430, 309, 382, 472, 12, 12194, 293, 30955, 309, 538, 261, 51764], "temperature": 0.0, "avg_logprob": -0.13085609851497235, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.003172507742419839}, {"id": 1595, "seek": 655292, "start": 6553.4800000000005, "end": 6555.88, "text": " So logits literally becomes the", "tokens": [50392, 407, 3565, 1208, 3736, 3643, 264, 50512], "temperature": 0.0, "avg_logprob": -0.20761670694722759, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.007459683809429407}, {"id": 1596, "seek": 655292, "start": 6558.04, "end": 6562.36, "text": " The appropriate row of w and that gets just as before", "tokens": [50620, 440, 6854, 5386, 295, 261, 293, 300, 2170, 445, 382, 949, 50836], "temperature": 0.0, "avg_logprob": -0.20761670694722759, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.007459683809429407}, {"id": 1597, "seek": 655292, "start": 6563.16, "end": 6566.84, "text": " Exponentiated to create the counts and then normalized and becomes probability", "tokens": [50876, 21391, 266, 23012, 770, 281, 1884, 264, 14893, 293, 550, 48704, 293, 3643, 8482, 51060], "temperature": 0.0, "avg_logprob": -0.20761670694722759, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.007459683809429407}, {"id": 1598, "seek": 655292, "start": 6567.4800000000005, "end": 6570.28, "text": " So this w here is literally", "tokens": [51092, 407, 341, 261, 510, 307, 3736, 51232], "temperature": 0.0, "avg_logprob": -0.20761670694722759, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.007459683809429407}, {"id": 1599, "seek": 655292, "start": 6571.4, "end": 6573.8, "text": " The same as this array here", "tokens": [51288, 440, 912, 382, 341, 10225, 510, 51408], "temperature": 0.0, "avg_logprob": -0.20761670694722759, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.007459683809429407}, {"id": 1600, "seek": 655292, "start": 6575.08, "end": 6581.16, "text": " But w remember is the log counts not the counts. So it's more precise to say that w", "tokens": [51472, 583, 261, 1604, 307, 264, 3565, 14893, 406, 264, 14893, 13, 407, 309, 311, 544, 13600, 281, 584, 300, 261, 51776], "temperature": 0.0, "avg_logprob": -0.20761670694722759, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.007459683809429407}, {"id": 1601, "seek": 658116, "start": 6581.4, "end": 6585.4, "text": " Exponentiated w.exp is this array", "tokens": [50376, 21391, 266, 23012, 770, 261, 13, 3121, 79, 307, 341, 10225, 50576], "temperature": 0.0, "avg_logprob": -0.1631420041307991, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.008846057578921318}, {"id": 1602, "seek": 658116, "start": 6586.2, "end": 6588.68, "text": " But this array was filled in by counting", "tokens": [50616, 583, 341, 10225, 390, 6412, 294, 538, 13251, 50740], "temperature": 0.0, "avg_logprob": -0.1631420041307991, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.008846057578921318}, {"id": 1603, "seek": 658116, "start": 6589.32, "end": 6591.32, "text": " and by basically", "tokens": [50772, 293, 538, 1936, 50872], "temperature": 0.0, "avg_logprob": -0.1631420041307991, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.008846057578921318}, {"id": 1604, "seek": 658116, "start": 6591.96, "end": 6595.639999999999, "text": " Populating the counts of bi-grams whereas in the gradient based framework", "tokens": [50904, 10215, 12162, 264, 14893, 295, 3228, 12, 1342, 82, 9735, 294, 264, 16235, 2361, 8388, 51088], "temperature": 0.0, "avg_logprob": -0.1631420041307991, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.008846057578921318}, {"id": 1605, "seek": 658116, "start": 6595.88, "end": 6598.76, "text": " We initialize it randomly and then we let the loss", "tokens": [51100, 492, 5883, 1125, 309, 16979, 293, 550, 321, 718, 264, 4470, 51244], "temperature": 0.0, "avg_logprob": -0.1631420041307991, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.008846057578921318}, {"id": 1606, "seek": 658116, "start": 6599.4, "end": 6602.44, "text": " Guide us to arrive at the exact same array", "tokens": [51276, 18727, 505, 281, 8881, 412, 264, 1900, 912, 10225, 51428], "temperature": 0.0, "avg_logprob": -0.1631420041307991, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.008846057578921318}, {"id": 1607, "seek": 658116, "start": 6603.24, "end": 6605.24, "text": " So this array exactly here", "tokens": [51468, 407, 341, 10225, 2293, 510, 51568], "temperature": 0.0, "avg_logprob": -0.1631420041307991, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.008846057578921318}, {"id": 1608, "seek": 658116, "start": 6605.8, "end": 6606.76, "text": " is", "tokens": [51596, 307, 51644], "temperature": 0.0, "avg_logprob": -0.1631420041307991, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.008846057578921318}, {"id": 1609, "seek": 660676, "start": 6606.84, "end": 6611.400000000001, "text": " Basically the array w at the end of optimization except we arrived at it", "tokens": [50368, 8537, 264, 10225, 261, 412, 264, 917, 295, 19618, 3993, 321, 6678, 412, 309, 50596], "temperature": 0.0, "avg_logprob": -0.11450444891097698, "compression_ratio": 1.7573221757322175, "no_speech_prob": 0.0005112133803777397}, {"id": 1610, "seek": 660676, "start": 6612.280000000001, "end": 6614.280000000001, "text": " piece by piece by following the loss", "tokens": [50640, 2522, 538, 2522, 538, 3480, 264, 4470, 50740], "temperature": 0.0, "avg_logprob": -0.11450444891097698, "compression_ratio": 1.7573221757322175, "no_speech_prob": 0.0005112133803777397}, {"id": 1611, "seek": 660676, "start": 6615.0, "end": 6619.88, "text": " And that's why we also obtained the same loss function at the end and the second note is if I come here", "tokens": [50776, 400, 300, 311, 983, 321, 611, 14879, 264, 912, 4470, 2445, 412, 264, 917, 293, 264, 1150, 3637, 307, 498, 286, 808, 510, 51020], "temperature": 0.0, "avg_logprob": -0.11450444891097698, "compression_ratio": 1.7573221757322175, "no_speech_prob": 0.0005112133803777397}, {"id": 1612, "seek": 660676, "start": 6620.52, "end": 6625.4800000000005, "text": " remember the smoothing where we added fake counts to our counts in order to", "tokens": [51052, 1604, 264, 899, 6259, 571, 689, 321, 3869, 7592, 14893, 281, 527, 14893, 294, 1668, 281, 51300], "temperature": 0.0, "avg_logprob": -0.11450444891097698, "compression_ratio": 1.7573221757322175, "no_speech_prob": 0.0005112133803777397}, {"id": 1613, "seek": 660676, "start": 6626.04, "end": 6630.280000000001, "text": " Smooth out and make more uniform the distributions of these probabilities", "tokens": [51328, 42404, 484, 293, 652, 544, 9452, 264, 37870, 295, 613, 33783, 51540], "temperature": 0.0, "avg_logprob": -0.11450444891097698, "compression_ratio": 1.7573221757322175, "no_speech_prob": 0.0005112133803777397}, {"id": 1614, "seek": 660676, "start": 6631.0, "end": 6633.64, "text": " And that prevented us from assigning zero probability to", "tokens": [51576, 400, 300, 27314, 505, 490, 49602, 4018, 8482, 281, 51708], "temperature": 0.0, "avg_logprob": -0.11450444891097698, "compression_ratio": 1.7573221757322175, "no_speech_prob": 0.0005112133803777397}, {"id": 1615, "seek": 663364, "start": 6634.360000000001, "end": 6636.360000000001, "text": " Um to any one bi-gram", "tokens": [50400, 3301, 281, 604, 472, 3228, 12, 1342, 50500], "temperature": 0.0, "avg_logprob": -0.15291817982991537, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.012819440104067326}, {"id": 1616, "seek": 663364, "start": 6637.240000000001, "end": 6639.56, "text": " Now if I increase the count here", "tokens": [50544, 823, 498, 286, 3488, 264, 1207, 510, 50660], "temperature": 0.0, "avg_logprob": -0.15291817982991537, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.012819440104067326}, {"id": 1617, "seek": 663364, "start": 6640.280000000001, "end": 6642.280000000001, "text": " What's happening to the probability?", "tokens": [50696, 708, 311, 2737, 281, 264, 8482, 30, 50796], "temperature": 0.0, "avg_logprob": -0.15291817982991537, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.012819440104067326}, {"id": 1618, "seek": 663364, "start": 6642.84, "end": 6647.240000000001, "text": " As I increase the count probability becomes more and more uniform", "tokens": [50824, 1018, 286, 3488, 264, 1207, 8482, 3643, 544, 293, 544, 9452, 51044], "temperature": 0.0, "avg_logprob": -0.15291817982991537, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.012819440104067326}, {"id": 1619, "seek": 663364, "start": 6647.96, "end": 6651.4800000000005, "text": " Right because these counts go only up to like 900 or whatever", "tokens": [51080, 1779, 570, 613, 14893, 352, 787, 493, 281, 411, 22016, 420, 2035, 51256], "temperature": 0.0, "avg_logprob": -0.15291817982991537, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.012819440104067326}, {"id": 1620, "seek": 663364, "start": 6651.4800000000005, "end": 6654.6, "text": " So if I'm adding plus a million to every single number here", "tokens": [51256, 407, 498, 286, 478, 5127, 1804, 257, 2459, 281, 633, 2167, 1230, 510, 51412], "temperature": 0.0, "avg_logprob": -0.15291817982991537, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.012819440104067326}, {"id": 1621, "seek": 663364, "start": 6655.160000000001, "end": 6658.84, "text": " You can see how uh the row and its probability then when we divide", "tokens": [51440, 509, 393, 536, 577, 2232, 264, 5386, 293, 1080, 8482, 550, 562, 321, 9845, 51624], "temperature": 0.0, "avg_logprob": -0.15291817982991537, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.012819440104067326}, {"id": 1622, "seek": 665884, "start": 6659.08, "end": 6663.96, "text": " Is just going to become more and more close to exactly even probability uniform distribution", "tokens": [50376, 1119, 445, 516, 281, 1813, 544, 293, 544, 1998, 281, 2293, 754, 8482, 9452, 7316, 50620], "temperature": 0.0, "avg_logprob": -0.09288957616785071, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0069029806181788445}, {"id": 1623, "seek": 665884, "start": 6665.16, "end": 6669.56, "text": " It turns out that the gradient based framework has an equivalent to smoothing", "tokens": [50680, 467, 4523, 484, 300, 264, 16235, 2361, 8388, 575, 364, 10344, 281, 899, 6259, 571, 50900], "temperature": 0.0, "avg_logprob": -0.09288957616785071, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0069029806181788445}, {"id": 1624, "seek": 665884, "start": 6670.76, "end": 6672.76, "text": " In particular", "tokens": [50960, 682, 1729, 51060], "temperature": 0.0, "avg_logprob": -0.09288957616785071, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0069029806181788445}, {"id": 1625, "seek": 665884, "start": 6673.16, "end": 6675.16, "text": " Think through these w's here", "tokens": [51080, 6557, 807, 613, 261, 311, 510, 51180], "temperature": 0.0, "avg_logprob": -0.09288957616785071, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0069029806181788445}, {"id": 1626, "seek": 665884, "start": 6675.88, "end": 6677.88, "text": " Which we initialized randomly", "tokens": [51216, 3013, 321, 5883, 1602, 16979, 51316], "temperature": 0.0, "avg_logprob": -0.09288957616785071, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0069029806181788445}, {"id": 1627, "seek": 665884, "start": 6678.52, "end": 6681.32, "text": " We could also think about initializing w's to be zero", "tokens": [51348, 492, 727, 611, 519, 466, 5883, 3319, 261, 311, 281, 312, 4018, 51488], "temperature": 0.0, "avg_logprob": -0.09288957616785071, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0069029806181788445}, {"id": 1628, "seek": 665884, "start": 6682.12, "end": 6684.12, "text": " If all the entries of w are zero", "tokens": [51528, 759, 439, 264, 23041, 295, 261, 366, 4018, 51628], "temperature": 0.0, "avg_logprob": -0.09288957616785071, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0069029806181788445}, {"id": 1629, "seek": 665884, "start": 6685.96, "end": 6688.12, "text": " Then you'll see that logits will become all zero", "tokens": [51720, 1396, 291, 603, 536, 300, 3565, 1208, 486, 1813, 439, 4018, 51828], "temperature": 0.0, "avg_logprob": -0.09288957616785071, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0069029806181788445}, {"id": 1630, "seek": 668884, "start": 6688.84, "end": 6691.16, "text": " And then exponentiating those logits becomes all one", "tokens": [50364, 400, 550, 37871, 72, 990, 729, 3565, 1208, 3643, 439, 472, 50480], "temperature": 0.0, "avg_logprob": -0.1265126150481555, "compression_ratio": 1.7974683544303798, "no_speech_prob": 0.0003514266572892666}, {"id": 1631, "seek": 668884, "start": 6692.12, "end": 6694.84, "text": " And then the probabilities turn out to be exactly uniform", "tokens": [50528, 400, 550, 264, 33783, 1261, 484, 281, 312, 2293, 9452, 50664], "temperature": 0.0, "avg_logprob": -0.1265126150481555, "compression_ratio": 1.7974683544303798, "no_speech_prob": 0.0003514266572892666}, {"id": 1632, "seek": 668884, "start": 6695.72, "end": 6700.04, "text": " So basically when w's are all equal to each other or say especially zero", "tokens": [50708, 407, 1936, 562, 261, 311, 366, 439, 2681, 281, 1184, 661, 420, 584, 2318, 4018, 50924], "temperature": 0.0, "avg_logprob": -0.1265126150481555, "compression_ratio": 1.7974683544303798, "no_speech_prob": 0.0003514266572892666}, {"id": 1633, "seek": 668884, "start": 6701.24, "end": 6703.4800000000005, "text": " Then the probabilities come out completely uniform", "tokens": [50984, 1396, 264, 33783, 808, 484, 2584, 9452, 51096], "temperature": 0.0, "avg_logprob": -0.1265126150481555, "compression_ratio": 1.7974683544303798, "no_speech_prob": 0.0003514266572892666}, {"id": 1634, "seek": 668884, "start": 6704.4400000000005, "end": 6705.4800000000005, "text": " So", "tokens": [51144, 407, 51196], "temperature": 0.0, "avg_logprob": -0.1265126150481555, "compression_ratio": 1.7974683544303798, "no_speech_prob": 0.0003514266572892666}, {"id": 1635, "seek": 668884, "start": 6705.4800000000005, "end": 6708.360000000001, "text": " Trying to incentivize w to be near zero", "tokens": [51196, 20180, 281, 35328, 1125, 261, 281, 312, 2651, 4018, 51340], "temperature": 0.0, "avg_logprob": -0.1265126150481555, "compression_ratio": 1.7974683544303798, "no_speech_prob": 0.0003514266572892666}, {"id": 1636, "seek": 668884, "start": 6709.24, "end": 6711.24, "text": " Is basically equivalent", "tokens": [51384, 1119, 1936, 10344, 51484], "temperature": 0.0, "avg_logprob": -0.1265126150481555, "compression_ratio": 1.7974683544303798, "no_speech_prob": 0.0003514266572892666}, {"id": 1637, "seek": 668884, "start": 6711.24, "end": 6718.04, "text": " To label smoothing and the more you incentivize that in a loss function the more smooth distribution you're going to achieve", "tokens": [51484, 1407, 7645, 899, 6259, 571, 293, 264, 544, 291, 35328, 1125, 300, 294, 257, 4470, 2445, 264, 544, 5508, 7316, 291, 434, 516, 281, 4584, 51824], "temperature": 0.0, "avg_logprob": -0.1265126150481555, "compression_ratio": 1.7974683544303798, "no_speech_prob": 0.0003514266572892666}, {"id": 1638, "seek": 671884, "start": 6718.92, "end": 6721.32, "text": " So this brings us to something that's called regularization", "tokens": [50368, 407, 341, 5607, 505, 281, 746, 300, 311, 1219, 3890, 2144, 50488], "temperature": 0.0, "avg_logprob": -0.0999126524295447, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.0016483055660501122}, {"id": 1639, "seek": 671884, "start": 6721.96, "end": 6727.72, "text": " Where we can actually augment the loss function to have a small component that we call a regularization loss", "tokens": [50520, 2305, 321, 393, 767, 29919, 264, 4470, 2445, 281, 362, 257, 1359, 6542, 300, 321, 818, 257, 3890, 2144, 4470, 50808], "temperature": 0.0, "avg_logprob": -0.0999126524295447, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.0016483055660501122}, {"id": 1640, "seek": 671884, "start": 6729.0, "end": 6733.72, "text": " In particular what we're going to do is we can take w and we can for example square all of its entries", "tokens": [50872, 682, 1729, 437, 321, 434, 516, 281, 360, 307, 321, 393, 747, 261, 293, 321, 393, 337, 1365, 3732, 439, 295, 1080, 23041, 51108], "temperature": 0.0, "avg_logprob": -0.0999126524295447, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.0016483055660501122}, {"id": 1641, "seek": 671884, "start": 6734.68, "end": 6736.68, "text": " And then we can um oops", "tokens": [51156, 400, 550, 321, 393, 1105, 34166, 51256], "temperature": 0.0, "avg_logprob": -0.0999126524295447, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.0016483055660501122}, {"id": 1642, "seek": 671884, "start": 6737.8, "end": 6739.0, "text": " Sorry about that", "tokens": [51312, 4919, 466, 300, 51372], "temperature": 0.0, "avg_logprob": -0.0999126524295447, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.0016483055660501122}, {"id": 1643, "seek": 671884, "start": 6739.0, "end": 6741.400000000001, "text": " We can take all the entries of w and we can sum them", "tokens": [51372, 492, 393, 747, 439, 264, 23041, 295, 261, 293, 321, 393, 2408, 552, 51492], "temperature": 0.0, "avg_logprob": -0.0999126524295447, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.0016483055660501122}, {"id": 1644, "seek": 671884, "start": 6743.64, "end": 6747.16, "text": " And because we're squaring uh, there will be no signs anymore", "tokens": [51604, 400, 570, 321, 434, 2339, 1921, 2232, 11, 456, 486, 312, 572, 7880, 3602, 51780], "temperature": 0.0, "avg_logprob": -0.0999126524295447, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.0016483055660501122}, {"id": 1645, "seek": 674716, "start": 6747.16, "end": 6748.36, "text": " Um", "tokens": [50364, 3301, 50424], "temperature": 0.0, "avg_logprob": -0.1255030632019043, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0027574740815907717}, {"id": 1646, "seek": 674716, "start": 6748.36, "end": 6750.92, "text": " negatives and positives all get squashed to be positive numbers", "tokens": [50424, 40019, 293, 35127, 439, 483, 2339, 12219, 281, 312, 3353, 3547, 50552], "temperature": 0.0, "avg_logprob": -0.1255030632019043, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0027574740815907717}, {"id": 1647, "seek": 674716, "start": 6751.48, "end": 6756.599999999999, "text": " And then the way this works is you achieve zero loss if w is exactly or zero", "tokens": [50580, 400, 550, 264, 636, 341, 1985, 307, 291, 4584, 4018, 4470, 498, 261, 307, 2293, 420, 4018, 50836], "temperature": 0.0, "avg_logprob": -0.1255030632019043, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0027574740815907717}, {"id": 1648, "seek": 674716, "start": 6757.16, "end": 6760.28, "text": " But if w has non-zero numbers you accumulate loss", "tokens": [50864, 583, 498, 261, 575, 2107, 12, 32226, 3547, 291, 33384, 4470, 51020], "temperature": 0.0, "avg_logprob": -0.1255030632019043, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0027574740815907717}, {"id": 1649, "seek": 674716, "start": 6761.16, "end": 6764.2, "text": " And so we can actually take this and we can add it on here", "tokens": [51064, 400, 370, 321, 393, 767, 747, 341, 293, 321, 393, 909, 309, 322, 510, 51216], "temperature": 0.0, "avg_logprob": -0.1255030632019043, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0027574740815907717}, {"id": 1650, "seek": 674716, "start": 6764.92, "end": 6767.8, "text": " So we can do something like loss plus", "tokens": [51252, 407, 321, 393, 360, 746, 411, 4470, 1804, 51396], "temperature": 0.0, "avg_logprob": -0.1255030632019043, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0027574740815907717}, {"id": 1651, "seek": 674716, "start": 6768.92, "end": 6770.44, "text": " w square", "tokens": [51452, 261, 3732, 51528], "temperature": 0.0, "avg_logprob": -0.1255030632019043, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0027574740815907717}, {"id": 1652, "seek": 674716, "start": 6770.44, "end": 6771.96, "text": " dot sum", "tokens": [51528, 5893, 2408, 51604], "temperature": 0.0, "avg_logprob": -0.1255030632019043, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0027574740815907717}, {"id": 1653, "seek": 674716, "start": 6771.96, "end": 6776.36, "text": " Or let's actually instead of some let's take a mean because otherwise the sum gets too large", "tokens": [51604, 1610, 718, 311, 767, 2602, 295, 512, 718, 311, 747, 257, 914, 570, 5911, 264, 2408, 2170, 886, 2416, 51824], "temperature": 0.0, "avg_logprob": -0.1255030632019043, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0027574740815907717}, {"id": 1654, "seek": 677716, "start": 6777.48, "end": 6779.48, "text": " So mean is like a little bit more manageable", "tokens": [50380, 407, 914, 307, 411, 257, 707, 857, 544, 38798, 50480], "temperature": 0.0, "avg_logprob": -0.10696271125306474, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.000939908844884485}, {"id": 1655, "seek": 677716, "start": 6781.32, "end": 6787.96, "text": " And then we have a regularization loss here like say 0.01 times or something like that you can choose the regularization strength", "tokens": [50572, 400, 550, 321, 362, 257, 3890, 2144, 4470, 510, 411, 584, 1958, 13, 10607, 1413, 420, 746, 411, 300, 291, 393, 2826, 264, 3890, 2144, 3800, 50904], "temperature": 0.0, "avg_logprob": -0.10696271125306474, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.000939908844884485}, {"id": 1656, "seek": 677716, "start": 6789.32, "end": 6791.32, "text": " And then we can just optimize this", "tokens": [50972, 400, 550, 321, 393, 445, 19719, 341, 51072], "temperature": 0.0, "avg_logprob": -0.10696271125306474, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.000939908844884485}, {"id": 1657, "seek": 677716, "start": 6792.12, "end": 6797.88, "text": " And now this optimization actually has two components not only is it trying to make all the probabilities work out", "tokens": [51112, 400, 586, 341, 19618, 767, 575, 732, 6677, 406, 787, 307, 309, 1382, 281, 652, 439, 264, 33783, 589, 484, 51400], "temperature": 0.0, "avg_logprob": -0.10696271125306474, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.000939908844884485}, {"id": 1658, "seek": 677716, "start": 6798.28, "end": 6803.4, "text": " But in addition to that there's an additional component that simultaneously tries to make all w's be zero", "tokens": [51420, 583, 294, 4500, 281, 300, 456, 311, 364, 4497, 6542, 300, 16561, 9898, 281, 652, 439, 261, 311, 312, 4018, 51676], "temperature": 0.0, "avg_logprob": -0.10696271125306474, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.000939908844884485}, {"id": 1659, "seek": 680340, "start": 6803.879999999999, "end": 6810.04, "text": " Because if w's are non-zero you feel a loss and so minimizing this the only way to achieve that is for w to be zero", "tokens": [50388, 1436, 498, 261, 311, 366, 2107, 12, 32226, 291, 841, 257, 4470, 293, 370, 46608, 341, 264, 787, 636, 281, 4584, 300, 307, 337, 261, 281, 312, 4018, 50696], "temperature": 0.0, "avg_logprob": -0.09416620307993666, "compression_ratio": 1.7865612648221343, "no_speech_prob": 0.01566177047789097}, {"id": 1660, "seek": 680340, "start": 6810.679999999999, "end": 6817.32, "text": " And so you can think of this as adding like a spring force or like a gravity force that that pushes w to be zero", "tokens": [50728, 400, 370, 291, 393, 519, 295, 341, 382, 5127, 411, 257, 5587, 3464, 420, 411, 257, 12110, 3464, 300, 300, 21020, 261, 281, 312, 4018, 51060], "temperature": 0.0, "avg_logprob": -0.09416620307993666, "compression_ratio": 1.7865612648221343, "no_speech_prob": 0.01566177047789097}, {"id": 1661, "seek": 680340, "start": 6817.799999999999, "end": 6820.92, "text": " So w wants to be zero and the probabilities want to be uniform", "tokens": [51084, 407, 261, 2738, 281, 312, 4018, 293, 264, 33783, 528, 281, 312, 9452, 51240], "temperature": 0.0, "avg_logprob": -0.09416620307993666, "compression_ratio": 1.7865612648221343, "no_speech_prob": 0.01566177047789097}, {"id": 1662, "seek": 680340, "start": 6821.4, "end": 6826.679999999999, "text": " But they also simultaneously want to match up your your probabilities as indicated by the data", "tokens": [51264, 583, 436, 611, 16561, 528, 281, 2995, 493, 428, 428, 33783, 382, 16176, 538, 264, 1412, 51528], "temperature": 0.0, "avg_logprob": -0.09416620307993666, "compression_ratio": 1.7865612648221343, "no_speech_prob": 0.01566177047789097}, {"id": 1663, "seek": 680340, "start": 6827.48, "end": 6830.2, "text": " And so the strength of this regularization", "tokens": [51568, 400, 370, 264, 3800, 295, 341, 3890, 2144, 51704], "temperature": 0.0, "avg_logprob": -0.09416620307993666, "compression_ratio": 1.7865612648221343, "no_speech_prob": 0.01566177047789097}, {"id": 1664, "seek": 680340, "start": 6830.759999999999, "end": 6832.759999999999, "text": " is exactly controlling", "tokens": [51732, 307, 2293, 14905, 51832], "temperature": 0.0, "avg_logprob": -0.09416620307993666, "compression_ratio": 1.7865612648221343, "no_speech_prob": 0.01566177047789097}, {"id": 1665, "seek": 683276, "start": 6832.76, "end": 6834.4400000000005, "text": " the amount of counts", "tokens": [50364, 264, 2372, 295, 14893, 50448], "temperature": 0.0, "avg_logprob": -0.1318341060118242, "compression_ratio": 1.736040609137056, "no_speech_prob": 0.001573048997670412}, {"id": 1666, "seek": 683276, "start": 6834.4400000000005, "end": 6836.4400000000005, "text": " that you add here", "tokens": [50448, 300, 291, 909, 510, 50548], "temperature": 0.0, "avg_logprob": -0.1318341060118242, "compression_ratio": 1.736040609137056, "no_speech_prob": 0.001573048997670412}, {"id": 1667, "seek": 683276, "start": 6837.24, "end": 6839.24, "text": " Adding a lot more counts", "tokens": [50588, 31204, 257, 688, 544, 14893, 50688], "temperature": 0.0, "avg_logprob": -0.1318341060118242, "compression_ratio": 1.736040609137056, "no_speech_prob": 0.001573048997670412}, {"id": 1668, "seek": 683276, "start": 6839.400000000001, "end": 6840.84, "text": " here", "tokens": [50696, 510, 50768], "temperature": 0.0, "avg_logprob": -0.1318341060118242, "compression_ratio": 1.736040609137056, "no_speech_prob": 0.001573048997670412}, {"id": 1669, "seek": 683276, "start": 6840.84, "end": 6842.84, "text": " corresponds to", "tokens": [50768, 23249, 281, 50868], "temperature": 0.0, "avg_logprob": -0.1318341060118242, "compression_ratio": 1.736040609137056, "no_speech_prob": 0.001573048997670412}, {"id": 1670, "seek": 683276, "start": 6842.84, "end": 6844.68, "text": " Increasing this number", "tokens": [50868, 30367, 3349, 341, 1230, 50960], "temperature": 0.0, "avg_logprob": -0.1318341060118242, "compression_ratio": 1.736040609137056, "no_speech_prob": 0.001573048997670412}, {"id": 1671, "seek": 683276, "start": 6844.68, "end": 6849.08, "text": " Because the more you increase it the more this part of the loss function dominates this part", "tokens": [50960, 1436, 264, 544, 291, 3488, 309, 264, 544, 341, 644, 295, 264, 4470, 2445, 8859, 1024, 341, 644, 51180], "temperature": 0.0, "avg_logprob": -0.1318341060118242, "compression_ratio": 1.736040609137056, "no_speech_prob": 0.001573048997670412}, {"id": 1672, "seek": 683276, "start": 6849.56, "end": 6854.68, "text": " And the more these these weights will be unable to grow because as they grow", "tokens": [51204, 400, 264, 544, 613, 613, 17443, 486, 312, 11299, 281, 1852, 570, 382, 436, 1852, 51460], "temperature": 0.0, "avg_logprob": -0.1318341060118242, "compression_ratio": 1.736040609137056, "no_speech_prob": 0.001573048997670412}, {"id": 1673, "seek": 683276, "start": 6855.4800000000005, "end": 6857.4800000000005, "text": " They accumulate way too much loss", "tokens": [51500, 814, 33384, 636, 886, 709, 4470, 51600], "temperature": 0.0, "avg_logprob": -0.1318341060118242, "compression_ratio": 1.736040609137056, "no_speech_prob": 0.001573048997670412}, {"id": 1674, "seek": 683276, "start": 6858.4400000000005, "end": 6860.4400000000005, "text": " And so if this is strong enough", "tokens": [51648, 400, 370, 498, 341, 307, 2068, 1547, 51748], "temperature": 0.0, "avg_logprob": -0.1318341060118242, "compression_ratio": 1.736040609137056, "no_speech_prob": 0.001573048997670412}, {"id": 1675, "seek": 686044, "start": 6861.24, "end": 6865.639999999999, "text": " Then we are not able to overcome the force of this loss and we will never", "tokens": [50404, 1396, 321, 366, 406, 1075, 281, 10473, 264, 3464, 295, 341, 4470, 293, 321, 486, 1128, 50624], "temperature": 0.0, "avg_logprob": -0.12761449813842773, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0024723834358155727}, {"id": 1676, "seek": 686044, "start": 6866.759999999999, "end": 6868.759999999999, "text": " And basically everything will be uniform predictions", "tokens": [50680, 400, 1936, 1203, 486, 312, 9452, 21264, 50780], "temperature": 0.0, "avg_logprob": -0.12761449813842773, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0024723834358155727}, {"id": 1677, "seek": 686044, "start": 6869.4, "end": 6872.44, "text": " So I thought that's kind of cool. Okay, and lastly before we wrap up", "tokens": [50812, 407, 286, 1194, 300, 311, 733, 295, 1627, 13, 1033, 11, 293, 16386, 949, 321, 7019, 493, 50964], "temperature": 0.0, "avg_logprob": -0.12761449813842773, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0024723834358155727}, {"id": 1678, "seek": 686044, "start": 6873.16, "end": 6876.04, "text": " I wanted to show you how you would sample from this neural net model", "tokens": [51000, 286, 1415, 281, 855, 291, 577, 291, 576, 6889, 490, 341, 18161, 2533, 2316, 51144], "temperature": 0.0, "avg_logprob": -0.12761449813842773, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0024723834358155727}, {"id": 1679, "seek": 686044, "start": 6876.919999999999, "end": 6879.799999999999, "text": " And I copy pasted the sampling code from before", "tokens": [51188, 400, 286, 5055, 1791, 292, 264, 21179, 3089, 490, 949, 51332], "temperature": 0.0, "avg_logprob": -0.12761449813842773, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0024723834358155727}, {"id": 1680, "seek": 686044, "start": 6880.759999999999, "end": 6883.639999999999, "text": " Where remember that we sampled five times", "tokens": [51380, 2305, 1604, 300, 321, 3247, 15551, 1732, 1413, 51524], "temperature": 0.0, "avg_logprob": -0.12761449813842773, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0024723834358155727}, {"id": 1681, "seek": 686044, "start": 6884.28, "end": 6889.639999999999, "text": " And all we did is we started zero we grabbed the current ix row of p", "tokens": [51556, 400, 439, 321, 630, 307, 321, 1409, 4018, 321, 18607, 264, 2190, 741, 87, 5386, 295, 280, 51824], "temperature": 0.0, "avg_logprob": -0.12761449813842773, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0024723834358155727}, {"id": 1682, "seek": 689044, "start": 6890.44, "end": 6892.44, "text": " And that was our probability row", "tokens": [50364, 400, 300, 390, 527, 8482, 5386, 50464], "temperature": 0.0, "avg_logprob": -0.12901107033530435, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0002823979884851724}, {"id": 1683, "seek": 689044, "start": 6892.44, "end": 6898.12, "text": " From which we sampled the next index and just accumulated that and break when zero", "tokens": [50464, 3358, 597, 321, 3247, 15551, 264, 958, 8186, 293, 445, 31346, 300, 293, 1821, 562, 4018, 50748], "temperature": 0.0, "avg_logprob": -0.12901107033530435, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0002823979884851724}, {"id": 1684, "seek": 689044, "start": 6898.919999999999, "end": 6901.879999999999, "text": " And running this gave us these results", "tokens": [50788, 400, 2614, 341, 2729, 505, 613, 3542, 50936], "temperature": 0.0, "avg_logprob": -0.12901107033530435, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0002823979884851724}, {"id": 1685, "seek": 689044, "start": 6903.879999999999, "end": 6907.879999999999, "text": " I still have the p in memory. So this is fine now", "tokens": [51036, 286, 920, 362, 264, 280, 294, 4675, 13, 407, 341, 307, 2489, 586, 51236], "temperature": 0.0, "avg_logprob": -0.12901107033530435, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0002823979884851724}, {"id": 1686, "seek": 689044, "start": 6909.16, "end": 6913.48, "text": " The speed doesn't come from the row of p instead. It comes from this neural net", "tokens": [51300, 440, 3073, 1177, 380, 808, 490, 264, 5386, 295, 280, 2602, 13, 467, 1487, 490, 341, 18161, 2533, 51516], "temperature": 0.0, "avg_logprob": -0.12901107033530435, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0002823979884851724}, {"id": 1687, "seek": 689044, "start": 6915.0, "end": 6917.0, "text": " First we take ix", "tokens": [51592, 2386, 321, 747, 741, 87, 51692], "temperature": 0.0, "avg_logprob": -0.12901107033530435, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0002823979884851724}, {"id": 1688, "seek": 689044, "start": 6917.08, "end": 6919.96, "text": " And we encode it into a one-hot row", "tokens": [51696, 400, 321, 2058, 1429, 309, 666, 257, 472, 12, 12194, 5386, 51840], "temperature": 0.0, "avg_logprob": -0.12901107033530435, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0002823979884851724}, {"id": 1689, "seek": 692044, "start": 6920.44, "end": 6922.44, "text": " of xank", "tokens": [50364, 295, 2031, 657, 50464], "temperature": 0.0, "avg_logprob": -0.15270529660311613, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.001810051267966628}, {"id": 1690, "seek": 692044, "start": 6922.44, "end": 6924.44, "text": " This xank multiplies our w", "tokens": [50464, 639, 2031, 657, 12788, 530, 527, 261, 50564], "temperature": 0.0, "avg_logprob": -0.15270529660311613, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.001810051267966628}, {"id": 1691, "seek": 692044, "start": 6925.16, "end": 6929.879999999999, "text": " Which really just plugs out the row of w corresponding to ix really that's what's happening", "tokens": [50600, 3013, 534, 445, 33899, 484, 264, 5386, 295, 261, 11760, 281, 741, 87, 534, 300, 311, 437, 311, 2737, 50836], "temperature": 0.0, "avg_logprob": -0.15270529660311613, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.001810051267966628}, {"id": 1692, "seek": 692044, "start": 6930.44, "end": 6934.2, "text": " And that gets our logits and then we normalize those logits", "tokens": [50864, 400, 300, 2170, 527, 3565, 1208, 293, 550, 321, 2710, 1125, 729, 3565, 1208, 51052], "temperature": 0.0, "avg_logprob": -0.15270529660311613, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.001810051267966628}, {"id": 1693, "seek": 692044, "start": 6934.919999999999, "end": 6940.679999999999, "text": " Exponentiate to get counts and then normalize to get the distribution and then we can sample from the distribution", "tokens": [51088, 21391, 266, 23012, 473, 281, 483, 14893, 293, 550, 2710, 1125, 281, 483, 264, 7316, 293, 550, 321, 393, 6889, 490, 264, 7316, 51376], "temperature": 0.0, "avg_logprob": -0.15270529660311613, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.001810051267966628}, {"id": 1694, "seek": 692044, "start": 6941.32, "end": 6943.32, "text": " So if I run this", "tokens": [51408, 407, 498, 286, 1190, 341, 51508], "temperature": 0.0, "avg_logprob": -0.15270529660311613, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.001810051267966628}, {"id": 1695, "seek": 692044, "start": 6945.24, "end": 6949.879999999999, "text": " Kind of anti-climatic or climatic depending how you look at it, but we get the exact same result", "tokens": [51604, 9242, 295, 6061, 12, 3474, 332, 2399, 420, 5644, 2399, 5413, 577, 291, 574, 412, 309, 11, 457, 321, 483, 264, 1900, 912, 1874, 51836], "temperature": 0.0, "avg_logprob": -0.15270529660311613, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.001810051267966628}, {"id": 1696, "seek": 695044, "start": 6951.16, "end": 6952.28, "text": " Um", "tokens": [50400, 3301, 50456], "temperature": 0.0, "avg_logprob": -0.12315131936754499, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0026313771959394217}, {"id": 1697, "seek": 695044, "start": 6952.28, "end": 6956.36, "text": " And that's because this is in the identical model. Not only does it achieve the same loss", "tokens": [50456, 400, 300, 311, 570, 341, 307, 294, 264, 14800, 2316, 13, 1726, 787, 775, 309, 4584, 264, 912, 4470, 50660], "temperature": 0.0, "avg_logprob": -0.12315131936754499, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0026313771959394217}, {"id": 1698, "seek": 695044, "start": 6957.0, "end": 6958.12, "text": " but", "tokens": [50692, 457, 50748], "temperature": 0.0, "avg_logprob": -0.12315131936754499, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0026313771959394217}, {"id": 1699, "seek": 695044, "start": 6958.12, "end": 6963.799999999999, "text": " As I mentioned, these are identical models and this w is the log counts of what we've estimated before", "tokens": [50748, 1018, 286, 2835, 11, 613, 366, 14800, 5245, 293, 341, 261, 307, 264, 3565, 14893, 295, 437, 321, 600, 14109, 949, 51032], "temperature": 0.0, "avg_logprob": -0.12315131936754499, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0026313771959394217}, {"id": 1700, "seek": 695044, "start": 6964.2, "end": 6969.08, "text": " But we came to this answer in a very different way and it's got a very different interpretation", "tokens": [51052, 583, 321, 1361, 281, 341, 1867, 294, 257, 588, 819, 636, 293, 309, 311, 658, 257, 588, 819, 14174, 51296], "temperature": 0.0, "avg_logprob": -0.12315131936754499, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0026313771959394217}, {"id": 1701, "seek": 695044, "start": 6969.4, "end": 6973.5599999999995, "text": " But fundamentally, this is basically the same model and gives the same samples here. And so", "tokens": [51312, 583, 17879, 11, 341, 307, 1936, 264, 912, 2316, 293, 2709, 264, 912, 10938, 510, 13, 400, 370, 51520], "temperature": 0.0, "avg_logprob": -0.12315131936754499, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0026313771959394217}, {"id": 1702, "seek": 695044, "start": 6974.839999999999, "end": 6977.48, "text": " That's kind of cool. Okay, so we've actually covered a lot of ground", "tokens": [51584, 663, 311, 733, 295, 1627, 13, 1033, 11, 370, 321, 600, 767, 5343, 257, 688, 295, 2727, 51716], "temperature": 0.0, "avg_logprob": -0.12315131936754499, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0026313771959394217}, {"id": 1703, "seek": 697748, "start": 6978.04, "end": 6981.24, "text": " We introduced the bi-gram character level language model", "tokens": [50392, 492, 7268, 264, 3228, 12, 1342, 2517, 1496, 2856, 2316, 50552], "temperature": 0.0, "avg_logprob": -0.13135818072727748, "compression_ratio": 1.9450980392156862, "no_speech_prob": 0.006487235426902771}, {"id": 1704, "seek": 697748, "start": 6982.04, "end": 6987.639999999999, "text": " We saw how we can train the model how we can sample from the model and how we can evaluate the quality of the model", "tokens": [50592, 492, 1866, 577, 321, 393, 3847, 264, 2316, 577, 321, 393, 6889, 490, 264, 2316, 293, 577, 321, 393, 13059, 264, 3125, 295, 264, 2316, 50872], "temperature": 0.0, "avg_logprob": -0.13135818072727748, "compression_ratio": 1.9450980392156862, "no_speech_prob": 0.006487235426902771}, {"id": 1705, "seek": 697748, "start": 6987.879999999999, "end": 6989.879999999999, "text": " Using the negative log likelihood loss", "tokens": [50884, 11142, 264, 3671, 3565, 22119, 4470, 50984], "temperature": 0.0, "avg_logprob": -0.13135818072727748, "compression_ratio": 1.9450980392156862, "no_speech_prob": 0.006487235426902771}, {"id": 1706, "seek": 697748, "start": 6990.2, "end": 6995.5599999999995, "text": " And then we actually trained the model in two completely different ways that actually get the same result and the same model", "tokens": [51000, 400, 550, 321, 767, 8895, 264, 2316, 294, 732, 2584, 819, 2098, 300, 767, 483, 264, 912, 1874, 293, 264, 912, 2316, 51268], "temperature": 0.0, "avg_logprob": -0.13135818072727748, "compression_ratio": 1.9450980392156862, "no_speech_prob": 0.006487235426902771}, {"id": 1707, "seek": 697748, "start": 6996.28, "end": 7000.839999999999, "text": " In the first way, we just counted up the frequency of all the bi-grams and normalized", "tokens": [51304, 682, 264, 700, 636, 11, 321, 445, 20150, 493, 264, 7893, 295, 439, 264, 3228, 12, 1342, 82, 293, 48704, 51532], "temperature": 0.0, "avg_logprob": -0.13135818072727748, "compression_ratio": 1.9450980392156862, "no_speech_prob": 0.006487235426902771}, {"id": 1708, "seek": 697748, "start": 7001.48, "end": 7007.16, "text": " In the second way, we used the uh negative log likelihood loss as a guide", "tokens": [51564, 682, 264, 1150, 636, 11, 321, 1143, 264, 2232, 3671, 3565, 22119, 4470, 382, 257, 5934, 51848], "temperature": 0.0, "avg_logprob": -0.13135818072727748, "compression_ratio": 1.9450980392156862, "no_speech_prob": 0.006487235426902771}, {"id": 1709, "seek": 700748, "start": 7007.719999999999, "end": 7009.719999999999, "text": " To optimizing the counts matrix", "tokens": [50376, 1407, 40425, 264, 14893, 8141, 50476], "temperature": 0.0, "avg_logprob": -0.11693507846039121, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0007321184384636581}, {"id": 1710, "seek": 700748, "start": 7010.839999999999, "end": 7015.48, "text": " Or the counts array so that the loss is minimized in the in a gradient based framework", "tokens": [50532, 1610, 264, 14893, 10225, 370, 300, 264, 4470, 307, 4464, 1602, 294, 264, 294, 257, 16235, 2361, 8388, 50764], "temperature": 0.0, "avg_logprob": -0.11693507846039121, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0007321184384636581}, {"id": 1711, "seek": 700748, "start": 7015.799999999999, "end": 7017.799999999999, "text": " and we saw that both of them give the same result", "tokens": [50780, 293, 321, 1866, 300, 1293, 295, 552, 976, 264, 912, 1874, 50880], "temperature": 0.0, "avg_logprob": -0.11693507846039121, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0007321184384636581}, {"id": 1712, "seek": 700748, "start": 7018.44, "end": 7020.36, "text": " and", "tokens": [50912, 293, 51008], "temperature": 0.0, "avg_logprob": -0.11693507846039121, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0007321184384636581}, {"id": 1713, "seek": 700748, "start": 7020.36, "end": 7021.4, "text": " That's it", "tokens": [51008, 663, 311, 309, 51060], "temperature": 0.0, "avg_logprob": -0.11693507846039121, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0007321184384636581}, {"id": 1714, "seek": 700748, "start": 7021.4, "end": 7024.44, "text": " Now the second one of these the gradient based framework is much more flexible", "tokens": [51060, 823, 264, 1150, 472, 295, 613, 264, 16235, 2361, 8388, 307, 709, 544, 11358, 51212], "temperature": 0.0, "avg_logprob": -0.11693507846039121, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0007321184384636581}, {"id": 1715, "seek": 700748, "start": 7024.919999999999, "end": 7027.4, "text": " And right now our neural network is super simple", "tokens": [51236, 400, 558, 586, 527, 18161, 3209, 307, 1687, 2199, 51360], "temperature": 0.0, "avg_logprob": -0.11693507846039121, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0007321184384636581}, {"id": 1716, "seek": 700748, "start": 7027.5599999999995, "end": 7033.4, "text": " We're taking a single previous character and we're taking it through a single linear layer to calculate the logits", "tokens": [51368, 492, 434, 1940, 257, 2167, 3894, 2517, 293, 321, 434, 1940, 309, 807, 257, 2167, 8213, 4583, 281, 8873, 264, 3565, 1208, 51660], "temperature": 0.0, "avg_logprob": -0.11693507846039121, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0007321184384636581}, {"id": 1717, "seek": 703340, "start": 7034.12, "end": 7039.719999999999, "text": " This is about to complexify. So in the follow-up videos, we're going to be taking more and more of these characters", "tokens": [50400, 639, 307, 466, 281, 3997, 2505, 13, 407, 294, 264, 1524, 12, 1010, 2145, 11, 321, 434, 516, 281, 312, 1940, 544, 293, 544, 295, 613, 4342, 50680], "temperature": 0.0, "avg_logprob": -0.1551021416982015, "compression_ratio": 1.903345724907063, "no_speech_prob": 0.009857830591499805}, {"id": 1718, "seek": 703340, "start": 7040.5199999999995, "end": 7042.44, "text": " And we're going to be feeding them into a neural net", "tokens": [50720, 400, 321, 434, 516, 281, 312, 12919, 552, 666, 257, 18161, 2533, 50816], "temperature": 0.0, "avg_logprob": -0.1551021416982015, "compression_ratio": 1.903345724907063, "no_speech_prob": 0.009857830591499805}, {"id": 1719, "seek": 703340, "start": 7042.92, "end": 7047.08, "text": " But this neural net will still output the exact same thing. The neural net will output logits", "tokens": [50840, 583, 341, 18161, 2533, 486, 920, 5598, 264, 1900, 912, 551, 13, 440, 18161, 2533, 486, 5598, 3565, 1208, 51048], "temperature": 0.0, "avg_logprob": -0.1551021416982015, "compression_ratio": 1.903345724907063, "no_speech_prob": 0.009857830591499805}, {"id": 1720, "seek": 703340, "start": 7048.04, "end": 7052.759999999999, "text": " And these logits will still be normalized in the exact same way and all the loss and everything else and the gradient", "tokens": [51096, 400, 613, 3565, 1208, 486, 920, 312, 48704, 294, 264, 1900, 912, 636, 293, 439, 264, 4470, 293, 1203, 1646, 293, 264, 16235, 51332], "temperature": 0.0, "avg_logprob": -0.1551021416982015, "compression_ratio": 1.903345724907063, "no_speech_prob": 0.009857830591499805}, {"id": 1721, "seek": 703340, "start": 7052.839999999999, "end": 7057.719999999999, "text": " Gradient based framework everything stays identical. It's just that this neural net will now", "tokens": [51336, 16710, 1196, 2361, 8388, 1203, 10834, 14800, 13, 467, 311, 445, 300, 341, 18161, 2533, 486, 586, 51580], "temperature": 0.0, "avg_logprob": -0.1551021416982015, "compression_ratio": 1.903345724907063, "no_speech_prob": 0.009857830591499805}, {"id": 1722, "seek": 703340, "start": 7058.139999999999, "end": 7060.139999999999, "text": " Complexify all the way to transformers", "tokens": [51601, 41184, 2505, 439, 264, 636, 281, 4088, 433, 51701], "temperature": 0.0, "avg_logprob": -0.1551021416982015, "compression_ratio": 1.903345724907063, "no_speech_prob": 0.009857830591499805}, {"id": 1723, "seek": 706014, "start": 7060.14, "end": 7064.3, "text": " So that's going to be pretty awesome and i'm looking forward to it for now. Bye", "tokens": [50384, 407, 300, 311, 516, 281, 312, 1238, 3476, 293, 741, 478, 1237, 2128, 281, 309, 337, 586, 13, 4621, 50572], "temperature": 0.0, "avg_logprob": -0.18560595945878464, "compression_ratio": 1.025974025974026, "no_speech_prob": 0.006190218031406403}], "language": "en"}