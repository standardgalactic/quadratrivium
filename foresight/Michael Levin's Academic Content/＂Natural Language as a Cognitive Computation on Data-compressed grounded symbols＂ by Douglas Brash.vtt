WEBVTT

00:00.000 --> 00:20.000
Okay. All right. So what I'm going to be talking about is how what natural language is, I think, a cognitive computation rather than a, say, purely linguistic one, which is probably why it's relevant to you guys.

00:20.000 --> 00:31.000
And everybody, I think, would agree that it has to do with symbols, although I find in linguistics, you could find somebody who'll argue about anything.

00:31.000 --> 00:45.000
And so whether we have internal representations, for example, at all. And then what I'm going to point out is that these symbols have been data compressed, and that's why language looks as complicated as it does.

00:45.000 --> 00:57.000
Frankly, if it weren't for that, probably everything I'm going to say could have been discovered by Aristotle. And it's just that I think nobody noticed these things.

00:57.000 --> 01:10.000
And then a implication of this would be, we'll get to the end and it allows computers, I think, to be to do cognition. And I'm a little worried that AI is going to stumble across this accident.

01:10.000 --> 01:26.000
So first, I'd like to do two warm up exercises, very brief. One, I'm going to say a sentence and ask yourself, are you having any trouble at all with this? So he saw the saw that I saw.

01:26.000 --> 01:34.000
Anybody have trouble with it? I think it's slid right down, right? So that's actually a little bit remarkable.

01:34.000 --> 01:50.000
Now, the second warm up question, we're going to take warm up exercise, we're going to take 10 seconds. And I want you to think, what are the parts of a chair?

01:50.000 --> 02:00.000
Okay, now, did anybody have in their list, glue, nuts, bolts or screws?

02:00.000 --> 02:10.000
Okay, this will become relevant later. But I want to want you to do it before you get biased towards thinking about things like that.

02:10.000 --> 02:22.000
So this is all part of what I think of as the Descartes-Cahal problem. You're starting out here, somehow you end up with this.

02:22.000 --> 02:35.000
And I'm not going to talk about how Shakespeare could possibly do this. I'm going to talk about how he could tell you these words and you understand what he's thinking.

02:35.000 --> 02:42.000
But in the course of that, we're going to be getting back to how does Shakespeare do this in the first place?

02:42.000 --> 02:51.000
So as far as understanding language, I think it's a much more amazing algorithm, whatever it is we do, than people really appreciate.

02:51.000 --> 02:55.000
And so I want to start off a little bit slowly and point out a few things.

02:55.000 --> 03:02.000
One is, I think it's a computation executed by neurons, most of us biologists would agree on that.

03:02.000 --> 03:13.000
And what you're doing is one person has this three-dimensional, instantaneous, simultaneous picture in his head about something about the way the world's going around.

03:13.000 --> 03:25.000
But we're going to transfer that 3D picture to somebody else using a linear string of symbols that are ambiguous, because words usually all have multiple meanings.

03:25.000 --> 03:35.000
And their arbitrary symbols, like the word cat has C-A-T, has no relation to a cat with two ears and a tail whatsoever.

03:35.000 --> 03:42.000
And the only relationship between these symbols is adjacency and succession.

03:42.000 --> 03:50.000
And somehow that doesn't trigger a bundle of associations in the listener's brain.

03:50.000 --> 03:56.000
It triggers, it assembles the coherent picture of the world, the one that the speaker had.

03:56.000 --> 03:58.000
So how do you pull that off?

03:58.000 --> 04:14.000
Well, the solution in the 20th century, starting with Bar-Hell-L, actually, and also Chomsky, was that language has three main points, and then the rest of the talk is going to be replacing each of these with something else.

04:14.000 --> 04:27.000
First, that language processes arbitrary symbols, unrelated to the thing they're talking about, and it processes them using syntactic rules that don't have anything to do with the meaning of those words.

04:27.000 --> 04:35.000
Second, the function, the language structure is dyadic, function argument like a mathematics.

04:35.000 --> 04:40.000
So there is a subject and a complement in a sentence.

04:40.000 --> 04:43.000
And the complement has a verb and an object.

04:43.000 --> 04:47.000
So each of these two parts, and it goes on in steadily more detail.

04:47.000 --> 04:55.000
And third, that the language processes is determined completely by the words that are sitting on the page.

04:55.000 --> 04:59.000
And I'm going to argue that none of these is true.

04:59.000 --> 05:07.000
And once you see through it, then it becomes simple and regular.

05:07.000 --> 05:16.000
And in fact, before the 20th century, there was a history that meaning did matter.

05:16.000 --> 05:23.000
There was a history that language structure had made of three parts, not two parts.

05:23.000 --> 05:25.000
It went by the wayside with Chomsky.

05:25.000 --> 05:32.000
And there are a lot of clever things that come about from transformational grammar.

05:32.000 --> 05:43.000
But I think the active effort in linguistic to suppress any other thinking has led to the problem.

05:43.000 --> 05:49.000
So, first point, I'd say that language symbols are grounded, not arbitrary.

05:49.000 --> 06:01.000
Grounded meaning that the symbol itself has some physical resemblance to the thing that it's talking about or that it's representing.

06:01.000 --> 06:03.000
But I'll get to that part in a minute.

06:03.000 --> 06:06.000
So an arbitrary symbol, so we have a sentence here.

06:06.000 --> 06:14.000
And the standard view, but here's the arbitrary symbol cats, and it points to a thing out in the real world of cats.

06:14.000 --> 06:17.000
My points to me and the dog points to the dog.

06:17.000 --> 06:23.000
And there's something that we're pointing to with the word chases.

06:23.000 --> 06:28.000
So that's thing out in the real world of the reference.

06:28.000 --> 06:32.000
So there are a number of attractions to this way of looking at things.

06:32.000 --> 06:36.000
So symbolic computations require a few things.

06:36.000 --> 06:38.000
They require variables.

06:38.000 --> 06:40.000
So the word dog can stand for any number of dogs.

06:40.000 --> 06:43.000
It can get instantiated by different dogs.

06:43.000 --> 06:50.000
There's a directional structure rather than just being an aggregate of stuff or a clustering.

06:50.000 --> 06:58.000
There's some hierarchies, you know, like my dog is a unit that's a subunit of this whole thing.

06:58.000 --> 07:01.000
And these symbols remain particular.

07:01.000 --> 07:04.000
It's not like red and white giving pink.

07:04.000 --> 07:11.000
The dog stays the dog and the cat stays the cat, which is a non trivial property of language.

07:11.000 --> 07:19.000
And the thing that Chomsky introduced is basically the idea that you want somewhere or somehow that the dog's chasing the cat.

07:19.000 --> 07:23.000
So somehow cats has to get here.

07:23.000 --> 07:25.000
Oh, can you guys see my pointer?

07:25.000 --> 07:28.000
Does that work? Okay, good. Thanks.

07:28.000 --> 07:33.000
And so Chomsky do attention to that problem and solve that problem.

07:33.000 --> 07:40.000
And actually much of what I'm going to say will wind up deriving a lot of the early transformational grammar,

07:40.000 --> 07:47.000
but from a cognitive basis rather than a linguistic basis and with a whole lot less ad hoc stuff.

07:47.000 --> 07:55.000
Now, there's a problem though, which is, well, actually, let me point out the problem.

07:55.000 --> 08:00.000
There's nothing in these words that says how to assemble this picture.

08:00.000 --> 08:03.000
So you've got these reference with how do you assemble it?

08:03.000 --> 08:14.000
It gets a little better if you are willing to agree that we have in our heads internal representation, which are these little thought balloons here.

08:14.000 --> 08:26.000
And that, which by the way is also something that people argue, you know, there's a whole school of thought says, oh, no, we don't have internal representations.

08:26.000 --> 08:34.000
But still there's nothing here that says how to assemble these internal representations either.

08:34.000 --> 08:43.000
So what happens if we take seriously the grounding that each one of these symbols is grounded somehow and something in the real world.

08:43.000 --> 08:50.000
In other words, physically represents it, whatever it is that's in our heads that represents a dog.

08:50.000 --> 08:53.000
Then you're in a little better shape.

08:53.000 --> 08:59.000
Oh, well, this is what I was saying is that if you don't assemble them, you just get this cluster of images.

08:59.000 --> 09:03.000
Oh, and this word ambiguity is significant.

09:03.000 --> 09:10.000
So 12 word sentence people have shown that can have between 10 to the fifth and 10 to the 28 potential parses.

09:10.000 --> 09:16.000
There's only 10 to the 60th atoms in the universe last time I saw a number.

09:16.000 --> 09:24.000
So that's a problem that's generally not addressed in linguistics.

09:24.000 --> 09:37.000
And, you know, in the 70s, 80s, 90s, the link places like IBM were trying to write programs that would use Chomsky and linguistics process language and they failed.

09:37.000 --> 09:45.000
And the famous joke is that one of the managers at IBM said that every time he fires a linguist the recognition rate goes up.

09:45.000 --> 09:53.000
So while he's definitely on to something, it's not the solution to the problem.

09:53.000 --> 09:57.000
And certainly no hand is towards going on biological.

09:57.000 --> 10:04.000
Now, if you take seriously the ground symbols, then you're doing computations on these representations.

10:04.000 --> 10:10.000
And that allows you to do two things that hard at pointed out a long time ago are critical for cognition.

10:10.000 --> 10:16.000
You have to be able to compare two representations and distinguish between them.

10:16.000 --> 10:19.000
Or you compare them and look for similarities.

10:19.000 --> 10:27.000
Let's you do discrimination on the one hand, let's you do category categorization and generalization on the other.

10:27.000 --> 10:32.000
So that you have this idea of dog independent of any particular dog.

10:32.000 --> 10:35.000
Without that, you can't do cognition.

10:35.000 --> 10:41.000
There's nothing about the word cats down here that I can compute with the word dog.

10:41.000 --> 10:49.000
That's going to let me tell you whether they have the same number of years can't be done the information not there.

10:49.000 --> 10:53.000
But it is up here if you have grounded symbols.

10:53.000 --> 10:56.000
So these are sometimes called icons.

10:56.000 --> 10:59.000
The classical one would be say a pedestrian crossing sign.

10:59.000 --> 11:03.000
It looks a little bit like a pedestrian crossing the street.

11:03.000 --> 11:10.000
Okay, so there's some correspondence and now you can do computations on them.

11:10.000 --> 11:22.000
So, oh, there is a problem, which is that we still remember we still want to not just point to these individual representations.

11:22.000 --> 11:25.000
We want to assemble them into higher things.

11:25.000 --> 11:36.000
So you want to assemble these two into my dog and you want to assemble the whole thing into something about cats and dogs chasing.

11:36.000 --> 11:40.000
And you also have to get cats over here somehow.

11:40.000 --> 11:43.000
So you've got to do a computation.

11:43.000 --> 11:54.000
But if the representations are grounded, there's no guarantee that your computations on them are still going to be grounded and therefore give you a valid result.

11:54.000 --> 12:03.000
So I would say the first thing I'm going to add is that, okay, whatever the language rules are, they also have to be grounded.

12:03.000 --> 12:10.000
And so that grounded this is preserved when you're doing computations on the individual grounded symbols.

12:10.000 --> 12:23.000
So the solution to that I would say got two parts once the grounded rule hypothesis, the speaker and the listener have operations on the arbitrary symbols of words.

12:23.000 --> 12:33.000
That are grounded in the external world so that they're manipulating those words in the same way that the representations would be manipulated.

12:33.000 --> 12:44.000
Or if you prefer, they actually are manipulating your representations, which is the more radical proposal, but that's, and I think that's what's going on, but I'm not going to demand that just yet.

12:44.000 --> 12:53.000
So then you can see how you could ground an object like a dog by pointing to it. How do you ground a rule?

12:53.000 --> 12:59.000
So then my second addition is what I would call the exotopic rule.

12:59.000 --> 13:04.000
So biology, the tissue mimics stuff in the outside world.

13:04.000 --> 13:30.000
So I think you guys probably know that the arrangement of pitches of notes or sound frequencies in the cochlea is in order of the frequency and the representation in your brain spatially is in that same order.

13:30.000 --> 13:40.000
And you have the same sort of thing for the representation of your hand in your arm. And so those are called like tonotopic and somatotopic and so forth.

13:40.000 --> 13:53.000
And you can see the same thing in genetics. You guys will appreciate this that the homeobox genes are expressed last time I checked in the same order that they are arranged in the chromosome.

13:53.000 --> 14:00.000
And so biology has this habit of mimicking in the biology, whatever is going outside.

14:00.000 --> 14:09.000
And so what I'm going to propose is, first of all, the way we see the world is there are things out there, there are relations in between them.

14:09.000 --> 14:20.000
And so you have two entities and a relation between them. And so I'm going to propose, first of all, that that's the structure of the world, at least as the way people see it.

14:20.000 --> 14:36.000
We don't see it as Hamiltonians. And second, we're going to, I'm going to propose that, okay, there's something in our biological tissue or in these rules for manipulating representations that mimics this entity relation entity structure.

14:36.000 --> 14:48.000
So that's the exotopic rule hypothesis. So the operands are going to be these icons, these grounded symbols or hierarchies of them.

14:48.000 --> 14:58.000
And the operations are going to build these hierarchies using only those three things and the result of doing it is still going to be one of those three things.

14:58.000 --> 15:02.000
So then the question is, okay, so how far can you get with this.

15:02.000 --> 15:16.000
So, let's start doing some data. So I assembled about 1000 different sentence fragments and sentences, not linguistics tends to use the Wall Street Journal because usually they were trying to sell to Wall Street people.

15:16.000 --> 15:31.000
But basically newspapers are not the way we really speak. And so I took a variety of, for example, dictionaries intended for foreign speakers tend to have example sentences in them that are.

15:31.000 --> 15:35.000
Real world sentences.

15:35.000 --> 15:43.000
There are that took some books that are written by clear writers and so forth.

15:43.000 --> 15:56.000
So let's take three of these. I'm going to go through it slowly semi obvious but I'm going to go through it for it slowly because once you get over this slide if you're okay with me and if you have objections here's the time to ask.

15:56.000 --> 16:01.000
And you'll see what I'm going through if you get through this and the whole rest of the talk is working through the details.

16:01.000 --> 16:08.000
So the word string, and these are all taken from this these corpuses of senses, experts predict improvements.

16:08.000 --> 16:23.000
So the meaning of that is roughly experts predict improvements. So this is this capital is a standard kind of abbreviation in the field meaning intended to mean that here's the meaning of the word.

16:23.000 --> 16:28.000
And I'm going to notate this parentheses around the entities.

16:28.000 --> 16:35.000
And then the relations don't get parentheses. So you see here we have an entity a relation and an added.

16:35.000 --> 16:38.000
Now, the green grassland.

16:38.000 --> 16:51.000
Well, the is pretty much what it means is that I'm going to tell you about something I already told you about, as opposed to our, where I'm going to tell you something new.

16:51.000 --> 16:59.000
And green is pretty much like green us, you know, yes, representation of color, whatever that is.

16:59.000 --> 17:02.000
And grassland is your representation of grassland.

17:02.000 --> 17:11.000
But what I'm also saying is that the green is a component over a property of the grassland.

17:11.000 --> 17:12.000
Okay.

17:12.000 --> 17:15.000
So I'm going to abbreviate that with this symbol here.

17:15.000 --> 17:23.000
You can think of it as a grassland and folding this component, this property of greenness.

17:23.000 --> 17:31.000
And this green grassland is part of the things I already told you about.

17:31.000 --> 17:36.000
So the has the component green grassland.

17:36.000 --> 17:40.000
So it's a similar symbol but facing the other direction.

17:40.000 --> 17:51.000
So the history to this symbol, which turns out to be wrong, but it's a nice symbol anyway, because it conveys this unfoldingness.

17:51.000 --> 18:00.000
But we don't speak these things doing some in Chinese sometimes in some situations they do, but in English it out.

18:00.000 --> 18:02.000
So we leave this out.

18:02.000 --> 18:05.000
Now here's another one the person who threw it.

18:05.000 --> 18:18.000
So person is, you know, some variable for a person to be instantiated, some particular person who is, you know, sort of referring to something.

18:18.000 --> 18:28.000
But again, arbitrary through we have a representation of throwing it refers to some item that I've already talked about some known item.

18:28.000 --> 18:36.000
The meaning here is also that this person is a component of this activity of somebody through something.

18:36.000 --> 18:39.000
So gets another one of these things.

18:39.000 --> 18:43.000
So again, we don't speak it though.

18:43.000 --> 18:50.000
Okay, so if you're with me so far, then this is why I asked you about the parts of the chair.

18:50.000 --> 18:57.000
We never think about these things we don't speak, just sort of all subconscious.

18:57.000 --> 19:09.000
So this is data compression, which is, you know, if you're familiar with computers you know about data compression if you're a linguist you don't know about it and you don't want to know about it oddly, I find.

19:09.000 --> 19:16.000
And so what this is saying is that when we speak in English, we're doing data compression.

19:16.000 --> 19:18.000
Here's a few more examples.

19:18.000 --> 19:21.000
And then I'll show you, well, I guess this is data.

19:21.000 --> 19:23.000
He met me there.

19:23.000 --> 19:28.000
Well, we don't say at, but that's kind of what you need in order to understand the sense.

19:28.000 --> 19:33.000
They raise prices 8%, well, by 8%.

19:33.000 --> 19:35.000
There's more of these.

19:35.000 --> 19:36.000
He found her asleep.

19:36.000 --> 19:43.000
Well, what he found was the condition of her having the property of being asleep.

19:43.000 --> 19:45.000
And you can go on with this.

19:45.000 --> 19:48.000
Well, indirect objects are important.

19:48.000 --> 20:00.000
He gave mama the, he gave the situation of mama owning the stress.

20:00.000 --> 20:12.000
This one, a linguist debate about, but there actually is a history pointed out to me by Steven Pinker, Steven Pinker actually, that there's strong arguments that ought to be treated this way.

20:12.000 --> 20:19.000
And then he gave mama a function that has two arguments and so forth, which is the standard linguistics way.

20:19.000 --> 20:25.000
So, and then one other thing I'll point out is that jaren's are really little miniature senses.

20:25.000 --> 20:29.000
So cooking, well, it means that something cooks something.

20:29.000 --> 20:34.000
And you just haven't said who the subject is and you haven't said who the object is.

20:34.000 --> 20:41.000
But it's an E, well, E, so that's an E, you know, I can use the word that to talk about it.

20:41.000 --> 20:44.000
It's an entity. Awesome.

20:44.000 --> 20:47.000
Oh, I'll do one more.

20:47.000 --> 20:51.000
Why do we have the word to in infinitives?

20:51.000 --> 20:52.000
What's the point?

20:52.000 --> 21:05.000
Well, if you, what's really going on here, Casey wants Casey to change to the situation of Casey throwing the ball.

21:05.000 --> 21:09.000
Pro is a standard linguistics thing, meaning an unspoken subject.

21:09.000 --> 21:14.000
So they're onto this idea that some things are not spoken.

21:14.000 --> 21:19.000
But my point is that this is common. You see it everywhere.

21:19.000 --> 21:24.000
Oh, the horse that raised past the barn fell. That's this classic garden path sense.

21:24.000 --> 21:33.000
Because if you just say the horse raised past the barn, you're expecting it to be a past tense verb and you've got this picture of a horse racing past the barn.

21:33.000 --> 21:37.000
Then you throw in the verb fell and now you're confused and you have to go read the sentence.

21:37.000 --> 21:43.000
And what you left out was that was the horse that was raised past the barn.

21:43.000 --> 21:45.000
Okay. So how common is this?

21:45.000 --> 21:50.000
It's about 25% of English sentences are not spoken.

21:50.000 --> 21:57.000
So these are various sources like the Longman's dictionary contemporary Englishes were non English speakers.

21:57.000 --> 22:05.000
The Oxford dictionary, the English language has a whole section on examples, which are quite nice.

22:05.000 --> 22:16.000
The Penn Tree Bank, which is a standard corpus linguistics corpus annotated for parts speech and so forth has lots of sentences and there are various other things.

22:16.000 --> 22:21.000
And pretty consistently, it's about 25% of the things that are not spoken.

22:21.000 --> 22:29.000
One of them are the system component relations, but sometimes the adverbs and so forth.

22:29.000 --> 22:36.000
Now, once we agree to that so and so no objection so far. Great.

22:36.000 --> 22:40.000
So what's the is there a pattern to this.

22:40.000 --> 22:48.000
Well, so now the entities are in green the relations are in violet.

22:48.000 --> 22:56.000
And it looks like continuously you're alternating entity relation entity relation.

22:56.000 --> 23:06.000
Well, um, okay, that's nice. So things are beginning to look a little less complex than language is supposed to be.

23:06.000 --> 23:12.000
Now, you can guide this or force the structure.

23:12.000 --> 23:20.000
If you say that there's something like a reading frame so you guys know genetics linguists don't so you'll get it immediately.

23:20.000 --> 23:28.000
So like a reading frame in genetics, you've got this pattern just waiting for the words to be dropped in.

23:28.000 --> 23:32.000
So entity relation entity relation, etc, etc, etc.

23:32.000 --> 23:37.000
So if we take the sentence like he saw the saw, I saw.

23:37.000 --> 23:42.000
So we're going to drop the entity into this box.

23:42.000 --> 23:51.000
Well, then the next box over is got to be is expecting a relation so it's going to take the verb sense of saw.

23:51.000 --> 23:54.000
Then the next box is expecting an entity.

23:54.000 --> 24:06.000
The goes in there just fine. Part of the dictionary definition of the is going to be that it's talking about something that's component something that you've already talked about.

24:06.000 --> 24:11.000
So this are composed of comes along from the dictionary.

24:11.000 --> 24:13.000
So now you're in this box.

24:13.000 --> 24:17.000
So you get saw again but now it's expecting an entity.

24:17.000 --> 24:29.000
And so this helps tremendously with the disambiguation of words and is a clue as to why we can get away with ambiguous words.

24:29.000 --> 24:32.000
I'll show you some actual data on this point later.

24:32.000 --> 24:35.000
You can keep on going.

24:35.000 --> 24:39.000
He saw the saw Clive saw what Clive.

24:39.000 --> 24:42.000
So you're here, but Clive is a proper now.

24:42.000 --> 24:44.000
So it's got to go here.

24:44.000 --> 24:47.000
Well, so now you got an empty space.

24:47.000 --> 24:52.000
So now that's telling you that okay time to do some data decompression.

24:52.000 --> 25:11.000
And we've got a table, which is not all that complicated, where you look up in the table what to insert here, given that you've got an entity here and an entity here and it just depends on things like whether these are proper nouns whether it's accountable now.

25:12.000 --> 25:17.000
You know, it's basically, you know, it's a table to look up table you filled in.

25:17.000 --> 25:23.000
If you had lots of computing time I'm sure one could optimize that table.

25:23.000 --> 25:29.000
But I'll show you later how well this works. And so then now you're here and then now you've got a verb again.

25:29.000 --> 25:36.000
And then you get to the period which is basically relation between sentences.

25:36.000 --> 25:38.000
Okay, you got another empty space.

25:38.000 --> 25:43.000
Okay, well, so you left an empty space here.

25:43.000 --> 25:59.000
Well, actually, that's good, because you really want this saw over here to wind up over here, just like in transformational grammar, because Clive saw that saw.

25:59.000 --> 26:04.000
And so I'll talk later about how to do that.

26:04.000 --> 26:12.000
But you can see that this sequencing template is on simplifies everything.

26:12.000 --> 26:14.000
Now I want to make two points.

26:14.000 --> 26:17.000
So it simplifies the disambiguation data question.

26:17.000 --> 26:27.000
But if the word is ambiguous, the dictionary alone and the word alone can't help you with it.

26:27.000 --> 26:34.000
And I tell you where to place the word in this series of boxes.

26:34.000 --> 26:38.000
So this template has to be exogenous to the sentence.

26:38.000 --> 26:41.000
It's not contained in the words in the page.

26:41.000 --> 26:46.000
And I think there's no other place for it to be than up here.

26:46.000 --> 26:56.000
And I think this is the thing that humans have that lets us process language.

26:56.000 --> 27:07.000
The whole literature in linguistics arguing that some of this structure comes from a previous ability to synchronize muscles in order to throw things.

27:07.000 --> 27:09.000
And I think that's probably on the right track.

27:09.000 --> 27:12.000
You already had some kind of sequencing track.

27:12.000 --> 27:16.000
And now you've learned to put words into it.

27:16.000 --> 27:26.000
The other thing is that if you're inserting gaps like this, well, that also can't be part of the words on the page.

27:26.000 --> 27:31.000
It has to come from an exogenous sequencing track.

27:31.000 --> 27:39.000
And for those of you who are, you know, like Dr. Christon, you think about Bayesian sentence processing.

27:39.000 --> 27:42.000
I would say that, okay, here's the backbone.

27:42.000 --> 27:45.000
Yes, you can add sophistication onto it.

27:45.000 --> 27:49.000
And I'll show you later how much sophistication you...

27:49.000 --> 27:56.000
How much you get from this alone and how much additional sophistication you need.

27:56.000 --> 27:58.000
Okay.

27:58.000 --> 28:02.000
Now, so far we just got a string of e's, r's, e's, r's, and so on.

28:02.000 --> 28:04.000
End of due relation sentence.

28:04.000 --> 28:06.000
Now we've got to build them up.

28:06.000 --> 28:08.000
So now I'll tell you how to do that.

28:08.000 --> 28:09.000
So this is the next thing.

28:09.000 --> 28:11.000
So we're on number four.

28:11.000 --> 28:18.000
The first three are me saying just the three things linguists are doing in the 20th century are wrong.

28:18.000 --> 28:22.000
And now the next set of things are going to be things we can do now that they can't.

28:22.000 --> 28:26.000
So how do you build this stuff?

28:26.000 --> 28:31.000
Well, so here's another sentence just from the corpus.

28:31.000 --> 28:36.000
Saracens built a network of highways to serve blah, blah, blah, blah.

28:36.000 --> 28:39.000
So Saracens built a network.

28:39.000 --> 28:42.000
So this all goes as I've been telling you.

28:42.000 --> 28:45.000
Now you get to all.

28:45.000 --> 28:53.000
What you want to happen is a network to be an entity now.

28:53.000 --> 29:03.000
And so we'll say for the moment that some place around the word of there's an operator that's building things.

29:03.000 --> 29:07.000
And what it's going to do is going to build a network.

29:07.000 --> 29:13.000
Now, it could do it again and build Saracens build a network.

29:13.000 --> 29:22.000
Oh, and if the next word were bricks, that would probably be the right thing to do because you'd have always an advert.

29:22.000 --> 29:25.000
But in this case, it's not the right thing to do.

29:25.000 --> 29:30.000
So you've got a preposition here, and you're just going to build this.

29:30.000 --> 29:35.000
It's all called prepositional attachment, and it's largely an unsolved problem in logistics.

29:35.000 --> 29:42.000
And for us, what we do in the computer program, I'll show you later, is we do it both ways.

29:42.000 --> 29:51.000
And what will happen if you guess wrong sooner or later, that parse dies because there'll be some other conflict that it creates.

29:51.000 --> 29:56.000
Now we keep on going of highways, you get to two.

29:56.000 --> 30:07.000
And now, if we take the adverbial sense, you would say, okay, now the adverb is going to build this whole thing.

30:07.000 --> 30:09.000
And then it does get to do it again.

30:09.000 --> 30:12.000
So you've got, you built a network of highway.

30:12.000 --> 30:14.000
And now it does it again.

30:14.000 --> 30:17.000
And you get certain build a network of highways.

30:17.000 --> 30:18.000
Why?

30:18.000 --> 30:23.000
Well, to advance serve the practical needs of commerce and to do more parsing.

30:23.000 --> 30:29.000
Now, if it had been, they build a network of highways to nowhere.

30:29.000 --> 30:32.000
This would be a preposition.

30:32.000 --> 30:38.000
You wouldn't be, you would build highways to nowhere.

30:38.000 --> 30:42.000
Well, two wouldn't down here would someplace.

30:42.000 --> 30:44.000
And so you would not be doing this.

30:44.000 --> 30:47.000
That's another fork you could take.

30:47.000 --> 30:53.000
But in the present case situation is this one.

30:53.000 --> 30:54.000
Okay, great.

30:54.000 --> 31:01.000
So actually all these operators are sitting right at the same place as the relations.

31:01.000 --> 31:05.000
What, what are the rules for these building operators?

31:05.000 --> 31:10.000
Well, turns out that the building operators are the relations.

31:10.000 --> 31:11.000
They're not just sitting there.

31:11.000 --> 31:13.000
They are the relations.

31:13.000 --> 31:17.000
And we sat down to figure out what those relations.

31:17.000 --> 31:19.000
What those rules are.

31:19.000 --> 31:21.000
Because we figured, okay, this is going to be complicated.

31:21.000 --> 31:25.000
You know, in such and such a circumstance that can only build this far and other

31:25.000 --> 31:30.000
circumstances that can build twice in a row.

31:30.000 --> 31:35.000
Or maybe it only builds from here to here and somebody else builds from there.

31:35.000 --> 31:38.000
Turns out it wasn't complicated.

31:38.000 --> 31:44.000
And so those of you who are engineers or computer guys, or even if you remember

31:44.000 --> 31:51.000
algebra operator precedence hierarchies, you know, you multiply first and then

31:51.000 --> 31:57.000
you do addition and subtraction or you do the exponents first or then you do the

31:57.000 --> 32:00.000
multiplying and dividing and then you do the addition subtraction.

32:00.000 --> 32:05.000
So for given operator somewhere in this precedence hierarchy.

32:05.000 --> 32:07.000
Something just turns out.

32:07.000 --> 32:12.000
I was amazed that you can just write an operator precedence hierarchy.

32:12.000 --> 32:14.000
So that's just another table.

32:14.000 --> 32:21.000
And you can sort of see things like this guy who comes after the law are pretty

32:21.000 --> 32:22.000
low down.

32:22.000 --> 32:26.000
You almost always build over him into something bigger.

32:26.000 --> 32:30.000
Whereas things like periods and question marks.

32:31.000 --> 32:32.000
Well, you don't.

32:32.000 --> 32:36.000
And then other guys are in between.

32:36.000 --> 32:38.000
So.

32:38.000 --> 32:42.000
Then the next thing I'll show you is these Chomsky and.

32:49.000 --> 32:51.000
I had a clock.

32:51.000 --> 32:55.000
I don't know where I put it.

32:55.000 --> 32:58.000
So the transformations.

33:03.000 --> 33:04.000
I did what I never do.

33:04.000 --> 33:07.000
I put it in my pocket because I thought that would be a good idea.

33:09.000 --> 33:10.000
Okay.

33:10.000 --> 33:14.000
Now, so can you do the Chomsky and transformations?

33:14.000 --> 33:22.000
Well, they have a series of rules for how to get cats into here.

33:22.000 --> 33:27.000
They're pretty good rule, but they're a little bit ad hoc sometimes.

33:27.000 --> 33:34.000
But here there's this next amazing thing on the basis of two words flanking.

33:34.000 --> 33:37.000
An empty space when we did the decompression.

33:37.000 --> 33:40.000
Remember, we inserted these various relations.

33:40.000 --> 33:46.000
Well, it turns out that for some reason that I really don't understand.

33:46.000 --> 33:51.000
These same relationships also act at a higher level.

33:51.000 --> 33:54.000
So we can now say cats.

33:54.000 --> 33:57.000
Oh, okay.

33:57.000 --> 34:03.000
We said that they are a component of this whole thing.

34:03.000 --> 34:09.000
Well, they are a component of that.

34:09.000 --> 34:11.000
So cats go into that.

34:11.000 --> 34:15.000
Well, that is a component of this whole thing.

34:15.000 --> 34:16.000
And guess what?

34:16.000 --> 34:19.000
There's even empty space for it.

34:19.000 --> 34:23.000
And so that's called successive, what is it?

34:23.000 --> 34:24.000
I forget what it is.

34:24.000 --> 34:26.000
Anyway, it's standard linguistics.

34:26.000 --> 34:29.000
The point is you do have to do these things in steps.

34:29.000 --> 34:35.000
And here all the algebraic instructions have already been inserted for you.

34:35.000 --> 34:40.000
So poof, all you have to do is the computation.

34:40.000 --> 34:46.000
And similar things.

34:46.000 --> 34:48.000
Well, I won't go into that.

34:48.000 --> 34:53.000
Similar rules can assign, well, actually I may have mentioned it.

34:53.000 --> 34:54.000
Yeah.

34:54.000 --> 34:58.000
So where you start and where you end are governed by some rules.

34:58.000 --> 35:03.000
And it's called C command in linguistics.

35:03.000 --> 35:05.000
There's a complete analog here.

35:05.000 --> 35:08.000
You just have to rewrite it in terms of entities and relations.

35:08.000 --> 35:13.000
And same kinds of rules for deciding on the relationship between pronouns and reference.

35:13.000 --> 35:18.000
If you violate those rules, the sentence isn't grammatical.

35:18.000 --> 35:24.000
So that's how you tell a grammatical sentence from an ungrammatical sentence.

35:24.000 --> 35:26.000
Now, this only English.

35:26.000 --> 35:31.000
So here's Lakota, which is the Sioux language.

35:31.000 --> 35:37.000
So where we would say John found that letter under the bed.

35:38.000 --> 35:44.000
In Sioux, it is John letter that bed the under found.

35:44.000 --> 35:45.000
Okay.

35:45.000 --> 35:49.000
That can be written as EE rel.

35:49.000 --> 35:57.000
There's a phenomenon in linguistics where you can classify languages according to different structures.

35:57.000 --> 36:04.000
And this, you know, E rel E happens to be one set of languages.

36:04.000 --> 36:06.000
E rel is another.

36:06.000 --> 36:14.000
And this is essentially like reverse Polish and, you know, mathematics or computer programming.

36:14.000 --> 36:16.000
But you can do it.

36:16.000 --> 36:20.000
And it's the same general idea.

36:20.000 --> 36:26.000
So let me show you a little bit of what happens with actual parsing.

36:26.000 --> 36:34.000
So Steve Senth, who was at Yale at the time, he's now at Marine Biological Laboratory in Woods Hall.

36:34.000 --> 36:44.000
He's the guy who wrote the original voxel view program that processes images, three dimensional images in terms of boxes that are pixels.

36:44.000 --> 36:52.000
So he wrote a program that's only eight megabytes.

36:52.000 --> 36:54.000
Half of that is the dictionary.

36:54.000 --> 37:04.000
So it's only four megabytes of processor, as opposed to these large language models, which are terabytes of RAM.

37:04.000 --> 37:07.000
So here's just an example that's sort of a hard one.

37:07.000 --> 37:09.000
You probably have to read it twice.

37:09.000 --> 37:13.000
The man who knew the man who Aaron knew knew Clive called.

37:13.000 --> 37:18.000
And there's actually two meanings that you can take out of that.

37:18.000 --> 37:24.000
The man who Aaron knew already, that person knew Clive called.

37:24.000 --> 37:30.000
Or the man who Aaron knew that person knew Clive called.

37:30.000 --> 37:34.000
And this program spits that out.

37:34.000 --> 37:39.000
It also is able to pull out of this who did what to who.

37:39.000 --> 37:43.000
So in this first one, the man knew something.

37:43.000 --> 37:47.000
Clive called somebody and Aaron knew the man.

37:47.000 --> 37:49.000
Whereas this one is different.

37:49.000 --> 37:55.000
The man called somebody Aaron knew something and the man knew Clive.

37:55.000 --> 38:03.000
And you may be familiar with pretty print, which is supposed to be a way of showing hierarchical structure.

38:03.000 --> 38:07.000
Steve came up with this other notation, which I think is much clearer.

38:07.000 --> 38:12.000
If you just look at this, you can say, well, okay, somebody knew something.

38:12.000 --> 38:14.000
Well, what did that person know?

38:14.000 --> 38:17.000
He knew the Clive called somebody.

38:17.000 --> 38:21.000
Well, over here, who knew something?

38:21.000 --> 38:28.000
Well, oh, these guys are all just variations on those left and right component of symbol.

38:28.000 --> 38:35.000
So the man, if you look down at this level, the man is a component of something.

38:35.000 --> 38:38.000
It's a component of who Aaron knew.

38:38.000 --> 38:41.000
Who is a component of Aaron knew?

38:41.000 --> 38:44.000
Because Aaron knew this guy who.

38:44.000 --> 38:49.000
And the man is a component of all that and it's a component of the who, et cetera.

38:49.000 --> 38:52.000
So it's all built in there.

38:52.000 --> 38:59.000
I can, you know, run a program in real time for you if you want.

38:59.000 --> 39:00.000
It's probably simple.

39:00.000 --> 39:05.000
It's just to finish the talk and we can play with that later if you want to run any sentences.

39:05.000 --> 39:10.000
So, um, oh, and there's also a cost function.

39:10.000 --> 39:11.000
That's what this is.

39:11.000 --> 39:16.000
So that if you have to start skipping boxes, you can do it, but it costs you.

39:16.000 --> 39:23.000
And so that's a way of telling whether you're doing something that's legal, but unlikely.

39:23.000 --> 39:27.000
And sooner or later, those things wind up having a huge cost.

39:27.000 --> 39:29.000
And you know, that's not right.

39:29.000 --> 39:32.000
Um, now it doesn't work.

39:32.000 --> 39:40.000
So I'm guessing you guys don't worry about linguistics much, but, um, there's some standard metrics.

39:40.000 --> 39:46.000
Like recall precision for this thing or 97% coverage and consistency of 77%.

39:46.000 --> 39:50.000
Basically, consistency is how many are correct parses do you get?

39:50.000 --> 39:57.000
Well, let me just say that when people do this, they use things like the pen tree bank corpus,

39:57.000 --> 40:04.000
which doesn't even bother with compound nominals like doghouse or prepositional attachments.

40:04.000 --> 40:07.000
You know, like I was showing you the two different ways of doing off.

40:07.000 --> 40:09.000
It doesn't even annotate for that.

40:09.000 --> 40:11.000
I've annotated for that.

40:11.000 --> 40:14.000
And it's still 77%.

40:14.000 --> 40:17.000
Um, if I didn't annotate for that, probably 97%.

40:17.000 --> 40:21.000
And then there's across brackets, which has to do with, well,

40:22.000 --> 40:28.000
how many subunits or is the computer trying to get you to say overlap each other when they shouldn't.

40:28.000 --> 40:30.000
And it's rather small.

40:30.000 --> 40:35.000
And it only happens in really long sentences where people get confused.

40:35.000 --> 40:37.000
So show you some data.

40:37.000 --> 40:48.000
So I told you there were lots of, because of word ambiguity and, um, the resulting parse ambiguity because parses build things differently than entities.

40:49.000 --> 40:52.000
There's a lot of alternative parses.

40:52.000 --> 40:55.000
So on the X axis, you have the number of words.

40:55.000 --> 40:57.000
So let's take out 15 word sentence.

40:57.000 --> 41:03.000
And so the sentences I tried, they have about 10 to the seventh possible parses.

41:03.000 --> 41:06.000
Oh, I forgot to show you.

41:06.000 --> 41:07.000
Back here.

41:07.000 --> 41:16.000
It's saying that for the man who Aaron knew new collide called it's saying that there are 3,800 potential parses and it's pulled out to

41:16.000 --> 41:21.000
both of them are, um, you know, correct.

41:21.000 --> 41:23.000
So back here.

41:23.000 --> 41:31.000
So you've got 10 to the seventh potential parses and you've dropped that down to about.

41:31.000 --> 41:34.000
Let's see.

41:34.000 --> 41:36.000
10 to the so one.

41:36.000 --> 41:43.000
Yeah, so this is dropped it down to about 10 parses here.

41:43.000 --> 41:46.000
Um, so that's pretty impressive.

41:46.000 --> 41:57.000
So what you're doing is removing with just this sequencing track and these cognitive rules.

41:57.000 --> 42:08.000
You're removing orders of magnitude of the ambiguity in the words and the alternative ways of parsing the sense and understanding the sense.

42:08.000 --> 42:11.000
So now you're only down to here.

42:11.000 --> 42:27.000
And so that any knowledge you have to have about, well, what's the guy probably trying to say or any of the Bayesian stuff only has to solve another like 10 fold of this.

42:27.000 --> 42:35.000
This ratio here turns out to be a reduction of 2.74 fold per word.

42:35.000 --> 42:37.000
So three fold.

42:37.000 --> 42:43.000
But that three, if you start raising it to the 15th power, it gets to be a big number.

42:43.000 --> 42:48.000
Then as I stared at that, I was thinking, Oh, 2.74 that number looks kind of familiar.

42:48.000 --> 42:54.000
And then I realized that this reduction here is one over e to the 1.008.

42:54.000 --> 42:56.000
In other words, it's one over e.

42:56.000 --> 43:04.000
And so the interpretation of that, there's a kind of like a target theory interpretation of that statistics.

43:04.000 --> 43:22.000
You're trying to hit a target with radiation or something. Basically, there are enough constraints that in this, in the cognitive rules, I told you about that you are just going to so castically eliminate sentences that are wrong.

43:22.000 --> 43:26.000
And so you're reducing things down to one over e.

43:26.000 --> 43:45.000
And the, then what's left, and that is what happens when you have independent events, though it was treating words as more or less independent.

43:45.000 --> 43:48.000
And the collisions are coming from the sequencing track rules.

43:48.000 --> 43:59.000
Then everything else you need to get from this 10 down to a single interpretation is either the meaning or things like us.

43:59.000 --> 44:12.000
Subject verb case agreement or subject for number agreement or cases, things like that that are relationships between a couple of words are not independent events.

44:12.000 --> 44:19.000
So that's what the rest of linguistics is doing is getting you from here down to here.

44:19.000 --> 44:36.000
Now, I hope at this point I've convinced you that, gee, language is maybe a whole lot more regular than we thought, and, and having a regular algorithm can process way more of it than we thought.

44:36.000 --> 44:53.000
You could then wonder, oh, and that what is processing is the, or building, are these representations rather than words per se.

44:53.000 --> 44:57.000
And then now you begin. So let's wonder about AI.

44:57.000 --> 45:14.000
And what have large language models discovered, all they would have to do is discover or teach themselves to make a sequencing track like this, and a couple of rules, and a sequencing track, what is that it's just a series of probability.

45:14.000 --> 45:27.000
Well, if I'm, if I am an entity word now, the probability is 1.0 that the word that the next word is going to be a relation and the probability of that is blah, blah.

45:27.000 --> 45:37.000
So, you know, it's sort of Bayesian, but it's, you know, as somebody told me, well, rules are just probabilities of 1.0.

45:37.000 --> 45:44.000
So a statistical learning machine could come up with this track.

45:44.000 --> 46:00.000
And then what are the implications of that? Well, let me show you one more thing about how this program works. So we're trying to build a sentence, build the parts into higher and higher hierarchies.

46:00.000 --> 46:07.000
So what we're doing, I'm going to show you that basically we have a shift register for grounded symbols.

46:07.000 --> 46:20.000
So you have cats, cats that because the dictionary definition of that is that has these two component things because cat is going to go into that, and that is going to go into something else.

46:20.000 --> 46:28.000
You've got my dogs, blah, blah, blah, you keep building here and now Chase is a relation.

46:28.000 --> 46:31.000
These guys don't build much.

46:31.000 --> 46:37.000
Chase's will build things. And what you want it to do is build my dog.

46:37.000 --> 46:52.000
And this is, it does the shift register thing. It's going to build, put by this relate the composed of a dog, all in the same box, because that grouping is now an entity.

46:52.000 --> 46:57.000
And it shifts these guys over to.

46:57.000 --> 47:01.000
And then keeps going. It gets to R.

47:01.000 --> 47:12.000
R is going to build. And now it's going to shift the parentheses and chases all into this box and put a new set of parentheses around it.

47:12.000 --> 47:21.000
Oh, these things are all just parentheses are just different shape parentheses to make it easy to follow.

47:21.000 --> 47:34.000
And then we're going to keep on going because our goes all the way to the beginning of the sentence that's going to shift it over again, lump all these things together, shift over again lump these things all together.

47:34.000 --> 47:45.000
And then all you need biologically is instead of parentheses you need something that is going to group.

47:45.000 --> 47:53.000
These subgroups into a bigger group without just averaging them like red and white into pink.

47:53.000 --> 48:09.000
Well, if then you think about whether representations could be doing this, whether your brain could be doing this on representations, in addition to just doing it on words.

48:09.000 --> 48:16.000
These words are really just representations to there's no words in your head. There's just like neural spikes. Right.

48:16.000 --> 48:21.000
So these guys so you've already got representations getting dropped in here.

48:21.000 --> 48:34.000
What happens. So it doesn't seem to me a very big step to be dropping these representations into the same sequencing track and doing the processing.

48:34.000 --> 48:46.000
So if a large language model happened to event the sequencing time, it could also support processing of sensory representations instead of word representations.

48:46.000 --> 48:50.000
And I've just shown you that the rules are the same.

48:50.000 --> 48:55.000
And then if that then let's assemble complex representations.

48:55.000 --> 49:04.000
And it does so in a way that you can now define a thing, which is the one remaining part, which I have an idea about how I could do it.

49:04.000 --> 49:08.000
Because what we think about our thing.

49:08.000 --> 49:21.000
But it can now retains all the information to do discrimination between them and do generalizations so that fulfills harness definition of what it takes to do cognition.

49:21.000 --> 49:34.000
So I'm quite concerned that his large language model model are accidentally going to stumble across a way to be doing cognition.

49:34.000 --> 49:38.000
So that is, I believe it. Yep.

49:38.000 --> 49:46.000
And open to any questions, or if you want to see the parts or an action that's fine.

49:46.000 --> 49:48.000
And I will.

