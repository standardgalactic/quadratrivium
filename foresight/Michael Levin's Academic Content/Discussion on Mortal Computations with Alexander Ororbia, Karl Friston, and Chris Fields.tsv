start	end	text
600	1680	No, that's great.
1680	3560	Okay, so maybe we could start.
3560	5840	I think, Alex, if you could start
5840	10840	and just give a few more words on this,
11160	13240	the whole notion of mortal computation
13240	18240	and specifically the issue of programmability
18720	23720	and morphology and how we can sort of distinguish
24920	26000	when we're looking at a system,
26000	27680	how can we distinguish what aspects
27680	30840	of mortal computation we're looking at.
30840	32040	Anything along those lines,
32040	34160	I think would be very useful for us.
34160	35880	Maybe we could start there.
35880	38760	Yeah, so I did, by the way, read last night
38760	41520	the paper you shared with me and Carl and Chris
41520	44640	at one point and some of your notions in there.
44640	46880	First of all, a lot of it sounds like parts
46880	49480	of mortal computation, just maybe you're not using
49480	51240	the word mortal computation phrase.
52240	56360	So in terms of the morphology part,
56360	59040	so the argument that Carl and I make in that paper
59040	61560	is just that the structure, right,
61560	65320	is critically important to the actual system itself.
65320	67360	So you don't want to devour,
67360	71040	there's a lot of this idea of the amorphic formulation
71040	72880	of computational models.
72880	76200	So when you take, and actually Carl and Chris had a paper
76200	79480	that predated the mortal computation paper,
79520	82080	but we developed that metaphor further,
82080	86480	you can draw a dot and arrow diagram of a neural network
86480	91160	and that endows it with this pedagogical morphology,
91160	92960	but really at the end of the day, it doesn't matter.
92960	95120	It's just a pile of linear algebra
95120	96840	that will do its calculations.
96840	98360	And then we have to go through
98360	101400	the von Neumann computing architecture
101400	104040	to transmit the weight, memory and all that.
104040	107560	And then that's where this great thermodynamic cost.
107560	112280	So living systems, I know as you know very well, Michael,
112280	114400	but in general are inherently morphic.
114400	116400	And actually this is the part where we didn't have it
116400	120000	in the paper, but your paper used the word polycomputing,
120000	121720	which is the idea that a substance
121720	123840	can compute many different things simultaneously
123840	126320	is like one way to look at that word.
126320	128800	And the idea is that it's all about the substrate.
128800	131240	And so mortal computation then just says,
131240	134120	if we're going to think about artificial
134120	136680	general intelligence or machine intelligence,
136680	138360	we're sort of going about it in the wrong,
138360	141360	potentially wrong direction by having this divorce
141360	145000	of the computational architecture and construct
145000	148240	separate from the morphology or the substrate too,
148240	150200	that this is going to be enacted on
150200	153880	because living systems, if you change the morphology,
153880	156160	you change the properties of the system,
156160	158840	you change also what it can compute, what it can do.
158840	162480	Now you talk about the liquid kind of brain
162480	165120	or the idea that we have this self over time,
165120	169120	which doesn't actually sit at odds with mortal computation.
169120	172640	So the idea is that you're constantly through change,
172640	176280	you're persisting, you're almost like I want to persist,
176280	177960	but I understand that my system
177960	179920	is going through like auto pieces.
179920	182080	So mortal computation sort of absorbs that
182080	184040	and tries to say we should be designing systems
184040	185420	from that perspective.
186360	188720	So just so I don't keep rambling,
188720	191720	you were talking about programmability.
191720	193600	And I think that was an interesting part
193600	196200	that we didn't really get to chat a whole lot about
196200	199320	and it wasn't 100% clear what was meant
199320	203120	by programming the morphology or the system
203120	204960	because actually after reading your paper,
204960	207560	I think that sort of gave the answer as to how,
207560	209240	because I said originally, oh well,
209240	212440	I'm thinking that the morphology or the substrate
212440	214880	dictates strictly what you can and can't do.
214880	217080	Yes, it will change and it will repair
217080	220960	and go through damage or do things like self-replication.
220960	223160	But I wasn't thinking about the human designer,
223160	225720	he would say if we are designing a chimeric system
225720	229160	or something new, manipulating that morphology so easily,
229160	230800	it's more like, well, we're gonna be looking
230800	234120	at computational simulations of that
234120	235520	and that would be programmable.
235520	236920	And then that's where I was talking about,
236920	240280	oh, we could do like a software simulation
240280	244440	of Anthrobots or something, or Xenobots of that form.
244440	246960	And then you could sort of set the properties
246960	249840	of the environment in the system roughly
249840	251480	according to something you wanna look at
251480	253840	and then simulate it and see what it does.
253840	255960	But then your paper sort of essentially,
255960	259320	in your work as an example of you can directly program
259320	264240	like the genetic aspects of a system,
264240	267440	or you can kind of manipulate the bioelectrical chemicals.
267440	271480	You had an example of like a tadpole or a froglet
271480	274840	where if we apply the right electrical stimulation,
274840	277840	you can get it to grow a tail or get it to grow a leg
277840	278960	and there's some properties there.
278960	283360	So I think that is where the programmability is.
283360	284640	And then that would be more,
284640	286920	you would have even actual experience
286920	288640	programming the morphologies.
288640	291080	Whereas I was thinking more from the perspective
291080	295080	of neural networks where we have the structure
295080	297240	and we want that to now be at the very top
297240	299120	of what Karl and I call mills.
299120	300400	And that you remember in the paper,
300400	303160	there's the mortal inference learning and selection.
303160	305240	And at the very, very top is structure.
305240	308080	And that would be something that I think is underexplored
308120	311720	in the area about, oh, maybe we have neurogenesis
311720	314200	and naphyogenesis and then the system sort of has to use
314200	318040	that as another aspect of how it evolves over time.
318040	319640	But that was thought of,
319640	322040	or at least the way I was writing it originally,
322040	324280	it's sort of doing that in its own way.
324280	325960	You're not really involved in saying,
325960	328280	oh, I'm gonna help you do model selection,
328280	330480	but maybe if you encode priors,
330480	332280	that was the other aspect I had.
332280	334120	If you were encoding certain constraints,
334120	337280	you say, well, I just, I'm gonna play,
337280	339280	I'm gonna skip ahead for what evolution
339280	341960	would naturally walk, randomly walk you to and say,
341960	343800	these types of structures are invalid.
343800	345680	That would be like programming,
345680	347880	maybe structure from that perspective.
347880	349520	I don't know if this is making sense,
349520	351320	but I think it's a tricky topic
351320	354800	because I wasn't entirely sure what exactly two
354800	356320	was meant by programmability
356320	359160	because you have experience actually doing that.
359160	363160	So the answer is yes, it can be done from your perspective
363160	366680	and we would just be translating it to chimeric systems
366720	368080	or artificial systems
368080	371240	rather than it only just being like biological material.
372240	373520	I hope that makes sense.
373520	375920	Yeah, yeah, I mean, the kind of programmability
375920	378640	I had in mind, so just as an example,
378640	380880	we have these flatworms, these planaria,
380880	382720	and you can chop them into pieces
382720	384560	and every piece regrows a complete,
384560	386400	perfectly patterned little worm.
386400	388200	And you could ask the question,
388200	390200	how does it know how many heads to make?
390200	392600	And so it turns out that there's an electrical pattern
392600	395400	that sort of a body-wide electrical pattern
395400	398080	that dictates the number and the location of the head.
398080	400160	And the amazing thing about that substrate
400160	403560	is that if you change that pattern,
403560	404560	the tissue will hold it.
404560	406160	So we can change the pattern to say no,
406160	408240	two heads instead of one, and it holds.
408240	411520	And those worms in perpetuity forevermore,
411520	413400	despite their completely normal genetics,
413400	416520	will continue to regenerate as two-headed worms.
416520	419680	So it's a very minimal example of reprogrammability
419680	422280	because we don't have anything like complete control yet,
422280	423960	I think in the future, maybe we will,
423960	425680	but at the moment we don't.
425680	428640	But it is an example where the hardware
428640	431320	in an important sense, so the genetics are normal,
431320	433440	all this, there are no weird nanomaterials,
433440	434520	there's no genomic editing,
434520	437160	there are no synthetic biology circuits,
437160	440680	it's stock hardware, but because of this experience,
440680	443000	this physiological experience that it's had,
443000	445200	it now has a different pattern that it uses
445200	447400	as the sort of target morphology
447400	449600	of what it's going to do if it gets cut.
449600	451760	So that's the kind of thing,
451880	454120	the kind of plasticity that it has where
454120	457200	the material is basically the same,
457200	461400	but it has really a memory of a past event
461400	463200	and that memory guides how it behaves
463200	464920	in anatomical space in the future.
464920	467800	So that's the kind of thing I wanted to sort of explore
467800	470000	with respect to this framework.
471560	474520	And I don't think what you described is at odds
474520	477160	with what you would do with a mortal computer.
477160	480440	I mean, the idea is that is where the programmability
480440	482880	comes into play, you're kind of encoding that
482880	484600	and then seeing how the morphology
484600	486160	and the system evolve over time.
486160	488920	If you want the two-headed worm example,
488920	491240	I don't see any reason why that wouldn't translate
491240	495720	to artificial systems or maybe non-biological systems
495720	498680	if we are able to formulate what that morphology looks like.
498680	500600	I think the key is setting that up
500600	503120	and that's what Carl and I have at the very end of the paper
503120	506080	we talk about, well, for example, someone like me,
506080	508320	a computational neuroscientist, computer scientist,
508320	511000	I don't have access to Xenobots
511000	513320	or the biological material that I'd love to,
513320	514280	or organoids, right?
514280	516560	I really find those fascinating,
516560	517840	but maybe we could simulate them
517840	520480	and that's what we have at the appendix, actually,
520480	523720	at the already long paper, digital morphology.
523720	526360	Maybe that could be a way to sort of bridge the gap
526360	528480	between the computational researchers
528480	530120	and researchers like yourself.
530120	533400	Oh, can we come up with benchmarks or system setups
533400	536040	that I could play with the properties,
536080	537920	mathematical models, maybe,
537920	540280	of these biological morphologies
540280	542720	and then kind of do investigation, right,
542720	546520	without the costs and the barriers to entry
546520	548840	to working with biological material
548840	550520	or some of the things I don't have.
550520	553080	So I don't see anything at odds.
553080	555880	I don't know if Carl would wanna add anything
555880	558220	that I might be missing or not getting.
562680	564840	No, that was very fluent.
564960	566960	I think you've covered everything there.
569320	571640	If I can just jump in for a minute.
571640	576640	It seems to me that this dimension of programmability
577840	582840	could also be expressed as the dimension from uniqueness,
584360	587240	which mortal computers, as I understand it,
587240	590600	have more of than my laptop,
591600	596600	to replicability or exact replicability, copyability.
599880	604400	And to the extent that a system is really unique,
604400	605680	you can't program it.
608680	613680	And in part, just because you don't have two copies of it,
613880	616720	so you can't test what you're doing in any way.
616720	618480	You can't test reproducibility
618480	621600	if you're working with a system that's completely unique.
626000	631000	And biological systems are somewhere in the middle, right?
631160	634560	Laptops are intended to be way out
634560	637160	on the extreme replicability end.
638760	643760	And so because a laptop is completely replicable,
645360	647640	it's a completely generic entity,
649320	652000	it's almost a Turing machine, right?
652000	654420	It's almost just an abstraction.
655800	657880	And so when we're programming,
657880	659800	we can treat it as an abstraction.
659800	662320	And we don't have to worry about things like
662320	663720	where the power comes from
663720	668720	and why the thing maintains the same shape over time
669120	671520	and et cetera, et cetera.
671520	674560	I mean, if it doesn't maintain the same shape over time,
674560	675920	you take it to the repair shop
675920	677680	or recycle it and buy another one.
679480	681680	Whereas in biological systems,
681680	684520	you have to worry about all of that stuff.
684520	687320	And as you point out in the paper,
688760	693760	and as the 4E people kind of have been pointing out
693880	698880	for decades now, that's part of the algorithm,
699920	704920	or that's part of the operating system, that shape.
705680	709640	Whereas it's not involved at all in my laptop
709640	711000	in the operating system.
711000	712640	I mean, even the operating system
712640	715360	can treat the hardware as an abstraction.
717240	722240	So we can, I think more or less identify those two axes,
723920	726040	the dimension of programmability
726040	727680	and the dimension of uniqueness.
729520	734240	And so one of the things that you emphasized in your paper,
734280	736960	I thought this was very interesting,
736960	741960	was the energetics of using the body
743960	746040	as part of the operating system.
747920	752920	And it meant that you didn't have to pay for a lot of memory,
756120	759240	for example, or quite so much processing power.
759680	764240	You do, of course, have to pay the cost
764240	766320	of keeping the body intact.
768000	773000	But as you pointed out, at least in organisms,
773000	778000	that's cheaper than the cost of stamping out more laptops
778880	782120	and then equipping them with enough voltage
782120	785160	to keep them in the classical domain
785160	788160	so that they don't start acting like quantum computers,
788160	789440	which they actually are.
791880	796880	So I think that it would be useful to try to
798520	801080	relate this issue or resource cost
801080	803360	to the issue of uniqueness,
805440	808120	and as well as the issue of programmability.
808120	810920	I'm not sure whether those are,
810920	813000	I suspect those are distinct dimensions,
813000	817440	but I think the usable area of that state space
817440	820400	involves a lot of correlation between those dimensions.
824080	825040	Yeah, I agree.
825040	828520	I think that would be very interesting to explore.
828520	830800	I don't know, Michael, if that resonated with you,
830800	832920	because I feel like that touches on even
832920	836160	pairing the Mortal Computation paper for me and Carl
836160	839160	in the paper you shared with all of us, of yours.
839160	841560	I think the two sort of start to get into that idea
841560	845120	of uniqueness, programmability, and resource cost.
845120	846960	And there are some different dimensions
846960	847800	that you could explore,
847800	850280	because yeah, I also just wanted to comment, Chris,
850280	854960	that at least when I first was writing the paper
854960	856440	and then I shared it with Carl,
856440	858920	I wasn't thinking of the,
858920	860840	I was thinking of biological systems
860840	862560	as sort of an ideal target, right?
862560	865680	You're sort of emulating some aspects of those systems.
865680	868280	So I guess I'd be giving up some uniqueness, right?
868280	871760	Because you said it lies in between the immortal laptop
871760	874720	and the perfectly unique system itself.
874720	877280	I think the other thing I was concerned with
877280	879800	is the artificial intelligence community
879800	883400	really liking the idea of immortal computation
883400	885080	and that complete divorcing,
885080	888280	because you're right, you do lose the moment you leave,
888280	891360	even just a few steps away from immortal computation,
891360	894160	that reproducibility, because it's,
894160	896360	that substrate now is important.
896360	899000	And then Carl and I argue even stronger,
899000	901200	it's the morphogenesis too,
901200	902880	and the changing process, which again,
902880	905880	compliments your paper, Michael, as well.
905880	908920	The idea is that change is also very important
908920	910680	in that evolution over time,
910680	912360	which is not something you're going to have
912360	914480	on your typical deep neural network
914480	918040	that just lives at the top of the von Neumann architecture.
918040	920080	And then the last comment I just wanted to make, Chris,
920080	922760	is yes, that's exactly the key,
922760	925720	is that in-memory processing that we want.
925720	929080	And that's why bringing ourselves as close as possible
929080	931840	until we eventually just reach what we can,
931840	933520	which is the Landauer limit,
933520	935600	and getting ourselves real close to the hardware,
935600	939040	now we're optimizing thermodynamic cost.
939040	941320	And then of course, as you and Carl
941320	943640	and everyone here has shown over time,
943640	946960	that's the flip side to the information theoretic,
946960	949880	variational free energy, but the thermodynamic free energy.
949880	951960	But at the end of the day, we want to be there,
951960	953960	because that's what biological systems are.
953960	957160	They are much closer to the Landauer limit
957160	960640	than pretty much anything in machine intelligence
960640	961600	that we have today.
961600	963840	And that would argue it's even getting worse
963840	967680	because big, big transformers are really bad.
967680	970960	And Carl also and I state the carbon footprint.
970960	972720	So that's a good motivator.
975960	978840	Yeah, I find that energetic analysis very compelling.
980880	985440	You know, I'm very interested in why biological systems
985440	986760	can be quite so efficient.
986760	990240	And I think in many cases, they're efficient
990240	994120	because they're able to use quantum resources
995720	998400	when they're doing molecular computing.
998400	1002120	And maybe even when they're doing macromolecular computing.
1005200	1009000	I guess the one other comment about a dimension
1009000	1011120	that I wanted to throw in,
1011120	1013800	which you mentioned a little bit about in the paper,
1013800	1018800	was this dimension of explainability.
1021240	1026240	Which AI is very obsessed with the explanation problem now.
1030160	1035160	And as one gets away from reproducibility,
1035600	1038520	the explanation problem gets harder and harder.
1038520	1041600	And in the limit of a unique system,
1041600	1043840	the explanation problem is infinitely hard
1043840	1046280	because you can't do experiments.
1046280	1048640	Because you can't replicate anything.
1049640	1054640	So we have that other access to work with also.
1062440	1064680	Maybe it would be kind of interesting
1064680	1068000	is since you were bringing up these so far three axes
1068000	1072080	that I caught, you know, uniqueness, programmability,
1072080	1075320	explainability, we also did talk about resource cost,
1075320	1076960	kind of putting out this grid
1077120	1080600	and then you saw Michael and actually I presented,
1080600	1082080	you guys would have seen in the paper,
1082080	1083600	but I presented it a couple of times,
1083600	1085320	like the different types of things
1085320	1089240	that Carl and I consider variants of mortal computers.
1089240	1091920	And so obviously Xenobot is a mortal computer.
1091920	1093840	It actually had a lot more qualities
1093840	1096880	after I re-read papers again, looking back.
1096880	1100080	But even, you know, the silicon model
1100080	1103720	that we had for the non-biological model from Ashby,
1103720	1107400	we can never forget the great homeostat or allostat.
1107400	1109880	And so maybe we could plot these a little bit
1109880	1114080	on those axes too is what degrees that they're trading off.
1114080	1115840	Obviously we need to figure out
1115840	1118920	which one of these starts to get real close to the,
1118920	1121520	like you said, Chris, the really unique.
1121520	1123880	And then, you know, that would be the extreme one
1123880	1126160	where explainability would be, you know,
1126160	1127680	really, really, really difficult.
1127680	1129400	And we could kind of plot where those are.
1129400	1132400	That could be an interesting figure to show examples.
1132400	1135080	And I'm sure, Michael, you probably have other examples
1135080	1137600	that Carl and I might have missed.
1137600	1141680	So there might be some other nice biological chimeric systems,
1141680	1143400	things that are even less biological,
1143400	1144840	but have a little bit of it.
1144840	1148040	You did touch on nano technology as well.
1148040	1151680	And in your, the polycomputing paper you shared with us.
1151680	1153880	So maybe there might be some in soft robotics.
1153880	1155240	There might be something there too
1155240	1159400	that could count as variations to mortal computers
1159400	1161160	that trade off on these axes.
1161160	1162000	That's something else.
1162000	1164720	I just thought of this, Chris explaining.
1166480	1169720	Yeah, so another model system to think about.
1169720	1173840	And by the way, we do have simulators of some of this stuff.
1173840	1176440	So we should be in touch, you know,
1176440	1177520	give you access to some of that
1177520	1181000	because maybe you can do some analyses.
1181000	1182680	You know, we have bioelectric simulators
1182680	1183680	and things like that.
1184800	1187400	You know, another kind of model system to think about,
1187400	1190800	and this is something that Patrick is doing at the bench.
1190800	1193080	And then I have somebody who is doing this,
1193080	1195080	you know, the computational analysis of it,
1195080	1197600	are these gene regulatory networks, right?
1197600	1200120	And so the abstraction, of course, is quite simple.
1200120	1204240	It's just, in the continuous case, it's a few ODE's
1204240	1205760	and they just, you know, their nodes
1205760	1207960	that turn each other on and off and that's it.
1207960	1209720	But if you study these things,
1209720	1213040	you find some really interesting features.
1213040	1215840	For us, one of the most interesting things is that
1215840	1220440	if you do temporary stimulation
1220440	1221440	of the different nodes, right?
1221440	1223080	So you just grab one of the node values
1223080	1225360	and you crank it up or down for a little bit
1225360	1229360	and you keep the structure of the network completely fixed.
1229360	1231080	So you're not changing the weights,
1231080	1233000	you're not changing the topology,
1233000	1234520	the hardware is completely fixed.
1234520	1237120	All you get to do is temporarily raise or lower
1237120	1239320	the activation of any node
1239320	1241080	and then you wait and you see what happens, right?
1241080	1242440	So if you do that and if you treat it
1242440	1245240	in a sort of behavioral science context,
1245240	1248680	you can show things like habituation,
1249520	1251120	basically six different kinds of memory,
1251120	1252880	including Pavlovian conditioning.
1252880	1254280	So these things learn.
1254280	1258080	And we've been very interested in this question.
1258080	1259920	I mean, so I have a couple of papers showing how they learn,
1259920	1263160	but one of the really interesting things
1263160	1266680	is because we don't let the hardware vary.
1266680	1267960	So this is not a scenario
1267960	1269080	where there's some kind of synapse
1269080	1272000	whose weight gets tweaked by experience.
1272000	1274320	The fact that they learn the most,
1274320	1276480	to me, one of the most interesting things about it is,
1276480	1278480	where is the learning stored?
1278480	1280800	And this is something that all of the reviewers
1280800	1282560	of the original two papers got hung up on
1282560	1285480	because we say again and again, the hardware does not change.
1285480	1287080	And then they all said, great,
1287080	1289600	but then you can't have it
1289600	1292280	because where could the memory possibly be, right?
1292280	1294320	And it's this dynamical systems thing
1294320	1295920	where they get chased into a regime
1295920	1299760	where future stimuli are going to cause
1299760	1302400	very different outcomes because of their history
1302400	1304080	than past outcomes.
1304080	1307000	But I wonder, and so this is what I was gonna ask you guys
1307000	1308280	to kind of think of them,
1308280	1311520	to talk about from your framework's perspective.
1311520	1314120	I wonder if the business of uniqueness
1314120	1316960	is related to this sort of issue.
1316960	1319760	I think maybe called privacy or something like that,
1319760	1322640	this idea that there is an inner perspective to a system
1322640	1325000	that's had a certain set of experiences, right?
1325000	1326800	It has a history in the world
1326800	1330680	that is not available to outside observers.
1330680	1334800	And this is, we spent a lot of time with my postdoc,
1335120	1337440	Federico and I spent a lot of time thinking about,
1337440	1338640	you look at a network,
1338640	1340640	can you tell whether it's been trained?
1340640	1342880	And if so, what hasn't been, like, can you read its mind,
1342880	1345120	you know, this kind of neural decoding kind of thing
1345120	1346640	because you're not gonna get it from the hardware.
1346640	1348160	You can, the nodes are no different, right?
1348160	1351080	So in fact, we have a visualizer
1351080	1354000	that tries to show various aspects of it.
1354000	1356760	And if you, you know, on the left and right of the screen,
1356760	1359480	first you start off with the hardware of view of it.
1359480	1362360	And that never changes throughout the whole time.
1362360	1363760	But as it learns, right,
1363760	1366320	over multiple experiences and stimuli,
1366320	1368000	something absolutely changes.
1368000	1369840	And then we have some ways of thinking about it.
1369840	1374120	But this question of, can you, as an outsider,
1374120	1376160	is there anything about mortal computation
1376160	1380160	that speaks to this issue of what you can tell
1380160	1382280	about a system as an outside observer
1382280	1384920	versus what you know as the system yourself?
1384920	1386480	You know, from the inner perspective,
1386480	1388480	is that something you guys think about?
1389480	1391400	Well, I'm gonna give a piece of it
1391400	1394280	and then I'm gonna hope Carl can tag in a little bit
1394280	1398000	because I think he can flesh this out a little bit better.
1398000	1400720	So, and this might be confusion
1400720	1402520	over what you might have explained, Michael,
1402520	1403680	about the reviewers.
1403680	1406280	So you said, I fixed the hardware
1406280	1408600	and on top of that, I fixed the plasticity
1408600	1410320	because you said we can't change the, you know,
1410320	1414240	the values of the synapses or the connection strengths.
1414240	1417200	And I do think mortal computer, mortal computation,
1417240	1418200	we did address this.
1418200	1420920	So Carl and I decomposed it in, again,
1420920	1422960	it goes back to mills.
1422960	1424520	But again, I might be misunderstanding.
1424520	1426760	So we're gonna decouple the privacy
1426760	1428560	and the observer perspective
1428560	1430760	because I wanna hear what Carl might have to say to that.
1430760	1433320	But for why learning would still happen,
1433320	1436600	even when you fix those things, it's just the inference.
1436600	1439000	And the way that we looked at it in mills
1439000	1441800	was there's these different time scales of learning.
1441800	1445880	So if you were to pin the structure of the S and mills
1445920	1449760	and then pin L and say, you can't modify those.
1449760	1451360	Well, we still had one more piece,
1451360	1453600	which was the very fast time scale.
1453600	1456400	And you talk in your poly computing paper,
1456400	1458120	I've done a lot of work in that.
1458120	1459680	Carl obviously has done a lot as well,
1459680	1461840	predictive coding, predictive processing.
1461840	1463680	We always have the inference dynamic.
1463680	1467400	So the idea is that, and I'm sure you thought of this.
1467400	1468600	This is why I was kind of surprised
1468600	1471480	the reviewers were maybe not understanding.
1471480	1473520	So there's like short-term plasticity, right?
1473560	1476000	So the idea is that when you're doing
1476000	1479080	expectation maximization in a predictive coding network,
1479080	1481880	I can still change the neuronal activities,
1481880	1484000	the firing rates or the spiking rates,
1484000	1485760	depending on what model you're constructing.
1485760	1487600	And the synapses never change
1487600	1489480	and forget about the morphology
1489480	1492120	because that's a whole nother ball game.
1492120	1493680	And I would get adaptation.
1493680	1496720	And there was a very interesting paper that came out,
1496720	1499160	I don't know now, just like two weeks ago,
1499160	1503240	Wolfgang Maas in spiking neural nets talked about,
1503240	1505520	well, look, I don't need to modify the synapses.
1505520	1508840	I'm gonna do everything in my spiking neural architecture
1508840	1511400	with just homeostatic variables,
1511400	1512880	which is, you didn't call it them,
1512880	1514680	but that's just the adaptive thresholds.
1514680	1515880	He's like, if these change,
1515880	1520880	so we have this short-term kind of non-synaptic adaptation,
1521520	1522560	you get all these effects.
1522560	1523680	And he actually showed it again,
1523680	1525240	it's a machine intelligence task,
1525240	1527080	but showing in all these tasks
1527080	1531720	without learning in the sense of modifying synapses.
1531720	1533960	And that was very interesting that you can go very far.
1533960	1535320	And I'll try to dig up that paper.
1535320	1538920	It was something I wanted to go in more detail later myself.
1539840	1542520	So in Mills, right, we're just saying,
1542520	1545000	well, okay, we're obviously under mortal,
1545000	1546760	but the inference dynamics
1546760	1549960	and the fact that these still follow the gradient flow
1549960	1552120	of the variational free energy
1552120	1553640	that defines your system
1553640	1556120	or your functionals that you're looking at
1556120	1561200	would explain why that adaptation that you found would happen.
1561200	1562240	And I'm sure you thought of that.
1562240	1565000	I don't know why the reviewers specifically wouldn't have said,
1565000	1566160	well, this doesn't make sense.
1566160	1567600	How could you learn?
1567600	1569240	If you would pin doll three,
1569240	1570400	well, it's a static system.
1570400	1571840	You are freezing it in time.
1571840	1574200	And then that would baffle me.
1575120	1576360	So that's my comment
1576360	1578960	that I do think the framework definitely speaks to that
1578960	1581760	because Carl and I were very adamant
1581760	1584360	about the separation of time scales,
1584360	1586120	at least these big time scales.
1586120	1587640	I mean, there's all these intermediate ones
1587640	1589640	that I'm sure you could bring up.
1589640	1590960	And you need them all
1590960	1592920	because there's a causal circularity
1592920	1595800	if you want to build the most powerful type
1595800	1597720	of mortal computer.
1597720	1599720	And there was a sentence I can't remember
1599720	1602880	because Carl and I have done many revisions of that paper.
1602880	1604560	It might have been in one of the earlier ones
1604560	1606160	where I mentioned something like,
1606160	1610160	well, even though I'm seeing morphology as important,
1610160	1612840	technically, if I was only allowed one,
1612840	1614320	I still have mills.
1614320	1616680	It's just a very simple search space, right?
1616680	1618600	It's a, well, we know that you're here.
1618600	1620880	You can't change the architecture.
1620880	1622800	So we didn't break our framework.
1622800	1625200	So that would allow us to subsume machine learning
1625200	1626600	and say, well, machine learning
1626600	1629360	is like this very, very narrow case.
1629360	1633000	It is doing something that you mills could explain.
1633000	1636680	It's not mortal, but at least it has like a fixed topology
1636680	1639280	and synaptic plasticity is there.
1639280	1641600	And we are just really, really speeding up
1641600	1644160	the inference dynamics by making it one step
1644160	1647680	because we don't use EM most times.
1647680	1650240	And deep neural nets for sure we don't.
1650240	1653320	So that was my comment about addressing the learning,
1653320	1656760	the fact that things, if you fix so much,
1656760	1658320	why would that still happen?
1658320	1659920	And I definitely think mills,
1659920	1663580	that piece of the backbone of mortal computation
1663580	1665280	would speak to that.
1665280	1667040	Now, in terms of the observer effect
1667040	1670320	and what does that tell us about what's going on inside?
1670320	1672160	I have tag team Carl.
1672160	1673920	What do you have to say to that Carl?
1678280	1679440	Right.
1679440	1680720	Well, before I address that,
1680720	1684600	which in my world is a very simple answer, you can't.
1684600	1688360	This week I come back to the,
1688360	1690200	so that was a really interesting exchange
1690200	1693800	and really interesting examples there.
1693800	1695760	And I was just thinking from the point of view
1695760	1700760	of the sort of the classical flows and physics
1701360	1706120	that would provide a simple picture
1706160	1708120	of how on earth you can remember stuff
1708120	1710160	without changing your connection weights.
1710160	1714400	And I think Alex, you identified the key thing here,
1714400	1716160	which is the temporal scale.
1717080	1720240	So, well, where to start?
1720240	1722120	It's interesting you introduced Wolfgang Maas
1722120	1725560	because he for many years has been the king
1725560	1728040	of liquid computation and neck of state machines,
1728040	1731760	which is not, has the same kind of semantics
1731760	1734880	as the liquid brain and it's a very powerful
1735880	1738920	black boxy like kind of a dynamical system
1738920	1743920	approximator that has been proposed as one architecture
1744560	1746360	for doing predictive processing
1746360	1750040	and model computation of the sort.
1750040	1753360	But the key, I think the key point that has just been made here
1754920	1758800	is that the dynamics matter
1758800	1763800	and the dynamics are shaped by the landscape
1764600	1767200	Lagrangian variation free energy, whatever you want.
1768200	1773200	And that is a function of the implicit gradients
1773640	1776840	that depend upon the sensitivity of all say the nodes
1776840	1778840	in any given network.
1778840	1783000	That sensitivity can either be read as a connection strength
1783000	1787880	or it can just be read as a sensitivity,
1787880	1792480	in terms of to what extent do I change my internal dynamics
1792520	1795480	given this particular external perturbation.
1795480	1798920	And of course, that becomes time and context sensitive
1798920	1800200	with any nonlinearities.
1800200	1803000	So if you're talking about a nonlinear system,
1804240	1809240	then there is a, the bright line between the connection
1809280	1812920	strengths and the current effective connectivity
1812920	1815440	at this point of time in this context,
1815440	1818160	in this part of face space or state space
1818160	1819480	becomes very blurred.
1820320	1822280	So if you're writing down the differential equations,
1822280	1823440	you could go one of two ways.
1823440	1827200	You could just write down a random differential equation
1827200	1832200	with loads of variables representing interactions
1832640	1835760	between different types of states
1835760	1838320	and the response, the rate of change
1838320	1842200	of any particular state that would entail
1842200	1844400	the nonlinearity in question.
1844400	1846160	Or you could arbitrarily say, okay,
1846160	1850240	now one subset of these variables changes very, very slowly
1850280	1853200	and I'm gonna call them connection strengths.
1853200	1855920	And I'm now gonna lift those out of my equation.
1855920	1858720	So I'm now left with a much simpler sort of
1858720	1860280	autonomous differential equations
1860280	1862960	that are now parameterized by other states
1862960	1865360	that change very, very slowly.
1865360	1867120	Mathematically, you haven't done anything
1867120	1869480	but introduce a separation of temporal time scales.
1869480	1873480	But in so doing, you have now got a different kind of rhetoric
1873480	1878160	where initially you were talking about voltage sensitive
1879120	1880720	receptors and sensitivity
1880720	1884600	and contextualization conductances and the like,
1884600	1886320	which sets the synaptic efficacy,
1886320	1888640	which is fluctuating moment to moment.
1888640	1892320	And now you're talking about these being the connection
1892320	1896080	strengths, the parameters of your structure
1896080	1901080	in a mills-like context or the strengths of your connections
1903000	1906520	or weights in a machine learning context.
1906520	1909120	But the only difference, I repeat, is just the time scale.
1909120	1911960	So talking to Mike's example,
1912960	1916160	how can you have memory without changing your connectivity?
1916160	1920520	Well, you're just appealing to initial conditions
1920520	1922080	in the context of a nonlinear dynamical
1922080	1923760	and run of a dynamical system.
1925880	1928560	At what point would you start calling this
1929680	1931600	the kind of memory that could be encoded
1931600	1932800	in terms of connection strengths?
1932800	1934960	Well, in those kinds of systems
1934960	1939440	where the key, not second order nonlinear interactions
1939440	1942040	rest upon a subset of variables that change very, very slowly
1942040	1945480	and you say, well, okay, under that adiabatic approximation,
1945480	1947520	then we'll now call this a different kind of memory.
1947520	1950560	And it's just because it's slightly slower.
1950560	1955560	So I think it's, well, I liked the emphasis
1956680	1959280	on the separation of time scales
1959280	1964280	because I think that would have dissolved the reviewer's concerns
1964760	1967760	if you were just talking about really fast learning
1967760	1970240	in the moment that is all in the nonlinearities
1970240	1972080	and the dynamics.
1972080	1974360	I keep emphasizing the nonlinearities, Mike,
1974360	1979360	because of that sort of the paradox of change.
1980000	1984480	So as soon as you have nonlinear dynamics in any system
1984480	1987600	that has at one particular time scale
1987600	1992160	an attracting set or a random or a pullback attractor,
1993040	1994960	you have that itinerancy,
1994960	1997320	which means that there will be some form
1997320	2002320	of changing sensitivity to all the things
2002520	2004600	that I am coupled to.
2004600	2009520	That is definitional of things that have that biotic
2009520	2011520	or sort of characteristic kind of set.
2011520	2015440	So, you know, the nonlinearities are certainly
2015440	2016520	from a classical perspective,
2016520	2018520	I think they're absolutely key here
2018520	2020880	and resolve a lot of the distinctions
2021320	2024840	and give you now a relatively simple picture
2024840	2029840	that if there was some way to tell the next version of me
2030080	2034720	where I started, give the next version of me
2034720	2037720	my initial conditions in the past version of me,
2037720	2040280	you can, I would imagine quite simply
2040280	2043080	just write down systems that have this kind of memory
2043080	2044920	which does not involve it anyway,
2044920	2047920	a change in the connection weights.
2047920	2050920	And I'm just wondering whether that, you know,
2050920	2055160	that if you wanted to simulate that remarkable fact
2055160	2058840	that the worms remember
2058840	2062760	that they are on a two-headed trajectory
2062760	2064920	even when they start again.
2064920	2067000	I mean, I think the deep question here is
2067000	2070000	how on earth did they inherit the initial conditions
2070000	2075000	that characterised the termination of their parent
2076000	2077960	or what they inherited from.
2077960	2081520	I think, again, that speaks to this coupling
2081520	2083600	between different temporal scales.
2083600	2086000	You know, is this a messenger RNA, you know,
2086000	2089760	and how does that propagate through to the electric fields
2089760	2091880	and how does it get back top-down causation,
2091880	2094160	get back in again, it's a fascinating example.
2094160	2096200	And I've heard that before, I'm sure you've told me
2096200	2099560	but I probably ignored it because it's so remarkable.
2101080	2102800	Not easy to explain.
2102800	2105520	In answer to the question, can you ever know
2106520	2109520	what's going on inside a system?
2109520	2110360	No.
2111600	2113560	And I say no polemically from the point of view
2113560	2114680	of the Fianco principle.
2114680	2118160	You can never know what's beneath a Markov blanket.
2118160	2119920	You can never know what's on the other side
2119920	2121200	of a holographic screen.
2121200	2123240	That's the whole point of a holographic screen
2123240	2124720	or a Markov blanket.
2124720	2127760	All you can do is bring a best guess
2127760	2130640	and as if explanation to the poly computing,
2130640	2134400	if you like, in the bulk on the other side,
2134400	2139400	which means, you know, I think that's simple observation.
2139400	2144400	The whole point of that screen or Markov boundary
2146320	2149040	is that there is a conditional independence
2149040	2151160	given what you can measure.
2151160	2155560	So you can never know other than infer
2155560	2157920	by what you measure from the behavior,
2157920	2161560	the inputs and the outputs of a particular system.
2163000	2163840	Amazing.
2163840	2165520	So two questions then.
2165520	2169920	One is, is there, is it just a flat no?
2169920	2172640	Or is there a degree that is easier to know
2172640	2173920	for certain kinds of systems?
2173920	2177280	And then for sort of advanced living cognitive systems,
2177280	2178120	it's really no.
2178120	2180400	Or is it just like, is it always the same?
2180400	2182560	Or is it a matter of degree?
2182560	2183680	If you're directing at me,
2183680	2186240	the answer I'm afraid is always no.
2186240	2189600	But I don't mean that in a sort of pessimistic or,
2189600	2192240	I mean, the question, you know,
2192240	2197240	how do you infer what kind of Bayesian mechanics
2199640	2202840	or poly computation is going on underneath the Markov blanket
2202840	2205200	or inside a cell or inside a brain?
2206200	2210000	That question is, of course, my day job
2210000	2212960	and the day job of nearly every neuroscientist.
2212960	2214960	It's peaking underneath the Markov blanket
2214960	2216840	in a noninvasive way that doesn't destroy it
2216840	2220800	to try and understand the mechanics
2220800	2223240	and to test hypotheses about what is going on.
2223240	2226720	But you're always testing hypotheses you will never know.
2226720	2229440	So there will be situations where the functional anatomy
2229440	2232640	or the architect reveals itself
2232640	2234480	through noninvasive imaging, for example,
2234480	2236640	or even invasive techniques
2236640	2239280	of the kind that you use every day.
2239280	2242480	But all you're doing is basically testing hypotheses
2242480	2246480	about what you think the gerative model is under the hood.
2247440	2250200	And once you know that or put it another way,
2250200	2251920	if you knew the gerative model,
2251920	2253040	then you know the Lagrangian.
2253040	2253880	If you know the Lagrangian,
2253880	2256120	you know the intrinsic or internal dynamics
2256120	2260080	and you can tell a story about poly computing,
2260080	2262120	tell a story about Bayesian mechanics.
2262120	2263960	You can tell a story about perception.
2263960	2265680	You can tell a story about memory.
2265680	2267520	You can tell your basal cognition.
2267520	2271320	But these are just stories predicated on the Lagrangian
2271320	2274320	that governs the intrinsic dynamics
2274320	2276880	and all that the free energy principle brings to the table
2276880	2280000	is that you can express that Lagrangian
2280000	2283640	as a function of a probabilistic gerative model.
2283640	2286960	So your job is now to identify the form,
2286960	2289840	the functional form and structure of that model
2289840	2292320	and all the processes that it entails.
2292320	2293720	But every time you do that,
2293720	2297680	you're just testing a hypothesis theory of mind for me
2297680	2299440	and theory of mind for your Xenobox
2300080	2302520	and theory of mind for yourselves.
2302520	2305600	Of course, you can break down at different scales.
2305600	2310320	So it would be possible to ask about the sense making
2310320	2313960	and sentient behavior of a single cell in my brain
2313960	2316960	if you were able to isolate it and get inside there
2316960	2320920	and do molecular biology or do cellular biology.
2320920	2324720	But you would no longer be looking at my brain at that scale.
2325600	2329040	And so, you know, but you could sort of cut across scales
2329040	2330640	in the good old fashioned way
2330640	2333280	and start to tell an internally coherent story
2333280	2336520	about how it all fits together across scales.
2338280	2340160	Could you back up what Carl said quickly?
2340160	2341280	Not to interrupt you, Michael.
2341280	2342120	Yeah, go for it, go for it.
2342120	2344920	And it's partially gonna be a question for Carl
2344920	2346600	as he triggered an interesting thought
2346600	2349040	and then just to quickly say to you, Michael,
2349040	2352400	that's also why I was hesitant because by definition
2352680	2355840	since mortal computation rests on the Markov blanket
2355840	2358400	and it's underwritten by the free energy principle,
2358400	2361160	I also commit to that as well that no, the answer is no,
2361160	2363760	you can't see what's under the Markov blanket.
2363760	2365400	And that's why I was curious enough Carl
2365400	2368360	would give me anything that may I just didn't know about.
2368360	2370480	So that was one comment to you,
2370480	2372960	to Carl and to everyone, really.
2373880	2378880	So mortal computation also subsumes artificial systems.
2378880	2381160	So this is where I was curious to know
2381160	2385480	if Carl, we were to design the internal states,
2385480	2389240	all the dynamics, we, again, we have the environment
2389240	2391200	but let's just say you dissimulate it,
2391200	2392920	you're designing the environment
2392920	2394520	and you design the Markov blanket
2394520	2397000	because we talk about even in our paper,
2397000	2399400	potential sketches of things that you could use
2399400	2403240	to build the boundary and the transduction pumps
2403240	2405720	and all the sub pumps and to actually build
2405720	2408400	a viable artificial organism.
2408400	2410720	Now we have the internal dynamics.
2411480	2414880	We are the designer and we have specified internal,
2414880	2417360	external and the boundary.
2417360	2420200	Is there something I'm missing that we would still,
2420200	2422040	cause we've created the Markov blanket.
2422040	2423560	So now we know what's on the other side.
2423560	2426660	So the answer to Michael is not for natural systems
2426660	2430200	that we obviously cannot, did not create, but we made it.
2430200	2431520	So we made the internal systems.
2431520	2434360	There's some other concept I'm missing
2434360	2436040	cause you would be able to now say,
2436040	2439440	I know everything cause I built the internal states,
2439440	2442040	I specified every bit of the dynamics
2443160	2445400	and let's say the environment, you know,
2445400	2447240	we've constructed that,
2447240	2450360	we've constructed the Markov blanket, the boundary.
2451280	2452880	What about that case?
2452880	2455760	Is it, now we are inspecting it cause we made it.
2455760	2458880	So we obviously don't need to infer it, we know it.
2458880	2460080	What about that?
2460080	2463440	I was curious to know your thought of designed internal states
2463440	2467240	and designed external states and designed Markov blankets.
2468200	2469600	If that makes any sense.
2469600	2474600	Yeah, Mike's gone to the door, so I'll respond to that.
2478440	2482040	So, yeah, I'm not going to give you
2482040	2485200	any deep philosophical insight.
2485200	2488120	You don't already have, but just a very practical one.
2488120	2491760	I mean, you know, what you just described
2491760	2496760	is an application of the free energy principle
2496920	2501920	as a method to simulate various mortal computations
2503880	2507480	in the service of building hypotheses
2507480	2509880	about how this thing might work mortally.
2510880	2514200	So practically that's what we use the free energy principle for
2514200	2519240	and the design is at least mathematically
2519240	2521680	very straightforward in the sense I repeat,
2521680	2524840	all you need to know is the gerative model.
2524840	2526680	So all you need to be able to write down
2526680	2529600	is a probability distribution of all the causes
2529600	2531840	and consequences that constitute your system.
2531840	2532840	That's it.
2532840	2534480	If you can write that down
2534480	2537720	and you can instantiate that in a von Neumann architecture,
2537720	2539640	you then just solve the equations of motion
2539640	2541680	that are the gradient flows.
2541680	2544800	We will have a certain amount of component on that Lagrangian
2544800	2546800	and you can simulate sentient behavior
2546800	2549560	and sense making, perceptual actions, self-organization,
2549560	2551600	everything that you want to do.
2551600	2553320	Why would you ever want to do that?
2553320	2555800	Well, in order to test hypotheses
2555800	2558880	that this reproduces the kind of behavioral thing of interest
2558880	2561600	which for something like me would be a psychiatric patient
2561600	2566600	for something like might be a multicellular organism.
2570920	2575920	So you are now using a simulation
2578800	2582560	as a way of generating predictions
2582640	2585880	that then you can match against the observable parts
2585880	2588120	of the system of interest which adjust the surface.
2588120	2591360	You have to adjust the action on that system
2591360	2594760	and to the extent that the sensory inputs of that system
2594760	2596360	are also known.
2596360	2597840	That's all you have access to.
2597840	2600080	So literally that's how we practically use
2600080	2601920	active inference for example.
2601920	2606920	We just create simulations of Bayesian mechanics
2608760	2610440	in a given paradigm
2610440	2612000	and then we adjust the gerative model
2612000	2614760	more specifically the priors of that gerative model
2614760	2619360	until it renders impurity observed choice behavior
2619360	2623440	the most likely under the probability distribution
2623440	2627200	of the actions of my simulated simulated.
2627200	2629440	So in that sense you're specifying the structure
2629440	2634440	but one could argue that even treating the laptop computer
2635080	2637600	that is so non-unique
2637600	2639760	because it affords the opportunity to abstract
2639760	2641560	and do these simulations.
2641560	2643640	I'll come back to Chris's point.
2643640	2645040	Even then you don't actually know
2645040	2647520	what's going on underneath the hood.
2647520	2649000	And certainly in conversation
2649000	2651240	with people designing some risk architectures
2651240	2654680	and sort of looking at the most efficient buses
2654680	2659680	they have to guess what's actually being passed here
2659840	2661840	and there and measure it and get proxies
2661840	2664320	like temperature and that kind of thing.
2664320	2666640	You can certainly specify the initial conditions
2666640	2670120	and the structure and you can do a,
2670120	2671920	you can reboot and reset it.
2671920	2675480	So you can to a certain precision specify
2675480	2678440	the initial conditions and the structure
2678440	2682320	of which the, you know, that the ensuing dynamics will occur
2682320	2687320	but to actually know the message passing of a computer
2687580	2690120	even in simulation, I think would be,
2691320	2694280	I think you would be able to return to your hard no.
2696600	2698640	Certainly on the level of, you know
2698640	2701480	the quantum level that Chris was referring to.
2701480	2703000	Again, it would be unknowable.
2703920	2704960	But it's an interesting point
2704960	2707680	but it does foreground the role of simulations in this,
2707680	2711320	I think and it comes back to, you know, this, you know
2711320	2712960	why do we want to know all this?
2712960	2715480	Well, it's just to basically build,
2716840	2721720	formalize hypotheses in terms of simulations
2721720	2724600	that now embody our hypothesis
2724600	2726920	and then look at the empirical system
2726920	2728040	to see whether, you know
2728040	2730680	that hypothesis was correct.
2732920	2736480	Can I just add another point of view on this?
2739960	2743600	And if we think about what we do in practice
2743600	2746520	with ordinary computers
2748280	2750400	where we have built the thing, et cetera,
2751360	2753560	part of building the thing,
2753560	2755900	it's not just assembling the hardware.
2756780	2761220	We also put a lot of work into building these interfaces
2761220	2763180	that we call programs.
2764340	2769340	And so if I'm using some sort of debugging tool
2769340	2770540	or something like that
2770540	2774540	where I can run a program in one window
2774540	2776100	and see what's happening
2778980	2783980	at some level of the execution trace in some other window
2784980	2789980	what I've done is constructed a Markov blanket effectively
2792860	2793940	to use that language
2794980	2798100	that has a bunch of different IO channels
2799180	2804180	that access different parts of what's going on in the device.
2804940	2809940	So we could think about from a biological perspective,
2810020	2811500	we have these cells
2812500	2817500	that come equipped with their own native IO channels.
2821860	2824300	But there's nothing that says that we couldn't,
2824300	2828900	in principle, build some more channels into the things
2830260	2832100	so that we could see,
2832100	2835980	we could see more about what was going on in the inside
2835980	2838900	not by penetrating the Markov blanket
2838900	2842860	but by adding some IO capacity to the Markov blanket.
2843780	2845500	What does that mean physically?
2845500	2847940	It just means you're using a different interaction
2850420	2854220	because it's the interaction that defines the blanket
2855060	2859180	as a set of information transmitting states.
2860180	2865180	So I think we always have the hard no of the Markov blanket
2868980	2871980	but we also from an engineering perspective
2873540	2878540	because we can interact with these systems
2879940	2883380	in ways that other parts of their environments
2883380	2886740	can't interact with them or don't interact with them at least.
2887020	2890780	We're a part of the environment that can open up
2892860	2895140	new communication channels through the blanket
2895140	2897660	by changing the interaction
2898580	2902900	that effectively changes the state space
2902900	2905020	in which the blanket is defined.
2909540	2912700	Augmenting the Markov, let's say with reporters
2912700	2914700	or with optogenetics, I would imagine
2914700	2916420	it's a good example of this too.
2918220	2920140	Yeah, well, I mean, in a sense,
2921380	2923420	FMRI is a good example of that.
2924620	2925580	Yes, yeah.
2925580	2928380	Right, we were just adding an IO channel to the brain
2928380	2931060	that wasn't there before.
2934820	2936220	I want to...
2936220	2937060	Sorry.
2937060	2938980	No, no, please, Carl, keep going.
2938980	2941500	No, I was just thinking out loud,
2942500	2947500	so the catch word in my world is sort of non-invasive,
2947980	2949580	brain imaging, and that has a meaning,
2949580	2951820	that you're non-invasive, but there isn't,
2951820	2956820	there is a whole centuries worth of legacy
2957020	2959900	of invasive studies and lesion deficit models
2959900	2962140	and depth recordings and the like,
2962140	2964980	which I think speak to this,
2964980	2968700	how far into the Markov blanket can you peer
2968700	2972380	without destroying the thing that you're trying to,
2972380	2975260	in the Heisenberg sense, trying to get out.
2975260	2976100	I was witching.
2976100	2977860	I'll shut up and have a quiet cigarette
2977860	2979860	while I listen to you now.
2979860	2984860	I just add that it's non-invasive kind of by convention
2984940	2988300	and that you're invading the brain with a magnetic field
2988300	2990260	that wasn't there before.
2990260	2992100	You're just not damaging it much.
2994180	2996940	Like Carl said, it's invasive,
2996940	2999700	but I wonder what that tells us then
2999700	3003260	about augmenting the Markov blanket.
3003260	3005780	Because yeah, Carl usually will say,
3005780	3007980	if we want to non-invasively understand it,
3007980	3011020	well, then yeah, you can't peer under the Markov blanket.
3011020	3013980	So mortal computation, I think,
3013980	3016500	kind of is connecting towards the idea
3016500	3017700	of what you're saying, Chris, right?
3017700	3020260	Because we can augment, we can add IO channels.
3020260	3023340	You are designing, engineering these things.
3023340	3025500	So now this was something that did not exist.
3025500	3028740	I don't know, Michael, how does that shed light
3028740	3031100	on the question that you originally wanted to get at,
3031100	3034420	which is, you know, peering at these internal states.
3034420	3036220	Because I think this is like an indirect way.
3036220	3039380	Because I was, Carl gave me great answer about,
3039380	3040460	well, if I just design,
3040460	3042460	because my brain goes to the ultimate engineering,
3042460	3044060	I'll just design it all myself.
3044060	3046940	And I even was thinking about if I build the hardware,
3046940	3049860	but Carl's right, even when you get to hardware,
3049860	3051420	you're guessing a lot of times,
3051420	3053300	even with the best educated guesses.
3053300	3056340	So there's still the Markov blanket
3056340	3058500	that you're not really breaching.
3058500	3061340	But if you perturb or change,
3061340	3064100	it's even like augmenting the cell membrane
3064100	3065980	with something in your lab's group
3065980	3069300	is, you know, an example of modifying these things.
3069300	3071540	What does that do to your question?
3071540	3073700	How does that shed light on what you're thinking?
3073700	3077300	Yeah, yeah, I mean, I even wanted to do,
3077300	3079580	talk about a much more annoying aspect of this,
3079580	3082620	which as I always do, which is to take it way down.
3082900	3086620	So nevermind brains, nevermind even cells, right?
3086620	3090220	My question, you know, also extends like that transition
3090220	3092140	from, you know, you got a pendulum,
3092140	3095220	you got a thermostat, you've got, you know, right?
3095220	3096700	And you can sort of build these things up
3096700	3098780	and then eventually at some point you get to a cell.
3098780	3103060	So I'm still curious about whether this impenetrability
3103060	3104740	goes all the way down.
3104740	3107020	So we can't read the mind of a pendulum either,
3107020	3112140	or there is some sense of progressive opacity
3112140	3115660	as you climb this sort of continuum of cognition
3115660	3117700	from extremely simple systems,
3117700	3120540	where I think the conventional story is,
3120540	3122740	hey, look, it's all third person accessible.
3122740	3124460	We know exactly what's going on,
3124460	3126660	but at some point you don't.
3126660	3128820	And so I'm curious, when does that happen?
3128820	3130580	And if we think there's a phase transition here,
3130580	3132500	or if we think this is smooth, I mean,
3132500	3134780	I tend to think everything is more or less smooth
3134780	3136740	in these cases, but maybe I'm wrong.
3138300	3141740	Yeah, that's one thing I wanted to kind of probe
3141740	3143460	a little bit is how does this play out
3143460	3145940	when you start, not start at the brain, you know,
3145940	3149060	where, okay, we can all agree that's sort of very opaque,
3149060	3152540	but what about from the most simple physics systems, right?
3152540	3154220	How do you get to that opacity?
3155420	3157460	Yeah, and then on the flip side,
3157460	3159740	and I'm conscious that I don't want to monopolize this up,
3159740	3160860	we only have five minutes left,
3160860	3165460	but something that's very interesting,
3165460	3168180	I think an implication of this is that we can't really know
3168180	3169340	and we're all inferring.
3169340	3173980	If you take the approach that I took in this memories paper,
3173980	3178180	where your future self has to make a lot of guesses
3178180	3179900	as to what your memories mean,
3179900	3181900	because they were written down by your past self
3181900	3183300	and you don't have all the metadata,
3183300	3186220	you have to now interpret these compressed n-grams,
3186220	3189260	then that leads to this kind of more,
3189260	3191540	the little kind of disturbing question is,
3191540	3194940	so we can't even tell what we used to think really, right?
3194940	3197780	We can sort of guess, but we don't really know then
3197780	3201580	if this is the case, if that impenetrability holds,
3201580	3203820	then it's there with respect to our past self
3203820	3205580	and our past memories too.
3205580	3207740	So that's kind of wild.
3207740	3209860	I don't know what we all have to say about those.
3211300	3213500	I'll refer to this wonderful thing
3213500	3215580	called the Conway-Cocon Theorem,
3215580	3217500	Conway of the Game of Life
3217500	3220420	and Cocon of Quantum Contextuality.
3221940	3224180	This was published three decades ago now
3224180	3225700	or something like this,
3225700	3229660	but they proved using mainly relativity theory
3229660	3234660	that if they considered a generic observational scenario
3238340	3242500	and they said, if in any generic scenario,
3242500	3246580	what you consider the observer has free will
3246580	3248820	in the following defined sense
3248820	3253260	that what the observer does is not completely determined
3253340	3255700	by that observer's past like tone.
3256540	3259860	So this is, if the observer is not subject
3259860	3262060	to local determinism,
3262060	3263500	then the thing being observed
3263500	3266020	is not subject to local determinism either.
3267140	3268980	And at the end of the paper, they say,
3268980	3272100	so one could ask, do we really mean
3272100	3274580	that electrons have free will
3274580	3276860	in the same sense that observers do?
3276860	3278460	And the answer is yes.
3280420	3283220	They say in their paper emphatically.
3283220	3285300	So if you did on the free will theorem,
3285300	3287500	yeah, I remember you're catching this.
3287500	3291700	Yeah, and if you drag quantum theory into the picture,
3291700	3293940	then you get an equally strong result
3293940	3298940	that any system has to be able to effectively choose
3303660	3306700	its own semantics for how it interprets
3306700	3308660	whatever incoming information is.
3309660	3311180	And if you take that choice away,
3311180	3312460	then you get entanglement.
3312780	3315820	The system ceases to have a separate identity.
3317260	3320380	So I think the answer,
3320380	3322860	kind of the principle answer about opacity is,
3322860	3324460	yeah, it goes all the way down.
3328020	3329620	Can I just follow up on that
3329620	3334100	and refer to Chris's in a screen hypothesis
3334100	3336860	and the notion of an irreducible Markov blanket?
3338300	3340620	If you've got a system
3340620	3342500	that has no internal Markov blankets,
3342500	3344660	has no deep structure or hierarchical structure
3344660	3346460	or heterarchal structure,
3346460	3351020	then that is, I think, where the hard no would apply
3351020	3354180	or the hard yes of unknowability.
3354180	3356580	But clearly if it has internal Markov blankets,
3356580	3358860	you can peel away and invasively
3358860	3361460	or non-invasively start to get within that.
3361460	3364260	So I think what you're talking about
3364260	3368180	is in that sort of vague gradation
3368180	3370740	of things that are noble and unknowable,
3370740	3372940	it's just the hierarchical, well,
3372940	3376380	the depth of the Markov blankets of the Markov blankets.
3376380	3379420	And there is a kernel of either
3379420	3381380	an irreducible Markov blanket,
3381380	3383060	which you can never get into
3383060	3387140	because you change the thing itself.
3387140	3388580	But there's also a limiting case
3388580	3390260	from the point of view of classical physics,
3390260	3392060	which is when the inner states,
3392060	3395340	intrinsic, the internal states are the empty set.
3395340	3399660	So things like inert particles or stones
3399660	3402260	don't have internal states,
3402260	3405340	they just have Markov boundary states.
3405340	3407420	So I think you're absolutely right to think of this
3407420	3412420	as a gradation, that there are inert things
3413500	3414860	that are defined operationally
3414860	3417740	in the sense that their internal states are the empty set.
3418820	3422060	And you could also say that there are sessile things
3422060	3423260	that don't have active states.
3423260	3427220	So all of the states of this kind of particle
3427220	3429460	are just sensory states, they're just inputs.
3430460	3432940	Inputs that can also influence the outside.
3432940	3434660	There's no restriction that sensory states
3434660	3437300	have to not influence the outside.
3437300	3438980	So there still can be observable.
3438980	3441540	And then you get to things that now have a non-antiactive
3441540	3446540	sector of the holographic screen or the Markov blanket.
3448180	3449300	And these are things that move.
3449300	3452020	So you might think these are natural kinds
3452060	3454420	that have mobility or motility.
3454420	3459420	And then you get to things that whose internal states now
3460580	3463500	have a Markov blanket within them.
3463500	3466700	And these would be the kinds of things that can fan.
3466700	3468820	And these are usually multicellular things
3468820	3472020	or certainly compartmentalized things.
3472020	3476180	I think at each stage, the no ability depends upon
3476180	3480460	whether either the internal sets are empty or not,
3481300	3483460	or they are irreducible in the sense
3483460	3485060	there are no Markov blankets within them.
3485060	3488580	So I think it's a nice, simple mathematical picture
3488580	3492900	of that gradation that speaks exactly to the electron
3492900	3496380	through to the pendulum, to the thermostat
3496380	3500340	through to a smart thermostat that starts to worry
3500340	3502940	about whether you want it warmer or colder or not
3502940	3505380	and starts to pan ahead and moves from homeostasis
3505380	3506620	to allostasis.
3506620	3509700	All of this would speak to at different scales,
3509740	3512180	equipping Markov blankets and Markov blankets
3512180	3513660	and inducing a deep structure.
