I appreciate that. Yeah, I'm really looking forward to this.
I think you guys both had some great ideas recently, and I'm looking forward to it.
Can we just start and maybe, let's see, Ben can go first and then David,
just say a couple of words about who you are and what your interests are.
I'm a former stockbroker with the background in economics,
and I'm currently studying the economics of collective intelligence,
looking to put together an understanding of how people get along
and accomplish things that no individual can.
Cool. I am by training a stockbroker engineer, but I've been into AI
ever since the high school reading age of intelligent machines,
and then just kept dipping in and out of the AI field and kind of being like,
it's not ready, it's not ready. After college, I co-founded one of the first AGI startups
with like Shane Legg, one of my co-founders was our first employee.
So like back in 2000s, and then went to work at Google and Facebook,
and then Asanda, mostly either working on ML, but most of my time has been building
like large-scale distributed systems. So really coming at this more from
an engineering background, but also about three years ago, I left my job and
dove back into AI and came across Carl Friston's work.
And I think that's when it kind of all clicked for me.
And I was like, wow, we have GPUs now, we have a theory of intelligence
that I feel like basically covers. It's like, I don't think there's any more secret sauce.
It's like, we kind of, the formulas from active inference look a lot like formulas
from traditional reinforcement learning. We have the hardware now, so I just like
got really excited about it and have been working on it.
So that's like my software and engineering side. I also co-founded Plurality Institute
with Glenn Wild, who's like an economist, and that's more on the human collective intelligence side.
Essentially, we're trying to create a new field, academic field called Plurality Studies that
focuses on human, scaling human cooperation and collaboration, essentially collective
intelligence as an academic field. And we've been bringing together researchers from like
political science, computer science, economics, peace building. I mean, it's such an interdisciplinary
field of like, how do we help humans cooperate and using technology, and now especially AI to
bridge people. So that's kind of my other project. But my main research focus is,
and I can share a little bit of like the research that I'm working on. And I would love, you know,
help and collaboration. And also would love to hear how you guys are thinking about this. And if I
can help, because I think this is like the most exciting thing that's happening in the world right
now. Cool. Yeah, please go for it. Yeah. Yes. So I will say yes. So Glenn Wild is an interesting name
because I think he's got a very important idea, which is that we should be using markets to
solve more problems because there's basically a perspective in economics that says,
anytime you've got a problem, there's a market that can solve that problem. And people just
haven't been creative enough historically about what kind of markets you can use. Like we're just
very used to like trading like standard property rights and things. And there's a lot of other
ways you could conceive of doing things. I like the fact that Glenn Wild is pushing that I'd be
interested in. So I don't actually know what he's doing. So actually, I'm just curious about that.
I don't actually know what he does with AI. So can you just tell me a little bit about what he does
with the Plurality Institute? Yeah, well, the Plurality Institute is not is more of the academic
like collective and human collective intelligence of which AI is like a tool. So for example,
and I mean markets are one collective intelligence system. But then there are other ones like even
like common sections on forums are collective intelligence mechanism where humans come together
to synchronize on epistemology like on things that they know and believe and being able to use
LLMs to say automatically deescalate flame wars in common sections and drag people to consensus
is like an example. In terms of the more economics things, so using quadratic funding for like public
goods funding is something he has been writing about creating identity like trust networks to
create identity like social identity so that you can use that as infrastructure for all these other
like market mechanisms is another one. There's someone that's working on vector prices. So rather
than there being a single price signal, there's a whole basket collection of currencies that you
hold and that people can specify their own values for different currencies. And then the way the
trading works is basically when you're trading with people that are aligned with you on values,
you automatically get discounts because they value your currencies more than they value some.
So there's like a whole bunch of them. He just has a book out open source isn't like another like
how large open source projects are incentivized and how collaborations occur without like a
corporate structure. He has a book out now called plurality with Audrey Tong who is another
inspiring person. So I would check that out and that's like is going to cover a lot of different
aspects of collect like plurality and collective intelligence. Cool. Is a lot of that just on
the plurality website? The plurality instant that instituted website, we like post events and talks
and we have the link to the plurality book by the plurality book is like much more detailed.
I will check that out. So maybe what I can do then is just kind of give a broad overview of what I
think the basics of economics of collective intelligence are and we can see how that relates
to our work. So the way I see it is that collective intelligence is all about getting people to
communicate in honest ways in honest and relevant ways such that everybody forms a shared model
and because everyone's behaving with that same shared model everyone ends up behaving as if
they're all following the same you know sort of commands if you will like a dictator or a virtual
governor and everyone ends up nicely coordinated to achieve that kind of signaling system. You need
something that's analogous to a price system and what that allows you to do is it's principles
very similar to what Mike has talked about in some of his work where you get a way of getting an
entity to see questions about other people as questions about itself and so like information
gets transmitted in such a way where like when someone else is in trouble you perceive that as
stress to yourself when something benefits someone else you perceive that as benefit to yourself
which we see very easily in the price system like if someone needs more food they'll you know have
a demand for food that'll raise the price of food so that stress gets transmitted to everyone else
which uses to buy less food or similarly if you help someone get food by supplying some food that
they like their benefit becomes your benefit via the revenue you make some money so that very
simple system ends up outlining really just the basic functionality of a collective intelligence
I think that any collective intelligence probably follows basic price like principles
where you need some system where people aren't incentivized to communicate honestly with each
other and such that everyone's signals gets condensed into like your own personal situation
so just by optimizing for yourself you end up inadvertently taking care of having one else
so are things like that when you're looking at like AI and technology and stuff like I don't
know what goes on inside a computer it's all magic to me is that is that what's going on with like the
transistors do they all have their own like individual model and they end up like communicating to
form a shared model you know I mean I guess it's not level everything is doing the longest right
everything is like mutually reducing free energy and you can feed every part of the
subsystem as an agent so in some in some level yes but I think what you probably mean is more
directly in a computational way and like typically what we do now is we train neural networks which
are a big pile of math but but essentially it's a thing that like learns function approximations
and the way it learns function approximations internally and we're still studying you know
what actually happens internally but probably yes things get partitioned into sub processes
like sub functions with sparse or communication between them so kind of what you're described
if you think like in your model an individual is this like very interconnected system and
then it has these sparse connections to other deeply interconnected systems and this is like
you know a cell is doing a lot of compute inside but then it's got just like a few
membrane exchanges with outside or corporation there's a lot of stuff going on inside but it
communicates with the rest of the economy the buying and selling products so in some sense anything
that you partition into subsystems is essentially doing this it is using some form of information
like sparse information propagation between units in terms of honesty I think that's a lot
trickier because it's like the communication doesn't need to be honest it needs to be valuable
to like I think there is like a theorem that like for communication to exist it has to benefit
both parties and so there has to be some value to it and you can maybe say that that value is the
truthiness and so you could like maybe decouple it into like signal and noise so there's like some
truth value and then maybe there's like a bunch of a noise value if it's all noise that communication
channel is just not going to persist and so in terms of I think when you're like hey there should
be honest pricing mechanism I would instead say there should be the two agents or whatever the
network of agents there needs to be ways for them to communicate usefully with each other which it
may be is the same thing but they kind of co-discover what that is and that doesn't necessarily need to
be imposed by a mechanism but if a market mechanism or some communication framework
allows honest communication channels then you're making it much easier for them to discover how
to use it. I don't know if that if any of that resonated. It does and maybe there's something
interesting to work out here so economics has been talking about the importance of getting
people to communicate honestly to achieve coordination for a long time and sort of the
basic picture of that which is very silly and false in some ways but also maybe important for
intuition is if you're trying to centrally plan an economy one conceivable way you do that is
you know you have an allocation problem who should get what goods you can just send everyone
like a giant questionnaire and say like do you need a new pair of shoes etc and the problem with
that is that people could lie and they say yeah I totally need a new pair of shoes and so there's
a lot about that picture that's very unrealistic but figuring out ways to get people to honestly
is maybe a word that has some some bad connotations because we interpret in terms of like deliberate
intention and like moral character it's really more just about like truthfully conveying your
intentions so you so basically maybe this is a way of thinking about it you can sort of think of
economics as a very complicated version of like a traffic you're just trying to get cars to not
crash into each other and that means that each car needs to predict where each other car is going
that means each car needs to be sending very clear signals I'm definitely going here and I'm not
going to like turn suddenly or something like that so you're trying to get people to convey what
their plans are so that everyone knows what everyone else is planning and so then they can pick a
plan that's consistent with all those other plans and I guess this is valuable it's like much more
of like I guess an information theory kind of take I think there's I think there's another
thing that that is missing from that model which is that if you have like a really narrow freeway
then some cars are going to get through and some cars are going to have to wait
and there's not an easy way to resolve which ones so I think like and this is kind of back to the
plurality way that I've been thinking about it is like these are all forms of alignment and I break
alignment down into like aligning on action which is kind of what economics in some sense lets you
do is like okay we all have common beliefs let's given our beliefs what should we do and how can we
coordinate our action how can we all send our like our routes so that we don't like collide
so that's kind of aligning on action there's aligning on epistemologies like how do we all come to
believe the same world models and then there's aligning on value which is how do we actually
like I want one thing you want a different thing they maybe maybe we want opposite things how do
we align on that and all of these these three systems are all interdependent because if you
actually want different things you're not going to necessarily even align on epistemology because
the things you know only things that you care about enough to know and if you care about different
things you're not even going to necessarily ask the same questions of the universe to get back
the same knowledge and if you have different beliefs you're not going to align on action
and they all have to be iterated so like if you can agree on the same epistemology then maybe you
can shift both shift your values because what you value depends on what you know and so in some
sense it's like you have to pull all of these three systems in and I think try and what that's
another cool thing about economics is it also does help value alignment and knowledge alignment
because prices are knowledge signals so they tell you know what goods are easier to get when
and they're also value alignment mechanisms because they allow you to trade something that you value
more for something that you value less so I think there are these pieces but they all have to kind
of play together so I want to go in a couple of different directions with that and maybe the first
thing I'll mention is actually pulling away from economics a bit and talk about some neuroscience
so on a different comment I left on Mike's blog post I talk about the work of Lisa Feldman Barrett
who's done super important stuff it's actually her work that really prepared me to understand
what Mike's talking about because I don't know anything about biology and the active inference
sort of predictive coding view of things that she's really advancing as a way of understanding
what the brain does you know sort of we we do often naturally draw these divides between
knowledge acting and valuing and the active inference way of viewing things very much pushes
all of that together it's actually more like action that drives perception and really your
goals are at the base of all of that but also your goals are determined by you know I think of
goals as basically like beliefs or measurements that you have like your goals are in a sense your
epistemology because your goals are your measurements about the world and your prior expectations
it's a weird sounding view and maybe we can talk about that but um that's kind of how I think about
that and so I would you know it's very useful sometimes to separate these things out because
there's a reason we draw these categories we categorize because there are practical reasons
to do so and so that can be useful but also it's I want a fully unified perspective where we're
just treating all of that as kind of the same thing in some level and in economics at the end
of the day when you're trading something you can disagree very strongly about you know like the trade
ends up compressing all of that like if I'm buying shoes and the reasons I'm buying that would make
no sense to the shoe seller as long as I like the price you know everything else is fine so
yeah I'm not quite sure what the balance is there it's um because sometimes it feels like you do need
to address things like that and sometimes it feels like it just takes care of itself when the system
is set up right um if you guys are up for it I'd love I made a couple of slides and videos I'd
love to share uh uh I don't know if that would be a fun uh is that okay yeah yeah go for it let's
let's see cool um
okay uh can you see can you see my uh slideshow here yeah we can yeah any chance okay full screen
yeah is that better much better go for it cool um and this is uh two different uh ideas that
kind of work together that I want to get uh both share and get you guys the feedback on
I'll go really fast and then we can drill into something but essentially you know the thing I
I'm really interested in is how do we make an AI agent um and uh like uh to define an agent is
basically something that takes past uh like a sequence of past observations and then produces
the next action so it's just I think of an agent uh I know Mike you think agency has goals uh but
those are externally observable the knowledge about an agent uh so uh I think that's interesting to
figure out so for this uh thing uh like I think of an agent it's just like some function that given a
I think you're breaking up for me yeah me too me too
and
sorry David I don't know if you can hear me but I'm not getting any of this
yeah me too I can't I can't hear what you're saying
good
technology's not a friend today I guess he'll try again yeah I've noticed that usually happens
when something really interesting's about to come out it's because it knows what we're doing
and it doesn't want us to make progress on these questions yeah there's a little bit of that
is that background somewhere you've been uh yeah this is uh years ago this is Alaska
oh nice okay I think I've been to Alaska but many years ago I don't remember much about it
yeah yeah it was pretty it was pretty incredible
this is my blank beige wall because I have that kind of apartment so
yeah cool well yeah hopefully hopefully he'll come back um if not we'll just chat
so actually I do have something I should address with you quickly so regarding the paper we've
discussed I just want to briefly mention to you I don't know a lot about collaborating on papers
and if there's something that you might expect a collaborator to be doing right now while you're
working on other things that might not be obvious to me is there anything specific that I should
be doing right now uh remind me remind me where it stands you have you you've sent me a draft
sent you an outline uh yeah then then it's stuck in my court so I've just been in pain so I will
I will get back to you with the next couple of days with it so yeah yeah and no rush I just
literally wasn't sure if there was something it's not you it's me I've got a I've got a stack of
drafts um on my just because I will get to it yeah okay let's see David is I'll be back
can you hear me hi David hey hey sorry my computer decided not to run zoom anymore so I'm on my phone
um okay let let me try to just do this without the slides but basically the idea is that like an
agent is essentially some function that maps past observations to to the next action and
general intelligence is essentially a set of behaviors of like I think in your paper might
have this thing where it's like how do we navigate a space uh adaptive how do we adaptively
and uh intelligently learn to navigate arbitrary spaces and I think of that as just like a collection
of uh algorithms that let you balance exploration exploitation essentially it's active inference
but active inference requires perfect Bayesian uh inference and really everything just implements a
an approximation so uh the question is like how do you learn or build an approximation of active
inference in various spaces and neural networks are function approximators that instead of building
you can train given the right uh input output pairs so the idea is uh to basically train a
uh a neural network that's a function approximator of uh intelligent space navigation and I think the
way to do that is agents are basically duels of their environment so if you want to train uh an
agent to do something you need to give it an environment where achieving fitness in that
environment uh gives you the the behaviors that you want and so I think if you want an agent that's
able to generalize learning you need an environment where there's always something new to learn uh and
that the behavior space needs to be dense enough that the agent is able to always discover some
new thing that it can learn uh that gives it adaptive fitness to the environment uh and therefore as it
is learning how to uh adapt to that particular environment it is also generalizing learning
it is meta learning learning so uh the idea is if you can give an agent uh always some new
environment where there's something new to learn it's going to learn that and it's also in the
process of doing that it's going to learn how to learn it's going to learn how do I balance
exploration exploitation what algorithms what sampling algorithms can I apply to this new space
how do I uh leave myself and grams uh uh like how do I store memory how do I structure memory
how do I interpret memory uh in these various ways and so uh what you want is an environment
that essentially gets harder or different as the agent gets smarter and I think the way to do that
is make the environment and I think the way evolution did this is make the environment
highly multi-agent so that uh what you're like there are there is complexity in the physics of
the environment sure but really most of the complexity is in the minds of the other learners
and this kind of goes back to economics and capitalism uh where uh you know markets are
kind of anti legible as soon as you find out some trick that gives you an advantage in the
market that advantage goes away because it gets arbitrage the way by everyone else that learns it
and so I think you kind of had this with multi-agent setups you kind of can get into this infinite
game where as the minds of agents get more complex you have to get more complex to
be fit in that environment and this this is kind of just like creates this intelligent
tread intelligence treadmill and I think this has been done in AI so this is how we get like
go player or chess players it's like they do self-play against themselves and as they get
better the game gets harder and but I think typically this is done in purely adversarial
settings uh where and with an fully adversarial setting you basically cannot explore a lot of
the behavior space because as soon as you deviate from some dominant strategy you get
exploited by all the other players and so it's very easy to just get stuck at local minima
because there's not a lot of ways to deviate from uh local maximum you basically have to
really discover something new really quickly or you just get out competed by people that are doing
the same old thing and I think the fix to that is basically blending this line of what an agent is
in this collective intelligence sense I think one really strong technique for that is kinship
so if you're in a world where other agents are say a lot of the agents around you are your brothers
or your clones or your cousins essentially this whole spectrum of kinship then another way to
think about agency is kind of you know if you have two agents that share the same goal you can
think of them as one agent uh with just like really bad cognitive architecture where instead of just
being it has to like learn how to pass messages to itself so one way to define agency is via uh
essentially goals and kinship is this way of aligning agents on goals so if you have an agent
that 100 shares the goal with another agent it's basically one agent if it's 70 sharing goals with
that agent then you know it's kind of this blended super agent and so the idea is if you have an
environment where all these agents are aligned with each other with various different kinship
relationships you can create this really high dimensional behavior space where you're not
always competing there's a million different ways to cooperate break form coalitions break
coalitions established trust and so the idea is you know can we create a relatively simple
fast to simulate in silico virtual world full of other agents the agents are aligned with each other
in various kinship scenarios and then we scale off neural then we train larger and larger neural
networks to drive those agents and get to LLM size you know billion parameter models that instead of
predicting the next token are predicting the next action that an agent should take given all of the
experience at scene and so the idea would be that if you give an environment like that you're
essentially providing a gradient towards increased intelligence and then you're using standard machine
learning techniques to train a function approximator for that to follow that gradient so that's kind of
the idea behind the like overall environment but then the other thing I wanted to incorporate is
this collective intelligence approach and I really wish I could present my slides I don't know what
happened with my computer maybe I'll pause for a second let other people speak I don't know if
anyone has feedback or thoughts or criticisms and then I'll try to see if I can get this other
slide to work meanwhile yeah thank you for that I mean I have certainly heard a somewhat similar
sounding theory that evolution evolved or intelligence evolved as some sort of you know social competition
that you know primates competing with primates and birds competing with birds is
where it comes from rather than trying to like you know build tools and so on
um so uh yeah the idea about yeah you need a form of like some kind of protection some
ability to innovate I mean there's a lot of aspects of economics where there's this
challenging tension between wanting to optimize and wanting to create room for innovations like
perfect competitions a classic example like you learned very early on perfect for competition
you're going to maximize price maximize output your price is going to be as low as it should be
but it's very hard to innovate in perfect competition because the way the model works is
whatever new technology you introduce everyone copies that instantly you make no profit so but
you're the one who made the investment so that just sucks so a little bit of monopoly can actually
be helpful because that way you're making money and then there's even artificial things like you
know like patents and stuff that give people monopolies to try to incentivize them to
make those investments and innovate so that is a very important uh aspect of intention of any
sort of collective intelligence system is trying to encourage parts of your system to improve and
do better and trying to protect them from parts of the system might say oh we're going to copy
that we're going to take that we're going to steal that um there's a part of this that
maybe I know you're trying to get your slides up but um I saw in your talk you talk a little bit
about like love and stuff at the end of that when you have you have a talk on YouTube and that's
something that I had to bring up because I am working on a couple of papers about moral psychology
and neuroscience including a paper on morality as a form of cognitive glue which it isn't really
but it's a sort of an interesting counter example in some ways and um basically
there's a model in which um kinship let me ask you this if um I suppose that you took all the
families and you rearranged them like people didn't know that they were in the wrong families but
you had families where no one's actually related to each other genetically uh would the system still
work? Yep. Yeah so um I think of this as like I think of this as essentially reward sharing so on
a genetic level uh the the genetic optimization process uh gives rise to organisms that care about
their kin right so because a gene is present in the kin that it's going to make the organisms that
come from it want to help other organisms that have that gene from the gene's perspective it
doesn't matter whether you reproduce or your brother reproduces uh twice uh so uh and similar
so essentially the the behavior that an organism exhibits that we call love is uh basically what
it looks like when the underlying optimization process shares rewards or goal or like uh
shares rewards between organisms but then the organism itself one of the things that it evolves
or learns or whatever needs is the is some way to recognize who it should care about so we have all
of these heuristics for knowing who our kin are um and uh once those things are in place uh it's the
heuristics that matter not the underlying gene so uh I think if you train agents that see other
agents and you give them a reliable signal that hey helping this agent is actually the same as
helping you so every time this agent something good happens to this agent you get part of that reward
and here is a marker that tells you who is kin and who is not the agent is going to learn behaviors
that are conditioned on that marker and then once you stop training and you just let these agents
go around behaving you can change that marker arbitrarily rewards are no longer even there
right genetic evolution or whatever has stopped now you're no longer learning new policy now the
learning is happening inside the mind of the organism not inside the the genes
conditioned behavior on kinship markers
does that did that make sense it does and there's a very important paper that anyone interested
in this should read it's called the sense of should a biologically based framework for modeling
social pressure and in some ways maybe the difference is subtle but I think there's a way
of looking at these sorts of morally motivated cooperative behaviors um that does not in fact
depend on any kind of evolved pressure to care for or intend to cooperate but instead it's just like
something your brain just constructs as a living organism in its environment
because it's trying to make it's in its social environment predictable and and so there is
something of emerging perspective and my papers are somewhat drawing on this research that's
basically saying let's not think about evolution let's not think about long-term cooperation let's
just think about trying to make the social environment predictable and that's how we can
actually get um these cooperative and moral behaviors and so that just might be something
to look at because that's another paper released as a co-author and I just bring up Lisa's stuff as
much as possible I mean this is also yeah I mean I go ahead Michael sorry I was just going to say
that that also sounds like um uh the the uh imperial model of multicellularity that Chris
feels and I published a bunch of years ago which is basically that you know if cells are trying to
predict an uncertain environment the least uncertain the least surprising thing around is a copy of
yourself and so this this is a this can be a driver for uh making cells stick around after
you've divided to to form a you know a predictive of a highly predictive um um uh surrounding for
yourself so that you live in this niche and then the frontline infantry is out there facing the
uncertainties of the outside world but but you can predict them because they're you and so it's a
lot easier that way right so so you can you can think about it that way as well have I talked to
you about comparative advantage uh yeah no I don't think I want to go ahead David yeah sorry I had a
question about that Michael uh so uh I don't uh so if you have a cell uh and the cell does uh you
know given situation a it does random stuff then being surrounded by copies of those cells
and you do random stuff and your clones do random stuff being surrounded by cells that in that
situation do random stuff is not actually going to help you predict uh the environment
so I think there's something more to that it's not that being surrounded by copies of yourself
gives you the ability to predict your environment I think it's being surrounded by copies of cells
that want to make your environment that want to help you make the environment more predictable
that actually uh matters uh right unless you think that a cell by being able to
introspect itself can predict the behavior of other copies of itself I mean I I don't think
they're likely to be doing random stuff they're probably situations in which the responsiveness
is kind of random but but I don't think that's the vast majority of what cells do and so I do
think that if if if you're um it's it's easier to uh to anticipate cycles and responses I mean
again I mean I think you're right if it's random it's not going to work but I actually don't think
that cells are random in that in that way but let's um anyway let's let's let David do his thing
oh okay yeah this is just uh so this is uh this this is actually mostly inspired by your work Michael
and economics so I'd love to get thoughts on this but okay so this is the standard
a reinforcement learning uh formulation right you have an environment you have an agent uh
the agent takes an action in the environment it gets back some observation and a reward and then
it learns to maximum to act in a way that maximizes reward and uh you can have a dual formulation
this is which is the inference where the agent is trying to predict observations but essentially
this is the the normal kind of loop and typically the way it's uh we can break this down into like
an agent basically there's a bunch of observation states from the environment that go into the mind
the mind is going to generate a bunch of actions over actuators uh there's some memory that maybe
the mind has that it's reading and writing from uh and it's getting a reward signal so this is how
we train agents in reinforcement learning or this could be trained or this could be evolved
doesn't really matter and so the way we do this now in the field of RL is you have this
giant neural net inside the mind and there's model 3 and model RL so they're like this is
skipping over some detail but essentially you're training this big blob of math that you're jiggling
using gradient descent to optimize your long-term expected reward and it is learning how to compress
and basically how to compress the observations into latent states maybe store the parts of those
latent states in its memory that then it can reinterpret and eventually to generate actions
and so these networks can be giant in language models there are 100 billions of parameters in
reinforcement learning they're much smaller because we just haven't figured out how to
do it yet but this is kind of the standard setup and what I am proposing is a different architecture
that is essentially instead of there being one mind there are many individual agents
each agent is mapped onto some subset of the observation space and some subset of the memory
space and some subset of the action space and you know they can be mapped to any or all of them
but essentially rather than training one brain I want to train a neuron that in a collection
when you put it together in a graph of other neurons knows how to co-organize collectively
to solve the overall problem that the brain has so rather than training 100 million model
network I want to train one million parameter network that when you copy and paste it and
throw it into this soup of other neurons will self-organize into solving the problem
and so the idea is rather than having to learn adaptive algorithms so in in this way you know
you can imagine that there's an algorithm that you apply to many different parts of the sub problem
but this network has to learn how to has to learn that algorithm in a bunch of different places
because every sub function call might have the same kind of you know oh this is just like a
regression problem but you have to learn how to solve regression in a bunch of different places
whereas here this thing can just learn okay you know here is how I divide my problem into sub
problems and send those messages out or here is how I you know pay attention to a pattern
happening at this time scale and that's my job so the idea is to break down this one big blob of
compute into something that can be done on this graph instead and I think and this is
why I wanted to talk to you then because I think basically I started out thinking well this is an
economy you know if these things were you know if the observations and memory were commodities and
these things were like buying and selling those commodities or something like that that would give
us so the question is you know the the inside of a neural cell is a neural network that we train
but then the question is what is the cognitive glue between all of these neural cells essentially
what is the protocol that they use such that when we train train the neural cell
on the overall objective they'll learn something useful and the thing I came up with
kind of influenced by active inference and collective intelligence is so now this is inside
a single neural cell right now now this is an agent that just sees some set of inputs
it has its own memory it's going to get some reward signal from the the cognitive glue
and it's going to produce some outputs and I think the way to do that is that basically
it it should be predicting its inputs active inference essentially learning a world model
of its little world inside the brain and then voting on the outputs
so that it can and in this case outputs can be inputs to another neural cell or just outputs to
an actuator that moves the organism around so I think essentially I'm proposing some kind of a
an agent who's that you know receives a set of inputs so this is some subset
and it has some memory and then what it's going to do is it's going to predict the output at the
next time step along with some bet so it has energy and it has finite energy it's going to
bet on what the values of that inputs are going to be at the next time step it's going to vote on
what it wants to set the outputs to also using this energy and then at the next time step it's
going to see well what were the actual inputs and which bets did it win and then it gets energy
in proportion to essentially gets energy back as a reward so the idea here is that these agents
are incentivized to only predict the inputs that they have a comparative advantage on
over other agents so the same input and output is mapped to multiple cells
so as an agent you should learn to only to basically monitor your environment form models
about it and then bet on the things where you have a comparative advantage to other cells
and then as you get more and more capabilities to model your environment you get more and more
energy that you can use to vote on steering around that environment this is all hand wavy and this
is where I think I need help and help from economics because I started out with pricing then
I had it actually be an auction or your or prediction market add an auction and then I
try to simplify it just down to just like straight betting markets but that's basically the idea is
can we come up with this like simple cognitive glue for a pool of these agents and then train
those agents I'll stop now yeah no that's very interesting I think it's definitely an economy
and you definitely should talk to economists I'm afraid I think a practical issue is I'm just not
sure economists have actually done a lot to explore this space even in theory for the most part they've
been content to just take the economy as it is and try to explain that and there's been I think
just kind of a certain lack of creativity there in terms of envisioning other economic spaces
and trying to think about building alternative economic systems like this one so I'm afraid
I myself don't have a lot of practical things to say right now that might actually be of any
specific use to you other than I think just affirmation that you're right that this is economics
and that talking to economists is a good idea I think the mechanism design people are the place
to start I mean I think that's where Glenn Wilde comes from but I gotta know if like
Eric Maskin is reachable at all but like he won a Nobel Prize for that and I had a
two-minute conversation with him many years ago it was very influential so you might just want to
see if you could send him an email and see what he says yeah I think this is cool I'm afraid I don't
think I can help you today with it but we should stay in touch and I'll let you know if if anything
comes along because this is very interesting and very important
