WEBVTT

00:00.000 --> 00:10.000
Yeah. So in case you don't know me, I'm Zemin Liu. I'm now a fourth year PhD student at MIT.

00:10.800 --> 00:18.640
Welcome, Professor Max Tagmark. My research interests the center around AI plus science,

00:18.640 --> 00:26.080
so it can go both ways. Either you develop better AIs for science, like AI scientists,

00:26.080 --> 00:34.080
or you use inspirations or like tools in science to enhance the improvement of AI.

00:35.040 --> 00:41.440
So like this work is like both ways. We first try to use some ideas from math and see if we can

00:41.440 --> 00:49.200
develop new AI tools and see if we can, whether these new tools can give something back to to

00:49.200 --> 00:56.000
science. Yeah. So this is about a recent work called The Comagra of Arnold Networks.

01:01.520 --> 01:10.320
Today's AI is built upon this math theorem called the universal approximation theorem,

01:12.320 --> 01:16.560
which basically says that a high-dimensional function, you can decompose a high-dimensional

01:16.560 --> 01:24.800
function into a linear combination of some nonlinear features with this nonlinear function sigma,

01:26.160 --> 01:33.280
which is basically just this two-layer network known as the multilayer perceptron. But you

01:33.280 --> 01:39.200
can make it deeper. That's why it got the name multilayer. But we are asking this question,

01:40.160 --> 01:46.080
are there any alternative theorems we can use? We can leverage these new theorems. Well, not

01:46.080 --> 01:51.040
necessarily new in the mathematical sense, but like in the AI world to build another AI

01:51.760 --> 01:57.920
skyscraper based on new building blocks. So here we are examining this theorem called The

01:57.920 --> 02:04.560
Comagra of Arnold Representation Theorem. First question, what is the KA representation theorem?

02:05.120 --> 02:11.520
It says, again, like you given a high-dimensional function, you can write it down as a finite

02:11.520 --> 02:19.120
composition of one-dimensional continuous functions and just the summing operation. So more

02:19.120 --> 02:26.000
concretely, you can write down an n-dimensional function in this form, where you have the outer

02:26.000 --> 02:35.840
functions capital phi q and the inner functions phi qp. They are both just one-dimensional functions,

02:35.840 --> 02:43.360
and they are finite. Like the number of these functions is depending on the number of inputs

02:43.360 --> 02:50.720
variables n here. So what's nice about this theorem is that it tells us that the only true

02:50.720 --> 02:57.840
multivariate functions is the sum. Like you can express, say, multiplication with just the summing

02:57.840 --> 03:07.680
operation plus some 1D activation functions. So that sounds like really great news for

03:08.640 --> 03:12.320
approximating high-dimensional functions, especially for machine learning,

03:12.320 --> 03:17.680
because in high dimensions, to learn high-dimensional functions, we know we have curse of dimensionality.

03:18.400 --> 03:23.680
But this theorem seems to tell us that you can decompose high-dimensional functions into

03:23.680 --> 03:28.400
one-dimensional functions, which do not suffer from curse of dimensionality, which are great.

03:29.920 --> 03:42.960
But immediately, there is this plot twist. Back in 1989, professor Jarosi and Poggio wrote this

03:42.960 --> 03:51.200
paper examining the relevance of Comagra's theorem in the context of learning. And

03:52.000 --> 03:59.440
they conclude that we review Comagra's theorem and show that it is irrelevant in the context of

03:59.440 --> 04:09.280
networks for learning. So this paper basically sentenced the Ka theorem to death, at least

04:09.280 --> 04:19.360
sentenced to jail for like 30 years. So is that the end of the story? Well, we want to tell you

04:19.360 --> 04:27.440
that that's not the end of the story. And there's again a plot twist. So let's try to see what they

04:27.440 --> 04:36.960
say in their paper, why they sentenced the theorem to jail. So their argument was that the theorem

04:36.960 --> 04:46.400
fails to be true when the inner functions are required to be smooth. Basically, the original

04:46.400 --> 04:51.760
theorem says that you can decompose a high-dimensional continuous function into one-dimensional

04:51.760 --> 04:59.200
functions, but it doesn't guarantee you to decompose it into one-dimensional functions.

04:59.840 --> 05:06.240
But to learn something, like with gradient descent, you would need the functions to be

05:06.240 --> 05:13.120
smooth. Only being continuous is not enough. Even with continuous functions, you can still have

05:13.120 --> 05:20.960
some really pathological behavior. You don't have the smoothness constraint. So that's basically

05:20.960 --> 05:26.560
the argument, because the theory does not tell you anything about the smoothness of the functions.

05:26.560 --> 05:33.040
But in practice, only smooth functions are feasible in practice.

05:36.640 --> 05:46.080
So one interesting word they use is that a stable and usable exact representations

05:46.080 --> 05:54.240
seems hopeless. So that's like the attacking point. That's the angle we're taking. Maybe

05:54.240 --> 05:59.920
we do not need exact representations. Sometimes approximate representations might suffice.

06:00.640 --> 06:10.640
Yeah. So their paper was theoretically sound, was sound theoretically, but I'm trained as a

06:10.640 --> 06:18.160
physics student. So my level of rigor is lower than mathematicians. So me and also Max, we tend

06:18.160 --> 06:27.680
to have lower bar of being rigorous. So we have this naive optimism. So firstly, maybe like

06:27.680 --> 06:32.160
empirically, maybe we don't care about exact representations. Sometimes approximate ones

06:32.160 --> 06:40.160
may suffice. As long as it has some levels of explainability or interpretability, that's fine.

06:40.720 --> 06:46.480
And secondly, a common wisdom in modern deep learning is just stacking more layers to get

06:46.480 --> 06:52.160
different networks. They are basically arguing against the two-layer commograph network. But

06:52.160 --> 06:57.920
what about we build deeper networks? Maybe for deeper networks, even under the smooth

06:57.920 --> 07:04.560
constraint, you can win the expressive power back. Lastly, just the can-do spirit. We cannot say

07:04.560 --> 07:10.880
something doesn't work before really building it with the state-of-the-art techniques. They

07:10.880 --> 07:19.040
have the negative claim back like 30 years ago. Back then, they didn't even have the

07:19.120 --> 07:25.760
back propagation. Like at least it was not popularized back then. So we want to take

07:25.760 --> 07:33.600
everything we have in the modern area of deep learning and see how far we can go. So that's

07:33.600 --> 07:43.280
the mindset. So just with the can-do spirit, we propose or more properly rediscover or we

07:43.280 --> 07:54.240
contextualize the idea of commograph networks in today's deep learning world. Yeah. So here's

07:54.240 --> 07:59.600
the overview. First, I will introduce the math foundation. Well, I already briefly mentioned

07:59.600 --> 08:07.120
it. I won't dig deeper into it, but just I will emphasize again like the mathematical beauty of

08:07.120 --> 08:17.440
it. And then I will talk about the properties of cans. Why? And in what scenarios cans are more

08:17.440 --> 08:26.000
accurate and interpretable than current deep learning models? Yeah. So first, the math foundation,

08:26.000 --> 08:32.880
I already covered this part, but I want to emphasize again that the theorem

08:33.200 --> 08:38.960
looks really nice. That allows you to decompose a high-dimensional functions into one-dimensional

08:38.960 --> 08:44.640
functions. And after the decomposition is done, your only need to just your only job would be

08:44.640 --> 08:53.120
just approximating the 1D functions. So that's the idea of that's the idea of the commograph

08:53.120 --> 09:01.600
networks. Like decomposition first and then learn the 1D functions. Well, yeah. So this

09:01.600 --> 09:07.600
representation looks a bit complicated. You'll see that there are this huge, this big summation

09:07.600 --> 09:15.120
symbol and you have two layers of composition. And this will be complicated. But don't worry

09:15.120 --> 09:22.400
about it. It's just equivalent to this two-layer network. Let's suppose we have two inputs,

09:22.400 --> 09:29.600
x1 and x2, and we have the outputs at the top here. So the representation in the original

09:29.600 --> 09:38.400
theorem is basically just that you have five hidden neurons in the middle. And to get the

09:38.400 --> 09:48.720
activations in each hidden neuron, you basically apply a 1D, possibly nonlinear function to x1

09:48.720 --> 09:58.560
and x2, and sum up these two nonlinear activations to get the activations at the node. And

09:59.600 --> 10:07.360
in the second layer, it's similar that you apply some nonlinear function to the hidden activations,

10:07.360 --> 10:14.080
summing everything up at the output node, and that's how you get the output. So the computation

10:14.080 --> 10:27.200
graph is super clear with just this diagram. And this might remind you a lot of, this looks

10:27.200 --> 10:31.360
just like a multi-layer perceptron, the fully connected networks, where everything just fully

10:31.360 --> 10:37.520
connected. But instead of having activation functions on the nodes, now we are having

10:37.520 --> 10:42.960
activation functions on the edges. And on the nodes, you simply just have the summation operation,

10:44.800 --> 10:52.240
which is really simple. Yeah, just to make it more elegant and beautiful, we can,

10:52.800 --> 10:57.040
or more intuitive, we can basically, because like 1D functions, we can basically just visualize them

10:57.040 --> 11:03.840
with 1D curves, with the x and x as the input and y as the output. So now can network is basically,

11:04.480 --> 11:09.680
you can picture it as this. And by staring at it, you can, you can have some idea what's happening

11:09.680 --> 11:18.640
inside. Yeah, so I mentioned this is just a, so the theorem, so the representation is equivalent

11:18.640 --> 11:25.840
to a two-layer network. But can we go deeper? The answer is yes. Algorithmically speaking,

11:25.840 --> 11:33.440
because it's just a stack of two layers, which, from which we can abstract a notion called the

11:33.440 --> 11:39.760
can layer. So the original two-layer network is basically a stack of two, two can layers.

11:39.760 --> 11:45.360
And for each can layer, it's basically just taking some number of inputs and outputs, some number

11:45.440 --> 11:51.200
of outputs, and in between is fully connected. And on each edge, you have the active, you have the

11:51.200 --> 11:56.800
some nonlinear, learnable nonlinear activation function. And in the end, and in the outputs,

11:56.800 --> 12:04.880
you, you, you summing up the incoming activations. That's how a can layer works. And you can simply

12:04.880 --> 12:11.760
stack more and more can layers to get deeper and deeper cans. This is just a three-layer can,

12:12.720 --> 12:19.040
like, like the first layer, we're taking two, output three. The second layer, input two, output

12:19.040 --> 12:26.720
three. And the last layer, you input three, output one. So this is the, this is a three-layer network,

12:26.720 --> 12:33.120
which approximates a scalar function in two dimensions. But obviously, you can easily

12:33.120 --> 12:40.000
extend it to arbitrary dimension, like arbitrary input, arbitrary output, arbitrary width, arbitrary

12:40.800 --> 12:45.760
depth. So, so you have all the flexibility to choose the size of the network.

12:49.120 --> 12:56.960
Yeah, one, the first question Professor Poggio asked me when I presented this network to him,

12:57.920 --> 13:03.200
he asked, is that why do you need this deep networks? Because the original theorem told you

13:03.200 --> 13:11.120
that you only need the two-layer constructions. And here's just a quick answer that I, I can give

13:11.120 --> 13:19.840
you an example. So please look at this symbolic formula. And if you examine it, you would immediately

13:19.840 --> 13:26.560
realize that you would need at least three compositions to do this, to construct this formula.

13:27.120 --> 13:31.680
You need the, you need the squared function, you need the sine function, you need the exponential

13:31.680 --> 13:38.240
function. They're like, because they're, because it's the compositional structure, they're in different

13:38.240 --> 13:46.640
layers. So you would at least, at least need three layers to, to, to learn this formula. And

13:46.640 --> 13:52.160
indeed, if you just use two-layer network, the activation functions becomes really oscillatory,

13:52.160 --> 13:59.360
becomes really pathological. And the performance is bad, and also the interpretability is bad.

13:59.360 --> 14:06.480
But in the right, I show that a three-layer network, a three-layer can train on this,

14:06.480 --> 14:11.680
train on this dataset. And after training, you would immediately see the learned activation

14:11.680 --> 14:15.600
functions. In the first layer, you got the squared, in the second layer, you got the sine,

14:15.600 --> 14:21.520
and the last layer, you got the exponential. Well, well, you may think of, you may think this is some

14:21.520 --> 14:29.200
other functions, maybe just some local quadratic, but yeah. But you can, but you can, like,

14:29.200 --> 14:34.160
do the template matching with the candidate functions and figure out which one fits the best.

14:38.400 --> 14:43.840
Yeah, so, so I, I said that this activation functions are learnable. How do we make them

14:43.840 --> 14:50.080
learnable? Because they're functions. And, and, and, and the common wisdom is that we need to make,

14:50.080 --> 14:56.160
we need to parametrize the things to be learned so that we can use gradient descent to learn,

14:58.000 --> 15:04.960
to learn this stuff. So the idea is that we parametrize a 1D function with, with B splines.

15:04.960 --> 15:13.520
So B splines is basically some piecewise, some, some local piecewise polynomial functions.

15:14.080 --> 15:22.720
So, so here I showed that there are some local B spline bases. And the way we construct the activation

15:22.720 --> 15:29.200
functions is by linearly combining this, this, this B spline functions. And the only learnable

15:29.200 --> 15:36.720
parameters are just the linear coefficients of, of, of this local basis. And what's nice about

15:36.720 --> 15:43.040
this formulation is that we, we inherit the advantage of B splines. We can easily switch

15:43.120 --> 15:49.520
between fine grains, fine grain grids and coarse grain grids. If you want something to be more

15:49.520 --> 15:55.200
accurate, you can, you can choose the mesh to be more fine grain. If you want the model to be

15:55.200 --> 16:00.480
smaller, so you can have a faster inference, you can, you can, you can choose a more coarse-grained

16:00.480 --> 16:11.360
model. Yeah, that's basically the idea of cans. And we can compare MLPs and cans side by side,

16:11.360 --> 16:15.840
because they do share some similarities, but also share some difference, but also have some

16:15.840 --> 16:23.440
differences. So MLPs are inspired by the universal approximation theorem cans, again,

16:23.440 --> 16:29.360
inspired by the Camargo Ravano representation theorem. The network looks a bit similar in the

16:29.360 --> 16:36.160
sense that they're both fully connected. But their dual, they're different, but they're dual in the

16:36.160 --> 16:41.840
sense that MLPs have fixed activation functions on nodes. Well, you can make them trainable, but

16:42.640 --> 16:48.720
but they're on nodes for sure. And in MLPs, we have learnable weights, learnable linear weights on

16:48.720 --> 16:57.920
edges. By contrast, cans have learnable activation functions on edges, while cans have this simple

16:57.920 --> 17:04.880
linear summation operation on nodes. So, so, so in this sense, cans does not separate the linear

17:04.880 --> 17:11.920
part and the long inner part as MLPs do, but it integrates both the linear part and long inner

17:11.920 --> 17:17.600
part altogether into the can layer. And the can network is simply just the stack of the can layers.

17:21.520 --> 17:30.000
Yeah, so in both cases, you are free to stack the model to become deeper and deeper because

17:30.000 --> 17:34.160
you have the basic notion of a layer, you just stack more layers to get the deeper networks.

17:35.360 --> 17:45.680
Yeah, so that's the basic, that's the basic ideas of cans. And now I want to elaborate more

17:48.800 --> 17:54.000
like why do we care about, why do we care about this? What are the advantages that cans can bring

17:54.000 --> 18:03.440
to us, but other black box models do not bring to us? So yeah, so the first property is the

18:03.440 --> 18:09.600
scaling behavior of cans. As I mentioned before, the idea of cans is decomposing a

18:09.600 --> 18:14.320
high dimensional functions into one dimensional functions. So that looks like really promising

18:16.400 --> 18:23.280
that it can get us, it can get us out of the curse of dimensionality. Let's suppose we are

18:23.920 --> 18:30.480
trying to approximate a d dimensional functions. And I suppose the function has no structure at all.

18:30.480 --> 18:37.440
So then we need to, we need to have a hypercube and have a uniform grid on the hypercube.

18:38.240 --> 18:44.720
Let's suppose we have 10 grid, 10 anchor points along each dimension, then we will need 10 to the

18:44.720 --> 18:49.760
power of d number of anchor points in the d dimensional hypercube. So that's exponentially

18:50.640 --> 18:57.120
expensive. So if you do the classical approximation theory, you would notice that the approximation

18:57.120 --> 19:06.160
error would decay as a parallel, as a parallel, as a function of the number of input dimensionality.

19:06.160 --> 19:12.560
And it's one, and the exponent is one over d, meaning that you got exponentially

19:13.680 --> 19:18.720
like slower when you have more and more, when you have more and more dimensions, like if you need

19:18.720 --> 19:25.200
10 points in one d, you would need 100 points in 2d, you will need 1000 points in 3d and so on.

19:27.200 --> 19:34.160
But if the function has some structure, like if it has the chromograph unknown representation,

19:34.160 --> 19:40.560
then we can decompose it into a bunch of 1d functions. And then our job would just be

19:40.560 --> 19:47.440
approximating 1d functions. So now effectively d becomes one. So you got a really, you got the

19:47.440 --> 19:54.480
fastest possible scaling laws. But the, but the caveats, immediately the caveat is that we,

19:54.560 --> 20:03.600
the assumption is that we, like the function has a smooth chromograph, a smooth finite size

20:03.600 --> 20:09.680
chromograph unknown representation. All of this, you know, all of these objectives,

20:09.680 --> 20:15.600
objectives like smooth or finite size are just, are just practical conditions for a real, for,

20:16.640 --> 20:22.880
for a network which we have access in practice that can really learn the network. We want it to

20:22.880 --> 20:28.400
be smooth because we parametrize it with b-splice, which are smooth. We want them to be finite size

20:28.400 --> 20:35.520
because, of course, you know, we cannot initialize, we cannot deal with an infinite size neural network.

20:38.720 --> 20:44.720
So, yeah, so, so, so we just did some sandwich check on, on some symbolic formulas.

20:45.120 --> 20:51.280
So, yeah, so symbolic formulas are like white, are like what we used in, in science. So that's why we

20:51.280 --> 20:58.640
test them first. So let's see. So the red dashed line is the theoretical prediction. Here we are

20:58.640 --> 21:12.000
using cubic spline. So k is k3. And the scaling exponent is k plus one equals four. And the

21:12.880 --> 21:21.920
curve, yeah, so the thick blue line is for the can network. And you see that almost like the,

21:21.920 --> 21:28.240
like empirical results for the can network almost agreed with, almost agrees with the

21:29.360 --> 21:35.120
theoretical prediction, although sometimes performed slightly worse. Or in this case,

21:35.120 --> 21:40.160
in the second to last case, there's a hundred dimensional case. And it performs much worse

21:40.160 --> 21:45.440
than a theoretical prediction because of the, the, the, the, because the dimension is just too

21:45.440 --> 21:52.080
high and the network can get, can get stuck at some local minima or whatever. But, but nevertheless,

21:52.080 --> 21:59.680
it's still output from MLPs, which you see up in the upright corner here, like the case really

21:59.680 --> 22:07.040
slow. But, but the cans, but cans at least can output form MLPs to a great margin,

22:07.040 --> 22:14.720
although still not saturating the theoretical prediction. But still the scaling law that can

22:14.720 --> 22:23.040
shows looks, looks promising that it's, it seems to not fully beats the curse of dimensionality,

22:23.040 --> 22:25.680
but at least partially beat the curse of dimensionality.

22:29.120 --> 22:35.920
Well, yeah, yeah. So, yeah, just to play the devil's advocate here, you may immediately notice

22:35.920 --> 22:42.240
that I'm on purpose just deliberately using this symbolic formulas. You, you might be wondering,

22:42.240 --> 22:49.920
well, maybe, maybe the functions we care. We encounter a lot in nature may not be symbolic,

22:49.920 --> 22:55.840
they might be some weird, you know, like, at least for special functions, they, they, they,

22:55.840 --> 23:02.160
they are like infinite series, which are hard to be represented with just the finite network. So,

23:02.160 --> 23:07.200
what are, what, what, so, so, so, so what's the cans performance in that scenario?

23:08.560 --> 23:14.880
So, yeah, so we just tried some special functions, which we know for most of them, they do not have

23:14.880 --> 23:23.360
analytical formulas. And indeed, we see that the scaling laws of cans do not saturate the

23:23.360 --> 23:28.720
theoretical prediction, meaning that probably you cannot decompose like a high dimensional

23:28.720 --> 23:35.600
functions into just a one D functions. But still, our goal here is to outcompetes MLPs.

23:36.240 --> 23:41.680
And, and, and it's a feature, not a bug, like not all the functions can be decomposed into

23:41.680 --> 23:51.600
smooth finite representations of this K representation. They may admit non smooth finite size or

23:51.600 --> 23:58.880
smooth infinite size. But in, but, but neither case is, is like the, is what's accessible in

23:58.880 --> 24:07.200
practice. So, yeah, so, so here we show that in most cases, we can achieve this minus two

24:07.200 --> 24:14.320
scaling law, which means that the can network, well, well, sorry. So here, this special functions

24:14.400 --> 24:22.000
are all just two dimensional. So this, so these, according to the spline theory, it would predict

24:22.000 --> 24:27.920
two dimensional, it would predicts a two dimension, like, like, like a scaling exponent to be two,

24:27.920 --> 24:33.600
which agrees with the can results. But in some, but in some case, we can, we can still got the

24:33.600 --> 24:41.680
minus four scaling law. And the reason is that actually, this is secretly, like, like, although

24:41.680 --> 24:47.440
we call, we call it a special function, spherical harmonics are not that special in the sense that

24:47.440 --> 24:54.320
they're still decomposable. So, so you can get the minus four scaling. But also in other, in other

24:54.320 --> 24:59.280
cases, you've got some worse behavior, like you've got the minus one scaling, which means that the

24:59.280 --> 25:06.880
can is underperforming even out on the performing compared to like the spline theory. But what's

25:06.880 --> 25:14.080
interesting is that in this case, MLPs are even more underperforming than cans. So, so this may

25:14.080 --> 25:20.480
tell us that maybe for low dimensional problems, neural networks are not that necessary. And even

25:20.480 --> 25:26.880
the spline theory, like the spline approximation can outcompete the neural network. So, so it's

25:26.880 --> 25:34.880
something to be think about. It's something good to keep in mind. Because neural networks just have

25:34.880 --> 25:39.760
too many degrees of freedom and may have optimization issue.

25:42.480 --> 25:50.000
Yeah. So, so beyond function approximation, we can go on to solving partial differential equations.

25:50.000 --> 25:56.080
So in the setup of physics in form learning, basically, we're trying to solve, we're basically

25:56.080 --> 26:08.640
trying to represent the solution of a PDE with MLP or with a can network. So, so the only difference

26:08.640 --> 26:13.520
from the previous results from the previous experiment is that we're just we're using the physics

26:13.520 --> 26:19.920
informed loss rather than the regression loss. So the optimization becomes more complicated,

26:19.920 --> 26:26.480
but still it's just approximating some function. Yeah. So, so we still see that we with cans,

26:26.480 --> 26:33.920
we can get this optimal scaling law. Well, with MLPs, you see, while with MLPs, you see that,

26:33.920 --> 26:37.840
well, it has the skill at first, but it plateaus really fast and then

26:38.720 --> 26:43.440
does not improve when you have more parameters beyond 10,000.

26:43.440 --> 26:52.480
Um, besides being more accurate, we can also gain some insight what the network is learning.

26:52.480 --> 26:58.720
So, so, yes, yes. So for this example, we can, for this PDE example, we can actually visualize

26:59.280 --> 27:05.360
the can network like this. And immediately you can see that there's some like sine waves and

27:05.360 --> 27:11.840
there's some linear functions. And you can even do symbolic regression to it, like,

27:12.080 --> 27:17.280
like, like our software provides you a way to do this, you can, you can do symbolic regression

27:17.280 --> 27:24.160
to it. And after you do this, you can do some further training. And you, and you can even extract

27:24.160 --> 27:30.080
out the symbolic formula, which gives you like a loss down to machine precision. This is something

27:30.800 --> 27:38.480
that that that standard neural networks would not give you because of because they usually

27:38.480 --> 27:43.760
cannot convert a neural network into a symbolic formula very easily, but with cans, you can easily

27:43.760 --> 27:54.720
do that. Another property of cans is that it has this property of continual learning, at least

27:54.720 --> 28:02.880
in 1D. Yeah, so people have, people have found that, you know, this claim might be invalid for

28:02.880 --> 28:10.080
high dimensions. So take my word with a grain of salt. But at least for 1D, the case is like,

28:11.040 --> 28:18.400
we have, we want to approximate this 1D functions with five peaks. But instead of feeding auto data

28:18.400 --> 28:25.920
at once to the network, we're feeding each time we just feed one peak to the network. And we do

28:25.920 --> 28:33.120
the sequential learning at each stage, the network is just, is just fed with just one peak.

28:33.120 --> 28:40.000
And with cans, and with cans, because we're using the local B-splines, so when it sees the new data,

28:40.000 --> 28:46.640
it does not update the parameters correspond to the old data. So it has this, it can't get rid of

28:46.640 --> 28:55.520
this catastrophic forgetting, like when new data are coming in, the can is able to memorize the

28:55.520 --> 29:02.480
old data and still do quite well on the old data. But this is not true for MLPs. Like when you're,

29:02.480 --> 29:08.080
when you're fed, when the MLPs are fed with the new data, it catastrophically, they catastrophically

29:08.080 --> 29:14.240
forget about the old data. Because in MLPs, you usually have this global activation functions

29:14.240 --> 29:20.720
like the silo functions or Reilu functions. So whenever you make adjustments locally, it will

29:20.720 --> 29:26.480
affect, you know, the predictions far away. So that's the reason why MLPs would not keep

29:28.240 --> 29:32.080
would, would catastrophically forget about the old data.

29:34.880 --> 29:42.080
Yeah, so, so, so that's the first part about accuracy. The second part is about interpretability.

29:42.080 --> 29:48.720
You might already have some sense because we are able to visualize the network as a diagram. So,

29:48.880 --> 29:53.600
the hope is that you can just stare at a diagram and gain some insight of what's happening inside

29:53.600 --> 30:05.040
the neural network. Yes, yeah, so here we have some, some, some toy examples of, yeah, for example,

30:05.040 --> 30:12.960
how do cans do the multiplication computation. So we have x and y as the input and the x times y

30:12.960 --> 30:19.280
as the output. So when we train the can network, we also have some, something similar to L1

30:19.280 --> 30:25.280
regularization to sparsify the network. So we can extract out the minimal network to do the task.

30:26.560 --> 30:31.520
So in the multiplication case, we see that only two neurons are active in the end,

30:31.520 --> 30:37.840
and we can read off the symbolic formulas of how, and get a sense of how it does the computation.

30:37.840 --> 30:47.280
Well, I marked the symbolic formulas here. It may take a while. It may take a while, but I,

30:47.280 --> 30:54.960
well, the point is that it basically just some squared functions and, and the way the can network

30:54.960 --> 31:02.720
learns to compute this multiplication is by leveraging some, some squared equality here.

31:03.680 --> 31:09.440
And the second example, the can is tasked with the division task,

31:10.400 --> 31:21.360
where we input two positive numbers, x and y, and can is asked to predict the x divided by y.

31:21.360 --> 31:27.840
And because here x and y are both positive, the can network learns to first take the

31:27.840 --> 31:32.960
logarithm transformation and then take the subtracted two logarithm and then

31:33.920 --> 31:41.040
transform the same back via the exponential, exponentiation. So, so, so that's really cute.

31:41.040 --> 31:47.120
Like, like one example of the commograph theorem is that you can basically do the multiplication

31:47.120 --> 31:53.360
of positive numbers or the division of positive numbers in the logarithmic scale, because that's,

31:53.360 --> 31:59.280
because the division or multiplication in the logarithmic scale, it transferred to,

31:59.280 --> 32:04.560
would translate to like addition and subtraction in, in the logarithmic scale.

32:08.240 --> 32:13.280
Yeah, so, so, so, so this examples are really simple. You might be wondering what about a more

32:13.280 --> 32:20.480
complicated formula, then, then it might be very complicated to decode what the cans have learned.

32:21.440 --> 32:26.880
So, I would want to argue that this might be a feature, not a bug, or you can call it a bug,

32:26.880 --> 32:30.400
but I can, but I won't call it a feature in the sense that,

32:32.720 --> 32:36.640
sorry, I didn't show the network here, but let's suppose we're doing this formula here,

32:36.640 --> 32:42.880
like u plus v divided by 1 plus uv. So, if you're familiar with relativity, you are special relativity

32:42.960 --> 32:48.320
on, you are, you are, you are, you are realized that this is the velocity, relativistic velocity

32:48.320 --> 32:56.880
addition. So, at first, I thought that I need the five-layer can to fix this function, because

32:56.880 --> 33:02.880
you would need multiplication, which would give, which would consume two layers, you would,

33:03.920 --> 33:08.880
two additions, consume another two layers, and also you, you, you would need the division.

33:08.880 --> 33:14.560
So, so that's in total five layers, but it turned out you can only, you can, you can just use a two-layer

33:14.560 --> 33:20.160
can to fix this function perfectly well. And in the end, I realized that this is just,

33:22.080 --> 33:28.080
this is just a rapidity trick known in special relativity, where you first do the arc-tange

33:28.080 --> 33:34.080
transformation to u and v separately, sum the thing to, to rapidity up, and then you do the

33:34.080 --> 33:40.640
tange transformation back to, to get this formula. So, in this sense, it's rediscovering the, it's,

33:40.640 --> 33:49.600
it's rediscovering the rapidity trick, known, well-known in, well-known in the special relativity.

33:49.600 --> 33:55.600
And in some cases, I indeed find that the network finds some more compressed,

33:55.600 --> 34:01.280
compact representation than I would expect. That's good news in the sense that's the,

34:01.760 --> 34:08.080
the, in the sense that the network is discovering something more compact. So, the representation

34:08.080 --> 34:13.840
is more powerful than I have expected. But the bad news is that sometimes the, the interpretation

34:13.840 --> 34:22.080
can be subtle, can be a bit more complicated. But, but I mean, it's, it can be a, it can be a

34:22.080 --> 34:31.520
feature, but depending on your view. Yeah, so, so, so, so this is one paragraph I take from

34:32.240 --> 34:38.320
the paper. This is criticized, this has been criticized a lot in the social media, but I,

34:39.120 --> 34:46.880
but I find this analogy really interesting. So, I still want to highlight this part.

34:47.840 --> 34:53.760
So, so to me, I think can is like a, it's sort of like language model or, or like language or

34:53.760 --> 34:59.680
even like language for AI plus science. The reason why language models are so transformative and

34:59.680 --> 35:05.200
powerful is because they are useful to anyone who can speak natural language. But the, but the

35:05.200 --> 35:12.720
language of science is functions or more advanced mathematical objects build on functions because

35:12.800 --> 35:20.000
cans are composed of functions which are 1D, so they are interpretable. So, when a human user stares

35:20.000 --> 35:26.720
at a can, it's like communicating it with, it's like communicating it with the, using the language

35:26.720 --> 35:33.680
of functions. So, to elaborate more and to make it more entertaining, let's suppose we are like

35:33.680 --> 35:40.080
my advisor Max is communicating with the can network. Here, I picture the can network as

35:40.160 --> 35:46.080
a trisolarian from the three-body problem. I'm not sure if guys have watched it, but basically

35:46.080 --> 35:53.360
the trisolarians, their brains are transparent, so they cannot hide any secrets from others. So,

35:53.360 --> 36:03.600
they are totally transparent. So, so Max went up, give, give the can, give the can a dataset.

36:03.760 --> 36:10.000
Max was like, here is my dataset. It contains the mystery of the universe. I want you,

36:10.800 --> 36:18.480
the can network, to figure out, to figure out the structure of the datasets and, and the can

36:19.360 --> 36:27.200
initialize the can network like this. So, here's the brain of the can network initially. And Max,

36:27.440 --> 36:35.040
given the dataset, Max wants to train the can network, to train the brain. And after training,

36:35.040 --> 36:41.280
you can get this sparse network, which you start to see some structure. But still, it's a bit,

36:41.280 --> 36:48.320
there's still some residual connections, which looks quite, which quite annoying. So, Max asked the

36:48.320 --> 36:56.080
can network to prune the redundant connections. And after pruning, you got, after pruning,

36:56.160 --> 37:05.920
you got this, you got this sub network, which is responsible for the computation. And Max further

37:05.920 --> 37:11.600
asked the network to the can network to symbolify it, because it looks like, it looks like in the

37:11.600 --> 37:17.200
bottom left, it looks like just a sign function. And in the bottom right, this looks just like a

37:17.200 --> 37:22.800
parabola. And this just looks like an exponential. So, maybe you can symbolify it to gain some more

37:22.800 --> 37:33.120
insight. So, yes, so the can network said, yes, I can symbolify it. And you can. And now the dataset

37:34.240 --> 37:41.680
goes all the way down to the symbolic formula. But, but we can imagine another conversation Max

37:41.680 --> 37:47.920
would have with an MLP. So, Max went up and give the dataset to MLP, and want the MLP to figure

37:47.920 --> 37:57.840
out the symbolic formula in it. And like before, the MLP initialized the brain, looked like something

37:57.840 --> 38:04.400
like this, really messy. After training, Max asked MLP to train the brain, but even after

38:04.400 --> 38:13.280
training, the connection still looks really messy. And MLPs were like, and the MLP is like, I really

38:13.280 --> 38:18.880
trained it, but the loss is pretty low. But it's just that the connections are still very complicated.

38:21.120 --> 38:29.360
Now Max got confused. Like, what's going on? What's going on with your brain? And now MLP is like,

38:29.360 --> 38:36.000
it's just that your humans are too stupid to understand my computations. You cannot say,

38:36.000 --> 38:42.000
I'm wrong, simply because you cannot understand me. So, Max now got really pissed off and turned

38:42.000 --> 38:51.920
back to CANS. So, those are just some imaginary stories I made up with the symphatic example.

38:51.920 --> 39:01.040
And that symphatic example is really simple. But I want to show that we can really use CANS

39:01.040 --> 39:07.440
as a collaborator in scientific research. And CANS can give us some non-trivial results.

39:08.080 --> 39:15.280
CANS can give us some new discoveries. So, the first example is,

39:17.840 --> 39:25.520
yeah, so this example was used in a DeepMind Nature paper three years ago, where they used MLP

39:25.520 --> 39:35.200
to discover a relationship in a NOT dataset. So, each NOT has some invariance. Basically,

39:35.200 --> 39:39.840
each NOT is associated some numbers. And these numbers, they have some relations. And we want

39:39.840 --> 39:48.160
to dig out the relations among these variables. So, what the DeepMind people did was they used

39:48.160 --> 39:53.760
the train and MLP and used the attribution methods, basically take the gradient with respect to these

39:53.760 --> 40:00.960
input variables, and use that as a score to attribute these features. And then rank these

40:00.960 --> 40:05.440
features to get a sense of which features are more important than other features. And they

40:05.440 --> 40:14.480
identified three important features. That's the only thing that's automated in their framework.

40:14.480 --> 40:20.800
And then the human scientist came in and tried to come up with a symbolic formula for it.

40:21.600 --> 40:26.160
So, we're asking this question, can we discover, have we discovered these results

40:26.160 --> 40:32.160
with more automation, with less, you know, efforts, and probably even discovering something new

40:32.160 --> 40:42.320
that the DeepMind paper were missing? So, first, we are able to discover the three important variables

40:42.320 --> 40:49.920
with the CAN network, with much more intuition and automation. So, their network, they used a

40:50.240 --> 40:56.480
three-layer, sorry, they used a five-layer MLP, and each hidden layer has 300 neurons. So,

40:56.480 --> 41:02.080
that's really hard to interpret. That's why they used the feature attribution. But we find that

41:02.080 --> 41:10.480
surprisingly, we only needed one hidden layer and one hidden neuron, the CAN network, to do the task,

41:10.480 --> 41:19.520
as well as their five-layer, like, a million-parameter MLPs. And with this, we can also clearly see

41:20.000 --> 41:28.960
the activations, the importance now basically becomes, you can basically understand the importance of

41:28.960 --> 41:38.960
these variables with the L1 norm of these activation functions. So, that's also how we visualize

41:39.520 --> 41:44.960
the connections. So, you can basically read off from this diagram that the strong connections

41:44.960 --> 41:49.920
are the important variables, while the weaker or even nearly transparent

41:50.960 --> 41:53.600
connections, meaning that irrelevant variables.

41:57.360 --> 42:04.320
Yeah, we can also discover symbolic formulas, as I said before, because the CAN network decomposes

42:04.320 --> 42:09.760
high-dimensional functions into 1D, and then we can just do template matching in 1D

42:09.760 --> 42:17.680
to see what each 1D functions represent symbolic formulas, and then compose all these 1D functions

42:18.560 --> 42:20.640
back to get these high-dimensional functions.

42:23.760 --> 42:32.720
Something beyond their paper we discovered is that their setup is a supervised learning setup.

42:32.720 --> 42:39.120
Basically, they need to partition the variables into inputs and outputs, and they use the inputs to

42:39.680 --> 42:45.680
predict at the outputs. But in some cases, we do not know how to partition the inputs and outputs,

42:45.680 --> 42:57.040
like all the variables, they are treated equally. So, we want to develop this unsupervised setup

42:58.080 --> 43:05.920
where all the variables serve as inputs. But we use some notion of contrastive learning to classify

43:05.920 --> 43:12.640
whether some given input is a real knot or a fake knot, or that might be too technical.

43:16.320 --> 43:23.680
The result is that we are able to discover more relations beyond the relation they've

43:23.680 --> 43:29.680
discovered, because they manually partition one variable as the output, so they can only discover

43:30.640 --> 43:36.320
the relations that involve that variable. But here, we are learning it in an unsupervised way,

43:36.320 --> 43:43.040
so we can learn more than just one relation. We also discovered the relation between these

43:43.040 --> 43:53.760
three variables and between these two variables. Unfortunately, or fortunately, these relations

43:54.080 --> 44:00.720
are already known in the literature of knot theory. So, the unfortunate part is that we did not

44:00.720 --> 44:09.040
discover anything new with our framework, but notice our network is just very preliminary,

44:09.040 --> 44:14.720
it's just a one layer, it's just a one layer if we ignore about the classifier in the second layer.

44:14.720 --> 44:21.280
So, hypothetically, we can make it deeper to get more complicated relations. But the

44:21.280 --> 44:27.040
fortunate part is that it verifies that's what we discovered with the network is something,

44:28.160 --> 44:32.800
is not bullshit, it's something that makes sense that people already know in the literature.

44:36.880 --> 44:42.960
Yeah, we also did this physics example, specifically Anderson localization.

44:43.920 --> 44:52.320
I don't want to bore you with the technical detail, but again, the goal here is to try to figure out

44:53.040 --> 45:00.080
the symbolic formula of the phase transition boundary. In Anderson localization, we have the

45:00.080 --> 45:06.240
localization phase and the extended phase, and there is a phase boundary, and we want to extract

45:06.240 --> 45:13.680
out the phase boundary, especially the symbolic formula of the phase boundary,

45:14.240 --> 45:19.040
if there exists a symbolic formula for it from the road data.

45:22.880 --> 45:28.720
Yeah, so for this slide, I don't want to go to the detail, but the point I want to make with

45:28.720 --> 45:36.560
this slide is that the CAN network has, just like cars, you have manual mode, you have manual mode,

45:36.560 --> 45:41.840
you have automatic mode, like if you're lazy, you can just delegate everything to CANS, and CANS

45:41.840 --> 45:48.320
will return you a symbolic formula fully automated, but that might not be correct. That might not

45:48.320 --> 45:55.520
be what you want. You can do that, it can give you reasonable accuracy, but may not be fully

45:55.520 --> 46:01.040
interpretable, but if you want to have some controllability over the CAN network,

46:01.040 --> 46:04.880
where you want to be more involved, you want to have some intervention, you can still do that.

46:05.520 --> 46:12.480
You can choose to, you can use the manual mode, where you just handpicked some activation functions,

46:12.480 --> 46:18.480
like some functions, obviously they're just quadratic, linear, you can set them to be

46:19.440 --> 46:26.320
exactly the linear or quadratic, and then you retrain the other activation functions,

46:26.320 --> 46:34.000
and you see, and after retraining, you will see what those activation functions would change to

46:34.000 --> 46:40.000
different, would change to different form, and then this gives you, again, gives you some insight,

46:41.200 --> 46:47.920
like give you better evidence what the next guess you would want to make, and this is like the

46:47.920 --> 46:54.320
iterative process, like it's sort of like you are arguing or you are debating with the CAN network,

46:54.320 --> 47:02.400
like the CAN network is give you some, or you can say debating or collaborating,

47:02.400 --> 47:07.040
just like how we interact with a human collaborator, sometimes we debate, sometimes we collaborate,

47:07.040 --> 47:14.480
but you seem to look at the same thing from different angles, like the CAN network is really

47:14.480 --> 47:20.080
great at decomposing high dimensional functions into 1D functions, but those 1D functions may

47:20.080 --> 47:27.040
not be perfect, and there might be some actual subtlety, but humans are super great at identifying

47:27.040 --> 47:34.800
the symbolic stuff, and also recognizing the modular structure from the CAN diagram.

47:35.600 --> 47:40.880
So, yeah, so the takeaway is that you can choose to be lazy, use the automatic mode,

47:40.880 --> 47:46.160
or you can choose to be more responsible and more involved using the manual mode.

47:49.520 --> 47:56.480
Yes, so maybe, yeah, maybe in the end, yeah, I will just finish this really quick,

47:57.920 --> 48:06.080
so people have asked why it looks like CANs can, you know, in terms of expressive power, CANs are

48:06.080 --> 48:13.520
just MLPs, are just like secretly just MLPs, so why do we need CANs? I want to argue that from a

48:13.520 --> 48:22.480
like a high level philosophical level, CANs and MLPs are somewhat different. CAN is like clock work,

48:23.760 --> 48:29.760
like in a clock, pieces are customized and have clear purposes corresponding to the learnable

48:29.760 --> 48:37.440
activation functions in CANs. So for a clock, it's easy to tear it apart, it's easy to tear

48:37.440 --> 48:45.280
the pieces apart and then reassemble the pieces back together to get the clock back, but MLP is

48:45.280 --> 48:52.160
like a house made of bricks, so in MLPs, the pieces are produced in the same standard way,

48:52.160 --> 48:58.960
like each neuron takes in some linear combinations and then do some do the same nonlinear transformation

49:00.480 --> 49:07.760
but it's difficult to tear, once you have a house built, it's difficult to tear apart the bricks

49:07.760 --> 49:18.160
and then reassemble them. So in summary, the source of complexity are different in CANs and in MLPs,

49:18.160 --> 49:24.560
the source of complexity in CANs come from the complexity of each individual object,

49:24.560 --> 49:31.200
like those 1D learnable functions, but because the functions are 1D, no matter how complicated

49:31.200 --> 49:38.880
they are, they're 1D and they have clear purposes in some sense, they are nevertheless, they are

49:38.880 --> 49:47.920
interpretable, but the complexity of CANs come from complicated interactions of individual parts,

49:47.920 --> 49:54.400
the individual parts are simple, but the connections between these individual parts

49:54.400 --> 50:01.680
are really complicated, I guess it's more like human brains, it's more like biology,

50:03.040 --> 50:12.560
yeah, I don't know, but CANs seem like more aligned with the philosophy of reductionism,

50:12.560 --> 50:19.040
where you hope that, where you expect that you can decompose a complicated object into a few

50:20.000 --> 50:27.600
like interpretable individual objects, well in MLPs, everything is connected,

50:28.400 --> 50:38.000
the reason why MLPs function is because they have this emergent behavior or collective behavior

50:38.000 --> 50:48.880
in some sense. Yeah, just some interesting question people ask, are CANs physical? So

50:49.760 --> 50:56.800
if we think of like the Feynman diagram as physical, then unfortunately, Feynman diagrams

50:56.800 --> 51:04.880
are sort of more like MLPs, because in Feynman diagrams, like on the edges, it's just like a

51:04.880 --> 51:14.960
free flow in space without anything interesting happen, but on the nodes where two particles

51:14.960 --> 51:20.080
or multiple particles collide, it's where interesting things happen, this is more aligned,

51:20.080 --> 51:27.520
this is probably more aligned with MLPs, but CANs is like interesting thing happens on the edges,

51:27.520 --> 51:36.400
but not on the nodes. And yeah, last question, people also ask are CANs biological, because

51:37.120 --> 51:45.280
people think that MLPs are inspired by the neurons in our brains, are there any biological

51:45.280 --> 51:57.040
analogy? And I don't know at first, but someone from Twitter wrote it that CANs actually is a

51:57.040 --> 52:08.400
bit analogous to the cells in retina, where each individual cell receive light, apply some

52:08.400 --> 52:14.080
nonlinear transformation to it before summing everything up, I'm not sure, you guys are experts,

52:14.080 --> 52:21.840
so please correct me if I'm wrong here. But the argument is that, well, maybe the mechanisms

52:21.840 --> 52:26.240
of CANs like your first applied nonlinear transformation, then summing everything up,

52:26.240 --> 52:35.280
is indeed biological, but I guess that's just for fun. That's just a minor justification why

52:36.960 --> 52:42.560
we need CANs, because in some sense, CANs are also biological.

52:47.040 --> 52:53.680
Yeah, so that's basically everything I would like to share, and I'm happy to chat more if you

52:53.680 --> 53:01.440
guys have questions. Super interesting, thank you. Questions?

53:04.240 --> 53:10.800
Well, for the last last example in retina, I have some questions. If the network,

53:10.800 --> 53:18.320
like CAN network, is deep enough, is it still matter if you say nonlinearities before or after

53:19.200 --> 53:21.040
the summation?

53:24.560 --> 53:32.560
Yeah, so I guess the key difference, well, yeah, so I guess the key difference is that

53:32.560 --> 53:41.200
in CANs, activation functions are learnable, so I guess, yeah, but whether to put activation

53:41.200 --> 53:50.560
functions on edges or on nodes, I don't think that might be the key difference,

53:50.560 --> 53:55.280
like the learnability of activation functions give you more flexibility.

53:57.520 --> 54:03.440
Yeah, when you talk about this, I was thinking about that CAN is decomposing

54:04.400 --> 54:13.760
different variables, input variables, like if you have x and y, then CAN could decompose it,

54:13.760 --> 54:21.760
because you would have a different combination of them, but like if you have a nested function,

54:21.760 --> 54:31.040
like sine x square, or exponential sine x square, then the CAN seems not able to decompose them,

54:31.040 --> 54:32.800
because they don't have these primitives.

54:33.600 --> 54:40.320
Yes, yes, yes, that's exactly correct. So CAN can only discover compositionality in the sense that

54:40.320 --> 54:48.000
all the 1D functions are boring to CANs. It can just be approximated with just one B-splice,

54:48.000 --> 54:58.080
it doesn't learn any compositionality for single variables. That might be one bug,

54:59.040 --> 55:09.440
if you will, for CANs, if you really want to figure out the symbolic formulas in the data set.

55:11.280 --> 55:21.840
But like for Professor Tomaso Poggio, the author of the 1989 paper who sentenced

55:22.800 --> 55:30.640
or who sentenced Theorem to jail, he wrote in his paper that all the 1D functions are boring.

55:33.600 --> 55:37.600
And what's interesting is this compositional sparsity when you are dealing with multiple

55:38.560 --> 55:43.600
variables, but I guess it depends on your goal. If your goal is just to learn the function efficiently,

55:43.600 --> 55:51.600
then it's fine. But if your goal is to really understand if it's sine of exponential, exponential

55:51.600 --> 55:57.520
of sine, then we probably need to think about ways to handle this.

56:00.080 --> 56:01.040
Yeah, thank you.

56:03.360 --> 56:09.760
Have you thought about combining CANs and MLPs, given their somewhat complementary nature,

56:09.760 --> 56:11.920
or despite their complementary nature?

56:12.880 --> 56:14.480
Yeah, that's a great question.

56:21.920 --> 56:28.640
So we have some primitives, like CANs can propose this CAN layer, which is a new primitive.

56:28.640 --> 56:36.640
And for MLPs, it has these linear layers and also the nonlinear activations, which are also

56:36.720 --> 56:47.120
primitive. I mean, these are like the building blocks. And I guess as long as they fit together,

56:50.240 --> 56:59.040
as long as they fit together, you can freely just combine them in ways that you want.

56:59.040 --> 57:05.280
But it's just that it's a bit hard to tell what's the advantage of combining. And

57:05.280 --> 57:11.760
because I guess there are many ways to integrate the two models and which way is the best. And I

57:11.760 --> 57:21.200
guess it's a case dependent question. It again depends on what's your application, what's your goal,

57:21.200 --> 57:22.240
something like that.

