1
00:00:00,000 --> 00:00:10,000
Yeah. So in case you don't know me, I'm Zemin Liu. I'm now a fourth year PhD student at MIT.

2
00:00:10,800 --> 00:00:18,640
Welcome, Professor Max Tagmark. My research interests the center around AI plus science,

3
00:00:18,640 --> 00:00:26,080
so it can go both ways. Either you develop better AIs for science, like AI scientists,

4
00:00:26,080 --> 00:00:34,080
or you use inspirations or like tools in science to enhance the improvement of AI.

5
00:00:35,040 --> 00:00:41,440
So like this work is like both ways. We first try to use some ideas from math and see if we can

6
00:00:41,440 --> 00:00:49,200
develop new AI tools and see if we can, whether these new tools can give something back to to

7
00:00:49,200 --> 00:00:56,000
science. Yeah. So this is about a recent work called The Comagra of Arnold Networks.

8
00:01:01,520 --> 00:01:10,320
Today's AI is built upon this math theorem called the universal approximation theorem,

9
00:01:12,320 --> 00:01:16,560
which basically says that a high-dimensional function, you can decompose a high-dimensional

10
00:01:16,560 --> 00:01:24,800
function into a linear combination of some nonlinear features with this nonlinear function sigma,

11
00:01:26,160 --> 00:01:33,280
which is basically just this two-layer network known as the multilayer perceptron. But you

12
00:01:33,280 --> 00:01:39,200
can make it deeper. That's why it got the name multilayer. But we are asking this question,

13
00:01:40,160 --> 00:01:46,080
are there any alternative theorems we can use? We can leverage these new theorems. Well, not

14
00:01:46,080 --> 00:01:51,040
necessarily new in the mathematical sense, but like in the AI world to build another AI

15
00:01:51,760 --> 00:01:57,920
skyscraper based on new building blocks. So here we are examining this theorem called The

16
00:01:57,920 --> 00:02:04,560
Comagra of Arnold Representation Theorem. First question, what is the KA representation theorem?

17
00:02:05,120 --> 00:02:11,520
It says, again, like you given a high-dimensional function, you can write it down as a finite

18
00:02:11,520 --> 00:02:19,120
composition of one-dimensional continuous functions and just the summing operation. So more

19
00:02:19,120 --> 00:02:26,000
concretely, you can write down an n-dimensional function in this form, where you have the outer

20
00:02:26,000 --> 00:02:35,840
functions capital phi q and the inner functions phi qp. They are both just one-dimensional functions,

21
00:02:35,840 --> 00:02:43,360
and they are finite. Like the number of these functions is depending on the number of inputs

22
00:02:43,360 --> 00:02:50,720
variables n here. So what's nice about this theorem is that it tells us that the only true

23
00:02:50,720 --> 00:02:57,840
multivariate functions is the sum. Like you can express, say, multiplication with just the summing

24
00:02:57,840 --> 00:03:07,680
operation plus some 1D activation functions. So that sounds like really great news for

25
00:03:08,640 --> 00:03:12,320
approximating high-dimensional functions, especially for machine learning,

26
00:03:12,320 --> 00:03:17,680
because in high dimensions, to learn high-dimensional functions, we know we have curse of dimensionality.

27
00:03:18,400 --> 00:03:23,680
But this theorem seems to tell us that you can decompose high-dimensional functions into

28
00:03:23,680 --> 00:03:28,400
one-dimensional functions, which do not suffer from curse of dimensionality, which are great.

29
00:03:29,920 --> 00:03:42,960
But immediately, there is this plot twist. Back in 1989, professor Jarosi and Poggio wrote this

30
00:03:42,960 --> 00:03:51,200
paper examining the relevance of Comagra's theorem in the context of learning. And

31
00:03:52,000 --> 00:03:59,440
they conclude that we review Comagra's theorem and show that it is irrelevant in the context of

32
00:03:59,440 --> 00:04:09,280
networks for learning. So this paper basically sentenced the Ka theorem to death, at least

33
00:04:09,280 --> 00:04:19,360
sentenced to jail for like 30 years. So is that the end of the story? Well, we want to tell you

34
00:04:19,360 --> 00:04:27,440
that that's not the end of the story. And there's again a plot twist. So let's try to see what they

35
00:04:27,440 --> 00:04:36,960
say in their paper, why they sentenced the theorem to jail. So their argument was that the theorem

36
00:04:36,960 --> 00:04:46,400
fails to be true when the inner functions are required to be smooth. Basically, the original

37
00:04:46,400 --> 00:04:51,760
theorem says that you can decompose a high-dimensional continuous function into one-dimensional

38
00:04:51,760 --> 00:04:59,200
functions, but it doesn't guarantee you to decompose it into one-dimensional functions.

39
00:04:59,840 --> 00:05:06,240
But to learn something, like with gradient descent, you would need the functions to be

40
00:05:06,240 --> 00:05:13,120
smooth. Only being continuous is not enough. Even with continuous functions, you can still have

41
00:05:13,120 --> 00:05:20,960
some really pathological behavior. You don't have the smoothness constraint. So that's basically

42
00:05:20,960 --> 00:05:26,560
the argument, because the theory does not tell you anything about the smoothness of the functions.

43
00:05:26,560 --> 00:05:33,040
But in practice, only smooth functions are feasible in practice.

44
00:05:36,640 --> 00:05:46,080
So one interesting word they use is that a stable and usable exact representations

45
00:05:46,080 --> 00:05:54,240
seems hopeless. So that's like the attacking point. That's the angle we're taking. Maybe

46
00:05:54,240 --> 00:05:59,920
we do not need exact representations. Sometimes approximate representations might suffice.

47
00:06:00,640 --> 00:06:10,640
Yeah. So their paper was theoretically sound, was sound theoretically, but I'm trained as a

48
00:06:10,640 --> 00:06:18,160
physics student. So my level of rigor is lower than mathematicians. So me and also Max, we tend

49
00:06:18,160 --> 00:06:27,680
to have lower bar of being rigorous. So we have this naive optimism. So firstly, maybe like

50
00:06:27,680 --> 00:06:32,160
empirically, maybe we don't care about exact representations. Sometimes approximate ones

51
00:06:32,160 --> 00:06:40,160
may suffice. As long as it has some levels of explainability or interpretability, that's fine.

52
00:06:40,720 --> 00:06:46,480
And secondly, a common wisdom in modern deep learning is just stacking more layers to get

53
00:06:46,480 --> 00:06:52,160
different networks. They are basically arguing against the two-layer commograph network. But

54
00:06:52,160 --> 00:06:57,920
what about we build deeper networks? Maybe for deeper networks, even under the smooth

55
00:06:57,920 --> 00:07:04,560
constraint, you can win the expressive power back. Lastly, just the can-do spirit. We cannot say

56
00:07:04,560 --> 00:07:10,880
something doesn't work before really building it with the state-of-the-art techniques. They

57
00:07:10,880 --> 00:07:19,040
have the negative claim back like 30 years ago. Back then, they didn't even have the

58
00:07:19,120 --> 00:07:25,760
back propagation. Like at least it was not popularized back then. So we want to take

59
00:07:25,760 --> 00:07:33,600
everything we have in the modern area of deep learning and see how far we can go. So that's

60
00:07:33,600 --> 00:07:43,280
the mindset. So just with the can-do spirit, we propose or more properly rediscover or we

61
00:07:43,280 --> 00:07:54,240
contextualize the idea of commograph networks in today's deep learning world. Yeah. So here's

62
00:07:54,240 --> 00:07:59,600
the overview. First, I will introduce the math foundation. Well, I already briefly mentioned

63
00:07:59,600 --> 00:08:07,120
it. I won't dig deeper into it, but just I will emphasize again like the mathematical beauty of

64
00:08:07,120 --> 00:08:17,440
it. And then I will talk about the properties of cans. Why? And in what scenarios cans are more

65
00:08:17,440 --> 00:08:26,000
accurate and interpretable than current deep learning models? Yeah. So first, the math foundation,

66
00:08:26,000 --> 00:08:32,880
I already covered this part, but I want to emphasize again that the theorem

67
00:08:33,200 --> 00:08:38,960
looks really nice. That allows you to decompose a high-dimensional functions into one-dimensional

68
00:08:38,960 --> 00:08:44,640
functions. And after the decomposition is done, your only need to just your only job would be

69
00:08:44,640 --> 00:08:53,120
just approximating the 1D functions. So that's the idea of that's the idea of the commograph

70
00:08:53,120 --> 00:09:01,600
networks. Like decomposition first and then learn the 1D functions. Well, yeah. So this

71
00:09:01,600 --> 00:09:07,600
representation looks a bit complicated. You'll see that there are this huge, this big summation

72
00:09:07,600 --> 00:09:15,120
symbol and you have two layers of composition. And this will be complicated. But don't worry

73
00:09:15,120 --> 00:09:22,400
about it. It's just equivalent to this two-layer network. Let's suppose we have two inputs,

74
00:09:22,400 --> 00:09:29,600
x1 and x2, and we have the outputs at the top here. So the representation in the original

75
00:09:29,600 --> 00:09:38,400
theorem is basically just that you have five hidden neurons in the middle. And to get the

76
00:09:38,400 --> 00:09:48,720
activations in each hidden neuron, you basically apply a 1D, possibly nonlinear function to x1

77
00:09:48,720 --> 00:09:58,560
and x2, and sum up these two nonlinear activations to get the activations at the node. And

78
00:09:59,600 --> 00:10:07,360
in the second layer, it's similar that you apply some nonlinear function to the hidden activations,

79
00:10:07,360 --> 00:10:14,080
summing everything up at the output node, and that's how you get the output. So the computation

80
00:10:14,080 --> 00:10:27,200
graph is super clear with just this diagram. And this might remind you a lot of, this looks

81
00:10:27,200 --> 00:10:31,360
just like a multi-layer perceptron, the fully connected networks, where everything just fully

82
00:10:31,360 --> 00:10:37,520
connected. But instead of having activation functions on the nodes, now we are having

83
00:10:37,520 --> 00:10:42,960
activation functions on the edges. And on the nodes, you simply just have the summation operation,

84
00:10:44,800 --> 00:10:52,240
which is really simple. Yeah, just to make it more elegant and beautiful, we can,

85
00:10:52,800 --> 00:10:57,040
or more intuitive, we can basically, because like 1D functions, we can basically just visualize them

86
00:10:57,040 --> 00:11:03,840
with 1D curves, with the x and x as the input and y as the output. So now can network is basically,

87
00:11:04,480 --> 00:11:09,680
you can picture it as this. And by staring at it, you can, you can have some idea what's happening

88
00:11:09,680 --> 00:11:18,640
inside. Yeah, so I mentioned this is just a, so the theorem, so the representation is equivalent

89
00:11:18,640 --> 00:11:25,840
to a two-layer network. But can we go deeper? The answer is yes. Algorithmically speaking,

90
00:11:25,840 --> 00:11:33,440
because it's just a stack of two layers, which, from which we can abstract a notion called the

91
00:11:33,440 --> 00:11:39,760
can layer. So the original two-layer network is basically a stack of two, two can layers.

92
00:11:39,760 --> 00:11:45,360
And for each can layer, it's basically just taking some number of inputs and outputs, some number

93
00:11:45,440 --> 00:11:51,200
of outputs, and in between is fully connected. And on each edge, you have the active, you have the

94
00:11:51,200 --> 00:11:56,800
some nonlinear, learnable nonlinear activation function. And in the end, and in the outputs,

95
00:11:56,800 --> 00:12:04,880
you, you, you summing up the incoming activations. That's how a can layer works. And you can simply

96
00:12:04,880 --> 00:12:11,760
stack more and more can layers to get deeper and deeper cans. This is just a three-layer can,

97
00:12:12,720 --> 00:12:19,040
like, like the first layer, we're taking two, output three. The second layer, input two, output

98
00:12:19,040 --> 00:12:26,720
three. And the last layer, you input three, output one. So this is the, this is a three-layer network,

99
00:12:26,720 --> 00:12:33,120
which approximates a scalar function in two dimensions. But obviously, you can easily

100
00:12:33,120 --> 00:12:40,000
extend it to arbitrary dimension, like arbitrary input, arbitrary output, arbitrary width, arbitrary

101
00:12:40,800 --> 00:12:45,760
depth. So, so you have all the flexibility to choose the size of the network.

102
00:12:49,120 --> 00:12:56,960
Yeah, one, the first question Professor Poggio asked me when I presented this network to him,

103
00:12:57,920 --> 00:13:03,200
he asked, is that why do you need this deep networks? Because the original theorem told you

104
00:13:03,200 --> 00:13:11,120
that you only need the two-layer constructions. And here's just a quick answer that I, I can give

105
00:13:11,120 --> 00:13:19,840
you an example. So please look at this symbolic formula. And if you examine it, you would immediately

106
00:13:19,840 --> 00:13:26,560
realize that you would need at least three compositions to do this, to construct this formula.

107
00:13:27,120 --> 00:13:31,680
You need the, you need the squared function, you need the sine function, you need the exponential

108
00:13:31,680 --> 00:13:38,240
function. They're like, because they're, because it's the compositional structure, they're in different

109
00:13:38,240 --> 00:13:46,640
layers. So you would at least, at least need three layers to, to, to learn this formula. And

110
00:13:46,640 --> 00:13:52,160
indeed, if you just use two-layer network, the activation functions becomes really oscillatory,

111
00:13:52,160 --> 00:13:59,360
becomes really pathological. And the performance is bad, and also the interpretability is bad.

112
00:13:59,360 --> 00:14:06,480
But in the right, I show that a three-layer network, a three-layer can train on this,

113
00:14:06,480 --> 00:14:11,680
train on this dataset. And after training, you would immediately see the learned activation

114
00:14:11,680 --> 00:14:15,600
functions. In the first layer, you got the squared, in the second layer, you got the sine,

115
00:14:15,600 --> 00:14:21,520
and the last layer, you got the exponential. Well, well, you may think of, you may think this is some

116
00:14:21,520 --> 00:14:29,200
other functions, maybe just some local quadratic, but yeah. But you can, but you can, like,

117
00:14:29,200 --> 00:14:34,160
do the template matching with the candidate functions and figure out which one fits the best.

118
00:14:38,400 --> 00:14:43,840
Yeah, so, so I, I said that this activation functions are learnable. How do we make them

119
00:14:43,840 --> 00:14:50,080
learnable? Because they're functions. And, and, and, and the common wisdom is that we need to make,

120
00:14:50,080 --> 00:14:56,160
we need to parametrize the things to be learned so that we can use gradient descent to learn,

121
00:14:58,000 --> 00:15:04,960
to learn this stuff. So the idea is that we parametrize a 1D function with, with B splines.

122
00:15:04,960 --> 00:15:13,520
So B splines is basically some piecewise, some, some local piecewise polynomial functions.

123
00:15:14,080 --> 00:15:22,720
So, so here I showed that there are some local B spline bases. And the way we construct the activation

124
00:15:22,720 --> 00:15:29,200
functions is by linearly combining this, this, this B spline functions. And the only learnable

125
00:15:29,200 --> 00:15:36,720
parameters are just the linear coefficients of, of, of this local basis. And what's nice about

126
00:15:36,720 --> 00:15:43,040
this formulation is that we, we inherit the advantage of B splines. We can easily switch

127
00:15:43,120 --> 00:15:49,520
between fine grains, fine grain grids and coarse grain grids. If you want something to be more

128
00:15:49,520 --> 00:15:55,200
accurate, you can, you can choose the mesh to be more fine grain. If you want the model to be

129
00:15:55,200 --> 00:16:00,480
smaller, so you can have a faster inference, you can, you can, you can choose a more coarse-grained

130
00:16:00,480 --> 00:16:11,360
model. Yeah, that's basically the idea of cans. And we can compare MLPs and cans side by side,

131
00:16:11,360 --> 00:16:15,840
because they do share some similarities, but also share some difference, but also have some

132
00:16:15,840 --> 00:16:23,440
differences. So MLPs are inspired by the universal approximation theorem cans, again,

133
00:16:23,440 --> 00:16:29,360
inspired by the Camargo Ravano representation theorem. The network looks a bit similar in the

134
00:16:29,360 --> 00:16:36,160
sense that they're both fully connected. But their dual, they're different, but they're dual in the

135
00:16:36,160 --> 00:16:41,840
sense that MLPs have fixed activation functions on nodes. Well, you can make them trainable, but

136
00:16:42,640 --> 00:16:48,720
but they're on nodes for sure. And in MLPs, we have learnable weights, learnable linear weights on

137
00:16:48,720 --> 00:16:57,920
edges. By contrast, cans have learnable activation functions on edges, while cans have this simple

138
00:16:57,920 --> 00:17:04,880
linear summation operation on nodes. So, so, so in this sense, cans does not separate the linear

139
00:17:04,880 --> 00:17:11,920
part and the long inner part as MLPs do, but it integrates both the linear part and long inner

140
00:17:11,920 --> 00:17:17,600
part altogether into the can layer. And the can network is simply just the stack of the can layers.

141
00:17:21,520 --> 00:17:30,000
Yeah, so in both cases, you are free to stack the model to become deeper and deeper because

142
00:17:30,000 --> 00:17:34,160
you have the basic notion of a layer, you just stack more layers to get the deeper networks.

143
00:17:35,360 --> 00:17:45,680
Yeah, so that's the basic, that's the basic ideas of cans. And now I want to elaborate more

144
00:17:48,800 --> 00:17:54,000
like why do we care about, why do we care about this? What are the advantages that cans can bring

145
00:17:54,000 --> 00:18:03,440
to us, but other black box models do not bring to us? So yeah, so the first property is the

146
00:18:03,440 --> 00:18:09,600
scaling behavior of cans. As I mentioned before, the idea of cans is decomposing a

147
00:18:09,600 --> 00:18:14,320
high dimensional functions into one dimensional functions. So that looks like really promising

148
00:18:16,400 --> 00:18:23,280
that it can get us, it can get us out of the curse of dimensionality. Let's suppose we are

149
00:18:23,920 --> 00:18:30,480
trying to approximate a d dimensional functions. And I suppose the function has no structure at all.

150
00:18:30,480 --> 00:18:37,440
So then we need to, we need to have a hypercube and have a uniform grid on the hypercube.

151
00:18:38,240 --> 00:18:44,720
Let's suppose we have 10 grid, 10 anchor points along each dimension, then we will need 10 to the

152
00:18:44,720 --> 00:18:49,760
power of d number of anchor points in the d dimensional hypercube. So that's exponentially

153
00:18:50,640 --> 00:18:57,120
expensive. So if you do the classical approximation theory, you would notice that the approximation

154
00:18:57,120 --> 00:19:06,160
error would decay as a parallel, as a parallel, as a function of the number of input dimensionality.

155
00:19:06,160 --> 00:19:12,560
And it's one, and the exponent is one over d, meaning that you got exponentially

156
00:19:13,680 --> 00:19:18,720
like slower when you have more and more, when you have more and more dimensions, like if you need

157
00:19:18,720 --> 00:19:25,200
10 points in one d, you would need 100 points in 2d, you will need 1000 points in 3d and so on.

158
00:19:27,200 --> 00:19:34,160
But if the function has some structure, like if it has the chromograph unknown representation,

159
00:19:34,160 --> 00:19:40,560
then we can decompose it into a bunch of 1d functions. And then our job would just be

160
00:19:40,560 --> 00:19:47,440
approximating 1d functions. So now effectively d becomes one. So you got a really, you got the

161
00:19:47,440 --> 00:19:54,480
fastest possible scaling laws. But the, but the caveats, immediately the caveat is that we,

162
00:19:54,560 --> 00:20:03,600
the assumption is that we, like the function has a smooth chromograph, a smooth finite size

163
00:20:03,600 --> 00:20:09,680
chromograph unknown representation. All of this, you know, all of these objectives,

164
00:20:09,680 --> 00:20:15,600
objectives like smooth or finite size are just, are just practical conditions for a real, for,

165
00:20:16,640 --> 00:20:22,880
for a network which we have access in practice that can really learn the network. We want it to

166
00:20:22,880 --> 00:20:28,400
be smooth because we parametrize it with b-splice, which are smooth. We want them to be finite size

167
00:20:28,400 --> 00:20:35,520
because, of course, you know, we cannot initialize, we cannot deal with an infinite size neural network.

168
00:20:38,720 --> 00:20:44,720
So, yeah, so, so, so we just did some sandwich check on, on some symbolic formulas.

169
00:20:45,120 --> 00:20:51,280
So, yeah, so symbolic formulas are like white, are like what we used in, in science. So that's why we

170
00:20:51,280 --> 00:20:58,640
test them first. So let's see. So the red dashed line is the theoretical prediction. Here we are

171
00:20:58,640 --> 00:21:12,000
using cubic spline. So k is k3. And the scaling exponent is k plus one equals four. And the

172
00:21:12,880 --> 00:21:21,920
curve, yeah, so the thick blue line is for the can network. And you see that almost like the,

173
00:21:21,920 --> 00:21:28,240
like empirical results for the can network almost agreed with, almost agrees with the

174
00:21:29,360 --> 00:21:35,120
theoretical prediction, although sometimes performed slightly worse. Or in this case,

175
00:21:35,120 --> 00:21:40,160
in the second to last case, there's a hundred dimensional case. And it performs much worse

176
00:21:40,160 --> 00:21:45,440
than a theoretical prediction because of the, the, the, the, because the dimension is just too

177
00:21:45,440 --> 00:21:52,080
high and the network can get, can get stuck at some local minima or whatever. But, but nevertheless,

178
00:21:52,080 --> 00:21:59,680
it's still output from MLPs, which you see up in the upright corner here, like the case really

179
00:21:59,680 --> 00:22:07,040
slow. But, but the cans, but cans at least can output form MLPs to a great margin,

180
00:22:07,040 --> 00:22:14,720
although still not saturating the theoretical prediction. But still the scaling law that can

181
00:22:14,720 --> 00:22:23,040
shows looks, looks promising that it's, it seems to not fully beats the curse of dimensionality,

182
00:22:23,040 --> 00:22:25,680
but at least partially beat the curse of dimensionality.

183
00:22:29,120 --> 00:22:35,920
Well, yeah, yeah. So, yeah, just to play the devil's advocate here, you may immediately notice

184
00:22:35,920 --> 00:22:42,240
that I'm on purpose just deliberately using this symbolic formulas. You, you might be wondering,

185
00:22:42,240 --> 00:22:49,920
well, maybe, maybe the functions we care. We encounter a lot in nature may not be symbolic,

186
00:22:49,920 --> 00:22:55,840
they might be some weird, you know, like, at least for special functions, they, they, they,

187
00:22:55,840 --> 00:23:02,160
they are like infinite series, which are hard to be represented with just the finite network. So,

188
00:23:02,160 --> 00:23:07,200
what are, what, what, so, so, so, so what's the cans performance in that scenario?

189
00:23:08,560 --> 00:23:14,880
So, yeah, so we just tried some special functions, which we know for most of them, they do not have

190
00:23:14,880 --> 00:23:23,360
analytical formulas. And indeed, we see that the scaling laws of cans do not saturate the

191
00:23:23,360 --> 00:23:28,720
theoretical prediction, meaning that probably you cannot decompose like a high dimensional

192
00:23:28,720 --> 00:23:35,600
functions into just a one D functions. But still, our goal here is to outcompetes MLPs.

193
00:23:36,240 --> 00:23:41,680
And, and, and it's a feature, not a bug, like not all the functions can be decomposed into

194
00:23:41,680 --> 00:23:51,600
smooth finite representations of this K representation. They may admit non smooth finite size or

195
00:23:51,600 --> 00:23:58,880
smooth infinite size. But in, but, but neither case is, is like the, is what's accessible in

196
00:23:58,880 --> 00:24:07,200
practice. So, yeah, so, so here we show that in most cases, we can achieve this minus two

197
00:24:07,200 --> 00:24:14,320
scaling law, which means that the can network, well, well, sorry. So here, this special functions

198
00:24:14,400 --> 00:24:22,000
are all just two dimensional. So this, so these, according to the spline theory, it would predict

199
00:24:22,000 --> 00:24:27,920
two dimensional, it would predicts a two dimension, like, like, like a scaling exponent to be two,

200
00:24:27,920 --> 00:24:33,600
which agrees with the can results. But in some, but in some case, we can, we can still got the

201
00:24:33,600 --> 00:24:41,680
minus four scaling law. And the reason is that actually, this is secretly, like, like, although

202
00:24:41,680 --> 00:24:47,440
we call, we call it a special function, spherical harmonics are not that special in the sense that

203
00:24:47,440 --> 00:24:54,320
they're still decomposable. So, so you can get the minus four scaling. But also in other, in other

204
00:24:54,320 --> 00:24:59,280
cases, you've got some worse behavior, like you've got the minus one scaling, which means that the

205
00:24:59,280 --> 00:25:06,880
can is underperforming even out on the performing compared to like the spline theory. But what's

206
00:25:06,880 --> 00:25:14,080
interesting is that in this case, MLPs are even more underperforming than cans. So, so this may

207
00:25:14,080 --> 00:25:20,480
tell us that maybe for low dimensional problems, neural networks are not that necessary. And even

208
00:25:20,480 --> 00:25:26,880
the spline theory, like the spline approximation can outcompete the neural network. So, so it's

209
00:25:26,880 --> 00:25:34,880
something to be think about. It's something good to keep in mind. Because neural networks just have

210
00:25:34,880 --> 00:25:39,760
too many degrees of freedom and may have optimization issue.

211
00:25:42,480 --> 00:25:50,000
Yeah. So, so beyond function approximation, we can go on to solving partial differential equations.

212
00:25:50,000 --> 00:25:56,080
So in the setup of physics in form learning, basically, we're trying to solve, we're basically

213
00:25:56,080 --> 00:26:08,640
trying to represent the solution of a PDE with MLP or with a can network. So, so the only difference

214
00:26:08,640 --> 00:26:13,520
from the previous results from the previous experiment is that we're just we're using the physics

215
00:26:13,520 --> 00:26:19,920
informed loss rather than the regression loss. So the optimization becomes more complicated,

216
00:26:19,920 --> 00:26:26,480
but still it's just approximating some function. Yeah. So, so we still see that we with cans,

217
00:26:26,480 --> 00:26:33,920
we can get this optimal scaling law. Well, with MLPs, you see, while with MLPs, you see that,

218
00:26:33,920 --> 00:26:37,840
well, it has the skill at first, but it plateaus really fast and then

219
00:26:38,720 --> 00:26:43,440
does not improve when you have more parameters beyond 10,000.

220
00:26:43,440 --> 00:26:52,480
Um, besides being more accurate, we can also gain some insight what the network is learning.

221
00:26:52,480 --> 00:26:58,720
So, so, yes, yes. So for this example, we can, for this PDE example, we can actually visualize

222
00:26:59,280 --> 00:27:05,360
the can network like this. And immediately you can see that there's some like sine waves and

223
00:27:05,360 --> 00:27:11,840
there's some linear functions. And you can even do symbolic regression to it, like,

224
00:27:12,080 --> 00:27:17,280
like, like our software provides you a way to do this, you can, you can do symbolic regression

225
00:27:17,280 --> 00:27:24,160
to it. And after you do this, you can do some further training. And you, and you can even extract

226
00:27:24,160 --> 00:27:30,080
out the symbolic formula, which gives you like a loss down to machine precision. This is something

227
00:27:30,800 --> 00:27:38,480
that that that standard neural networks would not give you because of because they usually

228
00:27:38,480 --> 00:27:43,760
cannot convert a neural network into a symbolic formula very easily, but with cans, you can easily

229
00:27:43,760 --> 00:27:54,720
do that. Another property of cans is that it has this property of continual learning, at least

230
00:27:54,720 --> 00:28:02,880
in 1D. Yeah, so people have, people have found that, you know, this claim might be invalid for

231
00:28:02,880 --> 00:28:10,080
high dimensions. So take my word with a grain of salt. But at least for 1D, the case is like,

232
00:28:11,040 --> 00:28:18,400
we have, we want to approximate this 1D functions with five peaks. But instead of feeding auto data

233
00:28:18,400 --> 00:28:25,920
at once to the network, we're feeding each time we just feed one peak to the network. And we do

234
00:28:25,920 --> 00:28:33,120
the sequential learning at each stage, the network is just, is just fed with just one peak.

235
00:28:33,120 --> 00:28:40,000
And with cans, and with cans, because we're using the local B-splines, so when it sees the new data,

236
00:28:40,000 --> 00:28:46,640
it does not update the parameters correspond to the old data. So it has this, it can't get rid of

237
00:28:46,640 --> 00:28:55,520
this catastrophic forgetting, like when new data are coming in, the can is able to memorize the

238
00:28:55,520 --> 00:29:02,480
old data and still do quite well on the old data. But this is not true for MLPs. Like when you're,

239
00:29:02,480 --> 00:29:08,080
when you're fed, when the MLPs are fed with the new data, it catastrophically, they catastrophically

240
00:29:08,080 --> 00:29:14,240
forget about the old data. Because in MLPs, you usually have this global activation functions

241
00:29:14,240 --> 00:29:20,720
like the silo functions or Reilu functions. So whenever you make adjustments locally, it will

242
00:29:20,720 --> 00:29:26,480
affect, you know, the predictions far away. So that's the reason why MLPs would not keep

243
00:29:28,240 --> 00:29:32,080
would, would catastrophically forget about the old data.

244
00:29:34,880 --> 00:29:42,080
Yeah, so, so, so that's the first part about accuracy. The second part is about interpretability.

245
00:29:42,080 --> 00:29:48,720
You might already have some sense because we are able to visualize the network as a diagram. So,

246
00:29:48,880 --> 00:29:53,600
the hope is that you can just stare at a diagram and gain some insight of what's happening inside

247
00:29:53,600 --> 00:30:05,040
the neural network. Yes, yeah, so here we have some, some, some toy examples of, yeah, for example,

248
00:30:05,040 --> 00:30:12,960
how do cans do the multiplication computation. So we have x and y as the input and the x times y

249
00:30:12,960 --> 00:30:19,280
as the output. So when we train the can network, we also have some, something similar to L1

250
00:30:19,280 --> 00:30:25,280
regularization to sparsify the network. So we can extract out the minimal network to do the task.

251
00:30:26,560 --> 00:30:31,520
So in the multiplication case, we see that only two neurons are active in the end,

252
00:30:31,520 --> 00:30:37,840
and we can read off the symbolic formulas of how, and get a sense of how it does the computation.

253
00:30:37,840 --> 00:30:47,280
Well, I marked the symbolic formulas here. It may take a while. It may take a while, but I,

254
00:30:47,280 --> 00:30:54,960
well, the point is that it basically just some squared functions and, and the way the can network

255
00:30:54,960 --> 00:31:02,720
learns to compute this multiplication is by leveraging some, some squared equality here.

256
00:31:03,680 --> 00:31:09,440
And the second example, the can is tasked with the division task,

257
00:31:10,400 --> 00:31:21,360
where we input two positive numbers, x and y, and can is asked to predict the x divided by y.

258
00:31:21,360 --> 00:31:27,840
And because here x and y are both positive, the can network learns to first take the

259
00:31:27,840 --> 00:31:32,960
logarithm transformation and then take the subtracted two logarithm and then

260
00:31:33,920 --> 00:31:41,040
transform the same back via the exponential, exponentiation. So, so, so that's really cute.

261
00:31:41,040 --> 00:31:47,120
Like, like one example of the commograph theorem is that you can basically do the multiplication

262
00:31:47,120 --> 00:31:53,360
of positive numbers or the division of positive numbers in the logarithmic scale, because that's,

263
00:31:53,360 --> 00:31:59,280
because the division or multiplication in the logarithmic scale, it transferred to,

264
00:31:59,280 --> 00:32:04,560
would translate to like addition and subtraction in, in the logarithmic scale.

265
00:32:08,240 --> 00:32:13,280
Yeah, so, so, so, so this examples are really simple. You might be wondering what about a more

266
00:32:13,280 --> 00:32:20,480
complicated formula, then, then it might be very complicated to decode what the cans have learned.

267
00:32:21,440 --> 00:32:26,880
So, I would want to argue that this might be a feature, not a bug, or you can call it a bug,

268
00:32:26,880 --> 00:32:30,400
but I can, but I won't call it a feature in the sense that,

269
00:32:32,720 --> 00:32:36,640
sorry, I didn't show the network here, but let's suppose we're doing this formula here,

270
00:32:36,640 --> 00:32:42,880
like u plus v divided by 1 plus uv. So, if you're familiar with relativity, you are special relativity

271
00:32:42,960 --> 00:32:48,320
on, you are, you are, you are, you are realized that this is the velocity, relativistic velocity

272
00:32:48,320 --> 00:32:56,880
addition. So, at first, I thought that I need the five-layer can to fix this function, because

273
00:32:56,880 --> 00:33:02,880
you would need multiplication, which would give, which would consume two layers, you would,

274
00:33:03,920 --> 00:33:08,880
two additions, consume another two layers, and also you, you, you would need the division.

275
00:33:08,880 --> 00:33:14,560
So, so that's in total five layers, but it turned out you can only, you can, you can just use a two-layer

276
00:33:14,560 --> 00:33:20,160
can to fix this function perfectly well. And in the end, I realized that this is just,

277
00:33:22,080 --> 00:33:28,080
this is just a rapidity trick known in special relativity, where you first do the arc-tange

278
00:33:28,080 --> 00:33:34,080
transformation to u and v separately, sum the thing to, to rapidity up, and then you do the

279
00:33:34,080 --> 00:33:40,640
tange transformation back to, to get this formula. So, in this sense, it's rediscovering the, it's,

280
00:33:40,640 --> 00:33:49,600
it's rediscovering the rapidity trick, known, well-known in, well-known in the special relativity.

281
00:33:49,600 --> 00:33:55,600
And in some cases, I indeed find that the network finds some more compressed,

282
00:33:55,600 --> 00:34:01,280
compact representation than I would expect. That's good news in the sense that's the,

283
00:34:01,760 --> 00:34:08,080
the, in the sense that the network is discovering something more compact. So, the representation

284
00:34:08,080 --> 00:34:13,840
is more powerful than I have expected. But the bad news is that sometimes the, the interpretation

285
00:34:13,840 --> 00:34:22,080
can be subtle, can be a bit more complicated. But, but I mean, it's, it can be a, it can be a

286
00:34:22,080 --> 00:34:31,520
feature, but depending on your view. Yeah, so, so, so, so this is one paragraph I take from

287
00:34:32,240 --> 00:34:38,320
the paper. This is criticized, this has been criticized a lot in the social media, but I,

288
00:34:39,120 --> 00:34:46,880
but I find this analogy really interesting. So, I still want to highlight this part.

289
00:34:47,840 --> 00:34:53,760
So, so to me, I think can is like a, it's sort of like language model or, or like language or

290
00:34:53,760 --> 00:34:59,680
even like language for AI plus science. The reason why language models are so transformative and

291
00:34:59,680 --> 00:35:05,200
powerful is because they are useful to anyone who can speak natural language. But the, but the

292
00:35:05,200 --> 00:35:12,720
language of science is functions or more advanced mathematical objects build on functions because

293
00:35:12,800 --> 00:35:20,000
cans are composed of functions which are 1D, so they are interpretable. So, when a human user stares

294
00:35:20,000 --> 00:35:26,720
at a can, it's like communicating it with, it's like communicating it with the, using the language

295
00:35:26,720 --> 00:35:33,680
of functions. So, to elaborate more and to make it more entertaining, let's suppose we are like

296
00:35:33,680 --> 00:35:40,080
my advisor Max is communicating with the can network. Here, I picture the can network as

297
00:35:40,160 --> 00:35:46,080
a trisolarian from the three-body problem. I'm not sure if guys have watched it, but basically

298
00:35:46,080 --> 00:35:53,360
the trisolarians, their brains are transparent, so they cannot hide any secrets from others. So,

299
00:35:53,360 --> 00:36:03,600
they are totally transparent. So, so Max went up, give, give the can, give the can a dataset.

300
00:36:03,760 --> 00:36:10,000
Max was like, here is my dataset. It contains the mystery of the universe. I want you,

301
00:36:10,800 --> 00:36:18,480
the can network, to figure out, to figure out the structure of the datasets and, and the can

302
00:36:19,360 --> 00:36:27,200
initialize the can network like this. So, here's the brain of the can network initially. And Max,

303
00:36:27,440 --> 00:36:35,040
given the dataset, Max wants to train the can network, to train the brain. And after training,

304
00:36:35,040 --> 00:36:41,280
you can get this sparse network, which you start to see some structure. But still, it's a bit,

305
00:36:41,280 --> 00:36:48,320
there's still some residual connections, which looks quite, which quite annoying. So, Max asked the

306
00:36:48,320 --> 00:36:56,080
can network to prune the redundant connections. And after pruning, you got, after pruning,

307
00:36:56,160 --> 00:37:05,920
you got this, you got this sub network, which is responsible for the computation. And Max further

308
00:37:05,920 --> 00:37:11,600
asked the network to the can network to symbolify it, because it looks like, it looks like in the

309
00:37:11,600 --> 00:37:17,200
bottom left, it looks like just a sign function. And in the bottom right, this looks just like a

310
00:37:17,200 --> 00:37:22,800
parabola. And this just looks like an exponential. So, maybe you can symbolify it to gain some more

311
00:37:22,800 --> 00:37:33,120
insight. So, yes, so the can network said, yes, I can symbolify it. And you can. And now the dataset

312
00:37:34,240 --> 00:37:41,680
goes all the way down to the symbolic formula. But, but we can imagine another conversation Max

313
00:37:41,680 --> 00:37:47,920
would have with an MLP. So, Max went up and give the dataset to MLP, and want the MLP to figure

314
00:37:47,920 --> 00:37:57,840
out the symbolic formula in it. And like before, the MLP initialized the brain, looked like something

315
00:37:57,840 --> 00:38:04,400
like this, really messy. After training, Max asked MLP to train the brain, but even after

316
00:38:04,400 --> 00:38:13,280
training, the connection still looks really messy. And MLPs were like, and the MLP is like, I really

317
00:38:13,280 --> 00:38:18,880
trained it, but the loss is pretty low. But it's just that the connections are still very complicated.

318
00:38:21,120 --> 00:38:29,360
Now Max got confused. Like, what's going on? What's going on with your brain? And now MLP is like,

319
00:38:29,360 --> 00:38:36,000
it's just that your humans are too stupid to understand my computations. You cannot say,

320
00:38:36,000 --> 00:38:42,000
I'm wrong, simply because you cannot understand me. So, Max now got really pissed off and turned

321
00:38:42,000 --> 00:38:51,920
back to CANS. So, those are just some imaginary stories I made up with the symphatic example.

322
00:38:51,920 --> 00:39:01,040
And that symphatic example is really simple. But I want to show that we can really use CANS

323
00:39:01,040 --> 00:39:07,440
as a collaborator in scientific research. And CANS can give us some non-trivial results.

324
00:39:08,080 --> 00:39:15,280
CANS can give us some new discoveries. So, the first example is,

325
00:39:17,840 --> 00:39:25,520
yeah, so this example was used in a DeepMind Nature paper three years ago, where they used MLP

326
00:39:25,520 --> 00:39:35,200
to discover a relationship in a NOT dataset. So, each NOT has some invariance. Basically,

327
00:39:35,200 --> 00:39:39,840
each NOT is associated some numbers. And these numbers, they have some relations. And we want

328
00:39:39,840 --> 00:39:48,160
to dig out the relations among these variables. So, what the DeepMind people did was they used

329
00:39:48,160 --> 00:39:53,760
the train and MLP and used the attribution methods, basically take the gradient with respect to these

330
00:39:53,760 --> 00:40:00,960
input variables, and use that as a score to attribute these features. And then rank these

331
00:40:00,960 --> 00:40:05,440
features to get a sense of which features are more important than other features. And they

332
00:40:05,440 --> 00:40:14,480
identified three important features. That's the only thing that's automated in their framework.

333
00:40:14,480 --> 00:40:20,800
And then the human scientist came in and tried to come up with a symbolic formula for it.

334
00:40:21,600 --> 00:40:26,160
So, we're asking this question, can we discover, have we discovered these results

335
00:40:26,160 --> 00:40:32,160
with more automation, with less, you know, efforts, and probably even discovering something new

336
00:40:32,160 --> 00:40:42,320
that the DeepMind paper were missing? So, first, we are able to discover the three important variables

337
00:40:42,320 --> 00:40:49,920
with the CAN network, with much more intuition and automation. So, their network, they used a

338
00:40:50,240 --> 00:40:56,480
three-layer, sorry, they used a five-layer MLP, and each hidden layer has 300 neurons. So,

339
00:40:56,480 --> 00:41:02,080
that's really hard to interpret. That's why they used the feature attribution. But we find that

340
00:41:02,080 --> 00:41:10,480
surprisingly, we only needed one hidden layer and one hidden neuron, the CAN network, to do the task,

341
00:41:10,480 --> 00:41:19,520
as well as their five-layer, like, a million-parameter MLPs. And with this, we can also clearly see

342
00:41:20,000 --> 00:41:28,960
the activations, the importance now basically becomes, you can basically understand the importance of

343
00:41:28,960 --> 00:41:38,960
these variables with the L1 norm of these activation functions. So, that's also how we visualize

344
00:41:39,520 --> 00:41:44,960
the connections. So, you can basically read off from this diagram that the strong connections

345
00:41:44,960 --> 00:41:49,920
are the important variables, while the weaker or even nearly transparent

346
00:41:50,960 --> 00:41:53,600
connections, meaning that irrelevant variables.

347
00:41:57,360 --> 00:42:04,320
Yeah, we can also discover symbolic formulas, as I said before, because the CAN network decomposes

348
00:42:04,320 --> 00:42:09,760
high-dimensional functions into 1D, and then we can just do template matching in 1D

349
00:42:09,760 --> 00:42:17,680
to see what each 1D functions represent symbolic formulas, and then compose all these 1D functions

350
00:42:18,560 --> 00:42:20,640
back to get these high-dimensional functions.

351
00:42:23,760 --> 00:42:32,720
Something beyond their paper we discovered is that their setup is a supervised learning setup.

352
00:42:32,720 --> 00:42:39,120
Basically, they need to partition the variables into inputs and outputs, and they use the inputs to

353
00:42:39,680 --> 00:42:45,680
predict at the outputs. But in some cases, we do not know how to partition the inputs and outputs,

354
00:42:45,680 --> 00:42:57,040
like all the variables, they are treated equally. So, we want to develop this unsupervised setup

355
00:42:58,080 --> 00:43:05,920
where all the variables serve as inputs. But we use some notion of contrastive learning to classify

356
00:43:05,920 --> 00:43:12,640
whether some given input is a real knot or a fake knot, or that might be too technical.

357
00:43:16,320 --> 00:43:23,680
The result is that we are able to discover more relations beyond the relation they've

358
00:43:23,680 --> 00:43:29,680
discovered, because they manually partition one variable as the output, so they can only discover

359
00:43:30,640 --> 00:43:36,320
the relations that involve that variable. But here, we are learning it in an unsupervised way,

360
00:43:36,320 --> 00:43:43,040
so we can learn more than just one relation. We also discovered the relation between these

361
00:43:43,040 --> 00:43:53,760
three variables and between these two variables. Unfortunately, or fortunately, these relations

362
00:43:54,080 --> 00:44:00,720
are already known in the literature of knot theory. So, the unfortunate part is that we did not

363
00:44:00,720 --> 00:44:09,040
discover anything new with our framework, but notice our network is just very preliminary,

364
00:44:09,040 --> 00:44:14,720
it's just a one layer, it's just a one layer if we ignore about the classifier in the second layer.

365
00:44:14,720 --> 00:44:21,280
So, hypothetically, we can make it deeper to get more complicated relations. But the

366
00:44:21,280 --> 00:44:27,040
fortunate part is that it verifies that's what we discovered with the network is something,

367
00:44:28,160 --> 00:44:32,800
is not bullshit, it's something that makes sense that people already know in the literature.

368
00:44:36,880 --> 00:44:42,960
Yeah, we also did this physics example, specifically Anderson localization.

369
00:44:43,920 --> 00:44:52,320
I don't want to bore you with the technical detail, but again, the goal here is to try to figure out

370
00:44:53,040 --> 00:45:00,080
the symbolic formula of the phase transition boundary. In Anderson localization, we have the

371
00:45:00,080 --> 00:45:06,240
localization phase and the extended phase, and there is a phase boundary, and we want to extract

372
00:45:06,240 --> 00:45:13,680
out the phase boundary, especially the symbolic formula of the phase boundary,

373
00:45:14,240 --> 00:45:19,040
if there exists a symbolic formula for it from the road data.

374
00:45:22,880 --> 00:45:28,720
Yeah, so for this slide, I don't want to go to the detail, but the point I want to make with

375
00:45:28,720 --> 00:45:36,560
this slide is that the CAN network has, just like cars, you have manual mode, you have manual mode,

376
00:45:36,560 --> 00:45:41,840
you have automatic mode, like if you're lazy, you can just delegate everything to CANS, and CANS

377
00:45:41,840 --> 00:45:48,320
will return you a symbolic formula fully automated, but that might not be correct. That might not

378
00:45:48,320 --> 00:45:55,520
be what you want. You can do that, it can give you reasonable accuracy, but may not be fully

379
00:45:55,520 --> 00:46:01,040
interpretable, but if you want to have some controllability over the CAN network,

380
00:46:01,040 --> 00:46:04,880
where you want to be more involved, you want to have some intervention, you can still do that.

381
00:46:05,520 --> 00:46:12,480
You can choose to, you can use the manual mode, where you just handpicked some activation functions,

382
00:46:12,480 --> 00:46:18,480
like some functions, obviously they're just quadratic, linear, you can set them to be

383
00:46:19,440 --> 00:46:26,320
exactly the linear or quadratic, and then you retrain the other activation functions,

384
00:46:26,320 --> 00:46:34,000
and you see, and after retraining, you will see what those activation functions would change to

385
00:46:34,000 --> 00:46:40,000
different, would change to different form, and then this gives you, again, gives you some insight,

386
00:46:41,200 --> 00:46:47,920
like give you better evidence what the next guess you would want to make, and this is like the

387
00:46:47,920 --> 00:46:54,320
iterative process, like it's sort of like you are arguing or you are debating with the CAN network,

388
00:46:54,320 --> 00:47:02,400
like the CAN network is give you some, or you can say debating or collaborating,

389
00:47:02,400 --> 00:47:07,040
just like how we interact with a human collaborator, sometimes we debate, sometimes we collaborate,

390
00:47:07,040 --> 00:47:14,480
but you seem to look at the same thing from different angles, like the CAN network is really

391
00:47:14,480 --> 00:47:20,080
great at decomposing high dimensional functions into 1D functions, but those 1D functions may

392
00:47:20,080 --> 00:47:27,040
not be perfect, and there might be some actual subtlety, but humans are super great at identifying

393
00:47:27,040 --> 00:47:34,800
the symbolic stuff, and also recognizing the modular structure from the CAN diagram.

394
00:47:35,600 --> 00:47:40,880
So, yeah, so the takeaway is that you can choose to be lazy, use the automatic mode,

395
00:47:40,880 --> 00:47:46,160
or you can choose to be more responsible and more involved using the manual mode.

396
00:47:49,520 --> 00:47:56,480
Yes, so maybe, yeah, maybe in the end, yeah, I will just finish this really quick,

397
00:47:57,920 --> 00:48:06,080
so people have asked why it looks like CANs can, you know, in terms of expressive power, CANs are

398
00:48:06,080 --> 00:48:13,520
just MLPs, are just like secretly just MLPs, so why do we need CANs? I want to argue that from a

399
00:48:13,520 --> 00:48:22,480
like a high level philosophical level, CANs and MLPs are somewhat different. CAN is like clock work,

400
00:48:23,760 --> 00:48:29,760
like in a clock, pieces are customized and have clear purposes corresponding to the learnable

401
00:48:29,760 --> 00:48:37,440
activation functions in CANs. So for a clock, it's easy to tear it apart, it's easy to tear

402
00:48:37,440 --> 00:48:45,280
the pieces apart and then reassemble the pieces back together to get the clock back, but MLP is

403
00:48:45,280 --> 00:48:52,160
like a house made of bricks, so in MLPs, the pieces are produced in the same standard way,

404
00:48:52,160 --> 00:48:58,960
like each neuron takes in some linear combinations and then do some do the same nonlinear transformation

405
00:49:00,480 --> 00:49:07,760
but it's difficult to tear, once you have a house built, it's difficult to tear apart the bricks

406
00:49:07,760 --> 00:49:18,160
and then reassemble them. So in summary, the source of complexity are different in CANs and in MLPs,

407
00:49:18,160 --> 00:49:24,560
the source of complexity in CANs come from the complexity of each individual object,

408
00:49:24,560 --> 00:49:31,200
like those 1D learnable functions, but because the functions are 1D, no matter how complicated

409
00:49:31,200 --> 00:49:38,880
they are, they're 1D and they have clear purposes in some sense, they are nevertheless, they are

410
00:49:38,880 --> 00:49:47,920
interpretable, but the complexity of CANs come from complicated interactions of individual parts,

411
00:49:47,920 --> 00:49:54,400
the individual parts are simple, but the connections between these individual parts

412
00:49:54,400 --> 00:50:01,680
are really complicated, I guess it's more like human brains, it's more like biology,

413
00:50:03,040 --> 00:50:12,560
yeah, I don't know, but CANs seem like more aligned with the philosophy of reductionism,

414
00:50:12,560 --> 00:50:19,040
where you hope that, where you expect that you can decompose a complicated object into a few

415
00:50:20,000 --> 00:50:27,600
like interpretable individual objects, well in MLPs, everything is connected,

416
00:50:28,400 --> 00:50:38,000
the reason why MLPs function is because they have this emergent behavior or collective behavior

417
00:50:38,000 --> 00:50:48,880
in some sense. Yeah, just some interesting question people ask, are CANs physical? So

418
00:50:49,760 --> 00:50:56,800
if we think of like the Feynman diagram as physical, then unfortunately, Feynman diagrams

419
00:50:56,800 --> 00:51:04,880
are sort of more like MLPs, because in Feynman diagrams, like on the edges, it's just like a

420
00:51:04,880 --> 00:51:14,960
free flow in space without anything interesting happen, but on the nodes where two particles

421
00:51:14,960 --> 00:51:20,080
or multiple particles collide, it's where interesting things happen, this is more aligned,

422
00:51:20,080 --> 00:51:27,520
this is probably more aligned with MLPs, but CANs is like interesting thing happens on the edges,

423
00:51:27,520 --> 00:51:36,400
but not on the nodes. And yeah, last question, people also ask are CANs biological, because

424
00:51:37,120 --> 00:51:45,280
people think that MLPs are inspired by the neurons in our brains, are there any biological

425
00:51:45,280 --> 00:51:57,040
analogy? And I don't know at first, but someone from Twitter wrote it that CANs actually is a

426
00:51:57,040 --> 00:52:08,400
bit analogous to the cells in retina, where each individual cell receive light, apply some

427
00:52:08,400 --> 00:52:14,080
nonlinear transformation to it before summing everything up, I'm not sure, you guys are experts,

428
00:52:14,080 --> 00:52:21,840
so please correct me if I'm wrong here. But the argument is that, well, maybe the mechanisms

429
00:52:21,840 --> 00:52:26,240
of CANs like your first applied nonlinear transformation, then summing everything up,

430
00:52:26,240 --> 00:52:35,280
is indeed biological, but I guess that's just for fun. That's just a minor justification why

431
00:52:36,960 --> 00:52:42,560
we need CANs, because in some sense, CANs are also biological.

432
00:52:47,040 --> 00:52:53,680
Yeah, so that's basically everything I would like to share, and I'm happy to chat more if you

433
00:52:53,680 --> 00:53:01,440
guys have questions. Super interesting, thank you. Questions?

434
00:53:04,240 --> 00:53:10,800
Well, for the last last example in retina, I have some questions. If the network,

435
00:53:10,800 --> 00:53:18,320
like CAN network, is deep enough, is it still matter if you say nonlinearities before or after

436
00:53:19,200 --> 00:53:21,040
the summation?

437
00:53:24,560 --> 00:53:32,560
Yeah, so I guess the key difference, well, yeah, so I guess the key difference is that

438
00:53:32,560 --> 00:53:41,200
in CANs, activation functions are learnable, so I guess, yeah, but whether to put activation

439
00:53:41,200 --> 00:53:50,560
functions on edges or on nodes, I don't think that might be the key difference,

440
00:53:50,560 --> 00:53:55,280
like the learnability of activation functions give you more flexibility.

441
00:53:57,520 --> 00:54:03,440
Yeah, when you talk about this, I was thinking about that CAN is decomposing

442
00:54:04,400 --> 00:54:13,760
different variables, input variables, like if you have x and y, then CAN could decompose it,

443
00:54:13,760 --> 00:54:21,760
because you would have a different combination of them, but like if you have a nested function,

444
00:54:21,760 --> 00:54:31,040
like sine x square, or exponential sine x square, then the CAN seems not able to decompose them,

445
00:54:31,040 --> 00:54:32,800
because they don't have these primitives.

446
00:54:33,600 --> 00:54:40,320
Yes, yes, yes, that's exactly correct. So CAN can only discover compositionality in the sense that

447
00:54:40,320 --> 00:54:48,000
all the 1D functions are boring to CANs. It can just be approximated with just one B-splice,

448
00:54:48,000 --> 00:54:58,080
it doesn't learn any compositionality for single variables. That might be one bug,

449
00:54:59,040 --> 00:55:09,440
if you will, for CANs, if you really want to figure out the symbolic formulas in the data set.

450
00:55:11,280 --> 00:55:21,840
But like for Professor Tomaso Poggio, the author of the 1989 paper who sentenced

451
00:55:22,800 --> 00:55:30,640
or who sentenced Theorem to jail, he wrote in his paper that all the 1D functions are boring.

452
00:55:33,600 --> 00:55:37,600
And what's interesting is this compositional sparsity when you are dealing with multiple

453
00:55:38,560 --> 00:55:43,600
variables, but I guess it depends on your goal. If your goal is just to learn the function efficiently,

454
00:55:43,600 --> 00:55:51,600
then it's fine. But if your goal is to really understand if it's sine of exponential, exponential

455
00:55:51,600 --> 00:55:57,520
of sine, then we probably need to think about ways to handle this.

456
00:56:00,080 --> 00:56:01,040
Yeah, thank you.

457
00:56:03,360 --> 00:56:09,760
Have you thought about combining CANs and MLPs, given their somewhat complementary nature,

458
00:56:09,760 --> 00:56:11,920
or despite their complementary nature?

459
00:56:12,880 --> 00:56:14,480
Yeah, that's a great question.

460
00:56:21,920 --> 00:56:28,640
So we have some primitives, like CANs can propose this CAN layer, which is a new primitive.

461
00:56:28,640 --> 00:56:36,640
And for MLPs, it has these linear layers and also the nonlinear activations, which are also

462
00:56:36,720 --> 00:56:47,120
primitive. I mean, these are like the building blocks. And I guess as long as they fit together,

463
00:56:50,240 --> 00:56:59,040
as long as they fit together, you can freely just combine them in ways that you want.

464
00:56:59,040 --> 00:57:05,280
But it's just that it's a bit hard to tell what's the advantage of combining. And

465
00:57:05,280 --> 00:57:11,760
because I guess there are many ways to integrate the two models and which way is the best. And I

466
00:57:11,760 --> 00:57:21,200
guess it's a case dependent question. It again depends on what's your application, what's your goal,

467
00:57:21,200 --> 00:57:22,240
something like that.

