start	end	text
0	20000	Okay. All right. So what I'm going to be talking about is how what natural language is, I think, a cognitive computation rather than a, say, purely linguistic one, which is probably why it's relevant to you guys.
20000	31000	And everybody, I think, would agree that it has to do with symbols, although I find in linguistics, you could find somebody who'll argue about anything.
31000	45000	And so whether we have internal representations, for example, at all. And then what I'm going to point out is that these symbols have been data compressed, and that's why language looks as complicated as it does.
45000	57000	Frankly, if it weren't for that, probably everything I'm going to say could have been discovered by Aristotle. And it's just that I think nobody noticed these things.
57000	70000	And then a implication of this would be, we'll get to the end and it allows computers, I think, to be to do cognition. And I'm a little worried that AI is going to stumble across this accident.
70000	86000	So first, I'd like to do two warm up exercises, very brief. One, I'm going to say a sentence and ask yourself, are you having any trouble at all with this? So he saw the saw that I saw.
86000	94000	Anybody have trouble with it? I think it's slid right down, right? So that's actually a little bit remarkable.
94000	110000	Now, the second warm up question, we're going to take warm up exercise, we're going to take 10 seconds. And I want you to think, what are the parts of a chair?
110000	120000	Okay, now, did anybody have in their list, glue, nuts, bolts or screws?
120000	130000	Okay, this will become relevant later. But I want to want you to do it before you get biased towards thinking about things like that.
130000	142000	So this is all part of what I think of as the Descartes-Cahal problem. You're starting out here, somehow you end up with this.
142000	155000	And I'm not going to talk about how Shakespeare could possibly do this. I'm going to talk about how he could tell you these words and you understand what he's thinking.
155000	162000	But in the course of that, we're going to be getting back to how does Shakespeare do this in the first place?
162000	171000	So as far as understanding language, I think it's a much more amazing algorithm, whatever it is we do, than people really appreciate.
171000	175000	And so I want to start off a little bit slowly and point out a few things.
175000	182000	One is, I think it's a computation executed by neurons, most of us biologists would agree on that.
182000	193000	And what you're doing is one person has this three-dimensional, instantaneous, simultaneous picture in his head about something about the way the world's going around.
193000	205000	But we're going to transfer that 3D picture to somebody else using a linear string of symbols that are ambiguous, because words usually all have multiple meanings.
205000	215000	And their arbitrary symbols, like the word cat has C-A-T, has no relation to a cat with two ears and a tail whatsoever.
215000	222000	And the only relationship between these symbols is adjacency and succession.
222000	230000	And somehow that doesn't trigger a bundle of associations in the listener's brain.
230000	236000	It triggers, it assembles the coherent picture of the world, the one that the speaker had.
236000	238000	So how do you pull that off?
238000	254000	Well, the solution in the 20th century, starting with Bar-Hell-L, actually, and also Chomsky, was that language has three main points, and then the rest of the talk is going to be replacing each of these with something else.
254000	267000	First, that language processes arbitrary symbols, unrelated to the thing they're talking about, and it processes them using syntactic rules that don't have anything to do with the meaning of those words.
267000	275000	Second, the function, the language structure is dyadic, function argument like a mathematics.
275000	280000	So there is a subject and a complement in a sentence.
280000	283000	And the complement has a verb and an object.
283000	287000	So each of these two parts, and it goes on in steadily more detail.
287000	295000	And third, that the language processes is determined completely by the words that are sitting on the page.
295000	299000	And I'm going to argue that none of these is true.
299000	307000	And once you see through it, then it becomes simple and regular.
307000	316000	And in fact, before the 20th century, there was a history that meaning did matter.
316000	323000	There was a history that language structure had made of three parts, not two parts.
323000	325000	It went by the wayside with Chomsky.
325000	332000	And there are a lot of clever things that come about from transformational grammar.
332000	343000	But I think the active effort in linguistic to suppress any other thinking has led to the problem.
343000	349000	So, first point, I'd say that language symbols are grounded, not arbitrary.
349000	361000	Grounded meaning that the symbol itself has some physical resemblance to the thing that it's talking about or that it's representing.
361000	363000	But I'll get to that part in a minute.
363000	366000	So an arbitrary symbol, so we have a sentence here.
366000	374000	And the standard view, but here's the arbitrary symbol cats, and it points to a thing out in the real world of cats.
374000	377000	My points to me and the dog points to the dog.
377000	383000	And there's something that we're pointing to with the word chases.
383000	388000	So that's thing out in the real world of the reference.
388000	392000	So there are a number of attractions to this way of looking at things.
392000	396000	So symbolic computations require a few things.
396000	398000	They require variables.
398000	400000	So the word dog can stand for any number of dogs.
400000	403000	It can get instantiated by different dogs.
403000	410000	There's a directional structure rather than just being an aggregate of stuff or a clustering.
410000	418000	There's some hierarchies, you know, like my dog is a unit that's a subunit of this whole thing.
418000	421000	And these symbols remain particular.
421000	424000	It's not like red and white giving pink.
424000	431000	The dog stays the dog and the cat stays the cat, which is a non trivial property of language.
431000	439000	And the thing that Chomsky introduced is basically the idea that you want somewhere or somehow that the dog's chasing the cat.
439000	443000	So somehow cats has to get here.
443000	445000	Oh, can you guys see my pointer?
445000	448000	Does that work? Okay, good. Thanks.
448000	453000	And so Chomsky do attention to that problem and solve that problem.
453000	460000	And actually much of what I'm going to say will wind up deriving a lot of the early transformational grammar,
460000	467000	but from a cognitive basis rather than a linguistic basis and with a whole lot less ad hoc stuff.
467000	475000	Now, there's a problem though, which is, well, actually, let me point out the problem.
475000	480000	There's nothing in these words that says how to assemble this picture.
480000	483000	So you've got these reference with how do you assemble it?
483000	494000	It gets a little better if you are willing to agree that we have in our heads internal representation, which are these little thought balloons here.
494000	506000	And that, which by the way is also something that people argue, you know, there's a whole school of thought says, oh, no, we don't have internal representations.
506000	514000	But still there's nothing here that says how to assemble these internal representations either.
514000	523000	So what happens if we take seriously the grounding that each one of these symbols is grounded somehow and something in the real world.
523000	530000	In other words, physically represents it, whatever it is that's in our heads that represents a dog.
530000	533000	Then you're in a little better shape.
533000	539000	Oh, well, this is what I was saying is that if you don't assemble them, you just get this cluster of images.
539000	543000	Oh, and this word ambiguity is significant.
543000	550000	So 12 word sentence people have shown that can have between 10 to the fifth and 10 to the 28 potential parses.
550000	556000	There's only 10 to the 60th atoms in the universe last time I saw a number.
556000	564000	So that's a problem that's generally not addressed in linguistics.
564000	577000	And, you know, in the 70s, 80s, 90s, the link places like IBM were trying to write programs that would use Chomsky and linguistics process language and they failed.
577000	585000	And the famous joke is that one of the managers at IBM said that every time he fires a linguist the recognition rate goes up.
585000	593000	So while he's definitely on to something, it's not the solution to the problem.
593000	597000	And certainly no hand is towards going on biological.
597000	604000	Now, if you take seriously the ground symbols, then you're doing computations on these representations.
604000	610000	And that allows you to do two things that hard at pointed out a long time ago are critical for cognition.
610000	616000	You have to be able to compare two representations and distinguish between them.
616000	619000	Or you compare them and look for similarities.
619000	627000	Let's you do discrimination on the one hand, let's you do category categorization and generalization on the other.
627000	632000	So that you have this idea of dog independent of any particular dog.
632000	635000	Without that, you can't do cognition.
635000	641000	There's nothing about the word cats down here that I can compute with the word dog.
641000	649000	That's going to let me tell you whether they have the same number of years can't be done the information not there.
649000	653000	But it is up here if you have grounded symbols.
653000	656000	So these are sometimes called icons.
656000	659000	The classical one would be say a pedestrian crossing sign.
659000	663000	It looks a little bit like a pedestrian crossing the street.
663000	670000	Okay, so there's some correspondence and now you can do computations on them.
670000	682000	So, oh, there is a problem, which is that we still remember we still want to not just point to these individual representations.
682000	685000	We want to assemble them into higher things.
685000	696000	So you want to assemble these two into my dog and you want to assemble the whole thing into something about cats and dogs chasing.
696000	700000	And you also have to get cats over here somehow.
700000	703000	So you've got to do a computation.
703000	714000	But if the representations are grounded, there's no guarantee that your computations on them are still going to be grounded and therefore give you a valid result.
714000	723000	So I would say the first thing I'm going to add is that, okay, whatever the language rules are, they also have to be grounded.
723000	730000	And so that grounded this is preserved when you're doing computations on the individual grounded symbols.
730000	743000	So the solution to that I would say got two parts once the grounded rule hypothesis, the speaker and the listener have operations on the arbitrary symbols of words.
743000	753000	That are grounded in the external world so that they're manipulating those words in the same way that the representations would be manipulated.
753000	764000	Or if you prefer, they actually are manipulating your representations, which is the more radical proposal, but that's, and I think that's what's going on, but I'm not going to demand that just yet.
764000	773000	So then you can see how you could ground an object like a dog by pointing to it. How do you ground a rule?
773000	779000	So then my second addition is what I would call the exotopic rule.
779000	784000	So biology, the tissue mimics stuff in the outside world.
784000	810000	So I think you guys probably know that the arrangement of pitches of notes or sound frequencies in the cochlea is in order of the frequency and the representation in your brain spatially is in that same order.
810000	820000	And you have the same sort of thing for the representation of your hand in your arm. And so those are called like tonotopic and somatotopic and so forth.
820000	833000	And you can see the same thing in genetics. You guys will appreciate this that the homeobox genes are expressed last time I checked in the same order that they are arranged in the chromosome.
833000	840000	And so biology has this habit of mimicking in the biology, whatever is going outside.
840000	849000	And so what I'm going to propose is, first of all, the way we see the world is there are things out there, there are relations in between them.
849000	860000	And so you have two entities and a relation between them. And so I'm going to propose, first of all, that that's the structure of the world, at least as the way people see it.
860000	876000	We don't see it as Hamiltonians. And second, we're going to, I'm going to propose that, okay, there's something in our biological tissue or in these rules for manipulating representations that mimics this entity relation entity structure.
876000	888000	So that's the exotopic rule hypothesis. So the operands are going to be these icons, these grounded symbols or hierarchies of them.
888000	898000	And the operations are going to build these hierarchies using only those three things and the result of doing it is still going to be one of those three things.
898000	902000	So then the question is, okay, so how far can you get with this.
902000	916000	So, let's start doing some data. So I assembled about 1000 different sentence fragments and sentences, not linguistics tends to use the Wall Street Journal because usually they were trying to sell to Wall Street people.
916000	931000	But basically newspapers are not the way we really speak. And so I took a variety of, for example, dictionaries intended for foreign speakers tend to have example sentences in them that are.
931000	935000	Real world sentences.
935000	943000	There are that took some books that are written by clear writers and so forth.
943000	956000	So let's take three of these. I'm going to go through it slowly semi obvious but I'm going to go through it for it slowly because once you get over this slide if you're okay with me and if you have objections here's the time to ask.
956000	961000	And you'll see what I'm going through if you get through this and the whole rest of the talk is working through the details.
961000	968000	So the word string, and these are all taken from this these corpuses of senses, experts predict improvements.
968000	983000	So the meaning of that is roughly experts predict improvements. So this is this capital is a standard kind of abbreviation in the field meaning intended to mean that here's the meaning of the word.
983000	988000	And I'm going to notate this parentheses around the entities.
988000	995000	And then the relations don't get parentheses. So you see here we have an entity a relation and an added.
995000	998000	Now, the green grassland.
998000	1011000	Well, the is pretty much what it means is that I'm going to tell you about something I already told you about, as opposed to our, where I'm going to tell you something new.
1011000	1019000	And green is pretty much like green us, you know, yes, representation of color, whatever that is.
1019000	1022000	And grassland is your representation of grassland.
1022000	1031000	But what I'm also saying is that the green is a component over a property of the grassland.
1031000	1032000	Okay.
1032000	1035000	So I'm going to abbreviate that with this symbol here.
1035000	1043000	You can think of it as a grassland and folding this component, this property of greenness.
1043000	1051000	And this green grassland is part of the things I already told you about.
1051000	1056000	So the has the component green grassland.
1056000	1060000	So it's a similar symbol but facing the other direction.
1060000	1071000	So the history to this symbol, which turns out to be wrong, but it's a nice symbol anyway, because it conveys this unfoldingness.
1071000	1080000	But we don't speak these things doing some in Chinese sometimes in some situations they do, but in English it out.
1080000	1082000	So we leave this out.
1082000	1085000	Now here's another one the person who threw it.
1085000	1098000	So person is, you know, some variable for a person to be instantiated, some particular person who is, you know, sort of referring to something.
1098000	1108000	But again, arbitrary through we have a representation of throwing it refers to some item that I've already talked about some known item.
1108000	1116000	The meaning here is also that this person is a component of this activity of somebody through something.
1116000	1119000	So gets another one of these things.
1119000	1123000	So again, we don't speak it though.
1123000	1130000	Okay, so if you're with me so far, then this is why I asked you about the parts of the chair.
1130000	1137000	We never think about these things we don't speak, just sort of all subconscious.
1137000	1149000	So this is data compression, which is, you know, if you're familiar with computers you know about data compression if you're a linguist you don't know about it and you don't want to know about it oddly, I find.
1149000	1156000	And so what this is saying is that when we speak in English, we're doing data compression.
1156000	1158000	Here's a few more examples.
1158000	1161000	And then I'll show you, well, I guess this is data.
1161000	1163000	He met me there.
1163000	1168000	Well, we don't say at, but that's kind of what you need in order to understand the sense.
1168000	1173000	They raise prices 8%, well, by 8%.
1173000	1175000	There's more of these.
1175000	1176000	He found her asleep.
1176000	1183000	Well, what he found was the condition of her having the property of being asleep.
1183000	1185000	And you can go on with this.
1185000	1188000	Well, indirect objects are important.
1188000	1200000	He gave mama the, he gave the situation of mama owning the stress.
1200000	1212000	This one, a linguist debate about, but there actually is a history pointed out to me by Steven Pinker, Steven Pinker actually, that there's strong arguments that ought to be treated this way.
1212000	1219000	And then he gave mama a function that has two arguments and so forth, which is the standard linguistics way.
1219000	1225000	So, and then one other thing I'll point out is that jaren's are really little miniature senses.
1225000	1229000	So cooking, well, it means that something cooks something.
1229000	1234000	And you just haven't said who the subject is and you haven't said who the object is.
1234000	1241000	But it's an E, well, E, so that's an E, you know, I can use the word that to talk about it.
1241000	1244000	It's an entity. Awesome.
1244000	1247000	Oh, I'll do one more.
1247000	1251000	Why do we have the word to in infinitives?
1251000	1252000	What's the point?
1252000	1265000	Well, if you, what's really going on here, Casey wants Casey to change to the situation of Casey throwing the ball.
1265000	1269000	Pro is a standard linguistics thing, meaning an unspoken subject.
1269000	1274000	So they're onto this idea that some things are not spoken.
1274000	1279000	But my point is that this is common. You see it everywhere.
1279000	1284000	Oh, the horse that raised past the barn fell. That's this classic garden path sense.
1284000	1293000	Because if you just say the horse raised past the barn, you're expecting it to be a past tense verb and you've got this picture of a horse racing past the barn.
1293000	1297000	Then you throw in the verb fell and now you're confused and you have to go read the sentence.
1297000	1303000	And what you left out was that was the horse that was raised past the barn.
1303000	1305000	Okay. So how common is this?
1305000	1310000	It's about 25% of English sentences are not spoken.
1310000	1317000	So these are various sources like the Longman's dictionary contemporary Englishes were non English speakers.
1317000	1325000	The Oxford dictionary, the English language has a whole section on examples, which are quite nice.
1325000	1336000	The Penn Tree Bank, which is a standard corpus linguistics corpus annotated for parts speech and so forth has lots of sentences and there are various other things.
1336000	1341000	And pretty consistently, it's about 25% of the things that are not spoken.
1341000	1349000	One of them are the system component relations, but sometimes the adverbs and so forth.
1349000	1356000	Now, once we agree to that so and so no objection so far. Great.
1356000	1360000	So what's the is there a pattern to this.
1360000	1368000	Well, so now the entities are in green the relations are in violet.
1368000	1376000	And it looks like continuously you're alternating entity relation entity relation.
1376000	1386000	Well, um, okay, that's nice. So things are beginning to look a little less complex than language is supposed to be.
1386000	1392000	Now, you can guide this or force the structure.
1392000	1400000	If you say that there's something like a reading frame so you guys know genetics linguists don't so you'll get it immediately.
1400000	1408000	So like a reading frame in genetics, you've got this pattern just waiting for the words to be dropped in.
1408000	1412000	So entity relation entity relation, etc, etc, etc.
1412000	1417000	So if we take the sentence like he saw the saw, I saw.
1417000	1422000	So we're going to drop the entity into this box.
1422000	1431000	Well, then the next box over is got to be is expecting a relation so it's going to take the verb sense of saw.
1431000	1434000	Then the next box is expecting an entity.
1434000	1446000	The goes in there just fine. Part of the dictionary definition of the is going to be that it's talking about something that's component something that you've already talked about.
1446000	1451000	So this are composed of comes along from the dictionary.
1451000	1453000	So now you're in this box.
1453000	1457000	So you get saw again but now it's expecting an entity.
1457000	1469000	And so this helps tremendously with the disambiguation of words and is a clue as to why we can get away with ambiguous words.
1469000	1472000	I'll show you some actual data on this point later.
1472000	1475000	You can keep on going.
1475000	1479000	He saw the saw Clive saw what Clive.
1479000	1482000	So you're here, but Clive is a proper now.
1482000	1484000	So it's got to go here.
1484000	1487000	Well, so now you got an empty space.
1487000	1492000	So now that's telling you that okay time to do some data decompression.
1492000	1511000	And we've got a table, which is not all that complicated, where you look up in the table what to insert here, given that you've got an entity here and an entity here and it just depends on things like whether these are proper nouns whether it's accountable now.
1512000	1517000	You know, it's basically, you know, it's a table to look up table you filled in.
1517000	1523000	If you had lots of computing time I'm sure one could optimize that table.
1523000	1529000	But I'll show you later how well this works. And so then now you're here and then now you've got a verb again.
1529000	1536000	And then you get to the period which is basically relation between sentences.
1536000	1538000	Okay, you got another empty space.
1538000	1543000	Okay, well, so you left an empty space here.
1543000	1559000	Well, actually, that's good, because you really want this saw over here to wind up over here, just like in transformational grammar, because Clive saw that saw.
1559000	1564000	And so I'll talk later about how to do that.
1564000	1572000	But you can see that this sequencing template is on simplifies everything.
1572000	1574000	Now I want to make two points.
1574000	1577000	So it simplifies the disambiguation data question.
1577000	1587000	But if the word is ambiguous, the dictionary alone and the word alone can't help you with it.
1587000	1594000	And I tell you where to place the word in this series of boxes.
1594000	1598000	So this template has to be exogenous to the sentence.
1598000	1601000	It's not contained in the words in the page.
1601000	1606000	And I think there's no other place for it to be than up here.
1606000	1616000	And I think this is the thing that humans have that lets us process language.
1616000	1627000	The whole literature in linguistics arguing that some of this structure comes from a previous ability to synchronize muscles in order to throw things.
1627000	1629000	And I think that's probably on the right track.
1629000	1632000	You already had some kind of sequencing track.
1632000	1636000	And now you've learned to put words into it.
1636000	1646000	The other thing is that if you're inserting gaps like this, well, that also can't be part of the words on the page.
1646000	1651000	It has to come from an exogenous sequencing track.
1651000	1659000	And for those of you who are, you know, like Dr. Christon, you think about Bayesian sentence processing.
1659000	1662000	I would say that, okay, here's the backbone.
1662000	1665000	Yes, you can add sophistication onto it.
1665000	1669000	And I'll show you later how much sophistication you...
1669000	1676000	How much you get from this alone and how much additional sophistication you need.
1676000	1678000	Okay.
1678000	1682000	Now, so far we just got a string of e's, r's, e's, r's, and so on.
1682000	1684000	End of due relation sentence.
1684000	1686000	Now we've got to build them up.
1686000	1688000	So now I'll tell you how to do that.
1688000	1689000	So this is the next thing.
1689000	1691000	So we're on number four.
1691000	1698000	The first three are me saying just the three things linguists are doing in the 20th century are wrong.
1698000	1702000	And now the next set of things are going to be things we can do now that they can't.
1702000	1706000	So how do you build this stuff?
1706000	1711000	Well, so here's another sentence just from the corpus.
1711000	1716000	Saracens built a network of highways to serve blah, blah, blah, blah.
1716000	1719000	So Saracens built a network.
1719000	1722000	So this all goes as I've been telling you.
1722000	1725000	Now you get to all.
1725000	1733000	What you want to happen is a network to be an entity now.
1733000	1743000	And so we'll say for the moment that some place around the word of there's an operator that's building things.
1743000	1747000	And what it's going to do is going to build a network.
1747000	1753000	Now, it could do it again and build Saracens build a network.
1753000	1762000	Oh, and if the next word were bricks, that would probably be the right thing to do because you'd have always an advert.
1762000	1765000	But in this case, it's not the right thing to do.
1765000	1770000	So you've got a preposition here, and you're just going to build this.
1770000	1775000	It's all called prepositional attachment, and it's largely an unsolved problem in logistics.
1775000	1782000	And for us, what we do in the computer program, I'll show you later, is we do it both ways.
1782000	1791000	And what will happen if you guess wrong sooner or later, that parse dies because there'll be some other conflict that it creates.
1791000	1796000	Now we keep on going of highways, you get to two.
1796000	1807000	And now, if we take the adverbial sense, you would say, okay, now the adverb is going to build this whole thing.
1807000	1809000	And then it does get to do it again.
1809000	1812000	So you've got, you built a network of highway.
1812000	1814000	And now it does it again.
1814000	1817000	And you get certain build a network of highways.
1817000	1818000	Why?
1818000	1823000	Well, to advance serve the practical needs of commerce and to do more parsing.
1823000	1829000	Now, if it had been, they build a network of highways to nowhere.
1829000	1832000	This would be a preposition.
1832000	1838000	You wouldn't be, you would build highways to nowhere.
1838000	1842000	Well, two wouldn't down here would someplace.
1842000	1844000	And so you would not be doing this.
1844000	1847000	That's another fork you could take.
1847000	1853000	But in the present case situation is this one.
1853000	1854000	Okay, great.
1854000	1861000	So actually all these operators are sitting right at the same place as the relations.
1861000	1865000	What, what are the rules for these building operators?
1865000	1870000	Well, turns out that the building operators are the relations.
1870000	1871000	They're not just sitting there.
1871000	1873000	They are the relations.
1873000	1877000	And we sat down to figure out what those relations.
1877000	1879000	What those rules are.
1879000	1881000	Because we figured, okay, this is going to be complicated.
1881000	1885000	You know, in such and such a circumstance that can only build this far and other
1885000	1890000	circumstances that can build twice in a row.
1890000	1895000	Or maybe it only builds from here to here and somebody else builds from there.
1895000	1898000	Turns out it wasn't complicated.
1898000	1904000	And so those of you who are engineers or computer guys, or even if you remember
1904000	1911000	algebra operator precedence hierarchies, you know, you multiply first and then
1911000	1917000	you do addition and subtraction or you do the exponents first or then you do the
1917000	1920000	multiplying and dividing and then you do the addition subtraction.
1920000	1925000	So for given operator somewhere in this precedence hierarchy.
1925000	1927000	Something just turns out.
1927000	1932000	I was amazed that you can just write an operator precedence hierarchy.
1932000	1934000	So that's just another table.
1934000	1941000	And you can sort of see things like this guy who comes after the law are pretty
1941000	1942000	low down.
1942000	1946000	You almost always build over him into something bigger.
1946000	1950000	Whereas things like periods and question marks.
1951000	1952000	Well, you don't.
1952000	1956000	And then other guys are in between.
1956000	1958000	So.
1958000	1962000	Then the next thing I'll show you is these Chomsky and.
1969000	1971000	I had a clock.
1971000	1975000	I don't know where I put it.
1975000	1978000	So the transformations.
1983000	1984000	I did what I never do.
1984000	1987000	I put it in my pocket because I thought that would be a good idea.
1989000	1990000	Okay.
1990000	1994000	Now, so can you do the Chomsky and transformations?
1994000	2002000	Well, they have a series of rules for how to get cats into here.
2002000	2007000	They're pretty good rule, but they're a little bit ad hoc sometimes.
2007000	2014000	But here there's this next amazing thing on the basis of two words flanking.
2014000	2017000	An empty space when we did the decompression.
2017000	2020000	Remember, we inserted these various relations.
2020000	2026000	Well, it turns out that for some reason that I really don't understand.
2026000	2031000	These same relationships also act at a higher level.
2031000	2034000	So we can now say cats.
2034000	2037000	Oh, okay.
2037000	2043000	We said that they are a component of this whole thing.
2043000	2049000	Well, they are a component of that.
2049000	2051000	So cats go into that.
2051000	2055000	Well, that is a component of this whole thing.
2055000	2056000	And guess what?
2056000	2059000	There's even empty space for it.
2059000	2063000	And so that's called successive, what is it?
2063000	2064000	I forget what it is.
2064000	2066000	Anyway, it's standard linguistics.
2066000	2069000	The point is you do have to do these things in steps.
2069000	2075000	And here all the algebraic instructions have already been inserted for you.
2075000	2080000	So poof, all you have to do is the computation.
2080000	2086000	And similar things.
2086000	2088000	Well, I won't go into that.
2088000	2093000	Similar rules can assign, well, actually I may have mentioned it.
2093000	2094000	Yeah.
2094000	2098000	So where you start and where you end are governed by some rules.
2098000	2103000	And it's called C command in linguistics.
2103000	2105000	There's a complete analog here.
2105000	2108000	You just have to rewrite it in terms of entities and relations.
2108000	2113000	And same kinds of rules for deciding on the relationship between pronouns and reference.
2113000	2118000	If you violate those rules, the sentence isn't grammatical.
2118000	2124000	So that's how you tell a grammatical sentence from an ungrammatical sentence.
2124000	2126000	Now, this only English.
2126000	2131000	So here's Lakota, which is the Sioux language.
2131000	2137000	So where we would say John found that letter under the bed.
2138000	2144000	In Sioux, it is John letter that bed the under found.
2144000	2145000	Okay.
2145000	2149000	That can be written as EE rel.
2149000	2157000	There's a phenomenon in linguistics where you can classify languages according to different structures.
2157000	2164000	And this, you know, E rel E happens to be one set of languages.
2164000	2166000	E rel is another.
2166000	2174000	And this is essentially like reverse Polish and, you know, mathematics or computer programming.
2174000	2176000	But you can do it.
2176000	2180000	And it's the same general idea.
2180000	2186000	So let me show you a little bit of what happens with actual parsing.
2186000	2194000	So Steve Senth, who was at Yale at the time, he's now at Marine Biological Laboratory in Woods Hall.
2194000	2204000	He's the guy who wrote the original voxel view program that processes images, three dimensional images in terms of boxes that are pixels.
2204000	2212000	So he wrote a program that's only eight megabytes.
2212000	2214000	Half of that is the dictionary.
2214000	2224000	So it's only four megabytes of processor, as opposed to these large language models, which are terabytes of RAM.
2224000	2227000	So here's just an example that's sort of a hard one.
2227000	2229000	You probably have to read it twice.
2229000	2233000	The man who knew the man who Aaron knew knew Clive called.
2233000	2238000	And there's actually two meanings that you can take out of that.
2238000	2244000	The man who Aaron knew already, that person knew Clive called.
2244000	2250000	Or the man who Aaron knew that person knew Clive called.
2250000	2254000	And this program spits that out.
2254000	2259000	It also is able to pull out of this who did what to who.
2259000	2263000	So in this first one, the man knew something.
2263000	2267000	Clive called somebody and Aaron knew the man.
2267000	2269000	Whereas this one is different.
2269000	2275000	The man called somebody Aaron knew something and the man knew Clive.
2275000	2283000	And you may be familiar with pretty print, which is supposed to be a way of showing hierarchical structure.
2283000	2287000	Steve came up with this other notation, which I think is much clearer.
2287000	2292000	If you just look at this, you can say, well, okay, somebody knew something.
2292000	2294000	Well, what did that person know?
2294000	2297000	He knew the Clive called somebody.
2297000	2301000	Well, over here, who knew something?
2301000	2308000	Well, oh, these guys are all just variations on those left and right component of symbol.
2308000	2315000	So the man, if you look down at this level, the man is a component of something.
2315000	2318000	It's a component of who Aaron knew.
2318000	2321000	Who is a component of Aaron knew?
2321000	2324000	Because Aaron knew this guy who.
2324000	2329000	And the man is a component of all that and it's a component of the who, et cetera.
2329000	2332000	So it's all built in there.
2332000	2339000	I can, you know, run a program in real time for you if you want.
2339000	2340000	It's probably simple.
2340000	2345000	It's just to finish the talk and we can play with that later if you want to run any sentences.
2345000	2350000	So, um, oh, and there's also a cost function.
2350000	2351000	That's what this is.
2351000	2356000	So that if you have to start skipping boxes, you can do it, but it costs you.
2356000	2363000	And so that's a way of telling whether you're doing something that's legal, but unlikely.
2363000	2367000	And sooner or later, those things wind up having a huge cost.
2367000	2369000	And you know, that's not right.
2369000	2372000	Um, now it doesn't work.
2372000	2380000	So I'm guessing you guys don't worry about linguistics much, but, um, there's some standard metrics.
2380000	2386000	Like recall precision for this thing or 97% coverage and consistency of 77%.
2386000	2390000	Basically, consistency is how many are correct parses do you get?
2390000	2397000	Well, let me just say that when people do this, they use things like the pen tree bank corpus,
2397000	2404000	which doesn't even bother with compound nominals like doghouse or prepositional attachments.
2404000	2407000	You know, like I was showing you the two different ways of doing off.
2407000	2409000	It doesn't even annotate for that.
2409000	2411000	I've annotated for that.
2411000	2414000	And it's still 77%.
2414000	2417000	Um, if I didn't annotate for that, probably 97%.
2417000	2421000	And then there's across brackets, which has to do with, well,
2422000	2428000	how many subunits or is the computer trying to get you to say overlap each other when they shouldn't.
2428000	2430000	And it's rather small.
2430000	2435000	And it only happens in really long sentences where people get confused.
2435000	2437000	So show you some data.
2437000	2448000	So I told you there were lots of, because of word ambiguity and, um, the resulting parse ambiguity because parses build things differently than entities.
2449000	2452000	There's a lot of alternative parses.
2452000	2455000	So on the X axis, you have the number of words.
2455000	2457000	So let's take out 15 word sentence.
2457000	2463000	And so the sentences I tried, they have about 10 to the seventh possible parses.
2463000	2466000	Oh, I forgot to show you.
2466000	2467000	Back here.
2467000	2476000	It's saying that for the man who Aaron knew new collide called it's saying that there are 3,800 potential parses and it's pulled out to
2476000	2481000	both of them are, um, you know, correct.
2481000	2483000	So back here.
2483000	2491000	So you've got 10 to the seventh potential parses and you've dropped that down to about.
2491000	2494000	Let's see.
2494000	2496000	10 to the so one.
2496000	2503000	Yeah, so this is dropped it down to about 10 parses here.
2503000	2506000	Um, so that's pretty impressive.
2506000	2517000	So what you're doing is removing with just this sequencing track and these cognitive rules.
2517000	2528000	You're removing orders of magnitude of the ambiguity in the words and the alternative ways of parsing the sense and understanding the sense.
2528000	2531000	So now you're only down to here.
2531000	2547000	And so that any knowledge you have to have about, well, what's the guy probably trying to say or any of the Bayesian stuff only has to solve another like 10 fold of this.
2547000	2555000	This ratio here turns out to be a reduction of 2.74 fold per word.
2555000	2557000	So three fold.
2557000	2563000	But that three, if you start raising it to the 15th power, it gets to be a big number.
2563000	2568000	Then as I stared at that, I was thinking, Oh, 2.74 that number looks kind of familiar.
2568000	2574000	And then I realized that this reduction here is one over e to the 1.008.
2574000	2576000	In other words, it's one over e.
2576000	2584000	And so the interpretation of that, there's a kind of like a target theory interpretation of that statistics.
2584000	2602000	You're trying to hit a target with radiation or something. Basically, there are enough constraints that in this, in the cognitive rules, I told you about that you are just going to so castically eliminate sentences that are wrong.
2602000	2606000	And so you're reducing things down to one over e.
2606000	2625000	And the, then what's left, and that is what happens when you have independent events, though it was treating words as more or less independent.
2625000	2628000	And the collisions are coming from the sequencing track rules.
2628000	2639000	Then everything else you need to get from this 10 down to a single interpretation is either the meaning or things like us.
2639000	2652000	Subject verb case agreement or subject for number agreement or cases, things like that that are relationships between a couple of words are not independent events.
2652000	2659000	So that's what the rest of linguistics is doing is getting you from here down to here.
2659000	2676000	Now, I hope at this point I've convinced you that, gee, language is maybe a whole lot more regular than we thought, and, and having a regular algorithm can process way more of it than we thought.
2676000	2693000	You could then wonder, oh, and that what is processing is the, or building, are these representations rather than words per se.
2693000	2697000	And then now you begin. So let's wonder about AI.
2697000	2714000	And what have large language models discovered, all they would have to do is discover or teach themselves to make a sequencing track like this, and a couple of rules, and a sequencing track, what is that it's just a series of probability.
2714000	2727000	Well, if I'm, if I am an entity word now, the probability is 1.0 that the word that the next word is going to be a relation and the probability of that is blah, blah.
2727000	2737000	So, you know, it's sort of Bayesian, but it's, you know, as somebody told me, well, rules are just probabilities of 1.0.
2737000	2744000	So a statistical learning machine could come up with this track.
2744000	2760000	And then what are the implications of that? Well, let me show you one more thing about how this program works. So we're trying to build a sentence, build the parts into higher and higher hierarchies.
2760000	2767000	So what we're doing, I'm going to show you that basically we have a shift register for grounded symbols.
2767000	2780000	So you have cats, cats that because the dictionary definition of that is that has these two component things because cat is going to go into that, and that is going to go into something else.
2780000	2788000	You've got my dogs, blah, blah, blah, you keep building here and now Chase is a relation.
2788000	2791000	These guys don't build much.
2791000	2797000	Chase's will build things. And what you want it to do is build my dog.
2797000	2812000	And this is, it does the shift register thing. It's going to build, put by this relate the composed of a dog, all in the same box, because that grouping is now an entity.
2812000	2817000	And it shifts these guys over to.
2817000	2821000	And then keeps going. It gets to R.
2821000	2832000	R is going to build. And now it's going to shift the parentheses and chases all into this box and put a new set of parentheses around it.
2832000	2841000	Oh, these things are all just parentheses are just different shape parentheses to make it easy to follow.
2841000	2854000	And then we're going to keep on going because our goes all the way to the beginning of the sentence that's going to shift it over again, lump all these things together, shift over again lump these things all together.
2854000	2865000	And then all you need biologically is instead of parentheses you need something that is going to group.
2865000	2873000	These subgroups into a bigger group without just averaging them like red and white into pink.
2873000	2889000	Well, if then you think about whether representations could be doing this, whether your brain could be doing this on representations, in addition to just doing it on words.
2889000	2896000	These words are really just representations to there's no words in your head. There's just like neural spikes. Right.
2896000	2901000	So these guys so you've already got representations getting dropped in here.
2901000	2914000	What happens. So it doesn't seem to me a very big step to be dropping these representations into the same sequencing track and doing the processing.
2914000	2926000	So if a large language model happened to event the sequencing time, it could also support processing of sensory representations instead of word representations.
2926000	2930000	And I've just shown you that the rules are the same.
2930000	2935000	And then if that then let's assemble complex representations.
2935000	2944000	And it does so in a way that you can now define a thing, which is the one remaining part, which I have an idea about how I could do it.
2944000	2948000	Because what we think about our thing.
2948000	2961000	But it can now retains all the information to do discrimination between them and do generalizations so that fulfills harness definition of what it takes to do cognition.
2961000	2974000	So I'm quite concerned that his large language model model are accidentally going to stumble across a way to be doing cognition.
2974000	2978000	So that is, I believe it. Yep.
2978000	2986000	And open to any questions, or if you want to see the parts or an action that's fine.
2986000	2988000	And I will.
