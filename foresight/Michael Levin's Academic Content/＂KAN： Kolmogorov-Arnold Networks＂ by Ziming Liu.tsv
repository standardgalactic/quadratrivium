start	end	text
0	10000	Yeah. So in case you don't know me, I'm Zemin Liu. I'm now a fourth year PhD student at MIT.
10800	18640	Welcome, Professor Max Tagmark. My research interests the center around AI plus science,
18640	26080	so it can go both ways. Either you develop better AIs for science, like AI scientists,
26080	34080	or you use inspirations or like tools in science to enhance the improvement of AI.
35040	41440	So like this work is like both ways. We first try to use some ideas from math and see if we can
41440	49200	develop new AI tools and see if we can, whether these new tools can give something back to to
49200	56000	science. Yeah. So this is about a recent work called The Comagra of Arnold Networks.
61520	70320	Today's AI is built upon this math theorem called the universal approximation theorem,
72320	76560	which basically says that a high-dimensional function, you can decompose a high-dimensional
76560	84800	function into a linear combination of some nonlinear features with this nonlinear function sigma,
86160	93280	which is basically just this two-layer network known as the multilayer perceptron. But you
93280	99200	can make it deeper. That's why it got the name multilayer. But we are asking this question,
100160	106080	are there any alternative theorems we can use? We can leverage these new theorems. Well, not
106080	111040	necessarily new in the mathematical sense, but like in the AI world to build another AI
111760	117920	skyscraper based on new building blocks. So here we are examining this theorem called The
117920	124560	Comagra of Arnold Representation Theorem. First question, what is the KA representation theorem?
125120	131520	It says, again, like you given a high-dimensional function, you can write it down as a finite
131520	139120	composition of one-dimensional continuous functions and just the summing operation. So more
139120	146000	concretely, you can write down an n-dimensional function in this form, where you have the outer
146000	155840	functions capital phi q and the inner functions phi qp. They are both just one-dimensional functions,
155840	163360	and they are finite. Like the number of these functions is depending on the number of inputs
163360	170720	variables n here. So what's nice about this theorem is that it tells us that the only true
170720	177840	multivariate functions is the sum. Like you can express, say, multiplication with just the summing
177840	187680	operation plus some 1D activation functions. So that sounds like really great news for
188640	192320	approximating high-dimensional functions, especially for machine learning,
192320	197680	because in high dimensions, to learn high-dimensional functions, we know we have curse of dimensionality.
198400	203680	But this theorem seems to tell us that you can decompose high-dimensional functions into
203680	208400	one-dimensional functions, which do not suffer from curse of dimensionality, which are great.
209920	222960	But immediately, there is this plot twist. Back in 1989, professor Jarosi and Poggio wrote this
222960	231200	paper examining the relevance of Comagra's theorem in the context of learning. And
232000	239440	they conclude that we review Comagra's theorem and show that it is irrelevant in the context of
239440	249280	networks for learning. So this paper basically sentenced the Ka theorem to death, at least
249280	259360	sentenced to jail for like 30 years. So is that the end of the story? Well, we want to tell you
259360	267440	that that's not the end of the story. And there's again a plot twist. So let's try to see what they
267440	276960	say in their paper, why they sentenced the theorem to jail. So their argument was that the theorem
276960	286400	fails to be true when the inner functions are required to be smooth. Basically, the original
286400	291760	theorem says that you can decompose a high-dimensional continuous function into one-dimensional
291760	299200	functions, but it doesn't guarantee you to decompose it into one-dimensional functions.
299840	306240	But to learn something, like with gradient descent, you would need the functions to be
306240	313120	smooth. Only being continuous is not enough. Even with continuous functions, you can still have
313120	320960	some really pathological behavior. You don't have the smoothness constraint. So that's basically
320960	326560	the argument, because the theory does not tell you anything about the smoothness of the functions.
326560	333040	But in practice, only smooth functions are feasible in practice.
336640	346080	So one interesting word they use is that a stable and usable exact representations
346080	354240	seems hopeless. So that's like the attacking point. That's the angle we're taking. Maybe
354240	359920	we do not need exact representations. Sometimes approximate representations might suffice.
360640	370640	Yeah. So their paper was theoretically sound, was sound theoretically, but I'm trained as a
370640	378160	physics student. So my level of rigor is lower than mathematicians. So me and also Max, we tend
378160	387680	to have lower bar of being rigorous. So we have this naive optimism. So firstly, maybe like
387680	392160	empirically, maybe we don't care about exact representations. Sometimes approximate ones
392160	400160	may suffice. As long as it has some levels of explainability or interpretability, that's fine.
400720	406480	And secondly, a common wisdom in modern deep learning is just stacking more layers to get
406480	412160	different networks. They are basically arguing against the two-layer commograph network. But
412160	417920	what about we build deeper networks? Maybe for deeper networks, even under the smooth
417920	424560	constraint, you can win the expressive power back. Lastly, just the can-do spirit. We cannot say
424560	430880	something doesn't work before really building it with the state-of-the-art techniques. They
430880	439040	have the negative claim back like 30 years ago. Back then, they didn't even have the
439120	445760	back propagation. Like at least it was not popularized back then. So we want to take
445760	453600	everything we have in the modern area of deep learning and see how far we can go. So that's
453600	463280	the mindset. So just with the can-do spirit, we propose or more properly rediscover or we
463280	474240	contextualize the idea of commograph networks in today's deep learning world. Yeah. So here's
474240	479600	the overview. First, I will introduce the math foundation. Well, I already briefly mentioned
479600	487120	it. I won't dig deeper into it, but just I will emphasize again like the mathematical beauty of
487120	497440	it. And then I will talk about the properties of cans. Why? And in what scenarios cans are more
497440	506000	accurate and interpretable than current deep learning models? Yeah. So first, the math foundation,
506000	512880	I already covered this part, but I want to emphasize again that the theorem
513200	518960	looks really nice. That allows you to decompose a high-dimensional functions into one-dimensional
518960	524640	functions. And after the decomposition is done, your only need to just your only job would be
524640	533120	just approximating the 1D functions. So that's the idea of that's the idea of the commograph
533120	541600	networks. Like decomposition first and then learn the 1D functions. Well, yeah. So this
541600	547600	representation looks a bit complicated. You'll see that there are this huge, this big summation
547600	555120	symbol and you have two layers of composition. And this will be complicated. But don't worry
555120	562400	about it. It's just equivalent to this two-layer network. Let's suppose we have two inputs,
562400	569600	x1 and x2, and we have the outputs at the top here. So the representation in the original
569600	578400	theorem is basically just that you have five hidden neurons in the middle. And to get the
578400	588720	activations in each hidden neuron, you basically apply a 1D, possibly nonlinear function to x1
588720	598560	and x2, and sum up these two nonlinear activations to get the activations at the node. And
599600	607360	in the second layer, it's similar that you apply some nonlinear function to the hidden activations,
607360	614080	summing everything up at the output node, and that's how you get the output. So the computation
614080	627200	graph is super clear with just this diagram. And this might remind you a lot of, this looks
627200	631360	just like a multi-layer perceptron, the fully connected networks, where everything just fully
631360	637520	connected. But instead of having activation functions on the nodes, now we are having
637520	642960	activation functions on the edges. And on the nodes, you simply just have the summation operation,
644800	652240	which is really simple. Yeah, just to make it more elegant and beautiful, we can,
652800	657040	or more intuitive, we can basically, because like 1D functions, we can basically just visualize them
657040	663840	with 1D curves, with the x and x as the input and y as the output. So now can network is basically,
664480	669680	you can picture it as this. And by staring at it, you can, you can have some idea what's happening
669680	678640	inside. Yeah, so I mentioned this is just a, so the theorem, so the representation is equivalent
678640	685840	to a two-layer network. But can we go deeper? The answer is yes. Algorithmically speaking,
685840	693440	because it's just a stack of two layers, which, from which we can abstract a notion called the
693440	699760	can layer. So the original two-layer network is basically a stack of two, two can layers.
699760	705360	And for each can layer, it's basically just taking some number of inputs and outputs, some number
705440	711200	of outputs, and in between is fully connected. And on each edge, you have the active, you have the
711200	716800	some nonlinear, learnable nonlinear activation function. And in the end, and in the outputs,
716800	724880	you, you, you summing up the incoming activations. That's how a can layer works. And you can simply
724880	731760	stack more and more can layers to get deeper and deeper cans. This is just a three-layer can,
732720	739040	like, like the first layer, we're taking two, output three. The second layer, input two, output
739040	746720	three. And the last layer, you input three, output one. So this is the, this is a three-layer network,
746720	753120	which approximates a scalar function in two dimensions. But obviously, you can easily
753120	760000	extend it to arbitrary dimension, like arbitrary input, arbitrary output, arbitrary width, arbitrary
760800	765760	depth. So, so you have all the flexibility to choose the size of the network.
769120	776960	Yeah, one, the first question Professor Poggio asked me when I presented this network to him,
777920	783200	he asked, is that why do you need this deep networks? Because the original theorem told you
783200	791120	that you only need the two-layer constructions. And here's just a quick answer that I, I can give
791120	799840	you an example. So please look at this symbolic formula. And if you examine it, you would immediately
799840	806560	realize that you would need at least three compositions to do this, to construct this formula.
807120	811680	You need the, you need the squared function, you need the sine function, you need the exponential
811680	818240	function. They're like, because they're, because it's the compositional structure, they're in different
818240	826640	layers. So you would at least, at least need three layers to, to, to learn this formula. And
826640	832160	indeed, if you just use two-layer network, the activation functions becomes really oscillatory,
832160	839360	becomes really pathological. And the performance is bad, and also the interpretability is bad.
839360	846480	But in the right, I show that a three-layer network, a three-layer can train on this,
846480	851680	train on this dataset. And after training, you would immediately see the learned activation
851680	855600	functions. In the first layer, you got the squared, in the second layer, you got the sine,
855600	861520	and the last layer, you got the exponential. Well, well, you may think of, you may think this is some
861520	869200	other functions, maybe just some local quadratic, but yeah. But you can, but you can, like,
869200	874160	do the template matching with the candidate functions and figure out which one fits the best.
878400	883840	Yeah, so, so I, I said that this activation functions are learnable. How do we make them
883840	890080	learnable? Because they're functions. And, and, and, and the common wisdom is that we need to make,
890080	896160	we need to parametrize the things to be learned so that we can use gradient descent to learn,
898000	904960	to learn this stuff. So the idea is that we parametrize a 1D function with, with B splines.
904960	913520	So B splines is basically some piecewise, some, some local piecewise polynomial functions.
914080	922720	So, so here I showed that there are some local B spline bases. And the way we construct the activation
922720	929200	functions is by linearly combining this, this, this B spline functions. And the only learnable
929200	936720	parameters are just the linear coefficients of, of, of this local basis. And what's nice about
936720	943040	this formulation is that we, we inherit the advantage of B splines. We can easily switch
943120	949520	between fine grains, fine grain grids and coarse grain grids. If you want something to be more
949520	955200	accurate, you can, you can choose the mesh to be more fine grain. If you want the model to be
955200	960480	smaller, so you can have a faster inference, you can, you can, you can choose a more coarse-grained
960480	971360	model. Yeah, that's basically the idea of cans. And we can compare MLPs and cans side by side,
971360	975840	because they do share some similarities, but also share some difference, but also have some
975840	983440	differences. So MLPs are inspired by the universal approximation theorem cans, again,
983440	989360	inspired by the Camargo Ravano representation theorem. The network looks a bit similar in the
989360	996160	sense that they're both fully connected. But their dual, they're different, but they're dual in the
996160	1001840	sense that MLPs have fixed activation functions on nodes. Well, you can make them trainable, but
1002640	1008720	but they're on nodes for sure. And in MLPs, we have learnable weights, learnable linear weights on
1008720	1017920	edges. By contrast, cans have learnable activation functions on edges, while cans have this simple
1017920	1024880	linear summation operation on nodes. So, so, so in this sense, cans does not separate the linear
1024880	1031920	part and the long inner part as MLPs do, but it integrates both the linear part and long inner
1031920	1037600	part altogether into the can layer. And the can network is simply just the stack of the can layers.
1041520	1050000	Yeah, so in both cases, you are free to stack the model to become deeper and deeper because
1050000	1054160	you have the basic notion of a layer, you just stack more layers to get the deeper networks.
1055360	1065680	Yeah, so that's the basic, that's the basic ideas of cans. And now I want to elaborate more
1068800	1074000	like why do we care about, why do we care about this? What are the advantages that cans can bring
1074000	1083440	to us, but other black box models do not bring to us? So yeah, so the first property is the
1083440	1089600	scaling behavior of cans. As I mentioned before, the idea of cans is decomposing a
1089600	1094320	high dimensional functions into one dimensional functions. So that looks like really promising
1096400	1103280	that it can get us, it can get us out of the curse of dimensionality. Let's suppose we are
1103920	1110480	trying to approximate a d dimensional functions. And I suppose the function has no structure at all.
1110480	1117440	So then we need to, we need to have a hypercube and have a uniform grid on the hypercube.
1118240	1124720	Let's suppose we have 10 grid, 10 anchor points along each dimension, then we will need 10 to the
1124720	1129760	power of d number of anchor points in the d dimensional hypercube. So that's exponentially
1130640	1137120	expensive. So if you do the classical approximation theory, you would notice that the approximation
1137120	1146160	error would decay as a parallel, as a parallel, as a function of the number of input dimensionality.
1146160	1152560	And it's one, and the exponent is one over d, meaning that you got exponentially
1153680	1158720	like slower when you have more and more, when you have more and more dimensions, like if you need
1158720	1165200	10 points in one d, you would need 100 points in 2d, you will need 1000 points in 3d and so on.
1167200	1174160	But if the function has some structure, like if it has the chromograph unknown representation,
1174160	1180560	then we can decompose it into a bunch of 1d functions. And then our job would just be
1180560	1187440	approximating 1d functions. So now effectively d becomes one. So you got a really, you got the
1187440	1194480	fastest possible scaling laws. But the, but the caveats, immediately the caveat is that we,
1194560	1203600	the assumption is that we, like the function has a smooth chromograph, a smooth finite size
1203600	1209680	chromograph unknown representation. All of this, you know, all of these objectives,
1209680	1215600	objectives like smooth or finite size are just, are just practical conditions for a real, for,
1216640	1222880	for a network which we have access in practice that can really learn the network. We want it to
1222880	1228400	be smooth because we parametrize it with b-splice, which are smooth. We want them to be finite size
1228400	1235520	because, of course, you know, we cannot initialize, we cannot deal with an infinite size neural network.
1238720	1244720	So, yeah, so, so, so we just did some sandwich check on, on some symbolic formulas.
1245120	1251280	So, yeah, so symbolic formulas are like white, are like what we used in, in science. So that's why we
1251280	1258640	test them first. So let's see. So the red dashed line is the theoretical prediction. Here we are
1258640	1272000	using cubic spline. So k is k3. And the scaling exponent is k plus one equals four. And the
1272880	1281920	curve, yeah, so the thick blue line is for the can network. And you see that almost like the,
1281920	1288240	like empirical results for the can network almost agreed with, almost agrees with the
1289360	1295120	theoretical prediction, although sometimes performed slightly worse. Or in this case,
1295120	1300160	in the second to last case, there's a hundred dimensional case. And it performs much worse
1300160	1305440	than a theoretical prediction because of the, the, the, the, because the dimension is just too
1305440	1312080	high and the network can get, can get stuck at some local minima or whatever. But, but nevertheless,
1312080	1319680	it's still output from MLPs, which you see up in the upright corner here, like the case really
1319680	1327040	slow. But, but the cans, but cans at least can output form MLPs to a great margin,
1327040	1334720	although still not saturating the theoretical prediction. But still the scaling law that can
1334720	1343040	shows looks, looks promising that it's, it seems to not fully beats the curse of dimensionality,
1343040	1345680	but at least partially beat the curse of dimensionality.
1349120	1355920	Well, yeah, yeah. So, yeah, just to play the devil's advocate here, you may immediately notice
1355920	1362240	that I'm on purpose just deliberately using this symbolic formulas. You, you might be wondering,
1362240	1369920	well, maybe, maybe the functions we care. We encounter a lot in nature may not be symbolic,
1369920	1375840	they might be some weird, you know, like, at least for special functions, they, they, they,
1375840	1382160	they are like infinite series, which are hard to be represented with just the finite network. So,
1382160	1387200	what are, what, what, so, so, so, so what's the cans performance in that scenario?
1388560	1394880	So, yeah, so we just tried some special functions, which we know for most of them, they do not have
1394880	1403360	analytical formulas. And indeed, we see that the scaling laws of cans do not saturate the
1403360	1408720	theoretical prediction, meaning that probably you cannot decompose like a high dimensional
1408720	1415600	functions into just a one D functions. But still, our goal here is to outcompetes MLPs.
1416240	1421680	And, and, and it's a feature, not a bug, like not all the functions can be decomposed into
1421680	1431600	smooth finite representations of this K representation. They may admit non smooth finite size or
1431600	1438880	smooth infinite size. But in, but, but neither case is, is like the, is what's accessible in
1438880	1447200	practice. So, yeah, so, so here we show that in most cases, we can achieve this minus two
1447200	1454320	scaling law, which means that the can network, well, well, sorry. So here, this special functions
1454400	1462000	are all just two dimensional. So this, so these, according to the spline theory, it would predict
1462000	1467920	two dimensional, it would predicts a two dimension, like, like, like a scaling exponent to be two,
1467920	1473600	which agrees with the can results. But in some, but in some case, we can, we can still got the
1473600	1481680	minus four scaling law. And the reason is that actually, this is secretly, like, like, although
1481680	1487440	we call, we call it a special function, spherical harmonics are not that special in the sense that
1487440	1494320	they're still decomposable. So, so you can get the minus four scaling. But also in other, in other
1494320	1499280	cases, you've got some worse behavior, like you've got the minus one scaling, which means that the
1499280	1506880	can is underperforming even out on the performing compared to like the spline theory. But what's
1506880	1514080	interesting is that in this case, MLPs are even more underperforming than cans. So, so this may
1514080	1520480	tell us that maybe for low dimensional problems, neural networks are not that necessary. And even
1520480	1526880	the spline theory, like the spline approximation can outcompete the neural network. So, so it's
1526880	1534880	something to be think about. It's something good to keep in mind. Because neural networks just have
1534880	1539760	too many degrees of freedom and may have optimization issue.
1542480	1550000	Yeah. So, so beyond function approximation, we can go on to solving partial differential equations.
1550000	1556080	So in the setup of physics in form learning, basically, we're trying to solve, we're basically
1556080	1568640	trying to represent the solution of a PDE with MLP or with a can network. So, so the only difference
1568640	1573520	from the previous results from the previous experiment is that we're just we're using the physics
1573520	1579920	informed loss rather than the regression loss. So the optimization becomes more complicated,
1579920	1586480	but still it's just approximating some function. Yeah. So, so we still see that we with cans,
1586480	1593920	we can get this optimal scaling law. Well, with MLPs, you see, while with MLPs, you see that,
1593920	1597840	well, it has the skill at first, but it plateaus really fast and then
1598720	1603440	does not improve when you have more parameters beyond 10,000.
1603440	1612480	Um, besides being more accurate, we can also gain some insight what the network is learning.
1612480	1618720	So, so, yes, yes. So for this example, we can, for this PDE example, we can actually visualize
1619280	1625360	the can network like this. And immediately you can see that there's some like sine waves and
1625360	1631840	there's some linear functions. And you can even do symbolic regression to it, like,
1632080	1637280	like, like our software provides you a way to do this, you can, you can do symbolic regression
1637280	1644160	to it. And after you do this, you can do some further training. And you, and you can even extract
1644160	1650080	out the symbolic formula, which gives you like a loss down to machine precision. This is something
1650800	1658480	that that that standard neural networks would not give you because of because they usually
1658480	1663760	cannot convert a neural network into a symbolic formula very easily, but with cans, you can easily
1663760	1674720	do that. Another property of cans is that it has this property of continual learning, at least
1674720	1682880	in 1D. Yeah, so people have, people have found that, you know, this claim might be invalid for
1682880	1690080	high dimensions. So take my word with a grain of salt. But at least for 1D, the case is like,
1691040	1698400	we have, we want to approximate this 1D functions with five peaks. But instead of feeding auto data
1698400	1705920	at once to the network, we're feeding each time we just feed one peak to the network. And we do
1705920	1713120	the sequential learning at each stage, the network is just, is just fed with just one peak.
1713120	1720000	And with cans, and with cans, because we're using the local B-splines, so when it sees the new data,
1720000	1726640	it does not update the parameters correspond to the old data. So it has this, it can't get rid of
1726640	1735520	this catastrophic forgetting, like when new data are coming in, the can is able to memorize the
1735520	1742480	old data and still do quite well on the old data. But this is not true for MLPs. Like when you're,
1742480	1748080	when you're fed, when the MLPs are fed with the new data, it catastrophically, they catastrophically
1748080	1754240	forget about the old data. Because in MLPs, you usually have this global activation functions
1754240	1760720	like the silo functions or Reilu functions. So whenever you make adjustments locally, it will
1760720	1766480	affect, you know, the predictions far away. So that's the reason why MLPs would not keep
1768240	1772080	would, would catastrophically forget about the old data.
1774880	1782080	Yeah, so, so, so that's the first part about accuracy. The second part is about interpretability.
1782080	1788720	You might already have some sense because we are able to visualize the network as a diagram. So,
1788880	1793600	the hope is that you can just stare at a diagram and gain some insight of what's happening inside
1793600	1805040	the neural network. Yes, yeah, so here we have some, some, some toy examples of, yeah, for example,
1805040	1812960	how do cans do the multiplication computation. So we have x and y as the input and the x times y
1812960	1819280	as the output. So when we train the can network, we also have some, something similar to L1
1819280	1825280	regularization to sparsify the network. So we can extract out the minimal network to do the task.
1826560	1831520	So in the multiplication case, we see that only two neurons are active in the end,
1831520	1837840	and we can read off the symbolic formulas of how, and get a sense of how it does the computation.
1837840	1847280	Well, I marked the symbolic formulas here. It may take a while. It may take a while, but I,
1847280	1854960	well, the point is that it basically just some squared functions and, and the way the can network
1854960	1862720	learns to compute this multiplication is by leveraging some, some squared equality here.
1863680	1869440	And the second example, the can is tasked with the division task,
1870400	1881360	where we input two positive numbers, x and y, and can is asked to predict the x divided by y.
1881360	1887840	And because here x and y are both positive, the can network learns to first take the
1887840	1892960	logarithm transformation and then take the subtracted two logarithm and then
1893920	1901040	transform the same back via the exponential, exponentiation. So, so, so that's really cute.
1901040	1907120	Like, like one example of the commograph theorem is that you can basically do the multiplication
1907120	1913360	of positive numbers or the division of positive numbers in the logarithmic scale, because that's,
1913360	1919280	because the division or multiplication in the logarithmic scale, it transferred to,
1919280	1924560	would translate to like addition and subtraction in, in the logarithmic scale.
1928240	1933280	Yeah, so, so, so, so this examples are really simple. You might be wondering what about a more
1933280	1940480	complicated formula, then, then it might be very complicated to decode what the cans have learned.
1941440	1946880	So, I would want to argue that this might be a feature, not a bug, or you can call it a bug,
1946880	1950400	but I can, but I won't call it a feature in the sense that,
1952720	1956640	sorry, I didn't show the network here, but let's suppose we're doing this formula here,
1956640	1962880	like u plus v divided by 1 plus uv. So, if you're familiar with relativity, you are special relativity
1962960	1968320	on, you are, you are, you are, you are realized that this is the velocity, relativistic velocity
1968320	1976880	addition. So, at first, I thought that I need the five-layer can to fix this function, because
1976880	1982880	you would need multiplication, which would give, which would consume two layers, you would,
1983920	1988880	two additions, consume another two layers, and also you, you, you would need the division.
1988880	1994560	So, so that's in total five layers, but it turned out you can only, you can, you can just use a two-layer
1994560	2000160	can to fix this function perfectly well. And in the end, I realized that this is just,
2002080	2008080	this is just a rapidity trick known in special relativity, where you first do the arc-tange
2008080	2014080	transformation to u and v separately, sum the thing to, to rapidity up, and then you do the
2014080	2020640	tange transformation back to, to get this formula. So, in this sense, it's rediscovering the, it's,
2020640	2029600	it's rediscovering the rapidity trick, known, well-known in, well-known in the special relativity.
2029600	2035600	And in some cases, I indeed find that the network finds some more compressed,
2035600	2041280	compact representation than I would expect. That's good news in the sense that's the,
2041760	2048080	the, in the sense that the network is discovering something more compact. So, the representation
2048080	2053840	is more powerful than I have expected. But the bad news is that sometimes the, the interpretation
2053840	2062080	can be subtle, can be a bit more complicated. But, but I mean, it's, it can be a, it can be a
2062080	2071520	feature, but depending on your view. Yeah, so, so, so, so this is one paragraph I take from
2072240	2078320	the paper. This is criticized, this has been criticized a lot in the social media, but I,
2079120	2086880	but I find this analogy really interesting. So, I still want to highlight this part.
2087840	2093760	So, so to me, I think can is like a, it's sort of like language model or, or like language or
2093760	2099680	even like language for AI plus science. The reason why language models are so transformative and
2099680	2105200	powerful is because they are useful to anyone who can speak natural language. But the, but the
2105200	2112720	language of science is functions or more advanced mathematical objects build on functions because
2112800	2120000	cans are composed of functions which are 1D, so they are interpretable. So, when a human user stares
2120000	2126720	at a can, it's like communicating it with, it's like communicating it with the, using the language
2126720	2133680	of functions. So, to elaborate more and to make it more entertaining, let's suppose we are like
2133680	2140080	my advisor Max is communicating with the can network. Here, I picture the can network as
2140160	2146080	a trisolarian from the three-body problem. I'm not sure if guys have watched it, but basically
2146080	2153360	the trisolarians, their brains are transparent, so they cannot hide any secrets from others. So,
2153360	2163600	they are totally transparent. So, so Max went up, give, give the can, give the can a dataset.
2163760	2170000	Max was like, here is my dataset. It contains the mystery of the universe. I want you,
2170800	2178480	the can network, to figure out, to figure out the structure of the datasets and, and the can
2179360	2187200	initialize the can network like this. So, here's the brain of the can network initially. And Max,
2187440	2195040	given the dataset, Max wants to train the can network, to train the brain. And after training,
2195040	2201280	you can get this sparse network, which you start to see some structure. But still, it's a bit,
2201280	2208320	there's still some residual connections, which looks quite, which quite annoying. So, Max asked the
2208320	2216080	can network to prune the redundant connections. And after pruning, you got, after pruning,
2216160	2225920	you got this, you got this sub network, which is responsible for the computation. And Max further
2225920	2231600	asked the network to the can network to symbolify it, because it looks like, it looks like in the
2231600	2237200	bottom left, it looks like just a sign function. And in the bottom right, this looks just like a
2237200	2242800	parabola. And this just looks like an exponential. So, maybe you can symbolify it to gain some more
2242800	2253120	insight. So, yes, so the can network said, yes, I can symbolify it. And you can. And now the dataset
2254240	2261680	goes all the way down to the symbolic formula. But, but we can imagine another conversation Max
2261680	2267920	would have with an MLP. So, Max went up and give the dataset to MLP, and want the MLP to figure
2267920	2277840	out the symbolic formula in it. And like before, the MLP initialized the brain, looked like something
2277840	2284400	like this, really messy. After training, Max asked MLP to train the brain, but even after
2284400	2293280	training, the connection still looks really messy. And MLPs were like, and the MLP is like, I really
2293280	2298880	trained it, but the loss is pretty low. But it's just that the connections are still very complicated.
2301120	2309360	Now Max got confused. Like, what's going on? What's going on with your brain? And now MLP is like,
2309360	2316000	it's just that your humans are too stupid to understand my computations. You cannot say,
2316000	2322000	I'm wrong, simply because you cannot understand me. So, Max now got really pissed off and turned
2322000	2331920	back to CANS. So, those are just some imaginary stories I made up with the symphatic example.
2331920	2341040	And that symphatic example is really simple. But I want to show that we can really use CANS
2341040	2347440	as a collaborator in scientific research. And CANS can give us some non-trivial results.
2348080	2355280	CANS can give us some new discoveries. So, the first example is,
2357840	2365520	yeah, so this example was used in a DeepMind Nature paper three years ago, where they used MLP
2365520	2375200	to discover a relationship in a NOT dataset. So, each NOT has some invariance. Basically,
2375200	2379840	each NOT is associated some numbers. And these numbers, they have some relations. And we want
2379840	2388160	to dig out the relations among these variables. So, what the DeepMind people did was they used
2388160	2393760	the train and MLP and used the attribution methods, basically take the gradient with respect to these
2393760	2400960	input variables, and use that as a score to attribute these features. And then rank these
2400960	2405440	features to get a sense of which features are more important than other features. And they
2405440	2414480	identified three important features. That's the only thing that's automated in their framework.
2414480	2420800	And then the human scientist came in and tried to come up with a symbolic formula for it.
2421600	2426160	So, we're asking this question, can we discover, have we discovered these results
2426160	2432160	with more automation, with less, you know, efforts, and probably even discovering something new
2432160	2442320	that the DeepMind paper were missing? So, first, we are able to discover the three important variables
2442320	2449920	with the CAN network, with much more intuition and automation. So, their network, they used a
2450240	2456480	three-layer, sorry, they used a five-layer MLP, and each hidden layer has 300 neurons. So,
2456480	2462080	that's really hard to interpret. That's why they used the feature attribution. But we find that
2462080	2470480	surprisingly, we only needed one hidden layer and one hidden neuron, the CAN network, to do the task,
2470480	2479520	as well as their five-layer, like, a million-parameter MLPs. And with this, we can also clearly see
2480000	2488960	the activations, the importance now basically becomes, you can basically understand the importance of
2488960	2498960	these variables with the L1 norm of these activation functions. So, that's also how we visualize
2499520	2504960	the connections. So, you can basically read off from this diagram that the strong connections
2504960	2509920	are the important variables, while the weaker or even nearly transparent
2510960	2513600	connections, meaning that irrelevant variables.
2517360	2524320	Yeah, we can also discover symbolic formulas, as I said before, because the CAN network decomposes
2524320	2529760	high-dimensional functions into 1D, and then we can just do template matching in 1D
2529760	2537680	to see what each 1D functions represent symbolic formulas, and then compose all these 1D functions
2538560	2540640	back to get these high-dimensional functions.
2543760	2552720	Something beyond their paper we discovered is that their setup is a supervised learning setup.
2552720	2559120	Basically, they need to partition the variables into inputs and outputs, and they use the inputs to
2559680	2565680	predict at the outputs. But in some cases, we do not know how to partition the inputs and outputs,
2565680	2577040	like all the variables, they are treated equally. So, we want to develop this unsupervised setup
2578080	2585920	where all the variables serve as inputs. But we use some notion of contrastive learning to classify
2585920	2592640	whether some given input is a real knot or a fake knot, or that might be too technical.
2596320	2603680	The result is that we are able to discover more relations beyond the relation they've
2603680	2609680	discovered, because they manually partition one variable as the output, so they can only discover
2610640	2616320	the relations that involve that variable. But here, we are learning it in an unsupervised way,
2616320	2623040	so we can learn more than just one relation. We also discovered the relation between these
2623040	2633760	three variables and between these two variables. Unfortunately, or fortunately, these relations
2634080	2640720	are already known in the literature of knot theory. So, the unfortunate part is that we did not
2640720	2649040	discover anything new with our framework, but notice our network is just very preliminary,
2649040	2654720	it's just a one layer, it's just a one layer if we ignore about the classifier in the second layer.
2654720	2661280	So, hypothetically, we can make it deeper to get more complicated relations. But the
2661280	2667040	fortunate part is that it verifies that's what we discovered with the network is something,
2668160	2672800	is not bullshit, it's something that makes sense that people already know in the literature.
2676880	2682960	Yeah, we also did this physics example, specifically Anderson localization.
2683920	2692320	I don't want to bore you with the technical detail, but again, the goal here is to try to figure out
2693040	2700080	the symbolic formula of the phase transition boundary. In Anderson localization, we have the
2700080	2706240	localization phase and the extended phase, and there is a phase boundary, and we want to extract
2706240	2713680	out the phase boundary, especially the symbolic formula of the phase boundary,
2714240	2719040	if there exists a symbolic formula for it from the road data.
2722880	2728720	Yeah, so for this slide, I don't want to go to the detail, but the point I want to make with
2728720	2736560	this slide is that the CAN network has, just like cars, you have manual mode, you have manual mode,
2736560	2741840	you have automatic mode, like if you're lazy, you can just delegate everything to CANS, and CANS
2741840	2748320	will return you a symbolic formula fully automated, but that might not be correct. That might not
2748320	2755520	be what you want. You can do that, it can give you reasonable accuracy, but may not be fully
2755520	2761040	interpretable, but if you want to have some controllability over the CAN network,
2761040	2764880	where you want to be more involved, you want to have some intervention, you can still do that.
2765520	2772480	You can choose to, you can use the manual mode, where you just handpicked some activation functions,
2772480	2778480	like some functions, obviously they're just quadratic, linear, you can set them to be
2779440	2786320	exactly the linear or quadratic, and then you retrain the other activation functions,
2786320	2794000	and you see, and after retraining, you will see what those activation functions would change to
2794000	2800000	different, would change to different form, and then this gives you, again, gives you some insight,
2801200	2807920	like give you better evidence what the next guess you would want to make, and this is like the
2807920	2814320	iterative process, like it's sort of like you are arguing or you are debating with the CAN network,
2814320	2822400	like the CAN network is give you some, or you can say debating or collaborating,
2822400	2827040	just like how we interact with a human collaborator, sometimes we debate, sometimes we collaborate,
2827040	2834480	but you seem to look at the same thing from different angles, like the CAN network is really
2834480	2840080	great at decomposing high dimensional functions into 1D functions, but those 1D functions may
2840080	2847040	not be perfect, and there might be some actual subtlety, but humans are super great at identifying
2847040	2854800	the symbolic stuff, and also recognizing the modular structure from the CAN diagram.
2855600	2860880	So, yeah, so the takeaway is that you can choose to be lazy, use the automatic mode,
2860880	2866160	or you can choose to be more responsible and more involved using the manual mode.
2869520	2876480	Yes, so maybe, yeah, maybe in the end, yeah, I will just finish this really quick,
2877920	2886080	so people have asked why it looks like CANs can, you know, in terms of expressive power, CANs are
2886080	2893520	just MLPs, are just like secretly just MLPs, so why do we need CANs? I want to argue that from a
2893520	2902480	like a high level philosophical level, CANs and MLPs are somewhat different. CAN is like clock work,
2903760	2909760	like in a clock, pieces are customized and have clear purposes corresponding to the learnable
2909760	2917440	activation functions in CANs. So for a clock, it's easy to tear it apart, it's easy to tear
2917440	2925280	the pieces apart and then reassemble the pieces back together to get the clock back, but MLP is
2925280	2932160	like a house made of bricks, so in MLPs, the pieces are produced in the same standard way,
2932160	2938960	like each neuron takes in some linear combinations and then do some do the same nonlinear transformation
2940480	2947760	but it's difficult to tear, once you have a house built, it's difficult to tear apart the bricks
2947760	2958160	and then reassemble them. So in summary, the source of complexity are different in CANs and in MLPs,
2958160	2964560	the source of complexity in CANs come from the complexity of each individual object,
2964560	2971200	like those 1D learnable functions, but because the functions are 1D, no matter how complicated
2971200	2978880	they are, they're 1D and they have clear purposes in some sense, they are nevertheless, they are
2978880	2987920	interpretable, but the complexity of CANs come from complicated interactions of individual parts,
2987920	2994400	the individual parts are simple, but the connections between these individual parts
2994400	3001680	are really complicated, I guess it's more like human brains, it's more like biology,
3003040	3012560	yeah, I don't know, but CANs seem like more aligned with the philosophy of reductionism,
3012560	3019040	where you hope that, where you expect that you can decompose a complicated object into a few
3020000	3027600	like interpretable individual objects, well in MLPs, everything is connected,
3028400	3038000	the reason why MLPs function is because they have this emergent behavior or collective behavior
3038000	3048880	in some sense. Yeah, just some interesting question people ask, are CANs physical? So
3049760	3056800	if we think of like the Feynman diagram as physical, then unfortunately, Feynman diagrams
3056800	3064880	are sort of more like MLPs, because in Feynman diagrams, like on the edges, it's just like a
3064880	3074960	free flow in space without anything interesting happen, but on the nodes where two particles
3074960	3080080	or multiple particles collide, it's where interesting things happen, this is more aligned,
3080080	3087520	this is probably more aligned with MLPs, but CANs is like interesting thing happens on the edges,
3087520	3096400	but not on the nodes. And yeah, last question, people also ask are CANs biological, because
3097120	3105280	people think that MLPs are inspired by the neurons in our brains, are there any biological
3105280	3117040	analogy? And I don't know at first, but someone from Twitter wrote it that CANs actually is a
3117040	3128400	bit analogous to the cells in retina, where each individual cell receive light, apply some
3128400	3134080	nonlinear transformation to it before summing everything up, I'm not sure, you guys are experts,
3134080	3141840	so please correct me if I'm wrong here. But the argument is that, well, maybe the mechanisms
3141840	3146240	of CANs like your first applied nonlinear transformation, then summing everything up,
3146240	3155280	is indeed biological, but I guess that's just for fun. That's just a minor justification why
3156960	3162560	we need CANs, because in some sense, CANs are also biological.
3167040	3173680	Yeah, so that's basically everything I would like to share, and I'm happy to chat more if you
3173680	3181440	guys have questions. Super interesting, thank you. Questions?
3184240	3190800	Well, for the last last example in retina, I have some questions. If the network,
3190800	3198320	like CAN network, is deep enough, is it still matter if you say nonlinearities before or after
3199200	3201040	the summation?
3204560	3212560	Yeah, so I guess the key difference, well, yeah, so I guess the key difference is that
3212560	3221200	in CANs, activation functions are learnable, so I guess, yeah, but whether to put activation
3221200	3230560	functions on edges or on nodes, I don't think that might be the key difference,
3230560	3235280	like the learnability of activation functions give you more flexibility.
3237520	3243440	Yeah, when you talk about this, I was thinking about that CAN is decomposing
3244400	3253760	different variables, input variables, like if you have x and y, then CAN could decompose it,
3253760	3261760	because you would have a different combination of them, but like if you have a nested function,
3261760	3271040	like sine x square, or exponential sine x square, then the CAN seems not able to decompose them,
3271040	3272800	because they don't have these primitives.
3273600	3280320	Yes, yes, yes, that's exactly correct. So CAN can only discover compositionality in the sense that
3280320	3288000	all the 1D functions are boring to CANs. It can just be approximated with just one B-splice,
3288000	3298080	it doesn't learn any compositionality for single variables. That might be one bug,
3299040	3309440	if you will, for CANs, if you really want to figure out the symbolic formulas in the data set.
3311280	3321840	But like for Professor Tomaso Poggio, the author of the 1989 paper who sentenced
3322800	3330640	or who sentenced Theorem to jail, he wrote in his paper that all the 1D functions are boring.
3333600	3337600	And what's interesting is this compositional sparsity when you are dealing with multiple
3338560	3343600	variables, but I guess it depends on your goal. If your goal is just to learn the function efficiently,
3343600	3351600	then it's fine. But if your goal is to really understand if it's sine of exponential, exponential
3351600	3357520	of sine, then we probably need to think about ways to handle this.
3360080	3361040	Yeah, thank you.
3363360	3369760	Have you thought about combining CANs and MLPs, given their somewhat complementary nature,
3369760	3371920	or despite their complementary nature?
3372880	3374480	Yeah, that's a great question.
3381920	3388640	So we have some primitives, like CANs can propose this CAN layer, which is a new primitive.
3388640	3396640	And for MLPs, it has these linear layers and also the nonlinear activations, which are also
3396720	3407120	primitive. I mean, these are like the building blocks. And I guess as long as they fit together,
3410240	3419040	as long as they fit together, you can freely just combine them in ways that you want.
3419040	3425280	But it's just that it's a bit hard to tell what's the advantage of combining. And
3425280	3431760	because I guess there are many ways to integrate the two models and which way is the best. And I
3431760	3441200	guess it's a case dependent question. It again depends on what's your application, what's your goal,
3441200	3442240	something like that.
