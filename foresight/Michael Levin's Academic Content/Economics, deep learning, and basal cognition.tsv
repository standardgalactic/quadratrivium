start	end	text
0	1680	I appreciate that. Yeah, I'm really looking forward to this.
1680	6120	I think you guys both had some great ideas recently, and I'm looking forward to it.
6280	10080	Can we just start and maybe, let's see, Ben can go first and then David,
10080	13360	just say a couple of words about who you are and what your interests are.
14400	16800	I'm a former stockbroker with the background in economics,
16800	19760	and I'm currently studying the economics of collective intelligence,
19760	22560	looking to put together an understanding of how people get along
22560	24400	and accomplish things that no individual can.
25280	32320	Cool. I am by training a stockbroker engineer, but I've been into AI
32320	35200	ever since the high school reading age of intelligent machines,
36160	40080	and then just kept dipping in and out of the AI field and kind of being like,
40080	44960	it's not ready, it's not ready. After college, I co-founded one of the first AGI startups
44960	48640	with like Shane Legg, one of my co-founders was our first employee.
48640	53760	So like back in 2000s, and then went to work at Google and Facebook,
53760	58960	and then Asanda, mostly either working on ML, but most of my time has been building
58960	62720	like large-scale distributed systems. So really coming at this more from
62720	68880	an engineering background, but also about three years ago, I left my job and
68880	73360	dove back into AI and came across Carl Friston's work.
73360	75600	And I think that's when it kind of all clicked for me.
75600	80800	And I was like, wow, we have GPUs now, we have a theory of intelligence
80800	87200	that I feel like basically covers. It's like, I don't think there's any more secret sauce.
87200	91920	It's like, we kind of, the formulas from active inference look a lot like formulas
91920	97680	from traditional reinforcement learning. We have the hardware now, so I just like
97680	100800	got really excited about it and have been working on it.
100800	106720	So that's like my software and engineering side. I also co-founded Plurality Institute
106720	112800	with Glenn Wild, who's like an economist, and that's more on the human collective intelligence side.
113520	118880	Essentially, we're trying to create a new field, academic field called Plurality Studies that
118880	123520	focuses on human, scaling human cooperation and collaboration, essentially collective
123520	128720	intelligence as an academic field. And we've been bringing together researchers from like
128720	135280	political science, computer science, economics, peace building. I mean, it's such an interdisciplinary
135280	141680	field of like, how do we help humans cooperate and using technology, and now especially AI to
142960	149280	bridge people. So that's kind of my other project. But my main research focus is,
149840	156960	and I can share a little bit of like the research that I'm working on. And I would love, you know,
156960	161840	help and collaboration. And also would love to hear how you guys are thinking about this. And if I
161840	167040	can help, because I think this is like the most exciting thing that's happening in the world right
167040	174640	now. Cool. Yeah, please go for it. Yeah. Yes. So I will say yes. So Glenn Wild is an interesting name
174640	180240	because I think he's got a very important idea, which is that we should be using markets to
180240	184080	solve more problems because there's basically a perspective in economics that says,
184960	189120	anytime you've got a problem, there's a market that can solve that problem. And people just
189120	195200	haven't been creative enough historically about what kind of markets you can use. Like we're just
195200	201040	very used to like trading like standard property rights and things. And there's a lot of other
201040	205440	ways you could conceive of doing things. I like the fact that Glenn Wild is pushing that I'd be
205440	209360	interested in. So I don't actually know what he's doing. So actually, I'm just curious about that.
209360	212720	I don't actually know what he does with AI. So can you just tell me a little bit about what he does
212720	218720	with the Plurality Institute? Yeah, well, the Plurality Institute is not is more of the academic
218720	224640	like collective and human collective intelligence of which AI is like a tool. So for example,
225680	232080	and I mean markets are one collective intelligence system. But then there are other ones like even
232640	237920	like common sections on forums are collective intelligence mechanism where humans come together
237920	242960	to synchronize on epistemology like on things that they know and believe and being able to use
242960	250000	LLMs to say automatically deescalate flame wars in common sections and drag people to consensus
250000	256320	is like an example. In terms of the more economics things, so using quadratic funding for like public
256320	263200	goods funding is something he has been writing about creating identity like trust networks to
263200	269520	create identity like social identity so that you can use that as infrastructure for all these other
270800	278400	like market mechanisms is another one. There's someone that's working on vector prices. So rather
278400	284560	than there being a single price signal, there's a whole basket collection of currencies that you
284560	290320	hold and that people can specify their own values for different currencies. And then the way the
290320	295760	trading works is basically when you're trading with people that are aligned with you on values,
295760	302320	you automatically get discounts because they value your currencies more than they value some.
302320	308720	So there's like a whole bunch of them. He just has a book out open source isn't like another like
308720	314160	how large open source projects are incentivized and how collaborations occur without like a
314160	320560	corporate structure. He has a book out now called plurality with Audrey Tong who is another
321200	328080	inspiring person. So I would check that out and that's like is going to cover a lot of different
328080	334160	aspects of collect like plurality and collective intelligence. Cool. Is a lot of that just on
334160	340000	the plurality website? The plurality instant that instituted website, we like post events and talks
340000	344400	and we have the link to the plurality book by the plurality book is like much more detailed.
345520	351680	I will check that out. So maybe what I can do then is just kind of give a broad overview of what I
351680	355680	think the basics of economics of collective intelligence are and we can see how that relates
355680	362400	to our work. So the way I see it is that collective intelligence is all about getting people to
362400	369600	communicate in honest ways in honest and relevant ways such that everybody forms a shared model
369680	373680	and because everyone's behaving with that same shared model everyone ends up behaving as if
373680	378480	they're all following the same you know sort of commands if you will like a dictator or a virtual
378480	384000	governor and everyone ends up nicely coordinated to achieve that kind of signaling system. You need
384000	390320	something that's analogous to a price system and what that allows you to do is it's principles
390320	396720	very similar to what Mike has talked about in some of his work where you get a way of getting an
396720	402400	entity to see questions about other people as questions about itself and so like information
402400	406240	gets transmitted in such a way where like when someone else is in trouble you perceive that as
406240	411120	stress to yourself when something benefits someone else you perceive that as benefit to yourself
411840	416160	which we see very easily in the price system like if someone needs more food they'll you know have
416160	420800	a demand for food that'll raise the price of food so that stress gets transmitted to everyone else
420800	425360	which uses to buy less food or similarly if you help someone get food by supplying some food that
425360	431600	they like their benefit becomes your benefit via the revenue you make some money so that very
431600	437840	simple system ends up outlining really just the basic functionality of a collective intelligence
437840	442160	I think that any collective intelligence probably follows basic price like principles
442160	446320	where you need some system where people aren't incentivized to communicate honestly with each
446320	453120	other and such that everyone's signals gets condensed into like your own personal situation
453120	458000	so just by optimizing for yourself you end up inadvertently taking care of having one else
458000	462480	so are things like that when you're looking at like AI and technology and stuff like I don't
462480	467200	know what goes on inside a computer it's all magic to me is that is that what's going on with like the
467200	472480	transistors do they all have their own like individual model and they end up like communicating to
472480	481520	form a shared model you know I mean I guess it's not level everything is doing the longest right
481520	487920	everything is like mutually reducing free energy and you can feed every part of the
487920	493840	subsystem as an agent so in some in some level yes but I think what you probably mean is more
493840	503040	directly in a computational way and like typically what we do now is we train neural networks which
503040	511600	are a big pile of math but but essentially it's a thing that like learns function approximations
512240	516880	and the way it learns function approximations internally and we're still studying you know
516880	524000	what actually happens internally but probably yes things get partitioned into sub processes
524000	529760	like sub functions with sparse or communication between them so kind of what you're described
530160	536240	if you think like in your model an individual is this like very interconnected system and
536240	542000	then it has these sparse connections to other deeply interconnected systems and this is like
542000	546000	you know a cell is doing a lot of compute inside but then it's got just like a few
547040	552480	membrane exchanges with outside or corporation there's a lot of stuff going on inside but it
552480	557840	communicates with the rest of the economy the buying and selling products so in some sense anything
557840	564960	that you partition into subsystems is essentially doing this it is using some form of information
566240	572400	like sparse information propagation between units in terms of honesty I think that's a lot
572400	578560	trickier because it's like the communication doesn't need to be honest it needs to be valuable
579120	585680	to like I think there is like a theorem that like for communication to exist it has to benefit
585680	591520	both parties and so there has to be some value to it and you can maybe say that that value is the
591520	598320	truthiness and so you could like maybe decouple it into like signal and noise so there's like some
598320	603280	truth value and then maybe there's like a bunch of a noise value if it's all noise that communication
603280	609440	channel is just not going to persist and so in terms of I think when you're like hey there should
609440	616720	be honest pricing mechanism I would instead say there should be the two agents or whatever the
616720	622000	network of agents there needs to be ways for them to communicate usefully with each other which it
622000	627280	may be is the same thing but they kind of co-discover what that is and that doesn't necessarily need to
627280	634640	be imposed by a mechanism but if a market mechanism or some communication framework
634640	640400	allows honest communication channels then you're making it much easier for them to discover how
640400	648240	to use it. I don't know if that if any of that resonated. It does and maybe there's something
648240	653040	interesting to work out here so economics has been talking about the importance of getting
653040	658480	people to communicate honestly to achieve coordination for a long time and sort of the
658480	662880	basic picture of that which is very silly and false in some ways but also maybe important for
663200	668640	intuition is if you're trying to centrally plan an economy one conceivable way you do that is
668640	672240	you know you have an allocation problem who should get what goods you can just send everyone
672240	677120	like a giant questionnaire and say like do you need a new pair of shoes etc and the problem with
677120	682320	that is that people could lie and they say yeah I totally need a new pair of shoes and so there's
682320	689280	a lot about that picture that's very unrealistic but figuring out ways to get people to honestly
689280	694480	is maybe a word that has some some bad connotations because we interpret in terms of like deliberate
694480	700960	intention and like moral character it's really more just about like truthfully conveying your
700960	705920	intentions so you so basically maybe this is a way of thinking about it you can sort of think of
705920	710720	economics as a very complicated version of like a traffic you're just trying to get cars to not
710720	716240	crash into each other and that means that each car needs to predict where each other car is going
716240	719680	that means each car needs to be sending very clear signals I'm definitely going here and I'm not
719680	724080	going to like turn suddenly or something like that so you're trying to get people to convey what
724080	728560	their plans are so that everyone knows what everyone else is planning and so then they can pick a
728560	737680	plan that's consistent with all those other plans and I guess this is valuable it's like much more
737680	743760	of like I guess an information theory kind of take I think there's I think there's another
743760	751840	thing that that is missing from that model which is that if you have like a really narrow freeway
751840	754880	then some cars are going to get through and some cars are going to have to wait
755760	762000	and there's not an easy way to resolve which ones so I think like and this is kind of back to the
762000	767840	plurality way that I've been thinking about it is like these are all forms of alignment and I break
767840	773200	alignment down into like aligning on action which is kind of what economics in some sense lets you
773200	778880	do is like okay we all have common beliefs let's given our beliefs what should we do and how can we
778880	786480	coordinate our action how can we all send our like our routes so that we don't like collide
786480	791920	so that's kind of aligning on action there's aligning on epistemologies like how do we all come to
791920	798720	believe the same world models and then there's aligning on value which is how do we actually
798720	803040	like I want one thing you want a different thing they maybe maybe we want opposite things how do
803040	810160	we align on that and all of these these three systems are all interdependent because if you
810160	815360	actually want different things you're not going to necessarily even align on epistemology because
815360	820640	the things you know only things that you care about enough to know and if you care about different
820640	825040	things you're not even going to necessarily ask the same questions of the universe to get back
825040	828960	the same knowledge and if you have different beliefs you're not going to align on action
829760	836400	and they all have to be iterated so like if you can agree on the same epistemology then maybe you
836400	842560	can shift both shift your values because what you value depends on what you know and so in some
842560	848640	sense it's like you have to pull all of these three systems in and I think try and what that's
848640	853680	another cool thing about economics is it also does help value alignment and knowledge alignment
853680	859360	because prices are knowledge signals so they tell you know what goods are easier to get when
859360	865360	and they're also value alignment mechanisms because they allow you to trade something that you value
865360	870240	more for something that you value less so I think there are these pieces but they all have to kind
870240	877280	of play together so I want to go in a couple of different directions with that and maybe the first
877280	881840	thing I'll mention is actually pulling away from economics a bit and talk about some neuroscience
881840	886880	so on a different comment I left on Mike's blog post I talk about the work of Lisa Feldman Barrett
887600	892240	who's done super important stuff it's actually her work that really prepared me to understand
892240	897920	what Mike's talking about because I don't know anything about biology and the active inference
898640	903680	sort of predictive coding view of things that she's really advancing as a way of understanding
903680	910480	what the brain does you know sort of we we do often naturally draw these divides between
911120	917280	knowledge acting and valuing and the active inference way of viewing things very much pushes
917280	922480	all of that together it's actually more like action that drives perception and really your
922480	930160	goals are at the base of all of that but also your goals are determined by you know I think of
930160	935200	goals as basically like beliefs or measurements that you have like your goals are in a sense your
935200	941280	epistemology because your goals are your measurements about the world and your prior expectations
941280	944960	it's a weird sounding view and maybe we can talk about that but um that's kind of how I think about
944960	952080	that and so I would you know it's very useful sometimes to separate these things out because
952960	957440	there's a reason we draw these categories we categorize because there are practical reasons
957440	963760	to do so and so that can be useful but also it's I want a fully unified perspective where we're
963760	967840	just treating all of that as kind of the same thing in some level and in economics at the end
967840	974320	of the day when you're trading something you can disagree very strongly about you know like the trade
974320	980400	ends up compressing all of that like if I'm buying shoes and the reasons I'm buying that would make
980400	986000	no sense to the shoe seller as long as I like the price you know everything else is fine so
988160	992400	yeah I'm not quite sure what the balance is there it's um because sometimes it feels like you do need
992400	996640	to address things like that and sometimes it feels like it just takes care of itself when the system
996640	1006000	is set up right um if you guys are up for it I'd love I made a couple of slides and videos I'd
1006000	1012960	love to share uh uh I don't know if that would be a fun uh is that okay yeah yeah go for it let's
1012960	1016480	let's see cool um
1027120	1035440	okay uh can you see can you see my uh slideshow here yeah we can yeah any chance okay full screen
1036080	1046640	yeah is that better much better go for it cool um and this is uh two different uh ideas that
1047440	1052880	kind of work together that I want to get uh both share and get you guys the feedback on
1052880	1059040	I'll go really fast and then we can drill into something but essentially you know the thing I
1059040	1066320	I'm really interested in is how do we make an AI agent um and uh like uh to define an agent is
1066320	1071840	basically something that takes past uh like a sequence of past observations and then produces
1071840	1078320	the next action so it's just I think of an agent uh I know Mike you think agency has goals uh but
1078320	1084560	those are externally observable the knowledge about an agent uh so uh I think that's interesting to
1084560	1089760	figure out so for this uh thing uh like I think of an agent it's just like some function that given a
1103760	1107760	I think you're breaking up for me yeah me too me too
1107920	1110960	and
1120160	1122960	sorry David I don't know if you can hear me but I'm not getting any of this
1133680	1135600	yeah me too I can't I can't hear what you're saying
1138400	1140400	good
1144640	1149040	technology's not a friend today I guess he'll try again yeah I've noticed that usually happens
1149040	1152960	when something really interesting's about to come out it's because it knows what we're doing
1152960	1156720	and it doesn't want us to make progress on these questions yeah there's a little bit of that
1160720	1165040	is that background somewhere you've been uh yeah this is uh years ago this is Alaska
1165840	1170240	oh nice okay I think I've been to Alaska but many years ago I don't remember much about it
1170240	1174320	yeah yeah it was pretty it was pretty incredible
1176240	1180320	this is my blank beige wall because I have that kind of apartment so
1182560	1187760	yeah cool well yeah hopefully hopefully he'll come back um if not we'll just chat
1188160	1195680	so actually I do have something I should address with you quickly so regarding the paper we've
1195680	1200000	discussed I just want to briefly mention to you I don't know a lot about collaborating on papers
1200000	1203520	and if there's something that you might expect a collaborator to be doing right now while you're
1203520	1208000	working on other things that might not be obvious to me is there anything specific that I should
1208000	1213040	be doing right now uh remind me remind me where it stands you have you you've sent me a draft
1213680	1219600	sent you an outline uh yeah then then it's stuck in my court so I've just been in pain so I will
1219600	1224240	I will get back to you with the next couple of days with it so yeah yeah and no rush I just
1224240	1227760	literally wasn't sure if there was something it's not you it's me I've got a I've got a stack of
1227760	1239280	drafts um on my just because I will get to it yeah okay let's see David is I'll be back
1239280	1261840	can you hear me hi David hey hey sorry my computer decided not to run zoom anymore so I'm on my phone
1262560	1273600	um okay let let me try to just do this without the slides but basically the idea is that like an
1273600	1280720	agent is essentially some function that maps past observations to to the next action and
1280720	1288400	general intelligence is essentially a set of behaviors of like I think in your paper might
1288400	1293920	have this thing where it's like how do we navigate a space uh adaptive how do we adaptively
1293920	1299920	and uh intelligently learn to navigate arbitrary spaces and I think of that as just like a collection
1299920	1305680	of uh algorithms that let you balance exploration exploitation essentially it's active inference
1305680	1312240	but active inference requires perfect Bayesian uh inference and really everything just implements a
1312240	1320160	an approximation so uh the question is like how do you learn or build an approximation of active
1320160	1326000	inference in various spaces and neural networks are function approximators that instead of building
1326000	1334480	you can train given the right uh input output pairs so the idea is uh to basically train a
1335200	1342480	uh a neural network that's a function approximator of uh intelligent space navigation and I think the
1342480	1348560	way to do that is agents are basically duels of their environment so if you want to train uh an
1348560	1354160	agent to do something you need to give it an environment where achieving fitness in that
1354160	1362400	environment uh gives you the the behaviors that you want and so I think if you want an agent that's
1362400	1369280	able to generalize learning you need an environment where there's always something new to learn uh and
1370080	1375360	that the behavior space needs to be dense enough that the agent is able to always discover some
1375360	1384000	new thing that it can learn uh that gives it adaptive fitness to the environment uh and therefore as it
1384000	1390800	is learning how to uh adapt to that particular environment it is also generalizing learning
1390800	1399280	it is meta learning learning so uh the idea is if you can give an agent uh always some new
1399280	1403520	environment where there's something new to learn it's going to learn that and it's also in the
1403520	1407840	process of doing that it's going to learn how to learn it's going to learn how do I balance
1407840	1413920	exploration exploitation what algorithms what sampling algorithms can I apply to this new space
1413920	1421520	how do I uh leave myself and grams uh uh like how do I store memory how do I structure memory
1421520	1427920	how do I interpret memory uh in these various ways and so uh what you want is an environment
1427920	1433840	that essentially gets harder or different as the agent gets smarter and I think the way to do that
1433840	1438240	is make the environment and I think the way evolution did this is make the environment
1438240	1445520	highly multi-agent so that uh what you're like there are there is complexity in the physics of
1445520	1450560	the environment sure but really most of the complexity is in the minds of the other learners
1450560	1456160	and this kind of goes back to economics and capitalism uh where uh you know markets are
1456160	1461040	kind of anti legible as soon as you find out some trick that gives you an advantage in the
1461040	1466320	market that advantage goes away because it gets arbitrage the way by everyone else that learns it
1467040	1474160	and so I think you kind of had this with multi-agent setups you kind of can get into this infinite
1474160	1481200	game where as the minds of agents get more complex you have to get more complex to
1482480	1487040	be fit in that environment and this this is kind of just like creates this intelligent
1487040	1492960	tread intelligence treadmill and I think this has been done in AI so this is how we get like
1492960	1499200	go player or chess players it's like they do self-play against themselves and as they get
1499200	1507440	better the game gets harder and but I think typically this is done in purely adversarial
1507440	1514960	settings uh where and with an fully adversarial setting you basically cannot explore a lot of
1514960	1522160	the behavior space because as soon as you deviate from some dominant strategy you get
1523040	1529760	exploited by all the other players and so it's very easy to just get stuck at local minima
1529760	1536480	because there's not a lot of ways to deviate from uh local maximum you basically have to
1536480	1541600	really discover something new really quickly or you just get out competed by people that are doing
1541600	1549600	the same old thing and I think the fix to that is basically blending this line of what an agent is
1550240	1554880	in this collective intelligence sense I think one really strong technique for that is kinship
1554880	1561760	so if you're in a world where other agents are say a lot of the agents around you are your brothers
1561760	1571360	or your clones or your cousins essentially this whole spectrum of kinship then another way to
1571360	1577040	think about agency is kind of you know if you have two agents that share the same goal you can
1577040	1583200	think of them as one agent uh with just like really bad cognitive architecture where instead of just
1583200	1589920	being it has to like learn how to pass messages to itself so one way to define agency is via uh
1589920	1596880	essentially goals and kinship is this way of aligning agents on goals so if you have an agent
1596880	1602720	that 100 shares the goal with another agent it's basically one agent if it's 70 sharing goals with
1602720	1610720	that agent then you know it's kind of this blended super agent and so the idea is if you have an
1610720	1617840	environment where all these agents are aligned with each other with various different kinship
1617840	1623120	relationships you can create this really high dimensional behavior space where you're not
1623120	1628240	always competing there's a million different ways to cooperate break form coalitions break
1628240	1636160	coalitions established trust and so the idea is you know can we create a relatively simple
1636160	1645760	fast to simulate in silico virtual world full of other agents the agents are aligned with each other
1645760	1653840	in various kinship scenarios and then we scale off neural then we train larger and larger neural
1653840	1663440	networks to drive those agents and get to LLM size you know billion parameter models that instead of
1663440	1669440	predicting the next token are predicting the next action that an agent should take given all of the
1669440	1675520	experience at scene and so the idea would be that if you give an environment like that you're
1675520	1682240	essentially providing a gradient towards increased intelligence and then you're using standard machine
1682240	1690560	learning techniques to train a function approximator for that to follow that gradient so that's kind of
1690560	1700320	the idea behind the like overall environment but then the other thing I wanted to incorporate is
1700320	1705200	this collective intelligence approach and I really wish I could present my slides I don't know what
1705200	1713760	happened with my computer maybe I'll pause for a second let other people speak I don't know if
1713760	1719520	anyone has feedback or thoughts or criticisms and then I'll try to see if I can get this other
1719600	1726480	slide to work meanwhile yeah thank you for that I mean I have certainly heard a somewhat similar
1726480	1732560	sounding theory that evolution evolved or intelligence evolved as some sort of you know social competition
1732560	1736640	that you know primates competing with primates and birds competing with birds is
1736640	1740080	where it comes from rather than trying to like you know build tools and so on
1740800	1750880	um so uh yeah the idea about yeah you need a form of like some kind of protection some
1750880	1756560	ability to innovate I mean there's a lot of aspects of economics where there's this
1757120	1762640	challenging tension between wanting to optimize and wanting to create room for innovations like
1762640	1767200	perfect competitions a classic example like you learned very early on perfect for competition
1767200	1771840	you're going to maximize price maximize output your price is going to be as low as it should be
1772720	1777920	but it's very hard to innovate in perfect competition because the way the model works is
1777920	1782320	whatever new technology you introduce everyone copies that instantly you make no profit so but
1782320	1786880	you're the one who made the investment so that just sucks so a little bit of monopoly can actually
1786880	1790880	be helpful because that way you're making money and then there's even artificial things like you
1790880	1795600	know like patents and stuff that give people monopolies to try to incentivize them to
1796320	1802160	make those investments and innovate so that is a very important uh aspect of intention of any
1802160	1807200	sort of collective intelligence system is trying to encourage parts of your system to improve and
1807200	1810960	do better and trying to protect them from parts of the system might say oh we're going to copy
1810960	1817440	that we're going to take that we're going to steal that um there's a part of this that
1818880	1824320	maybe I know you're trying to get your slides up but um I saw in your talk you talk a little bit
1824320	1830320	about like love and stuff at the end of that when you have you have a talk on YouTube and that's
1830320	1838160	something that I had to bring up because I am working on a couple of papers about moral psychology
1838160	1844720	and neuroscience including a paper on morality as a form of cognitive glue which it isn't really
1844720	1851120	but it's a sort of an interesting counter example in some ways and um basically
1851920	1860400	there's a model in which um kinship let me ask you this if um I suppose that you took all the
1860400	1865040	families and you rearranged them like people didn't know that they were in the wrong families but
1865040	1870240	you had families where no one's actually related to each other genetically uh would the system still
1870240	1882800	work? Yep. Yeah so um I think of this as like I think of this as essentially reward sharing so on
1882800	1890320	a genetic level uh the the genetic optimization process uh gives rise to organisms that care about
1890320	1896000	their kin right so because a gene is present in the kin that it's going to make the organisms that
1896000	1901200	come from it want to help other organisms that have that gene from the gene's perspective it
1901200	1910080	doesn't matter whether you reproduce or your brother reproduces uh twice uh so uh and similar
1910080	1920560	so essentially the the behavior that an organism exhibits that we call love is uh basically what
1920640	1927120	it looks like when the underlying optimization process shares rewards or goal or like uh
1928240	1935040	shares rewards between organisms but then the organism itself one of the things that it evolves
1935040	1942960	or learns or whatever needs is the is some way to recognize who it should care about so we have all
1942960	1952000	of these heuristics for knowing who our kin are um and uh once those things are in place uh it's the
1952000	1960560	heuristics that matter not the underlying gene so uh I think if you train agents that see other
1960560	1965840	agents and you give them a reliable signal that hey helping this agent is actually the same as
1965840	1971360	helping you so every time this agent something good happens to this agent you get part of that reward
1971360	1977680	and here is a marker that tells you who is kin and who is not the agent is going to learn behaviors
1977680	1983360	that are conditioned on that marker and then once you stop training and you just let these agents
1983360	1988560	go around behaving you can change that marker arbitrarily rewards are no longer even there
1988560	1995200	right genetic evolution or whatever has stopped now you're no longer learning new policy now the
1995200	2000720	learning is happening inside the mind of the organism not inside the the genes
2006880	2009280	conditioned behavior on kinship markers
2011520	2018000	does that did that make sense it does and there's a very important paper that anyone interested
2018000	2023520	in this should read it's called the sense of should a biologically based framework for modeling
2023520	2030960	social pressure and in some ways maybe the difference is subtle but I think there's a way
2030960	2039280	of looking at these sorts of morally motivated cooperative behaviors um that does not in fact
2039280	2046240	depend on any kind of evolved pressure to care for or intend to cooperate but instead it's just like
2048000	2052400	something your brain just constructs as a living organism in its environment
2052400	2059760	because it's trying to make it's in its social environment predictable and and so there is
2059760	2065040	something of emerging perspective and my papers are somewhat drawing on this research that's
2065040	2071200	basically saying let's not think about evolution let's not think about long-term cooperation let's
2071200	2075680	just think about trying to make the social environment predictable and that's how we can
2075680	2082160	actually get um these cooperative and moral behaviors and so that just might be something
2082160	2086160	to look at because that's another paper released as a co-author and I just bring up Lisa's stuff as
2086160	2096640	much as possible I mean this is also yeah I mean I go ahead Michael sorry I was just going to say
2096640	2102480	that that also sounds like um uh the the uh imperial model of multicellularity that Chris
2102480	2108000	feels and I published a bunch of years ago which is basically that you know if cells are trying to
2108000	2112720	predict an uncertain environment the least uncertain the least surprising thing around is a copy of
2112720	2119120	yourself and so this this is a this can be a driver for uh making cells stick around after
2119120	2126080	you've divided to to form a you know a predictive of a highly predictive um um uh surrounding for
2126080	2129840	yourself so that you live in this niche and then the frontline infantry is out there facing the
2130160	2134160	uncertainties of the outside world but but you can predict them because they're you and so it's a
2134160	2139200	lot easier that way right so so you can you can think about it that way as well have I talked to
2139200	2153760	you about comparative advantage uh yeah no I don't think I want to go ahead David yeah sorry I had a
2153760	2162720	question about that Michael uh so uh I don't uh so if you have a cell uh and the cell does uh you
2162720	2170320	know given situation a it does random stuff then being surrounded by copies of those cells
2170320	2176720	and you do random stuff and your clones do random stuff being surrounded by cells that in that
2176720	2182960	situation do random stuff is not actually going to help you predict uh the environment
2184720	2189520	so I think there's something more to that it's not that being surrounded by copies of yourself
2189520	2195360	gives you the ability to predict your environment I think it's being surrounded by copies of cells
2195360	2201280	that want to make your environment that want to help you make the environment more predictable
2202320	2209520	that actually uh matters uh right unless you think that a cell by being able to
2209520	2217120	introspect itself can predict the behavior of other copies of itself I mean I I don't think
2217120	2222160	they're likely to be doing random stuff they're probably situations in which the responsiveness
2222160	2228000	is kind of random but but I don't think that's the vast majority of what cells do and so I do
2228000	2236800	think that if if if you're um it's it's easier to uh to anticipate cycles and responses I mean
2236800	2240160	again I mean I think you're right if it's random it's not going to work but I actually don't think
2240160	2246400	that cells are random in that in that way but let's um anyway let's let's let David do his thing
2248160	2255680	oh okay yeah this is just uh so this is uh this this is actually mostly inspired by your work Michael
2255680	2261760	and economics so I'd love to get thoughts on this but okay so this is the standard
2261760	2267360	a reinforcement learning uh formulation right you have an environment you have an agent uh
2267360	2272880	the agent takes an action in the environment it gets back some observation and a reward and then
2273520	2279920	it learns to maximum to act in a way that maximizes reward and uh you can have a dual formulation
2279920	2286000	this is which is the inference where the agent is trying to predict observations but essentially
2286000	2293040	this is the the normal kind of loop and typically the way it's uh we can break this down into like
2293040	2300160	an agent basically there's a bunch of observation states from the environment that go into the mind
2300160	2307120	the mind is going to generate a bunch of actions over actuators uh there's some memory that maybe
2307120	2313040	the mind has that it's reading and writing from uh and it's getting a reward signal so this is how
2313040	2319520	we train agents in reinforcement learning or this could be trained or this could be evolved
2320720	2328160	doesn't really matter and so the way we do this now in the field of RL is you have this
2328160	2333040	giant neural net inside the mind and there's model 3 and model RL so they're like this is
2333040	2339520	skipping over some detail but essentially you're training this big blob of math that you're jiggling
2339520	2349200	using gradient descent to optimize your long-term expected reward and it is learning how to compress
2349200	2356960	and basically how to compress the observations into latent states maybe store the parts of those
2356960	2362880	latent states in its memory that then it can reinterpret and eventually to generate actions
2362880	2372320	and so these networks can be giant in language models there are 100 billions of parameters in
2372320	2375680	reinforcement learning they're much smaller because we just haven't figured out how to
2375680	2384480	do it yet but this is kind of the standard setup and what I am proposing is a different architecture
2385360	2394640	that is essentially instead of there being one mind there are many individual agents
2395680	2405360	each agent is mapped onto some subset of the observation space and some subset of the memory
2405360	2411520	space and some subset of the action space and you know they can be mapped to any or all of them
2411520	2418640	but essentially rather than training one brain I want to train a neuron that in a collection
2419280	2427520	when you put it together in a graph of other neurons knows how to co-organize collectively
2427520	2435200	to solve the overall problem that the brain has so rather than training 100 million model
2435200	2443040	network I want to train one million parameter network that when you copy and paste it and
2443760	2452720	throw it into this soup of other neurons will self-organize into solving the problem
2452720	2460240	and so the idea is rather than having to learn adaptive algorithms so in in this way you know
2460240	2465680	you can imagine that there's an algorithm that you apply to many different parts of the sub problem
2466240	2472880	but this network has to learn how to has to learn that algorithm in a bunch of different places
2472880	2478320	because every sub function call might have the same kind of you know oh this is just like a
2478320	2482960	regression problem but you have to learn how to solve regression in a bunch of different places
2482960	2490320	whereas here this thing can just learn okay you know here is how I divide my problem into sub
2490320	2498160	problems and send those messages out or here is how I you know pay attention to a pattern
2498160	2506080	happening at this time scale and that's my job so the idea is to break down this one big blob of
2506160	2514560	compute into something that can be done on this graph instead and I think and this is
2514560	2519760	why I wanted to talk to you then because I think basically I started out thinking well this is an
2519760	2526400	economy you know if these things were you know if the observations and memory were commodities and
2526400	2532160	these things were like buying and selling those commodities or something like that that would give
2532160	2538960	us so the question is you know the the inside of a neural cell is a neural network that we train
2539520	2544320	but then the question is what is the cognitive glue between all of these neural cells essentially
2544320	2550240	what is the protocol that they use such that when we train train the neural cell
2552080	2557760	on the overall objective they'll learn something useful and the thing I came up with
2558480	2565120	kind of influenced by active inference and collective intelligence is so now this is inside
2565120	2570560	a single neural cell right now now this is an agent that just sees some set of inputs
2571760	2579920	it has its own memory it's going to get some reward signal from the the cognitive glue
2579920	2585840	and it's going to produce some outputs and I think the way to do that is that basically
2586240	2593520	it it should be predicting its inputs active inference essentially learning a world model
2593520	2599760	of its little world inside the brain and then voting on the outputs
2602400	2609440	so that it can and in this case outputs can be inputs to another neural cell or just outputs to
2609440	2617520	an actuator that moves the organism around so I think essentially I'm proposing some kind of a
2618240	2624560	an agent who's that you know receives a set of inputs so this is some subset
2626400	2631600	and it has some memory and then what it's going to do is it's going to predict the output at the
2631600	2642320	next time step along with some bet so it has energy and it has finite energy it's going to
2642320	2648720	bet on what the values of that inputs are going to be at the next time step it's going to vote on
2648720	2654800	what it wants to set the outputs to also using this energy and then at the next time step it's
2654800	2662320	going to see well what were the actual inputs and which bets did it win and then it gets energy
2662320	2670320	in proportion to essentially gets energy back as a reward so the idea here is that these agents
2670320	2678640	are incentivized to only predict the inputs that they have a comparative advantage on
2678640	2682720	over other agents so the same input and output is mapped to multiple cells
2682720	2688640	so as an agent you should learn to only to basically monitor your environment form models
2688640	2695440	about it and then bet on the things where you have a comparative advantage to other cells
2696240	2704720	and then as you get more and more capabilities to model your environment you get more and more
2704720	2712240	energy that you can use to vote on steering around that environment this is all hand wavy and this
2712240	2718160	is where I think I need help and help from economics because I started out with pricing then
2718160	2725280	I had it actually be an auction or your or prediction market add an auction and then I
2725280	2730960	try to simplify it just down to just like straight betting markets but that's basically the idea is
2730960	2738080	can we come up with this like simple cognitive glue for a pool of these agents and then train
2738080	2746720	those agents I'll stop now yeah no that's very interesting I think it's definitely an economy
2746720	2752640	and you definitely should talk to economists I'm afraid I think a practical issue is I'm just not
2752640	2758480	sure economists have actually done a lot to explore this space even in theory for the most part they've
2758480	2764560	been content to just take the economy as it is and try to explain that and there's been I think
2764560	2769200	just kind of a certain lack of creativity there in terms of envisioning other economic spaces
2769200	2775440	and trying to think about building alternative economic systems like this one so I'm afraid
2775440	2781360	I myself don't have a lot of practical things to say right now that might actually be of any
2781360	2785760	specific use to you other than I think just affirmation that you're right that this is economics
2785760	2791840	and that talking to economists is a good idea I think the mechanism design people are the place
2791920	2796480	to start I mean I think that's where Glenn Wilde comes from but I gotta know if like
2796480	2800320	Eric Maskin is reachable at all but like he won a Nobel Prize for that and I had a
2800320	2804720	two-minute conversation with him many years ago it was very influential so you might just want to
2804720	2811680	see if you could send him an email and see what he says yeah I think this is cool I'm afraid I don't
2811680	2816640	think I can help you today with it but we should stay in touch and I'll let you know if if anything
2816640	2820960	comes along because this is very interesting and very important
