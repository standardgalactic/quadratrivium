Yeah. So in case you don't know me, I'm Zemin Liu. I'm now a fourth year PhD student at MIT.
Welcome, Professor Max Tagmark. My research interests the center around AI plus science,
so it can go both ways. Either you develop better AIs for science, like AI scientists,
or you use inspirations or like tools in science to enhance the improvement of AI.
So like this work is like both ways. We first try to use some ideas from math and see if we can
develop new AI tools and see if we can, whether these new tools can give something back to to
science. Yeah. So this is about a recent work called The Comagra of Arnold Networks.
Today's AI is built upon this math theorem called the universal approximation theorem,
which basically says that a high-dimensional function, you can decompose a high-dimensional
function into a linear combination of some nonlinear features with this nonlinear function sigma,
which is basically just this two-layer network known as the multilayer perceptron. But you
can make it deeper. That's why it got the name multilayer. But we are asking this question,
are there any alternative theorems we can use? We can leverage these new theorems. Well, not
necessarily new in the mathematical sense, but like in the AI world to build another AI
skyscraper based on new building blocks. So here we are examining this theorem called The
Comagra of Arnold Representation Theorem. First question, what is the KA representation theorem?
It says, again, like you given a high-dimensional function, you can write it down as a finite
composition of one-dimensional continuous functions and just the summing operation. So more
concretely, you can write down an n-dimensional function in this form, where you have the outer
functions capital phi q and the inner functions phi qp. They are both just one-dimensional functions,
and they are finite. Like the number of these functions is depending on the number of inputs
variables n here. So what's nice about this theorem is that it tells us that the only true
multivariate functions is the sum. Like you can express, say, multiplication with just the summing
operation plus some 1D activation functions. So that sounds like really great news for
approximating high-dimensional functions, especially for machine learning,
because in high dimensions, to learn high-dimensional functions, we know we have curse of dimensionality.
But this theorem seems to tell us that you can decompose high-dimensional functions into
one-dimensional functions, which do not suffer from curse of dimensionality, which are great.
But immediately, there is this plot twist. Back in 1989, professor Jarosi and Poggio wrote this
paper examining the relevance of Comagra's theorem in the context of learning. And
they conclude that we review Comagra's theorem and show that it is irrelevant in the context of
networks for learning. So this paper basically sentenced the Ka theorem to death, at least
sentenced to jail for like 30 years. So is that the end of the story? Well, we want to tell you
that that's not the end of the story. And there's again a plot twist. So let's try to see what they
say in their paper, why they sentenced the theorem to jail. So their argument was that the theorem
fails to be true when the inner functions are required to be smooth. Basically, the original
theorem says that you can decompose a high-dimensional continuous function into one-dimensional
functions, but it doesn't guarantee you to decompose it into one-dimensional functions.
But to learn something, like with gradient descent, you would need the functions to be
smooth. Only being continuous is not enough. Even with continuous functions, you can still have
some really pathological behavior. You don't have the smoothness constraint. So that's basically
the argument, because the theory does not tell you anything about the smoothness of the functions.
But in practice, only smooth functions are feasible in practice.
So one interesting word they use is that a stable and usable exact representations
seems hopeless. So that's like the attacking point. That's the angle we're taking. Maybe
we do not need exact representations. Sometimes approximate representations might suffice.
Yeah. So their paper was theoretically sound, was sound theoretically, but I'm trained as a
physics student. So my level of rigor is lower than mathematicians. So me and also Max, we tend
to have lower bar of being rigorous. So we have this naive optimism. So firstly, maybe like
empirically, maybe we don't care about exact representations. Sometimes approximate ones
may suffice. As long as it has some levels of explainability or interpretability, that's fine.
And secondly, a common wisdom in modern deep learning is just stacking more layers to get
different networks. They are basically arguing against the two-layer commograph network. But
what about we build deeper networks? Maybe for deeper networks, even under the smooth
constraint, you can win the expressive power back. Lastly, just the can-do spirit. We cannot say
something doesn't work before really building it with the state-of-the-art techniques. They
have the negative claim back like 30 years ago. Back then, they didn't even have the
back propagation. Like at least it was not popularized back then. So we want to take
everything we have in the modern area of deep learning and see how far we can go. So that's
the mindset. So just with the can-do spirit, we propose or more properly rediscover or we
contextualize the idea of commograph networks in today's deep learning world. Yeah. So here's
the overview. First, I will introduce the math foundation. Well, I already briefly mentioned
it. I won't dig deeper into it, but just I will emphasize again like the mathematical beauty of
it. And then I will talk about the properties of cans. Why? And in what scenarios cans are more
accurate and interpretable than current deep learning models? Yeah. So first, the math foundation,
I already covered this part, but I want to emphasize again that the theorem
looks really nice. That allows you to decompose a high-dimensional functions into one-dimensional
functions. And after the decomposition is done, your only need to just your only job would be
just approximating the 1D functions. So that's the idea of that's the idea of the commograph
networks. Like decomposition first and then learn the 1D functions. Well, yeah. So this
representation looks a bit complicated. You'll see that there are this huge, this big summation
symbol and you have two layers of composition. And this will be complicated. But don't worry
about it. It's just equivalent to this two-layer network. Let's suppose we have two inputs,
x1 and x2, and we have the outputs at the top here. So the representation in the original
theorem is basically just that you have five hidden neurons in the middle. And to get the
activations in each hidden neuron, you basically apply a 1D, possibly nonlinear function to x1
and x2, and sum up these two nonlinear activations to get the activations at the node. And
in the second layer, it's similar that you apply some nonlinear function to the hidden activations,
summing everything up at the output node, and that's how you get the output. So the computation
graph is super clear with just this diagram. And this might remind you a lot of, this looks
just like a multi-layer perceptron, the fully connected networks, where everything just fully
connected. But instead of having activation functions on the nodes, now we are having
activation functions on the edges. And on the nodes, you simply just have the summation operation,
which is really simple. Yeah, just to make it more elegant and beautiful, we can,
or more intuitive, we can basically, because like 1D functions, we can basically just visualize them
with 1D curves, with the x and x as the input and y as the output. So now can network is basically,
you can picture it as this. And by staring at it, you can, you can have some idea what's happening
inside. Yeah, so I mentioned this is just a, so the theorem, so the representation is equivalent
to a two-layer network. But can we go deeper? The answer is yes. Algorithmically speaking,
because it's just a stack of two layers, which, from which we can abstract a notion called the
can layer. So the original two-layer network is basically a stack of two, two can layers.
And for each can layer, it's basically just taking some number of inputs and outputs, some number
of outputs, and in between is fully connected. And on each edge, you have the active, you have the
some nonlinear, learnable nonlinear activation function. And in the end, and in the outputs,
you, you, you summing up the incoming activations. That's how a can layer works. And you can simply
stack more and more can layers to get deeper and deeper cans. This is just a three-layer can,
like, like the first layer, we're taking two, output three. The second layer, input two, output
three. And the last layer, you input three, output one. So this is the, this is a three-layer network,
which approximates a scalar function in two dimensions. But obviously, you can easily
extend it to arbitrary dimension, like arbitrary input, arbitrary output, arbitrary width, arbitrary
depth. So, so you have all the flexibility to choose the size of the network.
Yeah, one, the first question Professor Poggio asked me when I presented this network to him,
he asked, is that why do you need this deep networks? Because the original theorem told you
that you only need the two-layer constructions. And here's just a quick answer that I, I can give
you an example. So please look at this symbolic formula. And if you examine it, you would immediately
realize that you would need at least three compositions to do this, to construct this formula.
You need the, you need the squared function, you need the sine function, you need the exponential
function. They're like, because they're, because it's the compositional structure, they're in different
layers. So you would at least, at least need three layers to, to, to learn this formula. And
indeed, if you just use two-layer network, the activation functions becomes really oscillatory,
becomes really pathological. And the performance is bad, and also the interpretability is bad.
But in the right, I show that a three-layer network, a three-layer can train on this,
train on this dataset. And after training, you would immediately see the learned activation
functions. In the first layer, you got the squared, in the second layer, you got the sine,
and the last layer, you got the exponential. Well, well, you may think of, you may think this is some
other functions, maybe just some local quadratic, but yeah. But you can, but you can, like,
do the template matching with the candidate functions and figure out which one fits the best.
Yeah, so, so I, I said that this activation functions are learnable. How do we make them
learnable? Because they're functions. And, and, and, and the common wisdom is that we need to make,
we need to parametrize the things to be learned so that we can use gradient descent to learn,
to learn this stuff. So the idea is that we parametrize a 1D function with, with B splines.
So B splines is basically some piecewise, some, some local piecewise polynomial functions.
So, so here I showed that there are some local B spline bases. And the way we construct the activation
functions is by linearly combining this, this, this B spline functions. And the only learnable
parameters are just the linear coefficients of, of, of this local basis. And what's nice about
this formulation is that we, we inherit the advantage of B splines. We can easily switch
between fine grains, fine grain grids and coarse grain grids. If you want something to be more
accurate, you can, you can choose the mesh to be more fine grain. If you want the model to be
smaller, so you can have a faster inference, you can, you can, you can choose a more coarse-grained
model. Yeah, that's basically the idea of cans. And we can compare MLPs and cans side by side,
because they do share some similarities, but also share some difference, but also have some
differences. So MLPs are inspired by the universal approximation theorem cans, again,
inspired by the Camargo Ravano representation theorem. The network looks a bit similar in the
sense that they're both fully connected. But their dual, they're different, but they're dual in the
sense that MLPs have fixed activation functions on nodes. Well, you can make them trainable, but
but they're on nodes for sure. And in MLPs, we have learnable weights, learnable linear weights on
edges. By contrast, cans have learnable activation functions on edges, while cans have this simple
linear summation operation on nodes. So, so, so in this sense, cans does not separate the linear
part and the long inner part as MLPs do, but it integrates both the linear part and long inner
part altogether into the can layer. And the can network is simply just the stack of the can layers.
Yeah, so in both cases, you are free to stack the model to become deeper and deeper because
you have the basic notion of a layer, you just stack more layers to get the deeper networks.
Yeah, so that's the basic, that's the basic ideas of cans. And now I want to elaborate more
like why do we care about, why do we care about this? What are the advantages that cans can bring
to us, but other black box models do not bring to us? So yeah, so the first property is the
scaling behavior of cans. As I mentioned before, the idea of cans is decomposing a
high dimensional functions into one dimensional functions. So that looks like really promising
that it can get us, it can get us out of the curse of dimensionality. Let's suppose we are
trying to approximate a d dimensional functions. And I suppose the function has no structure at all.
So then we need to, we need to have a hypercube and have a uniform grid on the hypercube.
Let's suppose we have 10 grid, 10 anchor points along each dimension, then we will need 10 to the
power of d number of anchor points in the d dimensional hypercube. So that's exponentially
expensive. So if you do the classical approximation theory, you would notice that the approximation
error would decay as a parallel, as a parallel, as a function of the number of input dimensionality.
And it's one, and the exponent is one over d, meaning that you got exponentially
like slower when you have more and more, when you have more and more dimensions, like if you need
10 points in one d, you would need 100 points in 2d, you will need 1000 points in 3d and so on.
But if the function has some structure, like if it has the chromograph unknown representation,
then we can decompose it into a bunch of 1d functions. And then our job would just be
approximating 1d functions. So now effectively d becomes one. So you got a really, you got the
fastest possible scaling laws. But the, but the caveats, immediately the caveat is that we,
the assumption is that we, like the function has a smooth chromograph, a smooth finite size
chromograph unknown representation. All of this, you know, all of these objectives,
objectives like smooth or finite size are just, are just practical conditions for a real, for,
for a network which we have access in practice that can really learn the network. We want it to
be smooth because we parametrize it with b-splice, which are smooth. We want them to be finite size
because, of course, you know, we cannot initialize, we cannot deal with an infinite size neural network.
So, yeah, so, so, so we just did some sandwich check on, on some symbolic formulas.
So, yeah, so symbolic formulas are like white, are like what we used in, in science. So that's why we
test them first. So let's see. So the red dashed line is the theoretical prediction. Here we are
using cubic spline. So k is k3. And the scaling exponent is k plus one equals four. And the
curve, yeah, so the thick blue line is for the can network. And you see that almost like the,
like empirical results for the can network almost agreed with, almost agrees with the
theoretical prediction, although sometimes performed slightly worse. Or in this case,
in the second to last case, there's a hundred dimensional case. And it performs much worse
than a theoretical prediction because of the, the, the, the, because the dimension is just too
high and the network can get, can get stuck at some local minima or whatever. But, but nevertheless,
it's still output from MLPs, which you see up in the upright corner here, like the case really
slow. But, but the cans, but cans at least can output form MLPs to a great margin,
although still not saturating the theoretical prediction. But still the scaling law that can
shows looks, looks promising that it's, it seems to not fully beats the curse of dimensionality,
but at least partially beat the curse of dimensionality.
Well, yeah, yeah. So, yeah, just to play the devil's advocate here, you may immediately notice
that I'm on purpose just deliberately using this symbolic formulas. You, you might be wondering,
well, maybe, maybe the functions we care. We encounter a lot in nature may not be symbolic,
they might be some weird, you know, like, at least for special functions, they, they, they,
they are like infinite series, which are hard to be represented with just the finite network. So,
what are, what, what, so, so, so, so what's the cans performance in that scenario?
So, yeah, so we just tried some special functions, which we know for most of them, they do not have
analytical formulas. And indeed, we see that the scaling laws of cans do not saturate the
theoretical prediction, meaning that probably you cannot decompose like a high dimensional
functions into just a one D functions. But still, our goal here is to outcompetes MLPs.
And, and, and it's a feature, not a bug, like not all the functions can be decomposed into
smooth finite representations of this K representation. They may admit non smooth finite size or
smooth infinite size. But in, but, but neither case is, is like the, is what's accessible in
practice. So, yeah, so, so here we show that in most cases, we can achieve this minus two
scaling law, which means that the can network, well, well, sorry. So here, this special functions
are all just two dimensional. So this, so these, according to the spline theory, it would predict
two dimensional, it would predicts a two dimension, like, like, like a scaling exponent to be two,
which agrees with the can results. But in some, but in some case, we can, we can still got the
minus four scaling law. And the reason is that actually, this is secretly, like, like, although
we call, we call it a special function, spherical harmonics are not that special in the sense that
they're still decomposable. So, so you can get the minus four scaling. But also in other, in other
cases, you've got some worse behavior, like you've got the minus one scaling, which means that the
can is underperforming even out on the performing compared to like the spline theory. But what's
interesting is that in this case, MLPs are even more underperforming than cans. So, so this may
tell us that maybe for low dimensional problems, neural networks are not that necessary. And even
the spline theory, like the spline approximation can outcompete the neural network. So, so it's
something to be think about. It's something good to keep in mind. Because neural networks just have
too many degrees of freedom and may have optimization issue.
Yeah. So, so beyond function approximation, we can go on to solving partial differential equations.
So in the setup of physics in form learning, basically, we're trying to solve, we're basically
trying to represent the solution of a PDE with MLP or with a can network. So, so the only difference
from the previous results from the previous experiment is that we're just we're using the physics
informed loss rather than the regression loss. So the optimization becomes more complicated,
but still it's just approximating some function. Yeah. So, so we still see that we with cans,
we can get this optimal scaling law. Well, with MLPs, you see, while with MLPs, you see that,
well, it has the skill at first, but it plateaus really fast and then
does not improve when you have more parameters beyond 10,000.
Um, besides being more accurate, we can also gain some insight what the network is learning.
So, so, yes, yes. So for this example, we can, for this PDE example, we can actually visualize
the can network like this. And immediately you can see that there's some like sine waves and
there's some linear functions. And you can even do symbolic regression to it, like,
like, like our software provides you a way to do this, you can, you can do symbolic regression
to it. And after you do this, you can do some further training. And you, and you can even extract
out the symbolic formula, which gives you like a loss down to machine precision. This is something
that that that standard neural networks would not give you because of because they usually
cannot convert a neural network into a symbolic formula very easily, but with cans, you can easily
do that. Another property of cans is that it has this property of continual learning, at least
in 1D. Yeah, so people have, people have found that, you know, this claim might be invalid for
high dimensions. So take my word with a grain of salt. But at least for 1D, the case is like,
we have, we want to approximate this 1D functions with five peaks. But instead of feeding auto data
at once to the network, we're feeding each time we just feed one peak to the network. And we do
the sequential learning at each stage, the network is just, is just fed with just one peak.
And with cans, and with cans, because we're using the local B-splines, so when it sees the new data,
it does not update the parameters correspond to the old data. So it has this, it can't get rid of
this catastrophic forgetting, like when new data are coming in, the can is able to memorize the
old data and still do quite well on the old data. But this is not true for MLPs. Like when you're,
when you're fed, when the MLPs are fed with the new data, it catastrophically, they catastrophically
forget about the old data. Because in MLPs, you usually have this global activation functions
like the silo functions or Reilu functions. So whenever you make adjustments locally, it will
affect, you know, the predictions far away. So that's the reason why MLPs would not keep
would, would catastrophically forget about the old data.
Yeah, so, so, so that's the first part about accuracy. The second part is about interpretability.
You might already have some sense because we are able to visualize the network as a diagram. So,
the hope is that you can just stare at a diagram and gain some insight of what's happening inside
the neural network. Yes, yeah, so here we have some, some, some toy examples of, yeah, for example,
how do cans do the multiplication computation. So we have x and y as the input and the x times y
as the output. So when we train the can network, we also have some, something similar to L1
regularization to sparsify the network. So we can extract out the minimal network to do the task.
So in the multiplication case, we see that only two neurons are active in the end,
and we can read off the symbolic formulas of how, and get a sense of how it does the computation.
Well, I marked the symbolic formulas here. It may take a while. It may take a while, but I,
well, the point is that it basically just some squared functions and, and the way the can network
learns to compute this multiplication is by leveraging some, some squared equality here.
And the second example, the can is tasked with the division task,
where we input two positive numbers, x and y, and can is asked to predict the x divided by y.
And because here x and y are both positive, the can network learns to first take the
logarithm transformation and then take the subtracted two logarithm and then
transform the same back via the exponential, exponentiation. So, so, so that's really cute.
Like, like one example of the commograph theorem is that you can basically do the multiplication
of positive numbers or the division of positive numbers in the logarithmic scale, because that's,
because the division or multiplication in the logarithmic scale, it transferred to,
would translate to like addition and subtraction in, in the logarithmic scale.
Yeah, so, so, so, so this examples are really simple. You might be wondering what about a more
complicated formula, then, then it might be very complicated to decode what the cans have learned.
So, I would want to argue that this might be a feature, not a bug, or you can call it a bug,
but I can, but I won't call it a feature in the sense that,
sorry, I didn't show the network here, but let's suppose we're doing this formula here,
like u plus v divided by 1 plus uv. So, if you're familiar with relativity, you are special relativity
on, you are, you are, you are, you are realized that this is the velocity, relativistic velocity
addition. So, at first, I thought that I need the five-layer can to fix this function, because
you would need multiplication, which would give, which would consume two layers, you would,
two additions, consume another two layers, and also you, you, you would need the division.
So, so that's in total five layers, but it turned out you can only, you can, you can just use a two-layer
can to fix this function perfectly well. And in the end, I realized that this is just,
this is just a rapidity trick known in special relativity, where you first do the arc-tange
transformation to u and v separately, sum the thing to, to rapidity up, and then you do the
tange transformation back to, to get this formula. So, in this sense, it's rediscovering the, it's,
it's rediscovering the rapidity trick, known, well-known in, well-known in the special relativity.
And in some cases, I indeed find that the network finds some more compressed,
compact representation than I would expect. That's good news in the sense that's the,
the, in the sense that the network is discovering something more compact. So, the representation
is more powerful than I have expected. But the bad news is that sometimes the, the interpretation
can be subtle, can be a bit more complicated. But, but I mean, it's, it can be a, it can be a
feature, but depending on your view. Yeah, so, so, so, so this is one paragraph I take from
the paper. This is criticized, this has been criticized a lot in the social media, but I,
but I find this analogy really interesting. So, I still want to highlight this part.
So, so to me, I think can is like a, it's sort of like language model or, or like language or
even like language for AI plus science. The reason why language models are so transformative and
powerful is because they are useful to anyone who can speak natural language. But the, but the
language of science is functions or more advanced mathematical objects build on functions because
cans are composed of functions which are 1D, so they are interpretable. So, when a human user stares
at a can, it's like communicating it with, it's like communicating it with the, using the language
of functions. So, to elaborate more and to make it more entertaining, let's suppose we are like
my advisor Max is communicating with the can network. Here, I picture the can network as
a trisolarian from the three-body problem. I'm not sure if guys have watched it, but basically
the trisolarians, their brains are transparent, so they cannot hide any secrets from others. So,
they are totally transparent. So, so Max went up, give, give the can, give the can a dataset.
Max was like, here is my dataset. It contains the mystery of the universe. I want you,
the can network, to figure out, to figure out the structure of the datasets and, and the can
initialize the can network like this. So, here's the brain of the can network initially. And Max,
given the dataset, Max wants to train the can network, to train the brain. And after training,
you can get this sparse network, which you start to see some structure. But still, it's a bit,
there's still some residual connections, which looks quite, which quite annoying. So, Max asked the
can network to prune the redundant connections. And after pruning, you got, after pruning,
you got this, you got this sub network, which is responsible for the computation. And Max further
asked the network to the can network to symbolify it, because it looks like, it looks like in the
bottom left, it looks like just a sign function. And in the bottom right, this looks just like a
parabola. And this just looks like an exponential. So, maybe you can symbolify it to gain some more
insight. So, yes, so the can network said, yes, I can symbolify it. And you can. And now the dataset
goes all the way down to the symbolic formula. But, but we can imagine another conversation Max
would have with an MLP. So, Max went up and give the dataset to MLP, and want the MLP to figure
out the symbolic formula in it. And like before, the MLP initialized the brain, looked like something
like this, really messy. After training, Max asked MLP to train the brain, but even after
training, the connection still looks really messy. And MLPs were like, and the MLP is like, I really
trained it, but the loss is pretty low. But it's just that the connections are still very complicated.
Now Max got confused. Like, what's going on? What's going on with your brain? And now MLP is like,
it's just that your humans are too stupid to understand my computations. You cannot say,
I'm wrong, simply because you cannot understand me. So, Max now got really pissed off and turned
back to CANS. So, those are just some imaginary stories I made up with the symphatic example.
And that symphatic example is really simple. But I want to show that we can really use CANS
as a collaborator in scientific research. And CANS can give us some non-trivial results.
CANS can give us some new discoveries. So, the first example is,
yeah, so this example was used in a DeepMind Nature paper three years ago, where they used MLP
to discover a relationship in a NOT dataset. So, each NOT has some invariance. Basically,
each NOT is associated some numbers. And these numbers, they have some relations. And we want
to dig out the relations among these variables. So, what the DeepMind people did was they used
the train and MLP and used the attribution methods, basically take the gradient with respect to these
input variables, and use that as a score to attribute these features. And then rank these
features to get a sense of which features are more important than other features. And they
identified three important features. That's the only thing that's automated in their framework.
And then the human scientist came in and tried to come up with a symbolic formula for it.
So, we're asking this question, can we discover, have we discovered these results
with more automation, with less, you know, efforts, and probably even discovering something new
that the DeepMind paper were missing? So, first, we are able to discover the three important variables
with the CAN network, with much more intuition and automation. So, their network, they used a
three-layer, sorry, they used a five-layer MLP, and each hidden layer has 300 neurons. So,
that's really hard to interpret. That's why they used the feature attribution. But we find that
surprisingly, we only needed one hidden layer and one hidden neuron, the CAN network, to do the task,
as well as their five-layer, like, a million-parameter MLPs. And with this, we can also clearly see
the activations, the importance now basically becomes, you can basically understand the importance of
these variables with the L1 norm of these activation functions. So, that's also how we visualize
the connections. So, you can basically read off from this diagram that the strong connections
are the important variables, while the weaker or even nearly transparent
connections, meaning that irrelevant variables.
Yeah, we can also discover symbolic formulas, as I said before, because the CAN network decomposes
high-dimensional functions into 1D, and then we can just do template matching in 1D
to see what each 1D functions represent symbolic formulas, and then compose all these 1D functions
back to get these high-dimensional functions.
Something beyond their paper we discovered is that their setup is a supervised learning setup.
Basically, they need to partition the variables into inputs and outputs, and they use the inputs to
predict at the outputs. But in some cases, we do not know how to partition the inputs and outputs,
like all the variables, they are treated equally. So, we want to develop this unsupervised setup
where all the variables serve as inputs. But we use some notion of contrastive learning to classify
whether some given input is a real knot or a fake knot, or that might be too technical.
The result is that we are able to discover more relations beyond the relation they've
discovered, because they manually partition one variable as the output, so they can only discover
the relations that involve that variable. But here, we are learning it in an unsupervised way,
so we can learn more than just one relation. We also discovered the relation between these
three variables and between these two variables. Unfortunately, or fortunately, these relations
are already known in the literature of knot theory. So, the unfortunate part is that we did not
discover anything new with our framework, but notice our network is just very preliminary,
it's just a one layer, it's just a one layer if we ignore about the classifier in the second layer.
So, hypothetically, we can make it deeper to get more complicated relations. But the
fortunate part is that it verifies that's what we discovered with the network is something,
is not bullshit, it's something that makes sense that people already know in the literature.
Yeah, we also did this physics example, specifically Anderson localization.
I don't want to bore you with the technical detail, but again, the goal here is to try to figure out
the symbolic formula of the phase transition boundary. In Anderson localization, we have the
localization phase and the extended phase, and there is a phase boundary, and we want to extract
out the phase boundary, especially the symbolic formula of the phase boundary,
if there exists a symbolic formula for it from the road data.
Yeah, so for this slide, I don't want to go to the detail, but the point I want to make with
this slide is that the CAN network has, just like cars, you have manual mode, you have manual mode,
you have automatic mode, like if you're lazy, you can just delegate everything to CANS, and CANS
will return you a symbolic formula fully automated, but that might not be correct. That might not
be what you want. You can do that, it can give you reasonable accuracy, but may not be fully
interpretable, but if you want to have some controllability over the CAN network,
where you want to be more involved, you want to have some intervention, you can still do that.
You can choose to, you can use the manual mode, where you just handpicked some activation functions,
like some functions, obviously they're just quadratic, linear, you can set them to be
exactly the linear or quadratic, and then you retrain the other activation functions,
and you see, and after retraining, you will see what those activation functions would change to
different, would change to different form, and then this gives you, again, gives you some insight,
like give you better evidence what the next guess you would want to make, and this is like the
iterative process, like it's sort of like you are arguing or you are debating with the CAN network,
like the CAN network is give you some, or you can say debating or collaborating,
just like how we interact with a human collaborator, sometimes we debate, sometimes we collaborate,
but you seem to look at the same thing from different angles, like the CAN network is really
great at decomposing high dimensional functions into 1D functions, but those 1D functions may
not be perfect, and there might be some actual subtlety, but humans are super great at identifying
the symbolic stuff, and also recognizing the modular structure from the CAN diagram.
So, yeah, so the takeaway is that you can choose to be lazy, use the automatic mode,
or you can choose to be more responsible and more involved using the manual mode.
Yes, so maybe, yeah, maybe in the end, yeah, I will just finish this really quick,
so people have asked why it looks like CANs can, you know, in terms of expressive power, CANs are
just MLPs, are just like secretly just MLPs, so why do we need CANs? I want to argue that from a
like a high level philosophical level, CANs and MLPs are somewhat different. CAN is like clock work,
like in a clock, pieces are customized and have clear purposes corresponding to the learnable
activation functions in CANs. So for a clock, it's easy to tear it apart, it's easy to tear
the pieces apart and then reassemble the pieces back together to get the clock back, but MLP is
like a house made of bricks, so in MLPs, the pieces are produced in the same standard way,
like each neuron takes in some linear combinations and then do some do the same nonlinear transformation
but it's difficult to tear, once you have a house built, it's difficult to tear apart the bricks
and then reassemble them. So in summary, the source of complexity are different in CANs and in MLPs,
the source of complexity in CANs come from the complexity of each individual object,
like those 1D learnable functions, but because the functions are 1D, no matter how complicated
they are, they're 1D and they have clear purposes in some sense, they are nevertheless, they are
interpretable, but the complexity of CANs come from complicated interactions of individual parts,
the individual parts are simple, but the connections between these individual parts
are really complicated, I guess it's more like human brains, it's more like biology,
yeah, I don't know, but CANs seem like more aligned with the philosophy of reductionism,
where you hope that, where you expect that you can decompose a complicated object into a few
like interpretable individual objects, well in MLPs, everything is connected,
the reason why MLPs function is because they have this emergent behavior or collective behavior
in some sense. Yeah, just some interesting question people ask, are CANs physical? So
if we think of like the Feynman diagram as physical, then unfortunately, Feynman diagrams
are sort of more like MLPs, because in Feynman diagrams, like on the edges, it's just like a
free flow in space without anything interesting happen, but on the nodes where two particles
or multiple particles collide, it's where interesting things happen, this is more aligned,
this is probably more aligned with MLPs, but CANs is like interesting thing happens on the edges,
but not on the nodes. And yeah, last question, people also ask are CANs biological, because
people think that MLPs are inspired by the neurons in our brains, are there any biological
analogy? And I don't know at first, but someone from Twitter wrote it that CANs actually is a
bit analogous to the cells in retina, where each individual cell receive light, apply some
nonlinear transformation to it before summing everything up, I'm not sure, you guys are experts,
so please correct me if I'm wrong here. But the argument is that, well, maybe the mechanisms
of CANs like your first applied nonlinear transformation, then summing everything up,
is indeed biological, but I guess that's just for fun. That's just a minor justification why
we need CANs, because in some sense, CANs are also biological.
Yeah, so that's basically everything I would like to share, and I'm happy to chat more if you
guys have questions. Super interesting, thank you. Questions?
Well, for the last last example in retina, I have some questions. If the network,
like CAN network, is deep enough, is it still matter if you say nonlinearities before or after
the summation?
Yeah, so I guess the key difference, well, yeah, so I guess the key difference is that
in CANs, activation functions are learnable, so I guess, yeah, but whether to put activation
functions on edges or on nodes, I don't think that might be the key difference,
like the learnability of activation functions give you more flexibility.
Yeah, when you talk about this, I was thinking about that CAN is decomposing
different variables, input variables, like if you have x and y, then CAN could decompose it,
because you would have a different combination of them, but like if you have a nested function,
like sine x square, or exponential sine x square, then the CAN seems not able to decompose them,
because they don't have these primitives.
Yes, yes, yes, that's exactly correct. So CAN can only discover compositionality in the sense that
all the 1D functions are boring to CANs. It can just be approximated with just one B-splice,
it doesn't learn any compositionality for single variables. That might be one bug,
if you will, for CANs, if you really want to figure out the symbolic formulas in the data set.
But like for Professor Tomaso Poggio, the author of the 1989 paper who sentenced
or who sentenced Theorem to jail, he wrote in his paper that all the 1D functions are boring.
And what's interesting is this compositional sparsity when you are dealing with multiple
variables, but I guess it depends on your goal. If your goal is just to learn the function efficiently,
then it's fine. But if your goal is to really understand if it's sine of exponential, exponential
of sine, then we probably need to think about ways to handle this.
Yeah, thank you.
Have you thought about combining CANs and MLPs, given their somewhat complementary nature,
or despite their complementary nature?
Yeah, that's a great question.
So we have some primitives, like CANs can propose this CAN layer, which is a new primitive.
And for MLPs, it has these linear layers and also the nonlinear activations, which are also
primitive. I mean, these are like the building blocks. And I guess as long as they fit together,
as long as they fit together, you can freely just combine them in ways that you want.
But it's just that it's a bit hard to tell what's the advantage of combining. And
because I guess there are many ways to integrate the two models and which way is the best. And I
guess it's a case dependent question. It again depends on what's your application, what's your goal,
something like that.
