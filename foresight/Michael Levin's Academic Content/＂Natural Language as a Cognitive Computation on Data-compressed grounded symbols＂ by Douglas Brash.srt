1
00:00:00,000 --> 00:00:20,000
Okay. All right. So what I'm going to be talking about is how what natural language is, I think, a cognitive computation rather than a, say, purely linguistic one, which is probably why it's relevant to you guys.

2
00:00:20,000 --> 00:00:31,000
And everybody, I think, would agree that it has to do with symbols, although I find in linguistics, you could find somebody who'll argue about anything.

3
00:00:31,000 --> 00:00:45,000
And so whether we have internal representations, for example, at all. And then what I'm going to point out is that these symbols have been data compressed, and that's why language looks as complicated as it does.

4
00:00:45,000 --> 00:00:57,000
Frankly, if it weren't for that, probably everything I'm going to say could have been discovered by Aristotle. And it's just that I think nobody noticed these things.

5
00:00:57,000 --> 00:01:10,000
And then a implication of this would be, we'll get to the end and it allows computers, I think, to be to do cognition. And I'm a little worried that AI is going to stumble across this accident.

6
00:01:10,000 --> 00:01:26,000
So first, I'd like to do two warm up exercises, very brief. One, I'm going to say a sentence and ask yourself, are you having any trouble at all with this? So he saw the saw that I saw.

7
00:01:26,000 --> 00:01:34,000
Anybody have trouble with it? I think it's slid right down, right? So that's actually a little bit remarkable.

8
00:01:34,000 --> 00:01:50,000
Now, the second warm up question, we're going to take warm up exercise, we're going to take 10 seconds. And I want you to think, what are the parts of a chair?

9
00:01:50,000 --> 00:02:00,000
Okay, now, did anybody have in their list, glue, nuts, bolts or screws?

10
00:02:00,000 --> 00:02:10,000
Okay, this will become relevant later. But I want to want you to do it before you get biased towards thinking about things like that.

11
00:02:10,000 --> 00:02:22,000
So this is all part of what I think of as the Descartes-Cahal problem. You're starting out here, somehow you end up with this.

12
00:02:22,000 --> 00:02:35,000
And I'm not going to talk about how Shakespeare could possibly do this. I'm going to talk about how he could tell you these words and you understand what he's thinking.

13
00:02:35,000 --> 00:02:42,000
But in the course of that, we're going to be getting back to how does Shakespeare do this in the first place?

14
00:02:42,000 --> 00:02:51,000
So as far as understanding language, I think it's a much more amazing algorithm, whatever it is we do, than people really appreciate.

15
00:02:51,000 --> 00:02:55,000
And so I want to start off a little bit slowly and point out a few things.

16
00:02:55,000 --> 00:03:02,000
One is, I think it's a computation executed by neurons, most of us biologists would agree on that.

17
00:03:02,000 --> 00:03:13,000
And what you're doing is one person has this three-dimensional, instantaneous, simultaneous picture in his head about something about the way the world's going around.

18
00:03:13,000 --> 00:03:25,000
But we're going to transfer that 3D picture to somebody else using a linear string of symbols that are ambiguous, because words usually all have multiple meanings.

19
00:03:25,000 --> 00:03:35,000
And their arbitrary symbols, like the word cat has C-A-T, has no relation to a cat with two ears and a tail whatsoever.

20
00:03:35,000 --> 00:03:42,000
And the only relationship between these symbols is adjacency and succession.

21
00:03:42,000 --> 00:03:50,000
And somehow that doesn't trigger a bundle of associations in the listener's brain.

22
00:03:50,000 --> 00:03:56,000
It triggers, it assembles the coherent picture of the world, the one that the speaker had.

23
00:03:56,000 --> 00:03:58,000
So how do you pull that off?

24
00:03:58,000 --> 00:04:14,000
Well, the solution in the 20th century, starting with Bar-Hell-L, actually, and also Chomsky, was that language has three main points, and then the rest of the talk is going to be replacing each of these with something else.

25
00:04:14,000 --> 00:04:27,000
First, that language processes arbitrary symbols, unrelated to the thing they're talking about, and it processes them using syntactic rules that don't have anything to do with the meaning of those words.

26
00:04:27,000 --> 00:04:35,000
Second, the function, the language structure is dyadic, function argument like a mathematics.

27
00:04:35,000 --> 00:04:40,000
So there is a subject and a complement in a sentence.

28
00:04:40,000 --> 00:04:43,000
And the complement has a verb and an object.

29
00:04:43,000 --> 00:04:47,000
So each of these two parts, and it goes on in steadily more detail.

30
00:04:47,000 --> 00:04:55,000
And third, that the language processes is determined completely by the words that are sitting on the page.

31
00:04:55,000 --> 00:04:59,000
And I'm going to argue that none of these is true.

32
00:04:59,000 --> 00:05:07,000
And once you see through it, then it becomes simple and regular.

33
00:05:07,000 --> 00:05:16,000
And in fact, before the 20th century, there was a history that meaning did matter.

34
00:05:16,000 --> 00:05:23,000
There was a history that language structure had made of three parts, not two parts.

35
00:05:23,000 --> 00:05:25,000
It went by the wayside with Chomsky.

36
00:05:25,000 --> 00:05:32,000
And there are a lot of clever things that come about from transformational grammar.

37
00:05:32,000 --> 00:05:43,000
But I think the active effort in linguistic to suppress any other thinking has led to the problem.

38
00:05:43,000 --> 00:05:49,000
So, first point, I'd say that language symbols are grounded, not arbitrary.

39
00:05:49,000 --> 00:06:01,000
Grounded meaning that the symbol itself has some physical resemblance to the thing that it's talking about or that it's representing.

40
00:06:01,000 --> 00:06:03,000
But I'll get to that part in a minute.

41
00:06:03,000 --> 00:06:06,000
So an arbitrary symbol, so we have a sentence here.

42
00:06:06,000 --> 00:06:14,000
And the standard view, but here's the arbitrary symbol cats, and it points to a thing out in the real world of cats.

43
00:06:14,000 --> 00:06:17,000
My points to me and the dog points to the dog.

44
00:06:17,000 --> 00:06:23,000
And there's something that we're pointing to with the word chases.

45
00:06:23,000 --> 00:06:28,000
So that's thing out in the real world of the reference.

46
00:06:28,000 --> 00:06:32,000
So there are a number of attractions to this way of looking at things.

47
00:06:32,000 --> 00:06:36,000
So symbolic computations require a few things.

48
00:06:36,000 --> 00:06:38,000
They require variables.

49
00:06:38,000 --> 00:06:40,000
So the word dog can stand for any number of dogs.

50
00:06:40,000 --> 00:06:43,000
It can get instantiated by different dogs.

51
00:06:43,000 --> 00:06:50,000
There's a directional structure rather than just being an aggregate of stuff or a clustering.

52
00:06:50,000 --> 00:06:58,000
There's some hierarchies, you know, like my dog is a unit that's a subunit of this whole thing.

53
00:06:58,000 --> 00:07:01,000
And these symbols remain particular.

54
00:07:01,000 --> 00:07:04,000
It's not like red and white giving pink.

55
00:07:04,000 --> 00:07:11,000
The dog stays the dog and the cat stays the cat, which is a non trivial property of language.

56
00:07:11,000 --> 00:07:19,000
And the thing that Chomsky introduced is basically the idea that you want somewhere or somehow that the dog's chasing the cat.

57
00:07:19,000 --> 00:07:23,000
So somehow cats has to get here.

58
00:07:23,000 --> 00:07:25,000
Oh, can you guys see my pointer?

59
00:07:25,000 --> 00:07:28,000
Does that work? Okay, good. Thanks.

60
00:07:28,000 --> 00:07:33,000
And so Chomsky do attention to that problem and solve that problem.

61
00:07:33,000 --> 00:07:40,000
And actually much of what I'm going to say will wind up deriving a lot of the early transformational grammar,

62
00:07:40,000 --> 00:07:47,000
but from a cognitive basis rather than a linguistic basis and with a whole lot less ad hoc stuff.

63
00:07:47,000 --> 00:07:55,000
Now, there's a problem though, which is, well, actually, let me point out the problem.

64
00:07:55,000 --> 00:08:00,000
There's nothing in these words that says how to assemble this picture.

65
00:08:00,000 --> 00:08:03,000
So you've got these reference with how do you assemble it?

66
00:08:03,000 --> 00:08:14,000
It gets a little better if you are willing to agree that we have in our heads internal representation, which are these little thought balloons here.

67
00:08:14,000 --> 00:08:26,000
And that, which by the way is also something that people argue, you know, there's a whole school of thought says, oh, no, we don't have internal representations.

68
00:08:26,000 --> 00:08:34,000
But still there's nothing here that says how to assemble these internal representations either.

69
00:08:34,000 --> 00:08:43,000
So what happens if we take seriously the grounding that each one of these symbols is grounded somehow and something in the real world.

70
00:08:43,000 --> 00:08:50,000
In other words, physically represents it, whatever it is that's in our heads that represents a dog.

71
00:08:50,000 --> 00:08:53,000
Then you're in a little better shape.

72
00:08:53,000 --> 00:08:59,000
Oh, well, this is what I was saying is that if you don't assemble them, you just get this cluster of images.

73
00:08:59,000 --> 00:09:03,000
Oh, and this word ambiguity is significant.

74
00:09:03,000 --> 00:09:10,000
So 12 word sentence people have shown that can have between 10 to the fifth and 10 to the 28 potential parses.

75
00:09:10,000 --> 00:09:16,000
There's only 10 to the 60th atoms in the universe last time I saw a number.

76
00:09:16,000 --> 00:09:24,000
So that's a problem that's generally not addressed in linguistics.

77
00:09:24,000 --> 00:09:37,000
And, you know, in the 70s, 80s, 90s, the link places like IBM were trying to write programs that would use Chomsky and linguistics process language and they failed.

78
00:09:37,000 --> 00:09:45,000
And the famous joke is that one of the managers at IBM said that every time he fires a linguist the recognition rate goes up.

79
00:09:45,000 --> 00:09:53,000
So while he's definitely on to something, it's not the solution to the problem.

80
00:09:53,000 --> 00:09:57,000
And certainly no hand is towards going on biological.

81
00:09:57,000 --> 00:10:04,000
Now, if you take seriously the ground symbols, then you're doing computations on these representations.

82
00:10:04,000 --> 00:10:10,000
And that allows you to do two things that hard at pointed out a long time ago are critical for cognition.

83
00:10:10,000 --> 00:10:16,000
You have to be able to compare two representations and distinguish between them.

84
00:10:16,000 --> 00:10:19,000
Or you compare them and look for similarities.

85
00:10:19,000 --> 00:10:27,000
Let's you do discrimination on the one hand, let's you do category categorization and generalization on the other.

86
00:10:27,000 --> 00:10:32,000
So that you have this idea of dog independent of any particular dog.

87
00:10:32,000 --> 00:10:35,000
Without that, you can't do cognition.

88
00:10:35,000 --> 00:10:41,000
There's nothing about the word cats down here that I can compute with the word dog.

89
00:10:41,000 --> 00:10:49,000
That's going to let me tell you whether they have the same number of years can't be done the information not there.

90
00:10:49,000 --> 00:10:53,000
But it is up here if you have grounded symbols.

91
00:10:53,000 --> 00:10:56,000
So these are sometimes called icons.

92
00:10:56,000 --> 00:10:59,000
The classical one would be say a pedestrian crossing sign.

93
00:10:59,000 --> 00:11:03,000
It looks a little bit like a pedestrian crossing the street.

94
00:11:03,000 --> 00:11:10,000
Okay, so there's some correspondence and now you can do computations on them.

95
00:11:10,000 --> 00:11:22,000
So, oh, there is a problem, which is that we still remember we still want to not just point to these individual representations.

96
00:11:22,000 --> 00:11:25,000
We want to assemble them into higher things.

97
00:11:25,000 --> 00:11:36,000
So you want to assemble these two into my dog and you want to assemble the whole thing into something about cats and dogs chasing.

98
00:11:36,000 --> 00:11:40,000
And you also have to get cats over here somehow.

99
00:11:40,000 --> 00:11:43,000
So you've got to do a computation.

100
00:11:43,000 --> 00:11:54,000
But if the representations are grounded, there's no guarantee that your computations on them are still going to be grounded and therefore give you a valid result.

101
00:11:54,000 --> 00:12:03,000
So I would say the first thing I'm going to add is that, okay, whatever the language rules are, they also have to be grounded.

102
00:12:03,000 --> 00:12:10,000
And so that grounded this is preserved when you're doing computations on the individual grounded symbols.

103
00:12:10,000 --> 00:12:23,000
So the solution to that I would say got two parts once the grounded rule hypothesis, the speaker and the listener have operations on the arbitrary symbols of words.

104
00:12:23,000 --> 00:12:33,000
That are grounded in the external world so that they're manipulating those words in the same way that the representations would be manipulated.

105
00:12:33,000 --> 00:12:44,000
Or if you prefer, they actually are manipulating your representations, which is the more radical proposal, but that's, and I think that's what's going on, but I'm not going to demand that just yet.

106
00:12:44,000 --> 00:12:53,000
So then you can see how you could ground an object like a dog by pointing to it. How do you ground a rule?

107
00:12:53,000 --> 00:12:59,000
So then my second addition is what I would call the exotopic rule.

108
00:12:59,000 --> 00:13:04,000
So biology, the tissue mimics stuff in the outside world.

109
00:13:04,000 --> 00:13:30,000
So I think you guys probably know that the arrangement of pitches of notes or sound frequencies in the cochlea is in order of the frequency and the representation in your brain spatially is in that same order.

110
00:13:30,000 --> 00:13:40,000
And you have the same sort of thing for the representation of your hand in your arm. And so those are called like tonotopic and somatotopic and so forth.

111
00:13:40,000 --> 00:13:53,000
And you can see the same thing in genetics. You guys will appreciate this that the homeobox genes are expressed last time I checked in the same order that they are arranged in the chromosome.

112
00:13:53,000 --> 00:14:00,000
And so biology has this habit of mimicking in the biology, whatever is going outside.

113
00:14:00,000 --> 00:14:09,000
And so what I'm going to propose is, first of all, the way we see the world is there are things out there, there are relations in between them.

114
00:14:09,000 --> 00:14:20,000
And so you have two entities and a relation between them. And so I'm going to propose, first of all, that that's the structure of the world, at least as the way people see it.

115
00:14:20,000 --> 00:14:36,000
We don't see it as Hamiltonians. And second, we're going to, I'm going to propose that, okay, there's something in our biological tissue or in these rules for manipulating representations that mimics this entity relation entity structure.

116
00:14:36,000 --> 00:14:48,000
So that's the exotopic rule hypothesis. So the operands are going to be these icons, these grounded symbols or hierarchies of them.

117
00:14:48,000 --> 00:14:58,000
And the operations are going to build these hierarchies using only those three things and the result of doing it is still going to be one of those three things.

118
00:14:58,000 --> 00:15:02,000
So then the question is, okay, so how far can you get with this.

119
00:15:02,000 --> 00:15:16,000
So, let's start doing some data. So I assembled about 1000 different sentence fragments and sentences, not linguistics tends to use the Wall Street Journal because usually they were trying to sell to Wall Street people.

120
00:15:16,000 --> 00:15:31,000
But basically newspapers are not the way we really speak. And so I took a variety of, for example, dictionaries intended for foreign speakers tend to have example sentences in them that are.

121
00:15:31,000 --> 00:15:35,000
Real world sentences.

122
00:15:35,000 --> 00:15:43,000
There are that took some books that are written by clear writers and so forth.

123
00:15:43,000 --> 00:15:56,000
So let's take three of these. I'm going to go through it slowly semi obvious but I'm going to go through it for it slowly because once you get over this slide if you're okay with me and if you have objections here's the time to ask.

124
00:15:56,000 --> 00:16:01,000
And you'll see what I'm going through if you get through this and the whole rest of the talk is working through the details.

125
00:16:01,000 --> 00:16:08,000
So the word string, and these are all taken from this these corpuses of senses, experts predict improvements.

126
00:16:08,000 --> 00:16:23,000
So the meaning of that is roughly experts predict improvements. So this is this capital is a standard kind of abbreviation in the field meaning intended to mean that here's the meaning of the word.

127
00:16:23,000 --> 00:16:28,000
And I'm going to notate this parentheses around the entities.

128
00:16:28,000 --> 00:16:35,000
And then the relations don't get parentheses. So you see here we have an entity a relation and an added.

129
00:16:35,000 --> 00:16:38,000
Now, the green grassland.

130
00:16:38,000 --> 00:16:51,000
Well, the is pretty much what it means is that I'm going to tell you about something I already told you about, as opposed to our, where I'm going to tell you something new.

131
00:16:51,000 --> 00:16:59,000
And green is pretty much like green us, you know, yes, representation of color, whatever that is.

132
00:16:59,000 --> 00:17:02,000
And grassland is your representation of grassland.

133
00:17:02,000 --> 00:17:11,000
But what I'm also saying is that the green is a component over a property of the grassland.

134
00:17:11,000 --> 00:17:12,000
Okay.

135
00:17:12,000 --> 00:17:15,000
So I'm going to abbreviate that with this symbol here.

136
00:17:15,000 --> 00:17:23,000
You can think of it as a grassland and folding this component, this property of greenness.

137
00:17:23,000 --> 00:17:31,000
And this green grassland is part of the things I already told you about.

138
00:17:31,000 --> 00:17:36,000
So the has the component green grassland.

139
00:17:36,000 --> 00:17:40,000
So it's a similar symbol but facing the other direction.

140
00:17:40,000 --> 00:17:51,000
So the history to this symbol, which turns out to be wrong, but it's a nice symbol anyway, because it conveys this unfoldingness.

141
00:17:51,000 --> 00:18:00,000
But we don't speak these things doing some in Chinese sometimes in some situations they do, but in English it out.

142
00:18:00,000 --> 00:18:02,000
So we leave this out.

143
00:18:02,000 --> 00:18:05,000
Now here's another one the person who threw it.

144
00:18:05,000 --> 00:18:18,000
So person is, you know, some variable for a person to be instantiated, some particular person who is, you know, sort of referring to something.

145
00:18:18,000 --> 00:18:28,000
But again, arbitrary through we have a representation of throwing it refers to some item that I've already talked about some known item.

146
00:18:28,000 --> 00:18:36,000
The meaning here is also that this person is a component of this activity of somebody through something.

147
00:18:36,000 --> 00:18:39,000
So gets another one of these things.

148
00:18:39,000 --> 00:18:43,000
So again, we don't speak it though.

149
00:18:43,000 --> 00:18:50,000
Okay, so if you're with me so far, then this is why I asked you about the parts of the chair.

150
00:18:50,000 --> 00:18:57,000
We never think about these things we don't speak, just sort of all subconscious.

151
00:18:57,000 --> 00:19:09,000
So this is data compression, which is, you know, if you're familiar with computers you know about data compression if you're a linguist you don't know about it and you don't want to know about it oddly, I find.

152
00:19:09,000 --> 00:19:16,000
And so what this is saying is that when we speak in English, we're doing data compression.

153
00:19:16,000 --> 00:19:18,000
Here's a few more examples.

154
00:19:18,000 --> 00:19:21,000
And then I'll show you, well, I guess this is data.

155
00:19:21,000 --> 00:19:23,000
He met me there.

156
00:19:23,000 --> 00:19:28,000
Well, we don't say at, but that's kind of what you need in order to understand the sense.

157
00:19:28,000 --> 00:19:33,000
They raise prices 8%, well, by 8%.

158
00:19:33,000 --> 00:19:35,000
There's more of these.

159
00:19:35,000 --> 00:19:36,000
He found her asleep.

160
00:19:36,000 --> 00:19:43,000
Well, what he found was the condition of her having the property of being asleep.

161
00:19:43,000 --> 00:19:45,000
And you can go on with this.

162
00:19:45,000 --> 00:19:48,000
Well, indirect objects are important.

163
00:19:48,000 --> 00:20:00,000
He gave mama the, he gave the situation of mama owning the stress.

164
00:20:00,000 --> 00:20:12,000
This one, a linguist debate about, but there actually is a history pointed out to me by Steven Pinker, Steven Pinker actually, that there's strong arguments that ought to be treated this way.

165
00:20:12,000 --> 00:20:19,000
And then he gave mama a function that has two arguments and so forth, which is the standard linguistics way.

166
00:20:19,000 --> 00:20:25,000
So, and then one other thing I'll point out is that jaren's are really little miniature senses.

167
00:20:25,000 --> 00:20:29,000
So cooking, well, it means that something cooks something.

168
00:20:29,000 --> 00:20:34,000
And you just haven't said who the subject is and you haven't said who the object is.

169
00:20:34,000 --> 00:20:41,000
But it's an E, well, E, so that's an E, you know, I can use the word that to talk about it.

170
00:20:41,000 --> 00:20:44,000
It's an entity. Awesome.

171
00:20:44,000 --> 00:20:47,000
Oh, I'll do one more.

172
00:20:47,000 --> 00:20:51,000
Why do we have the word to in infinitives?

173
00:20:51,000 --> 00:20:52,000
What's the point?

174
00:20:52,000 --> 00:21:05,000
Well, if you, what's really going on here, Casey wants Casey to change to the situation of Casey throwing the ball.

175
00:21:05,000 --> 00:21:09,000
Pro is a standard linguistics thing, meaning an unspoken subject.

176
00:21:09,000 --> 00:21:14,000
So they're onto this idea that some things are not spoken.

177
00:21:14,000 --> 00:21:19,000
But my point is that this is common. You see it everywhere.

178
00:21:19,000 --> 00:21:24,000
Oh, the horse that raised past the barn fell. That's this classic garden path sense.

179
00:21:24,000 --> 00:21:33,000
Because if you just say the horse raised past the barn, you're expecting it to be a past tense verb and you've got this picture of a horse racing past the barn.

180
00:21:33,000 --> 00:21:37,000
Then you throw in the verb fell and now you're confused and you have to go read the sentence.

181
00:21:37,000 --> 00:21:43,000
And what you left out was that was the horse that was raised past the barn.

182
00:21:43,000 --> 00:21:45,000
Okay. So how common is this?

183
00:21:45,000 --> 00:21:50,000
It's about 25% of English sentences are not spoken.

184
00:21:50,000 --> 00:21:57,000
So these are various sources like the Longman's dictionary contemporary Englishes were non English speakers.

185
00:21:57,000 --> 00:22:05,000
The Oxford dictionary, the English language has a whole section on examples, which are quite nice.

186
00:22:05,000 --> 00:22:16,000
The Penn Tree Bank, which is a standard corpus linguistics corpus annotated for parts speech and so forth has lots of sentences and there are various other things.

187
00:22:16,000 --> 00:22:21,000
And pretty consistently, it's about 25% of the things that are not spoken.

188
00:22:21,000 --> 00:22:29,000
One of them are the system component relations, but sometimes the adverbs and so forth.

189
00:22:29,000 --> 00:22:36,000
Now, once we agree to that so and so no objection so far. Great.

190
00:22:36,000 --> 00:22:40,000
So what's the is there a pattern to this.

191
00:22:40,000 --> 00:22:48,000
Well, so now the entities are in green the relations are in violet.

192
00:22:48,000 --> 00:22:56,000
And it looks like continuously you're alternating entity relation entity relation.

193
00:22:56,000 --> 00:23:06,000
Well, um, okay, that's nice. So things are beginning to look a little less complex than language is supposed to be.

194
00:23:06,000 --> 00:23:12,000
Now, you can guide this or force the structure.

195
00:23:12,000 --> 00:23:20,000
If you say that there's something like a reading frame so you guys know genetics linguists don't so you'll get it immediately.

196
00:23:20,000 --> 00:23:28,000
So like a reading frame in genetics, you've got this pattern just waiting for the words to be dropped in.

197
00:23:28,000 --> 00:23:32,000
So entity relation entity relation, etc, etc, etc.

198
00:23:32,000 --> 00:23:37,000
So if we take the sentence like he saw the saw, I saw.

199
00:23:37,000 --> 00:23:42,000
So we're going to drop the entity into this box.

200
00:23:42,000 --> 00:23:51,000
Well, then the next box over is got to be is expecting a relation so it's going to take the verb sense of saw.

201
00:23:51,000 --> 00:23:54,000
Then the next box is expecting an entity.

202
00:23:54,000 --> 00:24:06,000
The goes in there just fine. Part of the dictionary definition of the is going to be that it's talking about something that's component something that you've already talked about.

203
00:24:06,000 --> 00:24:11,000
So this are composed of comes along from the dictionary.

204
00:24:11,000 --> 00:24:13,000
So now you're in this box.

205
00:24:13,000 --> 00:24:17,000
So you get saw again but now it's expecting an entity.

206
00:24:17,000 --> 00:24:29,000
And so this helps tremendously with the disambiguation of words and is a clue as to why we can get away with ambiguous words.

207
00:24:29,000 --> 00:24:32,000
I'll show you some actual data on this point later.

208
00:24:32,000 --> 00:24:35,000
You can keep on going.

209
00:24:35,000 --> 00:24:39,000
He saw the saw Clive saw what Clive.

210
00:24:39,000 --> 00:24:42,000
So you're here, but Clive is a proper now.

211
00:24:42,000 --> 00:24:44,000
So it's got to go here.

212
00:24:44,000 --> 00:24:47,000
Well, so now you got an empty space.

213
00:24:47,000 --> 00:24:52,000
So now that's telling you that okay time to do some data decompression.

214
00:24:52,000 --> 00:25:11,000
And we've got a table, which is not all that complicated, where you look up in the table what to insert here, given that you've got an entity here and an entity here and it just depends on things like whether these are proper nouns whether it's accountable now.

215
00:25:12,000 --> 00:25:17,000
You know, it's basically, you know, it's a table to look up table you filled in.

216
00:25:17,000 --> 00:25:23,000
If you had lots of computing time I'm sure one could optimize that table.

217
00:25:23,000 --> 00:25:29,000
But I'll show you later how well this works. And so then now you're here and then now you've got a verb again.

218
00:25:29,000 --> 00:25:36,000
And then you get to the period which is basically relation between sentences.

219
00:25:36,000 --> 00:25:38,000
Okay, you got another empty space.

220
00:25:38,000 --> 00:25:43,000
Okay, well, so you left an empty space here.

221
00:25:43,000 --> 00:25:59,000
Well, actually, that's good, because you really want this saw over here to wind up over here, just like in transformational grammar, because Clive saw that saw.

222
00:25:59,000 --> 00:26:04,000
And so I'll talk later about how to do that.

223
00:26:04,000 --> 00:26:12,000
But you can see that this sequencing template is on simplifies everything.

224
00:26:12,000 --> 00:26:14,000
Now I want to make two points.

225
00:26:14,000 --> 00:26:17,000
So it simplifies the disambiguation data question.

226
00:26:17,000 --> 00:26:27,000
But if the word is ambiguous, the dictionary alone and the word alone can't help you with it.

227
00:26:27,000 --> 00:26:34,000
And I tell you where to place the word in this series of boxes.

228
00:26:34,000 --> 00:26:38,000
So this template has to be exogenous to the sentence.

229
00:26:38,000 --> 00:26:41,000
It's not contained in the words in the page.

230
00:26:41,000 --> 00:26:46,000
And I think there's no other place for it to be than up here.

231
00:26:46,000 --> 00:26:56,000
And I think this is the thing that humans have that lets us process language.

232
00:26:56,000 --> 00:27:07,000
The whole literature in linguistics arguing that some of this structure comes from a previous ability to synchronize muscles in order to throw things.

233
00:27:07,000 --> 00:27:09,000
And I think that's probably on the right track.

234
00:27:09,000 --> 00:27:12,000
You already had some kind of sequencing track.

235
00:27:12,000 --> 00:27:16,000
And now you've learned to put words into it.

236
00:27:16,000 --> 00:27:26,000
The other thing is that if you're inserting gaps like this, well, that also can't be part of the words on the page.

237
00:27:26,000 --> 00:27:31,000
It has to come from an exogenous sequencing track.

238
00:27:31,000 --> 00:27:39,000
And for those of you who are, you know, like Dr. Christon, you think about Bayesian sentence processing.

239
00:27:39,000 --> 00:27:42,000
I would say that, okay, here's the backbone.

240
00:27:42,000 --> 00:27:45,000
Yes, you can add sophistication onto it.

241
00:27:45,000 --> 00:27:49,000
And I'll show you later how much sophistication you...

242
00:27:49,000 --> 00:27:56,000
How much you get from this alone and how much additional sophistication you need.

243
00:27:56,000 --> 00:27:58,000
Okay.

244
00:27:58,000 --> 00:28:02,000
Now, so far we just got a string of e's, r's, e's, r's, and so on.

245
00:28:02,000 --> 00:28:04,000
End of due relation sentence.

246
00:28:04,000 --> 00:28:06,000
Now we've got to build them up.

247
00:28:06,000 --> 00:28:08,000
So now I'll tell you how to do that.

248
00:28:08,000 --> 00:28:09,000
So this is the next thing.

249
00:28:09,000 --> 00:28:11,000
So we're on number four.

250
00:28:11,000 --> 00:28:18,000
The first three are me saying just the three things linguists are doing in the 20th century are wrong.

251
00:28:18,000 --> 00:28:22,000
And now the next set of things are going to be things we can do now that they can't.

252
00:28:22,000 --> 00:28:26,000
So how do you build this stuff?

253
00:28:26,000 --> 00:28:31,000
Well, so here's another sentence just from the corpus.

254
00:28:31,000 --> 00:28:36,000
Saracens built a network of highways to serve blah, blah, blah, blah.

255
00:28:36,000 --> 00:28:39,000
So Saracens built a network.

256
00:28:39,000 --> 00:28:42,000
So this all goes as I've been telling you.

257
00:28:42,000 --> 00:28:45,000
Now you get to all.

258
00:28:45,000 --> 00:28:53,000
What you want to happen is a network to be an entity now.

259
00:28:53,000 --> 00:29:03,000
And so we'll say for the moment that some place around the word of there's an operator that's building things.

260
00:29:03,000 --> 00:29:07,000
And what it's going to do is going to build a network.

261
00:29:07,000 --> 00:29:13,000
Now, it could do it again and build Saracens build a network.

262
00:29:13,000 --> 00:29:22,000
Oh, and if the next word were bricks, that would probably be the right thing to do because you'd have always an advert.

263
00:29:22,000 --> 00:29:25,000
But in this case, it's not the right thing to do.

264
00:29:25,000 --> 00:29:30,000
So you've got a preposition here, and you're just going to build this.

265
00:29:30,000 --> 00:29:35,000
It's all called prepositional attachment, and it's largely an unsolved problem in logistics.

266
00:29:35,000 --> 00:29:42,000
And for us, what we do in the computer program, I'll show you later, is we do it both ways.

267
00:29:42,000 --> 00:29:51,000
And what will happen if you guess wrong sooner or later, that parse dies because there'll be some other conflict that it creates.

268
00:29:51,000 --> 00:29:56,000
Now we keep on going of highways, you get to two.

269
00:29:56,000 --> 00:30:07,000
And now, if we take the adverbial sense, you would say, okay, now the adverb is going to build this whole thing.

270
00:30:07,000 --> 00:30:09,000
And then it does get to do it again.

271
00:30:09,000 --> 00:30:12,000
So you've got, you built a network of highway.

272
00:30:12,000 --> 00:30:14,000
And now it does it again.

273
00:30:14,000 --> 00:30:17,000
And you get certain build a network of highways.

274
00:30:17,000 --> 00:30:18,000
Why?

275
00:30:18,000 --> 00:30:23,000
Well, to advance serve the practical needs of commerce and to do more parsing.

276
00:30:23,000 --> 00:30:29,000
Now, if it had been, they build a network of highways to nowhere.

277
00:30:29,000 --> 00:30:32,000
This would be a preposition.

278
00:30:32,000 --> 00:30:38,000
You wouldn't be, you would build highways to nowhere.

279
00:30:38,000 --> 00:30:42,000
Well, two wouldn't down here would someplace.

280
00:30:42,000 --> 00:30:44,000
And so you would not be doing this.

281
00:30:44,000 --> 00:30:47,000
That's another fork you could take.

282
00:30:47,000 --> 00:30:53,000
But in the present case situation is this one.

283
00:30:53,000 --> 00:30:54,000
Okay, great.

284
00:30:54,000 --> 00:31:01,000
So actually all these operators are sitting right at the same place as the relations.

285
00:31:01,000 --> 00:31:05,000
What, what are the rules for these building operators?

286
00:31:05,000 --> 00:31:10,000
Well, turns out that the building operators are the relations.

287
00:31:10,000 --> 00:31:11,000
They're not just sitting there.

288
00:31:11,000 --> 00:31:13,000
They are the relations.

289
00:31:13,000 --> 00:31:17,000
And we sat down to figure out what those relations.

290
00:31:17,000 --> 00:31:19,000
What those rules are.

291
00:31:19,000 --> 00:31:21,000
Because we figured, okay, this is going to be complicated.

292
00:31:21,000 --> 00:31:25,000
You know, in such and such a circumstance that can only build this far and other

293
00:31:25,000 --> 00:31:30,000
circumstances that can build twice in a row.

294
00:31:30,000 --> 00:31:35,000
Or maybe it only builds from here to here and somebody else builds from there.

295
00:31:35,000 --> 00:31:38,000
Turns out it wasn't complicated.

296
00:31:38,000 --> 00:31:44,000
And so those of you who are engineers or computer guys, or even if you remember

297
00:31:44,000 --> 00:31:51,000
algebra operator precedence hierarchies, you know, you multiply first and then

298
00:31:51,000 --> 00:31:57,000
you do addition and subtraction or you do the exponents first or then you do the

299
00:31:57,000 --> 00:32:00,000
multiplying and dividing and then you do the addition subtraction.

300
00:32:00,000 --> 00:32:05,000
So for given operator somewhere in this precedence hierarchy.

301
00:32:05,000 --> 00:32:07,000
Something just turns out.

302
00:32:07,000 --> 00:32:12,000
I was amazed that you can just write an operator precedence hierarchy.

303
00:32:12,000 --> 00:32:14,000
So that's just another table.

304
00:32:14,000 --> 00:32:21,000
And you can sort of see things like this guy who comes after the law are pretty

305
00:32:21,000 --> 00:32:22,000
low down.

306
00:32:22,000 --> 00:32:26,000
You almost always build over him into something bigger.

307
00:32:26,000 --> 00:32:30,000
Whereas things like periods and question marks.

308
00:32:31,000 --> 00:32:32,000
Well, you don't.

309
00:32:32,000 --> 00:32:36,000
And then other guys are in between.

310
00:32:36,000 --> 00:32:38,000
So.

311
00:32:38,000 --> 00:32:42,000
Then the next thing I'll show you is these Chomsky and.

312
00:32:49,000 --> 00:32:51,000
I had a clock.

313
00:32:51,000 --> 00:32:55,000
I don't know where I put it.

314
00:32:55,000 --> 00:32:58,000
So the transformations.

315
00:33:03,000 --> 00:33:04,000
I did what I never do.

316
00:33:04,000 --> 00:33:07,000
I put it in my pocket because I thought that would be a good idea.

317
00:33:09,000 --> 00:33:10,000
Okay.

318
00:33:10,000 --> 00:33:14,000
Now, so can you do the Chomsky and transformations?

319
00:33:14,000 --> 00:33:22,000
Well, they have a series of rules for how to get cats into here.

320
00:33:22,000 --> 00:33:27,000
They're pretty good rule, but they're a little bit ad hoc sometimes.

321
00:33:27,000 --> 00:33:34,000
But here there's this next amazing thing on the basis of two words flanking.

322
00:33:34,000 --> 00:33:37,000
An empty space when we did the decompression.

323
00:33:37,000 --> 00:33:40,000
Remember, we inserted these various relations.

324
00:33:40,000 --> 00:33:46,000
Well, it turns out that for some reason that I really don't understand.

325
00:33:46,000 --> 00:33:51,000
These same relationships also act at a higher level.

326
00:33:51,000 --> 00:33:54,000
So we can now say cats.

327
00:33:54,000 --> 00:33:57,000
Oh, okay.

328
00:33:57,000 --> 00:34:03,000
We said that they are a component of this whole thing.

329
00:34:03,000 --> 00:34:09,000
Well, they are a component of that.

330
00:34:09,000 --> 00:34:11,000
So cats go into that.

331
00:34:11,000 --> 00:34:15,000
Well, that is a component of this whole thing.

332
00:34:15,000 --> 00:34:16,000
And guess what?

333
00:34:16,000 --> 00:34:19,000
There's even empty space for it.

334
00:34:19,000 --> 00:34:23,000
And so that's called successive, what is it?

335
00:34:23,000 --> 00:34:24,000
I forget what it is.

336
00:34:24,000 --> 00:34:26,000
Anyway, it's standard linguistics.

337
00:34:26,000 --> 00:34:29,000
The point is you do have to do these things in steps.

338
00:34:29,000 --> 00:34:35,000
And here all the algebraic instructions have already been inserted for you.

339
00:34:35,000 --> 00:34:40,000
So poof, all you have to do is the computation.

340
00:34:40,000 --> 00:34:46,000
And similar things.

341
00:34:46,000 --> 00:34:48,000
Well, I won't go into that.

342
00:34:48,000 --> 00:34:53,000
Similar rules can assign, well, actually I may have mentioned it.

343
00:34:53,000 --> 00:34:54,000
Yeah.

344
00:34:54,000 --> 00:34:58,000
So where you start and where you end are governed by some rules.

345
00:34:58,000 --> 00:35:03,000
And it's called C command in linguistics.

346
00:35:03,000 --> 00:35:05,000
There's a complete analog here.

347
00:35:05,000 --> 00:35:08,000
You just have to rewrite it in terms of entities and relations.

348
00:35:08,000 --> 00:35:13,000
And same kinds of rules for deciding on the relationship between pronouns and reference.

349
00:35:13,000 --> 00:35:18,000
If you violate those rules, the sentence isn't grammatical.

350
00:35:18,000 --> 00:35:24,000
So that's how you tell a grammatical sentence from an ungrammatical sentence.

351
00:35:24,000 --> 00:35:26,000
Now, this only English.

352
00:35:26,000 --> 00:35:31,000
So here's Lakota, which is the Sioux language.

353
00:35:31,000 --> 00:35:37,000
So where we would say John found that letter under the bed.

354
00:35:38,000 --> 00:35:44,000
In Sioux, it is John letter that bed the under found.

355
00:35:44,000 --> 00:35:45,000
Okay.

356
00:35:45,000 --> 00:35:49,000
That can be written as EE rel.

357
00:35:49,000 --> 00:35:57,000
There's a phenomenon in linguistics where you can classify languages according to different structures.

358
00:35:57,000 --> 00:36:04,000
And this, you know, E rel E happens to be one set of languages.

359
00:36:04,000 --> 00:36:06,000
E rel is another.

360
00:36:06,000 --> 00:36:14,000
And this is essentially like reverse Polish and, you know, mathematics or computer programming.

361
00:36:14,000 --> 00:36:16,000
But you can do it.

362
00:36:16,000 --> 00:36:20,000
And it's the same general idea.

363
00:36:20,000 --> 00:36:26,000
So let me show you a little bit of what happens with actual parsing.

364
00:36:26,000 --> 00:36:34,000
So Steve Senth, who was at Yale at the time, he's now at Marine Biological Laboratory in Woods Hall.

365
00:36:34,000 --> 00:36:44,000
He's the guy who wrote the original voxel view program that processes images, three dimensional images in terms of boxes that are pixels.

366
00:36:44,000 --> 00:36:52,000
So he wrote a program that's only eight megabytes.

367
00:36:52,000 --> 00:36:54,000
Half of that is the dictionary.

368
00:36:54,000 --> 00:37:04,000
So it's only four megabytes of processor, as opposed to these large language models, which are terabytes of RAM.

369
00:37:04,000 --> 00:37:07,000
So here's just an example that's sort of a hard one.

370
00:37:07,000 --> 00:37:09,000
You probably have to read it twice.

371
00:37:09,000 --> 00:37:13,000
The man who knew the man who Aaron knew knew Clive called.

372
00:37:13,000 --> 00:37:18,000
And there's actually two meanings that you can take out of that.

373
00:37:18,000 --> 00:37:24,000
The man who Aaron knew already, that person knew Clive called.

374
00:37:24,000 --> 00:37:30,000
Or the man who Aaron knew that person knew Clive called.

375
00:37:30,000 --> 00:37:34,000
And this program spits that out.

376
00:37:34,000 --> 00:37:39,000
It also is able to pull out of this who did what to who.

377
00:37:39,000 --> 00:37:43,000
So in this first one, the man knew something.

378
00:37:43,000 --> 00:37:47,000
Clive called somebody and Aaron knew the man.

379
00:37:47,000 --> 00:37:49,000
Whereas this one is different.

380
00:37:49,000 --> 00:37:55,000
The man called somebody Aaron knew something and the man knew Clive.

381
00:37:55,000 --> 00:38:03,000
And you may be familiar with pretty print, which is supposed to be a way of showing hierarchical structure.

382
00:38:03,000 --> 00:38:07,000
Steve came up with this other notation, which I think is much clearer.

383
00:38:07,000 --> 00:38:12,000
If you just look at this, you can say, well, okay, somebody knew something.

384
00:38:12,000 --> 00:38:14,000
Well, what did that person know?

385
00:38:14,000 --> 00:38:17,000
He knew the Clive called somebody.

386
00:38:17,000 --> 00:38:21,000
Well, over here, who knew something?

387
00:38:21,000 --> 00:38:28,000
Well, oh, these guys are all just variations on those left and right component of symbol.

388
00:38:28,000 --> 00:38:35,000
So the man, if you look down at this level, the man is a component of something.

389
00:38:35,000 --> 00:38:38,000
It's a component of who Aaron knew.

390
00:38:38,000 --> 00:38:41,000
Who is a component of Aaron knew?

391
00:38:41,000 --> 00:38:44,000
Because Aaron knew this guy who.

392
00:38:44,000 --> 00:38:49,000
And the man is a component of all that and it's a component of the who, et cetera.

393
00:38:49,000 --> 00:38:52,000
So it's all built in there.

394
00:38:52,000 --> 00:38:59,000
I can, you know, run a program in real time for you if you want.

395
00:38:59,000 --> 00:39:00,000
It's probably simple.

396
00:39:00,000 --> 00:39:05,000
It's just to finish the talk and we can play with that later if you want to run any sentences.

397
00:39:05,000 --> 00:39:10,000
So, um, oh, and there's also a cost function.

398
00:39:10,000 --> 00:39:11,000
That's what this is.

399
00:39:11,000 --> 00:39:16,000
So that if you have to start skipping boxes, you can do it, but it costs you.

400
00:39:16,000 --> 00:39:23,000
And so that's a way of telling whether you're doing something that's legal, but unlikely.

401
00:39:23,000 --> 00:39:27,000
And sooner or later, those things wind up having a huge cost.

402
00:39:27,000 --> 00:39:29,000
And you know, that's not right.

403
00:39:29,000 --> 00:39:32,000
Um, now it doesn't work.

404
00:39:32,000 --> 00:39:40,000
So I'm guessing you guys don't worry about linguistics much, but, um, there's some standard metrics.

405
00:39:40,000 --> 00:39:46,000
Like recall precision for this thing or 97% coverage and consistency of 77%.

406
00:39:46,000 --> 00:39:50,000
Basically, consistency is how many are correct parses do you get?

407
00:39:50,000 --> 00:39:57,000
Well, let me just say that when people do this, they use things like the pen tree bank corpus,

408
00:39:57,000 --> 00:40:04,000
which doesn't even bother with compound nominals like doghouse or prepositional attachments.

409
00:40:04,000 --> 00:40:07,000
You know, like I was showing you the two different ways of doing off.

410
00:40:07,000 --> 00:40:09,000
It doesn't even annotate for that.

411
00:40:09,000 --> 00:40:11,000
I've annotated for that.

412
00:40:11,000 --> 00:40:14,000
And it's still 77%.

413
00:40:14,000 --> 00:40:17,000
Um, if I didn't annotate for that, probably 97%.

414
00:40:17,000 --> 00:40:21,000
And then there's across brackets, which has to do with, well,

415
00:40:22,000 --> 00:40:28,000
how many subunits or is the computer trying to get you to say overlap each other when they shouldn't.

416
00:40:28,000 --> 00:40:30,000
And it's rather small.

417
00:40:30,000 --> 00:40:35,000
And it only happens in really long sentences where people get confused.

418
00:40:35,000 --> 00:40:37,000
So show you some data.

419
00:40:37,000 --> 00:40:48,000
So I told you there were lots of, because of word ambiguity and, um, the resulting parse ambiguity because parses build things differently than entities.

420
00:40:49,000 --> 00:40:52,000
There's a lot of alternative parses.

421
00:40:52,000 --> 00:40:55,000
So on the X axis, you have the number of words.

422
00:40:55,000 --> 00:40:57,000
So let's take out 15 word sentence.

423
00:40:57,000 --> 00:41:03,000
And so the sentences I tried, they have about 10 to the seventh possible parses.

424
00:41:03,000 --> 00:41:06,000
Oh, I forgot to show you.

425
00:41:06,000 --> 00:41:07,000
Back here.

426
00:41:07,000 --> 00:41:16,000
It's saying that for the man who Aaron knew new collide called it's saying that there are 3,800 potential parses and it's pulled out to

427
00:41:16,000 --> 00:41:21,000
both of them are, um, you know, correct.

428
00:41:21,000 --> 00:41:23,000
So back here.

429
00:41:23,000 --> 00:41:31,000
So you've got 10 to the seventh potential parses and you've dropped that down to about.

430
00:41:31,000 --> 00:41:34,000
Let's see.

431
00:41:34,000 --> 00:41:36,000
10 to the so one.

432
00:41:36,000 --> 00:41:43,000
Yeah, so this is dropped it down to about 10 parses here.

433
00:41:43,000 --> 00:41:46,000
Um, so that's pretty impressive.

434
00:41:46,000 --> 00:41:57,000
So what you're doing is removing with just this sequencing track and these cognitive rules.

435
00:41:57,000 --> 00:42:08,000
You're removing orders of magnitude of the ambiguity in the words and the alternative ways of parsing the sense and understanding the sense.

436
00:42:08,000 --> 00:42:11,000
So now you're only down to here.

437
00:42:11,000 --> 00:42:27,000
And so that any knowledge you have to have about, well, what's the guy probably trying to say or any of the Bayesian stuff only has to solve another like 10 fold of this.

438
00:42:27,000 --> 00:42:35,000
This ratio here turns out to be a reduction of 2.74 fold per word.

439
00:42:35,000 --> 00:42:37,000
So three fold.

440
00:42:37,000 --> 00:42:43,000
But that three, if you start raising it to the 15th power, it gets to be a big number.

441
00:42:43,000 --> 00:42:48,000
Then as I stared at that, I was thinking, Oh, 2.74 that number looks kind of familiar.

442
00:42:48,000 --> 00:42:54,000
And then I realized that this reduction here is one over e to the 1.008.

443
00:42:54,000 --> 00:42:56,000
In other words, it's one over e.

444
00:42:56,000 --> 00:43:04,000
And so the interpretation of that, there's a kind of like a target theory interpretation of that statistics.

445
00:43:04,000 --> 00:43:22,000
You're trying to hit a target with radiation or something. Basically, there are enough constraints that in this, in the cognitive rules, I told you about that you are just going to so castically eliminate sentences that are wrong.

446
00:43:22,000 --> 00:43:26,000
And so you're reducing things down to one over e.

447
00:43:26,000 --> 00:43:45,000
And the, then what's left, and that is what happens when you have independent events, though it was treating words as more or less independent.

448
00:43:45,000 --> 00:43:48,000
And the collisions are coming from the sequencing track rules.

449
00:43:48,000 --> 00:43:59,000
Then everything else you need to get from this 10 down to a single interpretation is either the meaning or things like us.

450
00:43:59,000 --> 00:44:12,000
Subject verb case agreement or subject for number agreement or cases, things like that that are relationships between a couple of words are not independent events.

451
00:44:12,000 --> 00:44:19,000
So that's what the rest of linguistics is doing is getting you from here down to here.

452
00:44:19,000 --> 00:44:36,000
Now, I hope at this point I've convinced you that, gee, language is maybe a whole lot more regular than we thought, and, and having a regular algorithm can process way more of it than we thought.

453
00:44:36,000 --> 00:44:53,000
You could then wonder, oh, and that what is processing is the, or building, are these representations rather than words per se.

454
00:44:53,000 --> 00:44:57,000
And then now you begin. So let's wonder about AI.

455
00:44:57,000 --> 00:45:14,000
And what have large language models discovered, all they would have to do is discover or teach themselves to make a sequencing track like this, and a couple of rules, and a sequencing track, what is that it's just a series of probability.

456
00:45:14,000 --> 00:45:27,000
Well, if I'm, if I am an entity word now, the probability is 1.0 that the word that the next word is going to be a relation and the probability of that is blah, blah.

457
00:45:27,000 --> 00:45:37,000
So, you know, it's sort of Bayesian, but it's, you know, as somebody told me, well, rules are just probabilities of 1.0.

458
00:45:37,000 --> 00:45:44,000
So a statistical learning machine could come up with this track.

459
00:45:44,000 --> 00:46:00,000
And then what are the implications of that? Well, let me show you one more thing about how this program works. So we're trying to build a sentence, build the parts into higher and higher hierarchies.

460
00:46:00,000 --> 00:46:07,000
So what we're doing, I'm going to show you that basically we have a shift register for grounded symbols.

461
00:46:07,000 --> 00:46:20,000
So you have cats, cats that because the dictionary definition of that is that has these two component things because cat is going to go into that, and that is going to go into something else.

462
00:46:20,000 --> 00:46:28,000
You've got my dogs, blah, blah, blah, you keep building here and now Chase is a relation.

463
00:46:28,000 --> 00:46:31,000
These guys don't build much.

464
00:46:31,000 --> 00:46:37,000
Chase's will build things. And what you want it to do is build my dog.

465
00:46:37,000 --> 00:46:52,000
And this is, it does the shift register thing. It's going to build, put by this relate the composed of a dog, all in the same box, because that grouping is now an entity.

466
00:46:52,000 --> 00:46:57,000
And it shifts these guys over to.

467
00:46:57,000 --> 00:47:01,000
And then keeps going. It gets to R.

468
00:47:01,000 --> 00:47:12,000
R is going to build. And now it's going to shift the parentheses and chases all into this box and put a new set of parentheses around it.

469
00:47:12,000 --> 00:47:21,000
Oh, these things are all just parentheses are just different shape parentheses to make it easy to follow.

470
00:47:21,000 --> 00:47:34,000
And then we're going to keep on going because our goes all the way to the beginning of the sentence that's going to shift it over again, lump all these things together, shift over again lump these things all together.

471
00:47:34,000 --> 00:47:45,000
And then all you need biologically is instead of parentheses you need something that is going to group.

472
00:47:45,000 --> 00:47:53,000
These subgroups into a bigger group without just averaging them like red and white into pink.

473
00:47:53,000 --> 00:48:09,000
Well, if then you think about whether representations could be doing this, whether your brain could be doing this on representations, in addition to just doing it on words.

474
00:48:09,000 --> 00:48:16,000
These words are really just representations to there's no words in your head. There's just like neural spikes. Right.

475
00:48:16,000 --> 00:48:21,000
So these guys so you've already got representations getting dropped in here.

476
00:48:21,000 --> 00:48:34,000
What happens. So it doesn't seem to me a very big step to be dropping these representations into the same sequencing track and doing the processing.

477
00:48:34,000 --> 00:48:46,000
So if a large language model happened to event the sequencing time, it could also support processing of sensory representations instead of word representations.

478
00:48:46,000 --> 00:48:50,000
And I've just shown you that the rules are the same.

479
00:48:50,000 --> 00:48:55,000
And then if that then let's assemble complex representations.

480
00:48:55,000 --> 00:49:04,000
And it does so in a way that you can now define a thing, which is the one remaining part, which I have an idea about how I could do it.

481
00:49:04,000 --> 00:49:08,000
Because what we think about our thing.

482
00:49:08,000 --> 00:49:21,000
But it can now retains all the information to do discrimination between them and do generalizations so that fulfills harness definition of what it takes to do cognition.

483
00:49:21,000 --> 00:49:34,000
So I'm quite concerned that his large language model model are accidentally going to stumble across a way to be doing cognition.

484
00:49:34,000 --> 00:49:38,000
So that is, I believe it. Yep.

485
00:49:38,000 --> 00:49:46,000
And open to any questions, or if you want to see the parts or an action that's fine.

486
00:49:46,000 --> 00:49:48,000
And I will.

