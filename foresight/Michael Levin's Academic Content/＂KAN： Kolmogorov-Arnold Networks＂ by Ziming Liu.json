{"text": " Yeah. So in case you don't know me, I'm Zemin Liu. I'm now a fourth year PhD student at MIT. Welcome, Professor Max Tagmark. My research interests the center around AI plus science, so it can go both ways. Either you develop better AIs for science, like AI scientists, or you use inspirations or like tools in science to enhance the improvement of AI. So like this work is like both ways. We first try to use some ideas from math and see if we can develop new AI tools and see if we can, whether these new tools can give something back to to science. Yeah. So this is about a recent work called The Comagra of Arnold Networks. Today's AI is built upon this math theorem called the universal approximation theorem, which basically says that a high-dimensional function, you can decompose a high-dimensional function into a linear combination of some nonlinear features with this nonlinear function sigma, which is basically just this two-layer network known as the multilayer perceptron. But you can make it deeper. That's why it got the name multilayer. But we are asking this question, are there any alternative theorems we can use? We can leverage these new theorems. Well, not necessarily new in the mathematical sense, but like in the AI world to build another AI skyscraper based on new building blocks. So here we are examining this theorem called The Comagra of Arnold Representation Theorem. First question, what is the KA representation theorem? It says, again, like you given a high-dimensional function, you can write it down as a finite composition of one-dimensional continuous functions and just the summing operation. So more concretely, you can write down an n-dimensional function in this form, where you have the outer functions capital phi q and the inner functions phi qp. They are both just one-dimensional functions, and they are finite. Like the number of these functions is depending on the number of inputs variables n here. So what's nice about this theorem is that it tells us that the only true multivariate functions is the sum. Like you can express, say, multiplication with just the summing operation plus some 1D activation functions. So that sounds like really great news for approximating high-dimensional functions, especially for machine learning, because in high dimensions, to learn high-dimensional functions, we know we have curse of dimensionality. But this theorem seems to tell us that you can decompose high-dimensional functions into one-dimensional functions, which do not suffer from curse of dimensionality, which are great. But immediately, there is this plot twist. Back in 1989, professor Jarosi and Poggio wrote this paper examining the relevance of Comagra's theorem in the context of learning. And they conclude that we review Comagra's theorem and show that it is irrelevant in the context of networks for learning. So this paper basically sentenced the Ka theorem to death, at least sentenced to jail for like 30 years. So is that the end of the story? Well, we want to tell you that that's not the end of the story. And there's again a plot twist. So let's try to see what they say in their paper, why they sentenced the theorem to jail. So their argument was that the theorem fails to be true when the inner functions are required to be smooth. Basically, the original theorem says that you can decompose a high-dimensional continuous function into one-dimensional functions, but it doesn't guarantee you to decompose it into one-dimensional functions. But to learn something, like with gradient descent, you would need the functions to be smooth. Only being continuous is not enough. Even with continuous functions, you can still have some really pathological behavior. You don't have the smoothness constraint. So that's basically the argument, because the theory does not tell you anything about the smoothness of the functions. But in practice, only smooth functions are feasible in practice. So one interesting word they use is that a stable and usable exact representations seems hopeless. So that's like the attacking point. That's the angle we're taking. Maybe we do not need exact representations. Sometimes approximate representations might suffice. Yeah. So their paper was theoretically sound, was sound theoretically, but I'm trained as a physics student. So my level of rigor is lower than mathematicians. So me and also Max, we tend to have lower bar of being rigorous. So we have this naive optimism. So firstly, maybe like empirically, maybe we don't care about exact representations. Sometimes approximate ones may suffice. As long as it has some levels of explainability or interpretability, that's fine. And secondly, a common wisdom in modern deep learning is just stacking more layers to get different networks. They are basically arguing against the two-layer commograph network. But what about we build deeper networks? Maybe for deeper networks, even under the smooth constraint, you can win the expressive power back. Lastly, just the can-do spirit. We cannot say something doesn't work before really building it with the state-of-the-art techniques. They have the negative claim back like 30 years ago. Back then, they didn't even have the back propagation. Like at least it was not popularized back then. So we want to take everything we have in the modern area of deep learning and see how far we can go. So that's the mindset. So just with the can-do spirit, we propose or more properly rediscover or we contextualize the idea of commograph networks in today's deep learning world. Yeah. So here's the overview. First, I will introduce the math foundation. Well, I already briefly mentioned it. I won't dig deeper into it, but just I will emphasize again like the mathematical beauty of it. And then I will talk about the properties of cans. Why? And in what scenarios cans are more accurate and interpretable than current deep learning models? Yeah. So first, the math foundation, I already covered this part, but I want to emphasize again that the theorem looks really nice. That allows you to decompose a high-dimensional functions into one-dimensional functions. And after the decomposition is done, your only need to just your only job would be just approximating the 1D functions. So that's the idea of that's the idea of the commograph networks. Like decomposition first and then learn the 1D functions. Well, yeah. So this representation looks a bit complicated. You'll see that there are this huge, this big summation symbol and you have two layers of composition. And this will be complicated. But don't worry about it. It's just equivalent to this two-layer network. Let's suppose we have two inputs, x1 and x2, and we have the outputs at the top here. So the representation in the original theorem is basically just that you have five hidden neurons in the middle. And to get the activations in each hidden neuron, you basically apply a 1D, possibly nonlinear function to x1 and x2, and sum up these two nonlinear activations to get the activations at the node. And in the second layer, it's similar that you apply some nonlinear function to the hidden activations, summing everything up at the output node, and that's how you get the output. So the computation graph is super clear with just this diagram. And this might remind you a lot of, this looks just like a multi-layer perceptron, the fully connected networks, where everything just fully connected. But instead of having activation functions on the nodes, now we are having activation functions on the edges. And on the nodes, you simply just have the summation operation, which is really simple. Yeah, just to make it more elegant and beautiful, we can, or more intuitive, we can basically, because like 1D functions, we can basically just visualize them with 1D curves, with the x and x as the input and y as the output. So now can network is basically, you can picture it as this. And by staring at it, you can, you can have some idea what's happening inside. Yeah, so I mentioned this is just a, so the theorem, so the representation is equivalent to a two-layer network. But can we go deeper? The answer is yes. Algorithmically speaking, because it's just a stack of two layers, which, from which we can abstract a notion called the can layer. So the original two-layer network is basically a stack of two, two can layers. And for each can layer, it's basically just taking some number of inputs and outputs, some number of outputs, and in between is fully connected. And on each edge, you have the active, you have the some nonlinear, learnable nonlinear activation function. And in the end, and in the outputs, you, you, you summing up the incoming activations. That's how a can layer works. And you can simply stack more and more can layers to get deeper and deeper cans. This is just a three-layer can, like, like the first layer, we're taking two, output three. The second layer, input two, output three. And the last layer, you input three, output one. So this is the, this is a three-layer network, which approximates a scalar function in two dimensions. But obviously, you can easily extend it to arbitrary dimension, like arbitrary input, arbitrary output, arbitrary width, arbitrary depth. So, so you have all the flexibility to choose the size of the network. Yeah, one, the first question Professor Poggio asked me when I presented this network to him, he asked, is that why do you need this deep networks? Because the original theorem told you that you only need the two-layer constructions. And here's just a quick answer that I, I can give you an example. So please look at this symbolic formula. And if you examine it, you would immediately realize that you would need at least three compositions to do this, to construct this formula. You need the, you need the squared function, you need the sine function, you need the exponential function. They're like, because they're, because it's the compositional structure, they're in different layers. So you would at least, at least need three layers to, to, to learn this formula. And indeed, if you just use two-layer network, the activation functions becomes really oscillatory, becomes really pathological. And the performance is bad, and also the interpretability is bad. But in the right, I show that a three-layer network, a three-layer can train on this, train on this dataset. And after training, you would immediately see the learned activation functions. In the first layer, you got the squared, in the second layer, you got the sine, and the last layer, you got the exponential. Well, well, you may think of, you may think this is some other functions, maybe just some local quadratic, but yeah. But you can, but you can, like, do the template matching with the candidate functions and figure out which one fits the best. Yeah, so, so I, I said that this activation functions are learnable. How do we make them learnable? Because they're functions. And, and, and, and the common wisdom is that we need to make, we need to parametrize the things to be learned so that we can use gradient descent to learn, to learn this stuff. So the idea is that we parametrize a 1D function with, with B splines. So B splines is basically some piecewise, some, some local piecewise polynomial functions. So, so here I showed that there are some local B spline bases. And the way we construct the activation functions is by linearly combining this, this, this B spline functions. And the only learnable parameters are just the linear coefficients of, of, of this local basis. And what's nice about this formulation is that we, we inherit the advantage of B splines. We can easily switch between fine grains, fine grain grids and coarse grain grids. If you want something to be more accurate, you can, you can choose the mesh to be more fine grain. If you want the model to be smaller, so you can have a faster inference, you can, you can, you can choose a more coarse-grained model. Yeah, that's basically the idea of cans. And we can compare MLPs and cans side by side, because they do share some similarities, but also share some difference, but also have some differences. So MLPs are inspired by the universal approximation theorem cans, again, inspired by the Camargo Ravano representation theorem. The network looks a bit similar in the sense that they're both fully connected. But their dual, they're different, but they're dual in the sense that MLPs have fixed activation functions on nodes. Well, you can make them trainable, but but they're on nodes for sure. And in MLPs, we have learnable weights, learnable linear weights on edges. By contrast, cans have learnable activation functions on edges, while cans have this simple linear summation operation on nodes. So, so, so in this sense, cans does not separate the linear part and the long inner part as MLPs do, but it integrates both the linear part and long inner part altogether into the can layer. And the can network is simply just the stack of the can layers. Yeah, so in both cases, you are free to stack the model to become deeper and deeper because you have the basic notion of a layer, you just stack more layers to get the deeper networks. Yeah, so that's the basic, that's the basic ideas of cans. And now I want to elaborate more like why do we care about, why do we care about this? What are the advantages that cans can bring to us, but other black box models do not bring to us? So yeah, so the first property is the scaling behavior of cans. As I mentioned before, the idea of cans is decomposing a high dimensional functions into one dimensional functions. So that looks like really promising that it can get us, it can get us out of the curse of dimensionality. Let's suppose we are trying to approximate a d dimensional functions. And I suppose the function has no structure at all. So then we need to, we need to have a hypercube and have a uniform grid on the hypercube. Let's suppose we have 10 grid, 10 anchor points along each dimension, then we will need 10 to the power of d number of anchor points in the d dimensional hypercube. So that's exponentially expensive. So if you do the classical approximation theory, you would notice that the approximation error would decay as a parallel, as a parallel, as a function of the number of input dimensionality. And it's one, and the exponent is one over d, meaning that you got exponentially like slower when you have more and more, when you have more and more dimensions, like if you need 10 points in one d, you would need 100 points in 2d, you will need 1000 points in 3d and so on. But if the function has some structure, like if it has the chromograph unknown representation, then we can decompose it into a bunch of 1d functions. And then our job would just be approximating 1d functions. So now effectively d becomes one. So you got a really, you got the fastest possible scaling laws. But the, but the caveats, immediately the caveat is that we, the assumption is that we, like the function has a smooth chromograph, a smooth finite size chromograph unknown representation. All of this, you know, all of these objectives, objectives like smooth or finite size are just, are just practical conditions for a real, for, for a network which we have access in practice that can really learn the network. We want it to be smooth because we parametrize it with b-splice, which are smooth. We want them to be finite size because, of course, you know, we cannot initialize, we cannot deal with an infinite size neural network. So, yeah, so, so, so we just did some sandwich check on, on some symbolic formulas. So, yeah, so symbolic formulas are like white, are like what we used in, in science. So that's why we test them first. So let's see. So the red dashed line is the theoretical prediction. Here we are using cubic spline. So k is k3. And the scaling exponent is k plus one equals four. And the curve, yeah, so the thick blue line is for the can network. And you see that almost like the, like empirical results for the can network almost agreed with, almost agrees with the theoretical prediction, although sometimes performed slightly worse. Or in this case, in the second to last case, there's a hundred dimensional case. And it performs much worse than a theoretical prediction because of the, the, the, the, because the dimension is just too high and the network can get, can get stuck at some local minima or whatever. But, but nevertheless, it's still output from MLPs, which you see up in the upright corner here, like the case really slow. But, but the cans, but cans at least can output form MLPs to a great margin, although still not saturating the theoretical prediction. But still the scaling law that can shows looks, looks promising that it's, it seems to not fully beats the curse of dimensionality, but at least partially beat the curse of dimensionality. Well, yeah, yeah. So, yeah, just to play the devil's advocate here, you may immediately notice that I'm on purpose just deliberately using this symbolic formulas. You, you might be wondering, well, maybe, maybe the functions we care. We encounter a lot in nature may not be symbolic, they might be some weird, you know, like, at least for special functions, they, they, they, they are like infinite series, which are hard to be represented with just the finite network. So, what are, what, what, so, so, so, so what's the cans performance in that scenario? So, yeah, so we just tried some special functions, which we know for most of them, they do not have analytical formulas. And indeed, we see that the scaling laws of cans do not saturate the theoretical prediction, meaning that probably you cannot decompose like a high dimensional functions into just a one D functions. But still, our goal here is to outcompetes MLPs. And, and, and it's a feature, not a bug, like not all the functions can be decomposed into smooth finite representations of this K representation. They may admit non smooth finite size or smooth infinite size. But in, but, but neither case is, is like the, is what's accessible in practice. So, yeah, so, so here we show that in most cases, we can achieve this minus two scaling law, which means that the can network, well, well, sorry. So here, this special functions are all just two dimensional. So this, so these, according to the spline theory, it would predict two dimensional, it would predicts a two dimension, like, like, like a scaling exponent to be two, which agrees with the can results. But in some, but in some case, we can, we can still got the minus four scaling law. And the reason is that actually, this is secretly, like, like, although we call, we call it a special function, spherical harmonics are not that special in the sense that they're still decomposable. So, so you can get the minus four scaling. But also in other, in other cases, you've got some worse behavior, like you've got the minus one scaling, which means that the can is underperforming even out on the performing compared to like the spline theory. But what's interesting is that in this case, MLPs are even more underperforming than cans. So, so this may tell us that maybe for low dimensional problems, neural networks are not that necessary. And even the spline theory, like the spline approximation can outcompete the neural network. So, so it's something to be think about. It's something good to keep in mind. Because neural networks just have too many degrees of freedom and may have optimization issue. Yeah. So, so beyond function approximation, we can go on to solving partial differential equations. So in the setup of physics in form learning, basically, we're trying to solve, we're basically trying to represent the solution of a PDE with MLP or with a can network. So, so the only difference from the previous results from the previous experiment is that we're just we're using the physics informed loss rather than the regression loss. So the optimization becomes more complicated, but still it's just approximating some function. Yeah. So, so we still see that we with cans, we can get this optimal scaling law. Well, with MLPs, you see, while with MLPs, you see that, well, it has the skill at first, but it plateaus really fast and then does not improve when you have more parameters beyond 10,000. Um, besides being more accurate, we can also gain some insight what the network is learning. So, so, yes, yes. So for this example, we can, for this PDE example, we can actually visualize the can network like this. And immediately you can see that there's some like sine waves and there's some linear functions. And you can even do symbolic regression to it, like, like, like our software provides you a way to do this, you can, you can do symbolic regression to it. And after you do this, you can do some further training. And you, and you can even extract out the symbolic formula, which gives you like a loss down to machine precision. This is something that that that standard neural networks would not give you because of because they usually cannot convert a neural network into a symbolic formula very easily, but with cans, you can easily do that. Another property of cans is that it has this property of continual learning, at least in 1D. Yeah, so people have, people have found that, you know, this claim might be invalid for high dimensions. So take my word with a grain of salt. But at least for 1D, the case is like, we have, we want to approximate this 1D functions with five peaks. But instead of feeding auto data at once to the network, we're feeding each time we just feed one peak to the network. And we do the sequential learning at each stage, the network is just, is just fed with just one peak. And with cans, and with cans, because we're using the local B-splines, so when it sees the new data, it does not update the parameters correspond to the old data. So it has this, it can't get rid of this catastrophic forgetting, like when new data are coming in, the can is able to memorize the old data and still do quite well on the old data. But this is not true for MLPs. Like when you're, when you're fed, when the MLPs are fed with the new data, it catastrophically, they catastrophically forget about the old data. Because in MLPs, you usually have this global activation functions like the silo functions or Reilu functions. So whenever you make adjustments locally, it will affect, you know, the predictions far away. So that's the reason why MLPs would not keep would, would catastrophically forget about the old data. Yeah, so, so, so that's the first part about accuracy. The second part is about interpretability. You might already have some sense because we are able to visualize the network as a diagram. So, the hope is that you can just stare at a diagram and gain some insight of what's happening inside the neural network. Yes, yeah, so here we have some, some, some toy examples of, yeah, for example, how do cans do the multiplication computation. So we have x and y as the input and the x times y as the output. So when we train the can network, we also have some, something similar to L1 regularization to sparsify the network. So we can extract out the minimal network to do the task. So in the multiplication case, we see that only two neurons are active in the end, and we can read off the symbolic formulas of how, and get a sense of how it does the computation. Well, I marked the symbolic formulas here. It may take a while. It may take a while, but I, well, the point is that it basically just some squared functions and, and the way the can network learns to compute this multiplication is by leveraging some, some squared equality here. And the second example, the can is tasked with the division task, where we input two positive numbers, x and y, and can is asked to predict the x divided by y. And because here x and y are both positive, the can network learns to first take the logarithm transformation and then take the subtracted two logarithm and then transform the same back via the exponential, exponentiation. So, so, so that's really cute. Like, like one example of the commograph theorem is that you can basically do the multiplication of positive numbers or the division of positive numbers in the logarithmic scale, because that's, because the division or multiplication in the logarithmic scale, it transferred to, would translate to like addition and subtraction in, in the logarithmic scale. Yeah, so, so, so, so this examples are really simple. You might be wondering what about a more complicated formula, then, then it might be very complicated to decode what the cans have learned. So, I would want to argue that this might be a feature, not a bug, or you can call it a bug, but I can, but I won't call it a feature in the sense that, sorry, I didn't show the network here, but let's suppose we're doing this formula here, like u plus v divided by 1 plus uv. So, if you're familiar with relativity, you are special relativity on, you are, you are, you are, you are realized that this is the velocity, relativistic velocity addition. So, at first, I thought that I need the five-layer can to fix this function, because you would need multiplication, which would give, which would consume two layers, you would, two additions, consume another two layers, and also you, you, you would need the division. So, so that's in total five layers, but it turned out you can only, you can, you can just use a two-layer can to fix this function perfectly well. And in the end, I realized that this is just, this is just a rapidity trick known in special relativity, where you first do the arc-tange transformation to u and v separately, sum the thing to, to rapidity up, and then you do the tange transformation back to, to get this formula. So, in this sense, it's rediscovering the, it's, it's rediscovering the rapidity trick, known, well-known in, well-known in the special relativity. And in some cases, I indeed find that the network finds some more compressed, compact representation than I would expect. That's good news in the sense that's the, the, in the sense that the network is discovering something more compact. So, the representation is more powerful than I have expected. But the bad news is that sometimes the, the interpretation can be subtle, can be a bit more complicated. But, but I mean, it's, it can be a, it can be a feature, but depending on your view. Yeah, so, so, so, so this is one paragraph I take from the paper. This is criticized, this has been criticized a lot in the social media, but I, but I find this analogy really interesting. So, I still want to highlight this part. So, so to me, I think can is like a, it's sort of like language model or, or like language or even like language for AI plus science. The reason why language models are so transformative and powerful is because they are useful to anyone who can speak natural language. But the, but the language of science is functions or more advanced mathematical objects build on functions because cans are composed of functions which are 1D, so they are interpretable. So, when a human user stares at a can, it's like communicating it with, it's like communicating it with the, using the language of functions. So, to elaborate more and to make it more entertaining, let's suppose we are like my advisor Max is communicating with the can network. Here, I picture the can network as a trisolarian from the three-body problem. I'm not sure if guys have watched it, but basically the trisolarians, their brains are transparent, so they cannot hide any secrets from others. So, they are totally transparent. So, so Max went up, give, give the can, give the can a dataset. Max was like, here is my dataset. It contains the mystery of the universe. I want you, the can network, to figure out, to figure out the structure of the datasets and, and the can initialize the can network like this. So, here's the brain of the can network initially. And Max, given the dataset, Max wants to train the can network, to train the brain. And after training, you can get this sparse network, which you start to see some structure. But still, it's a bit, there's still some residual connections, which looks quite, which quite annoying. So, Max asked the can network to prune the redundant connections. And after pruning, you got, after pruning, you got this, you got this sub network, which is responsible for the computation. And Max further asked the network to the can network to symbolify it, because it looks like, it looks like in the bottom left, it looks like just a sign function. And in the bottom right, this looks just like a parabola. And this just looks like an exponential. So, maybe you can symbolify it to gain some more insight. So, yes, so the can network said, yes, I can symbolify it. And you can. And now the dataset goes all the way down to the symbolic formula. But, but we can imagine another conversation Max would have with an MLP. So, Max went up and give the dataset to MLP, and want the MLP to figure out the symbolic formula in it. And like before, the MLP initialized the brain, looked like something like this, really messy. After training, Max asked MLP to train the brain, but even after training, the connection still looks really messy. And MLPs were like, and the MLP is like, I really trained it, but the loss is pretty low. But it's just that the connections are still very complicated. Now Max got confused. Like, what's going on? What's going on with your brain? And now MLP is like, it's just that your humans are too stupid to understand my computations. You cannot say, I'm wrong, simply because you cannot understand me. So, Max now got really pissed off and turned back to CANS. So, those are just some imaginary stories I made up with the symphatic example. And that symphatic example is really simple. But I want to show that we can really use CANS as a collaborator in scientific research. And CANS can give us some non-trivial results. CANS can give us some new discoveries. So, the first example is, yeah, so this example was used in a DeepMind Nature paper three years ago, where they used MLP to discover a relationship in a NOT dataset. So, each NOT has some invariance. Basically, each NOT is associated some numbers. And these numbers, they have some relations. And we want to dig out the relations among these variables. So, what the DeepMind people did was they used the train and MLP and used the attribution methods, basically take the gradient with respect to these input variables, and use that as a score to attribute these features. And then rank these features to get a sense of which features are more important than other features. And they identified three important features. That's the only thing that's automated in their framework. And then the human scientist came in and tried to come up with a symbolic formula for it. So, we're asking this question, can we discover, have we discovered these results with more automation, with less, you know, efforts, and probably even discovering something new that the DeepMind paper were missing? So, first, we are able to discover the three important variables with the CAN network, with much more intuition and automation. So, their network, they used a three-layer, sorry, they used a five-layer MLP, and each hidden layer has 300 neurons. So, that's really hard to interpret. That's why they used the feature attribution. But we find that surprisingly, we only needed one hidden layer and one hidden neuron, the CAN network, to do the task, as well as their five-layer, like, a million-parameter MLPs. And with this, we can also clearly see the activations, the importance now basically becomes, you can basically understand the importance of these variables with the L1 norm of these activation functions. So, that's also how we visualize the connections. So, you can basically read off from this diagram that the strong connections are the important variables, while the weaker or even nearly transparent connections, meaning that irrelevant variables. Yeah, we can also discover symbolic formulas, as I said before, because the CAN network decomposes high-dimensional functions into 1D, and then we can just do template matching in 1D to see what each 1D functions represent symbolic formulas, and then compose all these 1D functions back to get these high-dimensional functions. Something beyond their paper we discovered is that their setup is a supervised learning setup. Basically, they need to partition the variables into inputs and outputs, and they use the inputs to predict at the outputs. But in some cases, we do not know how to partition the inputs and outputs, like all the variables, they are treated equally. So, we want to develop this unsupervised setup where all the variables serve as inputs. But we use some notion of contrastive learning to classify whether some given input is a real knot or a fake knot, or that might be too technical. The result is that we are able to discover more relations beyond the relation they've discovered, because they manually partition one variable as the output, so they can only discover the relations that involve that variable. But here, we are learning it in an unsupervised way, so we can learn more than just one relation. We also discovered the relation between these three variables and between these two variables. Unfortunately, or fortunately, these relations are already known in the literature of knot theory. So, the unfortunate part is that we did not discover anything new with our framework, but notice our network is just very preliminary, it's just a one layer, it's just a one layer if we ignore about the classifier in the second layer. So, hypothetically, we can make it deeper to get more complicated relations. But the fortunate part is that it verifies that's what we discovered with the network is something, is not bullshit, it's something that makes sense that people already know in the literature. Yeah, we also did this physics example, specifically Anderson localization. I don't want to bore you with the technical detail, but again, the goal here is to try to figure out the symbolic formula of the phase transition boundary. In Anderson localization, we have the localization phase and the extended phase, and there is a phase boundary, and we want to extract out the phase boundary, especially the symbolic formula of the phase boundary, if there exists a symbolic formula for it from the road data. Yeah, so for this slide, I don't want to go to the detail, but the point I want to make with this slide is that the CAN network has, just like cars, you have manual mode, you have manual mode, you have automatic mode, like if you're lazy, you can just delegate everything to CANS, and CANS will return you a symbolic formula fully automated, but that might not be correct. That might not be what you want. You can do that, it can give you reasonable accuracy, but may not be fully interpretable, but if you want to have some controllability over the CAN network, where you want to be more involved, you want to have some intervention, you can still do that. You can choose to, you can use the manual mode, where you just handpicked some activation functions, like some functions, obviously they're just quadratic, linear, you can set them to be exactly the linear or quadratic, and then you retrain the other activation functions, and you see, and after retraining, you will see what those activation functions would change to different, would change to different form, and then this gives you, again, gives you some insight, like give you better evidence what the next guess you would want to make, and this is like the iterative process, like it's sort of like you are arguing or you are debating with the CAN network, like the CAN network is give you some, or you can say debating or collaborating, just like how we interact with a human collaborator, sometimes we debate, sometimes we collaborate, but you seem to look at the same thing from different angles, like the CAN network is really great at decomposing high dimensional functions into 1D functions, but those 1D functions may not be perfect, and there might be some actual subtlety, but humans are super great at identifying the symbolic stuff, and also recognizing the modular structure from the CAN diagram. So, yeah, so the takeaway is that you can choose to be lazy, use the automatic mode, or you can choose to be more responsible and more involved using the manual mode. Yes, so maybe, yeah, maybe in the end, yeah, I will just finish this really quick, so people have asked why it looks like CANs can, you know, in terms of expressive power, CANs are just MLPs, are just like secretly just MLPs, so why do we need CANs? I want to argue that from a like a high level philosophical level, CANs and MLPs are somewhat different. CAN is like clock work, like in a clock, pieces are customized and have clear purposes corresponding to the learnable activation functions in CANs. So for a clock, it's easy to tear it apart, it's easy to tear the pieces apart and then reassemble the pieces back together to get the clock back, but MLP is like a house made of bricks, so in MLPs, the pieces are produced in the same standard way, like each neuron takes in some linear combinations and then do some do the same nonlinear transformation but it's difficult to tear, once you have a house built, it's difficult to tear apart the bricks and then reassemble them. So in summary, the source of complexity are different in CANs and in MLPs, the source of complexity in CANs come from the complexity of each individual object, like those 1D learnable functions, but because the functions are 1D, no matter how complicated they are, they're 1D and they have clear purposes in some sense, they are nevertheless, they are interpretable, but the complexity of CANs come from complicated interactions of individual parts, the individual parts are simple, but the connections between these individual parts are really complicated, I guess it's more like human brains, it's more like biology, yeah, I don't know, but CANs seem like more aligned with the philosophy of reductionism, where you hope that, where you expect that you can decompose a complicated object into a few like interpretable individual objects, well in MLPs, everything is connected, the reason why MLPs function is because they have this emergent behavior or collective behavior in some sense. Yeah, just some interesting question people ask, are CANs physical? So if we think of like the Feynman diagram as physical, then unfortunately, Feynman diagrams are sort of more like MLPs, because in Feynman diagrams, like on the edges, it's just like a free flow in space without anything interesting happen, but on the nodes where two particles or multiple particles collide, it's where interesting things happen, this is more aligned, this is probably more aligned with MLPs, but CANs is like interesting thing happens on the edges, but not on the nodes. And yeah, last question, people also ask are CANs biological, because people think that MLPs are inspired by the neurons in our brains, are there any biological analogy? And I don't know at first, but someone from Twitter wrote it that CANs actually is a bit analogous to the cells in retina, where each individual cell receive light, apply some nonlinear transformation to it before summing everything up, I'm not sure, you guys are experts, so please correct me if I'm wrong here. But the argument is that, well, maybe the mechanisms of CANs like your first applied nonlinear transformation, then summing everything up, is indeed biological, but I guess that's just for fun. That's just a minor justification why we need CANs, because in some sense, CANs are also biological. Yeah, so that's basically everything I would like to share, and I'm happy to chat more if you guys have questions. Super interesting, thank you. Questions? Well, for the last last example in retina, I have some questions. If the network, like CAN network, is deep enough, is it still matter if you say nonlinearities before or after the summation? Yeah, so I guess the key difference, well, yeah, so I guess the key difference is that in CANs, activation functions are learnable, so I guess, yeah, but whether to put activation functions on edges or on nodes, I don't think that might be the key difference, like the learnability of activation functions give you more flexibility. Yeah, when you talk about this, I was thinking about that CAN is decomposing different variables, input variables, like if you have x and y, then CAN could decompose it, because you would have a different combination of them, but like if you have a nested function, like sine x square, or exponential sine x square, then the CAN seems not able to decompose them, because they don't have these primitives. Yes, yes, yes, that's exactly correct. So CAN can only discover compositionality in the sense that all the 1D functions are boring to CANs. It can just be approximated with just one B-splice, it doesn't learn any compositionality for single variables. That might be one bug, if you will, for CANs, if you really want to figure out the symbolic formulas in the data set. But like for Professor Tomaso Poggio, the author of the 1989 paper who sentenced or who sentenced Theorem to jail, he wrote in his paper that all the 1D functions are boring. And what's interesting is this compositional sparsity when you are dealing with multiple variables, but I guess it depends on your goal. If your goal is just to learn the function efficiently, then it's fine. But if your goal is to really understand if it's sine of exponential, exponential of sine, then we probably need to think about ways to handle this. Yeah, thank you. Have you thought about combining CANs and MLPs, given their somewhat complementary nature, or despite their complementary nature? Yeah, that's a great question. So we have some primitives, like CANs can propose this CAN layer, which is a new primitive. And for MLPs, it has these linear layers and also the nonlinear activations, which are also primitive. I mean, these are like the building blocks. And I guess as long as they fit together, as long as they fit together, you can freely just combine them in ways that you want. But it's just that it's a bit hard to tell what's the advantage of combining. And because I guess there are many ways to integrate the two models and which way is the best. And I guess it's a case dependent question. It again depends on what's your application, what's your goal, something like that.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " Yeah. So in case you don't know me, I'm Zemin Liu. I'm now a fourth year PhD student at MIT.", "tokens": [50364, 865, 13, 407, 294, 1389, 291, 500, 380, 458, 385, 11, 286, 478, 1176, 20008, 18056, 13, 286, 478, 586, 257, 6409, 1064, 14476, 3107, 412, 13100, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21556619095475707, "compression_ratio": 1.3673469387755102, "no_speech_prob": 0.026290133595466614}, {"id": 1, "seek": 0, "start": 10.8, "end": 18.64, "text": " Welcome, Professor Max Tagmark. My research interests the center around AI plus science,", "tokens": [50904, 4027, 11, 8419, 7402, 11204, 5638, 13, 1222, 2132, 8847, 264, 3056, 926, 7318, 1804, 3497, 11, 51296], "temperature": 0.0, "avg_logprob": -0.21556619095475707, "compression_ratio": 1.3673469387755102, "no_speech_prob": 0.026290133595466614}, {"id": 2, "seek": 0, "start": 18.64, "end": 26.080000000000002, "text": " so it can go both ways. Either you develop better AIs for science, like AI scientists,", "tokens": [51296, 370, 309, 393, 352, 1293, 2098, 13, 13746, 291, 1499, 1101, 316, 6802, 337, 3497, 11, 411, 7318, 7708, 11, 51668], "temperature": 0.0, "avg_logprob": -0.21556619095475707, "compression_ratio": 1.3673469387755102, "no_speech_prob": 0.026290133595466614}, {"id": 3, "seek": 2608, "start": 26.08, "end": 34.08, "text": " or you use inspirations or like tools in science to enhance the improvement of AI.", "tokens": [50364, 420, 291, 764, 17432, 763, 420, 411, 3873, 294, 3497, 281, 11985, 264, 10444, 295, 7318, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12400355058557846, "compression_ratio": 1.5722543352601157, "no_speech_prob": 0.004460680764168501}, {"id": 4, "seek": 2608, "start": 35.04, "end": 41.44, "text": " So like this work is like both ways. We first try to use some ideas from math and see if we can", "tokens": [50812, 407, 411, 341, 589, 307, 411, 1293, 2098, 13, 492, 700, 853, 281, 764, 512, 3487, 490, 5221, 293, 536, 498, 321, 393, 51132], "temperature": 0.0, "avg_logprob": -0.12400355058557846, "compression_ratio": 1.5722543352601157, "no_speech_prob": 0.004460680764168501}, {"id": 5, "seek": 2608, "start": 41.44, "end": 49.2, "text": " develop new AI tools and see if we can, whether these new tools can give something back to to", "tokens": [51132, 1499, 777, 7318, 3873, 293, 536, 498, 321, 393, 11, 1968, 613, 777, 3873, 393, 976, 746, 646, 281, 281, 51520], "temperature": 0.0, "avg_logprob": -0.12400355058557846, "compression_ratio": 1.5722543352601157, "no_speech_prob": 0.004460680764168501}, {"id": 6, "seek": 4920, "start": 49.2, "end": 56.0, "text": " science. Yeah. So this is about a recent work called The Comagra of Arnold Networks.", "tokens": [50364, 3497, 13, 865, 13, 407, 341, 307, 466, 257, 5162, 589, 1219, 440, 2432, 559, 424, 295, 30406, 12640, 82, 13, 50704], "temperature": 0.0, "avg_logprob": -0.26815162166472406, "compression_ratio": 1.4530386740331491, "no_speech_prob": 0.013421046547591686}, {"id": 7, "seek": 4920, "start": 61.52, "end": 70.32000000000001, "text": " Today's AI is built upon this math theorem called the universal approximation theorem,", "tokens": [50980, 2692, 311, 7318, 307, 3094, 3564, 341, 5221, 20904, 1219, 264, 11455, 28023, 20904, 11, 51420], "temperature": 0.0, "avg_logprob": -0.26815162166472406, "compression_ratio": 1.4530386740331491, "no_speech_prob": 0.013421046547591686}, {"id": 8, "seek": 4920, "start": 72.32000000000001, "end": 76.56, "text": " which basically says that a high-dimensional function, you can decompose a high-dimensional", "tokens": [51520, 597, 1936, 1619, 300, 257, 1090, 12, 18759, 2445, 11, 291, 393, 22867, 541, 257, 1090, 12, 18759, 51732], "temperature": 0.0, "avg_logprob": -0.26815162166472406, "compression_ratio": 1.4530386740331491, "no_speech_prob": 0.013421046547591686}, {"id": 9, "seek": 7656, "start": 76.56, "end": 84.8, "text": " function into a linear combination of some nonlinear features with this nonlinear function sigma,", "tokens": [50364, 2445, 666, 257, 8213, 6562, 295, 512, 2107, 28263, 4122, 365, 341, 2107, 28263, 2445, 12771, 11, 50776], "temperature": 0.0, "avg_logprob": -0.12051200866699219, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0035929756704717875}, {"id": 10, "seek": 7656, "start": 86.16, "end": 93.28, "text": " which is basically just this two-layer network known as the multilayer perceptron. But you", "tokens": [50844, 597, 307, 1936, 445, 341, 732, 12, 8376, 260, 3209, 2570, 382, 264, 2120, 388, 11167, 43276, 2044, 13, 583, 291, 51200], "temperature": 0.0, "avg_logprob": -0.12051200866699219, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0035929756704717875}, {"id": 11, "seek": 7656, "start": 93.28, "end": 99.2, "text": " can make it deeper. That's why it got the name multilayer. But we are asking this question,", "tokens": [51200, 393, 652, 309, 7731, 13, 663, 311, 983, 309, 658, 264, 1315, 2120, 388, 11167, 13, 583, 321, 366, 3365, 341, 1168, 11, 51496], "temperature": 0.0, "avg_logprob": -0.12051200866699219, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0035929756704717875}, {"id": 12, "seek": 9920, "start": 100.16, "end": 106.08, "text": " are there any alternative theorems we can use? We can leverage these new theorems. Well, not", "tokens": [50412, 366, 456, 604, 8535, 10299, 2592, 321, 393, 764, 30, 492, 393, 13982, 613, 777, 10299, 2592, 13, 1042, 11, 406, 50708], "temperature": 0.0, "avg_logprob": -0.14989013450090274, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.0010648745810613036}, {"id": 13, "seek": 9920, "start": 106.08, "end": 111.04, "text": " necessarily new in the mathematical sense, but like in the AI world to build another AI", "tokens": [50708, 4725, 777, 294, 264, 18894, 2020, 11, 457, 411, 294, 264, 7318, 1002, 281, 1322, 1071, 7318, 50956], "temperature": 0.0, "avg_logprob": -0.14989013450090274, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.0010648745810613036}, {"id": 14, "seek": 9920, "start": 111.76, "end": 117.92, "text": " skyscraper based on new building blocks. So here we are examining this theorem called The", "tokens": [50992, 48227, 39302, 610, 2361, 322, 777, 2390, 8474, 13, 407, 510, 321, 366, 34662, 341, 20904, 1219, 440, 51300], "temperature": 0.0, "avg_logprob": -0.14989013450090274, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.0010648745810613036}, {"id": 15, "seek": 9920, "start": 117.92, "end": 124.56, "text": " Comagra of Arnold Representation Theorem. First question, what is the KA representation theorem?", "tokens": [51300, 2432, 559, 424, 295, 30406, 19945, 399, 440, 37956, 13, 2386, 1168, 11, 437, 307, 264, 31233, 10290, 20904, 30, 51632], "temperature": 0.0, "avg_logprob": -0.14989013450090274, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.0010648745810613036}, {"id": 16, "seek": 12456, "start": 125.12, "end": 131.52, "text": " It says, again, like you given a high-dimensional function, you can write it down as a finite", "tokens": [50392, 467, 1619, 11, 797, 11, 411, 291, 2212, 257, 1090, 12, 18759, 2445, 11, 291, 393, 2464, 309, 760, 382, 257, 19362, 50712], "temperature": 0.0, "avg_logprob": -0.09706091524949714, "compression_ratio": 1.6726190476190477, "no_speech_prob": 0.004608179908245802}, {"id": 17, "seek": 12456, "start": 131.52, "end": 139.12, "text": " composition of one-dimensional continuous functions and just the summing operation. So more", "tokens": [50712, 12686, 295, 472, 12, 18759, 10957, 6828, 293, 445, 264, 2408, 2810, 6916, 13, 407, 544, 51092], "temperature": 0.0, "avg_logprob": -0.09706091524949714, "compression_ratio": 1.6726190476190477, "no_speech_prob": 0.004608179908245802}, {"id": 18, "seek": 12456, "start": 139.12, "end": 146.0, "text": " concretely, you can write down an n-dimensional function in this form, where you have the outer", "tokens": [51092, 39481, 736, 11, 291, 393, 2464, 760, 364, 297, 12, 18759, 2445, 294, 341, 1254, 11, 689, 291, 362, 264, 10847, 51436], "temperature": 0.0, "avg_logprob": -0.09706091524949714, "compression_ratio": 1.6726190476190477, "no_speech_prob": 0.004608179908245802}, {"id": 19, "seek": 14600, "start": 146.0, "end": 155.84, "text": " functions capital phi q and the inner functions phi qp. They are both just one-dimensional functions,", "tokens": [50364, 6828, 4238, 13107, 9505, 293, 264, 7284, 6828, 13107, 9505, 79, 13, 814, 366, 1293, 445, 472, 12, 18759, 6828, 11, 50856], "temperature": 0.0, "avg_logprob": -0.15832942279417123, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.009557297453284264}, {"id": 20, "seek": 14600, "start": 155.84, "end": 163.36, "text": " and they are finite. Like the number of these functions is depending on the number of inputs", "tokens": [50856, 293, 436, 366, 19362, 13, 1743, 264, 1230, 295, 613, 6828, 307, 5413, 322, 264, 1230, 295, 15743, 51232], "temperature": 0.0, "avg_logprob": -0.15832942279417123, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.009557297453284264}, {"id": 21, "seek": 14600, "start": 163.36, "end": 170.72, "text": " variables n here. So what's nice about this theorem is that it tells us that the only true", "tokens": [51232, 9102, 297, 510, 13, 407, 437, 311, 1481, 466, 341, 20904, 307, 300, 309, 5112, 505, 300, 264, 787, 2074, 51600], "temperature": 0.0, "avg_logprob": -0.15832942279417123, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.009557297453284264}, {"id": 22, "seek": 17072, "start": 170.72, "end": 177.84, "text": " multivariate functions is the sum. Like you can express, say, multiplication with just the summing", "tokens": [50364, 2120, 592, 3504, 473, 6828, 307, 264, 2408, 13, 1743, 291, 393, 5109, 11, 584, 11, 27290, 365, 445, 264, 2408, 2810, 50720], "temperature": 0.0, "avg_logprob": -0.12696633515534578, "compression_ratio": 1.7345971563981042, "no_speech_prob": 0.004133571870625019}, {"id": 23, "seek": 17072, "start": 177.84, "end": 187.68, "text": " operation plus some 1D activation functions. So that sounds like really great news for", "tokens": [50720, 6916, 1804, 512, 502, 35, 24433, 6828, 13, 407, 300, 3263, 411, 534, 869, 2583, 337, 51212], "temperature": 0.0, "avg_logprob": -0.12696633515534578, "compression_ratio": 1.7345971563981042, "no_speech_prob": 0.004133571870625019}, {"id": 24, "seek": 17072, "start": 188.64, "end": 192.32, "text": " approximating high-dimensional functions, especially for machine learning,", "tokens": [51260, 8542, 990, 1090, 12, 18759, 6828, 11, 2318, 337, 3479, 2539, 11, 51444], "temperature": 0.0, "avg_logprob": -0.12696633515534578, "compression_ratio": 1.7345971563981042, "no_speech_prob": 0.004133571870625019}, {"id": 25, "seek": 17072, "start": 192.32, "end": 197.68, "text": " because in high dimensions, to learn high-dimensional functions, we know we have curse of dimensionality.", "tokens": [51444, 570, 294, 1090, 12819, 11, 281, 1466, 1090, 12, 18759, 6828, 11, 321, 458, 321, 362, 17139, 295, 10139, 1860, 13, 51712], "temperature": 0.0, "avg_logprob": -0.12696633515534578, "compression_ratio": 1.7345971563981042, "no_speech_prob": 0.004133571870625019}, {"id": 26, "seek": 19768, "start": 198.4, "end": 203.68, "text": " But this theorem seems to tell us that you can decompose high-dimensional functions into", "tokens": [50400, 583, 341, 20904, 2544, 281, 980, 505, 300, 291, 393, 22867, 541, 1090, 12, 18759, 6828, 666, 50664], "temperature": 0.0, "avg_logprob": -0.22709086447051077, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.0011158597189933062}, {"id": 27, "seek": 19768, "start": 203.68, "end": 208.4, "text": " one-dimensional functions, which do not suffer from curse of dimensionality, which are great.", "tokens": [50664, 472, 12, 18759, 6828, 11, 597, 360, 406, 9753, 490, 17139, 295, 10139, 1860, 11, 597, 366, 869, 13, 50900], "temperature": 0.0, "avg_logprob": -0.22709086447051077, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.0011158597189933062}, {"id": 28, "seek": 19768, "start": 209.92000000000002, "end": 222.96, "text": " But immediately, there is this plot twist. Back in 1989, professor Jarosi and Poggio wrote this", "tokens": [50976, 583, 4258, 11, 456, 307, 341, 7542, 8203, 13, 5833, 294, 22427, 11, 8304, 23941, 21521, 293, 430, 664, 17862, 4114, 341, 51628], "temperature": 0.0, "avg_logprob": -0.22709086447051077, "compression_ratio": 1.5359116022099448, "no_speech_prob": 0.0011158597189933062}, {"id": 29, "seek": 22296, "start": 222.96, "end": 231.20000000000002, "text": " paper examining the relevance of Comagra's theorem in the context of learning. And", "tokens": [50364, 3035, 34662, 264, 32684, 295, 2432, 559, 424, 311, 20904, 294, 264, 4319, 295, 2539, 13, 400, 50776], "temperature": 0.0, "avg_logprob": -0.20801038126791677, "compression_ratio": 1.6918238993710693, "no_speech_prob": 0.004263606388121843}, {"id": 30, "seek": 22296, "start": 232.0, "end": 239.44, "text": " they conclude that we review Comagra's theorem and show that it is irrelevant in the context of", "tokens": [50816, 436, 16886, 300, 321, 3131, 2432, 559, 424, 311, 20904, 293, 855, 300, 309, 307, 28682, 294, 264, 4319, 295, 51188], "temperature": 0.0, "avg_logprob": -0.20801038126791677, "compression_ratio": 1.6918238993710693, "no_speech_prob": 0.004263606388121843}, {"id": 31, "seek": 22296, "start": 239.44, "end": 249.28, "text": " networks for learning. So this paper basically sentenced the Ka theorem to death, at least", "tokens": [51188, 9590, 337, 2539, 13, 407, 341, 3035, 1936, 30954, 264, 10988, 20904, 281, 2966, 11, 412, 1935, 51680], "temperature": 0.0, "avg_logprob": -0.20801038126791677, "compression_ratio": 1.6918238993710693, "no_speech_prob": 0.004263606388121843}, {"id": 32, "seek": 24928, "start": 249.28, "end": 259.36, "text": " sentenced to jail for like 30 years. So is that the end of the story? Well, we want to tell you", "tokens": [50364, 30954, 281, 10511, 337, 411, 2217, 924, 13, 407, 307, 300, 264, 917, 295, 264, 1657, 30, 1042, 11, 321, 528, 281, 980, 291, 50868], "temperature": 0.0, "avg_logprob": -0.09442421717521472, "compression_ratio": 1.699421965317919, "no_speech_prob": 0.000616534729488194}, {"id": 33, "seek": 24928, "start": 259.36, "end": 267.44, "text": " that that's not the end of the story. And there's again a plot twist. So let's try to see what they", "tokens": [50868, 300, 300, 311, 406, 264, 917, 295, 264, 1657, 13, 400, 456, 311, 797, 257, 7542, 8203, 13, 407, 718, 311, 853, 281, 536, 437, 436, 51272], "temperature": 0.0, "avg_logprob": -0.09442421717521472, "compression_ratio": 1.699421965317919, "no_speech_prob": 0.000616534729488194}, {"id": 34, "seek": 24928, "start": 267.44, "end": 276.96, "text": " say in their paper, why they sentenced the theorem to jail. So their argument was that the theorem", "tokens": [51272, 584, 294, 641, 3035, 11, 983, 436, 30954, 264, 20904, 281, 10511, 13, 407, 641, 6770, 390, 300, 264, 20904, 51748], "temperature": 0.0, "avg_logprob": -0.09442421717521472, "compression_ratio": 1.699421965317919, "no_speech_prob": 0.000616534729488194}, {"id": 35, "seek": 27696, "start": 276.96, "end": 286.4, "text": " fails to be true when the inner functions are required to be smooth. Basically, the original", "tokens": [50364, 18199, 281, 312, 2074, 562, 264, 7284, 6828, 366, 4739, 281, 312, 5508, 13, 8537, 11, 264, 3380, 50836], "temperature": 0.0, "avg_logprob": -0.12747540473937988, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.01472054049372673}, {"id": 36, "seek": 27696, "start": 286.4, "end": 291.76, "text": " theorem says that you can decompose a high-dimensional continuous function into one-dimensional", "tokens": [50836, 20904, 1619, 300, 291, 393, 22867, 541, 257, 1090, 12, 18759, 10957, 2445, 666, 472, 12, 18759, 51104], "temperature": 0.0, "avg_logprob": -0.12747540473937988, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.01472054049372673}, {"id": 37, "seek": 27696, "start": 291.76, "end": 299.2, "text": " functions, but it doesn't guarantee you to decompose it into one-dimensional functions.", "tokens": [51104, 6828, 11, 457, 309, 1177, 380, 10815, 291, 281, 22867, 541, 309, 666, 472, 12, 18759, 6828, 13, 51476], "temperature": 0.0, "avg_logprob": -0.12747540473937988, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.01472054049372673}, {"id": 38, "seek": 27696, "start": 299.84, "end": 306.24, "text": " But to learn something, like with gradient descent, you would need the functions to be", "tokens": [51508, 583, 281, 1466, 746, 11, 411, 365, 16235, 23475, 11, 291, 576, 643, 264, 6828, 281, 312, 51828], "temperature": 0.0, "avg_logprob": -0.12747540473937988, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.01472054049372673}, {"id": 39, "seek": 30624, "start": 306.24, "end": 313.12, "text": " smooth. Only being continuous is not enough. Even with continuous functions, you can still have", "tokens": [50364, 5508, 13, 5686, 885, 10957, 307, 406, 1547, 13, 2754, 365, 10957, 6828, 11, 291, 393, 920, 362, 50708], "temperature": 0.0, "avg_logprob": -0.09888492931019176, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.0012253114255145192}, {"id": 40, "seek": 30624, "start": 313.12, "end": 320.96000000000004, "text": " some really pathological behavior. You don't have the smoothness constraint. So that's basically", "tokens": [50708, 512, 534, 3100, 4383, 5223, 13, 509, 500, 380, 362, 264, 5508, 1287, 25534, 13, 407, 300, 311, 1936, 51100], "temperature": 0.0, "avg_logprob": -0.09888492931019176, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.0012253114255145192}, {"id": 41, "seek": 30624, "start": 320.96000000000004, "end": 326.56, "text": " the argument, because the theory does not tell you anything about the smoothness of the functions.", "tokens": [51100, 264, 6770, 11, 570, 264, 5261, 775, 406, 980, 291, 1340, 466, 264, 5508, 1287, 295, 264, 6828, 13, 51380], "temperature": 0.0, "avg_logprob": -0.09888492931019176, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.0012253114255145192}, {"id": 42, "seek": 30624, "start": 326.56, "end": 333.04, "text": " But in practice, only smooth functions are feasible in practice.", "tokens": [51380, 583, 294, 3124, 11, 787, 5508, 6828, 366, 26648, 294, 3124, 13, 51704], "temperature": 0.0, "avg_logprob": -0.09888492931019176, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.0012253114255145192}, {"id": 43, "seek": 33624, "start": 336.64, "end": 346.08, "text": " So one interesting word they use is that a stable and usable exact representations", "tokens": [50384, 407, 472, 1880, 1349, 436, 764, 307, 300, 257, 8351, 293, 29975, 1900, 33358, 50856], "temperature": 0.0, "avg_logprob": -0.17211868081774032, "compression_ratio": 1.6375, "no_speech_prob": 0.0009252303862012923}, {"id": 44, "seek": 33624, "start": 346.08, "end": 354.24, "text": " seems hopeless. So that's like the attacking point. That's the angle we're taking. Maybe", "tokens": [50856, 2544, 27317, 13, 407, 300, 311, 411, 264, 15010, 935, 13, 663, 311, 264, 5802, 321, 434, 1940, 13, 2704, 51264], "temperature": 0.0, "avg_logprob": -0.17211868081774032, "compression_ratio": 1.6375, "no_speech_prob": 0.0009252303862012923}, {"id": 45, "seek": 33624, "start": 354.24, "end": 359.92, "text": " we do not need exact representations. Sometimes approximate representations might suffice.", "tokens": [51264, 321, 360, 406, 643, 1900, 33358, 13, 4803, 30874, 33358, 1062, 3889, 573, 13, 51548], "temperature": 0.0, "avg_logprob": -0.17211868081774032, "compression_ratio": 1.6375, "no_speech_prob": 0.0009252303862012923}, {"id": 46, "seek": 35992, "start": 360.64000000000004, "end": 370.64000000000004, "text": " Yeah. So their paper was theoretically sound, was sound theoretically, but I'm trained as a", "tokens": [50400, 865, 13, 407, 641, 3035, 390, 29400, 1626, 11, 390, 1626, 29400, 11, 457, 286, 478, 8895, 382, 257, 50900], "temperature": 0.0, "avg_logprob": -0.2096171309982521, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.006095560267567635}, {"id": 47, "seek": 35992, "start": 370.64000000000004, "end": 378.16, "text": " physics student. So my level of rigor is lower than mathematicians. So me and also Max, we tend", "tokens": [50900, 10649, 3107, 13, 407, 452, 1496, 295, 42191, 307, 3126, 813, 32811, 2567, 13, 407, 385, 293, 611, 7402, 11, 321, 3928, 51276], "temperature": 0.0, "avg_logprob": -0.2096171309982521, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.006095560267567635}, {"id": 48, "seek": 35992, "start": 378.16, "end": 387.68, "text": " to have lower bar of being rigorous. So we have this naive optimism. So firstly, maybe like", "tokens": [51276, 281, 362, 3126, 2159, 295, 885, 29882, 13, 407, 321, 362, 341, 29052, 31074, 13, 407, 27376, 11, 1310, 411, 51752], "temperature": 0.0, "avg_logprob": -0.2096171309982521, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.006095560267567635}, {"id": 49, "seek": 38768, "start": 387.68, "end": 392.16, "text": " empirically, maybe we don't care about exact representations. Sometimes approximate ones", "tokens": [50364, 25790, 984, 11, 1310, 321, 500, 380, 1127, 466, 1900, 33358, 13, 4803, 30874, 2306, 50588], "temperature": 0.0, "avg_logprob": -0.14086418617062452, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.0025906944647431374}, {"id": 50, "seek": 38768, "start": 392.16, "end": 400.16, "text": " may suffice. As long as it has some levels of explainability or interpretability, that's fine.", "tokens": [50588, 815, 3889, 573, 13, 1018, 938, 382, 309, 575, 512, 4358, 295, 2903, 2310, 420, 7302, 2310, 11, 300, 311, 2489, 13, 50988], "temperature": 0.0, "avg_logprob": -0.14086418617062452, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.0025906944647431374}, {"id": 51, "seek": 38768, "start": 400.72, "end": 406.48, "text": " And secondly, a common wisdom in modern deep learning is just stacking more layers to get", "tokens": [51016, 400, 26246, 11, 257, 2689, 10712, 294, 4363, 2452, 2539, 307, 445, 41376, 544, 7914, 281, 483, 51304], "temperature": 0.0, "avg_logprob": -0.14086418617062452, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.0025906944647431374}, {"id": 52, "seek": 38768, "start": 406.48, "end": 412.16, "text": " different networks. They are basically arguing against the two-layer commograph network. But", "tokens": [51304, 819, 9590, 13, 814, 366, 1936, 19697, 1970, 264, 732, 12, 8376, 260, 800, 3108, 3209, 13, 583, 51588], "temperature": 0.0, "avg_logprob": -0.14086418617062452, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.0025906944647431374}, {"id": 53, "seek": 41216, "start": 412.16, "end": 417.92, "text": " what about we build deeper networks? Maybe for deeper networks, even under the smooth", "tokens": [50364, 437, 466, 321, 1322, 7731, 9590, 30, 2704, 337, 7731, 9590, 11, 754, 833, 264, 5508, 50652], "temperature": 0.0, "avg_logprob": -0.1212158422360475, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.009124348871409893}, {"id": 54, "seek": 41216, "start": 417.92, "end": 424.56, "text": " constraint, you can win the expressive power back. Lastly, just the can-do spirit. We cannot say", "tokens": [50652, 25534, 11, 291, 393, 1942, 264, 40189, 1347, 646, 13, 18072, 11, 445, 264, 393, 12, 2595, 3797, 13, 492, 2644, 584, 50984], "temperature": 0.0, "avg_logprob": -0.1212158422360475, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.009124348871409893}, {"id": 55, "seek": 41216, "start": 424.56, "end": 430.88, "text": " something doesn't work before really building it with the state-of-the-art techniques. They", "tokens": [50984, 746, 1177, 380, 589, 949, 534, 2390, 309, 365, 264, 1785, 12, 2670, 12, 3322, 12, 446, 7512, 13, 814, 51300], "temperature": 0.0, "avg_logprob": -0.1212158422360475, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.009124348871409893}, {"id": 56, "seek": 41216, "start": 430.88, "end": 439.04, "text": " have the negative claim back like 30 years ago. Back then, they didn't even have the", "tokens": [51300, 362, 264, 3671, 3932, 646, 411, 2217, 924, 2057, 13, 5833, 550, 11, 436, 994, 380, 754, 362, 264, 51708], "temperature": 0.0, "avg_logprob": -0.1212158422360475, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.009124348871409893}, {"id": 57, "seek": 43904, "start": 439.12, "end": 445.76000000000005, "text": " back propagation. Like at least it was not popularized back then. So we want to take", "tokens": [50368, 646, 38377, 13, 1743, 412, 1935, 309, 390, 406, 3743, 1602, 646, 550, 13, 407, 321, 528, 281, 747, 50700], "temperature": 0.0, "avg_logprob": -0.13082291423410608, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0031718171667307615}, {"id": 58, "seek": 43904, "start": 445.76000000000005, "end": 453.6, "text": " everything we have in the modern area of deep learning and see how far we can go. So that's", "tokens": [50700, 1203, 321, 362, 294, 264, 4363, 1859, 295, 2452, 2539, 293, 536, 577, 1400, 321, 393, 352, 13, 407, 300, 311, 51092], "temperature": 0.0, "avg_logprob": -0.13082291423410608, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0031718171667307615}, {"id": 59, "seek": 43904, "start": 453.6, "end": 463.28000000000003, "text": " the mindset. So just with the can-do spirit, we propose or more properly rediscover or we", "tokens": [51092, 264, 12543, 13, 407, 445, 365, 264, 393, 12, 2595, 3797, 11, 321, 17421, 420, 544, 6108, 2182, 40080, 420, 321, 51576], "temperature": 0.0, "avg_logprob": -0.13082291423410608, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0031718171667307615}, {"id": 60, "seek": 46328, "start": 463.28, "end": 474.23999999999995, "text": " contextualize the idea of commograph networks in today's deep learning world. Yeah. So here's", "tokens": [50364, 35526, 1125, 264, 1558, 295, 800, 3108, 9590, 294, 965, 311, 2452, 2539, 1002, 13, 865, 13, 407, 510, 311, 50912], "temperature": 0.0, "avg_logprob": -0.13801916321711755, "compression_ratio": 1.4314720812182742, "no_speech_prob": 0.013215542770922184}, {"id": 61, "seek": 46328, "start": 474.23999999999995, "end": 479.59999999999997, "text": " the overview. First, I will introduce the math foundation. Well, I already briefly mentioned", "tokens": [50912, 264, 12492, 13, 2386, 11, 286, 486, 5366, 264, 5221, 7030, 13, 1042, 11, 286, 1217, 10515, 2835, 51180], "temperature": 0.0, "avg_logprob": -0.13801916321711755, "compression_ratio": 1.4314720812182742, "no_speech_prob": 0.013215542770922184}, {"id": 62, "seek": 46328, "start": 479.59999999999997, "end": 487.11999999999995, "text": " it. I won't dig deeper into it, but just I will emphasize again like the mathematical beauty of", "tokens": [51180, 309, 13, 286, 1582, 380, 2528, 7731, 666, 309, 11, 457, 445, 286, 486, 16078, 797, 411, 264, 18894, 6643, 295, 51556], "temperature": 0.0, "avg_logprob": -0.13801916321711755, "compression_ratio": 1.4314720812182742, "no_speech_prob": 0.013215542770922184}, {"id": 63, "seek": 48712, "start": 487.12, "end": 497.44, "text": " it. And then I will talk about the properties of cans. Why? And in what scenarios cans are more", "tokens": [50364, 309, 13, 400, 550, 286, 486, 751, 466, 264, 7221, 295, 21835, 13, 1545, 30, 400, 294, 437, 15077, 21835, 366, 544, 50880], "temperature": 0.0, "avg_logprob": -0.1533016860485077, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.021607784554362297}, {"id": 64, "seek": 48712, "start": 497.44, "end": 506.0, "text": " accurate and interpretable than current deep learning models? Yeah. So first, the math foundation,", "tokens": [50880, 8559, 293, 7302, 712, 813, 2190, 2452, 2539, 5245, 30, 865, 13, 407, 700, 11, 264, 5221, 7030, 11, 51308], "temperature": 0.0, "avg_logprob": -0.1533016860485077, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.021607784554362297}, {"id": 65, "seek": 48712, "start": 506.0, "end": 512.88, "text": " I already covered this part, but I want to emphasize again that the theorem", "tokens": [51308, 286, 1217, 5343, 341, 644, 11, 457, 286, 528, 281, 16078, 797, 300, 264, 20904, 51652], "temperature": 0.0, "avg_logprob": -0.1533016860485077, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.021607784554362297}, {"id": 66, "seek": 51288, "start": 513.2, "end": 518.96, "text": " looks really nice. That allows you to decompose a high-dimensional functions into one-dimensional", "tokens": [50380, 1542, 534, 1481, 13, 663, 4045, 291, 281, 22867, 541, 257, 1090, 12, 18759, 6828, 666, 472, 12, 18759, 50668], "temperature": 0.0, "avg_logprob": -0.22638556662570225, "compression_ratio": 1.8146341463414635, "no_speech_prob": 0.004197041504085064}, {"id": 67, "seek": 51288, "start": 518.96, "end": 524.64, "text": " functions. And after the decomposition is done, your only need to just your only job would be", "tokens": [50668, 6828, 13, 400, 934, 264, 48356, 307, 1096, 11, 428, 787, 643, 281, 445, 428, 787, 1691, 576, 312, 50952], "temperature": 0.0, "avg_logprob": -0.22638556662570225, "compression_ratio": 1.8146341463414635, "no_speech_prob": 0.004197041504085064}, {"id": 68, "seek": 51288, "start": 524.64, "end": 533.12, "text": " just approximating the 1D functions. So that's the idea of that's the idea of the commograph", "tokens": [50952, 445, 8542, 990, 264, 502, 35, 6828, 13, 407, 300, 311, 264, 1558, 295, 300, 311, 264, 1558, 295, 264, 800, 3108, 51376], "temperature": 0.0, "avg_logprob": -0.22638556662570225, "compression_ratio": 1.8146341463414635, "no_speech_prob": 0.004197041504085064}, {"id": 69, "seek": 51288, "start": 533.12, "end": 541.6, "text": " networks. Like decomposition first and then learn the 1D functions. Well, yeah. So this", "tokens": [51376, 9590, 13, 1743, 48356, 700, 293, 550, 1466, 264, 502, 35, 6828, 13, 1042, 11, 1338, 13, 407, 341, 51800], "temperature": 0.0, "avg_logprob": -0.22638556662570225, "compression_ratio": 1.8146341463414635, "no_speech_prob": 0.004197041504085064}, {"id": 70, "seek": 54160, "start": 541.6, "end": 547.6, "text": " representation looks a bit complicated. You'll see that there are this huge, this big summation", "tokens": [50364, 10290, 1542, 257, 857, 6179, 13, 509, 603, 536, 300, 456, 366, 341, 2603, 11, 341, 955, 28811, 50664], "temperature": 0.0, "avg_logprob": -0.13463667164678159, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.003944896161556244}, {"id": 71, "seek": 54160, "start": 547.6, "end": 555.12, "text": " symbol and you have two layers of composition. And this will be complicated. But don't worry", "tokens": [50664, 5986, 293, 291, 362, 732, 7914, 295, 12686, 13, 400, 341, 486, 312, 6179, 13, 583, 500, 380, 3292, 51040], "temperature": 0.0, "avg_logprob": -0.13463667164678159, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.003944896161556244}, {"id": 72, "seek": 54160, "start": 555.12, "end": 562.4, "text": " about it. It's just equivalent to this two-layer network. Let's suppose we have two inputs,", "tokens": [51040, 466, 309, 13, 467, 311, 445, 10344, 281, 341, 732, 12, 8376, 260, 3209, 13, 961, 311, 7297, 321, 362, 732, 15743, 11, 51404], "temperature": 0.0, "avg_logprob": -0.13463667164678159, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.003944896161556244}, {"id": 73, "seek": 54160, "start": 562.4, "end": 569.6, "text": " x1 and x2, and we have the outputs at the top here. So the representation in the original", "tokens": [51404, 2031, 16, 293, 2031, 17, 11, 293, 321, 362, 264, 23930, 412, 264, 1192, 510, 13, 407, 264, 10290, 294, 264, 3380, 51764], "temperature": 0.0, "avg_logprob": -0.13463667164678159, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.003944896161556244}, {"id": 74, "seek": 56960, "start": 569.6, "end": 578.4, "text": " theorem is basically just that you have five hidden neurons in the middle. And to get the", "tokens": [50364, 20904, 307, 1936, 445, 300, 291, 362, 1732, 7633, 22027, 294, 264, 2808, 13, 400, 281, 483, 264, 50804], "temperature": 0.0, "avg_logprob": -0.15779442718063574, "compression_ratio": 1.729559748427673, "no_speech_prob": 0.009265521541237831}, {"id": 75, "seek": 56960, "start": 578.4, "end": 588.72, "text": " activations in each hidden neuron, you basically apply a 1D, possibly nonlinear function to x1", "tokens": [50804, 2430, 763, 294, 1184, 7633, 34090, 11, 291, 1936, 3079, 257, 502, 35, 11, 6264, 2107, 28263, 2445, 281, 2031, 16, 51320], "temperature": 0.0, "avg_logprob": -0.15779442718063574, "compression_ratio": 1.729559748427673, "no_speech_prob": 0.009265521541237831}, {"id": 76, "seek": 56960, "start": 588.72, "end": 598.5600000000001, "text": " and x2, and sum up these two nonlinear activations to get the activations at the node. And", "tokens": [51320, 293, 2031, 17, 11, 293, 2408, 493, 613, 732, 2107, 28263, 2430, 763, 281, 483, 264, 2430, 763, 412, 264, 9984, 13, 400, 51812], "temperature": 0.0, "avg_logprob": -0.15779442718063574, "compression_ratio": 1.729559748427673, "no_speech_prob": 0.009265521541237831}, {"id": 77, "seek": 59960, "start": 599.6, "end": 607.36, "text": " in the second layer, it's similar that you apply some nonlinear function to the hidden activations,", "tokens": [50364, 294, 264, 1150, 4583, 11, 309, 311, 2531, 300, 291, 3079, 512, 2107, 28263, 2445, 281, 264, 7633, 2430, 763, 11, 50752], "temperature": 0.0, "avg_logprob": -0.13430333818708148, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.00032501420355401933}, {"id": 78, "seek": 59960, "start": 607.36, "end": 614.08, "text": " summing everything up at the output node, and that's how you get the output. So the computation", "tokens": [50752, 2408, 2810, 1203, 493, 412, 264, 5598, 9984, 11, 293, 300, 311, 577, 291, 483, 264, 5598, 13, 407, 264, 24903, 51088], "temperature": 0.0, "avg_logprob": -0.13430333818708148, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.00032501420355401933}, {"id": 79, "seek": 59960, "start": 614.08, "end": 627.2, "text": " graph is super clear with just this diagram. And this might remind you a lot of, this looks", "tokens": [51088, 4295, 307, 1687, 1850, 365, 445, 341, 10686, 13, 400, 341, 1062, 4160, 291, 257, 688, 295, 11, 341, 1542, 51744], "temperature": 0.0, "avg_logprob": -0.13430333818708148, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.00032501420355401933}, {"id": 80, "seek": 62720, "start": 627.2, "end": 631.36, "text": " just like a multi-layer perceptron, the fully connected networks, where everything just fully", "tokens": [50364, 445, 411, 257, 4825, 12, 8376, 260, 43276, 2044, 11, 264, 4498, 4582, 9590, 11, 689, 1203, 445, 4498, 50572], "temperature": 0.0, "avg_logprob": -0.1433760631515319, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011684278957545757}, {"id": 81, "seek": 62720, "start": 631.36, "end": 637.5200000000001, "text": " connected. But instead of having activation functions on the nodes, now we are having", "tokens": [50572, 4582, 13, 583, 2602, 295, 1419, 24433, 6828, 322, 264, 13891, 11, 586, 321, 366, 1419, 50880], "temperature": 0.0, "avg_logprob": -0.1433760631515319, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011684278957545757}, {"id": 82, "seek": 62720, "start": 637.5200000000001, "end": 642.96, "text": " activation functions on the edges. And on the nodes, you simply just have the summation operation,", "tokens": [50880, 24433, 6828, 322, 264, 8819, 13, 400, 322, 264, 13891, 11, 291, 2935, 445, 362, 264, 28811, 6916, 11, 51152], "temperature": 0.0, "avg_logprob": -0.1433760631515319, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011684278957545757}, {"id": 83, "seek": 62720, "start": 644.8000000000001, "end": 652.24, "text": " which is really simple. Yeah, just to make it more elegant and beautiful, we can,", "tokens": [51244, 597, 307, 534, 2199, 13, 865, 11, 445, 281, 652, 309, 544, 21117, 293, 2238, 11, 321, 393, 11, 51616], "temperature": 0.0, "avg_logprob": -0.1433760631515319, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011684278957545757}, {"id": 84, "seek": 65224, "start": 652.8, "end": 657.04, "text": " or more intuitive, we can basically, because like 1D functions, we can basically just visualize them", "tokens": [50392, 420, 544, 21769, 11, 321, 393, 1936, 11, 570, 411, 502, 35, 6828, 11, 321, 393, 1936, 445, 23273, 552, 50604], "temperature": 0.0, "avg_logprob": -0.14576850551189763, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0012064926559105515}, {"id": 85, "seek": 65224, "start": 657.04, "end": 663.84, "text": " with 1D curves, with the x and x as the input and y as the output. So now can network is basically,", "tokens": [50604, 365, 502, 35, 19490, 11, 365, 264, 2031, 293, 2031, 382, 264, 4846, 293, 288, 382, 264, 5598, 13, 407, 586, 393, 3209, 307, 1936, 11, 50944], "temperature": 0.0, "avg_logprob": -0.14576850551189763, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0012064926559105515}, {"id": 86, "seek": 65224, "start": 664.48, "end": 669.6800000000001, "text": " you can picture it as this. And by staring at it, you can, you can have some idea what's happening", "tokens": [50976, 291, 393, 3036, 309, 382, 341, 13, 400, 538, 18043, 412, 309, 11, 291, 393, 11, 291, 393, 362, 512, 1558, 437, 311, 2737, 51236], "temperature": 0.0, "avg_logprob": -0.14576850551189763, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0012064926559105515}, {"id": 87, "seek": 65224, "start": 669.6800000000001, "end": 678.64, "text": " inside. Yeah, so I mentioned this is just a, so the theorem, so the representation is equivalent", "tokens": [51236, 1854, 13, 865, 11, 370, 286, 2835, 341, 307, 445, 257, 11, 370, 264, 20904, 11, 370, 264, 10290, 307, 10344, 51684], "temperature": 0.0, "avg_logprob": -0.14576850551189763, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0012064926559105515}, {"id": 88, "seek": 67864, "start": 678.64, "end": 685.84, "text": " to a two-layer network. But can we go deeper? The answer is yes. Algorithmically speaking,", "tokens": [50364, 281, 257, 732, 12, 8376, 260, 3209, 13, 583, 393, 321, 352, 7731, 30, 440, 1867, 307, 2086, 13, 35014, 6819, 76, 984, 4124, 11, 50724], "temperature": 0.0, "avg_logprob": -0.11767197628410495, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.01912030205130577}, {"id": 89, "seek": 67864, "start": 685.84, "end": 693.4399999999999, "text": " because it's just a stack of two layers, which, from which we can abstract a notion called the", "tokens": [50724, 570, 309, 311, 445, 257, 8630, 295, 732, 7914, 11, 597, 11, 490, 597, 321, 393, 12649, 257, 10710, 1219, 264, 51104], "temperature": 0.0, "avg_logprob": -0.11767197628410495, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.01912030205130577}, {"id": 90, "seek": 67864, "start": 693.4399999999999, "end": 699.76, "text": " can layer. So the original two-layer network is basically a stack of two, two can layers.", "tokens": [51104, 393, 4583, 13, 407, 264, 3380, 732, 12, 8376, 260, 3209, 307, 1936, 257, 8630, 295, 732, 11, 732, 393, 7914, 13, 51420], "temperature": 0.0, "avg_logprob": -0.11767197628410495, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.01912030205130577}, {"id": 91, "seek": 67864, "start": 699.76, "end": 705.36, "text": " And for each can layer, it's basically just taking some number of inputs and outputs, some number", "tokens": [51420, 400, 337, 1184, 393, 4583, 11, 309, 311, 1936, 445, 1940, 512, 1230, 295, 15743, 293, 23930, 11, 512, 1230, 51700], "temperature": 0.0, "avg_logprob": -0.11767197628410495, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.01912030205130577}, {"id": 92, "seek": 70536, "start": 705.44, "end": 711.2, "text": " of outputs, and in between is fully connected. And on each edge, you have the active, you have the", "tokens": [50368, 295, 23930, 11, 293, 294, 1296, 307, 4498, 4582, 13, 400, 322, 1184, 4691, 11, 291, 362, 264, 4967, 11, 291, 362, 264, 50656], "temperature": 0.0, "avg_logprob": -0.11967066222546148, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0028003884945064783}, {"id": 93, "seek": 70536, "start": 711.2, "end": 716.8000000000001, "text": " some nonlinear, learnable nonlinear activation function. And in the end, and in the outputs,", "tokens": [50656, 512, 2107, 28263, 11, 1466, 712, 2107, 28263, 24433, 2445, 13, 400, 294, 264, 917, 11, 293, 294, 264, 23930, 11, 50936], "temperature": 0.0, "avg_logprob": -0.11967066222546148, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0028003884945064783}, {"id": 94, "seek": 70536, "start": 716.8000000000001, "end": 724.88, "text": " you, you, you summing up the incoming activations. That's how a can layer works. And you can simply", "tokens": [50936, 291, 11, 291, 11, 291, 2408, 2810, 493, 264, 22341, 2430, 763, 13, 663, 311, 577, 257, 393, 4583, 1985, 13, 400, 291, 393, 2935, 51340], "temperature": 0.0, "avg_logprob": -0.11967066222546148, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0028003884945064783}, {"id": 95, "seek": 70536, "start": 724.88, "end": 731.76, "text": " stack more and more can layers to get deeper and deeper cans. This is just a three-layer can,", "tokens": [51340, 8630, 544, 293, 544, 393, 7914, 281, 483, 7731, 293, 7731, 21835, 13, 639, 307, 445, 257, 1045, 12, 8376, 260, 393, 11, 51684], "temperature": 0.0, "avg_logprob": -0.11967066222546148, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0028003884945064783}, {"id": 96, "seek": 73176, "start": 732.72, "end": 739.04, "text": " like, like the first layer, we're taking two, output three. The second layer, input two, output", "tokens": [50412, 411, 11, 411, 264, 700, 4583, 11, 321, 434, 1940, 732, 11, 5598, 1045, 13, 440, 1150, 4583, 11, 4846, 732, 11, 5598, 50728], "temperature": 0.0, "avg_logprob": -0.17248930829636594, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012254652101546526}, {"id": 97, "seek": 73176, "start": 739.04, "end": 746.72, "text": " three. And the last layer, you input three, output one. So this is the, this is a three-layer network,", "tokens": [50728, 1045, 13, 400, 264, 1036, 4583, 11, 291, 4846, 1045, 11, 5598, 472, 13, 407, 341, 307, 264, 11, 341, 307, 257, 1045, 12, 8376, 260, 3209, 11, 51112], "temperature": 0.0, "avg_logprob": -0.17248930829636594, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012254652101546526}, {"id": 98, "seek": 73176, "start": 746.72, "end": 753.12, "text": " which approximates a scalar function in two dimensions. But obviously, you can easily", "tokens": [51112, 597, 8542, 1024, 257, 39684, 2445, 294, 732, 12819, 13, 583, 2745, 11, 291, 393, 3612, 51432], "temperature": 0.0, "avg_logprob": -0.17248930829636594, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012254652101546526}, {"id": 99, "seek": 73176, "start": 753.12, "end": 760.0, "text": " extend it to arbitrary dimension, like arbitrary input, arbitrary output, arbitrary width, arbitrary", "tokens": [51432, 10101, 309, 281, 23211, 10139, 11, 411, 23211, 4846, 11, 23211, 5598, 11, 23211, 11402, 11, 23211, 51776], "temperature": 0.0, "avg_logprob": -0.17248930829636594, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012254652101546526}, {"id": 100, "seek": 76000, "start": 760.8, "end": 765.76, "text": " depth. So, so you have all the flexibility to choose the size of the network.", "tokens": [50404, 7161, 13, 407, 11, 370, 291, 362, 439, 264, 12635, 281, 2826, 264, 2744, 295, 264, 3209, 13, 50652], "temperature": 0.0, "avg_logprob": -0.18987934517137933, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.0018670757999643683}, {"id": 101, "seek": 76000, "start": 769.12, "end": 776.96, "text": " Yeah, one, the first question Professor Poggio asked me when I presented this network to him,", "tokens": [50820, 865, 11, 472, 11, 264, 700, 1168, 8419, 430, 664, 17862, 2351, 385, 562, 286, 8212, 341, 3209, 281, 796, 11, 51212], "temperature": 0.0, "avg_logprob": -0.18987934517137933, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.0018670757999643683}, {"id": 102, "seek": 76000, "start": 777.92, "end": 783.2, "text": " he asked, is that why do you need this deep networks? Because the original theorem told you", "tokens": [51260, 415, 2351, 11, 307, 300, 983, 360, 291, 643, 341, 2452, 9590, 30, 1436, 264, 3380, 20904, 1907, 291, 51524], "temperature": 0.0, "avg_logprob": -0.18987934517137933, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.0018670757999643683}, {"id": 103, "seek": 78320, "start": 783.2, "end": 791.12, "text": " that you only need the two-layer constructions. And here's just a quick answer that I, I can give", "tokens": [50364, 300, 291, 787, 643, 264, 732, 12, 8376, 260, 7690, 626, 13, 400, 510, 311, 445, 257, 1702, 1867, 300, 286, 11, 286, 393, 976, 50760], "temperature": 0.0, "avg_logprob": -0.11106902995008103, "compression_ratio": 1.7737556561085972, "no_speech_prob": 0.005138398613780737}, {"id": 104, "seek": 78320, "start": 791.12, "end": 799.84, "text": " you an example. So please look at this symbolic formula. And if you examine it, you would immediately", "tokens": [50760, 291, 364, 1365, 13, 407, 1767, 574, 412, 341, 25755, 8513, 13, 400, 498, 291, 17496, 309, 11, 291, 576, 4258, 51196], "temperature": 0.0, "avg_logprob": -0.11106902995008103, "compression_ratio": 1.7737556561085972, "no_speech_prob": 0.005138398613780737}, {"id": 105, "seek": 78320, "start": 799.84, "end": 806.5600000000001, "text": " realize that you would need at least three compositions to do this, to construct this formula.", "tokens": [51196, 4325, 300, 291, 576, 643, 412, 1935, 1045, 43401, 281, 360, 341, 11, 281, 7690, 341, 8513, 13, 51532], "temperature": 0.0, "avg_logprob": -0.11106902995008103, "compression_ratio": 1.7737556561085972, "no_speech_prob": 0.005138398613780737}, {"id": 106, "seek": 78320, "start": 807.12, "end": 811.6800000000001, "text": " You need the, you need the squared function, you need the sine function, you need the exponential", "tokens": [51560, 509, 643, 264, 11, 291, 643, 264, 8889, 2445, 11, 291, 643, 264, 18609, 2445, 11, 291, 643, 264, 21510, 51788], "temperature": 0.0, "avg_logprob": -0.11106902995008103, "compression_ratio": 1.7737556561085972, "no_speech_prob": 0.005138398613780737}, {"id": 107, "seek": 81168, "start": 811.68, "end": 818.2399999999999, "text": " function. They're like, because they're, because it's the compositional structure, they're in different", "tokens": [50364, 2445, 13, 814, 434, 411, 11, 570, 436, 434, 11, 570, 309, 311, 264, 10199, 2628, 3877, 11, 436, 434, 294, 819, 50692], "temperature": 0.0, "avg_logprob": -0.10233097888053731, "compression_ratio": 1.72, "no_speech_prob": 0.0036491008941084146}, {"id": 108, "seek": 81168, "start": 818.2399999999999, "end": 826.64, "text": " layers. So you would at least, at least need three layers to, to, to learn this formula. And", "tokens": [50692, 7914, 13, 407, 291, 576, 412, 1935, 11, 412, 1935, 643, 1045, 7914, 281, 11, 281, 11, 281, 1466, 341, 8513, 13, 400, 51112], "temperature": 0.0, "avg_logprob": -0.10233097888053731, "compression_ratio": 1.72, "no_speech_prob": 0.0036491008941084146}, {"id": 109, "seek": 81168, "start": 826.64, "end": 832.16, "text": " indeed, if you just use two-layer network, the activation functions becomes really oscillatory,", "tokens": [51112, 6451, 11, 498, 291, 445, 764, 732, 12, 8376, 260, 3209, 11, 264, 24433, 6828, 3643, 534, 18225, 4745, 11, 51388], "temperature": 0.0, "avg_logprob": -0.10233097888053731, "compression_ratio": 1.72, "no_speech_prob": 0.0036491008941084146}, {"id": 110, "seek": 81168, "start": 832.16, "end": 839.3599999999999, "text": " becomes really pathological. And the performance is bad, and also the interpretability is bad.", "tokens": [51388, 3643, 534, 3100, 4383, 13, 400, 264, 3389, 307, 1578, 11, 293, 611, 264, 7302, 2310, 307, 1578, 13, 51748], "temperature": 0.0, "avg_logprob": -0.10233097888053731, "compression_ratio": 1.72, "no_speech_prob": 0.0036491008941084146}, {"id": 111, "seek": 83936, "start": 839.36, "end": 846.48, "text": " But in the right, I show that a three-layer network, a three-layer can train on this,", "tokens": [50364, 583, 294, 264, 558, 11, 286, 855, 300, 257, 1045, 12, 8376, 260, 3209, 11, 257, 1045, 12, 8376, 260, 393, 3847, 322, 341, 11, 50720], "temperature": 0.0, "avg_logprob": -0.12208277838570732, "compression_ratio": 1.8316831683168318, "no_speech_prob": 0.0003799491096287966}, {"id": 112, "seek": 83936, "start": 846.48, "end": 851.6800000000001, "text": " train on this dataset. And after training, you would immediately see the learned activation", "tokens": [50720, 3847, 322, 341, 28872, 13, 400, 934, 3097, 11, 291, 576, 4258, 536, 264, 3264, 24433, 50980], "temperature": 0.0, "avg_logprob": -0.12208277838570732, "compression_ratio": 1.8316831683168318, "no_speech_prob": 0.0003799491096287966}, {"id": 113, "seek": 83936, "start": 851.6800000000001, "end": 855.6, "text": " functions. In the first layer, you got the squared, in the second layer, you got the sine,", "tokens": [50980, 6828, 13, 682, 264, 700, 4583, 11, 291, 658, 264, 8889, 11, 294, 264, 1150, 4583, 11, 291, 658, 264, 18609, 11, 51176], "temperature": 0.0, "avg_logprob": -0.12208277838570732, "compression_ratio": 1.8316831683168318, "no_speech_prob": 0.0003799491096287966}, {"id": 114, "seek": 83936, "start": 855.6, "end": 861.52, "text": " and the last layer, you got the exponential. Well, well, you may think of, you may think this is some", "tokens": [51176, 293, 264, 1036, 4583, 11, 291, 658, 264, 21510, 13, 1042, 11, 731, 11, 291, 815, 519, 295, 11, 291, 815, 519, 341, 307, 512, 51472], "temperature": 0.0, "avg_logprob": -0.12208277838570732, "compression_ratio": 1.8316831683168318, "no_speech_prob": 0.0003799491096287966}, {"id": 115, "seek": 86152, "start": 861.52, "end": 869.1999999999999, "text": " other functions, maybe just some local quadratic, but yeah. But you can, but you can, like,", "tokens": [50364, 661, 6828, 11, 1310, 445, 512, 2654, 37262, 11, 457, 1338, 13, 583, 291, 393, 11, 457, 291, 393, 11, 411, 11, 50748], "temperature": 0.0, "avg_logprob": -0.13695271809895834, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0035365994554013014}, {"id": 116, "seek": 86152, "start": 869.1999999999999, "end": 874.16, "text": " do the template matching with the candidate functions and figure out which one fits the best.", "tokens": [50748, 360, 264, 12379, 14324, 365, 264, 11532, 6828, 293, 2573, 484, 597, 472, 9001, 264, 1151, 13, 50996], "temperature": 0.0, "avg_logprob": -0.13695271809895834, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0035365994554013014}, {"id": 117, "seek": 86152, "start": 878.4, "end": 883.84, "text": " Yeah, so, so I, I said that this activation functions are learnable. How do we make them", "tokens": [51208, 865, 11, 370, 11, 370, 286, 11, 286, 848, 300, 341, 24433, 6828, 366, 1466, 712, 13, 1012, 360, 321, 652, 552, 51480], "temperature": 0.0, "avg_logprob": -0.13695271809895834, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0035365994554013014}, {"id": 118, "seek": 86152, "start": 883.84, "end": 890.0799999999999, "text": " learnable? Because they're functions. And, and, and, and the common wisdom is that we need to make,", "tokens": [51480, 1466, 712, 30, 1436, 436, 434, 6828, 13, 400, 11, 293, 11, 293, 11, 293, 264, 2689, 10712, 307, 300, 321, 643, 281, 652, 11, 51792], "temperature": 0.0, "avg_logprob": -0.13695271809895834, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0035365994554013014}, {"id": 119, "seek": 89008, "start": 890.08, "end": 896.1600000000001, "text": " we need to parametrize the things to be learned so that we can use gradient descent to learn,", "tokens": [50364, 321, 643, 281, 6220, 302, 470, 1381, 264, 721, 281, 312, 3264, 370, 300, 321, 393, 764, 16235, 23475, 281, 1466, 11, 50668], "temperature": 0.0, "avg_logprob": -0.14985155741373699, "compression_ratio": 1.6626506024096386, "no_speech_prob": 0.001388028496876359}, {"id": 120, "seek": 89008, "start": 898.0, "end": 904.96, "text": " to learn this stuff. So the idea is that we parametrize a 1D function with, with B splines.", "tokens": [50760, 281, 1466, 341, 1507, 13, 407, 264, 1558, 307, 300, 321, 6220, 302, 470, 1381, 257, 502, 35, 2445, 365, 11, 365, 363, 4732, 1652, 13, 51108], "temperature": 0.0, "avg_logprob": -0.14985155741373699, "compression_ratio": 1.6626506024096386, "no_speech_prob": 0.001388028496876359}, {"id": 121, "seek": 89008, "start": 904.96, "end": 913.5200000000001, "text": " So B splines is basically some piecewise, some, some local piecewise polynomial functions.", "tokens": [51108, 407, 363, 4732, 1652, 307, 1936, 512, 2522, 3711, 11, 512, 11, 512, 2654, 2522, 3711, 26110, 6828, 13, 51536], "temperature": 0.0, "avg_logprob": -0.14985155741373699, "compression_ratio": 1.6626506024096386, "no_speech_prob": 0.001388028496876359}, {"id": 122, "seek": 91352, "start": 914.0799999999999, "end": 922.72, "text": " So, so here I showed that there are some local B spline bases. And the way we construct the activation", "tokens": [50392, 407, 11, 370, 510, 286, 4712, 300, 456, 366, 512, 2654, 363, 4732, 533, 17949, 13, 400, 264, 636, 321, 7690, 264, 24433, 50824], "temperature": 0.0, "avg_logprob": -0.15123429505721384, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.0016228476306423545}, {"id": 123, "seek": 91352, "start": 922.72, "end": 929.1999999999999, "text": " functions is by linearly combining this, this, this B spline functions. And the only learnable", "tokens": [50824, 6828, 307, 538, 43586, 21928, 341, 11, 341, 11, 341, 363, 4732, 533, 6828, 13, 400, 264, 787, 1466, 712, 51148], "temperature": 0.0, "avg_logprob": -0.15123429505721384, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.0016228476306423545}, {"id": 124, "seek": 91352, "start": 929.1999999999999, "end": 936.72, "text": " parameters are just the linear coefficients of, of, of this local basis. And what's nice about", "tokens": [51148, 9834, 366, 445, 264, 8213, 31994, 295, 11, 295, 11, 295, 341, 2654, 5143, 13, 400, 437, 311, 1481, 466, 51524], "temperature": 0.0, "avg_logprob": -0.15123429505721384, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.0016228476306423545}, {"id": 125, "seek": 91352, "start": 936.72, "end": 943.04, "text": " this formulation is that we, we inherit the advantage of B splines. We can easily switch", "tokens": [51524, 341, 37642, 307, 300, 321, 11, 321, 21389, 264, 5002, 295, 363, 4732, 1652, 13, 492, 393, 3612, 3679, 51840], "temperature": 0.0, "avg_logprob": -0.15123429505721384, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.0016228476306423545}, {"id": 126, "seek": 94304, "start": 943.12, "end": 949.52, "text": " between fine grains, fine grain grids and coarse grain grids. If you want something to be more", "tokens": [50368, 1296, 2489, 22908, 11, 2489, 12837, 677, 3742, 293, 39312, 12837, 677, 3742, 13, 759, 291, 528, 746, 281, 312, 544, 50688], "temperature": 0.0, "avg_logprob": -0.09905391063505006, "compression_ratio": 1.8774509803921569, "no_speech_prob": 0.0017543445574119687}, {"id": 127, "seek": 94304, "start": 949.52, "end": 955.1999999999999, "text": " accurate, you can, you can choose the mesh to be more fine grain. If you want the model to be", "tokens": [50688, 8559, 11, 291, 393, 11, 291, 393, 2826, 264, 17407, 281, 312, 544, 2489, 12837, 13, 759, 291, 528, 264, 2316, 281, 312, 50972], "temperature": 0.0, "avg_logprob": -0.09905391063505006, "compression_ratio": 1.8774509803921569, "no_speech_prob": 0.0017543445574119687}, {"id": 128, "seek": 94304, "start": 955.1999999999999, "end": 960.48, "text": " smaller, so you can have a faster inference, you can, you can, you can choose a more coarse-grained", "tokens": [50972, 4356, 11, 370, 291, 393, 362, 257, 4663, 38253, 11, 291, 393, 11, 291, 393, 11, 291, 393, 2826, 257, 544, 39312, 12, 20735, 2001, 51236], "temperature": 0.0, "avg_logprob": -0.09905391063505006, "compression_ratio": 1.8774509803921569, "no_speech_prob": 0.0017543445574119687}, {"id": 129, "seek": 94304, "start": 960.48, "end": 971.36, "text": " model. Yeah, that's basically the idea of cans. And we can compare MLPs and cans side by side,", "tokens": [51236, 2316, 13, 865, 11, 300, 311, 1936, 264, 1558, 295, 21835, 13, 400, 321, 393, 6794, 21601, 23043, 293, 21835, 1252, 538, 1252, 11, 51780], "temperature": 0.0, "avg_logprob": -0.09905391063505006, "compression_ratio": 1.8774509803921569, "no_speech_prob": 0.0017543445574119687}, {"id": 130, "seek": 97136, "start": 971.36, "end": 975.84, "text": " because they do share some similarities, but also share some difference, but also have some", "tokens": [50364, 570, 436, 360, 2073, 512, 24197, 11, 457, 611, 2073, 512, 2649, 11, 457, 611, 362, 512, 50588], "temperature": 0.0, "avg_logprob": -0.19818891637465533, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.004133338574320078}, {"id": 131, "seek": 97136, "start": 975.84, "end": 983.44, "text": " differences. So MLPs are inspired by the universal approximation theorem cans, again,", "tokens": [50588, 7300, 13, 407, 21601, 23043, 366, 7547, 538, 264, 11455, 28023, 20904, 21835, 11, 797, 11, 50968], "temperature": 0.0, "avg_logprob": -0.19818891637465533, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.004133338574320078}, {"id": 132, "seek": 97136, "start": 983.44, "end": 989.36, "text": " inspired by the Camargo Ravano representation theorem. The network looks a bit similar in the", "tokens": [50968, 7547, 538, 264, 6886, 289, 1571, 497, 706, 3730, 10290, 20904, 13, 440, 3209, 1542, 257, 857, 2531, 294, 264, 51264], "temperature": 0.0, "avg_logprob": -0.19818891637465533, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.004133338574320078}, {"id": 133, "seek": 97136, "start": 989.36, "end": 996.16, "text": " sense that they're both fully connected. But their dual, they're different, but they're dual in the", "tokens": [51264, 2020, 300, 436, 434, 1293, 4498, 4582, 13, 583, 641, 11848, 11, 436, 434, 819, 11, 457, 436, 434, 11848, 294, 264, 51604], "temperature": 0.0, "avg_logprob": -0.19818891637465533, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.004133338574320078}, {"id": 134, "seek": 99616, "start": 996.16, "end": 1001.8399999999999, "text": " sense that MLPs have fixed activation functions on nodes. Well, you can make them trainable, but", "tokens": [50364, 2020, 300, 21601, 23043, 362, 6806, 24433, 6828, 322, 13891, 13, 1042, 11, 291, 393, 652, 552, 3847, 712, 11, 457, 50648], "temperature": 0.0, "avg_logprob": -0.11553311598928351, "compression_ratio": 1.835680751173709, "no_speech_prob": 0.0019265469163656235}, {"id": 135, "seek": 99616, "start": 1002.64, "end": 1008.7199999999999, "text": " but they're on nodes for sure. And in MLPs, we have learnable weights, learnable linear weights on", "tokens": [50688, 457, 436, 434, 322, 13891, 337, 988, 13, 400, 294, 21601, 23043, 11, 321, 362, 1466, 712, 17443, 11, 1466, 712, 8213, 17443, 322, 50992], "temperature": 0.0, "avg_logprob": -0.11553311598928351, "compression_ratio": 1.835680751173709, "no_speech_prob": 0.0019265469163656235}, {"id": 136, "seek": 99616, "start": 1008.7199999999999, "end": 1017.92, "text": " edges. By contrast, cans have learnable activation functions on edges, while cans have this simple", "tokens": [50992, 8819, 13, 3146, 8712, 11, 21835, 362, 1466, 712, 24433, 6828, 322, 8819, 11, 1339, 21835, 362, 341, 2199, 51452], "temperature": 0.0, "avg_logprob": -0.11553311598928351, "compression_ratio": 1.835680751173709, "no_speech_prob": 0.0019265469163656235}, {"id": 137, "seek": 99616, "start": 1017.92, "end": 1024.8799999999999, "text": " linear summation operation on nodes. So, so, so in this sense, cans does not separate the linear", "tokens": [51452, 8213, 28811, 6916, 322, 13891, 13, 407, 11, 370, 11, 370, 294, 341, 2020, 11, 21835, 775, 406, 4994, 264, 8213, 51800], "temperature": 0.0, "avg_logprob": -0.11553311598928351, "compression_ratio": 1.835680751173709, "no_speech_prob": 0.0019265469163656235}, {"id": 138, "seek": 102488, "start": 1024.88, "end": 1031.92, "text": " part and the long inner part as MLPs do, but it integrates both the linear part and long inner", "tokens": [50364, 644, 293, 264, 938, 7284, 644, 382, 21601, 23043, 360, 11, 457, 309, 3572, 1024, 1293, 264, 8213, 644, 293, 938, 7284, 50716], "temperature": 0.0, "avg_logprob": -0.14656678490016772, "compression_ratio": 1.7962085308056872, "no_speech_prob": 0.0020504097919911146}, {"id": 139, "seek": 102488, "start": 1031.92, "end": 1037.6000000000001, "text": " part altogether into the can layer. And the can network is simply just the stack of the can layers.", "tokens": [50716, 644, 19051, 666, 264, 393, 4583, 13, 400, 264, 393, 3209, 307, 2935, 445, 264, 8630, 295, 264, 393, 7914, 13, 51000], "temperature": 0.0, "avg_logprob": -0.14656678490016772, "compression_ratio": 1.7962085308056872, "no_speech_prob": 0.0020504097919911146}, {"id": 140, "seek": 102488, "start": 1041.5200000000002, "end": 1050.0, "text": " Yeah, so in both cases, you are free to stack the model to become deeper and deeper because", "tokens": [51196, 865, 11, 370, 294, 1293, 3331, 11, 291, 366, 1737, 281, 8630, 264, 2316, 281, 1813, 7731, 293, 7731, 570, 51620], "temperature": 0.0, "avg_logprob": -0.14656678490016772, "compression_ratio": 1.7962085308056872, "no_speech_prob": 0.0020504097919911146}, {"id": 141, "seek": 102488, "start": 1050.0, "end": 1054.16, "text": " you have the basic notion of a layer, you just stack more layers to get the deeper networks.", "tokens": [51620, 291, 362, 264, 3875, 10710, 295, 257, 4583, 11, 291, 445, 8630, 544, 7914, 281, 483, 264, 7731, 9590, 13, 51828], "temperature": 0.0, "avg_logprob": -0.14656678490016772, "compression_ratio": 1.7962085308056872, "no_speech_prob": 0.0020504097919911146}, {"id": 142, "seek": 105488, "start": 1055.3600000000001, "end": 1065.68, "text": " Yeah, so that's the basic, that's the basic ideas of cans. And now I want to elaborate more", "tokens": [50388, 865, 11, 370, 300, 311, 264, 3875, 11, 300, 311, 264, 3875, 3487, 295, 21835, 13, 400, 586, 286, 528, 281, 20945, 544, 50904], "temperature": 0.0, "avg_logprob": -0.2283963404203716, "compression_ratio": 1.6337209302325582, "no_speech_prob": 0.0004727821215055883}, {"id": 143, "seek": 105488, "start": 1068.8000000000002, "end": 1074.0, "text": " like why do we care about, why do we care about this? What are the advantages that cans can bring", "tokens": [51060, 411, 983, 360, 321, 1127, 466, 11, 983, 360, 321, 1127, 466, 341, 30, 708, 366, 264, 14906, 300, 21835, 393, 1565, 51320], "temperature": 0.0, "avg_logprob": -0.2283963404203716, "compression_ratio": 1.6337209302325582, "no_speech_prob": 0.0004727821215055883}, {"id": 144, "seek": 105488, "start": 1074.0, "end": 1083.44, "text": " to us, but other black box models do not bring to us? So yeah, so the first property is the", "tokens": [51320, 281, 505, 11, 457, 661, 2211, 2424, 5245, 360, 406, 1565, 281, 505, 30, 407, 1338, 11, 370, 264, 700, 4707, 307, 264, 51792], "temperature": 0.0, "avg_logprob": -0.2283963404203716, "compression_ratio": 1.6337209302325582, "no_speech_prob": 0.0004727821215055883}, {"id": 145, "seek": 108344, "start": 1083.44, "end": 1089.6000000000001, "text": " scaling behavior of cans. As I mentioned before, the idea of cans is decomposing a", "tokens": [50364, 21589, 5223, 295, 21835, 13, 1018, 286, 2835, 949, 11, 264, 1558, 295, 21835, 307, 22867, 6110, 257, 50672], "temperature": 0.0, "avg_logprob": -0.13509626615615117, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0015009988564997911}, {"id": 146, "seek": 108344, "start": 1089.6000000000001, "end": 1094.3200000000002, "text": " high dimensional functions into one dimensional functions. So that looks like really promising", "tokens": [50672, 1090, 18795, 6828, 666, 472, 18795, 6828, 13, 407, 300, 1542, 411, 534, 20257, 50908], "temperature": 0.0, "avg_logprob": -0.13509626615615117, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0015009988564997911}, {"id": 147, "seek": 108344, "start": 1096.4, "end": 1103.28, "text": " that it can get us, it can get us out of the curse of dimensionality. Let's suppose we are", "tokens": [51012, 300, 309, 393, 483, 505, 11, 309, 393, 483, 505, 484, 295, 264, 17139, 295, 10139, 1860, 13, 961, 311, 7297, 321, 366, 51356], "temperature": 0.0, "avg_logprob": -0.13509626615615117, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0015009988564997911}, {"id": 148, "seek": 108344, "start": 1103.92, "end": 1110.48, "text": " trying to approximate a d dimensional functions. And I suppose the function has no structure at all.", "tokens": [51388, 1382, 281, 30874, 257, 274, 18795, 6828, 13, 400, 286, 7297, 264, 2445, 575, 572, 3877, 412, 439, 13, 51716], "temperature": 0.0, "avg_logprob": -0.13509626615615117, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0015009988564997911}, {"id": 149, "seek": 111048, "start": 1110.48, "end": 1117.44, "text": " So then we need to, we need to have a hypercube and have a uniform grid on the hypercube.", "tokens": [50364, 407, 550, 321, 643, 281, 11, 321, 643, 281, 362, 257, 9848, 66, 1977, 293, 362, 257, 9452, 10748, 322, 264, 9848, 66, 1977, 13, 50712], "temperature": 0.0, "avg_logprob": -0.11518617855605259, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0008558175759389997}, {"id": 150, "seek": 111048, "start": 1118.24, "end": 1124.72, "text": " Let's suppose we have 10 grid, 10 anchor points along each dimension, then we will need 10 to the", "tokens": [50752, 961, 311, 7297, 321, 362, 1266, 10748, 11, 1266, 18487, 2793, 2051, 1184, 10139, 11, 550, 321, 486, 643, 1266, 281, 264, 51076], "temperature": 0.0, "avg_logprob": -0.11518617855605259, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0008558175759389997}, {"id": 151, "seek": 111048, "start": 1124.72, "end": 1129.76, "text": " power of d number of anchor points in the d dimensional hypercube. So that's exponentially", "tokens": [51076, 1347, 295, 274, 1230, 295, 18487, 2793, 294, 264, 274, 18795, 9848, 66, 1977, 13, 407, 300, 311, 37330, 51328], "temperature": 0.0, "avg_logprob": -0.11518617855605259, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0008558175759389997}, {"id": 152, "seek": 111048, "start": 1130.64, "end": 1137.1200000000001, "text": " expensive. So if you do the classical approximation theory, you would notice that the approximation", "tokens": [51372, 5124, 13, 407, 498, 291, 360, 264, 13735, 28023, 5261, 11, 291, 576, 3449, 300, 264, 28023, 51696], "temperature": 0.0, "avg_logprob": -0.11518617855605259, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0008558175759389997}, {"id": 153, "seek": 113712, "start": 1137.12, "end": 1146.1599999999999, "text": " error would decay as a parallel, as a parallel, as a function of the number of input dimensionality.", "tokens": [50364, 6713, 576, 21039, 382, 257, 8952, 11, 382, 257, 8952, 11, 382, 257, 2445, 295, 264, 1230, 295, 4846, 10139, 1860, 13, 50816], "temperature": 0.0, "avg_logprob": -0.14848891975953407, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.004981073085218668}, {"id": 154, "seek": 113712, "start": 1146.1599999999999, "end": 1152.56, "text": " And it's one, and the exponent is one over d, meaning that you got exponentially", "tokens": [50816, 400, 309, 311, 472, 11, 293, 264, 37871, 307, 472, 670, 274, 11, 3620, 300, 291, 658, 37330, 51136], "temperature": 0.0, "avg_logprob": -0.14848891975953407, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.004981073085218668}, {"id": 155, "seek": 113712, "start": 1153.6799999999998, "end": 1158.7199999999998, "text": " like slower when you have more and more, when you have more and more dimensions, like if you need", "tokens": [51192, 411, 14009, 562, 291, 362, 544, 293, 544, 11, 562, 291, 362, 544, 293, 544, 12819, 11, 411, 498, 291, 643, 51444], "temperature": 0.0, "avg_logprob": -0.14848891975953407, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.004981073085218668}, {"id": 156, "seek": 113712, "start": 1158.7199999999998, "end": 1165.1999999999998, "text": " 10 points in one d, you would need 100 points in 2d, you will need 1000 points in 3d and so on.", "tokens": [51444, 1266, 2793, 294, 472, 274, 11, 291, 576, 643, 2319, 2793, 294, 568, 67, 11, 291, 486, 643, 9714, 2793, 294, 805, 67, 293, 370, 322, 13, 51768], "temperature": 0.0, "avg_logprob": -0.14848891975953407, "compression_ratio": 1.893939393939394, "no_speech_prob": 0.004981073085218668}, {"id": 157, "seek": 116712, "start": 1167.1999999999998, "end": 1174.1599999999999, "text": " But if the function has some structure, like if it has the chromograph unknown representation,", "tokens": [50368, 583, 498, 264, 2445, 575, 512, 3877, 11, 411, 498, 309, 575, 264, 16209, 3108, 9841, 10290, 11, 50716], "temperature": 0.0, "avg_logprob": -0.16441117800199068, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.000503303948789835}, {"id": 158, "seek": 116712, "start": 1174.1599999999999, "end": 1180.56, "text": " then we can decompose it into a bunch of 1d functions. And then our job would just be", "tokens": [50716, 550, 321, 393, 22867, 541, 309, 666, 257, 3840, 295, 502, 67, 6828, 13, 400, 550, 527, 1691, 576, 445, 312, 51036], "temperature": 0.0, "avg_logprob": -0.16441117800199068, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.000503303948789835}, {"id": 159, "seek": 116712, "start": 1180.56, "end": 1187.4399999999998, "text": " approximating 1d functions. So now effectively d becomes one. So you got a really, you got the", "tokens": [51036, 8542, 990, 502, 67, 6828, 13, 407, 586, 8659, 274, 3643, 472, 13, 407, 291, 658, 257, 534, 11, 291, 658, 264, 51380], "temperature": 0.0, "avg_logprob": -0.16441117800199068, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.000503303948789835}, {"id": 160, "seek": 116712, "start": 1187.4399999999998, "end": 1194.4799999999998, "text": " fastest possible scaling laws. But the, but the caveats, immediately the caveat is that we,", "tokens": [51380, 14573, 1944, 21589, 6064, 13, 583, 264, 11, 457, 264, 11730, 1720, 11, 4258, 264, 43012, 307, 300, 321, 11, 51732], "temperature": 0.0, "avg_logprob": -0.16441117800199068, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.000503303948789835}, {"id": 161, "seek": 119448, "start": 1194.56, "end": 1203.6, "text": " the assumption is that we, like the function has a smooth chromograph, a smooth finite size", "tokens": [50368, 264, 15302, 307, 300, 321, 11, 411, 264, 2445, 575, 257, 5508, 16209, 3108, 11, 257, 5508, 19362, 2744, 50820], "temperature": 0.0, "avg_logprob": -0.15661230648265165, "compression_ratio": 1.7596153846153846, "no_speech_prob": 0.0028892960399389267}, {"id": 162, "seek": 119448, "start": 1203.6, "end": 1209.68, "text": " chromograph unknown representation. All of this, you know, all of these objectives,", "tokens": [50820, 16209, 3108, 9841, 10290, 13, 1057, 295, 341, 11, 291, 458, 11, 439, 295, 613, 15961, 11, 51124], "temperature": 0.0, "avg_logprob": -0.15661230648265165, "compression_ratio": 1.7596153846153846, "no_speech_prob": 0.0028892960399389267}, {"id": 163, "seek": 119448, "start": 1209.68, "end": 1215.6, "text": " objectives like smooth or finite size are just, are just practical conditions for a real, for,", "tokens": [51124, 15961, 411, 5508, 420, 19362, 2744, 366, 445, 11, 366, 445, 8496, 4487, 337, 257, 957, 11, 337, 11, 51420], "temperature": 0.0, "avg_logprob": -0.15661230648265165, "compression_ratio": 1.7596153846153846, "no_speech_prob": 0.0028892960399389267}, {"id": 164, "seek": 119448, "start": 1216.64, "end": 1222.88, "text": " for a network which we have access in practice that can really learn the network. We want it to", "tokens": [51472, 337, 257, 3209, 597, 321, 362, 2105, 294, 3124, 300, 393, 534, 1466, 264, 3209, 13, 492, 528, 309, 281, 51784], "temperature": 0.0, "avg_logprob": -0.15661230648265165, "compression_ratio": 1.7596153846153846, "no_speech_prob": 0.0028892960399389267}, {"id": 165, "seek": 122288, "start": 1222.88, "end": 1228.4, "text": " be smooth because we parametrize it with b-splice, which are smooth. We want them to be finite size", "tokens": [50364, 312, 5508, 570, 321, 6220, 302, 470, 1381, 309, 365, 272, 12, 46535, 573, 11, 597, 366, 5508, 13, 492, 528, 552, 281, 312, 19362, 2744, 50640], "temperature": 0.0, "avg_logprob": -0.2210643140575554, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.0008829875150695443}, {"id": 166, "seek": 122288, "start": 1228.4, "end": 1235.5200000000002, "text": " because, of course, you know, we cannot initialize, we cannot deal with an infinite size neural network.", "tokens": [50640, 570, 11, 295, 1164, 11, 291, 458, 11, 321, 2644, 5883, 1125, 11, 321, 2644, 2028, 365, 364, 13785, 2744, 18161, 3209, 13, 50996], "temperature": 0.0, "avg_logprob": -0.2210643140575554, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.0008829875150695443}, {"id": 167, "seek": 122288, "start": 1238.72, "end": 1244.72, "text": " So, yeah, so, so, so we just did some sandwich check on, on some symbolic formulas.", "tokens": [51156, 407, 11, 1338, 11, 370, 11, 370, 11, 370, 321, 445, 630, 512, 11141, 1520, 322, 11, 322, 512, 25755, 30546, 13, 51456], "temperature": 0.0, "avg_logprob": -0.2210643140575554, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.0008829875150695443}, {"id": 168, "seek": 124472, "start": 1245.1200000000001, "end": 1251.28, "text": " So, yeah, so symbolic formulas are like white, are like what we used in, in science. So that's why we", "tokens": [50384, 407, 11, 1338, 11, 370, 25755, 30546, 366, 411, 2418, 11, 366, 411, 437, 321, 1143, 294, 11, 294, 3497, 13, 407, 300, 311, 983, 321, 50692], "temperature": 0.0, "avg_logprob": -0.26164735393759647, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.005219589918851852}, {"id": 169, "seek": 124472, "start": 1251.28, "end": 1258.64, "text": " test them first. So let's see. So the red dashed line is the theoretical prediction. Here we are", "tokens": [50692, 1500, 552, 700, 13, 407, 718, 311, 536, 13, 407, 264, 2182, 8240, 292, 1622, 307, 264, 20864, 17630, 13, 1692, 321, 366, 51060], "temperature": 0.0, "avg_logprob": -0.26164735393759647, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.005219589918851852}, {"id": 170, "seek": 124472, "start": 1258.64, "end": 1272.0, "text": " using cubic spline. So k is k3. And the scaling exponent is k plus one equals four. And the", "tokens": [51060, 1228, 28733, 4732, 533, 13, 407, 350, 307, 350, 18, 13, 400, 264, 21589, 37871, 307, 350, 1804, 472, 6915, 1451, 13, 400, 264, 51728], "temperature": 0.0, "avg_logprob": -0.26164735393759647, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.005219589918851852}, {"id": 171, "seek": 127200, "start": 1272.88, "end": 1281.92, "text": " curve, yeah, so the thick blue line is for the can network. And you see that almost like the,", "tokens": [50408, 7605, 11, 1338, 11, 370, 264, 5060, 3344, 1622, 307, 337, 264, 393, 3209, 13, 400, 291, 536, 300, 1920, 411, 264, 11, 50860], "temperature": 0.0, "avg_logprob": -0.16856894081021534, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.0014547688188031316}, {"id": 172, "seek": 127200, "start": 1281.92, "end": 1288.24, "text": " like empirical results for the can network almost agreed with, almost agrees with the", "tokens": [50860, 411, 31886, 3542, 337, 264, 393, 3209, 1920, 9166, 365, 11, 1920, 26383, 365, 264, 51176], "temperature": 0.0, "avg_logprob": -0.16856894081021534, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.0014547688188031316}, {"id": 173, "seek": 127200, "start": 1289.36, "end": 1295.12, "text": " theoretical prediction, although sometimes performed slightly worse. Or in this case,", "tokens": [51232, 20864, 17630, 11, 4878, 2171, 10332, 4748, 5324, 13, 1610, 294, 341, 1389, 11, 51520], "temperature": 0.0, "avg_logprob": -0.16856894081021534, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.0014547688188031316}, {"id": 174, "seek": 127200, "start": 1295.12, "end": 1300.16, "text": " in the second to last case, there's a hundred dimensional case. And it performs much worse", "tokens": [51520, 294, 264, 1150, 281, 1036, 1389, 11, 456, 311, 257, 3262, 18795, 1389, 13, 400, 309, 26213, 709, 5324, 51772], "temperature": 0.0, "avg_logprob": -0.16856894081021534, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.0014547688188031316}, {"id": 175, "seek": 130016, "start": 1300.16, "end": 1305.44, "text": " than a theoretical prediction because of the, the, the, the, because the dimension is just too", "tokens": [50364, 813, 257, 20864, 17630, 570, 295, 264, 11, 264, 11, 264, 11, 264, 11, 570, 264, 10139, 307, 445, 886, 50628], "temperature": 0.0, "avg_logprob": -0.18805049620952802, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.005137907341122627}, {"id": 176, "seek": 130016, "start": 1305.44, "end": 1312.0800000000002, "text": " high and the network can get, can get stuck at some local minima or whatever. But, but nevertheless,", "tokens": [50628, 1090, 293, 264, 3209, 393, 483, 11, 393, 483, 5541, 412, 512, 2654, 4464, 64, 420, 2035, 13, 583, 11, 457, 26924, 11, 50960], "temperature": 0.0, "avg_logprob": -0.18805049620952802, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.005137907341122627}, {"id": 177, "seek": 130016, "start": 1312.0800000000002, "end": 1319.68, "text": " it's still output from MLPs, which you see up in the upright corner here, like the case really", "tokens": [50960, 309, 311, 920, 5598, 490, 21601, 23043, 11, 597, 291, 536, 493, 294, 264, 27405, 4538, 510, 11, 411, 264, 1389, 534, 51340], "temperature": 0.0, "avg_logprob": -0.18805049620952802, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.005137907341122627}, {"id": 178, "seek": 130016, "start": 1319.68, "end": 1327.0400000000002, "text": " slow. But, but the cans, but cans at least can output form MLPs to a great margin,", "tokens": [51340, 2964, 13, 583, 11, 457, 264, 21835, 11, 457, 21835, 412, 1935, 393, 5598, 1254, 21601, 23043, 281, 257, 869, 10270, 11, 51708], "temperature": 0.0, "avg_logprob": -0.18805049620952802, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.005137907341122627}, {"id": 179, "seek": 132704, "start": 1327.04, "end": 1334.72, "text": " although still not saturating the theoretical prediction. But still the scaling law that can", "tokens": [50364, 4878, 920, 406, 21160, 990, 264, 20864, 17630, 13, 583, 920, 264, 21589, 2101, 300, 393, 50748], "temperature": 0.0, "avg_logprob": -0.16234555656527294, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0005357360932976007}, {"id": 180, "seek": 132704, "start": 1334.72, "end": 1343.04, "text": " shows looks, looks promising that it's, it seems to not fully beats the curse of dimensionality,", "tokens": [50748, 3110, 1542, 11, 1542, 20257, 300, 309, 311, 11, 309, 2544, 281, 406, 4498, 16447, 264, 17139, 295, 10139, 1860, 11, 51164], "temperature": 0.0, "avg_logprob": -0.16234555656527294, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0005357360932976007}, {"id": 181, "seek": 132704, "start": 1343.04, "end": 1345.68, "text": " but at least partially beat the curse of dimensionality.", "tokens": [51164, 457, 412, 1935, 18886, 4224, 264, 17139, 295, 10139, 1860, 13, 51296], "temperature": 0.0, "avg_logprob": -0.16234555656527294, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0005357360932976007}, {"id": 182, "seek": 132704, "start": 1349.12, "end": 1355.92, "text": " Well, yeah, yeah. So, yeah, just to play the devil's advocate here, you may immediately notice", "tokens": [51468, 1042, 11, 1338, 11, 1338, 13, 407, 11, 1338, 11, 445, 281, 862, 264, 13297, 311, 14608, 510, 11, 291, 815, 4258, 3449, 51808], "temperature": 0.0, "avg_logprob": -0.16234555656527294, "compression_ratio": 1.6634146341463414, "no_speech_prob": 0.0005357360932976007}, {"id": 183, "seek": 135592, "start": 1355.92, "end": 1362.24, "text": " that I'm on purpose just deliberately using this symbolic formulas. You, you might be wondering,", "tokens": [50364, 300, 286, 478, 322, 4334, 445, 23506, 1228, 341, 25755, 30546, 13, 509, 11, 291, 1062, 312, 6359, 11, 50680], "temperature": 0.0, "avg_logprob": -0.1460877695391255, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.004828788340091705}, {"id": 184, "seek": 135592, "start": 1362.24, "end": 1369.92, "text": " well, maybe, maybe the functions we care. We encounter a lot in nature may not be symbolic,", "tokens": [50680, 731, 11, 1310, 11, 1310, 264, 6828, 321, 1127, 13, 492, 8593, 257, 688, 294, 3687, 815, 406, 312, 25755, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1460877695391255, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.004828788340091705}, {"id": 185, "seek": 135592, "start": 1369.92, "end": 1375.8400000000001, "text": " they might be some weird, you know, like, at least for special functions, they, they, they,", "tokens": [51064, 436, 1062, 312, 512, 3657, 11, 291, 458, 11, 411, 11, 412, 1935, 337, 2121, 6828, 11, 436, 11, 436, 11, 436, 11, 51360], "temperature": 0.0, "avg_logprob": -0.1460877695391255, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.004828788340091705}, {"id": 186, "seek": 135592, "start": 1375.8400000000001, "end": 1382.16, "text": " they are like infinite series, which are hard to be represented with just the finite network. So,", "tokens": [51360, 436, 366, 411, 13785, 2638, 11, 597, 366, 1152, 281, 312, 10379, 365, 445, 264, 19362, 3209, 13, 407, 11, 51676], "temperature": 0.0, "avg_logprob": -0.1460877695391255, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.004828788340091705}, {"id": 187, "seek": 138216, "start": 1382.16, "end": 1387.2, "text": " what are, what, what, so, so, so, so what's the cans performance in that scenario?", "tokens": [50364, 437, 366, 11, 437, 11, 437, 11, 370, 11, 370, 11, 370, 11, 370, 437, 311, 264, 21835, 3389, 294, 300, 9005, 30, 50616], "temperature": 0.0, "avg_logprob": -0.15090504752265083, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.00039201934123411775}, {"id": 188, "seek": 138216, "start": 1388.5600000000002, "end": 1394.88, "text": " So, yeah, so we just tried some special functions, which we know for most of them, they do not have", "tokens": [50684, 407, 11, 1338, 11, 370, 321, 445, 3031, 512, 2121, 6828, 11, 597, 321, 458, 337, 881, 295, 552, 11, 436, 360, 406, 362, 51000], "temperature": 0.0, "avg_logprob": -0.15090504752265083, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.00039201934123411775}, {"id": 189, "seek": 138216, "start": 1394.88, "end": 1403.3600000000001, "text": " analytical formulas. And indeed, we see that the scaling laws of cans do not saturate the", "tokens": [51000, 29579, 30546, 13, 400, 6451, 11, 321, 536, 300, 264, 21589, 6064, 295, 21835, 360, 406, 21160, 473, 264, 51424], "temperature": 0.0, "avg_logprob": -0.15090504752265083, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.00039201934123411775}, {"id": 190, "seek": 138216, "start": 1403.3600000000001, "end": 1408.72, "text": " theoretical prediction, meaning that probably you cannot decompose like a high dimensional", "tokens": [51424, 20864, 17630, 11, 3620, 300, 1391, 291, 2644, 22867, 541, 411, 257, 1090, 18795, 51692], "temperature": 0.0, "avg_logprob": -0.15090504752265083, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.00039201934123411775}, {"id": 191, "seek": 140872, "start": 1408.72, "end": 1415.6000000000001, "text": " functions into just a one D functions. But still, our goal here is to outcompetes MLPs.", "tokens": [50364, 6828, 666, 445, 257, 472, 413, 6828, 13, 583, 920, 11, 527, 3387, 510, 307, 281, 484, 1112, 7275, 279, 21601, 23043, 13, 50708], "temperature": 0.0, "avg_logprob": -0.23726310192699163, "compression_ratio": 1.5625, "no_speech_prob": 0.010813544504344463}, {"id": 192, "seek": 140872, "start": 1416.24, "end": 1421.68, "text": " And, and, and it's a feature, not a bug, like not all the functions can be decomposed into", "tokens": [50740, 400, 11, 293, 11, 293, 309, 311, 257, 4111, 11, 406, 257, 7426, 11, 411, 406, 439, 264, 6828, 393, 312, 22867, 1744, 666, 51012], "temperature": 0.0, "avg_logprob": -0.23726310192699163, "compression_ratio": 1.5625, "no_speech_prob": 0.010813544504344463}, {"id": 193, "seek": 140872, "start": 1421.68, "end": 1431.6000000000001, "text": " smooth finite representations of this K representation. They may admit non smooth finite size or", "tokens": [51012, 5508, 19362, 33358, 295, 341, 591, 10290, 13, 814, 815, 9796, 2107, 5508, 19362, 2744, 420, 51508], "temperature": 0.0, "avg_logprob": -0.23726310192699163, "compression_ratio": 1.5625, "no_speech_prob": 0.010813544504344463}, {"id": 194, "seek": 143160, "start": 1431.6, "end": 1438.8799999999999, "text": " smooth infinite size. But in, but, but neither case is, is like the, is what's accessible in", "tokens": [50364, 5508, 13785, 2744, 13, 583, 294, 11, 457, 11, 457, 9662, 1389, 307, 11, 307, 411, 264, 11, 307, 437, 311, 9515, 294, 50728], "temperature": 0.0, "avg_logprob": -0.1345335935291491, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.006487620994448662}, {"id": 195, "seek": 143160, "start": 1438.8799999999999, "end": 1447.1999999999998, "text": " practice. So, yeah, so, so here we show that in most cases, we can achieve this minus two", "tokens": [50728, 3124, 13, 407, 11, 1338, 11, 370, 11, 370, 510, 321, 855, 300, 294, 881, 3331, 11, 321, 393, 4584, 341, 3175, 732, 51144], "temperature": 0.0, "avg_logprob": -0.1345335935291491, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.006487620994448662}, {"id": 196, "seek": 143160, "start": 1447.1999999999998, "end": 1454.32, "text": " scaling law, which means that the can network, well, well, sorry. So here, this special functions", "tokens": [51144, 21589, 2101, 11, 597, 1355, 300, 264, 393, 3209, 11, 731, 11, 731, 11, 2597, 13, 407, 510, 11, 341, 2121, 6828, 51500], "temperature": 0.0, "avg_logprob": -0.1345335935291491, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.006487620994448662}, {"id": 197, "seek": 145432, "start": 1454.3999999999999, "end": 1462.0, "text": " are all just two dimensional. So this, so these, according to the spline theory, it would predict", "tokens": [50368, 366, 439, 445, 732, 18795, 13, 407, 341, 11, 370, 613, 11, 4650, 281, 264, 4732, 533, 5261, 11, 309, 576, 6069, 50748], "temperature": 0.0, "avg_logprob": -0.17121334075927735, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.010325906798243523}, {"id": 198, "seek": 145432, "start": 1462.0, "end": 1467.9199999999998, "text": " two dimensional, it would predicts a two dimension, like, like, like a scaling exponent to be two,", "tokens": [50748, 732, 18795, 11, 309, 576, 6069, 82, 257, 732, 10139, 11, 411, 11, 411, 11, 411, 257, 21589, 37871, 281, 312, 732, 11, 51044], "temperature": 0.0, "avg_logprob": -0.17121334075927735, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.010325906798243523}, {"id": 199, "seek": 145432, "start": 1467.9199999999998, "end": 1473.6, "text": " which agrees with the can results. But in some, but in some case, we can, we can still got the", "tokens": [51044, 597, 26383, 365, 264, 393, 3542, 13, 583, 294, 512, 11, 457, 294, 512, 1389, 11, 321, 393, 11, 321, 393, 920, 658, 264, 51328], "temperature": 0.0, "avg_logprob": -0.17121334075927735, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.010325906798243523}, {"id": 200, "seek": 145432, "start": 1473.6, "end": 1481.6799999999998, "text": " minus four scaling law. And the reason is that actually, this is secretly, like, like, although", "tokens": [51328, 3175, 1451, 21589, 2101, 13, 400, 264, 1778, 307, 300, 767, 11, 341, 307, 22611, 11, 411, 11, 411, 11, 4878, 51732], "temperature": 0.0, "avg_logprob": -0.17121334075927735, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.010325906798243523}, {"id": 201, "seek": 148168, "start": 1481.68, "end": 1487.44, "text": " we call, we call it a special function, spherical harmonics are not that special in the sense that", "tokens": [50364, 321, 818, 11, 321, 818, 309, 257, 2121, 2445, 11, 37300, 14750, 1167, 366, 406, 300, 2121, 294, 264, 2020, 300, 50652], "temperature": 0.0, "avg_logprob": -0.12921530791003294, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.00034597940975800157}, {"id": 202, "seek": 148168, "start": 1487.44, "end": 1494.3200000000002, "text": " they're still decomposable. So, so you can get the minus four scaling. But also in other, in other", "tokens": [50652, 436, 434, 920, 22867, 329, 712, 13, 407, 11, 370, 291, 393, 483, 264, 3175, 1451, 21589, 13, 583, 611, 294, 661, 11, 294, 661, 50996], "temperature": 0.0, "avg_logprob": -0.12921530791003294, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.00034597940975800157}, {"id": 203, "seek": 148168, "start": 1494.3200000000002, "end": 1499.28, "text": " cases, you've got some worse behavior, like you've got the minus one scaling, which means that the", "tokens": [50996, 3331, 11, 291, 600, 658, 512, 5324, 5223, 11, 411, 291, 600, 658, 264, 3175, 472, 21589, 11, 597, 1355, 300, 264, 51244], "temperature": 0.0, "avg_logprob": -0.12921530791003294, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.00034597940975800157}, {"id": 204, "seek": 148168, "start": 1499.28, "end": 1506.88, "text": " can is underperforming even out on the performing compared to like the spline theory. But what's", "tokens": [51244, 393, 307, 833, 26765, 278, 754, 484, 322, 264, 10205, 5347, 281, 411, 264, 4732, 533, 5261, 13, 583, 437, 311, 51624], "temperature": 0.0, "avg_logprob": -0.12921530791003294, "compression_ratio": 1.7466666666666666, "no_speech_prob": 0.00034597940975800157}, {"id": 205, "seek": 150688, "start": 1506.88, "end": 1514.0800000000002, "text": " interesting is that in this case, MLPs are even more underperforming than cans. So, so this may", "tokens": [50364, 1880, 307, 300, 294, 341, 1389, 11, 21601, 23043, 366, 754, 544, 833, 26765, 278, 813, 21835, 13, 407, 11, 370, 341, 815, 50724], "temperature": 0.0, "avg_logprob": -0.12337821595212246, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.013018906116485596}, {"id": 206, "seek": 150688, "start": 1514.0800000000002, "end": 1520.48, "text": " tell us that maybe for low dimensional problems, neural networks are not that necessary. And even", "tokens": [50724, 980, 505, 300, 1310, 337, 2295, 18795, 2740, 11, 18161, 9590, 366, 406, 300, 4818, 13, 400, 754, 51044], "temperature": 0.0, "avg_logprob": -0.12337821595212246, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.013018906116485596}, {"id": 207, "seek": 150688, "start": 1520.48, "end": 1526.88, "text": " the spline theory, like the spline approximation can outcompete the neural network. So, so it's", "tokens": [51044, 264, 4732, 533, 5261, 11, 411, 264, 4732, 533, 28023, 393, 484, 21541, 3498, 264, 18161, 3209, 13, 407, 11, 370, 309, 311, 51364], "temperature": 0.0, "avg_logprob": -0.12337821595212246, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.013018906116485596}, {"id": 208, "seek": 150688, "start": 1526.88, "end": 1534.88, "text": " something to be think about. It's something good to keep in mind. Because neural networks just have", "tokens": [51364, 746, 281, 312, 519, 466, 13, 467, 311, 746, 665, 281, 1066, 294, 1575, 13, 1436, 18161, 9590, 445, 362, 51764], "temperature": 0.0, "avg_logprob": -0.12337821595212246, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.013018906116485596}, {"id": 209, "seek": 153488, "start": 1534.88, "end": 1539.7600000000002, "text": " too many degrees of freedom and may have optimization issue.", "tokens": [50364, 886, 867, 5310, 295, 5645, 293, 815, 362, 19618, 2734, 13, 50608], "temperature": 0.0, "avg_logprob": -0.1698145139015327, "compression_ratio": 1.5088757396449703, "no_speech_prob": 0.0003740662068594247}, {"id": 210, "seek": 153488, "start": 1542.48, "end": 1550.0, "text": " Yeah. So, so beyond function approximation, we can go on to solving partial differential equations.", "tokens": [50744, 865, 13, 407, 11, 370, 4399, 2445, 28023, 11, 321, 393, 352, 322, 281, 12606, 14641, 15756, 11787, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1698145139015327, "compression_ratio": 1.5088757396449703, "no_speech_prob": 0.0003740662068594247}, {"id": 211, "seek": 153488, "start": 1550.0, "end": 1556.0800000000002, "text": " So in the setup of physics in form learning, basically, we're trying to solve, we're basically", "tokens": [51120, 407, 294, 264, 8657, 295, 10649, 294, 1254, 2539, 11, 1936, 11, 321, 434, 1382, 281, 5039, 11, 321, 434, 1936, 51424], "temperature": 0.0, "avg_logprob": -0.1698145139015327, "compression_ratio": 1.5088757396449703, "no_speech_prob": 0.0003740662068594247}, {"id": 212, "seek": 155608, "start": 1556.08, "end": 1568.6399999999999, "text": " trying to represent the solution of a PDE with MLP or with a can network. So, so the only difference", "tokens": [50364, 1382, 281, 2906, 264, 3827, 295, 257, 10464, 36, 365, 21601, 47, 420, 365, 257, 393, 3209, 13, 407, 11, 370, 264, 787, 2649, 50992], "temperature": 0.0, "avg_logprob": -0.12914686936598557, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0006165942177176476}, {"id": 213, "seek": 155608, "start": 1568.6399999999999, "end": 1573.52, "text": " from the previous results from the previous experiment is that we're just we're using the physics", "tokens": [50992, 490, 264, 3894, 3542, 490, 264, 3894, 5120, 307, 300, 321, 434, 445, 321, 434, 1228, 264, 10649, 51236], "temperature": 0.0, "avg_logprob": -0.12914686936598557, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0006165942177176476}, {"id": 214, "seek": 155608, "start": 1573.52, "end": 1579.9199999999998, "text": " informed loss rather than the regression loss. So the optimization becomes more complicated,", "tokens": [51236, 11740, 4470, 2831, 813, 264, 24590, 4470, 13, 407, 264, 19618, 3643, 544, 6179, 11, 51556], "temperature": 0.0, "avg_logprob": -0.12914686936598557, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0006165942177176476}, {"id": 215, "seek": 157992, "start": 1579.92, "end": 1586.48, "text": " but still it's just approximating some function. Yeah. So, so we still see that we with cans,", "tokens": [50364, 457, 920, 309, 311, 445, 8542, 990, 512, 2445, 13, 865, 13, 407, 11, 370, 321, 920, 536, 300, 321, 365, 21835, 11, 50692], "temperature": 0.0, "avg_logprob": -0.18387256579452685, "compression_ratio": 1.5485436893203883, "no_speech_prob": 0.010011120699346066}, {"id": 216, "seek": 157992, "start": 1586.48, "end": 1593.92, "text": " we can get this optimal scaling law. Well, with MLPs, you see, while with MLPs, you see that,", "tokens": [50692, 321, 393, 483, 341, 16252, 21589, 2101, 13, 1042, 11, 365, 21601, 23043, 11, 291, 536, 11, 1339, 365, 21601, 23043, 11, 291, 536, 300, 11, 51064], "temperature": 0.0, "avg_logprob": -0.18387256579452685, "compression_ratio": 1.5485436893203883, "no_speech_prob": 0.010011120699346066}, {"id": 217, "seek": 157992, "start": 1593.92, "end": 1597.8400000000001, "text": " well, it has the skill at first, but it plateaus really fast and then", "tokens": [51064, 731, 11, 309, 575, 264, 5389, 412, 700, 11, 457, 309, 5924, 8463, 534, 2370, 293, 550, 51260], "temperature": 0.0, "avg_logprob": -0.18387256579452685, "compression_ratio": 1.5485436893203883, "no_speech_prob": 0.010011120699346066}, {"id": 218, "seek": 157992, "start": 1598.72, "end": 1603.44, "text": " does not improve when you have more parameters beyond 10,000.", "tokens": [51304, 775, 406, 3470, 562, 291, 362, 544, 9834, 4399, 1266, 11, 1360, 13, 51540], "temperature": 0.0, "avg_logprob": -0.18387256579452685, "compression_ratio": 1.5485436893203883, "no_speech_prob": 0.010011120699346066}, {"id": 219, "seek": 160344, "start": 1603.44, "end": 1612.48, "text": " Um, besides being more accurate, we can also gain some insight what the network is learning.", "tokens": [50364, 3301, 11, 11868, 885, 544, 8559, 11, 321, 393, 611, 6052, 512, 11269, 437, 264, 3209, 307, 2539, 13, 50816], "temperature": 0.0, "avg_logprob": -0.17699571277784265, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00034596683690324426}, {"id": 220, "seek": 160344, "start": 1612.48, "end": 1618.72, "text": " So, so, yes, yes. So for this example, we can, for this PDE example, we can actually visualize", "tokens": [50816, 407, 11, 370, 11, 2086, 11, 2086, 13, 407, 337, 341, 1365, 11, 321, 393, 11, 337, 341, 10464, 36, 1365, 11, 321, 393, 767, 23273, 51128], "temperature": 0.0, "avg_logprob": -0.17699571277784265, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00034596683690324426}, {"id": 221, "seek": 160344, "start": 1619.28, "end": 1625.3600000000001, "text": " the can network like this. And immediately you can see that there's some like sine waves and", "tokens": [51156, 264, 393, 3209, 411, 341, 13, 400, 4258, 291, 393, 536, 300, 456, 311, 512, 411, 18609, 9417, 293, 51460], "temperature": 0.0, "avg_logprob": -0.17699571277784265, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00034596683690324426}, {"id": 222, "seek": 160344, "start": 1625.3600000000001, "end": 1631.8400000000001, "text": " there's some linear functions. And you can even do symbolic regression to it, like,", "tokens": [51460, 456, 311, 512, 8213, 6828, 13, 400, 291, 393, 754, 360, 25755, 24590, 281, 309, 11, 411, 11, 51784], "temperature": 0.0, "avg_logprob": -0.17699571277784265, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00034596683690324426}, {"id": 223, "seek": 163184, "start": 1632.08, "end": 1637.28, "text": " like, like our software provides you a way to do this, you can, you can do symbolic regression", "tokens": [50376, 411, 11, 411, 527, 4722, 6417, 291, 257, 636, 281, 360, 341, 11, 291, 393, 11, 291, 393, 360, 25755, 24590, 50636], "temperature": 0.0, "avg_logprob": -0.14563450116789742, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.001700604218058288}, {"id": 224, "seek": 163184, "start": 1637.28, "end": 1644.1599999999999, "text": " to it. And after you do this, you can do some further training. And you, and you can even extract", "tokens": [50636, 281, 309, 13, 400, 934, 291, 360, 341, 11, 291, 393, 360, 512, 3052, 3097, 13, 400, 291, 11, 293, 291, 393, 754, 8947, 50980], "temperature": 0.0, "avg_logprob": -0.14563450116789742, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.001700604218058288}, {"id": 225, "seek": 163184, "start": 1644.1599999999999, "end": 1650.08, "text": " out the symbolic formula, which gives you like a loss down to machine precision. This is something", "tokens": [50980, 484, 264, 25755, 8513, 11, 597, 2709, 291, 411, 257, 4470, 760, 281, 3479, 18356, 13, 639, 307, 746, 51276], "temperature": 0.0, "avg_logprob": -0.14563450116789742, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.001700604218058288}, {"id": 226, "seek": 163184, "start": 1650.8, "end": 1658.48, "text": " that that that standard neural networks would not give you because of because they usually", "tokens": [51312, 300, 300, 300, 3832, 18161, 9590, 576, 406, 976, 291, 570, 295, 570, 436, 2673, 51696], "temperature": 0.0, "avg_logprob": -0.14563450116789742, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.001700604218058288}, {"id": 227, "seek": 165848, "start": 1658.48, "end": 1663.76, "text": " cannot convert a neural network into a symbolic formula very easily, but with cans, you can easily", "tokens": [50364, 2644, 7620, 257, 18161, 3209, 666, 257, 25755, 8513, 588, 3612, 11, 457, 365, 21835, 11, 291, 393, 3612, 50628], "temperature": 0.0, "avg_logprob": -0.15546812138087313, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0011694632703438401}, {"id": 228, "seek": 165848, "start": 1663.76, "end": 1674.72, "text": " do that. Another property of cans is that it has this property of continual learning, at least", "tokens": [50628, 360, 300, 13, 3996, 4707, 295, 21835, 307, 300, 309, 575, 341, 4707, 295, 1421, 901, 2539, 11, 412, 1935, 51176], "temperature": 0.0, "avg_logprob": -0.15546812138087313, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0011694632703438401}, {"id": 229, "seek": 165848, "start": 1674.72, "end": 1682.88, "text": " in 1D. Yeah, so people have, people have found that, you know, this claim might be invalid for", "tokens": [51176, 294, 502, 35, 13, 865, 11, 370, 561, 362, 11, 561, 362, 1352, 300, 11, 291, 458, 11, 341, 3932, 1062, 312, 34702, 337, 51584], "temperature": 0.0, "avg_logprob": -0.15546812138087313, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.0011694632703438401}, {"id": 230, "seek": 168288, "start": 1682.88, "end": 1690.0800000000002, "text": " high dimensions. So take my word with a grain of salt. But at least for 1D, the case is like,", "tokens": [50364, 1090, 12819, 13, 407, 747, 452, 1349, 365, 257, 12837, 295, 5139, 13, 583, 412, 1935, 337, 502, 35, 11, 264, 1389, 307, 411, 11, 50724], "temperature": 0.0, "avg_logprob": -0.13940203035032595, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.016401391476392746}, {"id": 231, "seek": 168288, "start": 1691.0400000000002, "end": 1698.4, "text": " we have, we want to approximate this 1D functions with five peaks. But instead of feeding auto data", "tokens": [50772, 321, 362, 11, 321, 528, 281, 30874, 341, 502, 35, 6828, 365, 1732, 26897, 13, 583, 2602, 295, 12919, 8399, 1412, 51140], "temperature": 0.0, "avg_logprob": -0.13940203035032595, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.016401391476392746}, {"id": 232, "seek": 168288, "start": 1698.4, "end": 1705.92, "text": " at once to the network, we're feeding each time we just feed one peak to the network. And we do", "tokens": [51140, 412, 1564, 281, 264, 3209, 11, 321, 434, 12919, 1184, 565, 321, 445, 3154, 472, 10651, 281, 264, 3209, 13, 400, 321, 360, 51516], "temperature": 0.0, "avg_logprob": -0.13940203035032595, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.016401391476392746}, {"id": 233, "seek": 170592, "start": 1705.92, "end": 1713.1200000000001, "text": " the sequential learning at each stage, the network is just, is just fed with just one peak.", "tokens": [50364, 264, 42881, 2539, 412, 1184, 3233, 11, 264, 3209, 307, 445, 11, 307, 445, 4636, 365, 445, 472, 10651, 13, 50724], "temperature": 0.0, "avg_logprob": -0.15539073944091797, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.027582455426454544}, {"id": 234, "seek": 170592, "start": 1713.1200000000001, "end": 1720.0, "text": " And with cans, and with cans, because we're using the local B-splines, so when it sees the new data,", "tokens": [50724, 400, 365, 21835, 11, 293, 365, 21835, 11, 570, 321, 434, 1228, 264, 2654, 363, 12, 46535, 1652, 11, 370, 562, 309, 8194, 264, 777, 1412, 11, 51068], "temperature": 0.0, "avg_logprob": -0.15539073944091797, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.027582455426454544}, {"id": 235, "seek": 170592, "start": 1720.0, "end": 1726.64, "text": " it does not update the parameters correspond to the old data. So it has this, it can't get rid of", "tokens": [51068, 309, 775, 406, 5623, 264, 9834, 6805, 281, 264, 1331, 1412, 13, 407, 309, 575, 341, 11, 309, 393, 380, 483, 3973, 295, 51400], "temperature": 0.0, "avg_logprob": -0.15539073944091797, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.027582455426454544}, {"id": 236, "seek": 170592, "start": 1726.64, "end": 1735.52, "text": " this catastrophic forgetting, like when new data are coming in, the can is able to memorize the", "tokens": [51400, 341, 34915, 25428, 11, 411, 562, 777, 1412, 366, 1348, 294, 11, 264, 393, 307, 1075, 281, 27478, 264, 51844], "temperature": 0.0, "avg_logprob": -0.15539073944091797, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.027582455426454544}, {"id": 237, "seek": 173552, "start": 1735.52, "end": 1742.48, "text": " old data and still do quite well on the old data. But this is not true for MLPs. Like when you're,", "tokens": [50364, 1331, 1412, 293, 920, 360, 1596, 731, 322, 264, 1331, 1412, 13, 583, 341, 307, 406, 2074, 337, 21601, 23043, 13, 1743, 562, 291, 434, 11, 50712], "temperature": 0.0, "avg_logprob": -0.1417333268627678, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.0023229485377669334}, {"id": 238, "seek": 173552, "start": 1742.48, "end": 1748.08, "text": " when you're fed, when the MLPs are fed with the new data, it catastrophically, they catastrophically", "tokens": [50712, 562, 291, 434, 4636, 11, 562, 264, 21601, 23043, 366, 4636, 365, 264, 777, 1412, 11, 309, 28363, 984, 11, 436, 28363, 984, 50992], "temperature": 0.0, "avg_logprob": -0.1417333268627678, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.0023229485377669334}, {"id": 239, "seek": 173552, "start": 1748.08, "end": 1754.24, "text": " forget about the old data. Because in MLPs, you usually have this global activation functions", "tokens": [50992, 2870, 466, 264, 1331, 1412, 13, 1436, 294, 21601, 23043, 11, 291, 2673, 362, 341, 4338, 24433, 6828, 51300], "temperature": 0.0, "avg_logprob": -0.1417333268627678, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.0023229485377669334}, {"id": 240, "seek": 173552, "start": 1754.24, "end": 1760.72, "text": " like the silo functions or Reilu functions. So whenever you make adjustments locally, it will", "tokens": [51300, 411, 264, 3425, 78, 6828, 420, 1300, 388, 84, 6828, 13, 407, 5699, 291, 652, 18624, 16143, 11, 309, 486, 51624], "temperature": 0.0, "avg_logprob": -0.1417333268627678, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.0023229485377669334}, {"id": 241, "seek": 176072, "start": 1760.72, "end": 1766.48, "text": " affect, you know, the predictions far away. So that's the reason why MLPs would not keep", "tokens": [50364, 3345, 11, 291, 458, 11, 264, 21264, 1400, 1314, 13, 407, 300, 311, 264, 1778, 983, 21601, 23043, 576, 406, 1066, 50652], "temperature": 0.0, "avg_logprob": -0.1438240275663488, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0034831042867153883}, {"id": 242, "seek": 176072, "start": 1768.24, "end": 1772.08, "text": " would, would catastrophically forget about the old data.", "tokens": [50740, 576, 11, 576, 28363, 984, 2870, 466, 264, 1331, 1412, 13, 50932], "temperature": 0.0, "avg_logprob": -0.1438240275663488, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0034831042867153883}, {"id": 243, "seek": 176072, "start": 1774.88, "end": 1782.08, "text": " Yeah, so, so, so that's the first part about accuracy. The second part is about interpretability.", "tokens": [51072, 865, 11, 370, 11, 370, 11, 370, 300, 311, 264, 700, 644, 466, 14170, 13, 440, 1150, 644, 307, 466, 7302, 2310, 13, 51432], "temperature": 0.0, "avg_logprob": -0.1438240275663488, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0034831042867153883}, {"id": 244, "seek": 176072, "start": 1782.08, "end": 1788.72, "text": " You might already have some sense because we are able to visualize the network as a diagram. So,", "tokens": [51432, 509, 1062, 1217, 362, 512, 2020, 570, 321, 366, 1075, 281, 23273, 264, 3209, 382, 257, 10686, 13, 407, 11, 51764], "temperature": 0.0, "avg_logprob": -0.1438240275663488, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0034831042867153883}, {"id": 245, "seek": 178872, "start": 1788.88, "end": 1793.6000000000001, "text": " the hope is that you can just stare at a diagram and gain some insight of what's happening inside", "tokens": [50372, 264, 1454, 307, 300, 291, 393, 445, 22432, 412, 257, 10686, 293, 6052, 512, 11269, 295, 437, 311, 2737, 1854, 50608], "temperature": 0.0, "avg_logprob": -0.16966017923857035, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0011157505214214325}, {"id": 246, "seek": 178872, "start": 1793.6000000000001, "end": 1805.04, "text": " the neural network. Yes, yeah, so here we have some, some, some toy examples of, yeah, for example,", "tokens": [50608, 264, 18161, 3209, 13, 1079, 11, 1338, 11, 370, 510, 321, 362, 512, 11, 512, 11, 512, 12058, 5110, 295, 11, 1338, 11, 337, 1365, 11, 51180], "temperature": 0.0, "avg_logprob": -0.16966017923857035, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0011157505214214325}, {"id": 247, "seek": 178872, "start": 1805.04, "end": 1812.96, "text": " how do cans do the multiplication computation. So we have x and y as the input and the x times y", "tokens": [51180, 577, 360, 21835, 360, 264, 27290, 24903, 13, 407, 321, 362, 2031, 293, 288, 382, 264, 4846, 293, 264, 2031, 1413, 288, 51576], "temperature": 0.0, "avg_logprob": -0.16966017923857035, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0011157505214214325}, {"id": 248, "seek": 181296, "start": 1812.96, "end": 1819.28, "text": " as the output. So when we train the can network, we also have some, something similar to L1", "tokens": [50364, 382, 264, 5598, 13, 407, 562, 321, 3847, 264, 393, 3209, 11, 321, 611, 362, 512, 11, 746, 2531, 281, 441, 16, 50680], "temperature": 0.0, "avg_logprob": -0.10802228400047789, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.006192124914377928}, {"id": 249, "seek": 181296, "start": 1819.28, "end": 1825.28, "text": " regularization to sparsify the network. So we can extract out the minimal network to do the task.", "tokens": [50680, 3890, 2144, 281, 637, 685, 2505, 264, 3209, 13, 407, 321, 393, 8947, 484, 264, 13206, 3209, 281, 360, 264, 5633, 13, 50980], "temperature": 0.0, "avg_logprob": -0.10802228400047789, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.006192124914377928}, {"id": 250, "seek": 181296, "start": 1826.56, "end": 1831.52, "text": " So in the multiplication case, we see that only two neurons are active in the end,", "tokens": [51044, 407, 294, 264, 27290, 1389, 11, 321, 536, 300, 787, 732, 22027, 366, 4967, 294, 264, 917, 11, 51292], "temperature": 0.0, "avg_logprob": -0.10802228400047789, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.006192124914377928}, {"id": 251, "seek": 181296, "start": 1831.52, "end": 1837.8400000000001, "text": " and we can read off the symbolic formulas of how, and get a sense of how it does the computation.", "tokens": [51292, 293, 321, 393, 1401, 766, 264, 25755, 30546, 295, 577, 11, 293, 483, 257, 2020, 295, 577, 309, 775, 264, 24903, 13, 51608], "temperature": 0.0, "avg_logprob": -0.10802228400047789, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.006192124914377928}, {"id": 252, "seek": 183784, "start": 1837.84, "end": 1847.28, "text": " Well, I marked the symbolic formulas here. It may take a while. It may take a while, but I,", "tokens": [50364, 1042, 11, 286, 12658, 264, 25755, 30546, 510, 13, 467, 815, 747, 257, 1339, 13, 467, 815, 747, 257, 1339, 11, 457, 286, 11, 50836], "temperature": 0.0, "avg_logprob": -0.21583833267439656, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0013248749310150743}, {"id": 253, "seek": 183784, "start": 1847.28, "end": 1854.9599999999998, "text": " well, the point is that it basically just some squared functions and, and the way the can network", "tokens": [50836, 731, 11, 264, 935, 307, 300, 309, 1936, 445, 512, 8889, 6828, 293, 11, 293, 264, 636, 264, 393, 3209, 51220], "temperature": 0.0, "avg_logprob": -0.21583833267439656, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0013248749310150743}, {"id": 254, "seek": 183784, "start": 1854.9599999999998, "end": 1862.72, "text": " learns to compute this multiplication is by leveraging some, some squared equality here.", "tokens": [51220, 27152, 281, 14722, 341, 27290, 307, 538, 32666, 512, 11, 512, 8889, 14949, 510, 13, 51608], "temperature": 0.0, "avg_logprob": -0.21583833267439656, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0013248749310150743}, {"id": 255, "seek": 186272, "start": 1863.68, "end": 1869.44, "text": " And the second example, the can is tasked with the division task,", "tokens": [50412, 400, 264, 1150, 1365, 11, 264, 393, 307, 38621, 365, 264, 10044, 5633, 11, 50700], "temperature": 0.0, "avg_logprob": -0.1586831562102787, "compression_ratio": 1.5947712418300655, "no_speech_prob": 0.0006666413391940296}, {"id": 256, "seek": 186272, "start": 1870.4, "end": 1881.3600000000001, "text": " where we input two positive numbers, x and y, and can is asked to predict the x divided by y.", "tokens": [50748, 689, 321, 4846, 732, 3353, 3547, 11, 2031, 293, 288, 11, 293, 393, 307, 2351, 281, 6069, 264, 2031, 6666, 538, 288, 13, 51296], "temperature": 0.0, "avg_logprob": -0.1586831562102787, "compression_ratio": 1.5947712418300655, "no_speech_prob": 0.0006666413391940296}, {"id": 257, "seek": 186272, "start": 1881.3600000000001, "end": 1887.84, "text": " And because here x and y are both positive, the can network learns to first take the", "tokens": [51296, 400, 570, 510, 2031, 293, 288, 366, 1293, 3353, 11, 264, 393, 3209, 27152, 281, 700, 747, 264, 51620], "temperature": 0.0, "avg_logprob": -0.1586831562102787, "compression_ratio": 1.5947712418300655, "no_speech_prob": 0.0006666413391940296}, {"id": 258, "seek": 188784, "start": 1887.84, "end": 1892.9599999999998, "text": " logarithm transformation and then take the subtracted two logarithm and then", "tokens": [50364, 41473, 32674, 9887, 293, 550, 747, 264, 16390, 292, 732, 41473, 32674, 293, 550, 50620], "temperature": 0.0, "avg_logprob": -0.20958797041192112, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.001366861048154533}, {"id": 259, "seek": 188784, "start": 1893.9199999999998, "end": 1901.04, "text": " transform the same back via the exponential, exponentiation. So, so, so that's really cute.", "tokens": [50668, 4088, 264, 912, 646, 5766, 264, 21510, 11, 37871, 6642, 13, 407, 11, 370, 11, 370, 300, 311, 534, 4052, 13, 51024], "temperature": 0.0, "avg_logprob": -0.20958797041192112, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.001366861048154533}, {"id": 260, "seek": 188784, "start": 1901.04, "end": 1907.12, "text": " Like, like one example of the commograph theorem is that you can basically do the multiplication", "tokens": [51024, 1743, 11, 411, 472, 1365, 295, 264, 800, 3108, 20904, 307, 300, 291, 393, 1936, 360, 264, 27290, 51328], "temperature": 0.0, "avg_logprob": -0.20958797041192112, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.001366861048154533}, {"id": 261, "seek": 188784, "start": 1907.12, "end": 1913.36, "text": " of positive numbers or the division of positive numbers in the logarithmic scale, because that's,", "tokens": [51328, 295, 3353, 3547, 420, 264, 10044, 295, 3353, 3547, 294, 264, 41473, 355, 13195, 4373, 11, 570, 300, 311, 11, 51640], "temperature": 0.0, "avg_logprob": -0.20958797041192112, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.001366861048154533}, {"id": 262, "seek": 191336, "start": 1913.36, "end": 1919.28, "text": " because the division or multiplication in the logarithmic scale, it transferred to,", "tokens": [50364, 570, 264, 10044, 420, 27290, 294, 264, 41473, 355, 13195, 4373, 11, 309, 15809, 281, 11, 50660], "temperature": 0.0, "avg_logprob": -0.18523971424546354, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.00018521536549087614}, {"id": 263, "seek": 191336, "start": 1919.28, "end": 1924.56, "text": " would translate to like addition and subtraction in, in the logarithmic scale.", "tokens": [50660, 576, 13799, 281, 411, 4500, 293, 16390, 313, 294, 11, 294, 264, 41473, 355, 13195, 4373, 13, 50924], "temperature": 0.0, "avg_logprob": -0.18523971424546354, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.00018521536549087614}, {"id": 264, "seek": 191336, "start": 1928.24, "end": 1933.28, "text": " Yeah, so, so, so, so this examples are really simple. You might be wondering what about a more", "tokens": [51108, 865, 11, 370, 11, 370, 11, 370, 11, 370, 341, 5110, 366, 534, 2199, 13, 509, 1062, 312, 6359, 437, 466, 257, 544, 51360], "temperature": 0.0, "avg_logprob": -0.18523971424546354, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.00018521536549087614}, {"id": 265, "seek": 191336, "start": 1933.28, "end": 1940.4799999999998, "text": " complicated formula, then, then it might be very complicated to decode what the cans have learned.", "tokens": [51360, 6179, 8513, 11, 550, 11, 550, 309, 1062, 312, 588, 6179, 281, 979, 1429, 437, 264, 21835, 362, 3264, 13, 51720], "temperature": 0.0, "avg_logprob": -0.18523971424546354, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.00018521536549087614}, {"id": 266, "seek": 194048, "start": 1941.44, "end": 1946.88, "text": " So, I would want to argue that this might be a feature, not a bug, or you can call it a bug,", "tokens": [50412, 407, 11, 286, 576, 528, 281, 9695, 300, 341, 1062, 312, 257, 4111, 11, 406, 257, 7426, 11, 420, 291, 393, 818, 309, 257, 7426, 11, 50684], "temperature": 0.0, "avg_logprob": -0.15636830427208726, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.004069859161973}, {"id": 267, "seek": 194048, "start": 1946.88, "end": 1950.4, "text": " but I can, but I won't call it a feature in the sense that,", "tokens": [50684, 457, 286, 393, 11, 457, 286, 1582, 380, 818, 309, 257, 4111, 294, 264, 2020, 300, 11, 50860], "temperature": 0.0, "avg_logprob": -0.15636830427208726, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.004069859161973}, {"id": 268, "seek": 194048, "start": 1952.72, "end": 1956.64, "text": " sorry, I didn't show the network here, but let's suppose we're doing this formula here,", "tokens": [50976, 2597, 11, 286, 994, 380, 855, 264, 3209, 510, 11, 457, 718, 311, 7297, 321, 434, 884, 341, 8513, 510, 11, 51172], "temperature": 0.0, "avg_logprob": -0.15636830427208726, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.004069859161973}, {"id": 269, "seek": 194048, "start": 1956.64, "end": 1962.88, "text": " like u plus v divided by 1 plus uv. So, if you're familiar with relativity, you are special relativity", "tokens": [51172, 411, 344, 1804, 371, 6666, 538, 502, 1804, 344, 85, 13, 407, 11, 498, 291, 434, 4963, 365, 45675, 11, 291, 366, 2121, 45675, 51484], "temperature": 0.0, "avg_logprob": -0.15636830427208726, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.004069859161973}, {"id": 270, "seek": 196288, "start": 1962.96, "end": 1968.3200000000002, "text": " on, you are, you are, you are, you are realized that this is the velocity, relativistic velocity", "tokens": [50368, 322, 11, 291, 366, 11, 291, 366, 11, 291, 366, 11, 291, 366, 5334, 300, 341, 307, 264, 9269, 11, 21960, 3142, 9269, 50636], "temperature": 0.0, "avg_logprob": -0.19122454994603208, "compression_ratio": 1.898477157360406, "no_speech_prob": 0.01495303213596344}, {"id": 271, "seek": 196288, "start": 1968.3200000000002, "end": 1976.88, "text": " addition. So, at first, I thought that I need the five-layer can to fix this function, because", "tokens": [50636, 4500, 13, 407, 11, 412, 700, 11, 286, 1194, 300, 286, 643, 264, 1732, 12, 8376, 260, 393, 281, 3191, 341, 2445, 11, 570, 51064], "temperature": 0.0, "avg_logprob": -0.19122454994603208, "compression_ratio": 1.898477157360406, "no_speech_prob": 0.01495303213596344}, {"id": 272, "seek": 196288, "start": 1976.88, "end": 1982.88, "text": " you would need multiplication, which would give, which would consume two layers, you would,", "tokens": [51064, 291, 576, 643, 27290, 11, 597, 576, 976, 11, 597, 576, 14732, 732, 7914, 11, 291, 576, 11, 51364], "temperature": 0.0, "avg_logprob": -0.19122454994603208, "compression_ratio": 1.898477157360406, "no_speech_prob": 0.01495303213596344}, {"id": 273, "seek": 196288, "start": 1983.92, "end": 1988.88, "text": " two additions, consume another two layers, and also you, you, you would need the division.", "tokens": [51416, 732, 35113, 11, 14732, 1071, 732, 7914, 11, 293, 611, 291, 11, 291, 11, 291, 576, 643, 264, 10044, 13, 51664], "temperature": 0.0, "avg_logprob": -0.19122454994603208, "compression_ratio": 1.898477157360406, "no_speech_prob": 0.01495303213596344}, {"id": 274, "seek": 198888, "start": 1988.88, "end": 1994.5600000000002, "text": " So, so that's in total five layers, but it turned out you can only, you can, you can just use a two-layer", "tokens": [50364, 407, 11, 370, 300, 311, 294, 3217, 1732, 7914, 11, 457, 309, 3574, 484, 291, 393, 787, 11, 291, 393, 11, 291, 393, 445, 764, 257, 732, 12, 8376, 260, 50648], "temperature": 0.0, "avg_logprob": -0.12408369490243856, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0007095990004017949}, {"id": 275, "seek": 198888, "start": 1994.5600000000002, "end": 2000.16, "text": " can to fix this function perfectly well. And in the end, I realized that this is just,", "tokens": [50648, 393, 281, 3191, 341, 2445, 6239, 731, 13, 400, 294, 264, 917, 11, 286, 5334, 300, 341, 307, 445, 11, 50928], "temperature": 0.0, "avg_logprob": -0.12408369490243856, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0007095990004017949}, {"id": 276, "seek": 198888, "start": 2002.0800000000002, "end": 2008.0800000000002, "text": " this is just a rapidity trick known in special relativity, where you first do the arc-tange", "tokens": [51024, 341, 307, 445, 257, 7558, 507, 4282, 2570, 294, 2121, 45675, 11, 689, 291, 700, 360, 264, 10346, 12, 83, 933, 51324], "temperature": 0.0, "avg_logprob": -0.12408369490243856, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0007095990004017949}, {"id": 277, "seek": 198888, "start": 2008.0800000000002, "end": 2014.0800000000002, "text": " transformation to u and v separately, sum the thing to, to rapidity up, and then you do the", "tokens": [51324, 9887, 281, 344, 293, 371, 14759, 11, 2408, 264, 551, 281, 11, 281, 7558, 507, 493, 11, 293, 550, 291, 360, 264, 51624], "temperature": 0.0, "avg_logprob": -0.12408369490243856, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0007095990004017949}, {"id": 278, "seek": 201408, "start": 2014.08, "end": 2020.6399999999999, "text": " tange transformation back to, to get this formula. So, in this sense, it's rediscovering the, it's,", "tokens": [50364, 256, 933, 9887, 646, 281, 11, 281, 483, 341, 8513, 13, 407, 11, 294, 341, 2020, 11, 309, 311, 2182, 40080, 278, 264, 11, 309, 311, 11, 50692], "temperature": 0.0, "avg_logprob": -0.1274703840414683, "compression_ratio": 1.7075471698113207, "no_speech_prob": 0.006486544851213694}, {"id": 279, "seek": 201408, "start": 2020.6399999999999, "end": 2029.6, "text": " it's rediscovering the rapidity trick, known, well-known in, well-known in the special relativity.", "tokens": [50692, 309, 311, 2182, 40080, 278, 264, 7558, 507, 4282, 11, 2570, 11, 731, 12, 6861, 294, 11, 731, 12, 6861, 294, 264, 2121, 45675, 13, 51140], "temperature": 0.0, "avg_logprob": -0.1274703840414683, "compression_ratio": 1.7075471698113207, "no_speech_prob": 0.006486544851213694}, {"id": 280, "seek": 201408, "start": 2029.6, "end": 2035.6, "text": " And in some cases, I indeed find that the network finds some more compressed,", "tokens": [51140, 400, 294, 512, 3331, 11, 286, 6451, 915, 300, 264, 3209, 10704, 512, 544, 30353, 11, 51440], "temperature": 0.0, "avg_logprob": -0.1274703840414683, "compression_ratio": 1.7075471698113207, "no_speech_prob": 0.006486544851213694}, {"id": 281, "seek": 201408, "start": 2035.6, "end": 2041.28, "text": " compact representation than I would expect. That's good news in the sense that's the,", "tokens": [51440, 14679, 10290, 813, 286, 576, 2066, 13, 663, 311, 665, 2583, 294, 264, 2020, 300, 311, 264, 11, 51724], "temperature": 0.0, "avg_logprob": -0.1274703840414683, "compression_ratio": 1.7075471698113207, "no_speech_prob": 0.006486544851213694}, {"id": 282, "seek": 204128, "start": 2041.76, "end": 2048.08, "text": " the, in the sense that the network is discovering something more compact. So, the representation", "tokens": [50388, 264, 11, 294, 264, 2020, 300, 264, 3209, 307, 24773, 746, 544, 14679, 13, 407, 11, 264, 10290, 50704], "temperature": 0.0, "avg_logprob": -0.13374757766723633, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.00029594081570394337}, {"id": 283, "seek": 204128, "start": 2048.08, "end": 2053.84, "text": " is more powerful than I have expected. But the bad news is that sometimes the, the interpretation", "tokens": [50704, 307, 544, 4005, 813, 286, 362, 5176, 13, 583, 264, 1578, 2583, 307, 300, 2171, 264, 11, 264, 14174, 50992], "temperature": 0.0, "avg_logprob": -0.13374757766723633, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.00029594081570394337}, {"id": 284, "seek": 204128, "start": 2053.84, "end": 2062.08, "text": " can be subtle, can be a bit more complicated. But, but I mean, it's, it can be a, it can be a", "tokens": [50992, 393, 312, 13743, 11, 393, 312, 257, 857, 544, 6179, 13, 583, 11, 457, 286, 914, 11, 309, 311, 11, 309, 393, 312, 257, 11, 309, 393, 312, 257, 51404], "temperature": 0.0, "avg_logprob": -0.13374757766723633, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.00029594081570394337}, {"id": 285, "seek": 206208, "start": 2062.08, "end": 2071.52, "text": " feature, but depending on your view. Yeah, so, so, so, so this is one paragraph I take from", "tokens": [50364, 4111, 11, 457, 5413, 322, 428, 1910, 13, 865, 11, 370, 11, 370, 11, 370, 11, 370, 341, 307, 472, 18865, 286, 747, 490, 50836], "temperature": 0.0, "avg_logprob": -0.1621500740588551, "compression_ratio": 1.5739644970414202, "no_speech_prob": 0.07052627205848694}, {"id": 286, "seek": 206208, "start": 2072.24, "end": 2078.3199999999997, "text": " the paper. This is criticized, this has been criticized a lot in the social media, but I,", "tokens": [50872, 264, 3035, 13, 639, 307, 28011, 11, 341, 575, 668, 28011, 257, 688, 294, 264, 2093, 3021, 11, 457, 286, 11, 51176], "temperature": 0.0, "avg_logprob": -0.1621500740588551, "compression_ratio": 1.5739644970414202, "no_speech_prob": 0.07052627205848694}, {"id": 287, "seek": 206208, "start": 2079.12, "end": 2086.88, "text": " but I find this analogy really interesting. So, I still want to highlight this part.", "tokens": [51216, 457, 286, 915, 341, 21663, 534, 1880, 13, 407, 11, 286, 920, 528, 281, 5078, 341, 644, 13, 51604], "temperature": 0.0, "avg_logprob": -0.1621500740588551, "compression_ratio": 1.5739644970414202, "no_speech_prob": 0.07052627205848694}, {"id": 288, "seek": 208688, "start": 2087.84, "end": 2093.76, "text": " So, so to me, I think can is like a, it's sort of like language model or, or like language or", "tokens": [50412, 407, 11, 370, 281, 385, 11, 286, 519, 393, 307, 411, 257, 11, 309, 311, 1333, 295, 411, 2856, 2316, 420, 11, 420, 411, 2856, 420, 50708], "temperature": 0.0, "avg_logprob": -0.15141316347343978, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0075754811987280846}, {"id": 289, "seek": 208688, "start": 2093.76, "end": 2099.6800000000003, "text": " even like language for AI plus science. The reason why language models are so transformative and", "tokens": [50708, 754, 411, 2856, 337, 7318, 1804, 3497, 13, 440, 1778, 983, 2856, 5245, 366, 370, 36070, 293, 51004], "temperature": 0.0, "avg_logprob": -0.15141316347343978, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0075754811987280846}, {"id": 290, "seek": 208688, "start": 2099.6800000000003, "end": 2105.2000000000003, "text": " powerful is because they are useful to anyone who can speak natural language. But the, but the", "tokens": [51004, 4005, 307, 570, 436, 366, 4420, 281, 2878, 567, 393, 1710, 3303, 2856, 13, 583, 264, 11, 457, 264, 51280], "temperature": 0.0, "avg_logprob": -0.15141316347343978, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0075754811987280846}, {"id": 291, "seek": 208688, "start": 2105.2000000000003, "end": 2112.7200000000003, "text": " language of science is functions or more advanced mathematical objects build on functions because", "tokens": [51280, 2856, 295, 3497, 307, 6828, 420, 544, 7339, 18894, 6565, 1322, 322, 6828, 570, 51656], "temperature": 0.0, "avg_logprob": -0.15141316347343978, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0075754811987280846}, {"id": 292, "seek": 211272, "start": 2112.7999999999997, "end": 2120.0, "text": " cans are composed of functions which are 1D, so they are interpretable. So, when a human user stares", "tokens": [50368, 21835, 366, 18204, 295, 6828, 597, 366, 502, 35, 11, 370, 436, 366, 7302, 712, 13, 407, 11, 562, 257, 1952, 4195, 342, 8643, 50728], "temperature": 0.0, "avg_logprob": -0.1290460185000771, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0057290904223918915}, {"id": 293, "seek": 211272, "start": 2120.0, "end": 2126.72, "text": " at a can, it's like communicating it with, it's like communicating it with the, using the language", "tokens": [50728, 412, 257, 393, 11, 309, 311, 411, 17559, 309, 365, 11, 309, 311, 411, 17559, 309, 365, 264, 11, 1228, 264, 2856, 51064], "temperature": 0.0, "avg_logprob": -0.1290460185000771, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0057290904223918915}, {"id": 294, "seek": 211272, "start": 2126.72, "end": 2133.68, "text": " of functions. So, to elaborate more and to make it more entertaining, let's suppose we are like", "tokens": [51064, 295, 6828, 13, 407, 11, 281, 20945, 544, 293, 281, 652, 309, 544, 20402, 11, 718, 311, 7297, 321, 366, 411, 51412], "temperature": 0.0, "avg_logprob": -0.1290460185000771, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0057290904223918915}, {"id": 295, "seek": 211272, "start": 2133.68, "end": 2140.08, "text": " my advisor Max is communicating with the can network. Here, I picture the can network as", "tokens": [51412, 452, 19161, 7402, 307, 17559, 365, 264, 393, 3209, 13, 1692, 11, 286, 3036, 264, 393, 3209, 382, 51732], "temperature": 0.0, "avg_logprob": -0.1290460185000771, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.0057290904223918915}, {"id": 296, "seek": 214008, "start": 2140.16, "end": 2146.08, "text": " a trisolarian from the three-body problem. I'm not sure if guys have watched it, but basically", "tokens": [50368, 257, 504, 271, 15276, 952, 490, 264, 1045, 12, 1067, 1154, 13, 286, 478, 406, 988, 498, 1074, 362, 6337, 309, 11, 457, 1936, 50664], "temperature": 0.0, "avg_logprob": -0.16179677767631334, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.017436515539884567}, {"id": 297, "seek": 214008, "start": 2146.08, "end": 2153.36, "text": " the trisolarians, their brains are transparent, so they cannot hide any secrets from others. So,", "tokens": [50664, 264, 504, 271, 15276, 2567, 11, 641, 15442, 366, 12737, 11, 370, 436, 2644, 6479, 604, 14093, 490, 2357, 13, 407, 11, 51028], "temperature": 0.0, "avg_logprob": -0.16179677767631334, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.017436515539884567}, {"id": 298, "seek": 214008, "start": 2153.36, "end": 2163.6, "text": " they are totally transparent. So, so Max went up, give, give the can, give the can a dataset.", "tokens": [51028, 436, 366, 3879, 12737, 13, 407, 11, 370, 7402, 1437, 493, 11, 976, 11, 976, 264, 393, 11, 976, 264, 393, 257, 28872, 13, 51540], "temperature": 0.0, "avg_logprob": -0.16179677767631334, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.017436515539884567}, {"id": 299, "seek": 216360, "start": 2163.7599999999998, "end": 2170.0, "text": " Max was like, here is my dataset. It contains the mystery of the universe. I want you,", "tokens": [50372, 7402, 390, 411, 11, 510, 307, 452, 28872, 13, 467, 8306, 264, 11422, 295, 264, 6445, 13, 286, 528, 291, 11, 50684], "temperature": 0.0, "avg_logprob": -0.18940885099646163, "compression_ratio": 1.6993865030674846, "no_speech_prob": 0.03357652947306633}, {"id": 300, "seek": 216360, "start": 2170.7999999999997, "end": 2178.48, "text": " the can network, to figure out, to figure out the structure of the datasets and, and the can", "tokens": [50724, 264, 393, 3209, 11, 281, 2573, 484, 11, 281, 2573, 484, 264, 3877, 295, 264, 42856, 293, 11, 293, 264, 393, 51108], "temperature": 0.0, "avg_logprob": -0.18940885099646163, "compression_ratio": 1.6993865030674846, "no_speech_prob": 0.03357652947306633}, {"id": 301, "seek": 216360, "start": 2179.36, "end": 2187.2, "text": " initialize the can network like this. So, here's the brain of the can network initially. And Max,", "tokens": [51152, 5883, 1125, 264, 393, 3209, 411, 341, 13, 407, 11, 510, 311, 264, 3567, 295, 264, 393, 3209, 9105, 13, 400, 7402, 11, 51544], "temperature": 0.0, "avg_logprob": -0.18940885099646163, "compression_ratio": 1.6993865030674846, "no_speech_prob": 0.03357652947306633}, {"id": 302, "seek": 218720, "start": 2187.4399999999996, "end": 2195.04, "text": " given the dataset, Max wants to train the can network, to train the brain. And after training,", "tokens": [50376, 2212, 264, 28872, 11, 7402, 2738, 281, 3847, 264, 393, 3209, 11, 281, 3847, 264, 3567, 13, 400, 934, 3097, 11, 50756], "temperature": 0.0, "avg_logprob": -0.13690651456514993, "compression_ratio": 1.8357487922705313, "no_speech_prob": 0.0038840891793370247}, {"id": 303, "seek": 218720, "start": 2195.04, "end": 2201.2799999999997, "text": " you can get this sparse network, which you start to see some structure. But still, it's a bit,", "tokens": [50756, 291, 393, 483, 341, 637, 11668, 3209, 11, 597, 291, 722, 281, 536, 512, 3877, 13, 583, 920, 11, 309, 311, 257, 857, 11, 51068], "temperature": 0.0, "avg_logprob": -0.13690651456514993, "compression_ratio": 1.8357487922705313, "no_speech_prob": 0.0038840891793370247}, {"id": 304, "seek": 218720, "start": 2201.2799999999997, "end": 2208.3199999999997, "text": " there's still some residual connections, which looks quite, which quite annoying. So, Max asked the", "tokens": [51068, 456, 311, 920, 512, 27980, 9271, 11, 597, 1542, 1596, 11, 597, 1596, 11304, 13, 407, 11, 7402, 2351, 264, 51420], "temperature": 0.0, "avg_logprob": -0.13690651456514993, "compression_ratio": 1.8357487922705313, "no_speech_prob": 0.0038840891793370247}, {"id": 305, "seek": 218720, "start": 2208.3199999999997, "end": 2216.08, "text": " can network to prune the redundant connections. And after pruning, you got, after pruning,", "tokens": [51420, 393, 3209, 281, 582, 2613, 264, 40997, 9271, 13, 400, 934, 582, 37726, 11, 291, 658, 11, 934, 582, 37726, 11, 51808], "temperature": 0.0, "avg_logprob": -0.13690651456514993, "compression_ratio": 1.8357487922705313, "no_speech_prob": 0.0038840891793370247}, {"id": 306, "seek": 221608, "start": 2216.16, "end": 2225.92, "text": " you got this, you got this sub network, which is responsible for the computation. And Max further", "tokens": [50368, 291, 658, 341, 11, 291, 658, 341, 1422, 3209, 11, 597, 307, 6250, 337, 264, 24903, 13, 400, 7402, 3052, 50856], "temperature": 0.0, "avg_logprob": -0.11799844761484678, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.002844518283382058}, {"id": 307, "seek": 221608, "start": 2225.92, "end": 2231.6, "text": " asked the network to the can network to symbolify it, because it looks like, it looks like in the", "tokens": [50856, 2351, 264, 3209, 281, 264, 393, 3209, 281, 5986, 2505, 309, 11, 570, 309, 1542, 411, 11, 309, 1542, 411, 294, 264, 51140], "temperature": 0.0, "avg_logprob": -0.11799844761484678, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.002844518283382058}, {"id": 308, "seek": 221608, "start": 2231.6, "end": 2237.2, "text": " bottom left, it looks like just a sign function. And in the bottom right, this looks just like a", "tokens": [51140, 2767, 1411, 11, 309, 1542, 411, 445, 257, 1465, 2445, 13, 400, 294, 264, 2767, 558, 11, 341, 1542, 445, 411, 257, 51420], "temperature": 0.0, "avg_logprob": -0.11799844761484678, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.002844518283382058}, {"id": 309, "seek": 221608, "start": 2237.2, "end": 2242.7999999999997, "text": " parabola. And this just looks like an exponential. So, maybe you can symbolify it to gain some more", "tokens": [51420, 45729, 4711, 13, 400, 341, 445, 1542, 411, 364, 21510, 13, 407, 11, 1310, 291, 393, 5986, 2505, 309, 281, 6052, 512, 544, 51700], "temperature": 0.0, "avg_logprob": -0.11799844761484678, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.002844518283382058}, {"id": 310, "seek": 224280, "start": 2242.8, "end": 2253.1200000000003, "text": " insight. So, yes, so the can network said, yes, I can symbolify it. And you can. And now the dataset", "tokens": [50364, 11269, 13, 407, 11, 2086, 11, 370, 264, 393, 3209, 848, 11, 2086, 11, 286, 393, 5986, 2505, 309, 13, 400, 291, 393, 13, 400, 586, 264, 28872, 50880], "temperature": 0.0, "avg_logprob": -0.1752511699025224, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0009109916863963008}, {"id": 311, "seek": 224280, "start": 2254.2400000000002, "end": 2261.6800000000003, "text": " goes all the way down to the symbolic formula. But, but we can imagine another conversation Max", "tokens": [50936, 1709, 439, 264, 636, 760, 281, 264, 25755, 8513, 13, 583, 11, 457, 321, 393, 3811, 1071, 3761, 7402, 51308], "temperature": 0.0, "avg_logprob": -0.1752511699025224, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0009109916863963008}, {"id": 312, "seek": 224280, "start": 2261.6800000000003, "end": 2267.92, "text": " would have with an MLP. So, Max went up and give the dataset to MLP, and want the MLP to figure", "tokens": [51308, 576, 362, 365, 364, 21601, 47, 13, 407, 11, 7402, 1437, 493, 293, 976, 264, 28872, 281, 21601, 47, 11, 293, 528, 264, 21601, 47, 281, 2573, 51620], "temperature": 0.0, "avg_logprob": -0.1752511699025224, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0009109916863963008}, {"id": 313, "seek": 226792, "start": 2267.92, "end": 2277.84, "text": " out the symbolic formula in it. And like before, the MLP initialized the brain, looked like something", "tokens": [50364, 484, 264, 25755, 8513, 294, 309, 13, 400, 411, 949, 11, 264, 21601, 47, 5883, 1602, 264, 3567, 11, 2956, 411, 746, 50860], "temperature": 0.0, "avg_logprob": -0.1287875493367513, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.008060865104198456}, {"id": 314, "seek": 226792, "start": 2277.84, "end": 2284.4, "text": " like this, really messy. After training, Max asked MLP to train the brain, but even after", "tokens": [50860, 411, 341, 11, 534, 16191, 13, 2381, 3097, 11, 7402, 2351, 21601, 47, 281, 3847, 264, 3567, 11, 457, 754, 934, 51188], "temperature": 0.0, "avg_logprob": -0.1287875493367513, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.008060865104198456}, {"id": 315, "seek": 226792, "start": 2284.4, "end": 2293.28, "text": " training, the connection still looks really messy. And MLPs were like, and the MLP is like, I really", "tokens": [51188, 3097, 11, 264, 4984, 920, 1542, 534, 16191, 13, 400, 21601, 23043, 645, 411, 11, 293, 264, 21601, 47, 307, 411, 11, 286, 534, 51632], "temperature": 0.0, "avg_logprob": -0.1287875493367513, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.008060865104198456}, {"id": 316, "seek": 229328, "start": 2293.28, "end": 2298.88, "text": " trained it, but the loss is pretty low. But it's just that the connections are still very complicated.", "tokens": [50364, 8895, 309, 11, 457, 264, 4470, 307, 1238, 2295, 13, 583, 309, 311, 445, 300, 264, 9271, 366, 920, 588, 6179, 13, 50644], "temperature": 0.0, "avg_logprob": -0.1278440591060754, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.008575069718062878}, {"id": 317, "seek": 229328, "start": 2301.1200000000003, "end": 2309.36, "text": " Now Max got confused. Like, what's going on? What's going on with your brain? And now MLP is like,", "tokens": [50756, 823, 7402, 658, 9019, 13, 1743, 11, 437, 311, 516, 322, 30, 708, 311, 516, 322, 365, 428, 3567, 30, 400, 586, 21601, 47, 307, 411, 11, 51168], "temperature": 0.0, "avg_logprob": -0.1278440591060754, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.008575069718062878}, {"id": 318, "seek": 229328, "start": 2309.36, "end": 2316.0, "text": " it's just that your humans are too stupid to understand my computations. You cannot say,", "tokens": [51168, 309, 311, 445, 300, 428, 6255, 366, 886, 6631, 281, 1223, 452, 2807, 763, 13, 509, 2644, 584, 11, 51500], "temperature": 0.0, "avg_logprob": -0.1278440591060754, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.008575069718062878}, {"id": 319, "seek": 229328, "start": 2316.0, "end": 2322.0, "text": " I'm wrong, simply because you cannot understand me. So, Max now got really pissed off and turned", "tokens": [51500, 286, 478, 2085, 11, 2935, 570, 291, 2644, 1223, 385, 13, 407, 11, 7402, 586, 658, 534, 23795, 766, 293, 3574, 51800], "temperature": 0.0, "avg_logprob": -0.1278440591060754, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.008575069718062878}, {"id": 320, "seek": 232200, "start": 2322.0, "end": 2331.92, "text": " back to CANS. So, those are just some imaginary stories I made up with the symphatic example.", "tokens": [50364, 646, 281, 22931, 50, 13, 407, 11, 729, 366, 445, 512, 26164, 3676, 286, 1027, 493, 365, 264, 6697, 950, 2399, 1365, 13, 50860], "temperature": 0.0, "avg_logprob": -0.18809459686279298, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.004828655626624823}, {"id": 321, "seek": 232200, "start": 2331.92, "end": 2341.04, "text": " And that symphatic example is really simple. But I want to show that we can really use CANS", "tokens": [50860, 400, 300, 6697, 950, 2399, 1365, 307, 534, 2199, 13, 583, 286, 528, 281, 855, 300, 321, 393, 534, 764, 22931, 50, 51316], "temperature": 0.0, "avg_logprob": -0.18809459686279298, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.004828655626624823}, {"id": 322, "seek": 232200, "start": 2341.04, "end": 2347.44, "text": " as a collaborator in scientific research. And CANS can give us some non-trivial results.", "tokens": [51316, 382, 257, 5091, 1639, 294, 8134, 2132, 13, 400, 22931, 50, 393, 976, 505, 512, 2107, 12, 83, 470, 22640, 3542, 13, 51636], "temperature": 0.0, "avg_logprob": -0.18809459686279298, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.004828655626624823}, {"id": 323, "seek": 234744, "start": 2348.08, "end": 2355.28, "text": " CANS can give us some new discoveries. So, the first example is,", "tokens": [50396, 22931, 50, 393, 976, 505, 512, 777, 28400, 13, 407, 11, 264, 700, 1365, 307, 11, 50756], "temperature": 0.0, "avg_logprob": -0.24125722628920826, "compression_ratio": 1.447674418604651, "no_speech_prob": 0.004069555085152388}, {"id": 324, "seek": 234744, "start": 2357.84, "end": 2365.52, "text": " yeah, so this example was used in a DeepMind Nature paper three years ago, where they used MLP", "tokens": [50884, 1338, 11, 370, 341, 1365, 390, 1143, 294, 257, 14895, 44, 471, 20159, 3035, 1045, 924, 2057, 11, 689, 436, 1143, 21601, 47, 51268], "temperature": 0.0, "avg_logprob": -0.24125722628920826, "compression_ratio": 1.447674418604651, "no_speech_prob": 0.004069555085152388}, {"id": 325, "seek": 234744, "start": 2365.52, "end": 2375.2000000000003, "text": " to discover a relationship in a NOT dataset. So, each NOT has some invariance. Basically,", "tokens": [51268, 281, 4411, 257, 2480, 294, 257, 12854, 28872, 13, 407, 11, 1184, 12854, 575, 512, 33270, 719, 13, 8537, 11, 51752], "temperature": 0.0, "avg_logprob": -0.24125722628920826, "compression_ratio": 1.447674418604651, "no_speech_prob": 0.004069555085152388}, {"id": 326, "seek": 237520, "start": 2375.2, "end": 2379.8399999999997, "text": " each NOT is associated some numbers. And these numbers, they have some relations. And we want", "tokens": [50364, 1184, 12854, 307, 6615, 512, 3547, 13, 400, 613, 3547, 11, 436, 362, 512, 2299, 13, 400, 321, 528, 50596], "temperature": 0.0, "avg_logprob": -0.11973294344815341, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0008165939361788332}, {"id": 327, "seek": 237520, "start": 2379.8399999999997, "end": 2388.16, "text": " to dig out the relations among these variables. So, what the DeepMind people did was they used", "tokens": [50596, 281, 2528, 484, 264, 2299, 3654, 613, 9102, 13, 407, 11, 437, 264, 14895, 44, 471, 561, 630, 390, 436, 1143, 51012], "temperature": 0.0, "avg_logprob": -0.11973294344815341, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0008165939361788332}, {"id": 328, "seek": 237520, "start": 2388.16, "end": 2393.7599999999998, "text": " the train and MLP and used the attribution methods, basically take the gradient with respect to these", "tokens": [51012, 264, 3847, 293, 21601, 47, 293, 1143, 264, 9080, 1448, 7150, 11, 1936, 747, 264, 16235, 365, 3104, 281, 613, 51292], "temperature": 0.0, "avg_logprob": -0.11973294344815341, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0008165939361788332}, {"id": 329, "seek": 237520, "start": 2393.7599999999998, "end": 2400.96, "text": " input variables, and use that as a score to attribute these features. And then rank these", "tokens": [51292, 4846, 9102, 11, 293, 764, 300, 382, 257, 6175, 281, 19667, 613, 4122, 13, 400, 550, 6181, 613, 51652], "temperature": 0.0, "avg_logprob": -0.11973294344815341, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0008165939361788332}, {"id": 330, "seek": 240096, "start": 2400.96, "end": 2405.44, "text": " features to get a sense of which features are more important than other features. And they", "tokens": [50364, 4122, 281, 483, 257, 2020, 295, 597, 4122, 366, 544, 1021, 813, 661, 4122, 13, 400, 436, 50588], "temperature": 0.0, "avg_logprob": -0.10256342887878418, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.026346882805228233}, {"id": 331, "seek": 240096, "start": 2405.44, "end": 2414.48, "text": " identified three important features. That's the only thing that's automated in their framework.", "tokens": [50588, 9234, 1045, 1021, 4122, 13, 663, 311, 264, 787, 551, 300, 311, 18473, 294, 641, 8388, 13, 51040], "temperature": 0.0, "avg_logprob": -0.10256342887878418, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.026346882805228233}, {"id": 332, "seek": 240096, "start": 2414.48, "end": 2420.8, "text": " And then the human scientist came in and tried to come up with a symbolic formula for it.", "tokens": [51040, 400, 550, 264, 1952, 12662, 1361, 294, 293, 3031, 281, 808, 493, 365, 257, 25755, 8513, 337, 309, 13, 51356], "temperature": 0.0, "avg_logprob": -0.10256342887878418, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.026346882805228233}, {"id": 333, "seek": 240096, "start": 2421.6, "end": 2426.16, "text": " So, we're asking this question, can we discover, have we discovered these results", "tokens": [51396, 407, 11, 321, 434, 3365, 341, 1168, 11, 393, 321, 4411, 11, 362, 321, 6941, 613, 3542, 51624], "temperature": 0.0, "avg_logprob": -0.10256342887878418, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.026346882805228233}, {"id": 334, "seek": 242616, "start": 2426.16, "end": 2432.16, "text": " with more automation, with less, you know, efforts, and probably even discovering something new", "tokens": [50364, 365, 544, 17769, 11, 365, 1570, 11, 291, 458, 11, 6484, 11, 293, 1391, 754, 24773, 746, 777, 50664], "temperature": 0.0, "avg_logprob": -0.1634388110216926, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0014324174262583256}, {"id": 335, "seek": 242616, "start": 2432.16, "end": 2442.3199999999997, "text": " that the DeepMind paper were missing? So, first, we are able to discover the three important variables", "tokens": [50664, 300, 264, 14895, 44, 471, 3035, 645, 5361, 30, 407, 11, 700, 11, 321, 366, 1075, 281, 4411, 264, 1045, 1021, 9102, 51172], "temperature": 0.0, "avg_logprob": -0.1634388110216926, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0014324174262583256}, {"id": 336, "seek": 242616, "start": 2442.3199999999997, "end": 2449.92, "text": " with the CAN network, with much more intuition and automation. So, their network, they used a", "tokens": [51172, 365, 264, 22931, 3209, 11, 365, 709, 544, 24002, 293, 17769, 13, 407, 11, 641, 3209, 11, 436, 1143, 257, 51552], "temperature": 0.0, "avg_logprob": -0.1634388110216926, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0014324174262583256}, {"id": 337, "seek": 244992, "start": 2450.2400000000002, "end": 2456.48, "text": " three-layer, sorry, they used a five-layer MLP, and each hidden layer has 300 neurons. So,", "tokens": [50380, 1045, 12, 8376, 260, 11, 2597, 11, 436, 1143, 257, 1732, 12, 8376, 260, 21601, 47, 11, 293, 1184, 7633, 4583, 575, 6641, 22027, 13, 407, 11, 50692], "temperature": 0.0, "avg_logprob": -0.1549081978974519, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.01744065433740616}, {"id": 338, "seek": 244992, "start": 2456.48, "end": 2462.08, "text": " that's really hard to interpret. That's why they used the feature attribution. But we find that", "tokens": [50692, 300, 311, 534, 1152, 281, 7302, 13, 663, 311, 983, 436, 1143, 264, 4111, 9080, 1448, 13, 583, 321, 915, 300, 50972], "temperature": 0.0, "avg_logprob": -0.1549081978974519, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.01744065433740616}, {"id": 339, "seek": 244992, "start": 2462.08, "end": 2470.48, "text": " surprisingly, we only needed one hidden layer and one hidden neuron, the CAN network, to do the task,", "tokens": [50972, 17600, 11, 321, 787, 2978, 472, 7633, 4583, 293, 472, 7633, 34090, 11, 264, 22931, 3209, 11, 281, 360, 264, 5633, 11, 51392], "temperature": 0.0, "avg_logprob": -0.1549081978974519, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.01744065433740616}, {"id": 340, "seek": 244992, "start": 2470.48, "end": 2479.52, "text": " as well as their five-layer, like, a million-parameter MLPs. And with this, we can also clearly see", "tokens": [51392, 382, 731, 382, 641, 1732, 12, 8376, 260, 11, 411, 11, 257, 2459, 12, 2181, 335, 2398, 21601, 23043, 13, 400, 365, 341, 11, 321, 393, 611, 4448, 536, 51844], "temperature": 0.0, "avg_logprob": -0.1549081978974519, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.01744065433740616}, {"id": 341, "seek": 247992, "start": 2480.0, "end": 2488.96, "text": " the activations, the importance now basically becomes, you can basically understand the importance of", "tokens": [50368, 264, 2430, 763, 11, 264, 7379, 586, 1936, 3643, 11, 291, 393, 1936, 1223, 264, 7379, 295, 50816], "temperature": 0.0, "avg_logprob": -0.13435437602381553, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.002672324888408184}, {"id": 342, "seek": 247992, "start": 2488.96, "end": 2498.96, "text": " these variables with the L1 norm of these activation functions. So, that's also how we visualize", "tokens": [50816, 613, 9102, 365, 264, 441, 16, 2026, 295, 613, 24433, 6828, 13, 407, 11, 300, 311, 611, 577, 321, 23273, 51316], "temperature": 0.0, "avg_logprob": -0.13435437602381553, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.002672324888408184}, {"id": 343, "seek": 247992, "start": 2499.52, "end": 2504.96, "text": " the connections. So, you can basically read off from this diagram that the strong connections", "tokens": [51344, 264, 9271, 13, 407, 11, 291, 393, 1936, 1401, 766, 490, 341, 10686, 300, 264, 2068, 9271, 51616], "temperature": 0.0, "avg_logprob": -0.13435437602381553, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.002672324888408184}, {"id": 344, "seek": 250496, "start": 2504.96, "end": 2509.92, "text": " are the important variables, while the weaker or even nearly transparent", "tokens": [50364, 366, 264, 1021, 9102, 11, 1339, 264, 24286, 420, 754, 6217, 12737, 50612], "temperature": 0.0, "avg_logprob": -0.13470973688013413, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.0021153863053768873}, {"id": 345, "seek": 250496, "start": 2510.96, "end": 2513.6, "text": " connections, meaning that irrelevant variables.", "tokens": [50664, 9271, 11, 3620, 300, 28682, 9102, 13, 50796], "temperature": 0.0, "avg_logprob": -0.13470973688013413, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.0021153863053768873}, {"id": 346, "seek": 250496, "start": 2517.36, "end": 2524.32, "text": " Yeah, we can also discover symbolic formulas, as I said before, because the CAN network decomposes", "tokens": [50984, 865, 11, 321, 393, 611, 4411, 25755, 30546, 11, 382, 286, 848, 949, 11, 570, 264, 22931, 3209, 22867, 4201, 51332], "temperature": 0.0, "avg_logprob": -0.13470973688013413, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.0021153863053768873}, {"id": 347, "seek": 250496, "start": 2524.32, "end": 2529.76, "text": " high-dimensional functions into 1D, and then we can just do template matching in 1D", "tokens": [51332, 1090, 12, 18759, 6828, 666, 502, 35, 11, 293, 550, 321, 393, 445, 360, 12379, 14324, 294, 502, 35, 51604], "temperature": 0.0, "avg_logprob": -0.13470973688013413, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.0021153863053768873}, {"id": 348, "seek": 252976, "start": 2529.76, "end": 2537.6800000000003, "text": " to see what each 1D functions represent symbolic formulas, and then compose all these 1D functions", "tokens": [50364, 281, 536, 437, 1184, 502, 35, 6828, 2906, 25755, 30546, 11, 293, 550, 35925, 439, 613, 502, 35, 6828, 50760], "temperature": 0.0, "avg_logprob": -0.14550325315292567, "compression_ratio": 1.6298076923076923, "no_speech_prob": 0.0006877898122183979}, {"id": 349, "seek": 252976, "start": 2538.5600000000004, "end": 2540.6400000000003, "text": " back to get these high-dimensional functions.", "tokens": [50804, 646, 281, 483, 613, 1090, 12, 18759, 6828, 13, 50908], "temperature": 0.0, "avg_logprob": -0.14550325315292567, "compression_ratio": 1.6298076923076923, "no_speech_prob": 0.0006877898122183979}, {"id": 350, "seek": 252976, "start": 2543.76, "end": 2552.7200000000003, "text": " Something beyond their paper we discovered is that their setup is a supervised learning setup.", "tokens": [51064, 6595, 4399, 641, 3035, 321, 6941, 307, 300, 641, 8657, 307, 257, 46533, 2539, 8657, 13, 51512], "temperature": 0.0, "avg_logprob": -0.14550325315292567, "compression_ratio": 1.6298076923076923, "no_speech_prob": 0.0006877898122183979}, {"id": 351, "seek": 252976, "start": 2552.7200000000003, "end": 2559.1200000000003, "text": " Basically, they need to partition the variables into inputs and outputs, and they use the inputs to", "tokens": [51512, 8537, 11, 436, 643, 281, 24808, 264, 9102, 666, 15743, 293, 23930, 11, 293, 436, 764, 264, 15743, 281, 51832], "temperature": 0.0, "avg_logprob": -0.14550325315292567, "compression_ratio": 1.6298076923076923, "no_speech_prob": 0.0006877898122183979}, {"id": 352, "seek": 255912, "start": 2559.68, "end": 2565.68, "text": " predict at the outputs. But in some cases, we do not know how to partition the inputs and outputs,", "tokens": [50392, 6069, 412, 264, 23930, 13, 583, 294, 512, 3331, 11, 321, 360, 406, 458, 577, 281, 24808, 264, 15743, 293, 23930, 11, 50692], "temperature": 0.0, "avg_logprob": -0.13360842296055386, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0007096254266798496}, {"id": 353, "seek": 255912, "start": 2565.68, "end": 2577.04, "text": " like all the variables, they are treated equally. So, we want to develop this unsupervised setup", "tokens": [50692, 411, 439, 264, 9102, 11, 436, 366, 8668, 12309, 13, 407, 11, 321, 528, 281, 1499, 341, 2693, 12879, 24420, 8657, 51260], "temperature": 0.0, "avg_logprob": -0.13360842296055386, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0007096254266798496}, {"id": 354, "seek": 255912, "start": 2578.08, "end": 2585.92, "text": " where all the variables serve as inputs. But we use some notion of contrastive learning to classify", "tokens": [51312, 689, 439, 264, 9102, 4596, 382, 15743, 13, 583, 321, 764, 512, 10710, 295, 8712, 488, 2539, 281, 33872, 51704], "temperature": 0.0, "avg_logprob": -0.13360842296055386, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0007096254266798496}, {"id": 355, "seek": 258592, "start": 2585.92, "end": 2592.64, "text": " whether some given input is a real knot or a fake knot, or that might be too technical.", "tokens": [50364, 1968, 512, 2212, 4846, 307, 257, 957, 16966, 420, 257, 7592, 16966, 11, 420, 300, 1062, 312, 886, 6191, 13, 50700], "temperature": 0.0, "avg_logprob": -0.17690882135610111, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.0007792460382916033}, {"id": 356, "seek": 258592, "start": 2596.32, "end": 2603.6800000000003, "text": " The result is that we are able to discover more relations beyond the relation they've", "tokens": [50884, 440, 1874, 307, 300, 321, 366, 1075, 281, 4411, 544, 2299, 4399, 264, 9721, 436, 600, 51252], "temperature": 0.0, "avg_logprob": -0.17690882135610111, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.0007792460382916033}, {"id": 357, "seek": 258592, "start": 2603.6800000000003, "end": 2609.6800000000003, "text": " discovered, because they manually partition one variable as the output, so they can only discover", "tokens": [51252, 6941, 11, 570, 436, 16945, 24808, 472, 7006, 382, 264, 5598, 11, 370, 436, 393, 787, 4411, 51552], "temperature": 0.0, "avg_logprob": -0.17690882135610111, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.0007792460382916033}, {"id": 358, "seek": 260968, "start": 2610.64, "end": 2616.3199999999997, "text": " the relations that involve that variable. But here, we are learning it in an unsupervised way,", "tokens": [50412, 264, 2299, 300, 9494, 300, 7006, 13, 583, 510, 11, 321, 366, 2539, 309, 294, 364, 2693, 12879, 24420, 636, 11, 50696], "temperature": 0.0, "avg_logprob": -0.113758954845491, "compression_ratio": 1.75625, "no_speech_prob": 0.003323824843391776}, {"id": 359, "seek": 260968, "start": 2616.3199999999997, "end": 2623.04, "text": " so we can learn more than just one relation. We also discovered the relation between these", "tokens": [50696, 370, 321, 393, 1466, 544, 813, 445, 472, 9721, 13, 492, 611, 6941, 264, 9721, 1296, 613, 51032], "temperature": 0.0, "avg_logprob": -0.113758954845491, "compression_ratio": 1.75625, "no_speech_prob": 0.003323824843391776}, {"id": 360, "seek": 260968, "start": 2623.04, "end": 2633.7599999999998, "text": " three variables and between these two variables. Unfortunately, or fortunately, these relations", "tokens": [51032, 1045, 9102, 293, 1296, 613, 732, 9102, 13, 8590, 11, 420, 25511, 11, 613, 2299, 51568], "temperature": 0.0, "avg_logprob": -0.113758954845491, "compression_ratio": 1.75625, "no_speech_prob": 0.003323824843391776}, {"id": 361, "seek": 263376, "start": 2634.0800000000004, "end": 2640.7200000000003, "text": " are already known in the literature of knot theory. So, the unfortunate part is that we did not", "tokens": [50380, 366, 1217, 2570, 294, 264, 10394, 295, 16966, 5261, 13, 407, 11, 264, 17843, 644, 307, 300, 321, 630, 406, 50712], "temperature": 0.0, "avg_logprob": -0.13247933548487975, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.022278470918536186}, {"id": 362, "seek": 263376, "start": 2640.7200000000003, "end": 2649.0400000000004, "text": " discover anything new with our framework, but notice our network is just very preliminary,", "tokens": [50712, 4411, 1340, 777, 365, 527, 8388, 11, 457, 3449, 527, 3209, 307, 445, 588, 28817, 11, 51128], "temperature": 0.0, "avg_logprob": -0.13247933548487975, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.022278470918536186}, {"id": 363, "seek": 263376, "start": 2649.0400000000004, "end": 2654.7200000000003, "text": " it's just a one layer, it's just a one layer if we ignore about the classifier in the second layer.", "tokens": [51128, 309, 311, 445, 257, 472, 4583, 11, 309, 311, 445, 257, 472, 4583, 498, 321, 11200, 466, 264, 1508, 9902, 294, 264, 1150, 4583, 13, 51412], "temperature": 0.0, "avg_logprob": -0.13247933548487975, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.022278470918536186}, {"id": 364, "seek": 263376, "start": 2654.7200000000003, "end": 2661.28, "text": " So, hypothetically, we can make it deeper to get more complicated relations. But the", "tokens": [51412, 407, 11, 24371, 22652, 11, 321, 393, 652, 309, 7731, 281, 483, 544, 6179, 2299, 13, 583, 264, 51740], "temperature": 0.0, "avg_logprob": -0.13247933548487975, "compression_ratio": 1.6940639269406392, "no_speech_prob": 0.022278470918536186}, {"id": 365, "seek": 266128, "start": 2661.28, "end": 2667.0400000000004, "text": " fortunate part is that it verifies that's what we discovered with the network is something,", "tokens": [50364, 14096, 644, 307, 300, 309, 1306, 11221, 300, 311, 437, 321, 6941, 365, 264, 3209, 307, 746, 11, 50652], "temperature": 0.0, "avg_logprob": -0.23423651991219357, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0007672125939279795}, {"id": 366, "seek": 266128, "start": 2668.1600000000003, "end": 2672.8, "text": " is not bullshit, it's something that makes sense that people already know in the literature.", "tokens": [50708, 307, 406, 22676, 11, 309, 311, 746, 300, 1669, 2020, 300, 561, 1217, 458, 294, 264, 10394, 13, 50940], "temperature": 0.0, "avg_logprob": -0.23423651991219357, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0007672125939279795}, {"id": 367, "seek": 266128, "start": 2676.88, "end": 2682.96, "text": " Yeah, we also did this physics example, specifically Anderson localization.", "tokens": [51144, 865, 11, 321, 611, 630, 341, 10649, 1365, 11, 4682, 18768, 2654, 2144, 13, 51448], "temperature": 0.0, "avg_logprob": -0.23423651991219357, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0007672125939279795}, {"id": 368, "seek": 268296, "start": 2683.92, "end": 2692.32, "text": " I don't want to bore you with the technical detail, but again, the goal here is to try to figure out", "tokens": [50412, 286, 500, 380, 528, 281, 26002, 291, 365, 264, 6191, 2607, 11, 457, 797, 11, 264, 3387, 510, 307, 281, 853, 281, 2573, 484, 50832], "temperature": 0.0, "avg_logprob": -0.15541892120803613, "compression_ratio": 1.6201117318435754, "no_speech_prob": 0.04270542412996292}, {"id": 369, "seek": 268296, "start": 2693.04, "end": 2700.08, "text": " the symbolic formula of the phase transition boundary. In Anderson localization, we have the", "tokens": [50868, 264, 25755, 8513, 295, 264, 5574, 6034, 12866, 13, 682, 18768, 2654, 2144, 11, 321, 362, 264, 51220], "temperature": 0.0, "avg_logprob": -0.15541892120803613, "compression_ratio": 1.6201117318435754, "no_speech_prob": 0.04270542412996292}, {"id": 370, "seek": 268296, "start": 2700.08, "end": 2706.2400000000002, "text": " localization phase and the extended phase, and there is a phase boundary, and we want to extract", "tokens": [51220, 2654, 2144, 5574, 293, 264, 10913, 5574, 11, 293, 456, 307, 257, 5574, 12866, 11, 293, 321, 528, 281, 8947, 51528], "temperature": 0.0, "avg_logprob": -0.15541892120803613, "compression_ratio": 1.6201117318435754, "no_speech_prob": 0.04270542412996292}, {"id": 371, "seek": 270624, "start": 2706.24, "end": 2713.68, "text": " out the phase boundary, especially the symbolic formula of the phase boundary,", "tokens": [50364, 484, 264, 5574, 12866, 11, 2318, 264, 25755, 8513, 295, 264, 5574, 12866, 11, 50736], "temperature": 0.0, "avg_logprob": -0.18543548583984376, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.006192423868924379}, {"id": 372, "seek": 270624, "start": 2714.24, "end": 2719.04, "text": " if there exists a symbolic formula for it from the road data.", "tokens": [50764, 498, 456, 8198, 257, 25755, 8513, 337, 309, 490, 264, 3060, 1412, 13, 51004], "temperature": 0.0, "avg_logprob": -0.18543548583984376, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.006192423868924379}, {"id": 373, "seek": 270624, "start": 2722.8799999999997, "end": 2728.72, "text": " Yeah, so for this slide, I don't want to go to the detail, but the point I want to make with", "tokens": [51196, 865, 11, 370, 337, 341, 4137, 11, 286, 500, 380, 528, 281, 352, 281, 264, 2607, 11, 457, 264, 935, 286, 528, 281, 652, 365, 51488], "temperature": 0.0, "avg_logprob": -0.18543548583984376, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.006192423868924379}, {"id": 374, "seek": 272872, "start": 2728.72, "end": 2736.56, "text": " this slide is that the CAN network has, just like cars, you have manual mode, you have manual mode,", "tokens": [50364, 341, 4137, 307, 300, 264, 22931, 3209, 575, 11, 445, 411, 5163, 11, 291, 362, 9688, 4391, 11, 291, 362, 9688, 4391, 11, 50756], "temperature": 0.0, "avg_logprob": -0.1540142629564423, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.2017030417919159}, {"id": 375, "seek": 272872, "start": 2736.56, "end": 2741.8399999999997, "text": " you have automatic mode, like if you're lazy, you can just delegate everything to CANS, and CANS", "tokens": [50756, 291, 362, 12509, 4391, 11, 411, 498, 291, 434, 14847, 11, 291, 393, 445, 40999, 1203, 281, 22931, 50, 11, 293, 22931, 50, 51020], "temperature": 0.0, "avg_logprob": -0.1540142629564423, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.2017030417919159}, {"id": 376, "seek": 272872, "start": 2741.8399999999997, "end": 2748.3199999999997, "text": " will return you a symbolic formula fully automated, but that might not be correct. That might not", "tokens": [51020, 486, 2736, 291, 257, 25755, 8513, 4498, 18473, 11, 457, 300, 1062, 406, 312, 3006, 13, 663, 1062, 406, 51344], "temperature": 0.0, "avg_logprob": -0.1540142629564423, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.2017030417919159}, {"id": 377, "seek": 272872, "start": 2748.3199999999997, "end": 2755.52, "text": " be what you want. You can do that, it can give you reasonable accuracy, but may not be fully", "tokens": [51344, 312, 437, 291, 528, 13, 509, 393, 360, 300, 11, 309, 393, 976, 291, 10585, 14170, 11, 457, 815, 406, 312, 4498, 51704], "temperature": 0.0, "avg_logprob": -0.1540142629564423, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.2017030417919159}, {"id": 378, "seek": 275552, "start": 2755.52, "end": 2761.04, "text": " interpretable, but if you want to have some controllability over the CAN network,", "tokens": [50364, 7302, 712, 11, 457, 498, 291, 528, 281, 362, 512, 45159, 2310, 670, 264, 22931, 3209, 11, 50640], "temperature": 0.0, "avg_logprob": -0.13536689498207785, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.020960209891200066}, {"id": 379, "seek": 275552, "start": 2761.04, "end": 2764.88, "text": " where you want to be more involved, you want to have some intervention, you can still do that.", "tokens": [50640, 689, 291, 528, 281, 312, 544, 3288, 11, 291, 528, 281, 362, 512, 13176, 11, 291, 393, 920, 360, 300, 13, 50832], "temperature": 0.0, "avg_logprob": -0.13536689498207785, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.020960209891200066}, {"id": 380, "seek": 275552, "start": 2765.52, "end": 2772.48, "text": " You can choose to, you can use the manual mode, where you just handpicked some activation functions,", "tokens": [50864, 509, 393, 2826, 281, 11, 291, 393, 764, 264, 9688, 4391, 11, 689, 291, 445, 1011, 79, 12598, 512, 24433, 6828, 11, 51212], "temperature": 0.0, "avg_logprob": -0.13536689498207785, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.020960209891200066}, {"id": 381, "seek": 275552, "start": 2772.48, "end": 2778.48, "text": " like some functions, obviously they're just quadratic, linear, you can set them to be", "tokens": [51212, 411, 512, 6828, 11, 2745, 436, 434, 445, 37262, 11, 8213, 11, 291, 393, 992, 552, 281, 312, 51512], "temperature": 0.0, "avg_logprob": -0.13536689498207785, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.020960209891200066}, {"id": 382, "seek": 277848, "start": 2779.44, "end": 2786.32, "text": " exactly the linear or quadratic, and then you retrain the other activation functions,", "tokens": [50412, 2293, 264, 8213, 420, 37262, 11, 293, 550, 291, 1533, 7146, 264, 661, 24433, 6828, 11, 50756], "temperature": 0.0, "avg_logprob": -0.2209349787512491, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.02556087076663971}, {"id": 383, "seek": 277848, "start": 2786.32, "end": 2794.0, "text": " and you see, and after retraining, you will see what those activation functions would change to", "tokens": [50756, 293, 291, 536, 11, 293, 934, 49356, 1760, 11, 291, 486, 536, 437, 729, 24433, 6828, 576, 1319, 281, 51140], "temperature": 0.0, "avg_logprob": -0.2209349787512491, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.02556087076663971}, {"id": 384, "seek": 277848, "start": 2794.0, "end": 2800.0, "text": " different, would change to different form, and then this gives you, again, gives you some insight,", "tokens": [51140, 819, 11, 576, 1319, 281, 819, 1254, 11, 293, 550, 341, 2709, 291, 11, 797, 11, 2709, 291, 512, 11269, 11, 51440], "temperature": 0.0, "avg_logprob": -0.2209349787512491, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.02556087076663971}, {"id": 385, "seek": 277848, "start": 2801.2, "end": 2807.92, "text": " like give you better evidence what the next guess you would want to make, and this is like the", "tokens": [51500, 411, 976, 291, 1101, 4467, 437, 264, 958, 2041, 291, 576, 528, 281, 652, 11, 293, 341, 307, 411, 264, 51836], "temperature": 0.0, "avg_logprob": -0.2209349787512491, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.02556087076663971}, {"id": 386, "seek": 280792, "start": 2807.92, "end": 2814.32, "text": " iterative process, like it's sort of like you are arguing or you are debating with the CAN network,", "tokens": [50364, 17138, 1166, 1399, 11, 411, 309, 311, 1333, 295, 411, 291, 366, 19697, 420, 291, 366, 40647, 365, 264, 22931, 3209, 11, 50684], "temperature": 0.0, "avg_logprob": -0.12466167581492457, "compression_ratio": 1.903061224489796, "no_speech_prob": 0.002672781702131033}, {"id": 387, "seek": 280792, "start": 2814.32, "end": 2822.4, "text": " like the CAN network is give you some, or you can say debating or collaborating,", "tokens": [50684, 411, 264, 22931, 3209, 307, 976, 291, 512, 11, 420, 291, 393, 584, 40647, 420, 30188, 11, 51088], "temperature": 0.0, "avg_logprob": -0.12466167581492457, "compression_ratio": 1.903061224489796, "no_speech_prob": 0.002672781702131033}, {"id": 388, "seek": 280792, "start": 2822.4, "end": 2827.04, "text": " just like how we interact with a human collaborator, sometimes we debate, sometimes we collaborate,", "tokens": [51088, 445, 411, 577, 321, 4648, 365, 257, 1952, 5091, 1639, 11, 2171, 321, 7958, 11, 2171, 321, 18338, 11, 51320], "temperature": 0.0, "avg_logprob": -0.12466167581492457, "compression_ratio": 1.903061224489796, "no_speech_prob": 0.002672781702131033}, {"id": 389, "seek": 280792, "start": 2827.04, "end": 2834.48, "text": " but you seem to look at the same thing from different angles, like the CAN network is really", "tokens": [51320, 457, 291, 1643, 281, 574, 412, 264, 912, 551, 490, 819, 14708, 11, 411, 264, 22931, 3209, 307, 534, 51692], "temperature": 0.0, "avg_logprob": -0.12466167581492457, "compression_ratio": 1.903061224489796, "no_speech_prob": 0.002672781702131033}, {"id": 390, "seek": 283448, "start": 2834.48, "end": 2840.08, "text": " great at decomposing high dimensional functions into 1D functions, but those 1D functions may", "tokens": [50364, 869, 412, 22867, 6110, 1090, 18795, 6828, 666, 502, 35, 6828, 11, 457, 729, 502, 35, 6828, 815, 50644], "temperature": 0.0, "avg_logprob": -0.10374911813174977, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.00884607620537281}, {"id": 391, "seek": 283448, "start": 2840.08, "end": 2847.04, "text": " not be perfect, and there might be some actual subtlety, but humans are super great at identifying", "tokens": [50644, 406, 312, 2176, 11, 293, 456, 1062, 312, 512, 3539, 7257, 75, 2210, 11, 457, 6255, 366, 1687, 869, 412, 16696, 50992], "temperature": 0.0, "avg_logprob": -0.10374911813174977, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.00884607620537281}, {"id": 392, "seek": 283448, "start": 2847.04, "end": 2854.8, "text": " the symbolic stuff, and also recognizing the modular structure from the CAN diagram.", "tokens": [50992, 264, 25755, 1507, 11, 293, 611, 18538, 264, 31111, 3877, 490, 264, 22931, 10686, 13, 51380], "temperature": 0.0, "avg_logprob": -0.10374911813174977, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.00884607620537281}, {"id": 393, "seek": 283448, "start": 2855.6, "end": 2860.88, "text": " So, yeah, so the takeaway is that you can choose to be lazy, use the automatic mode,", "tokens": [51420, 407, 11, 1338, 11, 370, 264, 30681, 307, 300, 291, 393, 2826, 281, 312, 14847, 11, 764, 264, 12509, 4391, 11, 51684], "temperature": 0.0, "avg_logprob": -0.10374911813174977, "compression_ratio": 1.5807860262008733, "no_speech_prob": 0.00884607620537281}, {"id": 394, "seek": 286088, "start": 2860.88, "end": 2866.1600000000003, "text": " or you can choose to be more responsible and more involved using the manual mode.", "tokens": [50364, 420, 291, 393, 2826, 281, 312, 544, 6250, 293, 544, 3288, 1228, 264, 9688, 4391, 13, 50628], "temperature": 0.0, "avg_logprob": -0.1710350717817034, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.0025504438672214746}, {"id": 395, "seek": 286088, "start": 2869.52, "end": 2876.48, "text": " Yes, so maybe, yeah, maybe in the end, yeah, I will just finish this really quick,", "tokens": [50796, 1079, 11, 370, 1310, 11, 1338, 11, 1310, 294, 264, 917, 11, 1338, 11, 286, 486, 445, 2413, 341, 534, 1702, 11, 51144], "temperature": 0.0, "avg_logprob": -0.1710350717817034, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.0025504438672214746}, {"id": 396, "seek": 286088, "start": 2877.92, "end": 2886.08, "text": " so people have asked why it looks like CANs can, you know, in terms of expressive power, CANs are", "tokens": [51216, 370, 561, 362, 2351, 983, 309, 1542, 411, 22931, 82, 393, 11, 291, 458, 11, 294, 2115, 295, 40189, 1347, 11, 22931, 82, 366, 51624], "temperature": 0.0, "avg_logprob": -0.1710350717817034, "compression_ratio": 1.4636871508379887, "no_speech_prob": 0.0025504438672214746}, {"id": 397, "seek": 288608, "start": 2886.08, "end": 2893.52, "text": " just MLPs, are just like secretly just MLPs, so why do we need CANs? I want to argue that from a", "tokens": [50364, 445, 21601, 23043, 11, 366, 445, 411, 22611, 445, 21601, 23043, 11, 370, 983, 360, 321, 643, 22931, 82, 30, 286, 528, 281, 9695, 300, 490, 257, 50736], "temperature": 0.0, "avg_logprob": -0.13569431047181826, "compression_ratio": 1.515625, "no_speech_prob": 0.007009322755038738}, {"id": 398, "seek": 288608, "start": 2893.52, "end": 2902.48, "text": " like a high level philosophical level, CANs and MLPs are somewhat different. CAN is like clock work,", "tokens": [50736, 411, 257, 1090, 1496, 25066, 1496, 11, 22931, 82, 293, 21601, 23043, 366, 8344, 819, 13, 22931, 307, 411, 7830, 589, 11, 51184], "temperature": 0.0, "avg_logprob": -0.13569431047181826, "compression_ratio": 1.515625, "no_speech_prob": 0.007009322755038738}, {"id": 399, "seek": 288608, "start": 2903.7599999999998, "end": 2909.7599999999998, "text": " like in a clock, pieces are customized and have clear purposes corresponding to the learnable", "tokens": [51248, 411, 294, 257, 7830, 11, 3755, 366, 30581, 293, 362, 1850, 9932, 11760, 281, 264, 1466, 712, 51548], "temperature": 0.0, "avg_logprob": -0.13569431047181826, "compression_ratio": 1.515625, "no_speech_prob": 0.007009322755038738}, {"id": 400, "seek": 290976, "start": 2909.76, "end": 2917.44, "text": " activation functions in CANs. So for a clock, it's easy to tear it apart, it's easy to tear", "tokens": [50364, 24433, 6828, 294, 22931, 82, 13, 407, 337, 257, 7830, 11, 309, 311, 1858, 281, 12556, 309, 4936, 11, 309, 311, 1858, 281, 12556, 50748], "temperature": 0.0, "avg_logprob": -0.12671017139515978, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0045375279150903225}, {"id": 401, "seek": 290976, "start": 2917.44, "end": 2925.28, "text": " the pieces apart and then reassemble the pieces back together to get the clock back, but MLP is", "tokens": [50748, 264, 3755, 4936, 293, 550, 319, 37319, 264, 3755, 646, 1214, 281, 483, 264, 7830, 646, 11, 457, 21601, 47, 307, 51140], "temperature": 0.0, "avg_logprob": -0.12671017139515978, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0045375279150903225}, {"id": 402, "seek": 290976, "start": 2925.28, "end": 2932.1600000000003, "text": " like a house made of bricks, so in MLPs, the pieces are produced in the same standard way,", "tokens": [51140, 411, 257, 1782, 1027, 295, 25497, 11, 370, 294, 21601, 23043, 11, 264, 3755, 366, 7126, 294, 264, 912, 3832, 636, 11, 51484], "temperature": 0.0, "avg_logprob": -0.12671017139515978, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0045375279150903225}, {"id": 403, "seek": 290976, "start": 2932.1600000000003, "end": 2938.96, "text": " like each neuron takes in some linear combinations and then do some do the same nonlinear transformation", "tokens": [51484, 411, 1184, 34090, 2516, 294, 512, 8213, 21267, 293, 550, 360, 512, 360, 264, 912, 2107, 28263, 9887, 51824], "temperature": 0.0, "avg_logprob": -0.12671017139515978, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0045375279150903225}, {"id": 404, "seek": 293976, "start": 2940.48, "end": 2947.76, "text": " but it's difficult to tear, once you have a house built, it's difficult to tear apart the bricks", "tokens": [50400, 457, 309, 311, 2252, 281, 12556, 11, 1564, 291, 362, 257, 1782, 3094, 11, 309, 311, 2252, 281, 12556, 4936, 264, 25497, 50764], "temperature": 0.0, "avg_logprob": -0.11435083661760603, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00048022778355516493}, {"id": 405, "seek": 293976, "start": 2947.76, "end": 2958.1600000000003, "text": " and then reassemble them. So in summary, the source of complexity are different in CANs and in MLPs,", "tokens": [50764, 293, 550, 319, 37319, 552, 13, 407, 294, 12691, 11, 264, 4009, 295, 14024, 366, 819, 294, 22931, 82, 293, 294, 21601, 23043, 11, 51284], "temperature": 0.0, "avg_logprob": -0.11435083661760603, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00048022778355516493}, {"id": 406, "seek": 293976, "start": 2958.1600000000003, "end": 2964.5600000000004, "text": " the source of complexity in CANs come from the complexity of each individual object,", "tokens": [51284, 264, 4009, 295, 14024, 294, 22931, 82, 808, 490, 264, 14024, 295, 1184, 2609, 2657, 11, 51604], "temperature": 0.0, "avg_logprob": -0.11435083661760603, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.00048022778355516493}, {"id": 407, "seek": 296456, "start": 2964.56, "end": 2971.2, "text": " like those 1D learnable functions, but because the functions are 1D, no matter how complicated", "tokens": [50364, 411, 729, 502, 35, 1466, 712, 6828, 11, 457, 570, 264, 6828, 366, 502, 35, 11, 572, 1871, 577, 6179, 50696], "temperature": 0.0, "avg_logprob": -0.13953376397853945, "compression_ratio": 1.8934010152284264, "no_speech_prob": 0.013016210868954659}, {"id": 408, "seek": 296456, "start": 2971.2, "end": 2978.88, "text": " they are, they're 1D and they have clear purposes in some sense, they are nevertheless, they are", "tokens": [50696, 436, 366, 11, 436, 434, 502, 35, 293, 436, 362, 1850, 9932, 294, 512, 2020, 11, 436, 366, 26924, 11, 436, 366, 51080], "temperature": 0.0, "avg_logprob": -0.13953376397853945, "compression_ratio": 1.8934010152284264, "no_speech_prob": 0.013016210868954659}, {"id": 409, "seek": 296456, "start": 2978.88, "end": 2987.92, "text": " interpretable, but the complexity of CANs come from complicated interactions of individual parts,", "tokens": [51080, 7302, 712, 11, 457, 264, 14024, 295, 22931, 82, 808, 490, 6179, 13280, 295, 2609, 3166, 11, 51532], "temperature": 0.0, "avg_logprob": -0.13953376397853945, "compression_ratio": 1.8934010152284264, "no_speech_prob": 0.013016210868954659}, {"id": 410, "seek": 296456, "start": 2987.92, "end": 2994.4, "text": " the individual parts are simple, but the connections between these individual parts", "tokens": [51532, 264, 2609, 3166, 366, 2199, 11, 457, 264, 9271, 1296, 613, 2609, 3166, 51856], "temperature": 0.0, "avg_logprob": -0.13953376397853945, "compression_ratio": 1.8934010152284264, "no_speech_prob": 0.013016210868954659}, {"id": 411, "seek": 299440, "start": 2994.4, "end": 3001.6800000000003, "text": " are really complicated, I guess it's more like human brains, it's more like biology,", "tokens": [50364, 366, 534, 6179, 11, 286, 2041, 309, 311, 544, 411, 1952, 15442, 11, 309, 311, 544, 411, 14956, 11, 50728], "temperature": 0.0, "avg_logprob": -0.17697690138176306, "compression_ratio": 1.5375722543352601, "no_speech_prob": 0.004828037694096565}, {"id": 412, "seek": 299440, "start": 3003.04, "end": 3012.56, "text": " yeah, I don't know, but CANs seem like more aligned with the philosophy of reductionism,", "tokens": [50796, 1338, 11, 286, 500, 380, 458, 11, 457, 22931, 82, 1643, 411, 544, 17962, 365, 264, 10675, 295, 11004, 1434, 11, 51272], "temperature": 0.0, "avg_logprob": -0.17697690138176306, "compression_ratio": 1.5375722543352601, "no_speech_prob": 0.004828037694096565}, {"id": 413, "seek": 299440, "start": 3012.56, "end": 3019.04, "text": " where you hope that, where you expect that you can decompose a complicated object into a few", "tokens": [51272, 689, 291, 1454, 300, 11, 689, 291, 2066, 300, 291, 393, 22867, 541, 257, 6179, 2657, 666, 257, 1326, 51596], "temperature": 0.0, "avg_logprob": -0.17697690138176306, "compression_ratio": 1.5375722543352601, "no_speech_prob": 0.004828037694096565}, {"id": 414, "seek": 301904, "start": 3020.0, "end": 3027.6, "text": " like interpretable individual objects, well in MLPs, everything is connected,", "tokens": [50412, 411, 7302, 712, 2609, 6565, 11, 731, 294, 21601, 23043, 11, 1203, 307, 4582, 11, 50792], "temperature": 0.0, "avg_logprob": -0.19683110511909097, "compression_ratio": 1.4, "no_speech_prob": 0.0022513417061418295}, {"id": 415, "seek": 301904, "start": 3028.4, "end": 3038.0, "text": " the reason why MLPs function is because they have this emergent behavior or collective behavior", "tokens": [50832, 264, 1778, 983, 21601, 23043, 2445, 307, 570, 436, 362, 341, 4345, 6930, 5223, 420, 12590, 5223, 51312], "temperature": 0.0, "avg_logprob": -0.19683110511909097, "compression_ratio": 1.4, "no_speech_prob": 0.0022513417061418295}, {"id": 416, "seek": 301904, "start": 3038.0, "end": 3048.88, "text": " in some sense. Yeah, just some interesting question people ask, are CANs physical? So", "tokens": [51312, 294, 512, 2020, 13, 865, 11, 445, 512, 1880, 1168, 561, 1029, 11, 366, 22931, 82, 4001, 30, 407, 51856], "temperature": 0.0, "avg_logprob": -0.19683110511909097, "compression_ratio": 1.4, "no_speech_prob": 0.0022513417061418295}, {"id": 417, "seek": 304888, "start": 3049.76, "end": 3056.8, "text": " if we think of like the Feynman diagram as physical, then unfortunately, Feynman diagrams", "tokens": [50408, 498, 321, 519, 295, 411, 264, 46530, 77, 1601, 10686, 382, 4001, 11, 550, 7015, 11, 46530, 77, 1601, 36709, 50760], "temperature": 0.0, "avg_logprob": -0.16112573596014493, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.0006461290759034455}, {"id": 418, "seek": 304888, "start": 3056.8, "end": 3064.88, "text": " are sort of more like MLPs, because in Feynman diagrams, like on the edges, it's just like a", "tokens": [50760, 366, 1333, 295, 544, 411, 21601, 23043, 11, 570, 294, 46530, 77, 1601, 36709, 11, 411, 322, 264, 8819, 11, 309, 311, 445, 411, 257, 51164], "temperature": 0.0, "avg_logprob": -0.16112573596014493, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.0006461290759034455}, {"id": 419, "seek": 304888, "start": 3064.88, "end": 3074.96, "text": " free flow in space without anything interesting happen, but on the nodes where two particles", "tokens": [51164, 1737, 3095, 294, 1901, 1553, 1340, 1880, 1051, 11, 457, 322, 264, 13891, 689, 732, 10007, 51668], "temperature": 0.0, "avg_logprob": -0.16112573596014493, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.0006461290759034455}, {"id": 420, "seek": 307496, "start": 3074.96, "end": 3080.08, "text": " or multiple particles collide, it's where interesting things happen, this is more aligned,", "tokens": [50364, 420, 3866, 10007, 49093, 11, 309, 311, 689, 1880, 721, 1051, 11, 341, 307, 544, 17962, 11, 50620], "temperature": 0.0, "avg_logprob": -0.1377789369269983, "compression_ratio": 1.6, "no_speech_prob": 0.006902634631842375}, {"id": 421, "seek": 307496, "start": 3080.08, "end": 3087.52, "text": " this is probably more aligned with MLPs, but CANs is like interesting thing happens on the edges,", "tokens": [50620, 341, 307, 1391, 544, 17962, 365, 21601, 23043, 11, 457, 22931, 82, 307, 411, 1880, 551, 2314, 322, 264, 8819, 11, 50992], "temperature": 0.0, "avg_logprob": -0.1377789369269983, "compression_ratio": 1.6, "no_speech_prob": 0.006902634631842375}, {"id": 422, "seek": 307496, "start": 3087.52, "end": 3096.4, "text": " but not on the nodes. And yeah, last question, people also ask are CANs biological, because", "tokens": [50992, 457, 406, 322, 264, 13891, 13, 400, 1338, 11, 1036, 1168, 11, 561, 611, 1029, 366, 22931, 82, 13910, 11, 570, 51436], "temperature": 0.0, "avg_logprob": -0.1377789369269983, "compression_ratio": 1.6, "no_speech_prob": 0.006902634631842375}, {"id": 423, "seek": 309640, "start": 3097.12, "end": 3105.28, "text": " people think that MLPs are inspired by the neurons in our brains, are there any biological", "tokens": [50400, 561, 519, 300, 21601, 23043, 366, 7547, 538, 264, 22027, 294, 527, 15442, 11, 366, 456, 604, 13910, 50808], "temperature": 0.0, "avg_logprob": -0.14254151219907013, "compression_ratio": 1.2777777777777777, "no_speech_prob": 0.015421382151544094}, {"id": 424, "seek": 309640, "start": 3105.28, "end": 3117.04, "text": " analogy? And I don't know at first, but someone from Twitter wrote it that CANs actually is a", "tokens": [50808, 21663, 30, 400, 286, 500, 380, 458, 412, 700, 11, 457, 1580, 490, 5794, 4114, 309, 300, 22931, 82, 767, 307, 257, 51396], "temperature": 0.0, "avg_logprob": -0.14254151219907013, "compression_ratio": 1.2777777777777777, "no_speech_prob": 0.015421382151544094}, {"id": 425, "seek": 311704, "start": 3117.04, "end": 3128.4, "text": " bit analogous to the cells in retina, where each individual cell receive light, apply some", "tokens": [50364, 857, 16660, 563, 281, 264, 5438, 294, 1533, 1426, 11, 689, 1184, 2609, 2815, 4774, 1442, 11, 3079, 512, 50932], "temperature": 0.0, "avg_logprob": -0.2183168801394376, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.05663176625967026}, {"id": 426, "seek": 311704, "start": 3128.4, "end": 3134.08, "text": " nonlinear transformation to it before summing everything up, I'm not sure, you guys are experts,", "tokens": [50932, 2107, 28263, 9887, 281, 309, 949, 2408, 2810, 1203, 493, 11, 286, 478, 406, 988, 11, 291, 1074, 366, 8572, 11, 51216], "temperature": 0.0, "avg_logprob": -0.2183168801394376, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.05663176625967026}, {"id": 427, "seek": 311704, "start": 3134.08, "end": 3141.84, "text": " so please correct me if I'm wrong here. But the argument is that, well, maybe the mechanisms", "tokens": [51216, 370, 1767, 3006, 385, 498, 286, 478, 2085, 510, 13, 583, 264, 6770, 307, 300, 11, 731, 11, 1310, 264, 15902, 51604], "temperature": 0.0, "avg_logprob": -0.2183168801394376, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.05663176625967026}, {"id": 428, "seek": 311704, "start": 3141.84, "end": 3146.24, "text": " of CANs like your first applied nonlinear transformation, then summing everything up,", "tokens": [51604, 295, 22931, 82, 411, 428, 700, 6456, 2107, 28263, 9887, 11, 550, 2408, 2810, 1203, 493, 11, 51824], "temperature": 0.0, "avg_logprob": -0.2183168801394376, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.05663176625967026}, {"id": 429, "seek": 314624, "start": 3146.24, "end": 3155.2799999999997, "text": " is indeed biological, but I guess that's just for fun. That's just a minor justification why", "tokens": [50364, 307, 6451, 13910, 11, 457, 286, 2041, 300, 311, 445, 337, 1019, 13, 663, 311, 445, 257, 6696, 31591, 983, 50816], "temperature": 0.0, "avg_logprob": -0.14463371219057025, "compression_ratio": 1.447674418604651, "no_speech_prob": 0.002249786863103509}, {"id": 430, "seek": 314624, "start": 3156.9599999999996, "end": 3162.56, "text": " we need CANs, because in some sense, CANs are also biological.", "tokens": [50900, 321, 643, 22931, 82, 11, 570, 294, 512, 2020, 11, 22931, 82, 366, 611, 13910, 13, 51180], "temperature": 0.0, "avg_logprob": -0.14463371219057025, "compression_ratio": 1.447674418604651, "no_speech_prob": 0.002249786863103509}, {"id": 431, "seek": 314624, "start": 3167.04, "end": 3173.68, "text": " Yeah, so that's basically everything I would like to share, and I'm happy to chat more if you", "tokens": [51404, 865, 11, 370, 300, 311, 1936, 1203, 286, 576, 411, 281, 2073, 11, 293, 286, 478, 2055, 281, 5081, 544, 498, 291, 51736], "temperature": 0.0, "avg_logprob": -0.14463371219057025, "compression_ratio": 1.447674418604651, "no_speech_prob": 0.002249786863103509}, {"id": 432, "seek": 317368, "start": 3173.68, "end": 3181.44, "text": " guys have questions. Super interesting, thank you. Questions?", "tokens": [50364, 1074, 362, 1651, 13, 4548, 1880, 11, 1309, 291, 13, 27738, 30, 50752], "temperature": 0.0, "avg_logprob": -0.19915239928198641, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.0010808223159983754}, {"id": 433, "seek": 317368, "start": 3184.24, "end": 3190.7999999999997, "text": " Well, for the last last example in retina, I have some questions. If the network,", "tokens": [50892, 1042, 11, 337, 264, 1036, 1036, 1365, 294, 1533, 1426, 11, 286, 362, 512, 1651, 13, 759, 264, 3209, 11, 51220], "temperature": 0.0, "avg_logprob": -0.19915239928198641, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.0010808223159983754}, {"id": 434, "seek": 317368, "start": 3190.7999999999997, "end": 3198.3199999999997, "text": " like CAN network, is deep enough, is it still matter if you say nonlinearities before or after", "tokens": [51220, 411, 22931, 3209, 11, 307, 2452, 1547, 11, 307, 309, 920, 1871, 498, 291, 584, 2107, 28263, 1088, 949, 420, 934, 51596], "temperature": 0.0, "avg_logprob": -0.19915239928198641, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.0010808223159983754}, {"id": 435, "seek": 319832, "start": 3199.2000000000003, "end": 3201.04, "text": " the summation?", "tokens": [50408, 264, 28811, 30, 50500], "temperature": 0.0, "avg_logprob": -0.2408751721652049, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.011677720583975315}, {"id": 436, "seek": 319832, "start": 3204.56, "end": 3212.56, "text": " Yeah, so I guess the key difference, well, yeah, so I guess the key difference is that", "tokens": [50676, 865, 11, 370, 286, 2041, 264, 2141, 2649, 11, 731, 11, 1338, 11, 370, 286, 2041, 264, 2141, 2649, 307, 300, 51076], "temperature": 0.0, "avg_logprob": -0.2408751721652049, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.011677720583975315}, {"id": 437, "seek": 319832, "start": 3212.56, "end": 3221.2000000000003, "text": " in CANs, activation functions are learnable, so I guess, yeah, but whether to put activation", "tokens": [51076, 294, 22931, 82, 11, 24433, 6828, 366, 1466, 712, 11, 370, 286, 2041, 11, 1338, 11, 457, 1968, 281, 829, 24433, 51508], "temperature": 0.0, "avg_logprob": -0.2408751721652049, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.011677720583975315}, {"id": 438, "seek": 322120, "start": 3221.2, "end": 3230.56, "text": " functions on edges or on nodes, I don't think that might be the key difference,", "tokens": [50364, 6828, 322, 8819, 420, 322, 13891, 11, 286, 500, 380, 519, 300, 1062, 312, 264, 2141, 2649, 11, 50832], "temperature": 0.0, "avg_logprob": -0.19259164983575994, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.002714676782488823}, {"id": 439, "seek": 322120, "start": 3230.56, "end": 3235.2799999999997, "text": " like the learnability of activation functions give you more flexibility.", "tokens": [50832, 411, 264, 1466, 2310, 295, 24433, 6828, 976, 291, 544, 12635, 13, 51068], "temperature": 0.0, "avg_logprob": -0.19259164983575994, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.002714676782488823}, {"id": 440, "seek": 322120, "start": 3237.52, "end": 3243.4399999999996, "text": " Yeah, when you talk about this, I was thinking about that CAN is decomposing", "tokens": [51180, 865, 11, 562, 291, 751, 466, 341, 11, 286, 390, 1953, 466, 300, 22931, 307, 22867, 6110, 51476], "temperature": 0.0, "avg_logprob": -0.19259164983575994, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.002714676782488823}, {"id": 441, "seek": 324344, "start": 3244.4, "end": 3253.76, "text": " different variables, input variables, like if you have x and y, then CAN could decompose it,", "tokens": [50412, 819, 9102, 11, 4846, 9102, 11, 411, 498, 291, 362, 2031, 293, 288, 11, 550, 22931, 727, 22867, 541, 309, 11, 50880], "temperature": 0.0, "avg_logprob": -0.20797609947097134, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0138460798189044}, {"id": 442, "seek": 324344, "start": 3253.76, "end": 3261.76, "text": " because you would have a different combination of them, but like if you have a nested function,", "tokens": [50880, 570, 291, 576, 362, 257, 819, 6562, 295, 552, 11, 457, 411, 498, 291, 362, 257, 15646, 292, 2445, 11, 51280], "temperature": 0.0, "avg_logprob": -0.20797609947097134, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0138460798189044}, {"id": 443, "seek": 324344, "start": 3261.76, "end": 3271.04, "text": " like sine x square, or exponential sine x square, then the CAN seems not able to decompose them,", "tokens": [51280, 411, 18609, 2031, 3732, 11, 420, 21510, 18609, 2031, 3732, 11, 550, 264, 22931, 2544, 406, 1075, 281, 22867, 541, 552, 11, 51744], "temperature": 0.0, "avg_logprob": -0.20797609947097134, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0138460798189044}, {"id": 444, "seek": 327104, "start": 3271.04, "end": 3272.8, "text": " because they don't have these primitives.", "tokens": [50364, 570, 436, 500, 380, 362, 613, 2886, 38970, 13, 50452], "temperature": 0.0, "avg_logprob": -0.18048673652740846, "compression_ratio": 1.4976303317535544, "no_speech_prob": 0.02798907645046711}, {"id": 445, "seek": 327104, "start": 3273.6, "end": 3280.32, "text": " Yes, yes, yes, that's exactly correct. So CAN can only discover compositionality in the sense that", "tokens": [50492, 1079, 11, 2086, 11, 2086, 11, 300, 311, 2293, 3006, 13, 407, 22931, 393, 787, 4411, 12686, 1860, 294, 264, 2020, 300, 50828], "temperature": 0.0, "avg_logprob": -0.18048673652740846, "compression_ratio": 1.4976303317535544, "no_speech_prob": 0.02798907645046711}, {"id": 446, "seek": 327104, "start": 3280.32, "end": 3288.0, "text": " all the 1D functions are boring to CANs. It can just be approximated with just one B-splice,", "tokens": [50828, 439, 264, 502, 35, 6828, 366, 9989, 281, 22931, 82, 13, 467, 393, 445, 312, 8542, 770, 365, 445, 472, 363, 12, 46535, 573, 11, 51212], "temperature": 0.0, "avg_logprob": -0.18048673652740846, "compression_ratio": 1.4976303317535544, "no_speech_prob": 0.02798907645046711}, {"id": 447, "seek": 327104, "start": 3288.0, "end": 3298.08, "text": " it doesn't learn any compositionality for single variables. That might be one bug,", "tokens": [51212, 309, 1177, 380, 1466, 604, 12686, 1860, 337, 2167, 9102, 13, 663, 1062, 312, 472, 7426, 11, 51716], "temperature": 0.0, "avg_logprob": -0.18048673652740846, "compression_ratio": 1.4976303317535544, "no_speech_prob": 0.02798907645046711}, {"id": 448, "seek": 329808, "start": 3299.04, "end": 3309.44, "text": " if you will, for CANs, if you really want to figure out the symbolic formulas in the data set.", "tokens": [50412, 498, 291, 486, 11, 337, 22931, 82, 11, 498, 291, 534, 528, 281, 2573, 484, 264, 25755, 30546, 294, 264, 1412, 992, 13, 50932], "temperature": 0.0, "avg_logprob": -0.24624619585402469, "compression_ratio": 1.2773722627737227, "no_speech_prob": 0.002049208851531148}, {"id": 449, "seek": 329808, "start": 3311.2799999999997, "end": 3321.84, "text": " But like for Professor Tomaso Poggio, the author of the 1989 paper who sentenced", "tokens": [51024, 583, 411, 337, 8419, 5041, 35281, 430, 664, 17862, 11, 264, 3793, 295, 264, 22427, 3035, 567, 30954, 51552], "temperature": 0.0, "avg_logprob": -0.24624619585402469, "compression_ratio": 1.2773722627737227, "no_speech_prob": 0.002049208851531148}, {"id": 450, "seek": 332184, "start": 3322.8, "end": 3330.6400000000003, "text": " or who sentenced Theorem to jail, he wrote in his paper that all the 1D functions are boring.", "tokens": [50412, 420, 567, 30954, 440, 37956, 281, 10511, 11, 415, 4114, 294, 702, 3035, 300, 439, 264, 502, 35, 6828, 366, 9989, 13, 50804], "temperature": 0.0, "avg_logprob": -0.18704557418823242, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.009706126525998116}, {"id": 451, "seek": 332184, "start": 3333.6000000000004, "end": 3337.6000000000004, "text": " And what's interesting is this compositional sparsity when you are dealing with multiple", "tokens": [50952, 400, 437, 311, 1880, 307, 341, 10199, 2628, 637, 685, 507, 562, 291, 366, 6260, 365, 3866, 51152], "temperature": 0.0, "avg_logprob": -0.18704557418823242, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.009706126525998116}, {"id": 452, "seek": 332184, "start": 3338.56, "end": 3343.6000000000004, "text": " variables, but I guess it depends on your goal. If your goal is just to learn the function efficiently,", "tokens": [51200, 9102, 11, 457, 286, 2041, 309, 5946, 322, 428, 3387, 13, 759, 428, 3387, 307, 445, 281, 1466, 264, 2445, 19621, 11, 51452], "temperature": 0.0, "avg_logprob": -0.18704557418823242, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.009706126525998116}, {"id": 453, "seek": 332184, "start": 3343.6000000000004, "end": 3351.6000000000004, "text": " then it's fine. But if your goal is to really understand if it's sine of exponential, exponential", "tokens": [51452, 550, 309, 311, 2489, 13, 583, 498, 428, 3387, 307, 281, 534, 1223, 498, 309, 311, 18609, 295, 21510, 11, 21510, 51852], "temperature": 0.0, "avg_logprob": -0.18704557418823242, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.009706126525998116}, {"id": 454, "seek": 335160, "start": 3351.6, "end": 3357.52, "text": " of sine, then we probably need to think about ways to handle this.", "tokens": [50364, 295, 18609, 11, 550, 321, 1391, 643, 281, 519, 466, 2098, 281, 4813, 341, 13, 50660], "temperature": 0.0, "avg_logprob": -0.25871552161450656, "compression_ratio": 1.4105960264900663, "no_speech_prob": 0.0015222684014588594}, {"id": 455, "seek": 335160, "start": 3360.08, "end": 3361.04, "text": " Yeah, thank you.", "tokens": [50788, 865, 11, 1309, 291, 13, 50836], "temperature": 0.0, "avg_logprob": -0.25871552161450656, "compression_ratio": 1.4105960264900663, "no_speech_prob": 0.0015222684014588594}, {"id": 456, "seek": 335160, "start": 3363.36, "end": 3369.7599999999998, "text": " Have you thought about combining CANs and MLPs, given their somewhat complementary nature,", "tokens": [50952, 3560, 291, 1194, 466, 21928, 22931, 82, 293, 21601, 23043, 11, 2212, 641, 8344, 40705, 3687, 11, 51272], "temperature": 0.0, "avg_logprob": -0.25871552161450656, "compression_ratio": 1.4105960264900663, "no_speech_prob": 0.0015222684014588594}, {"id": 457, "seek": 335160, "start": 3369.7599999999998, "end": 3371.92, "text": " or despite their complementary nature?", "tokens": [51272, 420, 7228, 641, 40705, 3687, 30, 51380], "temperature": 0.0, "avg_logprob": -0.25871552161450656, "compression_ratio": 1.4105960264900663, "no_speech_prob": 0.0015222684014588594}, {"id": 458, "seek": 337192, "start": 3372.88, "end": 3374.48, "text": " Yeah, that's a great question.", "tokens": [50412, 865, 11, 300, 311, 257, 869, 1168, 13, 50492], "temperature": 0.0, "avg_logprob": -0.20866292209948523, "compression_ratio": 1.4266666666666667, "no_speech_prob": 0.030192906036973}, {"id": 459, "seek": 337192, "start": 3381.92, "end": 3388.64, "text": " So we have some primitives, like CANs can propose this CAN layer, which is a new primitive.", "tokens": [50864, 407, 321, 362, 512, 2886, 38970, 11, 411, 22931, 82, 393, 17421, 341, 22931, 4583, 11, 597, 307, 257, 777, 28540, 13, 51200], "temperature": 0.0, "avg_logprob": -0.20866292209948523, "compression_ratio": 1.4266666666666667, "no_speech_prob": 0.030192906036973}, {"id": 460, "seek": 337192, "start": 3388.64, "end": 3396.64, "text": " And for MLPs, it has these linear layers and also the nonlinear activations, which are also", "tokens": [51200, 400, 337, 21601, 23043, 11, 309, 575, 613, 8213, 7914, 293, 611, 264, 2107, 28263, 2430, 763, 11, 597, 366, 611, 51600], "temperature": 0.0, "avg_logprob": -0.20866292209948523, "compression_ratio": 1.4266666666666667, "no_speech_prob": 0.030192906036973}, {"id": 461, "seek": 339664, "start": 3396.72, "end": 3407.12, "text": " primitive. I mean, these are like the building blocks. And I guess as long as they fit together,", "tokens": [50368, 28540, 13, 286, 914, 11, 613, 366, 411, 264, 2390, 8474, 13, 400, 286, 2041, 382, 938, 382, 436, 3318, 1214, 11, 50888], "temperature": 0.0, "avg_logprob": -0.1352283505425937, "compression_ratio": 1.6196319018404908, "no_speech_prob": 0.05028042197227478}, {"id": 462, "seek": 339664, "start": 3410.24, "end": 3419.04, "text": " as long as they fit together, you can freely just combine them in ways that you want.", "tokens": [51044, 382, 938, 382, 436, 3318, 1214, 11, 291, 393, 16433, 445, 10432, 552, 294, 2098, 300, 291, 528, 13, 51484], "temperature": 0.0, "avg_logprob": -0.1352283505425937, "compression_ratio": 1.6196319018404908, "no_speech_prob": 0.05028042197227478}, {"id": 463, "seek": 339664, "start": 3419.04, "end": 3425.2799999999997, "text": " But it's just that it's a bit hard to tell what's the advantage of combining. And", "tokens": [51484, 583, 309, 311, 445, 300, 309, 311, 257, 857, 1152, 281, 980, 437, 311, 264, 5002, 295, 21928, 13, 400, 51796], "temperature": 0.0, "avg_logprob": -0.1352283505425937, "compression_ratio": 1.6196319018404908, "no_speech_prob": 0.05028042197227478}, {"id": 464, "seek": 342528, "start": 3425.28, "end": 3431.76, "text": " because I guess there are many ways to integrate the two models and which way is the best. And I", "tokens": [50364, 570, 286, 2041, 456, 366, 867, 2098, 281, 13365, 264, 732, 5245, 293, 597, 636, 307, 264, 1151, 13, 400, 286, 50688], "temperature": 0.0, "avg_logprob": -0.14851744969685873, "compression_ratio": 1.472972972972973, "no_speech_prob": 0.001409395830705762}, {"id": 465, "seek": 342528, "start": 3431.76, "end": 3441.2000000000003, "text": " guess it's a case dependent question. It again depends on what's your application, what's your goal,", "tokens": [50688, 2041, 309, 311, 257, 1389, 12334, 1168, 13, 467, 797, 5946, 322, 437, 311, 428, 3861, 11, 437, 311, 428, 3387, 11, 51160], "temperature": 0.0, "avg_logprob": -0.14851744969685873, "compression_ratio": 1.472972972972973, "no_speech_prob": 0.001409395830705762}, {"id": 466, "seek": 342528, "start": 3441.2000000000003, "end": 3442.2400000000002, "text": " something like that.", "tokens": [51160, 746, 411, 300, 13, 51212], "temperature": 0.0, "avg_logprob": -0.14851744969685873, "compression_ratio": 1.472972972972973, "no_speech_prob": 0.001409395830705762}], "language": "en"}