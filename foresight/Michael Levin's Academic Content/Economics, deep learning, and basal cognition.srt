1
00:00:00,000 --> 00:00:01,680
I appreciate that. Yeah, I'm really looking forward to this.

2
00:00:01,680 --> 00:00:06,120
I think you guys both had some great ideas recently, and I'm looking forward to it.

3
00:00:06,280 --> 00:00:10,080
Can we just start and maybe, let's see, Ben can go first and then David,

4
00:00:10,080 --> 00:00:13,360
just say a couple of words about who you are and what your interests are.

5
00:00:14,400 --> 00:00:16,800
I'm a former stockbroker with the background in economics,

6
00:00:16,800 --> 00:00:19,760
and I'm currently studying the economics of collective intelligence,

7
00:00:19,760 --> 00:00:22,560
looking to put together an understanding of how people get along

8
00:00:22,560 --> 00:00:24,400
and accomplish things that no individual can.

9
00:00:25,280 --> 00:00:32,320
Cool. I am by training a stockbroker engineer, but I've been into AI

10
00:00:32,320 --> 00:00:35,200
ever since the high school reading age of intelligent machines,

11
00:00:36,160 --> 00:00:40,080
and then just kept dipping in and out of the AI field and kind of being like,

12
00:00:40,080 --> 00:00:44,960
it's not ready, it's not ready. After college, I co-founded one of the first AGI startups

13
00:00:44,960 --> 00:00:48,640
with like Shane Legg, one of my co-founders was our first employee.

14
00:00:48,640 --> 00:00:53,760
So like back in 2000s, and then went to work at Google and Facebook,

15
00:00:53,760 --> 00:00:58,960
and then Asanda, mostly either working on ML, but most of my time has been building

16
00:00:58,960 --> 00:01:02,720
like large-scale distributed systems. So really coming at this more from

17
00:01:02,720 --> 00:01:08,880
an engineering background, but also about three years ago, I left my job and

18
00:01:08,880 --> 00:01:13,360
dove back into AI and came across Carl Friston's work.

19
00:01:13,360 --> 00:01:15,600
And I think that's when it kind of all clicked for me.

20
00:01:15,600 --> 00:01:20,800
And I was like, wow, we have GPUs now, we have a theory of intelligence

21
00:01:20,800 --> 00:01:27,200
that I feel like basically covers. It's like, I don't think there's any more secret sauce.

22
00:01:27,200 --> 00:01:31,920
It's like, we kind of, the formulas from active inference look a lot like formulas

23
00:01:31,920 --> 00:01:37,680
from traditional reinforcement learning. We have the hardware now, so I just like

24
00:01:37,680 --> 00:01:40,800
got really excited about it and have been working on it.

25
00:01:40,800 --> 00:01:46,720
So that's like my software and engineering side. I also co-founded Plurality Institute

26
00:01:46,720 --> 00:01:52,800
with Glenn Wild, who's like an economist, and that's more on the human collective intelligence side.

27
00:01:53,520 --> 00:01:58,880
Essentially, we're trying to create a new field, academic field called Plurality Studies that

28
00:01:58,880 --> 00:02:03,520
focuses on human, scaling human cooperation and collaboration, essentially collective

29
00:02:03,520 --> 00:02:08,720
intelligence as an academic field. And we've been bringing together researchers from like

30
00:02:08,720 --> 00:02:15,280
political science, computer science, economics, peace building. I mean, it's such an interdisciplinary

31
00:02:15,280 --> 00:02:21,680
field of like, how do we help humans cooperate and using technology, and now especially AI to

32
00:02:22,960 --> 00:02:29,280
bridge people. So that's kind of my other project. But my main research focus is,

33
00:02:29,840 --> 00:02:36,960
and I can share a little bit of like the research that I'm working on. And I would love, you know,

34
00:02:36,960 --> 00:02:41,840
help and collaboration. And also would love to hear how you guys are thinking about this. And if I

35
00:02:41,840 --> 00:02:47,040
can help, because I think this is like the most exciting thing that's happening in the world right

36
00:02:47,040 --> 00:02:54,640
now. Cool. Yeah, please go for it. Yeah. Yes. So I will say yes. So Glenn Wild is an interesting name

37
00:02:54,640 --> 00:03:00,240
because I think he's got a very important idea, which is that we should be using markets to

38
00:03:00,240 --> 00:03:04,080
solve more problems because there's basically a perspective in economics that says,

39
00:03:04,960 --> 00:03:09,120
anytime you've got a problem, there's a market that can solve that problem. And people just

40
00:03:09,120 --> 00:03:15,200
haven't been creative enough historically about what kind of markets you can use. Like we're just

41
00:03:15,200 --> 00:03:21,040
very used to like trading like standard property rights and things. And there's a lot of other

42
00:03:21,040 --> 00:03:25,440
ways you could conceive of doing things. I like the fact that Glenn Wild is pushing that I'd be

43
00:03:25,440 --> 00:03:29,360
interested in. So I don't actually know what he's doing. So actually, I'm just curious about that.

44
00:03:29,360 --> 00:03:32,720
I don't actually know what he does with AI. So can you just tell me a little bit about what he does

45
00:03:32,720 --> 00:03:38,720
with the Plurality Institute? Yeah, well, the Plurality Institute is not is more of the academic

46
00:03:38,720 --> 00:03:44,640
like collective and human collective intelligence of which AI is like a tool. So for example,

47
00:03:45,680 --> 00:03:52,080
and I mean markets are one collective intelligence system. But then there are other ones like even

48
00:03:52,640 --> 00:03:57,920
like common sections on forums are collective intelligence mechanism where humans come together

49
00:03:57,920 --> 00:04:02,960
to synchronize on epistemology like on things that they know and believe and being able to use

50
00:04:02,960 --> 00:04:10,000
LLMs to say automatically deescalate flame wars in common sections and drag people to consensus

51
00:04:10,000 --> 00:04:16,320
is like an example. In terms of the more economics things, so using quadratic funding for like public

52
00:04:16,320 --> 00:04:23,200
goods funding is something he has been writing about creating identity like trust networks to

53
00:04:23,200 --> 00:04:29,520
create identity like social identity so that you can use that as infrastructure for all these other

54
00:04:30,800 --> 00:04:38,400
like market mechanisms is another one. There's someone that's working on vector prices. So rather

55
00:04:38,400 --> 00:04:44,560
than there being a single price signal, there's a whole basket collection of currencies that you

56
00:04:44,560 --> 00:04:50,320
hold and that people can specify their own values for different currencies. And then the way the

57
00:04:50,320 --> 00:04:55,760
trading works is basically when you're trading with people that are aligned with you on values,

58
00:04:55,760 --> 00:05:02,320
you automatically get discounts because they value your currencies more than they value some.

59
00:05:02,320 --> 00:05:08,720
So there's like a whole bunch of them. He just has a book out open source isn't like another like

60
00:05:08,720 --> 00:05:14,160
how large open source projects are incentivized and how collaborations occur without like a

61
00:05:14,160 --> 00:05:20,560
corporate structure. He has a book out now called plurality with Audrey Tong who is another

62
00:05:21,200 --> 00:05:28,080
inspiring person. So I would check that out and that's like is going to cover a lot of different

63
00:05:28,080 --> 00:05:34,160
aspects of collect like plurality and collective intelligence. Cool. Is a lot of that just on

64
00:05:34,160 --> 00:05:40,000
the plurality website? The plurality instant that instituted website, we like post events and talks

65
00:05:40,000 --> 00:05:44,400
and we have the link to the plurality book by the plurality book is like much more detailed.

66
00:05:45,520 --> 00:05:51,680
I will check that out. So maybe what I can do then is just kind of give a broad overview of what I

67
00:05:51,680 --> 00:05:55,680
think the basics of economics of collective intelligence are and we can see how that relates

68
00:05:55,680 --> 00:06:02,400
to our work. So the way I see it is that collective intelligence is all about getting people to

69
00:06:02,400 --> 00:06:09,600
communicate in honest ways in honest and relevant ways such that everybody forms a shared model

70
00:06:09,680 --> 00:06:13,680
and because everyone's behaving with that same shared model everyone ends up behaving as if

71
00:06:13,680 --> 00:06:18,480
they're all following the same you know sort of commands if you will like a dictator or a virtual

72
00:06:18,480 --> 00:06:24,000
governor and everyone ends up nicely coordinated to achieve that kind of signaling system. You need

73
00:06:24,000 --> 00:06:30,320
something that's analogous to a price system and what that allows you to do is it's principles

74
00:06:30,320 --> 00:06:36,720
very similar to what Mike has talked about in some of his work where you get a way of getting an

75
00:06:36,720 --> 00:06:42,400
entity to see questions about other people as questions about itself and so like information

76
00:06:42,400 --> 00:06:46,240
gets transmitted in such a way where like when someone else is in trouble you perceive that as

77
00:06:46,240 --> 00:06:51,120
stress to yourself when something benefits someone else you perceive that as benefit to yourself

78
00:06:51,840 --> 00:06:56,160
which we see very easily in the price system like if someone needs more food they'll you know have

79
00:06:56,160 --> 00:07:00,800
a demand for food that'll raise the price of food so that stress gets transmitted to everyone else

80
00:07:00,800 --> 00:07:05,360
which uses to buy less food or similarly if you help someone get food by supplying some food that

81
00:07:05,360 --> 00:07:11,600
they like their benefit becomes your benefit via the revenue you make some money so that very

82
00:07:11,600 --> 00:07:17,840
simple system ends up outlining really just the basic functionality of a collective intelligence

83
00:07:17,840 --> 00:07:22,160
I think that any collective intelligence probably follows basic price like principles

84
00:07:22,160 --> 00:07:26,320
where you need some system where people aren't incentivized to communicate honestly with each

85
00:07:26,320 --> 00:07:33,120
other and such that everyone's signals gets condensed into like your own personal situation

86
00:07:33,120 --> 00:07:38,000
so just by optimizing for yourself you end up inadvertently taking care of having one else

87
00:07:38,000 --> 00:07:42,480
so are things like that when you're looking at like AI and technology and stuff like I don't

88
00:07:42,480 --> 00:07:47,200
know what goes on inside a computer it's all magic to me is that is that what's going on with like the

89
00:07:47,200 --> 00:07:52,480
transistors do they all have their own like individual model and they end up like communicating to

90
00:07:52,480 --> 00:08:01,520
form a shared model you know I mean I guess it's not level everything is doing the longest right

91
00:08:01,520 --> 00:08:07,920
everything is like mutually reducing free energy and you can feed every part of the

92
00:08:07,920 --> 00:08:13,840
subsystem as an agent so in some in some level yes but I think what you probably mean is more

93
00:08:13,840 --> 00:08:23,040
directly in a computational way and like typically what we do now is we train neural networks which

94
00:08:23,040 --> 00:08:31,600
are a big pile of math but but essentially it's a thing that like learns function approximations

95
00:08:32,240 --> 00:08:36,880
and the way it learns function approximations internally and we're still studying you know

96
00:08:36,880 --> 00:08:44,000
what actually happens internally but probably yes things get partitioned into sub processes

97
00:08:44,000 --> 00:08:49,760
like sub functions with sparse or communication between them so kind of what you're described

98
00:08:50,160 --> 00:08:56,240
if you think like in your model an individual is this like very interconnected system and

99
00:08:56,240 --> 00:09:02,000
then it has these sparse connections to other deeply interconnected systems and this is like

100
00:09:02,000 --> 00:09:06,000
you know a cell is doing a lot of compute inside but then it's got just like a few

101
00:09:07,040 --> 00:09:12,480
membrane exchanges with outside or corporation there's a lot of stuff going on inside but it

102
00:09:12,480 --> 00:09:17,840
communicates with the rest of the economy the buying and selling products so in some sense anything

103
00:09:17,840 --> 00:09:24,960
that you partition into subsystems is essentially doing this it is using some form of information

104
00:09:26,240 --> 00:09:32,400
like sparse information propagation between units in terms of honesty I think that's a lot

105
00:09:32,400 --> 00:09:38,560
trickier because it's like the communication doesn't need to be honest it needs to be valuable

106
00:09:39,120 --> 00:09:45,680
to like I think there is like a theorem that like for communication to exist it has to benefit

107
00:09:45,680 --> 00:09:51,520
both parties and so there has to be some value to it and you can maybe say that that value is the

108
00:09:51,520 --> 00:09:58,320
truthiness and so you could like maybe decouple it into like signal and noise so there's like some

109
00:09:58,320 --> 00:10:03,280
truth value and then maybe there's like a bunch of a noise value if it's all noise that communication

110
00:10:03,280 --> 00:10:09,440
channel is just not going to persist and so in terms of I think when you're like hey there should

111
00:10:09,440 --> 00:10:16,720
be honest pricing mechanism I would instead say there should be the two agents or whatever the

112
00:10:16,720 --> 00:10:22,000
network of agents there needs to be ways for them to communicate usefully with each other which it

113
00:10:22,000 --> 00:10:27,280
may be is the same thing but they kind of co-discover what that is and that doesn't necessarily need to

114
00:10:27,280 --> 00:10:34,640
be imposed by a mechanism but if a market mechanism or some communication framework

115
00:10:34,640 --> 00:10:40,400
allows honest communication channels then you're making it much easier for them to discover how

116
00:10:40,400 --> 00:10:48,240
to use it. I don't know if that if any of that resonated. It does and maybe there's something

117
00:10:48,240 --> 00:10:53,040
interesting to work out here so economics has been talking about the importance of getting

118
00:10:53,040 --> 00:10:58,480
people to communicate honestly to achieve coordination for a long time and sort of the

119
00:10:58,480 --> 00:11:02,880
basic picture of that which is very silly and false in some ways but also maybe important for

120
00:11:03,200 --> 00:11:08,640
intuition is if you're trying to centrally plan an economy one conceivable way you do that is

121
00:11:08,640 --> 00:11:12,240
you know you have an allocation problem who should get what goods you can just send everyone

122
00:11:12,240 --> 00:11:17,120
like a giant questionnaire and say like do you need a new pair of shoes etc and the problem with

123
00:11:17,120 --> 00:11:22,320
that is that people could lie and they say yeah I totally need a new pair of shoes and so there's

124
00:11:22,320 --> 00:11:29,280
a lot about that picture that's very unrealistic but figuring out ways to get people to honestly

125
00:11:29,280 --> 00:11:34,480
is maybe a word that has some some bad connotations because we interpret in terms of like deliberate

126
00:11:34,480 --> 00:11:40,960
intention and like moral character it's really more just about like truthfully conveying your

127
00:11:40,960 --> 00:11:45,920
intentions so you so basically maybe this is a way of thinking about it you can sort of think of

128
00:11:45,920 --> 00:11:50,720
economics as a very complicated version of like a traffic you're just trying to get cars to not

129
00:11:50,720 --> 00:11:56,240
crash into each other and that means that each car needs to predict where each other car is going

130
00:11:56,240 --> 00:11:59,680
that means each car needs to be sending very clear signals I'm definitely going here and I'm not

131
00:11:59,680 --> 00:12:04,080
going to like turn suddenly or something like that so you're trying to get people to convey what

132
00:12:04,080 --> 00:12:08,560
their plans are so that everyone knows what everyone else is planning and so then they can pick a

133
00:12:08,560 --> 00:12:17,680
plan that's consistent with all those other plans and I guess this is valuable it's like much more

134
00:12:17,680 --> 00:12:23,760
of like I guess an information theory kind of take I think there's I think there's another

135
00:12:23,760 --> 00:12:31,840
thing that that is missing from that model which is that if you have like a really narrow freeway

136
00:12:31,840 --> 00:12:34,880
then some cars are going to get through and some cars are going to have to wait

137
00:12:35,760 --> 00:12:42,000
and there's not an easy way to resolve which ones so I think like and this is kind of back to the

138
00:12:42,000 --> 00:12:47,840
plurality way that I've been thinking about it is like these are all forms of alignment and I break

139
00:12:47,840 --> 00:12:53,200
alignment down into like aligning on action which is kind of what economics in some sense lets you

140
00:12:53,200 --> 00:12:58,880
do is like okay we all have common beliefs let's given our beliefs what should we do and how can we

141
00:12:58,880 --> 00:13:06,480
coordinate our action how can we all send our like our routes so that we don't like collide

142
00:13:06,480 --> 00:13:11,920
so that's kind of aligning on action there's aligning on epistemologies like how do we all come to

143
00:13:11,920 --> 00:13:18,720
believe the same world models and then there's aligning on value which is how do we actually

144
00:13:18,720 --> 00:13:23,040
like I want one thing you want a different thing they maybe maybe we want opposite things how do

145
00:13:23,040 --> 00:13:30,160
we align on that and all of these these three systems are all interdependent because if you

146
00:13:30,160 --> 00:13:35,360
actually want different things you're not going to necessarily even align on epistemology because

147
00:13:35,360 --> 00:13:40,640
the things you know only things that you care about enough to know and if you care about different

148
00:13:40,640 --> 00:13:45,040
things you're not even going to necessarily ask the same questions of the universe to get back

149
00:13:45,040 --> 00:13:48,960
the same knowledge and if you have different beliefs you're not going to align on action

150
00:13:49,760 --> 00:13:56,400
and they all have to be iterated so like if you can agree on the same epistemology then maybe you

151
00:13:56,400 --> 00:14:02,560
can shift both shift your values because what you value depends on what you know and so in some

152
00:14:02,560 --> 00:14:08,640
sense it's like you have to pull all of these three systems in and I think try and what that's

153
00:14:08,640 --> 00:14:13,680
another cool thing about economics is it also does help value alignment and knowledge alignment

154
00:14:13,680 --> 00:14:19,360
because prices are knowledge signals so they tell you know what goods are easier to get when

155
00:14:19,360 --> 00:14:25,360
and they're also value alignment mechanisms because they allow you to trade something that you value

156
00:14:25,360 --> 00:14:30,240
more for something that you value less so I think there are these pieces but they all have to kind

157
00:14:30,240 --> 00:14:37,280
of play together so I want to go in a couple of different directions with that and maybe the first

158
00:14:37,280 --> 00:14:41,840
thing I'll mention is actually pulling away from economics a bit and talk about some neuroscience

159
00:14:41,840 --> 00:14:46,880
so on a different comment I left on Mike's blog post I talk about the work of Lisa Feldman Barrett

160
00:14:47,600 --> 00:14:52,240
who's done super important stuff it's actually her work that really prepared me to understand

161
00:14:52,240 --> 00:14:57,920
what Mike's talking about because I don't know anything about biology and the active inference

162
00:14:58,640 --> 00:15:03,680
sort of predictive coding view of things that she's really advancing as a way of understanding

163
00:15:03,680 --> 00:15:10,480
what the brain does you know sort of we we do often naturally draw these divides between

164
00:15:11,120 --> 00:15:17,280
knowledge acting and valuing and the active inference way of viewing things very much pushes

165
00:15:17,280 --> 00:15:22,480
all of that together it's actually more like action that drives perception and really your

166
00:15:22,480 --> 00:15:30,160
goals are at the base of all of that but also your goals are determined by you know I think of

167
00:15:30,160 --> 00:15:35,200
goals as basically like beliefs or measurements that you have like your goals are in a sense your

168
00:15:35,200 --> 00:15:41,280
epistemology because your goals are your measurements about the world and your prior expectations

169
00:15:41,280 --> 00:15:44,960
it's a weird sounding view and maybe we can talk about that but um that's kind of how I think about

170
00:15:44,960 --> 00:15:52,080
that and so I would you know it's very useful sometimes to separate these things out because

171
00:15:52,960 --> 00:15:57,440
there's a reason we draw these categories we categorize because there are practical reasons

172
00:15:57,440 --> 00:16:03,760
to do so and so that can be useful but also it's I want a fully unified perspective where we're

173
00:16:03,760 --> 00:16:07,840
just treating all of that as kind of the same thing in some level and in economics at the end

174
00:16:07,840 --> 00:16:14,320
of the day when you're trading something you can disagree very strongly about you know like the trade

175
00:16:14,320 --> 00:16:20,400
ends up compressing all of that like if I'm buying shoes and the reasons I'm buying that would make

176
00:16:20,400 --> 00:16:26,000
no sense to the shoe seller as long as I like the price you know everything else is fine so

177
00:16:28,160 --> 00:16:32,400
yeah I'm not quite sure what the balance is there it's um because sometimes it feels like you do need

178
00:16:32,400 --> 00:16:36,640
to address things like that and sometimes it feels like it just takes care of itself when the system

179
00:16:36,640 --> 00:16:46,000
is set up right um if you guys are up for it I'd love I made a couple of slides and videos I'd

180
00:16:46,000 --> 00:16:52,960
love to share uh uh I don't know if that would be a fun uh is that okay yeah yeah go for it let's

181
00:16:52,960 --> 00:16:56,480
let's see cool um

182
00:17:07,120 --> 00:17:15,440
okay uh can you see can you see my uh slideshow here yeah we can yeah any chance okay full screen

183
00:17:16,080 --> 00:17:26,640
yeah is that better much better go for it cool um and this is uh two different uh ideas that

184
00:17:27,440 --> 00:17:32,880
kind of work together that I want to get uh both share and get you guys the feedback on

185
00:17:32,880 --> 00:17:39,040
I'll go really fast and then we can drill into something but essentially you know the thing I

186
00:17:39,040 --> 00:17:46,320
I'm really interested in is how do we make an AI agent um and uh like uh to define an agent is

187
00:17:46,320 --> 00:17:51,840
basically something that takes past uh like a sequence of past observations and then produces

188
00:17:51,840 --> 00:17:58,320
the next action so it's just I think of an agent uh I know Mike you think agency has goals uh but

189
00:17:58,320 --> 00:18:04,560
those are externally observable the knowledge about an agent uh so uh I think that's interesting to

190
00:18:04,560 --> 00:18:09,760
figure out so for this uh thing uh like I think of an agent it's just like some function that given a

191
00:18:23,760 --> 00:18:27,760
I think you're breaking up for me yeah me too me too

192
00:18:27,920 --> 00:18:30,960
and

193
00:18:40,160 --> 00:18:42,960
sorry David I don't know if you can hear me but I'm not getting any of this

194
00:18:53,680 --> 00:18:55,600
yeah me too I can't I can't hear what you're saying

195
00:18:58,400 --> 00:19:00,400
good

196
00:19:04,640 --> 00:19:09,040
technology's not a friend today I guess he'll try again yeah I've noticed that usually happens

197
00:19:09,040 --> 00:19:12,960
when something really interesting's about to come out it's because it knows what we're doing

198
00:19:12,960 --> 00:19:16,720
and it doesn't want us to make progress on these questions yeah there's a little bit of that

199
00:19:20,720 --> 00:19:25,040
is that background somewhere you've been uh yeah this is uh years ago this is Alaska

200
00:19:25,840 --> 00:19:30,240
oh nice okay I think I've been to Alaska but many years ago I don't remember much about it

201
00:19:30,240 --> 00:19:34,320
yeah yeah it was pretty it was pretty incredible

202
00:19:36,240 --> 00:19:40,320
this is my blank beige wall because I have that kind of apartment so

203
00:19:42,560 --> 00:19:47,760
yeah cool well yeah hopefully hopefully he'll come back um if not we'll just chat

204
00:19:48,160 --> 00:19:55,680
so actually I do have something I should address with you quickly so regarding the paper we've

205
00:19:55,680 --> 00:20:00,000
discussed I just want to briefly mention to you I don't know a lot about collaborating on papers

206
00:20:00,000 --> 00:20:03,520
and if there's something that you might expect a collaborator to be doing right now while you're

207
00:20:03,520 --> 00:20:08,000
working on other things that might not be obvious to me is there anything specific that I should

208
00:20:08,000 --> 00:20:13,040
be doing right now uh remind me remind me where it stands you have you you've sent me a draft

209
00:20:13,680 --> 00:20:19,600
sent you an outline uh yeah then then it's stuck in my court so I've just been in pain so I will

210
00:20:19,600 --> 00:20:24,240
I will get back to you with the next couple of days with it so yeah yeah and no rush I just

211
00:20:24,240 --> 00:20:27,760
literally wasn't sure if there was something it's not you it's me I've got a I've got a stack of

212
00:20:27,760 --> 00:20:39,280
drafts um on my just because I will get to it yeah okay let's see David is I'll be back

213
00:20:39,280 --> 00:21:01,840
can you hear me hi David hey hey sorry my computer decided not to run zoom anymore so I'm on my phone

214
00:21:02,560 --> 00:21:13,600
um okay let let me try to just do this without the slides but basically the idea is that like an

215
00:21:13,600 --> 00:21:20,720
agent is essentially some function that maps past observations to to the next action and

216
00:21:20,720 --> 00:21:28,400
general intelligence is essentially a set of behaviors of like I think in your paper might

217
00:21:28,400 --> 00:21:33,920
have this thing where it's like how do we navigate a space uh adaptive how do we adaptively

218
00:21:33,920 --> 00:21:39,920
and uh intelligently learn to navigate arbitrary spaces and I think of that as just like a collection

219
00:21:39,920 --> 00:21:45,680
of uh algorithms that let you balance exploration exploitation essentially it's active inference

220
00:21:45,680 --> 00:21:52,240
but active inference requires perfect Bayesian uh inference and really everything just implements a

221
00:21:52,240 --> 00:22:00,160
an approximation so uh the question is like how do you learn or build an approximation of active

222
00:22:00,160 --> 00:22:06,000
inference in various spaces and neural networks are function approximators that instead of building

223
00:22:06,000 --> 00:22:14,480
you can train given the right uh input output pairs so the idea is uh to basically train a

224
00:22:15,200 --> 00:22:22,480
uh a neural network that's a function approximator of uh intelligent space navigation and I think the

225
00:22:22,480 --> 00:22:28,560
way to do that is agents are basically duels of their environment so if you want to train uh an

226
00:22:28,560 --> 00:22:34,160
agent to do something you need to give it an environment where achieving fitness in that

227
00:22:34,160 --> 00:22:42,400
environment uh gives you the the behaviors that you want and so I think if you want an agent that's

228
00:22:42,400 --> 00:22:49,280
able to generalize learning you need an environment where there's always something new to learn uh and

229
00:22:50,080 --> 00:22:55,360
that the behavior space needs to be dense enough that the agent is able to always discover some

230
00:22:55,360 --> 00:23:04,000
new thing that it can learn uh that gives it adaptive fitness to the environment uh and therefore as it

231
00:23:04,000 --> 00:23:10,800
is learning how to uh adapt to that particular environment it is also generalizing learning

232
00:23:10,800 --> 00:23:19,280
it is meta learning learning so uh the idea is if you can give an agent uh always some new

233
00:23:19,280 --> 00:23:23,520
environment where there's something new to learn it's going to learn that and it's also in the

234
00:23:23,520 --> 00:23:27,840
process of doing that it's going to learn how to learn it's going to learn how do I balance

235
00:23:27,840 --> 00:23:33,920
exploration exploitation what algorithms what sampling algorithms can I apply to this new space

236
00:23:33,920 --> 00:23:41,520
how do I uh leave myself and grams uh uh like how do I store memory how do I structure memory

237
00:23:41,520 --> 00:23:47,920
how do I interpret memory uh in these various ways and so uh what you want is an environment

238
00:23:47,920 --> 00:23:53,840
that essentially gets harder or different as the agent gets smarter and I think the way to do that

239
00:23:53,840 --> 00:23:58,240
is make the environment and I think the way evolution did this is make the environment

240
00:23:58,240 --> 00:24:05,520
highly multi-agent so that uh what you're like there are there is complexity in the physics of

241
00:24:05,520 --> 00:24:10,560
the environment sure but really most of the complexity is in the minds of the other learners

242
00:24:10,560 --> 00:24:16,160
and this kind of goes back to economics and capitalism uh where uh you know markets are

243
00:24:16,160 --> 00:24:21,040
kind of anti legible as soon as you find out some trick that gives you an advantage in the

244
00:24:21,040 --> 00:24:26,320
market that advantage goes away because it gets arbitrage the way by everyone else that learns it

245
00:24:27,040 --> 00:24:34,160
and so I think you kind of had this with multi-agent setups you kind of can get into this infinite

246
00:24:34,160 --> 00:24:41,200
game where as the minds of agents get more complex you have to get more complex to

247
00:24:42,480 --> 00:24:47,040
be fit in that environment and this this is kind of just like creates this intelligent

248
00:24:47,040 --> 00:24:52,960
tread intelligence treadmill and I think this has been done in AI so this is how we get like

249
00:24:52,960 --> 00:24:59,200
go player or chess players it's like they do self-play against themselves and as they get

250
00:24:59,200 --> 00:25:07,440
better the game gets harder and but I think typically this is done in purely adversarial

251
00:25:07,440 --> 00:25:14,960
settings uh where and with an fully adversarial setting you basically cannot explore a lot of

252
00:25:14,960 --> 00:25:22,160
the behavior space because as soon as you deviate from some dominant strategy you get

253
00:25:23,040 --> 00:25:29,760
exploited by all the other players and so it's very easy to just get stuck at local minima

254
00:25:29,760 --> 00:25:36,480
because there's not a lot of ways to deviate from uh local maximum you basically have to

255
00:25:36,480 --> 00:25:41,600
really discover something new really quickly or you just get out competed by people that are doing

256
00:25:41,600 --> 00:25:49,600
the same old thing and I think the fix to that is basically blending this line of what an agent is

257
00:25:50,240 --> 00:25:54,880
in this collective intelligence sense I think one really strong technique for that is kinship

258
00:25:54,880 --> 00:26:01,760
so if you're in a world where other agents are say a lot of the agents around you are your brothers

259
00:26:01,760 --> 00:26:11,360
or your clones or your cousins essentially this whole spectrum of kinship then another way to

260
00:26:11,360 --> 00:26:17,040
think about agency is kind of you know if you have two agents that share the same goal you can

261
00:26:17,040 --> 00:26:23,200
think of them as one agent uh with just like really bad cognitive architecture where instead of just

262
00:26:23,200 --> 00:26:29,920
being it has to like learn how to pass messages to itself so one way to define agency is via uh

263
00:26:29,920 --> 00:26:36,880
essentially goals and kinship is this way of aligning agents on goals so if you have an agent

264
00:26:36,880 --> 00:26:42,720
that 100 shares the goal with another agent it's basically one agent if it's 70 sharing goals with

265
00:26:42,720 --> 00:26:50,720
that agent then you know it's kind of this blended super agent and so the idea is if you have an

266
00:26:50,720 --> 00:26:57,840
environment where all these agents are aligned with each other with various different kinship

267
00:26:57,840 --> 00:27:03,120
relationships you can create this really high dimensional behavior space where you're not

268
00:27:03,120 --> 00:27:08,240
always competing there's a million different ways to cooperate break form coalitions break

269
00:27:08,240 --> 00:27:16,160
coalitions established trust and so the idea is you know can we create a relatively simple

270
00:27:16,160 --> 00:27:25,760
fast to simulate in silico virtual world full of other agents the agents are aligned with each other

271
00:27:25,760 --> 00:27:33,840
in various kinship scenarios and then we scale off neural then we train larger and larger neural

272
00:27:33,840 --> 00:27:43,440
networks to drive those agents and get to LLM size you know billion parameter models that instead of

273
00:27:43,440 --> 00:27:49,440
predicting the next token are predicting the next action that an agent should take given all of the

274
00:27:49,440 --> 00:27:55,520
experience at scene and so the idea would be that if you give an environment like that you're

275
00:27:55,520 --> 00:28:02,240
essentially providing a gradient towards increased intelligence and then you're using standard machine

276
00:28:02,240 --> 00:28:10,560
learning techniques to train a function approximator for that to follow that gradient so that's kind of

277
00:28:10,560 --> 00:28:20,320
the idea behind the like overall environment but then the other thing I wanted to incorporate is

278
00:28:20,320 --> 00:28:25,200
this collective intelligence approach and I really wish I could present my slides I don't know what

279
00:28:25,200 --> 00:28:33,760
happened with my computer maybe I'll pause for a second let other people speak I don't know if

280
00:28:33,760 --> 00:28:39,520
anyone has feedback or thoughts or criticisms and then I'll try to see if I can get this other

281
00:28:39,600 --> 00:28:46,480
slide to work meanwhile yeah thank you for that I mean I have certainly heard a somewhat similar

282
00:28:46,480 --> 00:28:52,560
sounding theory that evolution evolved or intelligence evolved as some sort of you know social competition

283
00:28:52,560 --> 00:28:56,640
that you know primates competing with primates and birds competing with birds is

284
00:28:56,640 --> 00:29:00,080
where it comes from rather than trying to like you know build tools and so on

285
00:29:00,800 --> 00:29:10,880
um so uh yeah the idea about yeah you need a form of like some kind of protection some

286
00:29:10,880 --> 00:29:16,560
ability to innovate I mean there's a lot of aspects of economics where there's this

287
00:29:17,120 --> 00:29:22,640
challenging tension between wanting to optimize and wanting to create room for innovations like

288
00:29:22,640 --> 00:29:27,200
perfect competitions a classic example like you learned very early on perfect for competition

289
00:29:27,200 --> 00:29:31,840
you're going to maximize price maximize output your price is going to be as low as it should be

290
00:29:32,720 --> 00:29:37,920
but it's very hard to innovate in perfect competition because the way the model works is

291
00:29:37,920 --> 00:29:42,320
whatever new technology you introduce everyone copies that instantly you make no profit so but

292
00:29:42,320 --> 00:29:46,880
you're the one who made the investment so that just sucks so a little bit of monopoly can actually

293
00:29:46,880 --> 00:29:50,880
be helpful because that way you're making money and then there's even artificial things like you

294
00:29:50,880 --> 00:29:55,600
know like patents and stuff that give people monopolies to try to incentivize them to

295
00:29:56,320 --> 00:30:02,160
make those investments and innovate so that is a very important uh aspect of intention of any

296
00:30:02,160 --> 00:30:07,200
sort of collective intelligence system is trying to encourage parts of your system to improve and

297
00:30:07,200 --> 00:30:10,960
do better and trying to protect them from parts of the system might say oh we're going to copy

298
00:30:10,960 --> 00:30:17,440
that we're going to take that we're going to steal that um there's a part of this that

299
00:30:18,880 --> 00:30:24,320
maybe I know you're trying to get your slides up but um I saw in your talk you talk a little bit

300
00:30:24,320 --> 00:30:30,320
about like love and stuff at the end of that when you have you have a talk on YouTube and that's

301
00:30:30,320 --> 00:30:38,160
something that I had to bring up because I am working on a couple of papers about moral psychology

302
00:30:38,160 --> 00:30:44,720
and neuroscience including a paper on morality as a form of cognitive glue which it isn't really

303
00:30:44,720 --> 00:30:51,120
but it's a sort of an interesting counter example in some ways and um basically

304
00:30:51,920 --> 00:31:00,400
there's a model in which um kinship let me ask you this if um I suppose that you took all the

305
00:31:00,400 --> 00:31:05,040
families and you rearranged them like people didn't know that they were in the wrong families but

306
00:31:05,040 --> 00:31:10,240
you had families where no one's actually related to each other genetically uh would the system still

307
00:31:10,240 --> 00:31:22,800
work? Yep. Yeah so um I think of this as like I think of this as essentially reward sharing so on

308
00:31:22,800 --> 00:31:30,320
a genetic level uh the the genetic optimization process uh gives rise to organisms that care about

309
00:31:30,320 --> 00:31:36,000
their kin right so because a gene is present in the kin that it's going to make the organisms that

310
00:31:36,000 --> 00:31:41,200
come from it want to help other organisms that have that gene from the gene's perspective it

311
00:31:41,200 --> 00:31:50,080
doesn't matter whether you reproduce or your brother reproduces uh twice uh so uh and similar

312
00:31:50,080 --> 00:32:00,560
so essentially the the behavior that an organism exhibits that we call love is uh basically what

313
00:32:00,640 --> 00:32:07,120
it looks like when the underlying optimization process shares rewards or goal or like uh

314
00:32:08,240 --> 00:32:15,040
shares rewards between organisms but then the organism itself one of the things that it evolves

315
00:32:15,040 --> 00:32:22,960
or learns or whatever needs is the is some way to recognize who it should care about so we have all

316
00:32:22,960 --> 00:32:32,000
of these heuristics for knowing who our kin are um and uh once those things are in place uh it's the

317
00:32:32,000 --> 00:32:40,560
heuristics that matter not the underlying gene so uh I think if you train agents that see other

318
00:32:40,560 --> 00:32:45,840
agents and you give them a reliable signal that hey helping this agent is actually the same as

319
00:32:45,840 --> 00:32:51,360
helping you so every time this agent something good happens to this agent you get part of that reward

320
00:32:51,360 --> 00:32:57,680
and here is a marker that tells you who is kin and who is not the agent is going to learn behaviors

321
00:32:57,680 --> 00:33:03,360
that are conditioned on that marker and then once you stop training and you just let these agents

322
00:33:03,360 --> 00:33:08,560
go around behaving you can change that marker arbitrarily rewards are no longer even there

323
00:33:08,560 --> 00:33:15,200
right genetic evolution or whatever has stopped now you're no longer learning new policy now the

324
00:33:15,200 --> 00:33:20,720
learning is happening inside the mind of the organism not inside the the genes

325
00:33:26,880 --> 00:33:29,280
conditioned behavior on kinship markers

326
00:33:31,520 --> 00:33:38,000
does that did that make sense it does and there's a very important paper that anyone interested

327
00:33:38,000 --> 00:33:43,520
in this should read it's called the sense of should a biologically based framework for modeling

328
00:33:43,520 --> 00:33:50,960
social pressure and in some ways maybe the difference is subtle but I think there's a way

329
00:33:50,960 --> 00:33:59,280
of looking at these sorts of morally motivated cooperative behaviors um that does not in fact

330
00:33:59,280 --> 00:34:06,240
depend on any kind of evolved pressure to care for or intend to cooperate but instead it's just like

331
00:34:08,000 --> 00:34:12,400
something your brain just constructs as a living organism in its environment

332
00:34:12,400 --> 00:34:19,760
because it's trying to make it's in its social environment predictable and and so there is

333
00:34:19,760 --> 00:34:25,040
something of emerging perspective and my papers are somewhat drawing on this research that's

334
00:34:25,040 --> 00:34:31,200
basically saying let's not think about evolution let's not think about long-term cooperation let's

335
00:34:31,200 --> 00:34:35,680
just think about trying to make the social environment predictable and that's how we can

336
00:34:35,680 --> 00:34:42,160
actually get um these cooperative and moral behaviors and so that just might be something

337
00:34:42,160 --> 00:34:46,160
to look at because that's another paper released as a co-author and I just bring up Lisa's stuff as

338
00:34:46,160 --> 00:34:56,640
much as possible I mean this is also yeah I mean I go ahead Michael sorry I was just going to say

339
00:34:56,640 --> 00:35:02,480
that that also sounds like um uh the the uh imperial model of multicellularity that Chris

340
00:35:02,480 --> 00:35:08,000
feels and I published a bunch of years ago which is basically that you know if cells are trying to

341
00:35:08,000 --> 00:35:12,720
predict an uncertain environment the least uncertain the least surprising thing around is a copy of

342
00:35:12,720 --> 00:35:19,120
yourself and so this this is a this can be a driver for uh making cells stick around after

343
00:35:19,120 --> 00:35:26,080
you've divided to to form a you know a predictive of a highly predictive um um uh surrounding for

344
00:35:26,080 --> 00:35:29,840
yourself so that you live in this niche and then the frontline infantry is out there facing the

345
00:35:30,160 --> 00:35:34,160
uncertainties of the outside world but but you can predict them because they're you and so it's a

346
00:35:34,160 --> 00:35:39,200
lot easier that way right so so you can you can think about it that way as well have I talked to

347
00:35:39,200 --> 00:35:53,760
you about comparative advantage uh yeah no I don't think I want to go ahead David yeah sorry I had a

348
00:35:53,760 --> 00:36:02,720
question about that Michael uh so uh I don't uh so if you have a cell uh and the cell does uh you

349
00:36:02,720 --> 00:36:10,320
know given situation a it does random stuff then being surrounded by copies of those cells

350
00:36:10,320 --> 00:36:16,720
and you do random stuff and your clones do random stuff being surrounded by cells that in that

351
00:36:16,720 --> 00:36:22,960
situation do random stuff is not actually going to help you predict uh the environment

352
00:36:24,720 --> 00:36:29,520
so I think there's something more to that it's not that being surrounded by copies of yourself

353
00:36:29,520 --> 00:36:35,360
gives you the ability to predict your environment I think it's being surrounded by copies of cells

354
00:36:35,360 --> 00:36:41,280
that want to make your environment that want to help you make the environment more predictable

355
00:36:42,320 --> 00:36:49,520
that actually uh matters uh right unless you think that a cell by being able to

356
00:36:49,520 --> 00:36:57,120
introspect itself can predict the behavior of other copies of itself I mean I I don't think

357
00:36:57,120 --> 00:37:02,160
they're likely to be doing random stuff they're probably situations in which the responsiveness

358
00:37:02,160 --> 00:37:08,000
is kind of random but but I don't think that's the vast majority of what cells do and so I do

359
00:37:08,000 --> 00:37:16,800
think that if if if you're um it's it's easier to uh to anticipate cycles and responses I mean

360
00:37:16,800 --> 00:37:20,160
again I mean I think you're right if it's random it's not going to work but I actually don't think

361
00:37:20,160 --> 00:37:26,400
that cells are random in that in that way but let's um anyway let's let's let David do his thing

362
00:37:28,160 --> 00:37:35,680
oh okay yeah this is just uh so this is uh this this is actually mostly inspired by your work Michael

363
00:37:35,680 --> 00:37:41,760
and economics so I'd love to get thoughts on this but okay so this is the standard

364
00:37:41,760 --> 00:37:47,360
a reinforcement learning uh formulation right you have an environment you have an agent uh

365
00:37:47,360 --> 00:37:52,880
the agent takes an action in the environment it gets back some observation and a reward and then

366
00:37:53,520 --> 00:37:59,920
it learns to maximum to act in a way that maximizes reward and uh you can have a dual formulation

367
00:37:59,920 --> 00:38:06,000
this is which is the inference where the agent is trying to predict observations but essentially

368
00:38:06,000 --> 00:38:13,040
this is the the normal kind of loop and typically the way it's uh we can break this down into like

369
00:38:13,040 --> 00:38:20,160
an agent basically there's a bunch of observation states from the environment that go into the mind

370
00:38:20,160 --> 00:38:27,120
the mind is going to generate a bunch of actions over actuators uh there's some memory that maybe

371
00:38:27,120 --> 00:38:33,040
the mind has that it's reading and writing from uh and it's getting a reward signal so this is how

372
00:38:33,040 --> 00:38:39,520
we train agents in reinforcement learning or this could be trained or this could be evolved

373
00:38:40,720 --> 00:38:48,160
doesn't really matter and so the way we do this now in the field of RL is you have this

374
00:38:48,160 --> 00:38:53,040
giant neural net inside the mind and there's model 3 and model RL so they're like this is

375
00:38:53,040 --> 00:38:59,520
skipping over some detail but essentially you're training this big blob of math that you're jiggling

376
00:38:59,520 --> 00:39:09,200
using gradient descent to optimize your long-term expected reward and it is learning how to compress

377
00:39:09,200 --> 00:39:16,960
and basically how to compress the observations into latent states maybe store the parts of those

378
00:39:16,960 --> 00:39:22,880
latent states in its memory that then it can reinterpret and eventually to generate actions

379
00:39:22,880 --> 00:39:32,320
and so these networks can be giant in language models there are 100 billions of parameters in

380
00:39:32,320 --> 00:39:35,680
reinforcement learning they're much smaller because we just haven't figured out how to

381
00:39:35,680 --> 00:39:44,480
do it yet but this is kind of the standard setup and what I am proposing is a different architecture

382
00:39:45,360 --> 00:39:54,640
that is essentially instead of there being one mind there are many individual agents

383
00:39:55,680 --> 00:40:05,360
each agent is mapped onto some subset of the observation space and some subset of the memory

384
00:40:05,360 --> 00:40:11,520
space and some subset of the action space and you know they can be mapped to any or all of them

385
00:40:11,520 --> 00:40:18,640
but essentially rather than training one brain I want to train a neuron that in a collection

386
00:40:19,280 --> 00:40:27,520
when you put it together in a graph of other neurons knows how to co-organize collectively

387
00:40:27,520 --> 00:40:35,200
to solve the overall problem that the brain has so rather than training 100 million model

388
00:40:35,200 --> 00:40:43,040
network I want to train one million parameter network that when you copy and paste it and

389
00:40:43,760 --> 00:40:52,720
throw it into this soup of other neurons will self-organize into solving the problem

390
00:40:52,720 --> 00:41:00,240
and so the idea is rather than having to learn adaptive algorithms so in in this way you know

391
00:41:00,240 --> 00:41:05,680
you can imagine that there's an algorithm that you apply to many different parts of the sub problem

392
00:41:06,240 --> 00:41:12,880
but this network has to learn how to has to learn that algorithm in a bunch of different places

393
00:41:12,880 --> 00:41:18,320
because every sub function call might have the same kind of you know oh this is just like a

394
00:41:18,320 --> 00:41:22,960
regression problem but you have to learn how to solve regression in a bunch of different places

395
00:41:22,960 --> 00:41:30,320
whereas here this thing can just learn okay you know here is how I divide my problem into sub

396
00:41:30,320 --> 00:41:38,160
problems and send those messages out or here is how I you know pay attention to a pattern

397
00:41:38,160 --> 00:41:46,080
happening at this time scale and that's my job so the idea is to break down this one big blob of

398
00:41:46,160 --> 00:41:54,560
compute into something that can be done on this graph instead and I think and this is

399
00:41:54,560 --> 00:41:59,760
why I wanted to talk to you then because I think basically I started out thinking well this is an

400
00:41:59,760 --> 00:42:06,400
economy you know if these things were you know if the observations and memory were commodities and

401
00:42:06,400 --> 00:42:12,160
these things were like buying and selling those commodities or something like that that would give

402
00:42:12,160 --> 00:42:18,960
us so the question is you know the the inside of a neural cell is a neural network that we train

403
00:42:19,520 --> 00:42:24,320
but then the question is what is the cognitive glue between all of these neural cells essentially

404
00:42:24,320 --> 00:42:30,240
what is the protocol that they use such that when we train train the neural cell

405
00:42:32,080 --> 00:42:37,760
on the overall objective they'll learn something useful and the thing I came up with

406
00:42:38,480 --> 00:42:45,120
kind of influenced by active inference and collective intelligence is so now this is inside

407
00:42:45,120 --> 00:42:50,560
a single neural cell right now now this is an agent that just sees some set of inputs

408
00:42:51,760 --> 00:42:59,920
it has its own memory it's going to get some reward signal from the the cognitive glue

409
00:42:59,920 --> 00:43:05,840
and it's going to produce some outputs and I think the way to do that is that basically

410
00:43:06,240 --> 00:43:13,520
it it should be predicting its inputs active inference essentially learning a world model

411
00:43:13,520 --> 00:43:19,760
of its little world inside the brain and then voting on the outputs

412
00:43:22,400 --> 00:43:29,440
so that it can and in this case outputs can be inputs to another neural cell or just outputs to

413
00:43:29,440 --> 00:43:37,520
an actuator that moves the organism around so I think essentially I'm proposing some kind of a

414
00:43:38,240 --> 00:43:44,560
an agent who's that you know receives a set of inputs so this is some subset

415
00:43:46,400 --> 00:43:51,600
and it has some memory and then what it's going to do is it's going to predict the output at the

416
00:43:51,600 --> 00:44:02,320
next time step along with some bet so it has energy and it has finite energy it's going to

417
00:44:02,320 --> 00:44:08,720
bet on what the values of that inputs are going to be at the next time step it's going to vote on

418
00:44:08,720 --> 00:44:14,800
what it wants to set the outputs to also using this energy and then at the next time step it's

419
00:44:14,800 --> 00:44:22,320
going to see well what were the actual inputs and which bets did it win and then it gets energy

420
00:44:22,320 --> 00:44:30,320
in proportion to essentially gets energy back as a reward so the idea here is that these agents

421
00:44:30,320 --> 00:44:38,640
are incentivized to only predict the inputs that they have a comparative advantage on

422
00:44:38,640 --> 00:44:42,720
over other agents so the same input and output is mapped to multiple cells

423
00:44:42,720 --> 00:44:48,640
so as an agent you should learn to only to basically monitor your environment form models

424
00:44:48,640 --> 00:44:55,440
about it and then bet on the things where you have a comparative advantage to other cells

425
00:44:56,240 --> 00:45:04,720
and then as you get more and more capabilities to model your environment you get more and more

426
00:45:04,720 --> 00:45:12,240
energy that you can use to vote on steering around that environment this is all hand wavy and this

427
00:45:12,240 --> 00:45:18,160
is where I think I need help and help from economics because I started out with pricing then

428
00:45:18,160 --> 00:45:25,280
I had it actually be an auction or your or prediction market add an auction and then I

429
00:45:25,280 --> 00:45:30,960
try to simplify it just down to just like straight betting markets but that's basically the idea is

430
00:45:30,960 --> 00:45:38,080
can we come up with this like simple cognitive glue for a pool of these agents and then train

431
00:45:38,080 --> 00:45:46,720
those agents I'll stop now yeah no that's very interesting I think it's definitely an economy

432
00:45:46,720 --> 00:45:52,640
and you definitely should talk to economists I'm afraid I think a practical issue is I'm just not

433
00:45:52,640 --> 00:45:58,480
sure economists have actually done a lot to explore this space even in theory for the most part they've

434
00:45:58,480 --> 00:46:04,560
been content to just take the economy as it is and try to explain that and there's been I think

435
00:46:04,560 --> 00:46:09,200
just kind of a certain lack of creativity there in terms of envisioning other economic spaces

436
00:46:09,200 --> 00:46:15,440
and trying to think about building alternative economic systems like this one so I'm afraid

437
00:46:15,440 --> 00:46:21,360
I myself don't have a lot of practical things to say right now that might actually be of any

438
00:46:21,360 --> 00:46:25,760
specific use to you other than I think just affirmation that you're right that this is economics

439
00:46:25,760 --> 00:46:31,840
and that talking to economists is a good idea I think the mechanism design people are the place

440
00:46:31,920 --> 00:46:36,480
to start I mean I think that's where Glenn Wilde comes from but I gotta know if like

441
00:46:36,480 --> 00:46:40,320
Eric Maskin is reachable at all but like he won a Nobel Prize for that and I had a

442
00:46:40,320 --> 00:46:44,720
two-minute conversation with him many years ago it was very influential so you might just want to

443
00:46:44,720 --> 00:46:51,680
see if you could send him an email and see what he says yeah I think this is cool I'm afraid I don't

444
00:46:51,680 --> 00:46:56,640
think I can help you today with it but we should stay in touch and I'll let you know if if anything

445
00:46:56,640 --> 00:47:00,960
comes along because this is very interesting and very important

