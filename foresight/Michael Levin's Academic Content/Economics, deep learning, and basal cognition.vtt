WEBVTT

00:00.000 --> 00:01.680
I appreciate that. Yeah, I'm really looking forward to this.

00:01.680 --> 00:06.120
I think you guys both had some great ideas recently, and I'm looking forward to it.

00:06.280 --> 00:10.080
Can we just start and maybe, let's see, Ben can go first and then David,

00:10.080 --> 00:13.360
just say a couple of words about who you are and what your interests are.

00:14.400 --> 00:16.800
I'm a former stockbroker with the background in economics,

00:16.800 --> 00:19.760
and I'm currently studying the economics of collective intelligence,

00:19.760 --> 00:22.560
looking to put together an understanding of how people get along

00:22.560 --> 00:24.400
and accomplish things that no individual can.

00:25.280 --> 00:32.320
Cool. I am by training a stockbroker engineer, but I've been into AI

00:32.320 --> 00:35.200
ever since the high school reading age of intelligent machines,

00:36.160 --> 00:40.080
and then just kept dipping in and out of the AI field and kind of being like,

00:40.080 --> 00:44.960
it's not ready, it's not ready. After college, I co-founded one of the first AGI startups

00:44.960 --> 00:48.640
with like Shane Legg, one of my co-founders was our first employee.

00:48.640 --> 00:53.760
So like back in 2000s, and then went to work at Google and Facebook,

00:53.760 --> 00:58.960
and then Asanda, mostly either working on ML, but most of my time has been building

00:58.960 --> 01:02.720
like large-scale distributed systems. So really coming at this more from

01:02.720 --> 01:08.880
an engineering background, but also about three years ago, I left my job and

01:08.880 --> 01:13.360
dove back into AI and came across Carl Friston's work.

01:13.360 --> 01:15.600
And I think that's when it kind of all clicked for me.

01:15.600 --> 01:20.800
And I was like, wow, we have GPUs now, we have a theory of intelligence

01:20.800 --> 01:27.200
that I feel like basically covers. It's like, I don't think there's any more secret sauce.

01:27.200 --> 01:31.920
It's like, we kind of, the formulas from active inference look a lot like formulas

01:31.920 --> 01:37.680
from traditional reinforcement learning. We have the hardware now, so I just like

01:37.680 --> 01:40.800
got really excited about it and have been working on it.

01:40.800 --> 01:46.720
So that's like my software and engineering side. I also co-founded Plurality Institute

01:46.720 --> 01:52.800
with Glenn Wild, who's like an economist, and that's more on the human collective intelligence side.

01:53.520 --> 01:58.880
Essentially, we're trying to create a new field, academic field called Plurality Studies that

01:58.880 --> 02:03.520
focuses on human, scaling human cooperation and collaboration, essentially collective

02:03.520 --> 02:08.720
intelligence as an academic field. And we've been bringing together researchers from like

02:08.720 --> 02:15.280
political science, computer science, economics, peace building. I mean, it's such an interdisciplinary

02:15.280 --> 02:21.680
field of like, how do we help humans cooperate and using technology, and now especially AI to

02:22.960 --> 02:29.280
bridge people. So that's kind of my other project. But my main research focus is,

02:29.840 --> 02:36.960
and I can share a little bit of like the research that I'm working on. And I would love, you know,

02:36.960 --> 02:41.840
help and collaboration. And also would love to hear how you guys are thinking about this. And if I

02:41.840 --> 02:47.040
can help, because I think this is like the most exciting thing that's happening in the world right

02:47.040 --> 02:54.640
now. Cool. Yeah, please go for it. Yeah. Yes. So I will say yes. So Glenn Wild is an interesting name

02:54.640 --> 03:00.240
because I think he's got a very important idea, which is that we should be using markets to

03:00.240 --> 03:04.080
solve more problems because there's basically a perspective in economics that says,

03:04.960 --> 03:09.120
anytime you've got a problem, there's a market that can solve that problem. And people just

03:09.120 --> 03:15.200
haven't been creative enough historically about what kind of markets you can use. Like we're just

03:15.200 --> 03:21.040
very used to like trading like standard property rights and things. And there's a lot of other

03:21.040 --> 03:25.440
ways you could conceive of doing things. I like the fact that Glenn Wild is pushing that I'd be

03:25.440 --> 03:29.360
interested in. So I don't actually know what he's doing. So actually, I'm just curious about that.

03:29.360 --> 03:32.720
I don't actually know what he does with AI. So can you just tell me a little bit about what he does

03:32.720 --> 03:38.720
with the Plurality Institute? Yeah, well, the Plurality Institute is not is more of the academic

03:38.720 --> 03:44.640
like collective and human collective intelligence of which AI is like a tool. So for example,

03:45.680 --> 03:52.080
and I mean markets are one collective intelligence system. But then there are other ones like even

03:52.640 --> 03:57.920
like common sections on forums are collective intelligence mechanism where humans come together

03:57.920 --> 04:02.960
to synchronize on epistemology like on things that they know and believe and being able to use

04:02.960 --> 04:10.000
LLMs to say automatically deescalate flame wars in common sections and drag people to consensus

04:10.000 --> 04:16.320
is like an example. In terms of the more economics things, so using quadratic funding for like public

04:16.320 --> 04:23.200
goods funding is something he has been writing about creating identity like trust networks to

04:23.200 --> 04:29.520
create identity like social identity so that you can use that as infrastructure for all these other

04:30.800 --> 04:38.400
like market mechanisms is another one. There's someone that's working on vector prices. So rather

04:38.400 --> 04:44.560
than there being a single price signal, there's a whole basket collection of currencies that you

04:44.560 --> 04:50.320
hold and that people can specify their own values for different currencies. And then the way the

04:50.320 --> 04:55.760
trading works is basically when you're trading with people that are aligned with you on values,

04:55.760 --> 05:02.320
you automatically get discounts because they value your currencies more than they value some.

05:02.320 --> 05:08.720
So there's like a whole bunch of them. He just has a book out open source isn't like another like

05:08.720 --> 05:14.160
how large open source projects are incentivized and how collaborations occur without like a

05:14.160 --> 05:20.560
corporate structure. He has a book out now called plurality with Audrey Tong who is another

05:21.200 --> 05:28.080
inspiring person. So I would check that out and that's like is going to cover a lot of different

05:28.080 --> 05:34.160
aspects of collect like plurality and collective intelligence. Cool. Is a lot of that just on

05:34.160 --> 05:40.000
the plurality website? The plurality instant that instituted website, we like post events and talks

05:40.000 --> 05:44.400
and we have the link to the plurality book by the plurality book is like much more detailed.

05:45.520 --> 05:51.680
I will check that out. So maybe what I can do then is just kind of give a broad overview of what I

05:51.680 --> 05:55.680
think the basics of economics of collective intelligence are and we can see how that relates

05:55.680 --> 06:02.400
to our work. So the way I see it is that collective intelligence is all about getting people to

06:02.400 --> 06:09.600
communicate in honest ways in honest and relevant ways such that everybody forms a shared model

06:09.680 --> 06:13.680
and because everyone's behaving with that same shared model everyone ends up behaving as if

06:13.680 --> 06:18.480
they're all following the same you know sort of commands if you will like a dictator or a virtual

06:18.480 --> 06:24.000
governor and everyone ends up nicely coordinated to achieve that kind of signaling system. You need

06:24.000 --> 06:30.320
something that's analogous to a price system and what that allows you to do is it's principles

06:30.320 --> 06:36.720
very similar to what Mike has talked about in some of his work where you get a way of getting an

06:36.720 --> 06:42.400
entity to see questions about other people as questions about itself and so like information

06:42.400 --> 06:46.240
gets transmitted in such a way where like when someone else is in trouble you perceive that as

06:46.240 --> 06:51.120
stress to yourself when something benefits someone else you perceive that as benefit to yourself

06:51.840 --> 06:56.160
which we see very easily in the price system like if someone needs more food they'll you know have

06:56.160 --> 07:00.800
a demand for food that'll raise the price of food so that stress gets transmitted to everyone else

07:00.800 --> 07:05.360
which uses to buy less food or similarly if you help someone get food by supplying some food that

07:05.360 --> 07:11.600
they like their benefit becomes your benefit via the revenue you make some money so that very

07:11.600 --> 07:17.840
simple system ends up outlining really just the basic functionality of a collective intelligence

07:17.840 --> 07:22.160
I think that any collective intelligence probably follows basic price like principles

07:22.160 --> 07:26.320
where you need some system where people aren't incentivized to communicate honestly with each

07:26.320 --> 07:33.120
other and such that everyone's signals gets condensed into like your own personal situation

07:33.120 --> 07:38.000
so just by optimizing for yourself you end up inadvertently taking care of having one else

07:38.000 --> 07:42.480
so are things like that when you're looking at like AI and technology and stuff like I don't

07:42.480 --> 07:47.200
know what goes on inside a computer it's all magic to me is that is that what's going on with like the

07:47.200 --> 07:52.480
transistors do they all have their own like individual model and they end up like communicating to

07:52.480 --> 08:01.520
form a shared model you know I mean I guess it's not level everything is doing the longest right

08:01.520 --> 08:07.920
everything is like mutually reducing free energy and you can feed every part of the

08:07.920 --> 08:13.840
subsystem as an agent so in some in some level yes but I think what you probably mean is more

08:13.840 --> 08:23.040
directly in a computational way and like typically what we do now is we train neural networks which

08:23.040 --> 08:31.600
are a big pile of math but but essentially it's a thing that like learns function approximations

08:32.240 --> 08:36.880
and the way it learns function approximations internally and we're still studying you know

08:36.880 --> 08:44.000
what actually happens internally but probably yes things get partitioned into sub processes

08:44.000 --> 08:49.760
like sub functions with sparse or communication between them so kind of what you're described

08:50.160 --> 08:56.240
if you think like in your model an individual is this like very interconnected system and

08:56.240 --> 09:02.000
then it has these sparse connections to other deeply interconnected systems and this is like

09:02.000 --> 09:06.000
you know a cell is doing a lot of compute inside but then it's got just like a few

09:07.040 --> 09:12.480
membrane exchanges with outside or corporation there's a lot of stuff going on inside but it

09:12.480 --> 09:17.840
communicates with the rest of the economy the buying and selling products so in some sense anything

09:17.840 --> 09:24.960
that you partition into subsystems is essentially doing this it is using some form of information

09:26.240 --> 09:32.400
like sparse information propagation between units in terms of honesty I think that's a lot

09:32.400 --> 09:38.560
trickier because it's like the communication doesn't need to be honest it needs to be valuable

09:39.120 --> 09:45.680
to like I think there is like a theorem that like for communication to exist it has to benefit

09:45.680 --> 09:51.520
both parties and so there has to be some value to it and you can maybe say that that value is the

09:51.520 --> 09:58.320
truthiness and so you could like maybe decouple it into like signal and noise so there's like some

09:58.320 --> 10:03.280
truth value and then maybe there's like a bunch of a noise value if it's all noise that communication

10:03.280 --> 10:09.440
channel is just not going to persist and so in terms of I think when you're like hey there should

10:09.440 --> 10:16.720
be honest pricing mechanism I would instead say there should be the two agents or whatever the

10:16.720 --> 10:22.000
network of agents there needs to be ways for them to communicate usefully with each other which it

10:22.000 --> 10:27.280
may be is the same thing but they kind of co-discover what that is and that doesn't necessarily need to

10:27.280 --> 10:34.640
be imposed by a mechanism but if a market mechanism or some communication framework

10:34.640 --> 10:40.400
allows honest communication channels then you're making it much easier for them to discover how

10:40.400 --> 10:48.240
to use it. I don't know if that if any of that resonated. It does and maybe there's something

10:48.240 --> 10:53.040
interesting to work out here so economics has been talking about the importance of getting

10:53.040 --> 10:58.480
people to communicate honestly to achieve coordination for a long time and sort of the

10:58.480 --> 11:02.880
basic picture of that which is very silly and false in some ways but also maybe important for

11:03.200 --> 11:08.640
intuition is if you're trying to centrally plan an economy one conceivable way you do that is

11:08.640 --> 11:12.240
you know you have an allocation problem who should get what goods you can just send everyone

11:12.240 --> 11:17.120
like a giant questionnaire and say like do you need a new pair of shoes etc and the problem with

11:17.120 --> 11:22.320
that is that people could lie and they say yeah I totally need a new pair of shoes and so there's

11:22.320 --> 11:29.280
a lot about that picture that's very unrealistic but figuring out ways to get people to honestly

11:29.280 --> 11:34.480
is maybe a word that has some some bad connotations because we interpret in terms of like deliberate

11:34.480 --> 11:40.960
intention and like moral character it's really more just about like truthfully conveying your

11:40.960 --> 11:45.920
intentions so you so basically maybe this is a way of thinking about it you can sort of think of

11:45.920 --> 11:50.720
economics as a very complicated version of like a traffic you're just trying to get cars to not

11:50.720 --> 11:56.240
crash into each other and that means that each car needs to predict where each other car is going

11:56.240 --> 11:59.680
that means each car needs to be sending very clear signals I'm definitely going here and I'm not

11:59.680 --> 12:04.080
going to like turn suddenly or something like that so you're trying to get people to convey what

12:04.080 --> 12:08.560
their plans are so that everyone knows what everyone else is planning and so then they can pick a

12:08.560 --> 12:17.680
plan that's consistent with all those other plans and I guess this is valuable it's like much more

12:17.680 --> 12:23.760
of like I guess an information theory kind of take I think there's I think there's another

12:23.760 --> 12:31.840
thing that that is missing from that model which is that if you have like a really narrow freeway

12:31.840 --> 12:34.880
then some cars are going to get through and some cars are going to have to wait

12:35.760 --> 12:42.000
and there's not an easy way to resolve which ones so I think like and this is kind of back to the

12:42.000 --> 12:47.840
plurality way that I've been thinking about it is like these are all forms of alignment and I break

12:47.840 --> 12:53.200
alignment down into like aligning on action which is kind of what economics in some sense lets you

12:53.200 --> 12:58.880
do is like okay we all have common beliefs let's given our beliefs what should we do and how can we

12:58.880 --> 13:06.480
coordinate our action how can we all send our like our routes so that we don't like collide

13:06.480 --> 13:11.920
so that's kind of aligning on action there's aligning on epistemologies like how do we all come to

13:11.920 --> 13:18.720
believe the same world models and then there's aligning on value which is how do we actually

13:18.720 --> 13:23.040
like I want one thing you want a different thing they maybe maybe we want opposite things how do

13:23.040 --> 13:30.160
we align on that and all of these these three systems are all interdependent because if you

13:30.160 --> 13:35.360
actually want different things you're not going to necessarily even align on epistemology because

13:35.360 --> 13:40.640
the things you know only things that you care about enough to know and if you care about different

13:40.640 --> 13:45.040
things you're not even going to necessarily ask the same questions of the universe to get back

13:45.040 --> 13:48.960
the same knowledge and if you have different beliefs you're not going to align on action

13:49.760 --> 13:56.400
and they all have to be iterated so like if you can agree on the same epistemology then maybe you

13:56.400 --> 14:02.560
can shift both shift your values because what you value depends on what you know and so in some

14:02.560 --> 14:08.640
sense it's like you have to pull all of these three systems in and I think try and what that's

14:08.640 --> 14:13.680
another cool thing about economics is it also does help value alignment and knowledge alignment

14:13.680 --> 14:19.360
because prices are knowledge signals so they tell you know what goods are easier to get when

14:19.360 --> 14:25.360
and they're also value alignment mechanisms because they allow you to trade something that you value

14:25.360 --> 14:30.240
more for something that you value less so I think there are these pieces but they all have to kind

14:30.240 --> 14:37.280
of play together so I want to go in a couple of different directions with that and maybe the first

14:37.280 --> 14:41.840
thing I'll mention is actually pulling away from economics a bit and talk about some neuroscience

14:41.840 --> 14:46.880
so on a different comment I left on Mike's blog post I talk about the work of Lisa Feldman Barrett

14:47.600 --> 14:52.240
who's done super important stuff it's actually her work that really prepared me to understand

14:52.240 --> 14:57.920
what Mike's talking about because I don't know anything about biology and the active inference

14:58.640 --> 15:03.680
sort of predictive coding view of things that she's really advancing as a way of understanding

15:03.680 --> 15:10.480
what the brain does you know sort of we we do often naturally draw these divides between

15:11.120 --> 15:17.280
knowledge acting and valuing and the active inference way of viewing things very much pushes

15:17.280 --> 15:22.480
all of that together it's actually more like action that drives perception and really your

15:22.480 --> 15:30.160
goals are at the base of all of that but also your goals are determined by you know I think of

15:30.160 --> 15:35.200
goals as basically like beliefs or measurements that you have like your goals are in a sense your

15:35.200 --> 15:41.280
epistemology because your goals are your measurements about the world and your prior expectations

15:41.280 --> 15:44.960
it's a weird sounding view and maybe we can talk about that but um that's kind of how I think about

15:44.960 --> 15:52.080
that and so I would you know it's very useful sometimes to separate these things out because

15:52.960 --> 15:57.440
there's a reason we draw these categories we categorize because there are practical reasons

15:57.440 --> 16:03.760
to do so and so that can be useful but also it's I want a fully unified perspective where we're

16:03.760 --> 16:07.840
just treating all of that as kind of the same thing in some level and in economics at the end

16:07.840 --> 16:14.320
of the day when you're trading something you can disagree very strongly about you know like the trade

16:14.320 --> 16:20.400
ends up compressing all of that like if I'm buying shoes and the reasons I'm buying that would make

16:20.400 --> 16:26.000
no sense to the shoe seller as long as I like the price you know everything else is fine so

16:28.160 --> 16:32.400
yeah I'm not quite sure what the balance is there it's um because sometimes it feels like you do need

16:32.400 --> 16:36.640
to address things like that and sometimes it feels like it just takes care of itself when the system

16:36.640 --> 16:46.000
is set up right um if you guys are up for it I'd love I made a couple of slides and videos I'd

16:46.000 --> 16:52.960
love to share uh uh I don't know if that would be a fun uh is that okay yeah yeah go for it let's

16:52.960 --> 16:56.480
let's see cool um

17:07.120 --> 17:15.440
okay uh can you see can you see my uh slideshow here yeah we can yeah any chance okay full screen

17:16.080 --> 17:26.640
yeah is that better much better go for it cool um and this is uh two different uh ideas that

17:27.440 --> 17:32.880
kind of work together that I want to get uh both share and get you guys the feedback on

17:32.880 --> 17:39.040
I'll go really fast and then we can drill into something but essentially you know the thing I

17:39.040 --> 17:46.320
I'm really interested in is how do we make an AI agent um and uh like uh to define an agent is

17:46.320 --> 17:51.840
basically something that takes past uh like a sequence of past observations and then produces

17:51.840 --> 17:58.320
the next action so it's just I think of an agent uh I know Mike you think agency has goals uh but

17:58.320 --> 18:04.560
those are externally observable the knowledge about an agent uh so uh I think that's interesting to

18:04.560 --> 18:09.760
figure out so for this uh thing uh like I think of an agent it's just like some function that given a

18:23.760 --> 18:27.760
I think you're breaking up for me yeah me too me too

18:27.920 --> 18:30.960
and

18:40.160 --> 18:42.960
sorry David I don't know if you can hear me but I'm not getting any of this

18:53.680 --> 18:55.600
yeah me too I can't I can't hear what you're saying

18:58.400 --> 19:00.400
good

19:04.640 --> 19:09.040
technology's not a friend today I guess he'll try again yeah I've noticed that usually happens

19:09.040 --> 19:12.960
when something really interesting's about to come out it's because it knows what we're doing

19:12.960 --> 19:16.720
and it doesn't want us to make progress on these questions yeah there's a little bit of that

19:20.720 --> 19:25.040
is that background somewhere you've been uh yeah this is uh years ago this is Alaska

19:25.840 --> 19:30.240
oh nice okay I think I've been to Alaska but many years ago I don't remember much about it

19:30.240 --> 19:34.320
yeah yeah it was pretty it was pretty incredible

19:36.240 --> 19:40.320
this is my blank beige wall because I have that kind of apartment so

19:42.560 --> 19:47.760
yeah cool well yeah hopefully hopefully he'll come back um if not we'll just chat

19:48.160 --> 19:55.680
so actually I do have something I should address with you quickly so regarding the paper we've

19:55.680 --> 20:00.000
discussed I just want to briefly mention to you I don't know a lot about collaborating on papers

20:00.000 --> 20:03.520
and if there's something that you might expect a collaborator to be doing right now while you're

20:03.520 --> 20:08.000
working on other things that might not be obvious to me is there anything specific that I should

20:08.000 --> 20:13.040
be doing right now uh remind me remind me where it stands you have you you've sent me a draft

20:13.680 --> 20:19.600
sent you an outline uh yeah then then it's stuck in my court so I've just been in pain so I will

20:19.600 --> 20:24.240
I will get back to you with the next couple of days with it so yeah yeah and no rush I just

20:24.240 --> 20:27.760
literally wasn't sure if there was something it's not you it's me I've got a I've got a stack of

20:27.760 --> 20:39.280
drafts um on my just because I will get to it yeah okay let's see David is I'll be back

20:39.280 --> 21:01.840
can you hear me hi David hey hey sorry my computer decided not to run zoom anymore so I'm on my phone

21:02.560 --> 21:13.600
um okay let let me try to just do this without the slides but basically the idea is that like an

21:13.600 --> 21:20.720
agent is essentially some function that maps past observations to to the next action and

21:20.720 --> 21:28.400
general intelligence is essentially a set of behaviors of like I think in your paper might

21:28.400 --> 21:33.920
have this thing where it's like how do we navigate a space uh adaptive how do we adaptively

21:33.920 --> 21:39.920
and uh intelligently learn to navigate arbitrary spaces and I think of that as just like a collection

21:39.920 --> 21:45.680
of uh algorithms that let you balance exploration exploitation essentially it's active inference

21:45.680 --> 21:52.240
but active inference requires perfect Bayesian uh inference and really everything just implements a

21:52.240 --> 22:00.160
an approximation so uh the question is like how do you learn or build an approximation of active

22:00.160 --> 22:06.000
inference in various spaces and neural networks are function approximators that instead of building

22:06.000 --> 22:14.480
you can train given the right uh input output pairs so the idea is uh to basically train a

22:15.200 --> 22:22.480
uh a neural network that's a function approximator of uh intelligent space navigation and I think the

22:22.480 --> 22:28.560
way to do that is agents are basically duels of their environment so if you want to train uh an

22:28.560 --> 22:34.160
agent to do something you need to give it an environment where achieving fitness in that

22:34.160 --> 22:42.400
environment uh gives you the the behaviors that you want and so I think if you want an agent that's

22:42.400 --> 22:49.280
able to generalize learning you need an environment where there's always something new to learn uh and

22:50.080 --> 22:55.360
that the behavior space needs to be dense enough that the agent is able to always discover some

22:55.360 --> 23:04.000
new thing that it can learn uh that gives it adaptive fitness to the environment uh and therefore as it

23:04.000 --> 23:10.800
is learning how to uh adapt to that particular environment it is also generalizing learning

23:10.800 --> 23:19.280
it is meta learning learning so uh the idea is if you can give an agent uh always some new

23:19.280 --> 23:23.520
environment where there's something new to learn it's going to learn that and it's also in the

23:23.520 --> 23:27.840
process of doing that it's going to learn how to learn it's going to learn how do I balance

23:27.840 --> 23:33.920
exploration exploitation what algorithms what sampling algorithms can I apply to this new space

23:33.920 --> 23:41.520
how do I uh leave myself and grams uh uh like how do I store memory how do I structure memory

23:41.520 --> 23:47.920
how do I interpret memory uh in these various ways and so uh what you want is an environment

23:47.920 --> 23:53.840
that essentially gets harder or different as the agent gets smarter and I think the way to do that

23:53.840 --> 23:58.240
is make the environment and I think the way evolution did this is make the environment

23:58.240 --> 24:05.520
highly multi-agent so that uh what you're like there are there is complexity in the physics of

24:05.520 --> 24:10.560
the environment sure but really most of the complexity is in the minds of the other learners

24:10.560 --> 24:16.160
and this kind of goes back to economics and capitalism uh where uh you know markets are

24:16.160 --> 24:21.040
kind of anti legible as soon as you find out some trick that gives you an advantage in the

24:21.040 --> 24:26.320
market that advantage goes away because it gets arbitrage the way by everyone else that learns it

24:27.040 --> 24:34.160
and so I think you kind of had this with multi-agent setups you kind of can get into this infinite

24:34.160 --> 24:41.200
game where as the minds of agents get more complex you have to get more complex to

24:42.480 --> 24:47.040
be fit in that environment and this this is kind of just like creates this intelligent

24:47.040 --> 24:52.960
tread intelligence treadmill and I think this has been done in AI so this is how we get like

24:52.960 --> 24:59.200
go player or chess players it's like they do self-play against themselves and as they get

24:59.200 --> 25:07.440
better the game gets harder and but I think typically this is done in purely adversarial

25:07.440 --> 25:14.960
settings uh where and with an fully adversarial setting you basically cannot explore a lot of

25:14.960 --> 25:22.160
the behavior space because as soon as you deviate from some dominant strategy you get

25:23.040 --> 25:29.760
exploited by all the other players and so it's very easy to just get stuck at local minima

25:29.760 --> 25:36.480
because there's not a lot of ways to deviate from uh local maximum you basically have to

25:36.480 --> 25:41.600
really discover something new really quickly or you just get out competed by people that are doing

25:41.600 --> 25:49.600
the same old thing and I think the fix to that is basically blending this line of what an agent is

25:50.240 --> 25:54.880
in this collective intelligence sense I think one really strong technique for that is kinship

25:54.880 --> 26:01.760
so if you're in a world where other agents are say a lot of the agents around you are your brothers

26:01.760 --> 26:11.360
or your clones or your cousins essentially this whole spectrum of kinship then another way to

26:11.360 --> 26:17.040
think about agency is kind of you know if you have two agents that share the same goal you can

26:17.040 --> 26:23.200
think of them as one agent uh with just like really bad cognitive architecture where instead of just

26:23.200 --> 26:29.920
being it has to like learn how to pass messages to itself so one way to define agency is via uh

26:29.920 --> 26:36.880
essentially goals and kinship is this way of aligning agents on goals so if you have an agent

26:36.880 --> 26:42.720
that 100 shares the goal with another agent it's basically one agent if it's 70 sharing goals with

26:42.720 --> 26:50.720
that agent then you know it's kind of this blended super agent and so the idea is if you have an

26:50.720 --> 26:57.840
environment where all these agents are aligned with each other with various different kinship

26:57.840 --> 27:03.120
relationships you can create this really high dimensional behavior space where you're not

27:03.120 --> 27:08.240
always competing there's a million different ways to cooperate break form coalitions break

27:08.240 --> 27:16.160
coalitions established trust and so the idea is you know can we create a relatively simple

27:16.160 --> 27:25.760
fast to simulate in silico virtual world full of other agents the agents are aligned with each other

27:25.760 --> 27:33.840
in various kinship scenarios and then we scale off neural then we train larger and larger neural

27:33.840 --> 27:43.440
networks to drive those agents and get to LLM size you know billion parameter models that instead of

27:43.440 --> 27:49.440
predicting the next token are predicting the next action that an agent should take given all of the

27:49.440 --> 27:55.520
experience at scene and so the idea would be that if you give an environment like that you're

27:55.520 --> 28:02.240
essentially providing a gradient towards increased intelligence and then you're using standard machine

28:02.240 --> 28:10.560
learning techniques to train a function approximator for that to follow that gradient so that's kind of

28:10.560 --> 28:20.320
the idea behind the like overall environment but then the other thing I wanted to incorporate is

28:20.320 --> 28:25.200
this collective intelligence approach and I really wish I could present my slides I don't know what

28:25.200 --> 28:33.760
happened with my computer maybe I'll pause for a second let other people speak I don't know if

28:33.760 --> 28:39.520
anyone has feedback or thoughts or criticisms and then I'll try to see if I can get this other

28:39.600 --> 28:46.480
slide to work meanwhile yeah thank you for that I mean I have certainly heard a somewhat similar

28:46.480 --> 28:52.560
sounding theory that evolution evolved or intelligence evolved as some sort of you know social competition

28:52.560 --> 28:56.640
that you know primates competing with primates and birds competing with birds is

28:56.640 --> 29:00.080
where it comes from rather than trying to like you know build tools and so on

29:00.800 --> 29:10.880
um so uh yeah the idea about yeah you need a form of like some kind of protection some

29:10.880 --> 29:16.560
ability to innovate I mean there's a lot of aspects of economics where there's this

29:17.120 --> 29:22.640
challenging tension between wanting to optimize and wanting to create room for innovations like

29:22.640 --> 29:27.200
perfect competitions a classic example like you learned very early on perfect for competition

29:27.200 --> 29:31.840
you're going to maximize price maximize output your price is going to be as low as it should be

29:32.720 --> 29:37.920
but it's very hard to innovate in perfect competition because the way the model works is

29:37.920 --> 29:42.320
whatever new technology you introduce everyone copies that instantly you make no profit so but

29:42.320 --> 29:46.880
you're the one who made the investment so that just sucks so a little bit of monopoly can actually

29:46.880 --> 29:50.880
be helpful because that way you're making money and then there's even artificial things like you

29:50.880 --> 29:55.600
know like patents and stuff that give people monopolies to try to incentivize them to

29:56.320 --> 30:02.160
make those investments and innovate so that is a very important uh aspect of intention of any

30:02.160 --> 30:07.200
sort of collective intelligence system is trying to encourage parts of your system to improve and

30:07.200 --> 30:10.960
do better and trying to protect them from parts of the system might say oh we're going to copy

30:10.960 --> 30:17.440
that we're going to take that we're going to steal that um there's a part of this that

30:18.880 --> 30:24.320
maybe I know you're trying to get your slides up but um I saw in your talk you talk a little bit

30:24.320 --> 30:30.320
about like love and stuff at the end of that when you have you have a talk on YouTube and that's

30:30.320 --> 30:38.160
something that I had to bring up because I am working on a couple of papers about moral psychology

30:38.160 --> 30:44.720
and neuroscience including a paper on morality as a form of cognitive glue which it isn't really

30:44.720 --> 30:51.120
but it's a sort of an interesting counter example in some ways and um basically

30:51.920 --> 31:00.400
there's a model in which um kinship let me ask you this if um I suppose that you took all the

31:00.400 --> 31:05.040
families and you rearranged them like people didn't know that they were in the wrong families but

31:05.040 --> 31:10.240
you had families where no one's actually related to each other genetically uh would the system still

31:10.240 --> 31:22.800
work? Yep. Yeah so um I think of this as like I think of this as essentially reward sharing so on

31:22.800 --> 31:30.320
a genetic level uh the the genetic optimization process uh gives rise to organisms that care about

31:30.320 --> 31:36.000
their kin right so because a gene is present in the kin that it's going to make the organisms that

31:36.000 --> 31:41.200
come from it want to help other organisms that have that gene from the gene's perspective it

31:41.200 --> 31:50.080
doesn't matter whether you reproduce or your brother reproduces uh twice uh so uh and similar

31:50.080 --> 32:00.560
so essentially the the behavior that an organism exhibits that we call love is uh basically what

32:00.640 --> 32:07.120
it looks like when the underlying optimization process shares rewards or goal or like uh

32:08.240 --> 32:15.040
shares rewards between organisms but then the organism itself one of the things that it evolves

32:15.040 --> 32:22.960
or learns or whatever needs is the is some way to recognize who it should care about so we have all

32:22.960 --> 32:32.000
of these heuristics for knowing who our kin are um and uh once those things are in place uh it's the

32:32.000 --> 32:40.560
heuristics that matter not the underlying gene so uh I think if you train agents that see other

32:40.560 --> 32:45.840
agents and you give them a reliable signal that hey helping this agent is actually the same as

32:45.840 --> 32:51.360
helping you so every time this agent something good happens to this agent you get part of that reward

32:51.360 --> 32:57.680
and here is a marker that tells you who is kin and who is not the agent is going to learn behaviors

32:57.680 --> 33:03.360
that are conditioned on that marker and then once you stop training and you just let these agents

33:03.360 --> 33:08.560
go around behaving you can change that marker arbitrarily rewards are no longer even there

33:08.560 --> 33:15.200
right genetic evolution or whatever has stopped now you're no longer learning new policy now the

33:15.200 --> 33:20.720
learning is happening inside the mind of the organism not inside the the genes

33:26.880 --> 33:29.280
conditioned behavior on kinship markers

33:31.520 --> 33:38.000
does that did that make sense it does and there's a very important paper that anyone interested

33:38.000 --> 33:43.520
in this should read it's called the sense of should a biologically based framework for modeling

33:43.520 --> 33:50.960
social pressure and in some ways maybe the difference is subtle but I think there's a way

33:50.960 --> 33:59.280
of looking at these sorts of morally motivated cooperative behaviors um that does not in fact

33:59.280 --> 34:06.240
depend on any kind of evolved pressure to care for or intend to cooperate but instead it's just like

34:08.000 --> 34:12.400
something your brain just constructs as a living organism in its environment

34:12.400 --> 34:19.760
because it's trying to make it's in its social environment predictable and and so there is

34:19.760 --> 34:25.040
something of emerging perspective and my papers are somewhat drawing on this research that's

34:25.040 --> 34:31.200
basically saying let's not think about evolution let's not think about long-term cooperation let's

34:31.200 --> 34:35.680
just think about trying to make the social environment predictable and that's how we can

34:35.680 --> 34:42.160
actually get um these cooperative and moral behaviors and so that just might be something

34:42.160 --> 34:46.160
to look at because that's another paper released as a co-author and I just bring up Lisa's stuff as

34:46.160 --> 34:56.640
much as possible I mean this is also yeah I mean I go ahead Michael sorry I was just going to say

34:56.640 --> 35:02.480
that that also sounds like um uh the the uh imperial model of multicellularity that Chris

35:02.480 --> 35:08.000
feels and I published a bunch of years ago which is basically that you know if cells are trying to

35:08.000 --> 35:12.720
predict an uncertain environment the least uncertain the least surprising thing around is a copy of

35:12.720 --> 35:19.120
yourself and so this this is a this can be a driver for uh making cells stick around after

35:19.120 --> 35:26.080
you've divided to to form a you know a predictive of a highly predictive um um uh surrounding for

35:26.080 --> 35:29.840
yourself so that you live in this niche and then the frontline infantry is out there facing the

35:30.160 --> 35:34.160
uncertainties of the outside world but but you can predict them because they're you and so it's a

35:34.160 --> 35:39.200
lot easier that way right so so you can you can think about it that way as well have I talked to

35:39.200 --> 35:53.760
you about comparative advantage uh yeah no I don't think I want to go ahead David yeah sorry I had a

35:53.760 --> 36:02.720
question about that Michael uh so uh I don't uh so if you have a cell uh and the cell does uh you

36:02.720 --> 36:10.320
know given situation a it does random stuff then being surrounded by copies of those cells

36:10.320 --> 36:16.720
and you do random stuff and your clones do random stuff being surrounded by cells that in that

36:16.720 --> 36:22.960
situation do random stuff is not actually going to help you predict uh the environment

36:24.720 --> 36:29.520
so I think there's something more to that it's not that being surrounded by copies of yourself

36:29.520 --> 36:35.360
gives you the ability to predict your environment I think it's being surrounded by copies of cells

36:35.360 --> 36:41.280
that want to make your environment that want to help you make the environment more predictable

36:42.320 --> 36:49.520
that actually uh matters uh right unless you think that a cell by being able to

36:49.520 --> 36:57.120
introspect itself can predict the behavior of other copies of itself I mean I I don't think

36:57.120 --> 37:02.160
they're likely to be doing random stuff they're probably situations in which the responsiveness

37:02.160 --> 37:08.000
is kind of random but but I don't think that's the vast majority of what cells do and so I do

37:08.000 --> 37:16.800
think that if if if you're um it's it's easier to uh to anticipate cycles and responses I mean

37:16.800 --> 37:20.160
again I mean I think you're right if it's random it's not going to work but I actually don't think

37:20.160 --> 37:26.400
that cells are random in that in that way but let's um anyway let's let's let David do his thing

37:28.160 --> 37:35.680
oh okay yeah this is just uh so this is uh this this is actually mostly inspired by your work Michael

37:35.680 --> 37:41.760
and economics so I'd love to get thoughts on this but okay so this is the standard

37:41.760 --> 37:47.360
a reinforcement learning uh formulation right you have an environment you have an agent uh

37:47.360 --> 37:52.880
the agent takes an action in the environment it gets back some observation and a reward and then

37:53.520 --> 37:59.920
it learns to maximum to act in a way that maximizes reward and uh you can have a dual formulation

37:59.920 --> 38:06.000
this is which is the inference where the agent is trying to predict observations but essentially

38:06.000 --> 38:13.040
this is the the normal kind of loop and typically the way it's uh we can break this down into like

38:13.040 --> 38:20.160
an agent basically there's a bunch of observation states from the environment that go into the mind

38:20.160 --> 38:27.120
the mind is going to generate a bunch of actions over actuators uh there's some memory that maybe

38:27.120 --> 38:33.040
the mind has that it's reading and writing from uh and it's getting a reward signal so this is how

38:33.040 --> 38:39.520
we train agents in reinforcement learning or this could be trained or this could be evolved

38:40.720 --> 38:48.160
doesn't really matter and so the way we do this now in the field of RL is you have this

38:48.160 --> 38:53.040
giant neural net inside the mind and there's model 3 and model RL so they're like this is

38:53.040 --> 38:59.520
skipping over some detail but essentially you're training this big blob of math that you're jiggling

38:59.520 --> 39:09.200
using gradient descent to optimize your long-term expected reward and it is learning how to compress

39:09.200 --> 39:16.960
and basically how to compress the observations into latent states maybe store the parts of those

39:16.960 --> 39:22.880
latent states in its memory that then it can reinterpret and eventually to generate actions

39:22.880 --> 39:32.320
and so these networks can be giant in language models there are 100 billions of parameters in

39:32.320 --> 39:35.680
reinforcement learning they're much smaller because we just haven't figured out how to

39:35.680 --> 39:44.480
do it yet but this is kind of the standard setup and what I am proposing is a different architecture

39:45.360 --> 39:54.640
that is essentially instead of there being one mind there are many individual agents

39:55.680 --> 40:05.360
each agent is mapped onto some subset of the observation space and some subset of the memory

40:05.360 --> 40:11.520
space and some subset of the action space and you know they can be mapped to any or all of them

40:11.520 --> 40:18.640
but essentially rather than training one brain I want to train a neuron that in a collection

40:19.280 --> 40:27.520
when you put it together in a graph of other neurons knows how to co-organize collectively

40:27.520 --> 40:35.200
to solve the overall problem that the brain has so rather than training 100 million model

40:35.200 --> 40:43.040
network I want to train one million parameter network that when you copy and paste it and

40:43.760 --> 40:52.720
throw it into this soup of other neurons will self-organize into solving the problem

40:52.720 --> 41:00.240
and so the idea is rather than having to learn adaptive algorithms so in in this way you know

41:00.240 --> 41:05.680
you can imagine that there's an algorithm that you apply to many different parts of the sub problem

41:06.240 --> 41:12.880
but this network has to learn how to has to learn that algorithm in a bunch of different places

41:12.880 --> 41:18.320
because every sub function call might have the same kind of you know oh this is just like a

41:18.320 --> 41:22.960
regression problem but you have to learn how to solve regression in a bunch of different places

41:22.960 --> 41:30.320
whereas here this thing can just learn okay you know here is how I divide my problem into sub

41:30.320 --> 41:38.160
problems and send those messages out or here is how I you know pay attention to a pattern

41:38.160 --> 41:46.080
happening at this time scale and that's my job so the idea is to break down this one big blob of

41:46.160 --> 41:54.560
compute into something that can be done on this graph instead and I think and this is

41:54.560 --> 41:59.760
why I wanted to talk to you then because I think basically I started out thinking well this is an

41:59.760 --> 42:06.400
economy you know if these things were you know if the observations and memory were commodities and

42:06.400 --> 42:12.160
these things were like buying and selling those commodities or something like that that would give

42:12.160 --> 42:18.960
us so the question is you know the the inside of a neural cell is a neural network that we train

42:19.520 --> 42:24.320
but then the question is what is the cognitive glue between all of these neural cells essentially

42:24.320 --> 42:30.240
what is the protocol that they use such that when we train train the neural cell

42:32.080 --> 42:37.760
on the overall objective they'll learn something useful and the thing I came up with

42:38.480 --> 42:45.120
kind of influenced by active inference and collective intelligence is so now this is inside

42:45.120 --> 42:50.560
a single neural cell right now now this is an agent that just sees some set of inputs

42:51.760 --> 42:59.920
it has its own memory it's going to get some reward signal from the the cognitive glue

42:59.920 --> 43:05.840
and it's going to produce some outputs and I think the way to do that is that basically

43:06.240 --> 43:13.520
it it should be predicting its inputs active inference essentially learning a world model

43:13.520 --> 43:19.760
of its little world inside the brain and then voting on the outputs

43:22.400 --> 43:29.440
so that it can and in this case outputs can be inputs to another neural cell or just outputs to

43:29.440 --> 43:37.520
an actuator that moves the organism around so I think essentially I'm proposing some kind of a

43:38.240 --> 43:44.560
an agent who's that you know receives a set of inputs so this is some subset

43:46.400 --> 43:51.600
and it has some memory and then what it's going to do is it's going to predict the output at the

43:51.600 --> 44:02.320
next time step along with some bet so it has energy and it has finite energy it's going to

44:02.320 --> 44:08.720
bet on what the values of that inputs are going to be at the next time step it's going to vote on

44:08.720 --> 44:14.800
what it wants to set the outputs to also using this energy and then at the next time step it's

44:14.800 --> 44:22.320
going to see well what were the actual inputs and which bets did it win and then it gets energy

44:22.320 --> 44:30.320
in proportion to essentially gets energy back as a reward so the idea here is that these agents

44:30.320 --> 44:38.640
are incentivized to only predict the inputs that they have a comparative advantage on

44:38.640 --> 44:42.720
over other agents so the same input and output is mapped to multiple cells

44:42.720 --> 44:48.640
so as an agent you should learn to only to basically monitor your environment form models

44:48.640 --> 44:55.440
about it and then bet on the things where you have a comparative advantage to other cells

44:56.240 --> 45:04.720
and then as you get more and more capabilities to model your environment you get more and more

45:04.720 --> 45:12.240
energy that you can use to vote on steering around that environment this is all hand wavy and this

45:12.240 --> 45:18.160
is where I think I need help and help from economics because I started out with pricing then

45:18.160 --> 45:25.280
I had it actually be an auction or your or prediction market add an auction and then I

45:25.280 --> 45:30.960
try to simplify it just down to just like straight betting markets but that's basically the idea is

45:30.960 --> 45:38.080
can we come up with this like simple cognitive glue for a pool of these agents and then train

45:38.080 --> 45:46.720
those agents I'll stop now yeah no that's very interesting I think it's definitely an economy

45:46.720 --> 45:52.640
and you definitely should talk to economists I'm afraid I think a practical issue is I'm just not

45:52.640 --> 45:58.480
sure economists have actually done a lot to explore this space even in theory for the most part they've

45:58.480 --> 46:04.560
been content to just take the economy as it is and try to explain that and there's been I think

46:04.560 --> 46:09.200
just kind of a certain lack of creativity there in terms of envisioning other economic spaces

46:09.200 --> 46:15.440
and trying to think about building alternative economic systems like this one so I'm afraid

46:15.440 --> 46:21.360
I myself don't have a lot of practical things to say right now that might actually be of any

46:21.360 --> 46:25.760
specific use to you other than I think just affirmation that you're right that this is economics

46:25.760 --> 46:31.840
and that talking to economists is a good idea I think the mechanism design people are the place

46:31.920 --> 46:36.480
to start I mean I think that's where Glenn Wilde comes from but I gotta know if like

46:36.480 --> 46:40.320
Eric Maskin is reachable at all but like he won a Nobel Prize for that and I had a

46:40.320 --> 46:44.720
two-minute conversation with him many years ago it was very influential so you might just want to

46:44.720 --> 46:51.680
see if you could send him an email and see what he says yeah I think this is cool I'm afraid I don't

46:51.680 --> 46:56.640
think I can help you today with it but we should stay in touch and I'll let you know if if anything

46:56.640 --> 47:00.960
comes along because this is very interesting and very important

