1
00:00:00,600 --> 00:00:01,680
No, that's great.

2
00:00:01,680 --> 00:00:03,560
Okay, so maybe we could start.

3
00:00:03,560 --> 00:00:05,840
I think, Alex, if you could start

4
00:00:05,840 --> 00:00:10,840
and just give a few more words on this,

5
00:00:11,160 --> 00:00:13,240
the whole notion of mortal computation

6
00:00:13,240 --> 00:00:18,240
and specifically the issue of programmability

7
00:00:18,720 --> 00:00:23,720
and morphology and how we can sort of distinguish

8
00:00:24,920 --> 00:00:26,000
when we're looking at a system,

9
00:00:26,000 --> 00:00:27,680
how can we distinguish what aspects

10
00:00:27,680 --> 00:00:30,840
of mortal computation we're looking at.

11
00:00:30,840 --> 00:00:32,040
Anything along those lines,

12
00:00:32,040 --> 00:00:34,160
I think would be very useful for us.

13
00:00:34,160 --> 00:00:35,880
Maybe we could start there.

14
00:00:35,880 --> 00:00:38,760
Yeah, so I did, by the way, read last night

15
00:00:38,760 --> 00:00:41,520
the paper you shared with me and Carl and Chris

16
00:00:41,520 --> 00:00:44,640
at one point and some of your notions in there.

17
00:00:44,640 --> 00:00:46,880
First of all, a lot of it sounds like parts

18
00:00:46,880 --> 00:00:49,480
of mortal computation, just maybe you're not using

19
00:00:49,480 --> 00:00:51,240
the word mortal computation phrase.

20
00:00:52,240 --> 00:00:56,360
So in terms of the morphology part,

21
00:00:56,360 --> 00:00:59,040
so the argument that Carl and I make in that paper

22
00:00:59,040 --> 00:01:01,560
is just that the structure, right,

23
00:01:01,560 --> 00:01:05,320
is critically important to the actual system itself.

24
00:01:05,320 --> 00:01:07,360
So you don't want to devour,

25
00:01:07,360 --> 00:01:11,040
there's a lot of this idea of the amorphic formulation

26
00:01:11,040 --> 00:01:12,880
of computational models.

27
00:01:12,880 --> 00:01:16,200
So when you take, and actually Carl and Chris had a paper

28
00:01:16,200 --> 00:01:19,480
that predated the mortal computation paper,

29
00:01:19,520 --> 00:01:22,080
but we developed that metaphor further,

30
00:01:22,080 --> 00:01:26,480
you can draw a dot and arrow diagram of a neural network

31
00:01:26,480 --> 00:01:31,160
and that endows it with this pedagogical morphology,

32
00:01:31,160 --> 00:01:32,960
but really at the end of the day, it doesn't matter.

33
00:01:32,960 --> 00:01:35,120
It's just a pile of linear algebra

34
00:01:35,120 --> 00:01:36,840
that will do its calculations.

35
00:01:36,840 --> 00:01:38,360
And then we have to go through

36
00:01:38,360 --> 00:01:41,400
the von Neumann computing architecture

37
00:01:41,400 --> 00:01:44,040
to transmit the weight, memory and all that.

38
00:01:44,040 --> 00:01:47,560
And then that's where this great thermodynamic cost.

39
00:01:47,560 --> 00:01:52,280
So living systems, I know as you know very well, Michael,

40
00:01:52,280 --> 00:01:54,400
but in general are inherently morphic.

41
00:01:54,400 --> 00:01:56,400
And actually this is the part where we didn't have it

42
00:01:56,400 --> 00:02:00,000
in the paper, but your paper used the word polycomputing,

43
00:02:00,000 --> 00:02:01,720
which is the idea that a substance

44
00:02:01,720 --> 00:02:03,840
can compute many different things simultaneously

45
00:02:03,840 --> 00:02:06,320
is like one way to look at that word.

46
00:02:06,320 --> 00:02:08,800
And the idea is that it's all about the substrate.

47
00:02:08,800 --> 00:02:11,240
And so mortal computation then just says,

48
00:02:11,240 --> 00:02:14,120
if we're going to think about artificial

49
00:02:14,120 --> 00:02:16,680
general intelligence or machine intelligence,

50
00:02:16,680 --> 00:02:18,360
we're sort of going about it in the wrong,

51
00:02:18,360 --> 00:02:21,360
potentially wrong direction by having this divorce

52
00:02:21,360 --> 00:02:25,000
of the computational architecture and construct

53
00:02:25,000 --> 00:02:28,240
separate from the morphology or the substrate too,

54
00:02:28,240 --> 00:02:30,200
that this is going to be enacted on

55
00:02:30,200 --> 00:02:33,880
because living systems, if you change the morphology,

56
00:02:33,880 --> 00:02:36,160
you change the properties of the system,

57
00:02:36,160 --> 00:02:38,840
you change also what it can compute, what it can do.

58
00:02:38,840 --> 00:02:42,480
Now you talk about the liquid kind of brain

59
00:02:42,480 --> 00:02:45,120
or the idea that we have this self over time,

60
00:02:45,120 --> 00:02:49,120
which doesn't actually sit at odds with mortal computation.

61
00:02:49,120 --> 00:02:52,640
So the idea is that you're constantly through change,

62
00:02:52,640 --> 00:02:56,280
you're persisting, you're almost like I want to persist,

63
00:02:56,280 --> 00:02:57,960
but I understand that my system

64
00:02:57,960 --> 00:02:59,920
is going through like auto pieces.

65
00:02:59,920 --> 00:03:02,080
So mortal computation sort of absorbs that

66
00:03:02,080 --> 00:03:04,040
and tries to say we should be designing systems

67
00:03:04,040 --> 00:03:05,420
from that perspective.

68
00:03:06,360 --> 00:03:08,720
So just so I don't keep rambling,

69
00:03:08,720 --> 00:03:11,720
you were talking about programmability.

70
00:03:11,720 --> 00:03:13,600
And I think that was an interesting part

71
00:03:13,600 --> 00:03:16,200
that we didn't really get to chat a whole lot about

72
00:03:16,200 --> 00:03:19,320
and it wasn't 100% clear what was meant

73
00:03:19,320 --> 00:03:23,120
by programming the morphology or the system

74
00:03:23,120 --> 00:03:24,960
because actually after reading your paper,

75
00:03:24,960 --> 00:03:27,560
I think that sort of gave the answer as to how,

76
00:03:27,560 --> 00:03:29,240
because I said originally, oh well,

77
00:03:29,240 --> 00:03:32,440
I'm thinking that the morphology or the substrate

78
00:03:32,440 --> 00:03:34,880
dictates strictly what you can and can't do.

79
00:03:34,880 --> 00:03:37,080
Yes, it will change and it will repair

80
00:03:37,080 --> 00:03:40,960
and go through damage or do things like self-replication.

81
00:03:40,960 --> 00:03:43,160
But I wasn't thinking about the human designer,

82
00:03:43,160 --> 00:03:45,720
he would say if we are designing a chimeric system

83
00:03:45,720 --> 00:03:49,160
or something new, manipulating that morphology so easily,

84
00:03:49,160 --> 00:03:50,800
it's more like, well, we're gonna be looking

85
00:03:50,800 --> 00:03:54,120
at computational simulations of that

86
00:03:54,120 --> 00:03:55,520
and that would be programmable.

87
00:03:55,520 --> 00:03:56,920
And then that's where I was talking about,

88
00:03:56,920 --> 00:04:00,280
oh, we could do like a software simulation

89
00:04:00,280 --> 00:04:04,440
of Anthrobots or something, or Xenobots of that form.

90
00:04:04,440 --> 00:04:06,960
And then you could sort of set the properties

91
00:04:06,960 --> 00:04:09,840
of the environment in the system roughly

92
00:04:09,840 --> 00:04:11,480
according to something you wanna look at

93
00:04:11,480 --> 00:04:13,840
and then simulate it and see what it does.

94
00:04:13,840 --> 00:04:15,960
But then your paper sort of essentially,

95
00:04:15,960 --> 00:04:19,320
in your work as an example of you can directly program

96
00:04:19,320 --> 00:04:24,240
like the genetic aspects of a system,

97
00:04:24,240 --> 00:04:27,440
or you can kind of manipulate the bioelectrical chemicals.

98
00:04:27,440 --> 00:04:31,480
You had an example of like a tadpole or a froglet

99
00:04:31,480 --> 00:04:34,840
where if we apply the right electrical stimulation,

100
00:04:34,840 --> 00:04:37,840
you can get it to grow a tail or get it to grow a leg

101
00:04:37,840 --> 00:04:38,960
and there's some properties there.

102
00:04:38,960 --> 00:04:43,360
So I think that is where the programmability is.

103
00:04:43,360 --> 00:04:44,640
And then that would be more,

104
00:04:44,640 --> 00:04:46,920
you would have even actual experience

105
00:04:46,920 --> 00:04:48,640
programming the morphologies.

106
00:04:48,640 --> 00:04:51,080
Whereas I was thinking more from the perspective

107
00:04:51,080 --> 00:04:55,080
of neural networks where we have the structure

108
00:04:55,080 --> 00:04:57,240
and we want that to now be at the very top

109
00:04:57,240 --> 00:04:59,120
of what Karl and I call mills.

110
00:04:59,120 --> 00:05:00,400
And that you remember in the paper,

111
00:05:00,400 --> 00:05:03,160
there's the mortal inference learning and selection.

112
00:05:03,160 --> 00:05:05,240
And at the very, very top is structure.

113
00:05:05,240 --> 00:05:08,080
And that would be something that I think is underexplored

114
00:05:08,120 --> 00:05:11,720
in the area about, oh, maybe we have neurogenesis

115
00:05:11,720 --> 00:05:14,200
and naphyogenesis and then the system sort of has to use

116
00:05:14,200 --> 00:05:18,040
that as another aspect of how it evolves over time.

117
00:05:18,040 --> 00:05:19,640
But that was thought of,

118
00:05:19,640 --> 00:05:22,040
or at least the way I was writing it originally,

119
00:05:22,040 --> 00:05:24,280
it's sort of doing that in its own way.

120
00:05:24,280 --> 00:05:25,960
You're not really involved in saying,

121
00:05:25,960 --> 00:05:28,280
oh, I'm gonna help you do model selection,

122
00:05:28,280 --> 00:05:30,480
but maybe if you encode priors,

123
00:05:30,480 --> 00:05:32,280
that was the other aspect I had.

124
00:05:32,280 --> 00:05:34,120
If you were encoding certain constraints,

125
00:05:34,120 --> 00:05:37,280
you say, well, I just, I'm gonna play,

126
00:05:37,280 --> 00:05:39,280
I'm gonna skip ahead for what evolution

127
00:05:39,280 --> 00:05:41,960
would naturally walk, randomly walk you to and say,

128
00:05:41,960 --> 00:05:43,800
these types of structures are invalid.

129
00:05:43,800 --> 00:05:45,680
That would be like programming,

130
00:05:45,680 --> 00:05:47,880
maybe structure from that perspective.

131
00:05:47,880 --> 00:05:49,520
I don't know if this is making sense,

132
00:05:49,520 --> 00:05:51,320
but I think it's a tricky topic

133
00:05:51,320 --> 00:05:54,800
because I wasn't entirely sure what exactly two

134
00:05:54,800 --> 00:05:56,320
was meant by programmability

135
00:05:56,320 --> 00:05:59,160
because you have experience actually doing that.

136
00:05:59,160 --> 00:06:03,160
So the answer is yes, it can be done from your perspective

137
00:06:03,160 --> 00:06:06,680
and we would just be translating it to chimeric systems

138
00:06:06,720 --> 00:06:08,080
or artificial systems

139
00:06:08,080 --> 00:06:11,240
rather than it only just being like biological material.

140
00:06:12,240 --> 00:06:13,520
I hope that makes sense.

141
00:06:13,520 --> 00:06:15,920
Yeah, yeah, I mean, the kind of programmability

142
00:06:15,920 --> 00:06:18,640
I had in mind, so just as an example,

143
00:06:18,640 --> 00:06:20,880
we have these flatworms, these planaria,

144
00:06:20,880 --> 00:06:22,720
and you can chop them into pieces

145
00:06:22,720 --> 00:06:24,560
and every piece regrows a complete,

146
00:06:24,560 --> 00:06:26,400
perfectly patterned little worm.

147
00:06:26,400 --> 00:06:28,200
And you could ask the question,

148
00:06:28,200 --> 00:06:30,200
how does it know how many heads to make?

149
00:06:30,200 --> 00:06:32,600
And so it turns out that there's an electrical pattern

150
00:06:32,600 --> 00:06:35,400
that sort of a body-wide electrical pattern

151
00:06:35,400 --> 00:06:38,080
that dictates the number and the location of the head.

152
00:06:38,080 --> 00:06:40,160
And the amazing thing about that substrate

153
00:06:40,160 --> 00:06:43,560
is that if you change that pattern,

154
00:06:43,560 --> 00:06:44,560
the tissue will hold it.

155
00:06:44,560 --> 00:06:46,160
So we can change the pattern to say no,

156
00:06:46,160 --> 00:06:48,240
two heads instead of one, and it holds.

157
00:06:48,240 --> 00:06:51,520
And those worms in perpetuity forevermore,

158
00:06:51,520 --> 00:06:53,400
despite their completely normal genetics,

159
00:06:53,400 --> 00:06:56,520
will continue to regenerate as two-headed worms.

160
00:06:56,520 --> 00:06:59,680
So it's a very minimal example of reprogrammability

161
00:06:59,680 --> 00:07:02,280
because we don't have anything like complete control yet,

162
00:07:02,280 --> 00:07:03,960
I think in the future, maybe we will,

163
00:07:03,960 --> 00:07:05,680
but at the moment we don't.

164
00:07:05,680 --> 00:07:08,640
But it is an example where the hardware

165
00:07:08,640 --> 00:07:11,320
in an important sense, so the genetics are normal,

166
00:07:11,320 --> 00:07:13,440
all this, there are no weird nanomaterials,

167
00:07:13,440 --> 00:07:14,520
there's no genomic editing,

168
00:07:14,520 --> 00:07:17,160
there are no synthetic biology circuits,

169
00:07:17,160 --> 00:07:20,680
it's stock hardware, but because of this experience,

170
00:07:20,680 --> 00:07:23,000
this physiological experience that it's had,

171
00:07:23,000 --> 00:07:25,200
it now has a different pattern that it uses

172
00:07:25,200 --> 00:07:27,400
as the sort of target morphology

173
00:07:27,400 --> 00:07:29,600
of what it's going to do if it gets cut.

174
00:07:29,600 --> 00:07:31,760
So that's the kind of thing,

175
00:07:31,880 --> 00:07:34,120
the kind of plasticity that it has where

176
00:07:34,120 --> 00:07:37,200
the material is basically the same,

177
00:07:37,200 --> 00:07:41,400
but it has really a memory of a past event

178
00:07:41,400 --> 00:07:43,200
and that memory guides how it behaves

179
00:07:43,200 --> 00:07:44,920
in anatomical space in the future.

180
00:07:44,920 --> 00:07:47,800
So that's the kind of thing I wanted to sort of explore

181
00:07:47,800 --> 00:07:50,000
with respect to this framework.

182
00:07:51,560 --> 00:07:54,520
And I don't think what you described is at odds

183
00:07:54,520 --> 00:07:57,160
with what you would do with a mortal computer.

184
00:07:57,160 --> 00:08:00,440
I mean, the idea is that is where the programmability

185
00:08:00,440 --> 00:08:02,880
comes into play, you're kind of encoding that

186
00:08:02,880 --> 00:08:04,600
and then seeing how the morphology

187
00:08:04,600 --> 00:08:06,160
and the system evolve over time.

188
00:08:06,160 --> 00:08:08,920
If you want the two-headed worm example,

189
00:08:08,920 --> 00:08:11,240
I don't see any reason why that wouldn't translate

190
00:08:11,240 --> 00:08:15,720
to artificial systems or maybe non-biological systems

191
00:08:15,720 --> 00:08:18,680
if we are able to formulate what that morphology looks like.

192
00:08:18,680 --> 00:08:20,600
I think the key is setting that up

193
00:08:20,600 --> 00:08:23,120
and that's what Carl and I have at the very end of the paper

194
00:08:23,120 --> 00:08:26,080
we talk about, well, for example, someone like me,

195
00:08:26,080 --> 00:08:28,320
a computational neuroscientist, computer scientist,

196
00:08:28,320 --> 00:08:31,000
I don't have access to Xenobots

197
00:08:31,000 --> 00:08:33,320
or the biological material that I'd love to,

198
00:08:33,320 --> 00:08:34,280
or organoids, right?

199
00:08:34,280 --> 00:08:36,560
I really find those fascinating,

200
00:08:36,560 --> 00:08:37,840
but maybe we could simulate them

201
00:08:37,840 --> 00:08:40,480
and that's what we have at the appendix, actually,

202
00:08:40,480 --> 00:08:43,720
at the already long paper, digital morphology.

203
00:08:43,720 --> 00:08:46,360
Maybe that could be a way to sort of bridge the gap

204
00:08:46,360 --> 00:08:48,480
between the computational researchers

205
00:08:48,480 --> 00:08:50,120
and researchers like yourself.

206
00:08:50,120 --> 00:08:53,400
Oh, can we come up with benchmarks or system setups

207
00:08:53,400 --> 00:08:56,040
that I could play with the properties,

208
00:08:56,080 --> 00:08:57,920
mathematical models, maybe,

209
00:08:57,920 --> 00:09:00,280
of these biological morphologies

210
00:09:00,280 --> 00:09:02,720
and then kind of do investigation, right,

211
00:09:02,720 --> 00:09:06,520
without the costs and the barriers to entry

212
00:09:06,520 --> 00:09:08,840
to working with biological material

213
00:09:08,840 --> 00:09:10,520
or some of the things I don't have.

214
00:09:10,520 --> 00:09:13,080
So I don't see anything at odds.

215
00:09:13,080 --> 00:09:15,880
I don't know if Carl would wanna add anything

216
00:09:15,880 --> 00:09:18,220
that I might be missing or not getting.

217
00:09:22,680 --> 00:09:24,840
No, that was very fluent.

218
00:09:24,960 --> 00:09:26,960
I think you've covered everything there.

219
00:09:29,320 --> 00:09:31,640
If I can just jump in for a minute.

220
00:09:31,640 --> 00:09:36,640
It seems to me that this dimension of programmability

221
00:09:37,840 --> 00:09:42,840
could also be expressed as the dimension from uniqueness,

222
00:09:44,360 --> 00:09:47,240
which mortal computers, as I understand it,

223
00:09:47,240 --> 00:09:50,600
have more of than my laptop,

224
00:09:51,600 --> 00:09:56,600
to replicability or exact replicability, copyability.

225
00:09:59,880 --> 00:10:04,400
And to the extent that a system is really unique,

226
00:10:04,400 --> 00:10:05,680
you can't program it.

227
00:10:08,680 --> 00:10:13,680
And in part, just because you don't have two copies of it,

228
00:10:13,880 --> 00:10:16,720
so you can't test what you're doing in any way.

229
00:10:16,720 --> 00:10:18,480
You can't test reproducibility

230
00:10:18,480 --> 00:10:21,600
if you're working with a system that's completely unique.

231
00:10:26,000 --> 00:10:31,000
And biological systems are somewhere in the middle, right?

232
00:10:31,160 --> 00:10:34,560
Laptops are intended to be way out

233
00:10:34,560 --> 00:10:37,160
on the extreme replicability end.

234
00:10:38,760 --> 00:10:43,760
And so because a laptop is completely replicable,

235
00:10:45,360 --> 00:10:47,640
it's a completely generic entity,

236
00:10:49,320 --> 00:10:52,000
it's almost a Turing machine, right?

237
00:10:52,000 --> 00:10:54,420
It's almost just an abstraction.

238
00:10:55,800 --> 00:10:57,880
And so when we're programming,

239
00:10:57,880 --> 00:10:59,800
we can treat it as an abstraction.

240
00:10:59,800 --> 00:11:02,320
And we don't have to worry about things like

241
00:11:02,320 --> 00:11:03,720
where the power comes from

242
00:11:03,720 --> 00:11:08,720
and why the thing maintains the same shape over time

243
00:11:09,120 --> 00:11:11,520
and et cetera, et cetera.

244
00:11:11,520 --> 00:11:14,560
I mean, if it doesn't maintain the same shape over time,

245
00:11:14,560 --> 00:11:15,920
you take it to the repair shop

246
00:11:15,920 --> 00:11:17,680
or recycle it and buy another one.

247
00:11:19,480 --> 00:11:21,680
Whereas in biological systems,

248
00:11:21,680 --> 00:11:24,520
you have to worry about all of that stuff.

249
00:11:24,520 --> 00:11:27,320
And as you point out in the paper,

250
00:11:28,760 --> 00:11:33,760
and as the 4E people kind of have been pointing out

251
00:11:33,880 --> 00:11:38,880
for decades now, that's part of the algorithm,

252
00:11:39,920 --> 00:11:44,920
or that's part of the operating system, that shape.

253
00:11:45,680 --> 00:11:49,640
Whereas it's not involved at all in my laptop

254
00:11:49,640 --> 00:11:51,000
in the operating system.

255
00:11:51,000 --> 00:11:52,640
I mean, even the operating system

256
00:11:52,640 --> 00:11:55,360
can treat the hardware as an abstraction.

257
00:11:57,240 --> 00:12:02,240
So we can, I think more or less identify those two axes,

258
00:12:03,920 --> 00:12:06,040
the dimension of programmability

259
00:12:06,040 --> 00:12:07,680
and the dimension of uniqueness.

260
00:12:09,520 --> 00:12:14,240
And so one of the things that you emphasized in your paper,

261
00:12:14,280 --> 00:12:16,960
I thought this was very interesting,

262
00:12:16,960 --> 00:12:21,960
was the energetics of using the body

263
00:12:23,960 --> 00:12:26,040
as part of the operating system.

264
00:12:27,920 --> 00:12:32,920
And it meant that you didn't have to pay for a lot of memory,

265
00:12:36,120 --> 00:12:39,240
for example, or quite so much processing power.

266
00:12:39,680 --> 00:12:44,240
You do, of course, have to pay the cost

267
00:12:44,240 --> 00:12:46,320
of keeping the body intact.

268
00:12:48,000 --> 00:12:53,000
But as you pointed out, at least in organisms,

269
00:12:53,000 --> 00:12:58,000
that's cheaper than the cost of stamping out more laptops

270
00:12:58,880 --> 00:13:02,120
and then equipping them with enough voltage

271
00:13:02,120 --> 00:13:05,160
to keep them in the classical domain

272
00:13:05,160 --> 00:13:08,160
so that they don't start acting like quantum computers,

273
00:13:08,160 --> 00:13:09,440
which they actually are.

274
00:13:11,880 --> 00:13:16,880
So I think that it would be useful to try to

275
00:13:18,520 --> 00:13:21,080
relate this issue or resource cost

276
00:13:21,080 --> 00:13:23,360
to the issue of uniqueness,

277
00:13:25,440 --> 00:13:28,120
and as well as the issue of programmability.

278
00:13:28,120 --> 00:13:30,920
I'm not sure whether those are,

279
00:13:30,920 --> 00:13:33,000
I suspect those are distinct dimensions,

280
00:13:33,000 --> 00:13:37,440
but I think the usable area of that state space

281
00:13:37,440 --> 00:13:40,400
involves a lot of correlation between those dimensions.

282
00:13:44,080 --> 00:13:45,040
Yeah, I agree.

283
00:13:45,040 --> 00:13:48,520
I think that would be very interesting to explore.

284
00:13:48,520 --> 00:13:50,800
I don't know, Michael, if that resonated with you,

285
00:13:50,800 --> 00:13:52,920
because I feel like that touches on even

286
00:13:52,920 --> 00:13:56,160
pairing the Mortal Computation paper for me and Carl

287
00:13:56,160 --> 00:13:59,160
in the paper you shared with all of us, of yours.

288
00:13:59,160 --> 00:14:01,560
I think the two sort of start to get into that idea

289
00:14:01,560 --> 00:14:05,120
of uniqueness, programmability, and resource cost.

290
00:14:05,120 --> 00:14:06,960
And there are some different dimensions

291
00:14:06,960 --> 00:14:07,800
that you could explore,

292
00:14:07,800 --> 00:14:10,280
because yeah, I also just wanted to comment, Chris,

293
00:14:10,280 --> 00:14:14,960
that at least when I first was writing the paper

294
00:14:14,960 --> 00:14:16,440
and then I shared it with Carl,

295
00:14:16,440 --> 00:14:18,920
I wasn't thinking of the,

296
00:14:18,920 --> 00:14:20,840
I was thinking of biological systems

297
00:14:20,840 --> 00:14:22,560
as sort of an ideal target, right?

298
00:14:22,560 --> 00:14:25,680
You're sort of emulating some aspects of those systems.

299
00:14:25,680 --> 00:14:28,280
So I guess I'd be giving up some uniqueness, right?

300
00:14:28,280 --> 00:14:31,760
Because you said it lies in between the immortal laptop

301
00:14:31,760 --> 00:14:34,720
and the perfectly unique system itself.

302
00:14:34,720 --> 00:14:37,280
I think the other thing I was concerned with

303
00:14:37,280 --> 00:14:39,800
is the artificial intelligence community

304
00:14:39,800 --> 00:14:43,400
really liking the idea of immortal computation

305
00:14:43,400 --> 00:14:45,080
and that complete divorcing,

306
00:14:45,080 --> 00:14:48,280
because you're right, you do lose the moment you leave,

307
00:14:48,280 --> 00:14:51,360
even just a few steps away from immortal computation,

308
00:14:51,360 --> 00:14:54,160
that reproducibility, because it's,

309
00:14:54,160 --> 00:14:56,360
that substrate now is important.

310
00:14:56,360 --> 00:14:59,000
And then Carl and I argue even stronger,

311
00:14:59,000 --> 00:15:01,200
it's the morphogenesis too,

312
00:15:01,200 --> 00:15:02,880
and the changing process, which again,

313
00:15:02,880 --> 00:15:05,880
compliments your paper, Michael, as well.

314
00:15:05,880 --> 00:15:08,920
The idea is that change is also very important

315
00:15:08,920 --> 00:15:10,680
in that evolution over time,

316
00:15:10,680 --> 00:15:12,360
which is not something you're going to have

317
00:15:12,360 --> 00:15:14,480
on your typical deep neural network

318
00:15:14,480 --> 00:15:18,040
that just lives at the top of the von Neumann architecture.

319
00:15:18,040 --> 00:15:20,080
And then the last comment I just wanted to make, Chris,

320
00:15:20,080 --> 00:15:22,760
is yes, that's exactly the key,

321
00:15:22,760 --> 00:15:25,720
is that in-memory processing that we want.

322
00:15:25,720 --> 00:15:29,080
And that's why bringing ourselves as close as possible

323
00:15:29,080 --> 00:15:31,840
until we eventually just reach what we can,

324
00:15:31,840 --> 00:15:33,520
which is the Landauer limit,

325
00:15:33,520 --> 00:15:35,600
and getting ourselves real close to the hardware,

326
00:15:35,600 --> 00:15:39,040
now we're optimizing thermodynamic cost.

327
00:15:39,040 --> 00:15:41,320
And then of course, as you and Carl

328
00:15:41,320 --> 00:15:43,640
and everyone here has shown over time,

329
00:15:43,640 --> 00:15:46,960
that's the flip side to the information theoretic,

330
00:15:46,960 --> 00:15:49,880
variational free energy, but the thermodynamic free energy.

331
00:15:49,880 --> 00:15:51,960
But at the end of the day, we want to be there,

332
00:15:51,960 --> 00:15:53,960
because that's what biological systems are.

333
00:15:53,960 --> 00:15:57,160
They are much closer to the Landauer limit

334
00:15:57,160 --> 00:16:00,640
than pretty much anything in machine intelligence

335
00:16:00,640 --> 00:16:01,600
that we have today.

336
00:16:01,600 --> 00:16:03,840
And that would argue it's even getting worse

337
00:16:03,840 --> 00:16:07,680
because big, big transformers are really bad.

338
00:16:07,680 --> 00:16:10,960
And Carl also and I state the carbon footprint.

339
00:16:10,960 --> 00:16:12,720
So that's a good motivator.

340
00:16:15,960 --> 00:16:18,840
Yeah, I find that energetic analysis very compelling.

341
00:16:20,880 --> 00:16:25,440
You know, I'm very interested in why biological systems

342
00:16:25,440 --> 00:16:26,760
can be quite so efficient.

343
00:16:26,760 --> 00:16:30,240
And I think in many cases, they're efficient

344
00:16:30,240 --> 00:16:34,120
because they're able to use quantum resources

345
00:16:35,720 --> 00:16:38,400
when they're doing molecular computing.

346
00:16:38,400 --> 00:16:42,120
And maybe even when they're doing macromolecular computing.

347
00:16:45,200 --> 00:16:49,000
I guess the one other comment about a dimension

348
00:16:49,000 --> 00:16:51,120
that I wanted to throw in,

349
00:16:51,120 --> 00:16:53,800
which you mentioned a little bit about in the paper,

350
00:16:53,800 --> 00:16:58,800
was this dimension of explainability.

351
00:17:01,240 --> 00:17:06,240
Which AI is very obsessed with the explanation problem now.

352
00:17:10,160 --> 00:17:15,160
And as one gets away from reproducibility,

353
00:17:15,600 --> 00:17:18,520
the explanation problem gets harder and harder.

354
00:17:18,520 --> 00:17:21,600
And in the limit of a unique system,

355
00:17:21,600 --> 00:17:23,840
the explanation problem is infinitely hard

356
00:17:23,840 --> 00:17:26,280
because you can't do experiments.

357
00:17:26,280 --> 00:17:28,640
Because you can't replicate anything.

358
00:17:29,640 --> 00:17:34,640
So we have that other access to work with also.

359
00:17:42,440 --> 00:17:44,680
Maybe it would be kind of interesting

360
00:17:44,680 --> 00:17:48,000
is since you were bringing up these so far three axes

361
00:17:48,000 --> 00:17:52,080
that I caught, you know, uniqueness, programmability,

362
00:17:52,080 --> 00:17:55,320
explainability, we also did talk about resource cost,

363
00:17:55,320 --> 00:17:56,960
kind of putting out this grid

364
00:17:57,120 --> 00:18:00,600
and then you saw Michael and actually I presented,

365
00:18:00,600 --> 00:18:02,080
you guys would have seen in the paper,

366
00:18:02,080 --> 00:18:03,600
but I presented it a couple of times,

367
00:18:03,600 --> 00:18:05,320
like the different types of things

368
00:18:05,320 --> 00:18:09,240
that Carl and I consider variants of mortal computers.

369
00:18:09,240 --> 00:18:11,920
And so obviously Xenobot is a mortal computer.

370
00:18:11,920 --> 00:18:13,840
It actually had a lot more qualities

371
00:18:13,840 --> 00:18:16,880
after I re-read papers again, looking back.

372
00:18:16,880 --> 00:18:20,080
But even, you know, the silicon model

373
00:18:20,080 --> 00:18:23,720
that we had for the non-biological model from Ashby,

374
00:18:23,720 --> 00:18:27,400
we can never forget the great homeostat or allostat.

375
00:18:27,400 --> 00:18:29,880
And so maybe we could plot these a little bit

376
00:18:29,880 --> 00:18:34,080
on those axes too is what degrees that they're trading off.

377
00:18:34,080 --> 00:18:35,840
Obviously we need to figure out

378
00:18:35,840 --> 00:18:38,920
which one of these starts to get real close to the,

379
00:18:38,920 --> 00:18:41,520
like you said, Chris, the really unique.

380
00:18:41,520 --> 00:18:43,880
And then, you know, that would be the extreme one

381
00:18:43,880 --> 00:18:46,160
where explainability would be, you know,

382
00:18:46,160 --> 00:18:47,680
really, really, really difficult.

383
00:18:47,680 --> 00:18:49,400
And we could kind of plot where those are.

384
00:18:49,400 --> 00:18:52,400
That could be an interesting figure to show examples.

385
00:18:52,400 --> 00:18:55,080
And I'm sure, Michael, you probably have other examples

386
00:18:55,080 --> 00:18:57,600
that Carl and I might have missed.

387
00:18:57,600 --> 00:19:01,680
So there might be some other nice biological chimeric systems,

388
00:19:01,680 --> 00:19:03,400
things that are even less biological,

389
00:19:03,400 --> 00:19:04,840
but have a little bit of it.

390
00:19:04,840 --> 00:19:08,040
You did touch on nano technology as well.

391
00:19:08,040 --> 00:19:11,680
And in your, the polycomputing paper you shared with us.

392
00:19:11,680 --> 00:19:13,880
So maybe there might be some in soft robotics.

393
00:19:13,880 --> 00:19:15,240
There might be something there too

394
00:19:15,240 --> 00:19:19,400
that could count as variations to mortal computers

395
00:19:19,400 --> 00:19:21,160
that trade off on these axes.

396
00:19:21,160 --> 00:19:22,000
That's something else.

397
00:19:22,000 --> 00:19:24,720
I just thought of this, Chris explaining.

398
00:19:26,480 --> 00:19:29,720
Yeah, so another model system to think about.

399
00:19:29,720 --> 00:19:33,840
And by the way, we do have simulators of some of this stuff.

400
00:19:33,840 --> 00:19:36,440
So we should be in touch, you know,

401
00:19:36,440 --> 00:19:37,520
give you access to some of that

402
00:19:37,520 --> 00:19:41,000
because maybe you can do some analyses.

403
00:19:41,000 --> 00:19:42,680
You know, we have bioelectric simulators

404
00:19:42,680 --> 00:19:43,680
and things like that.

405
00:19:44,800 --> 00:19:47,400
You know, another kind of model system to think about,

406
00:19:47,400 --> 00:19:50,800
and this is something that Patrick is doing at the bench.

407
00:19:50,800 --> 00:19:53,080
And then I have somebody who is doing this,

408
00:19:53,080 --> 00:19:55,080
you know, the computational analysis of it,

409
00:19:55,080 --> 00:19:57,600
are these gene regulatory networks, right?

410
00:19:57,600 --> 00:20:00,120
And so the abstraction, of course, is quite simple.

411
00:20:00,120 --> 00:20:04,240
It's just, in the continuous case, it's a few ODE's

412
00:20:04,240 --> 00:20:05,760
and they just, you know, their nodes

413
00:20:05,760 --> 00:20:07,960
that turn each other on and off and that's it.

414
00:20:07,960 --> 00:20:09,720
But if you study these things,

415
00:20:09,720 --> 00:20:13,040
you find some really interesting features.

416
00:20:13,040 --> 00:20:15,840
For us, one of the most interesting things is that

417
00:20:15,840 --> 00:20:20,440
if you do temporary stimulation

418
00:20:20,440 --> 00:20:21,440
of the different nodes, right?

419
00:20:21,440 --> 00:20:23,080
So you just grab one of the node values

420
00:20:23,080 --> 00:20:25,360
and you crank it up or down for a little bit

421
00:20:25,360 --> 00:20:29,360
and you keep the structure of the network completely fixed.

422
00:20:29,360 --> 00:20:31,080
So you're not changing the weights,

423
00:20:31,080 --> 00:20:33,000
you're not changing the topology,

424
00:20:33,000 --> 00:20:34,520
the hardware is completely fixed.

425
00:20:34,520 --> 00:20:37,120
All you get to do is temporarily raise or lower

426
00:20:37,120 --> 00:20:39,320
the activation of any node

427
00:20:39,320 --> 00:20:41,080
and then you wait and you see what happens, right?

428
00:20:41,080 --> 00:20:42,440
So if you do that and if you treat it

429
00:20:42,440 --> 00:20:45,240
in a sort of behavioral science context,

430
00:20:45,240 --> 00:20:48,680
you can show things like habituation,

431
00:20:49,520 --> 00:20:51,120
basically six different kinds of memory,

432
00:20:51,120 --> 00:20:52,880
including Pavlovian conditioning.

433
00:20:52,880 --> 00:20:54,280
So these things learn.

434
00:20:54,280 --> 00:20:58,080
And we've been very interested in this question.

435
00:20:58,080 --> 00:20:59,920
I mean, so I have a couple of papers showing how they learn,

436
00:20:59,920 --> 00:21:03,160
but one of the really interesting things

437
00:21:03,160 --> 00:21:06,680
is because we don't let the hardware vary.

438
00:21:06,680 --> 00:21:07,960
So this is not a scenario

439
00:21:07,960 --> 00:21:09,080
where there's some kind of synapse

440
00:21:09,080 --> 00:21:12,000
whose weight gets tweaked by experience.

441
00:21:12,000 --> 00:21:14,320
The fact that they learn the most,

442
00:21:14,320 --> 00:21:16,480
to me, one of the most interesting things about it is,

443
00:21:16,480 --> 00:21:18,480
where is the learning stored?

444
00:21:18,480 --> 00:21:20,800
And this is something that all of the reviewers

445
00:21:20,800 --> 00:21:22,560
of the original two papers got hung up on

446
00:21:22,560 --> 00:21:25,480
because we say again and again, the hardware does not change.

447
00:21:25,480 --> 00:21:27,080
And then they all said, great,

448
00:21:27,080 --> 00:21:29,600
but then you can't have it

449
00:21:29,600 --> 00:21:32,280
because where could the memory possibly be, right?

450
00:21:32,280 --> 00:21:34,320
And it's this dynamical systems thing

451
00:21:34,320 --> 00:21:35,920
where they get chased into a regime

452
00:21:35,920 --> 00:21:39,760
where future stimuli are going to cause

453
00:21:39,760 --> 00:21:42,400
very different outcomes because of their history

454
00:21:42,400 --> 00:21:44,080
than past outcomes.

455
00:21:44,080 --> 00:21:47,000
But I wonder, and so this is what I was gonna ask you guys

456
00:21:47,000 --> 00:21:48,280
to kind of think of them,

457
00:21:48,280 --> 00:21:51,520
to talk about from your framework's perspective.

458
00:21:51,520 --> 00:21:54,120
I wonder if the business of uniqueness

459
00:21:54,120 --> 00:21:56,960
is related to this sort of issue.

460
00:21:56,960 --> 00:21:59,760
I think maybe called privacy or something like that,

461
00:21:59,760 --> 00:22:02,640
this idea that there is an inner perspective to a system

462
00:22:02,640 --> 00:22:05,000
that's had a certain set of experiences, right?

463
00:22:05,000 --> 00:22:06,800
It has a history in the world

464
00:22:06,800 --> 00:22:10,680
that is not available to outside observers.

465
00:22:10,680 --> 00:22:14,800
And this is, we spent a lot of time with my postdoc,

466
00:22:15,120 --> 00:22:17,440
Federico and I spent a lot of time thinking about,

467
00:22:17,440 --> 00:22:18,640
you look at a network,

468
00:22:18,640 --> 00:22:20,640
can you tell whether it's been trained?

469
00:22:20,640 --> 00:22:22,880
And if so, what hasn't been, like, can you read its mind,

470
00:22:22,880 --> 00:22:25,120
you know, this kind of neural decoding kind of thing

471
00:22:25,120 --> 00:22:26,640
because you're not gonna get it from the hardware.

472
00:22:26,640 --> 00:22:28,160
You can, the nodes are no different, right?

473
00:22:28,160 --> 00:22:31,080
So in fact, we have a visualizer

474
00:22:31,080 --> 00:22:34,000
that tries to show various aspects of it.

475
00:22:34,000 --> 00:22:36,760
And if you, you know, on the left and right of the screen,

476
00:22:36,760 --> 00:22:39,480
first you start off with the hardware of view of it.

477
00:22:39,480 --> 00:22:42,360
And that never changes throughout the whole time.

478
00:22:42,360 --> 00:22:43,760
But as it learns, right,

479
00:22:43,760 --> 00:22:46,320
over multiple experiences and stimuli,

480
00:22:46,320 --> 00:22:48,000
something absolutely changes.

481
00:22:48,000 --> 00:22:49,840
And then we have some ways of thinking about it.

482
00:22:49,840 --> 00:22:54,120
But this question of, can you, as an outsider,

483
00:22:54,120 --> 00:22:56,160
is there anything about mortal computation

484
00:22:56,160 --> 00:23:00,160
that speaks to this issue of what you can tell

485
00:23:00,160 --> 00:23:02,280
about a system as an outside observer

486
00:23:02,280 --> 00:23:04,920
versus what you know as the system yourself?

487
00:23:04,920 --> 00:23:06,480
You know, from the inner perspective,

488
00:23:06,480 --> 00:23:08,480
is that something you guys think about?

489
00:23:09,480 --> 00:23:11,400
Well, I'm gonna give a piece of it

490
00:23:11,400 --> 00:23:14,280
and then I'm gonna hope Carl can tag in a little bit

491
00:23:14,280 --> 00:23:18,000
because I think he can flesh this out a little bit better.

492
00:23:18,000 --> 00:23:20,720
So, and this might be confusion

493
00:23:20,720 --> 00:23:22,520
over what you might have explained, Michael,

494
00:23:22,520 --> 00:23:23,680
about the reviewers.

495
00:23:23,680 --> 00:23:26,280
So you said, I fixed the hardware

496
00:23:26,280 --> 00:23:28,600
and on top of that, I fixed the plasticity

497
00:23:28,600 --> 00:23:30,320
because you said we can't change the, you know,

498
00:23:30,320 --> 00:23:34,240
the values of the synapses or the connection strengths.

499
00:23:34,240 --> 00:23:37,200
And I do think mortal computer, mortal computation,

500
00:23:37,240 --> 00:23:38,200
we did address this.

501
00:23:38,200 --> 00:23:40,920
So Carl and I decomposed it in, again,

502
00:23:40,920 --> 00:23:42,960
it goes back to mills.

503
00:23:42,960 --> 00:23:44,520
But again, I might be misunderstanding.

504
00:23:44,520 --> 00:23:46,760
So we're gonna decouple the privacy

505
00:23:46,760 --> 00:23:48,560
and the observer perspective

506
00:23:48,560 --> 00:23:50,760
because I wanna hear what Carl might have to say to that.

507
00:23:50,760 --> 00:23:53,320
But for why learning would still happen,

508
00:23:53,320 --> 00:23:56,600
even when you fix those things, it's just the inference.

509
00:23:56,600 --> 00:23:59,000
And the way that we looked at it in mills

510
00:23:59,000 --> 00:24:01,800
was there's these different time scales of learning.

511
00:24:01,800 --> 00:24:05,880
So if you were to pin the structure of the S and mills

512
00:24:05,920 --> 00:24:09,760
and then pin L and say, you can't modify those.

513
00:24:09,760 --> 00:24:11,360
Well, we still had one more piece,

514
00:24:11,360 --> 00:24:13,600
which was the very fast time scale.

515
00:24:13,600 --> 00:24:16,400
And you talk in your poly computing paper,

516
00:24:16,400 --> 00:24:18,120
I've done a lot of work in that.

517
00:24:18,120 --> 00:24:19,680
Carl obviously has done a lot as well,

518
00:24:19,680 --> 00:24:21,840
predictive coding, predictive processing.

519
00:24:21,840 --> 00:24:23,680
We always have the inference dynamic.

520
00:24:23,680 --> 00:24:27,400
So the idea is that, and I'm sure you thought of this.

521
00:24:27,400 --> 00:24:28,600
This is why I was kind of surprised

522
00:24:28,600 --> 00:24:31,480
the reviewers were maybe not understanding.

523
00:24:31,480 --> 00:24:33,520
So there's like short-term plasticity, right?

524
00:24:33,560 --> 00:24:36,000
So the idea is that when you're doing

525
00:24:36,000 --> 00:24:39,080
expectation maximization in a predictive coding network,

526
00:24:39,080 --> 00:24:41,880
I can still change the neuronal activities,

527
00:24:41,880 --> 00:24:44,000
the firing rates or the spiking rates,

528
00:24:44,000 --> 00:24:45,760
depending on what model you're constructing.

529
00:24:45,760 --> 00:24:47,600
And the synapses never change

530
00:24:47,600 --> 00:24:49,480
and forget about the morphology

531
00:24:49,480 --> 00:24:52,120
because that's a whole nother ball game.

532
00:24:52,120 --> 00:24:53,680
And I would get adaptation.

533
00:24:53,680 --> 00:24:56,720
And there was a very interesting paper that came out,

534
00:24:56,720 --> 00:24:59,160
I don't know now, just like two weeks ago,

535
00:24:59,160 --> 00:25:03,240
Wolfgang Maas in spiking neural nets talked about,

536
00:25:03,240 --> 00:25:05,520
well, look, I don't need to modify the synapses.

537
00:25:05,520 --> 00:25:08,840
I'm gonna do everything in my spiking neural architecture

538
00:25:08,840 --> 00:25:11,400
with just homeostatic variables,

539
00:25:11,400 --> 00:25:12,880
which is, you didn't call it them,

540
00:25:12,880 --> 00:25:14,680
but that's just the adaptive thresholds.

541
00:25:14,680 --> 00:25:15,880
He's like, if these change,

542
00:25:15,880 --> 00:25:20,880
so we have this short-term kind of non-synaptic adaptation,

543
00:25:21,520 --> 00:25:22,560
you get all these effects.

544
00:25:22,560 --> 00:25:23,680
And he actually showed it again,

545
00:25:23,680 --> 00:25:25,240
it's a machine intelligence task,

546
00:25:25,240 --> 00:25:27,080
but showing in all these tasks

547
00:25:27,080 --> 00:25:31,720
without learning in the sense of modifying synapses.

548
00:25:31,720 --> 00:25:33,960
And that was very interesting that you can go very far.

549
00:25:33,960 --> 00:25:35,320
And I'll try to dig up that paper.

550
00:25:35,320 --> 00:25:38,920
It was something I wanted to go in more detail later myself.

551
00:25:39,840 --> 00:25:42,520
So in Mills, right, we're just saying,

552
00:25:42,520 --> 00:25:45,000
well, okay, we're obviously under mortal,

553
00:25:45,000 --> 00:25:46,760
but the inference dynamics

554
00:25:46,760 --> 00:25:49,960
and the fact that these still follow the gradient flow

555
00:25:49,960 --> 00:25:52,120
of the variational free energy

556
00:25:52,120 --> 00:25:53,640
that defines your system

557
00:25:53,640 --> 00:25:56,120
or your functionals that you're looking at

558
00:25:56,120 --> 00:26:01,200
would explain why that adaptation that you found would happen.

559
00:26:01,200 --> 00:26:02,240
And I'm sure you thought of that.

560
00:26:02,240 --> 00:26:05,000
I don't know why the reviewers specifically wouldn't have said,

561
00:26:05,000 --> 00:26:06,160
well, this doesn't make sense.

562
00:26:06,160 --> 00:26:07,600
How could you learn?

563
00:26:07,600 --> 00:26:09,240
If you would pin doll three,

564
00:26:09,240 --> 00:26:10,400
well, it's a static system.

565
00:26:10,400 --> 00:26:11,840
You are freezing it in time.

566
00:26:11,840 --> 00:26:14,200
And then that would baffle me.

567
00:26:15,120 --> 00:26:16,360
So that's my comment

568
00:26:16,360 --> 00:26:18,960
that I do think the framework definitely speaks to that

569
00:26:18,960 --> 00:26:21,760
because Carl and I were very adamant

570
00:26:21,760 --> 00:26:24,360
about the separation of time scales,

571
00:26:24,360 --> 00:26:26,120
at least these big time scales.

572
00:26:26,120 --> 00:26:27,640
I mean, there's all these intermediate ones

573
00:26:27,640 --> 00:26:29,640
that I'm sure you could bring up.

574
00:26:29,640 --> 00:26:30,960
And you need them all

575
00:26:30,960 --> 00:26:32,920
because there's a causal circularity

576
00:26:32,920 --> 00:26:35,800
if you want to build the most powerful type

577
00:26:35,800 --> 00:26:37,720
of mortal computer.

578
00:26:37,720 --> 00:26:39,720
And there was a sentence I can't remember

579
00:26:39,720 --> 00:26:42,880
because Carl and I have done many revisions of that paper.

580
00:26:42,880 --> 00:26:44,560
It might have been in one of the earlier ones

581
00:26:44,560 --> 00:26:46,160
where I mentioned something like,

582
00:26:46,160 --> 00:26:50,160
well, even though I'm seeing morphology as important,

583
00:26:50,160 --> 00:26:52,840
technically, if I was only allowed one,

584
00:26:52,840 --> 00:26:54,320
I still have mills.

585
00:26:54,320 --> 00:26:56,680
It's just a very simple search space, right?

586
00:26:56,680 --> 00:26:58,600
It's a, well, we know that you're here.

587
00:26:58,600 --> 00:27:00,880
You can't change the architecture.

588
00:27:00,880 --> 00:27:02,800
So we didn't break our framework.

589
00:27:02,800 --> 00:27:05,200
So that would allow us to subsume machine learning

590
00:27:05,200 --> 00:27:06,600
and say, well, machine learning

591
00:27:06,600 --> 00:27:09,360
is like this very, very narrow case.

592
00:27:09,360 --> 00:27:13,000
It is doing something that you mills could explain.

593
00:27:13,000 --> 00:27:16,680
It's not mortal, but at least it has like a fixed topology

594
00:27:16,680 --> 00:27:19,280
and synaptic plasticity is there.

595
00:27:19,280 --> 00:27:21,600
And we are just really, really speeding up

596
00:27:21,600 --> 00:27:24,160
the inference dynamics by making it one step

597
00:27:24,160 --> 00:27:27,680
because we don't use EM most times.

598
00:27:27,680 --> 00:27:30,240
And deep neural nets for sure we don't.

599
00:27:30,240 --> 00:27:33,320
So that was my comment about addressing the learning,

600
00:27:33,320 --> 00:27:36,760
the fact that things, if you fix so much,

601
00:27:36,760 --> 00:27:38,320
why would that still happen?

602
00:27:38,320 --> 00:27:39,920
And I definitely think mills,

603
00:27:39,920 --> 00:27:43,580
that piece of the backbone of mortal computation

604
00:27:43,580 --> 00:27:45,280
would speak to that.

605
00:27:45,280 --> 00:27:47,040
Now, in terms of the observer effect

606
00:27:47,040 --> 00:27:50,320
and what does that tell us about what's going on inside?

607
00:27:50,320 --> 00:27:52,160
I have tag team Carl.

608
00:27:52,160 --> 00:27:53,920
What do you have to say to that Carl?

609
00:27:58,280 --> 00:27:59,440
Right.

610
00:27:59,440 --> 00:28:00,720
Well, before I address that,

611
00:28:00,720 --> 00:28:04,600
which in my world is a very simple answer, you can't.

612
00:28:04,600 --> 00:28:08,360
This week I come back to the,

613
00:28:08,360 --> 00:28:10,200
so that was a really interesting exchange

614
00:28:10,200 --> 00:28:13,800
and really interesting examples there.

615
00:28:13,800 --> 00:28:15,760
And I was just thinking from the point of view

616
00:28:15,760 --> 00:28:20,760
of the sort of the classical flows and physics

617
00:28:21,360 --> 00:28:26,120
that would provide a simple picture

618
00:28:26,160 --> 00:28:28,120
of how on earth you can remember stuff

619
00:28:28,120 --> 00:28:30,160
without changing your connection weights.

620
00:28:30,160 --> 00:28:34,400
And I think Alex, you identified the key thing here,

621
00:28:34,400 --> 00:28:36,160
which is the temporal scale.

622
00:28:37,080 --> 00:28:40,240
So, well, where to start?

623
00:28:40,240 --> 00:28:42,120
It's interesting you introduced Wolfgang Maas

624
00:28:42,120 --> 00:28:45,560
because he for many years has been the king

625
00:28:45,560 --> 00:28:48,040
of liquid computation and neck of state machines,

626
00:28:48,040 --> 00:28:51,760
which is not, has the same kind of semantics

627
00:28:51,760 --> 00:28:54,880
as the liquid brain and it's a very powerful

628
00:28:55,880 --> 00:28:58,920
black boxy like kind of a dynamical system

629
00:28:58,920 --> 00:29:03,920
approximator that has been proposed as one architecture

630
00:29:04,560 --> 00:29:06,360
for doing predictive processing

631
00:29:06,360 --> 00:29:10,040
and model computation of the sort.

632
00:29:10,040 --> 00:29:13,360
But the key, I think the key point that has just been made here

633
00:29:14,920 --> 00:29:18,800
is that the dynamics matter

634
00:29:18,800 --> 00:29:23,800
and the dynamics are shaped by the landscape

635
00:29:24,600 --> 00:29:27,200
Lagrangian variation free energy, whatever you want.

636
00:29:28,200 --> 00:29:33,200
And that is a function of the implicit gradients

637
00:29:33,640 --> 00:29:36,840
that depend upon the sensitivity of all say the nodes

638
00:29:36,840 --> 00:29:38,840
in any given network.

639
00:29:38,840 --> 00:29:43,000
That sensitivity can either be read as a connection strength

640
00:29:43,000 --> 00:29:47,880
or it can just be read as a sensitivity,

641
00:29:47,880 --> 00:29:52,480
in terms of to what extent do I change my internal dynamics

642
00:29:52,520 --> 00:29:55,480
given this particular external perturbation.

643
00:29:55,480 --> 00:29:58,920
And of course, that becomes time and context sensitive

644
00:29:58,920 --> 00:30:00,200
with any nonlinearities.

645
00:30:00,200 --> 00:30:03,000
So if you're talking about a nonlinear system,

646
00:30:04,240 --> 00:30:09,240
then there is a, the bright line between the connection

647
00:30:09,280 --> 00:30:12,920
strengths and the current effective connectivity

648
00:30:12,920 --> 00:30:15,440
at this point of time in this context,

649
00:30:15,440 --> 00:30:18,160
in this part of face space or state space

650
00:30:18,160 --> 00:30:19,480
becomes very blurred.

651
00:30:20,320 --> 00:30:22,280
So if you're writing down the differential equations,

652
00:30:22,280 --> 00:30:23,440
you could go one of two ways.

653
00:30:23,440 --> 00:30:27,200
You could just write down a random differential equation

654
00:30:27,200 --> 00:30:32,200
with loads of variables representing interactions

655
00:30:32,640 --> 00:30:35,760
between different types of states

656
00:30:35,760 --> 00:30:38,320
and the response, the rate of change

657
00:30:38,320 --> 00:30:42,200
of any particular state that would entail

658
00:30:42,200 --> 00:30:44,400
the nonlinearity in question.

659
00:30:44,400 --> 00:30:46,160
Or you could arbitrarily say, okay,

660
00:30:46,160 --> 00:30:50,240
now one subset of these variables changes very, very slowly

661
00:30:50,280 --> 00:30:53,200
and I'm gonna call them connection strengths.

662
00:30:53,200 --> 00:30:55,920
And I'm now gonna lift those out of my equation.

663
00:30:55,920 --> 00:30:58,720
So I'm now left with a much simpler sort of

664
00:30:58,720 --> 00:31:00,280
autonomous differential equations

665
00:31:00,280 --> 00:31:02,960
that are now parameterized by other states

666
00:31:02,960 --> 00:31:05,360
that change very, very slowly.

667
00:31:05,360 --> 00:31:07,120
Mathematically, you haven't done anything

668
00:31:07,120 --> 00:31:09,480
but introduce a separation of temporal time scales.

669
00:31:09,480 --> 00:31:13,480
But in so doing, you have now got a different kind of rhetoric

670
00:31:13,480 --> 00:31:18,160
where initially you were talking about voltage sensitive

671
00:31:19,120 --> 00:31:20,720
receptors and sensitivity

672
00:31:20,720 --> 00:31:24,600
and contextualization conductances and the like,

673
00:31:24,600 --> 00:31:26,320
which sets the synaptic efficacy,

674
00:31:26,320 --> 00:31:28,640
which is fluctuating moment to moment.

675
00:31:28,640 --> 00:31:32,320
And now you're talking about these being the connection

676
00:31:32,320 --> 00:31:36,080
strengths, the parameters of your structure

677
00:31:36,080 --> 00:31:41,080
in a mills-like context or the strengths of your connections

678
00:31:43,000 --> 00:31:46,520
or weights in a machine learning context.

679
00:31:46,520 --> 00:31:49,120
But the only difference, I repeat, is just the time scale.

680
00:31:49,120 --> 00:31:51,960
So talking to Mike's example,

681
00:31:52,960 --> 00:31:56,160
how can you have memory without changing your connectivity?

682
00:31:56,160 --> 00:32:00,520
Well, you're just appealing to initial conditions

683
00:32:00,520 --> 00:32:02,080
in the context of a nonlinear dynamical

684
00:32:02,080 --> 00:32:03,760
and run of a dynamical system.

685
00:32:05,880 --> 00:32:08,560
At what point would you start calling this

686
00:32:09,680 --> 00:32:11,600
the kind of memory that could be encoded

687
00:32:11,600 --> 00:32:12,800
in terms of connection strengths?

688
00:32:12,800 --> 00:32:14,960
Well, in those kinds of systems

689
00:32:14,960 --> 00:32:19,440
where the key, not second order nonlinear interactions

690
00:32:19,440 --> 00:32:22,040
rest upon a subset of variables that change very, very slowly

691
00:32:22,040 --> 00:32:25,480
and you say, well, okay, under that adiabatic approximation,

692
00:32:25,480 --> 00:32:27,520
then we'll now call this a different kind of memory.

693
00:32:27,520 --> 00:32:30,560
And it's just because it's slightly slower.

694
00:32:30,560 --> 00:32:35,560
So I think it's, well, I liked the emphasis

695
00:32:36,680 --> 00:32:39,280
on the separation of time scales

696
00:32:39,280 --> 00:32:44,280
because I think that would have dissolved the reviewer's concerns

697
00:32:44,760 --> 00:32:47,760
if you were just talking about really fast learning

698
00:32:47,760 --> 00:32:50,240
in the moment that is all in the nonlinearities

699
00:32:50,240 --> 00:32:52,080
and the dynamics.

700
00:32:52,080 --> 00:32:54,360
I keep emphasizing the nonlinearities, Mike,

701
00:32:54,360 --> 00:32:59,360
because of that sort of the paradox of change.

702
00:33:00,000 --> 00:33:04,480
So as soon as you have nonlinear dynamics in any system

703
00:33:04,480 --> 00:33:07,600
that has at one particular time scale

704
00:33:07,600 --> 00:33:12,160
an attracting set or a random or a pullback attractor,

705
00:33:13,040 --> 00:33:14,960
you have that itinerancy,

706
00:33:14,960 --> 00:33:17,320
which means that there will be some form

707
00:33:17,320 --> 00:33:22,320
of changing sensitivity to all the things

708
00:33:22,520 --> 00:33:24,600
that I am coupled to.

709
00:33:24,600 --> 00:33:29,520
That is definitional of things that have that biotic

710
00:33:29,520 --> 00:33:31,520
or sort of characteristic kind of set.

711
00:33:31,520 --> 00:33:35,440
So, you know, the nonlinearities are certainly

712
00:33:35,440 --> 00:33:36,520
from a classical perspective,

713
00:33:36,520 --> 00:33:38,520
I think they're absolutely key here

714
00:33:38,520 --> 00:33:40,880
and resolve a lot of the distinctions

715
00:33:41,320 --> 00:33:44,840
and give you now a relatively simple picture

716
00:33:44,840 --> 00:33:49,840
that if there was some way to tell the next version of me

717
00:33:50,080 --> 00:33:54,720
where I started, give the next version of me

718
00:33:54,720 --> 00:33:57,720
my initial conditions in the past version of me,

719
00:33:57,720 --> 00:34:00,280
you can, I would imagine quite simply

720
00:34:00,280 --> 00:34:03,080
just write down systems that have this kind of memory

721
00:34:03,080 --> 00:34:04,920
which does not involve it anyway,

722
00:34:04,920 --> 00:34:07,920
a change in the connection weights.

723
00:34:07,920 --> 00:34:10,920
And I'm just wondering whether that, you know,

724
00:34:10,920 --> 00:34:15,160
that if you wanted to simulate that remarkable fact

725
00:34:15,160 --> 00:34:18,840
that the worms remember

726
00:34:18,840 --> 00:34:22,760
that they are on a two-headed trajectory

727
00:34:22,760 --> 00:34:24,920
even when they start again.

728
00:34:24,920 --> 00:34:27,000
I mean, I think the deep question here is

729
00:34:27,000 --> 00:34:30,000
how on earth did they inherit the initial conditions

730
00:34:30,000 --> 00:34:35,000
that characterised the termination of their parent

731
00:34:36,000 --> 00:34:37,960
or what they inherited from.

732
00:34:37,960 --> 00:34:41,520
I think, again, that speaks to this coupling

733
00:34:41,520 --> 00:34:43,600
between different temporal scales.

734
00:34:43,600 --> 00:34:46,000
You know, is this a messenger RNA, you know,

735
00:34:46,000 --> 00:34:49,760
and how does that propagate through to the electric fields

736
00:34:49,760 --> 00:34:51,880
and how does it get back top-down causation,

737
00:34:51,880 --> 00:34:54,160
get back in again, it's a fascinating example.

738
00:34:54,160 --> 00:34:56,200
And I've heard that before, I'm sure you've told me

739
00:34:56,200 --> 00:34:59,560
but I probably ignored it because it's so remarkable.

740
00:35:01,080 --> 00:35:02,800
Not easy to explain.

741
00:35:02,800 --> 00:35:05,520
In answer to the question, can you ever know

742
00:35:06,520 --> 00:35:09,520
what's going on inside a system?

743
00:35:09,520 --> 00:35:10,360
No.

744
00:35:11,600 --> 00:35:13,560
And I say no polemically from the point of view

745
00:35:13,560 --> 00:35:14,680
of the Fianco principle.

746
00:35:14,680 --> 00:35:18,160
You can never know what's beneath a Markov blanket.

747
00:35:18,160 --> 00:35:19,920
You can never know what's on the other side

748
00:35:19,920 --> 00:35:21,200
of a holographic screen.

749
00:35:21,200 --> 00:35:23,240
That's the whole point of a holographic screen

750
00:35:23,240 --> 00:35:24,720
or a Markov blanket.

751
00:35:24,720 --> 00:35:27,760
All you can do is bring a best guess

752
00:35:27,760 --> 00:35:30,640
and as if explanation to the poly computing,

753
00:35:30,640 --> 00:35:34,400
if you like, in the bulk on the other side,

754
00:35:34,400 --> 00:35:39,400
which means, you know, I think that's simple observation.

755
00:35:39,400 --> 00:35:44,400
The whole point of that screen or Markov boundary

756
00:35:46,320 --> 00:35:49,040
is that there is a conditional independence

757
00:35:49,040 --> 00:35:51,160
given what you can measure.

758
00:35:51,160 --> 00:35:55,560
So you can never know other than infer

759
00:35:55,560 --> 00:35:57,920
by what you measure from the behavior,

760
00:35:57,920 --> 00:36:01,560
the inputs and the outputs of a particular system.

761
00:36:03,000 --> 00:36:03,840
Amazing.

762
00:36:03,840 --> 00:36:05,520
So two questions then.

763
00:36:05,520 --> 00:36:09,920
One is, is there, is it just a flat no?

764
00:36:09,920 --> 00:36:12,640
Or is there a degree that is easier to know

765
00:36:12,640 --> 00:36:13,920
for certain kinds of systems?

766
00:36:13,920 --> 00:36:17,280
And then for sort of advanced living cognitive systems,

767
00:36:17,280 --> 00:36:18,120
it's really no.

768
00:36:18,120 --> 00:36:20,400
Or is it just like, is it always the same?

769
00:36:20,400 --> 00:36:22,560
Or is it a matter of degree?

770
00:36:22,560 --> 00:36:23,680
If you're directing at me,

771
00:36:23,680 --> 00:36:26,240
the answer I'm afraid is always no.

772
00:36:26,240 --> 00:36:29,600
But I don't mean that in a sort of pessimistic or,

773
00:36:29,600 --> 00:36:32,240
I mean, the question, you know,

774
00:36:32,240 --> 00:36:37,240
how do you infer what kind of Bayesian mechanics

775
00:36:39,640 --> 00:36:42,840
or poly computation is going on underneath the Markov blanket

776
00:36:42,840 --> 00:36:45,200
or inside a cell or inside a brain?

777
00:36:46,200 --> 00:36:50,000
That question is, of course, my day job

778
00:36:50,000 --> 00:36:52,960
and the day job of nearly every neuroscientist.

779
00:36:52,960 --> 00:36:54,960
It's peaking underneath the Markov blanket

780
00:36:54,960 --> 00:36:56,840
in a noninvasive way that doesn't destroy it

781
00:36:56,840 --> 00:37:00,800
to try and understand the mechanics

782
00:37:00,800 --> 00:37:03,240
and to test hypotheses about what is going on.

783
00:37:03,240 --> 00:37:06,720
But you're always testing hypotheses you will never know.

784
00:37:06,720 --> 00:37:09,440
So there will be situations where the functional anatomy

785
00:37:09,440 --> 00:37:12,640
or the architect reveals itself

786
00:37:12,640 --> 00:37:14,480
through noninvasive imaging, for example,

787
00:37:14,480 --> 00:37:16,640
or even invasive techniques

788
00:37:16,640 --> 00:37:19,280
of the kind that you use every day.

789
00:37:19,280 --> 00:37:22,480
But all you're doing is basically testing hypotheses

790
00:37:22,480 --> 00:37:26,480
about what you think the gerative model is under the hood.

791
00:37:27,440 --> 00:37:30,200
And once you know that or put it another way,

792
00:37:30,200 --> 00:37:31,920
if you knew the gerative model,

793
00:37:31,920 --> 00:37:33,040
then you know the Lagrangian.

794
00:37:33,040 --> 00:37:33,880
If you know the Lagrangian,

795
00:37:33,880 --> 00:37:36,120
you know the intrinsic or internal dynamics

796
00:37:36,120 --> 00:37:40,080
and you can tell a story about poly computing,

797
00:37:40,080 --> 00:37:42,120
tell a story about Bayesian mechanics.

798
00:37:42,120 --> 00:37:43,960
You can tell a story about perception.

799
00:37:43,960 --> 00:37:45,680
You can tell a story about memory.

800
00:37:45,680 --> 00:37:47,520
You can tell your basal cognition.

801
00:37:47,520 --> 00:37:51,320
But these are just stories predicated on the Lagrangian

802
00:37:51,320 --> 00:37:54,320
that governs the intrinsic dynamics

803
00:37:54,320 --> 00:37:56,880
and all that the free energy principle brings to the table

804
00:37:56,880 --> 00:38:00,000
is that you can express that Lagrangian

805
00:38:00,000 --> 00:38:03,640
as a function of a probabilistic gerative model.

806
00:38:03,640 --> 00:38:06,960
So your job is now to identify the form,

807
00:38:06,960 --> 00:38:09,840
the functional form and structure of that model

808
00:38:09,840 --> 00:38:12,320
and all the processes that it entails.

809
00:38:12,320 --> 00:38:13,720
But every time you do that,

810
00:38:13,720 --> 00:38:17,680
you're just testing a hypothesis theory of mind for me

811
00:38:17,680 --> 00:38:19,440
and theory of mind for your Xenobox

812
00:38:20,080 --> 00:38:22,520
and theory of mind for yourselves.

813
00:38:22,520 --> 00:38:25,600
Of course, you can break down at different scales.

814
00:38:25,600 --> 00:38:30,320
So it would be possible to ask about the sense making

815
00:38:30,320 --> 00:38:33,960
and sentient behavior of a single cell in my brain

816
00:38:33,960 --> 00:38:36,960
if you were able to isolate it and get inside there

817
00:38:36,960 --> 00:38:40,920
and do molecular biology or do cellular biology.

818
00:38:40,920 --> 00:38:44,720
But you would no longer be looking at my brain at that scale.

819
00:38:45,600 --> 00:38:49,040
And so, you know, but you could sort of cut across scales

820
00:38:49,040 --> 00:38:50,640
in the good old fashioned way

821
00:38:50,640 --> 00:38:53,280
and start to tell an internally coherent story

822
00:38:53,280 --> 00:38:56,520
about how it all fits together across scales.

823
00:38:58,280 --> 00:39:00,160
Could you back up what Carl said quickly?

824
00:39:00,160 --> 00:39:01,280
Not to interrupt you, Michael.

825
00:39:01,280 --> 00:39:02,120
Yeah, go for it, go for it.

826
00:39:02,120 --> 00:39:04,920
And it's partially gonna be a question for Carl

827
00:39:04,920 --> 00:39:06,600
as he triggered an interesting thought

828
00:39:06,600 --> 00:39:09,040
and then just to quickly say to you, Michael,

829
00:39:09,040 --> 00:39:12,400
that's also why I was hesitant because by definition

830
00:39:12,680 --> 00:39:15,840
since mortal computation rests on the Markov blanket

831
00:39:15,840 --> 00:39:18,400
and it's underwritten by the free energy principle,

832
00:39:18,400 --> 00:39:21,160
I also commit to that as well that no, the answer is no,

833
00:39:21,160 --> 00:39:23,760
you can't see what's under the Markov blanket.

834
00:39:23,760 --> 00:39:25,400
And that's why I was curious enough Carl

835
00:39:25,400 --> 00:39:28,360
would give me anything that may I just didn't know about.

836
00:39:28,360 --> 00:39:30,480
So that was one comment to you,

837
00:39:30,480 --> 00:39:32,960
to Carl and to everyone, really.

838
00:39:33,880 --> 00:39:38,880
So mortal computation also subsumes artificial systems.

839
00:39:38,880 --> 00:39:41,160
So this is where I was curious to know

840
00:39:41,160 --> 00:39:45,480
if Carl, we were to design the internal states,

841
00:39:45,480 --> 00:39:49,240
all the dynamics, we, again, we have the environment

842
00:39:49,240 --> 00:39:51,200
but let's just say you dissimulate it,

843
00:39:51,200 --> 00:39:52,920
you're designing the environment

844
00:39:52,920 --> 00:39:54,520
and you design the Markov blanket

845
00:39:54,520 --> 00:39:57,000
because we talk about even in our paper,

846
00:39:57,000 --> 00:39:59,400
potential sketches of things that you could use

847
00:39:59,400 --> 00:40:03,240
to build the boundary and the transduction pumps

848
00:40:03,240 --> 00:40:05,720
and all the sub pumps and to actually build

849
00:40:05,720 --> 00:40:08,400
a viable artificial organism.

850
00:40:08,400 --> 00:40:10,720
Now we have the internal dynamics.

851
00:40:11,480 --> 00:40:14,880
We are the designer and we have specified internal,

852
00:40:14,880 --> 00:40:17,360
external and the boundary.

853
00:40:17,360 --> 00:40:20,200
Is there something I'm missing that we would still,

854
00:40:20,200 --> 00:40:22,040
cause we've created the Markov blanket.

855
00:40:22,040 --> 00:40:23,560
So now we know what's on the other side.

856
00:40:23,560 --> 00:40:26,660
So the answer to Michael is not for natural systems

857
00:40:26,660 --> 00:40:30,200
that we obviously cannot, did not create, but we made it.

858
00:40:30,200 --> 00:40:31,520
So we made the internal systems.

859
00:40:31,520 --> 00:40:34,360
There's some other concept I'm missing

860
00:40:34,360 --> 00:40:36,040
cause you would be able to now say,

861
00:40:36,040 --> 00:40:39,440
I know everything cause I built the internal states,

862
00:40:39,440 --> 00:40:42,040
I specified every bit of the dynamics

863
00:40:43,160 --> 00:40:45,400
and let's say the environment, you know,

864
00:40:45,400 --> 00:40:47,240
we've constructed that,

865
00:40:47,240 --> 00:40:50,360
we've constructed the Markov blanket, the boundary.

866
00:40:51,280 --> 00:40:52,880
What about that case?

867
00:40:52,880 --> 00:40:55,760
Is it, now we are inspecting it cause we made it.

868
00:40:55,760 --> 00:40:58,880
So we obviously don't need to infer it, we know it.

869
00:40:58,880 --> 00:41:00,080
What about that?

870
00:41:00,080 --> 00:41:03,440
I was curious to know your thought of designed internal states

871
00:41:03,440 --> 00:41:07,240
and designed external states and designed Markov blankets.

872
00:41:08,200 --> 00:41:09,600
If that makes any sense.

873
00:41:09,600 --> 00:41:14,600
Yeah, Mike's gone to the door, so I'll respond to that.

874
00:41:18,440 --> 00:41:22,040
So, yeah, I'm not going to give you

875
00:41:22,040 --> 00:41:25,200
any deep philosophical insight.

876
00:41:25,200 --> 00:41:28,120
You don't already have, but just a very practical one.

877
00:41:28,120 --> 00:41:31,760
I mean, you know, what you just described

878
00:41:31,760 --> 00:41:36,760
is an application of the free energy principle

879
00:41:36,920 --> 00:41:41,920
as a method to simulate various mortal computations

880
00:41:43,880 --> 00:41:47,480
in the service of building hypotheses

881
00:41:47,480 --> 00:41:49,880
about how this thing might work mortally.

882
00:41:50,880 --> 00:41:54,200
So practically that's what we use the free energy principle for

883
00:41:54,200 --> 00:41:59,240
and the design is at least mathematically

884
00:41:59,240 --> 00:42:01,680
very straightforward in the sense I repeat,

885
00:42:01,680 --> 00:42:04,840
all you need to know is the gerative model.

886
00:42:04,840 --> 00:42:06,680
So all you need to be able to write down

887
00:42:06,680 --> 00:42:09,600
is a probability distribution of all the causes

888
00:42:09,600 --> 00:42:11,840
and consequences that constitute your system.

889
00:42:11,840 --> 00:42:12,840
That's it.

890
00:42:12,840 --> 00:42:14,480
If you can write that down

891
00:42:14,480 --> 00:42:17,720
and you can instantiate that in a von Neumann architecture,

892
00:42:17,720 --> 00:42:19,640
you then just solve the equations of motion

893
00:42:19,640 --> 00:42:21,680
that are the gradient flows.

894
00:42:21,680 --> 00:42:24,800
We will have a certain amount of component on that Lagrangian

895
00:42:24,800 --> 00:42:26,800
and you can simulate sentient behavior

896
00:42:26,800 --> 00:42:29,560
and sense making, perceptual actions, self-organization,

897
00:42:29,560 --> 00:42:31,600
everything that you want to do.

898
00:42:31,600 --> 00:42:33,320
Why would you ever want to do that?

899
00:42:33,320 --> 00:42:35,800
Well, in order to test hypotheses

900
00:42:35,800 --> 00:42:38,880
that this reproduces the kind of behavioral thing of interest

901
00:42:38,880 --> 00:42:41,600
which for something like me would be a psychiatric patient

902
00:42:41,600 --> 00:42:46,600
for something like might be a multicellular organism.

903
00:42:50,920 --> 00:42:55,920
So you are now using a simulation

904
00:42:58,800 --> 00:43:02,560
as a way of generating predictions

905
00:43:02,640 --> 00:43:05,880
that then you can match against the observable parts

906
00:43:05,880 --> 00:43:08,120
of the system of interest which adjust the surface.

907
00:43:08,120 --> 00:43:11,360
You have to adjust the action on that system

908
00:43:11,360 --> 00:43:14,760
and to the extent that the sensory inputs of that system

909
00:43:14,760 --> 00:43:16,360
are also known.

910
00:43:16,360 --> 00:43:17,840
That's all you have access to.

911
00:43:17,840 --> 00:43:20,080
So literally that's how we practically use

912
00:43:20,080 --> 00:43:21,920
active inference for example.

913
00:43:21,920 --> 00:43:26,920
We just create simulations of Bayesian mechanics

914
00:43:28,760 --> 00:43:30,440
in a given paradigm

915
00:43:30,440 --> 00:43:32,000
and then we adjust the gerative model

916
00:43:32,000 --> 00:43:34,760
more specifically the priors of that gerative model

917
00:43:34,760 --> 00:43:39,360
until it renders impurity observed choice behavior

918
00:43:39,360 --> 00:43:43,440
the most likely under the probability distribution

919
00:43:43,440 --> 00:43:47,200
of the actions of my simulated simulated.

920
00:43:47,200 --> 00:43:49,440
So in that sense you're specifying the structure

921
00:43:49,440 --> 00:43:54,440
but one could argue that even treating the laptop computer

922
00:43:55,080 --> 00:43:57,600
that is so non-unique

923
00:43:57,600 --> 00:43:59,760
because it affords the opportunity to abstract

924
00:43:59,760 --> 00:44:01,560
and do these simulations.

925
00:44:01,560 --> 00:44:03,640
I'll come back to Chris's point.

926
00:44:03,640 --> 00:44:05,040
Even then you don't actually know

927
00:44:05,040 --> 00:44:07,520
what's going on underneath the hood.

928
00:44:07,520 --> 00:44:09,000
And certainly in conversation

929
00:44:09,000 --> 00:44:11,240
with people designing some risk architectures

930
00:44:11,240 --> 00:44:14,680
and sort of looking at the most efficient buses

931
00:44:14,680 --> 00:44:19,680
they have to guess what's actually being passed here

932
00:44:19,840 --> 00:44:21,840
and there and measure it and get proxies

933
00:44:21,840 --> 00:44:24,320
like temperature and that kind of thing.

934
00:44:24,320 --> 00:44:26,640
You can certainly specify the initial conditions

935
00:44:26,640 --> 00:44:30,120
and the structure and you can do a,

936
00:44:30,120 --> 00:44:31,920
you can reboot and reset it.

937
00:44:31,920 --> 00:44:35,480
So you can to a certain precision specify

938
00:44:35,480 --> 00:44:38,440
the initial conditions and the structure

939
00:44:38,440 --> 00:44:42,320
of which the, you know, that the ensuing dynamics will occur

940
00:44:42,320 --> 00:44:47,320
but to actually know the message passing of a computer

941
00:44:47,580 --> 00:44:50,120
even in simulation, I think would be,

942
00:44:51,320 --> 00:44:54,280
I think you would be able to return to your hard no.

943
00:44:56,600 --> 00:44:58,640
Certainly on the level of, you know

944
00:44:58,640 --> 00:45:01,480
the quantum level that Chris was referring to.

945
00:45:01,480 --> 00:45:03,000
Again, it would be unknowable.

946
00:45:03,920 --> 00:45:04,960
But it's an interesting point

947
00:45:04,960 --> 00:45:07,680
but it does foreground the role of simulations in this,

948
00:45:07,680 --> 00:45:11,320
I think and it comes back to, you know, this, you know

949
00:45:11,320 --> 00:45:12,960
why do we want to know all this?

950
00:45:12,960 --> 00:45:15,480
Well, it's just to basically build,

951
00:45:16,840 --> 00:45:21,720
formalize hypotheses in terms of simulations

952
00:45:21,720 --> 00:45:24,600
that now embody our hypothesis

953
00:45:24,600 --> 00:45:26,920
and then look at the empirical system

954
00:45:26,920 --> 00:45:28,040
to see whether, you know

955
00:45:28,040 --> 00:45:30,680
that hypothesis was correct.

956
00:45:32,920 --> 00:45:36,480
Can I just add another point of view on this?

957
00:45:39,960 --> 00:45:43,600
And if we think about what we do in practice

958
00:45:43,600 --> 00:45:46,520
with ordinary computers

959
00:45:48,280 --> 00:45:50,400
where we have built the thing, et cetera,

960
00:45:51,360 --> 00:45:53,560
part of building the thing,

961
00:45:53,560 --> 00:45:55,900
it's not just assembling the hardware.

962
00:45:56,780 --> 00:46:01,220
We also put a lot of work into building these interfaces

963
00:46:01,220 --> 00:46:03,180
that we call programs.

964
00:46:04,340 --> 00:46:09,340
And so if I'm using some sort of debugging tool

965
00:46:09,340 --> 00:46:10,540
or something like that

966
00:46:10,540 --> 00:46:14,540
where I can run a program in one window

967
00:46:14,540 --> 00:46:16,100
and see what's happening

968
00:46:18,980 --> 00:46:23,980
at some level of the execution trace in some other window

969
00:46:24,980 --> 00:46:29,980
what I've done is constructed a Markov blanket effectively

970
00:46:32,860 --> 00:46:33,940
to use that language

971
00:46:34,980 --> 00:46:38,100
that has a bunch of different IO channels

972
00:46:39,180 --> 00:46:44,180
that access different parts of what's going on in the device.

973
00:46:44,940 --> 00:46:49,940
So we could think about from a biological perspective,

974
00:46:50,020 --> 00:46:51,500
we have these cells

975
00:46:52,500 --> 00:46:57,500
that come equipped with their own native IO channels.

976
00:47:01,860 --> 00:47:04,300
But there's nothing that says that we couldn't,

977
00:47:04,300 --> 00:47:08,900
in principle, build some more channels into the things

978
00:47:10,260 --> 00:47:12,100
so that we could see,

979
00:47:12,100 --> 00:47:15,980
we could see more about what was going on in the inside

980
00:47:15,980 --> 00:47:18,900
not by penetrating the Markov blanket

981
00:47:18,900 --> 00:47:22,860
but by adding some IO capacity to the Markov blanket.

982
00:47:23,780 --> 00:47:25,500
What does that mean physically?

983
00:47:25,500 --> 00:47:27,940
It just means you're using a different interaction

984
00:47:30,420 --> 00:47:34,220
because it's the interaction that defines the blanket

985
00:47:35,060 --> 00:47:39,180
as a set of information transmitting states.

986
00:47:40,180 --> 00:47:45,180
So I think we always have the hard no of the Markov blanket

987
00:47:48,980 --> 00:47:51,980
but we also from an engineering perspective

988
00:47:53,540 --> 00:47:58,540
because we can interact with these systems

989
00:47:59,940 --> 00:48:03,380
in ways that other parts of their environments

990
00:48:03,380 --> 00:48:06,740
can't interact with them or don't interact with them at least.

991
00:48:07,020 --> 00:48:10,780
We're a part of the environment that can open up

992
00:48:12,860 --> 00:48:15,140
new communication channels through the blanket

993
00:48:15,140 --> 00:48:17,660
by changing the interaction

994
00:48:18,580 --> 00:48:22,900
that effectively changes the state space

995
00:48:22,900 --> 00:48:25,020
in which the blanket is defined.

996
00:48:29,540 --> 00:48:32,700
Augmenting the Markov, let's say with reporters

997
00:48:32,700 --> 00:48:34,700
or with optogenetics, I would imagine

998
00:48:34,700 --> 00:48:36,420
it's a good example of this too.

999
00:48:38,220 --> 00:48:40,140
Yeah, well, I mean, in a sense,

1000
00:48:41,380 --> 00:48:43,420
FMRI is a good example of that.

1001
00:48:44,620 --> 00:48:45,580
Yes, yeah.

1002
00:48:45,580 --> 00:48:48,380
Right, we were just adding an IO channel to the brain

1003
00:48:48,380 --> 00:48:51,060
that wasn't there before.

1004
00:48:54,820 --> 00:48:56,220
I want to...

1005
00:48:56,220 --> 00:48:57,060
Sorry.

1006
00:48:57,060 --> 00:48:58,980
No, no, please, Carl, keep going.

1007
00:48:58,980 --> 00:49:01,500
No, I was just thinking out loud,

1008
00:49:02,500 --> 00:49:07,500
so the catch word in my world is sort of non-invasive,

1009
00:49:07,980 --> 00:49:09,580
brain imaging, and that has a meaning,

1010
00:49:09,580 --> 00:49:11,820
that you're non-invasive, but there isn't,

1011
00:49:11,820 --> 00:49:16,820
there is a whole centuries worth of legacy

1012
00:49:17,020 --> 00:49:19,900
of invasive studies and lesion deficit models

1013
00:49:19,900 --> 00:49:22,140
and depth recordings and the like,

1014
00:49:22,140 --> 00:49:24,980
which I think speak to this,

1015
00:49:24,980 --> 00:49:28,700
how far into the Markov blanket can you peer

1016
00:49:28,700 --> 00:49:32,380
without destroying the thing that you're trying to,

1017
00:49:32,380 --> 00:49:35,260
in the Heisenberg sense, trying to get out.

1018
00:49:35,260 --> 00:49:36,100
I was witching.

1019
00:49:36,100 --> 00:49:37,860
I'll shut up and have a quiet cigarette

1020
00:49:37,860 --> 00:49:39,860
while I listen to you now.

1021
00:49:39,860 --> 00:49:44,860
I just add that it's non-invasive kind of by convention

1022
00:49:44,940 --> 00:49:48,300
and that you're invading the brain with a magnetic field

1023
00:49:48,300 --> 00:49:50,260
that wasn't there before.

1024
00:49:50,260 --> 00:49:52,100
You're just not damaging it much.

1025
00:49:54,180 --> 00:49:56,940
Like Carl said, it's invasive,

1026
00:49:56,940 --> 00:49:59,700
but I wonder what that tells us then

1027
00:49:59,700 --> 00:50:03,260
about augmenting the Markov blanket.

1028
00:50:03,260 --> 00:50:05,780
Because yeah, Carl usually will say,

1029
00:50:05,780 --> 00:50:07,980
if we want to non-invasively understand it,

1030
00:50:07,980 --> 00:50:11,020
well, then yeah, you can't peer under the Markov blanket.

1031
00:50:11,020 --> 00:50:13,980
So mortal computation, I think,

1032
00:50:13,980 --> 00:50:16,500
kind of is connecting towards the idea

1033
00:50:16,500 --> 00:50:17,700
of what you're saying, Chris, right?

1034
00:50:17,700 --> 00:50:20,260
Because we can augment, we can add IO channels.

1035
00:50:20,260 --> 00:50:23,340
You are designing, engineering these things.

1036
00:50:23,340 --> 00:50:25,500
So now this was something that did not exist.

1037
00:50:25,500 --> 00:50:28,740
I don't know, Michael, how does that shed light

1038
00:50:28,740 --> 00:50:31,100
on the question that you originally wanted to get at,

1039
00:50:31,100 --> 00:50:34,420
which is, you know, peering at these internal states.

1040
00:50:34,420 --> 00:50:36,220
Because I think this is like an indirect way.

1041
00:50:36,220 --> 00:50:39,380
Because I was, Carl gave me great answer about,

1042
00:50:39,380 --> 00:50:40,460
well, if I just design,

1043
00:50:40,460 --> 00:50:42,460
because my brain goes to the ultimate engineering,

1044
00:50:42,460 --> 00:50:44,060
I'll just design it all myself.

1045
00:50:44,060 --> 00:50:46,940
And I even was thinking about if I build the hardware,

1046
00:50:46,940 --> 00:50:49,860
but Carl's right, even when you get to hardware,

1047
00:50:49,860 --> 00:50:51,420
you're guessing a lot of times,

1048
00:50:51,420 --> 00:50:53,300
even with the best educated guesses.

1049
00:50:53,300 --> 00:50:56,340
So there's still the Markov blanket

1050
00:50:56,340 --> 00:50:58,500
that you're not really breaching.

1051
00:50:58,500 --> 00:51:01,340
But if you perturb or change,

1052
00:51:01,340 --> 00:51:04,100
it's even like augmenting the cell membrane

1053
00:51:04,100 --> 00:51:05,980
with something in your lab's group

1054
00:51:05,980 --> 00:51:09,300
is, you know, an example of modifying these things.

1055
00:51:09,300 --> 00:51:11,540
What does that do to your question?

1056
00:51:11,540 --> 00:51:13,700
How does that shed light on what you're thinking?

1057
00:51:13,700 --> 00:51:17,300
Yeah, yeah, I mean, I even wanted to do,

1058
00:51:17,300 --> 00:51:19,580
talk about a much more annoying aspect of this,

1059
00:51:19,580 --> 00:51:22,620
which as I always do, which is to take it way down.

1060
00:51:22,900 --> 00:51:26,620
So nevermind brains, nevermind even cells, right?

1061
00:51:26,620 --> 00:51:30,220
My question, you know, also extends like that transition

1062
00:51:30,220 --> 00:51:32,140
from, you know, you got a pendulum,

1063
00:51:32,140 --> 00:51:35,220
you got a thermostat, you've got, you know, right?

1064
00:51:35,220 --> 00:51:36,700
And you can sort of build these things up

1065
00:51:36,700 --> 00:51:38,780
and then eventually at some point you get to a cell.

1066
00:51:38,780 --> 00:51:43,060
So I'm still curious about whether this impenetrability

1067
00:51:43,060 --> 00:51:44,740
goes all the way down.

1068
00:51:44,740 --> 00:51:47,020
So we can't read the mind of a pendulum either,

1069
00:51:47,020 --> 00:51:52,140
or there is some sense of progressive opacity

1070
00:51:52,140 --> 00:51:55,660
as you climb this sort of continuum of cognition

1071
00:51:55,660 --> 00:51:57,700
from extremely simple systems,

1072
00:51:57,700 --> 00:52:00,540
where I think the conventional story is,

1073
00:52:00,540 --> 00:52:02,740
hey, look, it's all third person accessible.

1074
00:52:02,740 --> 00:52:04,460
We know exactly what's going on,

1075
00:52:04,460 --> 00:52:06,660
but at some point you don't.

1076
00:52:06,660 --> 00:52:08,820
And so I'm curious, when does that happen?

1077
00:52:08,820 --> 00:52:10,580
And if we think there's a phase transition here,

1078
00:52:10,580 --> 00:52:12,500
or if we think this is smooth, I mean,

1079
00:52:12,500 --> 00:52:14,780
I tend to think everything is more or less smooth

1080
00:52:14,780 --> 00:52:16,740
in these cases, but maybe I'm wrong.

1081
00:52:18,300 --> 00:52:21,740
Yeah, that's one thing I wanted to kind of probe

1082
00:52:21,740 --> 00:52:23,460
a little bit is how does this play out

1083
00:52:23,460 --> 00:52:25,940
when you start, not start at the brain, you know,

1084
00:52:25,940 --> 00:52:29,060
where, okay, we can all agree that's sort of very opaque,

1085
00:52:29,060 --> 00:52:32,540
but what about from the most simple physics systems, right?

1086
00:52:32,540 --> 00:52:34,220
How do you get to that opacity?

1087
00:52:35,420 --> 00:52:37,460
Yeah, and then on the flip side,

1088
00:52:37,460 --> 00:52:39,740
and I'm conscious that I don't want to monopolize this up,

1089
00:52:39,740 --> 00:52:40,860
we only have five minutes left,

1090
00:52:40,860 --> 00:52:45,460
but something that's very interesting,

1091
00:52:45,460 --> 00:52:48,180
I think an implication of this is that we can't really know

1092
00:52:48,180 --> 00:52:49,340
and we're all inferring.

1093
00:52:49,340 --> 00:52:53,980
If you take the approach that I took in this memories paper,

1094
00:52:53,980 --> 00:52:58,180
where your future self has to make a lot of guesses

1095
00:52:58,180 --> 00:52:59,900
as to what your memories mean,

1096
00:52:59,900 --> 00:53:01,900
because they were written down by your past self

1097
00:53:01,900 --> 00:53:03,300
and you don't have all the metadata,

1098
00:53:03,300 --> 00:53:06,220
you have to now interpret these compressed n-grams,

1099
00:53:06,220 --> 00:53:09,260
then that leads to this kind of more,

1100
00:53:09,260 --> 00:53:11,540
the little kind of disturbing question is,

1101
00:53:11,540 --> 00:53:14,940
so we can't even tell what we used to think really, right?

1102
00:53:14,940 --> 00:53:17,780
We can sort of guess, but we don't really know then

1103
00:53:17,780 --> 00:53:21,580
if this is the case, if that impenetrability holds,

1104
00:53:21,580 --> 00:53:23,820
then it's there with respect to our past self

1105
00:53:23,820 --> 00:53:25,580
and our past memories too.

1106
00:53:25,580 --> 00:53:27,740
So that's kind of wild.

1107
00:53:27,740 --> 00:53:29,860
I don't know what we all have to say about those.

1108
00:53:31,300 --> 00:53:33,500
I'll refer to this wonderful thing

1109
00:53:33,500 --> 00:53:35,580
called the Conway-Cocon Theorem,

1110
00:53:35,580 --> 00:53:37,500
Conway of the Game of Life

1111
00:53:37,500 --> 00:53:40,420
and Cocon of Quantum Contextuality.

1112
00:53:41,940 --> 00:53:44,180
This was published three decades ago now

1113
00:53:44,180 --> 00:53:45,700
or something like this,

1114
00:53:45,700 --> 00:53:49,660
but they proved using mainly relativity theory

1115
00:53:49,660 --> 00:53:54,660
that if they considered a generic observational scenario

1116
00:53:58,340 --> 00:54:02,500
and they said, if in any generic scenario,

1117
00:54:02,500 --> 00:54:06,580
what you consider the observer has free will

1118
00:54:06,580 --> 00:54:08,820
in the following defined sense

1119
00:54:08,820 --> 00:54:13,260
that what the observer does is not completely determined

1120
00:54:13,340 --> 00:54:15,700
by that observer's past like tone.

1121
00:54:16,540 --> 00:54:19,860
So this is, if the observer is not subject

1122
00:54:19,860 --> 00:54:22,060
to local determinism,

1123
00:54:22,060 --> 00:54:23,500
then the thing being observed

1124
00:54:23,500 --> 00:54:26,020
is not subject to local determinism either.

1125
00:54:27,140 --> 00:54:28,980
And at the end of the paper, they say,

1126
00:54:28,980 --> 00:54:32,100
so one could ask, do we really mean

1127
00:54:32,100 --> 00:54:34,580
that electrons have free will

1128
00:54:34,580 --> 00:54:36,860
in the same sense that observers do?

1129
00:54:36,860 --> 00:54:38,460
And the answer is yes.

1130
00:54:40,420 --> 00:54:43,220
They say in their paper emphatically.

1131
00:54:43,220 --> 00:54:45,300
So if you did on the free will theorem,

1132
00:54:45,300 --> 00:54:47,500
yeah, I remember you're catching this.

1133
00:54:47,500 --> 00:54:51,700
Yeah, and if you drag quantum theory into the picture,

1134
00:54:51,700 --> 00:54:53,940
then you get an equally strong result

1135
00:54:53,940 --> 00:54:58,940
that any system has to be able to effectively choose

1136
00:55:03,660 --> 00:55:06,700
its own semantics for how it interprets

1137
00:55:06,700 --> 00:55:08,660
whatever incoming information is.

1138
00:55:09,660 --> 00:55:11,180
And if you take that choice away,

1139
00:55:11,180 --> 00:55:12,460
then you get entanglement.

1140
00:55:12,780 --> 00:55:15,820
The system ceases to have a separate identity.

1141
00:55:17,260 --> 00:55:20,380
So I think the answer,

1142
00:55:20,380 --> 00:55:22,860
kind of the principle answer about opacity is,

1143
00:55:22,860 --> 00:55:24,460
yeah, it goes all the way down.

1144
00:55:28,020 --> 00:55:29,620
Can I just follow up on that

1145
00:55:29,620 --> 00:55:34,100
and refer to Chris's in a screen hypothesis

1146
00:55:34,100 --> 00:55:36,860
and the notion of an irreducible Markov blanket?

1147
00:55:38,300 --> 00:55:40,620
If you've got a system

1148
00:55:40,620 --> 00:55:42,500
that has no internal Markov blankets,

1149
00:55:42,500 --> 00:55:44,660
has no deep structure or hierarchical structure

1150
00:55:44,660 --> 00:55:46,460
or heterarchal structure,

1151
00:55:46,460 --> 00:55:51,020
then that is, I think, where the hard no would apply

1152
00:55:51,020 --> 00:55:54,180
or the hard yes of unknowability.

1153
00:55:54,180 --> 00:55:56,580
But clearly if it has internal Markov blankets,

1154
00:55:56,580 --> 00:55:58,860
you can peel away and invasively

1155
00:55:58,860 --> 00:56:01,460
or non-invasively start to get within that.

1156
00:56:01,460 --> 00:56:04,260
So I think what you're talking about

1157
00:56:04,260 --> 00:56:08,180
is in that sort of vague gradation

1158
00:56:08,180 --> 00:56:10,740
of things that are noble and unknowable,

1159
00:56:10,740 --> 00:56:12,940
it's just the hierarchical, well,

1160
00:56:12,940 --> 00:56:16,380
the depth of the Markov blankets of the Markov blankets.

1161
00:56:16,380 --> 00:56:19,420
And there is a kernel of either

1162
00:56:19,420 --> 00:56:21,380
an irreducible Markov blanket,

1163
00:56:21,380 --> 00:56:23,060
which you can never get into

1164
00:56:23,060 --> 00:56:27,140
because you change the thing itself.

1165
00:56:27,140 --> 00:56:28,580
But there's also a limiting case

1166
00:56:28,580 --> 00:56:30,260
from the point of view of classical physics,

1167
00:56:30,260 --> 00:56:32,060
which is when the inner states,

1168
00:56:32,060 --> 00:56:35,340
intrinsic, the internal states are the empty set.

1169
00:56:35,340 --> 00:56:39,660
So things like inert particles or stones

1170
00:56:39,660 --> 00:56:42,260
don't have internal states,

1171
00:56:42,260 --> 00:56:45,340
they just have Markov boundary states.

1172
00:56:45,340 --> 00:56:47,420
So I think you're absolutely right to think of this

1173
00:56:47,420 --> 00:56:52,420
as a gradation, that there are inert things

1174
00:56:53,500 --> 00:56:54,860
that are defined operationally

1175
00:56:54,860 --> 00:56:57,740
in the sense that their internal states are the empty set.

1176
00:56:58,820 --> 00:57:02,060
And you could also say that there are sessile things

1177
00:57:02,060 --> 00:57:03,260
that don't have active states.

1178
00:57:03,260 --> 00:57:07,220
So all of the states of this kind of particle

1179
00:57:07,220 --> 00:57:09,460
are just sensory states, they're just inputs.

1180
00:57:10,460 --> 00:57:12,940
Inputs that can also influence the outside.

1181
00:57:12,940 --> 00:57:14,660
There's no restriction that sensory states

1182
00:57:14,660 --> 00:57:17,300
have to not influence the outside.

1183
00:57:17,300 --> 00:57:18,980
So there still can be observable.

1184
00:57:18,980 --> 00:57:21,540
And then you get to things that now have a non-antiactive

1185
00:57:21,540 --> 00:57:26,540
sector of the holographic screen or the Markov blanket.

1186
00:57:28,180 --> 00:57:29,300
And these are things that move.

1187
00:57:29,300 --> 00:57:32,020
So you might think these are natural kinds

1188
00:57:32,060 --> 00:57:34,420
that have mobility or motility.

1189
00:57:34,420 --> 00:57:39,420
And then you get to things that whose internal states now

1190
00:57:40,580 --> 00:57:43,500
have a Markov blanket within them.

1191
00:57:43,500 --> 00:57:46,700
And these would be the kinds of things that can fan.

1192
00:57:46,700 --> 00:57:48,820
And these are usually multicellular things

1193
00:57:48,820 --> 00:57:52,020
or certainly compartmentalized things.

1194
00:57:52,020 --> 00:57:56,180
I think at each stage, the no ability depends upon

1195
00:57:56,180 --> 00:58:00,460
whether either the internal sets are empty or not,

1196
00:58:01,300 --> 00:58:03,460
or they are irreducible in the sense

1197
00:58:03,460 --> 00:58:05,060
there are no Markov blankets within them.

1198
00:58:05,060 --> 00:58:08,580
So I think it's a nice, simple mathematical picture

1199
00:58:08,580 --> 00:58:12,900
of that gradation that speaks exactly to the electron

1200
00:58:12,900 --> 00:58:16,380
through to the pendulum, to the thermostat

1201
00:58:16,380 --> 00:58:20,340
through to a smart thermostat that starts to worry

1202
00:58:20,340 --> 00:58:22,940
about whether you want it warmer or colder or not

1203
00:58:22,940 --> 00:58:25,380
and starts to pan ahead and moves from homeostasis

1204
00:58:25,380 --> 00:58:26,620
to allostasis.

1205
00:58:26,620 --> 00:58:29,700
All of this would speak to at different scales,

1206
00:58:29,740 --> 00:58:32,180
equipping Markov blankets and Markov blankets

1207
00:58:32,180 --> 00:58:33,660
and inducing a deep structure.

