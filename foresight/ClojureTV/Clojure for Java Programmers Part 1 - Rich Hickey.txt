Hi. I'm Racheke. I'm here to talk about Closure, which is a programming language I wrote for
the JVM. This particular talk is oriented towards people who program in Java or C sharp
or C++ in particular that I'm not going to presume any knowledge of Lisp. So you might
find some of it tedious, although I am preparing for a talk I'm going to give at ecoup to the
European Lisp workshop, where I'm going to talk about the ways Closure is a different
Lisp. So maybe some of this will be interesting to you in that respect. But that's the nature
of this talk. It's going to be an introduction to the language, a fly-by tour of some of
the features. I'll drill down into some of the others. I started to ask this question
before, but I'll just ask it again to sort of see, is there anyone here who knows or
uses any flavor of Lisp, common Lisp scheme or Closure? Okay. So mostly no. I presume
a lot of Java or anything in that family, C++, C sharp, Scala, anyone? You must be playing
with it, right? How about functional programming languages like ML or Haskell District? Guys,
anyone a little? Don't really want to raise their hands about that one. Okay. That's good. In
particular, I think coming from that background, you'll understand a lot of this straight away.
How about dynamic programming languages, Python, Ruby or Groovy? Yes, about half. And I asked
before, Closure, we have a few people with their toes in the water. The other key aspect
of Closure that would matter to you if you're a Java programmer is whether or not you do
any real multi-threaded programming in Java or in any language. Yes? So some. So you use
locks and all of that nightmare stuff. I'm a practitioner. I programmed in C and C++ and
Java and C sharp and Common Lisp and Python and JavaScript and a bunch of languages over the
years. Way back, this same group, I think it's the same lineage, was the CIG. And when I first
started to come, I started to teach C++ to the CIG and it became the C++ and CIG and eventually
the C++ and Java CIG and now the Java CIG. So back in the 90s, early 90s and mid 90s, I taught
C++ and advanced C++ to this group and ran study groups and I've come back tonight to apologize
for having done that to you and to try to set you off on a better track. So we're going to look at
the fundamentals of Closure and it will be also of Lisp in many ways. But I'm going to say Closure,
don't take offense, all these things or many of the things I say are true of Closure are true of
many Lisp. I didn't invent them, they're not unique to Closure, but some things are. They will
look at the syntax and evaluation model. This is the stuff that will seem most unusual to you if
you've come from a, you know, compile link run language and one of the curly brace C derives
like Java. Then we'll look at some aspects of Closure, sequences in particular and the Java
integration which I imagine will be interesting and I'll finally end up talking about concurrency,
why Closure has some of the features it does and how they address the problems of writing
concurrent programs that run on the new and indefinitely, you know, for the indefinite future
multi-core machines and I'll take some questions. At some point in the middle we'll probably take
a break. I don't know exactly where that's going to go. So what's the fundamentals of Closure?
Closure is a dynamic programming language and dynamic has a lot of different meanings, in
particular it's dynamically typed, that would be an expectation you'd have of Python or Ruby or
Groovy. It achieves that dynamic nature by being a Lisp and I'll talk more about that. I don't see a
lot of people who know Lisp here but that doesn't mean there isn't a bias against Lisp. How many
people have seen Lisp and said oh my god, I can't believe the parentheses and I would say I'd hope
you'd put that bias aside for the purposes of this talk. It ends up that for people who have not used
Lisp, those biases have no basis and for most people who have given it a solid try, they vanish and in
fact many of the things you consider to be problems with Lisp are features down the line. But having
said that, Closure is a very different Lisp. It's syntactically much leaner than a lot of Lisp's. It
has fewer parentheses. It uses more data structures in its syntax and as a result I think is more
succinct and more readable. So maybe the time to try Lisp again. Another aspect of Closure is that
it's a functional programming language and again I'm going to talk in detail about these things. For
now you can just say that means a focus on immutability in your programs. So write programs
primarily with immutable data structures and if you're coming from another Lisp, this will be an
area where Closure is definitely different. I mean different decisions about the data structures
in Closure. The third leg of Closure, you know, it sort of stands on four points. It's dynamic,
it's functional, it's hosted on the JVM and it embraces the JVM, its host platform. There are
ports of other languages that sort of just sit on the JVM. There are ports of, for instance,
common Lisp that sit on the JVM. But they don't really connect very well. For a number of reasons.
One is they're implementing a standard. The standard was written before Java was written and
you know, there's just no merging the type systems. On the other hand, Closure was written for the
JVM and so it's very heavily integrated with it. So not only does it reside there, which is a
benefit because you can run it if that's your environment. But it embraces it, which means the
integration is good and it's pretty transparent to go back and forth. The fourth aspect of Closure
is the concurrency aspect. You know, I work in C-Sharp with guys writing broadcast automation
systems. They're, you know, they're multi-threaded. They have all kinds of nasty stuff going on,
multiple connections to sockets, lots of databases, you know, data feeds from all kinds of places
and it's not fun writing programs like that that need to share data structures amongst
threads and to have them get maintained over time and have everybody remember what the locking
model is. It's extremely challenging. Anyone who's done any extensive multi-threaded programming
with the locking model knows how hard it is to get that right. So Closure is an effort on my
part to solve those problems in an automatic way with language support. And the last thing
is, you know, it is an open source language and it's very transparent, the implementation
and everything else is up there for you to see. We started to talk about this before. Why use a
dynamic language? Some people are very happy, of the people who are programming Java, how we are
happy about that. They like Java. They have no complaints. Okay, not too many. It ends up that I
think many Java programmers look at people who are using Python or Ruby and being very
productive and I think justifiably MV, their productivity, the succinctness, the flexibility
they have and in particular how quickly they can get things done. And it ends up that that is a
fact of the static languages, especially the ones like Java, that they're inherently slower
because of the amount of, well some people call it ceremony, but you have to go through to
communicate with the language. It slows you down. So flexibility is a key thing. You would look for
in a dynamic language. Interactivity is another key point. Again, this goes back to Lisp. Lisp has
pretty much always been an interactive language. And that means a lot of things. In particular, it
means that when you've got Lisp up and running, you feel like you are engaged with an environment as
opposed to, you know, shoveling your text through a compiler phase to produce something else out the
other end. So that interactivity is kind of a deep thing. The repul is part of it. That means read,
eval, print, loop, and I'll talk about that in detail in a little bit. Dynamic languages tend to be
more concise. That doesn't mean that static languages can't be. Haskell in particular is very
concise. But the curly brace languages are not concise. Java is probably a great example of a
language that's not concise. And that's just not a matter of tedium. It's a matter of where is your
logic? How far apart is your logic? How spread out is it? Can you see what you're thinking about? Or is it
in pieces? Is it spread out by a bunch of things that are not about your problem? Dynamic languages are
definitely more suitable for exploration. There's a certain aspect in which static languages are like
concrete. That's a good aspect when you're trying to, you know, finish. In some systems, you know,
concrete is going to be more resilient. It's, you know, it's more resilient to change. It's more
structured and it's rigid. On the other hand, that's not necessarily the kind of materials you
want to be working with when you're trying to figure out what your structure should look like in
the first place. So dynamic languages are better for exploration. And in particular, what I like
about dynamic languages and Lisp fundamentally, and I think in a way other languages don't achieve, is it
lets you focus on your problem. You can, with Lisp and its ability to do syntactic abstraction, suck
everything out of the way, except the problem. And for me, you know, when I discovered Lisp, I was
pretty expert, C++ programmer. I said to myself, what have I been doing with my life? It was that, that
big a deal. So there are many dynamic languages. I'm going to talk about closure and I will do, you
know, bashing of other languages, but I will try to highlight why you might choose closure over some of
the other options. Because in particular now, I think it's a great thing that there are many dynamic
languages available for the JVM and dynamic languages are supported as a concept in the Java community.
A Java one, there was plenty of presentations on Jython and JRuby and Groovy and these other languages.
And Sun has hired some of the developers of these languages and given it, you know, kind of official
support as something that's viable to do on the JVM. So you're going to see mixed language
programming being accepted in Java shops. So how do you pick? I think you can categorize languages in one
dimension pretty straightforward. Are they a port of a language that exists somewhere else or were they
written for the JVM? Ports have a bunch of challenges. One is there is a canonic version out there because
most of these languages are not defined by a specification. They're defined by a canonic
implementation. So they're CRuby. They're CPython. Those are really the languages. And the other things are
ports which have to struggle to follow along with the C version. The other problem ports have is a lot of
the infrastructure for the languages, especially the ones that don't perform very well, are written in C. In
other words, to get the library performance they need, the support libraries for Python are written in C. So
an effort to port Python to Java means having to replicate those C libraries. So there's that. I would
say the main appeal to a ported language is if you already have an investment in Ruby or Python or you
have to really love the language designs. That's a good way to go here. I would say if not, if you're just
starting from scratch, you may find that a language that's native to the JVM is going to give you better
integration. You know the version you're using is the canonic version. The canonic version of Ruby is a JVM
language. The canonic version of Closure is a JVM language. And I would say of the two, Groovy is going to let
you do what you do in Java, except a little bit more easily. Fewer semicolons, more dynamic, there are
some builders, there are some idioms, there are closures, sort of the fun of dynamic programming and a lot of
the similar syntax to Java. So I think if you're just interested in dynamic and want to continue to write
programs that are like your Java programs, Groovy can't be touched. Closure is not about writing programs like
your Java programs. Closure is about realizing what's wrong with your Java programs and doing something
different. And so you'll find some of that through the talk. So Closure itself, it inherits from Lisp, an
expressivity and elegance I think is unmatched. Depending on your mindset, you may or may not agree, but this is a
certain mathematical purity to lambda calculus and the way it's realized in Lisp. The uniformity of the syntax
is elegant. Closure also has very good performance. Again, I'm not going to get involved in any language
bashing, but I'm pretty confident no other dynamic language on the JVM approaches the performance of
Closure in any area and is unlikely to. But everybody's working on performance.
Certainly.
We've converted them.
So the performance is good. I made a point before starting the talk that the objective and objective of
Closure is to be useful in every area in which Java is useful. You can tackle the same kind of problems. I don't
write web apps and put stuff in and take it out of the database kind of applications. I write scheduling
systems, broadcast automation systems, election projection systems, machine listening systems, audio
analysis systems, and I write them in languages like C-sharp and Java and C++ and Closure can be used for those
kinds of problems. It doesn't mean they can't also be used for web apps and people did that right away with
Closure and database and UI stuff. But it has that same kind of reach. One of the nice things about Java is it
has a wide range. Closure has direct wrapper free access to Java. Some of the ported languages have to use
wrappers because those languages have their own object systems that imply a bunch of dynamic features that they
have to glom on top of Java objects when you interoperate with them. Closure was designed to provide direct
access to Java. It looks like Closure, but it's direct. Closure being a list is extensible in a deep way.
And we'll talk a little bit more about how you get syntactic extensibility through macros. And then Closure, I
think, is completely unique amongst the languages on the JVM in promoting immutability and concurrency, much more
so than even Scala, which is often talked about as a functional language, but it's also a
option. Closure is really oriented towards writing concurrent programs. And immutability for its other benefits
outside of concurrency. So how does Closure get to be these things? It is a list. It is a list. It is a list.
Again, put what you think about Lisp aside. I'll explain what that means in depth as I go into each of these
points, but Lisp in general is dynamic in that way, interacting with an environment, having a REPL,
having sort of introspection capabilities on the environment, being able to modify things in a running
program, or all characteristics that make it dynamic. A fundamental feature of all Lisp, if they want to be a
Lisp, is that code is represented as data. And again, I'll explain that in detail. There is a reader, which is part of the
implementation of code as data, sort of something in between your text and the evaluator. Being a Lisp
means having an extremely small core. You'll find when you contrast Closure to other languages, even
languages that are theoretically lightweight like Python or Ruby, Closure has way less syntax than those languages, far
less complexity. In spite of the fact that they appear easy. Lisp generally have tended to emphasize lists.
Closure is not exactly the same way. It is an area where Closure differs from Lisp in that it frees the abstract
of first and rest from a data structure, the con cells. And in doing so, offers the power of Lisp to many more data
structures than most Lisp do. So there's that sequence thing, and I'll talk more about that in detail. And syntactic
abstraction. Again, we have abstraction capabilities with functions or methods in most languages. Lisp
take that to the next level by allowing you to suck even more repetition out of your programs when that
repetition can't be sucked out by making a function. Okay, so we'll dig down a little bit more. What does it mean to do
dynamic development? It means that there's going to be something called a REPL, a read eval print loop in which you
can type things and press enter and see what happens. I guess we should probably do that. So this is a little
editor. It's kind of squashed in the screen resolution, but down below is the REPL. This is Closure in an
interactive mode, and we can go and we can say plus one, two, three, and we get six. We can do other things
Java like, I'll show you some more of that later. But the general idea is that you're going to be able to type
expressions or in your editor, say please evaluate this. I mean, I can go up here to meth.pi and hit the key
stroke that says evaluate this and see below, we get that. And that's kind of what it feels like to develop. I'm
going to show you even more after I explain what you're looking at because I don't want this talk to be yet another
people are shown Lisp and not having had to explain to them what they're looking at. So we're going to do that first.
But you have this interactive environment. You can define functions on the fly. You can fix functions on the fly.
You can have a running program and fix a bug in a running program. And that's not like being in a mode in a
debugger where you have the special capability to reload something. It's always present. If you build an
application with some access to the ability to load code, either a remote REPL connection or some way to do that,
your running production systems will have this capability to have fixes loaded into running programs.
In general, there isn't the same distinction between compile time and run time. Compiling happens all the time.
Every time you load code, every time you evaluate an expression, compilation occurs. So that notion of phases of
compilation is something you have to relax when you're looking at a language like closure. And I'll show you the
evaluation model in a second. I talked a little bit about the introspection, but that's present. You're sitting at a
REPL, closure is there, closure has namespaces, you can get a list of them. Closure has symbols, you can get a list of
those. You can look inside the infrastructure that underlies the run time and manipulate it. And that's what I
mean by an interactive environment. I just don't mean typing things in. I mean, there is a program behind your
program that is the run time of closure. And that's accessible. If I say something that you don't
understand, you can ask for clarification. I'm endeavoring to try to come up with the ideal way to explain
the list to people who have never seen it. And this is what I've come up with, which is to talk about data. Lots of
languages have syntax. You can talk about Java. You can talk about here's main and here's what public means and static.
And then you can dig into arguments to a function and things like that. But we're going to start here with data, in
particular data literals. I think everybody understands data literals from languages they're familiar with. You type in
1, 2, 3, 4, and you know that's going to mean 1,234 to your program. So closure has integers, they have
arbitrary precision. They can get as large as your memory can support. And the promotion of
small integers to larger integers while arithmetic is going on is automatic. It supports doubles as the floating
point format. Those are doubles. Those are big D double Java doubles. When you type them in. They're right. Right.
They're Java doubles, but they're the big D doubles. So one of the things you're going to see about closure is
everything is an object. Okay. All numbers are boxed. At least until you get inside a loop where I can unbox them. But it's a
language in which numbers are boxed. Unlike common lists where you have access under the hood to use tagged integers and
tagged numbers, which is more efficient than allocating them on the heap, no capability of doing that in the JVM.
There's been talk about it, them adding it, which is stunning to me. Apparently the guy, there's this guy, John Rose, at
Sun who really does understand this very well. And has talked about all kinds of really neat features, which if they make it into the
JVM would make it stunning. Like tail call elimination and tagged numbers. But in the absence of that, numbers are boxed so
that everything can be an object and can be treated uniformly. You have big decimal literals. You have ratios. 22 over 7 is
something. It's not divide 22 by 7. It's a number. It's a number that's not going to lose any information versus dividing 22 by
7 and either truncating or converting into a floating point format where you will lose information. So ratios are first class.
String literals are in double quotes. They are Java strings. Same thing. Immutable. No conversions, no mapping. Being, again,
being a native JVM language means I can just adopt the semantics of Java literals. I don't have to take strings from a
language spec that said, for instance, they could be mutable. I have to force it on the JVM by having my own type and conversions to
and from. So because I'm an immutability oriented language, I'm very happy with Java's definition of a string being an immutable
thing. So closure strings are Java strings. Yes. Is there any way to work for a cent on the line you do this? In other words,
to say there's a total of 1.234, or unless you know the center meter or something like that, you don't show, you don't know.
No. Try Frank. Have you ever seen it? No. Oh, you will love it. You can add all kinds of units and figure out how many, you
know, balloons of, you know, hydrogen it would take to move a camel across this much distance. It's amazing. Units for
absolutely everything. Old, ancient Egyptian unit, it's really, it's fantastic. The guys are just a fanatic about precision,
making sure you don't lose anything, but you can arbitrarily multiply all kinds of units. Everything is preserved. Everything works
correctly. Fantastic. Frank. Frank, F-R-I-N-K. But no. Frank? Frank is a language for the JVM. It's its own language, but it's a lot of
fun. I've seen the guy talking. He just, he has some great examples. You know, some involve how many belches it would take to, you
know, move a hot air balloon to the moon and things like that. Okay, so we have string literals and double quotes. We have
characters that are preceded by a slash, backslash. So that's a character literal, and that's a big C character, a
Java character. Now we're going to get to two things that are possibly a little bit different, because they're not first class
things in Java. One would be symbols, which are identifiers. They can't contain any spaces. They have no
adornments. Symbols are used as identifiers primarily in code, but they can be used for other things as well. They're
first class objects like strings. If you have one of these things, you can look at it, and it will be a symbol, closure
laying symbol. Fred and Ethel are two symbols. That's correct. The other thing, closure has our keywords, which are
very similar to symbols, except they always designate themselves. So they're not subject to evaluation or mapping to
values by the compiler, like symbols are. So symbol might be something you would use for a variable. You could make Fred be
equivalent to five. You could never make colon Fred be equal to five. Colon Fred will always mean itself. So when it gets
evaluated, the value of the keyword Fred is the keyword Fred. It's sort of an identity thing. And they're extremely useful.
They're very useful in particular as keys and maps because they're very fast for comparison, and they print as themselves
and read as themselves. That will make a little bit more sense in a minute. There are booleans. This is different from
Lisp, although there is still null as false, nil as false. But in addition, there are proper true and false, mostly for the
purposes of interoperability. It ends up that you can't solve the nil becoming false problem. At least I couldn't. So there are
true and false, and there for use in interoperability with Java, you can use them in your closure programs as
well. But conditional evaluation in closure looks for two things. It looks for false or nil, which is the next thing I
want to talk about. Nil means nothing. It also is the same thing in closure as Java null. Didn't have to be, but it is. So you can
rely on that. So nil means nothing, and it's the same value as Java null. So when you get back nulls from Java, they're
going to say nil. Nil is a traditional Lisp word. But I like it because also traditionally in Lisp, if you can say if nil,
it will evaluate to the else branch, because nil is false. Nil is not true. So that's another literal thing, that nil. There are
some other things. There are regex literals. So if the reader reads that, it's just a string regex, exactly the same syntax as
Java's, preceded by hash, will turn into a compiled pattern. So at read time, you can get compiled patterns, which you can then
incorporate in macros and things like that, which is very powerful. And shows how that delineation between compilation and runtime is a
little bit fungible. Correct. And there's a good reason for that. And the reason is empty list is no longer as special as it was once you
have empty vector and empty map. However, the sequencing primitives, the functions that manipulate sequences return nil when they're
done, not the empty list. So that aspect of being able to test for the end of iteration with if is still there. So closure sits in a
unique point. He's asking about aspects of closure that differ a little bit from common list and scheme. There's like a long standing fight between
what should the difference between false, nil and the empty list be? Should they be unified? They are in common list. Should there be some
differences? There are some differences in scheme. Closure actually does some of both. There is false. However, there are some
differences. However, nil is still testable in a conditional. It does not unify nil and the empty list, which is a difference from common
list. However, all of the sequencing or list operations, when they're done, return nil, not the empty list, which is an important thing for
common list like idioms, where you want to keep going until it says false, as opposed to having to test for empty explicitly, which you
would have to do in scheme. Does anybody know scheme here? But you know both, so you know what I'm talking about. For everyone
else, I wouldn't worry too much about that because you wouldn't have presumed nil would have been the empty list, right?
Probably not. Okay, so those are the atomic things. They can't be divided, right? That's what atomic means. You can't, a number is in a
single thing. But there are composite or aggregate data structures. Enclosure and they're kind of the core abstractions of computer
science. One is the list. And in this case, I mean very specifically, the singly linked list. And even more specifically, the singly
linked list in which things get added at the front. So when you add to a list, you're adding at the front. The list is a chain of things, which
means that finding the nth element is a linear time cost, right? It's going to take n steps to do that. On the other hand, taking
stuff on and off the front is constant time, because that's the nature of a singly linked list. So it has all the promises, all the
performance promises of a singly linked list with stuff at the front. And it's literal representation is stuff inside
parentheses separated by spaces. There's no need for commas. You'll see some commas. Commas are white space enclosures.
They're completely ignored. You can put them in if it makes you feel better or makes things somewhat more readable, but they're
not actually syntaxed or not considered by the evaluator. So any questions about lists? Stuff in parentheses?
Stuff in parentheses?
Right. Well, these commas, the ones between 12345 and Fred Atollusi are actually English commas. But there are some commas.
For instance, when we get down to maps here, you see commas inside the data structure. Those are ignored. Those are white space.
I don't support any commas inside numbers. The printed representations of numbers enclosures are those of Java.
In lists? No, in lists, they grow at the front. Cons A onto something makes A the first thing in that list. And that's true of
closure, too. Yes? Absolutely not. All of these data structures are unique to closure. I'm only giving you some very high-level descriptions of their
representation and their performance characteristics, but we're going to talk about that in a little bit later on.
Is it based on the retail structure of lists? Absolutely not. All of these data structures are unique to closure. I'm only giving you some very high-level descriptions of their representation and their performance characteristics, but what we're going to find out later is all of these things, in particular, I'm talking about adding to lists. All of these data structures are immutable. And they're persistent, which is another characteristic. I will explain that in a little bit later on.
These are very different beasts, and they have excellent performance, yet they're immutable, and it's sort of the secret sauce of closure. Without these, you can't do what I do in the language.
That's correct. Again, how this gets interpreted, we're going to talk about it in a little bit. Right now, what you're looking at is a list of three symbols. You may end up within your program, a data structure that's a list of three symbols. You may pass this to the evaluator and say,
evaluate this, in which case it's going to try to interpret, it's going to try to evaluate each of those symbols and find out its value and treat the first one as if it was a function. But we're not there yet.
So that is a list of three symbols. The list at the end is a list of one symbol and three numbers. So heterogeneous collections are supported. In all cases, I didn't necessarily show them everywhere, but they are.
It's not a list of something. It's a list. It can contain anything and any mix of things. Okay, with lists, the next thing is a vector uses square brackets. That should imply I would hope for Java programmers and people from that domain array, right? Square brackets mean arrays.
Well, they do now. So a vector is like an array. In particular, it supports efficient indexed access. It's an expectation you would have of a vector you wouldn't have of a linked list. That getting at the 50th guy is fast. It's not going to be 50 steps to do that.
And the closure vectors meet that performance expectation. Fast indexing. In addition, it's a little bit like Java util vector or array list in that it supports growing and in this case, at the end.
And that also is efficient, as efficient as your expectation would be of a ray list. That's a constant time operation to put things at the end.
Similarly, it can hold anything. The first is a vector of five numbers. The second is a vector of three symbols.
All the collections can be heterogeneous. Okay, so far. So that's going to behave like an array in terms of being able to find the element quickly.
And finally, as a core data structure, we have maps. And a map is like, well, it's like a Java map or any kind of associative data structure in providing a relationship between a key and a value.
Each key occurring only once and having a mapping to a value. So the way they're represented is in curly braces. And they're represented simply as key, value, key, value, key, value.
Again, the commas don't matter. So they're whitespace. They get eliminated. For instance, in the second map you see there, that's a map of the number one to the string ethyl and the number two to the string fred.
You don't need the commas. And the expectation with the map is that it provides fast access to the value at a particular key.
There are usually two kinds of maps you would encounter in ordinary programming languages. One would be sorted. Some sort of sorted map, in which case the access is going to be typically log n to find a particular guy, depending on how many things are in the map.
Because they use trees or red, black trees and things like that. And closure does have sorted maps. The one you get from the literal representation like this is a hash map. And the expectation of a hash map is constant or near constant time lookup of values at keys.
And that maps to hash tables. So what you have in the closure literal maps is the equivalent of a hash table. It's fast.
Okay, so far?
I think if I introduce another key, another key in this.
Another key?
A, A.
It will be replaced. Do you want to?
The last number replaced.
Correct. There are only one instance of a key in a map. Is that your question?
Yes.
Yes, so if you were to say, the function that...
No, I'm saying if I type it out like this.
Yes?
With the comma and up the cv with a again. Is it an error or is it just a replacement?
It's probably a replacement. I say in the same thing, yes. I don't think it's an error. That's a good question. I might type it in later for you.
Okay.
Yeah, I mean...
It's the same thing.
Well, but there's no associated values. So Fred will be there. So let's talk about sets. The fourth thing I'm showing you here is sets.
Sets are a set of unique values. Each value occurs only once in the set. And really the only thing the set can do for you is to tell you whether or not something's in it.
There's no associated values. Just does the set contain this key? Do you have a question?
Yes.
There are sorted sets and hash sets. Same thing as with the maps. The sets here are hash sets. So no, the order is not retained.
You can request a sorted set and the order will be the sort order.
Does that answer your question? Okay.
What is the test for equality?
What is the test for equality? Equal. The equal sign is the test for equality. And equality means the same thing for everything in closure. It means equal value.
You'll see that closure definitely de-emphasizes identity and completely. In fact, there is an identity function and I have yet to use it.
Closure is about values. Identical contents are identical from by equals. That's made faster than you might imagine by caching hash values.
But equality is equality of value in closure.
And this immutability helps?
Immutability helps certainly. Well, if you've ever read Henry Baker's paper on EGAL, closure implements EGAL. Finally.
If you haven't, don't worry about it.
So yes, equality is equality of value.
All right. Yes. Hi, Rob.
If you were going to Java, you would need an active Java?
No, you can make arrays and you can interact with Java arrays that are arrays of either objects or native arrays.
You can say float array and size and you'll get an array of floats.
So you have the ability to do Java stuff. I'm going to emphasize the closure data structures because they let you do what closure lets you do.
You can access Java, but if you start accessing mutable things, some of the things closure can do for you, we can't do.
It doesn't mean you're not allowed to do them.
But there's no point in me showing you how to interact with the Java, right? Except to show you the syntax, which I might later.
So the last point about this is that everything nests. A key in a map can be another map. It can be a vector.
Anything can be a key or a value. Because of this equality semantics, there's no problem having a vector or a map whose keys are vectors.
That's perfectly fine. So if you needed to use tuples as keys, you know, pairs of things as keys, that's just completely doable.
Well, you can get the hash of a vector.
Correct? Right.
Well, it depends on what you're doing. I would imagine that really complex structures are not frequently used as keys, but they could be.
Can that be helped? Yes. The fact that these are hash by default means that once and once only the hash value of some aggregate structure will be calculated.
And that will be cached. So there's a quick hash test. Otherwise, we do the deep value check.
But again, I don't think you're going to encounter complex data structures as hash values that often, but using kind of small things like tuples or other small maps as keys is tremendously useful.
It's really, really handy to not even have to think about that.
I think we got one other closure program arrived. Who can possibly attest, independent of me, how closure's performance is? How's closure's performance?
Fine to me, but I've actually been showing all of this.
Right. Well, now there's some extra numeric goodness in there. But these data structures are pretty good. What's the reality?
The reality of these data structures is I've tried to keep them all within one to four times a Java data structure, the equivalent Java data structure.
In other words, hash map, vector. Well, similar lists are pretty straightforward.
So they're within striking distance. The B side is in a concurrent program, there is no locking necessary for use with these data structures.
If you want to make an incremental change through data structure in a certain context, there's no copying required to do that.
So some of these other costs that would be very high with immutable data structure vanish.
So you have to be very careful in looking at that.
The other thing that's astounding to me, at least, is that the lookup time, again, the add times are higher than hash map.
But the lookup times can be much better because this has better hash cash locality than a big array for hash table.
Okay, we're all good on this. I probably have to move a little bit quicker. Yes, more quickly.
There is destructuring. Yes, I actually won't get to talk about that today, but there is destructuring.
There is not pattern matching. Okay, but there is destructuring to arbitrary depth of all of these.
Destructuring means a way to easily say, I want to make this set of symbols that I express in a similar data structure,
map to corresponding parts of a complex data structure on past.
Clojure has that. It has some really neat destructuring capabilities.
All right, so what's the syntax of closure? We just did it.
I'm not going to talk about semicolons, curly braces, you know, when you have to say this, when you have to have a new line, or anything else.
Because the structure of a closure program is a data structure or a series of data structures.
There is no other stuff. There are no rules about where things go. There are no precedence rules. There's nothing else.
You write a closure program by writing the data structures I just showed you. That's it.
I'll show you.
So you write a program by writing data structures. The data structures are the code.
That has huge implications. It's, you know, it is the nature of LISP.
There's a fancy name for it called Homo Iconicity.
And it means that the representation of the program is done in the core data structures of the program.
Which means that programs are amenable to processing by other programs because they're data structures.
So I'm not going to talk anymore about text-based syntax because there is no more.
Now, many people claim of LISPs, well, LISPs has no syntax. And that's not really true.
It doesn't have all this little fiddly character syntax necessarily.
There is syntax to the interpretation of the data structures.
You know, in those, you're going to see a lot of lists. They have different things at the front.
The thing at the front will tell you the meaning of the rest.
All right. So let's talk a little bit about evaluations. How does this all work?
This is, we should all know, from Java or many other languages like Java.
It types our program into a text file and we save it.
And then we send those characters of that text to the compiler who has a very involved, you know, abstract syntax tree
and parser and lexer that interpret the rules of the language.
This is what constitutes a character. This is what constitutes a number.
And then furthermore, you know, if you've said if and you put parens and then you said some stuff and you put a semicolon
and you're still in this construct called if, things like that.
It knows all about that and it deals with the text.
And it will tell you if you've met the requirements in terms of it being a valid program.
And then it will turn it into something that can run.
In the case of Java, that something will be bytecode.
And it will go into a class file or a draw file. We know this.
And then there's a separate step, which is called running.
We take that stored executable representation and we ask it to happen.
Usually, in this case, we'll say, you know, Java dash something, class file, and it will run.
And it will run and then it will end and it will be over.
And we could try again if we didn't like it.
That's the traditional edit compile run.
Be disappointed, start over.
Oh, correct.
I'm talking about the development process.
But yes, the runtime is just that long.
Until you realize it's not working and you have to ask everybody to please wait for our downage while we fix it.
That's the difference.
If you read about Erlang, which is getting a lot of press, they'll tell you about phone switches and how that's really not allowed.
And Lisp was doing this for a very long time, this kind of live hot swapping of code and running systems.
I think it goes more, in this case, it's less about the production thing than it is about what's the nature of developing a program.
Because as a developer, you know, seeing it run and saying, ooh, that was bad.
I wonder what happened.
I wish I had run it in debug mode.
I wish I had put a breakpoint somewhere interesting.
And I'm really sad that I spent an hour calculating that data and dropped it on the floor because I have to do it again with the breakpoint in.
That's a lot different experience than keeping your program around and having that data stay loaded and fixing your function and running it again without starting over.
So that's what happens in closure.
You take the code, text could be characters.
There is character representation and what you showed there can be represented in characters in ASCII.
It does not go first to the evaluator.
It goes to something called the reader.
And this is the core part of what makes something a Lisp, which is that the reader has a very simple job.
Its job is to take the description.
I just told you, you know, keyword starts with a colon and a list is in parentheses and a map is in curly braces and it's pairs of stuff.
Its job is to take those characters and turn it into data structures.
The data structures I described, you start with the parent, you say stuff, you close the parent, that's going to become a list when the reader is done with it.
You start with square brackets, that's going to become a vector when the reader is done with it.
So what comes out of the reader are data structures.
And what's unique about a Lisp enclosure is that the compiler compiles data structures.
It does not compile text.
It never sees text.
What the compiler gets handed is maybe a list with three symbols in it or a vector with five numbers in it.
That's actually what the compiler has.
It has a data structure in hand with actual data in it, not text.
And it compiles it and in the case of closure, it is a compiler.
There are many, well, there are actually many lists that are interpreters, but many people believe that Lisp is interpreted.
It's certainly easy to make an interpreter for Lisp that would take those data structures and on the fly produce the values they imply.
But closure is a compiler and in particular closure compiles those data structures to Java bytecode right away.
There is no interpretation in closure.
So it's a compiler that produces bytecode just like Java C does.
And because it's an interactive environment, it presents that bytecode right away to the JVM to execute.
And it executes right away and you can see the effect.
When you're in the REPL, you have AVM, right?
You have one thing.
So yes, your environment is your program.
Your compiler is in your program.
Yes.
Yeah, most commercial lists give you tools to take out the compiler in production, mostly because they don't want you giving away their compiler.
Normally, there's no reason to prevent that because it's a useful thing to have, particularly when you want to load code later to fix problems.
You're going to need that compiler there.
So in closure, there's no strip out the compiler option.
We'll see that there is a core of closure.
The data structures are written in Java.
The special operators are written in Java.
And then most of the rest of closure is written in closure.
There's no native code.
There's no native code.
Closure is completely a pure Java project.
There's no native code.
There's no C libraries.
It's all Java, either generated by Java itself or generated by closure.
It does not turn off the verifier or anything like that in order to get performances.
There have been some schemes that tried to do that.
Closure is completely legit that way.
So when we have this separation of concerns between the reader and the evaluator, we get a couple of things.
One of the other things we get is we don't have to get the text from a file, right?
We can get it right from you.
You just saw me type right into the REPL, an expression.
Never went through a file, never got stored.
So the first thing you get is this kind of interactivity of you can just type in stuff and say go.
That's a big deal.
I mean, if you've been programming in Java or C++ long enough to remember when the debuggers didn't give you the ability to evaluate expressions at a break point,
you can't remember how hard that was.
You always have that capability here to have expressions directly evaluated.
What else do we get from this?
Well, we get the ability to skip the characters completely.
For instance, it's quite possible to write a program that generates the data structures that the compiler wants to see and have it send them to the compiler to be evaluated.
Program generating programs are a common thing in this kind of an environment.
Whereas this kind of stuff when you're doing it with text is really messy.
By the way, what observation does Charlie actually give a good way to make the same?
The art forms I know, because of compliance requirements that they have,
we might be very comfortable with code producing stuff going into a reader or a programmer against a programmer that produces stuff.
But is that option of saying it's always like the first in this production environment in the influence of COVID to be as a security problem?
Well, I mean, that's a security policy thing, whether or not you expose this in a production system.
So I'm talking about you could if you needed to.
You could have that over a secure soccer channel and have it be just an administrator who knows what they're doing, have that capability.
Because the alternative is downing your system.
If you don't have that, and of course, opening this in a production system, that's completely a policy thing.
It has nothing to do with the language.
Except if your language doesn't let you do it, you can't do it.
That's fair.
So it does.
The other thing is that these data structures, you might write this program and have this happen directly.
Then you might say, I like this program.
Let me take those data structures and there's a thing called the printer, which will turn them back into that,
which you could store and so they could sign off on and say this is the canonic program,
which our program generated that we're going to use.
And we'll lock that down and do whatever.
Yes?
So are the data structures physical files?
No, they're in memory data structures.
The ones your program would see.
So, you know, an instance of closure laying persistent vector to the compiler.
The compiler's got to deal with it, figure it out.
So there's one more thing that this allows, and this is the secret sauce of all lists, including closure,
which is what would happen.
I mean, it's fine to sit standalone and write a program that generates a program.
But what would happen if we said, you know what?
We're handing these data structures to the compiler.
It would be great if the compiler would let us participate in this.
If they could send us the data structures.
When we write a real program, a very small program, and give it back different data structures,
then we could participate very easily in the extension of our language.
Because this compiler, it's going to know how to do with those types of data.
It's going to know what to do with the vector.
It's going to know what if means and a couple of other things.
But there'll be new things that we'll think of that we'd love to be able to set.
When you have something you'd love to be able to say in Java, what do you have to do with it?
You have to beg, son, and wait for years and hope other people beg for the same things and you get it.
That's it.
You have no say.
You have no ability to shape the language unless that's completely not what it's about.
It's about getting you in the loop.
And in fact, the language itself has a well-defined way for you to say,
this is a little program I'd like you to run.
When you encounter this name, I don't want you to evaluate it by the way.
I'd like you to send me that data structure.
I know what to do with it.
I'm going to give you back a different data structure and you evaluate that.
That's called a macro.
And it is what gives lists and closure syntactic abstraction and syntactic extensibility.
Can that happen in the context of the namespace?
Yes, it can.
There are namespaces in enclosure and they allow me to have my cool function and you to have your cool function.
Cool function.
Yes.
So that's what makes Lisp amazing.
It's something that I won't have time to dig deeply into tonight.
If you can come away with at least the understanding that that's how it works, that's how it's possible.
And the fact that these are data structures here makes it easy.
You could theoretically say, oh, I could write something and if the compiler could hand me the abstract syntax tree,
I could navigate it with some custom API and do whatever.
It's not nearly the same, though, when what the compiler is handing you are those three data structures.
I just showed you that every program knows how to manipulate and has a wildly huge library that directly can manipulate.
So that's how Lisp works.
I'm going to try to speed it up a little bit.
In closure, unlike Java, everything is an expression.
So you know in Java there's a difference between declarations and statements and expressions.
There's no distinction in closure.
Everything is an expression.
Everything has value.
Everything gets evaluated and produces a value.
Sometimes that value is nil, not particularly meaningful, but everything is an expression.
So the job of the compiler is to look at the data structures and evaluate them.
There's a really simple rule for that.
It's slightly oversimplified, but in general you can understand it this way.
All those data literals I showed you, right, symbols, numbers, character literals, vectors, maps, sets,
are all evaluated by the compiler to represent themselves, except lists and symbols.
Lists and symbols by default are treated specially by the evaluator.
So when it reads a list of symbols in particular, it's going to do some work.
It's not just going to return the list of symbols to your program.
It's going to try to understand them as an operation, which I'll show you in a second.
So symbols are going to try to, the compiler is going to try to map to values, like variables.
You know in a variable you can say int i equals five.
Later in your program in Java you say i.
Java is going to try to figure out, oh, that's five.
That's the i you set up there.
Same thing in closure.
When you use a symbol in your data structure, closure is going to try to find a value that's been associated with that symbol.
It can be associated with it through a construct called let, sort of the way you create a local name,
or through def, which is the way you create a global name.
So before it's a list, and it's going to say this is an operation of some sort.
I have to figure out what to do with a list.
So how does that work?
Well, again, we said what's the data structure?
It's friends, it starts with something, it may have more stuff or not.
But from the evaluator standpoint, all that matters is the first thing.
The first thing is the operator, or op.
That's going to determine what to do.
And it can be one of three things.
It can be a special op.
This is magic.
This is the stuff that's built into the compiler upon which everything else is bootstrapped.
So some things are special.
I'm going to enumerate them in a second.
It can be a macro like we saw before.
There's a way to register with the compiler to say, when you see the op, my cool thing,
go over here and run this function, which is going to give you something to use
in place of the my cool thing call.
And the third thing it could be is an ordinary expression.
It's going to use the normal means of evaluating an expression.
And it's going to say whatever value that yields on a treat as a function
and attempt to call with the calling mechanism of closure,
which is not limited to functions, but it's main purposes for functions.
So for people who know lists, closure is a list one.
It is a list one that supports def macro well.
And the use of namespaces and the way back quote works makes that possible.
And everyone else can ignore that.
In a way, back legs pull the bunch.
An expression in the genome is a function as opposed to it's the function.
Well, what it's going to encounter is it's going to encounter a list
and the first thing is going to be the symbol Fred.
Fred is not a special operator, no Fred enclosure.
Let's say no one has registered a macro called Fred.
Then it's going to use the rules we said before.
What about symbols to find the value of Fred?
Where hopefully someone before has said, Fred is this function.
It will keep evaluating.
It's going to evaluate that expression.
But there are other function like things or callable things in closure
in addition to functions.
I'll show you that in a second.
So let's dig down into each of these three pieces.
Yes.
It doesn't encounter any one of those three.
You have an error at runtime.
It'll say it's not a function.
Effectively what will happen is it will say this is not a function.
If you said Fred is deaf Fred one, so Fred is the number one
and you've tried to call Fred or use Fred as an operator,
it's going to say one is not a function.
Probably with a not very illuminating stack trace.
Okay, so special operators.
There are very few.
I think one of the things that's really cool about lists
and it's also cool about closure is you can define most of them
in terms of themselves.
One of the great brilliant things that John McCarthy did
when he invented lists was figure out that with only I think seven primitives,
you could define the evaluator for those seven primitives
and everything you could build on them.
Like the core of computation.
It still gives me goosebumps when I say that.
It is a beautiful thing.
It really is.
And if you've never looked at the lambda calculus or at least from that perspective,
it's quite stunning.
These early papers are just great and they're just brilliant in a transparent way.
So let's look at a couple.
I'm going to show you two and then I'm going to list the rest.
Deaf would be one.
How do we establish a value for a name?
There's this special operator called deaf.
It takes a name.
Now that name is going to be a symbol.
Obviously, that can't be evaluated, right?
Because the whole purpose of this special operator is to give it a value.
If the compiler were to use normal evaluation, the name position,
you'd have a problem because you're trying to define what it means.
How could you do that?
So one of the things about special operators that you have to remember,
and it's true of macros as well, is they can have non-normal evaluation of their arguments.
Like, the arguments might not be evaluated.
In fact, deaf doesn't evaluate the name.
It uses it as a symbol and it associates that symbol with the value.
It does not evaluate the symbol.
So this is a simple way to say, if I say deaf name, some expression,
the expression will be evaluated, the name will be mapped to that value
or bound to that value.
When you later go and say name, you'll get the value.
It was used to initialize it.
You actually can do that more than once.
You shouldn't do that more than once unless you're trying to fix something.
In other words, deaf should not be used as set.
But you can use deaf to define a function and later you can use it again to fix it.
So the things that are defined by deaf are mutable at the root
and it's probably, you know, it's the only escape hatch for that dynamic change enclosure.
That's not governed by transactions or some other mechanism.
Okay, so it establishes a global variable.
Again, there are namespaces.
I don't have the time to talk about them, but it's all subject to a namespace.
If you're in a namespace and you define the name, then it's in your namespace.
It's distinct from that same name in another namespace.
Namespaces are not the same as packages in CommonList.
They're very much different in particular.
Symbols are not inherently in a namespace.
Symbols have no value, sell, they're not places.
They're just labels.
And there are vars, which are the places more like CommonList symbols.
If is another thing that's built in.
And if you think about if in your language, which you may not have ever done.
If you thought about if is, why couldn't if be a function?
Why can't I say if some test expression, some expression, some else expression?
Why can't if be a function?
I mean, it looks like a function.
Well, it doesn't actually look like a function in Java, but why can't it be a function?
It should only evaluate one of these two.
That's why.
And a function evaluates what?
All of its arguments.
So if you try to write if as a function, you would have a problem because functions evaluate all their arguments.
So if has to be special, and if is special in closure too.
It evaluates the test expression.
And then, depending on the truth or falsity of this in kind of a generic sense,
in closure, if this is nil or false, it will evaluate that.
If it's anything else, it will evaluate this.
But it will only evaluate one of those two things.
It must have a false.
No, it doesn't have to.
The else can be missing in which case it defaults to nil.
So if is another example of something that has to be special.
It can't evaluate all of its arguments.
And then we have these others.
In fact, this is it.
There's something that defines a function.
Something that establishes names in a local scope.
A pair of things that allow you to do functional looping.
To create a loop in your program.
Something that lets you create a block of statements the last of which will be the value.
It allocates a new Java thing.
Access to members of Java thing.
Throw, try, do what you expect from Java.
Set, will rebind a value.
And code bar are kind of a special purpose for list manipulation things.
So I'm not going to get into them tonight.
Question.
Is that the entire list of?
Yeah.
So what's the equivalent of death macro?
Death macro is bootstrapped on this.
So there's a key border.
Oh no, there is death macro.
It's defined a couple of pages into the boot script for closure, which I might show you.
We have some time.
Yes.
I'm just intrigued.
The reason for the explanation point of the set is it trying to say something to the program?
Yeah, this is bad.
Why are you doing this for?
Yes.
No, it ends up that enclosure.
Macros are functions.
And so there's just a way to say this function is a macro and it will be treated as a macro instead of as a function.
Okay.
So that's a tiny set of things.
In fact, when you take out the stuff related to Java, it's an extremely tiny set.
I don't think I made it down to seven.
One, two, three, four, five, six, seven, eight.
I have more than more properties.
But I don't have dozens.
So how could this possibly work?
This is not enough to program with this.
No.
No, no, no.
So we need macros.
Okay.
There are plenty supplied with closure.
And what's beautiful about closure and LISPs is you have the same power that I have to write macros.
When you see the kinds of things that are implemented in closure as macros, you realize the kind of power you have as a developer
because you can write those same macros.
You could have written them.
You don't have to wait for me.
I'm not son.
This is not Java.
You want to do something.
You have something you want to express a certain way.
You want to extend the language that way.
If you can do it with a macro, you can do it without contacting me or asking me for the favor of adding a feature for you,
which means the language is much more extensible by programs.
So let's look a little bit about how they work.
If we remember, we're getting data structures passed in the compiler.
So it looked at the first thing, and somehow there's a way, and I can't show you that tonight,
to say this name designates a macro.
And associated with that name, then, is a function.
The function expects to be passed the rest of the stuff that's in the parentheses.
So we had this cool function, my cool macro.
Maybe it expects to be passed two things.
The things that gets passed are not evaluated.
It gets passed the data structures that the compiler got passed.
Because the compiler is going to say, you told me you know how to do this.
Here are the data structures.
Give me back the data structure I should be processing.
So it's a transformation process where the macro is handed the data that's inside the parentheses,
as arguments to the function that the macro is.
It will run any arbitrary program you want to convert that data structure into a different data structure.
You can write macros that look stuff up in databases.
That go and ask a rule-based system for advice.
Most are not that complicated.
But the thing is, it's an arbitrary program transformation.
There's not a pattern language.
There's not a set of rules about this can be turned into that.
It's an arbitrary program, a macro.
And in this way, it's like a common list macro.
That, given the data structure, gives back its own replacement.
Replace me, the expression that began with me, with this.
And then keep going.
Which may yield another macro and another round of matter.
It may yield something that already knows how to process.
Yes?
So would it be correct to say that a general macro is happening around that problem?
No, this is happening at compile time.
This is part of compilation, right?
The compiler got handed this data structure.
It said, oh, it begins with the macro name.
Hands it to the macro.
It comes back.
That transformation occurs.
It keeps compiling.
Then you get bytecode.
After you get bytecode, there's no more talking to the macro.
So macros replace themselves with another data structure.
And then compilation continues.
So we can look at a macro.
You'll notice on the list of primitives, there's no or.
Or is not primitive enclosure.
And in fact, if you think about or, or is not primitive.
Or is not a primitive logical operation.
You can build or on top of if.
Right?
The or, what I'm talking about is like the double bar or in, in Java in that what happens
if the first part tests true, what happens to the second part?
Not evaluated, right?
It's still got that magic thing, but if already knows how to do that.
If already knows how to do a conditional evaluation of only one of two choices,
which means we can define or in terms of if.
And so this is what happens.
So or is a macro.
When it's expanded by the compiler, it, it returns something like this.
I'm going to say or X or Y.
And this is what comes back.
Another data structure begins with the let, which we haven't seen so far,
that says it takes a set of pairs of things to make this mean,
mean this inside the scope of the let.
Like a local variable, except it's not variable.
You can't vary it.
But it has the same kind of scope.
So it says let's, let's do that.
And the reason why it does it is because this is going to be some expression.
It looks like X here, but it could be like a call to calculate some incredibly difficult
thing that's going to take an hour.
In which case, I finally want to repeat that more than once in my expansion,
because it would calculate that thing twice.
So we're going to take whatever that expression is, put in here,
assigns that into this variable name, which is made up because,
because obviously you didn't pick this name.
It's a good machine pick name.
So it makes a variable.
And then it says if that thing is true, right,
and you took an average calculator in this, right,
we have that idea.
If that's true, return it.
If, right, I know, isn't it going to do this?
If this is true.
Otherwise, it's going to do what?
And that's the implementation of OR.
If the first thing is true, it returns it.
Well, in fact, in Java, you don't get a good value,
but in Clojure, you get the value that was true.
Then the invocation of any function can both return a value in a true form
or you interpret certain types of values.
All values can be placed in a conditional, not just Booleans,
and it's subject to the rules I said before.
If it is nil or if it is false, you'll get the else expression evaluated.
If it is anything else, seven, the string fred, anything else is true.
So Clojure, like most LISPs, allows any expression to be evaluated
as the conditional test.
Here.
He's also getting the part that there are no spiting effects of the evacuating effects.
No, I talked about that.
Let's say this x for now.
A well-written macro will make sure it only gets evaluated once.
I could have put if x, x, y, yes?
No, this is the answer to your question.
If it said if x, x, otherwise y, then if x had side effects, it would happen twice.
Then we make this not a well-written macro.
This is a well-written macro where it needs to use that expression twice,
which means it's going to bind a temporary variable to the value,
which means x only happens if it appears only once here.
So if it had a side effect, it would happen only once.
If it took a long time, it would take a long time only once.
Simple as it is, I still have this simple question.
Let appears to take three arguments.
Let actually takes, at the top most level, it takes n arguments.
The first of which has to be a vector of pairs of things.
You can have multiple expressions.
Name, value, name, value, value in a letter.
This is one symbol there.
Let is a block, so it actually can have multiple expressions.
In this case, there's only one.
And then it just does whatever it's next.
It returns the value.
Well, this is a macro, and all it's going to do is get the compiler back this,
and the compiler has to keep going with this in hand now.
Yes, I'm just trying to figure out what led you.
Let will, led establishes this name to not this value.
Then, when led runs, the series of expressions inside led run,
and the last of them is the value of the led expression.
In this case, there's only one expression inside the led.
In this case, there's only one expression inside the led.
So the value of the if expression is the value of the led.
Which is what we want, we want this to mean or.
And that's the scope.
This is the end of the scope over here.
And this parent matches that value.
That's what I was noticing.
Yes, and well, it's one of the beautiful things about the system,
which we'll see clarified in a moment, is that all expressions are bound.
So we don't have a lot of complexity with precedents and terminators and things like that.
It started with the parent, it ends with the matching parent later.
Big Boolean?
In fact, it has to be big Boolean false.
If it's coming from Java, I test to make sure,
because an improperly constructed big Boolean may not be Boolean dot false.
New Boolean is wrong.
And in fact, not only is new Boolean wrong,
but the reflection API in Java uses it exactly that way.
So it returns multiple different values of big Boolean false.
I have a patch that looks for that because I got bit by that already.
So it will make conversions of big Boolean false that aren't Boolean dot false into Boolean dot false.
I'm sorry, I didn't write Java, I only wrote closure.
But the point here is that this seems like a primitive thing,
like if the language doesn't have it, you're in trouble.
It is not.
If I had somehow left out OR, you could have added it.
You could have written the macro that does this job and added OR to closure.
I'm sure I forgot some things in closure.
You could add them.
Many things.
In fact, we saw how tiny the special operators list is.
And OR, con, all kinds of things are built on top of these things as macros and OR functions.
And after the point of the special ops, you can add a special operator, but you can add a macro.
Now I've got more than 50,000 for the question.
So I get this, this is great.
So something to build a bunch of macros in these is that it's a powerful, another bunch of macros.
Somebody else has got this smoking advantage of this language.
People sort of come down to the industry for developers and the industry for locals in.
There's a one-time error and I get a stack trace of it going in.
You're going to get a reference to the expansion, the inside of the expansion.
That can be challenging.
It's still an area.
I think that one of the things that's good about a list is because you have the ability to work in the small
and to say, I just wrote this little component of this thing.
I'm going to run this right now.
I don't have to wait till the big program that contains this runs.
Your ability to do that immediate unit test to make sure that thing is working is good.
On the 50,000 foot level, propagating up from macros the source of the problem in the macro is something
that's being worked on.
Some compilers do it pretty well for common list.
It's an area I hope to enhance in closure.
But it will always be more challenging than a function.
And that's why macro writing is not for newcomers or the inexperienced part of the team.
It is language design.
It definitely is.
On the other hand, without it, you're limited to the abstraction capabilities of functions, which are limited.
Think about how much you repeat in Java.
Think about how much code you repeat to close files in Java.
Think about it.
Think about how many times you've written the exact same thing.
Having your IDE spit it out is a little bit handier.
But when you decide, oh, I need to change my policy about doing this.
I want to check something else.
All that generated code is not amenable to fixing.
So those kinds of things that can't be...
whose redundancy can't be eliminated by functions can be eliminated by macros.
And that's something you want to do.
Because the B side of this is, if you're doing all that stuff by hand, yes, it's transparent.
You get this debugger error.
Okay, you did that by hand.
Where?
All over your program.
Because you didn't have a macro that generated it.
You don't have one place to fix.
You have n places to fix.
So there's a...
If you have n places to fix, where you say, oh, I made this mistake everywhere.
But you still have to find everywhere you have to fix it.
And these things are idioms.
Everybody that programs in Java has to know this.
These idioms are only by convention
and they have to be manually replicated.
It is an attempt to address those cross-cutting concerns.
But it's still unproven as to whether or not people will describe them...
those things in advance.
Because what tends to happen is that you don't know it.
And then you say, oh, I'm doing this all over the place.
And then will you implement an aspect?
Is there a policy?
Is there a way to describe an aspect that will insert it
everywhere it's needed?
That's a very challenging problem.
But the problem of this summary is little easier with
aspect-oriented programming.
I mean, I think aspect-oriented programming is
interesting, but it's different.
So anyway, the trade-off with macros, yes, it may be less
transparent there.
On the other side, when you fix a macro, you fix
every usage of the macro.
Finally, we get to the easier thing.
I mean, start with special operators and macros, mostly
because that's the evaluation order.
But functions exist, and they're kind of straightforward.
The first thing about functions you need to know is that
they're first-class values.
They're values like any other.
Methods in Java are not first-class.
You can't put a method into a variable.
You can't pass a method to a function.
There are special things in LISPs.
And in fact, in most dynamic languages today, functions are
first-class, which means the function is a value.
So I've defined 5 to mean 5.
And of course, I don't need to do that.
But I'm showing you a depth of a symbol to a value.
Now I'm going to show you a depth of a symbol, as if you
are, to a value, which is according to one of the other
special operators called fun.
And what fun does is it creates a function object.
This is going to turn that code into something that gets
compiled into a function that takes one argument and
multiplies it by itself.
It's a regular function.
It's going to be an instance of a Java interface that
takes an argument.
It's a real, regular method in Java.
You'll have an invalidarity problem.
OK, I need to move a little bit more quickly.
So let's hold the functions for a little bit.
Let me move forward.
So this fun, I can't describe all of the features of fun.
It's an exciting and rich thing.
But this fun that we can take as being fun is a special
operator.
It takes a vector of the names of its arguments, the
simplest way of understanding.
And then it contains a set of expressions, which will be the
body of the function.
The last expression is the value returned by the function.
There's no return statement in closure.
So when we say square five, it returns 25.
This is a function call.
Again, we said, what does it do?
It says, is square a special operator?
No.
Is it a macro?
We're going to say, right now, it isn't.
So what's the value of square?
It's this function object.
OK, call it.
And pass it that.
The value of that, because the arguments to functions are
evaluated.
So it's going to pass square the number five.
Square is going to multiply by itself and return 25.
So functions are first class.
There are other things that are like functions.
In other words, the compiler says, can I call this?
The answer is true of funds.
It's also true of other things.
In particular, one of the neat things about closure is that
maps are functions.
Because if you think about maps mathematically, they are
functions.
Maps are functions of their keys.
Given a key, a map should return the value of that key.
And it does, in closure.
So maps are functions.
Sets are also functions.
Vectors are also functions.
Vectors are functions of their indices.
That's cool stuff.
And when you see idiomatic closure, some of it is quite
beautiful because of that relationship.
So we'll try to summarize this.
Things that would be declarations or control
structures, or function calls, or operators, or whatever.
In Java, all are uniform in closure, or any list.
In that, there are lists where the operator is the first
thing in the list.
So we've reduced all of this variation here to something
uniform.
So look at each one.
int i equals 5 establishes i as the name, whose meaning is
the value of 5.
That i does that as well.
Where in this does it say it's a definition?
Whatever.
Some rule about the shape of this thing says it's a
definition.
In closure, that says that's what it means.
If x is equal to 0, return y.
Otherwise, return z.
When does this end?
I'm showing the rest of the program.
Is this done?
Got me?
You don't know?
I don't know.
Because it could say else, else, else if.
I couldn't say it has to say else if, and then it could say
else.
We have to keep looking forward.
We could not have had an else.
It's not closed.
In addition, without these returns, it doesn't yield a
value.
This is a statement in Java.
There is an if conditional, which is an expression of two
different things.
In closure, if, against first, we know we're dealing with if.
We saw the syntax.
It takes three things.
What's the question mark in 0, the question mark in closure?
That's a function name.
You can have question marks in names.
Closure is much more liberal about the symbols that can
appear in names, but not completely liberal.
Because I need some symbols for myself.
x times y times z.
What are these?
Mathematical operators.
Again, another special thing about Java.
And they can go in between things.
And there's precedence rules.
All other kinds of guk, right?
Closure is wet.
It's a beginning.
I don't have to look anywhere.
I don't have to look in the middle or read or look for
semicolon.
What's happening?
Multiplication, first.
Also, you'll notice multiplication can take
multiple operands, more than two.
It's not just a binary operator.
It's an n-ary operator.
Foo x, y, z.
This is what?
Function call, right?
Foo x, y, z.
People complain about the parentheses.
List how many parentheses difference.
None.
You move it from here over there.
Same thing.
Same thing.
I don't know what you're talking about.
And you're not going to see curly, curly, curly,
curly, curly, curly, curly.
Yes, you may see friends like that.
But that's better, I'm telling you.
It keeps your program near itself.
You don't have to go down to the next page to see the next
step.
And then this member access, I'm going to talk more about the
Java interoperability.
But same kind of thing.
Different number of parentheses?
No.
Different number of dots?
No.
But dot goes first, because dot tells Clojure we're doing
some Java stuff here.
That has its own special interpretation, because dot is
a special operator, we saw before.
So there's a tremendous uniformity.
There's a lot of value to that uniformity.
I know a lot of programming languages.
And every time I have to learn the arcane, whatever the
rules are, syntax, and this thing next to that means
that, and this character means this, and you can have a
semicolon here, but not there, and it better be indented by
the same amount, or whatever it is.
I really get angry now, because there is no reason for
that.
It is not better than this.
And if you use this for any amount of time, you will not
disagree, because there's no one who has, who does.
But it also has to have its biggest
differences in some ways.
Who got far, far more cool for an X in Java?
How do I get the expression?
I'll show you later.
If I only have another hour, I have to go much faster.
Everybody ready?
So let's hold the questions until a question time, unless
you're really confused, but just general interest things
will hold, because I may cover it.
One of the things that is typical about a Lisp is that
it has a rich library for manipulating lists.
But it ends up that, I think, in my opinion, it's a
shortcoming of Lisp's traditionally, that those
functions are limited to a particular data structure,
which is the singly linked list.
Because the functions that underline that abstraction are
broader, and they're three of them.
The first is, I'd like to obtain some sort of a sequence
like thing from some sort of collection like thing.
That's an abstract way to say something.
Given that sequence like thing, I want and need only two
functions.
One is to say, give me the first thing.
The other is to say, give me the sequence that is the rest
of this sequence.
In the case of seek, if there is no stuff, it returns nil,
because nil means nothing.
Which means you can say seek call, and you can put that in
an if expression as a test thing.
And because nil returns logical false, you'll know
there's nothing to do.
That's an important idiom of common Lisp, closure preserves
unlike scheme, where you have to say empty all the time.
If it's not empty, you will get back an object.
That object only makes two promises.
You can call these two functions on it.
This function promises one thing.
There will be a first element, because we're already covered
if there's not a first element here.
So if you say first of the seek, and this is not nil, it
means you have a seek, you get back a guy.
The first thing in the sequence.
The second thing you can do with the seek is you can call
rest on it, which says, give me the sequence that represents
the rest, not including the first thing.
Of course, if there's no more, what should we get?
Nil, because we said here, nothing.
If we have nothing, we get nil.
Otherwise, we're going to get another seek.
This is an extremely abstract way to talk about lists.
But the advantage over common Lisp and scheme lists is they
would promise that the return value of this thing is a
consel.
And that is a real limitation, because now I can make
seek work on absolutely everything.
Seek works on lists, because they have the structure.
But it's possible to create a seek object if you think about
iterators, and I want to make this analogy extremely
weakly.
There's a way to walk through a vector.
Similarly, there's a way to walk through a map.
There's a way to walk through a string.
There's a way to walk through a file.
And it ends up that seek is supported on all those things.
You can walk through Java arrays, all the closure
collections, strings, files, everything.
And you can use these two operations to move around.
This abstraction of listness, which I call a sequence
because a list is more of a concrete thing, is bound to
lists in most Lisp's.
Wow, this is hard to say.
But it's not in closure.
And it's, I think, one advance of closure in the Lisp world,
which means that you can apply these things to everything.
So what does this mean?
Well, this is kind of primitive.
I mean, walking through step by step.
But what it means is that you can build a library on top
of these primitives that provides a lot of power for
manipulating data structures without loops.
I'm just going to show you a tiny, tiny little bit.
But it should give you a feel for what it's like to program
in closure if you would think about what it would take to
do these things in Java.
For instance, I have a set of things.
I'd like to have everything except the first two things.
We say drop two from whatever the collection is.
That happens to be a vector.
It could have been a list.
It could have been a string.
We'll drop the first two characters.
Whatever it is, there's a way to abstract out the notion of
walking through it.
Drop means leave out that many and give me the rest as a
sequence.
Take is the opposite.
It says, only give me nine of these things.
Look at the second function, cycle.
Cycle is a function called, it takes 1, 2, 3, 4.
In this case, it could take any sequenceable thing.
It returns an infinite list, an infinite sequence of those
things around and around in a cycle.
How could it do that?
Isn't that going to chew up all the memory of my machine?
Cycle sounds like a really scary function.
It does that because if we go back to the definition of
this, is there anything about the way I describe the
operation of these things that says that the rest of this
thing has to exist?
I could make up the rest right when you ask me, right?
And how much of it would I have to make up?
Just one more thing.
The thing I give you has to have one more thing in it, and
it's I'm OK.
It could delay the calculation of the next part until the
next time you call rest.
That's called laziness.
And in fact, all the sequence stuff I'm showing you for
closure is lazy, which means that you can write sequence
functions that return infinite sets.
And you can use them, as long as you don't try to consume
all of them, you can consume a little bit of them.
So in this case, we're making an infinite sequence out of
1, 2, 3, 4, or taking the first nine things from it.
This looks like a weird abstract thing, but I've had
plenty of programs in reality.
I've had to do exactly this thing, round robin.
You can use it to round robin, work dispersal.
You can use it to get distributions.
In cycle, it seems like some theoretical isn't as cool you
can make an infinite sequence.
But it really has utility.
It ends up in real programs.
And it goes on and on.
Interleave does what you think.
One from this sequence, one from that.
Makes a new sequence.
Again, one of these could be infinite.
You'd only make as much of this as you needed to match the
length of the non-infinite one.
Partition, split this up into pieces.
Think about the loops to do this stuff.
And in Java, you have to write everyone, every time.
Never mind the laziness part.
Now we get to a more interesting function, which is map.
Now we're not talking about map the data structure.
We're talking about map a function, which is, again, from
this list land, which says, take this function.
So the first argument of map is a function value.
And apply it to pairwise, or however many sequences I give
you, the elements of the sequences I provide.
So in this case, we're going to call the function vector.
And we're going to call it on a and one.
Then we're going to call it on b and two, and c and three,
and d and four, and e and five.
And vector makes vectors out of whatever you pass it.
So we're mapping vector across this pair of sequences to
vectorize corresponding elements of those sequences.
We get a set of data structures back out of this.
So map is a very powerful thing.
Instead of saying, for each blah, blah, blah, do this and
stick the answer into this collection, you say, just map
this function across this data.
And it'll give you back a set of new data, the result of
applying that function to each thing.
You can also apply it against multiple sequences.
That's what this is doing.
Maybe I shouldn't have done something this complex here.
Apply is also very interesting, and it's a unique thing to
lists and languages that are dynamic.
Apply says, I'm also going to pass you a function.
What I want you to do is take the next expression and figure
out the sequence it yields, and then use that as the
arguments to a call to this function.
So we're going to apply the function stir, and stir says,
given any set of things, turn it into a string.
Turn each part into a string, and can cat and make them all
back together into a string.
So we want to put that together, and what interpose
does is it says, take this thing and put it in between
everything in this sequence.
So interpose, comma, ASDF, turn ASDF into a sequence, and
return characters.
So we're going to have the character A and a comma, S
and a comma, D and a comma, F and a comma.
Seven things.
Yes?
Three-four things with three things in two.
Seven things.
And we say, apply stir to that, which means
string can cat and make them.
As if they were the arguments to stir.
In other words, if I called stir and said stir, A comma,
S comma, D comma, F, it would make a string out of them.
Well, I can just apply it to the sequence, as if I called it
with those arguments, and it will do the job.
I get back a single string with that in between.
Again, if you don't quite get these, it's OK.
I'm just trying to show you the power and the
succinctness of this.
Reduces another function that takes a function.
It says, apply this function to successive pairs of the
sequence you're given, taking the result of each application
and using it as the first argument of the next.
So if you say, reduce with plus, you're going to get the
first two things plus each other.
And then take that and do that plus the next thing.
And take that and do that plus the next thing.
That's what reduce does.
So this effectively is summing this range.
It's a function that returns a sequence of numbers.
And you can step where it starts and where it ends and how
it steps and things like that.
This is obviously a much higher level way to write
programs than you do in Java.
Yes?
No?
Your head hurts.
I don't know what it's going to be.
Yeah, let's take a break.
This is going to be a good time for a break.
Does anybody have any questions on this real quick?
Right, and cycle returns a sequence, which has only got
one in it.
And the recipe for producing the rest of the cycle.
Sort of like a delayed function.
That's what happens inside cycle.
It doesn't produce an infinite list, obviously.
It returns an object that satisfies.
It returns a sequence, correct.
Why can't you call stirred directly?
Well, in this case, well, I'd have to write a comma, s comma,
d comma, f comma.
Right?
Then you're passing stir a sequence.
And what I want to do is say, take that sequence and pretend
it was the arguments to stir.
Not an argument to stir, but n arguments to stir.
Because that's the syntax of closure.
Slash comma is a character literal for comma.
Quote is used for other things.
That's why I don't use it for character literals.
All right, let's take a break.
