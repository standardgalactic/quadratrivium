1
00:00:00,000 --> 00:00:28,000
So, let me start a little bit with the introduction of who Melanie Mitchell is.

2
00:00:28,000 --> 00:00:36,520
She is a Davis Professor of Complexity at Santa Fe Institute, and her research focuses

3
00:00:36,520 --> 00:00:42,360
on conceptual abstraction, analogy making, and visual recognition.

4
00:00:42,360 --> 00:00:48,440
She's done a couple of very popular and well-received books.

5
00:00:48,440 --> 00:00:54,860
The first one was Oxford University Press in 2009, Complexity, a guided tour.

6
00:00:54,860 --> 00:01:01,460
Her most recent book is Artificial Intelligence, a Guide for Thinking Humans, and I have to

7
00:01:01,460 --> 00:01:08,020
say that people who've talked about the Artificial Intelligence book have said some very, very

8
00:01:08,020 --> 00:01:09,980
good things.

9
00:01:09,980 --> 00:01:18,220
For example, we have Allison Gopnik, who says it's very intelligent, clear and sensible

10
00:01:18,220 --> 00:01:19,220
book.

11
00:01:19,220 --> 00:01:26,460
Jeffrey West at Santa Fe has said it's remarkably lucid, comprehensive overview.

12
00:01:26,460 --> 00:01:33,140
Sean Carroll, an astonishing intelligence, and John Paulus, who's been on the show,

13
00:01:33,140 --> 00:01:39,620
says that Mitchell sketches enough details and clever illustrations that one gets a good

14
00:01:39,620 --> 00:01:42,860
intuitive understanding of AI.

15
00:01:42,860 --> 00:01:51,140
So you have high praise from some of the titans of a number of different disciplines,

16
00:01:51,140 --> 00:01:54,740
and I think that's a good way to take the next step.

17
00:01:54,740 --> 00:02:04,600
The next step is I'm in conversation with someone who is familiar with at least three

18
00:02:04,600 --> 00:02:15,500
important domains, mathematics, physics, and computer science, as well as artificial

19
00:02:15,500 --> 00:02:22,920
intelligence, which I would put in as part of the computer science part of this.

20
00:02:22,920 --> 00:02:32,120
So I see from my research of your background is you really have excelled as an interpreter,

21
00:02:32,120 --> 00:02:38,880
as someone who's been able to translate between these domains.

22
00:02:38,880 --> 00:02:44,200
And as a result, we'll be looking a little later in the show about how your childhood

23
00:02:44,200 --> 00:02:53,640
reading and those books help you create this kind of ability to map and track other thought

24
00:02:53,640 --> 00:03:00,200
processes in other domains and make them accessible to ordinary people.

25
00:03:01,200 --> 00:03:10,400
Yeah, so we humans are all masters of analogy, even when we don't know it.

26
00:03:10,400 --> 00:03:16,200
So let's think about a kind of unconscious analogy.

27
00:03:16,200 --> 00:03:19,480
One example that I use is the notion of a bridge.

28
00:03:19,480 --> 00:03:21,280
We all know what a bridge is.

29
00:03:21,280 --> 00:03:25,800
We all drive across bridges or we walk across bridges, whatever, but we also talk about

30
00:03:25,800 --> 00:03:32,960
bridges like the bridging the gender gap, or the bridge of a song, or the bridge of

31
00:03:32,960 --> 00:03:41,960
my nose, or there's all kinds of ways in which these terms are used metaphorically or by

32
00:03:41,960 --> 00:03:46,800
analogy throughout language, even unconsciously.

33
00:03:46,800 --> 00:03:54,520
But this kind of ability we have to extend concepts in this metaphorical way, and it's

34
00:03:54,520 --> 00:04:01,920
not just bridge, you could do it with almost any concept we want to talk about, is, I claim,

35
00:04:01,920 --> 00:04:05,720
what gives us our unique kind of intelligence.

36
00:04:05,720 --> 00:04:15,240
It allows us to understand more and more abstract notions in terms of very physical notions.

37
00:04:15,240 --> 00:04:21,120
Like you just said, you know, she has sunny face, so we all instantly know what that means.

38
00:04:21,120 --> 00:04:26,480
And we know, we don't even think of it necessarily as a metaphor.

39
00:04:26,480 --> 00:04:31,560
It's just so, you know, I might say, oh, you gave me such a warm introduction, you know,

40
00:04:31,560 --> 00:04:35,800
and it was, it wasn't literally warm, right?

41
00:04:35,800 --> 00:04:41,040
The temperature in this room didn't literally go up, I don't think, but it's something

42
00:04:41,040 --> 00:04:45,480
that we all couch all of our abstract thinking in.

43
00:04:45,480 --> 00:04:50,520
And this is exactly what AI systems today are lacking.

44
00:04:50,520 --> 00:04:58,720
They can learn to recognize photos of bridges, but they can't make that extension and talk

45
00:04:58,720 --> 00:05:08,840
about, you know, a bridge between two people, you know, you and I, or you may be your bridgingness,

46
00:05:08,840 --> 00:05:13,280
the culture of science with the culture of humanities, or things like that.

47
00:05:13,280 --> 00:05:18,240
So that's fascinating to me, and that's what my research is on, is trying to understand

48
00:05:18,280 --> 00:05:26,120
how human concepts are formed, what kind of structure they have, and how we might give

49
00:05:26,120 --> 00:05:28,360
such a concept to a machine.

50
00:05:28,360 --> 00:05:37,800
Yeah, presumably we're some ways away from AI understanding it's raining cats and dogs.

51
00:05:37,800 --> 00:05:45,200
Or, or she has a heart of gold.

52
00:05:45,200 --> 00:05:51,360
I mean, yeah, we don't, we, one problem with talking about AI is that we, when we talk

53
00:05:51,360 --> 00:05:54,720
about understanding, we don't really know exactly what we mean.

54
00:05:54,720 --> 00:05:57,760
What is it to understand something?

55
00:05:57,760 --> 00:06:03,160
That's actually been debated quite a bit by philosophers for millennia.

56
00:06:03,160 --> 00:06:06,320
And it's really an unanswered question.

57
00:06:06,320 --> 00:06:11,760
It's one of those words we use to describe our mental state that we don't quite understand

58
00:06:11,760 --> 00:06:13,920
scientifically what it means.

59
00:06:14,000 --> 00:06:21,720
I think in terms of kind of the sort of intuitive understanding we have of that notion, you're

60
00:06:21,720 --> 00:06:26,280
absolutely right, machines don't have the same kind of understanding of language that

61
00:06:26,280 --> 00:06:28,080
we humans have.

62
00:06:28,080 --> 00:06:37,240
I'm wondering as well if part of that issue has to do with, you've written about GPT-3

63
00:06:37,240 --> 00:06:40,440
making a knowledges in medium.

64
00:06:40,440 --> 00:06:46,680
And you've said that with metaphors there isn't a right answer.

65
00:06:46,680 --> 00:06:51,400
There's an effective explanation or description that convey information.

66
00:06:51,400 --> 00:06:55,760
It's really a conveyor from the abstract to the concrete.

67
00:06:55,760 --> 00:07:02,200
But it's not, you can't really say it's 100% right.

68
00:07:02,200 --> 00:07:04,400
It can be close to right.

69
00:07:04,400 --> 00:07:12,440
What goes back to what you were talking about a moment ago is how we understand that transition

70
00:07:12,440 --> 00:07:17,360
from the abstract into something that's concrete.

71
00:07:17,360 --> 00:07:24,560
Is it giving us with that similarity a true representation from what the abstract really

72
00:07:24,560 --> 00:07:25,560
means?

73
00:07:25,920 --> 00:07:36,520
Yeah, it's machines, they've been shown to be what people in AI call very brittle, meaning

74
00:07:36,520 --> 00:07:43,680
that they have this kind of appearance of being intelligent until suddenly something

75
00:07:43,680 --> 00:07:45,200
shows you very clearly.

76
00:07:45,200 --> 00:07:50,280
It's like it's a breaking point and that's why it's brittleness that they didn't understand

77
00:07:50,280 --> 00:07:55,320
at all what the language they were using meant.

78
00:07:56,080 --> 00:08:06,160
What of my favorite things I saw on social media some time ago was someone had taken

79
00:08:06,160 --> 00:08:12,400
an apple and written the word iPad on a piece of paper and put that on the apple with a

80
00:08:12,400 --> 00:08:20,920
rubber band and then ask an AI for questions or for answers to what this is and it said

81
00:08:20,960 --> 00:08:24,040
with 90% probability it's an iPod.

82
00:08:27,640 --> 00:08:38,920
So that kind of literalism, you can see it's part of that inability to deal with analogies

83
00:08:38,920 --> 00:08:39,920
and metaphors.

84
00:08:39,920 --> 00:08:47,640
And again, you've talked about in how analogies shape our thoughts, what happens if you strip

85
00:08:47,720 --> 00:08:54,480
away metaphors, simile, and analogies from language, what is left?

86
00:08:55,960 --> 00:08:58,040
Are we left just with abstractions?

87
00:08:58,040 --> 00:09:05,280
Is that the part of the problem that we have with storytelling AIs is that they lack the

88
00:09:05,280 --> 00:09:14,120
essential building blocks of how you tell a story, which basically is the bricks of that

89
00:09:14,200 --> 00:09:17,920
building of that story really are metaphorical, a lot of them.

90
00:09:19,120 --> 00:09:21,200
That's one of the problems for sure.

91
00:09:21,880 --> 00:09:25,880
Another problem is that AI systems don't understand how the world works.

92
00:09:26,680 --> 00:09:36,840
You know, they don't know that they don't know that, you know, if a child is holding

93
00:09:36,840 --> 00:09:42,520
an ice cream cone and then the ice cream cone falls on the ground, that that would be upsetting

94
00:09:43,320 --> 00:09:45,000
because how are they going to know that?

95
00:09:45,000 --> 00:09:47,960
They haven't experienced the world the way that we have.

96
00:09:48,840 --> 00:09:54,600
They don't even know that, you know, when an ice cream cone hits the ground that it's going,

97
00:09:54,600 --> 00:09:57,640
it's not edible anymore, and you wouldn't want to eat it.

98
00:09:58,440 --> 00:09:59,320
How would they know that?

99
00:10:01,000 --> 00:10:05,480
It goes back to what you've written about as well of saying that the problem with AIs,

100
00:10:05,480 --> 00:10:06,760
that lacks common sense.

101
00:10:07,720 --> 00:10:13,960
And common sense is if you drop the ice cream cone, even a four-year-old knows it's no longer

102
00:10:13,960 --> 00:10:22,600
something you want to eat or an AI system would not have that particular response because it's

103
00:10:22,600 --> 00:10:29,080
not an embodied biological entity that learns certain things about food from a very early age.

104
00:10:29,720 --> 00:10:30,760
That's exactly right.

105
00:10:31,720 --> 00:10:36,520
So that's the, you know, that's one of the big problems is how do you get a system like

106
00:10:37,560 --> 00:10:44,120
GPT-3 or any other AI system to get that kind of knowledge about the world that we all have,

107
00:10:44,120 --> 00:10:50,200
you know, as children, we all learn so much by just living and existing in the world.

108
00:10:50,920 --> 00:10:55,960
You know, part of what you've written that strikes me as very interesting as well is

109
00:10:56,920 --> 00:11:02,840
that a lot of the analogy making we have comes from an unconscious part of our brains.

110
00:11:03,400 --> 00:11:09,880
There was, when you start to think of analogies, they will come to you and if I ask, well,

111
00:11:09,880 --> 00:11:10,840
where did that come from?

112
00:11:11,800 --> 00:11:12,760
Probably don't really know.

113
00:11:13,400 --> 00:11:19,640
It's just, it's been triggered by some, some gesture, some thought, something that you're not

114
00:11:19,720 --> 00:11:21,080
really connected to.

115
00:11:21,080 --> 00:11:25,880
You can make up a story of why you access that, but it's just that.

116
00:11:25,880 --> 00:11:27,240
It's just a story.

117
00:11:27,240 --> 00:11:28,200
You don't really know.

118
00:11:28,840 --> 00:11:38,200
So AI does not have an unconscious matrix in which to draw upon to pull those analogies

119
00:11:38,200 --> 00:11:43,320
and metaphors from a huge reservoir.

120
00:11:43,960 --> 00:11:50,920
I mean, presumably that material for analogy making is as close to infinite as a human can

121
00:11:50,920 --> 00:11:51,400
get to.

122
00:11:52,200 --> 00:12:00,600
And as a result, it's very hard, I would think, to program into a computer intelligence

123
00:12:00,600 --> 00:12:07,160
an analogy making function because it goes much deeper than just being able to calculate

124
00:12:07,240 --> 00:12:14,440
precisely what the next best move is in chess or goal or Atari.

125
00:12:15,800 --> 00:12:16,040
Right.

126
00:12:18,120 --> 00:12:26,040
Our analogy making comes in part from the fact that we have this huge store of memories.

127
00:12:27,720 --> 00:12:32,520
And, you know, if you might, you probably have the experience where somebody tells you a story

128
00:12:32,520 --> 00:12:35,160
and you say something like, oh, the same thing happened to me.

129
00:12:35,880 --> 00:12:36,200
Yes.

130
00:12:36,200 --> 00:12:36,760
Right.

131
00:12:36,840 --> 00:12:41,320
And of course, the same thing literally didn't happen to you because you're not that other person.

132
00:12:42,120 --> 00:12:50,920
But you, something about their story triggers a memory that you somehow mapped on to the story

133
00:12:50,920 --> 00:12:55,000
that they just told you happens to us all the time every day.

134
00:12:55,000 --> 00:12:58,360
You know, someone says something about their life and you say, oh, yeah, me too.

135
00:12:59,160 --> 00:12:59,560
Right.

136
00:12:59,560 --> 00:13:03,240
And that's like, you know, you're making an analogy between their life and your life.

137
00:13:03,240 --> 00:13:05,080
And it's just constant.

138
00:13:05,080 --> 00:13:13,480
But it has to do with an interaction between our perceptual abilities, our memories, our pattern

139
00:13:13,480 --> 00:13:17,160
recognition facilities and so on.

140
00:13:17,160 --> 00:13:20,200
And this is something we haven't yet figured out how to give to machines.

141
00:13:20,920 --> 00:13:28,520
I'm wondering as well whether that exchange also falls under the heading of empathy,

142
00:13:29,080 --> 00:13:31,240
which is something a machine doesn't have.

143
00:13:31,240 --> 00:13:36,840
Is that you will say, oh, that's similar because you have a theory of mind about that person.

144
00:13:36,840 --> 00:13:43,720
You can put yourself in that position and see that as happening to you and that you would have

145
00:13:43,720 --> 00:13:49,400
had a similar response to a particular event or an object or a gesture.

146
00:13:50,440 --> 00:13:51,960
Yeah, that's absolutely right.

147
00:13:51,960 --> 00:13:57,880
And, you know, one of the things about humans is they're incredibly socially oriented,

148
00:13:58,600 --> 00:14:05,880
meaning that they're always trying to understand other people that they encounter from the earliest

149
00:14:05,880 --> 00:14:10,360
infancy and they're able to put themselves in the position of other people.

150
00:14:11,080 --> 00:14:20,680
So that's something that, again, is very human and so perhaps essential for intelligence.

151
00:14:21,400 --> 00:14:22,680
You know, we don't know.

152
00:14:22,680 --> 00:14:27,640
Usually AI people often think of intelligence as something kind of separable from

153
00:14:28,520 --> 00:14:33,080
things like empathy and emotions and it's pure rationality.

154
00:14:33,080 --> 00:14:39,240
But I think a lot of people are now trying to make sense of how the centrality of these

155
00:14:40,040 --> 00:14:44,360
or emotional aspects of our intelligence.

156
00:14:45,000 --> 00:14:51,320
Right. Well, what I'd like now to do with this background in terms of metaphors,

157
00:14:51,320 --> 00:14:59,640
analogies to move on to your reading list, which I hope that we'll be able to get into how

158
00:15:00,440 --> 00:15:08,440
this may have been a training set and education for you into the world of metaphor,

159
00:15:08,440 --> 00:15:10,840
simile and analogy making.

160
00:15:11,480 --> 00:15:16,440
So let's start with really your background, your first books.

161
00:15:17,080 --> 00:15:19,160
You're at home.

162
00:15:20,040 --> 00:15:24,920
Was the first books from your mother or your father or a sibling?

163
00:15:25,560 --> 00:15:30,920
I mean, how did you come to first have that book, that first book in your hand?

164
00:15:32,120 --> 00:15:33,160
The first book?

165
00:15:33,800 --> 00:15:41,640
Yeah. In other words, we look at the first book on the list, which is The Phantom Tooth Toe Book,

166
00:15:41,640 --> 00:15:43,800
which is an absolutely wonderful book.

167
00:15:43,800 --> 00:15:48,920
I'm so glad you chose that. I can't wait to get into it, but let's just start for a moment.

168
00:15:48,920 --> 00:15:54,440
And how did you come to have that book in your possession and how old were you?

169
00:15:56,440 --> 00:16:02,840
Wow. I was, I'm guessing I was about eight or nine, something like that.

170
00:16:03,960 --> 00:16:11,560
And I really don't remember at all how I got that book, who gave it to me or anything.

171
00:16:11,640 --> 00:16:19,400
Somehow it appeared, sort of like The Phantom Toe Booth in the book just suddenly appeared.

172
00:16:19,400 --> 00:16:20,600
No one knew where it came from.

173
00:16:23,400 --> 00:16:29,400
Did you grow up in a reading environment where your parents were reading or siblings were reading?

174
00:16:29,400 --> 00:16:33,880
And then that kind of storytelling from reading was kind of a natural

175
00:16:35,240 --> 00:16:39,000
bridge from one to the other. You're reading books, you're telling stories, you're reading books.

176
00:16:39,880 --> 00:16:52,360
Yeah. My parents both were avid readers and read to us from earliest childhood.

177
00:16:58,280 --> 00:17:03,640
I just, I can't remember any time when I couldn't read or didn't spend most of my time reading.

178
00:17:03,640 --> 00:17:09,080
So I loved reading as a child. I did too. That's why I'm doing this show.

179
00:17:09,640 --> 00:17:17,400
And I think that the people who have that experience or fortunate for a lot of reasons,

180
00:17:17,400 --> 00:17:27,080
because it does ground, I think, a person in the ability to be curious about stories,

181
00:17:27,080 --> 00:17:32,680
about other people's stories, and to learn from them and to apply that learning to new

182
00:17:32,760 --> 00:17:40,520
and novel situations. Which is, you know, basically that's so much of what you're doing as well is

183
00:17:42,200 --> 00:17:48,920
how do you deal with surprise? And how you deal with surprise is something that you can learn

184
00:17:49,560 --> 00:17:56,520
through the experiences of characters in books. You can see that experience is something,

185
00:17:56,520 --> 00:18:03,080
oh, that person was surprised. They reacted in this way. And that has the profound implication

186
00:18:03,080 --> 00:18:14,440
for your own feeling about security in the world, how I would react. So the phantom toll booth by

187
00:18:15,240 --> 00:18:24,120
Norton Justin, you know, he, he just recently died in March of this year at age 92.

188
00:18:24,360 --> 00:18:33,800
But he left behind this absolutely wonderful book. You know, there's the little bored boy named Milo,

189
00:18:34,920 --> 00:18:42,040
who's, you know, has this kit, which he has to assemble, which is toll booth. Comes with two toll booths.

190
00:18:42,040 --> 00:18:44,520
It just appears also. That was very, it's

191
00:18:45,320 --> 00:18:49,480
Magically, it's striking to me that this thing just appeared in his bedroom.

192
00:18:49,480 --> 00:18:50,040
Right.

193
00:18:50,040 --> 00:18:51,560
Where did it come from?

194
00:18:52,520 --> 00:18:57,720
Exactly. So that's the first question you're asking at eight or nine, right? It is,

195
00:18:58,440 --> 00:19:07,160
how did Milo end up with this kit in his room? So what does he do with that?

196
00:19:08,360 --> 00:19:14,600
He puts it together. He assembles it, right? So there's an immediately kind of an engineering

197
00:19:14,600 --> 00:19:21,640
puzzle making aspect to the book, which I almost, all the scientists I've talked to,

198
00:19:21,640 --> 00:19:30,040
there's some book like that. Here is a puzzle. If I put it together, what representation do I see?

199
00:19:31,560 --> 00:19:39,640
And so for Milo, he also has a little electric car. So once he's assembled the toll booth,

200
00:19:40,200 --> 00:19:49,000
he's got the map, the rules, two tokens, drops one in, takes off and starts driving. And

201
00:19:51,000 --> 00:19:57,720
I could see where your love and passion for metaphor and analogy would have started at eight

202
00:19:57,720 --> 00:20:05,560
or nine with this book. I mean, a dog's body shape like a cock, a watchdog.

203
00:20:06,280 --> 00:20:15,160
Right. That was another thing. It was all the word play and Hans was just so enchanting.

204
00:20:17,160 --> 00:20:23,880
So yeah, I mean, there you go. You're certainly starting to see that word play,

205
00:20:24,840 --> 00:20:34,680
a clock body for a dog, a watchdog, and then a bug that just brags and brags about its own

206
00:20:35,480 --> 00:20:44,840
abilities and claims, which aren't true, which is a humbug. And so Milo basically teams up with

207
00:20:44,840 --> 00:20:55,960
these two, doesn't he? The humbug and the watchdog to go on an adventure and they end up in the empire

208
00:20:55,960 --> 00:21:03,800
of wisdom. But the problem is there's no rhyme or reason. Right. The princesses.

209
00:21:04,760 --> 00:21:15,080
Exactly. The two sisters. Yes. So they set out for a quest to rescue them, right,

210
00:21:15,880 --> 00:21:25,240
and to bring the two sisters rhyme and reason back into the empire of wisdom.

211
00:21:25,400 --> 00:21:32,760
And again, part of the word play here, it's quite wonderful, is on their journey,

212
00:21:33,320 --> 00:21:40,120
you know, running into the mountain of ignorance where there's the ever present word snatcher,

213
00:21:41,560 --> 00:21:45,160
which is quite the person who's constantly interrupting them.

214
00:21:45,480 --> 00:21:56,200
The terrible triumphant, waste time doing unimportant and trivial tasks. The sense

215
00:21:56,200 --> 00:22:02,440
taker, the person who wastes time filling out countless wasteful forms to no particular end.

216
00:22:04,040 --> 00:22:12,920
So ultimately, they, they succeed, right? Milo. Oh, yes. The humbug, the watchdog. Yes.

217
00:22:13,320 --> 00:22:26,440
By rescuing the two sisters rhyme and reason. Now, what out of that, the phantom of toll booth,

218
00:22:27,080 --> 00:22:34,760
if you can kind of recreate what it was that in your mind as you read it as a child,

219
00:22:35,320 --> 00:22:40,440
how you process that. I mean, you're processing some things which were a kid could be a little bit

220
00:22:40,440 --> 00:22:48,680
maybe frightening, like demons. And some of the demons are described in quite vivid detail,

221
00:22:48,680 --> 00:22:55,080
long nose, green eyed, curly haired, wide mouth, thick neck, broad shoulder, round body,

222
00:22:55,080 --> 00:23:01,960
short arm, bow legged, big footed monster. That's the demon of insecurity or insincerity.

223
00:23:02,600 --> 00:23:10,760
Mm-hmm. Yeah. That, I mean, part of the book, I remember, were some wonderful illustrations

224
00:23:11,480 --> 00:23:17,480
by Jules Pfeiffer. Right. I mean, I didn't know who he was when I was, you know, that child, but I

225
00:23:17,480 --> 00:23:26,040
certainly knew him later on. And they were fantastically evocative and terrifying, some of

226
00:23:26,040 --> 00:23:34,280
them. I still remember that there was a man who had no face. I don't know if you saw that one,

227
00:23:34,280 --> 00:23:42,760
but that was a really terrifying thing. Right. Right. So it introduced a wordplay

228
00:23:42,760 --> 00:23:51,160
world and a visual world in between the covers of the same book. Yes. Yes. And there were, you know,

229
00:23:51,800 --> 00:23:58,920
I, there were two kind of two different cities. One was concerned with words and the other was

230
00:23:58,920 --> 00:24:06,360
concerned with numbers. Right. So that this kind of separation between sort of the world of numbers

231
00:24:06,360 --> 00:24:11,560
and the world of words was one that I think I already was starting to resonate with because

232
00:24:11,560 --> 00:24:20,040
I felt like I had sort of interests in both, in both of those worlds. That's, that's interesting. So

233
00:24:21,560 --> 00:24:28,280
here's a child book, The Phantom Toe Booth, which actually opens up

234
00:24:30,840 --> 00:24:39,560
a window to two different kinds of cultures. Mm-hmm. There's the culture of language and words,

235
00:24:39,560 --> 00:24:47,080
and then there's the culture of mathematics and numbers. And there's a, there are border lines

236
00:24:47,080 --> 00:24:54,360
between those, but here's an author who's able to cross those borders at will, going from numbers

237
00:24:54,360 --> 00:25:00,360
to representations, representations to numbers. That had to be a thrilling thing for you as a child.

238
00:25:01,720 --> 00:25:06,920
Yeah. I remember really, really being struck by that idea that these things were, you know,

239
00:25:07,640 --> 00:25:12,760
they were very separate and that, and there was this, you know, I, I'm sort of only vaguely

240
00:25:12,760 --> 00:25:18,120
remembering a lot of this book I have to admit, but that's one of the things that was striking to me.

241
00:25:19,320 --> 00:25:26,920
That's interesting. So this is a book that I presume you would recommend for, for parents and

242
00:25:26,920 --> 00:25:36,200
teachers and others, if you want to inspire a young person in the world of metaphor and

243
00:25:36,200 --> 00:25:44,840
analogies and mathematical objects, this is, this is a place to start because it, it is accessible.

244
00:25:46,200 --> 00:25:50,920
It's very accessible and kids, children love it. I, I read it to my own two children,

245
00:25:51,880 --> 00:25:59,240
many years ago. And they, I, I don't know if they loved it as much as I did, but I remember them

246
00:25:59,240 --> 00:26:05,880
that they, they, they found it quite amusing, all the word play and puns and everything.

247
00:26:06,760 --> 00:26:12,920
That's interesting. So the tradition of the first book on your list that you had as a child,

248
00:26:13,640 --> 00:26:18,680
you made a point of sharing that experience with your own two children

249
00:26:19,480 --> 00:26:25,320
in order to pass along, which you hope would be a similar kind of experience.

250
00:26:26,280 --> 00:26:30,840
Yeah. And I haven't asked them if they remember that book. I should ask them someday.

251
00:26:31,320 --> 00:26:40,200
No. I think it would be interesting to go back for parents to go back to children, particularly

252
00:26:40,200 --> 00:26:48,360
if they're late teens or early adults and say, do you remember when I, when we read this book

253
00:26:48,360 --> 00:26:54,200
together? Do you, do you have any memories of this? Did it have any impact on your life?

254
00:26:54,200 --> 00:27:02,680
Because it had an impact on mine. I was on a conversation with someone on a talk show

255
00:27:03,720 --> 00:27:07,160
where I talked about this book and I talked about you. Right.

256
00:27:07,560 --> 00:27:12,200
Full circle. Right. Y'all, I'll ask them.

257
00:27:14,520 --> 00:27:22,520
The next book, another wonderful selection is A Wrinkle in Time by Madeleine Naingal.

258
00:27:24,440 --> 00:27:29,560
Again, I'll give just a little bit of an introduction because I don't expect you to

259
00:27:29,560 --> 00:27:34,840
remember all the details of this. You've read a long time ago, but before I get to that,

260
00:27:34,840 --> 00:27:40,920
about what age were you when you read this? That one I remember because I, we read it in

261
00:27:40,920 --> 00:27:49,320
my fourth grade class, which is nine years old. Okay. So at nine years old, you're reading about a

262
00:27:49,320 --> 00:27:57,480
high school student named Meg Murray and her younger brother Charles Wallace and their friend

263
00:27:57,560 --> 00:28:04,200
Calvin O'Keefe. And basically it's a quest, right? Where the father who's a scientist

264
00:28:04,760 --> 00:28:13,480
has gone missing. He's disappeared. No one knows where he is for a year. And suddenly out of kind

265
00:28:13,480 --> 00:28:21,800
of nowhere, this is the magic of children's literature, someone arrives called Mrs. Wetzit.

266
00:28:22,680 --> 00:28:29,160
And Mrs. Wetzit is one of these creatures from another dimension

267
00:28:30,040 --> 00:28:37,480
who comes with information about the father. And she also has two friends,

268
00:28:37,480 --> 00:28:48,520
Mrs. Who and Mrs. Witch. So Mrs. Wetzit, Mrs. Who and Mrs. Witch become the guides

269
00:28:49,240 --> 00:28:55,320
for these three youngsters who are able to find a wrinkle in time, a pteroset,

270
00:28:56,680 --> 00:29:03,240
to be able to go into another dimension to follow the trail of the missing father.

271
00:29:04,920 --> 00:29:15,080
Now, so what we have then is this notion of searching for a parent, which is something I think

272
00:29:15,080 --> 00:29:20,680
all children can probably relate to. The father's missing. What can I do to find my dad?

273
00:29:21,320 --> 00:29:26,520
What help can I find? It goes back to your earlier point where social creatures,

274
00:29:27,240 --> 00:29:35,480
we seek other people around us that we trust to help us on our quest. This goes from the Lord of

275
00:29:35,480 --> 00:29:44,600
the Rings to a wrinkle in time. It's the same kind of dynamic. As you find your allies, those people

276
00:29:44,600 --> 00:29:49,960
that you've bonded with who see the world pretty much like you do, that will watch your back when

277
00:29:49,960 --> 00:30:02,680
you're going into the unknown. So what I like about the story is the secret weapon that Meg

278
00:30:02,680 --> 00:30:11,800
discovers through Mary's trials and tribulations of trying to find the father. And ultimately,

279
00:30:11,800 --> 00:30:20,840
she goes back to confront this dark AI kind of force, which is taken over and made everything

280
00:30:20,840 --> 00:30:35,080
the same and controls the mind of everyone within this AI's domain. Except Mrs. Wetzit says there is

281
00:30:35,160 --> 00:30:43,480
a secret weapon. You'll have to look deep in yourself and you'll find it. If you can find that

282
00:30:43,480 --> 00:30:52,760
weapon and you go back to this place and use it, you will be able to find and retrieve your father.

283
00:30:53,320 --> 00:30:54,680
Do you remember what the weapon was?

284
00:30:59,640 --> 00:31:06,360
Love. Yes, that's what I was going to say. Right. Yeah, it was a little corny there.

285
00:31:10,040 --> 00:31:19,080
But yes. Corny in a way, but on the other hand, very humanizing because it is, again, with that

286
00:31:19,080 --> 00:31:28,040
notion of self-sacrifice. And maybe that's something that we do well as humans, and maybe we should

287
00:31:28,040 --> 00:31:37,400
do better. But the real notion of love is that ability to sacrifice yourself for another. It's

288
00:31:37,400 --> 00:31:42,840
not necessarily just the romantic part of it. It's a self-sacrifice. And here's a daughter

289
00:31:42,920 --> 00:31:53,640
who is willing to self-sacrifice for her father that disabled the AI. AI was defeated at that point.

290
00:31:55,880 --> 00:32:04,920
Gosh, I don't even remember that it was an AI. Was it a machine? Well, I'm saying an AI. It's called

291
00:32:05,480 --> 00:32:13,800
it. Okay, I do remember that. Yeah. And it's, my notes here, all objects and places appear exactly

292
00:32:13,800 --> 00:32:23,480
alike because the whole planet must conform to the terrifying rhythm pulsation of it. A giant

293
00:32:24,040 --> 00:32:32,600
disembodied brain. Now, that seems to me to be a reasonably good description of for a child of

294
00:32:32,600 --> 00:32:42,840
what an AI is. This big disembodied brain, in this case, having a kind of super level of intelligence

295
00:32:42,840 --> 00:32:50,840
as opposed to the kind of AI intelligence that confuses a writing of iPod for an apple.

296
00:32:51,240 --> 00:33:04,360
Right. Yeah. I mean, the thing that for me that stands out in my memory about this book is,

297
00:33:05,560 --> 00:33:13,240
first of all, the female scientists, like both Meg and her mother. Her mother was a brilliant

298
00:33:13,240 --> 00:33:19,560
scientist. And I really, you know, I think this was one of the first books I had read or

299
00:33:21,320 --> 00:33:27,800
encountered that had such characters that had female scientists, people who are interested in

300
00:33:27,800 --> 00:33:35,720
science. And also this notion of multiple dimensions. That was sort of my first introduction to that

301
00:33:35,720 --> 00:33:41,640
idea of like the fourth dimension and this notion of a tesseract. And I remember that very clearly,

302
00:33:41,640 --> 00:33:48,520
that that, you know, explaining that this is a four dimensional cube. And it was just fascinating

303
00:33:48,520 --> 00:33:58,680
to me. I think that's exactly the kind of thing I was hoping to hear is that here's a book that

304
00:33:58,680 --> 00:34:09,480
opened up the role of genders. And that you could see for the first time as a child that women are

305
00:34:09,480 --> 00:34:19,000
as capable as men, and that women can be scientists, that they can also head up a quest and an

306
00:34:19,000 --> 00:34:29,800
adventure. It doesn't have to be a boy. It can be a girl who actually does all the hard work of

307
00:34:30,760 --> 00:34:38,200
putting together the team, charting the journey, assessing the risks, and ultimately finding the

308
00:34:38,200 --> 00:34:48,520
solution. So that must have made a bit of a change in terms of how you thought about your own

309
00:34:48,520 --> 00:34:55,720
potential as a young girl. It probably did. You know, I don't remember that explicitly, but I do

310
00:34:55,720 --> 00:35:03,400
remember, you know, very much identifying with the Meg character. And also just being very intrigued

311
00:35:03,400 --> 00:35:11,080
by her mother. Right. Because at that time, my own mother at that time was a housewife.

312
00:35:13,160 --> 00:35:20,600
My father was an engineer. I hadn't really encountered women doing being scientists.

313
00:35:21,400 --> 00:35:29,880
But maybe that's part of the real beauty of early reading is it provides multiple role models for

314
00:35:29,880 --> 00:35:36,600
children. Because when you grow up, your role models are your mom, your dad. Okay, I have a model of

315
00:35:36,600 --> 00:35:43,960
a man as an engineer, a model of a woman as a housemaker. And suddenly you're reading a book where,

316
00:35:44,600 --> 00:35:52,200
you know, there are other models out there. There are other kinds of things that people do, other

317
00:35:52,200 --> 00:36:03,560
kinds of identities that they're able to shape and create a life with. So that so that that book,

318
00:36:03,560 --> 00:36:10,920
I could see would be a quite important foundational one for your own kind of psychological development

319
00:36:10,920 --> 00:36:17,960
and perhaps confidence that yeah, there are some other roles out there for women other than which

320
00:36:17,960 --> 00:36:30,440
just what I'm saying around me. Yeah, definitely. Okay, so to leave the world of fiction for a moment,

321
00:36:30,440 --> 00:36:39,320
to look at the next book on your list, which is The Universe and Dr. Einstein by Lincoln Barnett.

322
00:36:39,320 --> 00:36:46,440
Now, I've heard you say on another webcast that you read this in high school, and it was responsible

323
00:36:46,520 --> 00:36:54,280
for you to pursue a degree in physics. Right, so that I absolutely I read that in high school.

324
00:36:54,840 --> 00:37:01,800
I think it was suggested by my physics teacher. And I absolutely loved it. You know, it was a

325
00:37:01,800 --> 00:37:10,520
popular exposition of Einstein's theory of relativity. It was fairly short, extremely

326
00:37:11,240 --> 00:37:20,920
clear and well written. And it just, you know, astounded me, these ideas. And the fact that you

327
00:37:20,920 --> 00:37:30,280
could just, you know, Einstein relied quite a bit on thought experiments, you know, what she

328
00:37:30,280 --> 00:37:38,280
called gedonkin experiments, where he didn't actually have a lab or, you know, be doing,

329
00:37:38,280 --> 00:37:42,600
you know, writing down lots of equations. But he was just kind of thinking about how things might

330
00:37:42,600 --> 00:37:49,320
work. And thinking, you know, well, what happens if the speed of light is constant?

331
00:37:50,040 --> 00:37:55,400
But what would that imply? And all of that was just very eye opening for me. And I decided I

332
00:37:55,400 --> 00:38:02,680
wanted to study physics in college. It's interesting. Again, I can see a little bit of a link here

333
00:38:02,680 --> 00:38:09,000
between the wrinkle of time, because he's dealing with other dimensions. And also, he's dealing with

334
00:38:10,680 --> 00:38:18,440
reasoning, not in a lab, but by way of analogy, by way of visualization. So he's using

335
00:38:18,440 --> 00:38:28,280
metaphors, and he's using concrete examples of trains moving in order for that thought experiment

336
00:38:28,280 --> 00:38:35,560
to allow, in which he's doing a reversal, he's using analogies as a reversal process

337
00:38:36,600 --> 00:38:48,040
to come up with an abstraction about an aspect of time, space, velocity, momentum, and

338
00:38:49,880 --> 00:38:57,160
to come up with the mathematics of the metaphors that he thought at his desk.

339
00:38:58,360 --> 00:39:04,200
Exactly. Yeah. So that was, that was quite, quite fascinating for me.

340
00:39:05,240 --> 00:39:11,240
So again, I think, you know, I can see this a little bit in your own work that you,

341
00:39:11,240 --> 00:39:17,880
you're able to see the metaphor making isn't just a translation of the hard mathematics.

342
00:39:18,600 --> 00:39:27,160
Einstein in this book, the universe, and Dr. Einstein, shows that it can go the other way as

343
00:39:27,160 --> 00:39:37,320
well. You can go from the childlike storytelling of multidimensions to creation of mathematical

344
00:39:37,320 --> 00:39:47,000
models that try to come up with a more abstract general way of explaining what a phenomenon is.

345
00:39:47,000 --> 00:39:57,640
Is that? Yes. Yeah. I mean, much later, I read another book, you know, now as an adult about

346
00:39:57,640 --> 00:40:06,600
Einstein, and it hypothesized that, you know, Einstein was a patent officer in Switzerland.

347
00:40:07,240 --> 00:40:10,680
And one of the things that people in Switzerland were trying to do at the time

348
00:40:11,640 --> 00:40:17,400
was, how do you synchronize clocks between train stations? Because this was an important

349
00:40:17,400 --> 00:40:22,120
thing to make trains, you know, be able to follow their schedule. So, so he was thinking about this

350
00:40:22,120 --> 00:40:29,320
extremely practical problem. But thinking about the implications of what does it mean to synchronize?

351
00:40:29,320 --> 00:40:37,720
What does it mean for time to be synchronized? And blowing that up into this incredibly profound

352
00:40:38,440 --> 00:40:47,640
truth about the universe and about time itself. So, you know, that's like, as you say,

353
00:40:47,640 --> 00:40:54,680
it's kind of the reverse metaphor. It's like he sees the very concrete thing of trains and

354
00:40:54,680 --> 00:41:03,720
synchronization and so on. And then he is able to expand that idea into something much more general

355
00:41:03,720 --> 00:41:10,680
and much more profound. You wonder if he had that kind of aha moment because, you know, the

356
00:41:10,680 --> 00:41:18,360
history of our species, you know, for 200,000 years would have been, we think of time as absolute,

357
00:41:19,160 --> 00:41:26,360
of spaces unchanging and absolute. And suddenly to have a thought, it's not absolute,

358
00:41:27,160 --> 00:41:34,280
it's relative. And to take that on the implications of that on, and I think we're,

359
00:41:34,280 --> 00:41:41,480
we're still trying to come to terms with the implications of what that means with time being

360
00:41:41,480 --> 00:41:51,240
relative. Because all of our earthly experiences prepare us for kind of an absolute, absolutist

361
00:41:51,320 --> 00:41:58,920
notion of time. And you're someone who challenged what is probably one of the bedrock notions

362
00:42:00,040 --> 00:42:06,840
that most people have. And I can see you're in high school now, and you're thinking,

363
00:42:07,800 --> 00:42:15,080
if time can, is not absolute and can be challenged, what else is there in physics

364
00:42:15,080 --> 00:42:22,920
that will allow me to do similar kinds of challenges to what are basic understandings of

365
00:42:22,920 --> 00:42:27,960
the world, which work on one level, but are fundamentally incorrect?

366
00:42:31,480 --> 00:42:40,120
Yeah, I don't know if I was thinking all that, but yeah, I didn't, yeah, I was trying to just get

367
00:42:40,120 --> 00:42:49,960
my mind around that, that whole very counterintuitive ideas. Yes, yes. So by the time you're in high

368
00:42:49,960 --> 00:43:01,960
school, you've decided that physics is the career path that you wanted to take. And you've, you've

369
00:43:01,960 --> 00:43:08,760
said that this, this particular book is one of the books that probably was influential in setting

370
00:43:08,760 --> 00:43:18,680
you on that path. Yes. So again, I think it shows childhood reading has enormous implications,

371
00:43:19,880 --> 00:43:25,880
because what it does is it starts a train of thought, and that train of thought leads people

372
00:43:25,880 --> 00:43:32,520
into a direction which they may not have otherwise taken, but for having come across to read that

373
00:43:32,520 --> 00:43:35,480
book. Absolutely. Yes.

374
00:43:39,400 --> 00:43:49,320
The next book, Fads Balaces in the Name of Science by Martin Gardner. I hadn't been familiar with

375
00:43:49,320 --> 00:43:57,000
this book, but it's an absolutely timely book to, to discuss. I mean, this is pre-internet,

376
00:43:57,000 --> 00:44:08,200
pre-fake news, you know, pre silos on Twitter and Facebook and the rest of it. He's taking on

377
00:44:08,920 --> 00:44:18,760
the whole kind of area of quack remedies, of cranks, and how cranks have, you know, historically been

378
00:44:18,760 --> 00:44:24,520
able to get quite a good audience. They didn't need social media to come along to give them an

379
00:44:24,840 --> 00:44:32,280
audience. I mean, now that has been quite effective, but before they had publishers

380
00:44:33,320 --> 00:44:40,280
that were quite willing to publish their books because there was an audience for what cranks

381
00:44:41,480 --> 00:44:45,160
had to say, even though it was clearly wrong.

382
00:44:45,560 --> 00:44:56,040
So, tell me a little bit about what age you were and the circumstances of coming across this book.

383
00:44:58,840 --> 00:45:06,360
So, I think I was in college. I don't quite remember, but I loved Martin Gardner. You know,

384
00:45:06,360 --> 00:45:14,200
he was a long time columnist for Scientific American. He was an incredible communicator

385
00:45:15,240 --> 00:45:25,480
of math. He was a math puzzle fanatic and a math puzzle creator and a wonderful writer,

386
00:45:27,320 --> 00:45:34,440
just super playful person interested in all kinds of things. So, I had been reading, you know,

387
00:45:34,440 --> 00:45:42,120
I read his column and I picked up, I guess I picked up this book and found it really just,

388
00:45:42,120 --> 00:45:49,560
you know, thinking about what drives people to believe things and sort of, and as you say,

389
00:45:49,560 --> 00:45:57,880
it's very relevant now is sort of, what is science? Why do we believe science? What is

390
00:45:57,880 --> 00:46:04,840
different between science and all these kind of quack beliefs? And that's something, you know,

391
00:46:04,840 --> 00:46:10,680
I think we all still struggle with. I think it's probably right. I mean, one of the things that

392
00:46:10,680 --> 00:46:19,640
seemed to me from my reading of the book is the central role that Gardner thought that the

393
00:46:19,640 --> 00:46:26,120
scientific community played. And again, it goes back to that kind of social role that we are

394
00:46:26,920 --> 00:46:36,520
social people and that we learn from each other. And usually, he says the crank is someone who's

395
00:46:36,520 --> 00:46:43,800
almost always outside the scientific community. They don't have colleagues to bounce the ideas off

396
00:46:43,800 --> 00:46:48,680
and not going to conferences, listening to other people's papers. They're not presenting their

397
00:46:48,680 --> 00:46:58,440
papers for peer review or for comment by other people. And as a result, it's easy to get off

398
00:46:58,440 --> 00:47:05,800
the rails because there's no one to say you're off the rails. Right. And they get these cult

399
00:47:05,880 --> 00:47:18,120
followings where they get reinforced, you know, people people are believing them. And he said that

400
00:47:18,120 --> 00:47:28,840
there are really a couple of signifiers for a crank is one of them is that person stands entirely

401
00:47:28,840 --> 00:47:35,080
outside the closely integrated channels through which new ideas are introduced and evaluated.

402
00:47:35,800 --> 00:47:44,120
And secondly, as a tendency to paranoia. But I think one of the things that people may say

403
00:47:44,120 --> 00:47:52,280
is, well, the scientific community is not that perfect. That maybe some of the grants and the

404
00:47:52,280 --> 00:47:59,880
money that comes from them are twisting the dogma in ways that favor the people who are funding

405
00:47:59,880 --> 00:48:06,600
studies and so forth. And therefore, can we really trust dogma that's coming out of a scientific

406
00:48:06,600 --> 00:48:21,560
community that is not as pure science as we would wish it to be. Sure. And science is done by humans,

407
00:48:21,560 --> 00:48:27,000
right. And humans have their own prejudices and biases and

408
00:48:29,000 --> 00:48:34,680
influences. And it's just a human endeavor. But what we do have in science is we have

409
00:48:35,800 --> 00:48:43,160
replication, we have, you know, community sort of think things are wrong all the time. But

410
00:48:43,160 --> 00:48:51,480
eventually, people discover that they're wrong. We have kind of this notion of consensus.

411
00:48:52,200 --> 00:48:57,320
There's certain things that are consensus, eventually, in the scientific community. And

412
00:48:58,280 --> 00:49:05,560
that those consensus, the consensus believe can eventually be overturned. But it's,

413
00:49:06,280 --> 00:49:18,280
it's, it is a community. And I think, you know, it has, I don't know how to say this,

414
00:49:20,920 --> 00:49:26,600
why should we trust scientists as opposed to anyone else? It's because I think scientists

415
00:49:28,120 --> 00:49:34,280
are, I mean, part of the deal of being a scientist is that you're always trying to disprove your own

416
00:49:34,280 --> 00:49:41,880
work. And you try as hard as you can. And that's the fundamental thing. You always are trying to be

417
00:49:41,880 --> 00:49:48,760
skeptical of everything in your own work. And if you pound on it and pound on it and people

418
00:49:48,760 --> 00:49:54,280
replicate it and replicate it, finally you come to as close as you can to what you might call

419
00:49:54,280 --> 00:50:02,360
verification. So yeah, I mean, it's perfect for sure. That's a very good description. I mean,

420
00:50:03,320 --> 00:50:10,360
I think in terms of sciences, also it creates a cultural, cultural aspect of thinking,

421
00:50:11,080 --> 00:50:17,480
where scientists tend to be comfortable with uncertainty, at least more comfortable than

422
00:50:17,480 --> 00:50:24,520
most people, because they understand that even the existing dogma is a tentative position.

423
00:50:24,520 --> 00:50:31,320
It's not yet disproved. There will be flaws in it, in that every time someone thinks they've come

424
00:50:31,320 --> 00:50:39,000
up with an absolute answer, they've usually found out that that's not the case. And so there is more

425
00:50:39,000 --> 00:50:47,480
of a humility. I mean, you have some big eagles in science, as you would expect. But the idea is

426
00:50:47,480 --> 00:50:56,920
to be humble in the face of new changing information that requires constant revaluation

427
00:50:57,080 --> 00:51:05,240
and updating, where the people who are the cranks have the absolute truth. There is no updating

428
00:51:05,240 --> 00:51:12,600
that is needed. It's all defense of the existing structure. The structure is perfect,

429
00:51:13,240 --> 00:51:21,480
it can't be improved upon, and you either believe this or you don't. You're either part of our

430
00:51:21,480 --> 00:51:27,240
belief system or you're not. That, it seems to me, is very anti-scientific.

431
00:51:30,600 --> 00:51:37,400
So the book by Gardner, and his books have appeared on the list of other guests as well.

432
00:51:37,400 --> 00:51:44,040
He's someone who's had, I think, a profound influence in terms of educating a whole

433
00:51:45,000 --> 00:51:51,720
couple of generations of thinkers and mathematicians and physicists and artists and others who

434
00:51:52,760 --> 00:51:59,400
want to look deeply at those kinds of ways of thinking and processing reality.

435
00:52:00,120 --> 00:52:06,360
And science is just a different way of processing reality through investigation,

436
00:52:06,360 --> 00:52:14,680
observation, testing, reevaluation, and understanding that no matter what the result

437
00:52:14,680 --> 00:52:20,440
appears to show today, it may be overturned in the next week.

438
00:52:22,440 --> 00:52:29,000
I think one other thing you can say about Martin Gardner is that he was an incredible

439
00:52:29,560 --> 00:52:40,600
expositor of science and math, and kind of was a model for me, myself, of trying to write

440
00:52:40,600 --> 00:52:46,600
for the general public about science. How do you make things clear? How do you make things

441
00:52:46,600 --> 00:52:54,280
entertaining? How do you make things accessible? I'm glad you mentioned that, because the reviews

442
00:52:54,280 --> 00:53:02,680
of your books, all point to the fact that you've done this with enormous success, is to be able to

443
00:53:02,680 --> 00:53:11,400
use those metaphors and analogies to go from the very hard mathematical world of pure science

444
00:53:11,960 --> 00:53:18,760
into another language that most people can understand and relate to. That's a very

445
00:53:18,760 --> 00:53:25,720
particular skill. It's a little bit like translation. Where I live, translation is a very

446
00:53:25,720 --> 00:53:34,920
important aspect of life, and you get people who are, say, bilingual, but as I say, there's a

447
00:53:34,920 --> 00:53:42,120
difference between being bilingual and bicultural. To know another language is not necessarily to

448
00:53:42,120 --> 00:53:50,200
know that culture. You are bilingual and bicultural, and that's a real advantage, and I think someone

449
00:53:50,200 --> 00:53:58,600
like Martin Gardner was like that as well, who understood both cultures and understood the

450
00:53:58,600 --> 00:54:02,920
language and the barriers of communication between those cultures.

451
00:54:11,400 --> 00:54:20,920
Just for the people watching, some of the chapters in this book by Martin Gardner

452
00:54:20,920 --> 00:54:28,760
on fads and fallacies, he takes on the flat earth people, the hollow earth, I hadn't heard that one

453
00:54:28,760 --> 00:54:41,640
before, monsters of doom, flying saucers, down with Einstein, dousing rods and doodle bugs, geology

454
00:54:41,640 --> 00:54:52,520
versus Genesis, Atlantis, the Great Pyramids, medical cults, from homeopathy to natural

455
00:54:52,520 --> 00:55:04,760
apathy, and food fattice, so dynetics. He covers a fair range of the traditional kind of cult-like

456
00:55:05,480 --> 00:55:11,720
absolutists, fad friends that a lot of people were attracted to.

457
00:55:13,480 --> 00:55:21,400
Still are, in some cases. It still are, and as a result, maybe this is a book that has to be

458
00:55:22,200 --> 00:55:32,440
updated for modern times, because social media now is the main vehicle through the internet

459
00:55:33,160 --> 00:55:42,200
of being able to create these kinds of pseudoscientific communities which they talk to each other and

460
00:55:42,200 --> 00:55:49,080
reinforce each other, but they're reinforcing basically a highly flawed, mistaken view of

461
00:55:49,080 --> 00:55:57,080
reality. So the question is, how do you reach them? Martin Gardner's book reached you and probably

462
00:55:57,080 --> 00:56:02,840
reached a lot of people of your generation, but the question is, for the new generation,

463
00:56:04,040 --> 00:56:12,760
it will this book be one that will allow them to have a different window on social media once

464
00:56:12,760 --> 00:56:20,200
they've read it? Yeah, I don't know, I don't know if books are even relevant anymore for a lot of

465
00:56:20,200 --> 00:56:28,040
people. Well, this is becoming an issue, and it's part of the reason for the show,

466
00:56:28,920 --> 00:56:37,560
is to show the relevance of books. They've been very relevant in your life. I mean, so far when

467
00:56:37,560 --> 00:56:46,760
the books that we've gone through, we've seen the Professor Mitchell emerge as a mathematician,

468
00:56:47,720 --> 00:56:54,040
as a physicist, and as a computer science, artificial intelligence person,

469
00:56:55,160 --> 00:57:01,240
that part of that process, I'm certain it's far more complex, and of course complexity is another

470
00:57:01,240 --> 00:57:09,480
one of your areas, more complex than just childhood reading, but childhood reading is part of that

471
00:57:09,560 --> 00:57:18,840
nonlinear interplay of factors that have made you construct a model of reality that has served you

472
00:57:18,840 --> 00:57:27,880
very well. So the idea is, when people say books are not relevant, I think they're missing a very

473
00:57:27,880 --> 00:57:34,200
key important role in how they're relevant and why they're relevant at a particular age.

474
00:57:35,080 --> 00:57:41,080
Right. I mean, I was saying that facetiously, of course, but the young people I know today

475
00:57:41,080 --> 00:57:49,720
are tend to read a lot less or fewer books than I did as a child. They're reading things on the

476
00:57:49,720 --> 00:57:58,040
internet, as you say. We didn't have a choice. We only had books. Correct. So you're probably

477
00:57:58,040 --> 00:58:04,440
finding this then in your students? My students, oh yeah, my students, my own children,

478
00:58:06,520 --> 00:58:18,280
they just have so many different options for media that books are not number one on their list,

479
00:58:18,280 --> 00:58:28,440
I think, the way they were for me. Which is too bad. Yeah. Well, let's hope that this interview,

480
00:58:28,440 --> 00:58:37,560
this conversation, will inspire young people to say, I want to be like Melanie. I would love

481
00:58:37,560 --> 00:58:45,480
to be able to follow that career path. And here, if it worked for her, maybe this will be the kind

482
00:58:45,560 --> 00:58:54,520
of fine tuning that will help sculpt my own mind in a similar direction. That's the goal. We'll see.

483
00:58:57,800 --> 00:59:10,680
The next book is One, Two, Three, Infinity by George Gamov, which came out in 1947

484
00:59:11,560 --> 00:59:17,080
and explores the fundamental concepts of mathematics and science.

485
00:59:20,680 --> 00:59:24,200
Tell us a little bit about your initial reading of the book.

486
00:59:26,200 --> 00:59:34,600
Yeah, so this one I read in college. Okay. And I was thinking about all these

487
00:59:35,560 --> 00:59:42,280
fundamental ideas, you know, fascinated to read all of this stuff. I remember

488
00:59:43,240 --> 00:59:51,080
being very impressed and influenced by it, although I don't totally remember all the content of the

489
00:59:51,080 --> 01:00:00,840
book. But I do remember it was very fundamental ideas, especially in mathematics. There are two

490
01:00:00,920 --> 01:00:09,720
points in my research of the book that now make me think it had an influence on you is one,

491
01:00:09,720 --> 01:00:16,600
it's noted for its quirky sense of humor. That seems to be something that's a thread through

492
01:00:16,600 --> 01:00:23,240
all the books that we've looked at so far. And secondly, it's noted for its memorable metaphors.

493
01:00:24,200 --> 01:00:30,200
Again, again, something that is a thread through all the books that you've chosen

494
01:00:30,920 --> 01:00:36,600
that influenced you as a child. So you were growing up probably without realizing it,

495
01:00:36,600 --> 01:00:42,920
that you were having a master class with some of the best mentors on the planet,

496
01:00:43,880 --> 01:00:52,200
teaching metaphorical thinking, and teaching humor and ways of entertaining to get people's

497
01:00:52,200 --> 01:00:59,720
attention and to explain sometimes very hard, difficult, abstract concepts like number theory,

498
01:01:01,640 --> 01:01:04,440
which is done in this book.

499
01:01:07,000 --> 01:01:14,520
Yes, absolutely. So I think a lot of these books, I kind of read them and almost,

500
01:01:15,160 --> 01:01:21,640
you know, the way that like a film student will analyze a film at a much more kind of

501
01:01:22,440 --> 01:01:28,200
like trying to figure out how did the director pull that off? How did they set up that scene?

502
01:01:29,080 --> 01:01:34,680
I was quite interested in how do you explain things to people? How do you explain these hard

503
01:01:34,680 --> 01:01:40,200
concepts? So I was looking at how are they doing this? How is George Gamoff actually

504
01:01:40,760 --> 01:01:50,440
pulling this off? It's true because I think it's a very insightful observation you've made,

505
01:01:50,440 --> 01:01:57,320
because in a sense, books like this are a kind of performance art.

506
01:01:59,240 --> 01:02:05,560
And if I look at the other people who have cited this book as foundational,

507
01:02:06,200 --> 01:02:13,480
they are some of the great science communicators. For example, theoretical physicist Sean Carroll

508
01:02:14,440 --> 01:02:21,800
mentions this one, two, three, infinity as setting the trajectory of his professional life.

509
01:02:24,120 --> 01:02:29,560
Cognitive scientist Stephen Pinker read the book as a child and cited as contributing to his

510
01:02:29,560 --> 01:02:38,600
interest in popular science. And astrophysicist Neil deGrasse Tyson has also cited this as one

511
01:02:38,600 --> 01:02:47,320
of the greatest of two books that impact on his development, the other one being James Newman's

512
01:02:47,320 --> 01:02:56,280
Mathematics and Imagination. So here you have three other great scientific translators,

513
01:02:56,280 --> 01:03:06,280
communicators, masters of the metaphor who have had this book come into their life at the right time

514
01:03:07,160 --> 01:03:15,800
to say beyond just metaphor and simile and analogy, you have to find ways to entertain people

515
01:03:16,920 --> 01:03:25,640
and the entertainment is the way that most people learn. And if you're asking them to go back and do

516
01:03:25,640 --> 01:03:31,160
all the foundational for the original abstracts and say of number theory, they're not going to follow

517
01:03:31,240 --> 01:03:37,720
you there, they're not going to go down that path. But if you can give them a watchdog,

518
01:03:37,720 --> 01:03:45,640
a dog with a clock-like body, they're going to stay with you and follow you down the path

519
01:03:45,640 --> 01:03:55,160
because it is fun. Right. And I think George Gamoff also, he was Russian and he had that

520
01:03:55,160 --> 01:04:01,960
kind of very dry wit that the Russian writers often have. I think that was something I really

521
01:04:01,960 --> 01:04:12,600
appreciated. So humor and metaphor from George Gamoff, one, two, three and infinity, a book

522
01:04:13,320 --> 01:04:20,920
which clearly has inspired you and some of the other very important scientific communicators

523
01:04:20,920 --> 01:04:30,520
out there cite this book as well, which takes us to your next book, which is a book of essays by

524
01:04:30,520 --> 01:04:37,320
one of the world's foremost astronomers, Beyond the Observatory by Harlow Shapley,

525
01:04:39,880 --> 01:04:48,040
which is essays on scientific achievements of the 20th century. And one of the things which I

526
01:04:48,040 --> 01:04:55,400
thought was kind of an interesting presentation, Breathing the Future in the Past. I don't know

527
01:04:55,400 --> 01:05:01,800
if you recall that essay or not, but it was showing how your breath contains more than

528
01:05:02,440 --> 01:05:07,960
400,000 Aragon atoms that Gandhi breathed in his long life.

529
01:05:08,680 --> 01:05:21,720
Yeah, I do remember that. Bringing science again alive with an illustration here that makes it very vivid.

530
01:05:24,440 --> 01:05:30,680
Yes. When did you come? Was this a high school college you came across? That was in college.

531
01:05:31,400 --> 01:05:38,760
During college, I became very interested in astronomy and started to actually do some research

532
01:05:38,760 --> 01:05:45,320
in astronomy. We had a small observatory that I got involved in sort of a group of students

533
01:05:45,960 --> 01:05:54,040
who were doing independent research in astronomy. So I picked up this book and one essay that I

534
01:05:54,040 --> 01:06:04,440
remember really clearly was a discussion about how much you can infer from a sort of a single

535
01:06:04,440 --> 01:06:10,440
pinpoint of light. You know, you're looking at a star. All you're getting is very, you know,

536
01:06:11,160 --> 01:06:17,480
pinpoint of light. And then you can infer, like, what is the chemical makeup of the star? How old

537
01:06:17,480 --> 01:06:26,440
is it? Where it fits on the sort of spectrum of different kinds of stars? Is it you can infer? Is

538
01:06:26,440 --> 01:06:32,760
it orbiting another star? Is it going to explode? You know, all of these different things. It's just

539
01:06:32,760 --> 01:06:44,360
astounding what astronomy can achieve with just this incredibly kind of, you know, seemingly very

540
01:06:44,920 --> 01:06:48,760
limited amount of data. So I was very struck by that essay.

541
01:06:50,680 --> 01:06:58,600
Did you think of a career in astronomy? Yeah, I did think for a long time that I was going to

542
01:06:59,880 --> 01:07:07,080
have a career in astronomy and I did several internships in astronomy labs

543
01:07:08,360 --> 01:07:13,240
and thought about going to graduate school in astronomy, but ended up not doing that.

544
01:07:14,520 --> 01:07:22,760
Okay. So I guess maybe one of the lessons is that you can be intrigued and enthralled by a

545
01:07:22,760 --> 01:07:29,960
particular book that leads you in a direction, but ultimately you decide that's not exactly

546
01:07:29,960 --> 01:07:34,280
the full direction of where you want to go. You learn from it and you learn something about

547
01:07:34,280 --> 01:07:42,280
yourself from it and take on that knowledge, but get on with another career path.

548
01:07:44,280 --> 01:07:50,120
Yes, life is full of twists and turns, of course, that you can't predict.

549
01:07:53,320 --> 01:07:59,800
The other essay, which kind of fits into some of the other books that we've discussed,

550
01:08:00,440 --> 01:08:08,520
is the Science Outside the Lab, which was one of the essays there, again, which is back to the

551
01:08:08,520 --> 01:08:20,280
Einstein book that we discussed earlier, and that there is this kind of notion that science isn't

552
01:08:20,280 --> 01:08:31,880
just something that happens in the laboratory. Yeah, I mean, I don't remember that particular essay,

553
01:08:33,480 --> 01:08:39,720
but I assume he was talking about sort of what you see out in real life, I mean, what the kind

554
01:08:39,720 --> 01:08:45,880
of science takes place by observations that are outside of the lab, just like Einstein.

555
01:08:46,600 --> 01:08:54,360
Right, and again, what Shapley's book, from my understanding of these essays,

556
01:08:54,360 --> 01:09:01,160
is it's looking at science in a broad way. It's looking at science, sociology, and philosophy as

557
01:09:01,160 --> 01:09:08,760
well. In other words, the broader context in which science exists. It's a culture within a larger

558
01:09:08,760 --> 01:09:17,960
culture, and you are one of the bridge builders that are crossing that moat where the scientists

559
01:09:17,960 --> 01:09:25,000
are inside, talking to each other in a language which would be incomprehensible for most of us

560
01:09:25,000 --> 01:09:33,640
listening to ancient Greek. But you've mastered that language and can go over and say, this is what

561
01:09:33,640 --> 01:09:39,960
they're really talking about. This is what it really means. Here are some examples. Visualize this,

562
01:09:39,960 --> 01:09:50,840
visualize that. Which talking and visualization takes us to Hind's Teeth, Horses Tolls by Stephen

563
01:09:50,840 --> 01:10:00,120
Gould, which, oh, wonderful writer, I'm getting waiting for my Smithsonian Institute magazine

564
01:10:00,120 --> 01:10:07,400
to arrive here in Bangkok back in the early 90s. And sometimes it wouldn't arrive because I must

565
01:10:07,400 --> 01:10:12,120
have been a postman who shared my interest, decided to keep it himself. But in any event,

566
01:10:13,240 --> 01:10:18,920
he had a wonderful mind. And tell us how this book came into your life.

567
01:10:23,400 --> 01:10:29,560
So when I was in college, one of my housemates was an evolutionary biology major.

568
01:10:30,840 --> 01:10:36,680
And he was telling me how I should learn something about this field because I knew nothing about it.

569
01:10:36,680 --> 01:10:46,280
I'd never taken a biology class. So I tried to get into, there was a very popular course on

570
01:10:46,280 --> 01:10:51,800
evolutionary biology in my college, which I was unable to get into because it was so popular.

571
01:10:53,000 --> 01:10:59,640
So this housemate gave me this book, said, read this, this will teach you everything you need to

572
01:10:59,640 --> 01:11:06,120
know about. And I had never heard of Stephen J. Gould. I didn't know anything about evolution.

573
01:11:08,440 --> 01:11:14,760
So it was a real eye-opener. He's a fantastic writer, just like all of these books are

574
01:11:16,520 --> 01:11:23,720
these master science expositors. And talking about some of these

575
01:11:24,680 --> 01:11:32,040
sort of really interesting questions that you would never have thought to ask about evolution.

576
01:11:32,920 --> 01:11:37,800
So this was my first introduction to a lot of these ideas.

577
01:11:40,200 --> 01:11:46,840
What would you think would have been the influence of the book on your own thinking and your own

578
01:11:46,920 --> 01:11:59,240
choices and career? Well, it sparked my interest in ideas of evolution, which then

579
01:12:01,800 --> 01:12:11,880
went a lot further, especially when I was in graduate school and encountered an

580
01:12:11,880 --> 01:12:19,080
entire field called biologically inspired computation, an evolutionary computation,

581
01:12:19,720 --> 01:12:28,040
which merged ideas from evolution into computer science. And it's something that I got very deeply

582
01:12:28,040 --> 01:12:37,800
involved with later on. So that's an interesting point because in some ways you're drawing an

583
01:12:37,880 --> 01:12:46,440
analogy from the evolutionary biology world of Stephen Gould into a quite different realm

584
01:12:47,000 --> 01:12:55,080
and seeing that that perspective is transferable. Yes, exactly. That you can use,

585
01:12:55,960 --> 01:13:03,720
you could be inspired in a metaphorical way by ideas in one field and apply them into other

586
01:13:03,720 --> 01:13:12,520
fields. And interestingly, that's exactly what happened with Darwin, who sort of came up with

587
01:13:12,520 --> 01:13:19,960
a lot of these ideas. He was inspired by ideas from economics, from geology, and other fields,

588
01:13:19,960 --> 01:13:26,120
and he brought that into his own ideas about biology. So there's a lot of these cross-scientific

589
01:13:26,120 --> 01:13:33,960
analogies going on. It's an excellent point because certainly with Darwin's generation,

590
01:13:34,600 --> 01:13:42,680
there would have been a far broader education. And maybe that's part of the problem we have with AI,

591
01:13:42,680 --> 01:13:52,360
is that people are brilliant, but narrowly brilliant in very specific subdomains of subdomains. And so

592
01:13:52,440 --> 01:14:00,840
as a result, there isn't the same kind of cross-fertilization of ideas. Like for example,

593
01:14:00,840 --> 01:14:10,440
talking with someone who's coding an algorithm about empathy, there may not be an easy bridge

594
01:14:11,320 --> 01:14:17,320
with that coder and with those kinds of concepts simply because the background isn't there. They

595
01:14:17,320 --> 01:14:26,520
haven't come across the Steven Gould model that what you do is you find someone who is able to

596
01:14:26,520 --> 01:14:32,840
explain something brilliantly and see, again, you drew the metaphor of what he's talking about

597
01:14:32,840 --> 01:14:39,480
into another realm. You transported that infrastructure, that framework of thinking,

598
01:14:40,200 --> 01:14:46,200
and allowed you to see something quite different about computers that other people were not seeing.

599
01:14:48,120 --> 01:14:54,120
Well, yeah, I mean, I think I would say I was the one who came up with some of those analogies,

600
01:14:54,120 --> 01:15:05,560
but some of my mentors did. Talking about mentors, let's move on to Goethe Escher and Bach by

601
01:15:05,560 --> 01:15:13,640
Douglas Hofstadter, who I know was your mentor and an important intellectual force in your life.

602
01:15:14,440 --> 01:15:23,480
This book in 1973 was a monumental work. I want to pull a surprise. It is still read and discussed

603
01:15:23,480 --> 01:15:32,600
in the shape thinking of a couple generations since it was first published. So you must have

604
01:15:32,600 --> 01:15:43,640
read this book before you decided to, this was going to be your thing. You were at university

605
01:15:43,640 --> 01:15:55,880
somewhere, you got the book, what happened next? So I read it the year after I graduated from college.

606
01:15:56,520 --> 01:16:03,240
Okay. I actually read about it, I read a review of it in Martin Gardner's column in Scientific

607
01:16:03,240 --> 01:16:09,400
American, which is how I had heard about it in the first place. So I went out and bought it and

608
01:16:11,320 --> 01:16:19,720
it's a very long book. It's not like a beach read, if you will. It takes a lot of thinking.

609
01:16:20,680 --> 01:16:27,400
So I read it over the course of many months and decided that this is what I want to work on.

610
01:16:27,960 --> 01:16:35,720
And it's really a book about, it's about intelligence and how something like intelligence,

611
01:16:35,720 --> 01:16:43,320
as complex as intelligence, can emerge from a non-intelligent substrate, how consciousness

612
01:16:43,320 --> 01:16:49,320
might come about. And it uses ideas from mathematics, from music, from art,

613
01:16:50,040 --> 01:17:03,160
to kind of eliminate these core ideas. What was the connecting thread that Hofstadter

614
01:17:03,160 --> 01:17:09,720
discovered that links together Goethe and Bach and Escher?

615
01:17:13,000 --> 01:17:17,560
I say there are several connecting threads, but maybe one of them is this idea of self-reference

616
01:17:18,680 --> 01:17:27,080
that is fundamental in Goethe's theorem, his mathematical theorem, because you can get mathematical

617
01:17:27,880 --> 01:17:37,320
systems to be talking about themselves, to be referring to themselves. And that shows up in

618
01:17:37,320 --> 01:17:44,280
Escher. You see all these kinds of strange kind of loopy references.

619
01:17:44,920 --> 01:17:52,440
The hand drawing the hand. Yeah, the hand drawing the hand and so on. And in Bach's music, too,

620
01:17:52,440 --> 01:18:03,400
there's some examples that Hofstadter goes into in detail. And this is really the idea of

621
01:18:03,400 --> 01:18:09,480
intelligence is a self-rential, what he calls a strange loop, the strange loop of consciousness,

622
01:18:09,480 --> 01:18:16,680
where we're able to reflect on our own thinking. A kind of recursion. A kind of recursion, exactly.

623
01:18:17,080 --> 01:18:27,880
That goes on. And kind of a meta-thinking as well. So that at each stage recursive part, I mean,

624
01:18:27,880 --> 01:18:38,680
Bach's fugues, for example, there is a thing that reoccurs in them. From Nietzsche's eternal

625
01:18:38,680 --> 01:18:46,920
return. There are things that are patterns that show up against slightly altered, but recognizable.

626
01:18:48,360 --> 01:18:55,480
And that that that is the nature of intelligence, as we understand it.

627
01:18:56,920 --> 01:19:04,440
Something like that. Yeah, I mean, it's a... Hofstadter's ideas are not

628
01:19:05,400 --> 01:19:12,200
easily expressible in a short period of time. So, but that's kind of getting at the idea

629
01:19:12,200 --> 01:19:21,000
of this, what he calls this strange loop. Would you say that this, I mean, this is

630
01:19:21,000 --> 01:19:30,200
absolutely foundational book, that maybe it's at the outer edge of where you can use metaphor and

631
01:19:30,200 --> 01:19:38,200
summary with these abstractions to convey accurately what is being talked about.

632
01:19:40,760 --> 01:19:46,760
Because a lot of people find this book very, very difficult. Yeah, it is difficult. But what's really

633
01:19:46,760 --> 01:19:54,200
striking is how much Hofstadter comes up with amazing analogies and metaphors to talk about these

634
01:19:54,200 --> 01:20:05,160
things. And that's really, turns out to be what he's most fascinated by is how we think in analogies

635
01:20:05,160 --> 01:20:17,160
and metaphors. And he is a master of using these, you know, language and everyday examples

636
01:20:17,800 --> 01:20:22,760
to try and illustrate what's going on in these incredibly abstract ideas.

637
01:20:25,320 --> 01:20:36,040
So, he's a master cultural translator from these various realms. And again, using what,

638
01:20:36,040 --> 01:20:42,120
I mean, I can see the influence because you've done a number of articles on metaphor and

639
01:20:42,120 --> 01:20:50,200
analogy, particularly in the context of AI. So that understanding by way of metaphor and

640
01:20:50,200 --> 01:20:56,840
analogy has obviously been very central to your own development as a scientist and as an interpreter

641
01:20:56,840 --> 01:21:04,120
of science to a larger community. Yeah, and in fact, you know, Hofstadter was the one who

642
01:21:05,000 --> 01:21:12,840
introduced me to the idea of building AI systems that can make analogies. That was the

643
01:21:12,840 --> 01:21:21,160
topic of my PhD dissertation for which he was the advisor. Right. So this notion of analogy and

644
01:21:21,160 --> 01:21:29,560
metaphor kind of circles through all of my research. And it certainly circles through all of your

645
01:21:29,720 --> 01:21:39,160
readings as well. Evidently. You're starting to discover a certain kind of pattern that started

646
01:21:39,160 --> 01:21:47,720
at age eight. Yeah, I never had made that connection. It's kind of like psychotherapy, right?

647
01:21:49,640 --> 01:21:56,440
It's all started in your childhood. Are you still in contact with Professor Hofstadter?

648
01:21:56,440 --> 01:22:06,440
Yeah. Yeah. Talk to him from time to time. Okay. So the next book, unless there's something else

649
01:22:06,440 --> 01:22:12,680
you'd like to say about that, I think we've kind of established the fact that it's been

650
01:22:12,680 --> 01:22:19,960
transformational. I mean, your PhD thesis was about analogies, about the man who had written a book

651
01:22:20,520 --> 01:22:29,320
filled full of analogies and metaphors. And that was a watershed book and a watershed period of

652
01:22:29,320 --> 01:22:40,280
your life. Absolutely. Yep. The Recursive Universe by William Poundstone, which is really kind of the

653
01:22:40,280 --> 01:22:47,160
origin of complexity, which I know is another one of the fields that you've done a fair amount of

654
01:22:47,160 --> 01:22:57,400
research and study and writing. And explain a little bit about how this book came to you

655
01:22:57,400 --> 01:23:05,320
and what it said to you about complexity that still is important to you.

656
01:23:05,320 --> 01:23:18,760
Yes. So this book, I believe I read it in graduate school. I don't remember how I came across it.

657
01:23:20,440 --> 01:23:31,400
But what it is, it takes John Conway's Game of Life, which is

658
01:23:31,880 --> 01:23:43,880
what's called a cellular automaton. It's not exactly a game. It's more like a very idealized model,

659
01:23:46,280 --> 01:23:55,160
very simplified model of complex systems. And it uses this Game of Life and Game of Life is full

660
01:23:55,160 --> 01:24:04,280
of little patterns that people have discovered in things that are gliders and other kinds of

661
01:24:05,000 --> 01:24:17,800
structures that can do all kinds of computations. And Poundstone uses this as a way to talk about

662
01:24:17,800 --> 01:24:25,480
bigger ideas in cosmology and physics. And it's just a fascinating kind of approach to talking

663
01:24:25,480 --> 01:24:31,400
about those ideas. And it was one of my introductions to the Game of Life,

664
01:24:33,240 --> 01:24:38,920
and I think you've seen sort of how complex that all is and how complexity can emerge from these

665
01:24:38,920 --> 01:24:44,200
very simple rules, and then tying it to these much bigger ideas.

666
01:24:46,600 --> 01:24:50,760
So I just loved that book. I thought it was beautiful, and it's not that well known. I mean,

667
01:24:50,760 --> 01:25:01,560
it's surprisingly, I think, underappreciated. I think from what I've read, I haven't read the book,

668
01:25:01,560 --> 01:25:06,040
but I've read about it, and it seems absolutely fascinating in terms of

669
01:25:07,000 --> 01:25:13,560
of this notion of self-assembly, of how you can get very complex systems out of something that is

670
01:25:14,280 --> 01:25:23,000
in itself very simple. And for example, he gives the example of Pi, where, you know, basically

671
01:25:23,800 --> 01:25:30,840
you can encode it using only two terms and end up with this unbelievably complex

672
01:25:31,640 --> 01:25:41,400
number from just the initial to write the components to begin with. So I thought it was a

673
01:25:41,400 --> 01:25:47,080
quite interesting aspect of it. And the other thing is the initial information input and the

674
01:25:47,080 --> 01:25:57,960
relationship of information with entropy. You know, Claude Shannon with Bolson of looking at

675
01:25:57,960 --> 01:26:07,640
those two aspects of the universe, from an informational model to a model of

676
01:26:08,280 --> 01:26:14,200
the second law of thermodynamics or entropy, where things will go into greater disorder.

677
01:26:15,000 --> 01:26:21,720
Yes, exactly. So this book brought together a lot of ideas of complex systems that I had been

678
01:26:22,600 --> 01:26:27,320
thinking a little bit about, but never really found them brought all together,

679
01:26:28,040 --> 01:26:35,400
including entropy, information, computation, you know, he shows, he talks about Conway's proof that

680
01:26:36,280 --> 01:26:43,640
the game of life, which is just, it's just a two-dimensional grid of black and white,

681
01:26:45,160 --> 01:26:51,480
what they call cells, that influence each other in simple ways. But this actually you can

682
01:26:51,480 --> 01:27:01,240
embed an entire computer in this incredibly simple system. And it's just, it's full of very profound

683
01:27:01,240 --> 01:27:10,280
ideas. I know it's kind of fair you teach an online course on complexity, which is highly popular.

684
01:27:10,280 --> 01:27:18,360
I read somewhere like 25,000 students have more now, where I read is probably outdated.

685
01:27:18,440 --> 01:27:21,720
But it seems to be a very popular, is this one of the books that's

686
01:27:22,680 --> 01:27:29,080
in the course? So I don't use the book in that course, but I do talk about a lot of these ideas.

687
01:27:29,720 --> 01:27:39,640
I see. Yeah. So we cover that kind of thing. Yeah. Right. Okay. The last book on the list is

688
01:27:39,640 --> 01:27:45,960
Adaptations in Natural and Artificial Systems by John Holland. Last but not least,

689
01:27:46,920 --> 01:27:53,480
here we have adaptation is a biological process, rearranging genetic material, goes back to what

690
01:27:53,480 --> 01:28:01,640
you're talking earlier about Darwin's and in the evolutionary biology that you found an attractive

691
01:28:01,640 --> 01:28:10,680
way to take into another domain. So adaptation in Natural and Artificial Systems by John Holland,

692
01:28:11,480 --> 01:28:15,560
was this something from your university days or earlier?

693
01:28:15,640 --> 01:28:21,000
This was from graduate school. So John Holland was one of my professors at the University of Michigan.

694
01:28:22,040 --> 01:28:29,160
And he taught a course by, he was a computer science professor, yet he taught a course

695
01:28:29,160 --> 01:28:32,760
in computer science called Adaptation in Natural and Artificial Systems.

696
01:28:34,040 --> 01:28:39,880
It's a very technical book. So it's the only technical book on my list, really.

697
01:28:40,040 --> 01:28:49,080
But it was, John Holland was the founder of this field called Genetic Algorithms,

698
01:28:49,080 --> 01:28:55,720
which brought ideas from evolutionary biology into computer science.

699
01:28:57,400 --> 01:29:07,720
And this book was his theory of adaptation. So when you think about adaptation, maybe you think

700
01:29:07,720 --> 01:29:22,760
about some kind of species adapting to a particular niche. For instance, you have things like butterflies

701
01:29:22,760 --> 01:29:32,840
that the color of their wings change in response to their environment. And so this notion of adaptation

702
01:29:32,840 --> 01:29:40,520
is fundamental to biology. And yet Holland brought that into that field of computer science by saying

703
01:29:40,520 --> 01:29:49,720
what we want in computer science and AI is to have computers that adapt. So we don't want just

704
01:29:49,720 --> 01:29:56,360
living systems that adapt. We want machines that adapt, that are able to adapt to different

705
01:29:56,360 --> 01:30:06,520
environments and to be able to be flexible and learn the way that living systems do.

706
01:30:07,720 --> 01:30:17,480
So Holland became my co-advisor, along with Douglas Hofstetter. And this book was sort of set

707
01:30:17,480 --> 01:30:25,000
the path of a lot of my future research for up until now, really.

708
01:30:25,960 --> 01:30:34,200
Yeah. What seemed to me interesting about this book is the part that plays

709
01:30:34,920 --> 01:30:45,480
just conceptually of kind of perpetual novelty. That there is no kind of in-point that it's aimed

710
01:30:45,480 --> 01:30:52,760
for a particular in-point, that it's always open. And you can't quite predict where it will go,

711
01:30:52,760 --> 01:31:00,600
how that co-evolution or that adaptation will go next. I guess that's part of the non-linearity

712
01:31:00,600 --> 01:31:08,920
that he discusses in the book, is that we have trouble with things like exponential numbers,

713
01:31:08,920 --> 01:31:16,280
non-linearity. These are things that are outside our realm of experiences, like absolute time as

714
01:31:16,280 --> 01:31:24,360
opposed to relative time. And this is where you as a communicator of science try to come in and say,

715
01:31:24,360 --> 01:31:31,480
well, people are looking to take you to an in-point, but you have to be careful. Because the way

716
01:31:31,480 --> 01:31:36,440
in-points work in reality are quite different from the way that they're portrayed.

717
01:31:38,200 --> 01:31:44,840
Right. So one interesting thing about this idea of perpetual novelty that Holland

718
01:31:45,400 --> 01:31:52,600
talks about, you can imagine that in biology where you're having species continually evolving

719
01:31:52,600 --> 01:32:00,440
and changing and the environment's changing. But Holland also brought these ideas into economics.

720
01:32:01,480 --> 01:32:06,920
So interestingly, economics, the theory of economics that people,

721
01:32:08,440 --> 01:32:14,040
the sort of classical economic theory has to do with equilibrium. You want sort of this economic

722
01:32:14,040 --> 01:32:19,080
system to be in equilibrium or markets to be in equilibrium. But this idea of perpetual

723
01:32:19,080 --> 01:32:27,800
novelty says such systems are never in equilibrium. And therefore, the classical theories don't describe

724
01:32:27,800 --> 01:32:35,240
reality where we actually have this continually changing co-adaptive system of all these economic

725
01:32:35,240 --> 01:32:41,160
agents. So that's really been revolutionizing a lot of thinking in economics as well as in

726
01:32:41,160 --> 01:32:53,640
biology and computer science. Again, I guess things like global optimum and equilibrium

727
01:32:53,640 --> 01:33:00,520
are in a way metaphorical, trying to create the notion that there's an in-point where everything

728
01:33:00,520 --> 01:33:09,720
is absolutely balanced as opposed to the fact that in reality, nothing is ever balanced for long.

729
01:33:09,720 --> 01:33:18,200
There may be moments in time where it appears that birds and rabbits and dinosaurs had a

730
01:33:18,200 --> 01:33:26,120
particular adaptation, but then it doesn't last. It's overtaken by something else. And as a result,

731
01:33:26,680 --> 01:33:38,120
you have to live with the uncertainty of a perpetual changing environment and adaptation

732
01:33:38,120 --> 01:33:46,840
to those changes. Yes. I think one of your previous guests, John Allen Palos, has a great

733
01:33:46,840 --> 01:33:51,720
quote where he says, you know, living with uncertainty is the only certainty.

734
01:33:54,360 --> 01:34:02,520
That's something that I could just hear John saying. John's a friend I've known for years.

735
01:34:02,520 --> 01:34:09,800
Yeah, very, very brilliant mathematician and, again, a communicator of a high level

736
01:34:10,680 --> 01:34:16,600
as you are as well. So I want to thank you for this been a delightful conversation.

737
01:34:16,600 --> 01:34:19,880
Let me know, what do you think about the experience of being on the show?

738
01:34:22,760 --> 01:34:29,080
It's been a lot of fun. I mean, it's intimidating to try and have to think back on books that I've

739
01:34:29,080 --> 01:34:35,960
read years and years and years ago. But you reminded me of a lot of things that I'd forgotten,

740
01:34:36,520 --> 01:34:42,680
which has been great. And you also pointed out so many connections that I had never made about,

741
01:34:43,240 --> 01:34:47,400
you know, different the books, these books that I've read and the things that I've been

742
01:34:47,400 --> 01:34:54,600
thinking about for a long time. So I thank you for that. And thank you. This has been a wonderful

743
01:34:54,600 --> 01:35:04,200
conversation. And I will continue to follow your writings and learn from them. You're a master of

744
01:35:04,840 --> 01:35:12,760
communication between the arts and the science. And you contribute to both. And I think you have

745
01:35:13,560 --> 01:35:20,920
done a great job in that communication. Please keep up this wonderful work.

746
01:35:20,920 --> 01:35:33,240
Thank you so much. Bye for now. Okay, bye bye.

