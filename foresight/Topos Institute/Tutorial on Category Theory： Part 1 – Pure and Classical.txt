Okay, well, welcome to the tutorial on category theory.
This tutorial will be in two parts and they're only loosely connected.
So in part one, I'll be talking about pure classical category theory, which has been used in math for the last 75 years.
And in part two, which you can watch pretty much independently, I'll be talking about applications of category theory.
This video will be fairly long over an hour, and the other one will be fairly short, or comparatively short.
So let's get started here.
So, so just a tiny bit of history, it was category theory was invented by Eilenberg and McLean in 1945 ish is basically invented for mathematicians in order to organize the subject.
And they might say that they invented it to, to make a definition of natural transformation, or not even going to get that far today.
But, but really, it's been used, and beginning mainly by homotopy theorists like Whitehead, but it's, it was used to organize mathematics, I would say so a major advance was in 58 when Daniel Khan invented adjoint
mathematics, then Alexander Grothendieke really emphasized abstraction and universal properties throughout the 50s and 60s, and bill of here connected Grothendieke's notion of topos to topos is to logic and 60s invented categorical logic.
And then the Grothendieke school proved the Bay conjectures so Deline Grothendieke, I guess, a famous conjecture or set of conjectures and number theory.
And it kind of showed that this is not just abstraction but can do, do math that that you know the most serious and proud mathematicians would consider important.
And then, in terms of applications, although that's mainly going to be the focus of the second video.
Category theory has been applied for a long, long time to dynamical systems to programming languages to quantum processes much more recently in 2000s, and to linguistics.
So it's really got a lot of applications. But today, in this video, I'll focus on just the pure, more pure stuff.
What I'll often ask is category theory just a language and something about this seems funny to me I mean just a language to me that's, it's like, for me, language is what most separates us from other animals, it's, you know, if I, if I was going to have a tribe with like a really
strong alpha male in it or a tribe that had a really good language for coordinating I think that the coordinating tribe would easily be the other one so language is extremely powerful.
Arabic and Roman numerals they're both talking about numbers, and it's maybe just a difference in language.
And yet, one is much, much more like can you imagine doing finance compound interest in Roman numerals or, you know, any of, like in finance is not very high tech math, but you couldn't have a financial system like we have today with Roman numerals.
So, so just a language. So DNA is kind of a language right it's an operational code, you have four letters, but they, they actually have properties, you know, these, the nucleotides actually mean something and they can be operationalized into
proteins and so a good language is extremely powerful for organizing your thoughts for selecting what you want to do, selecting what's most efficient for a context right as in DNA.
So,
I think it's, it's a strange point of view to say just a language.
And it is like, apparently the pure ha language aren't compositional you can't form clauses and.
And so there's only a finite number of sentences they can make. And, and basically, when you have a lot of shared experience with people.
You don't maybe you don't need a lot of compositionality category theories on the other end of the spectrum.
In the pure ha world, so much can be said without language. And somehow that's, that's very analogous because when I think of when I go into a new mathematical field and try to learn about it often it feels like so much is being communicated without being said.
So here's what my colleague David Jazz Meyers talks about there's the fourth rumsfeldian fear so rumsfeld.
I think he's a secretary of defense in the US and he said, you know, you have the known knowns and the known unknowns but it's the unknown unknowns that really get you.
And we're here we're talking about the unknown knowns the things that you know with each other, but you don't even wreck you don't recognize that you know.
And because of those things that you're assuming without really being conscious of them, and the ways that you communicate, suddenly without without language, you somehow are able to communicate within yourselves but no one else can understand you.
So category theory is kind of the other on other end of the spectrum. It's built to be compositional it's built to connect different groups.
And yeah, so.
So that's kind of the modus operandi of category theory or the purpose. And I would say now in the answer to their question is category theory just a language that say no, I'd say it's very important how cool language it is but it includes lots of theorems.
Those theorems happen to be very abstract so they cover a lot of ground and so you know they're not extremely specific to a field.
If you want specific facts about one field like, like number theory.
You really have to be kind of a visionary like growth and deep to, to say, to use category theory there because what you do is you say, Here's what I really want to know in the most abstract terms.
Here's, here's, here's what I really want to get at. And I don't know yet how I'm going to do it and so you kind of construct a whole theory, a general theory around what you want.
Category theory is always really designed to be general.
So, finding the right abstractions is subtle philosophical work it's like what really makes the Arabic number system better than the Roman one.
What, what, you know how would you be convincing. So Fibonacci took these ideas from the, you know, Arab world and into the Western world.
How would you be convincing about that. It's tough but because for someone who hasn't learned yet but how do you really know when you found the right abstractions.
Well, for one thing like in, in programming you would say well, you know, it's, it's great when you don't have duplicate code you abstracted away what's duplicated among all these parts of my code, and you've, you've just said it once now.
Another thing that happens in category theory is that conceptual neighbors become apparent. As you abstract away the essentials of something, you start to see, oh, if I just moved to turn this knob a little bit, I get this whole other thing.
So for example, Laver said that, like, category, a certain notion of enrichment which we'll get to later very, very briefly defines metric spaces as a certain kind of category.
You can't get to that exactly but basically you find these conceptual neighbors you wouldn't have necessarily thought to see.
You handle the corner cases naturally not as one offs so like zero factorial being one, or the product, you know, five to the zero being one things like that aren't are handled within the general theory, as opposed to being corner cases you have to deal with later.
So what else makes the right abstractions well let's you compress big ideas into small spaces.
You get this kind of separation of concerns so with like plug and play in a computer you don't have to like the, the connector for the mouse doesn't have to know how the computer works and the computer doesn't really have to know what the mouse, how the mouse works.
They separate the concerns and they go through that interface. And I think category theory has a lot of that feeling that you can just kind of plug in what you, as long as you've, you've kind of said what it is you're looking at.
You don't have to, you can connect different disciplines without really knowing how either of them works internally very much.
And lastly just makes thinking easier, you've got the right abstractions, you can think bigger thoughts.
Or at least more generally feel like you're thinking more easily about the subject, because it's packaged correctly for you.
So, there's something from my point of view remarkable about category theory proofs.
I wasn't born a category theorist, I thought I liked logic, then I thought I liked algebraic geometry, and when I got to category theory something just changed and I just got happier.
But there's something remarkable about it, like in golf, if you've ever played golf or baseball or tennis, often a good hit comes with this kind of ping sound.
You hit the sweet spot you might say of the racket or the club or whatever, and you barely feel the ball when it hits, and yet the ball goes flying.
In contrast, there's maybe you could call it flubbing the ball, you get pain through the racket or the pain through the golf club, and you get no distance on the ball.
So the energy with the ping sound, the energy is delivered to the ball, and not somehow frittered away.
I don't know what that is, I haven't understood like what really makes that happen, that kind of trampoline effect, but the same thing feels like it happens in category theory.
So I've already mentioned David Jazz Meyers, and he, the way he said it was you set up the definition and the proof just floats.
So the theorems hypotheses kind of redistribute like fly through the air and land on the other side as the conclusions, and you kind of get this feeling that it's just right, and it's exhilarating.
Anyway, so the tutorial will be in two parts. The first part is more pure classical category theory, and the second part, which will be another video all together will focus on applications.
The first part, you know, it's hard.
I'm already talking fast and I'll continue to talk fast. And the thing is that category theory is almost as broad as math itself, even the pure part so, I mean, especially pure part so it's tough to encompass in a, even an hour and a half videos.
So, but I'll start with some familiar math like orders and less than or equal to. I'll introduce categories and give tons of examples, and then I'll conclude part one with a summary.
As you listen, you know, you don't have to get every concept you don't have to understand it. What I think is better is kind of to understand what's being focused on that the focus is about the connections between fields or what's really inherently
being looked at in a field and and how it allows you to generalize things.
So let's start with some familiar math. I first sets I just got to get this out of the way so that everyone here is set in the way I want them to.
There are theories of sets that that build up things like three as the set containing two and two as a second hitting one and stuff like that that's not of interest here.
Because in category theory we're not interested in how you built it exactly, or what's inside of it, or how it's implemented we're interested in what it does how it behaves with other sets, or with other orders or whatever.
So, a set for me in this talk is just a bag of dots a bag of dots you can tell the difference between. So 123 is a bag with dots 123 right this a bag full of stuff.
Like he said it has no dots there's the symbol that blackboard bold and for the natural numbers. It's a bag with lots of dots 0123 etc.
And the reals has even more dots. Now there's a comma two comma two that's a set that's almost a set except that the latter two twos are not distinguishable.
They're the same symbol so I would just toss it. Some people would say that that's the set a comma two and that's fine too.
And for later I'm going to need this notation, not really much but that bracket and is the sets one is the numbers one through N.
Okay, some other definitions will use given two sets and be their product is the set of all pairs a comma B, where a is in big a and B is in big B. So it's just a set of ordered pairs.
It's got the right number of elements, you know, a has three elements and B has seven and a times B will have 21. It's kind of a grid with a on one axis and beyond the other.
A relation between a and B is a subset of a times B. So in that big grid you just pick out certain ones out of those 21 you just pick out certain ones and say these are related in the ways I care about.
And that that subset is the relation. So for example if a is the natural numbers and B is the reals, then you could say I want all AB pairs, of course that's a gigantic grid, but I want this one is going to be kind of like a zigzag through it.
I want all those AB pairs where a is the floor of B, like a is this is the biggest integer less than B. So maybe a is three and B is 3.1, or a is three and B is three, or a is three and B is Pi.
In all those cases they would be in the relation but a but three comma nine would not be in the relation three comma 4.1 would not be relation relation.
So if a and B were the set of people, they're both the same set people, then you look at the grid of all people by people, right, you look at all the pairs of people in the world, and friend would be a relation, mother would be a relation.
So you take all the pairs a comma B where a is the mother of B. That's a relation. So anything if you call something a relation in real life, like on the, the, the, you look at all pairs of objects where a isn't an object that's on top of B.
That would be a relation. You have to take a snapshot of the world and then just list out all the AB pairs like that.
Okay, a binary relation on a is a relation between a and itself and a lot of the examples I've given so far were these binary relations on a set A.
Oh, right. And last thing is that a function from S to T, you might think of functions from calculus or algebra or something like sine, like the sine function or cosine or something like that but this is just a, they don't have to be continuous and there's no, you know, nothing
like that I'm just saying what is a function. It's just this kind of vertical line test from that point of view but another way of saying is it's a relation between S and T, where every element of s relates to exactly one T.
So friend is not like that. But floor is a function from the reels to the naturals.
Every real has exactly one floor, one natural that is its floor.
Okay, so a function is just a way of taking inputs from S and giving out outputs and T, but you can define them in terms of relations as relations where every S gets exactly one T.
So now we can define orders. Some people would call these pre orders related things you might call partial orders.
I'll say the relation in a minute relationship.
An order is a pair S less than where S is a set. So suppose you had a set and you had this binary relation on S, and it satisfied two laws and if it did you would call it an order.
The two laws are that for any S, we would say that S is less than or equal to itself.
That's a property. And secondly, if S one and S two and S three are elements of S, and S one is less than S two and S two is less than S three, then S one is less than S three.
And sometimes I accidentally say less than instead of less than or equal to just because it's so much shorter.
So soon we'll see that these two laws, the reflexive and the second one which is called transitive are prototypical laws for categories.
So I kind of generalize this, but yeah, right now I want to say if S one and S, if S one's less than S two and S two is less than S one, we'll call them isomorphic.
And a partial order if you've heard of those are is just an order where if two things are isomorphic then they're equal.
So you kind of, you don't let something be less than something else and it second one be less than the first.
So the usual ordering on the naturals says that three is less than four and four is less than 10 to the 73 and all that sort of stuff. But in golf you might use a different order you might say four is better than three for four is worse than three.
So the less than is like worse than, you know, in terms of money three is worse than four, but in terms of golf or is worse than three. So you can, you can take the opposite of a post that reverse the order.
In theory, we could still be looking at the naturals natural numbers, but we would say that a, you know, divides be we don't have to use that less than symbol we could use that bar symbol, and say a divides perfectly and to be.
So for example, three divides perfectly into nine but it does not divide perfectly into eight.
And that's still an order.
Eight doesn't divide perfectly into three and three doesn't divide perfectly into eight so it's not what's called a total order, given two elements they may not be comparable at all.
But, but if a is if a divides B and B divides C, then a does divide C, and that's what you need. And of course a divides itself.
In fact, just equals itself as an order.
You would like instead of the less than you just say, are they equal, if so I'll call it, you know, ordered. There's, there's just a big bag again, can't tell, but equals does satisfy.
You can't tell if one's less than, you know, you can tell if one's less than other but they never are, unless it's just itself. So it satisfies the two properties.
Is s less than equal, it is s equal itself, yes. If s one equals s two and s two equals s three does s one equal s three. Yes. Okay, so it's an order.
Next you might hope that people's preferences are ordered I don't know maybe there's like proofs that like they're really not ordered and stuff like that.
Of course, now this doesn't mean that for any A and B, either a is less than equal to be or B is less than equal to a.
In that case you would say they're comparable but some people might say, well, you know, these two things just aren't comparable. Right. And so it's when you can't put one above the other.
And another example is if you have a graph, you could use the symbol leads to, you know, this is the crooked arrow thing to say, you know, you could take all the vertices as s, as you're set, and you would say one vertex is less than
another if I can get from the first to the second via a path through the arrows.
That's an order, because you can get from a vertex to itself using like the trivial path, and then you can get from B to W and W to X you can get from B to X.
And so the notion of order gives you these rules for reasoning, the reflexive and the transitive, but you specify the content, whether you're in economics or you're in numbers and division or whatever.
So let's give a first look at universal properties the top and the bottom in the context of orders and we'll come back to universal properties, which I said growth and deep really emphasized, and which are, you know, very prevalent throughout category theory.
An element of a partial or of an order is called a top element, if everything else is less than if all s prime are less than it, including it itself.
And note that if s one and s two are both tops, then they must be isomorphic, because if s two is a top and s one must be less than it, and if s one is a top then s two must be less than it.
They're less than each other, less than equal to each other and yet and therefore that they're the same. So being a top characterizes you up to isomorphism, there aren't two different tops, all of them are isomorphic.
An element is a bottom if everything is bigger than it, if it's less than everything.
So for an example, not every post set, not every sorry order has a top.
The natural number is with less than doesn't have a top, there's no biggest natural, but it does have a bottom zero.
Now the natural numbers with dividing perfectly this bar thing does have both a bottom and a top. What are they.
So remember, you would say a is divides be if there is some number you can multiply by a to get B.
So is there some number.
That divides perfectly into everything if so it would be a bottom.
Is there a number that divides perfectly into everything. Well yeah one. So one is the bottom. And is there a number such that everything divides perfectly into it, like for every a, you can multiply that a by something to get this thing.
And you might think well that number would have to be huge to have to be infinity and natural numbers doesn't have an infinity. So how could this work.
What's kind of funny is that zero is the top because every number can be multiplied by something to get zero. So zero is the top and one is the bottom.
And it seems to reverse the order but it's not it's, it's, we get this, not because it feels right, but because it's the definition, it's the universal property.
And these universal properties will start to see name very important like zero and one you can at least say are important numbers in the naturals right and, and the universal properties always seem to this is just empirical always seem to identify important numbers or important elements of things.
In preferences I guess the bottom in economics the bottom would be, you know pure hell and the top would be heaven or something like that, just to give you a sense of what that might mean.
But here, okay, let's look continue to look at universal properties bottom and top or universal properties.
That's just the name of them, but there's also something called greatest lower bound and least upper bound.
If you have some subset s prime you've got this big order of all your preference, all your, you know, things you could have and the preferences or all the numbers in the world or whatever.
And I've got this subset of 69,000 numbers or something. Right. And so, what is the least upper bound of those 69,000 numbers.
It's this, it's an s such that s prime is less than it for every other for every number in that set in that 69,000 element set.
Not only is everything in that set less than s, but if something else had that property, if they were all less than T, then s is less than T so in other words, a universal thing says, I'm above all these guys.
And I'm the closest thing to them that's above them anyone else who's above them all goes through me.
So a least upper bound of two elements that set up 69,000 is called their joint and is denoted with that V symbol you do not have to remember that we're not really going to get use that V symbol.
But the point is, it would be something s one is less than the joint s two is less than the joint and anything else that s one and s two or less than the joint is also less than so that the joint intermediates or mediates between s one and s two and anything
above them both. It's the closest thing to them that's above them both.
So with the naturals and less than the joint symbol is max, the joint of s one and s two gives you the max.
Because the max of s one and s two is bigger than both, right bigger than or equal to both. And anything else that's bigger than or equal to both gives you, it would be bigger than their max to
the naturals and division, what would this join be so if I have two numbers, I don't know.
For and six, then the join would be something that both four and six divide into.
And that if anything else is, if they divide into anything else perfectly, then this join divides perfectly into that thing.
So, four and six both divide into 24.
Is it true that anything else they both divide into 24 divides into well no actually 12 is the answer it's the least common multiple so what we're seeing is that as I said universal properties often identify important concepts, that's why they're important that's
important in category three category three is looking for what unifies important concepts throughout math. So the join has gotten us now the max and the least common multiple.
And you might say, Well, I barely use either. I don't know, maybe you barely use either of us, but people do use both of those things right and and
So, so the point is like these interesting things from these subjects that you know if you want less than this you care about max you're going to care about it.
If you're interested in division you are going to care about these come multiple and and it's identified them as as this universal property.
And of course it works for more than two elements. And what's the least upper bound when s prime is empty. So let's just question that again.
So here we are s prime is empty. We're looking for an s where s prime is less than s for every s prime as well this is vacuous if s prime is empty. This is a vacuous condition there are no s primes.
So if t if s prime is less than T for all s prime less vacuous to so for any T s is less than T. So a least upper bound of an empty set is some s such that for any T s is less than T.
And so the least upper bound you get the bottom again, the least upper bound of an empty set is the bottom. So we've kind of redefined bottom in a more general way, as like the least upper bound of an empty set.
Now we understand the least upper bound of any set.
And those are universal to universal property. And of course we can reverse everything. So in category theory you can always reverse everything just turns out that every order has an opposite order we talked about that and see that every category has an opposite category.
The greatest lower bound of some subset s prime is an s that's less than everybody in s prime. And if T was also less than everybody in s prime. Well s primes closer to them.
Sorry, the s is closer to them T is T is less than s. If somebody else is also going to be less than everybody, then T will be less than that then it'll be less than s.
So this is the closest thing to them. That's also less than them all.
So greater lower bound of two things is called their meat and denote this way, and it would give you things like men, and greatest common divisor.
Another joining thing is the union of two sets. Another meat thing is an intersection of two sets. So if you look at a Venn diagram and you're interested in unions or intersections, those are pretty important things to look at and they are universal properties.
Okay, in category theory, we're always trying to focus on the relationships between things. And so if we've defined partial orders or orders, sorry, then we, we might say,
we're going to look at the maps between them that preserve the structure. And what is the structure we want to preserve. Well an order has two parts a set and a relation.
And so a map of orders, a morphism or map between the orders should preserve or deal with both of those parts.
And so a map of orders from s with some sort of less than symbol, maybe I'll call it less than equal to sub s, and to T with some less notion of less than there called less than or equal to sub T.
What is a map of orders, it's a function from s to T, every element of s gets an element of T.
If you have any two elements in s, if s one, if s would say that s one is less than s two, then T should say that f of s one is less than f of s two.
That's the definition of a map of orders. Now you could say, I have a different notion of structure preserving map in mind. And if you do, maybe you, that's fine, category theory, let's you define what you want, as long as you're consistent with yourself and it gives kind of rules for
what we think consistency really means. And I'll get to those later but let's give some examples. The inclusion of the naturals into the reals like the fact that three is 3.00000, right, so that like every natural is a real is actually a function that takes every natural and gives you a real to
the addition of all those naturals reals where there were kind of the same three, three.
Now that gives you a map of orders from the naturals with less than to the reals with less than because, you know, five is less than 20 in the naturals it's also less than, you know, 5.000 is less than 20.000.
Right, so that's a map of orders it satisfies these two properties. If it's less than in naturals it's less than in the reals.
It's a real function from x takes x and returns one over x gives a map of orders from the positive reals to the positive reals but switching the direction.
So, if one third, sorry, three is less than four, then one third is greater than one fourth. And so that that reversal, which everyone kind of learns in middle school or whatever elementary school ever we learn that thing that not that the, that the reciprocal
is encoded in the fact that we have a structure preserving map between these orders and we get to see, like, in symbols the thing that we all had intuitively in our minds, when we were thinking about reciprocal switching order.
So, now does the naturals give a map of orders from naturals with division divide perfectly into naturals with less than I'm asking if I take.
When I say does the identity of a map of orders I'm saying, well I'm not going to change my number I'm not going to do reciprocal or anything else like that.
I'm going to take my number and leave it intact keep three send three to three, but I want to know if if three divide six is three also less than six seems to work three is less than nine and four is less than four and stuff like four is less than eight so it's all seems to be working but if you remember the answer is no because
everything divides into zero perfectly and zero is not the biggest number.
But the answer is no, but the identity does give a map of orders, when you restrict to the positive natural numbers.
Once you eliminate zero you do get a map of orders. And so we, we get to see that we get to see that how that works.
Okay, and now some maps of orders preserve the top and element bottom elements and some don't so if you're saying if you're if you're not only interested in your order because of the orderness but you're also interested in reasoning using meats and joins and tops and bottoms, which comes up a lot.
Then you want to know whether your map of order preserves that sort of reasoning.
So, for example, the naturals with less than does does not preserve the bottom.
Well yeah because the naturals has a bottom, and the reels don't have a bottom. So, the natural zero is sent to zero, but zero wasn't a bottom here but not a bottom here.
So, if you look at these two has a top, there's no biggest natural or biggest real so I guess you could say that all the tops are preserved, but, you know, it's pretty vacuous.
Does it preserve meats and joins. Let's ask so if I took two naturals, and I took their joint that's the max.
I took the naturals and B and I took their max and I saw it as a real is that the same thing as turning the two reels into turning the two naturals into reels, and then taking their max.
So, I turned three and five into 3.000 and 5.000 and I take their max is that the same thing as the max of three and five point.
Yes, so this this inclusion does preserve meats and it does preserve joins it preserves men's and Max's does the floor map preserve joins. So if I take two reels.
The floor of a real means the biggest natural lesson. So like I take all my positive reels, and I take their floor.
So take two positive reels, take each of their floors, and then take the men.
So three, you take pie and 5.9, and we floor them both and we get three and five, and now I'm in and I get three.
Is that the same thing as men in first, and then taking the floor. Well the men of pie.
So five point nine is is pie. And if I floor it I get three and that's the same. So yes, the floor map does preserve joins.
So why would you ask this sort of question. Well, they tell us the sort of reasoning that only needs to be done once, like, you can take the max in the reels, map it over to the naturals by floor, or you can take the floor of the things and
take the men over the naturals that join or the meat in the naturals, and you get the same answer so you only need to do it once it kind of keeps your grammar and your reasoning intact as you map.
Okay, so that that's all I want to say about orders, and hopefully it's familiar to some extent. Let me try another familiar hopefully familiar part of math, namely matrices.
The matrix is a pretty different part of math right. It's a rectangular array of numbers here's a two by three matrix to two rows and three columns.
And then there are these rules that you learn like, what are these crazy rules so you can add together any two m by a matrices.
As so as long as the dimensions m and n agree, you could add this to another one, you can add it to another two by three matrix.
You can add it and less the dimensions agree. But strangely, when you multiply, you have to do a different thing, you have to check that the middle guys agree so you can multiply m by N and N by P, and you'll get an M by P.
That's, that's just the rule of how this, you know, how matrices work.
And then there's like these facts here. So one is that addition is associative addition is commutative and additional is unital. What does that mean.
Associative means that if you use parentheses you can only add two at a time so far we've only told ourselves how to add two at a time.
But in some sense you can add any number of matrices because you just add the first two and then take the answer and add the last one.
And if somebody else did it differently they took the first one they added to the sum of the last two.
And they get the same answer.
So it's no real problem there.
It tells us that the reasoning is not dependent on exactly the order of things. And here's another sense in which it's not dependent on the order of things, not only the order of how you which things you add first but the order of addition.
Plus, doesn't matter, you know, M plus N is M plus M. And unital means that there's this thing called zero, a zero matrix of all zeros, where M plus zero is M.
And multiplication is also associated in unital. M times N times P.
I don't care if you calculated this one first and multiply by P, or calculate this one first and multiply it on the left by M, you have the same answer.
And it's unital meaning there's some kind of matrix called the identity matrix, where the identity times M is M, and M times the other identity on the other side is also M.
So another thing we could say is that multiplication distributes over addition and all that means is that M times N plus P is M times N plus M times P.
So there's a bunch of facts and rules for what you're allowed to do and what kind of answers you get.
So what are all these, you know, where are these all coming from and what what's going on here.
So, in fact, you can repeat that whole story and again category theory like what's the theme here, what am I asking you to pay attention to is how we're going to repeat that story.
When for any rig replacing a number what's a rig. It means that the numbers in that matrix, the entries in that matrix don't have to be real numbers.
It can be anything and satisfy all the properties I told you, as long as that something that s like the real numbers is itself associative commutative and unital, it has like operation called plus and zero that's that works like I said the matrices themselves did.
You know, you replace the reels by something with the plus operation and a zero and a times operation and a one that's associative and where the multiplication distributes over addition, then that whole story I told you would work.
You might be saying, what, what are you talking about exactly. What's an example.
Well, I could let my entries of the of the matrix just be integers they don't have to be real numbers they could just be integers, and all the actions we talked about go through.
You can add them you can still multiply them and you're going to get re integers again, or you could say no I just want natural numbers, and I never enforced that there was a minus, I mean, like I could have, and then you would have gotten it less general but more specific so general
is good if you want to do a lot at once specific is good if you want to do extra, you know, powerful things within a single theory. And so this ability to move between general and specific is very useful.
We could have done everything we said if we just use natural numbers.
But much more strangely, we could have used real numbers or natural numbers, but where we used, we play every time you see a plus you use max, and every time you use the times you use plus.
So, you can multiply two matrices, but as you kind of multiply them.
Instead of adding up a bunch of products, where you can multiply two entries and add add add add add.
A lot of entries that are each computed by maxing. And that that again works. And so the whole notion of matrix multiplication and addition and all the properties work, even if we use max instead of plus and plus instead of times.
And that that's using, you know, think of tropical geometry and things like that that that is useful.
In some sense, replacing plus by max and times by plus is kind of like taking logs everywhere, but I'm going to leave that aside, you know, you can kind of think about that for yourself.
So the matrix story is really flexible. And yet the rules seem so particular what why what are all these rules, you know, you can do so much with this and so important math but where are all these things coming from.
And so I don't know, I'll call it the bias inversion I was once, I once asked a colleague a question and john bias was there and he's, he laughed and he said, When most mathematicians here a new theorem, they say, What's an example of this.
But when a category theorist here is a new theorem they say, What's this an example of this kind of strange inversion.
I think it's an important aspect of the category theory aesthetic. Some mathematicians it's very common to like cleverness to show their intellectual power by pulling off what seems like an amazing magic trick.
And that's, that's really cool. But category theorists don't don't so much love the magic trick as what made the trick work.
We pull off the particulars, and we ask, What made this possible. You know what makes matrices work the way they do.
What are both matrices and orders examples of, like we've seen lots of examples of matrices or of orders. But, you know, here we're saying that they're both examples of something that's interesting.
What, what kind of thing is general enough to include both of these things.
And then the answer is categories.
So a category others to find it and then give a ton of examples.
An onslaught of examples I hope it's not too many. A category consists of four interlocking parts. First, there's a collection called ob C, and they're going to call its elements objects, but it's just a bunch of things.
It's another, you know, set or bag of dots, right, ob C.
For every C one and C two for every two objects. Again, an object doesn't mean anything. It just means we've identified some things that we're going to call objects, some nameable things we're going to call objects.
So for every two things, every two objects, there's a set denoted, sometimes denoted harm C one C two you call them harm sets from the word comes from homomorphisms.
It seems to be useful notation to instead just write C of C one C two. What is C of C one C two. It's the set of what are going to be called arrows from C one to C two.
So if I have an element of this set here, it means I've got this category, I found two objects in it, and F is an arrow from C one to C two.
And so an arrow is denoted luckily with this arrow symbol. So we've seen it before.
C one arrow C two with F colon like F is a type arrow, or F annotates an arrow. Anyway, these are just different ways of denoting it but the point is so far that a category consists of four things of which we've named two.
So objects, some collection of things that we'll call objects and very to a notion of arrow between them.
Okay, what are the last two things for every object, there is a choice of element, there's a choice of a certain element in here, a certain arrow from C to C, called the identity on C.
We'll see here. So, every object has an identity arrow to itself.
And finally, if you have three things, three objects, then you can compose arrows so if I have an arrow from C one to C two, and an arrow from C two to C three, then I get an arrow from C one to C three.
So these two sets here set of arrows from C one to C two and C two to C three. If I take anything in that grid, like an arrow from C one to C two and one from C two to C three, I get one from C one to C three.
And so let's look at it this way. If I had an arrow from C one to C two and from C two to C three, then there's something called F semi colon G fat semi, it's like a kind of like a circle but annotated with this other thing.
Okay, so I get away to go from C one to C three.
And these are required to satisfy some rules so for any, you know, for any stuff here are the rules. It says that if I have F, and I use the identity and compose I get back F.
I take F and I compose with the identity on its. So F goes from C one to C two. If I compose with the identity on C one, I get back F, and if I compose with the identity on C two, I get back F.
And also, if I have F G and H, then if I compose F G, and then I compose that so if I compose these two and I get a map from C one to C three, or an arrow from C one to three, and then I compose that with H.
That would be the same as if I first figured out this composition C two to C four, and then I pre compose that with F.
I'll still get a map from C one to C four, and it will be the same as the other one.
Okay, and that's the category.
You know why that's so dang important, at least you know to me here, or in this tutorial, kind of difficult to say, I'm just going to try to explain using examples.
So here's an intuition categories give you graphs what's a graph, a graph is a bunch of vertices and arrows, just some, you know, arbitrary or very specific collection of vertices the dots and arrows.
So every category includes a graph.
Namely, you could draw all of its objects as dots vertices, and you could draw all of its arrows as arrows.
But every, if you had a category then every path would be assigned to composite arrow.
So if you go like this and then up and then down and then up and then over, that would be given a composite arrow that would you be able to go from here to here and you'd be able to compose and compose and compose and get an arrow from here to here.
And we already had an arrow from here to here.
In fact, of course, this would get very cluttered if you drew them all because you'd have to have the path. Like, first of all, every one of these guys would need a length zero path, like zero paths to the count.
And so I'd have like, you know, length zero path.
And then I'd have the composite of that with itself, lots of times like there.
Let's see, there'd be infinitely many paths from this dot to itself because we could go back and forth and back and forth and from this dot to itself.
So every path would need another arrow in here and so it start to get very cluttered.
And as we said, we might have this path from this arrow, you know, up down up down over that might or might not be the same as this arrow.
In a graph, you can't tell what it would mean for two paths to be the same. That's not really a thing.
But in the category paths become arrows, you can compose paths, compose any sequence of arrows into an arrow.
And so you have to ask in a category, when are two of those composites the same.
So graphs are good intuition except that a lot happens in the composites.
But this gives us our first example so if you have any graph. So I said every category gives you a graph and that it's actually more complicated than that, because of all the composites.
But here we're saying, every graph gives you a category. So if you have a graph, you can form a category of the vertices and paths.
And that way does work very well, you're not missing anything. So start with a graph.
You get a category for graph the free category on G is called in his objects are just the vertices of G that's easy, and an arrow in free category on G is just a head detail sequence of arrows path in G just arrow arrow arrow arrow through G.
For any natural number, many arrows, so you have zero arrows, you start with a dot and you go zero many arrows and you get the dot and that counts as a path.
But you could also go any sequence and get a path and those are the, those are the arrows of free G, so free G has a lot more arrows and G did.
Right and the identity on a vertex is that trivial path, and the composite of paths is just a path concatenation, you have a path from B to W through some arrows, and you have a path from W to X through some arrows.
Then you get a path from the all the way through X, all the way to X as composite.
Right so this is called the free category on G.
So every graph gives you a category.
Each set is a category. If you have a set, we talked about sets the very first slide basically one of the very first slides and every set is a category. It's called the discrete category on s it's got no, its objects are just s.
So if you have a set, you get a category with that set as its objects, but for any two objects as an S prime for any two elements of the set.
Then what arrows should we put. Well we're going to put basically no arrows, if s is not the same as s prime, we do not put an arrow, we're forced to put an arrow when s equals s prime because we need identities.
Again, we do the least we can, and we get a category. It's just a bunch of dots with no arrows except of course identity arrows.
And to specify we need to say what composites are but we only need to compose when, when you have an arrow that lands where another one starts, and all of our arrows start and end at the same spot.
So we just need to know what identity composed identity is, but identity composed anything is that thing so identity composed identity is itself identity.
You could say this is just the free category on the graph that has s vertices and no arrows. So in some sense we already had this example in the previous slide.
But there's also a category called set of all the sets, maybe in some set theoretic universe you like you, if you're concerned about this sort of thing and size issues and you know Russell's paradox and that sort of thing and you need to deal with this stuff, but we're not going to talk much about that.
So the objects in set are all the sets in this in the universe.
So the objects of set, what is an object in this category, it's a set a whole a set is one object. The previous thing the elements of the set were the objects, and we were thinking of s as a category.
Now s is just one object.
The morphisms or the arrows between sets, you know, they're not so simple as just, you know, only identities now there's tons of them between s and s prime, the arrows between them are all the functions.
This is kind of the prototypical category, the identity on s is the identity function, it's prototypical in the sense that like all of our terms like identity and composite come from, you know, come from functions.
It's a very familiar category, some sense, even if you've never heard the term category before.
Hopefully, you can imagine there's that all the sets in the world, and the functions between them can be grouped together and packaged into one thing, and that's the category of sets.
So in this category the objects are an object is a set, an arrow is a function, the identity arrow is the identity function.
The composite is the composite, you just compose functions.
Now as a graph this would be huge so even if you just had four sets, 0112 and 123, you'd have four objects or four bullets for vertices in your graph, but you'd have 60 arrows.
And if you added a fourth, you know, 1234 you'd have like, I don't know 100 400 more something like that. So, it's a huge few hundred more.
Like the graph would be so cluttered with four dots and 60 arrows.
So you don't really want to think of it as a graph and moreover, you're really concerned with how those arrows are composing and would you want to look at that graph and like, look at all the pairs of arrows and try to figure out which other arrow.
The composite of them was, it's just too cluttered.
But as a category when you relax and don't think about the graph, what you, the objects are very familiar they're just bags and dots that's just one object, packaged up, and the arrows are the functions, you know how they compose.
And so set is just the package of all that relatively simple information set is the category of all sets.
Okay, let's do it again so every order from before every order is a category. So if I have an order s less than we can think of it as a category s.
Its objects are the elements of s so we take this order.
And we just take out its objects the elements of the of s. There's one arrow from s to s prime whenever s is less than s prime.
And we draw an arrow whenever there's a less than relationship.
So I guess if you wanted you could say, what is a, what is the set of arrows rest as prime. It's the set consisting of yes, when s is less than s prime, and it's the empty set otherwise, but only shouts out yes when s is less than s prime.
Otherwise it doesn't, there's just no arrows.
Why did I say yes I could have said star I could have put a less than or equal to symbol in here. It's just any one element set when s is less than s prime I'm going to make my, my home set my set of arrows be a one element set.
And whenever s is not less than s prime, I'll make it empty.
If this works we were going to get a category we need identities, and we have one because less than or equal to is reflexive. We get our yes in the s s thing, we get an arrow from s to itself, because s is less than or equal to itself.
And when we want to compose two arrows as to s prime and s prime s prime s double prime, we need this arrow from s to s double prime, but arrows are just these, you know an arrow is just the yes of it being less than.
So if we knew that s was less than s prime, and s prime was less than s double prime, then by transitivity we would know that s was less than s double prime. So we'd have this arrow.
And so every order is a category.
And the axioms of orders are exactly what you need to do this construction.
The categories you get this way are called thin their categories alright, but there's at most one arrow between any two objects, there's never two parallel arrows between the same two objects.
There's just at most one so these are really simple categories. They're not simple like sets in the sense that they're very familiar. They're simple in the sense that they just don't have that much going on.
The category of sets has tons going on, but it's familiar. This one just doesn't have that much going on.
But then there's also the category of orders. This one's more like the category of sets. There's an order category or to all orders and order preserving maps, whose objects, an object in this thing is an order.
It's a pair s less than where s where less than is reflexive and transit. It's an order.
And this category is a map from s to from s less than to T less than what is a map it's a order preserving map it's a function from s to T, where one thing is less than other and according to s, and f of it is less than f of the other, according to T.
We call it an order preserving map before, and since the identity function is order preserving, and since the composite of two order preserving maps is order preserving.
Then we know how what identity and composition mean, and it is unital like we compose with the identity we get back our compose something with the identity, we get back our thing, and it's associative.
And so we get, we get what we wanted we get a category of all orders.
And this as a graph, it's huge. Right, it's got in really many things in it, and tons of maps and arrows between them is just a total mess as a graph.
But as a category, it just packages up the notion of order. It keeps track of the sort of relationship. So what are the arrows here, they're the sorts of relationships between orders that preserve the reasoning.
All right, more examples. So dimensions and matrices. There's a category whose objects are dimensions. What do I mean, I just mean numbers like five.
So the objects in this category are like five 12 and three and two and stuff.
Those are the objects, but the arrows from five from say two to three are the matrices.
Okay, so what's going on. Let's say you have a rig, which means you have a, you know, a set, something you're going to call zero something you're going to call plus something you're going to call one and times, and
they are distributive and associative and they have all the properties of the plus and times should now that's much more general than just plus and times in the real numbers.
You can think that way for this entire thing just read this are as reels throughout if you want.
But again, it could be with max and plus instead of plus and times. It's a general thing, but the point is Matt are is a category every time no matter what are is what category is it.
The objects are thus just numbers, which I'm calling dimensions. So two and three.
And the set of arrows from two to three is a set of M by N matrices just the set of functions that take any element of, you know, from one through M, a number from one through N, and give you an element of our.
So it's just M by N matrices, whose elements are in our just like we saw before. I'm not trying to say anything special here.
But what's going on. How is this a category, the objects are natural numbers like to the identity on a number two is the identity matrix, the thing we called I sub two.
The composite of two matrices is defined as their product. So we're kind of taking identity and product that we had before from matrices, and we're taking them to be the identity and the composite.
Now, so if I have a and an arrow from M to N and arrow from N to P, I need an arrow from M to P.
And that that is the composite. Okay, so this is just packaging the rules of matrix multiplication. It says that dimensions like the rules for when we're allowed to multiply matrices and stuff are all about dimensions.
It says, Oh, those are just the objects, and the matrices themselves are the arrows from like two to three. Now we're still missing matrix addition, but we'll get to that later.
The point is that matrices and all the rules about at least the rules about associativity of multiplication and unit totality of multiplication and their dimensions and stuff like that are being packaged in this way.
Okay, let's do another example. So a monoid.
If you've heard of groups, group monos are just a little bit more general. It's like a set of things, an element of that set called the identity, and a way to take two of them, and get back one so way to multiply two elements of that.
So you have a set, you have an element of that set, and you have a way to take two elements of the set and produce one multiplication, where he is a unit for top times and where times associated.
Okay, so that's what you get in a group in a group is a little more than a monoid it says it's a monoid but every, every element of the monoid is invertible for every element there's another element that you multiply together to get.
We don't need that here. We're not studying groups.
But of course, we could.
We're just studying monoids here. But what we're seeing is that these axioms they look so familiar you have this identity element that's unital.
And you have an associated multiplication. I mean, what is going on. Well the answer is that this is the axioms for category with one object.
So if you had a category was just called M, but it only had one dot in it.
All the information would be in the arrows of the category. And so we would take the arrows of our category with just one object to be the elements this set.
And so we're just seeing a dot with lots of arrows on it, you know, and they all go from the dot to itself.
And now we read off what the identity is the identity on that one object would be this E thing.
They satisfy the same properties right E when you multiply by anything gives you that thing and identity when you compose with anything gives you that thing. And so,
the composition, all morphisms are composable all arrows are composable, like in an arbitrary graph or category, you can't just take one arrow and another and compose them.
You can only do so if they land if the first one lands where the second one starts.
But when you only have one object, then they all start and end at the same point. So all the arrows are composable and the composition is the star operation.
And composition is associative and unital because star was.
So, in fact, a category with one object is a monoid they are the same data, they have the same data they have the same axioms they're the same notion.
And I was saying as a graph it would look like one vertex and lots of loops, but as usual, what we're interested in is this composition, like, we're not interested just in m we're interested in how star and you were in group theory or monoid theory.
For later I'm going to use the fact I'm going to use this definition that a commutative monoid is one where m times m prime is m prime times m.
The main point of the slide is that monoids are useful throughout math, and they're exactly categories with one object.
But there's also a category of monoids, where you could talk about monoid homomorphisms, these are functions from one monoid to another that preserve all the structure that send the identity to the identity.
And then you have to send that if you multiply two elements and then effort. It's the same thing as f and each of them and multiplying.
And these form a category called mon.
The objects of mon are all the monoids in the universe.
The arrows are all the monoid homomorphisms all the structure preserving or reasoning preserving maps.
The identity function preserves E and star so it is a home, it is a map.
It is an arrow, and the composite of two functions that preserve E and star also preserves them. So we know how to compose. We know what the identity is. And so we get a category.
I mean, we have to check that the identity and the composition are unital and associative, but you can that just comes from the fact that maps between sets are.
Okay, so what have we done so far we've said that sets are a category.
Each set is a category but so there's also a category of all sets, and every order is a category but there's also a category of all orders.
And every monoid is a category but there's also a category of all monoids. Now that that pattern is not, you know, so important.
I'm just saying, we're seeing the category notion come up, you know, at all levels led to at many different levels and in many different cases.
Matrices also form a category said, here's another one types and programs. If you have a programming language, you could take all the types, like the integers the strings the Booleans, and make those the objects of the category P.
And all the programs that input and a and output be as the arrows from a to B.
And I'm going to consider two programs the same if their outputs agree on any input.
Now, like I don't care about how you implement it I care about what it does. So, now, there's an identity program on a that just outputs whatever inputs.
You can compose a program that takes an A and outputs a B with a program that takes a B and outputs a C to get a program that outputs and that takes an A and outputs a C.
And so we get a category, and it's almost the category of sets, except not all sets are types, and not all functions are computable so P is really specific to that programming language it might not even be touring complete you can take any programming
as long as it has some notion of types and some number of notion of programs between those types, you would get a category.
You can also take the category of sets, but with relations between them. So the category rail has the same objects as set does the objects of rail are the objects of set.
So my arrow from a to B is a relation between a and B, not a function a relation. That's a perfectly good category, as long as we can write out an identity relation in the composite of two relations.
So the identity relation on a is the relation that has like only the diagonal out of the grid of all a by a, you would only see.
You only select things in the diagonal the a comma a's to be in the identity relation and composition, which if you know much about matrices that would be that looks a lot like the identity matrix.
And composition is like matrix multiplication. So if you have a relation between a and B, like friendship, and then you have motherhood.
So, like, so a is a friend of B B is the mother of C. What is the composite it's all AC, where there's some be that a is a friend of B and B is the mother of C. It's all the, you know, pairs of like, I don't know your friend your friends kid or something
So it's all AC pairs where there exists a be such that R of a B and S of BC, you can compare this to matrix multiplication, where our times s is the sum, so this backwards e it's a weird e.
Overall B and B of the entry in a B and the entry in BC. And in fact, relations and relation multiplication is a special case of of matrices, it would be using what's called the Boolean rig, where every entry every entry in the matrix is either a zero or one.
And one plus one is one. That is a perfectly good rig. And if you use that rig, rig means ring without negatives I probably should have said that. So if you use that rig, or semi ring, then this formula becomes this formula.
And because matrix multiplication is unital and associative so is a relation composition.
So take your own category in many ways if you have any two categories you could take their union of all the objects and all the morphisms.
You could take the product of, you know, all the pairs of objects.
One and C one and D.
A morphism between pairs is just a pair of morphisms.
Another thing you could do is you could choose an arbitrary subset of the objects and see, and use all of C's arrows between them, and you would know how to compose you would know what identity was.
So you could just take a, it's called a full subcategory of C, just a bunch of objects and see any collection you want, but take all the arrows between them.
You can start with 23 objects and 691 arrows, and require that some arrows were the composites of others, and then freely add the rest of the composites like you did for graphs, like you say, Oh, I have these 691 arrows.
And some of them, like when I compose arrow 23 with arrow 65, it's arrow 99.
And I know some facts like that and then the rest of them I just freely add composites.
There's categories in pure math, there's the category of groups category of rings the category of fields, metric spaces topological spaces measurable spaces manifolds overfolds quabortisms C star algebra Hilbert spaces bonnock spaces there's tons and tons of categories throughout throughout math.
You can make a very specific one like the category of three dimensional oriented Riemannian manifolds, like you can really narrow down to what you want to be looking at so mathematicians create new categories.
Every day, they're not like some fixed number of categories there's lots of them. And why would you do it because you want to first of all name what you're looking at, so that you're like, you can concentrate on it when you look away, you look back you remember what you're
doing, you can compare it with other objects of study.
And you can tap into the vocabulary and reasoning the category theory provides. So this kind of ends the bunch of examples about categories.
The point is that they're very, very general things.
And yet they're strangely you know there's some rules to them there's identities and composites and stuff like that.
Very, you know, for your enrichment but also for that there is something called an enriched category. So here's just an example of something you might do.
So we said that a category has a set of arrows between every two objects. If you have two objects you have a set of arrows between them, but we also said that set was itself a category.
So with this kind of bias inversion, if you want, we instead of, you know, we have this example here what is an example of why are the arrows from C to C prime a set when set is itself a category could we do that again.
And so the notion of enrichment says we could use something other than set there and I'm not going to explain it in any detail whatsoever.
But for example, we could have used commutative monoids instead of sets. And what that would mean is that between every two objects in our category.
We don't want just a set of arrows between them. We want to be able to add any two arrows.
Right, if I add two arrows from C to C prime I could add them and get a third arrow from C to C prime.
I would have a zero arrow from C to C prime. When I add it with any other arrow from C to C prime I get that arrow.
For example, matrices with R was enriched in commutative monoids and that's what the fact that we had to keep the dimensions the same, but once we keep the dimensions the same, we can add any two arrows.
And so that's what was going on here it explains like the definition of enrichment which I've not given you explains.
The definition okay that part I said above but it also explains the distributivity.
So like we're getting out now with the notion of enrichment.
We get all of the facts we had about matrices all kind of packages saying, Oh, matrices are just our, you know, it forms a category enriched in commutative monoids.
But any rig, what you could define a rig to be a monoid enriched in commutative monoids.
That's funny like all the rules of plus and times and zero and one are packaged in something called a monoid, enriched in commutative monoids.
But since now, like, how would you vary, you take the definition of rig and you're saying, Oh, I want another one like I want something else kind of like this.
What would you do. Well here, we know what we would do we could change the enriching thing. We could change this thing. Right and so it tells us kind of what knobs we're allowed to turn to take a rich.
To take a rig and take find something different. And this comes up in math, a lot, especially in applications where you have something that looks familiar, but it doesn't quite fit.
And you want to turn some knobs and get a natural definition that's going to take care of all the corner cases for you.
How are you going to do that well if you abstract this thing and see what it's an example of, then you can find other things that are examples of that.
If we were to take a different, we could instead of taking commutative monoids we could take categories enriched in probability distributions, and that way, every arrow between C and C prime would have an associated probability.
What's the probability of this function what's the probability that function from CDC prime, and the home set there would be a distribution that you know of numbers adding to one of all the functions from CDC prime.
I'm going to discuss enrichment again, but it's an example of what I might have said in the beginning of layering abstractions.
It's an example of something that comes up in category theory a lot where you want to abstract out and layer these things so you have a category, but the maps themselves, like the home sets are actually objects of something else besides set.
Okay, so we talked about universal properties and we're back to that now back in categories. So many important constructions in math and math are characterized by maps in in an order.
You know these very important things like top and bottom and meet and join which people really care about when they think about orders were characterized by universal property by the maps by the by the less thans.
So men and max and GC the greatest common divisor and least common multiple intersection union, all those things come from universal properties.
So if you remember the greatest lower bound of a subset was something that's bigger than everything in that. Sorry, lower bound. There's less than everything in that subset.
But if T was also less than everything in that subset, then T this is supposed to say s prime. If T was also less than s prime.
For every s prime in that subset, then T is less than s. So s is the greatest of all lower bounds. It's the closest to s prime of all the things that are less than everything that's right.
Now we know that less than or equal to is just a name for an arrow.
So we can ask, what is greatest lower bound an example of.
And the answer is it's a universal property, which we'll define and give some examples of.
So we kind of go back and forth what's an example of and what's examples of that. Okay, so initial and terminal objects.
The notion of top and bottom elements has analogues and categories, an object and see is initial. If there's exactly one map from C to C prime for every other C prime, and for itself, just a unique map from C to itself also.
We can call it terminal, if there's unique map to it from every other element.
So we already said if C is an order than initial, then, you know, well, universal properties, we saw bottom and top initial would be bottom and terminal would be top.
In the category of sets, what I'm going to tell you is that, you know, these universal properties always find the important, very important elements of your category or objects category.
In the set, we find that empty set is initial, there's a unique function out of it into any other set, and one or one element set is terminal.
Now when does a discrete so right.
These are the important elements and these are pretty important objects, pretty important sets the empty set in the one element set.
What if I had just some set s with 58 elements.
Does it have an initial and terminal object? Well, what I said was that we tend to empirically find that the initial and terminal objects would be important objects.
But if I just had a set with 59 elements, are anything in there like the important one.
Well, if s has two different elements, then there's no map between them there's no arrow between them and discuss.
There's only an arrow from s to s prime, if they're equal in this funny category here, this discrete thing.
So, there couldn't be a terminal object because we need exactly one map for if sorry if s is supposed to be initial, we would need exactly one map from s to s prime, but that just doesn't happen when there's two different elements.
And if s is empty there's no initial object because there's no object at all. So we find that s has an initial object if and only if it only has one object.
So a 59 element set doesn't have anything special in it. And so it doesn't have an initial and terminal object. If s has one object well that objects pretty special.
Okay.
The category of matrices has an initial and internal object. In fact, they're the same zero.
Because they're exactly one zero by n matrix and exactly one n by zero matrix for any n, the matrix.
Some people might say that's not a matrix there's nothing in it but we deal with that corner case like when you saw how I defined matrix way back when it did deal with the set with like the empty set of with zero as a dimension.
In rel, where you get something similar is the empty set is both initial and terminal. There's exactly one relation between the empty set and any other set.
And in the category of monoids is similar.
This the monoid with just one element is both initial and terminal. That's not always true. I mean it's set we have different, you know, initial and terminal.
And in this category we never have them at all, except when we have only one.
Okay, so
what we do is that the empty set is important with when it comes to relations and the zero thing is important in some sense, maybe you would say it's not important it's just so trivial, but
often those trivial things the corner cases are important to look at.
And again here and set there, they're kind of trivial, but empty set and one element set like why are those, you know, all that great. Well, you know that they are like of interest if you're going to find counter examples you're probably going to find to some theorem you're
probably going to find it there.
They're just interesting things to look at first.
They're your, they're, they're the first examples in some sense.
The notion of join and meet also have analogs and categories.
If C one and C two are objects, their co product is another object let's call it see where there's a map from C one to see, see, and I'm not from C two to see.
So it's got this, you know, every like C one maps into the co product C two maps into the co product. And if C one maps into any other X, and C two also maps into that other X, then there's a unique map from C to X with with these properties.
So it's like kind of a lot of stuff and I'm not going to really go through it. I'm just taking the idea from orders and making it more general. It's just saying like the co product of C one and C two has an arrow from both of them.
And anything else that has an arrow from both of them goes through this co product. See is the closest thing to see one and C two that has an arrow from both of them.
So in order, this was the joint, it was a max least common multiple or the union for sets, it's the disjoint union, we just throw all the elements of s one s two together and that's the co product.
So that max LCM union disjoint union for matrices is the sum of the mentioned so the union, that's the co product of five and five is 10, five and eight is 13.
That's the co product. And if you turn the arrows around and every arrow here you make go backwards, just make every single arrow go backwards here, and make these go backwards to
get what's called products. So in order, you get the meat operation for products. So we're generalizing meat, we're generalizing men and GCD and intersection, right in event diagram you take the intersection, that's a meat, and that's actually a product.
And for two sets, we get the product of those two sets from way back when. So we didn't like way back when we defined this as all the ordered pairs.
And now we're not defining it to be some implementation, we're defining it to have a property that the product that grid of, you know, five by nine grid or whatever.
Every element of it can be projected down onto the five and projected onto the nine you can project on the two axes.
And if anything else, give you a map to the two axes, it would give you a map to this thing to if you point at an element of the five, if you point at one of the five, and one of the nine, I could point instead just to the element of the 45 to the grid,
to be able to recover, which to you point to that. Anyway, so that might be way to abstract and kind of visualizing something that you might not be.
But my point is that we're recovering all these important ideas like product and men and GCD and intersection with a single definition that works across every category.
So instead of looking for the category of groups, instead of implementing what the product of groups is, or the code like the co product or free product of groups is, or rings or topological spaces or metric spaces or categories, instead of saying what you mean by
product of two metric spaces, what do you mean by product of two rings. I always mean this, and it recovers the notion of product you find in all of these different fields.
In the hand, you just use the universal property, and that means any theorem or we can prove about products, we would be proving simultaneously for the way products behave in all of these categories.
I guess I haven't told you that categories form a category but that's true. So, these aren't isolated properties GCD, and men and intersection and times of, you know, product of sets are not isolated concepts.
They're all the same concept over and over and to this. I know I haven't done told you this in enough detail or like gone through it with you and with real examples to make that clear but, but to give you a flavor that like we're really trying to define lots of things at once.
So, all of logic, what's called intuitionistic logic can be characterized by universal properties. So, intuitionist logic includes like all these symbols true false and or implies not for all exists includes all of it.
People, if you're like, Oh, I don't know what intuitionistic logic is. I know what logic is though. Well, it's all of logic. I just don't have the fact that not not PSP.
Anything that uses not in a real serious way, you have to be more careful of like if you want this thing you have to add it as an axiom, which is fine you can add this axiom, but it's not considered already baked in.
So not not P is not considered equal to P.
I think intuitionist logic is really nice. It lets you have a difference between someone says, Are you happy and someone else says, Well, I'm not not happy. And we might think haha that's so funny, but it's only funny because we thought that not not, you know, Oh, it's just, you know, we thought that not not P equals P but that's not true.
It's not not something.
It's actually quite nice to make to think of this as more general, like the person saying, Well, I wouldn't classify myself myself as exactly happy, but I'm kind of on the boundary is kind of hard to I'm not not happy.
And that more general thing is captured by intuitionist logic so
all I'm saying here is that intuitionist logic is what you're familiar with with all the same proof rules, except with a little bit less power on what not means.
So it's a little bit more general and if you want that you need to add it as an axiom.
Now all of intuitionist logic can be characterized by universal properties.
The notion of true and false and and or
implies not for all exists. These all
all the facts about them come from
universal properties. Now to really explain this I would need something like topos theory.
I can't do that in the time constraints here, we can almost get there.
And what am I talking about well logic propositions form an order under entailment. So entailment means like you can prove one from another.
And in this order, the idea is like P entails Q, if I can prove Q from P.
And so T true is the thing that everyone proves is the top element.
And false is the thing that proves everything false implies P for every P. And so false is the bottom.
And or is the joint. It's the thing that, you know, a P and Q.
P proves P entails P or Q Q entails P or Q and anything else that P and Q both entail P or Q also entails.
So each of these symbols are characterized by universal property.
But implies and also for all and exists.
Not is implies false. So we're almost there we were not going to do for all exists here, but we've got everything else.
So what this do implies, implies is that if P prime and P, when you and them that entails Q, then that mean what implies means that P prime entails this thing entails that P implies Q.
And this is a universal property, I haven't quite defined what universal property means but it says that P and P implies Q entails Q.
So P implies Q is sitting in this position where P and it entails Q. And if P prime also sits in that position where P and P prime entails Q, then P prime entails P implies Q.
And so you know that takes some time to unpack and stuff but the point is P implies Q is the closest thing.
So P implies Q is the thing that that when you end it with P, you get Q, and it's kind of the last thing of that sort, it's the closest thing to Q, in some sense.
So P implies Q is the universal thing that ends with P to entail Q anything else that does P implies Q is close is further up the chain.
Okay, let's get to functors. We are what are we five six through the talk.
Yeah, five six through the talk. Okay, so functors are mappings between categories, like any sort of mapping in category theory we wanted to preserve the reasoning.
Right, if we have two categories, we want a map between them to preserve the reasoning. So what structures are reasoning is there for a map to preserve.
They have objects, they both have arrows, they both have identities and they both have compositions, both categories, and that's what a category is.
And so a mapping between them should preserve objects arrows identities and compositions, a function should send objects of C to objects of D should send morphisms or arrows of C to arrows of D should send identities of C to identities of D and
and that's what a functor is.
Now, as you might guess, there's a category and I said earlier, of all categories and functors, the objects of cat are the categories, the arrows of cat are the functors, and you can compose and you can have an identity functor, but let's not get ahead of ourselves.
You know, you have these arrows so it's kind of natural to ask, oh are these actually arrows of a category. So see what itself be an object in cat, and the answer is yes.
But that is not, let's first just define it so if you have, excuse me if you have two categories C and D, a functor between them provides, well for every object of C, I have an object of D called f of C.
Every arrow in C from little C to little C prime, I can arrow and D from F of little C to F of little C prime.
And the rules are that like the F kind of like goes down F of identity of C is identity of F of C.
Like you get like this kind of sense of obviousness or that the reasoning is going to be your pencil on the paper is going to move in a way that lets you, I don't know feel comfortable like something about it when you get used to it.
So F of identity of C is identity of F of C, F of F compose G, so if you have two composable arrows, F of F compose G is F of F, composed F of G, you see how the F kind of like flies inside applies to F applies to G in the circle, this thing comes out.
There, there are familiar rules, it just says I take every object of C to an object of D, every arrow of C to an arrow of D, the identities in C go to identities in D, and the composites in C when I hit it with F go to composites in D.
We've already seen lots of examples. If I have a function from S to T, I get a functor from discrete S to discrete T.
It takes the objects of discrete S, those are the elements of S, to the objects of discrete T, those are the elements of T.
It takes the morphisms of S, well there weren't any except for identities. It takes the identities to identities, automatically, there's no composites to be interested in.
There's all those, and so we get a functor from discrete S to discrete T, which means we can kind of think of sets as the categories, as the discrete categories.
An order preserving map, if I have an order preserving map from S to S, S to T, it's a functor between the thin category on S and the thin category on T.
Remember, if I have an order, I can think of it as a category where the objects were S and there was a single morphism from one little S to one little S prime, when little S was less than or equal to S prime.
And if I had an order preserving map, I would get a functor between thin categories.
You could either at this point, if you're trying to figure this out, pause, remember everything, and check, and that would be a very good thing to do.
Or you could say, I think I'm following, basically, he's saying that orders are categories, and maps of orders are functors.
I don't know, I guess I believe him, because, you know, for some reason, but it makes sense, or it sounds right, I don't know. So anyway, that's all you really need to do is just follow the kind of idea here is that every order is a category and every order
preserving map is a functor. The functors are exactly the order for the remaps.
Every monochromomorphism gives you a functor between these two monoids considered as one object categories.
Every monoid is a category with one object, and a monochromomorphism is exactly what you need to send objects to objects where there's just one morphisms of m to morphisms of n, those are the elements of the monoid, and it preserves identity, and it preserves composition.
Those are the monochromomorphisms. Graph homomorphisms produce functors between free categories.
Every function is a relation, and that gives me a functor from set to rel. What I'm saying is every object and set is already an object in rel.
So to give a functor, I'll just send the object to the object. I'll send a morphism to a morphism. How do I do that?
Well, a morphism here or an arrow here is a function, and functions count as a special relation. Every arrow here gets turned into an arrow here.
Every function is a relation, and that preserves identity. Identity function becomes identity relation.
Composition of functions become composition of relations. That just works. You have to check it, and it's true. And you get a functor from set to rel.
There's a functor from mat r to set. It takes a number, like n, those are the objects here, and returns r to the power n, the n dimensional space of elements of r.
And it takes a matrix and gives a function that takes any rn, and adds a bunch of stuff and multiplies a bunch of stuff and gets an element of r, you know, m.
So there's a functor like this, and it preserves identity and composition, like all functors must.
But if we look at the top three, we see even more going on, like not only is every, like, we get a bunch of functors, but we in fact get functors from set, we get a functor from set to cat.
Every set gives us a category using discrete, and every function gives us a functor. And so now we're getting a functor from set to cat called disk.
We're getting a functor from order to cat that takes every order and gives us a thin category, or we're getting a functor from mon to cat that takes every monoid, and gives us a one object category.
And it takes any morphism of monoids to a morphism of categories, namely a functor. And so we're just, what I'm asking you to see here is not only how many different examples we've already kind of seen.
But also all this packaging, like, like this notion of thin.
And how it takes every order in the world and turns it into a category, and takes every map of orders and gives you a functor. It really packages a lot of information. And of course, it's only been 37 slides, you know, and a little bit over an hour.
And we're already packaging that much information someone who spends a few years in category theory packages up huge amounts of information into very small spaces.
So functors are like metaphors, you view one category, the category of categories, for example, through the lens of another, like, you might say, Oh, categories and functors that sounds hard but I do understand orders and order maps.
I do understand order preserving maps. So, you're telling me that every order is a category, and every order preserving map is a functor.
I think I can use that to get some insight into cat by looking through order. And similarly, I feel like I basically understand monoids are groups, and you're telling me that every group is a category.
And every group homomorphism is a functor. So that helps me understand what cat is. So it's like a metaphor that gives me insight into another world.
So, you know, the subject of category theory continues in this way. What am I, what do I mean, I mean it looks for these useful abstractions that repeat throughout math.
And since people have methodized everything they could for centuries like probability and measure and stuff. There's just a lot of content about the real world or at least the parts that you can abstract in category theory.
It just looks for the useful abstractions that appear throughout these various subjects.
And so right we didn't we didn't even touch on natural transformations which were the impetus of category theory. We've really only hit the tip of the iceberg. We didn't touch on monads which generate all algebraic theories like groups and rings and monoids and all that stuff.
We didn't touch on sheaves, which give a notion of local to global.
The second part of the tutorial I won't talk about any of those things either, but I will talk about some applications of category theory.
And one thing that I'll just give you a preview of now is graphical syntax.
Okay, so mathematical reasoning like we've done so far is just all text you just saw a bunch of text. This is like the first picture we've seen other than graphs way back when.
Following Roger Penrose.
Andre Gioia and Ross Street invented string diagrams in the 1980s, and their pictures like this. And what does this picture mean, like, we've got these boxes and we've actually got an outer box here, H, and there's a bunch of wires or strings.
What is all this stuff. Well, this thing can take meaning in many different settings.
Every string represent a set. So it's like I've got a set labeling every wire, and the boxes would represent a function takes an element of this set and returns a pair of an element in these two.
And if I had another one, a function from this set to this set, I could take G three and apply it to this pair, one coming from here, when coming from here.
I would put elements, apply it and get out to elements. And so if I had two elements, like an element of this set and an element of this set coming in, I would push them through all these different functions, and get an element coming out here and here, and therefore get a function H.
So it's like I've taken a bunch of functions, and I've composed them all together to give a function.
So the strings represent vector spaces and the boxes represent linear transformations.
These maybe represent are like you could label every string with a number natural number, and label these with me and put matrices inside appear. So what is inside of these guys is it are the strings representing sets, or are they representing vector spaces or what's
going on and those are all choices you get to make this same picture means something in many many different settings.
The same diagrams, even though they're not text are completely formal. This is a mathematical object.
I can define this object just as well as I can define anything in math.
And so the semantics what the boxes and the wires mean, that's your choice.
In fact, in any what's called a monoidal category, which is not the same as a monoid is a slightly more general thing, much more general thing.
In a monoidal category. This picture would make sense, it would.
If you assign a morphism in monoidal category to each of these things, then you would be assigning a morphism to this thing.
It's kind of like a big arrow being defined by smaller arrows. Anyway, the same syntax has many possible semantics. And that's what we'll start to touch on in the next thing.
So what sorts of string diagram pictures are valid. Well, it's kind of your choice what semantics you want.
If you're working in what's called a category, which we talked about a lot today, you're only allowed to define composites, you can have any number of composite fgh ij.
As long as they're wired together, one after the next after the next after the next, you could compose. So that's what this is meaning.
You can compose more general things. You can compose as long as as long as you know you have these wires in any picture like this in a monoidal category, you can compose in which called a traced monoidal category, which the name comes from trace of a matrix partial trace
which comes up in quantum mechanics and stuff.
So things like this, even with a feedback loop would make sense, you can compose this. If F and G are morphisms in a trace monoidal category, then this is too.
And in what's called a hypergraph category, you can have, you have pictures where you don't even have inputs and outputs on your morphisms, you just have ports, and any picture like this will make sense, you will be able to take this and compose as easily as, or as sensibly as you could
in a monoidal category.
I think I said all this stuff already. So right.
So the point is you can reason with these pictures. They're formal objects. Once you fix your kind of language, whether you're in a monoidal category or a trace monoidal category or whatever.
These pictures become formal objects that you can reason with.
And that that comes up in many, many fields, and we'll talk about that next time.
So that to explore and learn. And so why should you what what what might motivate you. Well, it makes you more of a polymath and what I mean is that once you learn category theory, you've realized there's people taking as much of math as possible, and converting it into the single language, where, and
that lets you learn these subjects new math subjects new category theory computer science subjects much more quickly, because people have condensed these subjects into abstractions you know well.
So once you learn category theory it's kind of a way of learning new subjects much more quickly.
And it provides this wealth of flexible and interoperable abstractions.
So what do we do today we began with some familiar math like orders and matrices.
We looked at universal properties like the property of tops and bottoms and ands and ors.
We looked at matrices and we considered kind of what are all these so many rules so many things interacting.
And then we defined categories and saw all these ideas in a broader context. We considered a lot of different examples of categories.
And ideas recursing on themselves and the structures layering, and we looked at orders and matrices in that light and how the universal properties co less things that come up throughout math like products.
GCD, you know, things that might seem pretty different intersection of sets and event diagram.
All of these things come up over and over, and they, they are given a single, you know, often given a single characterization and category theory.
And then we found we defined functor and give lots of examples.
And that's basically it so in part two I'll talk about some applications.
Okay.
