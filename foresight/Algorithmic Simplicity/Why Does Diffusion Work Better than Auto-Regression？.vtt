WEBVTT

00:00.000 --> 00:03.680
This is an artificial intelligence image generator.

00:03.680 --> 00:10.640
Given a text description of a picture, it will create, out of nothing, an image matching that description.

00:10.640 --> 00:16.480
As you can see, it is capable of generating high quality images of all kinds of different scenes.

00:16.480 --> 00:18.240
And it's not just images.

00:18.240 --> 00:23.840
In recent years, generative AI models have been developed that can generate text,

00:24.000 --> 00:29.600
audio, code, and soon videos too.

00:31.520 --> 00:36.960
All of these models are based on the same underlying technology, namely deep neural networks.

00:38.080 --> 00:43.120
In a few of my previous videos, I've explained how and why deep neural networks work so well,

00:43.680 --> 00:47.520
but I only explained how neural nets can solve prediction tasks.

00:48.240 --> 00:54.000
In a prediction task, the neural net is trained with a bunch of examples of inputs and their labels,

00:54.000 --> 00:58.480
and tries to predict what the label will be for a new input which it hasn't seen before.

00:59.360 --> 01:05.120
For example, if you trained a neural net on images labeled with the type of object appearing in each

01:05.120 --> 01:10.640
image, that neural net would learn to predict which object a human would say is in an image,

01:10.640 --> 01:13.120
even for new images which it hasn't seen before.

01:13.440 --> 01:19.520
Under the hood, the way that prediction tasks are solved is by converting the training dataset

01:19.520 --> 01:24.000
into a set of points in a space, and then fitting a curve through those points.

01:24.640 --> 01:28.080
So prediction tasks are also known as curve fitting tasks.

01:29.360 --> 01:34.160
And while prediction is certainly cool and very useful, it's not generation, right?

01:34.880 --> 01:38.400
This model is just fitting a curve to a set of points.

01:38.400 --> 01:40.000
It can't produce new images.

01:40.800 --> 01:45.040
So where does the creativity of these generative models come from,

01:45.040 --> 01:47.120
if neural nets can only do curve fitting?

01:48.880 --> 01:52.560
Well, all of these generative models are in fact just predictors.

01:53.360 --> 01:58.720
Yep, it turns out that the process of producing novel works of art can be reduced to a curve

01:58.720 --> 02:02.800
fitting exercise. And in this video, you'll learn exactly how.

02:04.960 --> 02:09.120
Suppose that we have a training dataset consisting of a bunch of images.

02:09.120 --> 02:14.240
We want to train a neural net to create new images which are similar in style to these

02:14.240 --> 02:20.320
training images. The first thing you might try is to simply use the images as labels

02:20.320 --> 02:26.000
to train the predictor. Here, we don't care about the mapping from inputs to outputs,

02:26.000 --> 02:30.960
so we can just use anything we like for the inputs, for example, a completely black image.

02:31.520 --> 02:35.680
Predictors learn to map inputs to outputs according to their training data.

02:35.680 --> 02:41.040
So this predictor, once trained, should be able to map the dummy or black image to

02:41.040 --> 02:44.240
new images like those seen in the training set, right?

02:48.400 --> 02:52.640
Uh, okay, maybe not quite. That didn't work so well.

02:52.640 --> 02:56.880
Instead of producing a nice beautiful picture, we just got this blurry mess.

02:58.000 --> 03:01.360
This demonstrates a very important fact about predictors.

03:01.360 --> 03:06.480
If there are multiple possible labels for the same input, the predictor will learn to output

03:06.480 --> 03:12.720
the average of those labels. For traditional classification tasks, this isn't really a

03:12.720 --> 03:17.280
problem because the average of multiple class labels can still be a meaningful label.

03:18.720 --> 03:22.800
For example, this image could plausibly be given two different labels.

03:22.800 --> 03:28.800
Both cat and dog would be valid labels. In this case, a classifier would learn to output the

03:28.800 --> 03:34.640
average of those labels, which means you end up with the score of 0.5 cat and 0.5 dog,

03:35.280 --> 03:41.280
which is still a useful label. In fact, it's arguably a better label than either of the original

03:41.280 --> 03:47.200
ones. On the other hand, when you average a bunch of images together, you do not get a

03:47.200 --> 03:54.320
meaningful image out, you just get a blurry mess. Let's try something a bit easier this time.

03:54.320 --> 03:59.520
How about instead of generating a new image from scratch, we try to complete an image which has

03:59.520 --> 04:05.120
part of it missing. In fact, let's make this really easy and suppose there is only one missing

04:05.120 --> 04:11.360
pixel, say the bottom right pixel. Can we train a neural net to predict the value of this one

04:11.360 --> 04:17.680
missing pixel? Well, as before, the neural net is going to output the average of plausible values

04:17.680 --> 04:22.800
that the missing pixel can take. But since it's only one pixel that we're predicting,

04:22.800 --> 04:28.400
the average value is still meaningful. The average of a bunch of colors is just another color.

04:28.400 --> 04:37.440
There's no blurring effect. So this model works perfectly fine. And we can use the value predicted

04:37.440 --> 04:41.360
by this neural net to complete images which are missing the bottom right pixel.

04:43.840 --> 04:48.400
Great, so we can complete images with one missing pixel. What about two?

04:48.800 --> 04:54.240
Well, we can do the same thing again. Train another neural net on images with two missing pixels,

04:54.240 --> 04:59.440
using the value of the second missing pixel as the label, and then use this neural net to fill

04:59.440 --> 05:07.440
in the second missing pixel. Now we have an image with just one missing pixel, and so we can use

05:07.440 --> 05:16.480
the first neural net to fill in that. Great. And we can do this for every pixel in the image.

05:17.440 --> 05:23.040
Train a neural net to predict the color of that pixel when it and all of the subsequent pixels

05:23.040 --> 05:29.520
are missing. Now we can complete an image starting from a fully black image and filling in one pixel

05:29.520 --> 05:35.920
at a time. Crucially, each neural net only predicts one pixel, and so there's no blurring effect.

05:38.000 --> 05:42.400
And there we have it. We have just generated a plausible image out of nothing.

05:43.120 --> 05:48.080
There's just one small problem. If we run this model again, it will generate exactly the same

05:48.080 --> 05:54.640
image. Not very creative, is it? But not to worry, we can fix this by introducing a bit of random

05:54.640 --> 06:00.800
sampling. You see, all predictors actually output a probability distribution over possible labels.

06:02.480 --> 06:07.120
Usually, we just take the label with the largest probability as the predicted value.

06:07.920 --> 06:14.400
But if we want diversity in our outputs, we can instead randomly sample a value from this

06:14.400 --> 06:20.240
probability distribution. This way, each time the model is run, it will sample different values

06:20.240 --> 06:25.200
at each step, which therefore changes the prediction for subsequent steps, and we get a

06:25.200 --> 06:30.640
completely different image each time. Now we have an interesting image generator.

06:31.120 --> 06:36.560
But still, at the end of the day, this model is made of predictors. They take as input a

06:36.560 --> 06:42.160
partially masked image, and predict the value of the next pixel. The only difference between this

06:42.160 --> 06:47.920
and a traditional image classifier is the label we used for training. The labels for our generator

06:47.920 --> 06:53.520
happen to be pixel colors, which come from the original image itself, and not a human labeler.

06:53.520 --> 06:58.800
This is a very important point in practice. It means we don't need humans to manually label

06:58.880 --> 07:04.480
images for this model. We can just scrape unlabeled images off the internet. But from the point of

07:04.480 --> 07:09.680
view of the neural net, it doesn't know, nor does it care, that the label came from the original

07:09.680 --> 07:15.200
image. As far as it's concerned, this is just a curve fitting exercise like any other.

07:16.400 --> 07:21.920
The generative model we've just created is called an autoregressor. We have a removal process,

07:21.920 --> 07:27.680
which removes pixels one at a time, and we train neural nets to undo this process. Generative

07:28.400 --> 07:30.960
generating and adding back in pixels one at a time.

07:32.880 --> 07:37.840
This is actually one of the oldest generative models. The very earliest use of autoregression

07:37.840 --> 07:44.000
dates back to 1927, where it was used to model the timing of sunspots. But autoregressors are

07:44.000 --> 07:52.160
still in use today. Most notably, ChatGPT is an autoregressor. ChatGPT generates text by using

07:52.160 --> 07:57.520
a transformer classifier to output a probability distribution over possible next words, given

07:57.520 --> 08:05.040
a partial piece of text. However, autoregressors are not used to generate images anymore. And the

08:05.040 --> 08:10.160
reason is that, while they can generate very realistic images, they take too long to run.

08:11.440 --> 08:16.720
In order to generate a sample with an autoregressor, we need to evaluate a neural net once for every

08:16.720 --> 08:22.720
element. This is fine for generating a few thousand words to make a piece of text, but large images

08:22.800 --> 08:28.320
can have tens of millions of pixels. How can we get away with fewer neural net evaluations?

08:33.600 --> 08:39.520
For our autoregressor, we removed pixels one at a time. But we don't have to remove only one

08:39.520 --> 08:46.320
pixel. We could, for example, remove a 4x4 patch of pixels at a time, and train the neural net to

08:46.320 --> 08:52.400
predict all 16 missing pixels at once. This way, when we use our model to generate an image,

08:52.400 --> 08:58.320
it can produce 16 pixels per evaluation, and so generation is 16 times as fast.

09:01.280 --> 09:07.200
But there is a limit to this. We can't generate too many pixels at the same time. In the extreme

09:07.200 --> 09:12.720
case, if we try to generate every pixel in the image at once, then we're back to the original

09:12.720 --> 09:17.840
problem. There are many possible labels that get averaged together into a blurry mess.

09:18.000 --> 09:23.120
To be clear, the reason why the image quality degrades is that, when we predict a bunch of pixels

09:23.120 --> 09:29.280
at the same time, the model has to decide on the values for all of them at once. There are lots

09:29.280 --> 09:35.040
of plausible ways that this missing patch could be filled in, and so the model outputs the average

09:35.040 --> 09:41.360
of those. The model isn't able to make sure that the generated values are consistent with each other.

09:41.520 --> 09:46.400
In contrast, when we predict one pixel at a time, the model gets to see the previously

09:46.400 --> 09:51.840
generated pixels, and so the model can change its prediction for this pixel to make it consistent

09:51.840 --> 09:56.960
with what has already been generated. This is why there's a trade-off. The more pixels we

09:56.960 --> 10:02.320
generate at once, the less computation we need to use, but the worse the quality of the generated

10:02.320 --> 10:08.400
images will be. Although this problem only arises in the end of the video, it's still

10:08.800 --> 10:14.160
although this problem only arises if the values we are predicting are related to each other.

10:14.720 --> 10:20.160
Suppose that the values were statistically independent of each other, that is, knowing

10:20.160 --> 10:26.160
one of them does not help to predict any others. In this case, the model doesn't need to look at

10:26.160 --> 10:31.200
the previously generated values since knowing what they were wouldn't change its prediction for the

10:31.200 --> 10:37.600
next value anyway. In this case, you can predict all of them at the same time without any loss in

10:37.600 --> 10:45.120
quality. So that means, ideally, we want our model to generate a set of pixels that are unrelated

10:45.120 --> 10:51.760
to each other. For natural images, nearby pixels are the most strongly related because they are

10:51.760 --> 10:57.840
usually part of the same object. Knowing the value of one pixel very often gives you a good idea of

10:57.840 --> 11:03.680
what color nearby pixels will be. This means that removing pixels in contiguous chunks is

11:03.680 --> 11:10.000
actually the worst way to do it. Instead, we should be removing pixels that are far away from each

11:10.000 --> 11:17.600
other and hence more likely to be unrelated. So, if in each step we remove a random set of pixels

11:17.600 --> 11:23.280
and predict values for those, then we can remove more pixels in each step for the same loss in

11:23.280 --> 11:29.520
image quality compared to contiguous chunks. In order to minimize the number of steps needed

11:29.520 --> 11:34.320
for generation, we want the pixels we remove in each step to be as spread out as possible.

11:34.960 --> 11:40.480
Removing pixels in a random order is a pretty good way of maximizing the average spread,

11:40.480 --> 11:47.840
but there is an even better way. We can think of our generative model as two processes. A removal

11:47.840 --> 11:53.520
process that gradually removes information from the input until nothing is left, and a generation

11:53.520 --> 12:00.000
process that uses neural nets to undo the removal process, generating and adding back in information.

12:01.280 --> 12:07.280
So far, we have been completely removing pixels, but rather than completely removing a pixel,

12:07.280 --> 12:13.040
we could instead remove only some of the information from a pixel by, for example,

12:13.040 --> 12:19.520
adding a small amount of random noise to it. This means we don't know exactly what the original

12:19.520 --> 12:24.000
pixel value was, but we do know it was somewhere close to the noisy value.

12:25.840 --> 12:31.280
Now, instead of removing a bunch of pixels in each step, we can add noise to the entire image.

12:32.080 --> 12:37.600
This way, we can remove information from every pixel in the image in a single step,

12:37.600 --> 12:42.960
which is the most spread out way of removing information. And since it's more spread out,

12:42.960 --> 12:47.760
you can remove more information in each step for the same loss in generation quality.

12:50.000 --> 12:54.240
There is one small problem with this, though. When we want to generate a new image,

12:54.240 --> 12:58.080
we need to start the neural net off with some initial blank image.

12:58.960 --> 13:04.720
When we were removing pixels, then every image eventually ends up as a completely black image.

13:04.720 --> 13:07.840
So of course, that's where we start the generation process from.

13:08.800 --> 13:13.200
But now that we're adding noise, the values just keep getting larger and larger,

13:13.200 --> 13:18.000
never converging to anything. So where do we start the generation process from?

13:18.640 --> 13:22.240
We can avoid this problem by changing our nosing step slightly,

13:22.240 --> 13:26.240
so that we first scale down the original value and then add the noise.

13:27.120 --> 13:30.800
This ensures that when we repeat this nosing step many times,

13:30.800 --> 13:35.680
information from the original image will disappear, and the result will be equivalent

13:35.680 --> 13:41.040
to a pure random sample from the noise distribution. So we can start our generation

13:41.040 --> 13:47.600
process from any such noise sample. And there we have it. This is known as a denoising diffusion

13:47.600 --> 13:53.520
model. The overall form is identical to an autoregressor. The only difference is the way

13:53.520 --> 13:59.040
in which we remove information at each step. By adding noise, we can spread out the removal

13:59.040 --> 14:04.000
of information all across the image, which makes the predicted values as independent

14:04.000 --> 14:08.320
of each other as possible, allowing you to use fewer neural net evaluations.

14:09.440 --> 14:14.160
Empirically, diffusion rules can produce high-quality photorealistic images in about

14:14.160 --> 14:17.440
100 steps, whereas autoregressors would take millions.

14:20.000 --> 14:24.080
Now that we understand how these generative models work at a conceptual level, if you are

14:24.080 --> 14:29.040
ever going to implement these models in practice, there are a few important technical details that

14:29.040 --> 14:35.360
you should be aware of. First, in the procedure I described for autoregression, I use the different

14:35.360 --> 14:40.960
neural net in each step of the process. This is certainly the best way to get the most accurate

14:40.960 --> 14:46.000
predictions, but it's also very inefficient, since we need to train a whole bunch of different

14:46.000 --> 14:53.920
neural nets. In practice, you would just use the same model to do every step. This gives slightly

14:53.920 --> 14:59.600
worse predictions, but the savings and computation time more than make up for it. In order to train

14:59.600 --> 15:04.960
a single neural net to perform all of the generation steps, you would remove a random number of pixels

15:04.960 --> 15:10.240
from each input and train the neural net to predict the corresponding next pixel of each input.

15:11.280 --> 15:16.400
Additionally, you can also give the number of pixels removed as an input to the neural net

15:16.400 --> 15:19.520
so that it knows which pixel it's supposed to be generating.

15:20.640 --> 15:24.000
Now this one neural net can be used for all generation steps.

15:25.440 --> 15:29.840
In the setup I just described, for each training image, the neural net is trained on

15:29.840 --> 15:36.240
only one generation step for that image. But ideally, we would like to train it on every

15:36.240 --> 15:41.120
generation step of every image. We can get more use out of our training data that way.

15:42.160 --> 15:47.360
If you did this the naive way, you would have to evaluate the neural net once for every generation

15:47.360 --> 15:52.960
step, which means a lot more computation. Fortunately, there exist special neural net

15:52.960 --> 15:58.160
architectures known as causal architectures that allow you to train on all of these generation

15:58.160 --> 16:04.800
steps while only evaluating the neural net once. There exist causal versions of all of the popular

16:04.800 --> 16:10.000
neural net architectures such as causal convolutional neural nets and causal transformers.

16:10.720 --> 16:15.120
Causal architectures actually give slightly worse predictions, but in practice,

16:15.120 --> 16:20.240
autoregression is almost always done with causal architectures, because the training is so much

16:20.240 --> 16:25.760
faster. The generation process for causal architectures is still exactly the same though.

16:26.560 --> 16:31.440
For diffusion models, you can't use causal architectures, and so you do have to just

16:31.440 --> 16:34.480
train with each data point at a random generation step.

16:37.360 --> 16:43.120
I described the diffusion model as predicting the slightly less noisy image from the previous step.

16:43.120 --> 16:48.160
However, it's actually better to predict the original completely clean image at every step.

16:49.040 --> 16:54.960
The reason for this is it makes the job of the neural net easier. If you make it predict the

16:54.960 --> 17:00.480
noisy next step image, then the neural net needs to learn how to generate images at all different

17:00.480 --> 17:06.160
noise levels. This means the model will waste some of its capacity learning to produce noisy

17:06.160 --> 17:12.400
versions of images. If you instead just have the neural net always predict the clean image,

17:12.400 --> 17:16.960
then the model only needs to learn how to generate clean images, which is all we care about.

17:17.840 --> 17:23.040
You can then take the predicted clean image and reapply the nosing process to it to get

17:23.040 --> 17:29.520
the next step of the generation process. Except that when you predict the clean image,

17:29.520 --> 17:35.440
then at the early steps of the generation process, the model only has pure noise as input,

17:35.440 --> 17:41.680
so the original clean image could have been anything. And so you get a blurry mess again.

17:43.200 --> 17:48.880
To avoid this, we can train the neural net to predict the noise which was added to the image.

17:48.880 --> 17:53.360
Once we have a predicted value for the noise, we can plug it into this equation to get a

17:53.360 --> 17:59.120
prediction for the original clean image. So we are still predicting the original clean image

17:59.120 --> 18:04.800
just in a roundabout way. The advantage of doing it this way is that now the model output is

18:04.800 --> 18:10.400
uncertain at the latest stages of the generation process, since any noise could have been added

18:10.400 --> 18:15.680
to the clean image, so the model outputs the average of a bunch of different noise samples,

18:15.680 --> 18:22.640
which is still valid noise. So far we've just been generating images from nothing,

18:22.640 --> 18:27.680
but most image generators actually allow you to provide a text prompt describing the image you

18:27.760 --> 18:33.120
want to make. The way this works is exactly the same, you just give the neural net the text as

18:33.120 --> 18:38.560
an additional input at each step. These models are trained on pairs of images and their corresponding

18:38.560 --> 18:45.040
text descriptions, usually scraped from image alt text tags found on the internet. This ensures

18:45.040 --> 18:49.920
that the generated image is something for which the text prompt could plausibly have been given

18:49.920 --> 18:55.840
as a description of that image. In principle, you can condition generative models on anything,

18:55.840 --> 19:00.960
not just text, so long as you can find appropriate training data. For example,

19:00.960 --> 19:03.680
here is a generative model that is conditioned on sketches.

19:08.560 --> 19:12.400
Finally, there's a technique to make conditional diffusion models work better,

19:12.400 --> 19:18.240
called classifier free guidance. For this, during training, the model will sometimes be given the

19:18.240 --> 19:24.240
text prompts as additional input, and sometimes it won't. This way, the same model learns to do

19:24.240 --> 19:30.000
predictions with or without the conditioning prompt as input. Then at each step of the denoising

19:30.000 --> 19:35.840
process, the model is run twice, once with the prompt and once without. The prediction without

19:35.840 --> 19:41.040
the prompt is subtracted from the prediction with the prompt, which removes details that are generated

19:41.040 --> 19:46.480
without the prompt, leaving only the details that came from the prompt, leading to generations which

19:46.480 --> 19:52.800
more closely flow the prompt. In conclusion, generative AI, like all machine learning,

19:52.800 --> 19:58.640
is just curve fitting. And that's all for this video. If you enjoyed it, please like and subscribe,

19:58.640 --> 20:03.040
and if you have any suggestions for topics you'd like to see me cover in a future video,

20:03.040 --> 20:08.720
leave a comment below.

