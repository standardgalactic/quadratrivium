WEBVTT

00:00.000 --> 00:05.680
Mamba is a new neural net architecture that is better than Transformers at language modelling.

00:05.680 --> 00:11.720
Yes, that's right, after reigning supreme for 7 years, the Transformer has finally been

00:11.720 --> 00:13.200
dethroned.

00:13.200 --> 00:16.640
Well, maybe.

00:16.640 --> 00:22.280
So far, Mamba has only been tested at small model sizes, up to a few billion parameters,

00:22.280 --> 00:24.960
but the results so far are promising.

00:24.960 --> 00:28.800
In addition, Mamba uses less compute than Transformers.

00:28.800 --> 00:34.480
For an input sequence of n words, Mamba only uses n log n compute, whereas Transformers

00:34.480 --> 00:36.160
use n squared.

00:36.160 --> 00:42.080
So Mamba-based language models should allow for much greater context sizes to be used.

00:42.080 --> 00:45.960
In this video, we're going to do a deep dive of the Mamba architecture.

00:45.960 --> 00:46.960
What is it?

00:46.960 --> 00:47.960
Why does it work so well?

00:47.960 --> 00:54.200
And how could you have gone about designing such an architecture yourself?

00:54.200 --> 00:59.000
Unfortunately Mamba is presented as an extension of something called a state space model.

00:59.000 --> 01:03.120
State space models are another type of sequence model that have been steadily gaining popularity

01:03.120 --> 01:08.920
over the past few years, but to be honest, the theory behind state space models is massively

01:08.920 --> 01:12.560
over complicated and uses some pretty advanced mathematics.

01:12.560 --> 01:18.160
Fortunately, Mamba can also be understood as an extension of recurrent neural networks,

01:18.160 --> 01:22.480
or RNNs for short, which are much easier to understand.

01:22.480 --> 01:28.200
So in this video, we will be taking the RNN path to understanding Mamba.

01:28.200 --> 01:29.840
Now let's get started.

01:29.840 --> 01:35.840
What is a recurrent neural network?

01:35.840 --> 01:40.640
Given a sequence of input vectors, a convolutional layer applies a neural net to consecutive

01:40.640 --> 01:42.880
groups of vectors.

01:42.880 --> 01:47.760
The key thing is that the neural net only sees a small number of vectors at a time, which

01:47.760 --> 01:49.840
makes the model easy to train.

01:50.320 --> 01:55.240
The downside is that information from vectors which are far away can't be combined until

01:55.240 --> 01:58.320
many convolutional layers have been applied.

01:58.320 --> 02:02.800
This makes it difficult for convolutional neural nets to understand long range dependencies

02:02.800 --> 02:10.080
in their input, and such long range dependencies occur all the time in natural language text.

02:10.080 --> 02:15.120
To remedy this flaw, the transformer architecture was invented, which successfully allows a

02:15.120 --> 02:20.240
single layer to combine information from vectors no matter how far away they are.

02:20.240 --> 02:25.280
I previously made a video explaining how and why transformers work in detail, which you

02:25.280 --> 02:27.680
can find here.

02:27.680 --> 02:32.440
And while transformers work great, they have a significant limitation, which is that the

02:32.440 --> 02:36.880
amount of compute they use is quadratic in the input length.

02:36.880 --> 02:41.120
This isn't a huge deal for small inputs, but if you want to have a million vectors in

02:41.120 --> 02:48.400
the input, that means you need to do a million times a million operations, which is a lot.

02:48.400 --> 02:53.320
Recurrent neural nets take a completely different approach to improving convolutional layers.

02:53.320 --> 02:55.840
The idea is very simple.

02:55.840 --> 02:59.800
Instead of applying the neural net to two consecutive input vectors, you apply it to

02:59.800 --> 03:05.160
one input vector and the previous output of the neural net.

03:05.160 --> 03:09.200
This seems like a small change, but it has profound consequences.

03:09.200 --> 03:14.640
Each output vector now contains information from all of the input vectors prior to it,

03:14.640 --> 03:17.720
instead of only one previous vector.

03:17.720 --> 03:22.600
This final output vector contains information from every vector in the input, no matter how

03:22.600 --> 03:24.760
many there are.

03:24.760 --> 03:28.320
And we have not used any more compute than a convolutional layer.

03:28.320 --> 03:32.920
We've managed to incorporate long range information for free.

03:32.920 --> 03:38.800
This is exactly what we want, or at least it would be if it weren't for two small problems

03:38.800 --> 03:43.960
with RNNs, which make them almost impossible to use in practice.

03:43.960 --> 03:48.760
The first problem is that while a recurrent layer uses the same amount of compute as a

03:48.760 --> 03:55.720
convolutional layer, that compute cannot be parallelized across multiple processes.

03:55.720 --> 04:00.400
Even if you have lots of processes available, you can't begin evaluating the neural net

04:00.400 --> 04:05.520
on an input until all of the previous steps have finished, because you need to feed the

04:05.520 --> 04:10.600
output from the previous step into the neural net.

04:10.600 --> 04:14.920
Compare this to a convolutional layer where the neural net only needs to see the original

04:14.920 --> 04:16.040
input.

04:16.040 --> 04:20.280
You can run the neural net on all inputs at the same time, so long as you have enough

04:20.280 --> 04:23.720
processes available.

04:23.720 --> 04:28.800
And since modern hardware such as GPUs are highly specialized for parallel computation

04:28.800 --> 04:37.360
with thousands of processes, RNNs are actually a lot slower than CNNs in practice.

04:37.360 --> 04:45.880
In fact, RNNs are even slower than transformers despite doing less computation.

04:45.880 --> 04:50.840
And the second problem is that RNNs are incredibly difficult to train.

04:50.840 --> 04:55.960
While in theory a single recurrent layer can incorporate information from arbitrarily

04:55.960 --> 04:59.280
many inputs, in practice they don't.

04:59.280 --> 05:06.200
Instead, they only learn to incorporate information from the previous few dozen inputs at most.

05:06.200 --> 05:11.600
The idea for RNNs has been around since the 1980s, but because of these two problems,

05:11.600 --> 05:16.400
RNNs have fallen out of favor with convolutional neural nets and transformers being much more

05:16.400 --> 05:18.560
successful in practice.

05:18.560 --> 05:24.240
In fact, RNNs have hardly been used at all in the past decade.

05:24.360 --> 05:30.840
Well, last year a new paper was published showing that linear RNNs can avoid both of

05:30.840 --> 05:37.520
these problems, and therefore linear RNNs are highly effective long sequence models.

05:37.520 --> 05:41.480
So what is a linear recurrent neural network?

05:41.480 --> 05:53.160
Well, you simply replace the neural net with a linear function.

05:53.160 --> 05:58.360
This might seem like a bad idea, since linear functions can only perform relatively simple

05:58.360 --> 06:03.480
transformations of their inputs, but we can make up for it by applying a full neural net

06:03.480 --> 06:06.800
to each output vector afterwards.

06:06.800 --> 06:11.120
This is similar to how in transformers you can replace the value neural nets with simple

06:11.120 --> 06:16.040
linear functions, and then add neural nets in between self-attention layers to make up

06:16.040 --> 06:19.600
for the lack of non-linear processing power.

06:19.600 --> 06:25.560
So just like in a transformer, we will alternate linear recurrent layers with element-wise

06:25.560 --> 06:28.640
neural networks.

06:28.640 --> 06:33.840
But importantly, by making the recurrent operation purely linear, it becomes possible

06:33.840 --> 06:37.360
to solve both of the RNN problems.

06:37.360 --> 06:42.480
To start with, I'll explain how a linear recurrence applied to n vectors can be computed

06:42.480 --> 06:46.760
in parallel in just log n time.

06:46.760 --> 06:52.280
And then I'll explain how the training issues that plague regular RNNs can be fixed in linear

06:52.280 --> 06:59.920
recurrences.

06:59.920 --> 07:03.040
The linear recurrence operator is given by this formula.

07:03.040 --> 07:08.920
To get the ith output vector, you multiply the previous i-1th output vector with a matrix

07:08.920 --> 07:15.400
w-y, and add the ith input vector multiplied by a different matrix w-x.

07:15.400 --> 07:20.680
The entries in the w matrices are the parameters which will be learned by the model, so they

07:20.680 --> 07:25.760
start off as random samples from a normal distribution centered at zero, and are then

07:25.760 --> 07:28.400
updated with gradient descent.

07:28.400 --> 07:33.600
And since the w-x matrix is just applied to each input independently, we can actually

07:33.600 --> 07:37.320
just think of it as being part of the previous layer.

07:37.320 --> 07:43.280
So we can simplify our recurrence operator to just add the input x, assuming that a linear

07:43.280 --> 07:48.440
function has already been applied to the input in the previous layer.

07:48.440 --> 07:53.080
A linear recurrence is actually a special case of a more general operation called the

07:53.080 --> 07:59.880
scan, so let's start with the simplest example of a scan, a cumulative sum.

07:59.880 --> 08:04.880
Given a list of n numbers as input, the goal is to compute the list of partial sums up

08:04.880 --> 08:11.280
to each term, so the ith item in the output list should be the sum of the first i-items

08:11.280 --> 08:14.760
of the input list.

08:14.760 --> 08:20.040
While it is trivial to compute this by simply adding the numbers together one at a time,

08:20.040 --> 08:24.800
we want to do it in parallel.

08:24.800 --> 08:28.240
And it turns out we can do so as follows.

08:28.240 --> 08:35.920
First add together each consecutive pair of numbers, then from the resulting list add

08:35.920 --> 08:47.040
together pairs of numbers which are two steps apart, then four steps apart, and eight, and

08:47.040 --> 08:49.080
so on.

08:49.080 --> 08:54.560
Each iteration doubling the step size, until the step size is as large as the entire input

08:54.560 --> 08:59.720
list, which will be after log n steps.

08:59.720 --> 09:04.920
This algorithm works because at each iteration the ith output element contains the sum of

09:04.920 --> 09:08.800
the previous step size numbers.

09:08.800 --> 09:14.600
For example, in the first iteration each output number is the sum of the previous two terms.

09:14.600 --> 09:20.200
In the next iteration each item contains the sum of the previous two terms plus the previous

09:20.200 --> 09:27.520
two terms starting two away, that is the sum of the previous four terms, and so on.

09:27.520 --> 09:32.920
When the step size is the size of the input, each output contains the sum of all previous

09:32.920 --> 09:36.280
terms as desired.

09:36.280 --> 09:41.640
It's trivial to see that each iteration can be computed in parallel, however the different

09:41.640 --> 09:48.540
iterations do still need to be computed sequentially, and there are log n iterations.

09:48.540 --> 09:54.640
So if you have n processors, the total runtime of this algorithm is log n, down from n of

09:54.640 --> 09:57.960
the naive sequential version.

09:57.960 --> 10:03.560
And this same algorithm works for computing lists of cumulative applications of any binary

10:03.560 --> 10:11.160
operator, not just addition, so long as the binary operator is associative.

10:11.160 --> 10:15.320
Associative means that you can change the order of application and you'll still end

10:15.320 --> 10:17.440
up with the same result.

10:17.440 --> 10:25.160
This is true of addition, which is why our parallel cumulative sum algorithm works.

10:25.160 --> 10:30.280
And it's also true of a bunch of other operations.

10:30.280 --> 10:35.640
In particular, this binary operator is associative.

10:35.640 --> 10:41.520
Note that this operator uses a pair of a matrix and a vector as input and output, instead

10:41.520 --> 10:46.200
of just a single number, like with addition.

10:46.200 --> 10:53.240
And remarkably, performing a scan with this operator is equivalent to a linear recurrence.

10:53.240 --> 10:58.240
We first need to replace our input list of vectors with a list of pairs where the first

10:58.240 --> 11:03.560
element is the recurrent weight matrix and the second element is the input vector, but

11:03.560 --> 11:09.600
then we just perform the scan as usual.

11:09.600 --> 11:14.760
You can check for yourself that this operator is in fact associative by expanding a few

11:14.760 --> 11:18.760
terms in the other order.

11:18.760 --> 11:24.400
To summarize, we just need to do our parallel cumulative sum algorithm with this operator

11:24.400 --> 11:26.600
in place of addition.

11:26.600 --> 11:33.320
And we get the result of a linear recurrent layer in just log n time.

11:33.320 --> 11:35.840
Except for one small problem.

11:35.840 --> 11:41.040
If you look closely at this operation, the way it works is by using the first element

11:41.040 --> 11:46.760
of the tuples as a cumulative matrix, which contains the product of all of the matrices

11:46.760 --> 11:48.520
seen so far.

11:48.520 --> 11:55.160
That's why the first element of the output tuple is the product of the two input matrices.

11:55.160 --> 12:01.280
But this means we're performing a d by d times d by d matrix multiplication in every

12:01.280 --> 12:05.900
step, where d is the dimension of the vectors.

12:05.900 --> 12:08.480
This is really slow.

12:08.480 --> 12:13.360
Note that in the original sequential RNN, we didn't need to keep track of this cumulative

12:13.360 --> 12:18.840
matrix, and so we only ever multiply the weight matrix with a length d input vector

12:18.840 --> 12:23.440
at each step, which is a d squared operation.

12:23.440 --> 12:28.480
But now we have to do a d cubed operation in every step.

12:28.480 --> 12:34.160
For standard model sizes, this is easily a thousand fault increase in computation.

12:34.160 --> 12:37.080
And that's bad.

12:37.080 --> 12:40.240
Fortunately there is a way around this.

12:40.240 --> 12:42.880
Matrix diagonalization.

12:42.880 --> 12:47.920
You see, almost every square matrix can be factored into the product of an invertible

12:47.920 --> 12:55.360
matrix P, a diagonal matrix D, and that first matrix P inverse, so long as the matrix elements

12:55.360 --> 12:58.360
are allowed to be complex numbers.

12:58.360 --> 13:02.680
Here is an example.

13:02.680 --> 13:08.040
Note that this middle matrix is diagonal, that is, all elements except for the main diagonal

13:08.040 --> 13:10.040
are zero.

13:10.040 --> 13:14.800
What's neat about this is when you multiply the matrix by itself in this form, the inner

13:14.800 --> 13:21.060
P inverse and P terms cancel, and the product of two diagonal matrices is just the diagonal

13:21.060 --> 13:24.000
matrix with the product of elements.

13:24.000 --> 13:29.000
That is, in order to compute d squared, all you need to do is square the elements on the

13:29.000 --> 13:35.560
main diagonal of d, which can be done in just m operations instead of m cubed.

13:35.560 --> 13:37.360
Much better.

13:37.360 --> 13:43.160
So then what we can do is represent the recurrent weight matrix in diagonalized form, which

13:43.160 --> 13:48.560
means we only need to use a complex vector which contains the elements of the main diagonal

13:48.560 --> 13:51.600
of d.

13:51.600 --> 13:57.440
That is to say, we first apply a complex matrix P to the input vectors, then perform the linear

13:57.440 --> 14:03.160
recurrence with a complex weight vector w, using element-wise multiplication, and finally

14:03.160 --> 14:07.640
apply P inverse to the output.

14:07.640 --> 14:12.640
The result of this will then be equivalent to a linear recurrence for some real-valued

14:12.640 --> 14:14.880
matrix w.

14:14.880 --> 14:19.440
But when computed this way, the recurrence operator only needs to compute element-wise

14:19.440 --> 14:26.200
multiplication between two vectors to update the cumulative weights instead of matrix multiplication.

14:26.200 --> 14:31.280
When we plug this operator into our parallel scan algorithm, the total compute is now adjust

14:31.280 --> 14:35.660
dn log n, and the parallel runtime is log n.

14:35.660 --> 14:38.160
Much better.

14:38.160 --> 14:42.680
Note that the parameters of this layer are the complex entries in the recurrent weight

14:42.680 --> 14:48.480
vector w and the matrix P. In practice, you would just use two separate real numbers to

14:48.480 --> 14:53.320
represent the real and imaginary components of each parameter, which are initialized by

14:53.320 --> 14:57.960
sampling from a normal distribution centered at zero, and updated with gradient descent

14:57.960 --> 15:01.200
as usual.

15:02.120 --> 15:07.600
Computing matrix inverses is really slow, so in practice we don't bother, and instead

15:07.600 --> 15:13.560
just use two independent complex matrices before and after the linear recurrence.

15:13.560 --> 15:17.880
This actually makes the model more expressive than a real-valued linear RNN, and it saves

15:17.880 --> 15:23.780
computation, but it does mean that the model is no longer equivalent to a real-valued recurrence,

15:23.780 --> 15:26.400
and the output can now be a complex number.

15:26.400 --> 15:30.920
So we will need to take the real-valued part of the output before passing it to the next

15:31.040 --> 15:33.040
layer.

15:33.040 --> 15:38.960
Okay, we have seen how to make linear RNNs fast for modern hardware, but what about the

15:38.960 --> 15:40.120
other problem?

15:40.120 --> 15:44.040
That RNNs are very difficult to train.

15:44.040 --> 15:49.040
Before we solve this problem, here is a quick recap of why training RNNs is so problematic

15:49.040 --> 15:50.880
in the first place.

15:50.880 --> 15:55.000
Neural nets are trained by subtracting the gradient of the loss function from each weight

15:55.000 --> 15:56.440
in the model.

15:56.440 --> 15:57.440
What is the gradient?

15:57.440 --> 16:04.760
Well, imagine evaluating the neural net, then increasing the value of a weight by a very

16:04.760 --> 16:09.760
small amount, and then evaluating it again.

16:09.760 --> 16:14.600
The difference in these scores is proportional to the gradient for that weight, and it tells

16:14.600 --> 16:19.880
you how to change the weight to make the neural net better, so let's evaluate the gradient

16:19.880 --> 16:21.920
of a linear recurrent layer.

16:22.800 --> 16:27.280
Actually, to make this a bit easier, let's simplify the model and suppose that every

16:27.280 --> 16:33.160
input after the first is zero, so we can just ignore them.

16:33.160 --> 16:37.960
When we evaluate the recurrent layer, at each step the previous output is multiplied by

16:37.960 --> 16:43.360
the weight vector, so after n steps, the output vector is equal to the recurrent weight vector

16:43.360 --> 16:48.280
to the power of n times the first vector, x1.

16:48.280 --> 16:56.920
When we increase the weight by a small amount and evaluate it again, we get this.

16:56.920 --> 17:03.040
Taking the difference, we get up to a constant scaling factor, w to the power of n-1 times

17:03.040 --> 17:04.680
x1.

17:04.680 --> 17:11.000
The problem here is that as n becomes large, this term, w to the n-1, either gets very

17:11.000 --> 17:16.840
small or very large, depending on whether the values in w are less than or greater than

17:16.840 --> 17:19.440
1.

17:19.440 --> 17:21.960
In either case, it's a problem.

17:21.960 --> 17:27.000
If the gradient is very large, then the neural net weights change too much, and the existing

17:27.000 --> 17:31.240
functionality already learned by the neural net gets destroyed.

17:31.240 --> 17:35.560
If the gradient is very small, then the weights don't change enough, and the neural net doesn't

17:35.560 --> 17:38.220
learn anything at all.

17:38.220 --> 17:41.500
This is what makes training RNNs difficult.

17:41.500 --> 17:47.660
While in principle, RNNs can use infinitely long context, in practice, with gradient-based

17:47.660 --> 17:53.560
training techniques, the RNN will only learn to use context for as many steps as the gradient

17:53.560 --> 17:56.340
remains the right size for learning.

17:56.340 --> 18:01.580
This is known as the problem of vanishing and exploding gradients.

18:01.580 --> 18:06.700
And when we add back in non-zero inputs, this problem only gets worse, as the additional

18:06.740 --> 18:11.260
inputs make the gradients even more unstable.

18:11.260 --> 18:16.340
And to be clear, the reason why this isn't a problem for regular neural nets is because

18:16.340 --> 18:20.100
they use different weights in each layer.

18:20.100 --> 18:23.780
Some layers can have weights smaller than one, and some layers can have weights larger

18:23.780 --> 18:28.820
than one, so long as the gradient remains about the same size, the neural net will be able

18:28.820 --> 18:32.660
to learn.

18:32.740 --> 18:37.620
There are lots and lots of different configurations of weights that result in stable gradients,

18:37.620 --> 18:44.220
and it's easy to stay in stable configurations all throughout training.

18:44.220 --> 18:50.140
But for RNNs, you're using the same weight in each step, so there is exactly one stable

18:50.140 --> 18:54.180
configuration, which is when the weight is one.

18:54.180 --> 19:01.020
Any deviation from one, and you have exponentially growing or decaying gradients.

19:01.020 --> 19:05.980
Note that when the weights are complex numbers, the same argument applies just using the absolute

19:05.980 --> 19:09.300
value of the weights.

19:09.300 --> 19:12.420
So how can we fix vanishing and exploding gradients?

19:12.420 --> 19:18.580
Well, we saw that the RNN gradients are stable, so long as their current weights are one,

19:18.580 --> 19:21.100
and the inputs are zero.

19:21.100 --> 19:27.580
So in the linear RNN paper, the authors propose to initialize their linear RNNs in this stable

19:27.580 --> 19:29.700
state.

19:29.700 --> 19:35.140
Specifically, they parameterize the weights in complex polar form A times E to the IB,

19:35.140 --> 19:37.820
where A is magnitude and B is angle.

19:37.820 --> 19:42.740
They then restrict the magnitude to be less than one by running A through this E to the

19:42.740 --> 19:48.540
minus E function, which always outputs a number between zero and one.

19:48.540 --> 19:52.780
And instead of randomly sampling A from a normal distribution centered at zero, as we

19:52.780 --> 19:58.620
usually do, they initialize A so that the magnitude of E to the minus E to the A is

19:58.620 --> 20:03.620
uniformly distributed between 0.999 and one.

20:03.620 --> 20:09.060
They initialize the angle B uniformly between zero and pi on 10 radians.

20:09.060 --> 20:13.820
This ensures that, at initialization, the weights are all very close to one.

20:13.820 --> 20:19.620
Finally, they multiply the inputs by delta, which is another learned parameter initialized

20:19.620 --> 20:26.300
to the square root of one minus E to the minus E to the A, which since E to the minus E to

20:26.300 --> 20:30.820
the A is close to one, this is some very small number.

20:30.820 --> 20:34.900
This ensures that, at initialization, the inputs are all close to zero, and so they

20:34.900 --> 20:37.540
don't interfere with the recurrence.

20:37.540 --> 20:44.740
So at initialization, this model is approximately the same as the stable RNN I showed you before.

20:44.740 --> 20:48.700
After the model begins training and the weights change, there's no guarantee that it will

20:48.700 --> 20:53.720
remain stable, but in practice it appears that just initializing the model like this

20:53.720 --> 20:59.220
is sufficient to allow it to learn to remember context for tens of thousands of steps.

20:59.220 --> 21:04.720
And there we have it, with these modifications, we now have a linear RNN that is faster compute

21:04.720 --> 21:08.320
and learns to use extremely long context.

21:08.320 --> 21:13.320
In the linear RNN paper, they evaluate this model on the long range arena benchmark, which

21:13.320 --> 21:18.560
is a collection of six synthetic tasks that evaluate a model's ability to perform long

21:18.560 --> 21:21.720
range reasoning.

21:21.720 --> 21:26.720
For example, in the path X task, the model must classify images as whether or not they

21:26.720 --> 21:34.000
contain a complete dotted path between two circles, except that the images are flattened

21:34.000 --> 21:41.960
into one long sequence of 16,000 pixels.

21:41.960 --> 21:47.720
The linear RNN achieved state of the art performance on the long range arena, outperforming transformers

21:47.720 --> 21:54.480
by about 33% on average across tasks.

21:54.480 --> 21:59.160
So now that we understand the linear RNN, what's with all the talk about state space

21:59.160 --> 22:00.160
models?

22:00.160 --> 22:06.040
Well, it turns out that state space models are just linear RNNs.

22:06.040 --> 22:10.360
State space models were inspired by a control theory and were derived from a totally different

22:10.360 --> 22:16.240
idea of trying to discretize a continuous dynamical system, but the end result is just

22:16.240 --> 22:20.920
a linear RNN with a slightly different initialization scheme.

22:20.920 --> 22:26.680
The most common form of state space model parameterizes each recurrent weight as w equals e to the

22:26.680 --> 22:32.480
delta times a plus bi, where delta is again a learnable parameter, which is initialized

22:32.480 --> 22:38.880
to a very small number, usually between 0.0001 and 0.1.

22:38.880 --> 22:43.080
Multiplying the weight by a small number makes it close to zero, and when you take e to the

22:43.080 --> 22:46.840
power of something close to zero, you get something close to one.

22:46.840 --> 22:50.840
This again ensures that at initialization, the recurrent weights are all approximately

22:50.840 --> 22:53.880
one, so training is stable.

22:53.880 --> 23:00.000
State space models also multiply the inputs by delta times a plus bi inverse times w minus

23:00.000 --> 23:04.720
one, because that's what's prescribed by a control theory, but empirically, you get

23:04.720 --> 23:11.440
the same performance when you just scale the inputs by delta as in the linear RNN setup.

23:11.440 --> 23:16.080
From the long range arena, the control theory inspired state space initialization performs

23:16.080 --> 23:19.360
roughly the same as the linear RNN initialization.

23:19.360 --> 23:24.480
Anyway, whenever you hear state space model, think linear RNN.

23:24.480 --> 23:26.880
And finally, we can talk about Mamba.

23:26.880 --> 23:32.200
You see, while linear RNNs do perform really well on the long range arena benchmark, this

23:32.200 --> 23:35.200
does not mean they are good language models.

23:35.200 --> 23:40.400
For language modeling, linear RNNs perform way worse than transformers.

23:40.520 --> 23:46.160
Whether the performance of various state-of-the-art language models lower is better on this graph.

23:46.160 --> 23:51.200
As you can see, everything including state space models does significantly worse than

23:51.200 --> 23:53.120
transformers.

23:53.120 --> 23:58.120
The reason for this, as identified in the Mamba paper, is that linear RNNs are incapable

23:58.120 --> 24:02.760
of selectively forgetting information from the output vector.

24:02.760 --> 24:07.120
If the weights are close to zero, then the output vector will be set to zero after every

24:07.200 --> 24:08.200
input.

24:08.200 --> 24:13.400
Effectively, the model will always immediately forget whatever came before the current input.

24:13.400 --> 24:17.240
If their current weights are close to one, then the output vector doesn't change when

24:17.240 --> 24:21.520
it's multiplied with the weights, so the output vector will accumulate information

24:21.520 --> 24:24.680
from all inputs observed.

24:24.680 --> 24:29.480
What you want is for the model to be able to decide when to store information and when

24:29.480 --> 24:34.360
to forget information based on what input it sees.

24:34.360 --> 24:40.000
Mamba proposes an elegant solution, instead of using the same weights in each step, use

24:40.000 --> 24:44.280
different weights which depend on the input.

24:44.280 --> 24:49.120
Mamba applies a linear function to each input vector to generate a separate weight vector

24:49.120 --> 24:52.640
for that input.

24:52.640 --> 24:57.520
Then the recurrent scan is performed using these generated weights.

24:57.520 --> 25:02.440
This way, certain inputs can generate weights close to zero and thereby erase information

25:02.440 --> 25:07.920
from the output vector, but other inputs can generate weights close to one, thereby leaving

25:07.920 --> 25:11.880
the output vector unchanged.

25:11.880 --> 25:16.600
And I also suspect that using different weights at each step helps with vanishing and exploiting

25:16.600 --> 25:22.120
gradients, since there should now be many different stable configurations, like in feed-forward

25:22.120 --> 25:27.840
networks, although this wasn't mentioned in the Mamba paper.

25:27.840 --> 25:33.560
Mamba also uses one more trick, which is to increase the size of the output vectors.

25:33.560 --> 25:39.040
In a standard RNN, the output vectors are the same size as the input vectors.

25:39.040 --> 25:43.800
Mamba expands the size of the output vectors by a factor of 16.

25:43.800 --> 25:48.840
This allows it to store much more information from previous inputs.

25:48.840 --> 25:53.400
The output vectors are then projected back down to the original size before being passed

25:53.400 --> 25:57.360
to the next layer.

25:57.680 --> 26:02.560
This would increase the computation time by a factor of 16, but it turns out that the

26:02.560 --> 26:07.880
major bottleneck of a Mamba layer on modern GPUs is the time it takes to read and write

26:07.880 --> 26:10.760
data into high-performance memory.

26:10.760 --> 26:14.880
You see, modern GPUs actually have two different types of memory.

26:14.880 --> 26:19.120
Data is stored in main memory, but in order to do computations, data first needs to be

26:19.120 --> 26:23.480
transferred into high-performance memory.

26:23.480 --> 26:28.520
For the Mamba recurrence operation, it turns out that the time taken to transfer data is

26:28.520 --> 26:34.280
actually much larger than the time it takes to do the computation itself.

26:34.280 --> 26:39.360
Mamba therefore transfers the input vectors and model parameters to high-performance memory,

26:39.360 --> 26:44.600
and then computes the whole Mamba operation in a single block, including projecting outputs

26:44.600 --> 26:51.080
back down to the smaller original size, before writing the results back to main memory.

26:51.080 --> 26:57.400
This way, you only transfer vectors of the original size to and from high-performance memory,

26:57.400 --> 27:01.480
so the transfer time is unchanged.

27:01.480 --> 27:06.840
The actual computation time is 16 times slower, but the computation time was so small compared

27:06.840 --> 27:10.560
to the transfer time that it doesn't really affect the overall time taken.

27:10.560 --> 27:15.400
You essentially get to use 16 times larger vectors for free.

27:15.400 --> 27:16.760
And there we have it.

27:16.760 --> 27:22.520
This, along with the few minor architectural modifications, is Mamba, the dynamic linear

27:22.520 --> 27:27.880
recurrent neural network, which performs better than transformers at language modeling while

27:27.880 --> 27:33.640
using only nlogn compute down from n squared.

27:33.640 --> 27:40.720
Okay, now that we've made it through all of those boring technical details, we can finally

27:40.720 --> 27:43.240
talk about what really matters.

27:43.240 --> 27:49.160
The Mamba Drama

27:49.160 --> 27:53.520
You see, the Mamba paper caused quite a bit of controversy in the machine learning community

27:53.520 --> 27:55.320
this year.

27:55.320 --> 28:00.760
The Mamba paper was submitted to ICLR 2024, which is one of the most prestigious machine

28:00.760 --> 28:07.760
learning conferences in the world, and in January it was rejected by peer reviewers.

28:07.760 --> 28:09.120
But so what?

28:09.120 --> 28:12.920
Papers get rejected from top conferences all the time, right?

28:12.920 --> 28:18.600
Well, to give some context, the Mamba preprint has been publicly available since last year,

28:18.600 --> 28:23.800
and during this time, several different groups have re-implemented Mamba and all successfully

28:23.800 --> 28:29.080
reproduced the results claimed in the Mamba paper, namely that Mamba performs better than

28:29.080 --> 28:32.840
transformers and uses less computation.

28:32.840 --> 28:37.960
And since transformers are all anyone has talked about for the last five years, that's

28:37.960 --> 28:40.040
kind of a big deal.

28:40.040 --> 28:45.240
Because of this, everyone in the community was expecting the Mamba paper to be accepted,

28:45.240 --> 28:50.600
if not win a best paper award.

28:50.600 --> 28:56.160
So then, if the Mamba architecture really works, what glaring flaws are in the paper

28:56.160 --> 28:58.600
that caused it to be rejected?

28:58.600 --> 29:07.520
Well, the ICLR peer review is publicly available for everyone to view, so let's take a look.

29:07.520 --> 29:13.200
According to the meta review, Mamba wasn't tested on the Long Range Arena benchmark.

29:13.200 --> 29:18.480
Remember that benchmark I talked about, where linear RNNs performed way better than transformers?

29:18.480 --> 29:23.400
This reviewer wanted to see how well Mamba performed on that task.

29:23.400 --> 29:28.560
Now this is a really dumb reason to reject a paper, because the Long Range Arena is a

29:28.560 --> 29:34.920
completely different task to language modelling, and Mamba is specifically a language model.

29:34.920 --> 29:40.040
Keep in mind, transformers perform way worse than linear RNNs on the Long Range Arena,

29:40.040 --> 29:43.360
but transformers are still way better language models.

29:43.360 --> 29:47.640
So performance on the Long Range Arena is in no way indicative of language modelling

29:47.640 --> 29:49.640
ability.

29:49.640 --> 29:52.200
Mamba sets a new state of the art for language modelling.

29:52.200 --> 29:59.040
It shouldn't be rejected because it doesn't also solve some other unrelated task.

29:59.040 --> 30:03.840
The only other major criticism in the meta review is that Mamba was only evaluated on

30:03.920 --> 30:10.240
language modelling, that is the accuracy when predicting the next word in a piece of text.

30:10.240 --> 30:15.320
The reviewers argue that this metric isn't indicative of a language model's utility,

30:15.320 --> 30:20.440
and instead Mamba should have been evaluated on downstream tasks that measure a model's

30:20.440 --> 30:22.720
reasoning ability.

30:22.720 --> 30:26.560
Except that this is exactly what they did in the Mamba paper.

30:26.560 --> 30:31.240
They pre-trained Mamba as a language model, and then performed zero shot prompting on

30:31.240 --> 30:37.280
a bunch of standard downstream benchmark tasks, and surprise, surprise, Mamba outperforms

30:37.280 --> 30:40.400
all other language models.

30:40.400 --> 30:43.800
As a bonus, another reviewer said, and I quote,

30:43.800 --> 30:50.360
Mamba has a quadratic memory requirement during training, just like transformers.

30:50.360 --> 30:53.640
Which is just not true.

30:53.640 --> 30:58.680
Neither Mamba nor transformers have quadratic memory costs.

30:58.680 --> 31:03.640
Mambas have a quadratic compute cost, but their memory cost is linear.

31:03.640 --> 31:04.640
So is Mambas.

31:04.640 --> 31:08.960
I'm not sure how it's even possible to come to the conclusion that Mamba has a quadratic

31:08.960 --> 31:13.160
memory cost if you understand how it works at all.

31:13.160 --> 31:18.000
So as you can imagine, this less than ideal peer review sparked some debate in the machine

31:18.000 --> 31:22.280
learning community about peer reviewing practices and whether or not Mamba should have been

31:22.280 --> 31:23.280
rejected.

31:23.280 --> 31:26.240
You can probably guess which side of the debate I fall on.

31:26.280 --> 31:31.520
But let me know your thoughts about how broken academic peer review is in the comments below.

31:31.520 --> 31:35.240
Or thoughts about the actual Mamba architecture itself, I guess that's fine too.

