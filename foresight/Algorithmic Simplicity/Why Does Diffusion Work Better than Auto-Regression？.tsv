start	end	text
0	3680	This is an artificial intelligence image generator.
3680	10640	Given a text description of a picture, it will create, out of nothing, an image matching that description.
10640	16480	As you can see, it is capable of generating high quality images of all kinds of different scenes.
16480	18240	And it's not just images.
18240	23840	In recent years, generative AI models have been developed that can generate text,
24000	29600	audio, code, and soon videos too.
31520	36960	All of these models are based on the same underlying technology, namely deep neural networks.
38080	43120	In a few of my previous videos, I've explained how and why deep neural networks work so well,
43680	47520	but I only explained how neural nets can solve prediction tasks.
48240	54000	In a prediction task, the neural net is trained with a bunch of examples of inputs and their labels,
54000	58480	and tries to predict what the label will be for a new input which it hasn't seen before.
59360	65120	For example, if you trained a neural net on images labeled with the type of object appearing in each
65120	70640	image, that neural net would learn to predict which object a human would say is in an image,
70640	73120	even for new images which it hasn't seen before.
73440	79520	Under the hood, the way that prediction tasks are solved is by converting the training dataset
79520	84000	into a set of points in a space, and then fitting a curve through those points.
84640	88080	So prediction tasks are also known as curve fitting tasks.
89360	94160	And while prediction is certainly cool and very useful, it's not generation, right?
94880	98400	This model is just fitting a curve to a set of points.
98400	100000	It can't produce new images.
100800	105040	So where does the creativity of these generative models come from,
105040	107120	if neural nets can only do curve fitting?
108880	112560	Well, all of these generative models are in fact just predictors.
113360	118720	Yep, it turns out that the process of producing novel works of art can be reduced to a curve
118720	122800	fitting exercise. And in this video, you'll learn exactly how.
124960	129120	Suppose that we have a training dataset consisting of a bunch of images.
129120	134240	We want to train a neural net to create new images which are similar in style to these
134240	140320	training images. The first thing you might try is to simply use the images as labels
140320	146000	to train the predictor. Here, we don't care about the mapping from inputs to outputs,
146000	150960	so we can just use anything we like for the inputs, for example, a completely black image.
151520	155680	Predictors learn to map inputs to outputs according to their training data.
155680	161040	So this predictor, once trained, should be able to map the dummy or black image to
161040	164240	new images like those seen in the training set, right?
168400	172640	Uh, okay, maybe not quite. That didn't work so well.
172640	176880	Instead of producing a nice beautiful picture, we just got this blurry mess.
178000	181360	This demonstrates a very important fact about predictors.
181360	186480	If there are multiple possible labels for the same input, the predictor will learn to output
186480	192720	the average of those labels. For traditional classification tasks, this isn't really a
192720	197280	problem because the average of multiple class labels can still be a meaningful label.
198720	202800	For example, this image could plausibly be given two different labels.
202800	208800	Both cat and dog would be valid labels. In this case, a classifier would learn to output the
208800	214640	average of those labels, which means you end up with the score of 0.5 cat and 0.5 dog,
215280	221280	which is still a useful label. In fact, it's arguably a better label than either of the original
221280	227200	ones. On the other hand, when you average a bunch of images together, you do not get a
227200	234320	meaningful image out, you just get a blurry mess. Let's try something a bit easier this time.
234320	239520	How about instead of generating a new image from scratch, we try to complete an image which has
239520	245120	part of it missing. In fact, let's make this really easy and suppose there is only one missing
245120	251360	pixel, say the bottom right pixel. Can we train a neural net to predict the value of this one
251360	257680	missing pixel? Well, as before, the neural net is going to output the average of plausible values
257680	262800	that the missing pixel can take. But since it's only one pixel that we're predicting,
262800	268400	the average value is still meaningful. The average of a bunch of colors is just another color.
268400	277440	There's no blurring effect. So this model works perfectly fine. And we can use the value predicted
277440	281360	by this neural net to complete images which are missing the bottom right pixel.
283840	288400	Great, so we can complete images with one missing pixel. What about two?
288800	294240	Well, we can do the same thing again. Train another neural net on images with two missing pixels,
294240	299440	using the value of the second missing pixel as the label, and then use this neural net to fill
299440	307440	in the second missing pixel. Now we have an image with just one missing pixel, and so we can use
307440	316480	the first neural net to fill in that. Great. And we can do this for every pixel in the image.
317440	323040	Train a neural net to predict the color of that pixel when it and all of the subsequent pixels
323040	329520	are missing. Now we can complete an image starting from a fully black image and filling in one pixel
329520	335920	at a time. Crucially, each neural net only predicts one pixel, and so there's no blurring effect.
338000	342400	And there we have it. We have just generated a plausible image out of nothing.
343120	348080	There's just one small problem. If we run this model again, it will generate exactly the same
348080	354640	image. Not very creative, is it? But not to worry, we can fix this by introducing a bit of random
354640	360800	sampling. You see, all predictors actually output a probability distribution over possible labels.
362480	367120	Usually, we just take the label with the largest probability as the predicted value.
367920	374400	But if we want diversity in our outputs, we can instead randomly sample a value from this
374400	380240	probability distribution. This way, each time the model is run, it will sample different values
380240	385200	at each step, which therefore changes the prediction for subsequent steps, and we get a
385200	390640	completely different image each time. Now we have an interesting image generator.
391120	396560	But still, at the end of the day, this model is made of predictors. They take as input a
396560	402160	partially masked image, and predict the value of the next pixel. The only difference between this
402160	407920	and a traditional image classifier is the label we used for training. The labels for our generator
407920	413520	happen to be pixel colors, which come from the original image itself, and not a human labeler.
413520	418800	This is a very important point in practice. It means we don't need humans to manually label
418880	424480	images for this model. We can just scrape unlabeled images off the internet. But from the point of
424480	429680	view of the neural net, it doesn't know, nor does it care, that the label came from the original
429680	435200	image. As far as it's concerned, this is just a curve fitting exercise like any other.
436400	441920	The generative model we've just created is called an autoregressor. We have a removal process,
441920	447680	which removes pixels one at a time, and we train neural nets to undo this process. Generative
448400	450960	generating and adding back in pixels one at a time.
452880	457840	This is actually one of the oldest generative models. The very earliest use of autoregression
457840	464000	dates back to 1927, where it was used to model the timing of sunspots. But autoregressors are
464000	472160	still in use today. Most notably, ChatGPT is an autoregressor. ChatGPT generates text by using
472160	477520	a transformer classifier to output a probability distribution over possible next words, given
477520	485040	a partial piece of text. However, autoregressors are not used to generate images anymore. And the
485040	490160	reason is that, while they can generate very realistic images, they take too long to run.
491440	496720	In order to generate a sample with an autoregressor, we need to evaluate a neural net once for every
496720	502720	element. This is fine for generating a few thousand words to make a piece of text, but large images
502800	508320	can have tens of millions of pixels. How can we get away with fewer neural net evaluations?
513600	519520	For our autoregressor, we removed pixels one at a time. But we don't have to remove only one
519520	526320	pixel. We could, for example, remove a 4x4 patch of pixels at a time, and train the neural net to
526320	532400	predict all 16 missing pixels at once. This way, when we use our model to generate an image,
532400	538320	it can produce 16 pixels per evaluation, and so generation is 16 times as fast.
541280	547200	But there is a limit to this. We can't generate too many pixels at the same time. In the extreme
547200	552720	case, if we try to generate every pixel in the image at once, then we're back to the original
552720	557840	problem. There are many possible labels that get averaged together into a blurry mess.
558000	563120	To be clear, the reason why the image quality degrades is that, when we predict a bunch of pixels
563120	569280	at the same time, the model has to decide on the values for all of them at once. There are lots
569280	575040	of plausible ways that this missing patch could be filled in, and so the model outputs the average
575040	581360	of those. The model isn't able to make sure that the generated values are consistent with each other.
581520	586400	In contrast, when we predict one pixel at a time, the model gets to see the previously
586400	591840	generated pixels, and so the model can change its prediction for this pixel to make it consistent
591840	596960	with what has already been generated. This is why there's a trade-off. The more pixels we
596960	602320	generate at once, the less computation we need to use, but the worse the quality of the generated
602320	608400	images will be. Although this problem only arises in the end of the video, it's still
608800	614160	although this problem only arises if the values we are predicting are related to each other.
614720	620160	Suppose that the values were statistically independent of each other, that is, knowing
620160	626160	one of them does not help to predict any others. In this case, the model doesn't need to look at
626160	631200	the previously generated values since knowing what they were wouldn't change its prediction for the
631200	637600	next value anyway. In this case, you can predict all of them at the same time without any loss in
637600	645120	quality. So that means, ideally, we want our model to generate a set of pixels that are unrelated
645120	651760	to each other. For natural images, nearby pixels are the most strongly related because they are
651760	657840	usually part of the same object. Knowing the value of one pixel very often gives you a good idea of
657840	663680	what color nearby pixels will be. This means that removing pixels in contiguous chunks is
663680	670000	actually the worst way to do it. Instead, we should be removing pixels that are far away from each
670000	677600	other and hence more likely to be unrelated. So, if in each step we remove a random set of pixels
677600	683280	and predict values for those, then we can remove more pixels in each step for the same loss in
683280	689520	image quality compared to contiguous chunks. In order to minimize the number of steps needed
689520	694320	for generation, we want the pixels we remove in each step to be as spread out as possible.
694960	700480	Removing pixels in a random order is a pretty good way of maximizing the average spread,
700480	707840	but there is an even better way. We can think of our generative model as two processes. A removal
707840	713520	process that gradually removes information from the input until nothing is left, and a generation
713520	720000	process that uses neural nets to undo the removal process, generating and adding back in information.
721280	727280	So far, we have been completely removing pixels, but rather than completely removing a pixel,
727280	733040	we could instead remove only some of the information from a pixel by, for example,
733040	739520	adding a small amount of random noise to it. This means we don't know exactly what the original
739520	744000	pixel value was, but we do know it was somewhere close to the noisy value.
745840	751280	Now, instead of removing a bunch of pixels in each step, we can add noise to the entire image.
752080	757600	This way, we can remove information from every pixel in the image in a single step,
757600	762960	which is the most spread out way of removing information. And since it's more spread out,
762960	767760	you can remove more information in each step for the same loss in generation quality.
770000	774240	There is one small problem with this, though. When we want to generate a new image,
774240	778080	we need to start the neural net off with some initial blank image.
778960	784720	When we were removing pixels, then every image eventually ends up as a completely black image.
784720	787840	So of course, that's where we start the generation process from.
788800	793200	But now that we're adding noise, the values just keep getting larger and larger,
793200	798000	never converging to anything. So where do we start the generation process from?
798640	802240	We can avoid this problem by changing our nosing step slightly,
802240	806240	so that we first scale down the original value and then add the noise.
807120	810800	This ensures that when we repeat this nosing step many times,
810800	815680	information from the original image will disappear, and the result will be equivalent
815680	821040	to a pure random sample from the noise distribution. So we can start our generation
821040	827600	process from any such noise sample. And there we have it. This is known as a denoising diffusion
827600	833520	model. The overall form is identical to an autoregressor. The only difference is the way
833520	839040	in which we remove information at each step. By adding noise, we can spread out the removal
839040	844000	of information all across the image, which makes the predicted values as independent
844000	848320	of each other as possible, allowing you to use fewer neural net evaluations.
849440	854160	Empirically, diffusion rules can produce high-quality photorealistic images in about
854160	857440	100 steps, whereas autoregressors would take millions.
860000	864080	Now that we understand how these generative models work at a conceptual level, if you are
864080	869040	ever going to implement these models in practice, there are a few important technical details that
869040	875360	you should be aware of. First, in the procedure I described for autoregression, I use the different
875360	880960	neural net in each step of the process. This is certainly the best way to get the most accurate
880960	886000	predictions, but it's also very inefficient, since we need to train a whole bunch of different
886000	893920	neural nets. In practice, you would just use the same model to do every step. This gives slightly
893920	899600	worse predictions, but the savings and computation time more than make up for it. In order to train
899600	904960	a single neural net to perform all of the generation steps, you would remove a random number of pixels
904960	910240	from each input and train the neural net to predict the corresponding next pixel of each input.
911280	916400	Additionally, you can also give the number of pixels removed as an input to the neural net
916400	919520	so that it knows which pixel it's supposed to be generating.
920640	924000	Now this one neural net can be used for all generation steps.
925440	929840	In the setup I just described, for each training image, the neural net is trained on
929840	936240	only one generation step for that image. But ideally, we would like to train it on every
936240	941120	generation step of every image. We can get more use out of our training data that way.
942160	947360	If you did this the naive way, you would have to evaluate the neural net once for every generation
947360	952960	step, which means a lot more computation. Fortunately, there exist special neural net
952960	958160	architectures known as causal architectures that allow you to train on all of these generation
958160	964800	steps while only evaluating the neural net once. There exist causal versions of all of the popular
964800	970000	neural net architectures such as causal convolutional neural nets and causal transformers.
970720	975120	Causal architectures actually give slightly worse predictions, but in practice,
975120	980240	autoregression is almost always done with causal architectures, because the training is so much
980240	985760	faster. The generation process for causal architectures is still exactly the same though.
986560	991440	For diffusion models, you can't use causal architectures, and so you do have to just
991440	994480	train with each data point at a random generation step.
997360	1003120	I described the diffusion model as predicting the slightly less noisy image from the previous step.
1003120	1008160	However, it's actually better to predict the original completely clean image at every step.
1009040	1014960	The reason for this is it makes the job of the neural net easier. If you make it predict the
1014960	1020480	noisy next step image, then the neural net needs to learn how to generate images at all different
1020480	1026160	noise levels. This means the model will waste some of its capacity learning to produce noisy
1026160	1032400	versions of images. If you instead just have the neural net always predict the clean image,
1032400	1036960	then the model only needs to learn how to generate clean images, which is all we care about.
1037840	1043040	You can then take the predicted clean image and reapply the nosing process to it to get
1043040	1049520	the next step of the generation process. Except that when you predict the clean image,
1049520	1055440	then at the early steps of the generation process, the model only has pure noise as input,
1055440	1061680	so the original clean image could have been anything. And so you get a blurry mess again.
1063200	1068880	To avoid this, we can train the neural net to predict the noise which was added to the image.
1068880	1073360	Once we have a predicted value for the noise, we can plug it into this equation to get a
1073360	1079120	prediction for the original clean image. So we are still predicting the original clean image
1079120	1084800	just in a roundabout way. The advantage of doing it this way is that now the model output is
1084800	1090400	uncertain at the latest stages of the generation process, since any noise could have been added
1090400	1095680	to the clean image, so the model outputs the average of a bunch of different noise samples,
1095680	1102640	which is still valid noise. So far we've just been generating images from nothing,
1102640	1107680	but most image generators actually allow you to provide a text prompt describing the image you
1107760	1113120	want to make. The way this works is exactly the same, you just give the neural net the text as
1113120	1118560	an additional input at each step. These models are trained on pairs of images and their corresponding
1118560	1125040	text descriptions, usually scraped from image alt text tags found on the internet. This ensures
1125040	1129920	that the generated image is something for which the text prompt could plausibly have been given
1129920	1135840	as a description of that image. In principle, you can condition generative models on anything,
1135840	1140960	not just text, so long as you can find appropriate training data. For example,
1140960	1143680	here is a generative model that is conditioned on sketches.
1148560	1152400	Finally, there's a technique to make conditional diffusion models work better,
1152400	1158240	called classifier free guidance. For this, during training, the model will sometimes be given the
1158240	1164240	text prompts as additional input, and sometimes it won't. This way, the same model learns to do
1164240	1170000	predictions with or without the conditioning prompt as input. Then at each step of the denoising
1170000	1175840	process, the model is run twice, once with the prompt and once without. The prediction without
1175840	1181040	the prompt is subtracted from the prediction with the prompt, which removes details that are generated
1181040	1186480	without the prompt, leaving only the details that came from the prompt, leading to generations which
1186480	1192800	more closely flow the prompt. In conclusion, generative AI, like all machine learning,
1192800	1198640	is just curve fitting. And that's all for this video. If you enjoyed it, please like and subscribe,
1198640	1203040	and if you have any suggestions for topics you'd like to see me cover in a future video,
1203040	1208720	leave a comment below.
