{"text": " Mamba is a new neural net architecture that is better than Transformers at language modelling. Yes, that's right, after reigning supreme for 7 years, the Transformer has finally been dethroned. Well, maybe. So far, Mamba has only been tested at small model sizes, up to a few billion parameters, but the results so far are promising. In addition, Mamba uses less compute than Transformers. For an input sequence of n words, Mamba only uses n log n compute, whereas Transformers use n squared. So Mamba-based language models should allow for much greater context sizes to be used. In this video, we're going to do a deep dive of the Mamba architecture. What is it? Why does it work so well? And how could you have gone about designing such an architecture yourself? Unfortunately Mamba is presented as an extension of something called a state space model. State space models are another type of sequence model that have been steadily gaining popularity over the past few years, but to be honest, the theory behind state space models is massively over complicated and uses some pretty advanced mathematics. Fortunately, Mamba can also be understood as an extension of recurrent neural networks, or RNNs for short, which are much easier to understand. So in this video, we will be taking the RNN path to understanding Mamba. Now let's get started. What is a recurrent neural network? Given a sequence of input vectors, a convolutional layer applies a neural net to consecutive groups of vectors. The key thing is that the neural net only sees a small number of vectors at a time, which makes the model easy to train. The downside is that information from vectors which are far away can't be combined until many convolutional layers have been applied. This makes it difficult for convolutional neural nets to understand long range dependencies in their input, and such long range dependencies occur all the time in natural language text. To remedy this flaw, the transformer architecture was invented, which successfully allows a single layer to combine information from vectors no matter how far away they are. I previously made a video explaining how and why transformers work in detail, which you can find here. And while transformers work great, they have a significant limitation, which is that the amount of compute they use is quadratic in the input length. This isn't a huge deal for small inputs, but if you want to have a million vectors in the input, that means you need to do a million times a million operations, which is a lot. Recurrent neural nets take a completely different approach to improving convolutional layers. The idea is very simple. Instead of applying the neural net to two consecutive input vectors, you apply it to one input vector and the previous output of the neural net. This seems like a small change, but it has profound consequences. Each output vector now contains information from all of the input vectors prior to it, instead of only one previous vector. This final output vector contains information from every vector in the input, no matter how many there are. And we have not used any more compute than a convolutional layer. We've managed to incorporate long range information for free. This is exactly what we want, or at least it would be if it weren't for two small problems with RNNs, which make them almost impossible to use in practice. The first problem is that while a recurrent layer uses the same amount of compute as a convolutional layer, that compute cannot be parallelized across multiple processes. Even if you have lots of processes available, you can't begin evaluating the neural net on an input until all of the previous steps have finished, because you need to feed the output from the previous step into the neural net. Compare this to a convolutional layer where the neural net only needs to see the original input. You can run the neural net on all inputs at the same time, so long as you have enough processes available. And since modern hardware such as GPUs are highly specialized for parallel computation with thousands of processes, RNNs are actually a lot slower than CNNs in practice. In fact, RNNs are even slower than transformers despite doing less computation. And the second problem is that RNNs are incredibly difficult to train. While in theory a single recurrent layer can incorporate information from arbitrarily many inputs, in practice they don't. Instead, they only learn to incorporate information from the previous few dozen inputs at most. The idea for RNNs has been around since the 1980s, but because of these two problems, RNNs have fallen out of favor with convolutional neural nets and transformers being much more successful in practice. In fact, RNNs have hardly been used at all in the past decade. Well, last year a new paper was published showing that linear RNNs can avoid both of these problems, and therefore linear RNNs are highly effective long sequence models. So what is a linear recurrent neural network? Well, you simply replace the neural net with a linear function. This might seem like a bad idea, since linear functions can only perform relatively simple transformations of their inputs, but we can make up for it by applying a full neural net to each output vector afterwards. This is similar to how in transformers you can replace the value neural nets with simple linear functions, and then add neural nets in between self-attention layers to make up for the lack of non-linear processing power. So just like in a transformer, we will alternate linear recurrent layers with element-wise neural networks. But importantly, by making the recurrent operation purely linear, it becomes possible to solve both of the RNN problems. To start with, I'll explain how a linear recurrence applied to n vectors can be computed in parallel in just log n time. And then I'll explain how the training issues that plague regular RNNs can be fixed in linear recurrences. The linear recurrence operator is given by this formula. To get the ith output vector, you multiply the previous i-1th output vector with a matrix w-y, and add the ith input vector multiplied by a different matrix w-x. The entries in the w matrices are the parameters which will be learned by the model, so they start off as random samples from a normal distribution centered at zero, and are then updated with gradient descent. And since the w-x matrix is just applied to each input independently, we can actually just think of it as being part of the previous layer. So we can simplify our recurrence operator to just add the input x, assuming that a linear function has already been applied to the input in the previous layer. A linear recurrence is actually a special case of a more general operation called the scan, so let's start with the simplest example of a scan, a cumulative sum. Given a list of n numbers as input, the goal is to compute the list of partial sums up to each term, so the ith item in the output list should be the sum of the first i-items of the input list. While it is trivial to compute this by simply adding the numbers together one at a time, we want to do it in parallel. And it turns out we can do so as follows. First add together each consecutive pair of numbers, then from the resulting list add together pairs of numbers which are two steps apart, then four steps apart, and eight, and so on. Each iteration doubling the step size, until the step size is as large as the entire input list, which will be after log n steps. This algorithm works because at each iteration the ith output element contains the sum of the previous step size numbers. For example, in the first iteration each output number is the sum of the previous two terms. In the next iteration each item contains the sum of the previous two terms plus the previous two terms starting two away, that is the sum of the previous four terms, and so on. When the step size is the size of the input, each output contains the sum of all previous terms as desired. It's trivial to see that each iteration can be computed in parallel, however the different iterations do still need to be computed sequentially, and there are log n iterations. So if you have n processors, the total runtime of this algorithm is log n, down from n of the naive sequential version. And this same algorithm works for computing lists of cumulative applications of any binary operator, not just addition, so long as the binary operator is associative. Associative means that you can change the order of application and you'll still end up with the same result. This is true of addition, which is why our parallel cumulative sum algorithm works. And it's also true of a bunch of other operations. In particular, this binary operator is associative. Note that this operator uses a pair of a matrix and a vector as input and output, instead of just a single number, like with addition. And remarkably, performing a scan with this operator is equivalent to a linear recurrence. We first need to replace our input list of vectors with a list of pairs where the first element is the recurrent weight matrix and the second element is the input vector, but then we just perform the scan as usual. You can check for yourself that this operator is in fact associative by expanding a few terms in the other order. To summarize, we just need to do our parallel cumulative sum algorithm with this operator in place of addition. And we get the result of a linear recurrent layer in just log n time. Except for one small problem. If you look closely at this operation, the way it works is by using the first element of the tuples as a cumulative matrix, which contains the product of all of the matrices seen so far. That's why the first element of the output tuple is the product of the two input matrices. But this means we're performing a d by d times d by d matrix multiplication in every step, where d is the dimension of the vectors. This is really slow. Note that in the original sequential RNN, we didn't need to keep track of this cumulative matrix, and so we only ever multiply the weight matrix with a length d input vector at each step, which is a d squared operation. But now we have to do a d cubed operation in every step. For standard model sizes, this is easily a thousand fault increase in computation. And that's bad. Fortunately there is a way around this. Matrix diagonalization. You see, almost every square matrix can be factored into the product of an invertible matrix P, a diagonal matrix D, and that first matrix P inverse, so long as the matrix elements are allowed to be complex numbers. Here is an example. Note that this middle matrix is diagonal, that is, all elements except for the main diagonal are zero. What's neat about this is when you multiply the matrix by itself in this form, the inner P inverse and P terms cancel, and the product of two diagonal matrices is just the diagonal matrix with the product of elements. That is, in order to compute d squared, all you need to do is square the elements on the main diagonal of d, which can be done in just m operations instead of m cubed. Much better. So then what we can do is represent the recurrent weight matrix in diagonalized form, which means we only need to use a complex vector which contains the elements of the main diagonal of d. That is to say, we first apply a complex matrix P to the input vectors, then perform the linear recurrence with a complex weight vector w, using element-wise multiplication, and finally apply P inverse to the output. The result of this will then be equivalent to a linear recurrence for some real-valued matrix w. But when computed this way, the recurrence operator only needs to compute element-wise multiplication between two vectors to update the cumulative weights instead of matrix multiplication. When we plug this operator into our parallel scan algorithm, the total compute is now adjust dn log n, and the parallel runtime is log n. Much better. Note that the parameters of this layer are the complex entries in the recurrent weight vector w and the matrix P. In practice, you would just use two separate real numbers to represent the real and imaginary components of each parameter, which are initialized by sampling from a normal distribution centered at zero, and updated with gradient descent as usual. Computing matrix inverses is really slow, so in practice we don't bother, and instead just use two independent complex matrices before and after the linear recurrence. This actually makes the model more expressive than a real-valued linear RNN, and it saves computation, but it does mean that the model is no longer equivalent to a real-valued recurrence, and the output can now be a complex number. So we will need to take the real-valued part of the output before passing it to the next layer. Okay, we have seen how to make linear RNNs fast for modern hardware, but what about the other problem? That RNNs are very difficult to train. Before we solve this problem, here is a quick recap of why training RNNs is so problematic in the first place. Neural nets are trained by subtracting the gradient of the loss function from each weight in the model. What is the gradient? Well, imagine evaluating the neural net, then increasing the value of a weight by a very small amount, and then evaluating it again. The difference in these scores is proportional to the gradient for that weight, and it tells you how to change the weight to make the neural net better, so let's evaluate the gradient of a linear recurrent layer. Actually, to make this a bit easier, let's simplify the model and suppose that every input after the first is zero, so we can just ignore them. When we evaluate the recurrent layer, at each step the previous output is multiplied by the weight vector, so after n steps, the output vector is equal to the recurrent weight vector to the power of n times the first vector, x1. When we increase the weight by a small amount and evaluate it again, we get this. Taking the difference, we get up to a constant scaling factor, w to the power of n-1 times x1. The problem here is that as n becomes large, this term, w to the n-1, either gets very small or very large, depending on whether the values in w are less than or greater than 1. In either case, it's a problem. If the gradient is very large, then the neural net weights change too much, and the existing functionality already learned by the neural net gets destroyed. If the gradient is very small, then the weights don't change enough, and the neural net doesn't learn anything at all. This is what makes training RNNs difficult. While in principle, RNNs can use infinitely long context, in practice, with gradient-based training techniques, the RNN will only learn to use context for as many steps as the gradient remains the right size for learning. This is known as the problem of vanishing and exploding gradients. And when we add back in non-zero inputs, this problem only gets worse, as the additional inputs make the gradients even more unstable. And to be clear, the reason why this isn't a problem for regular neural nets is because they use different weights in each layer. Some layers can have weights smaller than one, and some layers can have weights larger than one, so long as the gradient remains about the same size, the neural net will be able to learn. There are lots and lots of different configurations of weights that result in stable gradients, and it's easy to stay in stable configurations all throughout training. But for RNNs, you're using the same weight in each step, so there is exactly one stable configuration, which is when the weight is one. Any deviation from one, and you have exponentially growing or decaying gradients. Note that when the weights are complex numbers, the same argument applies just using the absolute value of the weights. So how can we fix vanishing and exploding gradients? Well, we saw that the RNN gradients are stable, so long as their current weights are one, and the inputs are zero. So in the linear RNN paper, the authors propose to initialize their linear RNNs in this stable state. Specifically, they parameterize the weights in complex polar form A times E to the IB, where A is magnitude and B is angle. They then restrict the magnitude to be less than one by running A through this E to the minus E function, which always outputs a number between zero and one. And instead of randomly sampling A from a normal distribution centered at zero, as we usually do, they initialize A so that the magnitude of E to the minus E to the A is uniformly distributed between 0.999 and one. They initialize the angle B uniformly between zero and pi on 10 radians. This ensures that, at initialization, the weights are all very close to one. Finally, they multiply the inputs by delta, which is another learned parameter initialized to the square root of one minus E to the minus E to the A, which since E to the minus E to the A is close to one, this is some very small number. This ensures that, at initialization, the inputs are all close to zero, and so they don't interfere with the recurrence. So at initialization, this model is approximately the same as the stable RNN I showed you before. After the model begins training and the weights change, there's no guarantee that it will remain stable, but in practice it appears that just initializing the model like this is sufficient to allow it to learn to remember context for tens of thousands of steps. And there we have it, with these modifications, we now have a linear RNN that is faster compute and learns to use extremely long context. In the linear RNN paper, they evaluate this model on the long range arena benchmark, which is a collection of six synthetic tasks that evaluate a model's ability to perform long range reasoning. For example, in the path X task, the model must classify images as whether or not they contain a complete dotted path between two circles, except that the images are flattened into one long sequence of 16,000 pixels. The linear RNN achieved state of the art performance on the long range arena, outperforming transformers by about 33% on average across tasks. So now that we understand the linear RNN, what's with all the talk about state space models? Well, it turns out that state space models are just linear RNNs. State space models were inspired by a control theory and were derived from a totally different idea of trying to discretize a continuous dynamical system, but the end result is just a linear RNN with a slightly different initialization scheme. The most common form of state space model parameterizes each recurrent weight as w equals e to the delta times a plus bi, where delta is again a learnable parameter, which is initialized to a very small number, usually between 0.0001 and 0.1. Multiplying the weight by a small number makes it close to zero, and when you take e to the power of something close to zero, you get something close to one. This again ensures that at initialization, the recurrent weights are all approximately one, so training is stable. State space models also multiply the inputs by delta times a plus bi inverse times w minus one, because that's what's prescribed by a control theory, but empirically, you get the same performance when you just scale the inputs by delta as in the linear RNN setup. From the long range arena, the control theory inspired state space initialization performs roughly the same as the linear RNN initialization. Anyway, whenever you hear state space model, think linear RNN. And finally, we can talk about Mamba. You see, while linear RNNs do perform really well on the long range arena benchmark, this does not mean they are good language models. For language modeling, linear RNNs perform way worse than transformers. Whether the performance of various state-of-the-art language models lower is better on this graph. As you can see, everything including state space models does significantly worse than transformers. The reason for this, as identified in the Mamba paper, is that linear RNNs are incapable of selectively forgetting information from the output vector. If the weights are close to zero, then the output vector will be set to zero after every input. Effectively, the model will always immediately forget whatever came before the current input. If their current weights are close to one, then the output vector doesn't change when it's multiplied with the weights, so the output vector will accumulate information from all inputs observed. What you want is for the model to be able to decide when to store information and when to forget information based on what input it sees. Mamba proposes an elegant solution, instead of using the same weights in each step, use different weights which depend on the input. Mamba applies a linear function to each input vector to generate a separate weight vector for that input. Then the recurrent scan is performed using these generated weights. This way, certain inputs can generate weights close to zero and thereby erase information from the output vector, but other inputs can generate weights close to one, thereby leaving the output vector unchanged. And I also suspect that using different weights at each step helps with vanishing and exploiting gradients, since there should now be many different stable configurations, like in feed-forward networks, although this wasn't mentioned in the Mamba paper. Mamba also uses one more trick, which is to increase the size of the output vectors. In a standard RNN, the output vectors are the same size as the input vectors. Mamba expands the size of the output vectors by a factor of 16. This allows it to store much more information from previous inputs. The output vectors are then projected back down to the original size before being passed to the next layer. This would increase the computation time by a factor of 16, but it turns out that the major bottleneck of a Mamba layer on modern GPUs is the time it takes to read and write data into high-performance memory. You see, modern GPUs actually have two different types of memory. Data is stored in main memory, but in order to do computations, data first needs to be transferred into high-performance memory. For the Mamba recurrence operation, it turns out that the time taken to transfer data is actually much larger than the time it takes to do the computation itself. Mamba therefore transfers the input vectors and model parameters to high-performance memory, and then computes the whole Mamba operation in a single block, including projecting outputs back down to the smaller original size, before writing the results back to main memory. This way, you only transfer vectors of the original size to and from high-performance memory, so the transfer time is unchanged. The actual computation time is 16 times slower, but the computation time was so small compared to the transfer time that it doesn't really affect the overall time taken. You essentially get to use 16 times larger vectors for free. And there we have it. This, along with the few minor architectural modifications, is Mamba, the dynamic linear recurrent neural network, which performs better than transformers at language modeling while using only nlogn compute down from n squared. Okay, now that we've made it through all of those boring technical details, we can finally talk about what really matters. The Mamba Drama You see, the Mamba paper caused quite a bit of controversy in the machine learning community this year. The Mamba paper was submitted to ICLR 2024, which is one of the most prestigious machine learning conferences in the world, and in January it was rejected by peer reviewers. But so what? Papers get rejected from top conferences all the time, right? Well, to give some context, the Mamba preprint has been publicly available since last year, and during this time, several different groups have re-implemented Mamba and all successfully reproduced the results claimed in the Mamba paper, namely that Mamba performs better than transformers and uses less computation. And since transformers are all anyone has talked about for the last five years, that's kind of a big deal. Because of this, everyone in the community was expecting the Mamba paper to be accepted, if not win a best paper award. So then, if the Mamba architecture really works, what glaring flaws are in the paper that caused it to be rejected? Well, the ICLR peer review is publicly available for everyone to view, so let's take a look. According to the meta review, Mamba wasn't tested on the Long Range Arena benchmark. Remember that benchmark I talked about, where linear RNNs performed way better than transformers? This reviewer wanted to see how well Mamba performed on that task. Now this is a really dumb reason to reject a paper, because the Long Range Arena is a completely different task to language modelling, and Mamba is specifically a language model. Keep in mind, transformers perform way worse than linear RNNs on the Long Range Arena, but transformers are still way better language models. So performance on the Long Range Arena is in no way indicative of language modelling ability. Mamba sets a new state of the art for language modelling. It shouldn't be rejected because it doesn't also solve some other unrelated task. The only other major criticism in the meta review is that Mamba was only evaluated on language modelling, that is the accuracy when predicting the next word in a piece of text. The reviewers argue that this metric isn't indicative of a language model's utility, and instead Mamba should have been evaluated on downstream tasks that measure a model's reasoning ability. Except that this is exactly what they did in the Mamba paper. They pre-trained Mamba as a language model, and then performed zero shot prompting on a bunch of standard downstream benchmark tasks, and surprise, surprise, Mamba outperforms all other language models. As a bonus, another reviewer said, and I quote, Mamba has a quadratic memory requirement during training, just like transformers. Which is just not true. Neither Mamba nor transformers have quadratic memory costs. Mambas have a quadratic compute cost, but their memory cost is linear. So is Mambas. I'm not sure how it's even possible to come to the conclusion that Mamba has a quadratic memory cost if you understand how it works at all. So as you can imagine, this less than ideal peer review sparked some debate in the machine learning community about peer reviewing practices and whether or not Mamba should have been rejected. You can probably guess which side of the debate I fall on. But let me know your thoughts about how broken academic peer review is in the comments below. Or thoughts about the actual Mamba architecture itself, I guess that's fine too.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.68, "text": " Mamba is a new neural net architecture that is better than Transformers at language modelling.", "tokens": [50364, 376, 23337, 307, 257, 777, 18161, 2533, 9482, 300, 307, 1101, 813, 27938, 433, 412, 2856, 42253, 13, 50648], "temperature": 0.0, "avg_logprob": -0.2364110839500856, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0026720226742327213}, {"id": 1, "seek": 0, "start": 5.68, "end": 11.72, "text": " Yes, that's right, after reigning supreme for 7 years, the Transformer has finally been", "tokens": [50648, 1079, 11, 300, 311, 558, 11, 934, 319, 9676, 27756, 337, 1614, 924, 11, 264, 27938, 260, 575, 2721, 668, 50950], "temperature": 0.0, "avg_logprob": -0.2364110839500856, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0026720226742327213}, {"id": 2, "seek": 0, "start": 11.72, "end": 13.200000000000001, "text": " dethroned.", "tokens": [50950, 1141, 1703, 19009, 13, 51024], "temperature": 0.0, "avg_logprob": -0.2364110839500856, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0026720226742327213}, {"id": 3, "seek": 0, "start": 13.200000000000001, "end": 16.64, "text": " Well, maybe.", "tokens": [51024, 1042, 11, 1310, 13, 51196], "temperature": 0.0, "avg_logprob": -0.2364110839500856, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0026720226742327213}, {"id": 4, "seek": 0, "start": 16.64, "end": 22.28, "text": " So far, Mamba has only been tested at small model sizes, up to a few billion parameters,", "tokens": [51196, 407, 1400, 11, 376, 23337, 575, 787, 668, 8246, 412, 1359, 2316, 11602, 11, 493, 281, 257, 1326, 5218, 9834, 11, 51478], "temperature": 0.0, "avg_logprob": -0.2364110839500856, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0026720226742327213}, {"id": 5, "seek": 0, "start": 22.28, "end": 24.96, "text": " but the results so far are promising.", "tokens": [51478, 457, 264, 3542, 370, 1400, 366, 20257, 13, 51612], "temperature": 0.0, "avg_logprob": -0.2364110839500856, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0026720226742327213}, {"id": 6, "seek": 2496, "start": 24.96, "end": 28.8, "text": " In addition, Mamba uses less compute than Transformers.", "tokens": [50364, 682, 4500, 11, 376, 23337, 4960, 1570, 14722, 813, 27938, 433, 13, 50556], "temperature": 0.0, "avg_logprob": -0.11931328187909043, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.017980150878429413}, {"id": 7, "seek": 2496, "start": 28.8, "end": 34.480000000000004, "text": " For an input sequence of n words, Mamba only uses n log n compute, whereas Transformers", "tokens": [50556, 1171, 364, 4846, 8310, 295, 297, 2283, 11, 376, 23337, 787, 4960, 297, 3565, 297, 14722, 11, 9735, 27938, 433, 50840], "temperature": 0.0, "avg_logprob": -0.11931328187909043, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.017980150878429413}, {"id": 8, "seek": 2496, "start": 34.480000000000004, "end": 36.160000000000004, "text": " use n squared.", "tokens": [50840, 764, 297, 8889, 13, 50924], "temperature": 0.0, "avg_logprob": -0.11931328187909043, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.017980150878429413}, {"id": 9, "seek": 2496, "start": 36.160000000000004, "end": 42.08, "text": " So Mamba-based language models should allow for much greater context sizes to be used.", "tokens": [50924, 407, 376, 23337, 12, 6032, 2856, 5245, 820, 2089, 337, 709, 5044, 4319, 11602, 281, 312, 1143, 13, 51220], "temperature": 0.0, "avg_logprob": -0.11931328187909043, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.017980150878429413}, {"id": 10, "seek": 2496, "start": 42.08, "end": 45.96, "text": " In this video, we're going to do a deep dive of the Mamba architecture.", "tokens": [51220, 682, 341, 960, 11, 321, 434, 516, 281, 360, 257, 2452, 9192, 295, 264, 376, 23337, 9482, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11931328187909043, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.017980150878429413}, {"id": 11, "seek": 2496, "start": 45.96, "end": 46.96, "text": " What is it?", "tokens": [51414, 708, 307, 309, 30, 51464], "temperature": 0.0, "avg_logprob": -0.11931328187909043, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.017980150878429413}, {"id": 12, "seek": 2496, "start": 46.96, "end": 47.96, "text": " Why does it work so well?", "tokens": [51464, 1545, 775, 309, 589, 370, 731, 30, 51514], "temperature": 0.0, "avg_logprob": -0.11931328187909043, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.017980150878429413}, {"id": 13, "seek": 2496, "start": 47.96, "end": 54.2, "text": " And how could you have gone about designing such an architecture yourself?", "tokens": [51514, 400, 577, 727, 291, 362, 2780, 466, 14685, 1270, 364, 9482, 1803, 30, 51826], "temperature": 0.0, "avg_logprob": -0.11931328187909043, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.017980150878429413}, {"id": 14, "seek": 5420, "start": 54.2, "end": 59.0, "text": " Unfortunately Mamba is presented as an extension of something called a state space model.", "tokens": [50364, 8590, 376, 23337, 307, 8212, 382, 364, 10320, 295, 746, 1219, 257, 1785, 1901, 2316, 13, 50604], "temperature": 0.0, "avg_logprob": -0.15424381769620454, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.038450539112091064}, {"id": 15, "seek": 5420, "start": 59.0, "end": 63.120000000000005, "text": " State space models are another type of sequence model that have been steadily gaining popularity", "tokens": [50604, 4533, 1901, 5245, 366, 1071, 2010, 295, 8310, 2316, 300, 362, 668, 36129, 19752, 19301, 50810], "temperature": 0.0, "avg_logprob": -0.15424381769620454, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.038450539112091064}, {"id": 16, "seek": 5420, "start": 63.120000000000005, "end": 68.92, "text": " over the past few years, but to be honest, the theory behind state space models is massively", "tokens": [50810, 670, 264, 1791, 1326, 924, 11, 457, 281, 312, 3245, 11, 264, 5261, 2261, 1785, 1901, 5245, 307, 29379, 51100], "temperature": 0.0, "avg_logprob": -0.15424381769620454, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.038450539112091064}, {"id": 17, "seek": 5420, "start": 68.92, "end": 72.56, "text": " over complicated and uses some pretty advanced mathematics.", "tokens": [51100, 670, 6179, 293, 4960, 512, 1238, 7339, 18666, 13, 51282], "temperature": 0.0, "avg_logprob": -0.15424381769620454, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.038450539112091064}, {"id": 18, "seek": 5420, "start": 72.56, "end": 78.16, "text": " Fortunately, Mamba can also be understood as an extension of recurrent neural networks,", "tokens": [51282, 20652, 11, 376, 23337, 393, 611, 312, 7320, 382, 364, 10320, 295, 18680, 1753, 18161, 9590, 11, 51562], "temperature": 0.0, "avg_logprob": -0.15424381769620454, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.038450539112091064}, {"id": 19, "seek": 5420, "start": 78.16, "end": 82.48, "text": " or RNNs for short, which are much easier to understand.", "tokens": [51562, 420, 45702, 45, 82, 337, 2099, 11, 597, 366, 709, 3571, 281, 1223, 13, 51778], "temperature": 0.0, "avg_logprob": -0.15424381769620454, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.038450539112091064}, {"id": 20, "seek": 8248, "start": 82.48, "end": 88.2, "text": " So in this video, we will be taking the RNN path to understanding Mamba.", "tokens": [50364, 407, 294, 341, 960, 11, 321, 486, 312, 1940, 264, 45702, 45, 3100, 281, 3701, 376, 23337, 13, 50650], "temperature": 0.0, "avg_logprob": -0.12883479396502176, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.01168522983789444}, {"id": 21, "seek": 8248, "start": 88.2, "end": 89.84, "text": " Now let's get started.", "tokens": [50650, 823, 718, 311, 483, 1409, 13, 50732], "temperature": 0.0, "avg_logprob": -0.12883479396502176, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.01168522983789444}, {"id": 22, "seek": 8248, "start": 89.84, "end": 95.84, "text": " What is a recurrent neural network?", "tokens": [50732, 708, 307, 257, 18680, 1753, 18161, 3209, 30, 51032], "temperature": 0.0, "avg_logprob": -0.12883479396502176, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.01168522983789444}, {"id": 23, "seek": 8248, "start": 95.84, "end": 100.64, "text": " Given a sequence of input vectors, a convolutional layer applies a neural net to consecutive", "tokens": [51032, 18600, 257, 8310, 295, 4846, 18875, 11, 257, 45216, 304, 4583, 13165, 257, 18161, 2533, 281, 30497, 51272], "temperature": 0.0, "avg_logprob": -0.12883479396502176, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.01168522983789444}, {"id": 24, "seek": 8248, "start": 100.64, "end": 102.88000000000001, "text": " groups of vectors.", "tokens": [51272, 3935, 295, 18875, 13, 51384], "temperature": 0.0, "avg_logprob": -0.12883479396502176, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.01168522983789444}, {"id": 25, "seek": 8248, "start": 102.88000000000001, "end": 107.76, "text": " The key thing is that the neural net only sees a small number of vectors at a time, which", "tokens": [51384, 440, 2141, 551, 307, 300, 264, 18161, 2533, 787, 8194, 257, 1359, 1230, 295, 18875, 412, 257, 565, 11, 597, 51628], "temperature": 0.0, "avg_logprob": -0.12883479396502176, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.01168522983789444}, {"id": 26, "seek": 8248, "start": 107.76, "end": 109.84, "text": " makes the model easy to train.", "tokens": [51628, 1669, 264, 2316, 1858, 281, 3847, 13, 51732], "temperature": 0.0, "avg_logprob": -0.12883479396502176, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.01168522983789444}, {"id": 27, "seek": 10984, "start": 110.32000000000001, "end": 115.24000000000001, "text": " The downside is that information from vectors which are far away can't be combined until", "tokens": [50388, 440, 25060, 307, 300, 1589, 490, 18875, 597, 366, 1400, 1314, 393, 380, 312, 9354, 1826, 50634], "temperature": 0.0, "avg_logprob": -0.09112435651112752, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.001454878249205649}, {"id": 28, "seek": 10984, "start": 115.24000000000001, "end": 118.32000000000001, "text": " many convolutional layers have been applied.", "tokens": [50634, 867, 45216, 304, 7914, 362, 668, 6456, 13, 50788], "temperature": 0.0, "avg_logprob": -0.09112435651112752, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.001454878249205649}, {"id": 29, "seek": 10984, "start": 118.32000000000001, "end": 122.80000000000001, "text": " This makes it difficult for convolutional neural nets to understand long range dependencies", "tokens": [50788, 639, 1669, 309, 2252, 337, 45216, 304, 18161, 36170, 281, 1223, 938, 3613, 36606, 51012], "temperature": 0.0, "avg_logprob": -0.09112435651112752, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.001454878249205649}, {"id": 30, "seek": 10984, "start": 122.80000000000001, "end": 130.08, "text": " in their input, and such long range dependencies occur all the time in natural language text.", "tokens": [51012, 294, 641, 4846, 11, 293, 1270, 938, 3613, 36606, 5160, 439, 264, 565, 294, 3303, 2856, 2487, 13, 51376], "temperature": 0.0, "avg_logprob": -0.09112435651112752, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.001454878249205649}, {"id": 31, "seek": 10984, "start": 130.08, "end": 135.12, "text": " To remedy this flaw, the transformer architecture was invented, which successfully allows a", "tokens": [51376, 1407, 31648, 341, 13717, 11, 264, 31782, 9482, 390, 14479, 11, 597, 10727, 4045, 257, 51628], "temperature": 0.0, "avg_logprob": -0.09112435651112752, "compression_ratio": 1.670731707317073, "no_speech_prob": 0.001454878249205649}, {"id": 32, "seek": 13512, "start": 135.12, "end": 140.24, "text": " single layer to combine information from vectors no matter how far away they are.", "tokens": [50364, 2167, 4583, 281, 10432, 1589, 490, 18875, 572, 1871, 577, 1400, 1314, 436, 366, 13, 50620], "temperature": 0.0, "avg_logprob": -0.09423313718853575, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.014501489698886871}, {"id": 33, "seek": 13512, "start": 140.24, "end": 145.28, "text": " I previously made a video explaining how and why transformers work in detail, which you", "tokens": [50620, 286, 8046, 1027, 257, 960, 13468, 577, 293, 983, 4088, 433, 589, 294, 2607, 11, 597, 291, 50872], "temperature": 0.0, "avg_logprob": -0.09423313718853575, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.014501489698886871}, {"id": 34, "seek": 13512, "start": 145.28, "end": 147.68, "text": " can find here.", "tokens": [50872, 393, 915, 510, 13, 50992], "temperature": 0.0, "avg_logprob": -0.09423313718853575, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.014501489698886871}, {"id": 35, "seek": 13512, "start": 147.68, "end": 152.44, "text": " And while transformers work great, they have a significant limitation, which is that the", "tokens": [50992, 400, 1339, 4088, 433, 589, 869, 11, 436, 362, 257, 4776, 27432, 11, 597, 307, 300, 264, 51230], "temperature": 0.0, "avg_logprob": -0.09423313718853575, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.014501489698886871}, {"id": 36, "seek": 13512, "start": 152.44, "end": 156.88, "text": " amount of compute they use is quadratic in the input length.", "tokens": [51230, 2372, 295, 14722, 436, 764, 307, 37262, 294, 264, 4846, 4641, 13, 51452], "temperature": 0.0, "avg_logprob": -0.09423313718853575, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.014501489698886871}, {"id": 37, "seek": 13512, "start": 156.88, "end": 161.12, "text": " This isn't a huge deal for small inputs, but if you want to have a million vectors in", "tokens": [51452, 639, 1943, 380, 257, 2603, 2028, 337, 1359, 15743, 11, 457, 498, 291, 528, 281, 362, 257, 2459, 18875, 294, 51664], "temperature": 0.0, "avg_logprob": -0.09423313718853575, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.014501489698886871}, {"id": 38, "seek": 16112, "start": 161.12, "end": 168.4, "text": " the input, that means you need to do a million times a million operations, which is a lot.", "tokens": [50364, 264, 4846, 11, 300, 1355, 291, 643, 281, 360, 257, 2459, 1413, 257, 2459, 7705, 11, 597, 307, 257, 688, 13, 50728], "temperature": 0.0, "avg_logprob": -0.10392297044092295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.006487659178674221}, {"id": 39, "seek": 16112, "start": 168.4, "end": 173.32, "text": " Recurrent neural nets take a completely different approach to improving convolutional layers.", "tokens": [50728, 9647, 374, 1753, 18161, 36170, 747, 257, 2584, 819, 3109, 281, 11470, 45216, 304, 7914, 13, 50974], "temperature": 0.0, "avg_logprob": -0.10392297044092295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.006487659178674221}, {"id": 40, "seek": 16112, "start": 173.32, "end": 175.84, "text": " The idea is very simple.", "tokens": [50974, 440, 1558, 307, 588, 2199, 13, 51100], "temperature": 0.0, "avg_logprob": -0.10392297044092295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.006487659178674221}, {"id": 41, "seek": 16112, "start": 175.84, "end": 179.8, "text": " Instead of applying the neural net to two consecutive input vectors, you apply it to", "tokens": [51100, 7156, 295, 9275, 264, 18161, 2533, 281, 732, 30497, 4846, 18875, 11, 291, 3079, 309, 281, 51298], "temperature": 0.0, "avg_logprob": -0.10392297044092295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.006487659178674221}, {"id": 42, "seek": 16112, "start": 179.8, "end": 185.16, "text": " one input vector and the previous output of the neural net.", "tokens": [51298, 472, 4846, 8062, 293, 264, 3894, 5598, 295, 264, 18161, 2533, 13, 51566], "temperature": 0.0, "avg_logprob": -0.10392297044092295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.006487659178674221}, {"id": 43, "seek": 16112, "start": 185.16, "end": 189.20000000000002, "text": " This seems like a small change, but it has profound consequences.", "tokens": [51566, 639, 2544, 411, 257, 1359, 1319, 11, 457, 309, 575, 14382, 10098, 13, 51768], "temperature": 0.0, "avg_logprob": -0.10392297044092295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.006487659178674221}, {"id": 44, "seek": 18920, "start": 189.2, "end": 194.64, "text": " Each output vector now contains information from all of the input vectors prior to it,", "tokens": [50364, 6947, 5598, 8062, 586, 8306, 1589, 490, 439, 295, 264, 4846, 18875, 4059, 281, 309, 11, 50636], "temperature": 0.0, "avg_logprob": -0.12841158367338634, "compression_ratio": 1.7578125, "no_speech_prob": 0.08505963534116745}, {"id": 45, "seek": 18920, "start": 194.64, "end": 197.72, "text": " instead of only one previous vector.", "tokens": [50636, 2602, 295, 787, 472, 3894, 8062, 13, 50790], "temperature": 0.0, "avg_logprob": -0.12841158367338634, "compression_ratio": 1.7578125, "no_speech_prob": 0.08505963534116745}, {"id": 46, "seek": 18920, "start": 197.72, "end": 202.6, "text": " This final output vector contains information from every vector in the input, no matter how", "tokens": [50790, 639, 2572, 5598, 8062, 8306, 1589, 490, 633, 8062, 294, 264, 4846, 11, 572, 1871, 577, 51034], "temperature": 0.0, "avg_logprob": -0.12841158367338634, "compression_ratio": 1.7578125, "no_speech_prob": 0.08505963534116745}, {"id": 47, "seek": 18920, "start": 202.6, "end": 204.76, "text": " many there are.", "tokens": [51034, 867, 456, 366, 13, 51142], "temperature": 0.0, "avg_logprob": -0.12841158367338634, "compression_ratio": 1.7578125, "no_speech_prob": 0.08505963534116745}, {"id": 48, "seek": 18920, "start": 204.76, "end": 208.32, "text": " And we have not used any more compute than a convolutional layer.", "tokens": [51142, 400, 321, 362, 406, 1143, 604, 544, 14722, 813, 257, 45216, 304, 4583, 13, 51320], "temperature": 0.0, "avg_logprob": -0.12841158367338634, "compression_ratio": 1.7578125, "no_speech_prob": 0.08505963534116745}, {"id": 49, "seek": 18920, "start": 208.32, "end": 212.92, "text": " We've managed to incorporate long range information for free.", "tokens": [51320, 492, 600, 6453, 281, 16091, 938, 3613, 1589, 337, 1737, 13, 51550], "temperature": 0.0, "avg_logprob": -0.12841158367338634, "compression_ratio": 1.7578125, "no_speech_prob": 0.08505963534116745}, {"id": 50, "seek": 18920, "start": 212.92, "end": 218.79999999999998, "text": " This is exactly what we want, or at least it would be if it weren't for two small problems", "tokens": [51550, 639, 307, 2293, 437, 321, 528, 11, 420, 412, 1935, 309, 576, 312, 498, 309, 4999, 380, 337, 732, 1359, 2740, 51844], "temperature": 0.0, "avg_logprob": -0.12841158367338634, "compression_ratio": 1.7578125, "no_speech_prob": 0.08505963534116745}, {"id": 51, "seek": 21880, "start": 218.8, "end": 223.96, "text": " with RNNs, which make them almost impossible to use in practice.", "tokens": [50364, 365, 45702, 45, 82, 11, 597, 652, 552, 1920, 6243, 281, 764, 294, 3124, 13, 50622], "temperature": 0.0, "avg_logprob": -0.09165737476754696, "compression_ratio": 1.5992217898832686, "no_speech_prob": 0.003272728994488716}, {"id": 52, "seek": 21880, "start": 223.96, "end": 228.76000000000002, "text": " The first problem is that while a recurrent layer uses the same amount of compute as a", "tokens": [50622, 440, 700, 1154, 307, 300, 1339, 257, 18680, 1753, 4583, 4960, 264, 912, 2372, 295, 14722, 382, 257, 50862], "temperature": 0.0, "avg_logprob": -0.09165737476754696, "compression_ratio": 1.5992217898832686, "no_speech_prob": 0.003272728994488716}, {"id": 53, "seek": 21880, "start": 228.76000000000002, "end": 235.72000000000003, "text": " convolutional layer, that compute cannot be parallelized across multiple processes.", "tokens": [50862, 45216, 304, 4583, 11, 300, 14722, 2644, 312, 8952, 1602, 2108, 3866, 7555, 13, 51210], "temperature": 0.0, "avg_logprob": -0.09165737476754696, "compression_ratio": 1.5992217898832686, "no_speech_prob": 0.003272728994488716}, {"id": 54, "seek": 21880, "start": 235.72000000000003, "end": 240.4, "text": " Even if you have lots of processes available, you can't begin evaluating the neural net", "tokens": [51210, 2754, 498, 291, 362, 3195, 295, 7555, 2435, 11, 291, 393, 380, 1841, 27479, 264, 18161, 2533, 51444], "temperature": 0.0, "avg_logprob": -0.09165737476754696, "compression_ratio": 1.5992217898832686, "no_speech_prob": 0.003272728994488716}, {"id": 55, "seek": 21880, "start": 240.4, "end": 245.52, "text": " on an input until all of the previous steps have finished, because you need to feed the", "tokens": [51444, 322, 364, 4846, 1826, 439, 295, 264, 3894, 4439, 362, 4335, 11, 570, 291, 643, 281, 3154, 264, 51700], "temperature": 0.0, "avg_logprob": -0.09165737476754696, "compression_ratio": 1.5992217898832686, "no_speech_prob": 0.003272728994488716}, {"id": 56, "seek": 24552, "start": 245.52, "end": 250.60000000000002, "text": " output from the previous step into the neural net.", "tokens": [50364, 5598, 490, 264, 3894, 1823, 666, 264, 18161, 2533, 13, 50618], "temperature": 0.0, "avg_logprob": -0.14315414428710938, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.023682106286287308}, {"id": 57, "seek": 24552, "start": 250.60000000000002, "end": 254.92000000000002, "text": " Compare this to a convolutional layer where the neural net only needs to see the original", "tokens": [50618, 48523, 341, 281, 257, 45216, 304, 4583, 689, 264, 18161, 2533, 787, 2203, 281, 536, 264, 3380, 50834], "temperature": 0.0, "avg_logprob": -0.14315414428710938, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.023682106286287308}, {"id": 58, "seek": 24552, "start": 254.92000000000002, "end": 256.04, "text": " input.", "tokens": [50834, 4846, 13, 50890], "temperature": 0.0, "avg_logprob": -0.14315414428710938, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.023682106286287308}, {"id": 59, "seek": 24552, "start": 256.04, "end": 260.28000000000003, "text": " You can run the neural net on all inputs at the same time, so long as you have enough", "tokens": [50890, 509, 393, 1190, 264, 18161, 2533, 322, 439, 15743, 412, 264, 912, 565, 11, 370, 938, 382, 291, 362, 1547, 51102], "temperature": 0.0, "avg_logprob": -0.14315414428710938, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.023682106286287308}, {"id": 60, "seek": 24552, "start": 260.28000000000003, "end": 263.72, "text": " processes available.", "tokens": [51102, 7555, 2435, 13, 51274], "temperature": 0.0, "avg_logprob": -0.14315414428710938, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.023682106286287308}, {"id": 61, "seek": 24552, "start": 263.72, "end": 268.8, "text": " And since modern hardware such as GPUs are highly specialized for parallel computation", "tokens": [51274, 400, 1670, 4363, 8837, 1270, 382, 18407, 82, 366, 5405, 19813, 337, 8952, 24903, 51528], "temperature": 0.0, "avg_logprob": -0.14315414428710938, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.023682106286287308}, {"id": 62, "seek": 26880, "start": 268.8, "end": 277.36, "text": " with thousands of processes, RNNs are actually a lot slower than CNNs in practice.", "tokens": [50364, 365, 5383, 295, 7555, 11, 45702, 45, 82, 366, 767, 257, 688, 14009, 813, 24859, 82, 294, 3124, 13, 50792], "temperature": 0.0, "avg_logprob": -0.0965395736694336, "compression_ratio": 1.5637254901960784, "no_speech_prob": 0.2392156422138214}, {"id": 63, "seek": 26880, "start": 277.36, "end": 285.88, "text": " In fact, RNNs are even slower than transformers despite doing less computation.", "tokens": [50792, 682, 1186, 11, 45702, 45, 82, 366, 754, 14009, 813, 4088, 433, 7228, 884, 1570, 24903, 13, 51218], "temperature": 0.0, "avg_logprob": -0.0965395736694336, "compression_ratio": 1.5637254901960784, "no_speech_prob": 0.2392156422138214}, {"id": 64, "seek": 26880, "start": 285.88, "end": 290.84000000000003, "text": " And the second problem is that RNNs are incredibly difficult to train.", "tokens": [51218, 400, 264, 1150, 1154, 307, 300, 45702, 45, 82, 366, 6252, 2252, 281, 3847, 13, 51466], "temperature": 0.0, "avg_logprob": -0.0965395736694336, "compression_ratio": 1.5637254901960784, "no_speech_prob": 0.2392156422138214}, {"id": 65, "seek": 26880, "start": 290.84000000000003, "end": 295.96000000000004, "text": " While in theory a single recurrent layer can incorporate information from arbitrarily", "tokens": [51466, 3987, 294, 5261, 257, 2167, 18680, 1753, 4583, 393, 16091, 1589, 490, 19071, 3289, 51722], "temperature": 0.0, "avg_logprob": -0.0965395736694336, "compression_ratio": 1.5637254901960784, "no_speech_prob": 0.2392156422138214}, {"id": 66, "seek": 29596, "start": 295.96, "end": 299.28, "text": " many inputs, in practice they don't.", "tokens": [50364, 867, 15743, 11, 294, 3124, 436, 500, 380, 13, 50530], "temperature": 0.0, "avg_logprob": -0.1211311132600992, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.06558098644018173}, {"id": 67, "seek": 29596, "start": 299.28, "end": 306.2, "text": " Instead, they only learn to incorporate information from the previous few dozen inputs at most.", "tokens": [50530, 7156, 11, 436, 787, 1466, 281, 16091, 1589, 490, 264, 3894, 1326, 16654, 15743, 412, 881, 13, 50876], "temperature": 0.0, "avg_logprob": -0.1211311132600992, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.06558098644018173}, {"id": 68, "seek": 29596, "start": 306.2, "end": 311.59999999999997, "text": " The idea for RNNs has been around since the 1980s, but because of these two problems,", "tokens": [50876, 440, 1558, 337, 45702, 45, 82, 575, 668, 926, 1670, 264, 13626, 82, 11, 457, 570, 295, 613, 732, 2740, 11, 51146], "temperature": 0.0, "avg_logprob": -0.1211311132600992, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.06558098644018173}, {"id": 69, "seek": 29596, "start": 311.59999999999997, "end": 316.4, "text": " RNNs have fallen out of favor with convolutional neural nets and transformers being much more", "tokens": [51146, 45702, 45, 82, 362, 11547, 484, 295, 2294, 365, 45216, 304, 18161, 36170, 293, 4088, 433, 885, 709, 544, 51386], "temperature": 0.0, "avg_logprob": -0.1211311132600992, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.06558098644018173}, {"id": 70, "seek": 29596, "start": 316.4, "end": 318.56, "text": " successful in practice.", "tokens": [51386, 4406, 294, 3124, 13, 51494], "temperature": 0.0, "avg_logprob": -0.1211311132600992, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.06558098644018173}, {"id": 71, "seek": 29596, "start": 318.56, "end": 324.24, "text": " In fact, RNNs have hardly been used at all in the past decade.", "tokens": [51494, 682, 1186, 11, 45702, 45, 82, 362, 13572, 668, 1143, 412, 439, 294, 264, 1791, 10378, 13, 51778], "temperature": 0.0, "avg_logprob": -0.1211311132600992, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.06558098644018173}, {"id": 72, "seek": 32424, "start": 324.36, "end": 330.84000000000003, "text": " Well, last year a new paper was published showing that linear RNNs can avoid both of", "tokens": [50370, 1042, 11, 1036, 1064, 257, 777, 3035, 390, 6572, 4099, 300, 8213, 45702, 45, 82, 393, 5042, 1293, 295, 50694], "temperature": 0.0, "avg_logprob": -0.19177992203656366, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.3919195532798767}, {"id": 73, "seek": 32424, "start": 330.84000000000003, "end": 337.52, "text": " these problems, and therefore linear RNNs are highly effective long sequence models.", "tokens": [50694, 613, 2740, 11, 293, 4412, 8213, 45702, 45, 82, 366, 5405, 4942, 938, 8310, 5245, 13, 51028], "temperature": 0.0, "avg_logprob": -0.19177992203656366, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.3919195532798767}, {"id": 74, "seek": 32424, "start": 337.52, "end": 341.48, "text": " So what is a linear recurrent neural network?", "tokens": [51028, 407, 437, 307, 257, 8213, 18680, 1753, 18161, 3209, 30, 51226], "temperature": 0.0, "avg_logprob": -0.19177992203656366, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.3919195532798767}, {"id": 75, "seek": 32424, "start": 341.48, "end": 353.16, "text": " Well, you simply replace the neural net with a linear function.", "tokens": [51226, 1042, 11, 291, 2935, 7406, 264, 18161, 2533, 365, 257, 8213, 2445, 13, 51810], "temperature": 0.0, "avg_logprob": -0.19177992203656366, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.3919195532798767}, {"id": 76, "seek": 35316, "start": 353.16, "end": 358.36, "text": " This might seem like a bad idea, since linear functions can only perform relatively simple", "tokens": [50364, 639, 1062, 1643, 411, 257, 1578, 1558, 11, 1670, 8213, 6828, 393, 787, 2042, 7226, 2199, 50624], "temperature": 0.0, "avg_logprob": -0.10438358422481653, "compression_ratio": 1.7429718875502007, "no_speech_prob": 0.04465324059128761}, {"id": 77, "seek": 35316, "start": 358.36, "end": 363.48, "text": " transformations of their inputs, but we can make up for it by applying a full neural net", "tokens": [50624, 34852, 295, 641, 15743, 11, 457, 321, 393, 652, 493, 337, 309, 538, 9275, 257, 1577, 18161, 2533, 50880], "temperature": 0.0, "avg_logprob": -0.10438358422481653, "compression_ratio": 1.7429718875502007, "no_speech_prob": 0.04465324059128761}, {"id": 78, "seek": 35316, "start": 363.48, "end": 366.8, "text": " to each output vector afterwards.", "tokens": [50880, 281, 1184, 5598, 8062, 10543, 13, 51046], "temperature": 0.0, "avg_logprob": -0.10438358422481653, "compression_ratio": 1.7429718875502007, "no_speech_prob": 0.04465324059128761}, {"id": 79, "seek": 35316, "start": 366.8, "end": 371.12, "text": " This is similar to how in transformers you can replace the value neural nets with simple", "tokens": [51046, 639, 307, 2531, 281, 577, 294, 4088, 433, 291, 393, 7406, 264, 2158, 18161, 36170, 365, 2199, 51262], "temperature": 0.0, "avg_logprob": -0.10438358422481653, "compression_ratio": 1.7429718875502007, "no_speech_prob": 0.04465324059128761}, {"id": 80, "seek": 35316, "start": 371.12, "end": 376.04, "text": " linear functions, and then add neural nets in between self-attention layers to make up", "tokens": [51262, 8213, 6828, 11, 293, 550, 909, 18161, 36170, 294, 1296, 2698, 12, 1591, 1251, 7914, 281, 652, 493, 51508], "temperature": 0.0, "avg_logprob": -0.10438358422481653, "compression_ratio": 1.7429718875502007, "no_speech_prob": 0.04465324059128761}, {"id": 81, "seek": 35316, "start": 376.04, "end": 379.6, "text": " for the lack of non-linear processing power.", "tokens": [51508, 337, 264, 5011, 295, 2107, 12, 28263, 9007, 1347, 13, 51686], "temperature": 0.0, "avg_logprob": -0.10438358422481653, "compression_ratio": 1.7429718875502007, "no_speech_prob": 0.04465324059128761}, {"id": 82, "seek": 37960, "start": 379.6, "end": 385.56, "text": " So just like in a transformer, we will alternate linear recurrent layers with element-wise", "tokens": [50364, 407, 445, 411, 294, 257, 31782, 11, 321, 486, 18873, 8213, 18680, 1753, 7914, 365, 4478, 12, 3711, 50662], "temperature": 0.0, "avg_logprob": -0.11575411641320517, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.003376396605744958}, {"id": 83, "seek": 37960, "start": 385.56, "end": 388.64000000000004, "text": " neural networks.", "tokens": [50662, 18161, 9590, 13, 50816], "temperature": 0.0, "avg_logprob": -0.11575411641320517, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.003376396605744958}, {"id": 84, "seek": 37960, "start": 388.64000000000004, "end": 393.84000000000003, "text": " But importantly, by making the recurrent operation purely linear, it becomes possible", "tokens": [50816, 583, 8906, 11, 538, 1455, 264, 18680, 1753, 6916, 17491, 8213, 11, 309, 3643, 1944, 51076], "temperature": 0.0, "avg_logprob": -0.11575411641320517, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.003376396605744958}, {"id": 85, "seek": 37960, "start": 393.84000000000003, "end": 397.36, "text": " to solve both of the RNN problems.", "tokens": [51076, 281, 5039, 1293, 295, 264, 45702, 45, 2740, 13, 51252], "temperature": 0.0, "avg_logprob": -0.11575411641320517, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.003376396605744958}, {"id": 86, "seek": 37960, "start": 397.36, "end": 402.48, "text": " To start with, I'll explain how a linear recurrence applied to n vectors can be computed", "tokens": [51252, 1407, 722, 365, 11, 286, 603, 2903, 577, 257, 8213, 18680, 10760, 6456, 281, 297, 18875, 393, 312, 40610, 51508], "temperature": 0.0, "avg_logprob": -0.11575411641320517, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.003376396605744958}, {"id": 87, "seek": 37960, "start": 402.48, "end": 406.76000000000005, "text": " in parallel in just log n time.", "tokens": [51508, 294, 8952, 294, 445, 3565, 297, 565, 13, 51722], "temperature": 0.0, "avg_logprob": -0.11575411641320517, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.003376396605744958}, {"id": 88, "seek": 40676, "start": 406.76, "end": 412.28, "text": " And then I'll explain how the training issues that plague regular RNNs can be fixed in linear", "tokens": [50364, 400, 550, 286, 603, 2903, 577, 264, 3097, 2663, 300, 28185, 3890, 45702, 45, 82, 393, 312, 6806, 294, 8213, 50640], "temperature": 0.0, "avg_logprob": -0.1630366862505332, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.0015011158538982272}, {"id": 89, "seek": 40676, "start": 412.28, "end": 419.92, "text": " recurrences.", "tokens": [50640, 18680, 38983, 13, 51022], "temperature": 0.0, "avg_logprob": -0.1630366862505332, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.0015011158538982272}, {"id": 90, "seek": 40676, "start": 419.92, "end": 423.03999999999996, "text": " The linear recurrence operator is given by this formula.", "tokens": [51022, 440, 8213, 18680, 10760, 12973, 307, 2212, 538, 341, 8513, 13, 51178], "temperature": 0.0, "avg_logprob": -0.1630366862505332, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.0015011158538982272}, {"id": 91, "seek": 40676, "start": 423.03999999999996, "end": 428.92, "text": " To get the ith output vector, you multiply the previous i-1th output vector with a matrix", "tokens": [51178, 1407, 483, 264, 309, 71, 5598, 8062, 11, 291, 12972, 264, 3894, 741, 12, 16, 392, 5598, 8062, 365, 257, 8141, 51472], "temperature": 0.0, "avg_logprob": -0.1630366862505332, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.0015011158538982272}, {"id": 92, "seek": 40676, "start": 428.92, "end": 435.4, "text": " w-y, and add the ith input vector multiplied by a different matrix w-x.", "tokens": [51472, 261, 12, 88, 11, 293, 909, 264, 309, 71, 4846, 8062, 17207, 538, 257, 819, 8141, 261, 12, 87, 13, 51796], "temperature": 0.0, "avg_logprob": -0.1630366862505332, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.0015011158538982272}, {"id": 93, "seek": 43540, "start": 435.4, "end": 440.67999999999995, "text": " The entries in the w matrices are the parameters which will be learned by the model, so they", "tokens": [50364, 440, 23041, 294, 264, 261, 32284, 366, 264, 9834, 597, 486, 312, 3264, 538, 264, 2316, 11, 370, 436, 50628], "temperature": 0.0, "avg_logprob": -0.10310950788479407, "compression_ratio": 1.647940074906367, "no_speech_prob": 0.20677345991134644}, {"id": 94, "seek": 43540, "start": 440.67999999999995, "end": 445.76, "text": " start off as random samples from a normal distribution centered at zero, and are then", "tokens": [50628, 722, 766, 382, 4974, 10938, 490, 257, 2710, 7316, 18988, 412, 4018, 11, 293, 366, 550, 50882], "temperature": 0.0, "avg_logprob": -0.10310950788479407, "compression_ratio": 1.647940074906367, "no_speech_prob": 0.20677345991134644}, {"id": 95, "seek": 43540, "start": 445.76, "end": 448.4, "text": " updated with gradient descent.", "tokens": [50882, 10588, 365, 16235, 23475, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10310950788479407, "compression_ratio": 1.647940074906367, "no_speech_prob": 0.20677345991134644}, {"id": 96, "seek": 43540, "start": 448.4, "end": 453.59999999999997, "text": " And since the w-x matrix is just applied to each input independently, we can actually", "tokens": [51014, 400, 1670, 264, 261, 12, 87, 8141, 307, 445, 6456, 281, 1184, 4846, 21761, 11, 321, 393, 767, 51274], "temperature": 0.0, "avg_logprob": -0.10310950788479407, "compression_ratio": 1.647940074906367, "no_speech_prob": 0.20677345991134644}, {"id": 97, "seek": 43540, "start": 453.59999999999997, "end": 457.32, "text": " just think of it as being part of the previous layer.", "tokens": [51274, 445, 519, 295, 309, 382, 885, 644, 295, 264, 3894, 4583, 13, 51460], "temperature": 0.0, "avg_logprob": -0.10310950788479407, "compression_ratio": 1.647940074906367, "no_speech_prob": 0.20677345991134644}, {"id": 98, "seek": 43540, "start": 457.32, "end": 463.28, "text": " So we can simplify our recurrence operator to just add the input x, assuming that a linear", "tokens": [51460, 407, 321, 393, 20460, 527, 18680, 10760, 12973, 281, 445, 909, 264, 4846, 2031, 11, 11926, 300, 257, 8213, 51758], "temperature": 0.0, "avg_logprob": -0.10310950788479407, "compression_ratio": 1.647940074906367, "no_speech_prob": 0.20677345991134644}, {"id": 99, "seek": 46328, "start": 463.28, "end": 468.44, "text": " function has already been applied to the input in the previous layer.", "tokens": [50364, 2445, 575, 1217, 668, 6456, 281, 264, 4846, 294, 264, 3894, 4583, 13, 50622], "temperature": 0.0, "avg_logprob": -0.10932514042530245, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.04083709791302681}, {"id": 100, "seek": 46328, "start": 468.44, "end": 473.08, "text": " A linear recurrence is actually a special case of a more general operation called the", "tokens": [50622, 316, 8213, 18680, 10760, 307, 767, 257, 2121, 1389, 295, 257, 544, 2674, 6916, 1219, 264, 50854], "temperature": 0.0, "avg_logprob": -0.10932514042530245, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.04083709791302681}, {"id": 101, "seek": 46328, "start": 473.08, "end": 479.88, "text": " scan, so let's start with the simplest example of a scan, a cumulative sum.", "tokens": [50854, 11049, 11, 370, 718, 311, 722, 365, 264, 22811, 1365, 295, 257, 11049, 11, 257, 38379, 2408, 13, 51194], "temperature": 0.0, "avg_logprob": -0.10932514042530245, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.04083709791302681}, {"id": 102, "seek": 46328, "start": 479.88, "end": 484.88, "text": " Given a list of n numbers as input, the goal is to compute the list of partial sums up", "tokens": [51194, 18600, 257, 1329, 295, 297, 3547, 382, 4846, 11, 264, 3387, 307, 281, 14722, 264, 1329, 295, 14641, 34499, 493, 51444], "temperature": 0.0, "avg_logprob": -0.10932514042530245, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.04083709791302681}, {"id": 103, "seek": 46328, "start": 484.88, "end": 491.28, "text": " to each term, so the ith item in the output list should be the sum of the first i-items", "tokens": [51444, 281, 1184, 1433, 11, 370, 264, 309, 71, 3174, 294, 264, 5598, 1329, 820, 312, 264, 2408, 295, 264, 700, 741, 12, 270, 9097, 51764], "temperature": 0.0, "avg_logprob": -0.10932514042530245, "compression_ratio": 1.6707818930041152, "no_speech_prob": 0.04083709791302681}, {"id": 104, "seek": 49128, "start": 491.28, "end": 494.76, "text": " of the input list.", "tokens": [50364, 295, 264, 4846, 1329, 13, 50538], "temperature": 0.0, "avg_logprob": -0.13551410730334296, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.08753491938114166}, {"id": 105, "seek": 49128, "start": 494.76, "end": 500.03999999999996, "text": " While it is trivial to compute this by simply adding the numbers together one at a time,", "tokens": [50538, 3987, 309, 307, 26703, 281, 14722, 341, 538, 2935, 5127, 264, 3547, 1214, 472, 412, 257, 565, 11, 50802], "temperature": 0.0, "avg_logprob": -0.13551410730334296, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.08753491938114166}, {"id": 106, "seek": 49128, "start": 500.03999999999996, "end": 504.79999999999995, "text": " we want to do it in parallel.", "tokens": [50802, 321, 528, 281, 360, 309, 294, 8952, 13, 51040], "temperature": 0.0, "avg_logprob": -0.13551410730334296, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.08753491938114166}, {"id": 107, "seek": 49128, "start": 504.79999999999995, "end": 508.23999999999995, "text": " And it turns out we can do so as follows.", "tokens": [51040, 400, 309, 4523, 484, 321, 393, 360, 370, 382, 10002, 13, 51212], "temperature": 0.0, "avg_logprob": -0.13551410730334296, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.08753491938114166}, {"id": 108, "seek": 49128, "start": 508.23999999999995, "end": 515.92, "text": " First add together each consecutive pair of numbers, then from the resulting list add", "tokens": [51212, 2386, 909, 1214, 1184, 30497, 6119, 295, 3547, 11, 550, 490, 264, 16505, 1329, 909, 51596], "temperature": 0.0, "avg_logprob": -0.13551410730334296, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.08753491938114166}, {"id": 109, "seek": 51592, "start": 515.92, "end": 527.04, "text": " together pairs of numbers which are two steps apart, then four steps apart, and eight, and", "tokens": [50364, 1214, 15494, 295, 3547, 597, 366, 732, 4439, 4936, 11, 550, 1451, 4439, 4936, 11, 293, 3180, 11, 293, 50920], "temperature": 0.0, "avg_logprob": -0.12735939025878906, "compression_ratio": 1.6861702127659575, "no_speech_prob": 0.11592768132686615}, {"id": 110, "seek": 51592, "start": 527.04, "end": 529.0799999999999, "text": " so on.", "tokens": [50920, 370, 322, 13, 51022], "temperature": 0.0, "avg_logprob": -0.12735939025878906, "compression_ratio": 1.6861702127659575, "no_speech_prob": 0.11592768132686615}, {"id": 111, "seek": 51592, "start": 529.0799999999999, "end": 534.56, "text": " Each iteration doubling the step size, until the step size is as large as the entire input", "tokens": [51022, 6947, 24784, 33651, 264, 1823, 2744, 11, 1826, 264, 1823, 2744, 307, 382, 2416, 382, 264, 2302, 4846, 51296], "temperature": 0.0, "avg_logprob": -0.12735939025878906, "compression_ratio": 1.6861702127659575, "no_speech_prob": 0.11592768132686615}, {"id": 112, "seek": 51592, "start": 534.56, "end": 539.7199999999999, "text": " list, which will be after log n steps.", "tokens": [51296, 1329, 11, 597, 486, 312, 934, 3565, 297, 4439, 13, 51554], "temperature": 0.0, "avg_logprob": -0.12735939025878906, "compression_ratio": 1.6861702127659575, "no_speech_prob": 0.11592768132686615}, {"id": 113, "seek": 51592, "start": 539.7199999999999, "end": 544.92, "text": " This algorithm works because at each iteration the ith output element contains the sum of", "tokens": [51554, 639, 9284, 1985, 570, 412, 1184, 24784, 264, 309, 71, 5598, 4478, 8306, 264, 2408, 295, 51814], "temperature": 0.0, "avg_logprob": -0.12735939025878906, "compression_ratio": 1.6861702127659575, "no_speech_prob": 0.11592768132686615}, {"id": 114, "seek": 54492, "start": 544.92, "end": 548.8, "text": " the previous step size numbers.", "tokens": [50364, 264, 3894, 1823, 2744, 3547, 13, 50558], "temperature": 0.0, "avg_logprob": -0.08644409589870002, "compression_ratio": 2.184357541899441, "no_speech_prob": 0.047400783747434616}, {"id": 115, "seek": 54492, "start": 548.8, "end": 554.5999999999999, "text": " For example, in the first iteration each output number is the sum of the previous two terms.", "tokens": [50558, 1171, 1365, 11, 294, 264, 700, 24784, 1184, 5598, 1230, 307, 264, 2408, 295, 264, 3894, 732, 2115, 13, 50848], "temperature": 0.0, "avg_logprob": -0.08644409589870002, "compression_ratio": 2.184357541899441, "no_speech_prob": 0.047400783747434616}, {"id": 116, "seek": 54492, "start": 554.5999999999999, "end": 560.1999999999999, "text": " In the next iteration each item contains the sum of the previous two terms plus the previous", "tokens": [50848, 682, 264, 958, 24784, 1184, 3174, 8306, 264, 2408, 295, 264, 3894, 732, 2115, 1804, 264, 3894, 51128], "temperature": 0.0, "avg_logprob": -0.08644409589870002, "compression_ratio": 2.184357541899441, "no_speech_prob": 0.047400783747434616}, {"id": 117, "seek": 54492, "start": 560.1999999999999, "end": 567.52, "text": " two terms starting two away, that is the sum of the previous four terms, and so on.", "tokens": [51128, 732, 2115, 2891, 732, 1314, 11, 300, 307, 264, 2408, 295, 264, 3894, 1451, 2115, 11, 293, 370, 322, 13, 51494], "temperature": 0.0, "avg_logprob": -0.08644409589870002, "compression_ratio": 2.184357541899441, "no_speech_prob": 0.047400783747434616}, {"id": 118, "seek": 54492, "start": 567.52, "end": 572.92, "text": " When the step size is the size of the input, each output contains the sum of all previous", "tokens": [51494, 1133, 264, 1823, 2744, 307, 264, 2744, 295, 264, 4846, 11, 1184, 5598, 8306, 264, 2408, 295, 439, 3894, 51764], "temperature": 0.0, "avg_logprob": -0.08644409589870002, "compression_ratio": 2.184357541899441, "no_speech_prob": 0.047400783747434616}, {"id": 119, "seek": 57292, "start": 572.92, "end": 576.28, "text": " terms as desired.", "tokens": [50364, 2115, 382, 14721, 13, 50532], "temperature": 0.0, "avg_logprob": -0.14198918091623405, "compression_ratio": 1.6102564102564103, "no_speech_prob": 0.16875965893268585}, {"id": 120, "seek": 57292, "start": 576.28, "end": 581.64, "text": " It's trivial to see that each iteration can be computed in parallel, however the different", "tokens": [50532, 467, 311, 26703, 281, 536, 300, 1184, 24784, 393, 312, 40610, 294, 8952, 11, 4461, 264, 819, 50800], "temperature": 0.0, "avg_logprob": -0.14198918091623405, "compression_ratio": 1.6102564102564103, "no_speech_prob": 0.16875965893268585}, {"id": 121, "seek": 57292, "start": 581.64, "end": 588.54, "text": " iterations do still need to be computed sequentially, and there are log n iterations.", "tokens": [50800, 36540, 360, 920, 643, 281, 312, 40610, 5123, 3137, 11, 293, 456, 366, 3565, 297, 36540, 13, 51145], "temperature": 0.0, "avg_logprob": -0.14198918091623405, "compression_ratio": 1.6102564102564103, "no_speech_prob": 0.16875965893268585}, {"id": 122, "seek": 57292, "start": 588.54, "end": 594.64, "text": " So if you have n processors, the total runtime of this algorithm is log n, down from n of", "tokens": [51145, 407, 498, 291, 362, 297, 27751, 11, 264, 3217, 34474, 295, 341, 9284, 307, 3565, 297, 11, 760, 490, 297, 295, 51450], "temperature": 0.0, "avg_logprob": -0.14198918091623405, "compression_ratio": 1.6102564102564103, "no_speech_prob": 0.16875965893268585}, {"id": 123, "seek": 57292, "start": 594.64, "end": 597.9599999999999, "text": " the naive sequential version.", "tokens": [51450, 264, 29052, 42881, 3037, 13, 51616], "temperature": 0.0, "avg_logprob": -0.14198918091623405, "compression_ratio": 1.6102564102564103, "no_speech_prob": 0.16875965893268585}, {"id": 124, "seek": 59796, "start": 597.96, "end": 603.5600000000001, "text": " And this same algorithm works for computing lists of cumulative applications of any binary", "tokens": [50364, 400, 341, 912, 9284, 1985, 337, 15866, 14511, 295, 38379, 5821, 295, 604, 17434, 50644], "temperature": 0.0, "avg_logprob": -0.10062943696975708, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.19180282950401306}, {"id": 125, "seek": 59796, "start": 603.5600000000001, "end": 611.1600000000001, "text": " operator, not just addition, so long as the binary operator is associative.", "tokens": [50644, 12973, 11, 406, 445, 4500, 11, 370, 938, 382, 264, 17434, 12973, 307, 4180, 1166, 13, 51024], "temperature": 0.0, "avg_logprob": -0.10062943696975708, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.19180282950401306}, {"id": 126, "seek": 59796, "start": 611.1600000000001, "end": 615.32, "text": " Associative means that you can change the order of application and you'll still end", "tokens": [51024, 8619, 1166, 1355, 300, 291, 393, 1319, 264, 1668, 295, 3861, 293, 291, 603, 920, 917, 51232], "temperature": 0.0, "avg_logprob": -0.10062943696975708, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.19180282950401306}, {"id": 127, "seek": 59796, "start": 615.32, "end": 617.44, "text": " up with the same result.", "tokens": [51232, 493, 365, 264, 912, 1874, 13, 51338], "temperature": 0.0, "avg_logprob": -0.10062943696975708, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.19180282950401306}, {"id": 128, "seek": 59796, "start": 617.44, "end": 625.1600000000001, "text": " This is true of addition, which is why our parallel cumulative sum algorithm works.", "tokens": [51338, 639, 307, 2074, 295, 4500, 11, 597, 307, 983, 527, 8952, 38379, 2408, 9284, 1985, 13, 51724], "temperature": 0.0, "avg_logprob": -0.10062943696975708, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.19180282950401306}, {"id": 129, "seek": 62516, "start": 625.16, "end": 630.28, "text": " And it's also true of a bunch of other operations.", "tokens": [50364, 400, 309, 311, 611, 2074, 295, 257, 3840, 295, 661, 7705, 13, 50620], "temperature": 0.0, "avg_logprob": -0.10417330265045166, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.08265694975852966}, {"id": 130, "seek": 62516, "start": 630.28, "end": 635.64, "text": " In particular, this binary operator is associative.", "tokens": [50620, 682, 1729, 11, 341, 17434, 12973, 307, 4180, 1166, 13, 50888], "temperature": 0.0, "avg_logprob": -0.10417330265045166, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.08265694975852966}, {"id": 131, "seek": 62516, "start": 635.64, "end": 641.52, "text": " Note that this operator uses a pair of a matrix and a vector as input and output, instead", "tokens": [50888, 11633, 300, 341, 12973, 4960, 257, 6119, 295, 257, 8141, 293, 257, 8062, 382, 4846, 293, 5598, 11, 2602, 51182], "temperature": 0.0, "avg_logprob": -0.10417330265045166, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.08265694975852966}, {"id": 132, "seek": 62516, "start": 641.52, "end": 646.1999999999999, "text": " of just a single number, like with addition.", "tokens": [51182, 295, 445, 257, 2167, 1230, 11, 411, 365, 4500, 13, 51416], "temperature": 0.0, "avg_logprob": -0.10417330265045166, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.08265694975852966}, {"id": 133, "seek": 62516, "start": 646.1999999999999, "end": 653.24, "text": " And remarkably, performing a scan with this operator is equivalent to a linear recurrence.", "tokens": [51416, 400, 37381, 11, 10205, 257, 11049, 365, 341, 12973, 307, 10344, 281, 257, 8213, 18680, 10760, 13, 51768], "temperature": 0.0, "avg_logprob": -0.10417330265045166, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.08265694975852966}, {"id": 134, "seek": 65324, "start": 653.24, "end": 658.24, "text": " We first need to replace our input list of vectors with a list of pairs where the first", "tokens": [50364, 492, 700, 643, 281, 7406, 527, 4846, 1329, 295, 18875, 365, 257, 1329, 295, 15494, 689, 264, 700, 50614], "temperature": 0.0, "avg_logprob": -0.10628510728667054, "compression_ratio": 1.64, "no_speech_prob": 0.008846367709338665}, {"id": 135, "seek": 65324, "start": 658.24, "end": 663.5600000000001, "text": " element is the recurrent weight matrix and the second element is the input vector, but", "tokens": [50614, 4478, 307, 264, 18680, 1753, 3364, 8141, 293, 264, 1150, 4478, 307, 264, 4846, 8062, 11, 457, 50880], "temperature": 0.0, "avg_logprob": -0.10628510728667054, "compression_ratio": 1.64, "no_speech_prob": 0.008846367709338665}, {"id": 136, "seek": 65324, "start": 663.5600000000001, "end": 669.6, "text": " then we just perform the scan as usual.", "tokens": [50880, 550, 321, 445, 2042, 264, 11049, 382, 7713, 13, 51182], "temperature": 0.0, "avg_logprob": -0.10628510728667054, "compression_ratio": 1.64, "no_speech_prob": 0.008846367709338665}, {"id": 137, "seek": 65324, "start": 669.6, "end": 674.76, "text": " You can check for yourself that this operator is in fact associative by expanding a few", "tokens": [51182, 509, 393, 1520, 337, 1803, 300, 341, 12973, 307, 294, 1186, 4180, 1166, 538, 14702, 257, 1326, 51440], "temperature": 0.0, "avg_logprob": -0.10628510728667054, "compression_ratio": 1.64, "no_speech_prob": 0.008846367709338665}, {"id": 138, "seek": 65324, "start": 674.76, "end": 678.76, "text": " terms in the other order.", "tokens": [51440, 2115, 294, 264, 661, 1668, 13, 51640], "temperature": 0.0, "avg_logprob": -0.10628510728667054, "compression_ratio": 1.64, "no_speech_prob": 0.008846367709338665}, {"id": 139, "seek": 67876, "start": 678.76, "end": 684.4, "text": " To summarize, we just need to do our parallel cumulative sum algorithm with this operator", "tokens": [50364, 1407, 20858, 11, 321, 445, 643, 281, 360, 527, 8952, 38379, 2408, 9284, 365, 341, 12973, 50646], "temperature": 0.0, "avg_logprob": -0.13042419514757522, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.00690320273861289}, {"id": 140, "seek": 67876, "start": 684.4, "end": 686.6, "text": " in place of addition.", "tokens": [50646, 294, 1081, 295, 4500, 13, 50756], "temperature": 0.0, "avg_logprob": -0.13042419514757522, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.00690320273861289}, {"id": 141, "seek": 67876, "start": 686.6, "end": 693.3199999999999, "text": " And we get the result of a linear recurrent layer in just log n time.", "tokens": [50756, 400, 321, 483, 264, 1874, 295, 257, 8213, 18680, 1753, 4583, 294, 445, 3565, 297, 565, 13, 51092], "temperature": 0.0, "avg_logprob": -0.13042419514757522, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.00690320273861289}, {"id": 142, "seek": 67876, "start": 693.3199999999999, "end": 695.84, "text": " Except for one small problem.", "tokens": [51092, 16192, 337, 472, 1359, 1154, 13, 51218], "temperature": 0.0, "avg_logprob": -0.13042419514757522, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.00690320273861289}, {"id": 143, "seek": 67876, "start": 695.84, "end": 701.04, "text": " If you look closely at this operation, the way it works is by using the first element", "tokens": [51218, 759, 291, 574, 8185, 412, 341, 6916, 11, 264, 636, 309, 1985, 307, 538, 1228, 264, 700, 4478, 51478], "temperature": 0.0, "avg_logprob": -0.13042419514757522, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.00690320273861289}, {"id": 144, "seek": 67876, "start": 701.04, "end": 706.76, "text": " of the tuples as a cumulative matrix, which contains the product of all of the matrices", "tokens": [51478, 295, 264, 2604, 2622, 382, 257, 38379, 8141, 11, 597, 8306, 264, 1674, 295, 439, 295, 264, 32284, 51764], "temperature": 0.0, "avg_logprob": -0.13042419514757522, "compression_ratio": 1.5975103734439835, "no_speech_prob": 0.00690320273861289}, {"id": 145, "seek": 70676, "start": 706.76, "end": 708.52, "text": " seen so far.", "tokens": [50364, 1612, 370, 1400, 13, 50452], "temperature": 0.0, "avg_logprob": -0.13837591108385022, "compression_ratio": 1.5585585585585586, "no_speech_prob": 0.03409307450056076}, {"id": 146, "seek": 70676, "start": 708.52, "end": 715.16, "text": " That's why the first element of the output tuple is the product of the two input matrices.", "tokens": [50452, 663, 311, 983, 264, 700, 4478, 295, 264, 5598, 2604, 781, 307, 264, 1674, 295, 264, 732, 4846, 32284, 13, 50784], "temperature": 0.0, "avg_logprob": -0.13837591108385022, "compression_ratio": 1.5585585585585586, "no_speech_prob": 0.03409307450056076}, {"id": 147, "seek": 70676, "start": 715.16, "end": 721.28, "text": " But this means we're performing a d by d times d by d matrix multiplication in every", "tokens": [50784, 583, 341, 1355, 321, 434, 10205, 257, 274, 538, 274, 1413, 274, 538, 274, 8141, 27290, 294, 633, 51090], "temperature": 0.0, "avg_logprob": -0.13837591108385022, "compression_ratio": 1.5585585585585586, "no_speech_prob": 0.03409307450056076}, {"id": 148, "seek": 70676, "start": 721.28, "end": 725.9, "text": " step, where d is the dimension of the vectors.", "tokens": [51090, 1823, 11, 689, 274, 307, 264, 10139, 295, 264, 18875, 13, 51321], "temperature": 0.0, "avg_logprob": -0.13837591108385022, "compression_ratio": 1.5585585585585586, "no_speech_prob": 0.03409307450056076}, {"id": 149, "seek": 70676, "start": 725.9, "end": 728.48, "text": " This is really slow.", "tokens": [51321, 639, 307, 534, 2964, 13, 51450], "temperature": 0.0, "avg_logprob": -0.13837591108385022, "compression_ratio": 1.5585585585585586, "no_speech_prob": 0.03409307450056076}, {"id": 150, "seek": 70676, "start": 728.48, "end": 733.36, "text": " Note that in the original sequential RNN, we didn't need to keep track of this cumulative", "tokens": [51450, 11633, 300, 294, 264, 3380, 42881, 45702, 45, 11, 321, 994, 380, 643, 281, 1066, 2837, 295, 341, 38379, 51694], "temperature": 0.0, "avg_logprob": -0.13837591108385022, "compression_ratio": 1.5585585585585586, "no_speech_prob": 0.03409307450056076}, {"id": 151, "seek": 73336, "start": 733.36, "end": 738.84, "text": " matrix, and so we only ever multiply the weight matrix with a length d input vector", "tokens": [50364, 8141, 11, 293, 370, 321, 787, 1562, 12972, 264, 3364, 8141, 365, 257, 4641, 274, 4846, 8062, 50638], "temperature": 0.0, "avg_logprob": -0.16837835311889648, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.004467998165637255}, {"id": 152, "seek": 73336, "start": 738.84, "end": 743.44, "text": " at each step, which is a d squared operation.", "tokens": [50638, 412, 1184, 1823, 11, 597, 307, 257, 274, 8889, 6916, 13, 50868], "temperature": 0.0, "avg_logprob": -0.16837835311889648, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.004467998165637255}, {"id": 153, "seek": 73336, "start": 743.44, "end": 748.48, "text": " But now we have to do a d cubed operation in every step.", "tokens": [50868, 583, 586, 321, 362, 281, 360, 257, 274, 36510, 6916, 294, 633, 1823, 13, 51120], "temperature": 0.0, "avg_logprob": -0.16837835311889648, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.004467998165637255}, {"id": 154, "seek": 73336, "start": 748.48, "end": 754.16, "text": " For standard model sizes, this is easily a thousand fault increase in computation.", "tokens": [51120, 1171, 3832, 2316, 11602, 11, 341, 307, 3612, 257, 4714, 7441, 3488, 294, 24903, 13, 51404], "temperature": 0.0, "avg_logprob": -0.16837835311889648, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.004467998165637255}, {"id": 155, "seek": 73336, "start": 754.16, "end": 757.08, "text": " And that's bad.", "tokens": [51404, 400, 300, 311, 1578, 13, 51550], "temperature": 0.0, "avg_logprob": -0.16837835311889648, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.004467998165637255}, {"id": 156, "seek": 73336, "start": 757.08, "end": 760.24, "text": " Fortunately there is a way around this.", "tokens": [51550, 20652, 456, 307, 257, 636, 926, 341, 13, 51708], "temperature": 0.0, "avg_logprob": -0.16837835311889648, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.004467998165637255}, {"id": 157, "seek": 73336, "start": 760.24, "end": 762.88, "text": " Matrix diagonalization.", "tokens": [51708, 36274, 21539, 2144, 13, 51840], "temperature": 0.0, "avg_logprob": -0.16837835311889648, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.004467998165637255}, {"id": 158, "seek": 76288, "start": 762.88, "end": 767.92, "text": " You see, almost every square matrix can be factored into the product of an invertible", "tokens": [50364, 509, 536, 11, 1920, 633, 3732, 8141, 393, 312, 1186, 2769, 666, 264, 1674, 295, 364, 33966, 964, 50616], "temperature": 0.0, "avg_logprob": -0.14728298852610033, "compression_ratio": 1.681592039800995, "no_speech_prob": 0.0002531486679799855}, {"id": 159, "seek": 76288, "start": 767.92, "end": 775.36, "text": " matrix P, a diagonal matrix D, and that first matrix P inverse, so long as the matrix elements", "tokens": [50616, 8141, 430, 11, 257, 21539, 8141, 413, 11, 293, 300, 700, 8141, 430, 17340, 11, 370, 938, 382, 264, 8141, 4959, 50988], "temperature": 0.0, "avg_logprob": -0.14728298852610033, "compression_ratio": 1.681592039800995, "no_speech_prob": 0.0002531486679799855}, {"id": 160, "seek": 76288, "start": 775.36, "end": 778.36, "text": " are allowed to be complex numbers.", "tokens": [50988, 366, 4350, 281, 312, 3997, 3547, 13, 51138], "temperature": 0.0, "avg_logprob": -0.14728298852610033, "compression_ratio": 1.681592039800995, "no_speech_prob": 0.0002531486679799855}, {"id": 161, "seek": 76288, "start": 778.36, "end": 782.68, "text": " Here is an example.", "tokens": [51138, 1692, 307, 364, 1365, 13, 51354], "temperature": 0.0, "avg_logprob": -0.14728298852610033, "compression_ratio": 1.681592039800995, "no_speech_prob": 0.0002531486679799855}, {"id": 162, "seek": 76288, "start": 782.68, "end": 788.04, "text": " Note that this middle matrix is diagonal, that is, all elements except for the main diagonal", "tokens": [51354, 11633, 300, 341, 2808, 8141, 307, 21539, 11, 300, 307, 11, 439, 4959, 3993, 337, 264, 2135, 21539, 51622], "temperature": 0.0, "avg_logprob": -0.14728298852610033, "compression_ratio": 1.681592039800995, "no_speech_prob": 0.0002531486679799855}, {"id": 163, "seek": 76288, "start": 788.04, "end": 790.04, "text": " are zero.", "tokens": [51622, 366, 4018, 13, 51722], "temperature": 0.0, "avg_logprob": -0.14728298852610033, "compression_ratio": 1.681592039800995, "no_speech_prob": 0.0002531486679799855}, {"id": 164, "seek": 79004, "start": 790.04, "end": 794.8, "text": " What's neat about this is when you multiply the matrix by itself in this form, the inner", "tokens": [50364, 708, 311, 10654, 466, 341, 307, 562, 291, 12972, 264, 8141, 538, 2564, 294, 341, 1254, 11, 264, 7284, 50602], "temperature": 0.0, "avg_logprob": -0.1154494571685791, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.04602307453751564}, {"id": 165, "seek": 79004, "start": 794.8, "end": 801.06, "text": " P inverse and P terms cancel, and the product of two diagonal matrices is just the diagonal", "tokens": [50602, 430, 17340, 293, 430, 2115, 10373, 11, 293, 264, 1674, 295, 732, 21539, 32284, 307, 445, 264, 21539, 50915], "temperature": 0.0, "avg_logprob": -0.1154494571685791, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.04602307453751564}, {"id": 166, "seek": 79004, "start": 801.06, "end": 804.0, "text": " matrix with the product of elements.", "tokens": [50915, 8141, 365, 264, 1674, 295, 4959, 13, 51062], "temperature": 0.0, "avg_logprob": -0.1154494571685791, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.04602307453751564}, {"id": 167, "seek": 79004, "start": 804.0, "end": 809.0, "text": " That is, in order to compute d squared, all you need to do is square the elements on the", "tokens": [51062, 663, 307, 11, 294, 1668, 281, 14722, 274, 8889, 11, 439, 291, 643, 281, 360, 307, 3732, 264, 4959, 322, 264, 51312], "temperature": 0.0, "avg_logprob": -0.1154494571685791, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.04602307453751564}, {"id": 168, "seek": 79004, "start": 809.0, "end": 815.56, "text": " main diagonal of d, which can be done in just m operations instead of m cubed.", "tokens": [51312, 2135, 21539, 295, 274, 11, 597, 393, 312, 1096, 294, 445, 275, 7705, 2602, 295, 275, 36510, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1154494571685791, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.04602307453751564}, {"id": 169, "seek": 79004, "start": 815.56, "end": 817.36, "text": " Much better.", "tokens": [51640, 12313, 1101, 13, 51730], "temperature": 0.0, "avg_logprob": -0.1154494571685791, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.04602307453751564}, {"id": 170, "seek": 81736, "start": 817.36, "end": 823.16, "text": " So then what we can do is represent the recurrent weight matrix in diagonalized form, which", "tokens": [50364, 407, 550, 437, 321, 393, 360, 307, 2906, 264, 18680, 1753, 3364, 8141, 294, 21539, 1602, 1254, 11, 597, 50654], "temperature": 0.0, "avg_logprob": -0.11544499772318294, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.0004728406493086368}, {"id": 171, "seek": 81736, "start": 823.16, "end": 828.5600000000001, "text": " means we only need to use a complex vector which contains the elements of the main diagonal", "tokens": [50654, 1355, 321, 787, 643, 281, 764, 257, 3997, 8062, 597, 8306, 264, 4959, 295, 264, 2135, 21539, 50924], "temperature": 0.0, "avg_logprob": -0.11544499772318294, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.0004728406493086368}, {"id": 172, "seek": 81736, "start": 828.5600000000001, "end": 831.6, "text": " of d.", "tokens": [50924, 295, 274, 13, 51076], "temperature": 0.0, "avg_logprob": -0.11544499772318294, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.0004728406493086368}, {"id": 173, "seek": 81736, "start": 831.6, "end": 837.44, "text": " That is to say, we first apply a complex matrix P to the input vectors, then perform the linear", "tokens": [51076, 663, 307, 281, 584, 11, 321, 700, 3079, 257, 3997, 8141, 430, 281, 264, 4846, 18875, 11, 550, 2042, 264, 8213, 51368], "temperature": 0.0, "avg_logprob": -0.11544499772318294, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.0004728406493086368}, {"id": 174, "seek": 81736, "start": 837.44, "end": 843.16, "text": " recurrence with a complex weight vector w, using element-wise multiplication, and finally", "tokens": [51368, 18680, 10760, 365, 257, 3997, 3364, 8062, 261, 11, 1228, 4478, 12, 3711, 27290, 11, 293, 2721, 51654], "temperature": 0.0, "avg_logprob": -0.11544499772318294, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.0004728406493086368}, {"id": 175, "seek": 84316, "start": 843.16, "end": 847.64, "text": " apply P inverse to the output.", "tokens": [50364, 3079, 430, 17340, 281, 264, 5598, 13, 50588], "temperature": 0.0, "avg_logprob": -0.1216985367156647, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.07583948969841003}, {"id": 176, "seek": 84316, "start": 847.64, "end": 852.64, "text": " The result of this will then be equivalent to a linear recurrence for some real-valued", "tokens": [50588, 440, 1874, 295, 341, 486, 550, 312, 10344, 281, 257, 8213, 18680, 10760, 337, 512, 957, 12, 3337, 5827, 50838], "temperature": 0.0, "avg_logprob": -0.1216985367156647, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.07583948969841003}, {"id": 177, "seek": 84316, "start": 852.64, "end": 854.88, "text": " matrix w.", "tokens": [50838, 8141, 261, 13, 50950], "temperature": 0.0, "avg_logprob": -0.1216985367156647, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.07583948969841003}, {"id": 178, "seek": 84316, "start": 854.88, "end": 859.4399999999999, "text": " But when computed this way, the recurrence operator only needs to compute element-wise", "tokens": [50950, 583, 562, 40610, 341, 636, 11, 264, 18680, 10760, 12973, 787, 2203, 281, 14722, 4478, 12, 3711, 51178], "temperature": 0.0, "avg_logprob": -0.1216985367156647, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.07583948969841003}, {"id": 179, "seek": 84316, "start": 859.4399999999999, "end": 866.1999999999999, "text": " multiplication between two vectors to update the cumulative weights instead of matrix multiplication.", "tokens": [51178, 27290, 1296, 732, 18875, 281, 5623, 264, 38379, 17443, 2602, 295, 8141, 27290, 13, 51516], "temperature": 0.0, "avg_logprob": -0.1216985367156647, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.07583948969841003}, {"id": 180, "seek": 84316, "start": 866.1999999999999, "end": 871.28, "text": " When we plug this operator into our parallel scan algorithm, the total compute is now adjust", "tokens": [51516, 1133, 321, 5452, 341, 12973, 666, 527, 8952, 11049, 9284, 11, 264, 3217, 14722, 307, 586, 4369, 51770], "temperature": 0.0, "avg_logprob": -0.1216985367156647, "compression_ratio": 1.6762295081967213, "no_speech_prob": 0.07583948969841003}, {"id": 181, "seek": 87128, "start": 871.28, "end": 875.66, "text": " dn log n, and the parallel runtime is log n.", "tokens": [50364, 274, 77, 3565, 297, 11, 293, 264, 8952, 34474, 307, 3565, 297, 13, 50583], "temperature": 0.0, "avg_logprob": -0.1552939366812658, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.039626020938158035}, {"id": 182, "seek": 87128, "start": 875.66, "end": 878.16, "text": " Much better.", "tokens": [50583, 12313, 1101, 13, 50708], "temperature": 0.0, "avg_logprob": -0.1552939366812658, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.039626020938158035}, {"id": 183, "seek": 87128, "start": 878.16, "end": 882.68, "text": " Note that the parameters of this layer are the complex entries in the recurrent weight", "tokens": [50708, 11633, 300, 264, 9834, 295, 341, 4583, 366, 264, 3997, 23041, 294, 264, 18680, 1753, 3364, 50934], "temperature": 0.0, "avg_logprob": -0.1552939366812658, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.039626020938158035}, {"id": 184, "seek": 87128, "start": 882.68, "end": 888.48, "text": " vector w and the matrix P. In practice, you would just use two separate real numbers to", "tokens": [50934, 8062, 261, 293, 264, 8141, 430, 13, 682, 3124, 11, 291, 576, 445, 764, 732, 4994, 957, 3547, 281, 51224], "temperature": 0.0, "avg_logprob": -0.1552939366812658, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.039626020938158035}, {"id": 185, "seek": 87128, "start": 888.48, "end": 893.3199999999999, "text": " represent the real and imaginary components of each parameter, which are initialized by", "tokens": [51224, 2906, 264, 957, 293, 26164, 6677, 295, 1184, 13075, 11, 597, 366, 5883, 1602, 538, 51466], "temperature": 0.0, "avg_logprob": -0.1552939366812658, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.039626020938158035}, {"id": 186, "seek": 87128, "start": 893.3199999999999, "end": 897.9599999999999, "text": " sampling from a normal distribution centered at zero, and updated with gradient descent", "tokens": [51466, 21179, 490, 257, 2710, 7316, 18988, 412, 4018, 11, 293, 10588, 365, 16235, 23475, 51698], "temperature": 0.0, "avg_logprob": -0.1552939366812658, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.039626020938158035}, {"id": 187, "seek": 87128, "start": 897.9599999999999, "end": 901.1999999999999, "text": " as usual.", "tokens": [51698, 382, 7713, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1552939366812658, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.039626020938158035}, {"id": 188, "seek": 90120, "start": 902.12, "end": 907.6, "text": " Computing matrix inverses is really slow, so in practice we don't bother, and instead", "tokens": [50410, 37804, 278, 8141, 21378, 279, 307, 534, 2964, 11, 370, 294, 3124, 321, 500, 380, 8677, 11, 293, 2602, 50684], "temperature": 0.0, "avg_logprob": -0.12489638328552247, "compression_ratio": 1.7553956834532374, "no_speech_prob": 0.03208271041512489}, {"id": 189, "seek": 90120, "start": 907.6, "end": 913.5600000000001, "text": " just use two independent complex matrices before and after the linear recurrence.", "tokens": [50684, 445, 764, 732, 6695, 3997, 32284, 949, 293, 934, 264, 8213, 18680, 10760, 13, 50982], "temperature": 0.0, "avg_logprob": -0.12489638328552247, "compression_ratio": 1.7553956834532374, "no_speech_prob": 0.03208271041512489}, {"id": 190, "seek": 90120, "start": 913.5600000000001, "end": 917.88, "text": " This actually makes the model more expressive than a real-valued linear RNN, and it saves", "tokens": [50982, 639, 767, 1669, 264, 2316, 544, 40189, 813, 257, 957, 12, 3337, 5827, 8213, 45702, 45, 11, 293, 309, 19155, 51198], "temperature": 0.0, "avg_logprob": -0.12489638328552247, "compression_ratio": 1.7553956834532374, "no_speech_prob": 0.03208271041512489}, {"id": 191, "seek": 90120, "start": 917.88, "end": 923.7800000000001, "text": " computation, but it does mean that the model is no longer equivalent to a real-valued recurrence,", "tokens": [51198, 24903, 11, 457, 309, 775, 914, 300, 264, 2316, 307, 572, 2854, 10344, 281, 257, 957, 12, 3337, 5827, 18680, 10760, 11, 51493], "temperature": 0.0, "avg_logprob": -0.12489638328552247, "compression_ratio": 1.7553956834532374, "no_speech_prob": 0.03208271041512489}, {"id": 192, "seek": 90120, "start": 923.7800000000001, "end": 926.4000000000001, "text": " and the output can now be a complex number.", "tokens": [51493, 293, 264, 5598, 393, 586, 312, 257, 3997, 1230, 13, 51624], "temperature": 0.0, "avg_logprob": -0.12489638328552247, "compression_ratio": 1.7553956834532374, "no_speech_prob": 0.03208271041512489}, {"id": 193, "seek": 90120, "start": 926.4000000000001, "end": 930.9200000000001, "text": " So we will need to take the real-valued part of the output before passing it to the next", "tokens": [51624, 407, 321, 486, 643, 281, 747, 264, 957, 12, 3337, 5827, 644, 295, 264, 5598, 949, 8437, 309, 281, 264, 958, 51850], "temperature": 0.0, "avg_logprob": -0.12489638328552247, "compression_ratio": 1.7553956834532374, "no_speech_prob": 0.03208271041512489}, {"id": 194, "seek": 93092, "start": 931.04, "end": 933.04, "text": " layer.", "tokens": [50370, 4583, 13, 50470], "temperature": 0.0, "avg_logprob": -0.19331638336181642, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.001032175263389945}, {"id": 195, "seek": 93092, "start": 933.04, "end": 938.9599999999999, "text": " Okay, we have seen how to make linear RNNs fast for modern hardware, but what about the", "tokens": [50470, 1033, 11, 321, 362, 1612, 577, 281, 652, 8213, 45702, 45, 82, 2370, 337, 4363, 8837, 11, 457, 437, 466, 264, 50766], "temperature": 0.0, "avg_logprob": -0.19331638336181642, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.001032175263389945}, {"id": 196, "seek": 93092, "start": 938.9599999999999, "end": 940.12, "text": " other problem?", "tokens": [50766, 661, 1154, 30, 50824], "temperature": 0.0, "avg_logprob": -0.19331638336181642, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.001032175263389945}, {"id": 197, "seek": 93092, "start": 940.12, "end": 944.04, "text": " That RNNs are very difficult to train.", "tokens": [50824, 663, 45702, 45, 82, 366, 588, 2252, 281, 3847, 13, 51020], "temperature": 0.0, "avg_logprob": -0.19331638336181642, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.001032175263389945}, {"id": 198, "seek": 93092, "start": 944.04, "end": 949.04, "text": " Before we solve this problem, here is a quick recap of why training RNNs is so problematic", "tokens": [51020, 4546, 321, 5039, 341, 1154, 11, 510, 307, 257, 1702, 20928, 295, 983, 3097, 45702, 45, 82, 307, 370, 19011, 51270], "temperature": 0.0, "avg_logprob": -0.19331638336181642, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.001032175263389945}, {"id": 199, "seek": 93092, "start": 949.04, "end": 950.88, "text": " in the first place.", "tokens": [51270, 294, 264, 700, 1081, 13, 51362], "temperature": 0.0, "avg_logprob": -0.19331638336181642, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.001032175263389945}, {"id": 200, "seek": 93092, "start": 950.88, "end": 955.0, "text": " Neural nets are trained by subtracting the gradient of the loss function from each weight", "tokens": [51362, 1734, 1807, 36170, 366, 8895, 538, 16390, 278, 264, 16235, 295, 264, 4470, 2445, 490, 1184, 3364, 51568], "temperature": 0.0, "avg_logprob": -0.19331638336181642, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.001032175263389945}, {"id": 201, "seek": 93092, "start": 955.0, "end": 956.4399999999999, "text": " in the model.", "tokens": [51568, 294, 264, 2316, 13, 51640], "temperature": 0.0, "avg_logprob": -0.19331638336181642, "compression_ratio": 1.5646551724137931, "no_speech_prob": 0.001032175263389945}, {"id": 202, "seek": 95644, "start": 956.44, "end": 957.44, "text": " What is the gradient?", "tokens": [50364, 708, 307, 264, 16235, 30, 50414], "temperature": 0.0, "avg_logprob": -0.15075476785724082, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.019120296463370323}, {"id": 203, "seek": 95644, "start": 957.44, "end": 964.7600000000001, "text": " Well, imagine evaluating the neural net, then increasing the value of a weight by a very", "tokens": [50414, 1042, 11, 3811, 27479, 264, 18161, 2533, 11, 550, 5662, 264, 2158, 295, 257, 3364, 538, 257, 588, 50780], "temperature": 0.0, "avg_logprob": -0.15075476785724082, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.019120296463370323}, {"id": 204, "seek": 95644, "start": 964.7600000000001, "end": 969.7600000000001, "text": " small amount, and then evaluating it again.", "tokens": [50780, 1359, 2372, 11, 293, 550, 27479, 309, 797, 13, 51030], "temperature": 0.0, "avg_logprob": -0.15075476785724082, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.019120296463370323}, {"id": 205, "seek": 95644, "start": 969.7600000000001, "end": 974.6, "text": " The difference in these scores is proportional to the gradient for that weight, and it tells", "tokens": [51030, 440, 2649, 294, 613, 13444, 307, 24969, 281, 264, 16235, 337, 300, 3364, 11, 293, 309, 5112, 51272], "temperature": 0.0, "avg_logprob": -0.15075476785724082, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.019120296463370323}, {"id": 206, "seek": 95644, "start": 974.6, "end": 979.8800000000001, "text": " you how to change the weight to make the neural net better, so let's evaluate the gradient", "tokens": [51272, 291, 577, 281, 1319, 264, 3364, 281, 652, 264, 18161, 2533, 1101, 11, 370, 718, 311, 13059, 264, 16235, 51536], "temperature": 0.0, "avg_logprob": -0.15075476785724082, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.019120296463370323}, {"id": 207, "seek": 95644, "start": 979.8800000000001, "end": 981.9200000000001, "text": " of a linear recurrent layer.", "tokens": [51536, 295, 257, 8213, 18680, 1753, 4583, 13, 51638], "temperature": 0.0, "avg_logprob": -0.15075476785724082, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.019120296463370323}, {"id": 208, "seek": 98192, "start": 982.8, "end": 987.28, "text": " Actually, to make this a bit easier, let's simplify the model and suppose that every", "tokens": [50408, 5135, 11, 281, 652, 341, 257, 857, 3571, 11, 718, 311, 20460, 264, 2316, 293, 7297, 300, 633, 50632], "temperature": 0.0, "avg_logprob": -0.14095014206906584, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0037067425437271595}, {"id": 209, "seek": 98192, "start": 987.28, "end": 993.16, "text": " input after the first is zero, so we can just ignore them.", "tokens": [50632, 4846, 934, 264, 700, 307, 4018, 11, 370, 321, 393, 445, 11200, 552, 13, 50926], "temperature": 0.0, "avg_logprob": -0.14095014206906584, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0037067425437271595}, {"id": 210, "seek": 98192, "start": 993.16, "end": 997.9599999999999, "text": " When we evaluate the recurrent layer, at each step the previous output is multiplied by", "tokens": [50926, 1133, 321, 13059, 264, 18680, 1753, 4583, 11, 412, 1184, 1823, 264, 3894, 5598, 307, 17207, 538, 51166], "temperature": 0.0, "avg_logprob": -0.14095014206906584, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0037067425437271595}, {"id": 211, "seek": 98192, "start": 997.9599999999999, "end": 1003.36, "text": " the weight vector, so after n steps, the output vector is equal to the recurrent weight vector", "tokens": [51166, 264, 3364, 8062, 11, 370, 934, 297, 4439, 11, 264, 5598, 8062, 307, 2681, 281, 264, 18680, 1753, 3364, 8062, 51436], "temperature": 0.0, "avg_logprob": -0.14095014206906584, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0037067425437271595}, {"id": 212, "seek": 98192, "start": 1003.36, "end": 1008.28, "text": " to the power of n times the first vector, x1.", "tokens": [51436, 281, 264, 1347, 295, 297, 1413, 264, 700, 8062, 11, 2031, 16, 13, 51682], "temperature": 0.0, "avg_logprob": -0.14095014206906584, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0037067425437271595}, {"id": 213, "seek": 100828, "start": 1008.28, "end": 1016.92, "text": " When we increase the weight by a small amount and evaluate it again, we get this.", "tokens": [50364, 1133, 321, 3488, 264, 3364, 538, 257, 1359, 2372, 293, 13059, 309, 797, 11, 321, 483, 341, 13, 50796], "temperature": 0.0, "avg_logprob": -0.13576692029049522, "compression_ratio": 1.5114942528735633, "no_speech_prob": 0.004331034608185291}, {"id": 214, "seek": 100828, "start": 1016.92, "end": 1023.04, "text": " Taking the difference, we get up to a constant scaling factor, w to the power of n-1 times", "tokens": [50796, 17837, 264, 2649, 11, 321, 483, 493, 281, 257, 5754, 21589, 5952, 11, 261, 281, 264, 1347, 295, 297, 12, 16, 1413, 51102], "temperature": 0.0, "avg_logprob": -0.13576692029049522, "compression_ratio": 1.5114942528735633, "no_speech_prob": 0.004331034608185291}, {"id": 215, "seek": 100828, "start": 1023.04, "end": 1024.68, "text": " x1.", "tokens": [51102, 2031, 16, 13, 51184], "temperature": 0.0, "avg_logprob": -0.13576692029049522, "compression_ratio": 1.5114942528735633, "no_speech_prob": 0.004331034608185291}, {"id": 216, "seek": 100828, "start": 1024.68, "end": 1031.0, "text": " The problem here is that as n becomes large, this term, w to the n-1, either gets very", "tokens": [51184, 440, 1154, 510, 307, 300, 382, 297, 3643, 2416, 11, 341, 1433, 11, 261, 281, 264, 297, 12, 16, 11, 2139, 2170, 588, 51500], "temperature": 0.0, "avg_logprob": -0.13576692029049522, "compression_ratio": 1.5114942528735633, "no_speech_prob": 0.004331034608185291}, {"id": 217, "seek": 103100, "start": 1031.0, "end": 1036.84, "text": " small or very large, depending on whether the values in w are less than or greater than", "tokens": [50364, 1359, 420, 588, 2416, 11, 5413, 322, 1968, 264, 4190, 294, 261, 366, 1570, 813, 420, 5044, 813, 50656], "temperature": 0.0, "avg_logprob": -0.10314972877502442, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.09531834721565247}, {"id": 218, "seek": 103100, "start": 1036.84, "end": 1039.44, "text": " 1.", "tokens": [50656, 502, 13, 50786], "temperature": 0.0, "avg_logprob": -0.10314972877502442, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.09531834721565247}, {"id": 219, "seek": 103100, "start": 1039.44, "end": 1041.96, "text": " In either case, it's a problem.", "tokens": [50786, 682, 2139, 1389, 11, 309, 311, 257, 1154, 13, 50912], "temperature": 0.0, "avg_logprob": -0.10314972877502442, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.09531834721565247}, {"id": 220, "seek": 103100, "start": 1041.96, "end": 1047.0, "text": " If the gradient is very large, then the neural net weights change too much, and the existing", "tokens": [50912, 759, 264, 16235, 307, 588, 2416, 11, 550, 264, 18161, 2533, 17443, 1319, 886, 709, 11, 293, 264, 6741, 51164], "temperature": 0.0, "avg_logprob": -0.10314972877502442, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.09531834721565247}, {"id": 221, "seek": 103100, "start": 1047.0, "end": 1051.24, "text": " functionality already learned by the neural net gets destroyed.", "tokens": [51164, 14980, 1217, 3264, 538, 264, 18161, 2533, 2170, 8937, 13, 51376], "temperature": 0.0, "avg_logprob": -0.10314972877502442, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.09531834721565247}, {"id": 222, "seek": 103100, "start": 1051.24, "end": 1055.56, "text": " If the gradient is very small, then the weights don't change enough, and the neural net doesn't", "tokens": [51376, 759, 264, 16235, 307, 588, 1359, 11, 550, 264, 17443, 500, 380, 1319, 1547, 11, 293, 264, 18161, 2533, 1177, 380, 51592], "temperature": 0.0, "avg_logprob": -0.10314972877502442, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.09531834721565247}, {"id": 223, "seek": 103100, "start": 1055.56, "end": 1058.22, "text": " learn anything at all.", "tokens": [51592, 1466, 1340, 412, 439, 13, 51725], "temperature": 0.0, "avg_logprob": -0.10314972877502442, "compression_ratio": 1.8425925925925926, "no_speech_prob": 0.09531834721565247}, {"id": 224, "seek": 105822, "start": 1058.22, "end": 1061.5, "text": " This is what makes training RNNs difficult.", "tokens": [50364, 639, 307, 437, 1669, 3097, 45702, 45, 82, 2252, 13, 50528], "temperature": 0.0, "avg_logprob": -0.10123662765209492, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.031131163239479065}, {"id": 225, "seek": 105822, "start": 1061.5, "end": 1067.66, "text": " While in principle, RNNs can use infinitely long context, in practice, with gradient-based", "tokens": [50528, 3987, 294, 8665, 11, 45702, 45, 82, 393, 764, 36227, 938, 4319, 11, 294, 3124, 11, 365, 16235, 12, 6032, 50836], "temperature": 0.0, "avg_logprob": -0.10123662765209492, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.031131163239479065}, {"id": 226, "seek": 105822, "start": 1067.66, "end": 1073.56, "text": " training techniques, the RNN will only learn to use context for as many steps as the gradient", "tokens": [50836, 3097, 7512, 11, 264, 45702, 45, 486, 787, 1466, 281, 764, 4319, 337, 382, 867, 4439, 382, 264, 16235, 51131], "temperature": 0.0, "avg_logprob": -0.10123662765209492, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.031131163239479065}, {"id": 227, "seek": 105822, "start": 1073.56, "end": 1076.34, "text": " remains the right size for learning.", "tokens": [51131, 7023, 264, 558, 2744, 337, 2539, 13, 51270], "temperature": 0.0, "avg_logprob": -0.10123662765209492, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.031131163239479065}, {"id": 228, "seek": 105822, "start": 1076.34, "end": 1081.58, "text": " This is known as the problem of vanishing and exploding gradients.", "tokens": [51270, 639, 307, 2570, 382, 264, 1154, 295, 3161, 3807, 293, 35175, 2771, 2448, 13, 51532], "temperature": 0.0, "avg_logprob": -0.10123662765209492, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.031131163239479065}, {"id": 229, "seek": 105822, "start": 1081.58, "end": 1086.7, "text": " And when we add back in non-zero inputs, this problem only gets worse, as the additional", "tokens": [51532, 400, 562, 321, 909, 646, 294, 2107, 12, 32226, 15743, 11, 341, 1154, 787, 2170, 5324, 11, 382, 264, 4497, 51788], "temperature": 0.0, "avg_logprob": -0.10123662765209492, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.031131163239479065}, {"id": 230, "seek": 108670, "start": 1086.74, "end": 1091.26, "text": " inputs make the gradients even more unstable.", "tokens": [50366, 15743, 652, 264, 2771, 2448, 754, 544, 23742, 13, 50592], "temperature": 0.0, "avg_logprob": -0.10672277814886544, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0488458089530468}, {"id": 231, "seek": 108670, "start": 1091.26, "end": 1096.3400000000001, "text": " And to be clear, the reason why this isn't a problem for regular neural nets is because", "tokens": [50592, 400, 281, 312, 1850, 11, 264, 1778, 983, 341, 1943, 380, 257, 1154, 337, 3890, 18161, 36170, 307, 570, 50846], "temperature": 0.0, "avg_logprob": -0.10672277814886544, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0488458089530468}, {"id": 232, "seek": 108670, "start": 1096.3400000000001, "end": 1100.1000000000001, "text": " they use different weights in each layer.", "tokens": [50846, 436, 764, 819, 17443, 294, 1184, 4583, 13, 51034], "temperature": 0.0, "avg_logprob": -0.10672277814886544, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0488458089530468}, {"id": 233, "seek": 108670, "start": 1100.1000000000001, "end": 1103.78, "text": " Some layers can have weights smaller than one, and some layers can have weights larger", "tokens": [51034, 2188, 7914, 393, 362, 17443, 4356, 813, 472, 11, 293, 512, 7914, 393, 362, 17443, 4833, 51218], "temperature": 0.0, "avg_logprob": -0.10672277814886544, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0488458089530468}, {"id": 234, "seek": 108670, "start": 1103.78, "end": 1108.82, "text": " than one, so long as the gradient remains about the same size, the neural net will be able", "tokens": [51218, 813, 472, 11, 370, 938, 382, 264, 16235, 7023, 466, 264, 912, 2744, 11, 264, 18161, 2533, 486, 312, 1075, 51470], "temperature": 0.0, "avg_logprob": -0.10672277814886544, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0488458089530468}, {"id": 235, "seek": 108670, "start": 1108.82, "end": 1112.66, "text": " to learn.", "tokens": [51470, 281, 1466, 13, 51662], "temperature": 0.0, "avg_logprob": -0.10672277814886544, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0488458089530468}, {"id": 236, "seek": 111266, "start": 1112.74, "end": 1117.6200000000001, "text": " There are lots and lots of different configurations of weights that result in stable gradients,", "tokens": [50368, 821, 366, 3195, 293, 3195, 295, 819, 31493, 295, 17443, 300, 1874, 294, 8351, 2771, 2448, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1312601195441352, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.09261000901460648}, {"id": 237, "seek": 111266, "start": 1117.6200000000001, "end": 1124.22, "text": " and it's easy to stay in stable configurations all throughout training.", "tokens": [50612, 293, 309, 311, 1858, 281, 1754, 294, 8351, 31493, 439, 3710, 3097, 13, 50942], "temperature": 0.0, "avg_logprob": -0.1312601195441352, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.09261000901460648}, {"id": 238, "seek": 111266, "start": 1124.22, "end": 1130.14, "text": " But for RNNs, you're using the same weight in each step, so there is exactly one stable", "tokens": [50942, 583, 337, 45702, 45, 82, 11, 291, 434, 1228, 264, 912, 3364, 294, 1184, 1823, 11, 370, 456, 307, 2293, 472, 8351, 51238], "temperature": 0.0, "avg_logprob": -0.1312601195441352, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.09261000901460648}, {"id": 239, "seek": 111266, "start": 1130.14, "end": 1134.18, "text": " configuration, which is when the weight is one.", "tokens": [51238, 11694, 11, 597, 307, 562, 264, 3364, 307, 472, 13, 51440], "temperature": 0.0, "avg_logprob": -0.1312601195441352, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.09261000901460648}, {"id": 240, "seek": 111266, "start": 1134.18, "end": 1141.02, "text": " Any deviation from one, and you have exponentially growing or decaying gradients.", "tokens": [51440, 2639, 25163, 490, 472, 11, 293, 291, 362, 37330, 4194, 420, 21039, 278, 2771, 2448, 13, 51782], "temperature": 0.0, "avg_logprob": -0.1312601195441352, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.09261000901460648}, {"id": 241, "seek": 114102, "start": 1141.02, "end": 1145.98, "text": " Note that when the weights are complex numbers, the same argument applies just using the absolute", "tokens": [50364, 11633, 300, 562, 264, 17443, 366, 3997, 3547, 11, 264, 912, 6770, 13165, 445, 1228, 264, 8236, 50612], "temperature": 0.0, "avg_logprob": -0.19267547831815832, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0012446345062926412}, {"id": 242, "seek": 114102, "start": 1145.98, "end": 1149.3, "text": " value of the weights.", "tokens": [50612, 2158, 295, 264, 17443, 13, 50778], "temperature": 0.0, "avg_logprob": -0.19267547831815832, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0012446345062926412}, {"id": 243, "seek": 114102, "start": 1149.3, "end": 1152.42, "text": " So how can we fix vanishing and exploding gradients?", "tokens": [50778, 407, 577, 393, 321, 3191, 3161, 3807, 293, 35175, 2771, 2448, 30, 50934], "temperature": 0.0, "avg_logprob": -0.19267547831815832, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0012446345062926412}, {"id": 244, "seek": 114102, "start": 1152.42, "end": 1158.58, "text": " Well, we saw that the RNN gradients are stable, so long as their current weights are one,", "tokens": [50934, 1042, 11, 321, 1866, 300, 264, 45702, 45, 2771, 2448, 366, 8351, 11, 370, 938, 382, 641, 2190, 17443, 366, 472, 11, 51242], "temperature": 0.0, "avg_logprob": -0.19267547831815832, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0012446345062926412}, {"id": 245, "seek": 114102, "start": 1158.58, "end": 1161.1, "text": " and the inputs are zero.", "tokens": [51242, 293, 264, 15743, 366, 4018, 13, 51368], "temperature": 0.0, "avg_logprob": -0.19267547831815832, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0012446345062926412}, {"id": 246, "seek": 114102, "start": 1161.1, "end": 1167.58, "text": " So in the linear RNN paper, the authors propose to initialize their linear RNNs in this stable", "tokens": [51368, 407, 294, 264, 8213, 45702, 45, 3035, 11, 264, 16552, 17421, 281, 5883, 1125, 641, 8213, 45702, 45, 82, 294, 341, 8351, 51692], "temperature": 0.0, "avg_logprob": -0.19267547831815832, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0012446345062926412}, {"id": 247, "seek": 114102, "start": 1167.58, "end": 1169.7, "text": " state.", "tokens": [51692, 1785, 13, 51798], "temperature": 0.0, "avg_logprob": -0.19267547831815832, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0012446345062926412}, {"id": 248, "seek": 116970, "start": 1169.7, "end": 1175.14, "text": " Specifically, they parameterize the weights in complex polar form A times E to the IB,", "tokens": [50364, 26058, 11, 436, 13075, 1125, 264, 17443, 294, 3997, 12367, 1254, 316, 1413, 462, 281, 264, 286, 33, 11, 50636], "temperature": 0.0, "avg_logprob": -0.1303203771780203, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.008845420554280281}, {"id": 249, "seek": 116970, "start": 1175.14, "end": 1177.82, "text": " where A is magnitude and B is angle.", "tokens": [50636, 689, 316, 307, 15668, 293, 363, 307, 5802, 13, 50770], "temperature": 0.0, "avg_logprob": -0.1303203771780203, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.008845420554280281}, {"id": 250, "seek": 116970, "start": 1177.82, "end": 1182.74, "text": " They then restrict the magnitude to be less than one by running A through this E to the", "tokens": [50770, 814, 550, 7694, 264, 15668, 281, 312, 1570, 813, 472, 538, 2614, 316, 807, 341, 462, 281, 264, 51016], "temperature": 0.0, "avg_logprob": -0.1303203771780203, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.008845420554280281}, {"id": 251, "seek": 116970, "start": 1182.74, "end": 1188.54, "text": " minus E function, which always outputs a number between zero and one.", "tokens": [51016, 3175, 462, 2445, 11, 597, 1009, 23930, 257, 1230, 1296, 4018, 293, 472, 13, 51306], "temperature": 0.0, "avg_logprob": -0.1303203771780203, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.008845420554280281}, {"id": 252, "seek": 116970, "start": 1188.54, "end": 1192.78, "text": " And instead of randomly sampling A from a normal distribution centered at zero, as we", "tokens": [51306, 400, 2602, 295, 16979, 21179, 316, 490, 257, 2710, 7316, 18988, 412, 4018, 11, 382, 321, 51518], "temperature": 0.0, "avg_logprob": -0.1303203771780203, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.008845420554280281}, {"id": 253, "seek": 116970, "start": 1192.78, "end": 1198.6200000000001, "text": " usually do, they initialize A so that the magnitude of E to the minus E to the A is", "tokens": [51518, 2673, 360, 11, 436, 5883, 1125, 316, 370, 300, 264, 15668, 295, 462, 281, 264, 3175, 462, 281, 264, 316, 307, 51810], "temperature": 0.0, "avg_logprob": -0.1303203771780203, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.008845420554280281}, {"id": 254, "seek": 119862, "start": 1198.62, "end": 1203.62, "text": " uniformly distributed between 0.999 and one.", "tokens": [50364, 48806, 12631, 1296, 1958, 13, 49017, 293, 472, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1562872846075829, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.012429610826075077}, {"id": 255, "seek": 119862, "start": 1203.62, "end": 1209.06, "text": " They initialize the angle B uniformly between zero and pi on 10 radians.", "tokens": [50614, 814, 5883, 1125, 264, 5802, 363, 48806, 1296, 4018, 293, 3895, 322, 1266, 2843, 2567, 13, 50886], "temperature": 0.0, "avg_logprob": -0.1562872846075829, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.012429610826075077}, {"id": 256, "seek": 119862, "start": 1209.06, "end": 1213.82, "text": " This ensures that, at initialization, the weights are all very close to one.", "tokens": [50886, 639, 28111, 300, 11, 412, 5883, 2144, 11, 264, 17443, 366, 439, 588, 1998, 281, 472, 13, 51124], "temperature": 0.0, "avg_logprob": -0.1562872846075829, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.012429610826075077}, {"id": 257, "seek": 119862, "start": 1213.82, "end": 1219.62, "text": " Finally, they multiply the inputs by delta, which is another learned parameter initialized", "tokens": [51124, 6288, 11, 436, 12972, 264, 15743, 538, 8289, 11, 597, 307, 1071, 3264, 13075, 5883, 1602, 51414], "temperature": 0.0, "avg_logprob": -0.1562872846075829, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.012429610826075077}, {"id": 258, "seek": 119862, "start": 1219.62, "end": 1226.3, "text": " to the square root of one minus E to the minus E to the A, which since E to the minus E to", "tokens": [51414, 281, 264, 3732, 5593, 295, 472, 3175, 462, 281, 264, 3175, 462, 281, 264, 316, 11, 597, 1670, 462, 281, 264, 3175, 462, 281, 51748], "temperature": 0.0, "avg_logprob": -0.1562872846075829, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.012429610826075077}, {"id": 259, "seek": 122630, "start": 1226.3, "end": 1230.82, "text": " the A is close to one, this is some very small number.", "tokens": [50364, 264, 316, 307, 1998, 281, 472, 11, 341, 307, 512, 588, 1359, 1230, 13, 50590], "temperature": 0.0, "avg_logprob": -0.12888674779769477, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.031138407066464424}, {"id": 260, "seek": 122630, "start": 1230.82, "end": 1234.8999999999999, "text": " This ensures that, at initialization, the inputs are all close to zero, and so they", "tokens": [50590, 639, 28111, 300, 11, 412, 5883, 2144, 11, 264, 15743, 366, 439, 1998, 281, 4018, 11, 293, 370, 436, 50794], "temperature": 0.0, "avg_logprob": -0.12888674779769477, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.031138407066464424}, {"id": 261, "seek": 122630, "start": 1234.8999999999999, "end": 1237.54, "text": " don't interfere with the recurrence.", "tokens": [50794, 500, 380, 23946, 365, 264, 18680, 10760, 13, 50926], "temperature": 0.0, "avg_logprob": -0.12888674779769477, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.031138407066464424}, {"id": 262, "seek": 122630, "start": 1237.54, "end": 1244.74, "text": " So at initialization, this model is approximately the same as the stable RNN I showed you before.", "tokens": [50926, 407, 412, 5883, 2144, 11, 341, 2316, 307, 10447, 264, 912, 382, 264, 8351, 45702, 45, 286, 4712, 291, 949, 13, 51286], "temperature": 0.0, "avg_logprob": -0.12888674779769477, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.031138407066464424}, {"id": 263, "seek": 122630, "start": 1244.74, "end": 1248.7, "text": " After the model begins training and the weights change, there's no guarantee that it will", "tokens": [51286, 2381, 264, 2316, 7338, 3097, 293, 264, 17443, 1319, 11, 456, 311, 572, 10815, 300, 309, 486, 51484], "temperature": 0.0, "avg_logprob": -0.12888674779769477, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.031138407066464424}, {"id": 264, "seek": 122630, "start": 1248.7, "end": 1253.72, "text": " remain stable, but in practice it appears that just initializing the model like this", "tokens": [51484, 6222, 8351, 11, 457, 294, 3124, 309, 7038, 300, 445, 5883, 3319, 264, 2316, 411, 341, 51735], "temperature": 0.0, "avg_logprob": -0.12888674779769477, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.031138407066464424}, {"id": 265, "seek": 125372, "start": 1253.72, "end": 1259.22, "text": " is sufficient to allow it to learn to remember context for tens of thousands of steps.", "tokens": [50364, 307, 11563, 281, 2089, 309, 281, 1466, 281, 1604, 4319, 337, 10688, 295, 5383, 295, 4439, 13, 50639], "temperature": 0.0, "avg_logprob": -0.12479780158218072, "compression_ratio": 1.676, "no_speech_prob": 0.05182530730962753}, {"id": 266, "seek": 125372, "start": 1259.22, "end": 1264.72, "text": " And there we have it, with these modifications, we now have a linear RNN that is faster compute", "tokens": [50639, 400, 456, 321, 362, 309, 11, 365, 613, 26881, 11, 321, 586, 362, 257, 8213, 45702, 45, 300, 307, 4663, 14722, 50914], "temperature": 0.0, "avg_logprob": -0.12479780158218072, "compression_ratio": 1.676, "no_speech_prob": 0.05182530730962753}, {"id": 267, "seek": 125372, "start": 1264.72, "end": 1268.32, "text": " and learns to use extremely long context.", "tokens": [50914, 293, 27152, 281, 764, 4664, 938, 4319, 13, 51094], "temperature": 0.0, "avg_logprob": -0.12479780158218072, "compression_ratio": 1.676, "no_speech_prob": 0.05182530730962753}, {"id": 268, "seek": 125372, "start": 1268.32, "end": 1273.32, "text": " In the linear RNN paper, they evaluate this model on the long range arena benchmark, which", "tokens": [51094, 682, 264, 8213, 45702, 45, 3035, 11, 436, 13059, 341, 2316, 322, 264, 938, 3613, 18451, 18927, 11, 597, 51344], "temperature": 0.0, "avg_logprob": -0.12479780158218072, "compression_ratio": 1.676, "no_speech_prob": 0.05182530730962753}, {"id": 269, "seek": 125372, "start": 1273.32, "end": 1278.56, "text": " is a collection of six synthetic tasks that evaluate a model's ability to perform long", "tokens": [51344, 307, 257, 5765, 295, 2309, 23420, 9608, 300, 13059, 257, 2316, 311, 3485, 281, 2042, 938, 51606], "temperature": 0.0, "avg_logprob": -0.12479780158218072, "compression_ratio": 1.676, "no_speech_prob": 0.05182530730962753}, {"id": 270, "seek": 125372, "start": 1278.56, "end": 1281.72, "text": " range reasoning.", "tokens": [51606, 3613, 21577, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12479780158218072, "compression_ratio": 1.676, "no_speech_prob": 0.05182530730962753}, {"id": 271, "seek": 128172, "start": 1281.72, "end": 1286.72, "text": " For example, in the path X task, the model must classify images as whether or not they", "tokens": [50364, 1171, 1365, 11, 294, 264, 3100, 1783, 5633, 11, 264, 2316, 1633, 33872, 5267, 382, 1968, 420, 406, 436, 50614], "temperature": 0.0, "avg_logprob": -0.0948594055677715, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.0003569613618310541}, {"id": 272, "seek": 128172, "start": 1286.72, "end": 1294.0, "text": " contain a complete dotted path between two circles, except that the images are flattened", "tokens": [50614, 5304, 257, 3566, 37459, 3100, 1296, 732, 13040, 11, 3993, 300, 264, 5267, 366, 24183, 292, 50978], "temperature": 0.0, "avg_logprob": -0.0948594055677715, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.0003569613618310541}, {"id": 273, "seek": 128172, "start": 1294.0, "end": 1301.96, "text": " into one long sequence of 16,000 pixels.", "tokens": [50978, 666, 472, 938, 8310, 295, 3165, 11, 1360, 18668, 13, 51376], "temperature": 0.0, "avg_logprob": -0.0948594055677715, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.0003569613618310541}, {"id": 274, "seek": 128172, "start": 1301.96, "end": 1307.72, "text": " The linear RNN achieved state of the art performance on the long range arena, outperforming transformers", "tokens": [51376, 440, 8213, 45702, 45, 11042, 1785, 295, 264, 1523, 3389, 322, 264, 938, 3613, 18451, 11, 484, 26765, 278, 4088, 433, 51664], "temperature": 0.0, "avg_logprob": -0.0948594055677715, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.0003569613618310541}, {"id": 275, "seek": 130772, "start": 1307.72, "end": 1314.48, "text": " by about 33% on average across tasks.", "tokens": [50364, 538, 466, 11816, 4, 322, 4274, 2108, 9608, 13, 50702], "temperature": 0.0, "avg_logprob": -0.10538782971970578, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.09531157463788986}, {"id": 276, "seek": 130772, "start": 1314.48, "end": 1319.16, "text": " So now that we understand the linear RNN, what's with all the talk about state space", "tokens": [50702, 407, 586, 300, 321, 1223, 264, 8213, 45702, 45, 11, 437, 311, 365, 439, 264, 751, 466, 1785, 1901, 50936], "temperature": 0.0, "avg_logprob": -0.10538782971970578, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.09531157463788986}, {"id": 277, "seek": 130772, "start": 1319.16, "end": 1320.16, "text": " models?", "tokens": [50936, 5245, 30, 50986], "temperature": 0.0, "avg_logprob": -0.10538782971970578, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.09531157463788986}, {"id": 278, "seek": 130772, "start": 1320.16, "end": 1326.04, "text": " Well, it turns out that state space models are just linear RNNs.", "tokens": [50986, 1042, 11, 309, 4523, 484, 300, 1785, 1901, 5245, 366, 445, 8213, 45702, 45, 82, 13, 51280], "temperature": 0.0, "avg_logprob": -0.10538782971970578, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.09531157463788986}, {"id": 279, "seek": 130772, "start": 1326.04, "end": 1330.3600000000001, "text": " State space models were inspired by a control theory and were derived from a totally different", "tokens": [51280, 4533, 1901, 5245, 645, 7547, 538, 257, 1969, 5261, 293, 645, 18949, 490, 257, 3879, 819, 51496], "temperature": 0.0, "avg_logprob": -0.10538782971970578, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.09531157463788986}, {"id": 280, "seek": 130772, "start": 1330.3600000000001, "end": 1336.24, "text": " idea of trying to discretize a continuous dynamical system, but the end result is just", "tokens": [51496, 1558, 295, 1382, 281, 25656, 1125, 257, 10957, 5999, 804, 1185, 11, 457, 264, 917, 1874, 307, 445, 51790], "temperature": 0.0, "avg_logprob": -0.10538782971970578, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.09531157463788986}, {"id": 281, "seek": 133624, "start": 1336.24, "end": 1340.92, "text": " a linear RNN with a slightly different initialization scheme.", "tokens": [50364, 257, 8213, 45702, 45, 365, 257, 4748, 819, 5883, 2144, 12232, 13, 50598], "temperature": 0.0, "avg_logprob": -0.16004205458235032, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.31372585892677307}, {"id": 282, "seek": 133624, "start": 1340.92, "end": 1346.68, "text": " The most common form of state space model parameterizes each recurrent weight as w equals e to the", "tokens": [50598, 440, 881, 2689, 1254, 295, 1785, 1901, 2316, 13075, 5660, 1184, 18680, 1753, 3364, 382, 261, 6915, 308, 281, 264, 50886], "temperature": 0.0, "avg_logprob": -0.16004205458235032, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.31372585892677307}, {"id": 283, "seek": 133624, "start": 1346.68, "end": 1352.48, "text": " delta times a plus bi, where delta is again a learnable parameter, which is initialized", "tokens": [50886, 8289, 1413, 257, 1804, 3228, 11, 689, 8289, 307, 797, 257, 1466, 712, 13075, 11, 597, 307, 5883, 1602, 51176], "temperature": 0.0, "avg_logprob": -0.16004205458235032, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.31372585892677307}, {"id": 284, "seek": 133624, "start": 1352.48, "end": 1358.88, "text": " to a very small number, usually between 0.0001 and 0.1.", "tokens": [51176, 281, 257, 588, 1359, 1230, 11, 2673, 1296, 1958, 13, 1360, 16, 293, 1958, 13, 16, 13, 51496], "temperature": 0.0, "avg_logprob": -0.16004205458235032, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.31372585892677307}, {"id": 285, "seek": 133624, "start": 1358.88, "end": 1363.08, "text": " Multiplying the weight by a small number makes it close to zero, and when you take e to the", "tokens": [51496, 31150, 7310, 264, 3364, 538, 257, 1359, 1230, 1669, 309, 1998, 281, 4018, 11, 293, 562, 291, 747, 308, 281, 264, 51706], "temperature": 0.0, "avg_logprob": -0.16004205458235032, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.31372585892677307}, {"id": 286, "seek": 136308, "start": 1363.08, "end": 1366.84, "text": " power of something close to zero, you get something close to one.", "tokens": [50364, 1347, 295, 746, 1998, 281, 4018, 11, 291, 483, 746, 1998, 281, 472, 13, 50552], "temperature": 0.0, "avg_logprob": -0.13926248096284413, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.02297336980700493}, {"id": 287, "seek": 136308, "start": 1366.84, "end": 1370.84, "text": " This again ensures that at initialization, the recurrent weights are all approximately", "tokens": [50552, 639, 797, 28111, 300, 412, 5883, 2144, 11, 264, 18680, 1753, 17443, 366, 439, 10447, 50752], "temperature": 0.0, "avg_logprob": -0.13926248096284413, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.02297336980700493}, {"id": 288, "seek": 136308, "start": 1370.84, "end": 1373.8799999999999, "text": " one, so training is stable.", "tokens": [50752, 472, 11, 370, 3097, 307, 8351, 13, 50904], "temperature": 0.0, "avg_logprob": -0.13926248096284413, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.02297336980700493}, {"id": 289, "seek": 136308, "start": 1373.8799999999999, "end": 1380.0, "text": " State space models also multiply the inputs by delta times a plus bi inverse times w minus", "tokens": [50904, 4533, 1901, 5245, 611, 12972, 264, 15743, 538, 8289, 1413, 257, 1804, 3228, 17340, 1413, 261, 3175, 51210], "temperature": 0.0, "avg_logprob": -0.13926248096284413, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.02297336980700493}, {"id": 290, "seek": 136308, "start": 1380.0, "end": 1384.72, "text": " one, because that's what's prescribed by a control theory, but empirically, you get", "tokens": [51210, 472, 11, 570, 300, 311, 437, 311, 29099, 538, 257, 1969, 5261, 11, 457, 25790, 984, 11, 291, 483, 51446], "temperature": 0.0, "avg_logprob": -0.13926248096284413, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.02297336980700493}, {"id": 291, "seek": 136308, "start": 1384.72, "end": 1391.4399999999998, "text": " the same performance when you just scale the inputs by delta as in the linear RNN setup.", "tokens": [51446, 264, 912, 3389, 562, 291, 445, 4373, 264, 15743, 538, 8289, 382, 294, 264, 8213, 45702, 45, 8657, 13, 51782], "temperature": 0.0, "avg_logprob": -0.13926248096284413, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.02297336980700493}, {"id": 292, "seek": 139144, "start": 1391.44, "end": 1396.0800000000002, "text": " From the long range arena, the control theory inspired state space initialization performs", "tokens": [50364, 3358, 264, 938, 3613, 18451, 11, 264, 1969, 5261, 7547, 1785, 1901, 5883, 2144, 26213, 50596], "temperature": 0.0, "avg_logprob": -0.1335949811068448, "compression_ratio": 1.8477366255144032, "no_speech_prob": 0.23916839063167572}, {"id": 293, "seek": 139144, "start": 1396.0800000000002, "end": 1399.3600000000001, "text": " roughly the same as the linear RNN initialization.", "tokens": [50596, 9810, 264, 912, 382, 264, 8213, 45702, 45, 5883, 2144, 13, 50760], "temperature": 0.0, "avg_logprob": -0.1335949811068448, "compression_ratio": 1.8477366255144032, "no_speech_prob": 0.23916839063167572}, {"id": 294, "seek": 139144, "start": 1399.3600000000001, "end": 1404.48, "text": " Anyway, whenever you hear state space model, think linear RNN.", "tokens": [50760, 5684, 11, 5699, 291, 1568, 1785, 1901, 2316, 11, 519, 8213, 45702, 45, 13, 51016], "temperature": 0.0, "avg_logprob": -0.1335949811068448, "compression_ratio": 1.8477366255144032, "no_speech_prob": 0.23916839063167572}, {"id": 295, "seek": 139144, "start": 1404.48, "end": 1406.88, "text": " And finally, we can talk about Mamba.", "tokens": [51016, 400, 2721, 11, 321, 393, 751, 466, 376, 23337, 13, 51136], "temperature": 0.0, "avg_logprob": -0.1335949811068448, "compression_ratio": 1.8477366255144032, "no_speech_prob": 0.23916839063167572}, {"id": 296, "seek": 139144, "start": 1406.88, "end": 1412.2, "text": " You see, while linear RNNs do perform really well on the long range arena benchmark, this", "tokens": [51136, 509, 536, 11, 1339, 8213, 45702, 45, 82, 360, 2042, 534, 731, 322, 264, 938, 3613, 18451, 18927, 11, 341, 51402], "temperature": 0.0, "avg_logprob": -0.1335949811068448, "compression_ratio": 1.8477366255144032, "no_speech_prob": 0.23916839063167572}, {"id": 297, "seek": 139144, "start": 1412.2, "end": 1415.2, "text": " does not mean they are good language models.", "tokens": [51402, 775, 406, 914, 436, 366, 665, 2856, 5245, 13, 51552], "temperature": 0.0, "avg_logprob": -0.1335949811068448, "compression_ratio": 1.8477366255144032, "no_speech_prob": 0.23916839063167572}, {"id": 298, "seek": 139144, "start": 1415.2, "end": 1420.4, "text": " For language modeling, linear RNNs perform way worse than transformers.", "tokens": [51552, 1171, 2856, 15983, 11, 8213, 45702, 45, 82, 2042, 636, 5324, 813, 4088, 433, 13, 51812], "temperature": 0.0, "avg_logprob": -0.1335949811068448, "compression_ratio": 1.8477366255144032, "no_speech_prob": 0.23916839063167572}, {"id": 299, "seek": 142040, "start": 1420.52, "end": 1426.16, "text": " Whether the performance of various state-of-the-art language models lower is better on this graph.", "tokens": [50370, 8503, 264, 3389, 295, 3683, 1785, 12, 2670, 12, 3322, 12, 446, 2856, 5245, 3126, 307, 1101, 322, 341, 4295, 13, 50652], "temperature": 0.0, "avg_logprob": -0.14935742172540403, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.0533839650452137}, {"id": 300, "seek": 142040, "start": 1426.16, "end": 1431.2, "text": " As you can see, everything including state space models does significantly worse than", "tokens": [50652, 1018, 291, 393, 536, 11, 1203, 3009, 1785, 1901, 5245, 775, 10591, 5324, 813, 50904], "temperature": 0.0, "avg_logprob": -0.14935742172540403, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.0533839650452137}, {"id": 301, "seek": 142040, "start": 1431.2, "end": 1433.1200000000001, "text": " transformers.", "tokens": [50904, 4088, 433, 13, 51000], "temperature": 0.0, "avg_logprob": -0.14935742172540403, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.0533839650452137}, {"id": 302, "seek": 142040, "start": 1433.1200000000001, "end": 1438.1200000000001, "text": " The reason for this, as identified in the Mamba paper, is that linear RNNs are incapable", "tokens": [51000, 440, 1778, 337, 341, 11, 382, 9234, 294, 264, 376, 23337, 3035, 11, 307, 300, 8213, 45702, 45, 82, 366, 44174, 51250], "temperature": 0.0, "avg_logprob": -0.14935742172540403, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.0533839650452137}, {"id": 303, "seek": 142040, "start": 1438.1200000000001, "end": 1442.76, "text": " of selectively forgetting information from the output vector.", "tokens": [51250, 295, 3048, 3413, 25428, 1589, 490, 264, 5598, 8062, 13, 51482], "temperature": 0.0, "avg_logprob": -0.14935742172540403, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.0533839650452137}, {"id": 304, "seek": 142040, "start": 1442.76, "end": 1447.1200000000001, "text": " If the weights are close to zero, then the output vector will be set to zero after every", "tokens": [51482, 759, 264, 17443, 366, 1998, 281, 4018, 11, 550, 264, 5598, 8062, 486, 312, 992, 281, 4018, 934, 633, 51700], "temperature": 0.0, "avg_logprob": -0.14935742172540403, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.0533839650452137}, {"id": 305, "seek": 144712, "start": 1447.1999999999998, "end": 1448.1999999999998, "text": " input.", "tokens": [50368, 4846, 13, 50418], "temperature": 0.0, "avg_logprob": -0.11816223300233179, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.003945023752748966}, {"id": 306, "seek": 144712, "start": 1448.1999999999998, "end": 1453.3999999999999, "text": " Effectively, the model will always immediately forget whatever came before the current input.", "tokens": [50418, 17764, 3413, 11, 264, 2316, 486, 1009, 4258, 2870, 2035, 1361, 949, 264, 2190, 4846, 13, 50678], "temperature": 0.0, "avg_logprob": -0.11816223300233179, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.003945023752748966}, {"id": 307, "seek": 144712, "start": 1453.3999999999999, "end": 1457.2399999999998, "text": " If their current weights are close to one, then the output vector doesn't change when", "tokens": [50678, 759, 641, 2190, 17443, 366, 1998, 281, 472, 11, 550, 264, 5598, 8062, 1177, 380, 1319, 562, 50870], "temperature": 0.0, "avg_logprob": -0.11816223300233179, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.003945023752748966}, {"id": 308, "seek": 144712, "start": 1457.2399999999998, "end": 1461.52, "text": " it's multiplied with the weights, so the output vector will accumulate information", "tokens": [50870, 309, 311, 17207, 365, 264, 17443, 11, 370, 264, 5598, 8062, 486, 33384, 1589, 51084], "temperature": 0.0, "avg_logprob": -0.11816223300233179, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.003945023752748966}, {"id": 309, "seek": 144712, "start": 1461.52, "end": 1464.6799999999998, "text": " from all inputs observed.", "tokens": [51084, 490, 439, 15743, 13095, 13, 51242], "temperature": 0.0, "avg_logprob": -0.11816223300233179, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.003945023752748966}, {"id": 310, "seek": 144712, "start": 1464.6799999999998, "end": 1469.4799999999998, "text": " What you want is for the model to be able to decide when to store information and when", "tokens": [51242, 708, 291, 528, 307, 337, 264, 2316, 281, 312, 1075, 281, 4536, 562, 281, 3531, 1589, 293, 562, 51482], "temperature": 0.0, "avg_logprob": -0.11816223300233179, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.003945023752748966}, {"id": 311, "seek": 144712, "start": 1469.4799999999998, "end": 1474.36, "text": " to forget information based on what input it sees.", "tokens": [51482, 281, 2870, 1589, 2361, 322, 437, 4846, 309, 8194, 13, 51726], "temperature": 0.0, "avg_logprob": -0.11816223300233179, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.003945023752748966}, {"id": 312, "seek": 147436, "start": 1474.36, "end": 1480.0, "text": " Mamba proposes an elegant solution, instead of using the same weights in each step, use", "tokens": [50364, 376, 23337, 2365, 4201, 364, 21117, 3827, 11, 2602, 295, 1228, 264, 912, 17443, 294, 1184, 1823, 11, 764, 50646], "temperature": 0.0, "avg_logprob": -0.13059414516795764, "compression_ratio": 1.7757847533632287, "no_speech_prob": 0.010984696447849274}, {"id": 313, "seek": 147436, "start": 1480.0, "end": 1484.28, "text": " different weights which depend on the input.", "tokens": [50646, 819, 17443, 597, 5672, 322, 264, 4846, 13, 50860], "temperature": 0.0, "avg_logprob": -0.13059414516795764, "compression_ratio": 1.7757847533632287, "no_speech_prob": 0.010984696447849274}, {"id": 314, "seek": 147436, "start": 1484.28, "end": 1489.12, "text": " Mamba applies a linear function to each input vector to generate a separate weight vector", "tokens": [50860, 376, 23337, 13165, 257, 8213, 2445, 281, 1184, 4846, 8062, 281, 8460, 257, 4994, 3364, 8062, 51102], "temperature": 0.0, "avg_logprob": -0.13059414516795764, "compression_ratio": 1.7757847533632287, "no_speech_prob": 0.010984696447849274}, {"id": 315, "seek": 147436, "start": 1489.12, "end": 1492.6399999999999, "text": " for that input.", "tokens": [51102, 337, 300, 4846, 13, 51278], "temperature": 0.0, "avg_logprob": -0.13059414516795764, "compression_ratio": 1.7757847533632287, "no_speech_prob": 0.010984696447849274}, {"id": 316, "seek": 147436, "start": 1492.6399999999999, "end": 1497.52, "text": " Then the recurrent scan is performed using these generated weights.", "tokens": [51278, 1396, 264, 18680, 1753, 11049, 307, 10332, 1228, 613, 10833, 17443, 13, 51522], "temperature": 0.0, "avg_logprob": -0.13059414516795764, "compression_ratio": 1.7757847533632287, "no_speech_prob": 0.010984696447849274}, {"id": 317, "seek": 147436, "start": 1497.52, "end": 1502.4399999999998, "text": " This way, certain inputs can generate weights close to zero and thereby erase information", "tokens": [51522, 639, 636, 11, 1629, 15743, 393, 8460, 17443, 1998, 281, 4018, 293, 28281, 23525, 1589, 51768], "temperature": 0.0, "avg_logprob": -0.13059414516795764, "compression_ratio": 1.7757847533632287, "no_speech_prob": 0.010984696447849274}, {"id": 318, "seek": 150244, "start": 1502.44, "end": 1507.92, "text": " from the output vector, but other inputs can generate weights close to one, thereby leaving", "tokens": [50364, 490, 264, 5598, 8062, 11, 457, 661, 15743, 393, 8460, 17443, 1998, 281, 472, 11, 28281, 5012, 50638], "temperature": 0.0, "avg_logprob": -0.17218832222812147, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.0029807048849761486}, {"id": 319, "seek": 150244, "start": 1507.92, "end": 1511.88, "text": " the output vector unchanged.", "tokens": [50638, 264, 5598, 8062, 44553, 13, 50836], "temperature": 0.0, "avg_logprob": -0.17218832222812147, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.0029807048849761486}, {"id": 320, "seek": 150244, "start": 1511.88, "end": 1516.6000000000001, "text": " And I also suspect that using different weights at each step helps with vanishing and exploiting", "tokens": [50836, 400, 286, 611, 9091, 300, 1228, 819, 17443, 412, 1184, 1823, 3665, 365, 3161, 3807, 293, 12382, 1748, 51072], "temperature": 0.0, "avg_logprob": -0.17218832222812147, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.0029807048849761486}, {"id": 321, "seek": 150244, "start": 1516.6000000000001, "end": 1522.1200000000001, "text": " gradients, since there should now be many different stable configurations, like in feed-forward", "tokens": [51072, 2771, 2448, 11, 1670, 456, 820, 586, 312, 867, 819, 8351, 31493, 11, 411, 294, 3154, 12, 13305, 51348], "temperature": 0.0, "avg_logprob": -0.17218832222812147, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.0029807048849761486}, {"id": 322, "seek": 150244, "start": 1522.1200000000001, "end": 1527.8400000000001, "text": " networks, although this wasn't mentioned in the Mamba paper.", "tokens": [51348, 9590, 11, 4878, 341, 2067, 380, 2835, 294, 264, 376, 23337, 3035, 13, 51634], "temperature": 0.0, "avg_logprob": -0.17218832222812147, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.0029807048849761486}, {"id": 323, "seek": 152784, "start": 1527.84, "end": 1533.56, "text": " Mamba also uses one more trick, which is to increase the size of the output vectors.", "tokens": [50364, 376, 23337, 611, 4960, 472, 544, 4282, 11, 597, 307, 281, 3488, 264, 2744, 295, 264, 5598, 18875, 13, 50650], "temperature": 0.0, "avg_logprob": -0.10420616305604273, "compression_ratio": 1.8026905829596414, "no_speech_prob": 0.5230831503868103}, {"id": 324, "seek": 152784, "start": 1533.56, "end": 1539.04, "text": " In a standard RNN, the output vectors are the same size as the input vectors.", "tokens": [50650, 682, 257, 3832, 45702, 45, 11, 264, 5598, 18875, 366, 264, 912, 2744, 382, 264, 4846, 18875, 13, 50924], "temperature": 0.0, "avg_logprob": -0.10420616305604273, "compression_ratio": 1.8026905829596414, "no_speech_prob": 0.5230831503868103}, {"id": 325, "seek": 152784, "start": 1539.04, "end": 1543.8, "text": " Mamba expands the size of the output vectors by a factor of 16.", "tokens": [50924, 376, 23337, 33706, 264, 2744, 295, 264, 5598, 18875, 538, 257, 5952, 295, 3165, 13, 51162], "temperature": 0.0, "avg_logprob": -0.10420616305604273, "compression_ratio": 1.8026905829596414, "no_speech_prob": 0.5230831503868103}, {"id": 326, "seek": 152784, "start": 1543.8, "end": 1548.84, "text": " This allows it to store much more information from previous inputs.", "tokens": [51162, 639, 4045, 309, 281, 3531, 709, 544, 1589, 490, 3894, 15743, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10420616305604273, "compression_ratio": 1.8026905829596414, "no_speech_prob": 0.5230831503868103}, {"id": 327, "seek": 152784, "start": 1548.84, "end": 1553.3999999999999, "text": " The output vectors are then projected back down to the original size before being passed", "tokens": [51414, 440, 5598, 18875, 366, 550, 26231, 646, 760, 281, 264, 3380, 2744, 949, 885, 4678, 51642], "temperature": 0.0, "avg_logprob": -0.10420616305604273, "compression_ratio": 1.8026905829596414, "no_speech_prob": 0.5230831503868103}, {"id": 328, "seek": 152784, "start": 1553.3999999999999, "end": 1557.36, "text": " to the next layer.", "tokens": [51642, 281, 264, 958, 4583, 13, 51840], "temperature": 0.0, "avg_logprob": -0.10420616305604273, "compression_ratio": 1.8026905829596414, "no_speech_prob": 0.5230831503868103}, {"id": 329, "seek": 155736, "start": 1557.6799999999998, "end": 1562.56, "text": " This would increase the computation time by a factor of 16, but it turns out that the", "tokens": [50380, 639, 576, 3488, 264, 24903, 565, 538, 257, 5952, 295, 3165, 11, 457, 309, 4523, 484, 300, 264, 50624], "temperature": 0.0, "avg_logprob": -0.14236639041711788, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.06005286052823067}, {"id": 330, "seek": 155736, "start": 1562.56, "end": 1567.8799999999999, "text": " major bottleneck of a Mamba layer on modern GPUs is the time it takes to read and write", "tokens": [50624, 2563, 44641, 547, 295, 257, 376, 23337, 4583, 322, 4363, 18407, 82, 307, 264, 565, 309, 2516, 281, 1401, 293, 2464, 50890], "temperature": 0.0, "avg_logprob": -0.14236639041711788, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.06005286052823067}, {"id": 331, "seek": 155736, "start": 1567.8799999999999, "end": 1570.76, "text": " data into high-performance memory.", "tokens": [50890, 1412, 666, 1090, 12, 50242, 4675, 13, 51034], "temperature": 0.0, "avg_logprob": -0.14236639041711788, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.06005286052823067}, {"id": 332, "seek": 155736, "start": 1570.76, "end": 1574.8799999999999, "text": " You see, modern GPUs actually have two different types of memory.", "tokens": [51034, 509, 536, 11, 4363, 18407, 82, 767, 362, 732, 819, 3467, 295, 4675, 13, 51240], "temperature": 0.0, "avg_logprob": -0.14236639041711788, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.06005286052823067}, {"id": 333, "seek": 155736, "start": 1574.8799999999999, "end": 1579.12, "text": " Data is stored in main memory, but in order to do computations, data first needs to be", "tokens": [51240, 11888, 307, 12187, 294, 2135, 4675, 11, 457, 294, 1668, 281, 360, 2807, 763, 11, 1412, 700, 2203, 281, 312, 51452], "temperature": 0.0, "avg_logprob": -0.14236639041711788, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.06005286052823067}, {"id": 334, "seek": 155736, "start": 1579.12, "end": 1583.4799999999998, "text": " transferred into high-performance memory.", "tokens": [51452, 15809, 666, 1090, 12, 50242, 4675, 13, 51670], "temperature": 0.0, "avg_logprob": -0.14236639041711788, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.06005286052823067}, {"id": 335, "seek": 158348, "start": 1583.48, "end": 1588.52, "text": " For the Mamba recurrence operation, it turns out that the time taken to transfer data is", "tokens": [50364, 1171, 264, 376, 23337, 18680, 10760, 6916, 11, 309, 4523, 484, 300, 264, 565, 2726, 281, 5003, 1412, 307, 50616], "temperature": 0.0, "avg_logprob": -0.09149261273835835, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0047544874250888824}, {"id": 336, "seek": 158348, "start": 1588.52, "end": 1594.28, "text": " actually much larger than the time it takes to do the computation itself.", "tokens": [50616, 767, 709, 4833, 813, 264, 565, 309, 2516, 281, 360, 264, 24903, 2564, 13, 50904], "temperature": 0.0, "avg_logprob": -0.09149261273835835, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0047544874250888824}, {"id": 337, "seek": 158348, "start": 1594.28, "end": 1599.3600000000001, "text": " Mamba therefore transfers the input vectors and model parameters to high-performance memory,", "tokens": [50904, 376, 23337, 4412, 29137, 264, 4846, 18875, 293, 2316, 9834, 281, 1090, 12, 50242, 4675, 11, 51158], "temperature": 0.0, "avg_logprob": -0.09149261273835835, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0047544874250888824}, {"id": 338, "seek": 158348, "start": 1599.3600000000001, "end": 1604.6, "text": " and then computes the whole Mamba operation in a single block, including projecting outputs", "tokens": [51158, 293, 550, 715, 1819, 264, 1379, 376, 23337, 6916, 294, 257, 2167, 3461, 11, 3009, 43001, 23930, 51420], "temperature": 0.0, "avg_logprob": -0.09149261273835835, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0047544874250888824}, {"id": 339, "seek": 158348, "start": 1604.6, "end": 1611.08, "text": " back down to the smaller original size, before writing the results back to main memory.", "tokens": [51420, 646, 760, 281, 264, 4356, 3380, 2744, 11, 949, 3579, 264, 3542, 646, 281, 2135, 4675, 13, 51744], "temperature": 0.0, "avg_logprob": -0.09149261273835835, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0047544874250888824}, {"id": 340, "seek": 161108, "start": 1611.08, "end": 1617.3999999999999, "text": " This way, you only transfer vectors of the original size to and from high-performance memory,", "tokens": [50364, 639, 636, 11, 291, 787, 5003, 18875, 295, 264, 3380, 2744, 281, 293, 490, 1090, 12, 50242, 4675, 11, 50680], "temperature": 0.0, "avg_logprob": -0.09232974582248264, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0012065030168741941}, {"id": 341, "seek": 161108, "start": 1617.3999999999999, "end": 1621.48, "text": " so the transfer time is unchanged.", "tokens": [50680, 370, 264, 5003, 565, 307, 44553, 13, 50884], "temperature": 0.0, "avg_logprob": -0.09232974582248264, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0012065030168741941}, {"id": 342, "seek": 161108, "start": 1621.48, "end": 1626.84, "text": " The actual computation time is 16 times slower, but the computation time was so small compared", "tokens": [50884, 440, 3539, 24903, 565, 307, 3165, 1413, 14009, 11, 457, 264, 24903, 565, 390, 370, 1359, 5347, 51152], "temperature": 0.0, "avg_logprob": -0.09232974582248264, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0012065030168741941}, {"id": 343, "seek": 161108, "start": 1626.84, "end": 1630.56, "text": " to the transfer time that it doesn't really affect the overall time taken.", "tokens": [51152, 281, 264, 5003, 565, 300, 309, 1177, 380, 534, 3345, 264, 4787, 565, 2726, 13, 51338], "temperature": 0.0, "avg_logprob": -0.09232974582248264, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0012065030168741941}, {"id": 344, "seek": 161108, "start": 1630.56, "end": 1635.3999999999999, "text": " You essentially get to use 16 times larger vectors for free.", "tokens": [51338, 509, 4476, 483, 281, 764, 3165, 1413, 4833, 18875, 337, 1737, 13, 51580], "temperature": 0.0, "avg_logprob": -0.09232974582248264, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0012065030168741941}, {"id": 345, "seek": 161108, "start": 1635.3999999999999, "end": 1636.76, "text": " And there we have it.", "tokens": [51580, 400, 456, 321, 362, 309, 13, 51648], "temperature": 0.0, "avg_logprob": -0.09232974582248264, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0012065030168741941}, {"id": 346, "seek": 163676, "start": 1636.76, "end": 1642.52, "text": " This, along with the few minor architectural modifications, is Mamba, the dynamic linear", "tokens": [50364, 639, 11, 2051, 365, 264, 1326, 6696, 26621, 26881, 11, 307, 376, 23337, 11, 264, 8546, 8213, 50652], "temperature": 0.0, "avg_logprob": -0.23606476783752442, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.019714143127202988}, {"id": 347, "seek": 163676, "start": 1642.52, "end": 1647.8799999999999, "text": " recurrent neural network, which performs better than transformers at language modeling while", "tokens": [50652, 18680, 1753, 18161, 3209, 11, 597, 26213, 1101, 813, 4088, 433, 412, 2856, 15983, 1339, 50920], "temperature": 0.0, "avg_logprob": -0.23606476783752442, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.019714143127202988}, {"id": 348, "seek": 163676, "start": 1647.8799999999999, "end": 1653.64, "text": " using only nlogn compute down from n squared.", "tokens": [50920, 1228, 787, 297, 4987, 77, 14722, 760, 490, 297, 8889, 13, 51208], "temperature": 0.0, "avg_logprob": -0.23606476783752442, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.019714143127202988}, {"id": 349, "seek": 163676, "start": 1653.64, "end": 1660.72, "text": " Okay, now that we've made it through all of those boring technical details, we can finally", "tokens": [51208, 1033, 11, 586, 300, 321, 600, 1027, 309, 807, 439, 295, 729, 9989, 6191, 4365, 11, 321, 393, 2721, 51562], "temperature": 0.0, "avg_logprob": -0.23606476783752442, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.019714143127202988}, {"id": 350, "seek": 163676, "start": 1660.72, "end": 1663.24, "text": " talk about what really matters.", "tokens": [51562, 751, 466, 437, 534, 7001, 13, 51688], "temperature": 0.0, "avg_logprob": -0.23606476783752442, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.019714143127202988}, {"id": 351, "seek": 166324, "start": 1663.24, "end": 1669.16, "text": " The Mamba Drama", "tokens": [50364, 440, 376, 23337, 45406, 50660], "temperature": 0.0, "avg_logprob": -0.1623188962218582, "compression_ratio": 1.6, "no_speech_prob": 0.054985471069812775}, {"id": 352, "seek": 166324, "start": 1669.16, "end": 1673.52, "text": " You see, the Mamba paper caused quite a bit of controversy in the machine learning community", "tokens": [50660, 509, 536, 11, 264, 376, 23337, 3035, 7008, 1596, 257, 857, 295, 22976, 294, 264, 3479, 2539, 1768, 50878], "temperature": 0.0, "avg_logprob": -0.1623188962218582, "compression_ratio": 1.6, "no_speech_prob": 0.054985471069812775}, {"id": 353, "seek": 166324, "start": 1673.52, "end": 1675.32, "text": " this year.", "tokens": [50878, 341, 1064, 13, 50968], "temperature": 0.0, "avg_logprob": -0.1623188962218582, "compression_ratio": 1.6, "no_speech_prob": 0.054985471069812775}, {"id": 354, "seek": 166324, "start": 1675.32, "end": 1680.76, "text": " The Mamba paper was submitted to ICLR 2024, which is one of the most prestigious machine", "tokens": [50968, 440, 376, 23337, 3035, 390, 14405, 281, 14360, 31722, 45237, 11, 597, 307, 472, 295, 264, 881, 33510, 3479, 51240], "temperature": 0.0, "avg_logprob": -0.1623188962218582, "compression_ratio": 1.6, "no_speech_prob": 0.054985471069812775}, {"id": 355, "seek": 166324, "start": 1680.76, "end": 1687.76, "text": " learning conferences in the world, and in January it was rejected by peer reviewers.", "tokens": [51240, 2539, 22032, 294, 264, 1002, 11, 293, 294, 7061, 309, 390, 15749, 538, 15108, 45837, 13, 51590], "temperature": 0.0, "avg_logprob": -0.1623188962218582, "compression_ratio": 1.6, "no_speech_prob": 0.054985471069812775}, {"id": 356, "seek": 166324, "start": 1687.76, "end": 1689.1200000000001, "text": " But so what?", "tokens": [51590, 583, 370, 437, 30, 51658], "temperature": 0.0, "avg_logprob": -0.1623188962218582, "compression_ratio": 1.6, "no_speech_prob": 0.054985471069812775}, {"id": 357, "seek": 166324, "start": 1689.1200000000001, "end": 1692.92, "text": " Papers get rejected from top conferences all the time, right?", "tokens": [51658, 430, 14441, 483, 15749, 490, 1192, 22032, 439, 264, 565, 11, 558, 30, 51848], "temperature": 0.0, "avg_logprob": -0.1623188962218582, "compression_ratio": 1.6, "no_speech_prob": 0.054985471069812775}, {"id": 358, "seek": 169292, "start": 1692.92, "end": 1698.6000000000001, "text": " Well, to give some context, the Mamba preprint has been publicly available since last year,", "tokens": [50364, 1042, 11, 281, 976, 512, 4319, 11, 264, 376, 23337, 659, 14030, 575, 668, 14843, 2435, 1670, 1036, 1064, 11, 50648], "temperature": 0.0, "avg_logprob": -0.13689690010220396, "compression_ratio": 1.6549019607843136, "no_speech_prob": 0.09002986550331116}, {"id": 359, "seek": 169292, "start": 1698.6000000000001, "end": 1703.8000000000002, "text": " and during this time, several different groups have re-implemented Mamba and all successfully", "tokens": [50648, 293, 1830, 341, 565, 11, 2940, 819, 3935, 362, 319, 12, 332, 781, 14684, 376, 23337, 293, 439, 10727, 50908], "temperature": 0.0, "avg_logprob": -0.13689690010220396, "compression_ratio": 1.6549019607843136, "no_speech_prob": 0.09002986550331116}, {"id": 360, "seek": 169292, "start": 1703.8000000000002, "end": 1709.0800000000002, "text": " reproduced the results claimed in the Mamba paper, namely that Mamba performs better than", "tokens": [50908, 11408, 1232, 264, 3542, 12941, 294, 264, 376, 23337, 3035, 11, 20926, 300, 376, 23337, 26213, 1101, 813, 51172], "temperature": 0.0, "avg_logprob": -0.13689690010220396, "compression_ratio": 1.6549019607843136, "no_speech_prob": 0.09002986550331116}, {"id": 361, "seek": 169292, "start": 1709.0800000000002, "end": 1712.8400000000001, "text": " transformers and uses less computation.", "tokens": [51172, 4088, 433, 293, 4960, 1570, 24903, 13, 51360], "temperature": 0.0, "avg_logprob": -0.13689690010220396, "compression_ratio": 1.6549019607843136, "no_speech_prob": 0.09002986550331116}, {"id": 362, "seek": 169292, "start": 1712.8400000000001, "end": 1717.96, "text": " And since transformers are all anyone has talked about for the last five years, that's", "tokens": [51360, 400, 1670, 4088, 433, 366, 439, 2878, 575, 2825, 466, 337, 264, 1036, 1732, 924, 11, 300, 311, 51616], "temperature": 0.0, "avg_logprob": -0.13689690010220396, "compression_ratio": 1.6549019607843136, "no_speech_prob": 0.09002986550331116}, {"id": 363, "seek": 169292, "start": 1717.96, "end": 1720.04, "text": " kind of a big deal.", "tokens": [51616, 733, 295, 257, 955, 2028, 13, 51720], "temperature": 0.0, "avg_logprob": -0.13689690010220396, "compression_ratio": 1.6549019607843136, "no_speech_prob": 0.09002986550331116}, {"id": 364, "seek": 172004, "start": 1720.04, "end": 1725.24, "text": " Because of this, everyone in the community was expecting the Mamba paper to be accepted,", "tokens": [50364, 1436, 295, 341, 11, 1518, 294, 264, 1768, 390, 9650, 264, 376, 23337, 3035, 281, 312, 9035, 11, 50624], "temperature": 0.0, "avg_logprob": -0.1346904621567837, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0007321523735299706}, {"id": 365, "seek": 172004, "start": 1725.24, "end": 1730.6, "text": " if not win a best paper award.", "tokens": [50624, 498, 406, 1942, 257, 1151, 3035, 7130, 13, 50892], "temperature": 0.0, "avg_logprob": -0.1346904621567837, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0007321523735299706}, {"id": 366, "seek": 172004, "start": 1730.6, "end": 1736.1599999999999, "text": " So then, if the Mamba architecture really works, what glaring flaws are in the paper", "tokens": [50892, 407, 550, 11, 498, 264, 376, 23337, 9482, 534, 1985, 11, 437, 1563, 1921, 27108, 366, 294, 264, 3035, 51170], "temperature": 0.0, "avg_logprob": -0.1346904621567837, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0007321523735299706}, {"id": 367, "seek": 172004, "start": 1736.1599999999999, "end": 1738.6, "text": " that caused it to be rejected?", "tokens": [51170, 300, 7008, 309, 281, 312, 15749, 30, 51292], "temperature": 0.0, "avg_logprob": -0.1346904621567837, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0007321523735299706}, {"id": 368, "seek": 172004, "start": 1738.6, "end": 1747.52, "text": " Well, the ICLR peer review is publicly available for everyone to view, so let's take a look.", "tokens": [51292, 1042, 11, 264, 14360, 31722, 15108, 3131, 307, 14843, 2435, 337, 1518, 281, 1910, 11, 370, 718, 311, 747, 257, 574, 13, 51738], "temperature": 0.0, "avg_logprob": -0.1346904621567837, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0007321523735299706}, {"id": 369, "seek": 174752, "start": 1747.52, "end": 1753.2, "text": " According to the meta review, Mamba wasn't tested on the Long Range Arena benchmark.", "tokens": [50364, 7328, 281, 264, 19616, 3131, 11, 376, 23337, 2067, 380, 8246, 322, 264, 8282, 33778, 34290, 18927, 13, 50648], "temperature": 0.0, "avg_logprob": -0.12476080114191229, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.42214837670326233}, {"id": 370, "seek": 174752, "start": 1753.2, "end": 1758.48, "text": " Remember that benchmark I talked about, where linear RNNs performed way better than transformers?", "tokens": [50648, 5459, 300, 18927, 286, 2825, 466, 11, 689, 8213, 45702, 45, 82, 10332, 636, 1101, 813, 4088, 433, 30, 50912], "temperature": 0.0, "avg_logprob": -0.12476080114191229, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.42214837670326233}, {"id": 371, "seek": 174752, "start": 1758.48, "end": 1763.4, "text": " This reviewer wanted to see how well Mamba performed on that task.", "tokens": [50912, 639, 3131, 260, 1415, 281, 536, 577, 731, 376, 23337, 10332, 322, 300, 5633, 13, 51158], "temperature": 0.0, "avg_logprob": -0.12476080114191229, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.42214837670326233}, {"id": 372, "seek": 174752, "start": 1763.4, "end": 1768.56, "text": " Now this is a really dumb reason to reject a paper, because the Long Range Arena is a", "tokens": [51158, 823, 341, 307, 257, 534, 10316, 1778, 281, 8248, 257, 3035, 11, 570, 264, 8282, 33778, 34290, 307, 257, 51416], "temperature": 0.0, "avg_logprob": -0.12476080114191229, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.42214837670326233}, {"id": 373, "seek": 174752, "start": 1768.56, "end": 1774.92, "text": " completely different task to language modelling, and Mamba is specifically a language model.", "tokens": [51416, 2584, 819, 5633, 281, 2856, 42253, 11, 293, 376, 23337, 307, 4682, 257, 2856, 2316, 13, 51734], "temperature": 0.0, "avg_logprob": -0.12476080114191229, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.42214837670326233}, {"id": 374, "seek": 177492, "start": 1774.92, "end": 1780.04, "text": " Keep in mind, transformers perform way worse than linear RNNs on the Long Range Arena,", "tokens": [50364, 5527, 294, 1575, 11, 4088, 433, 2042, 636, 5324, 813, 8213, 45702, 45, 82, 322, 264, 8282, 33778, 34290, 11, 50620], "temperature": 0.0, "avg_logprob": -0.12285218545056265, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.05497686564922333}, {"id": 375, "seek": 177492, "start": 1780.04, "end": 1783.3600000000001, "text": " but transformers are still way better language models.", "tokens": [50620, 457, 4088, 433, 366, 920, 636, 1101, 2856, 5245, 13, 50786], "temperature": 0.0, "avg_logprob": -0.12285218545056265, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.05497686564922333}, {"id": 376, "seek": 177492, "start": 1783.3600000000001, "end": 1787.64, "text": " So performance on the Long Range Arena is in no way indicative of language modelling", "tokens": [50786, 407, 3389, 322, 264, 8282, 33778, 34290, 307, 294, 572, 636, 47513, 295, 2856, 42253, 51000], "temperature": 0.0, "avg_logprob": -0.12285218545056265, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.05497686564922333}, {"id": 377, "seek": 177492, "start": 1787.64, "end": 1789.64, "text": " ability.", "tokens": [51000, 3485, 13, 51100], "temperature": 0.0, "avg_logprob": -0.12285218545056265, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.05497686564922333}, {"id": 378, "seek": 177492, "start": 1789.64, "end": 1792.2, "text": " Mamba sets a new state of the art for language modelling.", "tokens": [51100, 376, 23337, 6352, 257, 777, 1785, 295, 264, 1523, 337, 2856, 42253, 13, 51228], "temperature": 0.0, "avg_logprob": -0.12285218545056265, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.05497686564922333}, {"id": 379, "seek": 177492, "start": 1792.2, "end": 1799.04, "text": " It shouldn't be rejected because it doesn't also solve some other unrelated task.", "tokens": [51228, 467, 4659, 380, 312, 15749, 570, 309, 1177, 380, 611, 5039, 512, 661, 38967, 5633, 13, 51570], "temperature": 0.0, "avg_logprob": -0.12285218545056265, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.05497686564922333}, {"id": 380, "seek": 177492, "start": 1799.04, "end": 1803.8400000000001, "text": " The only other major criticism in the meta review is that Mamba was only evaluated on", "tokens": [51570, 440, 787, 661, 2563, 15835, 294, 264, 19616, 3131, 307, 300, 376, 23337, 390, 787, 25509, 322, 51810], "temperature": 0.0, "avg_logprob": -0.12285218545056265, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.05497686564922333}, {"id": 381, "seek": 180384, "start": 1803.9199999999998, "end": 1810.24, "text": " language modelling, that is the accuracy when predicting the next word in a piece of text.", "tokens": [50368, 2856, 42253, 11, 300, 307, 264, 14170, 562, 32884, 264, 958, 1349, 294, 257, 2522, 295, 2487, 13, 50684], "temperature": 0.0, "avg_logprob": -0.14101776274123995, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0037067208904772997}, {"id": 382, "seek": 180384, "start": 1810.24, "end": 1815.32, "text": " The reviewers argue that this metric isn't indicative of a language model's utility,", "tokens": [50684, 440, 45837, 9695, 300, 341, 20678, 1943, 380, 47513, 295, 257, 2856, 2316, 311, 14877, 11, 50938], "temperature": 0.0, "avg_logprob": -0.14101776274123995, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0037067208904772997}, {"id": 383, "seek": 180384, "start": 1815.32, "end": 1820.4399999999998, "text": " and instead Mamba should have been evaluated on downstream tasks that measure a model's", "tokens": [50938, 293, 2602, 376, 23337, 820, 362, 668, 25509, 322, 30621, 9608, 300, 3481, 257, 2316, 311, 51194], "temperature": 0.0, "avg_logprob": -0.14101776274123995, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0037067208904772997}, {"id": 384, "seek": 180384, "start": 1820.4399999999998, "end": 1822.72, "text": " reasoning ability.", "tokens": [51194, 21577, 3485, 13, 51308], "temperature": 0.0, "avg_logprob": -0.14101776274123995, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0037067208904772997}, {"id": 385, "seek": 180384, "start": 1822.72, "end": 1826.56, "text": " Except that this is exactly what they did in the Mamba paper.", "tokens": [51308, 16192, 300, 341, 307, 2293, 437, 436, 630, 294, 264, 376, 23337, 3035, 13, 51500], "temperature": 0.0, "avg_logprob": -0.14101776274123995, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0037067208904772997}, {"id": 386, "seek": 180384, "start": 1826.56, "end": 1831.24, "text": " They pre-trained Mamba as a language model, and then performed zero shot prompting on", "tokens": [51500, 814, 659, 12, 17227, 2001, 376, 23337, 382, 257, 2856, 2316, 11, 293, 550, 10332, 4018, 3347, 12391, 278, 322, 51734], "temperature": 0.0, "avg_logprob": -0.14101776274123995, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0037067208904772997}, {"id": 387, "seek": 183124, "start": 1831.24, "end": 1837.28, "text": " a bunch of standard downstream benchmark tasks, and surprise, surprise, Mamba outperforms", "tokens": [50364, 257, 3840, 295, 3832, 30621, 18927, 9608, 11, 293, 6365, 11, 6365, 11, 376, 23337, 484, 26765, 82, 50666], "temperature": 0.0, "avg_logprob": -0.2026322760233065, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.0064858319237828255}, {"id": 388, "seek": 183124, "start": 1837.28, "end": 1840.4, "text": " all other language models.", "tokens": [50666, 439, 661, 2856, 5245, 13, 50822], "temperature": 0.0, "avg_logprob": -0.2026322760233065, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.0064858319237828255}, {"id": 389, "seek": 183124, "start": 1840.4, "end": 1843.8, "text": " As a bonus, another reviewer said, and I quote,", "tokens": [50822, 1018, 257, 10882, 11, 1071, 3131, 260, 848, 11, 293, 286, 6513, 11, 50992], "temperature": 0.0, "avg_logprob": -0.2026322760233065, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.0064858319237828255}, {"id": 390, "seek": 183124, "start": 1843.8, "end": 1850.36, "text": " Mamba has a quadratic memory requirement during training, just like transformers.", "tokens": [50992, 376, 23337, 575, 257, 37262, 4675, 11695, 1830, 3097, 11, 445, 411, 4088, 433, 13, 51320], "temperature": 0.0, "avg_logprob": -0.2026322760233065, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.0064858319237828255}, {"id": 391, "seek": 183124, "start": 1850.36, "end": 1853.64, "text": " Which is just not true.", "tokens": [51320, 3013, 307, 445, 406, 2074, 13, 51484], "temperature": 0.0, "avg_logprob": -0.2026322760233065, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.0064858319237828255}, {"id": 392, "seek": 183124, "start": 1853.64, "end": 1858.68, "text": " Neither Mamba nor transformers have quadratic memory costs.", "tokens": [51484, 23956, 376, 23337, 6051, 4088, 433, 362, 37262, 4675, 5497, 13, 51736], "temperature": 0.0, "avg_logprob": -0.2026322760233065, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.0064858319237828255}, {"id": 393, "seek": 185868, "start": 1858.68, "end": 1863.64, "text": " Mambas have a quadratic compute cost, but their memory cost is linear.", "tokens": [50364, 376, 2173, 296, 362, 257, 37262, 14722, 2063, 11, 457, 641, 4675, 2063, 307, 8213, 13, 50612], "temperature": 0.0, "avg_logprob": -0.1399933630678834, "compression_ratio": 1.7, "no_speech_prob": 0.015902617946267128}, {"id": 394, "seek": 185868, "start": 1863.64, "end": 1864.64, "text": " So is Mambas.", "tokens": [50612, 407, 307, 376, 2173, 296, 13, 50662], "temperature": 0.0, "avg_logprob": -0.1399933630678834, "compression_ratio": 1.7, "no_speech_prob": 0.015902617946267128}, {"id": 395, "seek": 185868, "start": 1864.64, "end": 1868.96, "text": " I'm not sure how it's even possible to come to the conclusion that Mamba has a quadratic", "tokens": [50662, 286, 478, 406, 988, 577, 309, 311, 754, 1944, 281, 808, 281, 264, 10063, 300, 376, 23337, 575, 257, 37262, 50878], "temperature": 0.0, "avg_logprob": -0.1399933630678834, "compression_ratio": 1.7, "no_speech_prob": 0.015902617946267128}, {"id": 396, "seek": 185868, "start": 1868.96, "end": 1873.16, "text": " memory cost if you understand how it works at all.", "tokens": [50878, 4675, 2063, 498, 291, 1223, 577, 309, 1985, 412, 439, 13, 51088], "temperature": 0.0, "avg_logprob": -0.1399933630678834, "compression_ratio": 1.7, "no_speech_prob": 0.015902617946267128}, {"id": 397, "seek": 185868, "start": 1873.16, "end": 1878.0, "text": " So as you can imagine, this less than ideal peer review sparked some debate in the machine", "tokens": [51088, 407, 382, 291, 393, 3811, 11, 341, 1570, 813, 7157, 15108, 3131, 39653, 512, 7958, 294, 264, 3479, 51330], "temperature": 0.0, "avg_logprob": -0.1399933630678834, "compression_ratio": 1.7, "no_speech_prob": 0.015902617946267128}, {"id": 398, "seek": 185868, "start": 1878.0, "end": 1882.28, "text": " learning community about peer reviewing practices and whether or not Mamba should have been", "tokens": [51330, 2539, 1768, 466, 15108, 19576, 7525, 293, 1968, 420, 406, 376, 23337, 820, 362, 668, 51544], "temperature": 0.0, "avg_logprob": -0.1399933630678834, "compression_ratio": 1.7, "no_speech_prob": 0.015902617946267128}, {"id": 399, "seek": 185868, "start": 1882.28, "end": 1883.28, "text": " rejected.", "tokens": [51544, 15749, 13, 51594], "temperature": 0.0, "avg_logprob": -0.1399933630678834, "compression_ratio": 1.7, "no_speech_prob": 0.015902617946267128}, {"id": 400, "seek": 185868, "start": 1883.28, "end": 1886.24, "text": " You can probably guess which side of the debate I fall on.", "tokens": [51594, 509, 393, 1391, 2041, 597, 1252, 295, 264, 7958, 286, 2100, 322, 13, 51742], "temperature": 0.0, "avg_logprob": -0.1399933630678834, "compression_ratio": 1.7, "no_speech_prob": 0.015902617946267128}, {"id": 401, "seek": 188624, "start": 1886.28, "end": 1891.52, "text": " But let me know your thoughts about how broken academic peer review is in the comments below.", "tokens": [50366, 583, 718, 385, 458, 428, 4598, 466, 577, 5463, 7778, 15108, 3131, 307, 294, 264, 3053, 2507, 13, 50628], "temperature": 0.0, "avg_logprob": -0.25271406173706057, "compression_ratio": 1.359375, "no_speech_prob": 0.2595765292644501}, {"id": 402, "seek": 188624, "start": 1891.52, "end": 1895.24, "text": " Or thoughts about the actual Mamba architecture itself, I guess that's fine too.", "tokens": [50628, 1610, 4598, 466, 264, 3539, 376, 23337, 9482, 2564, 11, 286, 2041, 300, 311, 2489, 886, 13, 50814], "temperature": 0.0, "avg_logprob": -0.25271406173706057, "compression_ratio": 1.359375, "no_speech_prob": 0.2595765292644501}], "language": "en"}