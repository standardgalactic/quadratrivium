1
00:00:00,000 --> 00:00:03,680
This is an artificial intelligence image generator.

2
00:00:03,680 --> 00:00:10,640
Given a text description of a picture, it will create, out of nothing, an image matching that description.

3
00:00:10,640 --> 00:00:16,480
As you can see, it is capable of generating high quality images of all kinds of different scenes.

4
00:00:16,480 --> 00:00:18,240
And it's not just images.

5
00:00:18,240 --> 00:00:23,840
In recent years, generative AI models have been developed that can generate text,

6
00:00:24,000 --> 00:00:29,600
audio, code, and soon videos too.

7
00:00:31,520 --> 00:00:36,960
All of these models are based on the same underlying technology, namely deep neural networks.

8
00:00:38,080 --> 00:00:43,120
In a few of my previous videos, I've explained how and why deep neural networks work so well,

9
00:00:43,680 --> 00:00:47,520
but I only explained how neural nets can solve prediction tasks.

10
00:00:48,240 --> 00:00:54,000
In a prediction task, the neural net is trained with a bunch of examples of inputs and their labels,

11
00:00:54,000 --> 00:00:58,480
and tries to predict what the label will be for a new input which it hasn't seen before.

12
00:00:59,360 --> 00:01:05,120
For example, if you trained a neural net on images labeled with the type of object appearing in each

13
00:01:05,120 --> 00:01:10,640
image, that neural net would learn to predict which object a human would say is in an image,

14
00:01:10,640 --> 00:01:13,120
even for new images which it hasn't seen before.

15
00:01:13,440 --> 00:01:19,520
Under the hood, the way that prediction tasks are solved is by converting the training dataset

16
00:01:19,520 --> 00:01:24,000
into a set of points in a space, and then fitting a curve through those points.

17
00:01:24,640 --> 00:01:28,080
So prediction tasks are also known as curve fitting tasks.

18
00:01:29,360 --> 00:01:34,160
And while prediction is certainly cool and very useful, it's not generation, right?

19
00:01:34,880 --> 00:01:38,400
This model is just fitting a curve to a set of points.

20
00:01:38,400 --> 00:01:40,000
It can't produce new images.

21
00:01:40,800 --> 00:01:45,040
So where does the creativity of these generative models come from,

22
00:01:45,040 --> 00:01:47,120
if neural nets can only do curve fitting?

23
00:01:48,880 --> 00:01:52,560
Well, all of these generative models are in fact just predictors.

24
00:01:53,360 --> 00:01:58,720
Yep, it turns out that the process of producing novel works of art can be reduced to a curve

25
00:01:58,720 --> 00:02:02,800
fitting exercise. And in this video, you'll learn exactly how.

26
00:02:04,960 --> 00:02:09,120
Suppose that we have a training dataset consisting of a bunch of images.

27
00:02:09,120 --> 00:02:14,240
We want to train a neural net to create new images which are similar in style to these

28
00:02:14,240 --> 00:02:20,320
training images. The first thing you might try is to simply use the images as labels

29
00:02:20,320 --> 00:02:26,000
to train the predictor. Here, we don't care about the mapping from inputs to outputs,

30
00:02:26,000 --> 00:02:30,960
so we can just use anything we like for the inputs, for example, a completely black image.

31
00:02:31,520 --> 00:02:35,680
Predictors learn to map inputs to outputs according to their training data.

32
00:02:35,680 --> 00:02:41,040
So this predictor, once trained, should be able to map the dummy or black image to

33
00:02:41,040 --> 00:02:44,240
new images like those seen in the training set, right?

34
00:02:48,400 --> 00:02:52,640
Uh, okay, maybe not quite. That didn't work so well.

35
00:02:52,640 --> 00:02:56,880
Instead of producing a nice beautiful picture, we just got this blurry mess.

36
00:02:58,000 --> 00:03:01,360
This demonstrates a very important fact about predictors.

37
00:03:01,360 --> 00:03:06,480
If there are multiple possible labels for the same input, the predictor will learn to output

38
00:03:06,480 --> 00:03:12,720
the average of those labels. For traditional classification tasks, this isn't really a

39
00:03:12,720 --> 00:03:17,280
problem because the average of multiple class labels can still be a meaningful label.

40
00:03:18,720 --> 00:03:22,800
For example, this image could plausibly be given two different labels.

41
00:03:22,800 --> 00:03:28,800
Both cat and dog would be valid labels. In this case, a classifier would learn to output the

42
00:03:28,800 --> 00:03:34,640
average of those labels, which means you end up with the score of 0.5 cat and 0.5 dog,

43
00:03:35,280 --> 00:03:41,280
which is still a useful label. In fact, it's arguably a better label than either of the original

44
00:03:41,280 --> 00:03:47,200
ones. On the other hand, when you average a bunch of images together, you do not get a

45
00:03:47,200 --> 00:03:54,320
meaningful image out, you just get a blurry mess. Let's try something a bit easier this time.

46
00:03:54,320 --> 00:03:59,520
How about instead of generating a new image from scratch, we try to complete an image which has

47
00:03:59,520 --> 00:04:05,120
part of it missing. In fact, let's make this really easy and suppose there is only one missing

48
00:04:05,120 --> 00:04:11,360
pixel, say the bottom right pixel. Can we train a neural net to predict the value of this one

49
00:04:11,360 --> 00:04:17,680
missing pixel? Well, as before, the neural net is going to output the average of plausible values

50
00:04:17,680 --> 00:04:22,800
that the missing pixel can take. But since it's only one pixel that we're predicting,

51
00:04:22,800 --> 00:04:28,400
the average value is still meaningful. The average of a bunch of colors is just another color.

52
00:04:28,400 --> 00:04:37,440
There's no blurring effect. So this model works perfectly fine. And we can use the value predicted

53
00:04:37,440 --> 00:04:41,360
by this neural net to complete images which are missing the bottom right pixel.

54
00:04:43,840 --> 00:04:48,400
Great, so we can complete images with one missing pixel. What about two?

55
00:04:48,800 --> 00:04:54,240
Well, we can do the same thing again. Train another neural net on images with two missing pixels,

56
00:04:54,240 --> 00:04:59,440
using the value of the second missing pixel as the label, and then use this neural net to fill

57
00:04:59,440 --> 00:05:07,440
in the second missing pixel. Now we have an image with just one missing pixel, and so we can use

58
00:05:07,440 --> 00:05:16,480
the first neural net to fill in that. Great. And we can do this for every pixel in the image.

59
00:05:17,440 --> 00:05:23,040
Train a neural net to predict the color of that pixel when it and all of the subsequent pixels

60
00:05:23,040 --> 00:05:29,520
are missing. Now we can complete an image starting from a fully black image and filling in one pixel

61
00:05:29,520 --> 00:05:35,920
at a time. Crucially, each neural net only predicts one pixel, and so there's no blurring effect.

62
00:05:38,000 --> 00:05:42,400
And there we have it. We have just generated a plausible image out of nothing.

63
00:05:43,120 --> 00:05:48,080
There's just one small problem. If we run this model again, it will generate exactly the same

64
00:05:48,080 --> 00:05:54,640
image. Not very creative, is it? But not to worry, we can fix this by introducing a bit of random

65
00:05:54,640 --> 00:06:00,800
sampling. You see, all predictors actually output a probability distribution over possible labels.

66
00:06:02,480 --> 00:06:07,120
Usually, we just take the label with the largest probability as the predicted value.

67
00:06:07,920 --> 00:06:14,400
But if we want diversity in our outputs, we can instead randomly sample a value from this

68
00:06:14,400 --> 00:06:20,240
probability distribution. This way, each time the model is run, it will sample different values

69
00:06:20,240 --> 00:06:25,200
at each step, which therefore changes the prediction for subsequent steps, and we get a

70
00:06:25,200 --> 00:06:30,640
completely different image each time. Now we have an interesting image generator.

71
00:06:31,120 --> 00:06:36,560
But still, at the end of the day, this model is made of predictors. They take as input a

72
00:06:36,560 --> 00:06:42,160
partially masked image, and predict the value of the next pixel. The only difference between this

73
00:06:42,160 --> 00:06:47,920
and a traditional image classifier is the label we used for training. The labels for our generator

74
00:06:47,920 --> 00:06:53,520
happen to be pixel colors, which come from the original image itself, and not a human labeler.

75
00:06:53,520 --> 00:06:58,800
This is a very important point in practice. It means we don't need humans to manually label

76
00:06:58,880 --> 00:07:04,480
images for this model. We can just scrape unlabeled images off the internet. But from the point of

77
00:07:04,480 --> 00:07:09,680
view of the neural net, it doesn't know, nor does it care, that the label came from the original

78
00:07:09,680 --> 00:07:15,200
image. As far as it's concerned, this is just a curve fitting exercise like any other.

79
00:07:16,400 --> 00:07:21,920
The generative model we've just created is called an autoregressor. We have a removal process,

80
00:07:21,920 --> 00:07:27,680
which removes pixels one at a time, and we train neural nets to undo this process. Generative

81
00:07:28,400 --> 00:07:30,960
generating and adding back in pixels one at a time.

82
00:07:32,880 --> 00:07:37,840
This is actually one of the oldest generative models. The very earliest use of autoregression

83
00:07:37,840 --> 00:07:44,000
dates back to 1927, where it was used to model the timing of sunspots. But autoregressors are

84
00:07:44,000 --> 00:07:52,160
still in use today. Most notably, ChatGPT is an autoregressor. ChatGPT generates text by using

85
00:07:52,160 --> 00:07:57,520
a transformer classifier to output a probability distribution over possible next words, given

86
00:07:57,520 --> 00:08:05,040
a partial piece of text. However, autoregressors are not used to generate images anymore. And the

87
00:08:05,040 --> 00:08:10,160
reason is that, while they can generate very realistic images, they take too long to run.

88
00:08:11,440 --> 00:08:16,720
In order to generate a sample with an autoregressor, we need to evaluate a neural net once for every

89
00:08:16,720 --> 00:08:22,720
element. This is fine for generating a few thousand words to make a piece of text, but large images

90
00:08:22,800 --> 00:08:28,320
can have tens of millions of pixels. How can we get away with fewer neural net evaluations?

91
00:08:33,600 --> 00:08:39,520
For our autoregressor, we removed pixels one at a time. But we don't have to remove only one

92
00:08:39,520 --> 00:08:46,320
pixel. We could, for example, remove a 4x4 patch of pixels at a time, and train the neural net to

93
00:08:46,320 --> 00:08:52,400
predict all 16 missing pixels at once. This way, when we use our model to generate an image,

94
00:08:52,400 --> 00:08:58,320
it can produce 16 pixels per evaluation, and so generation is 16 times as fast.

95
00:09:01,280 --> 00:09:07,200
But there is a limit to this. We can't generate too many pixels at the same time. In the extreme

96
00:09:07,200 --> 00:09:12,720
case, if we try to generate every pixel in the image at once, then we're back to the original

97
00:09:12,720 --> 00:09:17,840
problem. There are many possible labels that get averaged together into a blurry mess.

98
00:09:18,000 --> 00:09:23,120
To be clear, the reason why the image quality degrades is that, when we predict a bunch of pixels

99
00:09:23,120 --> 00:09:29,280
at the same time, the model has to decide on the values for all of them at once. There are lots

100
00:09:29,280 --> 00:09:35,040
of plausible ways that this missing patch could be filled in, and so the model outputs the average

101
00:09:35,040 --> 00:09:41,360
of those. The model isn't able to make sure that the generated values are consistent with each other.

102
00:09:41,520 --> 00:09:46,400
In contrast, when we predict one pixel at a time, the model gets to see the previously

103
00:09:46,400 --> 00:09:51,840
generated pixels, and so the model can change its prediction for this pixel to make it consistent

104
00:09:51,840 --> 00:09:56,960
with what has already been generated. This is why there's a trade-off. The more pixels we

105
00:09:56,960 --> 00:10:02,320
generate at once, the less computation we need to use, but the worse the quality of the generated

106
00:10:02,320 --> 00:10:08,400
images will be. Although this problem only arises in the end of the video, it's still

107
00:10:08,800 --> 00:10:14,160
although this problem only arises if the values we are predicting are related to each other.

108
00:10:14,720 --> 00:10:20,160
Suppose that the values were statistically independent of each other, that is, knowing

109
00:10:20,160 --> 00:10:26,160
one of them does not help to predict any others. In this case, the model doesn't need to look at

110
00:10:26,160 --> 00:10:31,200
the previously generated values since knowing what they were wouldn't change its prediction for the

111
00:10:31,200 --> 00:10:37,600
next value anyway. In this case, you can predict all of them at the same time without any loss in

112
00:10:37,600 --> 00:10:45,120
quality. So that means, ideally, we want our model to generate a set of pixels that are unrelated

113
00:10:45,120 --> 00:10:51,760
to each other. For natural images, nearby pixels are the most strongly related because they are

114
00:10:51,760 --> 00:10:57,840
usually part of the same object. Knowing the value of one pixel very often gives you a good idea of

115
00:10:57,840 --> 00:11:03,680
what color nearby pixels will be. This means that removing pixels in contiguous chunks is

116
00:11:03,680 --> 00:11:10,000
actually the worst way to do it. Instead, we should be removing pixels that are far away from each

117
00:11:10,000 --> 00:11:17,600
other and hence more likely to be unrelated. So, if in each step we remove a random set of pixels

118
00:11:17,600 --> 00:11:23,280
and predict values for those, then we can remove more pixels in each step for the same loss in

119
00:11:23,280 --> 00:11:29,520
image quality compared to contiguous chunks. In order to minimize the number of steps needed

120
00:11:29,520 --> 00:11:34,320
for generation, we want the pixels we remove in each step to be as spread out as possible.

121
00:11:34,960 --> 00:11:40,480
Removing pixels in a random order is a pretty good way of maximizing the average spread,

122
00:11:40,480 --> 00:11:47,840
but there is an even better way. We can think of our generative model as two processes. A removal

123
00:11:47,840 --> 00:11:53,520
process that gradually removes information from the input until nothing is left, and a generation

124
00:11:53,520 --> 00:12:00,000
process that uses neural nets to undo the removal process, generating and adding back in information.

125
00:12:01,280 --> 00:12:07,280
So far, we have been completely removing pixels, but rather than completely removing a pixel,

126
00:12:07,280 --> 00:12:13,040
we could instead remove only some of the information from a pixel by, for example,

127
00:12:13,040 --> 00:12:19,520
adding a small amount of random noise to it. This means we don't know exactly what the original

128
00:12:19,520 --> 00:12:24,000
pixel value was, but we do know it was somewhere close to the noisy value.

129
00:12:25,840 --> 00:12:31,280
Now, instead of removing a bunch of pixels in each step, we can add noise to the entire image.

130
00:12:32,080 --> 00:12:37,600
This way, we can remove information from every pixel in the image in a single step,

131
00:12:37,600 --> 00:12:42,960
which is the most spread out way of removing information. And since it's more spread out,

132
00:12:42,960 --> 00:12:47,760
you can remove more information in each step for the same loss in generation quality.

133
00:12:50,000 --> 00:12:54,240
There is one small problem with this, though. When we want to generate a new image,

134
00:12:54,240 --> 00:12:58,080
we need to start the neural net off with some initial blank image.

135
00:12:58,960 --> 00:13:04,720
When we were removing pixels, then every image eventually ends up as a completely black image.

136
00:13:04,720 --> 00:13:07,840
So of course, that's where we start the generation process from.

137
00:13:08,800 --> 00:13:13,200
But now that we're adding noise, the values just keep getting larger and larger,

138
00:13:13,200 --> 00:13:18,000
never converging to anything. So where do we start the generation process from?

139
00:13:18,640 --> 00:13:22,240
We can avoid this problem by changing our nosing step slightly,

140
00:13:22,240 --> 00:13:26,240
so that we first scale down the original value and then add the noise.

141
00:13:27,120 --> 00:13:30,800
This ensures that when we repeat this nosing step many times,

142
00:13:30,800 --> 00:13:35,680
information from the original image will disappear, and the result will be equivalent

143
00:13:35,680 --> 00:13:41,040
to a pure random sample from the noise distribution. So we can start our generation

144
00:13:41,040 --> 00:13:47,600
process from any such noise sample. And there we have it. This is known as a denoising diffusion

145
00:13:47,600 --> 00:13:53,520
model. The overall form is identical to an autoregressor. The only difference is the way

146
00:13:53,520 --> 00:13:59,040
in which we remove information at each step. By adding noise, we can spread out the removal

147
00:13:59,040 --> 00:14:04,000
of information all across the image, which makes the predicted values as independent

148
00:14:04,000 --> 00:14:08,320
of each other as possible, allowing you to use fewer neural net evaluations.

149
00:14:09,440 --> 00:14:14,160
Empirically, diffusion rules can produce high-quality photorealistic images in about

150
00:14:14,160 --> 00:14:17,440
100 steps, whereas autoregressors would take millions.

151
00:14:20,000 --> 00:14:24,080
Now that we understand how these generative models work at a conceptual level, if you are

152
00:14:24,080 --> 00:14:29,040
ever going to implement these models in practice, there are a few important technical details that

153
00:14:29,040 --> 00:14:35,360
you should be aware of. First, in the procedure I described for autoregression, I use the different

154
00:14:35,360 --> 00:14:40,960
neural net in each step of the process. This is certainly the best way to get the most accurate

155
00:14:40,960 --> 00:14:46,000
predictions, but it's also very inefficient, since we need to train a whole bunch of different

156
00:14:46,000 --> 00:14:53,920
neural nets. In practice, you would just use the same model to do every step. This gives slightly

157
00:14:53,920 --> 00:14:59,600
worse predictions, but the savings and computation time more than make up for it. In order to train

158
00:14:59,600 --> 00:15:04,960
a single neural net to perform all of the generation steps, you would remove a random number of pixels

159
00:15:04,960 --> 00:15:10,240
from each input and train the neural net to predict the corresponding next pixel of each input.

160
00:15:11,280 --> 00:15:16,400
Additionally, you can also give the number of pixels removed as an input to the neural net

161
00:15:16,400 --> 00:15:19,520
so that it knows which pixel it's supposed to be generating.

162
00:15:20,640 --> 00:15:24,000
Now this one neural net can be used for all generation steps.

163
00:15:25,440 --> 00:15:29,840
In the setup I just described, for each training image, the neural net is trained on

164
00:15:29,840 --> 00:15:36,240
only one generation step for that image. But ideally, we would like to train it on every

165
00:15:36,240 --> 00:15:41,120
generation step of every image. We can get more use out of our training data that way.

166
00:15:42,160 --> 00:15:47,360
If you did this the naive way, you would have to evaluate the neural net once for every generation

167
00:15:47,360 --> 00:15:52,960
step, which means a lot more computation. Fortunately, there exist special neural net

168
00:15:52,960 --> 00:15:58,160
architectures known as causal architectures that allow you to train on all of these generation

169
00:15:58,160 --> 00:16:04,800
steps while only evaluating the neural net once. There exist causal versions of all of the popular

170
00:16:04,800 --> 00:16:10,000
neural net architectures such as causal convolutional neural nets and causal transformers.

171
00:16:10,720 --> 00:16:15,120
Causal architectures actually give slightly worse predictions, but in practice,

172
00:16:15,120 --> 00:16:20,240
autoregression is almost always done with causal architectures, because the training is so much

173
00:16:20,240 --> 00:16:25,760
faster. The generation process for causal architectures is still exactly the same though.

174
00:16:26,560 --> 00:16:31,440
For diffusion models, you can't use causal architectures, and so you do have to just

175
00:16:31,440 --> 00:16:34,480
train with each data point at a random generation step.

176
00:16:37,360 --> 00:16:43,120
I described the diffusion model as predicting the slightly less noisy image from the previous step.

177
00:16:43,120 --> 00:16:48,160
However, it's actually better to predict the original completely clean image at every step.

178
00:16:49,040 --> 00:16:54,960
The reason for this is it makes the job of the neural net easier. If you make it predict the

179
00:16:54,960 --> 00:17:00,480
noisy next step image, then the neural net needs to learn how to generate images at all different

180
00:17:00,480 --> 00:17:06,160
noise levels. This means the model will waste some of its capacity learning to produce noisy

181
00:17:06,160 --> 00:17:12,400
versions of images. If you instead just have the neural net always predict the clean image,

182
00:17:12,400 --> 00:17:16,960
then the model only needs to learn how to generate clean images, which is all we care about.

183
00:17:17,840 --> 00:17:23,040
You can then take the predicted clean image and reapply the nosing process to it to get

184
00:17:23,040 --> 00:17:29,520
the next step of the generation process. Except that when you predict the clean image,

185
00:17:29,520 --> 00:17:35,440
then at the early steps of the generation process, the model only has pure noise as input,

186
00:17:35,440 --> 00:17:41,680
so the original clean image could have been anything. And so you get a blurry mess again.

187
00:17:43,200 --> 00:17:48,880
To avoid this, we can train the neural net to predict the noise which was added to the image.

188
00:17:48,880 --> 00:17:53,360
Once we have a predicted value for the noise, we can plug it into this equation to get a

189
00:17:53,360 --> 00:17:59,120
prediction for the original clean image. So we are still predicting the original clean image

190
00:17:59,120 --> 00:18:04,800
just in a roundabout way. The advantage of doing it this way is that now the model output is

191
00:18:04,800 --> 00:18:10,400
uncertain at the latest stages of the generation process, since any noise could have been added

192
00:18:10,400 --> 00:18:15,680
to the clean image, so the model outputs the average of a bunch of different noise samples,

193
00:18:15,680 --> 00:18:22,640
which is still valid noise. So far we've just been generating images from nothing,

194
00:18:22,640 --> 00:18:27,680
but most image generators actually allow you to provide a text prompt describing the image you

195
00:18:27,760 --> 00:18:33,120
want to make. The way this works is exactly the same, you just give the neural net the text as

196
00:18:33,120 --> 00:18:38,560
an additional input at each step. These models are trained on pairs of images and their corresponding

197
00:18:38,560 --> 00:18:45,040
text descriptions, usually scraped from image alt text tags found on the internet. This ensures

198
00:18:45,040 --> 00:18:49,920
that the generated image is something for which the text prompt could plausibly have been given

199
00:18:49,920 --> 00:18:55,840
as a description of that image. In principle, you can condition generative models on anything,

200
00:18:55,840 --> 00:19:00,960
not just text, so long as you can find appropriate training data. For example,

201
00:19:00,960 --> 00:19:03,680
here is a generative model that is conditioned on sketches.

202
00:19:08,560 --> 00:19:12,400
Finally, there's a technique to make conditional diffusion models work better,

203
00:19:12,400 --> 00:19:18,240
called classifier free guidance. For this, during training, the model will sometimes be given the

204
00:19:18,240 --> 00:19:24,240
text prompts as additional input, and sometimes it won't. This way, the same model learns to do

205
00:19:24,240 --> 00:19:30,000
predictions with or without the conditioning prompt as input. Then at each step of the denoising

206
00:19:30,000 --> 00:19:35,840
process, the model is run twice, once with the prompt and once without. The prediction without

207
00:19:35,840 --> 00:19:41,040
the prompt is subtracted from the prediction with the prompt, which removes details that are generated

208
00:19:41,040 --> 00:19:46,480
without the prompt, leaving only the details that came from the prompt, leading to generations which

209
00:19:46,480 --> 00:19:52,800
more closely flow the prompt. In conclusion, generative AI, like all machine learning,

210
00:19:52,800 --> 00:19:58,640
is just curve fitting. And that's all for this video. If you enjoyed it, please like and subscribe,

211
00:19:58,640 --> 00:20:03,040
and if you have any suggestions for topics you'd like to see me cover in a future video,

212
00:20:03,040 --> 00:20:08,720
leave a comment below.

