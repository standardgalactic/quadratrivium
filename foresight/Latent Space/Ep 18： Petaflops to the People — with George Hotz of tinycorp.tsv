start	end	text
0	5080	Hey everyone, welcome to the Late In Space podcast.
5080	9800	This is Swix, writer and editor of Late In Space, and Alessio is taking over with the
9800	10800	intros.
10800	12640	Alessio's partner and CT1 residents at Decibel Partners.
12640	19640	Hey everyone, today we have GeoHot on the podcast, aka George Hots for the human name.
19640	22720	Everybody knows George, so I'm not going to do a big intro, a couple things that people
22720	23720	might have missed.
23720	28040	So you were the first to unlock the iPhone, you traded the first ever unlocked iPhone for
28040	31960	Nissan 350Z and three new iPhones.
31960	36680	You were then one of the first people to break into the PS3 around arbitrary code.
36680	40680	You got sued by Sony, you wrote a rap song to fight against that, which is still live
40680	44200	on YouTube, which we're going to have on the show notes.
44200	49520	Then you did not go to Tesla to build vision, and instead you started ComIi, which was an
49520	54600	amazing engineering feat in itself until you got a season disease from the government
54600	59200	to not put these things on the street, turned that into a research-only project.
59200	60200	You know they're out there.
60200	64800	Yeah, yeah, no, no, no, they're out there, but they're not a, you know, you market them
64800	66800	as a research kind of like no warranty.
66800	70040	Because I use the word DevKit, that's not about the government, that's nothing to do
70040	71040	with the government.
71040	72760	We offer a great one, you're warranty.
72760	77680	The truth about that is it's gatekeeping.
77680	81480	What's the difference between a DevKit and not a DevKit, nothing.
81480	85480	What's the question of do you think it's for you, and if you think it's for you, buy it.
85480	86480	It's a consumer product.
86480	87480	We call it a DevKit.
87480	90480	If you have a problem with that, it's not for you.
90480	91480	Great framing.
91480	93240	That's great insight.
93240	96440	And then I was going through your broadcast to get to the day, you've heard this post
96440	101440	about the hero's journey, and you link this thing called the portal story, which is kind
101440	106280	of the set of stories in movies and books about people living this arbitrary life and
106280	110760	then they run to this magic portals, kind of takes them into a new, very exciting life
110760	112040	and dimension.
112040	115800	When you've heard that post, you talked about TinyGrad, which is one of the projects you're
115800	116800	working on today.
116800	119960	And you mentioned this is more of a hobby, something that is not going to change the
119960	120960	course of history.
120960	124600	Obviously, you're not going full speed into it, so we will learn more about what was the
124600	128000	portal that you run into to get here.
128000	133880	Well, what you realize is, you know what made me realize that I absolutely had to do the
133880	134880	company?
134880	136960	Seeing Sam Maltlin go in front of Congress.
136960	138960	Why?
138960	142120	What are the odds they nationalize NVIDIA?
142120	146760	What are the odds that large organizations in the government, but of course I repeat
146760	152560	myself, decide to try to clamp down on accessibility of ML compute?
152560	159880	I want to make sure that can't happen structurally, so that's why I realize that it's really important
159880	160880	that I do this.
160880	164960	And actually, from a more practical perspective, I'm working with NVIDIA and Qualcomm to buy
164960	165960	chips.
165960	168800	NVIDIA has the best training chips, Qualcomm has the best inference chips.
168800	172160	Working with these companies is really difficult, so I'd like to start another organization
172160	177640	that eventually in the limit either works with people to make chips or makes chips itself
177640	181880	and makes them available to anybody.
181880	186120	You share kind of three core thesis to Tiny Core, maybe we can dive into each of them.
186120	192680	So, XLA, PrimeTorch, those are the complex instruction system, TinyGrad is the restricted
192680	197080	instruction system, so you're kind of focused on, again, TinyGrad being small, not being
197080	201640	over complicated and trying to get as close to the DSP as possible in a way where it's
201640	202640	at more.
202640	206240	Well, it's a very clear analogy from how processors developed.
206240	211120	So a lot of processors back in the day were CISC, complex instruction set, system 360
211120	213160	and then x86.
213160	215840	Then this isn't how things stayed.
215840	221320	They went to now the most common processors on ARM and people are excited about RISC
221320	222320	5.
222320	225000	RISC 5 is even less complex than ARM.
225000	229520	No one is excited about CISC processors anymore, they're excited about RISC Reduce Instruction
229520	230920	Set processors.
230920	237520	So TinyGrad is we're going to make a RISC offset for all ML models and yeah, it can
237520	243400	run all ML models with basically 25 instead of the 250 of XLA or PrimeTorch.
243400	245160	So about 10X less complex.
245160	246160	Yep.
246160	248760	You talk a lot about existing AI chips.
248760	252840	You said if you can write a fast ML framework for GPUs, you just can't write one for your
252840	253840	own chip.
253840	257400	So that's another one of your core insights, I don't know if you want to expand on that.
257400	258400	Yeah.
258400	259560	I mean, your chip is worse, right?
259560	262680	There's no way the chip that you're going to tape out, especially on the first try is
262680	265760	going to be easier to use than an AMD GPU, right?
265760	268400	And yet there's no good stack for AMD GPUs.
268400	271480	So why do you think you can make one for your chip?
271480	272480	You can't.
272480	273480	Right?
273480	277480	The only company, there's one other company, aside from Nvidia, who's succeeded at all
277480	278800	at making training chips.
278800	280800	What company?
280800	282800	AMD.
282800	283800	Intel?
283800	284800	No.
284800	285800	No.
285800	286800	No.
286800	287800	I've never trained.
287800	288800	Who's trained a model on AMD or Intel?
288800	289800	Nobody on AMD.
289800	290800	Cerebrus.
290800	294120	Cerebrus, I'm talking about, you might know some startups who trained models on these
294120	295120	chips.
295120	299480	I'm surprised no one immediately gets this because there is one other chip, aside from
299480	302240	Nvidia, that normal people have actually used for training.
302240	303880	That's a real neural engine?
303880	304880	No.
304880	305880	Used for training.
305880	306880	No.
306880	308880	You can only buy them in the cloud.
308880	309880	Oh, TPU.
309880	310880	Exactly.
310880	311880	Yeah.
311960	313800	So, mid-journey is trained on TPU.
313800	314800	Right?
314800	319080	Like, a lot of startups do actually train on TPUs, and they're the only other successful
319080	321280	training chip, aside from Nvidia.
321280	326240	But what's unique about Google is that they also wrote their own ML framework, right?
326240	330040	And if you can't write your own ML framework that is performant on Nvidia, there's no way
330040	332720	you're going to make it performant on your...
332720	335960	And they started from TensorFlow, and then they made the chip after.
335960	336960	Yeah.
336960	337960	Exactly.
337960	338960	Exactly.
338960	339960	And you have to do it in that direction.
339960	344560	Because you're going to end up, you know, a service, what are those things, a million
344560	345560	dollars?
345560	346560	I've never seen a service.
346560	349640	No one's ever like, oh, I trained my model on a service.
349640	352480	Most people are like, I trained my model on TPUs.
352480	356080	Some people, 20%, are like, I trained my model on TPUs.
356080	357080	Yeah.
357080	361480	And then the third one, which is the one that surprised me the most is, through incompleteness,
361480	362480	it's harmful.
362480	363480	It should be avoided.
363480	369240	It made sense once I read it, but maybe tell us a bit or more about how you got there.
369600	370600	Okay.
370600	378800	So, CPUs devote tons of their silicon and power to things like reorder buffers and speculative
378800	381440	execution and branch predictors.
381440	385560	And the reason that you need all these things is because at compile time, you can't understand
385560	387400	how the code's going to run.
387400	388800	This is Rice's theorem.
388800	391120	This is the Halting problem, and it's limit.
391120	393160	And this is not like, oh, the Halting problem is theoretical.
393160	394160	No, no, no, no.
394160	395360	It's actually very real.
395360	397320	Does this branch get taken or not?
397320	398320	It depends on X.
398320	399320	Where does X come from?
399320	401000	I forget it, right?
401000	404400	But no branches depend on X in a neural net.
404400	406120	Every branch is a static loop.
406120	410240	Like if you're doing a matrix multiple, it's a static loop over the inner dimension.
410240	411960	And neural networks are even better.
411960	413960	No loads even depend on X, right?
413960	416840	So with a GPU shader, right, you're like, your load might depend on which texture you're
416840	418320	actually loading into RAM.
418320	421440	But with a neural network, your load is, well, I load that way.
421440	422440	Why?
422440	424760	Well, because I load that way the other million times I ran the same net.
424760	429440	Every single time you run the net, you do the exact same set of loads, stores, and arithmetic.
429440	432320	The only thing that changes is the data.
432320	438680	And this gives you a very powerful ability to optimize that you can't do with CPU-style
438680	442640	things, which have branches, and even GPU-style things, which have loads and stores.
442640	444040	Oh, that makes sense.
444040	448080	Well, GPUs, if you want GPU-style stuff, you have load based on X, you now need a cache
448080	453680	hierarchy, and not an explicit cache hierarchy, an implicit cache hierarchy, with eviction
453680	456800	policies that are hard-coded into the CPU.
456800	461760	You start doing all this stuff, and you're never going to get theoretically good performance.
461760	463720	Again, I don't think there's 100X.
463720	466760	Some startups will talk about 100X, and they'll talk about absolutely ridiculous things like
466760	469080	clockless computing or analog computing.
469080	471920	Okay, analog computing just won't work.
471920	478080	And clockless computing, sure, it might work in theory, but your ETA tools are...
478080	482760	Maybe AIs will be able to design clockless chips, but not humans.
482880	486920	What actually is practical is changing cache hierarchies and removing branch predictors
486920	488440	and removing warp schedulers.
488440	491880	GPUs spend tons of power on warp scheduling, because we have to hide the latency from the
491880	492880	memory.
492880	496040	We'll have to hide the latency if everything's statically scheduled.
496040	500120	What do you think people are still hanging on to during complete?
500120	502640	Well, because it's really easy.
502640	504040	Turning complete is just really easy.
504040	510480	It's really easy to just be so nice if I could do an if statement here and actually branch
510480	512080	the code.
512080	517600	So it requires a lot more thought to do it without turning completeness.
517600	520800	And would this be qualitatively different than TPUs?
520800	522120	So TPUs are a lot closer.
522120	523120	Yeah.
523120	526200	TPUs are a lot closer to what I'm talking about than like CUDA.
526200	527200	Okay.
527200	528200	So what is CUDA?
528200	533760	Well, CUDA is a C-like language, which compiles to an LLVM like IR, which compiles to PTX,
533760	537240	which compiles to SAS, which are all turned complete.
537240	538920	TPUs are much more like this, yeah.
538920	541280	Their memory is pretty statically managed.
541640	544240	I did some reverse engineering on the TPU.
544240	546160	It's published in TinyGrad.
546160	549680	It has like a VLIW instruction and it runs them.
549680	550680	So it's similar.
550680	552360	I think the TPUs have a few problems.
552360	555440	I think systolic arrays are the wrong choice.
555440	558000	Systolic array, I think they have systolic arrays because that was the guy's PhD.
558000	559000	Right.
559000	560000	And of course, Amazon makes...
560000	561000	Jake, could you summarize systolic arrays for this?
561000	562000	Systolic arrays are just...
562000	563000	Okay.
563000	567480	So basically you have like, this is a way to do matrix multiplication, think of a grid
567480	571640	of malax and then the grid can multiply and then shift, multiply, then shift, multiply,
571640	572640	then shift.
572640	577640	And they are very power efficient, but it becomes hard to schedule a lot of stuff on
577640	583000	them if you're not doing like perfectly sized dense matrix multiplies, which you can argue,
583000	588400	well, design your models to use perfectly sized dense matrix multiplies, sure, but it's
588400	589400	just...
591400	594920	No, but thanks for indulging on these explanations.
594920	599880	I think we need to keep our audience along with us by pausing every non-dent to explain
599880	600880	key terms.
600880	605800	You know, when I say explain a systolic array, I just immediately get a picture in my head
605800	607760	of like tilting a matrix and shifting it.
607760	609160	It's hard to kind of explain.
609160	610160	Yeah.
610160	613600	We'll do some videos so you have your hand actions and we edit it in visuals.
613600	614600	Yeah.
614600	617560	There's some great graphics that just show you, oh, so that's what a systolic array is,
617560	622480	but it's a malax shift machine that looks kind of different from the typical like APU
622480	623480	sort of machine.
623480	628040	Sorry, ALU sort of machine, I think the right answer is something that looks more like queues
628040	632760	that feed into ALUs and then you can like prefetch the loads from the memory, put it
632760	639440	a bunch of queues and then the queue is just like and feeds into another queue over here.
639440	642360	But yeah, but that's not even the main problem with TPUs.
642360	645440	The main problem with TPUs is that they're closed source, not only is the chip closed
645440	651560	source, but all of XLA is open source, but the XLA to TPU compiler is a 32 megabyte binary
651560	654400	blob called libTPU on Google's cloud instances.
654400	658520	It's all closed source, it's all hidden stuff and you know, well, there's a reason Google
658520	662200	made a closed source, Amazon made a clone of the TPU, it's called Inferencia, or they
662200	665480	have some other name for it, a training, training, yeah, yeah, yeah, yeah, yeah, and you look
665480	667280	as a clone of the TPU.
667280	672120	It just software doesn't work though, and the Google software at least kind of works.
672120	674400	So those are kind of like the three quarter thesis.
674400	678720	The first thing you're working on that you've been working on is TinyGrad, and one of your
678720	682040	Twitch streams, he said, is the best thing you've ever written.
682040	686840	Yeah, tell us a bit more about that creation.
686840	690800	For a long time, TinyGrad had a hard limit of 1,000 lines of code.
690800	696080	And what this would force you to do is really make sure you were not wasting lines.
696080	700600	I got rid of the restriction because it became a little code golfy at the end, but once like
700600	707400	the core framework of TinyGrad was there, in those 1,000 lines, it's not huge now, it's
707480	710680	like 2,800 lines now, it's still very readable.
710680	716440	But like the core framework, the ideas are expressed with no boilerplate.
716440	720000	If you go read PyTorch, you know, PyTorch, I think it's actually pretty good code.
720000	725480	I think Facebook's pretty good, but there's so much boilerplate.
725480	730080	Go in PyTorch and try to track down how an LGU actually works.
730080	731080	Just a lot of distractions.
731080	736840	Oh, you're going to be diving down a long stack from Python to C to custom libraries
736880	739920	to dispatchers to, and then I don't even know how to read TensorFlow.
739920	744480	I don't even know where's an LGU in TensorFlow, nobody knows.
744480	746720	Someone at Google knows maybe.
746720	751640	Google as an organism, I don't know if anyone individually Google knows.
751640	755440	What are like the important ergonomics like for a developer as you think about designing
755440	756920	the TinyGrad API?
756920	760720	So the TinyGrad front end looks very similar to PyTorch.
760720	764760	There's an even higher level front end you can use for TinyGrad, which is just Onyx.
764840	769000	We have better support for Onyx than Core ML does, and we're going to have, I think
769000	770720	we're going to pass Onyx Runtime soon, too.
770720	773080	I like people think Onyx Runtime, that's a gold standard for Onyx.
773080	774080	No, you can do better.
774080	775080	Pass them in what?
775080	776080	Specifically?
776080	777400	Test, compliance tests.
777400	781880	So Onyx has a big set of compliance tests that you can check out.
781880	784880	And we have the running in TinyGrad, and there's some failures.
784880	788080	We're below Onyx Runtime, but we're beyond Core ML.
788080	791480	So that's where we are on Onyx support now, but we will pass, we will pass Onyx Runtime
791480	796240	soon, because it becomes very easy to add ops, because of how you don't need to do anything
796240	797240	at the lower levels.
797240	800640	You just do it at this very high level, and TinyGrad compiles it to something that's
800640	803160	fast using these minimal ops.
803160	808000	You can write, I mean, most concretely, what TinyGrad can do that PyTorch can't really
808000	813840	do is if you have something like A times B plus C, if you write that in naive PyTorch,
813840	818800	what it's going to do on the GPU is, well, read A, read B in a kernel, and then store
818800	825440	A times B in memory, and then launch another kernel to do A times B plus C, got to do those
825440	826440	loads for memory.
826440	829640	I know I did a whole extra round trip to memory that I just didn't have to do.
829640	832560	You're like, yeah, but you can use the TorchJIT and it corrects this.
832560	839200	Yeah, for that one example, for that one example of MULAC, but oh, now you did three multiplies,
839200	842040	six multiplies, right?
842040	844160	It won't compile arbitrary code.
844640	849720	If you looked into the other approaches like PyTorch Lightning to accelerate PyTorch itself.
849720	854840	Well, PyTorch Lightning, my understanding is it's mostly a framework around PyTorch, right?
854840	858440	PyTorch Lightning is not going to fix this fundamental problem of I multiply six tensors
858440	859440	together.
859440	863480	Why is it going to memory any more than a single read from each and a single write to the output?
863480	864480	Okay.
864480	871040	There are lower level things in PyTorch that are not exactly sure what Dynamo does, but
871040	874120	I know they're generating some Triton stuff, which is going to generate the kernels on
874120	875120	the fly.
875120	879120	But you know, PyTorch Lightning is at a higher level of abstraction.
879120	881600	So TinyGrid's front-end stuff looks like PyTorch.
881600	882600	I made a few tweaks.
882600	883840	There's a few things I don't like about PyTorch.
883840	885440	Why is ReLU a class?
885440	886440	Oh, really?
886440	889000	What was the state?
889000	890440	You make a class and there's a state.
890440	894160	Everything should just be Torch Functional and ReLU, but just dot ReLU on the tensor.
894160	899560	Also there's things in Torch where you have to do tensor dot and not a tensor dot.
899560	900560	Right?
900600	901600	Why?
901600	902600	Why are these things?
902600	907640	It just shows an API that's not perfectly refined, but when you're doing stuff TinyGrid
907640	911560	style where you don't have lines, well, it has to work this way because even the lines
911560	917200	to express the, well, you can't use the where operator unless, and the where operator in
917200	918200	PyTorch.
918200	922400	Why is it a true case, condition, false case?
922400	924400	The worst, that's how Python expresses ifs.
924400	925400	It's disgusting.
925400	929680	Where operators are much nicer, it should be, I can do my like, a less than zero dot
929680	933120	where, a comma one, right?
933120	935480	The very pandas like API.
935480	940200	Yeah, yeah, yeah, yeah, yeah, it's just, it's some, it looks like Torch numpy pandas.
940200	941600	They're all very similar.
941600	945320	I tried to take like the cleanest subset of them and express them, but like I said, you
945320	947400	can also interact with it using Onyx.
947400	948400	Yeah.
948400	951600	But I have a rewrite of stable diffusion, I have a rewrite of llama, I have a rewrite
951600	952600	of whisper.
952600	953600	You can look at them.
953600	954600	They're shorter than the Torch version than I think they're cleaner.
954600	955600	They stream them all.
955600	956600	Yeah.
956600	957600	Very nice.
957600	963000	Laziness is kind of the other important concept that you're leveraging to do, operation fusing.
963000	965840	Yeah, talk a bit more about that.
965840	973880	So yeah, you have, you have basically like a few different like models for compute.
973880	974880	The simplest one's eager.
974880	981120	All right, the simplest one is eager as soon as the interpreter or sees A times B, it actually
981120	983720	dispatches A times B, right?
983720	990320	Then you have graph like TensorFlow, which will put A times B into a graph and then we'll
990320	996280	do absolutely nothing until you actually compile the graph at the end.
996280	1000080	I like this third choice, just somewhere in the middle, laziness.
1000080	1002800	Laziness is you don't know when the ops are going to dispatch and don't worry about that.
1002800	1004560	You don't have to worry about this as a programmer.
1004560	1006120	You just write out all your stuff.
1006120	1010600	And then when you actually type dot numpy, it'll be ready by the time you copy the thing
1010600	1011600	back to CPU.
1011600	1017760	Or you can do dot realize and it will actually like force that tensor to be allocated in RAM.
1017760	1021640	But yeah, a lot of times, right, like, and if you think about it, PyTorch is kind of
1021640	1024960	lazy in a way, but they didn't extend the paradigm far enough, right?
1024960	1029600	When I do A times B in PyTorch, it's going to launch a CUDA kernel to do A times B, but
1029600	1032120	it's not going to wait for that CUDA kernel to complete.
1032120	1033840	So you're getting the worst possible world.
1033840	1038120	You're getting the same laziness, but you also can't get fusion because PyTorch doesn't
1038120	1041560	know that I'm then going to do plus C. There's no way for it to be like, whoa, whoa, whoa,
1041560	1045200	don't launch that CUDA kernel, whoa, just do this one too, right?
1045200	1050160	You can kind of like, again, this stuff, PyTorch is working on this.
1050160	1051720	And you know, it's a little bit harder.
1051720	1054920	Like in comma, I felt like I was competing against a lot of idiots here.
1054920	1060960	I'm competing against, you know, smart, smart, very smart people who made, yeah, who've made
1060960	1062760	some, I think, different trade offs, right?
1062760	1066440	We've made some different trade offs, whereas if you're trying to build something that is
1066440	1070840	just straight up good on Nvidia, and we have a lot of people in complexity to throw at
1071120	1072800	PyTorch made a lot of the right choices.
1072800	1077160	I'm trying to build something that manages complexity, like you can always make your
1077160	1078400	software do more.
1078400	1082160	The magic is when you can make your software do more without adding complexity, right?
1082160	1086240	Because, you know, complex things eventually collapse under the wind.
1086240	1087680	So it's kind of that.
1087680	1089520	How does fusing actually work?
1089520	1092360	Like TensorFlow actually collapsed under it.
1092360	1095400	It's kind of what happened, right?
1095400	1097280	How does fusing actually work?
1097280	1103600	So yeah, there's this thing called lazy.py, and when you do like a times b, that's, it's
1103600	1107080	put into a graph, but it's a very local graph.
1107080	1110520	There's no global graph optimizations, and even this can change, right?
1110520	1114920	Again, like the programming model for TinyGrad does not preclude eagerness, right?
1114920	1117240	Laziness is not guaranteed laziness.
1117240	1119200	It's just going to try its best.
1119200	1121800	So you put in a times b, and that's a binary app, right?
1121800	1124560	And then you put in a times b, like that's a node in the graph.
1124560	1126680	That's a virtual node, because it's not realized yet.
1126680	1127680	Plus c.
1127680	1130880	Okay, here's a new node, which takes the c tensor in here and takes the output of a
1130880	1131880	times b.
1131880	1133280	It's like, whoa, wait, there's two binary ops.
1133280	1135280	Okay, we'll just fuse those together, okay?
1135280	1136280	Here I have a kernel.
1136280	1138280	This kernel has a, b, and c as inputs.
1138280	1143760	It does a times b plus c in the local registers, and then outputs that to memory.
1143760	1147360	And you can graph dot one in TinyGrad.
1147360	1151880	Another like amazing thing that TinyGrad has that I've not seen in any other framework
1151880	1153520	is two things.
1153520	1156080	Graph dot one, graph equals one, which is an environment variable.
1156080	1158280	It will output a complete graph of all the operations.
1158280	1161600	People are like, oh, you can use PyTorch, export it to Onyx, and use Netron.
1161600	1164680	Yeah, you can, but like, what?
1164680	1165680	That's not what's real.
1165680	1169840	Right, graph dot one will show you the actual kernels that were dispatched to the GPU.
1169840	1175440	You can also type debug equals two, which will print those kernels out in your command
1175440	1181360	line, and it will tell you the exact number of flops and the exact number of memory accesses
1181360	1182360	in each kernel.
1183000	1187040	So you can immediately see, wait a second, okay, this kernel used this many flops.
1187040	1188540	This was the gigaflops.
1188540	1191240	This is how many bytes it read, and this was the gigabytes per second.
1191240	1196120	And then you can profile without having to like, okay, I mean in theory in PyTorch, sure,
1196120	1197800	use the NVIDIA insight profiler.
1197800	1198800	No one does that.
1198800	1203120	No one does, of course, because it's so difficult, right, like, like, actually NVIDIA used to
1203120	1206960	a pre, pre, I think kuda nine was the last one that had it.
1206960	1211160	They had a command line one, but now it's like, okay, I'm going to generate this blob,
1211160	1215480	use this NVIDIA GUI tool to convert it into a Chrome trace, and then load it, and yeah,
1215480	1216480	no one does that, right?
1216480	1220320	I'll just type debug equals two in any tiny grad model, and it will show you all the kernels
1220320	1223680	that it launches, and the efficiency of each kernel, basically.
1223680	1229720	Yeah, this is something that John Karmic has often commented about, is that when you code,
1229720	1234000	you need to build in your instrumentation or observability right into that.
1234000	1238360	I wonder if whatever John is working on, he's adopting this style, and maybe you can sort
1238360	1245760	of encourage it by, like, I don't know, naming it and coining it as a certain kind of debugging
1245760	1246760	style.
1246760	1251840	If he would like to start contributing to tiny grad, I'd be, I don't know, I've chatted
1251840	1252840	with a few times.
1252840	1253840	I'm not really sure what his company's doing.
1253840	1254840	Yeah.
1254840	1259880	I think it's all, I think it's pretty, but no, I mean, hopefully, like, we get tiny grad
1259880	1264280	to a point where people actually want to start using it.
1264280	1269120	So tiny grad right now is uncompetitive on, it's uncompetitive on NVIDIA, and it's uncompetitive
1269120	1270120	on x86.
1270120	1272640	And specifically, what do you care about when you say uncompetitive?
1272640	1273640	Speed.
1273640	1274640	Okay.
1274640	1275640	Shut up, speed.
1275640	1276640	It's correct.
1276640	1277640	The correctness is there.
1277640	1281080	The correctness for both forwards and backwards passes is there, but on NVIDIA, it's about
1281080	1284600	5x slower than PyTorch right now, like 5x, wow, this is, this is unsurmountable.
1284600	1287840	No, there's reasons it's 5x slower, and I can go through how we're going to make it
1287840	1290840	faster, and it used to be, you know, 100x slower, so, you know, we're making progress,
1290840	1295880	but there's one place where it actually is competitive, and that's Qualcomm GPUs.
1295880	1300320	So tiny grad is used to run the model in OpenPilot, like right now, it's been live in production
1300320	1307400	now for six months, and tiny grad is about 2x faster on the GPU than Qualcomm's library.
1307400	1309120	Why specifically Qualcomm?
1309120	1313400	Well, because we have Qualcomm, we use Qualcomm in the Comma devices.
1313400	1316920	Oh, I mean, like, what makes, what makes, what about Qualcomm architecture?
1316920	1318200	Oh, what makes it doable?
1318200	1319200	Yeah.
1319200	1322720	Well, Qualcomm has spent how many millions of man-hours to make NVIDIA fast, and Qualcomm
1322720	1326360	has a team of 10 Qualcomm engineers, okay, well, who can I be here?
1326360	1331240	Like, what I propose with, what I propose with tiny grad is that developer efficiency
1331240	1337680	is much higher, but even if I have 10x higher developer efficiency, I still lose on NVIDIA,
1337680	1338680	right?
1338680	1340560	You know, okay, I didn't put 100,000 man-hours into it, right?
1340560	1344320	If they put a million, like, like, that's what I'm saying, but that's what I'm saying,
1344320	1345600	we can get.
1345840	1350440	We are going to close the speed gap a lot, like, I don't support TensorFlow yet.
1350440	1354000	That's a big one that's just going to, okay, massively close the gap.
1354000	1359320	And then AMD, I can't even get, I don't even have a benchmark for AMD because I couldn't
1359320	1360320	get it compiled.
1360320	1361320	Oh, and I tried.
1361320	1362320	Oh, I tried.
1362320	1366800	I spent a day, like, I spent actually a day trying to get PyTorch, and I got it built,
1366800	1371200	I got it kind of working, and then I tried to run a model, like, there's all kinds of
1371200	1375560	weird errors, and the rabbit hole is just so deep on this, I'm like, so we, you know,
1375760	1378680	you can compare the speed, right now, you can run Lama, you can run anything you want
1378680	1383200	on AMD, it already all works, any OpenCL back-end works, and it's not terribly slow.
1383200	1387200	I mean, it's a lot faster than crashing, so it's infinitely times faster than PyTorch
1387200	1388200	on AMD.
1388200	1393080	But pretty soon, we're going to start getting close to theoretical maximums on AMD.
1393080	1397720	That's really where I'm pushing, and I want to get AMD on ML perf in a couple months,
1397720	1398720	hopefully.
1398720	1399720	Not that you bring up AMD.
1399720	1404280	Yeah, let's dive into that, because when you announced the TamiCorp fundraise, you mentioned
1404320	1410400	one of your first goals is build the framework Rhinetime and Driver for AMD, and then on
1410400	1414560	June 3rd on Twitch, you weren't as excited about AMD anymore.
1414560	1420680	Maybe let's talk a bit about that, and you compared the quality of commit messages from
1420680	1424800	the AMD kernel to the Intel work that people are doing there, what's important to know.
1424800	1429000	So when I said I want to write a framework, I didn't never intend on writing a kernel
1429000	1430000	driver.
1430120	1436400	I flirted with that idea briefly, but realistically, there's three parts to it, right?
1436400	1440840	There's the ML framework, there's the driver, and then there's the user space runtime.
1440840	1442920	I was even down to rewrite the user space runtime.
1442920	1447280	I have a GitHub repo called CUDA IO Control Sniffer, it's terribly called, but you can
1447280	1451520	actually launch a CUDA kernel without CUDA, so you don't need CUDA installed.
1451520	1456520	Just the NVIDIA open source driver and this open source repo can launch a CUDA kernel.
1456520	1459520	So rewriting the user space runtime is doable.
1459520	1460520	Rewriting the kernel driver?
1460520	1464080	You don't even have docs, I don't have any docs for the GPU, it would just be a massive
1464080	1466600	reverse engineering project.
1466600	1471760	So that is, when I saw that there, it wasn't, I wasn't complaining about it being slow,
1471760	1475160	I wasn't complaining about PyTorch not compiling, I was complaining about the thing crashing
1475160	1478680	my entire computer, it panics my kernel, and I have to wait five minutes while it reboots
1478680	1481800	because it's a server motherboard and they take five minutes to reboot.
1481800	1485720	So I was like, look, if you guys do not care enough to get me a decent kernel driver, there's
1485720	1489280	no way I'm wasting my time on this, especially when I can use Intel GPUs.
1489280	1493600	Intel GPUs have a stable kernel driver, and they have all their hardware documented.
1493600	1498280	You can go and you can find all the registered docs on Intel GPUs, so I'm like, why don't
1498280	1499280	I just use these?
1499280	1501280	Now, there's a downside to them.
1501280	1502280	Their GPU is $350.
1502280	1506780	You're like, what a deal, it's $350, you got about $350 for the performance, and if you're
1506780	1510920	paying about 400 for the PCIe slot to put it in, right, like between the power and all
1510920	1516400	the other stuff, you're like, okay, never mind, you got to use NVIDIA or AMD from that perspective.
1516400	1519760	But I sent an email to Lisa Sue, and she responded.
1519760	1522560	Oh, you can see you published that email in a Discord.
1522560	1526040	I did, I did, and she responded.
1526040	1533720	And I've had a few calls since, and what I did was like, what I tried to do, well, first
1533720	1534720	off, thank you for responding.
1534720	1540080	It shows me that if you don't care about your kernel panicking, this is just a huge waste
1540080	1541080	of my time, right?
1541080	1542920	I'll find someone who will care.
1542960	1548560	I'm not asking for your seven by seven Winograd convolution when transposed to be fast.
1548560	1549560	Like I'm not asking for that.
1549560	1551440	I'm asking literally for the basics.
1551440	1552440	To not value.
1552440	1553520	Oh, and this isn't tiny grad.
1553520	1554520	This is your demo apps.
1554520	1557960	I ran their demo apps in loops, and I got kernel panics.
1557960	1566000	I'm like, okay, there's, but no, Lisa Sue reached out, connected with a whole bunch
1566000	1567000	of different people.
1567000	1572000	They sent me a pre-release version of RockM 5.6.
1572000	1576040	They told me you can't really say which I'm like, why do you, why do you care?
1576040	1578960	But they say they're going to release it by the end of the month and it fixed the kernel
1578960	1580080	panic.
1580080	1586400	The guy managed to reproduce it with the two GPUs and the computer and yeah, sent me a
1586400	1587920	driver and it works.
1587920	1591840	So, yeah, I had, I had that experience.
1591840	1594840	And then I had another experience where I had two calls with like AMD's like communication
1594840	1598960	people and just like, I tried to explain to these people like open source culture, like
1599160	1600520	it's not open source.
1600520	1604560	If you dump the source code on a GitHub repo and then forget about it until the next release,
1604560	1605800	it's not open source.
1605800	1611880	If, you know, all your issues are from 2022, like, like, it's just no one's going to contribute
1611880	1612880	to that project.
1612880	1613880	Right.
1613880	1614880	Sure.
1614880	1615880	It's open source in a very like technical sense.
1615880	1616880	To be fair, it's better than nothing.
1616880	1622080	It's better than nothing, but I fixed a bug in Nickel that I fixed.
1622080	1625960	There's a fun fact, by the way, if you have a consumer, a consumer AMD GPU, they don't
1625960	1628040	support peer-to-peer.
1628040	1632480	And they're already spanned with this horrendously slow because it's using CUDA kernels to do
1632480	1633880	the copy between the GPUs.
1633880	1637640	And it's putting so many transactions on the PCIe bus that it's really slow, but you
1637640	1643320	can use CUDA mem copy and there's a flag to use CUDA mem copy, but that flag had a bug.
1643320	1647000	So I posted the issue on Nickel.
1647000	1648520	I expected nothing to happen.
1648520	1650200	The Nvidia guy replied to me within an hour.
1650200	1651200	He's like, try this other flag.
1651200	1652800	I'm like, okay, I tried the other flag.
1652800	1655200	It still doesn't work, but here's a clean repro.
1655200	1659080	And I spent like three hours writing a very clean repro.
1659080	1662760	I ended up tracking the issue down myself, but just the fact that somebody responded
1662760	1666640	to me within an hour and cared about fixing the issue, okay, you've shown that it's worth
1666640	1670560	my time and I will put my time in because let's make this better.
1670560	1671840	I'm here to help.
1671840	1676000	But if you show me that you're like, you're the kernel panics, let's just expect it.
1676000	1677000	Okay.
1677000	1679040	Well, it sounds like AMD is getting the message.
1679040	1680040	They are.
1680040	1683600	And I just, I don't really think they've had someone explain to them like, I was like,
1683600	1684600	you get to like build in public.
1684600	1686320	And they're like, what's an example of building in public?
1686320	1689120	I'm like, go look at PyTorch, go look at PyTorch, right?
1689120	1692720	Like, you know, I have, I have, I have two minor things merged into PyTorch because it's
1692720	1699040	very responsive, you know, like minor bug fixes, but I feel like it's, you know, yeah.
1699040	1701320	So that's kind of like the lowest level of the stack.
1701320	1706600	And then at a slightly higher level, obviously there's tiny grad, there's module, there's
1706600	1707600	GGML.
1707600	1712160	How are you thinking about breadth versus like depth and like where you decided to focus
1712160	1713160	early on?
1713160	1716760	Um, so GGML is very much like a, okay, everyone has M1s, right?
1716760	1721360	Actually, I was thinking, in the beginning I was thinking of something more like GGML,
1721360	1725360	focus on the M1s, but GGML showed up and was just like, we're actually just focusing
1725360	1726360	on the M1s.
1726360	1732160	Um, so, and actually M1 PyTorch is considerably better than AMD PyTorch.
1732160	1736680	And when PyTorch works, it only gives wrong answers sometimes and only crashes sometimes,
1736680	1742720	but like some models kind of run, um, when I was writing the metal back end, I was comparing
1742720	1748160	to MPS PyTorch and I had like a, I had a discrepancy, like TinyGrad checks all its outputs compared
1748160	1754000	to Torch and I had one where it didn't match, I'm like, I really, I checked the matrix by
1754000	1756960	hand, it matches TinyGrad, I don't understand.
1756960	1762600	And then I switched PyTorch back to CPU and it matched and I'm like, oh yeah, well, this
1762600	1765520	is like bugs, like if you like transpose the matrix because like, I think it's like has
1765520	1769160	to do with like multi views and PyTorch and like weird under the hood stuff that's not
1769160	1772320	exposed to you, like there's bugs and maybe they fix them, but like, you know, it seems
1772320	1777000	like there was a lot of momentum again, because you're getting a huge variety, you're getting
1777000	1781480	how many engineers care about making PyTorch work on M1, right, thousands, tens of thousands.
1781480	1782480	Yeah.
1782480	1785120	And you have an open development process and guess what, it's going to be good.
1785120	1788240	How many engineers care about AMD working with PyTorch AMD working?
1788240	1794000	Well, you got 10 guys that work for AMD and then like a couple hobbyists.
1794000	1798280	You revealed an interesting detail about how you debug, which is you check, you hand check
1798280	1799280	the matrix math.
1799640	1800640	No, I don't hand check it.
1800640	1806640	There's a, there's a, one of the best tests in tiny grad is a file called test ops.py
1806640	1812680	and it's just a hundred small examples written in tiny grad and PyTorch and it checks both
1812680	1814920	the forwards and backwards to make sure they match.
1814920	1815920	The test suite.
1815920	1816920	Yeah.
1816920	1817920	Very important.
1817920	1820080	That's, I mean, that's one of them where you like, I really, I put a lot of effort into
1820080	1821080	the CI for tiny grad.
1821080	1822560	I think CI is super important.
1822560	1825320	Like I want that green check to mean I can merge this.
1825320	1826320	Yeah.
1826320	1827320	Okay.
1827320	1828320	I don't want my tests too.
1828360	1830160	I don't want to manage to introduce a bug and get the green check.
1830160	1831160	Okay.
1831160	1832160	We're fixing the test top priority.
1832160	1833160	Mojo.
1833160	1834160	It's close source.
1834160	1835160	No, I'm not that interested.
1835160	1836160	Do you know what I mean?
1836160	1837160	Like, like, look, I like Chris Latner.
1837160	1838160	I think he's going to do great things.
1838160	1842160	And I understand the, the like kind of the wisdom, even in keeping a close source, but
1842160	1844160	you know, I'm interested when it's open.
1844160	1845160	Yeah.
1845160	1846160	Right.
1846160	1851160	You have an interesting design deviation from him because he's decided to be a promise
1851160	1855160	to be a superset of Python and you have decided to break with it.
1856000	1861000	And I think that's, that affects learnability and trans, transportability of code.
1861000	1869000	You know, if the PyTorch thing ends up being like a, like a stumbling block, I could write
1869000	1875000	a perfect PyTorch, like, like, like, like a, you know, instead of import PyTorch, instead
1875000	1879000	of like, yeah, import Torch, you type import tiny Torch as Torch.
1879000	1883000	And if that really becomes the stumbling block, I think it's going to be great.
1883000	1887000	If that really becomes the stumbling block, I will do that.
1887000	1888000	No.
1888000	1890240	Chris Latner went much further than PyTorch.
1890240	1894120	Replicating the PyTorch API is something I can do with a couple, you know, like an engineer
1894120	1895120	month or two.
1895120	1896120	Right.
1896120	1897120	Like a shim.
1897120	1898120	Yeah.
1898120	1899120	Replicating Python.
1899120	1901640	There's a, there's a, there's a big graveyard of those projects.
1901640	1904440	How's, how's, how's Piston going?
1904440	1911940	How's, oh, Jython, PyPy is all, you can go way back.
1911940	1917340	So tiny grad and small layer, you announced TinyBox recently, which is, you know, you
1917340	1918340	made it.
1918340	1921660	So your core mission is commoditizing the pedoflop.
1921660	1925860	And then your business goal is to sell computers for more than the cost to make, which seems
1925860	1928060	super reasonable.
1928060	1932220	What are, and you're going to have three TinyBoxes, red, green, blue.
1932220	1936860	That was my, look, you know, a lot of people, like, I love, you know, leaning into like
1936860	1938060	saying I'm giving up, right?
1938060	1940500	It's great to give up or giving up is this wonderful thing.
1940500	1941820	It's so liberating.
1941820	1944180	And then like, you can decide afterward if you really give up or not.
1944180	1947020	There's very little harm in saying you give up, except like, you know, great, Twitter
1947020	1950780	haters have something to talk about and all press is good press kids.
1950780	1959060	So, obviously, just read, only read, TinyBox, read, unless AMD, you know, upsets me again
1959060	1964100	and then we're back to, we're back to other colors, we have other colors to choose from.
1964100	1967820	When you think about hardware design, what are some of the numbers you look for?
1967820	1972980	So, Terraprop sits per second, it's one, but like memory bandwidth is another big limiter.
1972980	1974900	Like, how do you make those trade-offs?
1974900	1978340	Well, I mean, fundamentally unlimited what GPUs I can buy.
1978340	1981660	But yeah, for something that I think a lot of people are going to want to reasonably
1981660	1989060	do with a core core of mine, describe them as luxury AI computers, right?
1989060	1991180	Like luxury AI computers for people.
1991180	1992180	And that's like what we're building.
1992180	1996060	And I think a common thing people are going to want to do is run like large llama, right?
1996060	1997060	Or large, like Falcon.
1997300	1998300	FB16 llama.
1998300	1999300	FB16, exactly.
1999300	2000300	Exactly.
2000300	2002220	You know, in-date I think can work.
2002220	2006660	I think that like what GGML is doing to go to like N4, like this doesn't work.
2006660	2011220	Like have you done, maybe they have, but like I read what it was and I was like, this isn't
2011220	2012380	from any paper.
2012380	2014140	This is just some, like you're-
2014140	2015140	Squeezing as much as possible.
2015140	2018900	Yeah, you made up some quantization standard to make it run fast and like, like maybe it
2018900	2021940	works, but okay, where's like the hell's swag number, right?
2021940	2025140	Where's your, where's your, where's your, uh, you know, all your-
2025140	2028820	The thesis is right that like if you have billions, hundreds of billions of parameters
2028820	2031900	that the individual quantization doesn't actually matter that much.
2031900	2035340	Well, the real way to look at all of that is to just say you want to compress the weights,
2035340	2036340	right?
2036340	2037340	It's a form of weight compression.
2037340	2039140	Quantization is a form of weight compression right now.
2039140	2040140	This is obviously not lossless.
2040140	2041140	It's not a lossless compressor, right?
2041140	2044380	It's a lossless compressor and you can show that it's correct and okay, we don't have
2044380	2047460	to have any other conversation, but it's a lossy compressor.
2047460	2051740	And how do you know that your loss isn't actually losing the power of the model?
2051740	2057660	Maybe int465bLama is actually the same as fb167bLama, right?
2057660	2058660	We don't know.
2058660	2061500	Uh, maybe someone has done this yet, but I looked for it when it like first came out
2061500	2065980	and people were talking about it and I'm like, I just have, like it's not from a paper, right?
2065980	2069780	The in-date stuff is from a paper where they, like some of the in-date stuff is from a paper.
2069780	2076020	There's one paper, I think it's like in-llm.indate where they actually, uh, you know, do all the
2076020	2078900	tests and they didn't go fully in-date.
2078900	2083300	They made like 90% of it in-date and kept like 10% of it in fb16 for what they called
2083300	2086220	like the like outliers or whatever.
2086220	2088020	So I think that this is not quite so easy.
2088020	2091140	And I think being able, well, so first off, if you're training, no one's gotten training
2091140	2092140	to work with in-date yet.
2092140	2093700	There's a few papers that vaguely show up.
2093700	2097260	If you're training, you're going to need, uh, bf16 or float16.
2097260	2100660	Um, so this is why I target that.
2100660	2104060	Now the thing that you're going to want to do is run these large language models out
2104060	2107580	of the box on your hardware in fb16 and that's memory bandwidth.
2108580	2113020	You, you need, you need large amounts of memory bandwidth to, uh, so ask how I trade
2113020	2115540	off memory bandwidth in Flops, so what GPUs can I buy?
2115540	2121660	But, um, and I saw one of your, so first of all, you have this, um, hiring process, which
2121660	2125140	has, you got to solve one of the bounties, um, that are open on tiny grad.
2125140	2127340	There's no, uh, technical interview.
2127340	2129020	One of them is in-date support.
2129020	2132540	Do you already have some things you want to test on?
2132540	2133540	We have in-date support.
2133540	2139620	Um, what I'd like to see somebody do is just load the ggml intate llama into tiny grad
2139620	2142060	and then benchmark it against the fb16 one.
2142060	2146460	Uh, intate already works in, in tiny grad, it doesn't actually do the math in intate,
2146460	2151580	which is even a, which is even a stronger, like it does all the math still in fb32.
2151580	2155180	So intate can mean you just have your weights in intate or intate can mean you actually
2155180	2156180	do your math in intate.
2156180	2161180	And doing your math in intate, the big, like, gain that people care about is actually, uh,
2161940	2167420	having your weights in intate, because weights in intate mean less memory and less memory bandwidth.
2167420	2172340	Uh, whereas the math, keep it in fb32 with, with, with, on, on m ones, it doesn't even
2172340	2175700	matter if you're doing, it doesn't matter what data type you're doing in the, in the
2175700	2176700	, in the GPO.
2176700	2180740	I, I'm not even sure it can do intate, but fb16 and fb32 is the same, is the same taro
2180740	2181740	flops.
2181740	2185620	Um, so yeah, no, that's one of the bounties.
2185620	2190020	One of the bounties is get, get intate llama running with the intate weights.
2190020	2194540	And then actually what you could even do, if you really want to test this, just take
2194540	2198500	the fb16 weights, convert them to intate, then convert them back to fb16, then compare
2198500	2200460	the unconverted and converted.
2200460	2201780	Oh, that's a nice hack.
2201780	2202780	Oh yeah.
2202780	2203780	Right.
2203780	2205540	Like, like, like, this should be lossless in the other direction.
2205540	2206540	Well, yeah.
2206540	2211460	So, uh, yeah, I think fb16, it should be lossless in the other direction.
2211460	2213140	I'm actually not a hundred percent about that.
2213140	2214140	Why not?
2214140	2217580	Uh, oh, cause like, you ever try to like, like, if you want to represent, if it was like
2217580	2219980	int16, it's not lossless.
2219980	2220980	Sure.
2220980	2224020	I think, I think all of intate can be represented in fb16, but I'm not a hundred percent about
2224020	2225020	that.
2225020	2230340	Actually, I think it, we just draw the bytes and we just have to do it, right?
2230340	2231340	Just literally do it.
2231340	2237020	There's only 256 to check, like, um, but yeah, either way, or, I mean, into four, definitely.
2237020	2238940	So do your in four, convert it back.
2238940	2244140	And now see, even with in four weights and fb32 math, like, okay, how much does your
2244140	2245900	performance to grade of this model?
2245900	2246900	Yeah.
2246940	2251580	So, so can we, uh, I'm about to zoom out a little bit from the details.
2251580	2255980	I don't know if you, you had more, no, I think like the, you're playing to release
2255980	2260460	the first tiny box ship them in like two to six, eight months, something like that.
2260460	2265220	Uh, what's up with mine for you in terms of building a team who should, who are you calling
2265220	2266220	for?
2266220	2267220	Yeah.
2267220	2270020	Uh, well, to, to stay on the tiny box for, for, for, for, yeah, exactly.
2270020	2274860	Um, so at the GPUs picked out and you're like, well, I could make that computer with
2274860	2275860	the GPUs.
2275900	2280540	My answer is, can you, do you know how to put, do you know how hard it is to put six
2280540	2282660	GPUs on a computer?
2282660	2285780	People think it's really easy and it's really easy to put one GPU in a computer.
2285780	2289980	It's really easy to put two GPUs in a computer, but now you want to put in eight.
2289980	2290980	Okay.
2290980	2291980	So I'll tell you a few things about these GPUs.
2291980	2293480	They take up four slots.
2293480	2295580	What kind of computer?
2295580	2296900	You can buy the nicest super micro.
2296900	2298780	You can't put eight of those in there.
2298780	2299780	You need two slot blowers.
2299780	2303060	If you want to use one of those, those for your super micros, you need two slot blowers
2303060	2304060	or water cooling.
2304260	2306340	All right, if, if you're trying to get the four slot cards in there, you're going to
2306340	2310980	need some form of water cooling, uh, or you're going to need, there are some like Chinese
2310980	2312180	40 nineties that are blowers, right?
2312180	2314380	You have any blowers or water cooling if you're trying to get it in those things.
2314380	2315380	Right.
2315380	2317220	Um, so you, are you doing water?
2317220	2319300	No, I'm not using that chassis.
2319300	2320300	Okay.
2320300	2325700	Um, then the other thing that, okay, so now you want to get six GPUs in a computer.
2325700	2326700	So that's a big challenge.
2326700	2328540	You're like, oh, I'll just use a PCIe extenders.
2328540	2329540	I saw it online as tech tips.
2329540	2330540	It works great.
2330540	2331540	No, it doesn't.
2331540	2335180	It's PCIe extenders that work at PCIe 4.0 and interconnect bandwidth.
2335180	2336180	Super important.
2336180	2337180	Yes.
2337180	2338580	They don't work at 3.0.
2338580	2344420	No PCIe extender I've tested and I've bought 20 of them, uh, works at PCIe 4.0.
2344420	2347020	So you're going to need PCIe redrivers now.
2347020	2348020	Okay.
2348020	2349380	How much does that add in cost?
2349380	2350380	Right.
2350380	2351380	Like these things all get really hard.
2351380	2352380	And then tiny boxes.
2352380	2354140	I've even had another constraint to it.
2354140	2359420	I want this thing to be silent, not totally silent, but my limit is like 45, maybe 50
2359420	2362660	dB, but not super micro machine.
2362660	2363660	60 dB.
2363660	2368260	We have a small, we have a compute cluster, a comma, you got to wear your protection
2368260	2369260	to go in there.
2369260	2370260	I like it.
2370260	2371260	Yeah.
2371260	2372260	I've seen some videos where you give a tour.
2372260	2373260	Yeah.
2373260	2374260	Yeah.
2374260	2375260	It's noisy.
2375260	2376260	It's super loud.
2376260	2377260	Yeah.
2377260	2378260	10,000 RPM.
2378260	2379260	Just screaming.
2379260	2384540	Like I want to be able to use the normal big GPU fans and make this thing so it can
2384540	2388940	sit under your desk, plug into one outlet of power, right?
2388940	2394940	This GPUs, your GPUs are 350 Watts each, can't plug that into a wall outlet.
2394940	2395940	Okay.
2395940	2396940	So how are you going to deal with that?
2396940	2399540	Good questions, right?
2399540	2400540	And you're not sharing them.
2400540	2402620	Well, that one, I mean, that one is pretty obvious.
2402620	2404220	You have to limit the power on the GPUs, right?
2404220	2405980	You have to limit the power on the GPUs.
2405980	2410020	Now you can limit power on GPUs and still get, you can use like half the power and get
2410020	2412020	80% of the performance.
2412020	2415300	This is a known fact about GPUs, but like that's one of my design constraints.
2415300	2419500	So when you start to add all these design constraints, good luck building a tiny box
2419500	2420500	yourself.
2420500	2424980	You know, obviously it can be done, but you need something that has actually quite a bit
2424980	2427580	of scaling resources to do it.
2427580	2431620	And you see like the under the desk, it's like one of the main use cases, kind of like
2431620	2433620	individual developer use or.
2433620	2434620	Yeah.
2434620	2437900	What I also see is more of a like an AI hub for your home, right?
2437900	2442180	As we start to get like home robotics kind of stuff, you don't want to put the inference
2442180	2443180	on the robot.
2443740	2446620	But you also don't want to put the inference on the cloud.
2446620	2451860	You don't want to put it on the robot because, okay, it's 1500 Watts, tiny box, you put batters
2451860	2454380	and charge them.
2454380	2455380	Bad idea.
2455380	2457900	And just, just, just wireless, wireless is 0.5 milliseconds.
2457900	2458900	Yeah.
2458900	2459900	This is super fast.
2459900	2461740	You don't want to go to the cloud for two reasons.
2461740	2463260	One, cloud's far away.
2463260	2464260	Okay.
2464260	2465260	It's not that far away.
2465260	2471500	You can kind of address this, but two, cloud's also mad expensive, like cloud GPUs are way
2471500	2474260	more expensive than running that GPU at your house.
2474260	2476580	At least any rates you're going to get, right?
2476580	2479580	Maybe if you commit to buy, well, yeah, I'm going to buy 10,000 GPUs for three years,
2479580	2481380	then maybe the cloud will give you a good rate.
2481380	2483580	But like, you want to buy, you want to buy one GPU in the cloud?
2483580	2484580	Ooh.
2484580	2487700	I mean, okay, you can go to like Vast, but like if you're going to Azure, AWS, so that's
2487700	2488700	expensive.
2488700	2489700	Yeah.
2489700	2493180	This is like a, like a personal data center, you know, instead of a cloud data center.
2493180	2496180	We like the term compute cluster, so we can use NVIDIA GPUs.
2496180	2497180	Yeah.
2497180	2498580	Data centers may be a little bit dated.
2498860	2503540	It's a compute cluster, which is totally legal under the CUDA license agreement.
2503540	2505660	You talk a lot about the PCIe connection.
2505660	2508540	Do you think there's any fat there to the trim?
2508540	2509540	What do you mean?
2509540	2511700	Just you're limited by bandwidth, right?
2511700	2512700	Okay.
2512700	2513700	For some things, yes.
2513700	2520740	So the bandwidth is roughly 10x less than what you can get with NV linked A 100s.
2520740	2521740	Yeah.
2521740	2524500	NV linked A 100s are going to have, and then you can even get like full fabric and the
2524500	2528540	NVIDIA really pushes on that stuff, 600 gigabytes per second, right?
2528540	2530620	And PCIe four, you're going to get 60.
2530620	2531620	All right.
2531620	2532620	So you're getting 10x less.
2532620	2533620	Yeah.
2533620	2537420	Um, that said, why do you need the bandwidth, right?
2537420	2541540	And the answer is you need it for training huge models.
2541540	2545460	If you're training on a tiny box, your limit's going to be about 7 billion, right?
2545460	2549140	If you're, if you're training on big stuff, your limits could be like 70 billion, right?
2549140	2550140	Okay.
2550140	2551140	You can hack it to get a bit higher.
2551140	2554020	You can hack it like GBT hacked it to get a bit higher, but like that's 65 billion in
2554020	2555020	llama.
2555020	2556700	Like there's a reason they chose 65 billion, right?
2556700	2560380	And that's what can reasonably fit model parallel on, on, on a GPUs.
2560380	2561380	Right.
2561380	2565180	So, um, yes, you, you are going to end up training models.
2565180	2568180	The cap's going to be like 7 billion, but I actually heard this on your podcast.
2568180	2571740	I don't think that the best chatbot models are going to be the big ones.
2571740	2574740	I think the best chatbot models are going to be the ones where you had a thousand training
2574740	2576780	runs instead of one.
2576780	2580740	And I don't think that the interconnect bandwidth is going to matter that much.
2580740	2582940	So what are we optimizing for instead of compute optimal?
2582940	2585660	Uh, what do you mean compute optimal?
2585660	2590340	So the, this, you're talking about this, um, the llama style models where you train
2590340	2591340	for like 200,
2591340	2592340	You train longer.
2592340	2593340	Yeah.
2593340	2594340	Yeah.
2594340	2595340	Yeah.
2595340	2596340	So, okay.
2596340	2597340	You can always make your model better by doing one of two things.
2597340	2598340	Right.
2598340	2599340	And a comma, we just have a strict limit on it.
2599340	2601340	Um, you can always make your model better by training longer and you can always make
2601340	2603580	your model better by making it bigger.
2603580	2606180	But these aren't the interesting ones, right?
2606180	2608980	Particularly the making it bigger because training it longer, fine, you know, you're
2608980	2609980	getting a better set of weights.
2609980	2610980	The inference is the same.
2610980	2615620	The inference is the same, whether I trained it for a day or a week, but the, okay.
2615620	2618740	If it's one billion versus 10 billion, well, I 10x my inference too.
2618740	2619740	All right.
2619740	2623140	So I think that these big models are kind of a, sure they're great if you're research
2623140	2627140	labs and you're trying to like max out this thing, which you can talk about later.
2627140	2628140	Yeah.
2628140	2629140	Yeah.
2629140	2630140	Yeah.
2630140	2632020	But if you're, but if you're like a startup or you're like an individual or you're trying
2632020	2636060	to deploy this to the edge anywhere, you don't, you don't need that many weights.
2636060	2637060	Yeah.
2637060	2638060	Yeah.
2638060	2639060	You don't want them anyway.
2639060	2640060	Optimizing for inference rather than capabilities.
2640060	2641060	Yes.
2641060	2642060	Doing benchmarks.
2642060	2643060	Yes.
2643060	2644060	Yes.
2644060	2645060	Um, and I think the, the inference thing, right?
2645060	2648460	There should be so much more, right now the ratio between like training and inference
2648460	2651940	on clouds, I think it's only still like, it's like two or three acts, right?
2651940	2653740	It's two or three acts more inference, which doesn't make any sense.
2653740	2655140	Like there should be way more inference.
2655140	2656140	Yeah.
2656140	2659220	There should be a 10 to a hundred X more inference in the world than, than training.
2659220	2662500	Um, but then also like what is training, right?
2662500	2666380	You start to see these things like Laura, like, you're getting kind of, it's kind of
2666380	2668940	blurring the lines between inference and training.
2668940	2670940	And I think that that blurred line is actually really good.
2670940	2674860	I'd like to see much more like on device training or on device fine tuning of the final
2674860	2675860	layer.
2675860	2676860	Yeah.
2676860	2677860	Um, we're, we're pushing toward this stuff at comma.
2677860	2678860	Right.
2678860	2679860	Like why am I shipping a fixed model?
2679860	2683980	I totally want this model to fine tune based on like how, you know, your left tire is flat.
2683980	2684980	Right.
2684980	2688980	Like every time you cut the same turn because your left tire is flat.
2688980	2689980	Well, it should learn that.
2689980	2690980	Right.
2690980	2693540	So would comma pursue perimeter efficient fine tuning?
2693540	2694540	Yeah.
2694540	2695540	Yeah.
2695540	2697540	Where, where, where, where seems like a, we're looking at the stuff like that.
2697540	2700660	I mean, comma is already very parameter efficient because we have to like run this thing in
2700660	2702740	a car and you have to like cool it and power it.
2702740	2703740	Yeah.
2703740	2704740	Yeah.
2704740	2710020	And so this kind of like intelligence cluster you have in your home, you see when the person
2710020	2714900	is using third party model, they load them locally and kind of do the final fine tuning.
2714900	2716500	It kind of stays within the box.
2716500	2717500	Yeah.
2717500	2718940	I think that that's one thing.
2718940	2721340	That's one version of it for the privacy conscious.
2721340	2728340	Um, I also see a world where, uh, you can have your tiny box in its down cycles, um,
2728340	2729340	mine flop coin.
2729340	2730340	Right.
2730340	2732260	You know, not all, turns out not all crypto is a scam.
2732260	2733780	There's one way to tell if crypto is a scam.
2733780	2736740	If they're selling the coin before they make the product, it's a scam.
2736740	2739940	If they have the product and then they sell the coin, it's maybe not a scam.
2739940	2740940	Right.
2740940	2743620	So yeah, my thought is like each tiny box would let you, would have a private key on
2743620	2744620	it.
2744620	2745620	Uh, and you have to do it this way.
2745620	2747340	You can't just let anyone join because of civil attacks.
2747340	2748340	Right.
2748340	2751020	There's a real problem of like, how do I, uh, how do I ensure your data is correct?
2751020	2754340	And the way that I ensure your data is correct on the tiny net is if you ever send wrong
2754340	2756780	data, you're banned from the life.
2756780	2757780	Yeah.
2757780	2759620	You're, you're a $15,000 hardware box is banned.
2759620	2760620	So you know, don't cheat.
2760620	2763500	Um, obviously if it messes up, we'll forgive you.
2763540	2765940	But, um, I'm saying like some is going to try to jailbreak your devices.
2765940	2767940	There's no jailbreak.
2767940	2768940	There's no jailbreak.
2768940	2769940	There's just a different network.
2769940	2771140	Well, there's just a private key on each device.
2771140	2772140	Right.
2772140	2774100	Like if you buy a tiny box from the tiny corp, I give you a private key.
2774100	2775100	It's in my backend server.
2775100	2776100	Right.
2776100	2777100	You want to hack my server.
2777100	2778100	That's illegal.
2778100	2779100	Yeah.
2779100	2780100	Anything you want to do on the device, the device is yours.
2780100	2781100	My server's mine.
2781100	2782100	Right.
2782100	2783100	Like.
2783100	2784100	Yeah.
2784100	2785100	Yeah.
2785100	2786100	Uh, have you looked into like, uh, federated training at all?
2786100	2787100	Yeah.
2787100	2788100	So I mean, okay.
2788100	2789100	You're now, there's, okay.
2789100	2790100	There's a lot of magnitude of federated training.
2790100	2794100	I mean, like, uh, over the cloud and stuff, over the internet, over the internet, but
2794100	2796100	also distributed on a bunch of devices.
2796100	2797100	Right.
2797100	2798100	Yeah.
2798100	2801100	I'm, I'm, I'm very bearish on this stuff because you're an interconnect bandwidth.
2801100	2802100	Right.
2802100	2803100	So, okay.
2803100	2805700	At the high end, you have your interconnect bandwidth of envy link, which is 600 gigabytes
2805700	2806700	per second.
2806700	2807700	Right.
2807700	2813540	The tiny box has 60 gigabytes per second and then your internet has 125 megabytes per
2813540	2814540	second.
2814540	2815540	Right.
2815540	2816540	Not gigabits.
2816540	2817540	125 megabytes.
2817540	2818540	Right.
2818540	2819540	So, okay.
2819980	2822540	That's, that's how, that's how many orders of magnitude we're talking here.
2822540	2825500	Like from 60 down to 125, like, all right.
2825500	2826500	That's over a hundred.
2826500	2827500	There's over a hundred X.
2827500	2828500	That's 400 X.
2828500	2829500	Right.
2829500	2830500	So like, no.
2830500	2831500	Uh, but what you can do is inference.
2831500	2832500	Right.
2832500	2833500	Like there's, for inference, you don't care.
2833500	2834500	Right.
2834500	2838340	For inference, I, I, there's so little bandwidth at the top and the bottom of the model, um,
2838340	2840340	that like, yeah, you can do federated inference.
2840340	2841340	Right.
2841340	2842340	And that's kind of what I'm talking about.
2842340	2846580	Um, there's also interesting things to push into like, you're like, but okay, what if
2846580	2848700	you want to run close source models?
2848700	2852740	This stuff gets kind of interesting, like using TPMs on the boxes and stuff.
2852740	2853740	Um, yeah.
2853740	2855540	But then someone might jailbreak my device.
2855540	2857220	So, you know, maybe we don't try to do that.
2857220	2858220	Yeah.
2858220	2859500	What's like the enterprise use case?
2859500	2862860	Do you see companies buying a bunch of these and like stacking them together?
2862860	2867820	Um, so the tiny box is like the first version of what we're building, but what I really
2867820	2872780	want to do is be on the absolute edge of flops per dollar and flops per lot.
2872780	2874140	These are the two numbers that matter.
2874140	2878020	Uh, so the enterprise use case is you want to train like, like comma, right?
2878020	2879940	So comma just built out a new compute cluster.
2879940	2882540	It's about, uh, it's about a person and a half.
2882540	2887420	Uh, so, you know, it's decent size person, a person being 20 person is a person is 20
2887420	2888420	paid flops.
2888420	2889420	It's about 30 paid flops.
2889420	2894660	Um, we built out a little, uh, little compute cluster and you know, we paid double what
2894660	2897100	you theoretically could per flop, right?
2897100	2901340	You theoretically could pay half per flop if you designed a bunch of custom stuff.
2901340	2904980	And yeah, I mean, I could see that being, you know, tiny core when comma is going to
2904980	2905980	be the first customer.
2905980	2909460	I'm going to build a box for comma and then I'm going to show off the box I built for
2909460	2914060	comma and be like, okay, like, do you want to build, I sell $250,000 training computers
2914060	2915740	or how much does one H 100 box?
2915740	2917660	Uh, it's, uh, it's four under grand.
2917660	2918660	Okay.
2918660	2922980	I'll build you a 400 grand training computer and it'll be 10x better than that H 100 box
2922980	2926980	for again, not for every use case for some, you need the interconnect bandwidth, but for
2926980	2932620	90% of most companies model training use cases, the tiny box will be five X faster for the
2932620	2934620	same price.
2934620	2937420	You mentioned the person of compute.
2937420	2939420	How do we build a human for $20 million?
2939420	2941540	Oh, it's a lot cheaper now.
2941540	2942540	It's a lot cheaper now.
2942540	2946860	Uh, so like I said, we comma, comma spent about, uh, about half a million on our, on
2946860	2947860	our person and a half.
2947860	2954140	So, you know, what are some of the numbers people should think of when they compare compute
2954140	2955140	to like people.
2955140	2958620	So GBD four was a hundred person years of training.
2958620	2962620	That's more like on, on the time scale, um, 20 petaflops is one person.
2962620	2967340	I think you, um, right now the math was that for the price of the most expensive thing
2967340	2971860	we build, which is the international space station, we could build, uh, one Tampa of
2971860	2972860	one Tampa.
2972860	2973860	Yeah, yeah.
2973860	2974860	One Tampa of compute.
2974860	2975860	Yeah.
2975860	2976860	Which is 400,000 people.
2976860	2977860	Of measurement.
2977860	2978860	Um, yeah.
2978860	2979860	Yeah.
2979860	2980860	We could build.
2980860	2982860	So like the biggest training clusters today, I know less about how GBD four was trained.
2982860	2987620	I know some rough numbers on the weights and stuff, but, uh, llama trillion parameters.
2987620	2988620	Well, okay.
2988620	2993580	So GBD four is 220 billion in each head and then it's an eight way mixture model.
2993580	2995900	So mixture models are what you do when you're out of ideas.
2995900	2998220	Um, so, you know, it's a, it's a mixture model.
2998220	3001020	Uh, they just train the same model eight times and they have some little trick.
3001020	3005500	They actually do 16 inferences, but, uh, no, it's not like, so the multi modality is just
3005500	3008620	a vision model kind of glommed on.
3008620	3011020	I mean the multi modality is like obvious what it is too.
3011020	3013700	You just put the vision model in the same token space as your language model.
3013700	3015300	Oh, did people think it was something else?
3015300	3018220	No, the mixture has nothing to do with the vision or language aspect of it.
3018220	3022020	It just has to do with, well, okay, we can't really make models bigger than 220 billion
3022020	3023020	parameters.
3023020	3024700	Uh, we want it to be better.
3024700	3026020	Well, how can we make it better?
3026020	3031820	Well, we can train it longer and okay, we've actually already maxed that out, uh, getting
3031820	3032820	diminishing returns there.
3032820	3033820	Okay.
3033820	3034820	Make sure of experts.
3034820	3035820	Yeah, make sure of experts.
3035820	3036820	We'll train eight of them.
3036820	3037820	Right.
3037820	3041740	So, you know, you know, the real truth is whenever a start, whenever a company is secretive,
3041740	3045380	with the exception of Apple, Apple's the only exception, whenever a company is secretive,
3045380	3048100	it's because they're hiding something that's not that cool.
3048460	3051260	People have this wrong idea over and over again that they think they're hiding it because
3051260	3052260	it's really cool.
3052260	3053260	It must be amazing.
3053260	3054260	It's a trillion parameters.
3054260	3057380	No, it's a little bigger than GPT-3 and they did an eight-way mixture of experts.
3057380	3063980	Like, all right, dude, anyone can spend eight times the money and get that, um, but yeah,
3063980	3068860	so, uh, coming back to what I think is actually going to happen is, yeah, people are going
3068860	3074020	to train smaller models for longer and fine tune them and find all these tricks.
3074020	3075020	Right.
3075180	3078700	You know, I think, uh, opening, I used to publish stuff on this, you know, uh, when
3078700	3085140	they would publish stuff, uh, about how much better the training has gotten given the same
3085140	3090660	of holding compute constant and it's gotten a lot better, right, than compare, like, batch
3090660	3091660	norm to no batch norm.
3092260	3093260	Yeah.
3093260	3094260	And now we have like-
3094260	3095740	Is there a finding algorithms like flash attention?
3095740	3097740	Yeah, well, flash attention, yeah.
3097740	3098740	Yeah.
3098740	3101140	Um, my flash attention is the same compute.
3101140	3103540	A flash attention is an interesting fact where it's actually the identical compute.
3103580	3105100	It's just a more efficient way to do the compute.
3105100	3111140	But I'm even talking about, like, like, um, look at the new, look at the new, uh, embeddings
3111140	3112140	people are using.
3112140	3113140	Right.
3113140	3114140	They used to use these like boring old embeddings.
3114140	3116180	Now like Lama uses that complex one and that was like alibi.
3116180	3120740	I'm not up to date on all the latest stuff, but, uh, those tricks give you so much.
3120740	3122900	There's been a whole round trip with positional embeddings.
3122900	3124860	I don't know if you've, uh, seen this discussion.
3124860	3125860	I haven't followed-
3125860	3129060	Like you need them, you need rotational and then you don't need them.
3129060	3130660	I haven't followed exactly.
3130660	3134220	I mean, you quickly run into the obvious problem with positional embeddings, which
3134220	3137460	is you have to invalidate your KV cache if you run off the context.
3137460	3141780	So that's why I think these new ones that play with them, but, uh, I'm not that, I'm
3141780	3145820	not that, I'm not an expert on like the latest up-to-date language model stuff.
3145820	3146820	Yeah.
3146820	3153940	Um, I mean, we have what we do at comma, I don't know how that works, but like, um, what
3153940	3157220	are some of the things, I mean, that people are getting wrong.
3157220	3161500	So back to autonomous driving, there was like the whole like LiDAR versus vision thing.
3161500	3164460	You know, it's like, people don't get into accidents because they cannot see well, they
3164460	3167740	get into accidents because they got distracted and all these things.
3167740	3171020	What are, do you see similarities today on like the pathway GI?
3171020	3173460	Like are there people, like what are like the-
3173460	3177260	Nothing, nothing I say about this is ever going to compete with how Rich Sutton stated
3177260	3178260	it.
3178260	3179260	Rich Sutton is writer-
3179260	3180260	The bitter lesson.
3180260	3181260	The first millennium, the bitter lesson.
3181260	3182260	Nothing I say is ever going to compete with.
3182260	3185060	The bitter lesson is way better than any way I'm going to phrase this.
3185060	3188780	Just go read that and then like, I'm sorry, it's bitter, but you actually just have to
3188780	3189780	believe it.
3189780	3192420	Like over and over again, people make this mistake.
3192420	3194980	They're like, oh, we're going to hand it to you or this thing, we're going to hand-
3194980	3197180	No, like stop wasting time.
3197180	3200020	Which is, I mean, OpenAI is not taking the bitter lesson.
3200020	3201260	No, OpenAI-
3201260	3207540	They were, they were leaders in deep learning for a long, long, long time, but you're telling
3207540	3209140	me that GPT-4 is not.
3209140	3214060	Well, OpenAI was the absolute leader to the thesis that computers all you need, right?
3214060	3216940	And there's a question of how long this thesis is going to continue for.
3216940	3221140	It's a cool thesis and look, I think I would be lying along with everybody else.
3221140	3224900	I was into language models like way back in the day for the HotterPrice.
3224900	3227100	I got into AI through the HotterPrice.
3227100	3231140	Like 2014, I'm trying to build compressive models of Wikipedia and I'm like, okay, why
3231140	3232140	is this so hard?
3232140	3233900	Like what this is is a language model, right?
3233900	3238380	And I'm playing with these like, like Bayesian things and I'm just like, oh, but like I get
3238380	3239380	it.
3239380	3242220	Like it needs to be like, like it's like, I have two data points and they're like almost
3242220	3243220	the same.
3243220	3245260	And so I measure that almost, right?
3245260	3248780	I just like, you know, wrap my head around, I couldn't like, like wrap my head around
3248780	3249780	this.
3249780	3253380	And this was around the time Carpathia released the first like RNN that generated the Shakespeare
3253380	3254380	stuff.
3254380	3256820	And I'm like, okay, I get it, right?
3256820	3258780	It's neural networks that are compressors.
3258780	3261580	Now this isn't actually, you can't actually win the HotterPrice with these things because
3261580	3263580	HotterPrice is MDL.
3263580	3268100	It's the model, size of the model plus the size of the encodings, embeddings.
3268100	3273100	So yeah, you can't, I mean, probably now you can because it's gotten so good.
3273100	3275180	But yeah, back in the day, you kind of couldn't.
3275180	3276180	So I was like, okay, cool.
3276180	3277180	Like this is what it is.
3277180	3278180	I kind of get it.
3278180	3279180	Yeah.
3279180	3283540	I mean, I think I didn't expect that it would continue to work this well.
3283540	3286660	I thought there'd be real limits to how good autocomplete could get.
3286660	3287660	That's fancy autocomplete.
3287660	3291820	But yeah, no, like it works.
3291820	3292820	It works well.
3292820	3296820	So like, yeah, what is open AI getting wrong?
3296820	3297820	Technically not that much.
3297820	3298820	I don't know.
3298820	3302580	Like if I was a researcher, why would I go work there?
3302700	3303700	Yes.
3303700	3306300	So why is opening AI like the Miami Heat?
3306300	3308900	No, look, I don't know.
3308900	3309900	This is my technical stuff.
3309900	3312740	I don't really want to harp on this, but like, why go work at opening AI when you can go
3312740	3313740	work at Facebook?
3313740	3314740	Right.
3314740	3315740	As a researcher.
3315740	3320020	Like opening AI can keep ideologs who, you know, believe ideological stuff and Facebook
3320020	3323900	can keep every researcher who's like, dude, I just want to build AI and publish it.
3323900	3324900	Yeah.
3324900	3325900	Yeah.
3325900	3326900	Awesome.
3326900	3327900	Yeah.
3327900	3328900	Any other thoughts?
3328900	3329900	Corp, bounties?
3329900	3338620	Um, yeah, so we have, you know, I've been thinking a lot about like what it means to
3338620	3340740	hire in today's world.
3340740	3342740	What actually is the like core?
3342740	3343740	Okay.
3343740	3349900	Look, I'm a believer that machines are going to replace everything in about 20 years.
3349900	3351460	So okay.
3351460	3353580	What is that?
3353580	3358660	What is that thing that people can still do that computers can't, right?
3359420	3363020	This is a narrowing list, but like, you know, back in the day, like, imagine I was starting
3363020	3364020	your company in 1960.
3364020	3365020	Right?
3365020	3368220	Oh, we're going to have to hire a whole bunch of calculators in the basement to do all that,
3368220	3369700	you know, math to support the cabinet.
3369700	3371300	Dude, have you heard about computers?
3371300	3374060	Why don't we just buy a few of those?
3374060	3376580	Oh, oh, wow, man.
3376580	3377580	You're right.
3377580	3379940	Um, so like, I feel like that's kind of happening again.
3379940	3382460	I'm thinking about, I will post in my discord.
3382460	3386100	I'll be like, who wants to like, okay, I just changed my unary op.
3386100	3392300	There used to be log and X in like E. I changed them to be log to an X to because hardware
3392300	3394020	has log to an X to accelerators.
3394020	3395020	Yeah.
3395020	3396020	And of course you can use change of base.
3396020	3401420	It's one multiply to get it back to E. But like I made the primitives log to an X to right.
3401420	3403180	And this is the kind of, I just posted in the discord.
3403180	3406100	I'm like, could someone put this pull request up and someone eventually did and I merged
3406100	3407100	it.
3407100	3409980	But I'm like, this is almost to the level where models can do it.
3409980	3410980	Right?
3410980	3414180	We're almost to the point where I can say that to a model and the model can do it.
3415060	3416060	Have you tried?
3416060	3417060	Yeah.
3417060	3418940	I'm, I don't know.
3418940	3423500	I'm like, I'm, I think it went further.
3423500	3428620	I think autocomplete went further than I thought it would, but I'm also relatively unimpressed
3428620	3435340	with these chatbots, uh, with what I've seen from the language models like there.
3435340	3440180	The problem is if your loss function is categorical cross entropy on the internet, your responses
3440180	3441180	will always be mid.
3441180	3442180	Yes.
3442180	3443180	I don't know.
3443180	3444180	Mode collapse is what I call it.
3444180	3445180	I don't know.
3445180	3446180	I'm not even talking about mode collapse.
3446180	3447180	You're actually trying to predict these.
3447180	3448180	Like, like, look, I rap.
3448180	3452260	I'm a, I'm a hobbyist rapper and like, would I try to get these things to write rap?
3452260	3454700	The rap sound like the kind of raps you read in the YouTube comments.
3454700	3455700	Nursery school.
3455700	3456700	Yeah.
3456700	3457700	It's like, all right, great.
3457700	3458700	You're riding box with Fox.
3458700	3459700	Sick rhyme, bro.
3459700	3465140	Uh, you know, uh, you know, and rock or Drake is rhyming, give it up for me with napkins
3465140	3466140	and cutlery.
3466140	3467140	Right?
3467140	3468140	Like, like, all right, come on.
3468140	3469140	What's that?
3469140	3470140	Like this thing about orange.
3470140	3471140	Like orange is famous.
3471140	3472140	Yeah.
3472140	3478580	And now, of course, you know, four inch screws and orange juice is in, is in training corp.
3478580	3482900	But uh, yeah, so I think it went further than like everyone kind of thought it would.
3482900	3486860	But the thing that I really want to see is like somebody put 10 LLMs in a room and have
3486860	3488940	them discuss the answer before they give it to me.
3488940	3489940	Right?
3489940	3490940	You can actually do this, right?
3490940	3493100	Um, and I think the coding things have to be the same way.
3493100	3495780	There is no coder alive, no matter how good you are that sits down.
3495780	3500280	Well, I'm going to start at cell A one and type my program and then I'm going to press
3500280	3501680	run and there's going to work.
3501680	3504080	No one programs like that.
3504080	3505560	So why do we expect the models to write?
3505560	3508080	So, so there's a lot that like still needs to be done.
3508080	3511400	But you know, at the tiny corp, I want to be on the cutting edge of this too.
3511400	3513760	I want to be like program generation.
3513760	3514840	I mean, what is tiny grad?
3514840	3518120	It's a compiler generates programs generate the fastest program that meets the spec.
3518120	3519120	Right?
3519120	3520780	Why am I not just having a male do that?
3520780	3527400	So you know, it's kind of a, you have to exist fluidly with the machines and I come
3527400	3528400	around on a lot of stuff.
3528400	3531000	And I'm like, wait, tiny grad, tiny group should be a remote company.
3531000	3532000	Right?
3532000	3533000	Well, I can't do this in person.
3533000	3534000	Really?
3534000	3535000	Yeah.
3535000	3537320	Like, like comma makes sense to be in person like comma, sure.
3537320	3538320	Yeah.
3538320	3539320	We're getting off in San Diego.
3539320	3540320	Like, but that's a six year old company.
3540320	3541320	Right.
3541320	3543760	And it works and it works for a certain type of people and certain type of culture, but
3543760	3544760	what's going to be different this time.
3544760	3545760	Okay.
3545760	3546760	Remote, but now it's remote.
3546760	3550720	And now I'm getting these like people who apply and I'm like, I literally have a thousand
3550720	3551720	applications.
3551720	3554800	I'm not calling you to do a technical screen and I can't really tell anything from a
3554800	3555800	technical screen.
3555800	3556800	What am I going to do?
3556800	3557800	Make a code on a whiteboard?
3557840	3562320	Bring up, bring up a shared notebook document so we could, oh, like that's not going to
3562320	3563320	work.
3563320	3564320	Okay.
3564320	3565320	So then I'm moved to the next thing.
3565320	3567520	We do this a comma with good success programming challenges.
3567520	3570480	I've also found them to be like completely non predictive.
3570480	3575520	I found one thing to actually be predictive and it's, wait a second, just write code in
3575520	3576520	tiny grad.
3576520	3577520	It's open source.
3577520	3578520	Right.
3578520	3579520	And yeah.
3579520	3583160	So, you know, I'm talking to a few people who've been contributing and like contribute
3583160	3586440	or you know, the job's not for you, but you can do it remote and it's like it's a
3586440	3587440	chill job.
3587440	3589560	Like you're not, you're like, oh yeah, well, I work for the tiny corp.
3589560	3591680	Like, well, you're writing MIT license software.
3591680	3592680	Like you see what it's doing.
3592680	3593680	Right.
3593680	3596880	Like we'll just, I think it's maybe more of like a stipend than a salary and then also
3596880	3597880	some equity.
3597880	3598880	Like, you know, I get rich.
3598880	3599880	We all get rich.
3599880	3600880	Yeah.
3600880	3601880	Yeah.
3601880	3607400	How do you think about agents and kind of like thinking of them as people versus like
3607400	3608680	job to be done?
3608680	3613680	Sean, build this thing called small developer and then it's in the same vein like the human
3613680	3618120	in the loop with the language model and just iterating while you write code.
3618120	3620440	I think, I think that's, that's absolutely where it goes.
3620440	3622440	And there's like a, it's not like one thing.
3622440	3623960	It's like, there's small interpreter.
3623960	3624960	There's like small debugger.
3624960	3627320	It's kind of like all these different jobs to be done.
3627320	3628320	It's a small world.
3628320	3629320	Yeah.
3629320	3630320	It's a, I know this is like the small pockets.
3630320	3632480	It's like small AI meet tiny corp.
3632480	3634120	So we're all in the same wavelength.
3634120	3635120	How do you think about that?
3635120	3639840	Do you think people will have a human like interaction with like, oh, this is like the
3639840	3644440	AI developer or like, is it, I'm the human being supercharged by the AI tools?
3644440	3649120	Oh, I think it's much more like I'm the human supercharged by the AI tools.
3649120	3652040	I think that like coding is tool complete.
3652040	3653040	Right.
3653040	3654040	Like driving is not tool complete.
3654040	3655040	Right.
3655040	3657360	Like driving is just like, like we hire people to drive who are like below the API line.
3657360	3658360	Right.
3658360	3659360	There's an API line in the world.
3659360	3660360	Right.
3660360	3661360	Love that.
3661360	3662360	Yeah.
3662360	3663360	There's an API line in the world and like you can think like Uber's a really clear
3663360	3664360	example.
3664360	3665360	Right.
3665360	3666360	There's the people below the API line and the people above the API line.
3666400	3670400	The way you can tell if you're below or above, by the way, is, is your manager a computer?
3670400	3671400	Right.
3671400	3672400	Who's the manager of the Uber driver?
3672400	3673400	Well, computer.
3673400	3674400	There's a machine tell you what to do.
3674400	3675400	Do you tell machines what to do?
3675400	3676400	Exactly.
3676400	3677400	Exactly.
3677400	3678400	Um, so.
3678400	3679400	Coding is tool complete.
3679400	3680400	Right.
3680400	3681400	Coding is tool complete.
3681400	3682400	Coding is above the API line.
3682400	3687880	So it will always be, uh, tools supercharging your coding workflow and it will never be
3687880	3694880	you performing some like task like, okay, well, I can do everything except for actually starting
3694880	3695880	a docker container.
3695880	3696880	Like it just doesn't make any sense.
3696880	3697880	Right.
3697880	3698880	Um, yeah.
3698880	3699880	So it will always be sort of tools.
3699880	3703000	And you know, look, we see the same stuff with all the, like people are like stable diffusion
3703000	3705080	is going to replace artists or whatever.
3705080	3708640	It's like, dude, like it's going to create new artists, did Photoshop replace artists?
3708640	3710360	Like, what are you talking about?
3710360	3711360	Right.
3711360	3716320	Like, you know, real artists, finger paint, they can't use brushes, brushes are, you know,
3716320	3721440	brushes are going to replace all the, okay, like, I just can't like it's all just tools
3721440	3723440	and the tools are going to get better and better and better.
3723440	3724440	Eventually.
3724440	3725440	Yes.
3725440	3726440	The tools are going to replace us.
3726440	3727440	But you know, that's still 20 years away.
3727440	3730520	So now I got a company in the meantime.
3730520	3733440	So I've written about the API line before and I think that's from Venkatesh.
3733440	3735880	I don't know if you've definitely took it from someone.
3735880	3736880	It's definitely not mine.
3736880	3737880	VGR.
3737880	3738880	Yeah.
3738880	3741520	But I also have a speculated a higher line than that, which is the Kanban board, like
3741520	3743840	who tells the, the programmers what to do.
3743840	3744840	Hmm.
3744840	3745840	Right.
3745840	3749640	So are you above or below the Kanban board at this has that evolved your management
3749640	3750640	thinking?
3750640	3751640	Yeah.
3751640	3752640	Like that's sort of what I mean.
3752640	3756400	Like I'm just going to describe the pull request in two sentences and then like, yeah,
3756400	3757400	yeah.
3757400	3758880	So you are running the Kanban board or the bounties?
3758880	3759880	Yes.
3759880	3760880	Yeah.
3760880	3761880	The bounties of the Kanban board.
3761880	3762880	Yes.
3762880	3763880	Exactly.
3763880	3764880	And that is kind of the high level.
3764880	3768560	And then like, yeah, we'll get AIs to fill in some and we'll get people to fill in others.
3768560	3771280	And that's also what it means to be like full time, a tiny corp.
3771280	3772280	Right.
3772280	3775080	Would you start, and I wrote this up pretty concretely, I'm like, okay, step one is you
3775080	3776680	do bounties for the company.
3776680	3778680	Step two is you propose bounties for the company.
3778680	3779680	Right.
3779680	3780680	You don't obviously pay them.
3780680	3781680	We pay them.
3781720	3786400	And I'm like, yeah, that's a good bounty that like helps with the main workflow of the company
3786400	3791640	and step three is you get hired full time, you get equity, we all maybe get rich.
3791640	3794400	What else are you designing differently about the employee experience?
3794400	3800320	I mean, I'm very much alike, you know, some people really like to like, like keep a separation.
3800320	3801320	Right.
3801320	3805960	Some people really like to keep a separation between like employees and management or customers
3805960	3806960	and employees.
3806960	3810800	Like a comma, you know, the reason I do the dev kit thing, it's like, dude, you buy a
3810800	3813760	common thing, you're an employee of the company, like you're just part of the company.
3813760	3815080	It's all the same thing.
3815080	3819200	There's no like secrets, there's no dividing lines, there's no like, it's all a spectrum
3819200	3822840	for like, you know, down here at the spectrum, like you pay and then up here at the spectrum,
3822840	3823840	you get paid.
3823840	3825820	You understand this is the same spectrum of college, right?
3825820	3830000	Like for undergrad, you pay and then you get up here to like, you know, doing a PhD program,
3830000	3831000	you get paid.
3831000	3832000	Okay.
3832000	3833000	Well, cool.
3833000	3835720	Welcome to the, you know.
3835720	3838120	What about a comma bodies?
3838120	3841240	You know, you mentioned a lot of this stuff is clearly virtual, but then there's below
3841240	3843480	the API line you actually need.
3843480	3846280	This is the thing that's been announced?
3846280	3847280	Comma bodies?
3847280	3848280	We sell them.
3848280	3849280	Oh, okay.
3849280	3850280	You can buy them.
3850280	3851280	There's a thousand bucks on our website.
3851280	3852280	Oh, okay.
3852280	3853280	No, no, no.
3853280	3854280	I'm thinking about like the, what Tesla announced with like the humanoid robot.
3854280	3855280	It's the same thing.
3855280	3856280	Yeah.
3856280	3857280	Except of course we made the comma version of it.
3857280	3860000	Tesla uses 20 actuators, we use two, right?
3860000	3865440	Like, how do you, how do you build the simplest possible thing that can like turn the robotics
3865440	3867000	problem into entirely a software problem?
3867000	3872240	So right now it is literally just a comma three on a pole with two wheels.
3872240	3877080	It balances, keeps the comma three up there and like there's so much you could do with
3877080	3878080	that already.
3878080	3879080	Right?
3879080	3881800	Like this should replace, how many security guards could this replace?
3881800	3882800	Right?
3882800	3886920	If this thing could just competently wander around a space and take pictures and, you
3886920	3891200	know, focus in on things, send you a text message when someone's trying to break it
3891200	3895440	to your building, you know, like, like this could already do so much, of course, but
3895440	3896680	the software is not there yet.
3896680	3897680	Right?
3897680	3900720	So how do we turn robotics into a thing where it's very clearly a software problem?
3900720	3903280	You know, that people don't accept that self-driving cars are a software problem.
3903280	3905760	I'm like, I don't know what to tell you, man.
3905760	3909720	Like literally just watch the video yourself and then drive with a joystick.
3909720	3910720	Right?
3910720	3911720	Yeah.
3911720	3912720	Can you drive?
3912720	3913720	We've actually done this test.
3913720	3915840	We've actually done this test where we've had someone, okay, you just watch this video
3915840	3918600	and here's a joystick and you got to drive the car and of course they can drive the car.
3918600	3919600	Yeah.
3919600	3924000	It takes a little bit of practice to get used to the joystick, but the problem is all
3924000	3925000	the model.
3925120	3926120	Yeah.
3926120	3931920	It is specifically anything in computer vision that you think our second most popular episode
3931920	3936600	ever was about segment anything coming out of Facebook, which is as far as I understand
3936600	3940880	the state of the art in computer vision, what are you hoping for there that you need for
3940880	3941880	a comma?
3941880	3942880	I haven't used segment anything.
3942880	3947760	I mean, like the large yolos or not, I've used like large yolos and I'm super impressed
3947760	3948760	by them.
3948760	3949760	Yeah.
3949760	3950760	You think it's solved?
3950760	3951760	I got to check out segment anything.
3951760	3953880	I don't think it's a distinct problem, right?
3953880	3954880	Okay.
3954880	3955880	Here's something that I'm interested in.
3955880	3956880	All right.
3956880	3957880	We have great LLMs.
3957880	3960360	We have great text-to-speech models and we have great speech-to-text models.
3960360	3961360	Okay.
3961360	3963160	So why can I not talk to an LLM?
3963160	3964680	Like at how a normal conversation with them?
3964680	3967960	You can with the latency of like two seconds every time.
3967960	3968960	Right.
3968960	3969960	Why isn't this?
3969960	3971560	And then it feels so unnatural.
3971560	3975040	It's this like staccato, like I don't like the RLHF models.
3975040	3977000	I don't like the two inversions of them.
3977000	3981280	I think that they become, you take on the personality of a customer support agent.
3981280	3982280	Right.
3982280	3983280	Like, oh, come on.
3983280	3985960	I like LLAMA more than Chatchapiti.
3985960	3989440	Chatchapiti's personality just graded on me.
3989440	3990920	Was LLAMA like cool?
3990920	3992640	I write a little bit of pretext paragraph.
3992640	3994680	I can put you in any scenario I want, right?
3994680	3996160	Like that's interesting to me.
3996160	3998440	I don't want some like, you know, yeah.
3998440	4007400	So yeah, I think there is really no like distinction between computer vision and language and any
4007400	4009440	of this stuff.
4009440	4012400	It's all eventually going to be fused into one massive.
4012400	4015000	So to say computer vision is solved, well, it doesn't make any sense because what's the
4015000	4019840	output of computer vision model segmentation, like what a weird task, right?
4019840	4020840	Who cares?
4020840	4021840	OCR.
4021840	4022840	Who cares?
4022840	4025000	I don't care if you can segment which pixels make up that laptop, I care if you can pick
4025000	4026000	it up.
4026000	4027000	Yeah.
4027000	4030000	Interactive real world.
4030000	4032080	And you're going to have the local cluster.
4032080	4033920	You're going to have the body.
4033920	4034920	Yeah.
4034920	4035920	Yeah.
4035920	4036920	I think that's kind of where that goes.
4037200	4043120	So maybe we can paint the future of like, the year is 2050.
4043120	4046080	You've achieved all you wanted at TinyCorp.
4046080	4048880	What is the AI enabled future like?
4048880	4050720	Well, TinyCorp is the second company.
4050720	4052000	Comma was the first.
4052000	4053720	Comma builds the hardware infrastructure.
4053720	4055680	TinyCorp builds the software infrastructure.
4055680	4058960	The third company is the first one that's going to build a real product and that product
4058960	4060960	is AI Girlfriend.
4060960	4063400	No, like I'm dead serious, right?
4063400	4065280	Like this is the dream product, right?
4065280	4067840	This is the absolute dream product.
4067840	4069640	Girlfriend is just the like.
4069640	4070640	Stand-in.
4070640	4071640	Well, no, it's not a stand-in.
4071640	4072640	No, no, no.
4072640	4073640	I actually mean it, right?
4073640	4076880	So I've been wanting to merge with a machine ever since I was like, mad little, like, you
4076880	4079280	know, how do I merge with a machine, right?
4079280	4082120	And like you can look at like in like a maybe the Elon style way of thinking about his neural
4082120	4083120	link, right?
4083120	4086440	I'm like, I don't think we need any of this, right?
4086440	4091200	Some of your friends, maybe they get into relationships and you start thinking of, you
4091200	4092960	know, them and their partner as the same person.
4092960	4094840	You start thinking of them as like one person.
4094840	4097560	I mean, they are kind of like merged, right?
4097560	4099840	Like humans can just kind of do this.
4099840	4100840	It's so cool.
4100840	4102320	It's this ability that we already have.
4102320	4106920	It's all I need to put, you know, electrodes in my brain to merge with a machine.
4106920	4109480	I need an AI girlfriend, right?
4109480	4110480	So that's what I mean.
4110480	4112880	Like this is the third product.
4112880	4114560	This is the third company.
4114560	4118680	And yeah, in 2050, I mean like, ah, it's so hard.
4118960	4124560	Like maybe I can imagine like 2035, I don't even know 2050, like, yeah, 2035, like, yeah,
4124560	4125560	that'd be really great.
4125560	4128480	Like I have this like kind of, you know.
4128480	4133240	So in terms of merging, like, isn't it, shouldn't you work on brain upload rather than AI girlfriend?
4133240	4135240	But I don't need brain upload, right?
4135240	4136680	I don't need brain upload either.
4136680	4139600	Like there's, there's thousands of hours of me on YouTube, right?
4139600	4140600	Yes.
4140600	4142720	If you might, how much of my brain's already uploaded?
4142720	4144200	That's only the stuff that you voice.
4144200	4146040	Yeah, it's not that different.
4146040	4147600	It's not that different, right?
4147600	4152120	You really think a powerful, you really think a, a model with, you know, an exoflop of compute
4152120	4155040	couldn't extract everything that's really going on in my brain.
4155040	4156440	I'm a pretty open person, right?
4156440	4157800	Like I'm not running a complex filter.
4157800	4159520	Humans can't run that complex of a filter.
4159520	4160520	Yeah.
4160520	4161520	Like humans just can't.
4161520	4163800	Like this is actually a cool quirk of, of, of biology.
4163800	4166280	It's like, well, humans like can't lie that well.
4166280	4167280	Yeah.
4167280	4168280	Yeah.
4168280	4172280	So is it good or bad to put all of your stream of consciousness out there?
4172280	4175280	I mean, I think it's good.
4175280	4176280	Yeah.
4176280	4177280	I mean, I don't know.
4177280	4178280	I don't know.
4178280	4179280	I don't know.
4179280	4180280	I don't live forever.
4180280	4181280	Yeah.
4181280	4183520	We said off, off Mike, we may be the first immortals, right?
4183520	4184520	Yeah.
4184520	4185520	Yeah.
4185520	4186760	Like this is how you, this is how you live forever.
4186760	4189960	It's a question of, okay, how many weights do I have?
4189960	4190960	Right?
4190960	4191960	Okay.
4191960	4192960	Let's say I have a trillion weights.
4192960	4193960	So it's talking about terabytes, a hundred terabytes here.
4193960	4195880	Like if it's not really a hundred terabytes, right?
4195880	4196880	Because it's called homograph complexity.
4196880	4198520	How much redundancy is there in those weights?
4198520	4204240	So like maximally compressed, how big is the weight file for my brain quantize it whatever
4204240	4205240	you want.
4205240	4207600	And quantization is, is a poor man's compression.
4207600	4214080	Um, I think we're only talking really here about like maybe a couple gigabytes, right?
4214080	4219120	And then if you have like a couple gigabytes of true information of yourself up there, cool
4219120	4221840	man, like what does it mean for me to live forever?
4221840	4222840	Like that's me.
4222840	4223840	Yeah.
4223840	4224840	No, I think that's good.
4224840	4230200	And I think like the, there's a bit of like a professionalization of social media or like
4230200	4234680	a lot of people only have what's like PC out there, you know, and I feel like you're gonna
4234680	4237280	get, come back to the chat GPT thing, right?
4237280	4240800	You're gonna train a model and like everything that's public about a lot of people.
4240800	4248440	And it's like, no one's gonna run their model and they're gonna die on social media.
4248440	4251680	Your life could depend on it.
4251680	4255760	We have a segment, uh, so, uh, we're, we're moving on to a, what, what would normally
4255760	4258840	be called the lightning round, but just, uh, just general takes because you're a generally
4258840	4260920	interesting person with many other interests.
4260920	4264520	Um, uh, what does the goddess of everything else mean to you?
4265360	4270520	Oh, it means that AI is not really going to kill us.
4270520	4271520	Really?
4271520	4272520	Of course.
4272520	4274520	Tell us more.
4274520	4279120	Look, uh, Lex asked me this, like, is they are going to kill us all?
4279120	4282000	And I was quick to say yes, but I don't actually really believe it.
4282000	4285520	I think there's a decent chance that AI, I think there's a decent chance that AI kills
4285520	4287320	95% of us.
4287320	4288820	Okay.
4288820	4291360	But they saw on your Twitch streams that you're with them.
4291360	4294480	So they're not gonna, no, I don't think I actually, I don't know.
4294480	4295440	I also think it's AI.
4295440	4297600	Like I think the AI alignment problem is so misstated.
4297600	4301520	I think it's actually not a question of whether the computer is aligned with the company who
4301520	4302520	owns the computer.
4302520	4304880	It's a question of whether that company is aligned with you or that government's aligned
4304880	4305880	with you.
4305880	4306880	And the answer is no.
4306880	4307880	And that's how you end up dead.
4307880	4313440	But, um, so what, what the goddess of everything else means to me is like, the complexity will
4313440	4314440	continue.
4314440	4315440	Paper clippers don't exist.
4315440	4318600	You know, there are forces, the paper clippers cancer, right?
4318600	4322560	The paper clippers really just a perfect form of cancer and the goddess of everything
4322560	4328000	else says, yeah, if a cancer doesn't win, you know, yeah, it's a beautiful story for
4328000	4329200	those who haven't heard it.
4329200	4331720	And you read it out and I listen to it.
4331720	4332720	Um, yeah.
4332720	4333720	Good.
4333720	4334720	What else we have here?
4334720	4335720	Pick a question.
4335720	4336720	So many.
4336720	4337720	Yeah.
4337720	4338720	What are you grateful for today?
4338720	4339720	Oh, man.
4339720	4345800	I mean, it's all just like, I haven't, I haven't taken it about this stuff forever.
4345800	4350600	Like that it's actually like happening and it's happening in an accessible way too.
4350600	4352480	I guess that's what I'm really grateful for.
4352480	4357080	It's not like, like AI is not some Manhattan project style.
4357080	4358640	You don't know anything about it.
4358640	4359640	Close doors.
4359640	4360640	Close doors.
4360640	4361640	I'll fight really hard to keep it that way.
4361640	4367440	Uh, you know, uh, that's, that's a, I'm grateful for just, just how much is released
4367440	4371000	out there and how much I can just learn and stay up to date.
4371000	4375680	And I guess I'm grateful to the true fabric of reality that, you know, I didn't need differential
4375680	4376680	equations to understand it.
4376680	4380840	Like I don't need, you don't need, you don't need some like, like, like there's, there's
4380840	4381840	I've tried to do.
4382080	4386040	There's a limit to my, to my math ability is I can do most undergrad math, but I took
4386040	4389160	some grad math glasses and okay, now we're getting to the end of what I can do.
4389160	4393880	And it's just the actual like end of what I can do, like I'm limited by my brain, but
4393880	4397800	you know, ML stuff, you need high school math.
4397800	4398800	Yeah.
4398800	4399800	Like I could do all that.
4399800	4400800	Nothing like, you know what I mean?
4400800	4403440	When I learned to multiply a matrix, seventh grade, like it's all easy.
4403440	4406520	You need more electrical engineering than you need high school math early.
4406520	4407520	Yeah.
4407520	4410120	Well, you need electrical engineering to like build the machines, but even that, like
4410120	4414640	these machines are simpler than the machines that have existed before, the compute stack
4414640	4415640	looks really nice.
4415640	4419160	So, you know, yeah, I just, I'm grateful that it's all happening and I get to understand
4419160	4420160	it be here.
4420160	4421160	Yeah.
4421160	4422160	Yeah.
4422160	4425520	Um, John Carmack mentioned there's about six insights we have left.
4425520	4428800	Do you have an intuition for what some of the paths people should be taking?
4428800	4431080	Obviously you're working on one.
4431080	4434600	What are some of the other branches of the tree that people should go under?
4434600	4436640	I don't think I'm working on one of the six insights.
4436640	4439000	I don't think tiny grads any one of the six insights.
4439000	4444000	Um, something I really like that Elon does and I try to take it from, uh, try to be inspired
4444000	4452120	by it is, um, look at the boring tunnel machine and ask how you can build a 10x cheaper one.
4452120	4453120	All right.
4453120	4454120	Look at the rocket.
4454120	4455120	How can I build a 10x cheaper one?
4455120	4456120	All right.
4456120	4457120	Look at the electric car and say, how can I build a 10x cheaper?
4457120	4461000	Like, cheaper or, you know, can go further or whatever, whatever, whatever, right?
4461000	4462520	You just do the straight up physics math, right?
4462520	4466440	Like, I'm trying to do the same thing with, with, uh, ML frameworks, right?
4466440	4471480	And in, in, in doing so, making sure that this stuff remains accessible, right?
4471480	4476800	You could imagine a world where if Google TPUs were actually the ultimate, if Google
4476800	4479400	TPUs were actually the best training things, I mean, actually, you know, I'm kind of grateful
4479400	4480400	for NVIDIA, right?
4480400	4483760	Like, because if Google TPUs were the ultimate, now you have this huge closed source compiler
4483760	4489880	in between XLA and, and the hardware and yeah, that's, uh, just a really bad thing.
4489880	4493680	So I mean, something that is somewhat upsetting, but the tiny group is it, is that it is trying
4493680	4497320	to prevent downside, but, uh, it's not all trying to prevent downside.
4497320	4500720	Like we're also building computers and we're going to build some awesome, powerful, cheap
4500720	4502920	computers, uh, along the way.
4502920	4505800	Uh, so no, I'm not really working directly on any of the six tricks.
4505800	4510000	I also think the six tricks are kind of going to be like, luck, I think it's going to be
4510000	4513680	like, you know, please tell me more about what covariate shift is and how that inspired
4513680	4515720	you to come up with batch normalization.
4515720	4519760	Please tell me more about why it's a transformer and it has a query, a key and a value, right?
4519760	4523200	Like Schmidt-Huber described it better and fast weights, you know?
4523200	4527600	Like, like, I mean, my theory about why transformers work have nothing to do with this attention
4527600	4531160	mechanism and just the fact that like it's semi-weight sharing, right?
4531160	4534880	Because the weight matrix is being generated on the fly, you can, you can like compress
4534880	4535880	the weight matrix.
4535880	4536880	Right?
4536880	4540440	Like this is what that, there's, there's an operation in the, in the transformer, which,
4540440	4546000	uh, like, and by the way, this is like Qualcomm's S and PE can't run transformers for this reason.
4546000	4550960	So most matrix multiplies in neural networks are weights times values, right?
4550960	4555040	There is, um, you know, when you get to the, the, the outer product in, in, uh, transformers,
4555040	4558480	well, it's weights times weight, it's a, it's values times values, right?
4558480	4561560	So S and PE like doesn't even support that operation, right?
4561560	4565360	So it's like that operation that gives the transformer its power, it has nothing to do
4565360	4567280	with the fact that it's attention, right?
4567280	4570320	And this is a funny like, but that is one of the six tricks, right?
4570320	4574320	Batch, like these norms are a trick, transformers are a trick.
4574320	4575320	Okay.
4575320	4576320	Six more.
4576680	4581880	Is there a reason why, so you couldn't talk, you talk about, uh, attention as weight compression,
4581880	4584400	um,
4584400	4586040	Compression is not exactly the right word.
4586040	4589080	What I mean is that the weights can change dynamically based on the context.
4589080	4592840	So it was this thing in pack eight in the hot air prize that I absolutely loved.
4592840	4594840	And I've never seen it again in neural networks and a really good trick.
4594840	4595840	Okay.
4595840	4599460	Imagine you have 256 weight sets for a layer, right?
4599460	4604000	And then you choose which of the weight sets you're loading in based on some context and
4604000	4605840	that context can come from another neural net, right?
4605840	4611280	So I have another one at which protect, projects, you know, 256 wide, one hot, do a soft max,
4611280	4614560	predict it, and then I actually load the weights and I can do this operation at both test time
4614560	4615560	and train time.
4615560	4618520	I can do this operation at both training and inference, and I load in the weights given
4618520	4620880	the context, right?
4620880	4625960	Like that is what transformers do, but transformers instead of having 256 discrete ones, it's
4625960	4627360	actually just that but continuous.
4627360	4628360	Yeah.
4628360	4631320	Um, which is funny that that was in language models and I just like, when I understood
4631320	4634200	that about transformers, I'm like, Oh, this is a real trick and why are they using the
4634200	4635200	word attention?
4635720	4639240	And today is actually the anniversary of attention is all you need.
4639240	4640240	What?
4640240	4641240	Oh, that's a day.
4641240	4642240	Today, six years ago.
4642240	4643240	Six years.
4643240	4644240	Six years.
4644240	4645240	Change the world.
4645240	4646240	Wow.
4646240	4647240	Well, there's one of your envelope tricks, right?
4647240	4649760	And you can easily write it on an envelope, you know, think about how you write out that
4649760	4652480	how many times have you written that because it's not in any libraries because it's like
4652480	4654440	all used a little differently each time.
4654440	4655440	Yeah.
4655440	4659880	If you just write out that exact same, you know, yeah, yeah.
4659880	4661720	You've name checked Elon a few times.
4661720	4662720	Yeah.
4662760	4667760	Think about both of you as systems, thinkers, input, output, think something in between.
4667760	4668760	Sure.
4668760	4673720	Um, what, what's different about your style versus his, um, Elon's fundamental science
4673720	4676240	for the world is physics minus information theory.
4676240	4677240	Huh.
4677240	4679000	But you, you do a lot of physics as well.
4679000	4682720	I mean, like, and Elon does a lot of information theory as well too.
4682720	4686560	But if the question is fundamentally that the difference maybe is expressed in what
4686560	4688840	your ambitions are, right?
4688840	4692400	Elon's ambitions may be like, go to Mars.
4692400	4693400	Right.
4693400	4697120	Go to Mars is the ultimate modern, modernist physics ambition.
4697120	4698120	Right.
4698120	4699120	It's a physics problem getting to Mars.
4699120	4700120	Right.
4700120	4701120	What are electric cars?
4701120	4702120	It's a physics problem.
4702120	4703120	Right.
4703120	4704120	Okay.
4704120	4705120	Now he's like pushing on the autonomy stuff.
4705120	4708880	You push a little on information theory, but fundamentally his dreams are physics based
4708880	4709880	dreams.
4709880	4711120	My dreams are information based dreams.
4711120	4713760	I want to live forever in virtual reality with my agri-girlfriend.
4713760	4714760	Right.
4714760	4718160	Those are, those are the aspirations of someone who, who, who accepts information theories
4718160	4719160	of course science.
4719160	4720680	So I think that's the main difference to me and him.
4720680	4723400	He has physics based aspirations and I have information based aspirations.
4723400	4726120	Very, very neat.
4726120	4727120	Mark Andreessen.
4727120	4730960	He is a, hi Mark, he's a listener.
4730960	4734320	He is heavily, he's a big proponent of effective accelerationism.
4734320	4736000	You've been a bit more critical.
4736000	4739760	Why do you say that EAC is not taken seriously by its adherents?
4739760	4745600	Oh, well, only the left takes ideology seriously.
4745600	4747000	Why is that?
4747000	4748760	Well, just as a fact.
4748760	4750040	It's just like, it's just like a fact.
4750040	4751040	Is the right more cynical?
4751040	4752040	Is that, is that what it is?
4752040	4753040	I don't know.
4753040	4757240	It's like, it's like the left actually manages to get energy around the ideologies.
4757240	4758240	Right.
4758240	4762360	Like, like, like there's a lot more, look, here you have, you have two effective altruists
4762360	4764200	named Sam going in front of Congress.
4764200	4765200	I only one of them is in jail.
4765200	4767200	Um, you know, it's, it's interesting.
4767200	4769680	They're both calling for regulation in those spectra spaces, right?
4769680	4773400	So SPF is definitely like kind of a wolf in sheep's clothing kind of, right?
4773400	4777600	Like he, he only adopted EAC or EA to walk in.
4777600	4781600	Sam Altman is a genuinely good guy who is not interested in power seeking for himself.
4781600	4782600	All right.
4782600	4783600	Okay.
4783600	4784600	We don't, we don't have to go.
4784600	4785600	Fair enough.
4785600	4786600	Fair enough.
4786600	4790040	Um, but, uh, no, EAC is not like, like you are not serious, right?
4790040	4793520	You are not actually a serious ideology.
4793520	4797840	You know, uh, Mark Andreessen, I like Mark Andreessen, but I think that like some of
4797840	4801200	his Twitter things are like, dude, you like just like, it's like, it's like someone who's
4801200	4807560	like 2019 who's like, eyes were opened about like the political world being not exact.
4807560	4809920	You mean all the people on the news were lying to me?
4809920	4810920	Yeah, bro.
4810920	4811920	They were lying to you.
4811920	4814120	Like, okay, we all figured this out five years ago.
4814120	4815120	Now what are you going to do about it?
4815120	4816560	I'm going to complain about it on Twitter.
4816560	4817560	Great.
4817560	4818560	And that's what EAC is.
4818560	4825120	Um, last and maybe most important, uh, why was Avatar 2 bad?
4825120	4828880	Oh, I have a whole, you can go on my blog.
4828880	4830840	I rewrote the script of Avatar 2.
4830840	4834400	I wrote a script that actually might make you feel something for the characters.
4834400	4837080	I killed Jake Sully in the first scene like you had to.
4837440	4840480	Do you really think his second story are topped his first one?
4840480	4841480	No, of course not.
4841480	4844280	You had to kill the guy and make the movie about the brothers, right?
4844280	4848080	And just that alone and realizing that like you could have kept the Titanic scene.
4848080	4849080	It would have been fine.
4849080	4850080	I didn't even take it out.
4850080	4853880	I left your Titanic scene, James Cameron, but I wrote you a story that so, you know,
4853880	4857080	you just, just, just, he needs ships to sink in water.
4857080	4861440	He needs, well, look, it's a great scene, but like the movie was just like, like the
4861440	4862440	Roman, never.
4862440	4864720	Great CGI, you know, let down by the writing, maybe.
4865600	4869080	Yeah, no, but like the CGI, like it was, it was a beautiful world.
4869080	4870560	And that's why like I care so much, right?
4870560	4873080	Like you don't hear me ranting about Pirates of the Caribbean 2 being a
4873080	4875600	terrible story because come on, what do you expect, man?
4875960	4878360	Like Johnny Depp is like, wow, I had a movie that made me rich.
4878360	4879360	I love this.
4880800	4883640	But this goes back to like the midpoint, you know, I think you were like, feels
4883640	4885120	like Chad Chippity wrote the movie.
4885160	4887400	And that's my worry a little bit.
4887440	4889280	It's like kind of converging towards that.
4889480	4891440	Oh, I look, Malak wrote the movie.
4893440	4894840	Sorry, I didn't want to interrupt you.
4894840	4897880	No, I closed, I closed a pull request two days ago.
4897880	4899560	I was like, was this written by Chad Chippity?
4899560	4900560	And I just closed it.
4900760	4901320	Like, you know what?
4901320	4903600	I honestly feel bad if you were a human who wrote this.
4904200	4907120	Like you're incapable of being more perfect.
4907400	4911800	But if you have a classifier running in my head that asks, you know, is this
4911800	4913400	an AI or is this a human?
4913400	4919160	Like, you know, the only way to deal with all this, like, like, oh, God, it's
4919160	4920160	like the worst possible.
4920320	4924720	Like, you know, people are like, like, like, how are you mad about like these
4924720	4925240	chatbots?
4925240	4926560	You're not mad about like Tesla.
4926840	4929840	Well, because if I don't want to buy a Tesla, I'll have to buy a Tesla.
4929880	4931800	And it won't really impact my life negatively.
4931960	4934880	But if I don't want to use a chatbot, it's still going to impact my life negatively.
4935120	4939000	All the amount of like personalized spam that now makes me spend more cycles on
4939000	4942600	my classifier to tell if it's spam or not, because you can now use AI's and
4942600	4943840	generate this so cheaply.
4944480	4947440	Like, no, I mean, we have to move to a model where everything's just a dollar,
4947440	4947600	right?
4947600	4948960	Like you want to send me an email, it's a dollar.
4949200	4950000	Like you guys wouldn't care.
4950000	4950800	No, my friends would care.
4950800	4952680	No one would care except the spammers.
4953040	4953280	Right.
4953320	4954680	Like we just got to move to those sort of models.
4956440	4956840	Awesome.
4957080	4960520	Um, one last message you want everyone to remember?
4961680	4965560	Uh, look, go, uh, go try Tanygrad.
4966160	4971400	Uh, I hope that we're a serious competitor to, to what's out there.
4971760	4974080	And then I want to, you know, I want to take it all the way.
4974120	4977480	We'll start with just building something for GPUs and then we'll start building
4977480	4980640	chips and we'll start building fabs and we'll start building silicon mines.
4980640	4982880	And we'll have the first self-reproducing robot using.
4983120	4983280	Yeah.
4983280	4983520	Okay.
4985720	4986280	All right, George.
4986320	4987440	Thank you so much for coming on.
4987440	4988400	Thank you for the big inspiration.
4988440	4988800	Thank you.
4989080	4989320	Thanks.
4990120	4990400	All right.
4993280	4993760	How was that?
4993760	4997280	We, uh, not, not quite like, but we hope to do something.
