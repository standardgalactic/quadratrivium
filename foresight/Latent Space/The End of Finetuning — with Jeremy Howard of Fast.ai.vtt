WEBVTT

00:00.000 --> 00:13.960
Hey everyone, welcome to the Late in Space Pockets.

00:13.960 --> 00:18.400
This is Alessio, partner in CTO and residence at Decibel Partners, and I'm joined by my

00:18.400 --> 00:20.840
co-host, Swix, founder of SmallAI.

00:20.840 --> 00:26.680
Hey, and today we have in the remote studio Jeremy Howard from, all the way from Australia.

00:26.680 --> 00:27.680
Good morning.

00:27.760 --> 00:29.960
The remote studio, also known as my house.

00:29.960 --> 00:30.960
Good morning.

00:30.960 --> 00:32.960
Nice to see you, Ruth.

00:32.960 --> 00:34.460
Nice to see you too.

00:34.460 --> 00:39.840
I'm actually very used to seeing you in your mask as a message to people, but today we're

00:39.840 --> 00:40.840
mostly audio.

00:40.840 --> 00:46.120
But thank you for doing the very important public service of COVID awareness.

00:46.120 --> 00:51.000
Once there was a pleasure, it was all very annoying and frustrating and tedious, but

00:51.000 --> 00:52.000
somebody had to do it.

00:52.000 --> 00:56.960
Somebody had to do it, especially somebody with your profile, I think, Julie drives home

00:56.960 --> 00:59.120
the message.

00:59.120 --> 01:03.920
So we tend to introduce people for them and then ask people to fill in the blanks on the

01:03.920 --> 01:06.280
personal side.

01:06.280 --> 01:10.800
Something I did not know about you was that you graduated with a B in philosophy from the

01:10.800 --> 01:12.640
University of Melbourne.

01:12.640 --> 01:14.640
I assumed you had a PhD.

01:14.640 --> 01:23.080
No, I barely got through my BA because I was working 80 to 100 hour weeks at McKinsey and

01:23.080 --> 01:27.760
a company from 19 years old onwards.

01:27.760 --> 01:35.760
So I actually didn't attend any lectures in second and third year university.

01:35.760 --> 01:39.680
Well, I guess you didn't need it or you're very sort of self-driven and self-motivated.

01:39.680 --> 01:47.720
I just took two weeks off before each exam period when I was working at McKinsey and

01:47.720 --> 01:50.560
then I can't believe I got away with this in hindsight.

01:50.560 --> 01:54.960
I would go to all my professors and say, oh, I was meant to be in your class this semester

01:54.960 --> 01:59.920
and I didn't quite turn up, were there any assignments I was meant to have done, whatever.

01:59.920 --> 02:05.520
I can't believe all of them let me basically, they basically always would say like, okay,

02:05.520 --> 02:08.560
well, if you can have this written by tomorrow, I'll accept it.

02:08.560 --> 02:11.960
So yeah, stressful way to get through university.

02:11.960 --> 02:17.840
Well, it shows that, I guess, you min-maxed the opportunities.

02:17.840 --> 02:18.840
That definitely was a tricker.

02:19.080 --> 02:27.160
I mean, finally, in philosophy, the things I found interesting and focused on in the

02:27.160 --> 02:31.000
little bit of time I did spend on it was ethics and cognitive science.

02:31.000 --> 02:36.440
And it's kind of really amazing that now come back around and those are actually genuinely

02:36.440 --> 02:39.040
useful things to know about, which I never thought would happen.

02:39.040 --> 02:43.240
A lot of relevant conversations there.

02:43.240 --> 02:48.640
So you were a consultant for a while and then in the magical month of June, 1999, you found

02:48.640 --> 02:52.280
it, both optimal decisions and fast meal, which I also really used.

02:52.280 --> 02:53.280
So thank you for that.

02:53.280 --> 02:54.280
Oh, good for you.

02:54.280 --> 02:55.280
Yeah.

02:55.280 --> 02:58.520
Because I had read the statistics switches at like 90% or something of small businesses

02:58.520 --> 02:59.520
fail.

02:59.520 --> 03:02.840
So I thought if I start two businesses, I have a higher chance.

03:02.840 --> 03:05.400
In hindsight, I was thinking of it as some kind of stochastic thing.

03:05.400 --> 03:10.720
I didn't have control over it, but it's a bit hard, but anyway.

03:10.720 --> 03:18.400
And then you were president and chief scientist at Kaggle, which obviously is the composition

03:18.400 --> 03:21.960
platform of machine learning.

03:21.960 --> 03:26.960
And then in Lytec, where you were working on using deep learning to improve medical diagnostics

03:26.960 --> 03:27.960
and clinical decisions.

03:27.960 --> 03:28.960
Yeah.

03:28.960 --> 03:30.920
That was actually the first company to use deep learning in medicine.

03:30.920 --> 03:33.560
So it kind of founded the field.

03:33.560 --> 03:36.800
And even now, that's still like a pretty early phase.

03:36.800 --> 03:42.120
And I actually heard you on your new podcast with Tanishk, where you went very, very deep

03:42.120 --> 03:46.560
into the stuff, the kind of work that he's doing, such a young prodigy at his age.

03:47.560 --> 03:49.760
Maybe he's too old to be called a prodigy now.

03:49.760 --> 03:50.760
X prodigy.

03:50.760 --> 03:53.680
No, I think he still counts.

03:53.680 --> 03:58.400
And anyway, just to round out the bio, you have a lot more other credentials, obviously.

03:58.400 --> 04:03.880
But most recently, you started Fast.AI, which is still, I guess, your primary identity with

04:03.880 --> 04:04.880
Rachel Thomas.

04:04.880 --> 04:05.880
So welcome.

04:05.880 --> 04:06.880
Thanks.

04:06.880 --> 04:07.880
Thank you.

04:07.880 --> 04:08.880
Yeah.

04:08.880 --> 04:13.480
Being a lot of public service there with getting people involved in AI, and I can imagine

04:13.480 --> 04:18.480
a better way to describe it than Fast.AI is you teach people from nothing to stable

04:18.480 --> 04:20.680
diffusion in seven weeks or something.

04:20.680 --> 04:21.680
And that's amazing.

04:21.680 --> 04:22.680
Yeah.

04:22.680 --> 04:23.680
Yeah.

04:23.680 --> 04:24.680
I mean, it's funny.

04:24.680 --> 04:27.400
When we started that, what was that like 2016 or something?

04:27.400 --> 04:31.740
The idea that deep learning was something that you could make more accessible was generally

04:31.740 --> 04:37.960
considered stupid, but everybody knew that deep learning was a thing that you got a math

04:37.960 --> 04:42.840
through a computer science PhD, you know, those one of five labs that could give you

04:42.840 --> 04:49.720
the appropriate skills, then you would join, yeah, basically from one of those labs, you

04:49.720 --> 04:52.920
might be able to write some papers.

04:52.920 --> 05:00.360
So yeah, the idea that normal people could use that technology to do good work was considered

05:00.360 --> 05:03.200
kind of ridiculous when we started it.

05:03.200 --> 05:06.080
And we weren't sure if it was possible either, but we kind of felt like we had to give it

05:06.080 --> 05:10.720
a go because the alternative was we were pretty sure that deep learning was on its

05:10.720 --> 05:18.800
way to becoming the most or one of the most important technologies in human history.

05:18.800 --> 05:24.200
And if the only people that could use it were a handful of computer science PhDs, that seemed

05:24.200 --> 05:29.440
like A, a big waste and B, kind of dangerous.

05:29.440 --> 05:33.880
And you know, well, I just wanted to know one thing on your bio that at Kaggle, you

05:33.880 --> 05:37.960
were also the top rank participant in both 2010 and 2011.

05:37.960 --> 05:42.080
So sometimes you see a lot of founders running companies that are not really in touch with

05:42.080 --> 05:46.680
the problem, but you were clearly building something that you knew a lot about, which

05:46.680 --> 05:48.280
is awesome.

05:48.280 --> 05:53.640
And even, yeah, talking about deep learning, you created, published a paper on ULM fit,

05:53.640 --> 05:58.400
which was kind of the predecessor to multitask learning and a lot of the groundwork that

05:58.400 --> 06:00.320
then went to into transformers.

06:00.320 --> 06:07.080
I read back on the paper and you turn this model AWD LSTM, which I did the math and it

06:07.080 --> 06:13.120
was like 24 to 33 million parameters, depending on what training data set you use today.

06:13.120 --> 06:17.280
That's kind of like not even small, it's like super small.

06:17.280 --> 06:22.920
What were some of the kind of like contrarian takes that you had at the time and maybe set

06:22.920 --> 06:28.320
the stage a little bit for the rest of the audience on what was kind of like the state

06:28.320 --> 06:32.280
of the art, so to speak at the time and what people were working towards.

06:32.280 --> 06:35.480
Yeah, the whole thing was a contrarian take, you know.

06:35.480 --> 06:41.840
So okay, so we started first AI, my wife and I, and we, yeah, so we're trying to think,

06:41.840 --> 06:43.720
okay, how do we make it more accessible?

06:43.720 --> 06:48.760
So when we started thinking about it, it was very 2015 and then 2016, we started doing

06:48.760 --> 06:49.760
something about it.

06:49.760 --> 06:50.760
Why is it inaccessible?

06:50.760 --> 06:57.000
Okay, well, A, no one knows how to do it other than a few number of people and then when

06:57.000 --> 07:00.280
we asked those few number of people, well, how do you actually get good results?

07:00.280 --> 07:04.320
They would say like, oh, it's like, you know, a box of tricks that aren't published.

07:04.320 --> 07:08.280
So you have to join one of the, you know, labs and learn the tricks.

07:08.280 --> 07:14.320
So a bunch of unpublished tricks, not much software around, but you know, thankfully

07:14.320 --> 07:21.200
there was Theano and, you know, rappers and particularly Lasagna, the rapper.

07:21.200 --> 07:28.160
But yeah, not much software around, not much in the way of data sets, you know, very hard

07:28.160 --> 07:33.440
to get started in terms of the compute, like how do you get that set up?

07:33.440 --> 07:38.000
So you know, everything was kind of inaccessible.

07:38.000 --> 07:45.280
And you know, as we started looking into it, we had a key insight which was like, you know,

07:46.040 --> 07:53.440
most of the compute and data for image recognition, for example, we don't need to do it.

07:53.440 --> 07:57.720
You know, there's this thing which nobody knows about, nobody talks about called transfer

07:57.720 --> 08:04.040
learning where you take somebody else's model where they already figured out like how to

08:04.040 --> 08:08.520
detect edges and gradients and corners and text and whatever else, and then you can fine

08:08.520 --> 08:11.400
tune it to do the thing you want to do.

08:11.400 --> 08:16.880
And we thought that's the key, that's the key to becoming more accessible in terms of

08:16.880 --> 08:19.200
compute and data requirements.

08:19.200 --> 08:24.480
So when we started FastAI, we focused from day one on transfer learning, lesson one,

08:24.480 --> 08:29.120
in fact, was transfer learning, literally lesson one, something not normally even mentioned

08:29.120 --> 08:30.120
in.

08:30.120 --> 08:36.520
I mean, there wasn't much in the way of courses, you know, basically, really the courses out

08:36.600 --> 08:42.120
there were PhD programs that had happened to have recorded their lessons, they would

08:42.120 --> 08:43.720
really mention it at all.

08:43.720 --> 08:48.360
We wanted to show how to do four things that seemed really useful, you know, work with

08:48.360 --> 08:55.200
vision, work with tables of data, work with kind of recommendation systems and collaborative

08:55.200 --> 08:59.640
filtering and work with text, because we felt like those four kind of modalities covered

08:59.640 --> 09:04.600
a lot of the stuff that, you know, are useful in real life.

09:04.680 --> 09:07.000
And no one was doing anything much useful with text.

09:07.000 --> 09:14.080
Everybody was talking about Word2Vec, you know, like King plus, Queen minus, woman and

09:14.080 --> 09:20.480
blah, blah, blah, and it was like cool experiments, but nobody was doing anything like useful

09:20.480 --> 09:21.480
with it.

09:21.480 --> 09:29.880
NLP was all like lamertization and stop words and topic models and diagrams and SPMs, and

09:29.880 --> 09:34.760
it was really academic and not practical.

09:34.760 --> 09:42.360
But I mean, to be honest, I've been thinking about this crazy idea for nearly 30 years

09:42.360 --> 09:49.040
since I had done cognitive science at university, where we talked a lot about the cells Chinese

09:49.040 --> 09:53.800
room experiment, this idea of like, what if there was somebody that could kind of like

09:53.800 --> 10:01.000
knew all of the symbolic manipulations required to answer questions in Chinese, but they didn't

10:01.000 --> 10:06.640
speak Chinese, they were kind of inside a room with no other way to talk to the outside

10:06.640 --> 10:10.080
world other than taking in slips of paper with Chinese written on them and then they

10:10.080 --> 10:15.280
do all their rules and then they pass back a piece of paper with Chinese back and this

10:15.280 --> 10:19.520
room with a person in is actually fantastically good at answering any question you give them

10:19.520 --> 10:28.320
written in Chinese, do they understand Chinese and is this something that's intelligently

10:28.320 --> 10:35.280
working with Chinese ever since that time, I'd say to me the most thoughtful and compelling

10:35.280 --> 10:43.080
philosophical response is yes, intuitively it feels like no, that's just because we

10:43.080 --> 10:49.760
can't imagine such a large kind of system, but if it looks like a duck and acts like

10:49.760 --> 10:54.160
a duck, it's a duck or to all intents and purposes.

10:54.160 --> 10:59.040
And so I always kind of thought, so this is basically a kind of analysis of the limits

10:59.040 --> 11:05.520
of text and I kind of felt like, yeah, if something could ingest enough text and could

11:05.520 --> 11:17.000
use the patterns it saw to then generate text in response to text, it could appear to be

11:17.000 --> 11:22.640
intelligent, whether that means it is intelligent or not is a different discussion and not one

11:22.640 --> 11:23.880
I find very interesting.

11:23.880 --> 11:28.960
Yeah, and then when I came across neural nets when I was about 20, I learned about the universal

11:28.960 --> 11:33.120
approximation theorem and stuff and I started thinking like, oh, I wonder if like a neural

11:33.120 --> 11:40.760
net could ever get big enough, take in enough data to be a Chinese room experiment.

11:40.760 --> 11:46.920
With that background and this kind of like interest in transfer learning, I'd been thinking

11:46.920 --> 11:50.320
about this thing for kind of 30 years and I thought like, oh, I wonder if we're there

11:50.320 --> 11:56.640
yet, because we have a lot of text, like I can literally download Wikipedia, which is

11:56.640 --> 11:58.480
a lot of text.

11:58.480 --> 12:05.160
And I thought, you know, how would something learn to kind of answer questions or respond

12:05.160 --> 12:06.160
text?

12:06.160 --> 12:08.240
And I thought, well, what if we used a language model?

12:08.240 --> 12:11.680
So language models are already a thing, you know, they were not a popular or well known

12:11.680 --> 12:12.680
thing, but they were a thing.

12:12.680 --> 12:17.080
But language models exist to this idea that you could train a model to fill in the gaps

12:17.080 --> 12:20.960
or actually in those days, it wasn't fill in the gaps, it was finish a string.

12:20.960 --> 12:27.480
In fact, Andre Kapathy did his fantastic RNN demonstration from this at a similar time

12:27.480 --> 12:33.240
where he showed like you can have it ingest Shakespeare and it will generate something

12:33.240 --> 12:35.440
that looks a bit like Shakespeare.

12:35.440 --> 12:43.080
I thought, okay, so if I do this at a much bigger scale using all of Wikipedia, what

12:43.200 --> 12:50.880
would it need to be able to do to finish a sentence in Wikipedia effectively, to do it

12:50.880 --> 12:52.560
quite accurately quite often?

12:52.560 --> 12:55.760
I thought, Jesus, it would actually have to know a lot about the world, you know, it

12:55.760 --> 12:59.160
would have to know that there is a world and that there are objects and that objects relate

12:59.160 --> 13:03.880
to each other through time and cause each other to react in ways and that causes proceed

13:03.880 --> 13:09.680
effects and that, you know, when there are animals and there are people and that people

13:09.760 --> 13:14.880
couldn't be in certain positions during certain timeframes and then you could, you know, all

13:14.880 --> 13:21.160
that together, you can then finish a sentence like this was signed into law in 2016 by US

13:21.160 --> 13:24.600
President X and it would fill in the gaps, you know.

13:24.600 --> 13:30.200
So that's why I tried to create a, what in those days was considered a big language model

13:30.200 --> 13:34.560
trained on the entirety on Wikipedia, which is that was a bit unheard of and my interest

13:34.560 --> 13:41.120
was not in, you know, just having a language model, my interest was in like, what latent

13:41.120 --> 13:51.480
capabilities would such a system have that would allow it to finish those kind of sentences?

13:51.480 --> 13:56.440
Because I was pretty sure based on our work with transfer learning and vision that I could

13:56.440 --> 14:02.280
then suck out those latent capabilities by transfer learning, you know, by fine-tuning

14:02.280 --> 14:04.320
it on a task data set or whatever.

14:04.320 --> 14:06.480
So we generated this three-step system.

14:06.480 --> 14:09.840
So step one was train a language model on a big corpus.

14:09.840 --> 14:15.720
Step two was fine-tune a language model on a more curated corpus and step three was further

14:15.720 --> 14:21.160
fine-tune that model on a task and of course that's why everybody still does today, right?

14:21.160 --> 14:29.000
That's what chat GPT is and so the first time I tried it within hours I had a new state

14:29.000 --> 14:34.200
of the art academic result on IMDB and I was like, holy shit, it does work.

14:34.760 --> 14:41.120
So you asked to what degree was this kind of like pushing against the established wisdom?

14:41.120 --> 14:46.160
Every way, like the reason it took me so long to try it was because I asked all my friends

14:46.160 --> 14:51.120
in NLP if this could work and everybody said no, it definitely won't work.

14:51.120 --> 14:55.400
It wasn't like, oh maybe, everybody was like, it definitely won't work.

14:55.400 --> 14:58.320
NLP is much more complicated than vision.

14:58.320 --> 15:01.760
Languages are much more vastly complicated to main, you know, and you've got problems

15:01.760 --> 15:05.560
like the grounding problem we know from like philosophy and theory of mind that it's actually

15:05.560 --> 15:07.560
impossible for it to work.

15:07.560 --> 15:10.720
So yeah, so don't waste your time.

15:10.720 --> 15:16.000
Jeremy, had people not tried because it was like too complicated to actually get the data

15:16.000 --> 15:20.200
and like set up the training or like were people just lazy and kind of like, hey, this

15:20.200 --> 15:21.200
is just not going to work.

15:21.200 --> 15:22.200
No, it was lazy.

15:22.200 --> 15:25.960
So like, so the person I thought at that time who, there were two people I thought at that

15:25.960 --> 15:31.280
time actually who were the strongest at language models were Stephen Merity and Alec Radford.

15:31.800 --> 15:38.520
And at the time I didn't know Alec, but I, after we had both, after I'd released ULM Fit and

15:38.520 --> 15:45.240
he had released GPT, I organized a chat for both of us with Kate Metz of the New York

15:45.240 --> 15:50.160
Times and Kate Metz answered, and Alec answered this question for Kate and Kate just like,

15:50.160 --> 15:53.880
so how did, you know, GPT come about?

15:53.880 --> 15:59.680
And he said, well, I was pretty sure that pre-training on a general large corpus wouldn't work.

15:59.680 --> 16:01.480
So I hadn't tried it.

16:01.480 --> 16:06.280
And then I read ULM Fit and turns out it did work.

16:06.280 --> 16:09.680
And so I did it, you know, bigger and it worked even better.

16:09.680 --> 16:15.160
And similar with Stephen, you know, I asked Stephen Merity, like, why don't we just find,

16:15.160 --> 16:19.120
you know, I'll take your AWDSTLM and like, trade it on all of Wikipedia and fine tune

16:19.120 --> 16:20.120
it.

16:20.120 --> 16:23.200
And he's kind of like, I don't think that's going to really lie.

16:23.720 --> 16:31.240
Like two years before, I did a very popular talk at KDD, the conference where everybody

16:31.240 --> 16:33.840
in NLP was in the audience.

16:33.840 --> 16:39.320
I recognized after faces, you know, and I told them all this, I'm sure transfer learning

16:39.320 --> 16:40.480
is the key.

16:40.480 --> 16:48.800
I'm sure ImageNet, you know, is going to be an NLP thing as well.

16:48.800 --> 16:53.920
And you know, everybody was interested and people asked me questions afterwards, but

16:53.920 --> 16:59.880
just, yeah, nobody followed up because everybody knew that it didn't work.

16:59.880 --> 17:08.520
I mean, even like, so we were scooped a little bit by Dye and Lee at Google.

17:08.520 --> 17:12.800
They had, I already, I didn't even realize this, it's just a bit embarrassing.

17:12.800 --> 17:17.840
They had already done a large language model and fine tuned it.

17:17.880 --> 17:23.160
But again, they didn't create a general purpose large language model on a general purpose

17:23.160 --> 17:24.160
corpus.

17:24.160 --> 17:28.400
They only ever tested a domain specific corpus.

17:28.400 --> 17:32.840
And I haven't spoken to Kwok actually about that, but I assume that the reason was the

17:32.840 --> 17:33.840
same.

17:33.840 --> 17:38.800
It probably just didn't occur to them that the general approach could work.

17:38.800 --> 17:43.680
So maybe it was that kind of 30 years of mulling over the, this whole Chinese room experiment

17:43.680 --> 17:46.960
that had convinced me that it probably would work.

17:46.960 --> 17:47.960
I don't know.

17:47.960 --> 17:48.960
Yeah.

17:48.960 --> 17:49.960
Interesting.

17:49.960 --> 17:54.680
I just dug up Alec announcement tweet from Tony 18.

17:54.680 --> 17:57.800
He said, inspired by Kobe, Elmo and Yola and Fit.

17:57.800 --> 18:02.320
We showed a single transformer language model can be fine tuned to a variety.

18:02.320 --> 18:07.600
It's interesting because, you know, today people think of the leader kind of like, kind of

18:07.600 --> 18:09.960
like the research lab pushing forward the field.

18:09.960 --> 18:13.240
What was that at the time, you know, like kind of like going back five years, people

18:13.320 --> 18:16.840
think of it as an overnight success, but obviously it took a while.

18:16.840 --> 18:17.840
Yeah.

18:17.840 --> 18:18.840
Yeah.

18:18.840 --> 18:19.840
No, I mean, absolutely.

18:19.840 --> 18:23.040
And I'll say like, it's interesting that it mentioned Elmo because in some ways that

18:23.040 --> 18:29.160
was kind of diametrically opposed to, to ULM fit, you know, there was these kind of like,

18:29.160 --> 18:34.120
so there was a lot of, there was a lot of activity at the same time as ULM fits release.

18:34.120 --> 18:41.480
So there was, so before it, as Brian McCann, I think at Salesforce had come out with this

18:41.520 --> 18:46.240
neat model that did a kind of multitask learning.

18:46.240 --> 18:50.800
But again, they didn't create a general fine tune language model first.

18:50.800 --> 18:56.040
There was Elmo, which I think was a little, you know, actually quite a few months after

18:56.040 --> 18:59.160
the first ULM fit example, I think.

19:00.040 --> 19:01.400
But yeah, there was a bit of this stuff going on.

19:01.400 --> 19:08.400
And the problem was everybody was doing, and particularly after GPT came out there and

19:08.400 --> 19:12.600
everybody wanted to focus on zero shot and few shot learning, you know, everybody hated

19:12.600 --> 19:14.840
fine tuning, everybody hated transfer learning.

19:14.840 --> 19:20.560
And like I literally did tours trying to get people to start doing transfer learning.

19:20.560 --> 19:27.040
And people, you know, nobody was interested, particularly after GPT showed such good results

19:27.040 --> 19:29.480
with zero shot and few shot learning.

19:29.480 --> 19:33.480
And so I actually feel like we kind of went backwards for years and, and not to be honest,

19:33.480 --> 19:40.160
I mean, I'm a bit sad about this now, but I kind of got so disappointed and dissuaded

19:40.160 --> 19:46.360
by like, it felt like these bigger lab, much bigger labs, you know, like fast AI had only

19:46.360 --> 19:52.880
ever been just me and Rachel were getting all of this attention for an approach I thought

19:52.880 --> 19:54.560
was the wrong way to do it.

19:54.560 --> 19:56.600
You know, I was convinced was the wrong way to do it.

19:56.600 --> 20:00.720
And so yeah, for years, people were really focused on getting better zero shot and few

20:00.720 --> 20:01.720
shot.

20:01.720 --> 20:06.960
And it wasn't until, you know, this key idea of like, well, let's take the ULM fit approach.

20:06.960 --> 20:13.400
But for step two, rather than fine tuning on a kind of a domain corpus, let's fine tune

20:13.400 --> 20:15.840
on an instruction corpus.

20:15.840 --> 20:20.720
And then in step three, rather than fine tuning on a reasonably specific task classification,

20:20.720 --> 20:25.320
let's fine tune on a, on a RLHF class classification.

20:25.320 --> 20:27.840
And so that was really, that was really key, you know.

20:27.840 --> 20:33.840
So I was kind of like out of the NLP field for a few years there, because yeah, it just

20:33.840 --> 20:42.360
felt like, I don't know, pushing uphill against this vast tide, which I was convinced was

20:42.360 --> 20:45.480
not the right direction, but he's going to listen to me, you know, because I, as you

20:45.480 --> 20:51.520
said, I don't have a PhD, not at a university, or at least it wasn't then I don't have a

20:51.520 --> 20:56.360
big set of computers to fine tune huge transformer models.

20:56.360 --> 20:58.540
So yeah, it was definitely difficult.

20:58.540 --> 20:59.540
It's always been hard.

20:59.540 --> 21:03.240
You know, it's always been hard, like I've always been somebody who does not want to

21:03.240 --> 21:11.240
build stuff on lots of big computers, because most people don't have lots of big computers.

21:11.240 --> 21:15.800
And I hate creating stuff that most people can't use, you know, and also stuff that's

21:15.800 --> 21:21.080
created on lots of big computers has always been like much more media friendly.

21:21.080 --> 21:25.440
So like, it might seem like a recent thing, but actually throughout my 30 years in data

21:25.440 --> 21:32.080
science, the attention's always been on, you know, the big iron results.

21:32.080 --> 21:36.400
So when I first started, everybody was talking about data warehouses, and it was all about

21:36.400 --> 21:42.440
teradata, and it'd be like, oh, this big bank has this huge room full of computers, and

21:42.440 --> 21:46.640
they have like terabytes of data available, you know, the press for button.

21:46.640 --> 21:52.720
And yeah, that's always what people want to talk about, what people want to write about.

21:52.720 --> 21:56.680
And then of course, students coming out of their PhDs and stuff, that's where they want

21:56.680 --> 21:59.920
to go work, because that's where they read about.

21:59.920 --> 22:07.680
And to me, it's a huge distraction, you know, because like I say, most people don't have

22:07.680 --> 22:15.960
unlimited compute, and I want to help most people, not the small subset of the most well-off

22:15.960 --> 22:16.960
people.

22:16.960 --> 22:17.960
Yeah.

22:17.960 --> 22:18.960
That's awesome.

22:19.120 --> 22:24.080
It's great to hear, you know, you do such a great job educating that a lot of times,

22:24.080 --> 22:28.400
you're not telling your own story, you know, so I love this conversation.

22:28.400 --> 22:33.240
And the other thing before we jump into FASTI, actually, you know, a lot of people that I

22:33.240 --> 22:37.440
know, they run across a new architecture and one other, like, I got to start a company

22:37.440 --> 22:41.320
and raise a bunch of money and do all of this stuff, instead you were like, I want everybody

22:41.320 --> 22:43.960
to have access to this.

22:43.960 --> 22:45.320
Why was that the case for you?

22:45.320 --> 22:49.640
Was it because you already had like a successful, you know, venture and like fast mail and you

22:49.640 --> 22:50.920
were more interested in that?

22:50.920 --> 22:51.920
What was the reasoning?

22:51.920 --> 22:54.320
That's a really good question.

22:54.320 --> 22:58.760
So I guess the answer is yes, that's the reason why.

22:58.760 --> 23:05.280
So when I was a teenager, I thought it would be really cool to like have my own company.

23:05.280 --> 23:08.520
You know, I didn't know the word startup, I didn't know the word entrepreneur, I didn't

23:08.520 --> 23:12.760
know the word VC, and I didn't really know what any of those things were really until

23:12.800 --> 23:16.000
after we started Kaggle, to be honest, even though I'd started to what would now call

23:16.000 --> 23:22.440
startups, I just thought they were just small businesses, you know, they were just companies.

23:22.440 --> 23:25.040
So yeah, so those two companies were fast mail and optimal decisions.

23:25.040 --> 23:31.360
Fast mail was the first kind of synchronized email provider for non-businesses, so something

23:31.360 --> 23:37.880
you can get your same email at home on your laptop that were on your phone, whatever.

23:37.880 --> 23:43.920
And then optimal decisions invented a new approach to insurance pricing, so they called

23:43.920 --> 23:46.400
profit optimized insurance pricing.

23:46.400 --> 23:55.800
So I saw both of those companies, you know, after 10 years, and at that point I had achieved

23:55.800 --> 24:01.640
the thing that as a teenager I wanted to do, you know, it took a lot longer than it should

24:01.640 --> 24:04.520
have because I spent way longer in management consulting than I should have because I got

24:04.560 --> 24:09.360
caught up in that stupid rat race, but you know, eventually I got there and I remember

24:09.360 --> 24:14.680
my mom saying to me, oh, you must be so proud, you know, because she remembered, my dreams

24:14.680 --> 24:17.120
is like, you've done it.

24:17.120 --> 24:23.520
And I kind of reflected and I was like, I'm not, I'm not proud at all, you know, like

24:23.520 --> 24:27.520
people quite liked fast mail, you know, it's quite nice to have synchronized email, it

24:27.520 --> 24:33.360
probably would have happened anyway, yeah, I'm certainly not proud that I've helped

24:33.400 --> 24:37.800
some insurance companies suck more money out of their customers.

24:37.800 --> 24:42.640
Yeah, no, I'm not proud, you know, it's this actually, I haven't really helped the world

24:42.640 --> 24:47.280
very much, you know, maybe in the insurance case, I've made it a little bit worse.

24:47.280 --> 24:48.880
I don't know.

24:48.880 --> 24:57.880
So yeah, I was determined to not waste more years of my life doing things, working hard

24:57.920 --> 25:03.400
to do things which I could not be reasonably sure would have a lot of value.

25:03.400 --> 25:08.680
So, you know, I took some time off, I wasn't sure if I'd ever work again, actually, I didn't

25:08.680 --> 25:14.320
particularly want to, because it felt like, yeah, it felt like such a disappointment.

25:14.320 --> 25:17.960
And but you know, and I didn't need to, I had enough money, like I wasn't super rich,

25:17.960 --> 25:20.400
but I had enough money, I didn't need to work.

25:20.400 --> 25:25.400
And I certainly recognized that amongst the other people, I knew who had enough money

25:25.440 --> 25:29.800
that they didn't need to work, they all worked ridiculously hard, you know, and constantly

25:29.800 --> 25:32.440
put themselves in extremely stressful situations.

25:32.440 --> 25:39.600
And I thought, I don't want to be one of those idiots who's tied to, you know, buying a bigger

25:39.600 --> 25:44.520
plane than the next guy or whatever, you know, Kaggle came along and I mainly kind of did

25:44.520 --> 25:48.640
that just because it was fun and interesting to hang out with interesting people.

25:49.400 --> 25:57.200
But, you know, with fast AI in particular, you know, Rachel and I had a very explicit, you

25:57.200 --> 26:02.680
know, long series of conversations over a long period of time about like, well, how can we be

26:02.680 --> 26:09.600
the most helpful to society as a whole, and particularly to those people who maybe need

26:09.600 --> 26:10.720
more help, you know.

26:11.240 --> 26:18.000
And so we definitely saw the world going in a potentially pretty dystopian direction, if

26:18.000 --> 26:23.440
the world's most powerful technology was controlled by a small group of elites.

26:25.440 --> 26:29.920
So we thought, yeah, we should focus on trying to help that not happen.

26:31.000 --> 26:33.360
You know, sadly, it looks like it still is likely to happen.

26:33.360 --> 26:38.280
But I mean, I feel like we've, we've helped make it a little bit less likely.

26:38.320 --> 26:39.720
So we've done our best.

26:39.720 --> 26:41.520
You've shown that it's possible.

26:41.640 --> 26:49.280
And I think, I think your constant advocacy, your courses, your research that you publish,

26:49.280 --> 26:55.760
you know, just the other day you published a signing on, you know, learning that I think

26:55.760 --> 26:58.800
is still something that people are still talking about quite a lot.

26:59.040 --> 27:05.000
I think that that is the origin story of a lot of people who are going to be, you know,

27:05.000 --> 27:08.240
little Jeremy Howard's sort of in your mission with, you know, you don't have to do

27:08.240 --> 27:09.640
everything by yourself is what I'm saying.

27:09.680 --> 27:10.880
Definitely, definitely.

27:10.880 --> 27:15.440
You know, that was a, that was a big takeaway from like, and Lydic was that Lydic, it

27:15.440 --> 27:17.800
definitely felt like we had to do everything ourselves.

27:17.920 --> 27:20.040
And I kind of, I wanted to solve medicine.

27:20.160 --> 27:22.720
I'll say, yeah, okay, solving medicine is actually quite difficult.

27:22.720 --> 27:25.400
And I can't do it on my own.

27:25.400 --> 27:27.880
And there's a lot of other things I'd like to solve, and I can't do those either.

27:27.880 --> 27:34.000
So that was, that was definitely the other piece was like, yeah, you know, can we create

27:34.000 --> 27:41.320
an army of passionate domain experts who can change their little part of the world.

27:41.800 --> 27:42.760
And that's definitely happened.

27:42.760 --> 27:50.120
Like I find nowadays, at least half the time, probably quite a bit more, that I get in

27:50.120 --> 27:53.920
contact with somebody who's done really interesting work in some domain.

27:54.160 --> 27:57.320
Most of the time, I'd say, they say, yeah, I got my start with fast AI.

27:58.360 --> 28:00.400
So it's definitely, I can, I can see that.

28:00.400 --> 28:06.760
And I also know from talking to folks at places like Amazon and Adobe and stuff, which, you

28:06.760 --> 28:07.960
know, there's lots of alumni there.

28:07.960 --> 28:11.360
And they say, oh my God, I got here in like half of the people who are fast AI alumni.

28:12.240 --> 28:13.840
So it's fantastic.

28:14.640 --> 28:18.600
Yeah, actually, Andre Capati grabbed me when I saw him at Europe's a few years ago.

28:18.600 --> 28:21.320
And he's like, I have to tell you thanks to the fast AI courses.

28:21.320 --> 28:24.720
When people come to Tesla and they need to know more about deep learning, we always

28:24.720 --> 28:25.720
send them to your course.

28:26.440 --> 28:29.280
And the OpenAI scholars program was doing the same thing.

28:29.640 --> 28:37.480
So it's kind of like, yeah, it's had a surprising impact, you know, that's just one

28:37.480 --> 28:43.320
of like three things we do is the course, you know, and it's, it's, it's only ever

28:43.320 --> 28:47.160
been at most two people, either me and Rachel or me and Silver.

28:47.200 --> 28:48.160
Nowadays, it's just me.

28:49.200 --> 28:53.560
So, yeah, I think it shows you don't necessarily need a huge amount of money and a

28:53.560 --> 28:56.840
huge team of people to, to make an impact.

28:57.760 --> 28:58.040
Yeah.

28:58.920 --> 29:04.600
So just to reintroduce fast AI for people who may not have dived into it much.

29:04.960 --> 29:07.080
There is the courses that you do.

29:07.440 --> 29:12.160
There is the library that is, that is very well loved.

29:12.160 --> 29:17.200
And I kind of think of it as a nicer layer on top of PyTorch that people should

29:17.200 --> 29:21.080
start with by default and use it as the basis for a lot of your courses.

29:22.280 --> 29:26.760
And then you have, you have like NB Dev, which I don't know, is that the third

29:26.760 --> 29:32.520
one? Oh, so the three areas were research, software, and, and courses.

29:32.560 --> 29:33.000
Oh, sorry.

29:33.000 --> 29:33.520
I was going by.

29:33.520 --> 29:41.960
So then in software, you know, fast AI is the main thing, but NB Dev is not far

29:41.960 --> 29:49.240
behind, but then there's also things like Fastcore, GHAPI, I mean, dozens of open

29:49.240 --> 29:55.360
source projects that I've created and some of them have been pretty popular.

29:55.360 --> 29:57.320
And some of them are still a little bit hidden.

29:57.320 --> 30:01.040
Actually, I should, some of them I should try to do a better job of telling people

30:01.040 --> 30:02.040
about. What are you, what are you thinking about?

30:02.600 --> 30:02.840
Yeah.

30:02.840 --> 30:07.840
What, what's on this little things like, for example, for working with EC2 and AWS,

30:07.840 --> 30:12.360
I created a fast EC2 library, which I think is like way more convenient and nice

30:12.360 --> 30:13.880
to use than anything else out there.

30:14.400 --> 30:17.800
And it's literally got a whole autocomplete dynamic autocomplete that works

30:17.800 --> 30:21.440
both on the command line and in notebooks, sort of like autocomplete your

30:21.440 --> 30:23.920
instance names and everything like that.

30:24.120 --> 30:25.480
You know, just little things like that.

30:25.480 --> 30:32.400
I try to make like, when I work with some domain, I try to make it like, I want

30:32.400 --> 30:35.320
to make it as enjoyable as possible for me to do that.

30:35.640 --> 30:40.680
So I always try to kind of like, like with GHAPI, for example, I think that

30:40.680 --> 30:45.640
GitHub API is incredibly powerful, but I didn't find it good to work with

30:45.640 --> 30:47.680
because I didn't particularly like the libraries that are out there.

30:47.680 --> 30:52.920
So like GHAPI, like Fast EC2, it like autocompletes both at the command

30:52.920 --> 30:57.800
line or in a notebook or whatever, like literally the entire GitHub API.

30:59.680 --> 31:03.520
The entire thing is like, I think it's like less than a hundred K of code

31:03.640 --> 31:09.480
because it actually, as far as I know, the only one that grabs it directly

31:09.480 --> 31:13.440
from the official open API spec that GitHub produces.

31:14.160 --> 31:21.480
And like, if you're in GitHub and you just type an API, you know, autocomplete

31:21.720 --> 31:28.840
API method and it enter, it prints out the docs or the six brief docs and

31:28.840 --> 31:32.640
then gives you a link to the actual documentation page, you know, GitHub

31:32.640 --> 31:36.480
actions I can write now in Python, which is just so much easier than writing

31:36.480 --> 31:38.760
them in typescript and stuff.

31:38.760 --> 31:40.840
So, you know, just little things like that.

31:41.120 --> 31:44.800
I think that's a approach that more, I wish more developers took to publish

31:45.080 --> 31:46.200
some of the work along the way.

31:47.200 --> 31:51.000
You describe the third arm of Fast EIS research.

31:51.120 --> 31:52.960
It's not something I see often.

31:52.960 --> 31:58.080
Obviously, you do do some research and how do you run your research?

31:58.240 --> 31:59.440
What are your research interests?

31:59.840 --> 32:00.120
Yeah.

32:00.120 --> 32:03.240
So research is what I spend the vast majority of my time on.

32:04.120 --> 32:11.400
And the artifacts that come out of that are largely software and courses, you

32:11.400 --> 32:15.640
know, so to me, the main artifact shouldn't be papers because papers are

32:15.640 --> 32:18.120
things read by a small exclusive group of people.

32:18.200 --> 32:22.760
You know, to me, the main artifacts should be like something teaching you

32:22.760 --> 32:25.800
people, here's how to use this insight and here's software you can use that

32:25.920 --> 32:27.160
builds it in.

32:29.320 --> 32:33.000
So I think I've only ever done three first person papers in my life, you know,

32:33.160 --> 32:37.320
and they were, and none of those are ones I wanted to do, you know, they were all

32:37.320 --> 32:41.520
once like, so one was ULM fit where Sebastian Ruda reached out to me after

32:41.520 --> 32:45.160
seeing the course and said, like, you have to publish this as a paper, you know.

32:45.720 --> 32:48.160
And he said, I'll write it.

32:50.280 --> 32:51.320
He said, I want to write it.

32:51.320 --> 32:53.600
Cause if I do, I can put it on my PhD and that would be great.

32:53.600 --> 32:57.280
And it's like, okay, well, I want to help you with your PhD and that sounds great.

32:57.280 --> 33:03.520
So like, you know, one was the masks paper, which just had to exist and nobody

33:03.520 --> 33:04.640
else was writing it.

33:04.760 --> 33:13.920
And then the third was the fast AI library paper, which again, somebody reached

33:13.920 --> 33:16.440
out and said, please, please write this.

33:16.440 --> 33:21.280
We will waive the fee for the journal and everything and actually help you get it

33:21.280 --> 33:22.440
through publishing and stuff.

33:22.480 --> 33:26.520
So yeah, so I don't, other than that, I've never written a first author paper.

33:27.200 --> 33:32.960
So the research is like, well, so for example, you know, Don Bench was a

33:32.960 --> 33:36.240
competition which Stanford ran a few years ago.

33:38.000 --> 33:41.880
It was kind of the first big competition of like, who couldn't train

33:41.960 --> 33:44.960
neural nets the fastest rather than the most accurate.

33:45.960 --> 33:51.760
And specifically it was who couldn't train ImageNet the fastest.

33:52.760 --> 33:57.000
And this was like one of these things where it was created by necessity.

33:57.360 --> 33:59.600
So Google had just released their TPUs.

34:00.200 --> 34:04.680
And so I heard from my friends at Google that they had put together this big team

34:04.920 --> 34:09.800
to smash Don Bench so that they could prove to people that they had to use

34:10.000 --> 34:13.240
Google Cloud and use their TPUs and show who could their TPUs were.

34:14.040 --> 34:17.200
And we kind of thought, oh, shit, this would be a disaster if they do that

34:17.200 --> 34:20.120
because then everybody's going to be like, oh, deep learning is not accessible.

34:20.520 --> 34:23.240
You know, to actually be good at it, you have to be Google and you have to

34:23.240 --> 34:24.680
use special silicon and so.

34:25.040 --> 34:29.360
So, you know, we, we only found out about this 10 days before the competition finished.

34:30.120 --> 34:35.320
But, you know, we basically got together an emergency bunch of our students and

34:35.360 --> 34:41.640
Rachel and I and sat for the next 10 days and just tried to crunch through and

34:42.520 --> 34:47.240
tried to use all of our best ideas that had come from our research.

34:48.320 --> 34:52.400
That's a particularly progressive resizing, which is basically train mainly on small things.

34:53.480 --> 34:57.200
Train on non-square things, you know, stuff like that.

34:57.600 --> 35:00.160
And so, yeah, we ended up winning.

35:00.960 --> 35:01.480
Thank God.

35:02.080 --> 35:06.480
And so, you know, we turned it around from being like, oh, shit, you know, this is

35:06.480 --> 35:09.440
going to show that you have to be Google and have TPUs to being like, oh, my God,

35:09.440 --> 35:11.720
even the little guy can do deep learning.

35:13.560 --> 35:18.360
So that's an example of the kind of like research artifacts we do.

35:18.840 --> 35:24.320
And yeah, so all of my research is always, how do we do more with less, you know,

35:24.320 --> 35:29.840
so how do we get better results with less data, with less compute, with less complexity.

35:30.520 --> 35:34.480
With less education, you know, stuff like that.

35:34.480 --> 35:37.560
So your LLM fits obviously a good example of that.

35:38.960 --> 35:42.680
And most recently you published, can LLMs learn from a single example?

35:43.960 --> 35:46.080
Maybe could you tell the story a little bit behind that?

35:46.080 --> 35:50.160
And maybe that goes a little bit too far into the learning on very low resource.

35:51.920 --> 35:52.760
The literature.

35:53.200 --> 35:53.880
Yeah.

35:54.520 --> 35:54.880
Yeah.

35:54.880 --> 36:01.320
So me and my friend, John O'Whittaker, basically had been playing around with

36:01.320 --> 36:05.240
this fun Kaggle competition, which is actually still running as we speak,

36:05.240 --> 36:13.280
which is, can you create a model which can answer multiple choice questions

36:13.280 --> 36:15.240
about anything that's in Wikipedia?

36:15.920 --> 36:22.400
And the thing that makes it interesting is that your model has to run on Kaggle

36:22.760 --> 36:26.040
within nine hours and Kaggle is very, very limited.

36:26.040 --> 36:31.680
So you've only got 14 gig RAM, only two CPUs and a small, very old GPU.

36:33.680 --> 36:36.720
So this is cool, you know, if you can do well at this, and this is a good example

36:36.720 --> 36:38.240
of like, oh, you can do more with less.

36:39.600 --> 36:45.640
So yeah, John O and I were playing around with fine tuning, of course, transfer

36:45.640 --> 36:48.120
learning, pre-trained language models.

36:49.160 --> 36:55.000
And we saw this like, so we always, you know, plot our losses as we go.

36:55.040 --> 36:58.120
So here's another thing we created, we actually, Sylvain Gougir, when he worked

36:58.120 --> 37:02.760
with us created called Fast Progress, which is kind of like TQEDM, but we think

37:02.760 --> 37:03.200
a lot better.

37:03.600 --> 37:07.480
So you look at our Fast Progress curves, and they kind of go down, down, down,

37:07.480 --> 37:09.720
down, down, down, a little bit, a little bit, a little bit, and then suddenly

37:09.720 --> 37:11.400
go clunk, and they drop.

37:12.000 --> 37:15.000
And then down, down, down, down, a little bit, and then suddenly clunk, they drop.

37:15.000 --> 37:16.560
We're like, what the hell?

37:16.560 --> 37:19.800
These clunks are occurring at the end of each epoch.

37:20.560 --> 37:27.040
So normally in deep learning, this would be, you know, I've seen this before,

37:27.040 --> 37:28.040
it's always been a bug.

37:28.680 --> 37:31.880
It's always turned out that like, oh, we accidentally forgot to turn on

37:31.880 --> 37:33.680
eval mode during the validation set.

37:33.680 --> 37:39.440
So I was actually learning then, or, oh, we accidentally were kept letting moving

37:39.440 --> 37:41.480
average statistics throughout the epoch.

37:41.520 --> 37:44.360
So, you know, for it's recent, the moving average or whatever.

37:44.360 --> 37:46.680
And so we were using hugging face trainer.

37:47.280 --> 37:50.680
So, you know, I did not give my friends at hugging face the benefit

37:50.680 --> 37:51.200
of the doubt.

37:51.200 --> 37:56.040
I thought, oh, they fucked up hugging face trainer, you know, idiots.

37:56.160 --> 37:58.560
Well, you'll use the faster trainer instead.

37:58.560 --> 37:59.840
So we switched over to learner.

37:59.840 --> 38:01.200
We still saw the clunks.

38:01.680 --> 38:08.120
And, you know, that's, yeah, it shouldn't really happen because semantically

38:08.120 --> 38:13.080
speaking in the epoch isn't like, it's not a thing, you know, like nothing

38:13.080 --> 38:17.240
happens or nothing's meant to happen when you go from ending one epoch to

38:17.240 --> 38:18.240
starting the next one.

38:20.480 --> 38:21.560
So there shouldn't be a clunk.

38:22.400 --> 38:25.280
You know, so I kind of asked around on the open source discords.

38:25.560 --> 38:28.880
That's like, what's going on here?

38:29.200 --> 38:31.600
And everybody was just like, oh, that's just what, that's just what these

38:31.600 --> 38:32.560
training curves look like.

38:32.720 --> 38:33.680
Those all look like that.

38:33.880 --> 38:34.520
Don't worry about it.

38:35.080 --> 38:37.360
That's like, oh, are you all using trainer?

38:37.520 --> 38:38.200
Yes.

38:38.320 --> 38:40.440
Oh, well, there must be some buck with training.

38:40.440 --> 38:43.160
And it's like, well, we also saw it in learner and somebody else is like, no,

38:43.160 --> 38:44.120
we've got our own trainer.

38:44.120 --> 38:44.880
We get it as well.

38:45.320 --> 38:46.240
They're just like, don't worry about it.

38:46.240 --> 38:47.200
It's just something we see.

38:47.920 --> 38:48.560
It's just normal.

38:49.040 --> 38:50.080
I can't do that.

38:50.080 --> 38:54.640
I can't just be like, here's something that's like in the previous 30 years of

38:54.640 --> 38:56.520
neural networks, nobody ever saw it.

38:56.720 --> 38:57.960
And now suddenly we see it.

38:59.160 --> 39:00.120
So don't worry about it.

39:00.440 --> 39:02.240
I like, I just, I have to know why.

39:02.520 --> 39:06.440
Can I clarify this is, was everyone that you're talking to, were they all

39:06.440 --> 39:08.760
seeing it for the same data set or in different data sets?

39:08.880 --> 39:11.960
Data, different data sets, different trainers.

39:11.960 --> 39:14.360
They're just like, no, this is just, this is just what it looks like when

39:14.360 --> 39:15.640
you fine tune language models.

39:15.640 --> 39:16.320
Don't worry about it.

39:16.560 --> 39:21.360
You know, as I say, I hadn't seen it before, but I'd been kind of like, as I

39:21.360 --> 39:24.880
say, I, you know, I kept working on them for a couple of years after ULM fit.

39:24.880 --> 39:28.400
And then I kind of moved on to other things, partly out of frustration.

39:28.640 --> 39:35.040
So I hadn't been fine tuning, you know, um, I mean, Lamar's only been out for a

39:35.040 --> 39:35.760
few months, right?

39:35.760 --> 39:39.480
But I, I, I, I, I wasn't one of those people who jumped straight into it, you

39:39.480 --> 39:45.000
know, so I was relatively new to the kind of Lamar fine tuning world, or else

39:45.560 --> 39:49.400
these guys had been, you know, doing it since day one.

39:50.320 --> 39:53.080
Um, it's only a few months ago, but it's still quite a bit of time.

39:53.080 --> 39:55.880
So, so yeah, they're just like, no, this is all what we see.

39:56.280 --> 39:56.960
Don't worry about it.

39:57.960 --> 40:01.880
So yeah, I, I've got a very kind of like, I don't know, I've just got this

40:01.880 --> 40:03.680
brain where I have to know why things are.

40:04.200 --> 40:07.160
And so I kind of, I asked people like, well, why, why do you think it's happening?

40:07.160 --> 40:10.280
And they'd be like, oh, we're pretty obviously, cause it's like memorized the

40:10.280 --> 40:10.800
data set.

40:11.920 --> 40:14.520
It's just like, it can't be right.

40:14.520 --> 40:15.640
It's only seen it once.

40:15.680 --> 40:16.480
Like, look at this.

40:16.480 --> 40:24.040
The, the losses dropped by 0.3, 0.3, which is like, basically it knows the answer.

40:25.040 --> 40:30.000
Um, they're like, no, no, it's just, it is, it's just memorized the data set.

40:30.000 --> 40:30.880
So yeah.

40:30.880 --> 40:35.680
So look, John, when I did not discover this and John O and I did not come up with a

40:35.680 --> 40:39.040
hypothesis, you know, I guess we were just the ones, I guess, who had been around

40:39.040 --> 40:42.560
for long enough to recognize that like this, this isn't how it's meant to work.

40:42.920 --> 40:47.840
And so we, you know, and so we went back and like, okay, let's just run some

40:47.840 --> 40:50.840
experiments, you know, cause nobody since we've actually published anything about

40:50.840 --> 40:53.960
this, um, well, not quite true.

40:53.960 --> 40:56.480
Some people have published things, but nobody ever actually stepped back and

40:56.480 --> 40:59.840
said like, what the hell, you know, how can this be possible?

40:59.840 --> 41:00.600
Is it possible?

41:00.600 --> 41:01.480
Is this what's happening?

41:01.840 --> 41:05.520
And so yeah, we created a bunch of experiments where we basically predicted

41:05.520 --> 41:06.080
ahead of time.

41:06.080 --> 41:08.960
It's like, okay, if this hypothesis is correct, that it's memorized in the

41:08.960 --> 41:13.000
training set, then we ought to see blah under conditions, blah, but not only these

41:13.000 --> 41:13.600
conditions.

41:14.240 --> 41:17.280
And so we ran a bunch of experiments and all of them supported the hypothesis

41:18.280 --> 41:21.560
that it was memorizing the data set in a single thing at once.

41:22.120 --> 41:30.600
Um, and it's a pretty big data set, you know, um, which in hindsight, it's not

41:30.600 --> 41:35.240
totally surprising because the theory, remember, of the ULM fit theory was like

41:35.240 --> 41:40.240
what's kind of creating all these latent capabilities to make it easier for it to

41:40.240 --> 41:41.360
predict the next token.

41:41.880 --> 41:46.320
So if it's got all this kind of latent capability, it ought to also be really

41:46.400 --> 41:51.080
good at compressing new tokens because it can immediately recognize that it's

41:51.080 --> 41:52.680
like, oh, that's just a version of this.

41:53.920 --> 42:01.880
So it's, it's not so crazy, you know, but it is, um, it requires us to rethink

42:01.880 --> 42:06.760
everything because like, and nobody knows like, okay, so how do we fine tune

42:06.760 --> 42:07.320
these things?

42:07.320 --> 42:09.560
Because like, it doesn't even matter.

42:10.240 --> 42:11.320
Like maybe it's fine.

42:11.680 --> 42:14.760
Like maybe it's fine that it's memorized the data set after one go and you do a

42:14.760 --> 42:20.200
second go and okay, the validation loss is terrible because it's now really

42:20.200 --> 42:21.040
overconfident.

42:21.920 --> 42:22.480
That's fine.

42:22.600 --> 42:25.960
Don't, you know, don't keep telling people, don't track validation loss,

42:25.960 --> 42:29.920
track validation accuracy, um, because at least that, that will still be useful.

42:30.520 --> 42:34.400
Um, there's another thing that's got lost since ULM fit, nobody tracks accuracy

42:34.400 --> 42:35.600
of language models anymore.

42:36.400 --> 42:41.000
Um, but you know, it'll still keep learning and it does, it does keep improving.

42:41.240 --> 42:43.880
But is it worse?

42:44.240 --> 42:48.320
You know, like, is it like now that it's kind of memorized it, it's probably

42:48.320 --> 42:53.960
getting a less strong signal, you know, um, I don't know.

42:54.200 --> 42:56.720
So I still don't know how to fine tune language models properly.

42:56.720 --> 43:00.760
And I haven't found anybody who feels like they do, like nobody really knows

43:00.760 --> 43:05.760
whether this memorization thing is, it's probably a feature in some ways.

43:05.760 --> 43:07.760
There's probably some things that you can do usefully with it.

43:07.920 --> 43:13.560
It's probably, yeah, I have a feeling it's messing up training dynamics as well.

43:14.360 --> 43:17.480
It doesn't come at the cost of catastrophic forgetting as well, right?

43:17.480 --> 43:18.920
Like, which is the other side of the coin.

43:19.280 --> 43:25.640
Um, it does, um, to some extent, like, we know it does like look at CodeLama,

43:25.640 --> 43:26.280
for example.

43:26.280 --> 43:31.720
So CodeLama was a, I think it was like a 500 billion token fine tuning of

43:31.720 --> 43:36.800
CodeLama to using code and also pros about code that Meta did.

43:37.440 --> 43:43.040
And, um, honestly, they kind of blew it because CodeLama is good at coding,

43:43.040 --> 43:44.160
but it's bad at everything else.

43:44.680 --> 43:45.720
You know, and it used to be good.

43:46.120 --> 43:46.400
Yeah.

43:46.400 --> 43:50.320
I was pretty sure it was like, before they released it at me and lots of people

43:50.320 --> 43:53.240
in the open source discords were like, Oh my God, you know, we know this is

43:53.240 --> 43:53.600
coming.

43:53.600 --> 43:54.800
You're not going to say it's coming.

43:54.800 --> 43:58.800
I, I hope they kept at least like 50% long code data because otherwise

43:58.800 --> 43:59.960
it's going to forget everything else.

44:00.440 --> 44:07.000
And they didn't only like 0.3 of their 0.3% of their epochs were non-code data.

44:07.400 --> 44:08.840
So I did it, forgot everything else.

44:08.880 --> 44:12.040
So now it's good at code and it's bad at everything else.

44:12.840 --> 44:14.640
So we definitely have catastrophic forgetting.

44:14.640 --> 44:15.480
It's fixable.

44:15.520 --> 44:20.880
Just somebody has to do, you know, somebody, somebody has to spend their time

44:20.880 --> 44:24.280
training a model on a, a good mix of data.

44:24.360 --> 44:25.320
Like, so, okay.

44:25.320 --> 44:25.960
So here's the thing.

44:26.960 --> 44:32.840
Um, even though I originally created the three step approach that everybody

44:32.840 --> 44:36.560
now does, my view is it's actually wrong and we shouldn't use it.

44:37.000 --> 44:45.520
Um, um, and that's because people are using it in a way different to why I

44:45.520 --> 44:46.040
created it.

44:46.040 --> 44:50.560
You know, I created it thinking that the tasks specific models would be more

44:50.560 --> 44:54.760
specific, you know, it's like, Oh, this is like a sentiment classifier.

44:54.880 --> 45:00.960
This is an example of a task, you know, but the tasks now are like a, um, you

45:00.960 --> 45:04.040
know, RLHF, which is basically like answer questions that make people feel

45:04.040 --> 45:05.040
happy about your answer.

45:05.320 --> 45:09.280
So that's a much more general task and it's a really cool approach.

45:09.520 --> 45:14.880
And so we see, for example, RLHF also breaks models.

45:14.880 --> 45:20.880
Like, you know, like GPT for RLHDEFT, we know from kind of the, the work that

45:20.880 --> 45:25.720
Microsoft did, you know, the pre, the, the earlier less aligned version was better.

45:26.920 --> 45:29.840
Um, and these are all kind of examples of catastrophic forgetting.

45:30.200 --> 45:36.960
And so to me, the right way to do this is to fine-tune language models is to

45:36.960 --> 45:38.720
actually throw away the idea of fine-tuning.

45:38.880 --> 45:39.640
There's no such thing.

45:40.360 --> 45:41.800
There's only continued pre-training.

45:42.640 --> 45:47.280
Uh, and pre-training is something where from the very start, you try to include

45:47.280 --> 45:51.120
all the kinds of data that you care about, all the kinds of problems that you

45:51.120 --> 45:57.760
care about, instructions, exercises, code, general purpose document completion,

45:58.760 --> 45:59.200
whatever.

46:01.040 --> 46:06.440
And then as you train, you gradually curate that, you know, you gradually

46:06.440 --> 46:09.960
make that higher and higher quality and more and more specific to the kinds of

46:09.960 --> 46:10.840
tasks you want it to do.

46:12.080 --> 46:14.600
Um, but you never throw away any data.

46:14.840 --> 46:20.160
You always keep all of the data types there in reasonably high quantities.

46:20.640 --> 46:25.720
Um, you know, maybe the quality filter, you stop training on low-quality data.

46:26.240 --> 46:29.120
Cause that's probably fine to forget how to write badly, maybe.

46:29.840 --> 46:35.560
Um, so yeah, that's now my view is I think ULM fit is the wrong approach.

46:36.080 --> 46:41.120
Um, and that's why we're seeing a lot of these, uh, you know, so-called alignment

46:41.120 --> 46:45.720
tax and this view of like, oh, a model can't both code and do other things.

46:45.840 --> 46:47.920
You know, I think it's actually cause people are training them wrong.

46:49.240 --> 46:53.720
Well, I think you have a clear anti-laziness approach.

46:53.840 --> 46:57.760
I think other people are not as, uh, good hearted, you know, they're like,

46:57.760 --> 46:59.720
Hey, they told me this thing works.

46:59.880 --> 47:03.240
And if I release a model this way, people will appreciate it.

47:03.240 --> 47:06.440
I'll get promoted and I'll kind of make, make more money.

47:06.920 --> 47:07.760
Oh, absolutely.

47:08.280 --> 47:08.600
Yeah.

47:08.600 --> 47:09.440
And it's not just money.

47:09.440 --> 47:13.440
It's like, this is how citations work most, most badly, you know, so if you

47:13.440 --> 47:17.560
want to get cited, you need to write a paper that people in your field recognize

47:17.560 --> 47:21.640
as an advancement on things that we know are good.

47:22.160 --> 47:24.320
And so we've seen this happen again and again.

47:24.400 --> 47:29.200
So like I say, like zero shot and a few shot learning, everybody was writing about

47:29.200 --> 47:33.360
that or, you know, with, um, image generation, every, everybody just was writing

47:33.360 --> 47:37.880
about GANs, you know, and I was trying to say like, no, GANs are not the right approach.

47:37.880 --> 47:41.080
You know, when I showed again through research that we demonstrated in our

47:41.080 --> 47:47.400
videos, that you can do better than GANs much faster and with much less data.

47:48.320 --> 47:52.400
And nobody cared because again, like if you want to get published, you rewrite

47:52.400 --> 47:57.560
a GAN paper that slightly improves this part of GANs and this tiny field, you'll,

47:58.520 --> 48:03.600
you'll get published, you know, so it's, yeah, it's not set up for real innovation.

48:04.160 --> 48:12.120
Um, it's, you know, it's, again, it's really helpful for me, you know, have my own

48:12.120 --> 48:16.240
research lab with nobody telling me what to do and I don't even publish.

48:16.240 --> 48:18.040
So it doesn't matter if I get citations.

48:18.400 --> 48:21.360
So I just write what I think is actually matters.

48:21.800 --> 48:26.680
Um, I wish there was, and you know, it actually places like open AI, you know,

48:26.680 --> 48:28.520
the researchers there can do that as well.

48:29.160 --> 48:33.160
It's a shame, you know, I wish there was more academic, open,

48:33.280 --> 48:38.200
venues in which people can focus on like genuine innovation.

48:38.720 --> 48:44.000
Uh, Twitter, which is, uh, unironically has, has become a little bit of that form.

48:44.360 --> 48:47.880
Uh, I wanted to follow up on one thing that you mentioned, uh, which is that

48:47.880 --> 48:50.360
you checked around the open source discords.

48:50.720 --> 48:54.800
Uh, I don't know if it's, uh, to, uh, I don't know if it's a kosher to ask

48:54.800 --> 48:58.840
like what discords are lively, uh, or useful right now.

48:59.040 --> 49:03.560
Um, I think that something I definitely felt like I missed out on was the early

49:03.560 --> 49:06.840
days of Luther AI, which where, which is a fair hot bit.

49:07.200 --> 49:10.760
And, uh, you know, like what is the new Luther and you were, you actually

49:10.760 --> 49:14.000
shouted out the alignment lab AI discord in your blog posts.

49:14.000 --> 49:16.880
And it was the first time I even knew, like I saw them on Twitter and never

49:16.880 --> 49:18.840
knew they had a discord, never knew that there was actually

49:18.840 --> 49:22.240
substantive discussions going on in there and that you were an active member of it.

49:22.760 --> 49:23.040
Okay.

49:23.040 --> 49:23.280
Yeah.

49:23.320 --> 49:26.160
And then even then, if you do know about that, you go there, it'll look like

49:26.160 --> 49:27.000
it's totally dead.

49:27.400 --> 49:30.680
And that's because unfortunately, nearly all the discords, nearly all of

49:30.680 --> 49:36.320
the conversation happens in private channels, you know, um, and how does

49:36.320 --> 49:38.560
someone get into that world?

49:38.560 --> 49:42.720
Cause it's obviously very, very, um, instructive, right?

49:42.880 --> 49:46.280
You could just come to the first day I discord, which I'll be honest with you,

49:46.280 --> 49:52.000
it's less bustling than some of the others, but it's not terrible.

49:52.360 --> 49:56.760
And so like, at least, you know, to be fair, one of Emma's bustling

49:56.760 --> 49:57.720
channels is private.

49:59.080 --> 49:59.400
I guess.

50:01.120 --> 50:02.080
So I'm just thinking, why is that?

50:02.080 --> 50:03.560
It's just a nature of quality discussion, right?

50:04.040 --> 50:07.880
Yeah, I guess when I think about it, like, I didn't have any private

50:07.880 --> 50:09.760
discussions on a discord for years.

50:10.160 --> 50:14.840
Um, but there was a lot of people who came in with like, oh, I just had

50:14.840 --> 50:17.120
this amazing idea for AGI.

50:17.120 --> 50:21.640
If you just thought about like, if you imagine the AI is a brain and we, you

50:21.680 --> 50:23.680
know, this just, I don't want to talk about it.

50:23.840 --> 50:27.560
You know, I don't want to like, but you don't want to be dismissive or whatever.

50:27.560 --> 50:30.000
And it's like, oh, well, that's an interesting comment, but maybe you should

50:30.000 --> 50:33.040
like try training some models first to see if that aligns with your intuition.

50:33.040 --> 50:34.480
Like, oh, but how can I possibly learn?

50:34.480 --> 50:38.080
It's like, well, we have a course just actually spend time learning.

50:38.080 --> 50:39.120
Like, uh, yeah.

50:39.160 --> 50:39.480
Anyway.

50:40.120 --> 50:43.960
And there's like, okay, I know the people who always have good answers there.

50:43.960 --> 50:46.640
And so I created a private channel and put them all in it.

50:46.720 --> 50:50.280
And I got to admit, I, that's where I post more often because.

50:51.280 --> 50:57.520
There's much less, you know, flight of fancy views about how we could solve AGI,

50:57.520 --> 50:58.040
blah, blah, blah.

50:58.360 --> 51:02.880
So there is a bit of that, but having said that, like, I think the bar is pretty

51:02.880 --> 51:10.680
low, like if you join a discord and you can hit the, like participants or

51:10.680 --> 51:12.600
community or whatever button and you can see who's in it.

51:12.600 --> 51:16.320
And you'll see at the top who, who the admins or moderators or people in the dev

51:16.440 --> 51:25.120
role are and, uh, just DM one of them and say, like, oh, I, here's my GitHub.

51:25.600 --> 51:29.440
Well, here's some blog posts they wrote, you know, I'm interested in talking about

51:29.440 --> 51:34.120
this, you know, can I join the private channels and I've never heard of anybody

51:34.120 --> 51:40.320
saying no, I will say, you know, uh, a Luther's all pretty open.

51:40.840 --> 51:44.200
So you can do the Aleutha discord still, you know, one problem with your

51:44.200 --> 51:49.040
Luther discord is it's been going on for so long that it's like, it's very

51:49.040 --> 51:50.040
inside baseball.

51:50.320 --> 51:52.680
It's hard to, it's hard to get started.

51:53.080 --> 51:53.280
Yeah.

51:53.280 --> 51:58.760
Um, Kappa AI looks, I think it's all open.

51:59.840 --> 52:02.200
This just left a stability.

52:02.200 --> 52:03.160
That's more accessible.

52:03.760 --> 52:03.960
Yeah.

52:04.000 --> 52:11.000
Um, uh, there's also, uh, just recently, uh, now three search that

52:11.000 --> 52:16.160
does like the Hermes models and data set just, just opened, they've got some

52:16.160 --> 52:18.000
private channels, but it's pretty open.

52:18.000 --> 52:21.720
I think, uh, you mentioned alignment lab, that one, it's all the interesting

52:21.720 --> 52:22.800
stuff is on private channels.

52:22.800 --> 52:28.720
So just ask, um, if, if you know me, ask me, cause I've got admin on that one.

52:29.000 --> 52:34.920
There's also, yeah, uh, OS skunkworks, OS skunkworks AI is a good discord,

52:34.920 --> 52:37.760
which I think it's open.

52:38.880 --> 52:40.320
So they're, yeah, they're all pretty good.

52:40.520 --> 52:44.560
I don't want you to leak any, any, uh, you know, uh, discords that don't

52:44.560 --> 52:49.720
want any publicity, but we all want people, like we all want people.

52:49.720 --> 52:55.040
We just, we just want people who like want to build stuff, you know, um, rather

52:55.040 --> 53:01.520
than people who, and like, it's fine to not know anything as well, but if you

53:01.520 --> 53:04.360
don't know anything, but you want to tell everybody else what to do and how to

53:04.360 --> 53:05.200
do it, that's annoying.

53:05.520 --> 53:10.080
If you don't know anything and want to be told, like, here's a really small kind

53:10.080 --> 53:13.320
of task that as somebody who doesn't know anything, it's going to take you a

53:13.320 --> 53:15.560
really long time to do, but it would still be helpful.

53:15.880 --> 53:17.040
Then, and then you go and do it.

53:17.080 --> 53:17.640
That would be great.

53:18.200 --> 53:22.600
The truth is, yeah, like, I don't know, maybe 5% of people who come in with

53:22.600 --> 53:25.280
great enthusiasm and saying that they want to learn and they'll do anything.

53:25.280 --> 53:27.400
And then somebody says like, okay, here's some work you can do.

53:27.880 --> 53:29.240
Almost nobody does that work.

53:29.760 --> 53:36.040
So if you're somebody who actually does the work and follows up, you will massively

53:36.040 --> 53:36.800
stand out.

53:36.920 --> 53:40.920
That's an extreme rarity and everybody will then want to help you do more work.

53:41.080 --> 53:47.160
So, yeah, so just, um, yeah, just do work and people will want to support you.

53:47.920 --> 53:51.160
Our discord used to be referral only for a long time.

53:51.240 --> 53:55.120
We then have a public invite and then we opened it and they're kind of like

53:55.120 --> 53:55.840
channel gating.

53:56.280 --> 53:58.360
Um, yeah, a lot of people just want to do.

53:58.360 --> 54:00.600
I remember it used to be like, you know, a forum moderator.

54:00.880 --> 54:04.680
It's like people just want to do like drive by posting, you know, I'm like, they

54:04.680 --> 54:05.880
don't want to help the community.

54:05.880 --> 54:07.520
They just want to get their question answered.

54:07.760 --> 54:13.000
I mean, the funny thing is our, um, our forum community does not have any of that

54:13.000 --> 54:17.080
garbage, you know, there's something specific about the low latency thing.

54:17.080 --> 54:23.320
There were people like, they expect an instant answer and yeah, we're all somehow

54:23.320 --> 54:27.120
in a forum's tread where they know it's like they're forever.

54:28.040 --> 54:33.880
People are a bit more thoughtful, but then the forums are less active than they

54:33.880 --> 54:40.440
used to be because discord has got more popular, you know, so it's all a bit of

54:40.440 --> 54:45.520
a compromise, you know, running a healthy community is, yes, it's always a bit of a

54:45.520 --> 54:46.040
challenge.

54:46.480 --> 54:49.080
All right, we've got so many more things we want to dive in, but I don't want to

54:49.080 --> 54:50.240
keep you here for hours.

54:50.720 --> 54:52.960
Uh, this is not the, the lack of freedom in pockets.

54:52.960 --> 54:57.640
We always like to say, uh, one topic I would love to maybe chat a bit about is

54:57.760 --> 55:01.800
Mojo modular, you know, Chris Liner nominated you on the pockets.

55:01.880 --> 55:04.160
So, uh, we want to spend a little time there.

55:04.200 --> 55:08.080
You recently did a hacker's guide to language models and you ran through

55:08.080 --> 55:13.000
everything from quantized model to like smaller models, larger models and all of

55:13.000 --> 55:13.280
that.

55:13.640 --> 55:17.160
Um, but obviously modular is taking its own approach.

55:17.280 --> 55:18.560
Uh, yeah, we'll get you excited.

55:18.560 --> 55:22.320
I know you Chris have been talking about this for like years and a lot of the

55:22.320 --> 55:23.200
ideas you had, so.

55:23.680 --> 55:25.240
Yeah, yeah, yeah, absolutely.

55:25.240 --> 55:30.840
So I met Chris, I think it was at the first TensorFlow Dev Summit.

55:31.440 --> 55:35.280
And I don't think he had even like, I'm not sure if he'd even sufficiently

55:35.280 --> 55:37.480
started his employment with Google at that point.

55:37.520 --> 55:41.320
So I, I don't know, you know, certainly nothing had been mentioned.

55:42.560 --> 55:48.160
So I, you know, I admired him from afar with LLVM and Swift and whatever.

55:48.160 --> 55:53.200
And so I saw him walk into the courtyard at, at Google.

55:53.480 --> 55:56.440
It's just like, oh, man, Chris Lander.

55:57.600 --> 56:00.440
I wonder if he would lower his standards enough to talk to me.

56:01.280 --> 56:02.480
Was worth a try.

56:02.880 --> 56:05.800
So I caught up my carriage because like he, nobody was talking to him.

56:05.920 --> 56:08.480
He looked a bit lost and I wandered over and it's like, oh, you're Chris

56:08.480 --> 56:09.040
Latino, right?

56:09.040 --> 56:10.120
It's like, what are you doing here?

56:10.960 --> 56:12.440
And it's like, yeah, yeah, I am.

56:12.440 --> 56:13.600
I'm like, oh, I'm Jeremy Howard.

56:13.600 --> 56:15.800
It's like, oh, did you do some of this AI stuff?

56:15.800 --> 56:17.760
And I was like, yeah, yeah, I like this AI stuff.

56:18.520 --> 56:19.640
Are you doing AI stuff?

56:19.720 --> 56:22.520
It's like, well, I'm thinking about starting to do some AI stuff.

56:22.520 --> 56:23.640
Yeah, I think it's going to be cool.

56:23.640 --> 56:29.480
And so, well, so like I spent the next half hour just basically brain

56:29.520 --> 56:33.400
dumping all the ways in which AI was stupid to him and he listened

56:33.400 --> 56:37.800
patiently and I thought he probably wasn't even remember or care or whatever.

56:37.800 --> 56:42.840
But yeah, then I kind of like, I guess I re-caught up with him a few months

56:42.840 --> 56:45.600
later and it's like, I've been thinking about everything you said in that

56:45.600 --> 56:50.320
conversation and he like narrated back his response to every part of it, the

56:50.320 --> 56:51.520
projects he was planning to do.

56:51.520 --> 56:54.800
And it's just like, oh, this dude follows up, holy shit.

56:56.840 --> 56:58.240
And I was like, wow, okay.

56:58.240 --> 57:01.880
And he was like, yeah, so we're going to create this new thing called Swift

57:01.880 --> 57:05.080
for TensorFlow and it's going to be like, it's going to be a compiler

57:05.080 --> 57:08.280
with auto differentiation built in and blah, blah, blah, blah.

57:08.280 --> 57:10.160
And I say, wait, why would that help?

57:10.160 --> 57:12.840
You know, why would you, and he was like, okay, with a compiler during the

57:12.840 --> 57:16.480
forward pass, you don't have to worry about saving context, you know,

57:16.480 --> 57:19.200
because a lot of it will be optimized in the backward, but I was like, oh my God.

57:19.800 --> 57:21.520
Because I didn't really know much about compilers.

57:22.040 --> 57:26.200
You know, I spent enough to kind of like understand the ideas, but it hadn't

57:26.200 --> 57:30.240
occurred to me that a compiler basically solves a lot of the problems

57:30.240 --> 57:31.920
we have as end users.

57:32.600 --> 57:33.880
I was like, wow, that's amazing.

57:33.880 --> 57:37.560
Okay, you do know, right, that nobody's going to use this unless it's like usable.

57:37.880 --> 57:39.680
It's like, yeah, I know, right?

57:39.680 --> 57:42.240
So I was thinking you should create like a fast AI for this.

57:42.360 --> 57:46.240
So, okay, but I don't even know Swift.

57:46.400 --> 57:49.840
And it's like, well, why don't you start learning it?

57:50.040 --> 57:52.040
And if you have any questions, ask me.

57:52.640 --> 57:53.720
It's just like, holy shit.

57:53.800 --> 57:58.520
Like, not only is Chris Latner lowered his standards enough to talk to me, but

57:58.520 --> 58:02.320
he's offering me personal tutoring on the programming language that he made.

58:02.760 --> 58:05.080
So I was just like, I'm not going to let him down.

58:05.640 --> 58:10.040
So I spent like the next two months, like just nerding out on Swift.

58:10.040 --> 58:15.560
And it was just before Christmas that I kind of like started writing down what I'd learned.

58:16.640 --> 58:21.680
So I wrote a couple of blog posts on like, okay, this is like my attempt to

58:21.680 --> 58:23.480
do a numeric programming in Swift.

58:24.240 --> 58:28.920
And these are all the challenges I had and the some of the issues I had with like

58:30.200 --> 58:32.200
making things properly performant.

58:32.320 --> 58:35.600
And here are some libraries I wrote and I sent it to Chris and I was like,

58:35.600 --> 58:39.120
I hope he's not too disappointed with me, you know, because that would be the worst.

58:39.920 --> 58:43.200
It's like, you know, and I was also like, I was like, I hope he doesn't

58:43.320 --> 58:47.080
dislike the fact that I've, you know, didn't love everything.

58:48.040 --> 58:50.960
And yeah, he was like, oh, thanks for sending me that.

58:50.960 --> 58:52.480
Let's get on a call and talk about it.

58:52.480 --> 58:54.520
And we spoke and he was like, this is amazing.

58:54.880 --> 58:56.520
I can't believe that you made this.

58:56.520 --> 58:58.200
This is exactly what Swift needs.

58:58.200 --> 59:02.440
And he was like, and so like somebody set up like a new Swift,

59:03.640 --> 59:06.280
I don't remember what they call them, the equivalent of a pep, you know,

59:06.280 --> 59:09.680
kind of IFC thing of like, oh, you know, let's look at how we can implement

59:09.680 --> 59:11.280
Jeremy's ideas in the language.

59:11.280 --> 59:13.440
And he's just like, oh, wow.

59:13.720 --> 59:21.200
And so, yeah, you know, so, you know, and then we ended up like literally

59:21.200 --> 59:24.920
teaching some lessons together about Swift for TensorFlow.

59:24.920 --> 59:32.280
And we built a fast AI kind of equivalent with him and his team.

59:32.280 --> 59:33.240
It's so much fun.

59:34.440 --> 59:37.640
Then in the end, you know, Google didn't follow through just fair enough,

59:37.640 --> 59:42.920
like asking everybody to learn a new programming language is going to be tough.

59:42.960 --> 59:46.280
But like it was very obvious, very, very obvious at that time that TensorFlow

59:46.280 --> 59:52.720
2 is going to be a failure, you know, and so this felt like, okay, I, you know,

59:53.880 --> 59:55.080
well, you know, what are you going to do?

59:55.400 --> 01:00:01.960
Like, you can't focus on TensorFlow 2 because it's not going to, like it's

01:00:01.960 --> 01:00:02.480
not working.

01:00:02.480 --> 01:00:06.680
It's never going to work, you know, nobody at Google is using it internally.

01:00:07.680 --> 01:00:12.680
So, you know, in the end, Chris left, you know, Swift for TensorFlow got archived.

01:00:13.680 --> 01:00:15.160
There was no backup plan.

01:00:15.160 --> 01:00:20.680
So I kind of felt like Google was kind of screwed, you know, and Chris went and

01:00:20.680 --> 01:00:23.680
did something else, but we kept talking and I was like, look, Chris, you know,

01:00:25.680 --> 01:00:28.680
you've got to be your own boss, man, because like, you know, you've got the ideas,

01:00:29.680 --> 01:00:33.680
you know, like only you've got the ideas, you know, and if your ideas are implemented,

01:00:34.680 --> 01:00:40.680
we'd all be so much better off because like, Python's the best of a whole bunch

01:00:40.680 --> 01:00:45.680
of shit, you know, like, I would, it's amazing, but it's awful, you know, compared

01:00:45.680 --> 01:00:46.680
to what it could be.

01:00:47.680 --> 01:00:51.680
Anyway, so eventually a few years later, he called me up and he was like, Jeremy,

01:00:51.680 --> 01:00:52.680
I've taken your advice.

01:00:53.680 --> 01:00:57.680
I've started a company so much like, oh my God, so we got to create a new language.

01:00:57.680 --> 01:00:59.680
We're going to create a new infrastructure.

01:00:59.680 --> 01:01:02.680
It's going to build, it's going to have all the stuff we've talked about.

01:01:03.680 --> 01:01:04.680
And it's like, oh, wow.

01:01:05.680 --> 01:01:08.680
So that's what Mojo is.

01:01:09.680 --> 01:01:17.680
And so Mojo is like, you know, building on all the stuff that Chris has figured out

01:01:17.680 --> 01:01:23.680
over, I mean, really from when he did his PhD thesis, which developed LLVM onwards,

01:01:23.680 --> 01:01:30.680
you know, in Swift and MLAR, you know, the TensorFlow runtime engine, which is very good.

01:01:30.680 --> 01:01:33.680
You know, that was something that he built and has lasted.

01:01:35.680 --> 01:01:37.680
So yeah, I'm pumped about that.

01:01:37.680 --> 01:01:39.680
I mean, it's very speculative.

01:01:39.680 --> 01:01:41.680
Creating a whole new language is tough.

01:01:41.680 --> 01:01:46.680
I mean, Chris has done it before and he's created a whole C++ compiler amongst other things.

01:01:47.680 --> 01:01:48.680
Looking pretty hopeful.

01:01:49.680 --> 01:01:53.680
I mean, I hope it works because, you know, I mean.

01:01:53.680 --> 01:01:55.680
You're doing them to quit his job, so.

01:01:56.680 --> 01:02:02.680
And I mean, in the meantime, I will say, you know, Google now does have a backup plan, you know,

01:02:02.680 --> 01:02:05.680
they have JAX, which was never a strategy.

01:02:05.680 --> 01:02:10.680
It was just a bunch of people who also recognized TensorFlow 2 as shit and they just decided to build something else.

01:02:11.680 --> 01:02:15.680
And for years, my friends in that team were like, don't tell anybody about us because we, you know,

01:02:15.680 --> 01:02:17.680
we don't want to be anything but a research project.

01:02:18.680 --> 01:02:23.680
So now these poor guys suddenly, they're the great white hope for Google's future.

01:02:24.680 --> 01:02:28.680
And so JAX is, you know, also not terrible, but it's still written in Python.

01:02:28.680 --> 01:02:37.680
Like it would be cool if we had all the benefits of JAX but in a language that was designed for those kind of purposes.

01:02:39.680 --> 01:02:45.680
So, you know, fingers crossed that, yeah, that Mocho turns out great.

01:02:45.680 --> 01:02:46.680
Yeah.

01:02:47.680 --> 01:02:51.680
Any other thoughts on when, where people should be spending their time?

01:02:51.680 --> 01:02:57.680
So that's more the kind of language framework level than you have the, you know, GGML.

01:02:57.680 --> 01:03:01.680
Some of these are like quantization-focused kind of model-level things.

01:03:01.680 --> 01:03:03.680
Then you got the hardware people.

01:03:03.680 --> 01:03:04.680
It's like a whole other bucket.

01:03:05.680 --> 01:03:08.680
Yeah, what are some of the exciting stuff that you're excited about?

01:03:08.680 --> 01:03:17.680
Well, you won't be surprised to hear me say this, but I think fine-tuning, transfer learning is still a hugely underappreciated area.

01:03:17.680 --> 01:03:27.680
So today's zero-shot, few-shot learning equivalent is retrieval augmented generation, you know, RAG.

01:03:27.680 --> 01:03:31.680
Which is like, just like few-shot learning is a thing.

01:03:31.680 --> 01:03:32.680
Like it's a real thing.

01:03:32.680 --> 01:03:33.680
It's a useful thing.

01:03:33.680 --> 01:03:35.680
It's not a thing anybody would want to ignore.

01:03:35.680 --> 01:03:39.680
Why are people not spending at least as much effort on fine-tuning?

01:03:39.680 --> 01:03:45.680
You know, because, you know, RAG is like such an inefficient hack, really, isn't it?

01:03:45.680 --> 01:03:52.680
It's like, you know, segment up my data in some somewhat arbitrary way.

01:03:52.680 --> 01:03:55.680
Embed it, ask questions about that.

01:03:55.680 --> 01:04:03.680
You know, hope that my embedding model embeds questions in the same bedding space as a paragraph.

01:04:03.680 --> 01:04:08.680
Which obviously is not going to, if your question is like, if I've got a whole bunch of archive papers embeddings.

01:04:08.680 --> 01:04:17.680
And I asked like, what are all the ways in which we can make inference more efficient?

01:04:17.680 --> 01:04:26.680
The only paragraphs it'll find is like, if there's a review paper that says here's a list of ways to make, you know, inference more efficient.

01:04:26.680 --> 01:04:27.680
Doesn't have any of the specifics.

01:04:27.680 --> 01:04:34.680
No, it's not going to be like, oh, here's one way, here's one way, here's a different way in different papers, you know.

01:04:34.680 --> 01:04:46.680
Yeah, if you fine-tune a model, then all of that information is getting directly incorporated into the weights of your model in a much more efficient and nuanced way.

01:04:46.680 --> 01:04:49.680
And then you can use RAG on top of that.

01:04:49.680 --> 01:04:54.680
So I think that that's one area that's definitely underappreciated.

01:04:54.680 --> 01:05:00.680
And also the confluence of like, okay, how do you combine RAG and fine-tuning, for example.

01:05:00.680 --> 01:05:10.680
Something that I think a lot of people are uncertain about, and I don't expect you to know either, is that whether or not you can fine-tune new information in.

01:05:10.680 --> 01:05:16.680
And I think that that is the focus of some of your open questions and research.

01:05:16.680 --> 01:05:17.680
But of course you can, right?

01:05:17.680 --> 01:05:18.680
Because it's additional pre-training.

01:05:18.680 --> 01:05:22.680
Obviously you can, because there's no such thing as fine-tuning.

01:05:22.680 --> 01:05:24.680
There's only continued pre-training.

01:05:24.680 --> 01:05:29.680
So fine-tuning is pre-training, like they're literally the same thing.

01:05:30.680 --> 01:05:33.680
So the knowledge got in there in the first place through pre-training.

01:05:33.680 --> 01:05:37.680
So how could continuing to pre-train not put more knowledge in?

01:05:37.680 --> 01:05:39.680
Like it's the same thing.

01:05:40.680 --> 01:05:44.680
The problem is just we're really bad at it, because everybody's doing it dumb ways.

01:05:44.680 --> 01:05:49.680
So it's a good question, and it's not just new knowledge, but new capabilities.

01:05:50.680 --> 01:05:57.680
You know, I think like in my Hackers Guide to LL, into Hackers Guide to LLM's talk, I show simple.

01:05:57.680 --> 01:06:05.680
I mean, it's a funny, that's a simple example, because it doesn't sound it, but like taking a pre-trained based model and getting it to generate SQL.

01:06:05.680 --> 01:06:08.680
And it took 15 minutes to train on a single GPU.

01:06:08.680 --> 01:06:15.680
You know, I think that might surprise people that that capability is actual fingertips.

01:06:15.680 --> 01:06:20.680
And you know, because it was already there, it was just latent in the base model.

01:06:20.680 --> 01:06:27.680
Really pushing the boundaries of what you can do with small models, I think is a really interesting question.

01:06:27.680 --> 01:06:29.680
Like what can you do with a...

01:06:29.680 --> 01:06:32.680
I mean, there isn't much in the way of good small models.

01:06:32.680 --> 01:06:43.680
A really underappreciated one is a BTLM 3B, which is a like kind of 7B quality 3B model.

01:06:44.680 --> 01:06:46.680
There's not much of the 1-2B range, sadly.

01:06:46.680 --> 01:06:56.680
There are some code ones, but like the fact that there are some really good code ones in that 1-2B range shows you that that's a great size for doing complex tasks well.

01:06:57.680 --> 01:07:03.680
There was PHY1 recently, which has been the subject of a little bit of discussion about whether they're trained on benchmarks.

01:07:03.680 --> 01:07:05.680
Yeah, PHY1.5 as well.

01:07:05.680 --> 01:07:09.680
So that's not a good model yet.

01:07:10.680 --> 01:07:11.680
Why not?

01:07:12.680 --> 01:07:20.680
So PHY1 in particular is good at doing a very specific thing, which is creating very small Python snippets.

01:07:20.680 --> 01:07:21.680
The thing...

01:07:21.680 --> 01:07:26.680
Okay, so like PHY1.5 has never read Wikipedia, for example.

01:07:26.680 --> 01:07:30.680
So it doesn't know who Tom Cruise is, you know.

01:07:30.680 --> 01:07:33.680
It doesn't know who anybody is.

01:07:33.680 --> 01:07:35.680
He doesn't know about any movies.

01:07:35.680 --> 01:07:41.680
It doesn't really know anything about anything because it's never read anything.

01:07:41.680 --> 01:07:49.680
You know, it was trained on a nearly entirely synthetic data set, which is designed for it to learn reasoning.

01:07:49.680 --> 01:07:57.680
And so it was a research project and a really good one, and it definitely shows us a powerful direction in terms of what you can do with synthetic data.

01:07:57.680 --> 01:08:04.680
And wow, gosh, even these tiny models can get pretty good reasoning skills, pretty good math skills, pretty good toting skills.

01:08:06.680 --> 01:08:11.680
But I don't know if it's a model you could necessarily build on.

01:08:11.680 --> 01:08:14.680
Some people have tried to do some fine tunes of it.

01:08:14.680 --> 01:08:24.680
And again, they're like surprisingly good in some ways for a 1.5B model, but not sure you'd find it useful for anything.

01:08:24.680 --> 01:08:32.680
I think that's the struggle of pitching small models because small is great, you know, you don't need a lot of resources to run them.

01:08:32.680 --> 01:08:35.680
But the performance evaluation is always so iffy.

01:08:35.680 --> 01:08:40.680
It's always just like, yeah, it works on some things and we don't trust it for others.

01:08:40.680 --> 01:08:43.680
Yeah, so that's why we're back to fine tuning.

01:08:43.680 --> 01:08:50.680
I would say, so Microsoft did create a PHY1.5 web, but they didn't release it, unfortunately.

01:08:51.680 --> 01:09:04.680
I would say a PHY1.5 web with fine tuning for your task might solve a lot of tasks that people have in their kind of day-to-day lives,

01:09:06.680 --> 01:09:08.680
particularly in kind of an enterprise setting.

01:09:08.680 --> 01:09:13.680
I think there's a lot of repetitive kind of processing that has to be done.

01:09:13.680 --> 01:09:24.680
It's a useful thing for coders to know about because I think quite often you can replace some thousands and thousands of lines of complex buggy code maybe with a fine tune, you know.

01:09:25.680 --> 01:09:26.680
Good.

01:09:26.680 --> 01:09:27.680
Yeah.

01:09:28.680 --> 01:09:33.680
And Jeremy, before we let you go, I think one question on top of a lot of people's minds.

01:09:33.680 --> 01:09:39.680
So you've done practical deep learning for coders in 2018, 19, 21, 22.

01:09:39.680 --> 01:09:43.680
I feel like the more time goes by, the more the GPUs get concentrated.

01:09:44.680 --> 01:09:50.680
If you're somebody who's interested in deep learning today and you don't want to go join OpenAI, you don't want to join Anthropic,

01:09:50.680 --> 01:09:53.680
what's like the best use of their time?

01:09:53.680 --> 01:09:55.680
Should they focus on, yes, model development?

01:09:55.680 --> 01:09:58.680
Should they focus on fine tuning math and all of that?

01:09:58.680 --> 01:10:04.680
Should they just like focus on making rag not a hack and coming up with a better solution?

01:10:05.680 --> 01:10:09.680
Yeah, what's practical deep learning for coders 2024 kind of look like?

01:10:10.680 --> 01:10:11.680
Yeah.

01:10:11.680 --> 01:10:12.680
I mean, good question.

01:10:12.680 --> 01:10:15.680
I'm trying to figure that out for myself, you know, like what should I teach?

01:10:15.680 --> 01:10:21.680
Because I definitely feel like things have changed a bit, you know.

01:10:21.680 --> 01:10:26.680
One of the ways in which things have changed is that coding is much more accessible now.

01:10:26.680 --> 01:10:30.680
So if you look at a lot of the folks in the kind of open source LLM community,

01:10:30.680 --> 01:10:33.680
they're folks who really hadn't coded before a year ago.

01:10:34.680 --> 01:10:40.680
And they're using these models to help them build stuff they couldn't build before, which is just fantastic, you know.

01:10:42.680 --> 01:10:48.680
So one thing I kind of think is like, okay, well, we need a lot more material to help these people use this newfound skill they have,

01:10:48.680 --> 01:10:52.680
because they don't really know what they're doing, you know, and they don't claim to,

01:10:52.680 --> 01:10:53.680
but they're doing it anyway.

01:10:53.680 --> 01:10:55.680
And I think that's fantastic, you know.

01:10:55.680 --> 01:11:00.680
So like other things we could do to help people, you know, bridge this gap,

01:11:00.680 --> 01:11:09.680
because previously, you know, I know folks who were, you know, doing menial jobs a year ago,

01:11:09.680 --> 01:11:16.680
and now they're training language models thanks to the help of codecs and co-pilot and whatever.

01:11:16.680 --> 01:11:21.680
So, you know, yeah, what does it look like to like really grab this opportunity?

01:11:21.680 --> 01:11:27.680
You know, maybe fast AIs goals can be dramatically expanded now to being like,

01:11:27.680 --> 01:11:33.680
let's make coding more accessible, you know, or kind of AI-oriented coding more accessible.

01:11:34.680 --> 01:11:39.680
If so, our costs should probably look very different, you know,

01:11:39.680 --> 01:11:43.680
and we'd have to throw away that like, oh, you have to have at least a year of full-time programming,

01:11:43.680 --> 01:11:46.680
you know, as a prerequisite.

01:11:47.680 --> 01:11:49.680
Yeah, what would happen if we got rid of that?

01:11:49.680 --> 01:11:52.680
So that's kind of one thought that's in my head.

01:11:53.680 --> 01:12:00.680
You know, as to what should other people do, honestly, I don't think anybody has any idea,

01:12:00.680 --> 01:12:03.680
like the more I look at it, what's going on.

01:12:03.680 --> 01:12:07.680
I know I don't, you know, like, we don't really know how to do anything very well.

01:12:08.680 --> 01:12:14.680
Clearly open AI do, like they seem to be quite good at some things,

01:12:14.680 --> 01:12:18.680
or they're talking to folks at or who have recently left open AI.

01:12:18.680 --> 01:12:21.680
Even there, it's clear there's a lot of stuff they haven't really figured out,

01:12:21.680 --> 01:12:26.680
and they're just kind of like using recipes that they've noticed have been okay.

01:12:26.680 --> 01:12:29.680
So yeah, we don't really know how to train these models well.

01:12:29.680 --> 01:12:30.680
We don't know how to fine-tune them well.

01:12:30.680 --> 01:12:32.680
We don't know how to do RAG well.

01:12:32.680 --> 01:12:33.680
We don't know what they can do.

01:12:33.680 --> 01:12:34.680
We don't know what they can't do.

01:12:34.680 --> 01:12:37.680
We don't know how big a model you need to solve different kinds of problems.

01:12:37.680 --> 01:12:39.680
We don't know what kind of problems they can't do.

01:12:39.680 --> 01:12:43.680
We don't know what good prompting strategies are for particular problems, you know,

01:12:43.680 --> 01:12:49.680
like somebody sent me a message the other day saying they've written something

01:12:49.680 --> 01:12:54.680
that is a prompting strategy for GPT-4.

01:12:54.680 --> 01:12:57.680
They've written like 6,000 lines of Python code,

01:12:57.680 --> 01:13:00.680
and it's to help it play chess.

01:13:01.680 --> 01:13:05.680
And then they've said they've had it play against other chess engines,

01:13:05.680 --> 01:13:07.680
including the best stockfish engines,

01:13:07.680 --> 01:13:11.680
and it's got an ELO of 3400,

01:13:11.680 --> 01:13:15.680
which would make it close to the best chess engine in existence.

01:13:17.680 --> 01:13:22.680
And I think this is a good example of like people were saying like GPT-4 can't play chess.

01:13:22.680 --> 01:13:24.680
I mean, I was sure that was wrong.

01:13:24.680 --> 01:13:26.680
I mean, obviously it can play chess,

01:13:26.680 --> 01:13:30.680
but the difference between like with no prompting strategy,

01:13:30.680 --> 01:13:32.680
they can't even make legal moves.

01:13:32.680 --> 01:13:33.680
With good prompting strategies,

01:13:33.680 --> 01:13:36.680
it might be just about the best chess engine in the world,

01:13:36.680 --> 01:13:38.680
far better than any human player.

01:13:38.680 --> 01:13:41.680
So, yeah, I mean, we don't really know what the capabilities are yet.

01:13:41.680 --> 01:13:44.680
So I feel like it's all blue sky at this point.

01:13:44.680 --> 01:13:48.680
It feels like computer vision in 2013 to me,

01:13:48.680 --> 01:13:50.680
which was like in 2013 computer vision.

01:13:50.680 --> 01:13:51.680
We just had the AlexNet.

01:13:51.680 --> 01:13:53.680
We've had AlexNet.

01:13:53.680 --> 01:13:55.680
We've had VGGNet.

01:13:55.680 --> 01:13:57.680
It's around the time Xyler and Fergus like,

01:13:57.680 --> 01:13:59.680
no, it's probably before that.

01:13:59.680 --> 01:14:00.680
So we hadn't yet had the Xyler and Fergus like,

01:14:00.680 --> 01:14:02.680
oh, this is actually what's going on inside the layers.

01:14:02.680 --> 01:14:07.680
So, you know, we don't actually know what's happening inside these transformers.

01:14:07.680 --> 01:14:11.680
We don't know how to create good training dynamics.

01:14:11.680 --> 01:14:14.680
We don't really know anything much.

01:14:14.680 --> 01:14:17.680
And there's a reason for that, right?

01:14:17.680 --> 01:14:24.680
And the reason for that is language models suddenly got really useful.

01:14:24.680 --> 01:14:28.680
And so the kind of economically rational thing to do,

01:14:28.680 --> 01:14:30.680
like this is not criticism, this is true.

01:14:30.680 --> 01:14:32.680
The economic rational thing to do is to like,

01:14:32.680 --> 01:14:35.680
okay, like build that as fast as possible,

01:14:35.680 --> 01:14:38.680
you know, make something work, get it out there.

01:14:38.680 --> 01:14:42.680
And that's what, you know, open AI in particular did,

01:14:42.680 --> 01:14:46.680
anthropic kind of did.

01:14:46.680 --> 01:14:50.680
But there's a whole lot of technical debt everywhere, you know,

01:14:50.680 --> 01:14:55.680
nobody's really figured this stuff out because everybody's been so busy

01:14:55.680 --> 01:14:59.680
building what we know works as quickly as possible.

01:14:59.680 --> 01:15:02.680
So, yeah, I think there's a huge amount of opportunity to,

01:15:02.680 --> 01:15:09.680
you know, I think we'll find things can be made to work a lot faster,

01:15:09.680 --> 01:15:11.680
a lot less memory.

01:15:11.680 --> 01:15:14.680
I got a whole bunch of ideas I want to try, you know,

01:15:14.680 --> 01:15:18.680
every time I look at something closely, like really closely,

01:15:18.680 --> 01:15:22.680
I'm always like, oh, turns out this person actually had no idea what they're doing.

01:15:22.680 --> 01:15:27.680
You know, which is fine, like none of us know what we're doing.

01:15:27.680 --> 01:15:31.680
We should experiment with that.

01:15:31.680 --> 01:15:35.680
We had a treat out on the podcast who created flash attention.

01:15:35.680 --> 01:15:39.680
And I asked them, did nobody think of using SRAM before you?

01:15:39.680 --> 01:15:42.680
Like where people just like, you know, and he was like, yeah,

01:15:42.680 --> 01:15:45.680
people just didn't think of it, didn't try.

01:15:45.680 --> 01:15:47.680
They didn't come from like a systems background.

01:15:47.680 --> 01:15:48.680
Yeah.

01:15:48.680 --> 01:15:52.680
I mean, the thing about flash attention is, I mean,

01:15:52.680 --> 01:15:55.680
lots of people absolutely thought of that.

01:15:55.680 --> 01:15:56.680
So had I, right?

01:15:56.680 --> 01:16:00.680
But I mean, the honest truth is particularly before Triton,

01:16:00.680 --> 01:16:05.680
like everybody knew that tiling is the right way to solve anything.

01:16:05.680 --> 01:16:07.680
And everybody knew that attention,

01:16:07.680 --> 01:16:09.680
used attention, wasn't tiled.

01:16:09.680 --> 01:16:11.680
That was stupid.

01:16:11.680 --> 01:16:16.680
But not everybody's got his ability to like be like,

01:16:16.680 --> 01:16:22.680
oh, well, I'm confident enough in CUDA and or Triton

01:16:22.680 --> 01:16:24.680
to use that insight to write something better.

01:16:24.680 --> 01:16:27.680
And this is where like, I'm super excited about Mojo, right?

01:16:27.680 --> 01:16:30.680
And I always talk to Chris about flash attention as I'm like,

01:16:30.680 --> 01:16:36.680
there is a thousand flash attentions out there for us to build.

01:16:36.680 --> 01:16:39.680
You just got to make it easy for us to build them.

01:16:39.680 --> 01:16:42.680
So like Triton definitely helps.

01:16:42.680 --> 01:16:46.680
But it's still not easy.

01:16:46.680 --> 01:16:51.680
It still requires kind of really understanding the GPU architecture,

01:16:51.680 --> 01:16:54.680
writing it in that kind of very CUDA-ish way.

01:16:54.680 --> 01:16:59.680
So yeah, I think, you know, if Mojo or something equivalent

01:16:59.680 --> 01:17:04.680
can really work well, we're going to see a lot more flash attentions

01:17:04.680 --> 01:17:07.680
popping up.

01:17:07.680 --> 01:17:08.680
Great, Jaren.

01:17:08.680 --> 01:17:11.680
Before we wrap, we usually do a quick lightning round.

01:17:11.680 --> 01:17:13.680
We've got three simple questions.

01:17:13.680 --> 01:17:15.680
So the first one is around acceleration

01:17:15.680 --> 01:17:18.680
and you've been in this field a long time.

01:17:18.680 --> 01:17:20.680
What's something that it's already here today

01:17:20.680 --> 01:17:23.680
and AI that you thought would take much longer?

01:17:23.680 --> 01:17:24.680
I don't think anything.

01:17:24.680 --> 01:17:26.680
So I've actually been slightly too bullish.

01:17:26.680 --> 01:17:34.680
So in my 2014 TED Talk, I had a graph and I said like,

01:17:34.680 --> 01:17:36.680
this is like the slope of human capabilities

01:17:36.680 --> 01:17:39.680
and this is the slope of AI capabilities.

01:17:39.680 --> 01:17:42.680
And I said, oh, and I put a dot saying we are here.

01:17:42.680 --> 01:17:44.680
It was just before they passed.

01:17:44.680 --> 01:17:47.680
And I looked back at the transcript the other day

01:17:47.680 --> 01:17:52.680
and I said in five years, I think we might have crossed

01:17:52.680 --> 01:17:55.680
that threshold in which computers will be better

01:17:55.680 --> 01:17:58.680
at most human tasks than most humans, most average humans.

01:17:58.680 --> 01:18:05.680
And so that might be almost true now for non-physical tasks.

01:18:05.680 --> 01:18:14.680
So I took that twice as long as I thought it might.

01:18:14.680 --> 01:18:18.680
Yeah, no, I wouldn't say anything surprised me too much.

01:18:18.680 --> 01:18:21.680
It's still like definitely like, I got to admit,

01:18:21.680 --> 01:18:26.680
I had a very visceral reaction using GPT-4 for the first time,

01:18:26.680 --> 01:18:32.680
not because I found it surprising, but actually doing it.

01:18:32.680 --> 01:18:36.680
Something I was pretty sure would exist by about now,

01:18:36.680 --> 01:18:38.680
maybe a bit earlier.

01:18:38.680 --> 01:18:41.680
But actually using it definitely is different

01:18:41.680 --> 01:18:44.680
to just feeling like it's probably on its way.

01:18:44.680 --> 01:18:49.680
And yeah, whatever GPT-5 looks like,

01:18:49.680 --> 01:18:56.680
I'm sure I imagine I'll have the same visceral reaction.

01:18:56.680 --> 01:19:00.680
It's really amazing to watch develop.

01:19:00.680 --> 01:19:02.680
We also have an exploration question.

01:19:02.680 --> 01:19:04.680
So what do you think is the most interesting

01:19:04.680 --> 01:19:07.680
unsolved question in AI?

01:19:07.680 --> 01:19:10.680
How do language models learn?

01:19:10.680 --> 01:19:12.680
What are the training dynamics?

01:19:12.680 --> 01:19:19.680
Like I want to see, there was a great paper about Resnets

01:19:19.680 --> 01:19:25.680
a few years ago that showed how that was able to like plot

01:19:25.680 --> 01:19:28.680
a kind of projected three-dimensional loss surface

01:19:28.680 --> 01:19:34.680
for a ConvNet with and without skip connections.

01:19:34.680 --> 01:19:37.680
And you could very clearly see without the skip connections,

01:19:37.680 --> 01:19:39.680
it was super bumpy and with the skip connections,

01:19:39.680 --> 01:19:43.680
it was super smooth.

01:19:43.680 --> 01:19:45.680
That's the kind of work we need.

01:19:45.680 --> 01:19:47.680
So there was actually an interesting blog post

01:19:47.680 --> 01:19:50.680
that came out just today from the PyTorch team

01:19:50.680 --> 01:19:53.680
where some of them have created this like 3D

01:19:53.680 --> 01:19:56.680
matrix product visualization thing.

01:19:56.680 --> 01:20:00.680
And they actually showed some nice examples

01:20:00.680 --> 01:20:03.680
of like a GPT-2 attention layer

01:20:03.680 --> 01:20:06.680
and showed an animation and said,

01:20:06.680 --> 01:20:08.680
like, if you look at this, we can actually see a bit

01:20:08.680 --> 01:20:09.680
about what it's doing.

01:20:09.680 --> 01:20:12.680
You know, so again, it reminds me of the Zeiler

01:20:12.680 --> 01:20:15.680
and Fergus, you know, ConvNet paper.

01:20:15.680 --> 01:20:18.680
That was the first one to do these reverse convolutions

01:20:18.680 --> 01:20:20.680
to show what's actually being learned

01:20:20.680 --> 01:20:21.680
in each layer in a ConvNet.

01:20:21.680 --> 01:20:24.680
Yeah, we need a lot more of this, like,

01:20:24.680 --> 01:20:27.680
what is going on inside these models?

01:20:27.680 --> 01:20:29.680
How do they actually learn?

01:20:29.680 --> 01:20:31.680
And then how can we use those insights

01:20:31.680 --> 01:20:35.680
to help them to learn better?

01:20:35.680 --> 01:20:36.680
So I think that would be one.

01:20:36.680 --> 01:20:38.680
The other exploration I'd really like to see

01:20:38.680 --> 01:20:41.680
is a much more rigorous analysis

01:20:41.680 --> 01:20:44.680
of what kind of data do they need,

01:20:44.680 --> 01:20:47.680
at what level, and when do they need it,

01:20:47.680 --> 01:20:48.680
and how often.

01:20:48.680 --> 01:20:51.680
So that kind of like data set mixing, curation,

01:20:51.680 --> 01:20:54.680
so forth in order to get the best capabilities.

01:20:54.680 --> 01:20:55.680
Yeah.

01:20:55.680 --> 01:20:57.680
How much is Wikipedia?

01:20:57.680 --> 01:20:58.680
Yeah.

01:20:58.680 --> 01:20:59.680
Very uncertain.

01:20:59.680 --> 01:21:01.680
You know, to fine-tune what kind of mix

01:21:01.680 --> 01:21:04.680
do you need for it to keep its capabilities

01:21:04.680 --> 01:21:06.680
and what are the kind of underlying capabilities

01:21:06.680 --> 01:21:07.680
that it most needs to keep?

01:21:07.680 --> 01:21:09.680
And if it loses those, it would lose all these other ones

01:21:09.680 --> 01:21:11.680
and what data do you need to keep those?

01:21:11.680 --> 01:21:13.680
And, you know, are there things we can do

01:21:13.680 --> 01:21:16.680
to change the loss function, to help it,

01:21:16.680 --> 01:21:20.680
to not forget to do things, stuff like that?

01:21:20.680 --> 01:21:21.680
Awesome.

01:21:21.680 --> 01:21:24.680
And yeah, before wrapping, what's one message,

01:21:24.680 --> 01:21:26.680
one idea you want everyone to remember

01:21:26.680 --> 01:21:27.680
and think about?

01:21:27.680 --> 01:21:29.680
You know, I guess the main thing I want everybody

01:21:29.680 --> 01:21:31.680
to remember is that, you know,

01:21:31.680 --> 01:21:33.680
there's a lot of people in the world

01:21:33.680 --> 01:21:35.680
and they have a lot of, you know,

01:21:35.680 --> 01:21:38.680
diverse experiences and capabilities

01:21:38.680 --> 01:21:42.680
and, you know, they all matter.

01:21:42.680 --> 01:21:46.680
And now that we have, you know,

01:21:46.680 --> 01:21:49.680
nearly powerful technology in our lives,

01:21:49.680 --> 01:21:51.680
we could think of that in one of two ways.

01:21:51.680 --> 01:21:57.680
One would be, gee, that's really scary

01:21:57.680 --> 01:21:59.680
what would happen if all of these people in the world

01:21:59.680 --> 01:22:01.680
had access to this technology.

01:22:01.680 --> 01:22:03.680
One of them might be bad people.

01:22:03.680 --> 01:22:05.680
Let's make sure they can't have it.

01:22:05.680 --> 01:22:08.680
Or one might be, wow,

01:22:08.680 --> 01:22:10.680
if all those people in the world are better,

01:22:10.680 --> 01:22:13.680
a lot of them could really improve the lives

01:22:13.680 --> 01:22:17.680
of a lot of humanity if they had this tool.

01:22:17.680 --> 01:22:19.680
This has always been the case, you know,

01:22:19.680 --> 01:22:21.680
from the invention of writing

01:22:21.680 --> 01:22:23.680
to the invention of the printing press

01:22:23.680 --> 01:22:26.680
to the, you know, development of education.

01:22:26.680 --> 01:22:29.680
And it's been a constant battle

01:22:29.680 --> 01:22:33.680
between people who think that distributed power is unsafe

01:22:33.680 --> 01:22:36.680
and it should be held on to by an elite few

01:22:36.680 --> 01:22:43.680
and people who think that humanity on net,

01:22:43.680 --> 01:22:46.680
you know, is a marvellous species,

01:22:46.680 --> 01:22:49.680
particularly when part of a society and a civilization

01:22:49.680 --> 01:22:51.680
and we should do everything we can

01:22:51.680 --> 01:22:55.680
to enable more of them to contribute.

01:22:55.680 --> 01:22:59.680
There's a really big conversation right now

01:22:59.680 --> 01:23:04.680
and, you know, I want to see more and more people

01:23:04.680 --> 01:23:10.680
showing up and showing what, you know,

01:23:10.680 --> 01:23:13.680
what the great unwashed masses out there

01:23:13.680 --> 01:23:15.680
can actually achieve, you know,

01:23:15.680 --> 01:23:18.680
that actually, you know, regular people

01:23:18.680 --> 01:23:22.680
are going to do a lot of really valuable work

01:23:22.680 --> 01:23:28.680
and actually help us be, you know, more safe

01:23:28.680 --> 01:23:32.680
and also flourishing in our lives

01:23:32.680 --> 01:23:37.680
and providing a future for our children to flourish in,

01:23:37.680 --> 01:23:44.680
you know, if we lock things down

01:23:44.680 --> 01:23:48.680
to the people that we think, you know,

01:23:48.680 --> 01:23:52.680
the elites that we think can be trusted to run it for us.

01:23:52.680 --> 01:24:01.680
Yeah, I think all bets are off about where that lives as a society, you know.

01:24:01.680 --> 01:24:03.680
Yep.

01:24:03.680 --> 01:24:06.680
Now, that's an important message

01:24:06.680 --> 01:24:08.680
and yeah, that's why we've been promoting a lot

01:24:08.680 --> 01:24:11.680
of open source developers, open source communities.

01:24:11.680 --> 01:24:15.680
I think letting the builders build and explore,

01:24:15.680 --> 01:24:17.680
that's always a good idea.

01:24:17.680 --> 01:24:19.680
Thank you so much for coming on, Jeremy.

01:24:19.680 --> 01:24:20.680
This was great.

01:24:20.680 --> 01:24:22.680
Thank you for having me.

