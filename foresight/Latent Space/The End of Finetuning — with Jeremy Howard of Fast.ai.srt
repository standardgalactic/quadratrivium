1
00:00:00,000 --> 00:00:13,960
Hey everyone, welcome to the Late in Space Pockets.

2
00:00:13,960 --> 00:00:18,400
This is Alessio, partner in CTO and residence at Decibel Partners, and I'm joined by my

3
00:00:18,400 --> 00:00:20,840
co-host, Swix, founder of SmallAI.

4
00:00:20,840 --> 00:00:26,680
Hey, and today we have in the remote studio Jeremy Howard from, all the way from Australia.

5
00:00:26,680 --> 00:00:27,680
Good morning.

6
00:00:27,760 --> 00:00:29,960
The remote studio, also known as my house.

7
00:00:29,960 --> 00:00:30,960
Good morning.

8
00:00:30,960 --> 00:00:32,960
Nice to see you, Ruth.

9
00:00:32,960 --> 00:00:34,460
Nice to see you too.

10
00:00:34,460 --> 00:00:39,840
I'm actually very used to seeing you in your mask as a message to people, but today we're

11
00:00:39,840 --> 00:00:40,840
mostly audio.

12
00:00:40,840 --> 00:00:46,120
But thank you for doing the very important public service of COVID awareness.

13
00:00:46,120 --> 00:00:51,000
Once there was a pleasure, it was all very annoying and frustrating and tedious, but

14
00:00:51,000 --> 00:00:52,000
somebody had to do it.

15
00:00:52,000 --> 00:00:56,960
Somebody had to do it, especially somebody with your profile, I think, Julie drives home

16
00:00:56,960 --> 00:00:59,120
the message.

17
00:00:59,120 --> 00:01:03,920
So we tend to introduce people for them and then ask people to fill in the blanks on the

18
00:01:03,920 --> 00:01:06,280
personal side.

19
00:01:06,280 --> 00:01:10,800
Something I did not know about you was that you graduated with a B in philosophy from the

20
00:01:10,800 --> 00:01:12,640
University of Melbourne.

21
00:01:12,640 --> 00:01:14,640
I assumed you had a PhD.

22
00:01:14,640 --> 00:01:23,080
No, I barely got through my BA because I was working 80 to 100 hour weeks at McKinsey and

23
00:01:23,080 --> 00:01:27,760
a company from 19 years old onwards.

24
00:01:27,760 --> 00:01:35,760
So I actually didn't attend any lectures in second and third year university.

25
00:01:35,760 --> 00:01:39,680
Well, I guess you didn't need it or you're very sort of self-driven and self-motivated.

26
00:01:39,680 --> 00:01:47,720
I just took two weeks off before each exam period when I was working at McKinsey and

27
00:01:47,720 --> 00:01:50,560
then I can't believe I got away with this in hindsight.

28
00:01:50,560 --> 00:01:54,960
I would go to all my professors and say, oh, I was meant to be in your class this semester

29
00:01:54,960 --> 00:01:59,920
and I didn't quite turn up, were there any assignments I was meant to have done, whatever.

30
00:01:59,920 --> 00:02:05,520
I can't believe all of them let me basically, they basically always would say like, okay,

31
00:02:05,520 --> 00:02:08,560
well, if you can have this written by tomorrow, I'll accept it.

32
00:02:08,560 --> 00:02:11,960
So yeah, stressful way to get through university.

33
00:02:11,960 --> 00:02:17,840
Well, it shows that, I guess, you min-maxed the opportunities.

34
00:02:17,840 --> 00:02:18,840
That definitely was a tricker.

35
00:02:19,080 --> 00:02:27,160
I mean, finally, in philosophy, the things I found interesting and focused on in the

36
00:02:27,160 --> 00:02:31,000
little bit of time I did spend on it was ethics and cognitive science.

37
00:02:31,000 --> 00:02:36,440
And it's kind of really amazing that now come back around and those are actually genuinely

38
00:02:36,440 --> 00:02:39,040
useful things to know about, which I never thought would happen.

39
00:02:39,040 --> 00:02:43,240
A lot of relevant conversations there.

40
00:02:43,240 --> 00:02:48,640
So you were a consultant for a while and then in the magical month of June, 1999, you found

41
00:02:48,640 --> 00:02:52,280
it, both optimal decisions and fast meal, which I also really used.

42
00:02:52,280 --> 00:02:53,280
So thank you for that.

43
00:02:53,280 --> 00:02:54,280
Oh, good for you.

44
00:02:54,280 --> 00:02:55,280
Yeah.

45
00:02:55,280 --> 00:02:58,520
Because I had read the statistics switches at like 90% or something of small businesses

46
00:02:58,520 --> 00:02:59,520
fail.

47
00:02:59,520 --> 00:03:02,840
So I thought if I start two businesses, I have a higher chance.

48
00:03:02,840 --> 00:03:05,400
In hindsight, I was thinking of it as some kind of stochastic thing.

49
00:03:05,400 --> 00:03:10,720
I didn't have control over it, but it's a bit hard, but anyway.

50
00:03:10,720 --> 00:03:18,400
And then you were president and chief scientist at Kaggle, which obviously is the composition

51
00:03:18,400 --> 00:03:21,960
platform of machine learning.

52
00:03:21,960 --> 00:03:26,960
And then in Lytec, where you were working on using deep learning to improve medical diagnostics

53
00:03:26,960 --> 00:03:27,960
and clinical decisions.

54
00:03:27,960 --> 00:03:28,960
Yeah.

55
00:03:28,960 --> 00:03:30,920
That was actually the first company to use deep learning in medicine.

56
00:03:30,920 --> 00:03:33,560
So it kind of founded the field.

57
00:03:33,560 --> 00:03:36,800
And even now, that's still like a pretty early phase.

58
00:03:36,800 --> 00:03:42,120
And I actually heard you on your new podcast with Tanishk, where you went very, very deep

59
00:03:42,120 --> 00:03:46,560
into the stuff, the kind of work that he's doing, such a young prodigy at his age.

60
00:03:47,560 --> 00:03:49,760
Maybe he's too old to be called a prodigy now.

61
00:03:49,760 --> 00:03:50,760
X prodigy.

62
00:03:50,760 --> 00:03:53,680
No, I think he still counts.

63
00:03:53,680 --> 00:03:58,400
And anyway, just to round out the bio, you have a lot more other credentials, obviously.

64
00:03:58,400 --> 00:04:03,880
But most recently, you started Fast.AI, which is still, I guess, your primary identity with

65
00:04:03,880 --> 00:04:04,880
Rachel Thomas.

66
00:04:04,880 --> 00:04:05,880
So welcome.

67
00:04:05,880 --> 00:04:06,880
Thanks.

68
00:04:06,880 --> 00:04:07,880
Thank you.

69
00:04:07,880 --> 00:04:08,880
Yeah.

70
00:04:08,880 --> 00:04:13,480
Being a lot of public service there with getting people involved in AI, and I can imagine

71
00:04:13,480 --> 00:04:18,480
a better way to describe it than Fast.AI is you teach people from nothing to stable

72
00:04:18,480 --> 00:04:20,680
diffusion in seven weeks or something.

73
00:04:20,680 --> 00:04:21,680
And that's amazing.

74
00:04:21,680 --> 00:04:22,680
Yeah.

75
00:04:22,680 --> 00:04:23,680
Yeah.

76
00:04:23,680 --> 00:04:24,680
I mean, it's funny.

77
00:04:24,680 --> 00:04:27,400
When we started that, what was that like 2016 or something?

78
00:04:27,400 --> 00:04:31,740
The idea that deep learning was something that you could make more accessible was generally

79
00:04:31,740 --> 00:04:37,960
considered stupid, but everybody knew that deep learning was a thing that you got a math

80
00:04:37,960 --> 00:04:42,840
through a computer science PhD, you know, those one of five labs that could give you

81
00:04:42,840 --> 00:04:49,720
the appropriate skills, then you would join, yeah, basically from one of those labs, you

82
00:04:49,720 --> 00:04:52,920
might be able to write some papers.

83
00:04:52,920 --> 00:05:00,360
So yeah, the idea that normal people could use that technology to do good work was considered

84
00:05:00,360 --> 00:05:03,200
kind of ridiculous when we started it.

85
00:05:03,200 --> 00:05:06,080
And we weren't sure if it was possible either, but we kind of felt like we had to give it

86
00:05:06,080 --> 00:05:10,720
a go because the alternative was we were pretty sure that deep learning was on its

87
00:05:10,720 --> 00:05:18,800
way to becoming the most or one of the most important technologies in human history.

88
00:05:18,800 --> 00:05:24,200
And if the only people that could use it were a handful of computer science PhDs, that seemed

89
00:05:24,200 --> 00:05:29,440
like A, a big waste and B, kind of dangerous.

90
00:05:29,440 --> 00:05:33,880
And you know, well, I just wanted to know one thing on your bio that at Kaggle, you

91
00:05:33,880 --> 00:05:37,960
were also the top rank participant in both 2010 and 2011.

92
00:05:37,960 --> 00:05:42,080
So sometimes you see a lot of founders running companies that are not really in touch with

93
00:05:42,080 --> 00:05:46,680
the problem, but you were clearly building something that you knew a lot about, which

94
00:05:46,680 --> 00:05:48,280
is awesome.

95
00:05:48,280 --> 00:05:53,640
And even, yeah, talking about deep learning, you created, published a paper on ULM fit,

96
00:05:53,640 --> 00:05:58,400
which was kind of the predecessor to multitask learning and a lot of the groundwork that

97
00:05:58,400 --> 00:06:00,320
then went to into transformers.

98
00:06:00,320 --> 00:06:07,080
I read back on the paper and you turn this model AWD LSTM, which I did the math and it

99
00:06:07,080 --> 00:06:13,120
was like 24 to 33 million parameters, depending on what training data set you use today.

100
00:06:13,120 --> 00:06:17,280
That's kind of like not even small, it's like super small.

101
00:06:17,280 --> 00:06:22,920
What were some of the kind of like contrarian takes that you had at the time and maybe set

102
00:06:22,920 --> 00:06:28,320
the stage a little bit for the rest of the audience on what was kind of like the state

103
00:06:28,320 --> 00:06:32,280
of the art, so to speak at the time and what people were working towards.

104
00:06:32,280 --> 00:06:35,480
Yeah, the whole thing was a contrarian take, you know.

105
00:06:35,480 --> 00:06:41,840
So okay, so we started first AI, my wife and I, and we, yeah, so we're trying to think,

106
00:06:41,840 --> 00:06:43,720
okay, how do we make it more accessible?

107
00:06:43,720 --> 00:06:48,760
So when we started thinking about it, it was very 2015 and then 2016, we started doing

108
00:06:48,760 --> 00:06:49,760
something about it.

109
00:06:49,760 --> 00:06:50,760
Why is it inaccessible?

110
00:06:50,760 --> 00:06:57,000
Okay, well, A, no one knows how to do it other than a few number of people and then when

111
00:06:57,000 --> 00:07:00,280
we asked those few number of people, well, how do you actually get good results?

112
00:07:00,280 --> 00:07:04,320
They would say like, oh, it's like, you know, a box of tricks that aren't published.

113
00:07:04,320 --> 00:07:08,280
So you have to join one of the, you know, labs and learn the tricks.

114
00:07:08,280 --> 00:07:14,320
So a bunch of unpublished tricks, not much software around, but you know, thankfully

115
00:07:14,320 --> 00:07:21,200
there was Theano and, you know, rappers and particularly Lasagna, the rapper.

116
00:07:21,200 --> 00:07:28,160
But yeah, not much software around, not much in the way of data sets, you know, very hard

117
00:07:28,160 --> 00:07:33,440
to get started in terms of the compute, like how do you get that set up?

118
00:07:33,440 --> 00:07:38,000
So you know, everything was kind of inaccessible.

119
00:07:38,000 --> 00:07:45,280
And you know, as we started looking into it, we had a key insight which was like, you know,

120
00:07:46,040 --> 00:07:53,440
most of the compute and data for image recognition, for example, we don't need to do it.

121
00:07:53,440 --> 00:07:57,720
You know, there's this thing which nobody knows about, nobody talks about called transfer

122
00:07:57,720 --> 00:08:04,040
learning where you take somebody else's model where they already figured out like how to

123
00:08:04,040 --> 00:08:08,520
detect edges and gradients and corners and text and whatever else, and then you can fine

124
00:08:08,520 --> 00:08:11,400
tune it to do the thing you want to do.

125
00:08:11,400 --> 00:08:16,880
And we thought that's the key, that's the key to becoming more accessible in terms of

126
00:08:16,880 --> 00:08:19,200
compute and data requirements.

127
00:08:19,200 --> 00:08:24,480
So when we started FastAI, we focused from day one on transfer learning, lesson one,

128
00:08:24,480 --> 00:08:29,120
in fact, was transfer learning, literally lesson one, something not normally even mentioned

129
00:08:29,120 --> 00:08:30,120
in.

130
00:08:30,120 --> 00:08:36,520
I mean, there wasn't much in the way of courses, you know, basically, really the courses out

131
00:08:36,600 --> 00:08:42,120
there were PhD programs that had happened to have recorded their lessons, they would

132
00:08:42,120 --> 00:08:43,720
really mention it at all.

133
00:08:43,720 --> 00:08:48,360
We wanted to show how to do four things that seemed really useful, you know, work with

134
00:08:48,360 --> 00:08:55,200
vision, work with tables of data, work with kind of recommendation systems and collaborative

135
00:08:55,200 --> 00:08:59,640
filtering and work with text, because we felt like those four kind of modalities covered

136
00:08:59,640 --> 00:09:04,600
a lot of the stuff that, you know, are useful in real life.

137
00:09:04,680 --> 00:09:07,000
And no one was doing anything much useful with text.

138
00:09:07,000 --> 00:09:14,080
Everybody was talking about Word2Vec, you know, like King plus, Queen minus, woman and

139
00:09:14,080 --> 00:09:20,480
blah, blah, blah, and it was like cool experiments, but nobody was doing anything like useful

140
00:09:20,480 --> 00:09:21,480
with it.

141
00:09:21,480 --> 00:09:29,880
NLP was all like lamertization and stop words and topic models and diagrams and SPMs, and

142
00:09:29,880 --> 00:09:34,760
it was really academic and not practical.

143
00:09:34,760 --> 00:09:42,360
But I mean, to be honest, I've been thinking about this crazy idea for nearly 30 years

144
00:09:42,360 --> 00:09:49,040
since I had done cognitive science at university, where we talked a lot about the cells Chinese

145
00:09:49,040 --> 00:09:53,800
room experiment, this idea of like, what if there was somebody that could kind of like

146
00:09:53,800 --> 00:10:01,000
knew all of the symbolic manipulations required to answer questions in Chinese, but they didn't

147
00:10:01,000 --> 00:10:06,640
speak Chinese, they were kind of inside a room with no other way to talk to the outside

148
00:10:06,640 --> 00:10:10,080
world other than taking in slips of paper with Chinese written on them and then they

149
00:10:10,080 --> 00:10:15,280
do all their rules and then they pass back a piece of paper with Chinese back and this

150
00:10:15,280 --> 00:10:19,520
room with a person in is actually fantastically good at answering any question you give them

151
00:10:19,520 --> 00:10:28,320
written in Chinese, do they understand Chinese and is this something that's intelligently

152
00:10:28,320 --> 00:10:35,280
working with Chinese ever since that time, I'd say to me the most thoughtful and compelling

153
00:10:35,280 --> 00:10:43,080
philosophical response is yes, intuitively it feels like no, that's just because we

154
00:10:43,080 --> 00:10:49,760
can't imagine such a large kind of system, but if it looks like a duck and acts like

155
00:10:49,760 --> 00:10:54,160
a duck, it's a duck or to all intents and purposes.

156
00:10:54,160 --> 00:10:59,040
And so I always kind of thought, so this is basically a kind of analysis of the limits

157
00:10:59,040 --> 00:11:05,520
of text and I kind of felt like, yeah, if something could ingest enough text and could

158
00:11:05,520 --> 00:11:17,000
use the patterns it saw to then generate text in response to text, it could appear to be

159
00:11:17,000 --> 00:11:22,640
intelligent, whether that means it is intelligent or not is a different discussion and not one

160
00:11:22,640 --> 00:11:23,880
I find very interesting.

161
00:11:23,880 --> 00:11:28,960
Yeah, and then when I came across neural nets when I was about 20, I learned about the universal

162
00:11:28,960 --> 00:11:33,120
approximation theorem and stuff and I started thinking like, oh, I wonder if like a neural

163
00:11:33,120 --> 00:11:40,760
net could ever get big enough, take in enough data to be a Chinese room experiment.

164
00:11:40,760 --> 00:11:46,920
With that background and this kind of like interest in transfer learning, I'd been thinking

165
00:11:46,920 --> 00:11:50,320
about this thing for kind of 30 years and I thought like, oh, I wonder if we're there

166
00:11:50,320 --> 00:11:56,640
yet, because we have a lot of text, like I can literally download Wikipedia, which is

167
00:11:56,640 --> 00:11:58,480
a lot of text.

168
00:11:58,480 --> 00:12:05,160
And I thought, you know, how would something learn to kind of answer questions or respond

169
00:12:05,160 --> 00:12:06,160
text?

170
00:12:06,160 --> 00:12:08,240
And I thought, well, what if we used a language model?

171
00:12:08,240 --> 00:12:11,680
So language models are already a thing, you know, they were not a popular or well known

172
00:12:11,680 --> 00:12:12,680
thing, but they were a thing.

173
00:12:12,680 --> 00:12:17,080
But language models exist to this idea that you could train a model to fill in the gaps

174
00:12:17,080 --> 00:12:20,960
or actually in those days, it wasn't fill in the gaps, it was finish a string.

175
00:12:20,960 --> 00:12:27,480
In fact, Andre Kapathy did his fantastic RNN demonstration from this at a similar time

176
00:12:27,480 --> 00:12:33,240
where he showed like you can have it ingest Shakespeare and it will generate something

177
00:12:33,240 --> 00:12:35,440
that looks a bit like Shakespeare.

178
00:12:35,440 --> 00:12:43,080
I thought, okay, so if I do this at a much bigger scale using all of Wikipedia, what

179
00:12:43,200 --> 00:12:50,880
would it need to be able to do to finish a sentence in Wikipedia effectively, to do it

180
00:12:50,880 --> 00:12:52,560
quite accurately quite often?

181
00:12:52,560 --> 00:12:55,760
I thought, Jesus, it would actually have to know a lot about the world, you know, it

182
00:12:55,760 --> 00:12:59,160
would have to know that there is a world and that there are objects and that objects relate

183
00:12:59,160 --> 00:13:03,880
to each other through time and cause each other to react in ways and that causes proceed

184
00:13:03,880 --> 00:13:09,680
effects and that, you know, when there are animals and there are people and that people

185
00:13:09,760 --> 00:13:14,880
couldn't be in certain positions during certain timeframes and then you could, you know, all

186
00:13:14,880 --> 00:13:21,160
that together, you can then finish a sentence like this was signed into law in 2016 by US

187
00:13:21,160 --> 00:13:24,600
President X and it would fill in the gaps, you know.

188
00:13:24,600 --> 00:13:30,200
So that's why I tried to create a, what in those days was considered a big language model

189
00:13:30,200 --> 00:13:34,560
trained on the entirety on Wikipedia, which is that was a bit unheard of and my interest

190
00:13:34,560 --> 00:13:41,120
was not in, you know, just having a language model, my interest was in like, what latent

191
00:13:41,120 --> 00:13:51,480
capabilities would such a system have that would allow it to finish those kind of sentences?

192
00:13:51,480 --> 00:13:56,440
Because I was pretty sure based on our work with transfer learning and vision that I could

193
00:13:56,440 --> 00:14:02,280
then suck out those latent capabilities by transfer learning, you know, by fine-tuning

194
00:14:02,280 --> 00:14:04,320
it on a task data set or whatever.

195
00:14:04,320 --> 00:14:06,480
So we generated this three-step system.

196
00:14:06,480 --> 00:14:09,840
So step one was train a language model on a big corpus.

197
00:14:09,840 --> 00:14:15,720
Step two was fine-tune a language model on a more curated corpus and step three was further

198
00:14:15,720 --> 00:14:21,160
fine-tune that model on a task and of course that's why everybody still does today, right?

199
00:14:21,160 --> 00:14:29,000
That's what chat GPT is and so the first time I tried it within hours I had a new state

200
00:14:29,000 --> 00:14:34,200
of the art academic result on IMDB and I was like, holy shit, it does work.

201
00:14:34,760 --> 00:14:41,120
So you asked to what degree was this kind of like pushing against the established wisdom?

202
00:14:41,120 --> 00:14:46,160
Every way, like the reason it took me so long to try it was because I asked all my friends

203
00:14:46,160 --> 00:14:51,120
in NLP if this could work and everybody said no, it definitely won't work.

204
00:14:51,120 --> 00:14:55,400
It wasn't like, oh maybe, everybody was like, it definitely won't work.

205
00:14:55,400 --> 00:14:58,320
NLP is much more complicated than vision.

206
00:14:58,320 --> 00:15:01,760
Languages are much more vastly complicated to main, you know, and you've got problems

207
00:15:01,760 --> 00:15:05,560
like the grounding problem we know from like philosophy and theory of mind that it's actually

208
00:15:05,560 --> 00:15:07,560
impossible for it to work.

209
00:15:07,560 --> 00:15:10,720
So yeah, so don't waste your time.

210
00:15:10,720 --> 00:15:16,000
Jeremy, had people not tried because it was like too complicated to actually get the data

211
00:15:16,000 --> 00:15:20,200
and like set up the training or like were people just lazy and kind of like, hey, this

212
00:15:20,200 --> 00:15:21,200
is just not going to work.

213
00:15:21,200 --> 00:15:22,200
No, it was lazy.

214
00:15:22,200 --> 00:15:25,960
So like, so the person I thought at that time who, there were two people I thought at that

215
00:15:25,960 --> 00:15:31,280
time actually who were the strongest at language models were Stephen Merity and Alec Radford.

216
00:15:31,800 --> 00:15:38,520
And at the time I didn't know Alec, but I, after we had both, after I'd released ULM Fit and

217
00:15:38,520 --> 00:15:45,240
he had released GPT, I organized a chat for both of us with Kate Metz of the New York

218
00:15:45,240 --> 00:15:50,160
Times and Kate Metz answered, and Alec answered this question for Kate and Kate just like,

219
00:15:50,160 --> 00:15:53,880
so how did, you know, GPT come about?

220
00:15:53,880 --> 00:15:59,680
And he said, well, I was pretty sure that pre-training on a general large corpus wouldn't work.

221
00:15:59,680 --> 00:16:01,480
So I hadn't tried it.

222
00:16:01,480 --> 00:16:06,280
And then I read ULM Fit and turns out it did work.

223
00:16:06,280 --> 00:16:09,680
And so I did it, you know, bigger and it worked even better.

224
00:16:09,680 --> 00:16:15,160
And similar with Stephen, you know, I asked Stephen Merity, like, why don't we just find,

225
00:16:15,160 --> 00:16:19,120
you know, I'll take your AWDSTLM and like, trade it on all of Wikipedia and fine tune

226
00:16:19,120 --> 00:16:20,120
it.

227
00:16:20,120 --> 00:16:23,200
And he's kind of like, I don't think that's going to really lie.

228
00:16:23,720 --> 00:16:31,240
Like two years before, I did a very popular talk at KDD, the conference where everybody

229
00:16:31,240 --> 00:16:33,840
in NLP was in the audience.

230
00:16:33,840 --> 00:16:39,320
I recognized after faces, you know, and I told them all this, I'm sure transfer learning

231
00:16:39,320 --> 00:16:40,480
is the key.

232
00:16:40,480 --> 00:16:48,800
I'm sure ImageNet, you know, is going to be an NLP thing as well.

233
00:16:48,800 --> 00:16:53,920
And you know, everybody was interested and people asked me questions afterwards, but

234
00:16:53,920 --> 00:16:59,880
just, yeah, nobody followed up because everybody knew that it didn't work.

235
00:16:59,880 --> 00:17:08,520
I mean, even like, so we were scooped a little bit by Dye and Lee at Google.

236
00:17:08,520 --> 00:17:12,800
They had, I already, I didn't even realize this, it's just a bit embarrassing.

237
00:17:12,800 --> 00:17:17,840
They had already done a large language model and fine tuned it.

238
00:17:17,880 --> 00:17:23,160
But again, they didn't create a general purpose large language model on a general purpose

239
00:17:23,160 --> 00:17:24,160
corpus.

240
00:17:24,160 --> 00:17:28,400
They only ever tested a domain specific corpus.

241
00:17:28,400 --> 00:17:32,840
And I haven't spoken to Kwok actually about that, but I assume that the reason was the

242
00:17:32,840 --> 00:17:33,840
same.

243
00:17:33,840 --> 00:17:38,800
It probably just didn't occur to them that the general approach could work.

244
00:17:38,800 --> 00:17:43,680
So maybe it was that kind of 30 years of mulling over the, this whole Chinese room experiment

245
00:17:43,680 --> 00:17:46,960
that had convinced me that it probably would work.

246
00:17:46,960 --> 00:17:47,960
I don't know.

247
00:17:47,960 --> 00:17:48,960
Yeah.

248
00:17:48,960 --> 00:17:49,960
Interesting.

249
00:17:49,960 --> 00:17:54,680
I just dug up Alec announcement tweet from Tony 18.

250
00:17:54,680 --> 00:17:57,800
He said, inspired by Kobe, Elmo and Yola and Fit.

251
00:17:57,800 --> 00:18:02,320
We showed a single transformer language model can be fine tuned to a variety.

252
00:18:02,320 --> 00:18:07,600
It's interesting because, you know, today people think of the leader kind of like, kind of

253
00:18:07,600 --> 00:18:09,960
like the research lab pushing forward the field.

254
00:18:09,960 --> 00:18:13,240
What was that at the time, you know, like kind of like going back five years, people

255
00:18:13,320 --> 00:18:16,840
think of it as an overnight success, but obviously it took a while.

256
00:18:16,840 --> 00:18:17,840
Yeah.

257
00:18:17,840 --> 00:18:18,840
Yeah.

258
00:18:18,840 --> 00:18:19,840
No, I mean, absolutely.

259
00:18:19,840 --> 00:18:23,040
And I'll say like, it's interesting that it mentioned Elmo because in some ways that

260
00:18:23,040 --> 00:18:29,160
was kind of diametrically opposed to, to ULM fit, you know, there was these kind of like,

261
00:18:29,160 --> 00:18:34,120
so there was a lot of, there was a lot of activity at the same time as ULM fits release.

262
00:18:34,120 --> 00:18:41,480
So there was, so before it, as Brian McCann, I think at Salesforce had come out with this

263
00:18:41,520 --> 00:18:46,240
neat model that did a kind of multitask learning.

264
00:18:46,240 --> 00:18:50,800
But again, they didn't create a general fine tune language model first.

265
00:18:50,800 --> 00:18:56,040
There was Elmo, which I think was a little, you know, actually quite a few months after

266
00:18:56,040 --> 00:18:59,160
the first ULM fit example, I think.

267
00:19:00,040 --> 00:19:01,400
But yeah, there was a bit of this stuff going on.

268
00:19:01,400 --> 00:19:08,400
And the problem was everybody was doing, and particularly after GPT came out there and

269
00:19:08,400 --> 00:19:12,600
everybody wanted to focus on zero shot and few shot learning, you know, everybody hated

270
00:19:12,600 --> 00:19:14,840
fine tuning, everybody hated transfer learning.

271
00:19:14,840 --> 00:19:20,560
And like I literally did tours trying to get people to start doing transfer learning.

272
00:19:20,560 --> 00:19:27,040
And people, you know, nobody was interested, particularly after GPT showed such good results

273
00:19:27,040 --> 00:19:29,480
with zero shot and few shot learning.

274
00:19:29,480 --> 00:19:33,480
And so I actually feel like we kind of went backwards for years and, and not to be honest,

275
00:19:33,480 --> 00:19:40,160
I mean, I'm a bit sad about this now, but I kind of got so disappointed and dissuaded

276
00:19:40,160 --> 00:19:46,360
by like, it felt like these bigger lab, much bigger labs, you know, like fast AI had only

277
00:19:46,360 --> 00:19:52,880
ever been just me and Rachel were getting all of this attention for an approach I thought

278
00:19:52,880 --> 00:19:54,560
was the wrong way to do it.

279
00:19:54,560 --> 00:19:56,600
You know, I was convinced was the wrong way to do it.

280
00:19:56,600 --> 00:20:00,720
And so yeah, for years, people were really focused on getting better zero shot and few

281
00:20:00,720 --> 00:20:01,720
shot.

282
00:20:01,720 --> 00:20:06,960
And it wasn't until, you know, this key idea of like, well, let's take the ULM fit approach.

283
00:20:06,960 --> 00:20:13,400
But for step two, rather than fine tuning on a kind of a domain corpus, let's fine tune

284
00:20:13,400 --> 00:20:15,840
on an instruction corpus.

285
00:20:15,840 --> 00:20:20,720
And then in step three, rather than fine tuning on a reasonably specific task classification,

286
00:20:20,720 --> 00:20:25,320
let's fine tune on a, on a RLHF class classification.

287
00:20:25,320 --> 00:20:27,840
And so that was really, that was really key, you know.

288
00:20:27,840 --> 00:20:33,840
So I was kind of like out of the NLP field for a few years there, because yeah, it just

289
00:20:33,840 --> 00:20:42,360
felt like, I don't know, pushing uphill against this vast tide, which I was convinced was

290
00:20:42,360 --> 00:20:45,480
not the right direction, but he's going to listen to me, you know, because I, as you

291
00:20:45,480 --> 00:20:51,520
said, I don't have a PhD, not at a university, or at least it wasn't then I don't have a

292
00:20:51,520 --> 00:20:56,360
big set of computers to fine tune huge transformer models.

293
00:20:56,360 --> 00:20:58,540
So yeah, it was definitely difficult.

294
00:20:58,540 --> 00:20:59,540
It's always been hard.

295
00:20:59,540 --> 00:21:03,240
You know, it's always been hard, like I've always been somebody who does not want to

296
00:21:03,240 --> 00:21:11,240
build stuff on lots of big computers, because most people don't have lots of big computers.

297
00:21:11,240 --> 00:21:15,800
And I hate creating stuff that most people can't use, you know, and also stuff that's

298
00:21:15,800 --> 00:21:21,080
created on lots of big computers has always been like much more media friendly.

299
00:21:21,080 --> 00:21:25,440
So like, it might seem like a recent thing, but actually throughout my 30 years in data

300
00:21:25,440 --> 00:21:32,080
science, the attention's always been on, you know, the big iron results.

301
00:21:32,080 --> 00:21:36,400
So when I first started, everybody was talking about data warehouses, and it was all about

302
00:21:36,400 --> 00:21:42,440
teradata, and it'd be like, oh, this big bank has this huge room full of computers, and

303
00:21:42,440 --> 00:21:46,640
they have like terabytes of data available, you know, the press for button.

304
00:21:46,640 --> 00:21:52,720
And yeah, that's always what people want to talk about, what people want to write about.

305
00:21:52,720 --> 00:21:56,680
And then of course, students coming out of their PhDs and stuff, that's where they want

306
00:21:56,680 --> 00:21:59,920
to go work, because that's where they read about.

307
00:21:59,920 --> 00:22:07,680
And to me, it's a huge distraction, you know, because like I say, most people don't have

308
00:22:07,680 --> 00:22:15,960
unlimited compute, and I want to help most people, not the small subset of the most well-off

309
00:22:15,960 --> 00:22:16,960
people.

310
00:22:16,960 --> 00:22:17,960
Yeah.

311
00:22:17,960 --> 00:22:18,960
That's awesome.

312
00:22:19,120 --> 00:22:24,080
It's great to hear, you know, you do such a great job educating that a lot of times,

313
00:22:24,080 --> 00:22:28,400
you're not telling your own story, you know, so I love this conversation.

314
00:22:28,400 --> 00:22:33,240
And the other thing before we jump into FASTI, actually, you know, a lot of people that I

315
00:22:33,240 --> 00:22:37,440
know, they run across a new architecture and one other, like, I got to start a company

316
00:22:37,440 --> 00:22:41,320
and raise a bunch of money and do all of this stuff, instead you were like, I want everybody

317
00:22:41,320 --> 00:22:43,960
to have access to this.

318
00:22:43,960 --> 00:22:45,320
Why was that the case for you?

319
00:22:45,320 --> 00:22:49,640
Was it because you already had like a successful, you know, venture and like fast mail and you

320
00:22:49,640 --> 00:22:50,920
were more interested in that?

321
00:22:50,920 --> 00:22:51,920
What was the reasoning?

322
00:22:51,920 --> 00:22:54,320
That's a really good question.

323
00:22:54,320 --> 00:22:58,760
So I guess the answer is yes, that's the reason why.

324
00:22:58,760 --> 00:23:05,280
So when I was a teenager, I thought it would be really cool to like have my own company.

325
00:23:05,280 --> 00:23:08,520
You know, I didn't know the word startup, I didn't know the word entrepreneur, I didn't

326
00:23:08,520 --> 00:23:12,760
know the word VC, and I didn't really know what any of those things were really until

327
00:23:12,800 --> 00:23:16,000
after we started Kaggle, to be honest, even though I'd started to what would now call

328
00:23:16,000 --> 00:23:22,440
startups, I just thought they were just small businesses, you know, they were just companies.

329
00:23:22,440 --> 00:23:25,040
So yeah, so those two companies were fast mail and optimal decisions.

330
00:23:25,040 --> 00:23:31,360
Fast mail was the first kind of synchronized email provider for non-businesses, so something

331
00:23:31,360 --> 00:23:37,880
you can get your same email at home on your laptop that were on your phone, whatever.

332
00:23:37,880 --> 00:23:43,920
And then optimal decisions invented a new approach to insurance pricing, so they called

333
00:23:43,920 --> 00:23:46,400
profit optimized insurance pricing.

334
00:23:46,400 --> 00:23:55,800
So I saw both of those companies, you know, after 10 years, and at that point I had achieved

335
00:23:55,800 --> 00:24:01,640
the thing that as a teenager I wanted to do, you know, it took a lot longer than it should

336
00:24:01,640 --> 00:24:04,520
have because I spent way longer in management consulting than I should have because I got

337
00:24:04,560 --> 00:24:09,360
caught up in that stupid rat race, but you know, eventually I got there and I remember

338
00:24:09,360 --> 00:24:14,680
my mom saying to me, oh, you must be so proud, you know, because she remembered, my dreams

339
00:24:14,680 --> 00:24:17,120
is like, you've done it.

340
00:24:17,120 --> 00:24:23,520
And I kind of reflected and I was like, I'm not, I'm not proud at all, you know, like

341
00:24:23,520 --> 00:24:27,520
people quite liked fast mail, you know, it's quite nice to have synchronized email, it

342
00:24:27,520 --> 00:24:33,360
probably would have happened anyway, yeah, I'm certainly not proud that I've helped

343
00:24:33,400 --> 00:24:37,800
some insurance companies suck more money out of their customers.

344
00:24:37,800 --> 00:24:42,640
Yeah, no, I'm not proud, you know, it's this actually, I haven't really helped the world

345
00:24:42,640 --> 00:24:47,280
very much, you know, maybe in the insurance case, I've made it a little bit worse.

346
00:24:47,280 --> 00:24:48,880
I don't know.

347
00:24:48,880 --> 00:24:57,880
So yeah, I was determined to not waste more years of my life doing things, working hard

348
00:24:57,920 --> 00:25:03,400
to do things which I could not be reasonably sure would have a lot of value.

349
00:25:03,400 --> 00:25:08,680
So, you know, I took some time off, I wasn't sure if I'd ever work again, actually, I didn't

350
00:25:08,680 --> 00:25:14,320
particularly want to, because it felt like, yeah, it felt like such a disappointment.

351
00:25:14,320 --> 00:25:17,960
And but you know, and I didn't need to, I had enough money, like I wasn't super rich,

352
00:25:17,960 --> 00:25:20,400
but I had enough money, I didn't need to work.

353
00:25:20,400 --> 00:25:25,400
And I certainly recognized that amongst the other people, I knew who had enough money

354
00:25:25,440 --> 00:25:29,800
that they didn't need to work, they all worked ridiculously hard, you know, and constantly

355
00:25:29,800 --> 00:25:32,440
put themselves in extremely stressful situations.

356
00:25:32,440 --> 00:25:39,600
And I thought, I don't want to be one of those idiots who's tied to, you know, buying a bigger

357
00:25:39,600 --> 00:25:44,520
plane than the next guy or whatever, you know, Kaggle came along and I mainly kind of did

358
00:25:44,520 --> 00:25:48,640
that just because it was fun and interesting to hang out with interesting people.

359
00:25:49,400 --> 00:25:57,200
But, you know, with fast AI in particular, you know, Rachel and I had a very explicit, you

360
00:25:57,200 --> 00:26:02,680
know, long series of conversations over a long period of time about like, well, how can we be

361
00:26:02,680 --> 00:26:09,600
the most helpful to society as a whole, and particularly to those people who maybe need

362
00:26:09,600 --> 00:26:10,720
more help, you know.

363
00:26:11,240 --> 00:26:18,000
And so we definitely saw the world going in a potentially pretty dystopian direction, if

364
00:26:18,000 --> 00:26:23,440
the world's most powerful technology was controlled by a small group of elites.

365
00:26:25,440 --> 00:26:29,920
So we thought, yeah, we should focus on trying to help that not happen.

366
00:26:31,000 --> 00:26:33,360
You know, sadly, it looks like it still is likely to happen.

367
00:26:33,360 --> 00:26:38,280
But I mean, I feel like we've, we've helped make it a little bit less likely.

368
00:26:38,320 --> 00:26:39,720
So we've done our best.

369
00:26:39,720 --> 00:26:41,520
You've shown that it's possible.

370
00:26:41,640 --> 00:26:49,280
And I think, I think your constant advocacy, your courses, your research that you publish,

371
00:26:49,280 --> 00:26:55,760
you know, just the other day you published a signing on, you know, learning that I think

372
00:26:55,760 --> 00:26:58,800
is still something that people are still talking about quite a lot.

373
00:26:59,040 --> 00:27:05,000
I think that that is the origin story of a lot of people who are going to be, you know,

374
00:27:05,000 --> 00:27:08,240
little Jeremy Howard's sort of in your mission with, you know, you don't have to do

375
00:27:08,240 --> 00:27:09,640
everything by yourself is what I'm saying.

376
00:27:09,680 --> 00:27:10,880
Definitely, definitely.

377
00:27:10,880 --> 00:27:15,440
You know, that was a, that was a big takeaway from like, and Lydic was that Lydic, it

378
00:27:15,440 --> 00:27:17,800
definitely felt like we had to do everything ourselves.

379
00:27:17,920 --> 00:27:20,040
And I kind of, I wanted to solve medicine.

380
00:27:20,160 --> 00:27:22,720
I'll say, yeah, okay, solving medicine is actually quite difficult.

381
00:27:22,720 --> 00:27:25,400
And I can't do it on my own.

382
00:27:25,400 --> 00:27:27,880
And there's a lot of other things I'd like to solve, and I can't do those either.

383
00:27:27,880 --> 00:27:34,000
So that was, that was definitely the other piece was like, yeah, you know, can we create

384
00:27:34,000 --> 00:27:41,320
an army of passionate domain experts who can change their little part of the world.

385
00:27:41,800 --> 00:27:42,760
And that's definitely happened.

386
00:27:42,760 --> 00:27:50,120
Like I find nowadays, at least half the time, probably quite a bit more, that I get in

387
00:27:50,120 --> 00:27:53,920
contact with somebody who's done really interesting work in some domain.

388
00:27:54,160 --> 00:27:57,320
Most of the time, I'd say, they say, yeah, I got my start with fast AI.

389
00:27:58,360 --> 00:28:00,400
So it's definitely, I can, I can see that.

390
00:28:00,400 --> 00:28:06,760
And I also know from talking to folks at places like Amazon and Adobe and stuff, which, you

391
00:28:06,760 --> 00:28:07,960
know, there's lots of alumni there.

392
00:28:07,960 --> 00:28:11,360
And they say, oh my God, I got here in like half of the people who are fast AI alumni.

393
00:28:12,240 --> 00:28:13,840
So it's fantastic.

394
00:28:14,640 --> 00:28:18,600
Yeah, actually, Andre Capati grabbed me when I saw him at Europe's a few years ago.

395
00:28:18,600 --> 00:28:21,320
And he's like, I have to tell you thanks to the fast AI courses.

396
00:28:21,320 --> 00:28:24,720
When people come to Tesla and they need to know more about deep learning, we always

397
00:28:24,720 --> 00:28:25,720
send them to your course.

398
00:28:26,440 --> 00:28:29,280
And the OpenAI scholars program was doing the same thing.

399
00:28:29,640 --> 00:28:37,480
So it's kind of like, yeah, it's had a surprising impact, you know, that's just one

400
00:28:37,480 --> 00:28:43,320
of like three things we do is the course, you know, and it's, it's, it's only ever

401
00:28:43,320 --> 00:28:47,160
been at most two people, either me and Rachel or me and Silver.

402
00:28:47,200 --> 00:28:48,160
Nowadays, it's just me.

403
00:28:49,200 --> 00:28:53,560
So, yeah, I think it shows you don't necessarily need a huge amount of money and a

404
00:28:53,560 --> 00:28:56,840
huge team of people to, to make an impact.

405
00:28:57,760 --> 00:28:58,040
Yeah.

406
00:28:58,920 --> 00:29:04,600
So just to reintroduce fast AI for people who may not have dived into it much.

407
00:29:04,960 --> 00:29:07,080
There is the courses that you do.

408
00:29:07,440 --> 00:29:12,160
There is the library that is, that is very well loved.

409
00:29:12,160 --> 00:29:17,200
And I kind of think of it as a nicer layer on top of PyTorch that people should

410
00:29:17,200 --> 00:29:21,080
start with by default and use it as the basis for a lot of your courses.

411
00:29:22,280 --> 00:29:26,760
And then you have, you have like NB Dev, which I don't know, is that the third

412
00:29:26,760 --> 00:29:32,520
one? Oh, so the three areas were research, software, and, and courses.

413
00:29:32,560 --> 00:29:33,000
Oh, sorry.

414
00:29:33,000 --> 00:29:33,520
I was going by.

415
00:29:33,520 --> 00:29:41,960
So then in software, you know, fast AI is the main thing, but NB Dev is not far

416
00:29:41,960 --> 00:29:49,240
behind, but then there's also things like Fastcore, GHAPI, I mean, dozens of open

417
00:29:49,240 --> 00:29:55,360
source projects that I've created and some of them have been pretty popular.

418
00:29:55,360 --> 00:29:57,320
And some of them are still a little bit hidden.

419
00:29:57,320 --> 00:30:01,040
Actually, I should, some of them I should try to do a better job of telling people

420
00:30:01,040 --> 00:30:02,040
about. What are you, what are you thinking about?

421
00:30:02,600 --> 00:30:02,840
Yeah.

422
00:30:02,840 --> 00:30:07,840
What, what's on this little things like, for example, for working with EC2 and AWS,

423
00:30:07,840 --> 00:30:12,360
I created a fast EC2 library, which I think is like way more convenient and nice

424
00:30:12,360 --> 00:30:13,880
to use than anything else out there.

425
00:30:14,400 --> 00:30:17,800
And it's literally got a whole autocomplete dynamic autocomplete that works

426
00:30:17,800 --> 00:30:21,440
both on the command line and in notebooks, sort of like autocomplete your

427
00:30:21,440 --> 00:30:23,920
instance names and everything like that.

428
00:30:24,120 --> 00:30:25,480
You know, just little things like that.

429
00:30:25,480 --> 00:30:32,400
I try to make like, when I work with some domain, I try to make it like, I want

430
00:30:32,400 --> 00:30:35,320
to make it as enjoyable as possible for me to do that.

431
00:30:35,640 --> 00:30:40,680
So I always try to kind of like, like with GHAPI, for example, I think that

432
00:30:40,680 --> 00:30:45,640
GitHub API is incredibly powerful, but I didn't find it good to work with

433
00:30:45,640 --> 00:30:47,680
because I didn't particularly like the libraries that are out there.

434
00:30:47,680 --> 00:30:52,920
So like GHAPI, like Fast EC2, it like autocompletes both at the command

435
00:30:52,920 --> 00:30:57,800
line or in a notebook or whatever, like literally the entire GitHub API.

436
00:30:59,680 --> 00:31:03,520
The entire thing is like, I think it's like less than a hundred K of code

437
00:31:03,640 --> 00:31:09,480
because it actually, as far as I know, the only one that grabs it directly

438
00:31:09,480 --> 00:31:13,440
from the official open API spec that GitHub produces.

439
00:31:14,160 --> 00:31:21,480
And like, if you're in GitHub and you just type an API, you know, autocomplete

440
00:31:21,720 --> 00:31:28,840
API method and it enter, it prints out the docs or the six brief docs and

441
00:31:28,840 --> 00:31:32,640
then gives you a link to the actual documentation page, you know, GitHub

442
00:31:32,640 --> 00:31:36,480
actions I can write now in Python, which is just so much easier than writing

443
00:31:36,480 --> 00:31:38,760
them in typescript and stuff.

444
00:31:38,760 --> 00:31:40,840
So, you know, just little things like that.

445
00:31:41,120 --> 00:31:44,800
I think that's a approach that more, I wish more developers took to publish

446
00:31:45,080 --> 00:31:46,200
some of the work along the way.

447
00:31:47,200 --> 00:31:51,000
You describe the third arm of Fast EIS research.

448
00:31:51,120 --> 00:31:52,960
It's not something I see often.

449
00:31:52,960 --> 00:31:58,080
Obviously, you do do some research and how do you run your research?

450
00:31:58,240 --> 00:31:59,440
What are your research interests?

451
00:31:59,840 --> 00:32:00,120
Yeah.

452
00:32:00,120 --> 00:32:03,240
So research is what I spend the vast majority of my time on.

453
00:32:04,120 --> 00:32:11,400
And the artifacts that come out of that are largely software and courses, you

454
00:32:11,400 --> 00:32:15,640
know, so to me, the main artifact shouldn't be papers because papers are

455
00:32:15,640 --> 00:32:18,120
things read by a small exclusive group of people.

456
00:32:18,200 --> 00:32:22,760
You know, to me, the main artifacts should be like something teaching you

457
00:32:22,760 --> 00:32:25,800
people, here's how to use this insight and here's software you can use that

458
00:32:25,920 --> 00:32:27,160
builds it in.

459
00:32:29,320 --> 00:32:33,000
So I think I've only ever done three first person papers in my life, you know,

460
00:32:33,160 --> 00:32:37,320
and they were, and none of those are ones I wanted to do, you know, they were all

461
00:32:37,320 --> 00:32:41,520
once like, so one was ULM fit where Sebastian Ruda reached out to me after

462
00:32:41,520 --> 00:32:45,160
seeing the course and said, like, you have to publish this as a paper, you know.

463
00:32:45,720 --> 00:32:48,160
And he said, I'll write it.

464
00:32:50,280 --> 00:32:51,320
He said, I want to write it.

465
00:32:51,320 --> 00:32:53,600
Cause if I do, I can put it on my PhD and that would be great.

466
00:32:53,600 --> 00:32:57,280
And it's like, okay, well, I want to help you with your PhD and that sounds great.

467
00:32:57,280 --> 00:33:03,520
So like, you know, one was the masks paper, which just had to exist and nobody

468
00:33:03,520 --> 00:33:04,640
else was writing it.

469
00:33:04,760 --> 00:33:13,920
And then the third was the fast AI library paper, which again, somebody reached

470
00:33:13,920 --> 00:33:16,440
out and said, please, please write this.

471
00:33:16,440 --> 00:33:21,280
We will waive the fee for the journal and everything and actually help you get it

472
00:33:21,280 --> 00:33:22,440
through publishing and stuff.

473
00:33:22,480 --> 00:33:26,520
So yeah, so I don't, other than that, I've never written a first author paper.

474
00:33:27,200 --> 00:33:32,960
So the research is like, well, so for example, you know, Don Bench was a

475
00:33:32,960 --> 00:33:36,240
competition which Stanford ran a few years ago.

476
00:33:38,000 --> 00:33:41,880
It was kind of the first big competition of like, who couldn't train

477
00:33:41,960 --> 00:33:44,960
neural nets the fastest rather than the most accurate.

478
00:33:45,960 --> 00:33:51,760
And specifically it was who couldn't train ImageNet the fastest.

479
00:33:52,760 --> 00:33:57,000
And this was like one of these things where it was created by necessity.

480
00:33:57,360 --> 00:33:59,600
So Google had just released their TPUs.

481
00:34:00,200 --> 00:34:04,680
And so I heard from my friends at Google that they had put together this big team

482
00:34:04,920 --> 00:34:09,800
to smash Don Bench so that they could prove to people that they had to use

483
00:34:10,000 --> 00:34:13,240
Google Cloud and use their TPUs and show who could their TPUs were.

484
00:34:14,040 --> 00:34:17,200
And we kind of thought, oh, shit, this would be a disaster if they do that

485
00:34:17,200 --> 00:34:20,120
because then everybody's going to be like, oh, deep learning is not accessible.

486
00:34:20,520 --> 00:34:23,240
You know, to actually be good at it, you have to be Google and you have to

487
00:34:23,240 --> 00:34:24,680
use special silicon and so.

488
00:34:25,040 --> 00:34:29,360
So, you know, we, we only found out about this 10 days before the competition finished.

489
00:34:30,120 --> 00:34:35,320
But, you know, we basically got together an emergency bunch of our students and

490
00:34:35,360 --> 00:34:41,640
Rachel and I and sat for the next 10 days and just tried to crunch through and

491
00:34:42,520 --> 00:34:47,240
tried to use all of our best ideas that had come from our research.

492
00:34:48,320 --> 00:34:52,400
That's a particularly progressive resizing, which is basically train mainly on small things.

493
00:34:53,480 --> 00:34:57,200
Train on non-square things, you know, stuff like that.

494
00:34:57,600 --> 00:35:00,160
And so, yeah, we ended up winning.

495
00:35:00,960 --> 00:35:01,480
Thank God.

496
00:35:02,080 --> 00:35:06,480
And so, you know, we turned it around from being like, oh, shit, you know, this is

497
00:35:06,480 --> 00:35:09,440
going to show that you have to be Google and have TPUs to being like, oh, my God,

498
00:35:09,440 --> 00:35:11,720
even the little guy can do deep learning.

499
00:35:13,560 --> 00:35:18,360
So that's an example of the kind of like research artifacts we do.

500
00:35:18,840 --> 00:35:24,320
And yeah, so all of my research is always, how do we do more with less, you know,

501
00:35:24,320 --> 00:35:29,840
so how do we get better results with less data, with less compute, with less complexity.

502
00:35:30,520 --> 00:35:34,480
With less education, you know, stuff like that.

503
00:35:34,480 --> 00:35:37,560
So your LLM fits obviously a good example of that.

504
00:35:38,960 --> 00:35:42,680
And most recently you published, can LLMs learn from a single example?

505
00:35:43,960 --> 00:35:46,080
Maybe could you tell the story a little bit behind that?

506
00:35:46,080 --> 00:35:50,160
And maybe that goes a little bit too far into the learning on very low resource.

507
00:35:51,920 --> 00:35:52,760
The literature.

508
00:35:53,200 --> 00:35:53,880
Yeah.

509
00:35:54,520 --> 00:35:54,880
Yeah.

510
00:35:54,880 --> 00:36:01,320
So me and my friend, John O'Whittaker, basically had been playing around with

511
00:36:01,320 --> 00:36:05,240
this fun Kaggle competition, which is actually still running as we speak,

512
00:36:05,240 --> 00:36:13,280
which is, can you create a model which can answer multiple choice questions

513
00:36:13,280 --> 00:36:15,240
about anything that's in Wikipedia?

514
00:36:15,920 --> 00:36:22,400
And the thing that makes it interesting is that your model has to run on Kaggle

515
00:36:22,760 --> 00:36:26,040
within nine hours and Kaggle is very, very limited.

516
00:36:26,040 --> 00:36:31,680
So you've only got 14 gig RAM, only two CPUs and a small, very old GPU.

517
00:36:33,680 --> 00:36:36,720
So this is cool, you know, if you can do well at this, and this is a good example

518
00:36:36,720 --> 00:36:38,240
of like, oh, you can do more with less.

519
00:36:39,600 --> 00:36:45,640
So yeah, John O and I were playing around with fine tuning, of course, transfer

520
00:36:45,640 --> 00:36:48,120
learning, pre-trained language models.

521
00:36:49,160 --> 00:36:55,000
And we saw this like, so we always, you know, plot our losses as we go.

522
00:36:55,040 --> 00:36:58,120
So here's another thing we created, we actually, Sylvain Gougir, when he worked

523
00:36:58,120 --> 00:37:02,760
with us created called Fast Progress, which is kind of like TQEDM, but we think

524
00:37:02,760 --> 00:37:03,200
a lot better.

525
00:37:03,600 --> 00:37:07,480
So you look at our Fast Progress curves, and they kind of go down, down, down,

526
00:37:07,480 --> 00:37:09,720
down, down, down, a little bit, a little bit, a little bit, and then suddenly

527
00:37:09,720 --> 00:37:11,400
go clunk, and they drop.

528
00:37:12,000 --> 00:37:15,000
And then down, down, down, down, a little bit, and then suddenly clunk, they drop.

529
00:37:15,000 --> 00:37:16,560
We're like, what the hell?

530
00:37:16,560 --> 00:37:19,800
These clunks are occurring at the end of each epoch.

531
00:37:20,560 --> 00:37:27,040
So normally in deep learning, this would be, you know, I've seen this before,

532
00:37:27,040 --> 00:37:28,040
it's always been a bug.

533
00:37:28,680 --> 00:37:31,880
It's always turned out that like, oh, we accidentally forgot to turn on

534
00:37:31,880 --> 00:37:33,680
eval mode during the validation set.

535
00:37:33,680 --> 00:37:39,440
So I was actually learning then, or, oh, we accidentally were kept letting moving

536
00:37:39,440 --> 00:37:41,480
average statistics throughout the epoch.

537
00:37:41,520 --> 00:37:44,360
So, you know, for it's recent, the moving average or whatever.

538
00:37:44,360 --> 00:37:46,680
And so we were using hugging face trainer.

539
00:37:47,280 --> 00:37:50,680
So, you know, I did not give my friends at hugging face the benefit

540
00:37:50,680 --> 00:37:51,200
of the doubt.

541
00:37:51,200 --> 00:37:56,040
I thought, oh, they fucked up hugging face trainer, you know, idiots.

542
00:37:56,160 --> 00:37:58,560
Well, you'll use the faster trainer instead.

543
00:37:58,560 --> 00:37:59,840
So we switched over to learner.

544
00:37:59,840 --> 00:38:01,200
We still saw the clunks.

545
00:38:01,680 --> 00:38:08,120
And, you know, that's, yeah, it shouldn't really happen because semantically

546
00:38:08,120 --> 00:38:13,080
speaking in the epoch isn't like, it's not a thing, you know, like nothing

547
00:38:13,080 --> 00:38:17,240
happens or nothing's meant to happen when you go from ending one epoch to

548
00:38:17,240 --> 00:38:18,240
starting the next one.

549
00:38:20,480 --> 00:38:21,560
So there shouldn't be a clunk.

550
00:38:22,400 --> 00:38:25,280
You know, so I kind of asked around on the open source discords.

551
00:38:25,560 --> 00:38:28,880
That's like, what's going on here?

552
00:38:29,200 --> 00:38:31,600
And everybody was just like, oh, that's just what, that's just what these

553
00:38:31,600 --> 00:38:32,560
training curves look like.

554
00:38:32,720 --> 00:38:33,680
Those all look like that.

555
00:38:33,880 --> 00:38:34,520
Don't worry about it.

556
00:38:35,080 --> 00:38:37,360
That's like, oh, are you all using trainer?

557
00:38:37,520 --> 00:38:38,200
Yes.

558
00:38:38,320 --> 00:38:40,440
Oh, well, there must be some buck with training.

559
00:38:40,440 --> 00:38:43,160
And it's like, well, we also saw it in learner and somebody else is like, no,

560
00:38:43,160 --> 00:38:44,120
we've got our own trainer.

561
00:38:44,120 --> 00:38:44,880
We get it as well.

562
00:38:45,320 --> 00:38:46,240
They're just like, don't worry about it.

563
00:38:46,240 --> 00:38:47,200
It's just something we see.

564
00:38:47,920 --> 00:38:48,560
It's just normal.

565
00:38:49,040 --> 00:38:50,080
I can't do that.

566
00:38:50,080 --> 00:38:54,640
I can't just be like, here's something that's like in the previous 30 years of

567
00:38:54,640 --> 00:38:56,520
neural networks, nobody ever saw it.

568
00:38:56,720 --> 00:38:57,960
And now suddenly we see it.

569
00:38:59,160 --> 00:39:00,120
So don't worry about it.

570
00:39:00,440 --> 00:39:02,240
I like, I just, I have to know why.

571
00:39:02,520 --> 00:39:06,440
Can I clarify this is, was everyone that you're talking to, were they all

572
00:39:06,440 --> 00:39:08,760
seeing it for the same data set or in different data sets?

573
00:39:08,880 --> 00:39:11,960
Data, different data sets, different trainers.

574
00:39:11,960 --> 00:39:14,360
They're just like, no, this is just, this is just what it looks like when

575
00:39:14,360 --> 00:39:15,640
you fine tune language models.

576
00:39:15,640 --> 00:39:16,320
Don't worry about it.

577
00:39:16,560 --> 00:39:21,360
You know, as I say, I hadn't seen it before, but I'd been kind of like, as I

578
00:39:21,360 --> 00:39:24,880
say, I, you know, I kept working on them for a couple of years after ULM fit.

579
00:39:24,880 --> 00:39:28,400
And then I kind of moved on to other things, partly out of frustration.

580
00:39:28,640 --> 00:39:35,040
So I hadn't been fine tuning, you know, um, I mean, Lamar's only been out for a

581
00:39:35,040 --> 00:39:35,760
few months, right?

582
00:39:35,760 --> 00:39:39,480
But I, I, I, I, I wasn't one of those people who jumped straight into it, you

583
00:39:39,480 --> 00:39:45,000
know, so I was relatively new to the kind of Lamar fine tuning world, or else

584
00:39:45,560 --> 00:39:49,400
these guys had been, you know, doing it since day one.

585
00:39:50,320 --> 00:39:53,080
Um, it's only a few months ago, but it's still quite a bit of time.

586
00:39:53,080 --> 00:39:55,880
So, so yeah, they're just like, no, this is all what we see.

587
00:39:56,280 --> 00:39:56,960
Don't worry about it.

588
00:39:57,960 --> 00:40:01,880
So yeah, I, I've got a very kind of like, I don't know, I've just got this

589
00:40:01,880 --> 00:40:03,680
brain where I have to know why things are.

590
00:40:04,200 --> 00:40:07,160
And so I kind of, I asked people like, well, why, why do you think it's happening?

591
00:40:07,160 --> 00:40:10,280
And they'd be like, oh, we're pretty obviously, cause it's like memorized the

592
00:40:10,280 --> 00:40:10,800
data set.

593
00:40:11,920 --> 00:40:14,520
It's just like, it can't be right.

594
00:40:14,520 --> 00:40:15,640
It's only seen it once.

595
00:40:15,680 --> 00:40:16,480
Like, look at this.

596
00:40:16,480 --> 00:40:24,040
The, the losses dropped by 0.3, 0.3, which is like, basically it knows the answer.

597
00:40:25,040 --> 00:40:30,000
Um, they're like, no, no, it's just, it is, it's just memorized the data set.

598
00:40:30,000 --> 00:40:30,880
So yeah.

599
00:40:30,880 --> 00:40:35,680
So look, John, when I did not discover this and John O and I did not come up with a

600
00:40:35,680 --> 00:40:39,040
hypothesis, you know, I guess we were just the ones, I guess, who had been around

601
00:40:39,040 --> 00:40:42,560
for long enough to recognize that like this, this isn't how it's meant to work.

602
00:40:42,920 --> 00:40:47,840
And so we, you know, and so we went back and like, okay, let's just run some

603
00:40:47,840 --> 00:40:50,840
experiments, you know, cause nobody since we've actually published anything about

604
00:40:50,840 --> 00:40:53,960
this, um, well, not quite true.

605
00:40:53,960 --> 00:40:56,480
Some people have published things, but nobody ever actually stepped back and

606
00:40:56,480 --> 00:40:59,840
said like, what the hell, you know, how can this be possible?

607
00:40:59,840 --> 00:41:00,600
Is it possible?

608
00:41:00,600 --> 00:41:01,480
Is this what's happening?

609
00:41:01,840 --> 00:41:05,520
And so yeah, we created a bunch of experiments where we basically predicted

610
00:41:05,520 --> 00:41:06,080
ahead of time.

611
00:41:06,080 --> 00:41:08,960
It's like, okay, if this hypothesis is correct, that it's memorized in the

612
00:41:08,960 --> 00:41:13,000
training set, then we ought to see blah under conditions, blah, but not only these

613
00:41:13,000 --> 00:41:13,600
conditions.

614
00:41:14,240 --> 00:41:17,280
And so we ran a bunch of experiments and all of them supported the hypothesis

615
00:41:18,280 --> 00:41:21,560
that it was memorizing the data set in a single thing at once.

616
00:41:22,120 --> 00:41:30,600
Um, and it's a pretty big data set, you know, um, which in hindsight, it's not

617
00:41:30,600 --> 00:41:35,240
totally surprising because the theory, remember, of the ULM fit theory was like

618
00:41:35,240 --> 00:41:40,240
what's kind of creating all these latent capabilities to make it easier for it to

619
00:41:40,240 --> 00:41:41,360
predict the next token.

620
00:41:41,880 --> 00:41:46,320
So if it's got all this kind of latent capability, it ought to also be really

621
00:41:46,400 --> 00:41:51,080
good at compressing new tokens because it can immediately recognize that it's

622
00:41:51,080 --> 00:41:52,680
like, oh, that's just a version of this.

623
00:41:53,920 --> 00:42:01,880
So it's, it's not so crazy, you know, but it is, um, it requires us to rethink

624
00:42:01,880 --> 00:42:06,760
everything because like, and nobody knows like, okay, so how do we fine tune

625
00:42:06,760 --> 00:42:07,320
these things?

626
00:42:07,320 --> 00:42:09,560
Because like, it doesn't even matter.

627
00:42:10,240 --> 00:42:11,320
Like maybe it's fine.

628
00:42:11,680 --> 00:42:14,760
Like maybe it's fine that it's memorized the data set after one go and you do a

629
00:42:14,760 --> 00:42:20,200
second go and okay, the validation loss is terrible because it's now really

630
00:42:20,200 --> 00:42:21,040
overconfident.

631
00:42:21,920 --> 00:42:22,480
That's fine.

632
00:42:22,600 --> 00:42:25,960
Don't, you know, don't keep telling people, don't track validation loss,

633
00:42:25,960 --> 00:42:29,920
track validation accuracy, um, because at least that, that will still be useful.

634
00:42:30,520 --> 00:42:34,400
Um, there's another thing that's got lost since ULM fit, nobody tracks accuracy

635
00:42:34,400 --> 00:42:35,600
of language models anymore.

636
00:42:36,400 --> 00:42:41,000
Um, but you know, it'll still keep learning and it does, it does keep improving.

637
00:42:41,240 --> 00:42:43,880
But is it worse?

638
00:42:44,240 --> 00:42:48,320
You know, like, is it like now that it's kind of memorized it, it's probably

639
00:42:48,320 --> 00:42:53,960
getting a less strong signal, you know, um, I don't know.

640
00:42:54,200 --> 00:42:56,720
So I still don't know how to fine tune language models properly.

641
00:42:56,720 --> 00:43:00,760
And I haven't found anybody who feels like they do, like nobody really knows

642
00:43:00,760 --> 00:43:05,760
whether this memorization thing is, it's probably a feature in some ways.

643
00:43:05,760 --> 00:43:07,760
There's probably some things that you can do usefully with it.

644
00:43:07,920 --> 00:43:13,560
It's probably, yeah, I have a feeling it's messing up training dynamics as well.

645
00:43:14,360 --> 00:43:17,480
It doesn't come at the cost of catastrophic forgetting as well, right?

646
00:43:17,480 --> 00:43:18,920
Like, which is the other side of the coin.

647
00:43:19,280 --> 00:43:25,640
Um, it does, um, to some extent, like, we know it does like look at CodeLama,

648
00:43:25,640 --> 00:43:26,280
for example.

649
00:43:26,280 --> 00:43:31,720
So CodeLama was a, I think it was like a 500 billion token fine tuning of

650
00:43:31,720 --> 00:43:36,800
CodeLama to using code and also pros about code that Meta did.

651
00:43:37,440 --> 00:43:43,040
And, um, honestly, they kind of blew it because CodeLama is good at coding,

652
00:43:43,040 --> 00:43:44,160
but it's bad at everything else.

653
00:43:44,680 --> 00:43:45,720
You know, and it used to be good.

654
00:43:46,120 --> 00:43:46,400
Yeah.

655
00:43:46,400 --> 00:43:50,320
I was pretty sure it was like, before they released it at me and lots of people

656
00:43:50,320 --> 00:43:53,240
in the open source discords were like, Oh my God, you know, we know this is

657
00:43:53,240 --> 00:43:53,600
coming.

658
00:43:53,600 --> 00:43:54,800
You're not going to say it's coming.

659
00:43:54,800 --> 00:43:58,800
I, I hope they kept at least like 50% long code data because otherwise

660
00:43:58,800 --> 00:43:59,960
it's going to forget everything else.

661
00:44:00,440 --> 00:44:07,000
And they didn't only like 0.3 of their 0.3% of their epochs were non-code data.

662
00:44:07,400 --> 00:44:08,840
So I did it, forgot everything else.

663
00:44:08,880 --> 00:44:12,040
So now it's good at code and it's bad at everything else.

664
00:44:12,840 --> 00:44:14,640
So we definitely have catastrophic forgetting.

665
00:44:14,640 --> 00:44:15,480
It's fixable.

666
00:44:15,520 --> 00:44:20,880
Just somebody has to do, you know, somebody, somebody has to spend their time

667
00:44:20,880 --> 00:44:24,280
training a model on a, a good mix of data.

668
00:44:24,360 --> 00:44:25,320
Like, so, okay.

669
00:44:25,320 --> 00:44:25,960
So here's the thing.

670
00:44:26,960 --> 00:44:32,840
Um, even though I originally created the three step approach that everybody

671
00:44:32,840 --> 00:44:36,560
now does, my view is it's actually wrong and we shouldn't use it.

672
00:44:37,000 --> 00:44:45,520
Um, um, and that's because people are using it in a way different to why I

673
00:44:45,520 --> 00:44:46,040
created it.

674
00:44:46,040 --> 00:44:50,560
You know, I created it thinking that the tasks specific models would be more

675
00:44:50,560 --> 00:44:54,760
specific, you know, it's like, Oh, this is like a sentiment classifier.

676
00:44:54,880 --> 00:45:00,960
This is an example of a task, you know, but the tasks now are like a, um, you

677
00:45:00,960 --> 00:45:04,040
know, RLHF, which is basically like answer questions that make people feel

678
00:45:04,040 --> 00:45:05,040
happy about your answer.

679
00:45:05,320 --> 00:45:09,280
So that's a much more general task and it's a really cool approach.

680
00:45:09,520 --> 00:45:14,880
And so we see, for example, RLHF also breaks models.

681
00:45:14,880 --> 00:45:20,880
Like, you know, like GPT for RLHDEFT, we know from kind of the, the work that

682
00:45:20,880 --> 00:45:25,720
Microsoft did, you know, the pre, the, the earlier less aligned version was better.

683
00:45:26,920 --> 00:45:29,840
Um, and these are all kind of examples of catastrophic forgetting.

684
00:45:30,200 --> 00:45:36,960
And so to me, the right way to do this is to fine-tune language models is to

685
00:45:36,960 --> 00:45:38,720
actually throw away the idea of fine-tuning.

686
00:45:38,880 --> 00:45:39,640
There's no such thing.

687
00:45:40,360 --> 00:45:41,800
There's only continued pre-training.

688
00:45:42,640 --> 00:45:47,280
Uh, and pre-training is something where from the very start, you try to include

689
00:45:47,280 --> 00:45:51,120
all the kinds of data that you care about, all the kinds of problems that you

690
00:45:51,120 --> 00:45:57,760
care about, instructions, exercises, code, general purpose document completion,

691
00:45:58,760 --> 00:45:59,200
whatever.

692
00:46:01,040 --> 00:46:06,440
And then as you train, you gradually curate that, you know, you gradually

693
00:46:06,440 --> 00:46:09,960
make that higher and higher quality and more and more specific to the kinds of

694
00:46:09,960 --> 00:46:10,840
tasks you want it to do.

695
00:46:12,080 --> 00:46:14,600
Um, but you never throw away any data.

696
00:46:14,840 --> 00:46:20,160
You always keep all of the data types there in reasonably high quantities.

697
00:46:20,640 --> 00:46:25,720
Um, you know, maybe the quality filter, you stop training on low-quality data.

698
00:46:26,240 --> 00:46:29,120
Cause that's probably fine to forget how to write badly, maybe.

699
00:46:29,840 --> 00:46:35,560
Um, so yeah, that's now my view is I think ULM fit is the wrong approach.

700
00:46:36,080 --> 00:46:41,120
Um, and that's why we're seeing a lot of these, uh, you know, so-called alignment

701
00:46:41,120 --> 00:46:45,720
tax and this view of like, oh, a model can't both code and do other things.

702
00:46:45,840 --> 00:46:47,920
You know, I think it's actually cause people are training them wrong.

703
00:46:49,240 --> 00:46:53,720
Well, I think you have a clear anti-laziness approach.

704
00:46:53,840 --> 00:46:57,760
I think other people are not as, uh, good hearted, you know, they're like,

705
00:46:57,760 --> 00:46:59,720
Hey, they told me this thing works.

706
00:46:59,880 --> 00:47:03,240
And if I release a model this way, people will appreciate it.

707
00:47:03,240 --> 00:47:06,440
I'll get promoted and I'll kind of make, make more money.

708
00:47:06,920 --> 00:47:07,760
Oh, absolutely.

709
00:47:08,280 --> 00:47:08,600
Yeah.

710
00:47:08,600 --> 00:47:09,440
And it's not just money.

711
00:47:09,440 --> 00:47:13,440
It's like, this is how citations work most, most badly, you know, so if you

712
00:47:13,440 --> 00:47:17,560
want to get cited, you need to write a paper that people in your field recognize

713
00:47:17,560 --> 00:47:21,640
as an advancement on things that we know are good.

714
00:47:22,160 --> 00:47:24,320
And so we've seen this happen again and again.

715
00:47:24,400 --> 00:47:29,200
So like I say, like zero shot and a few shot learning, everybody was writing about

716
00:47:29,200 --> 00:47:33,360
that or, you know, with, um, image generation, every, everybody just was writing

717
00:47:33,360 --> 00:47:37,880
about GANs, you know, and I was trying to say like, no, GANs are not the right approach.

718
00:47:37,880 --> 00:47:41,080
You know, when I showed again through research that we demonstrated in our

719
00:47:41,080 --> 00:47:47,400
videos, that you can do better than GANs much faster and with much less data.

720
00:47:48,320 --> 00:47:52,400
And nobody cared because again, like if you want to get published, you rewrite

721
00:47:52,400 --> 00:47:57,560
a GAN paper that slightly improves this part of GANs and this tiny field, you'll,

722
00:47:58,520 --> 00:48:03,600
you'll get published, you know, so it's, yeah, it's not set up for real innovation.

723
00:48:04,160 --> 00:48:12,120
Um, it's, you know, it's, again, it's really helpful for me, you know, have my own

724
00:48:12,120 --> 00:48:16,240
research lab with nobody telling me what to do and I don't even publish.

725
00:48:16,240 --> 00:48:18,040
So it doesn't matter if I get citations.

726
00:48:18,400 --> 00:48:21,360
So I just write what I think is actually matters.

727
00:48:21,800 --> 00:48:26,680
Um, I wish there was, and you know, it actually places like open AI, you know,

728
00:48:26,680 --> 00:48:28,520
the researchers there can do that as well.

729
00:48:29,160 --> 00:48:33,160
It's a shame, you know, I wish there was more academic, open,

730
00:48:33,280 --> 00:48:38,200
venues in which people can focus on like genuine innovation.

731
00:48:38,720 --> 00:48:44,000
Uh, Twitter, which is, uh, unironically has, has become a little bit of that form.

732
00:48:44,360 --> 00:48:47,880
Uh, I wanted to follow up on one thing that you mentioned, uh, which is that

733
00:48:47,880 --> 00:48:50,360
you checked around the open source discords.

734
00:48:50,720 --> 00:48:54,800
Uh, I don't know if it's, uh, to, uh, I don't know if it's a kosher to ask

735
00:48:54,800 --> 00:48:58,840
like what discords are lively, uh, or useful right now.

736
00:48:59,040 --> 00:49:03,560
Um, I think that something I definitely felt like I missed out on was the early

737
00:49:03,560 --> 00:49:06,840
days of Luther AI, which where, which is a fair hot bit.

738
00:49:07,200 --> 00:49:10,760
And, uh, you know, like what is the new Luther and you were, you actually

739
00:49:10,760 --> 00:49:14,000
shouted out the alignment lab AI discord in your blog posts.

740
00:49:14,000 --> 00:49:16,880
And it was the first time I even knew, like I saw them on Twitter and never

741
00:49:16,880 --> 00:49:18,840
knew they had a discord, never knew that there was actually

742
00:49:18,840 --> 00:49:22,240
substantive discussions going on in there and that you were an active member of it.

743
00:49:22,760 --> 00:49:23,040
Okay.

744
00:49:23,040 --> 00:49:23,280
Yeah.

745
00:49:23,320 --> 00:49:26,160
And then even then, if you do know about that, you go there, it'll look like

746
00:49:26,160 --> 00:49:27,000
it's totally dead.

747
00:49:27,400 --> 00:49:30,680
And that's because unfortunately, nearly all the discords, nearly all of

748
00:49:30,680 --> 00:49:36,320
the conversation happens in private channels, you know, um, and how does

749
00:49:36,320 --> 00:49:38,560
someone get into that world?

750
00:49:38,560 --> 00:49:42,720
Cause it's obviously very, very, um, instructive, right?

751
00:49:42,880 --> 00:49:46,280
You could just come to the first day I discord, which I'll be honest with you,

752
00:49:46,280 --> 00:49:52,000
it's less bustling than some of the others, but it's not terrible.

753
00:49:52,360 --> 00:49:56,760
And so like, at least, you know, to be fair, one of Emma's bustling

754
00:49:56,760 --> 00:49:57,720
channels is private.

755
00:49:59,080 --> 00:49:59,400
I guess.

756
00:50:01,120 --> 00:50:02,080
So I'm just thinking, why is that?

757
00:50:02,080 --> 00:50:03,560
It's just a nature of quality discussion, right?

758
00:50:04,040 --> 00:50:07,880
Yeah, I guess when I think about it, like, I didn't have any private

759
00:50:07,880 --> 00:50:09,760
discussions on a discord for years.

760
00:50:10,160 --> 00:50:14,840
Um, but there was a lot of people who came in with like, oh, I just had

761
00:50:14,840 --> 00:50:17,120
this amazing idea for AGI.

762
00:50:17,120 --> 00:50:21,640
If you just thought about like, if you imagine the AI is a brain and we, you

763
00:50:21,680 --> 00:50:23,680
know, this just, I don't want to talk about it.

764
00:50:23,840 --> 00:50:27,560
You know, I don't want to like, but you don't want to be dismissive or whatever.

765
00:50:27,560 --> 00:50:30,000
And it's like, oh, well, that's an interesting comment, but maybe you should

766
00:50:30,000 --> 00:50:33,040
like try training some models first to see if that aligns with your intuition.

767
00:50:33,040 --> 00:50:34,480
Like, oh, but how can I possibly learn?

768
00:50:34,480 --> 00:50:38,080
It's like, well, we have a course just actually spend time learning.

769
00:50:38,080 --> 00:50:39,120
Like, uh, yeah.

770
00:50:39,160 --> 00:50:39,480
Anyway.

771
00:50:40,120 --> 00:50:43,960
And there's like, okay, I know the people who always have good answers there.

772
00:50:43,960 --> 00:50:46,640
And so I created a private channel and put them all in it.

773
00:50:46,720 --> 00:50:50,280
And I got to admit, I, that's where I post more often because.

774
00:50:51,280 --> 00:50:57,520
There's much less, you know, flight of fancy views about how we could solve AGI,

775
00:50:57,520 --> 00:50:58,040
blah, blah, blah.

776
00:50:58,360 --> 00:51:02,880
So there is a bit of that, but having said that, like, I think the bar is pretty

777
00:51:02,880 --> 00:51:10,680
low, like if you join a discord and you can hit the, like participants or

778
00:51:10,680 --> 00:51:12,600
community or whatever button and you can see who's in it.

779
00:51:12,600 --> 00:51:16,320
And you'll see at the top who, who the admins or moderators or people in the dev

780
00:51:16,440 --> 00:51:25,120
role are and, uh, just DM one of them and say, like, oh, I, here's my GitHub.

781
00:51:25,600 --> 00:51:29,440
Well, here's some blog posts they wrote, you know, I'm interested in talking about

782
00:51:29,440 --> 00:51:34,120
this, you know, can I join the private channels and I've never heard of anybody

783
00:51:34,120 --> 00:51:40,320
saying no, I will say, you know, uh, a Luther's all pretty open.

784
00:51:40,840 --> 00:51:44,200
So you can do the Aleutha discord still, you know, one problem with your

785
00:51:44,200 --> 00:51:49,040
Luther discord is it's been going on for so long that it's like, it's very

786
00:51:49,040 --> 00:51:50,040
inside baseball.

787
00:51:50,320 --> 00:51:52,680
It's hard to, it's hard to get started.

788
00:51:53,080 --> 00:51:53,280
Yeah.

789
00:51:53,280 --> 00:51:58,760
Um, Kappa AI looks, I think it's all open.

790
00:51:59,840 --> 00:52:02,200
This just left a stability.

791
00:52:02,200 --> 00:52:03,160
That's more accessible.

792
00:52:03,760 --> 00:52:03,960
Yeah.

793
00:52:04,000 --> 00:52:11,000
Um, uh, there's also, uh, just recently, uh, now three search that

794
00:52:11,000 --> 00:52:16,160
does like the Hermes models and data set just, just opened, they've got some

795
00:52:16,160 --> 00:52:18,000
private channels, but it's pretty open.

796
00:52:18,000 --> 00:52:21,720
I think, uh, you mentioned alignment lab, that one, it's all the interesting

797
00:52:21,720 --> 00:52:22,800
stuff is on private channels.

798
00:52:22,800 --> 00:52:28,720
So just ask, um, if, if you know me, ask me, cause I've got admin on that one.

799
00:52:29,000 --> 00:52:34,920
There's also, yeah, uh, OS skunkworks, OS skunkworks AI is a good discord,

800
00:52:34,920 --> 00:52:37,760
which I think it's open.

801
00:52:38,880 --> 00:52:40,320
So they're, yeah, they're all pretty good.

802
00:52:40,520 --> 00:52:44,560
I don't want you to leak any, any, uh, you know, uh, discords that don't

803
00:52:44,560 --> 00:52:49,720
want any publicity, but we all want people, like we all want people.

804
00:52:49,720 --> 00:52:55,040
We just, we just want people who like want to build stuff, you know, um, rather

805
00:52:55,040 --> 00:53:01,520
than people who, and like, it's fine to not know anything as well, but if you

806
00:53:01,520 --> 00:53:04,360
don't know anything, but you want to tell everybody else what to do and how to

807
00:53:04,360 --> 00:53:05,200
do it, that's annoying.

808
00:53:05,520 --> 00:53:10,080
If you don't know anything and want to be told, like, here's a really small kind

809
00:53:10,080 --> 00:53:13,320
of task that as somebody who doesn't know anything, it's going to take you a

810
00:53:13,320 --> 00:53:15,560
really long time to do, but it would still be helpful.

811
00:53:15,880 --> 00:53:17,040
Then, and then you go and do it.

812
00:53:17,080 --> 00:53:17,640
That would be great.

813
00:53:18,200 --> 00:53:22,600
The truth is, yeah, like, I don't know, maybe 5% of people who come in with

814
00:53:22,600 --> 00:53:25,280
great enthusiasm and saying that they want to learn and they'll do anything.

815
00:53:25,280 --> 00:53:27,400
And then somebody says like, okay, here's some work you can do.

816
00:53:27,880 --> 00:53:29,240
Almost nobody does that work.

817
00:53:29,760 --> 00:53:36,040
So if you're somebody who actually does the work and follows up, you will massively

818
00:53:36,040 --> 00:53:36,800
stand out.

819
00:53:36,920 --> 00:53:40,920
That's an extreme rarity and everybody will then want to help you do more work.

820
00:53:41,080 --> 00:53:47,160
So, yeah, so just, um, yeah, just do work and people will want to support you.

821
00:53:47,920 --> 00:53:51,160
Our discord used to be referral only for a long time.

822
00:53:51,240 --> 00:53:55,120
We then have a public invite and then we opened it and they're kind of like

823
00:53:55,120 --> 00:53:55,840
channel gating.

824
00:53:56,280 --> 00:53:58,360
Um, yeah, a lot of people just want to do.

825
00:53:58,360 --> 00:54:00,600
I remember it used to be like, you know, a forum moderator.

826
00:54:00,880 --> 00:54:04,680
It's like people just want to do like drive by posting, you know, I'm like, they

827
00:54:04,680 --> 00:54:05,880
don't want to help the community.

828
00:54:05,880 --> 00:54:07,520
They just want to get their question answered.

829
00:54:07,760 --> 00:54:13,000
I mean, the funny thing is our, um, our forum community does not have any of that

830
00:54:13,000 --> 00:54:17,080
garbage, you know, there's something specific about the low latency thing.

831
00:54:17,080 --> 00:54:23,320
There were people like, they expect an instant answer and yeah, we're all somehow

832
00:54:23,320 --> 00:54:27,120
in a forum's tread where they know it's like they're forever.

833
00:54:28,040 --> 00:54:33,880
People are a bit more thoughtful, but then the forums are less active than they

834
00:54:33,880 --> 00:54:40,440
used to be because discord has got more popular, you know, so it's all a bit of

835
00:54:40,440 --> 00:54:45,520
a compromise, you know, running a healthy community is, yes, it's always a bit of a

836
00:54:45,520 --> 00:54:46,040
challenge.

837
00:54:46,480 --> 00:54:49,080
All right, we've got so many more things we want to dive in, but I don't want to

838
00:54:49,080 --> 00:54:50,240
keep you here for hours.

839
00:54:50,720 --> 00:54:52,960
Uh, this is not the, the lack of freedom in pockets.

840
00:54:52,960 --> 00:54:57,640
We always like to say, uh, one topic I would love to maybe chat a bit about is

841
00:54:57,760 --> 00:55:01,800
Mojo modular, you know, Chris Liner nominated you on the pockets.

842
00:55:01,880 --> 00:55:04,160
So, uh, we want to spend a little time there.

843
00:55:04,200 --> 00:55:08,080
You recently did a hacker's guide to language models and you ran through

844
00:55:08,080 --> 00:55:13,000
everything from quantized model to like smaller models, larger models and all of

845
00:55:13,000 --> 00:55:13,280
that.

846
00:55:13,640 --> 00:55:17,160
Um, but obviously modular is taking its own approach.

847
00:55:17,280 --> 00:55:18,560
Uh, yeah, we'll get you excited.

848
00:55:18,560 --> 00:55:22,320
I know you Chris have been talking about this for like years and a lot of the

849
00:55:22,320 --> 00:55:23,200
ideas you had, so.

850
00:55:23,680 --> 00:55:25,240
Yeah, yeah, yeah, absolutely.

851
00:55:25,240 --> 00:55:30,840
So I met Chris, I think it was at the first TensorFlow Dev Summit.

852
00:55:31,440 --> 00:55:35,280
And I don't think he had even like, I'm not sure if he'd even sufficiently

853
00:55:35,280 --> 00:55:37,480
started his employment with Google at that point.

854
00:55:37,520 --> 00:55:41,320
So I, I don't know, you know, certainly nothing had been mentioned.

855
00:55:42,560 --> 00:55:48,160
So I, you know, I admired him from afar with LLVM and Swift and whatever.

856
00:55:48,160 --> 00:55:53,200
And so I saw him walk into the courtyard at, at Google.

857
00:55:53,480 --> 00:55:56,440
It's just like, oh, man, Chris Lander.

858
00:55:57,600 --> 00:56:00,440
I wonder if he would lower his standards enough to talk to me.

859
00:56:01,280 --> 00:56:02,480
Was worth a try.

860
00:56:02,880 --> 00:56:05,800
So I caught up my carriage because like he, nobody was talking to him.

861
00:56:05,920 --> 00:56:08,480
He looked a bit lost and I wandered over and it's like, oh, you're Chris

862
00:56:08,480 --> 00:56:09,040
Latino, right?

863
00:56:09,040 --> 00:56:10,120
It's like, what are you doing here?

864
00:56:10,960 --> 00:56:12,440
And it's like, yeah, yeah, I am.

865
00:56:12,440 --> 00:56:13,600
I'm like, oh, I'm Jeremy Howard.

866
00:56:13,600 --> 00:56:15,800
It's like, oh, did you do some of this AI stuff?

867
00:56:15,800 --> 00:56:17,760
And I was like, yeah, yeah, I like this AI stuff.

868
00:56:18,520 --> 00:56:19,640
Are you doing AI stuff?

869
00:56:19,720 --> 00:56:22,520
It's like, well, I'm thinking about starting to do some AI stuff.

870
00:56:22,520 --> 00:56:23,640
Yeah, I think it's going to be cool.

871
00:56:23,640 --> 00:56:29,480
And so, well, so like I spent the next half hour just basically brain

872
00:56:29,520 --> 00:56:33,400
dumping all the ways in which AI was stupid to him and he listened

873
00:56:33,400 --> 00:56:37,800
patiently and I thought he probably wasn't even remember or care or whatever.

874
00:56:37,800 --> 00:56:42,840
But yeah, then I kind of like, I guess I re-caught up with him a few months

875
00:56:42,840 --> 00:56:45,600
later and it's like, I've been thinking about everything you said in that

876
00:56:45,600 --> 00:56:50,320
conversation and he like narrated back his response to every part of it, the

877
00:56:50,320 --> 00:56:51,520
projects he was planning to do.

878
00:56:51,520 --> 00:56:54,800
And it's just like, oh, this dude follows up, holy shit.

879
00:56:56,840 --> 00:56:58,240
And I was like, wow, okay.

880
00:56:58,240 --> 00:57:01,880
And he was like, yeah, so we're going to create this new thing called Swift

881
00:57:01,880 --> 00:57:05,080
for TensorFlow and it's going to be like, it's going to be a compiler

882
00:57:05,080 --> 00:57:08,280
with auto differentiation built in and blah, blah, blah, blah.

883
00:57:08,280 --> 00:57:10,160
And I say, wait, why would that help?

884
00:57:10,160 --> 00:57:12,840
You know, why would you, and he was like, okay, with a compiler during the

885
00:57:12,840 --> 00:57:16,480
forward pass, you don't have to worry about saving context, you know,

886
00:57:16,480 --> 00:57:19,200
because a lot of it will be optimized in the backward, but I was like, oh my God.

887
00:57:19,800 --> 00:57:21,520
Because I didn't really know much about compilers.

888
00:57:22,040 --> 00:57:26,200
You know, I spent enough to kind of like understand the ideas, but it hadn't

889
00:57:26,200 --> 00:57:30,240
occurred to me that a compiler basically solves a lot of the problems

890
00:57:30,240 --> 00:57:31,920
we have as end users.

891
00:57:32,600 --> 00:57:33,880
I was like, wow, that's amazing.

892
00:57:33,880 --> 00:57:37,560
Okay, you do know, right, that nobody's going to use this unless it's like usable.

893
00:57:37,880 --> 00:57:39,680
It's like, yeah, I know, right?

894
00:57:39,680 --> 00:57:42,240
So I was thinking you should create like a fast AI for this.

895
00:57:42,360 --> 00:57:46,240
So, okay, but I don't even know Swift.

896
00:57:46,400 --> 00:57:49,840
And it's like, well, why don't you start learning it?

897
00:57:50,040 --> 00:57:52,040
And if you have any questions, ask me.

898
00:57:52,640 --> 00:57:53,720
It's just like, holy shit.

899
00:57:53,800 --> 00:57:58,520
Like, not only is Chris Latner lowered his standards enough to talk to me, but

900
00:57:58,520 --> 00:58:02,320
he's offering me personal tutoring on the programming language that he made.

901
00:58:02,760 --> 00:58:05,080
So I was just like, I'm not going to let him down.

902
00:58:05,640 --> 00:58:10,040
So I spent like the next two months, like just nerding out on Swift.

903
00:58:10,040 --> 00:58:15,560
And it was just before Christmas that I kind of like started writing down what I'd learned.

904
00:58:16,640 --> 00:58:21,680
So I wrote a couple of blog posts on like, okay, this is like my attempt to

905
00:58:21,680 --> 00:58:23,480
do a numeric programming in Swift.

906
00:58:24,240 --> 00:58:28,920
And these are all the challenges I had and the some of the issues I had with like

907
00:58:30,200 --> 00:58:32,200
making things properly performant.

908
00:58:32,320 --> 00:58:35,600
And here are some libraries I wrote and I sent it to Chris and I was like,

909
00:58:35,600 --> 00:58:39,120
I hope he's not too disappointed with me, you know, because that would be the worst.

910
00:58:39,920 --> 00:58:43,200
It's like, you know, and I was also like, I was like, I hope he doesn't

911
00:58:43,320 --> 00:58:47,080
dislike the fact that I've, you know, didn't love everything.

912
00:58:48,040 --> 00:58:50,960
And yeah, he was like, oh, thanks for sending me that.

913
00:58:50,960 --> 00:58:52,480
Let's get on a call and talk about it.

914
00:58:52,480 --> 00:58:54,520
And we spoke and he was like, this is amazing.

915
00:58:54,880 --> 00:58:56,520
I can't believe that you made this.

916
00:58:56,520 --> 00:58:58,200
This is exactly what Swift needs.

917
00:58:58,200 --> 00:59:02,440
And he was like, and so like somebody set up like a new Swift,

918
00:59:03,640 --> 00:59:06,280
I don't remember what they call them, the equivalent of a pep, you know,

919
00:59:06,280 --> 00:59:09,680
kind of IFC thing of like, oh, you know, let's look at how we can implement

920
00:59:09,680 --> 00:59:11,280
Jeremy's ideas in the language.

921
00:59:11,280 --> 00:59:13,440
And he's just like, oh, wow.

922
00:59:13,720 --> 00:59:21,200
And so, yeah, you know, so, you know, and then we ended up like literally

923
00:59:21,200 --> 00:59:24,920
teaching some lessons together about Swift for TensorFlow.

924
00:59:24,920 --> 00:59:32,280
And we built a fast AI kind of equivalent with him and his team.

925
00:59:32,280 --> 00:59:33,240
It's so much fun.

926
00:59:34,440 --> 00:59:37,640
Then in the end, you know, Google didn't follow through just fair enough,

927
00:59:37,640 --> 00:59:42,920
like asking everybody to learn a new programming language is going to be tough.

928
00:59:42,960 --> 00:59:46,280
But like it was very obvious, very, very obvious at that time that TensorFlow

929
00:59:46,280 --> 00:59:52,720
2 is going to be a failure, you know, and so this felt like, okay, I, you know,

930
00:59:53,880 --> 00:59:55,080
well, you know, what are you going to do?

931
00:59:55,400 --> 01:00:01,960
Like, you can't focus on TensorFlow 2 because it's not going to, like it's

932
01:00:01,960 --> 01:00:02,480
not working.

933
01:00:02,480 --> 01:00:06,680
It's never going to work, you know, nobody at Google is using it internally.

934
01:00:07,680 --> 01:00:12,680
So, you know, in the end, Chris left, you know, Swift for TensorFlow got archived.

935
01:00:13,680 --> 01:00:15,160
There was no backup plan.

936
01:00:15,160 --> 01:00:20,680
So I kind of felt like Google was kind of screwed, you know, and Chris went and

937
01:00:20,680 --> 01:00:23,680
did something else, but we kept talking and I was like, look, Chris, you know,

938
01:00:25,680 --> 01:00:28,680
you've got to be your own boss, man, because like, you know, you've got the ideas,

939
01:00:29,680 --> 01:00:33,680
you know, like only you've got the ideas, you know, and if your ideas are implemented,

940
01:00:34,680 --> 01:00:40,680
we'd all be so much better off because like, Python's the best of a whole bunch

941
01:00:40,680 --> 01:00:45,680
of shit, you know, like, I would, it's amazing, but it's awful, you know, compared

942
01:00:45,680 --> 01:00:46,680
to what it could be.

943
01:00:47,680 --> 01:00:51,680
Anyway, so eventually a few years later, he called me up and he was like, Jeremy,

944
01:00:51,680 --> 01:00:52,680
I've taken your advice.

945
01:00:53,680 --> 01:00:57,680
I've started a company so much like, oh my God, so we got to create a new language.

946
01:00:57,680 --> 01:00:59,680
We're going to create a new infrastructure.

947
01:00:59,680 --> 01:01:02,680
It's going to build, it's going to have all the stuff we've talked about.

948
01:01:03,680 --> 01:01:04,680
And it's like, oh, wow.

949
01:01:05,680 --> 01:01:08,680
So that's what Mojo is.

950
01:01:09,680 --> 01:01:17,680
And so Mojo is like, you know, building on all the stuff that Chris has figured out

951
01:01:17,680 --> 01:01:23,680
over, I mean, really from when he did his PhD thesis, which developed LLVM onwards,

952
01:01:23,680 --> 01:01:30,680
you know, in Swift and MLAR, you know, the TensorFlow runtime engine, which is very good.

953
01:01:30,680 --> 01:01:33,680
You know, that was something that he built and has lasted.

954
01:01:35,680 --> 01:01:37,680
So yeah, I'm pumped about that.

955
01:01:37,680 --> 01:01:39,680
I mean, it's very speculative.

956
01:01:39,680 --> 01:01:41,680
Creating a whole new language is tough.

957
01:01:41,680 --> 01:01:46,680
I mean, Chris has done it before and he's created a whole C++ compiler amongst other things.

958
01:01:47,680 --> 01:01:48,680
Looking pretty hopeful.

959
01:01:49,680 --> 01:01:53,680
I mean, I hope it works because, you know, I mean.

960
01:01:53,680 --> 01:01:55,680
You're doing them to quit his job, so.

961
01:01:56,680 --> 01:02:02,680
And I mean, in the meantime, I will say, you know, Google now does have a backup plan, you know,

962
01:02:02,680 --> 01:02:05,680
they have JAX, which was never a strategy.

963
01:02:05,680 --> 01:02:10,680
It was just a bunch of people who also recognized TensorFlow 2 as shit and they just decided to build something else.

964
01:02:11,680 --> 01:02:15,680
And for years, my friends in that team were like, don't tell anybody about us because we, you know,

965
01:02:15,680 --> 01:02:17,680
we don't want to be anything but a research project.

966
01:02:18,680 --> 01:02:23,680
So now these poor guys suddenly, they're the great white hope for Google's future.

967
01:02:24,680 --> 01:02:28,680
And so JAX is, you know, also not terrible, but it's still written in Python.

968
01:02:28,680 --> 01:02:37,680
Like it would be cool if we had all the benefits of JAX but in a language that was designed for those kind of purposes.

969
01:02:39,680 --> 01:02:45,680
So, you know, fingers crossed that, yeah, that Mocho turns out great.

970
01:02:45,680 --> 01:02:46,680
Yeah.

971
01:02:47,680 --> 01:02:51,680
Any other thoughts on when, where people should be spending their time?

972
01:02:51,680 --> 01:02:57,680
So that's more the kind of language framework level than you have the, you know, GGML.

973
01:02:57,680 --> 01:03:01,680
Some of these are like quantization-focused kind of model-level things.

974
01:03:01,680 --> 01:03:03,680
Then you got the hardware people.

975
01:03:03,680 --> 01:03:04,680
It's like a whole other bucket.

976
01:03:05,680 --> 01:03:08,680
Yeah, what are some of the exciting stuff that you're excited about?

977
01:03:08,680 --> 01:03:17,680
Well, you won't be surprised to hear me say this, but I think fine-tuning, transfer learning is still a hugely underappreciated area.

978
01:03:17,680 --> 01:03:27,680
So today's zero-shot, few-shot learning equivalent is retrieval augmented generation, you know, RAG.

979
01:03:27,680 --> 01:03:31,680
Which is like, just like few-shot learning is a thing.

980
01:03:31,680 --> 01:03:32,680
Like it's a real thing.

981
01:03:32,680 --> 01:03:33,680
It's a useful thing.

982
01:03:33,680 --> 01:03:35,680
It's not a thing anybody would want to ignore.

983
01:03:35,680 --> 01:03:39,680
Why are people not spending at least as much effort on fine-tuning?

984
01:03:39,680 --> 01:03:45,680
You know, because, you know, RAG is like such an inefficient hack, really, isn't it?

985
01:03:45,680 --> 01:03:52,680
It's like, you know, segment up my data in some somewhat arbitrary way.

986
01:03:52,680 --> 01:03:55,680
Embed it, ask questions about that.

987
01:03:55,680 --> 01:04:03,680
You know, hope that my embedding model embeds questions in the same bedding space as a paragraph.

988
01:04:03,680 --> 01:04:08,680
Which obviously is not going to, if your question is like, if I've got a whole bunch of archive papers embeddings.

989
01:04:08,680 --> 01:04:17,680
And I asked like, what are all the ways in which we can make inference more efficient?

990
01:04:17,680 --> 01:04:26,680
The only paragraphs it'll find is like, if there's a review paper that says here's a list of ways to make, you know, inference more efficient.

991
01:04:26,680 --> 01:04:27,680
Doesn't have any of the specifics.

992
01:04:27,680 --> 01:04:34,680
No, it's not going to be like, oh, here's one way, here's one way, here's a different way in different papers, you know.

993
01:04:34,680 --> 01:04:46,680
Yeah, if you fine-tune a model, then all of that information is getting directly incorporated into the weights of your model in a much more efficient and nuanced way.

994
01:04:46,680 --> 01:04:49,680
And then you can use RAG on top of that.

995
01:04:49,680 --> 01:04:54,680
So I think that that's one area that's definitely underappreciated.

996
01:04:54,680 --> 01:05:00,680
And also the confluence of like, okay, how do you combine RAG and fine-tuning, for example.

997
01:05:00,680 --> 01:05:10,680
Something that I think a lot of people are uncertain about, and I don't expect you to know either, is that whether or not you can fine-tune new information in.

998
01:05:10,680 --> 01:05:16,680
And I think that that is the focus of some of your open questions and research.

999
01:05:16,680 --> 01:05:17,680
But of course you can, right?

1000
01:05:17,680 --> 01:05:18,680
Because it's additional pre-training.

1001
01:05:18,680 --> 01:05:22,680
Obviously you can, because there's no such thing as fine-tuning.

1002
01:05:22,680 --> 01:05:24,680
There's only continued pre-training.

1003
01:05:24,680 --> 01:05:29,680
So fine-tuning is pre-training, like they're literally the same thing.

1004
01:05:30,680 --> 01:05:33,680
So the knowledge got in there in the first place through pre-training.

1005
01:05:33,680 --> 01:05:37,680
So how could continuing to pre-train not put more knowledge in?

1006
01:05:37,680 --> 01:05:39,680
Like it's the same thing.

1007
01:05:40,680 --> 01:05:44,680
The problem is just we're really bad at it, because everybody's doing it dumb ways.

1008
01:05:44,680 --> 01:05:49,680
So it's a good question, and it's not just new knowledge, but new capabilities.

1009
01:05:50,680 --> 01:05:57,680
You know, I think like in my Hackers Guide to LL, into Hackers Guide to LLM's talk, I show simple.

1010
01:05:57,680 --> 01:06:05,680
I mean, it's a funny, that's a simple example, because it doesn't sound it, but like taking a pre-trained based model and getting it to generate SQL.

1011
01:06:05,680 --> 01:06:08,680
And it took 15 minutes to train on a single GPU.

1012
01:06:08,680 --> 01:06:15,680
You know, I think that might surprise people that that capability is actual fingertips.

1013
01:06:15,680 --> 01:06:20,680
And you know, because it was already there, it was just latent in the base model.

1014
01:06:20,680 --> 01:06:27,680
Really pushing the boundaries of what you can do with small models, I think is a really interesting question.

1015
01:06:27,680 --> 01:06:29,680
Like what can you do with a...

1016
01:06:29,680 --> 01:06:32,680
I mean, there isn't much in the way of good small models.

1017
01:06:32,680 --> 01:06:43,680
A really underappreciated one is a BTLM 3B, which is a like kind of 7B quality 3B model.

1018
01:06:44,680 --> 01:06:46,680
There's not much of the 1-2B range, sadly.

1019
01:06:46,680 --> 01:06:56,680
There are some code ones, but like the fact that there are some really good code ones in that 1-2B range shows you that that's a great size for doing complex tasks well.

1020
01:06:57,680 --> 01:07:03,680
There was PHY1 recently, which has been the subject of a little bit of discussion about whether they're trained on benchmarks.

1021
01:07:03,680 --> 01:07:05,680
Yeah, PHY1.5 as well.

1022
01:07:05,680 --> 01:07:09,680
So that's not a good model yet.

1023
01:07:10,680 --> 01:07:11,680
Why not?

1024
01:07:12,680 --> 01:07:20,680
So PHY1 in particular is good at doing a very specific thing, which is creating very small Python snippets.

1025
01:07:20,680 --> 01:07:21,680
The thing...

1026
01:07:21,680 --> 01:07:26,680
Okay, so like PHY1.5 has never read Wikipedia, for example.

1027
01:07:26,680 --> 01:07:30,680
So it doesn't know who Tom Cruise is, you know.

1028
01:07:30,680 --> 01:07:33,680
It doesn't know who anybody is.

1029
01:07:33,680 --> 01:07:35,680
He doesn't know about any movies.

1030
01:07:35,680 --> 01:07:41,680
It doesn't really know anything about anything because it's never read anything.

1031
01:07:41,680 --> 01:07:49,680
You know, it was trained on a nearly entirely synthetic data set, which is designed for it to learn reasoning.

1032
01:07:49,680 --> 01:07:57,680
And so it was a research project and a really good one, and it definitely shows us a powerful direction in terms of what you can do with synthetic data.

1033
01:07:57,680 --> 01:08:04,680
And wow, gosh, even these tiny models can get pretty good reasoning skills, pretty good math skills, pretty good toting skills.

1034
01:08:06,680 --> 01:08:11,680
But I don't know if it's a model you could necessarily build on.

1035
01:08:11,680 --> 01:08:14,680
Some people have tried to do some fine tunes of it.

1036
01:08:14,680 --> 01:08:24,680
And again, they're like surprisingly good in some ways for a 1.5B model, but not sure you'd find it useful for anything.

1037
01:08:24,680 --> 01:08:32,680
I think that's the struggle of pitching small models because small is great, you know, you don't need a lot of resources to run them.

1038
01:08:32,680 --> 01:08:35,680
But the performance evaluation is always so iffy.

1039
01:08:35,680 --> 01:08:40,680
It's always just like, yeah, it works on some things and we don't trust it for others.

1040
01:08:40,680 --> 01:08:43,680
Yeah, so that's why we're back to fine tuning.

1041
01:08:43,680 --> 01:08:50,680
I would say, so Microsoft did create a PHY1.5 web, but they didn't release it, unfortunately.

1042
01:08:51,680 --> 01:09:04,680
I would say a PHY1.5 web with fine tuning for your task might solve a lot of tasks that people have in their kind of day-to-day lives,

1043
01:09:06,680 --> 01:09:08,680
particularly in kind of an enterprise setting.

1044
01:09:08,680 --> 01:09:13,680
I think there's a lot of repetitive kind of processing that has to be done.

1045
01:09:13,680 --> 01:09:24,680
It's a useful thing for coders to know about because I think quite often you can replace some thousands and thousands of lines of complex buggy code maybe with a fine tune, you know.

1046
01:09:25,680 --> 01:09:26,680
Good.

1047
01:09:26,680 --> 01:09:27,680
Yeah.

1048
01:09:28,680 --> 01:09:33,680
And Jeremy, before we let you go, I think one question on top of a lot of people's minds.

1049
01:09:33,680 --> 01:09:39,680
So you've done practical deep learning for coders in 2018, 19, 21, 22.

1050
01:09:39,680 --> 01:09:43,680
I feel like the more time goes by, the more the GPUs get concentrated.

1051
01:09:44,680 --> 01:09:50,680
If you're somebody who's interested in deep learning today and you don't want to go join OpenAI, you don't want to join Anthropic,

1052
01:09:50,680 --> 01:09:53,680
what's like the best use of their time?

1053
01:09:53,680 --> 01:09:55,680
Should they focus on, yes, model development?

1054
01:09:55,680 --> 01:09:58,680
Should they focus on fine tuning math and all of that?

1055
01:09:58,680 --> 01:10:04,680
Should they just like focus on making rag not a hack and coming up with a better solution?

1056
01:10:05,680 --> 01:10:09,680
Yeah, what's practical deep learning for coders 2024 kind of look like?

1057
01:10:10,680 --> 01:10:11,680
Yeah.

1058
01:10:11,680 --> 01:10:12,680
I mean, good question.

1059
01:10:12,680 --> 01:10:15,680
I'm trying to figure that out for myself, you know, like what should I teach?

1060
01:10:15,680 --> 01:10:21,680
Because I definitely feel like things have changed a bit, you know.

1061
01:10:21,680 --> 01:10:26,680
One of the ways in which things have changed is that coding is much more accessible now.

1062
01:10:26,680 --> 01:10:30,680
So if you look at a lot of the folks in the kind of open source LLM community,

1063
01:10:30,680 --> 01:10:33,680
they're folks who really hadn't coded before a year ago.

1064
01:10:34,680 --> 01:10:40,680
And they're using these models to help them build stuff they couldn't build before, which is just fantastic, you know.

1065
01:10:42,680 --> 01:10:48,680
So one thing I kind of think is like, okay, well, we need a lot more material to help these people use this newfound skill they have,

1066
01:10:48,680 --> 01:10:52,680
because they don't really know what they're doing, you know, and they don't claim to,

1067
01:10:52,680 --> 01:10:53,680
but they're doing it anyway.

1068
01:10:53,680 --> 01:10:55,680
And I think that's fantastic, you know.

1069
01:10:55,680 --> 01:11:00,680
So like other things we could do to help people, you know, bridge this gap,

1070
01:11:00,680 --> 01:11:09,680
because previously, you know, I know folks who were, you know, doing menial jobs a year ago,

1071
01:11:09,680 --> 01:11:16,680
and now they're training language models thanks to the help of codecs and co-pilot and whatever.

1072
01:11:16,680 --> 01:11:21,680
So, you know, yeah, what does it look like to like really grab this opportunity?

1073
01:11:21,680 --> 01:11:27,680
You know, maybe fast AIs goals can be dramatically expanded now to being like,

1074
01:11:27,680 --> 01:11:33,680
let's make coding more accessible, you know, or kind of AI-oriented coding more accessible.

1075
01:11:34,680 --> 01:11:39,680
If so, our costs should probably look very different, you know,

1076
01:11:39,680 --> 01:11:43,680
and we'd have to throw away that like, oh, you have to have at least a year of full-time programming,

1077
01:11:43,680 --> 01:11:46,680
you know, as a prerequisite.

1078
01:11:47,680 --> 01:11:49,680
Yeah, what would happen if we got rid of that?

1079
01:11:49,680 --> 01:11:52,680
So that's kind of one thought that's in my head.

1080
01:11:53,680 --> 01:12:00,680
You know, as to what should other people do, honestly, I don't think anybody has any idea,

1081
01:12:00,680 --> 01:12:03,680
like the more I look at it, what's going on.

1082
01:12:03,680 --> 01:12:07,680
I know I don't, you know, like, we don't really know how to do anything very well.

1083
01:12:08,680 --> 01:12:14,680
Clearly open AI do, like they seem to be quite good at some things,

1084
01:12:14,680 --> 01:12:18,680
or they're talking to folks at or who have recently left open AI.

1085
01:12:18,680 --> 01:12:21,680
Even there, it's clear there's a lot of stuff they haven't really figured out,

1086
01:12:21,680 --> 01:12:26,680
and they're just kind of like using recipes that they've noticed have been okay.

1087
01:12:26,680 --> 01:12:29,680
So yeah, we don't really know how to train these models well.

1088
01:12:29,680 --> 01:12:30,680
We don't know how to fine-tune them well.

1089
01:12:30,680 --> 01:12:32,680
We don't know how to do RAG well.

1090
01:12:32,680 --> 01:12:33,680
We don't know what they can do.

1091
01:12:33,680 --> 01:12:34,680
We don't know what they can't do.

1092
01:12:34,680 --> 01:12:37,680
We don't know how big a model you need to solve different kinds of problems.

1093
01:12:37,680 --> 01:12:39,680
We don't know what kind of problems they can't do.

1094
01:12:39,680 --> 01:12:43,680
We don't know what good prompting strategies are for particular problems, you know,

1095
01:12:43,680 --> 01:12:49,680
like somebody sent me a message the other day saying they've written something

1096
01:12:49,680 --> 01:12:54,680
that is a prompting strategy for GPT-4.

1097
01:12:54,680 --> 01:12:57,680
They've written like 6,000 lines of Python code,

1098
01:12:57,680 --> 01:13:00,680
and it's to help it play chess.

1099
01:13:01,680 --> 01:13:05,680
And then they've said they've had it play against other chess engines,

1100
01:13:05,680 --> 01:13:07,680
including the best stockfish engines,

1101
01:13:07,680 --> 01:13:11,680
and it's got an ELO of 3400,

1102
01:13:11,680 --> 01:13:15,680
which would make it close to the best chess engine in existence.

1103
01:13:17,680 --> 01:13:22,680
And I think this is a good example of like people were saying like GPT-4 can't play chess.

1104
01:13:22,680 --> 01:13:24,680
I mean, I was sure that was wrong.

1105
01:13:24,680 --> 01:13:26,680
I mean, obviously it can play chess,

1106
01:13:26,680 --> 01:13:30,680
but the difference between like with no prompting strategy,

1107
01:13:30,680 --> 01:13:32,680
they can't even make legal moves.

1108
01:13:32,680 --> 01:13:33,680
With good prompting strategies,

1109
01:13:33,680 --> 01:13:36,680
it might be just about the best chess engine in the world,

1110
01:13:36,680 --> 01:13:38,680
far better than any human player.

1111
01:13:38,680 --> 01:13:41,680
So, yeah, I mean, we don't really know what the capabilities are yet.

1112
01:13:41,680 --> 01:13:44,680
So I feel like it's all blue sky at this point.

1113
01:13:44,680 --> 01:13:48,680
It feels like computer vision in 2013 to me,

1114
01:13:48,680 --> 01:13:50,680
which was like in 2013 computer vision.

1115
01:13:50,680 --> 01:13:51,680
We just had the AlexNet.

1116
01:13:51,680 --> 01:13:53,680
We've had AlexNet.

1117
01:13:53,680 --> 01:13:55,680
We've had VGGNet.

1118
01:13:55,680 --> 01:13:57,680
It's around the time Xyler and Fergus like,

1119
01:13:57,680 --> 01:13:59,680
no, it's probably before that.

1120
01:13:59,680 --> 01:14:00,680
So we hadn't yet had the Xyler and Fergus like,

1121
01:14:00,680 --> 01:14:02,680
oh, this is actually what's going on inside the layers.

1122
01:14:02,680 --> 01:14:07,680
So, you know, we don't actually know what's happening inside these transformers.

1123
01:14:07,680 --> 01:14:11,680
We don't know how to create good training dynamics.

1124
01:14:11,680 --> 01:14:14,680
We don't really know anything much.

1125
01:14:14,680 --> 01:14:17,680
And there's a reason for that, right?

1126
01:14:17,680 --> 01:14:24,680
And the reason for that is language models suddenly got really useful.

1127
01:14:24,680 --> 01:14:28,680
And so the kind of economically rational thing to do,

1128
01:14:28,680 --> 01:14:30,680
like this is not criticism, this is true.

1129
01:14:30,680 --> 01:14:32,680
The economic rational thing to do is to like,

1130
01:14:32,680 --> 01:14:35,680
okay, like build that as fast as possible,

1131
01:14:35,680 --> 01:14:38,680
you know, make something work, get it out there.

1132
01:14:38,680 --> 01:14:42,680
And that's what, you know, open AI in particular did,

1133
01:14:42,680 --> 01:14:46,680
anthropic kind of did.

1134
01:14:46,680 --> 01:14:50,680
But there's a whole lot of technical debt everywhere, you know,

1135
01:14:50,680 --> 01:14:55,680
nobody's really figured this stuff out because everybody's been so busy

1136
01:14:55,680 --> 01:14:59,680
building what we know works as quickly as possible.

1137
01:14:59,680 --> 01:15:02,680
So, yeah, I think there's a huge amount of opportunity to,

1138
01:15:02,680 --> 01:15:09,680
you know, I think we'll find things can be made to work a lot faster,

1139
01:15:09,680 --> 01:15:11,680
a lot less memory.

1140
01:15:11,680 --> 01:15:14,680
I got a whole bunch of ideas I want to try, you know,

1141
01:15:14,680 --> 01:15:18,680
every time I look at something closely, like really closely,

1142
01:15:18,680 --> 01:15:22,680
I'm always like, oh, turns out this person actually had no idea what they're doing.

1143
01:15:22,680 --> 01:15:27,680
You know, which is fine, like none of us know what we're doing.

1144
01:15:27,680 --> 01:15:31,680
We should experiment with that.

1145
01:15:31,680 --> 01:15:35,680
We had a treat out on the podcast who created flash attention.

1146
01:15:35,680 --> 01:15:39,680
And I asked them, did nobody think of using SRAM before you?

1147
01:15:39,680 --> 01:15:42,680
Like where people just like, you know, and he was like, yeah,

1148
01:15:42,680 --> 01:15:45,680
people just didn't think of it, didn't try.

1149
01:15:45,680 --> 01:15:47,680
They didn't come from like a systems background.

1150
01:15:47,680 --> 01:15:48,680
Yeah.

1151
01:15:48,680 --> 01:15:52,680
I mean, the thing about flash attention is, I mean,

1152
01:15:52,680 --> 01:15:55,680
lots of people absolutely thought of that.

1153
01:15:55,680 --> 01:15:56,680
So had I, right?

1154
01:15:56,680 --> 01:16:00,680
But I mean, the honest truth is particularly before Triton,

1155
01:16:00,680 --> 01:16:05,680
like everybody knew that tiling is the right way to solve anything.

1156
01:16:05,680 --> 01:16:07,680
And everybody knew that attention,

1157
01:16:07,680 --> 01:16:09,680
used attention, wasn't tiled.

1158
01:16:09,680 --> 01:16:11,680
That was stupid.

1159
01:16:11,680 --> 01:16:16,680
But not everybody's got his ability to like be like,

1160
01:16:16,680 --> 01:16:22,680
oh, well, I'm confident enough in CUDA and or Triton

1161
01:16:22,680 --> 01:16:24,680
to use that insight to write something better.

1162
01:16:24,680 --> 01:16:27,680
And this is where like, I'm super excited about Mojo, right?

1163
01:16:27,680 --> 01:16:30,680
And I always talk to Chris about flash attention as I'm like,

1164
01:16:30,680 --> 01:16:36,680
there is a thousand flash attentions out there for us to build.

1165
01:16:36,680 --> 01:16:39,680
You just got to make it easy for us to build them.

1166
01:16:39,680 --> 01:16:42,680
So like Triton definitely helps.

1167
01:16:42,680 --> 01:16:46,680
But it's still not easy.

1168
01:16:46,680 --> 01:16:51,680
It still requires kind of really understanding the GPU architecture,

1169
01:16:51,680 --> 01:16:54,680
writing it in that kind of very CUDA-ish way.

1170
01:16:54,680 --> 01:16:59,680
So yeah, I think, you know, if Mojo or something equivalent

1171
01:16:59,680 --> 01:17:04,680
can really work well, we're going to see a lot more flash attentions

1172
01:17:04,680 --> 01:17:07,680
popping up.

1173
01:17:07,680 --> 01:17:08,680
Great, Jaren.

1174
01:17:08,680 --> 01:17:11,680
Before we wrap, we usually do a quick lightning round.

1175
01:17:11,680 --> 01:17:13,680
We've got three simple questions.

1176
01:17:13,680 --> 01:17:15,680
So the first one is around acceleration

1177
01:17:15,680 --> 01:17:18,680
and you've been in this field a long time.

1178
01:17:18,680 --> 01:17:20,680
What's something that it's already here today

1179
01:17:20,680 --> 01:17:23,680
and AI that you thought would take much longer?

1180
01:17:23,680 --> 01:17:24,680
I don't think anything.

1181
01:17:24,680 --> 01:17:26,680
So I've actually been slightly too bullish.

1182
01:17:26,680 --> 01:17:34,680
So in my 2014 TED Talk, I had a graph and I said like,

1183
01:17:34,680 --> 01:17:36,680
this is like the slope of human capabilities

1184
01:17:36,680 --> 01:17:39,680
and this is the slope of AI capabilities.

1185
01:17:39,680 --> 01:17:42,680
And I said, oh, and I put a dot saying we are here.

1186
01:17:42,680 --> 01:17:44,680
It was just before they passed.

1187
01:17:44,680 --> 01:17:47,680
And I looked back at the transcript the other day

1188
01:17:47,680 --> 01:17:52,680
and I said in five years, I think we might have crossed

1189
01:17:52,680 --> 01:17:55,680
that threshold in which computers will be better

1190
01:17:55,680 --> 01:17:58,680
at most human tasks than most humans, most average humans.

1191
01:17:58,680 --> 01:18:05,680
And so that might be almost true now for non-physical tasks.

1192
01:18:05,680 --> 01:18:14,680
So I took that twice as long as I thought it might.

1193
01:18:14,680 --> 01:18:18,680
Yeah, no, I wouldn't say anything surprised me too much.

1194
01:18:18,680 --> 01:18:21,680
It's still like definitely like, I got to admit,

1195
01:18:21,680 --> 01:18:26,680
I had a very visceral reaction using GPT-4 for the first time,

1196
01:18:26,680 --> 01:18:32,680
not because I found it surprising, but actually doing it.

1197
01:18:32,680 --> 01:18:36,680
Something I was pretty sure would exist by about now,

1198
01:18:36,680 --> 01:18:38,680
maybe a bit earlier.

1199
01:18:38,680 --> 01:18:41,680
But actually using it definitely is different

1200
01:18:41,680 --> 01:18:44,680
to just feeling like it's probably on its way.

1201
01:18:44,680 --> 01:18:49,680
And yeah, whatever GPT-5 looks like,

1202
01:18:49,680 --> 01:18:56,680
I'm sure I imagine I'll have the same visceral reaction.

1203
01:18:56,680 --> 01:19:00,680
It's really amazing to watch develop.

1204
01:19:00,680 --> 01:19:02,680
We also have an exploration question.

1205
01:19:02,680 --> 01:19:04,680
So what do you think is the most interesting

1206
01:19:04,680 --> 01:19:07,680
unsolved question in AI?

1207
01:19:07,680 --> 01:19:10,680
How do language models learn?

1208
01:19:10,680 --> 01:19:12,680
What are the training dynamics?

1209
01:19:12,680 --> 01:19:19,680
Like I want to see, there was a great paper about Resnets

1210
01:19:19,680 --> 01:19:25,680
a few years ago that showed how that was able to like plot

1211
01:19:25,680 --> 01:19:28,680
a kind of projected three-dimensional loss surface

1212
01:19:28,680 --> 01:19:34,680
for a ConvNet with and without skip connections.

1213
01:19:34,680 --> 01:19:37,680
And you could very clearly see without the skip connections,

1214
01:19:37,680 --> 01:19:39,680
it was super bumpy and with the skip connections,

1215
01:19:39,680 --> 01:19:43,680
it was super smooth.

1216
01:19:43,680 --> 01:19:45,680
That's the kind of work we need.

1217
01:19:45,680 --> 01:19:47,680
So there was actually an interesting blog post

1218
01:19:47,680 --> 01:19:50,680
that came out just today from the PyTorch team

1219
01:19:50,680 --> 01:19:53,680
where some of them have created this like 3D

1220
01:19:53,680 --> 01:19:56,680
matrix product visualization thing.

1221
01:19:56,680 --> 01:20:00,680
And they actually showed some nice examples

1222
01:20:00,680 --> 01:20:03,680
of like a GPT-2 attention layer

1223
01:20:03,680 --> 01:20:06,680
and showed an animation and said,

1224
01:20:06,680 --> 01:20:08,680
like, if you look at this, we can actually see a bit

1225
01:20:08,680 --> 01:20:09,680
about what it's doing.

1226
01:20:09,680 --> 01:20:12,680
You know, so again, it reminds me of the Zeiler

1227
01:20:12,680 --> 01:20:15,680
and Fergus, you know, ConvNet paper.

1228
01:20:15,680 --> 01:20:18,680
That was the first one to do these reverse convolutions

1229
01:20:18,680 --> 01:20:20,680
to show what's actually being learned

1230
01:20:20,680 --> 01:20:21,680
in each layer in a ConvNet.

1231
01:20:21,680 --> 01:20:24,680
Yeah, we need a lot more of this, like,

1232
01:20:24,680 --> 01:20:27,680
what is going on inside these models?

1233
01:20:27,680 --> 01:20:29,680
How do they actually learn?

1234
01:20:29,680 --> 01:20:31,680
And then how can we use those insights

1235
01:20:31,680 --> 01:20:35,680
to help them to learn better?

1236
01:20:35,680 --> 01:20:36,680
So I think that would be one.

1237
01:20:36,680 --> 01:20:38,680
The other exploration I'd really like to see

1238
01:20:38,680 --> 01:20:41,680
is a much more rigorous analysis

1239
01:20:41,680 --> 01:20:44,680
of what kind of data do they need,

1240
01:20:44,680 --> 01:20:47,680
at what level, and when do they need it,

1241
01:20:47,680 --> 01:20:48,680
and how often.

1242
01:20:48,680 --> 01:20:51,680
So that kind of like data set mixing, curation,

1243
01:20:51,680 --> 01:20:54,680
so forth in order to get the best capabilities.

1244
01:20:54,680 --> 01:20:55,680
Yeah.

1245
01:20:55,680 --> 01:20:57,680
How much is Wikipedia?

1246
01:20:57,680 --> 01:20:58,680
Yeah.

1247
01:20:58,680 --> 01:20:59,680
Very uncertain.

1248
01:20:59,680 --> 01:21:01,680
You know, to fine-tune what kind of mix

1249
01:21:01,680 --> 01:21:04,680
do you need for it to keep its capabilities

1250
01:21:04,680 --> 01:21:06,680
and what are the kind of underlying capabilities

1251
01:21:06,680 --> 01:21:07,680
that it most needs to keep?

1252
01:21:07,680 --> 01:21:09,680
And if it loses those, it would lose all these other ones

1253
01:21:09,680 --> 01:21:11,680
and what data do you need to keep those?

1254
01:21:11,680 --> 01:21:13,680
And, you know, are there things we can do

1255
01:21:13,680 --> 01:21:16,680
to change the loss function, to help it,

1256
01:21:16,680 --> 01:21:20,680
to not forget to do things, stuff like that?

1257
01:21:20,680 --> 01:21:21,680
Awesome.

1258
01:21:21,680 --> 01:21:24,680
And yeah, before wrapping, what's one message,

1259
01:21:24,680 --> 01:21:26,680
one idea you want everyone to remember

1260
01:21:26,680 --> 01:21:27,680
and think about?

1261
01:21:27,680 --> 01:21:29,680
You know, I guess the main thing I want everybody

1262
01:21:29,680 --> 01:21:31,680
to remember is that, you know,

1263
01:21:31,680 --> 01:21:33,680
there's a lot of people in the world

1264
01:21:33,680 --> 01:21:35,680
and they have a lot of, you know,

1265
01:21:35,680 --> 01:21:38,680
diverse experiences and capabilities

1266
01:21:38,680 --> 01:21:42,680
and, you know, they all matter.

1267
01:21:42,680 --> 01:21:46,680
And now that we have, you know,

1268
01:21:46,680 --> 01:21:49,680
nearly powerful technology in our lives,

1269
01:21:49,680 --> 01:21:51,680
we could think of that in one of two ways.

1270
01:21:51,680 --> 01:21:57,680
One would be, gee, that's really scary

1271
01:21:57,680 --> 01:21:59,680
what would happen if all of these people in the world

1272
01:21:59,680 --> 01:22:01,680
had access to this technology.

1273
01:22:01,680 --> 01:22:03,680
One of them might be bad people.

1274
01:22:03,680 --> 01:22:05,680
Let's make sure they can't have it.

1275
01:22:05,680 --> 01:22:08,680
Or one might be, wow,

1276
01:22:08,680 --> 01:22:10,680
if all those people in the world are better,

1277
01:22:10,680 --> 01:22:13,680
a lot of them could really improve the lives

1278
01:22:13,680 --> 01:22:17,680
of a lot of humanity if they had this tool.

1279
01:22:17,680 --> 01:22:19,680
This has always been the case, you know,

1280
01:22:19,680 --> 01:22:21,680
from the invention of writing

1281
01:22:21,680 --> 01:22:23,680
to the invention of the printing press

1282
01:22:23,680 --> 01:22:26,680
to the, you know, development of education.

1283
01:22:26,680 --> 01:22:29,680
And it's been a constant battle

1284
01:22:29,680 --> 01:22:33,680
between people who think that distributed power is unsafe

1285
01:22:33,680 --> 01:22:36,680
and it should be held on to by an elite few

1286
01:22:36,680 --> 01:22:43,680
and people who think that humanity on net,

1287
01:22:43,680 --> 01:22:46,680
you know, is a marvellous species,

1288
01:22:46,680 --> 01:22:49,680
particularly when part of a society and a civilization

1289
01:22:49,680 --> 01:22:51,680
and we should do everything we can

1290
01:22:51,680 --> 01:22:55,680
to enable more of them to contribute.

1291
01:22:55,680 --> 01:22:59,680
There's a really big conversation right now

1292
01:22:59,680 --> 01:23:04,680
and, you know, I want to see more and more people

1293
01:23:04,680 --> 01:23:10,680
showing up and showing what, you know,

1294
01:23:10,680 --> 01:23:13,680
what the great unwashed masses out there

1295
01:23:13,680 --> 01:23:15,680
can actually achieve, you know,

1296
01:23:15,680 --> 01:23:18,680
that actually, you know, regular people

1297
01:23:18,680 --> 01:23:22,680
are going to do a lot of really valuable work

1298
01:23:22,680 --> 01:23:28,680
and actually help us be, you know, more safe

1299
01:23:28,680 --> 01:23:32,680
and also flourishing in our lives

1300
01:23:32,680 --> 01:23:37,680
and providing a future for our children to flourish in,

1301
01:23:37,680 --> 01:23:44,680
you know, if we lock things down

1302
01:23:44,680 --> 01:23:48,680
to the people that we think, you know,

1303
01:23:48,680 --> 01:23:52,680
the elites that we think can be trusted to run it for us.

1304
01:23:52,680 --> 01:24:01,680
Yeah, I think all bets are off about where that lives as a society, you know.

1305
01:24:01,680 --> 01:24:03,680
Yep.

1306
01:24:03,680 --> 01:24:06,680
Now, that's an important message

1307
01:24:06,680 --> 01:24:08,680
and yeah, that's why we've been promoting a lot

1308
01:24:08,680 --> 01:24:11,680
of open source developers, open source communities.

1309
01:24:11,680 --> 01:24:15,680
I think letting the builders build and explore,

1310
01:24:15,680 --> 01:24:17,680
that's always a good idea.

1311
01:24:17,680 --> 01:24:19,680
Thank you so much for coming on, Jeremy.

1312
01:24:19,680 --> 01:24:20,680
This was great.

1313
01:24:20,680 --> 01:24:22,680
Thank you for having me.

