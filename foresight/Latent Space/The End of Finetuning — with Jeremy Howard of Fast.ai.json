{"text": " Hey everyone, welcome to the Late in Space Pockets. This is Alessio, partner in CTO and residence at Decibel Partners, and I'm joined by my co-host, Swix, founder of SmallAI. Hey, and today we have in the remote studio Jeremy Howard from, all the way from Australia. Good morning. The remote studio, also known as my house. Good morning. Nice to see you, Ruth. Nice to see you too. I'm actually very used to seeing you in your mask as a message to people, but today we're mostly audio. But thank you for doing the very important public service of COVID awareness. Once there was a pleasure, it was all very annoying and frustrating and tedious, but somebody had to do it. Somebody had to do it, especially somebody with your profile, I think, Julie drives home the message. So we tend to introduce people for them and then ask people to fill in the blanks on the personal side. Something I did not know about you was that you graduated with a B in philosophy from the University of Melbourne. I assumed you had a PhD. No, I barely got through my BA because I was working 80 to 100 hour weeks at McKinsey and a company from 19 years old onwards. So I actually didn't attend any lectures in second and third year university. Well, I guess you didn't need it or you're very sort of self-driven and self-motivated. I just took two weeks off before each exam period when I was working at McKinsey and then I can't believe I got away with this in hindsight. I would go to all my professors and say, oh, I was meant to be in your class this semester and I didn't quite turn up, were there any assignments I was meant to have done, whatever. I can't believe all of them let me basically, they basically always would say like, okay, well, if you can have this written by tomorrow, I'll accept it. So yeah, stressful way to get through university. Well, it shows that, I guess, you min-maxed the opportunities. That definitely was a tricker. I mean, finally, in philosophy, the things I found interesting and focused on in the little bit of time I did spend on it was ethics and cognitive science. And it's kind of really amazing that now come back around and those are actually genuinely useful things to know about, which I never thought would happen. A lot of relevant conversations there. So you were a consultant for a while and then in the magical month of June, 1999, you found it, both optimal decisions and fast meal, which I also really used. So thank you for that. Oh, good for you. Yeah. Because I had read the statistics switches at like 90% or something of small businesses fail. So I thought if I start two businesses, I have a higher chance. In hindsight, I was thinking of it as some kind of stochastic thing. I didn't have control over it, but it's a bit hard, but anyway. And then you were president and chief scientist at Kaggle, which obviously is the composition platform of machine learning. And then in Lytec, where you were working on using deep learning to improve medical diagnostics and clinical decisions. Yeah. That was actually the first company to use deep learning in medicine. So it kind of founded the field. And even now, that's still like a pretty early phase. And I actually heard you on your new podcast with Tanishk, where you went very, very deep into the stuff, the kind of work that he's doing, such a young prodigy at his age. Maybe he's too old to be called a prodigy now. X prodigy. No, I think he still counts. And anyway, just to round out the bio, you have a lot more other credentials, obviously. But most recently, you started Fast.AI, which is still, I guess, your primary identity with Rachel Thomas. So welcome. Thanks. Thank you. Yeah. Being a lot of public service there with getting people involved in AI, and I can imagine a better way to describe it than Fast.AI is you teach people from nothing to stable diffusion in seven weeks or something. And that's amazing. Yeah. Yeah. I mean, it's funny. When we started that, what was that like 2016 or something? The idea that deep learning was something that you could make more accessible was generally considered stupid, but everybody knew that deep learning was a thing that you got a math through a computer science PhD, you know, those one of five labs that could give you the appropriate skills, then you would join, yeah, basically from one of those labs, you might be able to write some papers. So yeah, the idea that normal people could use that technology to do good work was considered kind of ridiculous when we started it. And we weren't sure if it was possible either, but we kind of felt like we had to give it a go because the alternative was we were pretty sure that deep learning was on its way to becoming the most or one of the most important technologies in human history. And if the only people that could use it were a handful of computer science PhDs, that seemed like A, a big waste and B, kind of dangerous. And you know, well, I just wanted to know one thing on your bio that at Kaggle, you were also the top rank participant in both 2010 and 2011. So sometimes you see a lot of founders running companies that are not really in touch with the problem, but you were clearly building something that you knew a lot about, which is awesome. And even, yeah, talking about deep learning, you created, published a paper on ULM fit, which was kind of the predecessor to multitask learning and a lot of the groundwork that then went to into transformers. I read back on the paper and you turn this model AWD LSTM, which I did the math and it was like 24 to 33 million parameters, depending on what training data set you use today. That's kind of like not even small, it's like super small. What were some of the kind of like contrarian takes that you had at the time and maybe set the stage a little bit for the rest of the audience on what was kind of like the state of the art, so to speak at the time and what people were working towards. Yeah, the whole thing was a contrarian take, you know. So okay, so we started first AI, my wife and I, and we, yeah, so we're trying to think, okay, how do we make it more accessible? So when we started thinking about it, it was very 2015 and then 2016, we started doing something about it. Why is it inaccessible? Okay, well, A, no one knows how to do it other than a few number of people and then when we asked those few number of people, well, how do you actually get good results? They would say like, oh, it's like, you know, a box of tricks that aren't published. So you have to join one of the, you know, labs and learn the tricks. So a bunch of unpublished tricks, not much software around, but you know, thankfully there was Theano and, you know, rappers and particularly Lasagna, the rapper. But yeah, not much software around, not much in the way of data sets, you know, very hard to get started in terms of the compute, like how do you get that set up? So you know, everything was kind of inaccessible. And you know, as we started looking into it, we had a key insight which was like, you know, most of the compute and data for image recognition, for example, we don't need to do it. You know, there's this thing which nobody knows about, nobody talks about called transfer learning where you take somebody else's model where they already figured out like how to detect edges and gradients and corners and text and whatever else, and then you can fine tune it to do the thing you want to do. And we thought that's the key, that's the key to becoming more accessible in terms of compute and data requirements. So when we started FastAI, we focused from day one on transfer learning, lesson one, in fact, was transfer learning, literally lesson one, something not normally even mentioned in. I mean, there wasn't much in the way of courses, you know, basically, really the courses out there were PhD programs that had happened to have recorded their lessons, they would really mention it at all. We wanted to show how to do four things that seemed really useful, you know, work with vision, work with tables of data, work with kind of recommendation systems and collaborative filtering and work with text, because we felt like those four kind of modalities covered a lot of the stuff that, you know, are useful in real life. And no one was doing anything much useful with text. Everybody was talking about Word2Vec, you know, like King plus, Queen minus, woman and blah, blah, blah, and it was like cool experiments, but nobody was doing anything like useful with it. NLP was all like lamertization and stop words and topic models and diagrams and SPMs, and it was really academic and not practical. But I mean, to be honest, I've been thinking about this crazy idea for nearly 30 years since I had done cognitive science at university, where we talked a lot about the cells Chinese room experiment, this idea of like, what if there was somebody that could kind of like knew all of the symbolic manipulations required to answer questions in Chinese, but they didn't speak Chinese, they were kind of inside a room with no other way to talk to the outside world other than taking in slips of paper with Chinese written on them and then they do all their rules and then they pass back a piece of paper with Chinese back and this room with a person in is actually fantastically good at answering any question you give them written in Chinese, do they understand Chinese and is this something that's intelligently working with Chinese ever since that time, I'd say to me the most thoughtful and compelling philosophical response is yes, intuitively it feels like no, that's just because we can't imagine such a large kind of system, but if it looks like a duck and acts like a duck, it's a duck or to all intents and purposes. And so I always kind of thought, so this is basically a kind of analysis of the limits of text and I kind of felt like, yeah, if something could ingest enough text and could use the patterns it saw to then generate text in response to text, it could appear to be intelligent, whether that means it is intelligent or not is a different discussion and not one I find very interesting. Yeah, and then when I came across neural nets when I was about 20, I learned about the universal approximation theorem and stuff and I started thinking like, oh, I wonder if like a neural net could ever get big enough, take in enough data to be a Chinese room experiment. With that background and this kind of like interest in transfer learning, I'd been thinking about this thing for kind of 30 years and I thought like, oh, I wonder if we're there yet, because we have a lot of text, like I can literally download Wikipedia, which is a lot of text. And I thought, you know, how would something learn to kind of answer questions or respond text? And I thought, well, what if we used a language model? So language models are already a thing, you know, they were not a popular or well known thing, but they were a thing. But language models exist to this idea that you could train a model to fill in the gaps or actually in those days, it wasn't fill in the gaps, it was finish a string. In fact, Andre Kapathy did his fantastic RNN demonstration from this at a similar time where he showed like you can have it ingest Shakespeare and it will generate something that looks a bit like Shakespeare. I thought, okay, so if I do this at a much bigger scale using all of Wikipedia, what would it need to be able to do to finish a sentence in Wikipedia effectively, to do it quite accurately quite often? I thought, Jesus, it would actually have to know a lot about the world, you know, it would have to know that there is a world and that there are objects and that objects relate to each other through time and cause each other to react in ways and that causes proceed effects and that, you know, when there are animals and there are people and that people couldn't be in certain positions during certain timeframes and then you could, you know, all that together, you can then finish a sentence like this was signed into law in 2016 by US President X and it would fill in the gaps, you know. So that's why I tried to create a, what in those days was considered a big language model trained on the entirety on Wikipedia, which is that was a bit unheard of and my interest was not in, you know, just having a language model, my interest was in like, what latent capabilities would such a system have that would allow it to finish those kind of sentences? Because I was pretty sure based on our work with transfer learning and vision that I could then suck out those latent capabilities by transfer learning, you know, by fine-tuning it on a task data set or whatever. So we generated this three-step system. So step one was train a language model on a big corpus. Step two was fine-tune a language model on a more curated corpus and step three was further fine-tune that model on a task and of course that's why everybody still does today, right? That's what chat GPT is and so the first time I tried it within hours I had a new state of the art academic result on IMDB and I was like, holy shit, it does work. So you asked to what degree was this kind of like pushing against the established wisdom? Every way, like the reason it took me so long to try it was because I asked all my friends in NLP if this could work and everybody said no, it definitely won't work. It wasn't like, oh maybe, everybody was like, it definitely won't work. NLP is much more complicated than vision. Languages are much more vastly complicated to main, you know, and you've got problems like the grounding problem we know from like philosophy and theory of mind that it's actually impossible for it to work. So yeah, so don't waste your time. Jeremy, had people not tried because it was like too complicated to actually get the data and like set up the training or like were people just lazy and kind of like, hey, this is just not going to work. No, it was lazy. So like, so the person I thought at that time who, there were two people I thought at that time actually who were the strongest at language models were Stephen Merity and Alec Radford. And at the time I didn't know Alec, but I, after we had both, after I'd released ULM Fit and he had released GPT, I organized a chat for both of us with Kate Metz of the New York Times and Kate Metz answered, and Alec answered this question for Kate and Kate just like, so how did, you know, GPT come about? And he said, well, I was pretty sure that pre-training on a general large corpus wouldn't work. So I hadn't tried it. And then I read ULM Fit and turns out it did work. And so I did it, you know, bigger and it worked even better. And similar with Stephen, you know, I asked Stephen Merity, like, why don't we just find, you know, I'll take your AWDSTLM and like, trade it on all of Wikipedia and fine tune it. And he's kind of like, I don't think that's going to really lie. Like two years before, I did a very popular talk at KDD, the conference where everybody in NLP was in the audience. I recognized after faces, you know, and I told them all this, I'm sure transfer learning is the key. I'm sure ImageNet, you know, is going to be an NLP thing as well. And you know, everybody was interested and people asked me questions afterwards, but just, yeah, nobody followed up because everybody knew that it didn't work. I mean, even like, so we were scooped a little bit by Dye and Lee at Google. They had, I already, I didn't even realize this, it's just a bit embarrassing. They had already done a large language model and fine tuned it. But again, they didn't create a general purpose large language model on a general purpose corpus. They only ever tested a domain specific corpus. And I haven't spoken to Kwok actually about that, but I assume that the reason was the same. It probably just didn't occur to them that the general approach could work. So maybe it was that kind of 30 years of mulling over the, this whole Chinese room experiment that had convinced me that it probably would work. I don't know. Yeah. Interesting. I just dug up Alec announcement tweet from Tony 18. He said, inspired by Kobe, Elmo and Yola and Fit. We showed a single transformer language model can be fine tuned to a variety. It's interesting because, you know, today people think of the leader kind of like, kind of like the research lab pushing forward the field. What was that at the time, you know, like kind of like going back five years, people think of it as an overnight success, but obviously it took a while. Yeah. Yeah. No, I mean, absolutely. And I'll say like, it's interesting that it mentioned Elmo because in some ways that was kind of diametrically opposed to, to ULM fit, you know, there was these kind of like, so there was a lot of, there was a lot of activity at the same time as ULM fits release. So there was, so before it, as Brian McCann, I think at Salesforce had come out with this neat model that did a kind of multitask learning. But again, they didn't create a general fine tune language model first. There was Elmo, which I think was a little, you know, actually quite a few months after the first ULM fit example, I think. But yeah, there was a bit of this stuff going on. And the problem was everybody was doing, and particularly after GPT came out there and everybody wanted to focus on zero shot and few shot learning, you know, everybody hated fine tuning, everybody hated transfer learning. And like I literally did tours trying to get people to start doing transfer learning. And people, you know, nobody was interested, particularly after GPT showed such good results with zero shot and few shot learning. And so I actually feel like we kind of went backwards for years and, and not to be honest, I mean, I'm a bit sad about this now, but I kind of got so disappointed and dissuaded by like, it felt like these bigger lab, much bigger labs, you know, like fast AI had only ever been just me and Rachel were getting all of this attention for an approach I thought was the wrong way to do it. You know, I was convinced was the wrong way to do it. And so yeah, for years, people were really focused on getting better zero shot and few shot. And it wasn't until, you know, this key idea of like, well, let's take the ULM fit approach. But for step two, rather than fine tuning on a kind of a domain corpus, let's fine tune on an instruction corpus. And then in step three, rather than fine tuning on a reasonably specific task classification, let's fine tune on a, on a RLHF class classification. And so that was really, that was really key, you know. So I was kind of like out of the NLP field for a few years there, because yeah, it just felt like, I don't know, pushing uphill against this vast tide, which I was convinced was not the right direction, but he's going to listen to me, you know, because I, as you said, I don't have a PhD, not at a university, or at least it wasn't then I don't have a big set of computers to fine tune huge transformer models. So yeah, it was definitely difficult. It's always been hard. You know, it's always been hard, like I've always been somebody who does not want to build stuff on lots of big computers, because most people don't have lots of big computers. And I hate creating stuff that most people can't use, you know, and also stuff that's created on lots of big computers has always been like much more media friendly. So like, it might seem like a recent thing, but actually throughout my 30 years in data science, the attention's always been on, you know, the big iron results. So when I first started, everybody was talking about data warehouses, and it was all about teradata, and it'd be like, oh, this big bank has this huge room full of computers, and they have like terabytes of data available, you know, the press for button. And yeah, that's always what people want to talk about, what people want to write about. And then of course, students coming out of their PhDs and stuff, that's where they want to go work, because that's where they read about. And to me, it's a huge distraction, you know, because like I say, most people don't have unlimited compute, and I want to help most people, not the small subset of the most well-off people. Yeah. That's awesome. It's great to hear, you know, you do such a great job educating that a lot of times, you're not telling your own story, you know, so I love this conversation. And the other thing before we jump into FASTI, actually, you know, a lot of people that I know, they run across a new architecture and one other, like, I got to start a company and raise a bunch of money and do all of this stuff, instead you were like, I want everybody to have access to this. Why was that the case for you? Was it because you already had like a successful, you know, venture and like fast mail and you were more interested in that? What was the reasoning? That's a really good question. So I guess the answer is yes, that's the reason why. So when I was a teenager, I thought it would be really cool to like have my own company. You know, I didn't know the word startup, I didn't know the word entrepreneur, I didn't know the word VC, and I didn't really know what any of those things were really until after we started Kaggle, to be honest, even though I'd started to what would now call startups, I just thought they were just small businesses, you know, they were just companies. So yeah, so those two companies were fast mail and optimal decisions. Fast mail was the first kind of synchronized email provider for non-businesses, so something you can get your same email at home on your laptop that were on your phone, whatever. And then optimal decisions invented a new approach to insurance pricing, so they called profit optimized insurance pricing. So I saw both of those companies, you know, after 10 years, and at that point I had achieved the thing that as a teenager I wanted to do, you know, it took a lot longer than it should have because I spent way longer in management consulting than I should have because I got caught up in that stupid rat race, but you know, eventually I got there and I remember my mom saying to me, oh, you must be so proud, you know, because she remembered, my dreams is like, you've done it. And I kind of reflected and I was like, I'm not, I'm not proud at all, you know, like people quite liked fast mail, you know, it's quite nice to have synchronized email, it probably would have happened anyway, yeah, I'm certainly not proud that I've helped some insurance companies suck more money out of their customers. Yeah, no, I'm not proud, you know, it's this actually, I haven't really helped the world very much, you know, maybe in the insurance case, I've made it a little bit worse. I don't know. So yeah, I was determined to not waste more years of my life doing things, working hard to do things which I could not be reasonably sure would have a lot of value. So, you know, I took some time off, I wasn't sure if I'd ever work again, actually, I didn't particularly want to, because it felt like, yeah, it felt like such a disappointment. And but you know, and I didn't need to, I had enough money, like I wasn't super rich, but I had enough money, I didn't need to work. And I certainly recognized that amongst the other people, I knew who had enough money that they didn't need to work, they all worked ridiculously hard, you know, and constantly put themselves in extremely stressful situations. And I thought, I don't want to be one of those idiots who's tied to, you know, buying a bigger plane than the next guy or whatever, you know, Kaggle came along and I mainly kind of did that just because it was fun and interesting to hang out with interesting people. But, you know, with fast AI in particular, you know, Rachel and I had a very explicit, you know, long series of conversations over a long period of time about like, well, how can we be the most helpful to society as a whole, and particularly to those people who maybe need more help, you know. And so we definitely saw the world going in a potentially pretty dystopian direction, if the world's most powerful technology was controlled by a small group of elites. So we thought, yeah, we should focus on trying to help that not happen. You know, sadly, it looks like it still is likely to happen. But I mean, I feel like we've, we've helped make it a little bit less likely. So we've done our best. You've shown that it's possible. And I think, I think your constant advocacy, your courses, your research that you publish, you know, just the other day you published a signing on, you know, learning that I think is still something that people are still talking about quite a lot. I think that that is the origin story of a lot of people who are going to be, you know, little Jeremy Howard's sort of in your mission with, you know, you don't have to do everything by yourself is what I'm saying. Definitely, definitely. You know, that was a, that was a big takeaway from like, and Lydic was that Lydic, it definitely felt like we had to do everything ourselves. And I kind of, I wanted to solve medicine. I'll say, yeah, okay, solving medicine is actually quite difficult. And I can't do it on my own. And there's a lot of other things I'd like to solve, and I can't do those either. So that was, that was definitely the other piece was like, yeah, you know, can we create an army of passionate domain experts who can change their little part of the world. And that's definitely happened. Like I find nowadays, at least half the time, probably quite a bit more, that I get in contact with somebody who's done really interesting work in some domain. Most of the time, I'd say, they say, yeah, I got my start with fast AI. So it's definitely, I can, I can see that. And I also know from talking to folks at places like Amazon and Adobe and stuff, which, you know, there's lots of alumni there. And they say, oh my God, I got here in like half of the people who are fast AI alumni. So it's fantastic. Yeah, actually, Andre Capati grabbed me when I saw him at Europe's a few years ago. And he's like, I have to tell you thanks to the fast AI courses. When people come to Tesla and they need to know more about deep learning, we always send them to your course. And the OpenAI scholars program was doing the same thing. So it's kind of like, yeah, it's had a surprising impact, you know, that's just one of like three things we do is the course, you know, and it's, it's, it's only ever been at most two people, either me and Rachel or me and Silver. Nowadays, it's just me. So, yeah, I think it shows you don't necessarily need a huge amount of money and a huge team of people to, to make an impact. Yeah. So just to reintroduce fast AI for people who may not have dived into it much. There is the courses that you do. There is the library that is, that is very well loved. And I kind of think of it as a nicer layer on top of PyTorch that people should start with by default and use it as the basis for a lot of your courses. And then you have, you have like NB Dev, which I don't know, is that the third one? Oh, so the three areas were research, software, and, and courses. Oh, sorry. I was going by. So then in software, you know, fast AI is the main thing, but NB Dev is not far behind, but then there's also things like Fastcore, GHAPI, I mean, dozens of open source projects that I've created and some of them have been pretty popular. And some of them are still a little bit hidden. Actually, I should, some of them I should try to do a better job of telling people about. What are you, what are you thinking about? Yeah. What, what's on this little things like, for example, for working with EC2 and AWS, I created a fast EC2 library, which I think is like way more convenient and nice to use than anything else out there. And it's literally got a whole autocomplete dynamic autocomplete that works both on the command line and in notebooks, sort of like autocomplete your instance names and everything like that. You know, just little things like that. I try to make like, when I work with some domain, I try to make it like, I want to make it as enjoyable as possible for me to do that. So I always try to kind of like, like with GHAPI, for example, I think that GitHub API is incredibly powerful, but I didn't find it good to work with because I didn't particularly like the libraries that are out there. So like GHAPI, like Fast EC2, it like autocompletes both at the command line or in a notebook or whatever, like literally the entire GitHub API. The entire thing is like, I think it's like less than a hundred K of code because it actually, as far as I know, the only one that grabs it directly from the official open API spec that GitHub produces. And like, if you're in GitHub and you just type an API, you know, autocomplete API method and it enter, it prints out the docs or the six brief docs and then gives you a link to the actual documentation page, you know, GitHub actions I can write now in Python, which is just so much easier than writing them in typescript and stuff. So, you know, just little things like that. I think that's a approach that more, I wish more developers took to publish some of the work along the way. You describe the third arm of Fast EIS research. It's not something I see often. Obviously, you do do some research and how do you run your research? What are your research interests? Yeah. So research is what I spend the vast majority of my time on. And the artifacts that come out of that are largely software and courses, you know, so to me, the main artifact shouldn't be papers because papers are things read by a small exclusive group of people. You know, to me, the main artifacts should be like something teaching you people, here's how to use this insight and here's software you can use that builds it in. So I think I've only ever done three first person papers in my life, you know, and they were, and none of those are ones I wanted to do, you know, they were all once like, so one was ULM fit where Sebastian Ruda reached out to me after seeing the course and said, like, you have to publish this as a paper, you know. And he said, I'll write it. He said, I want to write it. Cause if I do, I can put it on my PhD and that would be great. And it's like, okay, well, I want to help you with your PhD and that sounds great. So like, you know, one was the masks paper, which just had to exist and nobody else was writing it. And then the third was the fast AI library paper, which again, somebody reached out and said, please, please write this. We will waive the fee for the journal and everything and actually help you get it through publishing and stuff. So yeah, so I don't, other than that, I've never written a first author paper. So the research is like, well, so for example, you know, Don Bench was a competition which Stanford ran a few years ago. It was kind of the first big competition of like, who couldn't train neural nets the fastest rather than the most accurate. And specifically it was who couldn't train ImageNet the fastest. And this was like one of these things where it was created by necessity. So Google had just released their TPUs. And so I heard from my friends at Google that they had put together this big team to smash Don Bench so that they could prove to people that they had to use Google Cloud and use their TPUs and show who could their TPUs were. And we kind of thought, oh, shit, this would be a disaster if they do that because then everybody's going to be like, oh, deep learning is not accessible. You know, to actually be good at it, you have to be Google and you have to use special silicon and so. So, you know, we, we only found out about this 10 days before the competition finished. But, you know, we basically got together an emergency bunch of our students and Rachel and I and sat for the next 10 days and just tried to crunch through and tried to use all of our best ideas that had come from our research. That's a particularly progressive resizing, which is basically train mainly on small things. Train on non-square things, you know, stuff like that. And so, yeah, we ended up winning. Thank God. And so, you know, we turned it around from being like, oh, shit, you know, this is going to show that you have to be Google and have TPUs to being like, oh, my God, even the little guy can do deep learning. So that's an example of the kind of like research artifacts we do. And yeah, so all of my research is always, how do we do more with less, you know, so how do we get better results with less data, with less compute, with less complexity. With less education, you know, stuff like that. So your LLM fits obviously a good example of that. And most recently you published, can LLMs learn from a single example? Maybe could you tell the story a little bit behind that? And maybe that goes a little bit too far into the learning on very low resource. The literature. Yeah. Yeah. So me and my friend, John O'Whittaker, basically had been playing around with this fun Kaggle competition, which is actually still running as we speak, which is, can you create a model which can answer multiple choice questions about anything that's in Wikipedia? And the thing that makes it interesting is that your model has to run on Kaggle within nine hours and Kaggle is very, very limited. So you've only got 14 gig RAM, only two CPUs and a small, very old GPU. So this is cool, you know, if you can do well at this, and this is a good example of like, oh, you can do more with less. So yeah, John O and I were playing around with fine tuning, of course, transfer learning, pre-trained language models. And we saw this like, so we always, you know, plot our losses as we go. So here's another thing we created, we actually, Sylvain Gougir, when he worked with us created called Fast Progress, which is kind of like TQEDM, but we think a lot better. So you look at our Fast Progress curves, and they kind of go down, down, down, down, down, down, a little bit, a little bit, a little bit, and then suddenly go clunk, and they drop. And then down, down, down, down, a little bit, and then suddenly clunk, they drop. We're like, what the hell? These clunks are occurring at the end of each epoch. So normally in deep learning, this would be, you know, I've seen this before, it's always been a bug. It's always turned out that like, oh, we accidentally forgot to turn on eval mode during the validation set. So I was actually learning then, or, oh, we accidentally were kept letting moving average statistics throughout the epoch. So, you know, for it's recent, the moving average or whatever. And so we were using hugging face trainer. So, you know, I did not give my friends at hugging face the benefit of the doubt. I thought, oh, they fucked up hugging face trainer, you know, idiots. Well, you'll use the faster trainer instead. So we switched over to learner. We still saw the clunks. And, you know, that's, yeah, it shouldn't really happen because semantically speaking in the epoch isn't like, it's not a thing, you know, like nothing happens or nothing's meant to happen when you go from ending one epoch to starting the next one. So there shouldn't be a clunk. You know, so I kind of asked around on the open source discords. That's like, what's going on here? And everybody was just like, oh, that's just what, that's just what these training curves look like. Those all look like that. Don't worry about it. That's like, oh, are you all using trainer? Yes. Oh, well, there must be some buck with training. And it's like, well, we also saw it in learner and somebody else is like, no, we've got our own trainer. We get it as well. They're just like, don't worry about it. It's just something we see. It's just normal. I can't do that. I can't just be like, here's something that's like in the previous 30 years of neural networks, nobody ever saw it. And now suddenly we see it. So don't worry about it. I like, I just, I have to know why. Can I clarify this is, was everyone that you're talking to, were they all seeing it for the same data set or in different data sets? Data, different data sets, different trainers. They're just like, no, this is just, this is just what it looks like when you fine tune language models. Don't worry about it. You know, as I say, I hadn't seen it before, but I'd been kind of like, as I say, I, you know, I kept working on them for a couple of years after ULM fit. And then I kind of moved on to other things, partly out of frustration. So I hadn't been fine tuning, you know, um, I mean, Lamar's only been out for a few months, right? But I, I, I, I, I wasn't one of those people who jumped straight into it, you know, so I was relatively new to the kind of Lamar fine tuning world, or else these guys had been, you know, doing it since day one. Um, it's only a few months ago, but it's still quite a bit of time. So, so yeah, they're just like, no, this is all what we see. Don't worry about it. So yeah, I, I've got a very kind of like, I don't know, I've just got this brain where I have to know why things are. And so I kind of, I asked people like, well, why, why do you think it's happening? And they'd be like, oh, we're pretty obviously, cause it's like memorized the data set. It's just like, it can't be right. It's only seen it once. Like, look at this. The, the losses dropped by 0.3, 0.3, which is like, basically it knows the answer. Um, they're like, no, no, it's just, it is, it's just memorized the data set. So yeah. So look, John, when I did not discover this and John O and I did not come up with a hypothesis, you know, I guess we were just the ones, I guess, who had been around for long enough to recognize that like this, this isn't how it's meant to work. And so we, you know, and so we went back and like, okay, let's just run some experiments, you know, cause nobody since we've actually published anything about this, um, well, not quite true. Some people have published things, but nobody ever actually stepped back and said like, what the hell, you know, how can this be possible? Is it possible? Is this what's happening? And so yeah, we created a bunch of experiments where we basically predicted ahead of time. It's like, okay, if this hypothesis is correct, that it's memorized in the training set, then we ought to see blah under conditions, blah, but not only these conditions. And so we ran a bunch of experiments and all of them supported the hypothesis that it was memorizing the data set in a single thing at once. Um, and it's a pretty big data set, you know, um, which in hindsight, it's not totally surprising because the theory, remember, of the ULM fit theory was like what's kind of creating all these latent capabilities to make it easier for it to predict the next token. So if it's got all this kind of latent capability, it ought to also be really good at compressing new tokens because it can immediately recognize that it's like, oh, that's just a version of this. So it's, it's not so crazy, you know, but it is, um, it requires us to rethink everything because like, and nobody knows like, okay, so how do we fine tune these things? Because like, it doesn't even matter. Like maybe it's fine. Like maybe it's fine that it's memorized the data set after one go and you do a second go and okay, the validation loss is terrible because it's now really overconfident. That's fine. Don't, you know, don't keep telling people, don't track validation loss, track validation accuracy, um, because at least that, that will still be useful. Um, there's another thing that's got lost since ULM fit, nobody tracks accuracy of language models anymore. Um, but you know, it'll still keep learning and it does, it does keep improving. But is it worse? You know, like, is it like now that it's kind of memorized it, it's probably getting a less strong signal, you know, um, I don't know. So I still don't know how to fine tune language models properly. And I haven't found anybody who feels like they do, like nobody really knows whether this memorization thing is, it's probably a feature in some ways. There's probably some things that you can do usefully with it. It's probably, yeah, I have a feeling it's messing up training dynamics as well. It doesn't come at the cost of catastrophic forgetting as well, right? Like, which is the other side of the coin. Um, it does, um, to some extent, like, we know it does like look at CodeLama, for example. So CodeLama was a, I think it was like a 500 billion token fine tuning of CodeLama to using code and also pros about code that Meta did. And, um, honestly, they kind of blew it because CodeLama is good at coding, but it's bad at everything else. You know, and it used to be good. Yeah. I was pretty sure it was like, before they released it at me and lots of people in the open source discords were like, Oh my God, you know, we know this is coming. You're not going to say it's coming. I, I hope they kept at least like 50% long code data because otherwise it's going to forget everything else. And they didn't only like 0.3 of their 0.3% of their epochs were non-code data. So I did it, forgot everything else. So now it's good at code and it's bad at everything else. So we definitely have catastrophic forgetting. It's fixable. Just somebody has to do, you know, somebody, somebody has to spend their time training a model on a, a good mix of data. Like, so, okay. So here's the thing. Um, even though I originally created the three step approach that everybody now does, my view is it's actually wrong and we shouldn't use it. Um, um, and that's because people are using it in a way different to why I created it. You know, I created it thinking that the tasks specific models would be more specific, you know, it's like, Oh, this is like a sentiment classifier. This is an example of a task, you know, but the tasks now are like a, um, you know, RLHF, which is basically like answer questions that make people feel happy about your answer. So that's a much more general task and it's a really cool approach. And so we see, for example, RLHF also breaks models. Like, you know, like GPT for RLHDEFT, we know from kind of the, the work that Microsoft did, you know, the pre, the, the earlier less aligned version was better. Um, and these are all kind of examples of catastrophic forgetting. And so to me, the right way to do this is to fine-tune language models is to actually throw away the idea of fine-tuning. There's no such thing. There's only continued pre-training. Uh, and pre-training is something where from the very start, you try to include all the kinds of data that you care about, all the kinds of problems that you care about, instructions, exercises, code, general purpose document completion, whatever. And then as you train, you gradually curate that, you know, you gradually make that higher and higher quality and more and more specific to the kinds of tasks you want it to do. Um, but you never throw away any data. You always keep all of the data types there in reasonably high quantities. Um, you know, maybe the quality filter, you stop training on low-quality data. Cause that's probably fine to forget how to write badly, maybe. Um, so yeah, that's now my view is I think ULM fit is the wrong approach. Um, and that's why we're seeing a lot of these, uh, you know, so-called alignment tax and this view of like, oh, a model can't both code and do other things. You know, I think it's actually cause people are training them wrong. Well, I think you have a clear anti-laziness approach. I think other people are not as, uh, good hearted, you know, they're like, Hey, they told me this thing works. And if I release a model this way, people will appreciate it. I'll get promoted and I'll kind of make, make more money. Oh, absolutely. Yeah. And it's not just money. It's like, this is how citations work most, most badly, you know, so if you want to get cited, you need to write a paper that people in your field recognize as an advancement on things that we know are good. And so we've seen this happen again and again. So like I say, like zero shot and a few shot learning, everybody was writing about that or, you know, with, um, image generation, every, everybody just was writing about GANs, you know, and I was trying to say like, no, GANs are not the right approach. You know, when I showed again through research that we demonstrated in our videos, that you can do better than GANs much faster and with much less data. And nobody cared because again, like if you want to get published, you rewrite a GAN paper that slightly improves this part of GANs and this tiny field, you'll, you'll get published, you know, so it's, yeah, it's not set up for real innovation. Um, it's, you know, it's, again, it's really helpful for me, you know, have my own research lab with nobody telling me what to do and I don't even publish. So it doesn't matter if I get citations. So I just write what I think is actually matters. Um, I wish there was, and you know, it actually places like open AI, you know, the researchers there can do that as well. It's a shame, you know, I wish there was more academic, open, venues in which people can focus on like genuine innovation. Uh, Twitter, which is, uh, unironically has, has become a little bit of that form. Uh, I wanted to follow up on one thing that you mentioned, uh, which is that you checked around the open source discords. Uh, I don't know if it's, uh, to, uh, I don't know if it's a kosher to ask like what discords are lively, uh, or useful right now. Um, I think that something I definitely felt like I missed out on was the early days of Luther AI, which where, which is a fair hot bit. And, uh, you know, like what is the new Luther and you were, you actually shouted out the alignment lab AI discord in your blog posts. And it was the first time I even knew, like I saw them on Twitter and never knew they had a discord, never knew that there was actually substantive discussions going on in there and that you were an active member of it. Okay. Yeah. And then even then, if you do know about that, you go there, it'll look like it's totally dead. And that's because unfortunately, nearly all the discords, nearly all of the conversation happens in private channels, you know, um, and how does someone get into that world? Cause it's obviously very, very, um, instructive, right? You could just come to the first day I discord, which I'll be honest with you, it's less bustling than some of the others, but it's not terrible. And so like, at least, you know, to be fair, one of Emma's bustling channels is private. I guess. So I'm just thinking, why is that? It's just a nature of quality discussion, right? Yeah, I guess when I think about it, like, I didn't have any private discussions on a discord for years. Um, but there was a lot of people who came in with like, oh, I just had this amazing idea for AGI. If you just thought about like, if you imagine the AI is a brain and we, you know, this just, I don't want to talk about it. You know, I don't want to like, but you don't want to be dismissive or whatever. And it's like, oh, well, that's an interesting comment, but maybe you should like try training some models first to see if that aligns with your intuition. Like, oh, but how can I possibly learn? It's like, well, we have a course just actually spend time learning. Like, uh, yeah. Anyway. And there's like, okay, I know the people who always have good answers there. And so I created a private channel and put them all in it. And I got to admit, I, that's where I post more often because. There's much less, you know, flight of fancy views about how we could solve AGI, blah, blah, blah. So there is a bit of that, but having said that, like, I think the bar is pretty low, like if you join a discord and you can hit the, like participants or community or whatever button and you can see who's in it. And you'll see at the top who, who the admins or moderators or people in the dev role are and, uh, just DM one of them and say, like, oh, I, here's my GitHub. Well, here's some blog posts they wrote, you know, I'm interested in talking about this, you know, can I join the private channels and I've never heard of anybody saying no, I will say, you know, uh, a Luther's all pretty open. So you can do the Aleutha discord still, you know, one problem with your Luther discord is it's been going on for so long that it's like, it's very inside baseball. It's hard to, it's hard to get started. Yeah. Um, Kappa AI looks, I think it's all open. This just left a stability. That's more accessible. Yeah. Um, uh, there's also, uh, just recently, uh, now three search that does like the Hermes models and data set just, just opened, they've got some private channels, but it's pretty open. I think, uh, you mentioned alignment lab, that one, it's all the interesting stuff is on private channels. So just ask, um, if, if you know me, ask me, cause I've got admin on that one. There's also, yeah, uh, OS skunkworks, OS skunkworks AI is a good discord, which I think it's open. So they're, yeah, they're all pretty good. I don't want you to leak any, any, uh, you know, uh, discords that don't want any publicity, but we all want people, like we all want people. We just, we just want people who like want to build stuff, you know, um, rather than people who, and like, it's fine to not know anything as well, but if you don't know anything, but you want to tell everybody else what to do and how to do it, that's annoying. If you don't know anything and want to be told, like, here's a really small kind of task that as somebody who doesn't know anything, it's going to take you a really long time to do, but it would still be helpful. Then, and then you go and do it. That would be great. The truth is, yeah, like, I don't know, maybe 5% of people who come in with great enthusiasm and saying that they want to learn and they'll do anything. And then somebody says like, okay, here's some work you can do. Almost nobody does that work. So if you're somebody who actually does the work and follows up, you will massively stand out. That's an extreme rarity and everybody will then want to help you do more work. So, yeah, so just, um, yeah, just do work and people will want to support you. Our discord used to be referral only for a long time. We then have a public invite and then we opened it and they're kind of like channel gating. Um, yeah, a lot of people just want to do. I remember it used to be like, you know, a forum moderator. It's like people just want to do like drive by posting, you know, I'm like, they don't want to help the community. They just want to get their question answered. I mean, the funny thing is our, um, our forum community does not have any of that garbage, you know, there's something specific about the low latency thing. There were people like, they expect an instant answer and yeah, we're all somehow in a forum's tread where they know it's like they're forever. People are a bit more thoughtful, but then the forums are less active than they used to be because discord has got more popular, you know, so it's all a bit of a compromise, you know, running a healthy community is, yes, it's always a bit of a challenge. All right, we've got so many more things we want to dive in, but I don't want to keep you here for hours. Uh, this is not the, the lack of freedom in pockets. We always like to say, uh, one topic I would love to maybe chat a bit about is Mojo modular, you know, Chris Liner nominated you on the pockets. So, uh, we want to spend a little time there. You recently did a hacker's guide to language models and you ran through everything from quantized model to like smaller models, larger models and all of that. Um, but obviously modular is taking its own approach. Uh, yeah, we'll get you excited. I know you Chris have been talking about this for like years and a lot of the ideas you had, so. Yeah, yeah, yeah, absolutely. So I met Chris, I think it was at the first TensorFlow Dev Summit. And I don't think he had even like, I'm not sure if he'd even sufficiently started his employment with Google at that point. So I, I don't know, you know, certainly nothing had been mentioned. So I, you know, I admired him from afar with LLVM and Swift and whatever. And so I saw him walk into the courtyard at, at Google. It's just like, oh, man, Chris Lander. I wonder if he would lower his standards enough to talk to me. Was worth a try. So I caught up my carriage because like he, nobody was talking to him. He looked a bit lost and I wandered over and it's like, oh, you're Chris Latino, right? It's like, what are you doing here? And it's like, yeah, yeah, I am. I'm like, oh, I'm Jeremy Howard. It's like, oh, did you do some of this AI stuff? And I was like, yeah, yeah, I like this AI stuff. Are you doing AI stuff? It's like, well, I'm thinking about starting to do some AI stuff. Yeah, I think it's going to be cool. And so, well, so like I spent the next half hour just basically brain dumping all the ways in which AI was stupid to him and he listened patiently and I thought he probably wasn't even remember or care or whatever. But yeah, then I kind of like, I guess I re-caught up with him a few months later and it's like, I've been thinking about everything you said in that conversation and he like narrated back his response to every part of it, the projects he was planning to do. And it's just like, oh, this dude follows up, holy shit. And I was like, wow, okay. And he was like, yeah, so we're going to create this new thing called Swift for TensorFlow and it's going to be like, it's going to be a compiler with auto differentiation built in and blah, blah, blah, blah. And I say, wait, why would that help? You know, why would you, and he was like, okay, with a compiler during the forward pass, you don't have to worry about saving context, you know, because a lot of it will be optimized in the backward, but I was like, oh my God. Because I didn't really know much about compilers. You know, I spent enough to kind of like understand the ideas, but it hadn't occurred to me that a compiler basically solves a lot of the problems we have as end users. I was like, wow, that's amazing. Okay, you do know, right, that nobody's going to use this unless it's like usable. It's like, yeah, I know, right? So I was thinking you should create like a fast AI for this. So, okay, but I don't even know Swift. And it's like, well, why don't you start learning it? And if you have any questions, ask me. It's just like, holy shit. Like, not only is Chris Latner lowered his standards enough to talk to me, but he's offering me personal tutoring on the programming language that he made. So I was just like, I'm not going to let him down. So I spent like the next two months, like just nerding out on Swift. And it was just before Christmas that I kind of like started writing down what I'd learned. So I wrote a couple of blog posts on like, okay, this is like my attempt to do a numeric programming in Swift. And these are all the challenges I had and the some of the issues I had with like making things properly performant. And here are some libraries I wrote and I sent it to Chris and I was like, I hope he's not too disappointed with me, you know, because that would be the worst. It's like, you know, and I was also like, I was like, I hope he doesn't dislike the fact that I've, you know, didn't love everything. And yeah, he was like, oh, thanks for sending me that. Let's get on a call and talk about it. And we spoke and he was like, this is amazing. I can't believe that you made this. This is exactly what Swift needs. And he was like, and so like somebody set up like a new Swift, I don't remember what they call them, the equivalent of a pep, you know, kind of IFC thing of like, oh, you know, let's look at how we can implement Jeremy's ideas in the language. And he's just like, oh, wow. And so, yeah, you know, so, you know, and then we ended up like literally teaching some lessons together about Swift for TensorFlow. And we built a fast AI kind of equivalent with him and his team. It's so much fun. Then in the end, you know, Google didn't follow through just fair enough, like asking everybody to learn a new programming language is going to be tough. But like it was very obvious, very, very obvious at that time that TensorFlow 2 is going to be a failure, you know, and so this felt like, okay, I, you know, well, you know, what are you going to do? Like, you can't focus on TensorFlow 2 because it's not going to, like it's not working. It's never going to work, you know, nobody at Google is using it internally. So, you know, in the end, Chris left, you know, Swift for TensorFlow got archived. There was no backup plan. So I kind of felt like Google was kind of screwed, you know, and Chris went and did something else, but we kept talking and I was like, look, Chris, you know, you've got to be your own boss, man, because like, you know, you've got the ideas, you know, like only you've got the ideas, you know, and if your ideas are implemented, we'd all be so much better off because like, Python's the best of a whole bunch of shit, you know, like, I would, it's amazing, but it's awful, you know, compared to what it could be. Anyway, so eventually a few years later, he called me up and he was like, Jeremy, I've taken your advice. I've started a company so much like, oh my God, so we got to create a new language. We're going to create a new infrastructure. It's going to build, it's going to have all the stuff we've talked about. And it's like, oh, wow. So that's what Mojo is. And so Mojo is like, you know, building on all the stuff that Chris has figured out over, I mean, really from when he did his PhD thesis, which developed LLVM onwards, you know, in Swift and MLAR, you know, the TensorFlow runtime engine, which is very good. You know, that was something that he built and has lasted. So yeah, I'm pumped about that. I mean, it's very speculative. Creating a whole new language is tough. I mean, Chris has done it before and he's created a whole C++ compiler amongst other things. Looking pretty hopeful. I mean, I hope it works because, you know, I mean. You're doing them to quit his job, so. And I mean, in the meantime, I will say, you know, Google now does have a backup plan, you know, they have JAX, which was never a strategy. It was just a bunch of people who also recognized TensorFlow 2 as shit and they just decided to build something else. And for years, my friends in that team were like, don't tell anybody about us because we, you know, we don't want to be anything but a research project. So now these poor guys suddenly, they're the great white hope for Google's future. And so JAX is, you know, also not terrible, but it's still written in Python. Like it would be cool if we had all the benefits of JAX but in a language that was designed for those kind of purposes. So, you know, fingers crossed that, yeah, that Mocho turns out great. Yeah. Any other thoughts on when, where people should be spending their time? So that's more the kind of language framework level than you have the, you know, GGML. Some of these are like quantization-focused kind of model-level things. Then you got the hardware people. It's like a whole other bucket. Yeah, what are some of the exciting stuff that you're excited about? Well, you won't be surprised to hear me say this, but I think fine-tuning, transfer learning is still a hugely underappreciated area. So today's zero-shot, few-shot learning equivalent is retrieval augmented generation, you know, RAG. Which is like, just like few-shot learning is a thing. Like it's a real thing. It's a useful thing. It's not a thing anybody would want to ignore. Why are people not spending at least as much effort on fine-tuning? You know, because, you know, RAG is like such an inefficient hack, really, isn't it? It's like, you know, segment up my data in some somewhat arbitrary way. Embed it, ask questions about that. You know, hope that my embedding model embeds questions in the same bedding space as a paragraph. Which obviously is not going to, if your question is like, if I've got a whole bunch of archive papers embeddings. And I asked like, what are all the ways in which we can make inference more efficient? The only paragraphs it'll find is like, if there's a review paper that says here's a list of ways to make, you know, inference more efficient. Doesn't have any of the specifics. No, it's not going to be like, oh, here's one way, here's one way, here's a different way in different papers, you know. Yeah, if you fine-tune a model, then all of that information is getting directly incorporated into the weights of your model in a much more efficient and nuanced way. And then you can use RAG on top of that. So I think that that's one area that's definitely underappreciated. And also the confluence of like, okay, how do you combine RAG and fine-tuning, for example. Something that I think a lot of people are uncertain about, and I don't expect you to know either, is that whether or not you can fine-tune new information in. And I think that that is the focus of some of your open questions and research. But of course you can, right? Because it's additional pre-training. Obviously you can, because there's no such thing as fine-tuning. There's only continued pre-training. So fine-tuning is pre-training, like they're literally the same thing. So the knowledge got in there in the first place through pre-training. So how could continuing to pre-train not put more knowledge in? Like it's the same thing. The problem is just we're really bad at it, because everybody's doing it dumb ways. So it's a good question, and it's not just new knowledge, but new capabilities. You know, I think like in my Hackers Guide to LL, into Hackers Guide to LLM's talk, I show simple. I mean, it's a funny, that's a simple example, because it doesn't sound it, but like taking a pre-trained based model and getting it to generate SQL. And it took 15 minutes to train on a single GPU. You know, I think that might surprise people that that capability is actual fingertips. And you know, because it was already there, it was just latent in the base model. Really pushing the boundaries of what you can do with small models, I think is a really interesting question. Like what can you do with a... I mean, there isn't much in the way of good small models. A really underappreciated one is a BTLM 3B, which is a like kind of 7B quality 3B model. There's not much of the 1-2B range, sadly. There are some code ones, but like the fact that there are some really good code ones in that 1-2B range shows you that that's a great size for doing complex tasks well. There was PHY1 recently, which has been the subject of a little bit of discussion about whether they're trained on benchmarks. Yeah, PHY1.5 as well. So that's not a good model yet. Why not? So PHY1 in particular is good at doing a very specific thing, which is creating very small Python snippets. The thing... Okay, so like PHY1.5 has never read Wikipedia, for example. So it doesn't know who Tom Cruise is, you know. It doesn't know who anybody is. He doesn't know about any movies. It doesn't really know anything about anything because it's never read anything. You know, it was trained on a nearly entirely synthetic data set, which is designed for it to learn reasoning. And so it was a research project and a really good one, and it definitely shows us a powerful direction in terms of what you can do with synthetic data. And wow, gosh, even these tiny models can get pretty good reasoning skills, pretty good math skills, pretty good toting skills. But I don't know if it's a model you could necessarily build on. Some people have tried to do some fine tunes of it. And again, they're like surprisingly good in some ways for a 1.5B model, but not sure you'd find it useful for anything. I think that's the struggle of pitching small models because small is great, you know, you don't need a lot of resources to run them. But the performance evaluation is always so iffy. It's always just like, yeah, it works on some things and we don't trust it for others. Yeah, so that's why we're back to fine tuning. I would say, so Microsoft did create a PHY1.5 web, but they didn't release it, unfortunately. I would say a PHY1.5 web with fine tuning for your task might solve a lot of tasks that people have in their kind of day-to-day lives, particularly in kind of an enterprise setting. I think there's a lot of repetitive kind of processing that has to be done. It's a useful thing for coders to know about because I think quite often you can replace some thousands and thousands of lines of complex buggy code maybe with a fine tune, you know. Good. Yeah. And Jeremy, before we let you go, I think one question on top of a lot of people's minds. So you've done practical deep learning for coders in 2018, 19, 21, 22. I feel like the more time goes by, the more the GPUs get concentrated. If you're somebody who's interested in deep learning today and you don't want to go join OpenAI, you don't want to join Anthropic, what's like the best use of their time? Should they focus on, yes, model development? Should they focus on fine tuning math and all of that? Should they just like focus on making rag not a hack and coming up with a better solution? Yeah, what's practical deep learning for coders 2024 kind of look like? Yeah. I mean, good question. I'm trying to figure that out for myself, you know, like what should I teach? Because I definitely feel like things have changed a bit, you know. One of the ways in which things have changed is that coding is much more accessible now. So if you look at a lot of the folks in the kind of open source LLM community, they're folks who really hadn't coded before a year ago. And they're using these models to help them build stuff they couldn't build before, which is just fantastic, you know. So one thing I kind of think is like, okay, well, we need a lot more material to help these people use this newfound skill they have, because they don't really know what they're doing, you know, and they don't claim to, but they're doing it anyway. And I think that's fantastic, you know. So like other things we could do to help people, you know, bridge this gap, because previously, you know, I know folks who were, you know, doing menial jobs a year ago, and now they're training language models thanks to the help of codecs and co-pilot and whatever. So, you know, yeah, what does it look like to like really grab this opportunity? You know, maybe fast AIs goals can be dramatically expanded now to being like, let's make coding more accessible, you know, or kind of AI-oriented coding more accessible. If so, our costs should probably look very different, you know, and we'd have to throw away that like, oh, you have to have at least a year of full-time programming, you know, as a prerequisite. Yeah, what would happen if we got rid of that? So that's kind of one thought that's in my head. You know, as to what should other people do, honestly, I don't think anybody has any idea, like the more I look at it, what's going on. I know I don't, you know, like, we don't really know how to do anything very well. Clearly open AI do, like they seem to be quite good at some things, or they're talking to folks at or who have recently left open AI. Even there, it's clear there's a lot of stuff they haven't really figured out, and they're just kind of like using recipes that they've noticed have been okay. So yeah, we don't really know how to train these models well. We don't know how to fine-tune them well. We don't know how to do RAG well. We don't know what they can do. We don't know what they can't do. We don't know how big a model you need to solve different kinds of problems. We don't know what kind of problems they can't do. We don't know what good prompting strategies are for particular problems, you know, like somebody sent me a message the other day saying they've written something that is a prompting strategy for GPT-4. They've written like 6,000 lines of Python code, and it's to help it play chess. And then they've said they've had it play against other chess engines, including the best stockfish engines, and it's got an ELO of 3400, which would make it close to the best chess engine in existence. And I think this is a good example of like people were saying like GPT-4 can't play chess. I mean, I was sure that was wrong. I mean, obviously it can play chess, but the difference between like with no prompting strategy, they can't even make legal moves. With good prompting strategies, it might be just about the best chess engine in the world, far better than any human player. So, yeah, I mean, we don't really know what the capabilities are yet. So I feel like it's all blue sky at this point. It feels like computer vision in 2013 to me, which was like in 2013 computer vision. We just had the AlexNet. We've had AlexNet. We've had VGGNet. It's around the time Xyler and Fergus like, no, it's probably before that. So we hadn't yet had the Xyler and Fergus like, oh, this is actually what's going on inside the layers. So, you know, we don't actually know what's happening inside these transformers. We don't know how to create good training dynamics. We don't really know anything much. And there's a reason for that, right? And the reason for that is language models suddenly got really useful. And so the kind of economically rational thing to do, like this is not criticism, this is true. The economic rational thing to do is to like, okay, like build that as fast as possible, you know, make something work, get it out there. And that's what, you know, open AI in particular did, anthropic kind of did. But there's a whole lot of technical debt everywhere, you know, nobody's really figured this stuff out because everybody's been so busy building what we know works as quickly as possible. So, yeah, I think there's a huge amount of opportunity to, you know, I think we'll find things can be made to work a lot faster, a lot less memory. I got a whole bunch of ideas I want to try, you know, every time I look at something closely, like really closely, I'm always like, oh, turns out this person actually had no idea what they're doing. You know, which is fine, like none of us know what we're doing. We should experiment with that. We had a treat out on the podcast who created flash attention. And I asked them, did nobody think of using SRAM before you? Like where people just like, you know, and he was like, yeah, people just didn't think of it, didn't try. They didn't come from like a systems background. Yeah. I mean, the thing about flash attention is, I mean, lots of people absolutely thought of that. So had I, right? But I mean, the honest truth is particularly before Triton, like everybody knew that tiling is the right way to solve anything. And everybody knew that attention, used attention, wasn't tiled. That was stupid. But not everybody's got his ability to like be like, oh, well, I'm confident enough in CUDA and or Triton to use that insight to write something better. And this is where like, I'm super excited about Mojo, right? And I always talk to Chris about flash attention as I'm like, there is a thousand flash attentions out there for us to build. You just got to make it easy for us to build them. So like Triton definitely helps. But it's still not easy. It still requires kind of really understanding the GPU architecture, writing it in that kind of very CUDA-ish way. So yeah, I think, you know, if Mojo or something equivalent can really work well, we're going to see a lot more flash attentions popping up. Great, Jaren. Before we wrap, we usually do a quick lightning round. We've got three simple questions. So the first one is around acceleration and you've been in this field a long time. What's something that it's already here today and AI that you thought would take much longer? I don't think anything. So I've actually been slightly too bullish. So in my 2014 TED Talk, I had a graph and I said like, this is like the slope of human capabilities and this is the slope of AI capabilities. And I said, oh, and I put a dot saying we are here. It was just before they passed. And I looked back at the transcript the other day and I said in five years, I think we might have crossed that threshold in which computers will be better at most human tasks than most humans, most average humans. And so that might be almost true now for non-physical tasks. So I took that twice as long as I thought it might. Yeah, no, I wouldn't say anything surprised me too much. It's still like definitely like, I got to admit, I had a very visceral reaction using GPT-4 for the first time, not because I found it surprising, but actually doing it. Something I was pretty sure would exist by about now, maybe a bit earlier. But actually using it definitely is different to just feeling like it's probably on its way. And yeah, whatever GPT-5 looks like, I'm sure I imagine I'll have the same visceral reaction. It's really amazing to watch develop. We also have an exploration question. So what do you think is the most interesting unsolved question in AI? How do language models learn? What are the training dynamics? Like I want to see, there was a great paper about Resnets a few years ago that showed how that was able to like plot a kind of projected three-dimensional loss surface for a ConvNet with and without skip connections. And you could very clearly see without the skip connections, it was super bumpy and with the skip connections, it was super smooth. That's the kind of work we need. So there was actually an interesting blog post that came out just today from the PyTorch team where some of them have created this like 3D matrix product visualization thing. And they actually showed some nice examples of like a GPT-2 attention layer and showed an animation and said, like, if you look at this, we can actually see a bit about what it's doing. You know, so again, it reminds me of the Zeiler and Fergus, you know, ConvNet paper. That was the first one to do these reverse convolutions to show what's actually being learned in each layer in a ConvNet. Yeah, we need a lot more of this, like, what is going on inside these models? How do they actually learn? And then how can we use those insights to help them to learn better? So I think that would be one. The other exploration I'd really like to see is a much more rigorous analysis of what kind of data do they need, at what level, and when do they need it, and how often. So that kind of like data set mixing, curation, so forth in order to get the best capabilities. Yeah. How much is Wikipedia? Yeah. Very uncertain. You know, to fine-tune what kind of mix do you need for it to keep its capabilities and what are the kind of underlying capabilities that it most needs to keep? And if it loses those, it would lose all these other ones and what data do you need to keep those? And, you know, are there things we can do to change the loss function, to help it, to not forget to do things, stuff like that? Awesome. And yeah, before wrapping, what's one message, one idea you want everyone to remember and think about? You know, I guess the main thing I want everybody to remember is that, you know, there's a lot of people in the world and they have a lot of, you know, diverse experiences and capabilities and, you know, they all matter. And now that we have, you know, nearly powerful technology in our lives, we could think of that in one of two ways. One would be, gee, that's really scary what would happen if all of these people in the world had access to this technology. One of them might be bad people. Let's make sure they can't have it. Or one might be, wow, if all those people in the world are better, a lot of them could really improve the lives of a lot of humanity if they had this tool. This has always been the case, you know, from the invention of writing to the invention of the printing press to the, you know, development of education. And it's been a constant battle between people who think that distributed power is unsafe and it should be held on to by an elite few and people who think that humanity on net, you know, is a marvellous species, particularly when part of a society and a civilization and we should do everything we can to enable more of them to contribute. There's a really big conversation right now and, you know, I want to see more and more people showing up and showing what, you know, what the great unwashed masses out there can actually achieve, you know, that actually, you know, regular people are going to do a lot of really valuable work and actually help us be, you know, more safe and also flourishing in our lives and providing a future for our children to flourish in, you know, if we lock things down to the people that we think, you know, the elites that we think can be trusted to run it for us. Yeah, I think all bets are off about where that lives as a society, you know. Yep. Now, that's an important message and yeah, that's why we've been promoting a lot of open source developers, open source communities. I think letting the builders build and explore, that's always a good idea. Thank you so much for coming on, Jeremy. This was great. Thank you for having me.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.96, "text": " Hey everyone, welcome to the Late in Space Pockets.", "tokens": [50364, 1911, 1518, 11, 2928, 281, 264, 31220, 294, 8705, 430, 1560, 1385, 13, 51062], "temperature": 0.0, "avg_logprob": -0.3725499425615583, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.042449526488780975}, {"id": 1, "seek": 0, "start": 13.96, "end": 18.400000000000002, "text": " This is Alessio, partner in CTO and residence at Decibel Partners, and I'm joined by my", "tokens": [51062, 639, 307, 967, 442, 1004, 11, 4975, 294, 383, 15427, 293, 19607, 412, 1346, 537, 5390, 28058, 11, 293, 286, 478, 6869, 538, 452, 51284], "temperature": 0.0, "avg_logprob": -0.3725499425615583, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.042449526488780975}, {"id": 2, "seek": 0, "start": 18.400000000000002, "end": 20.84, "text": " co-host, Swix, founder of SmallAI.", "tokens": [51284, 598, 12, 6037, 11, 3926, 970, 11, 14917, 295, 15287, 48698, 13, 51406], "temperature": 0.0, "avg_logprob": -0.3725499425615583, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.042449526488780975}, {"id": 3, "seek": 0, "start": 20.84, "end": 26.68, "text": " Hey, and today we have in the remote studio Jeremy Howard from, all the way from Australia.", "tokens": [51406, 1911, 11, 293, 965, 321, 362, 294, 264, 8607, 6811, 17809, 17626, 490, 11, 439, 264, 636, 490, 7060, 13, 51698], "temperature": 0.0, "avg_logprob": -0.3725499425615583, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.042449526488780975}, {"id": 4, "seek": 0, "start": 26.68, "end": 27.68, "text": " Good morning.", "tokens": [51698, 2205, 2446, 13, 51748], "temperature": 0.0, "avg_logprob": -0.3725499425615583, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.042449526488780975}, {"id": 5, "seek": 2768, "start": 27.759999999999998, "end": 29.96, "text": " The remote studio, also known as my house.", "tokens": [50368, 440, 8607, 6811, 11, 611, 2570, 382, 452, 1782, 13, 50478], "temperature": 0.0, "avg_logprob": -0.254802103117695, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.025762321427464485}, {"id": 6, "seek": 2768, "start": 29.96, "end": 30.96, "text": " Good morning.", "tokens": [50478, 2205, 2446, 13, 50528], "temperature": 0.0, "avg_logprob": -0.254802103117695, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.025762321427464485}, {"id": 7, "seek": 2768, "start": 30.96, "end": 32.96, "text": " Nice to see you, Ruth.", "tokens": [50528, 5490, 281, 536, 291, 11, 23544, 13, 50628], "temperature": 0.0, "avg_logprob": -0.254802103117695, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.025762321427464485}, {"id": 8, "seek": 2768, "start": 32.96, "end": 34.46, "text": " Nice to see you too.", "tokens": [50628, 5490, 281, 536, 291, 886, 13, 50703], "temperature": 0.0, "avg_logprob": -0.254802103117695, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.025762321427464485}, {"id": 9, "seek": 2768, "start": 34.46, "end": 39.84, "text": " I'm actually very used to seeing you in your mask as a message to people, but today we're", "tokens": [50703, 286, 478, 767, 588, 1143, 281, 2577, 291, 294, 428, 6094, 382, 257, 3636, 281, 561, 11, 457, 965, 321, 434, 50972], "temperature": 0.0, "avg_logprob": -0.254802103117695, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.025762321427464485}, {"id": 10, "seek": 2768, "start": 39.84, "end": 40.84, "text": " mostly audio.", "tokens": [50972, 5240, 6278, 13, 51022], "temperature": 0.0, "avg_logprob": -0.254802103117695, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.025762321427464485}, {"id": 11, "seek": 2768, "start": 40.84, "end": 46.120000000000005, "text": " But thank you for doing the very important public service of COVID awareness.", "tokens": [51022, 583, 1309, 291, 337, 884, 264, 588, 1021, 1908, 2643, 295, 4566, 8888, 13, 51286], "temperature": 0.0, "avg_logprob": -0.254802103117695, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.025762321427464485}, {"id": 12, "seek": 2768, "start": 46.120000000000005, "end": 51.0, "text": " Once there was a pleasure, it was all very annoying and frustrating and tedious, but", "tokens": [51286, 3443, 456, 390, 257, 6834, 11, 309, 390, 439, 588, 11304, 293, 16522, 293, 38284, 11, 457, 51530], "temperature": 0.0, "avg_logprob": -0.254802103117695, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.025762321427464485}, {"id": 13, "seek": 2768, "start": 51.0, "end": 52.0, "text": " somebody had to do it.", "tokens": [51530, 2618, 632, 281, 360, 309, 13, 51580], "temperature": 0.0, "avg_logprob": -0.254802103117695, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.025762321427464485}, {"id": 14, "seek": 2768, "start": 52.0, "end": 56.96, "text": " Somebody had to do it, especially somebody with your profile, I think, Julie drives home", "tokens": [51580, 13463, 632, 281, 360, 309, 11, 2318, 2618, 365, 428, 7964, 11, 286, 519, 11, 18794, 11754, 1280, 51828], "temperature": 0.0, "avg_logprob": -0.254802103117695, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.025762321427464485}, {"id": 15, "seek": 5696, "start": 56.96, "end": 59.120000000000005, "text": " the message.", "tokens": [50364, 264, 3636, 13, 50472], "temperature": 0.0, "avg_logprob": -0.23409874208511844, "compression_ratio": 1.453781512605042, "no_speech_prob": 0.00999618973582983}, {"id": 16, "seek": 5696, "start": 59.120000000000005, "end": 63.92, "text": " So we tend to introduce people for them and then ask people to fill in the blanks on the", "tokens": [50472, 407, 321, 3928, 281, 5366, 561, 337, 552, 293, 550, 1029, 561, 281, 2836, 294, 264, 8247, 82, 322, 264, 50712], "temperature": 0.0, "avg_logprob": -0.23409874208511844, "compression_ratio": 1.453781512605042, "no_speech_prob": 0.00999618973582983}, {"id": 17, "seek": 5696, "start": 63.92, "end": 66.28, "text": " personal side.", "tokens": [50712, 2973, 1252, 13, 50830], "temperature": 0.0, "avg_logprob": -0.23409874208511844, "compression_ratio": 1.453781512605042, "no_speech_prob": 0.00999618973582983}, {"id": 18, "seek": 5696, "start": 66.28, "end": 70.8, "text": " Something I did not know about you was that you graduated with a B in philosophy from the", "tokens": [50830, 6595, 286, 630, 406, 458, 466, 291, 390, 300, 291, 13693, 365, 257, 363, 294, 10675, 490, 264, 51056], "temperature": 0.0, "avg_logprob": -0.23409874208511844, "compression_ratio": 1.453781512605042, "no_speech_prob": 0.00999618973582983}, {"id": 19, "seek": 5696, "start": 70.8, "end": 72.64, "text": " University of Melbourne.", "tokens": [51056, 3535, 295, 27496, 13, 51148], "temperature": 0.0, "avg_logprob": -0.23409874208511844, "compression_ratio": 1.453781512605042, "no_speech_prob": 0.00999618973582983}, {"id": 20, "seek": 5696, "start": 72.64, "end": 74.64, "text": " I assumed you had a PhD.", "tokens": [51148, 286, 15895, 291, 632, 257, 14476, 13, 51248], "temperature": 0.0, "avg_logprob": -0.23409874208511844, "compression_ratio": 1.453781512605042, "no_speech_prob": 0.00999618973582983}, {"id": 21, "seek": 5696, "start": 74.64, "end": 83.08, "text": " No, I barely got through my BA because I was working 80 to 100 hour weeks at McKinsey and", "tokens": [51248, 883, 11, 286, 10268, 658, 807, 452, 21050, 570, 286, 390, 1364, 4688, 281, 2319, 1773, 3259, 412, 21765, 259, 7399, 293, 51670], "temperature": 0.0, "avg_logprob": -0.23409874208511844, "compression_ratio": 1.453781512605042, "no_speech_prob": 0.00999618973582983}, {"id": 22, "seek": 8308, "start": 83.08, "end": 87.75999999999999, "text": " a company from 19 years old onwards.", "tokens": [50364, 257, 2237, 490, 1294, 924, 1331, 34230, 13, 50598], "temperature": 0.0, "avg_logprob": -0.20673529998115872, "compression_ratio": 1.491304347826087, "no_speech_prob": 0.12066533416509628}, {"id": 23, "seek": 8308, "start": 87.75999999999999, "end": 95.75999999999999, "text": " So I actually didn't attend any lectures in second and third year university.", "tokens": [50598, 407, 286, 767, 994, 380, 6888, 604, 16564, 294, 1150, 293, 2636, 1064, 5454, 13, 50998], "temperature": 0.0, "avg_logprob": -0.20673529998115872, "compression_ratio": 1.491304347826087, "no_speech_prob": 0.12066533416509628}, {"id": 24, "seek": 8308, "start": 95.75999999999999, "end": 99.68, "text": " Well, I guess you didn't need it or you're very sort of self-driven and self-motivated.", "tokens": [50998, 1042, 11, 286, 2041, 291, 994, 380, 643, 309, 420, 291, 434, 588, 1333, 295, 2698, 12, 25456, 293, 2698, 12, 29778, 592, 770, 13, 51194], "temperature": 0.0, "avg_logprob": -0.20673529998115872, "compression_ratio": 1.491304347826087, "no_speech_prob": 0.12066533416509628}, {"id": 25, "seek": 8308, "start": 99.68, "end": 107.72, "text": " I just took two weeks off before each exam period when I was working at McKinsey and", "tokens": [51194, 286, 445, 1890, 732, 3259, 766, 949, 1184, 1139, 2896, 562, 286, 390, 1364, 412, 21765, 259, 7399, 293, 51596], "temperature": 0.0, "avg_logprob": -0.20673529998115872, "compression_ratio": 1.491304347826087, "no_speech_prob": 0.12066533416509628}, {"id": 26, "seek": 8308, "start": 107.72, "end": 110.56, "text": " then I can't believe I got away with this in hindsight.", "tokens": [51596, 550, 286, 393, 380, 1697, 286, 658, 1314, 365, 341, 294, 44357, 13, 51738], "temperature": 0.0, "avg_logprob": -0.20673529998115872, "compression_ratio": 1.491304347826087, "no_speech_prob": 0.12066533416509628}, {"id": 27, "seek": 11056, "start": 110.56, "end": 114.96000000000001, "text": " I would go to all my professors and say, oh, I was meant to be in your class this semester", "tokens": [50364, 286, 576, 352, 281, 439, 452, 15924, 293, 584, 11, 1954, 11, 286, 390, 4140, 281, 312, 294, 428, 1508, 341, 11894, 50584], "temperature": 0.0, "avg_logprob": -0.23492014685342477, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.04396730288863182}, {"id": 28, "seek": 11056, "start": 114.96000000000001, "end": 119.92, "text": " and I didn't quite turn up, were there any assignments I was meant to have done, whatever.", "tokens": [50584, 293, 286, 994, 380, 1596, 1261, 493, 11, 645, 456, 604, 22546, 286, 390, 4140, 281, 362, 1096, 11, 2035, 13, 50832], "temperature": 0.0, "avg_logprob": -0.23492014685342477, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.04396730288863182}, {"id": 29, "seek": 11056, "start": 119.92, "end": 125.52000000000001, "text": " I can't believe all of them let me basically, they basically always would say like, okay,", "tokens": [50832, 286, 393, 380, 1697, 439, 295, 552, 718, 385, 1936, 11, 436, 1936, 1009, 576, 584, 411, 11, 1392, 11, 51112], "temperature": 0.0, "avg_logprob": -0.23492014685342477, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.04396730288863182}, {"id": 30, "seek": 11056, "start": 125.52000000000001, "end": 128.56, "text": " well, if you can have this written by tomorrow, I'll accept it.", "tokens": [51112, 731, 11, 498, 291, 393, 362, 341, 3720, 538, 4153, 11, 286, 603, 3241, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.23492014685342477, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.04396730288863182}, {"id": 31, "seek": 11056, "start": 128.56, "end": 131.96, "text": " So yeah, stressful way to get through university.", "tokens": [51264, 407, 1338, 11, 19108, 636, 281, 483, 807, 5454, 13, 51434], "temperature": 0.0, "avg_logprob": -0.23492014685342477, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.04396730288863182}, {"id": 32, "seek": 11056, "start": 131.96, "end": 137.84, "text": " Well, it shows that, I guess, you min-maxed the opportunities.", "tokens": [51434, 1042, 11, 309, 3110, 300, 11, 286, 2041, 11, 291, 923, 12, 41167, 292, 264, 4786, 13, 51728], "temperature": 0.0, "avg_logprob": -0.23492014685342477, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.04396730288863182}, {"id": 33, "seek": 11056, "start": 137.84, "end": 138.84, "text": " That definitely was a tricker.", "tokens": [51728, 663, 2138, 390, 257, 4282, 260, 13, 51778], "temperature": 0.0, "avg_logprob": -0.23492014685342477, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.04396730288863182}, {"id": 34, "seek": 13884, "start": 139.08, "end": 147.16, "text": " I mean, finally, in philosophy, the things I found interesting and focused on in the", "tokens": [50376, 286, 914, 11, 2721, 11, 294, 10675, 11, 264, 721, 286, 1352, 1880, 293, 5178, 322, 294, 264, 50780], "temperature": 0.0, "avg_logprob": -0.22248898460751487, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.008572438731789589}, {"id": 35, "seek": 13884, "start": 147.16, "end": 151.0, "text": " little bit of time I did spend on it was ethics and cognitive science.", "tokens": [50780, 707, 857, 295, 565, 286, 630, 3496, 322, 309, 390, 19769, 293, 15605, 3497, 13, 50972], "temperature": 0.0, "avg_logprob": -0.22248898460751487, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.008572438731789589}, {"id": 36, "seek": 13884, "start": 151.0, "end": 156.44, "text": " And it's kind of really amazing that now come back around and those are actually genuinely", "tokens": [50972, 400, 309, 311, 733, 295, 534, 2243, 300, 586, 808, 646, 926, 293, 729, 366, 767, 17839, 51244], "temperature": 0.0, "avg_logprob": -0.22248898460751487, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.008572438731789589}, {"id": 37, "seek": 13884, "start": 156.44, "end": 159.04, "text": " useful things to know about, which I never thought would happen.", "tokens": [51244, 4420, 721, 281, 458, 466, 11, 597, 286, 1128, 1194, 576, 1051, 13, 51374], "temperature": 0.0, "avg_logprob": -0.22248898460751487, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.008572438731789589}, {"id": 38, "seek": 13884, "start": 159.04, "end": 163.24, "text": " A lot of relevant conversations there.", "tokens": [51374, 316, 688, 295, 7340, 7315, 456, 13, 51584], "temperature": 0.0, "avg_logprob": -0.22248898460751487, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.008572438731789589}, {"id": 39, "seek": 13884, "start": 163.24, "end": 168.64000000000001, "text": " So you were a consultant for a while and then in the magical month of June, 1999, you found", "tokens": [51584, 407, 291, 645, 257, 24676, 337, 257, 1339, 293, 550, 294, 264, 12066, 1618, 295, 6928, 11, 19952, 11, 291, 1352, 51854], "temperature": 0.0, "avg_logprob": -0.22248898460751487, "compression_ratio": 1.6072727272727272, "no_speech_prob": 0.008572438731789589}, {"id": 40, "seek": 16864, "start": 168.64, "end": 172.27999999999997, "text": " it, both optimal decisions and fast meal, which I also really used.", "tokens": [50364, 309, 11, 1293, 16252, 5327, 293, 2370, 6791, 11, 597, 286, 611, 534, 1143, 13, 50546], "temperature": 0.0, "avg_logprob": -0.2024247926824233, "compression_ratio": 1.62012987012987, "no_speech_prob": 0.006381268613040447}, {"id": 41, "seek": 16864, "start": 172.27999999999997, "end": 173.27999999999997, "text": " So thank you for that.", "tokens": [50546, 407, 1309, 291, 337, 300, 13, 50596], "temperature": 0.0, "avg_logprob": -0.2024247926824233, "compression_ratio": 1.62012987012987, "no_speech_prob": 0.006381268613040447}, {"id": 42, "seek": 16864, "start": 173.27999999999997, "end": 174.27999999999997, "text": " Oh, good for you.", "tokens": [50596, 876, 11, 665, 337, 291, 13, 50646], "temperature": 0.0, "avg_logprob": -0.2024247926824233, "compression_ratio": 1.62012987012987, "no_speech_prob": 0.006381268613040447}, {"id": 43, "seek": 16864, "start": 174.27999999999997, "end": 175.27999999999997, "text": " Yeah.", "tokens": [50646, 865, 13, 50696], "temperature": 0.0, "avg_logprob": -0.2024247926824233, "compression_ratio": 1.62012987012987, "no_speech_prob": 0.006381268613040447}, {"id": 44, "seek": 16864, "start": 175.27999999999997, "end": 178.51999999999998, "text": " Because I had read the statistics switches at like 90% or something of small businesses", "tokens": [50696, 1436, 286, 632, 1401, 264, 12523, 19458, 412, 411, 4289, 4, 420, 746, 295, 1359, 6011, 50858], "temperature": 0.0, "avg_logprob": -0.2024247926824233, "compression_ratio": 1.62012987012987, "no_speech_prob": 0.006381268613040447}, {"id": 45, "seek": 16864, "start": 178.51999999999998, "end": 179.51999999999998, "text": " fail.", "tokens": [50858, 3061, 13, 50908], "temperature": 0.0, "avg_logprob": -0.2024247926824233, "compression_ratio": 1.62012987012987, "no_speech_prob": 0.006381268613040447}, {"id": 46, "seek": 16864, "start": 179.51999999999998, "end": 182.83999999999997, "text": " So I thought if I start two businesses, I have a higher chance.", "tokens": [50908, 407, 286, 1194, 498, 286, 722, 732, 6011, 11, 286, 362, 257, 2946, 2931, 13, 51074], "temperature": 0.0, "avg_logprob": -0.2024247926824233, "compression_ratio": 1.62012987012987, "no_speech_prob": 0.006381268613040447}, {"id": 47, "seek": 16864, "start": 182.83999999999997, "end": 185.39999999999998, "text": " In hindsight, I was thinking of it as some kind of stochastic thing.", "tokens": [51074, 682, 44357, 11, 286, 390, 1953, 295, 309, 382, 512, 733, 295, 342, 8997, 2750, 551, 13, 51202], "temperature": 0.0, "avg_logprob": -0.2024247926824233, "compression_ratio": 1.62012987012987, "no_speech_prob": 0.006381268613040447}, {"id": 48, "seek": 16864, "start": 185.39999999999998, "end": 190.72, "text": " I didn't have control over it, but it's a bit hard, but anyway.", "tokens": [51202, 286, 994, 380, 362, 1969, 670, 309, 11, 457, 309, 311, 257, 857, 1152, 11, 457, 4033, 13, 51468], "temperature": 0.0, "avg_logprob": -0.2024247926824233, "compression_ratio": 1.62012987012987, "no_speech_prob": 0.006381268613040447}, {"id": 49, "seek": 16864, "start": 190.72, "end": 198.39999999999998, "text": " And then you were president and chief scientist at Kaggle, which obviously is the composition", "tokens": [51468, 400, 550, 291, 645, 3868, 293, 9588, 12662, 412, 48751, 22631, 11, 597, 2745, 307, 264, 12686, 51852], "temperature": 0.0, "avg_logprob": -0.2024247926824233, "compression_ratio": 1.62012987012987, "no_speech_prob": 0.006381268613040447}, {"id": 50, "seek": 19840, "start": 198.4, "end": 201.96, "text": " platform of machine learning.", "tokens": [50364, 3663, 295, 3479, 2539, 13, 50542], "temperature": 0.0, "avg_logprob": -0.22842494461887566, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.033043790608644485}, {"id": 51, "seek": 19840, "start": 201.96, "end": 206.96, "text": " And then in Lytec, where you were working on using deep learning to improve medical diagnostics", "tokens": [50542, 400, 550, 294, 12687, 975, 66, 11, 689, 291, 645, 1364, 322, 1228, 2452, 2539, 281, 3470, 4625, 43215, 1167, 50792], "temperature": 0.0, "avg_logprob": -0.22842494461887566, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.033043790608644485}, {"id": 52, "seek": 19840, "start": 206.96, "end": 207.96, "text": " and clinical decisions.", "tokens": [50792, 293, 9115, 5327, 13, 50842], "temperature": 0.0, "avg_logprob": -0.22842494461887566, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.033043790608644485}, {"id": 53, "seek": 19840, "start": 207.96, "end": 208.96, "text": " Yeah.", "tokens": [50842, 865, 13, 50892], "temperature": 0.0, "avg_logprob": -0.22842494461887566, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.033043790608644485}, {"id": 54, "seek": 19840, "start": 208.96, "end": 210.92000000000002, "text": " That was actually the first company to use deep learning in medicine.", "tokens": [50892, 663, 390, 767, 264, 700, 2237, 281, 764, 2452, 2539, 294, 7195, 13, 50990], "temperature": 0.0, "avg_logprob": -0.22842494461887566, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.033043790608644485}, {"id": 55, "seek": 19840, "start": 210.92000000000002, "end": 213.56, "text": " So it kind of founded the field.", "tokens": [50990, 407, 309, 733, 295, 13234, 264, 2519, 13, 51122], "temperature": 0.0, "avg_logprob": -0.22842494461887566, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.033043790608644485}, {"id": 56, "seek": 19840, "start": 213.56, "end": 216.8, "text": " And even now, that's still like a pretty early phase.", "tokens": [51122, 400, 754, 586, 11, 300, 311, 920, 411, 257, 1238, 2440, 5574, 13, 51284], "temperature": 0.0, "avg_logprob": -0.22842494461887566, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.033043790608644485}, {"id": 57, "seek": 19840, "start": 216.8, "end": 222.12, "text": " And I actually heard you on your new podcast with Tanishk, where you went very, very deep", "tokens": [51284, 400, 286, 767, 2198, 291, 322, 428, 777, 7367, 365, 314, 7524, 74, 11, 689, 291, 1437, 588, 11, 588, 2452, 51550], "temperature": 0.0, "avg_logprob": -0.22842494461887566, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.033043790608644485}, {"id": 58, "seek": 19840, "start": 222.12, "end": 226.56, "text": " into the stuff, the kind of work that he's doing, such a young prodigy at his age.", "tokens": [51550, 666, 264, 1507, 11, 264, 733, 295, 589, 300, 415, 311, 884, 11, 1270, 257, 2037, 15792, 328, 88, 412, 702, 3205, 13, 51772], "temperature": 0.0, "avg_logprob": -0.22842494461887566, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.033043790608644485}, {"id": 59, "seek": 22656, "start": 227.56, "end": 229.76, "text": " Maybe he's too old to be called a prodigy now.", "tokens": [50414, 2704, 415, 311, 886, 1331, 281, 312, 1219, 257, 15792, 328, 88, 586, 13, 50524], "temperature": 0.0, "avg_logprob": -0.258079947494879, "compression_ratio": 1.5261194029850746, "no_speech_prob": 0.028402971103787422}, {"id": 60, "seek": 22656, "start": 229.76, "end": 230.76, "text": " X prodigy.", "tokens": [50524, 1783, 15792, 328, 88, 13, 50574], "temperature": 0.0, "avg_logprob": -0.258079947494879, "compression_ratio": 1.5261194029850746, "no_speech_prob": 0.028402971103787422}, {"id": 61, "seek": 22656, "start": 230.76, "end": 233.68, "text": " No, I think he still counts.", "tokens": [50574, 883, 11, 286, 519, 415, 920, 14893, 13, 50720], "temperature": 0.0, "avg_logprob": -0.258079947494879, "compression_ratio": 1.5261194029850746, "no_speech_prob": 0.028402971103787422}, {"id": 62, "seek": 22656, "start": 233.68, "end": 238.4, "text": " And anyway, just to round out the bio, you have a lot more other credentials, obviously.", "tokens": [50720, 400, 4033, 11, 445, 281, 3098, 484, 264, 12198, 11, 291, 362, 257, 688, 544, 661, 27404, 11, 2745, 13, 50956], "temperature": 0.0, "avg_logprob": -0.258079947494879, "compression_ratio": 1.5261194029850746, "no_speech_prob": 0.028402971103787422}, {"id": 63, "seek": 22656, "start": 238.4, "end": 243.88, "text": " But most recently, you started Fast.AI, which is still, I guess, your primary identity with", "tokens": [50956, 583, 881, 3938, 11, 291, 1409, 15968, 13, 48698, 11, 597, 307, 920, 11, 286, 2041, 11, 428, 6194, 6575, 365, 51230], "temperature": 0.0, "avg_logprob": -0.258079947494879, "compression_ratio": 1.5261194029850746, "no_speech_prob": 0.028402971103787422}, {"id": 64, "seek": 22656, "start": 243.88, "end": 244.88, "text": " Rachel Thomas.", "tokens": [51230, 14246, 8500, 13, 51280], "temperature": 0.0, "avg_logprob": -0.258079947494879, "compression_ratio": 1.5261194029850746, "no_speech_prob": 0.028402971103787422}, {"id": 65, "seek": 22656, "start": 244.88, "end": 245.88, "text": " So welcome.", "tokens": [51280, 407, 2928, 13, 51330], "temperature": 0.0, "avg_logprob": -0.258079947494879, "compression_ratio": 1.5261194029850746, "no_speech_prob": 0.028402971103787422}, {"id": 66, "seek": 22656, "start": 245.88, "end": 246.88, "text": " Thanks.", "tokens": [51330, 2561, 13, 51380], "temperature": 0.0, "avg_logprob": -0.258079947494879, "compression_ratio": 1.5261194029850746, "no_speech_prob": 0.028402971103787422}, {"id": 67, "seek": 22656, "start": 246.88, "end": 247.88, "text": " Thank you.", "tokens": [51380, 1044, 291, 13, 51430], "temperature": 0.0, "avg_logprob": -0.258079947494879, "compression_ratio": 1.5261194029850746, "no_speech_prob": 0.028402971103787422}, {"id": 68, "seek": 22656, "start": 247.88, "end": 248.88, "text": " Yeah.", "tokens": [51430, 865, 13, 51480], "temperature": 0.0, "avg_logprob": -0.258079947494879, "compression_ratio": 1.5261194029850746, "no_speech_prob": 0.028402971103787422}, {"id": 69, "seek": 22656, "start": 248.88, "end": 253.48000000000002, "text": " Being a lot of public service there with getting people involved in AI, and I can imagine", "tokens": [51480, 8891, 257, 688, 295, 1908, 2643, 456, 365, 1242, 561, 3288, 294, 7318, 11, 293, 286, 393, 3811, 51710], "temperature": 0.0, "avg_logprob": -0.258079947494879, "compression_ratio": 1.5261194029850746, "no_speech_prob": 0.028402971103787422}, {"id": 70, "seek": 25348, "start": 253.48, "end": 258.48, "text": " a better way to describe it than Fast.AI is you teach people from nothing to stable", "tokens": [50364, 257, 1101, 636, 281, 6786, 309, 813, 15968, 13, 48698, 307, 291, 2924, 561, 490, 1825, 281, 8351, 50614], "temperature": 0.0, "avg_logprob": -0.18437821010373673, "compression_ratio": 1.680161943319838, "no_speech_prob": 0.006582476664334536}, {"id": 71, "seek": 25348, "start": 258.48, "end": 260.68, "text": " diffusion in seven weeks or something.", "tokens": [50614, 25242, 294, 3407, 3259, 420, 746, 13, 50724], "temperature": 0.0, "avg_logprob": -0.18437821010373673, "compression_ratio": 1.680161943319838, "no_speech_prob": 0.006582476664334536}, {"id": 72, "seek": 25348, "start": 260.68, "end": 261.68, "text": " And that's amazing.", "tokens": [50724, 400, 300, 311, 2243, 13, 50774], "temperature": 0.0, "avg_logprob": -0.18437821010373673, "compression_ratio": 1.680161943319838, "no_speech_prob": 0.006582476664334536}, {"id": 73, "seek": 25348, "start": 261.68, "end": 262.68, "text": " Yeah.", "tokens": [50774, 865, 13, 50824], "temperature": 0.0, "avg_logprob": -0.18437821010373673, "compression_ratio": 1.680161943319838, "no_speech_prob": 0.006582476664334536}, {"id": 74, "seek": 25348, "start": 262.68, "end": 263.68, "text": " Yeah.", "tokens": [50824, 865, 13, 50874], "temperature": 0.0, "avg_logprob": -0.18437821010373673, "compression_ratio": 1.680161943319838, "no_speech_prob": 0.006582476664334536}, {"id": 75, "seek": 25348, "start": 263.68, "end": 264.68, "text": " I mean, it's funny.", "tokens": [50874, 286, 914, 11, 309, 311, 4074, 13, 50924], "temperature": 0.0, "avg_logprob": -0.18437821010373673, "compression_ratio": 1.680161943319838, "no_speech_prob": 0.006582476664334536}, {"id": 76, "seek": 25348, "start": 264.68, "end": 267.4, "text": " When we started that, what was that like 2016 or something?", "tokens": [50924, 1133, 321, 1409, 300, 11, 437, 390, 300, 411, 6549, 420, 746, 30, 51060], "temperature": 0.0, "avg_logprob": -0.18437821010373673, "compression_ratio": 1.680161943319838, "no_speech_prob": 0.006582476664334536}, {"id": 77, "seek": 25348, "start": 267.4, "end": 271.74, "text": " The idea that deep learning was something that you could make more accessible was generally", "tokens": [51060, 440, 1558, 300, 2452, 2539, 390, 746, 300, 291, 727, 652, 544, 9515, 390, 5101, 51277], "temperature": 0.0, "avg_logprob": -0.18437821010373673, "compression_ratio": 1.680161943319838, "no_speech_prob": 0.006582476664334536}, {"id": 78, "seek": 25348, "start": 271.74, "end": 277.96, "text": " considered stupid, but everybody knew that deep learning was a thing that you got a math", "tokens": [51277, 4888, 6631, 11, 457, 2201, 2586, 300, 2452, 2539, 390, 257, 551, 300, 291, 658, 257, 5221, 51588], "temperature": 0.0, "avg_logprob": -0.18437821010373673, "compression_ratio": 1.680161943319838, "no_speech_prob": 0.006582476664334536}, {"id": 79, "seek": 27796, "start": 277.96, "end": 282.84, "text": " through a computer science PhD, you know, those one of five labs that could give you", "tokens": [50364, 807, 257, 3820, 3497, 14476, 11, 291, 458, 11, 729, 472, 295, 1732, 20339, 300, 727, 976, 291, 50608], "temperature": 0.0, "avg_logprob": -0.1950903919255622, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.11736872792243958}, {"id": 80, "seek": 27796, "start": 282.84, "end": 289.71999999999997, "text": " the appropriate skills, then you would join, yeah, basically from one of those labs, you", "tokens": [50608, 264, 6854, 3942, 11, 550, 291, 576, 3917, 11, 1338, 11, 1936, 490, 472, 295, 729, 20339, 11, 291, 50952], "temperature": 0.0, "avg_logprob": -0.1950903919255622, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.11736872792243958}, {"id": 81, "seek": 27796, "start": 289.71999999999997, "end": 292.91999999999996, "text": " might be able to write some papers.", "tokens": [50952, 1062, 312, 1075, 281, 2464, 512, 10577, 13, 51112], "temperature": 0.0, "avg_logprob": -0.1950903919255622, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.11736872792243958}, {"id": 82, "seek": 27796, "start": 292.91999999999996, "end": 300.35999999999996, "text": " So yeah, the idea that normal people could use that technology to do good work was considered", "tokens": [51112, 407, 1338, 11, 264, 1558, 300, 2710, 561, 727, 764, 300, 2899, 281, 360, 665, 589, 390, 4888, 51484], "temperature": 0.0, "avg_logprob": -0.1950903919255622, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.11736872792243958}, {"id": 83, "seek": 27796, "start": 300.35999999999996, "end": 303.2, "text": " kind of ridiculous when we started it.", "tokens": [51484, 733, 295, 11083, 562, 321, 1409, 309, 13, 51626], "temperature": 0.0, "avg_logprob": -0.1950903919255622, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.11736872792243958}, {"id": 84, "seek": 27796, "start": 303.2, "end": 306.08, "text": " And we weren't sure if it was possible either, but we kind of felt like we had to give it", "tokens": [51626, 400, 321, 4999, 380, 988, 498, 309, 390, 1944, 2139, 11, 457, 321, 733, 295, 2762, 411, 321, 632, 281, 976, 309, 51770], "temperature": 0.0, "avg_logprob": -0.1950903919255622, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.11736872792243958}, {"id": 85, "seek": 30608, "start": 306.08, "end": 310.71999999999997, "text": " a go because the alternative was we were pretty sure that deep learning was on its", "tokens": [50364, 257, 352, 570, 264, 8535, 390, 321, 645, 1238, 988, 300, 2452, 2539, 390, 322, 1080, 50596], "temperature": 0.0, "avg_logprob": -0.17415256692905617, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.023680198937654495}, {"id": 86, "seek": 30608, "start": 310.71999999999997, "end": 318.8, "text": " way to becoming the most or one of the most important technologies in human history.", "tokens": [50596, 636, 281, 5617, 264, 881, 420, 472, 295, 264, 881, 1021, 7943, 294, 1952, 2503, 13, 51000], "temperature": 0.0, "avg_logprob": -0.17415256692905617, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.023680198937654495}, {"id": 87, "seek": 30608, "start": 318.8, "end": 324.2, "text": " And if the only people that could use it were a handful of computer science PhDs, that seemed", "tokens": [51000, 400, 498, 264, 787, 561, 300, 727, 764, 309, 645, 257, 16458, 295, 3820, 3497, 14476, 82, 11, 300, 6576, 51270], "temperature": 0.0, "avg_logprob": -0.17415256692905617, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.023680198937654495}, {"id": 88, "seek": 30608, "start": 324.2, "end": 329.44, "text": " like A, a big waste and B, kind of dangerous.", "tokens": [51270, 411, 316, 11, 257, 955, 5964, 293, 363, 11, 733, 295, 5795, 13, 51532], "temperature": 0.0, "avg_logprob": -0.17415256692905617, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.023680198937654495}, {"id": 89, "seek": 30608, "start": 329.44, "end": 333.88, "text": " And you know, well, I just wanted to know one thing on your bio that at Kaggle, you", "tokens": [51532, 400, 291, 458, 11, 731, 11, 286, 445, 1415, 281, 458, 472, 551, 322, 428, 12198, 300, 412, 48751, 22631, 11, 291, 51754], "temperature": 0.0, "avg_logprob": -0.17415256692905617, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.023680198937654495}, {"id": 90, "seek": 33388, "start": 333.88, "end": 337.96, "text": " were also the top rank participant in both 2010 and 2011.", "tokens": [50364, 645, 611, 264, 1192, 6181, 24950, 294, 1293, 9657, 293, 10154, 13, 50568], "temperature": 0.0, "avg_logprob": -0.18198297704969132, "compression_ratio": 1.625, "no_speech_prob": 0.20671890676021576}, {"id": 91, "seek": 33388, "start": 337.96, "end": 342.08, "text": " So sometimes you see a lot of founders running companies that are not really in touch with", "tokens": [50568, 407, 2171, 291, 536, 257, 688, 295, 25608, 2614, 3431, 300, 366, 406, 534, 294, 2557, 365, 50774], "temperature": 0.0, "avg_logprob": -0.18198297704969132, "compression_ratio": 1.625, "no_speech_prob": 0.20671890676021576}, {"id": 92, "seek": 33388, "start": 342.08, "end": 346.68, "text": " the problem, but you were clearly building something that you knew a lot about, which", "tokens": [50774, 264, 1154, 11, 457, 291, 645, 4448, 2390, 746, 300, 291, 2586, 257, 688, 466, 11, 597, 51004], "temperature": 0.0, "avg_logprob": -0.18198297704969132, "compression_ratio": 1.625, "no_speech_prob": 0.20671890676021576}, {"id": 93, "seek": 33388, "start": 346.68, "end": 348.28, "text": " is awesome.", "tokens": [51004, 307, 3476, 13, 51084], "temperature": 0.0, "avg_logprob": -0.18198297704969132, "compression_ratio": 1.625, "no_speech_prob": 0.20671890676021576}, {"id": 94, "seek": 33388, "start": 348.28, "end": 353.64, "text": " And even, yeah, talking about deep learning, you created, published a paper on ULM fit,", "tokens": [51084, 400, 754, 11, 1338, 11, 1417, 466, 2452, 2539, 11, 291, 2942, 11, 6572, 257, 3035, 322, 624, 43, 44, 3318, 11, 51352], "temperature": 0.0, "avg_logprob": -0.18198297704969132, "compression_ratio": 1.625, "no_speech_prob": 0.20671890676021576}, {"id": 95, "seek": 33388, "start": 353.64, "end": 358.4, "text": " which was kind of the predecessor to multitask learning and a lot of the groundwork that", "tokens": [51352, 597, 390, 733, 295, 264, 34991, 281, 42338, 3863, 2539, 293, 257, 688, 295, 264, 2727, 1902, 300, 51590], "temperature": 0.0, "avg_logprob": -0.18198297704969132, "compression_ratio": 1.625, "no_speech_prob": 0.20671890676021576}, {"id": 96, "seek": 33388, "start": 358.4, "end": 360.32, "text": " then went to into transformers.", "tokens": [51590, 550, 1437, 281, 666, 4088, 433, 13, 51686], "temperature": 0.0, "avg_logprob": -0.18198297704969132, "compression_ratio": 1.625, "no_speech_prob": 0.20671890676021576}, {"id": 97, "seek": 36032, "start": 360.32, "end": 367.08, "text": " I read back on the paper and you turn this model AWD LSTM, which I did the math and it", "tokens": [50364, 286, 1401, 646, 322, 264, 3035, 293, 291, 1261, 341, 2316, 25815, 35, 441, 6840, 44, 11, 597, 286, 630, 264, 5221, 293, 309, 50702], "temperature": 0.0, "avg_logprob": -0.14956738533230002, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.014955291524529457}, {"id": 98, "seek": 36032, "start": 367.08, "end": 373.12, "text": " was like 24 to 33 million parameters, depending on what training data set you use today.", "tokens": [50702, 390, 411, 4022, 281, 11816, 2459, 9834, 11, 5413, 322, 437, 3097, 1412, 992, 291, 764, 965, 13, 51004], "temperature": 0.0, "avg_logprob": -0.14956738533230002, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.014955291524529457}, {"id": 99, "seek": 36032, "start": 373.12, "end": 377.28, "text": " That's kind of like not even small, it's like super small.", "tokens": [51004, 663, 311, 733, 295, 411, 406, 754, 1359, 11, 309, 311, 411, 1687, 1359, 13, 51212], "temperature": 0.0, "avg_logprob": -0.14956738533230002, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.014955291524529457}, {"id": 100, "seek": 36032, "start": 377.28, "end": 382.92, "text": " What were some of the kind of like contrarian takes that you had at the time and maybe set", "tokens": [51212, 708, 645, 512, 295, 264, 733, 295, 411, 660, 5352, 952, 2516, 300, 291, 632, 412, 264, 565, 293, 1310, 992, 51494], "temperature": 0.0, "avg_logprob": -0.14956738533230002, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.014955291524529457}, {"id": 101, "seek": 36032, "start": 382.92, "end": 388.32, "text": " the stage a little bit for the rest of the audience on what was kind of like the state", "tokens": [51494, 264, 3233, 257, 707, 857, 337, 264, 1472, 295, 264, 4034, 322, 437, 390, 733, 295, 411, 264, 1785, 51764], "temperature": 0.0, "avg_logprob": -0.14956738533230002, "compression_ratio": 1.6414342629482073, "no_speech_prob": 0.014955291524529457}, {"id": 102, "seek": 38832, "start": 388.32, "end": 392.28, "text": " of the art, so to speak at the time and what people were working towards.", "tokens": [50364, 295, 264, 1523, 11, 370, 281, 1710, 412, 264, 565, 293, 437, 561, 645, 1364, 3030, 13, 50562], "temperature": 0.0, "avg_logprob": -0.2612056046081104, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.2940368950366974}, {"id": 103, "seek": 38832, "start": 392.28, "end": 395.48, "text": " Yeah, the whole thing was a contrarian take, you know.", "tokens": [50562, 865, 11, 264, 1379, 551, 390, 257, 660, 5352, 952, 747, 11, 291, 458, 13, 50722], "temperature": 0.0, "avg_logprob": -0.2612056046081104, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.2940368950366974}, {"id": 104, "seek": 38832, "start": 395.48, "end": 401.84, "text": " So okay, so we started first AI, my wife and I, and we, yeah, so we're trying to think,", "tokens": [50722, 407, 1392, 11, 370, 321, 1409, 700, 7318, 11, 452, 3836, 293, 286, 11, 293, 321, 11, 1338, 11, 370, 321, 434, 1382, 281, 519, 11, 51040], "temperature": 0.0, "avg_logprob": -0.2612056046081104, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.2940368950366974}, {"id": 105, "seek": 38832, "start": 401.84, "end": 403.71999999999997, "text": " okay, how do we make it more accessible?", "tokens": [51040, 1392, 11, 577, 360, 321, 652, 309, 544, 9515, 30, 51134], "temperature": 0.0, "avg_logprob": -0.2612056046081104, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.2940368950366974}, {"id": 106, "seek": 38832, "start": 403.71999999999997, "end": 408.76, "text": " So when we started thinking about it, it was very 2015 and then 2016, we started doing", "tokens": [51134, 407, 562, 321, 1409, 1953, 466, 309, 11, 309, 390, 588, 7546, 293, 550, 6549, 11, 321, 1409, 884, 51386], "temperature": 0.0, "avg_logprob": -0.2612056046081104, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.2940368950366974}, {"id": 107, "seek": 38832, "start": 408.76, "end": 409.76, "text": " something about it.", "tokens": [51386, 746, 466, 309, 13, 51436], "temperature": 0.0, "avg_logprob": -0.2612056046081104, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.2940368950366974}, {"id": 108, "seek": 38832, "start": 409.76, "end": 410.76, "text": " Why is it inaccessible?", "tokens": [51436, 1545, 307, 309, 33230, 780, 964, 30, 51486], "temperature": 0.0, "avg_logprob": -0.2612056046081104, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.2940368950366974}, {"id": 109, "seek": 38832, "start": 410.76, "end": 417.0, "text": " Okay, well, A, no one knows how to do it other than a few number of people and then when", "tokens": [51486, 1033, 11, 731, 11, 316, 11, 572, 472, 3255, 577, 281, 360, 309, 661, 813, 257, 1326, 1230, 295, 561, 293, 550, 562, 51798], "temperature": 0.0, "avg_logprob": -0.2612056046081104, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.2940368950366974}, {"id": 110, "seek": 41700, "start": 417.0, "end": 420.28, "text": " we asked those few number of people, well, how do you actually get good results?", "tokens": [50364, 321, 2351, 729, 1326, 1230, 295, 561, 11, 731, 11, 577, 360, 291, 767, 483, 665, 3542, 30, 50528], "temperature": 0.0, "avg_logprob": -0.23684713575575086, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.05496126040816307}, {"id": 111, "seek": 41700, "start": 420.28, "end": 424.32, "text": " They would say like, oh, it's like, you know, a box of tricks that aren't published.", "tokens": [50528, 814, 576, 584, 411, 11, 1954, 11, 309, 311, 411, 11, 291, 458, 11, 257, 2424, 295, 11733, 300, 3212, 380, 6572, 13, 50730], "temperature": 0.0, "avg_logprob": -0.23684713575575086, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.05496126040816307}, {"id": 112, "seek": 41700, "start": 424.32, "end": 428.28, "text": " So you have to join one of the, you know, labs and learn the tricks.", "tokens": [50730, 407, 291, 362, 281, 3917, 472, 295, 264, 11, 291, 458, 11, 20339, 293, 1466, 264, 11733, 13, 50928], "temperature": 0.0, "avg_logprob": -0.23684713575575086, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.05496126040816307}, {"id": 113, "seek": 41700, "start": 428.28, "end": 434.32, "text": " So a bunch of unpublished tricks, not much software around, but you know, thankfully", "tokens": [50928, 407, 257, 3840, 295, 20994, 836, 4173, 11733, 11, 406, 709, 4722, 926, 11, 457, 291, 458, 11, 27352, 51230], "temperature": 0.0, "avg_logprob": -0.23684713575575086, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.05496126040816307}, {"id": 114, "seek": 41700, "start": 434.32, "end": 441.2, "text": " there was Theano and, you know, rappers and particularly Lasagna, the rapper.", "tokens": [51230, 456, 390, 440, 3730, 293, 11, 291, 458, 11, 45025, 293, 4098, 10663, 35697, 11, 264, 26457, 13, 51574], "temperature": 0.0, "avg_logprob": -0.23684713575575086, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.05496126040816307}, {"id": 115, "seek": 44120, "start": 441.2, "end": 448.15999999999997, "text": " But yeah, not much software around, not much in the way of data sets, you know, very hard", "tokens": [50364, 583, 1338, 11, 406, 709, 4722, 926, 11, 406, 709, 294, 264, 636, 295, 1412, 6352, 11, 291, 458, 11, 588, 1152, 50712], "temperature": 0.0, "avg_logprob": -0.1671724095064051, "compression_ratio": 1.6432432432432433, "no_speech_prob": 0.19185951352119446}, {"id": 116, "seek": 44120, "start": 448.15999999999997, "end": 453.44, "text": " to get started in terms of the compute, like how do you get that set up?", "tokens": [50712, 281, 483, 1409, 294, 2115, 295, 264, 14722, 11, 411, 577, 360, 291, 483, 300, 992, 493, 30, 50976], "temperature": 0.0, "avg_logprob": -0.1671724095064051, "compression_ratio": 1.6432432432432433, "no_speech_prob": 0.19185951352119446}, {"id": 117, "seek": 44120, "start": 453.44, "end": 458.0, "text": " So you know, everything was kind of inaccessible.", "tokens": [50976, 407, 291, 458, 11, 1203, 390, 733, 295, 33230, 780, 964, 13, 51204], "temperature": 0.0, "avg_logprob": -0.1671724095064051, "compression_ratio": 1.6432432432432433, "no_speech_prob": 0.19185951352119446}, {"id": 118, "seek": 44120, "start": 458.0, "end": 465.28, "text": " And you know, as we started looking into it, we had a key insight which was like, you know,", "tokens": [51204, 400, 291, 458, 11, 382, 321, 1409, 1237, 666, 309, 11, 321, 632, 257, 2141, 11269, 597, 390, 411, 11, 291, 458, 11, 51568], "temperature": 0.0, "avg_logprob": -0.1671724095064051, "compression_ratio": 1.6432432432432433, "no_speech_prob": 0.19185951352119446}, {"id": 119, "seek": 46528, "start": 466.03999999999996, "end": 473.44, "text": " most of the compute and data for image recognition, for example, we don't need to do it.", "tokens": [50402, 881, 295, 264, 14722, 293, 1412, 337, 3256, 11150, 11, 337, 1365, 11, 321, 500, 380, 643, 281, 360, 309, 13, 50772], "temperature": 0.0, "avg_logprob": -0.16034928372031765, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.007576182950288057}, {"id": 120, "seek": 46528, "start": 473.44, "end": 477.71999999999997, "text": " You know, there's this thing which nobody knows about, nobody talks about called transfer", "tokens": [50772, 509, 458, 11, 456, 311, 341, 551, 597, 5079, 3255, 466, 11, 5079, 6686, 466, 1219, 5003, 50986], "temperature": 0.0, "avg_logprob": -0.16034928372031765, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.007576182950288057}, {"id": 121, "seek": 46528, "start": 477.71999999999997, "end": 484.03999999999996, "text": " learning where you take somebody else's model where they already figured out like how to", "tokens": [50986, 2539, 689, 291, 747, 2618, 1646, 311, 2316, 689, 436, 1217, 8932, 484, 411, 577, 281, 51302], "temperature": 0.0, "avg_logprob": -0.16034928372031765, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.007576182950288057}, {"id": 122, "seek": 46528, "start": 484.03999999999996, "end": 488.52, "text": " detect edges and gradients and corners and text and whatever else, and then you can fine", "tokens": [51302, 5531, 8819, 293, 2771, 2448, 293, 12413, 293, 2487, 293, 2035, 1646, 11, 293, 550, 291, 393, 2489, 51526], "temperature": 0.0, "avg_logprob": -0.16034928372031765, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.007576182950288057}, {"id": 123, "seek": 46528, "start": 488.52, "end": 491.4, "text": " tune it to do the thing you want to do.", "tokens": [51526, 10864, 309, 281, 360, 264, 551, 291, 528, 281, 360, 13, 51670], "temperature": 0.0, "avg_logprob": -0.16034928372031765, "compression_ratio": 1.7217391304347827, "no_speech_prob": 0.007576182950288057}, {"id": 124, "seek": 49140, "start": 491.4, "end": 496.88, "text": " And we thought that's the key, that's the key to becoming more accessible in terms of", "tokens": [50364, 400, 321, 1194, 300, 311, 264, 2141, 11, 300, 311, 264, 2141, 281, 5617, 544, 9515, 294, 2115, 295, 50638], "temperature": 0.0, "avg_logprob": -0.22158660102136357, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.01639988273382187}, {"id": 125, "seek": 49140, "start": 496.88, "end": 499.2, "text": " compute and data requirements.", "tokens": [50638, 14722, 293, 1412, 7728, 13, 50754], "temperature": 0.0, "avg_logprob": -0.22158660102136357, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.01639988273382187}, {"id": 126, "seek": 49140, "start": 499.2, "end": 504.47999999999996, "text": " So when we started FastAI, we focused from day one on transfer learning, lesson one,", "tokens": [50754, 407, 562, 321, 1409, 15968, 48698, 11, 321, 5178, 490, 786, 472, 322, 5003, 2539, 11, 6898, 472, 11, 51018], "temperature": 0.0, "avg_logprob": -0.22158660102136357, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.01639988273382187}, {"id": 127, "seek": 49140, "start": 504.47999999999996, "end": 509.12, "text": " in fact, was transfer learning, literally lesson one, something not normally even mentioned", "tokens": [51018, 294, 1186, 11, 390, 5003, 2539, 11, 3736, 6898, 472, 11, 746, 406, 5646, 754, 2835, 51250], "temperature": 0.0, "avg_logprob": -0.22158660102136357, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.01639988273382187}, {"id": 128, "seek": 49140, "start": 509.12, "end": 510.12, "text": " in.", "tokens": [51250, 294, 13, 51300], "temperature": 0.0, "avg_logprob": -0.22158660102136357, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.01639988273382187}, {"id": 129, "seek": 49140, "start": 510.12, "end": 516.52, "text": " I mean, there wasn't much in the way of courses, you know, basically, really the courses out", "tokens": [51300, 286, 914, 11, 456, 2067, 380, 709, 294, 264, 636, 295, 7712, 11, 291, 458, 11, 1936, 11, 534, 264, 7712, 484, 51620], "temperature": 0.0, "avg_logprob": -0.22158660102136357, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.01639988273382187}, {"id": 130, "seek": 51652, "start": 516.6, "end": 522.12, "text": " there were PhD programs that had happened to have recorded their lessons, they would", "tokens": [50368, 456, 645, 14476, 4268, 300, 632, 2011, 281, 362, 8287, 641, 8820, 11, 436, 576, 50644], "temperature": 0.0, "avg_logprob": -0.12878348526445407, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.01640007458627224}, {"id": 131, "seek": 51652, "start": 522.12, "end": 523.72, "text": " really mention it at all.", "tokens": [50644, 534, 2152, 309, 412, 439, 13, 50724], "temperature": 0.0, "avg_logprob": -0.12878348526445407, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.01640007458627224}, {"id": 132, "seek": 51652, "start": 523.72, "end": 528.36, "text": " We wanted to show how to do four things that seemed really useful, you know, work with", "tokens": [50724, 492, 1415, 281, 855, 577, 281, 360, 1451, 721, 300, 6576, 534, 4420, 11, 291, 458, 11, 589, 365, 50956], "temperature": 0.0, "avg_logprob": -0.12878348526445407, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.01640007458627224}, {"id": 133, "seek": 51652, "start": 528.36, "end": 535.1999999999999, "text": " vision, work with tables of data, work with kind of recommendation systems and collaborative", "tokens": [50956, 5201, 11, 589, 365, 8020, 295, 1412, 11, 589, 365, 733, 295, 11879, 3652, 293, 16555, 51298], "temperature": 0.0, "avg_logprob": -0.12878348526445407, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.01640007458627224}, {"id": 134, "seek": 51652, "start": 535.1999999999999, "end": 539.64, "text": " filtering and work with text, because we felt like those four kind of modalities covered", "tokens": [51298, 30822, 293, 589, 365, 2487, 11, 570, 321, 2762, 411, 729, 1451, 733, 295, 1072, 16110, 5343, 51520], "temperature": 0.0, "avg_logprob": -0.12878348526445407, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.01640007458627224}, {"id": 135, "seek": 51652, "start": 539.64, "end": 544.6, "text": " a lot of the stuff that, you know, are useful in real life.", "tokens": [51520, 257, 688, 295, 264, 1507, 300, 11, 291, 458, 11, 366, 4420, 294, 957, 993, 13, 51768], "temperature": 0.0, "avg_logprob": -0.12878348526445407, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.01640007458627224}, {"id": 136, "seek": 54460, "start": 544.6800000000001, "end": 547.0, "text": " And no one was doing anything much useful with text.", "tokens": [50368, 400, 572, 472, 390, 884, 1340, 709, 4420, 365, 2487, 13, 50484], "temperature": 0.0, "avg_logprob": -0.3182895617051558, "compression_ratio": 1.603864734299517, "no_speech_prob": 0.04809289425611496}, {"id": 137, "seek": 54460, "start": 547.0, "end": 554.08, "text": " Everybody was talking about Word2Vec, you know, like King plus, Queen minus, woman and", "tokens": [50484, 7646, 390, 1417, 466, 8725, 17, 53, 3045, 11, 291, 458, 11, 411, 3819, 1804, 11, 10077, 3175, 11, 3059, 293, 50838], "temperature": 0.0, "avg_logprob": -0.3182895617051558, "compression_ratio": 1.603864734299517, "no_speech_prob": 0.04809289425611496}, {"id": 138, "seek": 54460, "start": 554.08, "end": 560.48, "text": " blah, blah, blah, and it was like cool experiments, but nobody was doing anything like useful", "tokens": [50838, 12288, 11, 12288, 11, 12288, 11, 293, 309, 390, 411, 1627, 12050, 11, 457, 5079, 390, 884, 1340, 411, 4420, 51158], "temperature": 0.0, "avg_logprob": -0.3182895617051558, "compression_ratio": 1.603864734299517, "no_speech_prob": 0.04809289425611496}, {"id": 139, "seek": 54460, "start": 560.48, "end": 561.48, "text": " with it.", "tokens": [51158, 365, 309, 13, 51208], "temperature": 0.0, "avg_logprob": -0.3182895617051558, "compression_ratio": 1.603864734299517, "no_speech_prob": 0.04809289425611496}, {"id": 140, "seek": 54460, "start": 561.48, "end": 569.88, "text": " NLP was all like lamertization and stop words and topic models and diagrams and SPMs, and", "tokens": [51208, 426, 45196, 390, 439, 411, 24688, 911, 2144, 293, 1590, 2283, 293, 4829, 5245, 293, 36709, 293, 8420, 26386, 11, 293, 51628], "temperature": 0.0, "avg_logprob": -0.3182895617051558, "compression_ratio": 1.603864734299517, "no_speech_prob": 0.04809289425611496}, {"id": 141, "seek": 56988, "start": 569.88, "end": 574.76, "text": " it was really academic and not practical.", "tokens": [50364, 309, 390, 534, 7778, 293, 406, 8496, 13, 50608], "temperature": 0.0, "avg_logprob": -0.16942637675517314, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.004330554511398077}, {"id": 142, "seek": 56988, "start": 574.76, "end": 582.36, "text": " But I mean, to be honest, I've been thinking about this crazy idea for nearly 30 years", "tokens": [50608, 583, 286, 914, 11, 281, 312, 3245, 11, 286, 600, 668, 1953, 466, 341, 3219, 1558, 337, 6217, 2217, 924, 50988], "temperature": 0.0, "avg_logprob": -0.16942637675517314, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.004330554511398077}, {"id": 143, "seek": 56988, "start": 582.36, "end": 589.04, "text": " since I had done cognitive science at university, where we talked a lot about the cells Chinese", "tokens": [50988, 1670, 286, 632, 1096, 15605, 3497, 412, 5454, 11, 689, 321, 2825, 257, 688, 466, 264, 5438, 4649, 51322], "temperature": 0.0, "avg_logprob": -0.16942637675517314, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.004330554511398077}, {"id": 144, "seek": 56988, "start": 589.04, "end": 593.8, "text": " room experiment, this idea of like, what if there was somebody that could kind of like", "tokens": [51322, 1808, 5120, 11, 341, 1558, 295, 411, 11, 437, 498, 456, 390, 2618, 300, 727, 733, 295, 411, 51560], "temperature": 0.0, "avg_logprob": -0.16942637675517314, "compression_ratio": 1.4880382775119618, "no_speech_prob": 0.004330554511398077}, {"id": 145, "seek": 59380, "start": 593.8, "end": 601.0, "text": " knew all of the symbolic manipulations required to answer questions in Chinese, but they didn't", "tokens": [50364, 2586, 439, 295, 264, 25755, 9258, 4136, 4739, 281, 1867, 1651, 294, 4649, 11, 457, 436, 994, 380, 50724], "temperature": 0.0, "avg_logprob": -0.12688182830810546, "compression_ratio": 1.8512396694214877, "no_speech_prob": 0.02930113673210144}, {"id": 146, "seek": 59380, "start": 601.0, "end": 606.64, "text": " speak Chinese, they were kind of inside a room with no other way to talk to the outside", "tokens": [50724, 1710, 4649, 11, 436, 645, 733, 295, 1854, 257, 1808, 365, 572, 661, 636, 281, 751, 281, 264, 2380, 51006], "temperature": 0.0, "avg_logprob": -0.12688182830810546, "compression_ratio": 1.8512396694214877, "no_speech_prob": 0.02930113673210144}, {"id": 147, "seek": 59380, "start": 606.64, "end": 610.0799999999999, "text": " world other than taking in slips of paper with Chinese written on them and then they", "tokens": [51006, 1002, 661, 813, 1940, 294, 44690, 295, 3035, 365, 4649, 3720, 322, 552, 293, 550, 436, 51178], "temperature": 0.0, "avg_logprob": -0.12688182830810546, "compression_ratio": 1.8512396694214877, "no_speech_prob": 0.02930113673210144}, {"id": 148, "seek": 59380, "start": 610.0799999999999, "end": 615.28, "text": " do all their rules and then they pass back a piece of paper with Chinese back and this", "tokens": [51178, 360, 439, 641, 4474, 293, 550, 436, 1320, 646, 257, 2522, 295, 3035, 365, 4649, 646, 293, 341, 51438], "temperature": 0.0, "avg_logprob": -0.12688182830810546, "compression_ratio": 1.8512396694214877, "no_speech_prob": 0.02930113673210144}, {"id": 149, "seek": 59380, "start": 615.28, "end": 619.52, "text": " room with a person in is actually fantastically good at answering any question you give them", "tokens": [51438, 1808, 365, 257, 954, 294, 307, 767, 4115, 22808, 665, 412, 13430, 604, 1168, 291, 976, 552, 51650], "temperature": 0.0, "avg_logprob": -0.12688182830810546, "compression_ratio": 1.8512396694214877, "no_speech_prob": 0.02930113673210144}, {"id": 150, "seek": 61952, "start": 619.52, "end": 628.3199999999999, "text": " written in Chinese, do they understand Chinese and is this something that's intelligently", "tokens": [50364, 3720, 294, 4649, 11, 360, 436, 1223, 4649, 293, 307, 341, 746, 300, 311, 5613, 2276, 50804], "temperature": 0.0, "avg_logprob": -0.2635435564764615, "compression_ratio": 1.5317919075144508, "no_speech_prob": 0.43739721179008484}, {"id": 151, "seek": 61952, "start": 628.3199999999999, "end": 635.28, "text": " working with Chinese ever since that time, I'd say to me the most thoughtful and compelling", "tokens": [50804, 1364, 365, 4649, 1562, 1670, 300, 565, 11, 286, 1116, 584, 281, 385, 264, 881, 21566, 293, 20050, 51152], "temperature": 0.0, "avg_logprob": -0.2635435564764615, "compression_ratio": 1.5317919075144508, "no_speech_prob": 0.43739721179008484}, {"id": 152, "seek": 61952, "start": 635.28, "end": 643.0799999999999, "text": " philosophical response is yes, intuitively it feels like no, that's just because we", "tokens": [51152, 25066, 4134, 307, 2086, 11, 46506, 309, 3417, 411, 572, 11, 300, 311, 445, 570, 321, 51542], "temperature": 0.0, "avg_logprob": -0.2635435564764615, "compression_ratio": 1.5317919075144508, "no_speech_prob": 0.43739721179008484}, {"id": 153, "seek": 64308, "start": 643.08, "end": 649.76, "text": " can't imagine such a large kind of system, but if it looks like a duck and acts like", "tokens": [50364, 393, 380, 3811, 1270, 257, 2416, 733, 295, 1185, 11, 457, 498, 309, 1542, 411, 257, 12482, 293, 10672, 411, 50698], "temperature": 0.0, "avg_logprob": -0.20528904596964517, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.3556039333343506}, {"id": 154, "seek": 64308, "start": 649.76, "end": 654.1600000000001, "text": " a duck, it's a duck or to all intents and purposes.", "tokens": [50698, 257, 12482, 11, 309, 311, 257, 12482, 420, 281, 439, 560, 791, 293, 9932, 13, 50918], "temperature": 0.0, "avg_logprob": -0.20528904596964517, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.3556039333343506}, {"id": 155, "seek": 64308, "start": 654.1600000000001, "end": 659.0400000000001, "text": " And so I always kind of thought, so this is basically a kind of analysis of the limits", "tokens": [50918, 400, 370, 286, 1009, 733, 295, 1194, 11, 370, 341, 307, 1936, 257, 733, 295, 5215, 295, 264, 10406, 51162], "temperature": 0.0, "avg_logprob": -0.20528904596964517, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.3556039333343506}, {"id": 156, "seek": 64308, "start": 659.0400000000001, "end": 665.5200000000001, "text": " of text and I kind of felt like, yeah, if something could ingest enough text and could", "tokens": [51162, 295, 2487, 293, 286, 733, 295, 2762, 411, 11, 1338, 11, 498, 746, 727, 3957, 377, 1547, 2487, 293, 727, 51486], "temperature": 0.0, "avg_logprob": -0.20528904596964517, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.3556039333343506}, {"id": 157, "seek": 66552, "start": 665.52, "end": 677.0, "text": " use the patterns it saw to then generate text in response to text, it could appear to be", "tokens": [50364, 764, 264, 8294, 309, 1866, 281, 550, 8460, 2487, 294, 4134, 281, 2487, 11, 309, 727, 4204, 281, 312, 50938], "temperature": 0.0, "avg_logprob": -0.18655461850373642, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.2392600029706955}, {"id": 158, "seek": 66552, "start": 677.0, "end": 682.64, "text": " intelligent, whether that means it is intelligent or not is a different discussion and not one", "tokens": [50938, 13232, 11, 1968, 300, 1355, 309, 307, 13232, 420, 406, 307, 257, 819, 5017, 293, 406, 472, 51220], "temperature": 0.0, "avg_logprob": -0.18655461850373642, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.2392600029706955}, {"id": 159, "seek": 66552, "start": 682.64, "end": 683.88, "text": " I find very interesting.", "tokens": [51220, 286, 915, 588, 1880, 13, 51282], "temperature": 0.0, "avg_logprob": -0.18655461850373642, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.2392600029706955}, {"id": 160, "seek": 66552, "start": 683.88, "end": 688.96, "text": " Yeah, and then when I came across neural nets when I was about 20, I learned about the universal", "tokens": [51282, 865, 11, 293, 550, 562, 286, 1361, 2108, 18161, 36170, 562, 286, 390, 466, 945, 11, 286, 3264, 466, 264, 11455, 51536], "temperature": 0.0, "avg_logprob": -0.18655461850373642, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.2392600029706955}, {"id": 161, "seek": 66552, "start": 688.96, "end": 693.12, "text": " approximation theorem and stuff and I started thinking like, oh, I wonder if like a neural", "tokens": [51536, 28023, 20904, 293, 1507, 293, 286, 1409, 1953, 411, 11, 1954, 11, 286, 2441, 498, 411, 257, 18161, 51744], "temperature": 0.0, "avg_logprob": -0.18655461850373642, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.2392600029706955}, {"id": 162, "seek": 69312, "start": 693.12, "end": 700.76, "text": " net could ever get big enough, take in enough data to be a Chinese room experiment.", "tokens": [50364, 2533, 727, 1562, 483, 955, 1547, 11, 747, 294, 1547, 1412, 281, 312, 257, 4649, 1808, 5120, 13, 50746], "temperature": 0.0, "avg_logprob": -0.18572063446044923, "compression_ratio": 1.5219298245614035, "no_speech_prob": 0.02096075750887394}, {"id": 163, "seek": 69312, "start": 700.76, "end": 706.92, "text": " With that background and this kind of like interest in transfer learning, I'd been thinking", "tokens": [50746, 2022, 300, 3678, 293, 341, 733, 295, 411, 1179, 294, 5003, 2539, 11, 286, 1116, 668, 1953, 51054], "temperature": 0.0, "avg_logprob": -0.18572063446044923, "compression_ratio": 1.5219298245614035, "no_speech_prob": 0.02096075750887394}, {"id": 164, "seek": 69312, "start": 706.92, "end": 710.32, "text": " about this thing for kind of 30 years and I thought like, oh, I wonder if we're there", "tokens": [51054, 466, 341, 551, 337, 733, 295, 2217, 924, 293, 286, 1194, 411, 11, 1954, 11, 286, 2441, 498, 321, 434, 456, 51224], "temperature": 0.0, "avg_logprob": -0.18572063446044923, "compression_ratio": 1.5219298245614035, "no_speech_prob": 0.02096075750887394}, {"id": 165, "seek": 69312, "start": 710.32, "end": 716.64, "text": " yet, because we have a lot of text, like I can literally download Wikipedia, which is", "tokens": [51224, 1939, 11, 570, 321, 362, 257, 688, 295, 2487, 11, 411, 286, 393, 3736, 5484, 28999, 11, 597, 307, 51540], "temperature": 0.0, "avg_logprob": -0.18572063446044923, "compression_ratio": 1.5219298245614035, "no_speech_prob": 0.02096075750887394}, {"id": 166, "seek": 71664, "start": 716.64, "end": 718.48, "text": " a lot of text.", "tokens": [50364, 257, 688, 295, 2487, 13, 50456], "temperature": 0.0, "avg_logprob": -0.17957385641629578, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.414716511964798}, {"id": 167, "seek": 71664, "start": 718.48, "end": 725.16, "text": " And I thought, you know, how would something learn to kind of answer questions or respond", "tokens": [50456, 400, 286, 1194, 11, 291, 458, 11, 577, 576, 746, 1466, 281, 733, 295, 1867, 1651, 420, 4196, 50790], "temperature": 0.0, "avg_logprob": -0.17957385641629578, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.414716511964798}, {"id": 168, "seek": 71664, "start": 725.16, "end": 726.16, "text": " text?", "tokens": [50790, 2487, 30, 50840], "temperature": 0.0, "avg_logprob": -0.17957385641629578, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.414716511964798}, {"id": 169, "seek": 71664, "start": 726.16, "end": 728.24, "text": " And I thought, well, what if we used a language model?", "tokens": [50840, 400, 286, 1194, 11, 731, 11, 437, 498, 321, 1143, 257, 2856, 2316, 30, 50944], "temperature": 0.0, "avg_logprob": -0.17957385641629578, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.414716511964798}, {"id": 170, "seek": 71664, "start": 728.24, "end": 731.68, "text": " So language models are already a thing, you know, they were not a popular or well known", "tokens": [50944, 407, 2856, 5245, 366, 1217, 257, 551, 11, 291, 458, 11, 436, 645, 406, 257, 3743, 420, 731, 2570, 51116], "temperature": 0.0, "avg_logprob": -0.17957385641629578, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.414716511964798}, {"id": 171, "seek": 71664, "start": 731.68, "end": 732.68, "text": " thing, but they were a thing.", "tokens": [51116, 551, 11, 457, 436, 645, 257, 551, 13, 51166], "temperature": 0.0, "avg_logprob": -0.17957385641629578, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.414716511964798}, {"id": 172, "seek": 71664, "start": 732.68, "end": 737.08, "text": " But language models exist to this idea that you could train a model to fill in the gaps", "tokens": [51166, 583, 2856, 5245, 2514, 281, 341, 1558, 300, 291, 727, 3847, 257, 2316, 281, 2836, 294, 264, 15031, 51386], "temperature": 0.0, "avg_logprob": -0.17957385641629578, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.414716511964798}, {"id": 173, "seek": 71664, "start": 737.08, "end": 740.96, "text": " or actually in those days, it wasn't fill in the gaps, it was finish a string.", "tokens": [51386, 420, 767, 294, 729, 1708, 11, 309, 2067, 380, 2836, 294, 264, 15031, 11, 309, 390, 2413, 257, 6798, 13, 51580], "temperature": 0.0, "avg_logprob": -0.17957385641629578, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.414716511964798}, {"id": 174, "seek": 74096, "start": 740.96, "end": 747.48, "text": " In fact, Andre Kapathy did his fantastic RNN demonstration from this at a similar time", "tokens": [50364, 682, 1186, 11, 20667, 21216, 9527, 630, 702, 5456, 45702, 45, 16520, 490, 341, 412, 257, 2531, 565, 50690], "temperature": 0.0, "avg_logprob": -0.18873496850331625, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.0953061580657959}, {"id": 175, "seek": 74096, "start": 747.48, "end": 753.24, "text": " where he showed like you can have it ingest Shakespeare and it will generate something", "tokens": [50690, 689, 415, 4712, 411, 291, 393, 362, 309, 3957, 377, 22825, 293, 309, 486, 8460, 746, 50978], "temperature": 0.0, "avg_logprob": -0.18873496850331625, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.0953061580657959}, {"id": 176, "seek": 74096, "start": 753.24, "end": 755.44, "text": " that looks a bit like Shakespeare.", "tokens": [50978, 300, 1542, 257, 857, 411, 22825, 13, 51088], "temperature": 0.0, "avg_logprob": -0.18873496850331625, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.0953061580657959}, {"id": 177, "seek": 74096, "start": 755.44, "end": 763.08, "text": " I thought, okay, so if I do this at a much bigger scale using all of Wikipedia, what", "tokens": [51088, 286, 1194, 11, 1392, 11, 370, 498, 286, 360, 341, 412, 257, 709, 3801, 4373, 1228, 439, 295, 28999, 11, 437, 51470], "temperature": 0.0, "avg_logprob": -0.18873496850331625, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.0953061580657959}, {"id": 178, "seek": 76308, "start": 763.2, "end": 770.88, "text": " would it need to be able to do to finish a sentence in Wikipedia effectively, to do it", "tokens": [50370, 576, 309, 643, 281, 312, 1075, 281, 360, 281, 2413, 257, 8174, 294, 28999, 8659, 11, 281, 360, 309, 50754], "temperature": 0.0, "avg_logprob": -0.18180550847734725, "compression_ratio": 1.9831223628691983, "no_speech_prob": 0.36270660161972046}, {"id": 179, "seek": 76308, "start": 770.88, "end": 772.5600000000001, "text": " quite accurately quite often?", "tokens": [50754, 1596, 20095, 1596, 2049, 30, 50838], "temperature": 0.0, "avg_logprob": -0.18180550847734725, "compression_ratio": 1.9831223628691983, "no_speech_prob": 0.36270660161972046}, {"id": 180, "seek": 76308, "start": 772.5600000000001, "end": 775.76, "text": " I thought, Jesus, it would actually have to know a lot about the world, you know, it", "tokens": [50838, 286, 1194, 11, 2705, 11, 309, 576, 767, 362, 281, 458, 257, 688, 466, 264, 1002, 11, 291, 458, 11, 309, 50998], "temperature": 0.0, "avg_logprob": -0.18180550847734725, "compression_ratio": 1.9831223628691983, "no_speech_prob": 0.36270660161972046}, {"id": 181, "seek": 76308, "start": 775.76, "end": 779.1600000000001, "text": " would have to know that there is a world and that there are objects and that objects relate", "tokens": [50998, 576, 362, 281, 458, 300, 456, 307, 257, 1002, 293, 300, 456, 366, 6565, 293, 300, 6565, 10961, 51168], "temperature": 0.0, "avg_logprob": -0.18180550847734725, "compression_ratio": 1.9831223628691983, "no_speech_prob": 0.36270660161972046}, {"id": 182, "seek": 76308, "start": 779.1600000000001, "end": 783.88, "text": " to each other through time and cause each other to react in ways and that causes proceed", "tokens": [51168, 281, 1184, 661, 807, 565, 293, 3082, 1184, 661, 281, 4515, 294, 2098, 293, 300, 7700, 8991, 51404], "temperature": 0.0, "avg_logprob": -0.18180550847734725, "compression_ratio": 1.9831223628691983, "no_speech_prob": 0.36270660161972046}, {"id": 183, "seek": 76308, "start": 783.88, "end": 789.6800000000001, "text": " effects and that, you know, when there are animals and there are people and that people", "tokens": [51404, 5065, 293, 300, 11, 291, 458, 11, 562, 456, 366, 4882, 293, 456, 366, 561, 293, 300, 561, 51694], "temperature": 0.0, "avg_logprob": -0.18180550847734725, "compression_ratio": 1.9831223628691983, "no_speech_prob": 0.36270660161972046}, {"id": 184, "seek": 78968, "start": 789.76, "end": 794.88, "text": " couldn't be in certain positions during certain timeframes and then you could, you know, all", "tokens": [50368, 2809, 380, 312, 294, 1629, 8432, 1830, 1629, 565, 43277, 293, 550, 291, 727, 11, 291, 458, 11, 439, 50624], "temperature": 0.0, "avg_logprob": -0.20445689033059514, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.05028552934527397}, {"id": 185, "seek": 78968, "start": 794.88, "end": 801.16, "text": " that together, you can then finish a sentence like this was signed into law in 2016 by US", "tokens": [50624, 300, 1214, 11, 291, 393, 550, 2413, 257, 8174, 411, 341, 390, 8175, 666, 2101, 294, 6549, 538, 2546, 50938], "temperature": 0.0, "avg_logprob": -0.20445689033059514, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.05028552934527397}, {"id": 186, "seek": 78968, "start": 801.16, "end": 804.5999999999999, "text": " President X and it would fill in the gaps, you know.", "tokens": [50938, 3117, 1783, 293, 309, 576, 2836, 294, 264, 15031, 11, 291, 458, 13, 51110], "temperature": 0.0, "avg_logprob": -0.20445689033059514, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.05028552934527397}, {"id": 187, "seek": 78968, "start": 804.5999999999999, "end": 810.1999999999999, "text": " So that's why I tried to create a, what in those days was considered a big language model", "tokens": [51110, 407, 300, 311, 983, 286, 3031, 281, 1884, 257, 11, 437, 294, 729, 1708, 390, 4888, 257, 955, 2856, 2316, 51390], "temperature": 0.0, "avg_logprob": -0.20445689033059514, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.05028552934527397}, {"id": 188, "seek": 78968, "start": 810.1999999999999, "end": 814.56, "text": " trained on the entirety on Wikipedia, which is that was a bit unheard of and my interest", "tokens": [51390, 8895, 322, 264, 31557, 322, 28999, 11, 597, 307, 300, 390, 257, 857, 517, 42915, 295, 293, 452, 1179, 51608], "temperature": 0.0, "avg_logprob": -0.20445689033059514, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.05028552934527397}, {"id": 189, "seek": 81456, "start": 814.56, "end": 821.1199999999999, "text": " was not in, you know, just having a language model, my interest was in like, what latent", "tokens": [50364, 390, 406, 294, 11, 291, 458, 11, 445, 1419, 257, 2856, 2316, 11, 452, 1179, 390, 294, 411, 11, 437, 48994, 50692], "temperature": 0.0, "avg_logprob": -0.12075641307424992, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.011155065149068832}, {"id": 190, "seek": 81456, "start": 821.1199999999999, "end": 831.4799999999999, "text": " capabilities would such a system have that would allow it to finish those kind of sentences?", "tokens": [50692, 10862, 576, 1270, 257, 1185, 362, 300, 576, 2089, 309, 281, 2413, 729, 733, 295, 16579, 30, 51210], "temperature": 0.0, "avg_logprob": -0.12075641307424992, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.011155065149068832}, {"id": 191, "seek": 81456, "start": 831.4799999999999, "end": 836.4399999999999, "text": " Because I was pretty sure based on our work with transfer learning and vision that I could", "tokens": [51210, 1436, 286, 390, 1238, 988, 2361, 322, 527, 589, 365, 5003, 2539, 293, 5201, 300, 286, 727, 51458], "temperature": 0.0, "avg_logprob": -0.12075641307424992, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.011155065149068832}, {"id": 192, "seek": 81456, "start": 836.4399999999999, "end": 842.28, "text": " then suck out those latent capabilities by transfer learning, you know, by fine-tuning", "tokens": [51458, 550, 9967, 484, 729, 48994, 10862, 538, 5003, 2539, 11, 291, 458, 11, 538, 2489, 12, 83, 37726, 51750], "temperature": 0.0, "avg_logprob": -0.12075641307424992, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.011155065149068832}, {"id": 193, "seek": 81456, "start": 842.28, "end": 844.3199999999999, "text": " it on a task data set or whatever.", "tokens": [51750, 309, 322, 257, 5633, 1412, 992, 420, 2035, 13, 51852], "temperature": 0.0, "avg_logprob": -0.12075641307424992, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.011155065149068832}, {"id": 194, "seek": 84432, "start": 844.32, "end": 846.48, "text": " So we generated this three-step system.", "tokens": [50364, 407, 321, 10833, 341, 1045, 12, 16792, 1185, 13, 50472], "temperature": 0.0, "avg_logprob": -0.17266562727631116, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0006665576947852969}, {"id": 195, "seek": 84432, "start": 846.48, "end": 849.84, "text": " So step one was train a language model on a big corpus.", "tokens": [50472, 407, 1823, 472, 390, 3847, 257, 2856, 2316, 322, 257, 955, 1181, 31624, 13, 50640], "temperature": 0.0, "avg_logprob": -0.17266562727631116, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0006665576947852969}, {"id": 196, "seek": 84432, "start": 849.84, "end": 855.72, "text": " Step two was fine-tune a language model on a more curated corpus and step three was further", "tokens": [50640, 5470, 732, 390, 2489, 12, 83, 2613, 257, 2856, 2316, 322, 257, 544, 47851, 1181, 31624, 293, 1823, 1045, 390, 3052, 50934], "temperature": 0.0, "avg_logprob": -0.17266562727631116, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0006665576947852969}, {"id": 197, "seek": 84432, "start": 855.72, "end": 861.1600000000001, "text": " fine-tune that model on a task and of course that's why everybody still does today, right?", "tokens": [50934, 2489, 12, 83, 2613, 300, 2316, 322, 257, 5633, 293, 295, 1164, 300, 311, 983, 2201, 920, 775, 965, 11, 558, 30, 51206], "temperature": 0.0, "avg_logprob": -0.17266562727631116, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0006665576947852969}, {"id": 198, "seek": 84432, "start": 861.1600000000001, "end": 869.0, "text": " That's what chat GPT is and so the first time I tried it within hours I had a new state", "tokens": [51206, 663, 311, 437, 5081, 26039, 51, 307, 293, 370, 264, 700, 565, 286, 3031, 309, 1951, 2496, 286, 632, 257, 777, 1785, 51598], "temperature": 0.0, "avg_logprob": -0.17266562727631116, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0006665576947852969}, {"id": 199, "seek": 84432, "start": 869.0, "end": 874.2, "text": " of the art academic result on IMDB and I was like, holy shit, it does work.", "tokens": [51598, 295, 264, 1523, 7778, 1874, 322, 21463, 27735, 293, 286, 390, 411, 11, 10622, 4611, 11, 309, 775, 589, 13, 51858], "temperature": 0.0, "avg_logprob": -0.17266562727631116, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0006665576947852969}, {"id": 200, "seek": 87420, "start": 874.76, "end": 881.12, "text": " So you asked to what degree was this kind of like pushing against the established wisdom?", "tokens": [50392, 407, 291, 2351, 281, 437, 4314, 390, 341, 733, 295, 411, 7380, 1970, 264, 7545, 10712, 30, 50710], "temperature": 0.0, "avg_logprob": -0.19011268944575868, "compression_ratio": 1.6977611940298507, "no_speech_prob": 0.021604742854833603}, {"id": 201, "seek": 87420, "start": 881.12, "end": 886.1600000000001, "text": " Every way, like the reason it took me so long to try it was because I asked all my friends", "tokens": [50710, 2048, 636, 11, 411, 264, 1778, 309, 1890, 385, 370, 938, 281, 853, 309, 390, 570, 286, 2351, 439, 452, 1855, 50962], "temperature": 0.0, "avg_logprob": -0.19011268944575868, "compression_ratio": 1.6977611940298507, "no_speech_prob": 0.021604742854833603}, {"id": 202, "seek": 87420, "start": 886.1600000000001, "end": 891.12, "text": " in NLP if this could work and everybody said no, it definitely won't work.", "tokens": [50962, 294, 426, 45196, 498, 341, 727, 589, 293, 2201, 848, 572, 11, 309, 2138, 1582, 380, 589, 13, 51210], "temperature": 0.0, "avg_logprob": -0.19011268944575868, "compression_ratio": 1.6977611940298507, "no_speech_prob": 0.021604742854833603}, {"id": 203, "seek": 87420, "start": 891.12, "end": 895.4000000000001, "text": " It wasn't like, oh maybe, everybody was like, it definitely won't work.", "tokens": [51210, 467, 2067, 380, 411, 11, 1954, 1310, 11, 2201, 390, 411, 11, 309, 2138, 1582, 380, 589, 13, 51424], "temperature": 0.0, "avg_logprob": -0.19011268944575868, "compression_ratio": 1.6977611940298507, "no_speech_prob": 0.021604742854833603}, {"id": 204, "seek": 87420, "start": 895.4000000000001, "end": 898.32, "text": " NLP is much more complicated than vision.", "tokens": [51424, 426, 45196, 307, 709, 544, 6179, 813, 5201, 13, 51570], "temperature": 0.0, "avg_logprob": -0.19011268944575868, "compression_ratio": 1.6977611940298507, "no_speech_prob": 0.021604742854833603}, {"id": 205, "seek": 87420, "start": 898.32, "end": 901.76, "text": " Languages are much more vastly complicated to main, you know, and you've got problems", "tokens": [51570, 13313, 84, 1660, 366, 709, 544, 41426, 6179, 281, 2135, 11, 291, 458, 11, 293, 291, 600, 658, 2740, 51742], "temperature": 0.0, "avg_logprob": -0.19011268944575868, "compression_ratio": 1.6977611940298507, "no_speech_prob": 0.021604742854833603}, {"id": 206, "seek": 90176, "start": 901.76, "end": 905.56, "text": " like the grounding problem we know from like philosophy and theory of mind that it's actually", "tokens": [50364, 411, 264, 46727, 1154, 321, 458, 490, 411, 10675, 293, 5261, 295, 1575, 300, 309, 311, 767, 50554], "temperature": 0.0, "avg_logprob": -0.2336058716673951, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.00141006032936275}, {"id": 207, "seek": 90176, "start": 905.56, "end": 907.56, "text": " impossible for it to work.", "tokens": [50554, 6243, 337, 309, 281, 589, 13, 50654], "temperature": 0.0, "avg_logprob": -0.2336058716673951, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.00141006032936275}, {"id": 208, "seek": 90176, "start": 907.56, "end": 910.72, "text": " So yeah, so don't waste your time.", "tokens": [50654, 407, 1338, 11, 370, 500, 380, 5964, 428, 565, 13, 50812], "temperature": 0.0, "avg_logprob": -0.2336058716673951, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.00141006032936275}, {"id": 209, "seek": 90176, "start": 910.72, "end": 916.0, "text": " Jeremy, had people not tried because it was like too complicated to actually get the data", "tokens": [50812, 17809, 11, 632, 561, 406, 3031, 570, 309, 390, 411, 886, 6179, 281, 767, 483, 264, 1412, 51076], "temperature": 0.0, "avg_logprob": -0.2336058716673951, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.00141006032936275}, {"id": 210, "seek": 90176, "start": 916.0, "end": 920.2, "text": " and like set up the training or like were people just lazy and kind of like, hey, this", "tokens": [51076, 293, 411, 992, 493, 264, 3097, 420, 411, 645, 561, 445, 14847, 293, 733, 295, 411, 11, 4177, 11, 341, 51286], "temperature": 0.0, "avg_logprob": -0.2336058716673951, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.00141006032936275}, {"id": 211, "seek": 90176, "start": 920.2, "end": 921.2, "text": " is just not going to work.", "tokens": [51286, 307, 445, 406, 516, 281, 589, 13, 51336], "temperature": 0.0, "avg_logprob": -0.2336058716673951, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.00141006032936275}, {"id": 212, "seek": 90176, "start": 921.2, "end": 922.2, "text": " No, it was lazy.", "tokens": [51336, 883, 11, 309, 390, 14847, 13, 51386], "temperature": 0.0, "avg_logprob": -0.2336058716673951, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.00141006032936275}, {"id": 213, "seek": 90176, "start": 922.2, "end": 925.96, "text": " So like, so the person I thought at that time who, there were two people I thought at that", "tokens": [51386, 407, 411, 11, 370, 264, 954, 286, 1194, 412, 300, 565, 567, 11, 456, 645, 732, 561, 286, 1194, 412, 300, 51574], "temperature": 0.0, "avg_logprob": -0.2336058716673951, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.00141006032936275}, {"id": 214, "seek": 90176, "start": 925.96, "end": 931.28, "text": " time actually who were the strongest at language models were Stephen Merity and Alec Radford.", "tokens": [51574, 565, 767, 567, 645, 264, 16595, 412, 2856, 5245, 645, 13391, 6124, 507, 293, 9366, 66, 9654, 7404, 13, 51840], "temperature": 0.0, "avg_logprob": -0.2336058716673951, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.00141006032936275}, {"id": 215, "seek": 93128, "start": 931.8, "end": 938.52, "text": " And at the time I didn't know Alec, but I, after we had both, after I'd released ULM Fit and", "tokens": [50390, 400, 412, 264, 565, 286, 994, 380, 458, 9366, 66, 11, 457, 286, 11, 934, 321, 632, 1293, 11, 934, 286, 1116, 4736, 624, 43, 44, 29263, 293, 50726], "temperature": 0.0, "avg_logprob": -0.2156584282883075, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.01064970251172781}, {"id": 216, "seek": 93128, "start": 938.52, "end": 945.24, "text": " he had released GPT, I organized a chat for both of us with Kate Metz of the New York", "tokens": [50726, 415, 632, 4736, 26039, 51, 11, 286, 9983, 257, 5081, 337, 1293, 295, 505, 365, 16251, 6377, 89, 295, 264, 1873, 3609, 51062], "temperature": 0.0, "avg_logprob": -0.2156584282883075, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.01064970251172781}, {"id": 217, "seek": 93128, "start": 945.24, "end": 950.16, "text": " Times and Kate Metz answered, and Alec answered this question for Kate and Kate just like,", "tokens": [51062, 11366, 293, 16251, 6377, 89, 10103, 11, 293, 9366, 66, 10103, 341, 1168, 337, 16251, 293, 16251, 445, 411, 11, 51308], "temperature": 0.0, "avg_logprob": -0.2156584282883075, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.01064970251172781}, {"id": 218, "seek": 93128, "start": 950.16, "end": 953.88, "text": " so how did, you know, GPT come about?", "tokens": [51308, 370, 577, 630, 11, 291, 458, 11, 26039, 51, 808, 466, 30, 51494], "temperature": 0.0, "avg_logprob": -0.2156584282883075, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.01064970251172781}, {"id": 219, "seek": 93128, "start": 953.88, "end": 959.68, "text": " And he said, well, I was pretty sure that pre-training on a general large corpus wouldn't work.", "tokens": [51494, 400, 415, 848, 11, 731, 11, 286, 390, 1238, 988, 300, 659, 12, 17227, 1760, 322, 257, 2674, 2416, 1181, 31624, 2759, 380, 589, 13, 51784], "temperature": 0.0, "avg_logprob": -0.2156584282883075, "compression_ratio": 1.592885375494071, "no_speech_prob": 0.01064970251172781}, {"id": 220, "seek": 95968, "start": 959.68, "end": 961.4799999999999, "text": " So I hadn't tried it.", "tokens": [50364, 407, 286, 8782, 380, 3031, 309, 13, 50454], "temperature": 0.0, "avg_logprob": -0.2113799733563888, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006096806842833757}, {"id": 221, "seek": 95968, "start": 961.4799999999999, "end": 966.28, "text": " And then I read ULM Fit and turns out it did work.", "tokens": [50454, 400, 550, 286, 1401, 624, 43, 44, 29263, 293, 4523, 484, 309, 630, 589, 13, 50694], "temperature": 0.0, "avg_logprob": -0.2113799733563888, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006096806842833757}, {"id": 222, "seek": 95968, "start": 966.28, "end": 969.68, "text": " And so I did it, you know, bigger and it worked even better.", "tokens": [50694, 400, 370, 286, 630, 309, 11, 291, 458, 11, 3801, 293, 309, 2732, 754, 1101, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2113799733563888, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006096806842833757}, {"id": 223, "seek": 95968, "start": 969.68, "end": 975.16, "text": " And similar with Stephen, you know, I asked Stephen Merity, like, why don't we just find,", "tokens": [50864, 400, 2531, 365, 13391, 11, 291, 458, 11, 286, 2351, 13391, 6124, 507, 11, 411, 11, 983, 500, 380, 321, 445, 915, 11, 51138], "temperature": 0.0, "avg_logprob": -0.2113799733563888, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006096806842833757}, {"id": 224, "seek": 95968, "start": 975.16, "end": 979.12, "text": " you know, I'll take your AWDSTLM and like, trade it on all of Wikipedia and fine tune", "tokens": [51138, 291, 458, 11, 286, 603, 747, 428, 25815, 35, 6840, 43, 44, 293, 411, 11, 4923, 309, 322, 439, 295, 28999, 293, 2489, 10864, 51336], "temperature": 0.0, "avg_logprob": -0.2113799733563888, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006096806842833757}, {"id": 225, "seek": 95968, "start": 979.12, "end": 980.12, "text": " it.", "tokens": [51336, 309, 13, 51386], "temperature": 0.0, "avg_logprob": -0.2113799733563888, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006096806842833757}, {"id": 226, "seek": 95968, "start": 980.12, "end": 983.1999999999999, "text": " And he's kind of like, I don't think that's going to really lie.", "tokens": [51386, 400, 415, 311, 733, 295, 411, 11, 286, 500, 380, 519, 300, 311, 516, 281, 534, 4544, 13, 51540], "temperature": 0.0, "avg_logprob": -0.2113799733563888, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.006096806842833757}, {"id": 227, "seek": 98320, "start": 983.72, "end": 991.24, "text": " Like two years before, I did a very popular talk at KDD, the conference where everybody", "tokens": [50390, 1743, 732, 924, 949, 11, 286, 630, 257, 588, 3743, 751, 412, 591, 20818, 11, 264, 7586, 689, 2201, 50766], "temperature": 0.0, "avg_logprob": -0.17406868072877446, "compression_ratio": 1.46875, "no_speech_prob": 0.04334966838359833}, {"id": 228, "seek": 98320, "start": 991.24, "end": 993.84, "text": " in NLP was in the audience.", "tokens": [50766, 294, 426, 45196, 390, 294, 264, 4034, 13, 50896], "temperature": 0.0, "avg_logprob": -0.17406868072877446, "compression_ratio": 1.46875, "no_speech_prob": 0.04334966838359833}, {"id": 229, "seek": 98320, "start": 993.84, "end": 999.32, "text": " I recognized after faces, you know, and I told them all this, I'm sure transfer learning", "tokens": [50896, 286, 9823, 934, 8475, 11, 291, 458, 11, 293, 286, 1907, 552, 439, 341, 11, 286, 478, 988, 5003, 2539, 51170], "temperature": 0.0, "avg_logprob": -0.17406868072877446, "compression_ratio": 1.46875, "no_speech_prob": 0.04334966838359833}, {"id": 230, "seek": 98320, "start": 999.32, "end": 1000.48, "text": " is the key.", "tokens": [51170, 307, 264, 2141, 13, 51228], "temperature": 0.0, "avg_logprob": -0.17406868072877446, "compression_ratio": 1.46875, "no_speech_prob": 0.04334966838359833}, {"id": 231, "seek": 98320, "start": 1000.48, "end": 1008.8000000000001, "text": " I'm sure ImageNet, you know, is going to be an NLP thing as well.", "tokens": [51228, 286, 478, 988, 29903, 31890, 11, 291, 458, 11, 307, 516, 281, 312, 364, 426, 45196, 551, 382, 731, 13, 51644], "temperature": 0.0, "avg_logprob": -0.17406868072877446, "compression_ratio": 1.46875, "no_speech_prob": 0.04334966838359833}, {"id": 232, "seek": 100880, "start": 1008.8, "end": 1013.92, "text": " And you know, everybody was interested and people asked me questions afterwards, but", "tokens": [50364, 400, 291, 458, 11, 2201, 390, 3102, 293, 561, 2351, 385, 1651, 10543, 11, 457, 50620], "temperature": 0.0, "avg_logprob": -0.2924051674044862, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.3804074823856354}, {"id": 233, "seek": 100880, "start": 1013.92, "end": 1019.88, "text": " just, yeah, nobody followed up because everybody knew that it didn't work.", "tokens": [50620, 445, 11, 1338, 11, 5079, 6263, 493, 570, 2201, 2586, 300, 309, 994, 380, 589, 13, 50918], "temperature": 0.0, "avg_logprob": -0.2924051674044862, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.3804074823856354}, {"id": 234, "seek": 100880, "start": 1019.88, "end": 1028.52, "text": " I mean, even like, so we were scooped a little bit by Dye and Lee at Google.", "tokens": [50918, 286, 914, 11, 754, 411, 11, 370, 321, 645, 19555, 292, 257, 707, 857, 538, 413, 1200, 293, 6957, 412, 3329, 13, 51350], "temperature": 0.0, "avg_logprob": -0.2924051674044862, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.3804074823856354}, {"id": 235, "seek": 100880, "start": 1028.52, "end": 1032.8, "text": " They had, I already, I didn't even realize this, it's just a bit embarrassing.", "tokens": [51350, 814, 632, 11, 286, 1217, 11, 286, 994, 380, 754, 4325, 341, 11, 309, 311, 445, 257, 857, 17299, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2924051674044862, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.3804074823856354}, {"id": 236, "seek": 100880, "start": 1032.8, "end": 1037.84, "text": " They had already done a large language model and fine tuned it.", "tokens": [51564, 814, 632, 1217, 1096, 257, 2416, 2856, 2316, 293, 2489, 10870, 309, 13, 51816], "temperature": 0.0, "avg_logprob": -0.2924051674044862, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.3804074823856354}, {"id": 237, "seek": 103784, "start": 1037.8799999999999, "end": 1043.1599999999999, "text": " But again, they didn't create a general purpose large language model on a general purpose", "tokens": [50366, 583, 797, 11, 436, 994, 380, 1884, 257, 2674, 4334, 2416, 2856, 2316, 322, 257, 2674, 4334, 50630], "temperature": 0.0, "avg_logprob": -0.19066977500915527, "compression_ratio": 1.59375, "no_speech_prob": 0.013214664533734322}, {"id": 238, "seek": 103784, "start": 1043.1599999999999, "end": 1044.1599999999999, "text": " corpus.", "tokens": [50630, 1181, 31624, 13, 50680], "temperature": 0.0, "avg_logprob": -0.19066977500915527, "compression_ratio": 1.59375, "no_speech_prob": 0.013214664533734322}, {"id": 239, "seek": 103784, "start": 1044.1599999999999, "end": 1048.3999999999999, "text": " They only ever tested a domain specific corpus.", "tokens": [50680, 814, 787, 1562, 8246, 257, 9274, 2685, 1181, 31624, 13, 50892], "temperature": 0.0, "avg_logprob": -0.19066977500915527, "compression_ratio": 1.59375, "no_speech_prob": 0.013214664533734322}, {"id": 240, "seek": 103784, "start": 1048.3999999999999, "end": 1052.84, "text": " And I haven't spoken to Kwok actually about that, but I assume that the reason was the", "tokens": [50892, 400, 286, 2378, 380, 10759, 281, 43432, 453, 767, 466, 300, 11, 457, 286, 6552, 300, 264, 1778, 390, 264, 51114], "temperature": 0.0, "avg_logprob": -0.19066977500915527, "compression_ratio": 1.59375, "no_speech_prob": 0.013214664533734322}, {"id": 241, "seek": 103784, "start": 1052.84, "end": 1053.84, "text": " same.", "tokens": [51114, 912, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19066977500915527, "compression_ratio": 1.59375, "no_speech_prob": 0.013214664533734322}, {"id": 242, "seek": 103784, "start": 1053.84, "end": 1058.8, "text": " It probably just didn't occur to them that the general approach could work.", "tokens": [51164, 467, 1391, 445, 994, 380, 5160, 281, 552, 300, 264, 2674, 3109, 727, 589, 13, 51412], "temperature": 0.0, "avg_logprob": -0.19066977500915527, "compression_ratio": 1.59375, "no_speech_prob": 0.013214664533734322}, {"id": 243, "seek": 103784, "start": 1058.8, "end": 1063.6799999999998, "text": " So maybe it was that kind of 30 years of mulling over the, this whole Chinese room experiment", "tokens": [51412, 407, 1310, 309, 390, 300, 733, 295, 2217, 924, 295, 275, 858, 278, 670, 264, 11, 341, 1379, 4649, 1808, 5120, 51656], "temperature": 0.0, "avg_logprob": -0.19066977500915527, "compression_ratio": 1.59375, "no_speech_prob": 0.013214664533734322}, {"id": 244, "seek": 106368, "start": 1063.68, "end": 1066.96, "text": " that had convinced me that it probably would work.", "tokens": [50364, 300, 632, 12561, 385, 300, 309, 1391, 576, 589, 13, 50528], "temperature": 0.0, "avg_logprob": -0.2634261036647185, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.0012251284206286073}, {"id": 245, "seek": 106368, "start": 1066.96, "end": 1067.96, "text": " I don't know.", "tokens": [50528, 286, 500, 380, 458, 13, 50578], "temperature": 0.0, "avg_logprob": -0.2634261036647185, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.0012251284206286073}, {"id": 246, "seek": 106368, "start": 1067.96, "end": 1068.96, "text": " Yeah.", "tokens": [50578, 865, 13, 50628], "temperature": 0.0, "avg_logprob": -0.2634261036647185, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.0012251284206286073}, {"id": 247, "seek": 106368, "start": 1068.96, "end": 1069.96, "text": " Interesting.", "tokens": [50628, 14711, 13, 50678], "temperature": 0.0, "avg_logprob": -0.2634261036647185, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.0012251284206286073}, {"id": 248, "seek": 106368, "start": 1069.96, "end": 1074.68, "text": " I just dug up Alec announcement tweet from Tony 18.", "tokens": [50678, 286, 445, 22954, 493, 9366, 66, 12847, 15258, 490, 10902, 2443, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2634261036647185, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.0012251284206286073}, {"id": 249, "seek": 106368, "start": 1074.68, "end": 1077.8, "text": " He said, inspired by Kobe, Elmo and Yola and Fit.", "tokens": [50914, 634, 848, 11, 7547, 538, 46296, 11, 38722, 293, 398, 4711, 293, 29263, 13, 51070], "temperature": 0.0, "avg_logprob": -0.2634261036647185, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.0012251284206286073}, {"id": 250, "seek": 106368, "start": 1077.8, "end": 1082.3200000000002, "text": " We showed a single transformer language model can be fine tuned to a variety.", "tokens": [51070, 492, 4712, 257, 2167, 31782, 2856, 2316, 393, 312, 2489, 10870, 281, 257, 5673, 13, 51296], "temperature": 0.0, "avg_logprob": -0.2634261036647185, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.0012251284206286073}, {"id": 251, "seek": 106368, "start": 1082.3200000000002, "end": 1087.6000000000001, "text": " It's interesting because, you know, today people think of the leader kind of like, kind of", "tokens": [51296, 467, 311, 1880, 570, 11, 291, 458, 11, 965, 561, 519, 295, 264, 5263, 733, 295, 411, 11, 733, 295, 51560], "temperature": 0.0, "avg_logprob": -0.2634261036647185, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.0012251284206286073}, {"id": 252, "seek": 106368, "start": 1087.6000000000001, "end": 1089.96, "text": " like the research lab pushing forward the field.", "tokens": [51560, 411, 264, 2132, 2715, 7380, 2128, 264, 2519, 13, 51678], "temperature": 0.0, "avg_logprob": -0.2634261036647185, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.0012251284206286073}, {"id": 253, "seek": 106368, "start": 1089.96, "end": 1093.24, "text": " What was that at the time, you know, like kind of like going back five years, people", "tokens": [51678, 708, 390, 300, 412, 264, 565, 11, 291, 458, 11, 411, 733, 295, 411, 516, 646, 1732, 924, 11, 561, 51842], "temperature": 0.0, "avg_logprob": -0.2634261036647185, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.0012251284206286073}, {"id": 254, "seek": 109324, "start": 1093.32, "end": 1096.84, "text": " think of it as an overnight success, but obviously it took a while.", "tokens": [50368, 519, 295, 309, 382, 364, 13935, 2245, 11, 457, 2745, 309, 1890, 257, 1339, 13, 50544], "temperature": 0.0, "avg_logprob": -0.18574536832651697, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.013631646521389484}, {"id": 255, "seek": 109324, "start": 1096.84, "end": 1097.84, "text": " Yeah.", "tokens": [50544, 865, 13, 50594], "temperature": 0.0, "avg_logprob": -0.18574536832651697, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.013631646521389484}, {"id": 256, "seek": 109324, "start": 1097.84, "end": 1098.84, "text": " Yeah.", "tokens": [50594, 865, 13, 50644], "temperature": 0.0, "avg_logprob": -0.18574536832651697, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.013631646521389484}, {"id": 257, "seek": 109324, "start": 1098.84, "end": 1099.84, "text": " No, I mean, absolutely.", "tokens": [50644, 883, 11, 286, 914, 11, 3122, 13, 50694], "temperature": 0.0, "avg_logprob": -0.18574536832651697, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.013631646521389484}, {"id": 258, "seek": 109324, "start": 1099.84, "end": 1103.04, "text": " And I'll say like, it's interesting that it mentioned Elmo because in some ways that", "tokens": [50694, 400, 286, 603, 584, 411, 11, 309, 311, 1880, 300, 309, 2835, 38722, 570, 294, 512, 2098, 300, 50854], "temperature": 0.0, "avg_logprob": -0.18574536832651697, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.013631646521389484}, {"id": 259, "seek": 109324, "start": 1103.04, "end": 1109.16, "text": " was kind of diametrically opposed to, to ULM fit, you know, there was these kind of like,", "tokens": [50854, 390, 733, 295, 7484, 27965, 984, 8851, 281, 11, 281, 624, 43, 44, 3318, 11, 291, 458, 11, 456, 390, 613, 733, 295, 411, 11, 51160], "temperature": 0.0, "avg_logprob": -0.18574536832651697, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.013631646521389484}, {"id": 260, "seek": 109324, "start": 1109.16, "end": 1114.1200000000001, "text": " so there was a lot of, there was a lot of activity at the same time as ULM fits release.", "tokens": [51160, 370, 456, 390, 257, 688, 295, 11, 456, 390, 257, 688, 295, 5191, 412, 264, 912, 565, 382, 624, 43, 44, 9001, 4374, 13, 51408], "temperature": 0.0, "avg_logprob": -0.18574536832651697, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.013631646521389484}, {"id": 261, "seek": 109324, "start": 1114.1200000000001, "end": 1121.48, "text": " So there was, so before it, as Brian McCann, I think at Salesforce had come out with this", "tokens": [51408, 407, 456, 390, 11, 370, 949, 309, 11, 382, 10765, 12061, 969, 11, 286, 519, 412, 40398, 632, 808, 484, 365, 341, 51776], "temperature": 0.0, "avg_logprob": -0.18574536832651697, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.013631646521389484}, {"id": 262, "seek": 112148, "start": 1121.52, "end": 1126.24, "text": " neat model that did a kind of multitask learning.", "tokens": [50366, 10654, 2316, 300, 630, 257, 733, 295, 42338, 3863, 2539, 13, 50602], "temperature": 0.0, "avg_logprob": -0.17320942642665146, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0013247887836769223}, {"id": 263, "seek": 112148, "start": 1126.24, "end": 1130.8, "text": " But again, they didn't create a general fine tune language model first.", "tokens": [50602, 583, 797, 11, 436, 994, 380, 1884, 257, 2674, 2489, 10864, 2856, 2316, 700, 13, 50830], "temperature": 0.0, "avg_logprob": -0.17320942642665146, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0013247887836769223}, {"id": 264, "seek": 112148, "start": 1130.8, "end": 1136.04, "text": " There was Elmo, which I think was a little, you know, actually quite a few months after", "tokens": [50830, 821, 390, 38722, 11, 597, 286, 519, 390, 257, 707, 11, 291, 458, 11, 767, 1596, 257, 1326, 2493, 934, 51092], "temperature": 0.0, "avg_logprob": -0.17320942642665146, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0013247887836769223}, {"id": 265, "seek": 112148, "start": 1136.04, "end": 1139.16, "text": " the first ULM fit example, I think.", "tokens": [51092, 264, 700, 624, 43, 44, 3318, 1365, 11, 286, 519, 13, 51248], "temperature": 0.0, "avg_logprob": -0.17320942642665146, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0013247887836769223}, {"id": 266, "seek": 112148, "start": 1140.04, "end": 1141.4, "text": " But yeah, there was a bit of this stuff going on.", "tokens": [51292, 583, 1338, 11, 456, 390, 257, 857, 295, 341, 1507, 516, 322, 13, 51360], "temperature": 0.0, "avg_logprob": -0.17320942642665146, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0013247887836769223}, {"id": 267, "seek": 112148, "start": 1141.4, "end": 1148.4, "text": " And the problem was everybody was doing, and particularly after GPT came out there and", "tokens": [51360, 400, 264, 1154, 390, 2201, 390, 884, 11, 293, 4098, 934, 26039, 51, 1361, 484, 456, 293, 51710], "temperature": 0.0, "avg_logprob": -0.17320942642665146, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0013247887836769223}, {"id": 268, "seek": 114840, "start": 1148.4, "end": 1152.6000000000001, "text": " everybody wanted to focus on zero shot and few shot learning, you know, everybody hated", "tokens": [50364, 2201, 1415, 281, 1879, 322, 4018, 3347, 293, 1326, 3347, 2539, 11, 291, 458, 11, 2201, 17398, 50574], "temperature": 0.0, "avg_logprob": -0.160066286722819, "compression_ratio": 1.8535564853556485, "no_speech_prob": 0.0021825393196195364}, {"id": 269, "seek": 114840, "start": 1152.6000000000001, "end": 1154.8400000000001, "text": " fine tuning, everybody hated transfer learning.", "tokens": [50574, 2489, 15164, 11, 2201, 17398, 5003, 2539, 13, 50686], "temperature": 0.0, "avg_logprob": -0.160066286722819, "compression_ratio": 1.8535564853556485, "no_speech_prob": 0.0021825393196195364}, {"id": 270, "seek": 114840, "start": 1154.8400000000001, "end": 1160.5600000000002, "text": " And like I literally did tours trying to get people to start doing transfer learning.", "tokens": [50686, 400, 411, 286, 3736, 630, 22911, 1382, 281, 483, 561, 281, 722, 884, 5003, 2539, 13, 50972], "temperature": 0.0, "avg_logprob": -0.160066286722819, "compression_ratio": 1.8535564853556485, "no_speech_prob": 0.0021825393196195364}, {"id": 271, "seek": 114840, "start": 1160.5600000000002, "end": 1167.0400000000002, "text": " And people, you know, nobody was interested, particularly after GPT showed such good results", "tokens": [50972, 400, 561, 11, 291, 458, 11, 5079, 390, 3102, 11, 4098, 934, 26039, 51, 4712, 1270, 665, 3542, 51296], "temperature": 0.0, "avg_logprob": -0.160066286722819, "compression_ratio": 1.8535564853556485, "no_speech_prob": 0.0021825393196195364}, {"id": 272, "seek": 114840, "start": 1167.0400000000002, "end": 1169.48, "text": " with zero shot and few shot learning.", "tokens": [51296, 365, 4018, 3347, 293, 1326, 3347, 2539, 13, 51418], "temperature": 0.0, "avg_logprob": -0.160066286722819, "compression_ratio": 1.8535564853556485, "no_speech_prob": 0.0021825393196195364}, {"id": 273, "seek": 114840, "start": 1169.48, "end": 1173.48, "text": " And so I actually feel like we kind of went backwards for years and, and not to be honest,", "tokens": [51418, 400, 370, 286, 767, 841, 411, 321, 733, 295, 1437, 12204, 337, 924, 293, 11, 293, 406, 281, 312, 3245, 11, 51618], "temperature": 0.0, "avg_logprob": -0.160066286722819, "compression_ratio": 1.8535564853556485, "no_speech_prob": 0.0021825393196195364}, {"id": 274, "seek": 117348, "start": 1173.48, "end": 1180.16, "text": " I mean, I'm a bit sad about this now, but I kind of got so disappointed and dissuaded", "tokens": [50364, 286, 914, 11, 286, 478, 257, 857, 4227, 466, 341, 586, 11, 457, 286, 733, 295, 658, 370, 13856, 293, 7802, 84, 12777, 50698], "temperature": 0.0, "avg_logprob": -0.13919356773639546, "compression_ratio": 1.688715953307393, "no_speech_prob": 0.01281788945198059}, {"id": 275, "seek": 117348, "start": 1180.16, "end": 1186.3600000000001, "text": " by like, it felt like these bigger lab, much bigger labs, you know, like fast AI had only", "tokens": [50698, 538, 411, 11, 309, 2762, 411, 613, 3801, 2715, 11, 709, 3801, 20339, 11, 291, 458, 11, 411, 2370, 7318, 632, 787, 51008], "temperature": 0.0, "avg_logprob": -0.13919356773639546, "compression_ratio": 1.688715953307393, "no_speech_prob": 0.01281788945198059}, {"id": 276, "seek": 117348, "start": 1186.3600000000001, "end": 1192.88, "text": " ever been just me and Rachel were getting all of this attention for an approach I thought", "tokens": [51008, 1562, 668, 445, 385, 293, 14246, 645, 1242, 439, 295, 341, 3202, 337, 364, 3109, 286, 1194, 51334], "temperature": 0.0, "avg_logprob": -0.13919356773639546, "compression_ratio": 1.688715953307393, "no_speech_prob": 0.01281788945198059}, {"id": 277, "seek": 117348, "start": 1192.88, "end": 1194.56, "text": " was the wrong way to do it.", "tokens": [51334, 390, 264, 2085, 636, 281, 360, 309, 13, 51418], "temperature": 0.0, "avg_logprob": -0.13919356773639546, "compression_ratio": 1.688715953307393, "no_speech_prob": 0.01281788945198059}, {"id": 278, "seek": 117348, "start": 1194.56, "end": 1196.6, "text": " You know, I was convinced was the wrong way to do it.", "tokens": [51418, 509, 458, 11, 286, 390, 12561, 390, 264, 2085, 636, 281, 360, 309, 13, 51520], "temperature": 0.0, "avg_logprob": -0.13919356773639546, "compression_ratio": 1.688715953307393, "no_speech_prob": 0.01281788945198059}, {"id": 279, "seek": 117348, "start": 1196.6, "end": 1200.72, "text": " And so yeah, for years, people were really focused on getting better zero shot and few", "tokens": [51520, 400, 370, 1338, 11, 337, 924, 11, 561, 645, 534, 5178, 322, 1242, 1101, 4018, 3347, 293, 1326, 51726], "temperature": 0.0, "avg_logprob": -0.13919356773639546, "compression_ratio": 1.688715953307393, "no_speech_prob": 0.01281788945198059}, {"id": 280, "seek": 120072, "start": 1200.72, "end": 1201.72, "text": " shot.", "tokens": [50364, 3347, 13, 50414], "temperature": 0.0, "avg_logprob": -0.12677223682403566, "compression_ratio": 1.8609865470852018, "no_speech_prob": 0.12934714555740356}, {"id": 281, "seek": 120072, "start": 1201.72, "end": 1206.96, "text": " And it wasn't until, you know, this key idea of like, well, let's take the ULM fit approach.", "tokens": [50414, 400, 309, 2067, 380, 1826, 11, 291, 458, 11, 341, 2141, 1558, 295, 411, 11, 731, 11, 718, 311, 747, 264, 624, 43, 44, 3318, 3109, 13, 50676], "temperature": 0.0, "avg_logprob": -0.12677223682403566, "compression_ratio": 1.8609865470852018, "no_speech_prob": 0.12934714555740356}, {"id": 282, "seek": 120072, "start": 1206.96, "end": 1213.4, "text": " But for step two, rather than fine tuning on a kind of a domain corpus, let's fine tune", "tokens": [50676, 583, 337, 1823, 732, 11, 2831, 813, 2489, 15164, 322, 257, 733, 295, 257, 9274, 1181, 31624, 11, 718, 311, 2489, 10864, 50998], "temperature": 0.0, "avg_logprob": -0.12677223682403566, "compression_ratio": 1.8609865470852018, "no_speech_prob": 0.12934714555740356}, {"id": 283, "seek": 120072, "start": 1213.4, "end": 1215.84, "text": " on an instruction corpus.", "tokens": [50998, 322, 364, 10951, 1181, 31624, 13, 51120], "temperature": 0.0, "avg_logprob": -0.12677223682403566, "compression_ratio": 1.8609865470852018, "no_speech_prob": 0.12934714555740356}, {"id": 284, "seek": 120072, "start": 1215.84, "end": 1220.72, "text": " And then in step three, rather than fine tuning on a reasonably specific task classification,", "tokens": [51120, 400, 550, 294, 1823, 1045, 11, 2831, 813, 2489, 15164, 322, 257, 23551, 2685, 5633, 21538, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12677223682403566, "compression_ratio": 1.8609865470852018, "no_speech_prob": 0.12934714555740356}, {"id": 285, "seek": 120072, "start": 1220.72, "end": 1225.32, "text": " let's fine tune on a, on a RLHF class classification.", "tokens": [51364, 718, 311, 2489, 10864, 322, 257, 11, 322, 257, 497, 43, 39, 37, 1508, 21538, 13, 51594], "temperature": 0.0, "avg_logprob": -0.12677223682403566, "compression_ratio": 1.8609865470852018, "no_speech_prob": 0.12934714555740356}, {"id": 286, "seek": 120072, "start": 1225.32, "end": 1227.84, "text": " And so that was really, that was really key, you know.", "tokens": [51594, 400, 370, 300, 390, 534, 11, 300, 390, 534, 2141, 11, 291, 458, 13, 51720], "temperature": 0.0, "avg_logprob": -0.12677223682403566, "compression_ratio": 1.8609865470852018, "no_speech_prob": 0.12934714555740356}, {"id": 287, "seek": 122784, "start": 1227.84, "end": 1233.84, "text": " So I was kind of like out of the NLP field for a few years there, because yeah, it just", "tokens": [50364, 407, 286, 390, 733, 295, 411, 484, 295, 264, 426, 45196, 2519, 337, 257, 1326, 924, 456, 11, 570, 1338, 11, 309, 445, 50664], "temperature": 0.0, "avg_logprob": -0.1545313784950658, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0005032461485825479}, {"id": 288, "seek": 122784, "start": 1233.84, "end": 1242.36, "text": " felt like, I don't know, pushing uphill against this vast tide, which I was convinced was", "tokens": [50664, 2762, 411, 11, 286, 500, 380, 458, 11, 7380, 39132, 1970, 341, 8369, 24662, 11, 597, 286, 390, 12561, 390, 51090], "temperature": 0.0, "avg_logprob": -0.1545313784950658, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0005032461485825479}, {"id": 289, "seek": 122784, "start": 1242.36, "end": 1245.48, "text": " not the right direction, but he's going to listen to me, you know, because I, as you", "tokens": [51090, 406, 264, 558, 3513, 11, 457, 415, 311, 516, 281, 2140, 281, 385, 11, 291, 458, 11, 570, 286, 11, 382, 291, 51246], "temperature": 0.0, "avg_logprob": -0.1545313784950658, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0005032461485825479}, {"id": 290, "seek": 122784, "start": 1245.48, "end": 1251.52, "text": " said, I don't have a PhD, not at a university, or at least it wasn't then I don't have a", "tokens": [51246, 848, 11, 286, 500, 380, 362, 257, 14476, 11, 406, 412, 257, 5454, 11, 420, 412, 1935, 309, 2067, 380, 550, 286, 500, 380, 362, 257, 51548], "temperature": 0.0, "avg_logprob": -0.1545313784950658, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0005032461485825479}, {"id": 291, "seek": 122784, "start": 1251.52, "end": 1256.36, "text": " big set of computers to fine tune huge transformer models.", "tokens": [51548, 955, 992, 295, 10807, 281, 2489, 10864, 2603, 31782, 5245, 13, 51790], "temperature": 0.0, "avg_logprob": -0.1545313784950658, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0005032461485825479}, {"id": 292, "seek": 125636, "start": 1256.36, "end": 1258.54, "text": " So yeah, it was definitely difficult.", "tokens": [50364, 407, 1338, 11, 309, 390, 2138, 2252, 13, 50473], "temperature": 0.0, "avg_logprob": -0.1215205929143642, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.024412812665104866}, {"id": 293, "seek": 125636, "start": 1258.54, "end": 1259.54, "text": " It's always been hard.", "tokens": [50473, 467, 311, 1009, 668, 1152, 13, 50523], "temperature": 0.0, "avg_logprob": -0.1215205929143642, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.024412812665104866}, {"id": 294, "seek": 125636, "start": 1259.54, "end": 1263.24, "text": " You know, it's always been hard, like I've always been somebody who does not want to", "tokens": [50523, 509, 458, 11, 309, 311, 1009, 668, 1152, 11, 411, 286, 600, 1009, 668, 2618, 567, 775, 406, 528, 281, 50708], "temperature": 0.0, "avg_logprob": -0.1215205929143642, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.024412812665104866}, {"id": 295, "seek": 125636, "start": 1263.24, "end": 1271.24, "text": " build stuff on lots of big computers, because most people don't have lots of big computers.", "tokens": [50708, 1322, 1507, 322, 3195, 295, 955, 10807, 11, 570, 881, 561, 500, 380, 362, 3195, 295, 955, 10807, 13, 51108], "temperature": 0.0, "avg_logprob": -0.1215205929143642, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.024412812665104866}, {"id": 296, "seek": 125636, "start": 1271.24, "end": 1275.8, "text": " And I hate creating stuff that most people can't use, you know, and also stuff that's", "tokens": [51108, 400, 286, 4700, 4084, 1507, 300, 881, 561, 393, 380, 764, 11, 291, 458, 11, 293, 611, 1507, 300, 311, 51336], "temperature": 0.0, "avg_logprob": -0.1215205929143642, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.024412812665104866}, {"id": 297, "seek": 125636, "start": 1275.8, "end": 1281.08, "text": " created on lots of big computers has always been like much more media friendly.", "tokens": [51336, 2942, 322, 3195, 295, 955, 10807, 575, 1009, 668, 411, 709, 544, 3021, 9208, 13, 51600], "temperature": 0.0, "avg_logprob": -0.1215205929143642, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.024412812665104866}, {"id": 298, "seek": 125636, "start": 1281.08, "end": 1285.4399999999998, "text": " So like, it might seem like a recent thing, but actually throughout my 30 years in data", "tokens": [51600, 407, 411, 11, 309, 1062, 1643, 411, 257, 5162, 551, 11, 457, 767, 3710, 452, 2217, 924, 294, 1412, 51818], "temperature": 0.0, "avg_logprob": -0.1215205929143642, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.024412812665104866}, {"id": 299, "seek": 128544, "start": 1285.44, "end": 1292.0800000000002, "text": " science, the attention's always been on, you know, the big iron results.", "tokens": [50364, 3497, 11, 264, 3202, 311, 1009, 668, 322, 11, 291, 458, 11, 264, 955, 6497, 3542, 13, 50696], "temperature": 0.0, "avg_logprob": -0.17663039101494682, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.04334631562232971}, {"id": 300, "seek": 128544, "start": 1292.0800000000002, "end": 1296.4, "text": " So when I first started, everybody was talking about data warehouses, and it was all about", "tokens": [50696, 407, 562, 286, 700, 1409, 11, 2201, 390, 1417, 466, 1412, 17464, 29578, 11, 293, 309, 390, 439, 466, 50912], "temperature": 0.0, "avg_logprob": -0.17663039101494682, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.04334631562232971}, {"id": 301, "seek": 128544, "start": 1296.4, "end": 1302.44, "text": " teradata, and it'd be like, oh, this big bank has this huge room full of computers, and", "tokens": [50912, 1796, 24695, 11, 293, 309, 1116, 312, 411, 11, 1954, 11, 341, 955, 3765, 575, 341, 2603, 1808, 1577, 295, 10807, 11, 293, 51214], "temperature": 0.0, "avg_logprob": -0.17663039101494682, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.04334631562232971}, {"id": 302, "seek": 128544, "start": 1302.44, "end": 1306.64, "text": " they have like terabytes of data available, you know, the press for button.", "tokens": [51214, 436, 362, 411, 1796, 24538, 295, 1412, 2435, 11, 291, 458, 11, 264, 1886, 337, 2960, 13, 51424], "temperature": 0.0, "avg_logprob": -0.17663039101494682, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.04334631562232971}, {"id": 303, "seek": 128544, "start": 1306.64, "end": 1312.72, "text": " And yeah, that's always what people want to talk about, what people want to write about.", "tokens": [51424, 400, 1338, 11, 300, 311, 1009, 437, 561, 528, 281, 751, 466, 11, 437, 561, 528, 281, 2464, 466, 13, 51728], "temperature": 0.0, "avg_logprob": -0.17663039101494682, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.04334631562232971}, {"id": 304, "seek": 131272, "start": 1312.72, "end": 1316.68, "text": " And then of course, students coming out of their PhDs and stuff, that's where they want", "tokens": [50364, 400, 550, 295, 1164, 11, 1731, 1348, 484, 295, 641, 14476, 82, 293, 1507, 11, 300, 311, 689, 436, 528, 50562], "temperature": 0.0, "avg_logprob": -0.16488672256469727, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.002980212913826108}, {"id": 305, "seek": 131272, "start": 1316.68, "end": 1319.92, "text": " to go work, because that's where they read about.", "tokens": [50562, 281, 352, 589, 11, 570, 300, 311, 689, 436, 1401, 466, 13, 50724], "temperature": 0.0, "avg_logprob": -0.16488672256469727, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.002980212913826108}, {"id": 306, "seek": 131272, "start": 1319.92, "end": 1327.68, "text": " And to me, it's a huge distraction, you know, because like I say, most people don't have", "tokens": [50724, 400, 281, 385, 11, 309, 311, 257, 2603, 30217, 11, 291, 458, 11, 570, 411, 286, 584, 11, 881, 561, 500, 380, 362, 51112], "temperature": 0.0, "avg_logprob": -0.16488672256469727, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.002980212913826108}, {"id": 307, "seek": 131272, "start": 1327.68, "end": 1335.96, "text": " unlimited compute, and I want to help most people, not the small subset of the most well-off", "tokens": [51112, 21950, 14722, 11, 293, 286, 528, 281, 854, 881, 561, 11, 406, 264, 1359, 25993, 295, 264, 881, 731, 12, 4506, 51526], "temperature": 0.0, "avg_logprob": -0.16488672256469727, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.002980212913826108}, {"id": 308, "seek": 131272, "start": 1335.96, "end": 1336.96, "text": " people.", "tokens": [51526, 561, 13, 51576], "temperature": 0.0, "avg_logprob": -0.16488672256469727, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.002980212913826108}, {"id": 309, "seek": 131272, "start": 1336.96, "end": 1337.96, "text": " Yeah.", "tokens": [51576, 865, 13, 51626], "temperature": 0.0, "avg_logprob": -0.16488672256469727, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.002980212913826108}, {"id": 310, "seek": 131272, "start": 1337.96, "end": 1338.96, "text": " That's awesome.", "tokens": [51626, 663, 311, 3476, 13, 51676], "temperature": 0.0, "avg_logprob": -0.16488672256469727, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.002980212913826108}, {"id": 311, "seek": 133896, "start": 1339.1200000000001, "end": 1344.08, "text": " It's great to hear, you know, you do such a great job educating that a lot of times,", "tokens": [50372, 467, 311, 869, 281, 1568, 11, 291, 458, 11, 291, 360, 1270, 257, 869, 1691, 28835, 300, 257, 688, 295, 1413, 11, 50620], "temperature": 0.0, "avg_logprob": -0.17709260237844368, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.10514172911643982}, {"id": 312, "seek": 133896, "start": 1344.08, "end": 1348.4, "text": " you're not telling your own story, you know, so I love this conversation.", "tokens": [50620, 291, 434, 406, 3585, 428, 1065, 1657, 11, 291, 458, 11, 370, 286, 959, 341, 3761, 13, 50836], "temperature": 0.0, "avg_logprob": -0.17709260237844368, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.10514172911643982}, {"id": 313, "seek": 133896, "start": 1348.4, "end": 1353.24, "text": " And the other thing before we jump into FASTI, actually, you know, a lot of people that I", "tokens": [50836, 400, 264, 661, 551, 949, 321, 3012, 666, 479, 3160, 5422, 11, 767, 11, 291, 458, 11, 257, 688, 295, 561, 300, 286, 51078], "temperature": 0.0, "avg_logprob": -0.17709260237844368, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.10514172911643982}, {"id": 314, "seek": 133896, "start": 1353.24, "end": 1357.44, "text": " know, they run across a new architecture and one other, like, I got to start a company", "tokens": [51078, 458, 11, 436, 1190, 2108, 257, 777, 9482, 293, 472, 661, 11, 411, 11, 286, 658, 281, 722, 257, 2237, 51288], "temperature": 0.0, "avg_logprob": -0.17709260237844368, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.10514172911643982}, {"id": 315, "seek": 133896, "start": 1357.44, "end": 1361.32, "text": " and raise a bunch of money and do all of this stuff, instead you were like, I want everybody", "tokens": [51288, 293, 5300, 257, 3840, 295, 1460, 293, 360, 439, 295, 341, 1507, 11, 2602, 291, 645, 411, 11, 286, 528, 2201, 51482], "temperature": 0.0, "avg_logprob": -0.17709260237844368, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.10514172911643982}, {"id": 316, "seek": 133896, "start": 1361.32, "end": 1363.96, "text": " to have access to this.", "tokens": [51482, 281, 362, 2105, 281, 341, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17709260237844368, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.10514172911643982}, {"id": 317, "seek": 133896, "start": 1363.96, "end": 1365.32, "text": " Why was that the case for you?", "tokens": [51614, 1545, 390, 300, 264, 1389, 337, 291, 30, 51682], "temperature": 0.0, "avg_logprob": -0.17709260237844368, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.10514172911643982}, {"id": 318, "seek": 136532, "start": 1365.32, "end": 1369.6399999999999, "text": " Was it because you already had like a successful, you know, venture and like fast mail and you", "tokens": [50364, 3027, 309, 570, 291, 1217, 632, 411, 257, 4406, 11, 291, 458, 11, 18474, 293, 411, 2370, 10071, 293, 291, 50580], "temperature": 0.0, "avg_logprob": -0.17011308670043945, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.01853516884148121}, {"id": 319, "seek": 136532, "start": 1369.6399999999999, "end": 1370.9199999999998, "text": " were more interested in that?", "tokens": [50580, 645, 544, 3102, 294, 300, 30, 50644], "temperature": 0.0, "avg_logprob": -0.17011308670043945, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.01853516884148121}, {"id": 320, "seek": 136532, "start": 1370.9199999999998, "end": 1371.9199999999998, "text": " What was the reasoning?", "tokens": [50644, 708, 390, 264, 21577, 30, 50694], "temperature": 0.0, "avg_logprob": -0.17011308670043945, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.01853516884148121}, {"id": 321, "seek": 136532, "start": 1371.9199999999998, "end": 1374.32, "text": " That's a really good question.", "tokens": [50694, 663, 311, 257, 534, 665, 1168, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17011308670043945, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.01853516884148121}, {"id": 322, "seek": 136532, "start": 1374.32, "end": 1378.76, "text": " So I guess the answer is yes, that's the reason why.", "tokens": [50814, 407, 286, 2041, 264, 1867, 307, 2086, 11, 300, 311, 264, 1778, 983, 13, 51036], "temperature": 0.0, "avg_logprob": -0.17011308670043945, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.01853516884148121}, {"id": 323, "seek": 136532, "start": 1378.76, "end": 1385.28, "text": " So when I was a teenager, I thought it would be really cool to like have my own company.", "tokens": [51036, 407, 562, 286, 390, 257, 21440, 11, 286, 1194, 309, 576, 312, 534, 1627, 281, 411, 362, 452, 1065, 2237, 13, 51362], "temperature": 0.0, "avg_logprob": -0.17011308670043945, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.01853516884148121}, {"id": 324, "seek": 136532, "start": 1385.28, "end": 1388.52, "text": " You know, I didn't know the word startup, I didn't know the word entrepreneur, I didn't", "tokens": [51362, 509, 458, 11, 286, 994, 380, 458, 264, 1349, 18578, 11, 286, 994, 380, 458, 264, 1349, 14307, 11, 286, 994, 380, 51524], "temperature": 0.0, "avg_logprob": -0.17011308670043945, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.01853516884148121}, {"id": 325, "seek": 136532, "start": 1388.52, "end": 1392.76, "text": " know the word VC, and I didn't really know what any of those things were really until", "tokens": [51524, 458, 264, 1349, 41922, 11, 293, 286, 994, 380, 534, 458, 437, 604, 295, 729, 721, 645, 534, 1826, 51736], "temperature": 0.0, "avg_logprob": -0.17011308670043945, "compression_ratio": 1.8131868131868132, "no_speech_prob": 0.01853516884148121}, {"id": 326, "seek": 139276, "start": 1392.8, "end": 1396.0, "text": " after we started Kaggle, to be honest, even though I'd started to what would now call", "tokens": [50366, 934, 321, 1409, 48751, 22631, 11, 281, 312, 3245, 11, 754, 1673, 286, 1116, 1409, 281, 437, 576, 586, 818, 50526], "temperature": 0.0, "avg_logprob": -0.19746329234196588, "compression_ratio": 1.712, "no_speech_prob": 0.014950738288462162}, {"id": 327, "seek": 139276, "start": 1396.0, "end": 1402.44, "text": " startups, I just thought they were just small businesses, you know, they were just companies.", "tokens": [50526, 28041, 11, 286, 445, 1194, 436, 645, 445, 1359, 6011, 11, 291, 458, 11, 436, 645, 445, 3431, 13, 50848], "temperature": 0.0, "avg_logprob": -0.19746329234196588, "compression_ratio": 1.712, "no_speech_prob": 0.014950738288462162}, {"id": 328, "seek": 139276, "start": 1402.44, "end": 1405.04, "text": " So yeah, so those two companies were fast mail and optimal decisions.", "tokens": [50848, 407, 1338, 11, 370, 729, 732, 3431, 645, 2370, 10071, 293, 16252, 5327, 13, 50978], "temperature": 0.0, "avg_logprob": -0.19746329234196588, "compression_ratio": 1.712, "no_speech_prob": 0.014950738288462162}, {"id": 329, "seek": 139276, "start": 1405.04, "end": 1411.36, "text": " Fast mail was the first kind of synchronized email provider for non-businesses, so something", "tokens": [50978, 15968, 10071, 390, 264, 700, 733, 295, 19331, 1602, 3796, 12398, 337, 2107, 12, 21441, 1324, 279, 11, 370, 746, 51294], "temperature": 0.0, "avg_logprob": -0.19746329234196588, "compression_ratio": 1.712, "no_speech_prob": 0.014950738288462162}, {"id": 330, "seek": 139276, "start": 1411.36, "end": 1417.8799999999999, "text": " you can get your same email at home on your laptop that were on your phone, whatever.", "tokens": [51294, 291, 393, 483, 428, 912, 3796, 412, 1280, 322, 428, 10732, 300, 645, 322, 428, 2593, 11, 2035, 13, 51620], "temperature": 0.0, "avg_logprob": -0.19746329234196588, "compression_ratio": 1.712, "no_speech_prob": 0.014950738288462162}, {"id": 331, "seek": 141788, "start": 1417.88, "end": 1423.92, "text": " And then optimal decisions invented a new approach to insurance pricing, so they called", "tokens": [50364, 400, 550, 16252, 5327, 14479, 257, 777, 3109, 281, 7214, 17621, 11, 370, 436, 1219, 50666], "temperature": 0.0, "avg_logprob": -0.21064080362734589, "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.010326236486434937}, {"id": 332, "seek": 141788, "start": 1423.92, "end": 1426.4, "text": " profit optimized insurance pricing.", "tokens": [50666, 7475, 26941, 7214, 17621, 13, 50790], "temperature": 0.0, "avg_logprob": -0.21064080362734589, "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.010326236486434937}, {"id": 333, "seek": 141788, "start": 1426.4, "end": 1435.8000000000002, "text": " So I saw both of those companies, you know, after 10 years, and at that point I had achieved", "tokens": [50790, 407, 286, 1866, 1293, 295, 729, 3431, 11, 291, 458, 11, 934, 1266, 924, 11, 293, 412, 300, 935, 286, 632, 11042, 51260], "temperature": 0.0, "avg_logprob": -0.21064080362734589, "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.010326236486434937}, {"id": 334, "seek": 141788, "start": 1435.8000000000002, "end": 1441.64, "text": " the thing that as a teenager I wanted to do, you know, it took a lot longer than it should", "tokens": [51260, 264, 551, 300, 382, 257, 21440, 286, 1415, 281, 360, 11, 291, 458, 11, 309, 1890, 257, 688, 2854, 813, 309, 820, 51552], "temperature": 0.0, "avg_logprob": -0.21064080362734589, "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.010326236486434937}, {"id": 335, "seek": 141788, "start": 1441.64, "end": 1444.5200000000002, "text": " have because I spent way longer in management consulting than I should have because I got", "tokens": [51552, 362, 570, 286, 4418, 636, 2854, 294, 4592, 23682, 813, 286, 820, 362, 570, 286, 658, 51696], "temperature": 0.0, "avg_logprob": -0.21064080362734589, "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.010326236486434937}, {"id": 336, "seek": 144452, "start": 1444.56, "end": 1449.36, "text": " caught up in that stupid rat race, but you know, eventually I got there and I remember", "tokens": [50366, 5415, 493, 294, 300, 6631, 5937, 4569, 11, 457, 291, 458, 11, 4728, 286, 658, 456, 293, 286, 1604, 50606], "temperature": 0.0, "avg_logprob": -0.1987372050209651, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.39199718832969666}, {"id": 337, "seek": 144452, "start": 1449.36, "end": 1454.68, "text": " my mom saying to me, oh, you must be so proud, you know, because she remembered, my dreams", "tokens": [50606, 452, 1225, 1566, 281, 385, 11, 1954, 11, 291, 1633, 312, 370, 4570, 11, 291, 458, 11, 570, 750, 13745, 11, 452, 7505, 50872], "temperature": 0.0, "avg_logprob": -0.1987372050209651, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.39199718832969666}, {"id": 338, "seek": 144452, "start": 1454.68, "end": 1457.12, "text": " is like, you've done it.", "tokens": [50872, 307, 411, 11, 291, 600, 1096, 309, 13, 50994], "temperature": 0.0, "avg_logprob": -0.1987372050209651, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.39199718832969666}, {"id": 339, "seek": 144452, "start": 1457.12, "end": 1463.52, "text": " And I kind of reflected and I was like, I'm not, I'm not proud at all, you know, like", "tokens": [50994, 400, 286, 733, 295, 15502, 293, 286, 390, 411, 11, 286, 478, 406, 11, 286, 478, 406, 4570, 412, 439, 11, 291, 458, 11, 411, 51314], "temperature": 0.0, "avg_logprob": -0.1987372050209651, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.39199718832969666}, {"id": 340, "seek": 144452, "start": 1463.52, "end": 1467.52, "text": " people quite liked fast mail, you know, it's quite nice to have synchronized email, it", "tokens": [51314, 561, 1596, 4501, 2370, 10071, 11, 291, 458, 11, 309, 311, 1596, 1481, 281, 362, 19331, 1602, 3796, 11, 309, 51514], "temperature": 0.0, "avg_logprob": -0.1987372050209651, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.39199718832969666}, {"id": 341, "seek": 144452, "start": 1467.52, "end": 1473.36, "text": " probably would have happened anyway, yeah, I'm certainly not proud that I've helped", "tokens": [51514, 1391, 576, 362, 2011, 4033, 11, 1338, 11, 286, 478, 3297, 406, 4570, 300, 286, 600, 4254, 51806], "temperature": 0.0, "avg_logprob": -0.1987372050209651, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.39199718832969666}, {"id": 342, "seek": 147336, "start": 1473.3999999999999, "end": 1477.8, "text": " some insurance companies suck more money out of their customers.", "tokens": [50366, 512, 7214, 3431, 9967, 544, 1460, 484, 295, 641, 4581, 13, 50586], "temperature": 0.0, "avg_logprob": -0.18299685353818146, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.004197449889034033}, {"id": 343, "seek": 147336, "start": 1477.8, "end": 1482.6399999999999, "text": " Yeah, no, I'm not proud, you know, it's this actually, I haven't really helped the world", "tokens": [50586, 865, 11, 572, 11, 286, 478, 406, 4570, 11, 291, 458, 11, 309, 311, 341, 767, 11, 286, 2378, 380, 534, 4254, 264, 1002, 50828], "temperature": 0.0, "avg_logprob": -0.18299685353818146, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.004197449889034033}, {"id": 344, "seek": 147336, "start": 1482.6399999999999, "end": 1487.28, "text": " very much, you know, maybe in the insurance case, I've made it a little bit worse.", "tokens": [50828, 588, 709, 11, 291, 458, 11, 1310, 294, 264, 7214, 1389, 11, 286, 600, 1027, 309, 257, 707, 857, 5324, 13, 51060], "temperature": 0.0, "avg_logprob": -0.18299685353818146, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.004197449889034033}, {"id": 345, "seek": 147336, "start": 1487.28, "end": 1488.8799999999999, "text": " I don't know.", "tokens": [51060, 286, 500, 380, 458, 13, 51140], "temperature": 0.0, "avg_logprob": -0.18299685353818146, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.004197449889034033}, {"id": 346, "seek": 147336, "start": 1488.8799999999999, "end": 1497.8799999999999, "text": " So yeah, I was determined to not waste more years of my life doing things, working hard", "tokens": [51140, 407, 1338, 11, 286, 390, 9540, 281, 406, 5964, 544, 924, 295, 452, 993, 884, 721, 11, 1364, 1152, 51590], "temperature": 0.0, "avg_logprob": -0.18299685353818146, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.004197449889034033}, {"id": 347, "seek": 149788, "start": 1497.92, "end": 1503.4, "text": " to do things which I could not be reasonably sure would have a lot of value.", "tokens": [50366, 281, 360, 721, 597, 286, 727, 406, 312, 23551, 988, 576, 362, 257, 688, 295, 2158, 13, 50640], "temperature": 0.0, "avg_logprob": -0.1795254345088042, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.030197758227586746}, {"id": 348, "seek": 149788, "start": 1503.4, "end": 1508.68, "text": " So, you know, I took some time off, I wasn't sure if I'd ever work again, actually, I didn't", "tokens": [50640, 407, 11, 291, 458, 11, 286, 1890, 512, 565, 766, 11, 286, 2067, 380, 988, 498, 286, 1116, 1562, 589, 797, 11, 767, 11, 286, 994, 380, 50904], "temperature": 0.0, "avg_logprob": -0.1795254345088042, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.030197758227586746}, {"id": 349, "seek": 149788, "start": 1508.68, "end": 1514.3200000000002, "text": " particularly want to, because it felt like, yeah, it felt like such a disappointment.", "tokens": [50904, 4098, 528, 281, 11, 570, 309, 2762, 411, 11, 1338, 11, 309, 2762, 411, 1270, 257, 28175, 13, 51186], "temperature": 0.0, "avg_logprob": -0.1795254345088042, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.030197758227586746}, {"id": 350, "seek": 149788, "start": 1514.3200000000002, "end": 1517.96, "text": " And but you know, and I didn't need to, I had enough money, like I wasn't super rich,", "tokens": [51186, 400, 457, 291, 458, 11, 293, 286, 994, 380, 643, 281, 11, 286, 632, 1547, 1460, 11, 411, 286, 2067, 380, 1687, 4593, 11, 51368], "temperature": 0.0, "avg_logprob": -0.1795254345088042, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.030197758227586746}, {"id": 351, "seek": 149788, "start": 1517.96, "end": 1520.4, "text": " but I had enough money, I didn't need to work.", "tokens": [51368, 457, 286, 632, 1547, 1460, 11, 286, 994, 380, 643, 281, 589, 13, 51490], "temperature": 0.0, "avg_logprob": -0.1795254345088042, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.030197758227586746}, {"id": 352, "seek": 149788, "start": 1520.4, "end": 1525.4, "text": " And I certainly recognized that amongst the other people, I knew who had enough money", "tokens": [51490, 400, 286, 3297, 9823, 300, 12918, 264, 661, 561, 11, 286, 2586, 567, 632, 1547, 1460, 51740], "temperature": 0.0, "avg_logprob": -0.1795254345088042, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.030197758227586746}, {"id": 353, "seek": 152540, "start": 1525.44, "end": 1529.8000000000002, "text": " that they didn't need to work, they all worked ridiculously hard, you know, and constantly", "tokens": [50366, 300, 436, 994, 380, 643, 281, 589, 11, 436, 439, 2732, 41358, 1152, 11, 291, 458, 11, 293, 6460, 50584], "temperature": 0.0, "avg_logprob": -0.1719752225008878, "compression_ratio": 1.6411290322580645, "no_speech_prob": 0.009407300502061844}, {"id": 354, "seek": 152540, "start": 1529.8000000000002, "end": 1532.44, "text": " put themselves in extremely stressful situations.", "tokens": [50584, 829, 2969, 294, 4664, 19108, 6851, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1719752225008878, "compression_ratio": 1.6411290322580645, "no_speech_prob": 0.009407300502061844}, {"id": 355, "seek": 152540, "start": 1532.44, "end": 1539.6000000000001, "text": " And I thought, I don't want to be one of those idiots who's tied to, you know, buying a bigger", "tokens": [50716, 400, 286, 1194, 11, 286, 500, 380, 528, 281, 312, 472, 295, 729, 36454, 567, 311, 9601, 281, 11, 291, 458, 11, 6382, 257, 3801, 51074], "temperature": 0.0, "avg_logprob": -0.1719752225008878, "compression_ratio": 1.6411290322580645, "no_speech_prob": 0.009407300502061844}, {"id": 356, "seek": 152540, "start": 1539.6000000000001, "end": 1544.52, "text": " plane than the next guy or whatever, you know, Kaggle came along and I mainly kind of did", "tokens": [51074, 5720, 813, 264, 958, 2146, 420, 2035, 11, 291, 458, 11, 48751, 22631, 1361, 2051, 293, 286, 8704, 733, 295, 630, 51320], "temperature": 0.0, "avg_logprob": -0.1719752225008878, "compression_ratio": 1.6411290322580645, "no_speech_prob": 0.009407300502061844}, {"id": 357, "seek": 152540, "start": 1544.52, "end": 1548.64, "text": " that just because it was fun and interesting to hang out with interesting people.", "tokens": [51320, 300, 445, 570, 309, 390, 1019, 293, 1880, 281, 3967, 484, 365, 1880, 561, 13, 51526], "temperature": 0.0, "avg_logprob": -0.1719752225008878, "compression_ratio": 1.6411290322580645, "no_speech_prob": 0.009407300502061844}, {"id": 358, "seek": 154864, "start": 1549.4, "end": 1557.2, "text": " But, you know, with fast AI in particular, you know, Rachel and I had a very explicit, you", "tokens": [50402, 583, 11, 291, 458, 11, 365, 2370, 7318, 294, 1729, 11, 291, 458, 11, 14246, 293, 286, 632, 257, 588, 13691, 11, 291, 50792], "temperature": 0.0, "avg_logprob": -0.13982409054471046, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.029293164610862732}, {"id": 359, "seek": 154864, "start": 1557.2, "end": 1562.68, "text": " know, long series of conversations over a long period of time about like, well, how can we be", "tokens": [50792, 458, 11, 938, 2638, 295, 7315, 670, 257, 938, 2896, 295, 565, 466, 411, 11, 731, 11, 577, 393, 321, 312, 51066], "temperature": 0.0, "avg_logprob": -0.13982409054471046, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.029293164610862732}, {"id": 360, "seek": 154864, "start": 1562.68, "end": 1569.6000000000001, "text": " the most helpful to society as a whole, and particularly to those people who maybe need", "tokens": [51066, 264, 881, 4961, 281, 4086, 382, 257, 1379, 11, 293, 4098, 281, 729, 561, 567, 1310, 643, 51412], "temperature": 0.0, "avg_logprob": -0.13982409054471046, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.029293164610862732}, {"id": 361, "seek": 154864, "start": 1569.6000000000001, "end": 1570.72, "text": " more help, you know.", "tokens": [51412, 544, 854, 11, 291, 458, 13, 51468], "temperature": 0.0, "avg_logprob": -0.13982409054471046, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.029293164610862732}, {"id": 362, "seek": 154864, "start": 1571.24, "end": 1578.0, "text": " And so we definitely saw the world going in a potentially pretty dystopian direction, if", "tokens": [51494, 400, 370, 321, 2138, 1866, 264, 1002, 516, 294, 257, 7263, 1238, 14584, 13559, 952, 3513, 11, 498, 51832], "temperature": 0.0, "avg_logprob": -0.13982409054471046, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.029293164610862732}, {"id": 363, "seek": 157800, "start": 1578.0, "end": 1583.44, "text": " the world's most powerful technology was controlled by a small group of elites.", "tokens": [50364, 264, 1002, 311, 881, 4005, 2899, 390, 10164, 538, 257, 1359, 1594, 295, 44678, 13, 50636], "temperature": 0.0, "avg_logprob": -0.16283289919194488, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.002395360730588436}, {"id": 364, "seek": 157800, "start": 1585.44, "end": 1589.92, "text": " So we thought, yeah, we should focus on trying to help that not happen.", "tokens": [50736, 407, 321, 1194, 11, 1338, 11, 321, 820, 1879, 322, 1382, 281, 854, 300, 406, 1051, 13, 50960], "temperature": 0.0, "avg_logprob": -0.16283289919194488, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.002395360730588436}, {"id": 365, "seek": 157800, "start": 1591.0, "end": 1593.36, "text": " You know, sadly, it looks like it still is likely to happen.", "tokens": [51014, 509, 458, 11, 22023, 11, 309, 1542, 411, 309, 920, 307, 3700, 281, 1051, 13, 51132], "temperature": 0.0, "avg_logprob": -0.16283289919194488, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.002395360730588436}, {"id": 366, "seek": 157800, "start": 1593.36, "end": 1598.28, "text": " But I mean, I feel like we've, we've helped make it a little bit less likely.", "tokens": [51132, 583, 286, 914, 11, 286, 841, 411, 321, 600, 11, 321, 600, 4254, 652, 309, 257, 707, 857, 1570, 3700, 13, 51378], "temperature": 0.0, "avg_logprob": -0.16283289919194488, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.002395360730588436}, {"id": 367, "seek": 157800, "start": 1598.32, "end": 1599.72, "text": " So we've done our best.", "tokens": [51380, 407, 321, 600, 1096, 527, 1151, 13, 51450], "temperature": 0.0, "avg_logprob": -0.16283289919194488, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.002395360730588436}, {"id": 368, "seek": 157800, "start": 1599.72, "end": 1601.52, "text": " You've shown that it's possible.", "tokens": [51450, 509, 600, 4898, 300, 309, 311, 1944, 13, 51540], "temperature": 0.0, "avg_logprob": -0.16283289919194488, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.002395360730588436}, {"id": 369, "seek": 160152, "start": 1601.6399999999999, "end": 1609.28, "text": " And I think, I think your constant advocacy, your courses, your research that you publish,", "tokens": [50370, 400, 286, 519, 11, 286, 519, 428, 5754, 22011, 11, 428, 7712, 11, 428, 2132, 300, 291, 11374, 11, 50752], "temperature": 0.0, "avg_logprob": -0.1792782514523237, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.007338280323892832}, {"id": 370, "seek": 160152, "start": 1609.28, "end": 1615.76, "text": " you know, just the other day you published a signing on, you know, learning that I think", "tokens": [50752, 291, 458, 11, 445, 264, 661, 786, 291, 6572, 257, 13393, 322, 11, 291, 458, 11, 2539, 300, 286, 519, 51076], "temperature": 0.0, "avg_logprob": -0.1792782514523237, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.007338280323892832}, {"id": 371, "seek": 160152, "start": 1615.76, "end": 1618.8, "text": " is still something that people are still talking about quite a lot.", "tokens": [51076, 307, 920, 746, 300, 561, 366, 920, 1417, 466, 1596, 257, 688, 13, 51228], "temperature": 0.0, "avg_logprob": -0.1792782514523237, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.007338280323892832}, {"id": 372, "seek": 160152, "start": 1619.04, "end": 1625.0, "text": " I think that that is the origin story of a lot of people who are going to be, you know,", "tokens": [51240, 286, 519, 300, 300, 307, 264, 4957, 1657, 295, 257, 688, 295, 561, 567, 366, 516, 281, 312, 11, 291, 458, 11, 51538], "temperature": 0.0, "avg_logprob": -0.1792782514523237, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.007338280323892832}, {"id": 373, "seek": 160152, "start": 1625.0, "end": 1628.24, "text": " little Jeremy Howard's sort of in your mission with, you know, you don't have to do", "tokens": [51538, 707, 17809, 17626, 311, 1333, 295, 294, 428, 4447, 365, 11, 291, 458, 11, 291, 500, 380, 362, 281, 360, 51700], "temperature": 0.0, "avg_logprob": -0.1792782514523237, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.007338280323892832}, {"id": 374, "seek": 160152, "start": 1628.24, "end": 1629.6399999999999, "text": " everything by yourself is what I'm saying.", "tokens": [51700, 1203, 538, 1803, 307, 437, 286, 478, 1566, 13, 51770], "temperature": 0.0, "avg_logprob": -0.1792782514523237, "compression_ratio": 1.855421686746988, "no_speech_prob": 0.007338280323892832}, {"id": 375, "seek": 162964, "start": 1629.68, "end": 1630.88, "text": " Definitely, definitely.", "tokens": [50366, 12151, 11, 2138, 13, 50426], "temperature": 0.0, "avg_logprob": -0.22367872808971545, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.003475611563771963}, {"id": 376, "seek": 162964, "start": 1630.88, "end": 1635.44, "text": " You know, that was a, that was a big takeaway from like, and Lydic was that Lydic, it", "tokens": [50426, 509, 458, 11, 300, 390, 257, 11, 300, 390, 257, 955, 30681, 490, 411, 11, 293, 441, 6655, 299, 390, 300, 441, 6655, 299, 11, 309, 50654], "temperature": 0.0, "avg_logprob": -0.22367872808971545, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.003475611563771963}, {"id": 377, "seek": 162964, "start": 1635.44, "end": 1637.8000000000002, "text": " definitely felt like we had to do everything ourselves.", "tokens": [50654, 2138, 2762, 411, 321, 632, 281, 360, 1203, 4175, 13, 50772], "temperature": 0.0, "avg_logprob": -0.22367872808971545, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.003475611563771963}, {"id": 378, "seek": 162964, "start": 1637.92, "end": 1640.0400000000002, "text": " And I kind of, I wanted to solve medicine.", "tokens": [50778, 400, 286, 733, 295, 11, 286, 1415, 281, 5039, 7195, 13, 50884], "temperature": 0.0, "avg_logprob": -0.22367872808971545, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.003475611563771963}, {"id": 379, "seek": 162964, "start": 1640.16, "end": 1642.72, "text": " I'll say, yeah, okay, solving medicine is actually quite difficult.", "tokens": [50890, 286, 603, 584, 11, 1338, 11, 1392, 11, 12606, 7195, 307, 767, 1596, 2252, 13, 51018], "temperature": 0.0, "avg_logprob": -0.22367872808971545, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.003475611563771963}, {"id": 380, "seek": 162964, "start": 1642.72, "end": 1645.4, "text": " And I can't do it on my own.", "tokens": [51018, 400, 286, 393, 380, 360, 309, 322, 452, 1065, 13, 51152], "temperature": 0.0, "avg_logprob": -0.22367872808971545, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.003475611563771963}, {"id": 381, "seek": 162964, "start": 1645.4, "end": 1647.88, "text": " And there's a lot of other things I'd like to solve, and I can't do those either.", "tokens": [51152, 400, 456, 311, 257, 688, 295, 661, 721, 286, 1116, 411, 281, 5039, 11, 293, 286, 393, 380, 360, 729, 2139, 13, 51276], "temperature": 0.0, "avg_logprob": -0.22367872808971545, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.003475611563771963}, {"id": 382, "seek": 162964, "start": 1647.88, "end": 1654.0, "text": " So that was, that was definitely the other piece was like, yeah, you know, can we create", "tokens": [51276, 407, 300, 390, 11, 300, 390, 2138, 264, 661, 2522, 390, 411, 11, 1338, 11, 291, 458, 11, 393, 321, 1884, 51582], "temperature": 0.0, "avg_logprob": -0.22367872808971545, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.003475611563771963}, {"id": 383, "seek": 165400, "start": 1654.0, "end": 1661.32, "text": " an army of passionate domain experts who can change their little part of the world.", "tokens": [50364, 364, 7267, 295, 11410, 9274, 8572, 567, 393, 1319, 641, 707, 644, 295, 264, 1002, 13, 50730], "temperature": 0.0, "avg_logprob": -0.16344104948497953, "compression_ratio": 1.625, "no_speech_prob": 0.009121356531977654}, {"id": 384, "seek": 165400, "start": 1661.8, "end": 1662.76, "text": " And that's definitely happened.", "tokens": [50754, 400, 300, 311, 2138, 2011, 13, 50802], "temperature": 0.0, "avg_logprob": -0.16344104948497953, "compression_ratio": 1.625, "no_speech_prob": 0.009121356531977654}, {"id": 385, "seek": 165400, "start": 1662.76, "end": 1670.12, "text": " Like I find nowadays, at least half the time, probably quite a bit more, that I get in", "tokens": [50802, 1743, 286, 915, 13434, 11, 412, 1935, 1922, 264, 565, 11, 1391, 1596, 257, 857, 544, 11, 300, 286, 483, 294, 51170], "temperature": 0.0, "avg_logprob": -0.16344104948497953, "compression_ratio": 1.625, "no_speech_prob": 0.009121356531977654}, {"id": 386, "seek": 165400, "start": 1670.12, "end": 1673.92, "text": " contact with somebody who's done really interesting work in some domain.", "tokens": [51170, 3385, 365, 2618, 567, 311, 1096, 534, 1880, 589, 294, 512, 9274, 13, 51360], "temperature": 0.0, "avg_logprob": -0.16344104948497953, "compression_ratio": 1.625, "no_speech_prob": 0.009121356531977654}, {"id": 387, "seek": 165400, "start": 1674.16, "end": 1677.32, "text": " Most of the time, I'd say, they say, yeah, I got my start with fast AI.", "tokens": [51372, 4534, 295, 264, 565, 11, 286, 1116, 584, 11, 436, 584, 11, 1338, 11, 286, 658, 452, 722, 365, 2370, 7318, 13, 51530], "temperature": 0.0, "avg_logprob": -0.16344104948497953, "compression_ratio": 1.625, "no_speech_prob": 0.009121356531977654}, {"id": 388, "seek": 165400, "start": 1678.36, "end": 1680.4, "text": " So it's definitely, I can, I can see that.", "tokens": [51582, 407, 309, 311, 2138, 11, 286, 393, 11, 286, 393, 536, 300, 13, 51684], "temperature": 0.0, "avg_logprob": -0.16344104948497953, "compression_ratio": 1.625, "no_speech_prob": 0.009121356531977654}, {"id": 389, "seek": 168040, "start": 1680.4, "end": 1686.76, "text": " And I also know from talking to folks at places like Amazon and Adobe and stuff, which, you", "tokens": [50364, 400, 286, 611, 458, 490, 1417, 281, 4024, 412, 3190, 411, 6795, 293, 24862, 293, 1507, 11, 597, 11, 291, 50682], "temperature": 0.0, "avg_logprob": -0.19867425282796225, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002529143530409783}, {"id": 390, "seek": 168040, "start": 1686.76, "end": 1687.96, "text": " know, there's lots of alumni there.", "tokens": [50682, 458, 11, 456, 311, 3195, 295, 16347, 456, 13, 50742], "temperature": 0.0, "avg_logprob": -0.19867425282796225, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002529143530409783}, {"id": 391, "seek": 168040, "start": 1687.96, "end": 1691.3600000000001, "text": " And they say, oh my God, I got here in like half of the people who are fast AI alumni.", "tokens": [50742, 400, 436, 584, 11, 1954, 452, 1265, 11, 286, 658, 510, 294, 411, 1922, 295, 264, 561, 567, 366, 2370, 7318, 16347, 13, 50912], "temperature": 0.0, "avg_logprob": -0.19867425282796225, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002529143530409783}, {"id": 392, "seek": 168040, "start": 1692.24, "end": 1693.8400000000001, "text": " So it's fantastic.", "tokens": [50956, 407, 309, 311, 5456, 13, 51036], "temperature": 0.0, "avg_logprob": -0.19867425282796225, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002529143530409783}, {"id": 393, "seek": 168040, "start": 1694.64, "end": 1698.6000000000001, "text": " Yeah, actually, Andre Capati grabbed me when I saw him at Europe's a few years ago.", "tokens": [51076, 865, 11, 767, 11, 20667, 8363, 6908, 18607, 385, 562, 286, 1866, 796, 412, 3315, 311, 257, 1326, 924, 2057, 13, 51274], "temperature": 0.0, "avg_logprob": -0.19867425282796225, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002529143530409783}, {"id": 394, "seek": 168040, "start": 1698.6000000000001, "end": 1701.3200000000002, "text": " And he's like, I have to tell you thanks to the fast AI courses.", "tokens": [51274, 400, 415, 311, 411, 11, 286, 362, 281, 980, 291, 3231, 281, 264, 2370, 7318, 7712, 13, 51410], "temperature": 0.0, "avg_logprob": -0.19867425282796225, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002529143530409783}, {"id": 395, "seek": 168040, "start": 1701.3200000000002, "end": 1704.72, "text": " When people come to Tesla and they need to know more about deep learning, we always", "tokens": [51410, 1133, 561, 808, 281, 13666, 293, 436, 643, 281, 458, 544, 466, 2452, 2539, 11, 321, 1009, 51580], "temperature": 0.0, "avg_logprob": -0.19867425282796225, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002529143530409783}, {"id": 396, "seek": 168040, "start": 1704.72, "end": 1705.72, "text": " send them to your course.", "tokens": [51580, 2845, 552, 281, 428, 1164, 13, 51630], "temperature": 0.0, "avg_logprob": -0.19867425282796225, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002529143530409783}, {"id": 397, "seek": 168040, "start": 1706.44, "end": 1709.2800000000002, "text": " And the OpenAI scholars program was doing the same thing.", "tokens": [51666, 400, 264, 7238, 48698, 8553, 1461, 390, 884, 264, 912, 551, 13, 51808], "temperature": 0.0, "avg_logprob": -0.19867425282796225, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002529143530409783}, {"id": 398, "seek": 170928, "start": 1709.6399999999999, "end": 1717.48, "text": " So it's kind of like, yeah, it's had a surprising impact, you know, that's just one", "tokens": [50382, 407, 309, 311, 733, 295, 411, 11, 1338, 11, 309, 311, 632, 257, 8830, 2712, 11, 291, 458, 11, 300, 311, 445, 472, 50774], "temperature": 0.0, "avg_logprob": -0.1924623355530856, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.000588017632253468}, {"id": 399, "seek": 170928, "start": 1717.48, "end": 1723.32, "text": " of like three things we do is the course, you know, and it's, it's, it's only ever", "tokens": [50774, 295, 411, 1045, 721, 321, 360, 307, 264, 1164, 11, 291, 458, 11, 293, 309, 311, 11, 309, 311, 11, 309, 311, 787, 1562, 51066], "temperature": 0.0, "avg_logprob": -0.1924623355530856, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.000588017632253468}, {"id": 400, "seek": 170928, "start": 1723.32, "end": 1727.16, "text": " been at most two people, either me and Rachel or me and Silver.", "tokens": [51066, 668, 412, 881, 732, 561, 11, 2139, 385, 293, 14246, 420, 385, 293, 15861, 13, 51258], "temperature": 0.0, "avg_logprob": -0.1924623355530856, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.000588017632253468}, {"id": 401, "seek": 170928, "start": 1727.2, "end": 1728.16, "text": " Nowadays, it's just me.", "tokens": [51260, 28908, 11, 309, 311, 445, 385, 13, 51308], "temperature": 0.0, "avg_logprob": -0.1924623355530856, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.000588017632253468}, {"id": 402, "seek": 170928, "start": 1729.2, "end": 1733.56, "text": " So, yeah, I think it shows you don't necessarily need a huge amount of money and a", "tokens": [51360, 407, 11, 1338, 11, 286, 519, 309, 3110, 291, 500, 380, 4725, 643, 257, 2603, 2372, 295, 1460, 293, 257, 51578], "temperature": 0.0, "avg_logprob": -0.1924623355530856, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.000588017632253468}, {"id": 403, "seek": 170928, "start": 1733.56, "end": 1736.84, "text": " huge team of people to, to make an impact.", "tokens": [51578, 2603, 1469, 295, 561, 281, 11, 281, 652, 364, 2712, 13, 51742], "temperature": 0.0, "avg_logprob": -0.1924623355530856, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.000588017632253468}, {"id": 404, "seek": 173684, "start": 1737.76, "end": 1738.04, "text": " Yeah.", "tokens": [50410, 865, 13, 50424], "temperature": 0.0, "avg_logprob": -0.16000460776962153, "compression_ratio": 1.62, "no_speech_prob": 0.0019263862632215023}, {"id": 405, "seek": 173684, "start": 1738.9199999999998, "end": 1744.6, "text": " So just to reintroduce fast AI for people who may not have dived into it much.", "tokens": [50468, 407, 445, 281, 319, 38132, 384, 2370, 7318, 337, 561, 567, 815, 406, 362, 274, 3194, 666, 309, 709, 13, 50752], "temperature": 0.0, "avg_logprob": -0.16000460776962153, "compression_ratio": 1.62, "no_speech_prob": 0.0019263862632215023}, {"id": 406, "seek": 173684, "start": 1744.9599999999998, "end": 1747.08, "text": " There is the courses that you do.", "tokens": [50770, 821, 307, 264, 7712, 300, 291, 360, 13, 50876], "temperature": 0.0, "avg_logprob": -0.16000460776962153, "compression_ratio": 1.62, "no_speech_prob": 0.0019263862632215023}, {"id": 407, "seek": 173684, "start": 1747.4399999999998, "end": 1752.1599999999999, "text": " There is the library that is, that is very well loved.", "tokens": [50894, 821, 307, 264, 6405, 300, 307, 11, 300, 307, 588, 731, 4333, 13, 51130], "temperature": 0.0, "avg_logprob": -0.16000460776962153, "compression_ratio": 1.62, "no_speech_prob": 0.0019263862632215023}, {"id": 408, "seek": 173684, "start": 1752.1599999999999, "end": 1757.1999999999998, "text": " And I kind of think of it as a nicer layer on top of PyTorch that people should", "tokens": [51130, 400, 286, 733, 295, 519, 295, 309, 382, 257, 22842, 4583, 322, 1192, 295, 9953, 51, 284, 339, 300, 561, 820, 51382], "temperature": 0.0, "avg_logprob": -0.16000460776962153, "compression_ratio": 1.62, "no_speech_prob": 0.0019263862632215023}, {"id": 409, "seek": 173684, "start": 1757.1999999999998, "end": 1761.08, "text": " start with by default and use it as the basis for a lot of your courses.", "tokens": [51382, 722, 365, 538, 7576, 293, 764, 309, 382, 264, 5143, 337, 257, 688, 295, 428, 7712, 13, 51576], "temperature": 0.0, "avg_logprob": -0.16000460776962153, "compression_ratio": 1.62, "no_speech_prob": 0.0019263862632215023}, {"id": 410, "seek": 173684, "start": 1762.28, "end": 1766.76, "text": " And then you have, you have like NB Dev, which I don't know, is that the third", "tokens": [51636, 400, 550, 291, 362, 11, 291, 362, 411, 426, 33, 9096, 11, 597, 286, 500, 380, 458, 11, 307, 300, 264, 2636, 51860], "temperature": 0.0, "avg_logprob": -0.16000460776962153, "compression_ratio": 1.62, "no_speech_prob": 0.0019263862632215023}, {"id": 411, "seek": 176676, "start": 1766.76, "end": 1772.52, "text": " one? Oh, so the three areas were research, software, and, and courses.", "tokens": [50364, 472, 30, 876, 11, 370, 264, 1045, 3179, 645, 2132, 11, 4722, 11, 293, 11, 293, 7712, 13, 50652], "temperature": 0.0, "avg_logprob": -0.24055809020996094, "compression_ratio": 1.506726457399103, "no_speech_prob": 0.0001970936282305047}, {"id": 412, "seek": 176676, "start": 1772.56, "end": 1773.0, "text": " Oh, sorry.", "tokens": [50654, 876, 11, 2597, 13, 50676], "temperature": 0.0, "avg_logprob": -0.24055809020996094, "compression_ratio": 1.506726457399103, "no_speech_prob": 0.0001970936282305047}, {"id": 413, "seek": 176676, "start": 1773.0, "end": 1773.52, "text": " I was going by.", "tokens": [50676, 286, 390, 516, 538, 13, 50702], "temperature": 0.0, "avg_logprob": -0.24055809020996094, "compression_ratio": 1.506726457399103, "no_speech_prob": 0.0001970936282305047}, {"id": 414, "seek": 176676, "start": 1773.52, "end": 1781.96, "text": " So then in software, you know, fast AI is the main thing, but NB Dev is not far", "tokens": [50702, 407, 550, 294, 4722, 11, 291, 458, 11, 2370, 7318, 307, 264, 2135, 551, 11, 457, 426, 33, 9096, 307, 406, 1400, 51124], "temperature": 0.0, "avg_logprob": -0.24055809020996094, "compression_ratio": 1.506726457399103, "no_speech_prob": 0.0001970936282305047}, {"id": 415, "seek": 176676, "start": 1781.96, "end": 1789.24, "text": " behind, but then there's also things like Fastcore, GHAPI, I mean, dozens of open", "tokens": [51124, 2261, 11, 457, 550, 456, 311, 611, 721, 411, 15968, 12352, 11, 40690, 4715, 40, 11, 286, 914, 11, 18431, 295, 1269, 51488], "temperature": 0.0, "avg_logprob": -0.24055809020996094, "compression_ratio": 1.506726457399103, "no_speech_prob": 0.0001970936282305047}, {"id": 416, "seek": 176676, "start": 1789.24, "end": 1795.36, "text": " source projects that I've created and some of them have been pretty popular.", "tokens": [51488, 4009, 4455, 300, 286, 600, 2942, 293, 512, 295, 552, 362, 668, 1238, 3743, 13, 51794], "temperature": 0.0, "avg_logprob": -0.24055809020996094, "compression_ratio": 1.506726457399103, "no_speech_prob": 0.0001970936282305047}, {"id": 417, "seek": 179536, "start": 1795.36, "end": 1797.32, "text": " And some of them are still a little bit hidden.", "tokens": [50364, 400, 512, 295, 552, 366, 920, 257, 707, 857, 7633, 13, 50462], "temperature": 0.0, "avg_logprob": -0.2166838896901984, "compression_ratio": 1.7925696594427245, "no_speech_prob": 0.005727482959628105}, {"id": 418, "seek": 179536, "start": 1797.32, "end": 1801.04, "text": " Actually, I should, some of them I should try to do a better job of telling people", "tokens": [50462, 5135, 11, 286, 820, 11, 512, 295, 552, 286, 820, 853, 281, 360, 257, 1101, 1691, 295, 3585, 561, 50648], "temperature": 0.0, "avg_logprob": -0.2166838896901984, "compression_ratio": 1.7925696594427245, "no_speech_prob": 0.005727482959628105}, {"id": 419, "seek": 179536, "start": 1801.04, "end": 1802.04, "text": " about. What are you, what are you thinking about?", "tokens": [50648, 466, 13, 708, 366, 291, 11, 437, 366, 291, 1953, 466, 30, 50698], "temperature": 0.0, "avg_logprob": -0.2166838896901984, "compression_ratio": 1.7925696594427245, "no_speech_prob": 0.005727482959628105}, {"id": 420, "seek": 179536, "start": 1802.6, "end": 1802.84, "text": " Yeah.", "tokens": [50726, 865, 13, 50738], "temperature": 0.0, "avg_logprob": -0.2166838896901984, "compression_ratio": 1.7925696594427245, "no_speech_prob": 0.005727482959628105}, {"id": 421, "seek": 179536, "start": 1802.84, "end": 1807.84, "text": " What, what's on this little things like, for example, for working with EC2 and AWS,", "tokens": [50738, 708, 11, 437, 311, 322, 341, 707, 721, 411, 11, 337, 1365, 11, 337, 1364, 365, 19081, 17, 293, 17650, 11, 50988], "temperature": 0.0, "avg_logprob": -0.2166838896901984, "compression_ratio": 1.7925696594427245, "no_speech_prob": 0.005727482959628105}, {"id": 422, "seek": 179536, "start": 1807.84, "end": 1812.36, "text": " I created a fast EC2 library, which I think is like way more convenient and nice", "tokens": [50988, 286, 2942, 257, 2370, 19081, 17, 6405, 11, 597, 286, 519, 307, 411, 636, 544, 10851, 293, 1481, 51214], "temperature": 0.0, "avg_logprob": -0.2166838896901984, "compression_ratio": 1.7925696594427245, "no_speech_prob": 0.005727482959628105}, {"id": 423, "seek": 179536, "start": 1812.36, "end": 1813.8799999999999, "text": " to use than anything else out there.", "tokens": [51214, 281, 764, 813, 1340, 1646, 484, 456, 13, 51290], "temperature": 0.0, "avg_logprob": -0.2166838896901984, "compression_ratio": 1.7925696594427245, "no_speech_prob": 0.005727482959628105}, {"id": 424, "seek": 179536, "start": 1814.3999999999999, "end": 1817.8, "text": " And it's literally got a whole autocomplete dynamic autocomplete that works", "tokens": [51316, 400, 309, 311, 3736, 658, 257, 1379, 45833, 298, 17220, 8546, 45833, 298, 17220, 300, 1985, 51486], "temperature": 0.0, "avg_logprob": -0.2166838896901984, "compression_ratio": 1.7925696594427245, "no_speech_prob": 0.005727482959628105}, {"id": 425, "seek": 179536, "start": 1817.8, "end": 1821.4399999999998, "text": " both on the command line and in notebooks, sort of like autocomplete your", "tokens": [51486, 1293, 322, 264, 5622, 1622, 293, 294, 43782, 11, 1333, 295, 411, 45833, 298, 17220, 428, 51668], "temperature": 0.0, "avg_logprob": -0.2166838896901984, "compression_ratio": 1.7925696594427245, "no_speech_prob": 0.005727482959628105}, {"id": 426, "seek": 179536, "start": 1821.4399999999998, "end": 1823.9199999999998, "text": " instance names and everything like that.", "tokens": [51668, 5197, 5288, 293, 1203, 411, 300, 13, 51792], "temperature": 0.0, "avg_logprob": -0.2166838896901984, "compression_ratio": 1.7925696594427245, "no_speech_prob": 0.005727482959628105}, {"id": 427, "seek": 182392, "start": 1824.1200000000001, "end": 1825.48, "text": " You know, just little things like that.", "tokens": [50374, 509, 458, 11, 445, 707, 721, 411, 300, 13, 50442], "temperature": 0.0, "avg_logprob": -0.13483640073819925, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.001597652561031282}, {"id": 428, "seek": 182392, "start": 1825.48, "end": 1832.4, "text": " I try to make like, when I work with some domain, I try to make it like, I want", "tokens": [50442, 286, 853, 281, 652, 411, 11, 562, 286, 589, 365, 512, 9274, 11, 286, 853, 281, 652, 309, 411, 11, 286, 528, 50788], "temperature": 0.0, "avg_logprob": -0.13483640073819925, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.001597652561031282}, {"id": 429, "seek": 182392, "start": 1832.4, "end": 1835.3200000000002, "text": " to make it as enjoyable as possible for me to do that.", "tokens": [50788, 281, 652, 309, 382, 20305, 382, 1944, 337, 385, 281, 360, 300, 13, 50934], "temperature": 0.0, "avg_logprob": -0.13483640073819925, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.001597652561031282}, {"id": 430, "seek": 182392, "start": 1835.64, "end": 1840.68, "text": " So I always try to kind of like, like with GHAPI, for example, I think that", "tokens": [50950, 407, 286, 1009, 853, 281, 733, 295, 411, 11, 411, 365, 40690, 4715, 40, 11, 337, 1365, 11, 286, 519, 300, 51202], "temperature": 0.0, "avg_logprob": -0.13483640073819925, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.001597652561031282}, {"id": 431, "seek": 182392, "start": 1840.68, "end": 1845.64, "text": " GitHub API is incredibly powerful, but I didn't find it good to work with", "tokens": [51202, 23331, 9362, 307, 6252, 4005, 11, 457, 286, 994, 380, 915, 309, 665, 281, 589, 365, 51450], "temperature": 0.0, "avg_logprob": -0.13483640073819925, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.001597652561031282}, {"id": 432, "seek": 182392, "start": 1845.64, "end": 1847.68, "text": " because I didn't particularly like the libraries that are out there.", "tokens": [51450, 570, 286, 994, 380, 4098, 411, 264, 15148, 300, 366, 484, 456, 13, 51552], "temperature": 0.0, "avg_logprob": -0.13483640073819925, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.001597652561031282}, {"id": 433, "seek": 182392, "start": 1847.68, "end": 1852.92, "text": " So like GHAPI, like Fast EC2, it like autocompletes both at the command", "tokens": [51552, 407, 411, 40690, 4715, 40, 11, 411, 15968, 19081, 17, 11, 309, 411, 45833, 298, 14657, 279, 1293, 412, 264, 5622, 51814], "temperature": 0.0, "avg_logprob": -0.13483640073819925, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.001597652561031282}, {"id": 434, "seek": 185292, "start": 1852.92, "end": 1857.8000000000002, "text": " line or in a notebook or whatever, like literally the entire GitHub API.", "tokens": [50364, 1622, 420, 294, 257, 21060, 420, 2035, 11, 411, 3736, 264, 2302, 23331, 9362, 13, 50608], "temperature": 0.0, "avg_logprob": -0.17384514757381972, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0019258930115029216}, {"id": 435, "seek": 185292, "start": 1859.68, "end": 1863.52, "text": " The entire thing is like, I think it's like less than a hundred K of code", "tokens": [50702, 440, 2302, 551, 307, 411, 11, 286, 519, 309, 311, 411, 1570, 813, 257, 3262, 591, 295, 3089, 50894], "temperature": 0.0, "avg_logprob": -0.17384514757381972, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0019258930115029216}, {"id": 436, "seek": 185292, "start": 1863.64, "end": 1869.48, "text": " because it actually, as far as I know, the only one that grabs it directly", "tokens": [50900, 570, 309, 767, 11, 382, 1400, 382, 286, 458, 11, 264, 787, 472, 300, 30028, 309, 3838, 51192], "temperature": 0.0, "avg_logprob": -0.17384514757381972, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0019258930115029216}, {"id": 437, "seek": 185292, "start": 1869.48, "end": 1873.44, "text": " from the official open API spec that GitHub produces.", "tokens": [51192, 490, 264, 4783, 1269, 9362, 1608, 300, 23331, 14725, 13, 51390], "temperature": 0.0, "avg_logprob": -0.17384514757381972, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0019258930115029216}, {"id": 438, "seek": 185292, "start": 1874.16, "end": 1881.48, "text": " And like, if you're in GitHub and you just type an API, you know, autocomplete", "tokens": [51426, 400, 411, 11, 498, 291, 434, 294, 23331, 293, 291, 445, 2010, 364, 9362, 11, 291, 458, 11, 45833, 298, 17220, 51792], "temperature": 0.0, "avg_logprob": -0.17384514757381972, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0019258930115029216}, {"id": 439, "seek": 188148, "start": 1881.72, "end": 1888.84, "text": " API method and it enter, it prints out the docs or the six brief docs and", "tokens": [50376, 9362, 3170, 293, 309, 3242, 11, 309, 22305, 484, 264, 45623, 420, 264, 2309, 5353, 45623, 293, 50732], "temperature": 0.0, "avg_logprob": -0.21627527124741497, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.001500919577665627}, {"id": 440, "seek": 188148, "start": 1888.84, "end": 1892.64, "text": " then gives you a link to the actual documentation page, you know, GitHub", "tokens": [50732, 550, 2709, 291, 257, 2113, 281, 264, 3539, 14333, 3028, 11, 291, 458, 11, 23331, 50922], "temperature": 0.0, "avg_logprob": -0.21627527124741497, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.001500919577665627}, {"id": 441, "seek": 188148, "start": 1892.64, "end": 1896.48, "text": " actions I can write now in Python, which is just so much easier than writing", "tokens": [50922, 5909, 286, 393, 2464, 586, 294, 15329, 11, 597, 307, 445, 370, 709, 3571, 813, 3579, 51114], "temperature": 0.0, "avg_logprob": -0.21627527124741497, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.001500919577665627}, {"id": 442, "seek": 188148, "start": 1896.48, "end": 1898.76, "text": " them in typescript and stuff.", "tokens": [51114, 552, 294, 3467, 5944, 293, 1507, 13, 51228], "temperature": 0.0, "avg_logprob": -0.21627527124741497, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.001500919577665627}, {"id": 443, "seek": 188148, "start": 1898.76, "end": 1900.84, "text": " So, you know, just little things like that.", "tokens": [51228, 407, 11, 291, 458, 11, 445, 707, 721, 411, 300, 13, 51332], "temperature": 0.0, "avg_logprob": -0.21627527124741497, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.001500919577665627}, {"id": 444, "seek": 188148, "start": 1901.1200000000001, "end": 1904.8, "text": " I think that's a approach that more, I wish more developers took to publish", "tokens": [51346, 286, 519, 300, 311, 257, 3109, 300, 544, 11, 286, 3172, 544, 8849, 1890, 281, 11374, 51530], "temperature": 0.0, "avg_logprob": -0.21627527124741497, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.001500919577665627}, {"id": 445, "seek": 188148, "start": 1905.08, "end": 1906.2, "text": " some of the work along the way.", "tokens": [51544, 512, 295, 264, 589, 2051, 264, 636, 13, 51600], "temperature": 0.0, "avg_logprob": -0.21627527124741497, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.001500919577665627}, {"id": 446, "seek": 188148, "start": 1907.2, "end": 1911.0, "text": " You describe the third arm of Fast EIS research.", "tokens": [51650, 509, 6786, 264, 2636, 3726, 295, 15968, 462, 2343, 2132, 13, 51840], "temperature": 0.0, "avg_logprob": -0.21627527124741497, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.001500919577665627}, {"id": 447, "seek": 191100, "start": 1911.12, "end": 1912.96, "text": " It's not something I see often.", "tokens": [50370, 467, 311, 406, 746, 286, 536, 2049, 13, 50462], "temperature": 0.0, "avg_logprob": -0.18143604351924017, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.00024531205417588353}, {"id": 448, "seek": 191100, "start": 1912.96, "end": 1918.08, "text": " Obviously, you do do some research and how do you run your research?", "tokens": [50462, 7580, 11, 291, 360, 360, 512, 2132, 293, 577, 360, 291, 1190, 428, 2132, 30, 50718], "temperature": 0.0, "avg_logprob": -0.18143604351924017, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.00024531205417588353}, {"id": 449, "seek": 191100, "start": 1918.24, "end": 1919.44, "text": " What are your research interests?", "tokens": [50726, 708, 366, 428, 2132, 8847, 30, 50786], "temperature": 0.0, "avg_logprob": -0.18143604351924017, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.00024531205417588353}, {"id": 450, "seek": 191100, "start": 1919.84, "end": 1920.12, "text": " Yeah.", "tokens": [50806, 865, 13, 50820], "temperature": 0.0, "avg_logprob": -0.18143604351924017, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.00024531205417588353}, {"id": 451, "seek": 191100, "start": 1920.12, "end": 1923.24, "text": " So research is what I spend the vast majority of my time on.", "tokens": [50820, 407, 2132, 307, 437, 286, 3496, 264, 8369, 6286, 295, 452, 565, 322, 13, 50976], "temperature": 0.0, "avg_logprob": -0.18143604351924017, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.00024531205417588353}, {"id": 452, "seek": 191100, "start": 1924.12, "end": 1931.4, "text": " And the artifacts that come out of that are largely software and courses, you", "tokens": [51020, 400, 264, 24617, 300, 808, 484, 295, 300, 366, 11611, 4722, 293, 7712, 11, 291, 51384], "temperature": 0.0, "avg_logprob": -0.18143604351924017, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.00024531205417588353}, {"id": 453, "seek": 191100, "start": 1931.4, "end": 1935.64, "text": " know, so to me, the main artifact shouldn't be papers because papers are", "tokens": [51384, 458, 11, 370, 281, 385, 11, 264, 2135, 34806, 4659, 380, 312, 10577, 570, 10577, 366, 51596], "temperature": 0.0, "avg_logprob": -0.18143604351924017, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.00024531205417588353}, {"id": 454, "seek": 191100, "start": 1935.64, "end": 1938.12, "text": " things read by a small exclusive group of people.", "tokens": [51596, 721, 1401, 538, 257, 1359, 13005, 1594, 295, 561, 13, 51720], "temperature": 0.0, "avg_logprob": -0.18143604351924017, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.00024531205417588353}, {"id": 455, "seek": 193812, "start": 1938.1999999999998, "end": 1942.76, "text": " You know, to me, the main artifacts should be like something teaching you", "tokens": [50368, 509, 458, 11, 281, 385, 11, 264, 2135, 24617, 820, 312, 411, 746, 4571, 291, 50596], "temperature": 0.0, "avg_logprob": -0.21000452988020335, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.002322432352229953}, {"id": 456, "seek": 193812, "start": 1942.76, "end": 1945.8, "text": " people, here's how to use this insight and here's software you can use that", "tokens": [50596, 561, 11, 510, 311, 577, 281, 764, 341, 11269, 293, 510, 311, 4722, 291, 393, 764, 300, 50748], "temperature": 0.0, "avg_logprob": -0.21000452988020335, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.002322432352229953}, {"id": 457, "seek": 193812, "start": 1945.9199999999998, "end": 1947.1599999999999, "text": " builds it in.", "tokens": [50754, 15182, 309, 294, 13, 50816], "temperature": 0.0, "avg_logprob": -0.21000452988020335, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.002322432352229953}, {"id": 458, "seek": 193812, "start": 1949.32, "end": 1953.0, "text": " So I think I've only ever done three first person papers in my life, you know,", "tokens": [50924, 407, 286, 519, 286, 600, 787, 1562, 1096, 1045, 700, 954, 10577, 294, 452, 993, 11, 291, 458, 11, 51108], "temperature": 0.0, "avg_logprob": -0.21000452988020335, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.002322432352229953}, {"id": 459, "seek": 193812, "start": 1953.1599999999999, "end": 1957.32, "text": " and they were, and none of those are ones I wanted to do, you know, they were all", "tokens": [51116, 293, 436, 645, 11, 293, 6022, 295, 729, 366, 2306, 286, 1415, 281, 360, 11, 291, 458, 11, 436, 645, 439, 51324], "temperature": 0.0, "avg_logprob": -0.21000452988020335, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.002322432352229953}, {"id": 460, "seek": 193812, "start": 1957.32, "end": 1961.52, "text": " once like, so one was ULM fit where Sebastian Ruda reached out to me after", "tokens": [51324, 1564, 411, 11, 370, 472, 390, 624, 43, 44, 3318, 689, 31102, 497, 11152, 6488, 484, 281, 385, 934, 51534], "temperature": 0.0, "avg_logprob": -0.21000452988020335, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.002322432352229953}, {"id": 461, "seek": 193812, "start": 1961.52, "end": 1965.1599999999999, "text": " seeing the course and said, like, you have to publish this as a paper, you know.", "tokens": [51534, 2577, 264, 1164, 293, 848, 11, 411, 11, 291, 362, 281, 11374, 341, 382, 257, 3035, 11, 291, 458, 13, 51716], "temperature": 0.0, "avg_logprob": -0.21000452988020335, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.002322432352229953}, {"id": 462, "seek": 196516, "start": 1965.72, "end": 1968.16, "text": " And he said, I'll write it.", "tokens": [50392, 400, 415, 848, 11, 286, 603, 2464, 309, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1805216304042883, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0013665808364748955}, {"id": 463, "seek": 196516, "start": 1970.28, "end": 1971.3200000000002, "text": " He said, I want to write it.", "tokens": [50620, 634, 848, 11, 286, 528, 281, 2464, 309, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1805216304042883, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0013665808364748955}, {"id": 464, "seek": 196516, "start": 1971.3200000000002, "end": 1973.6000000000001, "text": " Cause if I do, I can put it on my PhD and that would be great.", "tokens": [50672, 10865, 498, 286, 360, 11, 286, 393, 829, 309, 322, 452, 14476, 293, 300, 576, 312, 869, 13, 50786], "temperature": 0.0, "avg_logprob": -0.1805216304042883, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0013665808364748955}, {"id": 465, "seek": 196516, "start": 1973.6000000000001, "end": 1977.28, "text": " And it's like, okay, well, I want to help you with your PhD and that sounds great.", "tokens": [50786, 400, 309, 311, 411, 11, 1392, 11, 731, 11, 286, 528, 281, 854, 291, 365, 428, 14476, 293, 300, 3263, 869, 13, 50970], "temperature": 0.0, "avg_logprob": -0.1805216304042883, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0013665808364748955}, {"id": 466, "seek": 196516, "start": 1977.28, "end": 1983.52, "text": " So like, you know, one was the masks paper, which just had to exist and nobody", "tokens": [50970, 407, 411, 11, 291, 458, 11, 472, 390, 264, 11830, 3035, 11, 597, 445, 632, 281, 2514, 293, 5079, 51282], "temperature": 0.0, "avg_logprob": -0.1805216304042883, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0013665808364748955}, {"id": 467, "seek": 196516, "start": 1983.52, "end": 1984.64, "text": " else was writing it.", "tokens": [51282, 1646, 390, 3579, 309, 13, 51338], "temperature": 0.0, "avg_logprob": -0.1805216304042883, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0013665808364748955}, {"id": 468, "seek": 196516, "start": 1984.76, "end": 1993.92, "text": " And then the third was the fast AI library paper, which again, somebody reached", "tokens": [51344, 400, 550, 264, 2636, 390, 264, 2370, 7318, 6405, 3035, 11, 597, 797, 11, 2618, 6488, 51802], "temperature": 0.0, "avg_logprob": -0.1805216304042883, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0013665808364748955}, {"id": 469, "seek": 199392, "start": 1993.92, "end": 1996.44, "text": " out and said, please, please write this.", "tokens": [50364, 484, 293, 848, 11, 1767, 11, 1767, 2464, 341, 13, 50490], "temperature": 0.0, "avg_logprob": -0.16275853182362243, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.005217466503381729}, {"id": 470, "seek": 199392, "start": 1996.44, "end": 2001.28, "text": " We will waive the fee for the journal and everything and actually help you get it", "tokens": [50490, 492, 486, 5406, 488, 264, 12054, 337, 264, 6708, 293, 1203, 293, 767, 854, 291, 483, 309, 50732], "temperature": 0.0, "avg_logprob": -0.16275853182362243, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.005217466503381729}, {"id": 471, "seek": 199392, "start": 2001.28, "end": 2002.44, "text": " through publishing and stuff.", "tokens": [50732, 807, 17832, 293, 1507, 13, 50790], "temperature": 0.0, "avg_logprob": -0.16275853182362243, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.005217466503381729}, {"id": 472, "seek": 199392, "start": 2002.48, "end": 2006.52, "text": " So yeah, so I don't, other than that, I've never written a first author paper.", "tokens": [50792, 407, 1338, 11, 370, 286, 500, 380, 11, 661, 813, 300, 11, 286, 600, 1128, 3720, 257, 700, 3793, 3035, 13, 50994], "temperature": 0.0, "avg_logprob": -0.16275853182362243, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.005217466503381729}, {"id": 473, "seek": 199392, "start": 2007.2, "end": 2012.96, "text": " So the research is like, well, so for example, you know, Don Bench was a", "tokens": [51028, 407, 264, 2132, 307, 411, 11, 731, 11, 370, 337, 1365, 11, 291, 458, 11, 1468, 3964, 339, 390, 257, 51316], "temperature": 0.0, "avg_logprob": -0.16275853182362243, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.005217466503381729}, {"id": 474, "seek": 199392, "start": 2012.96, "end": 2016.24, "text": " competition which Stanford ran a few years ago.", "tokens": [51316, 6211, 597, 20374, 5872, 257, 1326, 924, 2057, 13, 51480], "temperature": 0.0, "avg_logprob": -0.16275853182362243, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.005217466503381729}, {"id": 475, "seek": 199392, "start": 2018.0, "end": 2021.88, "text": " It was kind of the first big competition of like, who couldn't train", "tokens": [51568, 467, 390, 733, 295, 264, 700, 955, 6211, 295, 411, 11, 567, 2809, 380, 3847, 51762], "temperature": 0.0, "avg_logprob": -0.16275853182362243, "compression_ratio": 1.6381322957198443, "no_speech_prob": 0.005217466503381729}, {"id": 476, "seek": 202188, "start": 2021.96, "end": 2024.96, "text": " neural nets the fastest rather than the most accurate.", "tokens": [50368, 18161, 36170, 264, 14573, 2831, 813, 264, 881, 8559, 13, 50518], "temperature": 0.0, "avg_logprob": -0.203739603360494, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.004197061061859131}, {"id": 477, "seek": 202188, "start": 2025.96, "end": 2031.7600000000002, "text": " And specifically it was who couldn't train ImageNet the fastest.", "tokens": [50568, 400, 4682, 309, 390, 567, 2809, 380, 3847, 29903, 31890, 264, 14573, 13, 50858], "temperature": 0.0, "avg_logprob": -0.203739603360494, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.004197061061859131}, {"id": 478, "seek": 202188, "start": 2032.7600000000002, "end": 2037.0, "text": " And this was like one of these things where it was created by necessity.", "tokens": [50908, 400, 341, 390, 411, 472, 295, 613, 721, 689, 309, 390, 2942, 538, 24217, 13, 51120], "temperature": 0.0, "avg_logprob": -0.203739603360494, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.004197061061859131}, {"id": 479, "seek": 202188, "start": 2037.3600000000001, "end": 2039.6000000000001, "text": " So Google had just released their TPUs.", "tokens": [51138, 407, 3329, 632, 445, 4736, 641, 314, 8115, 82, 13, 51250], "temperature": 0.0, "avg_logprob": -0.203739603360494, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.004197061061859131}, {"id": 480, "seek": 202188, "start": 2040.2, "end": 2044.68, "text": " And so I heard from my friends at Google that they had put together this big team", "tokens": [51280, 400, 370, 286, 2198, 490, 452, 1855, 412, 3329, 300, 436, 632, 829, 1214, 341, 955, 1469, 51504], "temperature": 0.0, "avg_logprob": -0.203739603360494, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.004197061061859131}, {"id": 481, "seek": 202188, "start": 2044.92, "end": 2049.8, "text": " to smash Don Bench so that they could prove to people that they had to use", "tokens": [51516, 281, 17960, 1468, 3964, 339, 370, 300, 436, 727, 7081, 281, 561, 300, 436, 632, 281, 764, 51760], "temperature": 0.0, "avg_logprob": -0.203739603360494, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.004197061061859131}, {"id": 482, "seek": 204980, "start": 2050.0, "end": 2053.2400000000002, "text": " Google Cloud and use their TPUs and show who could their TPUs were.", "tokens": [50374, 3329, 8061, 293, 764, 641, 314, 8115, 82, 293, 855, 567, 727, 641, 314, 8115, 82, 645, 13, 50536], "temperature": 0.0, "avg_logprob": -0.15812605937928645, "compression_ratio": 1.6711864406779662, "no_speech_prob": 0.006287967320531607}, {"id": 483, "seek": 204980, "start": 2054.04, "end": 2057.2000000000003, "text": " And we kind of thought, oh, shit, this would be a disaster if they do that", "tokens": [50576, 400, 321, 733, 295, 1194, 11, 1954, 11, 4611, 11, 341, 576, 312, 257, 11293, 498, 436, 360, 300, 50734], "temperature": 0.0, "avg_logprob": -0.15812605937928645, "compression_ratio": 1.6711864406779662, "no_speech_prob": 0.006287967320531607}, {"id": 484, "seek": 204980, "start": 2057.2000000000003, "end": 2060.1200000000003, "text": " because then everybody's going to be like, oh, deep learning is not accessible.", "tokens": [50734, 570, 550, 2201, 311, 516, 281, 312, 411, 11, 1954, 11, 2452, 2539, 307, 406, 9515, 13, 50880], "temperature": 0.0, "avg_logprob": -0.15812605937928645, "compression_ratio": 1.6711864406779662, "no_speech_prob": 0.006287967320531607}, {"id": 485, "seek": 204980, "start": 2060.52, "end": 2063.2400000000002, "text": " You know, to actually be good at it, you have to be Google and you have to", "tokens": [50900, 509, 458, 11, 281, 767, 312, 665, 412, 309, 11, 291, 362, 281, 312, 3329, 293, 291, 362, 281, 51036], "temperature": 0.0, "avg_logprob": -0.15812605937928645, "compression_ratio": 1.6711864406779662, "no_speech_prob": 0.006287967320531607}, {"id": 486, "seek": 204980, "start": 2063.2400000000002, "end": 2064.6800000000003, "text": " use special silicon and so.", "tokens": [51036, 764, 2121, 22848, 293, 370, 13, 51108], "temperature": 0.0, "avg_logprob": -0.15812605937928645, "compression_ratio": 1.6711864406779662, "no_speech_prob": 0.006287967320531607}, {"id": 487, "seek": 204980, "start": 2065.04, "end": 2069.36, "text": " So, you know, we, we only found out about this 10 days before the competition finished.", "tokens": [51126, 407, 11, 291, 458, 11, 321, 11, 321, 787, 1352, 484, 466, 341, 1266, 1708, 949, 264, 6211, 4335, 13, 51342], "temperature": 0.0, "avg_logprob": -0.15812605937928645, "compression_ratio": 1.6711864406779662, "no_speech_prob": 0.006287967320531607}, {"id": 488, "seek": 204980, "start": 2070.1200000000003, "end": 2075.32, "text": " But, you know, we basically got together an emergency bunch of our students and", "tokens": [51380, 583, 11, 291, 458, 11, 321, 1936, 658, 1214, 364, 7473, 3840, 295, 527, 1731, 293, 51640], "temperature": 0.0, "avg_logprob": -0.15812605937928645, "compression_ratio": 1.6711864406779662, "no_speech_prob": 0.006287967320531607}, {"id": 489, "seek": 207532, "start": 2075.36, "end": 2081.6400000000003, "text": " Rachel and I and sat for the next 10 days and just tried to crunch through and", "tokens": [50366, 14246, 293, 286, 293, 3227, 337, 264, 958, 1266, 1708, 293, 445, 3031, 281, 13386, 807, 293, 50680], "temperature": 0.0, "avg_logprob": -0.20081502337788426, "compression_ratio": 1.5022831050228311, "no_speech_prob": 0.0043302373960614204}, {"id": 490, "seek": 207532, "start": 2082.52, "end": 2087.2400000000002, "text": " tried to use all of our best ideas that had come from our research.", "tokens": [50724, 3031, 281, 764, 439, 295, 527, 1151, 3487, 300, 632, 808, 490, 527, 2132, 13, 50960], "temperature": 0.0, "avg_logprob": -0.20081502337788426, "compression_ratio": 1.5022831050228311, "no_speech_prob": 0.0043302373960614204}, {"id": 491, "seek": 207532, "start": 2088.32, "end": 2092.4, "text": " That's a particularly progressive resizing, which is basically train mainly on small things.", "tokens": [51014, 663, 311, 257, 4098, 16131, 725, 3319, 11, 597, 307, 1936, 3847, 8704, 322, 1359, 721, 13, 51218], "temperature": 0.0, "avg_logprob": -0.20081502337788426, "compression_ratio": 1.5022831050228311, "no_speech_prob": 0.0043302373960614204}, {"id": 492, "seek": 207532, "start": 2093.48, "end": 2097.2000000000003, "text": " Train on non-square things, you know, stuff like that.", "tokens": [51272, 28029, 322, 2107, 12, 33292, 543, 721, 11, 291, 458, 11, 1507, 411, 300, 13, 51458], "temperature": 0.0, "avg_logprob": -0.20081502337788426, "compression_ratio": 1.5022831050228311, "no_speech_prob": 0.0043302373960614204}, {"id": 493, "seek": 207532, "start": 2097.6000000000004, "end": 2100.1600000000003, "text": " And so, yeah, we ended up winning.", "tokens": [51478, 400, 370, 11, 1338, 11, 321, 4590, 493, 8224, 13, 51606], "temperature": 0.0, "avg_logprob": -0.20081502337788426, "compression_ratio": 1.5022831050228311, "no_speech_prob": 0.0043302373960614204}, {"id": 494, "seek": 210016, "start": 2100.96, "end": 2101.48, "text": " Thank God.", "tokens": [50404, 1044, 1265, 13, 50430], "temperature": 0.0, "avg_logprob": -0.1357562782228455, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.012427864596247673}, {"id": 495, "seek": 210016, "start": 2102.08, "end": 2106.48, "text": " And so, you know, we turned it around from being like, oh, shit, you know, this is", "tokens": [50460, 400, 370, 11, 291, 458, 11, 321, 3574, 309, 926, 490, 885, 411, 11, 1954, 11, 4611, 11, 291, 458, 11, 341, 307, 50680], "temperature": 0.0, "avg_logprob": -0.1357562782228455, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.012427864596247673}, {"id": 496, "seek": 210016, "start": 2106.48, "end": 2109.44, "text": " going to show that you have to be Google and have TPUs to being like, oh, my God,", "tokens": [50680, 516, 281, 855, 300, 291, 362, 281, 312, 3329, 293, 362, 314, 8115, 82, 281, 885, 411, 11, 1954, 11, 452, 1265, 11, 50828], "temperature": 0.0, "avg_logprob": -0.1357562782228455, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.012427864596247673}, {"id": 497, "seek": 210016, "start": 2109.44, "end": 2111.72, "text": " even the little guy can do deep learning.", "tokens": [50828, 754, 264, 707, 2146, 393, 360, 2452, 2539, 13, 50942], "temperature": 0.0, "avg_logprob": -0.1357562782228455, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.012427864596247673}, {"id": 498, "seek": 210016, "start": 2113.56, "end": 2118.3599999999997, "text": " So that's an example of the kind of like research artifacts we do.", "tokens": [51034, 407, 300, 311, 364, 1365, 295, 264, 733, 295, 411, 2132, 24617, 321, 360, 13, 51274], "temperature": 0.0, "avg_logprob": -0.1357562782228455, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.012427864596247673}, {"id": 499, "seek": 210016, "start": 2118.8399999999997, "end": 2124.3199999999997, "text": " And yeah, so all of my research is always, how do we do more with less, you know,", "tokens": [51298, 400, 1338, 11, 370, 439, 295, 452, 2132, 307, 1009, 11, 577, 360, 321, 360, 544, 365, 1570, 11, 291, 458, 11, 51572], "temperature": 0.0, "avg_logprob": -0.1357562782228455, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.012427864596247673}, {"id": 500, "seek": 210016, "start": 2124.3199999999997, "end": 2129.8399999999997, "text": " so how do we get better results with less data, with less compute, with less complexity.", "tokens": [51572, 370, 577, 360, 321, 483, 1101, 3542, 365, 1570, 1412, 11, 365, 1570, 14722, 11, 365, 1570, 14024, 13, 51848], "temperature": 0.0, "avg_logprob": -0.1357562782228455, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.012427864596247673}, {"id": 501, "seek": 213016, "start": 2130.52, "end": 2134.48, "text": " With less education, you know, stuff like that.", "tokens": [50382, 2022, 1570, 3309, 11, 291, 458, 11, 1507, 411, 300, 13, 50580], "temperature": 0.0, "avg_logprob": -0.2563252956309217, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.0005356123438104987}, {"id": 502, "seek": 213016, "start": 2134.48, "end": 2137.56, "text": " So your LLM fits obviously a good example of that.", "tokens": [50580, 407, 428, 441, 43, 44, 9001, 2745, 257, 665, 1365, 295, 300, 13, 50734], "temperature": 0.0, "avg_logprob": -0.2563252956309217, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.0005356123438104987}, {"id": 503, "seek": 213016, "start": 2138.96, "end": 2142.68, "text": " And most recently you published, can LLMs learn from a single example?", "tokens": [50804, 400, 881, 3938, 291, 6572, 11, 393, 441, 43, 26386, 1466, 490, 257, 2167, 1365, 30, 50990], "temperature": 0.0, "avg_logprob": -0.2563252956309217, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.0005356123438104987}, {"id": 504, "seek": 213016, "start": 2143.96, "end": 2146.08, "text": " Maybe could you tell the story a little bit behind that?", "tokens": [51054, 2704, 727, 291, 980, 264, 1657, 257, 707, 857, 2261, 300, 30, 51160], "temperature": 0.0, "avg_logprob": -0.2563252956309217, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.0005356123438104987}, {"id": 505, "seek": 213016, "start": 2146.08, "end": 2150.16, "text": " And maybe that goes a little bit too far into the learning on very low resource.", "tokens": [51160, 400, 1310, 300, 1709, 257, 707, 857, 886, 1400, 666, 264, 2539, 322, 588, 2295, 7684, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2563252956309217, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.0005356123438104987}, {"id": 506, "seek": 213016, "start": 2151.92, "end": 2152.7599999999998, "text": " The literature.", "tokens": [51452, 440, 10394, 13, 51494], "temperature": 0.0, "avg_logprob": -0.2563252956309217, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.0005356123438104987}, {"id": 507, "seek": 213016, "start": 2153.2, "end": 2153.8799999999997, "text": " Yeah.", "tokens": [51516, 865, 13, 51550], "temperature": 0.0, "avg_logprob": -0.2563252956309217, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.0005356123438104987}, {"id": 508, "seek": 213016, "start": 2154.52, "end": 2154.8799999999997, "text": " Yeah.", "tokens": [51582, 865, 13, 51600], "temperature": 0.0, "avg_logprob": -0.2563252956309217, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.0005356123438104987}, {"id": 509, "seek": 215488, "start": 2154.88, "end": 2161.32, "text": " So me and my friend, John O'Whittaker, basically had been playing around with", "tokens": [50364, 407, 385, 293, 452, 1277, 11, 2619, 422, 6, 2471, 593, 4003, 11, 1936, 632, 668, 2433, 926, 365, 50686], "temperature": 0.0, "avg_logprob": -0.164464689436413, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00041713903192430735}, {"id": 510, "seek": 215488, "start": 2161.32, "end": 2165.2400000000002, "text": " this fun Kaggle competition, which is actually still running as we speak,", "tokens": [50686, 341, 1019, 48751, 22631, 6211, 11, 597, 307, 767, 920, 2614, 382, 321, 1710, 11, 50882], "temperature": 0.0, "avg_logprob": -0.164464689436413, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00041713903192430735}, {"id": 511, "seek": 215488, "start": 2165.2400000000002, "end": 2173.28, "text": " which is, can you create a model which can answer multiple choice questions", "tokens": [50882, 597, 307, 11, 393, 291, 1884, 257, 2316, 597, 393, 1867, 3866, 3922, 1651, 51284], "temperature": 0.0, "avg_logprob": -0.164464689436413, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00041713903192430735}, {"id": 512, "seek": 215488, "start": 2173.28, "end": 2175.2400000000002, "text": " about anything that's in Wikipedia?", "tokens": [51284, 466, 1340, 300, 311, 294, 28999, 30, 51382], "temperature": 0.0, "avg_logprob": -0.164464689436413, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00041713903192430735}, {"id": 513, "seek": 215488, "start": 2175.92, "end": 2182.4, "text": " And the thing that makes it interesting is that your model has to run on Kaggle", "tokens": [51416, 400, 264, 551, 300, 1669, 309, 1880, 307, 300, 428, 2316, 575, 281, 1190, 322, 48751, 22631, 51740], "temperature": 0.0, "avg_logprob": -0.164464689436413, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00041713903192430735}, {"id": 514, "seek": 218240, "start": 2182.76, "end": 2186.04, "text": " within nine hours and Kaggle is very, very limited.", "tokens": [50382, 1951, 4949, 2496, 293, 48751, 22631, 307, 588, 11, 588, 5567, 13, 50546], "temperature": 0.0, "avg_logprob": -0.17010140173213997, "compression_ratio": 1.4840182648401827, "no_speech_prob": 0.005218889564275742}, {"id": 515, "seek": 218240, "start": 2186.04, "end": 2191.6800000000003, "text": " So you've only got 14 gig RAM, only two CPUs and a small, very old GPU.", "tokens": [50546, 407, 291, 600, 787, 658, 3499, 8741, 14561, 11, 787, 732, 13199, 82, 293, 257, 1359, 11, 588, 1331, 18407, 13, 50828], "temperature": 0.0, "avg_logprob": -0.17010140173213997, "compression_ratio": 1.4840182648401827, "no_speech_prob": 0.005218889564275742}, {"id": 516, "seek": 218240, "start": 2193.6800000000003, "end": 2196.7200000000003, "text": " So this is cool, you know, if you can do well at this, and this is a good example", "tokens": [50928, 407, 341, 307, 1627, 11, 291, 458, 11, 498, 291, 393, 360, 731, 412, 341, 11, 293, 341, 307, 257, 665, 1365, 51080], "temperature": 0.0, "avg_logprob": -0.17010140173213997, "compression_ratio": 1.4840182648401827, "no_speech_prob": 0.005218889564275742}, {"id": 517, "seek": 218240, "start": 2196.7200000000003, "end": 2198.2400000000002, "text": " of like, oh, you can do more with less.", "tokens": [51080, 295, 411, 11, 1954, 11, 291, 393, 360, 544, 365, 1570, 13, 51156], "temperature": 0.0, "avg_logprob": -0.17010140173213997, "compression_ratio": 1.4840182648401827, "no_speech_prob": 0.005218889564275742}, {"id": 518, "seek": 218240, "start": 2199.6, "end": 2205.64, "text": " So yeah, John O and I were playing around with fine tuning, of course, transfer", "tokens": [51224, 407, 1338, 11, 2619, 422, 293, 286, 645, 2433, 926, 365, 2489, 15164, 11, 295, 1164, 11, 5003, 51526], "temperature": 0.0, "avg_logprob": -0.17010140173213997, "compression_ratio": 1.4840182648401827, "no_speech_prob": 0.005218889564275742}, {"id": 519, "seek": 220564, "start": 2205.64, "end": 2208.12, "text": " learning, pre-trained language models.", "tokens": [50364, 2539, 11, 659, 12, 17227, 2001, 2856, 5245, 13, 50488], "temperature": 0.0, "avg_logprob": -0.22258650538433028, "compression_ratio": 1.9963636363636363, "no_speech_prob": 0.1870451271533966}, {"id": 520, "seek": 220564, "start": 2209.16, "end": 2215.0, "text": " And we saw this like, so we always, you know, plot our losses as we go.", "tokens": [50540, 400, 321, 1866, 341, 411, 11, 370, 321, 1009, 11, 291, 458, 11, 7542, 527, 15352, 382, 321, 352, 13, 50832], "temperature": 0.0, "avg_logprob": -0.22258650538433028, "compression_ratio": 1.9963636363636363, "no_speech_prob": 0.1870451271533966}, {"id": 521, "seek": 220564, "start": 2215.04, "end": 2218.12, "text": " So here's another thing we created, we actually, Sylvain Gougir, when he worked", "tokens": [50834, 407, 510, 311, 1071, 551, 321, 2942, 11, 321, 767, 11, 3902, 14574, 491, 460, 513, 347, 11, 562, 415, 2732, 50988], "temperature": 0.0, "avg_logprob": -0.22258650538433028, "compression_ratio": 1.9963636363636363, "no_speech_prob": 0.1870451271533966}, {"id": 522, "seek": 220564, "start": 2218.12, "end": 2222.7599999999998, "text": " with us created called Fast Progress, which is kind of like TQEDM, but we think", "tokens": [50988, 365, 505, 2942, 1219, 15968, 32587, 11, 597, 307, 733, 295, 411, 314, 48, 4731, 44, 11, 457, 321, 519, 51220], "temperature": 0.0, "avg_logprob": -0.22258650538433028, "compression_ratio": 1.9963636363636363, "no_speech_prob": 0.1870451271533966}, {"id": 523, "seek": 220564, "start": 2222.7599999999998, "end": 2223.2, "text": " a lot better.", "tokens": [51220, 257, 688, 1101, 13, 51242], "temperature": 0.0, "avg_logprob": -0.22258650538433028, "compression_ratio": 1.9963636363636363, "no_speech_prob": 0.1870451271533966}, {"id": 524, "seek": 220564, "start": 2223.6, "end": 2227.48, "text": " So you look at our Fast Progress curves, and they kind of go down, down, down,", "tokens": [51262, 407, 291, 574, 412, 527, 15968, 32587, 19490, 11, 293, 436, 733, 295, 352, 760, 11, 760, 11, 760, 11, 51456], "temperature": 0.0, "avg_logprob": -0.22258650538433028, "compression_ratio": 1.9963636363636363, "no_speech_prob": 0.1870451271533966}, {"id": 525, "seek": 220564, "start": 2227.48, "end": 2229.72, "text": " down, down, down, a little bit, a little bit, a little bit, and then suddenly", "tokens": [51456, 760, 11, 760, 11, 760, 11, 257, 707, 857, 11, 257, 707, 857, 11, 257, 707, 857, 11, 293, 550, 5800, 51568], "temperature": 0.0, "avg_logprob": -0.22258650538433028, "compression_ratio": 1.9963636363636363, "no_speech_prob": 0.1870451271533966}, {"id": 526, "seek": 220564, "start": 2229.72, "end": 2231.4, "text": " go clunk, and they drop.", "tokens": [51568, 352, 596, 3197, 11, 293, 436, 3270, 13, 51652], "temperature": 0.0, "avg_logprob": -0.22258650538433028, "compression_ratio": 1.9963636363636363, "no_speech_prob": 0.1870451271533966}, {"id": 527, "seek": 220564, "start": 2232.0, "end": 2235.0, "text": " And then down, down, down, down, a little bit, and then suddenly clunk, they drop.", "tokens": [51682, 400, 550, 760, 11, 760, 11, 760, 11, 760, 11, 257, 707, 857, 11, 293, 550, 5800, 596, 3197, 11, 436, 3270, 13, 51832], "temperature": 0.0, "avg_logprob": -0.22258650538433028, "compression_ratio": 1.9963636363636363, "no_speech_prob": 0.1870451271533966}, {"id": 528, "seek": 223500, "start": 2235.0, "end": 2236.56, "text": " We're like, what the hell?", "tokens": [50364, 492, 434, 411, 11, 437, 264, 4921, 30, 50442], "temperature": 0.0, "avg_logprob": -0.16656880674108995, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.00020339911861810833}, {"id": 529, "seek": 223500, "start": 2236.56, "end": 2239.8, "text": " These clunks are occurring at the end of each epoch.", "tokens": [50442, 1981, 596, 17627, 366, 18386, 412, 264, 917, 295, 1184, 30992, 339, 13, 50604], "temperature": 0.0, "avg_logprob": -0.16656880674108995, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.00020339911861810833}, {"id": 530, "seek": 223500, "start": 2240.56, "end": 2247.04, "text": " So normally in deep learning, this would be, you know, I've seen this before,", "tokens": [50642, 407, 5646, 294, 2452, 2539, 11, 341, 576, 312, 11, 291, 458, 11, 286, 600, 1612, 341, 949, 11, 50966], "temperature": 0.0, "avg_logprob": -0.16656880674108995, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.00020339911861810833}, {"id": 531, "seek": 223500, "start": 2247.04, "end": 2248.04, "text": " it's always been a bug.", "tokens": [50966, 309, 311, 1009, 668, 257, 7426, 13, 51016], "temperature": 0.0, "avg_logprob": -0.16656880674108995, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.00020339911861810833}, {"id": 532, "seek": 223500, "start": 2248.68, "end": 2251.88, "text": " It's always turned out that like, oh, we accidentally forgot to turn on", "tokens": [51048, 467, 311, 1009, 3574, 484, 300, 411, 11, 1954, 11, 321, 15715, 5298, 281, 1261, 322, 51208], "temperature": 0.0, "avg_logprob": -0.16656880674108995, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.00020339911861810833}, {"id": 533, "seek": 223500, "start": 2251.88, "end": 2253.68, "text": " eval mode during the validation set.", "tokens": [51208, 1073, 304, 4391, 1830, 264, 24071, 992, 13, 51298], "temperature": 0.0, "avg_logprob": -0.16656880674108995, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.00020339911861810833}, {"id": 534, "seek": 223500, "start": 2253.68, "end": 2259.44, "text": " So I was actually learning then, or, oh, we accidentally were kept letting moving", "tokens": [51298, 407, 286, 390, 767, 2539, 550, 11, 420, 11, 1954, 11, 321, 15715, 645, 4305, 8295, 2684, 51586], "temperature": 0.0, "avg_logprob": -0.16656880674108995, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.00020339911861810833}, {"id": 535, "seek": 223500, "start": 2259.44, "end": 2261.48, "text": " average statistics throughout the epoch.", "tokens": [51586, 4274, 12523, 3710, 264, 30992, 339, 13, 51688], "temperature": 0.0, "avg_logprob": -0.16656880674108995, "compression_ratio": 1.6788617886178863, "no_speech_prob": 0.00020339911861810833}, {"id": 536, "seek": 226148, "start": 2261.52, "end": 2264.36, "text": " So, you know, for it's recent, the moving average or whatever.", "tokens": [50366, 407, 11, 291, 458, 11, 337, 309, 311, 5162, 11, 264, 2684, 4274, 420, 2035, 13, 50508], "temperature": 0.0, "avg_logprob": -0.23335684499432963, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0007203912828117609}, {"id": 537, "seek": 226148, "start": 2264.36, "end": 2266.68, "text": " And so we were using hugging face trainer.", "tokens": [50508, 400, 370, 321, 645, 1228, 41706, 1851, 21110, 13, 50624], "temperature": 0.0, "avg_logprob": -0.23335684499432963, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0007203912828117609}, {"id": 538, "seek": 226148, "start": 2267.28, "end": 2270.68, "text": " So, you know, I did not give my friends at hugging face the benefit", "tokens": [50654, 407, 11, 291, 458, 11, 286, 630, 406, 976, 452, 1855, 412, 41706, 1851, 264, 5121, 50824], "temperature": 0.0, "avg_logprob": -0.23335684499432963, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0007203912828117609}, {"id": 539, "seek": 226148, "start": 2270.68, "end": 2271.2, "text": " of the doubt.", "tokens": [50824, 295, 264, 6385, 13, 50850], "temperature": 0.0, "avg_logprob": -0.23335684499432963, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0007203912828117609}, {"id": 540, "seek": 226148, "start": 2271.2, "end": 2276.04, "text": " I thought, oh, they fucked up hugging face trainer, you know, idiots.", "tokens": [50850, 286, 1194, 11, 1954, 11, 436, 22518, 493, 41706, 1851, 21110, 11, 291, 458, 11, 36454, 13, 51092], "temperature": 0.0, "avg_logprob": -0.23335684499432963, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0007203912828117609}, {"id": 541, "seek": 226148, "start": 2276.16, "end": 2278.56, "text": " Well, you'll use the faster trainer instead.", "tokens": [51098, 1042, 11, 291, 603, 764, 264, 4663, 21110, 2602, 13, 51218], "temperature": 0.0, "avg_logprob": -0.23335684499432963, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0007203912828117609}, {"id": 542, "seek": 226148, "start": 2278.56, "end": 2279.84, "text": " So we switched over to learner.", "tokens": [51218, 407, 321, 16858, 670, 281, 33347, 13, 51282], "temperature": 0.0, "avg_logprob": -0.23335684499432963, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0007203912828117609}, {"id": 543, "seek": 226148, "start": 2279.84, "end": 2281.2, "text": " We still saw the clunks.", "tokens": [51282, 492, 920, 1866, 264, 596, 17627, 13, 51350], "temperature": 0.0, "avg_logprob": -0.23335684499432963, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0007203912828117609}, {"id": 544, "seek": 226148, "start": 2281.68, "end": 2288.12, "text": " And, you know, that's, yeah, it shouldn't really happen because semantically", "tokens": [51374, 400, 11, 291, 458, 11, 300, 311, 11, 1338, 11, 309, 4659, 380, 534, 1051, 570, 4361, 49505, 51696], "temperature": 0.0, "avg_logprob": -0.23335684499432963, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0007203912828117609}, {"id": 545, "seek": 228812, "start": 2288.12, "end": 2293.08, "text": " speaking in the epoch isn't like, it's not a thing, you know, like nothing", "tokens": [50364, 4124, 294, 264, 30992, 339, 1943, 380, 411, 11, 309, 311, 406, 257, 551, 11, 291, 458, 11, 411, 1825, 50612], "temperature": 0.0, "avg_logprob": -0.20478987156000353, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0032725201454013586}, {"id": 546, "seek": 228812, "start": 2293.08, "end": 2297.24, "text": " happens or nothing's meant to happen when you go from ending one epoch to", "tokens": [50612, 2314, 420, 1825, 311, 4140, 281, 1051, 562, 291, 352, 490, 8121, 472, 30992, 339, 281, 50820], "temperature": 0.0, "avg_logprob": -0.20478987156000353, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0032725201454013586}, {"id": 547, "seek": 228812, "start": 2297.24, "end": 2298.24, "text": " starting the next one.", "tokens": [50820, 2891, 264, 958, 472, 13, 50870], "temperature": 0.0, "avg_logprob": -0.20478987156000353, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0032725201454013586}, {"id": 548, "seek": 228812, "start": 2300.48, "end": 2301.56, "text": " So there shouldn't be a clunk.", "tokens": [50982, 407, 456, 4659, 380, 312, 257, 596, 3197, 13, 51036], "temperature": 0.0, "avg_logprob": -0.20478987156000353, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0032725201454013586}, {"id": 549, "seek": 228812, "start": 2302.4, "end": 2305.2799999999997, "text": " You know, so I kind of asked around on the open source discords.", "tokens": [51078, 509, 458, 11, 370, 286, 733, 295, 2351, 926, 322, 264, 1269, 4009, 2983, 5703, 13, 51222], "temperature": 0.0, "avg_logprob": -0.20478987156000353, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0032725201454013586}, {"id": 550, "seek": 228812, "start": 2305.56, "end": 2308.88, "text": " That's like, what's going on here?", "tokens": [51236, 663, 311, 411, 11, 437, 311, 516, 322, 510, 30, 51402], "temperature": 0.0, "avg_logprob": -0.20478987156000353, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0032725201454013586}, {"id": 551, "seek": 228812, "start": 2309.2, "end": 2311.6, "text": " And everybody was just like, oh, that's just what, that's just what these", "tokens": [51418, 400, 2201, 390, 445, 411, 11, 1954, 11, 300, 311, 445, 437, 11, 300, 311, 445, 437, 613, 51538], "temperature": 0.0, "avg_logprob": -0.20478987156000353, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0032725201454013586}, {"id": 552, "seek": 228812, "start": 2311.6, "end": 2312.56, "text": " training curves look like.", "tokens": [51538, 3097, 19490, 574, 411, 13, 51586], "temperature": 0.0, "avg_logprob": -0.20478987156000353, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0032725201454013586}, {"id": 553, "seek": 228812, "start": 2312.72, "end": 2313.68, "text": " Those all look like that.", "tokens": [51594, 3950, 439, 574, 411, 300, 13, 51642], "temperature": 0.0, "avg_logprob": -0.20478987156000353, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0032725201454013586}, {"id": 554, "seek": 228812, "start": 2313.88, "end": 2314.52, "text": " Don't worry about it.", "tokens": [51652, 1468, 380, 3292, 466, 309, 13, 51684], "temperature": 0.0, "avg_logprob": -0.20478987156000353, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0032725201454013586}, {"id": 555, "seek": 231452, "start": 2315.08, "end": 2317.36, "text": " That's like, oh, are you all using trainer?", "tokens": [50392, 663, 311, 411, 11, 1954, 11, 366, 291, 439, 1228, 21110, 30, 50506], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 556, "seek": 231452, "start": 2317.52, "end": 2318.2, "text": " Yes.", "tokens": [50514, 1079, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 557, "seek": 231452, "start": 2318.32, "end": 2320.44, "text": " Oh, well, there must be some buck with training.", "tokens": [50554, 876, 11, 731, 11, 456, 1633, 312, 512, 14894, 365, 3097, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 558, "seek": 231452, "start": 2320.44, "end": 2323.16, "text": " And it's like, well, we also saw it in learner and somebody else is like, no,", "tokens": [50660, 400, 309, 311, 411, 11, 731, 11, 321, 611, 1866, 309, 294, 33347, 293, 2618, 1646, 307, 411, 11, 572, 11, 50796], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 559, "seek": 231452, "start": 2323.16, "end": 2324.12, "text": " we've got our own trainer.", "tokens": [50796, 321, 600, 658, 527, 1065, 21110, 13, 50844], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 560, "seek": 231452, "start": 2324.12, "end": 2324.88, "text": " We get it as well.", "tokens": [50844, 492, 483, 309, 382, 731, 13, 50882], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 561, "seek": 231452, "start": 2325.32, "end": 2326.24, "text": " They're just like, don't worry about it.", "tokens": [50904, 814, 434, 445, 411, 11, 500, 380, 3292, 466, 309, 13, 50950], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 562, "seek": 231452, "start": 2326.24, "end": 2327.2, "text": " It's just something we see.", "tokens": [50950, 467, 311, 445, 746, 321, 536, 13, 50998], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 563, "seek": 231452, "start": 2327.92, "end": 2328.56, "text": " It's just normal.", "tokens": [51034, 467, 311, 445, 2710, 13, 51066], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 564, "seek": 231452, "start": 2329.04, "end": 2330.08, "text": " I can't do that.", "tokens": [51090, 286, 393, 380, 360, 300, 13, 51142], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 565, "seek": 231452, "start": 2330.08, "end": 2334.64, "text": " I can't just be like, here's something that's like in the previous 30 years of", "tokens": [51142, 286, 393, 380, 445, 312, 411, 11, 510, 311, 746, 300, 311, 411, 294, 264, 3894, 2217, 924, 295, 51370], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 566, "seek": 231452, "start": 2334.64, "end": 2336.52, "text": " neural networks, nobody ever saw it.", "tokens": [51370, 18161, 9590, 11, 5079, 1562, 1866, 309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 567, "seek": 231452, "start": 2336.72, "end": 2337.96, "text": " And now suddenly we see it.", "tokens": [51474, 400, 586, 5800, 321, 536, 309, 13, 51536], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 568, "seek": 231452, "start": 2339.16, "end": 2340.12, "text": " So don't worry about it.", "tokens": [51596, 407, 500, 380, 3292, 466, 309, 13, 51644], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 569, "seek": 231452, "start": 2340.44, "end": 2342.24, "text": " I like, I just, I have to know why.", "tokens": [51660, 286, 411, 11, 286, 445, 11, 286, 362, 281, 458, 983, 13, 51750], "temperature": 0.0, "avg_logprob": -0.1783398595349542, "compression_ratio": 1.8213058419243986, "no_speech_prob": 0.0021153397392481565}, {"id": 570, "seek": 234224, "start": 2342.52, "end": 2346.4399999999996, "text": " Can I clarify this is, was everyone that you're talking to, were they all", "tokens": [50378, 1664, 286, 17594, 341, 307, 11, 390, 1518, 300, 291, 434, 1417, 281, 11, 645, 436, 439, 50574], "temperature": 0.0, "avg_logprob": -0.167063706840565, "compression_ratio": 1.7532894736842106, "no_speech_prob": 0.0006875908584333956}, {"id": 571, "seek": 234224, "start": 2346.4399999999996, "end": 2348.7599999999998, "text": " seeing it for the same data set or in different data sets?", "tokens": [50574, 2577, 309, 337, 264, 912, 1412, 992, 420, 294, 819, 1412, 6352, 30, 50690], "temperature": 0.0, "avg_logprob": -0.167063706840565, "compression_ratio": 1.7532894736842106, "no_speech_prob": 0.0006875908584333956}, {"id": 572, "seek": 234224, "start": 2348.8799999999997, "end": 2351.9599999999996, "text": " Data, different data sets, different trainers.", "tokens": [50696, 11888, 11, 819, 1412, 6352, 11, 819, 35393, 13, 50850], "temperature": 0.0, "avg_logprob": -0.167063706840565, "compression_ratio": 1.7532894736842106, "no_speech_prob": 0.0006875908584333956}, {"id": 573, "seek": 234224, "start": 2351.9599999999996, "end": 2354.3599999999997, "text": " They're just like, no, this is just, this is just what it looks like when", "tokens": [50850, 814, 434, 445, 411, 11, 572, 11, 341, 307, 445, 11, 341, 307, 445, 437, 309, 1542, 411, 562, 50970], "temperature": 0.0, "avg_logprob": -0.167063706840565, "compression_ratio": 1.7532894736842106, "no_speech_prob": 0.0006875908584333956}, {"id": 574, "seek": 234224, "start": 2354.3599999999997, "end": 2355.64, "text": " you fine tune language models.", "tokens": [50970, 291, 2489, 10864, 2856, 5245, 13, 51034], "temperature": 0.0, "avg_logprob": -0.167063706840565, "compression_ratio": 1.7532894736842106, "no_speech_prob": 0.0006875908584333956}, {"id": 575, "seek": 234224, "start": 2355.64, "end": 2356.3199999999997, "text": " Don't worry about it.", "tokens": [51034, 1468, 380, 3292, 466, 309, 13, 51068], "temperature": 0.0, "avg_logprob": -0.167063706840565, "compression_ratio": 1.7532894736842106, "no_speech_prob": 0.0006875908584333956}, {"id": 576, "seek": 234224, "start": 2356.56, "end": 2361.3599999999997, "text": " You know, as I say, I hadn't seen it before, but I'd been kind of like, as I", "tokens": [51080, 509, 458, 11, 382, 286, 584, 11, 286, 8782, 380, 1612, 309, 949, 11, 457, 286, 1116, 668, 733, 295, 411, 11, 382, 286, 51320], "temperature": 0.0, "avg_logprob": -0.167063706840565, "compression_ratio": 1.7532894736842106, "no_speech_prob": 0.0006875908584333956}, {"id": 577, "seek": 234224, "start": 2361.3599999999997, "end": 2364.8799999999997, "text": " say, I, you know, I kept working on them for a couple of years after ULM fit.", "tokens": [51320, 584, 11, 286, 11, 291, 458, 11, 286, 4305, 1364, 322, 552, 337, 257, 1916, 295, 924, 934, 624, 43, 44, 3318, 13, 51496], "temperature": 0.0, "avg_logprob": -0.167063706840565, "compression_ratio": 1.7532894736842106, "no_speech_prob": 0.0006875908584333956}, {"id": 578, "seek": 234224, "start": 2364.8799999999997, "end": 2368.3999999999996, "text": " And then I kind of moved on to other things, partly out of frustration.", "tokens": [51496, 400, 550, 286, 733, 295, 4259, 322, 281, 661, 721, 11, 17031, 484, 295, 20491, 13, 51672], "temperature": 0.0, "avg_logprob": -0.167063706840565, "compression_ratio": 1.7532894736842106, "no_speech_prob": 0.0006875908584333956}, {"id": 579, "seek": 236840, "start": 2368.64, "end": 2375.04, "text": " So I hadn't been fine tuning, you know, um, I mean, Lamar's only been out for a", "tokens": [50376, 407, 286, 8782, 380, 668, 2489, 15164, 11, 291, 458, 11, 1105, 11, 286, 914, 11, 18825, 289, 311, 787, 668, 484, 337, 257, 50696], "temperature": 0.0, "avg_logprob": -0.1791752480171822, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.000379953533411026}, {"id": 580, "seek": 236840, "start": 2375.04, "end": 2375.76, "text": " few months, right?", "tokens": [50696, 1326, 2493, 11, 558, 30, 50732], "temperature": 0.0, "avg_logprob": -0.1791752480171822, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.000379953533411026}, {"id": 581, "seek": 236840, "start": 2375.76, "end": 2379.48, "text": " But I, I, I, I, I wasn't one of those people who jumped straight into it, you", "tokens": [50732, 583, 286, 11, 286, 11, 286, 11, 286, 11, 286, 2067, 380, 472, 295, 729, 561, 567, 13864, 2997, 666, 309, 11, 291, 50918], "temperature": 0.0, "avg_logprob": -0.1791752480171822, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.000379953533411026}, {"id": 582, "seek": 236840, "start": 2379.48, "end": 2385.0, "text": " know, so I was relatively new to the kind of Lamar fine tuning world, or else", "tokens": [50918, 458, 11, 370, 286, 390, 7226, 777, 281, 264, 733, 295, 18825, 289, 2489, 15164, 1002, 11, 420, 1646, 51194], "temperature": 0.0, "avg_logprob": -0.1791752480171822, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.000379953533411026}, {"id": 583, "seek": 236840, "start": 2385.56, "end": 2389.4, "text": " these guys had been, you know, doing it since day one.", "tokens": [51222, 613, 1074, 632, 668, 11, 291, 458, 11, 884, 309, 1670, 786, 472, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1791752480171822, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.000379953533411026}, {"id": 584, "seek": 236840, "start": 2390.32, "end": 2393.08, "text": " Um, it's only a few months ago, but it's still quite a bit of time.", "tokens": [51460, 3301, 11, 309, 311, 787, 257, 1326, 2493, 2057, 11, 457, 309, 311, 920, 1596, 257, 857, 295, 565, 13, 51598], "temperature": 0.0, "avg_logprob": -0.1791752480171822, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.000379953533411026}, {"id": 585, "seek": 236840, "start": 2393.08, "end": 2395.88, "text": " So, so yeah, they're just like, no, this is all what we see.", "tokens": [51598, 407, 11, 370, 1338, 11, 436, 434, 445, 411, 11, 572, 11, 341, 307, 439, 437, 321, 536, 13, 51738], "temperature": 0.0, "avg_logprob": -0.1791752480171822, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.000379953533411026}, {"id": 586, "seek": 236840, "start": 2396.28, "end": 2396.96, "text": " Don't worry about it.", "tokens": [51758, 1468, 380, 3292, 466, 309, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1791752480171822, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.000379953533411026}, {"id": 587, "seek": 239696, "start": 2397.96, "end": 2401.88, "text": " So yeah, I, I've got a very kind of like, I don't know, I've just got this", "tokens": [50414, 407, 1338, 11, 286, 11, 286, 600, 658, 257, 588, 733, 295, 411, 11, 286, 500, 380, 458, 11, 286, 600, 445, 658, 341, 50610], "temperature": 0.0, "avg_logprob": -0.23054753517617985, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.00034593677264638245}, {"id": 588, "seek": 239696, "start": 2401.88, "end": 2403.68, "text": " brain where I have to know why things are.", "tokens": [50610, 3567, 689, 286, 362, 281, 458, 983, 721, 366, 13, 50700], "temperature": 0.0, "avg_logprob": -0.23054753517617985, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.00034593677264638245}, {"id": 589, "seek": 239696, "start": 2404.2, "end": 2407.16, "text": " And so I kind of, I asked people like, well, why, why do you think it's happening?", "tokens": [50726, 400, 370, 286, 733, 295, 11, 286, 2351, 561, 411, 11, 731, 11, 983, 11, 983, 360, 291, 519, 309, 311, 2737, 30, 50874], "temperature": 0.0, "avg_logprob": -0.23054753517617985, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.00034593677264638245}, {"id": 590, "seek": 239696, "start": 2407.16, "end": 2410.28, "text": " And they'd be like, oh, we're pretty obviously, cause it's like memorized the", "tokens": [50874, 400, 436, 1116, 312, 411, 11, 1954, 11, 321, 434, 1238, 2745, 11, 3082, 309, 311, 411, 46677, 264, 51030], "temperature": 0.0, "avg_logprob": -0.23054753517617985, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.00034593677264638245}, {"id": 591, "seek": 239696, "start": 2410.28, "end": 2410.8, "text": " data set.", "tokens": [51030, 1412, 992, 13, 51056], "temperature": 0.0, "avg_logprob": -0.23054753517617985, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.00034593677264638245}, {"id": 592, "seek": 239696, "start": 2411.92, "end": 2414.52, "text": " It's just like, it can't be right.", "tokens": [51112, 467, 311, 445, 411, 11, 309, 393, 380, 312, 558, 13, 51242], "temperature": 0.0, "avg_logprob": -0.23054753517617985, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.00034593677264638245}, {"id": 593, "seek": 239696, "start": 2414.52, "end": 2415.64, "text": " It's only seen it once.", "tokens": [51242, 467, 311, 787, 1612, 309, 1564, 13, 51298], "temperature": 0.0, "avg_logprob": -0.23054753517617985, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.00034593677264638245}, {"id": 594, "seek": 239696, "start": 2415.68, "end": 2416.48, "text": " Like, look at this.", "tokens": [51300, 1743, 11, 574, 412, 341, 13, 51340], "temperature": 0.0, "avg_logprob": -0.23054753517617985, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.00034593677264638245}, {"id": 595, "seek": 239696, "start": 2416.48, "end": 2424.04, "text": " The, the losses dropped by 0.3, 0.3, which is like, basically it knows the answer.", "tokens": [51340, 440, 11, 264, 15352, 8119, 538, 1958, 13, 18, 11, 1958, 13, 18, 11, 597, 307, 411, 11, 1936, 309, 3255, 264, 1867, 13, 51718], "temperature": 0.0, "avg_logprob": -0.23054753517617985, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.00034593677264638245}, {"id": 596, "seek": 242404, "start": 2425.04, "end": 2430.0, "text": " Um, they're like, no, no, it's just, it is, it's just memorized the data set.", "tokens": [50414, 3301, 11, 436, 434, 411, 11, 572, 11, 572, 11, 309, 311, 445, 11, 309, 307, 11, 309, 311, 445, 46677, 264, 1412, 992, 13, 50662], "temperature": 0.0, "avg_logprob": -0.20864129573740858, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0016480606282129884}, {"id": 597, "seek": 242404, "start": 2430.0, "end": 2430.88, "text": " So yeah.", "tokens": [50662, 407, 1338, 13, 50706], "temperature": 0.0, "avg_logprob": -0.20864129573740858, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0016480606282129884}, {"id": 598, "seek": 242404, "start": 2430.88, "end": 2435.68, "text": " So look, John, when I did not discover this and John O and I did not come up with a", "tokens": [50706, 407, 574, 11, 2619, 11, 562, 286, 630, 406, 4411, 341, 293, 2619, 422, 293, 286, 630, 406, 808, 493, 365, 257, 50946], "temperature": 0.0, "avg_logprob": -0.20864129573740858, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0016480606282129884}, {"id": 599, "seek": 242404, "start": 2435.68, "end": 2439.04, "text": " hypothesis, you know, I guess we were just the ones, I guess, who had been around", "tokens": [50946, 17291, 11, 291, 458, 11, 286, 2041, 321, 645, 445, 264, 2306, 11, 286, 2041, 11, 567, 632, 668, 926, 51114], "temperature": 0.0, "avg_logprob": -0.20864129573740858, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0016480606282129884}, {"id": 600, "seek": 242404, "start": 2439.04, "end": 2442.56, "text": " for long enough to recognize that like this, this isn't how it's meant to work.", "tokens": [51114, 337, 938, 1547, 281, 5521, 300, 411, 341, 11, 341, 1943, 380, 577, 309, 311, 4140, 281, 589, 13, 51290], "temperature": 0.0, "avg_logprob": -0.20864129573740858, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0016480606282129884}, {"id": 601, "seek": 242404, "start": 2442.92, "end": 2447.84, "text": " And so we, you know, and so we went back and like, okay, let's just run some", "tokens": [51308, 400, 370, 321, 11, 291, 458, 11, 293, 370, 321, 1437, 646, 293, 411, 11, 1392, 11, 718, 311, 445, 1190, 512, 51554], "temperature": 0.0, "avg_logprob": -0.20864129573740858, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0016480606282129884}, {"id": 602, "seek": 242404, "start": 2447.84, "end": 2450.84, "text": " experiments, you know, cause nobody since we've actually published anything about", "tokens": [51554, 12050, 11, 291, 458, 11, 3082, 5079, 1670, 321, 600, 767, 6572, 1340, 466, 51704], "temperature": 0.0, "avg_logprob": -0.20864129573740858, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0016480606282129884}, {"id": 603, "seek": 245084, "start": 2450.84, "end": 2453.96, "text": " this, um, well, not quite true.", "tokens": [50364, 341, 11, 1105, 11, 731, 11, 406, 1596, 2074, 13, 50520], "temperature": 0.0, "avg_logprob": -0.15418545405069986, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0012842129217460752}, {"id": 604, "seek": 245084, "start": 2453.96, "end": 2456.48, "text": " Some people have published things, but nobody ever actually stepped back and", "tokens": [50520, 2188, 561, 362, 6572, 721, 11, 457, 5079, 1562, 767, 15251, 646, 293, 50646], "temperature": 0.0, "avg_logprob": -0.15418545405069986, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0012842129217460752}, {"id": 605, "seek": 245084, "start": 2456.48, "end": 2459.84, "text": " said like, what the hell, you know, how can this be possible?", "tokens": [50646, 848, 411, 11, 437, 264, 4921, 11, 291, 458, 11, 577, 393, 341, 312, 1944, 30, 50814], "temperature": 0.0, "avg_logprob": -0.15418545405069986, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0012842129217460752}, {"id": 606, "seek": 245084, "start": 2459.84, "end": 2460.6000000000004, "text": " Is it possible?", "tokens": [50814, 1119, 309, 1944, 30, 50852], "temperature": 0.0, "avg_logprob": -0.15418545405069986, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0012842129217460752}, {"id": 607, "seek": 245084, "start": 2460.6000000000004, "end": 2461.48, "text": " Is this what's happening?", "tokens": [50852, 1119, 341, 437, 311, 2737, 30, 50896], "temperature": 0.0, "avg_logprob": -0.15418545405069986, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0012842129217460752}, {"id": 608, "seek": 245084, "start": 2461.84, "end": 2465.52, "text": " And so yeah, we created a bunch of experiments where we basically predicted", "tokens": [50914, 400, 370, 1338, 11, 321, 2942, 257, 3840, 295, 12050, 689, 321, 1936, 19147, 51098], "temperature": 0.0, "avg_logprob": -0.15418545405069986, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0012842129217460752}, {"id": 609, "seek": 245084, "start": 2465.52, "end": 2466.08, "text": " ahead of time.", "tokens": [51098, 2286, 295, 565, 13, 51126], "temperature": 0.0, "avg_logprob": -0.15418545405069986, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0012842129217460752}, {"id": 610, "seek": 245084, "start": 2466.08, "end": 2468.96, "text": " It's like, okay, if this hypothesis is correct, that it's memorized in the", "tokens": [51126, 467, 311, 411, 11, 1392, 11, 498, 341, 17291, 307, 3006, 11, 300, 309, 311, 46677, 294, 264, 51270], "temperature": 0.0, "avg_logprob": -0.15418545405069986, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0012842129217460752}, {"id": 611, "seek": 245084, "start": 2468.96, "end": 2473.0, "text": " training set, then we ought to see blah under conditions, blah, but not only these", "tokens": [51270, 3097, 992, 11, 550, 321, 13416, 281, 536, 12288, 833, 4487, 11, 12288, 11, 457, 406, 787, 613, 51472], "temperature": 0.0, "avg_logprob": -0.15418545405069986, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0012842129217460752}, {"id": 612, "seek": 245084, "start": 2473.0, "end": 2473.6000000000004, "text": " conditions.", "tokens": [51472, 4487, 13, 51502], "temperature": 0.0, "avg_logprob": -0.15418545405069986, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0012842129217460752}, {"id": 613, "seek": 245084, "start": 2474.2400000000002, "end": 2477.28, "text": " And so we ran a bunch of experiments and all of them supported the hypothesis", "tokens": [51534, 400, 370, 321, 5872, 257, 3840, 295, 12050, 293, 439, 295, 552, 8104, 264, 17291, 51686], "temperature": 0.0, "avg_logprob": -0.15418545405069986, "compression_ratio": 1.7547770700636942, "no_speech_prob": 0.0012842129217460752}, {"id": 614, "seek": 247728, "start": 2478.28, "end": 2481.5600000000004, "text": " that it was memorizing the data set in a single thing at once.", "tokens": [50414, 300, 309, 390, 10560, 3319, 264, 1412, 992, 294, 257, 2167, 551, 412, 1564, 13, 50578], "temperature": 0.0, "avg_logprob": -0.15058547479135018, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0033761279191821814}, {"id": 615, "seek": 247728, "start": 2482.1200000000003, "end": 2490.6000000000004, "text": " Um, and it's a pretty big data set, you know, um, which in hindsight, it's not", "tokens": [50606, 3301, 11, 293, 309, 311, 257, 1238, 955, 1412, 992, 11, 291, 458, 11, 1105, 11, 597, 294, 44357, 11, 309, 311, 406, 51030], "temperature": 0.0, "avg_logprob": -0.15058547479135018, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0033761279191821814}, {"id": 616, "seek": 247728, "start": 2490.6000000000004, "end": 2495.2400000000002, "text": " totally surprising because the theory, remember, of the ULM fit theory was like", "tokens": [51030, 3879, 8830, 570, 264, 5261, 11, 1604, 11, 295, 264, 624, 43, 44, 3318, 5261, 390, 411, 51262], "temperature": 0.0, "avg_logprob": -0.15058547479135018, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0033761279191821814}, {"id": 617, "seek": 247728, "start": 2495.2400000000002, "end": 2500.2400000000002, "text": " what's kind of creating all these latent capabilities to make it easier for it to", "tokens": [51262, 437, 311, 733, 295, 4084, 439, 613, 48994, 10862, 281, 652, 309, 3571, 337, 309, 281, 51512], "temperature": 0.0, "avg_logprob": -0.15058547479135018, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0033761279191821814}, {"id": 618, "seek": 247728, "start": 2500.2400000000002, "end": 2501.36, "text": " predict the next token.", "tokens": [51512, 6069, 264, 958, 14862, 13, 51568], "temperature": 0.0, "avg_logprob": -0.15058547479135018, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0033761279191821814}, {"id": 619, "seek": 247728, "start": 2501.88, "end": 2506.32, "text": " So if it's got all this kind of latent capability, it ought to also be really", "tokens": [51594, 407, 498, 309, 311, 658, 439, 341, 733, 295, 48994, 13759, 11, 309, 13416, 281, 611, 312, 534, 51816], "temperature": 0.0, "avg_logprob": -0.15058547479135018, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0033761279191821814}, {"id": 620, "seek": 250632, "start": 2506.4, "end": 2511.0800000000004, "text": " good at compressing new tokens because it can immediately recognize that it's", "tokens": [50368, 665, 412, 14778, 278, 777, 22667, 570, 309, 393, 4258, 5521, 300, 309, 311, 50602], "temperature": 0.0, "avg_logprob": -0.1495871778394355, "compression_ratio": 1.712, "no_speech_prob": 0.004466969985514879}, {"id": 621, "seek": 250632, "start": 2511.0800000000004, "end": 2512.6800000000003, "text": " like, oh, that's just a version of this.", "tokens": [50602, 411, 11, 1954, 11, 300, 311, 445, 257, 3037, 295, 341, 13, 50682], "temperature": 0.0, "avg_logprob": -0.1495871778394355, "compression_ratio": 1.712, "no_speech_prob": 0.004466969985514879}, {"id": 622, "seek": 250632, "start": 2513.92, "end": 2521.88, "text": " So it's, it's not so crazy, you know, but it is, um, it requires us to rethink", "tokens": [50744, 407, 309, 311, 11, 309, 311, 406, 370, 3219, 11, 291, 458, 11, 457, 309, 307, 11, 1105, 11, 309, 7029, 505, 281, 34595, 51142], "temperature": 0.0, "avg_logprob": -0.1495871778394355, "compression_ratio": 1.712, "no_speech_prob": 0.004466969985514879}, {"id": 623, "seek": 250632, "start": 2521.88, "end": 2526.76, "text": " everything because like, and nobody knows like, okay, so how do we fine tune", "tokens": [51142, 1203, 570, 411, 11, 293, 5079, 3255, 411, 11, 1392, 11, 370, 577, 360, 321, 2489, 10864, 51386], "temperature": 0.0, "avg_logprob": -0.1495871778394355, "compression_ratio": 1.712, "no_speech_prob": 0.004466969985514879}, {"id": 624, "seek": 250632, "start": 2526.76, "end": 2527.32, "text": " these things?", "tokens": [51386, 613, 721, 30, 51414], "temperature": 0.0, "avg_logprob": -0.1495871778394355, "compression_ratio": 1.712, "no_speech_prob": 0.004466969985514879}, {"id": 625, "seek": 250632, "start": 2527.32, "end": 2529.56, "text": " Because like, it doesn't even matter.", "tokens": [51414, 1436, 411, 11, 309, 1177, 380, 754, 1871, 13, 51526], "temperature": 0.0, "avg_logprob": -0.1495871778394355, "compression_ratio": 1.712, "no_speech_prob": 0.004466969985514879}, {"id": 626, "seek": 250632, "start": 2530.2400000000002, "end": 2531.32, "text": " Like maybe it's fine.", "tokens": [51560, 1743, 1310, 309, 311, 2489, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1495871778394355, "compression_ratio": 1.712, "no_speech_prob": 0.004466969985514879}, {"id": 627, "seek": 250632, "start": 2531.6800000000003, "end": 2534.76, "text": " Like maybe it's fine that it's memorized the data set after one go and you do a", "tokens": [51632, 1743, 1310, 309, 311, 2489, 300, 309, 311, 46677, 264, 1412, 992, 934, 472, 352, 293, 291, 360, 257, 51786], "temperature": 0.0, "avg_logprob": -0.1495871778394355, "compression_ratio": 1.712, "no_speech_prob": 0.004466969985514879}, {"id": 628, "seek": 253476, "start": 2534.76, "end": 2540.2000000000003, "text": " second go and okay, the validation loss is terrible because it's now really", "tokens": [50364, 1150, 352, 293, 1392, 11, 264, 24071, 4470, 307, 6237, 570, 309, 311, 586, 534, 50636], "temperature": 0.0, "avg_logprob": -0.18919922510782877, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0012064581969752908}, {"id": 629, "seek": 253476, "start": 2540.2000000000003, "end": 2541.0400000000004, "text": " overconfident.", "tokens": [50636, 670, 24697, 1078, 13, 50678], "temperature": 0.0, "avg_logprob": -0.18919922510782877, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0012064581969752908}, {"id": 630, "seek": 253476, "start": 2541.92, "end": 2542.48, "text": " That's fine.", "tokens": [50722, 663, 311, 2489, 13, 50750], "temperature": 0.0, "avg_logprob": -0.18919922510782877, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0012064581969752908}, {"id": 631, "seek": 253476, "start": 2542.6000000000004, "end": 2545.96, "text": " Don't, you know, don't keep telling people, don't track validation loss,", "tokens": [50756, 1468, 380, 11, 291, 458, 11, 500, 380, 1066, 3585, 561, 11, 500, 380, 2837, 24071, 4470, 11, 50924], "temperature": 0.0, "avg_logprob": -0.18919922510782877, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0012064581969752908}, {"id": 632, "seek": 253476, "start": 2545.96, "end": 2549.92, "text": " track validation accuracy, um, because at least that, that will still be useful.", "tokens": [50924, 2837, 24071, 14170, 11, 1105, 11, 570, 412, 1935, 300, 11, 300, 486, 920, 312, 4420, 13, 51122], "temperature": 0.0, "avg_logprob": -0.18919922510782877, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0012064581969752908}, {"id": 633, "seek": 253476, "start": 2550.5200000000004, "end": 2554.4, "text": " Um, there's another thing that's got lost since ULM fit, nobody tracks accuracy", "tokens": [51152, 3301, 11, 456, 311, 1071, 551, 300, 311, 658, 2731, 1670, 624, 43, 44, 3318, 11, 5079, 10218, 14170, 51346], "temperature": 0.0, "avg_logprob": -0.18919922510782877, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0012064581969752908}, {"id": 634, "seek": 253476, "start": 2554.4, "end": 2555.6000000000004, "text": " of language models anymore.", "tokens": [51346, 295, 2856, 5245, 3602, 13, 51406], "temperature": 0.0, "avg_logprob": -0.18919922510782877, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0012064581969752908}, {"id": 635, "seek": 253476, "start": 2556.4, "end": 2561.0, "text": " Um, but you know, it'll still keep learning and it does, it does keep improving.", "tokens": [51446, 3301, 11, 457, 291, 458, 11, 309, 603, 920, 1066, 2539, 293, 309, 775, 11, 309, 775, 1066, 11470, 13, 51676], "temperature": 0.0, "avg_logprob": -0.18919922510782877, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0012064581969752908}, {"id": 636, "seek": 256100, "start": 2561.24, "end": 2563.88, "text": " But is it worse?", "tokens": [50376, 583, 307, 309, 5324, 30, 50508], "temperature": 0.0, "avg_logprob": -0.13771987980247563, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.004467837978154421}, {"id": 637, "seek": 256100, "start": 2564.24, "end": 2568.32, "text": " You know, like, is it like now that it's kind of memorized it, it's probably", "tokens": [50526, 509, 458, 11, 411, 11, 307, 309, 411, 586, 300, 309, 311, 733, 295, 46677, 309, 11, 309, 311, 1391, 50730], "temperature": 0.0, "avg_logprob": -0.13771987980247563, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.004467837978154421}, {"id": 638, "seek": 256100, "start": 2568.32, "end": 2573.96, "text": " getting a less strong signal, you know, um, I don't know.", "tokens": [50730, 1242, 257, 1570, 2068, 6358, 11, 291, 458, 11, 1105, 11, 286, 500, 380, 458, 13, 51012], "temperature": 0.0, "avg_logprob": -0.13771987980247563, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.004467837978154421}, {"id": 639, "seek": 256100, "start": 2574.2, "end": 2576.72, "text": " So I still don't know how to fine tune language models properly.", "tokens": [51024, 407, 286, 920, 500, 380, 458, 577, 281, 2489, 10864, 2856, 5245, 6108, 13, 51150], "temperature": 0.0, "avg_logprob": -0.13771987980247563, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.004467837978154421}, {"id": 640, "seek": 256100, "start": 2576.72, "end": 2580.76, "text": " And I haven't found anybody who feels like they do, like nobody really knows", "tokens": [51150, 400, 286, 2378, 380, 1352, 4472, 567, 3417, 411, 436, 360, 11, 411, 5079, 534, 3255, 51352], "temperature": 0.0, "avg_logprob": -0.13771987980247563, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.004467837978154421}, {"id": 641, "seek": 256100, "start": 2580.76, "end": 2585.76, "text": " whether this memorization thing is, it's probably a feature in some ways.", "tokens": [51352, 1968, 341, 10560, 2144, 551, 307, 11, 309, 311, 1391, 257, 4111, 294, 512, 2098, 13, 51602], "temperature": 0.0, "avg_logprob": -0.13771987980247563, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.004467837978154421}, {"id": 642, "seek": 256100, "start": 2585.76, "end": 2587.76, "text": " There's probably some things that you can do usefully with it.", "tokens": [51602, 821, 311, 1391, 512, 721, 300, 291, 393, 360, 764, 2277, 365, 309, 13, 51702], "temperature": 0.0, "avg_logprob": -0.13771987980247563, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.004467837978154421}, {"id": 643, "seek": 258776, "start": 2587.92, "end": 2593.5600000000004, "text": " It's probably, yeah, I have a feeling it's messing up training dynamics as well.", "tokens": [50372, 467, 311, 1391, 11, 1338, 11, 286, 362, 257, 2633, 309, 311, 23258, 493, 3097, 15679, 382, 731, 13, 50654], "temperature": 0.0, "avg_logprob": -0.2214156922839937, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.0040694717317819595}, {"id": 644, "seek": 258776, "start": 2594.36, "end": 2597.48, "text": " It doesn't come at the cost of catastrophic forgetting as well, right?", "tokens": [50694, 467, 1177, 380, 808, 412, 264, 2063, 295, 34915, 25428, 382, 731, 11, 558, 30, 50850], "temperature": 0.0, "avg_logprob": -0.2214156922839937, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.0040694717317819595}, {"id": 645, "seek": 258776, "start": 2597.48, "end": 2598.92, "text": " Like, which is the other side of the coin.", "tokens": [50850, 1743, 11, 597, 307, 264, 661, 1252, 295, 264, 11464, 13, 50922], "temperature": 0.0, "avg_logprob": -0.2214156922839937, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.0040694717317819595}, {"id": 646, "seek": 258776, "start": 2599.28, "end": 2605.6400000000003, "text": " Um, it does, um, to some extent, like, we know it does like look at CodeLama,", "tokens": [50940, 3301, 11, 309, 775, 11, 1105, 11, 281, 512, 8396, 11, 411, 11, 321, 458, 309, 775, 411, 574, 412, 15549, 43, 2404, 11, 51258], "temperature": 0.0, "avg_logprob": -0.2214156922839937, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.0040694717317819595}, {"id": 647, "seek": 258776, "start": 2605.6400000000003, "end": 2606.28, "text": " for example.", "tokens": [51258, 337, 1365, 13, 51290], "temperature": 0.0, "avg_logprob": -0.2214156922839937, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.0040694717317819595}, {"id": 648, "seek": 258776, "start": 2606.28, "end": 2611.7200000000003, "text": " So CodeLama was a, I think it was like a 500 billion token fine tuning of", "tokens": [51290, 407, 15549, 43, 2404, 390, 257, 11, 286, 519, 309, 390, 411, 257, 5923, 5218, 14862, 2489, 15164, 295, 51562], "temperature": 0.0, "avg_logprob": -0.2214156922839937, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.0040694717317819595}, {"id": 649, "seek": 261172, "start": 2611.72, "end": 2616.7999999999997, "text": " CodeLama to using code and also pros about code that Meta did.", "tokens": [50364, 15549, 43, 2404, 281, 1228, 3089, 293, 611, 6267, 466, 3089, 300, 6377, 64, 630, 13, 50618], "temperature": 0.0, "avg_logprob": -0.2701347648323356, "compression_ratio": 1.7194719471947195, "no_speech_prob": 0.0558072105050087}, {"id": 650, "seek": 261172, "start": 2617.4399999999996, "end": 2623.04, "text": " And, um, honestly, they kind of blew it because CodeLama is good at coding,", "tokens": [50650, 400, 11, 1105, 11, 6095, 11, 436, 733, 295, 19075, 309, 570, 15549, 43, 2404, 307, 665, 412, 17720, 11, 50930], "temperature": 0.0, "avg_logprob": -0.2701347648323356, "compression_ratio": 1.7194719471947195, "no_speech_prob": 0.0558072105050087}, {"id": 651, "seek": 261172, "start": 2623.04, "end": 2624.16, "text": " but it's bad at everything else.", "tokens": [50930, 457, 309, 311, 1578, 412, 1203, 1646, 13, 50986], "temperature": 0.0, "avg_logprob": -0.2701347648323356, "compression_ratio": 1.7194719471947195, "no_speech_prob": 0.0558072105050087}, {"id": 652, "seek": 261172, "start": 2624.68, "end": 2625.72, "text": " You know, and it used to be good.", "tokens": [51012, 509, 458, 11, 293, 309, 1143, 281, 312, 665, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2701347648323356, "compression_ratio": 1.7194719471947195, "no_speech_prob": 0.0558072105050087}, {"id": 653, "seek": 261172, "start": 2626.12, "end": 2626.3999999999996, "text": " Yeah.", "tokens": [51084, 865, 13, 51098], "temperature": 0.0, "avg_logprob": -0.2701347648323356, "compression_ratio": 1.7194719471947195, "no_speech_prob": 0.0558072105050087}, {"id": 654, "seek": 261172, "start": 2626.3999999999996, "end": 2630.3199999999997, "text": " I was pretty sure it was like, before they released it at me and lots of people", "tokens": [51098, 286, 390, 1238, 988, 309, 390, 411, 11, 949, 436, 4736, 309, 412, 385, 293, 3195, 295, 561, 51294], "temperature": 0.0, "avg_logprob": -0.2701347648323356, "compression_ratio": 1.7194719471947195, "no_speech_prob": 0.0558072105050087}, {"id": 655, "seek": 261172, "start": 2630.3199999999997, "end": 2633.24, "text": " in the open source discords were like, Oh my God, you know, we know this is", "tokens": [51294, 294, 264, 1269, 4009, 2983, 5703, 645, 411, 11, 876, 452, 1265, 11, 291, 458, 11, 321, 458, 341, 307, 51440], "temperature": 0.0, "avg_logprob": -0.2701347648323356, "compression_ratio": 1.7194719471947195, "no_speech_prob": 0.0558072105050087}, {"id": 656, "seek": 261172, "start": 2633.24, "end": 2633.6, "text": " coming.", "tokens": [51440, 1348, 13, 51458], "temperature": 0.0, "avg_logprob": -0.2701347648323356, "compression_ratio": 1.7194719471947195, "no_speech_prob": 0.0558072105050087}, {"id": 657, "seek": 261172, "start": 2633.6, "end": 2634.7999999999997, "text": " You're not going to say it's coming.", "tokens": [51458, 509, 434, 406, 516, 281, 584, 309, 311, 1348, 13, 51518], "temperature": 0.0, "avg_logprob": -0.2701347648323356, "compression_ratio": 1.7194719471947195, "no_speech_prob": 0.0558072105050087}, {"id": 658, "seek": 261172, "start": 2634.7999999999997, "end": 2638.7999999999997, "text": " I, I hope they kept at least like 50% long code data because otherwise", "tokens": [51518, 286, 11, 286, 1454, 436, 4305, 412, 1935, 411, 2625, 4, 938, 3089, 1412, 570, 5911, 51718], "temperature": 0.0, "avg_logprob": -0.2701347648323356, "compression_ratio": 1.7194719471947195, "no_speech_prob": 0.0558072105050087}, {"id": 659, "seek": 261172, "start": 2638.7999999999997, "end": 2639.9599999999996, "text": " it's going to forget everything else.", "tokens": [51718, 309, 311, 516, 281, 2870, 1203, 1646, 13, 51776], "temperature": 0.0, "avg_logprob": -0.2701347648323356, "compression_ratio": 1.7194719471947195, "no_speech_prob": 0.0558072105050087}, {"id": 660, "seek": 263996, "start": 2640.44, "end": 2647.0, "text": " And they didn't only like 0.3 of their 0.3% of their epochs were non-code data.", "tokens": [50388, 400, 436, 994, 380, 787, 411, 1958, 13, 18, 295, 641, 1958, 13, 18, 4, 295, 641, 30992, 28346, 645, 2107, 12, 22332, 1412, 13, 50716], "temperature": 0.0, "avg_logprob": -0.16796280907802894, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0008293211576528847}, {"id": 661, "seek": 263996, "start": 2647.4, "end": 2648.84, "text": " So I did it, forgot everything else.", "tokens": [50736, 407, 286, 630, 309, 11, 5298, 1203, 1646, 13, 50808], "temperature": 0.0, "avg_logprob": -0.16796280907802894, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0008293211576528847}, {"id": 662, "seek": 263996, "start": 2648.88, "end": 2652.04, "text": " So now it's good at code and it's bad at everything else.", "tokens": [50810, 407, 586, 309, 311, 665, 412, 3089, 293, 309, 311, 1578, 412, 1203, 1646, 13, 50968], "temperature": 0.0, "avg_logprob": -0.16796280907802894, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0008293211576528847}, {"id": 663, "seek": 263996, "start": 2652.84, "end": 2654.64, "text": " So we definitely have catastrophic forgetting.", "tokens": [51008, 407, 321, 2138, 362, 34915, 25428, 13, 51098], "temperature": 0.0, "avg_logprob": -0.16796280907802894, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0008293211576528847}, {"id": 664, "seek": 263996, "start": 2654.64, "end": 2655.48, "text": " It's fixable.", "tokens": [51098, 467, 311, 3191, 712, 13, 51140], "temperature": 0.0, "avg_logprob": -0.16796280907802894, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0008293211576528847}, {"id": 665, "seek": 263996, "start": 2655.52, "end": 2660.88, "text": " Just somebody has to do, you know, somebody, somebody has to spend their time", "tokens": [51142, 1449, 2618, 575, 281, 360, 11, 291, 458, 11, 2618, 11, 2618, 575, 281, 3496, 641, 565, 51410], "temperature": 0.0, "avg_logprob": -0.16796280907802894, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0008293211576528847}, {"id": 666, "seek": 263996, "start": 2660.88, "end": 2664.28, "text": " training a model on a, a good mix of data.", "tokens": [51410, 3097, 257, 2316, 322, 257, 11, 257, 665, 2890, 295, 1412, 13, 51580], "temperature": 0.0, "avg_logprob": -0.16796280907802894, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0008293211576528847}, {"id": 667, "seek": 263996, "start": 2664.36, "end": 2665.32, "text": " Like, so, okay.", "tokens": [51584, 1743, 11, 370, 11, 1392, 13, 51632], "temperature": 0.0, "avg_logprob": -0.16796280907802894, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0008293211576528847}, {"id": 668, "seek": 263996, "start": 2665.32, "end": 2665.96, "text": " So here's the thing.", "tokens": [51632, 407, 510, 311, 264, 551, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16796280907802894, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0008293211576528847}, {"id": 669, "seek": 266596, "start": 2666.96, "end": 2672.84, "text": " Um, even though I originally created the three step approach that everybody", "tokens": [50414, 3301, 11, 754, 1673, 286, 7993, 2942, 264, 1045, 1823, 3109, 300, 2201, 50708], "temperature": 0.0, "avg_logprob": -0.17713685989379882, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0008557713590562344}, {"id": 670, "seek": 266596, "start": 2672.84, "end": 2676.56, "text": " now does, my view is it's actually wrong and we shouldn't use it.", "tokens": [50708, 586, 775, 11, 452, 1910, 307, 309, 311, 767, 2085, 293, 321, 4659, 380, 764, 309, 13, 50894], "temperature": 0.0, "avg_logprob": -0.17713685989379882, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0008557713590562344}, {"id": 671, "seek": 266596, "start": 2677.0, "end": 2685.52, "text": " Um, um, and that's because people are using it in a way different to why I", "tokens": [50916, 3301, 11, 1105, 11, 293, 300, 311, 570, 561, 366, 1228, 309, 294, 257, 636, 819, 281, 983, 286, 51342], "temperature": 0.0, "avg_logprob": -0.17713685989379882, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0008557713590562344}, {"id": 672, "seek": 266596, "start": 2685.52, "end": 2686.04, "text": " created it.", "tokens": [51342, 2942, 309, 13, 51368], "temperature": 0.0, "avg_logprob": -0.17713685989379882, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0008557713590562344}, {"id": 673, "seek": 266596, "start": 2686.04, "end": 2690.56, "text": " You know, I created it thinking that the tasks specific models would be more", "tokens": [51368, 509, 458, 11, 286, 2942, 309, 1953, 300, 264, 9608, 2685, 5245, 576, 312, 544, 51594], "temperature": 0.0, "avg_logprob": -0.17713685989379882, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0008557713590562344}, {"id": 674, "seek": 266596, "start": 2690.56, "end": 2694.76, "text": " specific, you know, it's like, Oh, this is like a sentiment classifier.", "tokens": [51594, 2685, 11, 291, 458, 11, 309, 311, 411, 11, 876, 11, 341, 307, 411, 257, 16149, 1508, 9902, 13, 51804], "temperature": 0.0, "avg_logprob": -0.17713685989379882, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0008557713590562344}, {"id": 675, "seek": 269476, "start": 2694.88, "end": 2700.96, "text": " This is an example of a task, you know, but the tasks now are like a, um, you", "tokens": [50370, 639, 307, 364, 1365, 295, 257, 5633, 11, 291, 458, 11, 457, 264, 9608, 586, 366, 411, 257, 11, 1105, 11, 291, 50674], "temperature": 0.0, "avg_logprob": -0.16741442473038384, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.004069287329912186}, {"id": 676, "seek": 269476, "start": 2700.96, "end": 2704.0400000000004, "text": " know, RLHF, which is basically like answer questions that make people feel", "tokens": [50674, 458, 11, 497, 43, 39, 37, 11, 597, 307, 1936, 411, 1867, 1651, 300, 652, 561, 841, 50828], "temperature": 0.0, "avg_logprob": -0.16741442473038384, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.004069287329912186}, {"id": 677, "seek": 269476, "start": 2704.0400000000004, "end": 2705.0400000000004, "text": " happy about your answer.", "tokens": [50828, 2055, 466, 428, 1867, 13, 50878], "temperature": 0.0, "avg_logprob": -0.16741442473038384, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.004069287329912186}, {"id": 678, "seek": 269476, "start": 2705.32, "end": 2709.28, "text": " So that's a much more general task and it's a really cool approach.", "tokens": [50892, 407, 300, 311, 257, 709, 544, 2674, 5633, 293, 309, 311, 257, 534, 1627, 3109, 13, 51090], "temperature": 0.0, "avg_logprob": -0.16741442473038384, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.004069287329912186}, {"id": 679, "seek": 269476, "start": 2709.5200000000004, "end": 2714.88, "text": " And so we see, for example, RLHF also breaks models.", "tokens": [51102, 400, 370, 321, 536, 11, 337, 1365, 11, 497, 43, 39, 37, 611, 9857, 5245, 13, 51370], "temperature": 0.0, "avg_logprob": -0.16741442473038384, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.004069287329912186}, {"id": 680, "seek": 269476, "start": 2714.88, "end": 2720.88, "text": " Like, you know, like GPT for RLHDEFT, we know from kind of the, the work that", "tokens": [51370, 1743, 11, 291, 458, 11, 411, 26039, 51, 337, 497, 43, 39, 22296, 25469, 11, 321, 458, 490, 733, 295, 264, 11, 264, 589, 300, 51670], "temperature": 0.0, "avg_logprob": -0.16741442473038384, "compression_ratio": 1.606837606837607, "no_speech_prob": 0.004069287329912186}, {"id": 681, "seek": 272088, "start": 2720.88, "end": 2725.7200000000003, "text": " Microsoft did, you know, the pre, the, the earlier less aligned version was better.", "tokens": [50364, 8116, 630, 11, 291, 458, 11, 264, 659, 11, 264, 11, 264, 3071, 1570, 17962, 3037, 390, 1101, 13, 50606], "temperature": 0.0, "avg_logprob": -0.1763230373984889, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0011334853479638696}, {"id": 682, "seek": 272088, "start": 2726.92, "end": 2729.84, "text": " Um, and these are all kind of examples of catastrophic forgetting.", "tokens": [50666, 3301, 11, 293, 613, 366, 439, 733, 295, 5110, 295, 34915, 25428, 13, 50812], "temperature": 0.0, "avg_logprob": -0.1763230373984889, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0011334853479638696}, {"id": 683, "seek": 272088, "start": 2730.2000000000003, "end": 2736.96, "text": " And so to me, the right way to do this is to fine-tune language models is to", "tokens": [50830, 400, 370, 281, 385, 11, 264, 558, 636, 281, 360, 341, 307, 281, 2489, 12, 83, 2613, 2856, 5245, 307, 281, 51168], "temperature": 0.0, "avg_logprob": -0.1763230373984889, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0011334853479638696}, {"id": 684, "seek": 272088, "start": 2736.96, "end": 2738.7200000000003, "text": " actually throw away the idea of fine-tuning.", "tokens": [51168, 767, 3507, 1314, 264, 1558, 295, 2489, 12, 83, 37726, 13, 51256], "temperature": 0.0, "avg_logprob": -0.1763230373984889, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0011334853479638696}, {"id": 685, "seek": 272088, "start": 2738.88, "end": 2739.6400000000003, "text": " There's no such thing.", "tokens": [51264, 821, 311, 572, 1270, 551, 13, 51302], "temperature": 0.0, "avg_logprob": -0.1763230373984889, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0011334853479638696}, {"id": 686, "seek": 272088, "start": 2740.36, "end": 2741.8, "text": " There's only continued pre-training.", "tokens": [51338, 821, 311, 787, 7014, 659, 12, 17227, 1760, 13, 51410], "temperature": 0.0, "avg_logprob": -0.1763230373984889, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0011334853479638696}, {"id": 687, "seek": 272088, "start": 2742.6400000000003, "end": 2747.28, "text": " Uh, and pre-training is something where from the very start, you try to include", "tokens": [51452, 4019, 11, 293, 659, 12, 17227, 1760, 307, 746, 689, 490, 264, 588, 722, 11, 291, 853, 281, 4090, 51684], "temperature": 0.0, "avg_logprob": -0.1763230373984889, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0011334853479638696}, {"id": 688, "seek": 274728, "start": 2747.28, "end": 2751.1200000000003, "text": " all the kinds of data that you care about, all the kinds of problems that you", "tokens": [50364, 439, 264, 3685, 295, 1412, 300, 291, 1127, 466, 11, 439, 264, 3685, 295, 2740, 300, 291, 50556], "temperature": 0.0, "avg_logprob": -0.14342267172677176, "compression_ratio": 1.8028169014084507, "no_speech_prob": 0.0011334816226735711}, {"id": 689, "seek": 274728, "start": 2751.1200000000003, "end": 2757.76, "text": " care about, instructions, exercises, code, general purpose document completion,", "tokens": [50556, 1127, 466, 11, 9415, 11, 11900, 11, 3089, 11, 2674, 4334, 4166, 19372, 11, 50888], "temperature": 0.0, "avg_logprob": -0.14342267172677176, "compression_ratio": 1.8028169014084507, "no_speech_prob": 0.0011334816226735711}, {"id": 690, "seek": 274728, "start": 2758.76, "end": 2759.2000000000003, "text": " whatever.", "tokens": [50938, 2035, 13, 50960], "temperature": 0.0, "avg_logprob": -0.14342267172677176, "compression_ratio": 1.8028169014084507, "no_speech_prob": 0.0011334816226735711}, {"id": 691, "seek": 274728, "start": 2761.0400000000004, "end": 2766.44, "text": " And then as you train, you gradually curate that, you know, you gradually", "tokens": [51052, 400, 550, 382, 291, 3847, 11, 291, 13145, 1262, 473, 300, 11, 291, 458, 11, 291, 13145, 51322], "temperature": 0.0, "avg_logprob": -0.14342267172677176, "compression_ratio": 1.8028169014084507, "no_speech_prob": 0.0011334816226735711}, {"id": 692, "seek": 274728, "start": 2766.44, "end": 2769.96, "text": " make that higher and higher quality and more and more specific to the kinds of", "tokens": [51322, 652, 300, 2946, 293, 2946, 3125, 293, 544, 293, 544, 2685, 281, 264, 3685, 295, 51498], "temperature": 0.0, "avg_logprob": -0.14342267172677176, "compression_ratio": 1.8028169014084507, "no_speech_prob": 0.0011334816226735711}, {"id": 693, "seek": 274728, "start": 2769.96, "end": 2770.84, "text": " tasks you want it to do.", "tokens": [51498, 9608, 291, 528, 309, 281, 360, 13, 51542], "temperature": 0.0, "avg_logprob": -0.14342267172677176, "compression_ratio": 1.8028169014084507, "no_speech_prob": 0.0011334816226735711}, {"id": 694, "seek": 274728, "start": 2772.0800000000004, "end": 2774.6000000000004, "text": " Um, but you never throw away any data.", "tokens": [51604, 3301, 11, 457, 291, 1128, 3507, 1314, 604, 1412, 13, 51730], "temperature": 0.0, "avg_logprob": -0.14342267172677176, "compression_ratio": 1.8028169014084507, "no_speech_prob": 0.0011334816226735711}, {"id": 695, "seek": 277460, "start": 2774.8399999999997, "end": 2780.16, "text": " You always keep all of the data types there in reasonably high quantities.", "tokens": [50376, 509, 1009, 1066, 439, 295, 264, 1412, 3467, 456, 294, 23551, 1090, 22927, 13, 50642], "temperature": 0.0, "avg_logprob": -0.15777820807236892, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.0013666512677446008}, {"id": 696, "seek": 277460, "start": 2780.64, "end": 2785.72, "text": " Um, you know, maybe the quality filter, you stop training on low-quality data.", "tokens": [50666, 3301, 11, 291, 458, 11, 1310, 264, 3125, 6608, 11, 291, 1590, 3097, 322, 2295, 12, 11286, 1412, 13, 50920], "temperature": 0.0, "avg_logprob": -0.15777820807236892, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.0013666512677446008}, {"id": 697, "seek": 277460, "start": 2786.24, "end": 2789.12, "text": " Cause that's probably fine to forget how to write badly, maybe.", "tokens": [50946, 10865, 300, 311, 1391, 2489, 281, 2870, 577, 281, 2464, 13425, 11, 1310, 13, 51090], "temperature": 0.0, "avg_logprob": -0.15777820807236892, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.0013666512677446008}, {"id": 698, "seek": 277460, "start": 2789.8399999999997, "end": 2795.56, "text": " Um, so yeah, that's now my view is I think ULM fit is the wrong approach.", "tokens": [51126, 3301, 11, 370, 1338, 11, 300, 311, 586, 452, 1910, 307, 286, 519, 624, 43, 44, 3318, 307, 264, 2085, 3109, 13, 51412], "temperature": 0.0, "avg_logprob": -0.15777820807236892, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.0013666512677446008}, {"id": 699, "seek": 277460, "start": 2796.08, "end": 2801.12, "text": " Um, and that's why we're seeing a lot of these, uh, you know, so-called alignment", "tokens": [51438, 3301, 11, 293, 300, 311, 983, 321, 434, 2577, 257, 688, 295, 613, 11, 2232, 11, 291, 458, 11, 370, 12, 11880, 18515, 51690], "temperature": 0.0, "avg_logprob": -0.15777820807236892, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.0013666512677446008}, {"id": 700, "seek": 280112, "start": 2801.12, "end": 2805.72, "text": " tax and this view of like, oh, a model can't both code and do other things.", "tokens": [50364, 3366, 293, 341, 1910, 295, 411, 11, 1954, 11, 257, 2316, 393, 380, 1293, 3089, 293, 360, 661, 721, 13, 50594], "temperature": 0.0, "avg_logprob": -0.2217815319697062, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004133293870836496}, {"id": 701, "seek": 280112, "start": 2805.8399999999997, "end": 2807.92, "text": " You know, I think it's actually cause people are training them wrong.", "tokens": [50600, 509, 458, 11, 286, 519, 309, 311, 767, 3082, 561, 366, 3097, 552, 2085, 13, 50704], "temperature": 0.0, "avg_logprob": -0.2217815319697062, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004133293870836496}, {"id": 702, "seek": 280112, "start": 2809.24, "end": 2813.72, "text": " Well, I think you have a clear anti-laziness approach.", "tokens": [50770, 1042, 11, 286, 519, 291, 362, 257, 1850, 6061, 12, 875, 89, 1324, 3109, 13, 50994], "temperature": 0.0, "avg_logprob": -0.2217815319697062, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004133293870836496}, {"id": 703, "seek": 280112, "start": 2813.8399999999997, "end": 2817.7599999999998, "text": " I think other people are not as, uh, good hearted, you know, they're like,", "tokens": [51000, 286, 519, 661, 561, 366, 406, 382, 11, 2232, 11, 665, 1917, 292, 11, 291, 458, 11, 436, 434, 411, 11, 51196], "temperature": 0.0, "avg_logprob": -0.2217815319697062, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004133293870836496}, {"id": 704, "seek": 280112, "start": 2817.7599999999998, "end": 2819.72, "text": " Hey, they told me this thing works.", "tokens": [51196, 1911, 11, 436, 1907, 385, 341, 551, 1985, 13, 51294], "temperature": 0.0, "avg_logprob": -0.2217815319697062, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004133293870836496}, {"id": 705, "seek": 280112, "start": 2819.88, "end": 2823.24, "text": " And if I release a model this way, people will appreciate it.", "tokens": [51302, 400, 498, 286, 4374, 257, 2316, 341, 636, 11, 561, 486, 4449, 309, 13, 51470], "temperature": 0.0, "avg_logprob": -0.2217815319697062, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004133293870836496}, {"id": 706, "seek": 280112, "start": 2823.24, "end": 2826.44, "text": " I'll get promoted and I'll kind of make, make more money.", "tokens": [51470, 286, 603, 483, 21162, 293, 286, 603, 733, 295, 652, 11, 652, 544, 1460, 13, 51630], "temperature": 0.0, "avg_logprob": -0.2217815319697062, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004133293870836496}, {"id": 707, "seek": 280112, "start": 2826.92, "end": 2827.7599999999998, "text": " Oh, absolutely.", "tokens": [51654, 876, 11, 3122, 13, 51696], "temperature": 0.0, "avg_logprob": -0.2217815319697062, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004133293870836496}, {"id": 708, "seek": 280112, "start": 2828.2799999999997, "end": 2828.6, "text": " Yeah.", "tokens": [51722, 865, 13, 51738], "temperature": 0.0, "avg_logprob": -0.2217815319697062, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004133293870836496}, {"id": 709, "seek": 280112, "start": 2828.6, "end": 2829.44, "text": " And it's not just money.", "tokens": [51738, 400, 309, 311, 406, 445, 1460, 13, 51780], "temperature": 0.0, "avg_logprob": -0.2217815319697062, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004133293870836496}, {"id": 710, "seek": 282944, "start": 2829.44, "end": 2833.44, "text": " It's like, this is how citations work most, most badly, you know, so if you", "tokens": [50364, 467, 311, 411, 11, 341, 307, 577, 4814, 763, 589, 881, 11, 881, 13425, 11, 291, 458, 11, 370, 498, 291, 50564], "temperature": 0.0, "avg_logprob": -0.15663723264421736, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.007340705022215843}, {"id": 711, "seek": 282944, "start": 2833.44, "end": 2837.56, "text": " want to get cited, you need to write a paper that people in your field recognize", "tokens": [50564, 528, 281, 483, 30134, 11, 291, 643, 281, 2464, 257, 3035, 300, 561, 294, 428, 2519, 5521, 50770], "temperature": 0.0, "avg_logprob": -0.15663723264421736, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.007340705022215843}, {"id": 712, "seek": 282944, "start": 2837.56, "end": 2841.64, "text": " as an advancement on things that we know are good.", "tokens": [50770, 382, 364, 35764, 322, 721, 300, 321, 458, 366, 665, 13, 50974], "temperature": 0.0, "avg_logprob": -0.15663723264421736, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.007340705022215843}, {"id": 713, "seek": 282944, "start": 2842.16, "end": 2844.32, "text": " And so we've seen this happen again and again.", "tokens": [51000, 400, 370, 321, 600, 1612, 341, 1051, 797, 293, 797, 13, 51108], "temperature": 0.0, "avg_logprob": -0.15663723264421736, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.007340705022215843}, {"id": 714, "seek": 282944, "start": 2844.4, "end": 2849.2000000000003, "text": " So like I say, like zero shot and a few shot learning, everybody was writing about", "tokens": [51112, 407, 411, 286, 584, 11, 411, 4018, 3347, 293, 257, 1326, 3347, 2539, 11, 2201, 390, 3579, 466, 51352], "temperature": 0.0, "avg_logprob": -0.15663723264421736, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.007340705022215843}, {"id": 715, "seek": 282944, "start": 2849.2000000000003, "end": 2853.36, "text": " that or, you know, with, um, image generation, every, everybody just was writing", "tokens": [51352, 300, 420, 11, 291, 458, 11, 365, 11, 1105, 11, 3256, 5125, 11, 633, 11, 2201, 445, 390, 3579, 51560], "temperature": 0.0, "avg_logprob": -0.15663723264421736, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.007340705022215843}, {"id": 716, "seek": 282944, "start": 2853.36, "end": 2857.88, "text": " about GANs, you know, and I was trying to say like, no, GANs are not the right approach.", "tokens": [51560, 466, 460, 1770, 82, 11, 291, 458, 11, 293, 286, 390, 1382, 281, 584, 411, 11, 572, 11, 460, 1770, 82, 366, 406, 264, 558, 3109, 13, 51786], "temperature": 0.0, "avg_logprob": -0.15663723264421736, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.007340705022215843}, {"id": 717, "seek": 285788, "start": 2857.88, "end": 2861.08, "text": " You know, when I showed again through research that we demonstrated in our", "tokens": [50364, 509, 458, 11, 562, 286, 4712, 797, 807, 2132, 300, 321, 18772, 294, 527, 50524], "temperature": 0.0, "avg_logprob": -0.17475427900041854, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.0013669482432305813}, {"id": 718, "seek": 285788, "start": 2861.08, "end": 2867.4, "text": " videos, that you can do better than GANs much faster and with much less data.", "tokens": [50524, 2145, 11, 300, 291, 393, 360, 1101, 813, 460, 1770, 82, 709, 4663, 293, 365, 709, 1570, 1412, 13, 50840], "temperature": 0.0, "avg_logprob": -0.17475427900041854, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.0013669482432305813}, {"id": 719, "seek": 285788, "start": 2868.32, "end": 2872.4, "text": " And nobody cared because again, like if you want to get published, you rewrite", "tokens": [50886, 400, 5079, 19779, 570, 797, 11, 411, 498, 291, 528, 281, 483, 6572, 11, 291, 28132, 51090], "temperature": 0.0, "avg_logprob": -0.17475427900041854, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.0013669482432305813}, {"id": 720, "seek": 285788, "start": 2872.4, "end": 2877.56, "text": " a GAN paper that slightly improves this part of GANs and this tiny field, you'll,", "tokens": [51090, 257, 460, 1770, 3035, 300, 4748, 24771, 341, 644, 295, 460, 1770, 82, 293, 341, 5870, 2519, 11, 291, 603, 11, 51348], "temperature": 0.0, "avg_logprob": -0.17475427900041854, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.0013669482432305813}, {"id": 721, "seek": 285788, "start": 2878.52, "end": 2883.6, "text": " you'll get published, you know, so it's, yeah, it's not set up for real innovation.", "tokens": [51396, 291, 603, 483, 6572, 11, 291, 458, 11, 370, 309, 311, 11, 1338, 11, 309, 311, 406, 992, 493, 337, 957, 8504, 13, 51650], "temperature": 0.0, "avg_logprob": -0.17475427900041854, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.0013669482432305813}, {"id": 722, "seek": 288360, "start": 2884.16, "end": 2892.12, "text": " Um, it's, you know, it's, again, it's really helpful for me, you know, have my own", "tokens": [50392, 3301, 11, 309, 311, 11, 291, 458, 11, 309, 311, 11, 797, 11, 309, 311, 534, 4961, 337, 385, 11, 291, 458, 11, 362, 452, 1065, 50790], "temperature": 0.0, "avg_logprob": -0.2101680725578248, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.002800525398924947}, {"id": 723, "seek": 288360, "start": 2892.12, "end": 2896.24, "text": " research lab with nobody telling me what to do and I don't even publish.", "tokens": [50790, 2132, 2715, 365, 5079, 3585, 385, 437, 281, 360, 293, 286, 500, 380, 754, 11374, 13, 50996], "temperature": 0.0, "avg_logprob": -0.2101680725578248, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.002800525398924947}, {"id": 724, "seek": 288360, "start": 2896.24, "end": 2898.04, "text": " So it doesn't matter if I get citations.", "tokens": [50996, 407, 309, 1177, 380, 1871, 498, 286, 483, 4814, 763, 13, 51086], "temperature": 0.0, "avg_logprob": -0.2101680725578248, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.002800525398924947}, {"id": 725, "seek": 288360, "start": 2898.4, "end": 2901.36, "text": " So I just write what I think is actually matters.", "tokens": [51104, 407, 286, 445, 2464, 437, 286, 519, 307, 767, 7001, 13, 51252], "temperature": 0.0, "avg_logprob": -0.2101680725578248, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.002800525398924947}, {"id": 726, "seek": 288360, "start": 2901.7999999999997, "end": 2906.68, "text": " Um, I wish there was, and you know, it actually places like open AI, you know,", "tokens": [51274, 3301, 11, 286, 3172, 456, 390, 11, 293, 291, 458, 11, 309, 767, 3190, 411, 1269, 7318, 11, 291, 458, 11, 51518], "temperature": 0.0, "avg_logprob": -0.2101680725578248, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.002800525398924947}, {"id": 727, "seek": 288360, "start": 2906.68, "end": 2908.52, "text": " the researchers there can do that as well.", "tokens": [51518, 264, 10309, 456, 393, 360, 300, 382, 731, 13, 51610], "temperature": 0.0, "avg_logprob": -0.2101680725578248, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.002800525398924947}, {"id": 728, "seek": 288360, "start": 2909.16, "end": 2913.16, "text": " It's a shame, you know, I wish there was more academic, open,", "tokens": [51642, 467, 311, 257, 10069, 11, 291, 458, 11, 286, 3172, 456, 390, 544, 7778, 11, 1269, 11, 51842], "temperature": 0.0, "avg_logprob": -0.2101680725578248, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.002800525398924947}, {"id": 729, "seek": 291316, "start": 2913.2799999999997, "end": 2918.2, "text": " venues in which people can focus on like genuine innovation.", "tokens": [50370, 32882, 294, 597, 561, 393, 1879, 322, 411, 16699, 8504, 13, 50616], "temperature": 0.0, "avg_logprob": -0.18712300769353318, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0014316632878035307}, {"id": 730, "seek": 291316, "start": 2918.72, "end": 2924.0, "text": " Uh, Twitter, which is, uh, unironically has, has become a little bit of that form.", "tokens": [50642, 4019, 11, 5794, 11, 597, 307, 11, 2232, 11, 517, 2088, 984, 575, 11, 575, 1813, 257, 707, 857, 295, 300, 1254, 13, 50906], "temperature": 0.0, "avg_logprob": -0.18712300769353318, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0014316632878035307}, {"id": 731, "seek": 291316, "start": 2924.3599999999997, "end": 2927.8799999999997, "text": " Uh, I wanted to follow up on one thing that you mentioned, uh, which is that", "tokens": [50924, 4019, 11, 286, 1415, 281, 1524, 493, 322, 472, 551, 300, 291, 2835, 11, 2232, 11, 597, 307, 300, 51100], "temperature": 0.0, "avg_logprob": -0.18712300769353318, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0014316632878035307}, {"id": 732, "seek": 291316, "start": 2927.8799999999997, "end": 2930.3599999999997, "text": " you checked around the open source discords.", "tokens": [51100, 291, 10033, 926, 264, 1269, 4009, 2983, 5703, 13, 51224], "temperature": 0.0, "avg_logprob": -0.18712300769353318, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0014316632878035307}, {"id": 733, "seek": 291316, "start": 2930.72, "end": 2934.7999999999997, "text": " Uh, I don't know if it's, uh, to, uh, I don't know if it's a kosher to ask", "tokens": [51242, 4019, 11, 286, 500, 380, 458, 498, 309, 311, 11, 2232, 11, 281, 11, 2232, 11, 286, 500, 380, 458, 498, 309, 311, 257, 19532, 511, 281, 1029, 51446], "temperature": 0.0, "avg_logprob": -0.18712300769353318, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0014316632878035307}, {"id": 734, "seek": 291316, "start": 2934.7999999999997, "end": 2938.8399999999997, "text": " like what discords are lively, uh, or useful right now.", "tokens": [51446, 411, 437, 2983, 5703, 366, 30866, 11, 2232, 11, 420, 4420, 558, 586, 13, 51648], "temperature": 0.0, "avg_logprob": -0.18712300769353318, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0014316632878035307}, {"id": 735, "seek": 293884, "start": 2939.04, "end": 2943.56, "text": " Um, I think that something I definitely felt like I missed out on was the early", "tokens": [50374, 3301, 11, 286, 519, 300, 746, 286, 2138, 2762, 411, 286, 6721, 484, 322, 390, 264, 2440, 50600], "temperature": 0.0, "avg_logprob": -0.19934654235839844, "compression_ratio": 1.8206686930091185, "no_speech_prob": 0.03675208240747452}, {"id": 736, "seek": 293884, "start": 2943.56, "end": 2946.84, "text": " days of Luther AI, which where, which is a fair hot bit.", "tokens": [50600, 1708, 295, 20693, 7318, 11, 597, 689, 11, 597, 307, 257, 3143, 2368, 857, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19934654235839844, "compression_ratio": 1.8206686930091185, "no_speech_prob": 0.03675208240747452}, {"id": 737, "seek": 293884, "start": 2947.2000000000003, "end": 2950.76, "text": " And, uh, you know, like what is the new Luther and you were, you actually", "tokens": [50782, 400, 11, 2232, 11, 291, 458, 11, 411, 437, 307, 264, 777, 20693, 293, 291, 645, 11, 291, 767, 50960], "temperature": 0.0, "avg_logprob": -0.19934654235839844, "compression_ratio": 1.8206686930091185, "no_speech_prob": 0.03675208240747452}, {"id": 738, "seek": 293884, "start": 2950.76, "end": 2954.0, "text": " shouted out the alignment lab AI discord in your blog posts.", "tokens": [50960, 37310, 484, 264, 18515, 2715, 7318, 32989, 294, 428, 6968, 12300, 13, 51122], "temperature": 0.0, "avg_logprob": -0.19934654235839844, "compression_ratio": 1.8206686930091185, "no_speech_prob": 0.03675208240747452}, {"id": 739, "seek": 293884, "start": 2954.0, "end": 2956.88, "text": " And it was the first time I even knew, like I saw them on Twitter and never", "tokens": [51122, 400, 309, 390, 264, 700, 565, 286, 754, 2586, 11, 411, 286, 1866, 552, 322, 5794, 293, 1128, 51266], "temperature": 0.0, "avg_logprob": -0.19934654235839844, "compression_ratio": 1.8206686930091185, "no_speech_prob": 0.03675208240747452}, {"id": 740, "seek": 293884, "start": 2956.88, "end": 2958.84, "text": " knew they had a discord, never knew that there was actually", "tokens": [51266, 2586, 436, 632, 257, 32989, 11, 1128, 2586, 300, 456, 390, 767, 51364], "temperature": 0.0, "avg_logprob": -0.19934654235839844, "compression_ratio": 1.8206686930091185, "no_speech_prob": 0.03675208240747452}, {"id": 741, "seek": 293884, "start": 2958.84, "end": 2962.2400000000002, "text": " substantive discussions going on in there and that you were an active member of it.", "tokens": [51364, 47113, 11088, 516, 322, 294, 456, 293, 300, 291, 645, 364, 4967, 4006, 295, 309, 13, 51534], "temperature": 0.0, "avg_logprob": -0.19934654235839844, "compression_ratio": 1.8206686930091185, "no_speech_prob": 0.03675208240747452}, {"id": 742, "seek": 293884, "start": 2962.76, "end": 2963.04, "text": " Okay.", "tokens": [51560, 1033, 13, 51574], "temperature": 0.0, "avg_logprob": -0.19934654235839844, "compression_ratio": 1.8206686930091185, "no_speech_prob": 0.03675208240747452}, {"id": 743, "seek": 293884, "start": 2963.04, "end": 2963.28, "text": " Yeah.", "tokens": [51574, 865, 13, 51586], "temperature": 0.0, "avg_logprob": -0.19934654235839844, "compression_ratio": 1.8206686930091185, "no_speech_prob": 0.03675208240747452}, {"id": 744, "seek": 293884, "start": 2963.32, "end": 2966.1600000000003, "text": " And then even then, if you do know about that, you go there, it'll look like", "tokens": [51588, 400, 550, 754, 550, 11, 498, 291, 360, 458, 466, 300, 11, 291, 352, 456, 11, 309, 603, 574, 411, 51730], "temperature": 0.0, "avg_logprob": -0.19934654235839844, "compression_ratio": 1.8206686930091185, "no_speech_prob": 0.03675208240747452}, {"id": 745, "seek": 293884, "start": 2966.1600000000003, "end": 2967.0, "text": " it's totally dead.", "tokens": [51730, 309, 311, 3879, 3116, 13, 51772], "temperature": 0.0, "avg_logprob": -0.19934654235839844, "compression_ratio": 1.8206686930091185, "no_speech_prob": 0.03675208240747452}, {"id": 746, "seek": 296700, "start": 2967.4, "end": 2970.68, "text": " And that's because unfortunately, nearly all the discords, nearly all of", "tokens": [50384, 400, 300, 311, 570, 7015, 11, 6217, 439, 264, 2983, 5703, 11, 6217, 439, 295, 50548], "temperature": 0.0, "avg_logprob": -0.22055756219542852, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.0018097194842994213}, {"id": 747, "seek": 296700, "start": 2970.68, "end": 2976.32, "text": " the conversation happens in private channels, you know, um, and how does", "tokens": [50548, 264, 3761, 2314, 294, 4551, 9235, 11, 291, 458, 11, 1105, 11, 293, 577, 775, 50830], "temperature": 0.0, "avg_logprob": -0.22055756219542852, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.0018097194842994213}, {"id": 748, "seek": 296700, "start": 2976.32, "end": 2978.56, "text": " someone get into that world?", "tokens": [50830, 1580, 483, 666, 300, 1002, 30, 50942], "temperature": 0.0, "avg_logprob": -0.22055756219542852, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.0018097194842994213}, {"id": 749, "seek": 296700, "start": 2978.56, "end": 2982.72, "text": " Cause it's obviously very, very, um, instructive, right?", "tokens": [50942, 10865, 309, 311, 2745, 588, 11, 588, 11, 1105, 11, 7232, 488, 11, 558, 30, 51150], "temperature": 0.0, "avg_logprob": -0.22055756219542852, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.0018097194842994213}, {"id": 750, "seek": 296700, "start": 2982.88, "end": 2986.28, "text": " You could just come to the first day I discord, which I'll be honest with you,", "tokens": [51158, 509, 727, 445, 808, 281, 264, 700, 786, 286, 32989, 11, 597, 286, 603, 312, 3245, 365, 291, 11, 51328], "temperature": 0.0, "avg_logprob": -0.22055756219542852, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.0018097194842994213}, {"id": 751, "seek": 296700, "start": 2986.28, "end": 2992.0, "text": " it's less bustling than some of the others, but it's not terrible.", "tokens": [51328, 309, 311, 1570, 19432, 1688, 813, 512, 295, 264, 2357, 11, 457, 309, 311, 406, 6237, 13, 51614], "temperature": 0.0, "avg_logprob": -0.22055756219542852, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.0018097194842994213}, {"id": 752, "seek": 299200, "start": 2992.36, "end": 2996.76, "text": " And so like, at least, you know, to be fair, one of Emma's bustling", "tokens": [50382, 400, 370, 411, 11, 412, 1935, 11, 291, 458, 11, 281, 312, 3143, 11, 472, 295, 17124, 311, 19432, 1688, 50602], "temperature": 0.0, "avg_logprob": -0.29752471599173036, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.01743210293352604}, {"id": 753, "seek": 299200, "start": 2996.76, "end": 2997.72, "text": " channels is private.", "tokens": [50602, 9235, 307, 4551, 13, 50650], "temperature": 0.0, "avg_logprob": -0.29752471599173036, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.01743210293352604}, {"id": 754, "seek": 299200, "start": 2999.08, "end": 2999.4, "text": " I guess.", "tokens": [50718, 286, 2041, 13, 50734], "temperature": 0.0, "avg_logprob": -0.29752471599173036, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.01743210293352604}, {"id": 755, "seek": 299200, "start": 3001.12, "end": 3002.08, "text": " So I'm just thinking, why is that?", "tokens": [50820, 407, 286, 478, 445, 1953, 11, 983, 307, 300, 30, 50868], "temperature": 0.0, "avg_logprob": -0.29752471599173036, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.01743210293352604}, {"id": 756, "seek": 299200, "start": 3002.08, "end": 3003.56, "text": " It's just a nature of quality discussion, right?", "tokens": [50868, 467, 311, 445, 257, 3687, 295, 3125, 5017, 11, 558, 30, 50942], "temperature": 0.0, "avg_logprob": -0.29752471599173036, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.01743210293352604}, {"id": 757, "seek": 299200, "start": 3004.04, "end": 3007.88, "text": " Yeah, I guess when I think about it, like, I didn't have any private", "tokens": [50966, 865, 11, 286, 2041, 562, 286, 519, 466, 309, 11, 411, 11, 286, 994, 380, 362, 604, 4551, 51158], "temperature": 0.0, "avg_logprob": -0.29752471599173036, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.01743210293352604}, {"id": 758, "seek": 299200, "start": 3007.88, "end": 3009.76, "text": " discussions on a discord for years.", "tokens": [51158, 11088, 322, 257, 32989, 337, 924, 13, 51252], "temperature": 0.0, "avg_logprob": -0.29752471599173036, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.01743210293352604}, {"id": 759, "seek": 299200, "start": 3010.16, "end": 3014.84, "text": " Um, but there was a lot of people who came in with like, oh, I just had", "tokens": [51272, 3301, 11, 457, 456, 390, 257, 688, 295, 561, 567, 1361, 294, 365, 411, 11, 1954, 11, 286, 445, 632, 51506], "temperature": 0.0, "avg_logprob": -0.29752471599173036, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.01743210293352604}, {"id": 760, "seek": 299200, "start": 3014.84, "end": 3017.12, "text": " this amazing idea for AGI.", "tokens": [51506, 341, 2243, 1558, 337, 316, 26252, 13, 51620], "temperature": 0.0, "avg_logprob": -0.29752471599173036, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.01743210293352604}, {"id": 761, "seek": 299200, "start": 3017.12, "end": 3021.64, "text": " If you just thought about like, if you imagine the AI is a brain and we, you", "tokens": [51620, 759, 291, 445, 1194, 466, 411, 11, 498, 291, 3811, 264, 7318, 307, 257, 3567, 293, 321, 11, 291, 51846], "temperature": 0.0, "avg_logprob": -0.29752471599173036, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.01743210293352604}, {"id": 762, "seek": 302164, "start": 3021.68, "end": 3023.68, "text": " know, this just, I don't want to talk about it.", "tokens": [50366, 458, 11, 341, 445, 11, 286, 500, 380, 528, 281, 751, 466, 309, 13, 50466], "temperature": 0.0, "avg_logprob": -0.19500649308359158, "compression_ratio": 1.7729885057471264, "no_speech_prob": 0.0014547468163073063}, {"id": 763, "seek": 302164, "start": 3023.8399999999997, "end": 3027.56, "text": " You know, I don't want to like, but you don't want to be dismissive or whatever.", "tokens": [50474, 509, 458, 11, 286, 500, 380, 528, 281, 411, 11, 457, 291, 500, 380, 528, 281, 312, 16974, 488, 420, 2035, 13, 50660], "temperature": 0.0, "avg_logprob": -0.19500649308359158, "compression_ratio": 1.7729885057471264, "no_speech_prob": 0.0014547468163073063}, {"id": 764, "seek": 302164, "start": 3027.56, "end": 3030.0, "text": " And it's like, oh, well, that's an interesting comment, but maybe you should", "tokens": [50660, 400, 309, 311, 411, 11, 1954, 11, 731, 11, 300, 311, 364, 1880, 2871, 11, 457, 1310, 291, 820, 50782], "temperature": 0.0, "avg_logprob": -0.19500649308359158, "compression_ratio": 1.7729885057471264, "no_speech_prob": 0.0014547468163073063}, {"id": 765, "seek": 302164, "start": 3030.0, "end": 3033.04, "text": " like try training some models first to see if that aligns with your intuition.", "tokens": [50782, 411, 853, 3097, 512, 5245, 700, 281, 536, 498, 300, 7975, 82, 365, 428, 24002, 13, 50934], "temperature": 0.0, "avg_logprob": -0.19500649308359158, "compression_ratio": 1.7729885057471264, "no_speech_prob": 0.0014547468163073063}, {"id": 766, "seek": 302164, "start": 3033.04, "end": 3034.48, "text": " Like, oh, but how can I possibly learn?", "tokens": [50934, 1743, 11, 1954, 11, 457, 577, 393, 286, 6264, 1466, 30, 51006], "temperature": 0.0, "avg_logprob": -0.19500649308359158, "compression_ratio": 1.7729885057471264, "no_speech_prob": 0.0014547468163073063}, {"id": 767, "seek": 302164, "start": 3034.48, "end": 3038.08, "text": " It's like, well, we have a course just actually spend time learning.", "tokens": [51006, 467, 311, 411, 11, 731, 11, 321, 362, 257, 1164, 445, 767, 3496, 565, 2539, 13, 51186], "temperature": 0.0, "avg_logprob": -0.19500649308359158, "compression_ratio": 1.7729885057471264, "no_speech_prob": 0.0014547468163073063}, {"id": 768, "seek": 302164, "start": 3038.08, "end": 3039.12, "text": " Like, uh, yeah.", "tokens": [51186, 1743, 11, 2232, 11, 1338, 13, 51238], "temperature": 0.0, "avg_logprob": -0.19500649308359158, "compression_ratio": 1.7729885057471264, "no_speech_prob": 0.0014547468163073063}, {"id": 769, "seek": 302164, "start": 3039.16, "end": 3039.48, "text": " Anyway.", "tokens": [51240, 5684, 13, 51256], "temperature": 0.0, "avg_logprob": -0.19500649308359158, "compression_ratio": 1.7729885057471264, "no_speech_prob": 0.0014547468163073063}, {"id": 770, "seek": 302164, "start": 3040.12, "end": 3043.96, "text": " And there's like, okay, I know the people who always have good answers there.", "tokens": [51288, 400, 456, 311, 411, 11, 1392, 11, 286, 458, 264, 561, 567, 1009, 362, 665, 6338, 456, 13, 51480], "temperature": 0.0, "avg_logprob": -0.19500649308359158, "compression_ratio": 1.7729885057471264, "no_speech_prob": 0.0014547468163073063}, {"id": 771, "seek": 302164, "start": 3043.96, "end": 3046.64, "text": " And so I created a private channel and put them all in it.", "tokens": [51480, 400, 370, 286, 2942, 257, 4551, 2269, 293, 829, 552, 439, 294, 309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19500649308359158, "compression_ratio": 1.7729885057471264, "no_speech_prob": 0.0014547468163073063}, {"id": 772, "seek": 302164, "start": 3046.72, "end": 3050.2799999999997, "text": " And I got to admit, I, that's where I post more often because.", "tokens": [51618, 400, 286, 658, 281, 9796, 11, 286, 11, 300, 311, 689, 286, 2183, 544, 2049, 570, 13, 51796], "temperature": 0.0, "avg_logprob": -0.19500649308359158, "compression_ratio": 1.7729885057471264, "no_speech_prob": 0.0014547468163073063}, {"id": 773, "seek": 305028, "start": 3051.28, "end": 3057.52, "text": " There's much less, you know, flight of fancy views about how we could solve AGI,", "tokens": [50414, 821, 311, 709, 1570, 11, 291, 458, 11, 7018, 295, 10247, 6809, 466, 577, 321, 727, 5039, 316, 26252, 11, 50726], "temperature": 0.0, "avg_logprob": -0.20860937663487025, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0030747789423912764}, {"id": 774, "seek": 305028, "start": 3057.52, "end": 3058.0400000000004, "text": " blah, blah, blah.", "tokens": [50726, 12288, 11, 12288, 11, 12288, 13, 50752], "temperature": 0.0, "avg_logprob": -0.20860937663487025, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0030747789423912764}, {"id": 775, "seek": 305028, "start": 3058.36, "end": 3062.88, "text": " So there is a bit of that, but having said that, like, I think the bar is pretty", "tokens": [50768, 407, 456, 307, 257, 857, 295, 300, 11, 457, 1419, 848, 300, 11, 411, 11, 286, 519, 264, 2159, 307, 1238, 50994], "temperature": 0.0, "avg_logprob": -0.20860937663487025, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0030747789423912764}, {"id": 776, "seek": 305028, "start": 3062.88, "end": 3070.6800000000003, "text": " low, like if you join a discord and you can hit the, like participants or", "tokens": [50994, 2295, 11, 411, 498, 291, 3917, 257, 32989, 293, 291, 393, 2045, 264, 11, 411, 10503, 420, 51384], "temperature": 0.0, "avg_logprob": -0.20860937663487025, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0030747789423912764}, {"id": 777, "seek": 305028, "start": 3070.6800000000003, "end": 3072.6000000000004, "text": " community or whatever button and you can see who's in it.", "tokens": [51384, 1768, 420, 2035, 2960, 293, 291, 393, 536, 567, 311, 294, 309, 13, 51480], "temperature": 0.0, "avg_logprob": -0.20860937663487025, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0030747789423912764}, {"id": 778, "seek": 305028, "start": 3072.6000000000004, "end": 3076.32, "text": " And you'll see at the top who, who the admins or moderators or people in the dev", "tokens": [51480, 400, 291, 603, 536, 412, 264, 1192, 567, 11, 567, 264, 5910, 1292, 420, 10494, 3391, 420, 561, 294, 264, 1905, 51666], "temperature": 0.0, "avg_logprob": -0.20860937663487025, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0030747789423912764}, {"id": 779, "seek": 307632, "start": 3076.44, "end": 3085.1200000000003, "text": " role are and, uh, just DM one of them and say, like, oh, I, here's my GitHub.", "tokens": [50370, 3090, 366, 293, 11, 2232, 11, 445, 15322, 472, 295, 552, 293, 584, 11, 411, 11, 1954, 11, 286, 11, 510, 311, 452, 23331, 13, 50804], "temperature": 0.0, "avg_logprob": -0.25164485622096705, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.005382841918617487}, {"id": 780, "seek": 307632, "start": 3085.6000000000004, "end": 3089.44, "text": " Well, here's some blog posts they wrote, you know, I'm interested in talking about", "tokens": [50828, 1042, 11, 510, 311, 512, 6968, 12300, 436, 4114, 11, 291, 458, 11, 286, 478, 3102, 294, 1417, 466, 51020], "temperature": 0.0, "avg_logprob": -0.25164485622096705, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.005382841918617487}, {"id": 781, "seek": 307632, "start": 3089.44, "end": 3094.1200000000003, "text": " this, you know, can I join the private channels and I've never heard of anybody", "tokens": [51020, 341, 11, 291, 458, 11, 393, 286, 3917, 264, 4551, 9235, 293, 286, 600, 1128, 2198, 295, 4472, 51254], "temperature": 0.0, "avg_logprob": -0.25164485622096705, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.005382841918617487}, {"id": 782, "seek": 307632, "start": 3094.1200000000003, "end": 3100.32, "text": " saying no, I will say, you know, uh, a Luther's all pretty open.", "tokens": [51254, 1566, 572, 11, 286, 486, 584, 11, 291, 458, 11, 2232, 11, 257, 20693, 311, 439, 1238, 1269, 13, 51564], "temperature": 0.0, "avg_logprob": -0.25164485622096705, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.005382841918617487}, {"id": 783, "seek": 307632, "start": 3100.84, "end": 3104.2000000000003, "text": " So you can do the Aleutha discord still, you know, one problem with your", "tokens": [51590, 407, 291, 393, 360, 264, 9366, 325, 1641, 32989, 920, 11, 291, 458, 11, 472, 1154, 365, 428, 51758], "temperature": 0.0, "avg_logprob": -0.25164485622096705, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.005382841918617487}, {"id": 784, "seek": 310420, "start": 3104.2, "end": 3109.04, "text": " Luther discord is it's been going on for so long that it's like, it's very", "tokens": [50364, 20693, 32989, 307, 309, 311, 668, 516, 322, 337, 370, 938, 300, 309, 311, 411, 11, 309, 311, 588, 50606], "temperature": 0.0, "avg_logprob": -0.3178439046822342, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.0024721133522689342}, {"id": 785, "seek": 310420, "start": 3109.04, "end": 3110.04, "text": " inside baseball.", "tokens": [50606, 1854, 14323, 13, 50656], "temperature": 0.0, "avg_logprob": -0.3178439046822342, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.0024721133522689342}, {"id": 786, "seek": 310420, "start": 3110.3199999999997, "end": 3112.68, "text": " It's hard to, it's hard to get started.", "tokens": [50670, 467, 311, 1152, 281, 11, 309, 311, 1152, 281, 483, 1409, 13, 50788], "temperature": 0.0, "avg_logprob": -0.3178439046822342, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.0024721133522689342}, {"id": 787, "seek": 310420, "start": 3113.08, "end": 3113.2799999999997, "text": " Yeah.", "tokens": [50808, 865, 13, 50818], "temperature": 0.0, "avg_logprob": -0.3178439046822342, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.0024721133522689342}, {"id": 788, "seek": 310420, "start": 3113.2799999999997, "end": 3118.7599999999998, "text": " Um, Kappa AI looks, I think it's all open.", "tokens": [50818, 3301, 11, 591, 25637, 7318, 1542, 11, 286, 519, 309, 311, 439, 1269, 13, 51092], "temperature": 0.0, "avg_logprob": -0.3178439046822342, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.0024721133522689342}, {"id": 789, "seek": 310420, "start": 3119.8399999999997, "end": 3122.2, "text": " This just left a stability.", "tokens": [51146, 639, 445, 1411, 257, 11826, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3178439046822342, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.0024721133522689342}, {"id": 790, "seek": 310420, "start": 3122.2, "end": 3123.16, "text": " That's more accessible.", "tokens": [51264, 663, 311, 544, 9515, 13, 51312], "temperature": 0.0, "avg_logprob": -0.3178439046822342, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.0024721133522689342}, {"id": 791, "seek": 310420, "start": 3123.7599999999998, "end": 3123.96, "text": " Yeah.", "tokens": [51342, 865, 13, 51352], "temperature": 0.0, "avg_logprob": -0.3178439046822342, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.0024721133522689342}, {"id": 792, "seek": 310420, "start": 3124.0, "end": 3131.0, "text": " Um, uh, there's also, uh, just recently, uh, now three search that", "tokens": [51354, 3301, 11, 2232, 11, 456, 311, 611, 11, 2232, 11, 445, 3938, 11, 2232, 11, 586, 1045, 3164, 300, 51704], "temperature": 0.0, "avg_logprob": -0.3178439046822342, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.0024721133522689342}, {"id": 793, "seek": 313100, "start": 3131.0, "end": 3136.16, "text": " does like the Hermes models and data set just, just opened, they've got some", "tokens": [50364, 775, 411, 264, 21842, 279, 5245, 293, 1412, 992, 445, 11, 445, 5625, 11, 436, 600, 658, 512, 50622], "temperature": 0.0, "avg_logprob": -0.2040094768299776, "compression_ratio": 1.7588932806324111, "no_speech_prob": 0.0034824013710021973}, {"id": 794, "seek": 313100, "start": 3136.16, "end": 3138.0, "text": " private channels, but it's pretty open.", "tokens": [50622, 4551, 9235, 11, 457, 309, 311, 1238, 1269, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2040094768299776, "compression_ratio": 1.7588932806324111, "no_speech_prob": 0.0034824013710021973}, {"id": 795, "seek": 313100, "start": 3138.0, "end": 3141.72, "text": " I think, uh, you mentioned alignment lab, that one, it's all the interesting", "tokens": [50714, 286, 519, 11, 2232, 11, 291, 2835, 18515, 2715, 11, 300, 472, 11, 309, 311, 439, 264, 1880, 50900], "temperature": 0.0, "avg_logprob": -0.2040094768299776, "compression_ratio": 1.7588932806324111, "no_speech_prob": 0.0034824013710021973}, {"id": 796, "seek": 313100, "start": 3141.72, "end": 3142.8, "text": " stuff is on private channels.", "tokens": [50900, 1507, 307, 322, 4551, 9235, 13, 50954], "temperature": 0.0, "avg_logprob": -0.2040094768299776, "compression_ratio": 1.7588932806324111, "no_speech_prob": 0.0034824013710021973}, {"id": 797, "seek": 313100, "start": 3142.8, "end": 3148.72, "text": " So just ask, um, if, if you know me, ask me, cause I've got admin on that one.", "tokens": [50954, 407, 445, 1029, 11, 1105, 11, 498, 11, 498, 291, 458, 385, 11, 1029, 385, 11, 3082, 286, 600, 658, 24236, 322, 300, 472, 13, 51250], "temperature": 0.0, "avg_logprob": -0.2040094768299776, "compression_ratio": 1.7588932806324111, "no_speech_prob": 0.0034824013710021973}, {"id": 798, "seek": 313100, "start": 3149.0, "end": 3154.92, "text": " There's also, yeah, uh, OS skunkworks, OS skunkworks AI is a good discord,", "tokens": [51264, 821, 311, 611, 11, 1338, 11, 2232, 11, 12731, 1110, 3197, 18357, 11, 12731, 1110, 3197, 18357, 7318, 307, 257, 665, 32989, 11, 51560], "temperature": 0.0, "avg_logprob": -0.2040094768299776, "compression_ratio": 1.7588932806324111, "no_speech_prob": 0.0034824013710021973}, {"id": 799, "seek": 313100, "start": 3154.92, "end": 3157.76, "text": " which I think it's open.", "tokens": [51560, 597, 286, 519, 309, 311, 1269, 13, 51702], "temperature": 0.0, "avg_logprob": -0.2040094768299776, "compression_ratio": 1.7588932806324111, "no_speech_prob": 0.0034824013710021973}, {"id": 800, "seek": 313100, "start": 3158.88, "end": 3160.32, "text": " So they're, yeah, they're all pretty good.", "tokens": [51758, 407, 436, 434, 11, 1338, 11, 436, 434, 439, 1238, 665, 13, 51830], "temperature": 0.0, "avg_logprob": -0.2040094768299776, "compression_ratio": 1.7588932806324111, "no_speech_prob": 0.0034824013710021973}, {"id": 801, "seek": 316032, "start": 3160.52, "end": 3164.56, "text": " I don't want you to leak any, any, uh, you know, uh, discords that don't", "tokens": [50374, 286, 500, 380, 528, 291, 281, 17143, 604, 11, 604, 11, 2232, 11, 291, 458, 11, 2232, 11, 2983, 5703, 300, 500, 380, 50576], "temperature": 0.0, "avg_logprob": -0.15400732585362026, "compression_ratio": 2.0294117647058822, "no_speech_prob": 0.0005612370441667736}, {"id": 802, "seek": 316032, "start": 3164.56, "end": 3169.7200000000003, "text": " want any publicity, but we all want people, like we all want people.", "tokens": [50576, 528, 604, 37264, 11, 457, 321, 439, 528, 561, 11, 411, 321, 439, 528, 561, 13, 50834], "temperature": 0.0, "avg_logprob": -0.15400732585362026, "compression_ratio": 2.0294117647058822, "no_speech_prob": 0.0005612370441667736}, {"id": 803, "seek": 316032, "start": 3169.7200000000003, "end": 3175.04, "text": " We just, we just want people who like want to build stuff, you know, um, rather", "tokens": [50834, 492, 445, 11, 321, 445, 528, 561, 567, 411, 528, 281, 1322, 1507, 11, 291, 458, 11, 1105, 11, 2831, 51100], "temperature": 0.0, "avg_logprob": -0.15400732585362026, "compression_ratio": 2.0294117647058822, "no_speech_prob": 0.0005612370441667736}, {"id": 804, "seek": 316032, "start": 3175.04, "end": 3181.52, "text": " than people who, and like, it's fine to not know anything as well, but if you", "tokens": [51100, 813, 561, 567, 11, 293, 411, 11, 309, 311, 2489, 281, 406, 458, 1340, 382, 731, 11, 457, 498, 291, 51424], "temperature": 0.0, "avg_logprob": -0.15400732585362026, "compression_ratio": 2.0294117647058822, "no_speech_prob": 0.0005612370441667736}, {"id": 805, "seek": 316032, "start": 3181.52, "end": 3184.36, "text": " don't know anything, but you want to tell everybody else what to do and how to", "tokens": [51424, 500, 380, 458, 1340, 11, 457, 291, 528, 281, 980, 2201, 1646, 437, 281, 360, 293, 577, 281, 51566], "temperature": 0.0, "avg_logprob": -0.15400732585362026, "compression_ratio": 2.0294117647058822, "no_speech_prob": 0.0005612370441667736}, {"id": 806, "seek": 316032, "start": 3184.36, "end": 3185.2000000000003, "text": " do it, that's annoying.", "tokens": [51566, 360, 309, 11, 300, 311, 11304, 13, 51608], "temperature": 0.0, "avg_logprob": -0.15400732585362026, "compression_ratio": 2.0294117647058822, "no_speech_prob": 0.0005612370441667736}, {"id": 807, "seek": 316032, "start": 3185.52, "end": 3190.0800000000004, "text": " If you don't know anything and want to be told, like, here's a really small kind", "tokens": [51624, 759, 291, 500, 380, 458, 1340, 293, 528, 281, 312, 1907, 11, 411, 11, 510, 311, 257, 534, 1359, 733, 51852], "temperature": 0.0, "avg_logprob": -0.15400732585362026, "compression_ratio": 2.0294117647058822, "no_speech_prob": 0.0005612370441667736}, {"id": 808, "seek": 319008, "start": 3190.08, "end": 3193.3199999999997, "text": " of task that as somebody who doesn't know anything, it's going to take you a", "tokens": [50364, 295, 5633, 300, 382, 2618, 567, 1177, 380, 458, 1340, 11, 309, 311, 516, 281, 747, 291, 257, 50526], "temperature": 0.0, "avg_logprob": -0.12869722016003668, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.0037064533680677414}, {"id": 809, "seek": 319008, "start": 3193.3199999999997, "end": 3195.56, "text": " really long time to do, but it would still be helpful.", "tokens": [50526, 534, 938, 565, 281, 360, 11, 457, 309, 576, 920, 312, 4961, 13, 50638], "temperature": 0.0, "avg_logprob": -0.12869722016003668, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.0037064533680677414}, {"id": 810, "seek": 319008, "start": 3195.88, "end": 3197.04, "text": " Then, and then you go and do it.", "tokens": [50654, 1396, 11, 293, 550, 291, 352, 293, 360, 309, 13, 50712], "temperature": 0.0, "avg_logprob": -0.12869722016003668, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.0037064533680677414}, {"id": 811, "seek": 319008, "start": 3197.08, "end": 3197.64, "text": " That would be great.", "tokens": [50714, 663, 576, 312, 869, 13, 50742], "temperature": 0.0, "avg_logprob": -0.12869722016003668, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.0037064533680677414}, {"id": 812, "seek": 319008, "start": 3198.2, "end": 3202.6, "text": " The truth is, yeah, like, I don't know, maybe 5% of people who come in with", "tokens": [50770, 440, 3494, 307, 11, 1338, 11, 411, 11, 286, 500, 380, 458, 11, 1310, 1025, 4, 295, 561, 567, 808, 294, 365, 50990], "temperature": 0.0, "avg_logprob": -0.12869722016003668, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.0037064533680677414}, {"id": 813, "seek": 319008, "start": 3202.6, "end": 3205.2799999999997, "text": " great enthusiasm and saying that they want to learn and they'll do anything.", "tokens": [50990, 869, 23417, 293, 1566, 300, 436, 528, 281, 1466, 293, 436, 603, 360, 1340, 13, 51124], "temperature": 0.0, "avg_logprob": -0.12869722016003668, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.0037064533680677414}, {"id": 814, "seek": 319008, "start": 3205.2799999999997, "end": 3207.4, "text": " And then somebody says like, okay, here's some work you can do.", "tokens": [51124, 400, 550, 2618, 1619, 411, 11, 1392, 11, 510, 311, 512, 589, 291, 393, 360, 13, 51230], "temperature": 0.0, "avg_logprob": -0.12869722016003668, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.0037064533680677414}, {"id": 815, "seek": 319008, "start": 3207.88, "end": 3209.24, "text": " Almost nobody does that work.", "tokens": [51254, 12627, 5079, 775, 300, 589, 13, 51322], "temperature": 0.0, "avg_logprob": -0.12869722016003668, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.0037064533680677414}, {"id": 816, "seek": 319008, "start": 3209.7599999999998, "end": 3216.04, "text": " So if you're somebody who actually does the work and follows up, you will massively", "tokens": [51348, 407, 498, 291, 434, 2618, 567, 767, 775, 264, 589, 293, 10002, 493, 11, 291, 486, 29379, 51662], "temperature": 0.0, "avg_logprob": -0.12869722016003668, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.0037064533680677414}, {"id": 817, "seek": 319008, "start": 3216.04, "end": 3216.7999999999997, "text": " stand out.", "tokens": [51662, 1463, 484, 13, 51700], "temperature": 0.0, "avg_logprob": -0.12869722016003668, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.0037064533680677414}, {"id": 818, "seek": 321680, "start": 3216.92, "end": 3220.92, "text": " That's an extreme rarity and everybody will then want to help you do more work.", "tokens": [50370, 663, 311, 364, 8084, 367, 17409, 293, 2201, 486, 550, 528, 281, 854, 291, 360, 544, 589, 13, 50570], "temperature": 0.0, "avg_logprob": -0.19930474417550223, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.01320990826934576}, {"id": 819, "seek": 321680, "start": 3221.0800000000004, "end": 3227.1600000000003, "text": " So, yeah, so just, um, yeah, just do work and people will want to support you.", "tokens": [50578, 407, 11, 1338, 11, 370, 445, 11, 1105, 11, 1338, 11, 445, 360, 589, 293, 561, 486, 528, 281, 1406, 291, 13, 50882], "temperature": 0.0, "avg_logprob": -0.19930474417550223, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.01320990826934576}, {"id": 820, "seek": 321680, "start": 3227.92, "end": 3231.1600000000003, "text": " Our discord used to be referral only for a long time.", "tokens": [50920, 2621, 32989, 1143, 281, 312, 33494, 787, 337, 257, 938, 565, 13, 51082], "temperature": 0.0, "avg_logprob": -0.19930474417550223, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.01320990826934576}, {"id": 821, "seek": 321680, "start": 3231.2400000000002, "end": 3235.1200000000003, "text": " We then have a public invite and then we opened it and they're kind of like", "tokens": [51086, 492, 550, 362, 257, 1908, 7980, 293, 550, 321, 5625, 309, 293, 436, 434, 733, 295, 411, 51280], "temperature": 0.0, "avg_logprob": -0.19930474417550223, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.01320990826934576}, {"id": 822, "seek": 321680, "start": 3235.1200000000003, "end": 3235.84, "text": " channel gating.", "tokens": [51280, 2269, 290, 990, 13, 51316], "temperature": 0.0, "avg_logprob": -0.19930474417550223, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.01320990826934576}, {"id": 823, "seek": 321680, "start": 3236.28, "end": 3238.36, "text": " Um, yeah, a lot of people just want to do.", "tokens": [51338, 3301, 11, 1338, 11, 257, 688, 295, 561, 445, 528, 281, 360, 13, 51442], "temperature": 0.0, "avg_logprob": -0.19930474417550223, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.01320990826934576}, {"id": 824, "seek": 321680, "start": 3238.36, "end": 3240.6000000000004, "text": " I remember it used to be like, you know, a forum moderator.", "tokens": [51442, 286, 1604, 309, 1143, 281, 312, 411, 11, 291, 458, 11, 257, 17542, 37778, 13, 51554], "temperature": 0.0, "avg_logprob": -0.19930474417550223, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.01320990826934576}, {"id": 825, "seek": 321680, "start": 3240.88, "end": 3244.6800000000003, "text": " It's like people just want to do like drive by posting, you know, I'm like, they", "tokens": [51568, 467, 311, 411, 561, 445, 528, 281, 360, 411, 3332, 538, 15978, 11, 291, 458, 11, 286, 478, 411, 11, 436, 51758], "temperature": 0.0, "avg_logprob": -0.19930474417550223, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.01320990826934576}, {"id": 826, "seek": 324468, "start": 3244.68, "end": 3245.8799999999997, "text": " don't want to help the community.", "tokens": [50364, 500, 380, 528, 281, 854, 264, 1768, 13, 50424], "temperature": 0.0, "avg_logprob": -0.17819199068792935, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0009108572849072516}, {"id": 827, "seek": 324468, "start": 3245.8799999999997, "end": 3247.52, "text": " They just want to get their question answered.", "tokens": [50424, 814, 445, 528, 281, 483, 641, 1168, 10103, 13, 50506], "temperature": 0.0, "avg_logprob": -0.17819199068792935, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0009108572849072516}, {"id": 828, "seek": 324468, "start": 3247.7599999999998, "end": 3253.0, "text": " I mean, the funny thing is our, um, our forum community does not have any of that", "tokens": [50518, 286, 914, 11, 264, 4074, 551, 307, 527, 11, 1105, 11, 527, 17542, 1768, 775, 406, 362, 604, 295, 300, 50780], "temperature": 0.0, "avg_logprob": -0.17819199068792935, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0009108572849072516}, {"id": 829, "seek": 324468, "start": 3253.0, "end": 3257.08, "text": " garbage, you know, there's something specific about the low latency thing.", "tokens": [50780, 14150, 11, 291, 458, 11, 456, 311, 746, 2685, 466, 264, 2295, 27043, 551, 13, 50984], "temperature": 0.0, "avg_logprob": -0.17819199068792935, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0009108572849072516}, {"id": 830, "seek": 324468, "start": 3257.08, "end": 3263.3199999999997, "text": " There were people like, they expect an instant answer and yeah, we're all somehow", "tokens": [50984, 821, 645, 561, 411, 11, 436, 2066, 364, 9836, 1867, 293, 1338, 11, 321, 434, 439, 6063, 51296], "temperature": 0.0, "avg_logprob": -0.17819199068792935, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0009108572849072516}, {"id": 831, "seek": 324468, "start": 3263.3199999999997, "end": 3267.12, "text": " in a forum's tread where they know it's like they're forever.", "tokens": [51296, 294, 257, 17542, 311, 28286, 689, 436, 458, 309, 311, 411, 436, 434, 5680, 13, 51486], "temperature": 0.0, "avg_logprob": -0.17819199068792935, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0009108572849072516}, {"id": 832, "seek": 324468, "start": 3268.04, "end": 3273.8799999999997, "text": " People are a bit more thoughtful, but then the forums are less active than they", "tokens": [51532, 3432, 366, 257, 857, 544, 21566, 11, 457, 550, 264, 26998, 366, 1570, 4967, 813, 436, 51824], "temperature": 0.0, "avg_logprob": -0.17819199068792935, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0009108572849072516}, {"id": 833, "seek": 327388, "start": 3273.88, "end": 3280.44, "text": " used to be because discord has got more popular, you know, so it's all a bit of", "tokens": [50364, 1143, 281, 312, 570, 32989, 575, 658, 544, 3743, 11, 291, 458, 11, 370, 309, 311, 439, 257, 857, 295, 50692], "temperature": 0.0, "avg_logprob": -0.2072595692367005, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.0011331775458529592}, {"id": 834, "seek": 327388, "start": 3280.44, "end": 3285.52, "text": " a compromise, you know, running a healthy community is, yes, it's always a bit of a", "tokens": [50692, 257, 18577, 11, 291, 458, 11, 2614, 257, 4627, 1768, 307, 11, 2086, 11, 309, 311, 1009, 257, 857, 295, 257, 50946], "temperature": 0.0, "avg_logprob": -0.2072595692367005, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.0011331775458529592}, {"id": 835, "seek": 327388, "start": 3285.52, "end": 3286.04, "text": " challenge.", "tokens": [50946, 3430, 13, 50972], "temperature": 0.0, "avg_logprob": -0.2072595692367005, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.0011331775458529592}, {"id": 836, "seek": 327388, "start": 3286.48, "end": 3289.08, "text": " All right, we've got so many more things we want to dive in, but I don't want to", "tokens": [50994, 1057, 558, 11, 321, 600, 658, 370, 867, 544, 721, 321, 528, 281, 9192, 294, 11, 457, 286, 500, 380, 528, 281, 51124], "temperature": 0.0, "avg_logprob": -0.2072595692367005, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.0011331775458529592}, {"id": 837, "seek": 327388, "start": 3289.08, "end": 3290.2400000000002, "text": " keep you here for hours.", "tokens": [51124, 1066, 291, 510, 337, 2496, 13, 51182], "temperature": 0.0, "avg_logprob": -0.2072595692367005, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.0011331775458529592}, {"id": 838, "seek": 327388, "start": 3290.7200000000003, "end": 3292.96, "text": " Uh, this is not the, the lack of freedom in pockets.", "tokens": [51206, 4019, 11, 341, 307, 406, 264, 11, 264, 5011, 295, 5645, 294, 16491, 13, 51318], "temperature": 0.0, "avg_logprob": -0.2072595692367005, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.0011331775458529592}, {"id": 839, "seek": 327388, "start": 3292.96, "end": 3297.6400000000003, "text": " We always like to say, uh, one topic I would love to maybe chat a bit about is", "tokens": [51318, 492, 1009, 411, 281, 584, 11, 2232, 11, 472, 4829, 286, 576, 959, 281, 1310, 5081, 257, 857, 466, 307, 51552], "temperature": 0.0, "avg_logprob": -0.2072595692367005, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.0011331775458529592}, {"id": 840, "seek": 327388, "start": 3297.76, "end": 3301.8, "text": " Mojo modular, you know, Chris Liner nominated you on the pockets.", "tokens": [51558, 3335, 5134, 31111, 11, 291, 458, 11, 6688, 441, 4564, 25159, 291, 322, 264, 16491, 13, 51760], "temperature": 0.0, "avg_logprob": -0.2072595692367005, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.0011331775458529592}, {"id": 841, "seek": 330180, "start": 3301.88, "end": 3304.1600000000003, "text": " So, uh, we want to spend a little time there.", "tokens": [50368, 407, 11, 2232, 11, 321, 528, 281, 3496, 257, 707, 565, 456, 13, 50482], "temperature": 0.0, "avg_logprob": -0.20493763596264283, "compression_ratio": 1.62, "no_speech_prob": 0.0031716367229819298}, {"id": 842, "seek": 330180, "start": 3304.2000000000003, "end": 3308.0800000000004, "text": " You recently did a hacker's guide to language models and you ran through", "tokens": [50484, 509, 3938, 630, 257, 38155, 311, 5934, 281, 2856, 5245, 293, 291, 5872, 807, 50678], "temperature": 0.0, "avg_logprob": -0.20493763596264283, "compression_ratio": 1.62, "no_speech_prob": 0.0031716367229819298}, {"id": 843, "seek": 330180, "start": 3308.0800000000004, "end": 3313.0, "text": " everything from quantized model to like smaller models, larger models and all of", "tokens": [50678, 1203, 490, 4426, 1602, 2316, 281, 411, 4356, 5245, 11, 4833, 5245, 293, 439, 295, 50924], "temperature": 0.0, "avg_logprob": -0.20493763596264283, "compression_ratio": 1.62, "no_speech_prob": 0.0031716367229819298}, {"id": 844, "seek": 330180, "start": 3313.0, "end": 3313.28, "text": " that.", "tokens": [50924, 300, 13, 50938], "temperature": 0.0, "avg_logprob": -0.20493763596264283, "compression_ratio": 1.62, "no_speech_prob": 0.0031716367229819298}, {"id": 845, "seek": 330180, "start": 3313.6400000000003, "end": 3317.1600000000003, "text": " Um, but obviously modular is taking its own approach.", "tokens": [50956, 3301, 11, 457, 2745, 31111, 307, 1940, 1080, 1065, 3109, 13, 51132], "temperature": 0.0, "avg_logprob": -0.20493763596264283, "compression_ratio": 1.62, "no_speech_prob": 0.0031716367229819298}, {"id": 846, "seek": 330180, "start": 3317.28, "end": 3318.5600000000004, "text": " Uh, yeah, we'll get you excited.", "tokens": [51138, 4019, 11, 1338, 11, 321, 603, 483, 291, 2919, 13, 51202], "temperature": 0.0, "avg_logprob": -0.20493763596264283, "compression_ratio": 1.62, "no_speech_prob": 0.0031716367229819298}, {"id": 847, "seek": 330180, "start": 3318.5600000000004, "end": 3322.32, "text": " I know you Chris have been talking about this for like years and a lot of the", "tokens": [51202, 286, 458, 291, 6688, 362, 668, 1417, 466, 341, 337, 411, 924, 293, 257, 688, 295, 264, 51390], "temperature": 0.0, "avg_logprob": -0.20493763596264283, "compression_ratio": 1.62, "no_speech_prob": 0.0031716367229819298}, {"id": 848, "seek": 330180, "start": 3322.32, "end": 3323.2000000000003, "text": " ideas you had, so.", "tokens": [51390, 3487, 291, 632, 11, 370, 13, 51434], "temperature": 0.0, "avg_logprob": -0.20493763596264283, "compression_ratio": 1.62, "no_speech_prob": 0.0031716367229819298}, {"id": 849, "seek": 330180, "start": 3323.6800000000003, "end": 3325.2400000000002, "text": " Yeah, yeah, yeah, absolutely.", "tokens": [51458, 865, 11, 1338, 11, 1338, 11, 3122, 13, 51536], "temperature": 0.0, "avg_logprob": -0.20493763596264283, "compression_ratio": 1.62, "no_speech_prob": 0.0031716367229819298}, {"id": 850, "seek": 330180, "start": 3325.2400000000002, "end": 3330.84, "text": " So I met Chris, I think it was at the first TensorFlow Dev Summit.", "tokens": [51536, 407, 286, 1131, 6688, 11, 286, 519, 309, 390, 412, 264, 700, 37624, 9096, 28726, 13, 51816], "temperature": 0.0, "avg_logprob": -0.20493763596264283, "compression_ratio": 1.62, "no_speech_prob": 0.0031716367229819298}, {"id": 851, "seek": 333084, "start": 3331.44, "end": 3335.28, "text": " And I don't think he had even like, I'm not sure if he'd even sufficiently", "tokens": [50394, 400, 286, 500, 380, 519, 415, 632, 754, 411, 11, 286, 478, 406, 988, 498, 415, 1116, 754, 31868, 50586], "temperature": 0.0, "avg_logprob": -0.19984564233998783, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.001064603216946125}, {"id": 852, "seek": 333084, "start": 3335.28, "end": 3337.48, "text": " started his employment with Google at that point.", "tokens": [50586, 1409, 702, 11949, 365, 3329, 412, 300, 935, 13, 50696], "temperature": 0.0, "avg_logprob": -0.19984564233998783, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.001064603216946125}, {"id": 853, "seek": 333084, "start": 3337.52, "end": 3341.32, "text": " So I, I don't know, you know, certainly nothing had been mentioned.", "tokens": [50698, 407, 286, 11, 286, 500, 380, 458, 11, 291, 458, 11, 3297, 1825, 632, 668, 2835, 13, 50888], "temperature": 0.0, "avg_logprob": -0.19984564233998783, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.001064603216946125}, {"id": 854, "seek": 333084, "start": 3342.56, "end": 3348.1600000000003, "text": " So I, you know, I admired him from afar with LLVM and Swift and whatever.", "tokens": [50950, 407, 286, 11, 291, 458, 11, 286, 39987, 796, 490, 41795, 365, 441, 43, 53, 44, 293, 25539, 293, 2035, 13, 51230], "temperature": 0.0, "avg_logprob": -0.19984564233998783, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.001064603216946125}, {"id": 855, "seek": 333084, "start": 3348.1600000000003, "end": 3353.2000000000003, "text": " And so I saw him walk into the courtyard at, at Google.", "tokens": [51230, 400, 370, 286, 1866, 796, 1792, 666, 264, 41364, 412, 11, 412, 3329, 13, 51482], "temperature": 0.0, "avg_logprob": -0.19984564233998783, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.001064603216946125}, {"id": 856, "seek": 333084, "start": 3353.48, "end": 3356.44, "text": " It's just like, oh, man, Chris Lander.", "tokens": [51496, 467, 311, 445, 411, 11, 1954, 11, 587, 11, 6688, 441, 4483, 13, 51644], "temperature": 0.0, "avg_logprob": -0.19984564233998783, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.001064603216946125}, {"id": 857, "seek": 333084, "start": 3357.6000000000004, "end": 3360.44, "text": " I wonder if he would lower his standards enough to talk to me.", "tokens": [51702, 286, 2441, 498, 415, 576, 3126, 702, 7787, 1547, 281, 751, 281, 385, 13, 51844], "temperature": 0.0, "avg_logprob": -0.19984564233998783, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.001064603216946125}, {"id": 858, "seek": 336084, "start": 3361.28, "end": 3362.48, "text": " Was worth a try.", "tokens": [50386, 3027, 3163, 257, 853, 13, 50446], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 859, "seek": 336084, "start": 3362.88, "end": 3365.8, "text": " So I caught up my carriage because like he, nobody was talking to him.", "tokens": [50466, 407, 286, 5415, 493, 452, 31811, 570, 411, 415, 11, 5079, 390, 1417, 281, 796, 13, 50612], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 860, "seek": 336084, "start": 3365.92, "end": 3368.48, "text": " He looked a bit lost and I wandered over and it's like, oh, you're Chris", "tokens": [50618, 634, 2956, 257, 857, 2731, 293, 286, 14304, 4073, 670, 293, 309, 311, 411, 11, 1954, 11, 291, 434, 6688, 50746], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 861, "seek": 336084, "start": 3368.48, "end": 3369.04, "text": " Latino, right?", "tokens": [50746, 25422, 11, 558, 30, 50774], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 862, "seek": 336084, "start": 3369.04, "end": 3370.1200000000003, "text": " It's like, what are you doing here?", "tokens": [50774, 467, 311, 411, 11, 437, 366, 291, 884, 510, 30, 50828], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 863, "seek": 336084, "start": 3370.96, "end": 3372.44, "text": " And it's like, yeah, yeah, I am.", "tokens": [50870, 400, 309, 311, 411, 11, 1338, 11, 1338, 11, 286, 669, 13, 50944], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 864, "seek": 336084, "start": 3372.44, "end": 3373.6000000000004, "text": " I'm like, oh, I'm Jeremy Howard.", "tokens": [50944, 286, 478, 411, 11, 1954, 11, 286, 478, 17809, 17626, 13, 51002], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 865, "seek": 336084, "start": 3373.6000000000004, "end": 3375.8, "text": " It's like, oh, did you do some of this AI stuff?", "tokens": [51002, 467, 311, 411, 11, 1954, 11, 630, 291, 360, 512, 295, 341, 7318, 1507, 30, 51112], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 866, "seek": 336084, "start": 3375.8, "end": 3377.76, "text": " And I was like, yeah, yeah, I like this AI stuff.", "tokens": [51112, 400, 286, 390, 411, 11, 1338, 11, 1338, 11, 286, 411, 341, 7318, 1507, 13, 51210], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 867, "seek": 336084, "start": 3378.52, "end": 3379.6400000000003, "text": " Are you doing AI stuff?", "tokens": [51248, 2014, 291, 884, 7318, 1507, 30, 51304], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 868, "seek": 336084, "start": 3379.7200000000003, "end": 3382.52, "text": " It's like, well, I'm thinking about starting to do some AI stuff.", "tokens": [51308, 467, 311, 411, 11, 731, 11, 286, 478, 1953, 466, 2891, 281, 360, 512, 7318, 1507, 13, 51448], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 869, "seek": 336084, "start": 3382.52, "end": 3383.6400000000003, "text": " Yeah, I think it's going to be cool.", "tokens": [51448, 865, 11, 286, 519, 309, 311, 516, 281, 312, 1627, 13, 51504], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 870, "seek": 336084, "start": 3383.6400000000003, "end": 3389.48, "text": " And so, well, so like I spent the next half hour just basically brain", "tokens": [51504, 400, 370, 11, 731, 11, 370, 411, 286, 4418, 264, 958, 1922, 1773, 445, 1936, 3567, 51796], "temperature": 0.0, "avg_logprob": -0.24211305187594506, "compression_ratio": 1.8973509933774835, "no_speech_prob": 0.0017534445505589247}, {"id": 871, "seek": 338948, "start": 3389.52, "end": 3393.4, "text": " dumping all the ways in which AI was stupid to him and he listened", "tokens": [50366, 42224, 439, 264, 2098, 294, 597, 7318, 390, 6631, 281, 796, 293, 415, 13207, 50560], "temperature": 0.0, "avg_logprob": -0.19823741195793437, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.004197200760245323}, {"id": 872, "seek": 338948, "start": 3393.4, "end": 3397.8, "text": " patiently and I thought he probably wasn't even remember or care or whatever.", "tokens": [50560, 49001, 293, 286, 1194, 415, 1391, 2067, 380, 754, 1604, 420, 1127, 420, 2035, 13, 50780], "temperature": 0.0, "avg_logprob": -0.19823741195793437, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.004197200760245323}, {"id": 873, "seek": 338948, "start": 3397.8, "end": 3402.84, "text": " But yeah, then I kind of like, I guess I re-caught up with him a few months", "tokens": [50780, 583, 1338, 11, 550, 286, 733, 295, 411, 11, 286, 2041, 286, 319, 12, 496, 1599, 493, 365, 796, 257, 1326, 2493, 51032], "temperature": 0.0, "avg_logprob": -0.19823741195793437, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.004197200760245323}, {"id": 874, "seek": 338948, "start": 3402.84, "end": 3405.6, "text": " later and it's like, I've been thinking about everything you said in that", "tokens": [51032, 1780, 293, 309, 311, 411, 11, 286, 600, 668, 1953, 466, 1203, 291, 848, 294, 300, 51170], "temperature": 0.0, "avg_logprob": -0.19823741195793437, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.004197200760245323}, {"id": 875, "seek": 338948, "start": 3405.6, "end": 3410.32, "text": " conversation and he like narrated back his response to every part of it, the", "tokens": [51170, 3761, 293, 415, 411, 6397, 770, 646, 702, 4134, 281, 633, 644, 295, 309, 11, 264, 51406], "temperature": 0.0, "avg_logprob": -0.19823741195793437, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.004197200760245323}, {"id": 876, "seek": 338948, "start": 3410.32, "end": 3411.52, "text": " projects he was planning to do.", "tokens": [51406, 4455, 415, 390, 5038, 281, 360, 13, 51466], "temperature": 0.0, "avg_logprob": -0.19823741195793437, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.004197200760245323}, {"id": 877, "seek": 338948, "start": 3411.52, "end": 3414.8, "text": " And it's just like, oh, this dude follows up, holy shit.", "tokens": [51466, 400, 309, 311, 445, 411, 11, 1954, 11, 341, 6449, 10002, 493, 11, 10622, 4611, 13, 51630], "temperature": 0.0, "avg_logprob": -0.19823741195793437, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.004197200760245323}, {"id": 878, "seek": 338948, "start": 3416.84, "end": 3418.2400000000002, "text": " And I was like, wow, okay.", "tokens": [51732, 400, 286, 390, 411, 11, 6076, 11, 1392, 13, 51802], "temperature": 0.0, "avg_logprob": -0.19823741195793437, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.004197200760245323}, {"id": 879, "seek": 341824, "start": 3418.24, "end": 3421.8799999999997, "text": " And he was like, yeah, so we're going to create this new thing called Swift", "tokens": [50364, 400, 415, 390, 411, 11, 1338, 11, 370, 321, 434, 516, 281, 1884, 341, 777, 551, 1219, 25539, 50546], "temperature": 0.0, "avg_logprob": -0.23678816370217196, "compression_ratio": 1.7994011976047903, "no_speech_prob": 0.0009694848558865488}, {"id": 880, "seek": 341824, "start": 3421.8799999999997, "end": 3425.08, "text": " for TensorFlow and it's going to be like, it's going to be a compiler", "tokens": [50546, 337, 37624, 293, 309, 311, 516, 281, 312, 411, 11, 309, 311, 516, 281, 312, 257, 31958, 50706], "temperature": 0.0, "avg_logprob": -0.23678816370217196, "compression_ratio": 1.7994011976047903, "no_speech_prob": 0.0009694848558865488}, {"id": 881, "seek": 341824, "start": 3425.08, "end": 3428.2799999999997, "text": " with auto differentiation built in and blah, blah, blah, blah.", "tokens": [50706, 365, 8399, 38902, 3094, 294, 293, 12288, 11, 12288, 11, 12288, 11, 12288, 13, 50866], "temperature": 0.0, "avg_logprob": -0.23678816370217196, "compression_ratio": 1.7994011976047903, "no_speech_prob": 0.0009694848558865488}, {"id": 882, "seek": 341824, "start": 3428.2799999999997, "end": 3430.16, "text": " And I say, wait, why would that help?", "tokens": [50866, 400, 286, 584, 11, 1699, 11, 983, 576, 300, 854, 30, 50960], "temperature": 0.0, "avg_logprob": -0.23678816370217196, "compression_ratio": 1.7994011976047903, "no_speech_prob": 0.0009694848558865488}, {"id": 883, "seek": 341824, "start": 3430.16, "end": 3432.8399999999997, "text": " You know, why would you, and he was like, okay, with a compiler during the", "tokens": [50960, 509, 458, 11, 983, 576, 291, 11, 293, 415, 390, 411, 11, 1392, 11, 365, 257, 31958, 1830, 264, 51094], "temperature": 0.0, "avg_logprob": -0.23678816370217196, "compression_ratio": 1.7994011976047903, "no_speech_prob": 0.0009694848558865488}, {"id": 884, "seek": 341824, "start": 3432.8399999999997, "end": 3436.4799999999996, "text": " forward pass, you don't have to worry about saving context, you know,", "tokens": [51094, 2128, 1320, 11, 291, 500, 380, 362, 281, 3292, 466, 6816, 4319, 11, 291, 458, 11, 51276], "temperature": 0.0, "avg_logprob": -0.23678816370217196, "compression_ratio": 1.7994011976047903, "no_speech_prob": 0.0009694848558865488}, {"id": 885, "seek": 341824, "start": 3436.4799999999996, "end": 3439.2, "text": " because a lot of it will be optimized in the backward, but I was like, oh my God.", "tokens": [51276, 570, 257, 688, 295, 309, 486, 312, 26941, 294, 264, 23897, 11, 457, 286, 390, 411, 11, 1954, 452, 1265, 13, 51412], "temperature": 0.0, "avg_logprob": -0.23678816370217196, "compression_ratio": 1.7994011976047903, "no_speech_prob": 0.0009694848558865488}, {"id": 886, "seek": 341824, "start": 3439.7999999999997, "end": 3441.52, "text": " Because I didn't really know much about compilers.", "tokens": [51442, 1436, 286, 994, 380, 534, 458, 709, 466, 715, 388, 433, 13, 51528], "temperature": 0.0, "avg_logprob": -0.23678816370217196, "compression_ratio": 1.7994011976047903, "no_speech_prob": 0.0009694848558865488}, {"id": 887, "seek": 341824, "start": 3442.04, "end": 3446.2, "text": " You know, I spent enough to kind of like understand the ideas, but it hadn't", "tokens": [51554, 509, 458, 11, 286, 4418, 1547, 281, 733, 295, 411, 1223, 264, 3487, 11, 457, 309, 8782, 380, 51762], "temperature": 0.0, "avg_logprob": -0.23678816370217196, "compression_ratio": 1.7994011976047903, "no_speech_prob": 0.0009694848558865488}, {"id": 888, "seek": 344620, "start": 3446.2, "end": 3450.24, "text": " occurred to me that a compiler basically solves a lot of the problems", "tokens": [50364, 11068, 281, 385, 300, 257, 31958, 1936, 39890, 257, 688, 295, 264, 2740, 50566], "temperature": 0.0, "avg_logprob": -0.16581487655639648, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0035931230522692204}, {"id": 889, "seek": 344620, "start": 3450.24, "end": 3451.9199999999996, "text": " we have as end users.", "tokens": [50566, 321, 362, 382, 917, 5022, 13, 50650], "temperature": 0.0, "avg_logprob": -0.16581487655639648, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0035931230522692204}, {"id": 890, "seek": 344620, "start": 3452.6, "end": 3453.8799999999997, "text": " I was like, wow, that's amazing.", "tokens": [50684, 286, 390, 411, 11, 6076, 11, 300, 311, 2243, 13, 50748], "temperature": 0.0, "avg_logprob": -0.16581487655639648, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0035931230522692204}, {"id": 891, "seek": 344620, "start": 3453.8799999999997, "end": 3457.56, "text": " Okay, you do know, right, that nobody's going to use this unless it's like usable.", "tokens": [50748, 1033, 11, 291, 360, 458, 11, 558, 11, 300, 5079, 311, 516, 281, 764, 341, 5969, 309, 311, 411, 29975, 13, 50932], "temperature": 0.0, "avg_logprob": -0.16581487655639648, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0035931230522692204}, {"id": 892, "seek": 344620, "start": 3457.8799999999997, "end": 3459.68, "text": " It's like, yeah, I know, right?", "tokens": [50948, 467, 311, 411, 11, 1338, 11, 286, 458, 11, 558, 30, 51038], "temperature": 0.0, "avg_logprob": -0.16581487655639648, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0035931230522692204}, {"id": 893, "seek": 344620, "start": 3459.68, "end": 3462.24, "text": " So I was thinking you should create like a fast AI for this.", "tokens": [51038, 407, 286, 390, 1953, 291, 820, 1884, 411, 257, 2370, 7318, 337, 341, 13, 51166], "temperature": 0.0, "avg_logprob": -0.16581487655639648, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0035931230522692204}, {"id": 894, "seek": 344620, "start": 3462.3599999999997, "end": 3466.24, "text": " So, okay, but I don't even know Swift.", "tokens": [51172, 407, 11, 1392, 11, 457, 286, 500, 380, 754, 458, 25539, 13, 51366], "temperature": 0.0, "avg_logprob": -0.16581487655639648, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0035931230522692204}, {"id": 895, "seek": 344620, "start": 3466.3999999999996, "end": 3469.8399999999997, "text": " And it's like, well, why don't you start learning it?", "tokens": [51374, 400, 309, 311, 411, 11, 731, 11, 983, 500, 380, 291, 722, 2539, 309, 30, 51546], "temperature": 0.0, "avg_logprob": -0.16581487655639648, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0035931230522692204}, {"id": 896, "seek": 344620, "start": 3470.04, "end": 3472.04, "text": " And if you have any questions, ask me.", "tokens": [51556, 400, 498, 291, 362, 604, 1651, 11, 1029, 385, 13, 51656], "temperature": 0.0, "avg_logprob": -0.16581487655639648, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0035931230522692204}, {"id": 897, "seek": 347204, "start": 3472.64, "end": 3473.72, "text": " It's just like, holy shit.", "tokens": [50394, 467, 311, 445, 411, 11, 10622, 4611, 13, 50448], "temperature": 0.0, "avg_logprob": -0.17087415664915054, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.012050427496433258}, {"id": 898, "seek": 347204, "start": 3473.8, "end": 3478.52, "text": " Like, not only is Chris Latner lowered his standards enough to talk to me, but", "tokens": [50452, 1743, 11, 406, 787, 307, 6688, 7354, 1193, 28466, 702, 7787, 1547, 281, 751, 281, 385, 11, 457, 50688], "temperature": 0.0, "avg_logprob": -0.17087415664915054, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.012050427496433258}, {"id": 899, "seek": 347204, "start": 3478.52, "end": 3482.32, "text": " he's offering me personal tutoring on the programming language that he made.", "tokens": [50688, 415, 311, 8745, 385, 2973, 44410, 322, 264, 9410, 2856, 300, 415, 1027, 13, 50878], "temperature": 0.0, "avg_logprob": -0.17087415664915054, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.012050427496433258}, {"id": 900, "seek": 347204, "start": 3482.7599999999998, "end": 3485.08, "text": " So I was just like, I'm not going to let him down.", "tokens": [50900, 407, 286, 390, 445, 411, 11, 286, 478, 406, 516, 281, 718, 796, 760, 13, 51016], "temperature": 0.0, "avg_logprob": -0.17087415664915054, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.012050427496433258}, {"id": 901, "seek": 347204, "start": 3485.64, "end": 3490.04, "text": " So I spent like the next two months, like just nerding out on Swift.", "tokens": [51044, 407, 286, 4418, 411, 264, 958, 732, 2493, 11, 411, 445, 18219, 3584, 484, 322, 25539, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17087415664915054, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.012050427496433258}, {"id": 902, "seek": 347204, "start": 3490.04, "end": 3495.56, "text": " And it was just before Christmas that I kind of like started writing down what I'd learned.", "tokens": [51264, 400, 309, 390, 445, 949, 5272, 300, 286, 733, 295, 411, 1409, 3579, 760, 437, 286, 1116, 3264, 13, 51540], "temperature": 0.0, "avg_logprob": -0.17087415664915054, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.012050427496433258}, {"id": 903, "seek": 347204, "start": 3496.64, "end": 3501.68, "text": " So I wrote a couple of blog posts on like, okay, this is like my attempt to", "tokens": [51594, 407, 286, 4114, 257, 1916, 295, 6968, 12300, 322, 411, 11, 1392, 11, 341, 307, 411, 452, 5217, 281, 51846], "temperature": 0.0, "avg_logprob": -0.17087415664915054, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.012050427496433258}, {"id": 904, "seek": 350168, "start": 3501.68, "end": 3503.48, "text": " do a numeric programming in Swift.", "tokens": [50364, 360, 257, 7866, 299, 9410, 294, 25539, 13, 50454], "temperature": 0.0, "avg_logprob": -0.22339716965590067, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0006459210417233407}, {"id": 905, "seek": 350168, "start": 3504.24, "end": 3508.9199999999996, "text": " And these are all the challenges I had and the some of the issues I had with like", "tokens": [50492, 400, 613, 366, 439, 264, 4759, 286, 632, 293, 264, 512, 295, 264, 2663, 286, 632, 365, 411, 50726], "temperature": 0.0, "avg_logprob": -0.22339716965590067, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0006459210417233407}, {"id": 906, "seek": 350168, "start": 3510.2, "end": 3512.2, "text": " making things properly performant.", "tokens": [50790, 1455, 721, 6108, 2042, 394, 13, 50890], "temperature": 0.0, "avg_logprob": -0.22339716965590067, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0006459210417233407}, {"id": 907, "seek": 350168, "start": 3512.3199999999997, "end": 3515.6, "text": " And here are some libraries I wrote and I sent it to Chris and I was like,", "tokens": [50896, 400, 510, 366, 512, 15148, 286, 4114, 293, 286, 2279, 309, 281, 6688, 293, 286, 390, 411, 11, 51060], "temperature": 0.0, "avg_logprob": -0.22339716965590067, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0006459210417233407}, {"id": 908, "seek": 350168, "start": 3515.6, "end": 3519.12, "text": " I hope he's not too disappointed with me, you know, because that would be the worst.", "tokens": [51060, 286, 1454, 415, 311, 406, 886, 13856, 365, 385, 11, 291, 458, 11, 570, 300, 576, 312, 264, 5855, 13, 51236], "temperature": 0.0, "avg_logprob": -0.22339716965590067, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0006459210417233407}, {"id": 909, "seek": 350168, "start": 3519.9199999999996, "end": 3523.2, "text": " It's like, you know, and I was also like, I was like, I hope he doesn't", "tokens": [51276, 467, 311, 411, 11, 291, 458, 11, 293, 286, 390, 611, 411, 11, 286, 390, 411, 11, 286, 1454, 415, 1177, 380, 51440], "temperature": 0.0, "avg_logprob": -0.22339716965590067, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0006459210417233407}, {"id": 910, "seek": 350168, "start": 3523.3199999999997, "end": 3527.08, "text": " dislike the fact that I've, you know, didn't love everything.", "tokens": [51446, 26006, 264, 1186, 300, 286, 600, 11, 291, 458, 11, 994, 380, 959, 1203, 13, 51634], "temperature": 0.0, "avg_logprob": -0.22339716965590067, "compression_ratio": 1.7943548387096775, "no_speech_prob": 0.0006459210417233407}, {"id": 911, "seek": 352708, "start": 3528.04, "end": 3530.96, "text": " And yeah, he was like, oh, thanks for sending me that.", "tokens": [50412, 400, 1338, 11, 415, 390, 411, 11, 1954, 11, 3231, 337, 7750, 385, 300, 13, 50558], "temperature": 0.0, "avg_logprob": -0.3480055380187579, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.006688104011118412}, {"id": 912, "seek": 352708, "start": 3530.96, "end": 3532.48, "text": " Let's get on a call and talk about it.", "tokens": [50558, 961, 311, 483, 322, 257, 818, 293, 751, 466, 309, 13, 50634], "temperature": 0.0, "avg_logprob": -0.3480055380187579, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.006688104011118412}, {"id": 913, "seek": 352708, "start": 3532.48, "end": 3534.52, "text": " And we spoke and he was like, this is amazing.", "tokens": [50634, 400, 321, 7179, 293, 415, 390, 411, 11, 341, 307, 2243, 13, 50736], "temperature": 0.0, "avg_logprob": -0.3480055380187579, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.006688104011118412}, {"id": 914, "seek": 352708, "start": 3534.88, "end": 3536.52, "text": " I can't believe that you made this.", "tokens": [50754, 286, 393, 380, 1697, 300, 291, 1027, 341, 13, 50836], "temperature": 0.0, "avg_logprob": -0.3480055380187579, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.006688104011118412}, {"id": 915, "seek": 352708, "start": 3536.52, "end": 3538.2, "text": " This is exactly what Swift needs.", "tokens": [50836, 639, 307, 2293, 437, 25539, 2203, 13, 50920], "temperature": 0.0, "avg_logprob": -0.3480055380187579, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.006688104011118412}, {"id": 916, "seek": 352708, "start": 3538.2, "end": 3542.44, "text": " And he was like, and so like somebody set up like a new Swift,", "tokens": [50920, 400, 415, 390, 411, 11, 293, 370, 411, 2618, 992, 493, 411, 257, 777, 25539, 11, 51132], "temperature": 0.0, "avg_logprob": -0.3480055380187579, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.006688104011118412}, {"id": 917, "seek": 352708, "start": 3543.64, "end": 3546.2799999999997, "text": " I don't remember what they call them, the equivalent of a pep, you know,", "tokens": [51192, 286, 500, 380, 1604, 437, 436, 818, 552, 11, 264, 10344, 295, 257, 520, 79, 11, 291, 458, 11, 51324], "temperature": 0.0, "avg_logprob": -0.3480055380187579, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.006688104011118412}, {"id": 918, "seek": 352708, "start": 3546.2799999999997, "end": 3549.68, "text": " kind of IFC thing of like, oh, you know, let's look at how we can implement", "tokens": [51324, 733, 295, 286, 18671, 551, 295, 411, 11, 1954, 11, 291, 458, 11, 718, 311, 574, 412, 577, 321, 393, 4445, 51494], "temperature": 0.0, "avg_logprob": -0.3480055380187579, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.006688104011118412}, {"id": 919, "seek": 352708, "start": 3549.68, "end": 3551.2799999999997, "text": " Jeremy's ideas in the language.", "tokens": [51494, 17809, 311, 3487, 294, 264, 2856, 13, 51574], "temperature": 0.0, "avg_logprob": -0.3480055380187579, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.006688104011118412}, {"id": 920, "seek": 352708, "start": 3551.2799999999997, "end": 3553.44, "text": " And he's just like, oh, wow.", "tokens": [51574, 400, 415, 311, 445, 411, 11, 1954, 11, 6076, 13, 51682], "temperature": 0.0, "avg_logprob": -0.3480055380187579, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.006688104011118412}, {"id": 921, "seek": 355344, "start": 3553.7200000000003, "end": 3561.2000000000003, "text": " And so, yeah, you know, so, you know, and then we ended up like literally", "tokens": [50378, 400, 370, 11, 1338, 11, 291, 458, 11, 370, 11, 291, 458, 11, 293, 550, 321, 4590, 493, 411, 3736, 50752], "temperature": 0.0, "avg_logprob": -0.23144630591074625, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.0016218619421124458}, {"id": 922, "seek": 355344, "start": 3561.2000000000003, "end": 3564.92, "text": " teaching some lessons together about Swift for TensorFlow.", "tokens": [50752, 4571, 512, 8820, 1214, 466, 25539, 337, 37624, 13, 50938], "temperature": 0.0, "avg_logprob": -0.23144630591074625, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.0016218619421124458}, {"id": 923, "seek": 355344, "start": 3564.92, "end": 3572.28, "text": " And we built a fast AI kind of equivalent with him and his team.", "tokens": [50938, 400, 321, 3094, 257, 2370, 7318, 733, 295, 10344, 365, 796, 293, 702, 1469, 13, 51306], "temperature": 0.0, "avg_logprob": -0.23144630591074625, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.0016218619421124458}, {"id": 924, "seek": 355344, "start": 3572.28, "end": 3573.2400000000002, "text": " It's so much fun.", "tokens": [51306, 467, 311, 370, 709, 1019, 13, 51354], "temperature": 0.0, "avg_logprob": -0.23144630591074625, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.0016218619421124458}, {"id": 925, "seek": 355344, "start": 3574.44, "end": 3577.64, "text": " Then in the end, you know, Google didn't follow through just fair enough,", "tokens": [51414, 1396, 294, 264, 917, 11, 291, 458, 11, 3329, 994, 380, 1524, 807, 445, 3143, 1547, 11, 51574], "temperature": 0.0, "avg_logprob": -0.23144630591074625, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.0016218619421124458}, {"id": 926, "seek": 355344, "start": 3577.64, "end": 3582.92, "text": " like asking everybody to learn a new programming language is going to be tough.", "tokens": [51574, 411, 3365, 2201, 281, 1466, 257, 777, 9410, 2856, 307, 516, 281, 312, 4930, 13, 51838], "temperature": 0.0, "avg_logprob": -0.23144630591074625, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.0016218619421124458}, {"id": 927, "seek": 358292, "start": 3582.96, "end": 3586.28, "text": " But like it was very obvious, very, very obvious at that time that TensorFlow", "tokens": [50366, 583, 411, 309, 390, 588, 6322, 11, 588, 11, 588, 6322, 412, 300, 565, 300, 37624, 50532], "temperature": 0.0, "avg_logprob": -0.23125238238640553, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0003352588100824505}, {"id": 928, "seek": 358292, "start": 3586.28, "end": 3592.7200000000003, "text": " 2 is going to be a failure, you know, and so this felt like, okay, I, you know,", "tokens": [50532, 568, 307, 516, 281, 312, 257, 7763, 11, 291, 458, 11, 293, 370, 341, 2762, 411, 11, 1392, 11, 286, 11, 291, 458, 11, 50854], "temperature": 0.0, "avg_logprob": -0.23125238238640553, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0003352588100824505}, {"id": 929, "seek": 358292, "start": 3593.88, "end": 3595.08, "text": " well, you know, what are you going to do?", "tokens": [50912, 731, 11, 291, 458, 11, 437, 366, 291, 516, 281, 360, 30, 50972], "temperature": 0.0, "avg_logprob": -0.23125238238640553, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0003352588100824505}, {"id": 930, "seek": 358292, "start": 3595.4, "end": 3601.96, "text": " Like, you can't focus on TensorFlow 2 because it's not going to, like it's", "tokens": [50988, 1743, 11, 291, 393, 380, 1879, 322, 37624, 568, 570, 309, 311, 406, 516, 281, 11, 411, 309, 311, 51316], "temperature": 0.0, "avg_logprob": -0.23125238238640553, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0003352588100824505}, {"id": 931, "seek": 358292, "start": 3601.96, "end": 3602.48, "text": " not working.", "tokens": [51316, 406, 1364, 13, 51342], "temperature": 0.0, "avg_logprob": -0.23125238238640553, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0003352588100824505}, {"id": 932, "seek": 358292, "start": 3602.48, "end": 3606.6800000000003, "text": " It's never going to work, you know, nobody at Google is using it internally.", "tokens": [51342, 467, 311, 1128, 516, 281, 589, 11, 291, 458, 11, 5079, 412, 3329, 307, 1228, 309, 19501, 13, 51552], "temperature": 0.0, "avg_logprob": -0.23125238238640553, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0003352588100824505}, {"id": 933, "seek": 360668, "start": 3607.68, "end": 3612.68, "text": " So, you know, in the end, Chris left, you know, Swift for TensorFlow got archived.", "tokens": [50414, 407, 11, 291, 458, 11, 294, 264, 917, 11, 6688, 1411, 11, 291, 458, 11, 25539, 337, 37624, 658, 3912, 3194, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2504342803955078, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.012817073613405228}, {"id": 934, "seek": 360668, "start": 3613.68, "end": 3615.16, "text": " There was no backup plan.", "tokens": [50714, 821, 390, 572, 14807, 1393, 13, 50788], "temperature": 0.0, "avg_logprob": -0.2504342803955078, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.012817073613405228}, {"id": 935, "seek": 360668, "start": 3615.16, "end": 3620.68, "text": " So I kind of felt like Google was kind of screwed, you know, and Chris went and", "tokens": [50788, 407, 286, 733, 295, 2762, 411, 3329, 390, 733, 295, 20331, 11, 291, 458, 11, 293, 6688, 1437, 293, 51064], "temperature": 0.0, "avg_logprob": -0.2504342803955078, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.012817073613405228}, {"id": 936, "seek": 360668, "start": 3620.68, "end": 3623.68, "text": " did something else, but we kept talking and I was like, look, Chris, you know,", "tokens": [51064, 630, 746, 1646, 11, 457, 321, 4305, 1417, 293, 286, 390, 411, 11, 574, 11, 6688, 11, 291, 458, 11, 51214], "temperature": 0.0, "avg_logprob": -0.2504342803955078, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.012817073613405228}, {"id": 937, "seek": 360668, "start": 3625.68, "end": 3628.68, "text": " you've got to be your own boss, man, because like, you know, you've got the ideas,", "tokens": [51314, 291, 600, 658, 281, 312, 428, 1065, 5741, 11, 587, 11, 570, 411, 11, 291, 458, 11, 291, 600, 658, 264, 3487, 11, 51464], "temperature": 0.0, "avg_logprob": -0.2504342803955078, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.012817073613405228}, {"id": 938, "seek": 360668, "start": 3629.68, "end": 3633.68, "text": " you know, like only you've got the ideas, you know, and if your ideas are implemented,", "tokens": [51514, 291, 458, 11, 411, 787, 291, 600, 658, 264, 3487, 11, 291, 458, 11, 293, 498, 428, 3487, 366, 12270, 11, 51714], "temperature": 0.0, "avg_logprob": -0.2504342803955078, "compression_ratio": 1.8675213675213675, "no_speech_prob": 0.012817073613405228}, {"id": 939, "seek": 363368, "start": 3634.68, "end": 3640.68, "text": " we'd all be so much better off because like, Python's the best of a whole bunch", "tokens": [50414, 321, 1116, 439, 312, 370, 709, 1101, 766, 570, 411, 11, 15329, 311, 264, 1151, 295, 257, 1379, 3840, 50714], "temperature": 0.0, "avg_logprob": -0.15560637729268678, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.004753724206238985}, {"id": 940, "seek": 363368, "start": 3640.68, "end": 3645.68, "text": " of shit, you know, like, I would, it's amazing, but it's awful, you know, compared", "tokens": [50714, 295, 4611, 11, 291, 458, 11, 411, 11, 286, 576, 11, 309, 311, 2243, 11, 457, 309, 311, 11232, 11, 291, 458, 11, 5347, 50964], "temperature": 0.0, "avg_logprob": -0.15560637729268678, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.004753724206238985}, {"id": 941, "seek": 363368, "start": 3645.68, "end": 3646.68, "text": " to what it could be.", "tokens": [50964, 281, 437, 309, 727, 312, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15560637729268678, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.004753724206238985}, {"id": 942, "seek": 363368, "start": 3647.68, "end": 3651.68, "text": " Anyway, so eventually a few years later, he called me up and he was like, Jeremy,", "tokens": [51064, 5684, 11, 370, 4728, 257, 1326, 924, 1780, 11, 415, 1219, 385, 493, 293, 415, 390, 411, 11, 17809, 11, 51264], "temperature": 0.0, "avg_logprob": -0.15560637729268678, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.004753724206238985}, {"id": 943, "seek": 363368, "start": 3651.68, "end": 3652.68, "text": " I've taken your advice.", "tokens": [51264, 286, 600, 2726, 428, 5192, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15560637729268678, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.004753724206238985}, {"id": 944, "seek": 363368, "start": 3653.68, "end": 3657.68, "text": " I've started a company so much like, oh my God, so we got to create a new language.", "tokens": [51364, 286, 600, 1409, 257, 2237, 370, 709, 411, 11, 1954, 452, 1265, 11, 370, 321, 658, 281, 1884, 257, 777, 2856, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15560637729268678, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.004753724206238985}, {"id": 945, "seek": 363368, "start": 3657.68, "end": 3659.68, "text": " We're going to create a new infrastructure.", "tokens": [51564, 492, 434, 516, 281, 1884, 257, 777, 6896, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15560637729268678, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.004753724206238985}, {"id": 946, "seek": 363368, "start": 3659.68, "end": 3662.68, "text": " It's going to build, it's going to have all the stuff we've talked about.", "tokens": [51664, 467, 311, 516, 281, 1322, 11, 309, 311, 516, 281, 362, 439, 264, 1507, 321, 600, 2825, 466, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15560637729268678, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.004753724206238985}, {"id": 947, "seek": 366368, "start": 3663.68, "end": 3664.68, "text": " And it's like, oh, wow.", "tokens": [50364, 400, 309, 311, 411, 11, 1954, 11, 6076, 13, 50414], "temperature": 0.0, "avg_logprob": -0.16536959781441637, "compression_ratio": 1.4805825242718447, "no_speech_prob": 0.0007095249020494521}, {"id": 948, "seek": 366368, "start": 3665.68, "end": 3668.68, "text": " So that's what Mojo is.", "tokens": [50464, 407, 300, 311, 437, 3335, 5134, 307, 13, 50614], "temperature": 0.0, "avg_logprob": -0.16536959781441637, "compression_ratio": 1.4805825242718447, "no_speech_prob": 0.0007095249020494521}, {"id": 949, "seek": 366368, "start": 3669.68, "end": 3677.68, "text": " And so Mojo is like, you know, building on all the stuff that Chris has figured out", "tokens": [50664, 400, 370, 3335, 5134, 307, 411, 11, 291, 458, 11, 2390, 322, 439, 264, 1507, 300, 6688, 575, 8932, 484, 51064], "temperature": 0.0, "avg_logprob": -0.16536959781441637, "compression_ratio": 1.4805825242718447, "no_speech_prob": 0.0007095249020494521}, {"id": 950, "seek": 366368, "start": 3677.68, "end": 3683.68, "text": " over, I mean, really from when he did his PhD thesis, which developed LLVM onwards,", "tokens": [51064, 670, 11, 286, 914, 11, 534, 490, 562, 415, 630, 702, 14476, 22288, 11, 597, 4743, 441, 43, 53, 44, 34230, 11, 51364], "temperature": 0.0, "avg_logprob": -0.16536959781441637, "compression_ratio": 1.4805825242718447, "no_speech_prob": 0.0007095249020494521}, {"id": 951, "seek": 366368, "start": 3683.68, "end": 3690.68, "text": " you know, in Swift and MLAR, you know, the TensorFlow runtime engine, which is very good.", "tokens": [51364, 291, 458, 11, 294, 25539, 293, 21601, 1899, 11, 291, 458, 11, 264, 37624, 34474, 2848, 11, 597, 307, 588, 665, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16536959781441637, "compression_ratio": 1.4805825242718447, "no_speech_prob": 0.0007095249020494521}, {"id": 952, "seek": 369068, "start": 3690.68, "end": 3693.68, "text": " You know, that was something that he built and has lasted.", "tokens": [50364, 509, 458, 11, 300, 390, 746, 300, 415, 3094, 293, 575, 21116, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1606949244704202, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0034810020588338375}, {"id": 953, "seek": 369068, "start": 3695.68, "end": 3697.68, "text": " So yeah, I'm pumped about that.", "tokens": [50614, 407, 1338, 11, 286, 478, 27774, 466, 300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1606949244704202, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0034810020588338375}, {"id": 954, "seek": 369068, "start": 3697.68, "end": 3699.68, "text": " I mean, it's very speculative.", "tokens": [50714, 286, 914, 11, 309, 311, 588, 49415, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1606949244704202, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0034810020588338375}, {"id": 955, "seek": 369068, "start": 3699.68, "end": 3701.68, "text": " Creating a whole new language is tough.", "tokens": [50814, 40002, 257, 1379, 777, 2856, 307, 4930, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1606949244704202, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0034810020588338375}, {"id": 956, "seek": 369068, "start": 3701.68, "end": 3706.68, "text": " I mean, Chris has done it before and he's created a whole C++ compiler amongst other things.", "tokens": [50914, 286, 914, 11, 6688, 575, 1096, 309, 949, 293, 415, 311, 2942, 257, 1379, 383, 25472, 31958, 12918, 661, 721, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1606949244704202, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0034810020588338375}, {"id": 957, "seek": 369068, "start": 3707.68, "end": 3708.68, "text": " Looking pretty hopeful.", "tokens": [51214, 11053, 1238, 20531, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1606949244704202, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0034810020588338375}, {"id": 958, "seek": 369068, "start": 3709.68, "end": 3713.68, "text": " I mean, I hope it works because, you know, I mean.", "tokens": [51314, 286, 914, 11, 286, 1454, 309, 1985, 570, 11, 291, 458, 11, 286, 914, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1606949244704202, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0034810020588338375}, {"id": 959, "seek": 369068, "start": 3713.68, "end": 3715.68, "text": " You're doing them to quit his job, so.", "tokens": [51514, 509, 434, 884, 552, 281, 10366, 702, 1691, 11, 370, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1606949244704202, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0034810020588338375}, {"id": 960, "seek": 371568, "start": 3716.68, "end": 3722.68, "text": " And I mean, in the meantime, I will say, you know, Google now does have a backup plan, you know,", "tokens": [50414, 400, 286, 914, 11, 294, 264, 14991, 11, 286, 486, 584, 11, 291, 458, 11, 3329, 586, 775, 362, 257, 14807, 1393, 11, 291, 458, 11, 50714], "temperature": 0.0, "avg_logprob": -0.12730425596237183, "compression_ratio": 1.6058631921824105, "no_speech_prob": 0.0023962308187037706}, {"id": 961, "seek": 371568, "start": 3722.68, "end": 3725.68, "text": " they have JAX, which was never a strategy.", "tokens": [50714, 436, 362, 26401, 55, 11, 597, 390, 1128, 257, 5206, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12730425596237183, "compression_ratio": 1.6058631921824105, "no_speech_prob": 0.0023962308187037706}, {"id": 962, "seek": 371568, "start": 3725.68, "end": 3730.68, "text": " It was just a bunch of people who also recognized TensorFlow 2 as shit and they just decided to build something else.", "tokens": [50864, 467, 390, 445, 257, 3840, 295, 561, 567, 611, 9823, 37624, 568, 382, 4611, 293, 436, 445, 3047, 281, 1322, 746, 1646, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12730425596237183, "compression_ratio": 1.6058631921824105, "no_speech_prob": 0.0023962308187037706}, {"id": 963, "seek": 371568, "start": 3731.68, "end": 3735.68, "text": " And for years, my friends in that team were like, don't tell anybody about us because we, you know,", "tokens": [51164, 400, 337, 924, 11, 452, 1855, 294, 300, 1469, 645, 411, 11, 500, 380, 980, 4472, 466, 505, 570, 321, 11, 291, 458, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12730425596237183, "compression_ratio": 1.6058631921824105, "no_speech_prob": 0.0023962308187037706}, {"id": 964, "seek": 371568, "start": 3735.68, "end": 3737.68, "text": " we don't want to be anything but a research project.", "tokens": [51364, 321, 500, 380, 528, 281, 312, 1340, 457, 257, 2132, 1716, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12730425596237183, "compression_ratio": 1.6058631921824105, "no_speech_prob": 0.0023962308187037706}, {"id": 965, "seek": 371568, "start": 3738.68, "end": 3743.68, "text": " So now these poor guys suddenly, they're the great white hope for Google's future.", "tokens": [51514, 407, 586, 613, 4716, 1074, 5800, 11, 436, 434, 264, 869, 2418, 1454, 337, 3329, 311, 2027, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12730425596237183, "compression_ratio": 1.6058631921824105, "no_speech_prob": 0.0023962308187037706}, {"id": 966, "seek": 374368, "start": 3744.68, "end": 3748.68, "text": " And so JAX is, you know, also not terrible, but it's still written in Python.", "tokens": [50414, 400, 370, 26401, 55, 307, 11, 291, 458, 11, 611, 406, 6237, 11, 457, 309, 311, 920, 3720, 294, 15329, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13061437200992665, "compression_ratio": 1.5065502183406114, "no_speech_prob": 0.004197859205305576}, {"id": 967, "seek": 374368, "start": 3748.68, "end": 3757.68, "text": " Like it would be cool if we had all the benefits of JAX but in a language that was designed for those kind of purposes.", "tokens": [50614, 1743, 309, 576, 312, 1627, 498, 321, 632, 439, 264, 5311, 295, 26401, 55, 457, 294, 257, 2856, 300, 390, 4761, 337, 729, 733, 295, 9932, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13061437200992665, "compression_ratio": 1.5065502183406114, "no_speech_prob": 0.004197859205305576}, {"id": 968, "seek": 374368, "start": 3759.68, "end": 3765.68, "text": " So, you know, fingers crossed that, yeah, that Mocho turns out great.", "tokens": [51164, 407, 11, 291, 458, 11, 7350, 14622, 300, 11, 1338, 11, 300, 3335, 5738, 4523, 484, 869, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13061437200992665, "compression_ratio": 1.5065502183406114, "no_speech_prob": 0.004197859205305576}, {"id": 969, "seek": 374368, "start": 3765.68, "end": 3766.68, "text": " Yeah.", "tokens": [51464, 865, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13061437200992665, "compression_ratio": 1.5065502183406114, "no_speech_prob": 0.004197859205305576}, {"id": 970, "seek": 374368, "start": 3767.68, "end": 3771.68, "text": " Any other thoughts on when, where people should be spending their time?", "tokens": [51564, 2639, 661, 4598, 322, 562, 11, 689, 561, 820, 312, 6434, 641, 565, 30, 51764], "temperature": 0.0, "avg_logprob": -0.13061437200992665, "compression_ratio": 1.5065502183406114, "no_speech_prob": 0.004197859205305576}, {"id": 971, "seek": 377168, "start": 3771.68, "end": 3777.68, "text": " So that's more the kind of language framework level than you have the, you know, GGML.", "tokens": [50364, 407, 300, 311, 544, 264, 733, 295, 2856, 8388, 1496, 813, 291, 362, 264, 11, 291, 458, 11, 460, 38, 12683, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17747752977454145, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0034281008411198854}, {"id": 972, "seek": 377168, "start": 3777.68, "end": 3781.68, "text": " Some of these are like quantization-focused kind of model-level things.", "tokens": [50664, 2188, 295, 613, 366, 411, 4426, 2144, 12, 44062, 733, 295, 2316, 12, 12418, 721, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17747752977454145, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0034281008411198854}, {"id": 973, "seek": 377168, "start": 3781.68, "end": 3783.68, "text": " Then you got the hardware people.", "tokens": [50864, 1396, 291, 658, 264, 8837, 561, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17747752977454145, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0034281008411198854}, {"id": 974, "seek": 377168, "start": 3783.68, "end": 3784.68, "text": " It's like a whole other bucket.", "tokens": [50964, 467, 311, 411, 257, 1379, 661, 13058, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17747752977454145, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0034281008411198854}, {"id": 975, "seek": 377168, "start": 3785.68, "end": 3788.68, "text": " Yeah, what are some of the exciting stuff that you're excited about?", "tokens": [51064, 865, 11, 437, 366, 512, 295, 264, 4670, 1507, 300, 291, 434, 2919, 466, 30, 51214], "temperature": 0.0, "avg_logprob": -0.17747752977454145, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0034281008411198854}, {"id": 976, "seek": 377168, "start": 3788.68, "end": 3797.68, "text": " Well, you won't be surprised to hear me say this, but I think fine-tuning, transfer learning is still a hugely underappreciated area.", "tokens": [51214, 1042, 11, 291, 1582, 380, 312, 6100, 281, 1568, 385, 584, 341, 11, 457, 286, 519, 2489, 12, 83, 37726, 11, 5003, 2539, 307, 920, 257, 27417, 833, 1746, 3326, 770, 1859, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17747752977454145, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0034281008411198854}, {"id": 977, "seek": 379768, "start": 3797.68, "end": 3807.68, "text": " So today's zero-shot, few-shot learning equivalent is retrieval augmented generation, you know, RAG.", "tokens": [50364, 407, 965, 311, 4018, 12, 18402, 11, 1326, 12, 18402, 2539, 10344, 307, 19817, 3337, 36155, 5125, 11, 291, 458, 11, 14626, 38, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12996362603229025, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.014726236462593079}, {"id": 978, "seek": 379768, "start": 3807.68, "end": 3811.68, "text": " Which is like, just like few-shot learning is a thing.", "tokens": [50864, 3013, 307, 411, 11, 445, 411, 1326, 12, 18402, 2539, 307, 257, 551, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12996362603229025, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.014726236462593079}, {"id": 979, "seek": 379768, "start": 3811.68, "end": 3812.68, "text": " Like it's a real thing.", "tokens": [51064, 1743, 309, 311, 257, 957, 551, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12996362603229025, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.014726236462593079}, {"id": 980, "seek": 379768, "start": 3812.68, "end": 3813.68, "text": " It's a useful thing.", "tokens": [51114, 467, 311, 257, 4420, 551, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12996362603229025, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.014726236462593079}, {"id": 981, "seek": 379768, "start": 3813.68, "end": 3815.68, "text": " It's not a thing anybody would want to ignore.", "tokens": [51164, 467, 311, 406, 257, 551, 4472, 576, 528, 281, 11200, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12996362603229025, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.014726236462593079}, {"id": 982, "seek": 379768, "start": 3815.68, "end": 3819.68, "text": " Why are people not spending at least as much effort on fine-tuning?", "tokens": [51264, 1545, 366, 561, 406, 6434, 412, 1935, 382, 709, 4630, 322, 2489, 12, 83, 37726, 30, 51464], "temperature": 0.0, "avg_logprob": -0.12996362603229025, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.014726236462593079}, {"id": 983, "seek": 381968, "start": 3819.68, "end": 3825.68, "text": " You know, because, you know, RAG is like such an inefficient hack, really, isn't it?", "tokens": [50364, 509, 458, 11, 570, 11, 291, 458, 11, 14626, 38, 307, 411, 1270, 364, 43495, 10339, 11, 534, 11, 1943, 380, 309, 30, 50664], "temperature": 0.0, "avg_logprob": -0.1375988864023751, "compression_ratio": 1.6330645161290323, "no_speech_prob": 0.04884384199976921}, {"id": 984, "seek": 381968, "start": 3825.68, "end": 3832.68, "text": " It's like, you know, segment up my data in some somewhat arbitrary way.", "tokens": [50664, 467, 311, 411, 11, 291, 458, 11, 9469, 493, 452, 1412, 294, 512, 8344, 23211, 636, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1375988864023751, "compression_ratio": 1.6330645161290323, "no_speech_prob": 0.04884384199976921}, {"id": 985, "seek": 381968, "start": 3832.68, "end": 3835.68, "text": " Embed it, ask questions about that.", "tokens": [51014, 24234, 292, 309, 11, 1029, 1651, 466, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1375988864023751, "compression_ratio": 1.6330645161290323, "no_speech_prob": 0.04884384199976921}, {"id": 986, "seek": 381968, "start": 3835.68, "end": 3843.68, "text": " You know, hope that my embedding model embeds questions in the same bedding space as a paragraph.", "tokens": [51164, 509, 458, 11, 1454, 300, 452, 12240, 3584, 2316, 12240, 82, 1651, 294, 264, 912, 2901, 3584, 1901, 382, 257, 18865, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1375988864023751, "compression_ratio": 1.6330645161290323, "no_speech_prob": 0.04884384199976921}, {"id": 987, "seek": 381968, "start": 3843.68, "end": 3848.68, "text": " Which obviously is not going to, if your question is like, if I've got a whole bunch of archive papers embeddings.", "tokens": [51564, 3013, 2745, 307, 406, 516, 281, 11, 498, 428, 1168, 307, 411, 11, 498, 286, 600, 658, 257, 1379, 3840, 295, 23507, 10577, 12240, 29432, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1375988864023751, "compression_ratio": 1.6330645161290323, "no_speech_prob": 0.04884384199976921}, {"id": 988, "seek": 384868, "start": 3848.68, "end": 3857.68, "text": " And I asked like, what are all the ways in which we can make inference more efficient?", "tokens": [50364, 400, 286, 2351, 411, 11, 437, 366, 439, 264, 2098, 294, 597, 321, 393, 652, 38253, 544, 7148, 30, 50814], "temperature": 0.0, "avg_logprob": -0.11757385730743408, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.004330671392381191}, {"id": 989, "seek": 384868, "start": 3857.68, "end": 3866.68, "text": " The only paragraphs it'll find is like, if there's a review paper that says here's a list of ways to make, you know, inference more efficient.", "tokens": [50814, 440, 787, 48910, 309, 603, 915, 307, 411, 11, 498, 456, 311, 257, 3131, 3035, 300, 1619, 510, 311, 257, 1329, 295, 2098, 281, 652, 11, 291, 458, 11, 38253, 544, 7148, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11757385730743408, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.004330671392381191}, {"id": 990, "seek": 384868, "start": 3866.68, "end": 3867.68, "text": " Doesn't have any of the specifics.", "tokens": [51264, 12955, 380, 362, 604, 295, 264, 28454, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11757385730743408, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.004330671392381191}, {"id": 991, "seek": 384868, "start": 3867.68, "end": 3874.68, "text": " No, it's not going to be like, oh, here's one way, here's one way, here's a different way in different papers, you know.", "tokens": [51314, 883, 11, 309, 311, 406, 516, 281, 312, 411, 11, 1954, 11, 510, 311, 472, 636, 11, 510, 311, 472, 636, 11, 510, 311, 257, 819, 636, 294, 819, 10577, 11, 291, 458, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11757385730743408, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.004330671392381191}, {"id": 992, "seek": 387468, "start": 3874.68, "end": 3886.68, "text": " Yeah, if you fine-tune a model, then all of that information is getting directly incorporated into the weights of your model in a much more efficient and nuanced way.", "tokens": [50364, 865, 11, 498, 291, 2489, 12, 83, 2613, 257, 2316, 11, 550, 439, 295, 300, 1589, 307, 1242, 3838, 21654, 666, 264, 17443, 295, 428, 2316, 294, 257, 709, 544, 7148, 293, 45115, 636, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09942500445307517, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.007814199663698673}, {"id": 993, "seek": 387468, "start": 3886.68, "end": 3889.68, "text": " And then you can use RAG on top of that.", "tokens": [50964, 400, 550, 291, 393, 764, 14626, 38, 322, 1192, 295, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09942500445307517, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.007814199663698673}, {"id": 994, "seek": 387468, "start": 3889.68, "end": 3894.68, "text": " So I think that that's one area that's definitely underappreciated.", "tokens": [51114, 407, 286, 519, 300, 300, 311, 472, 1859, 300, 311, 2138, 833, 1746, 3326, 770, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09942500445307517, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.007814199663698673}, {"id": 995, "seek": 387468, "start": 3894.68, "end": 3900.68, "text": " And also the confluence of like, okay, how do you combine RAG and fine-tuning, for example.", "tokens": [51364, 400, 611, 264, 1497, 40432, 295, 411, 11, 1392, 11, 577, 360, 291, 10432, 14626, 38, 293, 2489, 12, 83, 37726, 11, 337, 1365, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09942500445307517, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.007814199663698673}, {"id": 996, "seek": 390068, "start": 3900.68, "end": 3910.68, "text": " Something that I think a lot of people are uncertain about, and I don't expect you to know either, is that whether or not you can fine-tune new information in.", "tokens": [50364, 6595, 300, 286, 519, 257, 688, 295, 561, 366, 11308, 466, 11, 293, 286, 500, 380, 2066, 291, 281, 458, 2139, 11, 307, 300, 1968, 420, 406, 291, 393, 2489, 12, 83, 2613, 777, 1589, 294, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10211512150655266, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.02367917075753212}, {"id": 997, "seek": 390068, "start": 3910.68, "end": 3916.68, "text": " And I think that that is the focus of some of your open questions and research.", "tokens": [50864, 400, 286, 519, 300, 300, 307, 264, 1879, 295, 512, 295, 428, 1269, 1651, 293, 2132, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10211512150655266, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.02367917075753212}, {"id": 998, "seek": 390068, "start": 3916.68, "end": 3917.68, "text": " But of course you can, right?", "tokens": [51164, 583, 295, 1164, 291, 393, 11, 558, 30, 51214], "temperature": 0.0, "avg_logprob": -0.10211512150655266, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.02367917075753212}, {"id": 999, "seek": 390068, "start": 3917.68, "end": 3918.68, "text": " Because it's additional pre-training.", "tokens": [51214, 1436, 309, 311, 4497, 659, 12, 17227, 1760, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10211512150655266, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.02367917075753212}, {"id": 1000, "seek": 390068, "start": 3918.68, "end": 3922.68, "text": " Obviously you can, because there's no such thing as fine-tuning.", "tokens": [51264, 7580, 291, 393, 11, 570, 456, 311, 572, 1270, 551, 382, 2489, 12, 83, 37726, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10211512150655266, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.02367917075753212}, {"id": 1001, "seek": 390068, "start": 3922.68, "end": 3924.68, "text": " There's only continued pre-training.", "tokens": [51464, 821, 311, 787, 7014, 659, 12, 17227, 1760, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10211512150655266, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.02367917075753212}, {"id": 1002, "seek": 390068, "start": 3924.68, "end": 3929.68, "text": " So fine-tuning is pre-training, like they're literally the same thing.", "tokens": [51564, 407, 2489, 12, 83, 37726, 307, 659, 12, 17227, 1760, 11, 411, 436, 434, 3736, 264, 912, 551, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10211512150655266, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.02367917075753212}, {"id": 1003, "seek": 392968, "start": 3930.68, "end": 3933.68, "text": " So the knowledge got in there in the first place through pre-training.", "tokens": [50414, 407, 264, 3601, 658, 294, 456, 294, 264, 700, 1081, 807, 659, 12, 17227, 1760, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12849962711334229, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0027130639646202326}, {"id": 1004, "seek": 392968, "start": 3933.68, "end": 3937.68, "text": " So how could continuing to pre-train not put more knowledge in?", "tokens": [50564, 407, 577, 727, 9289, 281, 659, 12, 83, 7146, 406, 829, 544, 3601, 294, 30, 50764], "temperature": 0.0, "avg_logprob": -0.12849962711334229, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0027130639646202326}, {"id": 1005, "seek": 392968, "start": 3937.68, "end": 3939.68, "text": " Like it's the same thing.", "tokens": [50764, 1743, 309, 311, 264, 912, 551, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12849962711334229, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0027130639646202326}, {"id": 1006, "seek": 392968, "start": 3940.68, "end": 3944.68, "text": " The problem is just we're really bad at it, because everybody's doing it dumb ways.", "tokens": [50914, 440, 1154, 307, 445, 321, 434, 534, 1578, 412, 309, 11, 570, 2201, 311, 884, 309, 10316, 2098, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12849962711334229, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0027130639646202326}, {"id": 1007, "seek": 392968, "start": 3944.68, "end": 3949.68, "text": " So it's a good question, and it's not just new knowledge, but new capabilities.", "tokens": [51114, 407, 309, 311, 257, 665, 1168, 11, 293, 309, 311, 406, 445, 777, 3601, 11, 457, 777, 10862, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12849962711334229, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0027130639646202326}, {"id": 1008, "seek": 394968, "start": 3950.68, "end": 3957.68, "text": " You know, I think like in my Hackers Guide to LL, into Hackers Guide to LLM's talk, I show simple.", "tokens": [50414, 509, 458, 11, 286, 519, 411, 294, 452, 35170, 433, 18727, 281, 441, 43, 11, 666, 35170, 433, 18727, 281, 441, 43, 44, 311, 751, 11, 286, 855, 2199, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1915367234427974, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.3587901294231415}, {"id": 1009, "seek": 394968, "start": 3957.68, "end": 3965.68, "text": " I mean, it's a funny, that's a simple example, because it doesn't sound it, but like taking a pre-trained based model and getting it to generate SQL.", "tokens": [50764, 286, 914, 11, 309, 311, 257, 4074, 11, 300, 311, 257, 2199, 1365, 11, 570, 309, 1177, 380, 1626, 309, 11, 457, 411, 1940, 257, 659, 12, 17227, 2001, 2361, 2316, 293, 1242, 309, 281, 8460, 19200, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1915367234427974, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.3587901294231415}, {"id": 1010, "seek": 394968, "start": 3965.68, "end": 3968.68, "text": " And it took 15 minutes to train on a single GPU.", "tokens": [51164, 400, 309, 1890, 2119, 2077, 281, 3847, 322, 257, 2167, 18407, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1915367234427974, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.3587901294231415}, {"id": 1011, "seek": 394968, "start": 3968.68, "end": 3975.68, "text": " You know, I think that might surprise people that that capability is actual fingertips.", "tokens": [51314, 509, 458, 11, 286, 519, 300, 1062, 6365, 561, 300, 300, 13759, 307, 3539, 27715, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1915367234427974, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.3587901294231415}, {"id": 1012, "seek": 397568, "start": 3975.68, "end": 3980.68, "text": " And you know, because it was already there, it was just latent in the base model.", "tokens": [50364, 400, 291, 458, 11, 570, 309, 390, 1217, 456, 11, 309, 390, 445, 48994, 294, 264, 3096, 2316, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14040877518144626, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.001409851829521358}, {"id": 1013, "seek": 397568, "start": 3980.68, "end": 3987.68, "text": " Really pushing the boundaries of what you can do with small models, I think is a really interesting question.", "tokens": [50614, 4083, 7380, 264, 13180, 295, 437, 291, 393, 360, 365, 1359, 5245, 11, 286, 519, 307, 257, 534, 1880, 1168, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14040877518144626, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.001409851829521358}, {"id": 1014, "seek": 397568, "start": 3987.68, "end": 3989.68, "text": " Like what can you do with a...", "tokens": [50964, 1743, 437, 393, 291, 360, 365, 257, 485, 51064], "temperature": 0.0, "avg_logprob": -0.14040877518144626, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.001409851829521358}, {"id": 1015, "seek": 397568, "start": 3989.68, "end": 3992.68, "text": " I mean, there isn't much in the way of good small models.", "tokens": [51064, 286, 914, 11, 456, 1943, 380, 709, 294, 264, 636, 295, 665, 1359, 5245, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14040877518144626, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.001409851829521358}, {"id": 1016, "seek": 397568, "start": 3992.68, "end": 4003.68, "text": " A really underappreciated one is a BTLM 3B, which is a like kind of 7B quality 3B model.", "tokens": [51214, 316, 534, 833, 1746, 3326, 770, 472, 307, 257, 31144, 43, 44, 805, 33, 11, 597, 307, 257, 411, 733, 295, 1614, 33, 3125, 805, 33, 2316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14040877518144626, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.001409851829521358}, {"id": 1017, "seek": 400368, "start": 4004.68, "end": 4006.68, "text": " There's not much of the 1-2B range, sadly.", "tokens": [50414, 821, 311, 406, 709, 295, 264, 502, 12, 17, 33, 3613, 11, 22023, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13072555376135786, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0026309071108698845}, {"id": 1018, "seek": 400368, "start": 4006.68, "end": 4016.68, "text": " There are some code ones, but like the fact that there are some really good code ones in that 1-2B range shows you that that's a great size for doing complex tasks well.", "tokens": [50514, 821, 366, 512, 3089, 2306, 11, 457, 411, 264, 1186, 300, 456, 366, 512, 534, 665, 3089, 2306, 294, 300, 502, 12, 17, 33, 3613, 3110, 291, 300, 300, 311, 257, 869, 2744, 337, 884, 3997, 9608, 731, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13072555376135786, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0026309071108698845}, {"id": 1019, "seek": 400368, "start": 4017.68, "end": 4023.68, "text": " There was PHY1 recently, which has been the subject of a little bit of discussion about whether they're trained on benchmarks.", "tokens": [51064, 821, 390, 16530, 56, 16, 3938, 11, 597, 575, 668, 264, 3983, 295, 257, 707, 857, 295, 5017, 466, 1968, 436, 434, 8895, 322, 43751, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13072555376135786, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0026309071108698845}, {"id": 1020, "seek": 400368, "start": 4023.68, "end": 4025.68, "text": " Yeah, PHY1.5 as well.", "tokens": [51364, 865, 11, 16530, 56, 16, 13, 20, 382, 731, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13072555376135786, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0026309071108698845}, {"id": 1021, "seek": 400368, "start": 4025.68, "end": 4029.68, "text": " So that's not a good model yet.", "tokens": [51464, 407, 300, 311, 406, 257, 665, 2316, 1939, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13072555376135786, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0026309071108698845}, {"id": 1022, "seek": 400368, "start": 4030.68, "end": 4031.68, "text": " Why not?", "tokens": [51714, 1545, 406, 30, 51764], "temperature": 0.0, "avg_logprob": -0.13072555376135786, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0026309071108698845}, {"id": 1023, "seek": 403168, "start": 4032.68, "end": 4040.68, "text": " So PHY1 in particular is good at doing a very specific thing, which is creating very small Python snippets.", "tokens": [50414, 407, 16530, 56, 16, 294, 1729, 307, 665, 412, 884, 257, 588, 2685, 551, 11, 597, 307, 4084, 588, 1359, 15329, 35623, 1385, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14298559605390176, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.002888623857870698}, {"id": 1024, "seek": 403168, "start": 4040.68, "end": 4041.68, "text": " The thing...", "tokens": [50814, 440, 551, 485, 50864], "temperature": 0.0, "avg_logprob": -0.14298559605390176, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.002888623857870698}, {"id": 1025, "seek": 403168, "start": 4041.68, "end": 4046.68, "text": " Okay, so like PHY1.5 has never read Wikipedia, for example.", "tokens": [50864, 1033, 11, 370, 411, 16530, 56, 16, 13, 20, 575, 1128, 1401, 28999, 11, 337, 1365, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14298559605390176, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.002888623857870698}, {"id": 1026, "seek": 403168, "start": 4046.68, "end": 4050.68, "text": " So it doesn't know who Tom Cruise is, you know.", "tokens": [51114, 407, 309, 1177, 380, 458, 567, 5041, 39165, 307, 11, 291, 458, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14298559605390176, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.002888623857870698}, {"id": 1027, "seek": 403168, "start": 4050.68, "end": 4053.68, "text": " It doesn't know who anybody is.", "tokens": [51314, 467, 1177, 380, 458, 567, 4472, 307, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14298559605390176, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.002888623857870698}, {"id": 1028, "seek": 403168, "start": 4053.68, "end": 4055.68, "text": " He doesn't know about any movies.", "tokens": [51464, 634, 1177, 380, 458, 466, 604, 6233, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14298559605390176, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.002888623857870698}, {"id": 1029, "seek": 405568, "start": 4055.68, "end": 4061.68, "text": " It doesn't really know anything about anything because it's never read anything.", "tokens": [50364, 467, 1177, 380, 534, 458, 1340, 466, 1340, 570, 309, 311, 1128, 1401, 1340, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1209567265632825, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.004681088030338287}, {"id": 1030, "seek": 405568, "start": 4061.68, "end": 4069.68, "text": " You know, it was trained on a nearly entirely synthetic data set, which is designed for it to learn reasoning.", "tokens": [50664, 509, 458, 11, 309, 390, 8895, 322, 257, 6217, 7696, 23420, 1412, 992, 11, 597, 307, 4761, 337, 309, 281, 1466, 21577, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1209567265632825, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.004681088030338287}, {"id": 1031, "seek": 405568, "start": 4069.68, "end": 4077.68, "text": " And so it was a research project and a really good one, and it definitely shows us a powerful direction in terms of what you can do with synthetic data.", "tokens": [51064, 400, 370, 309, 390, 257, 2132, 1716, 293, 257, 534, 665, 472, 11, 293, 309, 2138, 3110, 505, 257, 4005, 3513, 294, 2115, 295, 437, 291, 393, 360, 365, 23420, 1412, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1209567265632825, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.004681088030338287}, {"id": 1032, "seek": 407768, "start": 4077.68, "end": 4084.68, "text": " And wow, gosh, even these tiny models can get pretty good reasoning skills, pretty good math skills, pretty good toting skills.", "tokens": [50364, 400, 6076, 11, 6502, 11, 754, 613, 5870, 5245, 393, 483, 1238, 665, 21577, 3942, 11, 1238, 665, 5221, 3942, 11, 1238, 665, 281, 783, 3942, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08110647201538086, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.21195322275161743}, {"id": 1033, "seek": 407768, "start": 4086.68, "end": 4091.68, "text": " But I don't know if it's a model you could necessarily build on.", "tokens": [50814, 583, 286, 500, 380, 458, 498, 309, 311, 257, 2316, 291, 727, 4725, 1322, 322, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08110647201538086, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.21195322275161743}, {"id": 1034, "seek": 407768, "start": 4091.68, "end": 4094.68, "text": " Some people have tried to do some fine tunes of it.", "tokens": [51064, 2188, 561, 362, 3031, 281, 360, 512, 2489, 38498, 295, 309, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08110647201538086, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.21195322275161743}, {"id": 1035, "seek": 407768, "start": 4094.68, "end": 4104.68, "text": " And again, they're like surprisingly good in some ways for a 1.5B model, but not sure you'd find it useful for anything.", "tokens": [51214, 400, 797, 11, 436, 434, 411, 17600, 665, 294, 512, 2098, 337, 257, 502, 13, 20, 33, 2316, 11, 457, 406, 988, 291, 1116, 915, 309, 4420, 337, 1340, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08110647201538086, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.21195322275161743}, {"id": 1036, "seek": 410468, "start": 4104.68, "end": 4112.68, "text": " I think that's the struggle of pitching small models because small is great, you know, you don't need a lot of resources to run them.", "tokens": [50364, 286, 519, 300, 311, 264, 7799, 295, 37499, 1359, 5245, 570, 1359, 307, 869, 11, 291, 458, 11, 291, 500, 380, 643, 257, 688, 295, 3593, 281, 1190, 552, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13824268749782018, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.0005527215544134378}, {"id": 1037, "seek": 410468, "start": 4112.68, "end": 4115.68, "text": " But the performance evaluation is always so iffy.", "tokens": [50764, 583, 264, 3389, 13344, 307, 1009, 370, 498, 22522, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13824268749782018, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.0005527215544134378}, {"id": 1038, "seek": 410468, "start": 4115.68, "end": 4120.68, "text": " It's always just like, yeah, it works on some things and we don't trust it for others.", "tokens": [50914, 467, 311, 1009, 445, 411, 11, 1338, 11, 309, 1985, 322, 512, 721, 293, 321, 500, 380, 3361, 309, 337, 2357, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13824268749782018, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.0005527215544134378}, {"id": 1039, "seek": 410468, "start": 4120.68, "end": 4123.68, "text": " Yeah, so that's why we're back to fine tuning.", "tokens": [51164, 865, 11, 370, 300, 311, 983, 321, 434, 646, 281, 2489, 15164, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13824268749782018, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.0005527215544134378}, {"id": 1040, "seek": 410468, "start": 4123.68, "end": 4130.68, "text": " I would say, so Microsoft did create a PHY1.5 web, but they didn't release it, unfortunately.", "tokens": [51314, 286, 576, 584, 11, 370, 8116, 630, 1884, 257, 16530, 56, 16, 13, 20, 3670, 11, 457, 436, 994, 380, 4374, 309, 11, 7015, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13824268749782018, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.0005527215544134378}, {"id": 1041, "seek": 413068, "start": 4131.68, "end": 4144.68, "text": " I would say a PHY1.5 web with fine tuning for your task might solve a lot of tasks that people have in their kind of day-to-day lives,", "tokens": [50414, 286, 576, 584, 257, 16530, 56, 16, 13, 20, 3670, 365, 2489, 15164, 337, 428, 5633, 1062, 5039, 257, 688, 295, 9608, 300, 561, 362, 294, 641, 733, 295, 786, 12, 1353, 12, 810, 2909, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09358164192973703, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.0012839300325140357}, {"id": 1042, "seek": 413068, "start": 4146.68, "end": 4148.68, "text": " particularly in kind of an enterprise setting.", "tokens": [51164, 4098, 294, 733, 295, 364, 14132, 3287, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09358164192973703, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.0012839300325140357}, {"id": 1043, "seek": 413068, "start": 4148.68, "end": 4153.68, "text": " I think there's a lot of repetitive kind of processing that has to be done.", "tokens": [51264, 286, 519, 456, 311, 257, 688, 295, 29404, 733, 295, 9007, 300, 575, 281, 312, 1096, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09358164192973703, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.0012839300325140357}, {"id": 1044, "seek": 415368, "start": 4153.68, "end": 4164.68, "text": " It's a useful thing for coders to know about because I think quite often you can replace some thousands and thousands of lines of complex buggy code maybe with a fine tune, you know.", "tokens": [50364, 467, 311, 257, 4420, 551, 337, 17656, 433, 281, 458, 466, 570, 286, 519, 1596, 2049, 291, 393, 7406, 512, 5383, 293, 5383, 295, 3876, 295, 3997, 7426, 1480, 3089, 1310, 365, 257, 2489, 10864, 11, 291, 458, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15525397501493754, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.1479131281375885}, {"id": 1045, "seek": 415368, "start": 4165.68, "end": 4166.68, "text": " Good.", "tokens": [50964, 2205, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15525397501493754, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.1479131281375885}, {"id": 1046, "seek": 415368, "start": 4166.68, "end": 4167.68, "text": " Yeah.", "tokens": [51014, 865, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15525397501493754, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.1479131281375885}, {"id": 1047, "seek": 415368, "start": 4168.68, "end": 4173.68, "text": " And Jeremy, before we let you go, I think one question on top of a lot of people's minds.", "tokens": [51114, 400, 17809, 11, 949, 321, 718, 291, 352, 11, 286, 519, 472, 1168, 322, 1192, 295, 257, 688, 295, 561, 311, 9634, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15525397501493754, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.1479131281375885}, {"id": 1048, "seek": 417368, "start": 4173.68, "end": 4179.68, "text": " So you've done practical deep learning for coders in 2018, 19, 21, 22.", "tokens": [50364, 407, 291, 600, 1096, 8496, 2452, 2539, 337, 17656, 433, 294, 6096, 11, 1294, 11, 5080, 11, 5853, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09517020672823476, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.46088671684265137}, {"id": 1049, "seek": 417368, "start": 4179.68, "end": 4183.68, "text": " I feel like the more time goes by, the more the GPUs get concentrated.", "tokens": [50664, 286, 841, 411, 264, 544, 565, 1709, 538, 11, 264, 544, 264, 18407, 82, 483, 21321, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09517020672823476, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.46088671684265137}, {"id": 1050, "seek": 417368, "start": 4184.68, "end": 4190.68, "text": " If you're somebody who's interested in deep learning today and you don't want to go join OpenAI, you don't want to join Anthropic,", "tokens": [50914, 759, 291, 434, 2618, 567, 311, 3102, 294, 2452, 2539, 965, 293, 291, 500, 380, 528, 281, 352, 3917, 7238, 48698, 11, 291, 500, 380, 528, 281, 3917, 12727, 39173, 11, 51214], "temperature": 0.0, "avg_logprob": -0.09517020672823476, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.46088671684265137}, {"id": 1051, "seek": 417368, "start": 4190.68, "end": 4193.68, "text": " what's like the best use of their time?", "tokens": [51214, 437, 311, 411, 264, 1151, 764, 295, 641, 565, 30, 51364], "temperature": 0.0, "avg_logprob": -0.09517020672823476, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.46088671684265137}, {"id": 1052, "seek": 417368, "start": 4193.68, "end": 4195.68, "text": " Should they focus on, yes, model development?", "tokens": [51364, 6454, 436, 1879, 322, 11, 2086, 11, 2316, 3250, 30, 51464], "temperature": 0.0, "avg_logprob": -0.09517020672823476, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.46088671684265137}, {"id": 1053, "seek": 417368, "start": 4195.68, "end": 4198.68, "text": " Should they focus on fine tuning math and all of that?", "tokens": [51464, 6454, 436, 1879, 322, 2489, 15164, 5221, 293, 439, 295, 300, 30, 51614], "temperature": 0.0, "avg_logprob": -0.09517020672823476, "compression_ratio": 1.6324110671936758, "no_speech_prob": 0.46088671684265137}, {"id": 1054, "seek": 419868, "start": 4198.68, "end": 4204.68, "text": " Should they just like focus on making rag not a hack and coming up with a better solution?", "tokens": [50364, 6454, 436, 445, 411, 1879, 322, 1455, 17539, 406, 257, 10339, 293, 1348, 493, 365, 257, 1101, 3827, 30, 50664], "temperature": 0.0, "avg_logprob": -0.12154770764437589, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.0025506557431071997}, {"id": 1055, "seek": 419868, "start": 4205.68, "end": 4209.68, "text": " Yeah, what's practical deep learning for coders 2024 kind of look like?", "tokens": [50714, 865, 11, 437, 311, 8496, 2452, 2539, 337, 17656, 433, 45237, 733, 295, 574, 411, 30, 50914], "temperature": 0.0, "avg_logprob": -0.12154770764437589, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.0025506557431071997}, {"id": 1056, "seek": 419868, "start": 4210.68, "end": 4211.68, "text": " Yeah.", "tokens": [50964, 865, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12154770764437589, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.0025506557431071997}, {"id": 1057, "seek": 419868, "start": 4211.68, "end": 4212.68, "text": " I mean, good question.", "tokens": [51014, 286, 914, 11, 665, 1168, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12154770764437589, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.0025506557431071997}, {"id": 1058, "seek": 419868, "start": 4212.68, "end": 4215.68, "text": " I'm trying to figure that out for myself, you know, like what should I teach?", "tokens": [51064, 286, 478, 1382, 281, 2573, 300, 484, 337, 2059, 11, 291, 458, 11, 411, 437, 820, 286, 2924, 30, 51214], "temperature": 0.0, "avg_logprob": -0.12154770764437589, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.0025506557431071997}, {"id": 1059, "seek": 419868, "start": 4215.68, "end": 4221.68, "text": " Because I definitely feel like things have changed a bit, you know.", "tokens": [51214, 1436, 286, 2138, 841, 411, 721, 362, 3105, 257, 857, 11, 291, 458, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12154770764437589, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.0025506557431071997}, {"id": 1060, "seek": 419868, "start": 4221.68, "end": 4226.68, "text": " One of the ways in which things have changed is that coding is much more accessible now.", "tokens": [51514, 1485, 295, 264, 2098, 294, 597, 721, 362, 3105, 307, 300, 17720, 307, 709, 544, 9515, 586, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12154770764437589, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.0025506557431071997}, {"id": 1061, "seek": 422668, "start": 4226.68, "end": 4230.68, "text": " So if you look at a lot of the folks in the kind of open source LLM community,", "tokens": [50364, 407, 498, 291, 574, 412, 257, 688, 295, 264, 4024, 294, 264, 733, 295, 1269, 4009, 441, 43, 44, 1768, 11, 50564], "temperature": 0.0, "avg_logprob": -0.08638574008283945, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0015483666211366653}, {"id": 1062, "seek": 422668, "start": 4230.68, "end": 4233.68, "text": " they're folks who really hadn't coded before a year ago.", "tokens": [50564, 436, 434, 4024, 567, 534, 8782, 380, 34874, 949, 257, 1064, 2057, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08638574008283945, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0015483666211366653}, {"id": 1063, "seek": 422668, "start": 4234.68, "end": 4240.68, "text": " And they're using these models to help them build stuff they couldn't build before, which is just fantastic, you know.", "tokens": [50764, 400, 436, 434, 1228, 613, 5245, 281, 854, 552, 1322, 1507, 436, 2809, 380, 1322, 949, 11, 597, 307, 445, 5456, 11, 291, 458, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08638574008283945, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0015483666211366653}, {"id": 1064, "seek": 422668, "start": 4242.68, "end": 4248.68, "text": " So one thing I kind of think is like, okay, well, we need a lot more material to help these people use this newfound skill they have,", "tokens": [51164, 407, 472, 551, 286, 733, 295, 519, 307, 411, 11, 1392, 11, 731, 11, 321, 643, 257, 688, 544, 2527, 281, 854, 613, 561, 764, 341, 777, 17493, 5389, 436, 362, 11, 51464], "temperature": 0.0, "avg_logprob": -0.08638574008283945, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0015483666211366653}, {"id": 1065, "seek": 422668, "start": 4248.68, "end": 4252.68, "text": " because they don't really know what they're doing, you know, and they don't claim to,", "tokens": [51464, 570, 436, 500, 380, 534, 458, 437, 436, 434, 884, 11, 291, 458, 11, 293, 436, 500, 380, 3932, 281, 11, 51664], "temperature": 0.0, "avg_logprob": -0.08638574008283945, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0015483666211366653}, {"id": 1066, "seek": 422668, "start": 4252.68, "end": 4253.68, "text": " but they're doing it anyway.", "tokens": [51664, 457, 436, 434, 884, 309, 4033, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08638574008283945, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0015483666211366653}, {"id": 1067, "seek": 422668, "start": 4253.68, "end": 4255.68, "text": " And I think that's fantastic, you know.", "tokens": [51714, 400, 286, 519, 300, 311, 5456, 11, 291, 458, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08638574008283945, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0015483666211366653}, {"id": 1068, "seek": 425568, "start": 4255.68, "end": 4260.68, "text": " So like other things we could do to help people, you know, bridge this gap,", "tokens": [50364, 407, 411, 661, 721, 321, 727, 360, 281, 854, 561, 11, 291, 458, 11, 7283, 341, 7417, 11, 50614], "temperature": 0.0, "avg_logprob": -0.10264823747717816, "compression_ratio": 1.6093023255813954, "no_speech_prob": 0.00017398396448697895}, {"id": 1069, "seek": 425568, "start": 4260.68, "end": 4269.68, "text": " because previously, you know, I know folks who were, you know, doing menial jobs a year ago,", "tokens": [50614, 570, 8046, 11, 291, 458, 11, 286, 458, 4024, 567, 645, 11, 291, 458, 11, 884, 1706, 831, 4782, 257, 1064, 2057, 11, 51064], "temperature": 0.0, "avg_logprob": -0.10264823747717816, "compression_ratio": 1.6093023255813954, "no_speech_prob": 0.00017398396448697895}, {"id": 1070, "seek": 425568, "start": 4269.68, "end": 4276.68, "text": " and now they're training language models thanks to the help of codecs and co-pilot and whatever.", "tokens": [51064, 293, 586, 436, 434, 3097, 2856, 5245, 3231, 281, 264, 854, 295, 3089, 14368, 293, 598, 12, 79, 31516, 293, 2035, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10264823747717816, "compression_ratio": 1.6093023255813954, "no_speech_prob": 0.00017398396448697895}, {"id": 1071, "seek": 425568, "start": 4276.68, "end": 4281.68, "text": " So, you know, yeah, what does it look like to like really grab this opportunity?", "tokens": [51414, 407, 11, 291, 458, 11, 1338, 11, 437, 775, 309, 574, 411, 281, 411, 534, 4444, 341, 2650, 30, 51664], "temperature": 0.0, "avg_logprob": -0.10264823747717816, "compression_ratio": 1.6093023255813954, "no_speech_prob": 0.00017398396448697895}, {"id": 1072, "seek": 428168, "start": 4281.68, "end": 4287.68, "text": " You know, maybe fast AIs goals can be dramatically expanded now to being like,", "tokens": [50364, 509, 458, 11, 1310, 2370, 316, 6802, 5493, 393, 312, 17548, 14342, 586, 281, 885, 411, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12106525146209442, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.006487065460532904}, {"id": 1073, "seek": 428168, "start": 4287.68, "end": 4293.68, "text": " let's make coding more accessible, you know, or kind of AI-oriented coding more accessible.", "tokens": [50664, 718, 311, 652, 17720, 544, 9515, 11, 291, 458, 11, 420, 733, 295, 7318, 12, 27414, 17720, 544, 9515, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12106525146209442, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.006487065460532904}, {"id": 1074, "seek": 428168, "start": 4294.68, "end": 4299.68, "text": " If so, our costs should probably look very different, you know,", "tokens": [51014, 759, 370, 11, 527, 5497, 820, 1391, 574, 588, 819, 11, 291, 458, 11, 51264], "temperature": 0.0, "avg_logprob": -0.12106525146209442, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.006487065460532904}, {"id": 1075, "seek": 428168, "start": 4299.68, "end": 4303.68, "text": " and we'd have to throw away that like, oh, you have to have at least a year of full-time programming,", "tokens": [51264, 293, 321, 1116, 362, 281, 3507, 1314, 300, 411, 11, 1954, 11, 291, 362, 281, 362, 412, 1935, 257, 1064, 295, 1577, 12, 3766, 9410, 11, 51464], "temperature": 0.0, "avg_logprob": -0.12106525146209442, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.006487065460532904}, {"id": 1076, "seek": 428168, "start": 4303.68, "end": 4306.68, "text": " you know, as a prerequisite.", "tokens": [51464, 291, 458, 11, 382, 257, 38333, 34152, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12106525146209442, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.006487065460532904}, {"id": 1077, "seek": 428168, "start": 4307.68, "end": 4309.68, "text": " Yeah, what would happen if we got rid of that?", "tokens": [51664, 865, 11, 437, 576, 1051, 498, 321, 658, 3973, 295, 300, 30, 51764], "temperature": 0.0, "avg_logprob": -0.12106525146209442, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.006487065460532904}, {"id": 1078, "seek": 430968, "start": 4309.68, "end": 4312.68, "text": " So that's kind of one thought that's in my head.", "tokens": [50364, 407, 300, 311, 733, 295, 472, 1194, 300, 311, 294, 452, 1378, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1297660885435162, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.0012446895707398653}, {"id": 1079, "seek": 430968, "start": 4313.68, "end": 4320.68, "text": " You know, as to what should other people do, honestly, I don't think anybody has any idea,", "tokens": [50564, 509, 458, 11, 382, 281, 437, 820, 661, 561, 360, 11, 6095, 11, 286, 500, 380, 519, 4472, 575, 604, 1558, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1297660885435162, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.0012446895707398653}, {"id": 1080, "seek": 430968, "start": 4320.68, "end": 4323.68, "text": " like the more I look at it, what's going on.", "tokens": [50914, 411, 264, 544, 286, 574, 412, 309, 11, 437, 311, 516, 322, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1297660885435162, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.0012446895707398653}, {"id": 1081, "seek": 430968, "start": 4323.68, "end": 4327.68, "text": " I know I don't, you know, like, we don't really know how to do anything very well.", "tokens": [51064, 286, 458, 286, 500, 380, 11, 291, 458, 11, 411, 11, 321, 500, 380, 534, 458, 577, 281, 360, 1340, 588, 731, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1297660885435162, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.0012446895707398653}, {"id": 1082, "seek": 430968, "start": 4328.68, "end": 4334.68, "text": " Clearly open AI do, like they seem to be quite good at some things,", "tokens": [51314, 24120, 1269, 7318, 360, 11, 411, 436, 1643, 281, 312, 1596, 665, 412, 512, 721, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1297660885435162, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.0012446895707398653}, {"id": 1083, "seek": 433468, "start": 4334.68, "end": 4338.68, "text": " or they're talking to folks at or who have recently left open AI.", "tokens": [50364, 420, 436, 434, 1417, 281, 4024, 412, 420, 567, 362, 3938, 1411, 1269, 7318, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06672915882534451, "compression_ratio": 2.067741935483871, "no_speech_prob": 0.013424915261566639}, {"id": 1084, "seek": 433468, "start": 4338.68, "end": 4341.68, "text": " Even there, it's clear there's a lot of stuff they haven't really figured out,", "tokens": [50564, 2754, 456, 11, 309, 311, 1850, 456, 311, 257, 688, 295, 1507, 436, 2378, 380, 534, 8932, 484, 11, 50714], "temperature": 0.0, "avg_logprob": -0.06672915882534451, "compression_ratio": 2.067741935483871, "no_speech_prob": 0.013424915261566639}, {"id": 1085, "seek": 433468, "start": 4341.68, "end": 4346.68, "text": " and they're just kind of like using recipes that they've noticed have been okay.", "tokens": [50714, 293, 436, 434, 445, 733, 295, 411, 1228, 13035, 300, 436, 600, 5694, 362, 668, 1392, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06672915882534451, "compression_ratio": 2.067741935483871, "no_speech_prob": 0.013424915261566639}, {"id": 1086, "seek": 433468, "start": 4346.68, "end": 4349.68, "text": " So yeah, we don't really know how to train these models well.", "tokens": [50964, 407, 1338, 11, 321, 500, 380, 534, 458, 577, 281, 3847, 613, 5245, 731, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06672915882534451, "compression_ratio": 2.067741935483871, "no_speech_prob": 0.013424915261566639}, {"id": 1087, "seek": 433468, "start": 4349.68, "end": 4350.68, "text": " We don't know how to fine-tune them well.", "tokens": [51114, 492, 500, 380, 458, 577, 281, 2489, 12, 83, 2613, 552, 731, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06672915882534451, "compression_ratio": 2.067741935483871, "no_speech_prob": 0.013424915261566639}, {"id": 1088, "seek": 433468, "start": 4350.68, "end": 4352.68, "text": " We don't know how to do RAG well.", "tokens": [51164, 492, 500, 380, 458, 577, 281, 360, 14626, 38, 731, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06672915882534451, "compression_ratio": 2.067741935483871, "no_speech_prob": 0.013424915261566639}, {"id": 1089, "seek": 433468, "start": 4352.68, "end": 4353.68, "text": " We don't know what they can do.", "tokens": [51264, 492, 500, 380, 458, 437, 436, 393, 360, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06672915882534451, "compression_ratio": 2.067741935483871, "no_speech_prob": 0.013424915261566639}, {"id": 1090, "seek": 433468, "start": 4353.68, "end": 4354.68, "text": " We don't know what they can't do.", "tokens": [51314, 492, 500, 380, 458, 437, 436, 393, 380, 360, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06672915882534451, "compression_ratio": 2.067741935483871, "no_speech_prob": 0.013424915261566639}, {"id": 1091, "seek": 433468, "start": 4354.68, "end": 4357.68, "text": " We don't know how big a model you need to solve different kinds of problems.", "tokens": [51364, 492, 500, 380, 458, 577, 955, 257, 2316, 291, 643, 281, 5039, 819, 3685, 295, 2740, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06672915882534451, "compression_ratio": 2.067741935483871, "no_speech_prob": 0.013424915261566639}, {"id": 1092, "seek": 433468, "start": 4357.68, "end": 4359.68, "text": " We don't know what kind of problems they can't do.", "tokens": [51514, 492, 500, 380, 458, 437, 733, 295, 2740, 436, 393, 380, 360, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06672915882534451, "compression_ratio": 2.067741935483871, "no_speech_prob": 0.013424915261566639}, {"id": 1093, "seek": 433468, "start": 4359.68, "end": 4363.68, "text": " We don't know what good prompting strategies are for particular problems, you know,", "tokens": [51614, 492, 500, 380, 458, 437, 665, 12391, 278, 9029, 366, 337, 1729, 2740, 11, 291, 458, 11, 51814], "temperature": 0.0, "avg_logprob": -0.06672915882534451, "compression_ratio": 2.067741935483871, "no_speech_prob": 0.013424915261566639}, {"id": 1094, "seek": 436368, "start": 4363.68, "end": 4369.68, "text": " like somebody sent me a message the other day saying they've written something", "tokens": [50364, 411, 2618, 2279, 385, 257, 3636, 264, 661, 786, 1566, 436, 600, 3720, 746, 50664], "temperature": 0.0, "avg_logprob": -0.13005882501602173, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.00446560001000762}, {"id": 1095, "seek": 436368, "start": 4369.68, "end": 4374.68, "text": " that is a prompting strategy for GPT-4.", "tokens": [50664, 300, 307, 257, 12391, 278, 5206, 337, 26039, 51, 12, 19, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13005882501602173, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.00446560001000762}, {"id": 1096, "seek": 436368, "start": 4374.68, "end": 4377.68, "text": " They've written like 6,000 lines of Python code,", "tokens": [50914, 814, 600, 3720, 411, 1386, 11, 1360, 3876, 295, 15329, 3089, 11, 51064], "temperature": 0.0, "avg_logprob": -0.13005882501602173, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.00446560001000762}, {"id": 1097, "seek": 436368, "start": 4377.68, "end": 4380.68, "text": " and it's to help it play chess.", "tokens": [51064, 293, 309, 311, 281, 854, 309, 862, 24122, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13005882501602173, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.00446560001000762}, {"id": 1098, "seek": 436368, "start": 4381.68, "end": 4385.68, "text": " And then they've said they've had it play against other chess engines,", "tokens": [51264, 400, 550, 436, 600, 848, 436, 600, 632, 309, 862, 1970, 661, 24122, 12982, 11, 51464], "temperature": 0.0, "avg_logprob": -0.13005882501602173, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.00446560001000762}, {"id": 1099, "seek": 436368, "start": 4385.68, "end": 4387.68, "text": " including the best stockfish engines,", "tokens": [51464, 3009, 264, 1151, 4127, 11608, 12982, 11, 51564], "temperature": 0.0, "avg_logprob": -0.13005882501602173, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.00446560001000762}, {"id": 1100, "seek": 436368, "start": 4387.68, "end": 4391.68, "text": " and it's got an ELO of 3400,", "tokens": [51564, 293, 309, 311, 658, 364, 14426, 46, 295, 12790, 628, 11, 51764], "temperature": 0.0, "avg_logprob": -0.13005882501602173, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.00446560001000762}, {"id": 1101, "seek": 439168, "start": 4391.68, "end": 4395.68, "text": " which would make it close to the best chess engine in existence.", "tokens": [50364, 597, 576, 652, 309, 1998, 281, 264, 1151, 24122, 2848, 294, 9123, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09192638397216797, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.001754268421791494}, {"id": 1102, "seek": 439168, "start": 4397.68, "end": 4402.68, "text": " And I think this is a good example of like people were saying like GPT-4 can't play chess.", "tokens": [50664, 400, 286, 519, 341, 307, 257, 665, 1365, 295, 411, 561, 645, 1566, 411, 26039, 51, 12, 19, 393, 380, 862, 24122, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09192638397216797, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.001754268421791494}, {"id": 1103, "seek": 439168, "start": 4402.68, "end": 4404.68, "text": " I mean, I was sure that was wrong.", "tokens": [50914, 286, 914, 11, 286, 390, 988, 300, 390, 2085, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09192638397216797, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.001754268421791494}, {"id": 1104, "seek": 439168, "start": 4404.68, "end": 4406.68, "text": " I mean, obviously it can play chess,", "tokens": [51014, 286, 914, 11, 2745, 309, 393, 862, 24122, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09192638397216797, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.001754268421791494}, {"id": 1105, "seek": 439168, "start": 4406.68, "end": 4410.68, "text": " but the difference between like with no prompting strategy,", "tokens": [51114, 457, 264, 2649, 1296, 411, 365, 572, 12391, 278, 5206, 11, 51314], "temperature": 0.0, "avg_logprob": -0.09192638397216797, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.001754268421791494}, {"id": 1106, "seek": 439168, "start": 4410.68, "end": 4412.68, "text": " they can't even make legal moves.", "tokens": [51314, 436, 393, 380, 754, 652, 5089, 6067, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09192638397216797, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.001754268421791494}, {"id": 1107, "seek": 439168, "start": 4412.68, "end": 4413.68, "text": " With good prompting strategies,", "tokens": [51414, 2022, 665, 12391, 278, 9029, 11, 51464], "temperature": 0.0, "avg_logprob": -0.09192638397216797, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.001754268421791494}, {"id": 1108, "seek": 439168, "start": 4413.68, "end": 4416.68, "text": " it might be just about the best chess engine in the world,", "tokens": [51464, 309, 1062, 312, 445, 466, 264, 1151, 24122, 2848, 294, 264, 1002, 11, 51614], "temperature": 0.0, "avg_logprob": -0.09192638397216797, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.001754268421791494}, {"id": 1109, "seek": 439168, "start": 4416.68, "end": 4418.68, "text": " far better than any human player.", "tokens": [51614, 1400, 1101, 813, 604, 1952, 4256, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09192638397216797, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.001754268421791494}, {"id": 1110, "seek": 441868, "start": 4418.68, "end": 4421.68, "text": " So, yeah, I mean, we don't really know what the capabilities are yet.", "tokens": [50364, 407, 11, 1338, 11, 286, 914, 11, 321, 500, 380, 534, 458, 437, 264, 10862, 366, 1939, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1111, "seek": 441868, "start": 4421.68, "end": 4424.68, "text": " So I feel like it's all blue sky at this point.", "tokens": [50514, 407, 286, 841, 411, 309, 311, 439, 3344, 5443, 412, 341, 935, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1112, "seek": 441868, "start": 4424.68, "end": 4428.68, "text": " It feels like computer vision in 2013 to me,", "tokens": [50664, 467, 3417, 411, 3820, 5201, 294, 9012, 281, 385, 11, 50864], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1113, "seek": 441868, "start": 4428.68, "end": 4430.68, "text": " which was like in 2013 computer vision.", "tokens": [50864, 597, 390, 411, 294, 9012, 3820, 5201, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1114, "seek": 441868, "start": 4430.68, "end": 4431.68, "text": " We just had the AlexNet.", "tokens": [50964, 492, 445, 632, 264, 5202, 31890, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1115, "seek": 441868, "start": 4431.68, "end": 4433.68, "text": " We've had AlexNet.", "tokens": [51014, 492, 600, 632, 5202, 31890, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1116, "seek": 441868, "start": 4433.68, "end": 4435.68, "text": " We've had VGGNet.", "tokens": [51114, 492, 600, 632, 691, 27561, 31890, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1117, "seek": 441868, "start": 4435.68, "end": 4437.68, "text": " It's around the time Xyler and Fergus like,", "tokens": [51214, 467, 311, 926, 264, 565, 1783, 88, 1918, 293, 36790, 411, 11, 51314], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1118, "seek": 441868, "start": 4437.68, "end": 4439.68, "text": " no, it's probably before that.", "tokens": [51314, 572, 11, 309, 311, 1391, 949, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1119, "seek": 441868, "start": 4439.68, "end": 4440.68, "text": " So we hadn't yet had the Xyler and Fergus like,", "tokens": [51414, 407, 321, 8782, 380, 1939, 632, 264, 1783, 88, 1918, 293, 36790, 411, 11, 51464], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1120, "seek": 441868, "start": 4440.68, "end": 4442.68, "text": " oh, this is actually what's going on inside the layers.", "tokens": [51464, 1954, 11, 341, 307, 767, 437, 311, 516, 322, 1854, 264, 7914, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1121, "seek": 441868, "start": 4442.68, "end": 4447.68, "text": " So, you know, we don't actually know what's happening inside these transformers.", "tokens": [51564, 407, 11, 291, 458, 11, 321, 500, 380, 767, 458, 437, 311, 2737, 1854, 613, 4088, 433, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12860913276672364, "compression_ratio": 1.8006872852233677, "no_speech_prob": 0.0010646788869053125}, {"id": 1122, "seek": 444768, "start": 4447.68, "end": 4451.68, "text": " We don't know how to create good training dynamics.", "tokens": [50364, 492, 500, 380, 458, 577, 281, 1884, 665, 3097, 15679, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08879443442467416, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0010320281144231558}, {"id": 1123, "seek": 444768, "start": 4451.68, "end": 4454.68, "text": " We don't really know anything much.", "tokens": [50564, 492, 500, 380, 534, 458, 1340, 709, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08879443442467416, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0010320281144231558}, {"id": 1124, "seek": 444768, "start": 4454.68, "end": 4457.68, "text": " And there's a reason for that, right?", "tokens": [50714, 400, 456, 311, 257, 1778, 337, 300, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.08879443442467416, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0010320281144231558}, {"id": 1125, "seek": 444768, "start": 4457.68, "end": 4464.68, "text": " And the reason for that is language models suddenly got really useful.", "tokens": [50864, 400, 264, 1778, 337, 300, 307, 2856, 5245, 5800, 658, 534, 4420, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08879443442467416, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0010320281144231558}, {"id": 1126, "seek": 444768, "start": 4464.68, "end": 4468.68, "text": " And so the kind of economically rational thing to do,", "tokens": [51214, 400, 370, 264, 733, 295, 26811, 15090, 551, 281, 360, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08879443442467416, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0010320281144231558}, {"id": 1127, "seek": 444768, "start": 4468.68, "end": 4470.68, "text": " like this is not criticism, this is true.", "tokens": [51414, 411, 341, 307, 406, 15835, 11, 341, 307, 2074, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08879443442467416, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0010320281144231558}, {"id": 1128, "seek": 444768, "start": 4470.68, "end": 4472.68, "text": " The economic rational thing to do is to like,", "tokens": [51514, 440, 4836, 15090, 551, 281, 360, 307, 281, 411, 11, 51614], "temperature": 0.0, "avg_logprob": -0.08879443442467416, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0010320281144231558}, {"id": 1129, "seek": 444768, "start": 4472.68, "end": 4475.68, "text": " okay, like build that as fast as possible,", "tokens": [51614, 1392, 11, 411, 1322, 300, 382, 2370, 382, 1944, 11, 51764], "temperature": 0.0, "avg_logprob": -0.08879443442467416, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0010320281144231558}, {"id": 1130, "seek": 447568, "start": 4475.68, "end": 4478.68, "text": " you know, make something work, get it out there.", "tokens": [50364, 291, 458, 11, 652, 746, 589, 11, 483, 309, 484, 456, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12023365020751953, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.005552537739276886}, {"id": 1131, "seek": 447568, "start": 4478.68, "end": 4482.68, "text": " And that's what, you know, open AI in particular did,", "tokens": [50514, 400, 300, 311, 437, 11, 291, 458, 11, 1269, 7318, 294, 1729, 630, 11, 50714], "temperature": 0.0, "avg_logprob": -0.12023365020751953, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.005552537739276886}, {"id": 1132, "seek": 447568, "start": 4482.68, "end": 4486.68, "text": " anthropic kind of did.", "tokens": [50714, 22727, 299, 733, 295, 630, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12023365020751953, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.005552537739276886}, {"id": 1133, "seek": 447568, "start": 4486.68, "end": 4490.68, "text": " But there's a whole lot of technical debt everywhere, you know,", "tokens": [50914, 583, 456, 311, 257, 1379, 688, 295, 6191, 7831, 5315, 11, 291, 458, 11, 51114], "temperature": 0.0, "avg_logprob": -0.12023365020751953, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.005552537739276886}, {"id": 1134, "seek": 447568, "start": 4490.68, "end": 4495.68, "text": " nobody's really figured this stuff out because everybody's been so busy", "tokens": [51114, 5079, 311, 534, 8932, 341, 1507, 484, 570, 2201, 311, 668, 370, 5856, 51364], "temperature": 0.0, "avg_logprob": -0.12023365020751953, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.005552537739276886}, {"id": 1135, "seek": 447568, "start": 4495.68, "end": 4499.68, "text": " building what we know works as quickly as possible.", "tokens": [51364, 2390, 437, 321, 458, 1985, 382, 2661, 382, 1944, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12023365020751953, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.005552537739276886}, {"id": 1136, "seek": 447568, "start": 4499.68, "end": 4502.68, "text": " So, yeah, I think there's a huge amount of opportunity to,", "tokens": [51564, 407, 11, 1338, 11, 286, 519, 456, 311, 257, 2603, 2372, 295, 2650, 281, 11, 51714], "temperature": 0.0, "avg_logprob": -0.12023365020751953, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.005552537739276886}, {"id": 1137, "seek": 450268, "start": 4502.68, "end": 4509.68, "text": " you know, I think we'll find things can be made to work a lot faster,", "tokens": [50364, 291, 458, 11, 286, 519, 321, 603, 915, 721, 393, 312, 1027, 281, 589, 257, 688, 4663, 11, 50714], "temperature": 0.0, "avg_logprob": -0.0918658807737018, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.003073849016800523}, {"id": 1138, "seek": 450268, "start": 4509.68, "end": 4511.68, "text": " a lot less memory.", "tokens": [50714, 257, 688, 1570, 4675, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0918658807737018, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.003073849016800523}, {"id": 1139, "seek": 450268, "start": 4511.68, "end": 4514.68, "text": " I got a whole bunch of ideas I want to try, you know,", "tokens": [50814, 286, 658, 257, 1379, 3840, 295, 3487, 286, 528, 281, 853, 11, 291, 458, 11, 50964], "temperature": 0.0, "avg_logprob": -0.0918658807737018, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.003073849016800523}, {"id": 1140, "seek": 450268, "start": 4514.68, "end": 4518.68, "text": " every time I look at something closely, like really closely,", "tokens": [50964, 633, 565, 286, 574, 412, 746, 8185, 11, 411, 534, 8185, 11, 51164], "temperature": 0.0, "avg_logprob": -0.0918658807737018, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.003073849016800523}, {"id": 1141, "seek": 450268, "start": 4518.68, "end": 4522.68, "text": " I'm always like, oh, turns out this person actually had no idea what they're doing.", "tokens": [51164, 286, 478, 1009, 411, 11, 1954, 11, 4523, 484, 341, 954, 767, 632, 572, 1558, 437, 436, 434, 884, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0918658807737018, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.003073849016800523}, {"id": 1142, "seek": 450268, "start": 4522.68, "end": 4527.68, "text": " You know, which is fine, like none of us know what we're doing.", "tokens": [51364, 509, 458, 11, 597, 307, 2489, 11, 411, 6022, 295, 505, 458, 437, 321, 434, 884, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0918658807737018, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.003073849016800523}, {"id": 1143, "seek": 450268, "start": 4527.68, "end": 4531.68, "text": " We should experiment with that.", "tokens": [51614, 492, 820, 5120, 365, 300, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0918658807737018, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.003073849016800523}, {"id": 1144, "seek": 453168, "start": 4531.68, "end": 4535.68, "text": " We had a treat out on the podcast who created flash attention.", "tokens": [50364, 492, 632, 257, 2387, 484, 322, 264, 7367, 567, 2942, 7319, 3202, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1986825362495754, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00856710597872734}, {"id": 1145, "seek": 453168, "start": 4535.68, "end": 4539.68, "text": " And I asked them, did nobody think of using SRAM before you?", "tokens": [50564, 400, 286, 2351, 552, 11, 630, 5079, 519, 295, 1228, 20840, 2865, 949, 291, 30, 50764], "temperature": 0.0, "avg_logprob": -0.1986825362495754, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00856710597872734}, {"id": 1146, "seek": 453168, "start": 4539.68, "end": 4542.68, "text": " Like where people just like, you know, and he was like, yeah,", "tokens": [50764, 1743, 689, 561, 445, 411, 11, 291, 458, 11, 293, 415, 390, 411, 11, 1338, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1986825362495754, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00856710597872734}, {"id": 1147, "seek": 453168, "start": 4542.68, "end": 4545.68, "text": " people just didn't think of it, didn't try.", "tokens": [50914, 561, 445, 994, 380, 519, 295, 309, 11, 994, 380, 853, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1986825362495754, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00856710597872734}, {"id": 1148, "seek": 453168, "start": 4545.68, "end": 4547.68, "text": " They didn't come from like a systems background.", "tokens": [51064, 814, 994, 380, 808, 490, 411, 257, 3652, 3678, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1986825362495754, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00856710597872734}, {"id": 1149, "seek": 453168, "start": 4547.68, "end": 4548.68, "text": " Yeah.", "tokens": [51164, 865, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1986825362495754, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00856710597872734}, {"id": 1150, "seek": 453168, "start": 4548.68, "end": 4552.68, "text": " I mean, the thing about flash attention is, I mean,", "tokens": [51214, 286, 914, 11, 264, 551, 466, 7319, 3202, 307, 11, 286, 914, 11, 51414], "temperature": 0.0, "avg_logprob": -0.1986825362495754, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00856710597872734}, {"id": 1151, "seek": 453168, "start": 4552.68, "end": 4555.68, "text": " lots of people absolutely thought of that.", "tokens": [51414, 3195, 295, 561, 3122, 1194, 295, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1986825362495754, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00856710597872734}, {"id": 1152, "seek": 453168, "start": 4555.68, "end": 4556.68, "text": " So had I, right?", "tokens": [51564, 407, 632, 286, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.1986825362495754, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.00856710597872734}, {"id": 1153, "seek": 455668, "start": 4556.68, "end": 4560.68, "text": " But I mean, the honest truth is particularly before Triton,", "tokens": [50364, 583, 286, 914, 11, 264, 3245, 3494, 307, 4098, 949, 1765, 270, 266, 11, 50564], "temperature": 0.0, "avg_logprob": -0.14665958668925974, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.008437108248472214}, {"id": 1154, "seek": 455668, "start": 4560.68, "end": 4565.68, "text": " like everybody knew that tiling is the right way to solve anything.", "tokens": [50564, 411, 2201, 2586, 300, 256, 4883, 307, 264, 558, 636, 281, 5039, 1340, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14665958668925974, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.008437108248472214}, {"id": 1155, "seek": 455668, "start": 4565.68, "end": 4567.68, "text": " And everybody knew that attention,", "tokens": [50814, 400, 2201, 2586, 300, 3202, 11, 50914], "temperature": 0.0, "avg_logprob": -0.14665958668925974, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.008437108248472214}, {"id": 1156, "seek": 455668, "start": 4567.68, "end": 4569.68, "text": " used attention, wasn't tiled.", "tokens": [50914, 1143, 3202, 11, 2067, 380, 256, 7292, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14665958668925974, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.008437108248472214}, {"id": 1157, "seek": 455668, "start": 4569.68, "end": 4571.68, "text": " That was stupid.", "tokens": [51014, 663, 390, 6631, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14665958668925974, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.008437108248472214}, {"id": 1158, "seek": 455668, "start": 4571.68, "end": 4576.68, "text": " But not everybody's got his ability to like be like,", "tokens": [51114, 583, 406, 2201, 311, 658, 702, 3485, 281, 411, 312, 411, 11, 51364], "temperature": 0.0, "avg_logprob": -0.14665958668925974, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.008437108248472214}, {"id": 1159, "seek": 455668, "start": 4576.68, "end": 4582.68, "text": " oh, well, I'm confident enough in CUDA and or Triton", "tokens": [51364, 1954, 11, 731, 11, 286, 478, 6679, 1547, 294, 29777, 7509, 293, 420, 1765, 270, 266, 51664], "temperature": 0.0, "avg_logprob": -0.14665958668925974, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.008437108248472214}, {"id": 1160, "seek": 455668, "start": 4582.68, "end": 4584.68, "text": " to use that insight to write something better.", "tokens": [51664, 281, 764, 300, 11269, 281, 2464, 746, 1101, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14665958668925974, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.008437108248472214}, {"id": 1161, "seek": 458468, "start": 4584.68, "end": 4587.68, "text": " And this is where like, I'm super excited about Mojo, right?", "tokens": [50364, 400, 341, 307, 689, 411, 11, 286, 478, 1687, 2919, 466, 3335, 5134, 11, 558, 30, 50514], "temperature": 0.0, "avg_logprob": -0.12265374925401476, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.015899108722805977}, {"id": 1162, "seek": 458468, "start": 4587.68, "end": 4590.68, "text": " And I always talk to Chris about flash attention as I'm like,", "tokens": [50514, 400, 286, 1009, 751, 281, 6688, 466, 7319, 3202, 382, 286, 478, 411, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12265374925401476, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.015899108722805977}, {"id": 1163, "seek": 458468, "start": 4590.68, "end": 4596.68, "text": " there is a thousand flash attentions out there for us to build.", "tokens": [50664, 456, 307, 257, 4714, 7319, 30980, 626, 484, 456, 337, 505, 281, 1322, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12265374925401476, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.015899108722805977}, {"id": 1164, "seek": 458468, "start": 4596.68, "end": 4599.68, "text": " You just got to make it easy for us to build them.", "tokens": [50964, 509, 445, 658, 281, 652, 309, 1858, 337, 505, 281, 1322, 552, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12265374925401476, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.015899108722805977}, {"id": 1165, "seek": 458468, "start": 4599.68, "end": 4602.68, "text": " So like Triton definitely helps.", "tokens": [51114, 407, 411, 1765, 270, 266, 2138, 3665, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12265374925401476, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.015899108722805977}, {"id": 1166, "seek": 458468, "start": 4602.68, "end": 4606.68, "text": " But it's still not easy.", "tokens": [51264, 583, 309, 311, 920, 406, 1858, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12265374925401476, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.015899108722805977}, {"id": 1167, "seek": 458468, "start": 4606.68, "end": 4611.68, "text": " It still requires kind of really understanding the GPU architecture,", "tokens": [51464, 467, 920, 7029, 733, 295, 534, 3701, 264, 18407, 9482, 11, 51714], "temperature": 0.0, "avg_logprob": -0.12265374925401476, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.015899108722805977}, {"id": 1168, "seek": 461168, "start": 4611.68, "end": 4614.68, "text": " writing it in that kind of very CUDA-ish way.", "tokens": [50364, 3579, 309, 294, 300, 733, 295, 588, 29777, 7509, 12, 742, 636, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1512803872426351, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.008314860053360462}, {"id": 1169, "seek": 461168, "start": 4614.68, "end": 4619.68, "text": " So yeah, I think, you know, if Mojo or something equivalent", "tokens": [50514, 407, 1338, 11, 286, 519, 11, 291, 458, 11, 498, 3335, 5134, 420, 746, 10344, 50764], "temperature": 0.0, "avg_logprob": -0.1512803872426351, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.008314860053360462}, {"id": 1170, "seek": 461168, "start": 4619.68, "end": 4624.68, "text": " can really work well, we're going to see a lot more flash attentions", "tokens": [50764, 393, 534, 589, 731, 11, 321, 434, 516, 281, 536, 257, 688, 544, 7319, 30980, 626, 51014], "temperature": 0.0, "avg_logprob": -0.1512803872426351, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.008314860053360462}, {"id": 1171, "seek": 461168, "start": 4624.68, "end": 4627.68, "text": " popping up.", "tokens": [51014, 18374, 493, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1512803872426351, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.008314860053360462}, {"id": 1172, "seek": 461168, "start": 4627.68, "end": 4628.68, "text": " Great, Jaren.", "tokens": [51164, 3769, 11, 508, 4484, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1512803872426351, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.008314860053360462}, {"id": 1173, "seek": 461168, "start": 4628.68, "end": 4631.68, "text": " Before we wrap, we usually do a quick lightning round.", "tokens": [51214, 4546, 321, 7019, 11, 321, 2673, 360, 257, 1702, 16589, 3098, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1512803872426351, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.008314860053360462}, {"id": 1174, "seek": 461168, "start": 4631.68, "end": 4633.68, "text": " We've got three simple questions.", "tokens": [51364, 492, 600, 658, 1045, 2199, 1651, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1512803872426351, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.008314860053360462}, {"id": 1175, "seek": 461168, "start": 4633.68, "end": 4635.68, "text": " So the first one is around acceleration", "tokens": [51464, 407, 264, 700, 472, 307, 926, 17162, 51564], "temperature": 0.0, "avg_logprob": -0.1512803872426351, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.008314860053360462}, {"id": 1176, "seek": 461168, "start": 4635.68, "end": 4638.68, "text": " and you've been in this field a long time.", "tokens": [51564, 293, 291, 600, 668, 294, 341, 2519, 257, 938, 565, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1512803872426351, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.008314860053360462}, {"id": 1177, "seek": 461168, "start": 4638.68, "end": 4640.68, "text": " What's something that it's already here today", "tokens": [51714, 708, 311, 746, 300, 309, 311, 1217, 510, 965, 51814], "temperature": 0.0, "avg_logprob": -0.1512803872426351, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.008314860053360462}, {"id": 1178, "seek": 464068, "start": 4640.68, "end": 4643.68, "text": " and AI that you thought would take much longer?", "tokens": [50364, 293, 7318, 300, 291, 1194, 576, 747, 709, 2854, 30, 50514], "temperature": 0.0, "avg_logprob": -0.11293796161273578, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.008832608349621296}, {"id": 1179, "seek": 464068, "start": 4643.68, "end": 4644.68, "text": " I don't think anything.", "tokens": [50514, 286, 500, 380, 519, 1340, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11293796161273578, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.008832608349621296}, {"id": 1180, "seek": 464068, "start": 4644.68, "end": 4646.68, "text": " So I've actually been slightly too bullish.", "tokens": [50564, 407, 286, 600, 767, 668, 4748, 886, 38692, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11293796161273578, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.008832608349621296}, {"id": 1181, "seek": 464068, "start": 4646.68, "end": 4654.68, "text": " So in my 2014 TED Talk, I had a graph and I said like,", "tokens": [50664, 407, 294, 452, 8227, 43036, 8780, 11, 286, 632, 257, 4295, 293, 286, 848, 411, 11, 51064], "temperature": 0.0, "avg_logprob": -0.11293796161273578, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.008832608349621296}, {"id": 1182, "seek": 464068, "start": 4654.68, "end": 4656.68, "text": " this is like the slope of human capabilities", "tokens": [51064, 341, 307, 411, 264, 13525, 295, 1952, 10862, 51164], "temperature": 0.0, "avg_logprob": -0.11293796161273578, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.008832608349621296}, {"id": 1183, "seek": 464068, "start": 4656.68, "end": 4659.68, "text": " and this is the slope of AI capabilities.", "tokens": [51164, 293, 341, 307, 264, 13525, 295, 7318, 10862, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11293796161273578, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.008832608349621296}, {"id": 1184, "seek": 464068, "start": 4659.68, "end": 4662.68, "text": " And I said, oh, and I put a dot saying we are here.", "tokens": [51314, 400, 286, 848, 11, 1954, 11, 293, 286, 829, 257, 5893, 1566, 321, 366, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11293796161273578, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.008832608349621296}, {"id": 1185, "seek": 464068, "start": 4662.68, "end": 4664.68, "text": " It was just before they passed.", "tokens": [51464, 467, 390, 445, 949, 436, 4678, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11293796161273578, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.008832608349621296}, {"id": 1186, "seek": 464068, "start": 4664.68, "end": 4667.68, "text": " And I looked back at the transcript the other day", "tokens": [51564, 400, 286, 2956, 646, 412, 264, 24444, 264, 661, 786, 51714], "temperature": 0.0, "avg_logprob": -0.11293796161273578, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.008832608349621296}, {"id": 1187, "seek": 466768, "start": 4667.68, "end": 4672.68, "text": " and I said in five years, I think we might have crossed", "tokens": [50364, 293, 286, 848, 294, 1732, 924, 11, 286, 519, 321, 1062, 362, 14622, 50614], "temperature": 0.0, "avg_logprob": -0.13546830007474717, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0021820245310664177}, {"id": 1188, "seek": 466768, "start": 4672.68, "end": 4675.68, "text": " that threshold in which computers will be better", "tokens": [50614, 300, 14678, 294, 597, 10807, 486, 312, 1101, 50764], "temperature": 0.0, "avg_logprob": -0.13546830007474717, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0021820245310664177}, {"id": 1189, "seek": 466768, "start": 4675.68, "end": 4678.68, "text": " at most human tasks than most humans, most average humans.", "tokens": [50764, 412, 881, 1952, 9608, 813, 881, 6255, 11, 881, 4274, 6255, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13546830007474717, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0021820245310664177}, {"id": 1190, "seek": 466768, "start": 4678.68, "end": 4685.68, "text": " And so that might be almost true now for non-physical tasks.", "tokens": [50914, 400, 370, 300, 1062, 312, 1920, 2074, 586, 337, 2107, 12, 950, 36280, 9608, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13546830007474717, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0021820245310664177}, {"id": 1191, "seek": 466768, "start": 4685.68, "end": 4694.68, "text": " So I took that twice as long as I thought it might.", "tokens": [51264, 407, 286, 1890, 300, 6091, 382, 938, 382, 286, 1194, 309, 1062, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13546830007474717, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0021820245310664177}, {"id": 1192, "seek": 469468, "start": 4694.68, "end": 4698.68, "text": " Yeah, no, I wouldn't say anything surprised me too much.", "tokens": [50364, 865, 11, 572, 11, 286, 2759, 380, 584, 1340, 6100, 385, 886, 709, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13848404174155377, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.010164924897253513}, {"id": 1193, "seek": 469468, "start": 4698.68, "end": 4701.68, "text": " It's still like definitely like, I got to admit,", "tokens": [50564, 467, 311, 920, 411, 2138, 411, 11, 286, 658, 281, 9796, 11, 50714], "temperature": 0.0, "avg_logprob": -0.13848404174155377, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.010164924897253513}, {"id": 1194, "seek": 469468, "start": 4701.68, "end": 4706.68, "text": " I had a very visceral reaction using GPT-4 for the first time,", "tokens": [50714, 286, 632, 257, 588, 1452, 47879, 5480, 1228, 26039, 51, 12, 19, 337, 264, 700, 565, 11, 50964], "temperature": 0.0, "avg_logprob": -0.13848404174155377, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.010164924897253513}, {"id": 1195, "seek": 469468, "start": 4706.68, "end": 4712.68, "text": " not because I found it surprising, but actually doing it.", "tokens": [50964, 406, 570, 286, 1352, 309, 8830, 11, 457, 767, 884, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13848404174155377, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.010164924897253513}, {"id": 1196, "seek": 469468, "start": 4712.68, "end": 4716.68, "text": " Something I was pretty sure would exist by about now,", "tokens": [51264, 6595, 286, 390, 1238, 988, 576, 2514, 538, 466, 586, 11, 51464], "temperature": 0.0, "avg_logprob": -0.13848404174155377, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.010164924897253513}, {"id": 1197, "seek": 469468, "start": 4716.68, "end": 4718.68, "text": " maybe a bit earlier.", "tokens": [51464, 1310, 257, 857, 3071, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13848404174155377, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.010164924897253513}, {"id": 1198, "seek": 469468, "start": 4718.68, "end": 4721.68, "text": " But actually using it definitely is different", "tokens": [51564, 583, 767, 1228, 309, 2138, 307, 819, 51714], "temperature": 0.0, "avg_logprob": -0.13848404174155377, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.010164924897253513}, {"id": 1199, "seek": 472168, "start": 4721.68, "end": 4724.68, "text": " to just feeling like it's probably on its way.", "tokens": [50364, 281, 445, 2633, 411, 309, 311, 1391, 322, 1080, 636, 13, 50514], "temperature": 0.0, "avg_logprob": -0.0541373093922933, "compression_ratio": 1.4429223744292237, "no_speech_prob": 0.001548190601170063}, {"id": 1200, "seek": 472168, "start": 4724.68, "end": 4729.68, "text": " And yeah, whatever GPT-5 looks like,", "tokens": [50514, 400, 1338, 11, 2035, 26039, 51, 12, 20, 1542, 411, 11, 50764], "temperature": 0.0, "avg_logprob": -0.0541373093922933, "compression_ratio": 1.4429223744292237, "no_speech_prob": 0.001548190601170063}, {"id": 1201, "seek": 472168, "start": 4729.68, "end": 4736.68, "text": " I'm sure I imagine I'll have the same visceral reaction.", "tokens": [50764, 286, 478, 988, 286, 3811, 286, 603, 362, 264, 912, 1452, 47879, 5480, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0541373093922933, "compression_ratio": 1.4429223744292237, "no_speech_prob": 0.001548190601170063}, {"id": 1202, "seek": 472168, "start": 4736.68, "end": 4740.68, "text": " It's really amazing to watch develop.", "tokens": [51114, 467, 311, 534, 2243, 281, 1159, 1499, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0541373093922933, "compression_ratio": 1.4429223744292237, "no_speech_prob": 0.001548190601170063}, {"id": 1203, "seek": 472168, "start": 4740.68, "end": 4742.68, "text": " We also have an exploration question.", "tokens": [51314, 492, 611, 362, 364, 16197, 1168, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0541373093922933, "compression_ratio": 1.4429223744292237, "no_speech_prob": 0.001548190601170063}, {"id": 1204, "seek": 472168, "start": 4742.68, "end": 4744.68, "text": " So what do you think is the most interesting", "tokens": [51414, 407, 437, 360, 291, 519, 307, 264, 881, 1880, 51514], "temperature": 0.0, "avg_logprob": -0.0541373093922933, "compression_ratio": 1.4429223744292237, "no_speech_prob": 0.001548190601170063}, {"id": 1205, "seek": 472168, "start": 4744.68, "end": 4747.68, "text": " unsolved question in AI?", "tokens": [51514, 2693, 29110, 1168, 294, 7318, 30, 51664], "temperature": 0.0, "avg_logprob": -0.0541373093922933, "compression_ratio": 1.4429223744292237, "no_speech_prob": 0.001548190601170063}, {"id": 1206, "seek": 472168, "start": 4747.68, "end": 4750.68, "text": " How do language models learn?", "tokens": [51664, 1012, 360, 2856, 5245, 1466, 30, 51814], "temperature": 0.0, "avg_logprob": -0.0541373093922933, "compression_ratio": 1.4429223744292237, "no_speech_prob": 0.001548190601170063}, {"id": 1207, "seek": 475068, "start": 4750.68, "end": 4752.68, "text": " What are the training dynamics?", "tokens": [50364, 708, 366, 264, 3097, 15679, 30, 50464], "temperature": 0.0, "avg_logprob": -0.157773235176183, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.004466831684112549}, {"id": 1208, "seek": 475068, "start": 4752.68, "end": 4759.68, "text": " Like I want to see, there was a great paper about Resnets", "tokens": [50464, 1743, 286, 528, 281, 536, 11, 456, 390, 257, 869, 3035, 466, 5015, 77, 1385, 50814], "temperature": 0.0, "avg_logprob": -0.157773235176183, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.004466831684112549}, {"id": 1209, "seek": 475068, "start": 4759.68, "end": 4765.68, "text": " a few years ago that showed how that was able to like plot", "tokens": [50814, 257, 1326, 924, 2057, 300, 4712, 577, 300, 390, 1075, 281, 411, 7542, 51114], "temperature": 0.0, "avg_logprob": -0.157773235176183, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.004466831684112549}, {"id": 1210, "seek": 475068, "start": 4765.68, "end": 4768.68, "text": " a kind of projected three-dimensional loss surface", "tokens": [51114, 257, 733, 295, 26231, 1045, 12, 18759, 4470, 3753, 51264], "temperature": 0.0, "avg_logprob": -0.157773235176183, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.004466831684112549}, {"id": 1211, "seek": 475068, "start": 4768.68, "end": 4774.68, "text": " for a ConvNet with and without skip connections.", "tokens": [51264, 337, 257, 2656, 85, 31890, 365, 293, 1553, 10023, 9271, 13, 51564], "temperature": 0.0, "avg_logprob": -0.157773235176183, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.004466831684112549}, {"id": 1212, "seek": 475068, "start": 4774.68, "end": 4777.68, "text": " And you could very clearly see without the skip connections,", "tokens": [51564, 400, 291, 727, 588, 4448, 536, 1553, 264, 10023, 9271, 11, 51714], "temperature": 0.0, "avg_logprob": -0.157773235176183, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.004466831684112549}, {"id": 1213, "seek": 477768, "start": 4777.68, "end": 4779.68, "text": " it was super bumpy and with the skip connections,", "tokens": [50364, 309, 390, 1687, 49400, 293, 365, 264, 10023, 9271, 11, 50464], "temperature": 0.0, "avg_logprob": -0.1383702938373272, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.004068279173225164}, {"id": 1214, "seek": 477768, "start": 4779.68, "end": 4783.68, "text": " it was super smooth.", "tokens": [50464, 309, 390, 1687, 5508, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1383702938373272, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.004068279173225164}, {"id": 1215, "seek": 477768, "start": 4783.68, "end": 4785.68, "text": " That's the kind of work we need.", "tokens": [50664, 663, 311, 264, 733, 295, 589, 321, 643, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1383702938373272, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.004068279173225164}, {"id": 1216, "seek": 477768, "start": 4785.68, "end": 4787.68, "text": " So there was actually an interesting blog post", "tokens": [50764, 407, 456, 390, 767, 364, 1880, 6968, 2183, 50864], "temperature": 0.0, "avg_logprob": -0.1383702938373272, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.004068279173225164}, {"id": 1217, "seek": 477768, "start": 4787.68, "end": 4790.68, "text": " that came out just today from the PyTorch team", "tokens": [50864, 300, 1361, 484, 445, 965, 490, 264, 9953, 51, 284, 339, 1469, 51014], "temperature": 0.0, "avg_logprob": -0.1383702938373272, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.004068279173225164}, {"id": 1218, "seek": 477768, "start": 4790.68, "end": 4793.68, "text": " where some of them have created this like 3D", "tokens": [51014, 689, 512, 295, 552, 362, 2942, 341, 411, 805, 35, 51164], "temperature": 0.0, "avg_logprob": -0.1383702938373272, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.004068279173225164}, {"id": 1219, "seek": 477768, "start": 4793.68, "end": 4796.68, "text": " matrix product visualization thing.", "tokens": [51164, 8141, 1674, 25801, 551, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1383702938373272, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.004068279173225164}, {"id": 1220, "seek": 477768, "start": 4796.68, "end": 4800.68, "text": " And they actually showed some nice examples", "tokens": [51314, 400, 436, 767, 4712, 512, 1481, 5110, 51514], "temperature": 0.0, "avg_logprob": -0.1383702938373272, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.004068279173225164}, {"id": 1221, "seek": 477768, "start": 4800.68, "end": 4803.68, "text": " of like a GPT-2 attention layer", "tokens": [51514, 295, 411, 257, 26039, 51, 12, 17, 3202, 4583, 51664], "temperature": 0.0, "avg_logprob": -0.1383702938373272, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.004068279173225164}, {"id": 1222, "seek": 477768, "start": 4803.68, "end": 4806.68, "text": " and showed an animation and said,", "tokens": [51664, 293, 4712, 364, 9603, 293, 848, 11, 51814], "temperature": 0.0, "avg_logprob": -0.1383702938373272, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.004068279173225164}, {"id": 1223, "seek": 480668, "start": 4806.68, "end": 4808.68, "text": " like, if you look at this, we can actually see a bit", "tokens": [50364, 411, 11, 498, 291, 574, 412, 341, 11, 321, 393, 767, 536, 257, 857, 50464], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1224, "seek": 480668, "start": 4808.68, "end": 4809.68, "text": " about what it's doing.", "tokens": [50464, 466, 437, 309, 311, 884, 13, 50514], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1225, "seek": 480668, "start": 4809.68, "end": 4812.68, "text": " You know, so again, it reminds me of the Zeiler", "tokens": [50514, 509, 458, 11, 370, 797, 11, 309, 12025, 385, 295, 264, 4853, 5441, 50664], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1226, "seek": 480668, "start": 4812.68, "end": 4815.68, "text": " and Fergus, you know, ConvNet paper.", "tokens": [50664, 293, 36790, 11, 291, 458, 11, 2656, 85, 31890, 3035, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1227, "seek": 480668, "start": 4815.68, "end": 4818.68, "text": " That was the first one to do these reverse convolutions", "tokens": [50814, 663, 390, 264, 700, 472, 281, 360, 613, 9943, 3754, 15892, 50964], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1228, "seek": 480668, "start": 4818.68, "end": 4820.68, "text": " to show what's actually being learned", "tokens": [50964, 281, 855, 437, 311, 767, 885, 3264, 51064], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1229, "seek": 480668, "start": 4820.68, "end": 4821.68, "text": " in each layer in a ConvNet.", "tokens": [51064, 294, 1184, 4583, 294, 257, 2656, 85, 31890, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1230, "seek": 480668, "start": 4821.68, "end": 4824.68, "text": " Yeah, we need a lot more of this, like,", "tokens": [51114, 865, 11, 321, 643, 257, 688, 544, 295, 341, 11, 411, 11, 51264], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1231, "seek": 480668, "start": 4824.68, "end": 4827.68, "text": " what is going on inside these models?", "tokens": [51264, 437, 307, 516, 322, 1854, 613, 5245, 30, 51414], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1232, "seek": 480668, "start": 4827.68, "end": 4829.68, "text": " How do they actually learn?", "tokens": [51414, 1012, 360, 436, 767, 1466, 30, 51514], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1233, "seek": 480668, "start": 4829.68, "end": 4831.68, "text": " And then how can we use those insights", "tokens": [51514, 400, 550, 577, 393, 321, 764, 729, 14310, 51614], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1234, "seek": 480668, "start": 4831.68, "end": 4835.68, "text": " to help them to learn better?", "tokens": [51614, 281, 854, 552, 281, 1466, 1101, 30, 51814], "temperature": 0.0, "avg_logprob": -0.14429718515147333, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0015486048068851233}, {"id": 1235, "seek": 483568, "start": 4835.68, "end": 4836.68, "text": " So I think that would be one.", "tokens": [50364, 407, 286, 519, 300, 576, 312, 472, 13, 50414], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1236, "seek": 483568, "start": 4836.68, "end": 4838.68, "text": " The other exploration I'd really like to see", "tokens": [50414, 440, 661, 16197, 286, 1116, 534, 411, 281, 536, 50514], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1237, "seek": 483568, "start": 4838.68, "end": 4841.68, "text": " is a much more rigorous analysis", "tokens": [50514, 307, 257, 709, 544, 29882, 5215, 50664], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1238, "seek": 483568, "start": 4841.68, "end": 4844.68, "text": " of what kind of data do they need,", "tokens": [50664, 295, 437, 733, 295, 1412, 360, 436, 643, 11, 50814], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1239, "seek": 483568, "start": 4844.68, "end": 4847.68, "text": " at what level, and when do they need it,", "tokens": [50814, 412, 437, 1496, 11, 293, 562, 360, 436, 643, 309, 11, 50964], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1240, "seek": 483568, "start": 4847.68, "end": 4848.68, "text": " and how often.", "tokens": [50964, 293, 577, 2049, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1241, "seek": 483568, "start": 4848.68, "end": 4851.68, "text": " So that kind of like data set mixing, curation,", "tokens": [51014, 407, 300, 733, 295, 411, 1412, 992, 11983, 11, 1262, 399, 11, 51164], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1242, "seek": 483568, "start": 4851.68, "end": 4854.68, "text": " so forth in order to get the best capabilities.", "tokens": [51164, 370, 5220, 294, 1668, 281, 483, 264, 1151, 10862, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1243, "seek": 483568, "start": 4854.68, "end": 4855.68, "text": " Yeah.", "tokens": [51314, 865, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1244, "seek": 483568, "start": 4855.68, "end": 4857.68, "text": " How much is Wikipedia?", "tokens": [51364, 1012, 709, 307, 28999, 30, 51464], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1245, "seek": 483568, "start": 4857.68, "end": 4858.68, "text": " Yeah.", "tokens": [51464, 865, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1246, "seek": 483568, "start": 4858.68, "end": 4859.68, "text": " Very uncertain.", "tokens": [51514, 4372, 11308, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1247, "seek": 483568, "start": 4859.68, "end": 4861.68, "text": " You know, to fine-tune what kind of mix", "tokens": [51564, 509, 458, 11, 281, 2489, 12, 83, 2613, 437, 733, 295, 2890, 51664], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1248, "seek": 483568, "start": 4861.68, "end": 4864.68, "text": " do you need for it to keep its capabilities", "tokens": [51664, 360, 291, 643, 337, 309, 281, 1066, 1080, 10862, 51814], "temperature": 0.0, "avg_logprob": -0.18477149443192917, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0005879810196347535}, {"id": 1249, "seek": 486468, "start": 4864.68, "end": 4866.68, "text": " and what are the kind of underlying capabilities", "tokens": [50364, 293, 437, 366, 264, 733, 295, 14217, 10862, 50464], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1250, "seek": 486468, "start": 4866.68, "end": 4867.68, "text": " that it most needs to keep?", "tokens": [50464, 300, 309, 881, 2203, 281, 1066, 30, 50514], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1251, "seek": 486468, "start": 4867.68, "end": 4869.68, "text": " And if it loses those, it would lose all these other ones", "tokens": [50514, 400, 498, 309, 18293, 729, 11, 309, 576, 3624, 439, 613, 661, 2306, 50614], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1252, "seek": 486468, "start": 4869.68, "end": 4871.68, "text": " and what data do you need to keep those?", "tokens": [50614, 293, 437, 1412, 360, 291, 643, 281, 1066, 729, 30, 50714], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1253, "seek": 486468, "start": 4871.68, "end": 4873.68, "text": " And, you know, are there things we can do", "tokens": [50714, 400, 11, 291, 458, 11, 366, 456, 721, 321, 393, 360, 50814], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1254, "seek": 486468, "start": 4873.68, "end": 4876.68, "text": " to change the loss function, to help it,", "tokens": [50814, 281, 1319, 264, 4470, 2445, 11, 281, 854, 309, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1255, "seek": 486468, "start": 4876.68, "end": 4880.68, "text": " to not forget to do things, stuff like that?", "tokens": [50964, 281, 406, 2870, 281, 360, 721, 11, 1507, 411, 300, 30, 51164], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1256, "seek": 486468, "start": 4880.68, "end": 4881.68, "text": " Awesome.", "tokens": [51164, 10391, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1257, "seek": 486468, "start": 4881.68, "end": 4884.68, "text": " And yeah, before wrapping, what's one message,", "tokens": [51214, 400, 1338, 11, 949, 21993, 11, 437, 311, 472, 3636, 11, 51364], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1258, "seek": 486468, "start": 4884.68, "end": 4886.68, "text": " one idea you want everyone to remember", "tokens": [51364, 472, 1558, 291, 528, 1518, 281, 1604, 51464], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1259, "seek": 486468, "start": 4886.68, "end": 4887.68, "text": " and think about?", "tokens": [51464, 293, 519, 466, 30, 51514], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1260, "seek": 486468, "start": 4887.68, "end": 4889.68, "text": " You know, I guess the main thing I want everybody", "tokens": [51514, 509, 458, 11, 286, 2041, 264, 2135, 551, 286, 528, 2201, 51614], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1261, "seek": 486468, "start": 4889.68, "end": 4891.68, "text": " to remember is that, you know,", "tokens": [51614, 281, 1604, 307, 300, 11, 291, 458, 11, 51714], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1262, "seek": 486468, "start": 4891.68, "end": 4893.68, "text": " there's a lot of people in the world", "tokens": [51714, 456, 311, 257, 688, 295, 561, 294, 264, 1002, 51814], "temperature": 0.0, "avg_logprob": -0.08725813188050922, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0012060848530381918}, {"id": 1263, "seek": 489368, "start": 4893.68, "end": 4895.68, "text": " and they have a lot of, you know,", "tokens": [50364, 293, 436, 362, 257, 688, 295, 11, 291, 458, 11, 50464], "temperature": 0.0, "avg_logprob": -0.07741898113919288, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.00018511431699153036}, {"id": 1264, "seek": 489368, "start": 4895.68, "end": 4898.68, "text": " diverse experiences and capabilities", "tokens": [50464, 9521, 5235, 293, 10862, 50614], "temperature": 0.0, "avg_logprob": -0.07741898113919288, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.00018511431699153036}, {"id": 1265, "seek": 489368, "start": 4898.68, "end": 4902.68, "text": " and, you know, they all matter.", "tokens": [50614, 293, 11, 291, 458, 11, 436, 439, 1871, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07741898113919288, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.00018511431699153036}, {"id": 1266, "seek": 489368, "start": 4902.68, "end": 4906.68, "text": " And now that we have, you know,", "tokens": [50814, 400, 586, 300, 321, 362, 11, 291, 458, 11, 51014], "temperature": 0.0, "avg_logprob": -0.07741898113919288, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.00018511431699153036}, {"id": 1267, "seek": 489368, "start": 4906.68, "end": 4909.68, "text": " nearly powerful technology in our lives,", "tokens": [51014, 6217, 4005, 2899, 294, 527, 2909, 11, 51164], "temperature": 0.0, "avg_logprob": -0.07741898113919288, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.00018511431699153036}, {"id": 1268, "seek": 489368, "start": 4909.68, "end": 4911.68, "text": " we could think of that in one of two ways.", "tokens": [51164, 321, 727, 519, 295, 300, 294, 472, 295, 732, 2098, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07741898113919288, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.00018511431699153036}, {"id": 1269, "seek": 489368, "start": 4911.68, "end": 4917.68, "text": " One would be, gee, that's really scary", "tokens": [51264, 1485, 576, 312, 11, 24105, 11, 300, 311, 534, 6958, 51564], "temperature": 0.0, "avg_logprob": -0.07741898113919288, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.00018511431699153036}, {"id": 1270, "seek": 489368, "start": 4917.68, "end": 4919.68, "text": " what would happen if all of these people in the world", "tokens": [51564, 437, 576, 1051, 498, 439, 295, 613, 561, 294, 264, 1002, 51664], "temperature": 0.0, "avg_logprob": -0.07741898113919288, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.00018511431699153036}, {"id": 1271, "seek": 489368, "start": 4919.68, "end": 4921.68, "text": " had access to this technology.", "tokens": [51664, 632, 2105, 281, 341, 2899, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07741898113919288, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.00018511431699153036}, {"id": 1272, "seek": 492168, "start": 4921.68, "end": 4923.68, "text": " One of them might be bad people.", "tokens": [50364, 1485, 295, 552, 1062, 312, 1578, 561, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09895060426097806, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005298067815601826}, {"id": 1273, "seek": 492168, "start": 4923.68, "end": 4925.68, "text": " Let's make sure they can't have it.", "tokens": [50464, 961, 311, 652, 988, 436, 393, 380, 362, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09895060426097806, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005298067815601826}, {"id": 1274, "seek": 492168, "start": 4925.68, "end": 4928.68, "text": " Or one might be, wow,", "tokens": [50564, 1610, 472, 1062, 312, 11, 6076, 11, 50714], "temperature": 0.0, "avg_logprob": -0.09895060426097806, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005298067815601826}, {"id": 1275, "seek": 492168, "start": 4928.68, "end": 4930.68, "text": " if all those people in the world are better,", "tokens": [50714, 498, 439, 729, 561, 294, 264, 1002, 366, 1101, 11, 50814], "temperature": 0.0, "avg_logprob": -0.09895060426097806, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005298067815601826}, {"id": 1276, "seek": 492168, "start": 4930.68, "end": 4933.68, "text": " a lot of them could really improve the lives", "tokens": [50814, 257, 688, 295, 552, 727, 534, 3470, 264, 2909, 50964], "temperature": 0.0, "avg_logprob": -0.09895060426097806, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005298067815601826}, {"id": 1277, "seek": 492168, "start": 4933.68, "end": 4937.68, "text": " of a lot of humanity if they had this tool.", "tokens": [50964, 295, 257, 688, 295, 10243, 498, 436, 632, 341, 2290, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09895060426097806, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005298067815601826}, {"id": 1278, "seek": 492168, "start": 4937.68, "end": 4939.68, "text": " This has always been the case, you know,", "tokens": [51164, 639, 575, 1009, 668, 264, 1389, 11, 291, 458, 11, 51264], "temperature": 0.0, "avg_logprob": -0.09895060426097806, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005298067815601826}, {"id": 1279, "seek": 492168, "start": 4939.68, "end": 4941.68, "text": " from the invention of writing", "tokens": [51264, 490, 264, 22265, 295, 3579, 51364], "temperature": 0.0, "avg_logprob": -0.09895060426097806, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005298067815601826}, {"id": 1280, "seek": 492168, "start": 4941.68, "end": 4943.68, "text": " to the invention of the printing press", "tokens": [51364, 281, 264, 22265, 295, 264, 14699, 1886, 51464], "temperature": 0.0, "avg_logprob": -0.09895060426097806, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005298067815601826}, {"id": 1281, "seek": 492168, "start": 4943.68, "end": 4946.68, "text": " to the, you know, development of education.", "tokens": [51464, 281, 264, 11, 291, 458, 11, 3250, 295, 3309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09895060426097806, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005298067815601826}, {"id": 1282, "seek": 492168, "start": 4946.68, "end": 4949.68, "text": " And it's been a constant battle", "tokens": [51614, 400, 309, 311, 668, 257, 5754, 4635, 51764], "temperature": 0.0, "avg_logprob": -0.09895060426097806, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.005298067815601826}, {"id": 1283, "seek": 494968, "start": 4949.68, "end": 4953.68, "text": " between people who think that distributed power is unsafe", "tokens": [50364, 1296, 561, 567, 519, 300, 12631, 1347, 307, 35948, 50564], "temperature": 0.0, "avg_logprob": -0.08435343488862243, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.006181825418025255}, {"id": 1284, "seek": 494968, "start": 4953.68, "end": 4956.68, "text": " and it should be held on to by an elite few", "tokens": [50564, 293, 309, 820, 312, 5167, 322, 281, 538, 364, 17801, 1326, 50714], "temperature": 0.0, "avg_logprob": -0.08435343488862243, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.006181825418025255}, {"id": 1285, "seek": 494968, "start": 4956.68, "end": 4963.68, "text": " and people who think that humanity on net,", "tokens": [50714, 293, 561, 567, 519, 300, 10243, 322, 2533, 11, 51064], "temperature": 0.0, "avg_logprob": -0.08435343488862243, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.006181825418025255}, {"id": 1286, "seek": 494968, "start": 4963.68, "end": 4966.68, "text": " you know, is a marvellous species,", "tokens": [51064, 291, 458, 11, 307, 257, 1849, 48592, 563, 6172, 11, 51214], "temperature": 0.0, "avg_logprob": -0.08435343488862243, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.006181825418025255}, {"id": 1287, "seek": 494968, "start": 4966.68, "end": 4969.68, "text": " particularly when part of a society and a civilization", "tokens": [51214, 4098, 562, 644, 295, 257, 4086, 293, 257, 18036, 51364], "temperature": 0.0, "avg_logprob": -0.08435343488862243, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.006181825418025255}, {"id": 1288, "seek": 494968, "start": 4969.68, "end": 4971.68, "text": " and we should do everything we can", "tokens": [51364, 293, 321, 820, 360, 1203, 321, 393, 51464], "temperature": 0.0, "avg_logprob": -0.08435343488862243, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.006181825418025255}, {"id": 1289, "seek": 494968, "start": 4971.68, "end": 4975.68, "text": " to enable more of them to contribute.", "tokens": [51464, 281, 9528, 544, 295, 552, 281, 10586, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08435343488862243, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.006181825418025255}, {"id": 1290, "seek": 497568, "start": 4975.68, "end": 4979.68, "text": " There's a really big conversation right now", "tokens": [50364, 821, 311, 257, 534, 955, 3761, 558, 586, 50564], "temperature": 0.0, "avg_logprob": -0.10989400404918043, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.03250560909509659}, {"id": 1291, "seek": 497568, "start": 4979.68, "end": 4984.68, "text": " and, you know, I want to see more and more people", "tokens": [50564, 293, 11, 291, 458, 11, 286, 528, 281, 536, 544, 293, 544, 561, 50814], "temperature": 0.0, "avg_logprob": -0.10989400404918043, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.03250560909509659}, {"id": 1292, "seek": 497568, "start": 4984.68, "end": 4990.68, "text": " showing up and showing what, you know,", "tokens": [50814, 4099, 493, 293, 4099, 437, 11, 291, 458, 11, 51114], "temperature": 0.0, "avg_logprob": -0.10989400404918043, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.03250560909509659}, {"id": 1293, "seek": 497568, "start": 4990.68, "end": 4993.68, "text": " what the great unwashed masses out there", "tokens": [51114, 437, 264, 869, 14853, 12219, 23935, 484, 456, 51264], "temperature": 0.0, "avg_logprob": -0.10989400404918043, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.03250560909509659}, {"id": 1294, "seek": 497568, "start": 4993.68, "end": 4995.68, "text": " can actually achieve, you know,", "tokens": [51264, 393, 767, 4584, 11, 291, 458, 11, 51364], "temperature": 0.0, "avg_logprob": -0.10989400404918043, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.03250560909509659}, {"id": 1295, "seek": 497568, "start": 4995.68, "end": 4998.68, "text": " that actually, you know, regular people", "tokens": [51364, 300, 767, 11, 291, 458, 11, 3890, 561, 51514], "temperature": 0.0, "avg_logprob": -0.10989400404918043, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.03250560909509659}, {"id": 1296, "seek": 497568, "start": 4998.68, "end": 5002.68, "text": " are going to do a lot of really valuable work", "tokens": [51514, 366, 516, 281, 360, 257, 688, 295, 534, 8263, 589, 51714], "temperature": 0.0, "avg_logprob": -0.10989400404918043, "compression_ratio": 1.7117647058823529, "no_speech_prob": 0.03250560909509659}, {"id": 1297, "seek": 500268, "start": 5002.68, "end": 5008.68, "text": " and actually help us be, you know, more safe", "tokens": [50364, 293, 767, 854, 505, 312, 11, 291, 458, 11, 544, 3273, 50664], "temperature": 0.0, "avg_logprob": -0.06948016861737785, "compression_ratio": 1.5968992248062015, "no_speech_prob": 0.003935064189136028}, {"id": 1298, "seek": 500268, "start": 5008.68, "end": 5012.68, "text": " and also flourishing in our lives", "tokens": [50664, 293, 611, 7693, 3807, 294, 527, 2909, 50864], "temperature": 0.0, "avg_logprob": -0.06948016861737785, "compression_ratio": 1.5968992248062015, "no_speech_prob": 0.003935064189136028}, {"id": 1299, "seek": 500268, "start": 5012.68, "end": 5017.68, "text": " and providing a future for our children to flourish in,", "tokens": [50864, 293, 6530, 257, 2027, 337, 527, 2227, 281, 38311, 294, 11, 51114], "temperature": 0.0, "avg_logprob": -0.06948016861737785, "compression_ratio": 1.5968992248062015, "no_speech_prob": 0.003935064189136028}, {"id": 1300, "seek": 500268, "start": 5017.68, "end": 5024.68, "text": " you know, if we lock things down", "tokens": [51114, 291, 458, 11, 498, 321, 4017, 721, 760, 51464], "temperature": 0.0, "avg_logprob": -0.06948016861737785, "compression_ratio": 1.5968992248062015, "no_speech_prob": 0.003935064189136028}, {"id": 1301, "seek": 500268, "start": 5024.68, "end": 5028.68, "text": " to the people that we think, you know,", "tokens": [51464, 281, 264, 561, 300, 321, 519, 11, 291, 458, 11, 51664], "temperature": 0.0, "avg_logprob": -0.06948016861737785, "compression_ratio": 1.5968992248062015, "no_speech_prob": 0.003935064189136028}, {"id": 1302, "seek": 502868, "start": 5028.68, "end": 5032.68, "text": " the elites that we think can be trusted to run it for us.", "tokens": [50364, 264, 44678, 300, 321, 519, 393, 312, 16034, 281, 1190, 309, 337, 505, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14757488683326958, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.005799923092126846}, {"id": 1303, "seek": 502868, "start": 5032.68, "end": 5041.68, "text": " Yeah, I think all bets are off about where that lives as a society, you know.", "tokens": [50564, 865, 11, 286, 519, 439, 39922, 366, 766, 466, 689, 300, 2909, 382, 257, 4086, 11, 291, 458, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14757488683326958, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.005799923092126846}, {"id": 1304, "seek": 502868, "start": 5041.68, "end": 5043.68, "text": " Yep.", "tokens": [51014, 7010, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14757488683326958, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.005799923092126846}, {"id": 1305, "seek": 502868, "start": 5043.68, "end": 5046.68, "text": " Now, that's an important message", "tokens": [51114, 823, 11, 300, 311, 364, 1021, 3636, 51264], "temperature": 0.0, "avg_logprob": -0.14757488683326958, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.005799923092126846}, {"id": 1306, "seek": 502868, "start": 5046.68, "end": 5048.68, "text": " and yeah, that's why we've been promoting a lot", "tokens": [51264, 293, 1338, 11, 300, 311, 983, 321, 600, 668, 16383, 257, 688, 51364], "temperature": 0.0, "avg_logprob": -0.14757488683326958, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.005799923092126846}, {"id": 1307, "seek": 502868, "start": 5048.68, "end": 5051.68, "text": " of open source developers, open source communities.", "tokens": [51364, 295, 1269, 4009, 8849, 11, 1269, 4009, 4456, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14757488683326958, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.005799923092126846}, {"id": 1308, "seek": 502868, "start": 5051.68, "end": 5055.68, "text": " I think letting the builders build and explore,", "tokens": [51514, 286, 519, 8295, 264, 36281, 1322, 293, 6839, 11, 51714], "temperature": 0.0, "avg_logprob": -0.14757488683326958, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.005799923092126846}, {"id": 1309, "seek": 502868, "start": 5055.68, "end": 5057.68, "text": " that's always a good idea.", "tokens": [51714, 300, 311, 1009, 257, 665, 1558, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14757488683326958, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.005799923092126846}, {"id": 1310, "seek": 505768, "start": 5057.68, "end": 5059.68, "text": " Thank you so much for coming on, Jeremy.", "tokens": [50364, 1044, 291, 370, 709, 337, 1348, 322, 11, 17809, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09359213157936379, "compression_ratio": 1.0657894736842106, "no_speech_prob": 0.025900376960635185}, {"id": 1311, "seek": 505768, "start": 5059.68, "end": 5060.68, "text": " This was great.", "tokens": [50464, 639, 390, 869, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09359213157936379, "compression_ratio": 1.0657894736842106, "no_speech_prob": 0.025900376960635185}, {"id": 1312, "seek": 505768, "start": 5060.68, "end": 5062.68, "text": " Thank you for having me.", "tokens": [50514, 1044, 291, 337, 1419, 385, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09359213157936379, "compression_ratio": 1.0657894736842106, "no_speech_prob": 0.025900376960635185}], "language": "en"}