1
00:00:00,000 --> 00:00:05,080
Hey everyone, welcome to the Late In Space podcast.

2
00:00:05,080 --> 00:00:09,800
This is Swix, writer and editor of Late In Space, and Alessio is taking over with the

3
00:00:09,800 --> 00:00:10,800
intros.

4
00:00:10,800 --> 00:00:12,640
Alessio's partner and CT1 residents at Decibel Partners.

5
00:00:12,640 --> 00:00:19,640
Hey everyone, today we have GeoHot on the podcast, aka George Hots for the human name.

6
00:00:19,640 --> 00:00:22,720
Everybody knows George, so I'm not going to do a big intro, a couple things that people

7
00:00:22,720 --> 00:00:23,720
might have missed.

8
00:00:23,720 --> 00:00:28,040
So you were the first to unlock the iPhone, you traded the first ever unlocked iPhone for

9
00:00:28,040 --> 00:00:31,960
Nissan 350Z and three new iPhones.

10
00:00:31,960 --> 00:00:36,680
You were then one of the first people to break into the PS3 around arbitrary code.

11
00:00:36,680 --> 00:00:40,680
You got sued by Sony, you wrote a rap song to fight against that, which is still live

12
00:00:40,680 --> 00:00:44,200
on YouTube, which we're going to have on the show notes.

13
00:00:44,200 --> 00:00:49,520
Then you did not go to Tesla to build vision, and instead you started ComIi, which was an

14
00:00:49,520 --> 00:00:54,600
amazing engineering feat in itself until you got a season disease from the government

15
00:00:54,600 --> 00:00:59,200
to not put these things on the street, turned that into a research-only project.

16
00:00:59,200 --> 00:01:00,200
You know they're out there.

17
00:01:00,200 --> 00:01:04,800
Yeah, yeah, no, no, no, they're out there, but they're not a, you know, you market them

18
00:01:04,800 --> 00:01:06,800
as a research kind of like no warranty.

19
00:01:06,800 --> 00:01:10,040
Because I use the word DevKit, that's not about the government, that's nothing to do

20
00:01:10,040 --> 00:01:11,040
with the government.

21
00:01:11,040 --> 00:01:12,760
We offer a great one, you're warranty.

22
00:01:12,760 --> 00:01:17,680
The truth about that is it's gatekeeping.

23
00:01:17,680 --> 00:01:21,480
What's the difference between a DevKit and not a DevKit, nothing.

24
00:01:21,480 --> 00:01:25,480
What's the question of do you think it's for you, and if you think it's for you, buy it.

25
00:01:25,480 --> 00:01:26,480
It's a consumer product.

26
00:01:26,480 --> 00:01:27,480
We call it a DevKit.

27
00:01:27,480 --> 00:01:30,480
If you have a problem with that, it's not for you.

28
00:01:30,480 --> 00:01:31,480
Great framing.

29
00:01:31,480 --> 00:01:33,240
That's great insight.

30
00:01:33,240 --> 00:01:36,440
And then I was going through your broadcast to get to the day, you've heard this post

31
00:01:36,440 --> 00:01:41,440
about the hero's journey, and you link this thing called the portal story, which is kind

32
00:01:41,440 --> 00:01:46,280
of the set of stories in movies and books about people living this arbitrary life and

33
00:01:46,280 --> 00:01:50,760
then they run to this magic portals, kind of takes them into a new, very exciting life

34
00:01:50,760 --> 00:01:52,040
and dimension.

35
00:01:52,040 --> 00:01:55,800
When you've heard that post, you talked about TinyGrad, which is one of the projects you're

36
00:01:55,800 --> 00:01:56,800
working on today.

37
00:01:56,800 --> 00:01:59,960
And you mentioned this is more of a hobby, something that is not going to change the

38
00:01:59,960 --> 00:02:00,960
course of history.

39
00:02:00,960 --> 00:02:04,600
Obviously, you're not going full speed into it, so we will learn more about what was the

40
00:02:04,600 --> 00:02:08,000
portal that you run into to get here.

41
00:02:08,000 --> 00:02:13,880
Well, what you realize is, you know what made me realize that I absolutely had to do the

42
00:02:13,880 --> 00:02:14,880
company?

43
00:02:14,880 --> 00:02:16,960
Seeing Sam Maltlin go in front of Congress.

44
00:02:16,960 --> 00:02:18,960
Why?

45
00:02:18,960 --> 00:02:22,120
What are the odds they nationalize NVIDIA?

46
00:02:22,120 --> 00:02:26,760
What are the odds that large organizations in the government, but of course I repeat

47
00:02:26,760 --> 00:02:32,560
myself, decide to try to clamp down on accessibility of ML compute?

48
00:02:32,560 --> 00:02:39,880
I want to make sure that can't happen structurally, so that's why I realize that it's really important

49
00:02:39,880 --> 00:02:40,880
that I do this.

50
00:02:40,880 --> 00:02:44,960
And actually, from a more practical perspective, I'm working with NVIDIA and Qualcomm to buy

51
00:02:44,960 --> 00:02:45,960
chips.

52
00:02:45,960 --> 00:02:48,800
NVIDIA has the best training chips, Qualcomm has the best inference chips.

53
00:02:48,800 --> 00:02:52,160
Working with these companies is really difficult, so I'd like to start another organization

54
00:02:52,160 --> 00:02:57,640
that eventually in the limit either works with people to make chips or makes chips itself

55
00:02:57,640 --> 00:03:01,880
and makes them available to anybody.

56
00:03:01,880 --> 00:03:06,120
You share kind of three core thesis to Tiny Core, maybe we can dive into each of them.

57
00:03:06,120 --> 00:03:12,680
So, XLA, PrimeTorch, those are the complex instruction system, TinyGrad is the restricted

58
00:03:12,680 --> 00:03:17,080
instruction system, so you're kind of focused on, again, TinyGrad being small, not being

59
00:03:17,080 --> 00:03:21,640
over complicated and trying to get as close to the DSP as possible in a way where it's

60
00:03:21,640 --> 00:03:22,640
at more.

61
00:03:22,640 --> 00:03:26,240
Well, it's a very clear analogy from how processors developed.

62
00:03:26,240 --> 00:03:31,120
So a lot of processors back in the day were CISC, complex instruction set, system 360

63
00:03:31,120 --> 00:03:33,160
and then x86.

64
00:03:33,160 --> 00:03:35,840
Then this isn't how things stayed.

65
00:03:35,840 --> 00:03:41,320
They went to now the most common processors on ARM and people are excited about RISC

66
00:03:41,320 --> 00:03:42,320
5.

67
00:03:42,320 --> 00:03:45,000
RISC 5 is even less complex than ARM.

68
00:03:45,000 --> 00:03:49,520
No one is excited about CISC processors anymore, they're excited about RISC Reduce Instruction

69
00:03:49,520 --> 00:03:50,920
Set processors.

70
00:03:50,920 --> 00:03:57,520
So TinyGrad is we're going to make a RISC offset for all ML models and yeah, it can

71
00:03:57,520 --> 00:04:03,400
run all ML models with basically 25 instead of the 250 of XLA or PrimeTorch.

72
00:04:03,400 --> 00:04:05,160
So about 10X less complex.

73
00:04:05,160 --> 00:04:06,160
Yep.

74
00:04:06,160 --> 00:04:08,760
You talk a lot about existing AI chips.

75
00:04:08,760 --> 00:04:12,840
You said if you can write a fast ML framework for GPUs, you just can't write one for your

76
00:04:12,840 --> 00:04:13,840
own chip.

77
00:04:13,840 --> 00:04:17,400
So that's another one of your core insights, I don't know if you want to expand on that.

78
00:04:17,400 --> 00:04:18,400
Yeah.

79
00:04:18,400 --> 00:04:19,560
I mean, your chip is worse, right?

80
00:04:19,560 --> 00:04:22,680
There's no way the chip that you're going to tape out, especially on the first try is

81
00:04:22,680 --> 00:04:25,760
going to be easier to use than an AMD GPU, right?

82
00:04:25,760 --> 00:04:28,400
And yet there's no good stack for AMD GPUs.

83
00:04:28,400 --> 00:04:31,480
So why do you think you can make one for your chip?

84
00:04:31,480 --> 00:04:32,480
You can't.

85
00:04:32,480 --> 00:04:33,480
Right?

86
00:04:33,480 --> 00:04:37,480
The only company, there's one other company, aside from Nvidia, who's succeeded at all

87
00:04:37,480 --> 00:04:38,800
at making training chips.

88
00:04:38,800 --> 00:04:40,800
What company?

89
00:04:40,800 --> 00:04:42,800
AMD.

90
00:04:42,800 --> 00:04:43,800
Intel?

91
00:04:43,800 --> 00:04:44,800
No.

92
00:04:44,800 --> 00:04:45,800
No.

93
00:04:45,800 --> 00:04:46,800
No.

94
00:04:46,800 --> 00:04:47,800
I've never trained.

95
00:04:47,800 --> 00:04:48,800
Who's trained a model on AMD or Intel?

96
00:04:48,800 --> 00:04:49,800
Nobody on AMD.

97
00:04:49,800 --> 00:04:50,800
Cerebrus.

98
00:04:50,800 --> 00:04:54,120
Cerebrus, I'm talking about, you might know some startups who trained models on these

99
00:04:54,120 --> 00:04:55,120
chips.

100
00:04:55,120 --> 00:04:59,480
I'm surprised no one immediately gets this because there is one other chip, aside from

101
00:04:59,480 --> 00:05:02,240
Nvidia, that normal people have actually used for training.

102
00:05:02,240 --> 00:05:03,880
That's a real neural engine?

103
00:05:03,880 --> 00:05:04,880
No.

104
00:05:04,880 --> 00:05:05,880
Used for training.

105
00:05:05,880 --> 00:05:06,880
No.

106
00:05:06,880 --> 00:05:08,880
You can only buy them in the cloud.

107
00:05:08,880 --> 00:05:09,880
Oh, TPU.

108
00:05:09,880 --> 00:05:10,880
Exactly.

109
00:05:10,880 --> 00:05:11,880
Yeah.

110
00:05:11,960 --> 00:05:13,800
So, mid-journey is trained on TPU.

111
00:05:13,800 --> 00:05:14,800
Right?

112
00:05:14,800 --> 00:05:19,080
Like, a lot of startups do actually train on TPUs, and they're the only other successful

113
00:05:19,080 --> 00:05:21,280
training chip, aside from Nvidia.

114
00:05:21,280 --> 00:05:26,240
But what's unique about Google is that they also wrote their own ML framework, right?

115
00:05:26,240 --> 00:05:30,040
And if you can't write your own ML framework that is performant on Nvidia, there's no way

116
00:05:30,040 --> 00:05:32,720
you're going to make it performant on your...

117
00:05:32,720 --> 00:05:35,960
And they started from TensorFlow, and then they made the chip after.

118
00:05:35,960 --> 00:05:36,960
Yeah.

119
00:05:36,960 --> 00:05:37,960
Exactly.

120
00:05:37,960 --> 00:05:38,960
Exactly.

121
00:05:38,960 --> 00:05:39,960
And you have to do it in that direction.

122
00:05:39,960 --> 00:05:44,560
Because you're going to end up, you know, a service, what are those things, a million

123
00:05:44,560 --> 00:05:45,560
dollars?

124
00:05:45,560 --> 00:05:46,560
I've never seen a service.

125
00:05:46,560 --> 00:05:49,640
No one's ever like, oh, I trained my model on a service.

126
00:05:49,640 --> 00:05:52,480
Most people are like, I trained my model on TPUs.

127
00:05:52,480 --> 00:05:56,080
Some people, 20%, are like, I trained my model on TPUs.

128
00:05:56,080 --> 00:05:57,080
Yeah.

129
00:05:57,080 --> 00:06:01,480
And then the third one, which is the one that surprised me the most is, through incompleteness,

130
00:06:01,480 --> 00:06:02,480
it's harmful.

131
00:06:02,480 --> 00:06:03,480
It should be avoided.

132
00:06:03,480 --> 00:06:09,240
It made sense once I read it, but maybe tell us a bit or more about how you got there.

133
00:06:09,600 --> 00:06:10,600
Okay.

134
00:06:10,600 --> 00:06:18,800
So, CPUs devote tons of their silicon and power to things like reorder buffers and speculative

135
00:06:18,800 --> 00:06:21,440
execution and branch predictors.

136
00:06:21,440 --> 00:06:25,560
And the reason that you need all these things is because at compile time, you can't understand

137
00:06:25,560 --> 00:06:27,400
how the code's going to run.

138
00:06:27,400 --> 00:06:28,800
This is Rice's theorem.

139
00:06:28,800 --> 00:06:31,120
This is the Halting problem, and it's limit.

140
00:06:31,120 --> 00:06:33,160
And this is not like, oh, the Halting problem is theoretical.

141
00:06:33,160 --> 00:06:34,160
No, no, no, no.

142
00:06:34,160 --> 00:06:35,360
It's actually very real.

143
00:06:35,360 --> 00:06:37,320
Does this branch get taken or not?

144
00:06:37,320 --> 00:06:38,320
It depends on X.

145
00:06:38,320 --> 00:06:39,320
Where does X come from?

146
00:06:39,320 --> 00:06:41,000
I forget it, right?

147
00:06:41,000 --> 00:06:44,400
But no branches depend on X in a neural net.

148
00:06:44,400 --> 00:06:46,120
Every branch is a static loop.

149
00:06:46,120 --> 00:06:50,240
Like if you're doing a matrix multiple, it's a static loop over the inner dimension.

150
00:06:50,240 --> 00:06:51,960
And neural networks are even better.

151
00:06:51,960 --> 00:06:53,960
No loads even depend on X, right?

152
00:06:53,960 --> 00:06:56,840
So with a GPU shader, right, you're like, your load might depend on which texture you're

153
00:06:56,840 --> 00:06:58,320
actually loading into RAM.

154
00:06:58,320 --> 00:07:01,440
But with a neural network, your load is, well, I load that way.

155
00:07:01,440 --> 00:07:02,440
Why?

156
00:07:02,440 --> 00:07:04,760
Well, because I load that way the other million times I ran the same net.

157
00:07:04,760 --> 00:07:09,440
Every single time you run the net, you do the exact same set of loads, stores, and arithmetic.

158
00:07:09,440 --> 00:07:12,320
The only thing that changes is the data.

159
00:07:12,320 --> 00:07:18,680
And this gives you a very powerful ability to optimize that you can't do with CPU-style

160
00:07:18,680 --> 00:07:22,640
things, which have branches, and even GPU-style things, which have loads and stores.

161
00:07:22,640 --> 00:07:24,040
Oh, that makes sense.

162
00:07:24,040 --> 00:07:28,080
Well, GPUs, if you want GPU-style stuff, you have load based on X, you now need a cache

163
00:07:28,080 --> 00:07:33,680
hierarchy, and not an explicit cache hierarchy, an implicit cache hierarchy, with eviction

164
00:07:33,680 --> 00:07:36,800
policies that are hard-coded into the CPU.

165
00:07:36,800 --> 00:07:41,760
You start doing all this stuff, and you're never going to get theoretically good performance.

166
00:07:41,760 --> 00:07:43,720
Again, I don't think there's 100X.

167
00:07:43,720 --> 00:07:46,760
Some startups will talk about 100X, and they'll talk about absolutely ridiculous things like

168
00:07:46,760 --> 00:07:49,080
clockless computing or analog computing.

169
00:07:49,080 --> 00:07:51,920
Okay, analog computing just won't work.

170
00:07:51,920 --> 00:07:58,080
And clockless computing, sure, it might work in theory, but your ETA tools are...

171
00:07:58,080 --> 00:08:02,760
Maybe AIs will be able to design clockless chips, but not humans.

172
00:08:02,880 --> 00:08:06,920
What actually is practical is changing cache hierarchies and removing branch predictors

173
00:08:06,920 --> 00:08:08,440
and removing warp schedulers.

174
00:08:08,440 --> 00:08:11,880
GPUs spend tons of power on warp scheduling, because we have to hide the latency from the

175
00:08:11,880 --> 00:08:12,880
memory.

176
00:08:12,880 --> 00:08:16,040
We'll have to hide the latency if everything's statically scheduled.

177
00:08:16,040 --> 00:08:20,120
What do you think people are still hanging on to during complete?

178
00:08:20,120 --> 00:08:22,640
Well, because it's really easy.

179
00:08:22,640 --> 00:08:24,040
Turning complete is just really easy.

180
00:08:24,040 --> 00:08:30,480
It's really easy to just be so nice if I could do an if statement here and actually branch

181
00:08:30,480 --> 00:08:32,080
the code.

182
00:08:32,080 --> 00:08:37,600
So it requires a lot more thought to do it without turning completeness.

183
00:08:37,600 --> 00:08:40,800
And would this be qualitatively different than TPUs?

184
00:08:40,800 --> 00:08:42,120
So TPUs are a lot closer.

185
00:08:42,120 --> 00:08:43,120
Yeah.

186
00:08:43,120 --> 00:08:46,200
TPUs are a lot closer to what I'm talking about than like CUDA.

187
00:08:46,200 --> 00:08:47,200
Okay.

188
00:08:47,200 --> 00:08:48,200
So what is CUDA?

189
00:08:48,200 --> 00:08:53,760
Well, CUDA is a C-like language, which compiles to an LLVM like IR, which compiles to PTX,

190
00:08:53,760 --> 00:08:57,240
which compiles to SAS, which are all turned complete.

191
00:08:57,240 --> 00:08:58,920
TPUs are much more like this, yeah.

192
00:08:58,920 --> 00:09:01,280
Their memory is pretty statically managed.

193
00:09:01,640 --> 00:09:04,240
I did some reverse engineering on the TPU.

194
00:09:04,240 --> 00:09:06,160
It's published in TinyGrad.

195
00:09:06,160 --> 00:09:09,680
It has like a VLIW instruction and it runs them.

196
00:09:09,680 --> 00:09:10,680
So it's similar.

197
00:09:10,680 --> 00:09:12,360
I think the TPUs have a few problems.

198
00:09:12,360 --> 00:09:15,440
I think systolic arrays are the wrong choice.

199
00:09:15,440 --> 00:09:18,000
Systolic array, I think they have systolic arrays because that was the guy's PhD.

200
00:09:18,000 --> 00:09:19,000
Right.

201
00:09:19,000 --> 00:09:20,000
And of course, Amazon makes...

202
00:09:20,000 --> 00:09:21,000
Jake, could you summarize systolic arrays for this?

203
00:09:21,000 --> 00:09:22,000
Systolic arrays are just...

204
00:09:22,000 --> 00:09:23,000
Okay.

205
00:09:23,000 --> 00:09:27,480
So basically you have like, this is a way to do matrix multiplication, think of a grid

206
00:09:27,480 --> 00:09:31,640
of malax and then the grid can multiply and then shift, multiply, then shift, multiply,

207
00:09:31,640 --> 00:09:32,640
then shift.

208
00:09:32,640 --> 00:09:37,640
And they are very power efficient, but it becomes hard to schedule a lot of stuff on

209
00:09:37,640 --> 00:09:43,000
them if you're not doing like perfectly sized dense matrix multiplies, which you can argue,

210
00:09:43,000 --> 00:09:48,400
well, design your models to use perfectly sized dense matrix multiplies, sure, but it's

211
00:09:48,400 --> 00:09:49,400
just...

212
00:09:51,400 --> 00:09:54,920
No, but thanks for indulging on these explanations.

213
00:09:54,920 --> 00:09:59,880
I think we need to keep our audience along with us by pausing every non-dent to explain

214
00:09:59,880 --> 00:10:00,880
key terms.

215
00:10:00,880 --> 00:10:05,800
You know, when I say explain a systolic array, I just immediately get a picture in my head

216
00:10:05,800 --> 00:10:07,760
of like tilting a matrix and shifting it.

217
00:10:07,760 --> 00:10:09,160
It's hard to kind of explain.

218
00:10:09,160 --> 00:10:10,160
Yeah.

219
00:10:10,160 --> 00:10:13,600
We'll do some videos so you have your hand actions and we edit it in visuals.

220
00:10:13,600 --> 00:10:14,600
Yeah.

221
00:10:14,600 --> 00:10:17,560
There's some great graphics that just show you, oh, so that's what a systolic array is,

222
00:10:17,560 --> 00:10:22,480
but it's a malax shift machine that looks kind of different from the typical like APU

223
00:10:22,480 --> 00:10:23,480
sort of machine.

224
00:10:23,480 --> 00:10:28,040
Sorry, ALU sort of machine, I think the right answer is something that looks more like queues

225
00:10:28,040 --> 00:10:32,760
that feed into ALUs and then you can like prefetch the loads from the memory, put it

226
00:10:32,760 --> 00:10:39,440
a bunch of queues and then the queue is just like and feeds into another queue over here.

227
00:10:39,440 --> 00:10:42,360
But yeah, but that's not even the main problem with TPUs.

228
00:10:42,360 --> 00:10:45,440
The main problem with TPUs is that they're closed source, not only is the chip closed

229
00:10:45,440 --> 00:10:51,560
source, but all of XLA is open source, but the XLA to TPU compiler is a 32 megabyte binary

230
00:10:51,560 --> 00:10:54,400
blob called libTPU on Google's cloud instances.

231
00:10:54,400 --> 00:10:58,520
It's all closed source, it's all hidden stuff and you know, well, there's a reason Google

232
00:10:58,520 --> 00:11:02,200
made a closed source, Amazon made a clone of the TPU, it's called Inferencia, or they

233
00:11:02,200 --> 00:11:05,480
have some other name for it, a training, training, yeah, yeah, yeah, yeah, yeah, and you look

234
00:11:05,480 --> 00:11:07,280
as a clone of the TPU.

235
00:11:07,280 --> 00:11:12,120
It just software doesn't work though, and the Google software at least kind of works.

236
00:11:12,120 --> 00:11:14,400
So those are kind of like the three quarter thesis.

237
00:11:14,400 --> 00:11:18,720
The first thing you're working on that you've been working on is TinyGrad, and one of your

238
00:11:18,720 --> 00:11:22,040
Twitch streams, he said, is the best thing you've ever written.

239
00:11:22,040 --> 00:11:26,840
Yeah, tell us a bit more about that creation.

240
00:11:26,840 --> 00:11:30,800
For a long time, TinyGrad had a hard limit of 1,000 lines of code.

241
00:11:30,800 --> 00:11:36,080
And what this would force you to do is really make sure you were not wasting lines.

242
00:11:36,080 --> 00:11:40,600
I got rid of the restriction because it became a little code golfy at the end, but once like

243
00:11:40,600 --> 00:11:47,400
the core framework of TinyGrad was there, in those 1,000 lines, it's not huge now, it's

244
00:11:47,480 --> 00:11:50,680
like 2,800 lines now, it's still very readable.

245
00:11:50,680 --> 00:11:56,440
But like the core framework, the ideas are expressed with no boilerplate.

246
00:11:56,440 --> 00:12:00,000
If you go read PyTorch, you know, PyTorch, I think it's actually pretty good code.

247
00:12:00,000 --> 00:12:05,480
I think Facebook's pretty good, but there's so much boilerplate.

248
00:12:05,480 --> 00:12:10,080
Go in PyTorch and try to track down how an LGU actually works.

249
00:12:10,080 --> 00:12:11,080
Just a lot of distractions.

250
00:12:11,080 --> 00:12:16,840
Oh, you're going to be diving down a long stack from Python to C to custom libraries

251
00:12:16,880 --> 00:12:19,920
to dispatchers to, and then I don't even know how to read TensorFlow.

252
00:12:19,920 --> 00:12:24,480
I don't even know where's an LGU in TensorFlow, nobody knows.

253
00:12:24,480 --> 00:12:26,720
Someone at Google knows maybe.

254
00:12:26,720 --> 00:12:31,640
Google as an organism, I don't know if anyone individually Google knows.

255
00:12:31,640 --> 00:12:35,440
What are like the important ergonomics like for a developer as you think about designing

256
00:12:35,440 --> 00:12:36,920
the TinyGrad API?

257
00:12:36,920 --> 00:12:40,720
So the TinyGrad front end looks very similar to PyTorch.

258
00:12:40,720 --> 00:12:44,760
There's an even higher level front end you can use for TinyGrad, which is just Onyx.

259
00:12:44,840 --> 00:12:49,000
We have better support for Onyx than Core ML does, and we're going to have, I think

260
00:12:49,000 --> 00:12:50,720
we're going to pass Onyx Runtime soon, too.

261
00:12:50,720 --> 00:12:53,080
I like people think Onyx Runtime, that's a gold standard for Onyx.

262
00:12:53,080 --> 00:12:54,080
No, you can do better.

263
00:12:54,080 --> 00:12:55,080
Pass them in what?

264
00:12:55,080 --> 00:12:56,080
Specifically?

265
00:12:56,080 --> 00:12:57,400
Test, compliance tests.

266
00:12:57,400 --> 00:13:01,880
So Onyx has a big set of compliance tests that you can check out.

267
00:13:01,880 --> 00:13:04,880
And we have the running in TinyGrad, and there's some failures.

268
00:13:04,880 --> 00:13:08,080
We're below Onyx Runtime, but we're beyond Core ML.

269
00:13:08,080 --> 00:13:11,480
So that's where we are on Onyx support now, but we will pass, we will pass Onyx Runtime

270
00:13:11,480 --> 00:13:16,240
soon, because it becomes very easy to add ops, because of how you don't need to do anything

271
00:13:16,240 --> 00:13:17,240
at the lower levels.

272
00:13:17,240 --> 00:13:20,640
You just do it at this very high level, and TinyGrad compiles it to something that's

273
00:13:20,640 --> 00:13:23,160
fast using these minimal ops.

274
00:13:23,160 --> 00:13:28,000
You can write, I mean, most concretely, what TinyGrad can do that PyTorch can't really

275
00:13:28,000 --> 00:13:33,840
do is if you have something like A times B plus C, if you write that in naive PyTorch,

276
00:13:33,840 --> 00:13:38,800
what it's going to do on the GPU is, well, read A, read B in a kernel, and then store

277
00:13:38,800 --> 00:13:45,440
A times B in memory, and then launch another kernel to do A times B plus C, got to do those

278
00:13:45,440 --> 00:13:46,440
loads for memory.

279
00:13:46,440 --> 00:13:49,640
I know I did a whole extra round trip to memory that I just didn't have to do.

280
00:13:49,640 --> 00:13:52,560
You're like, yeah, but you can use the TorchJIT and it corrects this.

281
00:13:52,560 --> 00:13:59,200
Yeah, for that one example, for that one example of MULAC, but oh, now you did three multiplies,

282
00:13:59,200 --> 00:14:02,040
six multiplies, right?

283
00:14:02,040 --> 00:14:04,160
It won't compile arbitrary code.

284
00:14:04,640 --> 00:14:09,720
If you looked into the other approaches like PyTorch Lightning to accelerate PyTorch itself.

285
00:14:09,720 --> 00:14:14,840
Well, PyTorch Lightning, my understanding is it's mostly a framework around PyTorch, right?

286
00:14:14,840 --> 00:14:18,440
PyTorch Lightning is not going to fix this fundamental problem of I multiply six tensors

287
00:14:18,440 --> 00:14:19,440
together.

288
00:14:19,440 --> 00:14:23,480
Why is it going to memory any more than a single read from each and a single write to the output?

289
00:14:23,480 --> 00:14:24,480
Okay.

290
00:14:24,480 --> 00:14:31,040
There are lower level things in PyTorch that are not exactly sure what Dynamo does, but

291
00:14:31,040 --> 00:14:34,120
I know they're generating some Triton stuff, which is going to generate the kernels on

292
00:14:34,120 --> 00:14:35,120
the fly.

293
00:14:35,120 --> 00:14:39,120
But you know, PyTorch Lightning is at a higher level of abstraction.

294
00:14:39,120 --> 00:14:41,600
So TinyGrid's front-end stuff looks like PyTorch.

295
00:14:41,600 --> 00:14:42,600
I made a few tweaks.

296
00:14:42,600 --> 00:14:43,840
There's a few things I don't like about PyTorch.

297
00:14:43,840 --> 00:14:45,440
Why is ReLU a class?

298
00:14:45,440 --> 00:14:46,440
Oh, really?

299
00:14:46,440 --> 00:14:49,000
What was the state?

300
00:14:49,000 --> 00:14:50,440
You make a class and there's a state.

301
00:14:50,440 --> 00:14:54,160
Everything should just be Torch Functional and ReLU, but just dot ReLU on the tensor.

302
00:14:54,160 --> 00:14:59,560
Also there's things in Torch where you have to do tensor dot and not a tensor dot.

303
00:14:59,560 --> 00:15:00,560
Right?

304
00:15:00,600 --> 00:15:01,600
Why?

305
00:15:01,600 --> 00:15:02,600
Why are these things?

306
00:15:02,600 --> 00:15:07,640
It just shows an API that's not perfectly refined, but when you're doing stuff TinyGrid

307
00:15:07,640 --> 00:15:11,560
style where you don't have lines, well, it has to work this way because even the lines

308
00:15:11,560 --> 00:15:17,200
to express the, well, you can't use the where operator unless, and the where operator in

309
00:15:17,200 --> 00:15:18,200
PyTorch.

310
00:15:18,200 --> 00:15:22,400
Why is it a true case, condition, false case?

311
00:15:22,400 --> 00:15:24,400
The worst, that's how Python expresses ifs.

312
00:15:24,400 --> 00:15:25,400
It's disgusting.

313
00:15:25,400 --> 00:15:29,680
Where operators are much nicer, it should be, I can do my like, a less than zero dot

314
00:15:29,680 --> 00:15:33,120
where, a comma one, right?

315
00:15:33,120 --> 00:15:35,480
The very pandas like API.

316
00:15:35,480 --> 00:15:40,200
Yeah, yeah, yeah, yeah, yeah, it's just, it's some, it looks like Torch numpy pandas.

317
00:15:40,200 --> 00:15:41,600
They're all very similar.

318
00:15:41,600 --> 00:15:45,320
I tried to take like the cleanest subset of them and express them, but like I said, you

319
00:15:45,320 --> 00:15:47,400
can also interact with it using Onyx.

320
00:15:47,400 --> 00:15:48,400
Yeah.

321
00:15:48,400 --> 00:15:51,600
But I have a rewrite of stable diffusion, I have a rewrite of llama, I have a rewrite

322
00:15:51,600 --> 00:15:52,600
of whisper.

323
00:15:52,600 --> 00:15:53,600
You can look at them.

324
00:15:53,600 --> 00:15:54,600
They're shorter than the Torch version than I think they're cleaner.

325
00:15:54,600 --> 00:15:55,600
They stream them all.

326
00:15:55,600 --> 00:15:56,600
Yeah.

327
00:15:56,600 --> 00:15:57,600
Very nice.

328
00:15:57,600 --> 00:16:03,000
Laziness is kind of the other important concept that you're leveraging to do, operation fusing.

329
00:16:03,000 --> 00:16:05,840
Yeah, talk a bit more about that.

330
00:16:05,840 --> 00:16:13,880
So yeah, you have, you have basically like a few different like models for compute.

331
00:16:13,880 --> 00:16:14,880
The simplest one's eager.

332
00:16:14,880 --> 00:16:21,120
All right, the simplest one is eager as soon as the interpreter or sees A times B, it actually

333
00:16:21,120 --> 00:16:23,720
dispatches A times B, right?

334
00:16:23,720 --> 00:16:30,320
Then you have graph like TensorFlow, which will put A times B into a graph and then we'll

335
00:16:30,320 --> 00:16:36,280
do absolutely nothing until you actually compile the graph at the end.

336
00:16:36,280 --> 00:16:40,080
I like this third choice, just somewhere in the middle, laziness.

337
00:16:40,080 --> 00:16:42,800
Laziness is you don't know when the ops are going to dispatch and don't worry about that.

338
00:16:42,800 --> 00:16:44,560
You don't have to worry about this as a programmer.

339
00:16:44,560 --> 00:16:46,120
You just write out all your stuff.

340
00:16:46,120 --> 00:16:50,600
And then when you actually type dot numpy, it'll be ready by the time you copy the thing

341
00:16:50,600 --> 00:16:51,600
back to CPU.

342
00:16:51,600 --> 00:16:57,760
Or you can do dot realize and it will actually like force that tensor to be allocated in RAM.

343
00:16:57,760 --> 00:17:01,640
But yeah, a lot of times, right, like, and if you think about it, PyTorch is kind of

344
00:17:01,640 --> 00:17:04,960
lazy in a way, but they didn't extend the paradigm far enough, right?

345
00:17:04,960 --> 00:17:09,600
When I do A times B in PyTorch, it's going to launch a CUDA kernel to do A times B, but

346
00:17:09,600 --> 00:17:12,120
it's not going to wait for that CUDA kernel to complete.

347
00:17:12,120 --> 00:17:13,840
So you're getting the worst possible world.

348
00:17:13,840 --> 00:17:18,120
You're getting the same laziness, but you also can't get fusion because PyTorch doesn't

349
00:17:18,120 --> 00:17:21,560
know that I'm then going to do plus C. There's no way for it to be like, whoa, whoa, whoa,

350
00:17:21,560 --> 00:17:25,200
don't launch that CUDA kernel, whoa, just do this one too, right?

351
00:17:25,200 --> 00:17:30,160
You can kind of like, again, this stuff, PyTorch is working on this.

352
00:17:30,160 --> 00:17:31,720
And you know, it's a little bit harder.

353
00:17:31,720 --> 00:17:34,920
Like in comma, I felt like I was competing against a lot of idiots here.

354
00:17:34,920 --> 00:17:40,960
I'm competing against, you know, smart, smart, very smart people who made, yeah, who've made

355
00:17:40,960 --> 00:17:42,760
some, I think, different trade offs, right?

356
00:17:42,760 --> 00:17:46,440
We've made some different trade offs, whereas if you're trying to build something that is

357
00:17:46,440 --> 00:17:50,840
just straight up good on Nvidia, and we have a lot of people in complexity to throw at

358
00:17:51,120 --> 00:17:52,800
PyTorch made a lot of the right choices.

359
00:17:52,800 --> 00:17:57,160
I'm trying to build something that manages complexity, like you can always make your

360
00:17:57,160 --> 00:17:58,400
software do more.

361
00:17:58,400 --> 00:18:02,160
The magic is when you can make your software do more without adding complexity, right?

362
00:18:02,160 --> 00:18:06,240
Because, you know, complex things eventually collapse under the wind.

363
00:18:06,240 --> 00:18:07,680
So it's kind of that.

364
00:18:07,680 --> 00:18:09,520
How does fusing actually work?

365
00:18:09,520 --> 00:18:12,360
Like TensorFlow actually collapsed under it.

366
00:18:12,360 --> 00:18:15,400
It's kind of what happened, right?

367
00:18:15,400 --> 00:18:17,280
How does fusing actually work?

368
00:18:17,280 --> 00:18:23,600
So yeah, there's this thing called lazy.py, and when you do like a times b, that's, it's

369
00:18:23,600 --> 00:18:27,080
put into a graph, but it's a very local graph.

370
00:18:27,080 --> 00:18:30,520
There's no global graph optimizations, and even this can change, right?

371
00:18:30,520 --> 00:18:34,920
Again, like the programming model for TinyGrad does not preclude eagerness, right?

372
00:18:34,920 --> 00:18:37,240
Laziness is not guaranteed laziness.

373
00:18:37,240 --> 00:18:39,200
It's just going to try its best.

374
00:18:39,200 --> 00:18:41,800
So you put in a times b, and that's a binary app, right?

375
00:18:41,800 --> 00:18:44,560
And then you put in a times b, like that's a node in the graph.

376
00:18:44,560 --> 00:18:46,680
That's a virtual node, because it's not realized yet.

377
00:18:46,680 --> 00:18:47,680
Plus c.

378
00:18:47,680 --> 00:18:50,880
Okay, here's a new node, which takes the c tensor in here and takes the output of a

379
00:18:50,880 --> 00:18:51,880
times b.

380
00:18:51,880 --> 00:18:53,280
It's like, whoa, wait, there's two binary ops.

381
00:18:53,280 --> 00:18:55,280
Okay, we'll just fuse those together, okay?

382
00:18:55,280 --> 00:18:56,280
Here I have a kernel.

383
00:18:56,280 --> 00:18:58,280
This kernel has a, b, and c as inputs.

384
00:18:58,280 --> 00:19:03,760
It does a times b plus c in the local registers, and then outputs that to memory.

385
00:19:03,760 --> 00:19:07,360
And you can graph dot one in TinyGrad.

386
00:19:07,360 --> 00:19:11,880
Another like amazing thing that TinyGrad has that I've not seen in any other framework

387
00:19:11,880 --> 00:19:13,520
is two things.

388
00:19:13,520 --> 00:19:16,080
Graph dot one, graph equals one, which is an environment variable.

389
00:19:16,080 --> 00:19:18,280
It will output a complete graph of all the operations.

390
00:19:18,280 --> 00:19:21,600
People are like, oh, you can use PyTorch, export it to Onyx, and use Netron.

391
00:19:21,600 --> 00:19:24,680
Yeah, you can, but like, what?

392
00:19:24,680 --> 00:19:25,680
That's not what's real.

393
00:19:25,680 --> 00:19:29,840
Right, graph dot one will show you the actual kernels that were dispatched to the GPU.

394
00:19:29,840 --> 00:19:35,440
You can also type debug equals two, which will print those kernels out in your command

395
00:19:35,440 --> 00:19:41,360
line, and it will tell you the exact number of flops and the exact number of memory accesses

396
00:19:41,360 --> 00:19:42,360
in each kernel.

397
00:19:43,000 --> 00:19:47,040
So you can immediately see, wait a second, okay, this kernel used this many flops.

398
00:19:47,040 --> 00:19:48,540
This was the gigaflops.

399
00:19:48,540 --> 00:19:51,240
This is how many bytes it read, and this was the gigabytes per second.

400
00:19:51,240 --> 00:19:56,120
And then you can profile without having to like, okay, I mean in theory in PyTorch, sure,

401
00:19:56,120 --> 00:19:57,800
use the NVIDIA insight profiler.

402
00:19:57,800 --> 00:19:58,800
No one does that.

403
00:19:58,800 --> 00:20:03,120
No one does, of course, because it's so difficult, right, like, like, actually NVIDIA used to

404
00:20:03,120 --> 00:20:06,960
a pre, pre, I think kuda nine was the last one that had it.

405
00:20:06,960 --> 00:20:11,160
They had a command line one, but now it's like, okay, I'm going to generate this blob,

406
00:20:11,160 --> 00:20:15,480
use this NVIDIA GUI tool to convert it into a Chrome trace, and then load it, and yeah,

407
00:20:15,480 --> 00:20:16,480
no one does that, right?

408
00:20:16,480 --> 00:20:20,320
I'll just type debug equals two in any tiny grad model, and it will show you all the kernels

409
00:20:20,320 --> 00:20:23,680
that it launches, and the efficiency of each kernel, basically.

410
00:20:23,680 --> 00:20:29,720
Yeah, this is something that John Karmic has often commented about, is that when you code,

411
00:20:29,720 --> 00:20:34,000
you need to build in your instrumentation or observability right into that.

412
00:20:34,000 --> 00:20:38,360
I wonder if whatever John is working on, he's adopting this style, and maybe you can sort

413
00:20:38,360 --> 00:20:45,760
of encourage it by, like, I don't know, naming it and coining it as a certain kind of debugging

414
00:20:45,760 --> 00:20:46,760
style.

415
00:20:46,760 --> 00:20:51,840
If he would like to start contributing to tiny grad, I'd be, I don't know, I've chatted

416
00:20:51,840 --> 00:20:52,840
with a few times.

417
00:20:52,840 --> 00:20:53,840
I'm not really sure what his company's doing.

418
00:20:53,840 --> 00:20:54,840
Yeah.

419
00:20:54,840 --> 00:20:59,880
I think it's all, I think it's pretty, but no, I mean, hopefully, like, we get tiny grad

420
00:20:59,880 --> 00:21:04,280
to a point where people actually want to start using it.

421
00:21:04,280 --> 00:21:09,120
So tiny grad right now is uncompetitive on, it's uncompetitive on NVIDIA, and it's uncompetitive

422
00:21:09,120 --> 00:21:10,120
on x86.

423
00:21:10,120 --> 00:21:12,640
And specifically, what do you care about when you say uncompetitive?

424
00:21:12,640 --> 00:21:13,640
Speed.

425
00:21:13,640 --> 00:21:14,640
Okay.

426
00:21:14,640 --> 00:21:15,640
Shut up, speed.

427
00:21:15,640 --> 00:21:16,640
It's correct.

428
00:21:16,640 --> 00:21:17,640
The correctness is there.

429
00:21:17,640 --> 00:21:21,080
The correctness for both forwards and backwards passes is there, but on NVIDIA, it's about

430
00:21:21,080 --> 00:21:24,600
5x slower than PyTorch right now, like 5x, wow, this is, this is unsurmountable.

431
00:21:24,600 --> 00:21:27,840
No, there's reasons it's 5x slower, and I can go through how we're going to make it

432
00:21:27,840 --> 00:21:30,840
faster, and it used to be, you know, 100x slower, so, you know, we're making progress,

433
00:21:30,840 --> 00:21:35,880
but there's one place where it actually is competitive, and that's Qualcomm GPUs.

434
00:21:35,880 --> 00:21:40,320
So tiny grad is used to run the model in OpenPilot, like right now, it's been live in production

435
00:21:40,320 --> 00:21:47,400
now for six months, and tiny grad is about 2x faster on the GPU than Qualcomm's library.

436
00:21:47,400 --> 00:21:49,120
Why specifically Qualcomm?

437
00:21:49,120 --> 00:21:53,400
Well, because we have Qualcomm, we use Qualcomm in the Comma devices.

438
00:21:53,400 --> 00:21:56,920
Oh, I mean, like, what makes, what makes, what about Qualcomm architecture?

439
00:21:56,920 --> 00:21:58,200
Oh, what makes it doable?

440
00:21:58,200 --> 00:21:59,200
Yeah.

441
00:21:59,200 --> 00:22:02,720
Well, Qualcomm has spent how many millions of man-hours to make NVIDIA fast, and Qualcomm

442
00:22:02,720 --> 00:22:06,360
has a team of 10 Qualcomm engineers, okay, well, who can I be here?

443
00:22:06,360 --> 00:22:11,240
Like, what I propose with, what I propose with tiny grad is that developer efficiency

444
00:22:11,240 --> 00:22:17,680
is much higher, but even if I have 10x higher developer efficiency, I still lose on NVIDIA,

445
00:22:17,680 --> 00:22:18,680
right?

446
00:22:18,680 --> 00:22:20,560
You know, okay, I didn't put 100,000 man-hours into it, right?

447
00:22:20,560 --> 00:22:24,320
If they put a million, like, like, that's what I'm saying, but that's what I'm saying,

448
00:22:24,320 --> 00:22:25,600
we can get.

449
00:22:25,840 --> 00:22:30,440
We are going to close the speed gap a lot, like, I don't support TensorFlow yet.

450
00:22:30,440 --> 00:22:34,000
That's a big one that's just going to, okay, massively close the gap.

451
00:22:34,000 --> 00:22:39,320
And then AMD, I can't even get, I don't even have a benchmark for AMD because I couldn't

452
00:22:39,320 --> 00:22:40,320
get it compiled.

453
00:22:40,320 --> 00:22:41,320
Oh, and I tried.

454
00:22:41,320 --> 00:22:42,320
Oh, I tried.

455
00:22:42,320 --> 00:22:46,800
I spent a day, like, I spent actually a day trying to get PyTorch, and I got it built,

456
00:22:46,800 --> 00:22:51,200
I got it kind of working, and then I tried to run a model, like, there's all kinds of

457
00:22:51,200 --> 00:22:55,560
weird errors, and the rabbit hole is just so deep on this, I'm like, so we, you know,

458
00:22:55,760 --> 00:22:58,680
you can compare the speed, right now, you can run Lama, you can run anything you want

459
00:22:58,680 --> 00:23:03,200
on AMD, it already all works, any OpenCL back-end works, and it's not terribly slow.

460
00:23:03,200 --> 00:23:07,200
I mean, it's a lot faster than crashing, so it's infinitely times faster than PyTorch

461
00:23:07,200 --> 00:23:08,200
on AMD.

462
00:23:08,200 --> 00:23:13,080
But pretty soon, we're going to start getting close to theoretical maximums on AMD.

463
00:23:13,080 --> 00:23:17,720
That's really where I'm pushing, and I want to get AMD on ML perf in a couple months,

464
00:23:17,720 --> 00:23:18,720
hopefully.

465
00:23:18,720 --> 00:23:19,720
Not that you bring up AMD.

466
00:23:19,720 --> 00:23:24,280
Yeah, let's dive into that, because when you announced the TamiCorp fundraise, you mentioned

467
00:23:24,320 --> 00:23:30,400
one of your first goals is build the framework Rhinetime and Driver for AMD, and then on

468
00:23:30,400 --> 00:23:34,560
June 3rd on Twitch, you weren't as excited about AMD anymore.

469
00:23:34,560 --> 00:23:40,680
Maybe let's talk a bit about that, and you compared the quality of commit messages from

470
00:23:40,680 --> 00:23:44,800
the AMD kernel to the Intel work that people are doing there, what's important to know.

471
00:23:44,800 --> 00:23:49,000
So when I said I want to write a framework, I didn't never intend on writing a kernel

472
00:23:49,000 --> 00:23:50,000
driver.

473
00:23:50,120 --> 00:23:56,400
I flirted with that idea briefly, but realistically, there's three parts to it, right?

474
00:23:56,400 --> 00:24:00,840
There's the ML framework, there's the driver, and then there's the user space runtime.

475
00:24:00,840 --> 00:24:02,920
I was even down to rewrite the user space runtime.

476
00:24:02,920 --> 00:24:07,280
I have a GitHub repo called CUDA IO Control Sniffer, it's terribly called, but you can

477
00:24:07,280 --> 00:24:11,520
actually launch a CUDA kernel without CUDA, so you don't need CUDA installed.

478
00:24:11,520 --> 00:24:16,520
Just the NVIDIA open source driver and this open source repo can launch a CUDA kernel.

479
00:24:16,520 --> 00:24:19,520
So rewriting the user space runtime is doable.

480
00:24:19,520 --> 00:24:20,520
Rewriting the kernel driver?

481
00:24:20,520 --> 00:24:24,080
You don't even have docs, I don't have any docs for the GPU, it would just be a massive

482
00:24:24,080 --> 00:24:26,600
reverse engineering project.

483
00:24:26,600 --> 00:24:31,760
So that is, when I saw that there, it wasn't, I wasn't complaining about it being slow,

484
00:24:31,760 --> 00:24:35,160
I wasn't complaining about PyTorch not compiling, I was complaining about the thing crashing

485
00:24:35,160 --> 00:24:38,680
my entire computer, it panics my kernel, and I have to wait five minutes while it reboots

486
00:24:38,680 --> 00:24:41,800
because it's a server motherboard and they take five minutes to reboot.

487
00:24:41,800 --> 00:24:45,720
So I was like, look, if you guys do not care enough to get me a decent kernel driver, there's

488
00:24:45,720 --> 00:24:49,280
no way I'm wasting my time on this, especially when I can use Intel GPUs.

489
00:24:49,280 --> 00:24:53,600
Intel GPUs have a stable kernel driver, and they have all their hardware documented.

490
00:24:53,600 --> 00:24:58,280
You can go and you can find all the registered docs on Intel GPUs, so I'm like, why don't

491
00:24:58,280 --> 00:24:59,280
I just use these?

492
00:24:59,280 --> 00:25:01,280
Now, there's a downside to them.

493
00:25:01,280 --> 00:25:02,280
Their GPU is $350.

494
00:25:02,280 --> 00:25:06,780
You're like, what a deal, it's $350, you got about $350 for the performance, and if you're

495
00:25:06,780 --> 00:25:10,920
paying about 400 for the PCIe slot to put it in, right, like between the power and all

496
00:25:10,920 --> 00:25:16,400
the other stuff, you're like, okay, never mind, you got to use NVIDIA or AMD from that perspective.

497
00:25:16,400 --> 00:25:19,760
But I sent an email to Lisa Sue, and she responded.

498
00:25:19,760 --> 00:25:22,560
Oh, you can see you published that email in a Discord.

499
00:25:22,560 --> 00:25:26,040
I did, I did, and she responded.

500
00:25:26,040 --> 00:25:33,720
And I've had a few calls since, and what I did was like, what I tried to do, well, first

501
00:25:33,720 --> 00:25:34,720
off, thank you for responding.

502
00:25:34,720 --> 00:25:40,080
It shows me that if you don't care about your kernel panicking, this is just a huge waste

503
00:25:40,080 --> 00:25:41,080
of my time, right?

504
00:25:41,080 --> 00:25:42,920
I'll find someone who will care.

505
00:25:42,960 --> 00:25:48,560
I'm not asking for your seven by seven Winograd convolution when transposed to be fast.

506
00:25:48,560 --> 00:25:49,560
Like I'm not asking for that.

507
00:25:49,560 --> 00:25:51,440
I'm asking literally for the basics.

508
00:25:51,440 --> 00:25:52,440
To not value.

509
00:25:52,440 --> 00:25:53,520
Oh, and this isn't tiny grad.

510
00:25:53,520 --> 00:25:54,520
This is your demo apps.

511
00:25:54,520 --> 00:25:57,960
I ran their demo apps in loops, and I got kernel panics.

512
00:25:57,960 --> 00:26:06,000
I'm like, okay, there's, but no, Lisa Sue reached out, connected with a whole bunch

513
00:26:06,000 --> 00:26:07,000
of different people.

514
00:26:07,000 --> 00:26:12,000
They sent me a pre-release version of RockM 5.6.

515
00:26:12,000 --> 00:26:16,040
They told me you can't really say which I'm like, why do you, why do you care?

516
00:26:16,040 --> 00:26:18,960
But they say they're going to release it by the end of the month and it fixed the kernel

517
00:26:18,960 --> 00:26:20,080
panic.

518
00:26:20,080 --> 00:26:26,400
The guy managed to reproduce it with the two GPUs and the computer and yeah, sent me a

519
00:26:26,400 --> 00:26:27,920
driver and it works.

520
00:26:27,920 --> 00:26:31,840
So, yeah, I had, I had that experience.

521
00:26:31,840 --> 00:26:34,840
And then I had another experience where I had two calls with like AMD's like communication

522
00:26:34,840 --> 00:26:38,960
people and just like, I tried to explain to these people like open source culture, like

523
00:26:39,160 --> 00:26:40,520
it's not open source.

524
00:26:40,520 --> 00:26:44,560
If you dump the source code on a GitHub repo and then forget about it until the next release,

525
00:26:44,560 --> 00:26:45,800
it's not open source.

526
00:26:45,800 --> 00:26:51,880
If, you know, all your issues are from 2022, like, like, it's just no one's going to contribute

527
00:26:51,880 --> 00:26:52,880
to that project.

528
00:26:52,880 --> 00:26:53,880
Right.

529
00:26:53,880 --> 00:26:54,880
Sure.

530
00:26:54,880 --> 00:26:55,880
It's open source in a very like technical sense.

531
00:26:55,880 --> 00:26:56,880
To be fair, it's better than nothing.

532
00:26:56,880 --> 00:27:02,080
It's better than nothing, but I fixed a bug in Nickel that I fixed.

533
00:27:02,080 --> 00:27:05,960
There's a fun fact, by the way, if you have a consumer, a consumer AMD GPU, they don't

534
00:27:05,960 --> 00:27:08,040
support peer-to-peer.

535
00:27:08,040 --> 00:27:12,480
And they're already spanned with this horrendously slow because it's using CUDA kernels to do

536
00:27:12,480 --> 00:27:13,880
the copy between the GPUs.

537
00:27:13,880 --> 00:27:17,640
And it's putting so many transactions on the PCIe bus that it's really slow, but you

538
00:27:17,640 --> 00:27:23,320
can use CUDA mem copy and there's a flag to use CUDA mem copy, but that flag had a bug.

539
00:27:23,320 --> 00:27:27,000
So I posted the issue on Nickel.

540
00:27:27,000 --> 00:27:28,520
I expected nothing to happen.

541
00:27:28,520 --> 00:27:30,200
The Nvidia guy replied to me within an hour.

542
00:27:30,200 --> 00:27:31,200
He's like, try this other flag.

543
00:27:31,200 --> 00:27:32,800
I'm like, okay, I tried the other flag.

544
00:27:32,800 --> 00:27:35,200
It still doesn't work, but here's a clean repro.

545
00:27:35,200 --> 00:27:39,080
And I spent like three hours writing a very clean repro.

546
00:27:39,080 --> 00:27:42,760
I ended up tracking the issue down myself, but just the fact that somebody responded

547
00:27:42,760 --> 00:27:46,640
to me within an hour and cared about fixing the issue, okay, you've shown that it's worth

548
00:27:46,640 --> 00:27:50,560
my time and I will put my time in because let's make this better.

549
00:27:50,560 --> 00:27:51,840
I'm here to help.

550
00:27:51,840 --> 00:27:56,000
But if you show me that you're like, you're the kernel panics, let's just expect it.

551
00:27:56,000 --> 00:27:57,000
Okay.

552
00:27:57,000 --> 00:27:59,040
Well, it sounds like AMD is getting the message.

553
00:27:59,040 --> 00:28:00,040
They are.

554
00:28:00,040 --> 00:28:03,600
And I just, I don't really think they've had someone explain to them like, I was like,

555
00:28:03,600 --> 00:28:04,600
you get to like build in public.

556
00:28:04,600 --> 00:28:06,320
And they're like, what's an example of building in public?

557
00:28:06,320 --> 00:28:09,120
I'm like, go look at PyTorch, go look at PyTorch, right?

558
00:28:09,120 --> 00:28:12,720
Like, you know, I have, I have, I have two minor things merged into PyTorch because it's

559
00:28:12,720 --> 00:28:19,040
very responsive, you know, like minor bug fixes, but I feel like it's, you know, yeah.

560
00:28:19,040 --> 00:28:21,320
So that's kind of like the lowest level of the stack.

561
00:28:21,320 --> 00:28:26,600
And then at a slightly higher level, obviously there's tiny grad, there's module, there's

562
00:28:26,600 --> 00:28:27,600
GGML.

563
00:28:27,600 --> 00:28:32,160
How are you thinking about breadth versus like depth and like where you decided to focus

564
00:28:32,160 --> 00:28:33,160
early on?

565
00:28:33,160 --> 00:28:36,760
Um, so GGML is very much like a, okay, everyone has M1s, right?

566
00:28:36,760 --> 00:28:41,360
Actually, I was thinking, in the beginning I was thinking of something more like GGML,

567
00:28:41,360 --> 00:28:45,360
focus on the M1s, but GGML showed up and was just like, we're actually just focusing

568
00:28:45,360 --> 00:28:46,360
on the M1s.

569
00:28:46,360 --> 00:28:52,160
Um, so, and actually M1 PyTorch is considerably better than AMD PyTorch.

570
00:28:52,160 --> 00:28:56,680
And when PyTorch works, it only gives wrong answers sometimes and only crashes sometimes,

571
00:28:56,680 --> 00:29:02,720
but like some models kind of run, um, when I was writing the metal back end, I was comparing

572
00:29:02,720 --> 00:29:08,160
to MPS PyTorch and I had like a, I had a discrepancy, like TinyGrad checks all its outputs compared

573
00:29:08,160 --> 00:29:14,000
to Torch and I had one where it didn't match, I'm like, I really, I checked the matrix by

574
00:29:14,000 --> 00:29:16,960
hand, it matches TinyGrad, I don't understand.

575
00:29:16,960 --> 00:29:22,600
And then I switched PyTorch back to CPU and it matched and I'm like, oh yeah, well, this

576
00:29:22,600 --> 00:29:25,520
is like bugs, like if you like transpose the matrix because like, I think it's like has

577
00:29:25,520 --> 00:29:29,160
to do with like multi views and PyTorch and like weird under the hood stuff that's not

578
00:29:29,160 --> 00:29:32,320
exposed to you, like there's bugs and maybe they fix them, but like, you know, it seems

579
00:29:32,320 --> 00:29:37,000
like there was a lot of momentum again, because you're getting a huge variety, you're getting

580
00:29:37,000 --> 00:29:41,480
how many engineers care about making PyTorch work on M1, right, thousands, tens of thousands.

581
00:29:41,480 --> 00:29:42,480
Yeah.

582
00:29:42,480 --> 00:29:45,120
And you have an open development process and guess what, it's going to be good.

583
00:29:45,120 --> 00:29:48,240
How many engineers care about AMD working with PyTorch AMD working?

584
00:29:48,240 --> 00:29:54,000
Well, you got 10 guys that work for AMD and then like a couple hobbyists.

585
00:29:54,000 --> 00:29:58,280
You revealed an interesting detail about how you debug, which is you check, you hand check

586
00:29:58,280 --> 00:29:59,280
the matrix math.

587
00:29:59,640 --> 00:30:00,640
No, I don't hand check it.

588
00:30:00,640 --> 00:30:06,640
There's a, there's a, one of the best tests in tiny grad is a file called test ops.py

589
00:30:06,640 --> 00:30:12,680
and it's just a hundred small examples written in tiny grad and PyTorch and it checks both

590
00:30:12,680 --> 00:30:14,920
the forwards and backwards to make sure they match.

591
00:30:14,920 --> 00:30:15,920
The test suite.

592
00:30:15,920 --> 00:30:16,920
Yeah.

593
00:30:16,920 --> 00:30:17,920
Very important.

594
00:30:17,920 --> 00:30:20,080
That's, I mean, that's one of them where you like, I really, I put a lot of effort into

595
00:30:20,080 --> 00:30:21,080
the CI for tiny grad.

596
00:30:21,080 --> 00:30:22,560
I think CI is super important.

597
00:30:22,560 --> 00:30:25,320
Like I want that green check to mean I can merge this.

598
00:30:25,320 --> 00:30:26,320
Yeah.

599
00:30:26,320 --> 00:30:27,320
Okay.

600
00:30:27,320 --> 00:30:28,320
I don't want my tests too.

601
00:30:28,360 --> 00:30:30,160
I don't want to manage to introduce a bug and get the green check.

602
00:30:30,160 --> 00:30:31,160
Okay.

603
00:30:31,160 --> 00:30:32,160
We're fixing the test top priority.

604
00:30:32,160 --> 00:30:33,160
Mojo.

605
00:30:33,160 --> 00:30:34,160
It's close source.

606
00:30:34,160 --> 00:30:35,160
No, I'm not that interested.

607
00:30:35,160 --> 00:30:36,160
Do you know what I mean?

608
00:30:36,160 --> 00:30:37,160
Like, like, look, I like Chris Latner.

609
00:30:37,160 --> 00:30:38,160
I think he's going to do great things.

610
00:30:38,160 --> 00:30:42,160
And I understand the, the like kind of the wisdom, even in keeping a close source, but

611
00:30:42,160 --> 00:30:44,160
you know, I'm interested when it's open.

612
00:30:44,160 --> 00:30:45,160
Yeah.

613
00:30:45,160 --> 00:30:46,160
Right.

614
00:30:46,160 --> 00:30:51,160
You have an interesting design deviation from him because he's decided to be a promise

615
00:30:51,160 --> 00:30:55,160
to be a superset of Python and you have decided to break with it.

616
00:30:56,000 --> 00:31:01,000
And I think that's, that affects learnability and trans, transportability of code.

617
00:31:01,000 --> 00:31:09,000
You know, if the PyTorch thing ends up being like a, like a stumbling block, I could write

618
00:31:09,000 --> 00:31:15,000
a perfect PyTorch, like, like, like, like a, you know, instead of import PyTorch, instead

619
00:31:15,000 --> 00:31:19,000
of like, yeah, import Torch, you type import tiny Torch as Torch.

620
00:31:19,000 --> 00:31:23,000
And if that really becomes the stumbling block, I think it's going to be great.

621
00:31:23,000 --> 00:31:27,000
If that really becomes the stumbling block, I will do that.

622
00:31:27,000 --> 00:31:28,000
No.

623
00:31:28,000 --> 00:31:30,240
Chris Latner went much further than PyTorch.

624
00:31:30,240 --> 00:31:34,120
Replicating the PyTorch API is something I can do with a couple, you know, like an engineer

625
00:31:34,120 --> 00:31:35,120
month or two.

626
00:31:35,120 --> 00:31:36,120
Right.

627
00:31:36,120 --> 00:31:37,120
Like a shim.

628
00:31:37,120 --> 00:31:38,120
Yeah.

629
00:31:38,120 --> 00:31:39,120
Replicating Python.

630
00:31:39,120 --> 00:31:41,640
There's a, there's a, there's a big graveyard of those projects.

631
00:31:41,640 --> 00:31:44,440
How's, how's, how's Piston going?

632
00:31:44,440 --> 00:31:51,940
How's, oh, Jython, PyPy is all, you can go way back.

633
00:31:51,940 --> 00:31:57,340
So tiny grad and small layer, you announced TinyBox recently, which is, you know, you

634
00:31:57,340 --> 00:31:58,340
made it.

635
00:31:58,340 --> 00:32:01,660
So your core mission is commoditizing the pedoflop.

636
00:32:01,660 --> 00:32:05,860
And then your business goal is to sell computers for more than the cost to make, which seems

637
00:32:05,860 --> 00:32:08,060
super reasonable.

638
00:32:08,060 --> 00:32:12,220
What are, and you're going to have three TinyBoxes, red, green, blue.

639
00:32:12,220 --> 00:32:16,860
That was my, look, you know, a lot of people, like, I love, you know, leaning into like

640
00:32:16,860 --> 00:32:18,060
saying I'm giving up, right?

641
00:32:18,060 --> 00:32:20,500
It's great to give up or giving up is this wonderful thing.

642
00:32:20,500 --> 00:32:21,820
It's so liberating.

643
00:32:21,820 --> 00:32:24,180
And then like, you can decide afterward if you really give up or not.

644
00:32:24,180 --> 00:32:27,020
There's very little harm in saying you give up, except like, you know, great, Twitter

645
00:32:27,020 --> 00:32:30,780
haters have something to talk about and all press is good press kids.

646
00:32:30,780 --> 00:32:39,060
So, obviously, just read, only read, TinyBox, read, unless AMD, you know, upsets me again

647
00:32:39,060 --> 00:32:44,100
and then we're back to, we're back to other colors, we have other colors to choose from.

648
00:32:44,100 --> 00:32:47,820
When you think about hardware design, what are some of the numbers you look for?

649
00:32:47,820 --> 00:32:52,980
So, Terraprop sits per second, it's one, but like memory bandwidth is another big limiter.

650
00:32:52,980 --> 00:32:54,900
Like, how do you make those trade-offs?

651
00:32:54,900 --> 00:32:58,340
Well, I mean, fundamentally unlimited what GPUs I can buy.

652
00:32:58,340 --> 00:33:01,660
But yeah, for something that I think a lot of people are going to want to reasonably

653
00:33:01,660 --> 00:33:09,060
do with a core core of mine, describe them as luxury AI computers, right?

654
00:33:09,060 --> 00:33:11,180
Like luxury AI computers for people.

655
00:33:11,180 --> 00:33:12,180
And that's like what we're building.

656
00:33:12,180 --> 00:33:16,060
And I think a common thing people are going to want to do is run like large llama, right?

657
00:33:16,060 --> 00:33:17,060
Or large, like Falcon.

658
00:33:17,300 --> 00:33:18,300
FB16 llama.

659
00:33:18,300 --> 00:33:19,300
FB16, exactly.

660
00:33:19,300 --> 00:33:20,300
Exactly.

661
00:33:20,300 --> 00:33:22,220
You know, in-date I think can work.

662
00:33:22,220 --> 00:33:26,660
I think that like what GGML is doing to go to like N4, like this doesn't work.

663
00:33:26,660 --> 00:33:31,220
Like have you done, maybe they have, but like I read what it was and I was like, this isn't

664
00:33:31,220 --> 00:33:32,380
from any paper.

665
00:33:32,380 --> 00:33:34,140
This is just some, like you're-

666
00:33:34,140 --> 00:33:35,140
Squeezing as much as possible.

667
00:33:35,140 --> 00:33:38,900
Yeah, you made up some quantization standard to make it run fast and like, like maybe it

668
00:33:38,900 --> 00:33:41,940
works, but okay, where's like the hell's swag number, right?

669
00:33:41,940 --> 00:33:45,140
Where's your, where's your, where's your, uh, you know, all your-

670
00:33:45,140 --> 00:33:48,820
The thesis is right that like if you have billions, hundreds of billions of parameters

671
00:33:48,820 --> 00:33:51,900
that the individual quantization doesn't actually matter that much.

672
00:33:51,900 --> 00:33:55,340
Well, the real way to look at all of that is to just say you want to compress the weights,

673
00:33:55,340 --> 00:33:56,340
right?

674
00:33:56,340 --> 00:33:57,340
It's a form of weight compression.

675
00:33:57,340 --> 00:33:59,140
Quantization is a form of weight compression right now.

676
00:33:59,140 --> 00:34:00,140
This is obviously not lossless.

677
00:34:00,140 --> 00:34:01,140
It's not a lossless compressor, right?

678
00:34:01,140 --> 00:34:04,380
It's a lossless compressor and you can show that it's correct and okay, we don't have

679
00:34:04,380 --> 00:34:07,460
to have any other conversation, but it's a lossy compressor.

680
00:34:07,460 --> 00:34:11,740
And how do you know that your loss isn't actually losing the power of the model?

681
00:34:11,740 --> 00:34:17,660
Maybe int465bLama is actually the same as fb167bLama, right?

682
00:34:17,660 --> 00:34:18,660
We don't know.

683
00:34:18,660 --> 00:34:21,500
Uh, maybe someone has done this yet, but I looked for it when it like first came out

684
00:34:21,500 --> 00:34:25,980
and people were talking about it and I'm like, I just have, like it's not from a paper, right?

685
00:34:25,980 --> 00:34:29,780
The in-date stuff is from a paper where they, like some of the in-date stuff is from a paper.

686
00:34:29,780 --> 00:34:36,020
There's one paper, I think it's like in-llm.indate where they actually, uh, you know, do all the

687
00:34:36,020 --> 00:34:38,900
tests and they didn't go fully in-date.

688
00:34:38,900 --> 00:34:43,300
They made like 90% of it in-date and kept like 10% of it in fb16 for what they called

689
00:34:43,300 --> 00:34:46,220
like the like outliers or whatever.

690
00:34:46,220 --> 00:34:48,020
So I think that this is not quite so easy.

691
00:34:48,020 --> 00:34:51,140
And I think being able, well, so first off, if you're training, no one's gotten training

692
00:34:51,140 --> 00:34:52,140
to work with in-date yet.

693
00:34:52,140 --> 00:34:53,700
There's a few papers that vaguely show up.

694
00:34:53,700 --> 00:34:57,260
If you're training, you're going to need, uh, bf16 or float16.

695
00:34:57,260 --> 00:35:00,660
Um, so this is why I target that.

696
00:35:00,660 --> 00:35:04,060
Now the thing that you're going to want to do is run these large language models out

697
00:35:04,060 --> 00:35:07,580
of the box on your hardware in fb16 and that's memory bandwidth.

698
00:35:08,580 --> 00:35:13,020
You, you need, you need large amounts of memory bandwidth to, uh, so ask how I trade

699
00:35:13,020 --> 00:35:15,540
off memory bandwidth in Flops, so what GPUs can I buy?

700
00:35:15,540 --> 00:35:21,660
But, um, and I saw one of your, so first of all, you have this, um, hiring process, which

701
00:35:21,660 --> 00:35:25,140
has, you got to solve one of the bounties, um, that are open on tiny grad.

702
00:35:25,140 --> 00:35:27,340
There's no, uh, technical interview.

703
00:35:27,340 --> 00:35:29,020
One of them is in-date support.

704
00:35:29,020 --> 00:35:32,540
Do you already have some things you want to test on?

705
00:35:32,540 --> 00:35:33,540
We have in-date support.

706
00:35:33,540 --> 00:35:39,620
Um, what I'd like to see somebody do is just load the ggml intate llama into tiny grad

707
00:35:39,620 --> 00:35:42,060
and then benchmark it against the fb16 one.

708
00:35:42,060 --> 00:35:46,460
Uh, intate already works in, in tiny grad, it doesn't actually do the math in intate,

709
00:35:46,460 --> 00:35:51,580
which is even a, which is even a stronger, like it does all the math still in fb32.

710
00:35:51,580 --> 00:35:55,180
So intate can mean you just have your weights in intate or intate can mean you actually

711
00:35:55,180 --> 00:35:56,180
do your math in intate.

712
00:35:56,180 --> 00:36:01,180
And doing your math in intate, the big, like, gain that people care about is actually, uh,

713
00:36:01,940 --> 00:36:07,420
having your weights in intate, because weights in intate mean less memory and less memory bandwidth.

714
00:36:07,420 --> 00:36:12,340
Uh, whereas the math, keep it in fb32 with, with, with, on, on m ones, it doesn't even

715
00:36:12,340 --> 00:36:15,700
matter if you're doing, it doesn't matter what data type you're doing in the, in the

716
00:36:15,700 --> 00:36:16,700
, in the GPO.

717
00:36:16,700 --> 00:36:20,740
I, I'm not even sure it can do intate, but fb16 and fb32 is the same, is the same taro

718
00:36:20,740 --> 00:36:21,740
flops.

719
00:36:21,740 --> 00:36:25,620
Um, so yeah, no, that's one of the bounties.

720
00:36:25,620 --> 00:36:30,020
One of the bounties is get, get intate llama running with the intate weights.

721
00:36:30,020 --> 00:36:34,540
And then actually what you could even do, if you really want to test this, just take

722
00:36:34,540 --> 00:36:38,500
the fb16 weights, convert them to intate, then convert them back to fb16, then compare

723
00:36:38,500 --> 00:36:40,460
the unconverted and converted.

724
00:36:40,460 --> 00:36:41,780
Oh, that's a nice hack.

725
00:36:41,780 --> 00:36:42,780
Oh yeah.

726
00:36:42,780 --> 00:36:43,780
Right.

727
00:36:43,780 --> 00:36:45,540
Like, like, like, this should be lossless in the other direction.

728
00:36:45,540 --> 00:36:46,540
Well, yeah.

729
00:36:46,540 --> 00:36:51,460
So, uh, yeah, I think fb16, it should be lossless in the other direction.

730
00:36:51,460 --> 00:36:53,140
I'm actually not a hundred percent about that.

731
00:36:53,140 --> 00:36:54,140
Why not?

732
00:36:54,140 --> 00:36:57,580
Uh, oh, cause like, you ever try to like, like, if you want to represent, if it was like

733
00:36:57,580 --> 00:36:59,980
int16, it's not lossless.

734
00:36:59,980 --> 00:37:00,980
Sure.

735
00:37:00,980 --> 00:37:04,020
I think, I think all of intate can be represented in fb16, but I'm not a hundred percent about

736
00:37:04,020 --> 00:37:05,020
that.

737
00:37:05,020 --> 00:37:10,340
Actually, I think it, we just draw the bytes and we just have to do it, right?

738
00:37:10,340 --> 00:37:11,340
Just literally do it.

739
00:37:11,340 --> 00:37:17,020
There's only 256 to check, like, um, but yeah, either way, or, I mean, into four, definitely.

740
00:37:17,020 --> 00:37:18,940
So do your in four, convert it back.

741
00:37:18,940 --> 00:37:24,140
And now see, even with in four weights and fb32 math, like, okay, how much does your

742
00:37:24,140 --> 00:37:25,900
performance to grade of this model?

743
00:37:25,900 --> 00:37:26,900
Yeah.

744
00:37:26,940 --> 00:37:31,580
So, so can we, uh, I'm about to zoom out a little bit from the details.

745
00:37:31,580 --> 00:37:35,980
I don't know if you, you had more, no, I think like the, you're playing to release

746
00:37:35,980 --> 00:37:40,460
the first tiny box ship them in like two to six, eight months, something like that.

747
00:37:40,460 --> 00:37:45,220
Uh, what's up with mine for you in terms of building a team who should, who are you calling

748
00:37:45,220 --> 00:37:46,220
for?

749
00:37:46,220 --> 00:37:47,220
Yeah.

750
00:37:47,220 --> 00:37:50,020
Uh, well, to, to stay on the tiny box for, for, for, for, yeah, exactly.

751
00:37:50,020 --> 00:37:54,860
Um, so at the GPUs picked out and you're like, well, I could make that computer with

752
00:37:54,860 --> 00:37:55,860
the GPUs.

753
00:37:55,900 --> 00:38:00,540
My answer is, can you, do you know how to put, do you know how hard it is to put six

754
00:38:00,540 --> 00:38:02,660
GPUs on a computer?

755
00:38:02,660 --> 00:38:05,780
People think it's really easy and it's really easy to put one GPU in a computer.

756
00:38:05,780 --> 00:38:09,980
It's really easy to put two GPUs in a computer, but now you want to put in eight.

757
00:38:09,980 --> 00:38:10,980
Okay.

758
00:38:10,980 --> 00:38:11,980
So I'll tell you a few things about these GPUs.

759
00:38:11,980 --> 00:38:13,480
They take up four slots.

760
00:38:13,480 --> 00:38:15,580
What kind of computer?

761
00:38:15,580 --> 00:38:16,900
You can buy the nicest super micro.

762
00:38:16,900 --> 00:38:18,780
You can't put eight of those in there.

763
00:38:18,780 --> 00:38:19,780
You need two slot blowers.

764
00:38:19,780 --> 00:38:23,060
If you want to use one of those, those for your super micros, you need two slot blowers

765
00:38:23,060 --> 00:38:24,060
or water cooling.

766
00:38:24,260 --> 00:38:26,340
All right, if, if you're trying to get the four slot cards in there, you're going to

767
00:38:26,340 --> 00:38:30,980
need some form of water cooling, uh, or you're going to need, there are some like Chinese

768
00:38:30,980 --> 00:38:32,180
40 nineties that are blowers, right?

769
00:38:32,180 --> 00:38:34,380
You have any blowers or water cooling if you're trying to get it in those things.

770
00:38:34,380 --> 00:38:35,380
Right.

771
00:38:35,380 --> 00:38:37,220
Um, so you, are you doing water?

772
00:38:37,220 --> 00:38:39,300
No, I'm not using that chassis.

773
00:38:39,300 --> 00:38:40,300
Okay.

774
00:38:40,300 --> 00:38:45,700
Um, then the other thing that, okay, so now you want to get six GPUs in a computer.

775
00:38:45,700 --> 00:38:46,700
So that's a big challenge.

776
00:38:46,700 --> 00:38:48,540
You're like, oh, I'll just use a PCIe extenders.

777
00:38:48,540 --> 00:38:49,540
I saw it online as tech tips.

778
00:38:49,540 --> 00:38:50,540
It works great.

779
00:38:50,540 --> 00:38:51,540
No, it doesn't.

780
00:38:51,540 --> 00:38:55,180
It's PCIe extenders that work at PCIe 4.0 and interconnect bandwidth.

781
00:38:55,180 --> 00:38:56,180
Super important.

782
00:38:56,180 --> 00:38:57,180
Yes.

783
00:38:57,180 --> 00:38:58,580
They don't work at 3.0.

784
00:38:58,580 --> 00:39:04,420
No PCIe extender I've tested and I've bought 20 of them, uh, works at PCIe 4.0.

785
00:39:04,420 --> 00:39:07,020
So you're going to need PCIe redrivers now.

786
00:39:07,020 --> 00:39:08,020
Okay.

787
00:39:08,020 --> 00:39:09,380
How much does that add in cost?

788
00:39:09,380 --> 00:39:10,380
Right.

789
00:39:10,380 --> 00:39:11,380
Like these things all get really hard.

790
00:39:11,380 --> 00:39:12,380
And then tiny boxes.

791
00:39:12,380 --> 00:39:14,140
I've even had another constraint to it.

792
00:39:14,140 --> 00:39:19,420
I want this thing to be silent, not totally silent, but my limit is like 45, maybe 50

793
00:39:19,420 --> 00:39:22,660
dB, but not super micro machine.

794
00:39:22,660 --> 00:39:23,660
60 dB.

795
00:39:23,660 --> 00:39:28,260
We have a small, we have a compute cluster, a comma, you got to wear your protection

796
00:39:28,260 --> 00:39:29,260
to go in there.

797
00:39:29,260 --> 00:39:30,260
I like it.

798
00:39:30,260 --> 00:39:31,260
Yeah.

799
00:39:31,260 --> 00:39:32,260
I've seen some videos where you give a tour.

800
00:39:32,260 --> 00:39:33,260
Yeah.

801
00:39:33,260 --> 00:39:34,260
Yeah.

802
00:39:34,260 --> 00:39:35,260
It's noisy.

803
00:39:35,260 --> 00:39:36,260
It's super loud.

804
00:39:36,260 --> 00:39:37,260
Yeah.

805
00:39:37,260 --> 00:39:38,260
10,000 RPM.

806
00:39:38,260 --> 00:39:39,260
Just screaming.

807
00:39:39,260 --> 00:39:44,540
Like I want to be able to use the normal big GPU fans and make this thing so it can

808
00:39:44,540 --> 00:39:48,940
sit under your desk, plug into one outlet of power, right?

809
00:39:48,940 --> 00:39:54,940
This GPUs, your GPUs are 350 Watts each, can't plug that into a wall outlet.

810
00:39:54,940 --> 00:39:55,940
Okay.

811
00:39:55,940 --> 00:39:56,940
So how are you going to deal with that?

812
00:39:56,940 --> 00:39:59,540
Good questions, right?

813
00:39:59,540 --> 00:40:00,540
And you're not sharing them.

814
00:40:00,540 --> 00:40:02,620
Well, that one, I mean, that one is pretty obvious.

815
00:40:02,620 --> 00:40:04,220
You have to limit the power on the GPUs, right?

816
00:40:04,220 --> 00:40:05,980
You have to limit the power on the GPUs.

817
00:40:05,980 --> 00:40:10,020
Now you can limit power on GPUs and still get, you can use like half the power and get

818
00:40:10,020 --> 00:40:12,020
80% of the performance.

819
00:40:12,020 --> 00:40:15,300
This is a known fact about GPUs, but like that's one of my design constraints.

820
00:40:15,300 --> 00:40:19,500
So when you start to add all these design constraints, good luck building a tiny box

821
00:40:19,500 --> 00:40:20,500
yourself.

822
00:40:20,500 --> 00:40:24,980
You know, obviously it can be done, but you need something that has actually quite a bit

823
00:40:24,980 --> 00:40:27,580
of scaling resources to do it.

824
00:40:27,580 --> 00:40:31,620
And you see like the under the desk, it's like one of the main use cases, kind of like

825
00:40:31,620 --> 00:40:33,620
individual developer use or.

826
00:40:33,620 --> 00:40:34,620
Yeah.

827
00:40:34,620 --> 00:40:37,900
What I also see is more of a like an AI hub for your home, right?

828
00:40:37,900 --> 00:40:42,180
As we start to get like home robotics kind of stuff, you don't want to put the inference

829
00:40:42,180 --> 00:40:43,180
on the robot.

830
00:40:43,740 --> 00:40:46,620
But you also don't want to put the inference on the cloud.

831
00:40:46,620 --> 00:40:51,860
You don't want to put it on the robot because, okay, it's 1500 Watts, tiny box, you put batters

832
00:40:51,860 --> 00:40:54,380
and charge them.

833
00:40:54,380 --> 00:40:55,380
Bad idea.

834
00:40:55,380 --> 00:40:57,900
And just, just, just wireless, wireless is 0.5 milliseconds.

835
00:40:57,900 --> 00:40:58,900
Yeah.

836
00:40:58,900 --> 00:40:59,900
This is super fast.

837
00:40:59,900 --> 00:41:01,740
You don't want to go to the cloud for two reasons.

838
00:41:01,740 --> 00:41:03,260
One, cloud's far away.

839
00:41:03,260 --> 00:41:04,260
Okay.

840
00:41:04,260 --> 00:41:05,260
It's not that far away.

841
00:41:05,260 --> 00:41:11,500
You can kind of address this, but two, cloud's also mad expensive, like cloud GPUs are way

842
00:41:11,500 --> 00:41:14,260
more expensive than running that GPU at your house.

843
00:41:14,260 --> 00:41:16,580
At least any rates you're going to get, right?

844
00:41:16,580 --> 00:41:19,580
Maybe if you commit to buy, well, yeah, I'm going to buy 10,000 GPUs for three years,

845
00:41:19,580 --> 00:41:21,380
then maybe the cloud will give you a good rate.

846
00:41:21,380 --> 00:41:23,580
But like, you want to buy, you want to buy one GPU in the cloud?

847
00:41:23,580 --> 00:41:24,580
Ooh.

848
00:41:24,580 --> 00:41:27,700
I mean, okay, you can go to like Vast, but like if you're going to Azure, AWS, so that's

849
00:41:27,700 --> 00:41:28,700
expensive.

850
00:41:28,700 --> 00:41:29,700
Yeah.

851
00:41:29,700 --> 00:41:33,180
This is like a, like a personal data center, you know, instead of a cloud data center.

852
00:41:33,180 --> 00:41:36,180
We like the term compute cluster, so we can use NVIDIA GPUs.

853
00:41:36,180 --> 00:41:37,180
Yeah.

854
00:41:37,180 --> 00:41:38,580
Data centers may be a little bit dated.

855
00:41:38,860 --> 00:41:43,540
It's a compute cluster, which is totally legal under the CUDA license agreement.

856
00:41:43,540 --> 00:41:45,660
You talk a lot about the PCIe connection.

857
00:41:45,660 --> 00:41:48,540
Do you think there's any fat there to the trim?

858
00:41:48,540 --> 00:41:49,540
What do you mean?

859
00:41:49,540 --> 00:41:51,700
Just you're limited by bandwidth, right?

860
00:41:51,700 --> 00:41:52,700
Okay.

861
00:41:52,700 --> 00:41:53,700
For some things, yes.

862
00:41:53,700 --> 00:42:00,740
So the bandwidth is roughly 10x less than what you can get with NV linked A 100s.

863
00:42:00,740 --> 00:42:01,740
Yeah.

864
00:42:01,740 --> 00:42:04,500
NV linked A 100s are going to have, and then you can even get like full fabric and the

865
00:42:04,500 --> 00:42:08,540
NVIDIA really pushes on that stuff, 600 gigabytes per second, right?

866
00:42:08,540 --> 00:42:10,620
And PCIe four, you're going to get 60.

867
00:42:10,620 --> 00:42:11,620
All right.

868
00:42:11,620 --> 00:42:12,620
So you're getting 10x less.

869
00:42:12,620 --> 00:42:13,620
Yeah.

870
00:42:13,620 --> 00:42:17,420
Um, that said, why do you need the bandwidth, right?

871
00:42:17,420 --> 00:42:21,540
And the answer is you need it for training huge models.

872
00:42:21,540 --> 00:42:25,460
If you're training on a tiny box, your limit's going to be about 7 billion, right?

873
00:42:25,460 --> 00:42:29,140
If you're, if you're training on big stuff, your limits could be like 70 billion, right?

874
00:42:29,140 --> 00:42:30,140
Okay.

875
00:42:30,140 --> 00:42:31,140
You can hack it to get a bit higher.

876
00:42:31,140 --> 00:42:34,020
You can hack it like GBT hacked it to get a bit higher, but like that's 65 billion in

877
00:42:34,020 --> 00:42:35,020
llama.

878
00:42:35,020 --> 00:42:36,700
Like there's a reason they chose 65 billion, right?

879
00:42:36,700 --> 00:42:40,380
And that's what can reasonably fit model parallel on, on, on a GPUs.

880
00:42:40,380 --> 00:42:41,380
Right.

881
00:42:41,380 --> 00:42:45,180
So, um, yes, you, you are going to end up training models.

882
00:42:45,180 --> 00:42:48,180
The cap's going to be like 7 billion, but I actually heard this on your podcast.

883
00:42:48,180 --> 00:42:51,740
I don't think that the best chatbot models are going to be the big ones.

884
00:42:51,740 --> 00:42:54,740
I think the best chatbot models are going to be the ones where you had a thousand training

885
00:42:54,740 --> 00:42:56,780
runs instead of one.

886
00:42:56,780 --> 00:43:00,740
And I don't think that the interconnect bandwidth is going to matter that much.

887
00:43:00,740 --> 00:43:02,940
So what are we optimizing for instead of compute optimal?

888
00:43:02,940 --> 00:43:05,660
Uh, what do you mean compute optimal?

889
00:43:05,660 --> 00:43:10,340
So the, this, you're talking about this, um, the llama style models where you train

890
00:43:10,340 --> 00:43:11,340
for like 200,

891
00:43:11,340 --> 00:43:12,340
You train longer.

892
00:43:12,340 --> 00:43:13,340
Yeah.

893
00:43:13,340 --> 00:43:14,340
Yeah.

894
00:43:14,340 --> 00:43:15,340
Yeah.

895
00:43:15,340 --> 00:43:16,340
So, okay.

896
00:43:16,340 --> 00:43:17,340
You can always make your model better by doing one of two things.

897
00:43:17,340 --> 00:43:18,340
Right.

898
00:43:18,340 --> 00:43:19,340
And a comma, we just have a strict limit on it.

899
00:43:19,340 --> 00:43:21,340
Um, you can always make your model better by training longer and you can always make

900
00:43:21,340 --> 00:43:23,580
your model better by making it bigger.

901
00:43:23,580 --> 00:43:26,180
But these aren't the interesting ones, right?

902
00:43:26,180 --> 00:43:28,980
Particularly the making it bigger because training it longer, fine, you know, you're

903
00:43:28,980 --> 00:43:29,980
getting a better set of weights.

904
00:43:29,980 --> 00:43:30,980
The inference is the same.

905
00:43:30,980 --> 00:43:35,620
The inference is the same, whether I trained it for a day or a week, but the, okay.

906
00:43:35,620 --> 00:43:38,740
If it's one billion versus 10 billion, well, I 10x my inference too.

907
00:43:38,740 --> 00:43:39,740
All right.

908
00:43:39,740 --> 00:43:43,140
So I think that these big models are kind of a, sure they're great if you're research

909
00:43:43,140 --> 00:43:47,140
labs and you're trying to like max out this thing, which you can talk about later.

910
00:43:47,140 --> 00:43:48,140
Yeah.

911
00:43:48,140 --> 00:43:49,140
Yeah.

912
00:43:49,140 --> 00:43:50,140
Yeah.

913
00:43:50,140 --> 00:43:52,020
But if you're, but if you're like a startup or you're like an individual or you're trying

914
00:43:52,020 --> 00:43:56,060
to deploy this to the edge anywhere, you don't, you don't need that many weights.

915
00:43:56,060 --> 00:43:57,060
Yeah.

916
00:43:57,060 --> 00:43:58,060
Yeah.

917
00:43:58,060 --> 00:43:59,060
You don't want them anyway.

918
00:43:59,060 --> 00:44:00,060
Optimizing for inference rather than capabilities.

919
00:44:00,060 --> 00:44:01,060
Yes.

920
00:44:01,060 --> 00:44:02,060
Doing benchmarks.

921
00:44:02,060 --> 00:44:03,060
Yes.

922
00:44:03,060 --> 00:44:04,060
Yes.

923
00:44:04,060 --> 00:44:05,060
Um, and I think the, the inference thing, right?

924
00:44:05,060 --> 00:44:08,460
There should be so much more, right now the ratio between like training and inference

925
00:44:08,460 --> 00:44:11,940
on clouds, I think it's only still like, it's like two or three acts, right?

926
00:44:11,940 --> 00:44:13,740
It's two or three acts more inference, which doesn't make any sense.

927
00:44:13,740 --> 00:44:15,140
Like there should be way more inference.

928
00:44:15,140 --> 00:44:16,140
Yeah.

929
00:44:16,140 --> 00:44:19,220
There should be a 10 to a hundred X more inference in the world than, than training.

930
00:44:19,220 --> 00:44:22,500
Um, but then also like what is training, right?

931
00:44:22,500 --> 00:44:26,380
You start to see these things like Laura, like, you're getting kind of, it's kind of

932
00:44:26,380 --> 00:44:28,940
blurring the lines between inference and training.

933
00:44:28,940 --> 00:44:30,940
And I think that that blurred line is actually really good.

934
00:44:30,940 --> 00:44:34,860
I'd like to see much more like on device training or on device fine tuning of the final

935
00:44:34,860 --> 00:44:35,860
layer.

936
00:44:35,860 --> 00:44:36,860
Yeah.

937
00:44:36,860 --> 00:44:37,860
Um, we're, we're pushing toward this stuff at comma.

938
00:44:37,860 --> 00:44:38,860
Right.

939
00:44:38,860 --> 00:44:39,860
Like why am I shipping a fixed model?

940
00:44:39,860 --> 00:44:43,980
I totally want this model to fine tune based on like how, you know, your left tire is flat.

941
00:44:43,980 --> 00:44:44,980
Right.

942
00:44:44,980 --> 00:44:48,980
Like every time you cut the same turn because your left tire is flat.

943
00:44:48,980 --> 00:44:49,980
Well, it should learn that.

944
00:44:49,980 --> 00:44:50,980
Right.

945
00:44:50,980 --> 00:44:53,540
So would comma pursue perimeter efficient fine tuning?

946
00:44:53,540 --> 00:44:54,540
Yeah.

947
00:44:54,540 --> 00:44:55,540
Yeah.

948
00:44:55,540 --> 00:44:57,540
Where, where, where, where seems like a, we're looking at the stuff like that.

949
00:44:57,540 --> 00:45:00,660
I mean, comma is already very parameter efficient because we have to like run this thing in

950
00:45:00,660 --> 00:45:02,740
a car and you have to like cool it and power it.

951
00:45:02,740 --> 00:45:03,740
Yeah.

952
00:45:03,740 --> 00:45:04,740
Yeah.

953
00:45:04,740 --> 00:45:10,020
And so this kind of like intelligence cluster you have in your home, you see when the person

954
00:45:10,020 --> 00:45:14,900
is using third party model, they load them locally and kind of do the final fine tuning.

955
00:45:14,900 --> 00:45:16,500
It kind of stays within the box.

956
00:45:16,500 --> 00:45:17,500
Yeah.

957
00:45:17,500 --> 00:45:18,940
I think that that's one thing.

958
00:45:18,940 --> 00:45:21,340
That's one version of it for the privacy conscious.

959
00:45:21,340 --> 00:45:28,340
Um, I also see a world where, uh, you can have your tiny box in its down cycles, um,

960
00:45:28,340 --> 00:45:29,340
mine flop coin.

961
00:45:29,340 --> 00:45:30,340
Right.

962
00:45:30,340 --> 00:45:32,260
You know, not all, turns out not all crypto is a scam.

963
00:45:32,260 --> 00:45:33,780
There's one way to tell if crypto is a scam.

964
00:45:33,780 --> 00:45:36,740
If they're selling the coin before they make the product, it's a scam.

965
00:45:36,740 --> 00:45:39,940
If they have the product and then they sell the coin, it's maybe not a scam.

966
00:45:39,940 --> 00:45:40,940
Right.

967
00:45:40,940 --> 00:45:43,620
So yeah, my thought is like each tiny box would let you, would have a private key on

968
00:45:43,620 --> 00:45:44,620
it.

969
00:45:44,620 --> 00:45:45,620
Uh, and you have to do it this way.

970
00:45:45,620 --> 00:45:47,340
You can't just let anyone join because of civil attacks.

971
00:45:47,340 --> 00:45:48,340
Right.

972
00:45:48,340 --> 00:45:51,020
There's a real problem of like, how do I, uh, how do I ensure your data is correct?

973
00:45:51,020 --> 00:45:54,340
And the way that I ensure your data is correct on the tiny net is if you ever send wrong

974
00:45:54,340 --> 00:45:56,780
data, you're banned from the life.

975
00:45:56,780 --> 00:45:57,780
Yeah.

976
00:45:57,780 --> 00:45:59,620
You're, you're a $15,000 hardware box is banned.

977
00:45:59,620 --> 00:46:00,620
So you know, don't cheat.

978
00:46:00,620 --> 00:46:03,500
Um, obviously if it messes up, we'll forgive you.

979
00:46:03,540 --> 00:46:05,940
But, um, I'm saying like some is going to try to jailbreak your devices.

980
00:46:05,940 --> 00:46:07,940
There's no jailbreak.

981
00:46:07,940 --> 00:46:08,940
There's no jailbreak.

982
00:46:08,940 --> 00:46:09,940
There's just a different network.

983
00:46:09,940 --> 00:46:11,140
Well, there's just a private key on each device.

984
00:46:11,140 --> 00:46:12,140
Right.

985
00:46:12,140 --> 00:46:14,100
Like if you buy a tiny box from the tiny corp, I give you a private key.

986
00:46:14,100 --> 00:46:15,100
It's in my backend server.

987
00:46:15,100 --> 00:46:16,100
Right.

988
00:46:16,100 --> 00:46:17,100
You want to hack my server.

989
00:46:17,100 --> 00:46:18,100
That's illegal.

990
00:46:18,100 --> 00:46:19,100
Yeah.

991
00:46:19,100 --> 00:46:20,100
Anything you want to do on the device, the device is yours.

992
00:46:20,100 --> 00:46:21,100
My server's mine.

993
00:46:21,100 --> 00:46:22,100
Right.

994
00:46:22,100 --> 00:46:23,100
Like.

995
00:46:23,100 --> 00:46:24,100
Yeah.

996
00:46:24,100 --> 00:46:25,100
Yeah.

997
00:46:25,100 --> 00:46:26,100
Uh, have you looked into like, uh, federated training at all?

998
00:46:26,100 --> 00:46:27,100
Yeah.

999
00:46:27,100 --> 00:46:28,100
So I mean, okay.

1000
00:46:28,100 --> 00:46:29,100
You're now, there's, okay.

1001
00:46:29,100 --> 00:46:30,100
There's a lot of magnitude of federated training.

1002
00:46:30,100 --> 00:46:34,100
I mean, like, uh, over the cloud and stuff, over the internet, over the internet, but

1003
00:46:34,100 --> 00:46:36,100
also distributed on a bunch of devices.

1004
00:46:36,100 --> 00:46:37,100
Right.

1005
00:46:37,100 --> 00:46:38,100
Yeah.

1006
00:46:38,100 --> 00:46:41,100
I'm, I'm, I'm very bearish on this stuff because you're an interconnect bandwidth.

1007
00:46:41,100 --> 00:46:42,100
Right.

1008
00:46:42,100 --> 00:46:43,100
So, okay.

1009
00:46:43,100 --> 00:46:45,700
At the high end, you have your interconnect bandwidth of envy link, which is 600 gigabytes

1010
00:46:45,700 --> 00:46:46,700
per second.

1011
00:46:46,700 --> 00:46:47,700
Right.

1012
00:46:47,700 --> 00:46:53,540
The tiny box has 60 gigabytes per second and then your internet has 125 megabytes per

1013
00:46:53,540 --> 00:46:54,540
second.

1014
00:46:54,540 --> 00:46:55,540
Right.

1015
00:46:55,540 --> 00:46:56,540
Not gigabits.

1016
00:46:56,540 --> 00:46:57,540
125 megabytes.

1017
00:46:57,540 --> 00:46:58,540
Right.

1018
00:46:58,540 --> 00:46:59,540
So, okay.

1019
00:46:59,980 --> 00:47:02,540
That's, that's how, that's how many orders of magnitude we're talking here.

1020
00:47:02,540 --> 00:47:05,500
Like from 60 down to 125, like, all right.

1021
00:47:05,500 --> 00:47:06,500
That's over a hundred.

1022
00:47:06,500 --> 00:47:07,500
There's over a hundred X.

1023
00:47:07,500 --> 00:47:08,500
That's 400 X.

1024
00:47:08,500 --> 00:47:09,500
Right.

1025
00:47:09,500 --> 00:47:10,500
So like, no.

1026
00:47:10,500 --> 00:47:11,500
Uh, but what you can do is inference.

1027
00:47:11,500 --> 00:47:12,500
Right.

1028
00:47:12,500 --> 00:47:13,500
Like there's, for inference, you don't care.

1029
00:47:13,500 --> 00:47:14,500
Right.

1030
00:47:14,500 --> 00:47:18,340
For inference, I, I, there's so little bandwidth at the top and the bottom of the model, um,

1031
00:47:18,340 --> 00:47:20,340
that like, yeah, you can do federated inference.

1032
00:47:20,340 --> 00:47:21,340
Right.

1033
00:47:21,340 --> 00:47:22,340
And that's kind of what I'm talking about.

1034
00:47:22,340 --> 00:47:26,580
Um, there's also interesting things to push into like, you're like, but okay, what if

1035
00:47:26,580 --> 00:47:28,700
you want to run close source models?

1036
00:47:28,700 --> 00:47:32,740
This stuff gets kind of interesting, like using TPMs on the boxes and stuff.

1037
00:47:32,740 --> 00:47:33,740
Um, yeah.

1038
00:47:33,740 --> 00:47:35,540
But then someone might jailbreak my device.

1039
00:47:35,540 --> 00:47:37,220
So, you know, maybe we don't try to do that.

1040
00:47:37,220 --> 00:47:38,220
Yeah.

1041
00:47:38,220 --> 00:47:39,500
What's like the enterprise use case?

1042
00:47:39,500 --> 00:47:42,860
Do you see companies buying a bunch of these and like stacking them together?

1043
00:47:42,860 --> 00:47:47,820
Um, so the tiny box is like the first version of what we're building, but what I really

1044
00:47:47,820 --> 00:47:52,780
want to do is be on the absolute edge of flops per dollar and flops per lot.

1045
00:47:52,780 --> 00:47:54,140
These are the two numbers that matter.

1046
00:47:54,140 --> 00:47:58,020
Uh, so the enterprise use case is you want to train like, like comma, right?

1047
00:47:58,020 --> 00:47:59,940
So comma just built out a new compute cluster.

1048
00:47:59,940 --> 00:48:02,540
It's about, uh, it's about a person and a half.

1049
00:48:02,540 --> 00:48:07,420
Uh, so, you know, it's decent size person, a person being 20 person is a person is 20

1050
00:48:07,420 --> 00:48:08,420
paid flops.

1051
00:48:08,420 --> 00:48:09,420
It's about 30 paid flops.

1052
00:48:09,420 --> 00:48:14,660
Um, we built out a little, uh, little compute cluster and you know, we paid double what

1053
00:48:14,660 --> 00:48:17,100
you theoretically could per flop, right?

1054
00:48:17,100 --> 00:48:21,340
You theoretically could pay half per flop if you designed a bunch of custom stuff.

1055
00:48:21,340 --> 00:48:24,980
And yeah, I mean, I could see that being, you know, tiny core when comma is going to

1056
00:48:24,980 --> 00:48:25,980
be the first customer.

1057
00:48:25,980 --> 00:48:29,460
I'm going to build a box for comma and then I'm going to show off the box I built for

1058
00:48:29,460 --> 00:48:34,060
comma and be like, okay, like, do you want to build, I sell $250,000 training computers

1059
00:48:34,060 --> 00:48:35,740
or how much does one H 100 box?

1060
00:48:35,740 --> 00:48:37,660
Uh, it's, uh, it's four under grand.

1061
00:48:37,660 --> 00:48:38,660
Okay.

1062
00:48:38,660 --> 00:48:42,980
I'll build you a 400 grand training computer and it'll be 10x better than that H 100 box

1063
00:48:42,980 --> 00:48:46,980
for again, not for every use case for some, you need the interconnect bandwidth, but for

1064
00:48:46,980 --> 00:48:52,620
90% of most companies model training use cases, the tiny box will be five X faster for the

1065
00:48:52,620 --> 00:48:54,620
same price.

1066
00:48:54,620 --> 00:48:57,420
You mentioned the person of compute.

1067
00:48:57,420 --> 00:48:59,420
How do we build a human for $20 million?

1068
00:48:59,420 --> 00:49:01,540
Oh, it's a lot cheaper now.

1069
00:49:01,540 --> 00:49:02,540
It's a lot cheaper now.

1070
00:49:02,540 --> 00:49:06,860
Uh, so like I said, we comma, comma spent about, uh, about half a million on our, on

1071
00:49:06,860 --> 00:49:07,860
our person and a half.

1072
00:49:07,860 --> 00:49:14,140
So, you know, what are some of the numbers people should think of when they compare compute

1073
00:49:14,140 --> 00:49:15,140
to like people.

1074
00:49:15,140 --> 00:49:18,620
So GBD four was a hundred person years of training.

1075
00:49:18,620 --> 00:49:22,620
That's more like on, on the time scale, um, 20 petaflops is one person.

1076
00:49:22,620 --> 00:49:27,340
I think you, um, right now the math was that for the price of the most expensive thing

1077
00:49:27,340 --> 00:49:31,860
we build, which is the international space station, we could build, uh, one Tampa of

1078
00:49:31,860 --> 00:49:32,860
one Tampa.

1079
00:49:32,860 --> 00:49:33,860
Yeah, yeah.

1080
00:49:33,860 --> 00:49:34,860
One Tampa of compute.

1081
00:49:34,860 --> 00:49:35,860
Yeah.

1082
00:49:35,860 --> 00:49:36,860
Which is 400,000 people.

1083
00:49:36,860 --> 00:49:37,860
Of measurement.

1084
00:49:37,860 --> 00:49:38,860
Um, yeah.

1085
00:49:38,860 --> 00:49:39,860
Yeah.

1086
00:49:39,860 --> 00:49:40,860
We could build.

1087
00:49:40,860 --> 00:49:42,860
So like the biggest training clusters today, I know less about how GBD four was trained.

1088
00:49:42,860 --> 00:49:47,620
I know some rough numbers on the weights and stuff, but, uh, llama trillion parameters.

1089
00:49:47,620 --> 00:49:48,620
Well, okay.

1090
00:49:48,620 --> 00:49:53,580
So GBD four is 220 billion in each head and then it's an eight way mixture model.

1091
00:49:53,580 --> 00:49:55,900
So mixture models are what you do when you're out of ideas.

1092
00:49:55,900 --> 00:49:58,220
Um, so, you know, it's a, it's a mixture model.

1093
00:49:58,220 --> 00:50:01,020
Uh, they just train the same model eight times and they have some little trick.

1094
00:50:01,020 --> 00:50:05,500
They actually do 16 inferences, but, uh, no, it's not like, so the multi modality is just

1095
00:50:05,500 --> 00:50:08,620
a vision model kind of glommed on.

1096
00:50:08,620 --> 00:50:11,020
I mean the multi modality is like obvious what it is too.

1097
00:50:11,020 --> 00:50:13,700
You just put the vision model in the same token space as your language model.

1098
00:50:13,700 --> 00:50:15,300
Oh, did people think it was something else?

1099
00:50:15,300 --> 00:50:18,220
No, the mixture has nothing to do with the vision or language aspect of it.

1100
00:50:18,220 --> 00:50:22,020
It just has to do with, well, okay, we can't really make models bigger than 220 billion

1101
00:50:22,020 --> 00:50:23,020
parameters.

1102
00:50:23,020 --> 00:50:24,700
Uh, we want it to be better.

1103
00:50:24,700 --> 00:50:26,020
Well, how can we make it better?

1104
00:50:26,020 --> 00:50:31,820
Well, we can train it longer and okay, we've actually already maxed that out, uh, getting

1105
00:50:31,820 --> 00:50:32,820
diminishing returns there.

1106
00:50:32,820 --> 00:50:33,820
Okay.

1107
00:50:33,820 --> 00:50:34,820
Make sure of experts.

1108
00:50:34,820 --> 00:50:35,820
Yeah, make sure of experts.

1109
00:50:35,820 --> 00:50:36,820
We'll train eight of them.

1110
00:50:36,820 --> 00:50:37,820
Right.

1111
00:50:37,820 --> 00:50:41,740
So, you know, you know, the real truth is whenever a start, whenever a company is secretive,

1112
00:50:41,740 --> 00:50:45,380
with the exception of Apple, Apple's the only exception, whenever a company is secretive,

1113
00:50:45,380 --> 00:50:48,100
it's because they're hiding something that's not that cool.

1114
00:50:48,460 --> 00:50:51,260
People have this wrong idea over and over again that they think they're hiding it because

1115
00:50:51,260 --> 00:50:52,260
it's really cool.

1116
00:50:52,260 --> 00:50:53,260
It must be amazing.

1117
00:50:53,260 --> 00:50:54,260
It's a trillion parameters.

1118
00:50:54,260 --> 00:50:57,380
No, it's a little bigger than GPT-3 and they did an eight-way mixture of experts.

1119
00:50:57,380 --> 00:51:03,980
Like, all right, dude, anyone can spend eight times the money and get that, um, but yeah,

1120
00:51:03,980 --> 00:51:08,860
so, uh, coming back to what I think is actually going to happen is, yeah, people are going

1121
00:51:08,860 --> 00:51:14,020
to train smaller models for longer and fine tune them and find all these tricks.

1122
00:51:14,020 --> 00:51:15,020
Right.

1123
00:51:15,180 --> 00:51:18,700
You know, I think, uh, opening, I used to publish stuff on this, you know, uh, when

1124
00:51:18,700 --> 00:51:25,140
they would publish stuff, uh, about how much better the training has gotten given the same

1125
00:51:25,140 --> 00:51:30,660
of holding compute constant and it's gotten a lot better, right, than compare, like, batch

1126
00:51:30,660 --> 00:51:31,660
norm to no batch norm.

1127
00:51:32,260 --> 00:51:33,260
Yeah.

1128
00:51:33,260 --> 00:51:34,260
And now we have like-

1129
00:51:34,260 --> 00:51:35,740
Is there a finding algorithms like flash attention?

1130
00:51:35,740 --> 00:51:37,740
Yeah, well, flash attention, yeah.

1131
00:51:37,740 --> 00:51:38,740
Yeah.

1132
00:51:38,740 --> 00:51:41,140
Um, my flash attention is the same compute.

1133
00:51:41,140 --> 00:51:43,540
A flash attention is an interesting fact where it's actually the identical compute.

1134
00:51:43,580 --> 00:51:45,100
It's just a more efficient way to do the compute.

1135
00:51:45,100 --> 00:51:51,140
But I'm even talking about, like, like, um, look at the new, look at the new, uh, embeddings

1136
00:51:51,140 --> 00:51:52,140
people are using.

1137
00:51:52,140 --> 00:51:53,140
Right.

1138
00:51:53,140 --> 00:51:54,140
They used to use these like boring old embeddings.

1139
00:51:54,140 --> 00:51:56,180
Now like Lama uses that complex one and that was like alibi.

1140
00:51:56,180 --> 00:52:00,740
I'm not up to date on all the latest stuff, but, uh, those tricks give you so much.

1141
00:52:00,740 --> 00:52:02,900
There's been a whole round trip with positional embeddings.

1142
00:52:02,900 --> 00:52:04,860
I don't know if you've, uh, seen this discussion.

1143
00:52:04,860 --> 00:52:05,860
I haven't followed-

1144
00:52:05,860 --> 00:52:09,060
Like you need them, you need rotational and then you don't need them.

1145
00:52:09,060 --> 00:52:10,660
I haven't followed exactly.

1146
00:52:10,660 --> 00:52:14,220
I mean, you quickly run into the obvious problem with positional embeddings, which

1147
00:52:14,220 --> 00:52:17,460
is you have to invalidate your KV cache if you run off the context.

1148
00:52:17,460 --> 00:52:21,780
So that's why I think these new ones that play with them, but, uh, I'm not that, I'm

1149
00:52:21,780 --> 00:52:25,820
not that, I'm not an expert on like the latest up-to-date language model stuff.

1150
00:52:25,820 --> 00:52:26,820
Yeah.

1151
00:52:26,820 --> 00:52:33,940
Um, I mean, we have what we do at comma, I don't know how that works, but like, um, what

1152
00:52:33,940 --> 00:52:37,220
are some of the things, I mean, that people are getting wrong.

1153
00:52:37,220 --> 00:52:41,500
So back to autonomous driving, there was like the whole like LiDAR versus vision thing.

1154
00:52:41,500 --> 00:52:44,460
You know, it's like, people don't get into accidents because they cannot see well, they

1155
00:52:44,460 --> 00:52:47,740
get into accidents because they got distracted and all these things.

1156
00:52:47,740 --> 00:52:51,020
What are, do you see similarities today on like the pathway GI?

1157
00:52:51,020 --> 00:52:53,460
Like are there people, like what are like the-

1158
00:52:53,460 --> 00:52:57,260
Nothing, nothing I say about this is ever going to compete with how Rich Sutton stated

1159
00:52:57,260 --> 00:52:58,260
it.

1160
00:52:58,260 --> 00:52:59,260
Rich Sutton is writer-

1161
00:52:59,260 --> 00:53:00,260
The bitter lesson.

1162
00:53:00,260 --> 00:53:01,260
The first millennium, the bitter lesson.

1163
00:53:01,260 --> 00:53:02,260
Nothing I say is ever going to compete with.

1164
00:53:02,260 --> 00:53:05,060
The bitter lesson is way better than any way I'm going to phrase this.

1165
00:53:05,060 --> 00:53:08,780
Just go read that and then like, I'm sorry, it's bitter, but you actually just have to

1166
00:53:08,780 --> 00:53:09,780
believe it.

1167
00:53:09,780 --> 00:53:12,420
Like over and over again, people make this mistake.

1168
00:53:12,420 --> 00:53:14,980
They're like, oh, we're going to hand it to you or this thing, we're going to hand-

1169
00:53:14,980 --> 00:53:17,180
No, like stop wasting time.

1170
00:53:17,180 --> 00:53:20,020
Which is, I mean, OpenAI is not taking the bitter lesson.

1171
00:53:20,020 --> 00:53:21,260
No, OpenAI-

1172
00:53:21,260 --> 00:53:27,540
They were, they were leaders in deep learning for a long, long, long time, but you're telling

1173
00:53:27,540 --> 00:53:29,140
me that GPT-4 is not.

1174
00:53:29,140 --> 00:53:34,060
Well, OpenAI was the absolute leader to the thesis that computers all you need, right?

1175
00:53:34,060 --> 00:53:36,940
And there's a question of how long this thesis is going to continue for.

1176
00:53:36,940 --> 00:53:41,140
It's a cool thesis and look, I think I would be lying along with everybody else.

1177
00:53:41,140 --> 00:53:44,900
I was into language models like way back in the day for the HotterPrice.

1178
00:53:44,900 --> 00:53:47,100
I got into AI through the HotterPrice.

1179
00:53:47,100 --> 00:53:51,140
Like 2014, I'm trying to build compressive models of Wikipedia and I'm like, okay, why

1180
00:53:51,140 --> 00:53:52,140
is this so hard?

1181
00:53:52,140 --> 00:53:53,900
Like what this is is a language model, right?

1182
00:53:53,900 --> 00:53:58,380
And I'm playing with these like, like Bayesian things and I'm just like, oh, but like I get

1183
00:53:58,380 --> 00:53:59,380
it.

1184
00:53:59,380 --> 00:54:02,220
Like it needs to be like, like it's like, I have two data points and they're like almost

1185
00:54:02,220 --> 00:54:03,220
the same.

1186
00:54:03,220 --> 00:54:05,260
And so I measure that almost, right?

1187
00:54:05,260 --> 00:54:08,780
I just like, you know, wrap my head around, I couldn't like, like wrap my head around

1188
00:54:08,780 --> 00:54:09,780
this.

1189
00:54:09,780 --> 00:54:13,380
And this was around the time Carpathia released the first like RNN that generated the Shakespeare

1190
00:54:13,380 --> 00:54:14,380
stuff.

1191
00:54:14,380 --> 00:54:16,820
And I'm like, okay, I get it, right?

1192
00:54:16,820 --> 00:54:18,780
It's neural networks that are compressors.

1193
00:54:18,780 --> 00:54:21,580
Now this isn't actually, you can't actually win the HotterPrice with these things because

1194
00:54:21,580 --> 00:54:23,580
HotterPrice is MDL.

1195
00:54:23,580 --> 00:54:28,100
It's the model, size of the model plus the size of the encodings, embeddings.

1196
00:54:28,100 --> 00:54:33,100
So yeah, you can't, I mean, probably now you can because it's gotten so good.

1197
00:54:33,100 --> 00:54:35,180
But yeah, back in the day, you kind of couldn't.

1198
00:54:35,180 --> 00:54:36,180
So I was like, okay, cool.

1199
00:54:36,180 --> 00:54:37,180
Like this is what it is.

1200
00:54:37,180 --> 00:54:38,180
I kind of get it.

1201
00:54:38,180 --> 00:54:39,180
Yeah.

1202
00:54:39,180 --> 00:54:43,540
I mean, I think I didn't expect that it would continue to work this well.

1203
00:54:43,540 --> 00:54:46,660
I thought there'd be real limits to how good autocomplete could get.

1204
00:54:46,660 --> 00:54:47,660
That's fancy autocomplete.

1205
00:54:47,660 --> 00:54:51,820
But yeah, no, like it works.

1206
00:54:51,820 --> 00:54:52,820
It works well.

1207
00:54:52,820 --> 00:54:56,820
So like, yeah, what is open AI getting wrong?

1208
00:54:56,820 --> 00:54:57,820
Technically not that much.

1209
00:54:57,820 --> 00:54:58,820
I don't know.

1210
00:54:58,820 --> 00:55:02,580
Like if I was a researcher, why would I go work there?

1211
00:55:02,700 --> 00:55:03,700
Yes.

1212
00:55:03,700 --> 00:55:06,300
So why is opening AI like the Miami Heat?

1213
00:55:06,300 --> 00:55:08,900
No, look, I don't know.

1214
00:55:08,900 --> 00:55:09,900
This is my technical stuff.

1215
00:55:09,900 --> 00:55:12,740
I don't really want to harp on this, but like, why go work at opening AI when you can go

1216
00:55:12,740 --> 00:55:13,740
work at Facebook?

1217
00:55:13,740 --> 00:55:14,740
Right.

1218
00:55:14,740 --> 00:55:15,740
As a researcher.

1219
00:55:15,740 --> 00:55:20,020
Like opening AI can keep ideologs who, you know, believe ideological stuff and Facebook

1220
00:55:20,020 --> 00:55:23,900
can keep every researcher who's like, dude, I just want to build AI and publish it.

1221
00:55:23,900 --> 00:55:24,900
Yeah.

1222
00:55:24,900 --> 00:55:25,900
Yeah.

1223
00:55:25,900 --> 00:55:26,900
Awesome.

1224
00:55:26,900 --> 00:55:27,900
Yeah.

1225
00:55:27,900 --> 00:55:28,900
Any other thoughts?

1226
00:55:28,900 --> 00:55:29,900
Corp, bounties?

1227
00:55:29,900 --> 00:55:38,620
Um, yeah, so we have, you know, I've been thinking a lot about like what it means to

1228
00:55:38,620 --> 00:55:40,740
hire in today's world.

1229
00:55:40,740 --> 00:55:42,740
What actually is the like core?

1230
00:55:42,740 --> 00:55:43,740
Okay.

1231
00:55:43,740 --> 00:55:49,900
Look, I'm a believer that machines are going to replace everything in about 20 years.

1232
00:55:49,900 --> 00:55:51,460
So okay.

1233
00:55:51,460 --> 00:55:53,580
What is that?

1234
00:55:53,580 --> 00:55:58,660
What is that thing that people can still do that computers can't, right?

1235
00:55:59,420 --> 00:56:03,020
This is a narrowing list, but like, you know, back in the day, like, imagine I was starting

1236
00:56:03,020 --> 00:56:04,020
your company in 1960.

1237
00:56:04,020 --> 00:56:05,020
Right?

1238
00:56:05,020 --> 00:56:08,220
Oh, we're going to have to hire a whole bunch of calculators in the basement to do all that,

1239
00:56:08,220 --> 00:56:09,700
you know, math to support the cabinet.

1240
00:56:09,700 --> 00:56:11,300
Dude, have you heard about computers?

1241
00:56:11,300 --> 00:56:14,060
Why don't we just buy a few of those?

1242
00:56:14,060 --> 00:56:16,580
Oh, oh, wow, man.

1243
00:56:16,580 --> 00:56:17,580
You're right.

1244
00:56:17,580 --> 00:56:19,940
Um, so like, I feel like that's kind of happening again.

1245
00:56:19,940 --> 00:56:22,460
I'm thinking about, I will post in my discord.

1246
00:56:22,460 --> 00:56:26,100
I'll be like, who wants to like, okay, I just changed my unary op.

1247
00:56:26,100 --> 00:56:32,300
There used to be log and X in like E. I changed them to be log to an X to because hardware

1248
00:56:32,300 --> 00:56:34,020
has log to an X to accelerators.

1249
00:56:34,020 --> 00:56:35,020
Yeah.

1250
00:56:35,020 --> 00:56:36,020
And of course you can use change of base.

1251
00:56:36,020 --> 00:56:41,420
It's one multiply to get it back to E. But like I made the primitives log to an X to right.

1252
00:56:41,420 --> 00:56:43,180
And this is the kind of, I just posted in the discord.

1253
00:56:43,180 --> 00:56:46,100
I'm like, could someone put this pull request up and someone eventually did and I merged

1254
00:56:46,100 --> 00:56:47,100
it.

1255
00:56:47,100 --> 00:56:49,980
But I'm like, this is almost to the level where models can do it.

1256
00:56:49,980 --> 00:56:50,980
Right?

1257
00:56:50,980 --> 00:56:54,180
We're almost to the point where I can say that to a model and the model can do it.

1258
00:56:55,060 --> 00:56:56,060
Have you tried?

1259
00:56:56,060 --> 00:56:57,060
Yeah.

1260
00:56:57,060 --> 00:56:58,940
I'm, I don't know.

1261
00:56:58,940 --> 00:57:03,500
I'm like, I'm, I think it went further.

1262
00:57:03,500 --> 00:57:08,620
I think autocomplete went further than I thought it would, but I'm also relatively unimpressed

1263
00:57:08,620 --> 00:57:15,340
with these chatbots, uh, with what I've seen from the language models like there.

1264
00:57:15,340 --> 00:57:20,180
The problem is if your loss function is categorical cross entropy on the internet, your responses

1265
00:57:20,180 --> 00:57:21,180
will always be mid.

1266
00:57:21,180 --> 00:57:22,180
Yes.

1267
00:57:22,180 --> 00:57:23,180
I don't know.

1268
00:57:23,180 --> 00:57:24,180
Mode collapse is what I call it.

1269
00:57:24,180 --> 00:57:25,180
I don't know.

1270
00:57:25,180 --> 00:57:26,180
I'm not even talking about mode collapse.

1271
00:57:26,180 --> 00:57:27,180
You're actually trying to predict these.

1272
00:57:27,180 --> 00:57:28,180
Like, like, look, I rap.

1273
00:57:28,180 --> 00:57:32,260
I'm a, I'm a hobbyist rapper and like, would I try to get these things to write rap?

1274
00:57:32,260 --> 00:57:34,700
The rap sound like the kind of raps you read in the YouTube comments.

1275
00:57:34,700 --> 00:57:35,700
Nursery school.

1276
00:57:35,700 --> 00:57:36,700
Yeah.

1277
00:57:36,700 --> 00:57:37,700
It's like, all right, great.

1278
00:57:37,700 --> 00:57:38,700
You're riding box with Fox.

1279
00:57:38,700 --> 00:57:39,700
Sick rhyme, bro.

1280
00:57:39,700 --> 00:57:45,140
Uh, you know, uh, you know, and rock or Drake is rhyming, give it up for me with napkins

1281
00:57:45,140 --> 00:57:46,140
and cutlery.

1282
00:57:46,140 --> 00:57:47,140
Right?

1283
00:57:47,140 --> 00:57:48,140
Like, like, all right, come on.

1284
00:57:48,140 --> 00:57:49,140
What's that?

1285
00:57:49,140 --> 00:57:50,140
Like this thing about orange.

1286
00:57:50,140 --> 00:57:51,140
Like orange is famous.

1287
00:57:51,140 --> 00:57:52,140
Yeah.

1288
00:57:52,140 --> 00:57:58,580
And now, of course, you know, four inch screws and orange juice is in, is in training corp.

1289
00:57:58,580 --> 00:58:02,900
But uh, yeah, so I think it went further than like everyone kind of thought it would.

1290
00:58:02,900 --> 00:58:06,860
But the thing that I really want to see is like somebody put 10 LLMs in a room and have

1291
00:58:06,860 --> 00:58:08,940
them discuss the answer before they give it to me.

1292
00:58:08,940 --> 00:58:09,940
Right?

1293
00:58:09,940 --> 00:58:10,940
You can actually do this, right?

1294
00:58:10,940 --> 00:58:13,100
Um, and I think the coding things have to be the same way.

1295
00:58:13,100 --> 00:58:15,780
There is no coder alive, no matter how good you are that sits down.

1296
00:58:15,780 --> 00:58:20,280
Well, I'm going to start at cell A one and type my program and then I'm going to press

1297
00:58:20,280 --> 00:58:21,680
run and there's going to work.

1298
00:58:21,680 --> 00:58:24,080
No one programs like that.

1299
00:58:24,080 --> 00:58:25,560
So why do we expect the models to write?

1300
00:58:25,560 --> 00:58:28,080
So, so there's a lot that like still needs to be done.

1301
00:58:28,080 --> 00:58:31,400
But you know, at the tiny corp, I want to be on the cutting edge of this too.

1302
00:58:31,400 --> 00:58:33,760
I want to be like program generation.

1303
00:58:33,760 --> 00:58:34,840
I mean, what is tiny grad?

1304
00:58:34,840 --> 00:58:38,120
It's a compiler generates programs generate the fastest program that meets the spec.

1305
00:58:38,120 --> 00:58:39,120
Right?

1306
00:58:39,120 --> 00:58:40,780
Why am I not just having a male do that?

1307
00:58:40,780 --> 00:58:47,400
So you know, it's kind of a, you have to exist fluidly with the machines and I come

1308
00:58:47,400 --> 00:58:48,400
around on a lot of stuff.

1309
00:58:48,400 --> 00:58:51,000
And I'm like, wait, tiny grad, tiny group should be a remote company.

1310
00:58:51,000 --> 00:58:52,000
Right?

1311
00:58:52,000 --> 00:58:53,000
Well, I can't do this in person.

1312
00:58:53,000 --> 00:58:54,000
Really?

1313
00:58:54,000 --> 00:58:55,000
Yeah.

1314
00:58:55,000 --> 00:58:57,320
Like, like comma makes sense to be in person like comma, sure.

1315
00:58:57,320 --> 00:58:58,320
Yeah.

1316
00:58:58,320 --> 00:58:59,320
We're getting off in San Diego.

1317
00:58:59,320 --> 00:59:00,320
Like, but that's a six year old company.

1318
00:59:00,320 --> 00:59:01,320
Right.

1319
00:59:01,320 --> 00:59:03,760
And it works and it works for a certain type of people and certain type of culture, but

1320
00:59:03,760 --> 00:59:04,760
what's going to be different this time.

1321
00:59:04,760 --> 00:59:05,760
Okay.

1322
00:59:05,760 --> 00:59:06,760
Remote, but now it's remote.

1323
00:59:06,760 --> 00:59:10,720
And now I'm getting these like people who apply and I'm like, I literally have a thousand

1324
00:59:10,720 --> 00:59:11,720
applications.

1325
00:59:11,720 --> 00:59:14,800
I'm not calling you to do a technical screen and I can't really tell anything from a

1326
00:59:14,800 --> 00:59:15,800
technical screen.

1327
00:59:15,800 --> 00:59:16,800
What am I going to do?

1328
00:59:16,800 --> 00:59:17,800
Make a code on a whiteboard?

1329
00:59:17,840 --> 00:59:22,320
Bring up, bring up a shared notebook document so we could, oh, like that's not going to

1330
00:59:22,320 --> 00:59:23,320
work.

1331
00:59:23,320 --> 00:59:24,320
Okay.

1332
00:59:24,320 --> 00:59:25,320
So then I'm moved to the next thing.

1333
00:59:25,320 --> 00:59:27,520
We do this a comma with good success programming challenges.

1334
00:59:27,520 --> 00:59:30,480
I've also found them to be like completely non predictive.

1335
00:59:30,480 --> 00:59:35,520
I found one thing to actually be predictive and it's, wait a second, just write code in

1336
00:59:35,520 --> 00:59:36,520
tiny grad.

1337
00:59:36,520 --> 00:59:37,520
It's open source.

1338
00:59:37,520 --> 00:59:38,520
Right.

1339
00:59:38,520 --> 00:59:39,520
And yeah.

1340
00:59:39,520 --> 00:59:43,160
So, you know, I'm talking to a few people who've been contributing and like contribute

1341
00:59:43,160 --> 00:59:46,440
or you know, the job's not for you, but you can do it remote and it's like it's a

1342
00:59:46,440 --> 00:59:47,440
chill job.

1343
00:59:47,440 --> 00:59:49,560
Like you're not, you're like, oh yeah, well, I work for the tiny corp.

1344
00:59:49,560 --> 00:59:51,680
Like, well, you're writing MIT license software.

1345
00:59:51,680 --> 00:59:52,680
Like you see what it's doing.

1346
00:59:52,680 --> 00:59:53,680
Right.

1347
00:59:53,680 --> 00:59:56,880
Like we'll just, I think it's maybe more of like a stipend than a salary and then also

1348
00:59:56,880 --> 00:59:57,880
some equity.

1349
00:59:57,880 --> 00:59:58,880
Like, you know, I get rich.

1350
00:59:58,880 --> 00:59:59,880
We all get rich.

1351
00:59:59,880 --> 01:00:00,880
Yeah.

1352
01:00:00,880 --> 01:00:01,880
Yeah.

1353
01:00:01,880 --> 01:00:07,400
How do you think about agents and kind of like thinking of them as people versus like

1354
01:00:07,400 --> 01:00:08,680
job to be done?

1355
01:00:08,680 --> 01:00:13,680
Sean, build this thing called small developer and then it's in the same vein like the human

1356
01:00:13,680 --> 01:00:18,120
in the loop with the language model and just iterating while you write code.

1357
01:00:18,120 --> 01:00:20,440
I think, I think that's, that's absolutely where it goes.

1358
01:00:20,440 --> 01:00:22,440
And there's like a, it's not like one thing.

1359
01:00:22,440 --> 01:00:23,960
It's like, there's small interpreter.

1360
01:00:23,960 --> 01:00:24,960
There's like small debugger.

1361
01:00:24,960 --> 01:00:27,320
It's kind of like all these different jobs to be done.

1362
01:00:27,320 --> 01:00:28,320
It's a small world.

1363
01:00:28,320 --> 01:00:29,320
Yeah.

1364
01:00:29,320 --> 01:00:30,320
It's a, I know this is like the small pockets.

1365
01:00:30,320 --> 01:00:32,480
It's like small AI meet tiny corp.

1366
01:00:32,480 --> 01:00:34,120
So we're all in the same wavelength.

1367
01:00:34,120 --> 01:00:35,120
How do you think about that?

1368
01:00:35,120 --> 01:00:39,840
Do you think people will have a human like interaction with like, oh, this is like the

1369
01:00:39,840 --> 01:00:44,440
AI developer or like, is it, I'm the human being supercharged by the AI tools?

1370
01:00:44,440 --> 01:00:49,120
Oh, I think it's much more like I'm the human supercharged by the AI tools.

1371
01:00:49,120 --> 01:00:52,040
I think that like coding is tool complete.

1372
01:00:52,040 --> 01:00:53,040
Right.

1373
01:00:53,040 --> 01:00:54,040
Like driving is not tool complete.

1374
01:00:54,040 --> 01:00:55,040
Right.

1375
01:00:55,040 --> 01:00:57,360
Like driving is just like, like we hire people to drive who are like below the API line.

1376
01:00:57,360 --> 01:00:58,360
Right.

1377
01:00:58,360 --> 01:00:59,360
There's an API line in the world.

1378
01:00:59,360 --> 01:01:00,360
Right.

1379
01:01:00,360 --> 01:01:01,360
Love that.

1380
01:01:01,360 --> 01:01:02,360
Yeah.

1381
01:01:02,360 --> 01:01:03,360
There's an API line in the world and like you can think like Uber's a really clear

1382
01:01:03,360 --> 01:01:04,360
example.

1383
01:01:04,360 --> 01:01:05,360
Right.

1384
01:01:05,360 --> 01:01:06,360
There's the people below the API line and the people above the API line.

1385
01:01:06,400 --> 01:01:10,400
The way you can tell if you're below or above, by the way, is, is your manager a computer?

1386
01:01:10,400 --> 01:01:11,400
Right.

1387
01:01:11,400 --> 01:01:12,400
Who's the manager of the Uber driver?

1388
01:01:12,400 --> 01:01:13,400
Well, computer.

1389
01:01:13,400 --> 01:01:14,400
There's a machine tell you what to do.

1390
01:01:14,400 --> 01:01:15,400
Do you tell machines what to do?

1391
01:01:15,400 --> 01:01:16,400
Exactly.

1392
01:01:16,400 --> 01:01:17,400
Exactly.

1393
01:01:17,400 --> 01:01:18,400
Um, so.

1394
01:01:18,400 --> 01:01:19,400
Coding is tool complete.

1395
01:01:19,400 --> 01:01:20,400
Right.

1396
01:01:20,400 --> 01:01:21,400
Coding is tool complete.

1397
01:01:21,400 --> 01:01:22,400
Coding is above the API line.

1398
01:01:22,400 --> 01:01:27,880
So it will always be, uh, tools supercharging your coding workflow and it will never be

1399
01:01:27,880 --> 01:01:34,880
you performing some like task like, okay, well, I can do everything except for actually starting

1400
01:01:34,880 --> 01:01:35,880
a docker container.

1401
01:01:35,880 --> 01:01:36,880
Like it just doesn't make any sense.

1402
01:01:36,880 --> 01:01:37,880
Right.

1403
01:01:37,880 --> 01:01:38,880
Um, yeah.

1404
01:01:38,880 --> 01:01:39,880
So it will always be sort of tools.

1405
01:01:39,880 --> 01:01:43,000
And you know, look, we see the same stuff with all the, like people are like stable diffusion

1406
01:01:43,000 --> 01:01:45,080
is going to replace artists or whatever.

1407
01:01:45,080 --> 01:01:48,640
It's like, dude, like it's going to create new artists, did Photoshop replace artists?

1408
01:01:48,640 --> 01:01:50,360
Like, what are you talking about?

1409
01:01:50,360 --> 01:01:51,360
Right.

1410
01:01:51,360 --> 01:01:56,320
Like, you know, real artists, finger paint, they can't use brushes, brushes are, you know,

1411
01:01:56,320 --> 01:02:01,440
brushes are going to replace all the, okay, like, I just can't like it's all just tools

1412
01:02:01,440 --> 01:02:03,440
and the tools are going to get better and better and better.

1413
01:02:03,440 --> 01:02:04,440
Eventually.

1414
01:02:04,440 --> 01:02:05,440
Yes.

1415
01:02:05,440 --> 01:02:06,440
The tools are going to replace us.

1416
01:02:06,440 --> 01:02:07,440
But you know, that's still 20 years away.

1417
01:02:07,440 --> 01:02:10,520
So now I got a company in the meantime.

1418
01:02:10,520 --> 01:02:13,440
So I've written about the API line before and I think that's from Venkatesh.

1419
01:02:13,440 --> 01:02:15,880
I don't know if you've definitely took it from someone.

1420
01:02:15,880 --> 01:02:16,880
It's definitely not mine.

1421
01:02:16,880 --> 01:02:17,880
VGR.

1422
01:02:17,880 --> 01:02:18,880
Yeah.

1423
01:02:18,880 --> 01:02:21,520
But I also have a speculated a higher line than that, which is the Kanban board, like

1424
01:02:21,520 --> 01:02:23,840
who tells the, the programmers what to do.

1425
01:02:23,840 --> 01:02:24,840
Hmm.

1426
01:02:24,840 --> 01:02:25,840
Right.

1427
01:02:25,840 --> 01:02:29,640
So are you above or below the Kanban board at this has that evolved your management

1428
01:02:29,640 --> 01:02:30,640
thinking?

1429
01:02:30,640 --> 01:02:31,640
Yeah.

1430
01:02:31,640 --> 01:02:32,640
Like that's sort of what I mean.

1431
01:02:32,640 --> 01:02:36,400
Like I'm just going to describe the pull request in two sentences and then like, yeah,

1432
01:02:36,400 --> 01:02:37,400
yeah.

1433
01:02:37,400 --> 01:02:38,880
So you are running the Kanban board or the bounties?

1434
01:02:38,880 --> 01:02:39,880
Yes.

1435
01:02:39,880 --> 01:02:40,880
Yeah.

1436
01:02:40,880 --> 01:02:41,880
The bounties of the Kanban board.

1437
01:02:41,880 --> 01:02:42,880
Yes.

1438
01:02:42,880 --> 01:02:43,880
Exactly.

1439
01:02:43,880 --> 01:02:44,880
And that is kind of the high level.

1440
01:02:44,880 --> 01:02:48,560
And then like, yeah, we'll get AIs to fill in some and we'll get people to fill in others.

1441
01:02:48,560 --> 01:02:51,280
And that's also what it means to be like full time, a tiny corp.

1442
01:02:51,280 --> 01:02:52,280
Right.

1443
01:02:52,280 --> 01:02:55,080
Would you start, and I wrote this up pretty concretely, I'm like, okay, step one is you

1444
01:02:55,080 --> 01:02:56,680
do bounties for the company.

1445
01:02:56,680 --> 01:02:58,680
Step two is you propose bounties for the company.

1446
01:02:58,680 --> 01:02:59,680
Right.

1447
01:02:59,680 --> 01:03:00,680
You don't obviously pay them.

1448
01:03:00,680 --> 01:03:01,680
We pay them.

1449
01:03:01,720 --> 01:03:06,400
And I'm like, yeah, that's a good bounty that like helps with the main workflow of the company

1450
01:03:06,400 --> 01:03:11,640
and step three is you get hired full time, you get equity, we all maybe get rich.

1451
01:03:11,640 --> 01:03:14,400
What else are you designing differently about the employee experience?

1452
01:03:14,400 --> 01:03:20,320
I mean, I'm very much alike, you know, some people really like to like, like keep a separation.

1453
01:03:20,320 --> 01:03:21,320
Right.

1454
01:03:21,320 --> 01:03:25,960
Some people really like to keep a separation between like employees and management or customers

1455
01:03:25,960 --> 01:03:26,960
and employees.

1456
01:03:26,960 --> 01:03:30,800
Like a comma, you know, the reason I do the dev kit thing, it's like, dude, you buy a

1457
01:03:30,800 --> 01:03:33,760
common thing, you're an employee of the company, like you're just part of the company.

1458
01:03:33,760 --> 01:03:35,080
It's all the same thing.

1459
01:03:35,080 --> 01:03:39,200
There's no like secrets, there's no dividing lines, there's no like, it's all a spectrum

1460
01:03:39,200 --> 01:03:42,840
for like, you know, down here at the spectrum, like you pay and then up here at the spectrum,

1461
01:03:42,840 --> 01:03:43,840
you get paid.

1462
01:03:43,840 --> 01:03:45,820
You understand this is the same spectrum of college, right?

1463
01:03:45,820 --> 01:03:50,000
Like for undergrad, you pay and then you get up here to like, you know, doing a PhD program,

1464
01:03:50,000 --> 01:03:51,000
you get paid.

1465
01:03:51,000 --> 01:03:52,000
Okay.

1466
01:03:52,000 --> 01:03:53,000
Well, cool.

1467
01:03:53,000 --> 01:03:55,720
Welcome to the, you know.

1468
01:03:55,720 --> 01:03:58,120
What about a comma bodies?

1469
01:03:58,120 --> 01:04:01,240
You know, you mentioned a lot of this stuff is clearly virtual, but then there's below

1470
01:04:01,240 --> 01:04:03,480
the API line you actually need.

1471
01:04:03,480 --> 01:04:06,280
This is the thing that's been announced?

1472
01:04:06,280 --> 01:04:07,280
Comma bodies?

1473
01:04:07,280 --> 01:04:08,280
We sell them.

1474
01:04:08,280 --> 01:04:09,280
Oh, okay.

1475
01:04:09,280 --> 01:04:10,280
You can buy them.

1476
01:04:10,280 --> 01:04:11,280
There's a thousand bucks on our website.

1477
01:04:11,280 --> 01:04:12,280
Oh, okay.

1478
01:04:12,280 --> 01:04:13,280
No, no, no.

1479
01:04:13,280 --> 01:04:14,280
I'm thinking about like the, what Tesla announced with like the humanoid robot.

1480
01:04:14,280 --> 01:04:15,280
It's the same thing.

1481
01:04:15,280 --> 01:04:16,280
Yeah.

1482
01:04:16,280 --> 01:04:17,280
Except of course we made the comma version of it.

1483
01:04:17,280 --> 01:04:20,000
Tesla uses 20 actuators, we use two, right?

1484
01:04:20,000 --> 01:04:25,440
Like, how do you, how do you build the simplest possible thing that can like turn the robotics

1485
01:04:25,440 --> 01:04:27,000
problem into entirely a software problem?

1486
01:04:27,000 --> 01:04:32,240
So right now it is literally just a comma three on a pole with two wheels.

1487
01:04:32,240 --> 01:04:37,080
It balances, keeps the comma three up there and like there's so much you could do with

1488
01:04:37,080 --> 01:04:38,080
that already.

1489
01:04:38,080 --> 01:04:39,080
Right?

1490
01:04:39,080 --> 01:04:41,800
Like this should replace, how many security guards could this replace?

1491
01:04:41,800 --> 01:04:42,800
Right?

1492
01:04:42,800 --> 01:04:46,920
If this thing could just competently wander around a space and take pictures and, you

1493
01:04:46,920 --> 01:04:51,200
know, focus in on things, send you a text message when someone's trying to break it

1494
01:04:51,200 --> 01:04:55,440
to your building, you know, like, like this could already do so much, of course, but

1495
01:04:55,440 --> 01:04:56,680
the software is not there yet.

1496
01:04:56,680 --> 01:04:57,680
Right?

1497
01:04:57,680 --> 01:05:00,720
So how do we turn robotics into a thing where it's very clearly a software problem?

1498
01:05:00,720 --> 01:05:03,280
You know, that people don't accept that self-driving cars are a software problem.

1499
01:05:03,280 --> 01:05:05,760
I'm like, I don't know what to tell you, man.

1500
01:05:05,760 --> 01:05:09,720
Like literally just watch the video yourself and then drive with a joystick.

1501
01:05:09,720 --> 01:05:10,720
Right?

1502
01:05:10,720 --> 01:05:11,720
Yeah.

1503
01:05:11,720 --> 01:05:12,720
Can you drive?

1504
01:05:12,720 --> 01:05:13,720
We've actually done this test.

1505
01:05:13,720 --> 01:05:15,840
We've actually done this test where we've had someone, okay, you just watch this video

1506
01:05:15,840 --> 01:05:18,600
and here's a joystick and you got to drive the car and of course they can drive the car.

1507
01:05:18,600 --> 01:05:19,600
Yeah.

1508
01:05:19,600 --> 01:05:24,000
It takes a little bit of practice to get used to the joystick, but the problem is all

1509
01:05:24,000 --> 01:05:25,000
the model.

1510
01:05:25,120 --> 01:05:26,120
Yeah.

1511
01:05:26,120 --> 01:05:31,920
It is specifically anything in computer vision that you think our second most popular episode

1512
01:05:31,920 --> 01:05:36,600
ever was about segment anything coming out of Facebook, which is as far as I understand

1513
01:05:36,600 --> 01:05:40,880
the state of the art in computer vision, what are you hoping for there that you need for

1514
01:05:40,880 --> 01:05:41,880
a comma?

1515
01:05:41,880 --> 01:05:42,880
I haven't used segment anything.

1516
01:05:42,880 --> 01:05:47,760
I mean, like the large yolos or not, I've used like large yolos and I'm super impressed

1517
01:05:47,760 --> 01:05:48,760
by them.

1518
01:05:48,760 --> 01:05:49,760
Yeah.

1519
01:05:49,760 --> 01:05:50,760
You think it's solved?

1520
01:05:50,760 --> 01:05:51,760
I got to check out segment anything.

1521
01:05:51,760 --> 01:05:53,880
I don't think it's a distinct problem, right?

1522
01:05:53,880 --> 01:05:54,880
Okay.

1523
01:05:54,880 --> 01:05:55,880
Here's something that I'm interested in.

1524
01:05:55,880 --> 01:05:56,880
All right.

1525
01:05:56,880 --> 01:05:57,880
We have great LLMs.

1526
01:05:57,880 --> 01:06:00,360
We have great text-to-speech models and we have great speech-to-text models.

1527
01:06:00,360 --> 01:06:01,360
Okay.

1528
01:06:01,360 --> 01:06:03,160
So why can I not talk to an LLM?

1529
01:06:03,160 --> 01:06:04,680
Like at how a normal conversation with them?

1530
01:06:04,680 --> 01:06:07,960
You can with the latency of like two seconds every time.

1531
01:06:07,960 --> 01:06:08,960
Right.

1532
01:06:08,960 --> 01:06:09,960
Why isn't this?

1533
01:06:09,960 --> 01:06:11,560
And then it feels so unnatural.

1534
01:06:11,560 --> 01:06:15,040
It's this like staccato, like I don't like the RLHF models.

1535
01:06:15,040 --> 01:06:17,000
I don't like the two inversions of them.

1536
01:06:17,000 --> 01:06:21,280
I think that they become, you take on the personality of a customer support agent.

1537
01:06:21,280 --> 01:06:22,280
Right.

1538
01:06:22,280 --> 01:06:23,280
Like, oh, come on.

1539
01:06:23,280 --> 01:06:25,960
I like LLAMA more than Chatchapiti.

1540
01:06:25,960 --> 01:06:29,440
Chatchapiti's personality just graded on me.

1541
01:06:29,440 --> 01:06:30,920
Was LLAMA like cool?

1542
01:06:30,920 --> 01:06:32,640
I write a little bit of pretext paragraph.

1543
01:06:32,640 --> 01:06:34,680
I can put you in any scenario I want, right?

1544
01:06:34,680 --> 01:06:36,160
Like that's interesting to me.

1545
01:06:36,160 --> 01:06:38,440
I don't want some like, you know, yeah.

1546
01:06:38,440 --> 01:06:47,400
So yeah, I think there is really no like distinction between computer vision and language and any

1547
01:06:47,400 --> 01:06:49,440
of this stuff.

1548
01:06:49,440 --> 01:06:52,400
It's all eventually going to be fused into one massive.

1549
01:06:52,400 --> 01:06:55,000
So to say computer vision is solved, well, it doesn't make any sense because what's the

1550
01:06:55,000 --> 01:06:59,840
output of computer vision model segmentation, like what a weird task, right?

1551
01:06:59,840 --> 01:07:00,840
Who cares?

1552
01:07:00,840 --> 01:07:01,840
OCR.

1553
01:07:01,840 --> 01:07:02,840
Who cares?

1554
01:07:02,840 --> 01:07:05,000
I don't care if you can segment which pixels make up that laptop, I care if you can pick

1555
01:07:05,000 --> 01:07:06,000
it up.

1556
01:07:06,000 --> 01:07:07,000
Yeah.

1557
01:07:07,000 --> 01:07:10,000
Interactive real world.

1558
01:07:10,000 --> 01:07:12,080
And you're going to have the local cluster.

1559
01:07:12,080 --> 01:07:13,920
You're going to have the body.

1560
01:07:13,920 --> 01:07:14,920
Yeah.

1561
01:07:14,920 --> 01:07:15,920
Yeah.

1562
01:07:15,920 --> 01:07:16,920
I think that's kind of where that goes.

1563
01:07:17,200 --> 01:07:23,120
So maybe we can paint the future of like, the year is 2050.

1564
01:07:23,120 --> 01:07:26,080
You've achieved all you wanted at TinyCorp.

1565
01:07:26,080 --> 01:07:28,880
What is the AI enabled future like?

1566
01:07:28,880 --> 01:07:30,720
Well, TinyCorp is the second company.

1567
01:07:30,720 --> 01:07:32,000
Comma was the first.

1568
01:07:32,000 --> 01:07:33,720
Comma builds the hardware infrastructure.

1569
01:07:33,720 --> 01:07:35,680
TinyCorp builds the software infrastructure.

1570
01:07:35,680 --> 01:07:38,960
The third company is the first one that's going to build a real product and that product

1571
01:07:38,960 --> 01:07:40,960
is AI Girlfriend.

1572
01:07:40,960 --> 01:07:43,400
No, like I'm dead serious, right?

1573
01:07:43,400 --> 01:07:45,280
Like this is the dream product, right?

1574
01:07:45,280 --> 01:07:47,840
This is the absolute dream product.

1575
01:07:47,840 --> 01:07:49,640
Girlfriend is just the like.

1576
01:07:49,640 --> 01:07:50,640
Stand-in.

1577
01:07:50,640 --> 01:07:51,640
Well, no, it's not a stand-in.

1578
01:07:51,640 --> 01:07:52,640
No, no, no.

1579
01:07:52,640 --> 01:07:53,640
I actually mean it, right?

1580
01:07:53,640 --> 01:07:56,880
So I've been wanting to merge with a machine ever since I was like, mad little, like, you

1581
01:07:56,880 --> 01:07:59,280
know, how do I merge with a machine, right?

1582
01:07:59,280 --> 01:08:02,120
And like you can look at like in like a maybe the Elon style way of thinking about his neural

1583
01:08:02,120 --> 01:08:03,120
link, right?

1584
01:08:03,120 --> 01:08:06,440
I'm like, I don't think we need any of this, right?

1585
01:08:06,440 --> 01:08:11,200
Some of your friends, maybe they get into relationships and you start thinking of, you

1586
01:08:11,200 --> 01:08:12,960
know, them and their partner as the same person.

1587
01:08:12,960 --> 01:08:14,840
You start thinking of them as like one person.

1588
01:08:14,840 --> 01:08:17,560
I mean, they are kind of like merged, right?

1589
01:08:17,560 --> 01:08:19,840
Like humans can just kind of do this.

1590
01:08:19,840 --> 01:08:20,840
It's so cool.

1591
01:08:20,840 --> 01:08:22,320
It's this ability that we already have.

1592
01:08:22,320 --> 01:08:26,920
It's all I need to put, you know, electrodes in my brain to merge with a machine.

1593
01:08:26,920 --> 01:08:29,480
I need an AI girlfriend, right?

1594
01:08:29,480 --> 01:08:30,480
So that's what I mean.

1595
01:08:30,480 --> 01:08:32,880
Like this is the third product.

1596
01:08:32,880 --> 01:08:34,560
This is the third company.

1597
01:08:34,560 --> 01:08:38,680
And yeah, in 2050, I mean like, ah, it's so hard.

1598
01:08:38,960 --> 01:08:44,560
Like maybe I can imagine like 2035, I don't even know 2050, like, yeah, 2035, like, yeah,

1599
01:08:44,560 --> 01:08:45,560
that'd be really great.

1600
01:08:45,560 --> 01:08:48,480
Like I have this like kind of, you know.

1601
01:08:48,480 --> 01:08:53,240
So in terms of merging, like, isn't it, shouldn't you work on brain upload rather than AI girlfriend?

1602
01:08:53,240 --> 01:08:55,240
But I don't need brain upload, right?

1603
01:08:55,240 --> 01:08:56,680
I don't need brain upload either.

1604
01:08:56,680 --> 01:08:59,600
Like there's, there's thousands of hours of me on YouTube, right?

1605
01:08:59,600 --> 01:09:00,600
Yes.

1606
01:09:00,600 --> 01:09:02,720
If you might, how much of my brain's already uploaded?

1607
01:09:02,720 --> 01:09:04,200
That's only the stuff that you voice.

1608
01:09:04,200 --> 01:09:06,040
Yeah, it's not that different.

1609
01:09:06,040 --> 01:09:07,600
It's not that different, right?

1610
01:09:07,600 --> 01:09:12,120
You really think a powerful, you really think a, a model with, you know, an exoflop of compute

1611
01:09:12,120 --> 01:09:15,040
couldn't extract everything that's really going on in my brain.

1612
01:09:15,040 --> 01:09:16,440
I'm a pretty open person, right?

1613
01:09:16,440 --> 01:09:17,800
Like I'm not running a complex filter.

1614
01:09:17,800 --> 01:09:19,520
Humans can't run that complex of a filter.

1615
01:09:19,520 --> 01:09:20,520
Yeah.

1616
01:09:20,520 --> 01:09:21,520
Like humans just can't.

1617
01:09:21,520 --> 01:09:23,800
Like this is actually a cool quirk of, of, of biology.

1618
01:09:23,800 --> 01:09:26,280
It's like, well, humans like can't lie that well.

1619
01:09:26,280 --> 01:09:27,280
Yeah.

1620
01:09:27,280 --> 01:09:28,280
Yeah.

1621
01:09:28,280 --> 01:09:32,280
So is it good or bad to put all of your stream of consciousness out there?

1622
01:09:32,280 --> 01:09:35,280
I mean, I think it's good.

1623
01:09:35,280 --> 01:09:36,280
Yeah.

1624
01:09:36,280 --> 01:09:37,280
I mean, I don't know.

1625
01:09:37,280 --> 01:09:38,280
I don't know.

1626
01:09:38,280 --> 01:09:39,280
I don't know.

1627
01:09:39,280 --> 01:09:40,280
I don't live forever.

1628
01:09:40,280 --> 01:09:41,280
Yeah.

1629
01:09:41,280 --> 01:09:43,520
We said off, off Mike, we may be the first immortals, right?

1630
01:09:43,520 --> 01:09:44,520
Yeah.

1631
01:09:44,520 --> 01:09:45,520
Yeah.

1632
01:09:45,520 --> 01:09:46,760
Like this is how you, this is how you live forever.

1633
01:09:46,760 --> 01:09:49,960
It's a question of, okay, how many weights do I have?

1634
01:09:49,960 --> 01:09:50,960
Right?

1635
01:09:50,960 --> 01:09:51,960
Okay.

1636
01:09:51,960 --> 01:09:52,960
Let's say I have a trillion weights.

1637
01:09:52,960 --> 01:09:53,960
So it's talking about terabytes, a hundred terabytes here.

1638
01:09:53,960 --> 01:09:55,880
Like if it's not really a hundred terabytes, right?

1639
01:09:55,880 --> 01:09:56,880
Because it's called homograph complexity.

1640
01:09:56,880 --> 01:09:58,520
How much redundancy is there in those weights?

1641
01:09:58,520 --> 01:10:04,240
So like maximally compressed, how big is the weight file for my brain quantize it whatever

1642
01:10:04,240 --> 01:10:05,240
you want.

1643
01:10:05,240 --> 01:10:07,600
And quantization is, is a poor man's compression.

1644
01:10:07,600 --> 01:10:14,080
Um, I think we're only talking really here about like maybe a couple gigabytes, right?

1645
01:10:14,080 --> 01:10:19,120
And then if you have like a couple gigabytes of true information of yourself up there, cool

1646
01:10:19,120 --> 01:10:21,840
man, like what does it mean for me to live forever?

1647
01:10:21,840 --> 01:10:22,840
Like that's me.

1648
01:10:22,840 --> 01:10:23,840
Yeah.

1649
01:10:23,840 --> 01:10:24,840
No, I think that's good.

1650
01:10:24,840 --> 01:10:30,200
And I think like the, there's a bit of like a professionalization of social media or like

1651
01:10:30,200 --> 01:10:34,680
a lot of people only have what's like PC out there, you know, and I feel like you're gonna

1652
01:10:34,680 --> 01:10:37,280
get, come back to the chat GPT thing, right?

1653
01:10:37,280 --> 01:10:40,800
You're gonna train a model and like everything that's public about a lot of people.

1654
01:10:40,800 --> 01:10:48,440
And it's like, no one's gonna run their model and they're gonna die on social media.

1655
01:10:48,440 --> 01:10:51,680
Your life could depend on it.

1656
01:10:51,680 --> 01:10:55,760
We have a segment, uh, so, uh, we're, we're moving on to a, what, what would normally

1657
01:10:55,760 --> 01:10:58,840
be called the lightning round, but just, uh, just general takes because you're a generally

1658
01:10:58,840 --> 01:11:00,920
interesting person with many other interests.

1659
01:11:00,920 --> 01:11:04,520
Um, uh, what does the goddess of everything else mean to you?

1660
01:11:05,360 --> 01:11:10,520
Oh, it means that AI is not really going to kill us.

1661
01:11:10,520 --> 01:11:11,520
Really?

1662
01:11:11,520 --> 01:11:12,520
Of course.

1663
01:11:12,520 --> 01:11:14,520
Tell us more.

1664
01:11:14,520 --> 01:11:19,120
Look, uh, Lex asked me this, like, is they are going to kill us all?

1665
01:11:19,120 --> 01:11:22,000
And I was quick to say yes, but I don't actually really believe it.

1666
01:11:22,000 --> 01:11:25,520
I think there's a decent chance that AI, I think there's a decent chance that AI kills

1667
01:11:25,520 --> 01:11:27,320
95% of us.

1668
01:11:27,320 --> 01:11:28,820
Okay.

1669
01:11:28,820 --> 01:11:31,360
But they saw on your Twitch streams that you're with them.

1670
01:11:31,360 --> 01:11:34,480
So they're not gonna, no, I don't think I actually, I don't know.

1671
01:11:34,480 --> 01:11:35,440
I also think it's AI.

1672
01:11:35,440 --> 01:11:37,600
Like I think the AI alignment problem is so misstated.

1673
01:11:37,600 --> 01:11:41,520
I think it's actually not a question of whether the computer is aligned with the company who

1674
01:11:41,520 --> 01:11:42,520
owns the computer.

1675
01:11:42,520 --> 01:11:44,880
It's a question of whether that company is aligned with you or that government's aligned

1676
01:11:44,880 --> 01:11:45,880
with you.

1677
01:11:45,880 --> 01:11:46,880
And the answer is no.

1678
01:11:46,880 --> 01:11:47,880
And that's how you end up dead.

1679
01:11:47,880 --> 01:11:53,440
But, um, so what, what the goddess of everything else means to me is like, the complexity will

1680
01:11:53,440 --> 01:11:54,440
continue.

1681
01:11:54,440 --> 01:11:55,440
Paper clippers don't exist.

1682
01:11:55,440 --> 01:11:58,600
You know, there are forces, the paper clippers cancer, right?

1683
01:11:58,600 --> 01:12:02,560
The paper clippers really just a perfect form of cancer and the goddess of everything

1684
01:12:02,560 --> 01:12:08,000
else says, yeah, if a cancer doesn't win, you know, yeah, it's a beautiful story for

1685
01:12:08,000 --> 01:12:09,200
those who haven't heard it.

1686
01:12:09,200 --> 01:12:11,720
And you read it out and I listen to it.

1687
01:12:11,720 --> 01:12:12,720
Um, yeah.

1688
01:12:12,720 --> 01:12:13,720
Good.

1689
01:12:13,720 --> 01:12:14,720
What else we have here?

1690
01:12:14,720 --> 01:12:15,720
Pick a question.

1691
01:12:15,720 --> 01:12:16,720
So many.

1692
01:12:16,720 --> 01:12:17,720
Yeah.

1693
01:12:17,720 --> 01:12:18,720
What are you grateful for today?

1694
01:12:18,720 --> 01:12:19,720
Oh, man.

1695
01:12:19,720 --> 01:12:25,800
I mean, it's all just like, I haven't, I haven't taken it about this stuff forever.

1696
01:12:25,800 --> 01:12:30,600
Like that it's actually like happening and it's happening in an accessible way too.

1697
01:12:30,600 --> 01:12:32,480
I guess that's what I'm really grateful for.

1698
01:12:32,480 --> 01:12:37,080
It's not like, like AI is not some Manhattan project style.

1699
01:12:37,080 --> 01:12:38,640
You don't know anything about it.

1700
01:12:38,640 --> 01:12:39,640
Close doors.

1701
01:12:39,640 --> 01:12:40,640
Close doors.

1702
01:12:40,640 --> 01:12:41,640
I'll fight really hard to keep it that way.

1703
01:12:41,640 --> 01:12:47,440
Uh, you know, uh, that's, that's a, I'm grateful for just, just how much is released

1704
01:12:47,440 --> 01:12:51,000
out there and how much I can just learn and stay up to date.

1705
01:12:51,000 --> 01:12:55,680
And I guess I'm grateful to the true fabric of reality that, you know, I didn't need differential

1706
01:12:55,680 --> 01:12:56,680
equations to understand it.

1707
01:12:56,680 --> 01:13:00,840
Like I don't need, you don't need, you don't need some like, like, like there's, there's

1708
01:13:00,840 --> 01:13:01,840
I've tried to do.

1709
01:13:02,080 --> 01:13:06,040
There's a limit to my, to my math ability is I can do most undergrad math, but I took

1710
01:13:06,040 --> 01:13:09,160
some grad math glasses and okay, now we're getting to the end of what I can do.

1711
01:13:09,160 --> 01:13:13,880
And it's just the actual like end of what I can do, like I'm limited by my brain, but

1712
01:13:13,880 --> 01:13:17,800
you know, ML stuff, you need high school math.

1713
01:13:17,800 --> 01:13:18,800
Yeah.

1714
01:13:18,800 --> 01:13:19,800
Like I could do all that.

1715
01:13:19,800 --> 01:13:20,800
Nothing like, you know what I mean?

1716
01:13:20,800 --> 01:13:23,440
When I learned to multiply a matrix, seventh grade, like it's all easy.

1717
01:13:23,440 --> 01:13:26,520
You need more electrical engineering than you need high school math early.

1718
01:13:26,520 --> 01:13:27,520
Yeah.

1719
01:13:27,520 --> 01:13:30,120
Well, you need electrical engineering to like build the machines, but even that, like

1720
01:13:30,120 --> 01:13:34,640
these machines are simpler than the machines that have existed before, the compute stack

1721
01:13:34,640 --> 01:13:35,640
looks really nice.

1722
01:13:35,640 --> 01:13:39,160
So, you know, yeah, I just, I'm grateful that it's all happening and I get to understand

1723
01:13:39,160 --> 01:13:40,160
it be here.

1724
01:13:40,160 --> 01:13:41,160
Yeah.

1725
01:13:41,160 --> 01:13:42,160
Yeah.

1726
01:13:42,160 --> 01:13:45,520
Um, John Carmack mentioned there's about six insights we have left.

1727
01:13:45,520 --> 01:13:48,800
Do you have an intuition for what some of the paths people should be taking?

1728
01:13:48,800 --> 01:13:51,080
Obviously you're working on one.

1729
01:13:51,080 --> 01:13:54,600
What are some of the other branches of the tree that people should go under?

1730
01:13:54,600 --> 01:13:56,640
I don't think I'm working on one of the six insights.

1731
01:13:56,640 --> 01:13:59,000
I don't think tiny grads any one of the six insights.

1732
01:13:59,000 --> 01:14:04,000
Um, something I really like that Elon does and I try to take it from, uh, try to be inspired

1733
01:14:04,000 --> 01:14:12,120
by it is, um, look at the boring tunnel machine and ask how you can build a 10x cheaper one.

1734
01:14:12,120 --> 01:14:13,120
All right.

1735
01:14:13,120 --> 01:14:14,120
Look at the rocket.

1736
01:14:14,120 --> 01:14:15,120
How can I build a 10x cheaper one?

1737
01:14:15,120 --> 01:14:16,120
All right.

1738
01:14:16,120 --> 01:14:17,120
Look at the electric car and say, how can I build a 10x cheaper?

1739
01:14:17,120 --> 01:14:21,000
Like, cheaper or, you know, can go further or whatever, whatever, whatever, right?

1740
01:14:21,000 --> 01:14:22,520
You just do the straight up physics math, right?

1741
01:14:22,520 --> 01:14:26,440
Like, I'm trying to do the same thing with, with, uh, ML frameworks, right?

1742
01:14:26,440 --> 01:14:31,480
And in, in, in doing so, making sure that this stuff remains accessible, right?

1743
01:14:31,480 --> 01:14:36,800
You could imagine a world where if Google TPUs were actually the ultimate, if Google

1744
01:14:36,800 --> 01:14:39,400
TPUs were actually the best training things, I mean, actually, you know, I'm kind of grateful

1745
01:14:39,400 --> 01:14:40,400
for NVIDIA, right?

1746
01:14:40,400 --> 01:14:43,760
Like, because if Google TPUs were the ultimate, now you have this huge closed source compiler

1747
01:14:43,760 --> 01:14:49,880
in between XLA and, and the hardware and yeah, that's, uh, just a really bad thing.

1748
01:14:49,880 --> 01:14:53,680
So I mean, something that is somewhat upsetting, but the tiny group is it, is that it is trying

1749
01:14:53,680 --> 01:14:57,320
to prevent downside, but, uh, it's not all trying to prevent downside.

1750
01:14:57,320 --> 01:15:00,720
Like we're also building computers and we're going to build some awesome, powerful, cheap

1751
01:15:00,720 --> 01:15:02,920
computers, uh, along the way.

1752
01:15:02,920 --> 01:15:05,800
Uh, so no, I'm not really working directly on any of the six tricks.

1753
01:15:05,800 --> 01:15:10,000
I also think the six tricks are kind of going to be like, luck, I think it's going to be

1754
01:15:10,000 --> 01:15:13,680
like, you know, please tell me more about what covariate shift is and how that inspired

1755
01:15:13,680 --> 01:15:15,720
you to come up with batch normalization.

1756
01:15:15,720 --> 01:15:19,760
Please tell me more about why it's a transformer and it has a query, a key and a value, right?

1757
01:15:19,760 --> 01:15:23,200
Like Schmidt-Huber described it better and fast weights, you know?

1758
01:15:23,200 --> 01:15:27,600
Like, like, I mean, my theory about why transformers work have nothing to do with this attention

1759
01:15:27,600 --> 01:15:31,160
mechanism and just the fact that like it's semi-weight sharing, right?

1760
01:15:31,160 --> 01:15:34,880
Because the weight matrix is being generated on the fly, you can, you can like compress

1761
01:15:34,880 --> 01:15:35,880
the weight matrix.

1762
01:15:35,880 --> 01:15:36,880
Right?

1763
01:15:36,880 --> 01:15:40,440
Like this is what that, there's, there's an operation in the, in the transformer, which,

1764
01:15:40,440 --> 01:15:46,000
uh, like, and by the way, this is like Qualcomm's S and PE can't run transformers for this reason.

1765
01:15:46,000 --> 01:15:50,960
So most matrix multiplies in neural networks are weights times values, right?

1766
01:15:50,960 --> 01:15:55,040
There is, um, you know, when you get to the, the, the outer product in, in, uh, transformers,

1767
01:15:55,040 --> 01:15:58,480
well, it's weights times weight, it's a, it's values times values, right?

1768
01:15:58,480 --> 01:16:01,560
So S and PE like doesn't even support that operation, right?

1769
01:16:01,560 --> 01:16:05,360
So it's like that operation that gives the transformer its power, it has nothing to do

1770
01:16:05,360 --> 01:16:07,280
with the fact that it's attention, right?

1771
01:16:07,280 --> 01:16:10,320
And this is a funny like, but that is one of the six tricks, right?

1772
01:16:10,320 --> 01:16:14,320
Batch, like these norms are a trick, transformers are a trick.

1773
01:16:14,320 --> 01:16:15,320
Okay.

1774
01:16:15,320 --> 01:16:16,320
Six more.

1775
01:16:16,680 --> 01:16:21,880
Is there a reason why, so you couldn't talk, you talk about, uh, attention as weight compression,

1776
01:16:21,880 --> 01:16:24,400
um,

1777
01:16:24,400 --> 01:16:26,040
Compression is not exactly the right word.

1778
01:16:26,040 --> 01:16:29,080
What I mean is that the weights can change dynamically based on the context.

1779
01:16:29,080 --> 01:16:32,840
So it was this thing in pack eight in the hot air prize that I absolutely loved.

1780
01:16:32,840 --> 01:16:34,840
And I've never seen it again in neural networks and a really good trick.

1781
01:16:34,840 --> 01:16:35,840
Okay.

1782
01:16:35,840 --> 01:16:39,460
Imagine you have 256 weight sets for a layer, right?

1783
01:16:39,460 --> 01:16:44,000
And then you choose which of the weight sets you're loading in based on some context and

1784
01:16:44,000 --> 01:16:45,840
that context can come from another neural net, right?

1785
01:16:45,840 --> 01:16:51,280
So I have another one at which protect, projects, you know, 256 wide, one hot, do a soft max,

1786
01:16:51,280 --> 01:16:54,560
predict it, and then I actually load the weights and I can do this operation at both test time

1787
01:16:54,560 --> 01:16:55,560
and train time.

1788
01:16:55,560 --> 01:16:58,520
I can do this operation at both training and inference, and I load in the weights given

1789
01:16:58,520 --> 01:17:00,880
the context, right?

1790
01:17:00,880 --> 01:17:05,960
Like that is what transformers do, but transformers instead of having 256 discrete ones, it's

1791
01:17:05,960 --> 01:17:07,360
actually just that but continuous.

1792
01:17:07,360 --> 01:17:08,360
Yeah.

1793
01:17:08,360 --> 01:17:11,320
Um, which is funny that that was in language models and I just like, when I understood

1794
01:17:11,320 --> 01:17:14,200
that about transformers, I'm like, Oh, this is a real trick and why are they using the

1795
01:17:14,200 --> 01:17:15,200
word attention?

1796
01:17:15,720 --> 01:17:19,240
And today is actually the anniversary of attention is all you need.

1797
01:17:19,240 --> 01:17:20,240
What?

1798
01:17:20,240 --> 01:17:21,240
Oh, that's a day.

1799
01:17:21,240 --> 01:17:22,240
Today, six years ago.

1800
01:17:22,240 --> 01:17:23,240
Six years.

1801
01:17:23,240 --> 01:17:24,240
Six years.

1802
01:17:24,240 --> 01:17:25,240
Change the world.

1803
01:17:25,240 --> 01:17:26,240
Wow.

1804
01:17:26,240 --> 01:17:27,240
Well, there's one of your envelope tricks, right?

1805
01:17:27,240 --> 01:17:29,760
And you can easily write it on an envelope, you know, think about how you write out that

1806
01:17:29,760 --> 01:17:32,480
how many times have you written that because it's not in any libraries because it's like

1807
01:17:32,480 --> 01:17:34,440
all used a little differently each time.

1808
01:17:34,440 --> 01:17:35,440
Yeah.

1809
01:17:35,440 --> 01:17:39,880
If you just write out that exact same, you know, yeah, yeah.

1810
01:17:39,880 --> 01:17:41,720
You've name checked Elon a few times.

1811
01:17:41,720 --> 01:17:42,720
Yeah.

1812
01:17:42,760 --> 01:17:47,760
Think about both of you as systems, thinkers, input, output, think something in between.

1813
01:17:47,760 --> 01:17:48,760
Sure.

1814
01:17:48,760 --> 01:17:53,720
Um, what, what's different about your style versus his, um, Elon's fundamental science

1815
01:17:53,720 --> 01:17:56,240
for the world is physics minus information theory.

1816
01:17:56,240 --> 01:17:57,240
Huh.

1817
01:17:57,240 --> 01:17:59,000
But you, you do a lot of physics as well.

1818
01:17:59,000 --> 01:18:02,720
I mean, like, and Elon does a lot of information theory as well too.

1819
01:18:02,720 --> 01:18:06,560
But if the question is fundamentally that the difference maybe is expressed in what

1820
01:18:06,560 --> 01:18:08,840
your ambitions are, right?

1821
01:18:08,840 --> 01:18:12,400
Elon's ambitions may be like, go to Mars.

1822
01:18:12,400 --> 01:18:13,400
Right.

1823
01:18:13,400 --> 01:18:17,120
Go to Mars is the ultimate modern, modernist physics ambition.

1824
01:18:17,120 --> 01:18:18,120
Right.

1825
01:18:18,120 --> 01:18:19,120
It's a physics problem getting to Mars.

1826
01:18:19,120 --> 01:18:20,120
Right.

1827
01:18:20,120 --> 01:18:21,120
What are electric cars?

1828
01:18:21,120 --> 01:18:22,120
It's a physics problem.

1829
01:18:22,120 --> 01:18:23,120
Right.

1830
01:18:23,120 --> 01:18:24,120
Okay.

1831
01:18:24,120 --> 01:18:25,120
Now he's like pushing on the autonomy stuff.

1832
01:18:25,120 --> 01:18:28,880
You push a little on information theory, but fundamentally his dreams are physics based

1833
01:18:28,880 --> 01:18:29,880
dreams.

1834
01:18:29,880 --> 01:18:31,120
My dreams are information based dreams.

1835
01:18:31,120 --> 01:18:33,760
I want to live forever in virtual reality with my agri-girlfriend.

1836
01:18:33,760 --> 01:18:34,760
Right.

1837
01:18:34,760 --> 01:18:38,160
Those are, those are the aspirations of someone who, who, who accepts information theories

1838
01:18:38,160 --> 01:18:39,160
of course science.

1839
01:18:39,160 --> 01:18:40,680
So I think that's the main difference to me and him.

1840
01:18:40,680 --> 01:18:43,400
He has physics based aspirations and I have information based aspirations.

1841
01:18:43,400 --> 01:18:46,120
Very, very neat.

1842
01:18:46,120 --> 01:18:47,120
Mark Andreessen.

1843
01:18:47,120 --> 01:18:50,960
He is a, hi Mark, he's a listener.

1844
01:18:50,960 --> 01:18:54,320
He is heavily, he's a big proponent of effective accelerationism.

1845
01:18:54,320 --> 01:18:56,000
You've been a bit more critical.

1846
01:18:56,000 --> 01:18:59,760
Why do you say that EAC is not taken seriously by its adherents?

1847
01:18:59,760 --> 01:19:05,600
Oh, well, only the left takes ideology seriously.

1848
01:19:05,600 --> 01:19:07,000
Why is that?

1849
01:19:07,000 --> 01:19:08,760
Well, just as a fact.

1850
01:19:08,760 --> 01:19:10,040
It's just like, it's just like a fact.

1851
01:19:10,040 --> 01:19:11,040
Is the right more cynical?

1852
01:19:11,040 --> 01:19:12,040
Is that, is that what it is?

1853
01:19:12,040 --> 01:19:13,040
I don't know.

1854
01:19:13,040 --> 01:19:17,240
It's like, it's like the left actually manages to get energy around the ideologies.

1855
01:19:17,240 --> 01:19:18,240
Right.

1856
01:19:18,240 --> 01:19:22,360
Like, like, like there's a lot more, look, here you have, you have two effective altruists

1857
01:19:22,360 --> 01:19:24,200
named Sam going in front of Congress.

1858
01:19:24,200 --> 01:19:25,200
I only one of them is in jail.

1859
01:19:25,200 --> 01:19:27,200
Um, you know, it's, it's interesting.

1860
01:19:27,200 --> 01:19:29,680
They're both calling for regulation in those spectra spaces, right?

1861
01:19:29,680 --> 01:19:33,400
So SPF is definitely like kind of a wolf in sheep's clothing kind of, right?

1862
01:19:33,400 --> 01:19:37,600
Like he, he only adopted EAC or EA to walk in.

1863
01:19:37,600 --> 01:19:41,600
Sam Altman is a genuinely good guy who is not interested in power seeking for himself.

1864
01:19:41,600 --> 01:19:42,600
All right.

1865
01:19:42,600 --> 01:19:43,600
Okay.

1866
01:19:43,600 --> 01:19:44,600
We don't, we don't have to go.

1867
01:19:44,600 --> 01:19:45,600
Fair enough.

1868
01:19:45,600 --> 01:19:46,600
Fair enough.

1869
01:19:46,600 --> 01:19:50,040
Um, but, uh, no, EAC is not like, like you are not serious, right?

1870
01:19:50,040 --> 01:19:53,520
You are not actually a serious ideology.

1871
01:19:53,520 --> 01:19:57,840
You know, uh, Mark Andreessen, I like Mark Andreessen, but I think that like some of

1872
01:19:57,840 --> 01:20:01,200
his Twitter things are like, dude, you like just like, it's like, it's like someone who's

1873
01:20:01,200 --> 01:20:07,560
like 2019 who's like, eyes were opened about like the political world being not exact.

1874
01:20:07,560 --> 01:20:09,920
You mean all the people on the news were lying to me?

1875
01:20:09,920 --> 01:20:10,920
Yeah, bro.

1876
01:20:10,920 --> 01:20:11,920
They were lying to you.

1877
01:20:11,920 --> 01:20:14,120
Like, okay, we all figured this out five years ago.

1878
01:20:14,120 --> 01:20:15,120
Now what are you going to do about it?

1879
01:20:15,120 --> 01:20:16,560
I'm going to complain about it on Twitter.

1880
01:20:16,560 --> 01:20:17,560
Great.

1881
01:20:17,560 --> 01:20:18,560
And that's what EAC is.

1882
01:20:18,560 --> 01:20:25,120
Um, last and maybe most important, uh, why was Avatar 2 bad?

1883
01:20:25,120 --> 01:20:28,880
Oh, I have a whole, you can go on my blog.

1884
01:20:28,880 --> 01:20:30,840
I rewrote the script of Avatar 2.

1885
01:20:30,840 --> 01:20:34,400
I wrote a script that actually might make you feel something for the characters.

1886
01:20:34,400 --> 01:20:37,080
I killed Jake Sully in the first scene like you had to.

1887
01:20:37,440 --> 01:20:40,480
Do you really think his second story are topped his first one?

1888
01:20:40,480 --> 01:20:41,480
No, of course not.

1889
01:20:41,480 --> 01:20:44,280
You had to kill the guy and make the movie about the brothers, right?

1890
01:20:44,280 --> 01:20:48,080
And just that alone and realizing that like you could have kept the Titanic scene.

1891
01:20:48,080 --> 01:20:49,080
It would have been fine.

1892
01:20:49,080 --> 01:20:50,080
I didn't even take it out.

1893
01:20:50,080 --> 01:20:53,880
I left your Titanic scene, James Cameron, but I wrote you a story that so, you know,

1894
01:20:53,880 --> 01:20:57,080
you just, just, just, he needs ships to sink in water.

1895
01:20:57,080 --> 01:21:01,440
He needs, well, look, it's a great scene, but like the movie was just like, like the

1896
01:21:01,440 --> 01:21:02,440
Roman, never.

1897
01:21:02,440 --> 01:21:04,720
Great CGI, you know, let down by the writing, maybe.

1898
01:21:05,600 --> 01:21:09,080
Yeah, no, but like the CGI, like it was, it was a beautiful world.

1899
01:21:09,080 --> 01:21:10,560
And that's why like I care so much, right?

1900
01:21:10,560 --> 01:21:13,080
Like you don't hear me ranting about Pirates of the Caribbean 2 being a

1901
01:21:13,080 --> 01:21:15,600
terrible story because come on, what do you expect, man?

1902
01:21:15,960 --> 01:21:18,360
Like Johnny Depp is like, wow, I had a movie that made me rich.

1903
01:21:18,360 --> 01:21:19,360
I love this.

1904
01:21:20,800 --> 01:21:23,640
But this goes back to like the midpoint, you know, I think you were like, feels

1905
01:21:23,640 --> 01:21:25,120
like Chad Chippity wrote the movie.

1906
01:21:25,160 --> 01:21:27,400
And that's my worry a little bit.

1907
01:21:27,440 --> 01:21:29,280
It's like kind of converging towards that.

1908
01:21:29,480 --> 01:21:31,440
Oh, I look, Malak wrote the movie.

1909
01:21:33,440 --> 01:21:34,840
Sorry, I didn't want to interrupt you.

1910
01:21:34,840 --> 01:21:37,880
No, I closed, I closed a pull request two days ago.

1911
01:21:37,880 --> 01:21:39,560
I was like, was this written by Chad Chippity?

1912
01:21:39,560 --> 01:21:40,560
And I just closed it.

1913
01:21:40,760 --> 01:21:41,320
Like, you know what?

1914
01:21:41,320 --> 01:21:43,600
I honestly feel bad if you were a human who wrote this.

1915
01:21:44,200 --> 01:21:47,120
Like you're incapable of being more perfect.

1916
01:21:47,400 --> 01:21:51,800
But if you have a classifier running in my head that asks, you know, is this

1917
01:21:51,800 --> 01:21:53,400
an AI or is this a human?

1918
01:21:53,400 --> 01:21:59,160
Like, you know, the only way to deal with all this, like, like, oh, God, it's

1919
01:21:59,160 --> 01:22:00,160
like the worst possible.

1920
01:22:00,320 --> 01:22:04,720
Like, you know, people are like, like, like, how are you mad about like these

1921
01:22:04,720 --> 01:22:05,240
chatbots?

1922
01:22:05,240 --> 01:22:06,560
You're not mad about like Tesla.

1923
01:22:06,840 --> 01:22:09,840
Well, because if I don't want to buy a Tesla, I'll have to buy a Tesla.

1924
01:22:09,880 --> 01:22:11,800
And it won't really impact my life negatively.

1925
01:22:11,960 --> 01:22:14,880
But if I don't want to use a chatbot, it's still going to impact my life negatively.

1926
01:22:15,120 --> 01:22:19,000
All the amount of like personalized spam that now makes me spend more cycles on

1927
01:22:19,000 --> 01:22:22,600
my classifier to tell if it's spam or not, because you can now use AI's and

1928
01:22:22,600 --> 01:22:23,840
generate this so cheaply.

1929
01:22:24,480 --> 01:22:27,440
Like, no, I mean, we have to move to a model where everything's just a dollar,

1930
01:22:27,440 --> 01:22:27,600
right?

1931
01:22:27,600 --> 01:22:28,960
Like you want to send me an email, it's a dollar.

1932
01:22:29,200 --> 01:22:30,000
Like you guys wouldn't care.

1933
01:22:30,000 --> 01:22:30,800
No, my friends would care.

1934
01:22:30,800 --> 01:22:32,680
No one would care except the spammers.

1935
01:22:33,040 --> 01:22:33,280
Right.

1936
01:22:33,320 --> 01:22:34,680
Like we just got to move to those sort of models.

1937
01:22:36,440 --> 01:22:36,840
Awesome.

1938
01:22:37,080 --> 01:22:40,520
Um, one last message you want everyone to remember?

1939
01:22:41,680 --> 01:22:45,560
Uh, look, go, uh, go try Tanygrad.

1940
01:22:46,160 --> 01:22:51,400
Uh, I hope that we're a serious competitor to, to what's out there.

1941
01:22:51,760 --> 01:22:54,080
And then I want to, you know, I want to take it all the way.

1942
01:22:54,120 --> 01:22:57,480
We'll start with just building something for GPUs and then we'll start building

1943
01:22:57,480 --> 01:23:00,640
chips and we'll start building fabs and we'll start building silicon mines.

1944
01:23:00,640 --> 01:23:02,880
And we'll have the first self-reproducing robot using.

1945
01:23:03,120 --> 01:23:03,280
Yeah.

1946
01:23:03,280 --> 01:23:03,520
Okay.

1947
01:23:05,720 --> 01:23:06,280
All right, George.

1948
01:23:06,320 --> 01:23:07,440
Thank you so much for coming on.

1949
01:23:07,440 --> 01:23:08,400
Thank you for the big inspiration.

1950
01:23:08,440 --> 01:23:08,800
Thank you.

1951
01:23:09,080 --> 01:23:09,320
Thanks.

1952
01:23:10,120 --> 01:23:10,400
All right.

1953
01:23:13,280 --> 01:23:13,760
How was that?

1954
01:23:13,760 --> 01:23:17,280
We, uh, not, not quite like, but we hope to do something.

