WEBVTT

00:00.000 --> 00:05.080
Hey everyone, welcome to the Late In Space podcast.

00:05.080 --> 00:09.800
This is Swix, writer and editor of Late In Space, and Alessio is taking over with the

00:09.800 --> 00:10.800
intros.

00:10.800 --> 00:12.640
Alessio's partner and CT1 residents at Decibel Partners.

00:12.640 --> 00:19.640
Hey everyone, today we have GeoHot on the podcast, aka George Hots for the human name.

00:19.640 --> 00:22.720
Everybody knows George, so I'm not going to do a big intro, a couple things that people

00:22.720 --> 00:23.720
might have missed.

00:23.720 --> 00:28.040
So you were the first to unlock the iPhone, you traded the first ever unlocked iPhone for

00:28.040 --> 00:31.960
Nissan 350Z and three new iPhones.

00:31.960 --> 00:36.680
You were then one of the first people to break into the PS3 around arbitrary code.

00:36.680 --> 00:40.680
You got sued by Sony, you wrote a rap song to fight against that, which is still live

00:40.680 --> 00:44.200
on YouTube, which we're going to have on the show notes.

00:44.200 --> 00:49.520
Then you did not go to Tesla to build vision, and instead you started ComIi, which was an

00:49.520 --> 00:54.600
amazing engineering feat in itself until you got a season disease from the government

00:54.600 --> 00:59.200
to not put these things on the street, turned that into a research-only project.

00:59.200 --> 01:00.200
You know they're out there.

01:00.200 --> 01:04.800
Yeah, yeah, no, no, no, they're out there, but they're not a, you know, you market them

01:04.800 --> 01:06.800
as a research kind of like no warranty.

01:06.800 --> 01:10.040
Because I use the word DevKit, that's not about the government, that's nothing to do

01:10.040 --> 01:11.040
with the government.

01:11.040 --> 01:12.760
We offer a great one, you're warranty.

01:12.760 --> 01:17.680
The truth about that is it's gatekeeping.

01:17.680 --> 01:21.480
What's the difference between a DevKit and not a DevKit, nothing.

01:21.480 --> 01:25.480
What's the question of do you think it's for you, and if you think it's for you, buy it.

01:25.480 --> 01:26.480
It's a consumer product.

01:26.480 --> 01:27.480
We call it a DevKit.

01:27.480 --> 01:30.480
If you have a problem with that, it's not for you.

01:30.480 --> 01:31.480
Great framing.

01:31.480 --> 01:33.240
That's great insight.

01:33.240 --> 01:36.440
And then I was going through your broadcast to get to the day, you've heard this post

01:36.440 --> 01:41.440
about the hero's journey, and you link this thing called the portal story, which is kind

01:41.440 --> 01:46.280
of the set of stories in movies and books about people living this arbitrary life and

01:46.280 --> 01:50.760
then they run to this magic portals, kind of takes them into a new, very exciting life

01:50.760 --> 01:52.040
and dimension.

01:52.040 --> 01:55.800
When you've heard that post, you talked about TinyGrad, which is one of the projects you're

01:55.800 --> 01:56.800
working on today.

01:56.800 --> 01:59.960
And you mentioned this is more of a hobby, something that is not going to change the

01:59.960 --> 02:00.960
course of history.

02:00.960 --> 02:04.600
Obviously, you're not going full speed into it, so we will learn more about what was the

02:04.600 --> 02:08.000
portal that you run into to get here.

02:08.000 --> 02:13.880
Well, what you realize is, you know what made me realize that I absolutely had to do the

02:13.880 --> 02:14.880
company?

02:14.880 --> 02:16.960
Seeing Sam Maltlin go in front of Congress.

02:16.960 --> 02:18.960
Why?

02:18.960 --> 02:22.120
What are the odds they nationalize NVIDIA?

02:22.120 --> 02:26.760
What are the odds that large organizations in the government, but of course I repeat

02:26.760 --> 02:32.560
myself, decide to try to clamp down on accessibility of ML compute?

02:32.560 --> 02:39.880
I want to make sure that can't happen structurally, so that's why I realize that it's really important

02:39.880 --> 02:40.880
that I do this.

02:40.880 --> 02:44.960
And actually, from a more practical perspective, I'm working with NVIDIA and Qualcomm to buy

02:44.960 --> 02:45.960
chips.

02:45.960 --> 02:48.800
NVIDIA has the best training chips, Qualcomm has the best inference chips.

02:48.800 --> 02:52.160
Working with these companies is really difficult, so I'd like to start another organization

02:52.160 --> 02:57.640
that eventually in the limit either works with people to make chips or makes chips itself

02:57.640 --> 03:01.880
and makes them available to anybody.

03:01.880 --> 03:06.120
You share kind of three core thesis to Tiny Core, maybe we can dive into each of them.

03:06.120 --> 03:12.680
So, XLA, PrimeTorch, those are the complex instruction system, TinyGrad is the restricted

03:12.680 --> 03:17.080
instruction system, so you're kind of focused on, again, TinyGrad being small, not being

03:17.080 --> 03:21.640
over complicated and trying to get as close to the DSP as possible in a way where it's

03:21.640 --> 03:22.640
at more.

03:22.640 --> 03:26.240
Well, it's a very clear analogy from how processors developed.

03:26.240 --> 03:31.120
So a lot of processors back in the day were CISC, complex instruction set, system 360

03:31.120 --> 03:33.160
and then x86.

03:33.160 --> 03:35.840
Then this isn't how things stayed.

03:35.840 --> 03:41.320
They went to now the most common processors on ARM and people are excited about RISC

03:41.320 --> 03:42.320
5.

03:42.320 --> 03:45.000
RISC 5 is even less complex than ARM.

03:45.000 --> 03:49.520
No one is excited about CISC processors anymore, they're excited about RISC Reduce Instruction

03:49.520 --> 03:50.920
Set processors.

03:50.920 --> 03:57.520
So TinyGrad is we're going to make a RISC offset for all ML models and yeah, it can

03:57.520 --> 04:03.400
run all ML models with basically 25 instead of the 250 of XLA or PrimeTorch.

04:03.400 --> 04:05.160
So about 10X less complex.

04:05.160 --> 04:06.160
Yep.

04:06.160 --> 04:08.760
You talk a lot about existing AI chips.

04:08.760 --> 04:12.840
You said if you can write a fast ML framework for GPUs, you just can't write one for your

04:12.840 --> 04:13.840
own chip.

04:13.840 --> 04:17.400
So that's another one of your core insights, I don't know if you want to expand on that.

04:17.400 --> 04:18.400
Yeah.

04:18.400 --> 04:19.560
I mean, your chip is worse, right?

04:19.560 --> 04:22.680
There's no way the chip that you're going to tape out, especially on the first try is

04:22.680 --> 04:25.760
going to be easier to use than an AMD GPU, right?

04:25.760 --> 04:28.400
And yet there's no good stack for AMD GPUs.

04:28.400 --> 04:31.480
So why do you think you can make one for your chip?

04:31.480 --> 04:32.480
You can't.

04:32.480 --> 04:33.480
Right?

04:33.480 --> 04:37.480
The only company, there's one other company, aside from Nvidia, who's succeeded at all

04:37.480 --> 04:38.800
at making training chips.

04:38.800 --> 04:40.800
What company?

04:40.800 --> 04:42.800
AMD.

04:42.800 --> 04:43.800
Intel?

04:43.800 --> 04:44.800
No.

04:44.800 --> 04:45.800
No.

04:45.800 --> 04:46.800
No.

04:46.800 --> 04:47.800
I've never trained.

04:47.800 --> 04:48.800
Who's trained a model on AMD or Intel?

04:48.800 --> 04:49.800
Nobody on AMD.

04:49.800 --> 04:50.800
Cerebrus.

04:50.800 --> 04:54.120
Cerebrus, I'm talking about, you might know some startups who trained models on these

04:54.120 --> 04:55.120
chips.

04:55.120 --> 04:59.480
I'm surprised no one immediately gets this because there is one other chip, aside from

04:59.480 --> 05:02.240
Nvidia, that normal people have actually used for training.

05:02.240 --> 05:03.880
That's a real neural engine?

05:03.880 --> 05:04.880
No.

05:04.880 --> 05:05.880
Used for training.

05:05.880 --> 05:06.880
No.

05:06.880 --> 05:08.880
You can only buy them in the cloud.

05:08.880 --> 05:09.880
Oh, TPU.

05:09.880 --> 05:10.880
Exactly.

05:10.880 --> 05:11.880
Yeah.

05:11.960 --> 05:13.800
So, mid-journey is trained on TPU.

05:13.800 --> 05:14.800
Right?

05:14.800 --> 05:19.080
Like, a lot of startups do actually train on TPUs, and they're the only other successful

05:19.080 --> 05:21.280
training chip, aside from Nvidia.

05:21.280 --> 05:26.240
But what's unique about Google is that they also wrote their own ML framework, right?

05:26.240 --> 05:30.040
And if you can't write your own ML framework that is performant on Nvidia, there's no way

05:30.040 --> 05:32.720
you're going to make it performant on your...

05:32.720 --> 05:35.960
And they started from TensorFlow, and then they made the chip after.

05:35.960 --> 05:36.960
Yeah.

05:36.960 --> 05:37.960
Exactly.

05:37.960 --> 05:38.960
Exactly.

05:38.960 --> 05:39.960
And you have to do it in that direction.

05:39.960 --> 05:44.560
Because you're going to end up, you know, a service, what are those things, a million

05:44.560 --> 05:45.560
dollars?

05:45.560 --> 05:46.560
I've never seen a service.

05:46.560 --> 05:49.640
No one's ever like, oh, I trained my model on a service.

05:49.640 --> 05:52.480
Most people are like, I trained my model on TPUs.

05:52.480 --> 05:56.080
Some people, 20%, are like, I trained my model on TPUs.

05:56.080 --> 05:57.080
Yeah.

05:57.080 --> 06:01.480
And then the third one, which is the one that surprised me the most is, through incompleteness,

06:01.480 --> 06:02.480
it's harmful.

06:02.480 --> 06:03.480
It should be avoided.

06:03.480 --> 06:09.240
It made sense once I read it, but maybe tell us a bit or more about how you got there.

06:09.600 --> 06:10.600
Okay.

06:10.600 --> 06:18.800
So, CPUs devote tons of their silicon and power to things like reorder buffers and speculative

06:18.800 --> 06:21.440
execution and branch predictors.

06:21.440 --> 06:25.560
And the reason that you need all these things is because at compile time, you can't understand

06:25.560 --> 06:27.400
how the code's going to run.

06:27.400 --> 06:28.800
This is Rice's theorem.

06:28.800 --> 06:31.120
This is the Halting problem, and it's limit.

06:31.120 --> 06:33.160
And this is not like, oh, the Halting problem is theoretical.

06:33.160 --> 06:34.160
No, no, no, no.

06:34.160 --> 06:35.360
It's actually very real.

06:35.360 --> 06:37.320
Does this branch get taken or not?

06:37.320 --> 06:38.320
It depends on X.

06:38.320 --> 06:39.320
Where does X come from?

06:39.320 --> 06:41.000
I forget it, right?

06:41.000 --> 06:44.400
But no branches depend on X in a neural net.

06:44.400 --> 06:46.120
Every branch is a static loop.

06:46.120 --> 06:50.240
Like if you're doing a matrix multiple, it's a static loop over the inner dimension.

06:50.240 --> 06:51.960
And neural networks are even better.

06:51.960 --> 06:53.960
No loads even depend on X, right?

06:53.960 --> 06:56.840
So with a GPU shader, right, you're like, your load might depend on which texture you're

06:56.840 --> 06:58.320
actually loading into RAM.

06:58.320 --> 07:01.440
But with a neural network, your load is, well, I load that way.

07:01.440 --> 07:02.440
Why?

07:02.440 --> 07:04.760
Well, because I load that way the other million times I ran the same net.

07:04.760 --> 07:09.440
Every single time you run the net, you do the exact same set of loads, stores, and arithmetic.

07:09.440 --> 07:12.320
The only thing that changes is the data.

07:12.320 --> 07:18.680
And this gives you a very powerful ability to optimize that you can't do with CPU-style

07:18.680 --> 07:22.640
things, which have branches, and even GPU-style things, which have loads and stores.

07:22.640 --> 07:24.040
Oh, that makes sense.

07:24.040 --> 07:28.080
Well, GPUs, if you want GPU-style stuff, you have load based on X, you now need a cache

07:28.080 --> 07:33.680
hierarchy, and not an explicit cache hierarchy, an implicit cache hierarchy, with eviction

07:33.680 --> 07:36.800
policies that are hard-coded into the CPU.

07:36.800 --> 07:41.760
You start doing all this stuff, and you're never going to get theoretically good performance.

07:41.760 --> 07:43.720
Again, I don't think there's 100X.

07:43.720 --> 07:46.760
Some startups will talk about 100X, and they'll talk about absolutely ridiculous things like

07:46.760 --> 07:49.080
clockless computing or analog computing.

07:49.080 --> 07:51.920
Okay, analog computing just won't work.

07:51.920 --> 07:58.080
And clockless computing, sure, it might work in theory, but your ETA tools are...

07:58.080 --> 08:02.760
Maybe AIs will be able to design clockless chips, but not humans.

08:02.880 --> 08:06.920
What actually is practical is changing cache hierarchies and removing branch predictors

08:06.920 --> 08:08.440
and removing warp schedulers.

08:08.440 --> 08:11.880
GPUs spend tons of power on warp scheduling, because we have to hide the latency from the

08:11.880 --> 08:12.880
memory.

08:12.880 --> 08:16.040
We'll have to hide the latency if everything's statically scheduled.

08:16.040 --> 08:20.120
What do you think people are still hanging on to during complete?

08:20.120 --> 08:22.640
Well, because it's really easy.

08:22.640 --> 08:24.040
Turning complete is just really easy.

08:24.040 --> 08:30.480
It's really easy to just be so nice if I could do an if statement here and actually branch

08:30.480 --> 08:32.080
the code.

08:32.080 --> 08:37.600
So it requires a lot more thought to do it without turning completeness.

08:37.600 --> 08:40.800
And would this be qualitatively different than TPUs?

08:40.800 --> 08:42.120
So TPUs are a lot closer.

08:42.120 --> 08:43.120
Yeah.

08:43.120 --> 08:46.200
TPUs are a lot closer to what I'm talking about than like CUDA.

08:46.200 --> 08:47.200
Okay.

08:47.200 --> 08:48.200
So what is CUDA?

08:48.200 --> 08:53.760
Well, CUDA is a C-like language, which compiles to an LLVM like IR, which compiles to PTX,

08:53.760 --> 08:57.240
which compiles to SAS, which are all turned complete.

08:57.240 --> 08:58.920
TPUs are much more like this, yeah.

08:58.920 --> 09:01.280
Their memory is pretty statically managed.

09:01.640 --> 09:04.240
I did some reverse engineering on the TPU.

09:04.240 --> 09:06.160
It's published in TinyGrad.

09:06.160 --> 09:09.680
It has like a VLIW instruction and it runs them.

09:09.680 --> 09:10.680
So it's similar.

09:10.680 --> 09:12.360
I think the TPUs have a few problems.

09:12.360 --> 09:15.440
I think systolic arrays are the wrong choice.

09:15.440 --> 09:18.000
Systolic array, I think they have systolic arrays because that was the guy's PhD.

09:18.000 --> 09:19.000
Right.

09:19.000 --> 09:20.000
And of course, Amazon makes...

09:20.000 --> 09:21.000
Jake, could you summarize systolic arrays for this?

09:21.000 --> 09:22.000
Systolic arrays are just...

09:22.000 --> 09:23.000
Okay.

09:23.000 --> 09:27.480
So basically you have like, this is a way to do matrix multiplication, think of a grid

09:27.480 --> 09:31.640
of malax and then the grid can multiply and then shift, multiply, then shift, multiply,

09:31.640 --> 09:32.640
then shift.

09:32.640 --> 09:37.640
And they are very power efficient, but it becomes hard to schedule a lot of stuff on

09:37.640 --> 09:43.000
them if you're not doing like perfectly sized dense matrix multiplies, which you can argue,

09:43.000 --> 09:48.400
well, design your models to use perfectly sized dense matrix multiplies, sure, but it's

09:48.400 --> 09:49.400
just...

09:51.400 --> 09:54.920
No, but thanks for indulging on these explanations.

09:54.920 --> 09:59.880
I think we need to keep our audience along with us by pausing every non-dent to explain

09:59.880 --> 10:00.880
key terms.

10:00.880 --> 10:05.800
You know, when I say explain a systolic array, I just immediately get a picture in my head

10:05.800 --> 10:07.760
of like tilting a matrix and shifting it.

10:07.760 --> 10:09.160
It's hard to kind of explain.

10:09.160 --> 10:10.160
Yeah.

10:10.160 --> 10:13.600
We'll do some videos so you have your hand actions and we edit it in visuals.

10:13.600 --> 10:14.600
Yeah.

10:14.600 --> 10:17.560
There's some great graphics that just show you, oh, so that's what a systolic array is,

10:17.560 --> 10:22.480
but it's a malax shift machine that looks kind of different from the typical like APU

10:22.480 --> 10:23.480
sort of machine.

10:23.480 --> 10:28.040
Sorry, ALU sort of machine, I think the right answer is something that looks more like queues

10:28.040 --> 10:32.760
that feed into ALUs and then you can like prefetch the loads from the memory, put it

10:32.760 --> 10:39.440
a bunch of queues and then the queue is just like and feeds into another queue over here.

10:39.440 --> 10:42.360
But yeah, but that's not even the main problem with TPUs.

10:42.360 --> 10:45.440
The main problem with TPUs is that they're closed source, not only is the chip closed

10:45.440 --> 10:51.560
source, but all of XLA is open source, but the XLA to TPU compiler is a 32 megabyte binary

10:51.560 --> 10:54.400
blob called libTPU on Google's cloud instances.

10:54.400 --> 10:58.520
It's all closed source, it's all hidden stuff and you know, well, there's a reason Google

10:58.520 --> 11:02.200
made a closed source, Amazon made a clone of the TPU, it's called Inferencia, or they

11:02.200 --> 11:05.480
have some other name for it, a training, training, yeah, yeah, yeah, yeah, yeah, and you look

11:05.480 --> 11:07.280
as a clone of the TPU.

11:07.280 --> 11:12.120
It just software doesn't work though, and the Google software at least kind of works.

11:12.120 --> 11:14.400
So those are kind of like the three quarter thesis.

11:14.400 --> 11:18.720
The first thing you're working on that you've been working on is TinyGrad, and one of your

11:18.720 --> 11:22.040
Twitch streams, he said, is the best thing you've ever written.

11:22.040 --> 11:26.840
Yeah, tell us a bit more about that creation.

11:26.840 --> 11:30.800
For a long time, TinyGrad had a hard limit of 1,000 lines of code.

11:30.800 --> 11:36.080
And what this would force you to do is really make sure you were not wasting lines.

11:36.080 --> 11:40.600
I got rid of the restriction because it became a little code golfy at the end, but once like

11:40.600 --> 11:47.400
the core framework of TinyGrad was there, in those 1,000 lines, it's not huge now, it's

11:47.480 --> 11:50.680
like 2,800 lines now, it's still very readable.

11:50.680 --> 11:56.440
But like the core framework, the ideas are expressed with no boilerplate.

11:56.440 --> 12:00.000
If you go read PyTorch, you know, PyTorch, I think it's actually pretty good code.

12:00.000 --> 12:05.480
I think Facebook's pretty good, but there's so much boilerplate.

12:05.480 --> 12:10.080
Go in PyTorch and try to track down how an LGU actually works.

12:10.080 --> 12:11.080
Just a lot of distractions.

12:11.080 --> 12:16.840
Oh, you're going to be diving down a long stack from Python to C to custom libraries

12:16.880 --> 12:19.920
to dispatchers to, and then I don't even know how to read TensorFlow.

12:19.920 --> 12:24.480
I don't even know where's an LGU in TensorFlow, nobody knows.

12:24.480 --> 12:26.720
Someone at Google knows maybe.

12:26.720 --> 12:31.640
Google as an organism, I don't know if anyone individually Google knows.

12:31.640 --> 12:35.440
What are like the important ergonomics like for a developer as you think about designing

12:35.440 --> 12:36.920
the TinyGrad API?

12:36.920 --> 12:40.720
So the TinyGrad front end looks very similar to PyTorch.

12:40.720 --> 12:44.760
There's an even higher level front end you can use for TinyGrad, which is just Onyx.

12:44.840 --> 12:49.000
We have better support for Onyx than Core ML does, and we're going to have, I think

12:49.000 --> 12:50.720
we're going to pass Onyx Runtime soon, too.

12:50.720 --> 12:53.080
I like people think Onyx Runtime, that's a gold standard for Onyx.

12:53.080 --> 12:54.080
No, you can do better.

12:54.080 --> 12:55.080
Pass them in what?

12:55.080 --> 12:56.080
Specifically?

12:56.080 --> 12:57.400
Test, compliance tests.

12:57.400 --> 13:01.880
So Onyx has a big set of compliance tests that you can check out.

13:01.880 --> 13:04.880
And we have the running in TinyGrad, and there's some failures.

13:04.880 --> 13:08.080
We're below Onyx Runtime, but we're beyond Core ML.

13:08.080 --> 13:11.480
So that's where we are on Onyx support now, but we will pass, we will pass Onyx Runtime

13:11.480 --> 13:16.240
soon, because it becomes very easy to add ops, because of how you don't need to do anything

13:16.240 --> 13:17.240
at the lower levels.

13:17.240 --> 13:20.640
You just do it at this very high level, and TinyGrad compiles it to something that's

13:20.640 --> 13:23.160
fast using these minimal ops.

13:23.160 --> 13:28.000
You can write, I mean, most concretely, what TinyGrad can do that PyTorch can't really

13:28.000 --> 13:33.840
do is if you have something like A times B plus C, if you write that in naive PyTorch,

13:33.840 --> 13:38.800
what it's going to do on the GPU is, well, read A, read B in a kernel, and then store

13:38.800 --> 13:45.440
A times B in memory, and then launch another kernel to do A times B plus C, got to do those

13:45.440 --> 13:46.440
loads for memory.

13:46.440 --> 13:49.640
I know I did a whole extra round trip to memory that I just didn't have to do.

13:49.640 --> 13:52.560
You're like, yeah, but you can use the TorchJIT and it corrects this.

13:52.560 --> 13:59.200
Yeah, for that one example, for that one example of MULAC, but oh, now you did three multiplies,

13:59.200 --> 14:02.040
six multiplies, right?

14:02.040 --> 14:04.160
It won't compile arbitrary code.

14:04.640 --> 14:09.720
If you looked into the other approaches like PyTorch Lightning to accelerate PyTorch itself.

14:09.720 --> 14:14.840
Well, PyTorch Lightning, my understanding is it's mostly a framework around PyTorch, right?

14:14.840 --> 14:18.440
PyTorch Lightning is not going to fix this fundamental problem of I multiply six tensors

14:18.440 --> 14:19.440
together.

14:19.440 --> 14:23.480
Why is it going to memory any more than a single read from each and a single write to the output?

14:23.480 --> 14:24.480
Okay.

14:24.480 --> 14:31.040
There are lower level things in PyTorch that are not exactly sure what Dynamo does, but

14:31.040 --> 14:34.120
I know they're generating some Triton stuff, which is going to generate the kernels on

14:34.120 --> 14:35.120
the fly.

14:35.120 --> 14:39.120
But you know, PyTorch Lightning is at a higher level of abstraction.

14:39.120 --> 14:41.600
So TinyGrid's front-end stuff looks like PyTorch.

14:41.600 --> 14:42.600
I made a few tweaks.

14:42.600 --> 14:43.840
There's a few things I don't like about PyTorch.

14:43.840 --> 14:45.440
Why is ReLU a class?

14:45.440 --> 14:46.440
Oh, really?

14:46.440 --> 14:49.000
What was the state?

14:49.000 --> 14:50.440
You make a class and there's a state.

14:50.440 --> 14:54.160
Everything should just be Torch Functional and ReLU, but just dot ReLU on the tensor.

14:54.160 --> 14:59.560
Also there's things in Torch where you have to do tensor dot and not a tensor dot.

14:59.560 --> 15:00.560
Right?

15:00.600 --> 15:01.600
Why?

15:01.600 --> 15:02.600
Why are these things?

15:02.600 --> 15:07.640
It just shows an API that's not perfectly refined, but when you're doing stuff TinyGrid

15:07.640 --> 15:11.560
style where you don't have lines, well, it has to work this way because even the lines

15:11.560 --> 15:17.200
to express the, well, you can't use the where operator unless, and the where operator in

15:17.200 --> 15:18.200
PyTorch.

15:18.200 --> 15:22.400
Why is it a true case, condition, false case?

15:22.400 --> 15:24.400
The worst, that's how Python expresses ifs.

15:24.400 --> 15:25.400
It's disgusting.

15:25.400 --> 15:29.680
Where operators are much nicer, it should be, I can do my like, a less than zero dot

15:29.680 --> 15:33.120
where, a comma one, right?

15:33.120 --> 15:35.480
The very pandas like API.

15:35.480 --> 15:40.200
Yeah, yeah, yeah, yeah, yeah, it's just, it's some, it looks like Torch numpy pandas.

15:40.200 --> 15:41.600
They're all very similar.

15:41.600 --> 15:45.320
I tried to take like the cleanest subset of them and express them, but like I said, you

15:45.320 --> 15:47.400
can also interact with it using Onyx.

15:47.400 --> 15:48.400
Yeah.

15:48.400 --> 15:51.600
But I have a rewrite of stable diffusion, I have a rewrite of llama, I have a rewrite

15:51.600 --> 15:52.600
of whisper.

15:52.600 --> 15:53.600
You can look at them.

15:53.600 --> 15:54.600
They're shorter than the Torch version than I think they're cleaner.

15:54.600 --> 15:55.600
They stream them all.

15:55.600 --> 15:56.600
Yeah.

15:56.600 --> 15:57.600
Very nice.

15:57.600 --> 16:03.000
Laziness is kind of the other important concept that you're leveraging to do, operation fusing.

16:03.000 --> 16:05.840
Yeah, talk a bit more about that.

16:05.840 --> 16:13.880
So yeah, you have, you have basically like a few different like models for compute.

16:13.880 --> 16:14.880
The simplest one's eager.

16:14.880 --> 16:21.120
All right, the simplest one is eager as soon as the interpreter or sees A times B, it actually

16:21.120 --> 16:23.720
dispatches A times B, right?

16:23.720 --> 16:30.320
Then you have graph like TensorFlow, which will put A times B into a graph and then we'll

16:30.320 --> 16:36.280
do absolutely nothing until you actually compile the graph at the end.

16:36.280 --> 16:40.080
I like this third choice, just somewhere in the middle, laziness.

16:40.080 --> 16:42.800
Laziness is you don't know when the ops are going to dispatch and don't worry about that.

16:42.800 --> 16:44.560
You don't have to worry about this as a programmer.

16:44.560 --> 16:46.120
You just write out all your stuff.

16:46.120 --> 16:50.600
And then when you actually type dot numpy, it'll be ready by the time you copy the thing

16:50.600 --> 16:51.600
back to CPU.

16:51.600 --> 16:57.760
Or you can do dot realize and it will actually like force that tensor to be allocated in RAM.

16:57.760 --> 17:01.640
But yeah, a lot of times, right, like, and if you think about it, PyTorch is kind of

17:01.640 --> 17:04.960
lazy in a way, but they didn't extend the paradigm far enough, right?

17:04.960 --> 17:09.600
When I do A times B in PyTorch, it's going to launch a CUDA kernel to do A times B, but

17:09.600 --> 17:12.120
it's not going to wait for that CUDA kernel to complete.

17:12.120 --> 17:13.840
So you're getting the worst possible world.

17:13.840 --> 17:18.120
You're getting the same laziness, but you also can't get fusion because PyTorch doesn't

17:18.120 --> 17:21.560
know that I'm then going to do plus C. There's no way for it to be like, whoa, whoa, whoa,

17:21.560 --> 17:25.200
don't launch that CUDA kernel, whoa, just do this one too, right?

17:25.200 --> 17:30.160
You can kind of like, again, this stuff, PyTorch is working on this.

17:30.160 --> 17:31.720
And you know, it's a little bit harder.

17:31.720 --> 17:34.920
Like in comma, I felt like I was competing against a lot of idiots here.

17:34.920 --> 17:40.960
I'm competing against, you know, smart, smart, very smart people who made, yeah, who've made

17:40.960 --> 17:42.760
some, I think, different trade offs, right?

17:42.760 --> 17:46.440
We've made some different trade offs, whereas if you're trying to build something that is

17:46.440 --> 17:50.840
just straight up good on Nvidia, and we have a lot of people in complexity to throw at

17:51.120 --> 17:52.800
PyTorch made a lot of the right choices.

17:52.800 --> 17:57.160
I'm trying to build something that manages complexity, like you can always make your

17:57.160 --> 17:58.400
software do more.

17:58.400 --> 18:02.160
The magic is when you can make your software do more without adding complexity, right?

18:02.160 --> 18:06.240
Because, you know, complex things eventually collapse under the wind.

18:06.240 --> 18:07.680
So it's kind of that.

18:07.680 --> 18:09.520
How does fusing actually work?

18:09.520 --> 18:12.360
Like TensorFlow actually collapsed under it.

18:12.360 --> 18:15.400
It's kind of what happened, right?

18:15.400 --> 18:17.280
How does fusing actually work?

18:17.280 --> 18:23.600
So yeah, there's this thing called lazy.py, and when you do like a times b, that's, it's

18:23.600 --> 18:27.080
put into a graph, but it's a very local graph.

18:27.080 --> 18:30.520
There's no global graph optimizations, and even this can change, right?

18:30.520 --> 18:34.920
Again, like the programming model for TinyGrad does not preclude eagerness, right?

18:34.920 --> 18:37.240
Laziness is not guaranteed laziness.

18:37.240 --> 18:39.200
It's just going to try its best.

18:39.200 --> 18:41.800
So you put in a times b, and that's a binary app, right?

18:41.800 --> 18:44.560
And then you put in a times b, like that's a node in the graph.

18:44.560 --> 18:46.680
That's a virtual node, because it's not realized yet.

18:46.680 --> 18:47.680
Plus c.

18:47.680 --> 18:50.880
Okay, here's a new node, which takes the c tensor in here and takes the output of a

18:50.880 --> 18:51.880
times b.

18:51.880 --> 18:53.280
It's like, whoa, wait, there's two binary ops.

18:53.280 --> 18:55.280
Okay, we'll just fuse those together, okay?

18:55.280 --> 18:56.280
Here I have a kernel.

18:56.280 --> 18:58.280
This kernel has a, b, and c as inputs.

18:58.280 --> 19:03.760
It does a times b plus c in the local registers, and then outputs that to memory.

19:03.760 --> 19:07.360
And you can graph dot one in TinyGrad.

19:07.360 --> 19:11.880
Another like amazing thing that TinyGrad has that I've not seen in any other framework

19:11.880 --> 19:13.520
is two things.

19:13.520 --> 19:16.080
Graph dot one, graph equals one, which is an environment variable.

19:16.080 --> 19:18.280
It will output a complete graph of all the operations.

19:18.280 --> 19:21.600
People are like, oh, you can use PyTorch, export it to Onyx, and use Netron.

19:21.600 --> 19:24.680
Yeah, you can, but like, what?

19:24.680 --> 19:25.680
That's not what's real.

19:25.680 --> 19:29.840
Right, graph dot one will show you the actual kernels that were dispatched to the GPU.

19:29.840 --> 19:35.440
You can also type debug equals two, which will print those kernels out in your command

19:35.440 --> 19:41.360
line, and it will tell you the exact number of flops and the exact number of memory accesses

19:41.360 --> 19:42.360
in each kernel.

19:43.000 --> 19:47.040
So you can immediately see, wait a second, okay, this kernel used this many flops.

19:47.040 --> 19:48.540
This was the gigaflops.

19:48.540 --> 19:51.240
This is how many bytes it read, and this was the gigabytes per second.

19:51.240 --> 19:56.120
And then you can profile without having to like, okay, I mean in theory in PyTorch, sure,

19:56.120 --> 19:57.800
use the NVIDIA insight profiler.

19:57.800 --> 19:58.800
No one does that.

19:58.800 --> 20:03.120
No one does, of course, because it's so difficult, right, like, like, actually NVIDIA used to

20:03.120 --> 20:06.960
a pre, pre, I think kuda nine was the last one that had it.

20:06.960 --> 20:11.160
They had a command line one, but now it's like, okay, I'm going to generate this blob,

20:11.160 --> 20:15.480
use this NVIDIA GUI tool to convert it into a Chrome trace, and then load it, and yeah,

20:15.480 --> 20:16.480
no one does that, right?

20:16.480 --> 20:20.320
I'll just type debug equals two in any tiny grad model, and it will show you all the kernels

20:20.320 --> 20:23.680
that it launches, and the efficiency of each kernel, basically.

20:23.680 --> 20:29.720
Yeah, this is something that John Karmic has often commented about, is that when you code,

20:29.720 --> 20:34.000
you need to build in your instrumentation or observability right into that.

20:34.000 --> 20:38.360
I wonder if whatever John is working on, he's adopting this style, and maybe you can sort

20:38.360 --> 20:45.760
of encourage it by, like, I don't know, naming it and coining it as a certain kind of debugging

20:45.760 --> 20:46.760
style.

20:46.760 --> 20:51.840
If he would like to start contributing to tiny grad, I'd be, I don't know, I've chatted

20:51.840 --> 20:52.840
with a few times.

20:52.840 --> 20:53.840
I'm not really sure what his company's doing.

20:53.840 --> 20:54.840
Yeah.

20:54.840 --> 20:59.880
I think it's all, I think it's pretty, but no, I mean, hopefully, like, we get tiny grad

20:59.880 --> 21:04.280
to a point where people actually want to start using it.

21:04.280 --> 21:09.120
So tiny grad right now is uncompetitive on, it's uncompetitive on NVIDIA, and it's uncompetitive

21:09.120 --> 21:10.120
on x86.

21:10.120 --> 21:12.640
And specifically, what do you care about when you say uncompetitive?

21:12.640 --> 21:13.640
Speed.

21:13.640 --> 21:14.640
Okay.

21:14.640 --> 21:15.640
Shut up, speed.

21:15.640 --> 21:16.640
It's correct.

21:16.640 --> 21:17.640
The correctness is there.

21:17.640 --> 21:21.080
The correctness for both forwards and backwards passes is there, but on NVIDIA, it's about

21:21.080 --> 21:24.600
5x slower than PyTorch right now, like 5x, wow, this is, this is unsurmountable.

21:24.600 --> 21:27.840
No, there's reasons it's 5x slower, and I can go through how we're going to make it

21:27.840 --> 21:30.840
faster, and it used to be, you know, 100x slower, so, you know, we're making progress,

21:30.840 --> 21:35.880
but there's one place where it actually is competitive, and that's Qualcomm GPUs.

21:35.880 --> 21:40.320
So tiny grad is used to run the model in OpenPilot, like right now, it's been live in production

21:40.320 --> 21:47.400
now for six months, and tiny grad is about 2x faster on the GPU than Qualcomm's library.

21:47.400 --> 21:49.120
Why specifically Qualcomm?

21:49.120 --> 21:53.400
Well, because we have Qualcomm, we use Qualcomm in the Comma devices.

21:53.400 --> 21:56.920
Oh, I mean, like, what makes, what makes, what about Qualcomm architecture?

21:56.920 --> 21:58.200
Oh, what makes it doable?

21:58.200 --> 21:59.200
Yeah.

21:59.200 --> 22:02.720
Well, Qualcomm has spent how many millions of man-hours to make NVIDIA fast, and Qualcomm

22:02.720 --> 22:06.360
has a team of 10 Qualcomm engineers, okay, well, who can I be here?

22:06.360 --> 22:11.240
Like, what I propose with, what I propose with tiny grad is that developer efficiency

22:11.240 --> 22:17.680
is much higher, but even if I have 10x higher developer efficiency, I still lose on NVIDIA,

22:17.680 --> 22:18.680
right?

22:18.680 --> 22:20.560
You know, okay, I didn't put 100,000 man-hours into it, right?

22:20.560 --> 22:24.320
If they put a million, like, like, that's what I'm saying, but that's what I'm saying,

22:24.320 --> 22:25.600
we can get.

22:25.840 --> 22:30.440
We are going to close the speed gap a lot, like, I don't support TensorFlow yet.

22:30.440 --> 22:34.000
That's a big one that's just going to, okay, massively close the gap.

22:34.000 --> 22:39.320
And then AMD, I can't even get, I don't even have a benchmark for AMD because I couldn't

22:39.320 --> 22:40.320
get it compiled.

22:40.320 --> 22:41.320
Oh, and I tried.

22:41.320 --> 22:42.320
Oh, I tried.

22:42.320 --> 22:46.800
I spent a day, like, I spent actually a day trying to get PyTorch, and I got it built,

22:46.800 --> 22:51.200
I got it kind of working, and then I tried to run a model, like, there's all kinds of

22:51.200 --> 22:55.560
weird errors, and the rabbit hole is just so deep on this, I'm like, so we, you know,

22:55.760 --> 22:58.680
you can compare the speed, right now, you can run Lama, you can run anything you want

22:58.680 --> 23:03.200
on AMD, it already all works, any OpenCL back-end works, and it's not terribly slow.

23:03.200 --> 23:07.200
I mean, it's a lot faster than crashing, so it's infinitely times faster than PyTorch

23:07.200 --> 23:08.200
on AMD.

23:08.200 --> 23:13.080
But pretty soon, we're going to start getting close to theoretical maximums on AMD.

23:13.080 --> 23:17.720
That's really where I'm pushing, and I want to get AMD on ML perf in a couple months,

23:17.720 --> 23:18.720
hopefully.

23:18.720 --> 23:19.720
Not that you bring up AMD.

23:19.720 --> 23:24.280
Yeah, let's dive into that, because when you announced the TamiCorp fundraise, you mentioned

23:24.320 --> 23:30.400
one of your first goals is build the framework Rhinetime and Driver for AMD, and then on

23:30.400 --> 23:34.560
June 3rd on Twitch, you weren't as excited about AMD anymore.

23:34.560 --> 23:40.680
Maybe let's talk a bit about that, and you compared the quality of commit messages from

23:40.680 --> 23:44.800
the AMD kernel to the Intel work that people are doing there, what's important to know.

23:44.800 --> 23:49.000
So when I said I want to write a framework, I didn't never intend on writing a kernel

23:49.000 --> 23:50.000
driver.

23:50.120 --> 23:56.400
I flirted with that idea briefly, but realistically, there's three parts to it, right?

23:56.400 --> 24:00.840
There's the ML framework, there's the driver, and then there's the user space runtime.

24:00.840 --> 24:02.920
I was even down to rewrite the user space runtime.

24:02.920 --> 24:07.280
I have a GitHub repo called CUDA IO Control Sniffer, it's terribly called, but you can

24:07.280 --> 24:11.520
actually launch a CUDA kernel without CUDA, so you don't need CUDA installed.

24:11.520 --> 24:16.520
Just the NVIDIA open source driver and this open source repo can launch a CUDA kernel.

24:16.520 --> 24:19.520
So rewriting the user space runtime is doable.

24:19.520 --> 24:20.520
Rewriting the kernel driver?

24:20.520 --> 24:24.080
You don't even have docs, I don't have any docs for the GPU, it would just be a massive

24:24.080 --> 24:26.600
reverse engineering project.

24:26.600 --> 24:31.760
So that is, when I saw that there, it wasn't, I wasn't complaining about it being slow,

24:31.760 --> 24:35.160
I wasn't complaining about PyTorch not compiling, I was complaining about the thing crashing

24:35.160 --> 24:38.680
my entire computer, it panics my kernel, and I have to wait five minutes while it reboots

24:38.680 --> 24:41.800
because it's a server motherboard and they take five minutes to reboot.

24:41.800 --> 24:45.720
So I was like, look, if you guys do not care enough to get me a decent kernel driver, there's

24:45.720 --> 24:49.280
no way I'm wasting my time on this, especially when I can use Intel GPUs.

24:49.280 --> 24:53.600
Intel GPUs have a stable kernel driver, and they have all their hardware documented.

24:53.600 --> 24:58.280
You can go and you can find all the registered docs on Intel GPUs, so I'm like, why don't

24:58.280 --> 24:59.280
I just use these?

24:59.280 --> 25:01.280
Now, there's a downside to them.

25:01.280 --> 25:02.280
Their GPU is $350.

25:02.280 --> 25:06.780
You're like, what a deal, it's $350, you got about $350 for the performance, and if you're

25:06.780 --> 25:10.920
paying about 400 for the PCIe slot to put it in, right, like between the power and all

25:10.920 --> 25:16.400
the other stuff, you're like, okay, never mind, you got to use NVIDIA or AMD from that perspective.

25:16.400 --> 25:19.760
But I sent an email to Lisa Sue, and she responded.

25:19.760 --> 25:22.560
Oh, you can see you published that email in a Discord.

25:22.560 --> 25:26.040
I did, I did, and she responded.

25:26.040 --> 25:33.720
And I've had a few calls since, and what I did was like, what I tried to do, well, first

25:33.720 --> 25:34.720
off, thank you for responding.

25:34.720 --> 25:40.080
It shows me that if you don't care about your kernel panicking, this is just a huge waste

25:40.080 --> 25:41.080
of my time, right?

25:41.080 --> 25:42.920
I'll find someone who will care.

25:42.960 --> 25:48.560
I'm not asking for your seven by seven Winograd convolution when transposed to be fast.

25:48.560 --> 25:49.560
Like I'm not asking for that.

25:49.560 --> 25:51.440
I'm asking literally for the basics.

25:51.440 --> 25:52.440
To not value.

25:52.440 --> 25:53.520
Oh, and this isn't tiny grad.

25:53.520 --> 25:54.520
This is your demo apps.

25:54.520 --> 25:57.960
I ran their demo apps in loops, and I got kernel panics.

25:57.960 --> 26:06.000
I'm like, okay, there's, but no, Lisa Sue reached out, connected with a whole bunch

26:06.000 --> 26:07.000
of different people.

26:07.000 --> 26:12.000
They sent me a pre-release version of RockM 5.6.

26:12.000 --> 26:16.040
They told me you can't really say which I'm like, why do you, why do you care?

26:16.040 --> 26:18.960
But they say they're going to release it by the end of the month and it fixed the kernel

26:18.960 --> 26:20.080
panic.

26:20.080 --> 26:26.400
The guy managed to reproduce it with the two GPUs and the computer and yeah, sent me a

26:26.400 --> 26:27.920
driver and it works.

26:27.920 --> 26:31.840
So, yeah, I had, I had that experience.

26:31.840 --> 26:34.840
And then I had another experience where I had two calls with like AMD's like communication

26:34.840 --> 26:38.960
people and just like, I tried to explain to these people like open source culture, like

26:39.160 --> 26:40.520
it's not open source.

26:40.520 --> 26:44.560
If you dump the source code on a GitHub repo and then forget about it until the next release,

26:44.560 --> 26:45.800
it's not open source.

26:45.800 --> 26:51.880
If, you know, all your issues are from 2022, like, like, it's just no one's going to contribute

26:51.880 --> 26:52.880
to that project.

26:52.880 --> 26:53.880
Right.

26:53.880 --> 26:54.880
Sure.

26:54.880 --> 26:55.880
It's open source in a very like technical sense.

26:55.880 --> 26:56.880
To be fair, it's better than nothing.

26:56.880 --> 27:02.080
It's better than nothing, but I fixed a bug in Nickel that I fixed.

27:02.080 --> 27:05.960
There's a fun fact, by the way, if you have a consumer, a consumer AMD GPU, they don't

27:05.960 --> 27:08.040
support peer-to-peer.

27:08.040 --> 27:12.480
And they're already spanned with this horrendously slow because it's using CUDA kernels to do

27:12.480 --> 27:13.880
the copy between the GPUs.

27:13.880 --> 27:17.640
And it's putting so many transactions on the PCIe bus that it's really slow, but you

27:17.640 --> 27:23.320
can use CUDA mem copy and there's a flag to use CUDA mem copy, but that flag had a bug.

27:23.320 --> 27:27.000
So I posted the issue on Nickel.

27:27.000 --> 27:28.520
I expected nothing to happen.

27:28.520 --> 27:30.200
The Nvidia guy replied to me within an hour.

27:30.200 --> 27:31.200
He's like, try this other flag.

27:31.200 --> 27:32.800
I'm like, okay, I tried the other flag.

27:32.800 --> 27:35.200
It still doesn't work, but here's a clean repro.

27:35.200 --> 27:39.080
And I spent like three hours writing a very clean repro.

27:39.080 --> 27:42.760
I ended up tracking the issue down myself, but just the fact that somebody responded

27:42.760 --> 27:46.640
to me within an hour and cared about fixing the issue, okay, you've shown that it's worth

27:46.640 --> 27:50.560
my time and I will put my time in because let's make this better.

27:50.560 --> 27:51.840
I'm here to help.

27:51.840 --> 27:56.000
But if you show me that you're like, you're the kernel panics, let's just expect it.

27:56.000 --> 27:57.000
Okay.

27:57.000 --> 27:59.040
Well, it sounds like AMD is getting the message.

27:59.040 --> 28:00.040
They are.

28:00.040 --> 28:03.600
And I just, I don't really think they've had someone explain to them like, I was like,

28:03.600 --> 28:04.600
you get to like build in public.

28:04.600 --> 28:06.320
And they're like, what's an example of building in public?

28:06.320 --> 28:09.120
I'm like, go look at PyTorch, go look at PyTorch, right?

28:09.120 --> 28:12.720
Like, you know, I have, I have, I have two minor things merged into PyTorch because it's

28:12.720 --> 28:19.040
very responsive, you know, like minor bug fixes, but I feel like it's, you know, yeah.

28:19.040 --> 28:21.320
So that's kind of like the lowest level of the stack.

28:21.320 --> 28:26.600
And then at a slightly higher level, obviously there's tiny grad, there's module, there's

28:26.600 --> 28:27.600
GGML.

28:27.600 --> 28:32.160
How are you thinking about breadth versus like depth and like where you decided to focus

28:32.160 --> 28:33.160
early on?

28:33.160 --> 28:36.760
Um, so GGML is very much like a, okay, everyone has M1s, right?

28:36.760 --> 28:41.360
Actually, I was thinking, in the beginning I was thinking of something more like GGML,

28:41.360 --> 28:45.360
focus on the M1s, but GGML showed up and was just like, we're actually just focusing

28:45.360 --> 28:46.360
on the M1s.

28:46.360 --> 28:52.160
Um, so, and actually M1 PyTorch is considerably better than AMD PyTorch.

28:52.160 --> 28:56.680
And when PyTorch works, it only gives wrong answers sometimes and only crashes sometimes,

28:56.680 --> 29:02.720
but like some models kind of run, um, when I was writing the metal back end, I was comparing

29:02.720 --> 29:08.160
to MPS PyTorch and I had like a, I had a discrepancy, like TinyGrad checks all its outputs compared

29:08.160 --> 29:14.000
to Torch and I had one where it didn't match, I'm like, I really, I checked the matrix by

29:14.000 --> 29:16.960
hand, it matches TinyGrad, I don't understand.

29:16.960 --> 29:22.600
And then I switched PyTorch back to CPU and it matched and I'm like, oh yeah, well, this

29:22.600 --> 29:25.520
is like bugs, like if you like transpose the matrix because like, I think it's like has

29:25.520 --> 29:29.160
to do with like multi views and PyTorch and like weird under the hood stuff that's not

29:29.160 --> 29:32.320
exposed to you, like there's bugs and maybe they fix them, but like, you know, it seems

29:32.320 --> 29:37.000
like there was a lot of momentum again, because you're getting a huge variety, you're getting

29:37.000 --> 29:41.480
how many engineers care about making PyTorch work on M1, right, thousands, tens of thousands.

29:41.480 --> 29:42.480
Yeah.

29:42.480 --> 29:45.120
And you have an open development process and guess what, it's going to be good.

29:45.120 --> 29:48.240
How many engineers care about AMD working with PyTorch AMD working?

29:48.240 --> 29:54.000
Well, you got 10 guys that work for AMD and then like a couple hobbyists.

29:54.000 --> 29:58.280
You revealed an interesting detail about how you debug, which is you check, you hand check

29:58.280 --> 29:59.280
the matrix math.

29:59.640 --> 30:00.640
No, I don't hand check it.

30:00.640 --> 30:06.640
There's a, there's a, one of the best tests in tiny grad is a file called test ops.py

30:06.640 --> 30:12.680
and it's just a hundred small examples written in tiny grad and PyTorch and it checks both

30:12.680 --> 30:14.920
the forwards and backwards to make sure they match.

30:14.920 --> 30:15.920
The test suite.

30:15.920 --> 30:16.920
Yeah.

30:16.920 --> 30:17.920
Very important.

30:17.920 --> 30:20.080
That's, I mean, that's one of them where you like, I really, I put a lot of effort into

30:20.080 --> 30:21.080
the CI for tiny grad.

30:21.080 --> 30:22.560
I think CI is super important.

30:22.560 --> 30:25.320
Like I want that green check to mean I can merge this.

30:25.320 --> 30:26.320
Yeah.

30:26.320 --> 30:27.320
Okay.

30:27.320 --> 30:28.320
I don't want my tests too.

30:28.360 --> 30:30.160
I don't want to manage to introduce a bug and get the green check.

30:30.160 --> 30:31.160
Okay.

30:31.160 --> 30:32.160
We're fixing the test top priority.

30:32.160 --> 30:33.160
Mojo.

30:33.160 --> 30:34.160
It's close source.

30:34.160 --> 30:35.160
No, I'm not that interested.

30:35.160 --> 30:36.160
Do you know what I mean?

30:36.160 --> 30:37.160
Like, like, look, I like Chris Latner.

30:37.160 --> 30:38.160
I think he's going to do great things.

30:38.160 --> 30:42.160
And I understand the, the like kind of the wisdom, even in keeping a close source, but

30:42.160 --> 30:44.160
you know, I'm interested when it's open.

30:44.160 --> 30:45.160
Yeah.

30:45.160 --> 30:46.160
Right.

30:46.160 --> 30:51.160
You have an interesting design deviation from him because he's decided to be a promise

30:51.160 --> 30:55.160
to be a superset of Python and you have decided to break with it.

30:56.000 --> 31:01.000
And I think that's, that affects learnability and trans, transportability of code.

31:01.000 --> 31:09.000
You know, if the PyTorch thing ends up being like a, like a stumbling block, I could write

31:09.000 --> 31:15.000
a perfect PyTorch, like, like, like, like a, you know, instead of import PyTorch, instead

31:15.000 --> 31:19.000
of like, yeah, import Torch, you type import tiny Torch as Torch.

31:19.000 --> 31:23.000
And if that really becomes the stumbling block, I think it's going to be great.

31:23.000 --> 31:27.000
If that really becomes the stumbling block, I will do that.

31:27.000 --> 31:28.000
No.

31:28.000 --> 31:30.240
Chris Latner went much further than PyTorch.

31:30.240 --> 31:34.120
Replicating the PyTorch API is something I can do with a couple, you know, like an engineer

31:34.120 --> 31:35.120
month or two.

31:35.120 --> 31:36.120
Right.

31:36.120 --> 31:37.120
Like a shim.

31:37.120 --> 31:38.120
Yeah.

31:38.120 --> 31:39.120
Replicating Python.

31:39.120 --> 31:41.640
There's a, there's a, there's a big graveyard of those projects.

31:41.640 --> 31:44.440
How's, how's, how's Piston going?

31:44.440 --> 31:51.940
How's, oh, Jython, PyPy is all, you can go way back.

31:51.940 --> 31:57.340
So tiny grad and small layer, you announced TinyBox recently, which is, you know, you

31:57.340 --> 31:58.340
made it.

31:58.340 --> 32:01.660
So your core mission is commoditizing the pedoflop.

32:01.660 --> 32:05.860
And then your business goal is to sell computers for more than the cost to make, which seems

32:05.860 --> 32:08.060
super reasonable.

32:08.060 --> 32:12.220
What are, and you're going to have three TinyBoxes, red, green, blue.

32:12.220 --> 32:16.860
That was my, look, you know, a lot of people, like, I love, you know, leaning into like

32:16.860 --> 32:18.060
saying I'm giving up, right?

32:18.060 --> 32:20.500
It's great to give up or giving up is this wonderful thing.

32:20.500 --> 32:21.820
It's so liberating.

32:21.820 --> 32:24.180
And then like, you can decide afterward if you really give up or not.

32:24.180 --> 32:27.020
There's very little harm in saying you give up, except like, you know, great, Twitter

32:27.020 --> 32:30.780
haters have something to talk about and all press is good press kids.

32:30.780 --> 32:39.060
So, obviously, just read, only read, TinyBox, read, unless AMD, you know, upsets me again

32:39.060 --> 32:44.100
and then we're back to, we're back to other colors, we have other colors to choose from.

32:44.100 --> 32:47.820
When you think about hardware design, what are some of the numbers you look for?

32:47.820 --> 32:52.980
So, Terraprop sits per second, it's one, but like memory bandwidth is another big limiter.

32:52.980 --> 32:54.900
Like, how do you make those trade-offs?

32:54.900 --> 32:58.340
Well, I mean, fundamentally unlimited what GPUs I can buy.

32:58.340 --> 33:01.660
But yeah, for something that I think a lot of people are going to want to reasonably

33:01.660 --> 33:09.060
do with a core core of mine, describe them as luxury AI computers, right?

33:09.060 --> 33:11.180
Like luxury AI computers for people.

33:11.180 --> 33:12.180
And that's like what we're building.

33:12.180 --> 33:16.060
And I think a common thing people are going to want to do is run like large llama, right?

33:16.060 --> 33:17.060
Or large, like Falcon.

33:17.300 --> 33:18.300
FB16 llama.

33:18.300 --> 33:19.300
FB16, exactly.

33:19.300 --> 33:20.300
Exactly.

33:20.300 --> 33:22.220
You know, in-date I think can work.

33:22.220 --> 33:26.660
I think that like what GGML is doing to go to like N4, like this doesn't work.

33:26.660 --> 33:31.220
Like have you done, maybe they have, but like I read what it was and I was like, this isn't

33:31.220 --> 33:32.380
from any paper.

33:32.380 --> 33:34.140
This is just some, like you're-

33:34.140 --> 33:35.140
Squeezing as much as possible.

33:35.140 --> 33:38.900
Yeah, you made up some quantization standard to make it run fast and like, like maybe it

33:38.900 --> 33:41.940
works, but okay, where's like the hell's swag number, right?

33:41.940 --> 33:45.140
Where's your, where's your, where's your, uh, you know, all your-

33:45.140 --> 33:48.820
The thesis is right that like if you have billions, hundreds of billions of parameters

33:48.820 --> 33:51.900
that the individual quantization doesn't actually matter that much.

33:51.900 --> 33:55.340
Well, the real way to look at all of that is to just say you want to compress the weights,

33:55.340 --> 33:56.340
right?

33:56.340 --> 33:57.340
It's a form of weight compression.

33:57.340 --> 33:59.140
Quantization is a form of weight compression right now.

33:59.140 --> 34:00.140
This is obviously not lossless.

34:00.140 --> 34:01.140
It's not a lossless compressor, right?

34:01.140 --> 34:04.380
It's a lossless compressor and you can show that it's correct and okay, we don't have

34:04.380 --> 34:07.460
to have any other conversation, but it's a lossy compressor.

34:07.460 --> 34:11.740
And how do you know that your loss isn't actually losing the power of the model?

34:11.740 --> 34:17.660
Maybe int465bLama is actually the same as fb167bLama, right?

34:17.660 --> 34:18.660
We don't know.

34:18.660 --> 34:21.500
Uh, maybe someone has done this yet, but I looked for it when it like first came out

34:21.500 --> 34:25.980
and people were talking about it and I'm like, I just have, like it's not from a paper, right?

34:25.980 --> 34:29.780
The in-date stuff is from a paper where they, like some of the in-date stuff is from a paper.

34:29.780 --> 34:36.020
There's one paper, I think it's like in-llm.indate where they actually, uh, you know, do all the

34:36.020 --> 34:38.900
tests and they didn't go fully in-date.

34:38.900 --> 34:43.300
They made like 90% of it in-date and kept like 10% of it in fb16 for what they called

34:43.300 --> 34:46.220
like the like outliers or whatever.

34:46.220 --> 34:48.020
So I think that this is not quite so easy.

34:48.020 --> 34:51.140
And I think being able, well, so first off, if you're training, no one's gotten training

34:51.140 --> 34:52.140
to work with in-date yet.

34:52.140 --> 34:53.700
There's a few papers that vaguely show up.

34:53.700 --> 34:57.260
If you're training, you're going to need, uh, bf16 or float16.

34:57.260 --> 35:00.660
Um, so this is why I target that.

35:00.660 --> 35:04.060
Now the thing that you're going to want to do is run these large language models out

35:04.060 --> 35:07.580
of the box on your hardware in fb16 and that's memory bandwidth.

35:08.580 --> 35:13.020
You, you need, you need large amounts of memory bandwidth to, uh, so ask how I trade

35:13.020 --> 35:15.540
off memory bandwidth in Flops, so what GPUs can I buy?

35:15.540 --> 35:21.660
But, um, and I saw one of your, so first of all, you have this, um, hiring process, which

35:21.660 --> 35:25.140
has, you got to solve one of the bounties, um, that are open on tiny grad.

35:25.140 --> 35:27.340
There's no, uh, technical interview.

35:27.340 --> 35:29.020
One of them is in-date support.

35:29.020 --> 35:32.540
Do you already have some things you want to test on?

35:32.540 --> 35:33.540
We have in-date support.

35:33.540 --> 35:39.620
Um, what I'd like to see somebody do is just load the ggml intate llama into tiny grad

35:39.620 --> 35:42.060
and then benchmark it against the fb16 one.

35:42.060 --> 35:46.460
Uh, intate already works in, in tiny grad, it doesn't actually do the math in intate,

35:46.460 --> 35:51.580
which is even a, which is even a stronger, like it does all the math still in fb32.

35:51.580 --> 35:55.180
So intate can mean you just have your weights in intate or intate can mean you actually

35:55.180 --> 35:56.180
do your math in intate.

35:56.180 --> 36:01.180
And doing your math in intate, the big, like, gain that people care about is actually, uh,

36:01.940 --> 36:07.420
having your weights in intate, because weights in intate mean less memory and less memory bandwidth.

36:07.420 --> 36:12.340
Uh, whereas the math, keep it in fb32 with, with, with, on, on m ones, it doesn't even

36:12.340 --> 36:15.700
matter if you're doing, it doesn't matter what data type you're doing in the, in the

36:15.700 --> 36:16.700
, in the GPO.

36:16.700 --> 36:20.740
I, I'm not even sure it can do intate, but fb16 and fb32 is the same, is the same taro

36:20.740 --> 36:21.740
flops.

36:21.740 --> 36:25.620
Um, so yeah, no, that's one of the bounties.

36:25.620 --> 36:30.020
One of the bounties is get, get intate llama running with the intate weights.

36:30.020 --> 36:34.540
And then actually what you could even do, if you really want to test this, just take

36:34.540 --> 36:38.500
the fb16 weights, convert them to intate, then convert them back to fb16, then compare

36:38.500 --> 36:40.460
the unconverted and converted.

36:40.460 --> 36:41.780
Oh, that's a nice hack.

36:41.780 --> 36:42.780
Oh yeah.

36:42.780 --> 36:43.780
Right.

36:43.780 --> 36:45.540
Like, like, like, this should be lossless in the other direction.

36:45.540 --> 36:46.540
Well, yeah.

36:46.540 --> 36:51.460
So, uh, yeah, I think fb16, it should be lossless in the other direction.

36:51.460 --> 36:53.140
I'm actually not a hundred percent about that.

36:53.140 --> 36:54.140
Why not?

36:54.140 --> 36:57.580
Uh, oh, cause like, you ever try to like, like, if you want to represent, if it was like

36:57.580 --> 36:59.980
int16, it's not lossless.

36:59.980 --> 37:00.980
Sure.

37:00.980 --> 37:04.020
I think, I think all of intate can be represented in fb16, but I'm not a hundred percent about

37:04.020 --> 37:05.020
that.

37:05.020 --> 37:10.340
Actually, I think it, we just draw the bytes and we just have to do it, right?

37:10.340 --> 37:11.340
Just literally do it.

37:11.340 --> 37:17.020
There's only 256 to check, like, um, but yeah, either way, or, I mean, into four, definitely.

37:17.020 --> 37:18.940
So do your in four, convert it back.

37:18.940 --> 37:24.140
And now see, even with in four weights and fb32 math, like, okay, how much does your

37:24.140 --> 37:25.900
performance to grade of this model?

37:25.900 --> 37:26.900
Yeah.

37:26.940 --> 37:31.580
So, so can we, uh, I'm about to zoom out a little bit from the details.

37:31.580 --> 37:35.980
I don't know if you, you had more, no, I think like the, you're playing to release

37:35.980 --> 37:40.460
the first tiny box ship them in like two to six, eight months, something like that.

37:40.460 --> 37:45.220
Uh, what's up with mine for you in terms of building a team who should, who are you calling

37:45.220 --> 37:46.220
for?

37:46.220 --> 37:47.220
Yeah.

37:47.220 --> 37:50.020
Uh, well, to, to stay on the tiny box for, for, for, for, yeah, exactly.

37:50.020 --> 37:54.860
Um, so at the GPUs picked out and you're like, well, I could make that computer with

37:54.860 --> 37:55.860
the GPUs.

37:55.900 --> 38:00.540
My answer is, can you, do you know how to put, do you know how hard it is to put six

38:00.540 --> 38:02.660
GPUs on a computer?

38:02.660 --> 38:05.780
People think it's really easy and it's really easy to put one GPU in a computer.

38:05.780 --> 38:09.980
It's really easy to put two GPUs in a computer, but now you want to put in eight.

38:09.980 --> 38:10.980
Okay.

38:10.980 --> 38:11.980
So I'll tell you a few things about these GPUs.

38:11.980 --> 38:13.480
They take up four slots.

38:13.480 --> 38:15.580
What kind of computer?

38:15.580 --> 38:16.900
You can buy the nicest super micro.

38:16.900 --> 38:18.780
You can't put eight of those in there.

38:18.780 --> 38:19.780
You need two slot blowers.

38:19.780 --> 38:23.060
If you want to use one of those, those for your super micros, you need two slot blowers

38:23.060 --> 38:24.060
or water cooling.

38:24.260 --> 38:26.340
All right, if, if you're trying to get the four slot cards in there, you're going to

38:26.340 --> 38:30.980
need some form of water cooling, uh, or you're going to need, there are some like Chinese

38:30.980 --> 38:32.180
40 nineties that are blowers, right?

38:32.180 --> 38:34.380
You have any blowers or water cooling if you're trying to get it in those things.

38:34.380 --> 38:35.380
Right.

38:35.380 --> 38:37.220
Um, so you, are you doing water?

38:37.220 --> 38:39.300
No, I'm not using that chassis.

38:39.300 --> 38:40.300
Okay.

38:40.300 --> 38:45.700
Um, then the other thing that, okay, so now you want to get six GPUs in a computer.

38:45.700 --> 38:46.700
So that's a big challenge.

38:46.700 --> 38:48.540
You're like, oh, I'll just use a PCIe extenders.

38:48.540 --> 38:49.540
I saw it online as tech tips.

38:49.540 --> 38:50.540
It works great.

38:50.540 --> 38:51.540
No, it doesn't.

38:51.540 --> 38:55.180
It's PCIe extenders that work at PCIe 4.0 and interconnect bandwidth.

38:55.180 --> 38:56.180
Super important.

38:56.180 --> 38:57.180
Yes.

38:57.180 --> 38:58.580
They don't work at 3.0.

38:58.580 --> 39:04.420
No PCIe extender I've tested and I've bought 20 of them, uh, works at PCIe 4.0.

39:04.420 --> 39:07.020
So you're going to need PCIe redrivers now.

39:07.020 --> 39:08.020
Okay.

39:08.020 --> 39:09.380
How much does that add in cost?

39:09.380 --> 39:10.380
Right.

39:10.380 --> 39:11.380
Like these things all get really hard.

39:11.380 --> 39:12.380
And then tiny boxes.

39:12.380 --> 39:14.140
I've even had another constraint to it.

39:14.140 --> 39:19.420
I want this thing to be silent, not totally silent, but my limit is like 45, maybe 50

39:19.420 --> 39:22.660
dB, but not super micro machine.

39:22.660 --> 39:23.660
60 dB.

39:23.660 --> 39:28.260
We have a small, we have a compute cluster, a comma, you got to wear your protection

39:28.260 --> 39:29.260
to go in there.

39:29.260 --> 39:30.260
I like it.

39:30.260 --> 39:31.260
Yeah.

39:31.260 --> 39:32.260
I've seen some videos where you give a tour.

39:32.260 --> 39:33.260
Yeah.

39:33.260 --> 39:34.260
Yeah.

39:34.260 --> 39:35.260
It's noisy.

39:35.260 --> 39:36.260
It's super loud.

39:36.260 --> 39:37.260
Yeah.

39:37.260 --> 39:38.260
10,000 RPM.

39:38.260 --> 39:39.260
Just screaming.

39:39.260 --> 39:44.540
Like I want to be able to use the normal big GPU fans and make this thing so it can

39:44.540 --> 39:48.940
sit under your desk, plug into one outlet of power, right?

39:48.940 --> 39:54.940
This GPUs, your GPUs are 350 Watts each, can't plug that into a wall outlet.

39:54.940 --> 39:55.940
Okay.

39:55.940 --> 39:56.940
So how are you going to deal with that?

39:56.940 --> 39:59.540
Good questions, right?

39:59.540 --> 40:00.540
And you're not sharing them.

40:00.540 --> 40:02.620
Well, that one, I mean, that one is pretty obvious.

40:02.620 --> 40:04.220
You have to limit the power on the GPUs, right?

40:04.220 --> 40:05.980
You have to limit the power on the GPUs.

40:05.980 --> 40:10.020
Now you can limit power on GPUs and still get, you can use like half the power and get

40:10.020 --> 40:12.020
80% of the performance.

40:12.020 --> 40:15.300
This is a known fact about GPUs, but like that's one of my design constraints.

40:15.300 --> 40:19.500
So when you start to add all these design constraints, good luck building a tiny box

40:19.500 --> 40:20.500
yourself.

40:20.500 --> 40:24.980
You know, obviously it can be done, but you need something that has actually quite a bit

40:24.980 --> 40:27.580
of scaling resources to do it.

40:27.580 --> 40:31.620
And you see like the under the desk, it's like one of the main use cases, kind of like

40:31.620 --> 40:33.620
individual developer use or.

40:33.620 --> 40:34.620
Yeah.

40:34.620 --> 40:37.900
What I also see is more of a like an AI hub for your home, right?

40:37.900 --> 40:42.180
As we start to get like home robotics kind of stuff, you don't want to put the inference

40:42.180 --> 40:43.180
on the robot.

40:43.740 --> 40:46.620
But you also don't want to put the inference on the cloud.

40:46.620 --> 40:51.860
You don't want to put it on the robot because, okay, it's 1500 Watts, tiny box, you put batters

40:51.860 --> 40:54.380
and charge them.

40:54.380 --> 40:55.380
Bad idea.

40:55.380 --> 40:57.900
And just, just, just wireless, wireless is 0.5 milliseconds.

40:57.900 --> 40:58.900
Yeah.

40:58.900 --> 40:59.900
This is super fast.

40:59.900 --> 41:01.740
You don't want to go to the cloud for two reasons.

41:01.740 --> 41:03.260
One, cloud's far away.

41:03.260 --> 41:04.260
Okay.

41:04.260 --> 41:05.260
It's not that far away.

41:05.260 --> 41:11.500
You can kind of address this, but two, cloud's also mad expensive, like cloud GPUs are way

41:11.500 --> 41:14.260
more expensive than running that GPU at your house.

41:14.260 --> 41:16.580
At least any rates you're going to get, right?

41:16.580 --> 41:19.580
Maybe if you commit to buy, well, yeah, I'm going to buy 10,000 GPUs for three years,

41:19.580 --> 41:21.380
then maybe the cloud will give you a good rate.

41:21.380 --> 41:23.580
But like, you want to buy, you want to buy one GPU in the cloud?

41:23.580 --> 41:24.580
Ooh.

41:24.580 --> 41:27.700
I mean, okay, you can go to like Vast, but like if you're going to Azure, AWS, so that's

41:27.700 --> 41:28.700
expensive.

41:28.700 --> 41:29.700
Yeah.

41:29.700 --> 41:33.180
This is like a, like a personal data center, you know, instead of a cloud data center.

41:33.180 --> 41:36.180
We like the term compute cluster, so we can use NVIDIA GPUs.

41:36.180 --> 41:37.180
Yeah.

41:37.180 --> 41:38.580
Data centers may be a little bit dated.

41:38.860 --> 41:43.540
It's a compute cluster, which is totally legal under the CUDA license agreement.

41:43.540 --> 41:45.660
You talk a lot about the PCIe connection.

41:45.660 --> 41:48.540
Do you think there's any fat there to the trim?

41:48.540 --> 41:49.540
What do you mean?

41:49.540 --> 41:51.700
Just you're limited by bandwidth, right?

41:51.700 --> 41:52.700
Okay.

41:52.700 --> 41:53.700
For some things, yes.

41:53.700 --> 42:00.740
So the bandwidth is roughly 10x less than what you can get with NV linked A 100s.

42:00.740 --> 42:01.740
Yeah.

42:01.740 --> 42:04.500
NV linked A 100s are going to have, and then you can even get like full fabric and the

42:04.500 --> 42:08.540
NVIDIA really pushes on that stuff, 600 gigabytes per second, right?

42:08.540 --> 42:10.620
And PCIe four, you're going to get 60.

42:10.620 --> 42:11.620
All right.

42:11.620 --> 42:12.620
So you're getting 10x less.

42:12.620 --> 42:13.620
Yeah.

42:13.620 --> 42:17.420
Um, that said, why do you need the bandwidth, right?

42:17.420 --> 42:21.540
And the answer is you need it for training huge models.

42:21.540 --> 42:25.460
If you're training on a tiny box, your limit's going to be about 7 billion, right?

42:25.460 --> 42:29.140
If you're, if you're training on big stuff, your limits could be like 70 billion, right?

42:29.140 --> 42:30.140
Okay.

42:30.140 --> 42:31.140
You can hack it to get a bit higher.

42:31.140 --> 42:34.020
You can hack it like GBT hacked it to get a bit higher, but like that's 65 billion in

42:34.020 --> 42:35.020
llama.

42:35.020 --> 42:36.700
Like there's a reason they chose 65 billion, right?

42:36.700 --> 42:40.380
And that's what can reasonably fit model parallel on, on, on a GPUs.

42:40.380 --> 42:41.380
Right.

42:41.380 --> 42:45.180
So, um, yes, you, you are going to end up training models.

42:45.180 --> 42:48.180
The cap's going to be like 7 billion, but I actually heard this on your podcast.

42:48.180 --> 42:51.740
I don't think that the best chatbot models are going to be the big ones.

42:51.740 --> 42:54.740
I think the best chatbot models are going to be the ones where you had a thousand training

42:54.740 --> 42:56.780
runs instead of one.

42:56.780 --> 43:00.740
And I don't think that the interconnect bandwidth is going to matter that much.

43:00.740 --> 43:02.940
So what are we optimizing for instead of compute optimal?

43:02.940 --> 43:05.660
Uh, what do you mean compute optimal?

43:05.660 --> 43:10.340
So the, this, you're talking about this, um, the llama style models where you train

43:10.340 --> 43:11.340
for like 200,

43:11.340 --> 43:12.340
You train longer.

43:12.340 --> 43:13.340
Yeah.

43:13.340 --> 43:14.340
Yeah.

43:14.340 --> 43:15.340
Yeah.

43:15.340 --> 43:16.340
So, okay.

43:16.340 --> 43:17.340
You can always make your model better by doing one of two things.

43:17.340 --> 43:18.340
Right.

43:18.340 --> 43:19.340
And a comma, we just have a strict limit on it.

43:19.340 --> 43:21.340
Um, you can always make your model better by training longer and you can always make

43:21.340 --> 43:23.580
your model better by making it bigger.

43:23.580 --> 43:26.180
But these aren't the interesting ones, right?

43:26.180 --> 43:28.980
Particularly the making it bigger because training it longer, fine, you know, you're

43:28.980 --> 43:29.980
getting a better set of weights.

43:29.980 --> 43:30.980
The inference is the same.

43:30.980 --> 43:35.620
The inference is the same, whether I trained it for a day or a week, but the, okay.

43:35.620 --> 43:38.740
If it's one billion versus 10 billion, well, I 10x my inference too.

43:38.740 --> 43:39.740
All right.

43:39.740 --> 43:43.140
So I think that these big models are kind of a, sure they're great if you're research

43:43.140 --> 43:47.140
labs and you're trying to like max out this thing, which you can talk about later.

43:47.140 --> 43:48.140
Yeah.

43:48.140 --> 43:49.140
Yeah.

43:49.140 --> 43:50.140
Yeah.

43:50.140 --> 43:52.020
But if you're, but if you're like a startup or you're like an individual or you're trying

43:52.020 --> 43:56.060
to deploy this to the edge anywhere, you don't, you don't need that many weights.

43:56.060 --> 43:57.060
Yeah.

43:57.060 --> 43:58.060
Yeah.

43:58.060 --> 43:59.060
You don't want them anyway.

43:59.060 --> 44:00.060
Optimizing for inference rather than capabilities.

44:00.060 --> 44:01.060
Yes.

44:01.060 --> 44:02.060
Doing benchmarks.

44:02.060 --> 44:03.060
Yes.

44:03.060 --> 44:04.060
Yes.

44:04.060 --> 44:05.060
Um, and I think the, the inference thing, right?

44:05.060 --> 44:08.460
There should be so much more, right now the ratio between like training and inference

44:08.460 --> 44:11.940
on clouds, I think it's only still like, it's like two or three acts, right?

44:11.940 --> 44:13.740
It's two or three acts more inference, which doesn't make any sense.

44:13.740 --> 44:15.140
Like there should be way more inference.

44:15.140 --> 44:16.140
Yeah.

44:16.140 --> 44:19.220
There should be a 10 to a hundred X more inference in the world than, than training.

44:19.220 --> 44:22.500
Um, but then also like what is training, right?

44:22.500 --> 44:26.380
You start to see these things like Laura, like, you're getting kind of, it's kind of

44:26.380 --> 44:28.940
blurring the lines between inference and training.

44:28.940 --> 44:30.940
And I think that that blurred line is actually really good.

44:30.940 --> 44:34.860
I'd like to see much more like on device training or on device fine tuning of the final

44:34.860 --> 44:35.860
layer.

44:35.860 --> 44:36.860
Yeah.

44:36.860 --> 44:37.860
Um, we're, we're pushing toward this stuff at comma.

44:37.860 --> 44:38.860
Right.

44:38.860 --> 44:39.860
Like why am I shipping a fixed model?

44:39.860 --> 44:43.980
I totally want this model to fine tune based on like how, you know, your left tire is flat.

44:43.980 --> 44:44.980
Right.

44:44.980 --> 44:48.980
Like every time you cut the same turn because your left tire is flat.

44:48.980 --> 44:49.980
Well, it should learn that.

44:49.980 --> 44:50.980
Right.

44:50.980 --> 44:53.540
So would comma pursue perimeter efficient fine tuning?

44:53.540 --> 44:54.540
Yeah.

44:54.540 --> 44:55.540
Yeah.

44:55.540 --> 44:57.540
Where, where, where, where seems like a, we're looking at the stuff like that.

44:57.540 --> 45:00.660
I mean, comma is already very parameter efficient because we have to like run this thing in

45:00.660 --> 45:02.740
a car and you have to like cool it and power it.

45:02.740 --> 45:03.740
Yeah.

45:03.740 --> 45:04.740
Yeah.

45:04.740 --> 45:10.020
And so this kind of like intelligence cluster you have in your home, you see when the person

45:10.020 --> 45:14.900
is using third party model, they load them locally and kind of do the final fine tuning.

45:14.900 --> 45:16.500
It kind of stays within the box.

45:16.500 --> 45:17.500
Yeah.

45:17.500 --> 45:18.940
I think that that's one thing.

45:18.940 --> 45:21.340
That's one version of it for the privacy conscious.

45:21.340 --> 45:28.340
Um, I also see a world where, uh, you can have your tiny box in its down cycles, um,

45:28.340 --> 45:29.340
mine flop coin.

45:29.340 --> 45:30.340
Right.

45:30.340 --> 45:32.260
You know, not all, turns out not all crypto is a scam.

45:32.260 --> 45:33.780
There's one way to tell if crypto is a scam.

45:33.780 --> 45:36.740
If they're selling the coin before they make the product, it's a scam.

45:36.740 --> 45:39.940
If they have the product and then they sell the coin, it's maybe not a scam.

45:39.940 --> 45:40.940
Right.

45:40.940 --> 45:43.620
So yeah, my thought is like each tiny box would let you, would have a private key on

45:43.620 --> 45:44.620
it.

45:44.620 --> 45:45.620
Uh, and you have to do it this way.

45:45.620 --> 45:47.340
You can't just let anyone join because of civil attacks.

45:47.340 --> 45:48.340
Right.

45:48.340 --> 45:51.020
There's a real problem of like, how do I, uh, how do I ensure your data is correct?

45:51.020 --> 45:54.340
And the way that I ensure your data is correct on the tiny net is if you ever send wrong

45:54.340 --> 45:56.780
data, you're banned from the life.

45:56.780 --> 45:57.780
Yeah.

45:57.780 --> 45:59.620
You're, you're a $15,000 hardware box is banned.

45:59.620 --> 46:00.620
So you know, don't cheat.

46:00.620 --> 46:03.500
Um, obviously if it messes up, we'll forgive you.

46:03.540 --> 46:05.940
But, um, I'm saying like some is going to try to jailbreak your devices.

46:05.940 --> 46:07.940
There's no jailbreak.

46:07.940 --> 46:08.940
There's no jailbreak.

46:08.940 --> 46:09.940
There's just a different network.

46:09.940 --> 46:11.140
Well, there's just a private key on each device.

46:11.140 --> 46:12.140
Right.

46:12.140 --> 46:14.100
Like if you buy a tiny box from the tiny corp, I give you a private key.

46:14.100 --> 46:15.100
It's in my backend server.

46:15.100 --> 46:16.100
Right.

46:16.100 --> 46:17.100
You want to hack my server.

46:17.100 --> 46:18.100
That's illegal.

46:18.100 --> 46:19.100
Yeah.

46:19.100 --> 46:20.100
Anything you want to do on the device, the device is yours.

46:20.100 --> 46:21.100
My server's mine.

46:21.100 --> 46:22.100
Right.

46:22.100 --> 46:23.100
Like.

46:23.100 --> 46:24.100
Yeah.

46:24.100 --> 46:25.100
Yeah.

46:25.100 --> 46:26.100
Uh, have you looked into like, uh, federated training at all?

46:26.100 --> 46:27.100
Yeah.

46:27.100 --> 46:28.100
So I mean, okay.

46:28.100 --> 46:29.100
You're now, there's, okay.

46:29.100 --> 46:30.100
There's a lot of magnitude of federated training.

46:30.100 --> 46:34.100
I mean, like, uh, over the cloud and stuff, over the internet, over the internet, but

46:34.100 --> 46:36.100
also distributed on a bunch of devices.

46:36.100 --> 46:37.100
Right.

46:37.100 --> 46:38.100
Yeah.

46:38.100 --> 46:41.100
I'm, I'm, I'm very bearish on this stuff because you're an interconnect bandwidth.

46:41.100 --> 46:42.100
Right.

46:42.100 --> 46:43.100
So, okay.

46:43.100 --> 46:45.700
At the high end, you have your interconnect bandwidth of envy link, which is 600 gigabytes

46:45.700 --> 46:46.700
per second.

46:46.700 --> 46:47.700
Right.

46:47.700 --> 46:53.540
The tiny box has 60 gigabytes per second and then your internet has 125 megabytes per

46:53.540 --> 46:54.540
second.

46:54.540 --> 46:55.540
Right.

46:55.540 --> 46:56.540
Not gigabits.

46:56.540 --> 46:57.540
125 megabytes.

46:57.540 --> 46:58.540
Right.

46:58.540 --> 46:59.540
So, okay.

46:59.980 --> 47:02.540
That's, that's how, that's how many orders of magnitude we're talking here.

47:02.540 --> 47:05.500
Like from 60 down to 125, like, all right.

47:05.500 --> 47:06.500
That's over a hundred.

47:06.500 --> 47:07.500
There's over a hundred X.

47:07.500 --> 47:08.500
That's 400 X.

47:08.500 --> 47:09.500
Right.

47:09.500 --> 47:10.500
So like, no.

47:10.500 --> 47:11.500
Uh, but what you can do is inference.

47:11.500 --> 47:12.500
Right.

47:12.500 --> 47:13.500
Like there's, for inference, you don't care.

47:13.500 --> 47:14.500
Right.

47:14.500 --> 47:18.340
For inference, I, I, there's so little bandwidth at the top and the bottom of the model, um,

47:18.340 --> 47:20.340
that like, yeah, you can do federated inference.

47:20.340 --> 47:21.340
Right.

47:21.340 --> 47:22.340
And that's kind of what I'm talking about.

47:22.340 --> 47:26.580
Um, there's also interesting things to push into like, you're like, but okay, what if

47:26.580 --> 47:28.700
you want to run close source models?

47:28.700 --> 47:32.740
This stuff gets kind of interesting, like using TPMs on the boxes and stuff.

47:32.740 --> 47:33.740
Um, yeah.

47:33.740 --> 47:35.540
But then someone might jailbreak my device.

47:35.540 --> 47:37.220
So, you know, maybe we don't try to do that.

47:37.220 --> 47:38.220
Yeah.

47:38.220 --> 47:39.500
What's like the enterprise use case?

47:39.500 --> 47:42.860
Do you see companies buying a bunch of these and like stacking them together?

47:42.860 --> 47:47.820
Um, so the tiny box is like the first version of what we're building, but what I really

47:47.820 --> 47:52.780
want to do is be on the absolute edge of flops per dollar and flops per lot.

47:52.780 --> 47:54.140
These are the two numbers that matter.

47:54.140 --> 47:58.020
Uh, so the enterprise use case is you want to train like, like comma, right?

47:58.020 --> 47:59.940
So comma just built out a new compute cluster.

47:59.940 --> 48:02.540
It's about, uh, it's about a person and a half.

48:02.540 --> 48:07.420
Uh, so, you know, it's decent size person, a person being 20 person is a person is 20

48:07.420 --> 48:08.420
paid flops.

48:08.420 --> 48:09.420
It's about 30 paid flops.

48:09.420 --> 48:14.660
Um, we built out a little, uh, little compute cluster and you know, we paid double what

48:14.660 --> 48:17.100
you theoretically could per flop, right?

48:17.100 --> 48:21.340
You theoretically could pay half per flop if you designed a bunch of custom stuff.

48:21.340 --> 48:24.980
And yeah, I mean, I could see that being, you know, tiny core when comma is going to

48:24.980 --> 48:25.980
be the first customer.

48:25.980 --> 48:29.460
I'm going to build a box for comma and then I'm going to show off the box I built for

48:29.460 --> 48:34.060
comma and be like, okay, like, do you want to build, I sell $250,000 training computers

48:34.060 --> 48:35.740
or how much does one H 100 box?

48:35.740 --> 48:37.660
Uh, it's, uh, it's four under grand.

48:37.660 --> 48:38.660
Okay.

48:38.660 --> 48:42.980
I'll build you a 400 grand training computer and it'll be 10x better than that H 100 box

48:42.980 --> 48:46.980
for again, not for every use case for some, you need the interconnect bandwidth, but for

48:46.980 --> 48:52.620
90% of most companies model training use cases, the tiny box will be five X faster for the

48:52.620 --> 48:54.620
same price.

48:54.620 --> 48:57.420
You mentioned the person of compute.

48:57.420 --> 48:59.420
How do we build a human for $20 million?

48:59.420 --> 49:01.540
Oh, it's a lot cheaper now.

49:01.540 --> 49:02.540
It's a lot cheaper now.

49:02.540 --> 49:06.860
Uh, so like I said, we comma, comma spent about, uh, about half a million on our, on

49:06.860 --> 49:07.860
our person and a half.

49:07.860 --> 49:14.140
So, you know, what are some of the numbers people should think of when they compare compute

49:14.140 --> 49:15.140
to like people.

49:15.140 --> 49:18.620
So GBD four was a hundred person years of training.

49:18.620 --> 49:22.620
That's more like on, on the time scale, um, 20 petaflops is one person.

49:22.620 --> 49:27.340
I think you, um, right now the math was that for the price of the most expensive thing

49:27.340 --> 49:31.860
we build, which is the international space station, we could build, uh, one Tampa of

49:31.860 --> 49:32.860
one Tampa.

49:32.860 --> 49:33.860
Yeah, yeah.

49:33.860 --> 49:34.860
One Tampa of compute.

49:34.860 --> 49:35.860
Yeah.

49:35.860 --> 49:36.860
Which is 400,000 people.

49:36.860 --> 49:37.860
Of measurement.

49:37.860 --> 49:38.860
Um, yeah.

49:38.860 --> 49:39.860
Yeah.

49:39.860 --> 49:40.860
We could build.

49:40.860 --> 49:42.860
So like the biggest training clusters today, I know less about how GBD four was trained.

49:42.860 --> 49:47.620
I know some rough numbers on the weights and stuff, but, uh, llama trillion parameters.

49:47.620 --> 49:48.620
Well, okay.

49:48.620 --> 49:53.580
So GBD four is 220 billion in each head and then it's an eight way mixture model.

49:53.580 --> 49:55.900
So mixture models are what you do when you're out of ideas.

49:55.900 --> 49:58.220
Um, so, you know, it's a, it's a mixture model.

49:58.220 --> 50:01.020
Uh, they just train the same model eight times and they have some little trick.

50:01.020 --> 50:05.500
They actually do 16 inferences, but, uh, no, it's not like, so the multi modality is just

50:05.500 --> 50:08.620
a vision model kind of glommed on.

50:08.620 --> 50:11.020
I mean the multi modality is like obvious what it is too.

50:11.020 --> 50:13.700
You just put the vision model in the same token space as your language model.

50:13.700 --> 50:15.300
Oh, did people think it was something else?

50:15.300 --> 50:18.220
No, the mixture has nothing to do with the vision or language aspect of it.

50:18.220 --> 50:22.020
It just has to do with, well, okay, we can't really make models bigger than 220 billion

50:22.020 --> 50:23.020
parameters.

50:23.020 --> 50:24.700
Uh, we want it to be better.

50:24.700 --> 50:26.020
Well, how can we make it better?

50:26.020 --> 50:31.820
Well, we can train it longer and okay, we've actually already maxed that out, uh, getting

50:31.820 --> 50:32.820
diminishing returns there.

50:32.820 --> 50:33.820
Okay.

50:33.820 --> 50:34.820
Make sure of experts.

50:34.820 --> 50:35.820
Yeah, make sure of experts.

50:35.820 --> 50:36.820
We'll train eight of them.

50:36.820 --> 50:37.820
Right.

50:37.820 --> 50:41.740
So, you know, you know, the real truth is whenever a start, whenever a company is secretive,

50:41.740 --> 50:45.380
with the exception of Apple, Apple's the only exception, whenever a company is secretive,

50:45.380 --> 50:48.100
it's because they're hiding something that's not that cool.

50:48.460 --> 50:51.260
People have this wrong idea over and over again that they think they're hiding it because

50:51.260 --> 50:52.260
it's really cool.

50:52.260 --> 50:53.260
It must be amazing.

50:53.260 --> 50:54.260
It's a trillion parameters.

50:54.260 --> 50:57.380
No, it's a little bigger than GPT-3 and they did an eight-way mixture of experts.

50:57.380 --> 51:03.980
Like, all right, dude, anyone can spend eight times the money and get that, um, but yeah,

51:03.980 --> 51:08.860
so, uh, coming back to what I think is actually going to happen is, yeah, people are going

51:08.860 --> 51:14.020
to train smaller models for longer and fine tune them and find all these tricks.

51:14.020 --> 51:15.020
Right.

51:15.180 --> 51:18.700
You know, I think, uh, opening, I used to publish stuff on this, you know, uh, when

51:18.700 --> 51:25.140
they would publish stuff, uh, about how much better the training has gotten given the same

51:25.140 --> 51:30.660
of holding compute constant and it's gotten a lot better, right, than compare, like, batch

51:30.660 --> 51:31.660
norm to no batch norm.

51:32.260 --> 51:33.260
Yeah.

51:33.260 --> 51:34.260
And now we have like-

51:34.260 --> 51:35.740
Is there a finding algorithms like flash attention?

51:35.740 --> 51:37.740
Yeah, well, flash attention, yeah.

51:37.740 --> 51:38.740
Yeah.

51:38.740 --> 51:41.140
Um, my flash attention is the same compute.

51:41.140 --> 51:43.540
A flash attention is an interesting fact where it's actually the identical compute.

51:43.580 --> 51:45.100
It's just a more efficient way to do the compute.

51:45.100 --> 51:51.140
But I'm even talking about, like, like, um, look at the new, look at the new, uh, embeddings

51:51.140 --> 51:52.140
people are using.

51:52.140 --> 51:53.140
Right.

51:53.140 --> 51:54.140
They used to use these like boring old embeddings.

51:54.140 --> 51:56.180
Now like Lama uses that complex one and that was like alibi.

51:56.180 --> 52:00.740
I'm not up to date on all the latest stuff, but, uh, those tricks give you so much.

52:00.740 --> 52:02.900
There's been a whole round trip with positional embeddings.

52:02.900 --> 52:04.860
I don't know if you've, uh, seen this discussion.

52:04.860 --> 52:05.860
I haven't followed-

52:05.860 --> 52:09.060
Like you need them, you need rotational and then you don't need them.

52:09.060 --> 52:10.660
I haven't followed exactly.

52:10.660 --> 52:14.220
I mean, you quickly run into the obvious problem with positional embeddings, which

52:14.220 --> 52:17.460
is you have to invalidate your KV cache if you run off the context.

52:17.460 --> 52:21.780
So that's why I think these new ones that play with them, but, uh, I'm not that, I'm

52:21.780 --> 52:25.820
not that, I'm not an expert on like the latest up-to-date language model stuff.

52:25.820 --> 52:26.820
Yeah.

52:26.820 --> 52:33.940
Um, I mean, we have what we do at comma, I don't know how that works, but like, um, what

52:33.940 --> 52:37.220
are some of the things, I mean, that people are getting wrong.

52:37.220 --> 52:41.500
So back to autonomous driving, there was like the whole like LiDAR versus vision thing.

52:41.500 --> 52:44.460
You know, it's like, people don't get into accidents because they cannot see well, they

52:44.460 --> 52:47.740
get into accidents because they got distracted and all these things.

52:47.740 --> 52:51.020
What are, do you see similarities today on like the pathway GI?

52:51.020 --> 52:53.460
Like are there people, like what are like the-

52:53.460 --> 52:57.260
Nothing, nothing I say about this is ever going to compete with how Rich Sutton stated

52:57.260 --> 52:58.260
it.

52:58.260 --> 52:59.260
Rich Sutton is writer-

52:59.260 --> 53:00.260
The bitter lesson.

53:00.260 --> 53:01.260
The first millennium, the bitter lesson.

53:01.260 --> 53:02.260
Nothing I say is ever going to compete with.

53:02.260 --> 53:05.060
The bitter lesson is way better than any way I'm going to phrase this.

53:05.060 --> 53:08.780
Just go read that and then like, I'm sorry, it's bitter, but you actually just have to

53:08.780 --> 53:09.780
believe it.

53:09.780 --> 53:12.420
Like over and over again, people make this mistake.

53:12.420 --> 53:14.980
They're like, oh, we're going to hand it to you or this thing, we're going to hand-

53:14.980 --> 53:17.180
No, like stop wasting time.

53:17.180 --> 53:20.020
Which is, I mean, OpenAI is not taking the bitter lesson.

53:20.020 --> 53:21.260
No, OpenAI-

53:21.260 --> 53:27.540
They were, they were leaders in deep learning for a long, long, long time, but you're telling

53:27.540 --> 53:29.140
me that GPT-4 is not.

53:29.140 --> 53:34.060
Well, OpenAI was the absolute leader to the thesis that computers all you need, right?

53:34.060 --> 53:36.940
And there's a question of how long this thesis is going to continue for.

53:36.940 --> 53:41.140
It's a cool thesis and look, I think I would be lying along with everybody else.

53:41.140 --> 53:44.900
I was into language models like way back in the day for the HotterPrice.

53:44.900 --> 53:47.100
I got into AI through the HotterPrice.

53:47.100 --> 53:51.140
Like 2014, I'm trying to build compressive models of Wikipedia and I'm like, okay, why

53:51.140 --> 53:52.140
is this so hard?

53:52.140 --> 53:53.900
Like what this is is a language model, right?

53:53.900 --> 53:58.380
And I'm playing with these like, like Bayesian things and I'm just like, oh, but like I get

53:58.380 --> 53:59.380
it.

53:59.380 --> 54:02.220
Like it needs to be like, like it's like, I have two data points and they're like almost

54:02.220 --> 54:03.220
the same.

54:03.220 --> 54:05.260
And so I measure that almost, right?

54:05.260 --> 54:08.780
I just like, you know, wrap my head around, I couldn't like, like wrap my head around

54:08.780 --> 54:09.780
this.

54:09.780 --> 54:13.380
And this was around the time Carpathia released the first like RNN that generated the Shakespeare

54:13.380 --> 54:14.380
stuff.

54:14.380 --> 54:16.820
And I'm like, okay, I get it, right?

54:16.820 --> 54:18.780
It's neural networks that are compressors.

54:18.780 --> 54:21.580
Now this isn't actually, you can't actually win the HotterPrice with these things because

54:21.580 --> 54:23.580
HotterPrice is MDL.

54:23.580 --> 54:28.100
It's the model, size of the model plus the size of the encodings, embeddings.

54:28.100 --> 54:33.100
So yeah, you can't, I mean, probably now you can because it's gotten so good.

54:33.100 --> 54:35.180
But yeah, back in the day, you kind of couldn't.

54:35.180 --> 54:36.180
So I was like, okay, cool.

54:36.180 --> 54:37.180
Like this is what it is.

54:37.180 --> 54:38.180
I kind of get it.

54:38.180 --> 54:39.180
Yeah.

54:39.180 --> 54:43.540
I mean, I think I didn't expect that it would continue to work this well.

54:43.540 --> 54:46.660
I thought there'd be real limits to how good autocomplete could get.

54:46.660 --> 54:47.660
That's fancy autocomplete.

54:47.660 --> 54:51.820
But yeah, no, like it works.

54:51.820 --> 54:52.820
It works well.

54:52.820 --> 54:56.820
So like, yeah, what is open AI getting wrong?

54:56.820 --> 54:57.820
Technically not that much.

54:57.820 --> 54:58.820
I don't know.

54:58.820 --> 55:02.580
Like if I was a researcher, why would I go work there?

55:02.700 --> 55:03.700
Yes.

55:03.700 --> 55:06.300
So why is opening AI like the Miami Heat?

55:06.300 --> 55:08.900
No, look, I don't know.

55:08.900 --> 55:09.900
This is my technical stuff.

55:09.900 --> 55:12.740
I don't really want to harp on this, but like, why go work at opening AI when you can go

55:12.740 --> 55:13.740
work at Facebook?

55:13.740 --> 55:14.740
Right.

55:14.740 --> 55:15.740
As a researcher.

55:15.740 --> 55:20.020
Like opening AI can keep ideologs who, you know, believe ideological stuff and Facebook

55:20.020 --> 55:23.900
can keep every researcher who's like, dude, I just want to build AI and publish it.

55:23.900 --> 55:24.900
Yeah.

55:24.900 --> 55:25.900
Yeah.

55:25.900 --> 55:26.900
Awesome.

55:26.900 --> 55:27.900
Yeah.

55:27.900 --> 55:28.900
Any other thoughts?

55:28.900 --> 55:29.900
Corp, bounties?

55:29.900 --> 55:38.620
Um, yeah, so we have, you know, I've been thinking a lot about like what it means to

55:38.620 --> 55:40.740
hire in today's world.

55:40.740 --> 55:42.740
What actually is the like core?

55:42.740 --> 55:43.740
Okay.

55:43.740 --> 55:49.900
Look, I'm a believer that machines are going to replace everything in about 20 years.

55:49.900 --> 55:51.460
So okay.

55:51.460 --> 55:53.580
What is that?

55:53.580 --> 55:58.660
What is that thing that people can still do that computers can't, right?

55:59.420 --> 56:03.020
This is a narrowing list, but like, you know, back in the day, like, imagine I was starting

56:03.020 --> 56:04.020
your company in 1960.

56:04.020 --> 56:05.020
Right?

56:05.020 --> 56:08.220
Oh, we're going to have to hire a whole bunch of calculators in the basement to do all that,

56:08.220 --> 56:09.700
you know, math to support the cabinet.

56:09.700 --> 56:11.300
Dude, have you heard about computers?

56:11.300 --> 56:14.060
Why don't we just buy a few of those?

56:14.060 --> 56:16.580
Oh, oh, wow, man.

56:16.580 --> 56:17.580
You're right.

56:17.580 --> 56:19.940
Um, so like, I feel like that's kind of happening again.

56:19.940 --> 56:22.460
I'm thinking about, I will post in my discord.

56:22.460 --> 56:26.100
I'll be like, who wants to like, okay, I just changed my unary op.

56:26.100 --> 56:32.300
There used to be log and X in like E. I changed them to be log to an X to because hardware

56:32.300 --> 56:34.020
has log to an X to accelerators.

56:34.020 --> 56:35.020
Yeah.

56:35.020 --> 56:36.020
And of course you can use change of base.

56:36.020 --> 56:41.420
It's one multiply to get it back to E. But like I made the primitives log to an X to right.

56:41.420 --> 56:43.180
And this is the kind of, I just posted in the discord.

56:43.180 --> 56:46.100
I'm like, could someone put this pull request up and someone eventually did and I merged

56:46.100 --> 56:47.100
it.

56:47.100 --> 56:49.980
But I'm like, this is almost to the level where models can do it.

56:49.980 --> 56:50.980
Right?

56:50.980 --> 56:54.180
We're almost to the point where I can say that to a model and the model can do it.

56:55.060 --> 56:56.060
Have you tried?

56:56.060 --> 56:57.060
Yeah.

56:57.060 --> 56:58.940
I'm, I don't know.

56:58.940 --> 57:03.500
I'm like, I'm, I think it went further.

57:03.500 --> 57:08.620
I think autocomplete went further than I thought it would, but I'm also relatively unimpressed

57:08.620 --> 57:15.340
with these chatbots, uh, with what I've seen from the language models like there.

57:15.340 --> 57:20.180
The problem is if your loss function is categorical cross entropy on the internet, your responses

57:20.180 --> 57:21.180
will always be mid.

57:21.180 --> 57:22.180
Yes.

57:22.180 --> 57:23.180
I don't know.

57:23.180 --> 57:24.180
Mode collapse is what I call it.

57:24.180 --> 57:25.180
I don't know.

57:25.180 --> 57:26.180
I'm not even talking about mode collapse.

57:26.180 --> 57:27.180
You're actually trying to predict these.

57:27.180 --> 57:28.180
Like, like, look, I rap.

57:28.180 --> 57:32.260
I'm a, I'm a hobbyist rapper and like, would I try to get these things to write rap?

57:32.260 --> 57:34.700
The rap sound like the kind of raps you read in the YouTube comments.

57:34.700 --> 57:35.700
Nursery school.

57:35.700 --> 57:36.700
Yeah.

57:36.700 --> 57:37.700
It's like, all right, great.

57:37.700 --> 57:38.700
You're riding box with Fox.

57:38.700 --> 57:39.700
Sick rhyme, bro.

57:39.700 --> 57:45.140
Uh, you know, uh, you know, and rock or Drake is rhyming, give it up for me with napkins

57:45.140 --> 57:46.140
and cutlery.

57:46.140 --> 57:47.140
Right?

57:47.140 --> 57:48.140
Like, like, all right, come on.

57:48.140 --> 57:49.140
What's that?

57:49.140 --> 57:50.140
Like this thing about orange.

57:50.140 --> 57:51.140
Like orange is famous.

57:51.140 --> 57:52.140
Yeah.

57:52.140 --> 57:58.580
And now, of course, you know, four inch screws and orange juice is in, is in training corp.

57:58.580 --> 58:02.900
But uh, yeah, so I think it went further than like everyone kind of thought it would.

58:02.900 --> 58:06.860
But the thing that I really want to see is like somebody put 10 LLMs in a room and have

58:06.860 --> 58:08.940
them discuss the answer before they give it to me.

58:08.940 --> 58:09.940
Right?

58:09.940 --> 58:10.940
You can actually do this, right?

58:10.940 --> 58:13.100
Um, and I think the coding things have to be the same way.

58:13.100 --> 58:15.780
There is no coder alive, no matter how good you are that sits down.

58:15.780 --> 58:20.280
Well, I'm going to start at cell A one and type my program and then I'm going to press

58:20.280 --> 58:21.680
run and there's going to work.

58:21.680 --> 58:24.080
No one programs like that.

58:24.080 --> 58:25.560
So why do we expect the models to write?

58:25.560 --> 58:28.080
So, so there's a lot that like still needs to be done.

58:28.080 --> 58:31.400
But you know, at the tiny corp, I want to be on the cutting edge of this too.

58:31.400 --> 58:33.760
I want to be like program generation.

58:33.760 --> 58:34.840
I mean, what is tiny grad?

58:34.840 --> 58:38.120
It's a compiler generates programs generate the fastest program that meets the spec.

58:38.120 --> 58:39.120
Right?

58:39.120 --> 58:40.780
Why am I not just having a male do that?

58:40.780 --> 58:47.400
So you know, it's kind of a, you have to exist fluidly with the machines and I come

58:47.400 --> 58:48.400
around on a lot of stuff.

58:48.400 --> 58:51.000
And I'm like, wait, tiny grad, tiny group should be a remote company.

58:51.000 --> 58:52.000
Right?

58:52.000 --> 58:53.000
Well, I can't do this in person.

58:53.000 --> 58:54.000
Really?

58:54.000 --> 58:55.000
Yeah.

58:55.000 --> 58:57.320
Like, like comma makes sense to be in person like comma, sure.

58:57.320 --> 58:58.320
Yeah.

58:58.320 --> 58:59.320
We're getting off in San Diego.

58:59.320 --> 59:00.320
Like, but that's a six year old company.

59:00.320 --> 59:01.320
Right.

59:01.320 --> 59:03.760
And it works and it works for a certain type of people and certain type of culture, but

59:03.760 --> 59:04.760
what's going to be different this time.

59:04.760 --> 59:05.760
Okay.

59:05.760 --> 59:06.760
Remote, but now it's remote.

59:06.760 --> 59:10.720
And now I'm getting these like people who apply and I'm like, I literally have a thousand

59:10.720 --> 59:11.720
applications.

59:11.720 --> 59:14.800
I'm not calling you to do a technical screen and I can't really tell anything from a

59:14.800 --> 59:15.800
technical screen.

59:15.800 --> 59:16.800
What am I going to do?

59:16.800 --> 59:17.800
Make a code on a whiteboard?

59:17.840 --> 59:22.320
Bring up, bring up a shared notebook document so we could, oh, like that's not going to

59:22.320 --> 59:23.320
work.

59:23.320 --> 59:24.320
Okay.

59:24.320 --> 59:25.320
So then I'm moved to the next thing.

59:25.320 --> 59:27.520
We do this a comma with good success programming challenges.

59:27.520 --> 59:30.480
I've also found them to be like completely non predictive.

59:30.480 --> 59:35.520
I found one thing to actually be predictive and it's, wait a second, just write code in

59:35.520 --> 59:36.520
tiny grad.

59:36.520 --> 59:37.520
It's open source.

59:37.520 --> 59:38.520
Right.

59:38.520 --> 59:39.520
And yeah.

59:39.520 --> 59:43.160
So, you know, I'm talking to a few people who've been contributing and like contribute

59:43.160 --> 59:46.440
or you know, the job's not for you, but you can do it remote and it's like it's a

59:46.440 --> 59:47.440
chill job.

59:47.440 --> 59:49.560
Like you're not, you're like, oh yeah, well, I work for the tiny corp.

59:49.560 --> 59:51.680
Like, well, you're writing MIT license software.

59:51.680 --> 59:52.680
Like you see what it's doing.

59:52.680 --> 59:53.680
Right.

59:53.680 --> 59:56.880
Like we'll just, I think it's maybe more of like a stipend than a salary and then also

59:56.880 --> 59:57.880
some equity.

59:57.880 --> 59:58.880
Like, you know, I get rich.

59:58.880 --> 59:59.880
We all get rich.

59:59.880 --> 01:00:00.880
Yeah.

01:00:00.880 --> 01:00:01.880
Yeah.

01:00:01.880 --> 01:00:07.400
How do you think about agents and kind of like thinking of them as people versus like

01:00:07.400 --> 01:00:08.680
job to be done?

01:00:08.680 --> 01:00:13.680
Sean, build this thing called small developer and then it's in the same vein like the human

01:00:13.680 --> 01:00:18.120
in the loop with the language model and just iterating while you write code.

01:00:18.120 --> 01:00:20.440
I think, I think that's, that's absolutely where it goes.

01:00:20.440 --> 01:00:22.440
And there's like a, it's not like one thing.

01:00:22.440 --> 01:00:23.960
It's like, there's small interpreter.

01:00:23.960 --> 01:00:24.960
There's like small debugger.

01:00:24.960 --> 01:00:27.320
It's kind of like all these different jobs to be done.

01:00:27.320 --> 01:00:28.320
It's a small world.

01:00:28.320 --> 01:00:29.320
Yeah.

01:00:29.320 --> 01:00:30.320
It's a, I know this is like the small pockets.

01:00:30.320 --> 01:00:32.480
It's like small AI meet tiny corp.

01:00:32.480 --> 01:00:34.120
So we're all in the same wavelength.

01:00:34.120 --> 01:00:35.120
How do you think about that?

01:00:35.120 --> 01:00:39.840
Do you think people will have a human like interaction with like, oh, this is like the

01:00:39.840 --> 01:00:44.440
AI developer or like, is it, I'm the human being supercharged by the AI tools?

01:00:44.440 --> 01:00:49.120
Oh, I think it's much more like I'm the human supercharged by the AI tools.

01:00:49.120 --> 01:00:52.040
I think that like coding is tool complete.

01:00:52.040 --> 01:00:53.040
Right.

01:00:53.040 --> 01:00:54.040
Like driving is not tool complete.

01:00:54.040 --> 01:00:55.040
Right.

01:00:55.040 --> 01:00:57.360
Like driving is just like, like we hire people to drive who are like below the API line.

01:00:57.360 --> 01:00:58.360
Right.

01:00:58.360 --> 01:00:59.360
There's an API line in the world.

01:00:59.360 --> 01:01:00.360
Right.

01:01:00.360 --> 01:01:01.360
Love that.

01:01:01.360 --> 01:01:02.360
Yeah.

01:01:02.360 --> 01:01:03.360
There's an API line in the world and like you can think like Uber's a really clear

01:01:03.360 --> 01:01:04.360
example.

01:01:04.360 --> 01:01:05.360
Right.

01:01:05.360 --> 01:01:06.360
There's the people below the API line and the people above the API line.

01:01:06.400 --> 01:01:10.400
The way you can tell if you're below or above, by the way, is, is your manager a computer?

01:01:10.400 --> 01:01:11.400
Right.

01:01:11.400 --> 01:01:12.400
Who's the manager of the Uber driver?

01:01:12.400 --> 01:01:13.400
Well, computer.

01:01:13.400 --> 01:01:14.400
There's a machine tell you what to do.

01:01:14.400 --> 01:01:15.400
Do you tell machines what to do?

01:01:15.400 --> 01:01:16.400
Exactly.

01:01:16.400 --> 01:01:17.400
Exactly.

01:01:17.400 --> 01:01:18.400
Um, so.

01:01:18.400 --> 01:01:19.400
Coding is tool complete.

01:01:19.400 --> 01:01:20.400
Right.

01:01:20.400 --> 01:01:21.400
Coding is tool complete.

01:01:21.400 --> 01:01:22.400
Coding is above the API line.

01:01:22.400 --> 01:01:27.880
So it will always be, uh, tools supercharging your coding workflow and it will never be

01:01:27.880 --> 01:01:34.880
you performing some like task like, okay, well, I can do everything except for actually starting

01:01:34.880 --> 01:01:35.880
a docker container.

01:01:35.880 --> 01:01:36.880
Like it just doesn't make any sense.

01:01:36.880 --> 01:01:37.880
Right.

01:01:37.880 --> 01:01:38.880
Um, yeah.

01:01:38.880 --> 01:01:39.880
So it will always be sort of tools.

01:01:39.880 --> 01:01:43.000
And you know, look, we see the same stuff with all the, like people are like stable diffusion

01:01:43.000 --> 01:01:45.080
is going to replace artists or whatever.

01:01:45.080 --> 01:01:48.640
It's like, dude, like it's going to create new artists, did Photoshop replace artists?

01:01:48.640 --> 01:01:50.360
Like, what are you talking about?

01:01:50.360 --> 01:01:51.360
Right.

01:01:51.360 --> 01:01:56.320
Like, you know, real artists, finger paint, they can't use brushes, brushes are, you know,

01:01:56.320 --> 01:02:01.440
brushes are going to replace all the, okay, like, I just can't like it's all just tools

01:02:01.440 --> 01:02:03.440
and the tools are going to get better and better and better.

01:02:03.440 --> 01:02:04.440
Eventually.

01:02:04.440 --> 01:02:05.440
Yes.

01:02:05.440 --> 01:02:06.440
The tools are going to replace us.

01:02:06.440 --> 01:02:07.440
But you know, that's still 20 years away.

01:02:07.440 --> 01:02:10.520
So now I got a company in the meantime.

01:02:10.520 --> 01:02:13.440
So I've written about the API line before and I think that's from Venkatesh.

01:02:13.440 --> 01:02:15.880
I don't know if you've definitely took it from someone.

01:02:15.880 --> 01:02:16.880
It's definitely not mine.

01:02:16.880 --> 01:02:17.880
VGR.

01:02:17.880 --> 01:02:18.880
Yeah.

01:02:18.880 --> 01:02:21.520
But I also have a speculated a higher line than that, which is the Kanban board, like

01:02:21.520 --> 01:02:23.840
who tells the, the programmers what to do.

01:02:23.840 --> 01:02:24.840
Hmm.

01:02:24.840 --> 01:02:25.840
Right.

01:02:25.840 --> 01:02:29.640
So are you above or below the Kanban board at this has that evolved your management

01:02:29.640 --> 01:02:30.640
thinking?

01:02:30.640 --> 01:02:31.640
Yeah.

01:02:31.640 --> 01:02:32.640
Like that's sort of what I mean.

01:02:32.640 --> 01:02:36.400
Like I'm just going to describe the pull request in two sentences and then like, yeah,

01:02:36.400 --> 01:02:37.400
yeah.

01:02:37.400 --> 01:02:38.880
So you are running the Kanban board or the bounties?

01:02:38.880 --> 01:02:39.880
Yes.

01:02:39.880 --> 01:02:40.880
Yeah.

01:02:40.880 --> 01:02:41.880
The bounties of the Kanban board.

01:02:41.880 --> 01:02:42.880
Yes.

01:02:42.880 --> 01:02:43.880
Exactly.

01:02:43.880 --> 01:02:44.880
And that is kind of the high level.

01:02:44.880 --> 01:02:48.560
And then like, yeah, we'll get AIs to fill in some and we'll get people to fill in others.

01:02:48.560 --> 01:02:51.280
And that's also what it means to be like full time, a tiny corp.

01:02:51.280 --> 01:02:52.280
Right.

01:02:52.280 --> 01:02:55.080
Would you start, and I wrote this up pretty concretely, I'm like, okay, step one is you

01:02:55.080 --> 01:02:56.680
do bounties for the company.

01:02:56.680 --> 01:02:58.680
Step two is you propose bounties for the company.

01:02:58.680 --> 01:02:59.680
Right.

01:02:59.680 --> 01:03:00.680
You don't obviously pay them.

01:03:00.680 --> 01:03:01.680
We pay them.

01:03:01.720 --> 01:03:06.400
And I'm like, yeah, that's a good bounty that like helps with the main workflow of the company

01:03:06.400 --> 01:03:11.640
and step three is you get hired full time, you get equity, we all maybe get rich.

01:03:11.640 --> 01:03:14.400
What else are you designing differently about the employee experience?

01:03:14.400 --> 01:03:20.320
I mean, I'm very much alike, you know, some people really like to like, like keep a separation.

01:03:20.320 --> 01:03:21.320
Right.

01:03:21.320 --> 01:03:25.960
Some people really like to keep a separation between like employees and management or customers

01:03:25.960 --> 01:03:26.960
and employees.

01:03:26.960 --> 01:03:30.800
Like a comma, you know, the reason I do the dev kit thing, it's like, dude, you buy a

01:03:30.800 --> 01:03:33.760
common thing, you're an employee of the company, like you're just part of the company.

01:03:33.760 --> 01:03:35.080
It's all the same thing.

01:03:35.080 --> 01:03:39.200
There's no like secrets, there's no dividing lines, there's no like, it's all a spectrum

01:03:39.200 --> 01:03:42.840
for like, you know, down here at the spectrum, like you pay and then up here at the spectrum,

01:03:42.840 --> 01:03:43.840
you get paid.

01:03:43.840 --> 01:03:45.820
You understand this is the same spectrum of college, right?

01:03:45.820 --> 01:03:50.000
Like for undergrad, you pay and then you get up here to like, you know, doing a PhD program,

01:03:50.000 --> 01:03:51.000
you get paid.

01:03:51.000 --> 01:03:52.000
Okay.

01:03:52.000 --> 01:03:53.000
Well, cool.

01:03:53.000 --> 01:03:55.720
Welcome to the, you know.

01:03:55.720 --> 01:03:58.120
What about a comma bodies?

01:03:58.120 --> 01:04:01.240
You know, you mentioned a lot of this stuff is clearly virtual, but then there's below

01:04:01.240 --> 01:04:03.480
the API line you actually need.

01:04:03.480 --> 01:04:06.280
This is the thing that's been announced?

01:04:06.280 --> 01:04:07.280
Comma bodies?

01:04:07.280 --> 01:04:08.280
We sell them.

01:04:08.280 --> 01:04:09.280
Oh, okay.

01:04:09.280 --> 01:04:10.280
You can buy them.

01:04:10.280 --> 01:04:11.280
There's a thousand bucks on our website.

01:04:11.280 --> 01:04:12.280
Oh, okay.

01:04:12.280 --> 01:04:13.280
No, no, no.

01:04:13.280 --> 01:04:14.280
I'm thinking about like the, what Tesla announced with like the humanoid robot.

01:04:14.280 --> 01:04:15.280
It's the same thing.

01:04:15.280 --> 01:04:16.280
Yeah.

01:04:16.280 --> 01:04:17.280
Except of course we made the comma version of it.

01:04:17.280 --> 01:04:20.000
Tesla uses 20 actuators, we use two, right?

01:04:20.000 --> 01:04:25.440
Like, how do you, how do you build the simplest possible thing that can like turn the robotics

01:04:25.440 --> 01:04:27.000
problem into entirely a software problem?

01:04:27.000 --> 01:04:32.240
So right now it is literally just a comma three on a pole with two wheels.

01:04:32.240 --> 01:04:37.080
It balances, keeps the comma three up there and like there's so much you could do with

01:04:37.080 --> 01:04:38.080
that already.

01:04:38.080 --> 01:04:39.080
Right?

01:04:39.080 --> 01:04:41.800
Like this should replace, how many security guards could this replace?

01:04:41.800 --> 01:04:42.800
Right?

01:04:42.800 --> 01:04:46.920
If this thing could just competently wander around a space and take pictures and, you

01:04:46.920 --> 01:04:51.200
know, focus in on things, send you a text message when someone's trying to break it

01:04:51.200 --> 01:04:55.440
to your building, you know, like, like this could already do so much, of course, but

01:04:55.440 --> 01:04:56.680
the software is not there yet.

01:04:56.680 --> 01:04:57.680
Right?

01:04:57.680 --> 01:05:00.720
So how do we turn robotics into a thing where it's very clearly a software problem?

01:05:00.720 --> 01:05:03.280
You know, that people don't accept that self-driving cars are a software problem.

01:05:03.280 --> 01:05:05.760
I'm like, I don't know what to tell you, man.

01:05:05.760 --> 01:05:09.720
Like literally just watch the video yourself and then drive with a joystick.

01:05:09.720 --> 01:05:10.720
Right?

01:05:10.720 --> 01:05:11.720
Yeah.

01:05:11.720 --> 01:05:12.720
Can you drive?

01:05:12.720 --> 01:05:13.720
We've actually done this test.

01:05:13.720 --> 01:05:15.840
We've actually done this test where we've had someone, okay, you just watch this video

01:05:15.840 --> 01:05:18.600
and here's a joystick and you got to drive the car and of course they can drive the car.

01:05:18.600 --> 01:05:19.600
Yeah.

01:05:19.600 --> 01:05:24.000
It takes a little bit of practice to get used to the joystick, but the problem is all

01:05:24.000 --> 01:05:25.000
the model.

01:05:25.120 --> 01:05:26.120
Yeah.

01:05:26.120 --> 01:05:31.920
It is specifically anything in computer vision that you think our second most popular episode

01:05:31.920 --> 01:05:36.600
ever was about segment anything coming out of Facebook, which is as far as I understand

01:05:36.600 --> 01:05:40.880
the state of the art in computer vision, what are you hoping for there that you need for

01:05:40.880 --> 01:05:41.880
a comma?

01:05:41.880 --> 01:05:42.880
I haven't used segment anything.

01:05:42.880 --> 01:05:47.760
I mean, like the large yolos or not, I've used like large yolos and I'm super impressed

01:05:47.760 --> 01:05:48.760
by them.

01:05:48.760 --> 01:05:49.760
Yeah.

01:05:49.760 --> 01:05:50.760
You think it's solved?

01:05:50.760 --> 01:05:51.760
I got to check out segment anything.

01:05:51.760 --> 01:05:53.880
I don't think it's a distinct problem, right?

01:05:53.880 --> 01:05:54.880
Okay.

01:05:54.880 --> 01:05:55.880
Here's something that I'm interested in.

01:05:55.880 --> 01:05:56.880
All right.

01:05:56.880 --> 01:05:57.880
We have great LLMs.

01:05:57.880 --> 01:06:00.360
We have great text-to-speech models and we have great speech-to-text models.

01:06:00.360 --> 01:06:01.360
Okay.

01:06:01.360 --> 01:06:03.160
So why can I not talk to an LLM?

01:06:03.160 --> 01:06:04.680
Like at how a normal conversation with them?

01:06:04.680 --> 01:06:07.960
You can with the latency of like two seconds every time.

01:06:07.960 --> 01:06:08.960
Right.

01:06:08.960 --> 01:06:09.960
Why isn't this?

01:06:09.960 --> 01:06:11.560
And then it feels so unnatural.

01:06:11.560 --> 01:06:15.040
It's this like staccato, like I don't like the RLHF models.

01:06:15.040 --> 01:06:17.000
I don't like the two inversions of them.

01:06:17.000 --> 01:06:21.280
I think that they become, you take on the personality of a customer support agent.

01:06:21.280 --> 01:06:22.280
Right.

01:06:22.280 --> 01:06:23.280
Like, oh, come on.

01:06:23.280 --> 01:06:25.960
I like LLAMA more than Chatchapiti.

01:06:25.960 --> 01:06:29.440
Chatchapiti's personality just graded on me.

01:06:29.440 --> 01:06:30.920
Was LLAMA like cool?

01:06:30.920 --> 01:06:32.640
I write a little bit of pretext paragraph.

01:06:32.640 --> 01:06:34.680
I can put you in any scenario I want, right?

01:06:34.680 --> 01:06:36.160
Like that's interesting to me.

01:06:36.160 --> 01:06:38.440
I don't want some like, you know, yeah.

01:06:38.440 --> 01:06:47.400
So yeah, I think there is really no like distinction between computer vision and language and any

01:06:47.400 --> 01:06:49.440
of this stuff.

01:06:49.440 --> 01:06:52.400
It's all eventually going to be fused into one massive.

01:06:52.400 --> 01:06:55.000
So to say computer vision is solved, well, it doesn't make any sense because what's the

01:06:55.000 --> 01:06:59.840
output of computer vision model segmentation, like what a weird task, right?

01:06:59.840 --> 01:07:00.840
Who cares?

01:07:00.840 --> 01:07:01.840
OCR.

01:07:01.840 --> 01:07:02.840
Who cares?

01:07:02.840 --> 01:07:05.000
I don't care if you can segment which pixels make up that laptop, I care if you can pick

01:07:05.000 --> 01:07:06.000
it up.

01:07:06.000 --> 01:07:07.000
Yeah.

01:07:07.000 --> 01:07:10.000
Interactive real world.

01:07:10.000 --> 01:07:12.080
And you're going to have the local cluster.

01:07:12.080 --> 01:07:13.920
You're going to have the body.

01:07:13.920 --> 01:07:14.920
Yeah.

01:07:14.920 --> 01:07:15.920
Yeah.

01:07:15.920 --> 01:07:16.920
I think that's kind of where that goes.

01:07:17.200 --> 01:07:23.120
So maybe we can paint the future of like, the year is 2050.

01:07:23.120 --> 01:07:26.080
You've achieved all you wanted at TinyCorp.

01:07:26.080 --> 01:07:28.880
What is the AI enabled future like?

01:07:28.880 --> 01:07:30.720
Well, TinyCorp is the second company.

01:07:30.720 --> 01:07:32.000
Comma was the first.

01:07:32.000 --> 01:07:33.720
Comma builds the hardware infrastructure.

01:07:33.720 --> 01:07:35.680
TinyCorp builds the software infrastructure.

01:07:35.680 --> 01:07:38.960
The third company is the first one that's going to build a real product and that product

01:07:38.960 --> 01:07:40.960
is AI Girlfriend.

01:07:40.960 --> 01:07:43.400
No, like I'm dead serious, right?

01:07:43.400 --> 01:07:45.280
Like this is the dream product, right?

01:07:45.280 --> 01:07:47.840
This is the absolute dream product.

01:07:47.840 --> 01:07:49.640
Girlfriend is just the like.

01:07:49.640 --> 01:07:50.640
Stand-in.

01:07:50.640 --> 01:07:51.640
Well, no, it's not a stand-in.

01:07:51.640 --> 01:07:52.640
No, no, no.

01:07:52.640 --> 01:07:53.640
I actually mean it, right?

01:07:53.640 --> 01:07:56.880
So I've been wanting to merge with a machine ever since I was like, mad little, like, you

01:07:56.880 --> 01:07:59.280
know, how do I merge with a machine, right?

01:07:59.280 --> 01:08:02.120
And like you can look at like in like a maybe the Elon style way of thinking about his neural

01:08:02.120 --> 01:08:03.120
link, right?

01:08:03.120 --> 01:08:06.440
I'm like, I don't think we need any of this, right?

01:08:06.440 --> 01:08:11.200
Some of your friends, maybe they get into relationships and you start thinking of, you

01:08:11.200 --> 01:08:12.960
know, them and their partner as the same person.

01:08:12.960 --> 01:08:14.840
You start thinking of them as like one person.

01:08:14.840 --> 01:08:17.560
I mean, they are kind of like merged, right?

01:08:17.560 --> 01:08:19.840
Like humans can just kind of do this.

01:08:19.840 --> 01:08:20.840
It's so cool.

01:08:20.840 --> 01:08:22.320
It's this ability that we already have.

01:08:22.320 --> 01:08:26.920
It's all I need to put, you know, electrodes in my brain to merge with a machine.

01:08:26.920 --> 01:08:29.480
I need an AI girlfriend, right?

01:08:29.480 --> 01:08:30.480
So that's what I mean.

01:08:30.480 --> 01:08:32.880
Like this is the third product.

01:08:32.880 --> 01:08:34.560
This is the third company.

01:08:34.560 --> 01:08:38.680
And yeah, in 2050, I mean like, ah, it's so hard.

01:08:38.960 --> 01:08:44.560
Like maybe I can imagine like 2035, I don't even know 2050, like, yeah, 2035, like, yeah,

01:08:44.560 --> 01:08:45.560
that'd be really great.

01:08:45.560 --> 01:08:48.480
Like I have this like kind of, you know.

01:08:48.480 --> 01:08:53.240
So in terms of merging, like, isn't it, shouldn't you work on brain upload rather than AI girlfriend?

01:08:53.240 --> 01:08:55.240
But I don't need brain upload, right?

01:08:55.240 --> 01:08:56.680
I don't need brain upload either.

01:08:56.680 --> 01:08:59.600
Like there's, there's thousands of hours of me on YouTube, right?

01:08:59.600 --> 01:09:00.600
Yes.

01:09:00.600 --> 01:09:02.720
If you might, how much of my brain's already uploaded?

01:09:02.720 --> 01:09:04.200
That's only the stuff that you voice.

01:09:04.200 --> 01:09:06.040
Yeah, it's not that different.

01:09:06.040 --> 01:09:07.600
It's not that different, right?

01:09:07.600 --> 01:09:12.120
You really think a powerful, you really think a, a model with, you know, an exoflop of compute

01:09:12.120 --> 01:09:15.040
couldn't extract everything that's really going on in my brain.

01:09:15.040 --> 01:09:16.440
I'm a pretty open person, right?

01:09:16.440 --> 01:09:17.800
Like I'm not running a complex filter.

01:09:17.800 --> 01:09:19.520
Humans can't run that complex of a filter.

01:09:19.520 --> 01:09:20.520
Yeah.

01:09:20.520 --> 01:09:21.520
Like humans just can't.

01:09:21.520 --> 01:09:23.800
Like this is actually a cool quirk of, of, of biology.

01:09:23.800 --> 01:09:26.280
It's like, well, humans like can't lie that well.

01:09:26.280 --> 01:09:27.280
Yeah.

01:09:27.280 --> 01:09:28.280
Yeah.

01:09:28.280 --> 01:09:32.280
So is it good or bad to put all of your stream of consciousness out there?

01:09:32.280 --> 01:09:35.280
I mean, I think it's good.

01:09:35.280 --> 01:09:36.280
Yeah.

01:09:36.280 --> 01:09:37.280
I mean, I don't know.

01:09:37.280 --> 01:09:38.280
I don't know.

01:09:38.280 --> 01:09:39.280
I don't know.

01:09:39.280 --> 01:09:40.280
I don't live forever.

01:09:40.280 --> 01:09:41.280
Yeah.

01:09:41.280 --> 01:09:43.520
We said off, off Mike, we may be the first immortals, right?

01:09:43.520 --> 01:09:44.520
Yeah.

01:09:44.520 --> 01:09:45.520
Yeah.

01:09:45.520 --> 01:09:46.760
Like this is how you, this is how you live forever.

01:09:46.760 --> 01:09:49.960
It's a question of, okay, how many weights do I have?

01:09:49.960 --> 01:09:50.960
Right?

01:09:50.960 --> 01:09:51.960
Okay.

01:09:51.960 --> 01:09:52.960
Let's say I have a trillion weights.

01:09:52.960 --> 01:09:53.960
So it's talking about terabytes, a hundred terabytes here.

01:09:53.960 --> 01:09:55.880
Like if it's not really a hundred terabytes, right?

01:09:55.880 --> 01:09:56.880
Because it's called homograph complexity.

01:09:56.880 --> 01:09:58.520
How much redundancy is there in those weights?

01:09:58.520 --> 01:10:04.240
So like maximally compressed, how big is the weight file for my brain quantize it whatever

01:10:04.240 --> 01:10:05.240
you want.

01:10:05.240 --> 01:10:07.600
And quantization is, is a poor man's compression.

01:10:07.600 --> 01:10:14.080
Um, I think we're only talking really here about like maybe a couple gigabytes, right?

01:10:14.080 --> 01:10:19.120
And then if you have like a couple gigabytes of true information of yourself up there, cool

01:10:19.120 --> 01:10:21.840
man, like what does it mean for me to live forever?

01:10:21.840 --> 01:10:22.840
Like that's me.

01:10:22.840 --> 01:10:23.840
Yeah.

01:10:23.840 --> 01:10:24.840
No, I think that's good.

01:10:24.840 --> 01:10:30.200
And I think like the, there's a bit of like a professionalization of social media or like

01:10:30.200 --> 01:10:34.680
a lot of people only have what's like PC out there, you know, and I feel like you're gonna

01:10:34.680 --> 01:10:37.280
get, come back to the chat GPT thing, right?

01:10:37.280 --> 01:10:40.800
You're gonna train a model and like everything that's public about a lot of people.

01:10:40.800 --> 01:10:48.440
And it's like, no one's gonna run their model and they're gonna die on social media.

01:10:48.440 --> 01:10:51.680
Your life could depend on it.

01:10:51.680 --> 01:10:55.760
We have a segment, uh, so, uh, we're, we're moving on to a, what, what would normally

01:10:55.760 --> 01:10:58.840
be called the lightning round, but just, uh, just general takes because you're a generally

01:10:58.840 --> 01:11:00.920
interesting person with many other interests.

01:11:00.920 --> 01:11:04.520
Um, uh, what does the goddess of everything else mean to you?

01:11:05.360 --> 01:11:10.520
Oh, it means that AI is not really going to kill us.

01:11:10.520 --> 01:11:11.520
Really?

01:11:11.520 --> 01:11:12.520
Of course.

01:11:12.520 --> 01:11:14.520
Tell us more.

01:11:14.520 --> 01:11:19.120
Look, uh, Lex asked me this, like, is they are going to kill us all?

01:11:19.120 --> 01:11:22.000
And I was quick to say yes, but I don't actually really believe it.

01:11:22.000 --> 01:11:25.520
I think there's a decent chance that AI, I think there's a decent chance that AI kills

01:11:25.520 --> 01:11:27.320
95% of us.

01:11:27.320 --> 01:11:28.820
Okay.

01:11:28.820 --> 01:11:31.360
But they saw on your Twitch streams that you're with them.

01:11:31.360 --> 01:11:34.480
So they're not gonna, no, I don't think I actually, I don't know.

01:11:34.480 --> 01:11:35.440
I also think it's AI.

01:11:35.440 --> 01:11:37.600
Like I think the AI alignment problem is so misstated.

01:11:37.600 --> 01:11:41.520
I think it's actually not a question of whether the computer is aligned with the company who

01:11:41.520 --> 01:11:42.520
owns the computer.

01:11:42.520 --> 01:11:44.880
It's a question of whether that company is aligned with you or that government's aligned

01:11:44.880 --> 01:11:45.880
with you.

01:11:45.880 --> 01:11:46.880
And the answer is no.

01:11:46.880 --> 01:11:47.880
And that's how you end up dead.

01:11:47.880 --> 01:11:53.440
But, um, so what, what the goddess of everything else means to me is like, the complexity will

01:11:53.440 --> 01:11:54.440
continue.

01:11:54.440 --> 01:11:55.440
Paper clippers don't exist.

01:11:55.440 --> 01:11:58.600
You know, there are forces, the paper clippers cancer, right?

01:11:58.600 --> 01:12:02.560
The paper clippers really just a perfect form of cancer and the goddess of everything

01:12:02.560 --> 01:12:08.000
else says, yeah, if a cancer doesn't win, you know, yeah, it's a beautiful story for

01:12:08.000 --> 01:12:09.200
those who haven't heard it.

01:12:09.200 --> 01:12:11.720
And you read it out and I listen to it.

01:12:11.720 --> 01:12:12.720
Um, yeah.

01:12:12.720 --> 01:12:13.720
Good.

01:12:13.720 --> 01:12:14.720
What else we have here?

01:12:14.720 --> 01:12:15.720
Pick a question.

01:12:15.720 --> 01:12:16.720
So many.

01:12:16.720 --> 01:12:17.720
Yeah.

01:12:17.720 --> 01:12:18.720
What are you grateful for today?

01:12:18.720 --> 01:12:19.720
Oh, man.

01:12:19.720 --> 01:12:25.800
I mean, it's all just like, I haven't, I haven't taken it about this stuff forever.

01:12:25.800 --> 01:12:30.600
Like that it's actually like happening and it's happening in an accessible way too.

01:12:30.600 --> 01:12:32.480
I guess that's what I'm really grateful for.

01:12:32.480 --> 01:12:37.080
It's not like, like AI is not some Manhattan project style.

01:12:37.080 --> 01:12:38.640
You don't know anything about it.

01:12:38.640 --> 01:12:39.640
Close doors.

01:12:39.640 --> 01:12:40.640
Close doors.

01:12:40.640 --> 01:12:41.640
I'll fight really hard to keep it that way.

01:12:41.640 --> 01:12:47.440
Uh, you know, uh, that's, that's a, I'm grateful for just, just how much is released

01:12:47.440 --> 01:12:51.000
out there and how much I can just learn and stay up to date.

01:12:51.000 --> 01:12:55.680
And I guess I'm grateful to the true fabric of reality that, you know, I didn't need differential

01:12:55.680 --> 01:12:56.680
equations to understand it.

01:12:56.680 --> 01:13:00.840
Like I don't need, you don't need, you don't need some like, like, like there's, there's

01:13:00.840 --> 01:13:01.840
I've tried to do.

01:13:02.080 --> 01:13:06.040
There's a limit to my, to my math ability is I can do most undergrad math, but I took

01:13:06.040 --> 01:13:09.160
some grad math glasses and okay, now we're getting to the end of what I can do.

01:13:09.160 --> 01:13:13.880
And it's just the actual like end of what I can do, like I'm limited by my brain, but

01:13:13.880 --> 01:13:17.800
you know, ML stuff, you need high school math.

01:13:17.800 --> 01:13:18.800
Yeah.

01:13:18.800 --> 01:13:19.800
Like I could do all that.

01:13:19.800 --> 01:13:20.800
Nothing like, you know what I mean?

01:13:20.800 --> 01:13:23.440
When I learned to multiply a matrix, seventh grade, like it's all easy.

01:13:23.440 --> 01:13:26.520
You need more electrical engineering than you need high school math early.

01:13:26.520 --> 01:13:27.520
Yeah.

01:13:27.520 --> 01:13:30.120
Well, you need electrical engineering to like build the machines, but even that, like

01:13:30.120 --> 01:13:34.640
these machines are simpler than the machines that have existed before, the compute stack

01:13:34.640 --> 01:13:35.640
looks really nice.

01:13:35.640 --> 01:13:39.160
So, you know, yeah, I just, I'm grateful that it's all happening and I get to understand

01:13:39.160 --> 01:13:40.160
it be here.

01:13:40.160 --> 01:13:41.160
Yeah.

01:13:41.160 --> 01:13:42.160
Yeah.

01:13:42.160 --> 01:13:45.520
Um, John Carmack mentioned there's about six insights we have left.

01:13:45.520 --> 01:13:48.800
Do you have an intuition for what some of the paths people should be taking?

01:13:48.800 --> 01:13:51.080
Obviously you're working on one.

01:13:51.080 --> 01:13:54.600
What are some of the other branches of the tree that people should go under?

01:13:54.600 --> 01:13:56.640
I don't think I'm working on one of the six insights.

01:13:56.640 --> 01:13:59.000
I don't think tiny grads any one of the six insights.

01:13:59.000 --> 01:14:04.000
Um, something I really like that Elon does and I try to take it from, uh, try to be inspired

01:14:04.000 --> 01:14:12.120
by it is, um, look at the boring tunnel machine and ask how you can build a 10x cheaper one.

01:14:12.120 --> 01:14:13.120
All right.

01:14:13.120 --> 01:14:14.120
Look at the rocket.

01:14:14.120 --> 01:14:15.120
How can I build a 10x cheaper one?

01:14:15.120 --> 01:14:16.120
All right.

01:14:16.120 --> 01:14:17.120
Look at the electric car and say, how can I build a 10x cheaper?

01:14:17.120 --> 01:14:21.000
Like, cheaper or, you know, can go further or whatever, whatever, whatever, right?

01:14:21.000 --> 01:14:22.520
You just do the straight up physics math, right?

01:14:22.520 --> 01:14:26.440
Like, I'm trying to do the same thing with, with, uh, ML frameworks, right?

01:14:26.440 --> 01:14:31.480
And in, in, in doing so, making sure that this stuff remains accessible, right?

01:14:31.480 --> 01:14:36.800
You could imagine a world where if Google TPUs were actually the ultimate, if Google

01:14:36.800 --> 01:14:39.400
TPUs were actually the best training things, I mean, actually, you know, I'm kind of grateful

01:14:39.400 --> 01:14:40.400
for NVIDIA, right?

01:14:40.400 --> 01:14:43.760
Like, because if Google TPUs were the ultimate, now you have this huge closed source compiler

01:14:43.760 --> 01:14:49.880
in between XLA and, and the hardware and yeah, that's, uh, just a really bad thing.

01:14:49.880 --> 01:14:53.680
So I mean, something that is somewhat upsetting, but the tiny group is it, is that it is trying

01:14:53.680 --> 01:14:57.320
to prevent downside, but, uh, it's not all trying to prevent downside.

01:14:57.320 --> 01:15:00.720
Like we're also building computers and we're going to build some awesome, powerful, cheap

01:15:00.720 --> 01:15:02.920
computers, uh, along the way.

01:15:02.920 --> 01:15:05.800
Uh, so no, I'm not really working directly on any of the six tricks.

01:15:05.800 --> 01:15:10.000
I also think the six tricks are kind of going to be like, luck, I think it's going to be

01:15:10.000 --> 01:15:13.680
like, you know, please tell me more about what covariate shift is and how that inspired

01:15:13.680 --> 01:15:15.720
you to come up with batch normalization.

01:15:15.720 --> 01:15:19.760
Please tell me more about why it's a transformer and it has a query, a key and a value, right?

01:15:19.760 --> 01:15:23.200
Like Schmidt-Huber described it better and fast weights, you know?

01:15:23.200 --> 01:15:27.600
Like, like, I mean, my theory about why transformers work have nothing to do with this attention

01:15:27.600 --> 01:15:31.160
mechanism and just the fact that like it's semi-weight sharing, right?

01:15:31.160 --> 01:15:34.880
Because the weight matrix is being generated on the fly, you can, you can like compress

01:15:34.880 --> 01:15:35.880
the weight matrix.

01:15:35.880 --> 01:15:36.880
Right?

01:15:36.880 --> 01:15:40.440
Like this is what that, there's, there's an operation in the, in the transformer, which,

01:15:40.440 --> 01:15:46.000
uh, like, and by the way, this is like Qualcomm's S and PE can't run transformers for this reason.

01:15:46.000 --> 01:15:50.960
So most matrix multiplies in neural networks are weights times values, right?

01:15:50.960 --> 01:15:55.040
There is, um, you know, when you get to the, the, the outer product in, in, uh, transformers,

01:15:55.040 --> 01:15:58.480
well, it's weights times weight, it's a, it's values times values, right?

01:15:58.480 --> 01:16:01.560
So S and PE like doesn't even support that operation, right?

01:16:01.560 --> 01:16:05.360
So it's like that operation that gives the transformer its power, it has nothing to do

01:16:05.360 --> 01:16:07.280
with the fact that it's attention, right?

01:16:07.280 --> 01:16:10.320
And this is a funny like, but that is one of the six tricks, right?

01:16:10.320 --> 01:16:14.320
Batch, like these norms are a trick, transformers are a trick.

01:16:14.320 --> 01:16:15.320
Okay.

01:16:15.320 --> 01:16:16.320
Six more.

01:16:16.680 --> 01:16:21.880
Is there a reason why, so you couldn't talk, you talk about, uh, attention as weight compression,

01:16:21.880 --> 01:16:24.400
um,

01:16:24.400 --> 01:16:26.040
Compression is not exactly the right word.

01:16:26.040 --> 01:16:29.080
What I mean is that the weights can change dynamically based on the context.

01:16:29.080 --> 01:16:32.840
So it was this thing in pack eight in the hot air prize that I absolutely loved.

01:16:32.840 --> 01:16:34.840
And I've never seen it again in neural networks and a really good trick.

01:16:34.840 --> 01:16:35.840
Okay.

01:16:35.840 --> 01:16:39.460
Imagine you have 256 weight sets for a layer, right?

01:16:39.460 --> 01:16:44.000
And then you choose which of the weight sets you're loading in based on some context and

01:16:44.000 --> 01:16:45.840
that context can come from another neural net, right?

01:16:45.840 --> 01:16:51.280
So I have another one at which protect, projects, you know, 256 wide, one hot, do a soft max,

01:16:51.280 --> 01:16:54.560
predict it, and then I actually load the weights and I can do this operation at both test time

01:16:54.560 --> 01:16:55.560
and train time.

01:16:55.560 --> 01:16:58.520
I can do this operation at both training and inference, and I load in the weights given

01:16:58.520 --> 01:17:00.880
the context, right?

01:17:00.880 --> 01:17:05.960
Like that is what transformers do, but transformers instead of having 256 discrete ones, it's

01:17:05.960 --> 01:17:07.360
actually just that but continuous.

01:17:07.360 --> 01:17:08.360
Yeah.

01:17:08.360 --> 01:17:11.320
Um, which is funny that that was in language models and I just like, when I understood

01:17:11.320 --> 01:17:14.200
that about transformers, I'm like, Oh, this is a real trick and why are they using the

01:17:14.200 --> 01:17:15.200
word attention?

01:17:15.720 --> 01:17:19.240
And today is actually the anniversary of attention is all you need.

01:17:19.240 --> 01:17:20.240
What?

01:17:20.240 --> 01:17:21.240
Oh, that's a day.

01:17:21.240 --> 01:17:22.240
Today, six years ago.

01:17:22.240 --> 01:17:23.240
Six years.

01:17:23.240 --> 01:17:24.240
Six years.

01:17:24.240 --> 01:17:25.240
Change the world.

01:17:25.240 --> 01:17:26.240
Wow.

01:17:26.240 --> 01:17:27.240
Well, there's one of your envelope tricks, right?

01:17:27.240 --> 01:17:29.760
And you can easily write it on an envelope, you know, think about how you write out that

01:17:29.760 --> 01:17:32.480
how many times have you written that because it's not in any libraries because it's like

01:17:32.480 --> 01:17:34.440
all used a little differently each time.

01:17:34.440 --> 01:17:35.440
Yeah.

01:17:35.440 --> 01:17:39.880
If you just write out that exact same, you know, yeah, yeah.

01:17:39.880 --> 01:17:41.720
You've name checked Elon a few times.

01:17:41.720 --> 01:17:42.720
Yeah.

01:17:42.760 --> 01:17:47.760
Think about both of you as systems, thinkers, input, output, think something in between.

01:17:47.760 --> 01:17:48.760
Sure.

01:17:48.760 --> 01:17:53.720
Um, what, what's different about your style versus his, um, Elon's fundamental science

01:17:53.720 --> 01:17:56.240
for the world is physics minus information theory.

01:17:56.240 --> 01:17:57.240
Huh.

01:17:57.240 --> 01:17:59.000
But you, you do a lot of physics as well.

01:17:59.000 --> 01:18:02.720
I mean, like, and Elon does a lot of information theory as well too.

01:18:02.720 --> 01:18:06.560
But if the question is fundamentally that the difference maybe is expressed in what

01:18:06.560 --> 01:18:08.840
your ambitions are, right?

01:18:08.840 --> 01:18:12.400
Elon's ambitions may be like, go to Mars.

01:18:12.400 --> 01:18:13.400
Right.

01:18:13.400 --> 01:18:17.120
Go to Mars is the ultimate modern, modernist physics ambition.

01:18:17.120 --> 01:18:18.120
Right.

01:18:18.120 --> 01:18:19.120
It's a physics problem getting to Mars.

01:18:19.120 --> 01:18:20.120
Right.

01:18:20.120 --> 01:18:21.120
What are electric cars?

01:18:21.120 --> 01:18:22.120
It's a physics problem.

01:18:22.120 --> 01:18:23.120
Right.

01:18:23.120 --> 01:18:24.120
Okay.

01:18:24.120 --> 01:18:25.120
Now he's like pushing on the autonomy stuff.

01:18:25.120 --> 01:18:28.880
You push a little on information theory, but fundamentally his dreams are physics based

01:18:28.880 --> 01:18:29.880
dreams.

01:18:29.880 --> 01:18:31.120
My dreams are information based dreams.

01:18:31.120 --> 01:18:33.760
I want to live forever in virtual reality with my agri-girlfriend.

01:18:33.760 --> 01:18:34.760
Right.

01:18:34.760 --> 01:18:38.160
Those are, those are the aspirations of someone who, who, who accepts information theories

01:18:38.160 --> 01:18:39.160
of course science.

01:18:39.160 --> 01:18:40.680
So I think that's the main difference to me and him.

01:18:40.680 --> 01:18:43.400
He has physics based aspirations and I have information based aspirations.

01:18:43.400 --> 01:18:46.120
Very, very neat.

01:18:46.120 --> 01:18:47.120
Mark Andreessen.

01:18:47.120 --> 01:18:50.960
He is a, hi Mark, he's a listener.

01:18:50.960 --> 01:18:54.320
He is heavily, he's a big proponent of effective accelerationism.

01:18:54.320 --> 01:18:56.000
You've been a bit more critical.

01:18:56.000 --> 01:18:59.760
Why do you say that EAC is not taken seriously by its adherents?

01:18:59.760 --> 01:19:05.600
Oh, well, only the left takes ideology seriously.

01:19:05.600 --> 01:19:07.000
Why is that?

01:19:07.000 --> 01:19:08.760
Well, just as a fact.

01:19:08.760 --> 01:19:10.040
It's just like, it's just like a fact.

01:19:10.040 --> 01:19:11.040
Is the right more cynical?

01:19:11.040 --> 01:19:12.040
Is that, is that what it is?

01:19:12.040 --> 01:19:13.040
I don't know.

01:19:13.040 --> 01:19:17.240
It's like, it's like the left actually manages to get energy around the ideologies.

01:19:17.240 --> 01:19:18.240
Right.

01:19:18.240 --> 01:19:22.360
Like, like, like there's a lot more, look, here you have, you have two effective altruists

01:19:22.360 --> 01:19:24.200
named Sam going in front of Congress.

01:19:24.200 --> 01:19:25.200
I only one of them is in jail.

01:19:25.200 --> 01:19:27.200
Um, you know, it's, it's interesting.

01:19:27.200 --> 01:19:29.680
They're both calling for regulation in those spectra spaces, right?

01:19:29.680 --> 01:19:33.400
So SPF is definitely like kind of a wolf in sheep's clothing kind of, right?

01:19:33.400 --> 01:19:37.600
Like he, he only adopted EAC or EA to walk in.

01:19:37.600 --> 01:19:41.600
Sam Altman is a genuinely good guy who is not interested in power seeking for himself.

01:19:41.600 --> 01:19:42.600
All right.

01:19:42.600 --> 01:19:43.600
Okay.

01:19:43.600 --> 01:19:44.600
We don't, we don't have to go.

01:19:44.600 --> 01:19:45.600
Fair enough.

01:19:45.600 --> 01:19:46.600
Fair enough.

01:19:46.600 --> 01:19:50.040
Um, but, uh, no, EAC is not like, like you are not serious, right?

01:19:50.040 --> 01:19:53.520
You are not actually a serious ideology.

01:19:53.520 --> 01:19:57.840
You know, uh, Mark Andreessen, I like Mark Andreessen, but I think that like some of

01:19:57.840 --> 01:20:01.200
his Twitter things are like, dude, you like just like, it's like, it's like someone who's

01:20:01.200 --> 01:20:07.560
like 2019 who's like, eyes were opened about like the political world being not exact.

01:20:07.560 --> 01:20:09.920
You mean all the people on the news were lying to me?

01:20:09.920 --> 01:20:10.920
Yeah, bro.

01:20:10.920 --> 01:20:11.920
They were lying to you.

01:20:11.920 --> 01:20:14.120
Like, okay, we all figured this out five years ago.

01:20:14.120 --> 01:20:15.120
Now what are you going to do about it?

01:20:15.120 --> 01:20:16.560
I'm going to complain about it on Twitter.

01:20:16.560 --> 01:20:17.560
Great.

01:20:17.560 --> 01:20:18.560
And that's what EAC is.

01:20:18.560 --> 01:20:25.120
Um, last and maybe most important, uh, why was Avatar 2 bad?

01:20:25.120 --> 01:20:28.880
Oh, I have a whole, you can go on my blog.

01:20:28.880 --> 01:20:30.840
I rewrote the script of Avatar 2.

01:20:30.840 --> 01:20:34.400
I wrote a script that actually might make you feel something for the characters.

01:20:34.400 --> 01:20:37.080
I killed Jake Sully in the first scene like you had to.

01:20:37.440 --> 01:20:40.480
Do you really think his second story are topped his first one?

01:20:40.480 --> 01:20:41.480
No, of course not.

01:20:41.480 --> 01:20:44.280
You had to kill the guy and make the movie about the brothers, right?

01:20:44.280 --> 01:20:48.080
And just that alone and realizing that like you could have kept the Titanic scene.

01:20:48.080 --> 01:20:49.080
It would have been fine.

01:20:49.080 --> 01:20:50.080
I didn't even take it out.

01:20:50.080 --> 01:20:53.880
I left your Titanic scene, James Cameron, but I wrote you a story that so, you know,

01:20:53.880 --> 01:20:57.080
you just, just, just, he needs ships to sink in water.

01:20:57.080 --> 01:21:01.440
He needs, well, look, it's a great scene, but like the movie was just like, like the

01:21:01.440 --> 01:21:02.440
Roman, never.

01:21:02.440 --> 01:21:04.720
Great CGI, you know, let down by the writing, maybe.

01:21:05.600 --> 01:21:09.080
Yeah, no, but like the CGI, like it was, it was a beautiful world.

01:21:09.080 --> 01:21:10.560
And that's why like I care so much, right?

01:21:10.560 --> 01:21:13.080
Like you don't hear me ranting about Pirates of the Caribbean 2 being a

01:21:13.080 --> 01:21:15.600
terrible story because come on, what do you expect, man?

01:21:15.960 --> 01:21:18.360
Like Johnny Depp is like, wow, I had a movie that made me rich.

01:21:18.360 --> 01:21:19.360
I love this.

01:21:20.800 --> 01:21:23.640
But this goes back to like the midpoint, you know, I think you were like, feels

01:21:23.640 --> 01:21:25.120
like Chad Chippity wrote the movie.

01:21:25.160 --> 01:21:27.400
And that's my worry a little bit.

01:21:27.440 --> 01:21:29.280
It's like kind of converging towards that.

01:21:29.480 --> 01:21:31.440
Oh, I look, Malak wrote the movie.

01:21:33.440 --> 01:21:34.840
Sorry, I didn't want to interrupt you.

01:21:34.840 --> 01:21:37.880
No, I closed, I closed a pull request two days ago.

01:21:37.880 --> 01:21:39.560
I was like, was this written by Chad Chippity?

01:21:39.560 --> 01:21:40.560
And I just closed it.

01:21:40.760 --> 01:21:41.320
Like, you know what?

01:21:41.320 --> 01:21:43.600
I honestly feel bad if you were a human who wrote this.

01:21:44.200 --> 01:21:47.120
Like you're incapable of being more perfect.

01:21:47.400 --> 01:21:51.800
But if you have a classifier running in my head that asks, you know, is this

01:21:51.800 --> 01:21:53.400
an AI or is this a human?

01:21:53.400 --> 01:21:59.160
Like, you know, the only way to deal with all this, like, like, oh, God, it's

01:21:59.160 --> 01:22:00.160
like the worst possible.

01:22:00.320 --> 01:22:04.720
Like, you know, people are like, like, like, how are you mad about like these

01:22:04.720 --> 01:22:05.240
chatbots?

01:22:05.240 --> 01:22:06.560
You're not mad about like Tesla.

01:22:06.840 --> 01:22:09.840
Well, because if I don't want to buy a Tesla, I'll have to buy a Tesla.

01:22:09.880 --> 01:22:11.800
And it won't really impact my life negatively.

01:22:11.960 --> 01:22:14.880
But if I don't want to use a chatbot, it's still going to impact my life negatively.

01:22:15.120 --> 01:22:19.000
All the amount of like personalized spam that now makes me spend more cycles on

01:22:19.000 --> 01:22:22.600
my classifier to tell if it's spam or not, because you can now use AI's and

01:22:22.600 --> 01:22:23.840
generate this so cheaply.

01:22:24.480 --> 01:22:27.440
Like, no, I mean, we have to move to a model where everything's just a dollar,

01:22:27.440 --> 01:22:27.600
right?

01:22:27.600 --> 01:22:28.960
Like you want to send me an email, it's a dollar.

01:22:29.200 --> 01:22:30.000
Like you guys wouldn't care.

01:22:30.000 --> 01:22:30.800
No, my friends would care.

01:22:30.800 --> 01:22:32.680
No one would care except the spammers.

01:22:33.040 --> 01:22:33.280
Right.

01:22:33.320 --> 01:22:34.680
Like we just got to move to those sort of models.

01:22:36.440 --> 01:22:36.840
Awesome.

01:22:37.080 --> 01:22:40.520
Um, one last message you want everyone to remember?

01:22:41.680 --> 01:22:45.560
Uh, look, go, uh, go try Tanygrad.

01:22:46.160 --> 01:22:51.400
Uh, I hope that we're a serious competitor to, to what's out there.

01:22:51.760 --> 01:22:54.080
And then I want to, you know, I want to take it all the way.

01:22:54.120 --> 01:22:57.480
We'll start with just building something for GPUs and then we'll start building

01:22:57.480 --> 01:23:00.640
chips and we'll start building fabs and we'll start building silicon mines.

01:23:00.640 --> 01:23:02.880
And we'll have the first self-reproducing robot using.

01:23:03.120 --> 01:23:03.280
Yeah.

01:23:03.280 --> 01:23:03.520
Okay.

01:23:05.720 --> 01:23:06.280
All right, George.

01:23:06.320 --> 01:23:07.440
Thank you so much for coming on.

01:23:07.440 --> 01:23:08.400
Thank you for the big inspiration.

01:23:08.440 --> 01:23:08.800
Thank you.

01:23:09.080 --> 01:23:09.320
Thanks.

01:23:10.120 --> 01:23:10.400
All right.

01:23:13.280 --> 01:23:13.760
How was that?

01:23:13.760 --> 01:23:17.280
We, uh, not, not quite like, but we hope to do something.

