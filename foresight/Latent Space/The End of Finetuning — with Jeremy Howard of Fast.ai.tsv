start	end	text
0	13960	Hey everyone, welcome to the Late in Space Pockets.
13960	18400	This is Alessio, partner in CTO and residence at Decibel Partners, and I'm joined by my
18400	20840	co-host, Swix, founder of SmallAI.
20840	26680	Hey, and today we have in the remote studio Jeremy Howard from, all the way from Australia.
26680	27680	Good morning.
27760	29960	The remote studio, also known as my house.
29960	30960	Good morning.
30960	32960	Nice to see you, Ruth.
32960	34460	Nice to see you too.
34460	39840	I'm actually very used to seeing you in your mask as a message to people, but today we're
39840	40840	mostly audio.
40840	46120	But thank you for doing the very important public service of COVID awareness.
46120	51000	Once there was a pleasure, it was all very annoying and frustrating and tedious, but
51000	52000	somebody had to do it.
52000	56960	Somebody had to do it, especially somebody with your profile, I think, Julie drives home
56960	59120	the message.
59120	63920	So we tend to introduce people for them and then ask people to fill in the blanks on the
63920	66280	personal side.
66280	70800	Something I did not know about you was that you graduated with a B in philosophy from the
70800	72640	University of Melbourne.
72640	74640	I assumed you had a PhD.
74640	83080	No, I barely got through my BA because I was working 80 to 100 hour weeks at McKinsey and
83080	87760	a company from 19 years old onwards.
87760	95760	So I actually didn't attend any lectures in second and third year university.
95760	99680	Well, I guess you didn't need it or you're very sort of self-driven and self-motivated.
99680	107720	I just took two weeks off before each exam period when I was working at McKinsey and
107720	110560	then I can't believe I got away with this in hindsight.
110560	114960	I would go to all my professors and say, oh, I was meant to be in your class this semester
114960	119920	and I didn't quite turn up, were there any assignments I was meant to have done, whatever.
119920	125520	I can't believe all of them let me basically, they basically always would say like, okay,
125520	128560	well, if you can have this written by tomorrow, I'll accept it.
128560	131960	So yeah, stressful way to get through university.
131960	137840	Well, it shows that, I guess, you min-maxed the opportunities.
137840	138840	That definitely was a tricker.
139080	147160	I mean, finally, in philosophy, the things I found interesting and focused on in the
147160	151000	little bit of time I did spend on it was ethics and cognitive science.
151000	156440	And it's kind of really amazing that now come back around and those are actually genuinely
156440	159040	useful things to know about, which I never thought would happen.
159040	163240	A lot of relevant conversations there.
163240	168640	So you were a consultant for a while and then in the magical month of June, 1999, you found
168640	172280	it, both optimal decisions and fast meal, which I also really used.
172280	173280	So thank you for that.
173280	174280	Oh, good for you.
174280	175280	Yeah.
175280	178520	Because I had read the statistics switches at like 90% or something of small businesses
178520	179520	fail.
179520	182840	So I thought if I start two businesses, I have a higher chance.
182840	185400	In hindsight, I was thinking of it as some kind of stochastic thing.
185400	190720	I didn't have control over it, but it's a bit hard, but anyway.
190720	198400	And then you were president and chief scientist at Kaggle, which obviously is the composition
198400	201960	platform of machine learning.
201960	206960	And then in Lytec, where you were working on using deep learning to improve medical diagnostics
206960	207960	and clinical decisions.
207960	208960	Yeah.
208960	210920	That was actually the first company to use deep learning in medicine.
210920	213560	So it kind of founded the field.
213560	216800	And even now, that's still like a pretty early phase.
216800	222120	And I actually heard you on your new podcast with Tanishk, where you went very, very deep
222120	226560	into the stuff, the kind of work that he's doing, such a young prodigy at his age.
227560	229760	Maybe he's too old to be called a prodigy now.
229760	230760	X prodigy.
230760	233680	No, I think he still counts.
233680	238400	And anyway, just to round out the bio, you have a lot more other credentials, obviously.
238400	243880	But most recently, you started Fast.AI, which is still, I guess, your primary identity with
243880	244880	Rachel Thomas.
244880	245880	So welcome.
245880	246880	Thanks.
246880	247880	Thank you.
247880	248880	Yeah.
248880	253480	Being a lot of public service there with getting people involved in AI, and I can imagine
253480	258480	a better way to describe it than Fast.AI is you teach people from nothing to stable
258480	260680	diffusion in seven weeks or something.
260680	261680	And that's amazing.
261680	262680	Yeah.
262680	263680	Yeah.
263680	264680	I mean, it's funny.
264680	267400	When we started that, what was that like 2016 or something?
267400	271740	The idea that deep learning was something that you could make more accessible was generally
271740	277960	considered stupid, but everybody knew that deep learning was a thing that you got a math
277960	282840	through a computer science PhD, you know, those one of five labs that could give you
282840	289720	the appropriate skills, then you would join, yeah, basically from one of those labs, you
289720	292920	might be able to write some papers.
292920	300360	So yeah, the idea that normal people could use that technology to do good work was considered
300360	303200	kind of ridiculous when we started it.
303200	306080	And we weren't sure if it was possible either, but we kind of felt like we had to give it
306080	310720	a go because the alternative was we were pretty sure that deep learning was on its
310720	318800	way to becoming the most or one of the most important technologies in human history.
318800	324200	And if the only people that could use it were a handful of computer science PhDs, that seemed
324200	329440	like A, a big waste and B, kind of dangerous.
329440	333880	And you know, well, I just wanted to know one thing on your bio that at Kaggle, you
333880	337960	were also the top rank participant in both 2010 and 2011.
337960	342080	So sometimes you see a lot of founders running companies that are not really in touch with
342080	346680	the problem, but you were clearly building something that you knew a lot about, which
346680	348280	is awesome.
348280	353640	And even, yeah, talking about deep learning, you created, published a paper on ULM fit,
353640	358400	which was kind of the predecessor to multitask learning and a lot of the groundwork that
358400	360320	then went to into transformers.
360320	367080	I read back on the paper and you turn this model AWD LSTM, which I did the math and it
367080	373120	was like 24 to 33 million parameters, depending on what training data set you use today.
373120	377280	That's kind of like not even small, it's like super small.
377280	382920	What were some of the kind of like contrarian takes that you had at the time and maybe set
382920	388320	the stage a little bit for the rest of the audience on what was kind of like the state
388320	392280	of the art, so to speak at the time and what people were working towards.
392280	395480	Yeah, the whole thing was a contrarian take, you know.
395480	401840	So okay, so we started first AI, my wife and I, and we, yeah, so we're trying to think,
401840	403720	okay, how do we make it more accessible?
403720	408760	So when we started thinking about it, it was very 2015 and then 2016, we started doing
408760	409760	something about it.
409760	410760	Why is it inaccessible?
410760	417000	Okay, well, A, no one knows how to do it other than a few number of people and then when
417000	420280	we asked those few number of people, well, how do you actually get good results?
420280	424320	They would say like, oh, it's like, you know, a box of tricks that aren't published.
424320	428280	So you have to join one of the, you know, labs and learn the tricks.
428280	434320	So a bunch of unpublished tricks, not much software around, but you know, thankfully
434320	441200	there was Theano and, you know, rappers and particularly Lasagna, the rapper.
441200	448160	But yeah, not much software around, not much in the way of data sets, you know, very hard
448160	453440	to get started in terms of the compute, like how do you get that set up?
453440	458000	So you know, everything was kind of inaccessible.
458000	465280	And you know, as we started looking into it, we had a key insight which was like, you know,
466040	473440	most of the compute and data for image recognition, for example, we don't need to do it.
473440	477720	You know, there's this thing which nobody knows about, nobody talks about called transfer
477720	484040	learning where you take somebody else's model where they already figured out like how to
484040	488520	detect edges and gradients and corners and text and whatever else, and then you can fine
488520	491400	tune it to do the thing you want to do.
491400	496880	And we thought that's the key, that's the key to becoming more accessible in terms of
496880	499200	compute and data requirements.
499200	504480	So when we started FastAI, we focused from day one on transfer learning, lesson one,
504480	509120	in fact, was transfer learning, literally lesson one, something not normally even mentioned
509120	510120	in.
510120	516520	I mean, there wasn't much in the way of courses, you know, basically, really the courses out
516600	522120	there were PhD programs that had happened to have recorded their lessons, they would
522120	523720	really mention it at all.
523720	528360	We wanted to show how to do four things that seemed really useful, you know, work with
528360	535200	vision, work with tables of data, work with kind of recommendation systems and collaborative
535200	539640	filtering and work with text, because we felt like those four kind of modalities covered
539640	544600	a lot of the stuff that, you know, are useful in real life.
544680	547000	And no one was doing anything much useful with text.
547000	554080	Everybody was talking about Word2Vec, you know, like King plus, Queen minus, woman and
554080	560480	blah, blah, blah, and it was like cool experiments, but nobody was doing anything like useful
560480	561480	with it.
561480	569880	NLP was all like lamertization and stop words and topic models and diagrams and SPMs, and
569880	574760	it was really academic and not practical.
574760	582360	But I mean, to be honest, I've been thinking about this crazy idea for nearly 30 years
582360	589040	since I had done cognitive science at university, where we talked a lot about the cells Chinese
589040	593800	room experiment, this idea of like, what if there was somebody that could kind of like
593800	601000	knew all of the symbolic manipulations required to answer questions in Chinese, but they didn't
601000	606640	speak Chinese, they were kind of inside a room with no other way to talk to the outside
606640	610080	world other than taking in slips of paper with Chinese written on them and then they
610080	615280	do all their rules and then they pass back a piece of paper with Chinese back and this
615280	619520	room with a person in is actually fantastically good at answering any question you give them
619520	628320	written in Chinese, do they understand Chinese and is this something that's intelligently
628320	635280	working with Chinese ever since that time, I'd say to me the most thoughtful and compelling
635280	643080	philosophical response is yes, intuitively it feels like no, that's just because we
643080	649760	can't imagine such a large kind of system, but if it looks like a duck and acts like
649760	654160	a duck, it's a duck or to all intents and purposes.
654160	659040	And so I always kind of thought, so this is basically a kind of analysis of the limits
659040	665520	of text and I kind of felt like, yeah, if something could ingest enough text and could
665520	677000	use the patterns it saw to then generate text in response to text, it could appear to be
677000	682640	intelligent, whether that means it is intelligent or not is a different discussion and not one
682640	683880	I find very interesting.
683880	688960	Yeah, and then when I came across neural nets when I was about 20, I learned about the universal
688960	693120	approximation theorem and stuff and I started thinking like, oh, I wonder if like a neural
693120	700760	net could ever get big enough, take in enough data to be a Chinese room experiment.
700760	706920	With that background and this kind of like interest in transfer learning, I'd been thinking
706920	710320	about this thing for kind of 30 years and I thought like, oh, I wonder if we're there
710320	716640	yet, because we have a lot of text, like I can literally download Wikipedia, which is
716640	718480	a lot of text.
718480	725160	And I thought, you know, how would something learn to kind of answer questions or respond
725160	726160	text?
726160	728240	And I thought, well, what if we used a language model?
728240	731680	So language models are already a thing, you know, they were not a popular or well known
731680	732680	thing, but they were a thing.
732680	737080	But language models exist to this idea that you could train a model to fill in the gaps
737080	740960	or actually in those days, it wasn't fill in the gaps, it was finish a string.
740960	747480	In fact, Andre Kapathy did his fantastic RNN demonstration from this at a similar time
747480	753240	where he showed like you can have it ingest Shakespeare and it will generate something
753240	755440	that looks a bit like Shakespeare.
755440	763080	I thought, okay, so if I do this at a much bigger scale using all of Wikipedia, what
763200	770880	would it need to be able to do to finish a sentence in Wikipedia effectively, to do it
770880	772560	quite accurately quite often?
772560	775760	I thought, Jesus, it would actually have to know a lot about the world, you know, it
775760	779160	would have to know that there is a world and that there are objects and that objects relate
779160	783880	to each other through time and cause each other to react in ways and that causes proceed
783880	789680	effects and that, you know, when there are animals and there are people and that people
789760	794880	couldn't be in certain positions during certain timeframes and then you could, you know, all
794880	801160	that together, you can then finish a sentence like this was signed into law in 2016 by US
801160	804600	President X and it would fill in the gaps, you know.
804600	810200	So that's why I tried to create a, what in those days was considered a big language model
810200	814560	trained on the entirety on Wikipedia, which is that was a bit unheard of and my interest
814560	821120	was not in, you know, just having a language model, my interest was in like, what latent
821120	831480	capabilities would such a system have that would allow it to finish those kind of sentences?
831480	836440	Because I was pretty sure based on our work with transfer learning and vision that I could
836440	842280	then suck out those latent capabilities by transfer learning, you know, by fine-tuning
842280	844320	it on a task data set or whatever.
844320	846480	So we generated this three-step system.
846480	849840	So step one was train a language model on a big corpus.
849840	855720	Step two was fine-tune a language model on a more curated corpus and step three was further
855720	861160	fine-tune that model on a task and of course that's why everybody still does today, right?
861160	869000	That's what chat GPT is and so the first time I tried it within hours I had a new state
869000	874200	of the art academic result on IMDB and I was like, holy shit, it does work.
874760	881120	So you asked to what degree was this kind of like pushing against the established wisdom?
881120	886160	Every way, like the reason it took me so long to try it was because I asked all my friends
886160	891120	in NLP if this could work and everybody said no, it definitely won't work.
891120	895400	It wasn't like, oh maybe, everybody was like, it definitely won't work.
895400	898320	NLP is much more complicated than vision.
898320	901760	Languages are much more vastly complicated to main, you know, and you've got problems
901760	905560	like the grounding problem we know from like philosophy and theory of mind that it's actually
905560	907560	impossible for it to work.
907560	910720	So yeah, so don't waste your time.
910720	916000	Jeremy, had people not tried because it was like too complicated to actually get the data
916000	920200	and like set up the training or like were people just lazy and kind of like, hey, this
920200	921200	is just not going to work.
921200	922200	No, it was lazy.
922200	925960	So like, so the person I thought at that time who, there were two people I thought at that
925960	931280	time actually who were the strongest at language models were Stephen Merity and Alec Radford.
931800	938520	And at the time I didn't know Alec, but I, after we had both, after I'd released ULM Fit and
938520	945240	he had released GPT, I organized a chat for both of us with Kate Metz of the New York
945240	950160	Times and Kate Metz answered, and Alec answered this question for Kate and Kate just like,
950160	953880	so how did, you know, GPT come about?
953880	959680	And he said, well, I was pretty sure that pre-training on a general large corpus wouldn't work.
959680	961480	So I hadn't tried it.
961480	966280	And then I read ULM Fit and turns out it did work.
966280	969680	And so I did it, you know, bigger and it worked even better.
969680	975160	And similar with Stephen, you know, I asked Stephen Merity, like, why don't we just find,
975160	979120	you know, I'll take your AWDSTLM and like, trade it on all of Wikipedia and fine tune
979120	980120	it.
980120	983200	And he's kind of like, I don't think that's going to really lie.
983720	991240	Like two years before, I did a very popular talk at KDD, the conference where everybody
991240	993840	in NLP was in the audience.
993840	999320	I recognized after faces, you know, and I told them all this, I'm sure transfer learning
999320	1000480	is the key.
1000480	1008800	I'm sure ImageNet, you know, is going to be an NLP thing as well.
1008800	1013920	And you know, everybody was interested and people asked me questions afterwards, but
1013920	1019880	just, yeah, nobody followed up because everybody knew that it didn't work.
1019880	1028520	I mean, even like, so we were scooped a little bit by Dye and Lee at Google.
1028520	1032800	They had, I already, I didn't even realize this, it's just a bit embarrassing.
1032800	1037840	They had already done a large language model and fine tuned it.
1037880	1043160	But again, they didn't create a general purpose large language model on a general purpose
1043160	1044160	corpus.
1044160	1048400	They only ever tested a domain specific corpus.
1048400	1052840	And I haven't spoken to Kwok actually about that, but I assume that the reason was the
1052840	1053840	same.
1053840	1058800	It probably just didn't occur to them that the general approach could work.
1058800	1063680	So maybe it was that kind of 30 years of mulling over the, this whole Chinese room experiment
1063680	1066960	that had convinced me that it probably would work.
1066960	1067960	I don't know.
1067960	1068960	Yeah.
1068960	1069960	Interesting.
1069960	1074680	I just dug up Alec announcement tweet from Tony 18.
1074680	1077800	He said, inspired by Kobe, Elmo and Yola and Fit.
1077800	1082320	We showed a single transformer language model can be fine tuned to a variety.
1082320	1087600	It's interesting because, you know, today people think of the leader kind of like, kind of
1087600	1089960	like the research lab pushing forward the field.
1089960	1093240	What was that at the time, you know, like kind of like going back five years, people
1093320	1096840	think of it as an overnight success, but obviously it took a while.
1096840	1097840	Yeah.
1097840	1098840	Yeah.
1098840	1099840	No, I mean, absolutely.
1099840	1103040	And I'll say like, it's interesting that it mentioned Elmo because in some ways that
1103040	1109160	was kind of diametrically opposed to, to ULM fit, you know, there was these kind of like,
1109160	1114120	so there was a lot of, there was a lot of activity at the same time as ULM fits release.
1114120	1121480	So there was, so before it, as Brian McCann, I think at Salesforce had come out with this
1121520	1126240	neat model that did a kind of multitask learning.
1126240	1130800	But again, they didn't create a general fine tune language model first.
1130800	1136040	There was Elmo, which I think was a little, you know, actually quite a few months after
1136040	1139160	the first ULM fit example, I think.
1140040	1141400	But yeah, there was a bit of this stuff going on.
1141400	1148400	And the problem was everybody was doing, and particularly after GPT came out there and
1148400	1152600	everybody wanted to focus on zero shot and few shot learning, you know, everybody hated
1152600	1154840	fine tuning, everybody hated transfer learning.
1154840	1160560	And like I literally did tours trying to get people to start doing transfer learning.
1160560	1167040	And people, you know, nobody was interested, particularly after GPT showed such good results
1167040	1169480	with zero shot and few shot learning.
1169480	1173480	And so I actually feel like we kind of went backwards for years and, and not to be honest,
1173480	1180160	I mean, I'm a bit sad about this now, but I kind of got so disappointed and dissuaded
1180160	1186360	by like, it felt like these bigger lab, much bigger labs, you know, like fast AI had only
1186360	1192880	ever been just me and Rachel were getting all of this attention for an approach I thought
1192880	1194560	was the wrong way to do it.
1194560	1196600	You know, I was convinced was the wrong way to do it.
1196600	1200720	And so yeah, for years, people were really focused on getting better zero shot and few
1200720	1201720	shot.
1201720	1206960	And it wasn't until, you know, this key idea of like, well, let's take the ULM fit approach.
1206960	1213400	But for step two, rather than fine tuning on a kind of a domain corpus, let's fine tune
1213400	1215840	on an instruction corpus.
1215840	1220720	And then in step three, rather than fine tuning on a reasonably specific task classification,
1220720	1225320	let's fine tune on a, on a RLHF class classification.
1225320	1227840	And so that was really, that was really key, you know.
1227840	1233840	So I was kind of like out of the NLP field for a few years there, because yeah, it just
1233840	1242360	felt like, I don't know, pushing uphill against this vast tide, which I was convinced was
1242360	1245480	not the right direction, but he's going to listen to me, you know, because I, as you
1245480	1251520	said, I don't have a PhD, not at a university, or at least it wasn't then I don't have a
1251520	1256360	big set of computers to fine tune huge transformer models.
1256360	1258540	So yeah, it was definitely difficult.
1258540	1259540	It's always been hard.
1259540	1263240	You know, it's always been hard, like I've always been somebody who does not want to
1263240	1271240	build stuff on lots of big computers, because most people don't have lots of big computers.
1271240	1275800	And I hate creating stuff that most people can't use, you know, and also stuff that's
1275800	1281080	created on lots of big computers has always been like much more media friendly.
1281080	1285440	So like, it might seem like a recent thing, but actually throughout my 30 years in data
1285440	1292080	science, the attention's always been on, you know, the big iron results.
1292080	1296400	So when I first started, everybody was talking about data warehouses, and it was all about
1296400	1302440	teradata, and it'd be like, oh, this big bank has this huge room full of computers, and
1302440	1306640	they have like terabytes of data available, you know, the press for button.
1306640	1312720	And yeah, that's always what people want to talk about, what people want to write about.
1312720	1316680	And then of course, students coming out of their PhDs and stuff, that's where they want
1316680	1319920	to go work, because that's where they read about.
1319920	1327680	And to me, it's a huge distraction, you know, because like I say, most people don't have
1327680	1335960	unlimited compute, and I want to help most people, not the small subset of the most well-off
1335960	1336960	people.
1336960	1337960	Yeah.
1337960	1338960	That's awesome.
1339120	1344080	It's great to hear, you know, you do such a great job educating that a lot of times,
1344080	1348400	you're not telling your own story, you know, so I love this conversation.
1348400	1353240	And the other thing before we jump into FASTI, actually, you know, a lot of people that I
1353240	1357440	know, they run across a new architecture and one other, like, I got to start a company
1357440	1361320	and raise a bunch of money and do all of this stuff, instead you were like, I want everybody
1361320	1363960	to have access to this.
1363960	1365320	Why was that the case for you?
1365320	1369640	Was it because you already had like a successful, you know, venture and like fast mail and you
1369640	1370920	were more interested in that?
1370920	1371920	What was the reasoning?
1371920	1374320	That's a really good question.
1374320	1378760	So I guess the answer is yes, that's the reason why.
1378760	1385280	So when I was a teenager, I thought it would be really cool to like have my own company.
1385280	1388520	You know, I didn't know the word startup, I didn't know the word entrepreneur, I didn't
1388520	1392760	know the word VC, and I didn't really know what any of those things were really until
1392800	1396000	after we started Kaggle, to be honest, even though I'd started to what would now call
1396000	1402440	startups, I just thought they were just small businesses, you know, they were just companies.
1402440	1405040	So yeah, so those two companies were fast mail and optimal decisions.
1405040	1411360	Fast mail was the first kind of synchronized email provider for non-businesses, so something
1411360	1417880	you can get your same email at home on your laptop that were on your phone, whatever.
1417880	1423920	And then optimal decisions invented a new approach to insurance pricing, so they called
1423920	1426400	profit optimized insurance pricing.
1426400	1435800	So I saw both of those companies, you know, after 10 years, and at that point I had achieved
1435800	1441640	the thing that as a teenager I wanted to do, you know, it took a lot longer than it should
1441640	1444520	have because I spent way longer in management consulting than I should have because I got
1444560	1449360	caught up in that stupid rat race, but you know, eventually I got there and I remember
1449360	1454680	my mom saying to me, oh, you must be so proud, you know, because she remembered, my dreams
1454680	1457120	is like, you've done it.
1457120	1463520	And I kind of reflected and I was like, I'm not, I'm not proud at all, you know, like
1463520	1467520	people quite liked fast mail, you know, it's quite nice to have synchronized email, it
1467520	1473360	probably would have happened anyway, yeah, I'm certainly not proud that I've helped
1473400	1477800	some insurance companies suck more money out of their customers.
1477800	1482640	Yeah, no, I'm not proud, you know, it's this actually, I haven't really helped the world
1482640	1487280	very much, you know, maybe in the insurance case, I've made it a little bit worse.
1487280	1488880	I don't know.
1488880	1497880	So yeah, I was determined to not waste more years of my life doing things, working hard
1497920	1503400	to do things which I could not be reasonably sure would have a lot of value.
1503400	1508680	So, you know, I took some time off, I wasn't sure if I'd ever work again, actually, I didn't
1508680	1514320	particularly want to, because it felt like, yeah, it felt like such a disappointment.
1514320	1517960	And but you know, and I didn't need to, I had enough money, like I wasn't super rich,
1517960	1520400	but I had enough money, I didn't need to work.
1520400	1525400	And I certainly recognized that amongst the other people, I knew who had enough money
1525440	1529800	that they didn't need to work, they all worked ridiculously hard, you know, and constantly
1529800	1532440	put themselves in extremely stressful situations.
1532440	1539600	And I thought, I don't want to be one of those idiots who's tied to, you know, buying a bigger
1539600	1544520	plane than the next guy or whatever, you know, Kaggle came along and I mainly kind of did
1544520	1548640	that just because it was fun and interesting to hang out with interesting people.
1549400	1557200	But, you know, with fast AI in particular, you know, Rachel and I had a very explicit, you
1557200	1562680	know, long series of conversations over a long period of time about like, well, how can we be
1562680	1569600	the most helpful to society as a whole, and particularly to those people who maybe need
1569600	1570720	more help, you know.
1571240	1578000	And so we definitely saw the world going in a potentially pretty dystopian direction, if
1578000	1583440	the world's most powerful technology was controlled by a small group of elites.
1585440	1589920	So we thought, yeah, we should focus on trying to help that not happen.
1591000	1593360	You know, sadly, it looks like it still is likely to happen.
1593360	1598280	But I mean, I feel like we've, we've helped make it a little bit less likely.
1598320	1599720	So we've done our best.
1599720	1601520	You've shown that it's possible.
1601640	1609280	And I think, I think your constant advocacy, your courses, your research that you publish,
1609280	1615760	you know, just the other day you published a signing on, you know, learning that I think
1615760	1618800	is still something that people are still talking about quite a lot.
1619040	1625000	I think that that is the origin story of a lot of people who are going to be, you know,
1625000	1628240	little Jeremy Howard's sort of in your mission with, you know, you don't have to do
1628240	1629640	everything by yourself is what I'm saying.
1629680	1630880	Definitely, definitely.
1630880	1635440	You know, that was a, that was a big takeaway from like, and Lydic was that Lydic, it
1635440	1637800	definitely felt like we had to do everything ourselves.
1637920	1640040	And I kind of, I wanted to solve medicine.
1640160	1642720	I'll say, yeah, okay, solving medicine is actually quite difficult.
1642720	1645400	And I can't do it on my own.
1645400	1647880	And there's a lot of other things I'd like to solve, and I can't do those either.
1647880	1654000	So that was, that was definitely the other piece was like, yeah, you know, can we create
1654000	1661320	an army of passionate domain experts who can change their little part of the world.
1661800	1662760	And that's definitely happened.
1662760	1670120	Like I find nowadays, at least half the time, probably quite a bit more, that I get in
1670120	1673920	contact with somebody who's done really interesting work in some domain.
1674160	1677320	Most of the time, I'd say, they say, yeah, I got my start with fast AI.
1678360	1680400	So it's definitely, I can, I can see that.
1680400	1686760	And I also know from talking to folks at places like Amazon and Adobe and stuff, which, you
1686760	1687960	know, there's lots of alumni there.
1687960	1691360	And they say, oh my God, I got here in like half of the people who are fast AI alumni.
1692240	1693840	So it's fantastic.
1694640	1698600	Yeah, actually, Andre Capati grabbed me when I saw him at Europe's a few years ago.
1698600	1701320	And he's like, I have to tell you thanks to the fast AI courses.
1701320	1704720	When people come to Tesla and they need to know more about deep learning, we always
1704720	1705720	send them to your course.
1706440	1709280	And the OpenAI scholars program was doing the same thing.
1709640	1717480	So it's kind of like, yeah, it's had a surprising impact, you know, that's just one
1717480	1723320	of like three things we do is the course, you know, and it's, it's, it's only ever
1723320	1727160	been at most two people, either me and Rachel or me and Silver.
1727200	1728160	Nowadays, it's just me.
1729200	1733560	So, yeah, I think it shows you don't necessarily need a huge amount of money and a
1733560	1736840	huge team of people to, to make an impact.
1737760	1738040	Yeah.
1738920	1744600	So just to reintroduce fast AI for people who may not have dived into it much.
1744960	1747080	There is the courses that you do.
1747440	1752160	There is the library that is, that is very well loved.
1752160	1757200	And I kind of think of it as a nicer layer on top of PyTorch that people should
1757200	1761080	start with by default and use it as the basis for a lot of your courses.
1762280	1766760	And then you have, you have like NB Dev, which I don't know, is that the third
1766760	1772520	one? Oh, so the three areas were research, software, and, and courses.
1772560	1773000	Oh, sorry.
1773000	1773520	I was going by.
1773520	1781960	So then in software, you know, fast AI is the main thing, but NB Dev is not far
1781960	1789240	behind, but then there's also things like Fastcore, GHAPI, I mean, dozens of open
1789240	1795360	source projects that I've created and some of them have been pretty popular.
1795360	1797320	And some of them are still a little bit hidden.
1797320	1801040	Actually, I should, some of them I should try to do a better job of telling people
1801040	1802040	about. What are you, what are you thinking about?
1802600	1802840	Yeah.
1802840	1807840	What, what's on this little things like, for example, for working with EC2 and AWS,
1807840	1812360	I created a fast EC2 library, which I think is like way more convenient and nice
1812360	1813880	to use than anything else out there.
1814400	1817800	And it's literally got a whole autocomplete dynamic autocomplete that works
1817800	1821440	both on the command line and in notebooks, sort of like autocomplete your
1821440	1823920	instance names and everything like that.
1824120	1825480	You know, just little things like that.
1825480	1832400	I try to make like, when I work with some domain, I try to make it like, I want
1832400	1835320	to make it as enjoyable as possible for me to do that.
1835640	1840680	So I always try to kind of like, like with GHAPI, for example, I think that
1840680	1845640	GitHub API is incredibly powerful, but I didn't find it good to work with
1845640	1847680	because I didn't particularly like the libraries that are out there.
1847680	1852920	So like GHAPI, like Fast EC2, it like autocompletes both at the command
1852920	1857800	line or in a notebook or whatever, like literally the entire GitHub API.
1859680	1863520	The entire thing is like, I think it's like less than a hundred K of code
1863640	1869480	because it actually, as far as I know, the only one that grabs it directly
1869480	1873440	from the official open API spec that GitHub produces.
1874160	1881480	And like, if you're in GitHub and you just type an API, you know, autocomplete
1881720	1888840	API method and it enter, it prints out the docs or the six brief docs and
1888840	1892640	then gives you a link to the actual documentation page, you know, GitHub
1892640	1896480	actions I can write now in Python, which is just so much easier than writing
1896480	1898760	them in typescript and stuff.
1898760	1900840	So, you know, just little things like that.
1901120	1904800	I think that's a approach that more, I wish more developers took to publish
1905080	1906200	some of the work along the way.
1907200	1911000	You describe the third arm of Fast EIS research.
1911120	1912960	It's not something I see often.
1912960	1918080	Obviously, you do do some research and how do you run your research?
1918240	1919440	What are your research interests?
1919840	1920120	Yeah.
1920120	1923240	So research is what I spend the vast majority of my time on.
1924120	1931400	And the artifacts that come out of that are largely software and courses, you
1931400	1935640	know, so to me, the main artifact shouldn't be papers because papers are
1935640	1938120	things read by a small exclusive group of people.
1938200	1942760	You know, to me, the main artifacts should be like something teaching you
1942760	1945800	people, here's how to use this insight and here's software you can use that
1945920	1947160	builds it in.
1949320	1953000	So I think I've only ever done three first person papers in my life, you know,
1953160	1957320	and they were, and none of those are ones I wanted to do, you know, they were all
1957320	1961520	once like, so one was ULM fit where Sebastian Ruda reached out to me after
1961520	1965160	seeing the course and said, like, you have to publish this as a paper, you know.
1965720	1968160	And he said, I'll write it.
1970280	1971320	He said, I want to write it.
1971320	1973600	Cause if I do, I can put it on my PhD and that would be great.
1973600	1977280	And it's like, okay, well, I want to help you with your PhD and that sounds great.
1977280	1983520	So like, you know, one was the masks paper, which just had to exist and nobody
1983520	1984640	else was writing it.
1984760	1993920	And then the third was the fast AI library paper, which again, somebody reached
1993920	1996440	out and said, please, please write this.
1996440	2001280	We will waive the fee for the journal and everything and actually help you get it
2001280	2002440	through publishing and stuff.
2002480	2006520	So yeah, so I don't, other than that, I've never written a first author paper.
2007200	2012960	So the research is like, well, so for example, you know, Don Bench was a
2012960	2016240	competition which Stanford ran a few years ago.
2018000	2021880	It was kind of the first big competition of like, who couldn't train
2021960	2024960	neural nets the fastest rather than the most accurate.
2025960	2031760	And specifically it was who couldn't train ImageNet the fastest.
2032760	2037000	And this was like one of these things where it was created by necessity.
2037360	2039600	So Google had just released their TPUs.
2040200	2044680	And so I heard from my friends at Google that they had put together this big team
2044920	2049800	to smash Don Bench so that they could prove to people that they had to use
2050000	2053240	Google Cloud and use their TPUs and show who could their TPUs were.
2054040	2057200	And we kind of thought, oh, shit, this would be a disaster if they do that
2057200	2060120	because then everybody's going to be like, oh, deep learning is not accessible.
2060520	2063240	You know, to actually be good at it, you have to be Google and you have to
2063240	2064680	use special silicon and so.
2065040	2069360	So, you know, we, we only found out about this 10 days before the competition finished.
2070120	2075320	But, you know, we basically got together an emergency bunch of our students and
2075360	2081640	Rachel and I and sat for the next 10 days and just tried to crunch through and
2082520	2087240	tried to use all of our best ideas that had come from our research.
2088320	2092400	That's a particularly progressive resizing, which is basically train mainly on small things.
2093480	2097200	Train on non-square things, you know, stuff like that.
2097600	2100160	And so, yeah, we ended up winning.
2100960	2101480	Thank God.
2102080	2106480	And so, you know, we turned it around from being like, oh, shit, you know, this is
2106480	2109440	going to show that you have to be Google and have TPUs to being like, oh, my God,
2109440	2111720	even the little guy can do deep learning.
2113560	2118360	So that's an example of the kind of like research artifacts we do.
2118840	2124320	And yeah, so all of my research is always, how do we do more with less, you know,
2124320	2129840	so how do we get better results with less data, with less compute, with less complexity.
2130520	2134480	With less education, you know, stuff like that.
2134480	2137560	So your LLM fits obviously a good example of that.
2138960	2142680	And most recently you published, can LLMs learn from a single example?
2143960	2146080	Maybe could you tell the story a little bit behind that?
2146080	2150160	And maybe that goes a little bit too far into the learning on very low resource.
2151920	2152760	The literature.
2153200	2153880	Yeah.
2154520	2154880	Yeah.
2154880	2161320	So me and my friend, John O'Whittaker, basically had been playing around with
2161320	2165240	this fun Kaggle competition, which is actually still running as we speak,
2165240	2173280	which is, can you create a model which can answer multiple choice questions
2173280	2175240	about anything that's in Wikipedia?
2175920	2182400	And the thing that makes it interesting is that your model has to run on Kaggle
2182760	2186040	within nine hours and Kaggle is very, very limited.
2186040	2191680	So you've only got 14 gig RAM, only two CPUs and a small, very old GPU.
2193680	2196720	So this is cool, you know, if you can do well at this, and this is a good example
2196720	2198240	of like, oh, you can do more with less.
2199600	2205640	So yeah, John O and I were playing around with fine tuning, of course, transfer
2205640	2208120	learning, pre-trained language models.
2209160	2215000	And we saw this like, so we always, you know, plot our losses as we go.
2215040	2218120	So here's another thing we created, we actually, Sylvain Gougir, when he worked
2218120	2222760	with us created called Fast Progress, which is kind of like TQEDM, but we think
2222760	2223200	a lot better.
2223600	2227480	So you look at our Fast Progress curves, and they kind of go down, down, down,
2227480	2229720	down, down, down, a little bit, a little bit, a little bit, and then suddenly
2229720	2231400	go clunk, and they drop.
2232000	2235000	And then down, down, down, down, a little bit, and then suddenly clunk, they drop.
2235000	2236560	We're like, what the hell?
2236560	2239800	These clunks are occurring at the end of each epoch.
2240560	2247040	So normally in deep learning, this would be, you know, I've seen this before,
2247040	2248040	it's always been a bug.
2248680	2251880	It's always turned out that like, oh, we accidentally forgot to turn on
2251880	2253680	eval mode during the validation set.
2253680	2259440	So I was actually learning then, or, oh, we accidentally were kept letting moving
2259440	2261480	average statistics throughout the epoch.
2261520	2264360	So, you know, for it's recent, the moving average or whatever.
2264360	2266680	And so we were using hugging face trainer.
2267280	2270680	So, you know, I did not give my friends at hugging face the benefit
2270680	2271200	of the doubt.
2271200	2276040	I thought, oh, they fucked up hugging face trainer, you know, idiots.
2276160	2278560	Well, you'll use the faster trainer instead.
2278560	2279840	So we switched over to learner.
2279840	2281200	We still saw the clunks.
2281680	2288120	And, you know, that's, yeah, it shouldn't really happen because semantically
2288120	2293080	speaking in the epoch isn't like, it's not a thing, you know, like nothing
2293080	2297240	happens or nothing's meant to happen when you go from ending one epoch to
2297240	2298240	starting the next one.
2300480	2301560	So there shouldn't be a clunk.
2302400	2305280	You know, so I kind of asked around on the open source discords.
2305560	2308880	That's like, what's going on here?
2309200	2311600	And everybody was just like, oh, that's just what, that's just what these
2311600	2312560	training curves look like.
2312720	2313680	Those all look like that.
2313880	2314520	Don't worry about it.
2315080	2317360	That's like, oh, are you all using trainer?
2317520	2318200	Yes.
2318320	2320440	Oh, well, there must be some buck with training.
2320440	2323160	And it's like, well, we also saw it in learner and somebody else is like, no,
2323160	2324120	we've got our own trainer.
2324120	2324880	We get it as well.
2325320	2326240	They're just like, don't worry about it.
2326240	2327200	It's just something we see.
2327920	2328560	It's just normal.
2329040	2330080	I can't do that.
2330080	2334640	I can't just be like, here's something that's like in the previous 30 years of
2334640	2336520	neural networks, nobody ever saw it.
2336720	2337960	And now suddenly we see it.
2339160	2340120	So don't worry about it.
2340440	2342240	I like, I just, I have to know why.
2342520	2346440	Can I clarify this is, was everyone that you're talking to, were they all
2346440	2348760	seeing it for the same data set or in different data sets?
2348880	2351960	Data, different data sets, different trainers.
2351960	2354360	They're just like, no, this is just, this is just what it looks like when
2354360	2355640	you fine tune language models.
2355640	2356320	Don't worry about it.
2356560	2361360	You know, as I say, I hadn't seen it before, but I'd been kind of like, as I
2361360	2364880	say, I, you know, I kept working on them for a couple of years after ULM fit.
2364880	2368400	And then I kind of moved on to other things, partly out of frustration.
2368640	2375040	So I hadn't been fine tuning, you know, um, I mean, Lamar's only been out for a
2375040	2375760	few months, right?
2375760	2379480	But I, I, I, I, I wasn't one of those people who jumped straight into it, you
2379480	2385000	know, so I was relatively new to the kind of Lamar fine tuning world, or else
2385560	2389400	these guys had been, you know, doing it since day one.
2390320	2393080	Um, it's only a few months ago, but it's still quite a bit of time.
2393080	2395880	So, so yeah, they're just like, no, this is all what we see.
2396280	2396960	Don't worry about it.
2397960	2401880	So yeah, I, I've got a very kind of like, I don't know, I've just got this
2401880	2403680	brain where I have to know why things are.
2404200	2407160	And so I kind of, I asked people like, well, why, why do you think it's happening?
2407160	2410280	And they'd be like, oh, we're pretty obviously, cause it's like memorized the
2410280	2410800	data set.
2411920	2414520	It's just like, it can't be right.
2414520	2415640	It's only seen it once.
2415680	2416480	Like, look at this.
2416480	2424040	The, the losses dropped by 0.3, 0.3, which is like, basically it knows the answer.
2425040	2430000	Um, they're like, no, no, it's just, it is, it's just memorized the data set.
2430000	2430880	So yeah.
2430880	2435680	So look, John, when I did not discover this and John O and I did not come up with a
2435680	2439040	hypothesis, you know, I guess we were just the ones, I guess, who had been around
2439040	2442560	for long enough to recognize that like this, this isn't how it's meant to work.
2442920	2447840	And so we, you know, and so we went back and like, okay, let's just run some
2447840	2450840	experiments, you know, cause nobody since we've actually published anything about
2450840	2453960	this, um, well, not quite true.
2453960	2456480	Some people have published things, but nobody ever actually stepped back and
2456480	2459840	said like, what the hell, you know, how can this be possible?
2459840	2460600	Is it possible?
2460600	2461480	Is this what's happening?
2461840	2465520	And so yeah, we created a bunch of experiments where we basically predicted
2465520	2466080	ahead of time.
2466080	2468960	It's like, okay, if this hypothesis is correct, that it's memorized in the
2468960	2473000	training set, then we ought to see blah under conditions, blah, but not only these
2473000	2473600	conditions.
2474240	2477280	And so we ran a bunch of experiments and all of them supported the hypothesis
2478280	2481560	that it was memorizing the data set in a single thing at once.
2482120	2490600	Um, and it's a pretty big data set, you know, um, which in hindsight, it's not
2490600	2495240	totally surprising because the theory, remember, of the ULM fit theory was like
2495240	2500240	what's kind of creating all these latent capabilities to make it easier for it to
2500240	2501360	predict the next token.
2501880	2506320	So if it's got all this kind of latent capability, it ought to also be really
2506400	2511080	good at compressing new tokens because it can immediately recognize that it's
2511080	2512680	like, oh, that's just a version of this.
2513920	2521880	So it's, it's not so crazy, you know, but it is, um, it requires us to rethink
2521880	2526760	everything because like, and nobody knows like, okay, so how do we fine tune
2526760	2527320	these things?
2527320	2529560	Because like, it doesn't even matter.
2530240	2531320	Like maybe it's fine.
2531680	2534760	Like maybe it's fine that it's memorized the data set after one go and you do a
2534760	2540200	second go and okay, the validation loss is terrible because it's now really
2540200	2541040	overconfident.
2541920	2542480	That's fine.
2542600	2545960	Don't, you know, don't keep telling people, don't track validation loss,
2545960	2549920	track validation accuracy, um, because at least that, that will still be useful.
2550520	2554400	Um, there's another thing that's got lost since ULM fit, nobody tracks accuracy
2554400	2555600	of language models anymore.
2556400	2561000	Um, but you know, it'll still keep learning and it does, it does keep improving.
2561240	2563880	But is it worse?
2564240	2568320	You know, like, is it like now that it's kind of memorized it, it's probably
2568320	2573960	getting a less strong signal, you know, um, I don't know.
2574200	2576720	So I still don't know how to fine tune language models properly.
2576720	2580760	And I haven't found anybody who feels like they do, like nobody really knows
2580760	2585760	whether this memorization thing is, it's probably a feature in some ways.
2585760	2587760	There's probably some things that you can do usefully with it.
2587920	2593560	It's probably, yeah, I have a feeling it's messing up training dynamics as well.
2594360	2597480	It doesn't come at the cost of catastrophic forgetting as well, right?
2597480	2598920	Like, which is the other side of the coin.
2599280	2605640	Um, it does, um, to some extent, like, we know it does like look at CodeLama,
2605640	2606280	for example.
2606280	2611720	So CodeLama was a, I think it was like a 500 billion token fine tuning of
2611720	2616800	CodeLama to using code and also pros about code that Meta did.
2617440	2623040	And, um, honestly, they kind of blew it because CodeLama is good at coding,
2623040	2624160	but it's bad at everything else.
2624680	2625720	You know, and it used to be good.
2626120	2626400	Yeah.
2626400	2630320	I was pretty sure it was like, before they released it at me and lots of people
2630320	2633240	in the open source discords were like, Oh my God, you know, we know this is
2633240	2633600	coming.
2633600	2634800	You're not going to say it's coming.
2634800	2638800	I, I hope they kept at least like 50% long code data because otherwise
2638800	2639960	it's going to forget everything else.
2640440	2647000	And they didn't only like 0.3 of their 0.3% of their epochs were non-code data.
2647400	2648840	So I did it, forgot everything else.
2648880	2652040	So now it's good at code and it's bad at everything else.
2652840	2654640	So we definitely have catastrophic forgetting.
2654640	2655480	It's fixable.
2655520	2660880	Just somebody has to do, you know, somebody, somebody has to spend their time
2660880	2664280	training a model on a, a good mix of data.
2664360	2665320	Like, so, okay.
2665320	2665960	So here's the thing.
2666960	2672840	Um, even though I originally created the three step approach that everybody
2672840	2676560	now does, my view is it's actually wrong and we shouldn't use it.
2677000	2685520	Um, um, and that's because people are using it in a way different to why I
2685520	2686040	created it.
2686040	2690560	You know, I created it thinking that the tasks specific models would be more
2690560	2694760	specific, you know, it's like, Oh, this is like a sentiment classifier.
2694880	2700960	This is an example of a task, you know, but the tasks now are like a, um, you
2700960	2704040	know, RLHF, which is basically like answer questions that make people feel
2704040	2705040	happy about your answer.
2705320	2709280	So that's a much more general task and it's a really cool approach.
2709520	2714880	And so we see, for example, RLHF also breaks models.
2714880	2720880	Like, you know, like GPT for RLHDEFT, we know from kind of the, the work that
2720880	2725720	Microsoft did, you know, the pre, the, the earlier less aligned version was better.
2726920	2729840	Um, and these are all kind of examples of catastrophic forgetting.
2730200	2736960	And so to me, the right way to do this is to fine-tune language models is to
2736960	2738720	actually throw away the idea of fine-tuning.
2738880	2739640	There's no such thing.
2740360	2741800	There's only continued pre-training.
2742640	2747280	Uh, and pre-training is something where from the very start, you try to include
2747280	2751120	all the kinds of data that you care about, all the kinds of problems that you
2751120	2757760	care about, instructions, exercises, code, general purpose document completion,
2758760	2759200	whatever.
2761040	2766440	And then as you train, you gradually curate that, you know, you gradually
2766440	2769960	make that higher and higher quality and more and more specific to the kinds of
2769960	2770840	tasks you want it to do.
2772080	2774600	Um, but you never throw away any data.
2774840	2780160	You always keep all of the data types there in reasonably high quantities.
2780640	2785720	Um, you know, maybe the quality filter, you stop training on low-quality data.
2786240	2789120	Cause that's probably fine to forget how to write badly, maybe.
2789840	2795560	Um, so yeah, that's now my view is I think ULM fit is the wrong approach.
2796080	2801120	Um, and that's why we're seeing a lot of these, uh, you know, so-called alignment
2801120	2805720	tax and this view of like, oh, a model can't both code and do other things.
2805840	2807920	You know, I think it's actually cause people are training them wrong.
2809240	2813720	Well, I think you have a clear anti-laziness approach.
2813840	2817760	I think other people are not as, uh, good hearted, you know, they're like,
2817760	2819720	Hey, they told me this thing works.
2819880	2823240	And if I release a model this way, people will appreciate it.
2823240	2826440	I'll get promoted and I'll kind of make, make more money.
2826920	2827760	Oh, absolutely.
2828280	2828600	Yeah.
2828600	2829440	And it's not just money.
2829440	2833440	It's like, this is how citations work most, most badly, you know, so if you
2833440	2837560	want to get cited, you need to write a paper that people in your field recognize
2837560	2841640	as an advancement on things that we know are good.
2842160	2844320	And so we've seen this happen again and again.
2844400	2849200	So like I say, like zero shot and a few shot learning, everybody was writing about
2849200	2853360	that or, you know, with, um, image generation, every, everybody just was writing
2853360	2857880	about GANs, you know, and I was trying to say like, no, GANs are not the right approach.
2857880	2861080	You know, when I showed again through research that we demonstrated in our
2861080	2867400	videos, that you can do better than GANs much faster and with much less data.
2868320	2872400	And nobody cared because again, like if you want to get published, you rewrite
2872400	2877560	a GAN paper that slightly improves this part of GANs and this tiny field, you'll,
2878520	2883600	you'll get published, you know, so it's, yeah, it's not set up for real innovation.
2884160	2892120	Um, it's, you know, it's, again, it's really helpful for me, you know, have my own
2892120	2896240	research lab with nobody telling me what to do and I don't even publish.
2896240	2898040	So it doesn't matter if I get citations.
2898400	2901360	So I just write what I think is actually matters.
2901800	2906680	Um, I wish there was, and you know, it actually places like open AI, you know,
2906680	2908520	the researchers there can do that as well.
2909160	2913160	It's a shame, you know, I wish there was more academic, open,
2913280	2918200	venues in which people can focus on like genuine innovation.
2918720	2924000	Uh, Twitter, which is, uh, unironically has, has become a little bit of that form.
2924360	2927880	Uh, I wanted to follow up on one thing that you mentioned, uh, which is that
2927880	2930360	you checked around the open source discords.
2930720	2934800	Uh, I don't know if it's, uh, to, uh, I don't know if it's a kosher to ask
2934800	2938840	like what discords are lively, uh, or useful right now.
2939040	2943560	Um, I think that something I definitely felt like I missed out on was the early
2943560	2946840	days of Luther AI, which where, which is a fair hot bit.
2947200	2950760	And, uh, you know, like what is the new Luther and you were, you actually
2950760	2954000	shouted out the alignment lab AI discord in your blog posts.
2954000	2956880	And it was the first time I even knew, like I saw them on Twitter and never
2956880	2958840	knew they had a discord, never knew that there was actually
2958840	2962240	substantive discussions going on in there and that you were an active member of it.
2962760	2963040	Okay.
2963040	2963280	Yeah.
2963320	2966160	And then even then, if you do know about that, you go there, it'll look like
2966160	2967000	it's totally dead.
2967400	2970680	And that's because unfortunately, nearly all the discords, nearly all of
2970680	2976320	the conversation happens in private channels, you know, um, and how does
2976320	2978560	someone get into that world?
2978560	2982720	Cause it's obviously very, very, um, instructive, right?
2982880	2986280	You could just come to the first day I discord, which I'll be honest with you,
2986280	2992000	it's less bustling than some of the others, but it's not terrible.
2992360	2996760	And so like, at least, you know, to be fair, one of Emma's bustling
2996760	2997720	channels is private.
2999080	2999400	I guess.
3001120	3002080	So I'm just thinking, why is that?
3002080	3003560	It's just a nature of quality discussion, right?
3004040	3007880	Yeah, I guess when I think about it, like, I didn't have any private
3007880	3009760	discussions on a discord for years.
3010160	3014840	Um, but there was a lot of people who came in with like, oh, I just had
3014840	3017120	this amazing idea for AGI.
3017120	3021640	If you just thought about like, if you imagine the AI is a brain and we, you
3021680	3023680	know, this just, I don't want to talk about it.
3023840	3027560	You know, I don't want to like, but you don't want to be dismissive or whatever.
3027560	3030000	And it's like, oh, well, that's an interesting comment, but maybe you should
3030000	3033040	like try training some models first to see if that aligns with your intuition.
3033040	3034480	Like, oh, but how can I possibly learn?
3034480	3038080	It's like, well, we have a course just actually spend time learning.
3038080	3039120	Like, uh, yeah.
3039160	3039480	Anyway.
3040120	3043960	And there's like, okay, I know the people who always have good answers there.
3043960	3046640	And so I created a private channel and put them all in it.
3046720	3050280	And I got to admit, I, that's where I post more often because.
3051280	3057520	There's much less, you know, flight of fancy views about how we could solve AGI,
3057520	3058040	blah, blah, blah.
3058360	3062880	So there is a bit of that, but having said that, like, I think the bar is pretty
3062880	3070680	low, like if you join a discord and you can hit the, like participants or
3070680	3072600	community or whatever button and you can see who's in it.
3072600	3076320	And you'll see at the top who, who the admins or moderators or people in the dev
3076440	3085120	role are and, uh, just DM one of them and say, like, oh, I, here's my GitHub.
3085600	3089440	Well, here's some blog posts they wrote, you know, I'm interested in talking about
3089440	3094120	this, you know, can I join the private channels and I've never heard of anybody
3094120	3100320	saying no, I will say, you know, uh, a Luther's all pretty open.
3100840	3104200	So you can do the Aleutha discord still, you know, one problem with your
3104200	3109040	Luther discord is it's been going on for so long that it's like, it's very
3109040	3110040	inside baseball.
3110320	3112680	It's hard to, it's hard to get started.
3113080	3113280	Yeah.
3113280	3118760	Um, Kappa AI looks, I think it's all open.
3119840	3122200	This just left a stability.
3122200	3123160	That's more accessible.
3123760	3123960	Yeah.
3124000	3131000	Um, uh, there's also, uh, just recently, uh, now three search that
3131000	3136160	does like the Hermes models and data set just, just opened, they've got some
3136160	3138000	private channels, but it's pretty open.
3138000	3141720	I think, uh, you mentioned alignment lab, that one, it's all the interesting
3141720	3142800	stuff is on private channels.
3142800	3148720	So just ask, um, if, if you know me, ask me, cause I've got admin on that one.
3149000	3154920	There's also, yeah, uh, OS skunkworks, OS skunkworks AI is a good discord,
3154920	3157760	which I think it's open.
3158880	3160320	So they're, yeah, they're all pretty good.
3160520	3164560	I don't want you to leak any, any, uh, you know, uh, discords that don't
3164560	3169720	want any publicity, but we all want people, like we all want people.
3169720	3175040	We just, we just want people who like want to build stuff, you know, um, rather
3175040	3181520	than people who, and like, it's fine to not know anything as well, but if you
3181520	3184360	don't know anything, but you want to tell everybody else what to do and how to
3184360	3185200	do it, that's annoying.
3185520	3190080	If you don't know anything and want to be told, like, here's a really small kind
3190080	3193320	of task that as somebody who doesn't know anything, it's going to take you a
3193320	3195560	really long time to do, but it would still be helpful.
3195880	3197040	Then, and then you go and do it.
3197080	3197640	That would be great.
3198200	3202600	The truth is, yeah, like, I don't know, maybe 5% of people who come in with
3202600	3205280	great enthusiasm and saying that they want to learn and they'll do anything.
3205280	3207400	And then somebody says like, okay, here's some work you can do.
3207880	3209240	Almost nobody does that work.
3209760	3216040	So if you're somebody who actually does the work and follows up, you will massively
3216040	3216800	stand out.
3216920	3220920	That's an extreme rarity and everybody will then want to help you do more work.
3221080	3227160	So, yeah, so just, um, yeah, just do work and people will want to support you.
3227920	3231160	Our discord used to be referral only for a long time.
3231240	3235120	We then have a public invite and then we opened it and they're kind of like
3235120	3235840	channel gating.
3236280	3238360	Um, yeah, a lot of people just want to do.
3238360	3240600	I remember it used to be like, you know, a forum moderator.
3240880	3244680	It's like people just want to do like drive by posting, you know, I'm like, they
3244680	3245880	don't want to help the community.
3245880	3247520	They just want to get their question answered.
3247760	3253000	I mean, the funny thing is our, um, our forum community does not have any of that
3253000	3257080	garbage, you know, there's something specific about the low latency thing.
3257080	3263320	There were people like, they expect an instant answer and yeah, we're all somehow
3263320	3267120	in a forum's tread where they know it's like they're forever.
3268040	3273880	People are a bit more thoughtful, but then the forums are less active than they
3273880	3280440	used to be because discord has got more popular, you know, so it's all a bit of
3280440	3285520	a compromise, you know, running a healthy community is, yes, it's always a bit of a
3285520	3286040	challenge.
3286480	3289080	All right, we've got so many more things we want to dive in, but I don't want to
3289080	3290240	keep you here for hours.
3290720	3292960	Uh, this is not the, the lack of freedom in pockets.
3292960	3297640	We always like to say, uh, one topic I would love to maybe chat a bit about is
3297760	3301800	Mojo modular, you know, Chris Liner nominated you on the pockets.
3301880	3304160	So, uh, we want to spend a little time there.
3304200	3308080	You recently did a hacker's guide to language models and you ran through
3308080	3313000	everything from quantized model to like smaller models, larger models and all of
3313000	3313280	that.
3313640	3317160	Um, but obviously modular is taking its own approach.
3317280	3318560	Uh, yeah, we'll get you excited.
3318560	3322320	I know you Chris have been talking about this for like years and a lot of the
3322320	3323200	ideas you had, so.
3323680	3325240	Yeah, yeah, yeah, absolutely.
3325240	3330840	So I met Chris, I think it was at the first TensorFlow Dev Summit.
3331440	3335280	And I don't think he had even like, I'm not sure if he'd even sufficiently
3335280	3337480	started his employment with Google at that point.
3337520	3341320	So I, I don't know, you know, certainly nothing had been mentioned.
3342560	3348160	So I, you know, I admired him from afar with LLVM and Swift and whatever.
3348160	3353200	And so I saw him walk into the courtyard at, at Google.
3353480	3356440	It's just like, oh, man, Chris Lander.
3357600	3360440	I wonder if he would lower his standards enough to talk to me.
3361280	3362480	Was worth a try.
3362880	3365800	So I caught up my carriage because like he, nobody was talking to him.
3365920	3368480	He looked a bit lost and I wandered over and it's like, oh, you're Chris
3368480	3369040	Latino, right?
3369040	3370120	It's like, what are you doing here?
3370960	3372440	And it's like, yeah, yeah, I am.
3372440	3373600	I'm like, oh, I'm Jeremy Howard.
3373600	3375800	It's like, oh, did you do some of this AI stuff?
3375800	3377760	And I was like, yeah, yeah, I like this AI stuff.
3378520	3379640	Are you doing AI stuff?
3379720	3382520	It's like, well, I'm thinking about starting to do some AI stuff.
3382520	3383640	Yeah, I think it's going to be cool.
3383640	3389480	And so, well, so like I spent the next half hour just basically brain
3389520	3393400	dumping all the ways in which AI was stupid to him and he listened
3393400	3397800	patiently and I thought he probably wasn't even remember or care or whatever.
3397800	3402840	But yeah, then I kind of like, I guess I re-caught up with him a few months
3402840	3405600	later and it's like, I've been thinking about everything you said in that
3405600	3410320	conversation and he like narrated back his response to every part of it, the
3410320	3411520	projects he was planning to do.
3411520	3414800	And it's just like, oh, this dude follows up, holy shit.
3416840	3418240	And I was like, wow, okay.
3418240	3421880	And he was like, yeah, so we're going to create this new thing called Swift
3421880	3425080	for TensorFlow and it's going to be like, it's going to be a compiler
3425080	3428280	with auto differentiation built in and blah, blah, blah, blah.
3428280	3430160	And I say, wait, why would that help?
3430160	3432840	You know, why would you, and he was like, okay, with a compiler during the
3432840	3436480	forward pass, you don't have to worry about saving context, you know,
3436480	3439200	because a lot of it will be optimized in the backward, but I was like, oh my God.
3439800	3441520	Because I didn't really know much about compilers.
3442040	3446200	You know, I spent enough to kind of like understand the ideas, but it hadn't
3446200	3450240	occurred to me that a compiler basically solves a lot of the problems
3450240	3451920	we have as end users.
3452600	3453880	I was like, wow, that's amazing.
3453880	3457560	Okay, you do know, right, that nobody's going to use this unless it's like usable.
3457880	3459680	It's like, yeah, I know, right?
3459680	3462240	So I was thinking you should create like a fast AI for this.
3462360	3466240	So, okay, but I don't even know Swift.
3466400	3469840	And it's like, well, why don't you start learning it?
3470040	3472040	And if you have any questions, ask me.
3472640	3473720	It's just like, holy shit.
3473800	3478520	Like, not only is Chris Latner lowered his standards enough to talk to me, but
3478520	3482320	he's offering me personal tutoring on the programming language that he made.
3482760	3485080	So I was just like, I'm not going to let him down.
3485640	3490040	So I spent like the next two months, like just nerding out on Swift.
3490040	3495560	And it was just before Christmas that I kind of like started writing down what I'd learned.
3496640	3501680	So I wrote a couple of blog posts on like, okay, this is like my attempt to
3501680	3503480	do a numeric programming in Swift.
3504240	3508920	And these are all the challenges I had and the some of the issues I had with like
3510200	3512200	making things properly performant.
3512320	3515600	And here are some libraries I wrote and I sent it to Chris and I was like,
3515600	3519120	I hope he's not too disappointed with me, you know, because that would be the worst.
3519920	3523200	It's like, you know, and I was also like, I was like, I hope he doesn't
3523320	3527080	dislike the fact that I've, you know, didn't love everything.
3528040	3530960	And yeah, he was like, oh, thanks for sending me that.
3530960	3532480	Let's get on a call and talk about it.
3532480	3534520	And we spoke and he was like, this is amazing.
3534880	3536520	I can't believe that you made this.
3536520	3538200	This is exactly what Swift needs.
3538200	3542440	And he was like, and so like somebody set up like a new Swift,
3543640	3546280	I don't remember what they call them, the equivalent of a pep, you know,
3546280	3549680	kind of IFC thing of like, oh, you know, let's look at how we can implement
3549680	3551280	Jeremy's ideas in the language.
3551280	3553440	And he's just like, oh, wow.
3553720	3561200	And so, yeah, you know, so, you know, and then we ended up like literally
3561200	3564920	teaching some lessons together about Swift for TensorFlow.
3564920	3572280	And we built a fast AI kind of equivalent with him and his team.
3572280	3573240	It's so much fun.
3574440	3577640	Then in the end, you know, Google didn't follow through just fair enough,
3577640	3582920	like asking everybody to learn a new programming language is going to be tough.
3582960	3586280	But like it was very obvious, very, very obvious at that time that TensorFlow
3586280	3592720	2 is going to be a failure, you know, and so this felt like, okay, I, you know,
3593880	3595080	well, you know, what are you going to do?
3595400	3601960	Like, you can't focus on TensorFlow 2 because it's not going to, like it's
3601960	3602480	not working.
3602480	3606680	It's never going to work, you know, nobody at Google is using it internally.
3607680	3612680	So, you know, in the end, Chris left, you know, Swift for TensorFlow got archived.
3613680	3615160	There was no backup plan.
3615160	3620680	So I kind of felt like Google was kind of screwed, you know, and Chris went and
3620680	3623680	did something else, but we kept talking and I was like, look, Chris, you know,
3625680	3628680	you've got to be your own boss, man, because like, you know, you've got the ideas,
3629680	3633680	you know, like only you've got the ideas, you know, and if your ideas are implemented,
3634680	3640680	we'd all be so much better off because like, Python's the best of a whole bunch
3640680	3645680	of shit, you know, like, I would, it's amazing, but it's awful, you know, compared
3645680	3646680	to what it could be.
3647680	3651680	Anyway, so eventually a few years later, he called me up and he was like, Jeremy,
3651680	3652680	I've taken your advice.
3653680	3657680	I've started a company so much like, oh my God, so we got to create a new language.
3657680	3659680	We're going to create a new infrastructure.
3659680	3662680	It's going to build, it's going to have all the stuff we've talked about.
3663680	3664680	And it's like, oh, wow.
3665680	3668680	So that's what Mojo is.
3669680	3677680	And so Mojo is like, you know, building on all the stuff that Chris has figured out
3677680	3683680	over, I mean, really from when he did his PhD thesis, which developed LLVM onwards,
3683680	3690680	you know, in Swift and MLAR, you know, the TensorFlow runtime engine, which is very good.
3690680	3693680	You know, that was something that he built and has lasted.
3695680	3697680	So yeah, I'm pumped about that.
3697680	3699680	I mean, it's very speculative.
3699680	3701680	Creating a whole new language is tough.
3701680	3706680	I mean, Chris has done it before and he's created a whole C++ compiler amongst other things.
3707680	3708680	Looking pretty hopeful.
3709680	3713680	I mean, I hope it works because, you know, I mean.
3713680	3715680	You're doing them to quit his job, so.
3716680	3722680	And I mean, in the meantime, I will say, you know, Google now does have a backup plan, you know,
3722680	3725680	they have JAX, which was never a strategy.
3725680	3730680	It was just a bunch of people who also recognized TensorFlow 2 as shit and they just decided to build something else.
3731680	3735680	And for years, my friends in that team were like, don't tell anybody about us because we, you know,
3735680	3737680	we don't want to be anything but a research project.
3738680	3743680	So now these poor guys suddenly, they're the great white hope for Google's future.
3744680	3748680	And so JAX is, you know, also not terrible, but it's still written in Python.
3748680	3757680	Like it would be cool if we had all the benefits of JAX but in a language that was designed for those kind of purposes.
3759680	3765680	So, you know, fingers crossed that, yeah, that Mocho turns out great.
3765680	3766680	Yeah.
3767680	3771680	Any other thoughts on when, where people should be spending their time?
3771680	3777680	So that's more the kind of language framework level than you have the, you know, GGML.
3777680	3781680	Some of these are like quantization-focused kind of model-level things.
3781680	3783680	Then you got the hardware people.
3783680	3784680	It's like a whole other bucket.
3785680	3788680	Yeah, what are some of the exciting stuff that you're excited about?
3788680	3797680	Well, you won't be surprised to hear me say this, but I think fine-tuning, transfer learning is still a hugely underappreciated area.
3797680	3807680	So today's zero-shot, few-shot learning equivalent is retrieval augmented generation, you know, RAG.
3807680	3811680	Which is like, just like few-shot learning is a thing.
3811680	3812680	Like it's a real thing.
3812680	3813680	It's a useful thing.
3813680	3815680	It's not a thing anybody would want to ignore.
3815680	3819680	Why are people not spending at least as much effort on fine-tuning?
3819680	3825680	You know, because, you know, RAG is like such an inefficient hack, really, isn't it?
3825680	3832680	It's like, you know, segment up my data in some somewhat arbitrary way.
3832680	3835680	Embed it, ask questions about that.
3835680	3843680	You know, hope that my embedding model embeds questions in the same bedding space as a paragraph.
3843680	3848680	Which obviously is not going to, if your question is like, if I've got a whole bunch of archive papers embeddings.
3848680	3857680	And I asked like, what are all the ways in which we can make inference more efficient?
3857680	3866680	The only paragraphs it'll find is like, if there's a review paper that says here's a list of ways to make, you know, inference more efficient.
3866680	3867680	Doesn't have any of the specifics.
3867680	3874680	No, it's not going to be like, oh, here's one way, here's one way, here's a different way in different papers, you know.
3874680	3886680	Yeah, if you fine-tune a model, then all of that information is getting directly incorporated into the weights of your model in a much more efficient and nuanced way.
3886680	3889680	And then you can use RAG on top of that.
3889680	3894680	So I think that that's one area that's definitely underappreciated.
3894680	3900680	And also the confluence of like, okay, how do you combine RAG and fine-tuning, for example.
3900680	3910680	Something that I think a lot of people are uncertain about, and I don't expect you to know either, is that whether or not you can fine-tune new information in.
3910680	3916680	And I think that that is the focus of some of your open questions and research.
3916680	3917680	But of course you can, right?
3917680	3918680	Because it's additional pre-training.
3918680	3922680	Obviously you can, because there's no such thing as fine-tuning.
3922680	3924680	There's only continued pre-training.
3924680	3929680	So fine-tuning is pre-training, like they're literally the same thing.
3930680	3933680	So the knowledge got in there in the first place through pre-training.
3933680	3937680	So how could continuing to pre-train not put more knowledge in?
3937680	3939680	Like it's the same thing.
3940680	3944680	The problem is just we're really bad at it, because everybody's doing it dumb ways.
3944680	3949680	So it's a good question, and it's not just new knowledge, but new capabilities.
3950680	3957680	You know, I think like in my Hackers Guide to LL, into Hackers Guide to LLM's talk, I show simple.
3957680	3965680	I mean, it's a funny, that's a simple example, because it doesn't sound it, but like taking a pre-trained based model and getting it to generate SQL.
3965680	3968680	And it took 15 minutes to train on a single GPU.
3968680	3975680	You know, I think that might surprise people that that capability is actual fingertips.
3975680	3980680	And you know, because it was already there, it was just latent in the base model.
3980680	3987680	Really pushing the boundaries of what you can do with small models, I think is a really interesting question.
3987680	3989680	Like what can you do with a...
3989680	3992680	I mean, there isn't much in the way of good small models.
3992680	4003680	A really underappreciated one is a BTLM 3B, which is a like kind of 7B quality 3B model.
4004680	4006680	There's not much of the 1-2B range, sadly.
4006680	4016680	There are some code ones, but like the fact that there are some really good code ones in that 1-2B range shows you that that's a great size for doing complex tasks well.
4017680	4023680	There was PHY1 recently, which has been the subject of a little bit of discussion about whether they're trained on benchmarks.
4023680	4025680	Yeah, PHY1.5 as well.
4025680	4029680	So that's not a good model yet.
4030680	4031680	Why not?
4032680	4040680	So PHY1 in particular is good at doing a very specific thing, which is creating very small Python snippets.
4040680	4041680	The thing...
4041680	4046680	Okay, so like PHY1.5 has never read Wikipedia, for example.
4046680	4050680	So it doesn't know who Tom Cruise is, you know.
4050680	4053680	It doesn't know who anybody is.
4053680	4055680	He doesn't know about any movies.
4055680	4061680	It doesn't really know anything about anything because it's never read anything.
4061680	4069680	You know, it was trained on a nearly entirely synthetic data set, which is designed for it to learn reasoning.
4069680	4077680	And so it was a research project and a really good one, and it definitely shows us a powerful direction in terms of what you can do with synthetic data.
4077680	4084680	And wow, gosh, even these tiny models can get pretty good reasoning skills, pretty good math skills, pretty good toting skills.
4086680	4091680	But I don't know if it's a model you could necessarily build on.
4091680	4094680	Some people have tried to do some fine tunes of it.
4094680	4104680	And again, they're like surprisingly good in some ways for a 1.5B model, but not sure you'd find it useful for anything.
4104680	4112680	I think that's the struggle of pitching small models because small is great, you know, you don't need a lot of resources to run them.
4112680	4115680	But the performance evaluation is always so iffy.
4115680	4120680	It's always just like, yeah, it works on some things and we don't trust it for others.
4120680	4123680	Yeah, so that's why we're back to fine tuning.
4123680	4130680	I would say, so Microsoft did create a PHY1.5 web, but they didn't release it, unfortunately.
4131680	4144680	I would say a PHY1.5 web with fine tuning for your task might solve a lot of tasks that people have in their kind of day-to-day lives,
4146680	4148680	particularly in kind of an enterprise setting.
4148680	4153680	I think there's a lot of repetitive kind of processing that has to be done.
4153680	4164680	It's a useful thing for coders to know about because I think quite often you can replace some thousands and thousands of lines of complex buggy code maybe with a fine tune, you know.
4165680	4166680	Good.
4166680	4167680	Yeah.
4168680	4173680	And Jeremy, before we let you go, I think one question on top of a lot of people's minds.
4173680	4179680	So you've done practical deep learning for coders in 2018, 19, 21, 22.
4179680	4183680	I feel like the more time goes by, the more the GPUs get concentrated.
4184680	4190680	If you're somebody who's interested in deep learning today and you don't want to go join OpenAI, you don't want to join Anthropic,
4190680	4193680	what's like the best use of their time?
4193680	4195680	Should they focus on, yes, model development?
4195680	4198680	Should they focus on fine tuning math and all of that?
4198680	4204680	Should they just like focus on making rag not a hack and coming up with a better solution?
4205680	4209680	Yeah, what's practical deep learning for coders 2024 kind of look like?
4210680	4211680	Yeah.
4211680	4212680	I mean, good question.
4212680	4215680	I'm trying to figure that out for myself, you know, like what should I teach?
4215680	4221680	Because I definitely feel like things have changed a bit, you know.
4221680	4226680	One of the ways in which things have changed is that coding is much more accessible now.
4226680	4230680	So if you look at a lot of the folks in the kind of open source LLM community,
4230680	4233680	they're folks who really hadn't coded before a year ago.
4234680	4240680	And they're using these models to help them build stuff they couldn't build before, which is just fantastic, you know.
4242680	4248680	So one thing I kind of think is like, okay, well, we need a lot more material to help these people use this newfound skill they have,
4248680	4252680	because they don't really know what they're doing, you know, and they don't claim to,
4252680	4253680	but they're doing it anyway.
4253680	4255680	And I think that's fantastic, you know.
4255680	4260680	So like other things we could do to help people, you know, bridge this gap,
4260680	4269680	because previously, you know, I know folks who were, you know, doing menial jobs a year ago,
4269680	4276680	and now they're training language models thanks to the help of codecs and co-pilot and whatever.
4276680	4281680	So, you know, yeah, what does it look like to like really grab this opportunity?
4281680	4287680	You know, maybe fast AIs goals can be dramatically expanded now to being like,
4287680	4293680	let's make coding more accessible, you know, or kind of AI-oriented coding more accessible.
4294680	4299680	If so, our costs should probably look very different, you know,
4299680	4303680	and we'd have to throw away that like, oh, you have to have at least a year of full-time programming,
4303680	4306680	you know, as a prerequisite.
4307680	4309680	Yeah, what would happen if we got rid of that?
4309680	4312680	So that's kind of one thought that's in my head.
4313680	4320680	You know, as to what should other people do, honestly, I don't think anybody has any idea,
4320680	4323680	like the more I look at it, what's going on.
4323680	4327680	I know I don't, you know, like, we don't really know how to do anything very well.
4328680	4334680	Clearly open AI do, like they seem to be quite good at some things,
4334680	4338680	or they're talking to folks at or who have recently left open AI.
4338680	4341680	Even there, it's clear there's a lot of stuff they haven't really figured out,
4341680	4346680	and they're just kind of like using recipes that they've noticed have been okay.
4346680	4349680	So yeah, we don't really know how to train these models well.
4349680	4350680	We don't know how to fine-tune them well.
4350680	4352680	We don't know how to do RAG well.
4352680	4353680	We don't know what they can do.
4353680	4354680	We don't know what they can't do.
4354680	4357680	We don't know how big a model you need to solve different kinds of problems.
4357680	4359680	We don't know what kind of problems they can't do.
4359680	4363680	We don't know what good prompting strategies are for particular problems, you know,
4363680	4369680	like somebody sent me a message the other day saying they've written something
4369680	4374680	that is a prompting strategy for GPT-4.
4374680	4377680	They've written like 6,000 lines of Python code,
4377680	4380680	and it's to help it play chess.
4381680	4385680	And then they've said they've had it play against other chess engines,
4385680	4387680	including the best stockfish engines,
4387680	4391680	and it's got an ELO of 3400,
4391680	4395680	which would make it close to the best chess engine in existence.
4397680	4402680	And I think this is a good example of like people were saying like GPT-4 can't play chess.
4402680	4404680	I mean, I was sure that was wrong.
4404680	4406680	I mean, obviously it can play chess,
4406680	4410680	but the difference between like with no prompting strategy,
4410680	4412680	they can't even make legal moves.
4412680	4413680	With good prompting strategies,
4413680	4416680	it might be just about the best chess engine in the world,
4416680	4418680	far better than any human player.
4418680	4421680	So, yeah, I mean, we don't really know what the capabilities are yet.
4421680	4424680	So I feel like it's all blue sky at this point.
4424680	4428680	It feels like computer vision in 2013 to me,
4428680	4430680	which was like in 2013 computer vision.
4430680	4431680	We just had the AlexNet.
4431680	4433680	We've had AlexNet.
4433680	4435680	We've had VGGNet.
4435680	4437680	It's around the time Xyler and Fergus like,
4437680	4439680	no, it's probably before that.
4439680	4440680	So we hadn't yet had the Xyler and Fergus like,
4440680	4442680	oh, this is actually what's going on inside the layers.
4442680	4447680	So, you know, we don't actually know what's happening inside these transformers.
4447680	4451680	We don't know how to create good training dynamics.
4451680	4454680	We don't really know anything much.
4454680	4457680	And there's a reason for that, right?
4457680	4464680	And the reason for that is language models suddenly got really useful.
4464680	4468680	And so the kind of economically rational thing to do,
4468680	4470680	like this is not criticism, this is true.
4470680	4472680	The economic rational thing to do is to like,
4472680	4475680	okay, like build that as fast as possible,
4475680	4478680	you know, make something work, get it out there.
4478680	4482680	And that's what, you know, open AI in particular did,
4482680	4486680	anthropic kind of did.
4486680	4490680	But there's a whole lot of technical debt everywhere, you know,
4490680	4495680	nobody's really figured this stuff out because everybody's been so busy
4495680	4499680	building what we know works as quickly as possible.
4499680	4502680	So, yeah, I think there's a huge amount of opportunity to,
4502680	4509680	you know, I think we'll find things can be made to work a lot faster,
4509680	4511680	a lot less memory.
4511680	4514680	I got a whole bunch of ideas I want to try, you know,
4514680	4518680	every time I look at something closely, like really closely,
4518680	4522680	I'm always like, oh, turns out this person actually had no idea what they're doing.
4522680	4527680	You know, which is fine, like none of us know what we're doing.
4527680	4531680	We should experiment with that.
4531680	4535680	We had a treat out on the podcast who created flash attention.
4535680	4539680	And I asked them, did nobody think of using SRAM before you?
4539680	4542680	Like where people just like, you know, and he was like, yeah,
4542680	4545680	people just didn't think of it, didn't try.
4545680	4547680	They didn't come from like a systems background.
4547680	4548680	Yeah.
4548680	4552680	I mean, the thing about flash attention is, I mean,
4552680	4555680	lots of people absolutely thought of that.
4555680	4556680	So had I, right?
4556680	4560680	But I mean, the honest truth is particularly before Triton,
4560680	4565680	like everybody knew that tiling is the right way to solve anything.
4565680	4567680	And everybody knew that attention,
4567680	4569680	used attention, wasn't tiled.
4569680	4571680	That was stupid.
4571680	4576680	But not everybody's got his ability to like be like,
4576680	4582680	oh, well, I'm confident enough in CUDA and or Triton
4582680	4584680	to use that insight to write something better.
4584680	4587680	And this is where like, I'm super excited about Mojo, right?
4587680	4590680	And I always talk to Chris about flash attention as I'm like,
4590680	4596680	there is a thousand flash attentions out there for us to build.
4596680	4599680	You just got to make it easy for us to build them.
4599680	4602680	So like Triton definitely helps.
4602680	4606680	But it's still not easy.
4606680	4611680	It still requires kind of really understanding the GPU architecture,
4611680	4614680	writing it in that kind of very CUDA-ish way.
4614680	4619680	So yeah, I think, you know, if Mojo or something equivalent
4619680	4624680	can really work well, we're going to see a lot more flash attentions
4624680	4627680	popping up.
4627680	4628680	Great, Jaren.
4628680	4631680	Before we wrap, we usually do a quick lightning round.
4631680	4633680	We've got three simple questions.
4633680	4635680	So the first one is around acceleration
4635680	4638680	and you've been in this field a long time.
4638680	4640680	What's something that it's already here today
4640680	4643680	and AI that you thought would take much longer?
4643680	4644680	I don't think anything.
4644680	4646680	So I've actually been slightly too bullish.
4646680	4654680	So in my 2014 TED Talk, I had a graph and I said like,
4654680	4656680	this is like the slope of human capabilities
4656680	4659680	and this is the slope of AI capabilities.
4659680	4662680	And I said, oh, and I put a dot saying we are here.
4662680	4664680	It was just before they passed.
4664680	4667680	And I looked back at the transcript the other day
4667680	4672680	and I said in five years, I think we might have crossed
4672680	4675680	that threshold in which computers will be better
4675680	4678680	at most human tasks than most humans, most average humans.
4678680	4685680	And so that might be almost true now for non-physical tasks.
4685680	4694680	So I took that twice as long as I thought it might.
4694680	4698680	Yeah, no, I wouldn't say anything surprised me too much.
4698680	4701680	It's still like definitely like, I got to admit,
4701680	4706680	I had a very visceral reaction using GPT-4 for the first time,
4706680	4712680	not because I found it surprising, but actually doing it.
4712680	4716680	Something I was pretty sure would exist by about now,
4716680	4718680	maybe a bit earlier.
4718680	4721680	But actually using it definitely is different
4721680	4724680	to just feeling like it's probably on its way.
4724680	4729680	And yeah, whatever GPT-5 looks like,
4729680	4736680	I'm sure I imagine I'll have the same visceral reaction.
4736680	4740680	It's really amazing to watch develop.
4740680	4742680	We also have an exploration question.
4742680	4744680	So what do you think is the most interesting
4744680	4747680	unsolved question in AI?
4747680	4750680	How do language models learn?
4750680	4752680	What are the training dynamics?
4752680	4759680	Like I want to see, there was a great paper about Resnets
4759680	4765680	a few years ago that showed how that was able to like plot
4765680	4768680	a kind of projected three-dimensional loss surface
4768680	4774680	for a ConvNet with and without skip connections.
4774680	4777680	And you could very clearly see without the skip connections,
4777680	4779680	it was super bumpy and with the skip connections,
4779680	4783680	it was super smooth.
4783680	4785680	That's the kind of work we need.
4785680	4787680	So there was actually an interesting blog post
4787680	4790680	that came out just today from the PyTorch team
4790680	4793680	where some of them have created this like 3D
4793680	4796680	matrix product visualization thing.
4796680	4800680	And they actually showed some nice examples
4800680	4803680	of like a GPT-2 attention layer
4803680	4806680	and showed an animation and said,
4806680	4808680	like, if you look at this, we can actually see a bit
4808680	4809680	about what it's doing.
4809680	4812680	You know, so again, it reminds me of the Zeiler
4812680	4815680	and Fergus, you know, ConvNet paper.
4815680	4818680	That was the first one to do these reverse convolutions
4818680	4820680	to show what's actually being learned
4820680	4821680	in each layer in a ConvNet.
4821680	4824680	Yeah, we need a lot more of this, like,
4824680	4827680	what is going on inside these models?
4827680	4829680	How do they actually learn?
4829680	4831680	And then how can we use those insights
4831680	4835680	to help them to learn better?
4835680	4836680	So I think that would be one.
4836680	4838680	The other exploration I'd really like to see
4838680	4841680	is a much more rigorous analysis
4841680	4844680	of what kind of data do they need,
4844680	4847680	at what level, and when do they need it,
4847680	4848680	and how often.
4848680	4851680	So that kind of like data set mixing, curation,
4851680	4854680	so forth in order to get the best capabilities.
4854680	4855680	Yeah.
4855680	4857680	How much is Wikipedia?
4857680	4858680	Yeah.
4858680	4859680	Very uncertain.
4859680	4861680	You know, to fine-tune what kind of mix
4861680	4864680	do you need for it to keep its capabilities
4864680	4866680	and what are the kind of underlying capabilities
4866680	4867680	that it most needs to keep?
4867680	4869680	And if it loses those, it would lose all these other ones
4869680	4871680	and what data do you need to keep those?
4871680	4873680	And, you know, are there things we can do
4873680	4876680	to change the loss function, to help it,
4876680	4880680	to not forget to do things, stuff like that?
4880680	4881680	Awesome.
4881680	4884680	And yeah, before wrapping, what's one message,
4884680	4886680	one idea you want everyone to remember
4886680	4887680	and think about?
4887680	4889680	You know, I guess the main thing I want everybody
4889680	4891680	to remember is that, you know,
4891680	4893680	there's a lot of people in the world
4893680	4895680	and they have a lot of, you know,
4895680	4898680	diverse experiences and capabilities
4898680	4902680	and, you know, they all matter.
4902680	4906680	And now that we have, you know,
4906680	4909680	nearly powerful technology in our lives,
4909680	4911680	we could think of that in one of two ways.
4911680	4917680	One would be, gee, that's really scary
4917680	4919680	what would happen if all of these people in the world
4919680	4921680	had access to this technology.
4921680	4923680	One of them might be bad people.
4923680	4925680	Let's make sure they can't have it.
4925680	4928680	Or one might be, wow,
4928680	4930680	if all those people in the world are better,
4930680	4933680	a lot of them could really improve the lives
4933680	4937680	of a lot of humanity if they had this tool.
4937680	4939680	This has always been the case, you know,
4939680	4941680	from the invention of writing
4941680	4943680	to the invention of the printing press
4943680	4946680	to the, you know, development of education.
4946680	4949680	And it's been a constant battle
4949680	4953680	between people who think that distributed power is unsafe
4953680	4956680	and it should be held on to by an elite few
4956680	4963680	and people who think that humanity on net,
4963680	4966680	you know, is a marvellous species,
4966680	4969680	particularly when part of a society and a civilization
4969680	4971680	and we should do everything we can
4971680	4975680	to enable more of them to contribute.
4975680	4979680	There's a really big conversation right now
4979680	4984680	and, you know, I want to see more and more people
4984680	4990680	showing up and showing what, you know,
4990680	4993680	what the great unwashed masses out there
4993680	4995680	can actually achieve, you know,
4995680	4998680	that actually, you know, regular people
4998680	5002680	are going to do a lot of really valuable work
5002680	5008680	and actually help us be, you know, more safe
5008680	5012680	and also flourishing in our lives
5012680	5017680	and providing a future for our children to flourish in,
5017680	5024680	you know, if we lock things down
5024680	5028680	to the people that we think, you know,
5028680	5032680	the elites that we think can be trusted to run it for us.
5032680	5041680	Yeah, I think all bets are off about where that lives as a society, you know.
5041680	5043680	Yep.
5043680	5046680	Now, that's an important message
5046680	5048680	and yeah, that's why we've been promoting a lot
5048680	5051680	of open source developers, open source communities.
5051680	5055680	I think letting the builders build and explore,
5055680	5057680	that's always a good idea.
5057680	5059680	Thank you so much for coming on, Jeremy.
5059680	5060680	This was great.
5060680	5062680	Thank you for having me.
