{"text": " Alright, let's get started. Thanks for everybody who showed up. We've got three of the course instructors here, myself, Anish in the back, and Jonas. And Anish is finishing up his final year, but we all were here. We all were students here. We all did our PhDs here. And it's good to be back. If you don't know any machine learning at all, you've never taken an intro class, then this will be probably a little too confusing. But as long as you've seen a little ML, then you'll have a good time. And ideally, you know a little bit of Python, and you know it like NumPy and Pandasar. So with that, let's just get started. So our goal in this course is to learn about data-centric AI. So just like a quick show of hands, prior to this course, how many people I've heard of data-centric AI? Okay, alright, well, get excited because you're going to learn some great things. It'll be really good. Alright, so let's just jump in. So this is a picture of a self-driving car that has had an accident. And you notice that the article focuses on when algorithms mess up, right? And this is the common case, right? In machine learning, we tend to focus on the model. And so it's like there's a crash, the algorithm must have done something wrong. And so I'd like us to sort of think of a different way of thinking about this. This is a paper many folks are familiar with, which basically shows one thing in that a neural network or a machine learning classifier can learn total randomness. It's like if you give it complete garbage data, just like completely random labels, it can learn to map like images to completely arbitrary labels or text to completely arbitrary labels. So basically, if you give it really bad data, it will just produce exactly what it learns, even if the data is completely wrong. And so I'd like to sort of rethink this title of this article as when algorithms are trained with erroneous data, things like car crashes can happen. And that's the way we'll sort of focus and think about this course. So traditional machine learning is very model-centric, right? Like when you take an ML class, you first learn machine learning, you're in school and then you get a data set. And usually the data set is pretty good. Like if anyone's seen like the cat dogs data set, you know, it's in images of cats and they're all cats and they're labeled cat. And then there's images of dog and they're all dogs and they're labeled dog and there's no, there's usually no like cows thrown in there, right? And there's usually not like a bunch of dogs that are labeled cat. It's like pretty well curated. And then your goal is to, you know, produce a good model, right? You want to train a model that takes in a new image that's either a cat or a dog and it predicts is it cat or is it dog. And that's sort of how we learn machine learning usually. And this is something that's standard if anyone here has taken like 6036, which has been renamed to 63390, the intro to ML class here. And you'll learn stuff like, you know, different types of models. And if you're familiar with neural networks, neural architectures, or you'll learn about tuning hyper parameters of a model, or you'll learn about modifying the loss function using different loss functions and regularizations that you don't overfit. And these are common things you learn in model centric AI. So let's just juxtapose that with the real world once you're out of the classroom. So in the classroom, usually the data sets fixed and it's pretty good, but then you go to the real world and actually the data set is not fixed, right? You have user data or customer data or real world data and you can get more or less, you can change the data and the data has all sorts of weird things in it. And so what tends to happen is that the company or the user, whoever's sort of using this model that you're trying to train, it doesn't really care as much about the cool ML fancy tricks as it does about like, does it actually work in the real world? And if you have really good machine learning models that work on highly curated data, but then the real world data is actually really messy, and it makes sense to actually focus on fixing the issues in the data. And a lot of people have been doing this, but what are systematic ways to do this? And that's what we'll focus on this course. What may surprise some of you that you may not know is that 10 of the most commonly cited and most used test sets in the field of machine learning all have wrong labels. And that may be a surprise for some of you. So we'll just take a quick look at this website, which I think will be pretty fun. This is labelairs.com. Can we see okay? So this is a site that you can check out on your own. So this is ImageNet, which is a common image data set. And these are in the test set, or in this case a validation set, which is what was released. But this is the data that is supposed to be the most accurate data, right? This is like your real world test data. And so this data should be some of the most highly curated, most accurate data. And what you'll find is that, for example, this scorpion is labeled tick. And you'll find all sorts of stuff. We can look at another data. This is by Google. This is a drawing hand drawn data set. This is labeled a t-shirt. And this is labeled a diving board. And this is labeled a scorpion. And it just keeps going and going. And there's some fun ones. Like this is a, this is totally a cake, but it's labeled a rake. Like it just didn't quite get the R to go all the way when they were pretending they're writing it out. But anyway, you can check these out on your own. This is for any type of data. So like text data or audio data. And you'll have fun. And I think the good idea to think about here is that this is, you know, a data set that's released by Google. It's a benchmark data set. And it's very difficult when you have millions of examples to know, like, what's the bad data? And ideally, if you didn't have that bad data, you could train better models. And so we need to learn how can we find these errors and find these issues automatically. So going back to the slides. So as seasoned data scientists, the way they'll approach this problem is that it's more worthwhile to invest in exploring and fixing the data than trying to tinker with the models to avoid this garbage in, garbage out issue. And the issue is that, like, if you have millions of data, right, or you have, like, a data set that's like 100 million, like, how do you do that without it being highly time consuming? All right. So we're here in a data-centric AI course. We've called this introduction to data-centric AI because we want it to be accessible. And so we're going to cover just, like, the very intro, what is data-centric AI? How does it differ from model-centric AI? And then we'll dive in a little deeper. So data-centric AI often takes one of two forms. So one form is that you have AI algorithms that understand something about data, and then they use this new information that they've understood to help a model train better. So an example of this is curriculum learning. And this is more of sort of something you'd encounter in grad school, so you may not have heard of it. But what curriculum, it's, you can check out the paper by Yasha Benjo. But the idea is you use a, some algorithm that looks at data and it identifies which data is probably easy to learn. And so the corollary here is imagine you're a student and you're in the classroom. All right. Should the teacher, what should they teach you? Like, should they teach you really, really hard examples as the very first examples? Like, if you're learning addition, should they start out with like 10,051 plus 1042 or would they start out with like one plus two? Right. Now we know this because, like, we've all learned addition. But when a machine learning model starts, it's starting from scratch. So it doesn't know right from the beginning what is the sort of easiest example. And so there are data-centric AI approaches that actually estimate what is the easiest example. And then when you train an ML model, you start with that example, and then you give it slightly harder ones and slightly harder them. And so it's like curriculum learning because it's like a student's curriculum. And so that's one way that you can sort of use new information. You're still training on all the same data, but you're just reordering it and using this additional information. You don't actually change the data set. Another sort of common form of data-centric AI is that you actually modify the data set to directly improve the performance of a model. So an example of this might be something like Confant Learning. And this is less known than curriculum learning, something I worked on. And this idea here is that you want to find what are label errors in a data set and then remove them prior to training, so that you just train on like correctly labeled stuff. And the corollary here in the sense of the student is that if your teacher is basically making mistakes 30% of the time, like imagine as I was teaching you, which hopefully I don't do today, that 30% of the time I told you wrong things. And then you compare that to another teacher who comes and they tell you right things 100% of the time. Then which teacher do you think you'll probably learn data-centric AI better from? And so the idea is that we want to take this bad sort of, you know, this 30% of wrong things and we want to get rid of them so that you could sort of come to the classroom and redo your learning experiences if those never happened. And that's the idea here. So you just learn on the good stuff. So those are two approaches. And then what we'll think about now is sort of what is the difference between model-centric AI and data-centric AI? So the sort of normal way you hear this is that given a data set, you try to produce the best model. And that's like the classical way of thinking about model-centric AI. And the idea is that you want to change the model somehow to improve, you know, some performance on AI task. And is there anyone here just out of curiosity who's not familiar with like AI tasks and some of the stuff we do in AI? Just want to make sure that we're on the same board, like classifying things. Okay, sweet. All right. So you've got some AI tasks and you're trying to classify something and the goal is you want to improve the performance on that. So usually it'll change the model to do that. And data-centric AI, instead, it's given some model, right, that may be fixed or you may change, but let's just assume it's fixed for now. You want to improve that model by improving your data set. So this is like the common way to think about the two. And the idea is to systematically or algorithmically not have a bunch of humans changing the data set, but like some algorithmic way that you can do this. Okay. All right. So our goal is to start thinking about ML in terms of data and not the model. And so we'll just start out with a simple example. And this is not data-centric AI, but it'll get us thinking about how we can think about AI in terms of data. So just quick show of hands. Who here is familiar with K nearest neighbors? Okay. Great. All right. So I won't belabor this point then. So do some chalk. That was a good thing to check beforehand. We were here last night, but the chalk has been removed. All right. So yeah, the key idea with K nearest neighbors and the key idea to think about in the context of data-centric learning is that K nearest neighbors, imagine you have, you know, a space, a 2D space, which, you know what, here's what we'll do. All right. So you've got some, all right, sweet. So you've got, say, you know, some data, you've got some triangles, and you've got some circles. You'll have to apologize for my drawing and you've got some squares. And so this is your data set. And let's say, you know, these are different types of images that you've put in some 2D space somehow where it's text that you've mapped a 2D space. And then the idea of K nearest neighbors, as many of you seem to know and are familiar with, is that you would have some new point, say, here. And with this, it's some weird thing. We don't know what it is. And we're trying to figure out, is it a triangle? Is it a circle? Is it a square? And so if this was sort of three K nearest neighbors, like then you would find the three nearest neighbors. So say this one, this one, and this one. And then can somebody tell me, like, what the class would be and why? Yeah, it'd be a square. That doesn't mean that it's not a cool point. And you're familiar with being a square. So this would be labeled a square, and that's based on majority voting. And there's a lot of different ways to do algorithms for deciding what the label should be. If it was, say, five neighbors, meaning five in N, where K is five, so it's K in N, then you would choose the five nearest neighbors and you would do a majority vote. Okay. The key idea here is that there is no loss function. Literally, any time you have a new point, so say I just have another new point here, I'm not sort of, there's no algorithm that I'm passing this into beyond just measuring a distance, some notion of distance between this and the nearest points. And you can do a bunch of pre-computation. There's a lot of smart work actually done in K nearest neighbors that is pretty impressive and you can do embeddings for all of these and pre-compute distances and you can do all sorts of fancy stuff. But ultimately, all this decision is based on is the data. And so I wanted to motivate this problem and this way of thinking because this whole decision process is made just based on data. And the quality of the data is as related as possible to the quality of the prediction. So if you have really good data, you're going to have really good predictions. And if you've got a bunch of errors in here, you're going to get the wrong prediction. And so this makes it really clear why fixing the data will make it, will improve your model. So is that kind of clear how this is motivating? Now this is not a data-centric AI algorithm. And by the end of this lecture, you'll definitely know what a data-centric AI algorithm is. But can someone tell me what the difference is between K and N and a data-centric AI algorithm? Yeah, yeah, totally. That's a great answer. K and N is just doing classification. And it's not actually modifying the data set. So yeah, that's exactly right. Alright, so what are a few other examples of what is not data-centric AI? Handpicking a bunch of points you think will improve a model. So can anybody help me understand why this is not data-centric AI? Yeah. Yeah, totally. It's just done by hand. Like this could, if you had a hundred million, a data set of a hundred million points, this would take a long time. What about doubling the size of your data set so that you can train an improved model? Yeah. Yeah, totally. This is just classical machine learning. It's still all the model, all the work you do as a model, but you're just paying more money for more data. So let's juxtapose this. So what would be the data-centric AI versions of this? And just out of curiosity, does anybody know for the first one what would be sort of the data-centric AI corollary? Yeah, totally. And there's a whole field of research on this and a whole subsection of ML that's called corset selection, where you have a data set and let's say that you train a model on that data set and you get like 98% accuracy. But the data set is like a hundred billion points. And so the goal is, can I find like, you know, a million points that if I just train on those, I can get like pretty close, like 97% accuracy. And that's corset selection. What about for number two? What would be like a data-centric AI corollary? Yeah, yeah. So the idea is, and how does anybody know of any ways that you might make your data set like bigger without just getting twice as much data? Yeah, totally. So like, say you have a, so data augmentation, awesome. And so say you have an image, you could just totally rotate that image. And now instead of one data point, you have like five data points or turn it black and white, or you could shift it or skew it or take the top and the bottom and move them a little bit or add some noise. There's lots of things you can do to make data bigger and actually improve a model just by changing the data set. And these are all fall within data-centric AI. Are you all familiar with what's called like back translation for text data augmentation? So this is a pretty fun thing. I don't like have particularly good translation skills. But like, if I say like, hi, you know, my name is Curtis. And then I translate this. Okay, so it becomes Ola. And then I translate this again. It might become, and now I've gotten one data point, and I just got another data point. So I was able to augment my text data set to get more versions of the same thing. And this could have some label, and the label could be introduction. You know, I'll just end it there because I know it's hard to see on that side. But this, this would be like the label. And this is text. And so this one is the same label. We haven't changed the label at all. So this is intro. And now you can see I have more labeled data, but I didn't have to do anything. I didn't have to pay more. I didn't have to, I just did this all computationally. Yeah. How do you see this in all the cases for that error to be taken? Oh yeah, totally can. So if, say that this label was wrong, now you have two label errors. Totally. Yeah. So you often want to combine two approaches. One is like first, try to fix or improve label errors before doing the augmentation. And that's a really good idea. All right. So what are some examples, we looked at examples that are not data-centric AI. So what are some things that are data-centric AI? And for this, I'll go through quick because we're going to learn all these in the course. And so one is outlier detection and removal. So I'll just be really quick to show you. So say you have some data set and we've got, let me use this board. And you've got, say, right? So you've got your sort of, your, you know, two classes. And so normally you would, you would draw your classifier and then you have some new point here and I'll be labeled a negative. But say in your training data, you have this really weird, you know, plus over here. And so maybe the boundary should be something like this, but you just don't have very much information and it's really out here and it seems like it's not very related to the rest of the data. And so what often people will do because they just don't have enough information is they'll identify this as an automatic, as an outlier because it seems very out of distribution and they'll remove it. And so then you get this line, which fits to all the data except for that one point. In terms of some other things in data-centric AI, so error detection and correction. And so that's, for example, you just have like a data point that is, it could be images, it's like all black, or you have a label error like we saw earlier. If this, you know, text example instead of being labeled intro was labeled like, you know, a goodbye clause or something. You'd want to find that and automatically correct it. Data augmentation, which is what we saw earlier. So that's like increasing the size of your data set. So you have more training data. Feature engineering and selections. The idea here is you have, if anyone's familiar like the early days of neural networks couldn't solve like the XOR problem, but you could always just generate the XOR as another column. A more concrete example, if you haven't heard of this, that one is you just have, you know, a bunch of tabular data. I worked on cheating detection at MIT. And, you know, if you want the machine learning model to learn, say who is a cheater from a bunch of, you know, education data, like where did they go to school and what's their background and what problems did they answer. It can really help if you also compute new features like how often did they submit answers within five seconds of another student. So you can always generate new features and then you pass those into the model and your model can, you know, if the features are relevant to the label you're trying to predict, it can do a lot better. Yeah. When we're doing outlier detection and removal, how do you know that an outlier is the result of something that should not have happened or indication of a very rare event that you should pay attention to? Yeah, that's a really fair and hard question to answer. You make assumptions. And so in this case, I was making an assumption, right? I was saying, like, I didn't draw very much data, but say that I had, like, millions of data points and then this one's really far away. Then the assumption you're making is that, like, I have so much data that suggests that this is the distribution and then I have very little data that suggests that this is part of that distribution. And the biggest issue is that if your classifier is changing dramatically for one data point, but you have, just think of it as, like, evidence. I've got a thousand or a million people saying it should be this thing and then I've got this one other person who may be right, they may be right, but they're saying that I think it's this other thing and it greatly skews the classifier. And so just because you want to trust the masses, you will say, hey, that one person is really, seems really off base. And this is very much a choice. And so what typically, what you do is you sort of rank every data point in terms of how in distribution it is and then you choose your cutoff and that's a human decision. Another data-centric AI task is to establish consensus labels. So if you guys have heard of, like, the self-driving cars, of course, then a lot of the ways that these models are trained is you'll have an image and then they want really high-quality data. So they'll have, like, 20 different people label, is this a scene of a street or are we on a bridge? Is this a stop sign? Because they really need to get accurate labels. The question is, when you're training your model, if you're just going to train with one label for that image or one set of labels for that image, you can't use all 20 of your annotator's, you know, guesses. You have to somehow combine them into a single training label. So how do you do that in a way that maximizes model performance? Another one is active learning. And this is a very classical problem. You have some data set, you train a model, it has 80% performance. Okay, I want now to get my model to 85% performance, but I want to pay for as little new data as possible. Or I want to improve the current existing data as little as possible. What are my next steps? And you can automate that process. You can actually get good signal to optimize in a way that you minimize the amount of new data that you need to collect information for or label in order to achieve that model accuracy. And then a final example is curriculum learning, which we already mentioned. So these are some examples of data-centric AI tasks, many of which we'll cover over the next two weeks. All right, so there's a lot of hype around data-centric AI. For those who are familiar with Andrew Ng, he's a pretty well-known person in AI from Stanford and has been at Google Research and Baidi Research and done a lot of things found in Coursera. So he's been really excited about data-centric AI. And let's look at some reasons, you know, why and some of the things that we've seen in the news. For example, he mentioned that like 80% of an AI developer's time is actually just spent on data, which is kind of funny, right? You know, you're an AI developer, you're not like a data scientist, but yet you're doing data science work all the time. And so there's something happening here in the real world that there isn't as much until recently actually systematic, you know, learning and teaching around how do we go about doing this. Also, if you're not familiar, bad data is very, very troublesome for businesses and for the government and for economies. And it's estimated this is out of Harvard Business Review that it cost the US alone about three trillion dollars in a given year. And you might see this and think, okay, that's really bad. But the good news is like a lot of people think that we can actually solve a lot of that three trillion issue that bad data causes with data-centric AI techniques. And so there's a lot of hype around it because it means a lot to a lot of people. This is a quick example. I did this internship at Fair in 2016 and I was in Jan's group and Jeff Hinton came to visit. If you're not familiar, these two recently won the Nobel Prize of Computer Science, which is called the Turing Award. And so I think they're old friends. And Jan has a dataset MNIST. Are folks familiar with MNIST? Okay, cool. Very classical machine learning dataset and we've been training models on it for like over 20 years. And people generally assume that it has perfect labels because that's a very common assumption. Not maybe now in 2023, but definitely like when it first came out. And it's a very high quality dataset. And so Jeff Hinton was presenting at this time, I think, Capsule Networks. He's very excited about it. And his aha culminating moment of his talk was that he found the label error in Jan Lacun's dataset. And so he's very excited to show, hey, this five image is actually labeled a three in your dataset, Jan. And he's like, aha, I got you. And so I think that it's worth mentioning that this is where we were in 2016. And now, you know, we're only six, seven years later, and we're able to systematically find millions of errors in datasets. And that's sort of how far we've come using these data-centric AI approaches. Who here is familiar with Dolly and Dolly too? Yeah, it's pretty cool, right? So it generates images and they're pretty cool, like you can generate images of pretty much anything you describe. And so if you check out the Dolly demo page, and there's the link here in the slides, if you want to check them after, there's a cool video. And you'll notice in that video, they talk about one of their biggest challenges. And so this is just screenshots from the video. And the technology is constantly evolving, but Dolly too has limitations. It's taught with objects that are incorrectly labeled with plain labeled car, for example. And this happens because it's a massive dataset. So if you don't use data-centric AI approaches, it's very difficult to clean, you know, whatever, hundreds of millions or more, probably billions of pairs of text and image. And so what they notice is that a user might actually try to generate a car, but Dolly will actually create a plane, because it's seen wrongly labeled data. And so this is very problematic for something that's deployed in the real world. Another example from that video is they talk about generating these baboons, but they emphasize that you can only do this correctly if you have accurate labels. And if you didn't, you'll totally, you can get the wrong thing. And so the key takeaway here is that this is a real world technology, lots of people are using today at scale, but the reliability of that model really does depend on the data quality. Another big example is familiar with chat GPT. Does anybody know, yeah, does anybody know sort of why or like what was the big innovation from GPT three, which obviously had a huge hype around it to chat GPT, which has even more hype around it? What was sort of one of the big things they did between the two? Yeah, totally. And do you know what they were doing with their reinforcement learning? They like inviting some users to talk about these chat GPT and actually some of the work. Yeah, totally. Yeah. What was happening there was they, they had a lot of bad outputs, you know, like chat GPT three was saying things that were like super biased, or like inappropriate, not even true, just wrong facts. And these outputs were tied to data it was trained on. And to parameters in the model that were learned from that data. And so what they did is they did in a reinforcement setting, which means they talked to people, they use that information to then update the model, then the model sort of explores with new outputs, then people see those. But what they were doing is they were having people actually rank them in terms of the quality of the prediction, right? And so they were ranking the quality of this data and then using that to update the model so that it would have improved less bias and better outputs. And so that was the key idea of chat GPT was actually to deal with a data quality issue. And if you've tried both, you can see that there is a pretty big performance boost. The downside is that they had to do this with a lot of manual work. And so we went to work on ways to automate that. You guys are probably familiar with Tesla. So this is Tesla's data engine. This is from a talk by Andre Carpathi, who is formerly the Tesla director of AI. And this is their data engine. And we'll just start in the top left, like you, the way they're training the, you know, the self-driving Tesla model is, you know, you have some data source. And then you'll notice some problem, which is like, hey, we're in a tunnel. And we don't have a lot of, you know, tunnel data. So the car is like doing weird things, right? And so then what they would do is they would collect a bunch more data in tunnels and then update their training data and label them and then redeploy and go in the world and then see what breaks then. And this is a very, you know, difficult iterative process because you have to send the car out and then see where things break and then collect a bunch more data. Or if you had a way to automate, okay, this is where my data is missing. This is stuff that's out of distribution. This is where we have a bunch of label errors. And you're able to automate that process. They could have reduced those cycles and gotten the car out a lot faster, at least the AI part of the car. And so this was a big pain that, that Andre mentioned. Another really, this is a fantastic example that Jonas shared with me. These are all examples of traffic lights. You know, so imagine that like Elon Musk comes to you and he says, you know, Andre, I need you to get this car to navigate any street in the world. And then, you know, you're like, oh, that's cool. Okay. So like it needs to stop at traffic lights. Like that seems like a pretty simple problem. And then you go out in the real world and like traffic lights are not a simple problem. They're really complicated and they're really messy. And this is actually like a total nightmare if you had to do this. And so how do you find sort of systematic ways to group data together and train in a way that's robust and reliable? Like real world data is super messy and complicated. And so Andre's big takeaway was that, you know, he shares this juxtaposition of the amount of sleep, you know, he lost over in his PhD. And it was like data sets is this tiny sliver, but definitely spent a lot of time on models and algorithms. And then he goes and he's leading, you know, the AI model at Tesla. And it's like, it's all data, you know, it's a big shock when you make this shift. And so that's why we really want to focus on ways we can improve that. A very common use case is when you're trying to train a model with noisy labels. Okay. So this is like the classical scenario of your, you have the dogs and cats, but now say 30% of your cats are labeled dog. Okay. And maybe 20% of the dogs are labeled cat. So how do you get a model that does as well or close to as well as if you had perfectly labeled data? And we'll go into that more in the next lecture. But I want to just motivate that I looked at a bunch of model centric methods and data centric methods over the last five years out of top institutions like Google and Facebook and so forth. And we benchmark them. And it turns out, and there's a lot on this slide, but there's really one key takeaway that the data centric AI methods all outperformed the model centric methods for this particular task, you know, on this particular data set. And this was pretty revealing and compelling that there's something here to data centric AI approaches. And to be very clear, what these models are doing is they are modifying the loss function or modifying the model so that they sort of don't train as much on what they think is bad data, but within the context of the modeling. And what these methods are doing is they're actually modifying the data set. They're either removing bad data or they're generating more data that sort of makes the error go away, but somehow they're actually changing the data set. And this is just how things stack up. And you'll see the data centric methods outperform model centric methods in this task. And this is a very common task that's of interest to the field. So it's cool. It's cool to see that this stuff is working and we're getting some benefits from it. So a sort of culminating thought is, you know, we were talking a lot about ways that we want to automate, but what did we do before? So obviously we've had to improve data sets in industry and like outside of academia in the past. It's like, how did we do it before there was data centric AI? And so we mostly relied on human powered solutions. For example, we would just spend more money for higher quality data. It's like you literally just pay for more labels or you would pay for more data. And that was a very classical way to improve a model. Also building custom tools. So like you saw at Tesla, they have this whole sort of data platform. And this is a lot, right? This is for one specific problem, the very important problem, but they had to build a lot of custom tech around it. Another common thing is just fixing data inside a Jupyter notebook. So just a quick show of hands like how many people have used Jupyter notebooks. Okay, that's really good because all the labs are in Jupyter notebooks for the most part for this lecture or for this course. So yeah, what people do is like you just sort data by like a loss function. And so you just say like the loss for this data point is the highest. So I think this is most likely to be wrong. Let me check it out. And then you would look at it by hand and then you would market or do something with it or you take the top 20 or something, but you would just do a lot of this by hand inside of Jupyter notebooks and printing things out. And that was actually a pretty normal and standard way that like, you know, somebody in industry or data scientist or grad student would try to fix a data set. And so the whole idea is we're going to look at ways that we systematize these approaches so that they're more reliable, more accurate, and they work on most data sets. So this course is about the following. So today we're just looking at what is model centric guy versus data centric guy, get the juices flowing, think about how to think things in terms of data and the impact, why matters. Next lecture, we'll focus on label errors. So how do you actually detect label errors automatically? How do you learn with label errors? What are good methods to do that? And what are some things to think about when you're doing this? Data set creation and curation will be on Thursday. And this is how do you construct a data set in such a way that you can train a good model? How do you arrange, you know, the classes? How do you choose good examples? And then finally on Friday, which is related to data set curation, we'll look at active learning and potentially core sets and active learning. As I mentioned, this is task where you're trying to choose the next data you want to add to your data set and you want to obtain a label for. Or do you want to improve some of the labels you currently have? And so you're just trying to decide, I have to pay a cost for new data that I'm going to add to my data set. And it costs me something like either money or time. So I don't want to do it too much. So what's like the best stuff to add to my data set now to improve my model? Next week, we'll sort of have a bit of a shift and we'll focus more on data, but we'll focus on some specific things for the first on Monday, we'll focus on class and balance and distribution shift. So this is, imagine like it's the stock market, right? And like if you're trying to predict things, you know, on Monday or on in January of this year versus now, it would be a very different market, right? And so over time, data changes. And so how do you continue to produce good reliable predictions even though data is changing? And then the class imbalances this problem where imagine that for that line over there on the left, we had like a million pluses and only a few minuses. Well, then a smart classifier could actually just always predict plus and get near 100% accuracy. So often it will just ignore the minuses. So how do we get around that problem? Interpretable features of data. So this is, who here's familiar with interpretability? All right, not as much. That one will be fun then. And thanks for raising your hand. That should be a good class. We'll learn about how do you interpret data in a way that you can understand what's going on from a data perspective? Like why is the model doing what it's doing in terms of the data? And so we'll understand model performance based on data. The next class on Wednesday of next week will be on data-centric evaluation of ML models. So like, how do we know, you know, from a data perspective, how good a model is, how reliable it is, how well it's working? On Thursday, we'll look at encoding human priors. So this is like, how do we augment data and also prompt engineering. So this class will cover a lot of things like GPT and chat GPT and transformer models and stuff like that. And then finally, our last class will be on data privacy and security. And this is very, very interesting, especially for a lot of people in like banking and finance and they're using a lot of machine learning models. How do you make sure that like a model doesn't actually secretly encode the data? Or like somehow if you had access to predictions, you could figure out someone's, you know, banking info and you can imagine all sorts of things that can happen in AI. So data security and privacy is really important. Any sort of questions while I'm on this slide? Sweet. Are you guys excited? Okay. Does it look like a good course? Okay. All right. Cool. All right. There's a lab for every lecture. And so you can find this on the course website, which we can, we can write on the board. So you probably have seen it in the email, but just in case. And there's, there's a lab, usually will be Jupyter notebooks. And the one for today will be a text classification task. And it has some bad, bad data that's gotten mixed in. And this is actual data that's been scraped from, I think, Amazon reviews. And it has some bad tags, some weird HTML has gotten in there. And what you'll look at is model centric approaches at first. And you'll realize, Oh, it can only get you so far because like the data is not that great. And so you'll have to figure out how to improve the data set. You know, so you can get a better classifier. And so that's sort of today's lab. Get your hands wet with data centric AI. We have office hours every class, 3pm to 5pm. So an hour after the lecture ends every day. And then tomorrow's lecture will focus on label errors, how to find them and how to train better models. This is the folks who are teaching the course for folks that are from MIT, to from Stanford. And yeah, I think it'll be a good time. Really quick. Are there any questions? Yeah. My question is like, how do you know if it's like data that's the problem and not the model? Let's say you train a bunch of times in a model with different parameters. It's not like doing good. How do you know the data is a problem? Do you just try it and see if it improves or do you can use it another way to see that? Yeah, that's a good question. So there's a few ways. So one thing you can do is you can look at a subset of the problem. So say you had like a very big complex problem and there's thousands of classes and millions of data points. You can take just 10,000 of those data points and a few of those classes and actually check, do some process to check, make sure you have really high quality data and see how well does your model perform. And if your model is performing very, very high accuracy, but then when you use the original data, you get a drop off. That gives you a good signal. Another thing is if you have similar data set and it's like MNIST for example, you can get near 100% performance. And so you have a similar data set, but you're getting like much worse performance or like significantly less performance, but using the same architecture that you've seen do very well on a similar task on a different data set. And that's a good indicator. You should probably look at the data. And there's two more things. One is just, just take a look at the data. I think it's really easy to just get a data set and your goal is like train a model. And so you're doing a lot of cool, you know, download this TensorFlow package, download this hugging face package, but you don't actually take time usually to look through all the data points or like hundreds of data points and really see like, does this data look like what I think it does? Like, does it seem to be kind of messy? And what you'll often find is after first like, you know, first few hundred, you'll be like, oh, there's some weird stuff in here. And if you have millions of data points, that weird stuff adds up. Yeah, I guess the problem with the data is that there's like two months, so like, you know, like how do you connect with all of them? Yeah, I guess if you just look at like a bunch. Yeah, totally. In the next class, we'll show ways that you can actually rank your data set so that you know what's the best example to look at first. That's probably wrong. And so there are ways that you can automate this and then you can look at the initial data. And that's really the right approach. Like if you just look at random, then you might waste some time. But if you've ranked your data in a way that is likely to give you a good ranking on quality, and then you look at like the first 100 and there's really no issues and the data looks really good, then yeah, you might be able to just optimize the model and be okay. Any other questions? Is this useful for anyone sort of immediately? Is anyone thinking like, hey, this might be useful for what I'm working on right now? I haven't done anything specific about it before, and I think one of the reasons is because, you know, I always study models before. It seems like, you know, studying models is like very important, but once you go and you want to like quickly do something, that's kind of like the first thing that you want to do, you know, like, okay, what they do is data and how they work. Yeah. All right, great. We'll be here for a little bit after. Thanks everybody for coming. The next lectures will be a bit more technical, but I think it should be a really exciting course, and glad to have everybody here. Thanks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " Alright, let's get started. Thanks for everybody who showed up. We've got three of the course", "tokens": [50364, 2798, 11, 718, 311, 483, 1409, 13, 2561, 337, 2201, 567, 4712, 493, 13, 492, 600, 658, 1045, 295, 264, 1164, 50864], "temperature": 0.0, "avg_logprob": -0.18249834060668946, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.1046280488371849}, {"id": 1, "seek": 0, "start": 10.0, "end": 17.92, "text": " instructors here, myself, Anish in the back, and Jonas. And Anish is finishing up his final", "tokens": [50864, 28367, 510, 11, 2059, 11, 1107, 742, 294, 264, 646, 11, 293, 34630, 13, 400, 1107, 742, 307, 12693, 493, 702, 2572, 51260], "temperature": 0.0, "avg_logprob": -0.18249834060668946, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.1046280488371849}, {"id": 2, "seek": 0, "start": 17.92, "end": 23.12, "text": " year, but we all were here. We all were students here. We all did our PhDs here. And it's good", "tokens": [51260, 1064, 11, 457, 321, 439, 645, 510, 13, 492, 439, 645, 1731, 510, 13, 492, 439, 630, 527, 14476, 82, 510, 13, 400, 309, 311, 665, 51520], "temperature": 0.0, "avg_logprob": -0.18249834060668946, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.1046280488371849}, {"id": 3, "seek": 0, "start": 23.12, "end": 27.400000000000002, "text": " to be back. If you don't know any machine learning at all, you've never taken an intro", "tokens": [51520, 281, 312, 646, 13, 759, 291, 500, 380, 458, 604, 3479, 2539, 412, 439, 11, 291, 600, 1128, 2726, 364, 12897, 51734], "temperature": 0.0, "avg_logprob": -0.18249834060668946, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.1046280488371849}, {"id": 4, "seek": 2740, "start": 27.4, "end": 32.519999999999996, "text": " class, then this will be probably a little too confusing. But as long as you've seen a", "tokens": [50364, 1508, 11, 550, 341, 486, 312, 1391, 257, 707, 886, 13181, 13, 583, 382, 938, 382, 291, 600, 1612, 257, 50620], "temperature": 0.0, "avg_logprob": -0.17638750076293946, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.033539921045303345}, {"id": 5, "seek": 2740, "start": 32.519999999999996, "end": 37.8, "text": " little ML, then you'll have a good time. And ideally, you know a little bit of Python,", "tokens": [50620, 707, 21601, 11, 550, 291, 603, 362, 257, 665, 565, 13, 400, 22915, 11, 291, 458, 257, 707, 857, 295, 15329, 11, 50884], "temperature": 0.0, "avg_logprob": -0.17638750076293946, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.033539921045303345}, {"id": 6, "seek": 2740, "start": 37.8, "end": 43.959999999999994, "text": " and you know it like NumPy and Pandasar. So with that, let's just get started. So our", "tokens": [50884, 293, 291, 458, 309, 411, 22592, 47, 88, 293, 16995, 296, 289, 13, 407, 365, 300, 11, 718, 311, 445, 483, 1409, 13, 407, 527, 51192], "temperature": 0.0, "avg_logprob": -0.17638750076293946, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.033539921045303345}, {"id": 7, "seek": 2740, "start": 43.959999999999994, "end": 49.36, "text": " goal in this course is to learn about data-centric AI. So just like a quick show of hands, prior", "tokens": [51192, 3387, 294, 341, 1164, 307, 281, 1466, 466, 1412, 12, 45300, 7318, 13, 407, 445, 411, 257, 1702, 855, 295, 2377, 11, 4059, 51462], "temperature": 0.0, "avg_logprob": -0.17638750076293946, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.033539921045303345}, {"id": 8, "seek": 4936, "start": 49.36, "end": 57.8, "text": " to this course, how many people I've heard of data-centric AI? Okay, alright, well, get", "tokens": [50364, 281, 341, 1164, 11, 577, 867, 561, 286, 600, 2198, 295, 1412, 12, 45300, 7318, 30, 1033, 11, 5845, 11, 731, 11, 483, 50786], "temperature": 0.0, "avg_logprob": -0.18203321416327295, "compression_ratio": 1.4936708860759493, "no_speech_prob": 0.14397931098937988}, {"id": 9, "seek": 4936, "start": 57.8, "end": 61.92, "text": " excited because you're going to learn some great things. It'll be really good. Alright,", "tokens": [50786, 2919, 570, 291, 434, 516, 281, 1466, 512, 869, 721, 13, 467, 603, 312, 534, 665, 13, 2798, 11, 50992], "temperature": 0.0, "avg_logprob": -0.18203321416327295, "compression_ratio": 1.4936708860759493, "no_speech_prob": 0.14397931098937988}, {"id": 10, "seek": 4936, "start": 61.92, "end": 70.92, "text": " so let's just jump in. So this is a picture of a self-driving car that has had an accident.", "tokens": [50992, 370, 718, 311, 445, 3012, 294, 13, 407, 341, 307, 257, 3036, 295, 257, 2698, 12, 47094, 1032, 300, 575, 632, 364, 6398, 13, 51442], "temperature": 0.0, "avg_logprob": -0.18203321416327295, "compression_ratio": 1.4936708860759493, "no_speech_prob": 0.14397931098937988}, {"id": 11, "seek": 4936, "start": 70.92, "end": 77.36, "text": " And you notice that the article focuses on when algorithms mess up, right? And this is", "tokens": [51442, 400, 291, 3449, 300, 264, 7222, 16109, 322, 562, 14642, 2082, 493, 11, 558, 30, 400, 341, 307, 51764], "temperature": 0.0, "avg_logprob": -0.18203321416327295, "compression_ratio": 1.4936708860759493, "no_speech_prob": 0.14397931098937988}, {"id": 12, "seek": 7736, "start": 77.36, "end": 80.76, "text": " the common case, right? In machine learning, we tend to focus on the model. And so it's", "tokens": [50364, 264, 2689, 1389, 11, 558, 30, 682, 3479, 2539, 11, 321, 3928, 281, 1879, 322, 264, 2316, 13, 400, 370, 309, 311, 50534], "temperature": 0.0, "avg_logprob": -0.10823036730289459, "compression_ratio": 1.752442996742671, "no_speech_prob": 0.03406434506177902}, {"id": 13, "seek": 7736, "start": 80.76, "end": 86.36, "text": " like there's a crash, the algorithm must have done something wrong. And so I'd like us to", "tokens": [50534, 411, 456, 311, 257, 8252, 11, 264, 9284, 1633, 362, 1096, 746, 2085, 13, 400, 370, 286, 1116, 411, 505, 281, 50814], "temperature": 0.0, "avg_logprob": -0.10823036730289459, "compression_ratio": 1.752442996742671, "no_speech_prob": 0.03406434506177902}, {"id": 14, "seek": 7736, "start": 86.36, "end": 91.36, "text": " sort of think of a different way of thinking about this. This is a paper many folks are", "tokens": [50814, 1333, 295, 519, 295, 257, 819, 636, 295, 1953, 466, 341, 13, 639, 307, 257, 3035, 867, 4024, 366, 51064], "temperature": 0.0, "avg_logprob": -0.10823036730289459, "compression_ratio": 1.752442996742671, "no_speech_prob": 0.03406434506177902}, {"id": 15, "seek": 7736, "start": 91.36, "end": 96.52, "text": " familiar with, which basically shows one thing in that a neural network or a machine learning", "tokens": [51064, 4963, 365, 11, 597, 1936, 3110, 472, 551, 294, 300, 257, 18161, 3209, 420, 257, 3479, 2539, 51322], "temperature": 0.0, "avg_logprob": -0.10823036730289459, "compression_ratio": 1.752442996742671, "no_speech_prob": 0.03406434506177902}, {"id": 16, "seek": 7736, "start": 96.52, "end": 101.52, "text": " classifier can learn total randomness. It's like if you give it complete garbage data,", "tokens": [51322, 1508, 9902, 393, 1466, 3217, 4974, 1287, 13, 467, 311, 411, 498, 291, 976, 309, 3566, 14150, 1412, 11, 51572], "temperature": 0.0, "avg_logprob": -0.10823036730289459, "compression_ratio": 1.752442996742671, "no_speech_prob": 0.03406434506177902}, {"id": 17, "seek": 7736, "start": 101.52, "end": 105.96000000000001, "text": " just like completely random labels, it can learn to map like images to completely arbitrary", "tokens": [51572, 445, 411, 2584, 4974, 16949, 11, 309, 393, 1466, 281, 4471, 411, 5267, 281, 2584, 23211, 51794], "temperature": 0.0, "avg_logprob": -0.10823036730289459, "compression_ratio": 1.752442996742671, "no_speech_prob": 0.03406434506177902}, {"id": 18, "seek": 10596, "start": 105.96, "end": 110.67999999999999, "text": " labels or text to completely arbitrary labels. So basically, if you give it really bad data,", "tokens": [50364, 16949, 420, 2487, 281, 2584, 23211, 16949, 13, 407, 1936, 11, 498, 291, 976, 309, 534, 1578, 1412, 11, 50600], "temperature": 0.0, "avg_logprob": -0.11822704032615379, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.0017542518908157945}, {"id": 19, "seek": 10596, "start": 110.67999999999999, "end": 115.52, "text": " it will just produce exactly what it learns, even if the data is completely wrong. And", "tokens": [50600, 309, 486, 445, 5258, 2293, 437, 309, 27152, 11, 754, 498, 264, 1412, 307, 2584, 2085, 13, 400, 50842], "temperature": 0.0, "avg_logprob": -0.11822704032615379, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.0017542518908157945}, {"id": 20, "seek": 10596, "start": 115.52, "end": 120.32, "text": " so I'd like to sort of rethink this title of this article as when algorithms are trained", "tokens": [50842, 370, 286, 1116, 411, 281, 1333, 295, 34595, 341, 4876, 295, 341, 7222, 382, 562, 14642, 366, 8895, 51082], "temperature": 0.0, "avg_logprob": -0.11822704032615379, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.0017542518908157945}, {"id": 21, "seek": 10596, "start": 120.32, "end": 125.28, "text": " with erroneous data, things like car crashes can happen. And that's the way we'll sort", "tokens": [51082, 365, 1189, 26446, 563, 1412, 11, 721, 411, 1032, 28642, 393, 1051, 13, 400, 300, 311, 264, 636, 321, 603, 1333, 51330], "temperature": 0.0, "avg_logprob": -0.11822704032615379, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.0017542518908157945}, {"id": 22, "seek": 10596, "start": 125.28, "end": 131.88, "text": " of focus and think about this course. So traditional machine learning is very model-centric, right?", "tokens": [51330, 295, 1879, 293, 519, 466, 341, 1164, 13, 407, 5164, 3479, 2539, 307, 588, 2316, 12, 45300, 11, 558, 30, 51660], "temperature": 0.0, "avg_logprob": -0.11822704032615379, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.0017542518908157945}, {"id": 23, "seek": 13188, "start": 131.88, "end": 136.07999999999998, "text": " Like when you take an ML class, you first learn machine learning, you're in school and", "tokens": [50364, 1743, 562, 291, 747, 364, 21601, 1508, 11, 291, 700, 1466, 3479, 2539, 11, 291, 434, 294, 1395, 293, 50574], "temperature": 0.0, "avg_logprob": -0.1514706508718806, "compression_ratio": 2.08203125, "no_speech_prob": 0.07799448817968369}, {"id": 24, "seek": 13188, "start": 136.07999999999998, "end": 140.16, "text": " then you get a data set. And usually the data set is pretty good. Like if anyone's seen", "tokens": [50574, 550, 291, 483, 257, 1412, 992, 13, 400, 2673, 264, 1412, 992, 307, 1238, 665, 13, 1743, 498, 2878, 311, 1612, 50778], "temperature": 0.0, "avg_logprob": -0.1514706508718806, "compression_ratio": 2.08203125, "no_speech_prob": 0.07799448817968369}, {"id": 25, "seek": 13188, "start": 140.16, "end": 145.84, "text": " like the cat dogs data set, you know, it's in images of cats and they're all cats and", "tokens": [50778, 411, 264, 3857, 7197, 1412, 992, 11, 291, 458, 11, 309, 311, 294, 5267, 295, 11111, 293, 436, 434, 439, 11111, 293, 51062], "temperature": 0.0, "avg_logprob": -0.1514706508718806, "compression_ratio": 2.08203125, "no_speech_prob": 0.07799448817968369}, {"id": 26, "seek": 13188, "start": 145.84, "end": 149.4, "text": " they're labeled cat. And then there's images of dog and they're all dogs and they're labeled", "tokens": [51062, 436, 434, 21335, 3857, 13, 400, 550, 456, 311, 5267, 295, 3000, 293, 436, 434, 439, 7197, 293, 436, 434, 21335, 51240], "temperature": 0.0, "avg_logprob": -0.1514706508718806, "compression_ratio": 2.08203125, "no_speech_prob": 0.07799448817968369}, {"id": 27, "seek": 13188, "start": 149.4, "end": 154.44, "text": " dog and there's no, there's usually no like cows thrown in there, right? And there's usually", "tokens": [51240, 3000, 293, 456, 311, 572, 11, 456, 311, 2673, 572, 411, 19148, 11732, 294, 456, 11, 558, 30, 400, 456, 311, 2673, 51492], "temperature": 0.0, "avg_logprob": -0.1514706508718806, "compression_ratio": 2.08203125, "no_speech_prob": 0.07799448817968369}, {"id": 28, "seek": 13188, "start": 154.44, "end": 158.88, "text": " not like a bunch of dogs that are labeled cat. It's like pretty well curated. And then", "tokens": [51492, 406, 411, 257, 3840, 295, 7197, 300, 366, 21335, 3857, 13, 467, 311, 411, 1238, 731, 47851, 13, 400, 550, 51714], "temperature": 0.0, "avg_logprob": -0.1514706508718806, "compression_ratio": 2.08203125, "no_speech_prob": 0.07799448817968369}, {"id": 29, "seek": 15888, "start": 158.88, "end": 163.51999999999998, "text": " your goal is to, you know, produce a good model, right? You want to train a model that", "tokens": [50364, 428, 3387, 307, 281, 11, 291, 458, 11, 5258, 257, 665, 2316, 11, 558, 30, 509, 528, 281, 3847, 257, 2316, 300, 50596], "temperature": 0.0, "avg_logprob": -0.1344001851183303, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.014951291494071484}, {"id": 30, "seek": 15888, "start": 163.51999999999998, "end": 168.76, "text": " takes in a new image that's either a cat or a dog and it predicts is it cat or is it dog.", "tokens": [50596, 2516, 294, 257, 777, 3256, 300, 311, 2139, 257, 3857, 420, 257, 3000, 293, 309, 6069, 82, 307, 309, 3857, 420, 307, 309, 3000, 13, 50858], "temperature": 0.0, "avg_logprob": -0.1344001851183303, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.014951291494071484}, {"id": 31, "seek": 15888, "start": 168.76, "end": 174.04, "text": " And that's sort of how we learn machine learning usually. And this is something that's standard", "tokens": [50858, 400, 300, 311, 1333, 295, 577, 321, 1466, 3479, 2539, 2673, 13, 400, 341, 307, 746, 300, 311, 3832, 51122], "temperature": 0.0, "avg_logprob": -0.1344001851183303, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.014951291494071484}, {"id": 32, "seek": 15888, "start": 174.04, "end": 180.12, "text": " if anyone here has taken like 6036, which has been renamed to 63390, the intro to ML class", "tokens": [51122, 498, 2878, 510, 575, 2726, 411, 4060, 11309, 11, 597, 575, 668, 40949, 281, 1386, 10191, 7771, 11, 264, 12897, 281, 21601, 1508, 51426], "temperature": 0.0, "avg_logprob": -0.1344001851183303, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.014951291494071484}, {"id": 33, "seek": 15888, "start": 180.12, "end": 184.56, "text": " here. And you'll learn stuff like, you know, different types of models. And if you're familiar", "tokens": [51426, 510, 13, 400, 291, 603, 1466, 1507, 411, 11, 291, 458, 11, 819, 3467, 295, 5245, 13, 400, 498, 291, 434, 4963, 51648], "temperature": 0.0, "avg_logprob": -0.1344001851183303, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.014951291494071484}, {"id": 34, "seek": 15888, "start": 184.56, "end": 187.96, "text": " with neural networks, neural architectures, or you'll learn about tuning hyper parameters", "tokens": [51648, 365, 18161, 9590, 11, 18161, 6331, 1303, 11, 420, 291, 603, 1466, 466, 15164, 9848, 9834, 51818], "temperature": 0.0, "avg_logprob": -0.1344001851183303, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.014951291494071484}, {"id": 35, "seek": 18796, "start": 187.96, "end": 193.28, "text": " of a model, or you'll learn about modifying the loss function using different loss functions", "tokens": [50364, 295, 257, 2316, 11, 420, 291, 603, 1466, 466, 42626, 264, 4470, 2445, 1228, 819, 4470, 6828, 50630], "temperature": 0.0, "avg_logprob": -0.14035199293449743, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.007119429763406515}, {"id": 36, "seek": 18796, "start": 193.28, "end": 196.84, "text": " and regularizations that you don't overfit. And these are common things you learn in model", "tokens": [50630, 293, 3890, 14455, 300, 291, 500, 380, 670, 6845, 13, 400, 613, 366, 2689, 721, 291, 1466, 294, 2316, 50808], "temperature": 0.0, "avg_logprob": -0.14035199293449743, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.007119429763406515}, {"id": 37, "seek": 18796, "start": 196.84, "end": 203.0, "text": " centric AI. So let's just juxtapose that with the real world once you're out of the classroom.", "tokens": [50808, 1489, 1341, 7318, 13, 407, 718, 311, 445, 3649, 734, 569, 541, 300, 365, 264, 957, 1002, 1564, 291, 434, 484, 295, 264, 7419, 13, 51116], "temperature": 0.0, "avg_logprob": -0.14035199293449743, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.007119429763406515}, {"id": 38, "seek": 18796, "start": 203.0, "end": 205.94, "text": " So in the classroom, usually the data sets fixed and it's pretty good, but then you", "tokens": [51116, 407, 294, 264, 7419, 11, 2673, 264, 1412, 6352, 6806, 293, 309, 311, 1238, 665, 11, 457, 550, 291, 51263], "temperature": 0.0, "avg_logprob": -0.14035199293449743, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.007119429763406515}, {"id": 39, "seek": 18796, "start": 205.94, "end": 210.20000000000002, "text": " go to the real world and actually the data set is not fixed, right? You have user data", "tokens": [51263, 352, 281, 264, 957, 1002, 293, 767, 264, 1412, 992, 307, 406, 6806, 11, 558, 30, 509, 362, 4195, 1412, 51476], "temperature": 0.0, "avg_logprob": -0.14035199293449743, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.007119429763406515}, {"id": 40, "seek": 18796, "start": 210.20000000000002, "end": 215.52, "text": " or customer data or real world data and you can get more or less, you can change the data", "tokens": [51476, 420, 5474, 1412, 420, 957, 1002, 1412, 293, 291, 393, 483, 544, 420, 1570, 11, 291, 393, 1319, 264, 1412, 51742], "temperature": 0.0, "avg_logprob": -0.14035199293449743, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.007119429763406515}, {"id": 41, "seek": 21552, "start": 215.52, "end": 221.0, "text": " and the data has all sorts of weird things in it. And so what tends to happen is that", "tokens": [50364, 293, 264, 1412, 575, 439, 7527, 295, 3657, 721, 294, 309, 13, 400, 370, 437, 12258, 281, 1051, 307, 300, 50638], "temperature": 0.0, "avg_logprob": -0.10481655263455114, "compression_ratio": 1.764, "no_speech_prob": 0.01638985052704811}, {"id": 42, "seek": 21552, "start": 221.0, "end": 226.20000000000002, "text": " the company or the user, whoever's sort of using this model that you're trying to train,", "tokens": [50638, 264, 2237, 420, 264, 4195, 11, 11387, 311, 1333, 295, 1228, 341, 2316, 300, 291, 434, 1382, 281, 3847, 11, 50898], "temperature": 0.0, "avg_logprob": -0.10481655263455114, "compression_ratio": 1.764, "no_speech_prob": 0.01638985052704811}, {"id": 43, "seek": 21552, "start": 226.20000000000002, "end": 231.52, "text": " it doesn't really care as much about the cool ML fancy tricks as it does about like, does", "tokens": [50898, 309, 1177, 380, 534, 1127, 382, 709, 466, 264, 1627, 21601, 10247, 11733, 382, 309, 775, 466, 411, 11, 775, 51164], "temperature": 0.0, "avg_logprob": -0.10481655263455114, "compression_ratio": 1.764, "no_speech_prob": 0.01638985052704811}, {"id": 44, "seek": 21552, "start": 231.52, "end": 235.28, "text": " it actually work in the real world? And if you have really good machine learning models", "tokens": [51164, 309, 767, 589, 294, 264, 957, 1002, 30, 400, 498, 291, 362, 534, 665, 3479, 2539, 5245, 51352], "temperature": 0.0, "avg_logprob": -0.10481655263455114, "compression_ratio": 1.764, "no_speech_prob": 0.01638985052704811}, {"id": 45, "seek": 21552, "start": 235.28, "end": 241.04000000000002, "text": " that work on highly curated data, but then the real world data is actually really messy,", "tokens": [51352, 300, 589, 322, 5405, 47851, 1412, 11, 457, 550, 264, 957, 1002, 1412, 307, 767, 534, 16191, 11, 51640], "temperature": 0.0, "avg_logprob": -0.10481655263455114, "compression_ratio": 1.764, "no_speech_prob": 0.01638985052704811}, {"id": 46, "seek": 24104, "start": 241.04, "end": 246.12, "text": " and it makes sense to actually focus on fixing the issues in the data. And a lot of people", "tokens": [50364, 293, 309, 1669, 2020, 281, 767, 1879, 322, 19442, 264, 2663, 294, 264, 1412, 13, 400, 257, 688, 295, 561, 50618], "temperature": 0.0, "avg_logprob": -0.10175070032343134, "compression_ratio": 1.7038461538461538, "no_speech_prob": 0.03729844093322754}, {"id": 47, "seek": 24104, "start": 246.12, "end": 249.51999999999998, "text": " have been doing this, but what are systematic ways to do this? And that's what we'll focus", "tokens": [50618, 362, 668, 884, 341, 11, 457, 437, 366, 27249, 2098, 281, 360, 341, 30, 400, 300, 311, 437, 321, 603, 1879, 50788], "temperature": 0.0, "avg_logprob": -0.10175070032343134, "compression_ratio": 1.7038461538461538, "no_speech_prob": 0.03729844093322754}, {"id": 48, "seek": 24104, "start": 249.51999999999998, "end": 255.07999999999998, "text": " on this course. What may surprise some of you that you may not know is that 10 of the", "tokens": [50788, 322, 341, 1164, 13, 708, 815, 6365, 512, 295, 291, 300, 291, 815, 406, 458, 307, 300, 1266, 295, 264, 51066], "temperature": 0.0, "avg_logprob": -0.10175070032343134, "compression_ratio": 1.7038461538461538, "no_speech_prob": 0.03729844093322754}, {"id": 49, "seek": 24104, "start": 255.07999999999998, "end": 261.84, "text": " most commonly cited and most used test sets in the field of machine learning all have", "tokens": [51066, 881, 12719, 30134, 293, 881, 1143, 1500, 6352, 294, 264, 2519, 295, 3479, 2539, 439, 362, 51404], "temperature": 0.0, "avg_logprob": -0.10175070032343134, "compression_ratio": 1.7038461538461538, "no_speech_prob": 0.03729844093322754}, {"id": 50, "seek": 24104, "start": 261.84, "end": 268.08, "text": " wrong labels. And that may be a surprise for some of you. So we'll just take a quick look", "tokens": [51404, 2085, 16949, 13, 400, 300, 815, 312, 257, 6365, 337, 512, 295, 291, 13, 407, 321, 603, 445, 747, 257, 1702, 574, 51716], "temperature": 0.0, "avg_logprob": -0.10175070032343134, "compression_ratio": 1.7038461538461538, "no_speech_prob": 0.03729844093322754}, {"id": 51, "seek": 26808, "start": 268.08, "end": 279.03999999999996, "text": " at this website, which I think will be pretty fun. This is labelairs.com. Can we see okay?", "tokens": [50364, 412, 341, 3144, 11, 597, 286, 519, 486, 312, 1238, 1019, 13, 639, 307, 7645, 4094, 13, 1112, 13, 1664, 321, 536, 1392, 30, 50912], "temperature": 0.0, "avg_logprob": -0.14234271971117549, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.009703990072011948}, {"id": 52, "seek": 26808, "start": 279.03999999999996, "end": 283.15999999999997, "text": " So this is a site that you can check out on your own. So this is ImageNet, which is a", "tokens": [50912, 407, 341, 307, 257, 3621, 300, 291, 393, 1520, 484, 322, 428, 1065, 13, 407, 341, 307, 29903, 31890, 11, 597, 307, 257, 51118], "temperature": 0.0, "avg_logprob": -0.14234271971117549, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.009703990072011948}, {"id": 53, "seek": 26808, "start": 283.15999999999997, "end": 288.2, "text": " common image data set. And these are in the test set, or in this case a validation set,", "tokens": [51118, 2689, 3256, 1412, 992, 13, 400, 613, 366, 294, 264, 1500, 992, 11, 420, 294, 341, 1389, 257, 24071, 992, 11, 51370], "temperature": 0.0, "avg_logprob": -0.14234271971117549, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.009703990072011948}, {"id": 54, "seek": 26808, "start": 288.2, "end": 292.4, "text": " which is what was released. But this is the data that is supposed to be the most accurate", "tokens": [51370, 597, 307, 437, 390, 4736, 13, 583, 341, 307, 264, 1412, 300, 307, 3442, 281, 312, 264, 881, 8559, 51580], "temperature": 0.0, "avg_logprob": -0.14234271971117549, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.009703990072011948}, {"id": 55, "seek": 26808, "start": 292.4, "end": 296.71999999999997, "text": " data, right? This is like your real world test data. And so this data should be some", "tokens": [51580, 1412, 11, 558, 30, 639, 307, 411, 428, 957, 1002, 1500, 1412, 13, 400, 370, 341, 1412, 820, 312, 512, 51796], "temperature": 0.0, "avg_logprob": -0.14234271971117549, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.009703990072011948}, {"id": 56, "seek": 29672, "start": 296.72, "end": 301.12, "text": " of the most highly curated, most accurate data. And what you'll find is that, for example,", "tokens": [50364, 295, 264, 881, 5405, 47851, 11, 881, 8559, 1412, 13, 400, 437, 291, 603, 915, 307, 300, 11, 337, 1365, 11, 50584], "temperature": 0.0, "avg_logprob": -0.129369208483192, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.03306639939546585}, {"id": 57, "seek": 29672, "start": 301.12, "end": 306.72, "text": " this scorpion is labeled tick. And you'll find all sorts of stuff. We can look at another", "tokens": [50584, 341, 46092, 313, 307, 21335, 5204, 13, 400, 291, 603, 915, 439, 7527, 295, 1507, 13, 492, 393, 574, 412, 1071, 50864], "temperature": 0.0, "avg_logprob": -0.129369208483192, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.03306639939546585}, {"id": 58, "seek": 29672, "start": 306.72, "end": 313.56, "text": " data. This is by Google. This is a drawing hand drawn data set. This is labeled a t-shirt.", "tokens": [50864, 1412, 13, 639, 307, 538, 3329, 13, 639, 307, 257, 6316, 1011, 10117, 1412, 992, 13, 639, 307, 21335, 257, 256, 12, 15313, 13, 51206], "temperature": 0.0, "avg_logprob": -0.129369208483192, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.03306639939546585}, {"id": 59, "seek": 29672, "start": 313.56, "end": 318.96000000000004, "text": " And this is labeled a diving board. And this is labeled a scorpion. And it just keeps going", "tokens": [51206, 400, 341, 307, 21335, 257, 20241, 3150, 13, 400, 341, 307, 21335, 257, 46092, 313, 13, 400, 309, 445, 5965, 516, 51476], "temperature": 0.0, "avg_logprob": -0.129369208483192, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.03306639939546585}, {"id": 60, "seek": 29672, "start": 318.96000000000004, "end": 323.32000000000005, "text": " and going. And there's some fun ones. Like this is a, this is totally a cake, but it's", "tokens": [51476, 293, 516, 13, 400, 456, 311, 512, 1019, 2306, 13, 1743, 341, 307, 257, 11, 341, 307, 3879, 257, 5908, 11, 457, 309, 311, 51694], "temperature": 0.0, "avg_logprob": -0.129369208483192, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.03306639939546585}, {"id": 61, "seek": 32332, "start": 323.36, "end": 328.08, "text": " labeled a rake. Like it just didn't quite get the R to go all the way when they were", "tokens": [50366, 21335, 257, 367, 619, 13, 1743, 309, 445, 994, 380, 1596, 483, 264, 497, 281, 352, 439, 264, 636, 562, 436, 645, 50602], "temperature": 0.0, "avg_logprob": -0.1326592763264974, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.07582268118858337}, {"id": 62, "seek": 32332, "start": 328.08, "end": 331.71999999999997, "text": " pretending they're writing it out. But anyway, you can check these out on your own. This", "tokens": [50602, 22106, 436, 434, 3579, 309, 484, 13, 583, 4033, 11, 291, 393, 1520, 613, 484, 322, 428, 1065, 13, 639, 50784], "temperature": 0.0, "avg_logprob": -0.1326592763264974, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.07582268118858337}, {"id": 63, "seek": 32332, "start": 331.71999999999997, "end": 336.56, "text": " is for any type of data. So like text data or audio data. And you'll have fun. And I", "tokens": [50784, 307, 337, 604, 2010, 295, 1412, 13, 407, 411, 2487, 1412, 420, 6278, 1412, 13, 400, 291, 603, 362, 1019, 13, 400, 286, 51026], "temperature": 0.0, "avg_logprob": -0.1326592763264974, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.07582268118858337}, {"id": 64, "seek": 32332, "start": 336.56, "end": 340.12, "text": " think the good idea to think about here is that this is, you know, a data set that's", "tokens": [51026, 519, 264, 665, 1558, 281, 519, 466, 510, 307, 300, 341, 307, 11, 291, 458, 11, 257, 1412, 992, 300, 311, 51204], "temperature": 0.0, "avg_logprob": -0.1326592763264974, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.07582268118858337}, {"id": 65, "seek": 32332, "start": 340.12, "end": 346.0, "text": " released by Google. It's a benchmark data set. And it's very difficult when you have", "tokens": [51204, 4736, 538, 3329, 13, 467, 311, 257, 18927, 1412, 992, 13, 400, 309, 311, 588, 2252, 562, 291, 362, 51498], "temperature": 0.0, "avg_logprob": -0.1326592763264974, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.07582268118858337}, {"id": 66, "seek": 32332, "start": 346.0, "end": 350.92, "text": " millions of examples to know, like, what's the bad data? And ideally, if you didn't have", "tokens": [51498, 6803, 295, 5110, 281, 458, 11, 411, 11, 437, 311, 264, 1578, 1412, 30, 400, 22915, 11, 498, 291, 994, 380, 362, 51744], "temperature": 0.0, "avg_logprob": -0.1326592763264974, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.07582268118858337}, {"id": 67, "seek": 35092, "start": 350.92, "end": 355.08000000000004, "text": " that bad data, you could train better models. And so we need to learn how can we find these", "tokens": [50364, 300, 1578, 1412, 11, 291, 727, 3847, 1101, 5245, 13, 400, 370, 321, 643, 281, 1466, 577, 393, 321, 915, 613, 50572], "temperature": 0.0, "avg_logprob": -0.18158713880791721, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0012064044130966067}, {"id": 68, "seek": 35092, "start": 355.08000000000004, "end": 361.08000000000004, "text": " errors and find these issues automatically. So going back to the slides. So as seasoned", "tokens": [50572, 13603, 293, 915, 613, 2663, 6772, 13, 407, 516, 646, 281, 264, 9788, 13, 407, 382, 30111, 50872], "temperature": 0.0, "avg_logprob": -0.18158713880791721, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0012064044130966067}, {"id": 69, "seek": 35092, "start": 368.72, "end": 371.96000000000004, "text": " data scientists, the way they'll approach this problem is that it's more worthwhile to", "tokens": [51254, 1412, 7708, 11, 264, 636, 436, 603, 3109, 341, 1154, 307, 300, 309, 311, 544, 28159, 281, 51416], "temperature": 0.0, "avg_logprob": -0.18158713880791721, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0012064044130966067}, {"id": 70, "seek": 35092, "start": 371.96000000000004, "end": 375.92, "text": " invest in exploring and fixing the data than trying to tinker with the models to avoid this", "tokens": [51416, 1963, 294, 12736, 293, 19442, 264, 1412, 813, 1382, 281, 256, 40467, 365, 264, 5245, 281, 5042, 341, 51614], "temperature": 0.0, "avg_logprob": -0.18158713880791721, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0012064044130966067}, {"id": 71, "seek": 37592, "start": 375.92, "end": 380.56, "text": " garbage in, garbage out issue. And the issue is that, like, if you have millions of data,", "tokens": [50364, 14150, 294, 11, 14150, 484, 2734, 13, 400, 264, 2734, 307, 300, 11, 411, 11, 498, 291, 362, 6803, 295, 1412, 11, 50596], "temperature": 0.0, "avg_logprob": -0.16293021794911977, "compression_ratio": 1.8600682593856654, "no_speech_prob": 0.01911775767803192}, {"id": 72, "seek": 37592, "start": 380.56, "end": 383.96000000000004, "text": " right, or you have, like, a data set that's like 100 million, like, how do you do that", "tokens": [50596, 558, 11, 420, 291, 362, 11, 411, 11, 257, 1412, 992, 300, 311, 411, 2319, 2459, 11, 411, 11, 577, 360, 291, 360, 300, 50766], "temperature": 0.0, "avg_logprob": -0.16293021794911977, "compression_ratio": 1.8600682593856654, "no_speech_prob": 0.01911775767803192}, {"id": 73, "seek": 37592, "start": 383.96000000000004, "end": 388.6, "text": " without it being highly time consuming? All right. So we're here in a data-centric AI", "tokens": [50766, 1553, 309, 885, 5405, 565, 19867, 30, 1057, 558, 13, 407, 321, 434, 510, 294, 257, 1412, 12, 45300, 7318, 50998], "temperature": 0.0, "avg_logprob": -0.16293021794911977, "compression_ratio": 1.8600682593856654, "no_speech_prob": 0.01911775767803192}, {"id": 74, "seek": 37592, "start": 388.6, "end": 394.68, "text": " course. We've called this introduction to data-centric AI because we want it to be accessible. And", "tokens": [50998, 1164, 13, 492, 600, 1219, 341, 9339, 281, 1412, 12, 45300, 7318, 570, 321, 528, 309, 281, 312, 9515, 13, 400, 51302], "temperature": 0.0, "avg_logprob": -0.16293021794911977, "compression_ratio": 1.8600682593856654, "no_speech_prob": 0.01911775767803192}, {"id": 75, "seek": 37592, "start": 394.68, "end": 398.32, "text": " so we're going to cover just, like, the very intro, what is data-centric AI? How does it", "tokens": [51302, 370, 321, 434, 516, 281, 2060, 445, 11, 411, 11, 264, 588, 12897, 11, 437, 307, 1412, 12, 45300, 7318, 30, 1012, 775, 309, 51484], "temperature": 0.0, "avg_logprob": -0.16293021794911977, "compression_ratio": 1.8600682593856654, "no_speech_prob": 0.01911775767803192}, {"id": 76, "seek": 37592, "start": 398.32, "end": 403.68, "text": " differ from model-centric AI? And then we'll dive in a little deeper. So data-centric AI often", "tokens": [51484, 743, 490, 2316, 12, 45300, 7318, 30, 400, 550, 321, 603, 9192, 294, 257, 707, 7731, 13, 407, 1412, 12, 45300, 7318, 2049, 51752], "temperature": 0.0, "avg_logprob": -0.16293021794911977, "compression_ratio": 1.8600682593856654, "no_speech_prob": 0.01911775767803192}, {"id": 77, "seek": 40368, "start": 403.72, "end": 409.04, "text": " takes one of two forms. So one form is that you have AI algorithms that understand something", "tokens": [50366, 2516, 472, 295, 732, 6422, 13, 407, 472, 1254, 307, 300, 291, 362, 7318, 14642, 300, 1223, 746, 50632], "temperature": 0.0, "avg_logprob": -0.14760084911785296, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.017700936645269394}, {"id": 78, "seek": 40368, "start": 409.04, "end": 413.96, "text": " about data, and then they use this new information that they've understood to help a model train", "tokens": [50632, 466, 1412, 11, 293, 550, 436, 764, 341, 777, 1589, 300, 436, 600, 7320, 281, 854, 257, 2316, 3847, 50878], "temperature": 0.0, "avg_logprob": -0.14760084911785296, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.017700936645269394}, {"id": 79, "seek": 40368, "start": 413.96, "end": 418.88, "text": " better. So an example of this is curriculum learning. And this is more of sort of something", "tokens": [50878, 1101, 13, 407, 364, 1365, 295, 341, 307, 14302, 2539, 13, 400, 341, 307, 544, 295, 1333, 295, 746, 51124], "temperature": 0.0, "avg_logprob": -0.14760084911785296, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.017700936645269394}, {"id": 80, "seek": 40368, "start": 418.88, "end": 422.76, "text": " you'd encounter in grad school, so you may not have heard of it. But what curriculum,", "tokens": [51124, 291, 1116, 8593, 294, 2771, 1395, 11, 370, 291, 815, 406, 362, 2198, 295, 309, 13, 583, 437, 14302, 11, 51318], "temperature": 0.0, "avg_logprob": -0.14760084911785296, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.017700936645269394}, {"id": 81, "seek": 40368, "start": 422.76, "end": 429.76, "text": " it's, you can check out the paper by Yasha Benjo. But the idea is you use a, some algorithm", "tokens": [51318, 309, 311, 11, 291, 393, 1520, 484, 264, 3035, 538, 398, 12137, 3964, 5134, 13, 583, 264, 1558, 307, 291, 764, 257, 11, 512, 9284, 51668], "temperature": 0.0, "avg_logprob": -0.14760084911785296, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.017700936645269394}, {"id": 82, "seek": 42976, "start": 429.84, "end": 435.59999999999997, "text": " that looks at data and it identifies which data is probably easy to learn. And so the", "tokens": [50368, 300, 1542, 412, 1412, 293, 309, 34597, 597, 1412, 307, 1391, 1858, 281, 1466, 13, 400, 370, 264, 50656], "temperature": 0.0, "avg_logprob": -0.15068546094392476, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.004753508139401674}, {"id": 83, "seek": 42976, "start": 435.59999999999997, "end": 440.4, "text": " corollary here is imagine you're a student and you're in the classroom. All right. Should", "tokens": [50656, 1181, 1833, 822, 510, 307, 3811, 291, 434, 257, 3107, 293, 291, 434, 294, 264, 7419, 13, 1057, 558, 13, 6454, 50896], "temperature": 0.0, "avg_logprob": -0.15068546094392476, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.004753508139401674}, {"id": 84, "seek": 42976, "start": 440.4, "end": 443.8, "text": " the teacher, what should they teach you? Like, should they teach you really, really hard", "tokens": [50896, 264, 5027, 11, 437, 820, 436, 2924, 291, 30, 1743, 11, 820, 436, 2924, 291, 534, 11, 534, 1152, 51066], "temperature": 0.0, "avg_logprob": -0.15068546094392476, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.004753508139401674}, {"id": 85, "seek": 42976, "start": 443.8, "end": 448.24, "text": " examples as the very first examples? Like, if you're learning addition, should they start", "tokens": [51066, 5110, 382, 264, 588, 700, 5110, 30, 1743, 11, 498, 291, 434, 2539, 4500, 11, 820, 436, 722, 51288], "temperature": 0.0, "avg_logprob": -0.15068546094392476, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.004753508139401674}, {"id": 86, "seek": 42976, "start": 448.24, "end": 455.52, "text": " out with like 10,051 plus 1042 or would they start out with like one plus two? Right. Now", "tokens": [51288, 484, 365, 411, 1266, 11, 13328, 16, 1804, 1266, 15628, 420, 576, 436, 722, 484, 365, 411, 472, 1804, 732, 30, 1779, 13, 823, 51652], "temperature": 0.0, "avg_logprob": -0.15068546094392476, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.004753508139401674}, {"id": 87, "seek": 42976, "start": 455.52, "end": 458.96, "text": " we know this because, like, we've all learned addition. But when a machine learning model", "tokens": [51652, 321, 458, 341, 570, 11, 411, 11, 321, 600, 439, 3264, 4500, 13, 583, 562, 257, 3479, 2539, 2316, 51824], "temperature": 0.0, "avg_logprob": -0.15068546094392476, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.004753508139401674}, {"id": 88, "seek": 45896, "start": 459.0, "end": 462.56, "text": " starts, it's starting from scratch. So it doesn't know right from the beginning what", "tokens": [50366, 3719, 11, 309, 311, 2891, 490, 8459, 13, 407, 309, 1177, 380, 458, 558, 490, 264, 2863, 437, 50544], "temperature": 0.0, "avg_logprob": -0.12015713730903521, "compression_ratio": 1.8875379939209727, "no_speech_prob": 0.008844118565320969}, {"id": 89, "seek": 45896, "start": 462.56, "end": 467.58, "text": " is the sort of easiest example. And so there are data-centric AI approaches that actually", "tokens": [50544, 307, 264, 1333, 295, 12889, 1365, 13, 400, 370, 456, 366, 1412, 12, 45300, 7318, 11587, 300, 767, 50795], "temperature": 0.0, "avg_logprob": -0.12015713730903521, "compression_ratio": 1.8875379939209727, "no_speech_prob": 0.008844118565320969}, {"id": 90, "seek": 45896, "start": 467.58, "end": 472.44, "text": " estimate what is the easiest example. And then when you train an ML model, you start", "tokens": [50795, 12539, 437, 307, 264, 12889, 1365, 13, 400, 550, 562, 291, 3847, 364, 21601, 2316, 11, 291, 722, 51038], "temperature": 0.0, "avg_logprob": -0.12015713730903521, "compression_ratio": 1.8875379939209727, "no_speech_prob": 0.008844118565320969}, {"id": 91, "seek": 45896, "start": 472.44, "end": 476.0, "text": " with that example, and then you give it slightly harder ones and slightly harder them. And", "tokens": [51038, 365, 300, 1365, 11, 293, 550, 291, 976, 309, 4748, 6081, 2306, 293, 4748, 6081, 552, 13, 400, 51216], "temperature": 0.0, "avg_logprob": -0.12015713730903521, "compression_ratio": 1.8875379939209727, "no_speech_prob": 0.008844118565320969}, {"id": 92, "seek": 45896, "start": 476.0, "end": 480.29999999999995, "text": " so it's like curriculum learning because it's like a student's curriculum. And so that's", "tokens": [51216, 370, 309, 311, 411, 14302, 2539, 570, 309, 311, 411, 257, 3107, 311, 14302, 13, 400, 370, 300, 311, 51431], "temperature": 0.0, "avg_logprob": -0.12015713730903521, "compression_ratio": 1.8875379939209727, "no_speech_prob": 0.008844118565320969}, {"id": 93, "seek": 45896, "start": 480.29999999999995, "end": 484.71999999999997, "text": " one way that you can sort of use new information. You're still training on all the same data,", "tokens": [51431, 472, 636, 300, 291, 393, 1333, 295, 764, 777, 1589, 13, 509, 434, 920, 3097, 322, 439, 264, 912, 1412, 11, 51652], "temperature": 0.0, "avg_logprob": -0.12015713730903521, "compression_ratio": 1.8875379939209727, "no_speech_prob": 0.008844118565320969}, {"id": 94, "seek": 45896, "start": 484.71999999999997, "end": 487.59999999999997, "text": " but you're just reordering it and using this additional information. You don't actually", "tokens": [51652, 457, 291, 434, 445, 319, 765, 1794, 309, 293, 1228, 341, 4497, 1589, 13, 509, 500, 380, 767, 51796], "temperature": 0.0, "avg_logprob": -0.12015713730903521, "compression_ratio": 1.8875379939209727, "no_speech_prob": 0.008844118565320969}, {"id": 95, "seek": 48760, "start": 487.6, "end": 492.40000000000003, "text": " change the data set. Another sort of common form of data-centric AI is that you actually", "tokens": [50364, 1319, 264, 1412, 992, 13, 3996, 1333, 295, 2689, 1254, 295, 1412, 12, 45300, 7318, 307, 300, 291, 767, 50604], "temperature": 0.0, "avg_logprob": -0.1771357218424479, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.0005032132030464709}, {"id": 96, "seek": 48760, "start": 492.40000000000003, "end": 498.08000000000004, "text": " modify the data set to directly improve the performance of a model. So an example of this", "tokens": [50604, 16927, 264, 1412, 992, 281, 3838, 3470, 264, 3389, 295, 257, 2316, 13, 407, 364, 1365, 295, 341, 50888], "temperature": 0.0, "avg_logprob": -0.1771357218424479, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.0005032132030464709}, {"id": 97, "seek": 48760, "start": 498.08000000000004, "end": 502.84000000000003, "text": " might be something like Confant Learning. And this is less known than curriculum learning,", "tokens": [50888, 1062, 312, 746, 411, 11701, 394, 15205, 13, 400, 341, 307, 1570, 2570, 813, 14302, 2539, 11, 51126], "temperature": 0.0, "avg_logprob": -0.1771357218424479, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.0005032132030464709}, {"id": 98, "seek": 48760, "start": 502.84000000000003, "end": 507.76000000000005, "text": " something I worked on. And this idea here is that you want to find what are label errors", "tokens": [51126, 746, 286, 2732, 322, 13, 400, 341, 1558, 510, 307, 300, 291, 528, 281, 915, 437, 366, 7645, 13603, 51372], "temperature": 0.0, "avg_logprob": -0.1771357218424479, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.0005032132030464709}, {"id": 99, "seek": 48760, "start": 507.76000000000005, "end": 512.84, "text": " in a data set and then remove them prior to training, so that you just train on like correctly", "tokens": [51372, 294, 257, 1412, 992, 293, 550, 4159, 552, 4059, 281, 3097, 11, 370, 300, 291, 445, 3847, 322, 411, 8944, 51626], "temperature": 0.0, "avg_logprob": -0.1771357218424479, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.0005032132030464709}, {"id": 100, "seek": 51284, "start": 512.84, "end": 518.2, "text": " labeled stuff. And the corollary here in the sense of the student is that if your teacher", "tokens": [50364, 21335, 1507, 13, 400, 264, 1181, 1833, 822, 510, 294, 264, 2020, 295, 264, 3107, 307, 300, 498, 428, 5027, 50632], "temperature": 0.0, "avg_logprob": -0.12619280158926588, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.34826987981796265}, {"id": 101, "seek": 51284, "start": 518.2, "end": 523.1600000000001, "text": " is basically making mistakes 30% of the time, like imagine as I was teaching you, which", "tokens": [50632, 307, 1936, 1455, 8038, 2217, 4, 295, 264, 565, 11, 411, 3811, 382, 286, 390, 4571, 291, 11, 597, 50880], "temperature": 0.0, "avg_logprob": -0.12619280158926588, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.34826987981796265}, {"id": 102, "seek": 51284, "start": 523.1600000000001, "end": 528.24, "text": " hopefully I don't do today, that 30% of the time I told you wrong things. And then you", "tokens": [50880, 4696, 286, 500, 380, 360, 965, 11, 300, 2217, 4, 295, 264, 565, 286, 1907, 291, 2085, 721, 13, 400, 550, 291, 51134], "temperature": 0.0, "avg_logprob": -0.12619280158926588, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.34826987981796265}, {"id": 103, "seek": 51284, "start": 528.24, "end": 532.76, "text": " compare that to another teacher who comes and they tell you right things 100% of the", "tokens": [51134, 6794, 300, 281, 1071, 5027, 567, 1487, 293, 436, 980, 291, 558, 721, 2319, 4, 295, 264, 51360], "temperature": 0.0, "avg_logprob": -0.12619280158926588, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.34826987981796265}, {"id": 104, "seek": 51284, "start": 532.76, "end": 538.24, "text": " time. Then which teacher do you think you'll probably learn data-centric AI better from?", "tokens": [51360, 565, 13, 1396, 597, 5027, 360, 291, 519, 291, 603, 1391, 1466, 1412, 12, 45300, 7318, 1101, 490, 30, 51634], "temperature": 0.0, "avg_logprob": -0.12619280158926588, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.34826987981796265}, {"id": 105, "seek": 53824, "start": 538.24, "end": 543.28, "text": " And so the idea is that we want to take this bad sort of, you know, this 30% of wrong things", "tokens": [50364, 400, 370, 264, 1558, 307, 300, 321, 528, 281, 747, 341, 1578, 1333, 295, 11, 291, 458, 11, 341, 2217, 4, 295, 2085, 721, 50616], "temperature": 0.0, "avg_logprob": -0.10621716790165461, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.006902281194925308}, {"id": 106, "seek": 53824, "start": 543.28, "end": 546.48, "text": " and we want to get rid of them so that you could sort of come to the classroom and redo", "tokens": [50616, 293, 321, 528, 281, 483, 3973, 295, 552, 370, 300, 291, 727, 1333, 295, 808, 281, 264, 7419, 293, 29956, 50776], "temperature": 0.0, "avg_logprob": -0.10621716790165461, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.006902281194925308}, {"id": 107, "seek": 53824, "start": 546.48, "end": 550.92, "text": " your learning experiences if those never happened. And that's the idea here. So you just learn", "tokens": [50776, 428, 2539, 5235, 498, 729, 1128, 2011, 13, 400, 300, 311, 264, 1558, 510, 13, 407, 291, 445, 1466, 50998], "temperature": 0.0, "avg_logprob": -0.10621716790165461, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.006902281194925308}, {"id": 108, "seek": 53824, "start": 550.92, "end": 557.08, "text": " on the good stuff. So those are two approaches. And then what we'll think about now is sort", "tokens": [50998, 322, 264, 665, 1507, 13, 407, 729, 366, 732, 11587, 13, 400, 550, 437, 321, 603, 519, 466, 586, 307, 1333, 51306], "temperature": 0.0, "avg_logprob": -0.10621716790165461, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.006902281194925308}, {"id": 109, "seek": 53824, "start": 557.08, "end": 563.64, "text": " of what is the difference between model-centric AI and data-centric AI? So the sort of normal", "tokens": [51306, 295, 437, 307, 264, 2649, 1296, 2316, 12, 45300, 7318, 293, 1412, 12, 45300, 7318, 30, 407, 264, 1333, 295, 2710, 51634], "temperature": 0.0, "avg_logprob": -0.10621716790165461, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.006902281194925308}, {"id": 110, "seek": 53824, "start": 563.64, "end": 568.12, "text": " way you hear this is that given a data set, you try to produce the best model. And that's", "tokens": [51634, 636, 291, 1568, 341, 307, 300, 2212, 257, 1412, 992, 11, 291, 853, 281, 5258, 264, 1151, 2316, 13, 400, 300, 311, 51858], "temperature": 0.0, "avg_logprob": -0.10621716790165461, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.006902281194925308}, {"id": 111, "seek": 56812, "start": 568.16, "end": 573.16, "text": " like the classical way of thinking about model-centric AI. And the idea is that you want to change", "tokens": [50366, 411, 264, 13735, 636, 295, 1953, 466, 2316, 12, 45300, 7318, 13, 400, 264, 1558, 307, 300, 291, 528, 281, 1319, 50616], "temperature": 0.0, "avg_logprob": -0.14390990313361673, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.002887967973947525}, {"id": 112, "seek": 56812, "start": 573.16, "end": 579.36, "text": " the model somehow to improve, you know, some performance on AI task. And is there anyone", "tokens": [50616, 264, 2316, 6063, 281, 3470, 11, 291, 458, 11, 512, 3389, 322, 7318, 5633, 13, 400, 307, 456, 2878, 50926], "temperature": 0.0, "avg_logprob": -0.14390990313361673, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.002887967973947525}, {"id": 113, "seek": 56812, "start": 579.36, "end": 584.0, "text": " here just out of curiosity who's not familiar with like AI tasks and some of the stuff we", "tokens": [50926, 510, 445, 484, 295, 18769, 567, 311, 406, 4963, 365, 411, 7318, 9608, 293, 512, 295, 264, 1507, 321, 51158], "temperature": 0.0, "avg_logprob": -0.14390990313361673, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.002887967973947525}, {"id": 114, "seek": 56812, "start": 584.0, "end": 588.68, "text": " do in AI? Just want to make sure that we're on the same board, like classifying things.", "tokens": [51158, 360, 294, 7318, 30, 1449, 528, 281, 652, 988, 300, 321, 434, 322, 264, 912, 3150, 11, 411, 1508, 5489, 721, 13, 51392], "temperature": 0.0, "avg_logprob": -0.14390990313361673, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.002887967973947525}, {"id": 115, "seek": 56812, "start": 588.68, "end": 592.72, "text": " Okay, sweet. All right. So you've got some AI tasks and you're trying to classify something", "tokens": [51392, 1033, 11, 3844, 13, 1057, 558, 13, 407, 291, 600, 658, 512, 7318, 9608, 293, 291, 434, 1382, 281, 33872, 746, 51594], "temperature": 0.0, "avg_logprob": -0.14390990313361673, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.002887967973947525}, {"id": 116, "seek": 56812, "start": 592.72, "end": 595.6800000000001, "text": " and the goal is you want to improve the performance on that. So usually it'll change the model", "tokens": [51594, 293, 264, 3387, 307, 291, 528, 281, 3470, 264, 3389, 322, 300, 13, 407, 2673, 309, 603, 1319, 264, 2316, 51742], "temperature": 0.0, "avg_logprob": -0.14390990313361673, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.002887967973947525}, {"id": 117, "seek": 59568, "start": 595.76, "end": 602.76, "text": " to do that. And data-centric AI, instead, it's given some model, right, that may be fixed", "tokens": [50368, 281, 360, 300, 13, 400, 1412, 12, 45300, 7318, 11, 2602, 11, 309, 311, 2212, 512, 2316, 11, 558, 11, 300, 815, 312, 6806, 50718], "temperature": 0.0, "avg_logprob": -0.12170076370239258, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.0019264169968664646}, {"id": 118, "seek": 59568, "start": 602.76, "end": 606.68, "text": " or you may change, but let's just assume it's fixed for now. You want to improve that model", "tokens": [50718, 420, 291, 815, 1319, 11, 457, 718, 311, 445, 6552, 309, 311, 6806, 337, 586, 13, 509, 528, 281, 3470, 300, 2316, 50914], "temperature": 0.0, "avg_logprob": -0.12170076370239258, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.0019264169968664646}, {"id": 119, "seek": 59568, "start": 606.68, "end": 611.0, "text": " by improving your data set. So this is like the common way to think about the two. And", "tokens": [50914, 538, 11470, 428, 1412, 992, 13, 407, 341, 307, 411, 264, 2689, 636, 281, 519, 466, 264, 732, 13, 400, 51130], "temperature": 0.0, "avg_logprob": -0.12170076370239258, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.0019264169968664646}, {"id": 120, "seek": 59568, "start": 611.0, "end": 615.4, "text": " the idea is to systematically or algorithmically not have a bunch of humans changing the data", "tokens": [51130, 264, 1558, 307, 281, 39531, 420, 9284, 984, 406, 362, 257, 3840, 295, 6255, 4473, 264, 1412, 51350], "temperature": 0.0, "avg_logprob": -0.12170076370239258, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.0019264169968664646}, {"id": 121, "seek": 59568, "start": 615.4, "end": 620.3199999999999, "text": " set, but like some algorithmic way that you can do this. Okay. All right. So our goal is", "tokens": [51350, 992, 11, 457, 411, 512, 9284, 299, 636, 300, 291, 393, 360, 341, 13, 1033, 13, 1057, 558, 13, 407, 527, 3387, 307, 51596], "temperature": 0.0, "avg_logprob": -0.12170076370239258, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.0019264169968664646}, {"id": 122, "seek": 59568, "start": 620.3199999999999, "end": 625.3599999999999, "text": " to start thinking about ML in terms of data and not the model. And so we'll just start", "tokens": [51596, 281, 722, 1953, 466, 21601, 294, 2115, 295, 1412, 293, 406, 264, 2316, 13, 400, 370, 321, 603, 445, 722, 51848], "temperature": 0.0, "avg_logprob": -0.12170076370239258, "compression_ratio": 1.7639344262295082, "no_speech_prob": 0.0019264169968664646}, {"id": 123, "seek": 62536, "start": 625.36, "end": 629.88, "text": " out with a simple example. And this is not data-centric AI, but it'll get us thinking", "tokens": [50364, 484, 365, 257, 2199, 1365, 13, 400, 341, 307, 406, 1412, 12, 45300, 7318, 11, 457, 309, 603, 483, 505, 1953, 50590], "temperature": 0.0, "avg_logprob": -0.14004002226159928, "compression_ratio": 1.4957264957264957, "no_speech_prob": 0.00043044317862950265}, {"id": 124, "seek": 62536, "start": 629.88, "end": 635.48, "text": " about how we can think about AI in terms of data. So just quick show of hands. Who here", "tokens": [50590, 466, 577, 321, 393, 519, 466, 7318, 294, 2115, 295, 1412, 13, 407, 445, 1702, 855, 295, 2377, 13, 2102, 510, 50870], "temperature": 0.0, "avg_logprob": -0.14004002226159928, "compression_ratio": 1.4957264957264957, "no_speech_prob": 0.00043044317862950265}, {"id": 125, "seek": 62536, "start": 635.48, "end": 642.12, "text": " is familiar with K nearest neighbors? Okay. Great. All right. So I won't belabor this", "tokens": [50870, 307, 4963, 365, 591, 23831, 12512, 30, 1033, 13, 3769, 13, 1057, 558, 13, 407, 286, 1582, 380, 989, 3816, 341, 51202], "temperature": 0.0, "avg_logprob": -0.14004002226159928, "compression_ratio": 1.4957264957264957, "no_speech_prob": 0.00043044317862950265}, {"id": 126, "seek": 62536, "start": 642.12, "end": 654.4, "text": " point then. So do some chalk. That was a good thing to check beforehand. We were here last", "tokens": [51202, 935, 550, 13, 407, 360, 512, 28660, 13, 663, 390, 257, 665, 551, 281, 1520, 22893, 13, 492, 645, 510, 1036, 51816], "temperature": 0.0, "avg_logprob": -0.14004002226159928, "compression_ratio": 1.4957264957264957, "no_speech_prob": 0.00043044317862950265}, {"id": 127, "seek": 65440, "start": 654.4, "end": 666.0, "text": " night, but the chalk has been removed. All right. So yeah, the key idea with K nearest", "tokens": [50364, 1818, 11, 457, 264, 28660, 575, 668, 7261, 13, 1057, 558, 13, 407, 1338, 11, 264, 2141, 1558, 365, 591, 23831, 50944], "temperature": 0.0, "avg_logprob": -0.1534554854683254, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.053367841988801956}, {"id": 128, "seek": 65440, "start": 666.0, "end": 671.4, "text": " neighbors and the key idea to think about in the context of data-centric learning is", "tokens": [50944, 12512, 293, 264, 2141, 1558, 281, 519, 466, 294, 264, 4319, 295, 1412, 12, 45300, 2539, 307, 51214], "temperature": 0.0, "avg_logprob": -0.1534554854683254, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.053367841988801956}, {"id": 129, "seek": 65440, "start": 671.4, "end": 678.88, "text": " that K nearest neighbors, imagine you have, you know, a space, a 2D space, which, you", "tokens": [51214, 300, 591, 23831, 12512, 11, 3811, 291, 362, 11, 291, 458, 11, 257, 1901, 11, 257, 568, 35, 1901, 11, 597, 11, 291, 51588], "temperature": 0.0, "avg_logprob": -0.1534554854683254, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.053367841988801956}, {"id": 130, "seek": 67888, "start": 678.88, "end": 695.16, "text": " know what, here's what we'll do. All right. So you've got some, all right, sweet. So you've", "tokens": [50364, 458, 437, 11, 510, 311, 437, 321, 603, 360, 13, 1057, 558, 13, 407, 291, 600, 658, 512, 11, 439, 558, 11, 3844, 13, 407, 291, 600, 51178], "temperature": 0.0, "avg_logprob": -0.17660820789826223, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.12935861945152283}, {"id": 131, "seek": 67888, "start": 695.16, "end": 701.96, "text": " got, say, you know, some data, you've got some triangles, and you've got some circles.", "tokens": [51178, 658, 11, 584, 11, 291, 458, 11, 512, 1412, 11, 291, 600, 658, 512, 29896, 11, 293, 291, 600, 658, 512, 13040, 13, 51518], "temperature": 0.0, "avg_logprob": -0.17660820789826223, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.12935861945152283}, {"id": 132, "seek": 67888, "start": 701.96, "end": 707.72, "text": " You'll have to apologize for my drawing and you've got some squares. And so this is your", "tokens": [51518, 509, 603, 362, 281, 12328, 337, 452, 6316, 293, 291, 600, 658, 512, 19368, 13, 400, 370, 341, 307, 428, 51806], "temperature": 0.0, "avg_logprob": -0.17660820789826223, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.12935861945152283}, {"id": 133, "seek": 70772, "start": 707.72, "end": 711.44, "text": " data set. And let's say, you know, these are different types of images that you've put in", "tokens": [50364, 1412, 992, 13, 400, 718, 311, 584, 11, 291, 458, 11, 613, 366, 819, 3467, 295, 5267, 300, 291, 600, 829, 294, 50550], "temperature": 0.0, "avg_logprob": -0.13062087803670805, "compression_ratio": 1.795221843003413, "no_speech_prob": 0.13649189472198486}, {"id": 134, "seek": 70772, "start": 711.44, "end": 715.48, "text": " some 2D space somehow where it's text that you've mapped a 2D space. And then the idea", "tokens": [50550, 512, 568, 35, 1901, 6063, 689, 309, 311, 2487, 300, 291, 600, 33318, 257, 568, 35, 1901, 13, 400, 550, 264, 1558, 50752], "temperature": 0.0, "avg_logprob": -0.13062087803670805, "compression_ratio": 1.795221843003413, "no_speech_prob": 0.13649189472198486}, {"id": 135, "seek": 70772, "start": 715.48, "end": 720.08, "text": " of K nearest neighbors, as many of you seem to know and are familiar with, is that you", "tokens": [50752, 295, 591, 23831, 12512, 11, 382, 867, 295, 291, 1643, 281, 458, 293, 366, 4963, 365, 11, 307, 300, 291, 50982], "temperature": 0.0, "avg_logprob": -0.13062087803670805, "compression_ratio": 1.795221843003413, "no_speech_prob": 0.13649189472198486}, {"id": 136, "seek": 70772, "start": 720.08, "end": 724.6, "text": " would have some new point, say, here. And with this, it's some weird thing. We don't know", "tokens": [50982, 576, 362, 512, 777, 935, 11, 584, 11, 510, 13, 400, 365, 341, 11, 309, 311, 512, 3657, 551, 13, 492, 500, 380, 458, 51208], "temperature": 0.0, "avg_logprob": -0.13062087803670805, "compression_ratio": 1.795221843003413, "no_speech_prob": 0.13649189472198486}, {"id": 137, "seek": 70772, "start": 724.6, "end": 728.12, "text": " what it is. And we're trying to figure out, is it a triangle? Is it a circle? Is it a", "tokens": [51208, 437, 309, 307, 13, 400, 321, 434, 1382, 281, 2573, 484, 11, 307, 309, 257, 13369, 30, 1119, 309, 257, 6329, 30, 1119, 309, 257, 51384], "temperature": 0.0, "avg_logprob": -0.13062087803670805, "compression_ratio": 1.795221843003413, "no_speech_prob": 0.13649189472198486}, {"id": 138, "seek": 70772, "start": 728.12, "end": 732.9200000000001, "text": " square? And so if this was sort of three K nearest neighbors, like then you would find", "tokens": [51384, 3732, 30, 400, 370, 498, 341, 390, 1333, 295, 1045, 591, 23831, 12512, 11, 411, 550, 291, 576, 915, 51624], "temperature": 0.0, "avg_logprob": -0.13062087803670805, "compression_ratio": 1.795221843003413, "no_speech_prob": 0.13649189472198486}, {"id": 139, "seek": 73292, "start": 732.92, "end": 739.76, "text": " the three nearest neighbors. So say this one, this one, and this one. And then can somebody", "tokens": [50364, 264, 1045, 23831, 12512, 13, 407, 584, 341, 472, 11, 341, 472, 11, 293, 341, 472, 13, 400, 550, 393, 2618, 50706], "temperature": 0.0, "avg_logprob": -0.12830252499924494, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.05831485986709595}, {"id": 140, "seek": 73292, "start": 739.76, "end": 748.88, "text": " tell me, like, what the class would be and why? Yeah, it'd be a square. That doesn't", "tokens": [50706, 980, 385, 11, 411, 11, 437, 264, 1508, 576, 312, 293, 983, 30, 865, 11, 309, 1116, 312, 257, 3732, 13, 663, 1177, 380, 51162], "temperature": 0.0, "avg_logprob": -0.12830252499924494, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.05831485986709595}, {"id": 141, "seek": 73292, "start": 748.88, "end": 755.68, "text": " mean that it's not a cool point. And you're familiar with being a square. So this would", "tokens": [51162, 914, 300, 309, 311, 406, 257, 1627, 935, 13, 400, 291, 434, 4963, 365, 885, 257, 3732, 13, 407, 341, 576, 51502], "temperature": 0.0, "avg_logprob": -0.12830252499924494, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.05831485986709595}, {"id": 142, "seek": 73292, "start": 755.68, "end": 759.36, "text": " be labeled a square, and that's based on majority voting. And there's a lot of different ways", "tokens": [51502, 312, 21335, 257, 3732, 11, 293, 300, 311, 2361, 322, 6286, 10419, 13, 400, 456, 311, 257, 688, 295, 819, 2098, 51686], "temperature": 0.0, "avg_logprob": -0.12830252499924494, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.05831485986709595}, {"id": 143, "seek": 75936, "start": 759.36, "end": 765.64, "text": " to do algorithms for deciding what the label should be. If it was, say, five neighbors,", "tokens": [50364, 281, 360, 14642, 337, 17990, 437, 264, 7645, 820, 312, 13, 759, 309, 390, 11, 584, 11, 1732, 12512, 11, 50678], "temperature": 0.0, "avg_logprob": -0.15117654962054752, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.12928177416324615}, {"id": 144, "seek": 75936, "start": 765.64, "end": 770.04, "text": " meaning five in N, where K is five, so it's K in N, then you would choose the five nearest", "tokens": [50678, 3620, 1732, 294, 426, 11, 689, 591, 307, 1732, 11, 370, 309, 311, 591, 294, 426, 11, 550, 291, 576, 2826, 264, 1732, 23831, 50898], "temperature": 0.0, "avg_logprob": -0.15117654962054752, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.12928177416324615}, {"id": 145, "seek": 75936, "start": 770.04, "end": 773.92, "text": " neighbors and you would do a majority vote. Okay. The key idea here is that there is no", "tokens": [50898, 12512, 293, 291, 576, 360, 257, 6286, 4740, 13, 1033, 13, 440, 2141, 1558, 510, 307, 300, 456, 307, 572, 51092], "temperature": 0.0, "avg_logprob": -0.15117654962054752, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.12928177416324615}, {"id": 146, "seek": 75936, "start": 773.92, "end": 779.76, "text": " loss function. Literally, any time you have a new point, so say I just have another new", "tokens": [51092, 4470, 2445, 13, 23768, 11, 604, 565, 291, 362, 257, 777, 935, 11, 370, 584, 286, 445, 362, 1071, 777, 51384], "temperature": 0.0, "avg_logprob": -0.15117654962054752, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.12928177416324615}, {"id": 147, "seek": 75936, "start": 779.76, "end": 786.2, "text": " point here, I'm not sort of, there's no algorithm that I'm passing this into beyond just measuring", "tokens": [51384, 935, 510, 11, 286, 478, 406, 1333, 295, 11, 456, 311, 572, 9284, 300, 286, 478, 8437, 341, 666, 4399, 445, 13389, 51706], "temperature": 0.0, "avg_logprob": -0.15117654962054752, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.12928177416324615}, {"id": 148, "seek": 78620, "start": 786.2, "end": 790.44, "text": " a distance, some notion of distance between this and the nearest points. And you can do", "tokens": [50364, 257, 4560, 11, 512, 10710, 295, 4560, 1296, 341, 293, 264, 23831, 2793, 13, 400, 291, 393, 360, 50576], "temperature": 0.0, "avg_logprob": -0.1241455660521529, "compression_ratio": 1.8294314381270902, "no_speech_prob": 0.043341074138879776}, {"id": 149, "seek": 78620, "start": 790.44, "end": 793.72, "text": " a bunch of pre-computation. There's a lot of smart work actually done in K nearest neighbors", "tokens": [50576, 257, 3840, 295, 659, 12, 1112, 2582, 399, 13, 821, 311, 257, 688, 295, 4069, 589, 767, 1096, 294, 591, 23831, 12512, 50740], "temperature": 0.0, "avg_logprob": -0.1241455660521529, "compression_ratio": 1.8294314381270902, "no_speech_prob": 0.043341074138879776}, {"id": 150, "seek": 78620, "start": 793.72, "end": 797.8000000000001, "text": " that is pretty impressive and you can do embeddings for all of these and pre-compute distances", "tokens": [50740, 300, 307, 1238, 8992, 293, 291, 393, 360, 12240, 29432, 337, 439, 295, 613, 293, 659, 12, 21541, 1169, 22182, 50944], "temperature": 0.0, "avg_logprob": -0.1241455660521529, "compression_ratio": 1.8294314381270902, "no_speech_prob": 0.043341074138879776}, {"id": 151, "seek": 78620, "start": 797.8000000000001, "end": 802.0, "text": " and you can do all sorts of fancy stuff. But ultimately, all this decision is based on", "tokens": [50944, 293, 291, 393, 360, 439, 7527, 295, 10247, 1507, 13, 583, 6284, 11, 439, 341, 3537, 307, 2361, 322, 51154], "temperature": 0.0, "avg_logprob": -0.1241455660521529, "compression_ratio": 1.8294314381270902, "no_speech_prob": 0.043341074138879776}, {"id": 152, "seek": 78620, "start": 802.0, "end": 808.1600000000001, "text": " is the data. And so I wanted to motivate this problem and this way of thinking because this", "tokens": [51154, 307, 264, 1412, 13, 400, 370, 286, 1415, 281, 28497, 341, 1154, 293, 341, 636, 295, 1953, 570, 341, 51462], "temperature": 0.0, "avg_logprob": -0.1241455660521529, "compression_ratio": 1.8294314381270902, "no_speech_prob": 0.043341074138879776}, {"id": 153, "seek": 78620, "start": 808.1600000000001, "end": 813.88, "text": " whole decision process is made just based on data. And the quality of the data is as related", "tokens": [51462, 1379, 3537, 1399, 307, 1027, 445, 2361, 322, 1412, 13, 400, 264, 3125, 295, 264, 1412, 307, 382, 4077, 51748], "temperature": 0.0, "avg_logprob": -0.1241455660521529, "compression_ratio": 1.8294314381270902, "no_speech_prob": 0.043341074138879776}, {"id": 154, "seek": 81388, "start": 813.88, "end": 818.84, "text": " as possible to the quality of the prediction. So if you have really good data, you're going", "tokens": [50364, 382, 1944, 281, 264, 3125, 295, 264, 17630, 13, 407, 498, 291, 362, 534, 665, 1412, 11, 291, 434, 516, 50612], "temperature": 0.0, "avg_logprob": -0.125560847195712, "compression_ratio": 1.8869257950530036, "no_speech_prob": 0.0041978927329182625}, {"id": 155, "seek": 81388, "start": 818.84, "end": 822.16, "text": " to have really good predictions. And if you've got a bunch of errors in here, you're going", "tokens": [50612, 281, 362, 534, 665, 21264, 13, 400, 498, 291, 600, 658, 257, 3840, 295, 13603, 294, 510, 11, 291, 434, 516, 50778], "temperature": 0.0, "avg_logprob": -0.125560847195712, "compression_ratio": 1.8869257950530036, "no_speech_prob": 0.0041978927329182625}, {"id": 156, "seek": 81388, "start": 822.16, "end": 827.2, "text": " to get the wrong prediction. And so this makes it really clear why fixing the data will make", "tokens": [50778, 281, 483, 264, 2085, 17630, 13, 400, 370, 341, 1669, 309, 534, 1850, 983, 19442, 264, 1412, 486, 652, 51030], "temperature": 0.0, "avg_logprob": -0.125560847195712, "compression_ratio": 1.8869257950530036, "no_speech_prob": 0.0041978927329182625}, {"id": 157, "seek": 81388, "start": 827.2, "end": 831.72, "text": " it, will improve your model. So is that kind of clear how this is motivating? Now this", "tokens": [51030, 309, 11, 486, 3470, 428, 2316, 13, 407, 307, 300, 733, 295, 1850, 577, 341, 307, 41066, 30, 823, 341, 51256], "temperature": 0.0, "avg_logprob": -0.125560847195712, "compression_ratio": 1.8869257950530036, "no_speech_prob": 0.0041978927329182625}, {"id": 158, "seek": 81388, "start": 831.72, "end": 836.28, "text": " is not a data-centric AI algorithm. And by the end of this lecture, you'll definitely", "tokens": [51256, 307, 406, 257, 1412, 12, 45300, 7318, 9284, 13, 400, 538, 264, 917, 295, 341, 7991, 11, 291, 603, 2138, 51484], "temperature": 0.0, "avg_logprob": -0.125560847195712, "compression_ratio": 1.8869257950530036, "no_speech_prob": 0.0041978927329182625}, {"id": 159, "seek": 81388, "start": 836.28, "end": 840.8, "text": " know what a data-centric AI algorithm is. But can someone tell me what the difference", "tokens": [51484, 458, 437, 257, 1412, 12, 45300, 7318, 9284, 307, 13, 583, 393, 1580, 980, 385, 437, 264, 2649, 51710], "temperature": 0.0, "avg_logprob": -0.125560847195712, "compression_ratio": 1.8869257950530036, "no_speech_prob": 0.0041978927329182625}, {"id": 160, "seek": 84080, "start": 840.8, "end": 870.4799999999999, "text": " is between K and N and a data-centric AI algorithm? Yeah, yeah, totally. That's a great", "tokens": [50364, 307, 1296, 591, 293, 426, 293, 257, 1412, 12, 45300, 7318, 9284, 30, 865, 11, 1338, 11, 3879, 13, 663, 311, 257, 869, 51848], "temperature": 0.0, "avg_logprob": -0.45925426483154297, "compression_ratio": 1.0235294117647058, "no_speech_prob": 0.1730811595916748}, {"id": 161, "seek": 87048, "start": 871.48, "end": 877.24, "text": " answer. K and N is just doing classification. And it's not actually modifying the data set.", "tokens": [50414, 1867, 13, 591, 293, 426, 307, 445, 884, 21538, 13, 400, 309, 311, 406, 767, 42626, 264, 1412, 992, 13, 50702], "temperature": 0.0, "avg_logprob": -0.18253964185714722, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.5031234622001648}, {"id": 162, "seek": 87048, "start": 877.24, "end": 881.28, "text": " So yeah, that's exactly right. Alright, so what are a few other examples of what is not", "tokens": [50702, 407, 1338, 11, 300, 311, 2293, 558, 13, 2798, 11, 370, 437, 366, 257, 1326, 661, 5110, 295, 437, 307, 406, 50904], "temperature": 0.0, "avg_logprob": -0.18253964185714722, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.5031234622001648}, {"id": 163, "seek": 87048, "start": 881.28, "end": 887.8000000000001, "text": " data-centric AI? Handpicking a bunch of points you think will improve a model. So can anybody", "tokens": [50904, 1412, 12, 45300, 7318, 30, 8854, 79, 10401, 257, 3840, 295, 2793, 291, 519, 486, 3470, 257, 2316, 13, 407, 393, 4472, 51230], "temperature": 0.0, "avg_logprob": -0.18253964185714722, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.5031234622001648}, {"id": 164, "seek": 87048, "start": 887.8000000000001, "end": 895.9200000000001, "text": " help me understand why this is not data-centric AI? Yeah. Yeah, totally. It's just done by", "tokens": [51230, 854, 385, 1223, 983, 341, 307, 406, 1412, 12, 45300, 7318, 30, 865, 13, 865, 11, 3879, 13, 467, 311, 445, 1096, 538, 51636], "temperature": 0.0, "avg_logprob": -0.18253964185714722, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.5031234622001648}, {"id": 165, "seek": 89592, "start": 895.92, "end": 900.16, "text": " hand. Like this could, if you had a hundred million, a data set of a hundred million points,", "tokens": [50364, 1011, 13, 1743, 341, 727, 11, 498, 291, 632, 257, 3262, 2459, 11, 257, 1412, 992, 295, 257, 3262, 2459, 2793, 11, 50576], "temperature": 0.0, "avg_logprob": -0.1660469014157531, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.0396256260573864}, {"id": 166, "seek": 89592, "start": 900.16, "end": 905.56, "text": " this would take a long time. What about doubling the size of your data set so that you can", "tokens": [50576, 341, 576, 747, 257, 938, 565, 13, 708, 466, 33651, 264, 2744, 295, 428, 1412, 992, 370, 300, 291, 393, 50846], "temperature": 0.0, "avg_logprob": -0.1660469014157531, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.0396256260573864}, {"id": 167, "seek": 89592, "start": 905.56, "end": 920.48, "text": " train an improved model? Yeah. Yeah, totally. This is just classical machine learning. It's", "tokens": [50846, 3847, 364, 9689, 2316, 30, 865, 13, 865, 11, 3879, 13, 639, 307, 445, 13735, 3479, 2539, 13, 467, 311, 51592], "temperature": 0.0, "avg_logprob": -0.1660469014157531, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.0396256260573864}, {"id": 168, "seek": 89592, "start": 920.48, "end": 923.68, "text": " still all the model, all the work you do as a model, but you're just paying more money", "tokens": [51592, 920, 439, 264, 2316, 11, 439, 264, 589, 291, 360, 382, 257, 2316, 11, 457, 291, 434, 445, 6229, 544, 1460, 51752], "temperature": 0.0, "avg_logprob": -0.1660469014157531, "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.0396256260573864}, {"id": 169, "seek": 92368, "start": 923.7199999999999, "end": 928.68, "text": " for more data. So let's juxtapose this. So what would be the data-centric AI versions", "tokens": [50366, 337, 544, 1412, 13, 407, 718, 311, 3649, 734, 569, 541, 341, 13, 407, 437, 576, 312, 264, 1412, 12, 45300, 7318, 9606, 50614], "temperature": 0.0, "avg_logprob": -0.13605580458769928, "compression_ratio": 1.5465116279069768, "no_speech_prob": 0.05831253156065941}, {"id": 170, "seek": 92368, "start": 928.68, "end": 935.52, "text": " of this? And just out of curiosity, does anybody know for the first one what would be sort", "tokens": [50614, 295, 341, 30, 400, 445, 484, 295, 18769, 11, 775, 4472, 458, 337, 264, 700, 472, 437, 576, 312, 1333, 50956], "temperature": 0.0, "avg_logprob": -0.13605580458769928, "compression_ratio": 1.5465116279069768, "no_speech_prob": 0.05831253156065941}, {"id": 171, "seek": 92368, "start": 935.52, "end": 949.04, "text": " of the data-centric AI corollary? Yeah, totally. And there's a whole field of research on", "tokens": [50956, 295, 264, 1412, 12, 45300, 7318, 1181, 1833, 822, 30, 865, 11, 3879, 13, 400, 456, 311, 257, 1379, 2519, 295, 2132, 322, 51632], "temperature": 0.0, "avg_logprob": -0.13605580458769928, "compression_ratio": 1.5465116279069768, "no_speech_prob": 0.05831253156065941}, {"id": 172, "seek": 94904, "start": 949.04, "end": 954.48, "text": " this and a whole subsection of ML that's called corset selection, where you have a data set", "tokens": [50364, 341, 293, 257, 1379, 1422, 11963, 295, 21601, 300, 311, 1219, 46511, 302, 9450, 11, 689, 291, 362, 257, 1412, 992, 50636], "temperature": 0.0, "avg_logprob": -0.19861185647607818, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.522842526435852}, {"id": 173, "seek": 94904, "start": 954.48, "end": 959.0, "text": " and let's say that you train a model on that data set and you get like 98% accuracy. But", "tokens": [50636, 293, 718, 311, 584, 300, 291, 3847, 257, 2316, 322, 300, 1412, 992, 293, 291, 483, 411, 20860, 4, 14170, 13, 583, 50862], "temperature": 0.0, "avg_logprob": -0.19861185647607818, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.522842526435852}, {"id": 174, "seek": 94904, "start": 959.0, "end": 963.5999999999999, "text": " the data set is like a hundred billion points. And so the goal is, can I find like, you know,", "tokens": [50862, 264, 1412, 992, 307, 411, 257, 3262, 5218, 2793, 13, 400, 370, 264, 3387, 307, 11, 393, 286, 915, 411, 11, 291, 458, 11, 51092], "temperature": 0.0, "avg_logprob": -0.19861185647607818, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.522842526435852}, {"id": 175, "seek": 94904, "start": 963.5999999999999, "end": 968.68, "text": " a million points that if I just train on those, I can get like pretty close, like 97% accuracy.", "tokens": [51092, 257, 2459, 2793, 300, 498, 286, 445, 3847, 322, 729, 11, 286, 393, 483, 411, 1238, 1998, 11, 411, 23399, 4, 14170, 13, 51346], "temperature": 0.0, "avg_logprob": -0.19861185647607818, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.522842526435852}, {"id": 176, "seek": 94904, "start": 968.68, "end": 974.16, "text": " And that's corset selection. What about for number two? What would be like a data-centric", "tokens": [51346, 400, 300, 311, 46511, 302, 9450, 13, 708, 466, 337, 1230, 732, 30, 708, 576, 312, 411, 257, 1412, 12, 45300, 51620], "temperature": 0.0, "avg_logprob": -0.19861185647607818, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.522842526435852}, {"id": 177, "seek": 97416, "start": 974.16, "end": 985.64, "text": " AI corollary? Yeah, yeah. So the idea is, and how does anybody know of any ways that", "tokens": [50364, 7318, 1181, 1833, 822, 30, 865, 11, 1338, 13, 407, 264, 1558, 307, 11, 293, 577, 775, 4472, 458, 295, 604, 2098, 300, 50938], "temperature": 0.0, "avg_logprob": -0.22854530334472656, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.006002116482704878}, {"id": 178, "seek": 97416, "start": 985.64, "end": 997.04, "text": " you might make your data set like bigger without just getting twice as much data? Yeah, totally.", "tokens": [50938, 291, 1062, 652, 428, 1412, 992, 411, 3801, 1553, 445, 1242, 6091, 382, 709, 1412, 30, 865, 11, 3879, 13, 51508], "temperature": 0.0, "avg_logprob": -0.22854530334472656, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.006002116482704878}, {"id": 179, "seek": 97416, "start": 997.04, "end": 1002.56, "text": " So like, say you have a, so data augmentation, awesome. And so say you have an image, you", "tokens": [51508, 407, 411, 11, 584, 291, 362, 257, 11, 370, 1412, 14501, 19631, 11, 3476, 13, 400, 370, 584, 291, 362, 364, 3256, 11, 291, 51784], "temperature": 0.0, "avg_logprob": -0.22854530334472656, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.006002116482704878}, {"id": 180, "seek": 100256, "start": 1002.56, "end": 1006.88, "text": " could just totally rotate that image. And now instead of one data point, you have like", "tokens": [50364, 727, 445, 3879, 13121, 300, 3256, 13, 400, 586, 2602, 295, 472, 1412, 935, 11, 291, 362, 411, 50580], "temperature": 0.0, "avg_logprob": -0.10626592636108398, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.010326480492949486}, {"id": 181, "seek": 100256, "start": 1006.88, "end": 1011.52, "text": " five data points or turn it black and white, or you could shift it or skew it or take the", "tokens": [50580, 1732, 1412, 2793, 420, 1261, 309, 2211, 293, 2418, 11, 420, 291, 727, 5513, 309, 420, 8756, 86, 309, 420, 747, 264, 50812], "temperature": 0.0, "avg_logprob": -0.10626592636108398, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.010326480492949486}, {"id": 182, "seek": 100256, "start": 1011.52, "end": 1015.4, "text": " top and the bottom and move them a little bit or add some noise. There's lots of things", "tokens": [50812, 1192, 293, 264, 2767, 293, 1286, 552, 257, 707, 857, 420, 909, 512, 5658, 13, 821, 311, 3195, 295, 721, 51006], "temperature": 0.0, "avg_logprob": -0.10626592636108398, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.010326480492949486}, {"id": 183, "seek": 100256, "start": 1015.4, "end": 1019.3599999999999, "text": " you can do to make data bigger and actually improve a model just by changing the data", "tokens": [51006, 291, 393, 360, 281, 652, 1412, 3801, 293, 767, 3470, 257, 2316, 445, 538, 4473, 264, 1412, 51204], "temperature": 0.0, "avg_logprob": -0.10626592636108398, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.010326480492949486}, {"id": 184, "seek": 100256, "start": 1019.3599999999999, "end": 1025.32, "text": " set. And these are all fall within data-centric AI. Are you all familiar with what's called", "tokens": [51204, 992, 13, 400, 613, 366, 439, 2100, 1951, 1412, 12, 45300, 7318, 13, 2014, 291, 439, 4963, 365, 437, 311, 1219, 51502], "temperature": 0.0, "avg_logprob": -0.10626592636108398, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.010326480492949486}, {"id": 185, "seek": 102532, "start": 1025.32, "end": 1034.56, "text": " like back translation for text data augmentation? So this is a pretty fun thing. I don't like", "tokens": [50364, 411, 646, 12853, 337, 2487, 1412, 14501, 19631, 30, 407, 341, 307, 257, 1238, 1019, 551, 13, 286, 500, 380, 411, 50826], "temperature": 0.0, "avg_logprob": -0.2131823681770487, "compression_ratio": 1.328358208955224, "no_speech_prob": 0.27502191066741943}, {"id": 186, "seek": 102532, "start": 1034.56, "end": 1043.28, "text": " have particularly good translation skills. But like, if I say like, hi, you know, my", "tokens": [50826, 362, 4098, 665, 12853, 3942, 13, 583, 411, 11, 498, 286, 584, 411, 11, 4879, 11, 291, 458, 11, 452, 51262], "temperature": 0.0, "avg_logprob": -0.2131823681770487, "compression_ratio": 1.328358208955224, "no_speech_prob": 0.27502191066741943}, {"id": 187, "seek": 104328, "start": 1043.28, "end": 1066.68, "text": " name is Curtis. And then I translate this. Okay, so it becomes Ola. And then I translate", "tokens": [50364, 1315, 307, 42140, 13, 400, 550, 286, 13799, 341, 13, 1033, 11, 370, 309, 3643, 422, 875, 13, 400, 550, 286, 13799, 51534], "temperature": 0.0, "avg_logprob": -0.2575964010678805, "compression_ratio": 1.2054794520547945, "no_speech_prob": 0.3997177481651306}, {"id": 188, "seek": 106668, "start": 1066.68, "end": 1079.52, "text": " this again. It might become, and now I've gotten one data point, and I just got another", "tokens": [50364, 341, 797, 13, 467, 1062, 1813, 11, 293, 586, 286, 600, 5768, 472, 1412, 935, 11, 293, 286, 445, 658, 1071, 51006], "temperature": 0.0, "avg_logprob": -0.1766313737438571, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.42231613397598267}, {"id": 189, "seek": 106668, "start": 1079.52, "end": 1084.52, "text": " data point. So I was able to augment my text data set to get more versions of the same", "tokens": [51006, 1412, 935, 13, 407, 286, 390, 1075, 281, 29919, 452, 2487, 1412, 992, 281, 483, 544, 9606, 295, 264, 912, 51256], "temperature": 0.0, "avg_logprob": -0.1766313737438571, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.42231613397598267}, {"id": 190, "seek": 106668, "start": 1084.52, "end": 1091.92, "text": " thing. And this could have some label, and the label could be introduction. You know,", "tokens": [51256, 551, 13, 400, 341, 727, 362, 512, 7645, 11, 293, 264, 7645, 727, 312, 9339, 13, 509, 458, 11, 51626], "temperature": 0.0, "avg_logprob": -0.1766313737438571, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.42231613397598267}, {"id": 191, "seek": 106668, "start": 1091.92, "end": 1094.96, "text": " I'll just end it there because I know it's hard to see on that side. But this, this would", "tokens": [51626, 286, 603, 445, 917, 309, 456, 570, 286, 458, 309, 311, 1152, 281, 536, 322, 300, 1252, 13, 583, 341, 11, 341, 576, 51778], "temperature": 0.0, "avg_logprob": -0.1766313737438571, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.42231613397598267}, {"id": 192, "seek": 109496, "start": 1094.96, "end": 1102.2, "text": " be like the label. And this is text. And so this one is the same label. We haven't changed", "tokens": [50364, 312, 411, 264, 7645, 13, 400, 341, 307, 2487, 13, 400, 370, 341, 472, 307, 264, 912, 7645, 13, 492, 2378, 380, 3105, 50726], "temperature": 0.0, "avg_logprob": -0.2785541827862079, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.17767363786697388}, {"id": 193, "seek": 109496, "start": 1102.2, "end": 1106.08, "text": " the label at all. So this is intro. And now you can see I have more labeled data, but", "tokens": [50726, 264, 7645, 412, 439, 13, 407, 341, 307, 12897, 13, 400, 586, 291, 393, 536, 286, 362, 544, 21335, 1412, 11, 457, 50920], "temperature": 0.0, "avg_logprob": -0.2785541827862079, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.17767363786697388}, {"id": 194, "seek": 109496, "start": 1106.08, "end": 1109.28, "text": " I didn't have to do anything. I didn't have to pay more. I didn't have to, I just did", "tokens": [50920, 286, 994, 380, 362, 281, 360, 1340, 13, 286, 994, 380, 362, 281, 1689, 544, 13, 286, 994, 380, 362, 281, 11, 286, 445, 630, 51080], "temperature": 0.0, "avg_logprob": -0.2785541827862079, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.17767363786697388}, {"id": 195, "seek": 109496, "start": 1109.28, "end": 1110.68, "text": " this all computationally. Yeah.", "tokens": [51080, 341, 439, 24903, 379, 13, 865, 13, 51150], "temperature": 0.0, "avg_logprob": -0.2785541827862079, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.17767363786697388}, {"id": 196, "seek": 109496, "start": 1110.68, "end": 1114.8400000000001, "text": " How do you see this in all the cases for that error to be taken?", "tokens": [51150, 1012, 360, 291, 536, 341, 294, 439, 264, 3331, 337, 300, 6713, 281, 312, 2726, 30, 51358], "temperature": 0.0, "avg_logprob": -0.2785541827862079, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.17767363786697388}, {"id": 197, "seek": 109496, "start": 1114.8400000000001, "end": 1120.3600000000001, "text": " Oh yeah, totally can. So if, say that this label was wrong, now you have two label errors.", "tokens": [51358, 876, 1338, 11, 3879, 393, 13, 407, 498, 11, 584, 300, 341, 7645, 390, 2085, 11, 586, 291, 362, 732, 7645, 13603, 13, 51634], "temperature": 0.0, "avg_logprob": -0.2785541827862079, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.17767363786697388}, {"id": 198, "seek": 112036, "start": 1120.84, "end": 1125.8799999999999, "text": " Totally. Yeah. So you often want to combine two approaches. One is like first, try to", "tokens": [50388, 22837, 13, 865, 13, 407, 291, 2049, 528, 281, 10432, 732, 11587, 13, 1485, 307, 411, 700, 11, 853, 281, 50640], "temperature": 0.0, "avg_logprob": -0.19791375865106997, "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.01798156462609768}, {"id": 199, "seek": 112036, "start": 1125.8799999999999, "end": 1130.12, "text": " fix or improve label errors before doing the augmentation. And that's a really good idea.", "tokens": [50640, 3191, 420, 3470, 7645, 13603, 949, 884, 264, 14501, 19631, 13, 400, 300, 311, 257, 534, 665, 1558, 13, 50852], "temperature": 0.0, "avg_logprob": -0.19791375865106997, "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.01798156462609768}, {"id": 200, "seek": 112036, "start": 1133.12, "end": 1136.3999999999999, "text": " All right. So what are some examples, we looked at examples that are not data-centric AI. So", "tokens": [51002, 1057, 558, 13, 407, 437, 366, 512, 5110, 11, 321, 2956, 412, 5110, 300, 366, 406, 1412, 12, 45300, 7318, 13, 407, 51166], "temperature": 0.0, "avg_logprob": -0.19791375865106997, "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.01798156462609768}, {"id": 201, "seek": 112036, "start": 1136.3999999999999, "end": 1141.28, "text": " what are some things that are data-centric AI? And for this, I'll go through quick because", "tokens": [51166, 437, 366, 512, 721, 300, 366, 1412, 12, 45300, 7318, 30, 400, 337, 341, 11, 286, 603, 352, 807, 1702, 570, 51410], "temperature": 0.0, "avg_logprob": -0.19791375865106997, "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.01798156462609768}, {"id": 202, "seek": 112036, "start": 1141.28, "end": 1146.6799999999998, "text": " we're going to learn all these in the course. And so one is outlier detection and removal.", "tokens": [51410, 321, 434, 516, 281, 1466, 439, 613, 294, 264, 1164, 13, 400, 370, 472, 307, 484, 2753, 17784, 293, 17933, 13, 51680], "temperature": 0.0, "avg_logprob": -0.19791375865106997, "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.01798156462609768}, {"id": 203, "seek": 114668, "start": 1146.68, "end": 1152.96, "text": " So I'll just be really quick to show you. So say you have some data set and we've got,", "tokens": [50364, 407, 286, 603, 445, 312, 534, 1702, 281, 855, 291, 13, 407, 584, 291, 362, 512, 1412, 992, 293, 321, 600, 658, 11, 50678], "temperature": 0.0, "avg_logprob": -0.1834922174010614, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.00034596299519762397}, {"id": 204, "seek": 114668, "start": 1152.96, "end": 1163.96, "text": " let me use this board. And you've got, say, right? So you've got your sort of, your, you", "tokens": [50678, 718, 385, 764, 341, 3150, 13, 400, 291, 600, 658, 11, 584, 11, 558, 30, 407, 291, 600, 658, 428, 1333, 295, 11, 428, 11, 291, 51228], "temperature": 0.0, "avg_logprob": -0.1834922174010614, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.00034596299519762397}, {"id": 205, "seek": 114668, "start": 1163.96, "end": 1168.76, "text": " know, two classes. And so normally you would, you would draw your classifier and then you", "tokens": [51228, 458, 11, 732, 5359, 13, 400, 370, 5646, 291, 576, 11, 291, 576, 2642, 428, 1508, 9902, 293, 550, 291, 51468], "temperature": 0.0, "avg_logprob": -0.1834922174010614, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.00034596299519762397}, {"id": 206, "seek": 114668, "start": 1168.76, "end": 1172.52, "text": " have some new point here and I'll be labeled a negative. But say in your training data,", "tokens": [51468, 362, 512, 777, 935, 510, 293, 286, 603, 312, 21335, 257, 3671, 13, 583, 584, 294, 428, 3097, 1412, 11, 51656], "temperature": 0.0, "avg_logprob": -0.1834922174010614, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.00034596299519762397}, {"id": 207, "seek": 117252, "start": 1172.52, "end": 1179.12, "text": " you have this really weird, you know, plus over here. And so maybe the boundary should", "tokens": [50364, 291, 362, 341, 534, 3657, 11, 291, 458, 11, 1804, 670, 510, 13, 400, 370, 1310, 264, 12866, 820, 50694], "temperature": 0.0, "avg_logprob": -0.1148512006744625, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0027136264834553003}, {"id": 208, "seek": 117252, "start": 1179.12, "end": 1182.52, "text": " be something like this, but you just don't have very much information and it's really", "tokens": [50694, 312, 746, 411, 341, 11, 457, 291, 445, 500, 380, 362, 588, 709, 1589, 293, 309, 311, 534, 50864], "temperature": 0.0, "avg_logprob": -0.1148512006744625, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0027136264834553003}, {"id": 209, "seek": 117252, "start": 1182.52, "end": 1187.84, "text": " out here and it seems like it's not very related to the rest of the data. And so what often", "tokens": [50864, 484, 510, 293, 309, 2544, 411, 309, 311, 406, 588, 4077, 281, 264, 1472, 295, 264, 1412, 13, 400, 370, 437, 2049, 51130], "temperature": 0.0, "avg_logprob": -0.1148512006744625, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0027136264834553003}, {"id": 210, "seek": 117252, "start": 1187.84, "end": 1191.4, "text": " people will do because they just don't have enough information is they'll identify this", "tokens": [51130, 561, 486, 360, 570, 436, 445, 500, 380, 362, 1547, 1589, 307, 436, 603, 5876, 341, 51308], "temperature": 0.0, "avg_logprob": -0.1148512006744625, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0027136264834553003}, {"id": 211, "seek": 117252, "start": 1191.4, "end": 1195.56, "text": " as an automatic, as an outlier because it seems very out of distribution and they'll", "tokens": [51308, 382, 364, 12509, 11, 382, 364, 484, 2753, 570, 309, 2544, 588, 484, 295, 7316, 293, 436, 603, 51516], "temperature": 0.0, "avg_logprob": -0.1148512006744625, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0027136264834553003}, {"id": 212, "seek": 117252, "start": 1195.56, "end": 1199.04, "text": " remove it. And so then you get this line, which fits to all the data except for that", "tokens": [51516, 4159, 309, 13, 400, 370, 550, 291, 483, 341, 1622, 11, 597, 9001, 281, 439, 264, 1412, 3993, 337, 300, 51690], "temperature": 0.0, "avg_logprob": -0.1148512006744625, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0027136264834553003}, {"id": 213, "seek": 119904, "start": 1199.04, "end": 1207.44, "text": " one point. In terms of some other things in data-centric AI, so error detection and correction.", "tokens": [50364, 472, 935, 13, 682, 2115, 295, 512, 661, 721, 294, 1412, 12, 45300, 7318, 11, 370, 6713, 17784, 293, 19984, 13, 50784], "temperature": 0.0, "avg_logprob": -0.1971460041246916, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.053339883685112}, {"id": 214, "seek": 119904, "start": 1207.44, "end": 1212.2, "text": " And so that's, for example, you just have like a data point that is, it could be images,", "tokens": [50784, 400, 370, 300, 311, 11, 337, 1365, 11, 291, 445, 362, 411, 257, 1412, 935, 300, 307, 11, 309, 727, 312, 5267, 11, 51022], "temperature": 0.0, "avg_logprob": -0.1971460041246916, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.053339883685112}, {"id": 215, "seek": 119904, "start": 1212.2, "end": 1217.96, "text": " it's like all black, or you have a label error like we saw earlier. If this, you know, text", "tokens": [51022, 309, 311, 411, 439, 2211, 11, 420, 291, 362, 257, 7645, 6713, 411, 321, 1866, 3071, 13, 759, 341, 11, 291, 458, 11, 2487, 51310], "temperature": 0.0, "avg_logprob": -0.1971460041246916, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.053339883685112}, {"id": 216, "seek": 119904, "start": 1217.96, "end": 1224.36, "text": " example instead of being labeled intro was labeled like, you know, a goodbye clause or", "tokens": [51310, 1365, 2602, 295, 885, 21335, 12897, 390, 21335, 411, 11, 291, 458, 11, 257, 12084, 25925, 420, 51630], "temperature": 0.0, "avg_logprob": -0.1971460041246916, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.053339883685112}, {"id": 217, "seek": 122436, "start": 1225.32, "end": 1232.3999999999999, "text": " something. You'd want to find that and automatically correct it. Data augmentation, which is what", "tokens": [50412, 746, 13, 509, 1116, 528, 281, 915, 300, 293, 6772, 3006, 309, 13, 11888, 14501, 19631, 11, 597, 307, 437, 50766], "temperature": 0.0, "avg_logprob": -0.13347460772540118, "compression_ratio": 1.6236933797909407, "no_speech_prob": 0.07581523805856705}, {"id": 218, "seek": 122436, "start": 1232.3999999999999, "end": 1236.36, "text": " we saw earlier. So that's like increasing the size of your data set. So you have more", "tokens": [50766, 321, 1866, 3071, 13, 407, 300, 311, 411, 5662, 264, 2744, 295, 428, 1412, 992, 13, 407, 291, 362, 544, 50964], "temperature": 0.0, "avg_logprob": -0.13347460772540118, "compression_ratio": 1.6236933797909407, "no_speech_prob": 0.07581523805856705}, {"id": 219, "seek": 122436, "start": 1236.36, "end": 1243.08, "text": " training data. Feature engineering and selections. The idea here is you have, if anyone's familiar", "tokens": [50964, 3097, 1412, 13, 3697, 1503, 7043, 293, 47829, 13, 440, 1558, 510, 307, 291, 362, 11, 498, 2878, 311, 4963, 51300], "temperature": 0.0, "avg_logprob": -0.13347460772540118, "compression_ratio": 1.6236933797909407, "no_speech_prob": 0.07581523805856705}, {"id": 220, "seek": 122436, "start": 1243.08, "end": 1248.1999999999998, "text": " like the early days of neural networks couldn't solve like the XOR problem, but you could", "tokens": [51300, 411, 264, 2440, 1708, 295, 18161, 9590, 2809, 380, 5039, 411, 264, 1783, 2483, 1154, 11, 457, 291, 727, 51556], "temperature": 0.0, "avg_logprob": -0.13347460772540118, "compression_ratio": 1.6236933797909407, "no_speech_prob": 0.07581523805856705}, {"id": 221, "seek": 122436, "start": 1248.1999999999998, "end": 1253.9199999999998, "text": " always just generate the XOR as another column. A more concrete example, if you haven't heard", "tokens": [51556, 1009, 445, 8460, 264, 1783, 2483, 382, 1071, 7738, 13, 316, 544, 9859, 1365, 11, 498, 291, 2378, 380, 2198, 51842], "temperature": 0.0, "avg_logprob": -0.13347460772540118, "compression_ratio": 1.6236933797909407, "no_speech_prob": 0.07581523805856705}, {"id": 222, "seek": 125392, "start": 1253.96, "end": 1259.48, "text": " of this, that one is you just have, you know, a bunch of tabular data. I worked on cheating", "tokens": [50366, 295, 341, 11, 300, 472, 307, 291, 445, 362, 11, 291, 458, 11, 257, 3840, 295, 4421, 1040, 1412, 13, 286, 2732, 322, 18309, 50642], "temperature": 0.0, "avg_logprob": -0.13319197722843715, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0027136835269629955}, {"id": 223, "seek": 125392, "start": 1259.48, "end": 1266.0800000000002, "text": " detection at MIT. And, you know, if you want the machine learning model to learn, say who", "tokens": [50642, 17784, 412, 13100, 13, 400, 11, 291, 458, 11, 498, 291, 528, 264, 3479, 2539, 2316, 281, 1466, 11, 584, 567, 50972], "temperature": 0.0, "avg_logprob": -0.13319197722843715, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0027136835269629955}, {"id": 224, "seek": 125392, "start": 1266.0800000000002, "end": 1272.0800000000002, "text": " is a cheater from a bunch of, you know, education data, like where did they go to school and", "tokens": [50972, 307, 257, 947, 771, 490, 257, 3840, 295, 11, 291, 458, 11, 3309, 1412, 11, 411, 689, 630, 436, 352, 281, 1395, 293, 51272], "temperature": 0.0, "avg_logprob": -0.13319197722843715, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0027136835269629955}, {"id": 225, "seek": 125392, "start": 1272.0800000000002, "end": 1276.48, "text": " what's their background and what problems did they answer. It can really help if you", "tokens": [51272, 437, 311, 641, 3678, 293, 437, 2740, 630, 436, 1867, 13, 467, 393, 534, 854, 498, 291, 51492], "temperature": 0.0, "avg_logprob": -0.13319197722843715, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0027136835269629955}, {"id": 226, "seek": 125392, "start": 1276.48, "end": 1282.24, "text": " also compute new features like how often did they submit answers within five seconds of", "tokens": [51492, 611, 14722, 777, 4122, 411, 577, 2049, 630, 436, 10315, 6338, 1951, 1732, 3949, 295, 51780], "temperature": 0.0, "avg_logprob": -0.13319197722843715, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0027136835269629955}, {"id": 227, "seek": 128224, "start": 1282.28, "end": 1286.44, "text": " another student. So you can always generate new features and then you pass those into the", "tokens": [50366, 1071, 3107, 13, 407, 291, 393, 1009, 8460, 777, 4122, 293, 550, 291, 1320, 729, 666, 264, 50574], "temperature": 0.0, "avg_logprob": -0.23941917808688418, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.0018091435777023435}, {"id": 228, "seek": 128224, "start": 1286.44, "end": 1290.6, "text": " model and your model can, you know, if the features are relevant to the label you're", "tokens": [50574, 2316, 293, 428, 2316, 393, 11, 291, 458, 11, 498, 264, 4122, 366, 7340, 281, 264, 7645, 291, 434, 50782], "temperature": 0.0, "avg_logprob": -0.23941917808688418, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.0018091435777023435}, {"id": 229, "seek": 128224, "start": 1290.6, "end": 1294.6, "text": " trying to predict, it can do a lot better. Yeah.", "tokens": [50782, 1382, 281, 6069, 11, 309, 393, 360, 257, 688, 1101, 13, 865, 13, 50982], "temperature": 0.0, "avg_logprob": -0.23941917808688418, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.0018091435777023435}, {"id": 230, "seek": 128224, "start": 1294.6, "end": 1303.6, "text": " When we're doing outlier detection and removal, how do you know that an outlier is the result", "tokens": [50982, 1133, 321, 434, 884, 484, 2753, 17784, 293, 17933, 11, 577, 360, 291, 458, 300, 364, 484, 2753, 307, 264, 1874, 51432], "temperature": 0.0, "avg_logprob": -0.23941917808688418, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.0018091435777023435}, {"id": 231, "seek": 128224, "start": 1303.6, "end": 1308.92, "text": " of something that should not have happened or indication of a very rare event that you", "tokens": [51432, 295, 746, 300, 820, 406, 362, 2011, 420, 18877, 295, 257, 588, 5892, 2280, 300, 291, 51698], "temperature": 0.0, "avg_logprob": -0.23941917808688418, "compression_ratio": 1.6903765690376569, "no_speech_prob": 0.0018091435777023435}, {"id": 232, "seek": 130892, "start": 1308.96, "end": 1314.96, "text": " should pay attention to? Yeah, that's a really fair and hard question to answer. You make", "tokens": [50366, 820, 1689, 3202, 281, 30, 865, 11, 300, 311, 257, 534, 3143, 293, 1152, 1168, 281, 1867, 13, 509, 652, 50666], "temperature": 0.0, "avg_logprob": -0.1570206186045771, "compression_ratio": 1.8647540983606556, "no_speech_prob": 0.004066859371960163}, {"id": 233, "seek": 130892, "start": 1314.96, "end": 1320.16, "text": " assumptions. And so in this case, I was making an assumption, right? I was saying, like, I", "tokens": [50666, 17695, 13, 400, 370, 294, 341, 1389, 11, 286, 390, 1455, 364, 15302, 11, 558, 30, 286, 390, 1566, 11, 411, 11, 286, 50926], "temperature": 0.0, "avg_logprob": -0.1570206186045771, "compression_ratio": 1.8647540983606556, "no_speech_prob": 0.004066859371960163}, {"id": 234, "seek": 130892, "start": 1320.16, "end": 1323.6000000000001, "text": " didn't draw very much data, but say that I had, like, millions of data points and then", "tokens": [50926, 994, 380, 2642, 588, 709, 1412, 11, 457, 584, 300, 286, 632, 11, 411, 11, 6803, 295, 1412, 2793, 293, 550, 51098], "temperature": 0.0, "avg_logprob": -0.1570206186045771, "compression_ratio": 1.8647540983606556, "no_speech_prob": 0.004066859371960163}, {"id": 235, "seek": 130892, "start": 1323.6000000000001, "end": 1329.1200000000001, "text": " this one's really far away. Then the assumption you're making is that, like, I have so much", "tokens": [51098, 341, 472, 311, 534, 1400, 1314, 13, 1396, 264, 15302, 291, 434, 1455, 307, 300, 11, 411, 11, 286, 362, 370, 709, 51374], "temperature": 0.0, "avg_logprob": -0.1570206186045771, "compression_ratio": 1.8647540983606556, "no_speech_prob": 0.004066859371960163}, {"id": 236, "seek": 130892, "start": 1329.1200000000001, "end": 1334.0, "text": " data that suggests that this is the distribution and then I have very little data that suggests", "tokens": [51374, 1412, 300, 13409, 300, 341, 307, 264, 7316, 293, 550, 286, 362, 588, 707, 1412, 300, 13409, 51618], "temperature": 0.0, "avg_logprob": -0.1570206186045771, "compression_ratio": 1.8647540983606556, "no_speech_prob": 0.004066859371960163}, {"id": 237, "seek": 133400, "start": 1334.0, "end": 1338.76, "text": " that this is part of that distribution. And the biggest issue is that if your classifier", "tokens": [50364, 300, 341, 307, 644, 295, 300, 7316, 13, 400, 264, 3880, 2734, 307, 300, 498, 428, 1508, 9902, 50602], "temperature": 0.0, "avg_logprob": -0.13063422432781135, "compression_ratio": 1.8127090301003344, "no_speech_prob": 0.02296263538300991}, {"id": 238, "seek": 133400, "start": 1338.76, "end": 1343.44, "text": " is changing dramatically for one data point, but you have, just think of it as, like, evidence.", "tokens": [50602, 307, 4473, 17548, 337, 472, 1412, 935, 11, 457, 291, 362, 11, 445, 519, 295, 309, 382, 11, 411, 11, 4467, 13, 50836], "temperature": 0.0, "avg_logprob": -0.13063422432781135, "compression_ratio": 1.8127090301003344, "no_speech_prob": 0.02296263538300991}, {"id": 239, "seek": 133400, "start": 1343.44, "end": 1348.28, "text": " I've got a thousand or a million people saying it should be this thing and then I've got", "tokens": [50836, 286, 600, 658, 257, 4714, 420, 257, 2459, 561, 1566, 309, 820, 312, 341, 551, 293, 550, 286, 600, 658, 51078], "temperature": 0.0, "avg_logprob": -0.13063422432781135, "compression_ratio": 1.8127090301003344, "no_speech_prob": 0.02296263538300991}, {"id": 240, "seek": 133400, "start": 1348.28, "end": 1352.4, "text": " this one other person who may be right, they may be right, but they're saying that I think", "tokens": [51078, 341, 472, 661, 954, 567, 815, 312, 558, 11, 436, 815, 312, 558, 11, 457, 436, 434, 1566, 300, 286, 519, 51284], "temperature": 0.0, "avg_logprob": -0.13063422432781135, "compression_ratio": 1.8127090301003344, "no_speech_prob": 0.02296263538300991}, {"id": 241, "seek": 133400, "start": 1352.4, "end": 1358.32, "text": " it's this other thing and it greatly skews the classifier. And so just because you want", "tokens": [51284, 309, 311, 341, 661, 551, 293, 309, 14147, 8756, 14358, 264, 1508, 9902, 13, 400, 370, 445, 570, 291, 528, 51580], "temperature": 0.0, "avg_logprob": -0.13063422432781135, "compression_ratio": 1.8127090301003344, "no_speech_prob": 0.02296263538300991}, {"id": 242, "seek": 133400, "start": 1358.32, "end": 1363.6, "text": " to trust the masses, you will say, hey, that one person is really, seems really off base.", "tokens": [51580, 281, 3361, 264, 23935, 11, 291, 486, 584, 11, 4177, 11, 300, 472, 954, 307, 534, 11, 2544, 534, 766, 3096, 13, 51844], "temperature": 0.0, "avg_logprob": -0.13063422432781135, "compression_ratio": 1.8127090301003344, "no_speech_prob": 0.02296263538300991}, {"id": 243, "seek": 136360, "start": 1363.6, "end": 1367.4399999999998, "text": " And this is very much a choice. And so what typically, what you do is you sort of rank", "tokens": [50364, 400, 341, 307, 588, 709, 257, 3922, 13, 400, 370, 437, 5850, 11, 437, 291, 360, 307, 291, 1333, 295, 6181, 50556], "temperature": 0.0, "avg_logprob": -0.1803068810320915, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.003170822048559785}, {"id": 244, "seek": 136360, "start": 1367.4399999999998, "end": 1373.36, "text": " every data point in terms of how in distribution it is and then you choose your cutoff and", "tokens": [50556, 633, 1412, 935, 294, 2115, 295, 577, 294, 7316, 309, 307, 293, 550, 291, 2826, 428, 1723, 4506, 293, 50852], "temperature": 0.0, "avg_logprob": -0.1803068810320915, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.003170822048559785}, {"id": 245, "seek": 136360, "start": 1373.36, "end": 1385.24, "text": " that's a human decision. Another data-centric AI task is to establish consensus labels. So", "tokens": [50852, 300, 311, 257, 1952, 3537, 13, 3996, 1412, 12, 45300, 7318, 5633, 307, 281, 8327, 19115, 16949, 13, 407, 51446], "temperature": 0.0, "avg_logprob": -0.1803068810320915, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.003170822048559785}, {"id": 246, "seek": 136360, "start": 1385.24, "end": 1389.9199999999998, "text": " if you guys have heard of, like, the self-driving cars, of course, then a lot of the ways that", "tokens": [51446, 498, 291, 1074, 362, 2198, 295, 11, 411, 11, 264, 2698, 12, 47094, 5163, 11, 295, 1164, 11, 550, 257, 688, 295, 264, 2098, 300, 51680], "temperature": 0.0, "avg_logprob": -0.1803068810320915, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.003170822048559785}, {"id": 247, "seek": 138992, "start": 1389.96, "end": 1394.0, "text": " these models are trained is you'll have an image and then they want really high-quality", "tokens": [50366, 613, 5245, 366, 8895, 307, 291, 603, 362, 364, 3256, 293, 550, 436, 528, 534, 1090, 12, 11286, 50568], "temperature": 0.0, "avg_logprob": -0.13430053096706585, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.12576261162757874}, {"id": 248, "seek": 138992, "start": 1394.0, "end": 1399.2, "text": " data. So they'll have, like, 20 different people label, is this a scene of a street or", "tokens": [50568, 1412, 13, 407, 436, 603, 362, 11, 411, 11, 945, 819, 561, 7645, 11, 307, 341, 257, 4145, 295, 257, 4838, 420, 50828], "temperature": 0.0, "avg_logprob": -0.13430053096706585, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.12576261162757874}, {"id": 249, "seek": 138992, "start": 1399.2, "end": 1403.92, "text": " are we on a bridge? Is this a stop sign? Because they really need to get accurate labels. The", "tokens": [50828, 366, 321, 322, 257, 7283, 30, 1119, 341, 257, 1590, 1465, 30, 1436, 436, 534, 643, 281, 483, 8559, 16949, 13, 440, 51064], "temperature": 0.0, "avg_logprob": -0.13430053096706585, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.12576261162757874}, {"id": 250, "seek": 138992, "start": 1403.92, "end": 1408.24, "text": " question is, when you're training your model, if you're just going to train with one label", "tokens": [51064, 1168, 307, 11, 562, 291, 434, 3097, 428, 2316, 11, 498, 291, 434, 445, 516, 281, 3847, 365, 472, 7645, 51280], "temperature": 0.0, "avg_logprob": -0.13430053096706585, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.12576261162757874}, {"id": 251, "seek": 138992, "start": 1408.24, "end": 1414.6000000000001, "text": " for that image or one set of labels for that image, you can't use all 20 of your annotator's,", "tokens": [51280, 337, 300, 3256, 420, 472, 992, 295, 16949, 337, 300, 3256, 11, 291, 393, 380, 764, 439, 945, 295, 428, 25339, 1639, 311, 11, 51598], "temperature": 0.0, "avg_logprob": -0.13430053096706585, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.12576261162757874}, {"id": 252, "seek": 141460, "start": 1414.7199999999998, "end": 1420.0, "text": " you know, guesses. You have to somehow combine them into a single training label. So how do", "tokens": [50370, 291, 458, 11, 42703, 13, 509, 362, 281, 6063, 10432, 552, 666, 257, 2167, 3097, 7645, 13, 407, 577, 360, 50634], "temperature": 0.0, "avg_logprob": -0.15044005163784685, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0715465396642685}, {"id": 253, "seek": 141460, "start": 1420.0, "end": 1425.8, "text": " you do that in a way that maximizes model performance? Another one is active learning.", "tokens": [50634, 291, 360, 300, 294, 257, 636, 300, 5138, 5660, 2316, 3389, 30, 3996, 472, 307, 4967, 2539, 13, 50924], "temperature": 0.0, "avg_logprob": -0.15044005163784685, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0715465396642685}, {"id": 254, "seek": 141460, "start": 1425.8, "end": 1431.24, "text": " And this is a very classical problem. You have some data set, you train a model, it has 80%", "tokens": [50924, 400, 341, 307, 257, 588, 13735, 1154, 13, 509, 362, 512, 1412, 992, 11, 291, 3847, 257, 2316, 11, 309, 575, 4688, 4, 51196], "temperature": 0.0, "avg_logprob": -0.15044005163784685, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0715465396642685}, {"id": 255, "seek": 141460, "start": 1431.24, "end": 1437.04, "text": " performance. Okay, I want now to get my model to 85% performance, but I want to pay for as", "tokens": [51196, 3389, 13, 1033, 11, 286, 528, 586, 281, 483, 452, 2316, 281, 14695, 4, 3389, 11, 457, 286, 528, 281, 1689, 337, 382, 51486], "temperature": 0.0, "avg_logprob": -0.15044005163784685, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0715465396642685}, {"id": 256, "seek": 141460, "start": 1437.04, "end": 1442.32, "text": " little new data as possible. Or I want to improve the current existing data as little as possible.", "tokens": [51486, 707, 777, 1412, 382, 1944, 13, 1610, 286, 528, 281, 3470, 264, 2190, 6741, 1412, 382, 707, 382, 1944, 13, 51750], "temperature": 0.0, "avg_logprob": -0.15044005163784685, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0715465396642685}, {"id": 257, "seek": 144232, "start": 1442.6399999999999, "end": 1446.96, "text": " What are my next steps? And you can automate that process. You can actually get good signal to", "tokens": [50380, 708, 366, 452, 958, 4439, 30, 400, 291, 393, 31605, 300, 1399, 13, 509, 393, 767, 483, 665, 6358, 281, 50596], "temperature": 0.0, "avg_logprob": -0.09249649877133577, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0009109752136282623}, {"id": 258, "seek": 144232, "start": 1446.96, "end": 1451.6799999999998, "text": " optimize in a way that you minimize the amount of new data that you need to collect information for", "tokens": [50596, 19719, 294, 257, 636, 300, 291, 17522, 264, 2372, 295, 777, 1412, 300, 291, 643, 281, 2500, 1589, 337, 50832], "temperature": 0.0, "avg_logprob": -0.09249649877133577, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0009109752136282623}, {"id": 259, "seek": 144232, "start": 1451.6799999999998, "end": 1458.24, "text": " or label in order to achieve that model accuracy. And then a final example is curriculum learning,", "tokens": [50832, 420, 7645, 294, 1668, 281, 4584, 300, 2316, 14170, 13, 400, 550, 257, 2572, 1365, 307, 14302, 2539, 11, 51160], "temperature": 0.0, "avg_logprob": -0.09249649877133577, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0009109752136282623}, {"id": 260, "seek": 144232, "start": 1458.24, "end": 1463.28, "text": " which we already mentioned. So these are some examples of data-centric AI tasks, many of which", "tokens": [51160, 597, 321, 1217, 2835, 13, 407, 613, 366, 512, 5110, 295, 1412, 12, 45300, 7318, 9608, 11, 867, 295, 597, 51412], "temperature": 0.0, "avg_logprob": -0.09249649877133577, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0009109752136282623}, {"id": 261, "seek": 144232, "start": 1463.28, "end": 1468.3999999999999, "text": " we'll cover over the next two weeks. All right, so there's a lot of hype around data-centric AI.", "tokens": [51412, 321, 603, 2060, 670, 264, 958, 732, 3259, 13, 1057, 558, 11, 370, 456, 311, 257, 688, 295, 24144, 926, 1412, 12, 45300, 7318, 13, 51668], "temperature": 0.0, "avg_logprob": -0.09249649877133577, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0009109752136282623}, {"id": 262, "seek": 146840, "start": 1468.48, "end": 1474.4, "text": " For those who are familiar with Andrew Ng, he's a pretty well-known person in AI from Stanford", "tokens": [50368, 1171, 729, 567, 366, 4963, 365, 10110, 21198, 11, 415, 311, 257, 1238, 731, 12, 6861, 954, 294, 7318, 490, 20374, 50664], "temperature": 0.0, "avg_logprob": -0.11765780411367341, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00884332787245512}, {"id": 263, "seek": 146840, "start": 1474.4, "end": 1480.64, "text": " and has been at Google Research and Baidi Research and done a lot of things found in Coursera.", "tokens": [50664, 293, 575, 668, 412, 3329, 10303, 293, 6777, 12716, 10303, 293, 1096, 257, 688, 295, 721, 1352, 294, 383, 5067, 1663, 13, 50976], "temperature": 0.0, "avg_logprob": -0.11765780411367341, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00884332787245512}, {"id": 264, "seek": 146840, "start": 1480.64, "end": 1486.5600000000002, "text": " So he's been really excited about data-centric AI. And let's look at some reasons, you know, why and", "tokens": [50976, 407, 415, 311, 668, 534, 2919, 466, 1412, 12, 45300, 7318, 13, 400, 718, 311, 574, 412, 512, 4112, 11, 291, 458, 11, 983, 293, 51272], "temperature": 0.0, "avg_logprob": -0.11765780411367341, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00884332787245512}, {"id": 265, "seek": 146840, "start": 1486.5600000000002, "end": 1491.1200000000001, "text": " some of the things that we've seen in the news. For example, he mentioned that like 80% of an AI", "tokens": [51272, 512, 295, 264, 721, 300, 321, 600, 1612, 294, 264, 2583, 13, 1171, 1365, 11, 415, 2835, 300, 411, 4688, 4, 295, 364, 7318, 51500], "temperature": 0.0, "avg_logprob": -0.11765780411367341, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00884332787245512}, {"id": 266, "seek": 146840, "start": 1491.1200000000001, "end": 1495.52, "text": " developer's time is actually just spent on data, which is kind of funny, right? You know, you're", "tokens": [51500, 10754, 311, 565, 307, 767, 445, 4418, 322, 1412, 11, 597, 307, 733, 295, 4074, 11, 558, 30, 509, 458, 11, 291, 434, 51720], "temperature": 0.0, "avg_logprob": -0.11765780411367341, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00884332787245512}, {"id": 267, "seek": 149552, "start": 1495.52, "end": 1500.0, "text": " an AI developer, you're not like a data scientist, but yet you're doing data science work all the", "tokens": [50364, 364, 7318, 10754, 11, 291, 434, 406, 411, 257, 1412, 12662, 11, 457, 1939, 291, 434, 884, 1412, 3497, 589, 439, 264, 50588], "temperature": 0.0, "avg_logprob": -0.06015180037902282, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.001064580399543047}, {"id": 268, "seek": 149552, "start": 1500.0, "end": 1505.84, "text": " time. And so there's something happening here in the real world that there isn't as much until", "tokens": [50588, 565, 13, 400, 370, 456, 311, 746, 2737, 510, 294, 264, 957, 1002, 300, 456, 1943, 380, 382, 709, 1826, 50880], "temperature": 0.0, "avg_logprob": -0.06015180037902282, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.001064580399543047}, {"id": 269, "seek": 149552, "start": 1505.84, "end": 1511.36, "text": " recently actually systematic, you know, learning and teaching around how do we go about doing this.", "tokens": [50880, 3938, 767, 27249, 11, 291, 458, 11, 2539, 293, 4571, 926, 577, 360, 321, 352, 466, 884, 341, 13, 51156], "temperature": 0.0, "avg_logprob": -0.06015180037902282, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.001064580399543047}, {"id": 270, "seek": 149552, "start": 1512.0, "end": 1518.08, "text": " Also, if you're not familiar, bad data is very, very troublesome for businesses and for the", "tokens": [51188, 2743, 11, 498, 291, 434, 406, 4963, 11, 1578, 1412, 307, 588, 11, 588, 46838, 337, 6011, 293, 337, 264, 51492], "temperature": 0.0, "avg_logprob": -0.06015180037902282, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.001064580399543047}, {"id": 271, "seek": 149552, "start": 1518.08, "end": 1523.12, "text": " government and for economies. And it's estimated this is out of Harvard Business Review that it", "tokens": [51492, 2463, 293, 337, 23158, 13, 400, 309, 311, 14109, 341, 307, 484, 295, 13378, 10715, 19954, 300, 309, 51744], "temperature": 0.0, "avg_logprob": -0.06015180037902282, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.001064580399543047}, {"id": 272, "seek": 152312, "start": 1523.12, "end": 1529.04, "text": " cost the US alone about three trillion dollars in a given year. And you might see this and think,", "tokens": [50364, 2063, 264, 2546, 3312, 466, 1045, 18723, 3808, 294, 257, 2212, 1064, 13, 400, 291, 1062, 536, 341, 293, 519, 11, 50660], "temperature": 0.0, "avg_logprob": -0.09004374447031918, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.0008556845714338124}, {"id": 273, "seek": 152312, "start": 1529.04, "end": 1533.28, "text": " okay, that's really bad. But the good news is like a lot of people think that we can actually solve", "tokens": [50660, 1392, 11, 300, 311, 534, 1578, 13, 583, 264, 665, 2583, 307, 411, 257, 688, 295, 561, 519, 300, 321, 393, 767, 5039, 50872], "temperature": 0.0, "avg_logprob": -0.09004374447031918, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.0008556845714338124}, {"id": 274, "seek": 152312, "start": 1533.28, "end": 1538.56, "text": " a lot of that three trillion issue that bad data causes with data-centric AI techniques.", "tokens": [50872, 257, 688, 295, 300, 1045, 18723, 2734, 300, 1578, 1412, 7700, 365, 1412, 12, 45300, 7318, 7512, 13, 51136], "temperature": 0.0, "avg_logprob": -0.09004374447031918, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.0008556845714338124}, {"id": 275, "seek": 152312, "start": 1539.12, "end": 1542.56, "text": " And so there's a lot of hype around it because it means a lot to a lot of people.", "tokens": [51164, 400, 370, 456, 311, 257, 688, 295, 24144, 926, 309, 570, 309, 1355, 257, 688, 281, 257, 688, 295, 561, 13, 51336], "temperature": 0.0, "avg_logprob": -0.09004374447031918, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.0008556845714338124}, {"id": 276, "seek": 152312, "start": 1543.4399999999998, "end": 1551.12, "text": " This is a quick example. I did this internship at Fair in 2016 and I was in Jan's group and", "tokens": [51380, 639, 307, 257, 1702, 1365, 13, 286, 630, 341, 16861, 412, 12157, 294, 6549, 293, 286, 390, 294, 4956, 311, 1594, 293, 51764], "temperature": 0.0, "avg_logprob": -0.09004374447031918, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.0008556845714338124}, {"id": 277, "seek": 155112, "start": 1551.1999999999998, "end": 1557.12, "text": " Jeff Hinton came to visit. If you're not familiar, these two recently won the Nobel Prize of Computer", "tokens": [50368, 7506, 389, 12442, 1361, 281, 3441, 13, 759, 291, 434, 406, 4963, 11, 613, 732, 3938, 1582, 264, 24611, 22604, 295, 22289, 50664], "temperature": 0.0, "avg_logprob": -0.10629152047513711, "compression_ratio": 1.5018726591760299, "no_speech_prob": 0.0003052007523365319}, {"id": 278, "seek": 155112, "start": 1557.12, "end": 1562.56, "text": " Science, which is called the Turing Award. And so I think they're old friends. And Jan has a dataset", "tokens": [50664, 8976, 11, 597, 307, 1219, 264, 314, 1345, 13894, 13, 400, 370, 286, 519, 436, 434, 1331, 1855, 13, 400, 4956, 575, 257, 28872, 50936], "temperature": 0.0, "avg_logprob": -0.10629152047513711, "compression_ratio": 1.5018726591760299, "no_speech_prob": 0.0003052007523365319}, {"id": 279, "seek": 155112, "start": 1562.56, "end": 1569.12, "text": " MNIST. Are folks familiar with MNIST? Okay, cool. Very classical machine learning dataset and we've", "tokens": [50936, 376, 45, 19756, 13, 2014, 4024, 4963, 365, 376, 45, 19756, 30, 1033, 11, 1627, 13, 4372, 13735, 3479, 2539, 28872, 293, 321, 600, 51264], "temperature": 0.0, "avg_logprob": -0.10629152047513711, "compression_ratio": 1.5018726591760299, "no_speech_prob": 0.0003052007523365319}, {"id": 280, "seek": 155112, "start": 1569.12, "end": 1574.8799999999999, "text": " been training models on it for like over 20 years. And people generally assume that it has perfect", "tokens": [51264, 668, 3097, 5245, 322, 309, 337, 411, 670, 945, 924, 13, 400, 561, 5101, 6552, 300, 309, 575, 2176, 51552], "temperature": 0.0, "avg_logprob": -0.10629152047513711, "compression_ratio": 1.5018726591760299, "no_speech_prob": 0.0003052007523365319}, {"id": 281, "seek": 157488, "start": 1574.88, "end": 1582.16, "text": " labels because that's a very common assumption. Not maybe now in 2023, but definitely like when", "tokens": [50364, 16949, 570, 300, 311, 257, 588, 2689, 15302, 13, 1726, 1310, 586, 294, 44377, 11, 457, 2138, 411, 562, 50728], "temperature": 0.0, "avg_logprob": -0.136156577763595, "compression_ratio": 1.6472602739726028, "no_speech_prob": 0.17319639027118683}, {"id": 282, "seek": 157488, "start": 1582.16, "end": 1586.0800000000002, "text": " it first came out. And it's a very high quality dataset. And so Jeff Hinton was presenting at", "tokens": [50728, 309, 700, 1361, 484, 13, 400, 309, 311, 257, 588, 1090, 3125, 28872, 13, 400, 370, 7506, 389, 12442, 390, 15578, 412, 50924], "temperature": 0.0, "avg_logprob": -0.136156577763595, "compression_ratio": 1.6472602739726028, "no_speech_prob": 0.17319639027118683}, {"id": 283, "seek": 157488, "start": 1586.0800000000002, "end": 1590.4, "text": " this time, I think, Capsule Networks. He's very excited about it. And his aha culminating moment", "tokens": [50924, 341, 565, 11, 286, 519, 11, 383, 2382, 2271, 12640, 82, 13, 634, 311, 588, 2919, 466, 309, 13, 400, 702, 47340, 28583, 990, 1623, 51140], "temperature": 0.0, "avg_logprob": -0.136156577763595, "compression_ratio": 1.6472602739726028, "no_speech_prob": 0.17319639027118683}, {"id": 284, "seek": 157488, "start": 1590.4, "end": 1595.7600000000002, "text": " of his talk was that he found the label error in Jan Lacun's dataset. And so he's very excited to", "tokens": [51140, 295, 702, 751, 390, 300, 415, 1352, 264, 7645, 6713, 294, 4956, 40113, 409, 311, 28872, 13, 400, 370, 415, 311, 588, 2919, 281, 51408], "temperature": 0.0, "avg_logprob": -0.136156577763595, "compression_ratio": 1.6472602739726028, "no_speech_prob": 0.17319639027118683}, {"id": 285, "seek": 157488, "start": 1595.7600000000002, "end": 1600.88, "text": " show, hey, this five image is actually labeled a three in your dataset, Jan. And he's like, aha,", "tokens": [51408, 855, 11, 4177, 11, 341, 1732, 3256, 307, 767, 21335, 257, 1045, 294, 428, 28872, 11, 4956, 13, 400, 415, 311, 411, 11, 47340, 11, 51664], "temperature": 0.0, "avg_logprob": -0.136156577763595, "compression_ratio": 1.6472602739726028, "no_speech_prob": 0.17319639027118683}, {"id": 286, "seek": 160088, "start": 1600.88, "end": 1607.3600000000001, "text": " I got you. And so I think that it's worth mentioning that this is where we were in 2016. And now,", "tokens": [50364, 286, 658, 291, 13, 400, 370, 286, 519, 300, 309, 311, 3163, 18315, 300, 341, 307, 689, 321, 645, 294, 6549, 13, 400, 586, 11, 50688], "temperature": 0.0, "avg_logprob": -0.09481735599851146, "compression_ratio": 1.5515873015873016, "no_speech_prob": 0.0020504563581198454}, {"id": 287, "seek": 160088, "start": 1607.3600000000001, "end": 1612.72, "text": " you know, we're only six, seven years later, and we're able to systematically find millions of", "tokens": [50688, 291, 458, 11, 321, 434, 787, 2309, 11, 3407, 924, 1780, 11, 293, 321, 434, 1075, 281, 39531, 915, 6803, 295, 50956], "temperature": 0.0, "avg_logprob": -0.09481735599851146, "compression_ratio": 1.5515873015873016, "no_speech_prob": 0.0020504563581198454}, {"id": 288, "seek": 160088, "start": 1612.72, "end": 1618.8000000000002, "text": " errors in datasets. And that's sort of how far we've come using these data-centric AI approaches.", "tokens": [50956, 13603, 294, 42856, 13, 400, 300, 311, 1333, 295, 577, 1400, 321, 600, 808, 1228, 613, 1412, 12, 45300, 7318, 11587, 13, 51260], "temperature": 0.0, "avg_logprob": -0.09481735599851146, "compression_ratio": 1.5515873015873016, "no_speech_prob": 0.0020504563581198454}, {"id": 289, "seek": 160088, "start": 1620.16, "end": 1627.2800000000002, "text": " Who here is familiar with Dolly and Dolly too? Yeah, it's pretty cool, right? So it generates images", "tokens": [51328, 2102, 510, 307, 4963, 365, 1144, 13020, 293, 1144, 13020, 886, 30, 865, 11, 309, 311, 1238, 1627, 11, 558, 30, 407, 309, 23815, 5267, 51684], "temperature": 0.0, "avg_logprob": -0.09481735599851146, "compression_ratio": 1.5515873015873016, "no_speech_prob": 0.0020504563581198454}, {"id": 290, "seek": 162728, "start": 1627.28, "end": 1632.0, "text": " and they're pretty cool, like you can generate images of pretty much anything you describe.", "tokens": [50364, 293, 436, 434, 1238, 1627, 11, 411, 291, 393, 8460, 5267, 295, 1238, 709, 1340, 291, 6786, 13, 50600], "temperature": 0.0, "avg_logprob": -0.0850639271556883, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.004132225643843412}, {"id": 291, "seek": 162728, "start": 1632.0, "end": 1636.32, "text": " And so if you check out the Dolly demo page, and there's the link here in the slides, if you want", "tokens": [50600, 400, 370, 498, 291, 1520, 484, 264, 1144, 13020, 10723, 3028, 11, 293, 456, 311, 264, 2113, 510, 294, 264, 9788, 11, 498, 291, 528, 50816], "temperature": 0.0, "avg_logprob": -0.0850639271556883, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.004132225643843412}, {"id": 292, "seek": 162728, "start": 1636.32, "end": 1641.44, "text": " to check them after, there's a cool video. And you'll notice in that video, they talk about one of", "tokens": [50816, 281, 1520, 552, 934, 11, 456, 311, 257, 1627, 960, 13, 400, 291, 603, 3449, 294, 300, 960, 11, 436, 751, 466, 472, 295, 51072], "temperature": 0.0, "avg_logprob": -0.0850639271556883, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.004132225643843412}, {"id": 293, "seek": 162728, "start": 1641.44, "end": 1646.16, "text": " their biggest challenges. And so this is just screenshots from the video. And the technology", "tokens": [51072, 641, 3880, 4759, 13, 400, 370, 341, 307, 445, 40661, 490, 264, 960, 13, 400, 264, 2899, 51308], "temperature": 0.0, "avg_logprob": -0.0850639271556883, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.004132225643843412}, {"id": 294, "seek": 162728, "start": 1646.16, "end": 1651.36, "text": " is constantly evolving, but Dolly too has limitations. It's taught with objects that are", "tokens": [51308, 307, 6460, 21085, 11, 457, 1144, 13020, 886, 575, 15705, 13, 467, 311, 5928, 365, 6565, 300, 366, 51568], "temperature": 0.0, "avg_logprob": -0.0850639271556883, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.004132225643843412}, {"id": 295, "seek": 162728, "start": 1651.36, "end": 1656.8, "text": " incorrectly labeled with plain labeled car, for example. And this happens because it's a", "tokens": [51568, 42892, 21335, 365, 11121, 21335, 1032, 11, 337, 1365, 13, 400, 341, 2314, 570, 309, 311, 257, 51840], "temperature": 0.0, "avg_logprob": -0.0850639271556883, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.004132225643843412}, {"id": 296, "seek": 165680, "start": 1656.8, "end": 1662.6399999999999, "text": " massive dataset. So if you don't use data-centric AI approaches, it's very difficult to clean, you", "tokens": [50364, 5994, 28872, 13, 407, 498, 291, 500, 380, 764, 1412, 12, 45300, 7318, 11587, 11, 309, 311, 588, 2252, 281, 2541, 11, 291, 50656], "temperature": 0.0, "avg_logprob": -0.07643491343448036, "compression_ratio": 1.6460481099656357, "no_speech_prob": 0.0008827736601233482}, {"id": 297, "seek": 165680, "start": 1662.6399999999999, "end": 1669.2, "text": " know, whatever, hundreds of millions or more, probably billions of pairs of text and image.", "tokens": [50656, 458, 11, 2035, 11, 6779, 295, 6803, 420, 544, 11, 1391, 17375, 295, 15494, 295, 2487, 293, 3256, 13, 50984], "temperature": 0.0, "avg_logprob": -0.07643491343448036, "compression_ratio": 1.6460481099656357, "no_speech_prob": 0.0008827736601233482}, {"id": 298, "seek": 165680, "start": 1669.2, "end": 1674.96, "text": " And so what they notice is that a user might actually try to generate a car, but Dolly will", "tokens": [50984, 400, 370, 437, 436, 3449, 307, 300, 257, 4195, 1062, 767, 853, 281, 8460, 257, 1032, 11, 457, 1144, 13020, 486, 51272], "temperature": 0.0, "avg_logprob": -0.07643491343448036, "compression_ratio": 1.6460481099656357, "no_speech_prob": 0.0008827736601233482}, {"id": 299, "seek": 165680, "start": 1674.96, "end": 1681.2, "text": " actually create a plane, because it's seen wrongly labeled data. And so this is very problematic", "tokens": [51272, 767, 1884, 257, 5720, 11, 570, 309, 311, 1612, 2085, 356, 21335, 1412, 13, 400, 370, 341, 307, 588, 19011, 51584], "temperature": 0.0, "avg_logprob": -0.07643491343448036, "compression_ratio": 1.6460481099656357, "no_speech_prob": 0.0008827736601233482}, {"id": 300, "seek": 165680, "start": 1681.2, "end": 1685.04, "text": " for something that's deployed in the real world. Another example from that video is they talk about", "tokens": [51584, 337, 746, 300, 311, 17826, 294, 264, 957, 1002, 13, 3996, 1365, 490, 300, 960, 307, 436, 751, 466, 51776], "temperature": 0.0, "avg_logprob": -0.07643491343448036, "compression_ratio": 1.6460481099656357, "no_speech_prob": 0.0008827736601233482}, {"id": 301, "seek": 168504, "start": 1685.04, "end": 1690.96, "text": " generating these baboons, but they emphasize that you can only do this correctly if you have", "tokens": [50364, 17746, 613, 7564, 16450, 11, 457, 436, 16078, 300, 291, 393, 787, 360, 341, 8944, 498, 291, 362, 50660], "temperature": 0.0, "avg_logprob": -0.0999067851475307, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.002322159940376878}, {"id": 302, "seek": 168504, "start": 1690.96, "end": 1695.92, "text": " accurate labels. And if you didn't, you'll totally, you can get the wrong thing. And so the key takeaway", "tokens": [50660, 8559, 16949, 13, 400, 498, 291, 994, 380, 11, 291, 603, 3879, 11, 291, 393, 483, 264, 2085, 551, 13, 400, 370, 264, 2141, 30681, 50908], "temperature": 0.0, "avg_logprob": -0.0999067851475307, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.002322159940376878}, {"id": 303, "seek": 168504, "start": 1695.92, "end": 1700.72, "text": " here is that this is a real world technology, lots of people are using today at scale, but the", "tokens": [50908, 510, 307, 300, 341, 307, 257, 957, 1002, 2899, 11, 3195, 295, 561, 366, 1228, 965, 412, 4373, 11, 457, 264, 51148], "temperature": 0.0, "avg_logprob": -0.0999067851475307, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.002322159940376878}, {"id": 304, "seek": 168504, "start": 1700.72, "end": 1707.28, "text": " reliability of that model really does depend on the data quality. Another big example is familiar", "tokens": [51148, 24550, 295, 300, 2316, 534, 775, 5672, 322, 264, 1412, 3125, 13, 3996, 955, 1365, 307, 4963, 51476], "temperature": 0.0, "avg_logprob": -0.0999067851475307, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.002322159940376878}, {"id": 305, "seek": 170728, "start": 1707.36, "end": 1717.76, "text": " with chat GPT. Does anybody know, yeah, does anybody know sort of why or like what was the", "tokens": [50368, 365, 5081, 26039, 51, 13, 4402, 4472, 458, 11, 1338, 11, 775, 4472, 458, 1333, 295, 983, 420, 411, 437, 390, 264, 50888], "temperature": 0.0, "avg_logprob": -0.14391964084499484, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.31705939769744873}, {"id": 306, "seek": 170728, "start": 1717.76, "end": 1724.72, "text": " big innovation from GPT three, which obviously had a huge hype around it to chat GPT, which has", "tokens": [50888, 955, 8504, 490, 26039, 51, 1045, 11, 597, 2745, 632, 257, 2603, 24144, 926, 309, 281, 5081, 26039, 51, 11, 597, 575, 51236], "temperature": 0.0, "avg_logprob": -0.14391964084499484, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.31705939769744873}, {"id": 307, "seek": 170728, "start": 1724.72, "end": 1729.28, "text": " even more hype around it? What was sort of one of the big things they did between the two?", "tokens": [51236, 754, 544, 24144, 926, 309, 30, 708, 390, 1333, 295, 472, 295, 264, 955, 721, 436, 630, 1296, 264, 732, 30, 51464], "temperature": 0.0, "avg_logprob": -0.14391964084499484, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.31705939769744873}, {"id": 308, "seek": 170728, "start": 1733.92, "end": 1737.04, "text": " Yeah, totally. And do you know what they were doing with their reinforcement learning?", "tokens": [51696, 865, 11, 3879, 13, 400, 360, 291, 458, 437, 436, 645, 884, 365, 641, 29280, 2539, 30, 51852], "temperature": 0.0, "avg_logprob": -0.14391964084499484, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.31705939769744873}, {"id": 309, "seek": 173728, "start": 1737.68, "end": 1744.32, "text": " They like inviting some users to talk about these chat GPT and actually some of the work.", "tokens": [50384, 814, 411, 18202, 512, 5022, 281, 751, 466, 613, 5081, 26039, 51, 293, 767, 512, 295, 264, 589, 13, 50716], "temperature": 0.0, "avg_logprob": -0.25976642147525325, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.0004107070271857083}, {"id": 310, "seek": 173728, "start": 1745.2, "end": 1752.72, "text": " Yeah, totally. Yeah. What was happening there was they, they had a lot of bad outputs, you know,", "tokens": [50760, 865, 11, 3879, 13, 865, 13, 708, 390, 2737, 456, 390, 436, 11, 436, 632, 257, 688, 295, 1578, 23930, 11, 291, 458, 11, 51136], "temperature": 0.0, "avg_logprob": -0.25976642147525325, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.0004107070271857083}, {"id": 311, "seek": 173728, "start": 1752.72, "end": 1758.24, "text": " like chat GPT three was saying things that were like super biased, or like inappropriate,", "tokens": [51136, 411, 5081, 26039, 51, 1045, 390, 1566, 721, 300, 645, 411, 1687, 28035, 11, 420, 411, 26723, 11, 51412], "temperature": 0.0, "avg_logprob": -0.25976642147525325, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.0004107070271857083}, {"id": 312, "seek": 173728, "start": 1759.12, "end": 1764.8, "text": " not even true, just wrong facts. And these outputs were tied to data it was trained on.", "tokens": [51456, 406, 754, 2074, 11, 445, 2085, 9130, 13, 400, 613, 23930, 645, 9601, 281, 1412, 309, 390, 8895, 322, 13, 51740], "temperature": 0.0, "avg_logprob": -0.25976642147525325, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.0004107070271857083}, {"id": 313, "seek": 176480, "start": 1764.8, "end": 1769.44, "text": " And to parameters in the model that were learned from that data. And so what they did is they did", "tokens": [50364, 400, 281, 9834, 294, 264, 2316, 300, 645, 3264, 490, 300, 1412, 13, 400, 370, 437, 436, 630, 307, 436, 630, 50596], "temperature": 0.0, "avg_logprob": -0.06255606992529072, "compression_ratio": 1.916, "no_speech_prob": 0.00030528768547810614}, {"id": 314, "seek": 176480, "start": 1769.44, "end": 1773.68, "text": " in a reinforcement setting, which means they talked to people, they use that information", "tokens": [50596, 294, 257, 29280, 3287, 11, 597, 1355, 436, 2825, 281, 561, 11, 436, 764, 300, 1589, 50808], "temperature": 0.0, "avg_logprob": -0.06255606992529072, "compression_ratio": 1.916, "no_speech_prob": 0.00030528768547810614}, {"id": 315, "seek": 176480, "start": 1773.68, "end": 1778.8799999999999, "text": " to then update the model, then the model sort of explores with new outputs, then people see those.", "tokens": [50808, 281, 550, 5623, 264, 2316, 11, 550, 264, 2316, 1333, 295, 45473, 365, 777, 23930, 11, 550, 561, 536, 729, 13, 51068], "temperature": 0.0, "avg_logprob": -0.06255606992529072, "compression_ratio": 1.916, "no_speech_prob": 0.00030528768547810614}, {"id": 316, "seek": 176480, "start": 1778.8799999999999, "end": 1783.52, "text": " But what they were doing is they were having people actually rank them in terms of the quality", "tokens": [51068, 583, 437, 436, 645, 884, 307, 436, 645, 1419, 561, 767, 6181, 552, 294, 2115, 295, 264, 3125, 51300], "temperature": 0.0, "avg_logprob": -0.06255606992529072, "compression_ratio": 1.916, "no_speech_prob": 0.00030528768547810614}, {"id": 317, "seek": 176480, "start": 1783.52, "end": 1788.8799999999999, "text": " of the prediction, right? And so they were ranking the quality of this data and then using that to", "tokens": [51300, 295, 264, 17630, 11, 558, 30, 400, 370, 436, 645, 17833, 264, 3125, 295, 341, 1412, 293, 550, 1228, 300, 281, 51568], "temperature": 0.0, "avg_logprob": -0.06255606992529072, "compression_ratio": 1.916, "no_speech_prob": 0.00030528768547810614}, {"id": 318, "seek": 178888, "start": 1788.88, "end": 1795.0400000000002, "text": " update the model so that it would have improved less bias and better outputs. And so that was", "tokens": [50364, 5623, 264, 2316, 370, 300, 309, 576, 362, 9689, 1570, 12577, 293, 1101, 23930, 13, 400, 370, 300, 390, 50672], "temperature": 0.0, "avg_logprob": -0.07696019988699057, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.0032726209610700607}, {"id": 319, "seek": 178888, "start": 1795.0400000000002, "end": 1800.72, "text": " the key idea of chat GPT was actually to deal with a data quality issue. And if you've tried both,", "tokens": [50672, 264, 2141, 1558, 295, 5081, 26039, 51, 390, 767, 281, 2028, 365, 257, 1412, 3125, 2734, 13, 400, 498, 291, 600, 3031, 1293, 11, 50956], "temperature": 0.0, "avg_logprob": -0.07696019988699057, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.0032726209610700607}, {"id": 320, "seek": 178888, "start": 1800.72, "end": 1805.8400000000001, "text": " you can see that there is a pretty big performance boost. The downside is that they had to do this", "tokens": [50956, 291, 393, 536, 300, 456, 307, 257, 1238, 955, 3389, 9194, 13, 440, 25060, 307, 300, 436, 632, 281, 360, 341, 51212], "temperature": 0.0, "avg_logprob": -0.07696019988699057, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.0032726209610700607}, {"id": 321, "seek": 178888, "start": 1805.8400000000001, "end": 1813.0400000000002, "text": " with a lot of manual work. And so we went to work on ways to automate that. You guys are probably", "tokens": [51212, 365, 257, 688, 295, 9688, 589, 13, 400, 370, 321, 1437, 281, 589, 322, 2098, 281, 31605, 300, 13, 509, 1074, 366, 1391, 51572], "temperature": 0.0, "avg_logprob": -0.07696019988699057, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.0032726209610700607}, {"id": 322, "seek": 181304, "start": 1813.04, "end": 1820.08, "text": " familiar with Tesla. So this is Tesla's data engine. This is from a talk by Andre Carpathi,", "tokens": [50364, 4963, 365, 13666, 13, 407, 341, 307, 13666, 311, 1412, 2848, 13, 639, 307, 490, 257, 751, 538, 20667, 2741, 31852, 72, 11, 50716], "temperature": 0.0, "avg_logprob": -0.11985897471886554, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.14989864826202393}, {"id": 323, "seek": 181304, "start": 1820.08, "end": 1826.1599999999999, "text": " who is formerly the Tesla director of AI. And this is their data engine. And we'll just start in", "tokens": [50716, 567, 307, 34777, 264, 13666, 5391, 295, 7318, 13, 400, 341, 307, 641, 1412, 2848, 13, 400, 321, 603, 445, 722, 294, 51020], "temperature": 0.0, "avg_logprob": -0.11985897471886554, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.14989864826202393}, {"id": 324, "seek": 181304, "start": 1826.1599999999999, "end": 1832.72, "text": " the top left, like you, the way they're training the, you know, the self-driving Tesla model is,", "tokens": [51020, 264, 1192, 1411, 11, 411, 291, 11, 264, 636, 436, 434, 3097, 264, 11, 291, 458, 11, 264, 2698, 12, 47094, 13666, 2316, 307, 11, 51348], "temperature": 0.0, "avg_logprob": -0.11985897471886554, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.14989864826202393}, {"id": 325, "seek": 181304, "start": 1832.72, "end": 1836.08, "text": " you know, you have some data source. And then you'll notice some problem, which is like, hey,", "tokens": [51348, 291, 458, 11, 291, 362, 512, 1412, 4009, 13, 400, 550, 291, 603, 3449, 512, 1154, 11, 597, 307, 411, 11, 4177, 11, 51516], "temperature": 0.0, "avg_logprob": -0.11985897471886554, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.14989864826202393}, {"id": 326, "seek": 181304, "start": 1836.8799999999999, "end": 1842.6399999999999, "text": " we're in a tunnel. And we don't have a lot of, you know, tunnel data. So the car is like", "tokens": [51556, 321, 434, 294, 257, 13186, 13, 400, 321, 500, 380, 362, 257, 688, 295, 11, 291, 458, 11, 13186, 1412, 13, 407, 264, 1032, 307, 411, 51844], "temperature": 0.0, "avg_logprob": -0.11985897471886554, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.14989864826202393}, {"id": 327, "seek": 184264, "start": 1842.64, "end": 1847.2, "text": " doing weird things, right? And so then what they would do is they would collect a bunch more data", "tokens": [50364, 884, 3657, 721, 11, 558, 30, 400, 370, 550, 437, 436, 576, 360, 307, 436, 576, 2500, 257, 3840, 544, 1412, 50592], "temperature": 0.0, "avg_logprob": -0.06709439989546655, "compression_ratio": 1.9431438127090301, "no_speech_prob": 0.0023962347768247128}, {"id": 328, "seek": 184264, "start": 1847.2, "end": 1852.8000000000002, "text": " in tunnels and then update their training data and label them and then redeploy and go in the", "tokens": [50592, 294, 30804, 293, 550, 5623, 641, 3097, 1412, 293, 7645, 552, 293, 550, 14328, 2384, 293, 352, 294, 264, 50872], "temperature": 0.0, "avg_logprob": -0.06709439989546655, "compression_ratio": 1.9431438127090301, "no_speech_prob": 0.0023962347768247128}, {"id": 329, "seek": 184264, "start": 1852.8000000000002, "end": 1857.3600000000001, "text": " world and then see what breaks then. And this is a very, you know, difficult iterative process", "tokens": [50872, 1002, 293, 550, 536, 437, 9857, 550, 13, 400, 341, 307, 257, 588, 11, 291, 458, 11, 2252, 17138, 1166, 1399, 51100], "temperature": 0.0, "avg_logprob": -0.06709439989546655, "compression_ratio": 1.9431438127090301, "no_speech_prob": 0.0023962347768247128}, {"id": 330, "seek": 184264, "start": 1857.3600000000001, "end": 1861.5200000000002, "text": " because you have to send the car out and then see where things break and then collect a bunch more", "tokens": [51100, 570, 291, 362, 281, 2845, 264, 1032, 484, 293, 550, 536, 689, 721, 1821, 293, 550, 2500, 257, 3840, 544, 51308], "temperature": 0.0, "avg_logprob": -0.06709439989546655, "compression_ratio": 1.9431438127090301, "no_speech_prob": 0.0023962347768247128}, {"id": 331, "seek": 184264, "start": 1861.5200000000002, "end": 1866.16, "text": " data. Or if you had a way to automate, okay, this is where my data is missing. This is stuff that's", "tokens": [51308, 1412, 13, 1610, 498, 291, 632, 257, 636, 281, 31605, 11, 1392, 11, 341, 307, 689, 452, 1412, 307, 5361, 13, 639, 307, 1507, 300, 311, 51540], "temperature": 0.0, "avg_logprob": -0.06709439989546655, "compression_ratio": 1.9431438127090301, "no_speech_prob": 0.0023962347768247128}, {"id": 332, "seek": 184264, "start": 1866.16, "end": 1869.92, "text": " out of distribution. This is where we have a bunch of label errors. And you're able to automate", "tokens": [51540, 484, 295, 7316, 13, 639, 307, 689, 321, 362, 257, 3840, 295, 7645, 13603, 13, 400, 291, 434, 1075, 281, 31605, 51728], "temperature": 0.0, "avg_logprob": -0.06709439989546655, "compression_ratio": 1.9431438127090301, "no_speech_prob": 0.0023962347768247128}, {"id": 333, "seek": 186992, "start": 1869.92, "end": 1873.44, "text": " that process. They could have reduced those cycles and gotten the car out a lot faster,", "tokens": [50364, 300, 1399, 13, 814, 727, 362, 9212, 729, 17796, 293, 5768, 264, 1032, 484, 257, 688, 4663, 11, 50540], "temperature": 0.0, "avg_logprob": -0.08317902992511618, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.002050283132120967}, {"id": 334, "seek": 186992, "start": 1874.24, "end": 1878.96, "text": " at least the AI part of the car. And so this was a big pain that, that Andre mentioned.", "tokens": [50580, 412, 1935, 264, 7318, 644, 295, 264, 1032, 13, 400, 370, 341, 390, 257, 955, 1822, 300, 11, 300, 20667, 2835, 13, 50816], "temperature": 0.0, "avg_logprob": -0.08317902992511618, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.002050283132120967}, {"id": 335, "seek": 186992, "start": 1880.0, "end": 1886.48, "text": " Another really, this is a fantastic example that Jonas shared with me. These are all examples of", "tokens": [50868, 3996, 534, 11, 341, 307, 257, 5456, 1365, 300, 34630, 5507, 365, 385, 13, 1981, 366, 439, 5110, 295, 51192], "temperature": 0.0, "avg_logprob": -0.08317902992511618, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.002050283132120967}, {"id": 336, "seek": 186992, "start": 1886.48, "end": 1891.92, "text": " traffic lights. You know, so imagine that like Elon Musk comes to you and he says, you know,", "tokens": [51192, 6419, 5811, 13, 509, 458, 11, 370, 3811, 300, 411, 28498, 26019, 1487, 281, 291, 293, 415, 1619, 11, 291, 458, 11, 51464], "temperature": 0.0, "avg_logprob": -0.08317902992511618, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.002050283132120967}, {"id": 337, "seek": 186992, "start": 1891.92, "end": 1898.5600000000002, "text": " Andre, I need you to get this car to navigate any street in the world. And then, you know,", "tokens": [51464, 20667, 11, 286, 643, 291, 281, 483, 341, 1032, 281, 12350, 604, 4838, 294, 264, 1002, 13, 400, 550, 11, 291, 458, 11, 51796], "temperature": 0.0, "avg_logprob": -0.08317902992511618, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.002050283132120967}, {"id": 338, "seek": 189856, "start": 1898.56, "end": 1901.84, "text": " you're like, oh, that's cool. Okay. So like it needs to stop at traffic lights. Like that seems", "tokens": [50364, 291, 434, 411, 11, 1954, 11, 300, 311, 1627, 13, 1033, 13, 407, 411, 309, 2203, 281, 1590, 412, 6419, 5811, 13, 1743, 300, 2544, 50528], "temperature": 0.0, "avg_logprob": -0.06578481447446596, "compression_ratio": 1.864951768488746, "no_speech_prob": 0.0019261876586824656}, {"id": 339, "seek": 189856, "start": 1901.84, "end": 1906.1599999999999, "text": " like a pretty simple problem. And then you go out in the real world and like traffic lights are", "tokens": [50528, 411, 257, 1238, 2199, 1154, 13, 400, 550, 291, 352, 484, 294, 264, 957, 1002, 293, 411, 6419, 5811, 366, 50744], "temperature": 0.0, "avg_logprob": -0.06578481447446596, "compression_ratio": 1.864951768488746, "no_speech_prob": 0.0019261876586824656}, {"id": 340, "seek": 189856, "start": 1906.1599999999999, "end": 1910.1599999999999, "text": " not a simple problem. They're really complicated and they're really messy. And this is actually", "tokens": [50744, 406, 257, 2199, 1154, 13, 814, 434, 534, 6179, 293, 436, 434, 534, 16191, 13, 400, 341, 307, 767, 50944], "temperature": 0.0, "avg_logprob": -0.06578481447446596, "compression_ratio": 1.864951768488746, "no_speech_prob": 0.0019261876586824656}, {"id": 341, "seek": 189856, "start": 1910.1599999999999, "end": 1915.84, "text": " like a total nightmare if you had to do this. And so how do you find sort of systematic ways to", "tokens": [50944, 411, 257, 3217, 18724, 498, 291, 632, 281, 360, 341, 13, 400, 370, 577, 360, 291, 915, 1333, 295, 27249, 2098, 281, 51228], "temperature": 0.0, "avg_logprob": -0.06578481447446596, "compression_ratio": 1.864951768488746, "no_speech_prob": 0.0019261876586824656}, {"id": 342, "seek": 189856, "start": 1915.84, "end": 1920.56, "text": " group data together and train in a way that's robust and reliable? Like real world data is super", "tokens": [51228, 1594, 1412, 1214, 293, 3847, 294, 257, 636, 300, 311, 13956, 293, 12924, 30, 1743, 957, 1002, 1412, 307, 1687, 51464], "temperature": 0.0, "avg_logprob": -0.06578481447446596, "compression_ratio": 1.864951768488746, "no_speech_prob": 0.0019261876586824656}, {"id": 343, "seek": 189856, "start": 1920.56, "end": 1926.56, "text": " messy and complicated. And so Andre's big takeaway was that, you know, he shares this juxtaposition", "tokens": [51464, 16191, 293, 6179, 13, 400, 370, 20667, 311, 955, 30681, 390, 300, 11, 291, 458, 11, 415, 12182, 341, 3649, 734, 569, 5830, 51764], "temperature": 0.0, "avg_logprob": -0.06578481447446596, "compression_ratio": 1.864951768488746, "no_speech_prob": 0.0019261876586824656}, {"id": 344, "seek": 192656, "start": 1926.56, "end": 1932.3999999999999, "text": " of the amount of sleep, you know, he lost over in his PhD. And it was like data sets is this tiny", "tokens": [50364, 295, 264, 2372, 295, 2817, 11, 291, 458, 11, 415, 2731, 670, 294, 702, 14476, 13, 400, 309, 390, 411, 1412, 6352, 307, 341, 5870, 50656], "temperature": 0.0, "avg_logprob": -0.0787396647713401, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.0019874325953423977}, {"id": 345, "seek": 192656, "start": 1932.3999999999999, "end": 1937.28, "text": " sliver, but definitely spent a lot of time on models and algorithms. And then he goes and he's", "tokens": [50656, 1061, 1837, 11, 457, 2138, 4418, 257, 688, 295, 565, 322, 5245, 293, 14642, 13, 400, 550, 415, 1709, 293, 415, 311, 50900], "temperature": 0.0, "avg_logprob": -0.0787396647713401, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.0019874325953423977}, {"id": 346, "seek": 192656, "start": 1937.28, "end": 1944.0, "text": " leading, you know, the AI model at Tesla. And it's like, it's all data, you know, it's a big shock", "tokens": [50900, 5775, 11, 291, 458, 11, 264, 7318, 2316, 412, 13666, 13, 400, 309, 311, 411, 11, 309, 311, 439, 1412, 11, 291, 458, 11, 309, 311, 257, 955, 5588, 51236], "temperature": 0.0, "avg_logprob": -0.0787396647713401, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.0019874325953423977}, {"id": 347, "seek": 192656, "start": 1944.0, "end": 1948.3999999999999, "text": " when you make this shift. And so that's why we really want to focus on ways we can improve that.", "tokens": [51236, 562, 291, 652, 341, 5513, 13, 400, 370, 300, 311, 983, 321, 534, 528, 281, 1879, 322, 2098, 321, 393, 3470, 300, 13, 51456], "temperature": 0.0, "avg_logprob": -0.0787396647713401, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.0019874325953423977}, {"id": 348, "seek": 192656, "start": 1949.44, "end": 1955.9199999999998, "text": " A very common use case is when you're trying to train a model with noisy labels. Okay. So", "tokens": [51508, 316, 588, 2689, 764, 1389, 307, 562, 291, 434, 1382, 281, 3847, 257, 2316, 365, 24518, 16949, 13, 1033, 13, 407, 51832], "temperature": 0.0, "avg_logprob": -0.0787396647713401, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.0019874325953423977}, {"id": 349, "seek": 195592, "start": 1956.0, "end": 1963.04, "text": " this is like the classical scenario of your, you have the dogs and cats, but now say 30% of", "tokens": [50368, 341, 307, 411, 264, 13735, 9005, 295, 428, 11, 291, 362, 264, 7197, 293, 11111, 11, 457, 586, 584, 2217, 4, 295, 50720], "temperature": 0.0, "avg_logprob": -0.08850616614023844, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0008039532694965601}, {"id": 350, "seek": 195592, "start": 1963.04, "end": 1969.3600000000001, "text": " your cats are labeled dog. Okay. And maybe 20% of the dogs are labeled cat. So how do you get a model", "tokens": [50720, 428, 11111, 366, 21335, 3000, 13, 1033, 13, 400, 1310, 945, 4, 295, 264, 7197, 366, 21335, 3857, 13, 407, 577, 360, 291, 483, 257, 2316, 51036], "temperature": 0.0, "avg_logprob": -0.08850616614023844, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0008039532694965601}, {"id": 351, "seek": 195592, "start": 1969.3600000000001, "end": 1974.3200000000002, "text": " that does as well or close to as well as if you had perfectly labeled data? And we'll go into that", "tokens": [51036, 300, 775, 382, 731, 420, 1998, 281, 382, 731, 382, 498, 291, 632, 6239, 21335, 1412, 30, 400, 321, 603, 352, 666, 300, 51284], "temperature": 0.0, "avg_logprob": -0.08850616614023844, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0008039532694965601}, {"id": 352, "seek": 195592, "start": 1974.3200000000002, "end": 1980.3200000000002, "text": " more in the next lecture. But I want to just motivate that I looked at a bunch of model", "tokens": [51284, 544, 294, 264, 958, 7991, 13, 583, 286, 528, 281, 445, 28497, 300, 286, 2956, 412, 257, 3840, 295, 2316, 51584], "temperature": 0.0, "avg_logprob": -0.08850616614023844, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0008039532694965601}, {"id": 353, "seek": 195592, "start": 1980.3200000000002, "end": 1985.1200000000001, "text": " centric methods and data centric methods over the last five years out of top institutions", "tokens": [51584, 1489, 1341, 7150, 293, 1412, 1489, 1341, 7150, 670, 264, 1036, 1732, 924, 484, 295, 1192, 8142, 51824], "temperature": 0.0, "avg_logprob": -0.08850616614023844, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0008039532694965601}, {"id": 354, "seek": 198512, "start": 1985.12, "end": 1990.8, "text": " like Google and Facebook and so forth. And we benchmark them. And it turns out, and there's a", "tokens": [50364, 411, 3329, 293, 4384, 293, 370, 5220, 13, 400, 321, 18927, 552, 13, 400, 309, 4523, 484, 11, 293, 456, 311, 257, 50648], "temperature": 0.0, "avg_logprob": -0.07558127566500827, "compression_ratio": 1.83203125, "no_speech_prob": 0.0008557225810363889}, {"id": 355, "seek": 198512, "start": 1990.8, "end": 1995.28, "text": " lot on this slide, but there's really one key takeaway that the data centric AI methods all", "tokens": [50648, 688, 322, 341, 4137, 11, 457, 456, 311, 534, 472, 2141, 30681, 300, 264, 1412, 1489, 1341, 7318, 7150, 439, 50872], "temperature": 0.0, "avg_logprob": -0.07558127566500827, "compression_ratio": 1.83203125, "no_speech_prob": 0.0008557225810363889}, {"id": 356, "seek": 198512, "start": 1995.28, "end": 2000.0, "text": " outperformed the model centric methods for this particular task, you know, on this particular", "tokens": [50872, 484, 610, 22892, 264, 2316, 1489, 1341, 7150, 337, 341, 1729, 5633, 11, 291, 458, 11, 322, 341, 1729, 51108], "temperature": 0.0, "avg_logprob": -0.07558127566500827, "compression_ratio": 1.83203125, "no_speech_prob": 0.0008557225810363889}, {"id": 357, "seek": 198512, "start": 2000.0, "end": 2005.1999999999998, "text": " data set. And this was pretty revealing and compelling that there's something here to data", "tokens": [51108, 1412, 992, 13, 400, 341, 390, 1238, 23983, 293, 20050, 300, 456, 311, 746, 510, 281, 1412, 51368], "temperature": 0.0, "avg_logprob": -0.07558127566500827, "compression_ratio": 1.83203125, "no_speech_prob": 0.0008557225810363889}, {"id": 358, "seek": 198512, "start": 2005.1999999999998, "end": 2010.4799999999998, "text": " centric AI approaches. And to be very clear, what these models are doing is they are modifying the", "tokens": [51368, 1489, 1341, 7318, 11587, 13, 400, 281, 312, 588, 1850, 11, 437, 613, 5245, 366, 884, 307, 436, 366, 42626, 264, 51632], "temperature": 0.0, "avg_logprob": -0.07558127566500827, "compression_ratio": 1.83203125, "no_speech_prob": 0.0008557225810363889}, {"id": 359, "seek": 201048, "start": 2010.48, "end": 2015.76, "text": " loss function or modifying the model so that they sort of don't train as much on what they think is", "tokens": [50364, 4470, 2445, 420, 42626, 264, 2316, 370, 300, 436, 1333, 295, 500, 380, 3847, 382, 709, 322, 437, 436, 519, 307, 50628], "temperature": 0.0, "avg_logprob": -0.07983640653897175, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.001500819344073534}, {"id": 360, "seek": 201048, "start": 2015.76, "end": 2021.3600000000001, "text": " bad data, but within the context of the modeling. And what these methods are doing is they're", "tokens": [50628, 1578, 1412, 11, 457, 1951, 264, 4319, 295, 264, 15983, 13, 400, 437, 613, 7150, 366, 884, 307, 436, 434, 50908], "temperature": 0.0, "avg_logprob": -0.07983640653897175, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.001500819344073534}, {"id": 361, "seek": 201048, "start": 2021.3600000000001, "end": 2026.0, "text": " actually modifying the data set. They're either removing bad data or they're generating more data", "tokens": [50908, 767, 42626, 264, 1412, 992, 13, 814, 434, 2139, 12720, 1578, 1412, 420, 436, 434, 17746, 544, 1412, 51140], "temperature": 0.0, "avg_logprob": -0.07983640653897175, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.001500819344073534}, {"id": 362, "seek": 201048, "start": 2026.0, "end": 2030.88, "text": " that sort of makes the error go away, but somehow they're actually changing the data set. And this", "tokens": [51140, 300, 1333, 295, 1669, 264, 6713, 352, 1314, 11, 457, 6063, 436, 434, 767, 4473, 264, 1412, 992, 13, 400, 341, 51384], "temperature": 0.0, "avg_logprob": -0.07983640653897175, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.001500819344073534}, {"id": 363, "seek": 201048, "start": 2030.88, "end": 2035.52, "text": " is just how things stack up. And you'll see the data centric methods outperform model centric", "tokens": [51384, 307, 445, 577, 721, 8630, 493, 13, 400, 291, 603, 536, 264, 1412, 1489, 1341, 7150, 484, 26765, 2316, 1489, 1341, 51616], "temperature": 0.0, "avg_logprob": -0.07983640653897175, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.001500819344073534}, {"id": 364, "seek": 203552, "start": 2035.52, "end": 2040.56, "text": " methods in this task. And this is a very common task that's of interest to the field. So it's", "tokens": [50364, 7150, 294, 341, 5633, 13, 400, 341, 307, 257, 588, 2689, 5633, 300, 311, 295, 1179, 281, 264, 2519, 13, 407, 309, 311, 50616], "temperature": 0.0, "avg_logprob": -0.07631671813226515, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.004329980351030827}, {"id": 365, "seek": 203552, "start": 2040.56, "end": 2046.72, "text": " cool. It's cool to see that this stuff is working and we're getting some benefits from it. So a", "tokens": [50616, 1627, 13, 467, 311, 1627, 281, 536, 300, 341, 1507, 307, 1364, 293, 321, 434, 1242, 512, 5311, 490, 309, 13, 407, 257, 50924], "temperature": 0.0, "avg_logprob": -0.07631671813226515, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.004329980351030827}, {"id": 366, "seek": 203552, "start": 2046.72, "end": 2054.08, "text": " sort of culminating thought is, you know, we were talking a lot about ways that we want to automate,", "tokens": [50924, 1333, 295, 28583, 990, 1194, 307, 11, 291, 458, 11, 321, 645, 1417, 257, 688, 466, 2098, 300, 321, 528, 281, 31605, 11, 51292], "temperature": 0.0, "avg_logprob": -0.07631671813226515, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.004329980351030827}, {"id": 367, "seek": 203552, "start": 2054.08, "end": 2060.56, "text": " but what did we do before? So obviously we've had to improve data sets in industry and like outside", "tokens": [51292, 457, 437, 630, 321, 360, 949, 30, 407, 2745, 321, 600, 632, 281, 3470, 1412, 6352, 294, 3518, 293, 411, 2380, 51616], "temperature": 0.0, "avg_logprob": -0.07631671813226515, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.004329980351030827}, {"id": 368, "seek": 203552, "start": 2060.56, "end": 2064.32, "text": " of academia in the past. It's like, how did we do it before there was data centric AI?", "tokens": [51616, 295, 28937, 294, 264, 1791, 13, 467, 311, 411, 11, 577, 630, 321, 360, 309, 949, 456, 390, 1412, 1489, 1341, 7318, 30, 51804], "temperature": 0.0, "avg_logprob": -0.07631671813226515, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.004329980351030827}, {"id": 369, "seek": 206552, "start": 2065.84, "end": 2072.48, "text": " And so we mostly relied on human powered solutions. For example, we would just spend more money", "tokens": [50380, 400, 370, 321, 5240, 35463, 322, 1952, 17786, 6547, 13, 1171, 1365, 11, 321, 576, 445, 3496, 544, 1460, 50712], "temperature": 0.0, "avg_logprob": -0.08519197722612801, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0002033923810813576}, {"id": 370, "seek": 206552, "start": 2072.48, "end": 2078.0, "text": " for higher quality data. It's like you literally just pay for more labels or you would pay for more", "tokens": [50712, 337, 2946, 3125, 1412, 13, 467, 311, 411, 291, 3736, 445, 1689, 337, 544, 16949, 420, 291, 576, 1689, 337, 544, 50988], "temperature": 0.0, "avg_logprob": -0.08519197722612801, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0002033923810813576}, {"id": 371, "seek": 206552, "start": 2078.0, "end": 2085.28, "text": " data. And that was a very classical way to improve a model. Also building custom tools. So like you", "tokens": [50988, 1412, 13, 400, 300, 390, 257, 588, 13735, 636, 281, 3470, 257, 2316, 13, 2743, 2390, 2375, 3873, 13, 407, 411, 291, 51352], "temperature": 0.0, "avg_logprob": -0.08519197722612801, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0002033923810813576}, {"id": 372, "seek": 206552, "start": 2085.28, "end": 2091.6, "text": " saw at Tesla, they have this whole sort of data platform. And this is a lot, right? This is for", "tokens": [51352, 1866, 412, 13666, 11, 436, 362, 341, 1379, 1333, 295, 1412, 3663, 13, 400, 341, 307, 257, 688, 11, 558, 30, 639, 307, 337, 51668], "temperature": 0.0, "avg_logprob": -0.08519197722612801, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0002033923810813576}, {"id": 373, "seek": 206552, "start": 2091.6, "end": 2095.28, "text": " one specific problem, the very important problem, but they had to build a lot of custom tech around", "tokens": [51668, 472, 2685, 1154, 11, 264, 588, 1021, 1154, 11, 457, 436, 632, 281, 1322, 257, 688, 295, 2375, 7553, 926, 51852], "temperature": 0.0, "avg_logprob": -0.08519197722612801, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0002033923810813576}, {"id": 374, "seek": 209528, "start": 2095.52, "end": 2101.36, "text": " it. Another common thing is just fixing data inside a Jupyter notebook. So just a quick show", "tokens": [50376, 309, 13, 3996, 2689, 551, 307, 445, 19442, 1412, 1854, 257, 22125, 88, 391, 21060, 13, 407, 445, 257, 1702, 855, 50668], "temperature": 0.0, "avg_logprob": -0.10037296862641643, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.00023047946160659194}, {"id": 375, "seek": 209528, "start": 2101.36, "end": 2106.6400000000003, "text": " of hands like how many people have used Jupyter notebooks. Okay, that's really good because", "tokens": [50668, 295, 2377, 411, 577, 867, 561, 362, 1143, 22125, 88, 391, 43782, 13, 1033, 11, 300, 311, 534, 665, 570, 50932], "temperature": 0.0, "avg_logprob": -0.10037296862641643, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.00023047946160659194}, {"id": 376, "seek": 209528, "start": 2106.6400000000003, "end": 2111.84, "text": " all the labs are in Jupyter notebooks for the most part for this lecture or for this course.", "tokens": [50932, 439, 264, 20339, 366, 294, 22125, 88, 391, 43782, 337, 264, 881, 644, 337, 341, 7991, 420, 337, 341, 1164, 13, 51192], "temperature": 0.0, "avg_logprob": -0.10037296862641643, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.00023047946160659194}, {"id": 377, "seek": 209528, "start": 2111.84, "end": 2118.1600000000003, "text": " So yeah, what people do is like you just sort data by like a loss function. And so you just say like", "tokens": [51192, 407, 1338, 11, 437, 561, 360, 307, 411, 291, 445, 1333, 1412, 538, 411, 257, 4470, 2445, 13, 400, 370, 291, 445, 584, 411, 51508], "temperature": 0.0, "avg_logprob": -0.10037296862641643, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.00023047946160659194}, {"id": 378, "seek": 209528, "start": 2118.1600000000003, "end": 2122.48, "text": " the loss for this data point is the highest. So I think this is most likely to be wrong. Let me", "tokens": [51508, 264, 4470, 337, 341, 1412, 935, 307, 264, 6343, 13, 407, 286, 519, 341, 307, 881, 3700, 281, 312, 2085, 13, 961, 385, 51724], "temperature": 0.0, "avg_logprob": -0.10037296862641643, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.00023047946160659194}, {"id": 379, "seek": 212248, "start": 2122.48, "end": 2126.8, "text": " check it out. And then you would look at it by hand and then you would market or do something with", "tokens": [50364, 1520, 309, 484, 13, 400, 550, 291, 576, 574, 412, 309, 538, 1011, 293, 550, 291, 576, 2142, 420, 360, 746, 365, 50580], "temperature": 0.0, "avg_logprob": -0.08110488670459692, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.002251558005809784}, {"id": 380, "seek": 212248, "start": 2126.8, "end": 2131.44, "text": " it or you take the top 20 or something, but you would just do a lot of this by hand inside of", "tokens": [50580, 309, 420, 291, 747, 264, 1192, 945, 420, 746, 11, 457, 291, 576, 445, 360, 257, 688, 295, 341, 538, 1011, 1854, 295, 50812], "temperature": 0.0, "avg_logprob": -0.08110488670459692, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.002251558005809784}, {"id": 381, "seek": 212248, "start": 2131.44, "end": 2135.92, "text": " Jupyter notebooks and printing things out. And that was actually a pretty normal and standard way", "tokens": [50812, 22125, 88, 391, 43782, 293, 14699, 721, 484, 13, 400, 300, 390, 767, 257, 1238, 2710, 293, 3832, 636, 51036], "temperature": 0.0, "avg_logprob": -0.08110488670459692, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.002251558005809784}, {"id": 382, "seek": 212248, "start": 2135.92, "end": 2141.28, "text": " that like, you know, somebody in industry or data scientist or grad student would try to fix a data", "tokens": [51036, 300, 411, 11, 291, 458, 11, 2618, 294, 3518, 420, 1412, 12662, 420, 2771, 3107, 576, 853, 281, 3191, 257, 1412, 51304], "temperature": 0.0, "avg_logprob": -0.08110488670459692, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.002251558005809784}, {"id": 383, "seek": 212248, "start": 2141.28, "end": 2147.52, "text": " set. And so the whole idea is we're going to look at ways that we systematize these approaches", "tokens": [51304, 992, 13, 400, 370, 264, 1379, 1558, 307, 321, 434, 516, 281, 574, 412, 2098, 300, 321, 1185, 267, 1125, 613, 11587, 51616], "temperature": 0.0, "avg_logprob": -0.08110488670459692, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.002251558005809784}, {"id": 384, "seek": 212248, "start": 2147.52, "end": 2151.2, "text": " so that they're more reliable, more accurate, and they work on most data sets.", "tokens": [51616, 370, 300, 436, 434, 544, 12924, 11, 544, 8559, 11, 293, 436, 589, 322, 881, 1412, 6352, 13, 51800], "temperature": 0.0, "avg_logprob": -0.08110488670459692, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.002251558005809784}, {"id": 385, "seek": 215248, "start": 2153.2, "end": 2157.84, "text": " So this course is about the following. So today we're just looking at what is model", "tokens": [50400, 407, 341, 1164, 307, 466, 264, 3480, 13, 407, 965, 321, 434, 445, 1237, 412, 437, 307, 2316, 50632], "temperature": 0.0, "avg_logprob": -0.10528944156788013, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.00010889117402257398}, {"id": 386, "seek": 215248, "start": 2157.84, "end": 2162.88, "text": " centric guy versus data centric guy, get the juices flowing, think about how to think things", "tokens": [50632, 1489, 1341, 2146, 5717, 1412, 1489, 1341, 2146, 11, 483, 264, 37027, 13974, 11, 519, 466, 577, 281, 519, 721, 50884], "temperature": 0.0, "avg_logprob": -0.10528944156788013, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.00010889117402257398}, {"id": 387, "seek": 215248, "start": 2162.88, "end": 2168.08, "text": " in terms of data and the impact, why matters. Next lecture, we'll focus on label errors.", "tokens": [50884, 294, 2115, 295, 1412, 293, 264, 2712, 11, 983, 7001, 13, 3087, 7991, 11, 321, 603, 1879, 322, 7645, 13603, 13, 51144], "temperature": 0.0, "avg_logprob": -0.10528944156788013, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.00010889117402257398}, {"id": 388, "seek": 215248, "start": 2168.8, "end": 2173.12, "text": " So how do you actually detect label errors automatically? How do you learn with label", "tokens": [51180, 407, 577, 360, 291, 767, 5531, 7645, 13603, 6772, 30, 1012, 360, 291, 1466, 365, 7645, 51396], "temperature": 0.0, "avg_logprob": -0.10528944156788013, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.00010889117402257398}, {"id": 389, "seek": 215248, "start": 2173.12, "end": 2177.12, "text": " errors? What are good methods to do that? And what are some things to think about when you're", "tokens": [51396, 13603, 30, 708, 366, 665, 7150, 281, 360, 300, 30, 400, 437, 366, 512, 721, 281, 519, 466, 562, 291, 434, 51596], "temperature": 0.0, "avg_logprob": -0.10528944156788013, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.00010889117402257398}, {"id": 390, "seek": 217712, "start": 2177.12, "end": 2183.3599999999997, "text": " doing this? Data set creation and curation will be on Thursday. And this is how do you construct", "tokens": [50364, 884, 341, 30, 11888, 992, 8016, 293, 1262, 399, 486, 312, 322, 10383, 13, 400, 341, 307, 577, 360, 291, 7690, 50676], "temperature": 0.0, "avg_logprob": -0.07669340443407369, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.03307404741644859}, {"id": 391, "seek": 217712, "start": 2183.3599999999997, "end": 2187.8399999999997, "text": " a data set in such a way that you can train a good model? How do you arrange, you know,", "tokens": [50676, 257, 1412, 992, 294, 1270, 257, 636, 300, 291, 393, 3847, 257, 665, 2316, 30, 1012, 360, 291, 9424, 11, 291, 458, 11, 50900], "temperature": 0.0, "avg_logprob": -0.07669340443407369, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.03307404741644859}, {"id": 392, "seek": 217712, "start": 2187.8399999999997, "end": 2193.3599999999997, "text": " the classes? How do you choose good examples? And then finally on Friday, which is related to", "tokens": [50900, 264, 5359, 30, 1012, 360, 291, 2826, 665, 5110, 30, 400, 550, 2721, 322, 6984, 11, 597, 307, 4077, 281, 51176], "temperature": 0.0, "avg_logprob": -0.07669340443407369, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.03307404741644859}, {"id": 393, "seek": 217712, "start": 2193.3599999999997, "end": 2199.8399999999997, "text": " data set curation, we'll look at active learning and potentially core sets and active learning.", "tokens": [51176, 1412, 992, 1262, 399, 11, 321, 603, 574, 412, 4967, 2539, 293, 7263, 4965, 6352, 293, 4967, 2539, 13, 51500], "temperature": 0.0, "avg_logprob": -0.07669340443407369, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.03307404741644859}, {"id": 394, "seek": 217712, "start": 2199.8399999999997, "end": 2203.7599999999998, "text": " As I mentioned, this is task where you're trying to choose the next data you want to add to your", "tokens": [51500, 1018, 286, 2835, 11, 341, 307, 5633, 689, 291, 434, 1382, 281, 2826, 264, 958, 1412, 291, 528, 281, 909, 281, 428, 51696], "temperature": 0.0, "avg_logprob": -0.07669340443407369, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.03307404741644859}, {"id": 395, "seek": 220376, "start": 2203.76, "end": 2209.1200000000003, "text": " data set and you want to obtain a label for. Or do you want to improve some of the labels you", "tokens": [50364, 1412, 992, 293, 291, 528, 281, 12701, 257, 7645, 337, 13, 1610, 360, 291, 528, 281, 3470, 512, 295, 264, 16949, 291, 50632], "temperature": 0.0, "avg_logprob": -0.0862808478505988, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.002631111303344369}, {"id": 396, "seek": 220376, "start": 2209.1200000000003, "end": 2213.6800000000003, "text": " currently have? And so you're just trying to decide, I have to pay a cost for new data that I'm going", "tokens": [50632, 4362, 362, 30, 400, 370, 291, 434, 445, 1382, 281, 4536, 11, 286, 362, 281, 1689, 257, 2063, 337, 777, 1412, 300, 286, 478, 516, 50860], "temperature": 0.0, "avg_logprob": -0.0862808478505988, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.002631111303344369}, {"id": 397, "seek": 220376, "start": 2213.6800000000003, "end": 2218.48, "text": " to add to my data set. And it costs me something like either money or time. So I don't want to do", "tokens": [50860, 281, 909, 281, 452, 1412, 992, 13, 400, 309, 5497, 385, 746, 411, 2139, 1460, 420, 565, 13, 407, 286, 500, 380, 528, 281, 360, 51100], "temperature": 0.0, "avg_logprob": -0.0862808478505988, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.002631111303344369}, {"id": 398, "seek": 220376, "start": 2218.48, "end": 2222.6400000000003, "text": " it too much. So what's like the best stuff to add to my data set now to improve my model?", "tokens": [51100, 309, 886, 709, 13, 407, 437, 311, 411, 264, 1151, 1507, 281, 909, 281, 452, 1412, 992, 586, 281, 3470, 452, 2316, 30, 51308], "temperature": 0.0, "avg_logprob": -0.0862808478505988, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.002631111303344369}, {"id": 399, "seek": 220376, "start": 2223.5200000000004, "end": 2227.36, "text": " Next week, we'll sort of have a bit of a shift and we'll focus more on data, but", "tokens": [51352, 3087, 1243, 11, 321, 603, 1333, 295, 362, 257, 857, 295, 257, 5513, 293, 321, 603, 1879, 544, 322, 1412, 11, 457, 51544], "temperature": 0.0, "avg_logprob": -0.0862808478505988, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.002631111303344369}, {"id": 400, "seek": 220376, "start": 2228.0, "end": 2232.88, "text": " we'll focus on some specific things for the first on Monday, we'll focus on class and balance and", "tokens": [51576, 321, 603, 1879, 322, 512, 2685, 721, 337, 264, 700, 322, 8138, 11, 321, 603, 1879, 322, 1508, 293, 4772, 293, 51820], "temperature": 0.0, "avg_logprob": -0.0862808478505988, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.002631111303344369}, {"id": 401, "seek": 223288, "start": 2232.88, "end": 2239.04, "text": " distribution shift. So this is, imagine like it's the stock market, right? And like if you're", "tokens": [50364, 7316, 5513, 13, 407, 341, 307, 11, 3811, 411, 309, 311, 264, 4127, 2142, 11, 558, 30, 400, 411, 498, 291, 434, 50672], "temperature": 0.0, "avg_logprob": -0.10732695207757464, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0004440703778527677}, {"id": 402, "seek": 223288, "start": 2239.04, "end": 2244.48, "text": " trying to predict things, you know, on Monday or on in January of this year versus now, it would be a", "tokens": [50672, 1382, 281, 6069, 721, 11, 291, 458, 11, 322, 8138, 420, 322, 294, 7061, 295, 341, 1064, 5717, 586, 11, 309, 576, 312, 257, 50944], "temperature": 0.0, "avg_logprob": -0.10732695207757464, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0004440703778527677}, {"id": 403, "seek": 223288, "start": 2244.48, "end": 2251.28, "text": " very different market, right? And so over time, data changes. And so how do you continue to produce", "tokens": [50944, 588, 819, 2142, 11, 558, 30, 400, 370, 670, 565, 11, 1412, 2962, 13, 400, 370, 577, 360, 291, 2354, 281, 5258, 51284], "temperature": 0.0, "avg_logprob": -0.10732695207757464, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0004440703778527677}, {"id": 404, "seek": 223288, "start": 2251.28, "end": 2256.7200000000003, "text": " good reliable predictions even though data is changing? And then the class imbalances this", "tokens": [51284, 665, 12924, 21264, 754, 1673, 1412, 307, 4473, 30, 400, 550, 264, 1508, 566, 2645, 2676, 341, 51556], "temperature": 0.0, "avg_logprob": -0.10732695207757464, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0004440703778527677}, {"id": 405, "seek": 223288, "start": 2256.7200000000003, "end": 2261.52, "text": " problem where imagine that for that line over there on the left, we had like a million pluses and", "tokens": [51556, 1154, 689, 3811, 300, 337, 300, 1622, 670, 456, 322, 264, 1411, 11, 321, 632, 411, 257, 2459, 1804, 279, 293, 51796], "temperature": 0.0, "avg_logprob": -0.10732695207757464, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0004440703778527677}, {"id": 406, "seek": 226152, "start": 2261.52, "end": 2268.48, "text": " only a few minuses. Well, then a smart classifier could actually just always predict plus and get", "tokens": [50364, 787, 257, 1326, 3175, 279, 13, 1042, 11, 550, 257, 4069, 1508, 9902, 727, 767, 445, 1009, 6069, 1804, 293, 483, 50712], "temperature": 0.0, "avg_logprob": -0.10186361011705901, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.00034594570752233267}, {"id": 407, "seek": 226152, "start": 2268.48, "end": 2273.44, "text": " near 100% accuracy. So often it will just ignore the minuses. So how do we get around that problem?", "tokens": [50712, 2651, 2319, 4, 14170, 13, 407, 2049, 309, 486, 445, 11200, 264, 3175, 279, 13, 407, 577, 360, 321, 483, 926, 300, 1154, 30, 50960], "temperature": 0.0, "avg_logprob": -0.10186361011705901, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.00034594570752233267}, {"id": 408, "seek": 226152, "start": 2275.6, "end": 2280.48, "text": " Interpretable features of data. So this is, who here's familiar with interpretability?", "tokens": [51068, 5751, 6629, 712, 4122, 295, 1412, 13, 407, 341, 307, 11, 567, 510, 311, 4963, 365, 7302, 2310, 30, 51312], "temperature": 0.0, "avg_logprob": -0.10186361011705901, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.00034594570752233267}, {"id": 409, "seek": 226152, "start": 2283.12, "end": 2287.04, "text": " All right, not as much. That one will be fun then. And thanks for raising your hand.", "tokens": [51444, 1057, 558, 11, 406, 382, 709, 13, 663, 472, 486, 312, 1019, 550, 13, 400, 3231, 337, 11225, 428, 1011, 13, 51640], "temperature": 0.0, "avg_logprob": -0.10186361011705901, "compression_ratio": 1.5311203319502074, "no_speech_prob": 0.00034594570752233267}, {"id": 410, "seek": 228704, "start": 2287.84, "end": 2292.88, "text": " That should be a good class. We'll learn about how do you interpret data in a way that you can", "tokens": [50404, 663, 820, 312, 257, 665, 1508, 13, 492, 603, 1466, 466, 577, 360, 291, 7302, 1412, 294, 257, 636, 300, 291, 393, 50656], "temperature": 0.0, "avg_logprob": -0.10419473415467798, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0006070403032936156}, {"id": 411, "seek": 228704, "start": 2292.88, "end": 2297.44, "text": " understand what's going on from a data perspective? Like why is the model doing what it's doing in", "tokens": [50656, 1223, 437, 311, 516, 322, 490, 257, 1412, 4585, 30, 1743, 983, 307, 264, 2316, 884, 437, 309, 311, 884, 294, 50884], "temperature": 0.0, "avg_logprob": -0.10419473415467798, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0006070403032936156}, {"id": 412, "seek": 228704, "start": 2297.44, "end": 2302.8, "text": " terms of the data? And so we'll understand model performance based on data. The next class on", "tokens": [50884, 2115, 295, 264, 1412, 30, 400, 370, 321, 603, 1223, 2316, 3389, 2361, 322, 1412, 13, 440, 958, 1508, 322, 51152], "temperature": 0.0, "avg_logprob": -0.10419473415467798, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0006070403032936156}, {"id": 413, "seek": 228704, "start": 2302.8, "end": 2307.68, "text": " Wednesday of next week will be on data-centric evaluation of ML models. So like, how do we know,", "tokens": [51152, 10579, 295, 958, 1243, 486, 312, 322, 1412, 12, 45300, 13344, 295, 21601, 5245, 13, 407, 411, 11, 577, 360, 321, 458, 11, 51396], "temperature": 0.0, "avg_logprob": -0.10419473415467798, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0006070403032936156}, {"id": 414, "seek": 228704, "start": 2307.68, "end": 2312.56, "text": " you know, from a data perspective, how good a model is, how reliable it is, how well it's working?", "tokens": [51396, 291, 458, 11, 490, 257, 1412, 4585, 11, 577, 665, 257, 2316, 307, 11, 577, 12924, 309, 307, 11, 577, 731, 309, 311, 1364, 30, 51640], "temperature": 0.0, "avg_logprob": -0.10419473415467798, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0006070403032936156}, {"id": 415, "seek": 231256, "start": 2313.52, "end": 2319.2799999999997, "text": " On Thursday, we'll look at encoding human priors. So this is like, how do we augment data and also", "tokens": [50412, 1282, 10383, 11, 321, 603, 574, 412, 43430, 1952, 1790, 830, 13, 407, 341, 307, 411, 11, 577, 360, 321, 29919, 1412, 293, 611, 50700], "temperature": 0.0, "avg_logprob": -0.07380339194988382, "compression_ratio": 1.75, "no_speech_prob": 0.0012064065085723996}, {"id": 416, "seek": 231256, "start": 2319.2799999999997, "end": 2324.88, "text": " prompt engineering. So this class will cover a lot of things like GPT and chat GPT and", "tokens": [50700, 12391, 7043, 13, 407, 341, 1508, 486, 2060, 257, 688, 295, 721, 411, 26039, 51, 293, 5081, 26039, 51, 293, 50980], "temperature": 0.0, "avg_logprob": -0.07380339194988382, "compression_ratio": 1.75, "no_speech_prob": 0.0012064065085723996}, {"id": 417, "seek": 231256, "start": 2324.88, "end": 2329.44, "text": " transformer models and stuff like that. And then finally, our last class will be on data privacy", "tokens": [50980, 31782, 5245, 293, 1507, 411, 300, 13, 400, 550, 2721, 11, 527, 1036, 1508, 486, 312, 322, 1412, 11427, 51208], "temperature": 0.0, "avg_logprob": -0.07380339194988382, "compression_ratio": 1.75, "no_speech_prob": 0.0012064065085723996}, {"id": 418, "seek": 231256, "start": 2329.44, "end": 2336.16, "text": " and security. And this is very, very interesting, especially for a lot of people in like banking", "tokens": [51208, 293, 3825, 13, 400, 341, 307, 588, 11, 588, 1880, 11, 2318, 337, 257, 688, 295, 561, 294, 411, 18261, 51544], "temperature": 0.0, "avg_logprob": -0.07380339194988382, "compression_ratio": 1.75, "no_speech_prob": 0.0012064065085723996}, {"id": 419, "seek": 231256, "start": 2336.16, "end": 2339.68, "text": " and finance and they're using a lot of machine learning models. How do you make sure that like a", "tokens": [51544, 293, 10719, 293, 436, 434, 1228, 257, 688, 295, 3479, 2539, 5245, 13, 1012, 360, 291, 652, 988, 300, 411, 257, 51720], "temperature": 0.0, "avg_logprob": -0.07380339194988382, "compression_ratio": 1.75, "no_speech_prob": 0.0012064065085723996}, {"id": 420, "seek": 233968, "start": 2339.68, "end": 2345.04, "text": " model doesn't actually secretly encode the data? Or like somehow if you had access to predictions,", "tokens": [50364, 2316, 1177, 380, 767, 22611, 2058, 1429, 264, 1412, 30, 1610, 411, 6063, 498, 291, 632, 2105, 281, 21264, 11, 50632], "temperature": 0.0, "avg_logprob": -0.08402527400425502, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.00045818687067367136}, {"id": 421, "seek": 233968, "start": 2345.04, "end": 2349.3599999999997, "text": " you could figure out someone's, you know, banking info and you can imagine all sorts of things that", "tokens": [50632, 291, 727, 2573, 484, 1580, 311, 11, 291, 458, 11, 18261, 13614, 293, 291, 393, 3811, 439, 7527, 295, 721, 300, 50848], "temperature": 0.0, "avg_logprob": -0.08402527400425502, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.00045818687067367136}, {"id": 422, "seek": 233968, "start": 2349.3599999999997, "end": 2355.7599999999998, "text": " can happen in AI. So data security and privacy is really important. Any sort of questions while I'm", "tokens": [50848, 393, 1051, 294, 7318, 13, 407, 1412, 3825, 293, 11427, 307, 534, 1021, 13, 2639, 1333, 295, 1651, 1339, 286, 478, 51168], "temperature": 0.0, "avg_logprob": -0.08402527400425502, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.00045818687067367136}, {"id": 423, "seek": 235576, "start": 2355.76, "end": 2370.48, "text": " on this slide? Sweet. Are you guys excited? Okay. Does it look like a good course?", "tokens": [50364, 322, 341, 4137, 30, 14653, 13, 2014, 291, 1074, 2919, 30, 1033, 13, 4402, 309, 574, 411, 257, 665, 1164, 30, 51100], "temperature": 0.0, "avg_logprob": -0.15125154099374447, "compression_ratio": 1.3407407407407408, "no_speech_prob": 0.0518263578414917}, {"id": 424, "seek": 235576, "start": 2373.44, "end": 2381.76, "text": " Okay. All right. Cool. All right. There's a lab for every lecture. And so you can find this on the", "tokens": [51248, 1033, 13, 1057, 558, 13, 8561, 13, 1057, 558, 13, 821, 311, 257, 2715, 337, 633, 7991, 13, 400, 370, 291, 393, 915, 341, 322, 264, 51664], "temperature": 0.0, "avg_logprob": -0.15125154099374447, "compression_ratio": 1.3407407407407408, "no_speech_prob": 0.0518263578414917}, {"id": 425, "seek": 238176, "start": 2381.76, "end": 2389.5200000000004, "text": " course website, which we can, we can write on the board. So you probably have seen it in the email,", "tokens": [50364, 1164, 3144, 11, 597, 321, 393, 11, 321, 393, 2464, 322, 264, 3150, 13, 407, 291, 1391, 362, 1612, 309, 294, 264, 3796, 11, 50752], "temperature": 0.0, "avg_logprob": -0.15143673636696556, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.007344888523221016}, {"id": 426, "seek": 238176, "start": 2389.5200000000004, "end": 2408.0800000000004, "text": " but just in case. And there's, there's a lab, usually will be Jupyter notebooks. And the one", "tokens": [50752, 457, 445, 294, 1389, 13, 400, 456, 311, 11, 456, 311, 257, 2715, 11, 2673, 486, 312, 22125, 88, 391, 43782, 13, 400, 264, 472, 51680], "temperature": 0.0, "avg_logprob": -0.15143673636696556, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.007344888523221016}, {"id": 427, "seek": 240808, "start": 2408.08, "end": 2414.4, "text": " for today will be a text classification task. And it has some bad, bad data that's gotten mixed", "tokens": [50364, 337, 965, 486, 312, 257, 2487, 21538, 5633, 13, 400, 309, 575, 512, 1578, 11, 1578, 1412, 300, 311, 5768, 7467, 50680], "temperature": 0.0, "avg_logprob": -0.10973935574293137, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.14208871126174927}, {"id": 428, "seek": 240808, "start": 2414.4, "end": 2421.2799999999997, "text": " in. And this is actual data that's been scraped from, I think, Amazon reviews. And it has some", "tokens": [50680, 294, 13, 400, 341, 307, 3539, 1412, 300, 311, 668, 13943, 3452, 490, 11, 286, 519, 11, 6795, 10229, 13, 400, 309, 575, 512, 51024], "temperature": 0.0, "avg_logprob": -0.10973935574293137, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.14208871126174927}, {"id": 429, "seek": 240808, "start": 2421.2799999999997, "end": 2428.08, "text": " bad tags, some weird HTML has gotten in there. And what you'll look at is model centric approaches", "tokens": [51024, 1578, 18632, 11, 512, 3657, 17995, 575, 5768, 294, 456, 13, 400, 437, 291, 603, 574, 412, 307, 2316, 1489, 1341, 11587, 51364], "temperature": 0.0, "avg_logprob": -0.10973935574293137, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.14208871126174927}, {"id": 430, "seek": 240808, "start": 2428.08, "end": 2432.24, "text": " at first. And you'll realize, Oh, it can only get you so far because like the data is not that great.", "tokens": [51364, 412, 700, 13, 400, 291, 603, 4325, 11, 876, 11, 309, 393, 787, 483, 291, 370, 1400, 570, 411, 264, 1412, 307, 406, 300, 869, 13, 51572], "temperature": 0.0, "avg_logprob": -0.10973935574293137, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.14208871126174927}, {"id": 431, "seek": 240808, "start": 2433.12, "end": 2437.92, "text": " And so you'll have to figure out how to improve the data set. You know, so you can get a", "tokens": [51616, 400, 370, 291, 603, 362, 281, 2573, 484, 577, 281, 3470, 264, 1412, 992, 13, 509, 458, 11, 370, 291, 393, 483, 257, 51856], "temperature": 0.0, "avg_logprob": -0.10973935574293137, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.14208871126174927}, {"id": 432, "seek": 243792, "start": 2437.92, "end": 2443.76, "text": " better classifier. And so that's sort of today's lab. Get your hands wet with data centric AI.", "tokens": [50364, 1101, 1508, 9902, 13, 400, 370, 300, 311, 1333, 295, 965, 311, 2715, 13, 3240, 428, 2377, 6630, 365, 1412, 1489, 1341, 7318, 13, 50656], "temperature": 0.0, "avg_logprob": -0.09995669314735814, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00040440112934447825}, {"id": 433, "seek": 243792, "start": 2444.64, "end": 2452.48, "text": " We have office hours every class, 3pm to 5pm. So an hour after the lecture ends every day.", "tokens": [50700, 492, 362, 3398, 2496, 633, 1508, 11, 805, 14395, 281, 1025, 14395, 13, 407, 364, 1773, 934, 264, 7991, 5314, 633, 786, 13, 51092], "temperature": 0.0, "avg_logprob": -0.09995669314735814, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00040440112934447825}, {"id": 434, "seek": 243792, "start": 2453.28, "end": 2457.2000000000003, "text": " And then tomorrow's lecture will focus on label errors, how to find them and how to train better", "tokens": [51132, 400, 550, 4153, 311, 7991, 486, 1879, 322, 7645, 13603, 11, 577, 281, 915, 552, 293, 577, 281, 3847, 1101, 51328], "temperature": 0.0, "avg_logprob": -0.09995669314735814, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00040440112934447825}, {"id": 435, "seek": 243792, "start": 2457.2000000000003, "end": 2463.28, "text": " models. This is the folks who are teaching the course for folks that are from MIT,", "tokens": [51328, 5245, 13, 639, 307, 264, 4024, 567, 366, 4571, 264, 1164, 337, 4024, 300, 366, 490, 13100, 11, 51632], "temperature": 0.0, "avg_logprob": -0.09995669314735814, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00040440112934447825}, {"id": 436, "seek": 246328, "start": 2464.2400000000002, "end": 2469.36, "text": " to from Stanford. And yeah, I think it'll be a good time.", "tokens": [50412, 281, 490, 20374, 13, 400, 1338, 11, 286, 519, 309, 603, 312, 257, 665, 565, 13, 50668], "temperature": 0.0, "avg_logprob": -0.285945078905891, "compression_ratio": 1.0625, "no_speech_prob": 0.004066642839461565}, {"id": 437, "seek": 246328, "start": 2470.96, "end": 2473.36, "text": " Really quick. Are there any questions?", "tokens": [50748, 4083, 1702, 13, 2014, 456, 604, 1651, 30, 50868], "temperature": 0.0, "avg_logprob": -0.285945078905891, "compression_ratio": 1.0625, "no_speech_prob": 0.004066642839461565}, {"id": 438, "seek": 246328, "start": 2485.2000000000003, "end": 2485.44, "text": " Yeah.", "tokens": [51460, 865, 13, 51472], "temperature": 0.0, "avg_logprob": -0.285945078905891, "compression_ratio": 1.0625, "no_speech_prob": 0.004066642839461565}, {"id": 439, "seek": 248544, "start": 2486.4, "end": 2491.28, "text": " My question is like, how do you know if it's like data that's the problem and not the model?", "tokens": [50412, 1222, 1168, 307, 411, 11, 577, 360, 291, 458, 498, 309, 311, 411, 1412, 300, 311, 264, 1154, 293, 406, 264, 2316, 30, 50656], "temperature": 0.0, "avg_logprob": -0.24207711407518762, "compression_ratio": 1.80078125, "no_speech_prob": 0.005998226813971996}, {"id": 440, "seek": 248544, "start": 2491.92, "end": 2496.16, "text": " Let's say you train a bunch of times in a model with different parameters. It's not like", "tokens": [50688, 961, 311, 584, 291, 3847, 257, 3840, 295, 1413, 294, 257, 2316, 365, 819, 9834, 13, 467, 311, 406, 411, 50900], "temperature": 0.0, "avg_logprob": -0.24207711407518762, "compression_ratio": 1.80078125, "no_speech_prob": 0.005998226813971996}, {"id": 441, "seek": 248544, "start": 2496.16, "end": 2501.84, "text": " doing good. How do you know the data is a problem? Do you just try it and see if it improves or", "tokens": [50900, 884, 665, 13, 1012, 360, 291, 458, 264, 1412, 307, 257, 1154, 30, 1144, 291, 445, 853, 309, 293, 536, 498, 309, 24771, 420, 51184], "temperature": 0.0, "avg_logprob": -0.24207711407518762, "compression_ratio": 1.80078125, "no_speech_prob": 0.005998226813971996}, {"id": 442, "seek": 248544, "start": 2501.84, "end": 2507.76, "text": " do you can use it another way to see that? Yeah, that's a good question. So there's a few ways.", "tokens": [51184, 360, 291, 393, 764, 309, 1071, 636, 281, 536, 300, 30, 865, 11, 300, 311, 257, 665, 1168, 13, 407, 456, 311, 257, 1326, 2098, 13, 51480], "temperature": 0.0, "avg_logprob": -0.24207711407518762, "compression_ratio": 1.80078125, "no_speech_prob": 0.005998226813971996}, {"id": 443, "seek": 248544, "start": 2507.76, "end": 2514.32, "text": " So one thing you can do is you can look at a subset of the problem. So say you had like", "tokens": [51480, 407, 472, 551, 291, 393, 360, 307, 291, 393, 574, 412, 257, 25993, 295, 264, 1154, 13, 407, 584, 291, 632, 411, 51808], "temperature": 0.0, "avg_logprob": -0.24207711407518762, "compression_ratio": 1.80078125, "no_speech_prob": 0.005998226813971996}, {"id": 444, "seek": 251432, "start": 2514.32, "end": 2518.7200000000003, "text": " a very big complex problem and there's thousands of classes and millions of data points.", "tokens": [50364, 257, 588, 955, 3997, 1154, 293, 456, 311, 5383, 295, 5359, 293, 6803, 295, 1412, 2793, 13, 50584], "temperature": 0.0, "avg_logprob": -0.12099689245223999, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.0018094712868332863}, {"id": 445, "seek": 251432, "start": 2518.7200000000003, "end": 2522.2400000000002, "text": " You can take just 10,000 of those data points and a few of those classes", "tokens": [50584, 509, 393, 747, 445, 1266, 11, 1360, 295, 729, 1412, 2793, 293, 257, 1326, 295, 729, 5359, 50760], "temperature": 0.0, "avg_logprob": -0.12099689245223999, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.0018094712868332863}, {"id": 446, "seek": 251432, "start": 2522.96, "end": 2527.2000000000003, "text": " and actually check, do some process to check, make sure you have really high quality data", "tokens": [50796, 293, 767, 1520, 11, 360, 512, 1399, 281, 1520, 11, 652, 988, 291, 362, 534, 1090, 3125, 1412, 51008], "temperature": 0.0, "avg_logprob": -0.12099689245223999, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.0018094712868332863}, {"id": 447, "seek": 251432, "start": 2527.2000000000003, "end": 2532.32, "text": " and see how well does your model perform. And if your model is performing very, very high accuracy,", "tokens": [51008, 293, 536, 577, 731, 775, 428, 2316, 2042, 13, 400, 498, 428, 2316, 307, 10205, 588, 11, 588, 1090, 14170, 11, 51264], "temperature": 0.0, "avg_logprob": -0.12099689245223999, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.0018094712868332863}, {"id": 448, "seek": 251432, "start": 2532.32, "end": 2536.96, "text": " but then when you use the original data, you get a drop off. That gives you a good signal.", "tokens": [51264, 457, 550, 562, 291, 764, 264, 3380, 1412, 11, 291, 483, 257, 3270, 766, 13, 663, 2709, 291, 257, 665, 6358, 13, 51496], "temperature": 0.0, "avg_logprob": -0.12099689245223999, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.0018094712868332863}, {"id": 449, "seek": 251432, "start": 2536.96, "end": 2542.0800000000004, "text": " Another thing is if you have similar data set and it's like MNIST for example,", "tokens": [51496, 3996, 551, 307, 498, 291, 362, 2531, 1412, 992, 293, 309, 311, 411, 376, 45, 19756, 337, 1365, 11, 51752], "temperature": 0.0, "avg_logprob": -0.12099689245223999, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.0018094712868332863}, {"id": 450, "seek": 254208, "start": 2542.08, "end": 2546.24, "text": " you can get near 100% performance. And so you have a similar data set, but you're getting", "tokens": [50364, 291, 393, 483, 2651, 2319, 4, 3389, 13, 400, 370, 291, 362, 257, 2531, 1412, 992, 11, 457, 291, 434, 1242, 50572], "temperature": 0.0, "avg_logprob": -0.0967906598691587, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0028890767134726048}, {"id": 451, "seek": 254208, "start": 2546.24, "end": 2551.52, "text": " like much worse performance or like significantly less performance, but using the same architecture", "tokens": [50572, 411, 709, 5324, 3389, 420, 411, 10591, 1570, 3389, 11, 457, 1228, 264, 912, 9482, 50836], "temperature": 0.0, "avg_logprob": -0.0967906598691587, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0028890767134726048}, {"id": 452, "seek": 254208, "start": 2551.52, "end": 2554.7999999999997, "text": " that you've seen do very well on a similar task on a different data set.", "tokens": [50836, 300, 291, 600, 1612, 360, 588, 731, 322, 257, 2531, 5633, 322, 257, 819, 1412, 992, 13, 51000], "temperature": 0.0, "avg_logprob": -0.0967906598691587, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0028890767134726048}, {"id": 453, "seek": 254208, "start": 2555.36, "end": 2560.0, "text": " And that's a good indicator. You should probably look at the data. And there's two more things.", "tokens": [51028, 400, 300, 311, 257, 665, 16961, 13, 509, 820, 1391, 574, 412, 264, 1412, 13, 400, 456, 311, 732, 544, 721, 13, 51260], "temperature": 0.0, "avg_logprob": -0.0967906598691587, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0028890767134726048}, {"id": 454, "seek": 254208, "start": 2560.0, "end": 2564.7999999999997, "text": " One is just, just take a look at the data. I think it's really easy to just get a data set", "tokens": [51260, 1485, 307, 445, 11, 445, 747, 257, 574, 412, 264, 1412, 13, 286, 519, 309, 311, 534, 1858, 281, 445, 483, 257, 1412, 992, 51500], "temperature": 0.0, "avg_logprob": -0.0967906598691587, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0028890767134726048}, {"id": 455, "seek": 254208, "start": 2564.7999999999997, "end": 2568.56, "text": " and your goal is like train a model. And so you're doing a lot of cool, you know,", "tokens": [51500, 293, 428, 3387, 307, 411, 3847, 257, 2316, 13, 400, 370, 291, 434, 884, 257, 688, 295, 1627, 11, 291, 458, 11, 51688], "temperature": 0.0, "avg_logprob": -0.0967906598691587, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.0028890767134726048}, {"id": 456, "seek": 256856, "start": 2568.56, "end": 2572.7999999999997, "text": " download this TensorFlow package, download this hugging face package, but you don't actually", "tokens": [50364, 5484, 341, 37624, 7372, 11, 5484, 341, 41706, 1851, 7372, 11, 457, 291, 500, 380, 767, 50576], "temperature": 0.0, "avg_logprob": -0.0973021483220974, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.0010321158915758133}, {"id": 457, "seek": 256856, "start": 2572.7999999999997, "end": 2578.4, "text": " take time usually to look through all the data points or like hundreds of data points and really", "tokens": [50576, 747, 565, 2673, 281, 574, 807, 439, 264, 1412, 2793, 420, 411, 6779, 295, 1412, 2793, 293, 534, 50856], "temperature": 0.0, "avg_logprob": -0.0973021483220974, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.0010321158915758133}, {"id": 458, "seek": 256856, "start": 2578.4, "end": 2583.2799999999997, "text": " see like, does this data look like what I think it does? Like, does it seem to be kind of messy?", "tokens": [50856, 536, 411, 11, 775, 341, 1412, 574, 411, 437, 286, 519, 309, 775, 30, 1743, 11, 775, 309, 1643, 281, 312, 733, 295, 16191, 30, 51100], "temperature": 0.0, "avg_logprob": -0.0973021483220974, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.0010321158915758133}, {"id": 459, "seek": 256856, "start": 2583.2799999999997, "end": 2587.2799999999997, "text": " And what you'll often find is after first like, you know, first few hundred, you'll be like,", "tokens": [51100, 400, 437, 291, 603, 2049, 915, 307, 934, 700, 411, 11, 291, 458, 11, 700, 1326, 3262, 11, 291, 603, 312, 411, 11, 51300], "temperature": 0.0, "avg_logprob": -0.0973021483220974, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.0010321158915758133}, {"id": 460, "seek": 256856, "start": 2587.2799999999997, "end": 2591.44, "text": " oh, there's some weird stuff in here. And if you have millions of data points, that weird stuff adds up.", "tokens": [51300, 1954, 11, 456, 311, 512, 3657, 1507, 294, 510, 13, 400, 498, 291, 362, 6803, 295, 1412, 2793, 11, 300, 3657, 1507, 10860, 493, 13, 51508], "temperature": 0.0, "avg_logprob": -0.0973021483220974, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.0010321158915758133}, {"id": 461, "seek": 259144, "start": 2591.44, "end": 2598.16, "text": " Yeah, I guess the problem with the data is that there's like two months, so like, you know,", "tokens": [50364, 865, 11, 286, 2041, 264, 1154, 365, 264, 1412, 307, 300, 456, 311, 411, 732, 2493, 11, 370, 411, 11, 291, 458, 11, 50700], "temperature": 0.0, "avg_logprob": -0.215147251036109, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0020182912703603506}, {"id": 462, "seek": 259144, "start": 2598.16, "end": 2602.32, "text": " like how do you connect with all of them? Yeah, I guess if you just look at like a bunch.", "tokens": [50700, 411, 577, 360, 291, 1745, 365, 439, 295, 552, 30, 865, 11, 286, 2041, 498, 291, 445, 574, 412, 411, 257, 3840, 13, 50908], "temperature": 0.0, "avg_logprob": -0.215147251036109, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0020182912703603506}, {"id": 463, "seek": 259144, "start": 2604.32, "end": 2610.0, "text": " Yeah, totally. In the next class, we'll show ways that you can actually rank your data set", "tokens": [51008, 865, 11, 3879, 13, 682, 264, 958, 1508, 11, 321, 603, 855, 2098, 300, 291, 393, 767, 6181, 428, 1412, 992, 51292], "temperature": 0.0, "avg_logprob": -0.215147251036109, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0020182912703603506}, {"id": 464, "seek": 259144, "start": 2610.64, "end": 2616.2400000000002, "text": " so that you know what's the best example to look at first. That's probably wrong. And so there are", "tokens": [51324, 370, 300, 291, 458, 437, 311, 264, 1151, 1365, 281, 574, 412, 700, 13, 663, 311, 1391, 2085, 13, 400, 370, 456, 366, 51604], "temperature": 0.0, "avg_logprob": -0.215147251036109, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0020182912703603506}, {"id": 465, "seek": 259144, "start": 2616.2400000000002, "end": 2620.88, "text": " ways that you can automate this and then you can look at the initial data. And that's really the", "tokens": [51604, 2098, 300, 291, 393, 31605, 341, 293, 550, 291, 393, 574, 412, 264, 5883, 1412, 13, 400, 300, 311, 534, 264, 51836], "temperature": 0.0, "avg_logprob": -0.215147251036109, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0020182912703603506}, {"id": 466, "seek": 262088, "start": 2620.88, "end": 2624.8, "text": " right approach. Like if you just look at random, then you might waste some time. But if you've", "tokens": [50364, 558, 3109, 13, 1743, 498, 291, 445, 574, 412, 4974, 11, 550, 291, 1062, 5964, 512, 565, 13, 583, 498, 291, 600, 50560], "temperature": 0.0, "avg_logprob": -0.10851432181693413, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.0017002280801534653}, {"id": 467, "seek": 262088, "start": 2624.8, "end": 2630.2400000000002, "text": " ranked your data in a way that is likely to give you a good ranking on quality, and then you look", "tokens": [50560, 20197, 428, 1412, 294, 257, 636, 300, 307, 3700, 281, 976, 291, 257, 665, 17833, 322, 3125, 11, 293, 550, 291, 574, 50832], "temperature": 0.0, "avg_logprob": -0.10851432181693413, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.0017002280801534653}, {"id": 468, "seek": 262088, "start": 2630.2400000000002, "end": 2635.92, "text": " at like the first 100 and there's really no issues and the data looks really good, then yeah, you", "tokens": [50832, 412, 411, 264, 700, 2319, 293, 456, 311, 534, 572, 2663, 293, 264, 1412, 1542, 534, 665, 11, 550, 1338, 11, 291, 51116], "temperature": 0.0, "avg_logprob": -0.10851432181693413, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.0017002280801534653}, {"id": 469, "seek": 263592, "start": 2635.92, "end": 2654.64, "text": " might be able to just optimize the model and be okay. Any other questions?", "tokens": [50364, 1062, 312, 1075, 281, 445, 19719, 264, 2316, 293, 312, 1392, 13, 2639, 661, 1651, 30, 51300], "temperature": 0.0, "avg_logprob": -0.16738733791169666, "compression_ratio": 1.3650793650793651, "no_speech_prob": 0.006902635563164949}, {"id": 470, "seek": 263592, "start": 2659.84, "end": 2664.4, "text": " Is this useful for anyone sort of immediately? Is anyone thinking like, hey, this might be useful", "tokens": [51560, 1119, 341, 4420, 337, 2878, 1333, 295, 4258, 30, 1119, 2878, 1953, 411, 11, 4177, 11, 341, 1062, 312, 4420, 51788], "temperature": 0.0, "avg_logprob": -0.16738733791169666, "compression_ratio": 1.3650793650793651, "no_speech_prob": 0.006902635563164949}, {"id": 471, "seek": 266440, "start": 2664.4, "end": 2666.64, "text": " for what I'm working on right now?", "tokens": [50364, 337, 437, 286, 478, 1364, 322, 558, 586, 30, 50476], "temperature": 0.0, "avg_logprob": -0.3385690759729456, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.012197285890579224}, {"id": 472, "seek": 266440, "start": 2675.6, "end": 2680.0, "text": " I haven't done anything specific about it before, and I think one of the reasons is", "tokens": [50924, 286, 2378, 380, 1096, 1340, 2685, 466, 309, 949, 11, 293, 286, 519, 472, 295, 264, 4112, 307, 51144], "temperature": 0.0, "avg_logprob": -0.3385690759729456, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.012197285890579224}, {"id": 473, "seek": 266440, "start": 2680.64, "end": 2685.6, "text": " because, you know, I always study models before. It seems like, you know, studying models is like", "tokens": [51176, 570, 11, 291, 458, 11, 286, 1009, 2979, 5245, 949, 13, 467, 2544, 411, 11, 291, 458, 11, 7601, 5245, 307, 411, 51424], "temperature": 0.0, "avg_logprob": -0.3385690759729456, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.012197285890579224}, {"id": 474, "seek": 266440, "start": 2685.6, "end": 2689.92, "text": " very important, but once you go and you want to like quickly do something, that's kind of like the", "tokens": [51424, 588, 1021, 11, 457, 1564, 291, 352, 293, 291, 528, 281, 411, 2661, 360, 746, 11, 300, 311, 733, 295, 411, 264, 51640], "temperature": 0.0, "avg_logprob": -0.3385690759729456, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.012197285890579224}, {"id": 475, "seek": 268992, "start": 2689.92, "end": 2694.96, "text": " first thing that you want to do, you know, like, okay, what they do is data and how they work.", "tokens": [50364, 700, 551, 300, 291, 528, 281, 360, 11, 291, 458, 11, 411, 11, 1392, 11, 437, 436, 360, 307, 1412, 293, 577, 436, 589, 13, 50616], "temperature": 0.0, "avg_logprob": -0.4274299621582031, "compression_ratio": 1.3214285714285714, "no_speech_prob": 0.020320842042565346}, {"id": 476, "seek": 268992, "start": 2698.64, "end": 2700.64, "text": " Yeah.", "tokens": [50800, 865, 13, 50900], "temperature": 0.0, "avg_logprob": -0.4274299621582031, "compression_ratio": 1.3214285714285714, "no_speech_prob": 0.020320842042565346}, {"id": 477, "seek": 268992, "start": 2713.52, "end": 2718.16, "text": " All right, great. We'll be here for a little bit after. Thanks everybody for coming.", "tokens": [51544, 1057, 558, 11, 869, 13, 492, 603, 312, 510, 337, 257, 707, 857, 934, 13, 2561, 2201, 337, 1348, 13, 51776], "temperature": 0.0, "avg_logprob": -0.4274299621582031, "compression_ratio": 1.3214285714285714, "no_speech_prob": 0.020320842042565346}, {"id": 478, "seek": 271816, "start": 2718.16, "end": 2724.08, "text": " The next lectures will be a bit more technical, but I think it should be a really exciting course,", "tokens": [50364, 440, 958, 16564, 486, 312, 257, 857, 544, 6191, 11, 457, 286, 519, 309, 820, 312, 257, 534, 4670, 1164, 11, 50660], "temperature": 0.0, "avg_logprob": -0.13748023169381277, "compression_ratio": 1.2522522522522523, "no_speech_prob": 0.00985394325107336}, {"id": 479, "seek": 271816, "start": 2724.08, "end": 2727.8399999999997, "text": " and glad to have everybody here. Thanks.", "tokens": [50660, 293, 5404, 281, 362, 2201, 510, 13, 2561, 13, 50848], "temperature": 0.0, "avg_logprob": -0.13748023169381277, "compression_ratio": 1.2522522522522523, "no_speech_prob": 0.00985394325107336}], "language": "en"}