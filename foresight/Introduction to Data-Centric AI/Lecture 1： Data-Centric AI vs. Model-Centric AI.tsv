start	end	text
0	10000	Alright, let's get started. Thanks for everybody who showed up. We've got three of the course
10000	17920	instructors here, myself, Anish in the back, and Jonas. And Anish is finishing up his final
17920	23120	year, but we all were here. We all were students here. We all did our PhDs here. And it's good
23120	27400	to be back. If you don't know any machine learning at all, you've never taken an intro
27400	32520	class, then this will be probably a little too confusing. But as long as you've seen a
32520	37800	little ML, then you'll have a good time. And ideally, you know a little bit of Python,
37800	43960	and you know it like NumPy and Pandasar. So with that, let's just get started. So our
43960	49360	goal in this course is to learn about data-centric AI. So just like a quick show of hands, prior
49360	57800	to this course, how many people I've heard of data-centric AI? Okay, alright, well, get
57800	61920	excited because you're going to learn some great things. It'll be really good. Alright,
61920	70920	so let's just jump in. So this is a picture of a self-driving car that has had an accident.
70920	77360	And you notice that the article focuses on when algorithms mess up, right? And this is
77360	80760	the common case, right? In machine learning, we tend to focus on the model. And so it's
80760	86360	like there's a crash, the algorithm must have done something wrong. And so I'd like us to
86360	91360	sort of think of a different way of thinking about this. This is a paper many folks are
91360	96520	familiar with, which basically shows one thing in that a neural network or a machine learning
96520	101520	classifier can learn total randomness. It's like if you give it complete garbage data,
101520	105960	just like completely random labels, it can learn to map like images to completely arbitrary
105960	110680	labels or text to completely arbitrary labels. So basically, if you give it really bad data,
110680	115520	it will just produce exactly what it learns, even if the data is completely wrong. And
115520	120320	so I'd like to sort of rethink this title of this article as when algorithms are trained
120320	125280	with erroneous data, things like car crashes can happen. And that's the way we'll sort
125280	131880	of focus and think about this course. So traditional machine learning is very model-centric, right?
131880	136080	Like when you take an ML class, you first learn machine learning, you're in school and
136080	140160	then you get a data set. And usually the data set is pretty good. Like if anyone's seen
140160	145840	like the cat dogs data set, you know, it's in images of cats and they're all cats and
145840	149400	they're labeled cat. And then there's images of dog and they're all dogs and they're labeled
149400	154440	dog and there's no, there's usually no like cows thrown in there, right? And there's usually
154440	158880	not like a bunch of dogs that are labeled cat. It's like pretty well curated. And then
158880	163520	your goal is to, you know, produce a good model, right? You want to train a model that
163520	168760	takes in a new image that's either a cat or a dog and it predicts is it cat or is it dog.
168760	174040	And that's sort of how we learn machine learning usually. And this is something that's standard
174040	180120	if anyone here has taken like 6036, which has been renamed to 63390, the intro to ML class
180120	184560	here. And you'll learn stuff like, you know, different types of models. And if you're familiar
184560	187960	with neural networks, neural architectures, or you'll learn about tuning hyper parameters
187960	193280	of a model, or you'll learn about modifying the loss function using different loss functions
193280	196840	and regularizations that you don't overfit. And these are common things you learn in model
196840	203000	centric AI. So let's just juxtapose that with the real world once you're out of the classroom.
203000	205940	So in the classroom, usually the data sets fixed and it's pretty good, but then you
205940	210200	go to the real world and actually the data set is not fixed, right? You have user data
210200	215520	or customer data or real world data and you can get more or less, you can change the data
215520	221000	and the data has all sorts of weird things in it. And so what tends to happen is that
221000	226200	the company or the user, whoever's sort of using this model that you're trying to train,
226200	231520	it doesn't really care as much about the cool ML fancy tricks as it does about like, does
231520	235280	it actually work in the real world? And if you have really good machine learning models
235280	241040	that work on highly curated data, but then the real world data is actually really messy,
241040	246120	and it makes sense to actually focus on fixing the issues in the data. And a lot of people
246120	249520	have been doing this, but what are systematic ways to do this? And that's what we'll focus
249520	255080	on this course. What may surprise some of you that you may not know is that 10 of the
255080	261840	most commonly cited and most used test sets in the field of machine learning all have
261840	268080	wrong labels. And that may be a surprise for some of you. So we'll just take a quick look
268080	279040	at this website, which I think will be pretty fun. This is labelairs.com. Can we see okay?
279040	283160	So this is a site that you can check out on your own. So this is ImageNet, which is a
283160	288200	common image data set. And these are in the test set, or in this case a validation set,
288200	292400	which is what was released. But this is the data that is supposed to be the most accurate
292400	296720	data, right? This is like your real world test data. And so this data should be some
296720	301120	of the most highly curated, most accurate data. And what you'll find is that, for example,
301120	306720	this scorpion is labeled tick. And you'll find all sorts of stuff. We can look at another
306720	313560	data. This is by Google. This is a drawing hand drawn data set. This is labeled a t-shirt.
313560	318960	And this is labeled a diving board. And this is labeled a scorpion. And it just keeps going
318960	323320	and going. And there's some fun ones. Like this is a, this is totally a cake, but it's
323360	328080	labeled a rake. Like it just didn't quite get the R to go all the way when they were
328080	331720	pretending they're writing it out. But anyway, you can check these out on your own. This
331720	336560	is for any type of data. So like text data or audio data. And you'll have fun. And I
336560	340120	think the good idea to think about here is that this is, you know, a data set that's
340120	346000	released by Google. It's a benchmark data set. And it's very difficult when you have
346000	350920	millions of examples to know, like, what's the bad data? And ideally, if you didn't have
350920	355080	that bad data, you could train better models. And so we need to learn how can we find these
355080	361080	errors and find these issues automatically. So going back to the slides. So as seasoned
368720	371960	data scientists, the way they'll approach this problem is that it's more worthwhile to
371960	375920	invest in exploring and fixing the data than trying to tinker with the models to avoid this
375920	380560	garbage in, garbage out issue. And the issue is that, like, if you have millions of data,
380560	383960	right, or you have, like, a data set that's like 100 million, like, how do you do that
383960	388600	without it being highly time consuming? All right. So we're here in a data-centric AI
388600	394680	course. We've called this introduction to data-centric AI because we want it to be accessible. And
394680	398320	so we're going to cover just, like, the very intro, what is data-centric AI? How does it
398320	403680	differ from model-centric AI? And then we'll dive in a little deeper. So data-centric AI often
403720	409040	takes one of two forms. So one form is that you have AI algorithms that understand something
409040	413960	about data, and then they use this new information that they've understood to help a model train
413960	418880	better. So an example of this is curriculum learning. And this is more of sort of something
418880	422760	you'd encounter in grad school, so you may not have heard of it. But what curriculum,
422760	429760	it's, you can check out the paper by Yasha Benjo. But the idea is you use a, some algorithm
429840	435600	that looks at data and it identifies which data is probably easy to learn. And so the
435600	440400	corollary here is imagine you're a student and you're in the classroom. All right. Should
440400	443800	the teacher, what should they teach you? Like, should they teach you really, really hard
443800	448240	examples as the very first examples? Like, if you're learning addition, should they start
448240	455520	out with like 10,051 plus 1042 or would they start out with like one plus two? Right. Now
455520	458960	we know this because, like, we've all learned addition. But when a machine learning model
459000	462560	starts, it's starting from scratch. So it doesn't know right from the beginning what
462560	467580	is the sort of easiest example. And so there are data-centric AI approaches that actually
467580	472440	estimate what is the easiest example. And then when you train an ML model, you start
472440	476000	with that example, and then you give it slightly harder ones and slightly harder them. And
476000	480300	so it's like curriculum learning because it's like a student's curriculum. And so that's
480300	484720	one way that you can sort of use new information. You're still training on all the same data,
484720	487600	but you're just reordering it and using this additional information. You don't actually
487600	492400	change the data set. Another sort of common form of data-centric AI is that you actually
492400	498080	modify the data set to directly improve the performance of a model. So an example of this
498080	502840	might be something like Confant Learning. And this is less known than curriculum learning,
502840	507760	something I worked on. And this idea here is that you want to find what are label errors
507760	512840	in a data set and then remove them prior to training, so that you just train on like correctly
512840	518200	labeled stuff. And the corollary here in the sense of the student is that if your teacher
518200	523160	is basically making mistakes 30% of the time, like imagine as I was teaching you, which
523160	528240	hopefully I don't do today, that 30% of the time I told you wrong things. And then you
528240	532760	compare that to another teacher who comes and they tell you right things 100% of the
532760	538240	time. Then which teacher do you think you'll probably learn data-centric AI better from?
538240	543280	And so the idea is that we want to take this bad sort of, you know, this 30% of wrong things
543280	546480	and we want to get rid of them so that you could sort of come to the classroom and redo
546480	550920	your learning experiences if those never happened. And that's the idea here. So you just learn
550920	557080	on the good stuff. So those are two approaches. And then what we'll think about now is sort
557080	563640	of what is the difference between model-centric AI and data-centric AI? So the sort of normal
563640	568120	way you hear this is that given a data set, you try to produce the best model. And that's
568160	573160	like the classical way of thinking about model-centric AI. And the idea is that you want to change
573160	579360	the model somehow to improve, you know, some performance on AI task. And is there anyone
579360	584000	here just out of curiosity who's not familiar with like AI tasks and some of the stuff we
584000	588680	do in AI? Just want to make sure that we're on the same board, like classifying things.
588680	592720	Okay, sweet. All right. So you've got some AI tasks and you're trying to classify something
592720	595680	and the goal is you want to improve the performance on that. So usually it'll change the model
595760	602760	to do that. And data-centric AI, instead, it's given some model, right, that may be fixed
602760	606680	or you may change, but let's just assume it's fixed for now. You want to improve that model
606680	611000	by improving your data set. So this is like the common way to think about the two. And
611000	615400	the idea is to systematically or algorithmically not have a bunch of humans changing the data
615400	620320	set, but like some algorithmic way that you can do this. Okay. All right. So our goal is
620320	625360	to start thinking about ML in terms of data and not the model. And so we'll just start
625360	629880	out with a simple example. And this is not data-centric AI, but it'll get us thinking
629880	635480	about how we can think about AI in terms of data. So just quick show of hands. Who here
635480	642120	is familiar with K nearest neighbors? Okay. Great. All right. So I won't belabor this
642120	654400	point then. So do some chalk. That was a good thing to check beforehand. We were here last
654400	666000	night, but the chalk has been removed. All right. So yeah, the key idea with K nearest
666000	671400	neighbors and the key idea to think about in the context of data-centric learning is
671400	678880	that K nearest neighbors, imagine you have, you know, a space, a 2D space, which, you
678880	695160	know what, here's what we'll do. All right. So you've got some, all right, sweet. So you've
695160	701960	got, say, you know, some data, you've got some triangles, and you've got some circles.
701960	707720	You'll have to apologize for my drawing and you've got some squares. And so this is your
707720	711440	data set. And let's say, you know, these are different types of images that you've put in
711440	715480	some 2D space somehow where it's text that you've mapped a 2D space. And then the idea
715480	720080	of K nearest neighbors, as many of you seem to know and are familiar with, is that you
720080	724600	would have some new point, say, here. And with this, it's some weird thing. We don't know
724600	728120	what it is. And we're trying to figure out, is it a triangle? Is it a circle? Is it a
728120	732920	square? And so if this was sort of three K nearest neighbors, like then you would find
732920	739760	the three nearest neighbors. So say this one, this one, and this one. And then can somebody
739760	748880	tell me, like, what the class would be and why? Yeah, it'd be a square. That doesn't
748880	755680	mean that it's not a cool point. And you're familiar with being a square. So this would
755680	759360	be labeled a square, and that's based on majority voting. And there's a lot of different ways
759360	765640	to do algorithms for deciding what the label should be. If it was, say, five neighbors,
765640	770040	meaning five in N, where K is five, so it's K in N, then you would choose the five nearest
770040	773920	neighbors and you would do a majority vote. Okay. The key idea here is that there is no
773920	779760	loss function. Literally, any time you have a new point, so say I just have another new
779760	786200	point here, I'm not sort of, there's no algorithm that I'm passing this into beyond just measuring
786200	790440	a distance, some notion of distance between this and the nearest points. And you can do
790440	793720	a bunch of pre-computation. There's a lot of smart work actually done in K nearest neighbors
793720	797800	that is pretty impressive and you can do embeddings for all of these and pre-compute distances
797800	802000	and you can do all sorts of fancy stuff. But ultimately, all this decision is based on
802000	808160	is the data. And so I wanted to motivate this problem and this way of thinking because this
808160	813880	whole decision process is made just based on data. And the quality of the data is as related
813880	818840	as possible to the quality of the prediction. So if you have really good data, you're going
818840	822160	to have really good predictions. And if you've got a bunch of errors in here, you're going
822160	827200	to get the wrong prediction. And so this makes it really clear why fixing the data will make
827200	831720	it, will improve your model. So is that kind of clear how this is motivating? Now this
831720	836280	is not a data-centric AI algorithm. And by the end of this lecture, you'll definitely
836280	840800	know what a data-centric AI algorithm is. But can someone tell me what the difference
840800	870480	is between K and N and a data-centric AI algorithm? Yeah, yeah, totally. That's a great
871480	877240	answer. K and N is just doing classification. And it's not actually modifying the data set.
877240	881280	So yeah, that's exactly right. Alright, so what are a few other examples of what is not
881280	887800	data-centric AI? Handpicking a bunch of points you think will improve a model. So can anybody
887800	895920	help me understand why this is not data-centric AI? Yeah. Yeah, totally. It's just done by
895920	900160	hand. Like this could, if you had a hundred million, a data set of a hundred million points,
900160	905560	this would take a long time. What about doubling the size of your data set so that you can
905560	920480	train an improved model? Yeah. Yeah, totally. This is just classical machine learning. It's
920480	923680	still all the model, all the work you do as a model, but you're just paying more money
923720	928680	for more data. So let's juxtapose this. So what would be the data-centric AI versions
928680	935520	of this? And just out of curiosity, does anybody know for the first one what would be sort
935520	949040	of the data-centric AI corollary? Yeah, totally. And there's a whole field of research on
949040	954480	this and a whole subsection of ML that's called corset selection, where you have a data set
954480	959000	and let's say that you train a model on that data set and you get like 98% accuracy. But
959000	963600	the data set is like a hundred billion points. And so the goal is, can I find like, you know,
963600	968680	a million points that if I just train on those, I can get like pretty close, like 97% accuracy.
968680	974160	And that's corset selection. What about for number two? What would be like a data-centric
974160	985640	AI corollary? Yeah, yeah. So the idea is, and how does anybody know of any ways that
985640	997040	you might make your data set like bigger without just getting twice as much data? Yeah, totally.
997040	1002560	So like, say you have a, so data augmentation, awesome. And so say you have an image, you
1002560	1006880	could just totally rotate that image. And now instead of one data point, you have like
1006880	1011520	five data points or turn it black and white, or you could shift it or skew it or take the
1011520	1015400	top and the bottom and move them a little bit or add some noise. There's lots of things
1015400	1019360	you can do to make data bigger and actually improve a model just by changing the data
1019360	1025320	set. And these are all fall within data-centric AI. Are you all familiar with what's called
1025320	1034560	like back translation for text data augmentation? So this is a pretty fun thing. I don't like
1034560	1043280	have particularly good translation skills. But like, if I say like, hi, you know, my
1043280	1066680	name is Curtis. And then I translate this. Okay, so it becomes Ola. And then I translate
1066680	1079520	this again. It might become, and now I've gotten one data point, and I just got another
1079520	1084520	data point. So I was able to augment my text data set to get more versions of the same
1084520	1091920	thing. And this could have some label, and the label could be introduction. You know,
1091920	1094960	I'll just end it there because I know it's hard to see on that side. But this, this would
1094960	1102200	be like the label. And this is text. And so this one is the same label. We haven't changed
1102200	1106080	the label at all. So this is intro. And now you can see I have more labeled data, but
1106080	1109280	I didn't have to do anything. I didn't have to pay more. I didn't have to, I just did
1109280	1110680	this all computationally. Yeah.
1110680	1114840	How do you see this in all the cases for that error to be taken?
1114840	1120360	Oh yeah, totally can. So if, say that this label was wrong, now you have two label errors.
1120840	1125880	Totally. Yeah. So you often want to combine two approaches. One is like first, try to
1125880	1130120	fix or improve label errors before doing the augmentation. And that's a really good idea.
1133120	1136400	All right. So what are some examples, we looked at examples that are not data-centric AI. So
1136400	1141280	what are some things that are data-centric AI? And for this, I'll go through quick because
1141280	1146680	we're going to learn all these in the course. And so one is outlier detection and removal.
1146680	1152960	So I'll just be really quick to show you. So say you have some data set and we've got,
1152960	1163960	let me use this board. And you've got, say, right? So you've got your sort of, your, you
1163960	1168760	know, two classes. And so normally you would, you would draw your classifier and then you
1168760	1172520	have some new point here and I'll be labeled a negative. But say in your training data,
1172520	1179120	you have this really weird, you know, plus over here. And so maybe the boundary should
1179120	1182520	be something like this, but you just don't have very much information and it's really
1182520	1187840	out here and it seems like it's not very related to the rest of the data. And so what often
1187840	1191400	people will do because they just don't have enough information is they'll identify this
1191400	1195560	as an automatic, as an outlier because it seems very out of distribution and they'll
1195560	1199040	remove it. And so then you get this line, which fits to all the data except for that
1199040	1207440	one point. In terms of some other things in data-centric AI, so error detection and correction.
1207440	1212200	And so that's, for example, you just have like a data point that is, it could be images,
1212200	1217960	it's like all black, or you have a label error like we saw earlier. If this, you know, text
1217960	1224360	example instead of being labeled intro was labeled like, you know, a goodbye clause or
1225320	1232400	something. You'd want to find that and automatically correct it. Data augmentation, which is what
1232400	1236360	we saw earlier. So that's like increasing the size of your data set. So you have more
1236360	1243080	training data. Feature engineering and selections. The idea here is you have, if anyone's familiar
1243080	1248200	like the early days of neural networks couldn't solve like the XOR problem, but you could
1248200	1253920	always just generate the XOR as another column. A more concrete example, if you haven't heard
1253960	1259480	of this, that one is you just have, you know, a bunch of tabular data. I worked on cheating
1259480	1266080	detection at MIT. And, you know, if you want the machine learning model to learn, say who
1266080	1272080	is a cheater from a bunch of, you know, education data, like where did they go to school and
1272080	1276480	what's their background and what problems did they answer. It can really help if you
1276480	1282240	also compute new features like how often did they submit answers within five seconds of
1282280	1286440	another student. So you can always generate new features and then you pass those into the
1286440	1290600	model and your model can, you know, if the features are relevant to the label you're
1290600	1294600	trying to predict, it can do a lot better. Yeah.
1294600	1303600	When we're doing outlier detection and removal, how do you know that an outlier is the result
1303600	1308920	of something that should not have happened or indication of a very rare event that you
1308960	1314960	should pay attention to? Yeah, that's a really fair and hard question to answer. You make
1314960	1320160	assumptions. And so in this case, I was making an assumption, right? I was saying, like, I
1320160	1323600	didn't draw very much data, but say that I had, like, millions of data points and then
1323600	1329120	this one's really far away. Then the assumption you're making is that, like, I have so much
1329120	1334000	data that suggests that this is the distribution and then I have very little data that suggests
1334000	1338760	that this is part of that distribution. And the biggest issue is that if your classifier
1338760	1343440	is changing dramatically for one data point, but you have, just think of it as, like, evidence.
1343440	1348280	I've got a thousand or a million people saying it should be this thing and then I've got
1348280	1352400	this one other person who may be right, they may be right, but they're saying that I think
1352400	1358320	it's this other thing and it greatly skews the classifier. And so just because you want
1358320	1363600	to trust the masses, you will say, hey, that one person is really, seems really off base.
1363600	1367440	And this is very much a choice. And so what typically, what you do is you sort of rank
1367440	1373360	every data point in terms of how in distribution it is and then you choose your cutoff and
1373360	1385240	that's a human decision. Another data-centric AI task is to establish consensus labels. So
1385240	1389920	if you guys have heard of, like, the self-driving cars, of course, then a lot of the ways that
1389960	1394000	these models are trained is you'll have an image and then they want really high-quality
1394000	1399200	data. So they'll have, like, 20 different people label, is this a scene of a street or
1399200	1403920	are we on a bridge? Is this a stop sign? Because they really need to get accurate labels. The
1403920	1408240	question is, when you're training your model, if you're just going to train with one label
1408240	1414600	for that image or one set of labels for that image, you can't use all 20 of your annotator's,
1414720	1420000	you know, guesses. You have to somehow combine them into a single training label. So how do
1420000	1425800	you do that in a way that maximizes model performance? Another one is active learning.
1425800	1431240	And this is a very classical problem. You have some data set, you train a model, it has 80%
1431240	1437040	performance. Okay, I want now to get my model to 85% performance, but I want to pay for as
1437040	1442320	little new data as possible. Or I want to improve the current existing data as little as possible.
1442640	1446960	What are my next steps? And you can automate that process. You can actually get good signal to
1446960	1451680	optimize in a way that you minimize the amount of new data that you need to collect information for
1451680	1458240	or label in order to achieve that model accuracy. And then a final example is curriculum learning,
1458240	1463280	which we already mentioned. So these are some examples of data-centric AI tasks, many of which
1463280	1468400	we'll cover over the next two weeks. All right, so there's a lot of hype around data-centric AI.
1468480	1474400	For those who are familiar with Andrew Ng, he's a pretty well-known person in AI from Stanford
1474400	1480640	and has been at Google Research and Baidi Research and done a lot of things found in Coursera.
1480640	1486560	So he's been really excited about data-centric AI. And let's look at some reasons, you know, why and
1486560	1491120	some of the things that we've seen in the news. For example, he mentioned that like 80% of an AI
1491120	1495520	developer's time is actually just spent on data, which is kind of funny, right? You know, you're
1495520	1500000	an AI developer, you're not like a data scientist, but yet you're doing data science work all the
1500000	1505840	time. And so there's something happening here in the real world that there isn't as much until
1505840	1511360	recently actually systematic, you know, learning and teaching around how do we go about doing this.
1512000	1518080	Also, if you're not familiar, bad data is very, very troublesome for businesses and for the
1518080	1523120	government and for economies. And it's estimated this is out of Harvard Business Review that it
1523120	1529040	cost the US alone about three trillion dollars in a given year. And you might see this and think,
1529040	1533280	okay, that's really bad. But the good news is like a lot of people think that we can actually solve
1533280	1538560	a lot of that three trillion issue that bad data causes with data-centric AI techniques.
1539120	1542560	And so there's a lot of hype around it because it means a lot to a lot of people.
1543440	1551120	This is a quick example. I did this internship at Fair in 2016 and I was in Jan's group and
1551200	1557120	Jeff Hinton came to visit. If you're not familiar, these two recently won the Nobel Prize of Computer
1557120	1562560	Science, which is called the Turing Award. And so I think they're old friends. And Jan has a dataset
1562560	1569120	MNIST. Are folks familiar with MNIST? Okay, cool. Very classical machine learning dataset and we've
1569120	1574880	been training models on it for like over 20 years. And people generally assume that it has perfect
1574880	1582160	labels because that's a very common assumption. Not maybe now in 2023, but definitely like when
1582160	1586080	it first came out. And it's a very high quality dataset. And so Jeff Hinton was presenting at
1586080	1590400	this time, I think, Capsule Networks. He's very excited about it. And his aha culminating moment
1590400	1595760	of his talk was that he found the label error in Jan Lacun's dataset. And so he's very excited to
1595760	1600880	show, hey, this five image is actually labeled a three in your dataset, Jan. And he's like, aha,
1600880	1607360	I got you. And so I think that it's worth mentioning that this is where we were in 2016. And now,
1607360	1612720	you know, we're only six, seven years later, and we're able to systematically find millions of
1612720	1618800	errors in datasets. And that's sort of how far we've come using these data-centric AI approaches.
1620160	1627280	Who here is familiar with Dolly and Dolly too? Yeah, it's pretty cool, right? So it generates images
1627280	1632000	and they're pretty cool, like you can generate images of pretty much anything you describe.
1632000	1636320	And so if you check out the Dolly demo page, and there's the link here in the slides, if you want
1636320	1641440	to check them after, there's a cool video. And you'll notice in that video, they talk about one of
1641440	1646160	their biggest challenges. And so this is just screenshots from the video. And the technology
1646160	1651360	is constantly evolving, but Dolly too has limitations. It's taught with objects that are
1651360	1656800	incorrectly labeled with plain labeled car, for example. And this happens because it's a
1656800	1662640	massive dataset. So if you don't use data-centric AI approaches, it's very difficult to clean, you
1662640	1669200	know, whatever, hundreds of millions or more, probably billions of pairs of text and image.
1669200	1674960	And so what they notice is that a user might actually try to generate a car, but Dolly will
1674960	1681200	actually create a plane, because it's seen wrongly labeled data. And so this is very problematic
1681200	1685040	for something that's deployed in the real world. Another example from that video is they talk about
1685040	1690960	generating these baboons, but they emphasize that you can only do this correctly if you have
1690960	1695920	accurate labels. And if you didn't, you'll totally, you can get the wrong thing. And so the key takeaway
1695920	1700720	here is that this is a real world technology, lots of people are using today at scale, but the
1700720	1707280	reliability of that model really does depend on the data quality. Another big example is familiar
1707360	1717760	with chat GPT. Does anybody know, yeah, does anybody know sort of why or like what was the
1717760	1724720	big innovation from GPT three, which obviously had a huge hype around it to chat GPT, which has
1724720	1729280	even more hype around it? What was sort of one of the big things they did between the two?
1733920	1737040	Yeah, totally. And do you know what they were doing with their reinforcement learning?
1737680	1744320	They like inviting some users to talk about these chat GPT and actually some of the work.
1745200	1752720	Yeah, totally. Yeah. What was happening there was they, they had a lot of bad outputs, you know,
1752720	1758240	like chat GPT three was saying things that were like super biased, or like inappropriate,
1759120	1764800	not even true, just wrong facts. And these outputs were tied to data it was trained on.
1764800	1769440	And to parameters in the model that were learned from that data. And so what they did is they did
1769440	1773680	in a reinforcement setting, which means they talked to people, they use that information
1773680	1778880	to then update the model, then the model sort of explores with new outputs, then people see those.
1778880	1783520	But what they were doing is they were having people actually rank them in terms of the quality
1783520	1788880	of the prediction, right? And so they were ranking the quality of this data and then using that to
1788880	1795040	update the model so that it would have improved less bias and better outputs. And so that was
1795040	1800720	the key idea of chat GPT was actually to deal with a data quality issue. And if you've tried both,
1800720	1805840	you can see that there is a pretty big performance boost. The downside is that they had to do this
1805840	1813040	with a lot of manual work. And so we went to work on ways to automate that. You guys are probably
1813040	1820080	familiar with Tesla. So this is Tesla's data engine. This is from a talk by Andre Carpathi,
1820080	1826160	who is formerly the Tesla director of AI. And this is their data engine. And we'll just start in
1826160	1832720	the top left, like you, the way they're training the, you know, the self-driving Tesla model is,
1832720	1836080	you know, you have some data source. And then you'll notice some problem, which is like, hey,
1836880	1842640	we're in a tunnel. And we don't have a lot of, you know, tunnel data. So the car is like
1842640	1847200	doing weird things, right? And so then what they would do is they would collect a bunch more data
1847200	1852800	in tunnels and then update their training data and label them and then redeploy and go in the
1852800	1857360	world and then see what breaks then. And this is a very, you know, difficult iterative process
1857360	1861520	because you have to send the car out and then see where things break and then collect a bunch more
1861520	1866160	data. Or if you had a way to automate, okay, this is where my data is missing. This is stuff that's
1866160	1869920	out of distribution. This is where we have a bunch of label errors. And you're able to automate
1869920	1873440	that process. They could have reduced those cycles and gotten the car out a lot faster,
1874240	1878960	at least the AI part of the car. And so this was a big pain that, that Andre mentioned.
1880000	1886480	Another really, this is a fantastic example that Jonas shared with me. These are all examples of
1886480	1891920	traffic lights. You know, so imagine that like Elon Musk comes to you and he says, you know,
1891920	1898560	Andre, I need you to get this car to navigate any street in the world. And then, you know,
1898560	1901840	you're like, oh, that's cool. Okay. So like it needs to stop at traffic lights. Like that seems
1901840	1906160	like a pretty simple problem. And then you go out in the real world and like traffic lights are
1906160	1910160	not a simple problem. They're really complicated and they're really messy. And this is actually
1910160	1915840	like a total nightmare if you had to do this. And so how do you find sort of systematic ways to
1915840	1920560	group data together and train in a way that's robust and reliable? Like real world data is super
1920560	1926560	messy and complicated. And so Andre's big takeaway was that, you know, he shares this juxtaposition
1926560	1932400	of the amount of sleep, you know, he lost over in his PhD. And it was like data sets is this tiny
1932400	1937280	sliver, but definitely spent a lot of time on models and algorithms. And then he goes and he's
1937280	1944000	leading, you know, the AI model at Tesla. And it's like, it's all data, you know, it's a big shock
1944000	1948400	when you make this shift. And so that's why we really want to focus on ways we can improve that.
1949440	1955920	A very common use case is when you're trying to train a model with noisy labels. Okay. So
1956000	1963040	this is like the classical scenario of your, you have the dogs and cats, but now say 30% of
1963040	1969360	your cats are labeled dog. Okay. And maybe 20% of the dogs are labeled cat. So how do you get a model
1969360	1974320	that does as well or close to as well as if you had perfectly labeled data? And we'll go into that
1974320	1980320	more in the next lecture. But I want to just motivate that I looked at a bunch of model
1980320	1985120	centric methods and data centric methods over the last five years out of top institutions
1985120	1990800	like Google and Facebook and so forth. And we benchmark them. And it turns out, and there's a
1990800	1995280	lot on this slide, but there's really one key takeaway that the data centric AI methods all
1995280	2000000	outperformed the model centric methods for this particular task, you know, on this particular
2000000	2005200	data set. And this was pretty revealing and compelling that there's something here to data
2005200	2010480	centric AI approaches. And to be very clear, what these models are doing is they are modifying the
2010480	2015760	loss function or modifying the model so that they sort of don't train as much on what they think is
2015760	2021360	bad data, but within the context of the modeling. And what these methods are doing is they're
2021360	2026000	actually modifying the data set. They're either removing bad data or they're generating more data
2026000	2030880	that sort of makes the error go away, but somehow they're actually changing the data set. And this
2030880	2035520	is just how things stack up. And you'll see the data centric methods outperform model centric
2035520	2040560	methods in this task. And this is a very common task that's of interest to the field. So it's
2040560	2046720	cool. It's cool to see that this stuff is working and we're getting some benefits from it. So a
2046720	2054080	sort of culminating thought is, you know, we were talking a lot about ways that we want to automate,
2054080	2060560	but what did we do before? So obviously we've had to improve data sets in industry and like outside
2060560	2064320	of academia in the past. It's like, how did we do it before there was data centric AI?
2065840	2072480	And so we mostly relied on human powered solutions. For example, we would just spend more money
2072480	2078000	for higher quality data. It's like you literally just pay for more labels or you would pay for more
2078000	2085280	data. And that was a very classical way to improve a model. Also building custom tools. So like you
2085280	2091600	saw at Tesla, they have this whole sort of data platform. And this is a lot, right? This is for
2091600	2095280	one specific problem, the very important problem, but they had to build a lot of custom tech around
2095520	2101360	it. Another common thing is just fixing data inside a Jupyter notebook. So just a quick show
2101360	2106640	of hands like how many people have used Jupyter notebooks. Okay, that's really good because
2106640	2111840	all the labs are in Jupyter notebooks for the most part for this lecture or for this course.
2111840	2118160	So yeah, what people do is like you just sort data by like a loss function. And so you just say like
2118160	2122480	the loss for this data point is the highest. So I think this is most likely to be wrong. Let me
2122480	2126800	check it out. And then you would look at it by hand and then you would market or do something with
2126800	2131440	it or you take the top 20 or something, but you would just do a lot of this by hand inside of
2131440	2135920	Jupyter notebooks and printing things out. And that was actually a pretty normal and standard way
2135920	2141280	that like, you know, somebody in industry or data scientist or grad student would try to fix a data
2141280	2147520	set. And so the whole idea is we're going to look at ways that we systematize these approaches
2147520	2151200	so that they're more reliable, more accurate, and they work on most data sets.
2153200	2157840	So this course is about the following. So today we're just looking at what is model
2157840	2162880	centric guy versus data centric guy, get the juices flowing, think about how to think things
2162880	2168080	in terms of data and the impact, why matters. Next lecture, we'll focus on label errors.
2168800	2173120	So how do you actually detect label errors automatically? How do you learn with label
2173120	2177120	errors? What are good methods to do that? And what are some things to think about when you're
2177120	2183360	doing this? Data set creation and curation will be on Thursday. And this is how do you construct
2183360	2187840	a data set in such a way that you can train a good model? How do you arrange, you know,
2187840	2193360	the classes? How do you choose good examples? And then finally on Friday, which is related to
2193360	2199840	data set curation, we'll look at active learning and potentially core sets and active learning.
2199840	2203760	As I mentioned, this is task where you're trying to choose the next data you want to add to your
2203760	2209120	data set and you want to obtain a label for. Or do you want to improve some of the labels you
2209120	2213680	currently have? And so you're just trying to decide, I have to pay a cost for new data that I'm going
2213680	2218480	to add to my data set. And it costs me something like either money or time. So I don't want to do
2218480	2222640	it too much. So what's like the best stuff to add to my data set now to improve my model?
2223520	2227360	Next week, we'll sort of have a bit of a shift and we'll focus more on data, but
2228000	2232880	we'll focus on some specific things for the first on Monday, we'll focus on class and balance and
2232880	2239040	distribution shift. So this is, imagine like it's the stock market, right? And like if you're
2239040	2244480	trying to predict things, you know, on Monday or on in January of this year versus now, it would be a
2244480	2251280	very different market, right? And so over time, data changes. And so how do you continue to produce
2251280	2256720	good reliable predictions even though data is changing? And then the class imbalances this
2256720	2261520	problem where imagine that for that line over there on the left, we had like a million pluses and
2261520	2268480	only a few minuses. Well, then a smart classifier could actually just always predict plus and get
2268480	2273440	near 100% accuracy. So often it will just ignore the minuses. So how do we get around that problem?
2275600	2280480	Interpretable features of data. So this is, who here's familiar with interpretability?
2283120	2287040	All right, not as much. That one will be fun then. And thanks for raising your hand.
2287840	2292880	That should be a good class. We'll learn about how do you interpret data in a way that you can
2292880	2297440	understand what's going on from a data perspective? Like why is the model doing what it's doing in
2297440	2302800	terms of the data? And so we'll understand model performance based on data. The next class on
2302800	2307680	Wednesday of next week will be on data-centric evaluation of ML models. So like, how do we know,
2307680	2312560	you know, from a data perspective, how good a model is, how reliable it is, how well it's working?
2313520	2319280	On Thursday, we'll look at encoding human priors. So this is like, how do we augment data and also
2319280	2324880	prompt engineering. So this class will cover a lot of things like GPT and chat GPT and
2324880	2329440	transformer models and stuff like that. And then finally, our last class will be on data privacy
2329440	2336160	and security. And this is very, very interesting, especially for a lot of people in like banking
2336160	2339680	and finance and they're using a lot of machine learning models. How do you make sure that like a
2339680	2345040	model doesn't actually secretly encode the data? Or like somehow if you had access to predictions,
2345040	2349360	you could figure out someone's, you know, banking info and you can imagine all sorts of things that
2349360	2355760	can happen in AI. So data security and privacy is really important. Any sort of questions while I'm
2355760	2370480	on this slide? Sweet. Are you guys excited? Okay. Does it look like a good course?
2373440	2381760	Okay. All right. Cool. All right. There's a lab for every lecture. And so you can find this on the
2381760	2389520	course website, which we can, we can write on the board. So you probably have seen it in the email,
2389520	2408080	but just in case. And there's, there's a lab, usually will be Jupyter notebooks. And the one
2408080	2414400	for today will be a text classification task. And it has some bad, bad data that's gotten mixed
2414400	2421280	in. And this is actual data that's been scraped from, I think, Amazon reviews. And it has some
2421280	2428080	bad tags, some weird HTML has gotten in there. And what you'll look at is model centric approaches
2428080	2432240	at first. And you'll realize, Oh, it can only get you so far because like the data is not that great.
2433120	2437920	And so you'll have to figure out how to improve the data set. You know, so you can get a
2437920	2443760	better classifier. And so that's sort of today's lab. Get your hands wet with data centric AI.
2444640	2452480	We have office hours every class, 3pm to 5pm. So an hour after the lecture ends every day.
2453280	2457200	And then tomorrow's lecture will focus on label errors, how to find them and how to train better
2457200	2463280	models. This is the folks who are teaching the course for folks that are from MIT,
2464240	2469360	to from Stanford. And yeah, I think it'll be a good time.
2470960	2473360	Really quick. Are there any questions?
2485200	2485440	Yeah.
2486400	2491280	My question is like, how do you know if it's like data that's the problem and not the model?
2491920	2496160	Let's say you train a bunch of times in a model with different parameters. It's not like
2496160	2501840	doing good. How do you know the data is a problem? Do you just try it and see if it improves or
2501840	2507760	do you can use it another way to see that? Yeah, that's a good question. So there's a few ways.
2507760	2514320	So one thing you can do is you can look at a subset of the problem. So say you had like
2514320	2518720	a very big complex problem and there's thousands of classes and millions of data points.
2518720	2522240	You can take just 10,000 of those data points and a few of those classes
2522960	2527200	and actually check, do some process to check, make sure you have really high quality data
2527200	2532320	and see how well does your model perform. And if your model is performing very, very high accuracy,
2532320	2536960	but then when you use the original data, you get a drop off. That gives you a good signal.
2536960	2542080	Another thing is if you have similar data set and it's like MNIST for example,
2542080	2546240	you can get near 100% performance. And so you have a similar data set, but you're getting
2546240	2551520	like much worse performance or like significantly less performance, but using the same architecture
2551520	2554800	that you've seen do very well on a similar task on a different data set.
2555360	2560000	And that's a good indicator. You should probably look at the data. And there's two more things.
2560000	2564800	One is just, just take a look at the data. I think it's really easy to just get a data set
2564800	2568560	and your goal is like train a model. And so you're doing a lot of cool, you know,
2568560	2572800	download this TensorFlow package, download this hugging face package, but you don't actually
2572800	2578400	take time usually to look through all the data points or like hundreds of data points and really
2578400	2583280	see like, does this data look like what I think it does? Like, does it seem to be kind of messy?
2583280	2587280	And what you'll often find is after first like, you know, first few hundred, you'll be like,
2587280	2591440	oh, there's some weird stuff in here. And if you have millions of data points, that weird stuff adds up.
2591440	2598160	Yeah, I guess the problem with the data is that there's like two months, so like, you know,
2598160	2602320	like how do you connect with all of them? Yeah, I guess if you just look at like a bunch.
2604320	2610000	Yeah, totally. In the next class, we'll show ways that you can actually rank your data set
2610640	2616240	so that you know what's the best example to look at first. That's probably wrong. And so there are
2616240	2620880	ways that you can automate this and then you can look at the initial data. And that's really the
2620880	2624800	right approach. Like if you just look at random, then you might waste some time. But if you've
2624800	2630240	ranked your data in a way that is likely to give you a good ranking on quality, and then you look
2630240	2635920	at like the first 100 and there's really no issues and the data looks really good, then yeah, you
2635920	2654640	might be able to just optimize the model and be okay. Any other questions?
2659840	2664400	Is this useful for anyone sort of immediately? Is anyone thinking like, hey, this might be useful
2664400	2666640	for what I'm working on right now?
2675600	2680000	I haven't done anything specific about it before, and I think one of the reasons is
2680640	2685600	because, you know, I always study models before. It seems like, you know, studying models is like
2685600	2689920	very important, but once you go and you want to like quickly do something, that's kind of like the
2689920	2694960	first thing that you want to do, you know, like, okay, what they do is data and how they work.
2698640	2700640	Yeah.
2713520	2718160	All right, great. We'll be here for a little bit after. Thanks everybody for coming.
2718160	2724080	The next lectures will be a bit more technical, but I think it should be a really exciting course,
2724080	2727840	and glad to have everybody here. Thanks.
