1
00:00:00,000 --> 00:00:10,000
Alright, let's get started. Thanks for everybody who showed up. We've got three of the course

2
00:00:10,000 --> 00:00:17,920
instructors here, myself, Anish in the back, and Jonas. And Anish is finishing up his final

3
00:00:17,920 --> 00:00:23,120
year, but we all were here. We all were students here. We all did our PhDs here. And it's good

4
00:00:23,120 --> 00:00:27,400
to be back. If you don't know any machine learning at all, you've never taken an intro

5
00:00:27,400 --> 00:00:32,520
class, then this will be probably a little too confusing. But as long as you've seen a

6
00:00:32,520 --> 00:00:37,800
little ML, then you'll have a good time. And ideally, you know a little bit of Python,

7
00:00:37,800 --> 00:00:43,960
and you know it like NumPy and Pandasar. So with that, let's just get started. So our

8
00:00:43,960 --> 00:00:49,360
goal in this course is to learn about data-centric AI. So just like a quick show of hands, prior

9
00:00:49,360 --> 00:00:57,800
to this course, how many people I've heard of data-centric AI? Okay, alright, well, get

10
00:00:57,800 --> 00:01:01,920
excited because you're going to learn some great things. It'll be really good. Alright,

11
00:01:01,920 --> 00:01:10,920
so let's just jump in. So this is a picture of a self-driving car that has had an accident.

12
00:01:10,920 --> 00:01:17,360
And you notice that the article focuses on when algorithms mess up, right? And this is

13
00:01:17,360 --> 00:01:20,760
the common case, right? In machine learning, we tend to focus on the model. And so it's

14
00:01:20,760 --> 00:01:26,360
like there's a crash, the algorithm must have done something wrong. And so I'd like us to

15
00:01:26,360 --> 00:01:31,360
sort of think of a different way of thinking about this. This is a paper many folks are

16
00:01:31,360 --> 00:01:36,520
familiar with, which basically shows one thing in that a neural network or a machine learning

17
00:01:36,520 --> 00:01:41,520
classifier can learn total randomness. It's like if you give it complete garbage data,

18
00:01:41,520 --> 00:01:45,960
just like completely random labels, it can learn to map like images to completely arbitrary

19
00:01:45,960 --> 00:01:50,680
labels or text to completely arbitrary labels. So basically, if you give it really bad data,

20
00:01:50,680 --> 00:01:55,520
it will just produce exactly what it learns, even if the data is completely wrong. And

21
00:01:55,520 --> 00:02:00,320
so I'd like to sort of rethink this title of this article as when algorithms are trained

22
00:02:00,320 --> 00:02:05,280
with erroneous data, things like car crashes can happen. And that's the way we'll sort

23
00:02:05,280 --> 00:02:11,880
of focus and think about this course. So traditional machine learning is very model-centric, right?

24
00:02:11,880 --> 00:02:16,080
Like when you take an ML class, you first learn machine learning, you're in school and

25
00:02:16,080 --> 00:02:20,160
then you get a data set. And usually the data set is pretty good. Like if anyone's seen

26
00:02:20,160 --> 00:02:25,840
like the cat dogs data set, you know, it's in images of cats and they're all cats and

27
00:02:25,840 --> 00:02:29,400
they're labeled cat. And then there's images of dog and they're all dogs and they're labeled

28
00:02:29,400 --> 00:02:34,440
dog and there's no, there's usually no like cows thrown in there, right? And there's usually

29
00:02:34,440 --> 00:02:38,880
not like a bunch of dogs that are labeled cat. It's like pretty well curated. And then

30
00:02:38,880 --> 00:02:43,520
your goal is to, you know, produce a good model, right? You want to train a model that

31
00:02:43,520 --> 00:02:48,760
takes in a new image that's either a cat or a dog and it predicts is it cat or is it dog.

32
00:02:48,760 --> 00:02:54,040
And that's sort of how we learn machine learning usually. And this is something that's standard

33
00:02:54,040 --> 00:03:00,120
if anyone here has taken like 6036, which has been renamed to 63390, the intro to ML class

34
00:03:00,120 --> 00:03:04,560
here. And you'll learn stuff like, you know, different types of models. And if you're familiar

35
00:03:04,560 --> 00:03:07,960
with neural networks, neural architectures, or you'll learn about tuning hyper parameters

36
00:03:07,960 --> 00:03:13,280
of a model, or you'll learn about modifying the loss function using different loss functions

37
00:03:13,280 --> 00:03:16,840
and regularizations that you don't overfit. And these are common things you learn in model

38
00:03:16,840 --> 00:03:23,000
centric AI. So let's just juxtapose that with the real world once you're out of the classroom.

39
00:03:23,000 --> 00:03:25,940
So in the classroom, usually the data sets fixed and it's pretty good, but then you

40
00:03:25,940 --> 00:03:30,200
go to the real world and actually the data set is not fixed, right? You have user data

41
00:03:30,200 --> 00:03:35,520
or customer data or real world data and you can get more or less, you can change the data

42
00:03:35,520 --> 00:03:41,000
and the data has all sorts of weird things in it. And so what tends to happen is that

43
00:03:41,000 --> 00:03:46,200
the company or the user, whoever's sort of using this model that you're trying to train,

44
00:03:46,200 --> 00:03:51,520
it doesn't really care as much about the cool ML fancy tricks as it does about like, does

45
00:03:51,520 --> 00:03:55,280
it actually work in the real world? And if you have really good machine learning models

46
00:03:55,280 --> 00:04:01,040
that work on highly curated data, but then the real world data is actually really messy,

47
00:04:01,040 --> 00:04:06,120
and it makes sense to actually focus on fixing the issues in the data. And a lot of people

48
00:04:06,120 --> 00:04:09,520
have been doing this, but what are systematic ways to do this? And that's what we'll focus

49
00:04:09,520 --> 00:04:15,080
on this course. What may surprise some of you that you may not know is that 10 of the

50
00:04:15,080 --> 00:04:21,840
most commonly cited and most used test sets in the field of machine learning all have

51
00:04:21,840 --> 00:04:28,080
wrong labels. And that may be a surprise for some of you. So we'll just take a quick look

52
00:04:28,080 --> 00:04:39,040
at this website, which I think will be pretty fun. This is labelairs.com. Can we see okay?

53
00:04:39,040 --> 00:04:43,160
So this is a site that you can check out on your own. So this is ImageNet, which is a

54
00:04:43,160 --> 00:04:48,200
common image data set. And these are in the test set, or in this case a validation set,

55
00:04:48,200 --> 00:04:52,400
which is what was released. But this is the data that is supposed to be the most accurate

56
00:04:52,400 --> 00:04:56,720
data, right? This is like your real world test data. And so this data should be some

57
00:04:56,720 --> 00:05:01,120
of the most highly curated, most accurate data. And what you'll find is that, for example,

58
00:05:01,120 --> 00:05:06,720
this scorpion is labeled tick. And you'll find all sorts of stuff. We can look at another

59
00:05:06,720 --> 00:05:13,560
data. This is by Google. This is a drawing hand drawn data set. This is labeled a t-shirt.

60
00:05:13,560 --> 00:05:18,960
And this is labeled a diving board. And this is labeled a scorpion. And it just keeps going

61
00:05:18,960 --> 00:05:23,320
and going. And there's some fun ones. Like this is a, this is totally a cake, but it's

62
00:05:23,360 --> 00:05:28,080
labeled a rake. Like it just didn't quite get the R to go all the way when they were

63
00:05:28,080 --> 00:05:31,720
pretending they're writing it out. But anyway, you can check these out on your own. This

64
00:05:31,720 --> 00:05:36,560
is for any type of data. So like text data or audio data. And you'll have fun. And I

65
00:05:36,560 --> 00:05:40,120
think the good idea to think about here is that this is, you know, a data set that's

66
00:05:40,120 --> 00:05:46,000
released by Google. It's a benchmark data set. And it's very difficult when you have

67
00:05:46,000 --> 00:05:50,920
millions of examples to know, like, what's the bad data? And ideally, if you didn't have

68
00:05:50,920 --> 00:05:55,080
that bad data, you could train better models. And so we need to learn how can we find these

69
00:05:55,080 --> 00:06:01,080
errors and find these issues automatically. So going back to the slides. So as seasoned

70
00:06:08,720 --> 00:06:11,960
data scientists, the way they'll approach this problem is that it's more worthwhile to

71
00:06:11,960 --> 00:06:15,920
invest in exploring and fixing the data than trying to tinker with the models to avoid this

72
00:06:15,920 --> 00:06:20,560
garbage in, garbage out issue. And the issue is that, like, if you have millions of data,

73
00:06:20,560 --> 00:06:23,960
right, or you have, like, a data set that's like 100 million, like, how do you do that

74
00:06:23,960 --> 00:06:28,600
without it being highly time consuming? All right. So we're here in a data-centric AI

75
00:06:28,600 --> 00:06:34,680
course. We've called this introduction to data-centric AI because we want it to be accessible. And

76
00:06:34,680 --> 00:06:38,320
so we're going to cover just, like, the very intro, what is data-centric AI? How does it

77
00:06:38,320 --> 00:06:43,680
differ from model-centric AI? And then we'll dive in a little deeper. So data-centric AI often

78
00:06:43,720 --> 00:06:49,040
takes one of two forms. So one form is that you have AI algorithms that understand something

79
00:06:49,040 --> 00:06:53,960
about data, and then they use this new information that they've understood to help a model train

80
00:06:53,960 --> 00:06:58,880
better. So an example of this is curriculum learning. And this is more of sort of something

81
00:06:58,880 --> 00:07:02,760
you'd encounter in grad school, so you may not have heard of it. But what curriculum,

82
00:07:02,760 --> 00:07:09,760
it's, you can check out the paper by Yasha Benjo. But the idea is you use a, some algorithm

83
00:07:09,840 --> 00:07:15,600
that looks at data and it identifies which data is probably easy to learn. And so the

84
00:07:15,600 --> 00:07:20,400
corollary here is imagine you're a student and you're in the classroom. All right. Should

85
00:07:20,400 --> 00:07:23,800
the teacher, what should they teach you? Like, should they teach you really, really hard

86
00:07:23,800 --> 00:07:28,240
examples as the very first examples? Like, if you're learning addition, should they start

87
00:07:28,240 --> 00:07:35,520
out with like 10,051 plus 1042 or would they start out with like one plus two? Right. Now

88
00:07:35,520 --> 00:07:38,960
we know this because, like, we've all learned addition. But when a machine learning model

89
00:07:39,000 --> 00:07:42,560
starts, it's starting from scratch. So it doesn't know right from the beginning what

90
00:07:42,560 --> 00:07:47,580
is the sort of easiest example. And so there are data-centric AI approaches that actually

91
00:07:47,580 --> 00:07:52,440
estimate what is the easiest example. And then when you train an ML model, you start

92
00:07:52,440 --> 00:07:56,000
with that example, and then you give it slightly harder ones and slightly harder them. And

93
00:07:56,000 --> 00:08:00,300
so it's like curriculum learning because it's like a student's curriculum. And so that's

94
00:08:00,300 --> 00:08:04,720
one way that you can sort of use new information. You're still training on all the same data,

95
00:08:04,720 --> 00:08:07,600
but you're just reordering it and using this additional information. You don't actually

96
00:08:07,600 --> 00:08:12,400
change the data set. Another sort of common form of data-centric AI is that you actually

97
00:08:12,400 --> 00:08:18,080
modify the data set to directly improve the performance of a model. So an example of this

98
00:08:18,080 --> 00:08:22,840
might be something like Confant Learning. And this is less known than curriculum learning,

99
00:08:22,840 --> 00:08:27,760
something I worked on. And this idea here is that you want to find what are label errors

100
00:08:27,760 --> 00:08:32,840
in a data set and then remove them prior to training, so that you just train on like correctly

101
00:08:32,840 --> 00:08:38,200
labeled stuff. And the corollary here in the sense of the student is that if your teacher

102
00:08:38,200 --> 00:08:43,160
is basically making mistakes 30% of the time, like imagine as I was teaching you, which

103
00:08:43,160 --> 00:08:48,240
hopefully I don't do today, that 30% of the time I told you wrong things. And then you

104
00:08:48,240 --> 00:08:52,760
compare that to another teacher who comes and they tell you right things 100% of the

105
00:08:52,760 --> 00:08:58,240
time. Then which teacher do you think you'll probably learn data-centric AI better from?

106
00:08:58,240 --> 00:09:03,280
And so the idea is that we want to take this bad sort of, you know, this 30% of wrong things

107
00:09:03,280 --> 00:09:06,480
and we want to get rid of them so that you could sort of come to the classroom and redo

108
00:09:06,480 --> 00:09:10,920
your learning experiences if those never happened. And that's the idea here. So you just learn

109
00:09:10,920 --> 00:09:17,080
on the good stuff. So those are two approaches. And then what we'll think about now is sort

110
00:09:17,080 --> 00:09:23,640
of what is the difference between model-centric AI and data-centric AI? So the sort of normal

111
00:09:23,640 --> 00:09:28,120
way you hear this is that given a data set, you try to produce the best model. And that's

112
00:09:28,160 --> 00:09:33,160
like the classical way of thinking about model-centric AI. And the idea is that you want to change

113
00:09:33,160 --> 00:09:39,360
the model somehow to improve, you know, some performance on AI task. And is there anyone

114
00:09:39,360 --> 00:09:44,000
here just out of curiosity who's not familiar with like AI tasks and some of the stuff we

115
00:09:44,000 --> 00:09:48,680
do in AI? Just want to make sure that we're on the same board, like classifying things.

116
00:09:48,680 --> 00:09:52,720
Okay, sweet. All right. So you've got some AI tasks and you're trying to classify something

117
00:09:52,720 --> 00:09:55,680
and the goal is you want to improve the performance on that. So usually it'll change the model

118
00:09:55,760 --> 00:10:02,760
to do that. And data-centric AI, instead, it's given some model, right, that may be fixed

119
00:10:02,760 --> 00:10:06,680
or you may change, but let's just assume it's fixed for now. You want to improve that model

120
00:10:06,680 --> 00:10:11,000
by improving your data set. So this is like the common way to think about the two. And

121
00:10:11,000 --> 00:10:15,400
the idea is to systematically or algorithmically not have a bunch of humans changing the data

122
00:10:15,400 --> 00:10:20,320
set, but like some algorithmic way that you can do this. Okay. All right. So our goal is

123
00:10:20,320 --> 00:10:25,360
to start thinking about ML in terms of data and not the model. And so we'll just start

124
00:10:25,360 --> 00:10:29,880
out with a simple example. And this is not data-centric AI, but it'll get us thinking

125
00:10:29,880 --> 00:10:35,480
about how we can think about AI in terms of data. So just quick show of hands. Who here

126
00:10:35,480 --> 00:10:42,120
is familiar with K nearest neighbors? Okay. Great. All right. So I won't belabor this

127
00:10:42,120 --> 00:10:54,400
point then. So do some chalk. That was a good thing to check beforehand. We were here last

128
00:10:54,400 --> 00:11:06,000
night, but the chalk has been removed. All right. So yeah, the key idea with K nearest

129
00:11:06,000 --> 00:11:11,400
neighbors and the key idea to think about in the context of data-centric learning is

130
00:11:11,400 --> 00:11:18,880
that K nearest neighbors, imagine you have, you know, a space, a 2D space, which, you

131
00:11:18,880 --> 00:11:35,160
know what, here's what we'll do. All right. So you've got some, all right, sweet. So you've

132
00:11:35,160 --> 00:11:41,960
got, say, you know, some data, you've got some triangles, and you've got some circles.

133
00:11:41,960 --> 00:11:47,720
You'll have to apologize for my drawing and you've got some squares. And so this is your

134
00:11:47,720 --> 00:11:51,440
data set. And let's say, you know, these are different types of images that you've put in

135
00:11:51,440 --> 00:11:55,480
some 2D space somehow where it's text that you've mapped a 2D space. And then the idea

136
00:11:55,480 --> 00:12:00,080
of K nearest neighbors, as many of you seem to know and are familiar with, is that you

137
00:12:00,080 --> 00:12:04,600
would have some new point, say, here. And with this, it's some weird thing. We don't know

138
00:12:04,600 --> 00:12:08,120
what it is. And we're trying to figure out, is it a triangle? Is it a circle? Is it a

139
00:12:08,120 --> 00:12:12,920
square? And so if this was sort of three K nearest neighbors, like then you would find

140
00:12:12,920 --> 00:12:19,760
the three nearest neighbors. So say this one, this one, and this one. And then can somebody

141
00:12:19,760 --> 00:12:28,880
tell me, like, what the class would be and why? Yeah, it'd be a square. That doesn't

142
00:12:28,880 --> 00:12:35,680
mean that it's not a cool point. And you're familiar with being a square. So this would

143
00:12:35,680 --> 00:12:39,360
be labeled a square, and that's based on majority voting. And there's a lot of different ways

144
00:12:39,360 --> 00:12:45,640
to do algorithms for deciding what the label should be. If it was, say, five neighbors,

145
00:12:45,640 --> 00:12:50,040
meaning five in N, where K is five, so it's K in N, then you would choose the five nearest

146
00:12:50,040 --> 00:12:53,920
neighbors and you would do a majority vote. Okay. The key idea here is that there is no

147
00:12:53,920 --> 00:12:59,760
loss function. Literally, any time you have a new point, so say I just have another new

148
00:12:59,760 --> 00:13:06,200
point here, I'm not sort of, there's no algorithm that I'm passing this into beyond just measuring

149
00:13:06,200 --> 00:13:10,440
a distance, some notion of distance between this and the nearest points. And you can do

150
00:13:10,440 --> 00:13:13,720
a bunch of pre-computation. There's a lot of smart work actually done in K nearest neighbors

151
00:13:13,720 --> 00:13:17,800
that is pretty impressive and you can do embeddings for all of these and pre-compute distances

152
00:13:17,800 --> 00:13:22,000
and you can do all sorts of fancy stuff. But ultimately, all this decision is based on

153
00:13:22,000 --> 00:13:28,160
is the data. And so I wanted to motivate this problem and this way of thinking because this

154
00:13:28,160 --> 00:13:33,880
whole decision process is made just based on data. And the quality of the data is as related

155
00:13:33,880 --> 00:13:38,840
as possible to the quality of the prediction. So if you have really good data, you're going

156
00:13:38,840 --> 00:13:42,160
to have really good predictions. And if you've got a bunch of errors in here, you're going

157
00:13:42,160 --> 00:13:47,200
to get the wrong prediction. And so this makes it really clear why fixing the data will make

158
00:13:47,200 --> 00:13:51,720
it, will improve your model. So is that kind of clear how this is motivating? Now this

159
00:13:51,720 --> 00:13:56,280
is not a data-centric AI algorithm. And by the end of this lecture, you'll definitely

160
00:13:56,280 --> 00:14:00,800
know what a data-centric AI algorithm is. But can someone tell me what the difference

161
00:14:00,800 --> 00:14:30,480
is between K and N and a data-centric AI algorithm? Yeah, yeah, totally. That's a great

162
00:14:31,480 --> 00:14:37,240
answer. K and N is just doing classification. And it's not actually modifying the data set.

163
00:14:37,240 --> 00:14:41,280
So yeah, that's exactly right. Alright, so what are a few other examples of what is not

164
00:14:41,280 --> 00:14:47,800
data-centric AI? Handpicking a bunch of points you think will improve a model. So can anybody

165
00:14:47,800 --> 00:14:55,920
help me understand why this is not data-centric AI? Yeah. Yeah, totally. It's just done by

166
00:14:55,920 --> 00:15:00,160
hand. Like this could, if you had a hundred million, a data set of a hundred million points,

167
00:15:00,160 --> 00:15:05,560
this would take a long time. What about doubling the size of your data set so that you can

168
00:15:05,560 --> 00:15:20,480
train an improved model? Yeah. Yeah, totally. This is just classical machine learning. It's

169
00:15:20,480 --> 00:15:23,680
still all the model, all the work you do as a model, but you're just paying more money

170
00:15:23,720 --> 00:15:28,680
for more data. So let's juxtapose this. So what would be the data-centric AI versions

171
00:15:28,680 --> 00:15:35,520
of this? And just out of curiosity, does anybody know for the first one what would be sort

172
00:15:35,520 --> 00:15:49,040
of the data-centric AI corollary? Yeah, totally. And there's a whole field of research on

173
00:15:49,040 --> 00:15:54,480
this and a whole subsection of ML that's called corset selection, where you have a data set

174
00:15:54,480 --> 00:15:59,000
and let's say that you train a model on that data set and you get like 98% accuracy. But

175
00:15:59,000 --> 00:16:03,600
the data set is like a hundred billion points. And so the goal is, can I find like, you know,

176
00:16:03,600 --> 00:16:08,680
a million points that if I just train on those, I can get like pretty close, like 97% accuracy.

177
00:16:08,680 --> 00:16:14,160
And that's corset selection. What about for number two? What would be like a data-centric

178
00:16:14,160 --> 00:16:25,640
AI corollary? Yeah, yeah. So the idea is, and how does anybody know of any ways that

179
00:16:25,640 --> 00:16:37,040
you might make your data set like bigger without just getting twice as much data? Yeah, totally.

180
00:16:37,040 --> 00:16:42,560
So like, say you have a, so data augmentation, awesome. And so say you have an image, you

181
00:16:42,560 --> 00:16:46,880
could just totally rotate that image. And now instead of one data point, you have like

182
00:16:46,880 --> 00:16:51,520
five data points or turn it black and white, or you could shift it or skew it or take the

183
00:16:51,520 --> 00:16:55,400
top and the bottom and move them a little bit or add some noise. There's lots of things

184
00:16:55,400 --> 00:16:59,360
you can do to make data bigger and actually improve a model just by changing the data

185
00:16:59,360 --> 00:17:05,320
set. And these are all fall within data-centric AI. Are you all familiar with what's called

186
00:17:05,320 --> 00:17:14,560
like back translation for text data augmentation? So this is a pretty fun thing. I don't like

187
00:17:14,560 --> 00:17:23,280
have particularly good translation skills. But like, if I say like, hi, you know, my

188
00:17:23,280 --> 00:17:46,680
name is Curtis. And then I translate this. Okay, so it becomes Ola. And then I translate

189
00:17:46,680 --> 00:17:59,520
this again. It might become, and now I've gotten one data point, and I just got another

190
00:17:59,520 --> 00:18:04,520
data point. So I was able to augment my text data set to get more versions of the same

191
00:18:04,520 --> 00:18:11,920
thing. And this could have some label, and the label could be introduction. You know,

192
00:18:11,920 --> 00:18:14,960
I'll just end it there because I know it's hard to see on that side. But this, this would

193
00:18:14,960 --> 00:18:22,200
be like the label. And this is text. And so this one is the same label. We haven't changed

194
00:18:22,200 --> 00:18:26,080
the label at all. So this is intro. And now you can see I have more labeled data, but

195
00:18:26,080 --> 00:18:29,280
I didn't have to do anything. I didn't have to pay more. I didn't have to, I just did

196
00:18:29,280 --> 00:18:30,680
this all computationally. Yeah.

197
00:18:30,680 --> 00:18:34,840
How do you see this in all the cases for that error to be taken?

198
00:18:34,840 --> 00:18:40,360
Oh yeah, totally can. So if, say that this label was wrong, now you have two label errors.

199
00:18:40,840 --> 00:18:45,880
Totally. Yeah. So you often want to combine two approaches. One is like first, try to

200
00:18:45,880 --> 00:18:50,120
fix or improve label errors before doing the augmentation. And that's a really good idea.

201
00:18:53,120 --> 00:18:56,400
All right. So what are some examples, we looked at examples that are not data-centric AI. So

202
00:18:56,400 --> 00:19:01,280
what are some things that are data-centric AI? And for this, I'll go through quick because

203
00:19:01,280 --> 00:19:06,680
we're going to learn all these in the course. And so one is outlier detection and removal.

204
00:19:06,680 --> 00:19:12,960
So I'll just be really quick to show you. So say you have some data set and we've got,

205
00:19:12,960 --> 00:19:23,960
let me use this board. And you've got, say, right? So you've got your sort of, your, you

206
00:19:23,960 --> 00:19:28,760
know, two classes. And so normally you would, you would draw your classifier and then you

207
00:19:28,760 --> 00:19:32,520
have some new point here and I'll be labeled a negative. But say in your training data,

208
00:19:32,520 --> 00:19:39,120
you have this really weird, you know, plus over here. And so maybe the boundary should

209
00:19:39,120 --> 00:19:42,520
be something like this, but you just don't have very much information and it's really

210
00:19:42,520 --> 00:19:47,840
out here and it seems like it's not very related to the rest of the data. And so what often

211
00:19:47,840 --> 00:19:51,400
people will do because they just don't have enough information is they'll identify this

212
00:19:51,400 --> 00:19:55,560
as an automatic, as an outlier because it seems very out of distribution and they'll

213
00:19:55,560 --> 00:19:59,040
remove it. And so then you get this line, which fits to all the data except for that

214
00:19:59,040 --> 00:20:07,440
one point. In terms of some other things in data-centric AI, so error detection and correction.

215
00:20:07,440 --> 00:20:12,200
And so that's, for example, you just have like a data point that is, it could be images,

216
00:20:12,200 --> 00:20:17,960
it's like all black, or you have a label error like we saw earlier. If this, you know, text

217
00:20:17,960 --> 00:20:24,360
example instead of being labeled intro was labeled like, you know, a goodbye clause or

218
00:20:25,320 --> 00:20:32,400
something. You'd want to find that and automatically correct it. Data augmentation, which is what

219
00:20:32,400 --> 00:20:36,360
we saw earlier. So that's like increasing the size of your data set. So you have more

220
00:20:36,360 --> 00:20:43,080
training data. Feature engineering and selections. The idea here is you have, if anyone's familiar

221
00:20:43,080 --> 00:20:48,200
like the early days of neural networks couldn't solve like the XOR problem, but you could

222
00:20:48,200 --> 00:20:53,920
always just generate the XOR as another column. A more concrete example, if you haven't heard

223
00:20:53,960 --> 00:20:59,480
of this, that one is you just have, you know, a bunch of tabular data. I worked on cheating

224
00:20:59,480 --> 00:21:06,080
detection at MIT. And, you know, if you want the machine learning model to learn, say who

225
00:21:06,080 --> 00:21:12,080
is a cheater from a bunch of, you know, education data, like where did they go to school and

226
00:21:12,080 --> 00:21:16,480
what's their background and what problems did they answer. It can really help if you

227
00:21:16,480 --> 00:21:22,240
also compute new features like how often did they submit answers within five seconds of

228
00:21:22,280 --> 00:21:26,440
another student. So you can always generate new features and then you pass those into the

229
00:21:26,440 --> 00:21:30,600
model and your model can, you know, if the features are relevant to the label you're

230
00:21:30,600 --> 00:21:34,600
trying to predict, it can do a lot better. Yeah.

231
00:21:34,600 --> 00:21:43,600
When we're doing outlier detection and removal, how do you know that an outlier is the result

232
00:21:43,600 --> 00:21:48,920
of something that should not have happened or indication of a very rare event that you

233
00:21:48,960 --> 00:21:54,960
should pay attention to? Yeah, that's a really fair and hard question to answer. You make

234
00:21:54,960 --> 00:22:00,160
assumptions. And so in this case, I was making an assumption, right? I was saying, like, I

235
00:22:00,160 --> 00:22:03,600
didn't draw very much data, but say that I had, like, millions of data points and then

236
00:22:03,600 --> 00:22:09,120
this one's really far away. Then the assumption you're making is that, like, I have so much

237
00:22:09,120 --> 00:22:14,000
data that suggests that this is the distribution and then I have very little data that suggests

238
00:22:14,000 --> 00:22:18,760
that this is part of that distribution. And the biggest issue is that if your classifier

239
00:22:18,760 --> 00:22:23,440
is changing dramatically for one data point, but you have, just think of it as, like, evidence.

240
00:22:23,440 --> 00:22:28,280
I've got a thousand or a million people saying it should be this thing and then I've got

241
00:22:28,280 --> 00:22:32,400
this one other person who may be right, they may be right, but they're saying that I think

242
00:22:32,400 --> 00:22:38,320
it's this other thing and it greatly skews the classifier. And so just because you want

243
00:22:38,320 --> 00:22:43,600
to trust the masses, you will say, hey, that one person is really, seems really off base.

244
00:22:43,600 --> 00:22:47,440
And this is very much a choice. And so what typically, what you do is you sort of rank

245
00:22:47,440 --> 00:22:53,360
every data point in terms of how in distribution it is and then you choose your cutoff and

246
00:22:53,360 --> 00:23:05,240
that's a human decision. Another data-centric AI task is to establish consensus labels. So

247
00:23:05,240 --> 00:23:09,920
if you guys have heard of, like, the self-driving cars, of course, then a lot of the ways that

248
00:23:09,960 --> 00:23:14,000
these models are trained is you'll have an image and then they want really high-quality

249
00:23:14,000 --> 00:23:19,200
data. So they'll have, like, 20 different people label, is this a scene of a street or

250
00:23:19,200 --> 00:23:23,920
are we on a bridge? Is this a stop sign? Because they really need to get accurate labels. The

251
00:23:23,920 --> 00:23:28,240
question is, when you're training your model, if you're just going to train with one label

252
00:23:28,240 --> 00:23:34,600
for that image or one set of labels for that image, you can't use all 20 of your annotator's,

253
00:23:34,720 --> 00:23:40,000
you know, guesses. You have to somehow combine them into a single training label. So how do

254
00:23:40,000 --> 00:23:45,800
you do that in a way that maximizes model performance? Another one is active learning.

255
00:23:45,800 --> 00:23:51,240
And this is a very classical problem. You have some data set, you train a model, it has 80%

256
00:23:51,240 --> 00:23:57,040
performance. Okay, I want now to get my model to 85% performance, but I want to pay for as

257
00:23:57,040 --> 00:24:02,320
little new data as possible. Or I want to improve the current existing data as little as possible.

258
00:24:02,640 --> 00:24:06,960
What are my next steps? And you can automate that process. You can actually get good signal to

259
00:24:06,960 --> 00:24:11,680
optimize in a way that you minimize the amount of new data that you need to collect information for

260
00:24:11,680 --> 00:24:18,240
or label in order to achieve that model accuracy. And then a final example is curriculum learning,

261
00:24:18,240 --> 00:24:23,280
which we already mentioned. So these are some examples of data-centric AI tasks, many of which

262
00:24:23,280 --> 00:24:28,400
we'll cover over the next two weeks. All right, so there's a lot of hype around data-centric AI.

263
00:24:28,480 --> 00:24:34,400
For those who are familiar with Andrew Ng, he's a pretty well-known person in AI from Stanford

264
00:24:34,400 --> 00:24:40,640
and has been at Google Research and Baidi Research and done a lot of things found in Coursera.

265
00:24:40,640 --> 00:24:46,560
So he's been really excited about data-centric AI. And let's look at some reasons, you know, why and

266
00:24:46,560 --> 00:24:51,120
some of the things that we've seen in the news. For example, he mentioned that like 80% of an AI

267
00:24:51,120 --> 00:24:55,520
developer's time is actually just spent on data, which is kind of funny, right? You know, you're

268
00:24:55,520 --> 00:25:00,000
an AI developer, you're not like a data scientist, but yet you're doing data science work all the

269
00:25:00,000 --> 00:25:05,840
time. And so there's something happening here in the real world that there isn't as much until

270
00:25:05,840 --> 00:25:11,360
recently actually systematic, you know, learning and teaching around how do we go about doing this.

271
00:25:12,000 --> 00:25:18,080
Also, if you're not familiar, bad data is very, very troublesome for businesses and for the

272
00:25:18,080 --> 00:25:23,120
government and for economies. And it's estimated this is out of Harvard Business Review that it

273
00:25:23,120 --> 00:25:29,040
cost the US alone about three trillion dollars in a given year. And you might see this and think,

274
00:25:29,040 --> 00:25:33,280
okay, that's really bad. But the good news is like a lot of people think that we can actually solve

275
00:25:33,280 --> 00:25:38,560
a lot of that three trillion issue that bad data causes with data-centric AI techniques.

276
00:25:39,120 --> 00:25:42,560
And so there's a lot of hype around it because it means a lot to a lot of people.

277
00:25:43,440 --> 00:25:51,120
This is a quick example. I did this internship at Fair in 2016 and I was in Jan's group and

278
00:25:51,200 --> 00:25:57,120
Jeff Hinton came to visit. If you're not familiar, these two recently won the Nobel Prize of Computer

279
00:25:57,120 --> 00:26:02,560
Science, which is called the Turing Award. And so I think they're old friends. And Jan has a dataset

280
00:26:02,560 --> 00:26:09,120
MNIST. Are folks familiar with MNIST? Okay, cool. Very classical machine learning dataset and we've

281
00:26:09,120 --> 00:26:14,880
been training models on it for like over 20 years. And people generally assume that it has perfect

282
00:26:14,880 --> 00:26:22,160
labels because that's a very common assumption. Not maybe now in 2023, but definitely like when

283
00:26:22,160 --> 00:26:26,080
it first came out. And it's a very high quality dataset. And so Jeff Hinton was presenting at

284
00:26:26,080 --> 00:26:30,400
this time, I think, Capsule Networks. He's very excited about it. And his aha culminating moment

285
00:26:30,400 --> 00:26:35,760
of his talk was that he found the label error in Jan Lacun's dataset. And so he's very excited to

286
00:26:35,760 --> 00:26:40,880
show, hey, this five image is actually labeled a three in your dataset, Jan. And he's like, aha,

287
00:26:40,880 --> 00:26:47,360
I got you. And so I think that it's worth mentioning that this is where we were in 2016. And now,

288
00:26:47,360 --> 00:26:52,720
you know, we're only six, seven years later, and we're able to systematically find millions of

289
00:26:52,720 --> 00:26:58,800
errors in datasets. And that's sort of how far we've come using these data-centric AI approaches.

290
00:27:00,160 --> 00:27:07,280
Who here is familiar with Dolly and Dolly too? Yeah, it's pretty cool, right? So it generates images

291
00:27:07,280 --> 00:27:12,000
and they're pretty cool, like you can generate images of pretty much anything you describe.

292
00:27:12,000 --> 00:27:16,320
And so if you check out the Dolly demo page, and there's the link here in the slides, if you want

293
00:27:16,320 --> 00:27:21,440
to check them after, there's a cool video. And you'll notice in that video, they talk about one of

294
00:27:21,440 --> 00:27:26,160
their biggest challenges. And so this is just screenshots from the video. And the technology

295
00:27:26,160 --> 00:27:31,360
is constantly evolving, but Dolly too has limitations. It's taught with objects that are

296
00:27:31,360 --> 00:27:36,800
incorrectly labeled with plain labeled car, for example. And this happens because it's a

297
00:27:36,800 --> 00:27:42,640
massive dataset. So if you don't use data-centric AI approaches, it's very difficult to clean, you

298
00:27:42,640 --> 00:27:49,200
know, whatever, hundreds of millions or more, probably billions of pairs of text and image.

299
00:27:49,200 --> 00:27:54,960
And so what they notice is that a user might actually try to generate a car, but Dolly will

300
00:27:54,960 --> 00:28:01,200
actually create a plane, because it's seen wrongly labeled data. And so this is very problematic

301
00:28:01,200 --> 00:28:05,040
for something that's deployed in the real world. Another example from that video is they talk about

302
00:28:05,040 --> 00:28:10,960
generating these baboons, but they emphasize that you can only do this correctly if you have

303
00:28:10,960 --> 00:28:15,920
accurate labels. And if you didn't, you'll totally, you can get the wrong thing. And so the key takeaway

304
00:28:15,920 --> 00:28:20,720
here is that this is a real world technology, lots of people are using today at scale, but the

305
00:28:20,720 --> 00:28:27,280
reliability of that model really does depend on the data quality. Another big example is familiar

306
00:28:27,360 --> 00:28:37,760
with chat GPT. Does anybody know, yeah, does anybody know sort of why or like what was the

307
00:28:37,760 --> 00:28:44,720
big innovation from GPT three, which obviously had a huge hype around it to chat GPT, which has

308
00:28:44,720 --> 00:28:49,280
even more hype around it? What was sort of one of the big things they did between the two?

309
00:28:53,920 --> 00:28:57,040
Yeah, totally. And do you know what they were doing with their reinforcement learning?

310
00:28:57,680 --> 00:29:04,320
They like inviting some users to talk about these chat GPT and actually some of the work.

311
00:29:05,200 --> 00:29:12,720
Yeah, totally. Yeah. What was happening there was they, they had a lot of bad outputs, you know,

312
00:29:12,720 --> 00:29:18,240
like chat GPT three was saying things that were like super biased, or like inappropriate,

313
00:29:19,120 --> 00:29:24,800
not even true, just wrong facts. And these outputs were tied to data it was trained on.

314
00:29:24,800 --> 00:29:29,440
And to parameters in the model that were learned from that data. And so what they did is they did

315
00:29:29,440 --> 00:29:33,680
in a reinforcement setting, which means they talked to people, they use that information

316
00:29:33,680 --> 00:29:38,880
to then update the model, then the model sort of explores with new outputs, then people see those.

317
00:29:38,880 --> 00:29:43,520
But what they were doing is they were having people actually rank them in terms of the quality

318
00:29:43,520 --> 00:29:48,880
of the prediction, right? And so they were ranking the quality of this data and then using that to

319
00:29:48,880 --> 00:29:55,040
update the model so that it would have improved less bias and better outputs. And so that was

320
00:29:55,040 --> 00:30:00,720
the key idea of chat GPT was actually to deal with a data quality issue. And if you've tried both,

321
00:30:00,720 --> 00:30:05,840
you can see that there is a pretty big performance boost. The downside is that they had to do this

322
00:30:05,840 --> 00:30:13,040
with a lot of manual work. And so we went to work on ways to automate that. You guys are probably

323
00:30:13,040 --> 00:30:20,080
familiar with Tesla. So this is Tesla's data engine. This is from a talk by Andre Carpathi,

324
00:30:20,080 --> 00:30:26,160
who is formerly the Tesla director of AI. And this is their data engine. And we'll just start in

325
00:30:26,160 --> 00:30:32,720
the top left, like you, the way they're training the, you know, the self-driving Tesla model is,

326
00:30:32,720 --> 00:30:36,080
you know, you have some data source. And then you'll notice some problem, which is like, hey,

327
00:30:36,880 --> 00:30:42,640
we're in a tunnel. And we don't have a lot of, you know, tunnel data. So the car is like

328
00:30:42,640 --> 00:30:47,200
doing weird things, right? And so then what they would do is they would collect a bunch more data

329
00:30:47,200 --> 00:30:52,800
in tunnels and then update their training data and label them and then redeploy and go in the

330
00:30:52,800 --> 00:30:57,360
world and then see what breaks then. And this is a very, you know, difficult iterative process

331
00:30:57,360 --> 00:31:01,520
because you have to send the car out and then see where things break and then collect a bunch more

332
00:31:01,520 --> 00:31:06,160
data. Or if you had a way to automate, okay, this is where my data is missing. This is stuff that's

333
00:31:06,160 --> 00:31:09,920
out of distribution. This is where we have a bunch of label errors. And you're able to automate

334
00:31:09,920 --> 00:31:13,440
that process. They could have reduced those cycles and gotten the car out a lot faster,

335
00:31:14,240 --> 00:31:18,960
at least the AI part of the car. And so this was a big pain that, that Andre mentioned.

336
00:31:20,000 --> 00:31:26,480
Another really, this is a fantastic example that Jonas shared with me. These are all examples of

337
00:31:26,480 --> 00:31:31,920
traffic lights. You know, so imagine that like Elon Musk comes to you and he says, you know,

338
00:31:31,920 --> 00:31:38,560
Andre, I need you to get this car to navigate any street in the world. And then, you know,

339
00:31:38,560 --> 00:31:41,840
you're like, oh, that's cool. Okay. So like it needs to stop at traffic lights. Like that seems

340
00:31:41,840 --> 00:31:46,160
like a pretty simple problem. And then you go out in the real world and like traffic lights are

341
00:31:46,160 --> 00:31:50,160
not a simple problem. They're really complicated and they're really messy. And this is actually

342
00:31:50,160 --> 00:31:55,840
like a total nightmare if you had to do this. And so how do you find sort of systematic ways to

343
00:31:55,840 --> 00:32:00,560
group data together and train in a way that's robust and reliable? Like real world data is super

344
00:32:00,560 --> 00:32:06,560
messy and complicated. And so Andre's big takeaway was that, you know, he shares this juxtaposition

345
00:32:06,560 --> 00:32:12,400
of the amount of sleep, you know, he lost over in his PhD. And it was like data sets is this tiny

346
00:32:12,400 --> 00:32:17,280
sliver, but definitely spent a lot of time on models and algorithms. And then he goes and he's

347
00:32:17,280 --> 00:32:24,000
leading, you know, the AI model at Tesla. And it's like, it's all data, you know, it's a big shock

348
00:32:24,000 --> 00:32:28,400
when you make this shift. And so that's why we really want to focus on ways we can improve that.

349
00:32:29,440 --> 00:32:35,920
A very common use case is when you're trying to train a model with noisy labels. Okay. So

350
00:32:36,000 --> 00:32:43,040
this is like the classical scenario of your, you have the dogs and cats, but now say 30% of

351
00:32:43,040 --> 00:32:49,360
your cats are labeled dog. Okay. And maybe 20% of the dogs are labeled cat. So how do you get a model

352
00:32:49,360 --> 00:32:54,320
that does as well or close to as well as if you had perfectly labeled data? And we'll go into that

353
00:32:54,320 --> 00:33:00,320
more in the next lecture. But I want to just motivate that I looked at a bunch of model

354
00:33:00,320 --> 00:33:05,120
centric methods and data centric methods over the last five years out of top institutions

355
00:33:05,120 --> 00:33:10,800
like Google and Facebook and so forth. And we benchmark them. And it turns out, and there's a

356
00:33:10,800 --> 00:33:15,280
lot on this slide, but there's really one key takeaway that the data centric AI methods all

357
00:33:15,280 --> 00:33:20,000
outperformed the model centric methods for this particular task, you know, on this particular

358
00:33:20,000 --> 00:33:25,200
data set. And this was pretty revealing and compelling that there's something here to data

359
00:33:25,200 --> 00:33:30,480
centric AI approaches. And to be very clear, what these models are doing is they are modifying the

360
00:33:30,480 --> 00:33:35,760
loss function or modifying the model so that they sort of don't train as much on what they think is

361
00:33:35,760 --> 00:33:41,360
bad data, but within the context of the modeling. And what these methods are doing is they're

362
00:33:41,360 --> 00:33:46,000
actually modifying the data set. They're either removing bad data or they're generating more data

363
00:33:46,000 --> 00:33:50,880
that sort of makes the error go away, but somehow they're actually changing the data set. And this

364
00:33:50,880 --> 00:33:55,520
is just how things stack up. And you'll see the data centric methods outperform model centric

365
00:33:55,520 --> 00:34:00,560
methods in this task. And this is a very common task that's of interest to the field. So it's

366
00:34:00,560 --> 00:34:06,720
cool. It's cool to see that this stuff is working and we're getting some benefits from it. So a

367
00:34:06,720 --> 00:34:14,080
sort of culminating thought is, you know, we were talking a lot about ways that we want to automate,

368
00:34:14,080 --> 00:34:20,560
but what did we do before? So obviously we've had to improve data sets in industry and like outside

369
00:34:20,560 --> 00:34:24,320
of academia in the past. It's like, how did we do it before there was data centric AI?

370
00:34:25,840 --> 00:34:32,480
And so we mostly relied on human powered solutions. For example, we would just spend more money

371
00:34:32,480 --> 00:34:38,000
for higher quality data. It's like you literally just pay for more labels or you would pay for more

372
00:34:38,000 --> 00:34:45,280
data. And that was a very classical way to improve a model. Also building custom tools. So like you

373
00:34:45,280 --> 00:34:51,600
saw at Tesla, they have this whole sort of data platform. And this is a lot, right? This is for

374
00:34:51,600 --> 00:34:55,280
one specific problem, the very important problem, but they had to build a lot of custom tech around

375
00:34:55,520 --> 00:35:01,360
it. Another common thing is just fixing data inside a Jupyter notebook. So just a quick show

376
00:35:01,360 --> 00:35:06,640
of hands like how many people have used Jupyter notebooks. Okay, that's really good because

377
00:35:06,640 --> 00:35:11,840
all the labs are in Jupyter notebooks for the most part for this lecture or for this course.

378
00:35:11,840 --> 00:35:18,160
So yeah, what people do is like you just sort data by like a loss function. And so you just say like

379
00:35:18,160 --> 00:35:22,480
the loss for this data point is the highest. So I think this is most likely to be wrong. Let me

380
00:35:22,480 --> 00:35:26,800
check it out. And then you would look at it by hand and then you would market or do something with

381
00:35:26,800 --> 00:35:31,440
it or you take the top 20 or something, but you would just do a lot of this by hand inside of

382
00:35:31,440 --> 00:35:35,920
Jupyter notebooks and printing things out. And that was actually a pretty normal and standard way

383
00:35:35,920 --> 00:35:41,280
that like, you know, somebody in industry or data scientist or grad student would try to fix a data

384
00:35:41,280 --> 00:35:47,520
set. And so the whole idea is we're going to look at ways that we systematize these approaches

385
00:35:47,520 --> 00:35:51,200
so that they're more reliable, more accurate, and they work on most data sets.

386
00:35:53,200 --> 00:35:57,840
So this course is about the following. So today we're just looking at what is model

387
00:35:57,840 --> 00:36:02,880
centric guy versus data centric guy, get the juices flowing, think about how to think things

388
00:36:02,880 --> 00:36:08,080
in terms of data and the impact, why matters. Next lecture, we'll focus on label errors.

389
00:36:08,800 --> 00:36:13,120
So how do you actually detect label errors automatically? How do you learn with label

390
00:36:13,120 --> 00:36:17,120
errors? What are good methods to do that? And what are some things to think about when you're

391
00:36:17,120 --> 00:36:23,360
doing this? Data set creation and curation will be on Thursday. And this is how do you construct

392
00:36:23,360 --> 00:36:27,840
a data set in such a way that you can train a good model? How do you arrange, you know,

393
00:36:27,840 --> 00:36:33,360
the classes? How do you choose good examples? And then finally on Friday, which is related to

394
00:36:33,360 --> 00:36:39,840
data set curation, we'll look at active learning and potentially core sets and active learning.

395
00:36:39,840 --> 00:36:43,760
As I mentioned, this is task where you're trying to choose the next data you want to add to your

396
00:36:43,760 --> 00:36:49,120
data set and you want to obtain a label for. Or do you want to improve some of the labels you

397
00:36:49,120 --> 00:36:53,680
currently have? And so you're just trying to decide, I have to pay a cost for new data that I'm going

398
00:36:53,680 --> 00:36:58,480
to add to my data set. And it costs me something like either money or time. So I don't want to do

399
00:36:58,480 --> 00:37:02,640
it too much. So what's like the best stuff to add to my data set now to improve my model?

400
00:37:03,520 --> 00:37:07,360
Next week, we'll sort of have a bit of a shift and we'll focus more on data, but

401
00:37:08,000 --> 00:37:12,880
we'll focus on some specific things for the first on Monday, we'll focus on class and balance and

402
00:37:12,880 --> 00:37:19,040
distribution shift. So this is, imagine like it's the stock market, right? And like if you're

403
00:37:19,040 --> 00:37:24,480
trying to predict things, you know, on Monday or on in January of this year versus now, it would be a

404
00:37:24,480 --> 00:37:31,280
very different market, right? And so over time, data changes. And so how do you continue to produce

405
00:37:31,280 --> 00:37:36,720
good reliable predictions even though data is changing? And then the class imbalances this

406
00:37:36,720 --> 00:37:41,520
problem where imagine that for that line over there on the left, we had like a million pluses and

407
00:37:41,520 --> 00:37:48,480
only a few minuses. Well, then a smart classifier could actually just always predict plus and get

408
00:37:48,480 --> 00:37:53,440
near 100% accuracy. So often it will just ignore the minuses. So how do we get around that problem?

409
00:37:55,600 --> 00:38:00,480
Interpretable features of data. So this is, who here's familiar with interpretability?

410
00:38:03,120 --> 00:38:07,040
All right, not as much. That one will be fun then. And thanks for raising your hand.

411
00:38:07,840 --> 00:38:12,880
That should be a good class. We'll learn about how do you interpret data in a way that you can

412
00:38:12,880 --> 00:38:17,440
understand what's going on from a data perspective? Like why is the model doing what it's doing in

413
00:38:17,440 --> 00:38:22,800
terms of the data? And so we'll understand model performance based on data. The next class on

414
00:38:22,800 --> 00:38:27,680
Wednesday of next week will be on data-centric evaluation of ML models. So like, how do we know,

415
00:38:27,680 --> 00:38:32,560
you know, from a data perspective, how good a model is, how reliable it is, how well it's working?

416
00:38:33,520 --> 00:38:39,280
On Thursday, we'll look at encoding human priors. So this is like, how do we augment data and also

417
00:38:39,280 --> 00:38:44,880
prompt engineering. So this class will cover a lot of things like GPT and chat GPT and

418
00:38:44,880 --> 00:38:49,440
transformer models and stuff like that. And then finally, our last class will be on data privacy

419
00:38:49,440 --> 00:38:56,160
and security. And this is very, very interesting, especially for a lot of people in like banking

420
00:38:56,160 --> 00:38:59,680
and finance and they're using a lot of machine learning models. How do you make sure that like a

421
00:38:59,680 --> 00:39:05,040
model doesn't actually secretly encode the data? Or like somehow if you had access to predictions,

422
00:39:05,040 --> 00:39:09,360
you could figure out someone's, you know, banking info and you can imagine all sorts of things that

423
00:39:09,360 --> 00:39:15,760
can happen in AI. So data security and privacy is really important. Any sort of questions while I'm

424
00:39:15,760 --> 00:39:30,480
on this slide? Sweet. Are you guys excited? Okay. Does it look like a good course?

425
00:39:33,440 --> 00:39:41,760
Okay. All right. Cool. All right. There's a lab for every lecture. And so you can find this on the

426
00:39:41,760 --> 00:39:49,520
course website, which we can, we can write on the board. So you probably have seen it in the email,

427
00:39:49,520 --> 00:40:08,080
but just in case. And there's, there's a lab, usually will be Jupyter notebooks. And the one

428
00:40:08,080 --> 00:40:14,400
for today will be a text classification task. And it has some bad, bad data that's gotten mixed

429
00:40:14,400 --> 00:40:21,280
in. And this is actual data that's been scraped from, I think, Amazon reviews. And it has some

430
00:40:21,280 --> 00:40:28,080
bad tags, some weird HTML has gotten in there. And what you'll look at is model centric approaches

431
00:40:28,080 --> 00:40:32,240
at first. And you'll realize, Oh, it can only get you so far because like the data is not that great.

432
00:40:33,120 --> 00:40:37,920
And so you'll have to figure out how to improve the data set. You know, so you can get a

433
00:40:37,920 --> 00:40:43,760
better classifier. And so that's sort of today's lab. Get your hands wet with data centric AI.

434
00:40:44,640 --> 00:40:52,480
We have office hours every class, 3pm to 5pm. So an hour after the lecture ends every day.

435
00:40:53,280 --> 00:40:57,200
And then tomorrow's lecture will focus on label errors, how to find them and how to train better

436
00:40:57,200 --> 00:41:03,280
models. This is the folks who are teaching the course for folks that are from MIT,

437
00:41:04,240 --> 00:41:09,360
to from Stanford. And yeah, I think it'll be a good time.

438
00:41:10,960 --> 00:41:13,360
Really quick. Are there any questions?

439
00:41:25,200 --> 00:41:25,440
Yeah.

440
00:41:26,400 --> 00:41:31,280
My question is like, how do you know if it's like data that's the problem and not the model?

441
00:41:31,920 --> 00:41:36,160
Let's say you train a bunch of times in a model with different parameters. It's not like

442
00:41:36,160 --> 00:41:41,840
doing good. How do you know the data is a problem? Do you just try it and see if it improves or

443
00:41:41,840 --> 00:41:47,760
do you can use it another way to see that? Yeah, that's a good question. So there's a few ways.

444
00:41:47,760 --> 00:41:54,320
So one thing you can do is you can look at a subset of the problem. So say you had like

445
00:41:54,320 --> 00:41:58,720
a very big complex problem and there's thousands of classes and millions of data points.

446
00:41:58,720 --> 00:42:02,240
You can take just 10,000 of those data points and a few of those classes

447
00:42:02,960 --> 00:42:07,200
and actually check, do some process to check, make sure you have really high quality data

448
00:42:07,200 --> 00:42:12,320
and see how well does your model perform. And if your model is performing very, very high accuracy,

449
00:42:12,320 --> 00:42:16,960
but then when you use the original data, you get a drop off. That gives you a good signal.

450
00:42:16,960 --> 00:42:22,080
Another thing is if you have similar data set and it's like MNIST for example,

451
00:42:22,080 --> 00:42:26,240
you can get near 100% performance. And so you have a similar data set, but you're getting

452
00:42:26,240 --> 00:42:31,520
like much worse performance or like significantly less performance, but using the same architecture

453
00:42:31,520 --> 00:42:34,800
that you've seen do very well on a similar task on a different data set.

454
00:42:35,360 --> 00:42:40,000
And that's a good indicator. You should probably look at the data. And there's two more things.

455
00:42:40,000 --> 00:42:44,800
One is just, just take a look at the data. I think it's really easy to just get a data set

456
00:42:44,800 --> 00:42:48,560
and your goal is like train a model. And so you're doing a lot of cool, you know,

457
00:42:48,560 --> 00:42:52,800
download this TensorFlow package, download this hugging face package, but you don't actually

458
00:42:52,800 --> 00:42:58,400
take time usually to look through all the data points or like hundreds of data points and really

459
00:42:58,400 --> 00:43:03,280
see like, does this data look like what I think it does? Like, does it seem to be kind of messy?

460
00:43:03,280 --> 00:43:07,280
And what you'll often find is after first like, you know, first few hundred, you'll be like,

461
00:43:07,280 --> 00:43:11,440
oh, there's some weird stuff in here. And if you have millions of data points, that weird stuff adds up.

462
00:43:11,440 --> 00:43:18,160
Yeah, I guess the problem with the data is that there's like two months, so like, you know,

463
00:43:18,160 --> 00:43:22,320
like how do you connect with all of them? Yeah, I guess if you just look at like a bunch.

464
00:43:24,320 --> 00:43:30,000
Yeah, totally. In the next class, we'll show ways that you can actually rank your data set

465
00:43:30,640 --> 00:43:36,240
so that you know what's the best example to look at first. That's probably wrong. And so there are

466
00:43:36,240 --> 00:43:40,880
ways that you can automate this and then you can look at the initial data. And that's really the

467
00:43:40,880 --> 00:43:44,800
right approach. Like if you just look at random, then you might waste some time. But if you've

468
00:43:44,800 --> 00:43:50,240
ranked your data in a way that is likely to give you a good ranking on quality, and then you look

469
00:43:50,240 --> 00:43:55,920
at like the first 100 and there's really no issues and the data looks really good, then yeah, you

470
00:43:55,920 --> 00:44:14,640
might be able to just optimize the model and be okay. Any other questions?

471
00:44:19,840 --> 00:44:24,400
Is this useful for anyone sort of immediately? Is anyone thinking like, hey, this might be useful

472
00:44:24,400 --> 00:44:26,640
for what I'm working on right now?

473
00:44:35,600 --> 00:44:40,000
I haven't done anything specific about it before, and I think one of the reasons is

474
00:44:40,640 --> 00:44:45,600
because, you know, I always study models before. It seems like, you know, studying models is like

475
00:44:45,600 --> 00:44:49,920
very important, but once you go and you want to like quickly do something, that's kind of like the

476
00:44:49,920 --> 00:44:54,960
first thing that you want to do, you know, like, okay, what they do is data and how they work.

477
00:44:58,640 --> 00:45:00,640
Yeah.

478
00:45:13,520 --> 00:45:18,160
All right, great. We'll be here for a little bit after. Thanks everybody for coming.

479
00:45:18,160 --> 00:45:24,080
The next lectures will be a bit more technical, but I think it should be a really exciting course,

480
00:45:24,080 --> 00:45:27,840
and glad to have everybody here. Thanks.

