WEBVTT

00:00.000 --> 00:10.000
Alright, let's get started. Thanks for everybody who showed up. We've got three of the course

00:10.000 --> 00:17.920
instructors here, myself, Anish in the back, and Jonas. And Anish is finishing up his final

00:17.920 --> 00:23.120
year, but we all were here. We all were students here. We all did our PhDs here. And it's good

00:23.120 --> 00:27.400
to be back. If you don't know any machine learning at all, you've never taken an intro

00:27.400 --> 00:32.520
class, then this will be probably a little too confusing. But as long as you've seen a

00:32.520 --> 00:37.800
little ML, then you'll have a good time. And ideally, you know a little bit of Python,

00:37.800 --> 00:43.960
and you know it like NumPy and Pandasar. So with that, let's just get started. So our

00:43.960 --> 00:49.360
goal in this course is to learn about data-centric AI. So just like a quick show of hands, prior

00:49.360 --> 00:57.800
to this course, how many people I've heard of data-centric AI? Okay, alright, well, get

00:57.800 --> 01:01.920
excited because you're going to learn some great things. It'll be really good. Alright,

01:01.920 --> 01:10.920
so let's just jump in. So this is a picture of a self-driving car that has had an accident.

01:10.920 --> 01:17.360
And you notice that the article focuses on when algorithms mess up, right? And this is

01:17.360 --> 01:20.760
the common case, right? In machine learning, we tend to focus on the model. And so it's

01:20.760 --> 01:26.360
like there's a crash, the algorithm must have done something wrong. And so I'd like us to

01:26.360 --> 01:31.360
sort of think of a different way of thinking about this. This is a paper many folks are

01:31.360 --> 01:36.520
familiar with, which basically shows one thing in that a neural network or a machine learning

01:36.520 --> 01:41.520
classifier can learn total randomness. It's like if you give it complete garbage data,

01:41.520 --> 01:45.960
just like completely random labels, it can learn to map like images to completely arbitrary

01:45.960 --> 01:50.680
labels or text to completely arbitrary labels. So basically, if you give it really bad data,

01:50.680 --> 01:55.520
it will just produce exactly what it learns, even if the data is completely wrong. And

01:55.520 --> 02:00.320
so I'd like to sort of rethink this title of this article as when algorithms are trained

02:00.320 --> 02:05.280
with erroneous data, things like car crashes can happen. And that's the way we'll sort

02:05.280 --> 02:11.880
of focus and think about this course. So traditional machine learning is very model-centric, right?

02:11.880 --> 02:16.080
Like when you take an ML class, you first learn machine learning, you're in school and

02:16.080 --> 02:20.160
then you get a data set. And usually the data set is pretty good. Like if anyone's seen

02:20.160 --> 02:25.840
like the cat dogs data set, you know, it's in images of cats and they're all cats and

02:25.840 --> 02:29.400
they're labeled cat. And then there's images of dog and they're all dogs and they're labeled

02:29.400 --> 02:34.440
dog and there's no, there's usually no like cows thrown in there, right? And there's usually

02:34.440 --> 02:38.880
not like a bunch of dogs that are labeled cat. It's like pretty well curated. And then

02:38.880 --> 02:43.520
your goal is to, you know, produce a good model, right? You want to train a model that

02:43.520 --> 02:48.760
takes in a new image that's either a cat or a dog and it predicts is it cat or is it dog.

02:48.760 --> 02:54.040
And that's sort of how we learn machine learning usually. And this is something that's standard

02:54.040 --> 03:00.120
if anyone here has taken like 6036, which has been renamed to 63390, the intro to ML class

03:00.120 --> 03:04.560
here. And you'll learn stuff like, you know, different types of models. And if you're familiar

03:04.560 --> 03:07.960
with neural networks, neural architectures, or you'll learn about tuning hyper parameters

03:07.960 --> 03:13.280
of a model, or you'll learn about modifying the loss function using different loss functions

03:13.280 --> 03:16.840
and regularizations that you don't overfit. And these are common things you learn in model

03:16.840 --> 03:23.000
centric AI. So let's just juxtapose that with the real world once you're out of the classroom.

03:23.000 --> 03:25.940
So in the classroom, usually the data sets fixed and it's pretty good, but then you

03:25.940 --> 03:30.200
go to the real world and actually the data set is not fixed, right? You have user data

03:30.200 --> 03:35.520
or customer data or real world data and you can get more or less, you can change the data

03:35.520 --> 03:41.000
and the data has all sorts of weird things in it. And so what tends to happen is that

03:41.000 --> 03:46.200
the company or the user, whoever's sort of using this model that you're trying to train,

03:46.200 --> 03:51.520
it doesn't really care as much about the cool ML fancy tricks as it does about like, does

03:51.520 --> 03:55.280
it actually work in the real world? And if you have really good machine learning models

03:55.280 --> 04:01.040
that work on highly curated data, but then the real world data is actually really messy,

04:01.040 --> 04:06.120
and it makes sense to actually focus on fixing the issues in the data. And a lot of people

04:06.120 --> 04:09.520
have been doing this, but what are systematic ways to do this? And that's what we'll focus

04:09.520 --> 04:15.080
on this course. What may surprise some of you that you may not know is that 10 of the

04:15.080 --> 04:21.840
most commonly cited and most used test sets in the field of machine learning all have

04:21.840 --> 04:28.080
wrong labels. And that may be a surprise for some of you. So we'll just take a quick look

04:28.080 --> 04:39.040
at this website, which I think will be pretty fun. This is labelairs.com. Can we see okay?

04:39.040 --> 04:43.160
So this is a site that you can check out on your own. So this is ImageNet, which is a

04:43.160 --> 04:48.200
common image data set. And these are in the test set, or in this case a validation set,

04:48.200 --> 04:52.400
which is what was released. But this is the data that is supposed to be the most accurate

04:52.400 --> 04:56.720
data, right? This is like your real world test data. And so this data should be some

04:56.720 --> 05:01.120
of the most highly curated, most accurate data. And what you'll find is that, for example,

05:01.120 --> 05:06.720
this scorpion is labeled tick. And you'll find all sorts of stuff. We can look at another

05:06.720 --> 05:13.560
data. This is by Google. This is a drawing hand drawn data set. This is labeled a t-shirt.

05:13.560 --> 05:18.960
And this is labeled a diving board. And this is labeled a scorpion. And it just keeps going

05:18.960 --> 05:23.320
and going. And there's some fun ones. Like this is a, this is totally a cake, but it's

05:23.360 --> 05:28.080
labeled a rake. Like it just didn't quite get the R to go all the way when they were

05:28.080 --> 05:31.720
pretending they're writing it out. But anyway, you can check these out on your own. This

05:31.720 --> 05:36.560
is for any type of data. So like text data or audio data. And you'll have fun. And I

05:36.560 --> 05:40.120
think the good idea to think about here is that this is, you know, a data set that's

05:40.120 --> 05:46.000
released by Google. It's a benchmark data set. And it's very difficult when you have

05:46.000 --> 05:50.920
millions of examples to know, like, what's the bad data? And ideally, if you didn't have

05:50.920 --> 05:55.080
that bad data, you could train better models. And so we need to learn how can we find these

05:55.080 --> 06:01.080
errors and find these issues automatically. So going back to the slides. So as seasoned

06:08.720 --> 06:11.960
data scientists, the way they'll approach this problem is that it's more worthwhile to

06:11.960 --> 06:15.920
invest in exploring and fixing the data than trying to tinker with the models to avoid this

06:15.920 --> 06:20.560
garbage in, garbage out issue. And the issue is that, like, if you have millions of data,

06:20.560 --> 06:23.960
right, or you have, like, a data set that's like 100 million, like, how do you do that

06:23.960 --> 06:28.600
without it being highly time consuming? All right. So we're here in a data-centric AI

06:28.600 --> 06:34.680
course. We've called this introduction to data-centric AI because we want it to be accessible. And

06:34.680 --> 06:38.320
so we're going to cover just, like, the very intro, what is data-centric AI? How does it

06:38.320 --> 06:43.680
differ from model-centric AI? And then we'll dive in a little deeper. So data-centric AI often

06:43.720 --> 06:49.040
takes one of two forms. So one form is that you have AI algorithms that understand something

06:49.040 --> 06:53.960
about data, and then they use this new information that they've understood to help a model train

06:53.960 --> 06:58.880
better. So an example of this is curriculum learning. And this is more of sort of something

06:58.880 --> 07:02.760
you'd encounter in grad school, so you may not have heard of it. But what curriculum,

07:02.760 --> 07:09.760
it's, you can check out the paper by Yasha Benjo. But the idea is you use a, some algorithm

07:09.840 --> 07:15.600
that looks at data and it identifies which data is probably easy to learn. And so the

07:15.600 --> 07:20.400
corollary here is imagine you're a student and you're in the classroom. All right. Should

07:20.400 --> 07:23.800
the teacher, what should they teach you? Like, should they teach you really, really hard

07:23.800 --> 07:28.240
examples as the very first examples? Like, if you're learning addition, should they start

07:28.240 --> 07:35.520
out with like 10,051 plus 1042 or would they start out with like one plus two? Right. Now

07:35.520 --> 07:38.960
we know this because, like, we've all learned addition. But when a machine learning model

07:39.000 --> 07:42.560
starts, it's starting from scratch. So it doesn't know right from the beginning what

07:42.560 --> 07:47.580
is the sort of easiest example. And so there are data-centric AI approaches that actually

07:47.580 --> 07:52.440
estimate what is the easiest example. And then when you train an ML model, you start

07:52.440 --> 07:56.000
with that example, and then you give it slightly harder ones and slightly harder them. And

07:56.000 --> 08:00.300
so it's like curriculum learning because it's like a student's curriculum. And so that's

08:00.300 --> 08:04.720
one way that you can sort of use new information. You're still training on all the same data,

08:04.720 --> 08:07.600
but you're just reordering it and using this additional information. You don't actually

08:07.600 --> 08:12.400
change the data set. Another sort of common form of data-centric AI is that you actually

08:12.400 --> 08:18.080
modify the data set to directly improve the performance of a model. So an example of this

08:18.080 --> 08:22.840
might be something like Confant Learning. And this is less known than curriculum learning,

08:22.840 --> 08:27.760
something I worked on. And this idea here is that you want to find what are label errors

08:27.760 --> 08:32.840
in a data set and then remove them prior to training, so that you just train on like correctly

08:32.840 --> 08:38.200
labeled stuff. And the corollary here in the sense of the student is that if your teacher

08:38.200 --> 08:43.160
is basically making mistakes 30% of the time, like imagine as I was teaching you, which

08:43.160 --> 08:48.240
hopefully I don't do today, that 30% of the time I told you wrong things. And then you

08:48.240 --> 08:52.760
compare that to another teacher who comes and they tell you right things 100% of the

08:52.760 --> 08:58.240
time. Then which teacher do you think you'll probably learn data-centric AI better from?

08:58.240 --> 09:03.280
And so the idea is that we want to take this bad sort of, you know, this 30% of wrong things

09:03.280 --> 09:06.480
and we want to get rid of them so that you could sort of come to the classroom and redo

09:06.480 --> 09:10.920
your learning experiences if those never happened. And that's the idea here. So you just learn

09:10.920 --> 09:17.080
on the good stuff. So those are two approaches. And then what we'll think about now is sort

09:17.080 --> 09:23.640
of what is the difference between model-centric AI and data-centric AI? So the sort of normal

09:23.640 --> 09:28.120
way you hear this is that given a data set, you try to produce the best model. And that's

09:28.160 --> 09:33.160
like the classical way of thinking about model-centric AI. And the idea is that you want to change

09:33.160 --> 09:39.360
the model somehow to improve, you know, some performance on AI task. And is there anyone

09:39.360 --> 09:44.000
here just out of curiosity who's not familiar with like AI tasks and some of the stuff we

09:44.000 --> 09:48.680
do in AI? Just want to make sure that we're on the same board, like classifying things.

09:48.680 --> 09:52.720
Okay, sweet. All right. So you've got some AI tasks and you're trying to classify something

09:52.720 --> 09:55.680
and the goal is you want to improve the performance on that. So usually it'll change the model

09:55.760 --> 10:02.760
to do that. And data-centric AI, instead, it's given some model, right, that may be fixed

10:02.760 --> 10:06.680
or you may change, but let's just assume it's fixed for now. You want to improve that model

10:06.680 --> 10:11.000
by improving your data set. So this is like the common way to think about the two. And

10:11.000 --> 10:15.400
the idea is to systematically or algorithmically not have a bunch of humans changing the data

10:15.400 --> 10:20.320
set, but like some algorithmic way that you can do this. Okay. All right. So our goal is

10:20.320 --> 10:25.360
to start thinking about ML in terms of data and not the model. And so we'll just start

10:25.360 --> 10:29.880
out with a simple example. And this is not data-centric AI, but it'll get us thinking

10:29.880 --> 10:35.480
about how we can think about AI in terms of data. So just quick show of hands. Who here

10:35.480 --> 10:42.120
is familiar with K nearest neighbors? Okay. Great. All right. So I won't belabor this

10:42.120 --> 10:54.400
point then. So do some chalk. That was a good thing to check beforehand. We were here last

10:54.400 --> 11:06.000
night, but the chalk has been removed. All right. So yeah, the key idea with K nearest

11:06.000 --> 11:11.400
neighbors and the key idea to think about in the context of data-centric learning is

11:11.400 --> 11:18.880
that K nearest neighbors, imagine you have, you know, a space, a 2D space, which, you

11:18.880 --> 11:35.160
know what, here's what we'll do. All right. So you've got some, all right, sweet. So you've

11:35.160 --> 11:41.960
got, say, you know, some data, you've got some triangles, and you've got some circles.

11:41.960 --> 11:47.720
You'll have to apologize for my drawing and you've got some squares. And so this is your

11:47.720 --> 11:51.440
data set. And let's say, you know, these are different types of images that you've put in

11:51.440 --> 11:55.480
some 2D space somehow where it's text that you've mapped a 2D space. And then the idea

11:55.480 --> 12:00.080
of K nearest neighbors, as many of you seem to know and are familiar with, is that you

12:00.080 --> 12:04.600
would have some new point, say, here. And with this, it's some weird thing. We don't know

12:04.600 --> 12:08.120
what it is. And we're trying to figure out, is it a triangle? Is it a circle? Is it a

12:08.120 --> 12:12.920
square? And so if this was sort of three K nearest neighbors, like then you would find

12:12.920 --> 12:19.760
the three nearest neighbors. So say this one, this one, and this one. And then can somebody

12:19.760 --> 12:28.880
tell me, like, what the class would be and why? Yeah, it'd be a square. That doesn't

12:28.880 --> 12:35.680
mean that it's not a cool point. And you're familiar with being a square. So this would

12:35.680 --> 12:39.360
be labeled a square, and that's based on majority voting. And there's a lot of different ways

12:39.360 --> 12:45.640
to do algorithms for deciding what the label should be. If it was, say, five neighbors,

12:45.640 --> 12:50.040
meaning five in N, where K is five, so it's K in N, then you would choose the five nearest

12:50.040 --> 12:53.920
neighbors and you would do a majority vote. Okay. The key idea here is that there is no

12:53.920 --> 12:59.760
loss function. Literally, any time you have a new point, so say I just have another new

12:59.760 --> 13:06.200
point here, I'm not sort of, there's no algorithm that I'm passing this into beyond just measuring

13:06.200 --> 13:10.440
a distance, some notion of distance between this and the nearest points. And you can do

13:10.440 --> 13:13.720
a bunch of pre-computation. There's a lot of smart work actually done in K nearest neighbors

13:13.720 --> 13:17.800
that is pretty impressive and you can do embeddings for all of these and pre-compute distances

13:17.800 --> 13:22.000
and you can do all sorts of fancy stuff. But ultimately, all this decision is based on

13:22.000 --> 13:28.160
is the data. And so I wanted to motivate this problem and this way of thinking because this

13:28.160 --> 13:33.880
whole decision process is made just based on data. And the quality of the data is as related

13:33.880 --> 13:38.840
as possible to the quality of the prediction. So if you have really good data, you're going

13:38.840 --> 13:42.160
to have really good predictions. And if you've got a bunch of errors in here, you're going

13:42.160 --> 13:47.200
to get the wrong prediction. And so this makes it really clear why fixing the data will make

13:47.200 --> 13:51.720
it, will improve your model. So is that kind of clear how this is motivating? Now this

13:51.720 --> 13:56.280
is not a data-centric AI algorithm. And by the end of this lecture, you'll definitely

13:56.280 --> 14:00.800
know what a data-centric AI algorithm is. But can someone tell me what the difference

14:00.800 --> 14:30.480
is between K and N and a data-centric AI algorithm? Yeah, yeah, totally. That's a great

14:31.480 --> 14:37.240
answer. K and N is just doing classification. And it's not actually modifying the data set.

14:37.240 --> 14:41.280
So yeah, that's exactly right. Alright, so what are a few other examples of what is not

14:41.280 --> 14:47.800
data-centric AI? Handpicking a bunch of points you think will improve a model. So can anybody

14:47.800 --> 14:55.920
help me understand why this is not data-centric AI? Yeah. Yeah, totally. It's just done by

14:55.920 --> 15:00.160
hand. Like this could, if you had a hundred million, a data set of a hundred million points,

15:00.160 --> 15:05.560
this would take a long time. What about doubling the size of your data set so that you can

15:05.560 --> 15:20.480
train an improved model? Yeah. Yeah, totally. This is just classical machine learning. It's

15:20.480 --> 15:23.680
still all the model, all the work you do as a model, but you're just paying more money

15:23.720 --> 15:28.680
for more data. So let's juxtapose this. So what would be the data-centric AI versions

15:28.680 --> 15:35.520
of this? And just out of curiosity, does anybody know for the first one what would be sort

15:35.520 --> 15:49.040
of the data-centric AI corollary? Yeah, totally. And there's a whole field of research on

15:49.040 --> 15:54.480
this and a whole subsection of ML that's called corset selection, where you have a data set

15:54.480 --> 15:59.000
and let's say that you train a model on that data set and you get like 98% accuracy. But

15:59.000 --> 16:03.600
the data set is like a hundred billion points. And so the goal is, can I find like, you know,

16:03.600 --> 16:08.680
a million points that if I just train on those, I can get like pretty close, like 97% accuracy.

16:08.680 --> 16:14.160
And that's corset selection. What about for number two? What would be like a data-centric

16:14.160 --> 16:25.640
AI corollary? Yeah, yeah. So the idea is, and how does anybody know of any ways that

16:25.640 --> 16:37.040
you might make your data set like bigger without just getting twice as much data? Yeah, totally.

16:37.040 --> 16:42.560
So like, say you have a, so data augmentation, awesome. And so say you have an image, you

16:42.560 --> 16:46.880
could just totally rotate that image. And now instead of one data point, you have like

16:46.880 --> 16:51.520
five data points or turn it black and white, or you could shift it or skew it or take the

16:51.520 --> 16:55.400
top and the bottom and move them a little bit or add some noise. There's lots of things

16:55.400 --> 16:59.360
you can do to make data bigger and actually improve a model just by changing the data

16:59.360 --> 17:05.320
set. And these are all fall within data-centric AI. Are you all familiar with what's called

17:05.320 --> 17:14.560
like back translation for text data augmentation? So this is a pretty fun thing. I don't like

17:14.560 --> 17:23.280
have particularly good translation skills. But like, if I say like, hi, you know, my

17:23.280 --> 17:46.680
name is Curtis. And then I translate this. Okay, so it becomes Ola. And then I translate

17:46.680 --> 17:59.520
this again. It might become, and now I've gotten one data point, and I just got another

17:59.520 --> 18:04.520
data point. So I was able to augment my text data set to get more versions of the same

18:04.520 --> 18:11.920
thing. And this could have some label, and the label could be introduction. You know,

18:11.920 --> 18:14.960
I'll just end it there because I know it's hard to see on that side. But this, this would

18:14.960 --> 18:22.200
be like the label. And this is text. And so this one is the same label. We haven't changed

18:22.200 --> 18:26.080
the label at all. So this is intro. And now you can see I have more labeled data, but

18:26.080 --> 18:29.280
I didn't have to do anything. I didn't have to pay more. I didn't have to, I just did

18:29.280 --> 18:30.680
this all computationally. Yeah.

18:30.680 --> 18:34.840
How do you see this in all the cases for that error to be taken?

18:34.840 --> 18:40.360
Oh yeah, totally can. So if, say that this label was wrong, now you have two label errors.

18:40.840 --> 18:45.880
Totally. Yeah. So you often want to combine two approaches. One is like first, try to

18:45.880 --> 18:50.120
fix or improve label errors before doing the augmentation. And that's a really good idea.

18:53.120 --> 18:56.400
All right. So what are some examples, we looked at examples that are not data-centric AI. So

18:56.400 --> 19:01.280
what are some things that are data-centric AI? And for this, I'll go through quick because

19:01.280 --> 19:06.680
we're going to learn all these in the course. And so one is outlier detection and removal.

19:06.680 --> 19:12.960
So I'll just be really quick to show you. So say you have some data set and we've got,

19:12.960 --> 19:23.960
let me use this board. And you've got, say, right? So you've got your sort of, your, you

19:23.960 --> 19:28.760
know, two classes. And so normally you would, you would draw your classifier and then you

19:28.760 --> 19:32.520
have some new point here and I'll be labeled a negative. But say in your training data,

19:32.520 --> 19:39.120
you have this really weird, you know, plus over here. And so maybe the boundary should

19:39.120 --> 19:42.520
be something like this, but you just don't have very much information and it's really

19:42.520 --> 19:47.840
out here and it seems like it's not very related to the rest of the data. And so what often

19:47.840 --> 19:51.400
people will do because they just don't have enough information is they'll identify this

19:51.400 --> 19:55.560
as an automatic, as an outlier because it seems very out of distribution and they'll

19:55.560 --> 19:59.040
remove it. And so then you get this line, which fits to all the data except for that

19:59.040 --> 20:07.440
one point. In terms of some other things in data-centric AI, so error detection and correction.

20:07.440 --> 20:12.200
And so that's, for example, you just have like a data point that is, it could be images,

20:12.200 --> 20:17.960
it's like all black, or you have a label error like we saw earlier. If this, you know, text

20:17.960 --> 20:24.360
example instead of being labeled intro was labeled like, you know, a goodbye clause or

20:25.320 --> 20:32.400
something. You'd want to find that and automatically correct it. Data augmentation, which is what

20:32.400 --> 20:36.360
we saw earlier. So that's like increasing the size of your data set. So you have more

20:36.360 --> 20:43.080
training data. Feature engineering and selections. The idea here is you have, if anyone's familiar

20:43.080 --> 20:48.200
like the early days of neural networks couldn't solve like the XOR problem, but you could

20:48.200 --> 20:53.920
always just generate the XOR as another column. A more concrete example, if you haven't heard

20:53.960 --> 20:59.480
of this, that one is you just have, you know, a bunch of tabular data. I worked on cheating

20:59.480 --> 21:06.080
detection at MIT. And, you know, if you want the machine learning model to learn, say who

21:06.080 --> 21:12.080
is a cheater from a bunch of, you know, education data, like where did they go to school and

21:12.080 --> 21:16.480
what's their background and what problems did they answer. It can really help if you

21:16.480 --> 21:22.240
also compute new features like how often did they submit answers within five seconds of

21:22.280 --> 21:26.440
another student. So you can always generate new features and then you pass those into the

21:26.440 --> 21:30.600
model and your model can, you know, if the features are relevant to the label you're

21:30.600 --> 21:34.600
trying to predict, it can do a lot better. Yeah.

21:34.600 --> 21:43.600
When we're doing outlier detection and removal, how do you know that an outlier is the result

21:43.600 --> 21:48.920
of something that should not have happened or indication of a very rare event that you

21:48.960 --> 21:54.960
should pay attention to? Yeah, that's a really fair and hard question to answer. You make

21:54.960 --> 22:00.160
assumptions. And so in this case, I was making an assumption, right? I was saying, like, I

22:00.160 --> 22:03.600
didn't draw very much data, but say that I had, like, millions of data points and then

22:03.600 --> 22:09.120
this one's really far away. Then the assumption you're making is that, like, I have so much

22:09.120 --> 22:14.000
data that suggests that this is the distribution and then I have very little data that suggests

22:14.000 --> 22:18.760
that this is part of that distribution. And the biggest issue is that if your classifier

22:18.760 --> 22:23.440
is changing dramatically for one data point, but you have, just think of it as, like, evidence.

22:23.440 --> 22:28.280
I've got a thousand or a million people saying it should be this thing and then I've got

22:28.280 --> 22:32.400
this one other person who may be right, they may be right, but they're saying that I think

22:32.400 --> 22:38.320
it's this other thing and it greatly skews the classifier. And so just because you want

22:38.320 --> 22:43.600
to trust the masses, you will say, hey, that one person is really, seems really off base.

22:43.600 --> 22:47.440
And this is very much a choice. And so what typically, what you do is you sort of rank

22:47.440 --> 22:53.360
every data point in terms of how in distribution it is and then you choose your cutoff and

22:53.360 --> 23:05.240
that's a human decision. Another data-centric AI task is to establish consensus labels. So

23:05.240 --> 23:09.920
if you guys have heard of, like, the self-driving cars, of course, then a lot of the ways that

23:09.960 --> 23:14.000
these models are trained is you'll have an image and then they want really high-quality

23:14.000 --> 23:19.200
data. So they'll have, like, 20 different people label, is this a scene of a street or

23:19.200 --> 23:23.920
are we on a bridge? Is this a stop sign? Because they really need to get accurate labels. The

23:23.920 --> 23:28.240
question is, when you're training your model, if you're just going to train with one label

23:28.240 --> 23:34.600
for that image or one set of labels for that image, you can't use all 20 of your annotator's,

23:34.720 --> 23:40.000
you know, guesses. You have to somehow combine them into a single training label. So how do

23:40.000 --> 23:45.800
you do that in a way that maximizes model performance? Another one is active learning.

23:45.800 --> 23:51.240
And this is a very classical problem. You have some data set, you train a model, it has 80%

23:51.240 --> 23:57.040
performance. Okay, I want now to get my model to 85% performance, but I want to pay for as

23:57.040 --> 24:02.320
little new data as possible. Or I want to improve the current existing data as little as possible.

24:02.640 --> 24:06.960
What are my next steps? And you can automate that process. You can actually get good signal to

24:06.960 --> 24:11.680
optimize in a way that you minimize the amount of new data that you need to collect information for

24:11.680 --> 24:18.240
or label in order to achieve that model accuracy. And then a final example is curriculum learning,

24:18.240 --> 24:23.280
which we already mentioned. So these are some examples of data-centric AI tasks, many of which

24:23.280 --> 24:28.400
we'll cover over the next two weeks. All right, so there's a lot of hype around data-centric AI.

24:28.480 --> 24:34.400
For those who are familiar with Andrew Ng, he's a pretty well-known person in AI from Stanford

24:34.400 --> 24:40.640
and has been at Google Research and Baidi Research and done a lot of things found in Coursera.

24:40.640 --> 24:46.560
So he's been really excited about data-centric AI. And let's look at some reasons, you know, why and

24:46.560 --> 24:51.120
some of the things that we've seen in the news. For example, he mentioned that like 80% of an AI

24:51.120 --> 24:55.520
developer's time is actually just spent on data, which is kind of funny, right? You know, you're

24:55.520 --> 25:00.000
an AI developer, you're not like a data scientist, but yet you're doing data science work all the

25:00.000 --> 25:05.840
time. And so there's something happening here in the real world that there isn't as much until

25:05.840 --> 25:11.360
recently actually systematic, you know, learning and teaching around how do we go about doing this.

25:12.000 --> 25:18.080
Also, if you're not familiar, bad data is very, very troublesome for businesses and for the

25:18.080 --> 25:23.120
government and for economies. And it's estimated this is out of Harvard Business Review that it

25:23.120 --> 25:29.040
cost the US alone about three trillion dollars in a given year. And you might see this and think,

25:29.040 --> 25:33.280
okay, that's really bad. But the good news is like a lot of people think that we can actually solve

25:33.280 --> 25:38.560
a lot of that three trillion issue that bad data causes with data-centric AI techniques.

25:39.120 --> 25:42.560
And so there's a lot of hype around it because it means a lot to a lot of people.

25:43.440 --> 25:51.120
This is a quick example. I did this internship at Fair in 2016 and I was in Jan's group and

25:51.200 --> 25:57.120
Jeff Hinton came to visit. If you're not familiar, these two recently won the Nobel Prize of Computer

25:57.120 --> 26:02.560
Science, which is called the Turing Award. And so I think they're old friends. And Jan has a dataset

26:02.560 --> 26:09.120
MNIST. Are folks familiar with MNIST? Okay, cool. Very classical machine learning dataset and we've

26:09.120 --> 26:14.880
been training models on it for like over 20 years. And people generally assume that it has perfect

26:14.880 --> 26:22.160
labels because that's a very common assumption. Not maybe now in 2023, but definitely like when

26:22.160 --> 26:26.080
it first came out. And it's a very high quality dataset. And so Jeff Hinton was presenting at

26:26.080 --> 26:30.400
this time, I think, Capsule Networks. He's very excited about it. And his aha culminating moment

26:30.400 --> 26:35.760
of his talk was that he found the label error in Jan Lacun's dataset. And so he's very excited to

26:35.760 --> 26:40.880
show, hey, this five image is actually labeled a three in your dataset, Jan. And he's like, aha,

26:40.880 --> 26:47.360
I got you. And so I think that it's worth mentioning that this is where we were in 2016. And now,

26:47.360 --> 26:52.720
you know, we're only six, seven years later, and we're able to systematically find millions of

26:52.720 --> 26:58.800
errors in datasets. And that's sort of how far we've come using these data-centric AI approaches.

27:00.160 --> 27:07.280
Who here is familiar with Dolly and Dolly too? Yeah, it's pretty cool, right? So it generates images

27:07.280 --> 27:12.000
and they're pretty cool, like you can generate images of pretty much anything you describe.

27:12.000 --> 27:16.320
And so if you check out the Dolly demo page, and there's the link here in the slides, if you want

27:16.320 --> 27:21.440
to check them after, there's a cool video. And you'll notice in that video, they talk about one of

27:21.440 --> 27:26.160
their biggest challenges. And so this is just screenshots from the video. And the technology

27:26.160 --> 27:31.360
is constantly evolving, but Dolly too has limitations. It's taught with objects that are

27:31.360 --> 27:36.800
incorrectly labeled with plain labeled car, for example. And this happens because it's a

27:36.800 --> 27:42.640
massive dataset. So if you don't use data-centric AI approaches, it's very difficult to clean, you

27:42.640 --> 27:49.200
know, whatever, hundreds of millions or more, probably billions of pairs of text and image.

27:49.200 --> 27:54.960
And so what they notice is that a user might actually try to generate a car, but Dolly will

27:54.960 --> 28:01.200
actually create a plane, because it's seen wrongly labeled data. And so this is very problematic

28:01.200 --> 28:05.040
for something that's deployed in the real world. Another example from that video is they talk about

28:05.040 --> 28:10.960
generating these baboons, but they emphasize that you can only do this correctly if you have

28:10.960 --> 28:15.920
accurate labels. And if you didn't, you'll totally, you can get the wrong thing. And so the key takeaway

28:15.920 --> 28:20.720
here is that this is a real world technology, lots of people are using today at scale, but the

28:20.720 --> 28:27.280
reliability of that model really does depend on the data quality. Another big example is familiar

28:27.360 --> 28:37.760
with chat GPT. Does anybody know, yeah, does anybody know sort of why or like what was the

28:37.760 --> 28:44.720
big innovation from GPT three, which obviously had a huge hype around it to chat GPT, which has

28:44.720 --> 28:49.280
even more hype around it? What was sort of one of the big things they did between the two?

28:53.920 --> 28:57.040
Yeah, totally. And do you know what they were doing with their reinforcement learning?

28:57.680 --> 29:04.320
They like inviting some users to talk about these chat GPT and actually some of the work.

29:05.200 --> 29:12.720
Yeah, totally. Yeah. What was happening there was they, they had a lot of bad outputs, you know,

29:12.720 --> 29:18.240
like chat GPT three was saying things that were like super biased, or like inappropriate,

29:19.120 --> 29:24.800
not even true, just wrong facts. And these outputs were tied to data it was trained on.

29:24.800 --> 29:29.440
And to parameters in the model that were learned from that data. And so what they did is they did

29:29.440 --> 29:33.680
in a reinforcement setting, which means they talked to people, they use that information

29:33.680 --> 29:38.880
to then update the model, then the model sort of explores with new outputs, then people see those.

29:38.880 --> 29:43.520
But what they were doing is they were having people actually rank them in terms of the quality

29:43.520 --> 29:48.880
of the prediction, right? And so they were ranking the quality of this data and then using that to

29:48.880 --> 29:55.040
update the model so that it would have improved less bias and better outputs. And so that was

29:55.040 --> 30:00.720
the key idea of chat GPT was actually to deal with a data quality issue. And if you've tried both,

30:00.720 --> 30:05.840
you can see that there is a pretty big performance boost. The downside is that they had to do this

30:05.840 --> 30:13.040
with a lot of manual work. And so we went to work on ways to automate that. You guys are probably

30:13.040 --> 30:20.080
familiar with Tesla. So this is Tesla's data engine. This is from a talk by Andre Carpathi,

30:20.080 --> 30:26.160
who is formerly the Tesla director of AI. And this is their data engine. And we'll just start in

30:26.160 --> 30:32.720
the top left, like you, the way they're training the, you know, the self-driving Tesla model is,

30:32.720 --> 30:36.080
you know, you have some data source. And then you'll notice some problem, which is like, hey,

30:36.880 --> 30:42.640
we're in a tunnel. And we don't have a lot of, you know, tunnel data. So the car is like

30:42.640 --> 30:47.200
doing weird things, right? And so then what they would do is they would collect a bunch more data

30:47.200 --> 30:52.800
in tunnels and then update their training data and label them and then redeploy and go in the

30:52.800 --> 30:57.360
world and then see what breaks then. And this is a very, you know, difficult iterative process

30:57.360 --> 31:01.520
because you have to send the car out and then see where things break and then collect a bunch more

31:01.520 --> 31:06.160
data. Or if you had a way to automate, okay, this is where my data is missing. This is stuff that's

31:06.160 --> 31:09.920
out of distribution. This is where we have a bunch of label errors. And you're able to automate

31:09.920 --> 31:13.440
that process. They could have reduced those cycles and gotten the car out a lot faster,

31:14.240 --> 31:18.960
at least the AI part of the car. And so this was a big pain that, that Andre mentioned.

31:20.000 --> 31:26.480
Another really, this is a fantastic example that Jonas shared with me. These are all examples of

31:26.480 --> 31:31.920
traffic lights. You know, so imagine that like Elon Musk comes to you and he says, you know,

31:31.920 --> 31:38.560
Andre, I need you to get this car to navigate any street in the world. And then, you know,

31:38.560 --> 31:41.840
you're like, oh, that's cool. Okay. So like it needs to stop at traffic lights. Like that seems

31:41.840 --> 31:46.160
like a pretty simple problem. And then you go out in the real world and like traffic lights are

31:46.160 --> 31:50.160
not a simple problem. They're really complicated and they're really messy. And this is actually

31:50.160 --> 31:55.840
like a total nightmare if you had to do this. And so how do you find sort of systematic ways to

31:55.840 --> 32:00.560
group data together and train in a way that's robust and reliable? Like real world data is super

32:00.560 --> 32:06.560
messy and complicated. And so Andre's big takeaway was that, you know, he shares this juxtaposition

32:06.560 --> 32:12.400
of the amount of sleep, you know, he lost over in his PhD. And it was like data sets is this tiny

32:12.400 --> 32:17.280
sliver, but definitely spent a lot of time on models and algorithms. And then he goes and he's

32:17.280 --> 32:24.000
leading, you know, the AI model at Tesla. And it's like, it's all data, you know, it's a big shock

32:24.000 --> 32:28.400
when you make this shift. And so that's why we really want to focus on ways we can improve that.

32:29.440 --> 32:35.920
A very common use case is when you're trying to train a model with noisy labels. Okay. So

32:36.000 --> 32:43.040
this is like the classical scenario of your, you have the dogs and cats, but now say 30% of

32:43.040 --> 32:49.360
your cats are labeled dog. Okay. And maybe 20% of the dogs are labeled cat. So how do you get a model

32:49.360 --> 32:54.320
that does as well or close to as well as if you had perfectly labeled data? And we'll go into that

32:54.320 --> 33:00.320
more in the next lecture. But I want to just motivate that I looked at a bunch of model

33:00.320 --> 33:05.120
centric methods and data centric methods over the last five years out of top institutions

33:05.120 --> 33:10.800
like Google and Facebook and so forth. And we benchmark them. And it turns out, and there's a

33:10.800 --> 33:15.280
lot on this slide, but there's really one key takeaway that the data centric AI methods all

33:15.280 --> 33:20.000
outperformed the model centric methods for this particular task, you know, on this particular

33:20.000 --> 33:25.200
data set. And this was pretty revealing and compelling that there's something here to data

33:25.200 --> 33:30.480
centric AI approaches. And to be very clear, what these models are doing is they are modifying the

33:30.480 --> 33:35.760
loss function or modifying the model so that they sort of don't train as much on what they think is

33:35.760 --> 33:41.360
bad data, but within the context of the modeling. And what these methods are doing is they're

33:41.360 --> 33:46.000
actually modifying the data set. They're either removing bad data or they're generating more data

33:46.000 --> 33:50.880
that sort of makes the error go away, but somehow they're actually changing the data set. And this

33:50.880 --> 33:55.520
is just how things stack up. And you'll see the data centric methods outperform model centric

33:55.520 --> 34:00.560
methods in this task. And this is a very common task that's of interest to the field. So it's

34:00.560 --> 34:06.720
cool. It's cool to see that this stuff is working and we're getting some benefits from it. So a

34:06.720 --> 34:14.080
sort of culminating thought is, you know, we were talking a lot about ways that we want to automate,

34:14.080 --> 34:20.560
but what did we do before? So obviously we've had to improve data sets in industry and like outside

34:20.560 --> 34:24.320
of academia in the past. It's like, how did we do it before there was data centric AI?

34:25.840 --> 34:32.480
And so we mostly relied on human powered solutions. For example, we would just spend more money

34:32.480 --> 34:38.000
for higher quality data. It's like you literally just pay for more labels or you would pay for more

34:38.000 --> 34:45.280
data. And that was a very classical way to improve a model. Also building custom tools. So like you

34:45.280 --> 34:51.600
saw at Tesla, they have this whole sort of data platform. And this is a lot, right? This is for

34:51.600 --> 34:55.280
one specific problem, the very important problem, but they had to build a lot of custom tech around

34:55.520 --> 35:01.360
it. Another common thing is just fixing data inside a Jupyter notebook. So just a quick show

35:01.360 --> 35:06.640
of hands like how many people have used Jupyter notebooks. Okay, that's really good because

35:06.640 --> 35:11.840
all the labs are in Jupyter notebooks for the most part for this lecture or for this course.

35:11.840 --> 35:18.160
So yeah, what people do is like you just sort data by like a loss function. And so you just say like

35:18.160 --> 35:22.480
the loss for this data point is the highest. So I think this is most likely to be wrong. Let me

35:22.480 --> 35:26.800
check it out. And then you would look at it by hand and then you would market or do something with

35:26.800 --> 35:31.440
it or you take the top 20 or something, but you would just do a lot of this by hand inside of

35:31.440 --> 35:35.920
Jupyter notebooks and printing things out. And that was actually a pretty normal and standard way

35:35.920 --> 35:41.280
that like, you know, somebody in industry or data scientist or grad student would try to fix a data

35:41.280 --> 35:47.520
set. And so the whole idea is we're going to look at ways that we systematize these approaches

35:47.520 --> 35:51.200
so that they're more reliable, more accurate, and they work on most data sets.

35:53.200 --> 35:57.840
So this course is about the following. So today we're just looking at what is model

35:57.840 --> 36:02.880
centric guy versus data centric guy, get the juices flowing, think about how to think things

36:02.880 --> 36:08.080
in terms of data and the impact, why matters. Next lecture, we'll focus on label errors.

36:08.800 --> 36:13.120
So how do you actually detect label errors automatically? How do you learn with label

36:13.120 --> 36:17.120
errors? What are good methods to do that? And what are some things to think about when you're

36:17.120 --> 36:23.360
doing this? Data set creation and curation will be on Thursday. And this is how do you construct

36:23.360 --> 36:27.840
a data set in such a way that you can train a good model? How do you arrange, you know,

36:27.840 --> 36:33.360
the classes? How do you choose good examples? And then finally on Friday, which is related to

36:33.360 --> 36:39.840
data set curation, we'll look at active learning and potentially core sets and active learning.

36:39.840 --> 36:43.760
As I mentioned, this is task where you're trying to choose the next data you want to add to your

36:43.760 --> 36:49.120
data set and you want to obtain a label for. Or do you want to improve some of the labels you

36:49.120 --> 36:53.680
currently have? And so you're just trying to decide, I have to pay a cost for new data that I'm going

36:53.680 --> 36:58.480
to add to my data set. And it costs me something like either money or time. So I don't want to do

36:58.480 --> 37:02.640
it too much. So what's like the best stuff to add to my data set now to improve my model?

37:03.520 --> 37:07.360
Next week, we'll sort of have a bit of a shift and we'll focus more on data, but

37:08.000 --> 37:12.880
we'll focus on some specific things for the first on Monday, we'll focus on class and balance and

37:12.880 --> 37:19.040
distribution shift. So this is, imagine like it's the stock market, right? And like if you're

37:19.040 --> 37:24.480
trying to predict things, you know, on Monday or on in January of this year versus now, it would be a

37:24.480 --> 37:31.280
very different market, right? And so over time, data changes. And so how do you continue to produce

37:31.280 --> 37:36.720
good reliable predictions even though data is changing? And then the class imbalances this

37:36.720 --> 37:41.520
problem where imagine that for that line over there on the left, we had like a million pluses and

37:41.520 --> 37:48.480
only a few minuses. Well, then a smart classifier could actually just always predict plus and get

37:48.480 --> 37:53.440
near 100% accuracy. So often it will just ignore the minuses. So how do we get around that problem?

37:55.600 --> 38:00.480
Interpretable features of data. So this is, who here's familiar with interpretability?

38:03.120 --> 38:07.040
All right, not as much. That one will be fun then. And thanks for raising your hand.

38:07.840 --> 38:12.880
That should be a good class. We'll learn about how do you interpret data in a way that you can

38:12.880 --> 38:17.440
understand what's going on from a data perspective? Like why is the model doing what it's doing in

38:17.440 --> 38:22.800
terms of the data? And so we'll understand model performance based on data. The next class on

38:22.800 --> 38:27.680
Wednesday of next week will be on data-centric evaluation of ML models. So like, how do we know,

38:27.680 --> 38:32.560
you know, from a data perspective, how good a model is, how reliable it is, how well it's working?

38:33.520 --> 38:39.280
On Thursday, we'll look at encoding human priors. So this is like, how do we augment data and also

38:39.280 --> 38:44.880
prompt engineering. So this class will cover a lot of things like GPT and chat GPT and

38:44.880 --> 38:49.440
transformer models and stuff like that. And then finally, our last class will be on data privacy

38:49.440 --> 38:56.160
and security. And this is very, very interesting, especially for a lot of people in like banking

38:56.160 --> 38:59.680
and finance and they're using a lot of machine learning models. How do you make sure that like a

38:59.680 --> 39:05.040
model doesn't actually secretly encode the data? Or like somehow if you had access to predictions,

39:05.040 --> 39:09.360
you could figure out someone's, you know, banking info and you can imagine all sorts of things that

39:09.360 --> 39:15.760
can happen in AI. So data security and privacy is really important. Any sort of questions while I'm

39:15.760 --> 39:30.480
on this slide? Sweet. Are you guys excited? Okay. Does it look like a good course?

39:33.440 --> 39:41.760
Okay. All right. Cool. All right. There's a lab for every lecture. And so you can find this on the

39:41.760 --> 39:49.520
course website, which we can, we can write on the board. So you probably have seen it in the email,

39:49.520 --> 40:08.080
but just in case. And there's, there's a lab, usually will be Jupyter notebooks. And the one

40:08.080 --> 40:14.400
for today will be a text classification task. And it has some bad, bad data that's gotten mixed

40:14.400 --> 40:21.280
in. And this is actual data that's been scraped from, I think, Amazon reviews. And it has some

40:21.280 --> 40:28.080
bad tags, some weird HTML has gotten in there. And what you'll look at is model centric approaches

40:28.080 --> 40:32.240
at first. And you'll realize, Oh, it can only get you so far because like the data is not that great.

40:33.120 --> 40:37.920
And so you'll have to figure out how to improve the data set. You know, so you can get a

40:37.920 --> 40:43.760
better classifier. And so that's sort of today's lab. Get your hands wet with data centric AI.

40:44.640 --> 40:52.480
We have office hours every class, 3pm to 5pm. So an hour after the lecture ends every day.

40:53.280 --> 40:57.200
And then tomorrow's lecture will focus on label errors, how to find them and how to train better

40:57.200 --> 41:03.280
models. This is the folks who are teaching the course for folks that are from MIT,

41:04.240 --> 41:09.360
to from Stanford. And yeah, I think it'll be a good time.

41:10.960 --> 41:13.360
Really quick. Are there any questions?

41:25.200 --> 41:25.440
Yeah.

41:26.400 --> 41:31.280
My question is like, how do you know if it's like data that's the problem and not the model?

41:31.920 --> 41:36.160
Let's say you train a bunch of times in a model with different parameters. It's not like

41:36.160 --> 41:41.840
doing good. How do you know the data is a problem? Do you just try it and see if it improves or

41:41.840 --> 41:47.760
do you can use it another way to see that? Yeah, that's a good question. So there's a few ways.

41:47.760 --> 41:54.320
So one thing you can do is you can look at a subset of the problem. So say you had like

41:54.320 --> 41:58.720
a very big complex problem and there's thousands of classes and millions of data points.

41:58.720 --> 42:02.240
You can take just 10,000 of those data points and a few of those classes

42:02.960 --> 42:07.200
and actually check, do some process to check, make sure you have really high quality data

42:07.200 --> 42:12.320
and see how well does your model perform. And if your model is performing very, very high accuracy,

42:12.320 --> 42:16.960
but then when you use the original data, you get a drop off. That gives you a good signal.

42:16.960 --> 42:22.080
Another thing is if you have similar data set and it's like MNIST for example,

42:22.080 --> 42:26.240
you can get near 100% performance. And so you have a similar data set, but you're getting

42:26.240 --> 42:31.520
like much worse performance or like significantly less performance, but using the same architecture

42:31.520 --> 42:34.800
that you've seen do very well on a similar task on a different data set.

42:35.360 --> 42:40.000
And that's a good indicator. You should probably look at the data. And there's two more things.

42:40.000 --> 42:44.800
One is just, just take a look at the data. I think it's really easy to just get a data set

42:44.800 --> 42:48.560
and your goal is like train a model. And so you're doing a lot of cool, you know,

42:48.560 --> 42:52.800
download this TensorFlow package, download this hugging face package, but you don't actually

42:52.800 --> 42:58.400
take time usually to look through all the data points or like hundreds of data points and really

42:58.400 --> 43:03.280
see like, does this data look like what I think it does? Like, does it seem to be kind of messy?

43:03.280 --> 43:07.280
And what you'll often find is after first like, you know, first few hundred, you'll be like,

43:07.280 --> 43:11.440
oh, there's some weird stuff in here. And if you have millions of data points, that weird stuff adds up.

43:11.440 --> 43:18.160
Yeah, I guess the problem with the data is that there's like two months, so like, you know,

43:18.160 --> 43:22.320
like how do you connect with all of them? Yeah, I guess if you just look at like a bunch.

43:24.320 --> 43:30.000
Yeah, totally. In the next class, we'll show ways that you can actually rank your data set

43:30.640 --> 43:36.240
so that you know what's the best example to look at first. That's probably wrong. And so there are

43:36.240 --> 43:40.880
ways that you can automate this and then you can look at the initial data. And that's really the

43:40.880 --> 43:44.800
right approach. Like if you just look at random, then you might waste some time. But if you've

43:44.800 --> 43:50.240
ranked your data in a way that is likely to give you a good ranking on quality, and then you look

43:50.240 --> 43:55.920
at like the first 100 and there's really no issues and the data looks really good, then yeah, you

43:55.920 --> 44:14.640
might be able to just optimize the model and be okay. Any other questions?

44:19.840 --> 44:24.400
Is this useful for anyone sort of immediately? Is anyone thinking like, hey, this might be useful

44:24.400 --> 44:26.640
for what I'm working on right now?

44:35.600 --> 44:40.000
I haven't done anything specific about it before, and I think one of the reasons is

44:40.640 --> 44:45.600
because, you know, I always study models before. It seems like, you know, studying models is like

44:45.600 --> 44:49.920
very important, but once you go and you want to like quickly do something, that's kind of like the

44:49.920 --> 44:54.960
first thing that you want to do, you know, like, okay, what they do is data and how they work.

44:58.640 --> 45:00.640
Yeah.

45:13.520 --> 45:18.160
All right, great. We'll be here for a little bit after. Thanks everybody for coming.

45:18.160 --> 45:24.080
The next lectures will be a bit more technical, but I think it should be a really exciting course,

45:24.080 --> 45:27.840
and glad to have everybody here. Thanks.

