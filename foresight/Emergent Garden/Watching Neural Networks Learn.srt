1
00:00:00,000 --> 00:00:04,400
You are currently watching a neural network learn about a year ago

2
00:00:04,400 --> 00:00:10,920
I made a video about how neural networks can learn almost anything and this is because they are universal function approximators

3
00:00:11,200 --> 00:00:17,540
Why is that so important? Well, you might as well ask why functions are important. They are important because

4
00:00:18,560 --> 00:00:20,400
functions

5
00:00:20,400 --> 00:00:22,240
describe

6
00:00:22,240 --> 00:00:24,240
the world

7
00:00:25,200 --> 00:00:28,280
Everything is described by functions

8
00:00:28,640 --> 00:00:33,260
That's right functions describe the sound of my voice on your eardrum

9
00:00:33,720 --> 00:00:37,200
Function the light that's kind of hitting your eyeballs right now

10
00:00:38,040 --> 00:00:44,520
Function different classes and mathematics different areas and mathematics study different kinds of function high school math studies second-degree

11
00:00:44,520 --> 00:00:50,220
One variable polynomials calculus studies smooth one variable functions and it goes on and on

12
00:00:50,840 --> 00:00:53,480
functions describe the world

13
00:00:55,240 --> 00:01:00,240
Yes, correct. Thanks Thomas. He gets a little excited, but he's right

14
00:01:00,240 --> 00:01:05,600
The world can fundamentally be described with numbers and relationships between numbers

15
00:01:05,720 --> 00:01:13,400
We call those relationships functions and with functions we can understand model and predict the world around us

16
00:01:14,040 --> 00:01:18,240
The goal of artificial intelligence is to write programs that can also

17
00:01:18,680 --> 00:01:22,800
Understand model and predict the world or rather have them write themselves

18
00:01:22,800 --> 00:01:25,520
So they must be able to build their own functions

19
00:01:25,880 --> 00:01:32,200
That is the point of function approximation and that is what neural networks do. They are function building machines

20
00:01:32,640 --> 00:01:33,600
In this video

21
00:01:33,600 --> 00:01:40,640
I want to expand on the ideas of my previous video by watching actual neural networks learn strange shapes and strange spaces

22
00:01:41,080 --> 00:01:43,880
Here we will encounter some very difficult challenges

23
00:01:44,200 --> 00:01:51,000
Discover the limitations of neural networks and explore other methods for machine learning and mathematics to approach this open problem

24
00:01:51,400 --> 00:01:57,120
Now I am a programmer not a mathematician and to be honest. I kind of hate math

25
00:01:57,240 --> 00:01:59,800
I've always found it difficult and intimidating

26
00:01:59,800 --> 00:02:05,240
But that's a bad attitude because math is unavoidably useful and occasionally beautiful

27
00:02:05,240 --> 00:02:08,920
I'll do my best to keep things simple and accurate for an audience like me

28
00:02:08,920 --> 00:02:13,000
But know that I'm gonna have to brush over a lot of things and I'm gonna be pretty informal

29
00:02:14,120 --> 00:02:18,960
I recommend you watch my previous video, but to summarize functions are input output machines

30
00:02:19,120 --> 00:02:27,000
They take an input set of numbers and output a corresponding set of numbers and the function defines the relationship between those numbers

31
00:02:27,280 --> 00:02:34,400
The particular problem that neural network solve is when we don't know the definition of the function that we're trying to approximate

32
00:02:34,880 --> 00:02:41,080
Instead we have a sample of data points from that function inputs and outputs. This is our data set

33
00:02:41,280 --> 00:02:48,960
We must approximate a function that fits these data points and allows us to accurately predict outputs given inputs that are not in our

34
00:02:49,000 --> 00:02:55,480
Data set this process is also called curve fitting and you can see why now this is not some handcrafted

35
00:02:55,920 --> 00:03:00,360
Animation it is an actual neural network attempting to fit the curve to the data

36
00:03:00,360 --> 00:03:03,640
And it does so by sort of bending the line into shape

37
00:03:04,080 --> 00:03:10,840
This process is generalizable such that it can fit the curve to any data set and thus construct any function

38
00:03:11,160 --> 00:03:14,280
This makes it a universal function approximator

39
00:03:17,000 --> 00:03:21,640
The network itself is also a function and should approximate some unknown target function

40
00:03:22,200 --> 00:03:28,120
The particular neural architecture we're dealing with in this video is called a fully connected feed forward network

41
00:03:28,440 --> 00:03:35,160
Its inputs and outputs are sometimes called features and predictions and they take the form of vectors arrays of numbers

42
00:03:35,600 --> 00:03:41,120
The overall function is made up of lots of simple functions called neurons that take many inputs

43
00:03:41,120 --> 00:03:48,880
But only produce one output each input is multiplied by its own weight and added up along with one extra weight called a bias

44
00:03:49,840 --> 00:03:52,880
Let's rewrite this weighted sum with some linear algebra

45
00:03:53,040 --> 00:04:00,080
We can put our inputs into a vector with an extra one for the bias and our weights into another vector and then take what is called

46
00:04:00,080 --> 00:04:03,360
The dot product. Let's just make up some example values

47
00:04:03,840 --> 00:04:09,120
To take the dot product we multiply each input by each weight and then add them all up

48
00:04:09,920 --> 00:04:16,560
Finally this dot product is then passed to a very simple activation function in this case a relu which here returns zero

49
00:04:17,280 --> 00:04:20,720
We could use a different activation function, but a relu looks like this

50
00:04:21,200 --> 00:04:27,800
The activation function defines the neurons mathematical shape while the weights shift and squeeze and stretch that shape

51
00:04:28,400 --> 00:04:34,980
We feed the original inputs of our network to a layer of neurons each with their own learned weights and each with their own

52
00:04:35,040 --> 00:04:40,560
Output value we stack these outputs together into a vector and then feed the output vector as

53
00:04:40,680 --> 00:04:46,080
Inputs to the next layer and the next and the next until we get the final output of the network

54
00:04:46,560 --> 00:04:54,160
Each neuron is responsible for learning its own little piece or feature of the overall function and by combining many neurons

55
00:04:54,160 --> 00:05:01,080
We can build an ever more intricate function with an infinite number of neurons. We can provably build any function

56
00:05:02,360 --> 00:05:06,840
The values of the weights or parameters are discovered through the training process

57
00:05:06,840 --> 00:05:12,520
We give the network inputs from our data set and ask it to predict the correct outputs over and over and over

58
00:05:13,000 --> 00:05:21,080
The goal is to minimize the network's error or loss which is some measurement of difference between the predicted outputs and the true outputs

59
00:05:22,000 --> 00:05:25,480
Over time the network should do better and better as loss goes down

60
00:05:25,720 --> 00:05:30,960
The algorithm for this is called back propagation, and I am again not going to explain it in this video

61
00:05:30,960 --> 00:05:34,440
I'll make a video on it. Eventually. I promise. It's a pretty magical algorithm

62
00:05:36,080 --> 00:05:44,300
However, this is a baby problem. What about functions with more than just one input or output that is to say higher dimensional problems

63
00:05:44,760 --> 00:05:50,400
The dimensionality of a vector is defined by the number of numbers in that vector

64
00:05:50,640 --> 00:05:54,280
For a higher dimensional problem. Let's try to learn an image

65
00:05:54,680 --> 00:06:01,560
The input vector is the row column coordinates of a pixel and the output vector is the value of the pixel itself

66
00:06:01,560 --> 00:06:05,520
In math speak we would say that this function maps from R2 to R1

67
00:06:05,880 --> 00:06:11,520
Our data set is all of the pixels in an image. Let's use this unhappy man as an example a

68
00:06:11,960 --> 00:06:18,000
Pixel value of zero is black and one is white although. I'm going to use different color schemes because it's pretty as

69
00:06:18,280 --> 00:06:22,960
As we train we take snapshots of the learned function as the approximation improves

70
00:06:23,360 --> 00:06:26,600
That's what you're saying now, and that's what you saw at the beginning of this video

71
00:06:27,160 --> 00:06:35,080
But to clarify this image is not a single output from the network rather every individual pixel is a single output

72
00:06:35,320 --> 00:06:41,040
We are looking at the entire function all at once and we can do this because it is very low dimensional

73
00:06:41,720 --> 00:06:47,480
You'll also notice that the learning seems to slow down. It's not changing as abruptly as it was at the beginning

74
00:06:47,840 --> 00:06:54,360
This is because we periodically reduce the learning rate a parameter that controls how much our training algorithm

75
00:06:54,560 --> 00:06:59,000
Alters the current function this allows it to progressively refine details

76
00:07:00,120 --> 00:07:04,800
Now just because our neural network should theoretically be able to learn any function

77
00:07:04,800 --> 00:07:10,120
There are things we can do to practically improve the approximation and optimize the learning process

78
00:07:10,840 --> 00:07:15,160
For instance one thing I'm doing here is normalizing the row column inputs

79
00:07:15,160 --> 00:07:17,680
Which means I'm moving the values from a range of zero

80
00:07:18,200 --> 00:07:25,080
1400 to the range of negative one one I do this with a simple linear transformation that shifts and scales the values

81
00:07:25,280 --> 00:07:30,960
The negative one one range is easier for the network to deal with because it's smaller and centered at zero

82
00:07:32,200 --> 00:07:35,920
Another trick is that I'm not using a relu as my activation function

83
00:07:35,920 --> 00:07:43,080
But rather something called a leaky relu a leaky relu can output negative values while still being non-linear and has been shown to

84
00:07:43,080 --> 00:07:48,800
Generally improve performance. So I'm using a leaky relu in all of my layers except for the last one

85
00:07:49,480 --> 00:07:53,960
Because the final output is a pixel value. It needs to be between zero and one

86
00:07:54,320 --> 00:07:56,440
To enforce this in the final layer

87
00:07:56,440 --> 00:08:01,920
We can use a sigmoid activation function which squishes its inputs between zero and one

88
00:08:03,040 --> 00:08:09,280
Except there is a different squishing function called tan H that squishes its inputs between negative one and one

89
00:08:09,280 --> 00:08:15,160
I can then normalize those outputs into the final range of zero one. Why go through the trouble?

90
00:08:15,160 --> 00:08:18,440
Well, tan H just tends to work better than sigmoid

91
00:08:19,480 --> 00:08:24,320
Intuitively, this is because tan H is centered at zero and plays much nicer with back propagation

92
00:08:24,320 --> 00:08:30,240
But ultimately the reasoning doesn't matter as much as the results both networks here are theoretically

93
00:08:30,400 --> 00:08:34,840
Universal function approximators, but practically one works much better than the other

94
00:08:35,200 --> 00:08:40,040
This can be measured empirically by calculating and comparing the error rates of both networks

95
00:08:40,040 --> 00:08:45,880
I think of this as the science of math where we must test our ideas and validate them with evidence

96
00:08:46,000 --> 00:08:48,040
Rather than providing formal proofs

97
00:08:48,240 --> 00:08:51,640
It'd be great if we could do both but that is not always possible

98
00:08:51,640 --> 00:08:56,240
And it is often much easier to just try and see what happens and that's my kind of math

99
00:08:57,400 --> 00:09:04,240
Let's make it harder. Here. We have a function that takes two inputs u v and produces three outputs x y z

100
00:09:04,440 --> 00:09:08,400
It's a parametric surface function and we'll use the equation for a sphere

101
00:09:08,640 --> 00:09:16,080
We can learn it the same way as before take a random sample of points across the surface of the sphere and ask our network to approximate it

102
00:09:16,400 --> 00:09:18,920
Now this is clearly a very silly way to make a sphere

103
00:09:18,920 --> 00:09:24,400
But the network is trying its best to sort of wrap the surface around the sphere to fit the data points

104
00:09:24,400 --> 00:09:28,520
I hope this also gives you a better view of what a parametric surface is

105
00:09:28,520 --> 00:09:33,920
It takes a flat 2d sheet and contorts it in 3d space according to some function

106
00:09:35,240 --> 00:09:39,320
Now this does okay though it never quite closes up around the poles

107
00:09:40,680 --> 00:09:44,320
For a real challenge, let's try this beautiful spiral shell surface

108
00:09:44,320 --> 00:09:49,640
I got the equation for this from this wonderful little website that lets you play with all kinds of shell surfaces

109
00:09:49,800 --> 00:09:52,600
See what I mean when I say that functions describe the world

110
00:09:53,320 --> 00:09:57,200
Anyway, let's sample some points across the spiral surface and start learning

111
00:10:05,200 --> 00:10:13,640
Well, it's working, but clearly we're having some trouble here. I'm using a fairly big neural network

112
00:10:13,640 --> 00:10:19,520
But this is a complicated shape and it seems to be getting a little bit confused. We'll come back to this one

113
00:10:21,320 --> 00:10:27,880
We can also make the problem harder not by increasing dimensionality, but by increasing the complexity of the function itself

114
00:10:28,400 --> 00:10:32,800
Let's use the Mandelbrot set an infinitely complex fractal

115
00:10:33,280 --> 00:10:39,240
But we can simply define a Mandelbrot function as taking two real valued inputs and producing one output

116
00:10:39,240 --> 00:10:41,880
The same dimensionality as the images we learned earlier

117
00:10:42,280 --> 00:10:46,560
Now I've defined my Mandelbrot function to output a value between zero and one

118
00:10:46,680 --> 00:10:50,680
Where one is in the Mandelbrot set and anything less than one is not

119
00:10:51,160 --> 00:10:54,680
Under the hood, it's iteratively operating on complex numbers

120
00:10:54,680 --> 00:10:58,200
And I added some stuff to output smooth values between zero and one

121
00:10:58,200 --> 00:11:00,400
But I'm not going to explain it much more than that

122
00:11:00,600 --> 00:11:04,920
After all a neural network doesn't know the function definition either and it shouldn't matter

123
00:11:04,920 --> 00:11:07,360
It should be able to approximate it all the same

124
00:11:07,720 --> 00:11:12,240
The data set here is randomized points drawn uniformly from this range

125
00:11:12,760 --> 00:11:19,040
Now this has actually been a pet project of mine for some time and I've made several videos trying this exact experiment over the years

126
00:11:19,040 --> 00:11:21,040
I hope you can see why it's interesting

127
00:11:21,920 --> 00:11:29,840
Despite being so low dimensional the Mandelbrot function is infinitely complex literally made with complex numbers and is uniquely difficult to approximate

128
00:11:30,400 --> 00:11:35,640
You can just keep fitting and fitting and fitting the function and you will always come up short

129
00:11:36,160 --> 00:11:40,120
You could do this with any fractal. I just use the Mandelbrot set because it's so well known

130
00:11:42,280 --> 00:11:48,320
So after training for a while we've made some progress, but clearly we're still missing an infinite amount of detail

131
00:11:48,560 --> 00:11:52,840
I've gotten this to look better in the past, but I'm not going to waste any more time training this network

132
00:11:52,960 --> 00:11:54,960
There are better ways of doing this

133
00:11:55,640 --> 00:12:00,920
Are there different methods for approximating functions besides neural networks?

134
00:12:01,360 --> 00:12:07,720
Yes, many actually. There are always many ways to solve the same problem though some ways are better than others

135
00:12:08,200 --> 00:12:11,760
Another mathematical tool we can use is called the Taylor series

136
00:12:12,400 --> 00:12:16,280
This is an infinite sum of a sequence of polynomial functions

137
00:12:16,280 --> 00:12:21,680
x plus x squared plus x cubed plus x to the fourth up to x to the n

138
00:12:21,680 --> 00:12:24,200
n is the order of the series

139
00:12:24,480 --> 00:12:29,120
Each of these terms are multiplied by their own value called a coefficient

140
00:12:29,560 --> 00:12:34,880
Each coefficient controls how much that individual term affects the overall function

141
00:12:36,120 --> 00:12:39,360
Given some target function by choosing the right coefficients

142
00:12:39,360 --> 00:12:44,160
We can approximate that target function around a specific point in this case zero

143
00:12:44,640 --> 00:12:52,480
The approximation gets better the more terms we add where an infinite sum of terms is exactly equivalent to the target function

144
00:12:53,440 --> 00:12:55,440
If we know the target function

145
00:12:55,440 --> 00:13:01,840
We can actually derive the exact coefficients using a general formula to calculate each coefficient for each term

146
00:13:02,360 --> 00:13:07,920
But of course in our particular problem. We don't know the function. We only have a sample of data points

147
00:13:07,920 --> 00:13:09,920
So how do we find the coefficients?

148
00:13:10,600 --> 00:13:14,160
Well, do you see anything familiar in this weighted sum of terms?

149
00:13:14,680 --> 00:13:20,080
We can put all of the x to the n terms into an inputs vector and put all of the coefficients

150
00:13:20,120 --> 00:13:24,800
into a weights vector and then take the dot product a weighted sum

151
00:13:25,120 --> 00:13:29,400
The Taylor series is effectively a single layer neural network

152
00:13:29,440 --> 00:13:37,840
But one where we compute a bunch of additional inputs x squared x cubed and so on we'll call these additional inputs Taylor features

153
00:13:38,200 --> 00:13:42,400
We can then learn the coefficients or weights with back propagation

154
00:13:42,400 --> 00:13:43,320
Of course

155
00:13:43,320 --> 00:13:48,120
We can only compute a finite number of these the partial Taylor series up to some order

156
00:13:48,200 --> 00:13:50,480
But the higher the order the better it should do

157
00:13:51,000 --> 00:13:56,360
Let's use this simple Taylor network to learn this function using eight orders of the Taylor series

158
00:13:56,800 --> 00:13:59,480
Here's our data set and here's the approximation

159
00:14:05,080 --> 00:14:07,080
That's not great

160
00:14:07,080 --> 00:14:10,400
Polynomials are pretty touchy as their values can explode very quickly

161
00:14:10,400 --> 00:14:15,680
So I think back propagation has a tough time finding the right coefficients, but we can do better

162
00:14:16,120 --> 00:14:22,360
Rather than using a single layer network, let's just give these Taylor features to a full multi-layered network

163
00:14:22,560 --> 00:14:24,560
Let's give it a shot

164
00:14:35,120 --> 00:14:37,760
It's a bit wonky, but this performs much better

165
00:14:38,040 --> 00:14:44,440
This trick of computing additional features to feed to the network is a well-known and commonly used one

166
00:14:45,120 --> 00:14:51,280
Intuitively, it's like giving the network different kinds of mathematical building blocks to build a more diverse complex function

167
00:14:52,960 --> 00:14:54,960
Let's try this on an image data set

168
00:15:05,040 --> 00:15:11,400
Well, that's pretty good it's learning, but it doesn't seem to work any better than just using a good old-fashioned neural network

169
00:15:11,840 --> 00:15:16,520
The Taylor series is made to approximate a function around a single given point

170
00:15:16,680 --> 00:15:23,440
While we want to approximate within a given range of points a better tool for this is the Fourier series

171
00:15:24,520 --> 00:15:27,960
The Fourier series acts very much like the Taylor series

172
00:15:27,960 --> 00:15:30,720
But is an infinite sum of sines and cosines

173
00:15:31,200 --> 00:15:36,680
Each order n of the series is made up of sine nx plus cosine nx

174
00:15:37,200 --> 00:15:44,640
Each sine and cosine is multiplied by its own coefficient again controlling how much that term affects the overall function

175
00:15:44,880 --> 00:15:53,560
N, these inner multiplier values, control the frequency of each wave function. The higher the frequency, the more hills the curve has

176
00:15:54,440 --> 00:16:02,640
By combining weighted waves of different frequencies, we can approximate a function within the range of two pi, one full period

177
00:16:03,240 --> 00:16:10,120
Again, if we know the function, we can compute the weights and even if we don't we could use something called the discrete Fourier

178
00:16:10,360 --> 00:16:13,920
Transform, which is really cool, but we're not dealing with it in this video. I

179
00:16:14,560 --> 00:16:18,440
Hope you see where I'm going with this. Let's just jump ahead and do what we did before

180
00:16:19,040 --> 00:16:25,440
Compute a bunch of terms of the Fourier series and feed them to a multi-layer network as additional inputs

181
00:16:25,800 --> 00:16:27,800
Fourier features

182
00:16:27,800 --> 00:16:33,640
Note that we have twice as many Fourier features as Taylor features since we have a sine and cosine

183
00:16:34,280 --> 00:16:36,280
Let's try it on this data set

184
00:16:38,680 --> 00:16:41,960
This works pretty well. It's a little wavy, but not too shabby

185
00:16:42,360 --> 00:16:48,520
Note that for this to work we need to normalize our inputs between negative pi and positive pi one full period

186
00:16:49,120 --> 00:16:51,120
Let's try this on an image

187
00:16:51,280 --> 00:16:57,760
That looks strange at first almost like static coming into focus, but it works and it works really well

188
00:16:58,280 --> 00:17:02,240
If we compare it to networks of the same size trained for the same amount of time

189
00:17:02,240 --> 00:17:09,480
We can see the Fourier network learns much better and faster than the network without Fourier features or the one with Taylor features

190
00:17:09,640 --> 00:17:14,600
Just look at the level of detail in those curly locks. You can hardly tell the difference from the real image

191
00:17:15,040 --> 00:17:22,880
Now I've glossed over a very important detail the example Fourier series

192
00:17:22,880 --> 00:17:28,920
I gave had one input this function has two inputs to handle this properly

193
00:17:28,960 --> 00:17:34,960
We have to use the two-dimensional Fourier series one that takes an input of X and Y

194
00:17:35,280 --> 00:17:37,280
What do we do with that extra Y?

195
00:17:38,000 --> 00:17:42,200
Here are the terms for the 2d Fourier series up to two orders

196
00:17:42,560 --> 00:17:49,600
We are now multiplying the X and Y terms together and end up with sine X cosine Y sine X sine Y

197
00:17:49,760 --> 00:17:56,520
cosine X cosine Y and cosine X sine Y every combination of sine and cosine and Y and X

198
00:17:57,360 --> 00:18:02,520
Not only that we also have every combination of frequencies that inner multiplier

199
00:18:02,880 --> 00:18:07,320
So sine 2x times cosine 1y and so on and so forth

200
00:18:07,880 --> 00:18:10,520
Here's up to three orders now four

201
00:18:11,160 --> 00:18:13,160
That is a lot of terms

202
00:18:13,480 --> 00:18:16,680
we have to calculate this many terms per order and

203
00:18:16,840 --> 00:18:22,800
This number grows very quickly as we increase the order much faster than it would for the 1d series and

204
00:18:23,080 --> 00:18:29,160
This is just for a baby 2d input for a 3d 4d 5d input forget it

205
00:18:29,160 --> 00:18:33,040
The number of computations needed for higher dimensional Fourier series

206
00:18:33,440 --> 00:18:36,880
Explodes as we increase the dimensionality of our inputs

207
00:18:37,360 --> 00:18:40,640
We have encountered the curse of dimensionality

208
00:18:41,040 --> 00:18:46,920
Lots of methods of function approximation and machine learning break down as dimensionality grows

209
00:18:47,440 --> 00:18:51,000
These methods might work well on low dimensional problems, but they become

210
00:18:51,560 --> 00:18:55,840
computationally impractical or impossible for higher dimensional problems

211
00:18:56,520 --> 00:19:00,440
Neural networks by contrast handle the dimensionality problem very well

212
00:19:01,080 --> 00:19:04,040
Comparatively it is trivial to add additional dimensions

213
00:19:04,960 --> 00:19:08,160
But we don't need to use the 2d Fourier series

214
00:19:08,240 --> 00:19:15,160
We can just treat each input as its own independent variable and compute 1d Fourier features for each input

215
00:19:15,560 --> 00:19:19,360
This is less theoretically sound but much more practical to compute

216
00:19:19,440 --> 00:19:25,720
It's still a lot of additional features, but it's manageable and it's worth it. It drastically improves performance

217
00:19:25,800 --> 00:19:28,560
That's what I've been using to get these image approximations

218
00:19:29,160 --> 00:19:36,200
It really shouldn't be surprising that Fourier features help so much here since the Fourier series and transform is used to compress

219
00:19:36,240 --> 00:19:43,080
Images, it's how the JPEG compression algorithm works turns out lots of things can be represented as combinations of waves

220
00:19:43,800 --> 00:19:46,480
So let's apply it to our Mandelbrot data set

221
00:19:47,080 --> 00:19:52,040
Again, it looks a little weird, but it is definitely capturing more detail than the previous attempt

222
00:19:52,320 --> 00:20:00,000
Well, that's fun to watch, but let's evaluate for comparison here is the real Mandelbrot set

223
00:20:01,680 --> 00:20:08,320
Actually, no, this is not the real Mandelbrot set. It is an approximation from our Fourier network

224
00:20:09,520 --> 00:20:13,320
You might be able to tell if you're on a 4k monitor, especially when I zoom in

225
00:20:13,720 --> 00:20:15,720
This network was given

226
00:20:15,840 --> 00:20:18,560
56 orders of the Fourier series, which means a

227
00:20:19,280 --> 00:20:24,280
1024 extra Fourier features being fed to the network and the network itself is pretty damn big

228
00:20:25,480 --> 00:20:32,760
When we really zoom in it becomes very obvious that this is not the real deal. It is still missing an infinite amount of detail

229
00:20:40,440 --> 00:20:44,760
Nonetheless, I am blown away by the quality of the Fourier networks approximation

230
00:20:45,160 --> 00:20:51,000
Fourier features are of course not my idea. They come from this paper that was suggested by a reddit commenter

231
00:20:51,000 --> 00:20:53,000
Who I think actually may have been a co-author?

232
00:20:53,360 --> 00:20:55,360
I'm still missing details from this

233
00:20:55,800 --> 00:21:03,040
Adding Fourier features was one of if not the most effective improvements to the approximation I've applied and it was really surprising

234
00:21:03,600 --> 00:21:06,480
To return to the tricky spiral shell surface

235
00:21:06,520 --> 00:21:10,440
We can see that our Fourier network does way better than our previous attempt

236
00:21:10,600 --> 00:21:15,600
Although the target function is literally defined with sines and cosines, so of course it will do well

237
00:21:17,840 --> 00:21:25,560
So if Fourier features help so much why don't we use them more often they hardly ever show up in real-world neural networks

238
00:21:26,000 --> 00:21:30,920
To state the obvious all of the approximations in this video so far are completely useless

239
00:21:31,280 --> 00:21:36,240
We know the functions and the images. We don't need a massive neural network to approximate them

240
00:21:36,920 --> 00:21:42,680
But I hope that you can see that we're not studying the functions. We're studying the methods of approximation

241
00:21:43,520 --> 00:21:46,120
Because these toy problems are so low dimensional

242
00:21:46,120 --> 00:21:51,920
We can visualize them and hopefully gain insights that will carry over into higher dimensional problems

243
00:21:52,280 --> 00:21:56,240
So let's bring it back to earth with a real problem that uses real data

244
00:21:57,560 --> 00:22:02,760
This is the MNIST dataset images of hand-drawn numbers and their labels

245
00:22:03,320 --> 00:22:09,640
Our input is an entire image flattened out into a vector and our output is a vector of 10 values

246
00:22:09,840 --> 00:22:13,760
Representing a label as to which number 0 through 9 is in the image

247
00:22:14,360 --> 00:22:21,560
There is some unknown function that describes the relationship between an image and its label and that's what we're trying to discover

248
00:22:22,560 --> 00:22:26,480
Even for tiny 28 by 28 black and white images that is a

249
00:22:27,480 --> 00:22:33,720
784 dimensional input that is a lot and this is still a very simple problem for real-world problems

250
00:22:33,720 --> 00:22:40,640
We must address the curse of dimensionality our method must be able to handle huge dimensional inputs and outputs

251
00:22:40,840 --> 00:22:48,200
We also can't visualize the entire approximation all at once as before any idea what a 700 dimensional space looks like

252
00:22:48,800 --> 00:22:53,000
But a normal neural network can handle this problem. Just fine. It's pretty trivial

253
00:22:53,320 --> 00:23:00,280
We can evaluate it by measuring the accuracy of its predictions on images from the dataset that it did not see during training

254
00:23:00,360 --> 00:23:04,560
We'll call this evaluation accuracy and a small network does pretty well

255
00:23:04,800 --> 00:23:08,760
What if we use Fourier features on this problem say up to eight orders?

256
00:23:09,400 --> 00:23:15,280
Well, it does do a little better, but we're adding a lot of additional features for only eight orders

257
00:23:15,280 --> 00:23:17,280
We're computing a total of thirteen

258
00:23:18,160 --> 00:23:21,360
2328 input features which is a lot more than

259
00:23:21,840 --> 00:23:27,880
784 and it's only 2% more accurate when we use 32 orders of the Fourier series

260
00:23:27,880 --> 00:23:33,040
It actually seems to harm performance up to 64 orders and its downright ruiness

261
00:23:33,520 --> 00:23:39,800
This may be due to something called overfitting where our approximation learns the data really well too well

262
00:23:39,880 --> 00:23:42,480
But fails to learn the underlying function

263
00:23:42,960 --> 00:23:48,920
Usually this is a product of not having enough data, but our Fourier network seems to be especially prone to this

264
00:23:49,240 --> 00:23:56,160
This seems consistent with the conclusions of the paper. I mentioned earlier and ultimately our Fourier network seems to be very good for low

265
00:23:56,160 --> 00:23:59,800
Dimensional problems, but not very good for high dimensional problems

266
00:24:00,160 --> 00:24:04,480
No single architecture model or method is the best fit for all tasks

267
00:24:04,680 --> 00:24:09,400
Indeed, there are all kinds of problems that require different approaches than the ones discussed here

268
00:24:10,280 --> 00:24:15,960
Now I'd be surprised if the Fourier series didn't have more to teach us about machine learning, but this is where I'll leave it

269
00:24:15,960 --> 00:24:24,000
I hope this video has helped you appreciate what function approximation is and why it's useful and maybe sparked your imagination with some alternative perspectives

270
00:24:24,560 --> 00:24:30,680
Neural networks are a kind of mathematical clay that can be molded into arbitrary shapes for arbitrary purposes

271
00:24:31,840 --> 00:24:38,400
I want to finish by opening up the Mandelbrot approximation problem as a fun challenge for anyone who's interested

272
00:24:38,680 --> 00:24:44,920
How precisely and deeply can you approximate the Mandelbrot set given only a random sample of points?

273
00:24:45,440 --> 00:24:51,360
There are probably a million things that could be done to improve on my approximation and the internet is much smarter than I am

274
00:24:51,720 --> 00:24:56,680
The only rule is that your solution must still be a universal function approximator

275
00:24:56,880 --> 00:25:00,800
Meaning it could still learn any other data set of any dimensionality

276
00:25:01,520 --> 00:25:06,920
This is just for fun, but potentially solutions to this toy problem could have uses in the real world

277
00:25:07,320 --> 00:25:14,080
There is no reason to think that we found the best way of doing this and there may be far better solutions waiting to be discovered

278
00:25:14,960 --> 00:25:16,960
Thanks for watching

