1
00:00:00,000 --> 00:00:04,760
You are currently watching an artificial neural network learn.

2
00:00:04,760 --> 00:00:09,240
In particular, it's learning the shape of an infinitely complex fractal known as the

3
00:00:09,240 --> 00:00:11,040
Mandelbrot set.

4
00:00:11,040 --> 00:00:14,760
This is what that set looks like, complexity all the way down.

5
00:00:14,760 --> 00:00:19,440
Now, in order to understand how a neural network can learn the Mandelbrot set, really how it

6
00:00:19,440 --> 00:00:26,280
can learn anything at all, we will need to start with a fundamental mathematical concept.

7
00:00:26,280 --> 00:00:28,720
What is a function?

8
00:00:28,720 --> 00:00:35,280
Informally, a function is just a system of inputs and outputs, numbers in, numbers out.

9
00:00:35,280 --> 00:00:38,940
In this case, you input an x and it outputs a y.

10
00:00:38,940 --> 00:00:44,000
You can plot all of a function's x and y values in a graph where it draws out a line.

11
00:00:44,000 --> 00:00:48,440
What is important is that if you know the function, you can always calculate the correct

12
00:00:48,440 --> 00:00:52,200
output y given any input x.

13
00:00:52,200 --> 00:00:58,080
But say we don't know the function, and instead only know some of its x and y values.

14
00:00:58,080 --> 00:01:03,520
We know the inputs and outputs, but we don't know the function used to produce them.

15
00:01:03,520 --> 00:01:08,880
Is there a way to reverse engineer that function that produced this data?

16
00:01:08,880 --> 00:01:13,720
If we could construct such a function, we could use it to calculate a y value given

17
00:01:13,720 --> 00:01:17,160
an x value that is not in our original data set.

18
00:01:17,160 --> 00:01:21,360
This would work even if there was a little bit of noise in our data, a little randomness.

19
00:01:21,360 --> 00:01:26,080
We can still capture the overall pattern of the data and continue producing y values that

20
00:01:26,080 --> 00:01:29,200
aren't perfect, but close enough to be useful.

21
00:01:29,200 --> 00:01:35,520
What we need is a function approximation, and more generally, a function approximator.

22
00:01:35,520 --> 00:01:38,120
That is what a neural network is.

23
00:01:38,120 --> 00:01:42,760
This is an online tool for visualizing neural networks, and I'll link it in the description

24
00:01:42,760 --> 00:01:43,760
below.

25
00:01:43,760 --> 00:01:48,480
This particular network takes two inputs, x1 and x2, and produces one output.

26
00:01:48,480 --> 00:01:52,400
Technically, this function would create a three-dimensional surface, but it's easier

27
00:01:52,400 --> 00:01:54,520
to visualize in two dimensions.

28
00:01:54,520 --> 00:01:59,720
This image is rendered by passing the xy coordinate of each pixel into the network, which then

29
00:01:59,720 --> 00:02:04,480
produces a value between negative one and one that is used as the pixel value.

30
00:02:04,480 --> 00:02:08,280
These points are our data set, and are used to train the network.

31
00:02:08,280 --> 00:02:12,640
When we begin training, it quickly constructs a shape that accurately distinguishes between

32
00:02:12,640 --> 00:02:16,920
blue and orange points, building a decision boundary that separates them.

33
00:02:16,920 --> 00:02:22,760
It is approximating the function that describes the data, its learning, and is capable of learning

34
00:02:22,760 --> 00:02:26,040
the different data sets that we throw at it.

35
00:02:26,040 --> 00:02:27,640
So what is this middle section then?

36
00:02:27,640 --> 00:02:31,440
Well, as the name implies, this is the network of neurons.

37
00:02:31,440 --> 00:02:35,520
Each one of these nodes is a neuron, which takes in all the inputs from the previous

38
00:02:35,520 --> 00:02:40,760
layer of neurons and produces one output, which is then fed to the next layer.

39
00:02:40,760 --> 00:02:43,800
Inputs and outputs sounds like we're dealing with a function.

40
00:02:43,800 --> 00:02:49,200
Indeed, a neuron itself is just a function, one that can take any number of inputs and

41
00:02:49,200 --> 00:02:50,920
has one output.

42
00:02:50,920 --> 00:02:55,800
Each input is multiplied by a weight, and all are added together along with a bias.

43
00:02:55,800 --> 00:03:00,480
The weights and bias make up the parameters of this neuron, values that can change as

44
00:03:00,480 --> 00:03:01,640
the network learns.

45
00:03:01,640 --> 00:03:06,640
To keep it easy to visualize, we'll simplify this down to a two-dimensional function, with

46
00:03:06,640 --> 00:03:09,480
only one input and one output.

47
00:03:09,480 --> 00:03:13,920
Now neurons are our building blocks of the larger network, building blocks that can be

48
00:03:13,920 --> 00:03:19,040
stretched and squeezed and shifted around, and ultimately work with other blocks to construct

49
00:03:19,040 --> 00:03:21,440
something larger than themselves.

50
00:03:21,440 --> 00:03:24,720
The neuron, as we've defined it here, works like a building block.

51
00:03:24,720 --> 00:03:29,800
It is actually an extremely simple linear function, one which forms the flat line, or

52
00:03:29,800 --> 00:03:32,040
plain when there's more than one input.

53
00:03:32,040 --> 00:03:36,480
With the two parameters, the weight and bias, we can stretch and squeeze and move our function

54
00:03:36,480 --> 00:03:38,840
up and down and left and right.

55
00:03:38,840 --> 00:03:44,440
As such, we should be able to combine it with other neurons to form a more complicated function,

56
00:03:44,440 --> 00:03:47,620
one built from lots of linear functions.

57
00:03:47,620 --> 00:03:51,340
So let's start with a target function, one we want to approximate.

58
00:03:51,340 --> 00:03:55,620
I've hard-coded a bunch of neurons whose parameters were found manually, and if we

59
00:03:55,620 --> 00:04:00,340
weight each one and add them up, as would happen in the final neuron of the network,

60
00:04:00,340 --> 00:04:03,540
we should get a function that looks like the target function.

61
00:04:03,540 --> 00:04:06,180
Well, that didn't work at all.

62
00:04:06,180 --> 00:04:07,180
What happened?

63
00:04:07,180 --> 00:04:12,380
Well, if we simplify our equation, distributing weights and combining like terms, we end up

64
00:04:12,380 --> 00:04:14,900
with a single linear function.

65
00:04:14,900 --> 00:04:19,280
Turns out, linear functions can only combine to make one linear function.

66
00:04:19,280 --> 00:04:24,440
This is a big problem because we need to make something more complicated than just a line.

67
00:04:24,440 --> 00:04:28,960
We need something that is not linear, a non-linearity.

68
00:04:28,960 --> 00:04:33,060
In our case, we will be using a ReLU, a rectified linear unit.

69
00:04:33,060 --> 00:04:39,220
We use it as our activation function, meaning we simply apply it to our previous naive neuron.

70
00:04:39,220 --> 00:04:43,420
This is about as close as you can get to a linear function without actually being one,

71
00:04:43,420 --> 00:04:46,060
and we can tune it with the same parameters as before.

72
00:04:46,060 --> 00:04:50,700
However, you may notice that we can't actually lift the function off of the x-axis, which

73
00:04:50,700 --> 00:04:52,580
seems like a pretty big limitation.

74
00:04:52,580 --> 00:04:56,500
Well, let's give it a shot anyway, and see if it performs any better than our previous

75
00:04:56,500 --> 00:04:57,500
attempt.

76
00:04:57,500 --> 00:05:01,020
We're still trying to approximate the same function, and we're using the same weights

77
00:05:01,020 --> 00:05:06,420
and biases as before, but this time we're using a ReLU as our activation function.

78
00:05:06,420 --> 00:05:09,660
And just like that, the approximation looks way better.

79
00:05:09,660 --> 00:05:14,180
Unlike before, our function cannot simplify down to a flat linear function.

80
00:05:14,180 --> 00:05:18,380
If we add the neurons one by one, we can see the simple ReLU functions building on one

81
00:05:18,380 --> 00:05:23,100
another, and the inability for one neuron to lift itself off the x-axis doesn't seem

82
00:05:23,100 --> 00:05:24,580
to be a problem.

83
00:05:24,580 --> 00:05:29,140
Many neurons working together overcome the limitations of individual neurons.

84
00:05:29,140 --> 00:05:33,940
Now, I manually found these weights and biases, but how would you find them automatically?

85
00:05:33,940 --> 00:05:38,580
The most common algorithm for this is called back propagation, and is in fact what we're

86
00:05:38,580 --> 00:05:40,700
researching when we run this program.

87
00:05:40,700 --> 00:05:46,500
It essentially tweaks and tunes the parameters of the network bit by bit to improve the approximation,

88
00:05:46,500 --> 00:05:50,020
and the intricacies of this algorithm are really beyond the scope of this video.

89
00:05:50,020 --> 00:05:53,300
I'll link some better explanations in the description.

90
00:05:53,300 --> 00:05:57,380
Now we can see how this shape is formed, and why it looks like it's made up of sort of

91
00:05:57,380 --> 00:05:58,780
sharp linear edges.

92
00:05:58,780 --> 00:06:02,020
It's the nature of the activation function we're using.

93
00:06:02,020 --> 00:06:07,380
We can also see why, if we use no activation function at all, the network utterly fails

94
00:06:07,380 --> 00:06:08,380
to learn.

95
00:06:08,580 --> 00:06:11,380
We need those non-linearities.

96
00:06:11,380 --> 00:06:14,980
So what if we try learning a more complicated data set, like this spiral?

97
00:06:14,980 --> 00:06:17,700
Let's give it a go.

98
00:06:17,700 --> 00:06:20,540
Seems to be struggling a little bit to capture the pattern.

99
00:06:20,540 --> 00:06:21,540
No problem.

100
00:06:21,540 --> 00:06:25,500
If we need a more complicated function, we can add more building blocks, more neurons,

101
00:06:25,500 --> 00:06:27,260
and layers of neurons.

102
00:06:27,260 --> 00:06:31,340
And the network should be able to piece together a better approximation, something that really

103
00:06:31,340 --> 00:06:33,500
captures the spiral.

104
00:06:33,500 --> 00:06:37,340
It seems to be working.

105
00:06:37,340 --> 00:06:41,020
In fact, no matter what the data set is, we can learn it.

106
00:06:41,020 --> 00:06:46,700
That is because neural networks can be rigorously proven to be universal function approximators.

107
00:06:46,700 --> 00:06:52,220
They can approximate any function to any degree of precision you could ever want.

108
00:06:52,220 --> 00:06:54,900
You can always add more neurons.

109
00:06:54,900 --> 00:06:59,340
This is essentially the whole point of deep learning, because it means that neural networks

110
00:06:59,340 --> 00:07:05,740
can approximate anything that can be expressed as a function, a system of inputs and outputs.

111
00:07:05,740 --> 00:07:09,580
This is an extremely general way of thinking about the world.

112
00:07:09,580 --> 00:07:14,260
The Mandelbrot set, for instance, can be written as a function and learned all the same.

113
00:07:14,260 --> 00:07:19,020
This is just a scaled-up version of the experiment we were just looking at, but with an infinitely

114
00:07:19,020 --> 00:07:20,500
complex data set.

115
00:07:20,500 --> 00:07:24,260
We don't even really need to know what the Mandelbrot set is.

116
00:07:24,260 --> 00:07:27,820
The network learns it for us, and that's kind of the point.

117
00:07:27,820 --> 00:07:33,060
If you can express any intelligent behavior, any process, any task as a function, then

118
00:07:33,060 --> 00:07:34,940
a network can learn it.

119
00:07:34,940 --> 00:07:39,100
For instance, your input could be an image and your output a label as to whether it's

120
00:07:39,100 --> 00:07:44,060
a cat or a dog, or your input could be text in English and your output a translation to

121
00:07:44,060 --> 00:07:45,060
Spanish.

122
00:07:45,060 --> 00:07:49,460
You just need to be able to encode your inputs and outputs as numbers, but computers do this

123
00:07:49,460 --> 00:07:50,460
all the time.

124
00:07:50,460 --> 00:07:55,700
Images, video, text, audio, they can all be represented as numbers, and any processing

125
00:07:55,700 --> 00:07:59,740
you may want to do with this data, so long as you can write it as a function, can be

126
00:07:59,740 --> 00:08:02,300
emulated with a neural network.

127
00:08:02,300 --> 00:08:03,900
It goes deeper than this though.

128
00:08:03,900 --> 00:08:08,860
Under a few more assumptions, neural networks are provably turing complete, meaning they

129
00:08:08,860 --> 00:08:13,580
can solve all of the same kinds of problems that any computer can solve.

130
00:08:13,580 --> 00:08:18,300
An implication of this is that any algorithm written in any programming language can be

131
00:08:18,300 --> 00:08:23,580
simulated on a neural network, but rather than being manually written by a human, it

132
00:08:23,580 --> 00:08:27,580
can be learned automatically with a function approximator.

133
00:08:27,660 --> 00:08:35,540
Okay, that is not true.

134
00:08:35,540 --> 00:08:38,620
First off, you can't have an infinite number of neurons.

135
00:08:38,620 --> 00:08:44,420
There are practical limitations on network size and what can be modeled in the real world.

136
00:08:44,420 --> 00:08:49,140
I've also ignored the learning process in this video, and just assumed that you can

137
00:08:49,140 --> 00:08:52,180
find the optimal parameters magically.

138
00:08:52,180 --> 00:08:57,420
How you realistically do this introduces its own constraints on what can be learned.

139
00:08:57,420 --> 00:09:02,260
Additionally, in order for neural networks to approximate a function, you need the data

140
00:09:02,260 --> 00:09:04,940
that actually describes that function.

141
00:09:04,940 --> 00:09:09,180
If you don't have enough data, your approximation will be all wrong.

142
00:09:09,180 --> 00:09:13,100
It doesn't matter how many neurons you have or how sophisticated your network is, you

143
00:09:13,100 --> 00:09:16,860
just have no idea what your actual function should look like.

144
00:09:16,860 --> 00:09:21,100
It also doesn't make a lot of sense to use a function approximator when you already know

145
00:09:21,100 --> 00:09:22,180
the function.

146
00:09:22,180 --> 00:09:26,340
You wouldn't build a huge neural network to, say, learn the Mandobrot set when you can

147
00:09:26,340 --> 00:09:30,300
just write three lines of code to generate it, unless of course you want to make a cool

148
00:09:30,300 --> 00:09:32,780
background visual for a YouTube video.

149
00:09:32,780 --> 00:09:37,620
There are countless other issues that have to be considered, but for all these complications,

150
00:09:37,620 --> 00:09:42,060
neural networks have proven themselves to be indispensable for a number of really rather

151
00:09:42,060 --> 00:09:44,860
famously difficult problems for computers.

152
00:09:44,860 --> 00:09:50,180
Usually, these problems require a certain level of intuition and fuzzy logic that computers

153
00:09:50,180 --> 00:09:55,380
generally lack, and are very difficult for us to manually write programs to solve.

154
00:09:55,380 --> 00:09:59,500
Things like computer vision, natural language processing, and other areas of machine learning

155
00:09:59,500 --> 00:10:03,340
have been utterly transformed by neural networks.

156
00:10:03,340 --> 00:10:08,420
And this is all because of the humble function, a simple yet powerful way to think about the

157
00:10:08,420 --> 00:10:09,500
world.

158
00:10:09,500 --> 00:10:14,580
And by combining simple computations, we can get computers to construct any function we

159
00:10:14,580 --> 00:10:16,300
could ever want.

160
00:10:16,300 --> 00:10:18,820
Neural networks can learn almost anything.

161
00:10:25,380 --> 00:10:26,380
Thank you.

162
00:10:26,380 --> 00:10:27,380
Thank you.

163
00:10:27,380 --> 00:10:28,380
Thank you.

164
00:10:28,380 --> 00:10:29,380
Bye.

165
00:10:29,380 --> 00:10:30,380
Bye.

