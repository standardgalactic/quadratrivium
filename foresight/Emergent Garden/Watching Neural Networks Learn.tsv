start	end	text
0	4400	You are currently watching a neural network learn about a year ago
4400	10920	I made a video about how neural networks can learn almost anything and this is because they are universal function approximators
11200	17540	Why is that so important? Well, you might as well ask why functions are important. They are important because
18560	20400	functions
20400	22240	describe
22240	24240	the world
25200	28280	Everything is described by functions
28640	33260	That's right functions describe the sound of my voice on your eardrum
33720	37200	Function the light that's kind of hitting your eyeballs right now
38040	44520	Function different classes and mathematics different areas and mathematics study different kinds of function high school math studies second-degree
44520	50220	One variable polynomials calculus studies smooth one variable functions and it goes on and on
50840	53480	functions describe the world
55240	60240	Yes, correct. Thanks Thomas. He gets a little excited, but he's right
60240	65600	The world can fundamentally be described with numbers and relationships between numbers
65720	73400	We call those relationships functions and with functions we can understand model and predict the world around us
74040	78240	The goal of artificial intelligence is to write programs that can also
78680	82800	Understand model and predict the world or rather have them write themselves
82800	85520	So they must be able to build their own functions
85880	92200	That is the point of function approximation and that is what neural networks do. They are function building machines
92640	93600	In this video
93600	100640	I want to expand on the ideas of my previous video by watching actual neural networks learn strange shapes and strange spaces
101080	103880	Here we will encounter some very difficult challenges
104200	111000	Discover the limitations of neural networks and explore other methods for machine learning and mathematics to approach this open problem
111400	117120	Now I am a programmer not a mathematician and to be honest. I kind of hate math
117240	119800	I've always found it difficult and intimidating
119800	125240	But that's a bad attitude because math is unavoidably useful and occasionally beautiful
125240	128920	I'll do my best to keep things simple and accurate for an audience like me
128920	133000	But know that I'm gonna have to brush over a lot of things and I'm gonna be pretty informal
134120	138960	I recommend you watch my previous video, but to summarize functions are input output machines
139120	147000	They take an input set of numbers and output a corresponding set of numbers and the function defines the relationship between those numbers
147280	154400	The particular problem that neural network solve is when we don't know the definition of the function that we're trying to approximate
154880	161080	Instead we have a sample of data points from that function inputs and outputs. This is our data set
161280	168960	We must approximate a function that fits these data points and allows us to accurately predict outputs given inputs that are not in our
169000	175480	Data set this process is also called curve fitting and you can see why now this is not some handcrafted
175920	180360	Animation it is an actual neural network attempting to fit the curve to the data
180360	183640	And it does so by sort of bending the line into shape
184080	190840	This process is generalizable such that it can fit the curve to any data set and thus construct any function
191160	194280	This makes it a universal function approximator
197000	201640	The network itself is also a function and should approximate some unknown target function
202200	208120	The particular neural architecture we're dealing with in this video is called a fully connected feed forward network
208440	215160	Its inputs and outputs are sometimes called features and predictions and they take the form of vectors arrays of numbers
215600	221120	The overall function is made up of lots of simple functions called neurons that take many inputs
221120	228880	But only produce one output each input is multiplied by its own weight and added up along with one extra weight called a bias
229840	232880	Let's rewrite this weighted sum with some linear algebra
233040	240080	We can put our inputs into a vector with an extra one for the bias and our weights into another vector and then take what is called
240080	243360	The dot product. Let's just make up some example values
243840	249120	To take the dot product we multiply each input by each weight and then add them all up
249920	256560	Finally this dot product is then passed to a very simple activation function in this case a relu which here returns zero
257280	260720	We could use a different activation function, but a relu looks like this
261200	267800	The activation function defines the neurons mathematical shape while the weights shift and squeeze and stretch that shape
268400	274980	We feed the original inputs of our network to a layer of neurons each with their own learned weights and each with their own
275040	280560	Output value we stack these outputs together into a vector and then feed the output vector as
280680	286080	Inputs to the next layer and the next and the next until we get the final output of the network
286560	294160	Each neuron is responsible for learning its own little piece or feature of the overall function and by combining many neurons
294160	301080	We can build an ever more intricate function with an infinite number of neurons. We can provably build any function
302360	306840	The values of the weights or parameters are discovered through the training process
306840	312520	We give the network inputs from our data set and ask it to predict the correct outputs over and over and over
313000	321080	The goal is to minimize the network's error or loss which is some measurement of difference between the predicted outputs and the true outputs
322000	325480	Over time the network should do better and better as loss goes down
325720	330960	The algorithm for this is called back propagation, and I am again not going to explain it in this video
330960	334440	I'll make a video on it. Eventually. I promise. It's a pretty magical algorithm
336080	344300	However, this is a baby problem. What about functions with more than just one input or output that is to say higher dimensional problems
344760	350400	The dimensionality of a vector is defined by the number of numbers in that vector
350640	354280	For a higher dimensional problem. Let's try to learn an image
354680	361560	The input vector is the row column coordinates of a pixel and the output vector is the value of the pixel itself
361560	365520	In math speak we would say that this function maps from R2 to R1
365880	371520	Our data set is all of the pixels in an image. Let's use this unhappy man as an example a
371960	378000	Pixel value of zero is black and one is white although. I'm going to use different color schemes because it's pretty as
378280	382960	As we train we take snapshots of the learned function as the approximation improves
383360	386600	That's what you're saying now, and that's what you saw at the beginning of this video
387160	395080	But to clarify this image is not a single output from the network rather every individual pixel is a single output
395320	401040	We are looking at the entire function all at once and we can do this because it is very low dimensional
401720	407480	You'll also notice that the learning seems to slow down. It's not changing as abruptly as it was at the beginning
407840	414360	This is because we periodically reduce the learning rate a parameter that controls how much our training algorithm
414560	419000	Alters the current function this allows it to progressively refine details
420120	424800	Now just because our neural network should theoretically be able to learn any function
424800	430120	There are things we can do to practically improve the approximation and optimize the learning process
430840	435160	For instance one thing I'm doing here is normalizing the row column inputs
435160	437680	Which means I'm moving the values from a range of zero
438200	445080	1400 to the range of negative one one I do this with a simple linear transformation that shifts and scales the values
445280	450960	The negative one one range is easier for the network to deal with because it's smaller and centered at zero
452200	455920	Another trick is that I'm not using a relu as my activation function
455920	463080	But rather something called a leaky relu a leaky relu can output negative values while still being non-linear and has been shown to
463080	468800	Generally improve performance. So I'm using a leaky relu in all of my layers except for the last one
469480	473960	Because the final output is a pixel value. It needs to be between zero and one
474320	476440	To enforce this in the final layer
476440	481920	We can use a sigmoid activation function which squishes its inputs between zero and one
483040	489280	Except there is a different squishing function called tan H that squishes its inputs between negative one and one
489280	495160	I can then normalize those outputs into the final range of zero one. Why go through the trouble?
495160	498440	Well, tan H just tends to work better than sigmoid
499480	504320	Intuitively, this is because tan H is centered at zero and plays much nicer with back propagation
504320	510240	But ultimately the reasoning doesn't matter as much as the results both networks here are theoretically
510400	514840	Universal function approximators, but practically one works much better than the other
515200	520040	This can be measured empirically by calculating and comparing the error rates of both networks
520040	525880	I think of this as the science of math where we must test our ideas and validate them with evidence
526000	528040	Rather than providing formal proofs
528240	531640	It'd be great if we could do both but that is not always possible
531640	536240	And it is often much easier to just try and see what happens and that's my kind of math
537400	544240	Let's make it harder. Here. We have a function that takes two inputs u v and produces three outputs x y z
544440	548400	It's a parametric surface function and we'll use the equation for a sphere
548640	556080	We can learn it the same way as before take a random sample of points across the surface of the sphere and ask our network to approximate it
556400	558920	Now this is clearly a very silly way to make a sphere
558920	564400	But the network is trying its best to sort of wrap the surface around the sphere to fit the data points
564400	568520	I hope this also gives you a better view of what a parametric surface is
568520	573920	It takes a flat 2d sheet and contorts it in 3d space according to some function
575240	579320	Now this does okay though it never quite closes up around the poles
580680	584320	For a real challenge, let's try this beautiful spiral shell surface
584320	589640	I got the equation for this from this wonderful little website that lets you play with all kinds of shell surfaces
589800	592600	See what I mean when I say that functions describe the world
593320	597200	Anyway, let's sample some points across the spiral surface and start learning
605200	613640	Well, it's working, but clearly we're having some trouble here. I'm using a fairly big neural network
613640	619520	But this is a complicated shape and it seems to be getting a little bit confused. We'll come back to this one
621320	627880	We can also make the problem harder not by increasing dimensionality, but by increasing the complexity of the function itself
628400	632800	Let's use the Mandelbrot set an infinitely complex fractal
633280	639240	But we can simply define a Mandelbrot function as taking two real valued inputs and producing one output
639240	641880	The same dimensionality as the images we learned earlier
642280	646560	Now I've defined my Mandelbrot function to output a value between zero and one
646680	650680	Where one is in the Mandelbrot set and anything less than one is not
651160	654680	Under the hood, it's iteratively operating on complex numbers
654680	658200	And I added some stuff to output smooth values between zero and one
658200	660400	But I'm not going to explain it much more than that
660600	664920	After all a neural network doesn't know the function definition either and it shouldn't matter
664920	667360	It should be able to approximate it all the same
667720	672240	The data set here is randomized points drawn uniformly from this range
672760	679040	Now this has actually been a pet project of mine for some time and I've made several videos trying this exact experiment over the years
679040	681040	I hope you can see why it's interesting
681920	689840	Despite being so low dimensional the Mandelbrot function is infinitely complex literally made with complex numbers and is uniquely difficult to approximate
690400	695640	You can just keep fitting and fitting and fitting the function and you will always come up short
696160	700120	You could do this with any fractal. I just use the Mandelbrot set because it's so well known
702280	708320	So after training for a while we've made some progress, but clearly we're still missing an infinite amount of detail
708560	712840	I've gotten this to look better in the past, but I'm not going to waste any more time training this network
712960	714960	There are better ways of doing this
715640	720920	Are there different methods for approximating functions besides neural networks?
721360	727720	Yes, many actually. There are always many ways to solve the same problem though some ways are better than others
728200	731760	Another mathematical tool we can use is called the Taylor series
732400	736280	This is an infinite sum of a sequence of polynomial functions
736280	741680	x plus x squared plus x cubed plus x to the fourth up to x to the n
741680	744200	n is the order of the series
744480	749120	Each of these terms are multiplied by their own value called a coefficient
749560	754880	Each coefficient controls how much that individual term affects the overall function
756120	759360	Given some target function by choosing the right coefficients
759360	764160	We can approximate that target function around a specific point in this case zero
764640	772480	The approximation gets better the more terms we add where an infinite sum of terms is exactly equivalent to the target function
773440	775440	If we know the target function
775440	781840	We can actually derive the exact coefficients using a general formula to calculate each coefficient for each term
782360	787920	But of course in our particular problem. We don't know the function. We only have a sample of data points
787920	789920	So how do we find the coefficients?
790600	794160	Well, do you see anything familiar in this weighted sum of terms?
794680	800080	We can put all of the x to the n terms into an inputs vector and put all of the coefficients
800120	804800	into a weights vector and then take the dot product a weighted sum
805120	809400	The Taylor series is effectively a single layer neural network
809440	817840	But one where we compute a bunch of additional inputs x squared x cubed and so on we'll call these additional inputs Taylor features
818200	822400	We can then learn the coefficients or weights with back propagation
822400	823320	Of course
823320	828120	We can only compute a finite number of these the partial Taylor series up to some order
828200	830480	But the higher the order the better it should do
831000	836360	Let's use this simple Taylor network to learn this function using eight orders of the Taylor series
836800	839480	Here's our data set and here's the approximation
845080	847080	That's not great
847080	850400	Polynomials are pretty touchy as their values can explode very quickly
850400	855680	So I think back propagation has a tough time finding the right coefficients, but we can do better
856120	862360	Rather than using a single layer network, let's just give these Taylor features to a full multi-layered network
862560	864560	Let's give it a shot
875120	877760	It's a bit wonky, but this performs much better
878040	884440	This trick of computing additional features to feed to the network is a well-known and commonly used one
885120	891280	Intuitively, it's like giving the network different kinds of mathematical building blocks to build a more diverse complex function
892960	894960	Let's try this on an image data set
905040	911400	Well, that's pretty good it's learning, but it doesn't seem to work any better than just using a good old-fashioned neural network
911840	916520	The Taylor series is made to approximate a function around a single given point
916680	923440	While we want to approximate within a given range of points a better tool for this is the Fourier series
924520	927960	The Fourier series acts very much like the Taylor series
927960	930720	But is an infinite sum of sines and cosines
931200	936680	Each order n of the series is made up of sine nx plus cosine nx
937200	944640	Each sine and cosine is multiplied by its own coefficient again controlling how much that term affects the overall function
944880	953560	N, these inner multiplier values, control the frequency of each wave function. The higher the frequency, the more hills the curve has
954440	962640	By combining weighted waves of different frequencies, we can approximate a function within the range of two pi, one full period
963240	970120	Again, if we know the function, we can compute the weights and even if we don't we could use something called the discrete Fourier
970360	973920	Transform, which is really cool, but we're not dealing with it in this video. I
974560	978440	Hope you see where I'm going with this. Let's just jump ahead and do what we did before
979040	985440	Compute a bunch of terms of the Fourier series and feed them to a multi-layer network as additional inputs
985800	987800	Fourier features
987800	993640	Note that we have twice as many Fourier features as Taylor features since we have a sine and cosine
994280	996280	Let's try it on this data set
998680	1001960	This works pretty well. It's a little wavy, but not too shabby
1002360	1008520	Note that for this to work we need to normalize our inputs between negative pi and positive pi one full period
1009120	1011120	Let's try this on an image
1011280	1017760	That looks strange at first almost like static coming into focus, but it works and it works really well
1018280	1022240	If we compare it to networks of the same size trained for the same amount of time
1022240	1029480	We can see the Fourier network learns much better and faster than the network without Fourier features or the one with Taylor features
1029640	1034600	Just look at the level of detail in those curly locks. You can hardly tell the difference from the real image
1035040	1042880	Now I've glossed over a very important detail the example Fourier series
1042880	1048920	I gave had one input this function has two inputs to handle this properly
1048960	1054960	We have to use the two-dimensional Fourier series one that takes an input of X and Y
1055280	1057280	What do we do with that extra Y?
1058000	1062200	Here are the terms for the 2d Fourier series up to two orders
1062560	1069600	We are now multiplying the X and Y terms together and end up with sine X cosine Y sine X sine Y
1069760	1076520	cosine X cosine Y and cosine X sine Y every combination of sine and cosine and Y and X
1077360	1082520	Not only that we also have every combination of frequencies that inner multiplier
1082880	1087320	So sine 2x times cosine 1y and so on and so forth
1087880	1090520	Here's up to three orders now four
1091160	1093160	That is a lot of terms
1093480	1096680	we have to calculate this many terms per order and
1096840	1102800	This number grows very quickly as we increase the order much faster than it would for the 1d series and
1103080	1109160	This is just for a baby 2d input for a 3d 4d 5d input forget it
1109160	1113040	The number of computations needed for higher dimensional Fourier series
1113440	1116880	Explodes as we increase the dimensionality of our inputs
1117360	1120640	We have encountered the curse of dimensionality
1121040	1126920	Lots of methods of function approximation and machine learning break down as dimensionality grows
1127440	1131000	These methods might work well on low dimensional problems, but they become
1131560	1135840	computationally impractical or impossible for higher dimensional problems
1136520	1140440	Neural networks by contrast handle the dimensionality problem very well
1141080	1144040	Comparatively it is trivial to add additional dimensions
1144960	1148160	But we don't need to use the 2d Fourier series
1148240	1155160	We can just treat each input as its own independent variable and compute 1d Fourier features for each input
1155560	1159360	This is less theoretically sound but much more practical to compute
1159440	1165720	It's still a lot of additional features, but it's manageable and it's worth it. It drastically improves performance
1165800	1168560	That's what I've been using to get these image approximations
1169160	1176200	It really shouldn't be surprising that Fourier features help so much here since the Fourier series and transform is used to compress
1176240	1183080	Images, it's how the JPEG compression algorithm works turns out lots of things can be represented as combinations of waves
1183800	1186480	So let's apply it to our Mandelbrot data set
1187080	1192040	Again, it looks a little weird, but it is definitely capturing more detail than the previous attempt
1192320	1200000	Well, that's fun to watch, but let's evaluate for comparison here is the real Mandelbrot set
1201680	1208320	Actually, no, this is not the real Mandelbrot set. It is an approximation from our Fourier network
1209520	1213320	You might be able to tell if you're on a 4k monitor, especially when I zoom in
1213720	1215720	This network was given
1215840	1218560	56 orders of the Fourier series, which means a
1219280	1224280	1024 extra Fourier features being fed to the network and the network itself is pretty damn big
1225480	1232760	When we really zoom in it becomes very obvious that this is not the real deal. It is still missing an infinite amount of detail
1240440	1244760	Nonetheless, I am blown away by the quality of the Fourier networks approximation
1245160	1251000	Fourier features are of course not my idea. They come from this paper that was suggested by a reddit commenter
1251000	1253000	Who I think actually may have been a co-author?
1253360	1255360	I'm still missing details from this
1255800	1263040	Adding Fourier features was one of if not the most effective improvements to the approximation I've applied and it was really surprising
1263600	1266480	To return to the tricky spiral shell surface
1266520	1270440	We can see that our Fourier network does way better than our previous attempt
1270600	1275600	Although the target function is literally defined with sines and cosines, so of course it will do well
1277840	1285560	So if Fourier features help so much why don't we use them more often they hardly ever show up in real-world neural networks
1286000	1290920	To state the obvious all of the approximations in this video so far are completely useless
1291280	1296240	We know the functions and the images. We don't need a massive neural network to approximate them
1296920	1302680	But I hope that you can see that we're not studying the functions. We're studying the methods of approximation
1303520	1306120	Because these toy problems are so low dimensional
1306120	1311920	We can visualize them and hopefully gain insights that will carry over into higher dimensional problems
1312280	1316240	So let's bring it back to earth with a real problem that uses real data
1317560	1322760	This is the MNIST dataset images of hand-drawn numbers and their labels
1323320	1329640	Our input is an entire image flattened out into a vector and our output is a vector of 10 values
1329840	1333760	Representing a label as to which number 0 through 9 is in the image
1334360	1341560	There is some unknown function that describes the relationship between an image and its label and that's what we're trying to discover
1342560	1346480	Even for tiny 28 by 28 black and white images that is a
1347480	1353720	784 dimensional input that is a lot and this is still a very simple problem for real-world problems
1353720	1360640	We must address the curse of dimensionality our method must be able to handle huge dimensional inputs and outputs
1360840	1368200	We also can't visualize the entire approximation all at once as before any idea what a 700 dimensional space looks like
1368800	1373000	But a normal neural network can handle this problem. Just fine. It's pretty trivial
1373320	1380280	We can evaluate it by measuring the accuracy of its predictions on images from the dataset that it did not see during training
1380360	1384560	We'll call this evaluation accuracy and a small network does pretty well
1384800	1388760	What if we use Fourier features on this problem say up to eight orders?
1389400	1395280	Well, it does do a little better, but we're adding a lot of additional features for only eight orders
1395280	1397280	We're computing a total of thirteen
1398160	1401360	2328 input features which is a lot more than
1401840	1407880	784 and it's only 2% more accurate when we use 32 orders of the Fourier series
1407880	1413040	It actually seems to harm performance up to 64 orders and its downright ruiness
1413520	1419800	This may be due to something called overfitting where our approximation learns the data really well too well
1419880	1422480	But fails to learn the underlying function
1422960	1428920	Usually this is a product of not having enough data, but our Fourier network seems to be especially prone to this
1429240	1436160	This seems consistent with the conclusions of the paper. I mentioned earlier and ultimately our Fourier network seems to be very good for low
1436160	1439800	Dimensional problems, but not very good for high dimensional problems
1440160	1444480	No single architecture model or method is the best fit for all tasks
1444680	1449400	Indeed, there are all kinds of problems that require different approaches than the ones discussed here
1450280	1455960	Now I'd be surprised if the Fourier series didn't have more to teach us about machine learning, but this is where I'll leave it
1455960	1464000	I hope this video has helped you appreciate what function approximation is and why it's useful and maybe sparked your imagination with some alternative perspectives
1464560	1470680	Neural networks are a kind of mathematical clay that can be molded into arbitrary shapes for arbitrary purposes
1471840	1478400	I want to finish by opening up the Mandelbrot approximation problem as a fun challenge for anyone who's interested
1478680	1484920	How precisely and deeply can you approximate the Mandelbrot set given only a random sample of points?
1485440	1491360	There are probably a million things that could be done to improve on my approximation and the internet is much smarter than I am
1491720	1496680	The only rule is that your solution must still be a universal function approximator
1496880	1500800	Meaning it could still learn any other data set of any dimensionality
1501520	1506920	This is just for fun, but potentially solutions to this toy problem could have uses in the real world
1507320	1514080	There is no reason to think that we found the best way of doing this and there may be far better solutions waiting to be discovered
1514960	1516960	Thanks for watching
