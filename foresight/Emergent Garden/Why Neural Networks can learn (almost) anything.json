{"text": " You are currently watching an artificial neural network learn. In particular, it's learning the shape of an infinitely complex fractal known as the Mandelbrot set. This is what that set looks like, complexity all the way down. Now, in order to understand how a neural network can learn the Mandelbrot set, really how it can learn anything at all, we will need to start with a fundamental mathematical concept. What is a function? Informally, a function is just a system of inputs and outputs, numbers in, numbers out. In this case, you input an x and it outputs a y. You can plot all of a function's x and y values in a graph where it draws out a line. What is important is that if you know the function, you can always calculate the correct output y given any input x. But say we don't know the function, and instead only know some of its x and y values. We know the inputs and outputs, but we don't know the function used to produce them. Is there a way to reverse engineer that function that produced this data? If we could construct such a function, we could use it to calculate a y value given an x value that is not in our original data set. This would work even if there was a little bit of noise in our data, a little randomness. We can still capture the overall pattern of the data and continue producing y values that aren't perfect, but close enough to be useful. What we need is a function approximation, and more generally, a function approximator. That is what a neural network is. This is an online tool for visualizing neural networks, and I'll link it in the description below. This particular network takes two inputs, x1 and x2, and produces one output. Technically, this function would create a three-dimensional surface, but it's easier to visualize in two dimensions. This image is rendered by passing the xy coordinate of each pixel into the network, which then produces a value between negative one and one that is used as the pixel value. These points are our data set, and are used to train the network. When we begin training, it quickly constructs a shape that accurately distinguishes between blue and orange points, building a decision boundary that separates them. It is approximating the function that describes the data, its learning, and is capable of learning the different data sets that we throw at it. So what is this middle section then? Well, as the name implies, this is the network of neurons. Each one of these nodes is a neuron, which takes in all the inputs from the previous layer of neurons and produces one output, which is then fed to the next layer. Inputs and outputs sounds like we're dealing with a function. Indeed, a neuron itself is just a function, one that can take any number of inputs and has one output. Each input is multiplied by a weight, and all are added together along with a bias. The weights and bias make up the parameters of this neuron, values that can change as the network learns. To keep it easy to visualize, we'll simplify this down to a two-dimensional function, with only one input and one output. Now neurons are our building blocks of the larger network, building blocks that can be stretched and squeezed and shifted around, and ultimately work with other blocks to construct something larger than themselves. The neuron, as we've defined it here, works like a building block. It is actually an extremely simple linear function, one which forms the flat line, or plain when there's more than one input. With the two parameters, the weight and bias, we can stretch and squeeze and move our function up and down and left and right. As such, we should be able to combine it with other neurons to form a more complicated function, one built from lots of linear functions. So let's start with a target function, one we want to approximate. I've hard-coded a bunch of neurons whose parameters were found manually, and if we weight each one and add them up, as would happen in the final neuron of the network, we should get a function that looks like the target function. Well, that didn't work at all. What happened? Well, if we simplify our equation, distributing weights and combining like terms, we end up with a single linear function. Turns out, linear functions can only combine to make one linear function. This is a big problem because we need to make something more complicated than just a line. We need something that is not linear, a non-linearity. In our case, we will be using a ReLU, a rectified linear unit. We use it as our activation function, meaning we simply apply it to our previous naive neuron. This is about as close as you can get to a linear function without actually being one, and we can tune it with the same parameters as before. However, you may notice that we can't actually lift the function off of the x-axis, which seems like a pretty big limitation. Well, let's give it a shot anyway, and see if it performs any better than our previous attempt. We're still trying to approximate the same function, and we're using the same weights and biases as before, but this time we're using a ReLU as our activation function. And just like that, the approximation looks way better. Unlike before, our function cannot simplify down to a flat linear function. If we add the neurons one by one, we can see the simple ReLU functions building on one another, and the inability for one neuron to lift itself off the x-axis doesn't seem to be a problem. Many neurons working together overcome the limitations of individual neurons. Now, I manually found these weights and biases, but how would you find them automatically? The most common algorithm for this is called back propagation, and is in fact what we're researching when we run this program. It essentially tweaks and tunes the parameters of the network bit by bit to improve the approximation, and the intricacies of this algorithm are really beyond the scope of this video. I'll link some better explanations in the description. Now we can see how this shape is formed, and why it looks like it's made up of sort of sharp linear edges. It's the nature of the activation function we're using. We can also see why, if we use no activation function at all, the network utterly fails to learn. We need those non-linearities. So what if we try learning a more complicated data set, like this spiral? Let's give it a go. Seems to be struggling a little bit to capture the pattern. No problem. If we need a more complicated function, we can add more building blocks, more neurons, and layers of neurons. And the network should be able to piece together a better approximation, something that really captures the spiral. It seems to be working. In fact, no matter what the data set is, we can learn it. That is because neural networks can be rigorously proven to be universal function approximators. They can approximate any function to any degree of precision you could ever want. You can always add more neurons. This is essentially the whole point of deep learning, because it means that neural networks can approximate anything that can be expressed as a function, a system of inputs and outputs. This is an extremely general way of thinking about the world. The Mandelbrot set, for instance, can be written as a function and learned all the same. This is just a scaled-up version of the experiment we were just looking at, but with an infinitely complex data set. We don't even really need to know what the Mandelbrot set is. The network learns it for us, and that's kind of the point. If you can express any intelligent behavior, any process, any task as a function, then a network can learn it. For instance, your input could be an image and your output a label as to whether it's a cat or a dog, or your input could be text in English and your output a translation to Spanish. You just need to be able to encode your inputs and outputs as numbers, but computers do this all the time. Images, video, text, audio, they can all be represented as numbers, and any processing you may want to do with this data, so long as you can write it as a function, can be emulated with a neural network. It goes deeper than this though. Under a few more assumptions, neural networks are provably turing complete, meaning they can solve all of the same kinds of problems that any computer can solve. An implication of this is that any algorithm written in any programming language can be simulated on a neural network, but rather than being manually written by a human, it can be learned automatically with a function approximator. Okay, that is not true. First off, you can't have an infinite number of neurons. There are practical limitations on network size and what can be modeled in the real world. I've also ignored the learning process in this video, and just assumed that you can find the optimal parameters magically. How you realistically do this introduces its own constraints on what can be learned. Additionally, in order for neural networks to approximate a function, you need the data that actually describes that function. If you don't have enough data, your approximation will be all wrong. It doesn't matter how many neurons you have or how sophisticated your network is, you just have no idea what your actual function should look like. It also doesn't make a lot of sense to use a function approximator when you already know the function. You wouldn't build a huge neural network to, say, learn the Mandobrot set when you can just write three lines of code to generate it, unless of course you want to make a cool background visual for a YouTube video. There are countless other issues that have to be considered, but for all these complications, neural networks have proven themselves to be indispensable for a number of really rather famously difficult problems for computers. Usually, these problems require a certain level of intuition and fuzzy logic that computers generally lack, and are very difficult for us to manually write programs to solve. Things like computer vision, natural language processing, and other areas of machine learning have been utterly transformed by neural networks. And this is all because of the humble function, a simple yet powerful way to think about the world. And by combining simple computations, we can get computers to construct any function we could ever want. Neural networks can learn almost anything. Thank you. Thank you. Thank you. Bye. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.76, "text": " You are currently watching an artificial neural network learn.", "tokens": [50364, 509, 366, 4362, 1976, 364, 11677, 18161, 3209, 1466, 13, 50602], "temperature": 0.0, "avg_logprob": -0.15827902678017305, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0045362780801951885}, {"id": 1, "seek": 0, "start": 4.76, "end": 9.24, "text": " In particular, it's learning the shape of an infinitely complex fractal known as the", "tokens": [50602, 682, 1729, 11, 309, 311, 2539, 264, 3909, 295, 364, 36227, 3997, 17948, 304, 2570, 382, 264, 50826], "temperature": 0.0, "avg_logprob": -0.15827902678017305, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0045362780801951885}, {"id": 2, "seek": 0, "start": 9.24, "end": 11.040000000000001, "text": " Mandelbrot set.", "tokens": [50826, 15458, 338, 1443, 310, 992, 13, 50916], "temperature": 0.0, "avg_logprob": -0.15827902678017305, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0045362780801951885}, {"id": 3, "seek": 0, "start": 11.040000000000001, "end": 14.76, "text": " This is what that set looks like, complexity all the way down.", "tokens": [50916, 639, 307, 437, 300, 992, 1542, 411, 11, 14024, 439, 264, 636, 760, 13, 51102], "temperature": 0.0, "avg_logprob": -0.15827902678017305, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0045362780801951885}, {"id": 4, "seek": 0, "start": 14.76, "end": 19.44, "text": " Now, in order to understand how a neural network can learn the Mandelbrot set, really how it", "tokens": [51102, 823, 11, 294, 1668, 281, 1223, 577, 257, 18161, 3209, 393, 1466, 264, 15458, 338, 1443, 310, 992, 11, 534, 577, 309, 51336], "temperature": 0.0, "avg_logprob": -0.15827902678017305, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0045362780801951885}, {"id": 5, "seek": 0, "start": 19.44, "end": 26.28, "text": " can learn anything at all, we will need to start with a fundamental mathematical concept.", "tokens": [51336, 393, 1466, 1340, 412, 439, 11, 321, 486, 643, 281, 722, 365, 257, 8088, 18894, 3410, 13, 51678], "temperature": 0.0, "avg_logprob": -0.15827902678017305, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0045362780801951885}, {"id": 6, "seek": 0, "start": 26.28, "end": 28.72, "text": " What is a function?", "tokens": [51678, 708, 307, 257, 2445, 30, 51800], "temperature": 0.0, "avg_logprob": -0.15827902678017305, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0045362780801951885}, {"id": 7, "seek": 2872, "start": 28.72, "end": 35.28, "text": " Informally, a function is just a system of inputs and outputs, numbers in, numbers out.", "tokens": [50364, 34301, 379, 11, 257, 2445, 307, 445, 257, 1185, 295, 15743, 293, 23930, 11, 3547, 294, 11, 3547, 484, 13, 50692], "temperature": 0.0, "avg_logprob": -0.10943643321161685, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.005729063879698515}, {"id": 8, "seek": 2872, "start": 35.28, "end": 38.94, "text": " In this case, you input an x and it outputs a y.", "tokens": [50692, 682, 341, 1389, 11, 291, 4846, 364, 2031, 293, 309, 23930, 257, 288, 13, 50875], "temperature": 0.0, "avg_logprob": -0.10943643321161685, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.005729063879698515}, {"id": 9, "seek": 2872, "start": 38.94, "end": 44.0, "text": " You can plot all of a function's x and y values in a graph where it draws out a line.", "tokens": [50875, 509, 393, 7542, 439, 295, 257, 2445, 311, 2031, 293, 288, 4190, 294, 257, 4295, 689, 309, 20045, 484, 257, 1622, 13, 51128], "temperature": 0.0, "avg_logprob": -0.10943643321161685, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.005729063879698515}, {"id": 10, "seek": 2872, "start": 44.0, "end": 48.44, "text": " What is important is that if you know the function, you can always calculate the correct", "tokens": [51128, 708, 307, 1021, 307, 300, 498, 291, 458, 264, 2445, 11, 291, 393, 1009, 8873, 264, 3006, 51350], "temperature": 0.0, "avg_logprob": -0.10943643321161685, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.005729063879698515}, {"id": 11, "seek": 2872, "start": 48.44, "end": 52.2, "text": " output y given any input x.", "tokens": [51350, 5598, 288, 2212, 604, 4846, 2031, 13, 51538], "temperature": 0.0, "avg_logprob": -0.10943643321161685, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.005729063879698515}, {"id": 12, "seek": 2872, "start": 52.2, "end": 58.08, "text": " But say we don't know the function, and instead only know some of its x and y values.", "tokens": [51538, 583, 584, 321, 500, 380, 458, 264, 2445, 11, 293, 2602, 787, 458, 512, 295, 1080, 2031, 293, 288, 4190, 13, 51832], "temperature": 0.0, "avg_logprob": -0.10943643321161685, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.005729063879698515}, {"id": 13, "seek": 5808, "start": 58.08, "end": 63.519999999999996, "text": " We know the inputs and outputs, but we don't know the function used to produce them.", "tokens": [50364, 492, 458, 264, 15743, 293, 23930, 11, 457, 321, 500, 380, 458, 264, 2445, 1143, 281, 5258, 552, 13, 50636], "temperature": 0.0, "avg_logprob": -0.06674751482511822, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0004305348265916109}, {"id": 14, "seek": 5808, "start": 63.519999999999996, "end": 68.88, "text": " Is there a way to reverse engineer that function that produced this data?", "tokens": [50636, 1119, 456, 257, 636, 281, 9943, 11403, 300, 2445, 300, 7126, 341, 1412, 30, 50904], "temperature": 0.0, "avg_logprob": -0.06674751482511822, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0004305348265916109}, {"id": 15, "seek": 5808, "start": 68.88, "end": 73.72, "text": " If we could construct such a function, we could use it to calculate a y value given", "tokens": [50904, 759, 321, 727, 7690, 1270, 257, 2445, 11, 321, 727, 764, 309, 281, 8873, 257, 288, 2158, 2212, 51146], "temperature": 0.0, "avg_logprob": -0.06674751482511822, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0004305348265916109}, {"id": 16, "seek": 5808, "start": 73.72, "end": 77.16, "text": " an x value that is not in our original data set.", "tokens": [51146, 364, 2031, 2158, 300, 307, 406, 294, 527, 3380, 1412, 992, 13, 51318], "temperature": 0.0, "avg_logprob": -0.06674751482511822, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0004305348265916109}, {"id": 17, "seek": 5808, "start": 77.16, "end": 81.36, "text": " This would work even if there was a little bit of noise in our data, a little randomness.", "tokens": [51318, 639, 576, 589, 754, 498, 456, 390, 257, 707, 857, 295, 5658, 294, 527, 1412, 11, 257, 707, 4974, 1287, 13, 51528], "temperature": 0.0, "avg_logprob": -0.06674751482511822, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0004305348265916109}, {"id": 18, "seek": 5808, "start": 81.36, "end": 86.08, "text": " We can still capture the overall pattern of the data and continue producing y values that", "tokens": [51528, 492, 393, 920, 7983, 264, 4787, 5102, 295, 264, 1412, 293, 2354, 10501, 288, 4190, 300, 51764], "temperature": 0.0, "avg_logprob": -0.06674751482511822, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0004305348265916109}, {"id": 19, "seek": 8608, "start": 86.08, "end": 89.2, "text": " aren't perfect, but close enough to be useful.", "tokens": [50364, 3212, 380, 2176, 11, 457, 1998, 1547, 281, 312, 4420, 13, 50520], "temperature": 0.0, "avg_logprob": -0.12455427750297214, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.429709792137146}, {"id": 20, "seek": 8608, "start": 89.2, "end": 95.52, "text": " What we need is a function approximation, and more generally, a function approximator.", "tokens": [50520, 708, 321, 643, 307, 257, 2445, 28023, 11, 293, 544, 5101, 11, 257, 2445, 8542, 1639, 13, 50836], "temperature": 0.0, "avg_logprob": -0.12455427750297214, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.429709792137146}, {"id": 21, "seek": 8608, "start": 95.52, "end": 98.12, "text": " That is what a neural network is.", "tokens": [50836, 663, 307, 437, 257, 18161, 3209, 307, 13, 50966], "temperature": 0.0, "avg_logprob": -0.12455427750297214, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.429709792137146}, {"id": 22, "seek": 8608, "start": 98.12, "end": 102.75999999999999, "text": " This is an online tool for visualizing neural networks, and I'll link it in the description", "tokens": [50966, 639, 307, 364, 2950, 2290, 337, 5056, 3319, 18161, 9590, 11, 293, 286, 603, 2113, 309, 294, 264, 3855, 51198], "temperature": 0.0, "avg_logprob": -0.12455427750297214, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.429709792137146}, {"id": 23, "seek": 8608, "start": 102.75999999999999, "end": 103.75999999999999, "text": " below.", "tokens": [51198, 2507, 13, 51248], "temperature": 0.0, "avg_logprob": -0.12455427750297214, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.429709792137146}, {"id": 24, "seek": 8608, "start": 103.75999999999999, "end": 108.48, "text": " This particular network takes two inputs, x1 and x2, and produces one output.", "tokens": [51248, 639, 1729, 3209, 2516, 732, 15743, 11, 2031, 16, 293, 2031, 17, 11, 293, 14725, 472, 5598, 13, 51484], "temperature": 0.0, "avg_logprob": -0.12455427750297214, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.429709792137146}, {"id": 25, "seek": 8608, "start": 108.48, "end": 112.4, "text": " Technically, this function would create a three-dimensional surface, but it's easier", "tokens": [51484, 42494, 11, 341, 2445, 576, 1884, 257, 1045, 12, 18759, 3753, 11, 457, 309, 311, 3571, 51680], "temperature": 0.0, "avg_logprob": -0.12455427750297214, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.429709792137146}, {"id": 26, "seek": 8608, "start": 112.4, "end": 114.52, "text": " to visualize in two dimensions.", "tokens": [51680, 281, 23273, 294, 732, 12819, 13, 51786], "temperature": 0.0, "avg_logprob": -0.12455427750297214, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.429709792137146}, {"id": 27, "seek": 11452, "start": 114.52, "end": 119.72, "text": " This image is rendered by passing the xy coordinate of each pixel into the network, which then", "tokens": [50364, 639, 3256, 307, 28748, 538, 8437, 264, 2031, 88, 15670, 295, 1184, 19261, 666, 264, 3209, 11, 597, 550, 50624], "temperature": 0.0, "avg_logprob": -0.09859877824783325, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.007576494012027979}, {"id": 28, "seek": 11452, "start": 119.72, "end": 124.47999999999999, "text": " produces a value between negative one and one that is used as the pixel value.", "tokens": [50624, 14725, 257, 2158, 1296, 3671, 472, 293, 472, 300, 307, 1143, 382, 264, 19261, 2158, 13, 50862], "temperature": 0.0, "avg_logprob": -0.09859877824783325, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.007576494012027979}, {"id": 29, "seek": 11452, "start": 124.47999999999999, "end": 128.28, "text": " These points are our data set, and are used to train the network.", "tokens": [50862, 1981, 2793, 366, 527, 1412, 992, 11, 293, 366, 1143, 281, 3847, 264, 3209, 13, 51052], "temperature": 0.0, "avg_logprob": -0.09859877824783325, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.007576494012027979}, {"id": 30, "seek": 11452, "start": 128.28, "end": 132.64, "text": " When we begin training, it quickly constructs a shape that accurately distinguishes between", "tokens": [51052, 1133, 321, 1841, 3097, 11, 309, 2661, 7690, 82, 257, 3909, 300, 20095, 11365, 16423, 1296, 51270], "temperature": 0.0, "avg_logprob": -0.09859877824783325, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.007576494012027979}, {"id": 31, "seek": 11452, "start": 132.64, "end": 136.92, "text": " blue and orange points, building a decision boundary that separates them.", "tokens": [51270, 3344, 293, 7671, 2793, 11, 2390, 257, 3537, 12866, 300, 34149, 552, 13, 51484], "temperature": 0.0, "avg_logprob": -0.09859877824783325, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.007576494012027979}, {"id": 32, "seek": 11452, "start": 136.92, "end": 142.76, "text": " It is approximating the function that describes the data, its learning, and is capable of learning", "tokens": [51484, 467, 307, 8542, 990, 264, 2445, 300, 15626, 264, 1412, 11, 1080, 2539, 11, 293, 307, 8189, 295, 2539, 51776], "temperature": 0.0, "avg_logprob": -0.09859877824783325, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.007576494012027979}, {"id": 33, "seek": 14276, "start": 142.76, "end": 146.04, "text": " the different data sets that we throw at it.", "tokens": [50364, 264, 819, 1412, 6352, 300, 321, 3507, 412, 309, 13, 50528], "temperature": 0.0, "avg_logprob": -0.11762665920570249, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0695161521434784}, {"id": 34, "seek": 14276, "start": 146.04, "end": 147.64, "text": " So what is this middle section then?", "tokens": [50528, 407, 437, 307, 341, 2808, 3541, 550, 30, 50608], "temperature": 0.0, "avg_logprob": -0.11762665920570249, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0695161521434784}, {"id": 35, "seek": 14276, "start": 147.64, "end": 151.44, "text": " Well, as the name implies, this is the network of neurons.", "tokens": [50608, 1042, 11, 382, 264, 1315, 18779, 11, 341, 307, 264, 3209, 295, 22027, 13, 50798], "temperature": 0.0, "avg_logprob": -0.11762665920570249, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0695161521434784}, {"id": 36, "seek": 14276, "start": 151.44, "end": 155.51999999999998, "text": " Each one of these nodes is a neuron, which takes in all the inputs from the previous", "tokens": [50798, 6947, 472, 295, 613, 13891, 307, 257, 34090, 11, 597, 2516, 294, 439, 264, 15743, 490, 264, 3894, 51002], "temperature": 0.0, "avg_logprob": -0.11762665920570249, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0695161521434784}, {"id": 37, "seek": 14276, "start": 155.51999999999998, "end": 160.76, "text": " layer of neurons and produces one output, which is then fed to the next layer.", "tokens": [51002, 4583, 295, 22027, 293, 14725, 472, 5598, 11, 597, 307, 550, 4636, 281, 264, 958, 4583, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11762665920570249, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0695161521434784}, {"id": 38, "seek": 14276, "start": 160.76, "end": 163.79999999999998, "text": " Inputs and outputs sounds like we're dealing with a function.", "tokens": [51264, 682, 2582, 82, 293, 23930, 3263, 411, 321, 434, 6260, 365, 257, 2445, 13, 51416], "temperature": 0.0, "avg_logprob": -0.11762665920570249, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0695161521434784}, {"id": 39, "seek": 14276, "start": 163.79999999999998, "end": 169.2, "text": " Indeed, a neuron itself is just a function, one that can take any number of inputs and", "tokens": [51416, 15061, 11, 257, 34090, 2564, 307, 445, 257, 2445, 11, 472, 300, 393, 747, 604, 1230, 295, 15743, 293, 51686], "temperature": 0.0, "avg_logprob": -0.11762665920570249, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0695161521434784}, {"id": 40, "seek": 14276, "start": 169.2, "end": 170.92, "text": " has one output.", "tokens": [51686, 575, 472, 5598, 13, 51772], "temperature": 0.0, "avg_logprob": -0.11762665920570249, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0695161521434784}, {"id": 41, "seek": 17092, "start": 170.92, "end": 175.79999999999998, "text": " Each input is multiplied by a weight, and all are added together along with a bias.", "tokens": [50364, 6947, 4846, 307, 17207, 538, 257, 3364, 11, 293, 439, 366, 3869, 1214, 2051, 365, 257, 12577, 13, 50608], "temperature": 0.0, "avg_logprob": -0.11100962309710748, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0034831815864890814}, {"id": 42, "seek": 17092, "start": 175.79999999999998, "end": 180.48, "text": " The weights and bias make up the parameters of this neuron, values that can change as", "tokens": [50608, 440, 17443, 293, 12577, 652, 493, 264, 9834, 295, 341, 34090, 11, 4190, 300, 393, 1319, 382, 50842], "temperature": 0.0, "avg_logprob": -0.11100962309710748, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0034831815864890814}, {"id": 43, "seek": 17092, "start": 180.48, "end": 181.64, "text": " the network learns.", "tokens": [50842, 264, 3209, 27152, 13, 50900], "temperature": 0.0, "avg_logprob": -0.11100962309710748, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0034831815864890814}, {"id": 44, "seek": 17092, "start": 181.64, "end": 186.64, "text": " To keep it easy to visualize, we'll simplify this down to a two-dimensional function, with", "tokens": [50900, 1407, 1066, 309, 1858, 281, 23273, 11, 321, 603, 20460, 341, 760, 281, 257, 732, 12, 18759, 2445, 11, 365, 51150], "temperature": 0.0, "avg_logprob": -0.11100962309710748, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0034831815864890814}, {"id": 45, "seek": 17092, "start": 186.64, "end": 189.48, "text": " only one input and one output.", "tokens": [51150, 787, 472, 4846, 293, 472, 5598, 13, 51292], "temperature": 0.0, "avg_logprob": -0.11100962309710748, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0034831815864890814}, {"id": 46, "seek": 17092, "start": 189.48, "end": 193.92, "text": " Now neurons are our building blocks of the larger network, building blocks that can be", "tokens": [51292, 823, 22027, 366, 527, 2390, 8474, 295, 264, 4833, 3209, 11, 2390, 8474, 300, 393, 312, 51514], "temperature": 0.0, "avg_logprob": -0.11100962309710748, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0034831815864890814}, {"id": 47, "seek": 17092, "start": 193.92, "end": 199.04, "text": " stretched and squeezed and shifted around, and ultimately work with other blocks to construct", "tokens": [51514, 23563, 293, 39470, 293, 18892, 926, 11, 293, 6284, 589, 365, 661, 8474, 281, 7690, 51770], "temperature": 0.0, "avg_logprob": -0.11100962309710748, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0034831815864890814}, {"id": 48, "seek": 19904, "start": 199.04, "end": 201.44, "text": " something larger than themselves.", "tokens": [50364, 746, 4833, 813, 2969, 13, 50484], "temperature": 0.0, "avg_logprob": -0.1259659210840861, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.006692151073366404}, {"id": 49, "seek": 19904, "start": 201.44, "end": 204.72, "text": " The neuron, as we've defined it here, works like a building block.", "tokens": [50484, 440, 34090, 11, 382, 321, 600, 7642, 309, 510, 11, 1985, 411, 257, 2390, 3461, 13, 50648], "temperature": 0.0, "avg_logprob": -0.1259659210840861, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.006692151073366404}, {"id": 50, "seek": 19904, "start": 204.72, "end": 209.79999999999998, "text": " It is actually an extremely simple linear function, one which forms the flat line, or", "tokens": [50648, 467, 307, 767, 364, 4664, 2199, 8213, 2445, 11, 472, 597, 6422, 264, 4962, 1622, 11, 420, 50902], "temperature": 0.0, "avg_logprob": -0.1259659210840861, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.006692151073366404}, {"id": 51, "seek": 19904, "start": 209.79999999999998, "end": 212.04, "text": " plain when there's more than one input.", "tokens": [50902, 11121, 562, 456, 311, 544, 813, 472, 4846, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1259659210840861, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.006692151073366404}, {"id": 52, "seek": 19904, "start": 212.04, "end": 216.48, "text": " With the two parameters, the weight and bias, we can stretch and squeeze and move our function", "tokens": [51014, 2022, 264, 732, 9834, 11, 264, 3364, 293, 12577, 11, 321, 393, 5985, 293, 13578, 293, 1286, 527, 2445, 51236], "temperature": 0.0, "avg_logprob": -0.1259659210840861, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.006692151073366404}, {"id": 53, "seek": 19904, "start": 216.48, "end": 218.84, "text": " up and down and left and right.", "tokens": [51236, 493, 293, 760, 293, 1411, 293, 558, 13, 51354], "temperature": 0.0, "avg_logprob": -0.1259659210840861, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.006692151073366404}, {"id": 54, "seek": 19904, "start": 218.84, "end": 224.44, "text": " As such, we should be able to combine it with other neurons to form a more complicated function,", "tokens": [51354, 1018, 1270, 11, 321, 820, 312, 1075, 281, 10432, 309, 365, 661, 22027, 281, 1254, 257, 544, 6179, 2445, 11, 51634], "temperature": 0.0, "avg_logprob": -0.1259659210840861, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.006692151073366404}, {"id": 55, "seek": 19904, "start": 224.44, "end": 227.62, "text": " one built from lots of linear functions.", "tokens": [51634, 472, 3094, 490, 3195, 295, 8213, 6828, 13, 51793], "temperature": 0.0, "avg_logprob": -0.1259659210840861, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.006692151073366404}, {"id": 56, "seek": 22762, "start": 227.62, "end": 231.34, "text": " So let's start with a target function, one we want to approximate.", "tokens": [50364, 407, 718, 311, 722, 365, 257, 3779, 2445, 11, 472, 321, 528, 281, 30874, 13, 50550], "temperature": 0.0, "avg_logprob": -0.1383379287078601, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0001233924413099885}, {"id": 57, "seek": 22762, "start": 231.34, "end": 235.62, "text": " I've hard-coded a bunch of neurons whose parameters were found manually, and if we", "tokens": [50550, 286, 600, 1152, 12, 66, 12340, 257, 3840, 295, 22027, 6104, 9834, 645, 1352, 16945, 11, 293, 498, 321, 50764], "temperature": 0.0, "avg_logprob": -0.1383379287078601, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0001233924413099885}, {"id": 58, "seek": 22762, "start": 235.62, "end": 240.34, "text": " weight each one and add them up, as would happen in the final neuron of the network,", "tokens": [50764, 3364, 1184, 472, 293, 909, 552, 493, 11, 382, 576, 1051, 294, 264, 2572, 34090, 295, 264, 3209, 11, 51000], "temperature": 0.0, "avg_logprob": -0.1383379287078601, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0001233924413099885}, {"id": 59, "seek": 22762, "start": 240.34, "end": 243.54, "text": " we should get a function that looks like the target function.", "tokens": [51000, 321, 820, 483, 257, 2445, 300, 1542, 411, 264, 3779, 2445, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1383379287078601, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0001233924413099885}, {"id": 60, "seek": 22762, "start": 243.54, "end": 246.18, "text": " Well, that didn't work at all.", "tokens": [51160, 1042, 11, 300, 994, 380, 589, 412, 439, 13, 51292], "temperature": 0.0, "avg_logprob": -0.1383379287078601, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0001233924413099885}, {"id": 61, "seek": 22762, "start": 246.18, "end": 247.18, "text": " What happened?", "tokens": [51292, 708, 2011, 30, 51342], "temperature": 0.0, "avg_logprob": -0.1383379287078601, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0001233924413099885}, {"id": 62, "seek": 22762, "start": 247.18, "end": 252.38, "text": " Well, if we simplify our equation, distributing weights and combining like terms, we end up", "tokens": [51342, 1042, 11, 498, 321, 20460, 527, 5367, 11, 41406, 17443, 293, 21928, 411, 2115, 11, 321, 917, 493, 51602], "temperature": 0.0, "avg_logprob": -0.1383379287078601, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0001233924413099885}, {"id": 63, "seek": 22762, "start": 252.38, "end": 254.9, "text": " with a single linear function.", "tokens": [51602, 365, 257, 2167, 8213, 2445, 13, 51728], "temperature": 0.0, "avg_logprob": -0.1383379287078601, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0001233924413099885}, {"id": 64, "seek": 25490, "start": 254.9, "end": 259.28000000000003, "text": " Turns out, linear functions can only combine to make one linear function.", "tokens": [50364, 29524, 484, 11, 8213, 6828, 393, 787, 10432, 281, 652, 472, 8213, 2445, 13, 50583], "temperature": 0.0, "avg_logprob": -0.1026535938526022, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.028431031852960587}, {"id": 65, "seek": 25490, "start": 259.28000000000003, "end": 264.44, "text": " This is a big problem because we need to make something more complicated than just a line.", "tokens": [50583, 639, 307, 257, 955, 1154, 570, 321, 643, 281, 652, 746, 544, 6179, 813, 445, 257, 1622, 13, 50841], "temperature": 0.0, "avg_logprob": -0.1026535938526022, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.028431031852960587}, {"id": 66, "seek": 25490, "start": 264.44, "end": 268.96, "text": " We need something that is not linear, a non-linearity.", "tokens": [50841, 492, 643, 746, 300, 307, 406, 8213, 11, 257, 2107, 12, 1889, 17409, 13, 51067], "temperature": 0.0, "avg_logprob": -0.1026535938526022, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.028431031852960587}, {"id": 67, "seek": 25490, "start": 268.96, "end": 273.06, "text": " In our case, we will be using a ReLU, a rectified linear unit.", "tokens": [51067, 682, 527, 1389, 11, 321, 486, 312, 1228, 257, 1300, 43, 52, 11, 257, 11048, 2587, 8213, 4985, 13, 51272], "temperature": 0.0, "avg_logprob": -0.1026535938526022, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.028431031852960587}, {"id": 68, "seek": 25490, "start": 273.06, "end": 279.22, "text": " We use it as our activation function, meaning we simply apply it to our previous naive neuron.", "tokens": [51272, 492, 764, 309, 382, 527, 24433, 2445, 11, 3620, 321, 2935, 3079, 309, 281, 527, 3894, 29052, 34090, 13, 51580], "temperature": 0.0, "avg_logprob": -0.1026535938526022, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.028431031852960587}, {"id": 69, "seek": 25490, "start": 279.22, "end": 283.42, "text": " This is about as close as you can get to a linear function without actually being one,", "tokens": [51580, 639, 307, 466, 382, 1998, 382, 291, 393, 483, 281, 257, 8213, 2445, 1553, 767, 885, 472, 11, 51790], "temperature": 0.0, "avg_logprob": -0.1026535938526022, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.028431031852960587}, {"id": 70, "seek": 28342, "start": 283.42, "end": 286.06, "text": " and we can tune it with the same parameters as before.", "tokens": [50364, 293, 321, 393, 10864, 309, 365, 264, 912, 9834, 382, 949, 13, 50496], "temperature": 0.0, "avg_logprob": -0.14587171375751495, "compression_ratio": 1.7335640138408304, "no_speech_prob": 0.002322989981621504}, {"id": 71, "seek": 28342, "start": 286.06, "end": 290.7, "text": " However, you may notice that we can't actually lift the function off of the x-axis, which", "tokens": [50496, 2908, 11, 291, 815, 3449, 300, 321, 393, 380, 767, 5533, 264, 2445, 766, 295, 264, 2031, 12, 24633, 11, 597, 50728], "temperature": 0.0, "avg_logprob": -0.14587171375751495, "compression_ratio": 1.7335640138408304, "no_speech_prob": 0.002322989981621504}, {"id": 72, "seek": 28342, "start": 290.7, "end": 292.58000000000004, "text": " seems like a pretty big limitation.", "tokens": [50728, 2544, 411, 257, 1238, 955, 27432, 13, 50822], "temperature": 0.0, "avg_logprob": -0.14587171375751495, "compression_ratio": 1.7335640138408304, "no_speech_prob": 0.002322989981621504}, {"id": 73, "seek": 28342, "start": 292.58000000000004, "end": 296.5, "text": " Well, let's give it a shot anyway, and see if it performs any better than our previous", "tokens": [50822, 1042, 11, 718, 311, 976, 309, 257, 3347, 4033, 11, 293, 536, 498, 309, 26213, 604, 1101, 813, 527, 3894, 51018], "temperature": 0.0, "avg_logprob": -0.14587171375751495, "compression_ratio": 1.7335640138408304, "no_speech_prob": 0.002322989981621504}, {"id": 74, "seek": 28342, "start": 296.5, "end": 297.5, "text": " attempt.", "tokens": [51018, 5217, 13, 51068], "temperature": 0.0, "avg_logprob": -0.14587171375751495, "compression_ratio": 1.7335640138408304, "no_speech_prob": 0.002322989981621504}, {"id": 75, "seek": 28342, "start": 297.5, "end": 301.02000000000004, "text": " We're still trying to approximate the same function, and we're using the same weights", "tokens": [51068, 492, 434, 920, 1382, 281, 30874, 264, 912, 2445, 11, 293, 321, 434, 1228, 264, 912, 17443, 51244], "temperature": 0.0, "avg_logprob": -0.14587171375751495, "compression_ratio": 1.7335640138408304, "no_speech_prob": 0.002322989981621504}, {"id": 76, "seek": 28342, "start": 301.02000000000004, "end": 306.42, "text": " and biases as before, but this time we're using a ReLU as our activation function.", "tokens": [51244, 293, 32152, 382, 949, 11, 457, 341, 565, 321, 434, 1228, 257, 1300, 43, 52, 382, 527, 24433, 2445, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14587171375751495, "compression_ratio": 1.7335640138408304, "no_speech_prob": 0.002322989981621504}, {"id": 77, "seek": 28342, "start": 306.42, "end": 309.66, "text": " And just like that, the approximation looks way better.", "tokens": [51514, 400, 445, 411, 300, 11, 264, 28023, 1542, 636, 1101, 13, 51676], "temperature": 0.0, "avg_logprob": -0.14587171375751495, "compression_ratio": 1.7335640138408304, "no_speech_prob": 0.002322989981621504}, {"id": 78, "seek": 30966, "start": 309.66, "end": 314.18, "text": " Unlike before, our function cannot simplify down to a flat linear function.", "tokens": [50364, 17657, 949, 11, 527, 2445, 2644, 20460, 760, 281, 257, 4962, 8213, 2445, 13, 50590], "temperature": 0.0, "avg_logprob": -0.12553698899316007, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.023685721680521965}, {"id": 79, "seek": 30966, "start": 314.18, "end": 318.38000000000005, "text": " If we add the neurons one by one, we can see the simple ReLU functions building on one", "tokens": [50590, 759, 321, 909, 264, 22027, 472, 538, 472, 11, 321, 393, 536, 264, 2199, 1300, 43, 52, 6828, 2390, 322, 472, 50800], "temperature": 0.0, "avg_logprob": -0.12553698899316007, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.023685721680521965}, {"id": 80, "seek": 30966, "start": 318.38000000000005, "end": 323.1, "text": " another, and the inability for one neuron to lift itself off the x-axis doesn't seem", "tokens": [50800, 1071, 11, 293, 264, 33162, 337, 472, 34090, 281, 5533, 2564, 766, 264, 2031, 12, 24633, 1177, 380, 1643, 51036], "temperature": 0.0, "avg_logprob": -0.12553698899316007, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.023685721680521965}, {"id": 81, "seek": 30966, "start": 323.1, "end": 324.58000000000004, "text": " to be a problem.", "tokens": [51036, 281, 312, 257, 1154, 13, 51110], "temperature": 0.0, "avg_logprob": -0.12553698899316007, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.023685721680521965}, {"id": 82, "seek": 30966, "start": 324.58000000000004, "end": 329.14000000000004, "text": " Many neurons working together overcome the limitations of individual neurons.", "tokens": [51110, 5126, 22027, 1364, 1214, 10473, 264, 15705, 295, 2609, 22027, 13, 51338], "temperature": 0.0, "avg_logprob": -0.12553698899316007, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.023685721680521965}, {"id": 83, "seek": 30966, "start": 329.14000000000004, "end": 333.94000000000005, "text": " Now, I manually found these weights and biases, but how would you find them automatically?", "tokens": [51338, 823, 11, 286, 16945, 1352, 613, 17443, 293, 32152, 11, 457, 577, 576, 291, 915, 552, 6772, 30, 51578], "temperature": 0.0, "avg_logprob": -0.12553698899316007, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.023685721680521965}, {"id": 84, "seek": 30966, "start": 333.94000000000005, "end": 338.58000000000004, "text": " The most common algorithm for this is called back propagation, and is in fact what we're", "tokens": [51578, 440, 881, 2689, 9284, 337, 341, 307, 1219, 646, 38377, 11, 293, 307, 294, 1186, 437, 321, 434, 51810], "temperature": 0.0, "avg_logprob": -0.12553698899316007, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.023685721680521965}, {"id": 85, "seek": 33858, "start": 338.58, "end": 340.7, "text": " researching when we run this program.", "tokens": [50364, 24176, 562, 321, 1190, 341, 1461, 13, 50470], "temperature": 0.0, "avg_logprob": -0.14219859174189678, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.3273351192474365}, {"id": 86, "seek": 33858, "start": 340.7, "end": 346.5, "text": " It essentially tweaks and tunes the parameters of the network bit by bit to improve the approximation,", "tokens": [50470, 467, 4476, 46664, 293, 38498, 264, 9834, 295, 264, 3209, 857, 538, 857, 281, 3470, 264, 28023, 11, 50760], "temperature": 0.0, "avg_logprob": -0.14219859174189678, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.3273351192474365}, {"id": 87, "seek": 33858, "start": 346.5, "end": 350.02, "text": " and the intricacies of this algorithm are really beyond the scope of this video.", "tokens": [50760, 293, 264, 30242, 20330, 295, 341, 9284, 366, 534, 4399, 264, 11923, 295, 341, 960, 13, 50936], "temperature": 0.0, "avg_logprob": -0.14219859174189678, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.3273351192474365}, {"id": 88, "seek": 33858, "start": 350.02, "end": 353.3, "text": " I'll link some better explanations in the description.", "tokens": [50936, 286, 603, 2113, 512, 1101, 28708, 294, 264, 3855, 13, 51100], "temperature": 0.0, "avg_logprob": -0.14219859174189678, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.3273351192474365}, {"id": 89, "seek": 33858, "start": 353.3, "end": 357.38, "text": " Now we can see how this shape is formed, and why it looks like it's made up of sort of", "tokens": [51100, 823, 321, 393, 536, 577, 341, 3909, 307, 8693, 11, 293, 983, 309, 1542, 411, 309, 311, 1027, 493, 295, 1333, 295, 51304], "temperature": 0.0, "avg_logprob": -0.14219859174189678, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.3273351192474365}, {"id": 90, "seek": 33858, "start": 357.38, "end": 358.78, "text": " sharp linear edges.", "tokens": [51304, 8199, 8213, 8819, 13, 51374], "temperature": 0.0, "avg_logprob": -0.14219859174189678, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.3273351192474365}, {"id": 91, "seek": 33858, "start": 358.78, "end": 362.02, "text": " It's the nature of the activation function we're using.", "tokens": [51374, 467, 311, 264, 3687, 295, 264, 24433, 2445, 321, 434, 1228, 13, 51536], "temperature": 0.0, "avg_logprob": -0.14219859174189678, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.3273351192474365}, {"id": 92, "seek": 33858, "start": 362.02, "end": 367.38, "text": " We can also see why, if we use no activation function at all, the network utterly fails", "tokens": [51536, 492, 393, 611, 536, 983, 11, 498, 321, 764, 572, 24433, 2445, 412, 439, 11, 264, 3209, 30251, 18199, 51804], "temperature": 0.0, "avg_logprob": -0.14219859174189678, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.3273351192474365}, {"id": 93, "seek": 33858, "start": 367.38, "end": 368.38, "text": " to learn.", "tokens": [51804, 281, 1466, 13, 51854], "temperature": 0.0, "avg_logprob": -0.14219859174189678, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.3273351192474365}, {"id": 94, "seek": 36838, "start": 368.58, "end": 371.38, "text": " We need those non-linearities.", "tokens": [50374, 492, 643, 729, 2107, 12, 28263, 1088, 13, 50514], "temperature": 0.0, "avg_logprob": -0.19758863530607304, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005219524726271629}, {"id": 95, "seek": 36838, "start": 371.38, "end": 374.98, "text": " So what if we try learning a more complicated data set, like this spiral?", "tokens": [50514, 407, 437, 498, 321, 853, 2539, 257, 544, 6179, 1412, 992, 11, 411, 341, 25165, 30, 50694], "temperature": 0.0, "avg_logprob": -0.19758863530607304, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005219524726271629}, {"id": 96, "seek": 36838, "start": 374.98, "end": 377.7, "text": " Let's give it a go.", "tokens": [50694, 961, 311, 976, 309, 257, 352, 13, 50830], "temperature": 0.0, "avg_logprob": -0.19758863530607304, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005219524726271629}, {"id": 97, "seek": 36838, "start": 377.7, "end": 380.54, "text": " Seems to be struggling a little bit to capture the pattern.", "tokens": [50830, 22524, 281, 312, 9314, 257, 707, 857, 281, 7983, 264, 5102, 13, 50972], "temperature": 0.0, "avg_logprob": -0.19758863530607304, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005219524726271629}, {"id": 98, "seek": 36838, "start": 380.54, "end": 381.54, "text": " No problem.", "tokens": [50972, 883, 1154, 13, 51022], "temperature": 0.0, "avg_logprob": -0.19758863530607304, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005219524726271629}, {"id": 99, "seek": 36838, "start": 381.54, "end": 385.5, "text": " If we need a more complicated function, we can add more building blocks, more neurons,", "tokens": [51022, 759, 321, 643, 257, 544, 6179, 2445, 11, 321, 393, 909, 544, 2390, 8474, 11, 544, 22027, 11, 51220], "temperature": 0.0, "avg_logprob": -0.19758863530607304, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005219524726271629}, {"id": 100, "seek": 36838, "start": 385.5, "end": 387.26, "text": " and layers of neurons.", "tokens": [51220, 293, 7914, 295, 22027, 13, 51308], "temperature": 0.0, "avg_logprob": -0.19758863530607304, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005219524726271629}, {"id": 101, "seek": 36838, "start": 387.26, "end": 391.34, "text": " And the network should be able to piece together a better approximation, something that really", "tokens": [51308, 400, 264, 3209, 820, 312, 1075, 281, 2522, 1214, 257, 1101, 28023, 11, 746, 300, 534, 51512], "temperature": 0.0, "avg_logprob": -0.19758863530607304, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005219524726271629}, {"id": 102, "seek": 36838, "start": 391.34, "end": 393.5, "text": " captures the spiral.", "tokens": [51512, 27986, 264, 25165, 13, 51620], "temperature": 0.0, "avg_logprob": -0.19758863530607304, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005219524726271629}, {"id": 103, "seek": 36838, "start": 393.5, "end": 397.34, "text": " It seems to be working.", "tokens": [51620, 467, 2544, 281, 312, 1364, 13, 51812], "temperature": 0.0, "avg_logprob": -0.19758863530607304, "compression_ratio": 1.651851851851852, "no_speech_prob": 0.005219524726271629}, {"id": 104, "seek": 39734, "start": 397.34, "end": 401.02, "text": " In fact, no matter what the data set is, we can learn it.", "tokens": [50364, 682, 1186, 11, 572, 1871, 437, 264, 1412, 992, 307, 11, 321, 393, 1466, 309, 13, 50548], "temperature": 0.0, "avg_logprob": -0.060328025262332656, "compression_ratio": 1.77734375, "no_speech_prob": 0.0034829676151275635}, {"id": 105, "seek": 39734, "start": 401.02, "end": 406.7, "text": " That is because neural networks can be rigorously proven to be universal function approximators.", "tokens": [50548, 663, 307, 570, 18161, 9590, 393, 312, 42191, 5098, 12785, 281, 312, 11455, 2445, 8542, 3391, 13, 50832], "temperature": 0.0, "avg_logprob": -0.060328025262332656, "compression_ratio": 1.77734375, "no_speech_prob": 0.0034829676151275635}, {"id": 106, "seek": 39734, "start": 406.7, "end": 412.21999999999997, "text": " They can approximate any function to any degree of precision you could ever want.", "tokens": [50832, 814, 393, 30874, 604, 2445, 281, 604, 4314, 295, 18356, 291, 727, 1562, 528, 13, 51108], "temperature": 0.0, "avg_logprob": -0.060328025262332656, "compression_ratio": 1.77734375, "no_speech_prob": 0.0034829676151275635}, {"id": 107, "seek": 39734, "start": 412.21999999999997, "end": 414.9, "text": " You can always add more neurons.", "tokens": [51108, 509, 393, 1009, 909, 544, 22027, 13, 51242], "temperature": 0.0, "avg_logprob": -0.060328025262332656, "compression_ratio": 1.77734375, "no_speech_prob": 0.0034829676151275635}, {"id": 108, "seek": 39734, "start": 414.9, "end": 419.34, "text": " This is essentially the whole point of deep learning, because it means that neural networks", "tokens": [51242, 639, 307, 4476, 264, 1379, 935, 295, 2452, 2539, 11, 570, 309, 1355, 300, 18161, 9590, 51464], "temperature": 0.0, "avg_logprob": -0.060328025262332656, "compression_ratio": 1.77734375, "no_speech_prob": 0.0034829676151275635}, {"id": 109, "seek": 39734, "start": 419.34, "end": 425.73999999999995, "text": " can approximate anything that can be expressed as a function, a system of inputs and outputs.", "tokens": [51464, 393, 30874, 1340, 300, 393, 312, 12675, 382, 257, 2445, 11, 257, 1185, 295, 15743, 293, 23930, 13, 51784], "temperature": 0.0, "avg_logprob": -0.060328025262332656, "compression_ratio": 1.77734375, "no_speech_prob": 0.0034829676151275635}, {"id": 110, "seek": 42574, "start": 425.74, "end": 429.58, "text": " This is an extremely general way of thinking about the world.", "tokens": [50364, 639, 307, 364, 4664, 2674, 636, 295, 1953, 466, 264, 1002, 13, 50556], "temperature": 0.0, "avg_logprob": -0.09141003458123458, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.06751527637243271}, {"id": 111, "seek": 42574, "start": 429.58, "end": 434.26, "text": " The Mandelbrot set, for instance, can be written as a function and learned all the same.", "tokens": [50556, 440, 15458, 338, 1443, 310, 992, 11, 337, 5197, 11, 393, 312, 3720, 382, 257, 2445, 293, 3264, 439, 264, 912, 13, 50790], "temperature": 0.0, "avg_logprob": -0.09141003458123458, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.06751527637243271}, {"id": 112, "seek": 42574, "start": 434.26, "end": 439.02, "text": " This is just a scaled-up version of the experiment we were just looking at, but with an infinitely", "tokens": [50790, 639, 307, 445, 257, 36039, 12, 1010, 3037, 295, 264, 5120, 321, 645, 445, 1237, 412, 11, 457, 365, 364, 36227, 51028], "temperature": 0.0, "avg_logprob": -0.09141003458123458, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.06751527637243271}, {"id": 113, "seek": 42574, "start": 439.02, "end": 440.5, "text": " complex data set.", "tokens": [51028, 3997, 1412, 992, 13, 51102], "temperature": 0.0, "avg_logprob": -0.09141003458123458, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.06751527637243271}, {"id": 114, "seek": 42574, "start": 440.5, "end": 444.26, "text": " We don't even really need to know what the Mandelbrot set is.", "tokens": [51102, 492, 500, 380, 754, 534, 643, 281, 458, 437, 264, 15458, 338, 1443, 310, 992, 307, 13, 51290], "temperature": 0.0, "avg_logprob": -0.09141003458123458, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.06751527637243271}, {"id": 115, "seek": 42574, "start": 444.26, "end": 447.82, "text": " The network learns it for us, and that's kind of the point.", "tokens": [51290, 440, 3209, 27152, 309, 337, 505, 11, 293, 300, 311, 733, 295, 264, 935, 13, 51468], "temperature": 0.0, "avg_logprob": -0.09141003458123458, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.06751527637243271}, {"id": 116, "seek": 42574, "start": 447.82, "end": 453.06, "text": " If you can express any intelligent behavior, any process, any task as a function, then", "tokens": [51468, 759, 291, 393, 5109, 604, 13232, 5223, 11, 604, 1399, 11, 604, 5633, 382, 257, 2445, 11, 550, 51730], "temperature": 0.0, "avg_logprob": -0.09141003458123458, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.06751527637243271}, {"id": 117, "seek": 42574, "start": 453.06, "end": 454.94, "text": " a network can learn it.", "tokens": [51730, 257, 3209, 393, 1466, 309, 13, 51824], "temperature": 0.0, "avg_logprob": -0.09141003458123458, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.06751527637243271}, {"id": 118, "seek": 45494, "start": 454.94, "end": 459.1, "text": " For instance, your input could be an image and your output a label as to whether it's", "tokens": [50364, 1171, 5197, 11, 428, 4846, 727, 312, 364, 3256, 293, 428, 5598, 257, 7645, 382, 281, 1968, 309, 311, 50572], "temperature": 0.0, "avg_logprob": -0.11178218957149622, "compression_ratio": 1.7927272727272727, "no_speech_prob": 0.008575715124607086}, {"id": 119, "seek": 45494, "start": 459.1, "end": 464.06, "text": " a cat or a dog, or your input could be text in English and your output a translation to", "tokens": [50572, 257, 3857, 420, 257, 3000, 11, 420, 428, 4846, 727, 312, 2487, 294, 3669, 293, 428, 5598, 257, 12853, 281, 50820], "temperature": 0.0, "avg_logprob": -0.11178218957149622, "compression_ratio": 1.7927272727272727, "no_speech_prob": 0.008575715124607086}, {"id": 120, "seek": 45494, "start": 464.06, "end": 465.06, "text": " Spanish.", "tokens": [50820, 8058, 13, 50870], "temperature": 0.0, "avg_logprob": -0.11178218957149622, "compression_ratio": 1.7927272727272727, "no_speech_prob": 0.008575715124607086}, {"id": 121, "seek": 45494, "start": 465.06, "end": 469.46, "text": " You just need to be able to encode your inputs and outputs as numbers, but computers do this", "tokens": [50870, 509, 445, 643, 281, 312, 1075, 281, 2058, 1429, 428, 15743, 293, 23930, 382, 3547, 11, 457, 10807, 360, 341, 51090], "temperature": 0.0, "avg_logprob": -0.11178218957149622, "compression_ratio": 1.7927272727272727, "no_speech_prob": 0.008575715124607086}, {"id": 122, "seek": 45494, "start": 469.46, "end": 470.46, "text": " all the time.", "tokens": [51090, 439, 264, 565, 13, 51140], "temperature": 0.0, "avg_logprob": -0.11178218957149622, "compression_ratio": 1.7927272727272727, "no_speech_prob": 0.008575715124607086}, {"id": 123, "seek": 45494, "start": 470.46, "end": 475.7, "text": " Images, video, text, audio, they can all be represented as numbers, and any processing", "tokens": [51140, 4331, 1660, 11, 960, 11, 2487, 11, 6278, 11, 436, 393, 439, 312, 10379, 382, 3547, 11, 293, 604, 9007, 51402], "temperature": 0.0, "avg_logprob": -0.11178218957149622, "compression_ratio": 1.7927272727272727, "no_speech_prob": 0.008575715124607086}, {"id": 124, "seek": 45494, "start": 475.7, "end": 479.74, "text": " you may want to do with this data, so long as you can write it as a function, can be", "tokens": [51402, 291, 815, 528, 281, 360, 365, 341, 1412, 11, 370, 938, 382, 291, 393, 2464, 309, 382, 257, 2445, 11, 393, 312, 51604], "temperature": 0.0, "avg_logprob": -0.11178218957149622, "compression_ratio": 1.7927272727272727, "no_speech_prob": 0.008575715124607086}, {"id": 125, "seek": 45494, "start": 479.74, "end": 482.3, "text": " emulated with a neural network.", "tokens": [51604, 846, 6987, 365, 257, 18161, 3209, 13, 51732], "temperature": 0.0, "avg_logprob": -0.11178218957149622, "compression_ratio": 1.7927272727272727, "no_speech_prob": 0.008575715124607086}, {"id": 126, "seek": 48230, "start": 482.3, "end": 483.90000000000003, "text": " It goes deeper than this though.", "tokens": [50364, 467, 1709, 7731, 813, 341, 1673, 13, 50444], "temperature": 0.0, "avg_logprob": -0.12304295991596423, "compression_ratio": 1.683794466403162, "no_speech_prob": 0.34821486473083496}, {"id": 127, "seek": 48230, "start": 483.90000000000003, "end": 488.86, "text": " Under a few more assumptions, neural networks are provably turing complete, meaning they", "tokens": [50444, 6974, 257, 1326, 544, 17695, 11, 18161, 9590, 366, 1439, 1188, 256, 1345, 3566, 11, 3620, 436, 50692], "temperature": 0.0, "avg_logprob": -0.12304295991596423, "compression_ratio": 1.683794466403162, "no_speech_prob": 0.34821486473083496}, {"id": 128, "seek": 48230, "start": 488.86, "end": 493.58, "text": " can solve all of the same kinds of problems that any computer can solve.", "tokens": [50692, 393, 5039, 439, 295, 264, 912, 3685, 295, 2740, 300, 604, 3820, 393, 5039, 13, 50928], "temperature": 0.0, "avg_logprob": -0.12304295991596423, "compression_ratio": 1.683794466403162, "no_speech_prob": 0.34821486473083496}, {"id": 129, "seek": 48230, "start": 493.58, "end": 498.3, "text": " An implication of this is that any algorithm written in any programming language can be", "tokens": [50928, 1107, 37814, 295, 341, 307, 300, 604, 9284, 3720, 294, 604, 9410, 2856, 393, 312, 51164], "temperature": 0.0, "avg_logprob": -0.12304295991596423, "compression_ratio": 1.683794466403162, "no_speech_prob": 0.34821486473083496}, {"id": 130, "seek": 48230, "start": 498.3, "end": 503.58000000000004, "text": " simulated on a neural network, but rather than being manually written by a human, it", "tokens": [51164, 41713, 322, 257, 18161, 3209, 11, 457, 2831, 813, 885, 16945, 3720, 538, 257, 1952, 11, 309, 51428], "temperature": 0.0, "avg_logprob": -0.12304295991596423, "compression_ratio": 1.683794466403162, "no_speech_prob": 0.34821486473083496}, {"id": 131, "seek": 48230, "start": 503.58000000000004, "end": 507.58000000000004, "text": " can be learned automatically with a function approximator.", "tokens": [51428, 393, 312, 3264, 6772, 365, 257, 2445, 8542, 1639, 13, 51628], "temperature": 0.0, "avg_logprob": -0.12304295991596423, "compression_ratio": 1.683794466403162, "no_speech_prob": 0.34821486473083496}, {"id": 132, "seek": 50758, "start": 507.65999999999997, "end": 515.54, "text": " Okay, that is not true.", "tokens": [50368, 1033, 11, 300, 307, 406, 2074, 13, 50762], "temperature": 0.0, "avg_logprob": -0.17676938517709798, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.2749064266681671}, {"id": 133, "seek": 50758, "start": 515.54, "end": 518.62, "text": " First off, you can't have an infinite number of neurons.", "tokens": [50762, 2386, 766, 11, 291, 393, 380, 362, 364, 13785, 1230, 295, 22027, 13, 50916], "temperature": 0.0, "avg_logprob": -0.17676938517709798, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.2749064266681671}, {"id": 134, "seek": 50758, "start": 518.62, "end": 524.42, "text": " There are practical limitations on network size and what can be modeled in the real world.", "tokens": [50916, 821, 366, 8496, 15705, 322, 3209, 2744, 293, 437, 393, 312, 37140, 294, 264, 957, 1002, 13, 51206], "temperature": 0.0, "avg_logprob": -0.17676938517709798, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.2749064266681671}, {"id": 135, "seek": 50758, "start": 524.42, "end": 529.14, "text": " I've also ignored the learning process in this video, and just assumed that you can", "tokens": [51206, 286, 600, 611, 19735, 264, 2539, 1399, 294, 341, 960, 11, 293, 445, 15895, 300, 291, 393, 51442], "temperature": 0.0, "avg_logprob": -0.17676938517709798, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.2749064266681671}, {"id": 136, "seek": 50758, "start": 529.14, "end": 532.18, "text": " find the optimal parameters magically.", "tokens": [51442, 915, 264, 16252, 9834, 39763, 13, 51594], "temperature": 0.0, "avg_logprob": -0.17676938517709798, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.2749064266681671}, {"id": 137, "seek": 50758, "start": 532.18, "end": 537.42, "text": " How you realistically do this introduces its own constraints on what can be learned.", "tokens": [51594, 1012, 291, 40734, 360, 341, 31472, 1080, 1065, 18491, 322, 437, 393, 312, 3264, 13, 51856], "temperature": 0.0, "avg_logprob": -0.17676938517709798, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.2749064266681671}, {"id": 138, "seek": 53742, "start": 537.42, "end": 542.26, "text": " Additionally, in order for neural networks to approximate a function, you need the data", "tokens": [50364, 19927, 11, 294, 1668, 337, 18161, 9590, 281, 30874, 257, 2445, 11, 291, 643, 264, 1412, 50606], "temperature": 0.0, "avg_logprob": -0.10133388098769301, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.005910330917686224}, {"id": 139, "seek": 53742, "start": 542.26, "end": 544.9399999999999, "text": " that actually describes that function.", "tokens": [50606, 300, 767, 15626, 300, 2445, 13, 50740], "temperature": 0.0, "avg_logprob": -0.10133388098769301, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.005910330917686224}, {"id": 140, "seek": 53742, "start": 544.9399999999999, "end": 549.18, "text": " If you don't have enough data, your approximation will be all wrong.", "tokens": [50740, 759, 291, 500, 380, 362, 1547, 1412, 11, 428, 28023, 486, 312, 439, 2085, 13, 50952], "temperature": 0.0, "avg_logprob": -0.10133388098769301, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.005910330917686224}, {"id": 141, "seek": 53742, "start": 549.18, "end": 553.0999999999999, "text": " It doesn't matter how many neurons you have or how sophisticated your network is, you", "tokens": [50952, 467, 1177, 380, 1871, 577, 867, 22027, 291, 362, 420, 577, 16950, 428, 3209, 307, 11, 291, 51148], "temperature": 0.0, "avg_logprob": -0.10133388098769301, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.005910330917686224}, {"id": 142, "seek": 53742, "start": 553.0999999999999, "end": 556.86, "text": " just have no idea what your actual function should look like.", "tokens": [51148, 445, 362, 572, 1558, 437, 428, 3539, 2445, 820, 574, 411, 13, 51336], "temperature": 0.0, "avg_logprob": -0.10133388098769301, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.005910330917686224}, {"id": 143, "seek": 53742, "start": 556.86, "end": 561.0999999999999, "text": " It also doesn't make a lot of sense to use a function approximator when you already know", "tokens": [51336, 467, 611, 1177, 380, 652, 257, 688, 295, 2020, 281, 764, 257, 2445, 8542, 1639, 562, 291, 1217, 458, 51548], "temperature": 0.0, "avg_logprob": -0.10133388098769301, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.005910330917686224}, {"id": 144, "seek": 53742, "start": 561.0999999999999, "end": 562.18, "text": " the function.", "tokens": [51548, 264, 2445, 13, 51602], "temperature": 0.0, "avg_logprob": -0.10133388098769301, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.005910330917686224}, {"id": 145, "seek": 53742, "start": 562.18, "end": 566.3399999999999, "text": " You wouldn't build a huge neural network to, say, learn the Mandobrot set when you can", "tokens": [51602, 509, 2759, 380, 1322, 257, 2603, 18161, 3209, 281, 11, 584, 11, 1466, 264, 15458, 996, 10536, 992, 562, 291, 393, 51810], "temperature": 0.0, "avg_logprob": -0.10133388098769301, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.005910330917686224}, {"id": 146, "seek": 56634, "start": 566.34, "end": 570.3000000000001, "text": " just write three lines of code to generate it, unless of course you want to make a cool", "tokens": [50364, 445, 2464, 1045, 3876, 295, 3089, 281, 8460, 309, 11, 5969, 295, 1164, 291, 528, 281, 652, 257, 1627, 50562], "temperature": 0.0, "avg_logprob": -0.11745337572964755, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.21710190176963806}, {"id": 147, "seek": 56634, "start": 570.3000000000001, "end": 572.7800000000001, "text": " background visual for a YouTube video.", "tokens": [50562, 3678, 5056, 337, 257, 3088, 960, 13, 50686], "temperature": 0.0, "avg_logprob": -0.11745337572964755, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.21710190176963806}, {"id": 148, "seek": 56634, "start": 572.7800000000001, "end": 577.62, "text": " There are countless other issues that have to be considered, but for all these complications,", "tokens": [50686, 821, 366, 19223, 661, 2663, 300, 362, 281, 312, 4888, 11, 457, 337, 439, 613, 26566, 11, 50928], "temperature": 0.0, "avg_logprob": -0.11745337572964755, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.21710190176963806}, {"id": 149, "seek": 56634, "start": 577.62, "end": 582.0600000000001, "text": " neural networks have proven themselves to be indispensable for a number of really rather", "tokens": [50928, 18161, 9590, 362, 12785, 2969, 281, 312, 47940, 337, 257, 1230, 295, 534, 2831, 51150], "temperature": 0.0, "avg_logprob": -0.11745337572964755, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.21710190176963806}, {"id": 150, "seek": 56634, "start": 582.0600000000001, "end": 584.86, "text": " famously difficult problems for computers.", "tokens": [51150, 34360, 2252, 2740, 337, 10807, 13, 51290], "temperature": 0.0, "avg_logprob": -0.11745337572964755, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.21710190176963806}, {"id": 151, "seek": 56634, "start": 584.86, "end": 590.1800000000001, "text": " Usually, these problems require a certain level of intuition and fuzzy logic that computers", "tokens": [51290, 11419, 11, 613, 2740, 3651, 257, 1629, 1496, 295, 24002, 293, 34710, 9952, 300, 10807, 51556], "temperature": 0.0, "avg_logprob": -0.11745337572964755, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.21710190176963806}, {"id": 152, "seek": 56634, "start": 590.1800000000001, "end": 595.38, "text": " generally lack, and are very difficult for us to manually write programs to solve.", "tokens": [51556, 5101, 5011, 11, 293, 366, 588, 2252, 337, 505, 281, 16945, 2464, 4268, 281, 5039, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11745337572964755, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.21710190176963806}, {"id": 153, "seek": 59538, "start": 595.38, "end": 599.5, "text": " Things like computer vision, natural language processing, and other areas of machine learning", "tokens": [50364, 9514, 411, 3820, 5201, 11, 3303, 2856, 9007, 11, 293, 661, 3179, 295, 3479, 2539, 50570], "temperature": 0.0, "avg_logprob": -0.1485714857605682, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.30707183480262756}, {"id": 154, "seek": 59538, "start": 599.5, "end": 603.34, "text": " have been utterly transformed by neural networks.", "tokens": [50570, 362, 668, 30251, 16894, 538, 18161, 9590, 13, 50762], "temperature": 0.0, "avg_logprob": -0.1485714857605682, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.30707183480262756}, {"id": 155, "seek": 59538, "start": 603.34, "end": 608.42, "text": " And this is all because of the humble function, a simple yet powerful way to think about the", "tokens": [50762, 400, 341, 307, 439, 570, 295, 264, 16735, 2445, 11, 257, 2199, 1939, 4005, 636, 281, 519, 466, 264, 51016], "temperature": 0.0, "avg_logprob": -0.1485714857605682, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.30707183480262756}, {"id": 156, "seek": 59538, "start": 608.42, "end": 609.5, "text": " world.", "tokens": [51016, 1002, 13, 51070], "temperature": 0.0, "avg_logprob": -0.1485714857605682, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.30707183480262756}, {"id": 157, "seek": 59538, "start": 609.5, "end": 614.58, "text": " And by combining simple computations, we can get computers to construct any function we", "tokens": [51070, 400, 538, 21928, 2199, 2807, 763, 11, 321, 393, 483, 10807, 281, 7690, 604, 2445, 321, 51324], "temperature": 0.0, "avg_logprob": -0.1485714857605682, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.30707183480262756}, {"id": 158, "seek": 59538, "start": 614.58, "end": 616.3, "text": " could ever want.", "tokens": [51324, 727, 1562, 528, 13, 51410], "temperature": 0.0, "avg_logprob": -0.1485714857605682, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.30707183480262756}, {"id": 159, "seek": 59538, "start": 616.3, "end": 618.82, "text": " Neural networks can learn almost anything.", "tokens": [51410, 1734, 1807, 9590, 393, 1466, 1920, 1340, 13, 51536], "temperature": 0.0, "avg_logprob": -0.1485714857605682, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.30707183480262756}, {"id": 160, "seek": 62538, "start": 625.38, "end": 626.38, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.8076667785644531, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.9887845516204834}, {"id": 161, "seek": 62538, "start": 626.38, "end": 627.38, "text": " Thank you.", "tokens": [50414, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.8076667785644531, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.9887845516204834}, {"id": 162, "seek": 62538, "start": 627.38, "end": 628.38, "text": " Thank you.", "tokens": [50464, 1044, 291, 13, 50514], "temperature": 0.0, "avg_logprob": -0.8076667785644531, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.9887845516204834}, {"id": 163, "seek": 62538, "start": 628.38, "end": 629.38, "text": " Bye.", "tokens": [50514, 4621, 13, 50564], "temperature": 0.0, "avg_logprob": -0.8076667785644531, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.9887845516204834}, {"id": 164, "seek": 62538, "start": 629.38, "end": 630.38, "text": " Bye.", "tokens": [50564, 4621, 13, 50614], "temperature": 0.0, "avg_logprob": -0.8076667785644531, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.9887845516204834}], "language": "en"}