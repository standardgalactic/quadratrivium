start	end	text
0	4760	You are currently watching an artificial neural network learn.
4760	9240	In particular, it's learning the shape of an infinitely complex fractal known as the
9240	11040	Mandelbrot set.
11040	14760	This is what that set looks like, complexity all the way down.
14760	19440	Now, in order to understand how a neural network can learn the Mandelbrot set, really how it
19440	26280	can learn anything at all, we will need to start with a fundamental mathematical concept.
26280	28720	What is a function?
28720	35280	Informally, a function is just a system of inputs and outputs, numbers in, numbers out.
35280	38940	In this case, you input an x and it outputs a y.
38940	44000	You can plot all of a function's x and y values in a graph where it draws out a line.
44000	48440	What is important is that if you know the function, you can always calculate the correct
48440	52200	output y given any input x.
52200	58080	But say we don't know the function, and instead only know some of its x and y values.
58080	63520	We know the inputs and outputs, but we don't know the function used to produce them.
63520	68880	Is there a way to reverse engineer that function that produced this data?
68880	73720	If we could construct such a function, we could use it to calculate a y value given
73720	77160	an x value that is not in our original data set.
77160	81360	This would work even if there was a little bit of noise in our data, a little randomness.
81360	86080	We can still capture the overall pattern of the data and continue producing y values that
86080	89200	aren't perfect, but close enough to be useful.
89200	95520	What we need is a function approximation, and more generally, a function approximator.
95520	98120	That is what a neural network is.
98120	102760	This is an online tool for visualizing neural networks, and I'll link it in the description
102760	103760	below.
103760	108480	This particular network takes two inputs, x1 and x2, and produces one output.
108480	112400	Technically, this function would create a three-dimensional surface, but it's easier
112400	114520	to visualize in two dimensions.
114520	119720	This image is rendered by passing the xy coordinate of each pixel into the network, which then
119720	124480	produces a value between negative one and one that is used as the pixel value.
124480	128280	These points are our data set, and are used to train the network.
128280	132640	When we begin training, it quickly constructs a shape that accurately distinguishes between
132640	136920	blue and orange points, building a decision boundary that separates them.
136920	142760	It is approximating the function that describes the data, its learning, and is capable of learning
142760	146040	the different data sets that we throw at it.
146040	147640	So what is this middle section then?
147640	151440	Well, as the name implies, this is the network of neurons.
151440	155520	Each one of these nodes is a neuron, which takes in all the inputs from the previous
155520	160760	layer of neurons and produces one output, which is then fed to the next layer.
160760	163800	Inputs and outputs sounds like we're dealing with a function.
163800	169200	Indeed, a neuron itself is just a function, one that can take any number of inputs and
169200	170920	has one output.
170920	175800	Each input is multiplied by a weight, and all are added together along with a bias.
175800	180480	The weights and bias make up the parameters of this neuron, values that can change as
180480	181640	the network learns.
181640	186640	To keep it easy to visualize, we'll simplify this down to a two-dimensional function, with
186640	189480	only one input and one output.
189480	193920	Now neurons are our building blocks of the larger network, building blocks that can be
193920	199040	stretched and squeezed and shifted around, and ultimately work with other blocks to construct
199040	201440	something larger than themselves.
201440	204720	The neuron, as we've defined it here, works like a building block.
204720	209800	It is actually an extremely simple linear function, one which forms the flat line, or
209800	212040	plain when there's more than one input.
212040	216480	With the two parameters, the weight and bias, we can stretch and squeeze and move our function
216480	218840	up and down and left and right.
218840	224440	As such, we should be able to combine it with other neurons to form a more complicated function,
224440	227620	one built from lots of linear functions.
227620	231340	So let's start with a target function, one we want to approximate.
231340	235620	I've hard-coded a bunch of neurons whose parameters were found manually, and if we
235620	240340	weight each one and add them up, as would happen in the final neuron of the network,
240340	243540	we should get a function that looks like the target function.
243540	246180	Well, that didn't work at all.
246180	247180	What happened?
247180	252380	Well, if we simplify our equation, distributing weights and combining like terms, we end up
252380	254900	with a single linear function.
254900	259280	Turns out, linear functions can only combine to make one linear function.
259280	264440	This is a big problem because we need to make something more complicated than just a line.
264440	268960	We need something that is not linear, a non-linearity.
268960	273060	In our case, we will be using a ReLU, a rectified linear unit.
273060	279220	We use it as our activation function, meaning we simply apply it to our previous naive neuron.
279220	283420	This is about as close as you can get to a linear function without actually being one,
283420	286060	and we can tune it with the same parameters as before.
286060	290700	However, you may notice that we can't actually lift the function off of the x-axis, which
290700	292580	seems like a pretty big limitation.
292580	296500	Well, let's give it a shot anyway, and see if it performs any better than our previous
296500	297500	attempt.
297500	301020	We're still trying to approximate the same function, and we're using the same weights
301020	306420	and biases as before, but this time we're using a ReLU as our activation function.
306420	309660	And just like that, the approximation looks way better.
309660	314180	Unlike before, our function cannot simplify down to a flat linear function.
314180	318380	If we add the neurons one by one, we can see the simple ReLU functions building on one
318380	323100	another, and the inability for one neuron to lift itself off the x-axis doesn't seem
323100	324580	to be a problem.
324580	329140	Many neurons working together overcome the limitations of individual neurons.
329140	333940	Now, I manually found these weights and biases, but how would you find them automatically?
333940	338580	The most common algorithm for this is called back propagation, and is in fact what we're
338580	340700	researching when we run this program.
340700	346500	It essentially tweaks and tunes the parameters of the network bit by bit to improve the approximation,
346500	350020	and the intricacies of this algorithm are really beyond the scope of this video.
350020	353300	I'll link some better explanations in the description.
353300	357380	Now we can see how this shape is formed, and why it looks like it's made up of sort of
357380	358780	sharp linear edges.
358780	362020	It's the nature of the activation function we're using.
362020	367380	We can also see why, if we use no activation function at all, the network utterly fails
367380	368380	to learn.
368580	371380	We need those non-linearities.
371380	374980	So what if we try learning a more complicated data set, like this spiral?
374980	377700	Let's give it a go.
377700	380540	Seems to be struggling a little bit to capture the pattern.
380540	381540	No problem.
381540	385500	If we need a more complicated function, we can add more building blocks, more neurons,
385500	387260	and layers of neurons.
387260	391340	And the network should be able to piece together a better approximation, something that really
391340	393500	captures the spiral.
393500	397340	It seems to be working.
397340	401020	In fact, no matter what the data set is, we can learn it.
401020	406700	That is because neural networks can be rigorously proven to be universal function approximators.
406700	412220	They can approximate any function to any degree of precision you could ever want.
412220	414900	You can always add more neurons.
414900	419340	This is essentially the whole point of deep learning, because it means that neural networks
419340	425740	can approximate anything that can be expressed as a function, a system of inputs and outputs.
425740	429580	This is an extremely general way of thinking about the world.
429580	434260	The Mandelbrot set, for instance, can be written as a function and learned all the same.
434260	439020	This is just a scaled-up version of the experiment we were just looking at, but with an infinitely
439020	440500	complex data set.
440500	444260	We don't even really need to know what the Mandelbrot set is.
444260	447820	The network learns it for us, and that's kind of the point.
447820	453060	If you can express any intelligent behavior, any process, any task as a function, then
453060	454940	a network can learn it.
454940	459100	For instance, your input could be an image and your output a label as to whether it's
459100	464060	a cat or a dog, or your input could be text in English and your output a translation to
464060	465060	Spanish.
465060	469460	You just need to be able to encode your inputs and outputs as numbers, but computers do this
469460	470460	all the time.
470460	475700	Images, video, text, audio, they can all be represented as numbers, and any processing
475700	479740	you may want to do with this data, so long as you can write it as a function, can be
479740	482300	emulated with a neural network.
482300	483900	It goes deeper than this though.
483900	488860	Under a few more assumptions, neural networks are provably turing complete, meaning they
488860	493580	can solve all of the same kinds of problems that any computer can solve.
493580	498300	An implication of this is that any algorithm written in any programming language can be
498300	503580	simulated on a neural network, but rather than being manually written by a human, it
503580	507580	can be learned automatically with a function approximator.
507660	515540	Okay, that is not true.
515540	518620	First off, you can't have an infinite number of neurons.
518620	524420	There are practical limitations on network size and what can be modeled in the real world.
524420	529140	I've also ignored the learning process in this video, and just assumed that you can
529140	532180	find the optimal parameters magically.
532180	537420	How you realistically do this introduces its own constraints on what can be learned.
537420	542260	Additionally, in order for neural networks to approximate a function, you need the data
542260	544940	that actually describes that function.
544940	549180	If you don't have enough data, your approximation will be all wrong.
549180	553100	It doesn't matter how many neurons you have or how sophisticated your network is, you
553100	556860	just have no idea what your actual function should look like.
556860	561100	It also doesn't make a lot of sense to use a function approximator when you already know
561100	562180	the function.
562180	566340	You wouldn't build a huge neural network to, say, learn the Mandobrot set when you can
566340	570300	just write three lines of code to generate it, unless of course you want to make a cool
570300	572780	background visual for a YouTube video.
572780	577620	There are countless other issues that have to be considered, but for all these complications,
577620	582060	neural networks have proven themselves to be indispensable for a number of really rather
582060	584860	famously difficult problems for computers.
584860	590180	Usually, these problems require a certain level of intuition and fuzzy logic that computers
590180	595380	generally lack, and are very difficult for us to manually write programs to solve.
595380	599500	Things like computer vision, natural language processing, and other areas of machine learning
599500	603340	have been utterly transformed by neural networks.
603340	608420	And this is all because of the humble function, a simple yet powerful way to think about the
608420	609500	world.
609500	614580	And by combining simple computations, we can get computers to construct any function we
614580	616300	could ever want.
616300	618820	Neural networks can learn almost anything.
625380	626380	Thank you.
626380	627380	Thank you.
627380	628380	Thank you.
628380	629380	Bye.
629380	630380	Bye.
