WEBVTT

00:00.000 --> 00:04.400
You are currently watching a neural network learn about a year ago

00:04.400 --> 00:10.920
I made a video about how neural networks can learn almost anything and this is because they are universal function approximators

00:11.200 --> 00:17.540
Why is that so important? Well, you might as well ask why functions are important. They are important because

00:18.560 --> 00:20.400
functions

00:20.400 --> 00:22.240
describe

00:22.240 --> 00:24.240
the world

00:25.200 --> 00:28.280
Everything is described by functions

00:28.640 --> 00:33.260
That's right functions describe the sound of my voice on your eardrum

00:33.720 --> 00:37.200
Function the light that's kind of hitting your eyeballs right now

00:38.040 --> 00:44.520
Function different classes and mathematics different areas and mathematics study different kinds of function high school math studies second-degree

00:44.520 --> 00:50.220
One variable polynomials calculus studies smooth one variable functions and it goes on and on

00:50.840 --> 00:53.480
functions describe the world

00:55.240 --> 01:00.240
Yes, correct. Thanks Thomas. He gets a little excited, but he's right

01:00.240 --> 01:05.600
The world can fundamentally be described with numbers and relationships between numbers

01:05.720 --> 01:13.400
We call those relationships functions and with functions we can understand model and predict the world around us

01:14.040 --> 01:18.240
The goal of artificial intelligence is to write programs that can also

01:18.680 --> 01:22.800
Understand model and predict the world or rather have them write themselves

01:22.800 --> 01:25.520
So they must be able to build their own functions

01:25.880 --> 01:32.200
That is the point of function approximation and that is what neural networks do. They are function building machines

01:32.640 --> 01:33.600
In this video

01:33.600 --> 01:40.640
I want to expand on the ideas of my previous video by watching actual neural networks learn strange shapes and strange spaces

01:41.080 --> 01:43.880
Here we will encounter some very difficult challenges

01:44.200 --> 01:51.000
Discover the limitations of neural networks and explore other methods for machine learning and mathematics to approach this open problem

01:51.400 --> 01:57.120
Now I am a programmer not a mathematician and to be honest. I kind of hate math

01:57.240 --> 01:59.800
I've always found it difficult and intimidating

01:59.800 --> 02:05.240
But that's a bad attitude because math is unavoidably useful and occasionally beautiful

02:05.240 --> 02:08.920
I'll do my best to keep things simple and accurate for an audience like me

02:08.920 --> 02:13.000
But know that I'm gonna have to brush over a lot of things and I'm gonna be pretty informal

02:14.120 --> 02:18.960
I recommend you watch my previous video, but to summarize functions are input output machines

02:19.120 --> 02:27.000
They take an input set of numbers and output a corresponding set of numbers and the function defines the relationship between those numbers

02:27.280 --> 02:34.400
The particular problem that neural network solve is when we don't know the definition of the function that we're trying to approximate

02:34.880 --> 02:41.080
Instead we have a sample of data points from that function inputs and outputs. This is our data set

02:41.280 --> 02:48.960
We must approximate a function that fits these data points and allows us to accurately predict outputs given inputs that are not in our

02:49.000 --> 02:55.480
Data set this process is also called curve fitting and you can see why now this is not some handcrafted

02:55.920 --> 03:00.360
Animation it is an actual neural network attempting to fit the curve to the data

03:00.360 --> 03:03.640
And it does so by sort of bending the line into shape

03:04.080 --> 03:10.840
This process is generalizable such that it can fit the curve to any data set and thus construct any function

03:11.160 --> 03:14.280
This makes it a universal function approximator

03:17.000 --> 03:21.640
The network itself is also a function and should approximate some unknown target function

03:22.200 --> 03:28.120
The particular neural architecture we're dealing with in this video is called a fully connected feed forward network

03:28.440 --> 03:35.160
Its inputs and outputs are sometimes called features and predictions and they take the form of vectors arrays of numbers

03:35.600 --> 03:41.120
The overall function is made up of lots of simple functions called neurons that take many inputs

03:41.120 --> 03:48.880
But only produce one output each input is multiplied by its own weight and added up along with one extra weight called a bias

03:49.840 --> 03:52.880
Let's rewrite this weighted sum with some linear algebra

03:53.040 --> 04:00.080
We can put our inputs into a vector with an extra one for the bias and our weights into another vector and then take what is called

04:00.080 --> 04:03.360
The dot product. Let's just make up some example values

04:03.840 --> 04:09.120
To take the dot product we multiply each input by each weight and then add them all up

04:09.920 --> 04:16.560
Finally this dot product is then passed to a very simple activation function in this case a relu which here returns zero

04:17.280 --> 04:20.720
We could use a different activation function, but a relu looks like this

04:21.200 --> 04:27.800
The activation function defines the neurons mathematical shape while the weights shift and squeeze and stretch that shape

04:28.400 --> 04:34.980
We feed the original inputs of our network to a layer of neurons each with their own learned weights and each with their own

04:35.040 --> 04:40.560
Output value we stack these outputs together into a vector and then feed the output vector as

04:40.680 --> 04:46.080
Inputs to the next layer and the next and the next until we get the final output of the network

04:46.560 --> 04:54.160
Each neuron is responsible for learning its own little piece or feature of the overall function and by combining many neurons

04:54.160 --> 05:01.080
We can build an ever more intricate function with an infinite number of neurons. We can provably build any function

05:02.360 --> 05:06.840
The values of the weights or parameters are discovered through the training process

05:06.840 --> 05:12.520
We give the network inputs from our data set and ask it to predict the correct outputs over and over and over

05:13.000 --> 05:21.080
The goal is to minimize the network's error or loss which is some measurement of difference between the predicted outputs and the true outputs

05:22.000 --> 05:25.480
Over time the network should do better and better as loss goes down

05:25.720 --> 05:30.960
The algorithm for this is called back propagation, and I am again not going to explain it in this video

05:30.960 --> 05:34.440
I'll make a video on it. Eventually. I promise. It's a pretty magical algorithm

05:36.080 --> 05:44.300
However, this is a baby problem. What about functions with more than just one input or output that is to say higher dimensional problems

05:44.760 --> 05:50.400
The dimensionality of a vector is defined by the number of numbers in that vector

05:50.640 --> 05:54.280
For a higher dimensional problem. Let's try to learn an image

05:54.680 --> 06:01.560
The input vector is the row column coordinates of a pixel and the output vector is the value of the pixel itself

06:01.560 --> 06:05.520
In math speak we would say that this function maps from R2 to R1

06:05.880 --> 06:11.520
Our data set is all of the pixels in an image. Let's use this unhappy man as an example a

06:11.960 --> 06:18.000
Pixel value of zero is black and one is white although. I'm going to use different color schemes because it's pretty as

06:18.280 --> 06:22.960
As we train we take snapshots of the learned function as the approximation improves

06:23.360 --> 06:26.600
That's what you're saying now, and that's what you saw at the beginning of this video

06:27.160 --> 06:35.080
But to clarify this image is not a single output from the network rather every individual pixel is a single output

06:35.320 --> 06:41.040
We are looking at the entire function all at once and we can do this because it is very low dimensional

06:41.720 --> 06:47.480
You'll also notice that the learning seems to slow down. It's not changing as abruptly as it was at the beginning

06:47.840 --> 06:54.360
This is because we periodically reduce the learning rate a parameter that controls how much our training algorithm

06:54.560 --> 06:59.000
Alters the current function this allows it to progressively refine details

07:00.120 --> 07:04.800
Now just because our neural network should theoretically be able to learn any function

07:04.800 --> 07:10.120
There are things we can do to practically improve the approximation and optimize the learning process

07:10.840 --> 07:15.160
For instance one thing I'm doing here is normalizing the row column inputs

07:15.160 --> 07:17.680
Which means I'm moving the values from a range of zero

07:18.200 --> 07:25.080
1400 to the range of negative one one I do this with a simple linear transformation that shifts and scales the values

07:25.280 --> 07:30.960
The negative one one range is easier for the network to deal with because it's smaller and centered at zero

07:32.200 --> 07:35.920
Another trick is that I'm not using a relu as my activation function

07:35.920 --> 07:43.080
But rather something called a leaky relu a leaky relu can output negative values while still being non-linear and has been shown to

07:43.080 --> 07:48.800
Generally improve performance. So I'm using a leaky relu in all of my layers except for the last one

07:49.480 --> 07:53.960
Because the final output is a pixel value. It needs to be between zero and one

07:54.320 --> 07:56.440
To enforce this in the final layer

07:56.440 --> 08:01.920
We can use a sigmoid activation function which squishes its inputs between zero and one

08:03.040 --> 08:09.280
Except there is a different squishing function called tan H that squishes its inputs between negative one and one

08:09.280 --> 08:15.160
I can then normalize those outputs into the final range of zero one. Why go through the trouble?

08:15.160 --> 08:18.440
Well, tan H just tends to work better than sigmoid

08:19.480 --> 08:24.320
Intuitively, this is because tan H is centered at zero and plays much nicer with back propagation

08:24.320 --> 08:30.240
But ultimately the reasoning doesn't matter as much as the results both networks here are theoretically

08:30.400 --> 08:34.840
Universal function approximators, but practically one works much better than the other

08:35.200 --> 08:40.040
This can be measured empirically by calculating and comparing the error rates of both networks

08:40.040 --> 08:45.880
I think of this as the science of math where we must test our ideas and validate them with evidence

08:46.000 --> 08:48.040
Rather than providing formal proofs

08:48.240 --> 08:51.640
It'd be great if we could do both but that is not always possible

08:51.640 --> 08:56.240
And it is often much easier to just try and see what happens and that's my kind of math

08:57.400 --> 09:04.240
Let's make it harder. Here. We have a function that takes two inputs u v and produces three outputs x y z

09:04.440 --> 09:08.400
It's a parametric surface function and we'll use the equation for a sphere

09:08.640 --> 09:16.080
We can learn it the same way as before take a random sample of points across the surface of the sphere and ask our network to approximate it

09:16.400 --> 09:18.920
Now this is clearly a very silly way to make a sphere

09:18.920 --> 09:24.400
But the network is trying its best to sort of wrap the surface around the sphere to fit the data points

09:24.400 --> 09:28.520
I hope this also gives you a better view of what a parametric surface is

09:28.520 --> 09:33.920
It takes a flat 2d sheet and contorts it in 3d space according to some function

09:35.240 --> 09:39.320
Now this does okay though it never quite closes up around the poles

09:40.680 --> 09:44.320
For a real challenge, let's try this beautiful spiral shell surface

09:44.320 --> 09:49.640
I got the equation for this from this wonderful little website that lets you play with all kinds of shell surfaces

09:49.800 --> 09:52.600
See what I mean when I say that functions describe the world

09:53.320 --> 09:57.200
Anyway, let's sample some points across the spiral surface and start learning

10:05.200 --> 10:13.640
Well, it's working, but clearly we're having some trouble here. I'm using a fairly big neural network

10:13.640 --> 10:19.520
But this is a complicated shape and it seems to be getting a little bit confused. We'll come back to this one

10:21.320 --> 10:27.880
We can also make the problem harder not by increasing dimensionality, but by increasing the complexity of the function itself

10:28.400 --> 10:32.800
Let's use the Mandelbrot set an infinitely complex fractal

10:33.280 --> 10:39.240
But we can simply define a Mandelbrot function as taking two real valued inputs and producing one output

10:39.240 --> 10:41.880
The same dimensionality as the images we learned earlier

10:42.280 --> 10:46.560
Now I've defined my Mandelbrot function to output a value between zero and one

10:46.680 --> 10:50.680
Where one is in the Mandelbrot set and anything less than one is not

10:51.160 --> 10:54.680
Under the hood, it's iteratively operating on complex numbers

10:54.680 --> 10:58.200
And I added some stuff to output smooth values between zero and one

10:58.200 --> 11:00.400
But I'm not going to explain it much more than that

11:00.600 --> 11:04.920
After all a neural network doesn't know the function definition either and it shouldn't matter

11:04.920 --> 11:07.360
It should be able to approximate it all the same

11:07.720 --> 11:12.240
The data set here is randomized points drawn uniformly from this range

11:12.760 --> 11:19.040
Now this has actually been a pet project of mine for some time and I've made several videos trying this exact experiment over the years

11:19.040 --> 11:21.040
I hope you can see why it's interesting

11:21.920 --> 11:29.840
Despite being so low dimensional the Mandelbrot function is infinitely complex literally made with complex numbers and is uniquely difficult to approximate

11:30.400 --> 11:35.640
You can just keep fitting and fitting and fitting the function and you will always come up short

11:36.160 --> 11:40.120
You could do this with any fractal. I just use the Mandelbrot set because it's so well known

11:42.280 --> 11:48.320
So after training for a while we've made some progress, but clearly we're still missing an infinite amount of detail

11:48.560 --> 11:52.840
I've gotten this to look better in the past, but I'm not going to waste any more time training this network

11:52.960 --> 11:54.960
There are better ways of doing this

11:55.640 --> 12:00.920
Are there different methods for approximating functions besides neural networks?

12:01.360 --> 12:07.720
Yes, many actually. There are always many ways to solve the same problem though some ways are better than others

12:08.200 --> 12:11.760
Another mathematical tool we can use is called the Taylor series

12:12.400 --> 12:16.280
This is an infinite sum of a sequence of polynomial functions

12:16.280 --> 12:21.680
x plus x squared plus x cubed plus x to the fourth up to x to the n

12:21.680 --> 12:24.200
n is the order of the series

12:24.480 --> 12:29.120
Each of these terms are multiplied by their own value called a coefficient

12:29.560 --> 12:34.880
Each coefficient controls how much that individual term affects the overall function

12:36.120 --> 12:39.360
Given some target function by choosing the right coefficients

12:39.360 --> 12:44.160
We can approximate that target function around a specific point in this case zero

12:44.640 --> 12:52.480
The approximation gets better the more terms we add where an infinite sum of terms is exactly equivalent to the target function

12:53.440 --> 12:55.440
If we know the target function

12:55.440 --> 13:01.840
We can actually derive the exact coefficients using a general formula to calculate each coefficient for each term

13:02.360 --> 13:07.920
But of course in our particular problem. We don't know the function. We only have a sample of data points

13:07.920 --> 13:09.920
So how do we find the coefficients?

13:10.600 --> 13:14.160
Well, do you see anything familiar in this weighted sum of terms?

13:14.680 --> 13:20.080
We can put all of the x to the n terms into an inputs vector and put all of the coefficients

13:20.120 --> 13:24.800
into a weights vector and then take the dot product a weighted sum

13:25.120 --> 13:29.400
The Taylor series is effectively a single layer neural network

13:29.440 --> 13:37.840
But one where we compute a bunch of additional inputs x squared x cubed and so on we'll call these additional inputs Taylor features

13:38.200 --> 13:42.400
We can then learn the coefficients or weights with back propagation

13:42.400 --> 13:43.320
Of course

13:43.320 --> 13:48.120
We can only compute a finite number of these the partial Taylor series up to some order

13:48.200 --> 13:50.480
But the higher the order the better it should do

13:51.000 --> 13:56.360
Let's use this simple Taylor network to learn this function using eight orders of the Taylor series

13:56.800 --> 13:59.480
Here's our data set and here's the approximation

14:05.080 --> 14:07.080
That's not great

14:07.080 --> 14:10.400
Polynomials are pretty touchy as their values can explode very quickly

14:10.400 --> 14:15.680
So I think back propagation has a tough time finding the right coefficients, but we can do better

14:16.120 --> 14:22.360
Rather than using a single layer network, let's just give these Taylor features to a full multi-layered network

14:22.560 --> 14:24.560
Let's give it a shot

14:35.120 --> 14:37.760
It's a bit wonky, but this performs much better

14:38.040 --> 14:44.440
This trick of computing additional features to feed to the network is a well-known and commonly used one

14:45.120 --> 14:51.280
Intuitively, it's like giving the network different kinds of mathematical building blocks to build a more diverse complex function

14:52.960 --> 14:54.960
Let's try this on an image data set

15:05.040 --> 15:11.400
Well, that's pretty good it's learning, but it doesn't seem to work any better than just using a good old-fashioned neural network

15:11.840 --> 15:16.520
The Taylor series is made to approximate a function around a single given point

15:16.680 --> 15:23.440
While we want to approximate within a given range of points a better tool for this is the Fourier series

15:24.520 --> 15:27.960
The Fourier series acts very much like the Taylor series

15:27.960 --> 15:30.720
But is an infinite sum of sines and cosines

15:31.200 --> 15:36.680
Each order n of the series is made up of sine nx plus cosine nx

15:37.200 --> 15:44.640
Each sine and cosine is multiplied by its own coefficient again controlling how much that term affects the overall function

15:44.880 --> 15:53.560
N, these inner multiplier values, control the frequency of each wave function. The higher the frequency, the more hills the curve has

15:54.440 --> 16:02.640
By combining weighted waves of different frequencies, we can approximate a function within the range of two pi, one full period

16:03.240 --> 16:10.120
Again, if we know the function, we can compute the weights and even if we don't we could use something called the discrete Fourier

16:10.360 --> 16:13.920
Transform, which is really cool, but we're not dealing with it in this video. I

16:14.560 --> 16:18.440
Hope you see where I'm going with this. Let's just jump ahead and do what we did before

16:19.040 --> 16:25.440
Compute a bunch of terms of the Fourier series and feed them to a multi-layer network as additional inputs

16:25.800 --> 16:27.800
Fourier features

16:27.800 --> 16:33.640
Note that we have twice as many Fourier features as Taylor features since we have a sine and cosine

16:34.280 --> 16:36.280
Let's try it on this data set

16:38.680 --> 16:41.960
This works pretty well. It's a little wavy, but not too shabby

16:42.360 --> 16:48.520
Note that for this to work we need to normalize our inputs between negative pi and positive pi one full period

16:49.120 --> 16:51.120
Let's try this on an image

16:51.280 --> 16:57.760
That looks strange at first almost like static coming into focus, but it works and it works really well

16:58.280 --> 17:02.240
If we compare it to networks of the same size trained for the same amount of time

17:02.240 --> 17:09.480
We can see the Fourier network learns much better and faster than the network without Fourier features or the one with Taylor features

17:09.640 --> 17:14.600
Just look at the level of detail in those curly locks. You can hardly tell the difference from the real image

17:15.040 --> 17:22.880
Now I've glossed over a very important detail the example Fourier series

17:22.880 --> 17:28.920
I gave had one input this function has two inputs to handle this properly

17:28.960 --> 17:34.960
We have to use the two-dimensional Fourier series one that takes an input of X and Y

17:35.280 --> 17:37.280
What do we do with that extra Y?

17:38.000 --> 17:42.200
Here are the terms for the 2d Fourier series up to two orders

17:42.560 --> 17:49.600
We are now multiplying the X and Y terms together and end up with sine X cosine Y sine X sine Y

17:49.760 --> 17:56.520
cosine X cosine Y and cosine X sine Y every combination of sine and cosine and Y and X

17:57.360 --> 18:02.520
Not only that we also have every combination of frequencies that inner multiplier

18:02.880 --> 18:07.320
So sine 2x times cosine 1y and so on and so forth

18:07.880 --> 18:10.520
Here's up to three orders now four

18:11.160 --> 18:13.160
That is a lot of terms

18:13.480 --> 18:16.680
we have to calculate this many terms per order and

18:16.840 --> 18:22.800
This number grows very quickly as we increase the order much faster than it would for the 1d series and

18:23.080 --> 18:29.160
This is just for a baby 2d input for a 3d 4d 5d input forget it

18:29.160 --> 18:33.040
The number of computations needed for higher dimensional Fourier series

18:33.440 --> 18:36.880
Explodes as we increase the dimensionality of our inputs

18:37.360 --> 18:40.640
We have encountered the curse of dimensionality

18:41.040 --> 18:46.920
Lots of methods of function approximation and machine learning break down as dimensionality grows

18:47.440 --> 18:51.000
These methods might work well on low dimensional problems, but they become

18:51.560 --> 18:55.840
computationally impractical or impossible for higher dimensional problems

18:56.520 --> 19:00.440
Neural networks by contrast handle the dimensionality problem very well

19:01.080 --> 19:04.040
Comparatively it is trivial to add additional dimensions

19:04.960 --> 19:08.160
But we don't need to use the 2d Fourier series

19:08.240 --> 19:15.160
We can just treat each input as its own independent variable and compute 1d Fourier features for each input

19:15.560 --> 19:19.360
This is less theoretically sound but much more practical to compute

19:19.440 --> 19:25.720
It's still a lot of additional features, but it's manageable and it's worth it. It drastically improves performance

19:25.800 --> 19:28.560
That's what I've been using to get these image approximations

19:29.160 --> 19:36.200
It really shouldn't be surprising that Fourier features help so much here since the Fourier series and transform is used to compress

19:36.240 --> 19:43.080
Images, it's how the JPEG compression algorithm works turns out lots of things can be represented as combinations of waves

19:43.800 --> 19:46.480
So let's apply it to our Mandelbrot data set

19:47.080 --> 19:52.040
Again, it looks a little weird, but it is definitely capturing more detail than the previous attempt

19:52.320 --> 20:00.000
Well, that's fun to watch, but let's evaluate for comparison here is the real Mandelbrot set

20:01.680 --> 20:08.320
Actually, no, this is not the real Mandelbrot set. It is an approximation from our Fourier network

20:09.520 --> 20:13.320
You might be able to tell if you're on a 4k monitor, especially when I zoom in

20:13.720 --> 20:15.720
This network was given

20:15.840 --> 20:18.560
56 orders of the Fourier series, which means a

20:19.280 --> 20:24.280
1024 extra Fourier features being fed to the network and the network itself is pretty damn big

20:25.480 --> 20:32.760
When we really zoom in it becomes very obvious that this is not the real deal. It is still missing an infinite amount of detail

20:40.440 --> 20:44.760
Nonetheless, I am blown away by the quality of the Fourier networks approximation

20:45.160 --> 20:51.000
Fourier features are of course not my idea. They come from this paper that was suggested by a reddit commenter

20:51.000 --> 20:53.000
Who I think actually may have been a co-author?

20:53.360 --> 20:55.360
I'm still missing details from this

20:55.800 --> 21:03.040
Adding Fourier features was one of if not the most effective improvements to the approximation I've applied and it was really surprising

21:03.600 --> 21:06.480
To return to the tricky spiral shell surface

21:06.520 --> 21:10.440
We can see that our Fourier network does way better than our previous attempt

21:10.600 --> 21:15.600
Although the target function is literally defined with sines and cosines, so of course it will do well

21:17.840 --> 21:25.560
So if Fourier features help so much why don't we use them more often they hardly ever show up in real-world neural networks

21:26.000 --> 21:30.920
To state the obvious all of the approximations in this video so far are completely useless

21:31.280 --> 21:36.240
We know the functions and the images. We don't need a massive neural network to approximate them

21:36.920 --> 21:42.680
But I hope that you can see that we're not studying the functions. We're studying the methods of approximation

21:43.520 --> 21:46.120
Because these toy problems are so low dimensional

21:46.120 --> 21:51.920
We can visualize them and hopefully gain insights that will carry over into higher dimensional problems

21:52.280 --> 21:56.240
So let's bring it back to earth with a real problem that uses real data

21:57.560 --> 22:02.760
This is the MNIST dataset images of hand-drawn numbers and their labels

22:03.320 --> 22:09.640
Our input is an entire image flattened out into a vector and our output is a vector of 10 values

22:09.840 --> 22:13.760
Representing a label as to which number 0 through 9 is in the image

22:14.360 --> 22:21.560
There is some unknown function that describes the relationship between an image and its label and that's what we're trying to discover

22:22.560 --> 22:26.480
Even for tiny 28 by 28 black and white images that is a

22:27.480 --> 22:33.720
784 dimensional input that is a lot and this is still a very simple problem for real-world problems

22:33.720 --> 22:40.640
We must address the curse of dimensionality our method must be able to handle huge dimensional inputs and outputs

22:40.840 --> 22:48.200
We also can't visualize the entire approximation all at once as before any idea what a 700 dimensional space looks like

22:48.800 --> 22:53.000
But a normal neural network can handle this problem. Just fine. It's pretty trivial

22:53.320 --> 23:00.280
We can evaluate it by measuring the accuracy of its predictions on images from the dataset that it did not see during training

23:00.360 --> 23:04.560
We'll call this evaluation accuracy and a small network does pretty well

23:04.800 --> 23:08.760
What if we use Fourier features on this problem say up to eight orders?

23:09.400 --> 23:15.280
Well, it does do a little better, but we're adding a lot of additional features for only eight orders

23:15.280 --> 23:17.280
We're computing a total of thirteen

23:18.160 --> 23:21.360
2328 input features which is a lot more than

23:21.840 --> 23:27.880
784 and it's only 2% more accurate when we use 32 orders of the Fourier series

23:27.880 --> 23:33.040
It actually seems to harm performance up to 64 orders and its downright ruiness

23:33.520 --> 23:39.800
This may be due to something called overfitting where our approximation learns the data really well too well

23:39.880 --> 23:42.480
But fails to learn the underlying function

23:42.960 --> 23:48.920
Usually this is a product of not having enough data, but our Fourier network seems to be especially prone to this

23:49.240 --> 23:56.160
This seems consistent with the conclusions of the paper. I mentioned earlier and ultimately our Fourier network seems to be very good for low

23:56.160 --> 23:59.800
Dimensional problems, but not very good for high dimensional problems

24:00.160 --> 24:04.480
No single architecture model or method is the best fit for all tasks

24:04.680 --> 24:09.400
Indeed, there are all kinds of problems that require different approaches than the ones discussed here

24:10.280 --> 24:15.960
Now I'd be surprised if the Fourier series didn't have more to teach us about machine learning, but this is where I'll leave it

24:15.960 --> 24:24.000
I hope this video has helped you appreciate what function approximation is and why it's useful and maybe sparked your imagination with some alternative perspectives

24:24.560 --> 24:30.680
Neural networks are a kind of mathematical clay that can be molded into arbitrary shapes for arbitrary purposes

24:31.840 --> 24:38.400
I want to finish by opening up the Mandelbrot approximation problem as a fun challenge for anyone who's interested

24:38.680 --> 24:44.920
How precisely and deeply can you approximate the Mandelbrot set given only a random sample of points?

24:45.440 --> 24:51.360
There are probably a million things that could be done to improve on my approximation and the internet is much smarter than I am

24:51.720 --> 24:56.680
The only rule is that your solution must still be a universal function approximator

24:56.880 --> 25:00.800
Meaning it could still learn any other data set of any dimensionality

25:01.520 --> 25:06.920
This is just for fun, but potentially solutions to this toy problem could have uses in the real world

25:07.320 --> 25:14.080
There is no reason to think that we found the best way of doing this and there may be far better solutions waiting to be discovered

25:14.960 --> 25:16.960
Thanks for watching

