You are currently watching a neural network learn about a year ago
I made a video about how neural networks can learn almost anything and this is because they are universal function approximators
Why is that so important? Well, you might as well ask why functions are important. They are important because
functions
describe
the world
Everything is described by functions
That's right functions describe the sound of my voice on your eardrum
Function the light that's kind of hitting your eyeballs right now
Function different classes and mathematics different areas and mathematics study different kinds of function high school math studies second-degree
One variable polynomials calculus studies smooth one variable functions and it goes on and on
functions describe the world
Yes, correct. Thanks Thomas. He gets a little excited, but he's right
The world can fundamentally be described with numbers and relationships between numbers
We call those relationships functions and with functions we can understand model and predict the world around us
The goal of artificial intelligence is to write programs that can also
Understand model and predict the world or rather have them write themselves
So they must be able to build their own functions
That is the point of function approximation and that is what neural networks do. They are function building machines
In this video
I want to expand on the ideas of my previous video by watching actual neural networks learn strange shapes and strange spaces
Here we will encounter some very difficult challenges
Discover the limitations of neural networks and explore other methods for machine learning and mathematics to approach this open problem
Now I am a programmer not a mathematician and to be honest. I kind of hate math
I've always found it difficult and intimidating
But that's a bad attitude because math is unavoidably useful and occasionally beautiful
I'll do my best to keep things simple and accurate for an audience like me
But know that I'm gonna have to brush over a lot of things and I'm gonna be pretty informal
I recommend you watch my previous video, but to summarize functions are input output machines
They take an input set of numbers and output a corresponding set of numbers and the function defines the relationship between those numbers
The particular problem that neural network solve is when we don't know the definition of the function that we're trying to approximate
Instead we have a sample of data points from that function inputs and outputs. This is our data set
We must approximate a function that fits these data points and allows us to accurately predict outputs given inputs that are not in our
Data set this process is also called curve fitting and you can see why now this is not some handcrafted
Animation it is an actual neural network attempting to fit the curve to the data
And it does so by sort of bending the line into shape
This process is generalizable such that it can fit the curve to any data set and thus construct any function
This makes it a universal function approximator
The network itself is also a function and should approximate some unknown target function
The particular neural architecture we're dealing with in this video is called a fully connected feed forward network
Its inputs and outputs are sometimes called features and predictions and they take the form of vectors arrays of numbers
The overall function is made up of lots of simple functions called neurons that take many inputs
But only produce one output each input is multiplied by its own weight and added up along with one extra weight called a bias
Let's rewrite this weighted sum with some linear algebra
We can put our inputs into a vector with an extra one for the bias and our weights into another vector and then take what is called
The dot product. Let's just make up some example values
To take the dot product we multiply each input by each weight and then add them all up
Finally this dot product is then passed to a very simple activation function in this case a relu which here returns zero
We could use a different activation function, but a relu looks like this
The activation function defines the neurons mathematical shape while the weights shift and squeeze and stretch that shape
We feed the original inputs of our network to a layer of neurons each with their own learned weights and each with their own
Output value we stack these outputs together into a vector and then feed the output vector as
Inputs to the next layer and the next and the next until we get the final output of the network
Each neuron is responsible for learning its own little piece or feature of the overall function and by combining many neurons
We can build an ever more intricate function with an infinite number of neurons. We can provably build any function
The values of the weights or parameters are discovered through the training process
We give the network inputs from our data set and ask it to predict the correct outputs over and over and over
The goal is to minimize the network's error or loss which is some measurement of difference between the predicted outputs and the true outputs
Over time the network should do better and better as loss goes down
The algorithm for this is called back propagation, and I am again not going to explain it in this video
I'll make a video on it. Eventually. I promise. It's a pretty magical algorithm
However, this is a baby problem. What about functions with more than just one input or output that is to say higher dimensional problems
The dimensionality of a vector is defined by the number of numbers in that vector
For a higher dimensional problem. Let's try to learn an image
The input vector is the row column coordinates of a pixel and the output vector is the value of the pixel itself
In math speak we would say that this function maps from R2 to R1
Our data set is all of the pixels in an image. Let's use this unhappy man as an example a
Pixel value of zero is black and one is white although. I'm going to use different color schemes because it's pretty as
As we train we take snapshots of the learned function as the approximation improves
That's what you're saying now, and that's what you saw at the beginning of this video
But to clarify this image is not a single output from the network rather every individual pixel is a single output
We are looking at the entire function all at once and we can do this because it is very low dimensional
You'll also notice that the learning seems to slow down. It's not changing as abruptly as it was at the beginning
This is because we periodically reduce the learning rate a parameter that controls how much our training algorithm
Alters the current function this allows it to progressively refine details
Now just because our neural network should theoretically be able to learn any function
There are things we can do to practically improve the approximation and optimize the learning process
For instance one thing I'm doing here is normalizing the row column inputs
Which means I'm moving the values from a range of zero
1400 to the range of negative one one I do this with a simple linear transformation that shifts and scales the values
The negative one one range is easier for the network to deal with because it's smaller and centered at zero
Another trick is that I'm not using a relu as my activation function
But rather something called a leaky relu a leaky relu can output negative values while still being non-linear and has been shown to
Generally improve performance. So I'm using a leaky relu in all of my layers except for the last one
Because the final output is a pixel value. It needs to be between zero and one
To enforce this in the final layer
We can use a sigmoid activation function which squishes its inputs between zero and one
Except there is a different squishing function called tan H that squishes its inputs between negative one and one
I can then normalize those outputs into the final range of zero one. Why go through the trouble?
Well, tan H just tends to work better than sigmoid
Intuitively, this is because tan H is centered at zero and plays much nicer with back propagation
But ultimately the reasoning doesn't matter as much as the results both networks here are theoretically
Universal function approximators, but practically one works much better than the other
This can be measured empirically by calculating and comparing the error rates of both networks
I think of this as the science of math where we must test our ideas and validate them with evidence
Rather than providing formal proofs
It'd be great if we could do both but that is not always possible
And it is often much easier to just try and see what happens and that's my kind of math
Let's make it harder. Here. We have a function that takes two inputs u v and produces three outputs x y z
It's a parametric surface function and we'll use the equation for a sphere
We can learn it the same way as before take a random sample of points across the surface of the sphere and ask our network to approximate it
Now this is clearly a very silly way to make a sphere
But the network is trying its best to sort of wrap the surface around the sphere to fit the data points
I hope this also gives you a better view of what a parametric surface is
It takes a flat 2d sheet and contorts it in 3d space according to some function
Now this does okay though it never quite closes up around the poles
For a real challenge, let's try this beautiful spiral shell surface
I got the equation for this from this wonderful little website that lets you play with all kinds of shell surfaces
See what I mean when I say that functions describe the world
Anyway, let's sample some points across the spiral surface and start learning
Well, it's working, but clearly we're having some trouble here. I'm using a fairly big neural network
But this is a complicated shape and it seems to be getting a little bit confused. We'll come back to this one
We can also make the problem harder not by increasing dimensionality, but by increasing the complexity of the function itself
Let's use the Mandelbrot set an infinitely complex fractal
But we can simply define a Mandelbrot function as taking two real valued inputs and producing one output
The same dimensionality as the images we learned earlier
Now I've defined my Mandelbrot function to output a value between zero and one
Where one is in the Mandelbrot set and anything less than one is not
Under the hood, it's iteratively operating on complex numbers
And I added some stuff to output smooth values between zero and one
But I'm not going to explain it much more than that
After all a neural network doesn't know the function definition either and it shouldn't matter
It should be able to approximate it all the same
The data set here is randomized points drawn uniformly from this range
Now this has actually been a pet project of mine for some time and I've made several videos trying this exact experiment over the years
I hope you can see why it's interesting
Despite being so low dimensional the Mandelbrot function is infinitely complex literally made with complex numbers and is uniquely difficult to approximate
You can just keep fitting and fitting and fitting the function and you will always come up short
You could do this with any fractal. I just use the Mandelbrot set because it's so well known
So after training for a while we've made some progress, but clearly we're still missing an infinite amount of detail
I've gotten this to look better in the past, but I'm not going to waste any more time training this network
There are better ways of doing this
Are there different methods for approximating functions besides neural networks?
Yes, many actually. There are always many ways to solve the same problem though some ways are better than others
Another mathematical tool we can use is called the Taylor series
This is an infinite sum of a sequence of polynomial functions
x plus x squared plus x cubed plus x to the fourth up to x to the n
n is the order of the series
Each of these terms are multiplied by their own value called a coefficient
Each coefficient controls how much that individual term affects the overall function
Given some target function by choosing the right coefficients
We can approximate that target function around a specific point in this case zero
The approximation gets better the more terms we add where an infinite sum of terms is exactly equivalent to the target function
If we know the target function
We can actually derive the exact coefficients using a general formula to calculate each coefficient for each term
But of course in our particular problem. We don't know the function. We only have a sample of data points
So how do we find the coefficients?
Well, do you see anything familiar in this weighted sum of terms?
We can put all of the x to the n terms into an inputs vector and put all of the coefficients
into a weights vector and then take the dot product a weighted sum
The Taylor series is effectively a single layer neural network
But one where we compute a bunch of additional inputs x squared x cubed and so on we'll call these additional inputs Taylor features
We can then learn the coefficients or weights with back propagation
Of course
We can only compute a finite number of these the partial Taylor series up to some order
But the higher the order the better it should do
Let's use this simple Taylor network to learn this function using eight orders of the Taylor series
Here's our data set and here's the approximation
That's not great
Polynomials are pretty touchy as their values can explode very quickly
So I think back propagation has a tough time finding the right coefficients, but we can do better
Rather than using a single layer network, let's just give these Taylor features to a full multi-layered network
Let's give it a shot
It's a bit wonky, but this performs much better
This trick of computing additional features to feed to the network is a well-known and commonly used one
Intuitively, it's like giving the network different kinds of mathematical building blocks to build a more diverse complex function
Let's try this on an image data set
Well, that's pretty good it's learning, but it doesn't seem to work any better than just using a good old-fashioned neural network
The Taylor series is made to approximate a function around a single given point
While we want to approximate within a given range of points a better tool for this is the Fourier series
The Fourier series acts very much like the Taylor series
But is an infinite sum of sines and cosines
Each order n of the series is made up of sine nx plus cosine nx
Each sine and cosine is multiplied by its own coefficient again controlling how much that term affects the overall function
N, these inner multiplier values, control the frequency of each wave function. The higher the frequency, the more hills the curve has
By combining weighted waves of different frequencies, we can approximate a function within the range of two pi, one full period
Again, if we know the function, we can compute the weights and even if we don't we could use something called the discrete Fourier
Transform, which is really cool, but we're not dealing with it in this video. I
Hope you see where I'm going with this. Let's just jump ahead and do what we did before
Compute a bunch of terms of the Fourier series and feed them to a multi-layer network as additional inputs
Fourier features
Note that we have twice as many Fourier features as Taylor features since we have a sine and cosine
Let's try it on this data set
This works pretty well. It's a little wavy, but not too shabby
Note that for this to work we need to normalize our inputs between negative pi and positive pi one full period
Let's try this on an image
That looks strange at first almost like static coming into focus, but it works and it works really well
If we compare it to networks of the same size trained for the same amount of time
We can see the Fourier network learns much better and faster than the network without Fourier features or the one with Taylor features
Just look at the level of detail in those curly locks. You can hardly tell the difference from the real image
Now I've glossed over a very important detail the example Fourier series
I gave had one input this function has two inputs to handle this properly
We have to use the two-dimensional Fourier series one that takes an input of X and Y
What do we do with that extra Y?
Here are the terms for the 2d Fourier series up to two orders
We are now multiplying the X and Y terms together and end up with sine X cosine Y sine X sine Y
cosine X cosine Y and cosine X sine Y every combination of sine and cosine and Y and X
Not only that we also have every combination of frequencies that inner multiplier
So sine 2x times cosine 1y and so on and so forth
Here's up to three orders now four
That is a lot of terms
we have to calculate this many terms per order and
This number grows very quickly as we increase the order much faster than it would for the 1d series and
This is just for a baby 2d input for a 3d 4d 5d input forget it
The number of computations needed for higher dimensional Fourier series
Explodes as we increase the dimensionality of our inputs
We have encountered the curse of dimensionality
Lots of methods of function approximation and machine learning break down as dimensionality grows
These methods might work well on low dimensional problems, but they become
computationally impractical or impossible for higher dimensional problems
Neural networks by contrast handle the dimensionality problem very well
Comparatively it is trivial to add additional dimensions
But we don't need to use the 2d Fourier series
We can just treat each input as its own independent variable and compute 1d Fourier features for each input
This is less theoretically sound but much more practical to compute
It's still a lot of additional features, but it's manageable and it's worth it. It drastically improves performance
That's what I've been using to get these image approximations
It really shouldn't be surprising that Fourier features help so much here since the Fourier series and transform is used to compress
Images, it's how the JPEG compression algorithm works turns out lots of things can be represented as combinations of waves
So let's apply it to our Mandelbrot data set
Again, it looks a little weird, but it is definitely capturing more detail than the previous attempt
Well, that's fun to watch, but let's evaluate for comparison here is the real Mandelbrot set
Actually, no, this is not the real Mandelbrot set. It is an approximation from our Fourier network
You might be able to tell if you're on a 4k monitor, especially when I zoom in
This network was given
56 orders of the Fourier series, which means a
1024 extra Fourier features being fed to the network and the network itself is pretty damn big
When we really zoom in it becomes very obvious that this is not the real deal. It is still missing an infinite amount of detail
Nonetheless, I am blown away by the quality of the Fourier networks approximation
Fourier features are of course not my idea. They come from this paper that was suggested by a reddit commenter
Who I think actually may have been a co-author?
I'm still missing details from this
Adding Fourier features was one of if not the most effective improvements to the approximation I've applied and it was really surprising
To return to the tricky spiral shell surface
We can see that our Fourier network does way better than our previous attempt
Although the target function is literally defined with sines and cosines, so of course it will do well
So if Fourier features help so much why don't we use them more often they hardly ever show up in real-world neural networks
To state the obvious all of the approximations in this video so far are completely useless
We know the functions and the images. We don't need a massive neural network to approximate them
But I hope that you can see that we're not studying the functions. We're studying the methods of approximation
Because these toy problems are so low dimensional
We can visualize them and hopefully gain insights that will carry over into higher dimensional problems
So let's bring it back to earth with a real problem that uses real data
This is the MNIST dataset images of hand-drawn numbers and their labels
Our input is an entire image flattened out into a vector and our output is a vector of 10 values
Representing a label as to which number 0 through 9 is in the image
There is some unknown function that describes the relationship between an image and its label and that's what we're trying to discover
Even for tiny 28 by 28 black and white images that is a
784 dimensional input that is a lot and this is still a very simple problem for real-world problems
We must address the curse of dimensionality our method must be able to handle huge dimensional inputs and outputs
We also can't visualize the entire approximation all at once as before any idea what a 700 dimensional space looks like
But a normal neural network can handle this problem. Just fine. It's pretty trivial
We can evaluate it by measuring the accuracy of its predictions on images from the dataset that it did not see during training
We'll call this evaluation accuracy and a small network does pretty well
What if we use Fourier features on this problem say up to eight orders?
Well, it does do a little better, but we're adding a lot of additional features for only eight orders
We're computing a total of thirteen
2328 input features which is a lot more than
784 and it's only 2% more accurate when we use 32 orders of the Fourier series
It actually seems to harm performance up to 64 orders and its downright ruiness
This may be due to something called overfitting where our approximation learns the data really well too well
But fails to learn the underlying function
Usually this is a product of not having enough data, but our Fourier network seems to be especially prone to this
This seems consistent with the conclusions of the paper. I mentioned earlier and ultimately our Fourier network seems to be very good for low
Dimensional problems, but not very good for high dimensional problems
No single architecture model or method is the best fit for all tasks
Indeed, there are all kinds of problems that require different approaches than the ones discussed here
Now I'd be surprised if the Fourier series didn't have more to teach us about machine learning, but this is where I'll leave it
I hope this video has helped you appreciate what function approximation is and why it's useful and maybe sparked your imagination with some alternative perspectives
Neural networks are a kind of mathematical clay that can be molded into arbitrary shapes for arbitrary purposes
I want to finish by opening up the Mandelbrot approximation problem as a fun challenge for anyone who's interested
How precisely and deeply can you approximate the Mandelbrot set given only a random sample of points?
There are probably a million things that could be done to improve on my approximation and the internet is much smarter than I am
The only rule is that your solution must still be a universal function approximator
Meaning it could still learn any other data set of any dimensionality
This is just for fun, but potentially solutions to this toy problem could have uses in the real world
There is no reason to think that we found the best way of doing this and there may be far better solutions waiting to be discovered
Thanks for watching
