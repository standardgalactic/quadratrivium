WEBVTT

00:00.000 --> 00:04.760
You are currently watching an artificial neural network learn.

00:04.760 --> 00:09.240
In particular, it's learning the shape of an infinitely complex fractal known as the

00:09.240 --> 00:11.040
Mandelbrot set.

00:11.040 --> 00:14.760
This is what that set looks like, complexity all the way down.

00:14.760 --> 00:19.440
Now, in order to understand how a neural network can learn the Mandelbrot set, really how it

00:19.440 --> 00:26.280
can learn anything at all, we will need to start with a fundamental mathematical concept.

00:26.280 --> 00:28.720
What is a function?

00:28.720 --> 00:35.280
Informally, a function is just a system of inputs and outputs, numbers in, numbers out.

00:35.280 --> 00:38.940
In this case, you input an x and it outputs a y.

00:38.940 --> 00:44.000
You can plot all of a function's x and y values in a graph where it draws out a line.

00:44.000 --> 00:48.440
What is important is that if you know the function, you can always calculate the correct

00:48.440 --> 00:52.200
output y given any input x.

00:52.200 --> 00:58.080
But say we don't know the function, and instead only know some of its x and y values.

00:58.080 --> 01:03.520
We know the inputs and outputs, but we don't know the function used to produce them.

01:03.520 --> 01:08.880
Is there a way to reverse engineer that function that produced this data?

01:08.880 --> 01:13.720
If we could construct such a function, we could use it to calculate a y value given

01:13.720 --> 01:17.160
an x value that is not in our original data set.

01:17.160 --> 01:21.360
This would work even if there was a little bit of noise in our data, a little randomness.

01:21.360 --> 01:26.080
We can still capture the overall pattern of the data and continue producing y values that

01:26.080 --> 01:29.200
aren't perfect, but close enough to be useful.

01:29.200 --> 01:35.520
What we need is a function approximation, and more generally, a function approximator.

01:35.520 --> 01:38.120
That is what a neural network is.

01:38.120 --> 01:42.760
This is an online tool for visualizing neural networks, and I'll link it in the description

01:42.760 --> 01:43.760
below.

01:43.760 --> 01:48.480
This particular network takes two inputs, x1 and x2, and produces one output.

01:48.480 --> 01:52.400
Technically, this function would create a three-dimensional surface, but it's easier

01:52.400 --> 01:54.520
to visualize in two dimensions.

01:54.520 --> 01:59.720
This image is rendered by passing the xy coordinate of each pixel into the network, which then

01:59.720 --> 02:04.480
produces a value between negative one and one that is used as the pixel value.

02:04.480 --> 02:08.280
These points are our data set, and are used to train the network.

02:08.280 --> 02:12.640
When we begin training, it quickly constructs a shape that accurately distinguishes between

02:12.640 --> 02:16.920
blue and orange points, building a decision boundary that separates them.

02:16.920 --> 02:22.760
It is approximating the function that describes the data, its learning, and is capable of learning

02:22.760 --> 02:26.040
the different data sets that we throw at it.

02:26.040 --> 02:27.640
So what is this middle section then?

02:27.640 --> 02:31.440
Well, as the name implies, this is the network of neurons.

02:31.440 --> 02:35.520
Each one of these nodes is a neuron, which takes in all the inputs from the previous

02:35.520 --> 02:40.760
layer of neurons and produces one output, which is then fed to the next layer.

02:40.760 --> 02:43.800
Inputs and outputs sounds like we're dealing with a function.

02:43.800 --> 02:49.200
Indeed, a neuron itself is just a function, one that can take any number of inputs and

02:49.200 --> 02:50.920
has one output.

02:50.920 --> 02:55.800
Each input is multiplied by a weight, and all are added together along with a bias.

02:55.800 --> 03:00.480
The weights and bias make up the parameters of this neuron, values that can change as

03:00.480 --> 03:01.640
the network learns.

03:01.640 --> 03:06.640
To keep it easy to visualize, we'll simplify this down to a two-dimensional function, with

03:06.640 --> 03:09.480
only one input and one output.

03:09.480 --> 03:13.920
Now neurons are our building blocks of the larger network, building blocks that can be

03:13.920 --> 03:19.040
stretched and squeezed and shifted around, and ultimately work with other blocks to construct

03:19.040 --> 03:21.440
something larger than themselves.

03:21.440 --> 03:24.720
The neuron, as we've defined it here, works like a building block.

03:24.720 --> 03:29.800
It is actually an extremely simple linear function, one which forms the flat line, or

03:29.800 --> 03:32.040
plain when there's more than one input.

03:32.040 --> 03:36.480
With the two parameters, the weight and bias, we can stretch and squeeze and move our function

03:36.480 --> 03:38.840
up and down and left and right.

03:38.840 --> 03:44.440
As such, we should be able to combine it with other neurons to form a more complicated function,

03:44.440 --> 03:47.620
one built from lots of linear functions.

03:47.620 --> 03:51.340
So let's start with a target function, one we want to approximate.

03:51.340 --> 03:55.620
I've hard-coded a bunch of neurons whose parameters were found manually, and if we

03:55.620 --> 04:00.340
weight each one and add them up, as would happen in the final neuron of the network,

04:00.340 --> 04:03.540
we should get a function that looks like the target function.

04:03.540 --> 04:06.180
Well, that didn't work at all.

04:06.180 --> 04:07.180
What happened?

04:07.180 --> 04:12.380
Well, if we simplify our equation, distributing weights and combining like terms, we end up

04:12.380 --> 04:14.900
with a single linear function.

04:14.900 --> 04:19.280
Turns out, linear functions can only combine to make one linear function.

04:19.280 --> 04:24.440
This is a big problem because we need to make something more complicated than just a line.

04:24.440 --> 04:28.960
We need something that is not linear, a non-linearity.

04:28.960 --> 04:33.060
In our case, we will be using a ReLU, a rectified linear unit.

04:33.060 --> 04:39.220
We use it as our activation function, meaning we simply apply it to our previous naive neuron.

04:39.220 --> 04:43.420
This is about as close as you can get to a linear function without actually being one,

04:43.420 --> 04:46.060
and we can tune it with the same parameters as before.

04:46.060 --> 04:50.700
However, you may notice that we can't actually lift the function off of the x-axis, which

04:50.700 --> 04:52.580
seems like a pretty big limitation.

04:52.580 --> 04:56.500
Well, let's give it a shot anyway, and see if it performs any better than our previous

04:56.500 --> 04:57.500
attempt.

04:57.500 --> 05:01.020
We're still trying to approximate the same function, and we're using the same weights

05:01.020 --> 05:06.420
and biases as before, but this time we're using a ReLU as our activation function.

05:06.420 --> 05:09.660
And just like that, the approximation looks way better.

05:09.660 --> 05:14.180
Unlike before, our function cannot simplify down to a flat linear function.

05:14.180 --> 05:18.380
If we add the neurons one by one, we can see the simple ReLU functions building on one

05:18.380 --> 05:23.100
another, and the inability for one neuron to lift itself off the x-axis doesn't seem

05:23.100 --> 05:24.580
to be a problem.

05:24.580 --> 05:29.140
Many neurons working together overcome the limitations of individual neurons.

05:29.140 --> 05:33.940
Now, I manually found these weights and biases, but how would you find them automatically?

05:33.940 --> 05:38.580
The most common algorithm for this is called back propagation, and is in fact what we're

05:38.580 --> 05:40.700
researching when we run this program.

05:40.700 --> 05:46.500
It essentially tweaks and tunes the parameters of the network bit by bit to improve the approximation,

05:46.500 --> 05:50.020
and the intricacies of this algorithm are really beyond the scope of this video.

05:50.020 --> 05:53.300
I'll link some better explanations in the description.

05:53.300 --> 05:57.380
Now we can see how this shape is formed, and why it looks like it's made up of sort of

05:57.380 --> 05:58.780
sharp linear edges.

05:58.780 --> 06:02.020
It's the nature of the activation function we're using.

06:02.020 --> 06:07.380
We can also see why, if we use no activation function at all, the network utterly fails

06:07.380 --> 06:08.380
to learn.

06:08.580 --> 06:11.380
We need those non-linearities.

06:11.380 --> 06:14.980
So what if we try learning a more complicated data set, like this spiral?

06:14.980 --> 06:17.700
Let's give it a go.

06:17.700 --> 06:20.540
Seems to be struggling a little bit to capture the pattern.

06:20.540 --> 06:21.540
No problem.

06:21.540 --> 06:25.500
If we need a more complicated function, we can add more building blocks, more neurons,

06:25.500 --> 06:27.260
and layers of neurons.

06:27.260 --> 06:31.340
And the network should be able to piece together a better approximation, something that really

06:31.340 --> 06:33.500
captures the spiral.

06:33.500 --> 06:37.340
It seems to be working.

06:37.340 --> 06:41.020
In fact, no matter what the data set is, we can learn it.

06:41.020 --> 06:46.700
That is because neural networks can be rigorously proven to be universal function approximators.

06:46.700 --> 06:52.220
They can approximate any function to any degree of precision you could ever want.

06:52.220 --> 06:54.900
You can always add more neurons.

06:54.900 --> 06:59.340
This is essentially the whole point of deep learning, because it means that neural networks

06:59.340 --> 07:05.740
can approximate anything that can be expressed as a function, a system of inputs and outputs.

07:05.740 --> 07:09.580
This is an extremely general way of thinking about the world.

07:09.580 --> 07:14.260
The Mandelbrot set, for instance, can be written as a function and learned all the same.

07:14.260 --> 07:19.020
This is just a scaled-up version of the experiment we were just looking at, but with an infinitely

07:19.020 --> 07:20.500
complex data set.

07:20.500 --> 07:24.260
We don't even really need to know what the Mandelbrot set is.

07:24.260 --> 07:27.820
The network learns it for us, and that's kind of the point.

07:27.820 --> 07:33.060
If you can express any intelligent behavior, any process, any task as a function, then

07:33.060 --> 07:34.940
a network can learn it.

07:34.940 --> 07:39.100
For instance, your input could be an image and your output a label as to whether it's

07:39.100 --> 07:44.060
a cat or a dog, or your input could be text in English and your output a translation to

07:44.060 --> 07:45.060
Spanish.

07:45.060 --> 07:49.460
You just need to be able to encode your inputs and outputs as numbers, but computers do this

07:49.460 --> 07:50.460
all the time.

07:50.460 --> 07:55.700
Images, video, text, audio, they can all be represented as numbers, and any processing

07:55.700 --> 07:59.740
you may want to do with this data, so long as you can write it as a function, can be

07:59.740 --> 08:02.300
emulated with a neural network.

08:02.300 --> 08:03.900
It goes deeper than this though.

08:03.900 --> 08:08.860
Under a few more assumptions, neural networks are provably turing complete, meaning they

08:08.860 --> 08:13.580
can solve all of the same kinds of problems that any computer can solve.

08:13.580 --> 08:18.300
An implication of this is that any algorithm written in any programming language can be

08:18.300 --> 08:23.580
simulated on a neural network, but rather than being manually written by a human, it

08:23.580 --> 08:27.580
can be learned automatically with a function approximator.

08:27.660 --> 08:35.540
Okay, that is not true.

08:35.540 --> 08:38.620
First off, you can't have an infinite number of neurons.

08:38.620 --> 08:44.420
There are practical limitations on network size and what can be modeled in the real world.

08:44.420 --> 08:49.140
I've also ignored the learning process in this video, and just assumed that you can

08:49.140 --> 08:52.180
find the optimal parameters magically.

08:52.180 --> 08:57.420
How you realistically do this introduces its own constraints on what can be learned.

08:57.420 --> 09:02.260
Additionally, in order for neural networks to approximate a function, you need the data

09:02.260 --> 09:04.940
that actually describes that function.

09:04.940 --> 09:09.180
If you don't have enough data, your approximation will be all wrong.

09:09.180 --> 09:13.100
It doesn't matter how many neurons you have or how sophisticated your network is, you

09:13.100 --> 09:16.860
just have no idea what your actual function should look like.

09:16.860 --> 09:21.100
It also doesn't make a lot of sense to use a function approximator when you already know

09:21.100 --> 09:22.180
the function.

09:22.180 --> 09:26.340
You wouldn't build a huge neural network to, say, learn the Mandobrot set when you can

09:26.340 --> 09:30.300
just write three lines of code to generate it, unless of course you want to make a cool

09:30.300 --> 09:32.780
background visual for a YouTube video.

09:32.780 --> 09:37.620
There are countless other issues that have to be considered, but for all these complications,

09:37.620 --> 09:42.060
neural networks have proven themselves to be indispensable for a number of really rather

09:42.060 --> 09:44.860
famously difficult problems for computers.

09:44.860 --> 09:50.180
Usually, these problems require a certain level of intuition and fuzzy logic that computers

09:50.180 --> 09:55.380
generally lack, and are very difficult for us to manually write programs to solve.

09:55.380 --> 09:59.500
Things like computer vision, natural language processing, and other areas of machine learning

09:59.500 --> 10:03.340
have been utterly transformed by neural networks.

10:03.340 --> 10:08.420
And this is all because of the humble function, a simple yet powerful way to think about the

10:08.420 --> 10:09.500
world.

10:09.500 --> 10:14.580
And by combining simple computations, we can get computers to construct any function we

10:14.580 --> 10:16.300
could ever want.

10:16.300 --> 10:18.820
Neural networks can learn almost anything.

10:25.380 --> 10:26.380
Thank you.

10:26.380 --> 10:27.380
Thank you.

10:27.380 --> 10:28.380
Thank you.

10:28.380 --> 10:29.380
Bye.

10:29.380 --> 10:30.380
Bye.

