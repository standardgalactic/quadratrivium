1
00:00:00,000 --> 00:00:05,000
Welcome everyone from and hello from Rainey, California.

2
00:00:05,240 --> 00:00:07,600
Today we're hosting an exciting event

3
00:00:07,600 --> 00:00:10,080
on the future of General SDI.

4
00:00:10,080 --> 00:00:11,600
Our guests today are professors

5
00:00:11,600 --> 00:00:14,600
Christoph Fondrell-Malsburg and Michael Levin.

6
00:00:14,600 --> 00:00:16,720
Michael Levin is a distinguished professor

7
00:00:16,720 --> 00:00:18,160
at the Department of Biology

8
00:00:18,160 --> 00:00:20,800
and the principal investigator at the Levin Lab

9
00:00:20,800 --> 00:00:22,720
at Tufts University.

10
00:00:22,720 --> 00:00:25,160
Christoph Fondrell-Malsburg is a senior fellow

11
00:00:25,160 --> 00:00:27,360
at the Frankfurt Institute for Advanced Studies

12
00:00:27,360 --> 00:00:28,560
and the visiting professor

13
00:00:28,560 --> 00:00:33,440
at the Institute of Neuroinformatics at ETH Zurich.

14
00:00:33,440 --> 00:00:35,920
And finally, our host today is Dr. Yoshabach,

15
00:00:35,920 --> 00:00:39,760
my colleague and the principal engineer at Intel Labs.

16
00:00:39,760 --> 00:00:41,360
My name is Tania Greenberg.

17
00:00:41,360 --> 00:00:44,640
I am an AI scientist at Intel Labs also.

18
00:00:44,640 --> 00:00:46,960
We would like to express our thanks to Intel Labs

19
00:00:46,960 --> 00:00:50,400
for sponsoring and hosting this event series.

20
00:00:50,400 --> 00:00:53,840
So without further ado, let's get started.

21
00:00:53,840 --> 00:00:56,720
The audience is going to be muted by default for this event.

22
00:00:56,720 --> 00:00:59,400
If you would like to ask a question or make a comment,

23
00:00:59,400 --> 00:01:01,560
please post your thoughts in the chat.

24
00:01:01,560 --> 00:01:03,480
You should have access to it.

25
00:01:03,480 --> 00:01:06,480
We will have a hopefully longer than usual Q&A session

26
00:01:06,480 --> 00:01:07,800
at the end of today's session.

27
00:01:07,800 --> 00:01:10,920
So we should be able to go through many of your questions.

28
00:01:10,920 --> 00:01:13,400
All right, Yoshab, the floor is yours.

29
00:01:14,800 --> 00:01:18,400
Okay, I proceed with our slides for now.

30
00:01:18,400 --> 00:01:19,240
Let's see.

31
00:01:20,080 --> 00:01:22,360
The question that we want to discuss

32
00:01:22,360 --> 00:01:25,080
is how we can build intelligent systems

33
00:01:25,080 --> 00:01:30,200
that move beyond the present limitations of deep learning.

34
00:01:30,200 --> 00:01:32,440
And this is quite a bold proposal

35
00:01:32,440 --> 00:01:35,080
because you don't know if deep learning has any limitations.

36
00:01:35,080 --> 00:01:36,160
At least I don't know.

37
00:01:36,160 --> 00:01:39,160
I don't know any proof that demonstrates

38
00:01:39,160 --> 00:01:40,720
that there is a certain thing

39
00:01:40,720 --> 00:01:43,200
that the methods of deep learning cannot do.

40
00:01:43,200 --> 00:01:46,200
It seems to be that the present generation of models,

41
00:01:46,200 --> 00:01:48,440
which is widely successful,

42
00:01:48,440 --> 00:01:51,360
generative AI is built on the transformer algorithm,

43
00:01:51,360 --> 00:01:54,800
which is a variant of the deep learning mechanisms

44
00:01:54,800 --> 00:01:58,160
that have been existing since the perceptron.

45
00:01:58,160 --> 00:02:02,560
And the limitations of this thing are not clear, right?

46
00:02:02,560 --> 00:02:04,920
We basically get to meaning in the limit.

47
00:02:04,920 --> 00:02:08,720
We need to use massive data and compute to get there.

48
00:02:08,720 --> 00:02:11,080
And these models have difficulty

49
00:02:11,080 --> 00:02:12,640
to become fully coherent,

50
00:02:12,640 --> 00:02:14,920
but it's not obvious that such a limitation

51
00:02:14,920 --> 00:02:16,840
cannot overcome as a loss function

52
00:02:16,840 --> 00:02:19,720
or this just using the existing regimes

53
00:02:19,720 --> 00:02:22,080
and scaling them up even further, right?

54
00:02:22,080 --> 00:02:26,080
We cannot prove that there's a combination of codecs

55
00:02:26,080 --> 00:02:28,320
and the existing methods and some real time

56
00:02:28,320 --> 00:02:30,240
and some online learning.

57
00:02:30,240 --> 00:02:31,720
We won't be able to build a system

58
00:02:31,720 --> 00:02:34,840
that is a good enough artificial general intelligence

59
00:02:34,840 --> 00:02:36,920
and is able to discover the next generation

60
00:02:36,920 --> 00:02:38,760
of algorithms for us.

61
00:02:38,760 --> 00:02:40,840
And this idea that deep learning

62
00:02:40,840 --> 00:02:42,320
in the way that it exists

63
00:02:42,320 --> 00:02:44,920
with basically the algorithms that we have right now,

64
00:02:44,920 --> 00:02:47,600
with slight changes to the loss functions and so on,

65
00:02:47,600 --> 00:02:49,440
that it is sufficient.

66
00:02:49,440 --> 00:02:51,960
This is called the scaling hypothesis.

67
00:02:52,640 --> 00:02:55,760
On the other hand, most people who work in the field

68
00:02:55,760 --> 00:03:00,360
have the sense that there is a problem with deep learning

69
00:03:00,360 --> 00:03:02,840
in the sense that it's boot forcing the job.

70
00:03:02,840 --> 00:03:05,160
It seems that biological organisms

71
00:03:05,160 --> 00:03:06,720
are able to make sense of the world

72
00:03:06,720 --> 00:03:10,680
with much, much less data and dramatically less compute.

73
00:03:10,680 --> 00:03:13,240
And there is of course a contention

74
00:03:13,240 --> 00:03:16,240
about how much compute a brain has

75
00:03:16,240 --> 00:03:18,760
and how much compute an organism has.

76
00:03:18,760 --> 00:03:21,720
And very often the calculation is made in such a way

77
00:03:21,720 --> 00:03:25,800
that we discuss how many GPUs you need

78
00:03:25,800 --> 00:03:28,560
to emulate a group of neurons.

79
00:03:28,560 --> 00:03:31,000
And depending on how you look at the neuron,

80
00:03:31,000 --> 00:03:34,560
the neuron might be something like a four or 12 layer

81
00:03:34,560 --> 00:03:38,320
neural network or it might be something even more complicated

82
00:03:38,320 --> 00:03:41,760
if you look down to modeling every synapse and so on.

83
00:03:41,760 --> 00:03:45,120
And on the other hand, relatively rarely we ask

84
00:03:45,120 --> 00:03:47,840
how many brains you would need to run macOS

85
00:03:47,840 --> 00:03:50,080
because the way our nervous system works

86
00:03:50,080 --> 00:03:53,120
is highly redundant and stochastic.

87
00:03:53,120 --> 00:03:55,760
And much of what the individual neuron is doing

88
00:03:55,760 --> 00:03:58,480
is probably only relevant to the individual neuron

89
00:03:58,480 --> 00:03:59,880
in its survival itself,

90
00:03:59,880 --> 00:04:03,000
because after all the neuron is a single celled animal.

91
00:04:03,000 --> 00:04:07,640
So if you take the job of an individual,

92
00:04:07,640 --> 00:04:09,360
for instance in a corporation,

93
00:04:09,360 --> 00:04:12,280
that individual is going to contribute

94
00:04:12,280 --> 00:04:14,960
a lot of its intellectual capability to the corporation,

95
00:04:14,960 --> 00:04:17,720
but it's going to be a tiny fraction

96
00:04:17,720 --> 00:04:20,000
of the total ability of the individual

97
00:04:20,000 --> 00:04:21,720
that it needs to survive by itself

98
00:04:21,720 --> 00:04:24,240
and to communicate with its immediate neighborhood.

99
00:04:24,240 --> 00:04:28,840
So just asking to emulate a single neuron

100
00:04:28,840 --> 00:04:31,600
and then multiplying this with the number of neurons

101
00:04:31,600 --> 00:04:33,720
is probably not the right way to do it.

102
00:04:33,720 --> 00:04:35,480
And it's not clear how many neurons

103
00:04:35,480 --> 00:04:37,240
you would need to run MNIST.

104
00:04:37,240 --> 00:04:40,920
As far as I know, there is no simulation so far

105
00:04:40,920 --> 00:04:43,320
that is using a close neural model

106
00:04:43,320 --> 00:04:47,440
or that is using a small group of neurons in vivo

107
00:04:47,440 --> 00:04:50,600
that are being trained to do MNIST.

108
00:04:52,280 --> 00:04:55,480
More generally, I think we need to answer the question

109
00:04:55,480 --> 00:04:56,520
what intelligence is.

110
00:04:56,520 --> 00:04:59,800
And for me, this question has evolved over the years

111
00:04:59,800 --> 00:05:03,800
and my current answer is that intelligence is the ability

112
00:05:03,800 --> 00:05:07,320
to construct a path through a space of computable functions.

113
00:05:08,560 --> 00:05:12,200
It's a slightly fancy way of saying

114
00:05:12,200 --> 00:05:14,760
that intelligence is the ability to make models

115
00:05:14,760 --> 00:05:16,920
because at the end of the day,

116
00:05:16,920 --> 00:05:19,040
models are computable functions

117
00:05:19,040 --> 00:05:24,120
that are designed in the service of control.

118
00:05:24,120 --> 00:05:27,920
So we have a system that is making some kind

119
00:05:27,920 --> 00:05:29,360
of regulation task

120
00:05:29,360 --> 00:05:31,520
or performing some kind of regulation tasks

121
00:05:31,520 --> 00:05:34,840
and when it's controlling the future, it's an agent.

122
00:05:34,840 --> 00:05:36,960
And to control the future and being an agent,

123
00:05:36,960 --> 00:05:40,200
you will need to construct some kind of control model

124
00:05:40,200 --> 00:05:41,240
that is counterfactual.

125
00:05:41,240 --> 00:05:43,320
You need to be able to represent states

126
00:05:43,320 --> 00:05:44,240
that are not here yet

127
00:05:44,240 --> 00:05:46,160
because the future is not there.

128
00:05:46,160 --> 00:05:48,480
And that means you need to have a system

129
00:05:48,480 --> 00:05:52,080
that is able to perform arbitrary causal transitions.

130
00:05:52,080 --> 00:05:54,600
And this causal insulation from the substrate

131
00:05:54,600 --> 00:05:57,440
to represent something that is not the case yet,

132
00:05:57,440 --> 00:06:00,640
that is basically what is a computer.

133
00:06:00,640 --> 00:06:03,880
And for the computer it does, it represents functions

134
00:06:03,880 --> 00:06:06,720
which means mapping from state descriptions

135
00:06:06,720 --> 00:06:08,640
to other state descriptions.

136
00:06:08,640 --> 00:06:12,880
And the computer can do this in completely arbitrary ways.

137
00:06:12,920 --> 00:06:14,720
And this is the basic idea

138
00:06:14,720 --> 00:06:18,160
of building a system of computable functions.

139
00:06:18,160 --> 00:06:21,440
And when you come up with a way

140
00:06:21,440 --> 00:06:23,320
to enumerate the computable functions,

141
00:06:23,320 --> 00:06:26,040
to list them all, then you can search them.

142
00:06:26,040 --> 00:06:28,200
And if you want to learn something,

143
00:06:28,200 --> 00:06:30,920
you need to in some sense enumerate,

144
00:06:30,920 --> 00:06:33,160
organize the space of computable functions

145
00:06:33,160 --> 00:06:35,280
in such a way that you find those functions

146
00:06:35,280 --> 00:06:38,200
that you are interested relatively early on.

147
00:06:38,200 --> 00:06:41,560
And this question of how you can construct a space

148
00:06:41,560 --> 00:06:43,440
for the computable functions that is tractable,

149
00:06:43,440 --> 00:06:46,160
that is able to converge to a solution

150
00:06:46,160 --> 00:06:47,560
for the problem at hand.

151
00:06:47,560 --> 00:06:48,760
That is the main issue, right?

152
00:06:48,760 --> 00:06:51,080
Making computer is relatively easy,

153
00:06:51,080 --> 00:06:53,040
especially if you have something like a cell

154
00:06:53,040 --> 00:06:55,040
because it's already contains a computer

155
00:06:55,040 --> 00:06:56,920
and they can also organize themselves

156
00:06:56,920 --> 00:06:58,680
into higher level computers

157
00:06:58,680 --> 00:07:02,920
and become less stochastic and so on while doing this.

158
00:07:02,920 --> 00:07:05,160
But the big difficulty is,

159
00:07:05,160 --> 00:07:09,520
how can we find the functions that we are interested in?

160
00:07:09,520 --> 00:07:11,200
And so when we look at deep learning,

161
00:07:11,200 --> 00:07:12,960
what is deep learning?

162
00:07:12,960 --> 00:07:15,080
The first thing that we have to note about deep learning

163
00:07:15,080 --> 00:07:17,520
is that deep learning is the only thing

164
00:07:17,520 --> 00:07:19,800
that currently works at scale.

165
00:07:19,800 --> 00:07:21,800
It's the only class of algorithms

166
00:07:21,800 --> 00:07:25,720
that is able to discover arbitrary functions

167
00:07:25,720 --> 00:07:28,040
in a reasonable amount of time.

168
00:07:28,040 --> 00:07:32,400
And reasonable being orders of magnitude more time

169
00:07:32,400 --> 00:07:35,720
and more data than a human being does.

170
00:07:35,720 --> 00:07:38,200
That's of course, the training time for something

171
00:07:38,200 --> 00:07:43,200
like diffusion stability AI model, right?

172
00:07:43,680 --> 00:07:46,680
This stable diffusion is a model of two gigabytes

173
00:07:46,680 --> 00:07:50,560
and it contains the whole of the art

174
00:07:50,560 --> 00:07:53,520
and you can get every celebrity you want,

175
00:07:53,520 --> 00:07:57,080
you can get every spaceship from popular culture,

176
00:07:57,080 --> 00:07:58,520
you can get dinosaurs,

177
00:07:58,520 --> 00:08:00,800
everything is in these two gigabytes.

178
00:08:00,800 --> 00:08:04,240
And training this takes weeks.

179
00:08:04,240 --> 00:08:07,360
And it sounds that's very little compared

180
00:08:07,360 --> 00:08:10,480
to the years that it takes to train a human brain.

181
00:08:10,480 --> 00:08:15,480
But during these weeks and now it comes down closer to days,

182
00:08:16,640 --> 00:08:21,040
this thing is going through hundreds of millions of images.

183
00:08:21,040 --> 00:08:22,960
Many more than a human being could process

184
00:08:22,960 --> 00:08:25,560
and the lifetimes and find correlations between

185
00:08:25,560 --> 00:08:28,600
using dramatically large server farms.

186
00:08:29,880 --> 00:08:32,080
So how does deep learning work?

187
00:08:32,080 --> 00:08:34,720
First of all, it's differentiable computing,

188
00:08:34,720 --> 00:08:37,200
which means it is representing all the functions

189
00:08:37,200 --> 00:08:39,680
in such a way that they form a continuous space

190
00:08:39,680 --> 00:08:43,440
in which neighboring variations of the functions

191
00:08:43,440 --> 00:08:45,640
still lead to interesting results.

192
00:08:45,640 --> 00:08:48,960
And you can move by a small nudges through the space

193
00:08:48,960 --> 00:08:51,840
of functions to get the function closer to what you want.

194
00:08:51,840 --> 00:08:54,240
That's different from discrete programs,

195
00:08:54,240 --> 00:08:55,600
program and source code.

196
00:08:55,600 --> 00:08:57,400
If you change a few bytes in the source code,

197
00:08:57,400 --> 00:08:59,120
the program will no longer work.

198
00:08:59,120 --> 00:09:02,560
If you change the neural network by a slight bit,

199
00:09:02,560 --> 00:09:05,440
then it's still going to give you useful results.

200
00:09:06,440 --> 00:09:09,600
And to do this, the neural network is describing

201
00:09:09,600 --> 00:09:14,600
the functions via weighted sums of inputs

202
00:09:15,400 --> 00:09:18,880
that are arranged into chains, basically in layers.

203
00:09:18,880 --> 00:09:21,360
And some non-linearities, which are in a way to make

204
00:09:21,360 --> 00:09:25,080
if sense, to make conditional breaks in this network.

205
00:09:25,080 --> 00:09:29,120
And this description via multiplications and sums

206
00:09:29,120 --> 00:09:32,280
is sufficient to represent all the functions that we want.

207
00:09:32,280 --> 00:09:34,840
And we train this network by setting it up

208
00:09:34,840 --> 00:09:38,080
in such a way that it has the potential to build enough things

209
00:09:38,080 --> 00:09:40,040
that has enough links between the nodes

210
00:09:40,040 --> 00:09:41,840
that it sums values up.

211
00:09:41,840 --> 00:09:43,720
And then it just changes the weights,

212
00:09:43,720 --> 00:09:46,440
which means the multiplying factors

213
00:09:46,440 --> 00:09:50,320
for the inputs of every node in a systematic way.

214
00:09:50,320 --> 00:09:53,480
And it, in some sense approximates all the functions

215
00:09:53,480 --> 00:09:54,560
via real numbers.

216
00:09:55,480 --> 00:09:57,920
And if you think about alternatives to deep learning,

217
00:09:57,920 --> 00:10:02,240
the main thing that comes to mind is to build up computations

218
00:10:02,240 --> 00:10:04,800
from deterministic discrete operators.

219
00:10:04,800 --> 00:10:09,000
Such as Boolean logic or simple automata,

220
00:10:09,000 --> 00:10:10,840
like cellular automata.

221
00:10:10,840 --> 00:10:13,880
And as a result, we get finite Turing machines.

222
00:10:15,160 --> 00:10:16,880
And in the finite case,

223
00:10:16,880 --> 00:10:19,640
dynamical systems and Turing machines

224
00:10:19,640 --> 00:10:23,040
and automata are the same thing, computationally speaking.

225
00:10:23,040 --> 00:10:25,000
They're all Turing machines,

226
00:10:25,000 --> 00:10:28,200
as long as you don't run out of resources.

227
00:10:28,200 --> 00:10:32,120
So every digital computer in reality is a dynamical system

228
00:10:32,120 --> 00:10:34,240
because the physics that digital computers

229
00:10:34,240 --> 00:10:36,880
and prevains on is somewhat continuous

230
00:10:36,880 --> 00:10:39,680
from the perspective of the individual transistors

231
00:10:39,680 --> 00:10:40,520
and so on.

232
00:10:40,520 --> 00:10:43,240
And we just try to find a region in physics

233
00:10:43,240 --> 00:10:45,200
that makes the transistor reliable enough

234
00:10:45,200 --> 00:10:47,560
as a discrete unit and deterministic

235
00:10:47,560 --> 00:10:50,040
from the perspective of the logical language

236
00:10:50,040 --> 00:10:53,000
that is implemented on the arrangement of transistors.

237
00:10:53,000 --> 00:10:55,800
But underneath, there is an analog system.

238
00:10:55,800 --> 00:10:57,080
It is just noisy.

239
00:10:57,080 --> 00:11:00,720
And that is a system that is discrete.

240
00:11:00,720 --> 00:11:03,840
On the other hand, every dynamical systems in physics

241
00:11:03,840 --> 00:11:06,000
at the lowest level is discrete again.

242
00:11:06,000 --> 00:11:09,000
If you zoom in, what you see is nothing continuous.

243
00:11:09,000 --> 00:11:11,040
What you see is individual atoms

244
00:11:11,040 --> 00:11:13,200
and individual charges and so on.

245
00:11:13,200 --> 00:11:15,440
And they are discrete again.

246
00:11:15,440 --> 00:11:17,960
And there is some discussion about

247
00:11:17,960 --> 00:11:20,200
whether everything has to be discrete at the level

248
00:11:20,200 --> 00:11:22,080
because of the nature of languages itself.

249
00:11:22,080 --> 00:11:23,680
And I think that's the case.

250
00:11:23,680 --> 00:11:27,280
I think that the discovery of the last century

251
00:11:27,280 --> 00:11:28,960
that was most important in philosophy

252
00:11:28,960 --> 00:11:31,840
is that those languages which assume

253
00:11:31,880 --> 00:11:34,600
that the bottom most layer can be truly continuous.

254
00:11:34,600 --> 00:11:36,760
They run into contradictions.

255
00:11:36,760 --> 00:11:38,960
But this only matters if you are really interested

256
00:11:38,960 --> 00:11:41,360
in modeling the bottom most layer.

257
00:11:41,360 --> 00:11:44,040
If you just think about computation,

258
00:11:44,040 --> 00:11:45,720
it doesn't really matter if you start out

259
00:11:45,720 --> 00:11:49,400
with a dynamical system or this discrete system

260
00:11:49,400 --> 00:11:52,000
as long as you are willing to allow

261
00:11:52,000 --> 00:11:55,000
that every dynamical system has only finite resolution.

262
00:11:55,000 --> 00:11:57,320
So there's only finitely many bits

263
00:11:57,320 --> 00:12:00,360
that you can manipulate at any given moment.

264
00:12:01,880 --> 00:12:06,400
And this equivalence between the continuous mathematics

265
00:12:06,400 --> 00:12:07,560
and the discrete mathematics

266
00:12:07,560 --> 00:12:09,800
has been shown basically in both directions.

267
00:12:09,800 --> 00:12:11,400
You can use a computer.

268
00:12:11,400 --> 00:12:13,080
And then just by using more bits,

269
00:12:13,080 --> 00:12:15,440
you can approximate continuous system

270
00:12:15,440 --> 00:12:17,640
with any degree of fidelity as you want

271
00:12:17,640 --> 00:12:20,000
in the same way as digitized music

272
00:12:20,000 --> 00:12:23,600
can sample the space of audio functions

273
00:12:23,600 --> 00:12:26,680
below the level of resolution

274
00:12:26,680 --> 00:12:29,600
that your medium can provide.

275
00:12:29,640 --> 00:12:32,240
So you will get to something that is equivalent.

276
00:12:32,240 --> 00:12:35,960
And the opposite direction has also been shown.

277
00:12:35,960 --> 00:12:38,080
Greg Chaitin has gone to the trouble

278
00:12:38,080 --> 00:12:40,640
to translate a LISP interpreter

279
00:12:40,640 --> 00:12:45,640
into a diophantine equation with 17,000 variables.

280
00:12:45,720 --> 00:12:47,320
Right, so this paper is called

281
00:12:47,320 --> 00:12:49,920
the Complete Arithmeticization of Evil.

282
00:12:49,920 --> 00:12:52,840
It's the evaluation function of LISP,

283
00:12:52,840 --> 00:12:57,040
which did in 1987, it's quite beautiful.

284
00:12:57,040 --> 00:12:58,600
And I don't think that is something

285
00:12:58,600 --> 00:13:00,040
people actually want to read.

286
00:13:00,040 --> 00:13:03,880
And this formula was generated

287
00:13:03,880 --> 00:13:05,840
with some generative procedure.

288
00:13:05,840 --> 00:13:09,000
He did not write these 900,000 characters by hand

289
00:13:09,000 --> 00:13:10,280
that went into this.

290
00:13:10,280 --> 00:13:12,720
But it's been shown, you can write down

291
00:13:12,720 --> 00:13:16,000
the LISP interpreter in continuous mathematics.

292
00:13:17,040 --> 00:13:18,960
So the alternative to deep learning

293
00:13:18,960 --> 00:13:21,440
might be to basically construct functions

294
00:13:21,440 --> 00:13:24,280
from the ground up using automata.

295
00:13:24,280 --> 00:13:28,360
And in a way, this is also what people have done

296
00:13:28,480 --> 00:13:31,760
because all our computers on which we run deep learning

297
00:13:31,760 --> 00:13:33,160
are discrete automata.

298
00:13:34,040 --> 00:13:36,680
People started to build this continuous arithmetic,

299
00:13:36,680 --> 00:13:41,480
they built end or addition, multiplication and so on

300
00:13:41,480 --> 00:13:43,920
from discrete logical units.

301
00:13:43,920 --> 00:13:45,080
And this is clearly practical,

302
00:13:45,080 --> 00:13:47,320
but it was done with relatively few people

303
00:13:47,320 --> 00:13:49,440
and relatively few years.

304
00:13:49,440 --> 00:13:53,160
The search process to build up continuous arithmetic

305
00:13:53,160 --> 00:13:56,240
from discrete logical operations is not very large.

306
00:13:56,240 --> 00:13:59,840
And if you want to get, to discover a deep learning

307
00:13:59,840 --> 00:14:02,880
algorithm as a special case over discrete automata

308
00:14:02,880 --> 00:14:06,280
from scratch, it's the existence proof

309
00:14:06,280 --> 00:14:07,920
has been shown by the computers

310
00:14:07,920 --> 00:14:09,920
and the deep learning mechanisms that we have.

311
00:14:09,920 --> 00:14:13,640
Because relatively few people use relatively little brain power

312
00:14:13,640 --> 00:14:15,520
compared to where you want to get to

313
00:14:15,520 --> 00:14:17,840
to discover all these solutions.

314
00:14:17,840 --> 00:14:20,080
By self-play, you could in the same way

315
00:14:20,080 --> 00:14:23,600
as computers played a goal, discover arithmetic

316
00:14:23,600 --> 00:14:26,000
but using discrete systems.

317
00:14:26,000 --> 00:14:28,640
So in many ways, this is going to be equivalent.

318
00:14:28,640 --> 00:14:30,800
And I suspect it might be interesting to start

319
00:14:30,800 --> 00:14:34,000
from this automata direction again and build up learning.

320
00:14:34,000 --> 00:14:36,320
And there are a few people which work in this area

321
00:14:36,320 --> 00:14:39,760
but so far nobody has come up with an alternative

322
00:14:39,760 --> 00:14:42,920
that scales up in the same way as deep learning.

323
00:14:42,920 --> 00:14:45,840
And then there is a slight, the different approach

324
00:14:45,840 --> 00:14:46,880
that we might be looking at.

325
00:14:46,880 --> 00:14:49,800
And it doesn't use a normal Turing machine

326
00:14:49,800 --> 00:14:51,840
but a non-deterministic Turing machine,

327
00:14:51,840 --> 00:14:53,720
a multi-phase system.

328
00:14:53,720 --> 00:14:58,440
And that is because systems in the lowest level of physics

329
00:14:58,440 --> 00:15:01,120
and I also suspect systems that are implemented

330
00:15:01,120 --> 00:15:04,960
on brains, non-deterministic systems.

331
00:15:04,960 --> 00:15:06,600
And this doesn't just mean that they're noisy

332
00:15:06,600 --> 00:15:08,920
or that they're random in many ways.

333
00:15:08,920 --> 00:15:12,360
A non-deterministic Turing machine is a paradigm

334
00:15:12,360 --> 00:15:17,080
from computer science that describes your state machine

335
00:15:17,080 --> 00:15:20,000
in such a way that not every state

336
00:15:20,000 --> 00:15:21,920
has exactly one successor state.

337
00:15:21,920 --> 00:15:23,960
It's not sufficiently constrained

338
00:15:23,960 --> 00:15:26,120
to have only one successor state.

339
00:15:26,120 --> 00:15:29,080
Our Turing machine that we normally use

340
00:15:29,080 --> 00:15:31,440
and this includes our digital von Neumann computers

341
00:15:31,440 --> 00:15:33,080
and so on, they're defined in such a way

342
00:15:33,080 --> 00:15:38,080
that every state has exactly one possible successor state.

343
00:15:38,320 --> 00:15:39,840
If there is a branch in the computer

344
00:15:39,840 --> 00:15:41,720
that's because it is depending

345
00:15:41,720 --> 00:15:43,480
on some environmental variable

346
00:15:43,480 --> 00:15:45,640
that is not relevant in the program

347
00:15:45,640 --> 00:15:47,640
which means some input of the program.

348
00:15:47,640 --> 00:15:49,480
But given the same input,

349
00:15:49,480 --> 00:15:52,600
the Turing machine is going to produce the same output

350
00:15:52,600 --> 00:15:56,200
and it's going to do this along exactly one path.

351
00:15:56,200 --> 00:15:59,800
And not just Turing machine, the computational sense

352
00:15:59,800 --> 00:16:01,960
is also going to give you the same output

353
00:16:01,960 --> 00:16:03,880
but it's going to do this on many paths

354
00:16:03,880 --> 00:16:06,760
because the constraints are not so narrow

355
00:16:06,760 --> 00:16:09,400
that you go into one state after every state

356
00:16:09,400 --> 00:16:11,840
but you can go into multiple states.

357
00:16:11,840 --> 00:16:13,440
And the non-deterministic Turing machine

358
00:16:13,440 --> 00:16:16,040
just goes into all of them branches.

359
00:16:16,040 --> 00:16:18,360
And these branches don't necessarily meet again, right?

360
00:16:18,360 --> 00:16:20,080
There is no way that they're connected again.

361
00:16:20,080 --> 00:16:22,280
It's basically a non-deterministic Turing machine

362
00:16:22,280 --> 00:16:25,240
is implementing some kind of multiverse.

363
00:16:25,240 --> 00:16:28,520
But it turns out that many of these multiverse branches

364
00:16:28,520 --> 00:16:30,880
that the non-deterministic Turing machine goes into

365
00:16:30,880 --> 00:16:32,880
have the same bit combination in them

366
00:16:32,880 --> 00:16:36,040
because there are only so many states that are reachable.

367
00:16:36,040 --> 00:16:38,520
And so by looking at the dynamics between these bits,

368
00:16:38,520 --> 00:16:41,960
you can get to the same point in the universe

369
00:16:41,960 --> 00:16:43,720
on multiple paths.

370
00:16:43,720 --> 00:16:46,200
And if the universe that we live in,

371
00:16:46,200 --> 00:16:47,720
the physical universe that we live in

372
00:16:47,720 --> 00:16:49,440
is such a multivase system,

373
00:16:49,440 --> 00:16:53,240
it's still going to be possible for observer

374
00:16:53,240 --> 00:16:54,480
that lives inside of it,

375
00:16:54,480 --> 00:16:57,200
despite it not knowing which path it goes down,

376
00:16:57,200 --> 00:17:01,000
determine statistical properties over the regional paths

377
00:17:01,000 --> 00:17:03,880
which means you cannot know in your universe

378
00:17:03,880 --> 00:17:08,120
which slit the double slit experiment the photon goes through

379
00:17:08,120 --> 00:17:09,920
but you can predict the patterns

380
00:17:09,920 --> 00:17:12,920
that many, many photons are going to make on the other side.

381
00:17:12,920 --> 00:17:15,200
And the things that we can predict in our universe

382
00:17:15,200 --> 00:17:16,840
are of that nature.

383
00:17:16,920 --> 00:17:19,160
There are statistical properties

384
00:17:19,160 --> 00:17:21,840
over all the trajectories that can happen

385
00:17:21,840 --> 00:17:24,800
that we are a part of.

386
00:17:24,800 --> 00:17:27,280
And how would something like this look in the brain?

387
00:17:27,280 --> 00:17:30,560
Like in the brain, obviously the brain can only hold

388
00:17:30,560 --> 00:17:32,320
a finite amount of state.

389
00:17:32,320 --> 00:17:34,520
But if you have redundancy, what you can do is

390
00:17:34,520 --> 00:17:38,320
if you don't tell the neuron to go into one particular state

391
00:17:38,320 --> 00:17:41,280
as a result of its present state

392
00:17:41,280 --> 00:17:44,280
but into a range of possible states.

393
00:17:44,280 --> 00:17:46,760
And this happens randomly.

394
00:17:46,760 --> 00:17:49,080
It means that there are many trajectories

395
00:17:49,080 --> 00:17:52,400
that activations can take in the brain at the same time.

396
00:17:52,400 --> 00:17:54,440
And they're going to meet on the same physical substrate

397
00:17:54,440 --> 00:17:55,640
and they're going to accumulate

398
00:17:55,640 --> 00:17:57,760
and you're going to sum up in some sense

399
00:17:57,760 --> 00:18:01,280
and could be some basically some voting over the different paths.

400
00:18:01,280 --> 00:18:04,120
And this means that a system like the brain

401
00:18:04,120 --> 00:18:05,920
could via transmitting activation

402
00:18:05,920 --> 00:18:08,440
try many, many things in parallel

403
00:18:08,440 --> 00:18:12,160
and stochastically with some degree of randomness.

404
00:18:12,160 --> 00:18:16,000
And as a result perform computations efficiently

405
00:18:16,000 --> 00:18:19,480
that can not be efficiently done with a linear machine.

406
00:18:19,480 --> 00:18:20,960
From a computational perspective,

407
00:18:20,960 --> 00:18:23,480
if you want to simulate this on a linearly

408
00:18:23,480 --> 00:18:25,240
on a sequentially operating machine,

409
00:18:25,240 --> 00:18:26,520
it's going to be highly inefficient

410
00:18:26,520 --> 00:18:28,440
because you're trying to do the same thing

411
00:18:28,440 --> 00:18:29,960
over and over again.

412
00:18:29,960 --> 00:18:32,560
And this randomness is going to delete some of the bits

413
00:18:32,560 --> 00:18:34,400
that you've been computing.

414
00:18:34,400 --> 00:18:37,280
But if your computational units are very cheap,

415
00:18:37,280 --> 00:18:39,920
like in the brain, in the brain time is expensive

416
00:18:39,920 --> 00:18:41,920
because taking one more step means

417
00:18:41,920 --> 00:18:42,920
that you're going to be slower

418
00:18:42,920 --> 00:18:44,800
in your interaction with nature.

419
00:18:44,800 --> 00:18:46,320
But parallelism is cheap.

420
00:18:46,320 --> 00:18:50,160
Just by doubling the cell count once more

421
00:18:50,160 --> 00:18:51,800
by dividing the brain cells,

422
00:18:51,800 --> 00:18:53,680
you get twice as many elements.

423
00:18:53,680 --> 00:18:55,440
If you do this quite a few times,

424
00:18:55,440 --> 00:18:57,600
you end up with billions of elements.

425
00:18:57,600 --> 00:19:00,240
And these billions of elements can perform

426
00:19:00,240 --> 00:19:02,560
many of these paths in parallel.

427
00:19:02,560 --> 00:19:04,360
And this is something that to my knowledge

428
00:19:04,360 --> 00:19:06,360
has not been seriously attempted yet

429
00:19:06,360 --> 00:19:08,360
to get to work for a learning system.

430
00:19:08,360 --> 00:19:10,680
There are some people which are thinking seriously about this.

431
00:19:10,680 --> 00:19:13,800
For instance, we had Jerome Buzemeier on a panel here.

432
00:19:13,800 --> 00:19:17,040
And he is describing the mental representations

433
00:19:17,040 --> 00:19:20,800
as superpositional states using such a multivase system.

434
00:19:20,800 --> 00:19:23,320
And it's also an idea that has occurred to Steven Wolfram

435
00:19:23,320 --> 00:19:26,960
who is thinking of the universe as a multivase system

436
00:19:26,960 --> 00:19:28,640
and who's open to the idea

437
00:19:28,640 --> 00:19:30,920
that the brain might be basically

438
00:19:30,920 --> 00:19:33,160
a bounded complexity multivase system.

439
00:19:34,920 --> 00:19:38,920
So this, we see that there are opportunities

440
00:19:38,920 --> 00:19:43,640
to build solutions that are inspired by biology,

441
00:19:43,640 --> 00:19:45,440
that we haven't tried yet.

442
00:19:45,440 --> 00:19:48,320
And if you look at the inspiration that happens

443
00:19:48,320 --> 00:19:51,200
so far from biology into artificial intelligence,

444
00:19:51,200 --> 00:19:53,680
despite many claims to the contrary,

445
00:19:53,680 --> 00:19:55,480
it almost never happens.

446
00:19:55,480 --> 00:19:58,560
I think that the last contribution of biology

447
00:19:58,560 --> 00:20:01,960
to the transformer was heavy on learning.

448
00:20:01,960 --> 00:20:05,680
And everything else that people got to work since then

449
00:20:05,680 --> 00:20:06,960
was mostly not happening

450
00:20:06,960 --> 00:20:09,200
because people looked at newer science results

451
00:20:09,200 --> 00:20:10,240
and implemented them

452
00:20:10,240 --> 00:20:13,760
and then ended up with a better machine learning algorithm.

453
00:20:13,760 --> 00:20:16,680
And if you take the ideas that newer scientists currently have

454
00:20:16,680 --> 00:20:18,800
about how the brain works, to my knowledge,

455
00:20:18,800 --> 00:20:20,520
you cannot implement them in such a way

456
00:20:20,520 --> 00:20:22,680
that they learn and control an organism.

457
00:20:22,680 --> 00:20:25,600
Even a very simple organism like C elegance,

458
00:20:25,600 --> 00:20:27,600
if we take the conic term of C elegance

459
00:20:27,600 --> 00:20:30,960
and translate this into a computer model and run it,

460
00:20:30,960 --> 00:20:33,920
it's not going to produce coherent warm behavior.

461
00:20:33,920 --> 00:20:36,040
C elegance is a small warm

462
00:20:36,040 --> 00:20:40,200
and it has only 300, I think 309 neurons.

463
00:20:40,200 --> 00:20:44,040
Please correct me if this number is off.

464
00:20:44,040 --> 00:20:47,120
And as a result, if you take this conic term

465
00:20:47,120 --> 00:20:49,600
and run it into a digital simulation,

466
00:20:49,600 --> 00:20:53,720
it's not going to produce the behavior that we want

467
00:20:53,720 --> 00:20:57,840
because presumably we have not caught up

468
00:20:57,840 --> 00:21:00,120
on all the functionality of the individual neurons

469
00:21:00,120 --> 00:21:01,760
in the context of that warm.

470
00:21:01,760 --> 00:21:03,720
Could be that there is stuff in the soma of the cells

471
00:21:03,720 --> 00:21:06,640
that is not visible in the conic term

472
00:21:06,640 --> 00:21:10,240
or that we made mistakes in digitizing the conic term

473
00:21:10,240 --> 00:21:13,200
or I don't have enough resolution to see all the vesicles

474
00:21:13,200 --> 00:21:15,760
that use different neurotransmitters in the conic term

475
00:21:15,760 --> 00:21:17,520
to get the functionality right.

476
00:21:17,520 --> 00:21:21,600
But for systems at scale that go beyond an organism

477
00:21:21,600 --> 00:21:26,600
with a few hundred neurons, something like cortical column

478
00:21:26,640 --> 00:21:28,480
and so on, we don't really have working models

479
00:21:28,480 --> 00:21:29,320
at the moment.

480
00:21:29,320 --> 00:21:33,640
The descriptions that the neurobiologists have at the moment,

481
00:21:33,640 --> 00:21:35,840
if you translate them into computer models,

482
00:21:35,840 --> 00:21:37,680
are not performing the same things

483
00:21:37,680 --> 00:21:39,680
as our digital models are doing.

484
00:21:39,680 --> 00:21:42,120
They're not able to discover these functions

485
00:21:42,120 --> 00:21:45,240
because these models are still incomplete

486
00:21:45,240 --> 00:21:47,640
and they're not ready yet for being used.

487
00:21:47,640 --> 00:21:49,160
There's a more fundamental problem

488
00:21:49,160 --> 00:21:52,680
that Konrad Hauding has highlighted in the paper

489
00:21:52,680 --> 00:21:54,920
where he used the methods of neuroscientists

490
00:21:54,920 --> 00:21:59,480
to reverse and engineer a microprocessor.

491
00:21:59,480 --> 00:22:01,200
If you take a normal microprocessor

492
00:22:01,200 --> 00:22:02,800
and give this to a neuroscientist,

493
00:22:02,800 --> 00:22:04,960
the neuroscientist does ablation studies

494
00:22:04,960 --> 00:22:08,120
and looks at the structure that the neuroscientist finds.

495
00:22:08,120 --> 00:22:10,840
Is the neuroscientist able to reverse engineer

496
00:22:10,840 --> 00:22:13,280
this microprocessor which is dramatically simpler

497
00:22:13,280 --> 00:22:15,120
than the nervous system, of course.

498
00:22:15,120 --> 00:22:19,200
And it turns out that Konrad Hauding

499
00:22:19,200 --> 00:22:21,520
who is a neuroscientist that is the methods

500
00:22:21,520 --> 00:22:25,760
of neuroscience might be for some more deep reason

501
00:22:25,760 --> 00:22:27,600
because they're not functionalist enough,

502
00:22:27,600 --> 00:22:31,160
able to discover how information processing

503
00:22:31,280 --> 00:22:32,680
and nervous system works yet,

504
00:22:32,680 --> 00:22:35,240
which means that the theoretical tools of neuroscientists

505
00:22:35,240 --> 00:22:36,960
might not yet be ready even

506
00:22:36,960 --> 00:22:38,960
to understand what brains are doing.

507
00:22:41,440 --> 00:22:44,280
And I think that when we as non-neuroscientists

508
00:22:44,280 --> 00:22:48,080
look at brains and at the textbooks of neuroscience

509
00:22:48,080 --> 00:22:50,520
that take what I learned at university,

510
00:22:50,520 --> 00:22:52,800
I find from my current perspective

511
00:22:52,800 --> 00:22:56,000
that there are some shortcomings in this description

512
00:22:56,000 --> 00:22:58,920
that if I want to sit down as a computer scientist

513
00:22:58,920 --> 00:23:01,520
and discover the space of possible solutions

514
00:23:01,520 --> 00:23:04,880
in nervous systems and for functional approximation

515
00:23:04,880 --> 00:23:06,520
and just get a machine to search through it

516
00:23:06,520 --> 00:23:08,520
and so on would be insufficient.

517
00:23:08,520 --> 00:23:10,320
And this starts out with the idea

518
00:23:10,320 --> 00:23:13,440
that a neuron is a specialized switch

519
00:23:13,440 --> 00:23:16,240
at the way in which we abstract the neurons

520
00:23:16,240 --> 00:23:18,520
since the perceptron is that the neuron

521
00:23:18,520 --> 00:23:23,080
is some very simple gate or something element

522
00:23:23,080 --> 00:23:25,680
like in a circuit that does something

523
00:23:25,680 --> 00:23:29,400
in a mechanized automatic algorithmic way.

524
00:23:29,400 --> 00:23:30,840
And I don't think that's really true.

525
00:23:30,840 --> 00:23:32,600
A neuron is not a specialized switch.

526
00:23:32,600 --> 00:23:34,960
A neuron is a single-celled animal.

527
00:23:34,960 --> 00:23:35,920
It's quite complicated.

528
00:23:35,920 --> 00:23:36,760
It wants something.

529
00:23:36,760 --> 00:23:38,200
It wants to survive.

530
00:23:38,200 --> 00:23:39,800
It is able to learn by itself.

531
00:23:39,800 --> 00:23:41,040
It is adaptive.

532
00:23:41,040 --> 00:23:42,520
Can do a lot of things.

533
00:23:42,520 --> 00:23:46,000
It's almost like an amoeba that links up with other amoebas

534
00:23:46,000 --> 00:23:48,240
and grows into this static structure

535
00:23:48,240 --> 00:23:51,600
depending on its environment to perform a particular task.

536
00:23:51,600 --> 00:23:54,080
And this is quite different as a perspective.

537
00:23:54,080 --> 00:23:57,280
It means that your neuron is not just some kind of integrator

538
00:23:57,280 --> 00:23:59,640
or some kind of weighted sum of real numbers

539
00:23:59,640 --> 00:24:01,520
of activations that come in.

540
00:24:01,520 --> 00:24:04,640
The neuron is really a reinforcement learning agent

541
00:24:04,640 --> 00:24:05,680
with a little bit of memory

542
00:24:05,680 --> 00:24:08,520
and a little bit of ability to look into the future.

543
00:24:08,520 --> 00:24:11,000
Not very far, but a little bit.

544
00:24:11,000 --> 00:24:14,840
And it implements a number of adaptive functions

545
00:24:14,840 --> 00:24:17,880
to deal with its environment and reap rewards

546
00:24:17,880 --> 00:24:21,200
that ultimately allow the neuron to survive in the brain

547
00:24:21,200 --> 00:24:23,640
and not be strapped by the organism or killed by it

548
00:24:23,640 --> 00:24:26,480
because it's not doing the right thing.

549
00:24:26,480 --> 00:24:28,360
The second misunderstanding, I think,

550
00:24:28,360 --> 00:24:31,920
beyond neurons are not just specialized switches is

551
00:24:31,920 --> 00:24:33,920
that it's only neurons.

552
00:24:33,920 --> 00:24:36,560
There is probably no fundamental difference

553
00:24:36,560 --> 00:24:38,920
between neurons and other cells.

554
00:24:38,920 --> 00:24:41,560
That is, every cell can do information processing

555
00:24:41,560 --> 00:24:43,200
in conjunction with other cells

556
00:24:43,200 --> 00:24:45,240
if it's in a multicellular animal.

557
00:24:45,240 --> 00:24:48,360
It probably needs to be somewhat multicellular

558
00:24:48,360 --> 00:24:50,440
because if it's a single-celled animal,

559
00:24:50,440 --> 00:24:53,000
it's maybe evolving in such a way

560
00:24:53,000 --> 00:24:55,120
that it's adversarial to its environment.

561
00:24:55,120 --> 00:24:59,040
It doesn't benefit if it computes information

562
00:24:59,040 --> 00:25:00,600
together with others.

563
00:25:00,600 --> 00:25:02,800
But if it lives together with others

564
00:25:02,800 --> 00:25:04,160
in a high degree of organization

565
00:25:04,160 --> 00:25:06,200
like the cells in our body are doing

566
00:25:06,200 --> 00:25:09,360
and you co-evolve them, then the cells can all send

567
00:25:09,360 --> 00:25:11,360
many types of messages via chemicals

568
00:25:11,360 --> 00:25:14,720
over the cellular membranes to neighboring cells.

569
00:25:14,720 --> 00:25:17,000
They can interpret those messages

570
00:25:17,000 --> 00:25:20,720
and they can learn how to respond to these messages,

571
00:25:20,720 --> 00:25:24,400
which means they can learn to perform arbitrary computation.

572
00:25:24,400 --> 00:25:27,640
So what's the difference between a neuron and another cell

573
00:25:27,640 --> 00:25:28,880
if they can do the same thing?

574
00:25:28,880 --> 00:25:31,160
Well, the specialization of neurons

575
00:25:31,160 --> 00:25:34,120
is that they have extremely long accents,

576
00:25:34,120 --> 00:25:36,960
so which they can send information coded

577
00:25:36,960 --> 00:25:39,960
as electrochemical impulses very quickly

578
00:25:39,960 --> 00:25:41,840
over long distances.

579
00:25:41,840 --> 00:25:43,920
I think that neurons might be best understood

580
00:25:43,920 --> 00:25:45,520
as telegraph cells.

581
00:25:45,520 --> 00:25:48,800
They're very special cells that have evolved only in animals

582
00:25:48,800 --> 00:25:52,200
or mostly in animals and that are very expensive to run

583
00:25:52,200 --> 00:25:53,840
because they need a lot of energy

584
00:25:53,840 --> 00:25:56,040
to send information so quickly.

585
00:25:56,040 --> 00:25:59,160
And the benefit is that they can move an animal very quickly

586
00:25:59,160 --> 00:26:01,480
so nature, so it can eat plants or other animals

587
00:26:01,480 --> 00:26:02,720
to get that energy.

588
00:26:02,720 --> 00:26:05,320
So basically the animal gets more energy

589
00:26:05,320 --> 00:26:08,000
that it could get from photosynthesis.

590
00:26:08,000 --> 00:26:12,080
And as a benefit, it's able to afford

591
00:26:12,080 --> 00:26:14,800
to have this expensive nervous system.

592
00:26:14,800 --> 00:26:17,480
And the code that the neurons are using

593
00:26:17,480 --> 00:26:20,440
is probably different from the code, from the chemical codes

594
00:26:20,440 --> 00:26:22,280
that the cells are using for the messages.

595
00:26:22,280 --> 00:26:24,160
It's like a Morse code, probably something

596
00:26:24,160 --> 00:26:27,960
like a telegraph system, so it can send information so quickly.

597
00:26:27,960 --> 00:26:31,120
And initially, it seems that to me

598
00:26:31,120 --> 00:26:34,600
that neurons might have evolved to move skeletal muscles

599
00:26:34,600 --> 00:26:35,640
at the limit of physics.

600
00:26:35,640 --> 00:26:38,560
So it can, from centralized coordination

601
00:26:38,560 --> 00:26:42,520
in the nervous system, from the central nervous system,

602
00:26:42,520 --> 00:26:43,960
send information so quickly

603
00:26:43,960 --> 00:26:47,440
that the whole organism is coordinated much, much faster

604
00:26:47,440 --> 00:26:51,200
at much shorter time spans than plants are.

605
00:26:51,200 --> 00:26:53,840
And the other thing is once you can move very quickly,

606
00:26:53,840 --> 00:26:56,000
you also need to perceive very quickly.

607
00:26:56,000 --> 00:26:58,600
So it's also driving sensory input

608
00:26:58,600 --> 00:27:00,880
and the evaluation of sensory input

609
00:27:00,880 --> 00:27:03,160
and decision-making and learning.

610
00:27:03,160 --> 00:27:06,280
So it's basically duplicating the information processing

611
00:27:06,280 --> 00:27:08,080
that existed in the body

612
00:27:08,080 --> 00:27:12,920
and it is creating something like a second information

613
00:27:12,920 --> 00:27:16,160
processor in the body that is running at much, much faster

614
00:27:16,160 --> 00:27:19,600
time scales than the normal cellular information processing

615
00:27:19,600 --> 00:27:22,880
that will also exist in large, long-lived plants.

616
00:27:22,880 --> 00:27:27,160
So my perspective is that just by looking at means and motive,

617
00:27:27,160 --> 00:27:29,360
the possibilities of what evolution can do

618
00:27:29,360 --> 00:27:32,040
and the capabilities of what an individual cell has,

619
00:27:32,040 --> 00:27:35,680
I suspect that every long-lived organism with many cells

620
00:27:35,680 --> 00:27:39,520
is basically going to function like a very, very slow brain.

621
00:27:39,520 --> 00:27:42,840
And there are almost no limits

622
00:27:42,960 --> 00:27:46,680
to what this slow brain can do if it lives long enough,

623
00:27:46,680 --> 00:27:48,960
but it's not going to do this at the same time frame.

624
00:27:48,960 --> 00:27:51,800
So animals will be able to outthink plants

625
00:27:51,800 --> 00:27:54,960
just because they are so fast and run circles around them.

626
00:27:56,960 --> 00:28:01,320
And the third misunderstanding is that we think

627
00:28:01,320 --> 00:28:04,000
that consciousness is extremely rare,

628
00:28:04,000 --> 00:28:06,840
that consciousness may be only existing in humans

629
00:28:06,840 --> 00:28:10,320
and forms only very late in the evolution

630
00:28:10,320 --> 00:28:12,040
of intelligent systems.

631
00:28:12,040 --> 00:28:15,840
But it turns out we don't get conscious after the PhD.

632
00:28:15,840 --> 00:28:19,360
We seem to be conscious before we can track a finger.

633
00:28:19,360 --> 00:28:23,280
And if that is the case, maybe self-reflexive attention

634
00:28:23,280 --> 00:28:25,640
is a requirement if you want to learn

635
00:28:25,640 --> 00:28:27,360
beyond happy and learning.

636
00:28:27,360 --> 00:28:29,440
If you don't want to just look at co-activation

637
00:28:29,440 --> 00:28:31,680
between cells as a learning paradigm, which is probably

638
00:28:31,680 --> 00:28:34,160
sufficient to map your body surface and so on,

639
00:28:34,160 --> 00:28:36,960
but you want to have a coherent model of reality,

640
00:28:36,960 --> 00:28:40,320
maybe you need to start out with some kind of core

641
00:28:40,320 --> 00:28:43,280
that organizes everything into coherence.

642
00:28:43,280 --> 00:28:45,760
And what we find confusing is about consciousness

643
00:28:45,760 --> 00:28:48,320
that is that we don't seem to need it for many things.

644
00:28:48,320 --> 00:28:51,440
A sleepwalker can do many of the things that normally

645
00:28:51,440 --> 00:28:52,520
require consciousness.

646
00:28:52,520 --> 00:28:55,480
And this is confusing philosophers to no end.

647
00:28:55,480 --> 00:28:57,720
But maybe consciousness is in some sense

648
00:28:57,720 --> 00:29:00,240
like government in a society.

649
00:29:00,240 --> 00:29:03,800
And I don't mean government as some kind of abstract principle,

650
00:29:03,800 --> 00:29:06,200
but as a real-time interaction that

651
00:29:06,200 --> 00:29:10,080
is making society coherent.

652
00:29:10,120 --> 00:29:13,640
And a society can do everything without government.

653
00:29:13,640 --> 00:29:16,360
If you would be shutting down the government today,

654
00:29:16,360 --> 00:29:21,600
it would be days before we notice and years before we crash.

655
00:29:21,600 --> 00:29:24,960
But to get to the state in which society is today,

656
00:29:24,960 --> 00:29:28,240
with streets and infrastructure and educational system

657
00:29:28,240 --> 00:29:30,440
and so on, you need to have a government.

658
00:29:30,440 --> 00:29:33,840
You're not going to bootstrap a group of people

659
00:29:33,840 --> 00:29:35,960
into an organization without having

660
00:29:35,960 --> 00:29:37,960
some kind of hierarchical organization that

661
00:29:37,960 --> 00:29:41,040
makes this hoop of people coherent in their actions

662
00:29:41,040 --> 00:29:43,880
and creates some next-level agent out of them.

663
00:29:43,880 --> 00:29:46,200
And to do this, the government needs

664
00:29:46,200 --> 00:29:48,720
to start out with some local coherence,

665
00:29:48,720 --> 00:29:51,080
with some making itself coherent,

666
00:29:51,080 --> 00:29:53,680
and then imposing some kind of organization

667
00:29:53,680 --> 00:29:55,520
on the environment that is branching out

668
00:29:55,520 --> 00:29:59,160
and scales over all the individual agents.

669
00:29:59,160 --> 00:30:02,680
And if you just put individual people together for long enough,

670
00:30:02,680 --> 00:30:04,520
then different forms of government

671
00:30:04,520 --> 00:30:07,600
will emerge and some kind of evolutionary competition

672
00:30:07,600 --> 00:30:08,440
between them.

673
00:30:08,440 --> 00:30:10,720
And eventually, one of them will take over

674
00:30:10,720 --> 00:30:13,480
and organize this group of people in such a way.

675
00:30:13,480 --> 00:30:15,480
And the idea that the same thing could happen

676
00:30:15,480 --> 00:30:17,200
among the neurons, that they're basically

677
00:30:17,200 --> 00:30:19,520
different forms of organization that start out

678
00:30:19,520 --> 00:30:23,480
from small cores and then move as activation patterns

679
00:30:23,480 --> 00:30:26,000
that are agnostic of the individual units that they run on.

680
00:30:26,000 --> 00:30:28,400
But they impose the same language on all of them,

681
00:30:28,400 --> 00:30:30,400
similar capabilities on all of them,

682
00:30:30,400 --> 00:30:34,760
so the locus of action can move around between them.

683
00:30:34,760 --> 00:30:36,960
This idea that the brain organization could evolve

684
00:30:36,960 --> 00:30:40,600
like this is similar to Gary Edelman's idea of neural

685
00:30:40,600 --> 00:30:43,560
Darwinism, that basically our brain organization,

686
00:30:43,560 --> 00:30:46,760
our mental organization is not hard-coded in the genome

687
00:30:46,760 --> 00:30:50,240
as a blueprint, but what the genome contains

688
00:30:50,240 --> 00:30:52,920
is the conditions to start this evolution

689
00:30:52,920 --> 00:30:55,640
between different forms of organization.

690
00:30:55,640 --> 00:30:58,520
And then rig the evolution in a particular way

691
00:30:58,520 --> 00:31:01,400
so it converges quickly.

692
00:31:01,440 --> 00:31:05,440
There are some ideas that we could take from biology

693
00:31:05,440 --> 00:31:07,200
into technical systems.

694
00:31:07,200 --> 00:31:10,080
And first of all, I think that the system

695
00:31:10,080 --> 00:31:11,320
needs to be real-time.

696
00:31:11,320 --> 00:31:13,640
It needs to be coupled to the environment

697
00:31:13,640 --> 00:31:16,200
and needs to go into resonance with whatever environment

698
00:31:16,200 --> 00:31:17,160
is coupled to.

699
00:31:17,160 --> 00:31:19,560
And it needs to regulate the interaction

700
00:31:19,560 --> 00:31:23,000
with that environment until at the level of coupling,

701
00:31:23,000 --> 00:31:25,680
at the temporal resolution that it has,

702
00:31:25,680 --> 00:31:29,680
is able to track reality around it.

703
00:31:29,680 --> 00:31:32,160
And it's something that our machine learning models

704
00:31:32,160 --> 00:31:35,640
are not doing yet and not real-time.

705
00:31:35,640 --> 00:31:38,520
Even something like stable diffusion

706
00:31:38,520 --> 00:31:41,520
is trained in digital images that are not happening in real-time.

707
00:31:41,520 --> 00:31:43,320
They're not even happening in the right order.

708
00:31:43,320 --> 00:31:46,360
There are just 800 million disconnected images,

709
00:31:46,360 --> 00:31:49,160
or which the system is trying to find structure.

710
00:31:49,160 --> 00:31:51,840
And this would not work for a logical organism.

711
00:31:51,840 --> 00:31:53,200
I don't think that we could converge

712
00:31:53,200 --> 00:31:54,680
from this amount of data.

713
00:31:54,680 --> 00:31:58,120
Instead, what we get is a world that changes continuously

714
00:31:58,120 --> 00:31:59,400
by small degrees.

715
00:31:59,440 --> 00:32:02,920
And these small changes make sure that every frame is

716
00:32:02,920 --> 00:32:04,720
related to the last frame.

717
00:32:04,720 --> 00:32:07,720
And we can learn universal laws in which these frames are

718
00:32:07,720 --> 00:32:08,280
related.

719
00:32:08,280 --> 00:32:10,760
There are laws of conservation of information.

720
00:32:10,760 --> 00:32:12,800
And without this conservation of information,

721
00:32:12,800 --> 00:32:15,840
where we learn the transitions between adjacent frames,

722
00:32:15,840 --> 00:32:19,360
I don't think that we would be able to learn from the universe.

723
00:32:19,360 --> 00:32:23,440
So one thing is we change the paradigm from image to video

724
00:32:23,440 --> 00:32:26,800
or other streaming data that has information preservation.

725
00:32:26,800 --> 00:32:29,160
And in the beginning, this might look more difficult to us.

726
00:32:29,160 --> 00:32:31,120
Isn't it harder to learn from video

727
00:32:31,120 --> 00:32:33,440
than it is to learn from single pictures?

728
00:32:33,440 --> 00:32:36,280
Well, what you want to learn is the fact

729
00:32:36,280 --> 00:32:38,880
that you are living in the universe with moving objects

730
00:32:38,880 --> 00:32:40,920
that happen in free space and so on.

731
00:32:40,920 --> 00:32:43,200
It's actually much easier to learn this from video,

732
00:32:43,200 --> 00:32:46,000
because it contains way more constraints than this way.

733
00:32:46,000 --> 00:32:48,600
They're much more obvious.

734
00:32:48,600 --> 00:32:51,760
The next thing is the way in which our brain is modeling

735
00:32:51,760 --> 00:32:54,080
these things seems to be made from lots

736
00:32:54,080 --> 00:32:58,560
of small periodic loops, small interlocking periodic loops.

737
00:32:59,400 --> 00:33:01,960
And first of all, it has to be loops,

738
00:33:01,960 --> 00:33:04,680
because the brain is relatively slow.

739
00:33:04,680 --> 00:33:08,480
Information transmission in the brain

740
00:33:08,480 --> 00:33:12,040
is so slow that it takes appreciable fraction

741
00:33:12,040 --> 00:33:16,080
of a second for a signal to cross the entire neocortex.

742
00:33:16,080 --> 00:33:17,960
And if you want to create simultaneity

743
00:33:17,960 --> 00:33:21,280
between these different parts of the brain,

744
00:33:21,280 --> 00:33:22,360
you're not going to get there.

745
00:33:22,360 --> 00:33:26,320
It's right in our computers and our CPUs and so on.

746
00:33:26,600 --> 00:33:29,400
Our GPUs, they make everything simultaneous

747
00:33:29,400 --> 00:33:34,080
by exploiting the speed of the signal transmission

748
00:33:34,080 --> 00:33:36,920
in our CPUs and GPUs.

749
00:33:36,920 --> 00:33:39,680
It also means that we cannot increase the frequency

750
00:33:39,680 --> 00:33:41,760
at which we run them arbitrarily,

751
00:33:41,760 --> 00:33:44,920
or we cannot make the CPUs and GPUs arbitrarily large,

752
00:33:44,920 --> 00:33:47,120
because it just takes so much time for a signal

753
00:33:47,120 --> 00:33:49,720
to cross over the entire circuit.

754
00:33:51,520 --> 00:33:53,360
The next step could be to use photonics.

755
00:33:53,360 --> 00:33:55,400
So we can go to the speed of light

756
00:33:55,400 --> 00:33:57,480
and make this system slightly faster

757
00:33:57,480 --> 00:33:58,880
and slightly larger again,

758
00:34:00,000 --> 00:34:03,400
while having coherence over the entire system.

759
00:34:03,400 --> 00:34:06,040
But our biological systems don't have a chance of doing that.

760
00:34:06,040 --> 00:34:08,560
They must live with the fact that it takes very long

761
00:34:08,560 --> 00:34:10,120
for signals to get there.

762
00:34:10,120 --> 00:34:12,040
And the way to deal with that is to make sure

763
00:34:12,040 --> 00:34:14,560
that you're okay if you are out of sync.

764
00:34:14,560 --> 00:34:17,120
You just need to be in the same phase.

765
00:34:17,120 --> 00:34:19,800
Basically, you go at the same frequency

766
00:34:19,800 --> 00:34:21,400
at different points of the brain,

767
00:34:21,400 --> 00:34:23,840
and you make sure that the signal eventually gets there,

768
00:34:23,840 --> 00:34:26,120
but it's okay if it's from the previous cycle

769
00:34:26,120 --> 00:34:29,400
or two cycles ago, or several cycles ago,

770
00:34:29,400 --> 00:34:32,400
as long as the content of the different brain areas

771
00:34:32,400 --> 00:34:34,520
that only changes gradually.

772
00:34:34,520 --> 00:34:37,680
If that happens, you're able to integrate over that.

773
00:34:37,680 --> 00:34:40,360
You can use predictive algorithms and so on,

774
00:34:40,360 --> 00:34:42,040
and can synchronize the whole thing.

775
00:34:42,040 --> 00:34:44,440
So basically, this idea of slow oscillators

776
00:34:44,440 --> 00:34:47,440
is something that we could translate into digital systems.

777
00:34:49,240 --> 00:34:51,440
The next thing is this emergent management,

778
00:34:51,440 --> 00:34:52,640
and here we'll Darwinism.

779
00:34:52,640 --> 00:34:55,720
So instead of having a particular circuit

780
00:34:55,720 --> 00:34:58,920
that is required to be like this,

781
00:34:58,920 --> 00:35:02,080
let's evolve this computational operators that we need.

782
00:35:02,880 --> 00:35:05,360
And the next one is that many of the functions

783
00:35:05,360 --> 00:35:09,040
that the brain discovering are only discovered once.

784
00:35:09,040 --> 00:35:12,400
And this is tool for simple functions,

785
00:35:12,400 --> 00:35:15,360
like addition, integration, multiplication,

786
00:35:15,360 --> 00:35:17,680
simple computational primitives, rotation,

787
00:35:17,680 --> 00:35:20,600
that are being used for many, many mathematical primitives

788
00:35:20,600 --> 00:35:23,160
that we require to describe the geometry of sound,

789
00:35:23,160 --> 00:35:25,280
of images, of thought.

790
00:35:25,280 --> 00:35:27,040
And this basic arithmetic,

791
00:35:27,040 --> 00:35:29,240
this basic library of computational functions

792
00:35:29,240 --> 00:35:31,920
at the moment to train this into a neural network

793
00:35:31,920 --> 00:35:33,600
takes a very long time, right?

794
00:35:33,600 --> 00:35:36,040
Despite the neural network being built over addition

795
00:35:36,040 --> 00:35:37,480
and multiplication,

796
00:35:37,480 --> 00:35:41,080
it's not easy for a neural network to learn arithmetic.

797
00:35:41,080 --> 00:35:42,640
It's possible to do it,

798
00:35:42,640 --> 00:35:45,400
but it takes enormous amount of training data.

799
00:35:45,400 --> 00:35:47,560
And in every context, locally,

800
00:35:47,560 --> 00:35:51,240
the neural network is performing the same arithmetic

801
00:35:51,240 --> 00:35:52,680
over and over again.

802
00:35:52,680 --> 00:35:55,240
It has to retrain these functions

803
00:35:55,240 --> 00:35:59,280
into a different region of the network again.

804
00:35:59,280 --> 00:36:03,200
And basically getting all these different computational

805
00:36:03,200 --> 00:36:05,920
primitives bootstrapped into the neural network

806
00:36:05,920 --> 00:36:08,040
is something that is hard.

807
00:36:08,040 --> 00:36:10,200
And it has an interesting effect.

808
00:36:10,200 --> 00:36:13,800
You can, for instance, train a neural network

809
00:36:13,800 --> 00:36:15,320
on audio input,

810
00:36:15,960 --> 00:36:18,760
and then use it to discover structure and vision.

811
00:36:18,760 --> 00:36:20,240
And it's going to be much faster

812
00:36:20,240 --> 00:36:23,360
because the audio input already prepares the neural network

813
00:36:23,360 --> 00:36:25,760
to learn many of the computational primitives,

814
00:36:25,760 --> 00:36:28,480
basic arithmetic in many parts of the network,

815
00:36:28,480 --> 00:36:31,480
that it can then adapt for a new task.

816
00:36:31,480 --> 00:36:33,360
And the way in which our brain is doing this

817
00:36:33,360 --> 00:36:35,640
is probably that it learns some useful functions

818
00:36:35,640 --> 00:36:37,720
and then these neurons have a way

819
00:36:37,720 --> 00:36:40,960
to exchange these functions possibly via RNA.

820
00:36:40,960 --> 00:36:44,640
And so basically the functions that are being computed

821
00:36:44,640 --> 00:36:48,040
are to some degree agnostic to the individual neuron.

822
00:36:48,040 --> 00:36:49,560
They are somewhat substrate independent.

823
00:36:49,560 --> 00:36:50,960
They're just migrate to.

824
00:36:50,960 --> 00:36:53,480
So a different paradigm might be instead

825
00:36:53,480 --> 00:36:55,800
of using local functions over the neighborhood

826
00:36:55,800 --> 00:36:56,640
of every neuron.

827
00:36:56,640 --> 00:36:59,400
And every neuron is learning its own set of functions.

828
00:36:59,400 --> 00:37:01,600
You learn a set of global functions

829
00:37:01,600 --> 00:37:04,560
and every neuron is deciding which ones of those to use.

830
00:37:07,000 --> 00:37:09,520
And it's also something that has almost never been tried

831
00:37:09,520 --> 00:37:13,200
in AI and that what I'd like to get seen.

832
00:37:14,200 --> 00:37:18,360
Another thing is that the main focus is on reward.

833
00:37:18,360 --> 00:37:20,160
What you try to do in the brain is

834
00:37:20,160 --> 00:37:25,040
to do the most useful thing with fixed resources.

835
00:37:25,040 --> 00:37:28,120
And this means you have to assess the global reward

836
00:37:28,120 --> 00:37:31,960
that the organism is getting out of the distributions

837
00:37:31,960 --> 00:37:33,120
of all the neurons.

838
00:37:33,120 --> 00:37:36,480
And then you need to distribute this reward

839
00:37:36,480 --> 00:37:39,000
among all the neurons that contribute to the result.

840
00:37:39,000 --> 00:37:42,480
This is similar to what you do in a corporation.

841
00:37:42,520 --> 00:37:44,640
It's an economic problem, but the corporation

842
00:37:44,640 --> 00:37:46,640
tries to do the most valuable thing

843
00:37:46,640 --> 00:37:49,080
that it can do with all the employees that it has.

844
00:37:49,080 --> 00:37:50,840
And to do this, it needs to get rewards

845
00:37:50,840 --> 00:37:53,160
to all the individual employees.

846
00:37:53,160 --> 00:37:55,680
And the rewards are not given in such a way

847
00:37:55,680 --> 00:37:59,440
that every employee gets a different amount of money.

848
00:37:59,440 --> 00:38:01,840
And the one that has the biggest contribution

849
00:38:01,840 --> 00:38:04,800
to the bottom line by the others are also important

850
00:38:04,800 --> 00:38:06,280
gets much, much more.

851
00:38:06,280 --> 00:38:09,000
There is some degree of this, but it's mostly depending

852
00:38:09,000 --> 00:38:11,680
on the negotiation power of individual employees

853
00:38:11,800 --> 00:38:12,640
on the market.

854
00:38:13,480 --> 00:38:16,360
If there was no fungibility

855
00:38:16,360 --> 00:38:18,280
and you would need to train all your employees,

856
00:38:18,280 --> 00:38:20,520
it would make sense to give them all the same amount

857
00:38:20,520 --> 00:38:21,640
of money, right?

858
00:38:21,640 --> 00:38:24,040
And in the liberal capitalism, it doesn't make sense

859
00:38:24,040 --> 00:38:29,040
to give a good retail worker the same amount of salary

860
00:38:29,680 --> 00:38:32,840
than it does to give money to a good manager.

861
00:38:32,840 --> 00:38:36,160
But if you need retail workers, you will have to employ one.

862
00:38:36,160 --> 00:38:39,240
And the reason why retail workers are less than managers

863
00:38:39,240 --> 00:38:41,320
is mostly because there are many more retail workers

864
00:38:41,760 --> 00:38:44,560
competing for positions than there are managers

865
00:38:44,560 --> 00:38:46,040
competing for positions.

866
00:38:46,040 --> 00:38:50,080
So the supply and demand regulates labor market

867
00:38:50,080 --> 00:38:51,680
in such a way.

868
00:38:51,680 --> 00:38:53,800
But this is not true in the biological system.

869
00:38:53,800 --> 00:38:56,360
Every new one basically is going to consume

870
00:38:56,360 --> 00:38:58,920
the same amount of resources just for existing

871
00:38:58,920 --> 00:39:00,960
and being ready to do something.

872
00:39:00,960 --> 00:39:03,120
So our reward here is different.

873
00:39:03,120 --> 00:39:05,240
It's not being accumulated in the bank account

874
00:39:05,240 --> 00:39:06,480
of the new one.

875
00:39:06,480 --> 00:39:10,120
Instead, this is just a signal that tells the organism

876
00:39:10,120 --> 00:39:11,840
that this new one is still going to get fat

877
00:39:11,840 --> 00:39:14,120
because it's useful in what it does.

878
00:39:14,120 --> 00:39:16,120
And the new one needs to get feedback similar

879
00:39:16,120 --> 00:39:17,960
to the feedback that you get from your colleagues.

880
00:39:17,960 --> 00:39:20,160
That's the actual reward that tells you

881
00:39:20,160 --> 00:39:22,800
you're doing the right thing, right?

882
00:39:22,800 --> 00:39:26,000
So it needs to be some kind of communicative reward,

883
00:39:26,000 --> 00:39:28,680
some messages that are not directly food,

884
00:39:28,680 --> 00:39:30,640
but that are more anticipated reward

885
00:39:30,640 --> 00:39:34,800
that are like money only without accumulation.

886
00:39:34,800 --> 00:39:38,080
And so there's basically going to be a reward-driven language

887
00:39:38,080 --> 00:39:41,720
and who is distributing all the reward in the brain?

888
00:39:41,720 --> 00:39:45,440
Well, all the cells are with all the neurons mostly,

889
00:39:45,440 --> 00:39:47,560
but also the other cells, maybe glia cells,

890
00:39:47,560 --> 00:39:48,960
and so on that contribute

891
00:39:48,960 --> 00:39:50,840
and the distribution of the rewards.

892
00:39:50,840 --> 00:39:52,760
And so what's happening in the biological system

893
00:39:52,760 --> 00:39:55,040
is that it involves a reward language.

894
00:39:55,040 --> 00:39:58,000
There are two types of signals that are being sent around.

895
00:39:58,000 --> 00:40:00,000
One is the results of the computation,

896
00:40:00,000 --> 00:40:01,560
which is read by other neurons

897
00:40:01,560 --> 00:40:03,760
based on what kind of activation they're interested

898
00:40:03,760 --> 00:40:05,720
in filtering out of the environment.

899
00:40:05,720 --> 00:40:08,000
And the other one is going to be signals

900
00:40:08,000 --> 00:40:11,000
that amount to reward and punishment

901
00:40:11,000 --> 00:40:12,600
that basically tell other neurons

902
00:40:12,600 --> 00:40:14,000
whether they should do more or less

903
00:40:14,000 --> 00:40:15,320
of a certain computation.

904
00:40:17,440 --> 00:40:22,200
And so basically reading of information is going to be pulled.

905
00:40:22,200 --> 00:40:24,680
You draw information from the environment

906
00:40:24,680 --> 00:40:26,720
and reward is going to be pushed

907
00:40:26,720 --> 00:40:27,800
because you should not be able

908
00:40:27,800 --> 00:40:31,120
to escape a negative reward, a punishment and so on, right?

909
00:40:32,160 --> 00:40:35,240
And I think we could approximate this

910
00:40:35,240 --> 00:40:37,800
using a new paradigm and a number of experiments

911
00:40:37,800 --> 00:40:39,600
that I would like to do in this regard.

912
00:40:39,600 --> 00:40:43,040
So basically a neuron has an internal state vector

913
00:40:43,040 --> 00:40:46,360
that contains the slight history of the neurons

914
00:40:46,360 --> 00:40:48,440
over the last few activations that is read

915
00:40:48,440 --> 00:40:50,760
and the type of the neuron and so on.

916
00:40:50,760 --> 00:40:52,120
And it has a selector function.

917
00:40:52,120 --> 00:40:53,800
The selector function is basically

918
00:40:53,800 --> 00:40:56,600
defining the receptive field of the neuron.

919
00:40:56,600 --> 00:40:58,080
And the receptive field of the neuron

920
00:40:58,080 --> 00:41:00,600
can be just the environment of the neuron

921
00:41:01,480 --> 00:41:03,240
interpreted as a certain topology.

922
00:41:03,240 --> 00:41:05,800
So basically it looks at its neighborhood

923
00:41:05,800 --> 00:41:09,440
as if it was a space with a certain number of dimensions.

924
00:41:09,440 --> 00:41:11,080
And how can this be?

925
00:41:11,080 --> 00:41:15,280
The neocortex is two-dimensional or two and a half-dimensional

926
00:41:15,280 --> 00:41:16,720
but it has a number of layers,

927
00:41:16,720 --> 00:41:18,880
that number of layers being very small

928
00:41:18,880 --> 00:41:21,240
and it's a large 2D area

929
00:41:21,240 --> 00:41:23,280
subdivided into different regions.

930
00:41:23,280 --> 00:41:27,320
Well, it turns out that you can interpret a 2D area

931
00:41:27,320 --> 00:41:29,160
as something that is in higher dimension

932
00:41:29,160 --> 00:41:31,160
in the same way as you can take the linear

933
00:41:31,160 --> 00:41:33,120
or one-dimensional address space of your computer

934
00:41:33,120 --> 00:41:35,600
and interpret it as a two-dimensional map

935
00:41:35,600 --> 00:41:37,920
or as a three-dimensional space

936
00:41:37,920 --> 00:41:40,280
or as something that happens in eight dimensions

937
00:41:40,280 --> 00:41:42,320
and can perform operations on it.

938
00:41:42,320 --> 00:41:44,040
And this is what the selector function does.

939
00:41:44,040 --> 00:41:45,880
The selector function is basically interpreting

940
00:41:45,880 --> 00:41:48,640
the environment of the individual cell

941
00:41:48,640 --> 00:41:51,520
as a space that contains information

942
00:41:51,520 --> 00:41:53,560
in a certain arrangement.

943
00:41:53,560 --> 00:41:55,280
And it doesn't need to be a regular space.

944
00:41:55,280 --> 00:41:56,360
It can be a manifold.

945
00:41:56,360 --> 00:41:58,000
It can be something that is very selective.

946
00:41:58,000 --> 00:42:00,920
It can be something that only uses five neighbors.

947
00:42:00,920 --> 00:42:04,200
And these neighbors can even be physically very distant.

948
00:42:04,240 --> 00:42:06,120
And so this neuron could be a juncture

949
00:42:06,120 --> 00:42:08,560
or some kind of hub that sends information locally

950
00:42:08,560 --> 00:42:09,920
into the network and so on.

951
00:42:09,920 --> 00:42:12,360
So you're going to have some neurons that have a topology

952
00:42:12,360 --> 00:42:14,560
that allow long distance connectivity

953
00:42:14,560 --> 00:42:19,560
and others that performs local maps and 2D or 3D

954
00:42:19,880 --> 00:42:21,880
and perform functions on them.

955
00:42:21,880 --> 00:42:23,840
And this next to the selector function,

956
00:42:23,840 --> 00:42:25,240
you have the modifier functions.

957
00:42:25,240 --> 00:42:28,120
The modifier functions tell the individual neuron

958
00:42:28,120 --> 00:42:31,200
how it should change its state based on its own state

959
00:42:31,200 --> 00:42:33,800
and the activation that it reads in the environment.

960
00:42:34,640 --> 00:42:37,360
By having some history in its own state,

961
00:42:37,360 --> 00:42:41,320
it's able to respond to a spatiotemporal activation

962
00:42:41,320 --> 00:42:42,880
distribution in its environment.

963
00:42:44,080 --> 00:42:46,600
And it's the use this idea

964
00:42:46,600 --> 00:42:49,200
that the neuron can use global functions.

965
00:42:49,200 --> 00:42:53,240
It means that we can arrange many neurons densely enough

966
00:42:53,240 --> 00:42:54,600
in some kind of lattice

967
00:42:54,600 --> 00:42:56,680
and these functions can arrange themselves

968
00:42:56,680 --> 00:42:58,240
as they need to be arranged.

969
00:42:58,240 --> 00:43:00,640
And they can shift around as they have to

970
00:43:00,640 --> 00:43:03,240
and duplicate themselves if they have to.

971
00:43:03,280 --> 00:43:06,000
Yasha, just in the interest of time.

972
00:43:06,000 --> 00:43:08,040
Yes, I'm basically done.

973
00:43:08,040 --> 00:43:09,960
Thank you for reminding me.

974
00:43:09,960 --> 00:43:14,960
So this selector function, modifier function paradigm

975
00:43:15,200 --> 00:43:18,360
allows us to come up with a new way

976
00:43:18,360 --> 00:43:23,000
of describing a function approximation beyond deep learning.

977
00:43:23,000 --> 00:43:25,360
And at the moment, the search space

978
00:43:25,360 --> 00:43:27,480
in my experiments is way too large.

979
00:43:27,480 --> 00:43:29,840
So basically there are too many ways

980
00:43:29,840 --> 00:43:32,000
in which this could be implemented

981
00:43:32,000 --> 00:43:33,800
from my current perspective

982
00:43:33,800 --> 00:43:37,200
to get this converge to a good solution.

983
00:43:37,200 --> 00:43:40,000
The alternative is I can just handcraft a solution

984
00:43:40,000 --> 00:43:41,560
but it might not be optimal.

985
00:43:41,560 --> 00:43:44,200
But it's something that I would like to definitely look more

986
00:43:44,200 --> 00:43:45,840
into in the future.

987
00:43:45,840 --> 00:43:48,600
And not just me, I suspect that many people

988
00:43:48,600 --> 00:43:51,120
are currently discovering such ideas

989
00:43:51,120 --> 00:43:54,000
and will be working on in the future.

990
00:43:54,000 --> 00:43:56,040
And while it's not clear

991
00:43:56,040 --> 00:43:59,120
that this can provide a viable alternative to deep learning

992
00:43:59,120 --> 00:44:01,520
that is much faster and converging faster

993
00:44:01,520 --> 00:44:05,200
and more efficiently than the present deep learning systems.

994
00:44:05,200 --> 00:44:07,240
This is something that I believe is closer

995
00:44:07,240 --> 00:44:09,600
to what biology has discovered.

996
00:44:09,600 --> 00:44:11,280
And that scales up very reliably

997
00:44:11,280 --> 00:44:13,480
over many, many classes of algorithms.

998
00:44:15,120 --> 00:44:16,280
Okay, that's it from me.

999
00:44:17,200 --> 00:44:20,880
All right, we would like to have a quick one

1000
00:44:20,880 --> 00:44:23,080
to two minute break right now

1001
00:44:23,080 --> 00:44:28,080
so that the panelists can set up their presentation.

1002
00:44:28,320 --> 00:44:33,320
We will do Michael Levin's presentation next.

1003
00:44:34,440 --> 00:44:36,960
So in the meantime, Yosha,

1004
00:44:36,960 --> 00:44:41,440
let me ask you some questions that's got posted

1005
00:44:41,440 --> 00:44:42,680
while you're talking.

1006
00:44:44,720 --> 00:44:47,120
So a question from Nikolai,

1007
00:44:47,120 --> 00:44:49,920
like how neurons are small organisms

1008
00:44:49,920 --> 00:44:52,440
that operate in the emerging super organism

1009
00:44:52,440 --> 00:44:53,280
that is a human,

1010
00:44:53,280 --> 00:44:57,360
would future AGI architecture need to be made up

1011
00:44:57,360 --> 00:44:59,240
of many smaller AIs?

1012
00:45:01,600 --> 00:45:04,640
So there is no centralized control in the brain

1013
00:45:04,640 --> 00:45:08,080
in the sense that all the centralized control

1014
00:45:08,080 --> 00:45:11,680
is emergent over all the organizations between the neurons.

1015
00:45:11,680 --> 00:45:13,200
There is no dedicated CPU

1016
00:45:13,200 --> 00:45:17,560
that is able to process every neuron and update its state.

1017
00:45:17,560 --> 00:45:21,040
Every state update happens locally in the individual cells.

1018
00:45:21,040 --> 00:45:23,280
But this is an engineering constraint

1019
00:45:23,280 --> 00:45:26,240
that doesn't exist in the technical systems.

1020
00:45:26,240 --> 00:45:28,120
And it's not clear yet to me

1021
00:45:28,120 --> 00:45:31,800
to which degree we need to have local control

1022
00:45:31,800 --> 00:45:33,360
to make it happen.

1023
00:45:33,360 --> 00:45:35,920
At the moment, the machine learning of organisms

1024
00:45:35,920 --> 00:45:40,360
that we are using are treating the individual nodes

1025
00:45:40,360 --> 00:45:42,680
in the network just as memory.

1026
00:45:42,680 --> 00:45:46,120
And the updates are done by a centralized algorithm

1027
00:45:46,120 --> 00:45:48,840
that is updating all this memory.

1028
00:45:48,840 --> 00:45:52,360
All right, and you either have a CPU

1029
00:45:52,360 --> 00:45:54,000
that is reading and writing,

1030
00:45:54,000 --> 00:45:58,320
or you have lots of local CPUs in the GPU

1031
00:45:58,320 --> 00:46:01,600
that is doing this with multiple pipelines in parallel.

1032
00:46:01,600 --> 00:46:05,440
And there are biologically inspired chips

1033
00:46:06,440 --> 00:46:08,320
that are mostly experimental at the moment,

1034
00:46:08,320 --> 00:46:09,880
like Intel's Lohi,

1035
00:46:09,880 --> 00:46:13,240
that are using many, many very small simple CPUs

1036
00:46:13,240 --> 00:46:14,080
that are doing this.

1037
00:46:14,080 --> 00:46:16,240
And it's not clear if the best solution

1038
00:46:16,240 --> 00:46:18,080
is to have a CPU for every memory cell.

1039
00:46:18,080 --> 00:46:19,840
It's probably not the case.

1040
00:46:19,840 --> 00:46:21,880
So there are some things that you can do

1041
00:46:21,880 --> 00:46:26,280
in the technical systems that informing conformance

1042
00:46:26,280 --> 00:46:27,640
to a centralized algorithm,

1043
00:46:27,640 --> 00:46:29,440
to centralized specifications

1044
00:46:29,440 --> 00:46:31,960
that the technical system is going to implement.

1045
00:46:31,960 --> 00:46:35,640
And in a biological system, this is just not feasible

1046
00:46:35,640 --> 00:46:38,240
because there is no such centralized authority,

1047
00:46:38,240 --> 00:46:42,360
no engineer who can make nature behave by itself.

1048
00:46:42,360 --> 00:46:45,400
But if you want to think about how to build a mind

1049
00:46:45,400 --> 00:46:47,160
from a biological perspective,

1050
00:46:47,160 --> 00:46:48,760
we have to think about how to build something

1051
00:46:48,760 --> 00:46:51,200
that wants to grow into a mind.

1052
00:46:51,200 --> 00:46:53,200
And we can take some of these ideas.

1053
00:46:53,200 --> 00:46:55,440
It's not clear that we need to make this

1054
00:46:55,440 --> 00:46:57,440
with completely local control only.

1055
00:46:57,440 --> 00:46:59,080
Maybe it's more efficient to have a mixture

1056
00:46:59,080 --> 00:47:01,640
of some local control and a lot of global control

1057
00:47:01,640 --> 00:47:04,960
where we already know what the control is going to be.

1058
00:47:04,960 --> 00:47:06,640
I think Mike is ready.

1059
00:47:06,640 --> 00:47:07,880
Yes, excellent.

1060
00:47:07,880 --> 00:47:10,000
Michael, the floor is now yours.

1061
00:47:10,000 --> 00:47:10,840
Great.

1062
00:47:10,840 --> 00:47:12,880
Okay, well, that was extremely interesting.

1063
00:47:12,880 --> 00:47:15,320
So let me see what I can add here.

1064
00:47:16,320 --> 00:47:18,480
I've got some slides.

1065
00:47:18,480 --> 00:47:19,320
So here we go.

1066
00:47:19,320 --> 00:47:20,920
Hopefully you can see that.

1067
00:47:20,960 --> 00:47:24,440
So what I would like to talk about,

1068
00:47:24,440 --> 00:47:26,560
of course there's this idea that biology

1069
00:47:26,560 --> 00:47:29,040
should be an inspiration for AI.

1070
00:47:29,040 --> 00:47:32,120
And what I would like to do is to deconstruct

1071
00:47:32,120 --> 00:47:34,480
some of the biology that people typically think about

1072
00:47:34,480 --> 00:47:38,600
in these contexts and ditch a lot of things

1073
00:47:38,600 --> 00:47:40,640
that are very common, binary categories

1074
00:47:40,640 --> 00:47:43,960
and a focus on brains, a focus on neurons,

1075
00:47:43,960 --> 00:47:45,280
a focus on humans.

1076
00:47:45,280 --> 00:47:47,080
I want to step away from all of that

1077
00:47:47,080 --> 00:47:49,280
and rebuild a different framework

1078
00:47:49,280 --> 00:47:54,040
that I think has many, many implications for AI.

1079
00:47:54,040 --> 00:47:56,200
So the first thing I want to talk about

1080
00:47:56,200 --> 00:47:59,400
is this idea of a typical human.

1081
00:47:59,400 --> 00:48:02,160
So there's this kind of classic idea

1082
00:48:02,160 --> 00:48:03,560
that we know what a human is

1083
00:48:03,560 --> 00:48:07,600
and it certainly works for practical purposes in society.

1084
00:48:07,600 --> 00:48:11,120
But the idea is sort of pre-scientific

1085
00:48:11,120 --> 00:48:12,880
and it's still, many people are still,

1086
00:48:12,880 --> 00:48:15,160
even scientists are often still caught up in this,

1087
00:48:15,160 --> 00:48:17,040
the idea that, okay, so we have these humans

1088
00:48:17,080 --> 00:48:19,520
and they are discrete natural kind

1089
00:48:19,520 --> 00:48:21,320
and they're different from other animals.

1090
00:48:21,320 --> 00:48:25,240
And so there's this, here's Adam naming the other animals

1091
00:48:25,240 --> 00:48:29,280
and so there's the discrete species and so on.

1092
00:48:29,280 --> 00:48:33,760
But if we take developmental biology and evolution

1093
00:48:33,760 --> 00:48:36,560
and synthetic biology and bioengineering,

1094
00:48:36,560 --> 00:48:38,360
if we take these things seriously,

1095
00:48:38,360 --> 00:48:40,240
then what we find out is that actually

1096
00:48:40,240 --> 00:48:43,320
there are no such natural kinds because all of this,

1097
00:48:43,320 --> 00:48:45,520
both on the evolutionary time scale

1098
00:48:45,520 --> 00:48:47,360
and the developmental time scale

1099
00:48:47,360 --> 00:48:50,040
and now in terms of the technological time scale,

1100
00:48:50,040 --> 00:48:55,200
there are very gradual, very small, very slow changes

1101
00:48:55,200 --> 00:48:57,680
that go all the way back from what people think of

1102
00:48:57,680 --> 00:49:00,400
as a typical human and their intelligence,

1103
00:49:00,400 --> 00:49:03,720
all the way back to very different types of organisms.

1104
00:49:03,720 --> 00:49:06,320
And developmental biology and of course evolution too

1105
00:49:06,320 --> 00:49:09,640
offers absolutely no place to put a sharp line

1106
00:49:09,640 --> 00:49:13,040
and say this creature was not,

1107
00:49:13,120 --> 00:49:15,560
pick an adjective, intelligent, cognitive, conscious,

1108
00:49:15,560 --> 00:49:17,200
well, whatever you like, pick an adjective

1109
00:49:17,200 --> 00:49:19,040
to say this creature was not it,

1110
00:49:19,040 --> 00:49:22,040
but it had some offspring and the offspring now are, right?

1111
00:49:22,040 --> 00:49:24,080
That just doesn't exist

1112
00:49:24,080 --> 00:49:26,400
because all of these changes are very slow

1113
00:49:26,400 --> 00:49:27,680
and very continuous.

1114
00:49:27,680 --> 00:49:31,880
And so there were changes during evolution,

1115
00:49:31,880 --> 00:49:34,760
we all start life as a single cell.

1116
00:49:34,760 --> 00:49:37,040
In the future, there will be all kinds of changes

1117
00:49:37,040 --> 00:49:39,640
to our bodies with biological

1118
00:49:39,640 --> 00:49:41,840
and engineered kinds of devices.

1119
00:49:41,840 --> 00:49:43,840
And so all of these are continua

1120
00:49:43,840 --> 00:49:47,400
of really novel types of embodiments.

1121
00:49:47,400 --> 00:49:52,400
And we build certain kinds of conceptual metaphors

1122
00:49:53,080 --> 00:49:56,520
that try to distinguish different categories here,

1123
00:49:56,520 --> 00:49:59,360
but these are discrete tools,

1124
00:49:59,360 --> 00:50:01,680
the phenomena themselves are deeply continuous

1125
00:50:01,680 --> 00:50:02,680
on multi-scale.

1126
00:50:02,680 --> 00:50:05,040
And to give you just a simple idea

1127
00:50:05,040 --> 00:50:07,320
and then we'll enlarge on this is this.

1128
00:50:07,320 --> 00:50:12,320
And so this caterpillar is a kind of soft-bodied robot

1129
00:50:12,320 --> 00:50:14,200
that lives in a two-dimensional world

1130
00:50:14,200 --> 00:50:15,680
that crawls around on leaves.

1131
00:50:15,680 --> 00:50:18,120
It likes to chew plants and it has this brain

1132
00:50:18,120 --> 00:50:20,480
that's very, very suitable for this purpose.

1133
00:50:20,480 --> 00:50:22,920
What it needs to do is turn into this creature,

1134
00:50:22,920 --> 00:50:24,280
which is completely different.

1135
00:50:24,280 --> 00:50:26,000
It lives in the three-dimensional world,

1136
00:50:26,000 --> 00:50:27,960
it doesn't care about the leaves at all,

1137
00:50:27,960 --> 00:50:30,800
it wants nectar and it flies and it does various things.

1138
00:50:30,800 --> 00:50:34,160
And so during this process, there is a metamorphosis

1139
00:50:34,160 --> 00:50:36,280
where not on an evolutionary time scale,

1140
00:50:36,320 --> 00:50:38,400
during the lifetime of the individual,

1141
00:50:38,400 --> 00:50:40,520
the brain is basically dissociated

1142
00:50:40,520 --> 00:50:42,960
and rebuilt into a new architecture.

1143
00:50:42,960 --> 00:50:47,960
And by the way, there are data that memories persist.

1144
00:50:47,960 --> 00:50:49,280
So if you train the caterpillar,

1145
00:50:49,280 --> 00:50:51,200
the butterfly or moth still remembers

1146
00:50:51,200 --> 00:50:52,480
the original information,

1147
00:50:52,480 --> 00:50:54,000
but you can sort of think about,

1148
00:50:55,480 --> 00:50:57,560
nevermind the question of what's it like to be a butterfly,

1149
00:50:57,560 --> 00:50:59,160
what's it like to be a caterpillar

1150
00:50:59,160 --> 00:51:01,400
changing into a butterfly, right?

1151
00:51:02,400 --> 00:51:06,920
That process of slow, but drastic change

1152
00:51:06,920 --> 00:51:08,440
in your embodiment.

1153
00:51:08,440 --> 00:51:10,840
And so from here, we can just remembering

1154
00:51:10,840 --> 00:51:12,400
that we are all made of parts

1155
00:51:12,400 --> 00:51:16,080
that can modify during our lifetime.

1156
00:51:16,080 --> 00:51:17,640
We can ask some interesting questions.

1157
00:51:17,640 --> 00:51:19,600
For example, you look at a brain

1158
00:51:19,600 --> 00:51:22,640
and we're sort of conditioned to expect that it's obvious

1159
00:51:22,640 --> 00:51:27,480
that a brain contains one human worth of intelligence.

1160
00:51:27,480 --> 00:51:29,240
But this is just because we're used to that

1161
00:51:29,240 --> 00:51:30,360
in terms of our interactions.

1162
00:51:30,360 --> 00:51:31,560
If I showed you a brain

1163
00:51:31,560 --> 00:51:34,120
and you didn't know what this was

1164
00:51:34,120 --> 00:51:36,800
and I asked you how many different cells are in there,

1165
00:51:36,800 --> 00:51:37,720
you would actually have,

1166
00:51:37,720 --> 00:51:40,000
we have no ability to answer that question.

1167
00:51:40,000 --> 00:51:43,200
We have no way to ask how much,

1168
00:51:43,200 --> 00:51:45,320
and I think Yosha got to some of this,

1169
00:51:45,320 --> 00:51:47,480
how much of this real estate is necessary

1170
00:51:47,480 --> 00:51:51,040
for one human's worth of performance?

1171
00:51:51,040 --> 00:51:54,200
We have no idea how much is actually in there.

1172
00:51:54,200 --> 00:51:56,840
And actually, very interestingly,

1173
00:51:56,840 --> 00:52:00,920
the same issue occurs in embryonic development.

1174
00:52:00,920 --> 00:52:03,920
So we all begin as a cellular blastoderm.

1175
00:52:03,920 --> 00:52:06,920
So this is a sheet, a two-dimensional sheet of cells.

1176
00:52:06,920 --> 00:52:09,720
And that sheet turns into an embryo.

1177
00:52:09,720 --> 00:52:10,600
Now, what does that mean?

1178
00:52:10,600 --> 00:52:12,200
First of all, can we guess in advance

1179
00:52:12,200 --> 00:52:14,640
how many embryos are going to come from that sheet?

1180
00:52:14,640 --> 00:52:17,160
Actually, we cannot, and I'll show you why.

1181
00:52:17,160 --> 00:52:18,440
And it's not genetics.

1182
00:52:18,440 --> 00:52:21,440
And then there's the question of what are we actually counting?

1183
00:52:21,440 --> 00:52:23,600
When we count an embryo, I mean, there's 50,000 cells,

1184
00:52:23,600 --> 00:52:25,520
let's say here, what is it that we're counting

1185
00:52:25,520 --> 00:52:26,680
when we say there's an embryo?

1186
00:52:26,680 --> 00:52:28,320
What are we actually counting when we say there's

1187
00:52:28,320 --> 00:52:31,880
a single human inhabitant in this bunch of tissue?

1188
00:52:31,880 --> 00:52:34,880
So one of the things that you can do in embryogenesis

1189
00:52:34,880 --> 00:52:37,600
is you take this blastoderm and you take a little needle

1190
00:52:37,600 --> 00:52:42,600
and you put some kind of scratches into this blastoderm.

1191
00:52:43,160 --> 00:52:44,880
And then they heal, but before they heal,

1192
00:52:44,880 --> 00:52:47,440
what will happen is that each of these regions

1193
00:52:47,440 --> 00:52:50,240
being isolated from the other regions

1194
00:52:50,240 --> 00:52:51,760
decides to organize an embryo

1195
00:52:51,760 --> 00:52:54,760
because they don't know for a while anyway

1196
00:52:54,760 --> 00:52:55,880
that the other regions are there.

1197
00:52:55,880 --> 00:52:58,600
Then when it heals up, it becomes conjoined twins.

1198
00:52:58,600 --> 00:53:00,520
And you can do this very easily in chicken and duck

1199
00:53:00,520 --> 00:53:03,320
and other embryos, but humans work exactly the same way.

1200
00:53:03,320 --> 00:53:05,840
And so then there will be multiple embryos

1201
00:53:05,840 --> 00:53:07,560
within the same blastoderm.

1202
00:53:07,560 --> 00:53:09,320
And then there will be some disputed zones here.

1203
00:53:09,320 --> 00:53:10,680
There's some cells that aren't quite sure

1204
00:53:10,680 --> 00:53:12,080
which one they belong to.

1205
00:53:12,080 --> 00:53:15,760
But this deep idea of individuation,

1206
00:53:15,760 --> 00:53:18,560
of taking some kind of a continuous,

1207
00:53:18,560 --> 00:53:20,000
in fact, it's even worse than continuous

1208
00:53:20,000 --> 00:53:22,520
because it's multi-scale substrate

1209
00:53:22,520 --> 00:53:26,800
and having itself organized into discrete what?

1210
00:53:26,800 --> 00:53:27,920
So in the case of embryos,

1211
00:53:27,920 --> 00:53:30,520
what you have are discrete groups of cells

1212
00:53:30,520 --> 00:53:33,880
that are trying to follow anatomical goals.

1213
00:53:33,880 --> 00:53:36,080
They're trying to achieve particular walks

1214
00:53:36,080 --> 00:53:37,440
in anatomical space.

1215
00:53:37,440 --> 00:53:39,520
They're gonna construct the right number of fingers,

1216
00:53:39,520 --> 00:53:42,600
the right number of eyes, whatever it is.

1217
00:53:42,600 --> 00:53:45,360
Same thing in cognitive development.

1218
00:53:45,360 --> 00:53:48,880
There are issues, there are disorders of individuation

1219
00:53:48,880 --> 00:53:50,640
that you see in split brain patients

1220
00:53:50,640 --> 00:53:52,320
and dissociations and so on.

1221
00:53:52,320 --> 00:53:54,200
So this question of how many are in there

1222
00:53:54,200 --> 00:53:57,720
is deeply interesting and it gets to the bottom

1223
00:53:57,720 --> 00:54:00,920
of what it means to be a coherent agent

1224
00:54:00,920 --> 00:54:02,800
when you're made of parts.

1225
00:54:02,800 --> 00:54:05,560
And I think Alan Turing, although as far as I can tell,

1226
00:54:05,560 --> 00:54:07,120
he didn't write directly about this.

1227
00:54:07,120 --> 00:54:09,320
I think he was well aware of this issue

1228
00:54:09,320 --> 00:54:11,760
because of course he was interested in intelligence

1229
00:54:11,760 --> 00:54:15,880
and generic embodiment and so on.

1230
00:54:15,880 --> 00:54:17,760
But he was also interested in morphogenesis.

1231
00:54:17,760 --> 00:54:21,040
He wrote this paper on biological morphogenesis.

1232
00:54:21,040 --> 00:54:23,160
And I think he understood that these are deeply

1233
00:54:23,160 --> 00:54:24,720
and profoundly the same problem.

1234
00:54:24,720 --> 00:54:25,960
The problem of morphogenesis

1235
00:54:25,960 --> 00:54:28,560
and the problem of the mind are the same problem

1236
00:54:28,560 --> 00:54:32,920
because of this emphasis on emerging

1237
00:54:32,920 --> 00:54:35,520
as a coherent entity from multiple parts.

1238
00:54:35,520 --> 00:54:38,320
So people often talk about, well,

1239
00:54:38,320 --> 00:54:41,000
ants and termites are some kind of collective intelligence

1240
00:54:41,000 --> 00:54:43,320
and we can argue about what that means,

1241
00:54:43,320 --> 00:54:47,120
but we are really a unified, a centralized intelligence.

1242
00:54:47,120 --> 00:54:49,840
We're not like bird flocks or ant colonies.

1243
00:54:49,840 --> 00:54:50,920
But actually, of course,

1244
00:54:50,920 --> 00:54:53,600
all biological systems are made of parts.

1245
00:54:53,600 --> 00:54:56,920
And so we too are a kind of collective intelligence.

1246
00:54:56,920 --> 00:54:58,920
What's interesting is the scaling interface

1247
00:54:58,920 --> 00:55:02,600
is what is it that allows these individual subunits

1248
00:55:02,600 --> 00:55:05,920
to work together and present to other intelligences

1249
00:55:05,920 --> 00:55:08,760
to themselves, by the way, and to the environment

1250
00:55:08,760 --> 00:55:11,960
in a picture of a coherent agent.

1251
00:55:11,960 --> 00:55:14,280
So this is the journey that we all took.

1252
00:55:14,280 --> 00:55:17,040
We began life as a piece of physics.

1253
00:55:17,040 --> 00:55:18,880
So basically as a quiescent oocyte.

1254
00:55:18,880 --> 00:55:21,640
So it's a blob of chemicals, not doing terribly much.

1255
00:55:21,640 --> 00:55:23,840
And then through this incredibly,

1256
00:55:23,840 --> 00:55:26,680
just magical process of embryonic development,

1257
00:55:26,680 --> 00:55:29,280
that we arrive at something like this,

1258
00:55:29,280 --> 00:55:32,440
which is a complex organism with metacognitive capacity

1259
00:55:32,440 --> 00:55:33,520
that's going to make statements

1260
00:55:33,520 --> 00:55:35,720
about how we're not just machines

1261
00:55:35,720 --> 00:55:39,040
and we're different than physics and all that.

1262
00:55:39,040 --> 00:55:42,560
But this whole process is extremely smooth and gradual.

1263
00:55:42,560 --> 00:55:44,080
It happens second by second.

1264
00:55:44,080 --> 00:55:46,000
There is no lightning flash

1265
00:55:46,000 --> 00:55:47,920
at which point physics becomes mind.

1266
00:55:47,920 --> 00:55:49,320
It's a gradual process.

1267
00:55:49,320 --> 00:55:52,240
And we can talk about face transitions and such,

1268
00:55:52,240 --> 00:55:56,080
but there's really not that much evidence

1269
00:55:56,080 --> 00:55:56,920
for any of that.

1270
00:55:56,920 --> 00:55:58,840
It's a very continuous process.

1271
00:55:58,840 --> 00:56:00,200
So this is the kind of thing.

1272
00:56:00,200 --> 00:56:03,080
So I think, Yosha alluded to this a few times.

1273
00:56:03,080 --> 00:56:04,160
This is the sort of thing.

1274
00:56:04,160 --> 00:56:05,160
I mean, not exactly this.

1275
00:56:05,160 --> 00:56:08,040
This is a lacrimaria, it's a free living organism.

1276
00:56:08,040 --> 00:56:09,760
But here's a single cell, right?

1277
00:56:09,760 --> 00:56:11,080
This is what we are made of.

1278
00:56:11,080 --> 00:56:13,440
These guys, there's no brain.

1279
00:56:13,440 --> 00:56:16,080
Here's those, there's no nervous system.

1280
00:56:16,240 --> 00:56:19,160
This is the single cell creature in real time

1281
00:56:19,160 --> 00:56:22,440
using all of the intelligence of its chemical networks.

1282
00:56:22,440 --> 00:56:23,400
And we can talk about this.

1283
00:56:23,400 --> 00:56:25,800
I mean, that quite literally chemical networks can learn

1284
00:56:25,800 --> 00:56:28,120
and they can do inference and many other things.

1285
00:56:29,120 --> 00:56:30,560
Even though it's a single organism

1286
00:56:30,560 --> 00:56:33,960
is handling all of its single cell agendas

1287
00:56:33,960 --> 00:56:35,360
in its environment.

1288
00:56:35,360 --> 00:56:39,240
So metabolically, physiologically, anatomically,

1289
00:56:39,240 --> 00:56:42,120
it's doing what it needs to do.

1290
00:56:42,120 --> 00:56:45,440
And so we are made of extremely competent parts.

1291
00:56:45,440 --> 00:56:46,360
Here's another example.

1292
00:56:46,360 --> 00:56:49,200
This is a, this whole thing, you'll see, you'll see this.

1293
00:56:49,200 --> 00:56:50,160
I'm gonna pause it.

1294
00:56:50,160 --> 00:56:51,720
Whoops.

1295
00:56:51,720 --> 00:56:52,560
I'm gonna pause this.

1296
00:56:52,560 --> 00:56:53,600
This whole thing right here,

1297
00:56:53,600 --> 00:56:55,960
this is called Pfizer and polycephalum.

1298
00:56:55,960 --> 00:56:57,200
It's a slime mold.

1299
00:56:57,200 --> 00:56:59,160
The whole thing is one cell, okay?

1300
00:56:59,160 --> 00:57:01,400
And what it's, what I'm showing you here

1301
00:57:01,400 --> 00:57:03,240
is that it's sitting in this environment.

1302
00:57:03,240 --> 00:57:04,840
These are three glass discs.

1303
00:57:04,840 --> 00:57:06,560
These are very, very light.

1304
00:57:06,560 --> 00:57:07,960
There's no chemicals, there's no food.

1305
00:57:07,960 --> 00:57:09,440
It's just glass, inner glass.

1306
00:57:09,440 --> 00:57:11,040
There's one glass disc here.

1307
00:57:11,040 --> 00:57:12,880
And what it's going to do is,

1308
00:57:12,880 --> 00:57:17,240
it's going to, for the first few hours,

1309
00:57:17,240 --> 00:57:20,440
it's going to just generically grow in all directions here.

1310
00:57:20,440 --> 00:57:21,920
What it's doing during this process

1311
00:57:21,920 --> 00:57:23,680
is it's tugging on its substrate

1312
00:57:23,680 --> 00:57:26,080
and feeling the vibrations that gets back.

1313
00:57:26,080 --> 00:57:28,360
And it can sense the strain angle

1314
00:57:28,360 --> 00:57:29,840
of the objects in its environment.

1315
00:57:29,840 --> 00:57:31,600
And then we'll eventually reliably grow out

1316
00:57:31,600 --> 00:57:32,840
to the heavier mass.

1317
00:57:32,840 --> 00:57:34,600
But during this, so that'll happen at this point,

1318
00:57:34,600 --> 00:57:37,440
but during this time is when it's processing

1319
00:57:37,440 --> 00:57:40,240
that information and learning from its environment.

1320
00:57:40,240 --> 00:57:42,440
And then boom, now the behavior begins.

1321
00:57:42,560 --> 00:57:44,400
So single cells are very competent,

1322
00:57:44,400 --> 00:57:47,400
even microbial single cells.

1323
00:57:47,400 --> 00:57:50,160
And so what we have to understand is that biology,

1324
00:57:50,160 --> 00:57:52,600
so here's a principle that I think is really important

1325
00:57:52,600 --> 00:57:56,800
for future AI, biology is deeply nested.

1326
00:57:56,800 --> 00:57:58,960
That is not merely structurally,

1327
00:57:58,960 --> 00:58:00,960
I mean, that's obvious we're made of organs, tissues,

1328
00:58:00,960 --> 00:58:04,240
and so on, but each layer is competent.

1329
00:58:04,240 --> 00:58:06,600
It solves problems in its own space.

1330
00:58:06,600 --> 00:58:09,520
All of these things from molecular networks

1331
00:58:09,520 --> 00:58:12,280
all the way up to whole organs and beyond

1332
00:58:12,280 --> 00:58:15,400
are solving specific problems in specific spaces.

1333
00:58:15,400 --> 00:58:16,920
So we are really interested in my group,

1334
00:58:16,920 --> 00:58:20,400
we're really interested in creating a framework

1335
00:58:20,400 --> 00:58:24,120
that allows us to relate to really

1336
00:58:24,120 --> 00:58:25,640
very diverse intelligences.

1337
00:58:25,640 --> 00:58:28,840
So, of course, familiar creatures,

1338
00:58:28,840 --> 00:58:30,400
all kinds of weird biologicals,

1339
00:58:30,400 --> 00:58:33,560
colonial organisms, swarms, of course new,

1340
00:58:33,560 --> 00:58:35,520
and I'll show you some in a couple of minutes,

1341
00:58:35,520 --> 00:58:38,680
new engineered creatures, artificial intelligences,

1342
00:58:38,680 --> 00:58:41,520
and maybe at some point exobiological,

1343
00:58:41,520 --> 00:58:42,680
truly alien agents.

1344
00:58:42,680 --> 00:58:44,200
We need to be able to deal with all of this.

1345
00:58:44,200 --> 00:58:47,120
It's not enough to deal with crows and monkeys

1346
00:58:47,120 --> 00:58:50,000
and then maybe octopus, that's way too narrow.

1347
00:58:50,000 --> 00:58:52,520
And so, of course, this is an idea

1348
00:58:52,520 --> 00:58:54,240
that has been addressed before.

1349
00:58:54,240 --> 00:58:58,240
So here's Wiener and colleagues trying to come up

1350
00:58:58,240 --> 00:59:00,560
with a very sort of cybernetic way

1351
00:59:00,560 --> 00:59:03,480
to classify different degrees of behavior

1352
00:59:03,480 --> 00:59:05,680
all the way from passive mechanical behavior

1353
00:59:05,680 --> 00:59:07,760
up to complex cognition

1354
00:59:07,800 --> 00:59:12,200
in a way that abstracts from its familiar embodiments.

1355
00:59:12,200 --> 00:59:13,800
So there's no talk of brains or neurons

1356
00:59:13,800 --> 00:59:14,640
or anything like that.

1357
00:59:14,640 --> 00:59:17,040
This is very sort of functionalist.

1358
00:59:17,040 --> 00:59:20,840
And one thing about us as humans

1359
00:59:20,840 --> 00:59:25,840
is that we are very primed to recognize intelligence

1360
00:59:27,280 --> 00:59:28,720
in the three-dimensional space.

1361
00:59:28,720 --> 00:59:31,800
So basically, medium-sized objects moving at medium speeds

1362
00:59:31,800 --> 00:59:32,960
through three-dimensional space.

1363
00:59:32,960 --> 00:59:34,800
When we see it, we know what agency looks like,

1364
00:59:34,800 --> 00:59:36,720
we know what intelligence looks like.

1365
00:59:36,720 --> 00:59:38,600
But we are really bad at,

1366
00:59:38,600 --> 00:59:40,560
and this is why we must get better at it,

1367
00:59:40,560 --> 00:59:43,880
recognizing intelligence in other types of problem spaces.

1368
00:59:43,880 --> 00:59:47,080
So imagine if you had a direct feeling

1369
00:59:47,080 --> 00:59:49,160
of all of your blood chemistry.

1370
00:59:49,160 --> 00:59:51,040
If you were able to feel your blood chemistry

1371
00:59:51,040 --> 00:59:53,680
the way that you can see objects in three-dimensional space,

1372
00:59:53,680 --> 00:59:56,120
you would be very obvious

1373
00:59:56,120 --> 00:59:57,920
that your kidneys, your liver, and so on

1374
00:59:57,920 --> 00:59:59,960
have a degree of intelligence

1375
00:59:59,960 --> 01:00:02,640
and they're doing amazing things in their problem spaces.

1376
01:00:02,640 --> 01:00:04,920
So we study how individual cells

1377
01:00:04,920 --> 01:00:07,520
navigate the space of gene expression,

1378
01:00:07,520 --> 01:00:10,400
hopefully the physiology and morpho space,

1379
01:00:10,400 --> 01:00:11,720
the space of patterns.

1380
01:00:11,720 --> 01:00:14,360
This is what I'm talking about today.

1381
01:00:14,360 --> 01:00:15,400
And just for a few minutes here,

1382
01:00:15,400 --> 01:00:18,520
here's an example of cells solving

1383
01:00:18,520 --> 01:00:21,600
an entirely novel problem in genetic space.

1384
01:00:21,600 --> 01:00:24,040
So here's a planarian, this is a flatworm.

1385
01:00:24,040 --> 01:00:26,760
They regenerate parts of their body when amputated.

1386
01:00:26,760 --> 01:00:28,720
What we did was we exposed planaria

1387
01:00:28,720 --> 01:00:30,080
to a solution of barium.

1388
01:00:30,080 --> 01:00:32,720
Barium blocks all of their potassium channels,

1389
01:00:32,720 --> 01:00:35,400
the cells and the neurons are really unhappy.

1390
01:00:35,400 --> 01:00:37,960
Their heads explode, literally just explode.

1391
01:00:38,960 --> 01:00:41,160
Over the next week or so,

1392
01:00:41,160 --> 01:00:43,560
they rebuild, keeping them in the barium,

1393
01:00:43,560 --> 01:00:45,280
they rebuild a brand new head.

1394
01:00:45,280 --> 01:00:47,040
The new head doesn't care about barium at all.

1395
01:00:47,040 --> 01:00:48,600
So we asked the simple question, how can that be?

1396
01:00:48,600 --> 01:00:49,960
What is the new head doing

1397
01:00:49,960 --> 01:00:51,600
that the original head couldn't do?

1398
01:00:51,600 --> 01:00:54,080
And we found out that there's actually very few genes

1399
01:00:54,080 --> 01:00:56,240
that the system up and down regulated

1400
01:00:56,240 --> 01:00:59,320
to be able to do its business in the presence of barium.

1401
01:00:59,320 --> 01:01:02,520
The kicker is, planaria never get exposed to barium

1402
01:01:03,400 --> 01:01:04,480
in the real world.

1403
01:01:04,480 --> 01:01:07,240
There is no ecological precedent for this.

1404
01:01:07,240 --> 01:01:09,880
So just imagine, you're a cell,

1405
01:01:09,880 --> 01:01:12,280
you've got, I don't know, tens of thousands

1406
01:01:12,280 --> 01:01:15,000
of possible genes, you've got a disaster,

1407
01:01:15,000 --> 01:01:16,720
a physiological disaster.

1408
01:01:16,720 --> 01:01:19,080
You don't have time to try every combination.

1409
01:01:19,080 --> 01:01:22,320
There is no time to try everything

1410
01:01:22,320 --> 01:01:23,880
and whoever survives, survives.

1411
01:01:23,880 --> 01:01:25,960
These cells don't turn over that fast.

1412
01:01:25,960 --> 01:01:27,840
You have to solve this novel problem,

1413
01:01:27,840 --> 01:01:29,800
possibly by generalizing,

1414
01:01:29,800 --> 01:01:31,480
because you've never seen barium before,

1415
01:01:31,480 --> 01:01:33,160
but you have seen epilepsy before.

1416
01:01:33,160 --> 01:01:35,600
And barium excitability might look a little bit

1417
01:01:35,600 --> 01:01:36,440
like epilepsy.

1418
01:01:36,440 --> 01:01:38,200
And so maybe you can do some of the same things.

1419
01:01:38,200 --> 01:01:40,760
So this idea of solving novel problems

1420
01:01:40,760 --> 01:01:44,000
in physiological space is one example

1421
01:01:44,000 --> 01:01:45,480
of what biology can do.

1422
01:01:45,480 --> 01:01:46,840
But here's another example.

1423
01:01:46,840 --> 01:01:48,720
So this is how we all start

1424
01:01:48,720 --> 01:01:51,400
as a kind of a collection of early cells.

1425
01:01:51,400 --> 01:01:54,160
But this is a cross-section through a human torso.

1426
01:01:54,160 --> 01:01:56,120
Now look at the incredible order here, right?

1427
01:01:56,120 --> 01:01:59,560
All the tissues, the organs, everything is in the right place,

1428
01:01:59,560 --> 01:02:02,560
the right size and shape and relative to each other.

1429
01:02:02,560 --> 01:02:04,560
Where does that come from?

1430
01:02:04,560 --> 01:02:06,160
You might be tempted to say DNA,

1431
01:02:06,160 --> 01:02:08,160
but of course we can read genomes now

1432
01:02:08,160 --> 01:02:09,920
and what's in the DNA isn't any of that.

1433
01:02:09,920 --> 01:02:12,360
What's in the DNA is the sequence

1434
01:02:12,360 --> 01:02:14,600
of the micro level sort of hardware

1435
01:02:14,600 --> 01:02:16,200
that every cell gets to have, the proteins.

1436
01:02:16,200 --> 01:02:17,680
That's what the DNA specified.

1437
01:02:17,680 --> 01:02:20,440
So you really, you still need to understand the physiology

1438
01:02:20,440 --> 01:02:23,640
by which these cells compute what to do here.

1439
01:02:23,640 --> 01:02:25,080
And then there are lots of questions,

1440
01:02:25,080 --> 01:02:26,640
as regenerative medicine workers,

1441
01:02:26,640 --> 01:02:29,040
we try to figure out what do we say to these cells

1442
01:02:29,040 --> 01:02:31,720
to rebuild pieces that are missing?

1443
01:02:31,720 --> 01:02:34,120
And as engineers, we want to know what's actually possible?

1444
01:02:34,120 --> 01:02:35,440
What can you reprogram this?

1445
01:02:35,440 --> 01:02:37,040
Can you make them do something else?

1446
01:02:37,040 --> 01:02:38,600
So the amazing thing about development

1447
01:02:38,600 --> 01:02:42,200
is that while it is incredibly reliable and robust

1448
01:02:42,200 --> 01:02:44,360
and in fact hides all of its intelligence from us

1449
01:02:44,360 --> 01:02:47,000
when we see acorns giving rights to oak trees

1450
01:02:47,000 --> 01:02:49,560
and frog eggs make frogs, we sort of assume,

1451
01:02:49,560 --> 01:02:50,520
well, what else is it gonna do?

1452
01:02:50,520 --> 01:02:51,440
Like that's obvious, right?

1453
01:02:51,440 --> 01:02:52,800
That's how it has to be.

1454
01:02:52,800 --> 01:02:55,480
But that's only what happens on the default condition.

1455
01:02:55,480 --> 01:02:57,040
What we find out is that, for example,

1456
01:02:57,040 --> 01:02:59,480
if you take an early embryo and cut it in half,

1457
01:02:59,480 --> 01:03:00,840
you don't get two half bodies.

1458
01:03:00,840 --> 01:03:03,320
You get two perfectly normal monosygotic twins.

1459
01:03:03,320 --> 01:03:05,480
And in fact, more generally,

1460
01:03:05,480 --> 01:03:09,120
the process of development can navigate this anatomical space

1461
01:03:09,120 --> 01:03:10,880
in a way to reach the same goal

1462
01:03:10,880 --> 01:03:12,680
from different starting positions

1463
01:03:12,680 --> 01:03:15,120
despite really drastic perturbations

1464
01:03:15,120 --> 01:03:17,040
by taking different paths.

1465
01:03:17,040 --> 01:03:19,480
It's not just a hardwired set of emergent.

1466
01:03:19,480 --> 01:03:20,640
This is not about emergence.

1467
01:03:20,640 --> 01:03:22,680
Of course, complex things emerge from simple rules.

1468
01:03:22,680 --> 01:03:23,800
This isn't that at all.

1469
01:03:23,800 --> 01:03:26,720
This is the ability of the system to get to its goal

1470
01:03:26,720 --> 01:03:29,520
despite really, really radical changes.

1471
01:03:29,520 --> 01:03:31,040
So here's one change, here's another change.

1472
01:03:31,040 --> 01:03:34,840
As an adult, some organisms like the salamander,

1473
01:03:34,840 --> 01:03:37,520
they regenerate their eyes, their limbs, their jaws,

1474
01:03:37,520 --> 01:03:41,360
their tails, you can make cuts anywhere you like along here.

1475
01:03:41,360 --> 01:03:43,920
And these cells will very rapidly grow

1476
01:03:43,920 --> 01:03:47,720
and undergo morphogenesis, and then they will stop.

1477
01:03:47,720 --> 01:03:48,840
When do they stop?

1478
01:03:48,840 --> 01:03:51,920
They stop when a correct salamander limb has formed.

1479
01:03:51,920 --> 01:03:53,080
Doesn't matter where you cut it,

1480
01:03:53,080 --> 01:03:54,920
it will only grow exactly the right amount

1481
01:03:54,960 --> 01:03:58,120
and it will stop when exactly the right thing has formed.

1482
01:03:58,120 --> 01:04:00,640
So you've got some sort of error minimization scheme

1483
01:04:00,640 --> 01:04:01,480
going on here.

1484
01:04:01,480 --> 01:04:02,640
It knows exactly what it looks like.

1485
01:04:02,640 --> 01:04:05,240
It knows what the target state is.

1486
01:04:05,240 --> 01:04:08,320
And in fact, this is something that we discovered

1487
01:04:08,320 --> 01:04:11,040
that, so this is a tadpole here, some eyes,

1488
01:04:11,040 --> 01:04:12,840
here's the brain, the gut, the nostrils.

1489
01:04:12,840 --> 01:04:15,640
These tadpoles have to become frogs.

1490
01:04:15,640 --> 01:04:17,680
In order to become frogs, they have to rearrange their face.

1491
01:04:17,680 --> 01:04:19,480
The jaws have to move, the nostrils have to,

1492
01:04:19,480 --> 01:04:20,680
everything has to move.

1493
01:04:20,680 --> 01:04:22,360
We find, and so you might imagine

1494
01:04:22,360 --> 01:04:26,720
that this is some sort of hardwired set of emergent outcomes

1495
01:04:26,720 --> 01:04:28,200
where every organ gets displaced

1496
01:04:28,200 --> 01:04:30,440
to its appropriate distance and direction.

1497
01:04:30,440 --> 01:04:32,840
So we made what's called Picasso tadpoles.

1498
01:04:32,840 --> 01:04:34,680
Basically, we scrambled everything

1499
01:04:34,680 --> 01:04:36,040
so that everything's in the wrong place.

1500
01:04:36,040 --> 01:04:37,480
The eyes are off to the side of the head,

1501
01:04:37,480 --> 01:04:38,960
the jaws are on the other side of everything

1502
01:04:38,960 --> 01:04:40,200
is just scrambled.

1503
01:04:40,200 --> 01:04:41,560
Because we have this hypothesis

1504
01:04:41,560 --> 01:04:45,680
that this is more intelligent than people gave a credit for.

1505
01:04:45,680 --> 01:04:50,000
Sure enough, what these guys do is every structure moves

1506
01:04:50,040 --> 01:04:52,320
in novel paths and keeps moving,

1507
01:04:52,320 --> 01:04:53,680
no matter where it started from,

1508
01:04:53,680 --> 01:04:56,680
until it gets to be a pretty normal looking frog.

1509
01:04:56,680 --> 01:04:59,080
So what the genetics gives you is not a piece of hardware

1510
01:04:59,080 --> 01:05:00,720
that does the same thing all the time.

1511
01:05:00,720 --> 01:05:04,920
It gives you a machine that can recognize unexpected changes

1512
01:05:04,920 --> 01:05:06,880
and take corrective action as needed

1513
01:05:06,880 --> 01:05:08,520
to get to the same goal.

1514
01:05:08,520 --> 01:05:11,240
The most amazing part of this is that in doing this,

1515
01:05:11,240 --> 01:05:13,560
and this is an example of top-down causation,

1516
01:05:13,560 --> 01:05:16,360
which is why it's really important to understand this,

1517
01:05:16,560 --> 01:05:18,720
how high level goals filter down

1518
01:05:18,720 --> 01:05:21,360
to the sort of implementation machinery,

1519
01:05:21,360 --> 01:05:24,400
is that what you see is that this is an example

1520
01:05:24,400 --> 01:05:27,480
from the kidney tubule of a nut.

1521
01:05:27,480 --> 01:05:28,960
If you take it in cross-section,

1522
01:05:28,960 --> 01:05:32,000
normally there's, I don't know, eight or nine cells

1523
01:05:32,000 --> 01:05:35,000
that work together to make the lumen of that tubule.

1524
01:05:35,000 --> 01:05:38,000
But one thing you can do is you can force these cells

1525
01:05:38,000 --> 01:05:39,160
to be gigantic.

1526
01:05:39,160 --> 01:05:41,400
And when you do this, when you make them larger,

1527
01:05:41,400 --> 01:05:43,480
fewer cells will do this,

1528
01:05:43,480 --> 01:05:46,000
forming exactly the same lumen diameter

1529
01:05:46,000 --> 01:05:47,960
until you make the cells so large

1530
01:05:47,960 --> 01:05:50,600
that a single cell will wrap around itself

1531
01:05:50,600 --> 01:05:53,680
to give you the same structure.

1532
01:05:53,680 --> 01:05:55,520
What's amazing about that is that these are completely

1533
01:05:55,520 --> 01:05:57,040
different molecular mechanisms.

1534
01:05:57,040 --> 01:05:58,760
This is cell-to-cell communication.

1535
01:05:58,760 --> 01:06:00,360
This is cytoskeletal bending.

1536
01:06:00,360 --> 01:06:02,920
So in the service of a high-level goal,

1537
01:06:02,920 --> 01:06:05,440
meaning make this large-scale anatomical structure,

1538
01:06:05,440 --> 01:06:09,360
different molecular mechanisms get activated, okay?

1539
01:06:09,360 --> 01:06:12,360
And this is very unusual.

1540
01:06:12,360 --> 01:06:14,080
This idea is very unusual in biology.

1541
01:06:15,080 --> 01:06:17,440
The biologists tend to think about things emerging

1542
01:06:17,440 --> 01:06:20,560
from molecules not going the other way,

1543
01:06:20,560 --> 01:06:23,000
but it has certain parallels in computer science

1544
01:06:23,000 --> 01:06:25,760
where the algorithm makes the electrons dance

1545
01:06:25,760 --> 01:06:26,920
in an important way, right?

1546
01:06:26,920 --> 01:06:29,000
In a functionally important way.

1547
01:06:29,000 --> 01:06:33,320
And so what we've been doing is trying to build models

1548
01:06:33,320 --> 01:06:35,720
that go, sort of full-stack models that go all the way up

1549
01:06:35,720 --> 01:06:39,160
from molecular kinds of activities

1550
01:06:39,160 --> 01:06:43,360
that set the ion channels and other things in the membrane.

1551
01:06:43,360 --> 01:06:45,920
Two, we specifically, I don't have too much time today,

1552
01:06:45,920 --> 01:06:48,000
but we specifically study bioelectrics.

1553
01:06:48,000 --> 01:06:50,680
We study how all cells, not just neurons,

1554
01:06:50,680 --> 01:06:52,840
all cells use electrical signaling

1555
01:06:52,840 --> 01:06:55,400
to form computational networks.

1556
01:06:55,400 --> 01:06:59,680
And so we study what the tissue-level electrical patterns

1557
01:06:59,680 --> 01:07:03,040
look like and then what the organ-level patterns look like

1558
01:07:03,040 --> 01:07:07,000
and then how that becomes literally an algorithmic set

1559
01:07:07,000 --> 01:07:09,320
of steps that determines things like how many heads

1560
01:07:09,320 --> 01:07:11,040
a flower is going to have.

1561
01:07:11,080 --> 01:07:15,520
And during this process, we want to know a few things.

1562
01:07:15,520 --> 01:07:18,320
We want to know how does the cognitive light cone,

1563
01:07:18,320 --> 01:07:21,880
and what I mean by that is simply the spatiotemporal size,

1564
01:07:21,880 --> 01:07:24,080
the scale of the largest goal

1565
01:07:24,080 --> 01:07:27,280
that that particular system can conceive of pursuing, right?

1566
01:07:27,280 --> 01:07:28,320
So if you're a bacterium,

1567
01:07:28,320 --> 01:07:29,920
your cognitive light cone is very tiny

1568
01:07:29,920 --> 01:07:31,440
because really all you care about

1569
01:07:31,440 --> 01:07:33,120
is the local sugar concentration

1570
01:07:33,120 --> 01:07:36,840
with about maybe 10 minutes forward and back.

1571
01:07:36,840 --> 01:07:39,360
But if you're a human, you can have gigantic goals

1572
01:07:39,360 --> 01:07:40,480
that exceed your lifespan.

1573
01:07:40,480 --> 01:07:41,840
It can be planetary-scale goals.

1574
01:07:41,840 --> 01:07:44,840
And then of course, every kind of creature in between.

1575
01:07:44,840 --> 01:07:46,840
So we define this kind of cognitive light cone

1576
01:07:46,840 --> 01:07:50,640
based around the types of goals that a system can pursue.

1577
01:07:50,640 --> 01:07:52,640
And so we need to understand during this process,

1578
01:07:52,640 --> 01:07:54,240
how do the goals enlarge?

1579
01:07:54,240 --> 01:07:56,640
How do they shift into different spaces?

1580
01:07:56,640 --> 01:07:59,600
So individual cells care about things in metabolic space

1581
01:07:59,600 --> 01:08:01,840
and physiological space and transcriptional space.

1582
01:08:01,840 --> 01:08:02,760
Those are their goals.

1583
01:08:02,760 --> 01:08:05,520
Collectives of cells care about very much larger goals,

1584
01:08:05,520 --> 01:08:07,200
such as the shape of your hand

1585
01:08:07,200 --> 01:08:08,560
and the fact that you have to have to have

1586
01:08:08,560 --> 01:08:10,240
exactly five fingers.

1587
01:08:10,240 --> 01:08:12,440
And then of course, this question of where do these goals come

1588
01:08:12,440 --> 01:08:13,280
from in the first place,

1589
01:08:13,280 --> 01:08:14,840
we'll address that momentarily.

1590
01:08:14,840 --> 01:08:17,240
So about the only piece of bioelectricity,

1591
01:08:17,240 --> 01:08:19,760
I'm gonna show you because Yosha brought up this idea

1592
01:08:19,760 --> 01:08:23,080
of counterfactual memories is simply this.

1593
01:08:24,080 --> 01:08:27,440
We treat the behavior of the cells and tissues

1594
01:08:27,440 --> 01:08:29,160
as a collective intelligence.

1595
01:08:29,160 --> 01:08:31,840
Literally the group of cells is a collective intelligence

1596
01:08:31,840 --> 01:08:34,960
that tries to solve problems in anatomical space.

1597
01:08:34,960 --> 01:08:37,320
And because we have some understanding now

1598
01:08:37,320 --> 01:08:40,160
of what the medium is of that collective intelligence,

1599
01:08:40,160 --> 01:08:42,880
not shockingly just like in the brain, it's bioelectric.

1600
01:08:42,880 --> 01:08:46,040
Why? Because that's how the brain learned its tricks.

1601
01:08:46,040 --> 01:08:47,800
You already heard and Yosha is absolutely right.

1602
01:08:47,800 --> 01:08:52,800
There are very difficult tasks to try to distinguish

1603
01:08:54,480 --> 01:08:56,440
what makes a neuron different from other cells

1604
01:08:56,440 --> 01:09:00,360
because even bacteria from the time of microbial biofilms

1605
01:09:00,360 --> 01:09:03,040
have already been using all of the same tricks

1606
01:09:03,040 --> 01:09:07,320
as the brain uses, this electrical network stuff is ancient.

1607
01:09:07,320 --> 01:09:09,680
And so what we are able to do is read and write

1608
01:09:09,680 --> 01:09:13,200
the memories of this collective intelligence.

1609
01:09:13,200 --> 01:09:15,080
And so we use a specific technique

1610
01:09:15,080 --> 01:09:16,880
that reads the electrical gradients.

1611
01:09:16,880 --> 01:09:18,640
This is just like neural decoding

1612
01:09:18,640 --> 01:09:20,720
as the neuroscientists try to do in the brain.

1613
01:09:20,720 --> 01:09:22,960
So here there's a particular pattern that says,

1614
01:09:22,960 --> 01:09:25,240
if injured, you're going to make one head.

1615
01:09:25,240 --> 01:09:27,480
We can rewrite that and we can create a worm.

1616
01:09:27,480 --> 01:09:29,440
Here is where the pattern says,

1617
01:09:29,440 --> 01:09:32,080
no, actually a correct worm should have two heads.

1618
01:09:32,080 --> 01:09:34,600
And if you go ahead and cut that animal,

1619
01:09:34,600 --> 01:09:36,200
they will go ahead and make two heads.

1620
01:09:36,200 --> 01:09:37,920
This is not Photoshop, these are real,

1621
01:09:37,920 --> 01:09:39,600
real two-headed malaria.

1622
01:09:39,600 --> 01:09:41,000
But the cool thing about this pattern

1623
01:09:41,000 --> 01:09:43,000
is this is not a reading of this animal.

1624
01:09:43,000 --> 01:09:45,640
This is a reading of this perfectly normal,

1625
01:09:45,640 --> 01:09:48,720
anatomically one-headed, genetically,

1626
01:09:48,720 --> 01:09:50,640
transcriptionally one-headed animal.

1627
01:09:50,640 --> 01:09:52,720
So this is a kind of counterfactual memory.

1628
01:09:52,720 --> 01:09:54,760
It's a representation of a state

1629
01:09:54,760 --> 01:09:57,320
that it's what you are going to do in the future

1630
01:09:57,320 --> 01:09:58,320
if you get injured.

1631
01:09:58,320 --> 01:09:59,840
If you don't get injured, it stays latent,

1632
01:09:59,840 --> 01:10:00,960
it never comes up.

1633
01:10:01,840 --> 01:10:03,520
And we have lots more data on this.

1634
01:10:03,520 --> 01:10:06,800
We can actually make heads of other species of worms

1635
01:10:06,800 --> 01:10:08,160
and many other things.

1636
01:10:08,160 --> 01:10:09,960
The idea is that a single body can

1637
01:10:09,960 --> 01:10:12,960
store one of two different representations

1638
01:10:12,960 --> 01:10:15,800
of what the goal state is going to be if they get injured

1639
01:10:15,800 --> 01:10:17,680
and then they build to that goal state.

1640
01:10:17,680 --> 01:10:19,040
So this should sound very familiar.

1641
01:10:19,040 --> 01:10:21,520
This is both the nervous system works this way

1642
01:10:21,520 --> 01:10:23,520
and of course, reprogrammable devices work this way.

1643
01:10:23,520 --> 01:10:26,440
The same hardware can hold onto multiple

1644
01:10:26,440 --> 01:10:27,920
computational goal states.

1645
01:10:27,920 --> 01:10:31,920
Now, let's go back to where we started with this,

1646
01:10:31,920 --> 01:10:36,120
which is this notion of scaling up from components.

1647
01:10:36,120 --> 01:10:38,960
So here's your single cell.

1648
01:10:38,960 --> 01:10:41,160
What evolution has done is allowed these cells

1649
01:10:41,160 --> 01:10:45,800
to merge into networks that are able to store

1650
01:10:45,800 --> 01:10:47,280
much larger goal states.

1651
01:10:47,280 --> 01:10:50,040
So this guy only cares about his own physiology

1652
01:10:50,040 --> 01:10:51,880
and his own metabolic.

1653
01:10:51,880 --> 01:10:56,560
This collection of cells is very competent

1654
01:10:56,600 --> 01:10:58,240
in reaching a particular region

1655
01:10:58,240 --> 01:11:00,400
of anatomical morpho space that looks like this.

1656
01:11:00,400 --> 01:11:02,600
The goal is huge at centimeters in size.

1657
01:11:02,600 --> 01:11:04,040
And if it's deviated from that,

1658
01:11:04,040 --> 01:11:05,600
it will do its best to come back

1659
01:11:05,600 --> 01:11:09,040
to even with the kind of drastic interventions.

1660
01:11:09,040 --> 01:11:10,920
But that process has a failure mode.

1661
01:11:10,920 --> 01:11:12,400
That failure mode is known as cancer.

1662
01:11:12,400 --> 01:11:13,240
What happens?

1663
01:11:13,240 --> 01:11:15,360
This is human glioblastoma cells.

1664
01:11:15,360 --> 01:11:17,800
If individual cells get disconnected

1665
01:11:17,800 --> 01:11:20,320
from this electrical and other signals as well,

1666
01:11:20,320 --> 01:11:22,160
from this network that binds them

1667
01:11:22,160 --> 01:11:26,120
towards a common journey in that space, that common goal,

1668
01:11:26,160 --> 01:11:29,000
they revert back to their evolutionarily ancient self.

1669
01:11:29,000 --> 01:11:30,360
What is the goal of a single cell?

1670
01:11:30,360 --> 01:11:31,680
Well, it's to become two cells

1671
01:11:31,680 --> 01:11:33,040
and to go wherever life is good.

1672
01:11:33,040 --> 01:11:34,360
That's metastasis.

1673
01:11:34,360 --> 01:11:37,960
And so you can see how what happens with these cancer cells

1674
01:11:37,960 --> 01:11:41,480
is they're not any more selfish than any other cell.

1675
01:11:41,480 --> 01:11:43,640
They're just their cells are smaller.

1676
01:11:43,640 --> 01:11:44,760
And we've talked, you know,

1677
01:11:44,760 --> 01:11:47,520
I talked to roboticists and folks like that

1678
01:11:47,520 --> 01:11:49,800
with this idea that why don't robots get cancer?

1679
01:11:49,800 --> 01:11:50,640
Right?

1680
01:11:50,640 --> 01:11:52,640
The reason that our current technology isn't prone to this

1681
01:11:52,640 --> 01:11:55,160
is because we do not have a multi-scale architecture

1682
01:11:55,160 --> 01:11:57,200
where the components have their own goals.

1683
01:11:57,200 --> 01:12:00,640
That we have some fairly dumb components typically.

1684
01:12:00,640 --> 01:12:02,360
And then we hope that the collective

1685
01:12:02,360 --> 01:12:03,640
that has some kind of, you know,

1686
01:12:03,640 --> 01:12:05,040
is doing some kind of computation,

1687
01:12:05,040 --> 01:12:07,080
but the parts are not trying to do anything.

1688
01:12:07,080 --> 01:12:08,160
Biology isn't like that.

1689
01:12:08,160 --> 01:12:11,040
Every component will do interesting things

1690
01:12:11,040 --> 01:12:13,280
if freed from its neighbors.

1691
01:12:13,280 --> 01:12:14,240
And I'll show you that.

1692
01:12:14,240 --> 01:12:15,560
But of course, you know, biomedically,

1693
01:12:15,560 --> 01:12:18,080
we can sort of take this kind of weird way

1694
01:12:18,080 --> 01:12:19,240
of looking at things and ask,

1695
01:12:19,240 --> 01:12:24,080
can we simply enlarge the boundary of the self?

1696
01:12:24,080 --> 01:12:27,680
Enlarge the border between self and outside world.

1697
01:12:27,680 --> 01:12:28,600
And so you can do that.

1698
01:12:28,600 --> 01:12:30,240
We have techniques to do that

1699
01:12:30,240 --> 01:12:35,240
where when we inject particular human oncogenes

1700
01:12:35,440 --> 01:12:37,560
into these tadpoles to make tumors,

1701
01:12:37,560 --> 01:12:39,800
and you can already see this is voltage imaging,

1702
01:12:39,800 --> 01:12:43,360
you can see that these cells are already starting to defect.

1703
01:12:43,360 --> 01:12:44,360
As far as they're concerned,

1704
01:12:44,360 --> 01:12:47,480
the rest of the animal is just outside environment.

1705
01:12:47,480 --> 01:12:49,080
So that's something else that Miocha mentioned

1706
01:12:49,080 --> 01:12:51,280
is this idea of being in conflict or not

1707
01:12:51,280 --> 01:12:52,200
with your environment.

1708
01:12:52,200 --> 01:12:55,840
It's never obvious to a new agent what the environment is.

1709
01:12:55,840 --> 01:12:59,240
Every cell is some other cells' external environment.

1710
01:12:59,240 --> 01:13:02,040
And so normally all of these cells believe

1711
01:13:02,040 --> 01:13:04,760
that the water out here is the external environment.

1712
01:13:04,760 --> 01:13:07,920
But once you disconnect them using these oncogenes,

1713
01:13:07,920 --> 01:13:09,800
then as far as the cells are concerned,

1714
01:13:09,800 --> 01:13:11,600
all of this stuff is external environment.

1715
01:13:11,600 --> 01:13:12,800
They don't care what happens to that.

1716
01:13:12,800 --> 01:13:14,200
They're gonna do their best.

1717
01:13:14,200 --> 01:13:15,560
They're gonna live their best life.

1718
01:13:15,560 --> 01:13:17,880
They're gonna dump entropy into the environment.

1719
01:13:17,880 --> 01:13:20,640
And of course, that's maladaptive for the organism.

1720
01:13:20,640 --> 01:13:22,880
But one thing you can do is you can force

1721
01:13:22,880 --> 01:13:24,160
using specific techniques,

1722
01:13:24,160 --> 01:13:26,360
including optogenetics and some other things,

1723
01:13:26,360 --> 01:13:29,600
you can force these cells to remain in electrical,

1724
01:13:29,600 --> 01:13:31,600
in the correct electrical state with their neighbors.

1725
01:13:31,600 --> 01:13:33,360
And if you do that, even though the oncogene,

1726
01:13:33,360 --> 01:13:34,400
this is the same animal here,

1727
01:13:34,400 --> 01:13:36,160
even though the oncogene is very strong,

1728
01:13:36,160 --> 01:13:41,160
there's no tumor because the hardware problem

1729
01:13:41,200 --> 01:13:42,680
isn't really fundamental.

1730
01:13:42,680 --> 01:13:44,240
It's the software that's fundamental.

1731
01:13:44,240 --> 01:13:47,240
It's are these cells working on a large goal

1732
01:13:47,240 --> 01:13:50,360
like making a nice liver and muscle and skin and whatever?

1733
01:13:50,360 --> 01:13:53,480
Or are they individual cells working on individual goals?

1734
01:13:53,480 --> 01:13:55,160
So we spend a lot of time thinking

1735
01:13:55,160 --> 01:13:56,000
about these kinds of things.

1736
01:13:56,000 --> 01:14:00,040
How do we, what are the mechanisms, of course,

1737
01:14:00,040 --> 01:14:03,800
but also algorithms, policies for connecting up

1738
01:14:03,800 --> 01:14:06,400
little tiny homeostats,

1739
01:14:06,400 --> 01:14:08,560
these cells that like to keep certain states

1740
01:14:08,560 --> 01:14:10,400
into much larger networks

1741
01:14:10,400 --> 01:14:11,960
that then have these interesting properties

1742
01:14:11,960 --> 01:14:13,640
that of course people in the connectionless world

1743
01:14:13,640 --> 01:14:15,000
have been studying for a really long time.

1744
01:14:15,000 --> 01:14:17,520
So in painting, out painting,

1745
01:14:17,520 --> 01:14:19,080
you know, all this kind of stuff.

1746
01:14:19,080 --> 01:14:22,000
So we can talk about our efforts

1747
01:14:22,000 --> 01:14:24,280
to sort of understand how the goals scale.

1748
01:14:24,280 --> 01:14:26,280
They scale from these really humble,

1749
01:14:26,280 --> 01:14:29,600
metabolic kinds of goals of individual cells, right?

1750
01:14:29,600 --> 01:14:33,320
These homeostatic loops into anatomical homeostasis,

1751
01:14:33,320 --> 01:14:35,280
eventually behavioral homeostasis

1752
01:14:35,280 --> 01:14:39,440
and behavioral clever motion through three-dimensional space

1753
01:14:39,440 --> 01:14:41,800
and eventually linguistic space and who knows what else.

1754
01:14:41,800 --> 01:14:43,480
So just for the last couple of minutes,

1755
01:14:43,480 --> 01:14:45,520
I just wanna show you one thing,

1756
01:14:45,520 --> 01:14:47,800
which is simply this.

1757
01:14:47,840 --> 01:14:52,840
In studying these kind of novel perturbations

1758
01:14:53,040 --> 01:14:55,920
and asking what are cells actually capable of?

1759
01:14:55,920 --> 01:14:59,120
What, you know, what other modes are there?

1760
01:14:59,920 --> 01:15:01,520
We asked the following thing

1761
01:15:01,520 --> 01:15:02,920
and I have to do a disclosure here

1762
01:15:02,920 --> 01:15:05,840
because Josh Bongard and I are co-founders

1763
01:15:05,840 --> 01:15:07,320
of this thing called Fauna Systems.

1764
01:15:07,320 --> 01:15:11,600
It's a biorebotics kind of company.

1765
01:15:11,600 --> 01:15:12,760
And so what we did in this,

1766
01:15:12,760 --> 01:15:15,520
all the biology was done by Doug Blackiston in my lab

1767
01:15:15,520 --> 01:15:17,800
and there was a lot of computer science here

1768
01:15:17,800 --> 01:15:20,480
done by Sam Kriegman in Josh's lab.

1769
01:15:20,480 --> 01:15:23,320
What we decided to do was to liberate cells

1770
01:15:23,320 --> 01:15:24,320
from the normal environment

1771
01:15:24,320 --> 01:15:27,200
and give them a chance to reboot their multicellularity.

1772
01:15:27,200 --> 01:15:28,600
How much creativity is there?

1773
01:15:28,600 --> 01:15:30,160
What else can they do?

1774
01:15:30,160 --> 01:15:33,480
And specifically, and I think somebody on the chat

1775
01:15:33,480 --> 01:15:35,760
asked this before, where do these goals come from?

1776
01:15:35,760 --> 01:15:36,920
So that's what we wanted to understand,

1777
01:15:36,920 --> 01:15:40,560
a completely novel creature that's never existed before.

1778
01:15:40,560 --> 01:15:41,520
What goals do they have?

1779
01:15:41,520 --> 01:15:42,720
Where do their goals come from?

1780
01:15:42,720 --> 01:15:45,040
Okay, and so I'm gonna just show you a couple of examples.

1781
01:15:45,040 --> 01:15:50,040
So what we did here is we took an early frog embryo

1782
01:15:50,040 --> 01:15:53,760
and so what Doug does is he takes all of these cells up here

1783
01:15:53,760 --> 01:15:57,000
which are skin, they're basically determined to be skin

1784
01:15:57,000 --> 01:15:58,800
and he dissociates them

1785
01:15:58,800 --> 01:16:01,280
and puts them into a little depression here.

1786
01:16:01,280 --> 01:16:03,440
Now, there are many things that they could have done

1787
01:16:03,440 --> 01:16:04,760
after that, they could die,

1788
01:16:04,760 --> 01:16:07,040
they could spread out and sort of walk away from each other,

1789
01:16:07,040 --> 01:16:09,120
they could form a flat two-dimensional monolayer

1790
01:16:09,120 --> 01:16:10,880
the way that cell culture does.

1791
01:16:10,880 --> 01:16:13,880
Instead, what happens is this, and this is time lapse,

1792
01:16:13,880 --> 01:16:16,800
of course, so overnight, these guys will get together

1793
01:16:16,800 --> 01:16:19,440
and they will coalesce into this interesting

1794
01:16:19,440 --> 01:16:22,840
little thing here and what is it?

1795
01:16:22,840 --> 01:16:24,280
Well, we call this a Xenobot,

1796
01:16:24,280 --> 01:16:25,920
Xenopus laevis is the name of the frog

1797
01:16:25,920 --> 01:16:27,680
and it's a biobot, so Xenobot.

1798
01:16:28,560 --> 01:16:30,840
What it's doing is it's using the little hairs

1799
01:16:30,840 --> 01:16:32,800
on its surface, these hairs are normally there

1800
01:16:32,800 --> 01:16:35,320
to spread mucus down the body of the frog.

1801
01:16:35,320 --> 01:16:38,400
What they've done is repurpose those hairs for swimming.

1802
01:16:38,400 --> 01:16:40,440
So here it goes, it's chugging along,

1803
01:16:40,440 --> 01:16:42,200
you can see that as they can go in circles,

1804
01:16:42,200 --> 01:16:44,120
they can sort of patrol back and forth like this,

1805
01:16:44,120 --> 01:16:45,800
they can have group behaviors,

1806
01:16:45,800 --> 01:16:47,560
this one's going on kind of a long journey,

1807
01:16:47,560 --> 01:16:48,760
these are interacting together,

1808
01:16:48,760 --> 01:16:50,920
these are having an arrest.

1809
01:16:50,920 --> 01:16:52,760
Here's what it does in a maze,

1810
01:16:52,760 --> 01:16:55,320
so you can see it swims along,

1811
01:16:55,320 --> 01:16:57,800
it's gonna take a turn here without having to bump

1812
01:16:57,800 --> 01:16:59,880
into this outside wall, so it takes a turn.

1813
01:16:59,880 --> 01:17:02,120
And then at this point, for some internal reason,

1814
01:17:02,120 --> 01:17:03,240
we have no idea about it,

1815
01:17:03,240 --> 01:17:05,360
it decides to turn around and go back where it came from.

1816
01:17:05,360 --> 01:17:08,760
Okay, so there's all sorts of primitive kinds of dynamics.

1817
01:17:08,760 --> 01:17:11,840
Just keep in mind, even though these things have,

1818
01:17:11,840 --> 01:17:13,480
this is calcium signaling you see,

1819
01:17:13,480 --> 01:17:16,480
it's the kind of thing you see when you do brain imaging,

1820
01:17:16,480 --> 01:17:18,360
there are no neurons here, this is just skin.

1821
01:17:18,360 --> 01:17:20,960
This whole thing is just skin cells,

1822
01:17:20,960 --> 01:17:23,360
but they're doing a lot of, calcium readout

1823
01:17:23,360 --> 01:17:26,120
is a great readout of computation.

1824
01:17:26,120 --> 01:17:28,560
And could they be saying something to each other?

1825
01:17:28,560 --> 01:17:29,400
Of course we don't know,

1826
01:17:29,400 --> 01:17:32,960
this is still a very much ongoing subject of investigation,

1827
01:17:32,960 --> 01:17:37,600
but one of the amazing things that Doug and Sam discovered

1828
01:17:37,600 --> 01:17:40,520
is that their computational models of these guys

1829
01:17:40,520 --> 01:17:43,320
make predictions that differently shaped bots

1830
01:17:43,320 --> 01:17:45,120
are going to rearrange their environment

1831
01:17:45,120 --> 01:17:47,720
in different ways, so they did a lot of simulations.

1832
01:17:47,720 --> 01:17:50,880
And so then we tried it and we just did it in vivo.

1833
01:17:50,880 --> 01:17:52,000
And here's what we found.

1834
01:17:52,000 --> 01:17:55,560
So here are the bots, the white stuff here is their cells,

1835
01:17:55,560 --> 01:17:58,760
they're loose skin cells that we sprinkled into the dish.

1836
01:17:58,760 --> 01:18:00,240
And what they're basically doing,

1837
01:18:00,240 --> 01:18:03,280
because we made it impossible for them to reproduce

1838
01:18:03,280 --> 01:18:05,320
in the normal froggy fashion,

1839
01:18:05,320 --> 01:18:07,680
they are basically implementing von Neumann's dream,

1840
01:18:07,680 --> 01:18:11,400
they are constructing other,

1841
01:18:11,400 --> 01:18:12,840
what they do is they run around

1842
01:18:12,840 --> 01:18:15,160
and they sort of collect these skin cells

1843
01:18:15,160 --> 01:18:19,040
into little piles, then they kind of polish the piles.

1844
01:18:19,040 --> 01:18:20,720
And these piles, because they're working

1845
01:18:20,720 --> 01:18:22,480
with an agential material,

1846
01:18:22,480 --> 01:18:24,200
they're not working with passive particles,

1847
01:18:24,200 --> 01:18:26,360
they're working with cells, what do these cells like to do?

1848
01:18:26,360 --> 01:18:29,080
They like to become the Xenobot.

1849
01:18:29,080 --> 01:18:31,480
And so of course they create the next generation of Xenobot,

1850
01:18:31,480 --> 01:18:35,080
which then matures and guess what, it does the same thing

1851
01:18:35,080 --> 01:18:36,800
and then you get the next generation and so on.

1852
01:18:36,840 --> 01:18:39,160
So this is kinematic self-replication,

1853
01:18:39,160 --> 01:18:42,000
and they'll make multiple generations of this.

1854
01:18:42,000 --> 01:18:46,320
So here's here a couple of interesting corollaries

1855
01:18:46,320 --> 01:18:48,800
to this and then I'm almost done.

1856
01:18:48,800 --> 01:18:50,120
The exact same genome,

1857
01:18:50,120 --> 01:18:52,800
so here's the specification of the micro level hardware,

1858
01:18:52,800 --> 01:18:54,920
this is what every cell gets to have,

1859
01:18:54,920 --> 01:18:57,720
can do one of two things under normal circumstances,

1860
01:18:57,720 --> 01:19:00,200
it will do this, it has this developmental sequence,

1861
01:19:00,200 --> 01:19:02,640
then it makes these tadpoles that do various things.

1862
01:19:02,640 --> 01:19:05,360
But under other circumstances, it makes this,

1863
01:19:05,400 --> 01:19:07,720
this is a Xenobot, this is a developmental sequence,

1864
01:19:07,720 --> 01:19:10,600
this is I think a month old or something Xenobot,

1865
01:19:10,600 --> 01:19:12,280
where did the shape come from, right?

1866
01:19:12,280 --> 01:19:13,680
And they have a different behavior

1867
01:19:13,680 --> 01:19:15,880
with this thing called kinematic self-replication.

1868
01:19:15,880 --> 01:19:18,840
So here's a few interesting things, number one.

1869
01:19:18,840 --> 01:19:21,440
Typically when you talk about why a certain creature

1870
01:19:21,440 --> 01:19:24,240
has certain capacities, everybody leans on evolution.

1871
01:19:24,240 --> 01:19:27,760
Well, for eons, it was selected to do this or that.

1872
01:19:27,760 --> 01:19:29,320
Well, there's never been any Xenobots,

1873
01:19:29,320 --> 01:19:30,800
there's never been any selective pressure

1874
01:19:30,800 --> 01:19:33,160
to be a good Xenobot, this is completely emergent.

1875
01:19:33,160 --> 01:19:37,800
They do this, they form this coherent kind of system

1876
01:19:37,800 --> 01:19:42,320
with new behaviors, both anatomically and with mortality,

1877
01:19:42,320 --> 01:19:44,880
basically overnight, this has never been selected

1878
01:19:44,880 --> 01:19:48,160
for specifically, they're completely new in the biosphere.

1879
01:19:48,160 --> 01:19:50,880
As far as we know, no other living creature

1880
01:19:50,880 --> 01:19:53,520
does kinematic self-replication, that's the first thing.

1881
01:19:53,520 --> 01:19:56,440
The second thing is that, how did we engineer these?

1882
01:19:56,440 --> 01:19:59,440
I mean, there are no trans genes here.

1883
01:19:59,440 --> 01:20:01,200
So the, if you sequence this, all you see

1884
01:20:01,200 --> 01:20:03,160
is normal Xenopus latus, there's nothing wrong

1885
01:20:03,160 --> 01:20:04,800
with the genome, it's wild type.

1886
01:20:04,800 --> 01:20:09,240
There are no nanomaterials, some of them,

1887
01:20:09,240 --> 01:20:12,520
some of them Doug can make some modifications

1888
01:20:12,520 --> 01:20:15,040
to them surgically, according to the AI

1889
01:20:15,040 --> 01:20:16,760
that Josh and Sam built.

1890
01:20:16,760 --> 01:20:19,720
But basically, the way we engineered these

1891
01:20:19,720 --> 01:20:22,520
is not by adding anything, it's by liberating them

1892
01:20:22,520 --> 01:20:25,400
from the influence of the other cells.

1893
01:20:25,400 --> 01:20:27,840
So normally, if you just look at the normal path

1894
01:20:27,840 --> 01:20:30,800
of this biological system, you would say,

1895
01:20:30,800 --> 01:20:32,520
what do the skin cells like to do?

1896
01:20:32,520 --> 01:20:35,080
Well, they like to be, and in fact, all they can be,

1897
01:20:35,080 --> 01:20:37,680
is to be the outside two-dimensional layer

1898
01:20:37,680 --> 01:20:38,840
of keeping out the bacteria.

1899
01:20:38,840 --> 01:20:40,120
It's very boring passive life,

1900
01:20:40,120 --> 01:20:42,600
they just sort of sit there and keep out the bacteria.

1901
01:20:42,600 --> 01:20:45,080
But that's only what happens when they're basically

1902
01:20:45,080 --> 01:20:46,960
bullied into it by the other cells.

1903
01:20:46,960 --> 01:20:49,720
It's behavior shaping, it's instructive interactions

1904
01:20:49,720 --> 01:20:52,480
from the other cells that tell them to sit quietly

1905
01:20:52,480 --> 01:20:53,560
and be the outer layer.

1906
01:20:53,560 --> 01:20:56,000
In the absence of all that stuff, liberated from all that,

1907
01:20:56,000 --> 01:20:58,400
they have a completely different default lifestyle.

1908
01:20:58,400 --> 01:21:00,200
And this is it, which you would not see

1909
01:21:00,800 --> 01:21:02,240
without this thing.

1910
01:21:02,240 --> 01:21:04,520
So, and we don't know what else,

1911
01:21:04,520 --> 01:21:05,840
certainly we're studying right now,

1912
01:21:05,840 --> 01:21:07,840
all the kinds of behavioral capacities,

1913
01:21:07,840 --> 01:21:09,480
do they learn, do they anticipate,

1914
01:21:09,480 --> 01:21:12,960
all sorts of things, I'm not making any claims yet about that.

1915
01:21:12,960 --> 01:21:17,760
So, but this idea of what evolution I think really does,

1916
01:21:17,760 --> 01:21:19,200
and we can talk about why,

1917
01:21:19,200 --> 01:21:21,840
I think we have now some ideas about why,

1918
01:21:21,840 --> 01:21:24,440
it doesn't produce solutions to specific problems,

1919
01:21:24,440 --> 01:21:27,400
it produces generic problem-solving machines.

1920
01:21:27,400 --> 01:21:31,120
And so the big thing that every living system has to do,

1921
01:21:31,120 --> 01:21:33,000
and I think these are, if I had to make a list,

1922
01:21:33,000 --> 01:21:36,080
these are some things that I think are required

1923
01:21:36,080 --> 01:21:37,440
for the kind of thing we want.

1924
01:21:37,440 --> 01:21:39,560
First of all, I think it's really important

1925
01:21:39,560 --> 01:21:41,360
that your parts have agendas.

1926
01:21:41,360 --> 01:21:42,680
It's not enough to have dumb parts

1927
01:21:42,680 --> 01:21:46,640
and try to engineer an agenda for the whole system.

1928
01:21:46,640 --> 01:21:50,280
You have to have a marketplace where every layer

1929
01:21:50,280 --> 01:21:52,720
is competing, cooperating,

1930
01:21:52,720 --> 01:21:55,880
and attempting to do its own thing.

1931
01:21:55,880 --> 01:21:57,400
You, of course, risk failure modes,

1932
01:21:57,400 --> 01:22:00,000
you risk parts trying to go off on their own.

1933
01:22:00,000 --> 01:22:02,040
That's one of the trade-offs,

1934
01:22:02,040 --> 01:22:03,600
but overall it becomes,

1935
01:22:03,600 --> 01:22:05,960
I think, an incredibly powerful architecture.

1936
01:22:07,000 --> 01:22:09,120
They have to emerge spontaneously.

1937
01:22:09,120 --> 01:22:13,120
That is real agents don't know where their boundaries are.

1938
01:22:13,120 --> 01:22:15,720
If you are a new embryo coming into the world,

1939
01:22:15,720 --> 01:22:18,400
you don't know how many cells you're going to have,

1940
01:22:18,400 --> 01:22:19,760
because we might remove half of them,

1941
01:22:19,760 --> 01:22:21,320
and you still have to make a good embryo.

1942
01:22:21,320 --> 01:22:23,320
You don't know how big your cells are,

1943
01:22:23,320 --> 01:22:26,080
because we might make gigantic cells or smaller cells.

1944
01:22:26,080 --> 01:22:28,920
You don't know exactly how many chromosomes

1945
01:22:28,920 --> 01:22:29,760
you're going to have,

1946
01:22:29,760 --> 01:22:34,080
because we can make all kinds of weird chimeras and so on.

1947
01:22:35,240 --> 01:22:37,480
You have to be able to, surviving life,

1948
01:22:37,480 --> 01:22:40,720
has to be able to play the hand and stealth from scratch.

1949
01:22:40,720 --> 01:22:44,360
You really can't take past experience too seriously.

1950
01:22:44,360 --> 01:22:46,280
You have to improvise on the fly.

1951
01:22:46,280 --> 01:22:48,520
This is what biology does.

1952
01:22:48,520 --> 01:22:49,920
So it does not have, nobody says,

1953
01:22:49,920 --> 01:22:51,600
this is the border, this is where you are,

1954
01:22:51,600 --> 01:22:53,040
and then everything else is the outside world.

1955
01:22:53,040 --> 01:22:55,280
It has to guess, and it has to make a self-model,

1956
01:22:55,280 --> 01:22:57,040
and it has to make a world model.

1957
01:22:57,040 --> 01:22:59,160
Then there are the energy constraints.

1958
01:22:59,160 --> 01:23:01,360
Typical AIs, as far as I know,

1959
01:23:01,360 --> 01:23:03,080
have all their energy needs met.

1960
01:23:03,080 --> 01:23:05,560
They can do whatever they want.

1961
01:23:05,560 --> 01:23:07,440
They don't have to worry about it.

1962
01:23:07,440 --> 01:23:10,920
Organisms evolved under very stringent energy

1963
01:23:10,920 --> 01:23:11,880
and time constraints,

1964
01:23:11,880 --> 01:23:15,160
which means that they cannot afford to be

1965
01:23:15,160 --> 01:23:17,840
some kind of a Laplacian demon

1966
01:23:17,840 --> 01:23:20,440
paying attention to all the micro states of the world.

1967
01:23:20,440 --> 01:23:23,160
They have to do a lot of core screening

1968
01:23:23,160 --> 01:23:27,080
and they have to kind of bundle all sorts,

1969
01:23:27,080 --> 01:23:29,840
they have to generalize all sorts of things that go on

1970
01:23:29,840 --> 01:23:34,320
into models of agents doing things, of selves doing things.

1971
01:23:34,320 --> 01:23:35,760
That's the only way you have the time

1972
01:23:35,760 --> 01:23:37,760
to compute what you should do next.

1973
01:23:37,760 --> 01:23:38,720
And if you get good at that,

1974
01:23:38,720 --> 01:23:40,760
eventually you turn that on yourself

1975
01:23:40,760 --> 01:23:42,600
and you start telling stories about,

1976
01:23:42,600 --> 01:23:45,960
meaning making internals and models of yourself doing things.

1977
01:23:45,960 --> 01:23:48,040
And this becomes this idea,

1978
01:23:48,040 --> 01:23:50,600
why do we all innately believe in free will?

1979
01:23:50,600 --> 01:23:53,440
Because from the time that we were single cells,

1980
01:23:53,440 --> 01:23:56,680
we had to tell stories about agents doing things

1981
01:23:56,680 --> 01:23:57,520
and making choices.

1982
01:23:57,520 --> 01:24:00,920
Otherwise, we just wouldn't survive without that ability.

1983
01:24:00,920 --> 01:24:03,680
And then there's some other things like the shared stress

1984
01:24:03,680 --> 01:24:07,600
and the scaling of stress via sharing it among parts

1985
01:24:07,600 --> 01:24:08,840
and so on, we can talk about that.

1986
01:24:08,840 --> 01:24:12,040
And the idea that it's open-ended,

1987
01:24:12,040 --> 01:24:14,360
living things select their own problem space

1988
01:24:14,360 --> 01:24:16,520
and explore it and so on.

1989
01:24:16,520 --> 01:24:19,840
So this is, I'm just gonna stop here,

1990
01:24:19,840 --> 01:24:22,720
but this is what I tell people is that because of this,

1991
01:24:22,720 --> 01:24:26,040
because biology is so incredibly interoperative

1992
01:24:26,040 --> 01:24:27,920
because none of the parts make any assumptions

1993
01:24:27,920 --> 01:24:29,200
about what's going to happen,

1994
01:24:29,200 --> 01:24:31,480
they do their best in whatever environment

1995
01:24:31,480 --> 01:24:33,200
that they happen to be in,

1996
01:24:33,200 --> 01:24:36,320
every combination of evolved material,

1997
01:24:36,320 --> 01:24:40,240
some sort of engineered material and software

1998
01:24:40,240 --> 01:24:42,720
is potentially a viable agent.

1999
01:24:42,720 --> 01:24:47,760
So hybrids, cyborgs, biorobots, all of this,

2000
01:24:47,760 --> 01:24:50,960
there's this huge option space of new creatures,

2001
01:24:50,960 --> 01:24:53,200
of new bodies and new minds.

2002
01:24:53,200 --> 01:24:55,480
Everything, when Darwin said endless forms,

2003
01:24:55,480 --> 01:24:57,160
most beautiful, sort of impressed

2004
01:24:57,160 --> 01:24:59,320
with the variety of living beings,

2005
01:24:59,320 --> 01:25:00,880
all of that stuff is a tiny dot.

2006
01:25:00,880 --> 01:25:02,080
It's a tiny corner.

2007
01:25:02,080 --> 01:25:04,000
Everything on earth is a tiny corner

2008
01:25:04,000 --> 01:25:05,600
of the space of possible beings.

2009
01:25:05,600 --> 01:25:06,880
It's truly immense.

2010
01:25:06,880 --> 01:25:08,360
And all of these things,

2011
01:25:08,360 --> 01:25:10,080
and we're gonna be surrounded by these things.

2012
01:25:10,080 --> 01:25:12,000
Some of this already exists as some hybrids

2013
01:25:12,000 --> 01:25:13,080
and cyborgs already exist,

2014
01:25:13,080 --> 01:25:15,000
but there's gonna be an incredible variety of them

2015
01:25:15,000 --> 01:25:16,920
that we are going to be living with.

2016
01:25:16,920 --> 01:25:20,080
This has major implications for ethics, for example,

2017
01:25:20,080 --> 01:25:21,840
because up until now,

2018
01:25:21,840 --> 01:25:26,160
we were, all of our ethical frameworks

2019
01:25:26,160 --> 01:25:28,560
about how to relate to other beings

2020
01:25:28,560 --> 01:25:30,440
really boiled down to two things.

2021
01:25:30,440 --> 01:25:32,040
Do they look like us?

2022
01:25:32,040 --> 01:25:33,200
And did they come from,

2023
01:25:33,200 --> 01:25:35,360
do they have the same origin story as us?

2024
01:25:35,360 --> 01:25:40,360
And so this is, even today in bioethics sessions

2025
01:25:40,640 --> 01:25:41,400
at conferences, people say,

2026
01:25:41,400 --> 01:25:42,760
well, does it look like a human brain?

2027
01:25:42,760 --> 01:25:44,280
Then we have to worry about.

2028
01:25:44,280 --> 01:25:47,800
But the reality is that these categories are,

2029
01:25:47,800 --> 01:25:49,920
they're not gonna survive the next couple of decades.

2030
01:25:49,920 --> 01:25:54,920
We cannot gauge anything about the potential intelligence

2031
01:25:55,640 --> 01:25:57,520
in terms of the type of cognition

2032
01:25:57,520 --> 01:25:59,320
and what space they're working in,

2033
01:25:59,320 --> 01:26:01,880
by looking at where they come from the family tree,

2034
01:26:01,880 --> 01:26:03,960
because they're not going to be on our family tree.

2035
01:26:03,960 --> 01:26:06,040
And we have to have completely different frameworks for this.

2036
01:26:06,040 --> 01:26:08,600
And the kinds of AIs that we're talking about now

2037
01:26:08,600 --> 01:26:10,080
are only one part of this.

2038
01:26:10,720 --> 01:26:12,960
We're going to be facing the exact same problem

2039
01:26:12,960 --> 01:26:17,000
of dealing with the software AIs in biology.

2040
01:26:17,000 --> 01:26:20,000
So if anybody's interested in these things,

2041
01:26:20,000 --> 01:26:22,960
there are lots of papers where we go into this.

2042
01:26:22,960 --> 01:26:25,480
And I just wanna thank the students and postdocs

2043
01:26:25,480 --> 01:26:27,520
that did all the work that I showed you.

2044
01:26:27,520 --> 01:26:29,240
And of course, again, the disclosure.

2045
01:26:29,240 --> 01:26:31,400
So I will end there.

2046
01:26:35,160 --> 01:26:36,520
Thank you so much, Michael.

2047
01:26:36,520 --> 01:26:37,720
That was wonderful.

2048
01:26:38,720 --> 01:26:41,760
I don't think that I need to introduce Christoph.

2049
01:26:41,760 --> 01:26:46,840
We already had the pleasure of having you on the previous panel.

2050
01:26:46,840 --> 01:26:51,840
And Christoph is currently a professor at Etihad,

2051
01:26:52,120 --> 01:26:54,960
the INI in Zurich.

2052
01:26:54,960 --> 01:26:58,800
And he is a first-generation cyber nutrition

2053
01:26:58,800 --> 01:27:03,800
in a way as a physicist who is using very broad perspective

2054
01:27:04,080 --> 01:27:06,560
on understanding intelligent systems.

2055
01:27:06,560 --> 01:27:11,080
And without further ado, Christoph, please.

2056
01:27:11,080 --> 01:27:12,160
The stage is yours.

2057
01:27:13,080 --> 01:27:15,040
Yeah, I haven't prepared anything.

2058
01:27:15,040 --> 01:27:20,040
I didn't know I was expected to prepare anything.

2059
01:27:20,520 --> 01:27:24,960
So I am at liberty to respond to some of the things

2060
01:27:24,960 --> 01:27:27,560
you have said, the two of you have said.

2061
01:27:27,560 --> 01:27:31,560
Let me start with a point,

2062
01:27:31,560 --> 01:27:33,280
Yosha, you made about the brain,

2063
01:27:33,280 --> 01:27:38,280
which was it is a noisy, it is a noisy entity.

2064
01:27:39,440 --> 01:27:44,440
It is not a digital device, not on the basic level.

2065
01:27:45,240 --> 01:27:50,240
So if you want to have billions of entities,

2066
01:27:50,240 --> 01:27:55,240
synapses or neurons to interact in any useful sense,

2067
01:27:57,960 --> 01:28:00,120
you need attractor dynamics.

2068
01:28:00,120 --> 01:28:03,280
So there must be certain states of the thing

2069
01:28:03,280 --> 01:28:07,760
that have the property of being stable under noise,

2070
01:28:07,760 --> 01:28:12,760
of having attractor dynamics.

2071
01:28:13,520 --> 01:28:18,520
And we know that the brain is, of course,

2072
01:28:19,080 --> 01:28:20,800
essentially a network.

2073
01:28:20,800 --> 01:28:25,800
So in each moment of time, a subset of the neurons fire,

2074
01:28:26,640 --> 01:28:30,600
and this subset must be stable.

2075
01:28:30,600 --> 01:28:33,640
And for a short moment, a metastable,

2076
01:28:33,640 --> 01:28:35,480
if you want to call it that way,

2077
01:28:35,480 --> 01:28:38,960
you want to go through a trajectory of stable states.

2078
01:28:38,960 --> 01:28:43,600
And that means individual fibers,

2079
01:28:43,600 --> 01:28:46,600
the individual interaction between neurons

2080
01:28:46,600 --> 01:28:51,600
must be embedded in alternate alternative pathways,

2081
01:28:51,800 --> 01:28:55,320
which run to the same effect.

2082
01:28:55,400 --> 01:29:00,400
So a signal emanating from a single neuron,

2083
01:29:02,720 --> 01:29:05,720
going off on different pathways,

2084
01:29:05,720 --> 01:29:09,800
many of these signals must come together again

2085
01:29:09,800 --> 01:29:13,840
and coincide in space, meaning on the same neuron,

2086
01:29:13,840 --> 01:29:14,960
and in time.

2087
01:29:16,000 --> 01:29:19,360
And this is a selection criterion

2088
01:29:19,360 --> 01:29:23,320
for the kind of activity states

2089
01:29:23,320 --> 01:29:26,960
and the underlying connectivity states

2090
01:29:26,960 --> 01:29:30,360
that make those states stable.

2091
01:29:30,360 --> 01:29:33,480
And I would like to submit the idea

2092
01:29:33,480 --> 01:29:38,480
that the brain is totally dominated

2093
01:29:38,680 --> 01:29:43,680
by those appropriately shaped connectivity patterns

2094
01:29:46,440 --> 01:29:48,640
that have this property.

2095
01:29:48,640 --> 01:29:52,080
These connectivity patterns emerge

2096
01:29:52,080 --> 01:29:55,200
through a process of self-interaction.

2097
01:29:55,200 --> 01:30:00,200
You have self-interaction also on the slow time scale

2098
01:30:00,480 --> 01:30:03,440
of individual synapses adapting

2099
01:30:03,440 --> 01:30:07,160
and individual synapses finding out

2100
01:30:07,160 --> 01:30:11,360
where they can find coincidences of signals.

2101
01:30:11,360 --> 01:30:15,760
Each axonal branch has a small choice,

2102
01:30:15,760 --> 01:30:18,760
a small sphere of a search space

2103
01:30:18,840 --> 01:30:22,680
where it can end up in a plasticity.

2104
01:30:22,680 --> 01:30:27,680
And all the endpoints of axons are searching around

2105
01:30:29,560 --> 01:30:31,960
in order to find meeting places

2106
01:30:31,960 --> 01:30:34,360
where they have a high likelihood

2107
01:30:34,360 --> 01:30:38,960
of coinciding with the signals of other branches.

2108
01:30:38,960 --> 01:30:42,200
And this process of self-interaction,

2109
01:30:42,200 --> 01:30:44,600
of network self-organization,

2110
01:30:44,600 --> 01:30:48,080
singles out from the space

2111
01:30:48,080 --> 01:30:52,720
of all combinatorially possible connectivity patterns

2112
01:30:52,720 --> 01:30:55,880
a very, very small subset.

2113
01:30:55,880 --> 01:30:58,080
Let me remind you of the fact

2114
01:30:58,080 --> 01:31:00,840
that the brain, the whole organism,

2115
01:31:00,840 --> 01:31:03,440
the brain is constructed on the basis

2116
01:31:03,440 --> 01:31:08,440
of one gigabyte of genetic information.

2117
01:31:08,480 --> 01:31:10,920
And in order to describe the connectivity pattern

2118
01:31:10,920 --> 01:31:13,640
of the brain, it's an easy calculation,

2119
01:31:13,640 --> 01:31:18,640
you need a petabyte, 10 to the 15 bytes of information

2120
01:31:19,080 --> 01:31:22,320
which is a million gigabytes as you well know.

2121
01:31:22,320 --> 01:31:27,320
So the genes can only select from the space

2122
01:31:28,480 --> 01:31:31,400
of all connections, a very small,

2123
01:31:31,400 --> 01:31:34,320
can only be able to select from that space

2124
01:31:34,320 --> 01:31:36,720
a very small subset.

2125
01:31:36,720 --> 01:31:39,880
And I think it is very important to know more

2126
01:31:39,880 --> 01:31:44,880
about this subset of self-supporting activity states

2127
01:31:47,840 --> 01:31:50,960
and connectivity states.

2128
01:31:50,960 --> 01:31:53,880
So in order to put that in action,

2129
01:31:53,880 --> 01:31:56,880
let me remind you that your brain

2130
01:31:56,880 --> 01:31:59,820
is in every moment of waking time

2131
01:31:59,820 --> 01:32:03,720
representing the situation in which you are immersed.

2132
01:32:03,720 --> 01:32:07,680
You have a representation of your actual environment

2133
01:32:07,680 --> 01:32:11,360
if you open up your eyes.

2134
01:32:11,360 --> 01:32:15,040
And this representation is so good

2135
01:32:15,040 --> 01:32:20,040
that you usually equated with the reality out there.

2136
01:32:20,080 --> 01:32:22,720
You are not aware of any differences

2137
01:32:22,720 --> 01:32:26,080
between this reconstructed,

2138
01:32:26,080 --> 01:32:31,080
this model of the outside world and the outside world.

2139
01:32:31,080 --> 01:32:35,960
And you are so confident that it is the reality

2140
01:32:35,960 --> 01:32:38,800
and not just an imagination

2141
01:32:38,800 --> 01:32:41,520
because you continuously do experiments.

2142
01:32:41,520 --> 01:32:45,440
You move around so that the perspective

2143
01:32:45,440 --> 01:32:48,200
of the world changers

2144
01:32:49,280 --> 01:32:52,960
and you test whether your representation

2145
01:32:52,960 --> 01:32:57,960
is stays in tune over time with the sensory information.

2146
01:32:58,720 --> 01:33:01,560
You do experiments, you touch objects

2147
01:33:01,560 --> 01:33:06,560
and you experiment continuously

2148
01:33:08,000 --> 01:33:11,240
with the environment

2149
01:33:11,240 --> 01:33:14,600
in order to make sure that your representation

2150
01:33:14,600 --> 01:33:18,160
is in tune with it, is consistent with it,

2151
01:33:18,160 --> 01:33:22,520
is rendering the reality.

2152
01:33:22,520 --> 01:33:25,880
Of course, what you are representing

2153
01:33:25,880 --> 01:33:28,680
is only a small sector of what is out there.

2154
01:33:28,680 --> 01:33:33,680
Your attention is always picking out only part of it.

2155
01:33:34,120 --> 01:33:38,880
But what you are picking out is for you,

2156
01:33:38,880 --> 01:33:42,160
for all intents and purposes, reality.

2157
01:33:42,160 --> 01:33:47,320
I find it amazing that our models,

2158
01:33:47,320 --> 01:33:51,360
our theories of intelligence, of brain function

2159
01:33:52,440 --> 01:33:55,480
make so little of this very fundamental fact

2160
01:33:55,480 --> 01:33:57,760
of our individual life.

2161
01:33:59,520 --> 01:34:03,520
Now, according to what I said,

2162
01:34:03,520 --> 01:34:08,400
I have been explicit about the data structure

2163
01:34:08,400 --> 01:34:12,360
which is used to create this reality,

2164
01:34:12,360 --> 01:34:16,600
to represent a model of this reality.

2165
01:34:16,600 --> 01:34:18,200
The data structure is, of course,

2166
01:34:18,200 --> 01:34:21,760
everybody believes firing neurons,

2167
01:34:21,760 --> 01:34:25,040
but I would like to change your perspective

2168
01:34:25,040 --> 01:34:27,600
in saying, don't look at the neurons,

2169
01:34:27,600 --> 01:34:32,600
each neuron by itself doesn't have any significant meaning.

2170
01:34:35,600 --> 01:34:38,600
It is the environment, the neural environment,

2171
01:34:38,600 --> 01:34:41,800
the firing environment in which the neuron fires,

2172
01:34:41,800 --> 01:34:43,400
which is the important thing.

2173
01:34:43,400 --> 01:34:48,280
You have to look at quite a number of co-firing neurons

2174
01:34:48,280 --> 01:34:50,880
in order to be able to make sense of it.

2175
01:34:50,880 --> 01:34:55,880
When you look at a TV screen and can see only one pixel,

2176
01:34:56,200 --> 01:34:59,800
there is no way you can connect that with any meaning.

2177
01:35:01,640 --> 01:35:04,920
The pixel is something real, so to speak,

2178
01:35:04,920 --> 01:35:07,160
but it doesn't tell you anything.

2179
01:35:07,160 --> 01:35:11,920
It has no significance.

2180
01:35:11,920 --> 01:35:15,880
In order to understand anything on a TV screen,

2181
01:35:15,880 --> 01:35:19,160
you need to see quite a patch of it.

2182
01:35:19,160 --> 01:35:21,320
In order to understand anything

2183
01:35:21,320 --> 01:35:24,520
of the data structure of a brain,

2184
01:35:24,520 --> 01:35:28,840
you need to see hundreds, probably thousands of neurons,

2185
01:35:28,840 --> 01:35:32,200
at a time, and a given neuron can take part

2186
01:35:32,200 --> 01:35:35,960
in quite a number of such, not an infinite number,

2187
01:35:35,960 --> 01:35:40,520
but quite a number of such activity patterns,

2188
01:35:40,520 --> 01:35:44,680
which I would like to call fragments.

2189
01:35:44,680 --> 01:35:49,200
I think the perspective on the nervous system

2190
01:35:49,200 --> 01:35:52,200
has to be changed very fundamentally

2191
01:35:52,200 --> 01:35:55,560
in order to see it as a data structure

2192
01:35:55,560 --> 01:35:59,320
that is up to the job of representing reality.

2193
01:36:01,200 --> 01:36:06,120
Now, I would like, one of the last statements

2194
01:36:06,120 --> 01:36:12,120
you, Michael, made was the range of things,

2195
01:36:12,120 --> 01:36:14,320
of intelligent things, of organized things

2196
01:36:14,320 --> 01:36:18,920
that can be generated, you said, is infinite.

2197
01:36:18,920 --> 01:36:24,120
I would rather like to emphasize the opposite.

2198
01:36:24,120 --> 01:36:26,640
I've recently read an interesting book

2199
01:36:26,640 --> 01:36:31,400
by an author named Morris, a book that

2200
01:36:31,400 --> 01:36:34,280
was totally focused on the phenomenon,

2201
01:36:34,280 --> 01:36:38,280
looking at evolution, the phenomenon of convergence.

2202
01:36:38,280 --> 01:36:43,280
A lens eye has been invented 12 times or something like that.

2203
01:36:43,280 --> 01:36:47,200
The facet eye has been invented again and again.

2204
01:36:47,200 --> 01:36:51,760
The lifestyle of a wolf pack has been invented again and again.

2205
01:36:51,760 --> 01:36:56,560
The lifestyle of social insects or social animals,

2206
01:36:56,560 --> 01:37:00,960
you social animals, has been invented again and again.

2207
01:37:00,960 --> 01:37:06,320
So the space of all possible organic patterns

2208
01:37:06,320 --> 01:37:09,360
that make sense, that have inner coherence,

2209
01:37:09,360 --> 01:37:12,800
where the parts support each other in order

2210
01:37:12,800 --> 01:37:16,440
to create something that is stable and significant,

2211
01:37:16,440 --> 01:37:20,680
that is stable in itself and is coherent with environment,

2212
01:37:20,680 --> 01:37:25,040
the space of those shapes and structures is limited.

2213
01:37:25,040 --> 01:37:30,800
And here is a great opportunity for theory

2214
01:37:30,800 --> 01:37:35,640
to come forward to understand what to expect from biology.

2215
01:37:35,640 --> 01:37:40,360
The last thing I want to say is that what is missing,

2216
01:37:40,360 --> 01:37:43,680
completely missing so far, almost completely missing

2217
01:37:43,680 --> 01:37:48,440
so far from our artificial intelligence,

2218
01:37:48,440 --> 01:37:52,600
from our machine learning, is the equivalent

2219
01:37:52,600 --> 01:37:56,960
of biological behavior, of behavioral goals.

2220
01:37:56,960 --> 01:38:00,800
Of course, an animal has the fundamental goal

2221
01:38:00,800 --> 01:38:05,720
of self-preservation of the own structure.

2222
01:38:05,720 --> 01:38:12,040
But evolution has built into the individual species

2223
01:38:12,080 --> 01:38:18,000
a number of sub-goals, like feed yourself and avoid danger and so on,

2224
01:38:18,000 --> 01:38:23,680
and find social contexts, sub-goals,

2225
01:38:23,680 --> 01:38:27,440
which make up your life.

2226
01:38:27,440 --> 01:38:30,360
And the intelligence in the eyes of many people

2227
01:38:30,360 --> 01:38:36,520
is just the ability to pursue those goals in a changing context.

2228
01:38:36,520 --> 01:38:41,480
That is a slightly different definition from yours,

2229
01:38:41,480 --> 01:38:47,160
because yours was computing functions.

2230
01:38:47,160 --> 01:38:53,880
The biological goals is the raison d'etre

2231
01:38:53,880 --> 01:39:00,000
of biological intelligence, of course, to pursue those goals.

2232
01:39:00,000 --> 01:39:07,560
And I think in the present situation around things like chat, GTP and so on,

2233
01:39:07,560 --> 01:39:15,960
it is becoming quickly clear that those beasts are not intelligent in our sense,

2234
01:39:15,960 --> 01:39:21,160
because what they do doesn't make sense in the light of the goals

2235
01:39:21,160 --> 01:39:23,840
that we all recognize as such.

2236
01:39:23,840 --> 01:39:30,440
And it would be, I think there will soon be an important drive towards

2237
01:39:30,440 --> 01:39:37,440
installing in such systems the equivalent of the sense of responsibility.

2238
01:39:37,440 --> 01:39:43,720
The sense of the consequences and utterance that is made may have

2239
01:39:43,720 --> 01:39:49,160
down the line for ethical, for legal reasons.

2240
01:39:49,160 --> 01:39:54,960
But also I feel, although I don't have a very strong argument in that favor,

2241
01:39:54,960 --> 01:40:00,440
I have the feeling that in order for an entity to be truly intelligent,

2242
01:40:00,440 --> 01:40:11,600
it needs to have a set of set goals with which it can pursue in its environment.

2243
01:40:11,600 --> 01:40:16,000
Last remark I want to make is about consciousness.

2244
01:40:16,000 --> 01:40:23,520
I think in my view, the definition of the conscious state of your own mind,

2245
01:40:23,520 --> 01:40:28,800
of your own brain is a state in which you concentrate on one topic.

2246
01:40:28,800 --> 01:40:32,080
Your whole brain is concentrated on one topic.

2247
01:40:32,080 --> 01:40:37,160
And all the different submodalities in your brain are in tune with each other,

2248
01:40:37,160 --> 01:40:39,240
are in mutual understanding.

2249
01:40:39,240 --> 01:40:43,080
So if any change happens in any part of the system,

2250
01:40:43,080 --> 01:40:49,240
all the other agency, other modalities can immediately respond to that.

2251
01:40:49,240 --> 01:40:55,640
So consciousness is not the icing on a cake.

2252
01:40:55,640 --> 01:41:01,360
I don't think it makes sense to talk of zombies.

2253
01:41:01,360 --> 01:41:06,520
Consciousness is a condition for a system to be functional.

2254
01:41:06,520 --> 01:41:13,640
And if you go down in the letter of evolution to simple animals,

2255
01:41:13,640 --> 01:41:19,640
I don't think you can find a point that is a point Michael also made.

2256
01:41:19,640 --> 01:41:24,240
You can find a point where consciousness disappears.

2257
01:41:24,240 --> 01:41:27,520
But consciousness just loses volume.

2258
01:41:27,520 --> 01:41:31,520
You lose language when you go from humans to animals.

2259
01:41:31,520 --> 01:41:37,120
And you use the imagination of distant future when you go to animals.

2260
01:41:37,120 --> 01:41:44,800
And so going further and further down the evolutionary ladder,

2261
01:41:44,800 --> 01:41:47,840
the volume of consciousness gets less.

2262
01:41:47,840 --> 01:41:57,400
But I would have a hard time, I think when talking about a fly,

2263
01:41:57,400 --> 01:42:01,680
it has its own level of consciousness.

2264
01:42:01,680 --> 01:42:09,680
So when approaching a wall, it senses the impending approach and reacts accordingly.

2265
01:42:09,680 --> 01:42:13,640
So the whole organism is able to react to signals.

2266
01:42:13,640 --> 01:42:17,720
So that is my view on consciousness.

2267
01:42:17,720 --> 01:42:20,240
Thank you.

2268
01:42:20,240 --> 01:42:22,480
Thank you so much, Christoph.

2269
01:42:22,480 --> 01:42:27,040
So we're on to the discussion and Q&A question.

2270
01:42:27,040 --> 01:42:32,480
I would like to start by asking a question of Michael Levin.

2271
01:42:32,480 --> 01:42:40,520
So Michael, you were talking about how a lot of the problem solving within the organism

2272
01:42:40,520 --> 01:42:43,760
is actually done at the local scale.

2273
01:42:43,800 --> 01:42:49,040
And there is also the interesting remark that was made by one of participants in the chat

2274
01:42:49,040 --> 01:42:55,680
that you mentioned that the planaria tail would have become the head had not been bullied

2275
01:42:55,680 --> 01:42:57,520
by the rest of the organism.

2276
01:42:57,520 --> 01:43:02,600
So what do you think are the communication protocols that are necessary

2277
01:43:02,600 --> 01:43:05,680
to enable different types of intelligence?

2278
01:43:05,680 --> 01:43:09,000
And is it the case that humans, for example, have cancer

2279
01:43:09,000 --> 01:43:14,720
because we don't have enough intelligence at the lower local organismic level?

2280
01:43:14,720 --> 01:43:18,280
Or is it because we pursue the higher scale goal

2281
01:43:18,280 --> 01:43:22,080
and some kind of trade-off has to be made?

2282
01:43:22,080 --> 01:43:23,600
Yeah, great questions.

2283
01:43:23,600 --> 01:43:24,360
A few things.

2284
01:43:24,360 --> 01:43:27,280
First of all, it is definitely not local.

2285
01:43:27,280 --> 01:43:33,200
So one of the key things about all of this stuff is that larger systems make decisions

2286
01:43:33,200 --> 01:43:38,040
in spaces that are much larger than their parts.

2287
01:43:38,080 --> 01:43:40,320
And so here's a very simple example.

2288
01:43:40,320 --> 01:43:42,160
If you have a planarian, it's got a head and a tail.

2289
01:43:42,160 --> 01:43:43,640
You cut it in half.

2290
01:43:43,640 --> 01:43:47,240
These two cells on either side of the cut, these guys will have to make a new head.

2291
01:43:47,240 --> 01:43:48,880
These guys will have to make a new tail.

2292
01:43:48,880 --> 01:43:52,360
But they were sitting right next to each other before you separated them with a scalpel.

2293
01:43:52,360 --> 01:43:55,200
You cannot locally decide whether you're a head or a tail.

2294
01:43:55,200 --> 01:43:58,360
It's a decision that has to take into account, well, do we already have a head?

2295
01:43:58,360 --> 01:43:59,080
Do we have a tail?

2296
01:43:59,080 --> 01:44:00,560
Which way is the wound facing?

2297
01:44:00,560 --> 01:44:01,760
This is a global decision.

2298
01:44:01,760 --> 01:44:03,000
It cannot be made locally.

2299
01:44:03,000 --> 01:44:05,920
All of this stuff is like that.

2300
01:44:05,920 --> 01:44:14,920
And it uses the exact same scheme that bacterial biofilms use to decide when different parts

2301
01:44:14,920 --> 01:44:18,360
of the thing should eat so that everybody has a turn and the exact same thing,

2302
01:44:18,360 --> 01:44:23,320
the exact same set of mechanisms that brains use to try to synthesize the activity

2303
01:44:23,320 --> 01:44:28,320
of individual neurons into some sort of global goal for the rat or human or whatever.

2304
01:44:28,320 --> 01:44:30,600
It's an electrical network.

2305
01:44:30,600 --> 01:44:33,520
It has certain properties, only a few of which we understand.

2306
01:44:33,520 --> 01:44:35,120
But it is absolutely not local.

2307
01:44:35,120 --> 01:44:44,520
What this bioelectricity is very good at is at implementing integrated information

2308
01:44:44,520 --> 01:44:49,440
across space and time to make decisions in new spaces.

2309
01:44:49,440 --> 01:44:53,960
And that's, I think I forgot, what was the second part of your, I lost track of it.

2310
01:44:53,960 --> 01:45:00,200
The second part is so do humans, so you remind that humans have cancer,

2311
01:45:00,200 --> 01:45:02,760
but some other animals don't.

2312
01:45:02,760 --> 01:45:05,120
And some other animals are in fact immortal.

2313
01:45:05,120 --> 01:45:10,080
So what is it that, what is it in your opinion that doesn't allow human organisms

2314
01:45:10,080 --> 01:45:12,880
to solve the problem of immortality?

2315
01:45:12,880 --> 01:45:18,200
Does it have something to do with higher level goals or is it just a lack of intelligence?

2316
01:45:18,200 --> 01:45:24,680
Yeah, I think that, well, so there's two, there's kind of a simple answer

2317
01:45:24,680 --> 01:45:25,960
and then there's a more interesting answer.

2318
01:45:25,960 --> 01:45:32,640
The simple answer that people usually give is simply that all by, so evolution, of course,

2319
01:45:32,640 --> 01:45:36,720
doesn't really optimize for long life, happiness, intelligence.

2320
01:45:36,720 --> 01:45:37,840
It doesn't optimize for any of that.

2321
01:45:37,840 --> 01:45:39,840
It optimizes for biomass, that's it.

2322
01:45:39,840 --> 01:45:46,240
And so, right, and so the simple answer is we don't need to be immortal

2323
01:45:46,240 --> 01:45:52,160
and cancer resistant because it's perfectly possible to be a human

2324
01:45:52,160 --> 01:45:56,280
and have lots of offspring and still get cancer and die after your reproductive years.

2325
01:45:56,280 --> 01:45:59,920
That's it, that's the standard answer that it's actually,

2326
01:45:59,920 --> 01:46:04,200
there's just not a lot of pressure for humans to do anything different.

2327
01:46:04,200 --> 01:46:08,200
Now, I think the more interesting answer is this.

2328
01:46:08,200 --> 01:46:13,560
There are most organisms do get cancer and do age, there are a few that are resistant.

2329
01:46:13,560 --> 01:46:14,720
Let's look at the planaria.

2330
01:46:14,720 --> 01:46:19,320
One really interesting thing about planaria is that many of them reproduce

2331
01:46:19,320 --> 01:46:22,640
by tearing themselves in half and regenerating.

2332
01:46:22,640 --> 01:46:27,480
Now, one interesting thing that the implications of that are unlike for us.

2333
01:46:27,840 --> 01:46:30,120
If you get a mutation in your body during your lifetime,

2334
01:46:30,120 --> 01:46:31,960
it doesn't get passed on to your offspring, right?

2335
01:46:31,960 --> 01:46:36,040
So because of the Weissmann's barrier in sexual reproduction.

2336
01:46:36,040 --> 01:46:41,120
In planaria that do this, every cell that doesn't die from that mutation

2337
01:46:41,120 --> 01:46:44,600
contributes copies of itself to the next body, right?

2338
01:46:44,600 --> 01:46:48,200
Because they have to repopulate and have to regenerate the new one.

2339
01:46:48,200 --> 01:46:50,600
So planaria accumulate mutations like crazy.

2340
01:46:50,600 --> 01:46:53,320
So over 400 million years that they've been around,

2341
01:46:53,320 --> 01:46:55,680
their genomes are a complete mess.

2342
01:46:55,720 --> 01:46:57,760
They basically look like a tumor, they're mix-employed.

2343
01:46:57,760 --> 01:46:59,960
Every cell might have a different number of chromosomes.

2344
01:46:59,960 --> 01:47:00,960
It's a disaster.

2345
01:47:00,960 --> 01:47:09,160
Now, this is really a scandal because nowhere in a typical biology curriculum

2346
01:47:09,160 --> 01:47:12,320
will you hear that the animal with the worst genome is, by the way,

2347
01:47:12,320 --> 01:47:14,960
immortal, cancer-resistant and highly regenerative, right?

2348
01:47:14,960 --> 01:47:16,240
They have the best anatomy.

2349
01:47:16,240 --> 01:47:16,880
What's going on?

2350
01:47:16,880 --> 01:47:21,120
We're told that our genomes are, that's where your body information is, right?

2351
01:47:21,120 --> 01:47:22,440
How can this be?

2352
01:47:22,440 --> 01:47:25,560
So this has been bugging me for a really long time.

2353
01:47:25,760 --> 01:47:26,640
This disconnect.

2354
01:47:26,640 --> 01:47:30,000
And I think we finally have an idea of what's going on.

2355
01:47:30,000 --> 01:47:34,400
We just, like two days ago, just published a paper on some simulations

2356
01:47:34,400 --> 01:47:35,360
that talk about this.

2357
01:47:35,360 --> 01:47:36,960
I'll just give you a very simple example.

2358
01:47:40,160 --> 01:47:44,560
One thing you have to do is you have to model not just the genotype and the phenotype,

2359
01:47:44,560 --> 01:47:48,400
meaning the genome and then the thing that gets evaluated in these evolutionary simulations,

2360
01:47:48,400 --> 01:47:52,280
but you have to model the morphogenetic process in between those two.

2361
01:47:52,560 --> 01:47:54,920
The morphogenetic process has certain competencies.

2362
01:47:54,920 --> 01:47:57,560
For example, some of them I've showed you, there are many more.

2363
01:47:57,560 --> 01:48:02,880
So for example, if there's some mutation that puts your mouth off to the side,

2364
01:48:02,880 --> 01:48:05,600
the mouth is perfectly competent to come back where it needs to be.

2365
01:48:05,600 --> 01:48:09,040
If it's a mutation that causes you to fall apart as an early embryo,

2366
01:48:09,040 --> 01:48:12,040
you'll just be a bunch of twins, multiples.

2367
01:48:12,040 --> 01:48:16,320
If we took some eyes, Doug Blackiston did this to us,

2368
01:48:16,320 --> 01:48:19,720
he took eyes and put them instead on the animal's tail,

2369
01:48:19,720 --> 01:48:21,480
they can see perfectly well out of those eyes.

2370
01:48:21,480 --> 01:48:23,760
No problem, no period of adaptation needed.

2371
01:48:23,760 --> 01:48:26,720
It's all good, the nerves come, find the spinal cord, it's all good.

2372
01:48:26,720 --> 01:48:28,600
So all of those kinds of things,

2373
01:48:28,600 --> 01:48:33,160
abilities to make up for these kinds of issues we call developmental competencies.

2374
01:48:33,160 --> 01:48:38,200
Now, one thing that happens is that when you have an animal with a little bit of developmental competency,

2375
01:48:38,200 --> 01:48:42,320
you come up for selection and it turns out you're very good, right?

2376
01:48:42,320 --> 01:48:43,240
But why are you good?

2377
01:48:43,240 --> 01:48:47,480
Selection cannot tell whether you have a great genome or you're good because you're highly competent

2378
01:48:47,480 --> 01:48:50,800
and you fixed all the things your genome actually was pretty sloppy about.

2379
01:48:50,800 --> 01:48:54,000
So that means it's harder for evolution to see the good genomes.

2380
01:48:54,000 --> 01:48:56,640
You can't do all as much work in perfecting the genome,

2381
01:48:56,640 --> 01:48:59,920
but what it can do is crank up the competencies, right?

2382
01:48:59,920 --> 01:49:02,120
So when you do that, then of course that makes the problem worse

2383
01:49:02,120 --> 01:49:06,920
because the more competent you are, the less it's possible to find the best genomes.

2384
01:49:06,920 --> 01:49:09,360
And so there's this positive feedback loop that's ratchet

2385
01:49:09,360 --> 01:49:11,880
and there are some other things that sort of work against it.

2386
01:49:11,880 --> 01:49:16,720
But I think what happened is that, and this is very much a hypothesis still,

2387
01:49:16,760 --> 01:49:20,600
I think what happened is that planaria went all the way,

2388
01:49:20,600 --> 01:49:24,960
meaning that in that lineage, probably because they reproduce this way,

2389
01:49:24,960 --> 01:49:28,160
it doesn't make any sense to assume that your genome is any good.

2390
01:49:28,160 --> 01:49:32,440
And the only architecture that survives is where the algorithm is so good

2391
01:49:32,440 --> 01:49:35,240
that we're going to make a perfect worm no matter what happens to the genome.

2392
01:49:35,240 --> 01:49:40,360
This means aging, carcinogenic mutations, the algorithm,

2393
01:49:40,400 --> 01:49:46,240
meaning the machinery that maintains that goal state

2394
01:49:46,240 --> 01:49:49,360
and physiological and anatomical space is so good

2395
01:49:49,360 --> 01:49:52,960
that it can pretty much ignore a lot of issues in the hardware.

2396
01:49:52,960 --> 01:49:55,600
Most of us aren't like that. Salamanders are sort of in the middle.

2397
01:49:55,600 --> 01:49:58,680
So salamanders are highly regenerative, but they age and die.

2398
01:49:58,680 --> 01:50:02,320
And so I think what salamanders sort of went part of the way there

2399
01:50:02,320 --> 01:50:06,360
and they can fix certain things, but not enough to really keep it going forever.

2400
01:50:06,360 --> 01:50:09,160
Mammals probably stopped even earlier than that.

2401
01:50:09,160 --> 01:50:13,680
But I actually don't think any of this is fundamental.

2402
01:50:13,680 --> 01:50:16,560
I mean, we're working on regeneration in mammals now.

2403
01:50:16,560 --> 01:50:19,760
I do think someday we will all sort of regenerate like planaria.

2404
01:50:19,760 --> 01:50:21,840
I think it is going to be possible.

2405
01:50:21,840 --> 01:50:26,560
But I do think that evolution makes these trade-offs that they're

2406
01:50:26,560 --> 01:50:28,920
just easier ways to be a human, I think.

2407
01:50:31,360 --> 01:50:34,280
So question for everyone.

2408
01:50:34,280 --> 01:50:37,680
So in terms of communication protocols,

2409
01:50:37,680 --> 01:50:43,440
to what extent is intelligence simply the ability to organize the cells

2410
01:50:43,440 --> 01:50:46,760
and what are the conditions necessary for that to occur?

2411
01:50:46,760 --> 01:50:50,800
And to what extent is intelligence is some internal competence,

2412
01:50:50,800 --> 01:50:56,760
competence of the cell or neuron or whatever computational unit we're talking about?

2413
01:51:00,920 --> 01:51:02,200
Yosha, would you like to start?

2414
01:51:03,200 --> 01:51:11,200
I'm currently thinking about the question of whether it's possible to make something

2415
01:51:11,200 --> 01:51:15,200
that is as long lived as the planaria that doesn't look like a blob.

2416
01:51:16,200 --> 01:51:22,200
There seems to be some correlation between the structural coherence of the organism

2417
01:51:22,200 --> 01:51:29,200
and the detail and solution that it has and the degree of fidelity.

2418
01:51:29,200 --> 01:51:35,200
That is expected from interpreting the operators defined in its genome.

2419
01:51:35,200 --> 01:51:43,200
And more specifically, I wonder what, how we can formalize the idea

2420
01:51:43,200 --> 01:51:48,200
that Michael put up earlier of multi-scale organization and such a way

2421
01:51:48,200 --> 01:51:50,200
that it leads to coherence.

2422
01:51:50,200 --> 01:51:55,200
What is the criterion that makes a single agent coherent in itself

2423
01:51:55,200 --> 01:51:58,200
and leads to this coherence on a particular level?

2424
01:51:58,200 --> 01:52:02,200
Arguably, our own mind is some kind of society of agents

2425
01:52:02,200 --> 01:52:04,200
and the organism has lots of local agents.

2426
01:52:04,200 --> 01:52:06,200
Every organ is an agent in a way.

2427
01:52:06,200 --> 01:52:08,200
Every cell is an agent.

2428
01:52:08,200 --> 01:52:11,200
But there is also a globally coherent agent.

2429
01:52:11,200 --> 01:52:17,200
And that is different from having multiple twins coexisting next to each other

2430
01:52:17,200 --> 01:52:20,200
and forming some cooperative chimera.

2431
01:52:20,200 --> 01:52:24,200
But that leads to some global element.

2432
01:52:24,200 --> 01:52:29,200
On the other hand, Christoph has pointed this out.

2433
01:52:29,200 --> 01:52:35,200
If you think about consciousness, it seems to relate to a unified experience.

2434
01:52:35,200 --> 01:52:40,200
And this unified experience of all sensory data is what makes it specific.

2435
01:52:40,200 --> 01:52:42,200
What is interesting about consciousness

2436
01:52:42,200 --> 01:52:46,200
is that I normally don't have multiple conscious experiences

2437
01:52:46,200 --> 01:52:49,200
unified in one perspective.

2438
01:52:49,200 --> 01:52:52,200
How is this unity being realized?

2439
01:52:52,200 --> 01:53:00,200
Or more generally speaking, can we come up with some kind of formal criterion

2440
01:53:00,200 --> 01:53:05,200
that defines how everything has a place in the greater whole

2441
01:53:05,200 --> 01:53:08,200
and the condition needs to be measurable

2442
01:53:08,200 --> 01:53:13,200
and lead to globally coherent behavior on the next level of organization?

2443
01:53:13,200 --> 01:53:18,200
If we take this to account and if you look at Michael's diagram

2444
01:53:18,200 --> 01:53:21,200
that he brought up in the context of ethics,

2445
01:53:21,200 --> 01:53:26,200
all the different agents that all seem to be centered about individual humans,

2446
01:53:26,200 --> 01:53:32,200
it turns out that individual humans are not the main agents in the human sphere.

2447
01:53:32,200 --> 01:53:37,200
The organizations of humans are much more powerful than individual human beings.

2448
01:53:37,200 --> 01:53:42,200
And while individual human beings implement these organizations for the most part,

2449
01:53:42,200 --> 01:53:47,200
we gradually transition more of that to machines that we are building.

2450
01:53:47,200 --> 01:53:52,200
It seems to me that there is different levels of organization

2451
01:53:52,200 --> 01:53:56,200
that transcend the individual organisms.

2452
01:53:56,200 --> 01:53:59,200
This multi-scale organization doesn't stop with humans

2453
01:53:59,200 --> 01:54:02,200
and the next scales are getting more and more agency.

2454
01:54:02,200 --> 01:54:05,200
Also, I don't think that humans are all that important.

2455
01:54:05,200 --> 01:54:10,200
It seems to me that humans are a very specific thing that has a very specific role.

2456
01:54:10,200 --> 01:54:12,200
All our cousin species are dead.

2457
01:54:12,200 --> 01:54:14,200
Women are not long-lived species.

2458
01:54:14,200 --> 01:54:20,200
And it seems to be that the reason why Gaia brought us up is that we fulfill our job,

2459
01:54:20,200 --> 01:54:23,200
which is to burn all the fossil fuels as quickly as possible.

2460
01:54:23,200 --> 01:54:24,200
This is what we're here for.

2461
01:54:24,200 --> 01:54:26,200
Then we burn ourselves out.

2462
01:54:26,200 --> 01:54:29,200
If we manage to teach the rocks how to sink in the meantime,

2463
01:54:29,200 --> 01:54:30,200
that's a stretch goal.

2464
01:54:30,200 --> 01:54:33,200
But after we are gone, there will be more intelligent species.

2465
01:54:33,200 --> 01:54:35,200
And we are a very specific one, right?

2466
01:54:35,200 --> 01:54:40,200
We are this type of monkey that is not going to get his hand out of the cauldron trap

2467
01:54:40,200 --> 01:54:42,200
if there's fossil fuel inside.

2468
01:54:42,200 --> 01:54:46,200
And that's somewhat predictable if you look at the way in which we work

2469
01:54:46,200 --> 01:54:50,200
because we are very smart and intelligent on very short timescales,

2470
01:54:50,200 --> 01:54:52,200
but we are not globally coherent.

2471
01:54:52,200 --> 01:54:56,200
We don't find ourselves in this global, coherent, godlike,

2472
01:54:56,200 --> 01:54:57,200
organization.

2473
01:54:57,200 --> 01:55:01,200
And if we succeed in building the next level of intelligence,

2474
01:55:01,200 --> 01:55:05,200
maybe this next level organization, some kind of very fast,

2475
01:55:05,200 --> 01:55:07,200
tightly integrated globally,

2476
01:55:07,200 --> 01:55:09,200
coherent mind is going to be emerging.

2477
01:55:09,200 --> 01:55:13,200
And maybe humans will play a very small part in whatever is going to come

2478
01:55:13,200 --> 01:55:14,200
afterwards.

2479
01:55:14,200 --> 01:55:16,200
But it's not about us, right?

2480
01:55:16,200 --> 01:55:18,200
Life on Earth is not about us.

2481
01:55:18,200 --> 01:55:20,200
Life on Earth is about the cell.

2482
01:55:20,200 --> 01:55:23,200
And overall, it's about fighting back entropy.

2483
01:55:23,200 --> 01:55:28,200
It's about sustaining yourself through maintaining complexity.

2484
01:55:28,200 --> 01:55:34,200
So my question would be to Christoph and to Michael,

2485
01:55:34,200 --> 01:55:41,200
can we come up with the criterion that determines coherence?

2486
01:55:41,200 --> 01:55:46,200
Yeah, yeah, I think the important thing is, as I've said,

2487
01:55:46,200 --> 01:55:50,200
that a coherent form has a kind of stability.

2488
01:55:50,200 --> 01:55:54,200
It is made up out of continuous variables,

2489
01:55:54,200 --> 01:55:58,200
which are prone to noise.

2490
01:55:58,200 --> 01:56:06,200
And for the whole thing to have a lasting existence is a different

2491
01:56:06,200 --> 01:56:08,200
signals that converge on one point.

2492
01:56:08,200 --> 01:56:10,200
They have to agree with each other.

2493
01:56:10,200 --> 01:56:12,200
They have to stabilize each other.

2494
01:56:12,200 --> 01:56:16,200
The same way as a crystal is formed,

2495
01:56:16,200 --> 01:56:21,200
a rigid body in that the individual forces between atoms,

2496
01:56:21,200 --> 01:56:26,200
which are also acting, of course, in a liquid,

2497
01:56:26,200 --> 01:56:32,200
but are not able to form something like a stable shape in a liquid.

2498
01:56:32,200 --> 01:56:38,200
But in a crystal, they have fallen into a configuration in which

2499
01:56:38,200 --> 01:56:40,200
in each individual interaction,

2500
01:56:40,200 --> 01:56:48,200
each individual force gets support by other indirect pathways.

2501
01:56:48,200 --> 01:56:53,200
And so I think just as in the space of all mathematics,

2502
01:56:53,200 --> 01:57:01,200
those pieces of mathematics that have been found are singular points

2503
01:57:01,200 --> 01:57:08,200
that admit no change if you have a direct interaction

2504
01:57:08,200 --> 01:57:10,200
that admit no change.

2505
01:57:10,200 --> 01:57:13,200
If you have come up with the idea of a group,

2506
01:57:13,200 --> 01:57:16,200
then the rest of the whole story,

2507
01:57:16,200 --> 01:57:21,200
thousands of pages in mathematical journals,

2508
01:57:21,200 --> 01:57:27,200
follows by force from the definition of what a group is,

2509
01:57:27,200 --> 01:57:31,200
a group of finite number of elements.

2510
01:57:31,200 --> 01:57:37,200
And the same way the shapes that dominate life have this inherent

2511
01:57:37,200 --> 01:57:42,200
self-consistency that is the different chains of forces

2512
01:57:42,200 --> 01:57:45,200
that interact, support each other.

2513
01:57:45,200 --> 01:57:49,200
Christoph, my apologies.

2514
01:57:49,200 --> 01:57:51,200
Christoph, my apologies.

2515
01:57:51,200 --> 01:57:55,200
I think we're almost on top of the hour and Michael has to go at 11.

2516
01:57:55,200 --> 01:57:59,200
So, or, well, in one minute, Michael,

2517
01:57:59,200 --> 01:58:03,200
any last comments from you before we let you run?

2518
01:58:03,200 --> 01:58:05,200
I'm sorry for interrupting.

2519
01:58:05,200 --> 01:58:07,200
This is extremely interesting.

2520
01:58:07,200 --> 01:58:09,200
Thank you so much for having me here.

2521
01:58:09,200 --> 01:58:11,200
This was amazing.

2522
01:58:11,200 --> 01:58:13,200
Thanks for coming.

2523
01:58:13,200 --> 01:58:16,200
I really wanted to introduce you to Christoph and have one more

2524
01:58:16,200 --> 01:58:21,200
conversation with you after our lucky podcast.

2525
01:58:21,200 --> 01:58:26,200
And so I'm very, very happy that you could come and hope to see you

2526
01:58:26,200 --> 01:58:28,200
again soon and stay in touch.

2527
01:58:28,200 --> 01:58:34,200
I really like many of your ideas in the space of self-organizing

2528
01:58:34,200 --> 01:58:35,200
systems.

2529
01:58:35,200 --> 01:58:40,200
And it's very lucky that we could have you here today.

2530
01:58:40,200 --> 01:58:43,200
Christoph, do you have some more time to stay on?

2531
01:58:43,200 --> 01:58:45,200
Yes, I do.

2532
01:58:45,200 --> 01:58:46,200
Perfect.

2533
01:58:46,200 --> 01:58:47,200
Thank you very much, everybody.

2534
01:58:47,200 --> 01:58:48,200
I've got to run.

2535
01:58:48,200 --> 01:58:49,200
I'm happy to do more.

2536
01:58:49,200 --> 01:58:50,200
Great meeting you.

2537
01:58:50,200 --> 01:58:53,200
And I'm fascinated by all the examples that you have shown.

2538
01:58:53,200 --> 01:58:56,200
It's just great.

2539
01:58:56,200 --> 01:58:57,200
Cool.

2540
01:58:57,200 --> 01:58:58,200
Thank you so much.

2541
01:58:58,200 --> 01:58:59,200
See you all later.

2542
01:58:59,200 --> 01:59:00,200
Thank you, Michael.

2543
01:59:00,200 --> 01:59:01,200
Thank you.

2544
01:59:01,200 --> 01:59:03,200
Bye.

2545
01:59:03,200 --> 01:59:07,200
So we're going to continue for a little bit more.

2546
01:59:07,200 --> 01:59:09,200
If it's fine with you, Tanya, you have time, right?

2547
01:59:09,200 --> 01:59:12,200
Yes, yes, of course.

2548
01:59:12,200 --> 01:59:17,200
So Christoph, you mentioned that you think that the genome is that

2549
01:59:17,200 --> 01:59:20,200
contributes to the brain is a gigabyte.

2550
01:59:20,200 --> 01:59:22,200
But that's the whole of the genome, right?

2551
01:59:22,200 --> 01:59:26,200
And so it's the part that quotes for the brain is going to be a

2552
01:59:26,200 --> 01:59:28,200
small fraction of that.

2553
01:59:28,200 --> 01:59:32,200
Yeah, I wonder how much information is remarkable that the

2554
01:59:32,200 --> 01:59:35,200
genome has expanded from mouse to man.

2555
01:59:35,200 --> 01:59:39,200
So making a larger brain doesn't need more genes.

2556
01:59:39,200 --> 01:59:40,200
Yes.

2557
01:59:40,200 --> 01:59:43,200
Of course, if you take the genome and you drop it into physics,

2558
01:59:43,200 --> 01:59:46,200
it's not going to form a cell.

2559
01:59:46,200 --> 01:59:51,200
So, and every cell that exists is the result, except for the

2560
01:59:51,200 --> 01:59:57,200
first one of the server application of another cell, right?

2561
01:59:57,200 --> 02:00:00,200
So all cells in some sense depend on the existence of a cell

2562
02:00:00,200 --> 02:00:03,200
and the cell is not empty.

2563
02:00:03,200 --> 02:00:09,200
And I wonder what the comorgo of complexity of the cell itself is

2564
02:00:09,200 --> 02:00:10,200
right.

2565
02:00:10,200 --> 02:00:13,200
How complex is this machinery that is being copied and copied all

2566
02:00:13,200 --> 02:00:14,200
over?

2567
02:00:14,200 --> 02:00:17,200
Maybe that's much larger than a gigabyte.

2568
02:00:17,200 --> 02:00:21,200
And of course, the principles of self-organization embodied in the

2569
02:00:21,200 --> 02:00:25,200
cell lead to the search for more coherent organization on the next

2570
02:00:25,200 --> 02:00:26,200
level.

2571
02:00:26,200 --> 02:00:29,200
And so the cell is already an agent, a self-replicator,

2572
02:00:29,200 --> 02:00:33,200
a Turing machine, and an entropy extractor.

2573
02:00:33,200 --> 02:00:36,200
And the self-replication is the main feat of the cell to make that

2574
02:00:36,200 --> 02:00:38,200
happen robustly in physics.

2575
02:00:38,200 --> 02:00:40,200
And this is what enables everything else.

2576
02:00:40,200 --> 02:00:44,200
I wonder how much of that we need?

2577
02:00:44,200 --> 02:00:51,200
Yeah, I could imagine that the brunt of the genetic information

2578
02:00:51,200 --> 02:00:54,200
needed for a single cell already.

2579
02:00:54,200 --> 02:01:00,200
And then you may just add, I don't know how much,

2580
02:01:00,200 --> 02:01:06,200
on top of that in the form of the preexisting cell.

2581
02:01:06,200 --> 02:01:09,200
Each cell is the daughter of another cell.

2582
02:01:09,200 --> 02:01:14,200
And so there is some information in the arrangement,

2583
02:01:14,200 --> 02:01:18,200
at least in the molecular arrangement of the cell,

2584
02:01:18,200 --> 02:01:21,200
that needs to be counted as information.

2585
02:01:21,200 --> 02:01:27,200
Although I have a hard time seeing that that will be in a

2586
02:01:27,200 --> 02:01:31,200
significant way more than a gigabyte.

2587
02:01:31,200 --> 02:01:35,200
You know, at the present time, there are groups that try to create

2588
02:01:35,200 --> 02:01:37,200
artificial cells.

2589
02:01:37,200 --> 02:01:43,200
And they learn the hard time that a lot needs to be in place to

2590
02:01:43,200 --> 02:01:47,200
make a cell tick, you know, the different kinds of membranes.

2591
02:01:47,200 --> 02:01:51,200
The membranes shape themselves, that's very important.

2592
02:01:51,200 --> 02:01:56,200
They shape themselves into the Golgi apparatus,

2593
02:01:56,200 --> 02:01:58,200
things like that.

2594
02:01:58,200 --> 02:02:03,200
And so there is information in the preexisting cell.

2595
02:02:03,200 --> 02:02:09,200
But quantitatively, I don't think it is going to be more than a

2596
02:02:09,200 --> 02:02:12,200
gigabyte.

2597
02:02:12,200 --> 02:02:15,200
So I guess if you were to build an intelligent system,

2598
02:02:15,200 --> 02:02:20,200
say artificial visual cortex, how would you go about it?

2599
02:02:20,200 --> 02:02:23,200
And what would be the main differences to the existing

2600
02:02:23,200 --> 02:02:25,200
approaches?

2601
02:02:25,200 --> 02:02:30,200
Well, I think, you know, when you look at, with your own brain,

2602
02:02:30,200 --> 02:02:36,200
with your own eyes, at a moving body, you can predict the next

2603
02:02:36,200 --> 02:02:42,200
split second and compare it to the signals that come in.

2604
02:02:42,200 --> 02:02:47,200
So we have this machinery in our visual system that is able to do

2605
02:02:47,200 --> 02:02:52,200
the differential geometry, taking into account the shape of the

2606
02:02:52,200 --> 02:02:59,200
surface, the play of light on the surface, the movement.

2607
02:02:59,200 --> 02:03:11,200
And so what I would create is a sequence of an array of ARIA,

2608
02:03:11,200 --> 02:03:17,200
like V1, the primary visual cortex, and MT, which is concentrated

2609
02:03:17,200 --> 02:03:23,200
on motion, maybe V3 concentrated on color and so on,

2610
02:03:23,200 --> 02:03:29,200
and a number of submodalities, which each are two and a half D

2611
02:03:29,200 --> 02:03:32,200
entities.

2612
02:03:32,200 --> 02:03:40,200
They refer to the two-dimensional way we perceive the world and the

2613
02:03:40,200 --> 02:03:47,200
with added internal spaces of quality spaces, like color,

2614
02:03:47,200 --> 02:03:51,200
a three-dimensional space, and the texture, I don't know what,

2615
02:03:51,200 --> 02:03:56,200
maybe a 40-dimensional space, depth, a one-dimensional space,

2616
02:03:56,200 --> 02:03:59,200
motion, a two-dimensional space, and so on.

2617
02:03:59,200 --> 02:04:08,200
And so they reflect the retinal image on the one hand in these

2618
02:04:08,200 --> 02:04:15,200
different modalities, and they, another set of such ARIA,

2619
02:04:15,200 --> 02:04:21,200
they build up the invariant static scene as such,

2620
02:04:21,200 --> 02:04:28,200
couple them by projection patterns, which have to be dynamic

2621
02:04:28,200 --> 02:04:32,200
because if you roll your eyes, the image moves.

2622
02:04:32,200 --> 02:04:37,200
And in order to connect the moving images to the static

2623
02:04:37,200 --> 02:04:43,200
representation, you need dynamic projection patterns.

2624
02:04:43,200 --> 02:04:52,200
These are very important in themselves in the deforming

2625
02:04:52,200 --> 02:04:58,200
projection of the moving retinal image of a rotating object.

2626
02:04:58,200 --> 02:05:06,200
The deforming projection of that onto a static version of that

2627
02:05:06,200 --> 02:05:10,200
thing tells you about the shape of the object.

2628
02:05:10,200 --> 02:05:19,200
And so I think what you need is a huge array of local

2629
02:05:19,200 --> 02:05:27,200
texture, local modality descriptions, all linked together

2630
02:05:27,200 --> 02:05:33,200
with dynamic patterns, such that the whole thing is a self-supporting

2631
02:05:33,200 --> 02:05:41,200
attractor space and describes the external world in detail as far

2632
02:05:41,200 --> 02:05:44,200
as you concentrate on it, of course.

2633
02:05:44,200 --> 02:05:52,200
It's a, I think it's quite an amount of work that is necessary.

2634
02:05:52,200 --> 02:05:59,200
I once tried to put together a company where I believed I would

2635
02:05:59,200 --> 02:06:06,200
need something like six or eight intelligent co-workers that

2636
02:06:06,200 --> 02:06:09,200
together create this structure.

2637
02:06:09,200 --> 02:06:15,200
The first feat to convince investors would be to let the system

2638
02:06:15,200 --> 02:06:20,200
look at moving objects and build up an instant model of that

2639
02:06:20,200 --> 02:06:27,200
moving object in its 3D form, an instant replica of that,

2640
02:06:27,200 --> 02:06:35,200
with the ability to handle it, to connect to self-motion,

2641
02:06:35,200 --> 02:06:38,200
to connect to manipulative motion.

2642
02:06:38,200 --> 02:06:44,200
I think, you know, the complete abysmal failure of the car

2643
02:06:44,200 --> 02:06:49,200
industry to come up with level five autonomy has very much to do

2644
02:06:49,200 --> 02:06:55,200
with the inability to represent the traffic scene in this sense.

2645
02:06:55,200 --> 02:07:01,200
And so my idea was I would get, investors would be ready to

2646
02:07:01,200 --> 02:07:04,200
invest in that direction.

2647
02:07:04,200 --> 02:07:09,200
However, I found out that this whole perspective of mine is so

2648
02:07:09,200 --> 02:07:13,200
much sailing against the wind that I wouldn't even find the

2649
02:07:13,200 --> 02:07:21,200
co-workers to help me create it, let alone the investors.

2650
02:07:21,200 --> 02:07:26,200
I suspect that part of the difficulty to create self-driving cars

2651
02:07:26,200 --> 02:07:29,200
has to do with the way in which the model is being generated,

2652
02:07:29,200 --> 02:07:33,200
which means a deep learning currently relies on building

2653
02:07:33,200 --> 02:07:36,200
classifiers for individual things.

2654
02:07:36,200 --> 02:07:41,200
And there is no end-to-end train system for deep learning that is

2655
02:07:41,200 --> 02:07:45,200
self-driving in a sense, and it is at the same time reliable.

2656
02:07:45,200 --> 02:07:49,200
If you want to create reliable behavior that is rule-based,

2657
02:07:49,200 --> 02:07:52,200
that where you basically have a set of traffic laws and safety

2658
02:07:52,200 --> 02:07:55,200
measures and precautions that are built into the system that

2659
02:07:55,200 --> 02:08:00,200
drive all the behavior, the object that this system is going

2660
02:08:00,200 --> 02:08:04,200
to relate to are crafted by hand.

2661
02:08:04,200 --> 02:08:08,200
So the self-driving car exists in a handcrafted software world

2662
02:08:08,200 --> 02:08:12,200
where all the objects are being defined by a developer.

2663
02:08:12,200 --> 02:08:15,200
Whereas the world that we are living in is an open world.

2664
02:08:15,200 --> 02:08:18,200
And when we see new phenomena, we are able to integrate them

2665
02:08:18,200 --> 02:08:20,200
into this model.

2666
02:08:20,200 --> 02:08:23,200
And when the self-driving car sees something new that hasn't

2667
02:08:23,200 --> 02:08:26,200
seen before that the developer didn't expect, like a bicycle

2668
02:08:26,200 --> 02:08:31,200
painted on the outside of a truck, this might lead to confusions

2669
02:08:31,200 --> 02:08:33,200
for the classifiers.

2670
02:08:33,200 --> 02:08:38,200
Yeah, you made a very important observation that kids learn

2671
02:08:38,200 --> 02:08:43,200
on the basis of very few examples compared to deep learning.

2672
02:08:43,200 --> 02:08:50,200
They learn, moreover, in a very simple environment in their nursery

2673
02:08:50,200 --> 02:08:57,200
with fairy tales and interacting with a few people and playing

2674
02:08:57,200 --> 02:08:58,200
with objects.

2675
02:08:58,200 --> 02:09:02,200
And then they walk out into the world and understand traffic

2676
02:09:02,200 --> 02:09:03,200
situations.

2677
02:09:03,200 --> 02:09:06,200
You don't hand down the key to the car yet because they don't

2678
02:09:06,200 --> 02:09:08,200
have a sense of responsibility.

2679
02:09:08,200 --> 02:09:13,200
They can't foresee the long-term effects of their actions.

2680
02:09:13,200 --> 02:09:16,200
So you only let them drive when they're 18.

2681
02:09:16,200 --> 02:09:21,200
But they understand traffic scenes very well when they are six

2682
02:09:21,200 --> 02:09:23,200
or 10.

2683
02:09:23,200 --> 02:09:30,200
So all of this is driven by learning by interaction with a

2684
02:09:30,200 --> 02:09:34,200
simple environment and generalization from there.

2685
02:09:35,200 --> 02:09:40,200
Yeah, well, of course, in 99.7% of all the cases, the self-driving

2686
02:09:40,200 --> 02:09:41,200
car is good enough.

2687
02:09:41,200 --> 02:09:46,200
It's mostly the long tail of cases that leads to situations where

2688
02:09:46,200 --> 02:09:50,200
the system is producing undesirable behavior.

2689
02:09:50,200 --> 02:09:55,200
I was joking a couple of years ago that whenever a journalist

2690
02:09:55,200 --> 02:09:59,200
writes that there will never be self-driving cars, police is

2691
02:09:59,200 --> 02:10:03,200
stopping Tesla with the sleeping driver safely on their way home.

2692
02:10:04,200 --> 02:10:10,200
And so in many ways, self-driving cars exist.

2693
02:10:10,200 --> 02:10:13,200
And they are almost as good enough in the sense that they are

2694
02:10:13,200 --> 02:10:15,200
better than a really, really bad driver.

2695
02:10:15,200 --> 02:10:20,200
But they're just not working to the degree of perfection of a

2696
02:10:20,200 --> 02:10:21,200
very competent driver.

2697
02:10:21,200 --> 02:10:26,200
Yeah, it's very mean to ask them to be so perfect, much more

2698
02:10:26,200 --> 02:10:27,200
perfect than humans.

2699
02:10:27,200 --> 02:10:31,200
They are, as you said, they are, if all cars were self-driving,

2700
02:10:31,200 --> 02:10:34,200
traffic would be much more safe than now.

2701
02:10:34,200 --> 02:10:41,200
But the public takes it very badly if an accident happens that

2702
02:10:41,200 --> 02:10:43,200
could have been prevented.

2703
02:10:43,200 --> 02:10:47,200
Yeah, we're also in an interesting situation where the public is

2704
02:10:47,200 --> 02:10:48,200
mostly the media.

2705
02:10:48,200 --> 02:10:52,200
And the media is at the moment in the US very much seeing itself

2706
02:10:52,200 --> 02:10:56,200
in competition with the tech industry because they are competing

2707
02:10:56,200 --> 02:10:59,200
for the same advertising revenue for the most part.

2708
02:10:59,200 --> 02:11:04,200
And so it's at the moment very difficult to find articles that

2709
02:11:04,200 --> 02:11:08,200
are optimistic and positive about technological developments

2710
02:11:08,200 --> 02:11:10,200
in the media, I find.

2711
02:11:10,200 --> 02:11:14,200
So this creates a very unique situation where even useful

2712
02:11:14,200 --> 02:11:18,200
developments are delayed that could save lives.

2713
02:11:18,200 --> 02:11:22,200
Because they're being seen in competition with existing

2714
02:11:22,200 --> 02:11:26,200
economic and social structures, which also creates enormous

2715
02:11:26,200 --> 02:11:30,200
pressure on AI models like JetGPT.

2716
02:11:30,200 --> 02:11:33,200
I think that JetGPT is a tremendous achievement.

2717
02:11:33,200 --> 02:11:35,200
My kids have been playing with it.

2718
02:11:35,200 --> 02:11:40,200
My daughter has been creating a story of a horse that she got to

2719
02:11:40,200 --> 02:11:43,200
know on the way home from school and then created several

2720
02:11:43,200 --> 02:11:46,200
variants by modifying the prompt until she had the story that

2721
02:11:46,200 --> 02:11:47,200
she liked.

2722
02:11:47,200 --> 02:11:51,200
And then she turned it into a poem that's very catchy rhymes.

2723
02:11:51,200 --> 02:11:56,200
My son used it to explain the system,

2724
02:11:56,200 --> 02:11:59,200
to explain to him how to implement a platformer game.

2725
02:11:59,200 --> 02:12:01,200
And it was explaining him how to structure the project.

2726
02:12:01,200 --> 02:12:05,200
And then he was asking how to make an event loop in Python

2727
02:12:05,200 --> 02:12:09,200
and it printed out source code and explained the source code.

2728
02:12:09,200 --> 02:12:13,200
And he spent several hours copying the code and replete

2729
02:12:13,200 --> 02:12:14,200
and getting it to work.

2730
02:12:14,200 --> 02:12:18,200
So to me, these are systems where you have a little bit of human

2731
02:12:18,200 --> 02:12:21,200
in the loop to make it coherent for a particular task.

2732
02:12:21,200 --> 02:12:25,200
And it's amazing what the thing can already do.

2733
02:12:25,200 --> 02:12:29,200
And it seems to me what's missing to get the system to work is to a

2734
02:12:29,200 --> 02:12:31,200
system that makes it coherent.

2735
02:12:31,200 --> 02:12:36,200
Basically, you can decompose the mind into perceptual systems that

2736
02:12:36,200 --> 02:12:41,200
can in some sense to image guided and audio guided diffusion to

2737
02:12:41,200 --> 02:12:44,200
coalesce to an internal state that is able to reproduce the

2738
02:12:44,200 --> 02:12:45,200
sensory data.

2739
02:12:45,200 --> 02:12:49,200
And then confabulation to build alternatives for solutions,

2740
02:12:49,200 --> 02:12:52,200
alternatives for what could be alternatives for the future.

2741
02:12:52,200 --> 02:12:56,200
And then the third component, which doesn't exist yet, which is

2742
02:12:56,200 --> 02:13:00,200
proving from first principles what works, basically rejecting

2743
02:13:00,200 --> 02:13:02,200
those generations that don't work.

2744
02:13:02,200 --> 02:13:06,200
And then learning those that worked and building up the system

2745
02:13:06,200 --> 02:13:09,200
in a way that is continuously learning.

2746
02:13:09,200 --> 02:13:12,200
It also seems to me that many people cannot change their

2747
02:13:13,200 --> 02:13:15,200
opinion in real time.

2748
02:13:15,200 --> 02:13:18,200
And you have talked to a person that has a strong opinion about

2749
02:13:18,200 --> 02:13:20,200
something that moves deeply into their mind.

2750
02:13:20,200 --> 02:13:24,200
You can present them as arguments, but you have to talk to them the

2751
02:13:24,200 --> 02:13:27,200
next day if you want to see any changes, which seems that seem to

2752
02:13:27,200 --> 02:13:31,200
be parts of mental organization at least in some people require

2753
02:13:31,200 --> 02:13:33,200
offline retraining.

2754
02:13:33,200 --> 02:13:36,200
There's limits to what we can do in online learning.

2755
02:13:36,200 --> 02:13:39,200
Some balancing needs to be done offline while we are decoupling the

2756
02:13:39,200 --> 02:13:42,200
system from the environment and producing data augmentation and

2757
02:13:42,200 --> 02:13:44,200
restructuring.

2758
02:13:44,200 --> 02:13:47,200
I wonder how much of that retraining will also be built into the

2759
02:13:47,200 --> 02:13:50,200
systems where the artificial systems will have to sleep and to

2760
02:13:50,200 --> 02:13:52,200
dream.

2761
02:13:52,200 --> 02:13:57,200
Yeah, before you take an important decision, you have to sleep

2762
02:13:57,200 --> 02:14:02,200
over it and give your subconscious mind the opportunity to

2763
02:14:02,200 --> 02:14:08,200
work on making the ideas more consistent than you are able to

2764
02:14:08,200 --> 02:14:11,200
make them under conscious control.

2765
02:14:15,200 --> 02:14:17,200
Yeah, very much so.

2766
02:14:17,200 --> 02:14:28,200
So I think you rightly said these achievements of GPT-3,

2767
02:14:28,200 --> 02:14:31,200
TETGTP and so on are extremely impressive.

2768
02:14:31,200 --> 02:14:34,200
It's very difficult to see where the limit is.

2769
02:14:34,200 --> 02:14:36,200
I agree with you.

2770
02:14:37,200 --> 02:14:43,200
The transformers have a new, very new architectural feature,

2771
02:14:43,200 --> 02:14:49,200
which is the online computation on the fly of connections and

2772
02:14:49,200 --> 02:14:53,200
of these representation vectors.

2773
02:14:53,200 --> 02:14:56,200
They are computed on the fly.

2774
02:14:56,200 --> 02:14:58,200
That's all very promising.

2775
02:14:58,200 --> 02:15:05,200
But these systems don't have any insight into real world

2776
02:15:06,200 --> 02:15:11,200
geometric mechanics and so on representations of what they talk

2777
02:15:11,200 --> 02:15:13,200
about.

2778
02:15:13,200 --> 02:15:17,200
And they are lambasted mainly for that reason, that they don't

2779
02:15:17,200 --> 02:15:21,200
know what it means, something is dead.

2780
02:15:21,200 --> 02:15:28,200
They just know how people talk about it, but they don't know the

2781
02:15:28,200 --> 02:15:33,200
significance of it or the geometrical arrangement of something.

2782
02:15:33,200 --> 02:15:38,200
And so that is, of course, due to lack of insight, lack of

2783
02:15:38,200 --> 02:15:44,200
interaction, you know, they cannot play with toy objects as kids

2784
02:15:44,200 --> 02:15:50,200
do and cannot get the corresponding insights.

2785
02:15:50,200 --> 02:15:58,200
But I still think that what is missing, what is sort of needs to

2786
02:15:58,200 --> 02:16:03,200
be improved is the data structure of representation of

2787
02:16:03,200 --> 02:16:07,200
themes and of realities.

2788
02:16:07,200 --> 02:16:14,200
And I don't think these vectors that I use these days are up to

2789
02:16:14,200 --> 02:16:17,200
the job.

2790
02:16:17,200 --> 02:16:21,200
I think that the embedding spaces are not necessarily represented

2791
02:16:21,200 --> 02:16:23,200
in full, right?

2792
02:16:23,200 --> 02:16:27,200
If you think about the embedding space as a manifold with 30,000

2793
02:16:27,200 --> 02:16:31,200
dimensions and a lot of resolution, trying to expand this space

2794
02:16:31,200 --> 02:16:35,200
and store it in memory is not going to be feasible for the most

2795
02:16:35,200 --> 02:16:37,200
part.

2796
02:16:37,200 --> 02:16:40,200
So instead what is required is a language that allows to sparsely

2797
02:16:40,200 --> 02:16:43,200
and efficiently construct representations in that embedding

2798
02:16:43,200 --> 02:16:45,200
space.

2799
02:16:45,200 --> 02:16:49,200
The embedding space is a mathematical construct that is basically

2800
02:16:49,200 --> 02:16:53,200
every dimension is a function that describes a feature.

2801
02:16:53,200 --> 02:16:56,200
And that feature has parameters.

2802
02:16:56,200 --> 02:17:00,200
But I see every dimension is a parameter in that feature.

2803
02:17:00,200 --> 02:17:02,200
Yeah, that's right, of course.

2804
02:17:02,200 --> 02:17:08,200
So, you know, I once applied for funds to do face recognition and

2805
02:17:08,200 --> 02:17:14,200
the idea was to collect data images which varied in all those

2806
02:17:14,200 --> 02:17:18,200
dimensions, the identity of the person, the illumination, the

2807
02:17:19,200 --> 02:17:22,200
expression, the texture and so on.

2808
02:17:22,200 --> 02:17:26,200
And I didn't get the money and I'm very glad I didn't get the money

2809
02:17:26,200 --> 02:17:31,200
because this 15-dimensional space or so cannot be filled with

2810
02:17:31,200 --> 02:17:33,200
examples.

2811
02:17:33,200 --> 02:17:35,200
That's totally impossible.

2812
02:17:35,200 --> 02:17:38,200
There's too much space in high-dimensional spaces.

2813
02:17:38,200 --> 02:17:41,200
That's the point you want to make, I suppose.

2814
02:17:41,200 --> 02:17:46,200
So one has to find a way of representing only sub-dimensions,

2815
02:17:47,200 --> 02:17:53,200
low-dimensional projections of that and a means of pasting them

2816
02:17:53,200 --> 02:17:58,200
together as an equivalent of the high-dimensional thing.

2817
02:17:58,200 --> 02:18:02,200
It turns out that when we conceptualize an object, it's chunked

2818
02:18:02,200 --> 02:18:07,200
and a chunk is basically a node that is composed of features

2819
02:18:07,200 --> 02:18:10,200
that define the nature of that bunk.

2820
02:18:10,200 --> 02:18:13,200
And you have these seven plus minus two features.

2821
02:18:13,200 --> 02:18:17,200
It's less, so it's more like five, which means that if you can

2822
02:18:17,200 --> 02:18:21,200
define an object by five features, you have a local function,

2823
02:18:21,200 --> 02:18:23,200
a locally five-dimensional space.

2824
02:18:23,200 --> 02:18:25,200
Maybe sometimes it's nine-dimensional, but it's not much

2825
02:18:25,200 --> 02:18:27,200
more.

2826
02:18:27,200 --> 02:18:31,200
And these few dimensions allow us to construct a family of

2827
02:18:31,200 --> 02:18:36,200
operators that would allow us up to these few dimensions construct

2828
02:18:36,200 --> 02:18:42,200
all the 30,000 other dimensions or millions of other dimensions

2829
02:18:42,200 --> 02:18:45,200
depending on how we look at this function space.

2830
02:18:45,200 --> 02:18:46,200
Right.

2831
02:18:46,200 --> 02:18:52,200
So the essential point here is that a high-dimensional thing gets

2832
02:18:52,200 --> 02:18:57,200
projected down in our brain onto low-dimensional representations

2833
02:18:57,200 --> 02:19:01,200
plus the ability to glue them together.

2834
02:19:01,200 --> 02:19:05,200
And this gluing together, I don't see in the present

2835
02:19:05,200 --> 02:19:09,200
neural technology.

2836
02:19:09,200 --> 02:19:12,200
I think that it happens on a level that we normally don't look

2837
02:19:12,200 --> 02:19:13,200
at.

2838
02:19:13,200 --> 02:19:15,200
It happens in the activation traces in the network.

2839
02:19:15,200 --> 02:19:18,200
So it's not in the weights between the links.

2840
02:19:18,200 --> 02:19:22,200
And it's also not in the synaptic connections between neurons.

2841
02:19:22,200 --> 02:19:26,200
It is in the content of the traces that are moving through this.

2842
02:19:26,200 --> 02:19:32,200
So the neurons and the nodes in the neural network are providing

2843
02:19:32,200 --> 02:19:36,200
the computational machinery to modulate these patterns according

2844
02:19:36,200 --> 02:19:38,200
to the content of the patterns.

2845
02:19:38,200 --> 02:19:42,200
And it's the content of the activation wave that is determining

2846
02:19:42,200 --> 02:19:47,200
how the activation wave is being processed.

2847
02:19:47,200 --> 02:19:51,200
This is something like a distributed computational pipeline.

2848
02:19:51,200 --> 02:19:57,200
I'm involved in a multi-month or probably multi-year intensive

2849
02:19:57,200 --> 02:20:02,200
discussion with a colleague at Amy in Zurich,

2850
02:20:03,200 --> 02:20:06,200
Institute of Neural Informatics, Matthew Cook.

2851
02:20:06,200 --> 02:20:11,200
And he doesn't want to hear of dynamic mappings.

2852
02:20:11,200 --> 02:20:18,200
He says anything like schemata and roll fillers,

2853
02:20:18,200 --> 02:20:21,200
that's all nonsense, he believes.

2854
02:20:21,200 --> 02:20:28,200
And he claims all you need is components,

2855
02:20:28,200 --> 02:20:30,200
which he calls them slips of paper,

2856
02:20:30,200 --> 02:20:35,200
onto which various features are written into slips of paper

2857
02:20:35,200 --> 02:20:38,200
can overlap in a subset of the features.

2858
02:20:38,200 --> 02:20:45,200
And so that makes clear how they belong together.

2859
02:20:45,200 --> 02:20:53,200
Components, each of which is a small set of entities.

2860
02:20:53,200 --> 02:20:57,200
And they overlap in subsets of these entities.

2861
02:20:57,200 --> 02:21:02,200
And that way they can cover a complex situation.

2862
02:21:02,200 --> 02:21:11,200
I defy him again and again to create that way a system

2863
02:21:11,200 --> 02:21:15,200
that can do something like invariant object recognition.

2864
02:21:15,200 --> 02:21:20,200
Or the application of a syntactical rule,

2865
02:21:20,200 --> 02:21:25,200
like subject, verb, object to an arbitrary set of components,

2866
02:21:25,200 --> 02:21:30,200
of appropriate components, of course, to nouns and the verb.

2867
02:21:30,200 --> 02:21:33,200
And which is, of course, very important,

2868
02:21:33,200 --> 02:21:37,200
the cognitive scientists insist on that,

2869
02:21:37,200 --> 02:21:42,200
on the ability to impose an abstract pattern

2870
02:21:42,200 --> 02:21:44,200
onto concrete elements.

2871
02:21:44,200 --> 02:21:48,200
And I think for that you have to have variables

2872
02:21:48,200 --> 02:21:54,200
that make clear this abstract node belongs to this concrete node.

2873
02:21:54,200 --> 02:21:59,200
If you want to represent the sentence John loves Mary,

2874
02:21:59,200 --> 02:22:03,200
you have to make clear that the subject is linked to John

2875
02:22:03,200 --> 02:22:05,200
and the object is linked to Mary,

2876
02:22:05,200 --> 02:22:10,200
because you can also have the sentence Mary loves John

2877
02:22:10,200 --> 02:22:14,200
and then they are called in a different way.

2878
02:22:14,200 --> 02:22:17,200
You need variables to make that distinction.

2879
02:22:17,200 --> 02:22:19,200
What are those variables?

2880
02:22:19,200 --> 02:22:22,200
That's what I call the glue,

2881
02:22:22,200 --> 02:22:26,200
that glues together the abstract form and the concrete elements

2882
02:22:26,200 --> 02:22:28,200
that make it up, for instance.

2883
02:22:28,200 --> 02:22:33,200
Or the texture, you know, the computer graphics people

2884
02:22:33,200 --> 02:22:38,200
have a very good ontological theory of visual scenes.

2885
02:22:38,200 --> 02:22:41,200
They can create them in a very convincing way

2886
02:22:41,200 --> 02:22:44,200
and they create them out of part descriptions

2887
02:22:44,200 --> 02:22:49,200
like shape only or texture only or illumination only

2888
02:22:49,200 --> 02:22:52,200
or motion applied to a shape

2889
02:22:52,200 --> 02:22:56,200
and have a way of putting them together.

2890
02:22:56,200 --> 02:23:00,200
And for that you need to have variables

2891
02:23:00,200 --> 02:23:04,200
that make clear this textural element belongs

2892
02:23:04,200 --> 02:23:10,200
onto this form, on this point on that form.

2893
02:23:10,200 --> 02:23:12,200
And what are those variables?

2894
02:23:12,200 --> 02:23:15,200
We need to find the minimal set of link types

2895
02:23:15,200 --> 02:23:17,200
that we need to...

2896
02:23:17,200 --> 02:23:19,200
One minimal set.

2897
02:23:19,200 --> 02:23:21,200
We didn't optimize for the smallest one

2898
02:23:21,200 --> 02:23:24,200
to perform all the semantic representations

2899
02:23:24,200 --> 02:23:26,200
that we wanted to have.

2900
02:23:26,200 --> 02:23:31,200
And the model that we used was inspired by Aristotle

2901
02:23:31,200 --> 02:23:34,200
and his Four Causa,

2902
02:23:34,200 --> 02:23:38,200
which basically means you have Causa formalis

2903
02:23:39,200 --> 02:23:44,200
and these four cases describe

2904
02:23:44,200 --> 02:23:47,200
partonomic links part of and being composed of

2905
02:23:47,200 --> 02:23:50,200
and being caused by and leading to.

2906
02:23:50,200 --> 02:23:52,200
So basically you have lateral links

2907
02:23:52,200 --> 02:23:54,200
that give you a causal ordering

2908
02:23:54,200 --> 02:23:57,200
and you have compositional links

2909
02:23:57,200 --> 02:24:00,200
that allow you to compose a script

2910
02:24:00,200 --> 02:24:03,200
or a task of subtasks.

2911
02:24:03,200 --> 02:24:07,200
And in this way you can describe arbitrary scripts

2912
02:24:07,200 --> 02:24:11,200
and these arbitrary scripts can express arbitrary functions

2913
02:24:11,200 --> 02:24:14,200
when you combine them with low level operators

2914
02:24:14,200 --> 02:24:17,200
that can, for instance, perform some basic operations

2915
02:24:17,200 --> 02:24:20,200
on the network, sense data in the environment

2916
02:24:20,200 --> 02:24:23,200
in the network itself and so on.

2917
02:24:23,200 --> 02:24:26,200
I mentioned earlier on that I think of intelligence

2918
02:24:26,200 --> 02:24:29,200
as the ability to construct a path

2919
02:24:29,200 --> 02:24:31,200
to a space of computable functions.

2920
02:24:31,200 --> 02:24:33,200
So intelligence is not the ability to compute the function,

2921
02:24:33,200 --> 02:24:35,200
every computer can compute a function

2922
02:24:35,200 --> 02:24:37,200
without being intelligent.

2923
02:24:37,200 --> 02:24:40,200
The trick is to discover that function in the first place

2924
02:24:40,200 --> 02:24:42,200
and to discover this function

2925
02:24:42,200 --> 02:24:45,200
we basically have three perspectives on how to do this.

2926
02:24:45,200 --> 02:24:47,200
The first one is to converge to it.

2927
02:24:47,200 --> 02:24:49,200
That is what deep learning does.

2928
02:24:49,200 --> 02:24:52,200
You have a space of possible functions

2929
02:24:52,200 --> 02:24:55,200
and in that space of possible functions you make large enough

2930
02:24:55,200 --> 02:24:57,200
you start out with some random function

2931
02:24:57,200 --> 02:25:00,200
and then you modify that function along many dimensions

2932
02:25:00,200 --> 02:25:04,200
nudge it many billions of times or trillions of times

2933
02:25:04,200 --> 02:25:07,200
until it gets close to what you wanted to do.

2934
02:25:07,200 --> 02:25:10,200
And you follow a gradient.

2935
02:25:10,200 --> 02:25:12,200
For this it needs to be differentiable

2936
02:25:12,200 --> 02:25:15,200
and this algorithm of stochastic gradient descent

2937
02:25:15,200 --> 02:25:18,200
using back propagation is still the workhorse

2938
02:25:18,200 --> 02:25:20,200
of all machine learning at this point.

2939
02:25:20,200 --> 02:25:24,200
And a second approach is to do hierarchical pattern matching.

2940
02:25:24,200 --> 02:25:27,200
So you look for operators that you've already learned,

2941
02:25:27,200 --> 02:25:30,200
a small library of efficient operators that you have evolved

2942
02:25:30,200 --> 02:25:34,200
and evolution is all you need from this perspective

2943
02:25:34,200 --> 02:25:37,200
that lets you get to the situation that you want

2944
02:25:37,200 --> 02:25:40,200
based on the configuration that you have.

2945
02:25:40,200 --> 02:25:43,200
So these operators are basically looking for activation patterns

2946
02:25:43,200 --> 02:25:45,200
that they match on

2947
02:25:45,200 --> 02:25:48,200
and they change the activation pattern into the next one.

2948
02:25:48,200 --> 02:25:52,200
And in this way you can perform arbitrary functions in the way

2949
02:25:52,200 --> 02:25:57,200
this is also the way in our computers are computing functions.

2950
02:25:58,200 --> 02:26:01,200
And the third one is construction

2951
02:26:01,200 --> 02:26:05,200
and construction requires some degree of memory

2952
02:26:05,200 --> 02:26:08,200
and because you need to be able to retrace your steps

2953
02:26:08,200 --> 02:26:11,200
and you need a way to justify the steps that you're making

2954
02:26:11,200 --> 02:26:14,200
and when to retract them.

2955
02:26:14,200 --> 02:26:17,200
Our consciousness seems to be strongly involved

2956
02:26:17,200 --> 02:26:19,200
in such a construction process

2957
02:26:19,200 --> 02:26:22,200
where we have a stream of consciousness that allows me

2958
02:26:22,200 --> 02:26:25,200
oh I tried this thought before and this didn't work

2959
02:26:25,200 --> 02:26:29,200
so I now retrace it, retract it, modify it

2960
02:26:29,200 --> 02:26:32,200
and I think this one should work for the following reasons

2961
02:26:32,200 --> 02:26:34,200
and then I see the outcome and say no it didn't work

2962
02:26:34,200 --> 02:26:37,200
so this reason was not wrong so I try the next thing

2963
02:26:37,200 --> 02:26:39,200
and this is something that is difficult to achieve

2964
02:26:39,200 --> 02:26:42,200
just this pattern matching or this gradient descent.

2965
02:26:42,200 --> 02:26:46,200
So this constructive discovery of solutions

2966
02:26:46,200 --> 02:26:48,200
seems to be crucial

2967
02:26:48,200 --> 02:26:52,200
and while it seems to me that our deep learning models

2968
02:26:52,200 --> 02:26:54,200
are not very good at constructing

2969
02:26:54,200 --> 02:26:56,200
they are very much able to emulate

2970
02:26:56,200 --> 02:26:58,200
what it would be like to be constructing, right?

2971
02:26:58,200 --> 02:27:02,200
So while GPT-3 or chat GPT are not conscious

2972
02:27:02,200 --> 02:27:05,200
they are able to create a story about something

2973
02:27:05,200 --> 02:27:07,200
that is conscious while they are unable to reason

2974
02:27:07,200 --> 02:27:09,200
they create a story about the reasoner

2975
02:27:09,200 --> 02:27:12,200
and draw on the inferences from that reasoning

2976
02:27:12,200 --> 02:27:15,200
and the more closely you describe the reasoning

2977
02:27:15,200 --> 02:27:18,200
that's reason step by step and so on

2978
02:27:18,200 --> 02:27:20,200
the better the results can become

2979
02:27:20,200 --> 02:27:23,200
and so it's very difficult to determine the difference

2980
02:27:23,200 --> 02:27:27,200
between a system that pretends to perform a certain thing

2981
02:27:27,200 --> 02:27:29,200
and it actually does it, right?

2982
02:27:29,200 --> 02:27:33,200
If you can pretend it well enough you're actually doing it.

2983
02:27:33,200 --> 02:27:37,200
Yeah, yeah, so the same way as you pretend to be conscious

2984
02:27:37,200 --> 02:27:42,200
and I know the only conscious being in the world is myself.

2985
02:27:43,200 --> 02:27:46,200
When did you figure that out?

2986
02:27:46,200 --> 02:27:50,200
We know, we know, we're NPCs.

2987
02:27:50,200 --> 02:27:53,200
How did I figure that out?

2988
02:27:53,200 --> 02:27:56,200
Okay, yeah.

2989
02:27:56,200 --> 02:27:58,200
Christoph, I enjoyed this very much.

2990
02:27:58,200 --> 02:28:01,200
We are at the end of my time limit for now

2991
02:28:01,200 --> 02:28:04,200
and I hope that we get to continue the conversation

2992
02:28:04,200 --> 02:28:07,200
and I hope that we get to continue the conversation

2993
02:28:07,200 --> 02:28:10,200
and I hope that we get to continue the conversation

2994
02:28:10,200 --> 02:28:16,200
soon in Zurich and looking forward to talking more to you

2995
02:28:16,200 --> 02:28:20,200
and I'm very glad that you could make it today.

2996
02:28:20,200 --> 02:28:22,200
Yeah, it was a great pleasure.

2997
02:28:22,200 --> 02:28:27,200
I just had to take an earlier train from Frankfurt to Berlin

2998
02:28:27,200 --> 02:28:31,200
because I had to be there only tomorrow morning.

2999
02:28:31,200 --> 02:28:35,200
I'm sitting here in the room in the so-called Harnak House

3000
02:28:35,200 --> 02:28:37,200
of the guest house.

3001
02:28:37,200 --> 02:28:40,200
Okay, have a nice day.

3002
02:28:40,200 --> 02:28:43,200
Thank you so much for organizing this

3003
02:28:43,200 --> 02:28:46,200
and setting everything up and supporting our discussion

3004
02:28:46,200 --> 02:28:49,200
and I'd also like to thank our audience

3005
02:28:49,200 --> 02:28:51,200
for paying attention, asking questions.

3006
02:28:51,200 --> 02:28:56,200
We didn't get to discuss all of them

3007
02:28:56,200 --> 02:29:00,200
but I'm very glad that you could make this event happen.

3008
02:29:00,200 --> 02:29:02,200
Thank you, Christoph.

3009
02:29:02,200 --> 02:29:04,200
See you soon.

3010
02:29:04,200 --> 02:29:06,200
Thank you, everyone.

3011
02:29:06,200 --> 02:29:07,200
Bye-bye now.

3012
02:29:07,200 --> 02:29:08,200
Bye-bye.

