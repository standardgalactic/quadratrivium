WEBVTT

00:00.000 --> 00:06.000
Hello, everyone. Can everyone see my screen?

00:06.000 --> 00:09.000
You can see your screen, yes.

00:09.000 --> 00:11.000
Excellent.

00:11.000 --> 00:17.000
Great. So, my name is Tanya Greenberg. Today we're going to be talking about self-organization.

00:17.000 --> 00:23.000
Biological and social agents are very different from our present approaches to design artificial agents.

00:23.000 --> 00:28.000
Technological systems are constructed from outside in.

00:28.000 --> 00:35.000
They extend the part of the world with known functionality by forging its deterministic substrate into required functions.

00:35.000 --> 00:43.000
This is, however, this is true whether we are building a bicycle in a workshop or learning algorithm in a software development environment.

00:43.000 --> 00:47.000
However, biological systems are growing from inside out.

00:47.000 --> 01:01.000
They organize an indeterministic substrate with unreliable properties into a structure that converges to serving the required function and which will even self heal and regrow when they are being damaged or disturbed.

01:01.000 --> 01:08.000
Today, we will be exploring this dichotomy and how it applies to technological systems, especially artificial intelligence.

01:08.000 --> 01:14.000
So, our first presenter today will be Professor Christoph von der Malzburg.

01:14.000 --> 01:27.000
Today, Professor von der Malzburg is a senior fellow at the Frankfurt Institute for Advanced Studies and a visiting professor at the Institute of Neuroinformatics at ETH Zurich.

01:27.000 --> 01:46.000
Previously, he served as a research scientist at Max Planck Institute in GÃ¶ttingen and professor of computer science, neuroscience, physics and psychology at University of Southern California, and as director of the Institute of Neuroinformatics at Bochum University.

01:46.000 --> 02:00.000
He has founded two successful companies and has received many awards, including Pioneer Award of the Neural Network Council IEEE and the HAB Award of the International Neural Network Society.

02:00.000 --> 02:04.000
Christoph, the floor is now yours.

02:04.000 --> 02:06.000
Okay, thank you.

02:06.000 --> 02:09.000
Can I share my screen?

02:09.000 --> 02:11.000
Yes, you should be able to.

02:11.000 --> 02:13.000
I will stop my show.

02:13.000 --> 02:22.000
Yeah, continue.

02:22.000 --> 02:27.000
Okay.

02:27.000 --> 02:29.000
Here we go.

02:29.000 --> 02:42.000
So, first, I would like to discuss the how to determine the information content of the brain and of the environment with which the brain is dealing.

02:42.000 --> 02:48.000
And the right way of going about that is Kolmogorov complexity.

02:48.000 --> 03:03.000
The information content according to the information content of the structure is measured in terms of the bit length of the shortest algorithm that creates that structure that can create that structure.

03:03.000 --> 03:15.000
Let me give you a simple example. The Julia set is a very complicated thing. In order to describe it as is, you need a literally an infinite amount of information.

03:15.000 --> 03:30.000
You can blow it up as long as you want and it still continues to create more to contain more structure, but in order to create it, it needs a few line algorithms, the guts of which are shown on the lower left.

03:30.000 --> 03:51.000
The flip side of low Kolmogorov complexity is that the structure it is describing is highly structured. It has high structural regularity, which is again displayed here you can see, there is a lot of structural regularity, which never repeats, but you

03:51.000 --> 03:57.000
can see the inner coherence of the whole thing.

03:57.000 --> 04:02.000
Now, what about our natural environment.

04:02.000 --> 04:16.000
I claim it has very low Kolmogorov complexity. Otherwise we couldn't perceive it and science wouldn't work. And I think virtual reality is a very good tool to show that

04:16.000 --> 04:31.000
the working environment in including their dynamical interaction with a viewer can be created with very little information. So the basis of that is computer graphics in computer graphics.

04:31.000 --> 04:45.000
That's an amazing field that has developed over 20 or 30 years the ability to create very realistically looking scenes and also dynamic scenes.

04:45.000 --> 05:06.000
And that created something like an ontology of our visual environment. And I guess you all know how it is done it is a is a compositional game you start with a representation of the shape in terms of shapes in terms of wire frame models.

05:06.000 --> 05:27.000
When you give them surface markings and reflectivity, you transform them by known laws into a scene in position them in the scene, you, you project the scene with projective geometry into a virtual camera yet you illuminated.

05:27.000 --> 05:34.000
So that is something that can be done with a few gigabytes of program code.

05:34.000 --> 05:44.000
We all learn in a very deliberately designed simple environment in nursery.

05:44.000 --> 05:59.000
And although this nursery is very limited in terms of numbers of samples and complexity, we later walk out into the world and can reflect it represented in our brain.

05:59.000 --> 06:16.000
Now, as I said, to generate this kind of to simulate if one care to do that, that kind of environment including the social interactions, it would take a few gigabytes of a program like a computer games program virtual reality program.

06:16.000 --> 06:27.000
So the color of complexity of our environment as far as is needed for learning is gigabytes only.

06:27.000 --> 06:49.000
So let me remind you that, at least to psychologists, the speed with the rate at which we pick up information into our long term memory is a bit or so per second, so that over your whole lifetime you pick up a gigabit.

06:49.000 --> 07:09.000
I've heard that I found that offenses to the rich to the richness of my inner life, but turned it the other way around and say it takes a gigabyte to create the world as I know it to, to represent it.

07:09.000 --> 07:27.000
The line is a very complicated thing, a simple estimate of the amount of information you need to describe the wiring is a petabyte 10 to 15 bytes. Here's a little computation on the screen, how I arrived at that.

07:28.000 --> 07:42.000
The color of complexity of it must be very low. It takes one gigabyte of genetic information to create an organism like mine, including the brain.

07:42.000 --> 07:56.000
And as we saw it takes a few gigabytes to train it to at least some level of competence. So the color of complexity of the brain is very low.

07:56.000 --> 08:11.000
The brain is highly structured. And I think this is the query theory has to shoot for to describe the particular kind of structure that is dominating the brain.

08:11.000 --> 08:23.000
The question arises, what is the Kolmogorov algorithm by which the brain is actually created.

08:23.000 --> 08:29.000
The, as I said, the brain is dominated.

08:29.000 --> 08:41.000
No, here's my slide. The brain is the result of self organization, of course, where the genes play the role of parametric control.

08:41.000 --> 08:59.000
And likewise the sensory signals they just influence this process of self organization, which however is totally dominated by the structure that we just concluded must be structuring it.

08:59.000 --> 09:22.000
So the brain is dominated by attractor patterns and see that in, in analogy to crystals or to touring patterns if you heard of, heard of them to soap bubbles, or the Golgi apparatus that is in all the eukaryotic cells, all the cells of your button.

09:23.000 --> 09:49.000
Those structures arise by the more or less chaotic interaction, dynamic interaction of the building elements of those systems. These patterns emerge as one, as one called seven speaks of emergence they just arise because they can self stabilize they can prevail in a dynamic game.

09:50.000 --> 09:56.000
So that poses the very important question what are the emerging patterns in the brain.

09:56.000 --> 10:12.000
There is, thank goodness, a very good paradigm for that that has been studied 40 years ago, very intensively I must say I was in the middle of this with dozens of different types of experiments done on frog and fish.

10:12.000 --> 10:35.000
The question is, how come that the fibers that grow out in some stage in the embryo from the, from the retina on the left on the left, and grow out to target structures in the brain the optic tecton for instance here on the right.

10:35.000 --> 10:47.000
And I found that these fibers, which at least in some species start out in a rather random pattern, order themselves in over some time.

10:47.000 --> 11:06.000
Well, at the end, you have a beautiful picture, appearing here in the tecton, when a picture is shown into the retina and a connectivity pattern when neighboring ganglion cells in the retina connect to neighboring cells in the tecton.

11:06.000 --> 11:28.000
And I found these fibers nowhere to go. There was a handful of theories, all of them died, given the enormous flexibility of the system, its resistance to experimental interventions and quirks of natural development.

11:28.000 --> 11:43.000
And the one that survived, and I was one of the people who proposed it with my friend David Wiltshire and Alexander Heusler is network self organization.

11:43.000 --> 11:54.000
The claim is that is the Kolmogorov algorithm of the brain, and it works like so a network and initially more or less random network create signals.

11:54.000 --> 12:19.000
The signals act back on the network by synaptic plasticity, and this loop this initially unstable loop continues until a network structure is reached that stabilizes itself via the signals that create signals that stabilize that same network and retinotopy is just one example of that.

12:19.000 --> 12:37.000
In some leap of faith, one has to see, or if you don't take it as a logical conclusion, one has to see the structure of the brain as an overlay of lots of self organized network patterns.

12:37.000 --> 12:53.000
These attractor networks are characterized by two properties one is the consistency of different pathways, a single pathway can only grow if it gets cooperative help from other pathways coming from the same source.

12:53.000 --> 13:09.000
This is going to the same target cooperativity. And this is bridled in by a sparsity constraints varsity meaning a limited number of inputs to the same neuron or outputs from the same neuron.

13:09.000 --> 13:14.000
That's a simple characterization of attractor networks.

13:14.000 --> 13:35.000
The reason for this is that the brain is an overlay of lots of attractor nets, each having its own support as a set of of neurons, and each time that set of neurons active, it works a little bit on on its connectivity until it has reached attractor structure.

13:35.000 --> 13:41.000
And you can be part of a number of attractor nets.

13:41.000 --> 13:52.000
The central thesis towards I have been driving is the emergent nets self organized nets are the brain spires.

13:52.000 --> 14:08.000
And this is a very bold, it took me a while to sort of put my belief behind this. This is a very bold statement of course, saying that the mechanics or network self organization is at the base of the structure of our mind.

14:08.000 --> 14:25.000
The structure of our brain. That is one half of that of the hypothesis and the other one is the structures that arise by this mechanics are a powerful Bay bias to understand the environment.

14:25.000 --> 14:42.000
The statistical learning mechanism needs to have a very strong bias, it tunes it to the phenomenon that is to be learned from. And so the idea here is the conclusion here is that the buyers in our brain is network suck.

14:43.000 --> 15:06.000
The attractor nets are the data structure of the mind they they form a construction kit for mine states. They are the Lego blocks of the mind of the chunks of cognitive theory and the syntax the grammar, under which they combine into each other is any

15:06.000 --> 15:26.000
structure of co active networks has to form a stabilized a self organized a attractor net that would be given enough time to be stable under self organization is in principle a very simple system.

15:26.000 --> 15:41.000
I just give a simple illustration to make it more concrete face recognition that was the base at the base of two companies I've co funded in the past in California in Germany.

15:41.000 --> 16:03.000
So the idea is the image of an object or a face enters primary visual context as we all know, as a 2D array of local feature detectors, which are, as we also know, connected literally connected by short range connections.

16:03.000 --> 16:22.000
So it is a tiny step from there to assuming that what the input image activates is local connectivity fragments fragments of connectivity, which seamlessly blend into each other to represent the whole face.

16:22.000 --> 16:44.000
One order to record in in order to recognize it, you need a model of that phase in another part of the brain if you see form complex, and in order to establish the relationship the similarity relationship you need fiber projections between a primary visual

16:44.000 --> 17:05.000
and the, and the info temporal context, which links corresponding parts to each other. These projection path networks have to switch as quickly as the images change in the primary context.

17:05.000 --> 17:11.000
They are called shifter circuits by Charles Anderson and by David van Essen.

17:11.000 --> 17:31.000
Now let me just point out that the whole structure here that arises in a fraction of the second when you look at the face is an attractor network is composed of lots of little net fragments that have been previously prepared, and that fuse into each other to form this representation.

17:31.000 --> 17:54.000
So let me, let me go one step further and submit the idea that we comprehend objects by linking them sensory objects by linking them to schematic description there are schematic description that are put together by

17:54.000 --> 18:07.000
artwork fragments that have been trained before. So let me come to my end and repeat my central thesis emergent nets are the brain spires.

18:07.000 --> 18:10.000
Thank you.

18:10.000 --> 18:14.000
Thank you very much Christoph that was an excellent presentation.

18:14.000 --> 18:18.000
So.

18:18.000 --> 18:21.000
You want me to stop sharing I suppose.

18:21.000 --> 18:23.000
Yes, you could do that.

18:23.000 --> 18:43.000
I think the share will be interrupted when Yuri starts. So, um, our next presenter is Professor Yuri was a Bojaki, Yuri Bojaki is the biggest professor of neuroscience, NYU School of Medicine, and is among the top 0.2% of most cited neuroscientists.

18:43.000 --> 19:01.000
He has contributed to the emerging under emerging understanding of the dynamics of hippocampal system, and the recognition of the importance of temporal firing properties in the formation of neural codes is overarching hypothesis is that the numerous rhythms that the brain perpetually

19:01.000 --> 19:15.000
are responsible for segmentation of neural information and communication across brain regions. He proposed how these rhythms support a brain syntax, a physiological basis of cognitive operations.

19:15.000 --> 19:25.000
His most influential work is known as the two stage model of memory trace consolidation, and has been adopted as a framework at several leading laboratories around the world.

19:25.000 --> 19:28.000
Yuri the floor is now yours.

19:28.000 --> 19:36.000
Thank you Tatiana. Let me use this opportunity to ask a few questions and then, then I go to my presentation.

19:36.000 --> 19:52.000
The first thing I like to ask is that, you know, what makes us so successful as a species. And the answer is that we are the, perhaps only or very exceptional species that was capable of externalizing brain function.

19:52.000 --> 20:03.000
We expand the body and expand the brain we make machines clocks rulers computers to help us and obey us, not to substitute or compete with us.

20:03.000 --> 20:09.000
Now, if you take this position and you can ask, you know, how is AI.

20:09.000 --> 20:22.000
And why do we call it artificial intelligence in the first place I did always bug me know what if we are talking about artificial smartness artificial creativity thinking artificial consciousness.

20:22.000 --> 20:41.000
What just happened that AI survived and because that was a community agreement, you know, and what if, for nine months, named a computer and artificial brain then we would be having a branch sponsored by governments and we will be all researching artificial

20:41.000 --> 20:47.000
brains. Now, what is so special about intelligence.

20:47.000 --> 20:55.000
Intelligence in biology is typically defined as the ability of the species to survive and prosper in its own niche.

20:55.000 --> 21:05.000
And the emphasis to me is on niche, because it would be unfair to put a human underwater for 10 minutes and then conclude that the fish are more intelligent.

21:06.000 --> 21:12.000
All all species survive and perform in their own niche.

21:12.000 --> 21:26.000
And the question is then what is the niche for AI. If every intelligent agent requires a, a, a, a environment to leave what is AI is defined environment.

21:26.000 --> 21:29.000
Now,

21:29.000 --> 21:38.000
we need to do a artificial version of something or anything. We need to give it have a good definition of the real thing first.

21:38.000 --> 21:47.000
And let's see how it works with intelligence, like space and time. Intelligent is an abstract the used idea.

21:47.000 --> 22:02.000
Ideas become scientific only then we begin to measure them. Measuring needs scales. Time and space became scientific terms that began to measure duration and distance by clocks and rulers.

22:02.000 --> 22:12.000
Intelligence also became a scientific term that we made up a scale for measuring IQ. Historically, this was introduced as a tool for selecting soldiers.

22:12.000 --> 22:23.000
During the first fall, first world war, for particular goals, you know, goals is a very important thing is was classification can happen only on the in the presence of a goal.

22:23.000 --> 22:34.000
But today, the goals of AI are very different, you know, different groups of people come up with their, their own creative definitions and relate them to their own rulers.

22:34.000 --> 22:43.000
Unfortunately, there is no agreed common ruler against which we can measure intelligence of humans, other animals, machines and computer algorithms.

22:43.000 --> 22:53.000
So, when it comes to defining AI, one may wonder this is a fantastic system or a field that can leave alone.

22:53.000 --> 23:10.000
Or, indeed, as some people say, you know, AI should be inspired by the brain, or whether brain scientists like myself should be looking out for AI people like you guys to see how much we can steal from you to understand the brain.

23:10.000 --> 23:23.000
So, in order to do that, you know, we'd like to see is what kind of, of aspects of hypothesis or statements AI is using currently from brain models.

23:23.000 --> 23:28.000
Now, I have to share the screen here.

23:28.000 --> 23:31.000
And

23:31.000 --> 23:33.000
there we go.

23:33.000 --> 23:35.000
Share

23:35.000 --> 23:37.000
and just view.

23:37.000 --> 23:39.000
And

23:39.000 --> 23:43.000
let's go. So,

23:43.000 --> 23:55.000
we can distinguish basically three extreme versions of how we think about brains in general. One is, I would say is still dominant.

23:55.000 --> 24:07.000
And it's based on the key words like random organization, egalitarian, have rules, the eye balance, noise, and sleep plays no importance whatsoever.

24:07.000 --> 24:18.000
You know, we create robots so they can work 24 hours a day, and we create algorithms to work 24 hours a day but be humans and other animals sleep also.

24:18.000 --> 24:27.000
And the whole idea here is that you start with a simple brain and make it more and more complex and complexity scales with experience.

24:27.000 --> 24:30.000
You can call this model Tableau-Raza and writing in.

24:30.000 --> 24:41.000
And Christoph very eloquently already pointed out that you know this is not the kind of brains we almost brain scientists look at or imagine.

24:41.000 --> 24:51.000
The brains have internal dynamics, and it's a common word, but we don't really explain what it is so I give it a try and explain it but it is.

24:51.000 --> 25:08.000
So the alternative of the AI-like or the Tableau-Raza or the Blake-Slang model would be an alternative which is an autonomous system, a self-organized system, which has very different rules instead of egalitarian.

25:08.000 --> 25:18.000
It has strong skewed rules and sleep is an essential feature that is necessary to maintain its own dynamics.

25:19.000 --> 25:39.000
Instead of the clean paper, the brains come with a pre-configured dynamic, and that pre-configured dynamic gives you an enormous realm of possibilities, typically sequences, and learning under this model is not a synthesis,

25:39.000 --> 25:47.000
it's not putting something into the brain, but I'm masking and matching an existing pattern with the experience.

25:48.000 --> 26:00.000
The way how we can summarize these two models in just very simple sentences, you know this is the famous short summary of have is that neurons that fight together, why together.

26:00.000 --> 26:15.000
In the other end, you can say that neurons that are born together will why together, therefore they will fight together with a much higher probability that neurons that are not born together.

26:15.000 --> 26:25.000
Now, this is a very simplified view, of course, of the brain and I have a checkbox here saying which side I prefer.

26:25.000 --> 26:30.000
And of course, you would like to know whether there are supports for any of these things.

26:30.000 --> 26:40.000
You know, I have never met anybody in my neuroscience community would say, oh, the brain is a Tableau-Raza, but there are famous people such as this guy here who said exactly that.

26:41.000 --> 26:48.000
You know, it's not so long ago that there's a notebook and then you have to fill it up with experience.

26:48.000 --> 26:57.000
Here's another giant, you know, John Hopfield, who not so long ago said, you know, large number of simple equivalent components.

26:57.000 --> 27:08.000
So, this is exactly where a lot majority of AI people live on and I'm happy to hear the, you know, you contradicting me.

27:08.000 --> 27:19.000
And of course, there are new areas of AI that are different and they are more brain inspired but I think large and by a large even your sciences is following these principles.

27:19.000 --> 27:31.000
In contrast, you know, if we are looking for fundamental laws or fundamental rules in the brain, then not too many. This is not like physics, but at least we have one overwhelming rule in the brain.

27:31.000 --> 27:38.000
This is called the wave effectal law, which describes our subjective perception as a log rule.

27:38.000 --> 27:53.000
And there are many, many attempts to show, you know, why this is the case and it's also not only for our perceptions, but it applies to almost everything, such as short-term memory, long-term memory, space, duration perception and so on.

27:53.000 --> 28:10.000
The reason, the fundamental reason, I think, why this log rule occurs is because the brain has a dynamic range, extraordinary wide dynamic range in almost everything from synaptic ways to firing race, population, synchrony and so on.

28:10.000 --> 28:34.000
And a lot of log-lobal, they show log-normal distribution, or log-normal like distributions. And this dynamic is based by a skewed system, which is as, as Christoph already pointed out, this is not from random, not random organization at all, but the connectivity can be described by a log rule,

28:34.000 --> 28:45.000
which means that one area of the brain is connected to a handful of other areas in the brain very strongly.

28:45.000 --> 28:59.000
50% of these connections are to a handful of areas, but the other 50% is to a very large number of other areas with weak connections. And this is what is expressed by the log rule.

28:59.000 --> 29:08.000
Of course, these are interesting observations, these are only statistics, but the question is whether these are statistical curiosities only, or they serve something interesting.

29:08.000 --> 29:26.000
And I think what they serve is exactly what the brain have to solve, which is a fight, it's a continuous struggle of war between wide dynamic range, stability, resilience, homostasis, redundancy, degeneracy,

29:26.000 --> 29:32.000
and, you know, this is one end, the other end, if you want, will be plasticity and robustness.

29:32.000 --> 29:48.000
So, if these are fundamental rules of the brains, then we know that there are small brains and large brains, and how will they react in their niche where they live.

29:48.000 --> 29:58.000
And this is that, not the answer, but my hypothesis is that the main goal of the brain is to maintain its own dynamic.

29:58.000 --> 30:07.000
Second, is to generate an output, and to see the consequences of its output, and predict the consequences of this output.

30:07.000 --> 30:20.000
This whole creature is generating an output. It interacts with the body and with his own niche in the environment, and then it events sensors to make the prediction better.

30:20.000 --> 30:25.000
And the goal, of course, is to predict this is the consequences of its own actions.

30:25.000 --> 30:41.000
They get larger. There are many, many loops added to this. But the entire goal of this is exactly the same. But now, this organism, such as your brain and my brain, can predict the future at a much longer time scale, and in the much,

30:41.000 --> 30:46.000
much more noisy complex environment.

30:46.000 --> 30:50.000
Now, this is not yet cognition. There is one more trick here.

30:50.000 --> 30:57.000
Namely, we have to disengage the brain from the world.

30:57.000 --> 31:13.000
What I see is not because there are photons on my retina. I can see, you know, I close my eyes, and the screen doesn't appear. I can still see Tatiana and, and, and everybody on the screen in my mind is because what we see is actually the brain

31:14.000 --> 31:32.000
computation. And if that is true, then we can temporarily, or for a longer time, disengage or remove from the environment and keep going on with the computation and do, and compute what if scenarios, what if I didn't wake up this morning early enough to come to this talk,

31:32.000 --> 31:37.000
and what would be the consequences of this. Therefore, I put my alarm clock and so on.

31:37.000 --> 31:42.000
And so these are the kind of things that these complex brain can do.

31:42.000 --> 31:56.000
Now, there is one more bug that is, is complicated because all you see is here is loops loops loops loops and complicated loops and interactive loops and this looks like a, a Tullamo cortical system.

31:56.000 --> 32:00.000
So here is our cortex and the Tullamo's.

32:00.000 --> 32:10.000
There are two tricks here, one is that we, the brains invented side loops. There are two side loops here, the cerebellum and the base of ganglia. They are first cousins.

32:10.000 --> 32:18.000
They like each other, they compute similar things and they complement each other. And the third loop is the hippocampus.

32:18.000 --> 32:27.000
You may wonder if you would like to make a AI system, you know, why are these side loops, what they are good for, and how they work.

32:27.000 --> 32:32.000
So in the rest of my time, I'd like to show you some examples how they actually work.

32:32.000 --> 32:42.000
So my first claim was that the number one goal for the brain is to maintain its own dynamics. Is it expensive or is it, does it come for free.

32:42.000 --> 33:00.000
So recently, then living standard grad students in my lab devoted his five years here and calculated what we did is that what you see on the left is the distribution of inter spike intervals, that is how regular spikes spike or work.

33:00.000 --> 33:11.000
And that they are pretty irregular most of the time. And in the x axis you can do this it is like interval and every single line there are a few thousand neurons here in the hippocampus.

33:11.000 --> 33:16.000
Every line is the inter spike interval distribution of a neuron.

33:16.000 --> 33:23.000
And you can see is that that there is this nice shift that is different from neuron to neuron.

33:23.000 --> 33:39.000
So there are fingerprints that are neuron individual neuron characteristic firing patterns, which is on the low end of the firing. This can be zero to zero point zero one zero point zero zero one very low frequency firing.

33:39.000 --> 33:55.000
I've ordered many people for a long time you know why do neurons fire it is such a low rate that doesn't make sense you know they cannot do anything, except that was discharging their partner inter neurons, but they don't have any information value.

33:55.000 --> 34:06.000
Now, it turns out that there are other regimes, such as here you can see a band here that all neurons are in the same band. This is in the hippocampus this is the theta band.

34:06.000 --> 34:21.000
There's another band here but but gamma there's another one here is a bursting pattern and so on. So, by analogy to physics, we call this state here, the individual specific state, the ground state.

34:21.000 --> 34:33.000
And then the other states that are common to all neurons or most of the neurons is the activated states. So this is shown here in the cartoon manner that every neuron can have its own ground state.

34:33.000 --> 34:41.000
And is activated state activated states means they are working together with many other neurons for a purpose.

34:41.000 --> 34:56.000
What fraction of the neurons pan in their ground state to do nothing or to maintain your dynamics nothing meaning no communication no perception no thinking and so on.

34:56.000 --> 35:12.000
And the answer is astonishingly high. This is a figure that shows the hippocampus the preform cortex, basal anatomical blood, frontal cortex, visual cortex, the thalamus doesn't matter where you go and what state but it sleep was waking.

35:12.000 --> 35:18.000
Most of the spikes everywhere in the brains are devoted to maintain the brain dynamic.

35:18.000 --> 35:30.000
So I made my first point. My second point is that there are many euros that are silent, or they are firing these scattered spikes, such as the ones I just showed you that they are they are in the ground state.

35:30.000 --> 35:47.000
So, for example, if you are a place neuron, you can see my cursor here probably if the animal is running in an environment and you are recording from the hippocampus then, then there are play cells and this is place cell number one place and number two place and number three four and so on.

35:47.000 --> 35:53.000
But there are half of more of the neurons that died like this yellow cells that they don't do anything.

35:53.000 --> 36:02.000
So how do we know that are they part of this attractor that Christopher was talking about, talking about, or they are distinct from it from each other.

36:02.000 --> 36:20.000
So how do we know that the way how to do it is do a trick what Manuel Valero did is is optogenetically activated cell for a short period of time and again short period of time periodically 20 milliseconds for thousands of times that doesn't perturb the system.

36:20.000 --> 36:32.000
So how are you, and then you're on fires a spike, and then it fires a spike and not a spike. So now this neuron the same neuron that you have seen before that doesn't have any place field all of a sudden does have a place for.

36:32.000 --> 36:41.000
So we can be we show that and this place field is married to a particular position just like normal places and this is another one.

36:41.000 --> 36:45.000
And then there are many of these. In fact, most of them.

36:45.000 --> 36:58.000
If you have the probing done right. So the middle one would be the noisy one that you wouldn't see otherwise because the firing rates are too low compared to outside a field and we don't know where the field is.

36:58.000 --> 37:08.000
But if you probe the neuron, then we can unmask place fields from every single C1 pyramidal neuron in the hippocampus.

37:08.000 --> 37:16.000
I mean, it is part of the same exact attractor, except it doesn't spike it doesn't have an output.

37:17.000 --> 37:27.000
Unmasking is as effective as making. So remember that, you know, on the one hand you have to synthesize you have to make neurons and you make them.

37:27.000 --> 37:43.000
The fire the way how the world makes them fire or the other one is that it just happens when you are searching into something. There's a high probability that a particular pattern occurs and that pattern can be associated no link to something meaningful.

37:43.000 --> 37:53.000
So where do these preconfigured lists of sequences come from.

37:53.000 --> 38:02.000
And the answer is perhaps by from either evolution or evolution or embryonic states.

38:02.000 --> 38:20.000
So now we are doing a little bit of a neuro-ocular and we are asking is that if you are recording from an adult brain, how would we be smarter to know how this neuron should behave if we know that this neuron and another neuron born in the same day or in a different day.

38:20.000 --> 38:34.000
And to do that, Roman who started my lab, he labels neurons at different stages of development, such as embryonic states 131415 and 16 days, mark them permanently.

38:34.000 --> 38:45.000
So when the animals grow up, then we can compare the adult animals as and and and know which neurons were born on the same day, and which neurons were not.

38:46.000 --> 39:03.000
And the answer is, even if we go to those neurons that that we're born together, they will fight together the same data cycle, the same shop with cycles, the population level they like to be together, they form cell assemblies, and even at the very extreme

39:03.000 --> 39:12.000
level, this black neuron was born on on 14 days, if 14 days and it has one field and it has another field.

39:12.000 --> 39:21.000
The blue neuron was born on the same day and it has a similar field, and the red neuron was born at a different day at this similar field.

39:21.000 --> 39:33.000
And if you do the statistics on a large number of neurons, we find that neurons that are born together, they do many things similarly, and they fire together and fire together.

39:33.000 --> 39:47.000
Well, this brings us to the, the new brain picture, which is my spaghetti brain, which shows what suggests that we are not making complex brain from simple brains.

39:47.000 --> 40:06.000
The brains are devoted, devoting their enormous resources to maintain the dynamic that brain dynamic does not change or scales with learning or experience.

40:06.000 --> 40:23.000
My brain, your brain or that of a, a totally unexperienced brain or the brain of Albert Einstein are not necessarily different in complexity, because learning is not a, not adding, but it's a matching process.

40:24.000 --> 40:45.000
In this spaghetti, the thicker ones that have already be associated or linked to experience. And so, you know, without without analogy, it would be like a Chinese dictionary where there are many symbols all the symbols are there for communication, which is don't understand them but we have to ground them by knowledge such as knowledge of English words or in our

40:45.000 --> 40:48.000
cases is experience.

40:48.000 --> 41:11.000
So, the interesting challenge to day I if you are interested in brain inspired system, maybe we should be looking at the right end of my first slide, which is this slide here that we can create a system that already has a realm of possibilities for matching, but another requirement that that system should be

41:11.000 --> 41:20.000
supporting a goal it has to live in a niche. It has to be embodied, it has to have a

41:20.000 --> 41:29.000
thing that is deciding or constraining the features that it can support with itself organized complexity.

41:29.000 --> 41:38.000
And, you know, I have a few more things to say but if you're interested then just read this book. Thanks a lot.

41:38.000 --> 41:58.000
Thank you very much. That was an excellent presentation. So, um, next up, we have Professor Dave, actually, let me quickly create a spotlight.

41:58.000 --> 42:01.000
Okay.

42:01.000 --> 42:11.000
So, David Ackley is emeritus professor of computer science at University of New Mexico. David received his PhD from Carnegie Mellon University.

42:11.000 --> 42:25.000
Before starting his academic position at the University of New Mexico. He was a member of the cognitive science research group at Bell Core is ongoing research interests center on artificial life models and real artificial

42:25.000 --> 42:40.000
life models. Current research emphasis include genetic algorithms and programming distributed and social computing robust self aware systems and computer security. Dave the floors know yours.

42:40.000 --> 42:56.000
Yeah, I wasn't exactly sure how I fit in here. Christoph is an incredible legend I mean I started doing computer neural networks in the 80s and graduate school and I was already reading his papers.

42:56.000 --> 43:02.000
And then Georgie's talk was just an incredibly perfect setup for what I want to say.

43:02.000 --> 43:19.000
You know, I, my interest is making computers do new things by themselves, because that's cool. And so I've been doing that my entire career and I, the big problem is of course is that once I've gotten to do it it's not cool anymore so I have to kind of keep moving on and keep feeding the

43:19.000 --> 43:35.000
with new cool stuff so I went from neural networks to genetic algorithms to artificial life started doing in the 90s started doing biological approaches to computer security trying to do automated diversity generation to try to make it so that you know if

43:35.000 --> 43:47.000
they're going to be bugs in software and there were going to be bugs in software. Well then maybe the attacker would have to solve a different problem for each copy of Excel rather than just being able to kill all of them at once.

43:47.000 --> 44:07.000
And, you know, by the late oos. I was depressed about computer security I was depressed about the impossibility of fixing computer security, you know, and the problem wasn't that we couldn't get the you know users to change their passwords or apply patches

44:07.000 --> 44:17.000
or that programmers were incompetent or that managers ship crap, although all those things are true. The problem is, even if those were all fixed.

44:17.000 --> 44:34.000
We still would have incredible computer security problems that are just getting worse and are going to continue to get worse, as people figure out how to exploit the incredible fact that the way we've designed computers is one bug is all it takes to take over the entire machine.

44:34.000 --> 44:49.000
And, you know, physical systems, living systems, brain systems are not like that. I mean, except in very, very rare circumstances. And so I said, Well, you know, what is the actual problem here.

44:49.000 --> 45:06.000
And the problem is, I've concluded the underlying architecture of computation, the CPU and random access model is broken. I mean, it's fine for small systems. But as the system gets bigger and bigger and bigger, it's terrible.

45:07.000 --> 45:24.000
My, my goal is to say, Well, how can we come up with a new architecture, which will be more inherently robust, much more like the attractor networks, the overlapping attractor networks that Chris was talking about, much more like the brain spaghetti that you're

45:25.000 --> 45:43.000
just showing, and yet still be able to engineer with it somehow. And my answer is, Well, number one, we have to stop eating the glass sandwich, and this is the glass sandwich. The idea is, the purpose of a computation is to connect physics to value to connect

45:43.000 --> 45:56.000
the matter to money to put it not to find a point on it. And the way we do that is we build hardware, we build these digital electronic circuits that in fact have tremendous redundancy in them.

45:56.000 --> 46:12.000
A wire that we use to carry one bit could easily carry dozens or hundreds or thousands of different values, what some degree of error, but we don't do that we send one bit down and we stick amplifiers everywhere to squish that one bit back over and over and over again.

46:12.000 --> 46:29.000
And that heroic act of redundancy in digital circuitry is what makes the hardware so reliable that the software level can just assume that it's perfect. Just assume a trick to present a synchronous unit.

46:29.000 --> 46:33.000
I need to find the best way to learn math and science.

46:33.000 --> 46:36.000
Okay.

46:36.000 --> 46:43.000
I'm not sure who I heard.

46:43.000 --> 46:55.000
The circuitry of the electronic circuitry is incredibly redundant and therefore software can be incredibly non redundant. And that's baked into the DNA of computer science.

46:55.000 --> 47:06.000
The idea is, you should never, you know, don't repeat yourself that's a mantra and software engineering. You should use caches if you've computed something you should remember it don't compute it again.

47:06.000 --> 47:23.000
And all of that is built on top of the idea of, we have to trust the hardware is perfect, but that guarantee that hardware provides always has an asterisk, because of course there is still some probability of failure some probability of an undetected error coming

47:23.000 --> 47:36.000
from the hardware level and reaching the software level, we just engineer it so that it's low enough that we can finish playing solitaire, or doing whatever program we want to do with having vanishing low chance of anything going wrong.

47:36.000 --> 47:47.000
Similarly, once the computations get really really big like data several data center level 10s of thousands of these machines all owned by a single organization.

47:47.000 --> 48:04.000
And that's bringing them failing because that remaining level of failure is there. And they start applying robustness features, but for everybody else there's basically, you know, electronic circuit robustness, and then this fragile and incredibly efficient, which equals

48:04.000 --> 48:20.000
incredibly non redundant, incredibly fragile non robust software built on top. And the claim is the suggestion is, we have to stop eating the glass sandwich. And here's my conclusions.

48:20.000 --> 48:35.000
This is a 12 step program for inventing a new architecture that in fact, almost completely inverts the design assumptions that underlie deterministic execution with CPU and ramp.

48:35.000 --> 48:42.000
And I won't any have anywhere near enough time to go through all of these, but several of them have already been mentioned.

48:43.000 --> 49:01.000
Number seven, look at life for lessons that's what I take from both Yuri and Kristoff using their studying the brain and studying the, the nervous systems of animals and people and everything for understanding about how these things work and the fact that for

49:01.000 --> 49:19.000
brains aren't just sitting there they're doing stuff all the time if you want to do something with a brain, you have to work with the dynamics that it's got that's very different than a computer with memory just sitting there empty you load it up with a program and then it does whatever you wish.

49:19.000 --> 49:37.000
So, it begins with step one, admit we have a problem and for me, that is driven by computer security, I think, you know, 50 years from now, hopefully less are great grandkids will not believe the world that we've lived in as far as computer security

49:37.000 --> 49:53.000
today of the computers we have today are so unbelievably gullible. I mean, one mistake, I mean, they're, they're like complete, you know, you just talk to them, and they become zombies, it's like a horror movie where all you have to do is say the thing, and then they start

49:53.000 --> 50:10.000
sending spam to Russia for you or whatever it is that they do. And that is not the way it has to be. That is the way it is because the way we've designed the glass sandwich, but we could do something else and so you know, the idea is instead of thinking

50:10.000 --> 50:24.000
about correctness and efficiency only at software, we have to think in terms of robustness first think robust first, and admit we have a problem, we can sing the whole song, pick new metrics, and so on.

50:24.000 --> 50:35.000
But I won't do that I just want to get a little bit more unpack the idea, show a few demos, and then stop there and let's go with discussion for it. Okay.

50:35.000 --> 50:50.000
So here's the answer. The answer to how to stop eating the glass sandwich is go for structure at all scales and rather than saying we've got physics, we've got hardware that produces universality as close to the physics as possible.

50:50.000 --> 50:59.000
And then it's all just software, we're going to say no, actually we want to delay universality and have that happen much closer to the value.

50:59.000 --> 51:12.000
And we want instead, just like you were talking about just like talking about have attractor nets, things that self stabilize things that have their own internal goals, other than get the credit card number.

51:12.000 --> 51:20.000
Yes, we have to get the credit card number at the end otherwise we can't pay for the whole operation, but we want to do it with things inside that are worrying about their own goals.

51:20.000 --> 51:33.000
You know, am I still is everything all cleaned up do I have enough room to work drive enough copies or enough cousins and brothers and so forth that are available to do it in case something happens to me, and so forth, and they themselves are made of even smaller systems like

51:33.000 --> 51:49.000
that. So rather than saying you know it's turtles all the way down, we want to build bottom up and say you know it's thermostats all the way up, made out of more and more and more complicated ones layered on, and I want to engineer that I want to actually build

51:49.000 --> 51:59.000
that learn by implementing them, you know we can learn by studying the brain we can learn by studying animals, that's great, but that's not what I do, I want to do it by building it and seeing cool new things happen.

51:59.000 --> 52:17.000
All right, so I've already said this, don't blame the users the programmers the managers blame the architecture in particular blame random access memory, you know, which is this wonderful thing, and it's far far too wonderful, because the instant

52:17.000 --> 52:31.000
we manage to knock a program off its kilter. Once you have this huge field of opportunities you can find anything you need a little bit here and there 17 of these carry loop make a thing by one.

52:31.000 --> 52:39.000
You're doing whatever you wish. How do you stop that what we had now all the stuff that we're doing now the people doing computer security work you know it's there.

52:39.000 --> 52:50.000
We're going to do it because here's the engineering field that we're currently living and we got to play it as it lays, but it's patch patch patch, I mean, not just for the users but for the whole idea.

52:50.000 --> 52:56.000
It's a glass dam that is going to spring leaks, we have to keep patching it up.

52:56.000 --> 53:11.000
The whole thing is going to go to hell, but we could if we wanted actually start to build another dam a little further away out of stern or stuff out of stuff that's built out of rip stop nylon instead of out of brittle glass.

53:11.000 --> 53:18.000
And in order to do that we have to ditch random access memory. And what are we going to replace it with if we get rid of random access memory.

53:18.000 --> 53:36.000
Well, the suggestion I make is, it's going to be some kind of cellular automata. Now, if you're not familiar with cellular automata. It's the idea of instead of having one big central processing unit, and one massive Ram, you have a whole bunch of teeny little processors with little teeny bits of Ram, and

53:36.000 --> 53:51.000
you can talk to nearest neighbors or nearest two or three neighbors or whatever it is some limited neighborhood that is all that they can get to, and there's no general purpose pointer, they cannot go well, I need that little bit in this little bit.

53:51.000 --> 54:03.000
So does that mean everything's guaranteed to be fine. No, does that mean everything is safe. No, but it does mean that if an attack is going to be mustered in the most critical moments when you first knocked the system off kilter.

54:03.000 --> 54:17.000
So, very little bit of stuff to work with, and it's all very delicate and you have to do a land war. You have to take over the next stop and the next bit and the next bit and spread it, rather than going boom, I won.

54:17.000 --> 54:21.000
Now, most people that are familiar with cellular automata.

54:21.000 --> 54:39.000
There are things like Conway's Game of Life, and that has a two characteristics that I don't like, and in particular number one, it's deterministic. So it inherits that exact same flaw that believing in perfect hardware that CPU and Ram inherits from.

54:39.000 --> 54:55.000
So if we get around the problems that CPU and Ram and deterministic execution are causing, we have to give up on deterministic execution, we have to admit that there will be flaws, and they have to be handled software can't just say reliability is a hardware problem.

54:55.000 --> 55:11.000
It can't be synchronous, the whole thing can't go kuchunk kuchunk kuchunk at once, because the whole point of cellular automata is that it can get bigger, we can add more we can grow the thing out, and the bigger the thing gets the worst synchronization becomes.

55:11.000 --> 55:22.000
One of the tricks you can do that people keep discovering every decade or two about ways to kind of lay a synchronous thing on top of an asynchronous thing, but they all rely on deterministic execution.

55:22.000 --> 55:33.000
If there's a possibility of errors, that means the synchronous overlay, it actually locks up the entire universe grinds to a halt, not a good outcome for having one bit flip.

55:33.000 --> 55:52.000
We have to embrace asynchronous valuable hardware and say how could we program on that. And wow that's harder than programming on CPU and Ram. But hey, we know a little bit more about software now than we did when we first invented the von Neumann machine and all the subsequent CPU and Ram stuff.

55:52.000 --> 56:09.000
That's the idea, how do we make a cellular automata got it. That's can be indefinitely scalable. We make it by making an individual tile, a chunk of cellular automata that can connect with others of its own kind and we just keep plugging it out.

56:09.000 --> 56:20.000
So here, this is a T two tile this is the specific tile that I've been working on for the last decade or so. It's a small project so it's going slowly.

56:20.000 --> 56:32.000
But in fact we had an earlier version called the Illuminato X Machina back in 2008 that similarly it's a 2D thing that you plug them together and so forth those were actually briefly marketed.

56:32.000 --> 56:50.000
And recently we have the T two tiles the one that I just showed you. And these things are, you know, ridiculous, huge, heavy, hot $100 expensive each, because they're a research prototype. And, you know, the key is to figure out what we want to figure out a new deal between hardware and

56:50.000 --> 56:57.000
software. And then yes, the hardware guys can come in and figure out how to do this beautiful.

56:57.000 --> 57:11.000
We have to know what we want first. So, that's it. There's a the T two tile project has bi weekly videos on YouTube. There's also a T two demos, which just shows examples of the stuff running.

57:11.000 --> 57:24.000
And you know we've got a new programming language called alarm which is a procedural language for coding up transition rules for these things. We also have a spatial programming language called splat, where you actually can make these

57:24.000 --> 57:39.000
little, these little diagrams you know little ASCII pictures, saying this 2D pattern goes to that 2D pattern and so forth. And that you know it's very simple but it made stuff possible it made stuff work, like that I've been trying to get to work forever.

57:39.000 --> 57:45.000
Now, I'm just going to show you this one and then I'm going to I guess I'll stop because my 15 minutes are up.

57:45.000 --> 58:01.000
One of the problems with asynchronous cellular automata is how do you move a big thing. How do you move something that's bigger than you can move all at once. If it's synchronous you kind of imagine going chunk in one step, but when it's asynchronous you can't you have to somehow

58:01.000 --> 58:16.000
do it. And that's what this example was doing. It's the little blue guys that go through they create swap lines that come up and pass through the rectangle and each time they pass through the matrix the matrix moves one step in the opposite direction that's what a swap

58:16.000 --> 58:28.000
line does. So a swap line is designed so that it never gets more than 45 degrees down the line. So it's a limited amount of synchronization, the passing of the line is a certain degree of synchronization.

58:28.000 --> 58:48.000
And that's just one example of learn by doing this is another example I'll just let this run I guess and well, and this is taking a plate, a grid of sites that communally say let's create a common spatial origin let's create a 00 and a

58:48.000 --> 58:58.000
whatever it is. There's no overall grid coordinates in the underlying architecture there's no 00 because it's indefinitely scalable there's no beginning.

58:58.000 --> 59:09.000
Everything just thinks it's the center of the universe and it goes from there, but that doesn't mean we can set up our own private little one. And once we do that we can do all kinds of things with it.

59:09.000 --> 59:16.000
So this is more stuff with plates. Oh, and I'll just jump to this last one, if I can. There it is.

59:16.000 --> 59:29.000
This is an incredibly lame neural network, kind of encoded in an movable feast machine written in ULAM. And here it is actually running.

59:29.000 --> 59:40.000
The two blocks that you see on the upper side are crossbar matrices that connect inbound inbound wires and outbound wires and they each have a weight at the connection.

59:40.000 --> 59:55.000
And then down so it's actually doing a simple function optimization where the one on the upper left represents the function the one on the upper right represents the knowledge that the algorithm is gaining and then it actually had a little

59:55.000 --> 01:00:02.000
speed out on the screen to produce human output. So that's it.

01:00:02.000 --> 01:00:08.000
Well, I will stop there. And

01:00:08.000 --> 01:00:21.000
it's going incredibly slow, but hopefully some folks will be inspired, you know, step step 11 is, you know, it, we have to make it happen we have to get organized and so forth.

01:00:22.000 --> 01:00:37.000
This is not going to be running Excel in a year this is a long term fundamental research project to kind of go for a mulligan to kind of do a do over instead of having determinism at the bottom we have determinism at the top instead of having data center software reliability at

01:00:37.000 --> 01:00:41.000
the top we have it at the bottom, and so on.

01:00:41.000 --> 01:00:42.000
Thanks for listening.

01:00:42.000 --> 01:00:43.000
Thank you David.

01:00:43.000 --> 01:00:50.000
I from the chat they see this was an extremely inspiring presentations.

01:00:50.000 --> 01:00:56.000
Okay, I'll switch to us.

01:00:56.000 --> 01:01:00.000
So, our final presenter today is Dr. Yosha Bach.

01:01:00.000 --> 01:01:09.000
Yosha Bach is a cognitive scientist and AI researcher with a focus on computational models of cognition and neuro symbolic AI.

01:01:09.000 --> 01:01:24.000
He has taught and worked in AI research at Humboldt University of Berlin, the Institute for cognitive science in Osnabrook, the MIT media lab, the Harvard program for evolutionary dynamics, and is currently a principal AI

01:01:24.000 --> 01:01:29.000
researcher at Intel Labs California.

01:01:29.000 --> 01:01:36.000
Yosha, the floor is now yours.

01:01:36.000 --> 01:01:38.000
Thank you.

01:01:39.000 --> 01:01:57.000
The machine that builds the machine is a very interesting topic when we think about the brain because our mind is something that is not just designed as a technological system from the outside in, but as biological and social systems are from the inside out.

01:01:57.000 --> 01:02:13.000
When you look at this difference, when we design a system in our lab we start from a deterministic environment and we take a substrate to this environment that is not structured yet in the way in which we want it to be structured but we can fully control it, and we extend our

01:02:13.000 --> 01:02:24.000
determinism into this new part of the universe to basically extend our deterministic world into this particular thing so we are coming from the outside in a technological design.

01:02:24.000 --> 01:02:41.000
In the biological system, you do not have this deterministic environment to start this. Instead, you have an indeterministic environment and you start out with some seed that needs to colonize the environment to branch out to subdue it to turn it into something that the seed knows how to deal with.

01:02:41.000 --> 01:02:56.000
And then gradually turn its own structure and then you go beyond simple organic roles, you look at organismic roles where you already know the structure around you because you have created this or you are part of something that had shared destiny at some point.

01:02:56.000 --> 01:03:12.000
And so now you have known units around you this which you can collaborate and organize and so you are colonizing the outside internally you are organizing from the ground up from local units inside out and

01:03:12.000 --> 01:03:29.000
we can ask ourselves what an organism is does an organism actually exist right we sometimes think of organisms as things, but the organism is the virtual thing it's a function that describes the coherent pattern and the activity of many cells.

01:03:29.000 --> 01:03:47.000
Right so the individual cells are all serving that function and by making them coherent due to evolutionary pressure and the design constraints that are built into the cells as a result of that you see a coherent pattern emerging and this coherent pattern that we see emerging.

01:03:47.000 --> 01:03:58.000
That is the causal structure that we call the organism and the organism, like every other thing that exists to exist is to be implemented is implemented to some extent.

01:03:58.000 --> 01:04:07.000
But it's not implemented in the way in which computer chip is implemented, but the degree of approximation is much more vague.

01:04:07.000 --> 01:04:15.000
And it's the organism exists to the degree that the patterns in the interaction of the cells in the organism are coherent.

01:04:15.000 --> 01:04:28.000
So in some sense we see a pattern of this organization, a software that emerges over this information processing that looks as if there is a coherent agent that is inhabiting this thing.

01:04:28.000 --> 01:04:36.000
And traditionally the word for this emerging operating system for an autonomous robot in nature is called spirit.

01:04:36.000 --> 01:04:45.000
In this sense, the emergent agency that we see in organisms is this actually what was called spirits by the previous civilization.

01:04:45.000 --> 01:05:01.000
And the spirits also happen to be in our own minds and they also happen to emerge in groups of people in nation states and ecosystems and so on and these spirits are virtual machines that possess agency that is they play a controlling wall by being able to model the future,

01:05:01.000 --> 01:05:07.000
in a certain place in the universe to some degree and they are approximately implemented.

01:05:07.000 --> 01:05:16.000
And so, in this sense, the spirit is a very high level of an organization that is required to see the emergence of such a spirit.

01:05:16.000 --> 01:05:22.000
And then we look at the hierarchy of course the systems at the lowest level.

01:05:22.000 --> 01:05:35.000
They have something like our automata like the game of life, and the automata, they can already be curing complete it's not that they are limited in any way, if you give them some memory and so on.

01:05:35.000 --> 01:05:46.000
They can implement arbitrarily complex structure to set them up in the right way, but you need to do this from the outside where some kind of design process.

01:05:46.000 --> 01:05:54.000
The automaton that might be able to do this without it was a random starting state is our universe itself.

01:05:54.000 --> 01:06:03.000
That might be a big automaton at its bottom level and it just happened to inhabit a region that has interesting enough complexity to contain us.

01:06:03.000 --> 01:06:15.000
So if you look at the mechanism that is slightly more complicated than causal that an automaton that's usually a stateful thing and it is entwined with a substrate.

01:06:15.000 --> 01:06:22.000
Here I use as an example the famous machines by.

01:06:22.000 --> 01:06:29.000
Sorry, my brain is blanking out.

01:06:30.000 --> 01:06:33.000
Tonya, you know his name.

01:06:33.000 --> 01:06:50.000
Theo Janssen, Theo Janssen strand piece, and he has been building these amazing machines that are driven usually by wind power, and they're completely mechanical things that are fully coupled with the environment that don't have an existence that is in the sense independent of the environment.

01:06:50.000 --> 01:07:05.000
So if you look at feedback systems you can have open loop systems that are coupled for instance in oscillators you and synthesizers you produce interesting patterns that are the result of some static coupling.

01:07:05.000 --> 01:07:25.000
If you take this coupling dynamic, for instance, in the regulator for a steam engine, you can have cybernetics control system, and the controller and the cybernetics system is built by having a system that cares about the target value and wants to minimize the distance to the target value.

01:07:25.000 --> 01:07:33.000
And if you extend this controller with computation, you can get an agent.

01:07:34.000 --> 01:07:51.000
But to have this decoupling of this ability to model the future, you need to have a system that is a computer a Turing machine in some sense is a mechanism that is disentangling itself decoupling a causal structure from the underlying dynamic of the universe.

01:07:51.000 --> 01:08:06.000
And the only thing about our computers is that they do the same thing, regardless of what the environment is doing they work the same way, whether you are carrying them to America and New York, or whether the temperature is a few degrees higher or lower or whether your climate

01:08:06.000 --> 01:08:20.000
is down, or whether the wind is blowing or not. And the same kind of computer also exists inside of our skulls. It's a slightly different one, but this principle of the computer is that enables an arbitrary causal structure.

01:08:20.000 --> 01:08:31.000
And you need to have this arbitrary causal structure to be able to make models of the future, right, because you want to look at different futures, regardless of what the universe is doing right now.

01:08:31.000 --> 01:08:42.000
And the simple system that we know in nature that is in the sense Turing complete and has the powers of functional approximation and self organization to achieve that is the cell.

01:08:42.000 --> 01:08:58.000
So the cell basically is able to perform computations that are decoupled from its immediate substrate, and then enable the cell to make models that predict the future to so the cell can regulate against future disturbances and keep itself stable against these disturbances.

01:08:58.000 --> 01:09:03.000
So that's why the cell is such a complex system is able to exist.

01:09:03.000 --> 01:09:20.000
So the cell is an agent, and an agent is basically a controller, combined with an internal set point generator, and the ability to model the future so the agent is not just acting like a thermostat on the next frame and tries to optimize the state of the next

01:09:20.000 --> 01:09:26.000
frame but it's integrating the expected set point deviations over the future and tries to minimize them.

01:09:26.000 --> 01:09:38.000
And if the modeling capacity is sufficient, then this agent is able to model different branches of the universe and the effect that its decisions will have on these branches.

01:09:38.000 --> 01:09:54.000
And as a result, you basically get beliefs desires and intentions all emerging from a simple controller that is able to model the future. So this is basically a minimal concept of agency and agents can start to collaborate with each other and

01:09:55.000 --> 01:10:09.000
as a groups for agents and groups agents, the typically have individual motivation and a reputation system among the agents that makes sure that their actions are harmonious and beneficial to the members of the group.

01:10:09.000 --> 01:10:16.000
And slightly different extension of the group agent is a state building agent.

01:10:16.000 --> 01:10:21.000
A state building agent is one that scales beyond reputation system.

01:10:21.000 --> 01:10:31.000
It means that the individuals do not have to know each other individually to have to maintain a model of their reputation and that transaction and synchronize this reputation system somehow like a tribe does.

01:10:31.000 --> 01:10:38.000
A state is fundamentally different from the tribe, because the members of the state become interchangeable they have functional roles now.

01:10:38.000 --> 01:10:59.000
And these state building agents can grow very large, but the size of the state building agents depends on the size of the effective colonial structure that we can maintain so basically logistics chain to build such a colony of units such as synchronize the state needs to

01:11:00.000 --> 01:11:18.000
have a way to impose administration on its substrate and extract more like entropy from the substrate, then the administration costs, and this limits the size of the design of state building system, especially once you have such a state building system and evolution it's

01:11:18.000 --> 01:11:22.000
going to repeat the similar systems for the same leg entropy.

01:11:22.000 --> 01:11:30.000
And so you are basically the set of principles that has outcompeted all other systems from extracting like entropy from your volume of space.

01:11:30.000 --> 01:11:40.000
And to do this, you need to impose a coordinated patterns of organization onto your volume of space.

01:11:40.000 --> 01:11:59.000
And because you're competing with others limits the size that you can have and there are very few organisms that have cracked the code and have become infinitely scaling state building agents right most state building systems have a limited size that design is limited by the stable logistics chain that they can use to impose their

01:12:00.000 --> 01:12:10.000
environment. And this here for instance is Pando forest it's ash trees in Utah. It's one of the largest organisms that we have on earth all these trees are the same tree.

01:12:10.000 --> 01:12:15.000
They're not just genetic clones for each other, but their wounds are connected.

01:12:15.000 --> 01:12:27.000
This is basically one big tree that can grow as large as it wants. It's very old, and it didn't mutate very much since it's gross right so it needs to be evolutionary stable so it doesn't drift.

01:12:27.000 --> 01:12:39.000
Another example for an infinitely scaling state building agent is linear PC by you mean it's a Brazilian and that has spread around the world.

01:12:39.000 --> 01:12:50.000
And all members of this and of these end colonies will not attack each other they will all treat each other as part of the same cohesive colony.

01:12:50.000 --> 01:12:56.000
This is the exception of a few drifted colonies for instance in the US.

01:12:56.000 --> 01:13:14.000
When they get in touch with the other colonies they will attack each other. So they had some kind of genetic drift after the colony was established that makes them appear to be strangers to each other but the species has cracked the problem of becoming infinitely scaling

01:13:15.000 --> 01:13:19.000
There's no limit to the size of this colony apparently.

01:13:19.000 --> 01:13:33.000
And if you look at the design constraints for such systems, the mechanical component needs to have an outside in design by some external agent so it is not going to exist by itself and it's not able to adapt by itself.

01:13:33.000 --> 01:13:49.000
And then you build a controller controller gets resilient that can be larger than the mechanical component because it's able to adapt its states to a slightly changing environmental circumstances and maintain an attractor state by itself via dynamic control.

01:13:49.000 --> 01:14:04.000
And if your controller is able to model the future via decoupled computation then it's able to integrate future reward and is able to not just adapt itself to the environment but it's also able to adapt the environment to itself.

01:14:04.000 --> 01:14:08.000
And in group agent you can do this on the next level.

01:14:08.000 --> 01:14:19.000
You can basically build an agent that is composed of multiple sub agents and each of these sub agents has its individual motivation and the reputation system to coordinate the group.

01:14:19.000 --> 01:14:28.000
And in a state building agent, you change this reputation system or extended via hierarchy of the governments and this thing needs an immune system.

01:14:28.000 --> 01:14:35.000
The sub agents are submitting to this governance and are not building their own government that is competing with the main government.

01:14:35.000 --> 01:14:48.000
And you will have limited autonomy of the sub agent so there will be set up in such a way that they per default, most of them will want to submit to the state building group, rather than doing their own thing.

01:14:48.000 --> 01:15:07.000
You can see this in humans via basically a domesticated species of the hominids and as a result you're not just tribal homo sapiens is state building because the most of the individuals in our species are beginning to submit to the group before they do their own thing.

01:15:07.000 --> 01:15:20.000
And if you go to an infinitely scalable state building agent you can do at some level less than you can do and these other groups because the need to be static you cannot have an evolutionary drift.

01:15:20.000 --> 01:15:33.000
You cannot adapt to the environment beyond the mechanisms that are built into the system because if you were to drift, then this this infinite scale would break and you would no longer be consistent.

01:15:33.000 --> 01:15:43.000
So, if you think of hierarchical governments as a principle, we have a trade of their between adaptivity and coherence in the system.

01:15:43.000 --> 01:16:02.000
The more adaptive it is the harder it is to maintain coherence and the individual agents here are incentivized to defect from the system very often and you might have to limit this by having an agent that imposes an offset to the payoff matrix to the individuals and this is what you call a government.

01:16:02.000 --> 01:16:14.000
The need for a government comes not from political constraints or from the desire to exploit people or something else just game theoretic thing that you can derive from first principles.

01:16:14.000 --> 01:16:31.000
And the purpose of this government is to integrate the total reward which can happen from bottom up and to top down to credit assignment to make sure that the individual behavior is adjusted in such a way that the national equilibrium of the individual agents become compatible with the common good.

01:16:32.000 --> 01:16:44.000
And this is something that also needs to happen in some sense in our own brain the neurons are autonomous reinforcement learners and the interaction between the neurons needs to be coordinated into a coherent structure.

01:16:44.000 --> 01:16:58.000
And Jerry Aylman has suggested that the organization that happens in our own mind is something that evolves in every individual in some kind of what he calls evolution because you will Darwinism.

01:16:58.000 --> 01:17:16.000
And so maybe the top down process that is harmonizing about bottom up perception is something like a governance a colonizing agent so our brain is not just playing free jazz, which it does to some extent, but it also plays a coordinated symphony.

01:17:16.000 --> 01:17:31.000
It is serving coherent goals, it is has an emerging coherent agent that is forming inside the organization of the cells, it makes our organism more efficient by giving it coordinated spirit.

01:17:31.000 --> 01:17:45.000
And because this relevant to AI current measure learning representations all have an outside in design and organisms the representations are different your organisms are coupled to the environments with the features are not static.

01:17:45.000 --> 01:17:55.000
They are functions that basically operators on your government representation to give you the next state, and they're tuned in such a way that they track the sensory patterns.

01:17:55.000 --> 01:18:06.000
These features are kept stable and coordinated by an individual controller so every feature is probably some control structure that maintains its stability and its coordination with the environment.

01:18:06.000 --> 01:18:19.000
And the entire thing goes beyond jazz by having some kind of emergent governance that harmonizes it and instantiates individual features and resolve some and they're no longer necessary destroys the attractor states.

01:18:19.000 --> 01:18:24.000
And governance when it's at the level of the scene that you currently use to interpret the world.

01:18:24.000 --> 01:18:38.000
And you have multiple systems of interacting agency in there, visit the scene agent that is trying to predict how the world continues. You also have a self agent that is driven by the motivation of our organism.

01:18:38.000 --> 01:18:47.000
And you have an attention agent that is figuring out which features to select and harmonize and to instantiate and to dissolve at any given moment.

01:18:47.000 --> 01:18:56.000
In this way, organisms can have an essentialized causes tractor, but the central is cross instructor is not like a CPU and a computer rather it's an.

01:18:56.000 --> 01:19:11.000
It's more like the centralized process structure in a society of people that emerges as a result of an evolution that makes the society more efficient and better at competing with other societies.

01:19:11.000 --> 01:19:14.000
So, this is very for today.

01:19:15.000 --> 01:19:29.000
Right, so now we are to the discussion part of the session. Let me see what's what we have in the chat for questions.

01:19:29.000 --> 01:19:43.000
So, first of all, please feel free if any of the panelists has comments about each other's presentations. Please feel free to voice them.

01:19:43.000 --> 01:19:49.000
In terms of questions so first question is from Thomas McGee.

01:19:49.000 --> 01:19:52.000
And it is not.

01:19:52.000 --> 01:20:00.000
It was posted, I believe during Yuri's present presentation but it may have been to anyone.

01:20:00.000 --> 01:20:20.000
Here are some examples of multistable and metastable neural attractor dynamics with the slow fluctuation of externally oriented delta theta and dominated oscillations and internally oriented alpha beta oscillations be an example of an endogenous multi stable attractor dynamic in the brain.

01:20:20.000 --> 01:20:26.000
Also curious about some examples of neural heteroclinic cycles.

01:20:26.000 --> 01:20:30.000
There are many questions and there are many components to this.

01:20:30.000 --> 01:20:39.000
One of the most beautiful things in in in brain evolution about scales is brain rhythms.

01:20:39.000 --> 01:20:51.000
Now, what you would like to know in general about scaling is that what do you want to preserve. And what do you, what are you allowed to sacrifice.

01:20:51.000 --> 01:21:06.000
The brain rhythms are extremely useful on this because they are pretty much the same in every single brain in, at least in the mammals from the mouse, the human.

01:21:06.000 --> 01:21:10.000
And the mechanisms are the same the pharmacological sensitivity is the same.

01:21:10.000 --> 01:21:23.000
One interesting thing is that they form a just just to finish this line, which means that the most important thing for the brain is to keep timing preserved.

01:21:23.000 --> 01:21:33.000
And the reason why timing is so important is because we control muscles, and the muscles are the same in all species and the speed of the muscle hasn't changed the evolution at all.

01:21:33.000 --> 01:21:44.000
So other creatures have pretty much the same speed so that's probably one of the evolutionary pressures that allowed or force to keep timing the same and then you sacrifice a lot.

01:21:44.000 --> 01:22:01.000
You sacrifice a structure, you put a lot of lines that is axons in a larger brain that are much more much faster conducting because you have to deliver the information in the same time to a more distant target.

01:22:01.000 --> 01:22:14.000
Then the calibers of the axons grow and so on and that these are beautifully demonstrated by comparisons of the fibers in the copper scolosum and in the, in the retina, the optic tract and so on.

01:22:14.000 --> 01:22:25.000
Now, these oscillations or returns that this questioner asked about form a beautiful hierarchical system.

01:22:25.000 --> 01:22:47.000
The hierarchical organization is simple, which is called phase amplitude coupling, which means that the lower oscillations phase modulate the amplitude or the magnitude of the faster one and the phase of the fast one modulates the amplitude of the even faster one and so on and so on and so on.

01:22:48.000 --> 01:23:01.000
The consequence of all this is that when you have a short period of time and you've got a short period of an oscillator or short waveform, then only events can occur locally.

01:23:01.000 --> 01:23:11.000
When you have more time and then you have a slow oscillator that it engages all the other ones in a larger neuronal space.

01:23:11.000 --> 01:23:26.000
Very different from me, I probably had this is if I, if I understood it right from, from, from Dave, he alluded to this that that we try to believe a stuff to distant architectures rather than just the neighbors.

01:23:26.000 --> 01:23:33.000
This is what the brain does in both ways, namely that most of the organizational pattern is local.

01:23:33.000 --> 01:23:52.000
But the large oscillation is allowing that local computation is broadcasted locally globally a little bit, and the global computations that is the global oscillations constraints, what goes on in any local situation.

01:23:52.000 --> 01:24:04.000
This allows this hierarchical system allows you to have a brain syntax to package information in short chunks and longer chunks.

01:24:04.000 --> 01:24:15.000
So, you asked about various various names but for the audience it doesn't matter, let's just call the faster ones, gamma oscillations which are about 2030 millisecond.

01:24:15.000 --> 01:24:23.000
This could be the content of this is a bunch of neurons firing together with the one and only purpose is to discharge a post synaptic target neuron.

01:24:23.000 --> 01:24:38.000
It can be called a neuronal ladder. Now a theta and an alpha oscillation can concatenate several of these slots, the gamma oscillations to a neuronal word, and the neuronal word can be combined into a longer segment.

01:24:38.000 --> 01:25:02.000
So, this is the way I think it was, was Christoph who already mentioned that, you know, without a generative rule, a syntactical rule you cannot really grow the information content infinitely but if you have such a syntactical rule such as the brain oscillatory hierarchy,

01:25:02.000 --> 01:25:13.000
then it allows the brain to generate infinite number of sequences from limited number of elements. So this is a complicated answer to the brain rhythms.

01:25:13.000 --> 01:25:19.000
If you I just didn't throw the ball I had a previous book, but that's a short answer.

01:25:19.000 --> 01:25:28.000
I want to follow up on that. Is it the case so you're saying these brain rhythms are extremely conserved across mammalian species at least.

01:25:28.000 --> 01:25:38.000
And does that mean that the size of the brain doesn't affect the rate of a given class of oscillation.

01:25:38.000 --> 01:25:43.000
The elephants are going no faster, no slower.

01:25:43.000 --> 01:26:01.000
Even though they have much more distance in just raw centimeters to cover. I mean, and why the hell would that be. I mean, why, because they just stretch the axons out that makes hardly any difference in the propagation time.

01:26:01.000 --> 01:26:16.000
I think the idea of physical systems was was discussed a couple of times here. No physical this physical systems are different from from theoretical creations with unlimited speed and so on.

01:26:16.000 --> 01:26:20.000
I wanted to to keep the.

01:26:20.000 --> 01:26:30.000
If you if you look at my mark here, it has color and all sorts of features, as you know, it has been studied, and for Mars, we'll study it about the mining issue.

01:26:30.000 --> 01:26:41.000
All these things have to come together, but they have to come together somewhere in a small brain and the large brain, in the same time, in order to perceive it and they do understand why same time.

01:26:41.000 --> 01:26:53.000
Why couldn't it be a little longer in the big brain because we are. Well, not because but this is the speculation. I'm interacting with another species and another species and another species and another species and all of this.

01:26:53.000 --> 01:27:01.000
If that would be a tremendous, you know, order of magnitude advantage, then in speed, for example, I would be that.

01:27:01.000 --> 01:27:11.000
I think the short cycles I could expect to be preserved or forced chemically or something like that, but it's the long ones that I would expect could have more variation.

01:27:11.000 --> 01:27:26.000
This is the interesting kick brains do this for a goal and we can talk about the goal a little bit later, because this if you are interested in what is preserved in the brain throughout evolution then my answer is timing and the brain oscillation.

01:27:26.000 --> 01:27:43.000
It's not because the brains cannot do something else. Breathing, which is also an oscillation a very regular thing is organized by by about a few thousand neurons that can have several orders of magnitude different in written, you know, the ways and the heart rate.

01:27:43.000 --> 01:27:55.000
So they are very, very different. So if you want, you can do it. But all the other oscillation that I use for cognition and controlling the body seems to be preserved.

01:27:55.000 --> 01:28:09.000
Thank you. And as I mentioned briefly, that there is there are beautiful full anatomical data showing that the connection between this part of my brain that this part of the brain is about 100 times smaller in a smaller brain.

01:28:09.000 --> 01:28:20.000
And the conduction velocity is 10 times more in my brain, because it's needed that you get there at the same time.

01:28:20.000 --> 01:28:32.000
The cost is the size the cost is enormous because you have to put my lane and energetically, it's so costly to maintain to deliver electricity one from one place or another.

01:28:32.000 --> 01:28:34.000
But this is what brains do.

01:28:34.000 --> 01:28:45.000
Oh, that's the same in digital manufacturing computers the more metal you have to run the more energy you pay the more later you pay the more area you pay. It costs a ton.

01:28:45.000 --> 01:28:55.000
And if you want to crank this clock the same speed on a big physical system, the brain is just like your chips, you know, in that sense, thank you.

01:28:55.000 --> 01:28:59.000
I have a question to dirty.

01:28:59.000 --> 01:29:14.000
If you look at the organization of the new cortex, how much of that do you think is the result of deliberate circuitry building and how much of it is just a stochastic substrate.

01:29:14.000 --> 01:29:23.000
You point out that the complexity doesn't very much change to be learning, but it's not that the brain starts out with no structure or the new cortex starts out with no structure.

01:29:23.000 --> 01:29:38.000
After the initial setup, of course, and then it forms all this intricate structure after interacting with the environment, it seeks the structure, but the complexity at which a child perceives the world is probably not that much different from the complexity at which

01:29:38.000 --> 01:29:53.000
a child perceives the world, even though the functions are being learned are slightly different, but how much of that is basically reflected via the circuitry and how much is the circuitry just a result of of the learning of

01:29:53.000 --> 01:30:03.000
for instance, RNA based memory that is stored in the activation pattern or propensity of the individual cells.

01:30:03.000 --> 01:30:17.000
The propensity of the individual cells seems to be preserved throughout much of lifetime. So the firing rate of a neurons is almost like a fingerprint that you can perturb it you can do lots of things that it do the same.

01:30:17.000 --> 01:30:22.000
Now you take a cortical unit a cortical module it's called the column.

01:30:23.000 --> 01:30:35.000
And then many people are very much interested in how the different columns compute something differently in the visual system versus a thinking system such as the prefrontal cortex.

01:30:35.000 --> 01:30:47.000
But the if you look at the connectivity of these two systems they are not very different. So the commonality is much stronger so much so that experiments have been done 20 years ago.

01:30:48.000 --> 01:31:05.000
When either a newborn or a prenatal animal is taken out and you take a piece of tissue from here and you put it here and you change it, there will be a relatively normal brain because the system can tolerate that kind of discrepancy.

01:31:05.000 --> 01:31:11.000
Now when you look at a small brain a large brain and you say how many different types of neurons that are there.

01:31:11.000 --> 01:31:31.000
Then today, on these days, you know, people celebrate if they find one neuron that in the human brain, or in a primate brain that looks a little bit different, or genetically looks different than in the brain of a mouse, because the component diversity is relatively limited

01:31:31.000 --> 01:31:50.000
throughout the mammalian evolution. Now, having said that the neocortex is a modular system, it can just grow because you can add element, and then you need the agent that you talked about the controller that brings this together that maybe the Talibus that allows all these things to be modulated together work together.

01:31:50.000 --> 01:32:01.000
But there is this different type of organization called the hippocampus, which is a single giant cortical module.

01:32:01.000 --> 01:32:21.000
It's the same structure, there's no modularity in it, it just grows from the tree shrew to the whale, and it gets larger and larger, because it is quote unquote design as a random graph that you would like to go from anywhere to anywhere else in just two steps.

01:32:21.000 --> 01:32:38.000
That may be an interesting thing, you know, why, and I don't have a good answer, nobody does, why do we have this, this external loops, why does the neocortex cannot solve the problem of the, of the, but the hippocampus can do but what I learned today is that even in computer science and

01:32:38.000 --> 01:32:53.000
architecture is the primary thing that determines form defines function and brain is not an exception from this. In fact, it's I think the prime example.

01:32:53.000 --> 01:33:14.000
Yuri. So, so we have a question from we scroll to, to Christoph and Yuri, looking at your insights is the current architectural paradigm for artificial neural networks still stuck in an architecture, ignoring the endless loop with environment paradigm of real

01:33:14.000 --> 01:33:25.000
networks. So, despite, despite the deep learning advances, will we continue getting more stuck.

01:33:25.000 --> 01:33:39.000
This question has a plus script saying some alternatives like cellular automata still totally missed the feature of being massively looped, or massively parallel, in my opinion, also a dead end street.

01:33:39.000 --> 01:33:59.000
But I take it as asking is deep learning, the end of the story, so to speak, and, and can that be continued indefinitely by just enlarging the system or is enlarging the, the training sets.

01:33:59.000 --> 01:34:20.000
What I mean here is that deep learning is stuck. The main problem being that it's a priori assumptions, it's, it's fundamental data model is not tuned to our environment.

01:34:20.000 --> 01:34:36.000
To some particular application fields but as far as we are, and after animal type human type intelligence deep learning doesn't have the right data architecture.

01:34:36.000 --> 01:34:59.000
That is the reason for it being very poor in generalizing, you know, if you show a new kind of object to a three year old child, the child gets it after the inspection of a single sample of that object, and recognize it in under massive changes

01:34:59.000 --> 01:35:02.000
that are in exact shape and so on.

01:35:02.000 --> 01:35:16.000
And so what deep learning is poor in is its feet forward architecture, which is forced on it by the necessity to back propagate the error.

01:35:16.000 --> 01:35:39.000
This is a very poor data format. And as I have tried to point out, self organized networks which differ from what we have today in deep learning structures, fundamentally, by having cycles of, of connectivity, you know lateral connections

01:35:39.000 --> 01:35:45.000
between within a layer, speaking about the layer system.

01:35:45.000 --> 01:36:08.000
So I think the enormous efficiency with it, with which animals learn or or humans learn is something which is beyond reach of the deep learning paradigm.

01:36:08.000 --> 01:36:23.000
So first off, I have a question about comical complexity. If you think of physics the comical complexity of the universe might be very low in the sense that it could be a fractal which has a simple generator function.

01:36:23.000 --> 01:36:28.000
But the complexity of the particle universe is extremely high.

01:36:28.000 --> 01:36:43.000
Right, so if we tried to describe the detailed fine grain structure of the universe around us, we would need an enormous amount of code because you cannot compress it very well, because we don't have access to the underlying function and our position in the

01:36:44.000 --> 01:36:54.000
But the description that we are making of the world with Newtonian physics and basically the game engine that our mind inhabits again has a much, much lower complexity.

01:36:54.000 --> 01:37:02.000
So there's basically an emergent causal structure that we use to predict the world that fascinatingly works quite well.

01:37:02.000 --> 01:37:12.000
And that can be described efficiently in what you describe as a gigabyte of code, probably a lot less if you write the code down more efficiently than we do.

01:37:12.000 --> 01:37:26.000
And now an interesting question is what is the actual comical complexity of our mind where the complexity of building a brain by setting up self organization process between the cells is probably relatively low.

01:37:26.000 --> 01:37:40.000
It's not the entire gigabyte of genetic information, but probably more in the order of kilobytes or megabytes that are required to basically form out the brain as in the self organization process.

01:37:40.000 --> 01:37:51.000
But what is the comical complexity of the emergent system if you were to design the brain as an engineering project from the outside in.

01:37:51.000 --> 01:38:01.000
Is this going to be similar to the Newtonian world where you have a relatively efficient structure could we come up with classical AI architecture describe the brain efficiently.

01:38:01.000 --> 01:38:13.000
Or do you have to deal with the fact that the effective structure of the brain is going to be very complicated so you will need to have a self organization to understand it.

01:38:13.000 --> 01:38:28.000
My argument was of course that the brain is of low Conmogorov complexity, because it can be built with so little genetic information.

01:38:29.000 --> 01:38:38.000
Some people claim or have computed that the 3.3 billion

01:38:38.000 --> 01:39:07.000
basis can be compressed to something like 50 mega mega bit. So the complexity is very low and the in order to build the brain you need a process of self organization that's as we know starts with a single fertilized cell and goes through divisions and goes through a sequence of

01:39:08.000 --> 01:39:26.000
a tractor states. That's the way the brain is built. Now, our way of building outside in completely ignore such constraints as are implicit implicit in the organic growth from one state to the next.

01:39:26.000 --> 01:39:49.000
We can sort of when putting together blueprint, we can, we have the full, the full universe of potential patterns we can can throw on to our blueprint, and the only constraint that we are observing is the design ideas we have in mind.

01:39:49.000 --> 01:40:18.000
So I think, of course, you can buy by knowing the exact, the procedures of self organization, the mechanism of self organization, you can let them play in your computer that you use for the design and get the final result and then that impose that final result outside in on on a piece of hardware.

01:40:19.000 --> 01:40:42.000
But I find it pretty against the nature of things to do that. So, I think complex, complex brains, artificial brains will be built inside out, and if only they are constructed on digital computers.

01:40:42.000 --> 01:41:00.000
As a present time, I'm simulating all my systems of course on the digital computer. What can I do, even on the digital computer, you would rely on an organic growth process, generating the final structure as a sequence of intermediate states.

01:41:00.000 --> 01:41:06.000
So I think you are stuck with self organization.

01:41:06.000 --> 01:41:29.000
By the way, the reason why I wanted to have Dave on this panel is I thought that it would be interesting to see if there is an interaction between the ways in which he is designing computers by self organizing principles and the way in which we think in neuroscience and cognitive science about self organization for information processing.

01:41:29.000 --> 01:41:32.000
So, comment on that.

01:41:32.000 --> 01:41:42.000
He made it very clear that reliability in our digital computers is is solely on the on the hardware level.

01:41:42.000 --> 01:42:05.000
And by by forcing the signals to decide either for one of a zero with, you know, these, these self interacting loops, driving the signals to saturation, and, and building on that very personal as he puts it software.

01:42:05.000 --> 01:42:21.000
There are two flaws number one, the software needs to be signed outside in there is no way the computer prefers more functional strategies software prefers more functional structures less functions factor by itself.

01:42:21.000 --> 01:42:40.000
All the functional structures must be brought from the outside in. And number two, the unreliability talks about. So if on the software level, you have something that is built on redundancy multiple pathways,

01:42:40.000 --> 01:42:59.000
built on a tractor dynamics, you can work with you can live with underlying hardware neuromorphic hardware for instance, which is analog, which is prone to a noise noise that is bridled in by these.

01:42:59.000 --> 01:43:17.000
If you, if you want to refer to that as such by the software level bridled in by the by what we now have as software. So I'm completely in tune with David at least I love his way of looking at them.

01:43:17.000 --> 01:43:35.000
Thank you Christoph I mean, you know, this was such a big aha for me, because I was raised as a computer scientist I was raised to be all about efficiency, all about correctness, and then to realize that that was all just purchased on this phenomenal act of redundancy at the amplifier digital

01:43:35.000 --> 01:43:55.000
and that you know we were just sort of cruising on that ever since. And yeah so the question is, is how can you build bottom up in terms of software and the idea all that I come up with is that we build simple agents and then we build more complex agents at bigger slower and more complex agents by combining

01:43:55.000 --> 01:44:12.000
the collaborations of smaller, simple ones, and you know, exactly how to do that depends on what the actual hardware you're trying to deal with if you're trying to deal with analog neuromorphic stuff that has one set of affordances that you now have to figure out how to work with.

01:44:12.000 --> 01:44:24.000
I work with, you know, stuff that's a little bit more traditional like cellular automata except, you know, best effort only and asynchronous only and so forth.

01:44:24.000 --> 01:44:35.000
But there's clear set of problems, like control of space and how to reduce variation around space so that you can now do something more specific.

01:44:35.000 --> 01:44:41.000
And if I could take one, one minute I could show a little demo that I wanted to show.

01:44:41.000 --> 01:44:44.000
Because it kind of shows the idea maybe.

01:44:44.000 --> 01:44:54.000
So, you know, one approach is to just make everything big and rigid. Right, so, so here is a block of wall, so to speak.

01:44:54.000 --> 01:45:06.000
And the idea is well but you know instead we want to do something more adaptive more reactive so this is a

01:45:06.000 --> 01:45:15.000
simple cell membrane that's kind of knocking around. And, oops, and I think I may have just killed it. Let's start a new one.

01:45:15.000 --> 01:45:21.000
So much for robustness there you know if God is going to mess it up you know it's not really his fault.

01:45:21.000 --> 01:45:32.000
But the point is, is that you know the existing stuff that we have tries to be rigid, and it counts on the, the underlying hardware remaining rigid and so forth.

01:45:32.000 --> 01:45:35.000
Okay, there's our rigid thing.

01:45:35.000 --> 01:45:41.000
And, but one of the things that we do, because there's this terrible problem you know if you try to be robust.

01:45:41.000 --> 01:45:54.000
But there actually is nothing challenging the robustness, the robustness looks like waste. So there's this inherent tendency to start chipping away at the redundancy, because if there's no actual

01:45:54.000 --> 01:46:09.000
problems, then it doesn't matter, you can get away with it. So, for me can happen in natural evolutionary processes as well. So one of the things that we do in what we build is we build stuff into deliberately challenge the system.

01:46:09.000 --> 01:46:22.000
And one of the things we've got is an element called drag it stands for dynamic regulator. And what it does is, it just wanders around and randomly erases stuff that it's next to every so often, not all the time.

01:46:22.000 --> 01:46:31.000
And every so often it just creates a new sort of food particle that can be used for anything. So I'm going to flood the world with drag right now.

01:46:32.000 --> 01:46:39.000
So, you know what we see happen is the, the rigid structure gets eaten up, right.

01:46:39.000 --> 01:46:49.000
But the, the cell cell, I want to call it a cell membrane it's not a real cell membrane obviously this is so abstracted away.

01:46:49.000 --> 01:46:56.000
It actually does active transport to bring the little food particles inside and it grows.

01:46:56.000 --> 01:47:15.000
So this is my suggestion, an example, you know, a thought experiment well it's implemented not exactly a thought experiment of how you can actually start building up that you build first you build small fast structures, then you build bigger slower structures out of multiple units of

01:47:16.000 --> 01:47:27.000
things. And we just try to keep going up and at each stage of the operation, we're going to have data sheets, saying you know this thing is good for this it'll work at this it'll do badly at that.

01:47:27.000 --> 01:47:36.000
If you go outside these parameters behaviors it's your fault, and so on, just like real systems always have data sheets, but computer science never did.

01:47:36.000 --> 01:47:46.000
And it was again because of that same dam determinism that the hardware guarantee was supposedly providing us.

01:47:46.000 --> 01:47:52.000
What I really like about your work is that you don't try to imitate brains.

01:47:52.000 --> 01:48:01.000
And it's beautiful work about spike neural networks but why you spikes if you can set arbitrary messages and so you come under this organization from a completely different angle.

01:48:01.000 --> 01:48:14.000
And I wonder, have you thought about learning systems in this way, could you build structure that is learning new functions, how would a meta learning system look like because you have to design eventually the meta learning right.

01:48:15.000 --> 01:48:29.000
The last example that I showed the function optimizer as a simple example that was actually implementing the sigh algorithm that I did my dissertation on in 1987.

01:48:29.000 --> 01:48:41.000
And here it is coming back. And one thing that was interesting to me when I was implementing it in the movable feast machine in this architecture is that it was really hard to do a bipartite graph.

01:48:41.000 --> 01:48:52.000
It was to connect everything by equally length units, and I ended up not doing it I ended up doing a crossbar switch and putting, you know, mate weight matrix weight weights at each intersection.

01:48:52.000 --> 01:49:08.000
But then, since it's all asynchronous. That means that the stuff that's close to the crotch where they meet is much faster, and it's a lot like a georgies picture where he showed the simple brain, and then the bigger brains wrapped around it with the bigger loops the slower

01:49:08.000 --> 01:49:24.000
structures fell out in the sigh algorithm in the sigh implementation that I did on the movable feast, because once again, we still had space. There's no random access memory you can't just pretend everything is next to everything.

01:49:24.000 --> 01:49:36.000
Yeah, I think so I think it's going to impact the structure of learning systems in a fundamental way, and it's going to be a pain, because right now you can just throw around matrices.

01:49:36.000 --> 01:49:49.000
You can tensor them up and just do the deep learning things the way people are doing and they're having great fun if I was younger, I would totally be doing that but you know I did my version of it in the 80s and I moved on like that.

01:49:49.000 --> 01:50:01.000
All of that all put together is still under the same as Christoph was saying outside in where you pick the architecture I've got 10 layers and a concentrator and a convolutional layer and then the transform attentional thing.

01:50:01.000 --> 01:50:04.000
That's all done by the human and then it's fixed.

01:50:04.000 --> 01:50:21.000
And I want to have something that's underneath all of that that says you know we can we can do these things, but then we can reprogram them by sending down new software rather than saying we have to throw out the machine and build a new one.

01:50:21.000 --> 01:50:39.000
Maybe that's the brain goal is to put a lot of effort into making the system resistant to catastrophic interference and that's the primary goal probably your brain and my brain never experiences catastrophic interference showed up, you know, a car accident or something

01:50:39.000 --> 01:50:41.000
like that.

01:50:41.000 --> 01:50:50.000
I like a lot of statements you had Dave, my favorite one was this, we have to know what we want.

01:50:50.000 --> 01:50:59.000
First, which is an interesting statement, because this is exactly what AI is about it has to have a goal.

01:50:59.000 --> 01:51:05.000
On the other hand, you, your mind is very fascinated about self organization.

01:51:05.000 --> 01:51:14.000
And there are two interesting things here, you know, one is that it just grows and we don't know what the end product will be. The other one is the goal.

01:51:14.000 --> 01:51:30.000
I think, and this is I think what most of the people don't understand out there, the difference between evolution and and intelligent design is that evolution can be explained best with a goal.

01:51:30.000 --> 01:51:41.000
But that's just a convenience. And of course, evolution doesn't tremendously tremendously misleading sometimes convenience but virtually irresistible.

01:51:42.000 --> 01:51:53.000
And we think that says something to Christos point about low order structure in the environment, so that there is some specific thing to want.

01:51:53.000 --> 01:52:07.000
It's not just like every possible environmental input is equally likely. There is structure there that our brain is trying to latch on to. And once it finds it that feels like a goal, I want to get better at doing that.

01:52:07.000 --> 01:52:17.000
And then we go off to the races, then we can do deep learning. Once we've got something that says, you know, I want this this is a good mapping, then boom, go hill climb and be merry.

01:52:17.000 --> 01:52:26.000
I like that. And you know, there's a whole big field is called decision making, but it is the natural intelligent design kind of.

01:52:26.000 --> 01:52:28.000
It sounds like it.

01:52:28.000 --> 01:52:34.000
It's a great Augustin's definition, but in the real brain decisions are made post hoc.

01:52:34.000 --> 01:52:40.000
Most of the decisions that I made in my life, I justified and it looks like decision.

01:52:40.000 --> 01:52:54.000
After the fact, rather than. Yeah, the dig I make is that intelligence intelligence likes to think it's the captain but really it's the historian.

01:52:54.000 --> 01:52:59.000
There is an excellent question from Kevin to all of the panelists.

01:52:59.000 --> 01:53:09.000
So, what are the panel panels thoughts and artificially creating the phenomenal aspects of our existence. For example, consciousness.

01:53:09.000 --> 01:53:24.000
For example, there is a what it is like to recall an episodic memory while basil ganglier learning and memory can operate outside of awareness, connecting to the ideas of chunking and attractors slash complexity, rhythms of the brain and

01:53:24.000 --> 01:53:26.000
hardware software considerations.

01:53:26.000 --> 01:53:43.000
Would we need anything different to go from artificial intelligence to artificial consciousness and qualia, potentially metabolic or biochemical universities or perhaps artificial conscious computing can already be accomplished without the bits of info at the molecular

01:53:43.000 --> 01:53:53.000
level, like for an artificial hippocampus medial temporal lobe system to create what it is like to encode and recall an episodic memory.

01:53:53.000 --> 01:53:54.000
Wow.

01:53:54.000 --> 01:53:57.000
And can I give a shot.

01:53:57.000 --> 01:53:59.000
Go ahead.

01:53:59.000 --> 01:54:15.000
You know, I to me at least the state of consciousness in my brain is one where a large number of modalities like seeing and hearing and wanting and feeling and emotion.

01:54:15.000 --> 01:54:35.000
And the aims I am after at the present time, where all these modalities are in sync with each other, they speak of the same thing they understand each other they one responds to the other in a useful way, like when you drive your car.

01:54:35.000 --> 01:54:46.000
The signal coming in into your peripheral vision immediately lets your arms turn the steering wheel or your foot hit the break.

01:54:46.000 --> 01:55:04.000
So the phenomenon of consciousness is one of of integration of all subsystems into one coherent state. And I see that as a an example of

01:55:04.000 --> 01:55:18.000
the different fragments, connectivity fragments in the different modalities being activated, activated by activating the participating neurons.

01:55:18.000 --> 01:55:40.000
Activated such that they all fit together into one brain spanning coherent network and network that if it was given time would just recreate a stabilize itself by the exchange of signals that's how I see the, the generation of

01:55:40.000 --> 01:55:42.000
conscious states.

01:55:42.000 --> 01:55:54.000
Just to add to that, you know, the organizer is the brain rhythms, the system of brain rhythms, they are the ones that bring all this together. So probably that's why it is so important.

01:55:54.000 --> 01:56:07.000
And of course you can see the evolution of brain rhythms, and you can study it. Your questions, the question is more complex, I don't think anybody can give a one centers explanation about quality.

01:56:07.000 --> 01:56:18.000
But I can give you an explanation about the role of the hippocampus, for example. So I take out my hippocampus, and I will probably still know I'm Yuri Bojaki.

01:56:18.000 --> 01:56:24.000
I can do a lot of things, except that I will be missing my past.

01:56:24.000 --> 01:56:45.000
I will be missing what is called the no static consciousness that is, I don't know whether I was an agent of a episode or not. I don't remember anything about my life as being a participant of this and this is done entirely by one system this side loop of the hippocampus

01:56:45.000 --> 01:57:02.000
and what kind of consciousness is this. It's a good question because you always say you are awake. Fine, you know your name, and you know that you are distinct from others, but I don't know who I am in terms of my history.

01:57:02.000 --> 01:57:23.000
It's a story that lives in the environment and responds according to the socially agreed norms. This is the kind of thing that I think AI can do not so long from now, but the feeling of it or being you and putting you as the first person

01:57:24.000 --> 01:57:41.000
This is the big question is that when a AI system can be saying that I am the viewer, I'm the first person experienced rather than the third person describer of the events out there.

01:57:41.000 --> 01:57:53.000
A bit more optimistic than your, basically our model of reality is a model of what the world is like right and our model of the self is a model of what our interaction with the world is like.

01:57:53.000 --> 01:58:05.000
And the model is written for us and then informs our future behavior so basically it's the historian that is helping the emergent captain that makes a decision based on the model contents.

01:58:05.000 --> 01:58:15.000
And as a result, this story is continued with the consideration that the emergent captain is making right the entire system is virtual the captain is virtual.

01:58:15.000 --> 01:58:32.000
The, the agent that lives inside of the world that we observe as being us is a virtual construct that is getting causal power, because it would be very useful to know what it would be like for the brain, if such a person existed and coordinated the behavior of the organism right

01:58:32.000 --> 01:58:47.000
So it's a spirit that is being conjured by the interaction of the cell and the phenomenal experience that we have this feeling of what it's like and the content of what it's like is a model of what it is like to attend to our model of reality.

01:58:47.000 --> 01:59:03.000
The purpose of modeling is control so the purpose of our modeling phenomenon experience in our own mind and being able to access it and reflect on it is to control the attention that we are paying and the way in which you are coordinating our inner reality.

01:59:03.000 --> 01:59:18.000
And the issue is that we have to grasp that the consciousness is not physical it's not something that exists at the level of neurons or physics. It is entirely virtual it's a story, and we live inside of that story.

01:59:18.000 --> 01:59:35.000
It is being accessed by this loom that leaves the story to continue the weaving and the contents of the weaving when we reflect on it that is the reflection that we have that the experience as our phenomenology.

01:59:35.000 --> 01:59:54.000
This is not in one brain. No, the word consciousness means joint knowledge, knowledge of yours and mine. And in Russian it's called Susnanya. This is for Tatiana, which also means knowledge mirroring from you.

01:59:54.000 --> 02:00:11.000
I have no idea who I am. If I'm the only person in my own niche, and I'm surrounded only by alligators, I would have a very interesting opinion about myself, certainly very different than living in New York.

02:00:11.000 --> 02:00:22.000
Interesting. You know, you could interpret consciousness also by saying that the different parts of your brain know together.

02:00:22.000 --> 02:00:33.000
That's exactly the first view, because now you are you looking at yourself, but you're looking at yourself as your past experience, which is.

02:00:33.000 --> 02:00:51.000
Mr. Yuri, why you think that this particular function of knowing your own history of being conscious of your own self in as you just said in a social setting couldn't be modeled in artificial intelligence.

02:00:51.000 --> 02:00:54.000
Why would that be a barrier.

02:00:54.000 --> 02:01:07.000
I don't think there are limits, because there are no limits in our world, right. I just say this is the most difficult thing to do.

02:01:07.000 --> 02:01:31.000
The kind of consciousness which is just mirroring that are I know that I'm separate. It's a relatively easy one because the way how I would approach it as a biologist to say what, how can I study these things in an animal and when I say first person second or third person view, then I can study an animal of low organization such as a mouse,

02:01:31.000 --> 02:01:38.000
because in every animal, the most important distinction is the self from everything else.

02:01:38.000 --> 02:01:49.000
There are boundaries and so on this is this is a biological thing that has to be internalized very early on in evolution, because that's the important thing.

02:01:49.000 --> 02:01:57.000
So this is the egocentric world, and then then the allocentric kind of thing that comes much later.

02:01:57.000 --> 02:02:08.000
Because that requires that you generate something that is transferable to the other organism, which is one of your species, you know, this is the social this way it comes up.

02:02:08.000 --> 02:02:15.000
So all these complicated things such as social is, and individual is can come down to the ego versus.

02:02:16.000 --> 02:02:30.000
I mean, as a, as an engineer, I approach consciousness with how can I build it or what would it take to make a crappy little substitute for it and say, maybe it's something like this.

02:02:30.000 --> 02:02:38.000
And I think there, I can get a little bit mostly I don't talk about this because I think the appropriate level of implementation is so much lower than it.

02:02:38.000 --> 02:02:48.000
So consciousness is something that is kind of architectural and structural that is going to see much less mysterious once we earn our way up to that level of architecture.

02:02:48.000 --> 02:03:02.000
But I do have a thought about it and you know one of the things is is okay so we're willing to buy Dave's pitch that we have to have more redundancy in software software is going to have to start taking over some of the reliability work which we had said,

02:03:03.000 --> 02:03:23.000
I saw hardware problem for the first 70 years. And so what that redundancy look like and I think the best example of or the most obvious wonderful example is in software is unit tests that having tests for code is a redundant representation, the format of the

02:03:23.000 --> 02:03:33.000
test and the procedural code that it's testing are getting at similar dynamics, but they get it in a completely different way one does it the other one checks to see if it was done.

02:03:33.000 --> 02:03:47.000
And so that, you know, it used to be in computer science when I was young tests were like, because you're too stupid to prove it's correct, you know, but now we live in a different world where if you don't have tests, then you're essentially negligent.

02:03:47.000 --> 02:04:02.000
And that is a kind of software redundancy right there, that is, you know, you can bite it, you can eat it people love it, because it's good. And so to cover go dot dot dot way past anything that I can actually cover with an implementation right now.

02:04:02.000 --> 02:04:17.000
The sense of qualia the sense of consciousness that the guts of it is going to be that we have these redundant representations, one sort of procedural thing that is we're doing that's actually running our sequencing control what we do move the hand do this.

02:04:17.000 --> 02:04:33.000
And the other one the historian that folks have been referring to that is describing what we've done, and we're going to get a kind of matching between those two representations that's going to be like the test going green thing.

02:04:33.000 --> 02:04:46.000
And that ultimately is going to turn out to be the contents of conscious experience that that green book when the test is passed. We now have told the story. That's my engineer story.

02:04:46.000 --> 02:05:05.000
Thank you Dave so one last question from the audience I'm sorry we couldn't get through everyone's questions so this one is from Paul Cassidy to everyone, which thinkers theories and concepts are you currently finding most fascinating and stimulating.

02:05:05.000 --> 02:05:22.000
These can be either directly related to the discussion topics or not highly creative and interesting people are the best to get recommendations off in my experience.

02:05:22.000 --> 02:05:29.000
So thinkers theories concepts, most fascinating and stimulating cheese.

02:05:29.000 --> 02:05:36.000
You know the funny thing is for all the engineering and science I try to do I really don't have that much time for non friction.

02:05:36.000 --> 02:05:42.000
So you know, I see I feel like I learned a lot much more about the world from fiction.

02:05:43.000 --> 02:05:54.000
So all I can say is, at the moment I'm reading walk away by Corey doctor and I'm finding it fun.

02:05:54.000 --> 02:06:15.000
You know, two authors who are often cited these days in circles that discuss consciousness are Julia to know me and Bernard bars, and both of them actually talk in different ways about coherence of the state.

02:06:15.000 --> 02:06:39.000
And I'm finding coherence of the state into knowing the, the basic idea is the space of all possible states of neurons in the brain is reduced by connection so if, if you cut the brain in half, then the two halves are free to think to create states.

02:06:39.000 --> 02:07:04.000
We're able to to to to create in the presence of the influence from the other half. So that is the kind of, you know, he talks about, of course, entropy and such things, whereas Bernard bars things consciousness comes to comes into existence by there being a central blackboard,

02:07:04.000 --> 02:07:18.000
kind of exchange medium in the middle in the virtual middle of the brain, to which all the different agent or different sub modalities can put stuff and from which they can read stuff.

02:07:18.000 --> 02:07:28.000
So, again, a kind of means of creating order of creating coherence in the whole thing.

02:07:28.000 --> 02:07:36.000
So many of the discussions I've seen. These are referred to as the main theories.

02:07:36.000 --> 02:07:39.000
Whether you like it or not.

02:07:39.000 --> 02:07:46.000
So Christoph, what do you think of integrated information to know any stuff to buy it.

02:07:46.000 --> 02:07:59.000
Yeah, I agree with his basic idea. But I mean you cannot make a whole career out of out of this idea with a five function you cannot measure it you cannot compute it.

02:07:59.000 --> 02:08:04.000
You cannot deduce anything from it. It's a nice thought.

02:08:05.000 --> 02:08:20.000
But so what I find to to narrow the the different modalities where my brain cannot speak the same language, they cannot communicate in the same language on the blackboard.

02:08:20.000 --> 02:08:24.000
So they speak different pairs of languages.

02:08:24.000 --> 02:08:27.000
And so that I find that to narrow.

02:08:27.000 --> 02:08:42.000
Yeah, to me it seems it's metaphorical at best and even if it's a metaphor, I don't know if it's a good one but it's well it seems popular I mean just like the markup blanket stuff seems popular but it seems.

02:08:42.000 --> 02:08:43.000
I don't know.

02:08:43.000 --> 02:08:45.000
I have trouble buying it.

02:08:45.000 --> 02:08:46.000
Thank you.

02:08:46.000 --> 02:08:49.000
I'm an active leader.

02:08:49.000 --> 02:08:52.000
I'm an active leader. I read a lot.

02:08:52.000 --> 02:08:58.000
I would make a short list of whom I would recommend because every single time I find some great idea.

02:08:58.000 --> 02:09:04.000
What fascinates me most is that how far I have to go back in history to find the same idea.

02:09:04.000 --> 02:09:11.000
You always do so I my, my favorite books are always old.

02:09:11.000 --> 02:09:28.000
You mentioned in the during the prep session you mentioned the prepared unprepared counter counter prepared concepts who was that by selling month I can send you the reference right away.

02:09:28.000 --> 02:09:37.000
So, for example, you know that one is a extraordinary person when we talk about, you know, in my book inside out.

02:09:37.000 --> 02:09:43.000
I go back in time and I realize that you know I maybe I have to go all the way to plateau.

02:09:43.000 --> 02:09:55.000
Because that was the odd to be decided out but then Seligman and many others that Breland and Breland that that we discussed at the before the meeting they are great thinkers.

02:09:55.000 --> 02:09:59.000
You know, ideas come by every 50 years.

02:09:59.000 --> 02:10:05.000
Small additions come by every day.

02:10:05.000 --> 02:10:09.000
Rain cycles do continue to lower herds.

02:10:09.000 --> 02:10:11.000
Very low.

02:10:11.000 --> 02:10:13.000
Indeed.

02:10:13.000 --> 02:10:17.000
Of the most interesting current authors.

02:10:17.000 --> 02:10:26.000
I think, and Greg Egan, I think Greg Egan is probably the most interesting philosopher for mind was currently alive.

02:10:26.000 --> 02:10:31.000
greg and this last name is Egan E G a n.

02:10:31.000 --> 02:10:35.000
I certainly will pitch permutation city to anybody.

02:10:35.000 --> 02:10:37.000
Yes, sir.

02:10:37.000 --> 02:10:42.000
Yes, as books to read.

02:10:42.000 --> 02:10:58.000
Yes, it's also very beautiful, mathematically inspired short stories. He's, he's a very mathematically inclined and physically, physicists like person who plays a lot with this ideas in his mind and it's not

02:10:58.000 --> 02:11:08.000
a story driven the things that he writes the stories are vehicles to explore ideas and mental fractals. It's quite beautiful and intricate.

02:11:08.000 --> 02:11:19.000
But I do agree with viewers statement that most of the good stuff is actually quite old learned a lot by reading can and

02:11:19.000 --> 02:11:32.000
I also besides that I never found them that exciting I grew up with him. And now I realize he's very good. He just was not exciting because he was just the intellectual ground zero where I grew up. It was just so normal.

02:11:32.000 --> 02:11:39.000
But it's still very good stuff.

02:11:39.000 --> 02:11:44.000
Excellent. Thank you, Yosha. So, with this, we wrap up.

02:11:44.000 --> 02:11:50.000
We would like to thank Intel Labs for allowing us to host this event.

02:11:50.000 --> 02:12:01.000
And we would like to thank all of the presenters for participating and for delivering these amazing presentations and the lively discussion afterwards.

02:12:01.000 --> 02:12:20.000
The meeting has been recorded and will be posted on our YouTube channel. The link can be found in the chat and the chat log with all the references will also be posted on our website so don't worry, you don't need to copy paste anything in a rush it will be there.

02:12:20.000 --> 02:12:32.000
So with this, thank you everyone. It was great meeting you all thank you the audience for participating and enjoy the rest of your day.

02:12:32.000 --> 02:12:39.000
Thanks to all our guests. It was amazing and thank you Tanya for having with the organization and doing the moderation today.

02:12:39.000 --> 02:12:41.000
Thank you, thank you.

02:12:41.000 --> 02:12:44.000
Nice meeting you guys.

02:12:44.000 --> 02:12:49.000
Absolutely, it was great.

