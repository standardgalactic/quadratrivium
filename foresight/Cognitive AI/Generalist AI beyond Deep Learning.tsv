start	end	text
0	5000	Welcome everyone from and hello from Rainey, California.
5240	7600	Today we're hosting an exciting event
7600	10080	on the future of General SDI.
10080	11600	Our guests today are professors
11600	14600	Christoph Fondrell-Malsburg and Michael Levin.
14600	16720	Michael Levin is a distinguished professor
16720	18160	at the Department of Biology
18160	20800	and the principal investigator at the Levin Lab
20800	22720	at Tufts University.
22720	25160	Christoph Fondrell-Malsburg is a senior fellow
25160	27360	at the Frankfurt Institute for Advanced Studies
27360	28560	and the visiting professor
28560	33440	at the Institute of Neuroinformatics at ETH Zurich.
33440	35920	And finally, our host today is Dr. Yoshabach,
35920	39760	my colleague and the principal engineer at Intel Labs.
39760	41360	My name is Tania Greenberg.
41360	44640	I am an AI scientist at Intel Labs also.
44640	46960	We would like to express our thanks to Intel Labs
46960	50400	for sponsoring and hosting this event series.
50400	53840	So without further ado, let's get started.
53840	56720	The audience is going to be muted by default for this event.
56720	59400	If you would like to ask a question or make a comment,
59400	61560	please post your thoughts in the chat.
61560	63480	You should have access to it.
63480	66480	We will have a hopefully longer than usual Q&A session
66480	67800	at the end of today's session.
67800	70920	So we should be able to go through many of your questions.
70920	73400	All right, Yoshab, the floor is yours.
74800	78400	Okay, I proceed with our slides for now.
78400	79240	Let's see.
80080	82360	The question that we want to discuss
82360	85080	is how we can build intelligent systems
85080	90200	that move beyond the present limitations of deep learning.
90200	92440	And this is quite a bold proposal
92440	95080	because you don't know if deep learning has any limitations.
95080	96160	At least I don't know.
96160	99160	I don't know any proof that demonstrates
99160	100720	that there is a certain thing
100720	103200	that the methods of deep learning cannot do.
103200	106200	It seems to be that the present generation of models,
106200	108440	which is widely successful,
108440	111360	generative AI is built on the transformer algorithm,
111360	114800	which is a variant of the deep learning mechanisms
114800	118160	that have been existing since the perceptron.
118160	122560	And the limitations of this thing are not clear, right?
122560	124920	We basically get to meaning in the limit.
124920	128720	We need to use massive data and compute to get there.
128720	131080	And these models have difficulty
131080	132640	to become fully coherent,
132640	134920	but it's not obvious that such a limitation
134920	136840	cannot overcome as a loss function
136840	139720	or this just using the existing regimes
139720	142080	and scaling them up even further, right?
142080	146080	We cannot prove that there's a combination of codecs
146080	148320	and the existing methods and some real time
148320	150240	and some online learning.
150240	151720	We won't be able to build a system
151720	154840	that is a good enough artificial general intelligence
154840	156920	and is able to discover the next generation
156920	158760	of algorithms for us.
158760	160840	And this idea that deep learning
160840	162320	in the way that it exists
162320	164920	with basically the algorithms that we have right now,
164920	167600	with slight changes to the loss functions and so on,
167600	169440	that it is sufficient.
169440	171960	This is called the scaling hypothesis.
172640	175760	On the other hand, most people who work in the field
175760	180360	have the sense that there is a problem with deep learning
180360	182840	in the sense that it's boot forcing the job.
182840	185160	It seems that biological organisms
185160	186720	are able to make sense of the world
186720	190680	with much, much less data and dramatically less compute.
190680	193240	And there is of course a contention
193240	196240	about how much compute a brain has
196240	198760	and how much compute an organism has.
198760	201720	And very often the calculation is made in such a way
201720	205800	that we discuss how many GPUs you need
205800	208560	to emulate a group of neurons.
208560	211000	And depending on how you look at the neuron,
211000	214560	the neuron might be something like a four or 12 layer
214560	218320	neural network or it might be something even more complicated
218320	221760	if you look down to modeling every synapse and so on.
221760	225120	And on the other hand, relatively rarely we ask
225120	227840	how many brains you would need to run macOS
227840	230080	because the way our nervous system works
230080	233120	is highly redundant and stochastic.
233120	235760	And much of what the individual neuron is doing
235760	238480	is probably only relevant to the individual neuron
238480	239880	in its survival itself,
239880	243000	because after all the neuron is a single celled animal.
243000	247640	So if you take the job of an individual,
247640	249360	for instance in a corporation,
249360	252280	that individual is going to contribute
252280	254960	a lot of its intellectual capability to the corporation,
254960	257720	but it's going to be a tiny fraction
257720	260000	of the total ability of the individual
260000	261720	that it needs to survive by itself
261720	264240	and to communicate with its immediate neighborhood.
264240	268840	So just asking to emulate a single neuron
268840	271600	and then multiplying this with the number of neurons
271600	273720	is probably not the right way to do it.
273720	275480	And it's not clear how many neurons
275480	277240	you would need to run MNIST.
277240	280920	As far as I know, there is no simulation so far
280920	283320	that is using a close neural model
283320	287440	or that is using a small group of neurons in vivo
287440	290600	that are being trained to do MNIST.
292280	295480	More generally, I think we need to answer the question
295480	296520	what intelligence is.
296520	299800	And for me, this question has evolved over the years
299800	303800	and my current answer is that intelligence is the ability
303800	307320	to construct a path through a space of computable functions.
308560	312200	It's a slightly fancy way of saying
312200	314760	that intelligence is the ability to make models
314760	316920	because at the end of the day,
316920	319040	models are computable functions
319040	324120	that are designed in the service of control.
324120	327920	So we have a system that is making some kind
327920	329360	of regulation task
329360	331520	or performing some kind of regulation tasks
331520	334840	and when it's controlling the future, it's an agent.
334840	336960	And to control the future and being an agent,
336960	340200	you will need to construct some kind of control model
340200	341240	that is counterfactual.
341240	343320	You need to be able to represent states
343320	344240	that are not here yet
344240	346160	because the future is not there.
346160	348480	And that means you need to have a system
348480	352080	that is able to perform arbitrary causal transitions.
352080	354600	And this causal insulation from the substrate
354600	357440	to represent something that is not the case yet,
357440	360640	that is basically what is a computer.
360640	363880	And for the computer it does, it represents functions
363880	366720	which means mapping from state descriptions
366720	368640	to other state descriptions.
368640	372880	And the computer can do this in completely arbitrary ways.
372920	374720	And this is the basic idea
374720	378160	of building a system of computable functions.
378160	381440	And when you come up with a way
381440	383320	to enumerate the computable functions,
383320	386040	to list them all, then you can search them.
386040	388200	And if you want to learn something,
388200	390920	you need to in some sense enumerate,
390920	393160	organize the space of computable functions
393160	395280	in such a way that you find those functions
395280	398200	that you are interested relatively early on.
398200	401560	And this question of how you can construct a space
401560	403440	for the computable functions that is tractable,
403440	406160	that is able to converge to a solution
406160	407560	for the problem at hand.
407560	408760	That is the main issue, right?
408760	411080	Making computer is relatively easy,
411080	413040	especially if you have something like a cell
413040	415040	because it's already contains a computer
415040	416920	and they can also organize themselves
416920	418680	into higher level computers
418680	422920	and become less stochastic and so on while doing this.
422920	425160	But the big difficulty is,
425160	429520	how can we find the functions that we are interested in?
429520	431200	And so when we look at deep learning,
431200	432960	what is deep learning?
432960	435080	The first thing that we have to note about deep learning
435080	437520	is that deep learning is the only thing
437520	439800	that currently works at scale.
439800	441800	It's the only class of algorithms
441800	445720	that is able to discover arbitrary functions
445720	448040	in a reasonable amount of time.
448040	452400	And reasonable being orders of magnitude more time
452400	455720	and more data than a human being does.
455720	458200	That's of course, the training time for something
458200	463200	like diffusion stability AI model, right?
463680	466680	This stable diffusion is a model of two gigabytes
466680	470560	and it contains the whole of the art
470560	473520	and you can get every celebrity you want,
473520	477080	you can get every spaceship from popular culture,
477080	478520	you can get dinosaurs,
478520	480800	everything is in these two gigabytes.
480800	484240	And training this takes weeks.
484240	487360	And it sounds that's very little compared
487360	490480	to the years that it takes to train a human brain.
490480	495480	But during these weeks and now it comes down closer to days,
496640	501040	this thing is going through hundreds of millions of images.
501040	502960	Many more than a human being could process
502960	505560	and the lifetimes and find correlations between
505560	508600	using dramatically large server farms.
509880	512080	So how does deep learning work?
512080	514720	First of all, it's differentiable computing,
514720	517200	which means it is representing all the functions
517200	519680	in such a way that they form a continuous space
519680	523440	in which neighboring variations of the functions
523440	525640	still lead to interesting results.
525640	528960	And you can move by a small nudges through the space
528960	531840	of functions to get the function closer to what you want.
531840	534240	That's different from discrete programs,
534240	535600	program and source code.
535600	537400	If you change a few bytes in the source code,
537400	539120	the program will no longer work.
539120	542560	If you change the neural network by a slight bit,
542560	545440	then it's still going to give you useful results.
546440	549600	And to do this, the neural network is describing
549600	554600	the functions via weighted sums of inputs
555400	558880	that are arranged into chains, basically in layers.
558880	561360	And some non-linearities, which are in a way to make
561360	565080	if sense, to make conditional breaks in this network.
565080	569120	And this description via multiplications and sums
569120	572280	is sufficient to represent all the functions that we want.
572280	574840	And we train this network by setting it up
574840	578080	in such a way that it has the potential to build enough things
578080	580040	that has enough links between the nodes
580040	581840	that it sums values up.
581840	583720	And then it just changes the weights,
583720	586440	which means the multiplying factors
586440	590320	for the inputs of every node in a systematic way.
590320	593480	And it, in some sense approximates all the functions
593480	594560	via real numbers.
595480	597920	And if you think about alternatives to deep learning,
597920	602240	the main thing that comes to mind is to build up computations
602240	604800	from deterministic discrete operators.
604800	609000	Such as Boolean logic or simple automata,
609000	610840	like cellular automata.
610840	613880	And as a result, we get finite Turing machines.
615160	616880	And in the finite case,
616880	619640	dynamical systems and Turing machines
619640	623040	and automata are the same thing, computationally speaking.
623040	625000	They're all Turing machines,
625000	628200	as long as you don't run out of resources.
628200	632120	So every digital computer in reality is a dynamical system
632120	634240	because the physics that digital computers
634240	636880	and prevains on is somewhat continuous
636880	639680	from the perspective of the individual transistors
639680	640520	and so on.
640520	643240	And we just try to find a region in physics
643240	645200	that makes the transistor reliable enough
645200	647560	as a discrete unit and deterministic
647560	650040	from the perspective of the logical language
650040	653000	that is implemented on the arrangement of transistors.
653000	655800	But underneath, there is an analog system.
655800	657080	It is just noisy.
657080	660720	And that is a system that is discrete.
660720	663840	On the other hand, every dynamical systems in physics
663840	666000	at the lowest level is discrete again.
666000	669000	If you zoom in, what you see is nothing continuous.
669000	671040	What you see is individual atoms
671040	673200	and individual charges and so on.
673200	675440	And they are discrete again.
675440	677960	And there is some discussion about
677960	680200	whether everything has to be discrete at the level
680200	682080	because of the nature of languages itself.
682080	683680	And I think that's the case.
683680	687280	I think that the discovery of the last century
687280	688960	that was most important in philosophy
688960	691840	is that those languages which assume
691880	694600	that the bottom most layer can be truly continuous.
694600	696760	They run into contradictions.
696760	698960	But this only matters if you are really interested
698960	701360	in modeling the bottom most layer.
701360	704040	If you just think about computation,
704040	705720	it doesn't really matter if you start out
705720	709400	with a dynamical system or this discrete system
709400	712000	as long as you are willing to allow
712000	715000	that every dynamical system has only finite resolution.
715000	717320	So there's only finitely many bits
717320	720360	that you can manipulate at any given moment.
721880	726400	And this equivalence between the continuous mathematics
726400	727560	and the discrete mathematics
727560	729800	has been shown basically in both directions.
729800	731400	You can use a computer.
731400	733080	And then just by using more bits,
733080	735440	you can approximate continuous system
735440	737640	with any degree of fidelity as you want
737640	740000	in the same way as digitized music
740000	743600	can sample the space of audio functions
743600	746680	below the level of resolution
746680	749600	that your medium can provide.
749640	752240	So you will get to something that is equivalent.
752240	755960	And the opposite direction has also been shown.
755960	758080	Greg Chaitin has gone to the trouble
758080	760640	to translate a LISP interpreter
760640	765640	into a diophantine equation with 17,000 variables.
765720	767320	Right, so this paper is called
767320	769920	the Complete Arithmeticization of Evil.
769920	772840	It's the evaluation function of LISP,
772840	777040	which did in 1987, it's quite beautiful.
777040	778600	And I don't think that is something
778600	780040	people actually want to read.
780040	783880	And this formula was generated
783880	785840	with some generative procedure.
785840	789000	He did not write these 900,000 characters by hand
789000	790280	that went into this.
790280	792720	But it's been shown, you can write down
792720	796000	the LISP interpreter in continuous mathematics.
797040	798960	So the alternative to deep learning
798960	801440	might be to basically construct functions
801440	804280	from the ground up using automata.
804280	808360	And in a way, this is also what people have done
808480	811760	because all our computers on which we run deep learning
811760	813160	are discrete automata.
814040	816680	People started to build this continuous arithmetic,
816680	821480	they built end or addition, multiplication and so on
821480	823920	from discrete logical units.
823920	825080	And this is clearly practical,
825080	827320	but it was done with relatively few people
827320	829440	and relatively few years.
829440	833160	The search process to build up continuous arithmetic
833160	836240	from discrete logical operations is not very large.
836240	839840	And if you want to get, to discover a deep learning
839840	842880	algorithm as a special case over discrete automata
842880	846280	from scratch, it's the existence proof
846280	847920	has been shown by the computers
847920	849920	and the deep learning mechanisms that we have.
849920	853640	Because relatively few people use relatively little brain power
853640	855520	compared to where you want to get to
855520	857840	to discover all these solutions.
857840	860080	By self-play, you could in the same way
860080	863600	as computers played a goal, discover arithmetic
863600	866000	but using discrete systems.
866000	868640	So in many ways, this is going to be equivalent.
868640	870800	And I suspect it might be interesting to start
870800	874000	from this automata direction again and build up learning.
874000	876320	And there are a few people which work in this area
876320	879760	but so far nobody has come up with an alternative
879760	882920	that scales up in the same way as deep learning.
882920	885840	And then there is a slight, the different approach
885840	886880	that we might be looking at.
886880	889800	And it doesn't use a normal Turing machine
889800	891840	but a non-deterministic Turing machine,
891840	893720	a multi-phase system.
893720	898440	And that is because systems in the lowest level of physics
898440	901120	and I also suspect systems that are implemented
901120	904960	on brains, non-deterministic systems.
904960	906600	And this doesn't just mean that they're noisy
906600	908920	or that they're random in many ways.
908920	912360	A non-deterministic Turing machine is a paradigm
912360	917080	from computer science that describes your state machine
917080	920000	in such a way that not every state
920000	921920	has exactly one successor state.
921920	923960	It's not sufficiently constrained
923960	926120	to have only one successor state.
926120	929080	Our Turing machine that we normally use
929080	931440	and this includes our digital von Neumann computers
931440	933080	and so on, they're defined in such a way
933080	938080	that every state has exactly one possible successor state.
938320	939840	If there is a branch in the computer
939840	941720	that's because it is depending
941720	943480	on some environmental variable
943480	945640	that is not relevant in the program
945640	947640	which means some input of the program.
947640	949480	But given the same input,
949480	952600	the Turing machine is going to produce the same output
952600	956200	and it's going to do this along exactly one path.
956200	959800	And not just Turing machine, the computational sense
959800	961960	is also going to give you the same output
961960	963880	but it's going to do this on many paths
963880	966760	because the constraints are not so narrow
966760	969400	that you go into one state after every state
969400	971840	but you can go into multiple states.
971840	973440	And the non-deterministic Turing machine
973440	976040	just goes into all of them branches.
976040	978360	And these branches don't necessarily meet again, right?
978360	980080	There is no way that they're connected again.
980080	982280	It's basically a non-deterministic Turing machine
982280	985240	is implementing some kind of multiverse.
985240	988520	But it turns out that many of these multiverse branches
988520	990880	that the non-deterministic Turing machine goes into
990880	992880	have the same bit combination in them
992880	996040	because there are only so many states that are reachable.
996040	998520	And so by looking at the dynamics between these bits,
998520	1001960	you can get to the same point in the universe
1001960	1003720	on multiple paths.
1003720	1006200	And if the universe that we live in,
1006200	1007720	the physical universe that we live in
1007720	1009440	is such a multivase system,
1009440	1013240	it's still going to be possible for observer
1013240	1014480	that lives inside of it,
1014480	1017200	despite it not knowing which path it goes down,
1017200	1021000	determine statistical properties over the regional paths
1021000	1023880	which means you cannot know in your universe
1023880	1028120	which slit the double slit experiment the photon goes through
1028120	1029920	but you can predict the patterns
1029920	1032920	that many, many photons are going to make on the other side.
1032920	1035200	And the things that we can predict in our universe
1035200	1036840	are of that nature.
1036920	1039160	There are statistical properties
1039160	1041840	over all the trajectories that can happen
1041840	1044800	that we are a part of.
1044800	1047280	And how would something like this look in the brain?
1047280	1050560	Like in the brain, obviously the brain can only hold
1050560	1052320	a finite amount of state.
1052320	1054520	But if you have redundancy, what you can do is
1054520	1058320	if you don't tell the neuron to go into one particular state
1058320	1061280	as a result of its present state
1061280	1064280	but into a range of possible states.
1064280	1066760	And this happens randomly.
1066760	1069080	It means that there are many trajectories
1069080	1072400	that activations can take in the brain at the same time.
1072400	1074440	And they're going to meet on the same physical substrate
1074440	1075640	and they're going to accumulate
1075640	1077760	and you're going to sum up in some sense
1077760	1081280	and could be some basically some voting over the different paths.
1081280	1084120	And this means that a system like the brain
1084120	1085920	could via transmitting activation
1085920	1088440	try many, many things in parallel
1088440	1092160	and stochastically with some degree of randomness.
1092160	1096000	And as a result perform computations efficiently
1096000	1099480	that can not be efficiently done with a linear machine.
1099480	1100960	From a computational perspective,
1100960	1103480	if you want to simulate this on a linearly
1103480	1105240	on a sequentially operating machine,
1105240	1106520	it's going to be highly inefficient
1106520	1108440	because you're trying to do the same thing
1108440	1109960	over and over again.
1109960	1112560	And this randomness is going to delete some of the bits
1112560	1114400	that you've been computing.
1114400	1117280	But if your computational units are very cheap,
1117280	1119920	like in the brain, in the brain time is expensive
1119920	1121920	because taking one more step means
1121920	1122920	that you're going to be slower
1122920	1124800	in your interaction with nature.
1124800	1126320	But parallelism is cheap.
1126320	1130160	Just by doubling the cell count once more
1130160	1131800	by dividing the brain cells,
1131800	1133680	you get twice as many elements.
1133680	1135440	If you do this quite a few times,
1135440	1137600	you end up with billions of elements.
1137600	1140240	And these billions of elements can perform
1140240	1142560	many of these paths in parallel.
1142560	1144360	And this is something that to my knowledge
1144360	1146360	has not been seriously attempted yet
1146360	1148360	to get to work for a learning system.
1148360	1150680	There are some people which are thinking seriously about this.
1150680	1153800	For instance, we had Jerome Buzemeier on a panel here.
1153800	1157040	And he is describing the mental representations
1157040	1160800	as superpositional states using such a multivase system.
1160800	1163320	And it's also an idea that has occurred to Steven Wolfram
1163320	1166960	who is thinking of the universe as a multivase system
1166960	1168640	and who's open to the idea
1168640	1170920	that the brain might be basically
1170920	1173160	a bounded complexity multivase system.
1174920	1178920	So this, we see that there are opportunities
1178920	1183640	to build solutions that are inspired by biology,
1183640	1185440	that we haven't tried yet.
1185440	1188320	And if you look at the inspiration that happens
1188320	1191200	so far from biology into artificial intelligence,
1191200	1193680	despite many claims to the contrary,
1193680	1195480	it almost never happens.
1195480	1198560	I think that the last contribution of biology
1198560	1201960	to the transformer was heavy on learning.
1201960	1205680	And everything else that people got to work since then
1205680	1206960	was mostly not happening
1206960	1209200	because people looked at newer science results
1209200	1210240	and implemented them
1210240	1213760	and then ended up with a better machine learning algorithm.
1213760	1216680	And if you take the ideas that newer scientists currently have
1216680	1218800	about how the brain works, to my knowledge,
1218800	1220520	you cannot implement them in such a way
1220520	1222680	that they learn and control an organism.
1222680	1225600	Even a very simple organism like C elegance,
1225600	1227600	if we take the conic term of C elegance
1227600	1230960	and translate this into a computer model and run it,
1230960	1233920	it's not going to produce coherent warm behavior.
1233920	1236040	C elegance is a small warm
1236040	1240200	and it has only 300, I think 309 neurons.
1240200	1244040	Please correct me if this number is off.
1244040	1247120	And as a result, if you take this conic term
1247120	1249600	and run it into a digital simulation,
1249600	1253720	it's not going to produce the behavior that we want
1253720	1257840	because presumably we have not caught up
1257840	1260120	on all the functionality of the individual neurons
1260120	1261760	in the context of that warm.
1261760	1263720	Could be that there is stuff in the soma of the cells
1263720	1266640	that is not visible in the conic term
1266640	1270240	or that we made mistakes in digitizing the conic term
1270240	1273200	or I don't have enough resolution to see all the vesicles
1273200	1275760	that use different neurotransmitters in the conic term
1275760	1277520	to get the functionality right.
1277520	1281600	But for systems at scale that go beyond an organism
1281600	1286600	with a few hundred neurons, something like cortical column
1286640	1288480	and so on, we don't really have working models
1288480	1289320	at the moment.
1289320	1293640	The descriptions that the neurobiologists have at the moment,
1293640	1295840	if you translate them into computer models,
1295840	1297680	are not performing the same things
1297680	1299680	as our digital models are doing.
1299680	1302120	They're not able to discover these functions
1302120	1305240	because these models are still incomplete
1305240	1307640	and they're not ready yet for being used.
1307640	1309160	There's a more fundamental problem
1309160	1312680	that Konrad Hauding has highlighted in the paper
1312680	1314920	where he used the methods of neuroscientists
1314920	1319480	to reverse and engineer a microprocessor.
1319480	1321200	If you take a normal microprocessor
1321200	1322800	and give this to a neuroscientist,
1322800	1324960	the neuroscientist does ablation studies
1324960	1328120	and looks at the structure that the neuroscientist finds.
1328120	1330840	Is the neuroscientist able to reverse engineer
1330840	1333280	this microprocessor which is dramatically simpler
1333280	1335120	than the nervous system, of course.
1335120	1339200	And it turns out that Konrad Hauding
1339200	1341520	who is a neuroscientist that is the methods
1341520	1345760	of neuroscience might be for some more deep reason
1345760	1347600	because they're not functionalist enough,
1347600	1351160	able to discover how information processing
1351280	1352680	and nervous system works yet,
1352680	1355240	which means that the theoretical tools of neuroscientists
1355240	1356960	might not yet be ready even
1356960	1358960	to understand what brains are doing.
1361440	1364280	And I think that when we as non-neuroscientists
1364280	1368080	look at brains and at the textbooks of neuroscience
1368080	1370520	that take what I learned at university,
1370520	1372800	I find from my current perspective
1372800	1376000	that there are some shortcomings in this description
1376000	1378920	that if I want to sit down as a computer scientist
1378920	1381520	and discover the space of possible solutions
1381520	1384880	in nervous systems and for functional approximation
1384880	1386520	and just get a machine to search through it
1386520	1388520	and so on would be insufficient.
1388520	1390320	And this starts out with the idea
1390320	1393440	that a neuron is a specialized switch
1393440	1396240	at the way in which we abstract the neurons
1396240	1398520	since the perceptron is that the neuron
1398520	1403080	is some very simple gate or something element
1403080	1405680	like in a circuit that does something
1405680	1409400	in a mechanized automatic algorithmic way.
1409400	1410840	And I don't think that's really true.
1410840	1412600	A neuron is not a specialized switch.
1412600	1414960	A neuron is a single-celled animal.
1414960	1415920	It's quite complicated.
1415920	1416760	It wants something.
1416760	1418200	It wants to survive.
1418200	1419800	It is able to learn by itself.
1419800	1421040	It is adaptive.
1421040	1422520	Can do a lot of things.
1422520	1426000	It's almost like an amoeba that links up with other amoebas
1426000	1428240	and grows into this static structure
1428240	1431600	depending on its environment to perform a particular task.
1431600	1434080	And this is quite different as a perspective.
1434080	1437280	It means that your neuron is not just some kind of integrator
1437280	1439640	or some kind of weighted sum of real numbers
1439640	1441520	of activations that come in.
1441520	1444640	The neuron is really a reinforcement learning agent
1444640	1445680	with a little bit of memory
1445680	1448520	and a little bit of ability to look into the future.
1448520	1451000	Not very far, but a little bit.
1451000	1454840	And it implements a number of adaptive functions
1454840	1457880	to deal with its environment and reap rewards
1457880	1461200	that ultimately allow the neuron to survive in the brain
1461200	1463640	and not be strapped by the organism or killed by it
1463640	1466480	because it's not doing the right thing.
1466480	1468360	The second misunderstanding, I think,
1468360	1471920	beyond neurons are not just specialized switches is
1471920	1473920	that it's only neurons.
1473920	1476560	There is probably no fundamental difference
1476560	1478920	between neurons and other cells.
1478920	1481560	That is, every cell can do information processing
1481560	1483200	in conjunction with other cells
1483200	1485240	if it's in a multicellular animal.
1485240	1488360	It probably needs to be somewhat multicellular
1488360	1490440	because if it's a single-celled animal,
1490440	1493000	it's maybe evolving in such a way
1493000	1495120	that it's adversarial to its environment.
1495120	1499040	It doesn't benefit if it computes information
1499040	1500600	together with others.
1500600	1502800	But if it lives together with others
1502800	1504160	in a high degree of organization
1504160	1506200	like the cells in our body are doing
1506200	1509360	and you co-evolve them, then the cells can all send
1509360	1511360	many types of messages via chemicals
1511360	1514720	over the cellular membranes to neighboring cells.
1514720	1517000	They can interpret those messages
1517000	1520720	and they can learn how to respond to these messages,
1520720	1524400	which means they can learn to perform arbitrary computation.
1524400	1527640	So what's the difference between a neuron and another cell
1527640	1528880	if they can do the same thing?
1528880	1531160	Well, the specialization of neurons
1531160	1534120	is that they have extremely long accents,
1534120	1536960	so which they can send information coded
1536960	1539960	as electrochemical impulses very quickly
1539960	1541840	over long distances.
1541840	1543920	I think that neurons might be best understood
1543920	1545520	as telegraph cells.
1545520	1548800	They're very special cells that have evolved only in animals
1548800	1552200	or mostly in animals and that are very expensive to run
1552200	1553840	because they need a lot of energy
1553840	1556040	to send information so quickly.
1556040	1559160	And the benefit is that they can move an animal very quickly
1559160	1561480	so nature, so it can eat plants or other animals
1561480	1562720	to get that energy.
1562720	1565320	So basically the animal gets more energy
1565320	1568000	that it could get from photosynthesis.
1568000	1572080	And as a benefit, it's able to afford
1572080	1574800	to have this expensive nervous system.
1574800	1577480	And the code that the neurons are using
1577480	1580440	is probably different from the code, from the chemical codes
1580440	1582280	that the cells are using for the messages.
1582280	1584160	It's like a Morse code, probably something
1584160	1587960	like a telegraph system, so it can send information so quickly.
1587960	1591120	And initially, it seems that to me
1591120	1594600	that neurons might have evolved to move skeletal muscles
1594600	1595640	at the limit of physics.
1595640	1598560	So it can, from centralized coordination
1598560	1602520	in the nervous system, from the central nervous system,
1602520	1603960	send information so quickly
1603960	1607440	that the whole organism is coordinated much, much faster
1607440	1611200	at much shorter time spans than plants are.
1611200	1613840	And the other thing is once you can move very quickly,
1613840	1616000	you also need to perceive very quickly.
1616000	1618600	So it's also driving sensory input
1618600	1620880	and the evaluation of sensory input
1620880	1623160	and decision-making and learning.
1623160	1626280	So it's basically duplicating the information processing
1626280	1628080	that existed in the body
1628080	1632920	and it is creating something like a second information
1632920	1636160	processor in the body that is running at much, much faster
1636160	1639600	time scales than the normal cellular information processing
1639600	1642880	that will also exist in large, long-lived plants.
1642880	1647160	So my perspective is that just by looking at means and motive,
1647160	1649360	the possibilities of what evolution can do
1649360	1652040	and the capabilities of what an individual cell has,
1652040	1655680	I suspect that every long-lived organism with many cells
1655680	1659520	is basically going to function like a very, very slow brain.
1659520	1662840	And there are almost no limits
1662960	1666680	to what this slow brain can do if it lives long enough,
1666680	1668960	but it's not going to do this at the same time frame.
1668960	1671800	So animals will be able to outthink plants
1671800	1674960	just because they are so fast and run circles around them.
1676960	1681320	And the third misunderstanding is that we think
1681320	1684000	that consciousness is extremely rare,
1684000	1686840	that consciousness may be only existing in humans
1686840	1690320	and forms only very late in the evolution
1690320	1692040	of intelligent systems.
1692040	1695840	But it turns out we don't get conscious after the PhD.
1695840	1699360	We seem to be conscious before we can track a finger.
1699360	1703280	And if that is the case, maybe self-reflexive attention
1703280	1705640	is a requirement if you want to learn
1705640	1707360	beyond happy and learning.
1707360	1709440	If you don't want to just look at co-activation
1709440	1711680	between cells as a learning paradigm, which is probably
1711680	1714160	sufficient to map your body surface and so on,
1714160	1716960	but you want to have a coherent model of reality,
1716960	1720320	maybe you need to start out with some kind of core
1720320	1723280	that organizes everything into coherence.
1723280	1725760	And what we find confusing is about consciousness
1725760	1728320	that is that we don't seem to need it for many things.
1728320	1731440	A sleepwalker can do many of the things that normally
1731440	1732520	require consciousness.
1732520	1735480	And this is confusing philosophers to no end.
1735480	1737720	But maybe consciousness is in some sense
1737720	1740240	like government in a society.
1740240	1743800	And I don't mean government as some kind of abstract principle,
1743800	1746200	but as a real-time interaction that
1746200	1750080	is making society coherent.
1750120	1753640	And a society can do everything without government.
1753640	1756360	If you would be shutting down the government today,
1756360	1761600	it would be days before we notice and years before we crash.
1761600	1764960	But to get to the state in which society is today,
1764960	1768240	with streets and infrastructure and educational system
1768240	1770440	and so on, you need to have a government.
1770440	1773840	You're not going to bootstrap a group of people
1773840	1775960	into an organization without having
1775960	1777960	some kind of hierarchical organization that
1777960	1781040	makes this hoop of people coherent in their actions
1781040	1783880	and creates some next-level agent out of them.
1783880	1786200	And to do this, the government needs
1786200	1788720	to start out with some local coherence,
1788720	1791080	with some making itself coherent,
1791080	1793680	and then imposing some kind of organization
1793680	1795520	on the environment that is branching out
1795520	1799160	and scales over all the individual agents.
1799160	1802680	And if you just put individual people together for long enough,
1802680	1804520	then different forms of government
1804520	1807600	will emerge and some kind of evolutionary competition
1807600	1808440	between them.
1808440	1810720	And eventually, one of them will take over
1810720	1813480	and organize this group of people in such a way.
1813480	1815480	And the idea that the same thing could happen
1815480	1817200	among the neurons, that they're basically
1817200	1819520	different forms of organization that start out
1819520	1823480	from small cores and then move as activation patterns
1823480	1826000	that are agnostic of the individual units that they run on.
1826000	1828400	But they impose the same language on all of them,
1828400	1830400	similar capabilities on all of them,
1830400	1834760	so the locus of action can move around between them.
1834760	1836960	This idea that the brain organization could evolve
1836960	1840600	like this is similar to Gary Edelman's idea of neural
1840600	1843560	Darwinism, that basically our brain organization,
1843560	1846760	our mental organization is not hard-coded in the genome
1846760	1850240	as a blueprint, but what the genome contains
1850240	1852920	is the conditions to start this evolution
1852920	1855640	between different forms of organization.
1855640	1858520	And then rig the evolution in a particular way
1858520	1861400	so it converges quickly.
1861440	1865440	There are some ideas that we could take from biology
1865440	1867200	into technical systems.
1867200	1870080	And first of all, I think that the system
1870080	1871320	needs to be real-time.
1871320	1873640	It needs to be coupled to the environment
1873640	1876200	and needs to go into resonance with whatever environment
1876200	1877160	is coupled to.
1877160	1879560	And it needs to regulate the interaction
1879560	1883000	with that environment until at the level of coupling,
1883000	1885680	at the temporal resolution that it has,
1885680	1889680	is able to track reality around it.
1889680	1892160	And it's something that our machine learning models
1892160	1895640	are not doing yet and not real-time.
1895640	1898520	Even something like stable diffusion
1898520	1901520	is trained in digital images that are not happening in real-time.
1901520	1903320	They're not even happening in the right order.
1903320	1906360	There are just 800 million disconnected images,
1906360	1909160	or which the system is trying to find structure.
1909160	1911840	And this would not work for a logical organism.
1911840	1913200	I don't think that we could converge
1913200	1914680	from this amount of data.
1914680	1918120	Instead, what we get is a world that changes continuously
1918120	1919400	by small degrees.
1919440	1922920	And these small changes make sure that every frame is
1922920	1924720	related to the last frame.
1924720	1927720	And we can learn universal laws in which these frames are
1927720	1928280	related.
1928280	1930760	There are laws of conservation of information.
1930760	1932800	And without this conservation of information,
1932800	1935840	where we learn the transitions between adjacent frames,
1935840	1939360	I don't think that we would be able to learn from the universe.
1939360	1943440	So one thing is we change the paradigm from image to video
1943440	1946800	or other streaming data that has information preservation.
1946800	1949160	And in the beginning, this might look more difficult to us.
1949160	1951120	Isn't it harder to learn from video
1951120	1953440	than it is to learn from single pictures?
1953440	1956280	Well, what you want to learn is the fact
1956280	1958880	that you are living in the universe with moving objects
1958880	1960920	that happen in free space and so on.
1960920	1963200	It's actually much easier to learn this from video,
1963200	1966000	because it contains way more constraints than this way.
1966000	1968600	They're much more obvious.
1968600	1971760	The next thing is the way in which our brain is modeling
1971760	1974080	these things seems to be made from lots
1974080	1978560	of small periodic loops, small interlocking periodic loops.
1979400	1981960	And first of all, it has to be loops,
1981960	1984680	because the brain is relatively slow.
1984680	1988480	Information transmission in the brain
1988480	1992040	is so slow that it takes appreciable fraction
1992040	1996080	of a second for a signal to cross the entire neocortex.
1996080	1997960	And if you want to create simultaneity
1997960	2001280	between these different parts of the brain,
2001280	2002360	you're not going to get there.
2002360	2006320	It's right in our computers and our CPUs and so on.
2006600	2009400	Our GPUs, they make everything simultaneous
2009400	2014080	by exploiting the speed of the signal transmission
2014080	2016920	in our CPUs and GPUs.
2016920	2019680	It also means that we cannot increase the frequency
2019680	2021760	at which we run them arbitrarily,
2021760	2024920	or we cannot make the CPUs and GPUs arbitrarily large,
2024920	2027120	because it just takes so much time for a signal
2027120	2029720	to cross over the entire circuit.
2031520	2033360	The next step could be to use photonics.
2033360	2035400	So we can go to the speed of light
2035400	2037480	and make this system slightly faster
2037480	2038880	and slightly larger again,
2040000	2043400	while having coherence over the entire system.
2043400	2046040	But our biological systems don't have a chance of doing that.
2046040	2048560	They must live with the fact that it takes very long
2048560	2050120	for signals to get there.
2050120	2052040	And the way to deal with that is to make sure
2052040	2054560	that you're okay if you are out of sync.
2054560	2057120	You just need to be in the same phase.
2057120	2059800	Basically, you go at the same frequency
2059800	2061400	at different points of the brain,
2061400	2063840	and you make sure that the signal eventually gets there,
2063840	2066120	but it's okay if it's from the previous cycle
2066120	2069400	or two cycles ago, or several cycles ago,
2069400	2072400	as long as the content of the different brain areas
2072400	2074520	that only changes gradually.
2074520	2077680	If that happens, you're able to integrate over that.
2077680	2080360	You can use predictive algorithms and so on,
2080360	2082040	and can synchronize the whole thing.
2082040	2084440	So basically, this idea of slow oscillators
2084440	2087440	is something that we could translate into digital systems.
2089240	2091440	The next thing is this emergent management,
2091440	2092640	and here we'll Darwinism.
2092640	2095720	So instead of having a particular circuit
2095720	2098920	that is required to be like this,
2098920	2102080	let's evolve this computational operators that we need.
2102880	2105360	And the next one is that many of the functions
2105360	2109040	that the brain discovering are only discovered once.
2109040	2112400	And this is tool for simple functions,
2112400	2115360	like addition, integration, multiplication,
2115360	2117680	simple computational primitives, rotation,
2117680	2120600	that are being used for many, many mathematical primitives
2120600	2123160	that we require to describe the geometry of sound,
2123160	2125280	of images, of thought.
2125280	2127040	And this basic arithmetic,
2127040	2129240	this basic library of computational functions
2129240	2131920	at the moment to train this into a neural network
2131920	2133600	takes a very long time, right?
2133600	2136040	Despite the neural network being built over addition
2136040	2137480	and multiplication,
2137480	2141080	it's not easy for a neural network to learn arithmetic.
2141080	2142640	It's possible to do it,
2142640	2145400	but it takes enormous amount of training data.
2145400	2147560	And in every context, locally,
2147560	2151240	the neural network is performing the same arithmetic
2151240	2152680	over and over again.
2152680	2155240	It has to retrain these functions
2155240	2159280	into a different region of the network again.
2159280	2163200	And basically getting all these different computational
2163200	2165920	primitives bootstrapped into the neural network
2165920	2168040	is something that is hard.
2168040	2170200	And it has an interesting effect.
2170200	2173800	You can, for instance, train a neural network
2173800	2175320	on audio input,
2175960	2178760	and then use it to discover structure and vision.
2178760	2180240	And it's going to be much faster
2180240	2183360	because the audio input already prepares the neural network
2183360	2185760	to learn many of the computational primitives,
2185760	2188480	basic arithmetic in many parts of the network,
2188480	2191480	that it can then adapt for a new task.
2191480	2193360	And the way in which our brain is doing this
2193360	2195640	is probably that it learns some useful functions
2195640	2197720	and then these neurons have a way
2197720	2200960	to exchange these functions possibly via RNA.
2200960	2204640	And so basically the functions that are being computed
2204640	2208040	are to some degree agnostic to the individual neuron.
2208040	2209560	They are somewhat substrate independent.
2209560	2210960	They're just migrate to.
2210960	2213480	So a different paradigm might be instead
2213480	2215800	of using local functions over the neighborhood
2215800	2216640	of every neuron.
2216640	2219400	And every neuron is learning its own set of functions.
2219400	2221600	You learn a set of global functions
2221600	2224560	and every neuron is deciding which ones of those to use.
2227000	2229520	And it's also something that has almost never been tried
2229520	2233200	in AI and that what I'd like to get seen.
2234200	2238360	Another thing is that the main focus is on reward.
2238360	2240160	What you try to do in the brain is
2240160	2245040	to do the most useful thing with fixed resources.
2245040	2248120	And this means you have to assess the global reward
2248120	2251960	that the organism is getting out of the distributions
2251960	2253120	of all the neurons.
2253120	2256480	And then you need to distribute this reward
2256480	2259000	among all the neurons that contribute to the result.
2259000	2262480	This is similar to what you do in a corporation.
2262520	2264640	It's an economic problem, but the corporation
2264640	2266640	tries to do the most valuable thing
2266640	2269080	that it can do with all the employees that it has.
2269080	2270840	And to do this, it needs to get rewards
2270840	2273160	to all the individual employees.
2273160	2275680	And the rewards are not given in such a way
2275680	2279440	that every employee gets a different amount of money.
2279440	2281840	And the one that has the biggest contribution
2281840	2284800	to the bottom line by the others are also important
2284800	2286280	gets much, much more.
2286280	2289000	There is some degree of this, but it's mostly depending
2289000	2291680	on the negotiation power of individual employees
2291800	2292640	on the market.
2293480	2296360	If there was no fungibility
2296360	2298280	and you would need to train all your employees,
2298280	2300520	it would make sense to give them all the same amount
2300520	2301640	of money, right?
2301640	2304040	And in the liberal capitalism, it doesn't make sense
2304040	2309040	to give a good retail worker the same amount of salary
2309680	2312840	than it does to give money to a good manager.
2312840	2316160	But if you need retail workers, you will have to employ one.
2316160	2319240	And the reason why retail workers are less than managers
2319240	2321320	is mostly because there are many more retail workers
2321760	2324560	competing for positions than there are managers
2324560	2326040	competing for positions.
2326040	2330080	So the supply and demand regulates labor market
2330080	2331680	in such a way.
2331680	2333800	But this is not true in the biological system.
2333800	2336360	Every new one basically is going to consume
2336360	2338920	the same amount of resources just for existing
2338920	2340960	and being ready to do something.
2340960	2343120	So our reward here is different.
2343120	2345240	It's not being accumulated in the bank account
2345240	2346480	of the new one.
2346480	2350120	Instead, this is just a signal that tells the organism
2350120	2351840	that this new one is still going to get fat
2351840	2354120	because it's useful in what it does.
2354120	2356120	And the new one needs to get feedback similar
2356120	2357960	to the feedback that you get from your colleagues.
2357960	2360160	That's the actual reward that tells you
2360160	2362800	you're doing the right thing, right?
2362800	2366000	So it needs to be some kind of communicative reward,
2366000	2368680	some messages that are not directly food,
2368680	2370640	but that are more anticipated reward
2370640	2374800	that are like money only without accumulation.
2374800	2378080	And so there's basically going to be a reward-driven language
2378080	2381720	and who is distributing all the reward in the brain?
2381720	2385440	Well, all the cells are with all the neurons mostly,
2385440	2387560	but also the other cells, maybe glia cells,
2387560	2388960	and so on that contribute
2388960	2390840	and the distribution of the rewards.
2390840	2392760	And so what's happening in the biological system
2392760	2395040	is that it involves a reward language.
2395040	2398000	There are two types of signals that are being sent around.
2398000	2400000	One is the results of the computation,
2400000	2401560	which is read by other neurons
2401560	2403760	based on what kind of activation they're interested
2403760	2405720	in filtering out of the environment.
2405720	2408000	And the other one is going to be signals
2408000	2411000	that amount to reward and punishment
2411000	2412600	that basically tell other neurons
2412600	2414000	whether they should do more or less
2414000	2415320	of a certain computation.
2417440	2422200	And so basically reading of information is going to be pulled.
2422200	2424680	You draw information from the environment
2424680	2426720	and reward is going to be pushed
2426720	2427800	because you should not be able
2427800	2431120	to escape a negative reward, a punishment and so on, right?
2432160	2435240	And I think we could approximate this
2435240	2437800	using a new paradigm and a number of experiments
2437800	2439600	that I would like to do in this regard.
2439600	2443040	So basically a neuron has an internal state vector
2443040	2446360	that contains the slight history of the neurons
2446360	2448440	over the last few activations that is read
2448440	2450760	and the type of the neuron and so on.
2450760	2452120	And it has a selector function.
2452120	2453800	The selector function is basically
2453800	2456600	defining the receptive field of the neuron.
2456600	2458080	And the receptive field of the neuron
2458080	2460600	can be just the environment of the neuron
2461480	2463240	interpreted as a certain topology.
2463240	2465800	So basically it looks at its neighborhood
2465800	2469440	as if it was a space with a certain number of dimensions.
2469440	2471080	And how can this be?
2471080	2475280	The neocortex is two-dimensional or two and a half-dimensional
2475280	2476720	but it has a number of layers,
2476720	2478880	that number of layers being very small
2478880	2481240	and it's a large 2D area
2481240	2483280	subdivided into different regions.
2483280	2487320	Well, it turns out that you can interpret a 2D area
2487320	2489160	as something that is in higher dimension
2489160	2491160	in the same way as you can take the linear
2491160	2493120	or one-dimensional address space of your computer
2493120	2495600	and interpret it as a two-dimensional map
2495600	2497920	or as a three-dimensional space
2497920	2500280	or as something that happens in eight dimensions
2500280	2502320	and can perform operations on it.
2502320	2504040	And this is what the selector function does.
2504040	2505880	The selector function is basically interpreting
2505880	2508640	the environment of the individual cell
2508640	2511520	as a space that contains information
2511520	2513560	in a certain arrangement.
2513560	2515280	And it doesn't need to be a regular space.
2515280	2516360	It can be a manifold.
2516360	2518000	It can be something that is very selective.
2518000	2520920	It can be something that only uses five neighbors.
2520920	2524200	And these neighbors can even be physically very distant.
2524240	2526120	And so this neuron could be a juncture
2526120	2528560	or some kind of hub that sends information locally
2528560	2529920	into the network and so on.
2529920	2532360	So you're going to have some neurons that have a topology
2532360	2534560	that allow long distance connectivity
2534560	2539560	and others that performs local maps and 2D or 3D
2539880	2541880	and perform functions on them.
2541880	2543840	And this next to the selector function,
2543840	2545240	you have the modifier functions.
2545240	2548120	The modifier functions tell the individual neuron
2548120	2551200	how it should change its state based on its own state
2551200	2553800	and the activation that it reads in the environment.
2554640	2557360	By having some history in its own state,
2557360	2561320	it's able to respond to a spatiotemporal activation
2561320	2562880	distribution in its environment.
2564080	2566600	And it's the use this idea
2566600	2569200	that the neuron can use global functions.
2569200	2573240	It means that we can arrange many neurons densely enough
2573240	2574600	in some kind of lattice
2574600	2576680	and these functions can arrange themselves
2576680	2578240	as they need to be arranged.
2578240	2580640	And they can shift around as they have to
2580640	2583240	and duplicate themselves if they have to.
2583280	2586000	Yasha, just in the interest of time.
2586000	2588040	Yes, I'm basically done.
2588040	2589960	Thank you for reminding me.
2589960	2594960	So this selector function, modifier function paradigm
2595200	2598360	allows us to come up with a new way
2598360	2603000	of describing a function approximation beyond deep learning.
2603000	2605360	And at the moment, the search space
2605360	2607480	in my experiments is way too large.
2607480	2609840	So basically there are too many ways
2609840	2612000	in which this could be implemented
2612000	2613800	from my current perspective
2613800	2617200	to get this converge to a good solution.
2617200	2620000	The alternative is I can just handcraft a solution
2620000	2621560	but it might not be optimal.
2621560	2624200	But it's something that I would like to definitely look more
2624200	2625840	into in the future.
2625840	2628600	And not just me, I suspect that many people
2628600	2631120	are currently discovering such ideas
2631120	2634000	and will be working on in the future.
2634000	2636040	And while it's not clear
2636040	2639120	that this can provide a viable alternative to deep learning
2639120	2641520	that is much faster and converging faster
2641520	2645200	and more efficiently than the present deep learning systems.
2645200	2647240	This is something that I believe is closer
2647240	2649600	to what biology has discovered.
2649600	2651280	And that scales up very reliably
2651280	2653480	over many, many classes of algorithms.
2655120	2656280	Okay, that's it from me.
2657200	2660880	All right, we would like to have a quick one
2660880	2663080	to two minute break right now
2663080	2668080	so that the panelists can set up their presentation.
2668320	2673320	We will do Michael Levin's presentation next.
2674440	2676960	So in the meantime, Yosha,
2676960	2681440	let me ask you some questions that's got posted
2681440	2682680	while you're talking.
2684720	2687120	So a question from Nikolai,
2687120	2689920	like how neurons are small organisms
2689920	2692440	that operate in the emerging super organism
2692440	2693280	that is a human,
2693280	2697360	would future AGI architecture need to be made up
2697360	2699240	of many smaller AIs?
2701600	2704640	So there is no centralized control in the brain
2704640	2708080	in the sense that all the centralized control
2708080	2711680	is emergent over all the organizations between the neurons.
2711680	2713200	There is no dedicated CPU
2713200	2717560	that is able to process every neuron and update its state.
2717560	2721040	Every state update happens locally in the individual cells.
2721040	2723280	But this is an engineering constraint
2723280	2726240	that doesn't exist in the technical systems.
2726240	2728120	And it's not clear yet to me
2728120	2731800	to which degree we need to have local control
2731800	2733360	to make it happen.
2733360	2735920	At the moment, the machine learning of organisms
2735920	2740360	that we are using are treating the individual nodes
2740360	2742680	in the network just as memory.
2742680	2746120	And the updates are done by a centralized algorithm
2746120	2748840	that is updating all this memory.
2748840	2752360	All right, and you either have a CPU
2752360	2754000	that is reading and writing,
2754000	2758320	or you have lots of local CPUs in the GPU
2758320	2761600	that is doing this with multiple pipelines in parallel.
2761600	2765440	And there are biologically inspired chips
2766440	2768320	that are mostly experimental at the moment,
2768320	2769880	like Intel's Lohi,
2769880	2773240	that are using many, many very small simple CPUs
2773240	2774080	that are doing this.
2774080	2776240	And it's not clear if the best solution
2776240	2778080	is to have a CPU for every memory cell.
2778080	2779840	It's probably not the case.
2779840	2781880	So there are some things that you can do
2781880	2786280	in the technical systems that informing conformance
2786280	2787640	to a centralized algorithm,
2787640	2789440	to centralized specifications
2789440	2791960	that the technical system is going to implement.
2791960	2795640	And in a biological system, this is just not feasible
2795640	2798240	because there is no such centralized authority,
2798240	2802360	no engineer who can make nature behave by itself.
2802360	2805400	But if you want to think about how to build a mind
2805400	2807160	from a biological perspective,
2807160	2808760	we have to think about how to build something
2808760	2811200	that wants to grow into a mind.
2811200	2813200	And we can take some of these ideas.
2813200	2815440	It's not clear that we need to make this
2815440	2817440	with completely local control only.
2817440	2819080	Maybe it's more efficient to have a mixture
2819080	2821640	of some local control and a lot of global control
2821640	2824960	where we already know what the control is going to be.
2824960	2826640	I think Mike is ready.
2826640	2827880	Yes, excellent.
2827880	2830000	Michael, the floor is now yours.
2830000	2830840	Great.
2830840	2832880	Okay, well, that was extremely interesting.
2832880	2835320	So let me see what I can add here.
2836320	2838480	I've got some slides.
2838480	2839320	So here we go.
2839320	2840920	Hopefully you can see that.
2840960	2844440	So what I would like to talk about,
2844440	2846560	of course there's this idea that biology
2846560	2849040	should be an inspiration for AI.
2849040	2852120	And what I would like to do is to deconstruct
2852120	2854480	some of the biology that people typically think about
2854480	2858600	in these contexts and ditch a lot of things
2858600	2860640	that are very common, binary categories
2860640	2863960	and a focus on brains, a focus on neurons,
2863960	2865280	a focus on humans.
2865280	2867080	I want to step away from all of that
2867080	2869280	and rebuild a different framework
2869280	2874040	that I think has many, many implications for AI.
2874040	2876200	So the first thing I want to talk about
2876200	2879400	is this idea of a typical human.
2879400	2882160	So there's this kind of classic idea
2882160	2883560	that we know what a human is
2883560	2887600	and it certainly works for practical purposes in society.
2887600	2891120	But the idea is sort of pre-scientific
2891120	2892880	and it's still, many people are still,
2892880	2895160	even scientists are often still caught up in this,
2895160	2897040	the idea that, okay, so we have these humans
2897080	2899520	and they are discrete natural kind
2899520	2901320	and they're different from other animals.
2901320	2905240	And so there's this, here's Adam naming the other animals
2905240	2909280	and so there's the discrete species and so on.
2909280	2913760	But if we take developmental biology and evolution
2913760	2916560	and synthetic biology and bioengineering,
2916560	2918360	if we take these things seriously,
2918360	2920240	then what we find out is that actually
2920240	2923320	there are no such natural kinds because all of this,
2923320	2925520	both on the evolutionary time scale
2925520	2927360	and the developmental time scale
2927360	2930040	and now in terms of the technological time scale,
2930040	2935200	there are very gradual, very small, very slow changes
2935200	2937680	that go all the way back from what people think of
2937680	2940400	as a typical human and their intelligence,
2940400	2943720	all the way back to very different types of organisms.
2943720	2946320	And developmental biology and of course evolution too
2946320	2949640	offers absolutely no place to put a sharp line
2949640	2953040	and say this creature was not,
2953120	2955560	pick an adjective, intelligent, cognitive, conscious,
2955560	2957200	well, whatever you like, pick an adjective
2957200	2959040	to say this creature was not it,
2959040	2962040	but it had some offspring and the offspring now are, right?
2962040	2964080	That just doesn't exist
2964080	2966400	because all of these changes are very slow
2966400	2967680	and very continuous.
2967680	2971880	And so there were changes during evolution,
2971880	2974760	we all start life as a single cell.
2974760	2977040	In the future, there will be all kinds of changes
2977040	2979640	to our bodies with biological
2979640	2981840	and engineered kinds of devices.
2981840	2983840	And so all of these are continua
2983840	2987400	of really novel types of embodiments.
2987400	2992400	And we build certain kinds of conceptual metaphors
2993080	2996520	that try to distinguish different categories here,
2996520	2999360	but these are discrete tools,
2999360	3001680	the phenomena themselves are deeply continuous
3001680	3002680	on multi-scale.
3002680	3005040	And to give you just a simple idea
3005040	3007320	and then we'll enlarge on this is this.
3007320	3012320	And so this caterpillar is a kind of soft-bodied robot
3012320	3014200	that lives in a two-dimensional world
3014200	3015680	that crawls around on leaves.
3015680	3018120	It likes to chew plants and it has this brain
3018120	3020480	that's very, very suitable for this purpose.
3020480	3022920	What it needs to do is turn into this creature,
3022920	3024280	which is completely different.
3024280	3026000	It lives in the three-dimensional world,
3026000	3027960	it doesn't care about the leaves at all,
3027960	3030800	it wants nectar and it flies and it does various things.
3030800	3034160	And so during this process, there is a metamorphosis
3034160	3036280	where not on an evolutionary time scale,
3036320	3038400	during the lifetime of the individual,
3038400	3040520	the brain is basically dissociated
3040520	3042960	and rebuilt into a new architecture.
3042960	3047960	And by the way, there are data that memories persist.
3047960	3049280	So if you train the caterpillar,
3049280	3051200	the butterfly or moth still remembers
3051200	3052480	the original information,
3052480	3054000	but you can sort of think about,
3055480	3057560	nevermind the question of what's it like to be a butterfly,
3057560	3059160	what's it like to be a caterpillar
3059160	3061400	changing into a butterfly, right?
3062400	3066920	That process of slow, but drastic change
3066920	3068440	in your embodiment.
3068440	3070840	And so from here, we can just remembering
3070840	3072400	that we are all made of parts
3072400	3076080	that can modify during our lifetime.
3076080	3077640	We can ask some interesting questions.
3077640	3079600	For example, you look at a brain
3079600	3082640	and we're sort of conditioned to expect that it's obvious
3082640	3087480	that a brain contains one human worth of intelligence.
3087480	3089240	But this is just because we're used to that
3089240	3090360	in terms of our interactions.
3090360	3091560	If I showed you a brain
3091560	3094120	and you didn't know what this was
3094120	3096800	and I asked you how many different cells are in there,
3096800	3097720	you would actually have,
3097720	3100000	we have no ability to answer that question.
3100000	3103200	We have no way to ask how much,
3103200	3105320	and I think Yosha got to some of this,
3105320	3107480	how much of this real estate is necessary
3107480	3111040	for one human's worth of performance?
3111040	3114200	We have no idea how much is actually in there.
3114200	3116840	And actually, very interestingly,
3116840	3120920	the same issue occurs in embryonic development.
3120920	3123920	So we all begin as a cellular blastoderm.
3123920	3126920	So this is a sheet, a two-dimensional sheet of cells.
3126920	3129720	And that sheet turns into an embryo.
3129720	3130600	Now, what does that mean?
3130600	3132200	First of all, can we guess in advance
3132200	3134640	how many embryos are going to come from that sheet?
3134640	3137160	Actually, we cannot, and I'll show you why.
3137160	3138440	And it's not genetics.
3138440	3141440	And then there's the question of what are we actually counting?
3141440	3143600	When we count an embryo, I mean, there's 50,000 cells,
3143600	3145520	let's say here, what is it that we're counting
3145520	3146680	when we say there's an embryo?
3146680	3148320	What are we actually counting when we say there's
3148320	3151880	a single human inhabitant in this bunch of tissue?
3151880	3154880	So one of the things that you can do in embryogenesis
3154880	3157600	is you take this blastoderm and you take a little needle
3157600	3162600	and you put some kind of scratches into this blastoderm.
3163160	3164880	And then they heal, but before they heal,
3164880	3167440	what will happen is that each of these regions
3167440	3170240	being isolated from the other regions
3170240	3171760	decides to organize an embryo
3171760	3174760	because they don't know for a while anyway
3174760	3175880	that the other regions are there.
3175880	3178600	Then when it heals up, it becomes conjoined twins.
3178600	3180520	And you can do this very easily in chicken and duck
3180520	3183320	and other embryos, but humans work exactly the same way.
3183320	3185840	And so then there will be multiple embryos
3185840	3187560	within the same blastoderm.
3187560	3189320	And then there will be some disputed zones here.
3189320	3190680	There's some cells that aren't quite sure
3190680	3192080	which one they belong to.
3192080	3195760	But this deep idea of individuation,
3195760	3198560	of taking some kind of a continuous,
3198560	3200000	in fact, it's even worse than continuous
3200000	3202520	because it's multi-scale substrate
3202520	3206800	and having itself organized into discrete what?
3206800	3207920	So in the case of embryos,
3207920	3210520	what you have are discrete groups of cells
3210520	3213880	that are trying to follow anatomical goals.
3213880	3216080	They're trying to achieve particular walks
3216080	3217440	in anatomical space.
3217440	3219520	They're gonna construct the right number of fingers,
3219520	3222600	the right number of eyes, whatever it is.
3222600	3225360	Same thing in cognitive development.
3225360	3228880	There are issues, there are disorders of individuation
3228880	3230640	that you see in split brain patients
3230640	3232320	and dissociations and so on.
3232320	3234200	So this question of how many are in there
3234200	3237720	is deeply interesting and it gets to the bottom
3237720	3240920	of what it means to be a coherent agent
3240920	3242800	when you're made of parts.
3242800	3245560	And I think Alan Turing, although as far as I can tell,
3245560	3247120	he didn't write directly about this.
3247120	3249320	I think he was well aware of this issue
3249320	3251760	because of course he was interested in intelligence
3251760	3255880	and generic embodiment and so on.
3255880	3257760	But he was also interested in morphogenesis.
3257760	3261040	He wrote this paper on biological morphogenesis.
3261040	3263160	And I think he understood that these are deeply
3263160	3264720	and profoundly the same problem.
3264720	3265960	The problem of morphogenesis
3265960	3268560	and the problem of the mind are the same problem
3268560	3272920	because of this emphasis on emerging
3272920	3275520	as a coherent entity from multiple parts.
3275520	3278320	So people often talk about, well,
3278320	3281000	ants and termites are some kind of collective intelligence
3281000	3283320	and we can argue about what that means,
3283320	3287120	but we are really a unified, a centralized intelligence.
3287120	3289840	We're not like bird flocks or ant colonies.
3289840	3290920	But actually, of course,
3290920	3293600	all biological systems are made of parts.
3293600	3296920	And so we too are a kind of collective intelligence.
3296920	3298920	What's interesting is the scaling interface
3298920	3302600	is what is it that allows these individual subunits
3302600	3305920	to work together and present to other intelligences
3305920	3308760	to themselves, by the way, and to the environment
3308760	3311960	in a picture of a coherent agent.
3311960	3314280	So this is the journey that we all took.
3314280	3317040	We began life as a piece of physics.
3317040	3318880	So basically as a quiescent oocyte.
3318880	3321640	So it's a blob of chemicals, not doing terribly much.
3321640	3323840	And then through this incredibly,
3323840	3326680	just magical process of embryonic development,
3326680	3329280	that we arrive at something like this,
3329280	3332440	which is a complex organism with metacognitive capacity
3332440	3333520	that's going to make statements
3333520	3335720	about how we're not just machines
3335720	3339040	and we're different than physics and all that.
3339040	3342560	But this whole process is extremely smooth and gradual.
3342560	3344080	It happens second by second.
3344080	3346000	There is no lightning flash
3346000	3347920	at which point physics becomes mind.
3347920	3349320	It's a gradual process.
3349320	3352240	And we can talk about face transitions and such,
3352240	3356080	but there's really not that much evidence
3356080	3356920	for any of that.
3356920	3358840	It's a very continuous process.
3358840	3360200	So this is the kind of thing.
3360200	3363080	So I think, Yosha alluded to this a few times.
3363080	3364160	This is the sort of thing.
3364160	3365160	I mean, not exactly this.
3365160	3368040	This is a lacrimaria, it's a free living organism.
3368040	3369760	But here's a single cell, right?
3369760	3371080	This is what we are made of.
3371080	3373440	These guys, there's no brain.
3373440	3376080	Here's those, there's no nervous system.
3376240	3379160	This is the single cell creature in real time
3379160	3382440	using all of the intelligence of its chemical networks.
3382440	3383400	And we can talk about this.
3383400	3385800	I mean, that quite literally chemical networks can learn
3385800	3388120	and they can do inference and many other things.
3389120	3390560	Even though it's a single organism
3390560	3393960	is handling all of its single cell agendas
3393960	3395360	in its environment.
3395360	3399240	So metabolically, physiologically, anatomically,
3399240	3402120	it's doing what it needs to do.
3402120	3405440	And so we are made of extremely competent parts.
3405440	3406360	Here's another example.
3406360	3409200	This is a, this whole thing, you'll see, you'll see this.
3409200	3410160	I'm gonna pause it.
3410160	3411720	Whoops.
3411720	3412560	I'm gonna pause this.
3412560	3413600	This whole thing right here,
3413600	3415960	this is called Pfizer and polycephalum.
3415960	3417200	It's a slime mold.
3417200	3419160	The whole thing is one cell, okay?
3419160	3421400	And what it's, what I'm showing you here
3421400	3423240	is that it's sitting in this environment.
3423240	3424840	These are three glass discs.
3424840	3426560	These are very, very light.
3426560	3427960	There's no chemicals, there's no food.
3427960	3429440	It's just glass, inner glass.
3429440	3431040	There's one glass disc here.
3431040	3432880	And what it's going to do is,
3432880	3437240	it's going to, for the first few hours,
3437240	3440440	it's going to just generically grow in all directions here.
3440440	3441920	What it's doing during this process
3441920	3443680	is it's tugging on its substrate
3443680	3446080	and feeling the vibrations that gets back.
3446080	3448360	And it can sense the strain angle
3448360	3449840	of the objects in its environment.
3449840	3451600	And then we'll eventually reliably grow out
3451600	3452840	to the heavier mass.
3452840	3454600	But during this, so that'll happen at this point,
3454600	3457440	but during this time is when it's processing
3457440	3460240	that information and learning from its environment.
3460240	3462440	And then boom, now the behavior begins.
3462560	3464400	So single cells are very competent,
3464400	3467400	even microbial single cells.
3467400	3470160	And so what we have to understand is that biology,
3470160	3472600	so here's a principle that I think is really important
3472600	3476800	for future AI, biology is deeply nested.
3476800	3478960	That is not merely structurally,
3478960	3480960	I mean, that's obvious we're made of organs, tissues,
3480960	3484240	and so on, but each layer is competent.
3484240	3486600	It solves problems in its own space.
3486600	3489520	All of these things from molecular networks
3489520	3492280	all the way up to whole organs and beyond
3492280	3495400	are solving specific problems in specific spaces.
3495400	3496920	So we are really interested in my group,
3496920	3500400	we're really interested in creating a framework
3500400	3504120	that allows us to relate to really
3504120	3505640	very diverse intelligences.
3505640	3508840	So, of course, familiar creatures,
3508840	3510400	all kinds of weird biologicals,
3510400	3513560	colonial organisms, swarms, of course new,
3513560	3515520	and I'll show you some in a couple of minutes,
3515520	3518680	new engineered creatures, artificial intelligences,
3518680	3521520	and maybe at some point exobiological,
3521520	3522680	truly alien agents.
3522680	3524200	We need to be able to deal with all of this.
3524200	3527120	It's not enough to deal with crows and monkeys
3527120	3530000	and then maybe octopus, that's way too narrow.
3530000	3532520	And so, of course, this is an idea
3532520	3534240	that has been addressed before.
3534240	3538240	So here's Wiener and colleagues trying to come up
3538240	3540560	with a very sort of cybernetic way
3540560	3543480	to classify different degrees of behavior
3543480	3545680	all the way from passive mechanical behavior
3545680	3547760	up to complex cognition
3547800	3552200	in a way that abstracts from its familiar embodiments.
3552200	3553800	So there's no talk of brains or neurons
3553800	3554640	or anything like that.
3554640	3557040	This is very sort of functionalist.
3557040	3560840	And one thing about us as humans
3560840	3565840	is that we are very primed to recognize intelligence
3567280	3568720	in the three-dimensional space.
3568720	3571800	So basically, medium-sized objects moving at medium speeds
3571800	3572960	through three-dimensional space.
3572960	3574800	When we see it, we know what agency looks like,
3574800	3576720	we know what intelligence looks like.
3576720	3578600	But we are really bad at,
3578600	3580560	and this is why we must get better at it,
3580560	3583880	recognizing intelligence in other types of problem spaces.
3583880	3587080	So imagine if you had a direct feeling
3587080	3589160	of all of your blood chemistry.
3589160	3591040	If you were able to feel your blood chemistry
3591040	3593680	the way that you can see objects in three-dimensional space,
3593680	3596120	you would be very obvious
3596120	3597920	that your kidneys, your liver, and so on
3597920	3599960	have a degree of intelligence
3599960	3602640	and they're doing amazing things in their problem spaces.
3602640	3604920	So we study how individual cells
3604920	3607520	navigate the space of gene expression,
3607520	3610400	hopefully the physiology and morpho space,
3610400	3611720	the space of patterns.
3611720	3614360	This is what I'm talking about today.
3614360	3615400	And just for a few minutes here,
3615400	3618520	here's an example of cells solving
3618520	3621600	an entirely novel problem in genetic space.
3621600	3624040	So here's a planarian, this is a flatworm.
3624040	3626760	They regenerate parts of their body when amputated.
3626760	3628720	What we did was we exposed planaria
3628720	3630080	to a solution of barium.
3630080	3632720	Barium blocks all of their potassium channels,
3632720	3635400	the cells and the neurons are really unhappy.
3635400	3637960	Their heads explode, literally just explode.
3638960	3641160	Over the next week or so,
3641160	3643560	they rebuild, keeping them in the barium,
3643560	3645280	they rebuild a brand new head.
3645280	3647040	The new head doesn't care about barium at all.
3647040	3648600	So we asked the simple question, how can that be?
3648600	3649960	What is the new head doing
3649960	3651600	that the original head couldn't do?
3651600	3654080	And we found out that there's actually very few genes
3654080	3656240	that the system up and down regulated
3656240	3659320	to be able to do its business in the presence of barium.
3659320	3662520	The kicker is, planaria never get exposed to barium
3663400	3664480	in the real world.
3664480	3667240	There is no ecological precedent for this.
3667240	3669880	So just imagine, you're a cell,
3669880	3672280	you've got, I don't know, tens of thousands
3672280	3675000	of possible genes, you've got a disaster,
3675000	3676720	a physiological disaster.
3676720	3679080	You don't have time to try every combination.
3679080	3682320	There is no time to try everything
3682320	3683880	and whoever survives, survives.
3683880	3685960	These cells don't turn over that fast.
3685960	3687840	You have to solve this novel problem,
3687840	3689800	possibly by generalizing,
3689800	3691480	because you've never seen barium before,
3691480	3693160	but you have seen epilepsy before.
3693160	3695600	And barium excitability might look a little bit
3695600	3696440	like epilepsy.
3696440	3698200	And so maybe you can do some of the same things.
3698200	3700760	So this idea of solving novel problems
3700760	3704000	in physiological space is one example
3704000	3705480	of what biology can do.
3705480	3706840	But here's another example.
3706840	3708720	So this is how we all start
3708720	3711400	as a kind of a collection of early cells.
3711400	3714160	But this is a cross-section through a human torso.
3714160	3716120	Now look at the incredible order here, right?
3716120	3719560	All the tissues, the organs, everything is in the right place,
3719560	3722560	the right size and shape and relative to each other.
3722560	3724560	Where does that come from?
3724560	3726160	You might be tempted to say DNA,
3726160	3728160	but of course we can read genomes now
3728160	3729920	and what's in the DNA isn't any of that.
3729920	3732360	What's in the DNA is the sequence
3732360	3734600	of the micro level sort of hardware
3734600	3736200	that every cell gets to have, the proteins.
3736200	3737680	That's what the DNA specified.
3737680	3740440	So you really, you still need to understand the physiology
3740440	3743640	by which these cells compute what to do here.
3743640	3745080	And then there are lots of questions,
3745080	3746640	as regenerative medicine workers,
3746640	3749040	we try to figure out what do we say to these cells
3749040	3751720	to rebuild pieces that are missing?
3751720	3754120	And as engineers, we want to know what's actually possible?
3754120	3755440	What can you reprogram this?
3755440	3757040	Can you make them do something else?
3757040	3758600	So the amazing thing about development
3758600	3762200	is that while it is incredibly reliable and robust
3762200	3764360	and in fact hides all of its intelligence from us
3764360	3767000	when we see acorns giving rights to oak trees
3767000	3769560	and frog eggs make frogs, we sort of assume,
3769560	3770520	well, what else is it gonna do?
3770520	3771440	Like that's obvious, right?
3771440	3772800	That's how it has to be.
3772800	3775480	But that's only what happens on the default condition.
3775480	3777040	What we find out is that, for example,
3777040	3779480	if you take an early embryo and cut it in half,
3779480	3780840	you don't get two half bodies.
3780840	3783320	You get two perfectly normal monosygotic twins.
3783320	3785480	And in fact, more generally,
3785480	3789120	the process of development can navigate this anatomical space
3789120	3790880	in a way to reach the same goal
3790880	3792680	from different starting positions
3792680	3795120	despite really drastic perturbations
3795120	3797040	by taking different paths.
3797040	3799480	It's not just a hardwired set of emergent.
3799480	3800640	This is not about emergence.
3800640	3802680	Of course, complex things emerge from simple rules.
3802680	3803800	This isn't that at all.
3803800	3806720	This is the ability of the system to get to its goal
3806720	3809520	despite really, really radical changes.
3809520	3811040	So here's one change, here's another change.
3811040	3814840	As an adult, some organisms like the salamander,
3814840	3817520	they regenerate their eyes, their limbs, their jaws,
3817520	3821360	their tails, you can make cuts anywhere you like along here.
3821360	3823920	And these cells will very rapidly grow
3823920	3827720	and undergo morphogenesis, and then they will stop.
3827720	3828840	When do they stop?
3828840	3831920	They stop when a correct salamander limb has formed.
3831920	3833080	Doesn't matter where you cut it,
3833080	3834920	it will only grow exactly the right amount
3834960	3838120	and it will stop when exactly the right thing has formed.
3838120	3840640	So you've got some sort of error minimization scheme
3840640	3841480	going on here.
3841480	3842640	It knows exactly what it looks like.
3842640	3845240	It knows what the target state is.
3845240	3848320	And in fact, this is something that we discovered
3848320	3851040	that, so this is a tadpole here, some eyes,
3851040	3852840	here's the brain, the gut, the nostrils.
3852840	3855640	These tadpoles have to become frogs.
3855640	3857680	In order to become frogs, they have to rearrange their face.
3857680	3859480	The jaws have to move, the nostrils have to,
3859480	3860680	everything has to move.
3860680	3862360	We find, and so you might imagine
3862360	3866720	that this is some sort of hardwired set of emergent outcomes
3866720	3868200	where every organ gets displaced
3868200	3870440	to its appropriate distance and direction.
3870440	3872840	So we made what's called Picasso tadpoles.
3872840	3874680	Basically, we scrambled everything
3874680	3876040	so that everything's in the wrong place.
3876040	3877480	The eyes are off to the side of the head,
3877480	3878960	the jaws are on the other side of everything
3878960	3880200	is just scrambled.
3880200	3881560	Because we have this hypothesis
3881560	3885680	that this is more intelligent than people gave a credit for.
3885680	3890000	Sure enough, what these guys do is every structure moves
3890040	3892320	in novel paths and keeps moving,
3892320	3893680	no matter where it started from,
3893680	3896680	until it gets to be a pretty normal looking frog.
3896680	3899080	So what the genetics gives you is not a piece of hardware
3899080	3900720	that does the same thing all the time.
3900720	3904920	It gives you a machine that can recognize unexpected changes
3904920	3906880	and take corrective action as needed
3906880	3908520	to get to the same goal.
3908520	3911240	The most amazing part of this is that in doing this,
3911240	3913560	and this is an example of top-down causation,
3913560	3916360	which is why it's really important to understand this,
3916560	3918720	how high level goals filter down
3918720	3921360	to the sort of implementation machinery,
3921360	3924400	is that what you see is that this is an example
3924400	3927480	from the kidney tubule of a nut.
3927480	3928960	If you take it in cross-section,
3928960	3932000	normally there's, I don't know, eight or nine cells
3932000	3935000	that work together to make the lumen of that tubule.
3935000	3938000	But one thing you can do is you can force these cells
3938000	3939160	to be gigantic.
3939160	3941400	And when you do this, when you make them larger,
3941400	3943480	fewer cells will do this,
3943480	3946000	forming exactly the same lumen diameter
3946000	3947960	until you make the cells so large
3947960	3950600	that a single cell will wrap around itself
3950600	3953680	to give you the same structure.
3953680	3955520	What's amazing about that is that these are completely
3955520	3957040	different molecular mechanisms.
3957040	3958760	This is cell-to-cell communication.
3958760	3960360	This is cytoskeletal bending.
3960360	3962920	So in the service of a high-level goal,
3962920	3965440	meaning make this large-scale anatomical structure,
3965440	3969360	different molecular mechanisms get activated, okay?
3969360	3972360	And this is very unusual.
3972360	3974080	This idea is very unusual in biology.
3975080	3977440	The biologists tend to think about things emerging
3977440	3980560	from molecules not going the other way,
3980560	3983000	but it has certain parallels in computer science
3983000	3985760	where the algorithm makes the electrons dance
3985760	3986920	in an important way, right?
3986920	3989000	In a functionally important way.
3989000	3993320	And so what we've been doing is trying to build models
3993320	3995720	that go, sort of full-stack models that go all the way up
3995720	3999160	from molecular kinds of activities
3999160	4003360	that set the ion channels and other things in the membrane.
4003360	4005920	Two, we specifically, I don't have too much time today,
4005920	4008000	but we specifically study bioelectrics.
4008000	4010680	We study how all cells, not just neurons,
4010680	4012840	all cells use electrical signaling
4012840	4015400	to form computational networks.
4015400	4019680	And so we study what the tissue-level electrical patterns
4019680	4023040	look like and then what the organ-level patterns look like
4023040	4027000	and then how that becomes literally an algorithmic set
4027000	4029320	of steps that determines things like how many heads
4029320	4031040	a flower is going to have.
4031080	4035520	And during this process, we want to know a few things.
4035520	4038320	We want to know how does the cognitive light cone,
4038320	4041880	and what I mean by that is simply the spatiotemporal size,
4041880	4044080	the scale of the largest goal
4044080	4047280	that that particular system can conceive of pursuing, right?
4047280	4048320	So if you're a bacterium,
4048320	4049920	your cognitive light cone is very tiny
4049920	4051440	because really all you care about
4051440	4053120	is the local sugar concentration
4053120	4056840	with about maybe 10 minutes forward and back.
4056840	4059360	But if you're a human, you can have gigantic goals
4059360	4060480	that exceed your lifespan.
4060480	4061840	It can be planetary-scale goals.
4061840	4064840	And then of course, every kind of creature in between.
4064840	4066840	So we define this kind of cognitive light cone
4066840	4070640	based around the types of goals that a system can pursue.
4070640	4072640	And so we need to understand during this process,
4072640	4074240	how do the goals enlarge?
4074240	4076640	How do they shift into different spaces?
4076640	4079600	So individual cells care about things in metabolic space
4079600	4081840	and physiological space and transcriptional space.
4081840	4082760	Those are their goals.
4082760	4085520	Collectives of cells care about very much larger goals,
4085520	4087200	such as the shape of your hand
4087200	4088560	and the fact that you have to have to have
4088560	4090240	exactly five fingers.
4090240	4092440	And then of course, this question of where do these goals come
4092440	4093280	from in the first place,
4093280	4094840	we'll address that momentarily.
4094840	4097240	So about the only piece of bioelectricity,
4097240	4099760	I'm gonna show you because Yosha brought up this idea
4099760	4103080	of counterfactual memories is simply this.
4104080	4107440	We treat the behavior of the cells and tissues
4107440	4109160	as a collective intelligence.
4109160	4111840	Literally the group of cells is a collective intelligence
4111840	4114960	that tries to solve problems in anatomical space.
4114960	4117320	And because we have some understanding now
4117320	4120160	of what the medium is of that collective intelligence,
4120160	4122880	not shockingly just like in the brain, it's bioelectric.
4122880	4126040	Why? Because that's how the brain learned its tricks.
4126040	4127800	You already heard and Yosha is absolutely right.
4127800	4132800	There are very difficult tasks to try to distinguish
4134480	4136440	what makes a neuron different from other cells
4136440	4140360	because even bacteria from the time of microbial biofilms
4140360	4143040	have already been using all of the same tricks
4143040	4147320	as the brain uses, this electrical network stuff is ancient.
4147320	4149680	And so what we are able to do is read and write
4149680	4153200	the memories of this collective intelligence.
4153200	4155080	And so we use a specific technique
4155080	4156880	that reads the electrical gradients.
4156880	4158640	This is just like neural decoding
4158640	4160720	as the neuroscientists try to do in the brain.
4160720	4162960	So here there's a particular pattern that says,
4162960	4165240	if injured, you're going to make one head.
4165240	4167480	We can rewrite that and we can create a worm.
4167480	4169440	Here is where the pattern says,
4169440	4172080	no, actually a correct worm should have two heads.
4172080	4174600	And if you go ahead and cut that animal,
4174600	4176200	they will go ahead and make two heads.
4176200	4177920	This is not Photoshop, these are real,
4177920	4179600	real two-headed malaria.
4179600	4181000	But the cool thing about this pattern
4181000	4183000	is this is not a reading of this animal.
4183000	4185640	This is a reading of this perfectly normal,
4185640	4188720	anatomically one-headed, genetically,
4188720	4190640	transcriptionally one-headed animal.
4190640	4192720	So this is a kind of counterfactual memory.
4192720	4194760	It's a representation of a state
4194760	4197320	that it's what you are going to do in the future
4197320	4198320	if you get injured.
4198320	4199840	If you don't get injured, it stays latent,
4199840	4200960	it never comes up.
4201840	4203520	And we have lots more data on this.
4203520	4206800	We can actually make heads of other species of worms
4206800	4208160	and many other things.
4208160	4209960	The idea is that a single body can
4209960	4212960	store one of two different representations
4212960	4215800	of what the goal state is going to be if they get injured
4215800	4217680	and then they build to that goal state.
4217680	4219040	So this should sound very familiar.
4219040	4221520	This is both the nervous system works this way
4221520	4223520	and of course, reprogrammable devices work this way.
4223520	4226440	The same hardware can hold onto multiple
4226440	4227920	computational goal states.
4227920	4231920	Now, let's go back to where we started with this,
4231920	4236120	which is this notion of scaling up from components.
4236120	4238960	So here's your single cell.
4238960	4241160	What evolution has done is allowed these cells
4241160	4245800	to merge into networks that are able to store
4245800	4247280	much larger goal states.
4247280	4250040	So this guy only cares about his own physiology
4250040	4251880	and his own metabolic.
4251880	4256560	This collection of cells is very competent
4256600	4258240	in reaching a particular region
4258240	4260400	of anatomical morpho space that looks like this.
4260400	4262600	The goal is huge at centimeters in size.
4262600	4264040	And if it's deviated from that,
4264040	4265600	it will do its best to come back
4265600	4269040	to even with the kind of drastic interventions.
4269040	4270920	But that process has a failure mode.
4270920	4272400	That failure mode is known as cancer.
4272400	4273240	What happens?
4273240	4275360	This is human glioblastoma cells.
4275360	4277800	If individual cells get disconnected
4277800	4280320	from this electrical and other signals as well,
4280320	4282160	from this network that binds them
4282160	4286120	towards a common journey in that space, that common goal,
4286160	4289000	they revert back to their evolutionarily ancient self.
4289000	4290360	What is the goal of a single cell?
4290360	4291680	Well, it's to become two cells
4291680	4293040	and to go wherever life is good.
4293040	4294360	That's metastasis.
4294360	4297960	And so you can see how what happens with these cancer cells
4297960	4301480	is they're not any more selfish than any other cell.
4301480	4303640	They're just their cells are smaller.
4303640	4304760	And we've talked, you know,
4304760	4307520	I talked to roboticists and folks like that
4307520	4309800	with this idea that why don't robots get cancer?
4309800	4310640	Right?
4310640	4312640	The reason that our current technology isn't prone to this
4312640	4315160	is because we do not have a multi-scale architecture
4315160	4317200	where the components have their own goals.
4317200	4320640	That we have some fairly dumb components typically.
4320640	4322360	And then we hope that the collective
4322360	4323640	that has some kind of, you know,
4323640	4325040	is doing some kind of computation,
4325040	4327080	but the parts are not trying to do anything.
4327080	4328160	Biology isn't like that.
4328160	4331040	Every component will do interesting things
4331040	4333280	if freed from its neighbors.
4333280	4334240	And I'll show you that.
4334240	4335560	But of course, you know, biomedically,
4335560	4338080	we can sort of take this kind of weird way
4338080	4339240	of looking at things and ask,
4339240	4344080	can we simply enlarge the boundary of the self?
4344080	4347680	Enlarge the border between self and outside world.
4347680	4348600	And so you can do that.
4348600	4350240	We have techniques to do that
4350240	4355240	where when we inject particular human oncogenes
4355440	4357560	into these tadpoles to make tumors,
4357560	4359800	and you can already see this is voltage imaging,
4359800	4363360	you can see that these cells are already starting to defect.
4363360	4364360	As far as they're concerned,
4364360	4367480	the rest of the animal is just outside environment.
4367480	4369080	So that's something else that Miocha mentioned
4369080	4371280	is this idea of being in conflict or not
4371280	4372200	with your environment.
4372200	4375840	It's never obvious to a new agent what the environment is.
4375840	4379240	Every cell is some other cells' external environment.
4379240	4382040	And so normally all of these cells believe
4382040	4384760	that the water out here is the external environment.
4384760	4387920	But once you disconnect them using these oncogenes,
4387920	4389800	then as far as the cells are concerned,
4389800	4391600	all of this stuff is external environment.
4391600	4392800	They don't care what happens to that.
4392800	4394200	They're gonna do their best.
4394200	4395560	They're gonna live their best life.
4395560	4397880	They're gonna dump entropy into the environment.
4397880	4400640	And of course, that's maladaptive for the organism.
4400640	4402880	But one thing you can do is you can force
4402880	4404160	using specific techniques,
4404160	4406360	including optogenetics and some other things,
4406360	4409600	you can force these cells to remain in electrical,
4409600	4411600	in the correct electrical state with their neighbors.
4411600	4413360	And if you do that, even though the oncogene,
4413360	4414400	this is the same animal here,
4414400	4416160	even though the oncogene is very strong,
4416160	4421160	there's no tumor because the hardware problem
4421200	4422680	isn't really fundamental.
4422680	4424240	It's the software that's fundamental.
4424240	4427240	It's are these cells working on a large goal
4427240	4430360	like making a nice liver and muscle and skin and whatever?
4430360	4433480	Or are they individual cells working on individual goals?
4433480	4435160	So we spend a lot of time thinking
4435160	4436000	about these kinds of things.
4436000	4440040	How do we, what are the mechanisms, of course,
4440040	4443800	but also algorithms, policies for connecting up
4443800	4446400	little tiny homeostats,
4446400	4448560	these cells that like to keep certain states
4448560	4450400	into much larger networks
4450400	4451960	that then have these interesting properties
4451960	4453640	that of course people in the connectionless world
4453640	4455000	have been studying for a really long time.
4455000	4457520	So in painting, out painting,
4457520	4459080	you know, all this kind of stuff.
4459080	4462000	So we can talk about our efforts
4462000	4464280	to sort of understand how the goals scale.
4464280	4466280	They scale from these really humble,
4466280	4469600	metabolic kinds of goals of individual cells, right?
4469600	4473320	These homeostatic loops into anatomical homeostasis,
4473320	4475280	eventually behavioral homeostasis
4475280	4479440	and behavioral clever motion through three-dimensional space
4479440	4481800	and eventually linguistic space and who knows what else.
4481800	4483480	So just for the last couple of minutes,
4483480	4485520	I just wanna show you one thing,
4485520	4487800	which is simply this.
4487840	4492840	In studying these kind of novel perturbations
4493040	4495920	and asking what are cells actually capable of?
4495920	4499120	What, you know, what other modes are there?
4499920	4501520	We asked the following thing
4501520	4502920	and I have to do a disclosure here
4502920	4505840	because Josh Bongard and I are co-founders
4505840	4507320	of this thing called Fauna Systems.
4507320	4511600	It's a biorebotics kind of company.
4511600	4512760	And so what we did in this,
4512760	4515520	all the biology was done by Doug Blackiston in my lab
4515520	4517800	and there was a lot of computer science here
4517800	4520480	done by Sam Kriegman in Josh's lab.
4520480	4523320	What we decided to do was to liberate cells
4523320	4524320	from the normal environment
4524320	4527200	and give them a chance to reboot their multicellularity.
4527200	4528600	How much creativity is there?
4528600	4530160	What else can they do?
4530160	4533480	And specifically, and I think somebody on the chat
4533480	4535760	asked this before, where do these goals come from?
4535760	4536920	So that's what we wanted to understand,
4536920	4540560	a completely novel creature that's never existed before.
4540560	4541520	What goals do they have?
4541520	4542720	Where do their goals come from?
4542720	4545040	Okay, and so I'm gonna just show you a couple of examples.
4545040	4550040	So what we did here is we took an early frog embryo
4550040	4553760	and so what Doug does is he takes all of these cells up here
4553760	4557000	which are skin, they're basically determined to be skin
4557000	4558800	and he dissociates them
4558800	4561280	and puts them into a little depression here.
4561280	4563440	Now, there are many things that they could have done
4563440	4564760	after that, they could die,
4564760	4567040	they could spread out and sort of walk away from each other,
4567040	4569120	they could form a flat two-dimensional monolayer
4569120	4570880	the way that cell culture does.
4570880	4573880	Instead, what happens is this, and this is time lapse,
4573880	4576800	of course, so overnight, these guys will get together
4576800	4579440	and they will coalesce into this interesting
4579440	4582840	little thing here and what is it?
4582840	4584280	Well, we call this a Xenobot,
4584280	4585920	Xenopus laevis is the name of the frog
4585920	4587680	and it's a biobot, so Xenobot.
4588560	4590840	What it's doing is it's using the little hairs
4590840	4592800	on its surface, these hairs are normally there
4592800	4595320	to spread mucus down the body of the frog.
4595320	4598400	What they've done is repurpose those hairs for swimming.
4598400	4600440	So here it goes, it's chugging along,
4600440	4602200	you can see that as they can go in circles,
4602200	4604120	they can sort of patrol back and forth like this,
4604120	4605800	they can have group behaviors,
4605800	4607560	this one's going on kind of a long journey,
4607560	4608760	these are interacting together,
4608760	4610920	these are having an arrest.
4610920	4612760	Here's what it does in a maze,
4612760	4615320	so you can see it swims along,
4615320	4617800	it's gonna take a turn here without having to bump
4617800	4619880	into this outside wall, so it takes a turn.
4619880	4622120	And then at this point, for some internal reason,
4622120	4623240	we have no idea about it,
4623240	4625360	it decides to turn around and go back where it came from.
4625360	4628760	Okay, so there's all sorts of primitive kinds of dynamics.
4628760	4631840	Just keep in mind, even though these things have,
4631840	4633480	this is calcium signaling you see,
4633480	4636480	it's the kind of thing you see when you do brain imaging,
4636480	4638360	there are no neurons here, this is just skin.
4638360	4640960	This whole thing is just skin cells,
4640960	4643360	but they're doing a lot of, calcium readout
4643360	4646120	is a great readout of computation.
4646120	4648560	And could they be saying something to each other?
4648560	4649400	Of course we don't know,
4649400	4652960	this is still a very much ongoing subject of investigation,
4652960	4657600	but one of the amazing things that Doug and Sam discovered
4657600	4660520	is that their computational models of these guys
4660520	4663320	make predictions that differently shaped bots
4663320	4665120	are going to rearrange their environment
4665120	4667720	in different ways, so they did a lot of simulations.
4667720	4670880	And so then we tried it and we just did it in vivo.
4670880	4672000	And here's what we found.
4672000	4675560	So here are the bots, the white stuff here is their cells,
4675560	4678760	they're loose skin cells that we sprinkled into the dish.
4678760	4680240	And what they're basically doing,
4680240	4683280	because we made it impossible for them to reproduce
4683280	4685320	in the normal froggy fashion,
4685320	4687680	they are basically implementing von Neumann's dream,
4687680	4691400	they are constructing other,
4691400	4692840	what they do is they run around
4692840	4695160	and they sort of collect these skin cells
4695160	4699040	into little piles, then they kind of polish the piles.
4699040	4700720	And these piles, because they're working
4700720	4702480	with an agential material,
4702480	4704200	they're not working with passive particles,
4704200	4706360	they're working with cells, what do these cells like to do?
4706360	4709080	They like to become the Xenobot.
4709080	4711480	And so of course they create the next generation of Xenobot,
4711480	4715080	which then matures and guess what, it does the same thing
4715080	4716800	and then you get the next generation and so on.
4716840	4719160	So this is kinematic self-replication,
4719160	4722000	and they'll make multiple generations of this.
4722000	4726320	So here's here a couple of interesting corollaries
4726320	4728800	to this and then I'm almost done.
4728800	4730120	The exact same genome,
4730120	4732800	so here's the specification of the micro level hardware,
4732800	4734920	this is what every cell gets to have,
4734920	4737720	can do one of two things under normal circumstances,
4737720	4740200	it will do this, it has this developmental sequence,
4740200	4742640	then it makes these tadpoles that do various things.
4742640	4745360	But under other circumstances, it makes this,
4745400	4747720	this is a Xenobot, this is a developmental sequence,
4747720	4750600	this is I think a month old or something Xenobot,
4750600	4752280	where did the shape come from, right?
4752280	4753680	And they have a different behavior
4753680	4755880	with this thing called kinematic self-replication.
4755880	4758840	So here's a few interesting things, number one.
4758840	4761440	Typically when you talk about why a certain creature
4761440	4764240	has certain capacities, everybody leans on evolution.
4764240	4767760	Well, for eons, it was selected to do this or that.
4767760	4769320	Well, there's never been any Xenobots,
4769320	4770800	there's never been any selective pressure
4770800	4773160	to be a good Xenobot, this is completely emergent.
4773160	4777800	They do this, they form this coherent kind of system
4777800	4782320	with new behaviors, both anatomically and with mortality,
4782320	4784880	basically overnight, this has never been selected
4784880	4788160	for specifically, they're completely new in the biosphere.
4788160	4790880	As far as we know, no other living creature
4790880	4793520	does kinematic self-replication, that's the first thing.
4793520	4796440	The second thing is that, how did we engineer these?
4796440	4799440	I mean, there are no trans genes here.
4799440	4801200	So the, if you sequence this, all you see
4801200	4803160	is normal Xenopus latus, there's nothing wrong
4803160	4804800	with the genome, it's wild type.
4804800	4809240	There are no nanomaterials, some of them,
4809240	4812520	some of them Doug can make some modifications
4812520	4815040	to them surgically, according to the AI
4815040	4816760	that Josh and Sam built.
4816760	4819720	But basically, the way we engineered these
4819720	4822520	is not by adding anything, it's by liberating them
4822520	4825400	from the influence of the other cells.
4825400	4827840	So normally, if you just look at the normal path
4827840	4830800	of this biological system, you would say,
4830800	4832520	what do the skin cells like to do?
4832520	4835080	Well, they like to be, and in fact, all they can be,
4835080	4837680	is to be the outside two-dimensional layer
4837680	4838840	of keeping out the bacteria.
4838840	4840120	It's very boring passive life,
4840120	4842600	they just sort of sit there and keep out the bacteria.
4842600	4845080	But that's only what happens when they're basically
4845080	4846960	bullied into it by the other cells.
4846960	4849720	It's behavior shaping, it's instructive interactions
4849720	4852480	from the other cells that tell them to sit quietly
4852480	4853560	and be the outer layer.
4853560	4856000	In the absence of all that stuff, liberated from all that,
4856000	4858400	they have a completely different default lifestyle.
4858400	4860200	And this is it, which you would not see
4860800	4862240	without this thing.
4862240	4864520	So, and we don't know what else,
4864520	4865840	certainly we're studying right now,
4865840	4867840	all the kinds of behavioral capacities,
4867840	4869480	do they learn, do they anticipate,
4869480	4872960	all sorts of things, I'm not making any claims yet about that.
4872960	4877760	So, but this idea of what evolution I think really does,
4877760	4879200	and we can talk about why,
4879200	4881840	I think we have now some ideas about why,
4881840	4884440	it doesn't produce solutions to specific problems,
4884440	4887400	it produces generic problem-solving machines.
4887400	4891120	And so the big thing that every living system has to do,
4891120	4893000	and I think these are, if I had to make a list,
4893000	4896080	these are some things that I think are required
4896080	4897440	for the kind of thing we want.
4897440	4899560	First of all, I think it's really important
4899560	4901360	that your parts have agendas.
4901360	4902680	It's not enough to have dumb parts
4902680	4906640	and try to engineer an agenda for the whole system.
4906640	4910280	You have to have a marketplace where every layer
4910280	4912720	is competing, cooperating,
4912720	4915880	and attempting to do its own thing.
4915880	4917400	You, of course, risk failure modes,
4917400	4920000	you risk parts trying to go off on their own.
4920000	4922040	That's one of the trade-offs,
4922040	4923600	but overall it becomes,
4923600	4925960	I think, an incredibly powerful architecture.
4927000	4929120	They have to emerge spontaneously.
4929120	4933120	That is real agents don't know where their boundaries are.
4933120	4935720	If you are a new embryo coming into the world,
4935720	4938400	you don't know how many cells you're going to have,
4938400	4939760	because we might remove half of them,
4939760	4941320	and you still have to make a good embryo.
4941320	4943320	You don't know how big your cells are,
4943320	4946080	because we might make gigantic cells or smaller cells.
4946080	4948920	You don't know exactly how many chromosomes
4948920	4949760	you're going to have,
4949760	4954080	because we can make all kinds of weird chimeras and so on.
4955240	4957480	You have to be able to, surviving life,
4957480	4960720	has to be able to play the hand and stealth from scratch.
4960720	4964360	You really can't take past experience too seriously.
4964360	4966280	You have to improvise on the fly.
4966280	4968520	This is what biology does.
4968520	4969920	So it does not have, nobody says,
4969920	4971600	this is the border, this is where you are,
4971600	4973040	and then everything else is the outside world.
4973040	4975280	It has to guess, and it has to make a self-model,
4975280	4977040	and it has to make a world model.
4977040	4979160	Then there are the energy constraints.
4979160	4981360	Typical AIs, as far as I know,
4981360	4983080	have all their energy needs met.
4983080	4985560	They can do whatever they want.
4985560	4987440	They don't have to worry about it.
4987440	4990920	Organisms evolved under very stringent energy
4990920	4991880	and time constraints,
4991880	4995160	which means that they cannot afford to be
4995160	4997840	some kind of a Laplacian demon
4997840	5000440	paying attention to all the micro states of the world.
5000440	5003160	They have to do a lot of core screening
5003160	5007080	and they have to kind of bundle all sorts,
5007080	5009840	they have to generalize all sorts of things that go on
5009840	5014320	into models of agents doing things, of selves doing things.
5014320	5015760	That's the only way you have the time
5015760	5017760	to compute what you should do next.
5017760	5018720	And if you get good at that,
5018720	5020760	eventually you turn that on yourself
5020760	5022600	and you start telling stories about,
5022600	5025960	meaning making internals and models of yourself doing things.
5025960	5028040	And this becomes this idea,
5028040	5030600	why do we all innately believe in free will?
5030600	5033440	Because from the time that we were single cells,
5033440	5036680	we had to tell stories about agents doing things
5036680	5037520	and making choices.
5037520	5040920	Otherwise, we just wouldn't survive without that ability.
5040920	5043680	And then there's some other things like the shared stress
5043680	5047600	and the scaling of stress via sharing it among parts
5047600	5048840	and so on, we can talk about that.
5048840	5052040	And the idea that it's open-ended,
5052040	5054360	living things select their own problem space
5054360	5056520	and explore it and so on.
5056520	5059840	So this is, I'm just gonna stop here,
5059840	5062720	but this is what I tell people is that because of this,
5062720	5066040	because biology is so incredibly interoperative
5066040	5067920	because none of the parts make any assumptions
5067920	5069200	about what's going to happen,
5069200	5071480	they do their best in whatever environment
5071480	5073200	that they happen to be in,
5073200	5076320	every combination of evolved material,
5076320	5080240	some sort of engineered material and software
5080240	5082720	is potentially a viable agent.
5082720	5087760	So hybrids, cyborgs, biorobots, all of this,
5087760	5090960	there's this huge option space of new creatures,
5090960	5093200	of new bodies and new minds.
5093200	5095480	Everything, when Darwin said endless forms,
5095480	5097160	most beautiful, sort of impressed
5097160	5099320	with the variety of living beings,
5099320	5100880	all of that stuff is a tiny dot.
5100880	5102080	It's a tiny corner.
5102080	5104000	Everything on earth is a tiny corner
5104000	5105600	of the space of possible beings.
5105600	5106880	It's truly immense.
5106880	5108360	And all of these things,
5108360	5110080	and we're gonna be surrounded by these things.
5110080	5112000	Some of this already exists as some hybrids
5112000	5113080	and cyborgs already exist,
5113080	5115000	but there's gonna be an incredible variety of them
5115000	5116920	that we are going to be living with.
5116920	5120080	This has major implications for ethics, for example,
5120080	5121840	because up until now,
5121840	5126160	we were, all of our ethical frameworks
5126160	5128560	about how to relate to other beings
5128560	5130440	really boiled down to two things.
5130440	5132040	Do they look like us?
5132040	5133200	And did they come from,
5133200	5135360	do they have the same origin story as us?
5135360	5140360	And so this is, even today in bioethics sessions
5140640	5141400	at conferences, people say,
5141400	5142760	well, does it look like a human brain?
5142760	5144280	Then we have to worry about.
5144280	5147800	But the reality is that these categories are,
5147800	5149920	they're not gonna survive the next couple of decades.
5149920	5154920	We cannot gauge anything about the potential intelligence
5155640	5157520	in terms of the type of cognition
5157520	5159320	and what space they're working in,
5159320	5161880	by looking at where they come from the family tree,
5161880	5163960	because they're not going to be on our family tree.
5163960	5166040	And we have to have completely different frameworks for this.
5166040	5168600	And the kinds of AIs that we're talking about now
5168600	5170080	are only one part of this.
5170720	5172960	We're going to be facing the exact same problem
5172960	5177000	of dealing with the software AIs in biology.
5177000	5180000	So if anybody's interested in these things,
5180000	5182960	there are lots of papers where we go into this.
5182960	5185480	And I just wanna thank the students and postdocs
5185480	5187520	that did all the work that I showed you.
5187520	5189240	And of course, again, the disclosure.
5189240	5191400	So I will end there.
5195160	5196520	Thank you so much, Michael.
5196520	5197720	That was wonderful.
5198720	5201760	I don't think that I need to introduce Christoph.
5201760	5206840	We already had the pleasure of having you on the previous panel.
5206840	5211840	And Christoph is currently a professor at Etihad,
5212120	5214960	the INI in Zurich.
5214960	5218800	And he is a first-generation cyber nutrition
5218800	5223800	in a way as a physicist who is using very broad perspective
5224080	5226560	on understanding intelligent systems.
5226560	5231080	And without further ado, Christoph, please.
5231080	5232160	The stage is yours.
5233080	5235040	Yeah, I haven't prepared anything.
5235040	5240040	I didn't know I was expected to prepare anything.
5240520	5244960	So I am at liberty to respond to some of the things
5244960	5247560	you have said, the two of you have said.
5247560	5251560	Let me start with a point,
5251560	5253280	Yosha, you made about the brain,
5253280	5258280	which was it is a noisy, it is a noisy entity.
5259440	5264440	It is not a digital device, not on the basic level.
5265240	5270240	So if you want to have billions of entities,
5270240	5275240	synapses or neurons to interact in any useful sense,
5277960	5280120	you need attractor dynamics.
5280120	5283280	So there must be certain states of the thing
5283280	5287760	that have the property of being stable under noise,
5287760	5292760	of having attractor dynamics.
5293520	5298520	And we know that the brain is, of course,
5299080	5300800	essentially a network.
5300800	5305800	So in each moment of time, a subset of the neurons fire,
5306640	5310600	and this subset must be stable.
5310600	5313640	And for a short moment, a metastable,
5313640	5315480	if you want to call it that way,
5315480	5318960	you want to go through a trajectory of stable states.
5318960	5323600	And that means individual fibers,
5323600	5326600	the individual interaction between neurons
5326600	5331600	must be embedded in alternate alternative pathways,
5331800	5335320	which run to the same effect.
5335400	5340400	So a signal emanating from a single neuron,
5342720	5345720	going off on different pathways,
5345720	5349800	many of these signals must come together again
5349800	5353840	and coincide in space, meaning on the same neuron,
5353840	5354960	and in time.
5356000	5359360	And this is a selection criterion
5359360	5363320	for the kind of activity states
5363320	5366960	and the underlying connectivity states
5366960	5370360	that make those states stable.
5370360	5373480	And I would like to submit the idea
5373480	5378480	that the brain is totally dominated
5378680	5383680	by those appropriately shaped connectivity patterns
5386440	5388640	that have this property.
5388640	5392080	These connectivity patterns emerge
5392080	5395200	through a process of self-interaction.
5395200	5400200	You have self-interaction also on the slow time scale
5400480	5403440	of individual synapses adapting
5403440	5407160	and individual synapses finding out
5407160	5411360	where they can find coincidences of signals.
5411360	5415760	Each axonal branch has a small choice,
5415760	5418760	a small sphere of a search space
5418840	5422680	where it can end up in a plasticity.
5422680	5427680	And all the endpoints of axons are searching around
5429560	5431960	in order to find meeting places
5431960	5434360	where they have a high likelihood
5434360	5438960	of coinciding with the signals of other branches.
5438960	5442200	And this process of self-interaction,
5442200	5444600	of network self-organization,
5444600	5448080	singles out from the space
5448080	5452720	of all combinatorially possible connectivity patterns
5452720	5455880	a very, very small subset.
5455880	5458080	Let me remind you of the fact
5458080	5460840	that the brain, the whole organism,
5460840	5463440	the brain is constructed on the basis
5463440	5468440	of one gigabyte of genetic information.
5468480	5470920	And in order to describe the connectivity pattern
5470920	5473640	of the brain, it's an easy calculation,
5473640	5478640	you need a petabyte, 10 to the 15 bytes of information
5479080	5482320	which is a million gigabytes as you well know.
5482320	5487320	So the genes can only select from the space
5488480	5491400	of all connections, a very small,
5491400	5494320	can only be able to select from that space
5494320	5496720	a very small subset.
5496720	5499880	And I think it is very important to know more
5499880	5504880	about this subset of self-supporting activity states
5507840	5510960	and connectivity states.
5510960	5513880	So in order to put that in action,
5513880	5516880	let me remind you that your brain
5516880	5519820	is in every moment of waking time
5519820	5523720	representing the situation in which you are immersed.
5523720	5527680	You have a representation of your actual environment
5527680	5531360	if you open up your eyes.
5531360	5535040	And this representation is so good
5535040	5540040	that you usually equated with the reality out there.
5540080	5542720	You are not aware of any differences
5542720	5546080	between this reconstructed,
5546080	5551080	this model of the outside world and the outside world.
5551080	5555960	And you are so confident that it is the reality
5555960	5558800	and not just an imagination
5558800	5561520	because you continuously do experiments.
5561520	5565440	You move around so that the perspective
5565440	5568200	of the world changers
5569280	5572960	and you test whether your representation
5572960	5577960	is stays in tune over time with the sensory information.
5578720	5581560	You do experiments, you touch objects
5581560	5586560	and you experiment continuously
5588000	5591240	with the environment
5591240	5594600	in order to make sure that your representation
5594600	5598160	is in tune with it, is consistent with it,
5598160	5602520	is rendering the reality.
5602520	5605880	Of course, what you are representing
5605880	5608680	is only a small sector of what is out there.
5608680	5613680	Your attention is always picking out only part of it.
5614120	5618880	But what you are picking out is for you,
5618880	5622160	for all intents and purposes, reality.
5622160	5627320	I find it amazing that our models,
5627320	5631360	our theories of intelligence, of brain function
5632440	5635480	make so little of this very fundamental fact
5635480	5637760	of our individual life.
5639520	5643520	Now, according to what I said,
5643520	5648400	I have been explicit about the data structure
5648400	5652360	which is used to create this reality,
5652360	5656600	to represent a model of this reality.
5656600	5658200	The data structure is, of course,
5658200	5661760	everybody believes firing neurons,
5661760	5665040	but I would like to change your perspective
5665040	5667600	in saying, don't look at the neurons,
5667600	5672600	each neuron by itself doesn't have any significant meaning.
5675600	5678600	It is the environment, the neural environment,
5678600	5681800	the firing environment in which the neuron fires,
5681800	5683400	which is the important thing.
5683400	5688280	You have to look at quite a number of co-firing neurons
5688280	5690880	in order to be able to make sense of it.
5690880	5695880	When you look at a TV screen and can see only one pixel,
5696200	5699800	there is no way you can connect that with any meaning.
5701640	5704920	The pixel is something real, so to speak,
5704920	5707160	but it doesn't tell you anything.
5707160	5711920	It has no significance.
5711920	5715880	In order to understand anything on a TV screen,
5715880	5719160	you need to see quite a patch of it.
5719160	5721320	In order to understand anything
5721320	5724520	of the data structure of a brain,
5724520	5728840	you need to see hundreds, probably thousands of neurons,
5728840	5732200	at a time, and a given neuron can take part
5732200	5735960	in quite a number of such, not an infinite number,
5735960	5740520	but quite a number of such activity patterns,
5740520	5744680	which I would like to call fragments.
5744680	5749200	I think the perspective on the nervous system
5749200	5752200	has to be changed very fundamentally
5752200	5755560	in order to see it as a data structure
5755560	5759320	that is up to the job of representing reality.
5761200	5766120	Now, I would like, one of the last statements
5766120	5772120	you, Michael, made was the range of things,
5772120	5774320	of intelligent things, of organized things
5774320	5778920	that can be generated, you said, is infinite.
5778920	5784120	I would rather like to emphasize the opposite.
5784120	5786640	I've recently read an interesting book
5786640	5791400	by an author named Morris, a book that
5791400	5794280	was totally focused on the phenomenon,
5794280	5798280	looking at evolution, the phenomenon of convergence.
5798280	5803280	A lens eye has been invented 12 times or something like that.
5803280	5807200	The facet eye has been invented again and again.
5807200	5811760	The lifestyle of a wolf pack has been invented again and again.
5811760	5816560	The lifestyle of social insects or social animals,
5816560	5820960	you social animals, has been invented again and again.
5820960	5826320	So the space of all possible organic patterns
5826320	5829360	that make sense, that have inner coherence,
5829360	5832800	where the parts support each other in order
5832800	5836440	to create something that is stable and significant,
5836440	5840680	that is stable in itself and is coherent with environment,
5840680	5845040	the space of those shapes and structures is limited.
5845040	5850800	And here is a great opportunity for theory
5850800	5855640	to come forward to understand what to expect from biology.
5855640	5860360	The last thing I want to say is that what is missing,
5860360	5863680	completely missing so far, almost completely missing
5863680	5868440	so far from our artificial intelligence,
5868440	5872600	from our machine learning, is the equivalent
5872600	5876960	of biological behavior, of behavioral goals.
5876960	5880800	Of course, an animal has the fundamental goal
5880800	5885720	of self-preservation of the own structure.
5885720	5892040	But evolution has built into the individual species
5892080	5898000	a number of sub-goals, like feed yourself and avoid danger and so on,
5898000	5903680	and find social contexts, sub-goals,
5903680	5907440	which make up your life.
5907440	5910360	And the intelligence in the eyes of many people
5910360	5916520	is just the ability to pursue those goals in a changing context.
5916520	5921480	That is a slightly different definition from yours,
5921480	5927160	because yours was computing functions.
5927160	5933880	The biological goals is the raison d'etre
5933880	5940000	of biological intelligence, of course, to pursue those goals.
5940000	5947560	And I think in the present situation around things like chat, GTP and so on,
5947560	5955960	it is becoming quickly clear that those beasts are not intelligent in our sense,
5955960	5961160	because what they do doesn't make sense in the light of the goals
5961160	5963840	that we all recognize as such.
5963840	5970440	And it would be, I think there will soon be an important drive towards
5970440	5977440	installing in such systems the equivalent of the sense of responsibility.
5977440	5983720	The sense of the consequences and utterance that is made may have
5983720	5989160	down the line for ethical, for legal reasons.
5989160	5994960	But also I feel, although I don't have a very strong argument in that favor,
5994960	6000440	I have the feeling that in order for an entity to be truly intelligent,
6000440	6011600	it needs to have a set of set goals with which it can pursue in its environment.
6011600	6016000	Last remark I want to make is about consciousness.
6016000	6023520	I think in my view, the definition of the conscious state of your own mind,
6023520	6028800	of your own brain is a state in which you concentrate on one topic.
6028800	6032080	Your whole brain is concentrated on one topic.
6032080	6037160	And all the different submodalities in your brain are in tune with each other,
6037160	6039240	are in mutual understanding.
6039240	6043080	So if any change happens in any part of the system,
6043080	6049240	all the other agency, other modalities can immediately respond to that.
6049240	6055640	So consciousness is not the icing on a cake.
6055640	6061360	I don't think it makes sense to talk of zombies.
6061360	6066520	Consciousness is a condition for a system to be functional.
6066520	6073640	And if you go down in the letter of evolution to simple animals,
6073640	6079640	I don't think you can find a point that is a point Michael also made.
6079640	6084240	You can find a point where consciousness disappears.
6084240	6087520	But consciousness just loses volume.
6087520	6091520	You lose language when you go from humans to animals.
6091520	6097120	And you use the imagination of distant future when you go to animals.
6097120	6104800	And so going further and further down the evolutionary ladder,
6104800	6107840	the volume of consciousness gets less.
6107840	6117400	But I would have a hard time, I think when talking about a fly,
6117400	6121680	it has its own level of consciousness.
6121680	6129680	So when approaching a wall, it senses the impending approach and reacts accordingly.
6129680	6133640	So the whole organism is able to react to signals.
6133640	6137720	So that is my view on consciousness.
6137720	6140240	Thank you.
6140240	6142480	Thank you so much, Christoph.
6142480	6147040	So we're on to the discussion and Q&A question.
6147040	6152480	I would like to start by asking a question of Michael Levin.
6152480	6160520	So Michael, you were talking about how a lot of the problem solving within the organism
6160520	6163760	is actually done at the local scale.
6163800	6169040	And there is also the interesting remark that was made by one of participants in the chat
6169040	6175680	that you mentioned that the planaria tail would have become the head had not been bullied
6175680	6177520	by the rest of the organism.
6177520	6182600	So what do you think are the communication protocols that are necessary
6182600	6185680	to enable different types of intelligence?
6185680	6189000	And is it the case that humans, for example, have cancer
6189000	6194720	because we don't have enough intelligence at the lower local organismic level?
6194720	6198280	Or is it because we pursue the higher scale goal
6198280	6202080	and some kind of trade-off has to be made?
6202080	6203600	Yeah, great questions.
6203600	6204360	A few things.
6204360	6207280	First of all, it is definitely not local.
6207280	6213200	So one of the key things about all of this stuff is that larger systems make decisions
6213200	6218040	in spaces that are much larger than their parts.
6218080	6220320	And so here's a very simple example.
6220320	6222160	If you have a planarian, it's got a head and a tail.
6222160	6223640	You cut it in half.
6223640	6227240	These two cells on either side of the cut, these guys will have to make a new head.
6227240	6228880	These guys will have to make a new tail.
6228880	6232360	But they were sitting right next to each other before you separated them with a scalpel.
6232360	6235200	You cannot locally decide whether you're a head or a tail.
6235200	6238360	It's a decision that has to take into account, well, do we already have a head?
6238360	6239080	Do we have a tail?
6239080	6240560	Which way is the wound facing?
6240560	6241760	This is a global decision.
6241760	6243000	It cannot be made locally.
6243000	6245920	All of this stuff is like that.
6245920	6254920	And it uses the exact same scheme that bacterial biofilms use to decide when different parts
6254920	6258360	of the thing should eat so that everybody has a turn and the exact same thing,
6258360	6263320	the exact same set of mechanisms that brains use to try to synthesize the activity
6263320	6268320	of individual neurons into some sort of global goal for the rat or human or whatever.
6268320	6270600	It's an electrical network.
6270600	6273520	It has certain properties, only a few of which we understand.
6273520	6275120	But it is absolutely not local.
6275120	6284520	What this bioelectricity is very good at is at implementing integrated information
6284520	6289440	across space and time to make decisions in new spaces.
6289440	6293960	And that's, I think I forgot, what was the second part of your, I lost track of it.
6293960	6300200	The second part is so do humans, so you remind that humans have cancer,
6300200	6302760	but some other animals don't.
6302760	6305120	And some other animals are in fact immortal.
6305120	6310080	So what is it that, what is it in your opinion that doesn't allow human organisms
6310080	6312880	to solve the problem of immortality?
6312880	6318200	Does it have something to do with higher level goals or is it just a lack of intelligence?
6318200	6324680	Yeah, I think that, well, so there's two, there's kind of a simple answer
6324680	6325960	and then there's a more interesting answer.
6325960	6332640	The simple answer that people usually give is simply that all by, so evolution, of course,
6332640	6336720	doesn't really optimize for long life, happiness, intelligence.
6336720	6337840	It doesn't optimize for any of that.
6337840	6339840	It optimizes for biomass, that's it.
6339840	6346240	And so, right, and so the simple answer is we don't need to be immortal
6346240	6352160	and cancer resistant because it's perfectly possible to be a human
6352160	6356280	and have lots of offspring and still get cancer and die after your reproductive years.
6356280	6359920	That's it, that's the standard answer that it's actually,
6359920	6364200	there's just not a lot of pressure for humans to do anything different.
6364200	6368200	Now, I think the more interesting answer is this.
6368200	6373560	There are most organisms do get cancer and do age, there are a few that are resistant.
6373560	6374720	Let's look at the planaria.
6374720	6379320	One really interesting thing about planaria is that many of them reproduce
6379320	6382640	by tearing themselves in half and regenerating.
6382640	6387480	Now, one interesting thing that the implications of that are unlike for us.
6387840	6390120	If you get a mutation in your body during your lifetime,
6390120	6391960	it doesn't get passed on to your offspring, right?
6391960	6396040	So because of the Weissmann's barrier in sexual reproduction.
6396040	6401120	In planaria that do this, every cell that doesn't die from that mutation
6401120	6404600	contributes copies of itself to the next body, right?
6404600	6408200	Because they have to repopulate and have to regenerate the new one.
6408200	6410600	So planaria accumulate mutations like crazy.
6410600	6413320	So over 400 million years that they've been around,
6413320	6415680	their genomes are a complete mess.
6415720	6417760	They basically look like a tumor, they're mix-employed.
6417760	6419960	Every cell might have a different number of chromosomes.
6419960	6420960	It's a disaster.
6420960	6429160	Now, this is really a scandal because nowhere in a typical biology curriculum
6429160	6432320	will you hear that the animal with the worst genome is, by the way,
6432320	6434960	immortal, cancer-resistant and highly regenerative, right?
6434960	6436240	They have the best anatomy.
6436240	6436880	What's going on?
6436880	6441120	We're told that our genomes are, that's where your body information is, right?
6441120	6442440	How can this be?
6442440	6445560	So this has been bugging me for a really long time.
6445760	6446640	This disconnect.
6446640	6450000	And I think we finally have an idea of what's going on.
6450000	6454400	We just, like two days ago, just published a paper on some simulations
6454400	6455360	that talk about this.
6455360	6456960	I'll just give you a very simple example.
6460160	6464560	One thing you have to do is you have to model not just the genotype and the phenotype,
6464560	6468400	meaning the genome and then the thing that gets evaluated in these evolutionary simulations,
6468400	6472280	but you have to model the morphogenetic process in between those two.
6472560	6474920	The morphogenetic process has certain competencies.
6474920	6477560	For example, some of them I've showed you, there are many more.
6477560	6482880	So for example, if there's some mutation that puts your mouth off to the side,
6482880	6485600	the mouth is perfectly competent to come back where it needs to be.
6485600	6489040	If it's a mutation that causes you to fall apart as an early embryo,
6489040	6492040	you'll just be a bunch of twins, multiples.
6492040	6496320	If we took some eyes, Doug Blackiston did this to us,
6496320	6499720	he took eyes and put them instead on the animal's tail,
6499720	6501480	they can see perfectly well out of those eyes.
6501480	6503760	No problem, no period of adaptation needed.
6503760	6506720	It's all good, the nerves come, find the spinal cord, it's all good.
6506720	6508600	So all of those kinds of things,
6508600	6513160	abilities to make up for these kinds of issues we call developmental competencies.
6513160	6518200	Now, one thing that happens is that when you have an animal with a little bit of developmental competency,
6518200	6522320	you come up for selection and it turns out you're very good, right?
6522320	6523240	But why are you good?
6523240	6527480	Selection cannot tell whether you have a great genome or you're good because you're highly competent
6527480	6530800	and you fixed all the things your genome actually was pretty sloppy about.
6530800	6534000	So that means it's harder for evolution to see the good genomes.
6534000	6536640	You can't do all as much work in perfecting the genome,
6536640	6539920	but what it can do is crank up the competencies, right?
6539920	6542120	So when you do that, then of course that makes the problem worse
6542120	6546920	because the more competent you are, the less it's possible to find the best genomes.
6546920	6549360	And so there's this positive feedback loop that's ratchet
6549360	6551880	and there are some other things that sort of work against it.
6551880	6556720	But I think what happened is that, and this is very much a hypothesis still,
6556760	6560600	I think what happened is that planaria went all the way,
6560600	6564960	meaning that in that lineage, probably because they reproduce this way,
6564960	6568160	it doesn't make any sense to assume that your genome is any good.
6568160	6572440	And the only architecture that survives is where the algorithm is so good
6572440	6575240	that we're going to make a perfect worm no matter what happens to the genome.
6575240	6580360	This means aging, carcinogenic mutations, the algorithm,
6580400	6586240	meaning the machinery that maintains that goal state
6586240	6589360	and physiological and anatomical space is so good
6589360	6592960	that it can pretty much ignore a lot of issues in the hardware.
6592960	6595600	Most of us aren't like that. Salamanders are sort of in the middle.
6595600	6598680	So salamanders are highly regenerative, but they age and die.
6598680	6602320	And so I think what salamanders sort of went part of the way there
6602320	6606360	and they can fix certain things, but not enough to really keep it going forever.
6606360	6609160	Mammals probably stopped even earlier than that.
6609160	6613680	But I actually don't think any of this is fundamental.
6613680	6616560	I mean, we're working on regeneration in mammals now.
6616560	6619760	I do think someday we will all sort of regenerate like planaria.
6619760	6621840	I think it is going to be possible.
6621840	6626560	But I do think that evolution makes these trade-offs that they're
6626560	6628920	just easier ways to be a human, I think.
6631360	6634280	So question for everyone.
6634280	6637680	So in terms of communication protocols,
6637680	6643440	to what extent is intelligence simply the ability to organize the cells
6643440	6646760	and what are the conditions necessary for that to occur?
6646760	6650800	And to what extent is intelligence is some internal competence,
6650800	6656760	competence of the cell or neuron or whatever computational unit we're talking about?
6660920	6662200	Yosha, would you like to start?
6663200	6671200	I'm currently thinking about the question of whether it's possible to make something
6671200	6675200	that is as long lived as the planaria that doesn't look like a blob.
6676200	6682200	There seems to be some correlation between the structural coherence of the organism
6682200	6689200	and the detail and solution that it has and the degree of fidelity.
6689200	6695200	That is expected from interpreting the operators defined in its genome.
6695200	6703200	And more specifically, I wonder what, how we can formalize the idea
6703200	6708200	that Michael put up earlier of multi-scale organization and such a way
6708200	6710200	that it leads to coherence.
6710200	6715200	What is the criterion that makes a single agent coherent in itself
6715200	6718200	and leads to this coherence on a particular level?
6718200	6722200	Arguably, our own mind is some kind of society of agents
6722200	6724200	and the organism has lots of local agents.
6724200	6726200	Every organ is an agent in a way.
6726200	6728200	Every cell is an agent.
6728200	6731200	But there is also a globally coherent agent.
6731200	6737200	And that is different from having multiple twins coexisting next to each other
6737200	6740200	and forming some cooperative chimera.
6740200	6744200	But that leads to some global element.
6744200	6749200	On the other hand, Christoph has pointed this out.
6749200	6755200	If you think about consciousness, it seems to relate to a unified experience.
6755200	6760200	And this unified experience of all sensory data is what makes it specific.
6760200	6762200	What is interesting about consciousness
6762200	6766200	is that I normally don't have multiple conscious experiences
6766200	6769200	unified in one perspective.
6769200	6772200	How is this unity being realized?
6772200	6780200	Or more generally speaking, can we come up with some kind of formal criterion
6780200	6785200	that defines how everything has a place in the greater whole
6785200	6788200	and the condition needs to be measurable
6788200	6793200	and lead to globally coherent behavior on the next level of organization?
6793200	6798200	If we take this to account and if you look at Michael's diagram
6798200	6801200	that he brought up in the context of ethics,
6801200	6806200	all the different agents that all seem to be centered about individual humans,
6806200	6812200	it turns out that individual humans are not the main agents in the human sphere.
6812200	6817200	The organizations of humans are much more powerful than individual human beings.
6817200	6822200	And while individual human beings implement these organizations for the most part,
6822200	6827200	we gradually transition more of that to machines that we are building.
6827200	6832200	It seems to me that there is different levels of organization
6832200	6836200	that transcend the individual organisms.
6836200	6839200	This multi-scale organization doesn't stop with humans
6839200	6842200	and the next scales are getting more and more agency.
6842200	6845200	Also, I don't think that humans are all that important.
6845200	6850200	It seems to me that humans are a very specific thing that has a very specific role.
6850200	6852200	All our cousin species are dead.
6852200	6854200	Women are not long-lived species.
6854200	6860200	And it seems to be that the reason why Gaia brought us up is that we fulfill our job,
6860200	6863200	which is to burn all the fossil fuels as quickly as possible.
6863200	6864200	This is what we're here for.
6864200	6866200	Then we burn ourselves out.
6866200	6869200	If we manage to teach the rocks how to sink in the meantime,
6869200	6870200	that's a stretch goal.
6870200	6873200	But after we are gone, there will be more intelligent species.
6873200	6875200	And we are a very specific one, right?
6875200	6880200	We are this type of monkey that is not going to get his hand out of the cauldron trap
6880200	6882200	if there's fossil fuel inside.
6882200	6886200	And that's somewhat predictable if you look at the way in which we work
6886200	6890200	because we are very smart and intelligent on very short timescales,
6890200	6892200	but we are not globally coherent.
6892200	6896200	We don't find ourselves in this global, coherent, godlike,
6896200	6897200	organization.
6897200	6901200	And if we succeed in building the next level of intelligence,
6901200	6905200	maybe this next level organization, some kind of very fast,
6905200	6907200	tightly integrated globally,
6907200	6909200	coherent mind is going to be emerging.
6909200	6913200	And maybe humans will play a very small part in whatever is going to come
6913200	6914200	afterwards.
6914200	6916200	But it's not about us, right?
6916200	6918200	Life on Earth is not about us.
6918200	6920200	Life on Earth is about the cell.
6920200	6923200	And overall, it's about fighting back entropy.
6923200	6928200	It's about sustaining yourself through maintaining complexity.
6928200	6934200	So my question would be to Christoph and to Michael,
6934200	6941200	can we come up with the criterion that determines coherence?
6941200	6946200	Yeah, yeah, I think the important thing is, as I've said,
6946200	6950200	that a coherent form has a kind of stability.
6950200	6954200	It is made up out of continuous variables,
6954200	6958200	which are prone to noise.
6958200	6966200	And for the whole thing to have a lasting existence is a different
6966200	6968200	signals that converge on one point.
6968200	6970200	They have to agree with each other.
6970200	6972200	They have to stabilize each other.
6972200	6976200	The same way as a crystal is formed,
6976200	6981200	a rigid body in that the individual forces between atoms,
6981200	6986200	which are also acting, of course, in a liquid,
6986200	6992200	but are not able to form something like a stable shape in a liquid.
6992200	6998200	But in a crystal, they have fallen into a configuration in which
6998200	7000200	in each individual interaction,
7000200	7008200	each individual force gets support by other indirect pathways.
7008200	7013200	And so I think just as in the space of all mathematics,
7013200	7021200	those pieces of mathematics that have been found are singular points
7021200	7028200	that admit no change if you have a direct interaction
7028200	7030200	that admit no change.
7030200	7033200	If you have come up with the idea of a group,
7033200	7036200	then the rest of the whole story,
7036200	7041200	thousands of pages in mathematical journals,
7041200	7047200	follows by force from the definition of what a group is,
7047200	7051200	a group of finite number of elements.
7051200	7057200	And the same way the shapes that dominate life have this inherent
7057200	7062200	self-consistency that is the different chains of forces
7062200	7065200	that interact, support each other.
7065200	7069200	Christoph, my apologies.
7069200	7071200	Christoph, my apologies.
7071200	7075200	I think we're almost on top of the hour and Michael has to go at 11.
7075200	7079200	So, or, well, in one minute, Michael,
7079200	7083200	any last comments from you before we let you run?
7083200	7085200	I'm sorry for interrupting.
7085200	7087200	This is extremely interesting.
7087200	7089200	Thank you so much for having me here.
7089200	7091200	This was amazing.
7091200	7093200	Thanks for coming.
7093200	7096200	I really wanted to introduce you to Christoph and have one more
7096200	7101200	conversation with you after our lucky podcast.
7101200	7106200	And so I'm very, very happy that you could come and hope to see you
7106200	7108200	again soon and stay in touch.
7108200	7114200	I really like many of your ideas in the space of self-organizing
7114200	7115200	systems.
7115200	7120200	And it's very lucky that we could have you here today.
7120200	7123200	Christoph, do you have some more time to stay on?
7123200	7125200	Yes, I do.
7125200	7126200	Perfect.
7126200	7127200	Thank you very much, everybody.
7127200	7128200	I've got to run.
7128200	7129200	I'm happy to do more.
7129200	7130200	Great meeting you.
7130200	7133200	And I'm fascinated by all the examples that you have shown.
7133200	7136200	It's just great.
7136200	7137200	Cool.
7137200	7138200	Thank you so much.
7138200	7139200	See you all later.
7139200	7140200	Thank you, Michael.
7140200	7141200	Thank you.
7141200	7143200	Bye.
7143200	7147200	So we're going to continue for a little bit more.
7147200	7149200	If it's fine with you, Tanya, you have time, right?
7149200	7152200	Yes, yes, of course.
7152200	7157200	So Christoph, you mentioned that you think that the genome is that
7157200	7160200	contributes to the brain is a gigabyte.
7160200	7162200	But that's the whole of the genome, right?
7162200	7166200	And so it's the part that quotes for the brain is going to be a
7166200	7168200	small fraction of that.
7168200	7172200	Yeah, I wonder how much information is remarkable that the
7172200	7175200	genome has expanded from mouse to man.
7175200	7179200	So making a larger brain doesn't need more genes.
7179200	7180200	Yes.
7180200	7183200	Of course, if you take the genome and you drop it into physics,
7183200	7186200	it's not going to form a cell.
7186200	7191200	So, and every cell that exists is the result, except for the
7191200	7197200	first one of the server application of another cell, right?
7197200	7200200	So all cells in some sense depend on the existence of a cell
7200200	7203200	and the cell is not empty.
7203200	7209200	And I wonder what the comorgo of complexity of the cell itself is
7209200	7210200	right.
7210200	7213200	How complex is this machinery that is being copied and copied all
7213200	7214200	over?
7214200	7217200	Maybe that's much larger than a gigabyte.
7217200	7221200	And of course, the principles of self-organization embodied in the
7221200	7225200	cell lead to the search for more coherent organization on the next
7225200	7226200	level.
7226200	7229200	And so the cell is already an agent, a self-replicator,
7229200	7233200	a Turing machine, and an entropy extractor.
7233200	7236200	And the self-replication is the main feat of the cell to make that
7236200	7238200	happen robustly in physics.
7238200	7240200	And this is what enables everything else.
7240200	7244200	I wonder how much of that we need?
7244200	7251200	Yeah, I could imagine that the brunt of the genetic information
7251200	7254200	needed for a single cell already.
7254200	7260200	And then you may just add, I don't know how much,
7260200	7266200	on top of that in the form of the preexisting cell.
7266200	7269200	Each cell is the daughter of another cell.
7269200	7274200	And so there is some information in the arrangement,
7274200	7278200	at least in the molecular arrangement of the cell,
7278200	7281200	that needs to be counted as information.
7281200	7287200	Although I have a hard time seeing that that will be in a
7287200	7291200	significant way more than a gigabyte.
7291200	7295200	You know, at the present time, there are groups that try to create
7295200	7297200	artificial cells.
7297200	7303200	And they learn the hard time that a lot needs to be in place to
7303200	7307200	make a cell tick, you know, the different kinds of membranes.
7307200	7311200	The membranes shape themselves, that's very important.
7311200	7316200	They shape themselves into the Golgi apparatus,
7316200	7318200	things like that.
7318200	7323200	And so there is information in the preexisting cell.
7323200	7329200	But quantitatively, I don't think it is going to be more than a
7329200	7332200	gigabyte.
7332200	7335200	So I guess if you were to build an intelligent system,
7335200	7340200	say artificial visual cortex, how would you go about it?
7340200	7343200	And what would be the main differences to the existing
7343200	7345200	approaches?
7345200	7350200	Well, I think, you know, when you look at, with your own brain,
7350200	7356200	with your own eyes, at a moving body, you can predict the next
7356200	7362200	split second and compare it to the signals that come in.
7362200	7367200	So we have this machinery in our visual system that is able to do
7367200	7372200	the differential geometry, taking into account the shape of the
7372200	7379200	surface, the play of light on the surface, the movement.
7379200	7391200	And so what I would create is a sequence of an array of ARIA,
7391200	7397200	like V1, the primary visual cortex, and MT, which is concentrated
7397200	7403200	on motion, maybe V3 concentrated on color and so on,
7403200	7409200	and a number of submodalities, which each are two and a half D
7409200	7412200	entities.
7412200	7420200	They refer to the two-dimensional way we perceive the world and the
7420200	7427200	with added internal spaces of quality spaces, like color,
7427200	7431200	a three-dimensional space, and the texture, I don't know what,
7431200	7436200	maybe a 40-dimensional space, depth, a one-dimensional space,
7436200	7439200	motion, a two-dimensional space, and so on.
7439200	7448200	And so they reflect the retinal image on the one hand in these
7448200	7455200	different modalities, and they, another set of such ARIA,
7455200	7461200	they build up the invariant static scene as such,
7461200	7468200	couple them by projection patterns, which have to be dynamic
7468200	7472200	because if you roll your eyes, the image moves.
7472200	7477200	And in order to connect the moving images to the static
7477200	7483200	representation, you need dynamic projection patterns.
7483200	7492200	These are very important in themselves in the deforming
7492200	7498200	projection of the moving retinal image of a rotating object.
7498200	7506200	The deforming projection of that onto a static version of that
7506200	7510200	thing tells you about the shape of the object.
7510200	7519200	And so I think what you need is a huge array of local
7519200	7527200	texture, local modality descriptions, all linked together
7527200	7533200	with dynamic patterns, such that the whole thing is a self-supporting
7533200	7541200	attractor space and describes the external world in detail as far
7541200	7544200	as you concentrate on it, of course.
7544200	7552200	It's a, I think it's quite an amount of work that is necessary.
7552200	7559200	I once tried to put together a company where I believed I would
7559200	7566200	need something like six or eight intelligent co-workers that
7566200	7569200	together create this structure.
7569200	7575200	The first feat to convince investors would be to let the system
7575200	7580200	look at moving objects and build up an instant model of that
7580200	7587200	moving object in its 3D form, an instant replica of that,
7587200	7595200	with the ability to handle it, to connect to self-motion,
7595200	7598200	to connect to manipulative motion.
7598200	7604200	I think, you know, the complete abysmal failure of the car
7604200	7609200	industry to come up with level five autonomy has very much to do
7609200	7615200	with the inability to represent the traffic scene in this sense.
7615200	7621200	And so my idea was I would get, investors would be ready to
7621200	7624200	invest in that direction.
7624200	7629200	However, I found out that this whole perspective of mine is so
7629200	7633200	much sailing against the wind that I wouldn't even find the
7633200	7641200	co-workers to help me create it, let alone the investors.
7641200	7646200	I suspect that part of the difficulty to create self-driving cars
7646200	7649200	has to do with the way in which the model is being generated,
7649200	7653200	which means a deep learning currently relies on building
7653200	7656200	classifiers for individual things.
7656200	7661200	And there is no end-to-end train system for deep learning that is
7661200	7665200	self-driving in a sense, and it is at the same time reliable.
7665200	7669200	If you want to create reliable behavior that is rule-based,
7669200	7672200	that where you basically have a set of traffic laws and safety
7672200	7675200	measures and precautions that are built into the system that
7675200	7680200	drive all the behavior, the object that this system is going
7680200	7684200	to relate to are crafted by hand.
7684200	7688200	So the self-driving car exists in a handcrafted software world
7688200	7692200	where all the objects are being defined by a developer.
7692200	7695200	Whereas the world that we are living in is an open world.
7695200	7698200	And when we see new phenomena, we are able to integrate them
7698200	7700200	into this model.
7700200	7703200	And when the self-driving car sees something new that hasn't
7703200	7706200	seen before that the developer didn't expect, like a bicycle
7706200	7711200	painted on the outside of a truck, this might lead to confusions
7711200	7713200	for the classifiers.
7713200	7718200	Yeah, you made a very important observation that kids learn
7718200	7723200	on the basis of very few examples compared to deep learning.
7723200	7730200	They learn, moreover, in a very simple environment in their nursery
7730200	7737200	with fairy tales and interacting with a few people and playing
7737200	7738200	with objects.
7738200	7742200	And then they walk out into the world and understand traffic
7742200	7743200	situations.
7743200	7746200	You don't hand down the key to the car yet because they don't
7746200	7748200	have a sense of responsibility.
7748200	7753200	They can't foresee the long-term effects of their actions.
7753200	7756200	So you only let them drive when they're 18.
7756200	7761200	But they understand traffic scenes very well when they are six
7761200	7763200	or 10.
7763200	7770200	So all of this is driven by learning by interaction with a
7770200	7774200	simple environment and generalization from there.
7775200	7780200	Yeah, well, of course, in 99.7% of all the cases, the self-driving
7780200	7781200	car is good enough.
7781200	7786200	It's mostly the long tail of cases that leads to situations where
7786200	7790200	the system is producing undesirable behavior.
7790200	7795200	I was joking a couple of years ago that whenever a journalist
7795200	7799200	writes that there will never be self-driving cars, police is
7799200	7803200	stopping Tesla with the sleeping driver safely on their way home.
7804200	7810200	And so in many ways, self-driving cars exist.
7810200	7813200	And they are almost as good enough in the sense that they are
7813200	7815200	better than a really, really bad driver.
7815200	7820200	But they're just not working to the degree of perfection of a
7820200	7821200	very competent driver.
7821200	7826200	Yeah, it's very mean to ask them to be so perfect, much more
7826200	7827200	perfect than humans.
7827200	7831200	They are, as you said, they are, if all cars were self-driving,
7831200	7834200	traffic would be much more safe than now.
7834200	7841200	But the public takes it very badly if an accident happens that
7841200	7843200	could have been prevented.
7843200	7847200	Yeah, we're also in an interesting situation where the public is
7847200	7848200	mostly the media.
7848200	7852200	And the media is at the moment in the US very much seeing itself
7852200	7856200	in competition with the tech industry because they are competing
7856200	7859200	for the same advertising revenue for the most part.
7859200	7864200	And so it's at the moment very difficult to find articles that
7864200	7868200	are optimistic and positive about technological developments
7868200	7870200	in the media, I find.
7870200	7874200	So this creates a very unique situation where even useful
7874200	7878200	developments are delayed that could save lives.
7878200	7882200	Because they're being seen in competition with existing
7882200	7886200	economic and social structures, which also creates enormous
7886200	7890200	pressure on AI models like JetGPT.
7890200	7893200	I think that JetGPT is a tremendous achievement.
7893200	7895200	My kids have been playing with it.
7895200	7900200	My daughter has been creating a story of a horse that she got to
7900200	7903200	know on the way home from school and then created several
7903200	7906200	variants by modifying the prompt until she had the story that
7906200	7907200	she liked.
7907200	7911200	And then she turned it into a poem that's very catchy rhymes.
7911200	7916200	My son used it to explain the system,
7916200	7919200	to explain to him how to implement a platformer game.
7919200	7921200	And it was explaining him how to structure the project.
7921200	7925200	And then he was asking how to make an event loop in Python
7925200	7929200	and it printed out source code and explained the source code.
7929200	7933200	And he spent several hours copying the code and replete
7933200	7934200	and getting it to work.
7934200	7938200	So to me, these are systems where you have a little bit of human
7938200	7941200	in the loop to make it coherent for a particular task.
7941200	7945200	And it's amazing what the thing can already do.
7945200	7949200	And it seems to me what's missing to get the system to work is to a
7949200	7951200	system that makes it coherent.
7951200	7956200	Basically, you can decompose the mind into perceptual systems that
7956200	7961200	can in some sense to image guided and audio guided diffusion to
7961200	7964200	coalesce to an internal state that is able to reproduce the
7964200	7965200	sensory data.
7965200	7969200	And then confabulation to build alternatives for solutions,
7969200	7972200	alternatives for what could be alternatives for the future.
7972200	7976200	And then the third component, which doesn't exist yet, which is
7976200	7980200	proving from first principles what works, basically rejecting
7980200	7982200	those generations that don't work.
7982200	7986200	And then learning those that worked and building up the system
7986200	7989200	in a way that is continuously learning.
7989200	7992200	It also seems to me that many people cannot change their
7993200	7995200	opinion in real time.
7995200	7998200	And you have talked to a person that has a strong opinion about
7998200	8000200	something that moves deeply into their mind.
8000200	8004200	You can present them as arguments, but you have to talk to them the
8004200	8007200	next day if you want to see any changes, which seems that seem to
8007200	8011200	be parts of mental organization at least in some people require
8011200	8013200	offline retraining.
8013200	8016200	There's limits to what we can do in online learning.
8016200	8019200	Some balancing needs to be done offline while we are decoupling the
8019200	8022200	system from the environment and producing data augmentation and
8022200	8024200	restructuring.
8024200	8027200	I wonder how much of that retraining will also be built into the
8027200	8030200	systems where the artificial systems will have to sleep and to
8030200	8032200	dream.
8032200	8037200	Yeah, before you take an important decision, you have to sleep
8037200	8042200	over it and give your subconscious mind the opportunity to
8042200	8048200	work on making the ideas more consistent than you are able to
8048200	8051200	make them under conscious control.
8055200	8057200	Yeah, very much so.
8057200	8068200	So I think you rightly said these achievements of GPT-3,
8068200	8071200	TETGTP and so on are extremely impressive.
8071200	8074200	It's very difficult to see where the limit is.
8074200	8076200	I agree with you.
8077200	8083200	The transformers have a new, very new architectural feature,
8083200	8089200	which is the online computation on the fly of connections and
8089200	8093200	of these representation vectors.
8093200	8096200	They are computed on the fly.
8096200	8098200	That's all very promising.
8098200	8105200	But these systems don't have any insight into real world
8106200	8111200	geometric mechanics and so on representations of what they talk
8111200	8113200	about.
8113200	8117200	And they are lambasted mainly for that reason, that they don't
8117200	8121200	know what it means, something is dead.
8121200	8128200	They just know how people talk about it, but they don't know the
8128200	8133200	significance of it or the geometrical arrangement of something.
8133200	8138200	And so that is, of course, due to lack of insight, lack of
8138200	8144200	interaction, you know, they cannot play with toy objects as kids
8144200	8150200	do and cannot get the corresponding insights.
8150200	8158200	But I still think that what is missing, what is sort of needs to
8158200	8163200	be improved is the data structure of representation of
8163200	8167200	themes and of realities.
8167200	8174200	And I don't think these vectors that I use these days are up to
8174200	8177200	the job.
8177200	8181200	I think that the embedding spaces are not necessarily represented
8181200	8183200	in full, right?
8183200	8187200	If you think about the embedding space as a manifold with 30,000
8187200	8191200	dimensions and a lot of resolution, trying to expand this space
8191200	8195200	and store it in memory is not going to be feasible for the most
8195200	8197200	part.
8197200	8200200	So instead what is required is a language that allows to sparsely
8200200	8203200	and efficiently construct representations in that embedding
8203200	8205200	space.
8205200	8209200	The embedding space is a mathematical construct that is basically
8209200	8213200	every dimension is a function that describes a feature.
8213200	8216200	And that feature has parameters.
8216200	8220200	But I see every dimension is a parameter in that feature.
8220200	8222200	Yeah, that's right, of course.
8222200	8228200	So, you know, I once applied for funds to do face recognition and
8228200	8234200	the idea was to collect data images which varied in all those
8234200	8238200	dimensions, the identity of the person, the illumination, the
8239200	8242200	expression, the texture and so on.
8242200	8246200	And I didn't get the money and I'm very glad I didn't get the money
8246200	8251200	because this 15-dimensional space or so cannot be filled with
8251200	8253200	examples.
8253200	8255200	That's totally impossible.
8255200	8258200	There's too much space in high-dimensional spaces.
8258200	8261200	That's the point you want to make, I suppose.
8261200	8266200	So one has to find a way of representing only sub-dimensions,
8267200	8273200	low-dimensional projections of that and a means of pasting them
8273200	8278200	together as an equivalent of the high-dimensional thing.
8278200	8282200	It turns out that when we conceptualize an object, it's chunked
8282200	8287200	and a chunk is basically a node that is composed of features
8287200	8290200	that define the nature of that bunk.
8290200	8293200	And you have these seven plus minus two features.
8293200	8297200	It's less, so it's more like five, which means that if you can
8297200	8301200	define an object by five features, you have a local function,
8301200	8303200	a locally five-dimensional space.
8303200	8305200	Maybe sometimes it's nine-dimensional, but it's not much
8305200	8307200	more.
8307200	8311200	And these few dimensions allow us to construct a family of
8311200	8316200	operators that would allow us up to these few dimensions construct
8316200	8322200	all the 30,000 other dimensions or millions of other dimensions
8322200	8325200	depending on how we look at this function space.
8325200	8326200	Right.
8326200	8332200	So the essential point here is that a high-dimensional thing gets
8332200	8337200	projected down in our brain onto low-dimensional representations
8337200	8341200	plus the ability to glue them together.
8341200	8345200	And this gluing together, I don't see in the present
8345200	8349200	neural technology.
8349200	8352200	I think that it happens on a level that we normally don't look
8352200	8353200	at.
8353200	8355200	It happens in the activation traces in the network.
8355200	8358200	So it's not in the weights between the links.
8358200	8362200	And it's also not in the synaptic connections between neurons.
8362200	8366200	It is in the content of the traces that are moving through this.
8366200	8372200	So the neurons and the nodes in the neural network are providing
8372200	8376200	the computational machinery to modulate these patterns according
8376200	8378200	to the content of the patterns.
8378200	8382200	And it's the content of the activation wave that is determining
8382200	8387200	how the activation wave is being processed.
8387200	8391200	This is something like a distributed computational pipeline.
8391200	8397200	I'm involved in a multi-month or probably multi-year intensive
8397200	8402200	discussion with a colleague at Amy in Zurich,
8403200	8406200	Institute of Neural Informatics, Matthew Cook.
8406200	8411200	And he doesn't want to hear of dynamic mappings.
8411200	8418200	He says anything like schemata and roll fillers,
8418200	8421200	that's all nonsense, he believes.
8421200	8428200	And he claims all you need is components,
8428200	8430200	which he calls them slips of paper,
8430200	8435200	onto which various features are written into slips of paper
8435200	8438200	can overlap in a subset of the features.
8438200	8445200	And so that makes clear how they belong together.
8445200	8453200	Components, each of which is a small set of entities.
8453200	8457200	And they overlap in subsets of these entities.
8457200	8462200	And that way they can cover a complex situation.
8462200	8471200	I defy him again and again to create that way a system
8471200	8475200	that can do something like invariant object recognition.
8475200	8480200	Or the application of a syntactical rule,
8480200	8485200	like subject, verb, object to an arbitrary set of components,
8485200	8490200	of appropriate components, of course, to nouns and the verb.
8490200	8493200	And which is, of course, very important,
8493200	8497200	the cognitive scientists insist on that,
8497200	8502200	on the ability to impose an abstract pattern
8502200	8504200	onto concrete elements.
8504200	8508200	And I think for that you have to have variables
8508200	8514200	that make clear this abstract node belongs to this concrete node.
8514200	8519200	If you want to represent the sentence John loves Mary,
8519200	8523200	you have to make clear that the subject is linked to John
8523200	8525200	and the object is linked to Mary,
8525200	8530200	because you can also have the sentence Mary loves John
8530200	8534200	and then they are called in a different way.
8534200	8537200	You need variables to make that distinction.
8537200	8539200	What are those variables?
8539200	8542200	That's what I call the glue,
8542200	8546200	that glues together the abstract form and the concrete elements
8546200	8548200	that make it up, for instance.
8548200	8553200	Or the texture, you know, the computer graphics people
8553200	8558200	have a very good ontological theory of visual scenes.
8558200	8561200	They can create them in a very convincing way
8561200	8564200	and they create them out of part descriptions
8564200	8569200	like shape only or texture only or illumination only
8569200	8572200	or motion applied to a shape
8572200	8576200	and have a way of putting them together.
8576200	8580200	And for that you need to have variables
8580200	8584200	that make clear this textural element belongs
8584200	8590200	onto this form, on this point on that form.
8590200	8592200	And what are those variables?
8592200	8595200	We need to find the minimal set of link types
8595200	8597200	that we need to...
8597200	8599200	One minimal set.
8599200	8601200	We didn't optimize for the smallest one
8601200	8604200	to perform all the semantic representations
8604200	8606200	that we wanted to have.
8606200	8611200	And the model that we used was inspired by Aristotle
8611200	8614200	and his Four Causa,
8614200	8618200	which basically means you have Causa formalis
8619200	8624200	and these four cases describe
8624200	8627200	partonomic links part of and being composed of
8627200	8630200	and being caused by and leading to.
8630200	8632200	So basically you have lateral links
8632200	8634200	that give you a causal ordering
8634200	8637200	and you have compositional links
8637200	8640200	that allow you to compose a script
8640200	8643200	or a task of subtasks.
8643200	8647200	And in this way you can describe arbitrary scripts
8647200	8651200	and these arbitrary scripts can express arbitrary functions
8651200	8654200	when you combine them with low level operators
8654200	8657200	that can, for instance, perform some basic operations
8657200	8660200	on the network, sense data in the environment
8660200	8663200	in the network itself and so on.
8663200	8666200	I mentioned earlier on that I think of intelligence
8666200	8669200	as the ability to construct a path
8669200	8671200	to a space of computable functions.
8671200	8673200	So intelligence is not the ability to compute the function,
8673200	8675200	every computer can compute a function
8675200	8677200	without being intelligent.
8677200	8680200	The trick is to discover that function in the first place
8680200	8682200	and to discover this function
8682200	8685200	we basically have three perspectives on how to do this.
8685200	8687200	The first one is to converge to it.
8687200	8689200	That is what deep learning does.
8689200	8692200	You have a space of possible functions
8692200	8695200	and in that space of possible functions you make large enough
8695200	8697200	you start out with some random function
8697200	8700200	and then you modify that function along many dimensions
8700200	8704200	nudge it many billions of times or trillions of times
8704200	8707200	until it gets close to what you wanted to do.
8707200	8710200	And you follow a gradient.
8710200	8712200	For this it needs to be differentiable
8712200	8715200	and this algorithm of stochastic gradient descent
8715200	8718200	using back propagation is still the workhorse
8718200	8720200	of all machine learning at this point.
8720200	8724200	And a second approach is to do hierarchical pattern matching.
8724200	8727200	So you look for operators that you've already learned,
8727200	8730200	a small library of efficient operators that you have evolved
8730200	8734200	and evolution is all you need from this perspective
8734200	8737200	that lets you get to the situation that you want
8737200	8740200	based on the configuration that you have.
8740200	8743200	So these operators are basically looking for activation patterns
8743200	8745200	that they match on
8745200	8748200	and they change the activation pattern into the next one.
8748200	8752200	And in this way you can perform arbitrary functions in the way
8752200	8757200	this is also the way in our computers are computing functions.
8758200	8761200	And the third one is construction
8761200	8765200	and construction requires some degree of memory
8765200	8768200	and because you need to be able to retrace your steps
8768200	8771200	and you need a way to justify the steps that you're making
8771200	8774200	and when to retract them.
8774200	8777200	Our consciousness seems to be strongly involved
8777200	8779200	in such a construction process
8779200	8782200	where we have a stream of consciousness that allows me
8782200	8785200	oh I tried this thought before and this didn't work
8785200	8789200	so I now retrace it, retract it, modify it
8789200	8792200	and I think this one should work for the following reasons
8792200	8794200	and then I see the outcome and say no it didn't work
8794200	8797200	so this reason was not wrong so I try the next thing
8797200	8799200	and this is something that is difficult to achieve
8799200	8802200	just this pattern matching or this gradient descent.
8802200	8806200	So this constructive discovery of solutions
8806200	8808200	seems to be crucial
8808200	8812200	and while it seems to me that our deep learning models
8812200	8814200	are not very good at constructing
8814200	8816200	they are very much able to emulate
8816200	8818200	what it would be like to be constructing, right?
8818200	8822200	So while GPT-3 or chat GPT are not conscious
8822200	8825200	they are able to create a story about something
8825200	8827200	that is conscious while they are unable to reason
8827200	8829200	they create a story about the reasoner
8829200	8832200	and draw on the inferences from that reasoning
8832200	8835200	and the more closely you describe the reasoning
8835200	8838200	that's reason step by step and so on
8838200	8840200	the better the results can become
8840200	8843200	and so it's very difficult to determine the difference
8843200	8847200	between a system that pretends to perform a certain thing
8847200	8849200	and it actually does it, right?
8849200	8853200	If you can pretend it well enough you're actually doing it.
8853200	8857200	Yeah, yeah, so the same way as you pretend to be conscious
8857200	8862200	and I know the only conscious being in the world is myself.
8863200	8866200	When did you figure that out?
8866200	8870200	We know, we know, we're NPCs.
8870200	8873200	How did I figure that out?
8873200	8876200	Okay, yeah.
8876200	8878200	Christoph, I enjoyed this very much.
8878200	8881200	We are at the end of my time limit for now
8881200	8884200	and I hope that we get to continue the conversation
8884200	8887200	and I hope that we get to continue the conversation
8887200	8890200	and I hope that we get to continue the conversation
8890200	8896200	soon in Zurich and looking forward to talking more to you
8896200	8900200	and I'm very glad that you could make it today.
8900200	8902200	Yeah, it was a great pleasure.
8902200	8907200	I just had to take an earlier train from Frankfurt to Berlin
8907200	8911200	because I had to be there only tomorrow morning.
8911200	8915200	I'm sitting here in the room in the so-called Harnak House
8915200	8917200	of the guest house.
8917200	8920200	Okay, have a nice day.
8920200	8923200	Thank you so much for organizing this
8923200	8926200	and setting everything up and supporting our discussion
8926200	8929200	and I'd also like to thank our audience
8929200	8931200	for paying attention, asking questions.
8931200	8936200	We didn't get to discuss all of them
8936200	8940200	but I'm very glad that you could make this event happen.
8940200	8942200	Thank you, Christoph.
8942200	8944200	See you soon.
8944200	8946200	Thank you, everyone.
8946200	8947200	Bye-bye now.
8947200	8948200	Bye-bye.
