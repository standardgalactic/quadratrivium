WEBVTT

00:00.000 --> 00:05.000
Welcome everyone from and hello from Rainey, California.

00:05.240 --> 00:07.600
Today we're hosting an exciting event

00:07.600 --> 00:10.080
on the future of General SDI.

00:10.080 --> 00:11.600
Our guests today are professors

00:11.600 --> 00:14.600
Christoph Fondrell-Malsburg and Michael Levin.

00:14.600 --> 00:16.720
Michael Levin is a distinguished professor

00:16.720 --> 00:18.160
at the Department of Biology

00:18.160 --> 00:20.800
and the principal investigator at the Levin Lab

00:20.800 --> 00:22.720
at Tufts University.

00:22.720 --> 00:25.160
Christoph Fondrell-Malsburg is a senior fellow

00:25.160 --> 00:27.360
at the Frankfurt Institute for Advanced Studies

00:27.360 --> 00:28.560
and the visiting professor

00:28.560 --> 00:33.440
at the Institute of Neuroinformatics at ETH Zurich.

00:33.440 --> 00:35.920
And finally, our host today is Dr. Yoshabach,

00:35.920 --> 00:39.760
my colleague and the principal engineer at Intel Labs.

00:39.760 --> 00:41.360
My name is Tania Greenberg.

00:41.360 --> 00:44.640
I am an AI scientist at Intel Labs also.

00:44.640 --> 00:46.960
We would like to express our thanks to Intel Labs

00:46.960 --> 00:50.400
for sponsoring and hosting this event series.

00:50.400 --> 00:53.840
So without further ado, let's get started.

00:53.840 --> 00:56.720
The audience is going to be muted by default for this event.

00:56.720 --> 00:59.400
If you would like to ask a question or make a comment,

00:59.400 --> 01:01.560
please post your thoughts in the chat.

01:01.560 --> 01:03.480
You should have access to it.

01:03.480 --> 01:06.480
We will have a hopefully longer than usual Q&A session

01:06.480 --> 01:07.800
at the end of today's session.

01:07.800 --> 01:10.920
So we should be able to go through many of your questions.

01:10.920 --> 01:13.400
All right, Yoshab, the floor is yours.

01:14.800 --> 01:18.400
Okay, I proceed with our slides for now.

01:18.400 --> 01:19.240
Let's see.

01:20.080 --> 01:22.360
The question that we want to discuss

01:22.360 --> 01:25.080
is how we can build intelligent systems

01:25.080 --> 01:30.200
that move beyond the present limitations of deep learning.

01:30.200 --> 01:32.440
And this is quite a bold proposal

01:32.440 --> 01:35.080
because you don't know if deep learning has any limitations.

01:35.080 --> 01:36.160
At least I don't know.

01:36.160 --> 01:39.160
I don't know any proof that demonstrates

01:39.160 --> 01:40.720
that there is a certain thing

01:40.720 --> 01:43.200
that the methods of deep learning cannot do.

01:43.200 --> 01:46.200
It seems to be that the present generation of models,

01:46.200 --> 01:48.440
which is widely successful,

01:48.440 --> 01:51.360
generative AI is built on the transformer algorithm,

01:51.360 --> 01:54.800
which is a variant of the deep learning mechanisms

01:54.800 --> 01:58.160
that have been existing since the perceptron.

01:58.160 --> 02:02.560
And the limitations of this thing are not clear, right?

02:02.560 --> 02:04.920
We basically get to meaning in the limit.

02:04.920 --> 02:08.720
We need to use massive data and compute to get there.

02:08.720 --> 02:11.080
And these models have difficulty

02:11.080 --> 02:12.640
to become fully coherent,

02:12.640 --> 02:14.920
but it's not obvious that such a limitation

02:14.920 --> 02:16.840
cannot overcome as a loss function

02:16.840 --> 02:19.720
or this just using the existing regimes

02:19.720 --> 02:22.080
and scaling them up even further, right?

02:22.080 --> 02:26.080
We cannot prove that there's a combination of codecs

02:26.080 --> 02:28.320
and the existing methods and some real time

02:28.320 --> 02:30.240
and some online learning.

02:30.240 --> 02:31.720
We won't be able to build a system

02:31.720 --> 02:34.840
that is a good enough artificial general intelligence

02:34.840 --> 02:36.920
and is able to discover the next generation

02:36.920 --> 02:38.760
of algorithms for us.

02:38.760 --> 02:40.840
And this idea that deep learning

02:40.840 --> 02:42.320
in the way that it exists

02:42.320 --> 02:44.920
with basically the algorithms that we have right now,

02:44.920 --> 02:47.600
with slight changes to the loss functions and so on,

02:47.600 --> 02:49.440
that it is sufficient.

02:49.440 --> 02:51.960
This is called the scaling hypothesis.

02:52.640 --> 02:55.760
On the other hand, most people who work in the field

02:55.760 --> 03:00.360
have the sense that there is a problem with deep learning

03:00.360 --> 03:02.840
in the sense that it's boot forcing the job.

03:02.840 --> 03:05.160
It seems that biological organisms

03:05.160 --> 03:06.720
are able to make sense of the world

03:06.720 --> 03:10.680
with much, much less data and dramatically less compute.

03:10.680 --> 03:13.240
And there is of course a contention

03:13.240 --> 03:16.240
about how much compute a brain has

03:16.240 --> 03:18.760
and how much compute an organism has.

03:18.760 --> 03:21.720
And very often the calculation is made in such a way

03:21.720 --> 03:25.800
that we discuss how many GPUs you need

03:25.800 --> 03:28.560
to emulate a group of neurons.

03:28.560 --> 03:31.000
And depending on how you look at the neuron,

03:31.000 --> 03:34.560
the neuron might be something like a four or 12 layer

03:34.560 --> 03:38.320
neural network or it might be something even more complicated

03:38.320 --> 03:41.760
if you look down to modeling every synapse and so on.

03:41.760 --> 03:45.120
And on the other hand, relatively rarely we ask

03:45.120 --> 03:47.840
how many brains you would need to run macOS

03:47.840 --> 03:50.080
because the way our nervous system works

03:50.080 --> 03:53.120
is highly redundant and stochastic.

03:53.120 --> 03:55.760
And much of what the individual neuron is doing

03:55.760 --> 03:58.480
is probably only relevant to the individual neuron

03:58.480 --> 03:59.880
in its survival itself,

03:59.880 --> 04:03.000
because after all the neuron is a single celled animal.

04:03.000 --> 04:07.640
So if you take the job of an individual,

04:07.640 --> 04:09.360
for instance in a corporation,

04:09.360 --> 04:12.280
that individual is going to contribute

04:12.280 --> 04:14.960
a lot of its intellectual capability to the corporation,

04:14.960 --> 04:17.720
but it's going to be a tiny fraction

04:17.720 --> 04:20.000
of the total ability of the individual

04:20.000 --> 04:21.720
that it needs to survive by itself

04:21.720 --> 04:24.240
and to communicate with its immediate neighborhood.

04:24.240 --> 04:28.840
So just asking to emulate a single neuron

04:28.840 --> 04:31.600
and then multiplying this with the number of neurons

04:31.600 --> 04:33.720
is probably not the right way to do it.

04:33.720 --> 04:35.480
And it's not clear how many neurons

04:35.480 --> 04:37.240
you would need to run MNIST.

04:37.240 --> 04:40.920
As far as I know, there is no simulation so far

04:40.920 --> 04:43.320
that is using a close neural model

04:43.320 --> 04:47.440
or that is using a small group of neurons in vivo

04:47.440 --> 04:50.600
that are being trained to do MNIST.

04:52.280 --> 04:55.480
More generally, I think we need to answer the question

04:55.480 --> 04:56.520
what intelligence is.

04:56.520 --> 04:59.800
And for me, this question has evolved over the years

04:59.800 --> 05:03.800
and my current answer is that intelligence is the ability

05:03.800 --> 05:07.320
to construct a path through a space of computable functions.

05:08.560 --> 05:12.200
It's a slightly fancy way of saying

05:12.200 --> 05:14.760
that intelligence is the ability to make models

05:14.760 --> 05:16.920
because at the end of the day,

05:16.920 --> 05:19.040
models are computable functions

05:19.040 --> 05:24.120
that are designed in the service of control.

05:24.120 --> 05:27.920
So we have a system that is making some kind

05:27.920 --> 05:29.360
of regulation task

05:29.360 --> 05:31.520
or performing some kind of regulation tasks

05:31.520 --> 05:34.840
and when it's controlling the future, it's an agent.

05:34.840 --> 05:36.960
And to control the future and being an agent,

05:36.960 --> 05:40.200
you will need to construct some kind of control model

05:40.200 --> 05:41.240
that is counterfactual.

05:41.240 --> 05:43.320
You need to be able to represent states

05:43.320 --> 05:44.240
that are not here yet

05:44.240 --> 05:46.160
because the future is not there.

05:46.160 --> 05:48.480
And that means you need to have a system

05:48.480 --> 05:52.080
that is able to perform arbitrary causal transitions.

05:52.080 --> 05:54.600
And this causal insulation from the substrate

05:54.600 --> 05:57.440
to represent something that is not the case yet,

05:57.440 --> 06:00.640
that is basically what is a computer.

06:00.640 --> 06:03.880
And for the computer it does, it represents functions

06:03.880 --> 06:06.720
which means mapping from state descriptions

06:06.720 --> 06:08.640
to other state descriptions.

06:08.640 --> 06:12.880
And the computer can do this in completely arbitrary ways.

06:12.920 --> 06:14.720
And this is the basic idea

06:14.720 --> 06:18.160
of building a system of computable functions.

06:18.160 --> 06:21.440
And when you come up with a way

06:21.440 --> 06:23.320
to enumerate the computable functions,

06:23.320 --> 06:26.040
to list them all, then you can search them.

06:26.040 --> 06:28.200
And if you want to learn something,

06:28.200 --> 06:30.920
you need to in some sense enumerate,

06:30.920 --> 06:33.160
organize the space of computable functions

06:33.160 --> 06:35.280
in such a way that you find those functions

06:35.280 --> 06:38.200
that you are interested relatively early on.

06:38.200 --> 06:41.560
And this question of how you can construct a space

06:41.560 --> 06:43.440
for the computable functions that is tractable,

06:43.440 --> 06:46.160
that is able to converge to a solution

06:46.160 --> 06:47.560
for the problem at hand.

06:47.560 --> 06:48.760
That is the main issue, right?

06:48.760 --> 06:51.080
Making computer is relatively easy,

06:51.080 --> 06:53.040
especially if you have something like a cell

06:53.040 --> 06:55.040
because it's already contains a computer

06:55.040 --> 06:56.920
and they can also organize themselves

06:56.920 --> 06:58.680
into higher level computers

06:58.680 --> 07:02.920
and become less stochastic and so on while doing this.

07:02.920 --> 07:05.160
But the big difficulty is,

07:05.160 --> 07:09.520
how can we find the functions that we are interested in?

07:09.520 --> 07:11.200
And so when we look at deep learning,

07:11.200 --> 07:12.960
what is deep learning?

07:12.960 --> 07:15.080
The first thing that we have to note about deep learning

07:15.080 --> 07:17.520
is that deep learning is the only thing

07:17.520 --> 07:19.800
that currently works at scale.

07:19.800 --> 07:21.800
It's the only class of algorithms

07:21.800 --> 07:25.720
that is able to discover arbitrary functions

07:25.720 --> 07:28.040
in a reasonable amount of time.

07:28.040 --> 07:32.400
And reasonable being orders of magnitude more time

07:32.400 --> 07:35.720
and more data than a human being does.

07:35.720 --> 07:38.200
That's of course, the training time for something

07:38.200 --> 07:43.200
like diffusion stability AI model, right?

07:43.680 --> 07:46.680
This stable diffusion is a model of two gigabytes

07:46.680 --> 07:50.560
and it contains the whole of the art

07:50.560 --> 07:53.520
and you can get every celebrity you want,

07:53.520 --> 07:57.080
you can get every spaceship from popular culture,

07:57.080 --> 07:58.520
you can get dinosaurs,

07:58.520 --> 08:00.800
everything is in these two gigabytes.

08:00.800 --> 08:04.240
And training this takes weeks.

08:04.240 --> 08:07.360
And it sounds that's very little compared

08:07.360 --> 08:10.480
to the years that it takes to train a human brain.

08:10.480 --> 08:15.480
But during these weeks and now it comes down closer to days,

08:16.640 --> 08:21.040
this thing is going through hundreds of millions of images.

08:21.040 --> 08:22.960
Many more than a human being could process

08:22.960 --> 08:25.560
and the lifetimes and find correlations between

08:25.560 --> 08:28.600
using dramatically large server farms.

08:29.880 --> 08:32.080
So how does deep learning work?

08:32.080 --> 08:34.720
First of all, it's differentiable computing,

08:34.720 --> 08:37.200
which means it is representing all the functions

08:37.200 --> 08:39.680
in such a way that they form a continuous space

08:39.680 --> 08:43.440
in which neighboring variations of the functions

08:43.440 --> 08:45.640
still lead to interesting results.

08:45.640 --> 08:48.960
And you can move by a small nudges through the space

08:48.960 --> 08:51.840
of functions to get the function closer to what you want.

08:51.840 --> 08:54.240
That's different from discrete programs,

08:54.240 --> 08:55.600
program and source code.

08:55.600 --> 08:57.400
If you change a few bytes in the source code,

08:57.400 --> 08:59.120
the program will no longer work.

08:59.120 --> 09:02.560
If you change the neural network by a slight bit,

09:02.560 --> 09:05.440
then it's still going to give you useful results.

09:06.440 --> 09:09.600
And to do this, the neural network is describing

09:09.600 --> 09:14.600
the functions via weighted sums of inputs

09:15.400 --> 09:18.880
that are arranged into chains, basically in layers.

09:18.880 --> 09:21.360
And some non-linearities, which are in a way to make

09:21.360 --> 09:25.080
if sense, to make conditional breaks in this network.

09:25.080 --> 09:29.120
And this description via multiplications and sums

09:29.120 --> 09:32.280
is sufficient to represent all the functions that we want.

09:32.280 --> 09:34.840
And we train this network by setting it up

09:34.840 --> 09:38.080
in such a way that it has the potential to build enough things

09:38.080 --> 09:40.040
that has enough links between the nodes

09:40.040 --> 09:41.840
that it sums values up.

09:41.840 --> 09:43.720
And then it just changes the weights,

09:43.720 --> 09:46.440
which means the multiplying factors

09:46.440 --> 09:50.320
for the inputs of every node in a systematic way.

09:50.320 --> 09:53.480
And it, in some sense approximates all the functions

09:53.480 --> 09:54.560
via real numbers.

09:55.480 --> 09:57.920
And if you think about alternatives to deep learning,

09:57.920 --> 10:02.240
the main thing that comes to mind is to build up computations

10:02.240 --> 10:04.800
from deterministic discrete operators.

10:04.800 --> 10:09.000
Such as Boolean logic or simple automata,

10:09.000 --> 10:10.840
like cellular automata.

10:10.840 --> 10:13.880
And as a result, we get finite Turing machines.

10:15.160 --> 10:16.880
And in the finite case,

10:16.880 --> 10:19.640
dynamical systems and Turing machines

10:19.640 --> 10:23.040
and automata are the same thing, computationally speaking.

10:23.040 --> 10:25.000
They're all Turing machines,

10:25.000 --> 10:28.200
as long as you don't run out of resources.

10:28.200 --> 10:32.120
So every digital computer in reality is a dynamical system

10:32.120 --> 10:34.240
because the physics that digital computers

10:34.240 --> 10:36.880
and prevains on is somewhat continuous

10:36.880 --> 10:39.680
from the perspective of the individual transistors

10:39.680 --> 10:40.520
and so on.

10:40.520 --> 10:43.240
And we just try to find a region in physics

10:43.240 --> 10:45.200
that makes the transistor reliable enough

10:45.200 --> 10:47.560
as a discrete unit and deterministic

10:47.560 --> 10:50.040
from the perspective of the logical language

10:50.040 --> 10:53.000
that is implemented on the arrangement of transistors.

10:53.000 --> 10:55.800
But underneath, there is an analog system.

10:55.800 --> 10:57.080
It is just noisy.

10:57.080 --> 11:00.720
And that is a system that is discrete.

11:00.720 --> 11:03.840
On the other hand, every dynamical systems in physics

11:03.840 --> 11:06.000
at the lowest level is discrete again.

11:06.000 --> 11:09.000
If you zoom in, what you see is nothing continuous.

11:09.000 --> 11:11.040
What you see is individual atoms

11:11.040 --> 11:13.200
and individual charges and so on.

11:13.200 --> 11:15.440
And they are discrete again.

11:15.440 --> 11:17.960
And there is some discussion about

11:17.960 --> 11:20.200
whether everything has to be discrete at the level

11:20.200 --> 11:22.080
because of the nature of languages itself.

11:22.080 --> 11:23.680
And I think that's the case.

11:23.680 --> 11:27.280
I think that the discovery of the last century

11:27.280 --> 11:28.960
that was most important in philosophy

11:28.960 --> 11:31.840
is that those languages which assume

11:31.880 --> 11:34.600
that the bottom most layer can be truly continuous.

11:34.600 --> 11:36.760
They run into contradictions.

11:36.760 --> 11:38.960
But this only matters if you are really interested

11:38.960 --> 11:41.360
in modeling the bottom most layer.

11:41.360 --> 11:44.040
If you just think about computation,

11:44.040 --> 11:45.720
it doesn't really matter if you start out

11:45.720 --> 11:49.400
with a dynamical system or this discrete system

11:49.400 --> 11:52.000
as long as you are willing to allow

11:52.000 --> 11:55.000
that every dynamical system has only finite resolution.

11:55.000 --> 11:57.320
So there's only finitely many bits

11:57.320 --> 12:00.360
that you can manipulate at any given moment.

12:01.880 --> 12:06.400
And this equivalence between the continuous mathematics

12:06.400 --> 12:07.560
and the discrete mathematics

12:07.560 --> 12:09.800
has been shown basically in both directions.

12:09.800 --> 12:11.400
You can use a computer.

12:11.400 --> 12:13.080
And then just by using more bits,

12:13.080 --> 12:15.440
you can approximate continuous system

12:15.440 --> 12:17.640
with any degree of fidelity as you want

12:17.640 --> 12:20.000
in the same way as digitized music

12:20.000 --> 12:23.600
can sample the space of audio functions

12:23.600 --> 12:26.680
below the level of resolution

12:26.680 --> 12:29.600
that your medium can provide.

12:29.640 --> 12:32.240
So you will get to something that is equivalent.

12:32.240 --> 12:35.960
And the opposite direction has also been shown.

12:35.960 --> 12:38.080
Greg Chaitin has gone to the trouble

12:38.080 --> 12:40.640
to translate a LISP interpreter

12:40.640 --> 12:45.640
into a diophantine equation with 17,000 variables.

12:45.720 --> 12:47.320
Right, so this paper is called

12:47.320 --> 12:49.920
the Complete Arithmeticization of Evil.

12:49.920 --> 12:52.840
It's the evaluation function of LISP,

12:52.840 --> 12:57.040
which did in 1987, it's quite beautiful.

12:57.040 --> 12:58.600
And I don't think that is something

12:58.600 --> 13:00.040
people actually want to read.

13:00.040 --> 13:03.880
And this formula was generated

13:03.880 --> 13:05.840
with some generative procedure.

13:05.840 --> 13:09.000
He did not write these 900,000 characters by hand

13:09.000 --> 13:10.280
that went into this.

13:10.280 --> 13:12.720
But it's been shown, you can write down

13:12.720 --> 13:16.000
the LISP interpreter in continuous mathematics.

13:17.040 --> 13:18.960
So the alternative to deep learning

13:18.960 --> 13:21.440
might be to basically construct functions

13:21.440 --> 13:24.280
from the ground up using automata.

13:24.280 --> 13:28.360
And in a way, this is also what people have done

13:28.480 --> 13:31.760
because all our computers on which we run deep learning

13:31.760 --> 13:33.160
are discrete automata.

13:34.040 --> 13:36.680
People started to build this continuous arithmetic,

13:36.680 --> 13:41.480
they built end or addition, multiplication and so on

13:41.480 --> 13:43.920
from discrete logical units.

13:43.920 --> 13:45.080
And this is clearly practical,

13:45.080 --> 13:47.320
but it was done with relatively few people

13:47.320 --> 13:49.440
and relatively few years.

13:49.440 --> 13:53.160
The search process to build up continuous arithmetic

13:53.160 --> 13:56.240
from discrete logical operations is not very large.

13:56.240 --> 13:59.840
And if you want to get, to discover a deep learning

13:59.840 --> 14:02.880
algorithm as a special case over discrete automata

14:02.880 --> 14:06.280
from scratch, it's the existence proof

14:06.280 --> 14:07.920
has been shown by the computers

14:07.920 --> 14:09.920
and the deep learning mechanisms that we have.

14:09.920 --> 14:13.640
Because relatively few people use relatively little brain power

14:13.640 --> 14:15.520
compared to where you want to get to

14:15.520 --> 14:17.840
to discover all these solutions.

14:17.840 --> 14:20.080
By self-play, you could in the same way

14:20.080 --> 14:23.600
as computers played a goal, discover arithmetic

14:23.600 --> 14:26.000
but using discrete systems.

14:26.000 --> 14:28.640
So in many ways, this is going to be equivalent.

14:28.640 --> 14:30.800
And I suspect it might be interesting to start

14:30.800 --> 14:34.000
from this automata direction again and build up learning.

14:34.000 --> 14:36.320
And there are a few people which work in this area

14:36.320 --> 14:39.760
but so far nobody has come up with an alternative

14:39.760 --> 14:42.920
that scales up in the same way as deep learning.

14:42.920 --> 14:45.840
And then there is a slight, the different approach

14:45.840 --> 14:46.880
that we might be looking at.

14:46.880 --> 14:49.800
And it doesn't use a normal Turing machine

14:49.800 --> 14:51.840
but a non-deterministic Turing machine,

14:51.840 --> 14:53.720
a multi-phase system.

14:53.720 --> 14:58.440
And that is because systems in the lowest level of physics

14:58.440 --> 15:01.120
and I also suspect systems that are implemented

15:01.120 --> 15:04.960
on brains, non-deterministic systems.

15:04.960 --> 15:06.600
And this doesn't just mean that they're noisy

15:06.600 --> 15:08.920
or that they're random in many ways.

15:08.920 --> 15:12.360
A non-deterministic Turing machine is a paradigm

15:12.360 --> 15:17.080
from computer science that describes your state machine

15:17.080 --> 15:20.000
in such a way that not every state

15:20.000 --> 15:21.920
has exactly one successor state.

15:21.920 --> 15:23.960
It's not sufficiently constrained

15:23.960 --> 15:26.120
to have only one successor state.

15:26.120 --> 15:29.080
Our Turing machine that we normally use

15:29.080 --> 15:31.440
and this includes our digital von Neumann computers

15:31.440 --> 15:33.080
and so on, they're defined in such a way

15:33.080 --> 15:38.080
that every state has exactly one possible successor state.

15:38.320 --> 15:39.840
If there is a branch in the computer

15:39.840 --> 15:41.720
that's because it is depending

15:41.720 --> 15:43.480
on some environmental variable

15:43.480 --> 15:45.640
that is not relevant in the program

15:45.640 --> 15:47.640
which means some input of the program.

15:47.640 --> 15:49.480
But given the same input,

15:49.480 --> 15:52.600
the Turing machine is going to produce the same output

15:52.600 --> 15:56.200
and it's going to do this along exactly one path.

15:56.200 --> 15:59.800
And not just Turing machine, the computational sense

15:59.800 --> 16:01.960
is also going to give you the same output

16:01.960 --> 16:03.880
but it's going to do this on many paths

16:03.880 --> 16:06.760
because the constraints are not so narrow

16:06.760 --> 16:09.400
that you go into one state after every state

16:09.400 --> 16:11.840
but you can go into multiple states.

16:11.840 --> 16:13.440
And the non-deterministic Turing machine

16:13.440 --> 16:16.040
just goes into all of them branches.

16:16.040 --> 16:18.360
And these branches don't necessarily meet again, right?

16:18.360 --> 16:20.080
There is no way that they're connected again.

16:20.080 --> 16:22.280
It's basically a non-deterministic Turing machine

16:22.280 --> 16:25.240
is implementing some kind of multiverse.

16:25.240 --> 16:28.520
But it turns out that many of these multiverse branches

16:28.520 --> 16:30.880
that the non-deterministic Turing machine goes into

16:30.880 --> 16:32.880
have the same bit combination in them

16:32.880 --> 16:36.040
because there are only so many states that are reachable.

16:36.040 --> 16:38.520
And so by looking at the dynamics between these bits,

16:38.520 --> 16:41.960
you can get to the same point in the universe

16:41.960 --> 16:43.720
on multiple paths.

16:43.720 --> 16:46.200
And if the universe that we live in,

16:46.200 --> 16:47.720
the physical universe that we live in

16:47.720 --> 16:49.440
is such a multivase system,

16:49.440 --> 16:53.240
it's still going to be possible for observer

16:53.240 --> 16:54.480
that lives inside of it,

16:54.480 --> 16:57.200
despite it not knowing which path it goes down,

16:57.200 --> 17:01.000
determine statistical properties over the regional paths

17:01.000 --> 17:03.880
which means you cannot know in your universe

17:03.880 --> 17:08.120
which slit the double slit experiment the photon goes through

17:08.120 --> 17:09.920
but you can predict the patterns

17:09.920 --> 17:12.920
that many, many photons are going to make on the other side.

17:12.920 --> 17:15.200
And the things that we can predict in our universe

17:15.200 --> 17:16.840
are of that nature.

17:16.920 --> 17:19.160
There are statistical properties

17:19.160 --> 17:21.840
over all the trajectories that can happen

17:21.840 --> 17:24.800
that we are a part of.

17:24.800 --> 17:27.280
And how would something like this look in the brain?

17:27.280 --> 17:30.560
Like in the brain, obviously the brain can only hold

17:30.560 --> 17:32.320
a finite amount of state.

17:32.320 --> 17:34.520
But if you have redundancy, what you can do is

17:34.520 --> 17:38.320
if you don't tell the neuron to go into one particular state

17:38.320 --> 17:41.280
as a result of its present state

17:41.280 --> 17:44.280
but into a range of possible states.

17:44.280 --> 17:46.760
And this happens randomly.

17:46.760 --> 17:49.080
It means that there are many trajectories

17:49.080 --> 17:52.400
that activations can take in the brain at the same time.

17:52.400 --> 17:54.440
And they're going to meet on the same physical substrate

17:54.440 --> 17:55.640
and they're going to accumulate

17:55.640 --> 17:57.760
and you're going to sum up in some sense

17:57.760 --> 18:01.280
and could be some basically some voting over the different paths.

18:01.280 --> 18:04.120
And this means that a system like the brain

18:04.120 --> 18:05.920
could via transmitting activation

18:05.920 --> 18:08.440
try many, many things in parallel

18:08.440 --> 18:12.160
and stochastically with some degree of randomness.

18:12.160 --> 18:16.000
And as a result perform computations efficiently

18:16.000 --> 18:19.480
that can not be efficiently done with a linear machine.

18:19.480 --> 18:20.960
From a computational perspective,

18:20.960 --> 18:23.480
if you want to simulate this on a linearly

18:23.480 --> 18:25.240
on a sequentially operating machine,

18:25.240 --> 18:26.520
it's going to be highly inefficient

18:26.520 --> 18:28.440
because you're trying to do the same thing

18:28.440 --> 18:29.960
over and over again.

18:29.960 --> 18:32.560
And this randomness is going to delete some of the bits

18:32.560 --> 18:34.400
that you've been computing.

18:34.400 --> 18:37.280
But if your computational units are very cheap,

18:37.280 --> 18:39.920
like in the brain, in the brain time is expensive

18:39.920 --> 18:41.920
because taking one more step means

18:41.920 --> 18:42.920
that you're going to be slower

18:42.920 --> 18:44.800
in your interaction with nature.

18:44.800 --> 18:46.320
But parallelism is cheap.

18:46.320 --> 18:50.160
Just by doubling the cell count once more

18:50.160 --> 18:51.800
by dividing the brain cells,

18:51.800 --> 18:53.680
you get twice as many elements.

18:53.680 --> 18:55.440
If you do this quite a few times,

18:55.440 --> 18:57.600
you end up with billions of elements.

18:57.600 --> 19:00.240
And these billions of elements can perform

19:00.240 --> 19:02.560
many of these paths in parallel.

19:02.560 --> 19:04.360
And this is something that to my knowledge

19:04.360 --> 19:06.360
has not been seriously attempted yet

19:06.360 --> 19:08.360
to get to work for a learning system.

19:08.360 --> 19:10.680
There are some people which are thinking seriously about this.

19:10.680 --> 19:13.800
For instance, we had Jerome Buzemeier on a panel here.

19:13.800 --> 19:17.040
And he is describing the mental representations

19:17.040 --> 19:20.800
as superpositional states using such a multivase system.

19:20.800 --> 19:23.320
And it's also an idea that has occurred to Steven Wolfram

19:23.320 --> 19:26.960
who is thinking of the universe as a multivase system

19:26.960 --> 19:28.640
and who's open to the idea

19:28.640 --> 19:30.920
that the brain might be basically

19:30.920 --> 19:33.160
a bounded complexity multivase system.

19:34.920 --> 19:38.920
So this, we see that there are opportunities

19:38.920 --> 19:43.640
to build solutions that are inspired by biology,

19:43.640 --> 19:45.440
that we haven't tried yet.

19:45.440 --> 19:48.320
And if you look at the inspiration that happens

19:48.320 --> 19:51.200
so far from biology into artificial intelligence,

19:51.200 --> 19:53.680
despite many claims to the contrary,

19:53.680 --> 19:55.480
it almost never happens.

19:55.480 --> 19:58.560
I think that the last contribution of biology

19:58.560 --> 20:01.960
to the transformer was heavy on learning.

20:01.960 --> 20:05.680
And everything else that people got to work since then

20:05.680 --> 20:06.960
was mostly not happening

20:06.960 --> 20:09.200
because people looked at newer science results

20:09.200 --> 20:10.240
and implemented them

20:10.240 --> 20:13.760
and then ended up with a better machine learning algorithm.

20:13.760 --> 20:16.680
And if you take the ideas that newer scientists currently have

20:16.680 --> 20:18.800
about how the brain works, to my knowledge,

20:18.800 --> 20:20.520
you cannot implement them in such a way

20:20.520 --> 20:22.680
that they learn and control an organism.

20:22.680 --> 20:25.600
Even a very simple organism like C elegance,

20:25.600 --> 20:27.600
if we take the conic term of C elegance

20:27.600 --> 20:30.960
and translate this into a computer model and run it,

20:30.960 --> 20:33.920
it's not going to produce coherent warm behavior.

20:33.920 --> 20:36.040
C elegance is a small warm

20:36.040 --> 20:40.200
and it has only 300, I think 309 neurons.

20:40.200 --> 20:44.040
Please correct me if this number is off.

20:44.040 --> 20:47.120
And as a result, if you take this conic term

20:47.120 --> 20:49.600
and run it into a digital simulation,

20:49.600 --> 20:53.720
it's not going to produce the behavior that we want

20:53.720 --> 20:57.840
because presumably we have not caught up

20:57.840 --> 21:00.120
on all the functionality of the individual neurons

21:00.120 --> 21:01.760
in the context of that warm.

21:01.760 --> 21:03.720
Could be that there is stuff in the soma of the cells

21:03.720 --> 21:06.640
that is not visible in the conic term

21:06.640 --> 21:10.240
or that we made mistakes in digitizing the conic term

21:10.240 --> 21:13.200
or I don't have enough resolution to see all the vesicles

21:13.200 --> 21:15.760
that use different neurotransmitters in the conic term

21:15.760 --> 21:17.520
to get the functionality right.

21:17.520 --> 21:21.600
But for systems at scale that go beyond an organism

21:21.600 --> 21:26.600
with a few hundred neurons, something like cortical column

21:26.640 --> 21:28.480
and so on, we don't really have working models

21:28.480 --> 21:29.320
at the moment.

21:29.320 --> 21:33.640
The descriptions that the neurobiologists have at the moment,

21:33.640 --> 21:35.840
if you translate them into computer models,

21:35.840 --> 21:37.680
are not performing the same things

21:37.680 --> 21:39.680
as our digital models are doing.

21:39.680 --> 21:42.120
They're not able to discover these functions

21:42.120 --> 21:45.240
because these models are still incomplete

21:45.240 --> 21:47.640
and they're not ready yet for being used.

21:47.640 --> 21:49.160
There's a more fundamental problem

21:49.160 --> 21:52.680
that Konrad Hauding has highlighted in the paper

21:52.680 --> 21:54.920
where he used the methods of neuroscientists

21:54.920 --> 21:59.480
to reverse and engineer a microprocessor.

21:59.480 --> 22:01.200
If you take a normal microprocessor

22:01.200 --> 22:02.800
and give this to a neuroscientist,

22:02.800 --> 22:04.960
the neuroscientist does ablation studies

22:04.960 --> 22:08.120
and looks at the structure that the neuroscientist finds.

22:08.120 --> 22:10.840
Is the neuroscientist able to reverse engineer

22:10.840 --> 22:13.280
this microprocessor which is dramatically simpler

22:13.280 --> 22:15.120
than the nervous system, of course.

22:15.120 --> 22:19.200
And it turns out that Konrad Hauding

22:19.200 --> 22:21.520
who is a neuroscientist that is the methods

22:21.520 --> 22:25.760
of neuroscience might be for some more deep reason

22:25.760 --> 22:27.600
because they're not functionalist enough,

22:27.600 --> 22:31.160
able to discover how information processing

22:31.280 --> 22:32.680
and nervous system works yet,

22:32.680 --> 22:35.240
which means that the theoretical tools of neuroscientists

22:35.240 --> 22:36.960
might not yet be ready even

22:36.960 --> 22:38.960
to understand what brains are doing.

22:41.440 --> 22:44.280
And I think that when we as non-neuroscientists

22:44.280 --> 22:48.080
look at brains and at the textbooks of neuroscience

22:48.080 --> 22:50.520
that take what I learned at university,

22:50.520 --> 22:52.800
I find from my current perspective

22:52.800 --> 22:56.000
that there are some shortcomings in this description

22:56.000 --> 22:58.920
that if I want to sit down as a computer scientist

22:58.920 --> 23:01.520
and discover the space of possible solutions

23:01.520 --> 23:04.880
in nervous systems and for functional approximation

23:04.880 --> 23:06.520
and just get a machine to search through it

23:06.520 --> 23:08.520
and so on would be insufficient.

23:08.520 --> 23:10.320
And this starts out with the idea

23:10.320 --> 23:13.440
that a neuron is a specialized switch

23:13.440 --> 23:16.240
at the way in which we abstract the neurons

23:16.240 --> 23:18.520
since the perceptron is that the neuron

23:18.520 --> 23:23.080
is some very simple gate or something element

23:23.080 --> 23:25.680
like in a circuit that does something

23:25.680 --> 23:29.400
in a mechanized automatic algorithmic way.

23:29.400 --> 23:30.840
And I don't think that's really true.

23:30.840 --> 23:32.600
A neuron is not a specialized switch.

23:32.600 --> 23:34.960
A neuron is a single-celled animal.

23:34.960 --> 23:35.920
It's quite complicated.

23:35.920 --> 23:36.760
It wants something.

23:36.760 --> 23:38.200
It wants to survive.

23:38.200 --> 23:39.800
It is able to learn by itself.

23:39.800 --> 23:41.040
It is adaptive.

23:41.040 --> 23:42.520
Can do a lot of things.

23:42.520 --> 23:46.000
It's almost like an amoeba that links up with other amoebas

23:46.000 --> 23:48.240
and grows into this static structure

23:48.240 --> 23:51.600
depending on its environment to perform a particular task.

23:51.600 --> 23:54.080
And this is quite different as a perspective.

23:54.080 --> 23:57.280
It means that your neuron is not just some kind of integrator

23:57.280 --> 23:59.640
or some kind of weighted sum of real numbers

23:59.640 --> 24:01.520
of activations that come in.

24:01.520 --> 24:04.640
The neuron is really a reinforcement learning agent

24:04.640 --> 24:05.680
with a little bit of memory

24:05.680 --> 24:08.520
and a little bit of ability to look into the future.

24:08.520 --> 24:11.000
Not very far, but a little bit.

24:11.000 --> 24:14.840
And it implements a number of adaptive functions

24:14.840 --> 24:17.880
to deal with its environment and reap rewards

24:17.880 --> 24:21.200
that ultimately allow the neuron to survive in the brain

24:21.200 --> 24:23.640
and not be strapped by the organism or killed by it

24:23.640 --> 24:26.480
because it's not doing the right thing.

24:26.480 --> 24:28.360
The second misunderstanding, I think,

24:28.360 --> 24:31.920
beyond neurons are not just specialized switches is

24:31.920 --> 24:33.920
that it's only neurons.

24:33.920 --> 24:36.560
There is probably no fundamental difference

24:36.560 --> 24:38.920
between neurons and other cells.

24:38.920 --> 24:41.560
That is, every cell can do information processing

24:41.560 --> 24:43.200
in conjunction with other cells

24:43.200 --> 24:45.240
if it's in a multicellular animal.

24:45.240 --> 24:48.360
It probably needs to be somewhat multicellular

24:48.360 --> 24:50.440
because if it's a single-celled animal,

24:50.440 --> 24:53.000
it's maybe evolving in such a way

24:53.000 --> 24:55.120
that it's adversarial to its environment.

24:55.120 --> 24:59.040
It doesn't benefit if it computes information

24:59.040 --> 25:00.600
together with others.

25:00.600 --> 25:02.800
But if it lives together with others

25:02.800 --> 25:04.160
in a high degree of organization

25:04.160 --> 25:06.200
like the cells in our body are doing

25:06.200 --> 25:09.360
and you co-evolve them, then the cells can all send

25:09.360 --> 25:11.360
many types of messages via chemicals

25:11.360 --> 25:14.720
over the cellular membranes to neighboring cells.

25:14.720 --> 25:17.000
They can interpret those messages

25:17.000 --> 25:20.720
and they can learn how to respond to these messages,

25:20.720 --> 25:24.400
which means they can learn to perform arbitrary computation.

25:24.400 --> 25:27.640
So what's the difference between a neuron and another cell

25:27.640 --> 25:28.880
if they can do the same thing?

25:28.880 --> 25:31.160
Well, the specialization of neurons

25:31.160 --> 25:34.120
is that they have extremely long accents,

25:34.120 --> 25:36.960
so which they can send information coded

25:36.960 --> 25:39.960
as electrochemical impulses very quickly

25:39.960 --> 25:41.840
over long distances.

25:41.840 --> 25:43.920
I think that neurons might be best understood

25:43.920 --> 25:45.520
as telegraph cells.

25:45.520 --> 25:48.800
They're very special cells that have evolved only in animals

25:48.800 --> 25:52.200
or mostly in animals and that are very expensive to run

25:52.200 --> 25:53.840
because they need a lot of energy

25:53.840 --> 25:56.040
to send information so quickly.

25:56.040 --> 25:59.160
And the benefit is that they can move an animal very quickly

25:59.160 --> 26:01.480
so nature, so it can eat plants or other animals

26:01.480 --> 26:02.720
to get that energy.

26:02.720 --> 26:05.320
So basically the animal gets more energy

26:05.320 --> 26:08.000
that it could get from photosynthesis.

26:08.000 --> 26:12.080
And as a benefit, it's able to afford

26:12.080 --> 26:14.800
to have this expensive nervous system.

26:14.800 --> 26:17.480
And the code that the neurons are using

26:17.480 --> 26:20.440
is probably different from the code, from the chemical codes

26:20.440 --> 26:22.280
that the cells are using for the messages.

26:22.280 --> 26:24.160
It's like a Morse code, probably something

26:24.160 --> 26:27.960
like a telegraph system, so it can send information so quickly.

26:27.960 --> 26:31.120
And initially, it seems that to me

26:31.120 --> 26:34.600
that neurons might have evolved to move skeletal muscles

26:34.600 --> 26:35.640
at the limit of physics.

26:35.640 --> 26:38.560
So it can, from centralized coordination

26:38.560 --> 26:42.520
in the nervous system, from the central nervous system,

26:42.520 --> 26:43.960
send information so quickly

26:43.960 --> 26:47.440
that the whole organism is coordinated much, much faster

26:47.440 --> 26:51.200
at much shorter time spans than plants are.

26:51.200 --> 26:53.840
And the other thing is once you can move very quickly,

26:53.840 --> 26:56.000
you also need to perceive very quickly.

26:56.000 --> 26:58.600
So it's also driving sensory input

26:58.600 --> 27:00.880
and the evaluation of sensory input

27:00.880 --> 27:03.160
and decision-making and learning.

27:03.160 --> 27:06.280
So it's basically duplicating the information processing

27:06.280 --> 27:08.080
that existed in the body

27:08.080 --> 27:12.920
and it is creating something like a second information

27:12.920 --> 27:16.160
processor in the body that is running at much, much faster

27:16.160 --> 27:19.600
time scales than the normal cellular information processing

27:19.600 --> 27:22.880
that will also exist in large, long-lived plants.

27:22.880 --> 27:27.160
So my perspective is that just by looking at means and motive,

27:27.160 --> 27:29.360
the possibilities of what evolution can do

27:29.360 --> 27:32.040
and the capabilities of what an individual cell has,

27:32.040 --> 27:35.680
I suspect that every long-lived organism with many cells

27:35.680 --> 27:39.520
is basically going to function like a very, very slow brain.

27:39.520 --> 27:42.840
And there are almost no limits

27:42.960 --> 27:46.680
to what this slow brain can do if it lives long enough,

27:46.680 --> 27:48.960
but it's not going to do this at the same time frame.

27:48.960 --> 27:51.800
So animals will be able to outthink plants

27:51.800 --> 27:54.960
just because they are so fast and run circles around them.

27:56.960 --> 28:01.320
And the third misunderstanding is that we think

28:01.320 --> 28:04.000
that consciousness is extremely rare,

28:04.000 --> 28:06.840
that consciousness may be only existing in humans

28:06.840 --> 28:10.320
and forms only very late in the evolution

28:10.320 --> 28:12.040
of intelligent systems.

28:12.040 --> 28:15.840
But it turns out we don't get conscious after the PhD.

28:15.840 --> 28:19.360
We seem to be conscious before we can track a finger.

28:19.360 --> 28:23.280
And if that is the case, maybe self-reflexive attention

28:23.280 --> 28:25.640
is a requirement if you want to learn

28:25.640 --> 28:27.360
beyond happy and learning.

28:27.360 --> 28:29.440
If you don't want to just look at co-activation

28:29.440 --> 28:31.680
between cells as a learning paradigm, which is probably

28:31.680 --> 28:34.160
sufficient to map your body surface and so on,

28:34.160 --> 28:36.960
but you want to have a coherent model of reality,

28:36.960 --> 28:40.320
maybe you need to start out with some kind of core

28:40.320 --> 28:43.280
that organizes everything into coherence.

28:43.280 --> 28:45.760
And what we find confusing is about consciousness

28:45.760 --> 28:48.320
that is that we don't seem to need it for many things.

28:48.320 --> 28:51.440
A sleepwalker can do many of the things that normally

28:51.440 --> 28:52.520
require consciousness.

28:52.520 --> 28:55.480
And this is confusing philosophers to no end.

28:55.480 --> 28:57.720
But maybe consciousness is in some sense

28:57.720 --> 29:00.240
like government in a society.

29:00.240 --> 29:03.800
And I don't mean government as some kind of abstract principle,

29:03.800 --> 29:06.200
but as a real-time interaction that

29:06.200 --> 29:10.080
is making society coherent.

29:10.120 --> 29:13.640
And a society can do everything without government.

29:13.640 --> 29:16.360
If you would be shutting down the government today,

29:16.360 --> 29:21.600
it would be days before we notice and years before we crash.

29:21.600 --> 29:24.960
But to get to the state in which society is today,

29:24.960 --> 29:28.240
with streets and infrastructure and educational system

29:28.240 --> 29:30.440
and so on, you need to have a government.

29:30.440 --> 29:33.840
You're not going to bootstrap a group of people

29:33.840 --> 29:35.960
into an organization without having

29:35.960 --> 29:37.960
some kind of hierarchical organization that

29:37.960 --> 29:41.040
makes this hoop of people coherent in their actions

29:41.040 --> 29:43.880
and creates some next-level agent out of them.

29:43.880 --> 29:46.200
And to do this, the government needs

29:46.200 --> 29:48.720
to start out with some local coherence,

29:48.720 --> 29:51.080
with some making itself coherent,

29:51.080 --> 29:53.680
and then imposing some kind of organization

29:53.680 --> 29:55.520
on the environment that is branching out

29:55.520 --> 29:59.160
and scales over all the individual agents.

29:59.160 --> 30:02.680
And if you just put individual people together for long enough,

30:02.680 --> 30:04.520
then different forms of government

30:04.520 --> 30:07.600
will emerge and some kind of evolutionary competition

30:07.600 --> 30:08.440
between them.

30:08.440 --> 30:10.720
And eventually, one of them will take over

30:10.720 --> 30:13.480
and organize this group of people in such a way.

30:13.480 --> 30:15.480
And the idea that the same thing could happen

30:15.480 --> 30:17.200
among the neurons, that they're basically

30:17.200 --> 30:19.520
different forms of organization that start out

30:19.520 --> 30:23.480
from small cores and then move as activation patterns

30:23.480 --> 30:26.000
that are agnostic of the individual units that they run on.

30:26.000 --> 30:28.400
But they impose the same language on all of them,

30:28.400 --> 30:30.400
similar capabilities on all of them,

30:30.400 --> 30:34.760
so the locus of action can move around between them.

30:34.760 --> 30:36.960
This idea that the brain organization could evolve

30:36.960 --> 30:40.600
like this is similar to Gary Edelman's idea of neural

30:40.600 --> 30:43.560
Darwinism, that basically our brain organization,

30:43.560 --> 30:46.760
our mental organization is not hard-coded in the genome

30:46.760 --> 30:50.240
as a blueprint, but what the genome contains

30:50.240 --> 30:52.920
is the conditions to start this evolution

30:52.920 --> 30:55.640
between different forms of organization.

30:55.640 --> 30:58.520
And then rig the evolution in a particular way

30:58.520 --> 31:01.400
so it converges quickly.

31:01.440 --> 31:05.440
There are some ideas that we could take from biology

31:05.440 --> 31:07.200
into technical systems.

31:07.200 --> 31:10.080
And first of all, I think that the system

31:10.080 --> 31:11.320
needs to be real-time.

31:11.320 --> 31:13.640
It needs to be coupled to the environment

31:13.640 --> 31:16.200
and needs to go into resonance with whatever environment

31:16.200 --> 31:17.160
is coupled to.

31:17.160 --> 31:19.560
And it needs to regulate the interaction

31:19.560 --> 31:23.000
with that environment until at the level of coupling,

31:23.000 --> 31:25.680
at the temporal resolution that it has,

31:25.680 --> 31:29.680
is able to track reality around it.

31:29.680 --> 31:32.160
And it's something that our machine learning models

31:32.160 --> 31:35.640
are not doing yet and not real-time.

31:35.640 --> 31:38.520
Even something like stable diffusion

31:38.520 --> 31:41.520
is trained in digital images that are not happening in real-time.

31:41.520 --> 31:43.320
They're not even happening in the right order.

31:43.320 --> 31:46.360
There are just 800 million disconnected images,

31:46.360 --> 31:49.160
or which the system is trying to find structure.

31:49.160 --> 31:51.840
And this would not work for a logical organism.

31:51.840 --> 31:53.200
I don't think that we could converge

31:53.200 --> 31:54.680
from this amount of data.

31:54.680 --> 31:58.120
Instead, what we get is a world that changes continuously

31:58.120 --> 31:59.400
by small degrees.

31:59.440 --> 32:02.920
And these small changes make sure that every frame is

32:02.920 --> 32:04.720
related to the last frame.

32:04.720 --> 32:07.720
And we can learn universal laws in which these frames are

32:07.720 --> 32:08.280
related.

32:08.280 --> 32:10.760
There are laws of conservation of information.

32:10.760 --> 32:12.800
And without this conservation of information,

32:12.800 --> 32:15.840
where we learn the transitions between adjacent frames,

32:15.840 --> 32:19.360
I don't think that we would be able to learn from the universe.

32:19.360 --> 32:23.440
So one thing is we change the paradigm from image to video

32:23.440 --> 32:26.800
or other streaming data that has information preservation.

32:26.800 --> 32:29.160
And in the beginning, this might look more difficult to us.

32:29.160 --> 32:31.120
Isn't it harder to learn from video

32:31.120 --> 32:33.440
than it is to learn from single pictures?

32:33.440 --> 32:36.280
Well, what you want to learn is the fact

32:36.280 --> 32:38.880
that you are living in the universe with moving objects

32:38.880 --> 32:40.920
that happen in free space and so on.

32:40.920 --> 32:43.200
It's actually much easier to learn this from video,

32:43.200 --> 32:46.000
because it contains way more constraints than this way.

32:46.000 --> 32:48.600
They're much more obvious.

32:48.600 --> 32:51.760
The next thing is the way in which our brain is modeling

32:51.760 --> 32:54.080
these things seems to be made from lots

32:54.080 --> 32:58.560
of small periodic loops, small interlocking periodic loops.

32:59.400 --> 33:01.960
And first of all, it has to be loops,

33:01.960 --> 33:04.680
because the brain is relatively slow.

33:04.680 --> 33:08.480
Information transmission in the brain

33:08.480 --> 33:12.040
is so slow that it takes appreciable fraction

33:12.040 --> 33:16.080
of a second for a signal to cross the entire neocortex.

33:16.080 --> 33:17.960
And if you want to create simultaneity

33:17.960 --> 33:21.280
between these different parts of the brain,

33:21.280 --> 33:22.360
you're not going to get there.

33:22.360 --> 33:26.320
It's right in our computers and our CPUs and so on.

33:26.600 --> 33:29.400
Our GPUs, they make everything simultaneous

33:29.400 --> 33:34.080
by exploiting the speed of the signal transmission

33:34.080 --> 33:36.920
in our CPUs and GPUs.

33:36.920 --> 33:39.680
It also means that we cannot increase the frequency

33:39.680 --> 33:41.760
at which we run them arbitrarily,

33:41.760 --> 33:44.920
or we cannot make the CPUs and GPUs arbitrarily large,

33:44.920 --> 33:47.120
because it just takes so much time for a signal

33:47.120 --> 33:49.720
to cross over the entire circuit.

33:51.520 --> 33:53.360
The next step could be to use photonics.

33:53.360 --> 33:55.400
So we can go to the speed of light

33:55.400 --> 33:57.480
and make this system slightly faster

33:57.480 --> 33:58.880
and slightly larger again,

34:00.000 --> 34:03.400
while having coherence over the entire system.

34:03.400 --> 34:06.040
But our biological systems don't have a chance of doing that.

34:06.040 --> 34:08.560
They must live with the fact that it takes very long

34:08.560 --> 34:10.120
for signals to get there.

34:10.120 --> 34:12.040
And the way to deal with that is to make sure

34:12.040 --> 34:14.560
that you're okay if you are out of sync.

34:14.560 --> 34:17.120
You just need to be in the same phase.

34:17.120 --> 34:19.800
Basically, you go at the same frequency

34:19.800 --> 34:21.400
at different points of the brain,

34:21.400 --> 34:23.840
and you make sure that the signal eventually gets there,

34:23.840 --> 34:26.120
but it's okay if it's from the previous cycle

34:26.120 --> 34:29.400
or two cycles ago, or several cycles ago,

34:29.400 --> 34:32.400
as long as the content of the different brain areas

34:32.400 --> 34:34.520
that only changes gradually.

34:34.520 --> 34:37.680
If that happens, you're able to integrate over that.

34:37.680 --> 34:40.360
You can use predictive algorithms and so on,

34:40.360 --> 34:42.040
and can synchronize the whole thing.

34:42.040 --> 34:44.440
So basically, this idea of slow oscillators

34:44.440 --> 34:47.440
is something that we could translate into digital systems.

34:49.240 --> 34:51.440
The next thing is this emergent management,

34:51.440 --> 34:52.640
and here we'll Darwinism.

34:52.640 --> 34:55.720
So instead of having a particular circuit

34:55.720 --> 34:58.920
that is required to be like this,

34:58.920 --> 35:02.080
let's evolve this computational operators that we need.

35:02.880 --> 35:05.360
And the next one is that many of the functions

35:05.360 --> 35:09.040
that the brain discovering are only discovered once.

35:09.040 --> 35:12.400
And this is tool for simple functions,

35:12.400 --> 35:15.360
like addition, integration, multiplication,

35:15.360 --> 35:17.680
simple computational primitives, rotation,

35:17.680 --> 35:20.600
that are being used for many, many mathematical primitives

35:20.600 --> 35:23.160
that we require to describe the geometry of sound,

35:23.160 --> 35:25.280
of images, of thought.

35:25.280 --> 35:27.040
And this basic arithmetic,

35:27.040 --> 35:29.240
this basic library of computational functions

35:29.240 --> 35:31.920
at the moment to train this into a neural network

35:31.920 --> 35:33.600
takes a very long time, right?

35:33.600 --> 35:36.040
Despite the neural network being built over addition

35:36.040 --> 35:37.480
and multiplication,

35:37.480 --> 35:41.080
it's not easy for a neural network to learn arithmetic.

35:41.080 --> 35:42.640
It's possible to do it,

35:42.640 --> 35:45.400
but it takes enormous amount of training data.

35:45.400 --> 35:47.560
And in every context, locally,

35:47.560 --> 35:51.240
the neural network is performing the same arithmetic

35:51.240 --> 35:52.680
over and over again.

35:52.680 --> 35:55.240
It has to retrain these functions

35:55.240 --> 35:59.280
into a different region of the network again.

35:59.280 --> 36:03.200
And basically getting all these different computational

36:03.200 --> 36:05.920
primitives bootstrapped into the neural network

36:05.920 --> 36:08.040
is something that is hard.

36:08.040 --> 36:10.200
And it has an interesting effect.

36:10.200 --> 36:13.800
You can, for instance, train a neural network

36:13.800 --> 36:15.320
on audio input,

36:15.960 --> 36:18.760
and then use it to discover structure and vision.

36:18.760 --> 36:20.240
And it's going to be much faster

36:20.240 --> 36:23.360
because the audio input already prepares the neural network

36:23.360 --> 36:25.760
to learn many of the computational primitives,

36:25.760 --> 36:28.480
basic arithmetic in many parts of the network,

36:28.480 --> 36:31.480
that it can then adapt for a new task.

36:31.480 --> 36:33.360
And the way in which our brain is doing this

36:33.360 --> 36:35.640
is probably that it learns some useful functions

36:35.640 --> 36:37.720
and then these neurons have a way

36:37.720 --> 36:40.960
to exchange these functions possibly via RNA.

36:40.960 --> 36:44.640
And so basically the functions that are being computed

36:44.640 --> 36:48.040
are to some degree agnostic to the individual neuron.

36:48.040 --> 36:49.560
They are somewhat substrate independent.

36:49.560 --> 36:50.960
They're just migrate to.

36:50.960 --> 36:53.480
So a different paradigm might be instead

36:53.480 --> 36:55.800
of using local functions over the neighborhood

36:55.800 --> 36:56.640
of every neuron.

36:56.640 --> 36:59.400
And every neuron is learning its own set of functions.

36:59.400 --> 37:01.600
You learn a set of global functions

37:01.600 --> 37:04.560
and every neuron is deciding which ones of those to use.

37:07.000 --> 37:09.520
And it's also something that has almost never been tried

37:09.520 --> 37:13.200
in AI and that what I'd like to get seen.

37:14.200 --> 37:18.360
Another thing is that the main focus is on reward.

37:18.360 --> 37:20.160
What you try to do in the brain is

37:20.160 --> 37:25.040
to do the most useful thing with fixed resources.

37:25.040 --> 37:28.120
And this means you have to assess the global reward

37:28.120 --> 37:31.960
that the organism is getting out of the distributions

37:31.960 --> 37:33.120
of all the neurons.

37:33.120 --> 37:36.480
And then you need to distribute this reward

37:36.480 --> 37:39.000
among all the neurons that contribute to the result.

37:39.000 --> 37:42.480
This is similar to what you do in a corporation.

37:42.520 --> 37:44.640
It's an economic problem, but the corporation

37:44.640 --> 37:46.640
tries to do the most valuable thing

37:46.640 --> 37:49.080
that it can do with all the employees that it has.

37:49.080 --> 37:50.840
And to do this, it needs to get rewards

37:50.840 --> 37:53.160
to all the individual employees.

37:53.160 --> 37:55.680
And the rewards are not given in such a way

37:55.680 --> 37:59.440
that every employee gets a different amount of money.

37:59.440 --> 38:01.840
And the one that has the biggest contribution

38:01.840 --> 38:04.800
to the bottom line by the others are also important

38:04.800 --> 38:06.280
gets much, much more.

38:06.280 --> 38:09.000
There is some degree of this, but it's mostly depending

38:09.000 --> 38:11.680
on the negotiation power of individual employees

38:11.800 --> 38:12.640
on the market.

38:13.480 --> 38:16.360
If there was no fungibility

38:16.360 --> 38:18.280
and you would need to train all your employees,

38:18.280 --> 38:20.520
it would make sense to give them all the same amount

38:20.520 --> 38:21.640
of money, right?

38:21.640 --> 38:24.040
And in the liberal capitalism, it doesn't make sense

38:24.040 --> 38:29.040
to give a good retail worker the same amount of salary

38:29.680 --> 38:32.840
than it does to give money to a good manager.

38:32.840 --> 38:36.160
But if you need retail workers, you will have to employ one.

38:36.160 --> 38:39.240
And the reason why retail workers are less than managers

38:39.240 --> 38:41.320
is mostly because there are many more retail workers

38:41.760 --> 38:44.560
competing for positions than there are managers

38:44.560 --> 38:46.040
competing for positions.

38:46.040 --> 38:50.080
So the supply and demand regulates labor market

38:50.080 --> 38:51.680
in such a way.

38:51.680 --> 38:53.800
But this is not true in the biological system.

38:53.800 --> 38:56.360
Every new one basically is going to consume

38:56.360 --> 38:58.920
the same amount of resources just for existing

38:58.920 --> 39:00.960
and being ready to do something.

39:00.960 --> 39:03.120
So our reward here is different.

39:03.120 --> 39:05.240
It's not being accumulated in the bank account

39:05.240 --> 39:06.480
of the new one.

39:06.480 --> 39:10.120
Instead, this is just a signal that tells the organism

39:10.120 --> 39:11.840
that this new one is still going to get fat

39:11.840 --> 39:14.120
because it's useful in what it does.

39:14.120 --> 39:16.120
And the new one needs to get feedback similar

39:16.120 --> 39:17.960
to the feedback that you get from your colleagues.

39:17.960 --> 39:20.160
That's the actual reward that tells you

39:20.160 --> 39:22.800
you're doing the right thing, right?

39:22.800 --> 39:26.000
So it needs to be some kind of communicative reward,

39:26.000 --> 39:28.680
some messages that are not directly food,

39:28.680 --> 39:30.640
but that are more anticipated reward

39:30.640 --> 39:34.800
that are like money only without accumulation.

39:34.800 --> 39:38.080
And so there's basically going to be a reward-driven language

39:38.080 --> 39:41.720
and who is distributing all the reward in the brain?

39:41.720 --> 39:45.440
Well, all the cells are with all the neurons mostly,

39:45.440 --> 39:47.560
but also the other cells, maybe glia cells,

39:47.560 --> 39:48.960
and so on that contribute

39:48.960 --> 39:50.840
and the distribution of the rewards.

39:50.840 --> 39:52.760
And so what's happening in the biological system

39:52.760 --> 39:55.040
is that it involves a reward language.

39:55.040 --> 39:58.000
There are two types of signals that are being sent around.

39:58.000 --> 40:00.000
One is the results of the computation,

40:00.000 --> 40:01.560
which is read by other neurons

40:01.560 --> 40:03.760
based on what kind of activation they're interested

40:03.760 --> 40:05.720
in filtering out of the environment.

40:05.720 --> 40:08.000
And the other one is going to be signals

40:08.000 --> 40:11.000
that amount to reward and punishment

40:11.000 --> 40:12.600
that basically tell other neurons

40:12.600 --> 40:14.000
whether they should do more or less

40:14.000 --> 40:15.320
of a certain computation.

40:17.440 --> 40:22.200
And so basically reading of information is going to be pulled.

40:22.200 --> 40:24.680
You draw information from the environment

40:24.680 --> 40:26.720
and reward is going to be pushed

40:26.720 --> 40:27.800
because you should not be able

40:27.800 --> 40:31.120
to escape a negative reward, a punishment and so on, right?

40:32.160 --> 40:35.240
And I think we could approximate this

40:35.240 --> 40:37.800
using a new paradigm and a number of experiments

40:37.800 --> 40:39.600
that I would like to do in this regard.

40:39.600 --> 40:43.040
So basically a neuron has an internal state vector

40:43.040 --> 40:46.360
that contains the slight history of the neurons

40:46.360 --> 40:48.440
over the last few activations that is read

40:48.440 --> 40:50.760
and the type of the neuron and so on.

40:50.760 --> 40:52.120
And it has a selector function.

40:52.120 --> 40:53.800
The selector function is basically

40:53.800 --> 40:56.600
defining the receptive field of the neuron.

40:56.600 --> 40:58.080
And the receptive field of the neuron

40:58.080 --> 41:00.600
can be just the environment of the neuron

41:01.480 --> 41:03.240
interpreted as a certain topology.

41:03.240 --> 41:05.800
So basically it looks at its neighborhood

41:05.800 --> 41:09.440
as if it was a space with a certain number of dimensions.

41:09.440 --> 41:11.080
And how can this be?

41:11.080 --> 41:15.280
The neocortex is two-dimensional or two and a half-dimensional

41:15.280 --> 41:16.720
but it has a number of layers,

41:16.720 --> 41:18.880
that number of layers being very small

41:18.880 --> 41:21.240
and it's a large 2D area

41:21.240 --> 41:23.280
subdivided into different regions.

41:23.280 --> 41:27.320
Well, it turns out that you can interpret a 2D area

41:27.320 --> 41:29.160
as something that is in higher dimension

41:29.160 --> 41:31.160
in the same way as you can take the linear

41:31.160 --> 41:33.120
or one-dimensional address space of your computer

41:33.120 --> 41:35.600
and interpret it as a two-dimensional map

41:35.600 --> 41:37.920
or as a three-dimensional space

41:37.920 --> 41:40.280
or as something that happens in eight dimensions

41:40.280 --> 41:42.320
and can perform operations on it.

41:42.320 --> 41:44.040
And this is what the selector function does.

41:44.040 --> 41:45.880
The selector function is basically interpreting

41:45.880 --> 41:48.640
the environment of the individual cell

41:48.640 --> 41:51.520
as a space that contains information

41:51.520 --> 41:53.560
in a certain arrangement.

41:53.560 --> 41:55.280
And it doesn't need to be a regular space.

41:55.280 --> 41:56.360
It can be a manifold.

41:56.360 --> 41:58.000
It can be something that is very selective.

41:58.000 --> 42:00.920
It can be something that only uses five neighbors.

42:00.920 --> 42:04.200
And these neighbors can even be physically very distant.

42:04.240 --> 42:06.120
And so this neuron could be a juncture

42:06.120 --> 42:08.560
or some kind of hub that sends information locally

42:08.560 --> 42:09.920
into the network and so on.

42:09.920 --> 42:12.360
So you're going to have some neurons that have a topology

42:12.360 --> 42:14.560
that allow long distance connectivity

42:14.560 --> 42:19.560
and others that performs local maps and 2D or 3D

42:19.880 --> 42:21.880
and perform functions on them.

42:21.880 --> 42:23.840
And this next to the selector function,

42:23.840 --> 42:25.240
you have the modifier functions.

42:25.240 --> 42:28.120
The modifier functions tell the individual neuron

42:28.120 --> 42:31.200
how it should change its state based on its own state

42:31.200 --> 42:33.800
and the activation that it reads in the environment.

42:34.640 --> 42:37.360
By having some history in its own state,

42:37.360 --> 42:41.320
it's able to respond to a spatiotemporal activation

42:41.320 --> 42:42.880
distribution in its environment.

42:44.080 --> 42:46.600
And it's the use this idea

42:46.600 --> 42:49.200
that the neuron can use global functions.

42:49.200 --> 42:53.240
It means that we can arrange many neurons densely enough

42:53.240 --> 42:54.600
in some kind of lattice

42:54.600 --> 42:56.680
and these functions can arrange themselves

42:56.680 --> 42:58.240
as they need to be arranged.

42:58.240 --> 43:00.640
And they can shift around as they have to

43:00.640 --> 43:03.240
and duplicate themselves if they have to.

43:03.280 --> 43:06.000
Yasha, just in the interest of time.

43:06.000 --> 43:08.040
Yes, I'm basically done.

43:08.040 --> 43:09.960
Thank you for reminding me.

43:09.960 --> 43:14.960
So this selector function, modifier function paradigm

43:15.200 --> 43:18.360
allows us to come up with a new way

43:18.360 --> 43:23.000
of describing a function approximation beyond deep learning.

43:23.000 --> 43:25.360
And at the moment, the search space

43:25.360 --> 43:27.480
in my experiments is way too large.

43:27.480 --> 43:29.840
So basically there are too many ways

43:29.840 --> 43:32.000
in which this could be implemented

43:32.000 --> 43:33.800
from my current perspective

43:33.800 --> 43:37.200
to get this converge to a good solution.

43:37.200 --> 43:40.000
The alternative is I can just handcraft a solution

43:40.000 --> 43:41.560
but it might not be optimal.

43:41.560 --> 43:44.200
But it's something that I would like to definitely look more

43:44.200 --> 43:45.840
into in the future.

43:45.840 --> 43:48.600
And not just me, I suspect that many people

43:48.600 --> 43:51.120
are currently discovering such ideas

43:51.120 --> 43:54.000
and will be working on in the future.

43:54.000 --> 43:56.040
And while it's not clear

43:56.040 --> 43:59.120
that this can provide a viable alternative to deep learning

43:59.120 --> 44:01.520
that is much faster and converging faster

44:01.520 --> 44:05.200
and more efficiently than the present deep learning systems.

44:05.200 --> 44:07.240
This is something that I believe is closer

44:07.240 --> 44:09.600
to what biology has discovered.

44:09.600 --> 44:11.280
And that scales up very reliably

44:11.280 --> 44:13.480
over many, many classes of algorithms.

44:15.120 --> 44:16.280
Okay, that's it from me.

44:17.200 --> 44:20.880
All right, we would like to have a quick one

44:20.880 --> 44:23.080
to two minute break right now

44:23.080 --> 44:28.080
so that the panelists can set up their presentation.

44:28.320 --> 44:33.320
We will do Michael Levin's presentation next.

44:34.440 --> 44:36.960
So in the meantime, Yosha,

44:36.960 --> 44:41.440
let me ask you some questions that's got posted

44:41.440 --> 44:42.680
while you're talking.

44:44.720 --> 44:47.120
So a question from Nikolai,

44:47.120 --> 44:49.920
like how neurons are small organisms

44:49.920 --> 44:52.440
that operate in the emerging super organism

44:52.440 --> 44:53.280
that is a human,

44:53.280 --> 44:57.360
would future AGI architecture need to be made up

44:57.360 --> 44:59.240
of many smaller AIs?

45:01.600 --> 45:04.640
So there is no centralized control in the brain

45:04.640 --> 45:08.080
in the sense that all the centralized control

45:08.080 --> 45:11.680
is emergent over all the organizations between the neurons.

45:11.680 --> 45:13.200
There is no dedicated CPU

45:13.200 --> 45:17.560
that is able to process every neuron and update its state.

45:17.560 --> 45:21.040
Every state update happens locally in the individual cells.

45:21.040 --> 45:23.280
But this is an engineering constraint

45:23.280 --> 45:26.240
that doesn't exist in the technical systems.

45:26.240 --> 45:28.120
And it's not clear yet to me

45:28.120 --> 45:31.800
to which degree we need to have local control

45:31.800 --> 45:33.360
to make it happen.

45:33.360 --> 45:35.920
At the moment, the machine learning of organisms

45:35.920 --> 45:40.360
that we are using are treating the individual nodes

45:40.360 --> 45:42.680
in the network just as memory.

45:42.680 --> 45:46.120
And the updates are done by a centralized algorithm

45:46.120 --> 45:48.840
that is updating all this memory.

45:48.840 --> 45:52.360
All right, and you either have a CPU

45:52.360 --> 45:54.000
that is reading and writing,

45:54.000 --> 45:58.320
or you have lots of local CPUs in the GPU

45:58.320 --> 46:01.600
that is doing this with multiple pipelines in parallel.

46:01.600 --> 46:05.440
And there are biologically inspired chips

46:06.440 --> 46:08.320
that are mostly experimental at the moment,

46:08.320 --> 46:09.880
like Intel's Lohi,

46:09.880 --> 46:13.240
that are using many, many very small simple CPUs

46:13.240 --> 46:14.080
that are doing this.

46:14.080 --> 46:16.240
And it's not clear if the best solution

46:16.240 --> 46:18.080
is to have a CPU for every memory cell.

46:18.080 --> 46:19.840
It's probably not the case.

46:19.840 --> 46:21.880
So there are some things that you can do

46:21.880 --> 46:26.280
in the technical systems that informing conformance

46:26.280 --> 46:27.640
to a centralized algorithm,

46:27.640 --> 46:29.440
to centralized specifications

46:29.440 --> 46:31.960
that the technical system is going to implement.

46:31.960 --> 46:35.640
And in a biological system, this is just not feasible

46:35.640 --> 46:38.240
because there is no such centralized authority,

46:38.240 --> 46:42.360
no engineer who can make nature behave by itself.

46:42.360 --> 46:45.400
But if you want to think about how to build a mind

46:45.400 --> 46:47.160
from a biological perspective,

46:47.160 --> 46:48.760
we have to think about how to build something

46:48.760 --> 46:51.200
that wants to grow into a mind.

46:51.200 --> 46:53.200
And we can take some of these ideas.

46:53.200 --> 46:55.440
It's not clear that we need to make this

46:55.440 --> 46:57.440
with completely local control only.

46:57.440 --> 46:59.080
Maybe it's more efficient to have a mixture

46:59.080 --> 47:01.640
of some local control and a lot of global control

47:01.640 --> 47:04.960
where we already know what the control is going to be.

47:04.960 --> 47:06.640
I think Mike is ready.

47:06.640 --> 47:07.880
Yes, excellent.

47:07.880 --> 47:10.000
Michael, the floor is now yours.

47:10.000 --> 47:10.840
Great.

47:10.840 --> 47:12.880
Okay, well, that was extremely interesting.

47:12.880 --> 47:15.320
So let me see what I can add here.

47:16.320 --> 47:18.480
I've got some slides.

47:18.480 --> 47:19.320
So here we go.

47:19.320 --> 47:20.920
Hopefully you can see that.

47:20.960 --> 47:24.440
So what I would like to talk about,

47:24.440 --> 47:26.560
of course there's this idea that biology

47:26.560 --> 47:29.040
should be an inspiration for AI.

47:29.040 --> 47:32.120
And what I would like to do is to deconstruct

47:32.120 --> 47:34.480
some of the biology that people typically think about

47:34.480 --> 47:38.600
in these contexts and ditch a lot of things

47:38.600 --> 47:40.640
that are very common, binary categories

47:40.640 --> 47:43.960
and a focus on brains, a focus on neurons,

47:43.960 --> 47:45.280
a focus on humans.

47:45.280 --> 47:47.080
I want to step away from all of that

47:47.080 --> 47:49.280
and rebuild a different framework

47:49.280 --> 47:54.040
that I think has many, many implications for AI.

47:54.040 --> 47:56.200
So the first thing I want to talk about

47:56.200 --> 47:59.400
is this idea of a typical human.

47:59.400 --> 48:02.160
So there's this kind of classic idea

48:02.160 --> 48:03.560
that we know what a human is

48:03.560 --> 48:07.600
and it certainly works for practical purposes in society.

48:07.600 --> 48:11.120
But the idea is sort of pre-scientific

48:11.120 --> 48:12.880
and it's still, many people are still,

48:12.880 --> 48:15.160
even scientists are often still caught up in this,

48:15.160 --> 48:17.040
the idea that, okay, so we have these humans

48:17.080 --> 48:19.520
and they are discrete natural kind

48:19.520 --> 48:21.320
and they're different from other animals.

48:21.320 --> 48:25.240
And so there's this, here's Adam naming the other animals

48:25.240 --> 48:29.280
and so there's the discrete species and so on.

48:29.280 --> 48:33.760
But if we take developmental biology and evolution

48:33.760 --> 48:36.560
and synthetic biology and bioengineering,

48:36.560 --> 48:38.360
if we take these things seriously,

48:38.360 --> 48:40.240
then what we find out is that actually

48:40.240 --> 48:43.320
there are no such natural kinds because all of this,

48:43.320 --> 48:45.520
both on the evolutionary time scale

48:45.520 --> 48:47.360
and the developmental time scale

48:47.360 --> 48:50.040
and now in terms of the technological time scale,

48:50.040 --> 48:55.200
there are very gradual, very small, very slow changes

48:55.200 --> 48:57.680
that go all the way back from what people think of

48:57.680 --> 49:00.400
as a typical human and their intelligence,

49:00.400 --> 49:03.720
all the way back to very different types of organisms.

49:03.720 --> 49:06.320
And developmental biology and of course evolution too

49:06.320 --> 49:09.640
offers absolutely no place to put a sharp line

49:09.640 --> 49:13.040
and say this creature was not,

49:13.120 --> 49:15.560
pick an adjective, intelligent, cognitive, conscious,

49:15.560 --> 49:17.200
well, whatever you like, pick an adjective

49:17.200 --> 49:19.040
to say this creature was not it,

49:19.040 --> 49:22.040
but it had some offspring and the offspring now are, right?

49:22.040 --> 49:24.080
That just doesn't exist

49:24.080 --> 49:26.400
because all of these changes are very slow

49:26.400 --> 49:27.680
and very continuous.

49:27.680 --> 49:31.880
And so there were changes during evolution,

49:31.880 --> 49:34.760
we all start life as a single cell.

49:34.760 --> 49:37.040
In the future, there will be all kinds of changes

49:37.040 --> 49:39.640
to our bodies with biological

49:39.640 --> 49:41.840
and engineered kinds of devices.

49:41.840 --> 49:43.840
And so all of these are continua

49:43.840 --> 49:47.400
of really novel types of embodiments.

49:47.400 --> 49:52.400
And we build certain kinds of conceptual metaphors

49:53.080 --> 49:56.520
that try to distinguish different categories here,

49:56.520 --> 49:59.360
but these are discrete tools,

49:59.360 --> 50:01.680
the phenomena themselves are deeply continuous

50:01.680 --> 50:02.680
on multi-scale.

50:02.680 --> 50:05.040
And to give you just a simple idea

50:05.040 --> 50:07.320
and then we'll enlarge on this is this.

50:07.320 --> 50:12.320
And so this caterpillar is a kind of soft-bodied robot

50:12.320 --> 50:14.200
that lives in a two-dimensional world

50:14.200 --> 50:15.680
that crawls around on leaves.

50:15.680 --> 50:18.120
It likes to chew plants and it has this brain

50:18.120 --> 50:20.480
that's very, very suitable for this purpose.

50:20.480 --> 50:22.920
What it needs to do is turn into this creature,

50:22.920 --> 50:24.280
which is completely different.

50:24.280 --> 50:26.000
It lives in the three-dimensional world,

50:26.000 --> 50:27.960
it doesn't care about the leaves at all,

50:27.960 --> 50:30.800
it wants nectar and it flies and it does various things.

50:30.800 --> 50:34.160
And so during this process, there is a metamorphosis

50:34.160 --> 50:36.280
where not on an evolutionary time scale,

50:36.320 --> 50:38.400
during the lifetime of the individual,

50:38.400 --> 50:40.520
the brain is basically dissociated

50:40.520 --> 50:42.960
and rebuilt into a new architecture.

50:42.960 --> 50:47.960
And by the way, there are data that memories persist.

50:47.960 --> 50:49.280
So if you train the caterpillar,

50:49.280 --> 50:51.200
the butterfly or moth still remembers

50:51.200 --> 50:52.480
the original information,

50:52.480 --> 50:54.000
but you can sort of think about,

50:55.480 --> 50:57.560
nevermind the question of what's it like to be a butterfly,

50:57.560 --> 50:59.160
what's it like to be a caterpillar

50:59.160 --> 51:01.400
changing into a butterfly, right?

51:02.400 --> 51:06.920
That process of slow, but drastic change

51:06.920 --> 51:08.440
in your embodiment.

51:08.440 --> 51:10.840
And so from here, we can just remembering

51:10.840 --> 51:12.400
that we are all made of parts

51:12.400 --> 51:16.080
that can modify during our lifetime.

51:16.080 --> 51:17.640
We can ask some interesting questions.

51:17.640 --> 51:19.600
For example, you look at a brain

51:19.600 --> 51:22.640
and we're sort of conditioned to expect that it's obvious

51:22.640 --> 51:27.480
that a brain contains one human worth of intelligence.

51:27.480 --> 51:29.240
But this is just because we're used to that

51:29.240 --> 51:30.360
in terms of our interactions.

51:30.360 --> 51:31.560
If I showed you a brain

51:31.560 --> 51:34.120
and you didn't know what this was

51:34.120 --> 51:36.800
and I asked you how many different cells are in there,

51:36.800 --> 51:37.720
you would actually have,

51:37.720 --> 51:40.000
we have no ability to answer that question.

51:40.000 --> 51:43.200
We have no way to ask how much,

51:43.200 --> 51:45.320
and I think Yosha got to some of this,

51:45.320 --> 51:47.480
how much of this real estate is necessary

51:47.480 --> 51:51.040
for one human's worth of performance?

51:51.040 --> 51:54.200
We have no idea how much is actually in there.

51:54.200 --> 51:56.840
And actually, very interestingly,

51:56.840 --> 52:00.920
the same issue occurs in embryonic development.

52:00.920 --> 52:03.920
So we all begin as a cellular blastoderm.

52:03.920 --> 52:06.920
So this is a sheet, a two-dimensional sheet of cells.

52:06.920 --> 52:09.720
And that sheet turns into an embryo.

52:09.720 --> 52:10.600
Now, what does that mean?

52:10.600 --> 52:12.200
First of all, can we guess in advance

52:12.200 --> 52:14.640
how many embryos are going to come from that sheet?

52:14.640 --> 52:17.160
Actually, we cannot, and I'll show you why.

52:17.160 --> 52:18.440
And it's not genetics.

52:18.440 --> 52:21.440
And then there's the question of what are we actually counting?

52:21.440 --> 52:23.600
When we count an embryo, I mean, there's 50,000 cells,

52:23.600 --> 52:25.520
let's say here, what is it that we're counting

52:25.520 --> 52:26.680
when we say there's an embryo?

52:26.680 --> 52:28.320
What are we actually counting when we say there's

52:28.320 --> 52:31.880
a single human inhabitant in this bunch of tissue?

52:31.880 --> 52:34.880
So one of the things that you can do in embryogenesis

52:34.880 --> 52:37.600
is you take this blastoderm and you take a little needle

52:37.600 --> 52:42.600
and you put some kind of scratches into this blastoderm.

52:43.160 --> 52:44.880
And then they heal, but before they heal,

52:44.880 --> 52:47.440
what will happen is that each of these regions

52:47.440 --> 52:50.240
being isolated from the other regions

52:50.240 --> 52:51.760
decides to organize an embryo

52:51.760 --> 52:54.760
because they don't know for a while anyway

52:54.760 --> 52:55.880
that the other regions are there.

52:55.880 --> 52:58.600
Then when it heals up, it becomes conjoined twins.

52:58.600 --> 53:00.520
And you can do this very easily in chicken and duck

53:00.520 --> 53:03.320
and other embryos, but humans work exactly the same way.

53:03.320 --> 53:05.840
And so then there will be multiple embryos

53:05.840 --> 53:07.560
within the same blastoderm.

53:07.560 --> 53:09.320
And then there will be some disputed zones here.

53:09.320 --> 53:10.680
There's some cells that aren't quite sure

53:10.680 --> 53:12.080
which one they belong to.

53:12.080 --> 53:15.760
But this deep idea of individuation,

53:15.760 --> 53:18.560
of taking some kind of a continuous,

53:18.560 --> 53:20.000
in fact, it's even worse than continuous

53:20.000 --> 53:22.520
because it's multi-scale substrate

53:22.520 --> 53:26.800
and having itself organized into discrete what?

53:26.800 --> 53:27.920
So in the case of embryos,

53:27.920 --> 53:30.520
what you have are discrete groups of cells

53:30.520 --> 53:33.880
that are trying to follow anatomical goals.

53:33.880 --> 53:36.080
They're trying to achieve particular walks

53:36.080 --> 53:37.440
in anatomical space.

53:37.440 --> 53:39.520
They're gonna construct the right number of fingers,

53:39.520 --> 53:42.600
the right number of eyes, whatever it is.

53:42.600 --> 53:45.360
Same thing in cognitive development.

53:45.360 --> 53:48.880
There are issues, there are disorders of individuation

53:48.880 --> 53:50.640
that you see in split brain patients

53:50.640 --> 53:52.320
and dissociations and so on.

53:52.320 --> 53:54.200
So this question of how many are in there

53:54.200 --> 53:57.720
is deeply interesting and it gets to the bottom

53:57.720 --> 54:00.920
of what it means to be a coherent agent

54:00.920 --> 54:02.800
when you're made of parts.

54:02.800 --> 54:05.560
And I think Alan Turing, although as far as I can tell,

54:05.560 --> 54:07.120
he didn't write directly about this.

54:07.120 --> 54:09.320
I think he was well aware of this issue

54:09.320 --> 54:11.760
because of course he was interested in intelligence

54:11.760 --> 54:15.880
and generic embodiment and so on.

54:15.880 --> 54:17.760
But he was also interested in morphogenesis.

54:17.760 --> 54:21.040
He wrote this paper on biological morphogenesis.

54:21.040 --> 54:23.160
And I think he understood that these are deeply

54:23.160 --> 54:24.720
and profoundly the same problem.

54:24.720 --> 54:25.960
The problem of morphogenesis

54:25.960 --> 54:28.560
and the problem of the mind are the same problem

54:28.560 --> 54:32.920
because of this emphasis on emerging

54:32.920 --> 54:35.520
as a coherent entity from multiple parts.

54:35.520 --> 54:38.320
So people often talk about, well,

54:38.320 --> 54:41.000
ants and termites are some kind of collective intelligence

54:41.000 --> 54:43.320
and we can argue about what that means,

54:43.320 --> 54:47.120
but we are really a unified, a centralized intelligence.

54:47.120 --> 54:49.840
We're not like bird flocks or ant colonies.

54:49.840 --> 54:50.920
But actually, of course,

54:50.920 --> 54:53.600
all biological systems are made of parts.

54:53.600 --> 54:56.920
And so we too are a kind of collective intelligence.

54:56.920 --> 54:58.920
What's interesting is the scaling interface

54:58.920 --> 55:02.600
is what is it that allows these individual subunits

55:02.600 --> 55:05.920
to work together and present to other intelligences

55:05.920 --> 55:08.760
to themselves, by the way, and to the environment

55:08.760 --> 55:11.960
in a picture of a coherent agent.

55:11.960 --> 55:14.280
So this is the journey that we all took.

55:14.280 --> 55:17.040
We began life as a piece of physics.

55:17.040 --> 55:18.880
So basically as a quiescent oocyte.

55:18.880 --> 55:21.640
So it's a blob of chemicals, not doing terribly much.

55:21.640 --> 55:23.840
And then through this incredibly,

55:23.840 --> 55:26.680
just magical process of embryonic development,

55:26.680 --> 55:29.280
that we arrive at something like this,

55:29.280 --> 55:32.440
which is a complex organism with metacognitive capacity

55:32.440 --> 55:33.520
that's going to make statements

55:33.520 --> 55:35.720
about how we're not just machines

55:35.720 --> 55:39.040
and we're different than physics and all that.

55:39.040 --> 55:42.560
But this whole process is extremely smooth and gradual.

55:42.560 --> 55:44.080
It happens second by second.

55:44.080 --> 55:46.000
There is no lightning flash

55:46.000 --> 55:47.920
at which point physics becomes mind.

55:47.920 --> 55:49.320
It's a gradual process.

55:49.320 --> 55:52.240
And we can talk about face transitions and such,

55:52.240 --> 55:56.080
but there's really not that much evidence

55:56.080 --> 55:56.920
for any of that.

55:56.920 --> 55:58.840
It's a very continuous process.

55:58.840 --> 56:00.200
So this is the kind of thing.

56:00.200 --> 56:03.080
So I think, Yosha alluded to this a few times.

56:03.080 --> 56:04.160
This is the sort of thing.

56:04.160 --> 56:05.160
I mean, not exactly this.

56:05.160 --> 56:08.040
This is a lacrimaria, it's a free living organism.

56:08.040 --> 56:09.760
But here's a single cell, right?

56:09.760 --> 56:11.080
This is what we are made of.

56:11.080 --> 56:13.440
These guys, there's no brain.

56:13.440 --> 56:16.080
Here's those, there's no nervous system.

56:16.240 --> 56:19.160
This is the single cell creature in real time

56:19.160 --> 56:22.440
using all of the intelligence of its chemical networks.

56:22.440 --> 56:23.400
And we can talk about this.

56:23.400 --> 56:25.800
I mean, that quite literally chemical networks can learn

56:25.800 --> 56:28.120
and they can do inference and many other things.

56:29.120 --> 56:30.560
Even though it's a single organism

56:30.560 --> 56:33.960
is handling all of its single cell agendas

56:33.960 --> 56:35.360
in its environment.

56:35.360 --> 56:39.240
So metabolically, physiologically, anatomically,

56:39.240 --> 56:42.120
it's doing what it needs to do.

56:42.120 --> 56:45.440
And so we are made of extremely competent parts.

56:45.440 --> 56:46.360
Here's another example.

56:46.360 --> 56:49.200
This is a, this whole thing, you'll see, you'll see this.

56:49.200 --> 56:50.160
I'm gonna pause it.

56:50.160 --> 56:51.720
Whoops.

56:51.720 --> 56:52.560
I'm gonna pause this.

56:52.560 --> 56:53.600
This whole thing right here,

56:53.600 --> 56:55.960
this is called Pfizer and polycephalum.

56:55.960 --> 56:57.200
It's a slime mold.

56:57.200 --> 56:59.160
The whole thing is one cell, okay?

56:59.160 --> 57:01.400
And what it's, what I'm showing you here

57:01.400 --> 57:03.240
is that it's sitting in this environment.

57:03.240 --> 57:04.840
These are three glass discs.

57:04.840 --> 57:06.560
These are very, very light.

57:06.560 --> 57:07.960
There's no chemicals, there's no food.

57:07.960 --> 57:09.440
It's just glass, inner glass.

57:09.440 --> 57:11.040
There's one glass disc here.

57:11.040 --> 57:12.880
And what it's going to do is,

57:12.880 --> 57:17.240
it's going to, for the first few hours,

57:17.240 --> 57:20.440
it's going to just generically grow in all directions here.

57:20.440 --> 57:21.920
What it's doing during this process

57:21.920 --> 57:23.680
is it's tugging on its substrate

57:23.680 --> 57:26.080
and feeling the vibrations that gets back.

57:26.080 --> 57:28.360
And it can sense the strain angle

57:28.360 --> 57:29.840
of the objects in its environment.

57:29.840 --> 57:31.600
And then we'll eventually reliably grow out

57:31.600 --> 57:32.840
to the heavier mass.

57:32.840 --> 57:34.600
But during this, so that'll happen at this point,

57:34.600 --> 57:37.440
but during this time is when it's processing

57:37.440 --> 57:40.240
that information and learning from its environment.

57:40.240 --> 57:42.440
And then boom, now the behavior begins.

57:42.560 --> 57:44.400
So single cells are very competent,

57:44.400 --> 57:47.400
even microbial single cells.

57:47.400 --> 57:50.160
And so what we have to understand is that biology,

57:50.160 --> 57:52.600
so here's a principle that I think is really important

57:52.600 --> 57:56.800
for future AI, biology is deeply nested.

57:56.800 --> 57:58.960
That is not merely structurally,

57:58.960 --> 58:00.960
I mean, that's obvious we're made of organs, tissues,

58:00.960 --> 58:04.240
and so on, but each layer is competent.

58:04.240 --> 58:06.600
It solves problems in its own space.

58:06.600 --> 58:09.520
All of these things from molecular networks

58:09.520 --> 58:12.280
all the way up to whole organs and beyond

58:12.280 --> 58:15.400
are solving specific problems in specific spaces.

58:15.400 --> 58:16.920
So we are really interested in my group,

58:16.920 --> 58:20.400
we're really interested in creating a framework

58:20.400 --> 58:24.120
that allows us to relate to really

58:24.120 --> 58:25.640
very diverse intelligences.

58:25.640 --> 58:28.840
So, of course, familiar creatures,

58:28.840 --> 58:30.400
all kinds of weird biologicals,

58:30.400 --> 58:33.560
colonial organisms, swarms, of course new,

58:33.560 --> 58:35.520
and I'll show you some in a couple of minutes,

58:35.520 --> 58:38.680
new engineered creatures, artificial intelligences,

58:38.680 --> 58:41.520
and maybe at some point exobiological,

58:41.520 --> 58:42.680
truly alien agents.

58:42.680 --> 58:44.200
We need to be able to deal with all of this.

58:44.200 --> 58:47.120
It's not enough to deal with crows and monkeys

58:47.120 --> 58:50.000
and then maybe octopus, that's way too narrow.

58:50.000 --> 58:52.520
And so, of course, this is an idea

58:52.520 --> 58:54.240
that has been addressed before.

58:54.240 --> 58:58.240
So here's Wiener and colleagues trying to come up

58:58.240 --> 59:00.560
with a very sort of cybernetic way

59:00.560 --> 59:03.480
to classify different degrees of behavior

59:03.480 --> 59:05.680
all the way from passive mechanical behavior

59:05.680 --> 59:07.760
up to complex cognition

59:07.800 --> 59:12.200
in a way that abstracts from its familiar embodiments.

59:12.200 --> 59:13.800
So there's no talk of brains or neurons

59:13.800 --> 59:14.640
or anything like that.

59:14.640 --> 59:17.040
This is very sort of functionalist.

59:17.040 --> 59:20.840
And one thing about us as humans

59:20.840 --> 59:25.840
is that we are very primed to recognize intelligence

59:27.280 --> 59:28.720
in the three-dimensional space.

59:28.720 --> 59:31.800
So basically, medium-sized objects moving at medium speeds

59:31.800 --> 59:32.960
through three-dimensional space.

59:32.960 --> 59:34.800
When we see it, we know what agency looks like,

59:34.800 --> 59:36.720
we know what intelligence looks like.

59:36.720 --> 59:38.600
But we are really bad at,

59:38.600 --> 59:40.560
and this is why we must get better at it,

59:40.560 --> 59:43.880
recognizing intelligence in other types of problem spaces.

59:43.880 --> 59:47.080
So imagine if you had a direct feeling

59:47.080 --> 59:49.160
of all of your blood chemistry.

59:49.160 --> 59:51.040
If you were able to feel your blood chemistry

59:51.040 --> 59:53.680
the way that you can see objects in three-dimensional space,

59:53.680 --> 59:56.120
you would be very obvious

59:56.120 --> 59:57.920
that your kidneys, your liver, and so on

59:57.920 --> 59:59.960
have a degree of intelligence

59:59.960 --> 01:00:02.640
and they're doing amazing things in their problem spaces.

01:00:02.640 --> 01:00:04.920
So we study how individual cells

01:00:04.920 --> 01:00:07.520
navigate the space of gene expression,

01:00:07.520 --> 01:00:10.400
hopefully the physiology and morpho space,

01:00:10.400 --> 01:00:11.720
the space of patterns.

01:00:11.720 --> 01:00:14.360
This is what I'm talking about today.

01:00:14.360 --> 01:00:15.400
And just for a few minutes here,

01:00:15.400 --> 01:00:18.520
here's an example of cells solving

01:00:18.520 --> 01:00:21.600
an entirely novel problem in genetic space.

01:00:21.600 --> 01:00:24.040
So here's a planarian, this is a flatworm.

01:00:24.040 --> 01:00:26.760
They regenerate parts of their body when amputated.

01:00:26.760 --> 01:00:28.720
What we did was we exposed planaria

01:00:28.720 --> 01:00:30.080
to a solution of barium.

01:00:30.080 --> 01:00:32.720
Barium blocks all of their potassium channels,

01:00:32.720 --> 01:00:35.400
the cells and the neurons are really unhappy.

01:00:35.400 --> 01:00:37.960
Their heads explode, literally just explode.

01:00:38.960 --> 01:00:41.160
Over the next week or so,

01:00:41.160 --> 01:00:43.560
they rebuild, keeping them in the barium,

01:00:43.560 --> 01:00:45.280
they rebuild a brand new head.

01:00:45.280 --> 01:00:47.040
The new head doesn't care about barium at all.

01:00:47.040 --> 01:00:48.600
So we asked the simple question, how can that be?

01:00:48.600 --> 01:00:49.960
What is the new head doing

01:00:49.960 --> 01:00:51.600
that the original head couldn't do?

01:00:51.600 --> 01:00:54.080
And we found out that there's actually very few genes

01:00:54.080 --> 01:00:56.240
that the system up and down regulated

01:00:56.240 --> 01:00:59.320
to be able to do its business in the presence of barium.

01:00:59.320 --> 01:01:02.520
The kicker is, planaria never get exposed to barium

01:01:03.400 --> 01:01:04.480
in the real world.

01:01:04.480 --> 01:01:07.240
There is no ecological precedent for this.

01:01:07.240 --> 01:01:09.880
So just imagine, you're a cell,

01:01:09.880 --> 01:01:12.280
you've got, I don't know, tens of thousands

01:01:12.280 --> 01:01:15.000
of possible genes, you've got a disaster,

01:01:15.000 --> 01:01:16.720
a physiological disaster.

01:01:16.720 --> 01:01:19.080
You don't have time to try every combination.

01:01:19.080 --> 01:01:22.320
There is no time to try everything

01:01:22.320 --> 01:01:23.880
and whoever survives, survives.

01:01:23.880 --> 01:01:25.960
These cells don't turn over that fast.

01:01:25.960 --> 01:01:27.840
You have to solve this novel problem,

01:01:27.840 --> 01:01:29.800
possibly by generalizing,

01:01:29.800 --> 01:01:31.480
because you've never seen barium before,

01:01:31.480 --> 01:01:33.160
but you have seen epilepsy before.

01:01:33.160 --> 01:01:35.600
And barium excitability might look a little bit

01:01:35.600 --> 01:01:36.440
like epilepsy.

01:01:36.440 --> 01:01:38.200
And so maybe you can do some of the same things.

01:01:38.200 --> 01:01:40.760
So this idea of solving novel problems

01:01:40.760 --> 01:01:44.000
in physiological space is one example

01:01:44.000 --> 01:01:45.480
of what biology can do.

01:01:45.480 --> 01:01:46.840
But here's another example.

01:01:46.840 --> 01:01:48.720
So this is how we all start

01:01:48.720 --> 01:01:51.400
as a kind of a collection of early cells.

01:01:51.400 --> 01:01:54.160
But this is a cross-section through a human torso.

01:01:54.160 --> 01:01:56.120
Now look at the incredible order here, right?

01:01:56.120 --> 01:01:59.560
All the tissues, the organs, everything is in the right place,

01:01:59.560 --> 01:02:02.560
the right size and shape and relative to each other.

01:02:02.560 --> 01:02:04.560
Where does that come from?

01:02:04.560 --> 01:02:06.160
You might be tempted to say DNA,

01:02:06.160 --> 01:02:08.160
but of course we can read genomes now

01:02:08.160 --> 01:02:09.920
and what's in the DNA isn't any of that.

01:02:09.920 --> 01:02:12.360
What's in the DNA is the sequence

01:02:12.360 --> 01:02:14.600
of the micro level sort of hardware

01:02:14.600 --> 01:02:16.200
that every cell gets to have, the proteins.

01:02:16.200 --> 01:02:17.680
That's what the DNA specified.

01:02:17.680 --> 01:02:20.440
So you really, you still need to understand the physiology

01:02:20.440 --> 01:02:23.640
by which these cells compute what to do here.

01:02:23.640 --> 01:02:25.080
And then there are lots of questions,

01:02:25.080 --> 01:02:26.640
as regenerative medicine workers,

01:02:26.640 --> 01:02:29.040
we try to figure out what do we say to these cells

01:02:29.040 --> 01:02:31.720
to rebuild pieces that are missing?

01:02:31.720 --> 01:02:34.120
And as engineers, we want to know what's actually possible?

01:02:34.120 --> 01:02:35.440
What can you reprogram this?

01:02:35.440 --> 01:02:37.040
Can you make them do something else?

01:02:37.040 --> 01:02:38.600
So the amazing thing about development

01:02:38.600 --> 01:02:42.200
is that while it is incredibly reliable and robust

01:02:42.200 --> 01:02:44.360
and in fact hides all of its intelligence from us

01:02:44.360 --> 01:02:47.000
when we see acorns giving rights to oak trees

01:02:47.000 --> 01:02:49.560
and frog eggs make frogs, we sort of assume,

01:02:49.560 --> 01:02:50.520
well, what else is it gonna do?

01:02:50.520 --> 01:02:51.440
Like that's obvious, right?

01:02:51.440 --> 01:02:52.800
That's how it has to be.

01:02:52.800 --> 01:02:55.480
But that's only what happens on the default condition.

01:02:55.480 --> 01:02:57.040
What we find out is that, for example,

01:02:57.040 --> 01:02:59.480
if you take an early embryo and cut it in half,

01:02:59.480 --> 01:03:00.840
you don't get two half bodies.

01:03:00.840 --> 01:03:03.320
You get two perfectly normal monosygotic twins.

01:03:03.320 --> 01:03:05.480
And in fact, more generally,

01:03:05.480 --> 01:03:09.120
the process of development can navigate this anatomical space

01:03:09.120 --> 01:03:10.880
in a way to reach the same goal

01:03:10.880 --> 01:03:12.680
from different starting positions

01:03:12.680 --> 01:03:15.120
despite really drastic perturbations

01:03:15.120 --> 01:03:17.040
by taking different paths.

01:03:17.040 --> 01:03:19.480
It's not just a hardwired set of emergent.

01:03:19.480 --> 01:03:20.640
This is not about emergence.

01:03:20.640 --> 01:03:22.680
Of course, complex things emerge from simple rules.

01:03:22.680 --> 01:03:23.800
This isn't that at all.

01:03:23.800 --> 01:03:26.720
This is the ability of the system to get to its goal

01:03:26.720 --> 01:03:29.520
despite really, really radical changes.

01:03:29.520 --> 01:03:31.040
So here's one change, here's another change.

01:03:31.040 --> 01:03:34.840
As an adult, some organisms like the salamander,

01:03:34.840 --> 01:03:37.520
they regenerate their eyes, their limbs, their jaws,

01:03:37.520 --> 01:03:41.360
their tails, you can make cuts anywhere you like along here.

01:03:41.360 --> 01:03:43.920
And these cells will very rapidly grow

01:03:43.920 --> 01:03:47.720
and undergo morphogenesis, and then they will stop.

01:03:47.720 --> 01:03:48.840
When do they stop?

01:03:48.840 --> 01:03:51.920
They stop when a correct salamander limb has formed.

01:03:51.920 --> 01:03:53.080
Doesn't matter where you cut it,

01:03:53.080 --> 01:03:54.920
it will only grow exactly the right amount

01:03:54.960 --> 01:03:58.120
and it will stop when exactly the right thing has formed.

01:03:58.120 --> 01:04:00.640
So you've got some sort of error minimization scheme

01:04:00.640 --> 01:04:01.480
going on here.

01:04:01.480 --> 01:04:02.640
It knows exactly what it looks like.

01:04:02.640 --> 01:04:05.240
It knows what the target state is.

01:04:05.240 --> 01:04:08.320
And in fact, this is something that we discovered

01:04:08.320 --> 01:04:11.040
that, so this is a tadpole here, some eyes,

01:04:11.040 --> 01:04:12.840
here's the brain, the gut, the nostrils.

01:04:12.840 --> 01:04:15.640
These tadpoles have to become frogs.

01:04:15.640 --> 01:04:17.680
In order to become frogs, they have to rearrange their face.

01:04:17.680 --> 01:04:19.480
The jaws have to move, the nostrils have to,

01:04:19.480 --> 01:04:20.680
everything has to move.

01:04:20.680 --> 01:04:22.360
We find, and so you might imagine

01:04:22.360 --> 01:04:26.720
that this is some sort of hardwired set of emergent outcomes

01:04:26.720 --> 01:04:28.200
where every organ gets displaced

01:04:28.200 --> 01:04:30.440
to its appropriate distance and direction.

01:04:30.440 --> 01:04:32.840
So we made what's called Picasso tadpoles.

01:04:32.840 --> 01:04:34.680
Basically, we scrambled everything

01:04:34.680 --> 01:04:36.040
so that everything's in the wrong place.

01:04:36.040 --> 01:04:37.480
The eyes are off to the side of the head,

01:04:37.480 --> 01:04:38.960
the jaws are on the other side of everything

01:04:38.960 --> 01:04:40.200
is just scrambled.

01:04:40.200 --> 01:04:41.560
Because we have this hypothesis

01:04:41.560 --> 01:04:45.680
that this is more intelligent than people gave a credit for.

01:04:45.680 --> 01:04:50.000
Sure enough, what these guys do is every structure moves

01:04:50.040 --> 01:04:52.320
in novel paths and keeps moving,

01:04:52.320 --> 01:04:53.680
no matter where it started from,

01:04:53.680 --> 01:04:56.680
until it gets to be a pretty normal looking frog.

01:04:56.680 --> 01:04:59.080
So what the genetics gives you is not a piece of hardware

01:04:59.080 --> 01:05:00.720
that does the same thing all the time.

01:05:00.720 --> 01:05:04.920
It gives you a machine that can recognize unexpected changes

01:05:04.920 --> 01:05:06.880
and take corrective action as needed

01:05:06.880 --> 01:05:08.520
to get to the same goal.

01:05:08.520 --> 01:05:11.240
The most amazing part of this is that in doing this,

01:05:11.240 --> 01:05:13.560
and this is an example of top-down causation,

01:05:13.560 --> 01:05:16.360
which is why it's really important to understand this,

01:05:16.560 --> 01:05:18.720
how high level goals filter down

01:05:18.720 --> 01:05:21.360
to the sort of implementation machinery,

01:05:21.360 --> 01:05:24.400
is that what you see is that this is an example

01:05:24.400 --> 01:05:27.480
from the kidney tubule of a nut.

01:05:27.480 --> 01:05:28.960
If you take it in cross-section,

01:05:28.960 --> 01:05:32.000
normally there's, I don't know, eight or nine cells

01:05:32.000 --> 01:05:35.000
that work together to make the lumen of that tubule.

01:05:35.000 --> 01:05:38.000
But one thing you can do is you can force these cells

01:05:38.000 --> 01:05:39.160
to be gigantic.

01:05:39.160 --> 01:05:41.400
And when you do this, when you make them larger,

01:05:41.400 --> 01:05:43.480
fewer cells will do this,

01:05:43.480 --> 01:05:46.000
forming exactly the same lumen diameter

01:05:46.000 --> 01:05:47.960
until you make the cells so large

01:05:47.960 --> 01:05:50.600
that a single cell will wrap around itself

01:05:50.600 --> 01:05:53.680
to give you the same structure.

01:05:53.680 --> 01:05:55.520
What's amazing about that is that these are completely

01:05:55.520 --> 01:05:57.040
different molecular mechanisms.

01:05:57.040 --> 01:05:58.760
This is cell-to-cell communication.

01:05:58.760 --> 01:06:00.360
This is cytoskeletal bending.

01:06:00.360 --> 01:06:02.920
So in the service of a high-level goal,

01:06:02.920 --> 01:06:05.440
meaning make this large-scale anatomical structure,

01:06:05.440 --> 01:06:09.360
different molecular mechanisms get activated, okay?

01:06:09.360 --> 01:06:12.360
And this is very unusual.

01:06:12.360 --> 01:06:14.080
This idea is very unusual in biology.

01:06:15.080 --> 01:06:17.440
The biologists tend to think about things emerging

01:06:17.440 --> 01:06:20.560
from molecules not going the other way,

01:06:20.560 --> 01:06:23.000
but it has certain parallels in computer science

01:06:23.000 --> 01:06:25.760
where the algorithm makes the electrons dance

01:06:25.760 --> 01:06:26.920
in an important way, right?

01:06:26.920 --> 01:06:29.000
In a functionally important way.

01:06:29.000 --> 01:06:33.320
And so what we've been doing is trying to build models

01:06:33.320 --> 01:06:35.720
that go, sort of full-stack models that go all the way up

01:06:35.720 --> 01:06:39.160
from molecular kinds of activities

01:06:39.160 --> 01:06:43.360
that set the ion channels and other things in the membrane.

01:06:43.360 --> 01:06:45.920
Two, we specifically, I don't have too much time today,

01:06:45.920 --> 01:06:48.000
but we specifically study bioelectrics.

01:06:48.000 --> 01:06:50.680
We study how all cells, not just neurons,

01:06:50.680 --> 01:06:52.840
all cells use electrical signaling

01:06:52.840 --> 01:06:55.400
to form computational networks.

01:06:55.400 --> 01:06:59.680
And so we study what the tissue-level electrical patterns

01:06:59.680 --> 01:07:03.040
look like and then what the organ-level patterns look like

01:07:03.040 --> 01:07:07.000
and then how that becomes literally an algorithmic set

01:07:07.000 --> 01:07:09.320
of steps that determines things like how many heads

01:07:09.320 --> 01:07:11.040
a flower is going to have.

01:07:11.080 --> 01:07:15.520
And during this process, we want to know a few things.

01:07:15.520 --> 01:07:18.320
We want to know how does the cognitive light cone,

01:07:18.320 --> 01:07:21.880
and what I mean by that is simply the spatiotemporal size,

01:07:21.880 --> 01:07:24.080
the scale of the largest goal

01:07:24.080 --> 01:07:27.280
that that particular system can conceive of pursuing, right?

01:07:27.280 --> 01:07:28.320
So if you're a bacterium,

01:07:28.320 --> 01:07:29.920
your cognitive light cone is very tiny

01:07:29.920 --> 01:07:31.440
because really all you care about

01:07:31.440 --> 01:07:33.120
is the local sugar concentration

01:07:33.120 --> 01:07:36.840
with about maybe 10 minutes forward and back.

01:07:36.840 --> 01:07:39.360
But if you're a human, you can have gigantic goals

01:07:39.360 --> 01:07:40.480
that exceed your lifespan.

01:07:40.480 --> 01:07:41.840
It can be planetary-scale goals.

01:07:41.840 --> 01:07:44.840
And then of course, every kind of creature in between.

01:07:44.840 --> 01:07:46.840
So we define this kind of cognitive light cone

01:07:46.840 --> 01:07:50.640
based around the types of goals that a system can pursue.

01:07:50.640 --> 01:07:52.640
And so we need to understand during this process,

01:07:52.640 --> 01:07:54.240
how do the goals enlarge?

01:07:54.240 --> 01:07:56.640
How do they shift into different spaces?

01:07:56.640 --> 01:07:59.600
So individual cells care about things in metabolic space

01:07:59.600 --> 01:08:01.840
and physiological space and transcriptional space.

01:08:01.840 --> 01:08:02.760
Those are their goals.

01:08:02.760 --> 01:08:05.520
Collectives of cells care about very much larger goals,

01:08:05.520 --> 01:08:07.200
such as the shape of your hand

01:08:07.200 --> 01:08:08.560
and the fact that you have to have to have

01:08:08.560 --> 01:08:10.240
exactly five fingers.

01:08:10.240 --> 01:08:12.440
And then of course, this question of where do these goals come

01:08:12.440 --> 01:08:13.280
from in the first place,

01:08:13.280 --> 01:08:14.840
we'll address that momentarily.

01:08:14.840 --> 01:08:17.240
So about the only piece of bioelectricity,

01:08:17.240 --> 01:08:19.760
I'm gonna show you because Yosha brought up this idea

01:08:19.760 --> 01:08:23.080
of counterfactual memories is simply this.

01:08:24.080 --> 01:08:27.440
We treat the behavior of the cells and tissues

01:08:27.440 --> 01:08:29.160
as a collective intelligence.

01:08:29.160 --> 01:08:31.840
Literally the group of cells is a collective intelligence

01:08:31.840 --> 01:08:34.960
that tries to solve problems in anatomical space.

01:08:34.960 --> 01:08:37.320
And because we have some understanding now

01:08:37.320 --> 01:08:40.160
of what the medium is of that collective intelligence,

01:08:40.160 --> 01:08:42.880
not shockingly just like in the brain, it's bioelectric.

01:08:42.880 --> 01:08:46.040
Why? Because that's how the brain learned its tricks.

01:08:46.040 --> 01:08:47.800
You already heard and Yosha is absolutely right.

01:08:47.800 --> 01:08:52.800
There are very difficult tasks to try to distinguish

01:08:54.480 --> 01:08:56.440
what makes a neuron different from other cells

01:08:56.440 --> 01:09:00.360
because even bacteria from the time of microbial biofilms

01:09:00.360 --> 01:09:03.040
have already been using all of the same tricks

01:09:03.040 --> 01:09:07.320
as the brain uses, this electrical network stuff is ancient.

01:09:07.320 --> 01:09:09.680
And so what we are able to do is read and write

01:09:09.680 --> 01:09:13.200
the memories of this collective intelligence.

01:09:13.200 --> 01:09:15.080
And so we use a specific technique

01:09:15.080 --> 01:09:16.880
that reads the electrical gradients.

01:09:16.880 --> 01:09:18.640
This is just like neural decoding

01:09:18.640 --> 01:09:20.720
as the neuroscientists try to do in the brain.

01:09:20.720 --> 01:09:22.960
So here there's a particular pattern that says,

01:09:22.960 --> 01:09:25.240
if injured, you're going to make one head.

01:09:25.240 --> 01:09:27.480
We can rewrite that and we can create a worm.

01:09:27.480 --> 01:09:29.440
Here is where the pattern says,

01:09:29.440 --> 01:09:32.080
no, actually a correct worm should have two heads.

01:09:32.080 --> 01:09:34.600
And if you go ahead and cut that animal,

01:09:34.600 --> 01:09:36.200
they will go ahead and make two heads.

01:09:36.200 --> 01:09:37.920
This is not Photoshop, these are real,

01:09:37.920 --> 01:09:39.600
real two-headed malaria.

01:09:39.600 --> 01:09:41.000
But the cool thing about this pattern

01:09:41.000 --> 01:09:43.000
is this is not a reading of this animal.

01:09:43.000 --> 01:09:45.640
This is a reading of this perfectly normal,

01:09:45.640 --> 01:09:48.720
anatomically one-headed, genetically,

01:09:48.720 --> 01:09:50.640
transcriptionally one-headed animal.

01:09:50.640 --> 01:09:52.720
So this is a kind of counterfactual memory.

01:09:52.720 --> 01:09:54.760
It's a representation of a state

01:09:54.760 --> 01:09:57.320
that it's what you are going to do in the future

01:09:57.320 --> 01:09:58.320
if you get injured.

01:09:58.320 --> 01:09:59.840
If you don't get injured, it stays latent,

01:09:59.840 --> 01:10:00.960
it never comes up.

01:10:01.840 --> 01:10:03.520
And we have lots more data on this.

01:10:03.520 --> 01:10:06.800
We can actually make heads of other species of worms

01:10:06.800 --> 01:10:08.160
and many other things.

01:10:08.160 --> 01:10:09.960
The idea is that a single body can

01:10:09.960 --> 01:10:12.960
store one of two different representations

01:10:12.960 --> 01:10:15.800
of what the goal state is going to be if they get injured

01:10:15.800 --> 01:10:17.680
and then they build to that goal state.

01:10:17.680 --> 01:10:19.040
So this should sound very familiar.

01:10:19.040 --> 01:10:21.520
This is both the nervous system works this way

01:10:21.520 --> 01:10:23.520
and of course, reprogrammable devices work this way.

01:10:23.520 --> 01:10:26.440
The same hardware can hold onto multiple

01:10:26.440 --> 01:10:27.920
computational goal states.

01:10:27.920 --> 01:10:31.920
Now, let's go back to where we started with this,

01:10:31.920 --> 01:10:36.120
which is this notion of scaling up from components.

01:10:36.120 --> 01:10:38.960
So here's your single cell.

01:10:38.960 --> 01:10:41.160
What evolution has done is allowed these cells

01:10:41.160 --> 01:10:45.800
to merge into networks that are able to store

01:10:45.800 --> 01:10:47.280
much larger goal states.

01:10:47.280 --> 01:10:50.040
So this guy only cares about his own physiology

01:10:50.040 --> 01:10:51.880
and his own metabolic.

01:10:51.880 --> 01:10:56.560
This collection of cells is very competent

01:10:56.600 --> 01:10:58.240
in reaching a particular region

01:10:58.240 --> 01:11:00.400
of anatomical morpho space that looks like this.

01:11:00.400 --> 01:11:02.600
The goal is huge at centimeters in size.

01:11:02.600 --> 01:11:04.040
And if it's deviated from that,

01:11:04.040 --> 01:11:05.600
it will do its best to come back

01:11:05.600 --> 01:11:09.040
to even with the kind of drastic interventions.

01:11:09.040 --> 01:11:10.920
But that process has a failure mode.

01:11:10.920 --> 01:11:12.400
That failure mode is known as cancer.

01:11:12.400 --> 01:11:13.240
What happens?

01:11:13.240 --> 01:11:15.360
This is human glioblastoma cells.

01:11:15.360 --> 01:11:17.800
If individual cells get disconnected

01:11:17.800 --> 01:11:20.320
from this electrical and other signals as well,

01:11:20.320 --> 01:11:22.160
from this network that binds them

01:11:22.160 --> 01:11:26.120
towards a common journey in that space, that common goal,

01:11:26.160 --> 01:11:29.000
they revert back to their evolutionarily ancient self.

01:11:29.000 --> 01:11:30.360
What is the goal of a single cell?

01:11:30.360 --> 01:11:31.680
Well, it's to become two cells

01:11:31.680 --> 01:11:33.040
and to go wherever life is good.

01:11:33.040 --> 01:11:34.360
That's metastasis.

01:11:34.360 --> 01:11:37.960
And so you can see how what happens with these cancer cells

01:11:37.960 --> 01:11:41.480
is they're not any more selfish than any other cell.

01:11:41.480 --> 01:11:43.640
They're just their cells are smaller.

01:11:43.640 --> 01:11:44.760
And we've talked, you know,

01:11:44.760 --> 01:11:47.520
I talked to roboticists and folks like that

01:11:47.520 --> 01:11:49.800
with this idea that why don't robots get cancer?

01:11:49.800 --> 01:11:50.640
Right?

01:11:50.640 --> 01:11:52.640
The reason that our current technology isn't prone to this

01:11:52.640 --> 01:11:55.160
is because we do not have a multi-scale architecture

01:11:55.160 --> 01:11:57.200
where the components have their own goals.

01:11:57.200 --> 01:12:00.640
That we have some fairly dumb components typically.

01:12:00.640 --> 01:12:02.360
And then we hope that the collective

01:12:02.360 --> 01:12:03.640
that has some kind of, you know,

01:12:03.640 --> 01:12:05.040
is doing some kind of computation,

01:12:05.040 --> 01:12:07.080
but the parts are not trying to do anything.

01:12:07.080 --> 01:12:08.160
Biology isn't like that.

01:12:08.160 --> 01:12:11.040
Every component will do interesting things

01:12:11.040 --> 01:12:13.280
if freed from its neighbors.

01:12:13.280 --> 01:12:14.240
And I'll show you that.

01:12:14.240 --> 01:12:15.560
But of course, you know, biomedically,

01:12:15.560 --> 01:12:18.080
we can sort of take this kind of weird way

01:12:18.080 --> 01:12:19.240
of looking at things and ask,

01:12:19.240 --> 01:12:24.080
can we simply enlarge the boundary of the self?

01:12:24.080 --> 01:12:27.680
Enlarge the border between self and outside world.

01:12:27.680 --> 01:12:28.600
And so you can do that.

01:12:28.600 --> 01:12:30.240
We have techniques to do that

01:12:30.240 --> 01:12:35.240
where when we inject particular human oncogenes

01:12:35.440 --> 01:12:37.560
into these tadpoles to make tumors,

01:12:37.560 --> 01:12:39.800
and you can already see this is voltage imaging,

01:12:39.800 --> 01:12:43.360
you can see that these cells are already starting to defect.

01:12:43.360 --> 01:12:44.360
As far as they're concerned,

01:12:44.360 --> 01:12:47.480
the rest of the animal is just outside environment.

01:12:47.480 --> 01:12:49.080
So that's something else that Miocha mentioned

01:12:49.080 --> 01:12:51.280
is this idea of being in conflict or not

01:12:51.280 --> 01:12:52.200
with your environment.

01:12:52.200 --> 01:12:55.840
It's never obvious to a new agent what the environment is.

01:12:55.840 --> 01:12:59.240
Every cell is some other cells' external environment.

01:12:59.240 --> 01:13:02.040
And so normally all of these cells believe

01:13:02.040 --> 01:13:04.760
that the water out here is the external environment.

01:13:04.760 --> 01:13:07.920
But once you disconnect them using these oncogenes,

01:13:07.920 --> 01:13:09.800
then as far as the cells are concerned,

01:13:09.800 --> 01:13:11.600
all of this stuff is external environment.

01:13:11.600 --> 01:13:12.800
They don't care what happens to that.

01:13:12.800 --> 01:13:14.200
They're gonna do their best.

01:13:14.200 --> 01:13:15.560
They're gonna live their best life.

01:13:15.560 --> 01:13:17.880
They're gonna dump entropy into the environment.

01:13:17.880 --> 01:13:20.640
And of course, that's maladaptive for the organism.

01:13:20.640 --> 01:13:22.880
But one thing you can do is you can force

01:13:22.880 --> 01:13:24.160
using specific techniques,

01:13:24.160 --> 01:13:26.360
including optogenetics and some other things,

01:13:26.360 --> 01:13:29.600
you can force these cells to remain in electrical,

01:13:29.600 --> 01:13:31.600
in the correct electrical state with their neighbors.

01:13:31.600 --> 01:13:33.360
And if you do that, even though the oncogene,

01:13:33.360 --> 01:13:34.400
this is the same animal here,

01:13:34.400 --> 01:13:36.160
even though the oncogene is very strong,

01:13:36.160 --> 01:13:41.160
there's no tumor because the hardware problem

01:13:41.200 --> 01:13:42.680
isn't really fundamental.

01:13:42.680 --> 01:13:44.240
It's the software that's fundamental.

01:13:44.240 --> 01:13:47.240
It's are these cells working on a large goal

01:13:47.240 --> 01:13:50.360
like making a nice liver and muscle and skin and whatever?

01:13:50.360 --> 01:13:53.480
Or are they individual cells working on individual goals?

01:13:53.480 --> 01:13:55.160
So we spend a lot of time thinking

01:13:55.160 --> 01:13:56.000
about these kinds of things.

01:13:56.000 --> 01:14:00.040
How do we, what are the mechanisms, of course,

01:14:00.040 --> 01:14:03.800
but also algorithms, policies for connecting up

01:14:03.800 --> 01:14:06.400
little tiny homeostats,

01:14:06.400 --> 01:14:08.560
these cells that like to keep certain states

01:14:08.560 --> 01:14:10.400
into much larger networks

01:14:10.400 --> 01:14:11.960
that then have these interesting properties

01:14:11.960 --> 01:14:13.640
that of course people in the connectionless world

01:14:13.640 --> 01:14:15.000
have been studying for a really long time.

01:14:15.000 --> 01:14:17.520
So in painting, out painting,

01:14:17.520 --> 01:14:19.080
you know, all this kind of stuff.

01:14:19.080 --> 01:14:22.000
So we can talk about our efforts

01:14:22.000 --> 01:14:24.280
to sort of understand how the goals scale.

01:14:24.280 --> 01:14:26.280
They scale from these really humble,

01:14:26.280 --> 01:14:29.600
metabolic kinds of goals of individual cells, right?

01:14:29.600 --> 01:14:33.320
These homeostatic loops into anatomical homeostasis,

01:14:33.320 --> 01:14:35.280
eventually behavioral homeostasis

01:14:35.280 --> 01:14:39.440
and behavioral clever motion through three-dimensional space

01:14:39.440 --> 01:14:41.800
and eventually linguistic space and who knows what else.

01:14:41.800 --> 01:14:43.480
So just for the last couple of minutes,

01:14:43.480 --> 01:14:45.520
I just wanna show you one thing,

01:14:45.520 --> 01:14:47.800
which is simply this.

01:14:47.840 --> 01:14:52.840
In studying these kind of novel perturbations

01:14:53.040 --> 01:14:55.920
and asking what are cells actually capable of?

01:14:55.920 --> 01:14:59.120
What, you know, what other modes are there?

01:14:59.920 --> 01:15:01.520
We asked the following thing

01:15:01.520 --> 01:15:02.920
and I have to do a disclosure here

01:15:02.920 --> 01:15:05.840
because Josh Bongard and I are co-founders

01:15:05.840 --> 01:15:07.320
of this thing called Fauna Systems.

01:15:07.320 --> 01:15:11.600
It's a biorebotics kind of company.

01:15:11.600 --> 01:15:12.760
And so what we did in this,

01:15:12.760 --> 01:15:15.520
all the biology was done by Doug Blackiston in my lab

01:15:15.520 --> 01:15:17.800
and there was a lot of computer science here

01:15:17.800 --> 01:15:20.480
done by Sam Kriegman in Josh's lab.

01:15:20.480 --> 01:15:23.320
What we decided to do was to liberate cells

01:15:23.320 --> 01:15:24.320
from the normal environment

01:15:24.320 --> 01:15:27.200
and give them a chance to reboot their multicellularity.

01:15:27.200 --> 01:15:28.600
How much creativity is there?

01:15:28.600 --> 01:15:30.160
What else can they do?

01:15:30.160 --> 01:15:33.480
And specifically, and I think somebody on the chat

01:15:33.480 --> 01:15:35.760
asked this before, where do these goals come from?

01:15:35.760 --> 01:15:36.920
So that's what we wanted to understand,

01:15:36.920 --> 01:15:40.560
a completely novel creature that's never existed before.

01:15:40.560 --> 01:15:41.520
What goals do they have?

01:15:41.520 --> 01:15:42.720
Where do their goals come from?

01:15:42.720 --> 01:15:45.040
Okay, and so I'm gonna just show you a couple of examples.

01:15:45.040 --> 01:15:50.040
So what we did here is we took an early frog embryo

01:15:50.040 --> 01:15:53.760
and so what Doug does is he takes all of these cells up here

01:15:53.760 --> 01:15:57.000
which are skin, they're basically determined to be skin

01:15:57.000 --> 01:15:58.800
and he dissociates them

01:15:58.800 --> 01:16:01.280
and puts them into a little depression here.

01:16:01.280 --> 01:16:03.440
Now, there are many things that they could have done

01:16:03.440 --> 01:16:04.760
after that, they could die,

01:16:04.760 --> 01:16:07.040
they could spread out and sort of walk away from each other,

01:16:07.040 --> 01:16:09.120
they could form a flat two-dimensional monolayer

01:16:09.120 --> 01:16:10.880
the way that cell culture does.

01:16:10.880 --> 01:16:13.880
Instead, what happens is this, and this is time lapse,

01:16:13.880 --> 01:16:16.800
of course, so overnight, these guys will get together

01:16:16.800 --> 01:16:19.440
and they will coalesce into this interesting

01:16:19.440 --> 01:16:22.840
little thing here and what is it?

01:16:22.840 --> 01:16:24.280
Well, we call this a Xenobot,

01:16:24.280 --> 01:16:25.920
Xenopus laevis is the name of the frog

01:16:25.920 --> 01:16:27.680
and it's a biobot, so Xenobot.

01:16:28.560 --> 01:16:30.840
What it's doing is it's using the little hairs

01:16:30.840 --> 01:16:32.800
on its surface, these hairs are normally there

01:16:32.800 --> 01:16:35.320
to spread mucus down the body of the frog.

01:16:35.320 --> 01:16:38.400
What they've done is repurpose those hairs for swimming.

01:16:38.400 --> 01:16:40.440
So here it goes, it's chugging along,

01:16:40.440 --> 01:16:42.200
you can see that as they can go in circles,

01:16:42.200 --> 01:16:44.120
they can sort of patrol back and forth like this,

01:16:44.120 --> 01:16:45.800
they can have group behaviors,

01:16:45.800 --> 01:16:47.560
this one's going on kind of a long journey,

01:16:47.560 --> 01:16:48.760
these are interacting together,

01:16:48.760 --> 01:16:50.920
these are having an arrest.

01:16:50.920 --> 01:16:52.760
Here's what it does in a maze,

01:16:52.760 --> 01:16:55.320
so you can see it swims along,

01:16:55.320 --> 01:16:57.800
it's gonna take a turn here without having to bump

01:16:57.800 --> 01:16:59.880
into this outside wall, so it takes a turn.

01:16:59.880 --> 01:17:02.120
And then at this point, for some internal reason,

01:17:02.120 --> 01:17:03.240
we have no idea about it,

01:17:03.240 --> 01:17:05.360
it decides to turn around and go back where it came from.

01:17:05.360 --> 01:17:08.760
Okay, so there's all sorts of primitive kinds of dynamics.

01:17:08.760 --> 01:17:11.840
Just keep in mind, even though these things have,

01:17:11.840 --> 01:17:13.480
this is calcium signaling you see,

01:17:13.480 --> 01:17:16.480
it's the kind of thing you see when you do brain imaging,

01:17:16.480 --> 01:17:18.360
there are no neurons here, this is just skin.

01:17:18.360 --> 01:17:20.960
This whole thing is just skin cells,

01:17:20.960 --> 01:17:23.360
but they're doing a lot of, calcium readout

01:17:23.360 --> 01:17:26.120
is a great readout of computation.

01:17:26.120 --> 01:17:28.560
And could they be saying something to each other?

01:17:28.560 --> 01:17:29.400
Of course we don't know,

01:17:29.400 --> 01:17:32.960
this is still a very much ongoing subject of investigation,

01:17:32.960 --> 01:17:37.600
but one of the amazing things that Doug and Sam discovered

01:17:37.600 --> 01:17:40.520
is that their computational models of these guys

01:17:40.520 --> 01:17:43.320
make predictions that differently shaped bots

01:17:43.320 --> 01:17:45.120
are going to rearrange their environment

01:17:45.120 --> 01:17:47.720
in different ways, so they did a lot of simulations.

01:17:47.720 --> 01:17:50.880
And so then we tried it and we just did it in vivo.

01:17:50.880 --> 01:17:52.000
And here's what we found.

01:17:52.000 --> 01:17:55.560
So here are the bots, the white stuff here is their cells,

01:17:55.560 --> 01:17:58.760
they're loose skin cells that we sprinkled into the dish.

01:17:58.760 --> 01:18:00.240
And what they're basically doing,

01:18:00.240 --> 01:18:03.280
because we made it impossible for them to reproduce

01:18:03.280 --> 01:18:05.320
in the normal froggy fashion,

01:18:05.320 --> 01:18:07.680
they are basically implementing von Neumann's dream,

01:18:07.680 --> 01:18:11.400
they are constructing other,

01:18:11.400 --> 01:18:12.840
what they do is they run around

01:18:12.840 --> 01:18:15.160
and they sort of collect these skin cells

01:18:15.160 --> 01:18:19.040
into little piles, then they kind of polish the piles.

01:18:19.040 --> 01:18:20.720
And these piles, because they're working

01:18:20.720 --> 01:18:22.480
with an agential material,

01:18:22.480 --> 01:18:24.200
they're not working with passive particles,

01:18:24.200 --> 01:18:26.360
they're working with cells, what do these cells like to do?

01:18:26.360 --> 01:18:29.080
They like to become the Xenobot.

01:18:29.080 --> 01:18:31.480
And so of course they create the next generation of Xenobot,

01:18:31.480 --> 01:18:35.080
which then matures and guess what, it does the same thing

01:18:35.080 --> 01:18:36.800
and then you get the next generation and so on.

01:18:36.840 --> 01:18:39.160
So this is kinematic self-replication,

01:18:39.160 --> 01:18:42.000
and they'll make multiple generations of this.

01:18:42.000 --> 01:18:46.320
So here's here a couple of interesting corollaries

01:18:46.320 --> 01:18:48.800
to this and then I'm almost done.

01:18:48.800 --> 01:18:50.120
The exact same genome,

01:18:50.120 --> 01:18:52.800
so here's the specification of the micro level hardware,

01:18:52.800 --> 01:18:54.920
this is what every cell gets to have,

01:18:54.920 --> 01:18:57.720
can do one of two things under normal circumstances,

01:18:57.720 --> 01:19:00.200
it will do this, it has this developmental sequence,

01:19:00.200 --> 01:19:02.640
then it makes these tadpoles that do various things.

01:19:02.640 --> 01:19:05.360
But under other circumstances, it makes this,

01:19:05.400 --> 01:19:07.720
this is a Xenobot, this is a developmental sequence,

01:19:07.720 --> 01:19:10.600
this is I think a month old or something Xenobot,

01:19:10.600 --> 01:19:12.280
where did the shape come from, right?

01:19:12.280 --> 01:19:13.680
And they have a different behavior

01:19:13.680 --> 01:19:15.880
with this thing called kinematic self-replication.

01:19:15.880 --> 01:19:18.840
So here's a few interesting things, number one.

01:19:18.840 --> 01:19:21.440
Typically when you talk about why a certain creature

01:19:21.440 --> 01:19:24.240
has certain capacities, everybody leans on evolution.

01:19:24.240 --> 01:19:27.760
Well, for eons, it was selected to do this or that.

01:19:27.760 --> 01:19:29.320
Well, there's never been any Xenobots,

01:19:29.320 --> 01:19:30.800
there's never been any selective pressure

01:19:30.800 --> 01:19:33.160
to be a good Xenobot, this is completely emergent.

01:19:33.160 --> 01:19:37.800
They do this, they form this coherent kind of system

01:19:37.800 --> 01:19:42.320
with new behaviors, both anatomically and with mortality,

01:19:42.320 --> 01:19:44.880
basically overnight, this has never been selected

01:19:44.880 --> 01:19:48.160
for specifically, they're completely new in the biosphere.

01:19:48.160 --> 01:19:50.880
As far as we know, no other living creature

01:19:50.880 --> 01:19:53.520
does kinematic self-replication, that's the first thing.

01:19:53.520 --> 01:19:56.440
The second thing is that, how did we engineer these?

01:19:56.440 --> 01:19:59.440
I mean, there are no trans genes here.

01:19:59.440 --> 01:20:01.200
So the, if you sequence this, all you see

01:20:01.200 --> 01:20:03.160
is normal Xenopus latus, there's nothing wrong

01:20:03.160 --> 01:20:04.800
with the genome, it's wild type.

01:20:04.800 --> 01:20:09.240
There are no nanomaterials, some of them,

01:20:09.240 --> 01:20:12.520
some of them Doug can make some modifications

01:20:12.520 --> 01:20:15.040
to them surgically, according to the AI

01:20:15.040 --> 01:20:16.760
that Josh and Sam built.

01:20:16.760 --> 01:20:19.720
But basically, the way we engineered these

01:20:19.720 --> 01:20:22.520
is not by adding anything, it's by liberating them

01:20:22.520 --> 01:20:25.400
from the influence of the other cells.

01:20:25.400 --> 01:20:27.840
So normally, if you just look at the normal path

01:20:27.840 --> 01:20:30.800
of this biological system, you would say,

01:20:30.800 --> 01:20:32.520
what do the skin cells like to do?

01:20:32.520 --> 01:20:35.080
Well, they like to be, and in fact, all they can be,

01:20:35.080 --> 01:20:37.680
is to be the outside two-dimensional layer

01:20:37.680 --> 01:20:38.840
of keeping out the bacteria.

01:20:38.840 --> 01:20:40.120
It's very boring passive life,

01:20:40.120 --> 01:20:42.600
they just sort of sit there and keep out the bacteria.

01:20:42.600 --> 01:20:45.080
But that's only what happens when they're basically

01:20:45.080 --> 01:20:46.960
bullied into it by the other cells.

01:20:46.960 --> 01:20:49.720
It's behavior shaping, it's instructive interactions

01:20:49.720 --> 01:20:52.480
from the other cells that tell them to sit quietly

01:20:52.480 --> 01:20:53.560
and be the outer layer.

01:20:53.560 --> 01:20:56.000
In the absence of all that stuff, liberated from all that,

01:20:56.000 --> 01:20:58.400
they have a completely different default lifestyle.

01:20:58.400 --> 01:21:00.200
And this is it, which you would not see

01:21:00.800 --> 01:21:02.240
without this thing.

01:21:02.240 --> 01:21:04.520
So, and we don't know what else,

01:21:04.520 --> 01:21:05.840
certainly we're studying right now,

01:21:05.840 --> 01:21:07.840
all the kinds of behavioral capacities,

01:21:07.840 --> 01:21:09.480
do they learn, do they anticipate,

01:21:09.480 --> 01:21:12.960
all sorts of things, I'm not making any claims yet about that.

01:21:12.960 --> 01:21:17.760
So, but this idea of what evolution I think really does,

01:21:17.760 --> 01:21:19.200
and we can talk about why,

01:21:19.200 --> 01:21:21.840
I think we have now some ideas about why,

01:21:21.840 --> 01:21:24.440
it doesn't produce solutions to specific problems,

01:21:24.440 --> 01:21:27.400
it produces generic problem-solving machines.

01:21:27.400 --> 01:21:31.120
And so the big thing that every living system has to do,

01:21:31.120 --> 01:21:33.000
and I think these are, if I had to make a list,

01:21:33.000 --> 01:21:36.080
these are some things that I think are required

01:21:36.080 --> 01:21:37.440
for the kind of thing we want.

01:21:37.440 --> 01:21:39.560
First of all, I think it's really important

01:21:39.560 --> 01:21:41.360
that your parts have agendas.

01:21:41.360 --> 01:21:42.680
It's not enough to have dumb parts

01:21:42.680 --> 01:21:46.640
and try to engineer an agenda for the whole system.

01:21:46.640 --> 01:21:50.280
You have to have a marketplace where every layer

01:21:50.280 --> 01:21:52.720
is competing, cooperating,

01:21:52.720 --> 01:21:55.880
and attempting to do its own thing.

01:21:55.880 --> 01:21:57.400
You, of course, risk failure modes,

01:21:57.400 --> 01:22:00.000
you risk parts trying to go off on their own.

01:22:00.000 --> 01:22:02.040
That's one of the trade-offs,

01:22:02.040 --> 01:22:03.600
but overall it becomes,

01:22:03.600 --> 01:22:05.960
I think, an incredibly powerful architecture.

01:22:07.000 --> 01:22:09.120
They have to emerge spontaneously.

01:22:09.120 --> 01:22:13.120
That is real agents don't know where their boundaries are.

01:22:13.120 --> 01:22:15.720
If you are a new embryo coming into the world,

01:22:15.720 --> 01:22:18.400
you don't know how many cells you're going to have,

01:22:18.400 --> 01:22:19.760
because we might remove half of them,

01:22:19.760 --> 01:22:21.320
and you still have to make a good embryo.

01:22:21.320 --> 01:22:23.320
You don't know how big your cells are,

01:22:23.320 --> 01:22:26.080
because we might make gigantic cells or smaller cells.

01:22:26.080 --> 01:22:28.920
You don't know exactly how many chromosomes

01:22:28.920 --> 01:22:29.760
you're going to have,

01:22:29.760 --> 01:22:34.080
because we can make all kinds of weird chimeras and so on.

01:22:35.240 --> 01:22:37.480
You have to be able to, surviving life,

01:22:37.480 --> 01:22:40.720
has to be able to play the hand and stealth from scratch.

01:22:40.720 --> 01:22:44.360
You really can't take past experience too seriously.

01:22:44.360 --> 01:22:46.280
You have to improvise on the fly.

01:22:46.280 --> 01:22:48.520
This is what biology does.

01:22:48.520 --> 01:22:49.920
So it does not have, nobody says,

01:22:49.920 --> 01:22:51.600
this is the border, this is where you are,

01:22:51.600 --> 01:22:53.040
and then everything else is the outside world.

01:22:53.040 --> 01:22:55.280
It has to guess, and it has to make a self-model,

01:22:55.280 --> 01:22:57.040
and it has to make a world model.

01:22:57.040 --> 01:22:59.160
Then there are the energy constraints.

01:22:59.160 --> 01:23:01.360
Typical AIs, as far as I know,

01:23:01.360 --> 01:23:03.080
have all their energy needs met.

01:23:03.080 --> 01:23:05.560
They can do whatever they want.

01:23:05.560 --> 01:23:07.440
They don't have to worry about it.

01:23:07.440 --> 01:23:10.920
Organisms evolved under very stringent energy

01:23:10.920 --> 01:23:11.880
and time constraints,

01:23:11.880 --> 01:23:15.160
which means that they cannot afford to be

01:23:15.160 --> 01:23:17.840
some kind of a Laplacian demon

01:23:17.840 --> 01:23:20.440
paying attention to all the micro states of the world.

01:23:20.440 --> 01:23:23.160
They have to do a lot of core screening

01:23:23.160 --> 01:23:27.080
and they have to kind of bundle all sorts,

01:23:27.080 --> 01:23:29.840
they have to generalize all sorts of things that go on

01:23:29.840 --> 01:23:34.320
into models of agents doing things, of selves doing things.

01:23:34.320 --> 01:23:35.760
That's the only way you have the time

01:23:35.760 --> 01:23:37.760
to compute what you should do next.

01:23:37.760 --> 01:23:38.720
And if you get good at that,

01:23:38.720 --> 01:23:40.760
eventually you turn that on yourself

01:23:40.760 --> 01:23:42.600
and you start telling stories about,

01:23:42.600 --> 01:23:45.960
meaning making internals and models of yourself doing things.

01:23:45.960 --> 01:23:48.040
And this becomes this idea,

01:23:48.040 --> 01:23:50.600
why do we all innately believe in free will?

01:23:50.600 --> 01:23:53.440
Because from the time that we were single cells,

01:23:53.440 --> 01:23:56.680
we had to tell stories about agents doing things

01:23:56.680 --> 01:23:57.520
and making choices.

01:23:57.520 --> 01:24:00.920
Otherwise, we just wouldn't survive without that ability.

01:24:00.920 --> 01:24:03.680
And then there's some other things like the shared stress

01:24:03.680 --> 01:24:07.600
and the scaling of stress via sharing it among parts

01:24:07.600 --> 01:24:08.840
and so on, we can talk about that.

01:24:08.840 --> 01:24:12.040
And the idea that it's open-ended,

01:24:12.040 --> 01:24:14.360
living things select their own problem space

01:24:14.360 --> 01:24:16.520
and explore it and so on.

01:24:16.520 --> 01:24:19.840
So this is, I'm just gonna stop here,

01:24:19.840 --> 01:24:22.720
but this is what I tell people is that because of this,

01:24:22.720 --> 01:24:26.040
because biology is so incredibly interoperative

01:24:26.040 --> 01:24:27.920
because none of the parts make any assumptions

01:24:27.920 --> 01:24:29.200
about what's going to happen,

01:24:29.200 --> 01:24:31.480
they do their best in whatever environment

01:24:31.480 --> 01:24:33.200
that they happen to be in,

01:24:33.200 --> 01:24:36.320
every combination of evolved material,

01:24:36.320 --> 01:24:40.240
some sort of engineered material and software

01:24:40.240 --> 01:24:42.720
is potentially a viable agent.

01:24:42.720 --> 01:24:47.760
So hybrids, cyborgs, biorobots, all of this,

01:24:47.760 --> 01:24:50.960
there's this huge option space of new creatures,

01:24:50.960 --> 01:24:53.200
of new bodies and new minds.

01:24:53.200 --> 01:24:55.480
Everything, when Darwin said endless forms,

01:24:55.480 --> 01:24:57.160
most beautiful, sort of impressed

01:24:57.160 --> 01:24:59.320
with the variety of living beings,

01:24:59.320 --> 01:25:00.880
all of that stuff is a tiny dot.

01:25:00.880 --> 01:25:02.080
It's a tiny corner.

01:25:02.080 --> 01:25:04.000
Everything on earth is a tiny corner

01:25:04.000 --> 01:25:05.600
of the space of possible beings.

01:25:05.600 --> 01:25:06.880
It's truly immense.

01:25:06.880 --> 01:25:08.360
And all of these things,

01:25:08.360 --> 01:25:10.080
and we're gonna be surrounded by these things.

01:25:10.080 --> 01:25:12.000
Some of this already exists as some hybrids

01:25:12.000 --> 01:25:13.080
and cyborgs already exist,

01:25:13.080 --> 01:25:15.000
but there's gonna be an incredible variety of them

01:25:15.000 --> 01:25:16.920
that we are going to be living with.

01:25:16.920 --> 01:25:20.080
This has major implications for ethics, for example,

01:25:20.080 --> 01:25:21.840
because up until now,

01:25:21.840 --> 01:25:26.160
we were, all of our ethical frameworks

01:25:26.160 --> 01:25:28.560
about how to relate to other beings

01:25:28.560 --> 01:25:30.440
really boiled down to two things.

01:25:30.440 --> 01:25:32.040
Do they look like us?

01:25:32.040 --> 01:25:33.200
And did they come from,

01:25:33.200 --> 01:25:35.360
do they have the same origin story as us?

01:25:35.360 --> 01:25:40.360
And so this is, even today in bioethics sessions

01:25:40.640 --> 01:25:41.400
at conferences, people say,

01:25:41.400 --> 01:25:42.760
well, does it look like a human brain?

01:25:42.760 --> 01:25:44.280
Then we have to worry about.

01:25:44.280 --> 01:25:47.800
But the reality is that these categories are,

01:25:47.800 --> 01:25:49.920
they're not gonna survive the next couple of decades.

01:25:49.920 --> 01:25:54.920
We cannot gauge anything about the potential intelligence

01:25:55.640 --> 01:25:57.520
in terms of the type of cognition

01:25:57.520 --> 01:25:59.320
and what space they're working in,

01:25:59.320 --> 01:26:01.880
by looking at where they come from the family tree,

01:26:01.880 --> 01:26:03.960
because they're not going to be on our family tree.

01:26:03.960 --> 01:26:06.040
And we have to have completely different frameworks for this.

01:26:06.040 --> 01:26:08.600
And the kinds of AIs that we're talking about now

01:26:08.600 --> 01:26:10.080
are only one part of this.

01:26:10.720 --> 01:26:12.960
We're going to be facing the exact same problem

01:26:12.960 --> 01:26:17.000
of dealing with the software AIs in biology.

01:26:17.000 --> 01:26:20.000
So if anybody's interested in these things,

01:26:20.000 --> 01:26:22.960
there are lots of papers where we go into this.

01:26:22.960 --> 01:26:25.480
And I just wanna thank the students and postdocs

01:26:25.480 --> 01:26:27.520
that did all the work that I showed you.

01:26:27.520 --> 01:26:29.240
And of course, again, the disclosure.

01:26:29.240 --> 01:26:31.400
So I will end there.

01:26:35.160 --> 01:26:36.520
Thank you so much, Michael.

01:26:36.520 --> 01:26:37.720
That was wonderful.

01:26:38.720 --> 01:26:41.760
I don't think that I need to introduce Christoph.

01:26:41.760 --> 01:26:46.840
We already had the pleasure of having you on the previous panel.

01:26:46.840 --> 01:26:51.840
And Christoph is currently a professor at Etihad,

01:26:52.120 --> 01:26:54.960
the INI in Zurich.

01:26:54.960 --> 01:26:58.800
And he is a first-generation cyber nutrition

01:26:58.800 --> 01:27:03.800
in a way as a physicist who is using very broad perspective

01:27:04.080 --> 01:27:06.560
on understanding intelligent systems.

01:27:06.560 --> 01:27:11.080
And without further ado, Christoph, please.

01:27:11.080 --> 01:27:12.160
The stage is yours.

01:27:13.080 --> 01:27:15.040
Yeah, I haven't prepared anything.

01:27:15.040 --> 01:27:20.040
I didn't know I was expected to prepare anything.

01:27:20.520 --> 01:27:24.960
So I am at liberty to respond to some of the things

01:27:24.960 --> 01:27:27.560
you have said, the two of you have said.

01:27:27.560 --> 01:27:31.560
Let me start with a point,

01:27:31.560 --> 01:27:33.280
Yosha, you made about the brain,

01:27:33.280 --> 01:27:38.280
which was it is a noisy, it is a noisy entity.

01:27:39.440 --> 01:27:44.440
It is not a digital device, not on the basic level.

01:27:45.240 --> 01:27:50.240
So if you want to have billions of entities,

01:27:50.240 --> 01:27:55.240
synapses or neurons to interact in any useful sense,

01:27:57.960 --> 01:28:00.120
you need attractor dynamics.

01:28:00.120 --> 01:28:03.280
So there must be certain states of the thing

01:28:03.280 --> 01:28:07.760
that have the property of being stable under noise,

01:28:07.760 --> 01:28:12.760
of having attractor dynamics.

01:28:13.520 --> 01:28:18.520
And we know that the brain is, of course,

01:28:19.080 --> 01:28:20.800
essentially a network.

01:28:20.800 --> 01:28:25.800
So in each moment of time, a subset of the neurons fire,

01:28:26.640 --> 01:28:30.600
and this subset must be stable.

01:28:30.600 --> 01:28:33.640
And for a short moment, a metastable,

01:28:33.640 --> 01:28:35.480
if you want to call it that way,

01:28:35.480 --> 01:28:38.960
you want to go through a trajectory of stable states.

01:28:38.960 --> 01:28:43.600
And that means individual fibers,

01:28:43.600 --> 01:28:46.600
the individual interaction between neurons

01:28:46.600 --> 01:28:51.600
must be embedded in alternate alternative pathways,

01:28:51.800 --> 01:28:55.320
which run to the same effect.

01:28:55.400 --> 01:29:00.400
So a signal emanating from a single neuron,

01:29:02.720 --> 01:29:05.720
going off on different pathways,

01:29:05.720 --> 01:29:09.800
many of these signals must come together again

01:29:09.800 --> 01:29:13.840
and coincide in space, meaning on the same neuron,

01:29:13.840 --> 01:29:14.960
and in time.

01:29:16.000 --> 01:29:19.360
And this is a selection criterion

01:29:19.360 --> 01:29:23.320
for the kind of activity states

01:29:23.320 --> 01:29:26.960
and the underlying connectivity states

01:29:26.960 --> 01:29:30.360
that make those states stable.

01:29:30.360 --> 01:29:33.480
And I would like to submit the idea

01:29:33.480 --> 01:29:38.480
that the brain is totally dominated

01:29:38.680 --> 01:29:43.680
by those appropriately shaped connectivity patterns

01:29:46.440 --> 01:29:48.640
that have this property.

01:29:48.640 --> 01:29:52.080
These connectivity patterns emerge

01:29:52.080 --> 01:29:55.200
through a process of self-interaction.

01:29:55.200 --> 01:30:00.200
You have self-interaction also on the slow time scale

01:30:00.480 --> 01:30:03.440
of individual synapses adapting

01:30:03.440 --> 01:30:07.160
and individual synapses finding out

01:30:07.160 --> 01:30:11.360
where they can find coincidences of signals.

01:30:11.360 --> 01:30:15.760
Each axonal branch has a small choice,

01:30:15.760 --> 01:30:18.760
a small sphere of a search space

01:30:18.840 --> 01:30:22.680
where it can end up in a plasticity.

01:30:22.680 --> 01:30:27.680
And all the endpoints of axons are searching around

01:30:29.560 --> 01:30:31.960
in order to find meeting places

01:30:31.960 --> 01:30:34.360
where they have a high likelihood

01:30:34.360 --> 01:30:38.960
of coinciding with the signals of other branches.

01:30:38.960 --> 01:30:42.200
And this process of self-interaction,

01:30:42.200 --> 01:30:44.600
of network self-organization,

01:30:44.600 --> 01:30:48.080
singles out from the space

01:30:48.080 --> 01:30:52.720
of all combinatorially possible connectivity patterns

01:30:52.720 --> 01:30:55.880
a very, very small subset.

01:30:55.880 --> 01:30:58.080
Let me remind you of the fact

01:30:58.080 --> 01:31:00.840
that the brain, the whole organism,

01:31:00.840 --> 01:31:03.440
the brain is constructed on the basis

01:31:03.440 --> 01:31:08.440
of one gigabyte of genetic information.

01:31:08.480 --> 01:31:10.920
And in order to describe the connectivity pattern

01:31:10.920 --> 01:31:13.640
of the brain, it's an easy calculation,

01:31:13.640 --> 01:31:18.640
you need a petabyte, 10 to the 15 bytes of information

01:31:19.080 --> 01:31:22.320
which is a million gigabytes as you well know.

01:31:22.320 --> 01:31:27.320
So the genes can only select from the space

01:31:28.480 --> 01:31:31.400
of all connections, a very small,

01:31:31.400 --> 01:31:34.320
can only be able to select from that space

01:31:34.320 --> 01:31:36.720
a very small subset.

01:31:36.720 --> 01:31:39.880
And I think it is very important to know more

01:31:39.880 --> 01:31:44.880
about this subset of self-supporting activity states

01:31:47.840 --> 01:31:50.960
and connectivity states.

01:31:50.960 --> 01:31:53.880
So in order to put that in action,

01:31:53.880 --> 01:31:56.880
let me remind you that your brain

01:31:56.880 --> 01:31:59.820
is in every moment of waking time

01:31:59.820 --> 01:32:03.720
representing the situation in which you are immersed.

01:32:03.720 --> 01:32:07.680
You have a representation of your actual environment

01:32:07.680 --> 01:32:11.360
if you open up your eyes.

01:32:11.360 --> 01:32:15.040
And this representation is so good

01:32:15.040 --> 01:32:20.040
that you usually equated with the reality out there.

01:32:20.080 --> 01:32:22.720
You are not aware of any differences

01:32:22.720 --> 01:32:26.080
between this reconstructed,

01:32:26.080 --> 01:32:31.080
this model of the outside world and the outside world.

01:32:31.080 --> 01:32:35.960
And you are so confident that it is the reality

01:32:35.960 --> 01:32:38.800
and not just an imagination

01:32:38.800 --> 01:32:41.520
because you continuously do experiments.

01:32:41.520 --> 01:32:45.440
You move around so that the perspective

01:32:45.440 --> 01:32:48.200
of the world changers

01:32:49.280 --> 01:32:52.960
and you test whether your representation

01:32:52.960 --> 01:32:57.960
is stays in tune over time with the sensory information.

01:32:58.720 --> 01:33:01.560
You do experiments, you touch objects

01:33:01.560 --> 01:33:06.560
and you experiment continuously

01:33:08.000 --> 01:33:11.240
with the environment

01:33:11.240 --> 01:33:14.600
in order to make sure that your representation

01:33:14.600 --> 01:33:18.160
is in tune with it, is consistent with it,

01:33:18.160 --> 01:33:22.520
is rendering the reality.

01:33:22.520 --> 01:33:25.880
Of course, what you are representing

01:33:25.880 --> 01:33:28.680
is only a small sector of what is out there.

01:33:28.680 --> 01:33:33.680
Your attention is always picking out only part of it.

01:33:34.120 --> 01:33:38.880
But what you are picking out is for you,

01:33:38.880 --> 01:33:42.160
for all intents and purposes, reality.

01:33:42.160 --> 01:33:47.320
I find it amazing that our models,

01:33:47.320 --> 01:33:51.360
our theories of intelligence, of brain function

01:33:52.440 --> 01:33:55.480
make so little of this very fundamental fact

01:33:55.480 --> 01:33:57.760
of our individual life.

01:33:59.520 --> 01:34:03.520
Now, according to what I said,

01:34:03.520 --> 01:34:08.400
I have been explicit about the data structure

01:34:08.400 --> 01:34:12.360
which is used to create this reality,

01:34:12.360 --> 01:34:16.600
to represent a model of this reality.

01:34:16.600 --> 01:34:18.200
The data structure is, of course,

01:34:18.200 --> 01:34:21.760
everybody believes firing neurons,

01:34:21.760 --> 01:34:25.040
but I would like to change your perspective

01:34:25.040 --> 01:34:27.600
in saying, don't look at the neurons,

01:34:27.600 --> 01:34:32.600
each neuron by itself doesn't have any significant meaning.

01:34:35.600 --> 01:34:38.600
It is the environment, the neural environment,

01:34:38.600 --> 01:34:41.800
the firing environment in which the neuron fires,

01:34:41.800 --> 01:34:43.400
which is the important thing.

01:34:43.400 --> 01:34:48.280
You have to look at quite a number of co-firing neurons

01:34:48.280 --> 01:34:50.880
in order to be able to make sense of it.

01:34:50.880 --> 01:34:55.880
When you look at a TV screen and can see only one pixel,

01:34:56.200 --> 01:34:59.800
there is no way you can connect that with any meaning.

01:35:01.640 --> 01:35:04.920
The pixel is something real, so to speak,

01:35:04.920 --> 01:35:07.160
but it doesn't tell you anything.

01:35:07.160 --> 01:35:11.920
It has no significance.

01:35:11.920 --> 01:35:15.880
In order to understand anything on a TV screen,

01:35:15.880 --> 01:35:19.160
you need to see quite a patch of it.

01:35:19.160 --> 01:35:21.320
In order to understand anything

01:35:21.320 --> 01:35:24.520
of the data structure of a brain,

01:35:24.520 --> 01:35:28.840
you need to see hundreds, probably thousands of neurons,

01:35:28.840 --> 01:35:32.200
at a time, and a given neuron can take part

01:35:32.200 --> 01:35:35.960
in quite a number of such, not an infinite number,

01:35:35.960 --> 01:35:40.520
but quite a number of such activity patterns,

01:35:40.520 --> 01:35:44.680
which I would like to call fragments.

01:35:44.680 --> 01:35:49.200
I think the perspective on the nervous system

01:35:49.200 --> 01:35:52.200
has to be changed very fundamentally

01:35:52.200 --> 01:35:55.560
in order to see it as a data structure

01:35:55.560 --> 01:35:59.320
that is up to the job of representing reality.

01:36:01.200 --> 01:36:06.120
Now, I would like, one of the last statements

01:36:06.120 --> 01:36:12.120
you, Michael, made was the range of things,

01:36:12.120 --> 01:36:14.320
of intelligent things, of organized things

01:36:14.320 --> 01:36:18.920
that can be generated, you said, is infinite.

01:36:18.920 --> 01:36:24.120
I would rather like to emphasize the opposite.

01:36:24.120 --> 01:36:26.640
I've recently read an interesting book

01:36:26.640 --> 01:36:31.400
by an author named Morris, a book that

01:36:31.400 --> 01:36:34.280
was totally focused on the phenomenon,

01:36:34.280 --> 01:36:38.280
looking at evolution, the phenomenon of convergence.

01:36:38.280 --> 01:36:43.280
A lens eye has been invented 12 times or something like that.

01:36:43.280 --> 01:36:47.200
The facet eye has been invented again and again.

01:36:47.200 --> 01:36:51.760
The lifestyle of a wolf pack has been invented again and again.

01:36:51.760 --> 01:36:56.560
The lifestyle of social insects or social animals,

01:36:56.560 --> 01:37:00.960
you social animals, has been invented again and again.

01:37:00.960 --> 01:37:06.320
So the space of all possible organic patterns

01:37:06.320 --> 01:37:09.360
that make sense, that have inner coherence,

01:37:09.360 --> 01:37:12.800
where the parts support each other in order

01:37:12.800 --> 01:37:16.440
to create something that is stable and significant,

01:37:16.440 --> 01:37:20.680
that is stable in itself and is coherent with environment,

01:37:20.680 --> 01:37:25.040
the space of those shapes and structures is limited.

01:37:25.040 --> 01:37:30.800
And here is a great opportunity for theory

01:37:30.800 --> 01:37:35.640
to come forward to understand what to expect from biology.

01:37:35.640 --> 01:37:40.360
The last thing I want to say is that what is missing,

01:37:40.360 --> 01:37:43.680
completely missing so far, almost completely missing

01:37:43.680 --> 01:37:48.440
so far from our artificial intelligence,

01:37:48.440 --> 01:37:52.600
from our machine learning, is the equivalent

01:37:52.600 --> 01:37:56.960
of biological behavior, of behavioral goals.

01:37:56.960 --> 01:38:00.800
Of course, an animal has the fundamental goal

01:38:00.800 --> 01:38:05.720
of self-preservation of the own structure.

01:38:05.720 --> 01:38:12.040
But evolution has built into the individual species

01:38:12.080 --> 01:38:18.000
a number of sub-goals, like feed yourself and avoid danger and so on,

01:38:18.000 --> 01:38:23.680
and find social contexts, sub-goals,

01:38:23.680 --> 01:38:27.440
which make up your life.

01:38:27.440 --> 01:38:30.360
And the intelligence in the eyes of many people

01:38:30.360 --> 01:38:36.520
is just the ability to pursue those goals in a changing context.

01:38:36.520 --> 01:38:41.480
That is a slightly different definition from yours,

01:38:41.480 --> 01:38:47.160
because yours was computing functions.

01:38:47.160 --> 01:38:53.880
The biological goals is the raison d'etre

01:38:53.880 --> 01:39:00.000
of biological intelligence, of course, to pursue those goals.

01:39:00.000 --> 01:39:07.560
And I think in the present situation around things like chat, GTP and so on,

01:39:07.560 --> 01:39:15.960
it is becoming quickly clear that those beasts are not intelligent in our sense,

01:39:15.960 --> 01:39:21.160
because what they do doesn't make sense in the light of the goals

01:39:21.160 --> 01:39:23.840
that we all recognize as such.

01:39:23.840 --> 01:39:30.440
And it would be, I think there will soon be an important drive towards

01:39:30.440 --> 01:39:37.440
installing in such systems the equivalent of the sense of responsibility.

01:39:37.440 --> 01:39:43.720
The sense of the consequences and utterance that is made may have

01:39:43.720 --> 01:39:49.160
down the line for ethical, for legal reasons.

01:39:49.160 --> 01:39:54.960
But also I feel, although I don't have a very strong argument in that favor,

01:39:54.960 --> 01:40:00.440
I have the feeling that in order for an entity to be truly intelligent,

01:40:00.440 --> 01:40:11.600
it needs to have a set of set goals with which it can pursue in its environment.

01:40:11.600 --> 01:40:16.000
Last remark I want to make is about consciousness.

01:40:16.000 --> 01:40:23.520
I think in my view, the definition of the conscious state of your own mind,

01:40:23.520 --> 01:40:28.800
of your own brain is a state in which you concentrate on one topic.

01:40:28.800 --> 01:40:32.080
Your whole brain is concentrated on one topic.

01:40:32.080 --> 01:40:37.160
And all the different submodalities in your brain are in tune with each other,

01:40:37.160 --> 01:40:39.240
are in mutual understanding.

01:40:39.240 --> 01:40:43.080
So if any change happens in any part of the system,

01:40:43.080 --> 01:40:49.240
all the other agency, other modalities can immediately respond to that.

01:40:49.240 --> 01:40:55.640
So consciousness is not the icing on a cake.

01:40:55.640 --> 01:41:01.360
I don't think it makes sense to talk of zombies.

01:41:01.360 --> 01:41:06.520
Consciousness is a condition for a system to be functional.

01:41:06.520 --> 01:41:13.640
And if you go down in the letter of evolution to simple animals,

01:41:13.640 --> 01:41:19.640
I don't think you can find a point that is a point Michael also made.

01:41:19.640 --> 01:41:24.240
You can find a point where consciousness disappears.

01:41:24.240 --> 01:41:27.520
But consciousness just loses volume.

01:41:27.520 --> 01:41:31.520
You lose language when you go from humans to animals.

01:41:31.520 --> 01:41:37.120
And you use the imagination of distant future when you go to animals.

01:41:37.120 --> 01:41:44.800
And so going further and further down the evolutionary ladder,

01:41:44.800 --> 01:41:47.840
the volume of consciousness gets less.

01:41:47.840 --> 01:41:57.400
But I would have a hard time, I think when talking about a fly,

01:41:57.400 --> 01:42:01.680
it has its own level of consciousness.

01:42:01.680 --> 01:42:09.680
So when approaching a wall, it senses the impending approach and reacts accordingly.

01:42:09.680 --> 01:42:13.640
So the whole organism is able to react to signals.

01:42:13.640 --> 01:42:17.720
So that is my view on consciousness.

01:42:17.720 --> 01:42:20.240
Thank you.

01:42:20.240 --> 01:42:22.480
Thank you so much, Christoph.

01:42:22.480 --> 01:42:27.040
So we're on to the discussion and Q&A question.

01:42:27.040 --> 01:42:32.480
I would like to start by asking a question of Michael Levin.

01:42:32.480 --> 01:42:40.520
So Michael, you were talking about how a lot of the problem solving within the organism

01:42:40.520 --> 01:42:43.760
is actually done at the local scale.

01:42:43.800 --> 01:42:49.040
And there is also the interesting remark that was made by one of participants in the chat

01:42:49.040 --> 01:42:55.680
that you mentioned that the planaria tail would have become the head had not been bullied

01:42:55.680 --> 01:42:57.520
by the rest of the organism.

01:42:57.520 --> 01:43:02.600
So what do you think are the communication protocols that are necessary

01:43:02.600 --> 01:43:05.680
to enable different types of intelligence?

01:43:05.680 --> 01:43:09.000
And is it the case that humans, for example, have cancer

01:43:09.000 --> 01:43:14.720
because we don't have enough intelligence at the lower local organismic level?

01:43:14.720 --> 01:43:18.280
Or is it because we pursue the higher scale goal

01:43:18.280 --> 01:43:22.080
and some kind of trade-off has to be made?

01:43:22.080 --> 01:43:23.600
Yeah, great questions.

01:43:23.600 --> 01:43:24.360
A few things.

01:43:24.360 --> 01:43:27.280
First of all, it is definitely not local.

01:43:27.280 --> 01:43:33.200
So one of the key things about all of this stuff is that larger systems make decisions

01:43:33.200 --> 01:43:38.040
in spaces that are much larger than their parts.

01:43:38.080 --> 01:43:40.320
And so here's a very simple example.

01:43:40.320 --> 01:43:42.160
If you have a planarian, it's got a head and a tail.

01:43:42.160 --> 01:43:43.640
You cut it in half.

01:43:43.640 --> 01:43:47.240
These two cells on either side of the cut, these guys will have to make a new head.

01:43:47.240 --> 01:43:48.880
These guys will have to make a new tail.

01:43:48.880 --> 01:43:52.360
But they were sitting right next to each other before you separated them with a scalpel.

01:43:52.360 --> 01:43:55.200
You cannot locally decide whether you're a head or a tail.

01:43:55.200 --> 01:43:58.360
It's a decision that has to take into account, well, do we already have a head?

01:43:58.360 --> 01:43:59.080
Do we have a tail?

01:43:59.080 --> 01:44:00.560
Which way is the wound facing?

01:44:00.560 --> 01:44:01.760
This is a global decision.

01:44:01.760 --> 01:44:03.000
It cannot be made locally.

01:44:03.000 --> 01:44:05.920
All of this stuff is like that.

01:44:05.920 --> 01:44:14.920
And it uses the exact same scheme that bacterial biofilms use to decide when different parts

01:44:14.920 --> 01:44:18.360
of the thing should eat so that everybody has a turn and the exact same thing,

01:44:18.360 --> 01:44:23.320
the exact same set of mechanisms that brains use to try to synthesize the activity

01:44:23.320 --> 01:44:28.320
of individual neurons into some sort of global goal for the rat or human or whatever.

01:44:28.320 --> 01:44:30.600
It's an electrical network.

01:44:30.600 --> 01:44:33.520
It has certain properties, only a few of which we understand.

01:44:33.520 --> 01:44:35.120
But it is absolutely not local.

01:44:35.120 --> 01:44:44.520
What this bioelectricity is very good at is at implementing integrated information

01:44:44.520 --> 01:44:49.440
across space and time to make decisions in new spaces.

01:44:49.440 --> 01:44:53.960
And that's, I think I forgot, what was the second part of your, I lost track of it.

01:44:53.960 --> 01:45:00.200
The second part is so do humans, so you remind that humans have cancer,

01:45:00.200 --> 01:45:02.760
but some other animals don't.

01:45:02.760 --> 01:45:05.120
And some other animals are in fact immortal.

01:45:05.120 --> 01:45:10.080
So what is it that, what is it in your opinion that doesn't allow human organisms

01:45:10.080 --> 01:45:12.880
to solve the problem of immortality?

01:45:12.880 --> 01:45:18.200
Does it have something to do with higher level goals or is it just a lack of intelligence?

01:45:18.200 --> 01:45:24.680
Yeah, I think that, well, so there's two, there's kind of a simple answer

01:45:24.680 --> 01:45:25.960
and then there's a more interesting answer.

01:45:25.960 --> 01:45:32.640
The simple answer that people usually give is simply that all by, so evolution, of course,

01:45:32.640 --> 01:45:36.720
doesn't really optimize for long life, happiness, intelligence.

01:45:36.720 --> 01:45:37.840
It doesn't optimize for any of that.

01:45:37.840 --> 01:45:39.840
It optimizes for biomass, that's it.

01:45:39.840 --> 01:45:46.240
And so, right, and so the simple answer is we don't need to be immortal

01:45:46.240 --> 01:45:52.160
and cancer resistant because it's perfectly possible to be a human

01:45:52.160 --> 01:45:56.280
and have lots of offspring and still get cancer and die after your reproductive years.

01:45:56.280 --> 01:45:59.920
That's it, that's the standard answer that it's actually,

01:45:59.920 --> 01:46:04.200
there's just not a lot of pressure for humans to do anything different.

01:46:04.200 --> 01:46:08.200
Now, I think the more interesting answer is this.

01:46:08.200 --> 01:46:13.560
There are most organisms do get cancer and do age, there are a few that are resistant.

01:46:13.560 --> 01:46:14.720
Let's look at the planaria.

01:46:14.720 --> 01:46:19.320
One really interesting thing about planaria is that many of them reproduce

01:46:19.320 --> 01:46:22.640
by tearing themselves in half and regenerating.

01:46:22.640 --> 01:46:27.480
Now, one interesting thing that the implications of that are unlike for us.

01:46:27.840 --> 01:46:30.120
If you get a mutation in your body during your lifetime,

01:46:30.120 --> 01:46:31.960
it doesn't get passed on to your offspring, right?

01:46:31.960 --> 01:46:36.040
So because of the Weissmann's barrier in sexual reproduction.

01:46:36.040 --> 01:46:41.120
In planaria that do this, every cell that doesn't die from that mutation

01:46:41.120 --> 01:46:44.600
contributes copies of itself to the next body, right?

01:46:44.600 --> 01:46:48.200
Because they have to repopulate and have to regenerate the new one.

01:46:48.200 --> 01:46:50.600
So planaria accumulate mutations like crazy.

01:46:50.600 --> 01:46:53.320
So over 400 million years that they've been around,

01:46:53.320 --> 01:46:55.680
their genomes are a complete mess.

01:46:55.720 --> 01:46:57.760
They basically look like a tumor, they're mix-employed.

01:46:57.760 --> 01:46:59.960
Every cell might have a different number of chromosomes.

01:46:59.960 --> 01:47:00.960
It's a disaster.

01:47:00.960 --> 01:47:09.160
Now, this is really a scandal because nowhere in a typical biology curriculum

01:47:09.160 --> 01:47:12.320
will you hear that the animal with the worst genome is, by the way,

01:47:12.320 --> 01:47:14.960
immortal, cancer-resistant and highly regenerative, right?

01:47:14.960 --> 01:47:16.240
They have the best anatomy.

01:47:16.240 --> 01:47:16.880
What's going on?

01:47:16.880 --> 01:47:21.120
We're told that our genomes are, that's where your body information is, right?

01:47:21.120 --> 01:47:22.440
How can this be?

01:47:22.440 --> 01:47:25.560
So this has been bugging me for a really long time.

01:47:25.760 --> 01:47:26.640
This disconnect.

01:47:26.640 --> 01:47:30.000
And I think we finally have an idea of what's going on.

01:47:30.000 --> 01:47:34.400
We just, like two days ago, just published a paper on some simulations

01:47:34.400 --> 01:47:35.360
that talk about this.

01:47:35.360 --> 01:47:36.960
I'll just give you a very simple example.

01:47:40.160 --> 01:47:44.560
One thing you have to do is you have to model not just the genotype and the phenotype,

01:47:44.560 --> 01:47:48.400
meaning the genome and then the thing that gets evaluated in these evolutionary simulations,

01:47:48.400 --> 01:47:52.280
but you have to model the morphogenetic process in between those two.

01:47:52.560 --> 01:47:54.920
The morphogenetic process has certain competencies.

01:47:54.920 --> 01:47:57.560
For example, some of them I've showed you, there are many more.

01:47:57.560 --> 01:48:02.880
So for example, if there's some mutation that puts your mouth off to the side,

01:48:02.880 --> 01:48:05.600
the mouth is perfectly competent to come back where it needs to be.

01:48:05.600 --> 01:48:09.040
If it's a mutation that causes you to fall apart as an early embryo,

01:48:09.040 --> 01:48:12.040
you'll just be a bunch of twins, multiples.

01:48:12.040 --> 01:48:16.320
If we took some eyes, Doug Blackiston did this to us,

01:48:16.320 --> 01:48:19.720
he took eyes and put them instead on the animal's tail,

01:48:19.720 --> 01:48:21.480
they can see perfectly well out of those eyes.

01:48:21.480 --> 01:48:23.760
No problem, no period of adaptation needed.

01:48:23.760 --> 01:48:26.720
It's all good, the nerves come, find the spinal cord, it's all good.

01:48:26.720 --> 01:48:28.600
So all of those kinds of things,

01:48:28.600 --> 01:48:33.160
abilities to make up for these kinds of issues we call developmental competencies.

01:48:33.160 --> 01:48:38.200
Now, one thing that happens is that when you have an animal with a little bit of developmental competency,

01:48:38.200 --> 01:48:42.320
you come up for selection and it turns out you're very good, right?

01:48:42.320 --> 01:48:43.240
But why are you good?

01:48:43.240 --> 01:48:47.480
Selection cannot tell whether you have a great genome or you're good because you're highly competent

01:48:47.480 --> 01:48:50.800
and you fixed all the things your genome actually was pretty sloppy about.

01:48:50.800 --> 01:48:54.000
So that means it's harder for evolution to see the good genomes.

01:48:54.000 --> 01:48:56.640
You can't do all as much work in perfecting the genome,

01:48:56.640 --> 01:48:59.920
but what it can do is crank up the competencies, right?

01:48:59.920 --> 01:49:02.120
So when you do that, then of course that makes the problem worse

01:49:02.120 --> 01:49:06.920
because the more competent you are, the less it's possible to find the best genomes.

01:49:06.920 --> 01:49:09.360
And so there's this positive feedback loop that's ratchet

01:49:09.360 --> 01:49:11.880
and there are some other things that sort of work against it.

01:49:11.880 --> 01:49:16.720
But I think what happened is that, and this is very much a hypothesis still,

01:49:16.760 --> 01:49:20.600
I think what happened is that planaria went all the way,

01:49:20.600 --> 01:49:24.960
meaning that in that lineage, probably because they reproduce this way,

01:49:24.960 --> 01:49:28.160
it doesn't make any sense to assume that your genome is any good.

01:49:28.160 --> 01:49:32.440
And the only architecture that survives is where the algorithm is so good

01:49:32.440 --> 01:49:35.240
that we're going to make a perfect worm no matter what happens to the genome.

01:49:35.240 --> 01:49:40.360
This means aging, carcinogenic mutations, the algorithm,

01:49:40.400 --> 01:49:46.240
meaning the machinery that maintains that goal state

01:49:46.240 --> 01:49:49.360
and physiological and anatomical space is so good

01:49:49.360 --> 01:49:52.960
that it can pretty much ignore a lot of issues in the hardware.

01:49:52.960 --> 01:49:55.600
Most of us aren't like that. Salamanders are sort of in the middle.

01:49:55.600 --> 01:49:58.680
So salamanders are highly regenerative, but they age and die.

01:49:58.680 --> 01:50:02.320
And so I think what salamanders sort of went part of the way there

01:50:02.320 --> 01:50:06.360
and they can fix certain things, but not enough to really keep it going forever.

01:50:06.360 --> 01:50:09.160
Mammals probably stopped even earlier than that.

01:50:09.160 --> 01:50:13.680
But I actually don't think any of this is fundamental.

01:50:13.680 --> 01:50:16.560
I mean, we're working on regeneration in mammals now.

01:50:16.560 --> 01:50:19.760
I do think someday we will all sort of regenerate like planaria.

01:50:19.760 --> 01:50:21.840
I think it is going to be possible.

01:50:21.840 --> 01:50:26.560
But I do think that evolution makes these trade-offs that they're

01:50:26.560 --> 01:50:28.920
just easier ways to be a human, I think.

01:50:31.360 --> 01:50:34.280
So question for everyone.

01:50:34.280 --> 01:50:37.680
So in terms of communication protocols,

01:50:37.680 --> 01:50:43.440
to what extent is intelligence simply the ability to organize the cells

01:50:43.440 --> 01:50:46.760
and what are the conditions necessary for that to occur?

01:50:46.760 --> 01:50:50.800
And to what extent is intelligence is some internal competence,

01:50:50.800 --> 01:50:56.760
competence of the cell or neuron or whatever computational unit we're talking about?

01:51:00.920 --> 01:51:02.200
Yosha, would you like to start?

01:51:03.200 --> 01:51:11.200
I'm currently thinking about the question of whether it's possible to make something

01:51:11.200 --> 01:51:15.200
that is as long lived as the planaria that doesn't look like a blob.

01:51:16.200 --> 01:51:22.200
There seems to be some correlation between the structural coherence of the organism

01:51:22.200 --> 01:51:29.200
and the detail and solution that it has and the degree of fidelity.

01:51:29.200 --> 01:51:35.200
That is expected from interpreting the operators defined in its genome.

01:51:35.200 --> 01:51:43.200
And more specifically, I wonder what, how we can formalize the idea

01:51:43.200 --> 01:51:48.200
that Michael put up earlier of multi-scale organization and such a way

01:51:48.200 --> 01:51:50.200
that it leads to coherence.

01:51:50.200 --> 01:51:55.200
What is the criterion that makes a single agent coherent in itself

01:51:55.200 --> 01:51:58.200
and leads to this coherence on a particular level?

01:51:58.200 --> 01:52:02.200
Arguably, our own mind is some kind of society of agents

01:52:02.200 --> 01:52:04.200
and the organism has lots of local agents.

01:52:04.200 --> 01:52:06.200
Every organ is an agent in a way.

01:52:06.200 --> 01:52:08.200
Every cell is an agent.

01:52:08.200 --> 01:52:11.200
But there is also a globally coherent agent.

01:52:11.200 --> 01:52:17.200
And that is different from having multiple twins coexisting next to each other

01:52:17.200 --> 01:52:20.200
and forming some cooperative chimera.

01:52:20.200 --> 01:52:24.200
But that leads to some global element.

01:52:24.200 --> 01:52:29.200
On the other hand, Christoph has pointed this out.

01:52:29.200 --> 01:52:35.200
If you think about consciousness, it seems to relate to a unified experience.

01:52:35.200 --> 01:52:40.200
And this unified experience of all sensory data is what makes it specific.

01:52:40.200 --> 01:52:42.200
What is interesting about consciousness

01:52:42.200 --> 01:52:46.200
is that I normally don't have multiple conscious experiences

01:52:46.200 --> 01:52:49.200
unified in one perspective.

01:52:49.200 --> 01:52:52.200
How is this unity being realized?

01:52:52.200 --> 01:53:00.200
Or more generally speaking, can we come up with some kind of formal criterion

01:53:00.200 --> 01:53:05.200
that defines how everything has a place in the greater whole

01:53:05.200 --> 01:53:08.200
and the condition needs to be measurable

01:53:08.200 --> 01:53:13.200
and lead to globally coherent behavior on the next level of organization?

01:53:13.200 --> 01:53:18.200
If we take this to account and if you look at Michael's diagram

01:53:18.200 --> 01:53:21.200
that he brought up in the context of ethics,

01:53:21.200 --> 01:53:26.200
all the different agents that all seem to be centered about individual humans,

01:53:26.200 --> 01:53:32.200
it turns out that individual humans are not the main agents in the human sphere.

01:53:32.200 --> 01:53:37.200
The organizations of humans are much more powerful than individual human beings.

01:53:37.200 --> 01:53:42.200
And while individual human beings implement these organizations for the most part,

01:53:42.200 --> 01:53:47.200
we gradually transition more of that to machines that we are building.

01:53:47.200 --> 01:53:52.200
It seems to me that there is different levels of organization

01:53:52.200 --> 01:53:56.200
that transcend the individual organisms.

01:53:56.200 --> 01:53:59.200
This multi-scale organization doesn't stop with humans

01:53:59.200 --> 01:54:02.200
and the next scales are getting more and more agency.

01:54:02.200 --> 01:54:05.200
Also, I don't think that humans are all that important.

01:54:05.200 --> 01:54:10.200
It seems to me that humans are a very specific thing that has a very specific role.

01:54:10.200 --> 01:54:12.200
All our cousin species are dead.

01:54:12.200 --> 01:54:14.200
Women are not long-lived species.

01:54:14.200 --> 01:54:20.200
And it seems to be that the reason why Gaia brought us up is that we fulfill our job,

01:54:20.200 --> 01:54:23.200
which is to burn all the fossil fuels as quickly as possible.

01:54:23.200 --> 01:54:24.200
This is what we're here for.

01:54:24.200 --> 01:54:26.200
Then we burn ourselves out.

01:54:26.200 --> 01:54:29.200
If we manage to teach the rocks how to sink in the meantime,

01:54:29.200 --> 01:54:30.200
that's a stretch goal.

01:54:30.200 --> 01:54:33.200
But after we are gone, there will be more intelligent species.

01:54:33.200 --> 01:54:35.200
And we are a very specific one, right?

01:54:35.200 --> 01:54:40.200
We are this type of monkey that is not going to get his hand out of the cauldron trap

01:54:40.200 --> 01:54:42.200
if there's fossil fuel inside.

01:54:42.200 --> 01:54:46.200
And that's somewhat predictable if you look at the way in which we work

01:54:46.200 --> 01:54:50.200
because we are very smart and intelligent on very short timescales,

01:54:50.200 --> 01:54:52.200
but we are not globally coherent.

01:54:52.200 --> 01:54:56.200
We don't find ourselves in this global, coherent, godlike,

01:54:56.200 --> 01:54:57.200
organization.

01:54:57.200 --> 01:55:01.200
And if we succeed in building the next level of intelligence,

01:55:01.200 --> 01:55:05.200
maybe this next level organization, some kind of very fast,

01:55:05.200 --> 01:55:07.200
tightly integrated globally,

01:55:07.200 --> 01:55:09.200
coherent mind is going to be emerging.

01:55:09.200 --> 01:55:13.200
And maybe humans will play a very small part in whatever is going to come

01:55:13.200 --> 01:55:14.200
afterwards.

01:55:14.200 --> 01:55:16.200
But it's not about us, right?

01:55:16.200 --> 01:55:18.200
Life on Earth is not about us.

01:55:18.200 --> 01:55:20.200
Life on Earth is about the cell.

01:55:20.200 --> 01:55:23.200
And overall, it's about fighting back entropy.

01:55:23.200 --> 01:55:28.200
It's about sustaining yourself through maintaining complexity.

01:55:28.200 --> 01:55:34.200
So my question would be to Christoph and to Michael,

01:55:34.200 --> 01:55:41.200
can we come up with the criterion that determines coherence?

01:55:41.200 --> 01:55:46.200
Yeah, yeah, I think the important thing is, as I've said,

01:55:46.200 --> 01:55:50.200
that a coherent form has a kind of stability.

01:55:50.200 --> 01:55:54.200
It is made up out of continuous variables,

01:55:54.200 --> 01:55:58.200
which are prone to noise.

01:55:58.200 --> 01:56:06.200
And for the whole thing to have a lasting existence is a different

01:56:06.200 --> 01:56:08.200
signals that converge on one point.

01:56:08.200 --> 01:56:10.200
They have to agree with each other.

01:56:10.200 --> 01:56:12.200
They have to stabilize each other.

01:56:12.200 --> 01:56:16.200
The same way as a crystal is formed,

01:56:16.200 --> 01:56:21.200
a rigid body in that the individual forces between atoms,

01:56:21.200 --> 01:56:26.200
which are also acting, of course, in a liquid,

01:56:26.200 --> 01:56:32.200
but are not able to form something like a stable shape in a liquid.

01:56:32.200 --> 01:56:38.200
But in a crystal, they have fallen into a configuration in which

01:56:38.200 --> 01:56:40.200
in each individual interaction,

01:56:40.200 --> 01:56:48.200
each individual force gets support by other indirect pathways.

01:56:48.200 --> 01:56:53.200
And so I think just as in the space of all mathematics,

01:56:53.200 --> 01:57:01.200
those pieces of mathematics that have been found are singular points

01:57:01.200 --> 01:57:08.200
that admit no change if you have a direct interaction

01:57:08.200 --> 01:57:10.200
that admit no change.

01:57:10.200 --> 01:57:13.200
If you have come up with the idea of a group,

01:57:13.200 --> 01:57:16.200
then the rest of the whole story,

01:57:16.200 --> 01:57:21.200
thousands of pages in mathematical journals,

01:57:21.200 --> 01:57:27.200
follows by force from the definition of what a group is,

01:57:27.200 --> 01:57:31.200
a group of finite number of elements.

01:57:31.200 --> 01:57:37.200
And the same way the shapes that dominate life have this inherent

01:57:37.200 --> 01:57:42.200
self-consistency that is the different chains of forces

01:57:42.200 --> 01:57:45.200
that interact, support each other.

01:57:45.200 --> 01:57:49.200
Christoph, my apologies.

01:57:49.200 --> 01:57:51.200
Christoph, my apologies.

01:57:51.200 --> 01:57:55.200
I think we're almost on top of the hour and Michael has to go at 11.

01:57:55.200 --> 01:57:59.200
So, or, well, in one minute, Michael,

01:57:59.200 --> 01:58:03.200
any last comments from you before we let you run?

01:58:03.200 --> 01:58:05.200
I'm sorry for interrupting.

01:58:05.200 --> 01:58:07.200
This is extremely interesting.

01:58:07.200 --> 01:58:09.200
Thank you so much for having me here.

01:58:09.200 --> 01:58:11.200
This was amazing.

01:58:11.200 --> 01:58:13.200
Thanks for coming.

01:58:13.200 --> 01:58:16.200
I really wanted to introduce you to Christoph and have one more

01:58:16.200 --> 01:58:21.200
conversation with you after our lucky podcast.

01:58:21.200 --> 01:58:26.200
And so I'm very, very happy that you could come and hope to see you

01:58:26.200 --> 01:58:28.200
again soon and stay in touch.

01:58:28.200 --> 01:58:34.200
I really like many of your ideas in the space of self-organizing

01:58:34.200 --> 01:58:35.200
systems.

01:58:35.200 --> 01:58:40.200
And it's very lucky that we could have you here today.

01:58:40.200 --> 01:58:43.200
Christoph, do you have some more time to stay on?

01:58:43.200 --> 01:58:45.200
Yes, I do.

01:58:45.200 --> 01:58:46.200
Perfect.

01:58:46.200 --> 01:58:47.200
Thank you very much, everybody.

01:58:47.200 --> 01:58:48.200
I've got to run.

01:58:48.200 --> 01:58:49.200
I'm happy to do more.

01:58:49.200 --> 01:58:50.200
Great meeting you.

01:58:50.200 --> 01:58:53.200
And I'm fascinated by all the examples that you have shown.

01:58:53.200 --> 01:58:56.200
It's just great.

01:58:56.200 --> 01:58:57.200
Cool.

01:58:57.200 --> 01:58:58.200
Thank you so much.

01:58:58.200 --> 01:58:59.200
See you all later.

01:58:59.200 --> 01:59:00.200
Thank you, Michael.

01:59:00.200 --> 01:59:01.200
Thank you.

01:59:01.200 --> 01:59:03.200
Bye.

01:59:03.200 --> 01:59:07.200
So we're going to continue for a little bit more.

01:59:07.200 --> 01:59:09.200
If it's fine with you, Tanya, you have time, right?

01:59:09.200 --> 01:59:12.200
Yes, yes, of course.

01:59:12.200 --> 01:59:17.200
So Christoph, you mentioned that you think that the genome is that

01:59:17.200 --> 01:59:20.200
contributes to the brain is a gigabyte.

01:59:20.200 --> 01:59:22.200
But that's the whole of the genome, right?

01:59:22.200 --> 01:59:26.200
And so it's the part that quotes for the brain is going to be a

01:59:26.200 --> 01:59:28.200
small fraction of that.

01:59:28.200 --> 01:59:32.200
Yeah, I wonder how much information is remarkable that the

01:59:32.200 --> 01:59:35.200
genome has expanded from mouse to man.

01:59:35.200 --> 01:59:39.200
So making a larger brain doesn't need more genes.

01:59:39.200 --> 01:59:40.200
Yes.

01:59:40.200 --> 01:59:43.200
Of course, if you take the genome and you drop it into physics,

01:59:43.200 --> 01:59:46.200
it's not going to form a cell.

01:59:46.200 --> 01:59:51.200
So, and every cell that exists is the result, except for the

01:59:51.200 --> 01:59:57.200
first one of the server application of another cell, right?

01:59:57.200 --> 02:00:00.200
So all cells in some sense depend on the existence of a cell

02:00:00.200 --> 02:00:03.200
and the cell is not empty.

02:00:03.200 --> 02:00:09.200
And I wonder what the comorgo of complexity of the cell itself is

02:00:09.200 --> 02:00:10.200
right.

02:00:10.200 --> 02:00:13.200
How complex is this machinery that is being copied and copied all

02:00:13.200 --> 02:00:14.200
over?

02:00:14.200 --> 02:00:17.200
Maybe that's much larger than a gigabyte.

02:00:17.200 --> 02:00:21.200
And of course, the principles of self-organization embodied in the

02:00:21.200 --> 02:00:25.200
cell lead to the search for more coherent organization on the next

02:00:25.200 --> 02:00:26.200
level.

02:00:26.200 --> 02:00:29.200
And so the cell is already an agent, a self-replicator,

02:00:29.200 --> 02:00:33.200
a Turing machine, and an entropy extractor.

02:00:33.200 --> 02:00:36.200
And the self-replication is the main feat of the cell to make that

02:00:36.200 --> 02:00:38.200
happen robustly in physics.

02:00:38.200 --> 02:00:40.200
And this is what enables everything else.

02:00:40.200 --> 02:00:44.200
I wonder how much of that we need?

02:00:44.200 --> 02:00:51.200
Yeah, I could imagine that the brunt of the genetic information

02:00:51.200 --> 02:00:54.200
needed for a single cell already.

02:00:54.200 --> 02:01:00.200
And then you may just add, I don't know how much,

02:01:00.200 --> 02:01:06.200
on top of that in the form of the preexisting cell.

02:01:06.200 --> 02:01:09.200
Each cell is the daughter of another cell.

02:01:09.200 --> 02:01:14.200
And so there is some information in the arrangement,

02:01:14.200 --> 02:01:18.200
at least in the molecular arrangement of the cell,

02:01:18.200 --> 02:01:21.200
that needs to be counted as information.

02:01:21.200 --> 02:01:27.200
Although I have a hard time seeing that that will be in a

02:01:27.200 --> 02:01:31.200
significant way more than a gigabyte.

02:01:31.200 --> 02:01:35.200
You know, at the present time, there are groups that try to create

02:01:35.200 --> 02:01:37.200
artificial cells.

02:01:37.200 --> 02:01:43.200
And they learn the hard time that a lot needs to be in place to

02:01:43.200 --> 02:01:47.200
make a cell tick, you know, the different kinds of membranes.

02:01:47.200 --> 02:01:51.200
The membranes shape themselves, that's very important.

02:01:51.200 --> 02:01:56.200
They shape themselves into the Golgi apparatus,

02:01:56.200 --> 02:01:58.200
things like that.

02:01:58.200 --> 02:02:03.200
And so there is information in the preexisting cell.

02:02:03.200 --> 02:02:09.200
But quantitatively, I don't think it is going to be more than a

02:02:09.200 --> 02:02:12.200
gigabyte.

02:02:12.200 --> 02:02:15.200
So I guess if you were to build an intelligent system,

02:02:15.200 --> 02:02:20.200
say artificial visual cortex, how would you go about it?

02:02:20.200 --> 02:02:23.200
And what would be the main differences to the existing

02:02:23.200 --> 02:02:25.200
approaches?

02:02:25.200 --> 02:02:30.200
Well, I think, you know, when you look at, with your own brain,

02:02:30.200 --> 02:02:36.200
with your own eyes, at a moving body, you can predict the next

02:02:36.200 --> 02:02:42.200
split second and compare it to the signals that come in.

02:02:42.200 --> 02:02:47.200
So we have this machinery in our visual system that is able to do

02:02:47.200 --> 02:02:52.200
the differential geometry, taking into account the shape of the

02:02:52.200 --> 02:02:59.200
surface, the play of light on the surface, the movement.

02:02:59.200 --> 02:03:11.200
And so what I would create is a sequence of an array of ARIA,

02:03:11.200 --> 02:03:17.200
like V1, the primary visual cortex, and MT, which is concentrated

02:03:17.200 --> 02:03:23.200
on motion, maybe V3 concentrated on color and so on,

02:03:23.200 --> 02:03:29.200
and a number of submodalities, which each are two and a half D

02:03:29.200 --> 02:03:32.200
entities.

02:03:32.200 --> 02:03:40.200
They refer to the two-dimensional way we perceive the world and the

02:03:40.200 --> 02:03:47.200
with added internal spaces of quality spaces, like color,

02:03:47.200 --> 02:03:51.200
a three-dimensional space, and the texture, I don't know what,

02:03:51.200 --> 02:03:56.200
maybe a 40-dimensional space, depth, a one-dimensional space,

02:03:56.200 --> 02:03:59.200
motion, a two-dimensional space, and so on.

02:03:59.200 --> 02:04:08.200
And so they reflect the retinal image on the one hand in these

02:04:08.200 --> 02:04:15.200
different modalities, and they, another set of such ARIA,

02:04:15.200 --> 02:04:21.200
they build up the invariant static scene as such,

02:04:21.200 --> 02:04:28.200
couple them by projection patterns, which have to be dynamic

02:04:28.200 --> 02:04:32.200
because if you roll your eyes, the image moves.

02:04:32.200 --> 02:04:37.200
And in order to connect the moving images to the static

02:04:37.200 --> 02:04:43.200
representation, you need dynamic projection patterns.

02:04:43.200 --> 02:04:52.200
These are very important in themselves in the deforming

02:04:52.200 --> 02:04:58.200
projection of the moving retinal image of a rotating object.

02:04:58.200 --> 02:05:06.200
The deforming projection of that onto a static version of that

02:05:06.200 --> 02:05:10.200
thing tells you about the shape of the object.

02:05:10.200 --> 02:05:19.200
And so I think what you need is a huge array of local

02:05:19.200 --> 02:05:27.200
texture, local modality descriptions, all linked together

02:05:27.200 --> 02:05:33.200
with dynamic patterns, such that the whole thing is a self-supporting

02:05:33.200 --> 02:05:41.200
attractor space and describes the external world in detail as far

02:05:41.200 --> 02:05:44.200
as you concentrate on it, of course.

02:05:44.200 --> 02:05:52.200
It's a, I think it's quite an amount of work that is necessary.

02:05:52.200 --> 02:05:59.200
I once tried to put together a company where I believed I would

02:05:59.200 --> 02:06:06.200
need something like six or eight intelligent co-workers that

02:06:06.200 --> 02:06:09.200
together create this structure.

02:06:09.200 --> 02:06:15.200
The first feat to convince investors would be to let the system

02:06:15.200 --> 02:06:20.200
look at moving objects and build up an instant model of that

02:06:20.200 --> 02:06:27.200
moving object in its 3D form, an instant replica of that,

02:06:27.200 --> 02:06:35.200
with the ability to handle it, to connect to self-motion,

02:06:35.200 --> 02:06:38.200
to connect to manipulative motion.

02:06:38.200 --> 02:06:44.200
I think, you know, the complete abysmal failure of the car

02:06:44.200 --> 02:06:49.200
industry to come up with level five autonomy has very much to do

02:06:49.200 --> 02:06:55.200
with the inability to represent the traffic scene in this sense.

02:06:55.200 --> 02:07:01.200
And so my idea was I would get, investors would be ready to

02:07:01.200 --> 02:07:04.200
invest in that direction.

02:07:04.200 --> 02:07:09.200
However, I found out that this whole perspective of mine is so

02:07:09.200 --> 02:07:13.200
much sailing against the wind that I wouldn't even find the

02:07:13.200 --> 02:07:21.200
co-workers to help me create it, let alone the investors.

02:07:21.200 --> 02:07:26.200
I suspect that part of the difficulty to create self-driving cars

02:07:26.200 --> 02:07:29.200
has to do with the way in which the model is being generated,

02:07:29.200 --> 02:07:33.200
which means a deep learning currently relies on building

02:07:33.200 --> 02:07:36.200
classifiers for individual things.

02:07:36.200 --> 02:07:41.200
And there is no end-to-end train system for deep learning that is

02:07:41.200 --> 02:07:45.200
self-driving in a sense, and it is at the same time reliable.

02:07:45.200 --> 02:07:49.200
If you want to create reliable behavior that is rule-based,

02:07:49.200 --> 02:07:52.200
that where you basically have a set of traffic laws and safety

02:07:52.200 --> 02:07:55.200
measures and precautions that are built into the system that

02:07:55.200 --> 02:08:00.200
drive all the behavior, the object that this system is going

02:08:00.200 --> 02:08:04.200
to relate to are crafted by hand.

02:08:04.200 --> 02:08:08.200
So the self-driving car exists in a handcrafted software world

02:08:08.200 --> 02:08:12.200
where all the objects are being defined by a developer.

02:08:12.200 --> 02:08:15.200
Whereas the world that we are living in is an open world.

02:08:15.200 --> 02:08:18.200
And when we see new phenomena, we are able to integrate them

02:08:18.200 --> 02:08:20.200
into this model.

02:08:20.200 --> 02:08:23.200
And when the self-driving car sees something new that hasn't

02:08:23.200 --> 02:08:26.200
seen before that the developer didn't expect, like a bicycle

02:08:26.200 --> 02:08:31.200
painted on the outside of a truck, this might lead to confusions

02:08:31.200 --> 02:08:33.200
for the classifiers.

02:08:33.200 --> 02:08:38.200
Yeah, you made a very important observation that kids learn

02:08:38.200 --> 02:08:43.200
on the basis of very few examples compared to deep learning.

02:08:43.200 --> 02:08:50.200
They learn, moreover, in a very simple environment in their nursery

02:08:50.200 --> 02:08:57.200
with fairy tales and interacting with a few people and playing

02:08:57.200 --> 02:08:58.200
with objects.

02:08:58.200 --> 02:09:02.200
And then they walk out into the world and understand traffic

02:09:02.200 --> 02:09:03.200
situations.

02:09:03.200 --> 02:09:06.200
You don't hand down the key to the car yet because they don't

02:09:06.200 --> 02:09:08.200
have a sense of responsibility.

02:09:08.200 --> 02:09:13.200
They can't foresee the long-term effects of their actions.

02:09:13.200 --> 02:09:16.200
So you only let them drive when they're 18.

02:09:16.200 --> 02:09:21.200
But they understand traffic scenes very well when they are six

02:09:21.200 --> 02:09:23.200
or 10.

02:09:23.200 --> 02:09:30.200
So all of this is driven by learning by interaction with a

02:09:30.200 --> 02:09:34.200
simple environment and generalization from there.

02:09:35.200 --> 02:09:40.200
Yeah, well, of course, in 99.7% of all the cases, the self-driving

02:09:40.200 --> 02:09:41.200
car is good enough.

02:09:41.200 --> 02:09:46.200
It's mostly the long tail of cases that leads to situations where

02:09:46.200 --> 02:09:50.200
the system is producing undesirable behavior.

02:09:50.200 --> 02:09:55.200
I was joking a couple of years ago that whenever a journalist

02:09:55.200 --> 02:09:59.200
writes that there will never be self-driving cars, police is

02:09:59.200 --> 02:10:03.200
stopping Tesla with the sleeping driver safely on their way home.

02:10:04.200 --> 02:10:10.200
And so in many ways, self-driving cars exist.

02:10:10.200 --> 02:10:13.200
And they are almost as good enough in the sense that they are

02:10:13.200 --> 02:10:15.200
better than a really, really bad driver.

02:10:15.200 --> 02:10:20.200
But they're just not working to the degree of perfection of a

02:10:20.200 --> 02:10:21.200
very competent driver.

02:10:21.200 --> 02:10:26.200
Yeah, it's very mean to ask them to be so perfect, much more

02:10:26.200 --> 02:10:27.200
perfect than humans.

02:10:27.200 --> 02:10:31.200
They are, as you said, they are, if all cars were self-driving,

02:10:31.200 --> 02:10:34.200
traffic would be much more safe than now.

02:10:34.200 --> 02:10:41.200
But the public takes it very badly if an accident happens that

02:10:41.200 --> 02:10:43.200
could have been prevented.

02:10:43.200 --> 02:10:47.200
Yeah, we're also in an interesting situation where the public is

02:10:47.200 --> 02:10:48.200
mostly the media.

02:10:48.200 --> 02:10:52.200
And the media is at the moment in the US very much seeing itself

02:10:52.200 --> 02:10:56.200
in competition with the tech industry because they are competing

02:10:56.200 --> 02:10:59.200
for the same advertising revenue for the most part.

02:10:59.200 --> 02:11:04.200
And so it's at the moment very difficult to find articles that

02:11:04.200 --> 02:11:08.200
are optimistic and positive about technological developments

02:11:08.200 --> 02:11:10.200
in the media, I find.

02:11:10.200 --> 02:11:14.200
So this creates a very unique situation where even useful

02:11:14.200 --> 02:11:18.200
developments are delayed that could save lives.

02:11:18.200 --> 02:11:22.200
Because they're being seen in competition with existing

02:11:22.200 --> 02:11:26.200
economic and social structures, which also creates enormous

02:11:26.200 --> 02:11:30.200
pressure on AI models like JetGPT.

02:11:30.200 --> 02:11:33.200
I think that JetGPT is a tremendous achievement.

02:11:33.200 --> 02:11:35.200
My kids have been playing with it.

02:11:35.200 --> 02:11:40.200
My daughter has been creating a story of a horse that she got to

02:11:40.200 --> 02:11:43.200
know on the way home from school and then created several

02:11:43.200 --> 02:11:46.200
variants by modifying the prompt until she had the story that

02:11:46.200 --> 02:11:47.200
she liked.

02:11:47.200 --> 02:11:51.200
And then she turned it into a poem that's very catchy rhymes.

02:11:51.200 --> 02:11:56.200
My son used it to explain the system,

02:11:56.200 --> 02:11:59.200
to explain to him how to implement a platformer game.

02:11:59.200 --> 02:12:01.200
And it was explaining him how to structure the project.

02:12:01.200 --> 02:12:05.200
And then he was asking how to make an event loop in Python

02:12:05.200 --> 02:12:09.200
and it printed out source code and explained the source code.

02:12:09.200 --> 02:12:13.200
And he spent several hours copying the code and replete

02:12:13.200 --> 02:12:14.200
and getting it to work.

02:12:14.200 --> 02:12:18.200
So to me, these are systems where you have a little bit of human

02:12:18.200 --> 02:12:21.200
in the loop to make it coherent for a particular task.

02:12:21.200 --> 02:12:25.200
And it's amazing what the thing can already do.

02:12:25.200 --> 02:12:29.200
And it seems to me what's missing to get the system to work is to a

02:12:29.200 --> 02:12:31.200
system that makes it coherent.

02:12:31.200 --> 02:12:36.200
Basically, you can decompose the mind into perceptual systems that

02:12:36.200 --> 02:12:41.200
can in some sense to image guided and audio guided diffusion to

02:12:41.200 --> 02:12:44.200
coalesce to an internal state that is able to reproduce the

02:12:44.200 --> 02:12:45.200
sensory data.

02:12:45.200 --> 02:12:49.200
And then confabulation to build alternatives for solutions,

02:12:49.200 --> 02:12:52.200
alternatives for what could be alternatives for the future.

02:12:52.200 --> 02:12:56.200
And then the third component, which doesn't exist yet, which is

02:12:56.200 --> 02:13:00.200
proving from first principles what works, basically rejecting

02:13:00.200 --> 02:13:02.200
those generations that don't work.

02:13:02.200 --> 02:13:06.200
And then learning those that worked and building up the system

02:13:06.200 --> 02:13:09.200
in a way that is continuously learning.

02:13:09.200 --> 02:13:12.200
It also seems to me that many people cannot change their

02:13:13.200 --> 02:13:15.200
opinion in real time.

02:13:15.200 --> 02:13:18.200
And you have talked to a person that has a strong opinion about

02:13:18.200 --> 02:13:20.200
something that moves deeply into their mind.

02:13:20.200 --> 02:13:24.200
You can present them as arguments, but you have to talk to them the

02:13:24.200 --> 02:13:27.200
next day if you want to see any changes, which seems that seem to

02:13:27.200 --> 02:13:31.200
be parts of mental organization at least in some people require

02:13:31.200 --> 02:13:33.200
offline retraining.

02:13:33.200 --> 02:13:36.200
There's limits to what we can do in online learning.

02:13:36.200 --> 02:13:39.200
Some balancing needs to be done offline while we are decoupling the

02:13:39.200 --> 02:13:42.200
system from the environment and producing data augmentation and

02:13:42.200 --> 02:13:44.200
restructuring.

02:13:44.200 --> 02:13:47.200
I wonder how much of that retraining will also be built into the

02:13:47.200 --> 02:13:50.200
systems where the artificial systems will have to sleep and to

02:13:50.200 --> 02:13:52.200
dream.

02:13:52.200 --> 02:13:57.200
Yeah, before you take an important decision, you have to sleep

02:13:57.200 --> 02:14:02.200
over it and give your subconscious mind the opportunity to

02:14:02.200 --> 02:14:08.200
work on making the ideas more consistent than you are able to

02:14:08.200 --> 02:14:11.200
make them under conscious control.

02:14:15.200 --> 02:14:17.200
Yeah, very much so.

02:14:17.200 --> 02:14:28.200
So I think you rightly said these achievements of GPT-3,

02:14:28.200 --> 02:14:31.200
TETGTP and so on are extremely impressive.

02:14:31.200 --> 02:14:34.200
It's very difficult to see where the limit is.

02:14:34.200 --> 02:14:36.200
I agree with you.

02:14:37.200 --> 02:14:43.200
The transformers have a new, very new architectural feature,

02:14:43.200 --> 02:14:49.200
which is the online computation on the fly of connections and

02:14:49.200 --> 02:14:53.200
of these representation vectors.

02:14:53.200 --> 02:14:56.200
They are computed on the fly.

02:14:56.200 --> 02:14:58.200
That's all very promising.

02:14:58.200 --> 02:15:05.200
But these systems don't have any insight into real world

02:15:06.200 --> 02:15:11.200
geometric mechanics and so on representations of what they talk

02:15:11.200 --> 02:15:13.200
about.

02:15:13.200 --> 02:15:17.200
And they are lambasted mainly for that reason, that they don't

02:15:17.200 --> 02:15:21.200
know what it means, something is dead.

02:15:21.200 --> 02:15:28.200
They just know how people talk about it, but they don't know the

02:15:28.200 --> 02:15:33.200
significance of it or the geometrical arrangement of something.

02:15:33.200 --> 02:15:38.200
And so that is, of course, due to lack of insight, lack of

02:15:38.200 --> 02:15:44.200
interaction, you know, they cannot play with toy objects as kids

02:15:44.200 --> 02:15:50.200
do and cannot get the corresponding insights.

02:15:50.200 --> 02:15:58.200
But I still think that what is missing, what is sort of needs to

02:15:58.200 --> 02:16:03.200
be improved is the data structure of representation of

02:16:03.200 --> 02:16:07.200
themes and of realities.

02:16:07.200 --> 02:16:14.200
And I don't think these vectors that I use these days are up to

02:16:14.200 --> 02:16:17.200
the job.

02:16:17.200 --> 02:16:21.200
I think that the embedding spaces are not necessarily represented

02:16:21.200 --> 02:16:23.200
in full, right?

02:16:23.200 --> 02:16:27.200
If you think about the embedding space as a manifold with 30,000

02:16:27.200 --> 02:16:31.200
dimensions and a lot of resolution, trying to expand this space

02:16:31.200 --> 02:16:35.200
and store it in memory is not going to be feasible for the most

02:16:35.200 --> 02:16:37.200
part.

02:16:37.200 --> 02:16:40.200
So instead what is required is a language that allows to sparsely

02:16:40.200 --> 02:16:43.200
and efficiently construct representations in that embedding

02:16:43.200 --> 02:16:45.200
space.

02:16:45.200 --> 02:16:49.200
The embedding space is a mathematical construct that is basically

02:16:49.200 --> 02:16:53.200
every dimension is a function that describes a feature.

02:16:53.200 --> 02:16:56.200
And that feature has parameters.

02:16:56.200 --> 02:17:00.200
But I see every dimension is a parameter in that feature.

02:17:00.200 --> 02:17:02.200
Yeah, that's right, of course.

02:17:02.200 --> 02:17:08.200
So, you know, I once applied for funds to do face recognition and

02:17:08.200 --> 02:17:14.200
the idea was to collect data images which varied in all those

02:17:14.200 --> 02:17:18.200
dimensions, the identity of the person, the illumination, the

02:17:19.200 --> 02:17:22.200
expression, the texture and so on.

02:17:22.200 --> 02:17:26.200
And I didn't get the money and I'm very glad I didn't get the money

02:17:26.200 --> 02:17:31.200
because this 15-dimensional space or so cannot be filled with

02:17:31.200 --> 02:17:33.200
examples.

02:17:33.200 --> 02:17:35.200
That's totally impossible.

02:17:35.200 --> 02:17:38.200
There's too much space in high-dimensional spaces.

02:17:38.200 --> 02:17:41.200
That's the point you want to make, I suppose.

02:17:41.200 --> 02:17:46.200
So one has to find a way of representing only sub-dimensions,

02:17:47.200 --> 02:17:53.200
low-dimensional projections of that and a means of pasting them

02:17:53.200 --> 02:17:58.200
together as an equivalent of the high-dimensional thing.

02:17:58.200 --> 02:18:02.200
It turns out that when we conceptualize an object, it's chunked

02:18:02.200 --> 02:18:07.200
and a chunk is basically a node that is composed of features

02:18:07.200 --> 02:18:10.200
that define the nature of that bunk.

02:18:10.200 --> 02:18:13.200
And you have these seven plus minus two features.

02:18:13.200 --> 02:18:17.200
It's less, so it's more like five, which means that if you can

02:18:17.200 --> 02:18:21.200
define an object by five features, you have a local function,

02:18:21.200 --> 02:18:23.200
a locally five-dimensional space.

02:18:23.200 --> 02:18:25.200
Maybe sometimes it's nine-dimensional, but it's not much

02:18:25.200 --> 02:18:27.200
more.

02:18:27.200 --> 02:18:31.200
And these few dimensions allow us to construct a family of

02:18:31.200 --> 02:18:36.200
operators that would allow us up to these few dimensions construct

02:18:36.200 --> 02:18:42.200
all the 30,000 other dimensions or millions of other dimensions

02:18:42.200 --> 02:18:45.200
depending on how we look at this function space.

02:18:45.200 --> 02:18:46.200
Right.

02:18:46.200 --> 02:18:52.200
So the essential point here is that a high-dimensional thing gets

02:18:52.200 --> 02:18:57.200
projected down in our brain onto low-dimensional representations

02:18:57.200 --> 02:19:01.200
plus the ability to glue them together.

02:19:01.200 --> 02:19:05.200
And this gluing together, I don't see in the present

02:19:05.200 --> 02:19:09.200
neural technology.

02:19:09.200 --> 02:19:12.200
I think that it happens on a level that we normally don't look

02:19:12.200 --> 02:19:13.200
at.

02:19:13.200 --> 02:19:15.200
It happens in the activation traces in the network.

02:19:15.200 --> 02:19:18.200
So it's not in the weights between the links.

02:19:18.200 --> 02:19:22.200
And it's also not in the synaptic connections between neurons.

02:19:22.200 --> 02:19:26.200
It is in the content of the traces that are moving through this.

02:19:26.200 --> 02:19:32.200
So the neurons and the nodes in the neural network are providing

02:19:32.200 --> 02:19:36.200
the computational machinery to modulate these patterns according

02:19:36.200 --> 02:19:38.200
to the content of the patterns.

02:19:38.200 --> 02:19:42.200
And it's the content of the activation wave that is determining

02:19:42.200 --> 02:19:47.200
how the activation wave is being processed.

02:19:47.200 --> 02:19:51.200
This is something like a distributed computational pipeline.

02:19:51.200 --> 02:19:57.200
I'm involved in a multi-month or probably multi-year intensive

02:19:57.200 --> 02:20:02.200
discussion with a colleague at Amy in Zurich,

02:20:03.200 --> 02:20:06.200
Institute of Neural Informatics, Matthew Cook.

02:20:06.200 --> 02:20:11.200
And he doesn't want to hear of dynamic mappings.

02:20:11.200 --> 02:20:18.200
He says anything like schemata and roll fillers,

02:20:18.200 --> 02:20:21.200
that's all nonsense, he believes.

02:20:21.200 --> 02:20:28.200
And he claims all you need is components,

02:20:28.200 --> 02:20:30.200
which he calls them slips of paper,

02:20:30.200 --> 02:20:35.200
onto which various features are written into slips of paper

02:20:35.200 --> 02:20:38.200
can overlap in a subset of the features.

02:20:38.200 --> 02:20:45.200
And so that makes clear how they belong together.

02:20:45.200 --> 02:20:53.200
Components, each of which is a small set of entities.

02:20:53.200 --> 02:20:57.200
And they overlap in subsets of these entities.

02:20:57.200 --> 02:21:02.200
And that way they can cover a complex situation.

02:21:02.200 --> 02:21:11.200
I defy him again and again to create that way a system

02:21:11.200 --> 02:21:15.200
that can do something like invariant object recognition.

02:21:15.200 --> 02:21:20.200
Or the application of a syntactical rule,

02:21:20.200 --> 02:21:25.200
like subject, verb, object to an arbitrary set of components,

02:21:25.200 --> 02:21:30.200
of appropriate components, of course, to nouns and the verb.

02:21:30.200 --> 02:21:33.200
And which is, of course, very important,

02:21:33.200 --> 02:21:37.200
the cognitive scientists insist on that,

02:21:37.200 --> 02:21:42.200
on the ability to impose an abstract pattern

02:21:42.200 --> 02:21:44.200
onto concrete elements.

02:21:44.200 --> 02:21:48.200
And I think for that you have to have variables

02:21:48.200 --> 02:21:54.200
that make clear this abstract node belongs to this concrete node.

02:21:54.200 --> 02:21:59.200
If you want to represent the sentence John loves Mary,

02:21:59.200 --> 02:22:03.200
you have to make clear that the subject is linked to John

02:22:03.200 --> 02:22:05.200
and the object is linked to Mary,

02:22:05.200 --> 02:22:10.200
because you can also have the sentence Mary loves John

02:22:10.200 --> 02:22:14.200
and then they are called in a different way.

02:22:14.200 --> 02:22:17.200
You need variables to make that distinction.

02:22:17.200 --> 02:22:19.200
What are those variables?

02:22:19.200 --> 02:22:22.200
That's what I call the glue,

02:22:22.200 --> 02:22:26.200
that glues together the abstract form and the concrete elements

02:22:26.200 --> 02:22:28.200
that make it up, for instance.

02:22:28.200 --> 02:22:33.200
Or the texture, you know, the computer graphics people

02:22:33.200 --> 02:22:38.200
have a very good ontological theory of visual scenes.

02:22:38.200 --> 02:22:41.200
They can create them in a very convincing way

02:22:41.200 --> 02:22:44.200
and they create them out of part descriptions

02:22:44.200 --> 02:22:49.200
like shape only or texture only or illumination only

02:22:49.200 --> 02:22:52.200
or motion applied to a shape

02:22:52.200 --> 02:22:56.200
and have a way of putting them together.

02:22:56.200 --> 02:23:00.200
And for that you need to have variables

02:23:00.200 --> 02:23:04.200
that make clear this textural element belongs

02:23:04.200 --> 02:23:10.200
onto this form, on this point on that form.

02:23:10.200 --> 02:23:12.200
And what are those variables?

02:23:12.200 --> 02:23:15.200
We need to find the minimal set of link types

02:23:15.200 --> 02:23:17.200
that we need to...

02:23:17.200 --> 02:23:19.200
One minimal set.

02:23:19.200 --> 02:23:21.200
We didn't optimize for the smallest one

02:23:21.200 --> 02:23:24.200
to perform all the semantic representations

02:23:24.200 --> 02:23:26.200
that we wanted to have.

02:23:26.200 --> 02:23:31.200
And the model that we used was inspired by Aristotle

02:23:31.200 --> 02:23:34.200
and his Four Causa,

02:23:34.200 --> 02:23:38.200
which basically means you have Causa formalis

02:23:39.200 --> 02:23:44.200
and these four cases describe

02:23:44.200 --> 02:23:47.200
partonomic links part of and being composed of

02:23:47.200 --> 02:23:50.200
and being caused by and leading to.

02:23:50.200 --> 02:23:52.200
So basically you have lateral links

02:23:52.200 --> 02:23:54.200
that give you a causal ordering

02:23:54.200 --> 02:23:57.200
and you have compositional links

02:23:57.200 --> 02:24:00.200
that allow you to compose a script

02:24:00.200 --> 02:24:03.200
or a task of subtasks.

02:24:03.200 --> 02:24:07.200
And in this way you can describe arbitrary scripts

02:24:07.200 --> 02:24:11.200
and these arbitrary scripts can express arbitrary functions

02:24:11.200 --> 02:24:14.200
when you combine them with low level operators

02:24:14.200 --> 02:24:17.200
that can, for instance, perform some basic operations

02:24:17.200 --> 02:24:20.200
on the network, sense data in the environment

02:24:20.200 --> 02:24:23.200
in the network itself and so on.

02:24:23.200 --> 02:24:26.200
I mentioned earlier on that I think of intelligence

02:24:26.200 --> 02:24:29.200
as the ability to construct a path

02:24:29.200 --> 02:24:31.200
to a space of computable functions.

02:24:31.200 --> 02:24:33.200
So intelligence is not the ability to compute the function,

02:24:33.200 --> 02:24:35.200
every computer can compute a function

02:24:35.200 --> 02:24:37.200
without being intelligent.

02:24:37.200 --> 02:24:40.200
The trick is to discover that function in the first place

02:24:40.200 --> 02:24:42.200
and to discover this function

02:24:42.200 --> 02:24:45.200
we basically have three perspectives on how to do this.

02:24:45.200 --> 02:24:47.200
The first one is to converge to it.

02:24:47.200 --> 02:24:49.200
That is what deep learning does.

02:24:49.200 --> 02:24:52.200
You have a space of possible functions

02:24:52.200 --> 02:24:55.200
and in that space of possible functions you make large enough

02:24:55.200 --> 02:24:57.200
you start out with some random function

02:24:57.200 --> 02:25:00.200
and then you modify that function along many dimensions

02:25:00.200 --> 02:25:04.200
nudge it many billions of times or trillions of times

02:25:04.200 --> 02:25:07.200
until it gets close to what you wanted to do.

02:25:07.200 --> 02:25:10.200
And you follow a gradient.

02:25:10.200 --> 02:25:12.200
For this it needs to be differentiable

02:25:12.200 --> 02:25:15.200
and this algorithm of stochastic gradient descent

02:25:15.200 --> 02:25:18.200
using back propagation is still the workhorse

02:25:18.200 --> 02:25:20.200
of all machine learning at this point.

02:25:20.200 --> 02:25:24.200
And a second approach is to do hierarchical pattern matching.

02:25:24.200 --> 02:25:27.200
So you look for operators that you've already learned,

02:25:27.200 --> 02:25:30.200
a small library of efficient operators that you have evolved

02:25:30.200 --> 02:25:34.200
and evolution is all you need from this perspective

02:25:34.200 --> 02:25:37.200
that lets you get to the situation that you want

02:25:37.200 --> 02:25:40.200
based on the configuration that you have.

02:25:40.200 --> 02:25:43.200
So these operators are basically looking for activation patterns

02:25:43.200 --> 02:25:45.200
that they match on

02:25:45.200 --> 02:25:48.200
and they change the activation pattern into the next one.

02:25:48.200 --> 02:25:52.200
And in this way you can perform arbitrary functions in the way

02:25:52.200 --> 02:25:57.200
this is also the way in our computers are computing functions.

02:25:58.200 --> 02:26:01.200
And the third one is construction

02:26:01.200 --> 02:26:05.200
and construction requires some degree of memory

02:26:05.200 --> 02:26:08.200
and because you need to be able to retrace your steps

02:26:08.200 --> 02:26:11.200
and you need a way to justify the steps that you're making

02:26:11.200 --> 02:26:14.200
and when to retract them.

02:26:14.200 --> 02:26:17.200
Our consciousness seems to be strongly involved

02:26:17.200 --> 02:26:19.200
in such a construction process

02:26:19.200 --> 02:26:22.200
where we have a stream of consciousness that allows me

02:26:22.200 --> 02:26:25.200
oh I tried this thought before and this didn't work

02:26:25.200 --> 02:26:29.200
so I now retrace it, retract it, modify it

02:26:29.200 --> 02:26:32.200
and I think this one should work for the following reasons

02:26:32.200 --> 02:26:34.200
and then I see the outcome and say no it didn't work

02:26:34.200 --> 02:26:37.200
so this reason was not wrong so I try the next thing

02:26:37.200 --> 02:26:39.200
and this is something that is difficult to achieve

02:26:39.200 --> 02:26:42.200
just this pattern matching or this gradient descent.

02:26:42.200 --> 02:26:46.200
So this constructive discovery of solutions

02:26:46.200 --> 02:26:48.200
seems to be crucial

02:26:48.200 --> 02:26:52.200
and while it seems to me that our deep learning models

02:26:52.200 --> 02:26:54.200
are not very good at constructing

02:26:54.200 --> 02:26:56.200
they are very much able to emulate

02:26:56.200 --> 02:26:58.200
what it would be like to be constructing, right?

02:26:58.200 --> 02:27:02.200
So while GPT-3 or chat GPT are not conscious

02:27:02.200 --> 02:27:05.200
they are able to create a story about something

02:27:05.200 --> 02:27:07.200
that is conscious while they are unable to reason

02:27:07.200 --> 02:27:09.200
they create a story about the reasoner

02:27:09.200 --> 02:27:12.200
and draw on the inferences from that reasoning

02:27:12.200 --> 02:27:15.200
and the more closely you describe the reasoning

02:27:15.200 --> 02:27:18.200
that's reason step by step and so on

02:27:18.200 --> 02:27:20.200
the better the results can become

02:27:20.200 --> 02:27:23.200
and so it's very difficult to determine the difference

02:27:23.200 --> 02:27:27.200
between a system that pretends to perform a certain thing

02:27:27.200 --> 02:27:29.200
and it actually does it, right?

02:27:29.200 --> 02:27:33.200
If you can pretend it well enough you're actually doing it.

02:27:33.200 --> 02:27:37.200
Yeah, yeah, so the same way as you pretend to be conscious

02:27:37.200 --> 02:27:42.200
and I know the only conscious being in the world is myself.

02:27:43.200 --> 02:27:46.200
When did you figure that out?

02:27:46.200 --> 02:27:50.200
We know, we know, we're NPCs.

02:27:50.200 --> 02:27:53.200
How did I figure that out?

02:27:53.200 --> 02:27:56.200
Okay, yeah.

02:27:56.200 --> 02:27:58.200
Christoph, I enjoyed this very much.

02:27:58.200 --> 02:28:01.200
We are at the end of my time limit for now

02:28:01.200 --> 02:28:04.200
and I hope that we get to continue the conversation

02:28:04.200 --> 02:28:07.200
and I hope that we get to continue the conversation

02:28:07.200 --> 02:28:10.200
and I hope that we get to continue the conversation

02:28:10.200 --> 02:28:16.200
soon in Zurich and looking forward to talking more to you

02:28:16.200 --> 02:28:20.200
and I'm very glad that you could make it today.

02:28:20.200 --> 02:28:22.200
Yeah, it was a great pleasure.

02:28:22.200 --> 02:28:27.200
I just had to take an earlier train from Frankfurt to Berlin

02:28:27.200 --> 02:28:31.200
because I had to be there only tomorrow morning.

02:28:31.200 --> 02:28:35.200
I'm sitting here in the room in the so-called Harnak House

02:28:35.200 --> 02:28:37.200
of the guest house.

02:28:37.200 --> 02:28:40.200
Okay, have a nice day.

02:28:40.200 --> 02:28:43.200
Thank you so much for organizing this

02:28:43.200 --> 02:28:46.200
and setting everything up and supporting our discussion

02:28:46.200 --> 02:28:49.200
and I'd also like to thank our audience

02:28:49.200 --> 02:28:51.200
for paying attention, asking questions.

02:28:51.200 --> 02:28:56.200
We didn't get to discuss all of them

02:28:56.200 --> 02:29:00.200
but I'm very glad that you could make this event happen.

02:29:00.200 --> 02:29:02.200
Thank you, Christoph.

02:29:02.200 --> 02:29:04.200
See you soon.

02:29:04.200 --> 02:29:06.200
Thank you, everyone.

02:29:06.200 --> 02:29:07.200
Bye-bye now.

02:29:07.200 --> 02:29:08.200
Bye-bye.

