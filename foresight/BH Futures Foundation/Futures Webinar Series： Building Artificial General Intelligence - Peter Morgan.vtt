WEBVTT

00:00.000 --> 00:16.000
Okay, good evening everyone and welcome to Future's webinar series by BH Future's Foundation.

00:16.000 --> 00:18.800
I am Ivana Yevtovich, webinar coordinator.

00:18.800 --> 00:25.200
Tonight we will hear about artificial intelligence, the development of artificial general intelligence

00:25.200 --> 00:31.280
is going to require algorithms that can do things like inductive reasoning, planning and optimal

00:31.280 --> 00:37.760
learning from limited amounts of data. Several efforts are underway to develop these technologies

00:38.560 --> 00:44.640
for deployment in future systems. We are currently witnessing these new approaches being given more

00:44.640 --> 00:50.400
and more attention as we enter the new and perhaps golden age of artificial general intelligence.

00:51.040 --> 00:57.440
In this talk we will present some of the newer more general approaches to solving intelligence

00:57.440 --> 01:04.320
that are really currently under development with the view to deployment in the intelligence system

01:04.320 --> 01:11.360
of the future. Tonight's speaker is our guest Peter Morgan. Peter is author of the popular report

01:11.360 --> 01:18.320
Machine Learning is changing the rules, ways businesses can utilize artificial intelligence

01:18.400 --> 01:24.400
to innovate. Published by O'Reilly, he's passionate about artificial intelligence and the positive

01:24.400 --> 01:32.480
changes in this technology can and is bringing to society. Peter founded the artificial intelligence

01:32.480 --> 01:38.480
consult company, Deep Learning Partnership to carry out his mission of helping to bring artificial

01:38.480 --> 01:45.200
intelligence to the world. He advises and mentors technology startups and is a speaker at artificial

01:45.200 --> 01:50.960
intelligence conferences and meetups. Peter founded the popular London Deep Learning Lab

01:50.960 --> 01:55.920
Meetup. Links for the mentioned companies are listed down below so you can check it out.

01:56.720 --> 02:00.800
Peter, it's an honor to have you here tonight and thank you so much for being with us.

02:01.840 --> 02:10.800
Thanks Havana. Okay, so let's get started. So Havana just introduced me and there is a data

02:10.800 --> 02:19.600
science conference on the 2nd of July at Croatia that I'll just put a little shout out now for.

02:19.600 --> 02:26.080
So if you want to look out for that, tune in for that as well. Okay, so there's a little

02:26.080 --> 02:35.680
book I wrote that Havana mentioned on a practical book on implementing machine learning in real

02:36.240 --> 02:43.680
deployment for companies. That's what I do for living. I'm a consultant, machine learning consultant.

02:43.680 --> 02:49.040
Today's talk is going to be a little more futuristic in that there are no actual deployments yet

02:50.480 --> 02:58.960
but it's a look towards the future where we are now, where we want to get to and how we will get there.

02:59.200 --> 03:06.720
So here's an outline of my talk. What is intelligence? Let's start off with the basic

03:06.720 --> 03:15.120
question we're trying to solve and then look at how the world produces intelligent systems,

03:15.120 --> 03:19.520
either biological or non-biological. We're trying to non-biological route here.

03:20.880 --> 03:25.280
We are the biological, humans are the most intelligent thing that we know of in the

03:25.280 --> 03:32.560
universe so far, although at times it doesn't seem like it, right? And I'll do a very quick recap

03:32.560 --> 03:42.080
of deep learning and just to see if that's part of the journey or not, if it's a dead end or if

03:42.080 --> 03:47.680
that's a step along the way to artificial general intelligence. And hopefully everybody's at that

03:47.680 --> 03:52.000
kind of level, you've heard of deep learning, machine learning, that will probably help understand

03:52.000 --> 03:56.400
the talk but even if you don't it, it doesn't really matter. This is a fairly general talk high

03:56.400 --> 04:04.800
level. And then we'll look at AGI or artificial general intelligence and we'll see maybe how we

04:04.800 --> 04:09.920
might build such a system or if it's possible and then if it is how we might build it and we'll

04:09.920 --> 04:19.600
wrap up. So my talk will be about 40 minutes long and so let's get into it. So I mean why do we even

04:19.600 --> 04:25.840
want to build general intelligence? Well the idea is, and you may have heard DeepMind say that,

04:25.840 --> 04:31.760
it's a company based here in London under the Google umbrella these days, because then we'll

04:31.760 --> 04:36.960
build intelligence and use it to solve everything else, right? That's the big goal, that's the vision.

04:38.400 --> 04:43.520
So at the moment, like I say, we're the most intelligent things around so we're solving

04:43.520 --> 04:52.720
all scientific problems, medicine, climate change, just every problem right now, humans are tackling it

04:53.280 --> 04:59.280
as we have been doing from the beginning of time. So it's a big vision, right, is to build systems

04:59.280 --> 05:05.360
that can basically supersede us and accelerate science. Isn't that an amazing goal, right?

05:05.360 --> 05:10.400
Isn't that an amazing achievement if we could do that? Clearly, it's worth a lot of money so

05:10.400 --> 05:16.480
everyone's working on it, Microsoft, Google, a lot of bunch of startups. So let's see where we are on

05:16.480 --> 05:22.480
that journey. Okay, now first thing I'll mention is that people do get a little bit worried, oh my

05:22.480 --> 05:26.880
god, what happens when, you know, we're no longer the most intelligent things. That's a rather

05:26.880 --> 05:31.040
philosophical question and there's a lot of debate and books been written about that. It's a very hot

05:31.040 --> 05:36.000
topic at the moment. It comes under the umbrella of AI safety and so if you want to look into that,

05:36.800 --> 05:40.640
you know, I encourage you to do that, but we'll focus on the engineering and this talk and the

05:40.640 --> 05:46.720
science. Okay, we'll leave the philosophical problems to the philosophers, I'm afraid,

05:46.720 --> 05:52.480
or that that's a topic for another talk, which I could give too, but I won't on this one. So

05:53.440 --> 05:58.960
again, you know, what is intelligence? Is it, you know, beating go? Is it, you know, winning

05:58.960 --> 06:04.640
at Starcraft, you know, beating the best, go play at the best players in all of these games,

06:04.640 --> 06:11.360
basically, go Starcraft, chess, Jeopardy. Well, as amazing achievements as all of those were,

06:11.360 --> 06:16.640
like breathtaking, you know, when it happened at the moment, I remember, you know, watching

06:16.640 --> 06:21.280
all of them actually and just, you know, put my breath away. But no, it's not actually general

06:21.280 --> 06:25.680
intelligence. Those are examples of very narrow intelligence and pressure as they may be,

06:26.640 --> 06:35.600
because the system, the clever system that beat Gary Gatt, Casper Robert, chess, it can't,

06:36.720 --> 06:41.360
you know, beat Lisa Doll that go, it's essentially dumb outside of its very narrow domain,

06:42.080 --> 06:46.560
in which it's superhuman intelligence. Okay, so they're not generally intelligent machine systems.

06:48.000 --> 06:52.880
Okay, so they're all examples of narrow AI. So what is intelligence? Well,

06:53.840 --> 06:59.680
Howard Gardner at Harvard, a couple of decades ago, you know, it seems so obvious now,

06:59.680 --> 07:03.520
but at the time, you know, no one had kind of thought about this or written it down,

07:03.520 --> 07:10.800
which I, you know, it's one of those things. But he did, and I like it because it sort of says

07:10.800 --> 07:15.120
there's about nine different components to intelligence, right? So, you know, the go,

07:15.120 --> 07:20.560
the chess, that's a logical mathematical component that's superhuman at the moment.

07:20.560 --> 07:27.200
I mean, a calculator that we can buy for five bucks or whatever, you know, can do long multiplication,

07:27.760 --> 07:33.360
you know, far faster than we can. Okay, so there's clearly superhuman in that domain.

07:33.360 --> 07:39.360
But what about the others? Musical, nature, spatial, you know, robots maneuvering, navigating

07:39.360 --> 07:44.480
through space, time, self-driving cars, that kind of thing. Intrapersonal, now these are the harder

07:44.480 --> 07:50.480
ones, aren't they? The personal intelligence, social intelligence, and intro knowing ourselves,

07:50.480 --> 07:55.760
and then enter knowing about others. Okay, so that those are probably, you know, the most

07:55.760 --> 08:02.080
challenging thing. And the was that, you know, people say, you know, a lot of skeptics know,

08:02.080 --> 08:05.760
will never have, you know, be able to build systems like that. Well, I'm going to argue,

08:05.760 --> 08:11.280
there's nothing in physics, laws of physics that says we can't. And clearly, biology has done it,

08:11.280 --> 08:16.720
right? And so that's, we're a physical system. There are people, and, you know, it's fine,

08:16.720 --> 08:22.160
80% of the world believes in some religion, that's fine too. But I would argue that,

08:22.880 --> 08:27.360
you know, once you get the right physical systems that, you know, they will be capable,

08:27.360 --> 08:32.560
have the capacity of, you know, believing in higher powers and stuff like that too. So, you know,

08:32.560 --> 08:37.120
there's all these philosophical questions come up as well. But they can be, you know, I will try to

08:37.120 --> 08:41.520
reason to scientific questions, engineering questions. There's linguistic language,

08:42.240 --> 08:49.120
natural language processing, translation, you know, Siri, Google translate, you know, these are

08:49.120 --> 08:54.960
amazing systems, but they're not quite there yet. They're not quite superhuman. Go is, you know,

08:54.960 --> 09:01.200
calculators are, but, you know, all of these are the types of intelligence we are working on. Okay,

09:01.200 --> 09:05.680
we're not there yet. We're not human level intelligence, but we're certainly working on them.

09:05.680 --> 09:14.400
You know, the bodily kinesthetic, you know, you watch, is it Atlas, the Boston Robotics,

09:14.400 --> 09:18.960
Boston Dynamics, whatever they're called company, you know, doing flips now, you know,

09:18.960 --> 09:23.920
like a gymnast. So we're making a lot of progress on many of the, on most, on all of them, actually.

09:24.560 --> 09:29.040
And it's essentially, why are we here? You can imagine a robot sitting there going, hmm,

09:29.040 --> 09:33.520
why am I here? You know, that's how we'll know we've got to learn intelligence, probably.

09:34.480 --> 09:39.760
And there's a nice beautiful picture of one right there. And so how far have we come?

09:40.480 --> 09:45.520
I've covered all these, you know, we've got the calculation, but they're not creative. We have

09:45.520 --> 09:51.360
to tell them what to do. These systems, they're not self aware. They don't have subjective

09:51.360 --> 09:58.560
experience. So the existential down the bottom is zero. They don't ponder, you know, as far as I

09:58.560 --> 10:04.240
know, my laptop's not wondering, you know, why it's here. Maybe, maybe it is. And just not telling

10:04.240 --> 10:12.320
me, but I don't think so. Okay, so, um, so how will we get to AGI then? Well, it will take a village

10:12.320 --> 10:19.120
to create, you know, it takes a village to create a child. The well known saying in psychology will

10:19.120 --> 10:24.080
the same thing. It's going to take a village to create artificial general intelligence. And in that

10:24.080 --> 10:29.680
village will basically comprise of computer scientists, physicists, neuroscientists, of course,

10:29.680 --> 10:35.680
because we are the example and colleges, sociologists. So general intelligence is general,

10:35.680 --> 10:40.960
right? We're trying to, you know, basically build human liberal intelligence and then suppress it.

10:40.960 --> 10:45.680
So it's going to take, not just computer scientists set in sight of Google anymore, it's

10:45.680 --> 10:52.480
going to take physicists and neuroscientists and psychologists. No doubt, you know, Microsoft,

10:52.480 --> 11:00.400
Google, they have teams like this in place. Okay, so it's a little early, they don't talk about it,

11:00.400 --> 11:06.560
but yeah, but people are working on this stuff right now. So we'll have a look at some of the

11:06.560 --> 11:13.040
efforts and some of the history and some of the future of that, which is what this talks about.

11:13.040 --> 11:20.560
Okay, so physical systems, what have we got? So we've got biological, you know, I can argue that

11:20.640 --> 11:27.600
plants, bacteria, insects, mammals, us, we're all natures full of, you know, intelligent systems.

11:27.600 --> 11:33.680
How many species are there? You know, absolutely millions. And, you know, locus is intelligent,

11:33.680 --> 11:41.600
it's adapted to its environment. You know, it can overcome and reproduce. It doesn't really

11:41.600 --> 11:47.360
make goals and accomplish them, though. It doesn't consciously do that. You know, mammals are the

11:47.360 --> 11:53.440
only things that do that. And so there are various levels of intelligence, too. So we're trying to

11:53.440 --> 12:00.640
build the human level intelligence, the mammalian intelligence. And, you know, perhaps we'll start

12:00.640 --> 12:06.960
off with an ape intelligence and then, you know, progress to human level and then superhuman. So

12:06.960 --> 12:13.120
that's where we are. And so what are we, what, physically, what do we have? We have CPUs,

12:13.120 --> 12:20.320
processors, we have GPUs, FPGA, ASICs. And we also, those are all digital, examples of digital

12:20.960 --> 12:27.760
processing systems, okay. And those of you who have done a computer science degree or

12:27.760 --> 12:31.680
electrical engineering degree, you'll be familiar with all of those. And then we have

12:31.680 --> 12:38.640
something called neuromorphic, which is, which processes based on biology, okay. And those are

12:38.640 --> 12:44.880
very interesting. Those exist today. And they're quite large as well. We're up to about a billion

12:45.680 --> 12:52.560
neurons, okay, artificial neurons. And that's like a mouse. So we could argue we have mouse level

12:52.560 --> 12:59.840
intelligence today. And we'll see a little bit of that later on. And so that's exciting. Digital,

12:59.840 --> 13:04.960
it's always going to be an emulation neuromorphic, it's going to be more of a direct simulation,

13:04.960 --> 13:08.560
okay. So there's the difference between an emulation, which is something that you run on

13:08.560 --> 13:15.360
a digital processor and an emulation, which is an actual physical or simulation rather,

13:15.360 --> 13:21.120
which is an actual simulation of the actual physical system, which is the brain. So neuromorphic,

13:21.120 --> 13:25.600
we can do simulations, we can run it on real time, just like the brain or artificial neurons,

13:26.160 --> 13:33.120
all the rest, emulations on digital processors, which actually takes a lot more energy and time.

13:33.120 --> 13:40.240
It's, it's not, you know, we can get there, but it's, it's just, it's not a direct way of getting

13:40.240 --> 13:44.720
there, whereas neuromorphic is. And then we have quantum. Is there any, is quantum

13:45.680 --> 13:49.680
processors, you've all, we've all heard of quantum computing nowadays is catching the news.

13:49.680 --> 13:56.640
Is that a part of the journey or not? We don't know, maybe the brain uses quantum physics

13:56.640 --> 14:01.200
at the level of microtubules. Roger Penrose would argue it does,

14:02.320 --> 14:08.240
who's to argue with him, he's very clever physicist there in Oxford. But, you know,

14:09.920 --> 14:15.520
the fact is, you know, we don't know. Okay, so we'll look at it, we'll look a little bit

14:15.520 --> 14:19.520
deeper into all of those and then we'll wrap up at the end and I'll tell you where we are today.

14:20.160 --> 14:27.520
So biology is what I mentioned from the lowest little, you know, single celled amoeba to the,

14:28.320 --> 14:35.120
you know, C. elegans, that little worm there with 137 neurons. We can count them. We built

14:35.120 --> 14:40.640
that system actually. So we can actually, we built that little guy there. B, we haven't built.

14:40.640 --> 14:45.840
That's got about a million neurons and that tiny little brain there. It's a little nervous

14:45.840 --> 14:50.080
system, central nervous system. And then the human brain, that's a hundred billion neurons that,

14:50.080 --> 14:54.240
you know, that's clearly a tough one to crack. That's what we're trying to go.

14:55.520 --> 15:02.640
So a little bit of the big picture, you know, the brain is a system at what level do we attack this

15:02.640 --> 15:08.000
on? Do we attack it at the level of atoms and molecules like, you know, Roger Penrose is doing,

15:08.000 --> 15:14.240
that's where the quantum physics is after all. Or do we start at synapses? Or do we start,

15:14.240 --> 15:21.120
you know, at a collection of neurons and synapses like the Kinectome? Is that a better level to

15:21.120 --> 15:25.600
start? Or do we have to start right down at the molecular level? So these are very fundamental

15:25.600 --> 15:30.560
questions that need to be sort of asked and experimented with some pro. This is a science. So,

15:30.560 --> 15:34.480
you know, there's theory, but there's also experiment and test ideas out.

15:37.120 --> 15:42.160
If we just start at the neuron, this is what we have to build. Now that's a very complex system,

15:42.160 --> 15:46.800
right? But this is quite wonderful because this is how nature does intelligence. Like who were

15:46.800 --> 15:53.440
the guests, right? We needed such a complex thing to come up with conscious creatures,

15:53.440 --> 15:57.920
you know, like ourselves. I mean, this is very fundamental, you know, it's a neuron

15:57.920 --> 16:05.440
of intelligence, one could argue. Now, do we have to actually recreate that? Or can we do it some

16:05.440 --> 16:12.320
other way? Okay, that's, that's a very good question. Clearly, if we could just reproduce,

16:12.320 --> 16:17.920
we would just be recreating nature, right? So a lot of this is these artificial neurons are not

16:17.920 --> 16:24.960
this complex, the simplified versions. Can they, you know, can that give rise to consciousness?

16:24.960 --> 16:31.520
Yeah, we don't know yet. Okay, so clearly, it's a tough problem. So it'd be a tough engineering

16:31.520 --> 16:38.400
problem to build something like that. Okay, so the next level up really is, you know, we have

16:38.400 --> 16:43.200
about 2 million cortical columns made up of these neurons in the brain, there's 100 billion neurons,

16:43.200 --> 16:50.240
so you do the math, and you see, you know, there's 10,000 neurons per cortical column or something

16:50.240 --> 16:56.080
like that, maybe 50,000. So, but there's basically 2 million, maybe these are the fundamental units,

16:56.080 --> 17:02.640
maybe this is, if we build 2 million of these, this, this is all we'd need to do to build consciousness

17:02.640 --> 17:08.000
and human level intelligence. So, I mean, neuroscience, as a science has been around

17:08.000 --> 17:12.880
150 years, I mean, a lot of progress has been made in understanding the brain, right, with the

17:12.880 --> 17:18.960
hippocampus, all those different parts. And so, you know, we know a heck of a lot actually about

17:18.960 --> 17:23.840
intelligence, biological intelligence. So, you know, the question we've got to ask ourselves is,

17:23.840 --> 17:27.280
you know, what level do we start at, what level is needed, and then how do we build,

17:27.280 --> 17:32.800
is a lot of this comes down to an engineering problem. So, there's a connectome, there's a

17:32.800 --> 17:39.520
next level up. So, we started with neuron, next level up, these cortical columns in the slightly

17:39.520 --> 17:44.480
high level, we mentioned earlier, the connectome. And so, that's, you know, from one side of the

17:44.480 --> 17:52.880
brain to the other, you know, our emotions, you know, creativity, poetry, painting, you know,

17:53.600 --> 18:00.560
sculpture, engineering structures, you know, is this, you know, is this the level it's kind of

18:00.560 --> 18:05.680
happening at, you know, or does it happen at cortical column or the neuron, you know, or can

18:05.680 --> 18:11.040
we just start here, the kind of bigger conceptual. So, these are good questions. These are all good

18:11.040 --> 18:17.840
engineering questions. And then, of course, you know, if we want to embody this stuff, not just

18:17.840 --> 18:21.920
have it running in our laptop or data, we need to, you know, we'll need a central nervous system,

18:21.920 --> 18:26.400
basically. So, if we're going to build robots like Atlas, you know, we're going to have to connect,

18:26.400 --> 18:35.040
you know, CPU, but it also reaches out in a very complex central nervous system as well.

18:35.840 --> 18:41.440
The beautiful thing, nature, you know, we've been, the Earth's been here 4.5 billion years,

18:41.440 --> 18:46.880
first life was about a billion years ago, that single cell protozoa, you know, so we've had about

18:46.960 --> 18:53.600
a billion years to develop this, you know, evolution, dead end, branching, 99% of species

18:53.600 --> 19:00.160
are extinct. You know, evolution just tries everything, okay, the laws of physical try

19:00.160 --> 19:05.600
everything, intelligent, you know, survivor, the fittest, the things that are well adapted to the

19:05.600 --> 19:14.160
environment, they get to survive. And, you know, over time, you know, what was it a million years

19:14.160 --> 19:19.440
ago, the Neanderthal, and then they developed a bigger neocortex, which enabled them to think a

19:19.440 --> 19:25.440
bit better than the next type of ape. And all the rest kind of basically went extinct, right? So,

19:25.440 --> 19:34.880
we want homo sapiens, and we don't, you know, trying to build ourselves, okay, replicate nature.

19:34.880 --> 19:42.800
So, and then the last level up is social systems. So, you know, if we build a, you know, a few robots,

19:42.800 --> 19:48.320
they've got to get on. You know, we have our social systems, you know, we're a lot more peaceful,

19:48.320 --> 19:53.520
maybe than we used to be. We still have the capacity to go to war, but maybe there's less war.

19:54.320 --> 20:00.400
So, you know, we want to build these higher level structure, hierarchy, societal structures, okay.

20:01.360 --> 20:06.800
And two side people say utopia, dystopia, what's it going to be? Are we going to all get along

20:06.800 --> 20:10.800
these super intelligent machines? Are they going to show us how to live in harmony and,

20:10.800 --> 20:15.040
you know, solve science and cancer and everything? Or are they going to be like the terminator and

20:15.040 --> 20:19.840
come and just get it, wipe us out? Because they see us as kind of destructive and kind of stupid

20:19.840 --> 20:28.800
in a way, okay. Good philosophical questions, right? So, we're not there yet. So, but one day

20:28.800 --> 20:37.920
we will be, and the people give kind of serious attention. So, biological hardware, okay, so

20:37.920 --> 20:42.880
there's the CPUs, GPUs and something called the IP intelligent processing, you know, there's a

20:42.880 --> 20:50.480
company called Graphcore based of the UK. It just got 200 million or 500 million dollars

20:50.480 --> 20:57.200
funding every day, it seems to go up to build this new type of processor. Basically, it's not

20:57.200 --> 21:02.800
really intelligent, it just does matrix multiplication much farther than the GPU. So, it's kind of,

21:03.600 --> 21:10.400
I probably shouldn't know, it's specialized to do deep learning, it's very good at artificial,

21:10.400 --> 21:14.800
you know, deep learning calculation. I'm going to argue deep learning won't get us a general

21:14.800 --> 21:20.960
intelligence, but they're very good at classifying and, you know, drawing graphs, that kind of stuff.

21:22.080 --> 21:27.200
They're not intelligent though, they'll never become conscious. But this is just basically

21:27.200 --> 21:33.760
showing you the CPU, GPU, IP, digital processors, and there they are, that's what they look like.

21:33.760 --> 21:39.840
They build them, they ship them, they get paid money for them. So, maybe some of you didn't

21:39.840 --> 21:44.320
know that, you know, where we are with the ASICs. It's kind of a very, they're calling it like the

21:44.320 --> 21:49.520
Cambrian Explosion and processors. There's a hundred different companies building these ASIC

21:49.520 --> 21:57.280
processors, but they're all designed to optimize the deep learning calculations.

21:57.280 --> 22:03.600
Okay, so we had the CPU there, the Intel, we had the Nvidia GPU, AMD make them too,

22:03.600 --> 22:08.960
but now there's the Google make the GPU, Graphcore make the bottom two ASICs.

22:09.920 --> 22:14.240
They're really good at deep learning, but they're not going to get us to general intelligence.

22:14.240 --> 22:22.560
Again, they're narrow, what I would call narrow AI. And, but yeah, there's a lot of money going

22:22.560 --> 22:26.800
into building these things right now. A lot of these startups are raising a lot of money

22:26.800 --> 22:30.720
to build these for the deep learning. So basically we're solving deep learning, okay,

22:30.720 --> 22:35.840
which is not general intelligence, but we're well on the way to optimizing the hell out of that.

22:35.840 --> 22:40.560
Okay, but it will not get us to general intelligence. So that's what a cloud TPU looks like.

22:40.560 --> 22:46.000
So today we can log into these things. They're a little bit more expensive than a GPUs,

22:46.000 --> 22:49.680
which we can also log into. So say I want to do a deep learning calculation,

22:49.680 --> 22:55.520
I go to Google Cloud, you know, I just say, you know, you go to Amazon, Google, Microsoft,

22:55.520 --> 23:00.240
Azure, the three main cloud providers, only Google have the TPU. That's what they look like.

23:00.240 --> 23:06.800
That's a hundred petaflops. Okay, so that's incredible. I mean, the biggest supercomputer in

23:06.800 --> 23:12.320
the world is about a hundred, is a thousand petaflops. And they take up more than 10 times

23:12.320 --> 23:18.000
the space. These are very powerful things, but they're very domain specific. They narrow AI,

23:18.000 --> 23:23.200
but they're, they're hugely powerful thing and super impressive. And if you're, if you're into

23:23.200 --> 23:28.800
hardware, these things, you know, Google hired 10 years ago, some guys to the best in the world,

23:28.800 --> 23:34.000
to build these things for them. And they kept them secret for a while. That's how DeepMind

23:34.560 --> 23:41.360
AlphaGo, by the way, they, they use these. Okay, so those are TPUs. Again, this is a

23:41.360 --> 23:46.560
larger supercomputer, but it's as dumb as a brick, okay? It can multiply to very large numbers or

23:46.560 --> 23:54.240
factor something very quickly in the, you know, flash of an eye, but they don't, they're no

23:54.240 --> 24:00.720
consciousness, there's no creativity, there's nothing, okay? So these things, they, they're

24:00.720 --> 24:04.880
good at number crunching, but that's about it. But they're hugely impressive and they

24:04.880 --> 24:10.240
cost a lot of money. They also use a lot of power. This thing, the summit supercomputer by IBM.

24:10.880 --> 24:16.640
So it's not just a matter of raw compute, right? This thing uses 13 million watts of power. The,

24:16.640 --> 24:25.760
the brain uses 30 watts. So it's almost a factor of a million more efficient, okay? The brain,

24:25.760 --> 24:31.280
and then we do a heck of a lot more biology is very clever, isn't it? So, you know, what these,

24:31.280 --> 24:34.480
that just gives me more and more respect for biology when I see things like this.

24:35.920 --> 24:42.800
So yeah, there it is. There is the creation. 1.1 million, or you could say 4.5 billion years of

24:42.800 --> 24:51.120
evolution is optimized to this, to us here today. You know, Einstein, Picasso, Van Gogh, you know,

24:52.080 --> 24:57.600
Leonardo da Vinci, who's the smartest guy on the planet today, maybe Ed Whitton at Princeton,

24:57.600 --> 25:03.360
he's a physicist. We can do a lot, which computers cannot. So how the heck do we do it, right?

25:03.360 --> 25:09.840
It's 30 watts. It's, it's just, you know, fits in the skull. It's, it's 1.5 kilo. So what's going

25:09.840 --> 25:18.800
on? What are we missing? Okay, this is where neuromorphic computing comes in. And there's a

25:18.800 --> 25:25.360
project called Spinnaker. So this guy called Steve Furber up in Manchester, he helped build the arm

25:26.000 --> 25:34.160
processor, which is a CPU, and recently sold to SoftBank, I think for 20 billion.

25:37.040 --> 25:42.880
Now he, out of that 20 years ago, he started to build neuromorphic computing. And so Spinnaker is

25:42.880 --> 25:47.920
his effort. It's now part of the human brain project. So these things are, my point is that,

25:47.920 --> 25:51.920
you know, these things are real. You may not have heard of them, but, you know, they've been

25:51.920 --> 25:57.920
going 20 years now. IBM have an effort called True North. So these things are based, these

25:57.920 --> 26:03.920
processes are based on how the brain works. Okay, they're not like the summit, they're not, not

26:04.560 --> 26:09.040
emulation. They're actually direct simulation of how the brain, so they're basically building

26:09.040 --> 26:16.240
artificial neurons and silicon. Okay, artificial neurons and silicon. And we're up to about a billion,

26:16.240 --> 26:22.480
which is about a mouse. Okay, today, 20 years ago, we had one right now. Everything's on a

26:22.480 --> 26:28.560
Moore's Law. So yeah, we're up to about a billion. Maybe we need just to scale it another 100 times

26:28.560 --> 26:33.600
and have a human brain. That's quite possible, actually, it's in the human brain project. It has

26:33.600 --> 26:40.000
a billion euro funding. So that's that's that's moving as fast as it can move. Okay, it's a big

26:40.000 --> 26:48.160
European project now. So yeah, and it's available today in the cloud. IBM have theirs. Intel also

26:48.160 --> 26:56.560
have something called the Ohi, the Hawaiian word. There are many startups as well. So again,

26:56.560 --> 27:00.560
there are, you know, maybe 50 startups, neuromorphic, just Google it, neuromorphic computing.

27:01.600 --> 27:07.440
There are bigger efforts, brain scales is another human brain project one study started in Germany.

27:07.520 --> 27:14.320
It's been a Chrissy UK one true north is IBM and Intel have one. And then there's all the startups

27:14.320 --> 27:22.800
as well. Exciting times, we live in exciting times. So, so basically, you know, we have the

27:22.800 --> 27:28.880
digital processing, which I showed you, including the ASICs, TPUs, the Graphcore, we have neuromorphic,

27:28.880 --> 27:34.560
which is biologically inspired computing, and we have quantum computing. Okay, so, you know,

27:34.560 --> 27:40.000
20 years ago, we just really had the digital. Today, we have three different types of computing.

27:40.000 --> 27:44.320
And so things are going to get interesting. And we live in interesting times the next 20 years

27:44.320 --> 27:49.680
are going to be very interesting in terms of computation. Because we now have this neuromorphic,

27:49.680 --> 27:54.240
which is like biology, and we also have quantum computing. So things are going to get very,

27:54.240 --> 27:59.040
very interesting. So that's what it looks like. There's five cabinets, we're up to 10 now.

27:59.840 --> 28:07.200
And this is how it scales, you start off with 1000 neurons in the core, you put 18 cores on

28:07.200 --> 28:12.160
a chip, then you put 48, this is all engineering at this point, it goes to a fab and it gets built.

28:12.160 --> 28:18.320
You have 48 chips on the board, 24 boards in a rack, and then you put five racks per cabinet and 10

28:18.320 --> 28:24.880
cabinets. That's the intelligence of a mouse right there. That's a billion artificial neurons,

28:24.880 --> 28:31.120
much bigger than a biological neuron. But again, all of this is on Moore's law when they first

28:31.120 --> 28:38.320
we had 1000 remember in 1970, I think we had 1024, the first processor that ever built. And it was

28:38.320 --> 28:47.280
the size of a one inch by one inch 1000. Now we can fit 10 billion neurons on a one inch by one

28:47.280 --> 28:54.000
inch. Okay, it's Moore's law. So these things are shrinking. You know, so, you know, it's on a

28:54.000 --> 28:59.200
trajectory. So that will scale down nicely in 10, 20 years, that will be the size of the mouse brain.

29:00.080 --> 29:04.240
Okay, so that's what they look like. That's on brain scales on the left, that's a

29:06.080 --> 29:13.120
human brain project from Germany. And then there's Google GTPs on the right. So analog versus digital,

29:13.120 --> 29:18.320
I'm arguing that we need the newer, we need the neuromorphic on the left to get us to general

29:18.320 --> 29:23.920
intelligence. The one on the right will do it, we can simulate it, but it's like a thousand

29:23.920 --> 29:29.760
time classification is impressive as an engineering fee is that is, but it's very good at deep learning.

29:31.040 --> 29:38.800
Okay, what about quantum? Maybe we're making progress there, we're up to about 100 qubits,

29:38.800 --> 29:43.600
quantum bits, that's on its own Moore's law. So, you know, within 10 years, we're on 1000,

29:43.600 --> 29:50.160
then we'll start doing interesting calculations. And does the brain use quantum? Roger Penrose

29:50.160 --> 29:57.280
would argue yes. Yeah, we don't know if we have to go to that level. Roger Penrose actually argues

29:57.280 --> 30:06.160
that, you know, consciousness comes, is the only way to get a conscious thing is system is to go

30:06.160 --> 30:12.080
down to the quantum level. Interesting, huh? Okay, so there we have it. Those are the four

30:12.080 --> 30:16.480
types of compute, we have digital, we have neuromorphic, we have quantum, and we have biology.

30:19.280 --> 30:27.520
Okay, and that's what they look like at the microscopic level. So in some sense, you know,

30:28.080 --> 30:33.360
their computation, you know, the physics of computation is called information processing,

30:33.360 --> 30:40.320
okay. And in some sense, you know, when you get down to it, you know, the equations are the same

30:41.040 --> 30:45.440
for the three classical systems. For the quantum computing, they're different, they obey the laws

30:45.440 --> 30:51.280
of quantum mechanics. But theoretically, if you're interested, you could just Google information

30:51.280 --> 30:57.520
processing. Eventually, you wind up here. But the trick is to build them, right? Okay, so that's

30:57.520 --> 31:02.320
what the data center of the future is going to look like. Right now, that's just almost, you know,

31:02.320 --> 31:09.760
99% classical digital computing. You know, there's a few spinnaker machines, you can buy that spinnaker

31:09.760 --> 31:17.600
processor, you know, you can actually buy them. So, but not many data centers have them. But the

31:17.600 --> 31:22.400
point is you can buy them. Quantum computing, you can also buy or you can buy that you can log

31:22.400 --> 31:27.040
into the cloud today and log into an IBM machine, they have 20 quantum computers on the cloud as

31:27.040 --> 31:35.840
of today, and also log into a regati machine. Amazon and Azure, Microsoft and their cloud

31:35.840 --> 31:41.280
are also going to offer quantum computing as a service soon. You can log into spinnaker at the

31:41.280 --> 31:46.000
human brain project, but I think you have to be an academic to do that. It won't be long before

31:46.000 --> 31:51.680
they commercialize that too. But you can see the road ahead. I'm arguing that probably neuromorphic

31:51.680 --> 31:55.840
is the way to get us a different intelligence. Quantum is going to be very useful for

31:56.480 --> 32:03.200
simulating molecules, molecular dynamics, drug discovery, in classical will be good for, you

32:03.440 --> 32:09.760
know, the internet, our iPhones, etc. So, each type of computing has its own specific use. I'm

32:09.760 --> 32:15.520
going to argue neuromorphic is probably the one for AGI. Okay, so deep learning, what about deep

32:15.520 --> 32:22.880
learning? I'll end the time here, 30 minutes. Okay, probably be another 15 minutes. Okay,

32:22.880 --> 32:31.200
probably two thirds through the talk. So, we keep on this call, everyone's heard of deep learning.

32:31.520 --> 32:36.080
You know, I make a living out of it. I've been doing it for six years, seven years now.

32:37.760 --> 32:42.320
That's, you know, it's not going to get us a general intelligence. So, I basically put that in

32:42.320 --> 32:47.920
here to just point that out. It's good at classifying images. It does some sort of language

32:47.920 --> 32:54.320
translation and text to speech, etc. There's neural network, deep learning optimized chip

32:54.320 --> 33:01.760
in all phones now, or you see all laptops, okay. Yeah, they're called system on a chip. You have

33:01.760 --> 33:05.680
your CPU, you have your GPU, but you also have a neural networking deep learning

33:05.680 --> 33:11.440
processor as well these days, okay. So, they're used to some things, mostly, you know, to do

33:11.440 --> 33:17.920
with image recognition, classification and language. Okay, so it just swaps over to whatever

33:17.920 --> 33:24.000
processor is best for the time. So, if you have an application on your phone that requires

33:24.000 --> 33:29.040
deep learning, it will swap over to the deep learning processor. And these are known as

33:29.040 --> 33:34.560
co-processors. So, the CPU is always the main core processor. But if it has something that

33:34.560 --> 33:39.840
needs to offload on the GPU or the neural deep learning processor, it will do. And then the

33:39.840 --> 33:47.200
back and forth between CPU, GPU, they call it co-processors. So, you know, these neural network

33:47.920 --> 33:52.640
are on all devices these days, okay. Probably in the last five years, they've started putting

33:52.640 --> 33:57.200
them on all devices, because deep learning is a thing now, right. There's no neuromorphic

33:57.200 --> 34:02.640
processors yet. Maybe that's to come. Okay, they're too big, right. They won't fit in the phone yet.

34:03.360 --> 34:07.760
You have to use them on the cloud. And so, you know, we've all heard of the different

34:07.760 --> 34:13.120
framework. TensorFlow is the most popular. PyTorch has become quite popular as well. The two main

34:13.120 --> 34:17.520
deep learning frameworks are TensorFlow and PyTorch, which are interesting. And that's it.

34:17.520 --> 34:22.960
This isn't a talk about deep learning, or TensorFlow or PyTorch. But I just put it in there

34:22.960 --> 34:28.160
to see whether, you know, deep learning is getting all the press, you know, is it part of the journey?

34:28.160 --> 34:41.200
No. It doesn't give you what a neuron does. No. It can do, like I say, image classification language,

34:41.200 --> 34:45.520
but it will not do the other types of intelligence. Remember, the creativity of moving around

34:45.520 --> 34:50.160
navigations through space, interpersonal, interpersonal won't give you that at all.

34:51.760 --> 34:57.440
Zilch zero. Okay. So we're going to need something else. I mentioned neuromorphic. But what about

34:57.440 --> 35:03.200
the theory? Okay, so let's have a quick look at that now. Okay, so general reason of intelligence.

35:03.200 --> 35:07.280
So, you know, what do we need? What are the different approaches? I'll cover them.

35:08.480 --> 35:13.360
There's called active inference, which I'm going to really focus on. And then what do we need to

35:13.360 --> 35:22.240
build an AGI processor? Okay, so what do we need? So what we do is we turn to physics, right? We're

35:22.240 --> 35:31.680
a physical system, the human brain of a fly, a snake, a monkey, you know, these are all physical

35:31.680 --> 35:38.560
systems. And so what do we need to understand in physics? There's no easy way out. There's no magic.

35:38.560 --> 35:44.240
We've got to go down. We've got to do the math, right? So, you know, are we missing anything in

35:44.240 --> 35:49.760
physics? Well, there's everything we know about the universe right there. And all of that can be

35:49.760 --> 35:56.000
described as a very fundamental principle called the principle of least action. Now, about two or

35:56.000 --> 36:01.520
three years ago, this book published two years ago, in fact, became a university press called the

36:01.520 --> 36:07.440
principle of least action. All the physics can be actually reframed as, you know, this principle,

36:07.440 --> 36:11.920
which basically says, now it gets mathematical for the next couple of slides, switch off if you

36:11.920 --> 36:17.120
want to. But conceptually, what we do is we write down something called the action, which we denote

36:17.120 --> 36:24.720
as s. And we write down that equation, it basically goes back to Hamilton Lagrange, about 150 years

36:24.720 --> 36:30.720
ago. And, you know, it's quite common if you're a physicist to formulate things in terms of Lagrangians,

36:30.720 --> 36:37.600
Hamiltonians, the action is a function of these Lagrangians. And, you know, you do some scary math

36:37.600 --> 36:45.520
and basic point, I can formulate any system in the world, whether it's a brain, or a car going down

36:45.520 --> 36:51.600
the road, a couple of billiard balls on a billiard table, anything like atomic quantum physics,

36:52.640 --> 36:58.880
classical physics, the universe cosmology, turns out there's this very unifying principle called

36:58.880 --> 37:06.480
the principle of least action. Okay. So maybe we could start there. Okay. So what we have to do,

37:06.480 --> 37:11.280
what's different about the brain compared to say, you know, a black hole, or, or, you know,

37:11.280 --> 37:18.640
a system of galaxies, or, you know, a chemical interaction, okay, what's different here? So

37:18.640 --> 37:24.880
we got to understand the system we're trying to model, right? So we need to, we need to build a

37:24.880 --> 37:31.280
system that can model the world. Hmm. Okay. Just like a mouse makes a little model of the maze, or

37:31.280 --> 37:35.920
we make a model of how we're going to make money, or get up and go to work, or, you know,

37:35.920 --> 37:39.760
get in the car and go on a holiday, you know, we're always modeling the world crossing the road

37:39.760 --> 37:45.280
without getting one over, et cetera, et cetera. You know, finding a mate, getting married, having

37:45.280 --> 37:51.920
kids, you know, we're modeling all the time, you know, solving, you know, problem in chemistry,

37:51.920 --> 37:54.960
we're modeling, modeling, modeling, we're always trying to model the world. So

37:54.960 --> 37:59.440
we need to build a system that can do that using the principle of least action. That to me is

37:59.440 --> 38:03.200
general. That would be a generally intelligent system. Okay. So how do we go about doing that?

38:03.760 --> 38:09.280
Just so happens that a lot of very smart people in the world have been working on this for the last

38:09.280 --> 38:15.200
30 years or so and doing the math. Okay. They've been writing papers, publishing them, writing

38:15.200 --> 38:20.480
the equations down, building prototype systems. Okay. But it's got to, these systems have to be

38:20.560 --> 38:27.360
able to explain and understand what they see, play all these board games, go chess, Sudoku,

38:27.360 --> 38:31.120
but also, you know, cross the street without getting run over. They have to be able to model

38:31.120 --> 38:36.080
their world. Okay. If you build a robot, no use having it just walk in front of a car, right?

38:36.080 --> 38:41.600
It has to be modeling its environment. I mean, that Atlas robot doing flips. I mean, that's

38:41.600 --> 38:46.160
mind boggling. So that's a robot modeling its world that it can do that.

38:47.040 --> 38:51.520
But also it has, that's physical and it also has to be smart, right? That

38:52.160 --> 38:55.680
Atlas is dumb as a brick, you can't play chess. We have to have the whole thing,

38:55.680 --> 39:00.480
like we have, you know, 1.5 kilos, 30 watts, we have to make the whole thing,

39:00.480 --> 39:05.440
which is where the neuromorphic process would come in. So it can plan problem solve,

39:05.440 --> 39:10.080
build new models as they learn more about the world, update their prior knowledge,

39:10.080 --> 39:15.600
you know, just like we do imagining things, have imagination into personal skills. Okay. So here's

39:15.600 --> 39:20.000
some approaches. I mentioned some very clever people who work on this. There's a guy called

39:20.000 --> 39:25.840
Karl Friston here at the UCL in London. There's a guy, Tishby in Israel, Bialek at Princeton,

39:25.840 --> 39:34.000
Hooter is in Australia at the moment. He started in Germany, I think, Schmidt-Huber is in India,

39:34.000 --> 39:39.600
in Switzerland. He's, he's set up a company called Golden Machine with lots of patents.

39:39.600 --> 39:44.480
He's very smart guy. So, and there's many more. So these are very clever people being,

39:45.200 --> 39:50.160
you know, trying to solve intelligence for their life. I mean, you know, as children,

39:50.160 --> 39:53.920
they were going, you know, how do I solve intelligence? You know, some of us say, well,

39:53.920 --> 39:58.320
yeah, how do I get up at nine and come home at five? I mean, these guys, you know,

39:58.320 --> 40:03.280
just like Einstein, natural born with this thing, right? I want to solve intelligence.

40:04.160 --> 40:08.960
So these are the guys. Okay, so let's focus in on Karl Friston.

40:09.280 --> 40:18.400
So basically, he's using the principle of least action. It's, I call it the free energy principle,

40:18.400 --> 40:23.200
but it's the same thing, basically, it uses physics. Okay, so basically, his stance is that

40:23.200 --> 40:28.240
systems, you know, operate in a world to minimize your free energy, something called free energy.

40:28.800 --> 40:34.880
And, you know, that will cause them to explore, you know, you get the explore exploit tradeoff,

40:34.880 --> 40:38.640
they will explore their environment. How else do we learn? Remember, as little kids,

40:38.640 --> 40:43.280
baby, watch a baby, they just roam around, crawl around exploring their environment,

40:43.280 --> 40:48.720
putting things in their mouth. Yeah, this is how we learn. Okay, so the principle of free energy,

40:49.920 --> 40:54.960
you know, this is what basically our brains are doing. They're just minimizing the free

40:54.960 --> 40:59.840
energy the whole time. If there's uncertainty, we want to minimize it, right? That maybe that's

40:59.840 --> 41:03.760
what the best definition of intelligence is, minimizing uncertainty as quickly as possible

41:03.760 --> 41:09.760
and effectively as possible. Okay, how do we, you know, solve chemistry or physics or figure

41:09.760 --> 41:13.600
out the universe? Well, these are uncertainties, we're just trying to minimize them as quickly as

41:13.600 --> 41:18.560
possible. And it turns out the brain, we're not separate from the universe, okay, we're a physical

41:18.560 --> 41:24.080
system within the universe, which, you know, our environment. So, you know, they're interacting

41:24.080 --> 41:29.680
the whole time. So it's just physics, okay, basically, I say just in quotation marks. So

41:29.680 --> 41:34.240
you can write down equations for these interactions and how our system might minimize

41:34.240 --> 41:40.080
this free energy in the areas that's called first and I mean,

41:44.320 --> 41:50.560
we could listen to him for two seconds.

42:30.080 --> 42:56.320
So I guess technical quite quickly, a lot to do with probability, Bayesian influence,

42:56.320 --> 43:03.040
he mentioned the word influence. Yeah, so the brain, it's always, there's always sensory input,

43:03.040 --> 43:07.920
right? Even when we're sleeping, you know, we have dreams, there's subconscious, there's conscious,

43:07.920 --> 43:14.640
I mean, you know, there's internal sensory perception that is perception from our environment.

43:15.600 --> 43:20.400
You know, there's mental illness, schizophrenia, where the information processing goes awry.

43:21.360 --> 43:25.520
You know, it's a complex system, there's 100 billion neurons all different parts, layers upon

43:25.520 --> 43:30.960
layers, it's true evolution. The neocortex is what enables us to come up with things like

43:30.960 --> 43:35.920
a theory of relativity and build cars and, you know, solve physics and chemistry problems and

43:35.920 --> 43:41.280
mathematics. So yeah, it's a complex system, but it's basically a system that's based on the

43:41.280 --> 43:46.080
free energy principle, which you heard Professor Friston explaining a little bit there. That's

43:46.080 --> 43:50.880
what it looks like in a diagram. These other guys like Schmidt-Huber, they have their own systems,

43:50.880 --> 43:55.280
but I think they will all basically boil down to, you know, the free energy principle,

43:56.000 --> 44:00.640
the equations, you know, they look kind of similar. They may use different terminology

44:00.640 --> 44:05.440
vocabulary, but physics is physics, right? Okay, so they're all trying to model a physical system

44:05.440 --> 44:11.760
called the brain. Right, so we have external states, internal states, and there's only a few

44:11.760 --> 44:17.680
more slides now. And something called Markov-Lanker, which kind of delineates the two, the internal

44:17.680 --> 44:24.480
from the external environment, but there's no real, you know, there's no fixed thing, right? I mean,

44:24.480 --> 44:29.920
things enter and we take action, enter our brain and we take action, right, all the time. We're

44:29.920 --> 44:36.320
always interacting with our environment throughout pores and our skin. There is no fixed line,

44:36.320 --> 44:43.120
but it's a theoretical construct to help us through the max. Okay, and it's completely

44:43.120 --> 44:49.440
general. It works for cells. Cells are intelligent. They reproduce, you know, they survive in their

44:49.440 --> 44:57.840
environment right up to brains. Okay, and this is the math, pretty ugly. Okay, so last few slides.

44:57.840 --> 45:03.200
Can we build general intelligence? Okay, very clever math. These guys might end up getting

45:03.200 --> 45:08.080
Nobel Prizes and whatnot, because, you know, one could argue it is maybe the defining problem about

45:08.240 --> 45:13.600
time, right? Build intelligence, you know, solve climate change, solve cancer, build intelligence,

45:13.600 --> 45:19.120
you know, these are the big problems. If someone solves it and builds the system, you know,

45:19.120 --> 45:23.920
they're going to get a lot of money and a lot of prizes. So, I mean, a lot of these guys have a

45:23.920 --> 45:30.640
super lot of recognition already if you're in the field. So, and, you know, it's a combination of,

45:30.640 --> 45:34.880
like I showed before, computer science, so a lot of them are computer science, physics,

45:34.880 --> 45:40.560
and neuroscience. So, a lot of them are neuroscientists, but some are more computer science, but

45:40.560 --> 45:44.960
with a little neuroscience, some are neuroscientists with a little computer science, you know,

45:44.960 --> 45:49.520
a little physics. But yeah, the one thing is they're all really, really good at math. Okay,

45:49.520 --> 45:56.800
so can we build? The answer is yes. We have candidate theories. We have some algorithm

45:56.800 --> 46:02.320
software, some math. We have the hardware now, and we have masses of data sets. So,

46:03.280 --> 46:09.280
when, right, basically, the question is, how about if it's when? It's going to be like a

46:09.280 --> 46:13.600
TensorFlow for general intelligence, just to kind of touch base with something we're familiar with.

46:14.160 --> 46:18.720
And we're going to need a lot of software engineers to actually build these systems

46:18.720 --> 46:23.920
and hardware engineers to build the neural network processors. It's a big project. I mean,

46:23.920 --> 46:27.280
the human brain project, you know, is an Apollo project of our time.

46:27.280 --> 46:33.760
You know, it's going to require funding. There's a lot of funding going into this stuff at the

46:33.760 --> 46:39.680
moment. And you've got the human brain project, DeepMind, you've got the US brain project, China's

46:39.680 --> 46:44.960
working on this, everyone's working on it. Should we build AGI? Again, that comes back to the

46:44.960 --> 46:52.320
philosophy, philosophical question, you know, safety, ethics, singularity. But that's for another

46:52.320 --> 47:00.000
time. Okay, so here are some AGI projects I mentioned from countries all over the world,

47:00.000 --> 47:07.680
all very, very viable projects, in my opinion, all of them. No clear winner. All good stuff.

47:09.040 --> 47:16.160
Okay, so in conclusion, it's obvious that deep learning is lacking the foundations to build

47:16.160 --> 47:21.600
general intelligence. It's based on statistics, not physics. It's a statistical hack. It's very

47:21.600 --> 47:27.120
clever. But it takes a million times as much energy as the brain to do, you know, about one

47:27.120 --> 47:32.160
percent of what the brain does. So clearly, that's a no. Research groups are looking into

47:32.160 --> 47:39.120
bio plausible models. That's the way to get there. That's, you know, in terms of theory and hardware.

47:39.680 --> 47:45.840
So the real question is when, right? So we've got prototypes now, Schmidt-Huber,

47:45.840 --> 47:51.760
you know, he's building stuff, commercializing it. Oh, Friston, you know, has a huge code base,

47:52.400 --> 47:57.200
you know, hundreds of papers. So we were on our way. We're at the beginning of an

47:57.200 --> 48:04.560
exponential curve, I'd say, right? So yeah, it's not a when, it's not a defining moment. It's a

48:04.560 --> 48:08.640
continuum. We're at the beginning. We've got a, we've got a billion neurons,

48:09.280 --> 48:13.680
artificial neurons in a human brain project. We've got the brain of a mouse that could do

48:13.680 --> 48:18.320
Sudoku puzzles that can solve stuff that humans can solve that deep learning would never be able

48:18.320 --> 48:24.560
to solve at a thousand times less energy. So we've got the hardware that's on its own Moore's law.

48:25.760 --> 48:31.680
And what else? So finally, even Jeff Hinton, he's a godfather of deep learning, right? Won

48:31.680 --> 48:39.360
a Turing Award with Jan Lacoon and Yoshua Benjo last year for deep learning. He's saying, assuming,

48:39.360 --> 48:44.880
you know, he knows as much about anybody about deep learning. He's one of the founders. Assuming

48:44.880 --> 48:48.800
that the computer industry can keep producing better hardware, I think business issues are

48:48.800 --> 48:53.600
going to take us a long way. He's talking about the TPUs, the Graphcore, all of that. Obviously,

48:53.600 --> 49:00.160
if we get the big conceptual breakthroughs, probably from neuroscience, it'll take us further. I think

49:00.240 --> 49:03.840
one of the big breakthroughs that's going to come is we're going to understand the brain.

49:04.880 --> 49:10.720
Yeah, it's all about understanding how nature has done it. And nature is used to laws of physics.

49:10.720 --> 49:20.640
And Carl Friston, this act of inference, formulates that beautifully in some beautiful mathematics.

49:30.160 --> 49:55.280
Okay, so was this your final word? Can we move on to questions?

49:56.160 --> 50:07.360
Yeah, so any questions? Thank you. Okay, we have a few questions. It was a really enthusiastic

50:07.360 --> 50:14.080
speech of yours. And so these are the questions. If we managed to create artificial general

50:14.080 --> 50:23.760
intelligence, what would be the odds of the AGI being sentient? Yeah, so that will fall out naturally.

50:24.160 --> 50:30.400
So sentience is part of intelligence. Remember the types of intelligence? So that's the existential

50:30.400 --> 50:38.320
part. That's what that meant. So yeah, they have to be sentient. Okay, they have to be sentient.

50:39.040 --> 50:43.040
I mean, first of all, we'll build ape-like intelligence. Apes are sentient, right? They

50:43.040 --> 50:48.160
know what's going on in that level. Okay, so they will be sentient. And as we get more and more

50:48.160 --> 50:53.600
intelligent, they'll become more and more sentient. Yeah, we haven't seen that movie here. That's a

50:53.600 --> 51:00.400
beautiful movie with Joachim Phoenix. Have you seen it here? The thing on the iPhone gets smarter

51:00.400 --> 51:06.640
and smarter, more and more sentient until it goes off into its hyper universe somewhere. I mean,

51:06.640 --> 51:11.200
that's the kind of thing. It will start kind of like human level, ape level, then get the human

51:11.200 --> 51:18.480
level, but then go into, you know, they'll be more sentient than us. Okay, and together with this

51:18.480 --> 51:25.120
question, if it grasps the concept of the humor, it controls us by giving us false predictions.

51:25.680 --> 51:31.440
Can you trust it and secure it? Is that slavery? Yeah, so that's a good question too.

51:31.440 --> 51:35.920
No, they'll have rights like us, right? Just like, you know, there's animal rights,

51:35.920 --> 51:43.040
animal cruelty is a no-no, right? You go to prison if you harm an animal, right? It's not right.

51:44.080 --> 51:48.800
So yeah, that comes down to ethics and law. Yeah, they'll have their own rights.

51:52.240 --> 51:53.040
They will have rights.

51:53.040 --> 52:08.000
Okay, and okay, what do you think about artificial intelligence in drug design?

52:10.080 --> 52:15.600
Yeah, so it's going to help a lot. So right now we took a question because we're using deep learning

52:15.600 --> 52:22.960
to help us discover drugs to explore the chemically molecular dynamic space

52:23.920 --> 52:27.840
much quicker than we can if, you know, we do it ourselves. Okay, so deep learning is actually

52:27.840 --> 52:32.800
accelerating drug discovery right now. Okay, the idea is to accelerate it even further by

52:32.800 --> 52:38.080
using an artificially journal intelligence system. So yes, right now deep learning is

52:38.080 --> 52:42.480
absolutely accelerating drug discovery and has witnessed in the deep mind-winning,

52:42.960 --> 52:46.960
particular in competition recently, which is, you know, molecular dynamics, which is

52:46.960 --> 52:54.560
similar to drug discovery. Okay, so there's just one comment. A lot of science fiction's work have

52:55.360 --> 53:01.680
gave rise and inspiration to many great technologies and research. So they want to know

53:02.480 --> 53:08.640
which science fiction or any other general books have gave you inspiration and helped

53:08.640 --> 53:11.680
you shape yourself towards the professional that you are today?

53:13.680 --> 53:18.720
Yeah, so science fiction is, you know, when I was younger, I always read science fiction and

53:18.720 --> 53:25.680
watched Star Trek, you know, huge science fiction fan, really. So yeah, I mean, as the most foundation

53:25.680 --> 53:32.800
series, Robert Heinlein, Isaac Asimov, Arthur C. Clarke, those are kind of the, but yeah, there's

53:33.120 --> 53:38.720
new guys around right now with, you know, different science fiction series. Always good

53:38.720 --> 53:45.840
for the imagination to read science fiction. There's nothing wrong with the fact that it's

53:45.840 --> 53:51.120
encouraged, right, to, science fiction is a great thing. That's, you know, it's the start of all

53:51.120 --> 53:59.200
sciences in science fiction. So yeah, you can encourage it. Okay. And if we ever reach artificial

53:59.200 --> 54:05.680
general intelligence, do you think laws would be implemented on time, or we would firstly see

54:05.680 --> 54:12.080
abuse of artificial general intelligence in daily life? You partially gave the answer on this.

54:13.360 --> 54:17.280
Yeah, so, you know, there's always going to be good actors and bad actors, right? So yeah, we're

54:17.280 --> 54:21.920
going to have to come up with laws and try to find ways of enforcing it. There's going to be some

54:21.920 --> 54:27.120
silly things happen, of course, you know, decide with nuclear energy, right? Very, all very powerful

54:27.120 --> 54:32.960
technologies can get misused by bad actors. And so we are, we're going to have to keep an eye out

54:32.960 --> 54:38.560
for that. We have to enforce it. We have to have police at that point, right? Yeah, of course.

54:39.200 --> 54:48.400
And the last final question, as far as I can see, how, oh, okay, one more. Okay, how artificial

54:48.400 --> 54:56.960
general intelligence could affect religions? What's that? Okay, how the artificial general

54:57.200 --> 55:03.440
intelligence could affect religions? Oh, yeah, so I did mention that, I think

55:04.480 --> 55:11.200
sentient being will have the capacity for religion and religious and spirituality and all that good

55:11.200 --> 55:19.520
stuff. You know, so yeah, it just comes along with a religion. It's a type of intelligence,

55:19.840 --> 55:25.120
intelligence is a type of intelligence. Yeah, of course. Okay, as far as I can see,

55:25.120 --> 55:33.040
that those are all questions, but we'll wait a few minutes more. Thank you so much for answers and

55:33.040 --> 55:39.600
for this presentation and being with us tonight. It was really enthusiastic and inspiring to watch

55:39.600 --> 55:49.040
this and to understand it a bit more. So we really thank you. Thank you so much, me and all the people

55:49.120 --> 56:00.080
from Foundation. Sure, I look forward to watching it on YouTube. Yeah, and we hope to collaborate

56:00.080 --> 56:07.440
again with you, of course. So thank you so much. There are no more questions, so I would call it.

56:08.960 --> 56:16.240
And okay, thank you everyone for watching. See you on the next webinar.

