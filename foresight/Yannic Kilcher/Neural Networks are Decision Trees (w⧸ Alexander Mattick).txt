Hello everyone, today we're talking about neural networks and decision trees.
I have Alexander Matic with me who is maybe you want to introduce yourself.
Yeah, I'm currently a student at FAU in Germany and most people don't know me probably through
for Janik, for his discord. I'm one of the people who manage the paper discussions every week and
present more of the theoretical papers usually.
So we came across this paper all across social media. I saw it at one point and I'm like meh
and then I saw it like all over LinkedIn being like whoa neural networks are no longer a black box.
We now know what's going on. I saw it on Twitter. I saw it essentially like it like really got some
some push behind it. As I said when I first saw it it was like meh this has been known for a while.
So what does this paper say in a general sense and has it been known for a while or is there
actually something in there? Okay so basically what this network does is what this paper does
it shows how you can basically take a neural network which is like a sequence of weights
with non-linearism between and then you can kind of each virtually if you rewrite them
by effectively pulling out the right slopes and merging them up into new weights and that would
give you effectively this kind of structure. It's important to say this is only for if the
if the non-linearity is piecewise linear for example a relu non-linearity otherwise we
we have an approximation but this is an actually any exact mapping that we're doing right here.
So we just rewrite the neural network somehow and then we get out what? So we get out such a
tree and effectively you can see these w hats here and these w hats I think they're defined
somewhere yeah I think somewhere up here yeah effectively just unroll the the piecewise slopes
always from the layer above. So effectively we go and we draw the different cases that happened
through the previous layer we draw them up into the subsequent weights and that gives us kind of
this tree structure because we of course get this unfolding of kind of which path can we go in the
neural network and then the next net then the next layer can kind of enhance that path and so on.
I think it's still a bit unclear maybe to some people who are not super familiar with this
they might be under like the general notion is a neural network is a non-linear function right
therefore I wouldn't just be able to represent it with a single even if the w and the w hat
are different right I still at the bottom here I see you know x times w something which is a
linear function so why all of a sudden I have a neural network why do I arrive at a bunch of
linear functions? This mostly has to do with the fact that neural networks intrinsically are just
compositions of these piecewise linear functions for example there's been more recent work I think
here in the spline theory of deep learning so more recent work more recent than the paper we're
looking at no recent in a sense of it was published after 2000 okay this paper from I think 2018 and
there they make this very very explicit where effectively they show that you can unfold almost
every network into what is called splines and you can think of splines as kind of regions
which then in and of itself are affine linear so it's a linear transform with some bias against
it and these deep neural networks are just different regions all of which have their own slope and
slope and bias. If we imagine a neural network with ReLU non-linearities if we imagine a point
somewhere in the input if we move that point like just a tiny bit then we move it small enough
so that none it crosses none of these boundaries a ReLU is essentially like like this so it has
like a boundary here where the slope changes but if we move just small enough that either the signal
is in the slope so it changes a bit in the slope or it doesn't change at all because it's in the
zero part so if we move just a bit we don't change the ReLU activation pattern and that essentially
means since all the functions are either linear or piecewise linear but we don't switch the piece
that means that within such a ReLU cell it's essentially a linear function I think that's
what we see here at the end of the decision tree the decision tree essentially says with this particular
input which of these ReLU cells am I in and at inside of that cell it's actually a linear function
and that's what's described here the neural network in total is non-linear because obviously
we piece together super many of these tiny ReLU cell regions and that can make something that
appears almost like smooth like because if we zoom out then you know it's like a video game where
everything is made of triangles but you zoom out and it kind of looks round it kind of looks smooth
the paper shows you can rewrite the neural network and you get something like this what does it mean
that's an entire different question because so there are many different ways of viewing such a
conversion one is through a practical lens another one is from a lens of what does it help us to
study decision trees another one is how it does it help us to study neural networks
from a position of studying decision trees it doesn't really help us that much because neural
networks are inherently a lot more uh impenetrable than decision trees really studying a neural network
and that helping us to figure out something about decision trees is rather hard additionally we have
the problem that decision trees fundamentally so the decision tree learning algorithms we build
are they themselves don't map to neural networks perfectly what I mean by that is you can take
a decision tree like this thing here and transform it into a neural network however during the
decision tree training process what you usually do is you take one of those effectively edges
and then you split it up into two lower ones and for that you may need a new neural network
because the capacity of the original one doesn't work out anymore so from a perspective of taking
a neural network and then helping to figure stuff out for decision trees it's pretty hard
on the other hand we can use these decision trees to find figure out stuff about neural networks
there's a lot more promising but there's often the case that to do the kind of analysis you can do
with the decision trees you don't necessarily have to explicitly build this tree like the
spline theory of deep learning paper which does lots and lots of analysis so for example there
was a recent paper which specifically looks at what batch norm actually does through this lens
but they don't need to build the explicit decision tree because they are just interested
in this piecewise linearity they're not necessarily interested in how exactly this fits to the actual
neural network part or the actual tree part and then last but not least we can also analyze it
through the view of let's take an existing neural network like a ResNet and try and make it more
interpretable so that's where I also saw a lot of the hype going on where because well decision
trees are more interpretable they you could obviously go and take a ResNet transform it into a decision
tree and have this great interpretability but in practice this doesn't really line up that well
and the reason is again kind of connected to this idea of decision trees making being small and
then progressively growing when neural networks are large and just basically large enough to fit
everything inside of them that means that the actual size of these neural network trees can
become rather gigantic the way we can do analysis with a theoretical lens is by studying something
called the VC dimension or the Vapnick-Schevenenkin's dimension which effectively just tells us how many
different points can network distinguish which of course for a decision tree if you have a fully
balanced tree like this one would be two to the power of the depth of the tree while for
a neural network it's a lot harder to figure out because you have all of these different
architectures what you can do though is we can go in we can bound this and there's been like lots
of work in trying to figure out bounds so for example the the best bound I could find is from
this paper from 2017 which provides nearly tight bounds and specifically they provide this kind
of theorem for a lower bound meaning what they basically shows there's some universal constant
which has this constraint so effectively the square of it has to be less than number of
weights you get a minimum amount of regions of resolution from a neural network of w so the
number of weights times l which is the depth of the network times the logarithm of w over l and
then you have this c constant in here so it effectively means the number of regions we have
scales a little bit more than linearly because we have this w in the log and it stays a little bit
less than linearly with the number of layers because we divide by l here so if we now take this
absolute lower bound so what we can say is because we divide by c here we can just set
w equals to so we can just set c square equal to the square root of w because that's kind of the
the worst case scenario it gives us the smallest bound and we can try to run this so I have here
this very trivial neural network which has one hidden layer we go from one to
one so like this or we can also look at something like 1024 to look at something that would happen
for example in a transformer we have these individual layers if we if we run this we get for this
relatively small network we get a depth of this full decision tree of about 16 if you
would try to plot this so this is now going to run for a very very long time I mean 16 it doesn't
seem that much but this is essentially an exponent this is an exponent so it is is a giant number
yeah we have two to the power 16 again I'm taking here that I'm rounding the depth down
two to the power 16 different regions which is going to crush most algorithms even if you could
build such a decision tree so actually build one it becomes rather hard to reason about them
simply because the reason neural networks are hard to interpret is not necessarily because
each individual component is hard to interpret it's because the emergent properties of putting
all of these things together and these billions of parameters or millions of parameters even
together that makes the problem hard yes and I was I was so just to say that this 16 depth
three that's kind of the best case scenario right that's that's our bound on what what would be
possible in order for transferring a neural network to like what's the minimum size of
tree we need to even represent that it could be the case that it's more but that was my my impression
as well is when I look at a decision tree I have sort of one path to go down to make the
decisions by right but if I look at a classification problem it's not always one path it's not just
you know is is the picture bright or dark well okay if it's dark is it this and this at some point
you get the same question right is the picture bright or dark yes is there a small or a large
object in it let's say this question you might want to ask whether it's light or dark so you
have like a matrix right light picture big object light picture small object dark picture and so on
and but these are represented by two different nodes in a decision tree no matter how you how
you structure it you have to ask one question first and the other question later and that means
one of these questions is necessarily going to be represented by two different
nodes in the decision tree and so that just for me looking at the decision tree I no longer
notice I no longer recognize or the algorithm doesn't anymore tell me that these two things are
actually related in some way so whereas in a neural network I have internal representation
I have features or weights that you know look at particular features inside of these representations
one set of the neural network might look at the lighting condition the other part of the neural
network may look at the shape of something and they can work in parallel in a decision tree
it's one after the other and therefore I'm no longer the analysis gets way harder because
stuff in the decision tree happens everywhere and it doesn't no algorithm can tell me by the way
these things represent the same feature it kind of boils down to this fundamental tension between
having parametric and non-parametric approaches because so for the people who don't know the
distinction here is effectively a neural network is a fixed skeleton with lots of blank spaces
and the objective of fitting to that known fitting the function in that neural network is
figuring out what should be put into its blank spaces this is a parametric approach because we
have lots of parameters decision trees are non-parametric approaches so what you do is
you effectively say we have this entire family of different trees which not only have parameters
like this w but also you have effectively the architecture which gets optimized along the
way and if you have non-parametric approaches this usually gives you way different classifiers
because in a parametric approach because we have stuff like gradients which make a lot of sense
in parametric approaches you can say something like I don't necessarily want an optimal split
I just want some split that effectively amounts to you go and you take this w and just move it
around a little bit to go to go closer to a good split where decision trees do it a lot
differently because this decision trees have to work with this gigantic family of functions
we now have to do optimal splits at least to some optimality constraint because you just
randomly kind of pull out decision trees and try to figure out is this the right decision tree
you're never going to be able to finish this is also why decision trees tend to work well in
stuff like tabular datasets because you have relatively few features which are very well
defined and you can compute the statistics for them which help you to figure out what would be
the perfect split for a specific feature and which feature should I split next while for something
like an image think about it you have an image which is 224 by 224 by three by three RGB channels
the statistics you can get even with a massive dataset are not that great especially since you
have to consider things like shifting around the image a little bit to basically make the statistics
more robust and that means you it's very hard to fit a decision tree because statistics are always bad
under network performs way better because it doesn't care about how well it split it just
does some split and hopes for the best this means that under network is by its nature going to be
less optimal but it's also going to make some progress even if they are only very bad statistics
where decision tree always has some sense of optimality if you fit it with something like
card because you only do somewhat optimal splits um of course at the cost of you have to have some
notion of what optimal means so you have to know so you need those statistics yeah and something
like this algorithm it is a decision tree so it's what one would call a simple function in
like mathematical speak so decision trees are effectively just nice representations of simple
functions but it's not really a decision tree as it would be produced by a decision tree algorithm
and that's the problem what makes them uninterpretable because they just grow without bounds these
neural network trees so when we look at let's get back to the to the paper at hand um by the way
this is still running which which i like uh back to the paper at hand is this is the proof sound
the proof that neural networks are decision trees right it's like it is it is absolutely
sound is not wrong or good is it new or unknown no so there are multiple things to that one is
there are already papers in the past which did that so for example this paper I think is from
1999 yeah November 1999 uh they also showed like um algorithm for extraction of decision trees from
artificial neural networks so this is known and it's also one of those things that often
happens to plop out as a corollary so there are very few people who go and explicitly write this
proof down because it's kind of a natural thing that occurs if you have some algorithm which splits
the world up into kind of classification polygons or simplex or simplices or affine regions which
for example this paper does then getting this decision tree form is effectively just a corollary
which just plops out passively so this paper here for example the spline theory of deep learning
paper could easily just say well yeah the decision of which spline we are in is made hierarchically
in the form of a decision tree so it would be a one sentence and that just plops out the same would
be true for many of these theoretical proofs where first of all very rarely do you actually need
this decision tree kind of realized but oftentimes the proof behind it that for example abuses the
fact that we have this really max function which effectively tells us to go either to the left
where you have the zero region or to the right where we have new values that is often just there
you don't need to do any more to get the actual decision tree out I also I know this from because
I used to work quite a bit in the field of adversarial examples and there I think it was made
oftentimes quite quite explicit to some degree because obviously people as long as stuff is
linear you could have some some kind of bounds on how worse it can get but then as soon as it's
non-linear it gets a bit more tricky and you've also shown me before like a paper on verification
of neural networks which is exactly right sort of in this area where people are trying to say well
how bad can it get and they use the fact that also there we have these essentially these these cells
of linearity so one of the problems that's also what this real complex algorithm the idea is that
you can view this max operation effectively as splitting everything up into a simplex
then you can make arguments about with something like an SMT solver you can try to make arguments
okay what happens inside the simplex or basically what can happen inside the neural network and
you can do that to guarantee some safety to have some safety guarantees but even this algorithm
gets crushed at scale and the scale as we've seen here I think it's still running yeah it
explodes rather quickly so and they of course don't explicitly build this but yeah this this idea of
neural networks mapping well to decision trees kind of boils down to the fact that a feed forward
network is effectively just a gigantic graph you can just take every you can effectively compute
the spanning tree of that graph and that gives you a decision tree at least in the case of a
relu and that's basically also what this paper does we compute the spanning tree by computing these
w hats these w hats take the slope from uh can get appropriate slope from the previous layer
and come and build up the appropriate w hats so maybe for for people so the if we can just go
to these formulas with one of the a's because that's kind of the crucial part of the math right here
is these a vectors uh and you have to like it still seems a bit like magic we have like the
non-linear composition of function and then all of a sudden we have these a vectors and somehow
now all is linear but one has to remember that so on the bottom here we have the non-linearity
that not essentially what I do is I take the signal that comes through the network at and I
look at the signal at the non-linearity and there I say well where is the signal such
that the relu is active and where is the signal such that the relu is inactive and I just replace
that by a vector of ones and zeros or or the the slopes and zeros right but these these vectors are
dependent on the signal and that's why the they're going to look different if the input is different
and that's why it's a linear function for a given input in a given very tiny circle right so that's
I think that's the connection now the paper also has some experimental result and there is a small
claim uh but there is a claim that the decision tree representation might be advantageous in a
computational manner so they have a table one comparing the decision tree and the neural networks
for the same function in terms of uh their computational complexity so it turns out the
decision trees have more parameters which is which is a odd for non-parametric function but
because they're not learned parameters yet the neural networks use more multiplications and
additions than the decision tree what do we make of that well computation often is not the same as
computation because you may have more multiplications or additions but they may be in a form which is
just nicer for uh for you to work with so for example if we look at the trees or like here or
let's go back up to the this kind of prototypical tree where effectively we have these uh these
multiplications with the uh with this x zero input what happens is that we do have fewer
multiplications using that structure because effectively we abuse the fact that we don't
have to compute the entire matrix we only have to compute the part which is actually going to
be relevant uh for us later on that of course reduces the amount of multiplications but on the
other hand we now have this spreading out we have more decisions in here and less multiplications
and depending how your uh how your hardware ends up it might be that basically paying for more
computation and having less decisions is better that's why training a decision tree on a cpu
makes more sense than training it on a gpu on the other hand there are also approaches which
take decision trees and basically compile them into well it's effectively binary matrix
multiplication these algorithms tend to of course for inference in that case but these algorithms
tend to be a lot faster simply because even though you do more addition and multiplication and stuff
like that you end up having so much parallelism that this what is it uh a factor of four roughly
is not that meaningful or it's closer to three well on the left it's eight but it's two versus 16
well in in any case but but that's that's the point right if if one were to actually implement
the decision tree on like a gpu one would actually regain all of these multiplications and additions
because it just makes more sense to put the binary vector there with a lot of zeros and then multiply
all of these zeros instead of trying to mask out stuff and uh because the gpu can just parallelize
so hard yeah it's supposed to that gpu's don't tend to do well with lots of decision making and
lots of sparsity yeah because just of the way they are designed they're designed to do large
operations on a lot of data very basically monotonically they they just do a large matrix
multiplication with very little decision making every single one of these thousands of course
effectively does exactly the same thing and that then gives you kind of this boost because
there are thousands of cores doing very simple very repetitive actions and if you have more
decision making about uh you have more decision making in there that just makes it slower i i
think i interviewed uh near shavit of neural magic and effectively they're they're doing something
very similar where they say okay what we do is we take like a bird or something like this we prune it
very uh in a in a in a special way such that the rest is something we can infer on cpu really well
uh which is essentially like very similar to to this paper right here so the idea of
sort of pruning it down and all of a sudden you may end up with something that sparse requires
more if else but then is very much suited to a cpu if we think about maybe the last question
for today if we think about okay this this this paper is is it's certainly correct and all but
we think it has it has been known or it's it's i don't like the word trivial uh because nothing
like i i i used to hate that as a student because to me nothing ever was super trivial and it's
even if it's trivial it's good that it's written down explicitly somewhere right you can point to
a place here but in a sense it is like something that a lot of people have just kind of done on the
side because it it is fairly like natural a natural outcome of working with uh these these systems
but if we look at a bit beyond that and say is there a a way in which decision trees can kind of
make a bit of a comeback in today's world of deep learning maybe not as a substitute but as an
augmentation of neural networks can be like what kind of properties does a problem need to have
such that a combination of something like decision tree algorithms like decision tree
learning algorithms and neural networks are the best so decision trees really like to have these
very very well defined statistics because that helps them to do their splits effectively neural
neural networks scale with gradients so if you can't get gradients you have a hard time and they
also scale with size simply because as we've seen here you just get more possible uh more
representational power so it's just better you can effectively simulate a small decision tree
inside a large number of neural network but just setting everything else for zero around it the
trick that makes decision trees work well is if you have these statistics so that's why decision
trees work incredibly well on something like tabular data like you can also tabular like deep
learning but that's probably like you're going to go you're going to do research you're going to do
probably a phd and then out plops a project which may or may not be competitive on tabular data
well in the other hand i can just use xj boost and get great results right now what you would want
to do to get decision trees to work well is you would want to take these very very high
dimensions very very information spars for example images and transport it into like a lower
dimensional space where you can then get the statistics so for example if we have a two-stage
approach where you have major networks uh inferring different features of the same thing so you first
try to classify whether it's a cat or a dog then you try to classify i don't know its size or whatever
and you put them all down then you can start doing a decision tree learning and the decision tree is
probably going to be a lot more performance simply because you get this smaller size through the fact
that the neural net that the decision tree is much more optimal in how it uses its splits in capacity
it seems like the current wave of self supervised learning might actually be a good candidate to
build something like this on top because the self supervised algorithm they tend they tend to sort
of extract many different kinds of features whereas like if i pre-train a classifier on image
net let's say that classifier is going to be attuned to very few features for the bunch of classes it
needs to classify but just from what i can observe the self supervised approach is they they just tend
to kind of get this rich representation out of images and we see that if you know we look at at
anything that uses a vq-gan encoder nowadays which is almost all of the ai art projects so they're
so rich such a rich representation so this this could be especially maybe the quantized stuff
could be like a very fertile ground to then put like decision trees random forests whatever on
top of that yeah cool all right i think that's that's about the paper is kind of really short
it's i guess four four or five pages if you if you if you you know it is it is very like i think
it's very approachable so um you know if you've never heard of any sort of equivalence like this
or or any math in this area it's very helpful i think to actually look at it and just see how it's
done um i give you a bit of an insight and yeah alexander thank you so much for being here it was
a pleasure thank you for having me cool and everyone if you want to hear more rants of alexander
myself uh we have discussions on discord almost every saturday evening well in at least evening
in europe right cool bye everyone bye
