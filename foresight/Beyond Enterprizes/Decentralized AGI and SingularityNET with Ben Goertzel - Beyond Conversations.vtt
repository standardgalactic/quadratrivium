WEBVTT

00:00.000 --> 00:02.000
You

00:30.000 --> 00:32.000
You

01:00.440 --> 01:02.440
You

01:14.600 --> 01:16.720
Welcome to beyond conversations

01:17.520 --> 01:20.720
Today's episode is very exciting

01:21.240 --> 01:24.480
it has to do with a hot topic that

01:24.680 --> 01:31.800
Has been brewing for years or decades even but it is now

01:32.560 --> 01:34.560
on the minds of

01:36.000 --> 01:39.160
Much more numerous group of people

01:40.280 --> 01:44.200
Really orders of magnitude potentially more than before

01:44.920 --> 01:51.680
Thanks to recent developments and it is the topic of artificial intelligence

01:52.240 --> 01:57.320
My name is David or Ben and our guest today is Ben Gertzel

01:57.680 --> 01:59.680
Ben is a cognitive scientist

02:00.200 --> 02:04.760
Artificial intelligence researcher. He's the cof and founder of

02:05.520 --> 02:07.440
singularity net a

02:07.440 --> 02:11.920
project that aims to democratize access to artificial intelligence and

02:12.720 --> 02:16.120
He actually helped popularize the term

02:16.840 --> 02:22.920
Artificial general intelligence which refers to the goal of creating machines that can perform

02:23.360 --> 02:26.400
Any intellectual task that humans can?

02:27.240 --> 02:30.040
Welcome Ben to beyond conversations

02:32.320 --> 02:35.520
Hey David, thank you. It's a pleasure to

02:36.240 --> 02:42.440
Go over these topics with you for what is a not the first time right you and I have been

02:43.160 --> 02:49.200
Digging into these issues. I mean presumably since way before we met each other

02:49.200 --> 02:53.600
I've been thinking about these things since the early 1970s, but we we must have been

02:54.520 --> 02:56.360
talking among

02:56.360 --> 03:01.400
Ourselves about this for 15 20 years or something quite quite quite a long time

03:02.840 --> 03:08.560
That's correct. I am I am proud to be your your friend and full disclosure

03:08.760 --> 03:12.960
I'm also the chairman of the singularity net

03:13.760 --> 03:14.880
supervisory

03:14.880 --> 03:17.960
Board we will have an event

03:18.520 --> 03:21.200
in just a couple of days

03:22.120 --> 03:27.720
talking about the next phase of the singularity net decentralization and

03:29.040 --> 03:32.320
That will be also an interesting opportunity to talk about

03:33.040 --> 03:38.120
Governance that is the reason why we are not going to maybe touch that

03:38.560 --> 03:40.560
as much today

03:40.720 --> 03:42.000
but

03:42.000 --> 03:44.200
Why don't you highlight maybe

03:45.320 --> 03:50.400
Some aspects and recent developments of the singularity net

03:51.160 --> 03:52.280
ecosystem

03:52.280 --> 03:54.520
Which is developing so

03:55.400 --> 03:57.400
rapidly and so broadly

03:57.760 --> 03:59.440
that I'm sure

03:59.440 --> 04:05.360
Just you know mentioning a few of the most recent components would be would be helpful

04:06.040 --> 04:08.040
Yeah, so I've

04:09.960 --> 04:14.400
Been thinking about the singularity and the the future and

04:15.840 --> 04:22.160
AGI before the term singularity or AGI became became became current right and and

04:23.120 --> 04:25.120
I've been

04:25.160 --> 04:27.160
working on trying to build

04:27.400 --> 04:31.240
Thinking machines or build toward the building of thinking machines

04:32.160 --> 04:39.200
Actively since the mid 1990s now as soon as the internet came about in

04:39.920 --> 04:46.600
The in the in the mid 90s and the dot-com boom in the late 90s. It was it was clear to me, you know a

04:47.560 --> 04:49.560
In one way or another

04:49.600 --> 04:53.920
the birth of the singularity and AGI and all this was likely to be

04:55.160 --> 05:00.640
Distributed widely across a bunch of different machines rather than being in just one

05:01.680 --> 05:07.640
Super computer sitting in some secret lab or in in somebody's basement. I mean not not that this

05:08.480 --> 05:10.480
physically centralized approach

05:10.760 --> 05:14.280
Was impossible, but it just didn't seem to be the way

05:15.120 --> 05:21.440
Things were going right like I mean, I'd known Danny Hillis. It was building the connection machine this massively parallel

05:22.080 --> 05:25.840
AI supercomputer and it was brilliant and fascinating

05:26.600 --> 05:34.400
But the tech world was obviously evolving in a in it in a different direction both in the physical infrastructure being distributed and

05:34.880 --> 05:38.000
the emergence of collaborative networks of people in

05:39.320 --> 05:45.200
Far-flung places working working together to bring things into being so actually I

05:46.840 --> 05:52.240
Started thinking in 99 2000 about how do you use strong encryption and

05:52.400 --> 05:54.400
and

05:54.680 --> 06:00.400
Distributed processing and how do you bring these together to create?

06:03.680 --> 06:08.520
Distribute decentralized AI networks without a single owner or controller almost some

06:09.080 --> 06:15.560
Emergent aspect to the intelligence in the in the network, you know, I ran into serious

06:16.560 --> 06:22.360
Practical obstacles at that time. I mean I was working with Java 1.1. It was just really

06:23.080 --> 06:27.080
Really slow machines didn't have the ram they do now networks fees weren't what they are now

06:27.080 --> 06:31.120
I I didn't hit upon the idea of starting with decentralized money

06:31.560 --> 06:37.640
Instead of decentralized AI if I had I would have been Satoshi, which unfortunately

06:37.640 --> 06:41.760
I'm not and my AI projects would be tremendously better

06:42.120 --> 06:49.760
Founded although we're not doing incredibly badly, but you know, it was clear then as soon as the internet came about it was really clear

06:50.720 --> 06:58.840
You needed decentralized control and then when if you fast-forward to 2015 I saw Ethereum came out and

07:00.120 --> 07:03.440
You had the notion of a smart contract which was clear to me immediately

07:04.200 --> 07:10.000
Was neither smart nor a contract. It's really sort of a persistent script sort of involved in a

07:10.800 --> 07:17.400
Decentralized validation and consensus mechanism and I hated the Solidity language from from the get-go just as a

07:18.040 --> 07:24.920
Computer scientist and from up from a programming language perspective, but it was clearly the beginning of something incredible, right?

07:24.920 --> 07:28.960
Like you were you were able to write these relatively simple

07:28.960 --> 07:34.360
I'll be ugly looking scripts that were sort of nodes in this global world's computer

07:34.360 --> 07:38.040
Which it would seem by putting the right thing in those scripts

07:38.040 --> 07:42.320
You could then make them nodes in the in the global brain, right? And this this was super exciting

07:43.200 --> 07:46.000
This led in 2017 to me founding

07:47.320 --> 07:49.320
Singularity net which was

07:50.440 --> 07:52.440
basically an attempt to make a

07:53.360 --> 07:55.080
platform for

07:55.080 --> 07:57.560
allowing multiple AI agents to

07:58.400 --> 08:01.040
cooperate and collaborate in in

08:02.040 --> 08:04.040
Flexible ways so from an AI view

08:05.040 --> 08:10.840
The idea is somewhat like what pioneer Marvin Minsky called the Society of Mind

08:10.840 --> 08:14.560
and I don't I'm not actually quite as far in that direction as Minsky, but

08:15.240 --> 08:17.240
His thinking was inspirational here

08:17.240 --> 08:22.280
He was Minsky was looking at a human mind or any mind in his view as a

08:22.880 --> 08:25.720
Society of smaller agents each carrying out particular

08:26.720 --> 08:34.860
Fragmentarily intelligent functions and then all cooperating together to achieve sort of emergent dynamics of intelligence now minceki

08:35.720 --> 08:42.880
means he didn't like nonlinear dynamics and complexity and chaos which perplexed me because there's a strong emergence aspect to the

08:43.480 --> 08:48.680
multi-agent systems that he himself was was putting forward he was a complex and

08:49.680 --> 08:55.400
Fascinating character unfortunately isn't with us anymore. I'd love to bring him back if post-singularity

08:56.360 --> 08:58.360
Techno technology

08:58.400 --> 09:03.760
Allows, but anyway with singularity net we basically created a decentralized blockchain based platform

09:04.600 --> 09:09.560
for multi-agent systems anyone in the world can put put an agent online they can

09:10.080 --> 09:16.920
Have that agent announced by the singularity net protocol. Hey, I'm here to all the other agents running the SNP protocol

09:16.920 --> 09:22.840
Then they can they can pay other agents and that work for services. They can outsource work for them

09:22.840 --> 09:26.280
They can cooperate together doing tasks. They can rate read each other's

09:27.640 --> 09:34.440
Reputations and we we rolled out a version of this platform a few years ago. It's still getting better and better. So I

09:35.280 --> 09:42.680
Mean we're pretty close to rolling out what we're calling the AI DSL domain specific language for a eyes, which is a special

09:43.680 --> 09:50.360
Programming language for the AIs to describe their properties and capabilities and preferences to each other

09:50.360 --> 09:56.520
Which is a key ingredient to allow automated assembly of different agents into into

09:57.480 --> 09:59.480
Collectives in

09:59.520 --> 10:02.720
In building this though, we realized there was a lot of other

10:03.960 --> 10:06.400
Missing pieces you need to get a whole decentralized

10:07.400 --> 10:10.400
Text stack which leads to some of the other

10:10.960 --> 10:13.320
Interesting things going on in singularity net

10:14.280 --> 10:16.560
Ecosystem right because sing singularity net

10:17.040 --> 10:23.240
We have we have the core protocol and platform built by singularity net foundation and an open source community

10:23.800 --> 10:25.800
Then we have a number of different

10:26.960 --> 10:31.560
Entities which are part of the ecosystem that are building stuff using this platform or

10:32.400 --> 10:34.400
building sort of foundational tools

10:35.360 --> 10:43.280
Complementary to the core singularity net platform forming part of sort of decentralized tool stack and that that gets into new net which allows

10:43.800 --> 10:45.320
decentralization of

10:45.320 --> 10:50.280
Processing power and tokenization of processing power in a flexible way suitable for AI

10:51.040 --> 10:53.040
then the open cog

10:53.800 --> 10:55.800
hyperon toolkit which is our

10:57.560 --> 11:02.480
Probably main thrust in terms of how to actually build a GI on this decentralized

11:03.360 --> 11:07.880
Platform and you can build a GI in any way you want on singularity that platform

11:08.240 --> 11:12.320
Hyperon is our particular sort of neural symbolic evolutionary approach

11:12.320 --> 11:18.920
Which we'll talk about more in a few minutes when we get into a lot of lans and stuff and then then there's hypercycle

11:19.400 --> 11:21.400
which is a

11:21.880 --> 11:25.520
collaboration with two fee saliva and dense toliver and folks from the

11:26.240 --> 11:28.240
Tota ecosystem and

11:28.680 --> 11:29.760
hypercycle

11:29.760 --> 11:35.360
Like new net and hypercycle both live in a way below singularity net in the tech stack

11:35.360 --> 11:38.000
Right new net tokenizes the processing power itself

11:38.280 --> 11:45.760
So if you can contribute processing power for tokens and receive tokens for processing power and that process power can then run singularity

11:45.760 --> 11:47.280
agents or other things

11:47.280 --> 11:50.000
hypercycle is a layer one blockchain

11:50.880 --> 11:52.880
customized for AI and

11:53.120 --> 11:55.120
You can run singular on

11:55.680 --> 12:01.800
Ethereum we can we're most of the way there to making able to run it on Cardano as an alternative and Cardano

12:01.800 --> 12:06.320
We found more efficient and scalable than Ethereum for AI purposes hypercycle

12:07.320 --> 12:15.560
Customized blockchain just for for AI will let singularity net decentralized AI agents run in a much faster and and more scalable way

12:15.560 --> 12:17.560
And I mean hypercycle is designed

12:18.760 --> 12:22.360
To work very closely with Cardano in particular

12:22.760 --> 12:24.920
But it's a different design we get rid of the ledger

12:25.080 --> 12:29.560
So that's probably more than we can go into in depth in this in this call

12:29.560 --> 12:37.320
But if you look at Ethereum Bitcoin and so on they're all based on this replicated ledger that stores all transactions have ever happened in the blockchain

12:37.320 --> 12:39.320
Right so in in hypercycle

12:39.760 --> 12:47.680
We get rid of this ledger and store transactions sort of fractionated all throughout the network in a more localized way

12:47.720 --> 12:53.680
And this this lets you have a much more efficient blockchain underpinning for for

12:54.640 --> 13:00.880
Decentralized AI so we we've been working hard on all these aspects of a decentralized infrastructure

13:01.120 --> 13:08.120
And now we're feeling a need to like move way way way faster even than we then we've been doing because we see like the

13:08.640 --> 13:10.640
Centralized tech be a moths

13:10.880 --> 13:17.400
Well, I don't think they have the golden key to a GI the centralized tech be a moths are certainly getting

13:18.160 --> 13:20.160
extremely interesting AI

13:20.440 --> 13:22.440
functionalities

13:26.760 --> 13:34.400
Thank you for that that introduction to the singularity net ecosystem and let's now as you mentioned jump over to LLM's

13:35.040 --> 13:38.920
large language models a lot of people in

13:40.080 --> 13:41.600
like

13:41.600 --> 13:43.600
moving goal posts and

13:44.600 --> 13:48.760
Always defining AI by the things it cannot do

13:50.440 --> 13:53.640
When it couldn't play chess very well

13:54.760 --> 13:56.760
People would say oh AI is

13:57.600 --> 14:02.440
Playing chess and then when it could beat every human

14:03.440 --> 14:07.660
Suddenly it it wasn't chess anymore. Now. It seems to be

14:08.600 --> 14:10.600
natural language processing

14:10.880 --> 14:13.380
and chat GPT which

14:13.760 --> 14:18.320
Now everyone knows I am sure everyone following this

14:19.360 --> 14:21.360
video stream

14:22.440 --> 14:26.480
Is able to manage text in a manner

14:27.320 --> 14:29.320
that I would say

14:29.440 --> 14:33.400
many humans cannot do it creates a

14:34.080 --> 14:36.080
text that is

14:36.760 --> 14:38.080
competent

14:38.080 --> 14:41.560
in many many different fields now a

14:41.840 --> 14:43.000
a

14:43.000 --> 14:48.000
That was sufficiently surprising including two experts

14:49.000 --> 14:50.680
to

14:50.680 --> 14:52.680
generate a

14:53.360 --> 14:55.360
Particular alarm

14:55.560 --> 14:58.800
Not about what it can do now, but it

14:59.520 --> 15:05.200
What may be able to do in in the near future and there was this open letter

15:06.200 --> 15:07.600
A

15:07.600 --> 15:09.600
Formulated by the future of life

15:10.160 --> 15:17.840
Institute at MIT and signed by a bunch of people and not signed by another bunch of people and by the way

15:18.200 --> 15:23.400
We belong to those two different camps. I did sign the letter and you didn't and

15:24.440 --> 15:29.640
I feel that these these agreements are very very healthy

15:30.640 --> 15:38.880
Especially in a decentralized and distributed world if we were in complete agreement and alignment about everything

15:39.320 --> 15:41.320
Then there would be no reason

15:42.000 --> 15:47.560
To experiment with different approaches and we would just be in a single

15:48.240 --> 15:50.240
hierarchical centralized field

15:50.560 --> 15:52.560
so a

15:52.920 --> 15:59.440
The open letter has been interpreted and in misinterpreted in many ways

16:00.040 --> 16:01.440
but

16:01.440 --> 16:03.440
very simply it says

16:04.600 --> 16:07.040
Let's in my opinion it says

16:08.200 --> 16:12.360
Let's give a little breeding space

16:13.680 --> 16:14.840
for

16:14.840 --> 16:17.280
everyone to understand what is going on

16:18.800 --> 16:20.800
especially

16:21.200 --> 16:28.880
To those who are in charge at these large tech companies and the regulators

16:29.680 --> 16:31.000
who

16:31.000 --> 16:38.000
Should be moving faster and should be looking at what these tech companies are doing

16:38.640 --> 16:44.200
Six months, which is the pause of the open letter is not going to be sufficient for sure

16:45.440 --> 16:51.440
But at least it raised the conversation about this need so

16:52.640 --> 16:54.640
Two questions to you

16:54.920 --> 16:56.920
one

16:57.240 --> 16:59.240
What

16:59.800 --> 17:01.800
Makes you feel that

17:02.760 --> 17:04.760
This is

17:04.920 --> 17:06.920
Not necessary or not useful

17:07.800 --> 17:09.000
and

17:09.000 --> 17:10.680
to

17:10.680 --> 17:16.240
What are the things in your opinion that are missing from the large language models?

17:17.200 --> 17:19.200
that

17:19.320 --> 17:20.960
would

17:20.960 --> 17:22.960
require the kind of

17:23.760 --> 17:25.760
careful

17:26.760 --> 17:29.720
Steps that the open letter advocates

17:35.680 --> 17:40.440
So I think as the singularity approaches

17:42.440 --> 17:44.800
And I think it is approaching I

17:46.160 --> 17:52.520
Still see Ray Kurzweil's prediction of human level AI in 2029 as a reasonable stab

17:52.520 --> 17:56.760
I mean, I think we may get there a few years in advance of that

17:57.400 --> 18:01.800
I'm pushing for that. It may take a few years longer, but I don't think it's going to be

18:03.280 --> 18:04.760
Many decades longer

18:04.760 --> 18:09.680
I mean if it turns out somehow human level intelligence requires

18:10.200 --> 18:16.560
You know advanced quantum computing or quantum gravity super computing maybe that could make it take a few decades longer

18:16.560 --> 18:20.440
I give that a not zero but very low percentage

18:20.680 --> 18:25.600
Chance based on everything we know about the brain and everything we can do with with computers now

18:25.880 --> 18:30.000
so if that's true that the singularity is coming in

18:31.120 --> 18:35.160
Five to ten years three to ten years. Whatever at least

18:35.760 --> 18:41.960
Human level AI coming then which will certainly accelerate the next stages of progress towards super superhuman AI

18:42.360 --> 18:44.360
there's going to be a lot of

18:44.360 --> 18:46.360
crazy things going down a

18:46.640 --> 18:47.800
lot

18:47.800 --> 18:52.960
More impressive and a lot more disturbing to some people then then chat GBT which in the end is a

18:53.760 --> 18:57.800
Rather banal and only half useful chatbot

18:57.840 --> 19:06.600
Although very very very impressive in in many ways compared to what what came before right and I think in in dealing with these sorts of

19:07.560 --> 19:08.840
unprecedented

19:08.840 --> 19:10.840
crazy seeming situations

19:12.760 --> 19:14.760
Some basic

19:15.760 --> 19:23.000
Principled thinking is is helpful. Otherwise when we just swung around like crazy by the wild weird things that are

19:23.400 --> 19:30.480
That that that are happening and so max more who we both know who's been one of the leading philosophers of transhumanism

19:31.680 --> 19:33.680
Articulate some time ago what he called the

19:34.720 --> 19:42.040
Proactionary principle which was intended as a contrast to the precautionary principle that others had talked about and the

19:42.680 --> 19:44.680
precautionary principle is basically

19:44.760 --> 19:48.080
Better safe than sorry hold off on new developments

19:48.520 --> 19:53.280
You know until you know for sure they're not going to cause harm and this

19:53.760 --> 19:59.120
This was the principle of Australian indigenous society for which allowed them to be

20:00.000 --> 20:01.440
fairly

20:01.440 --> 20:08.240
constant and stable for 60,000 years and they had they developed many things of great beauty in this in this stable

20:08.960 --> 20:14.360
Society and develop their consciousness and dimensions that are not that common in our modern world

20:14.760 --> 20:20.680
It's been a principle in Chinese history. Certainly. I mean if you look in the 12th 13th century China had

20:21.120 --> 20:26.200
quite advanced technology all sorts of machinery and so forth and then they they sent

20:26.480 --> 20:28.880
sailing ships all over the world and then they decided to

20:29.560 --> 20:34.000
Stop that because it was being disruptive and they didn't know where it was going to leave right now

20:35.800 --> 20:42.120
Modern Western society has generally not proceeded on on this sort of basis, which is really

20:42.640 --> 20:51.280
Why we've developed computers and AI and the internet and modern medicine and gone to the moon and all this stuff, right? We have

20:52.400 --> 20:54.400
Implicitly and with a lot of argumentation

20:56.440 --> 21:01.520
Along the way, we've largely followed what max called the

21:03.440 --> 21:06.320
Proactionary principle which basically means

21:07.320 --> 21:14.720
You know get have a bias toward doing stuff and not making prohibitions and a buy bias toward letting processes

21:15.800 --> 21:19.840
Unfold in a in a natural way without trying to blockade them

21:19.840 --> 21:25.840
It doesn't mean there's nothing that you should ever prohibit and and and try to stop

21:25.840 --> 21:33.840
I mean, I'm not advocating selling briefcase nuclear weapons in Walmart, right? But it it means that when when there is

21:34.840 --> 21:45.440
Complex technologies complex situations with you know richly nuanced and balanced pluses and and and minuses and

21:46.320 --> 21:51.480
It's hard to understand the implications for good or for bad in a situation like that

21:52.280 --> 21:54.920
the proactionary principle would argue

21:56.800 --> 21:59.920
Just do it and you know keep an eye out

22:00.480 --> 22:06.280
Watch watch to be sure. You're not doing anything extraordinarily dangerous, but don't don't be

22:07.280 --> 22:13.560
Paranoid don't be a wimp and you know look at look at the the cost of not doing something along with the costs of

22:14.680 --> 22:18.080
Doing that thing and so justifying

22:19.040 --> 22:21.040
Proaction versus precaution

22:21.280 --> 22:25.520
on a sort of historical societal and philosophical level is

22:26.000 --> 22:29.840
Very interesting and important would be would be a long long and deep

22:31.200 --> 22:39.280
Conversation which gets into epistemology and metaphysics and all sorts of things, but I wanted to mention that because

22:40.160 --> 22:43.840
In general, that's where I'm coming from and I think that attitude

22:44.560 --> 22:48.160
Is why we have advanced technology. That's why we have

22:49.040 --> 22:54.320
Modern dentistry and and vaccines and then computers and so forth

22:54.400 --> 22:56.560
I mean this is why this is why we have

22:57.200 --> 22:59.200
developed weird new technologies

22:59.760 --> 23:04.800
Which in every case we didn't know what would be the good or bad aspects and how they would be

23:05.360 --> 23:08.240
They would be balanced, right? So I do think they're

23:09.760 --> 23:11.760
They're cases

23:12.080 --> 23:14.480
Where you don't want to be proactive no matter how

23:15.280 --> 23:21.040
Gonzo for the for the the future you are. I mean briefcase nukes is one

23:21.840 --> 23:23.840
gain of function research on

23:24.960 --> 23:31.920
Viruses it can infect humans is certainly an interesting borderline case which society chose to proceed with

23:32.880 --> 23:34.800
anyway in spite of

23:34.800 --> 23:40.720
Some pretty obvious risks to do to do with the leakage from from from labs and so forth

23:41.120 --> 23:43.440
But the thing is with with these technologies

23:44.400 --> 23:46.400
nuclear weapons

23:46.720 --> 23:50.640
Have very few uses besides blowing people on their possessions up

23:50.640 --> 23:54.240
I mean you can use them to blast starships as as freeman dyson

23:54.720 --> 24:01.760
Suggested but by and large they're considered a weapons technology. I'm you know taking animal viruses and

24:02.320 --> 24:07.840
Uplifting them so they can kill humans better again. There's not a lot of immediate

24:09.280 --> 24:17.120
Humanitarian uses for that. I mean the the the the purpose is to better understand them in case they develop later in

24:17.600 --> 24:22.720
In nature or something, but I mean AI is different than that, right? There's tremendous

24:23.760 --> 24:28.160
obvious positive benefit of these technologies and so then

24:29.040 --> 24:35.280
Bouncing the cost of not building the attack against the cost of doing it is complex. But what this also means is that

24:36.480 --> 24:38.480
If you try a prohibition

24:38.880 --> 24:44.480
Many many actors who are not particularly bad actors are going to wangle the way around the prohibition

24:45.200 --> 24:50.880
Anyway, I mean the only reason to what to wangle around a prohibition on briefcase nukes

24:51.360 --> 24:57.520
Is pretty much if you want to blow people up or you're a really really really ethics free conniving business person who

24:57.760 --> 25:02.560
Wants to make a living selling briefcase nukes to other jerks who do want to blow people up, right? But

25:03.200 --> 25:06.160
reasons to keep developing AI are just

25:07.280 --> 25:14.240
Extremely numerous beyond wanting to do any anything bad. So I think there there are two things that are sort of mixed up here

25:14.480 --> 25:19.040
And they're mixed up in a very natural way. So what what what what one one thing is

25:20.240 --> 25:22.720
I think this sort of prohibition is

25:23.280 --> 25:28.640
Ethically very very dubious because balancing the good and the bad of this sort of technology is very very hard

25:29.280 --> 25:30.880
The other thing is

25:30.880 --> 25:34.400
This sort of prohibition will never work and is completely

25:35.040 --> 25:42.000
Impossible in practice and I think these are tied together because when the good and bad are all tangled up and complex and hard to assess

25:42.960 --> 25:47.280
You know this kind of prohibition is going to be way way hard to put into place because

25:48.480 --> 25:51.920
So many parties won't won't go along

25:52.640 --> 25:57.360
Along with it any anyway, right and you can you can see both the aspects with this proposed

25:58.400 --> 26:02.880
Six-month pause. I mean, I think chat gpt l lm's and so forth are not

26:04.080 --> 26:06.080
general intelligences

26:06.480 --> 26:08.480
they're not going to be sort of

26:09.200 --> 26:14.240
Incrementally morphable into general intelligences just by tweaking the architecture a little bit

26:14.800 --> 26:18.640
I do think they could serve as components of general intelligences

26:19.120 --> 26:21.120
And whether the llm component

26:21.440 --> 26:25.280
Is 60 percent or 20 percent of the final general intelligence

26:26.000 --> 26:30.720
I'm I'm more toward the 20 percent side, but I think I think there's still a very interesting

26:31.360 --> 26:37.280
Ingredient in in agi systems that that that are going to be built, but they're not agis yet

26:37.360 --> 26:40.560
They're software systems. They're very interesting software systems

26:41.280 --> 26:48.960
I'm engaged with a few people building amazing commercial applications on top of llm's. I'm also engaged with some research

26:49.920 --> 26:55.760
Trying to couple llm's with logical reasoning systems to make them make more sense and carry out multi-step reasoning

26:56.880 --> 26:59.680
So that can they be used for unpleasant things?

27:00.320 --> 27:04.560
Of of course they can be used for unpleasant things. I mean they can generate a lot of

27:05.200 --> 27:08.320
Nonsense so people can be tricked to believe it's not nonsense

27:09.600 --> 27:12.160
I mean if we're going to ban things that people can use

27:12.800 --> 27:17.600
To fool people and spread bullshit. We're going to ban the whole internet and mobile phones very

27:18.240 --> 27:22.480
Very rapidly and these also have complex pluses and minuses that we don't we don't know

27:23.200 --> 27:28.080
We don't know how to balance. So I just think there there's clear potential for tremendous good from these

27:28.400 --> 27:32.320
There's potential that they serve an ingredient of agis that they can do even more good

27:32.640 --> 27:34.640
There's also ways people can do

27:34.800 --> 27:37.120
Harm with them. I see this is not different than

27:37.680 --> 27:44.080
So many other technologies out there and the the practicality of a pause is a whole other thing because if you if you

27:45.200 --> 27:50.480
If such a pause were really adopted on paper, which is obviously not going to happen anyway

27:50.560 --> 27:56.320
So I think signing the petition is mostly virtue signaling rather than anyone thinking the pause was really going to happen

27:56.400 --> 28:00.240
But I mean if if such a pause really did happen

28:01.040 --> 28:05.120
I mean what would happen first of all Putin and Xi Jinping aren't going to do that pause

28:05.440 --> 28:11.200
Even if they said they were they're not going to china can't even debate the world trade organization sort of basic

28:11.680 --> 28:19.440
Ethics about not stealing other people's ip right so on the other hand big tech companies like at google and microsoft and so on

28:19.440 --> 28:23.120
They're not really going to pause either. They're just going to develop a i

28:23.760 --> 28:29.280
Internally and use it inside the products in the in a quieter way rather than making it available to the consumer

28:29.280 --> 28:35.040
So if such a pause were adopted, which it won't be that would result in a combination of

28:35.520 --> 28:37.520
nasty dictators

28:37.600 --> 28:39.600
gaining in ai over the

28:40.080 --> 28:43.120
Democratic world and it would result in greater

28:43.680 --> 28:52.560
Centralization of ai within the developed world as big big tech companies take their lm projects black and don't open them up to the

28:52.640 --> 28:54.880
to the consumer so I think

28:55.840 --> 29:02.320
But I think almost everyone can see this and they just want to sign it so they could look like they're ethical and

29:03.200 --> 29:05.200
Concerned right because because that

29:05.200 --> 29:07.200
well

29:07.680 --> 29:09.680
We have a lot of questions from

29:10.560 --> 29:12.320
the people following

29:12.320 --> 29:14.320
The live and I want to

29:14.320 --> 29:16.640
Tackle some of them and they are

29:17.520 --> 29:23.280
Touching these these topics, but let's start with a different question from Rohit

29:24.240 --> 29:26.240
What is the picture behind the wall?

29:27.360 --> 29:29.360
On the wall behind you

29:30.480 --> 29:33.840
Oh that that orange picture. I think that's yes

29:34.240 --> 29:40.560
That is the that is the album cover of access oldest love by by jimmy hendrix. So go

29:41.120 --> 29:43.120
All right

29:43.600 --> 29:47.200
Listen to access boldest love one of the great great albums in music history

29:48.400 --> 29:50.720
All right, Rohit. So you can check it out

29:51.440 --> 29:52.720
and

29:52.800 --> 29:54.960
David is asking

29:54.960 --> 29:58.880
Why is the mainstream pushing the ai is going to kill us all narrative?

29:59.920 --> 30:01.920
And then he proposes

30:02.160 --> 30:06.640
a benign answer saying maybe to create a common alien enemy

30:07.520 --> 30:08.480
to

30:08.480 --> 30:13.040
unite us and the and the wars between nations

30:13.520 --> 30:14.400
so

30:14.400 --> 30:15.520
to to

30:15.520 --> 30:17.520
answer david in pot

30:17.520 --> 30:18.800
in

30:18.800 --> 30:22.400
ai deserves a lot of conversations and

30:23.280 --> 30:25.040
I hope

30:25.040 --> 30:27.440
more would happen especially

30:29.280 --> 30:35.200
Illustrating the benefits of of ai like ben said in a in a pro-actionary

30:36.160 --> 30:38.160
attitude

30:38.400 --> 30:45.440
And the mainstream media is driven by fear is driven by the the the worst possible narrative

30:45.840 --> 30:50.000
That attracts attention. So it is natural that they would

30:50.720 --> 30:52.560
um

30:52.560 --> 30:54.320
Push push that

30:54.320 --> 30:56.320
in on on the other hand

30:57.120 --> 30:58.400
it is

30:58.400 --> 31:00.400
definitely the case

31:00.560 --> 31:02.320
that

31:02.320 --> 31:04.320
There are

31:04.320 --> 31:08.000
Approaches that we have taken to iteratively

31:09.200 --> 31:11.680
Develop solutions. For example

31:13.280 --> 31:19.120
The the SpaceX rockets that are now landing regularly

31:20.160 --> 31:22.160
Belonged to science fiction

31:22.560 --> 31:24.880
And nasa or the russians

31:25.440 --> 31:27.600
Or the chinese or the europeans

31:28.160 --> 31:30.160
Would have never developed them

31:30.800 --> 31:31.520
if

31:31.520 --> 31:37.920
Crazial and musk didn't come around and said i don't care that everyone says it's impossible

31:38.320 --> 31:41.200
I will just blow them up until we don't succeed

31:42.320 --> 31:45.040
And so that is where the alignment

31:46.000 --> 31:47.920
issue comes up

31:47.920 --> 31:49.920
because

31:50.080 --> 31:52.640
There are people who believe that

31:53.840 --> 31:56.640
Not current agi, but future agi

31:57.760 --> 31:59.760
can

32:00.320 --> 32:02.320
Become agente

32:02.640 --> 32:06.720
It can design and then execute goals of its own

32:07.840 --> 32:10.080
And it can slowly

32:10.800 --> 32:12.640
Diverge

32:12.640 --> 32:19.360
And then the side effects of whatever it does can have very big effects on us

32:20.640 --> 32:22.640
and both

32:22.800 --> 32:25.440
The speed of its divergence and

32:26.960 --> 32:34.320
The size of these effects can be such to represent an existential threat to human civilization

32:35.520 --> 32:39.600
Without us being able to say after the fact

32:40.240 --> 32:47.120
Oh, sorry, we destroyed the current one. Let's bring a new human civilization in its place and iteratively

32:48.080 --> 32:50.080
Find what works?

32:50.400 --> 32:51.600
so

32:51.600 --> 32:53.600
Again two questions

32:54.640 --> 32:58.320
Do you believe this framing of the alignment problem?

32:59.200 --> 33:00.240
is

33:00.240 --> 33:02.240
Worth considering now

33:02.880 --> 33:05.120
Given that by your own assessment

33:06.960 --> 33:08.960
The singularity

33:08.960 --> 33:12.640
May be as close as five years from now

33:13.440 --> 33:15.440
and the second question is

33:16.240 --> 33:17.360
If

33:17.360 --> 33:21.360
We don't have a current solution to the alignment problem

33:21.840 --> 33:29.440
What are the paths that we need to take in order to find those solutions so that we have them when we need them?

33:31.680 --> 33:36.960
Good good questions. Yeah, I want to I forgot to say one thing about the pause

33:37.040 --> 33:41.440
Which I want to address briefly then I'll go to this more bigger and deeper more interesting question

33:41.520 --> 33:47.680
I think another strange thing about the pause petition is it focused on training larger and larger language models

33:47.840 --> 33:51.280
so so one one thing that was ironic when I saw that is well actually

33:53.040 --> 33:59.040
If people really stopped training larger language models and started working on more productive approaches to agi

33:59.520 --> 34:07.440
Then that might actually have the opposite impact of what people signed the petition thought that might accelerate the path toward agi

34:07.840 --> 34:11.760
Sam Altman actually made the same point in recent remarks at mit. He's like well

34:12.480 --> 34:19.760
This lacks technical nuance because what we're doing at open ai now is not so much just obsessing on bigger models

34:20.320 --> 34:24.480
Anyway, I mean they're integrating gbd forward with microsoft knowledge graph

34:24.560 --> 34:26.560
They're working on a lot of other

34:26.640 --> 34:32.240
Other things besides just bigger and bigger models. So this is a difficulty of prohibitions on complex things

34:32.240 --> 34:36.960
Like you you're going to say no no language model with more than you know

34:37.920 --> 34:43.600
One trillion parameters. What if someone makes a complex valued parameter which equals two real valued

34:44.160 --> 34:51.200
Parameters, right? I mean what what if you have five different models with half a trillion parameters living in different places that that that

34:51.840 --> 34:55.600
Co-operate with with with each other. I mean it's these things are too

34:56.480 --> 34:57.600
slippery

34:57.600 --> 35:03.760
Inter the same reason the software patents are much harder than biotech or nuclear energy patents or something, right?

35:04.080 --> 35:06.800
There's slippery in there in their definition

35:07.360 --> 35:13.840
As well as slippery and balancing good and bad aspects in the in in the short term and I think

35:16.080 --> 35:19.360
Yeah going to the align alignment question now

35:21.280 --> 35:23.280
I I don't

35:23.280 --> 35:25.280
really like

35:25.680 --> 35:30.160
The word alignment that that's used in in discussing the

35:31.440 --> 35:33.120
future

35:33.120 --> 35:35.680
coordination of ai mines and

35:36.800 --> 35:37.920
and

35:37.920 --> 35:40.400
human minds, I mean I understand what people are

35:41.280 --> 35:47.680
Getting at when they talk about ai alignment. I don't want to nitpick on on word choice in too much of a

35:48.960 --> 35:50.960
Ridiculous way, but to me

35:51.440 --> 35:56.400
There's nothing to align with because human

35:58.880 --> 36:01.760
Morals human ethics human ideals human ideas

36:02.800 --> 36:04.160
are

36:04.160 --> 36:07.840
Intercontradictory and kind of scattered all over the place and they're also

36:08.880 --> 36:10.240
rapidly

36:10.240 --> 36:15.120
Devolving right so I mean if you look if you look in at, you know, indigenous

36:16.400 --> 36:18.400
Australians at

36:19.360 --> 36:21.360
You know

36:21.920 --> 36:27.440
Asians at at the average Ugandan subsistence farmer at

36:28.240 --> 36:29.680
you know

36:29.680 --> 36:36.720
Islamic people in in rural Saudi Arabia and then look at people in the tech hubs in the US today

36:37.760 --> 36:40.080
Right now. There's tremendously different views on

36:40.880 --> 36:42.880
so many things and

36:42.880 --> 36:44.880
if you roll back

36:45.120 --> 36:46.880
50 100 years

36:46.880 --> 36:52.960
There were divergent views among these different groups, but also each of those groups have a different view than they then they have now

36:53.040 --> 36:55.440
where like, I mean when I when I was growing up

36:56.320 --> 36:57.760
in

36:57.760 --> 37:00.560
New Jersey in the 1970s in part of my childhood

37:00.960 --> 37:05.760
I mean you would get the crap beaten out of you for being publicly gay or even being suspected of it

37:06.320 --> 37:11.920
Now that's not true in New Jersey anymore, but you can get killed for being gay in many parts of sub Saharan Africa, right?

37:11.920 --> 37:14.960
So there there's there's not a coherent thing

37:16.640 --> 37:23.520
To align with at any level of precision and the way humanity thinks about what's good and right

37:24.240 --> 37:26.240
50 years from now post-singularity

37:27.120 --> 37:30.080
Is going to bear only a loose resemblance to how we feel

37:31.840 --> 37:37.440
Now, I mean, I think some things that we take very much for granted now or like a

37:38.320 --> 37:42.160
Focus on our biological family, right? I mean, that's a big thing for me

37:42.480 --> 37:49.040
Five children and a granddaughter. I live near my mother who's almost 80. That's a big thing if you're

37:49.920 --> 37:53.360
We're post-singularity people aging and death is is gone

37:53.840 --> 38:00.640
Reproduction doesn't happen to happen have to happen in the traditional biological way, although it may still sometimes

38:00.720 --> 38:02.720
I mean the the whole role that

38:03.200 --> 38:04.800
family has

38:04.880 --> 38:13.120
In psychology may be quite different 50 years from now than then then now and this may sound very disturbing

38:14.160 --> 38:16.480
To some people now. I mean just as

38:17.120 --> 38:22.320
You know gay people were very disturbing to the people I knew in New Jersey in the in the in the 70s, right?

38:22.400 --> 38:26.560
So I mean things evolve. So then we're not saying take

38:27.680 --> 38:29.680
you know the world view of

38:30.560 --> 38:31.840
San Francisco

38:31.920 --> 38:33.920
Silicon Valley tech bros in

38:34.400 --> 38:41.840
2023 and make future agis always aligned with a world view of 2023 silicon valley tech bros, right like that

38:42.240 --> 38:45.920
That's not what we want any more than we want to forever be aligned with

38:46.240 --> 38:50.480
You know the the elite in Rome in 1521 or something, right?

38:50.480 --> 38:57.840
So what what you want is somehow for the the chaotic self contradictory evolving system of human values to

38:58.480 --> 38:59.680
be

38:59.680 --> 39:04.160
Coordinated and co-evolving somehow with the evolving set of of

39:04.960 --> 39:09.840
AI and transhuman and synthetic organism values and that that's

39:10.400 --> 39:16.080
That's that's an interesting thing to think about. It doesn't seem like the kind of thing that can be

39:17.200 --> 39:23.920
Guaranteed in any way nor the kind of thing that you can pre figure and and design in a in a detailed way

39:24.000 --> 39:28.960
We're talking about the coupling of two pieces

39:29.520 --> 39:34.160
Two subsystems of a complex self-organizing system, which by its nature is

39:35.200 --> 39:37.200
Going to be evolving in ways we cannot now

39:38.480 --> 39:45.760
Understand so I mean you have people talking about we want an AI that will provably never deviate from its initial goals

39:45.760 --> 39:49.600
I'm like shit. What if what if the pope had created an AI in 1400?

39:49.840 --> 39:54.880
That would provably never deviate from the goals the pope gave it and it was super powerful

39:54.960 --> 39:56.960
And it would we'd all be forced to be

39:56.960 --> 40:01.360
Catholics at risk of being blown up by AI drones coming out of the Vatican or something, right?

40:01.600 --> 40:02.800
I mean

40:02.800 --> 40:07.200
That that would be a tremendously horrible thing to do to a superhuman mind

40:07.680 --> 40:14.400
If you could somehow wire its brain to never deviate from the goals of 2023 silicon valley tech bros

40:15.600 --> 40:17.600
Fortunately, there's no way to really do that anyway

40:18.400 --> 40:20.400
So

40:22.720 --> 40:24.720
I

40:24.720 --> 40:26.720
cherish our diversity

40:27.360 --> 40:29.360
and we have

40:29.360 --> 40:31.360
the ability to

40:31.440 --> 40:34.880
Aggregate and we have built societies that

40:35.920 --> 40:38.960
either accept develop or impose

40:39.840 --> 40:43.520
a certain set of consistent views and behaviors

40:44.240 --> 40:48.800
And and today as a human civilization we brought this at

40:50.160 --> 40:53.760
Roughly speaking continental level, right?

40:55.440 --> 40:57.840
There are different kinds of

41:00.560 --> 41:04.000
Expectations of of behavior from humans

41:05.360 --> 41:07.860
within societies and across

41:08.480 --> 41:13.040
societies as well, but we more or less subscribe to

41:14.240 --> 41:16.240
certain sets

41:16.400 --> 41:21.040
of behaviors and expectations around those behaviors

41:22.560 --> 41:23.840
So

41:23.840 --> 41:26.720
The opportunity I think is huge

41:27.920 --> 41:29.440
for

41:29.440 --> 41:31.440
singularity net

41:31.760 --> 41:33.760
to find

41:33.760 --> 41:36.000
a way for the decentralized

41:37.120 --> 41:39.120
AI and agi systems

41:40.080 --> 41:42.080
to

41:42.640 --> 41:44.320
navigate

41:44.320 --> 41:47.200
The complex sets of behaviors

41:48.160 --> 41:49.680
across

41:49.680 --> 41:51.520
a parameter

41:51.520 --> 41:57.520
space that is vastly larger than not what humanity has been able to explore

41:58.320 --> 41:59.840
today

41:59.840 --> 42:01.840
to find

42:01.840 --> 42:03.840
consistent and coherent

42:04.480 --> 42:05.840
a

42:05.840 --> 42:07.520
niches

42:07.520 --> 42:09.680
that can build

42:10.560 --> 42:12.560
desirable futures

42:13.200 --> 42:16.800
Both for them as well as as well as for

42:17.360 --> 42:18.720
ourselves

42:18.720 --> 42:20.720
current and future humans

42:21.440 --> 42:28.000
Under an admittedly broader definition of what it means to be human and what it means to be

42:28.640 --> 42:31.920
Living a dignified life than not what medieval

42:33.600 --> 42:35.600
Expectations would would support

42:36.880 --> 42:38.880
so

42:40.480 --> 42:42.480
Ashley is actually asking

42:43.440 --> 42:45.440
how far is

42:46.000 --> 42:48.000
Singularity net from agi

42:48.720 --> 42:53.200
Considering that open cog is very robust, but not there yet

42:57.280 --> 43:02.880
Well, so from a singularity net new net hypercycle view the technology is

43:03.760 --> 43:06.960
somewhat agnostic to what approach you

43:07.040 --> 43:13.440
Take to to building toward agi on top of it and we have a project called

43:14.080 --> 43:16.720
Zarko as the ARQ a which we're spinning out now

43:17.040 --> 43:23.920
Which is pretty much building LLMs to run decentralized on on singularity net new net hypercycle platform, right?

43:23.920 --> 43:30.320
And we will enhance those LLMs with open cog or whatever other interesting tools come up come up certainly

43:30.400 --> 43:35.600
but I mean you you can you can do LLMs you can you can do other sorts of of

43:36.320 --> 43:38.320
neural nets you could build

43:38.480 --> 43:44.640
Totally different sort of of AI system. You could build paywangs non axiomatic reasoning system, right?

43:44.640 --> 43:46.640
You could do anything that can be

43:47.600 --> 43:51.520
Implement modern software languages and we can we can we can decentralize it

43:52.160 --> 43:54.400
and that that I think that's it's

43:55.520 --> 43:57.520
important both as a modular

43:58.320 --> 44:00.320
software design

44:00.720 --> 44:02.720
and in terms of

44:03.040 --> 44:10.000
Building a community I want to attract people developing weird new AI ideas on onto the platform that that that said

44:11.520 --> 44:13.360
my own

44:13.360 --> 44:16.560
agi research effort is still oriented on

44:17.440 --> 44:21.520
on the new version of open cog, which is called hyperon and

44:22.400 --> 44:24.400
I I think

44:24.560 --> 44:26.560
in the next

44:27.040 --> 44:29.040
One to two years

44:29.040 --> 44:30.720
We're going to get to a

44:30.720 --> 44:32.400
massively

44:32.400 --> 44:42.000
Scalable version of open cog hyperon and I mean we're we're making quite good progress. I mean we have we have a distributed version of open cogs

44:42.960 --> 44:44.160
metagare

44:44.160 --> 44:49.120
base the adams space which is built on top of mongo db and and and couch base and we're

44:50.080 --> 44:52.080
We're working on a massively

44:53.600 --> 44:55.600
accelerated

44:56.240 --> 45:01.840
Compiler and execution engine for open cog hyperons agi programming language meta

45:02.160 --> 45:04.160
which which basically works by

45:04.880 --> 45:06.880
Compiling open cog meta code

45:07.120 --> 45:12.720
Into roland code that runs on the on the roland interpreter that was developed in the r chain blockchain

45:12.720 --> 45:17.280
So we we've got a bunch of interesting stuff going on the infrastructure side

45:17.920 --> 45:23.680
There's a collaboration with simulai rachel st. Clairs company out of florida on building an optimized

45:24.480 --> 45:29.680
Chip for open cog pattern matching and then an ag agi board as well. So I think we we

45:30.640 --> 45:32.480
Have a lot of

45:32.480 --> 45:34.480
different thrusts going on to make

45:35.200 --> 45:37.200
massively scalable

45:37.760 --> 45:44.080
Infrastructure for open cog hyperon as well as as well as ongoing r&d on the mathematical

45:45.200 --> 45:47.040
Foundations of how to get

45:47.040 --> 45:53.840
large language models and other deep neural nets logical reasoning and then evolutionary learning to work together and I think this is key because

45:54.800 --> 46:00.240
You know llm's and other other generative neural networks are great at recognizing and synthesizing patterns

46:01.440 --> 46:09.360
Logical reasoning engines are better at telling truth from false. So then carrying out, you know, multi chain logical inferences

46:09.920 --> 46:11.680
evolutionary learning

46:11.680 --> 46:16.320
Simulating natural selection and the computer is good at creativity, which is how evolution created us

46:16.320 --> 46:22.240
So I think getting these three ai paradigms working together in this distributed network is

46:23.040 --> 46:24.320
is going to be

46:24.320 --> 46:26.320
big and I think I think that

46:27.280 --> 46:29.280
If we didn't exist in open cog

46:30.160 --> 46:35.040
The mainstream of big tech ai would get to a similar place eventually you can already see

46:35.520 --> 46:41.440
They're taking llm's and they're glomming microsoft knowledge graph onto it and google they're glomming google's knowledge graph onto it

46:41.440 --> 46:47.840
Why because they want to ground it in reality and eventually they'll want to so solving the

46:49.120 --> 46:51.760
hallucination and bullshit problem is

46:52.400 --> 46:57.040
High on the list and integrating with knowledge graphs that can do reasoning is an obvious route there

46:57.280 --> 47:00.560
Their knowledge graphs are not as advanced as the one we have in open cog

47:00.640 --> 47:03.360
But then once the bullshitting problem is solved

47:04.000 --> 47:09.280
Then there's the banality problem of just yes, it's amazing chat gbd can write poetry

47:09.360 --> 47:14.080
But on the other hand, it's all really bad poetry, right? So then how do you how do you get it to be?

47:14.720 --> 47:16.400
creative and and

47:16.480 --> 47:22.240
Inspired like people are and I think then then you're going to inevitably move towards some sort of evolutionary

47:22.880 --> 47:28.320
Algorithm approach. So I think I think big tech will get there to the ingredients that we have already

47:28.880 --> 47:35.760
in the open cog design, but if we can get there faster by deploying open cog hyper on its scale on a

47:36.480 --> 47:38.480
decentralized infrastructure

47:38.640 --> 47:41.920
I mean, then we're doing a couple different things, right? What one is

47:43.520 --> 47:45.280
you know, we're

47:46.000 --> 47:50.240
Past where big tech will get because they're fixated on the particular

47:50.880 --> 47:57.680
Techniques that will best leverage their their advantage in in data rather than on on techniques or not as as

47:58.240 --> 48:00.240
As data training data set bound

48:00.880 --> 48:04.880
But the other thing we're doing is we're making it so that hey lo and behold

48:05.760 --> 48:08.240
the next big breakthrough the breakthrough from

48:09.600 --> 48:11.600
To agi happened

48:12.080 --> 48:17.520
In the decentralized ecosystem rather than the centralized ecosystem, right now if you're if if you're

48:18.320 --> 48:25.120
Nick Bostrom or Elias or Yadkowski, that's going to seem far worse than having it happen within a centralized company because

48:25.760 --> 48:28.640
from the mindset that some of these ai ethics

48:30.080 --> 48:35.280
Buffins have right they they would rather have a small group of people controlling the agi

48:35.760 --> 48:39.280
So so so so then the small group people could be convinced to shut it down

48:39.840 --> 48:47.840
Whereas if you have an agi rolled out in a decentralized way where it's running on millions of different machines in every country on the planet, then

48:48.960 --> 48:50.960
nobody can

48:51.120 --> 48:53.840
Turn it off in such a simple way now

48:53.840 --> 48:57.440
It's not to say humanity couldn't shut it off if humanity is a whole one or two

48:57.440 --> 49:02.480
I mean, that's like think about bitcoin or ethereum. I mean, how would you shut down those networks?

49:03.280 --> 49:09.120
It's not easy for a government to shut down those networks if bitcoin or ethereum. We're going to kill everyone

49:09.440 --> 49:11.840
Well, yeah, people could just start running their nodes, right?

49:11.840 --> 49:18.480
I mean, I mean so and if you have a majority of people who will refuse to confirm transactions

49:18.480 --> 49:20.960
It won't get confirmed, right? So being I mean being

49:21.920 --> 49:23.920
Decentralized in a blockchain based mechanism

49:24.320 --> 49:30.000
Doesn't mean like this will run them up and if all humans wanted to stop it. They couldn't they couldn't stop it

49:30.000 --> 49:35.920
I mean eventually things could get there, but that's not implied in the in the use of a decentralized fabric

49:36.000 --> 49:40.240
But it but it it does mean that it started to shut down in the same way that

49:41.040 --> 49:47.600
Bit bitcoin or bit torn or something are hard to shut down as long as long as there's a substantial group of people

49:48.400 --> 49:54.480
With a bunch of hardware who who want to keep the thing going that then it will keep going and

49:55.920 --> 49:57.360
personally

49:57.360 --> 50:01.360
I see risks either way like having having a centralized entity

50:01.920 --> 50:06.640
Could be the best if the people running that centralized entity are open-minded open-hearted

50:07.280 --> 50:12.880
Compassionate normal individuals who don't don't fall prey to the pathologies that have befallen

50:13.600 --> 50:19.040
Almost every group of humans in an elite power position in human history, but it's possible. It could go well

50:20.080 --> 50:22.880
That decentralized approach has the downside that

50:23.520 --> 50:26.560
You know nasty groups of people could try to take over the network

50:26.560 --> 50:31.200
They can form the open source code. They can try to do do their own unpleasant thing

50:31.280 --> 50:34.640
Right, so we are placing a bet on the vast chaotic

50:35.280 --> 50:38.800
team of unwashed masses of humanity over

50:39.280 --> 50:46.080
self-appointed elites with huge amounts of of money and close military intelligence connections, right? So you I mean

50:46.720 --> 50:48.720
neither neither one is

50:48.960 --> 50:50.560
an

50:50.560 --> 50:51.600
incredibly

50:51.680 --> 50:53.200
safe and certain

50:53.200 --> 50:55.120
path forward

50:55.120 --> 50:56.320
my

50:56.320 --> 50:58.080
view is that

50:58.080 --> 51:00.080
a fundamentally

51:00.080 --> 51:01.280
compassionate

51:01.280 --> 51:03.680
and human benefit oriented agi

51:04.320 --> 51:08.240
Is a bit more likely to emerge from the the decentralized

51:09.120 --> 51:10.160
approach

51:10.160 --> 51:18.720
I don't think this is the thing you can have a completely solid like mathematical proof of right because we're we're not just talking about the algorithms and structures

51:18.720 --> 51:20.720
we're talking about how the human systems

51:21.520 --> 51:23.520
in interact with the

51:24.480 --> 51:26.240
with the compute

51:26.240 --> 51:29.840
fabric and the ai ai algorithms, but I think there's

51:30.720 --> 51:32.320
lots of reasons to be

51:32.320 --> 51:34.320
optimistic. I mean, I

51:34.800 --> 51:36.800
I think

51:37.120 --> 51:39.280
You know with the advanced technology we have right now

51:39.760 --> 51:45.040
You could have so much more destruction in the world that then then what we have but

51:45.920 --> 51:52.720
In fact, there's remarkably few groups of highly competent technically trained individuals out there

51:53.040 --> 51:58.640
Who are massing together to put their energy toward killing a lot of people are causing that causing mass destruction

51:58.640 --> 52:01.520
like you I mean you you don't have a situation like the

52:02.240 --> 52:04.240
you know the amount of people behind

52:04.480 --> 52:10.960
Say a strong tech startup say Anthropod or stable diffusion just not to hover my own company

52:10.960 --> 52:13.040
so the amount of money and energy behind

52:14.000 --> 52:18.480
A startup tech company and the brilliant people going into it and the money going into that hardware

52:18.800 --> 52:22.560
You don't have initiatives like that focus on blowing up a lot of people or or

52:23.040 --> 52:28.800
Reaching great mayhem in the world right you could you could theoretically right but in in fact

52:29.440 --> 52:31.600
In fact the mass of technical innovation

52:32.160 --> 52:34.160
It's going toward making people money

52:34.480 --> 52:39.600
Some bits of it are going toward doing doing doing good for people some things may may may may go awry

52:40.080 --> 52:42.560
but what what what we what we see is

52:44.080 --> 52:46.080
On on the whole most

52:47.440 --> 52:49.440
humans who are

52:49.920 --> 52:53.760
banding together in groups to advance technology are

52:54.640 --> 52:58.400
Are actually not trying to do horrible nasty shit with it right and then I think

52:59.200 --> 53:01.200
I think that's going to continue

53:01.520 --> 53:06.240
It's going to continue as we move toward agi in an open and decentralized manner

53:06.960 --> 53:08.960
There are a lot of people following the

53:09.840 --> 53:11.840
the the stream asking questions

53:12.400 --> 53:14.400
evidently very

53:14.400 --> 53:16.400
passionate about these

53:16.480 --> 53:21.280
Let's let's let's try to do a few quick questions and if I can I can try to give some short answers

53:22.000 --> 53:24.640
one of them one of them is quite

53:26.000 --> 53:27.520
general, but

53:27.520 --> 53:29.520
But I think important

53:30.480 --> 53:33.200
If they want to get involved with

53:34.000 --> 53:38.080
the project what kind of talent what kind of skills

53:39.600 --> 53:45.360
Both in the core as well as in the ecosystem projects are most valuable

53:46.080 --> 53:54.080
In order to further the missions of of singularity net and of the single ecosystem projects as well

53:56.720 --> 53:58.880
There's a great variety of skills that are

54:00.480 --> 54:02.480
Are useful for advancing

54:03.360 --> 54:08.560
Singularity net ecosystem projects and other projects out out there which are which are

54:09.600 --> 54:10.640
advancing

54:10.640 --> 54:12.640
you know singularity

54:13.040 --> 54:17.440
Oriented technology in in in beneficial ways. So I wouldn't want to

54:18.160 --> 54:23.440
Be rigid about it. I mean because I mean we're working with lawyers. We're working with social media people were

54:24.080 --> 54:29.120
We're working with all people from all different walks of life as well as

54:29.520 --> 54:33.280
The more obvious things like AI developers software engineers

54:34.640 --> 54:39.840
Blockchain developers, but biologists cognitive scientists and whatnot, but as a generalization

54:40.880 --> 54:42.880
I would I would I would say

54:46.160 --> 54:52.240
People who can spit so I mean if you have the mentality and interest to be a core agi developer

54:53.760 --> 54:56.000
That's great. We can always use more people who know

54:56.640 --> 55:01.760
Chit loads of math computer science engineering and so forth right if if that's not your thing

55:02.480 --> 55:05.440
I would say there's always a need for people who have

55:06.800 --> 55:12.640
mastery of some other some other domain area and a decent knowledge of AI because when you're trying to

55:13.840 --> 55:16.080
Apply AI and say biology or

55:16.800 --> 55:19.040
Supply chain or media or whatever it is

55:19.680 --> 55:23.280
It still comes down to some small group of humans who understand that domain

55:23.680 --> 55:26.720
And have a decent sense of the AI to make to make that bridge

55:27.360 --> 55:31.040
AI is not yet figuring out how to apply itself in one after another

55:31.680 --> 55:34.720
Critical area. So I I think we're entering an era where

55:35.360 --> 55:37.360
cross-disciplinarity is

55:37.520 --> 55:39.120
is

55:39.120 --> 55:42.080
Going to be key and and highly valued

55:43.600 --> 55:46.960
The day after tomorrow there will be the decentralized governance

55:47.680 --> 55:49.120
summit

55:49.120 --> 55:51.120
What can people expect?

55:51.920 --> 55:59.440
To to find there. What are the the kind of contributions that you will also provide as well as the other speakers?

56:01.600 --> 56:03.600
Yeah, good question. So

56:04.000 --> 56:05.440
you know making

56:05.440 --> 56:12.000
A decentralized AI network has a number of aspects and I've tended to focus on the technical aspects

56:12.080 --> 56:17.040
Just of how do you get large-scale compute processes of the sort you need in AI

56:17.600 --> 56:22.640
To actually run across a huge number of machines without a central controller because there's

56:23.200 --> 56:27.040
There's a huge load of hard technical problems you have to solve to make this

56:27.680 --> 56:34.960
Happen which we're working on in Singularity that new net hypercycle open cog to agi zarka, blah, blah, blah, but I mean the

56:35.920 --> 56:37.920
the other aspect to

56:38.400 --> 56:40.240
making AI

56:40.240 --> 56:42.240
fundamentally decentralized

56:42.320 --> 56:43.680
is

56:43.680 --> 56:45.680
governance of this

56:46.400 --> 56:50.160
Network right because it's not all about the computing yet

56:50.880 --> 56:53.760
All the all these compute nodes in your decentralized network

56:54.320 --> 56:59.440
Are run by people they have to they have to decide to run run this software on their machine

56:59.520 --> 57:01.840
They have to decide one to one to upgrade

57:02.480 --> 57:06.240
upgrade to a new version of software there will be decisions about the

57:06.960 --> 57:12.480
Parameters of the software and the network that have to be made for now by the people who are running the software nodes and

57:12.800 --> 57:17.200
And the people who are using using using the software running on the nodes, right? So there's

57:17.840 --> 57:21.200
There's a significant governance aspect to decentralized

57:22.080 --> 57:24.000
AI networks

57:24.000 --> 57:27.600
as well when we founded Singularity Net in 2017

57:28.560 --> 57:31.440
one of our goals was to gradually

57:32.080 --> 57:37.680
Transform the governance of the whole Singularity Net network into a a DAO of some form a decentralized

57:38.400 --> 57:45.840
Autonomous organization not meaning initially would just be the AI nodes making the decisions, but but meaning meaning that it would be a

57:47.520 --> 57:50.800
democratically governed network of humans and and

57:51.440 --> 57:57.120
and AIs with the whole collective of participants in the network playing a meaning meaningful role

57:57.600 --> 57:59.600
in in in the ongoing

58:00.000 --> 58:03.680
Decisions of evolving the the network over over time

58:04.240 --> 58:06.000
So we've we've had

58:06.000 --> 58:11.120
Some measure of that so far. I mean we've had some pretty big decisions in the history of Singularity Net

58:12.640 --> 58:16.400
Made by community vote and and community discussion

58:17.040 --> 58:19.040
But we haven't gone as far in that direction

58:19.680 --> 58:23.040
as I would I would like to have and so that basically

58:24.560 --> 58:26.400
the governance

58:26.400 --> 58:31.200
powwow that summit that we're having on on the Friday is

58:31.840 --> 58:33.840
oriented toward pulling together

58:34.560 --> 58:39.120
you know various people involved in different aspects of Singularity Net ecosystem and

58:39.840 --> 58:41.840
gathering energy toward

58:42.080 --> 58:43.440
in the next

58:43.440 --> 58:46.560
two years, how can we transform Singularity Net

58:47.360 --> 58:49.360
into a radically more

58:50.080 --> 58:53.680
decentralized organization where the breadth of participants

58:54.320 --> 58:55.680
in the in the

58:55.920 --> 59:03.040
Network are playing a greater role in making the important decisions about the evolution of what happens in the network and

59:04.080 --> 59:07.760
I mean I picked the timing of two years in my head

59:08.320 --> 59:10.320
with a couple things in mind one is

59:10.880 --> 59:16.480
I don't want to rush things to an insane degree because we're doing very valuable stuff and I don't want to

59:17.200 --> 59:23.440
Disrupt it in some stupid way and there are many pathologies that that that we've seen in the blockchain world from from

59:24.240 --> 59:30.960
From Dow governance on the other hand, you know if if we could get to human level AGI in three years

59:32.480 --> 59:34.080
Then I want to have

59:34.080 --> 59:36.320
Fundamentally and fully decentralized governance

59:37.040 --> 59:43.360
Before that because I think it's going to be key in the transition from their AI to to AGI

59:44.080 --> 59:50.000
In all sorts of ways. We can't prefigure now right like I'm I'm a big optimist about what happened after the Singularity

59:50.960 --> 59:58.160
More based on my heart and and soul and feeling the based on rational analysis because I think based on rational analysis

59:58.640 --> 01:00:00.640
the confidence intervals just

01:00:00.960 --> 01:00:04.960
Very wide and there's a lot that we can't rationally know but I I have a strong

01:00:06.000 --> 01:00:13.600
Faith that things are going to come out pretty amazingly once we have superhuman AGI that we've raised up in a compassionate and democratic way

01:00:14.400 --> 01:00:16.720
but I can see a lot of risk of

01:00:17.680 --> 01:00:25.120
Chaos unfolding en route to this utopic future and I think that having the

01:00:26.160 --> 01:00:28.160
network in which AGI emerges

01:00:29.200 --> 01:00:33.040
Fully decentralized and very richly democratically governed

01:00:33.920 --> 01:00:35.920
that gives a lot of

01:00:36.320 --> 01:00:39.440
important degrees of freedom for how the emerging

01:00:40.080 --> 01:00:42.080
AI network

01:00:42.080 --> 01:00:48.480
Sort of deals with any chaos that happens between here and the singularity. So I think this is it's both

01:00:49.440 --> 01:00:52.880
It's both the direction we committed to when we founded Singularity Net and

01:00:53.600 --> 01:00:55.600
potentially could be really really

01:00:56.160 --> 01:00:59.120
important to humanity if the breakthrough to AGI

01:00:59.760 --> 01:01:03.600
Happens within the Singularity Net network, which is obviously what we're pushing for

01:01:03.600 --> 01:01:05.600
And

01:01:08.960 --> 01:01:10.960
I know that you are

01:01:11.760 --> 01:01:12.960
also

01:01:12.960 --> 01:01:16.480
Going to be at a couple of events that you wanted to you to mention

01:01:17.360 --> 01:01:18.720
in

01:01:18.720 --> 01:01:20.720
the consensus

01:01:20.720 --> 01:01:22.720
in Austin and

01:01:25.520 --> 01:01:29.760
Yeah, events are coming back right so spending a couple years sitting. Yes, and

01:01:30.080 --> 01:01:32.080
And

01:01:32.320 --> 01:01:34.320
Web Summit go ahead

01:01:34.640 --> 01:01:36.640
Yeah, yeah, yeah, I think

01:01:37.120 --> 01:01:40.560
Events have come back right during the pandemic. I was mostly sitting home

01:01:41.520 --> 01:01:45.840
working on AI on blockchain and playing with my kids and now

01:01:46.640 --> 01:01:47.840
Now

01:01:47.840 --> 01:01:53.280
Conferences are back. I'm trying to keep it under control so I can keep getting work done, but I'm going going to

01:01:54.400 --> 01:01:56.640
Consensus event in Austin

01:01:57.360 --> 01:01:59.360
Next week I'll I'll be doing a

01:02:01.360 --> 01:02:06.560
Conversation or interview with Edward Snowden on AI and the security where we'll

01:02:07.360 --> 01:02:09.360
Hit on a bunch a bunch of these same points

01:02:09.920 --> 01:02:11.360
also doing a

01:02:11.360 --> 01:02:17.360
Concert at the Speak Easy Club where I'm playing keyboards with the jam galaxy band with our robot lead vocalist

01:02:18.240 --> 01:02:20.240
Desna Mona the Sophia robots

01:02:21.680 --> 01:02:24.800
Little sister there'll be a bunch of Singularity Net people

01:02:25.520 --> 01:02:27.920
around around the consensus so definitely

01:02:28.640 --> 01:02:30.800
Come come and hang out with us then

01:02:31.440 --> 01:02:33.200
following that I'll be at

01:02:33.200 --> 01:02:36.480
web summit Rio and also doing an AI meet-up in

01:02:37.120 --> 01:02:44.640
In Rio on one of the evenings of web summit and I was I was born in brazil way back in 1966. So I'm a

01:02:45.360 --> 01:02:49.520
The world's worst brazilian citizen because my portuguese is is horrible

01:02:49.520 --> 01:02:53.440
But I'm I'm a dual citizen us in brazil always always feels good to

01:02:54.000 --> 01:03:00.320
To go go go back to the to the motherland. So there's a yeah, if you're if you're gonna be in australia real then

01:03:01.280 --> 01:03:07.920
You know, there's a bunch of singularity people people around in the next next few weeks, but of of course

01:03:09.520 --> 01:03:15.280
The core work we're doing is distributed around the globe and you can you can get get involved on the

01:03:16.240 --> 01:03:18.960
On the interwebs if you go to Singularity Net

01:03:19.680 --> 01:03:24.720
I owe you can join that join mailing list and find various get however repose and

01:03:25.520 --> 01:03:30.640
Telegram linked in and and ever everything else. We encourage people to get involved these will be

01:03:32.000 --> 01:03:37.920
The next few years maybe the last few years that you have a chance to help bring about a beneficial singularity unless you're

01:03:38.320 --> 01:03:41.040
Be doing it in some post-singularity video game or something

01:03:41.920 --> 01:03:49.280
Uh, Ben, uh, this was wonderful. Thank you very much for covering so many of the topics and

01:03:50.400 --> 01:03:54.160
I'm sure that there will be a lot more to talk about and

01:03:55.440 --> 01:03:58.640
Congratulations for everything that you have been able to

01:03:59.280 --> 01:04:00.560
accomplish

01:04:00.560 --> 01:04:01.520
with

01:04:01.520 --> 01:04:03.280
Singularity Net

01:04:03.280 --> 01:04:05.280
everyone in the teams

01:04:05.840 --> 01:04:07.040
and

01:04:07.040 --> 01:04:14.960
Good luck with further developing all the projects and I will see you at the decentralization summit

01:04:16.080 --> 01:04:18.560
Just in a in a couple of days

01:04:19.600 --> 01:04:21.120
We will both

01:04:21.120 --> 01:04:23.120
be speaking there to talk about

01:04:23.840 --> 01:04:25.840
governance and how to

01:04:26.720 --> 01:04:28.560
ride the tiger

01:04:28.560 --> 01:04:32.000
Really in the next period of time. Thank you again

01:04:33.600 --> 01:04:35.600
Thanks a lot. David. It's been a pleasure

01:04:38.000 --> 01:04:44.400
Thank you, uh, everyone for being here with us today at beyond conversations

01:04:45.280 --> 01:04:50.320
I am sure that there will be a lot more opportunities to

01:04:52.240 --> 01:04:55.440
Ask questions and to really

01:04:56.240 --> 01:05:01.200
Find out how technology is impacting and benefiting

01:05:01.840 --> 01:05:03.840
Society around us

01:05:03.920 --> 01:05:05.920
Goodbye

01:05:07.040 --> 01:05:09.200
You

