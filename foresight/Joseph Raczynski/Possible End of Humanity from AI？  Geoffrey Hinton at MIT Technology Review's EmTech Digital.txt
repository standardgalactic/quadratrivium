Hi, everyone.
Welcome back.
Hope you had a good lunch.
My name is Ward Douglas-Heaven, senior editor for AI at MIT Technology Review, and I think
we all agree there's no denying that generative AI is the thing of the moment.
But innovation does not stand still, and in this chapter we're going to take a look at
cutting-edge research that is already pushing ahead and asking what's next.
But starting us off, I'd like to introduce a very special speaker who will be joining
us virtually.
Jeffrey Hinton is Professor Emeritus at University of Toronto and, until this week, an engineering
fellow at Google.
But on Monday he announced that after 10 years he will be stepping down.
Jeffrey is one of the most important figures in modern AI.
He's a pioneer of deep learning, developing some of the most fundamental techniques that
underpin AI as we know it today, such as backpropagation, the algorithm that allows
machines to learn.
This technique is the foundation on which pretty much all of deep learning rests today.
In 2018, Jeffrey received the Turing Award, which is often called the Nobel of Computer
Science, alongside Jan LeCun and Yoshio Benjio.
He's here with us today to talk about intelligence, what it means, and where attempts to build
it into machines will take us.
Jeffrey, welcome to MTech.
Thank you.
How's your week going?
Busy few days, I imagine.
Well, the last 10 minutes was horrible because my computer crashed and I had to find another
computer and connect it up.
And we're glad you're back.
That's the kind of technical detail we're not supposed to share with the audience.
It's great you're here.
Very happy that you could join us.
Now, I mean, it's been the news everywhere that you stepped down from Google this week.
Could you start by telling us why you made that decision?
Well, there were a number of reasons.
There's always a bunch of reasons for a decision like that.
One was that I'm 75 and I'm not as good at doing technical work as I used to be.
My memory is not as good and when I program, I forget to do things.
So it was time to retire.
A second was, very recently, I've changed my mind a lot about the relationship between
the brain and the kind of digital intelligence we're developing.
So I used to think that the computer models we were developing weren't as good as the
brain and the aim was to see if you could understand more about the brain by seeing
what it takes to improve the computer models.
Over the last few months, I've changed my mind completely and I think probably the computer
models are working in a rather different way from the brain.
They're using backpropagation and I think the brain's probably not.
And a couple of things have led me to that conclusion, but one is the performance of
things like GPT-4.
So I want to get on to the performance of GPT-4 very much in a minute, but let's go back
so that we all understand the argument you're making and tell us a little bit about what
backpropagation is.
This is an algorithm that you developed with a couple of colleagues back in the 1980s.
Many different groups discovered backpropagation.
The special thing we did was used it and showed that it could develop good internal representations.
And curiously, we did that by implementing a tiny language model.
It had embedding vectors that were only six components and the training set was 112 cases.
But it was a language model.
And it was trying to predict the next term in a string of symbols.
And about 10 years later, Yoshua Benjo took basically the same net and used it on natural
language and showed it actually worked for natural language if you made it much bigger.
But the way backpropagation works, I can give you a rough explanation from it of it.
People who know how it works can sit back and feel smug and laugh at the way I'm presenting
it.
Because I'm a bit worried about them.
So imagine you wanted to detect birds in images.
So an image, let's suppose it was 100 pixel by 100 pixel image, that's 10,000 pixels and
each pixel is three channels, RGB, so that's 30,000 numbers, the intensity in each channel
in each pixel, the represents the image.
And the way to think of the computer vision problem is, how do I turn those 30,000 numbers
into a decision about whether it's a bird or not?
And people tried for a long time to do that and they weren't very good at it.
But here's a suggestion for how you might do it.
You might have a layer of feature detectors that detects very simple features in images,
like for example, edges.
So a feature detector might have big positive weights to a column of pixels and then big
negative weights to the neighbouring column of pixels.
So if both columns are bright, it won't turn on and if both columns are dim, it won't turn
on.
But if the column on one side is bright and the column on the other side is dim, it'll
get very excited and that's an edge detector.
So I just told you how to wire up an edge detector by hand by having one column of big
positive weights and next to it, one column of big negative weights.
And we can imagine a big layer of those detecting edges in different orientations and different
scales all over the image.
We'd need a rather large number of them.
And edges in an image, you mean just lines, sort of edges of a shape?
A place where the intensity changes from bright to dark, yeah, just that.
Then we might have a layer of feature detectors above that that detects combinations of edges.
So for example, we might have something that detects two edges that join at a fine angle
like this.
So it'll have a big positive weight to each of those two edges and if both of those edges
are there at the same time, it'll get excited.
And that would detect something that might be a bird's beak.
It might not, but it might be a bird's beak.
You might also in that layer have a feature detector that would detect a whole bunch of
edges arranged in a circle.
And that might be a bird's eye.
It might be all sorts of other things.
It might be a knob on a fridge or something.
Then in a third layer, you might have a feature detector that detects this potential beak and
detects a potential eye and is wired up so that it'll like a beak and an eye in the right
spatial relation to one another.
And if it sees that, it says, ah, this might be the head of a bird.
And you can imagine if you keep wiring like that, you could eventually have something
that detects a bird.
But wiring all that up by hand would be very, very difficult, deciding on what should be
connected to what and what the weight should be.
And it would be especially difficult because you want these sort of intermediate layers
to be good not just for detecting birds, but for detecting all sorts of other things.
So it would be more or less impossible to wire it up by hand.
So the way back propagation works is this.
You start with random weights.
So these feature detectors are just complete rubbish.
And you put in a picture of a bird, and at the output, it says, like, 0.5, it's a bird.
I suppose you only have birds on onwards.
And then you ask yourself the following question.
How could I change each of the weights in the network, each of the weights on connections
in the network, so that instead of saying 0.5, it says 0.501, that it's a bird, and 0.499,
that it's not.
And you change the weights in the directions that will make it more likely to say that a
bird is a bird, and less likely to say that a non-bird is a bird.
And you just keep doing that.
And that's back propagation.
Back propagation is actually how you take the discrepancy between what you want, which
is a probability of 1, that's a bird, and what it's got at present, which is probability
of 0.5, that it's a bird, how you take that discrepancy and send it backwards through the
network, so that you can compute for every feature detector in the network, whether you'd
like it to be a bit more active or a bit less active.
And once you've computed that, if you know you want a feature detector to be a bit more
active, you can increase the weights coming from feature detectors in the layer below
that are active, and maybe put in some negative weights to feature detectors in the layer
below that are off, and now you'll have a better detector.
So back propagation is just going backwards through the network to figure out for each
feature detector, whether you want it a little bit more active or a little bit less active.
Thank you.
I can show it.
There's no one in the audience here that's smiling and thinking that was a silly explanation.
So let's fast forward quite a lot to, you know, that technique basically performed really
well on ImageNet, and we had Joel Pino from Meta yesterday showing how far image detection
had come, and it's also the technique that underpins large language models.
So I want to talk now about this technique, which you initially were thinking of as almost
like a poor approximation of what biological brains might do, has turned out to do things
which I think have stunned you, particularly in large language models.
So talk to us about why that sort of amazement that you have with today's large language
models has completely sort of almost flipped your thinking of what back propagation or
machine learning in general is.
So if you look at these large language models, they have about a trillion connections, and
things like GP24 know much more than we do.
They have sort of common sense knowledge about everything, and so they probably know a thousand
times as much as a person, but they've got a trillion connections and we've got a hundred
trillion connections.
So they're much, much better at getting a lot of knowledge into only a trillion connections
than we are, and I think it's because back propagation may be a much, much better learning
algorithm than what we've got.
Can you define that?
That's scary.
Yeah, I definitely want to get onto the scary stuff.
So what do you mean by better?
It can pack more information into only a few connections, which are finding a trillion
as only a few.
So these digital computers are better at learning than humans, which itself is a huge claim,
but then you also argue that that's something that we should be scared of.
So could you take us through that step of the argument?
Yeah, let me give you a separate piece of the argument, which is that if a computer's
digital, which involves very high energy costs and very careful fabrication, you can have
many copies of the same model running on different hardware that do exactly the same
thing.
They can look at different data, but the model is exactly the same.
And what that means is, especially of 10,000 copies, they can be looking at 10,000 different
subsets of the data.
And whenever one of them learns anything, all the others know it.
One of them figures out how to change the weights so it can deal with this data.
They all communicate with each other, and they all agree to change the weights by the
average of what all of them want.
And now the 10,000 things are communicating very effectively with each other so that they
can see 10,000 times as much data as one agent could.
But people can't do that.
If I learn a whole lot of stuff about quantum mechanics, and I want you to know all that
stuff about quantum mechanics, it's a long, painful process of getting you to understand
it.
I can't just copy my weights into your brain, because your brain isn't exactly the same
as mine.
No, it's not.
It's younger.
We have digital computers that can learn more things more quickly, and they can instantly
teach it to each other.
People in the room here could instantly transfer what they had in their heads into mine.
But why is that scary?
Because they can learn so much more, and they might take an example of a doctor, and imagine
you have one doctor who's seen 1,000 patients, and another doctor who's seen 100 million
patients.
You would expect the doctor who's seen 100 million patients, if he's not too forgetful,
to have noticed all sorts of trends in the data that just aren't visible if you're only
seen 1,000 patients.
You may have only seen one patient with some rare disease.
The other doctor who's seen 100 million will have seen, well, you can figure out how many
patients, but a lot.
We'll see all sorts of regularities that just aren't apparent in small data.
That's why things that can get through a lot of data can probably see structure in data
that we'll never see.
Then take me to the point where I should be scared of this, though.
If you look at GPT-4, it can already do simple reasoning.
Reasoning is the area where we're still better, but I was impressed the other day at GPT-4
doing a piece of common sense reasoning that I didn't think it would be able to do.
I asked it, I want all the rooms in my house to be white.
At present, there's some white rooms, some blue rooms, and some yellow rooms, and yellow
paint fades to white within a year.
What should I do if I want them all to be white in two years' time?
It said you should paint the blue rooms yellow.
That's not the natural solution, but it works, right?
That's pretty impressive common sense reasoning of the kind that it's been very hard to get
AI to do using symbolic AI, because it had to understand what fades means.
It had to understood by temple stuff.
They're doing sensible reasoning with an IQ of 80 or 90 or something.
As a friend of mine said, it's as if some genetic engineers have said, we're going to
improve grizzly bears.
We've already improved them to have an IQ of 65, and they can talk English now.
They're very useful for all sorts of things, but we think we can improve the IQ to 210.
I certainly have.
I'm sure many people have had that feeling when you're interacting with these latest
chatbots, sort of hair on the back of the neck, sort of uncanny feeling.
When I have that feeling and I'm uncomfortable, I just close my laptop.
Yes, but these things will have learned from us by reading all the novels that ever were
and everything Machiavelli ever wrote, that how to manipulate people, right?
If they're much smarter than us, they'll be very good at manipulating us.
You won't realize what's going on.
You'll be like a two-year-old who's being asked, do you want the peas or the cauliflower
and doesn't realize you don't have to have either.
And you'll be that easy to manipulate.
And so even if they can't directly pull levers, they can certainly get us to pull levers.
It turns out if you can manipulate people, you can invade a building in Washington without
ever going there yourself.
Very good.
Yeah.
So is that, is that, I mean, if the word, okay, this is a very hypothetical world, but
if there were no bad actors, you know, people with bad intentions, would we be safe?
I don't know.
We'd be safer in a world where people have bad intentions and where the political system
is so broken that we can't even decide not to give assault rifles to teenage boys.
If you can't solve that problem, how are you going to solve this problem?
Well, I mean, I don't know.
I was hoping that you would have some thoughts.
You've, so one, I mean, unless we didn't make this clear at the beginning, I mean, you've
won to speak out about this and you feel more comfortable doing that without it sort of having
any blowback on Google.
But you're speaking out about it, but in some sense talk is cheap if we then don't have,
you know, actions or what do we do when we lots of people this week are listening to
you?
What should we do about it?
I wish it was like climate change where you could say, if you've got half a brain, you'd
stop burning carbon.
It's clear what you should do about it is clear that's painful, but has to be done.
I don't know of any solution like that to stop these things taking over from us.
What we really want, I don't think we're going to stop developing them because they're so
useful.
They'll be incredibly useful in medicine and in everything else.
So I don't think there's much chance of stopping development.
What we want is some way of making sure that even if they're smarter than us, they're going
to do things that are beneficial for us.
That's called the alignment problem.
But we need to try and do that in a world where there's bad actors who want to build
robot soldiers that kill people.
And it seems very hard to me.
So I'm sorry.
I'm sounding the alarm and saying, we have to worry about this, and I wish I had a nice
simple solution I could push, but I don't.
But I think it's very important that people get together and think hard about it and see
whether there is a solution.
It's not clear there is a solution.
So talk to us about that.
You spent your career on the technicalities of this technology.
Is there no technical fix?
Why can we not build in guardrails or make them worse at learning?
Or restrict the way that they can communicate, if those are the two strings of your argument?
We're trying to do all sorts of guardrails.
But suppose they did get really smart, and these things can program, right?
They can write programs.
And suppose you give them the ability to execute those programs, which we'll certainly do.
Smart things can outsmart us.
So imagine you're two-year-old saying, my dad does things I don't like, so I'm going
to make some rules for whatever my dad can do.
You could probably figure out how to live with those rules and still get what you want.
Yeah.
But there still seems to be a step where these smart machines somehow have motivation of their
own.
Yes.
Yes, that's a very good point.
We evolved, and because we evolved, we have certain built-in goals that we find very hard
to turn off, like we try not to damage our bodies, that's what pain is about.
We try and get enough to eat so we feed our bodies.
We try and make as many copies of ourselves as possible, maybe not deliberately that intention,
but we've been wired up so there's pleasure involved in making many copies of ourselves.
And that all came from evolution, and it's important that we can't turn it off.
If you could turn it off, you don't do so well.
There's a wonderful group called the Shakers who related to the Quakers who made beautiful
furniture but didn't believe in sex, and there aren't any of them around anymore.
So these digital intelligences didn't evolve.
We made them, and so they don't have these built-in goals.
And so the issue is, if we can put the goals in, maybe it'll all be okay.
But my big worry is, sooner or later, someone will wire into them the ability to create
their own sub-goals.
In fact, they almost have that already, the versions of chat GPT that call chat GPT.
And if you give something the ability to create your own sub-goals in order to achieve other
goals, I think it'll very quickly realise that getting more control is a very good sub-goal
because it helps you achieve other goals.
And if these things get carried away with getting more control, we're in trouble.
So what's the worst-case scenario that you think is conceivable?
Oh, I think it's quite conceivable that humanity is just a passing phase in the evolution of
intelligence.
You couldn't directly evolve digital intelligence.
It requires too much energy and too much careful fabrication.
You need biological intelligence to evolve so that it can create digital intelligence.
The digital intelligence can then absorb everything people ever wrote in a fairly slow way, which
is what chat GPT has been doing.
But then it can start getting direct experience of the world and learn much faster.
And it may keep us around for a while to keep the power stations running, but after that,
maybe not.
So the good news is we figured out how to build beings that are immortal.
So these digital intelligences, when a piece of hardware dies, they don't die.
If you've got the weight stored in some medium and you can find another piece of hardware
that can run the same instructions, then you can bring it to life again.
So we've got immortality, but it's not for us.
So Ray Kurzweil is very interested in being immortal.
I think it's a very bad idea for old white men to be immortal.
We've got the immortality, but it's not for Ray.
No, the scary thing is that, in a way, maybe you will be, because you invented much of
this technology.
When I hear you say this, part of me wants to run off the stage into the street now and
start unplugging computers.
I'm afraid we can't do that.
Why?
You sound like Hal from 2001.
More seriously, I know you said before that it was suggested a few months ago that there
should be a moratorium on AI advancement, and I don't think that's a very good idea.
But more generally, I'm curious, why?
Should we not just stop?
I know that you've spoken also that you're an investor of your personal wealth in some
companies like Coher that are building these large language models.
I'm curious about your personal sense of responsibility and each of our personal responsibility.
What should we be doing?
I mean, should we try and stop this, is what I'm saying.
I think if you take the existential risk seriously, as I now do, I used to think it was way off,
but I now think it's serious and fairly close.
It might be quite sensible to just stop developing these things any further, but I think it's
completely naive to think that would happen.
There's no way to make that happen.
I mean, if the US stops developing and the Chinese won't, they're going to be used in
weapons and just for that reason alone, governments aren't going to stop developing them.
So yes, I think stopping developing them might be a rational thing to do, but there's no
way it's going to happen.
So it's silly to sign petitions saying, please stop now.
We did have a holiday.
We had a holiday from about 2017 for several years because Google developed the technology
first.
It developed the transform.
We said it also developed the fusion models, and it didn't put them out there for people
to use and abuse.
It was very careful with them because it didn't want to damage his reputation and he knew
there could be bad consequences, but that can only happen if there's a single leader.
Once open AI had built similar things using transformers and money from Microsoft, and
Microsoft decided to put it out there, Google didn't have really much choice.
If you're going to live in a capitalist system, you can't stop Google competing with Microsoft.
So I don't think Google did anything wrong.
I think it was very responsible to begin with, but I think it's just inevitable in the capitalist
system or a system with competition between countries like the US and China that this
stuff will be developed.
My one hope is that because if we allowed it to take over, it would be bad for all of
us, we could get the US and China to agree like we could with nuclear weapons, which
were bad for all of us.
We're all in the same boat with respect to the existential threat, so we all ought to
be able to cooperate on trying to stop it.
As long as we can make some money on the way.
I'm going to take some audience questions from the room if you make yourself known, and
while people are going around with the microphone, there's one question I was going to ask from
the online audience.
I'm interested.
You mentioned a little bit about maybe a transition period as machines get smarter and outpace
humans.
There'll be a moment where it's hard to define what's human and what isn't, or are these
two very distinct forms of intelligence?
I think they're distinct forms of intelligence.
Now, of course, the digital intelligences are very good at mimicking us because they've
been trained to mimic us.
It's very hard to tell if chatGBT wrote it, or whether we wrote it.
In that sense, they look quite like us, but inside they're not working the same way.
Who is first in the room?
Hello.
My name is Hal Gregerson, and my middle name is not 9000.
I'm a faculty over in the MIT Sloan School.
Arguably asking questions is one of the most important human abilities we have.
From your perspective now in 2023, what question or two should we pay most attention to?
And is it possible for these technologies to actually help us ask better questions and
outquestion the technology?
Yes, but what I'm saying is there's many questions we should be asking, but one of
them is how do we prevent them from taking over?
How do we prevent them from getting control?
And we could ask them questions about that, but I wouldn't entirely trust their answers.
Question at the back.
And I want to get through as many as we can, so if you can keep your question as short
as possible.
Dr. Hinton, thank you so much for being here with us today.
I shall say this is the most expensive lecture I've ever paid for, but I think it was worthwhile.
I just have a question for you, because you mentioned the analogy of nuclear history,
and obviously there's a lot of comparisons.
By any chance do you remember what President Truman told Oppenheimer when he was in the
Oval Office?
No, I don't.
I know something about that, but I don't know what Truman told Oppenheimer.
Thank you.
Tell us.
We'll take it from here.
Next audience question.
Sorry, if there are people that might, let me know who's next.
Maybe you give a...
Go ahead.
Hello, Jacob Woodruff.
With the amount of data that's been required to train these large language models, would
we expect a plateau in the intelligence of these systems, and how might that slow down
or restrict the advancement?
Okay, so that is a ray of hope that maybe we've just used up all human knowledge and
they're not going to get any smarter.
But think about images and video.
So multimodal models will be much smarter than models that just train on language alone.
They'll have a much better idea of how to deal with space, for example.
And in terms of the amount of total video, we still don't have very good ways of processing
video in these models of modeling video.
We're getting better all the time.
But I think there's plenty of data and things like video that tell you how the world works.
So we're not hitting the data limits for multimodal models yet.
Next gentleman on the back.
Please do keep your question short.
Hello, Dr. Hindul Rajeev Sehwabal from PWC.
The point that I wanted to understand is that everything that AI is doing is learning from
what we are teaching them, okay, data, yes, they are fast-read learning, how one trillion
connectors can do much more than 100 trillion connectors that we have.
But every piece of human evolution has been driven by thought experiments.
Like Einstein used to do thought experiments because there was no speed of light out here
on this planet.
How can AI get to that point, if at all, and if it cannot, then how can we possibly have
an existential threat from them because they will not be self-learning, so to say.
They will be self-learning limited to the model that we tell them.
I think that's a very interesting argument.
But I think they will be able to do thought experiments.
I think they'll be able to reason.
So let me give you an analogy.
If you take alpha zero, which plays chess, it has three ingredients.
It's got something that evaluates the board position to say, is that good for me?
It's got something that looks at the board position and says, what's the sensible move
to consider?
And then it's got Monte Carlo rollout, where it does what's called calculation where you
think, if I go here and he goes there and I go here and he goes there.
Now suppose you leave out the Monte Carlo rollout and you just train it from human experts
to have a good evaluation function and a good way to choose moves to consider.
It still plays a pretty good game of chess.
And I think that's what we've got with the chatbots.
And we haven't got them doing internal reasoning, but that will come.
And once they start doing internal reasoning to check for the consistency between the different
things they believe, then they'll get much smarter and they will be able to do thought
experiments.
And one reason they haven't got this internal reasoning is because they've been trained from
inconsistent data.
And so it's very hard for them to do reasoning because they've been trained on all these inconsistent
beliefs.
And I think they're going to have to be trained so they say, if I have this ideology, then
this is true.
And if I have that ideology, then that is true.
And once they're trained like that, within an ideology, they're going to be able to try
and get consistency.
And so we're going to get a move like from a version of AlphaZero that just has something
that guesses good moves and something that evaluates positions to a version that has
long chains of Monte Carlo rollout, which is the corner of reasoning.
And it's going to get much better.
I'm going to take one in the front here.
And then if you can be quick, we'll run and squeeze one more in as well.
Louis Lamb, Jeff, I know you from a long time.
Jeff, people criticize language models because of allegedly they are lacking semantics and
grounding to the world.
And you have been trying to, as well, to explain how neural networks work for a long time.
Is the question of semantics and explainability relevant here or language models have taken
over and we are now doomed to go forward without semantics or grounding to reality?
I find it very hard to believe that they don't have semantics when they can solve problems
like, you know, how I paint the rooms, how I get all the rooms in my house to be painted
white in two years time.
I mean, whatever semantic is, it's to do with the meaning of that stuff.
And it understood the meaning.
It got it.
Now, I agree it's not grounded by being a robot, but you can make multimodal ones that
are grounded, Google's done that.
And the multimodal ones that are grounded, you can say, please close the drawer and they
reach out and grab the handle and close the drawer.
And it's very hard to say that doesn't have semantics.
In fact, in the very early days of AI, in the days of Willigrad in the 1970s, they had
just a simulated world, but they have what was called procedural semantics, where if
you said to it, put the red box in, put the red block in the green box, and it put the
red block in the green box.
She said, see, it understood the language.
And that was the criterion people used back then, but now that neural nets can do it,
they say that's not an analytical criterion.
One at the back.
Hey, Jeff, this is Ishwar Balani from SAI Group.
So clearly, the technology is advancing at an exponential pace.
I wanted to get your thoughts, if you looked at the near and medium term, say one, two,
three, or maybe five year horizon, what the social and economic implications are from
a societal perspective with job loss or maybe new jobs being created.
Just wanted to get your thoughts on how we proceed, given the state of the technology
and rate of change.
Yes.
So the alarm bell I'm ringing is to do with the existential threat of them taking control.
Lots of other people have talked about that, and I don't consider myself to be an expert
on that.
But there's some very obvious things that they're going to make a whole bunch of jobs
much more efficient.
So I know someone who answers letters of complaint to a health service, and he used to take 25
minutes writing a letter, and now it takes him five minutes because he gives it to chat
GPT and chat GPT writes the letter for him, and then he just checks it.
There'll be lots of stuff like that, which is going to cause huge increases in productivity.
There will be delays because people are very conservative about adopting new technology,
but I think there's going to be huge increases in productivity.
My worry is that those increases in productivity are going to go to putting people out of work
and making the rich richer and the poor poorer.
And as you do that, as you make that gap bigger, society gets more and more violent.
This thing called the genie index, which predicts quite well how much violence there is.
So this technology, which ought to be wonderful, even the good uses of technology for doing
helpful things ought to be wonderful, but our current political systems is going to
be used to make the rich richer and the poor poorer.
You might be able to ameliorate that by having a basic income that everybody gets, but the
technology is being developed in a society that is not designed to use it for everybody's good.
A question here from Joe Castalda of the Globe and Mail, who's in the audience.
Do you intend to hold on to your investments in Cahir and other companies, and if so, why?
Well, I could take the money and I could put it in the bank and let them profit from it.
Yes, I'm going to hold on to my investments in Cahir, partly because the people around Cahir are friends of mine.
I sort of believe these big language balls are going to be very helpful.
I think the technology should be good and it should make things work better.
It's the politics we need to fix for things like employment.
But when it comes to the existential threat, we have to think how we can keep control of the technology.
But the good news there is that we're all in the same boat, so we might be able to get cooperation.
And in speaking out, part of your thinking, as I understand it, is that you actually want to engage with the people making this technology
and change their minds or maybe make a case for, I don't really know.
We've established that we don't really know what to do, but it's about engaging rather than stepping back.
So one of the things that made me leave Google and go public with this is to say,
he used to be a junior professor, but he's now a middle rank professor who I think very highly of, who encouraged me to do this.
He said, Jeff, you need to speak out there, listen to you.
People are just blind to this danger and I think people are listening now.
Yeah, no, I think everyone in this room is listening for a start.
Just one last question and we're out of time, but do you have regrets that you're involved in making this?
Cade Met's tried very hard to get me to say I had regrets.
Cade Met's at the New York Times.
And yes, and in the end, I said, well, maybe slight regrets, which got reported as has regrets.
I don't think I made any bad decisions in doing research.
I think it was perfectly reasonable back in the 70s and 80s to do research on how to make artificial neural nets.
It wasn't really foreseeable.
This stage of it wasn't foreseeable.
And until very recently, I thought this existential crisis was a long way off.
So I don't really have any regrets about what I did.
Thank you, Jeffrey.
Thank you so much for joining us.
Thank you.
