start	end	text
0	11200	Hi, everyone.
11200	12200	Welcome back.
12200	13640	Hope you had a good lunch.
13640	19120	My name is Ward Douglas-Heaven, senior editor for AI at MIT Technology Review, and I think
19120	23120	we all agree there's no denying that generative AI is the thing of the moment.
23120	26560	But innovation does not stand still, and in this chapter we're going to take a look at
26560	30600	cutting-edge research that is already pushing ahead and asking what's next.
30600	35200	But starting us off, I'd like to introduce a very special speaker who will be joining
35200	36840	us virtually.
36840	42560	Jeffrey Hinton is Professor Emeritus at University of Toronto and, until this week, an engineering
42560	43560	fellow at Google.
43560	49120	But on Monday he announced that after 10 years he will be stepping down.
49120	52200	Jeffrey is one of the most important figures in modern AI.
52200	56080	He's a pioneer of deep learning, developing some of the most fundamental techniques that
56080	60640	underpin AI as we know it today, such as backpropagation, the algorithm that allows
60640	63840	machines to learn.
63840	69000	This technique is the foundation on which pretty much all of deep learning rests today.
69000	73240	In 2018, Jeffrey received the Turing Award, which is often called the Nobel of Computer
73240	78440	Science, alongside Jan LeCun and Yoshio Benjio.
78440	82920	He's here with us today to talk about intelligence, what it means, and where attempts to build
82920	85200	it into machines will take us.
85320	88240	Jeffrey, welcome to MTech.
88240	89240	Thank you.
89240	90240	How's your week going?
90240	91240	Busy few days, I imagine.
91240	95880	Well, the last 10 minutes was horrible because my computer crashed and I had to find another
95880	98440	computer and connect it up.
98440	99440	And we're glad you're back.
99440	104120	That's the kind of technical detail we're not supposed to share with the audience.
104120	105360	It's great you're here.
105360	107080	Very happy that you could join us.
107080	112040	Now, I mean, it's been the news everywhere that you stepped down from Google this week.
112360	116560	Could you start by telling us why you made that decision?
116560	118160	Well, there were a number of reasons.
118160	120680	There's always a bunch of reasons for a decision like that.
120680	127320	One was that I'm 75 and I'm not as good at doing technical work as I used to be.
127320	131440	My memory is not as good and when I program, I forget to do things.
131440	133760	So it was time to retire.
133760	140360	A second was, very recently, I've changed my mind a lot about the relationship between
140360	145080	the brain and the kind of digital intelligence we're developing.
145080	150960	So I used to think that the computer models we were developing weren't as good as the
150960	155520	brain and the aim was to see if you could understand more about the brain by seeing
155520	159400	what it takes to improve the computer models.
159400	164640	Over the last few months, I've changed my mind completely and I think probably the computer
164640	167280	models are working in a rather different way from the brain.
167280	170600	They're using backpropagation and I think the brain's probably not.
170600	174320	And a couple of things have led me to that conclusion, but one is the performance of
174320	176600	things like GPT-4.
176600	182520	So I want to get on to the performance of GPT-4 very much in a minute, but let's go back
182520	188320	so that we all understand the argument you're making and tell us a little bit about what
188320	189560	backpropagation is.
189560	196040	This is an algorithm that you developed with a couple of colleagues back in the 1980s.
196040	199720	Many different groups discovered backpropagation.
199720	206520	The special thing we did was used it and showed that it could develop good internal representations.
206520	215440	And curiously, we did that by implementing a tiny language model.
215440	224040	It had embedding vectors that were only six components and the training set was 112 cases.
224040	225040	But it was a language model.
225040	230640	And it was trying to predict the next term in a string of symbols.
230640	236360	And about 10 years later, Yoshua Benjo took basically the same net and used it on natural
236360	242000	language and showed it actually worked for natural language if you made it much bigger.
242000	249520	But the way backpropagation works, I can give you a rough explanation from it of it.
249520	255400	People who know how it works can sit back and feel smug and laugh at the way I'm presenting
255400	256400	it.
256400	260440	Because I'm a bit worried about them.
260440	265640	So imagine you wanted to detect birds in images.
265640	272080	So an image, let's suppose it was 100 pixel by 100 pixel image, that's 10,000 pixels and
272080	278640	each pixel is three channels, RGB, so that's 30,000 numbers, the intensity in each channel
278640	281000	in each pixel, the represents the image.
281000	286040	And the way to think of the computer vision problem is, how do I turn those 30,000 numbers
286040	289440	into a decision about whether it's a bird or not?
289440	293360	And people tried for a long time to do that and they weren't very good at it.
293360	296240	But here's a suggestion for how you might do it.
296240	301840	You might have a layer of feature detectors that detects very simple features in images,
301840	303560	like for example, edges.
303560	310640	So a feature detector might have big positive weights to a column of pixels and then big
310640	314120	negative weights to the neighbouring column of pixels.
314120	318040	So if both columns are bright, it won't turn on and if both columns are dim, it won't turn
318040	319040	on.
319040	323440	But if the column on one side is bright and the column on the other side is dim, it'll
323440	326880	get very excited and that's an edge detector.
326880	330920	So I just told you how to wire up an edge detector by hand by having one column of big
330920	333280	positive weights and next to it, one column of big negative weights.
333280	337760	And we can imagine a big layer of those detecting edges in different orientations and different
337760	339800	scales all over the image.
339800	341880	We'd need a rather large number of them.
341880	345480	And edges in an image, you mean just lines, sort of edges of a shape?
345480	352280	A place where the intensity changes from bright to dark, yeah, just that.
352280	357360	Then we might have a layer of feature detectors above that that detects combinations of edges.
357360	364320	So for example, we might have something that detects two edges that join at a fine angle
364320	367280	like this.
367280	372160	So it'll have a big positive weight to each of those two edges and if both of those edges
372160	375120	are there at the same time, it'll get excited.
375120	378520	And that would detect something that might be a bird's beak.
378520	380560	It might not, but it might be a bird's beak.
380560	385000	You might also in that layer have a feature detector that would detect a whole bunch of
385000	389080	edges arranged in a circle.
389080	390440	And that might be a bird's eye.
390440	391720	It might be all sorts of other things.
391720	396720	It might be a knob on a fridge or something.
396720	402200	Then in a third layer, you might have a feature detector that detects this potential beak and
402200	407480	detects a potential eye and is wired up so that it'll like a beak and an eye in the right
407480	409040	spatial relation to one another.
409040	413320	And if it sees that, it says, ah, this might be the head of a bird.
413320	417240	And you can imagine if you keep wiring like that, you could eventually have something
417240	419680	that detects a bird.
419680	424800	But wiring all that up by hand would be very, very difficult, deciding on what should be
424800	426680	connected to what and what the weight should be.
426680	430520	And it would be especially difficult because you want these sort of intermediate layers
430520	436080	to be good not just for detecting birds, but for detecting all sorts of other things.
436080	440920	So it would be more or less impossible to wire it up by hand.
440920	443080	So the way back propagation works is this.
443080	445040	You start with random weights.
445040	448720	So these feature detectors are just complete rubbish.
448720	454200	And you put in a picture of a bird, and at the output, it says, like, 0.5, it's a bird.
454200	457440	I suppose you only have birds on onwards.
457440	460080	And then you ask yourself the following question.
460080	466000	How could I change each of the weights in the network, each of the weights on connections
466000	472920	in the network, so that instead of saying 0.5, it says 0.501, that it's a bird, and 0.499,
472920	475120	that it's not.
475120	480080	And you change the weights in the directions that will make it more likely to say that a
480080	485360	bird is a bird, and less likely to say that a non-bird is a bird.
485360	487040	And you just keep doing that.
487040	488040	And that's back propagation.
488040	493200	Back propagation is actually how you take the discrepancy between what you want, which
493200	497080	is a probability of 1, that's a bird, and what it's got at present, which is probability
497080	502000	of 0.5, that it's a bird, how you take that discrepancy and send it backwards through the
502000	508360	network, so that you can compute for every feature detector in the network, whether you'd
508360	511040	like it to be a bit more active or a bit less active.
511040	514120	And once you've computed that, if you know you want a feature detector to be a bit more
514120	518600	active, you can increase the weights coming from feature detectors in the layer below
518600	524200	that are active, and maybe put in some negative weights to feature detectors in the layer
524200	528600	below that are off, and now you'll have a better detector.
528600	531640	So back propagation is just going backwards through the network to figure out for each
531640	535640	feature detector, whether you want it a little bit more active or a little bit less active.
535640	536640	Thank you.
536640	537640	I can show it.
537640	543200	There's no one in the audience here that's smiling and thinking that was a silly explanation.
543200	549840	So let's fast forward quite a lot to, you know, that technique basically performed really
549840	555680	well on ImageNet, and we had Joel Pino from Meta yesterday showing how far image detection
555680	561320	had come, and it's also the technique that underpins large language models.
561320	570120	So I want to talk now about this technique, which you initially were thinking of as almost
570120	577280	like a poor approximation of what biological brains might do, has turned out to do things
577280	581480	which I think have stunned you, particularly in large language models.
581480	588600	So talk to us about why that sort of amazement that you have with today's large language
588600	594280	models has completely sort of almost flipped your thinking of what back propagation or
594280	597560	machine learning in general is.
597560	603640	So if you look at these large language models, they have about a trillion connections, and
603640	607760	things like GP24 know much more than we do.
607760	613520	They have sort of common sense knowledge about everything, and so they probably know a thousand
613520	618520	times as much as a person, but they've got a trillion connections and we've got a hundred
618520	620640	trillion connections.
620640	625200	So they're much, much better at getting a lot of knowledge into only a trillion connections
625200	631000	than we are, and I think it's because back propagation may be a much, much better learning
631000	633280	algorithm than what we've got.
633280	634280	Can you define that?
634280	635280	That's scary.
635280	637000	Yeah, I definitely want to get onto the scary stuff.
637000	640080	So what do you mean by better?
640080	645560	It can pack more information into only a few connections, which are finding a trillion
645560	648440	as only a few.
648440	657760	So these digital computers are better at learning than humans, which itself is a huge claim,
657760	663280	but then you also argue that that's something that we should be scared of.
663280	665520	So could you take us through that step of the argument?
666000	674680	Yeah, let me give you a separate piece of the argument, which is that if a computer's
674680	680000	digital, which involves very high energy costs and very careful fabrication, you can have
680000	684040	many copies of the same model running on different hardware that do exactly the same
684040	685040	thing.
685040	688040	They can look at different data, but the model is exactly the same.
688040	693760	And what that means is, especially of 10,000 copies, they can be looking at 10,000 different
693760	695920	subsets of the data.
695920	699960	And whenever one of them learns anything, all the others know it.
699960	704640	One of them figures out how to change the weights so it can deal with this data.
704640	708040	They all communicate with each other, and they all agree to change the weights by the
708040	711000	average of what all of them want.
711000	719200	And now the 10,000 things are communicating very effectively with each other so that they
719200	723240	can see 10,000 times as much data as one agent could.
723240	725080	But people can't do that.
725080	728840	If I learn a whole lot of stuff about quantum mechanics, and I want you to know all that
728840	733320	stuff about quantum mechanics, it's a long, painful process of getting you to understand
733320	734320	it.
734320	739560	I can't just copy my weights into your brain, because your brain isn't exactly the same
739560	740560	as mine.
740560	744560	No, it's not.
744560	749060	It's younger.
750020	759100	We have digital computers that can learn more things more quickly, and they can instantly
759100	762180	teach it to each other.
762180	766220	People in the room here could instantly transfer what they had in their heads into mine.
766220	770460	But why is that scary?
770460	776860	Because they can learn so much more, and they might take an example of a doctor, and imagine
776860	783460	you have one doctor who's seen 1,000 patients, and another doctor who's seen 100 million
783460	785660	patients.
785660	790340	You would expect the doctor who's seen 100 million patients, if he's not too forgetful,
790340	794180	to have noticed all sorts of trends in the data that just aren't visible if you're only
794180	796380	seen 1,000 patients.
796380	799780	You may have only seen one patient with some rare disease.
799780	803340	The other doctor who's seen 100 million will have seen, well, you can figure out how many
803340	805980	patients, but a lot.
806100	812860	We'll see all sorts of regularities that just aren't apparent in small data.
812860	818140	That's why things that can get through a lot of data can probably see structure in data
818140	821100	that we'll never see.
821100	829100	Then take me to the point where I should be scared of this, though.
829100	833140	If you look at GPT-4, it can already do simple reasoning.
833860	840140	Reasoning is the area where we're still better, but I was impressed the other day at GPT-4
840140	845180	doing a piece of common sense reasoning that I didn't think it would be able to do.
845180	850860	I asked it, I want all the rooms in my house to be white.
850860	856900	At present, there's some white rooms, some blue rooms, and some yellow rooms, and yellow
856900	860100	paint fades to white within a year.
860100	865460	What should I do if I want them all to be white in two years' time?
865460	869300	It said you should paint the blue rooms yellow.
869300	874180	That's not the natural solution, but it works, right?
874180	878700	That's pretty impressive common sense reasoning of the kind that it's been very hard to get
878700	884620	AI to do using symbolic AI, because it had to understand what fades means.
884620	890820	It had to understood by temple stuff.
890820	900380	They're doing sensible reasoning with an IQ of 80 or 90 or something.
900380	908220	As a friend of mine said, it's as if some genetic engineers have said, we're going to
908220	909980	improve grizzly bears.
909980	914340	We've already improved them to have an IQ of 65, and they can talk English now.
914340	924020	They're very useful for all sorts of things, but we think we can improve the IQ to 210.
924020	925020	I certainly have.
925020	930500	I'm sure many people have had that feeling when you're interacting with these latest
930500	935020	chatbots, sort of hair on the back of the neck, sort of uncanny feeling.
935020	939260	When I have that feeling and I'm uncomfortable, I just close my laptop.
940260	947460	Yes, but these things will have learned from us by reading all the novels that ever were
947460	956060	and everything Machiavelli ever wrote, that how to manipulate people, right?
956060	958980	If they're much smarter than us, they'll be very good at manipulating us.
958980	960500	You won't realize what's going on.
960500	965060	You'll be like a two-year-old who's being asked, do you want the peas or the cauliflower
965060	968860	and doesn't realize you don't have to have either.
968860	972580	And you'll be that easy to manipulate.
972580	978620	And so even if they can't directly pull levers, they can certainly get us to pull levers.
978620	983420	It turns out if you can manipulate people, you can invade a building in Washington without
983420	988860	ever going there yourself.
988860	989860	Very good.
989860	990860	Yeah.
990860	996980	So is that, is that, I mean, if the word, okay, this is a very hypothetical world, but
996980	1005700	if there were no bad actors, you know, people with bad intentions, would we be safe?
1005700	1006700	I don't know.
1006700	1012500	We'd be safer in a world where people have bad intentions and where the political system
1012500	1019020	is so broken that we can't even decide not to give assault rifles to teenage boys.
1019020	1021980	If you can't solve that problem, how are you going to solve this problem?
1022980	1023980	Well, I mean, I don't know.
1023980	1027340	I was hoping that you would have some thoughts.
1027340	1035060	You've, so one, I mean, unless we didn't make this clear at the beginning, I mean, you've
1035060	1040700	won to speak out about this and you feel more comfortable doing that without it sort of having
1040700	1044660	any blowback on Google.
1044660	1050380	But you're speaking out about it, but in some sense talk is cheap if we then don't have,
1050380	1055700	you know, actions or what do we do when we lots of people this week are listening to
1055700	1056700	you?
1056700	1058180	What should we do about it?
1058180	1063340	I wish it was like climate change where you could say, if you've got half a brain, you'd
1063340	1066380	stop burning carbon.
1066380	1071820	It's clear what you should do about it is clear that's painful, but has to be done.
1071820	1075820	I don't know of any solution like that to stop these things taking over from us.
1075820	1078900	What we really want, I don't think we're going to stop developing them because they're so
1078900	1079900	useful.
1079940	1084220	They'll be incredibly useful in medicine and in everything else.
1084220	1087060	So I don't think there's much chance of stopping development.
1087060	1093060	What we want is some way of making sure that even if they're smarter than us, they're going
1093060	1095100	to do things that are beneficial for us.
1095100	1096940	That's called the alignment problem.
1096940	1101980	But we need to try and do that in a world where there's bad actors who want to build
1101980	1105100	robot soldiers that kill people.
1105100	1107060	And it seems very hard to me.
1107060	1108060	So I'm sorry.
1108060	1112380	I'm sounding the alarm and saying, we have to worry about this, and I wish I had a nice
1112380	1114380	simple solution I could push, but I don't.
1114380	1117660	But I think it's very important that people get together and think hard about it and see
1117660	1119020	whether there is a solution.
1119020	1121180	It's not clear there is a solution.
1121180	1124260	So talk to us about that.
1124260	1129620	You spent your career on the technicalities of this technology.
1129620	1131580	Is there no technical fix?
1131580	1137020	Why can we not build in guardrails or make them worse at learning?
1137980	1144100	Or restrict the way that they can communicate, if those are the two strings of your argument?
1144100	1147820	We're trying to do all sorts of guardrails.
1147820	1151500	But suppose they did get really smart, and these things can program, right?
1151500	1152500	They can write programs.
1152500	1160260	And suppose you give them the ability to execute those programs, which we'll certainly do.
1160260	1163980	Smart things can outsmart us.
1163980	1171620	So imagine you're two-year-old saying, my dad does things I don't like, so I'm going
1171620	1174700	to make some rules for whatever my dad can do.
1174700	1179300	You could probably figure out how to live with those rules and still get what you want.
1179300	1181300	Yeah.
1181300	1190540	But there still seems to be a step where these smart machines somehow have motivation of their
1190540	1191540	own.
1191540	1192540	Yes.
1192540	1193540	Yes, that's a very good point.
1194020	1200300	We evolved, and because we evolved, we have certain built-in goals that we find very hard
1200300	1207140	to turn off, like we try not to damage our bodies, that's what pain is about.
1207140	1212660	We try and get enough to eat so we feed our bodies.
1212660	1218980	We try and make as many copies of ourselves as possible, maybe not deliberately that intention,
1218980	1224060	but we've been wired up so there's pleasure involved in making many copies of ourselves.
1224060	1230740	And that all came from evolution, and it's important that we can't turn it off.
1230740	1234620	If you could turn it off, you don't do so well.
1234620	1237820	There's a wonderful group called the Shakers who related to the Quakers who made beautiful
1237820	1246180	furniture but didn't believe in sex, and there aren't any of them around anymore.
1246180	1251140	So these digital intelligences didn't evolve.
1251140	1255860	We made them, and so they don't have these built-in goals.
1255860	1260860	And so the issue is, if we can put the goals in, maybe it'll all be okay.
1260860	1266540	But my big worry is, sooner or later, someone will wire into them the ability to create
1266540	1267540	their own sub-goals.
1267540	1274980	In fact, they almost have that already, the versions of chat GPT that call chat GPT.
1274980	1279820	And if you give something the ability to create your own sub-goals in order to achieve other
1279820	1286220	goals, I think it'll very quickly realise that getting more control is a very good sub-goal
1286220	1289700	because it helps you achieve other goals.
1289700	1294100	And if these things get carried away with getting more control, we're in trouble.
1294100	1299180	So what's the worst-case scenario that you think is conceivable?
1299820	1305900	Oh, I think it's quite conceivable that humanity is just a passing phase in the evolution of
1305900	1307260	intelligence.
1307260	1309220	You couldn't directly evolve digital intelligence.
1309220	1313780	It requires too much energy and too much careful fabrication.
1313780	1319460	You need biological intelligence to evolve so that it can create digital intelligence.
1319460	1326820	The digital intelligence can then absorb everything people ever wrote in a fairly slow way, which
1326900	1330580	is what chat GPT has been doing.
1330580	1335340	But then it can start getting direct experience of the world and learn much faster.
1335340	1342900	And it may keep us around for a while to keep the power stations running, but after that,
1342900	1343900	maybe not.
1343900	1349820	So the good news is we figured out how to build beings that are immortal.
1349820	1354540	So these digital intelligences, when a piece of hardware dies, they don't die.
1354580	1359140	If you've got the weight stored in some medium and you can find another piece of hardware
1359140	1364620	that can run the same instructions, then you can bring it to life again.
1364620	1369660	So we've got immortality, but it's not for us.
1369660	1373140	So Ray Kurzweil is very interested in being immortal.
1373140	1377700	I think it's a very bad idea for old white men to be immortal.
1377700	1381620	We've got the immortality, but it's not for Ray.
1381900	1387660	No, the scary thing is that, in a way, maybe you will be, because you invented much of
1387660	1391780	this technology.
1391780	1394980	When I hear you say this, part of me wants to run off the stage into the street now and
1394980	1399580	start unplugging computers.
1399580	1401380	I'm afraid we can't do that.
1401380	1402380	Why?
1402380	1404300	You sound like Hal from 2001.
1404300	1416820	More seriously, I know you said before that it was suggested a few months ago that there
1416820	1425060	should be a moratorium on AI advancement, and I don't think that's a very good idea.
1425060	1428860	But more generally, I'm curious, why?
1428860	1431300	Should we not just stop?
1431300	1438700	I know that you've spoken also that you're an investor of your personal wealth in some
1438700	1441260	companies like Coher that are building these large language models.
1441260	1446420	I'm curious about your personal sense of responsibility and each of our personal responsibility.
1446420	1447420	What should we be doing?
1447420	1451260	I mean, should we try and stop this, is what I'm saying.
1451260	1456580	I think if you take the existential risk seriously, as I now do, I used to think it was way off,
1456580	1460340	but I now think it's serious and fairly close.
1460340	1465740	It might be quite sensible to just stop developing these things any further, but I think it's
1465740	1468780	completely naive to think that would happen.
1468780	1472180	There's no way to make that happen.
1472180	1476460	I mean, if the US stops developing and the Chinese won't, they're going to be used in
1476460	1481700	weapons and just for that reason alone, governments aren't going to stop developing them.
1481700	1487340	So yes, I think stopping developing them might be a rational thing to do, but there's no
1487340	1488340	way it's going to happen.
1488340	1491460	So it's silly to sign petitions saying, please stop now.
1491460	1492780	We did have a holiday.
1492780	1498900	We had a holiday from about 2017 for several years because Google developed the technology
1498900	1499900	first.
1499900	1500900	It developed the transform.
1500900	1506020	We said it also developed the fusion models, and it didn't put them out there for people
1506020	1507580	to use and abuse.
1507580	1510540	It was very careful with them because it didn't want to damage his reputation and he knew
1510540	1516380	there could be bad consequences, but that can only happen if there's a single leader.
1516380	1524820	Once open AI had built similar things using transformers and money from Microsoft, and
1524820	1529740	Microsoft decided to put it out there, Google didn't have really much choice.
1529740	1535660	If you're going to live in a capitalist system, you can't stop Google competing with Microsoft.
1535660	1538380	So I don't think Google did anything wrong.
1538380	1542660	I think it was very responsible to begin with, but I think it's just inevitable in the capitalist
1542660	1546580	system or a system with competition between countries like the US and China that this
1546580	1549540	stuff will be developed.
1549540	1554500	My one hope is that because if we allowed it to take over, it would be bad for all of
1554500	1558460	us, we could get the US and China to agree like we could with nuclear weapons, which
1558460	1560100	were bad for all of us.
1560100	1563580	We're all in the same boat with respect to the existential threat, so we all ought to
1563580	1566460	be able to cooperate on trying to stop it.
1566460	1569940	As long as we can make some money on the way.
1569940	1574180	I'm going to take some audience questions from the room if you make yourself known, and
1574180	1577620	while people are going around with the microphone, there's one question I was going to ask from
1577620	1578740	the online audience.
1578740	1580100	I'm interested.
1580100	1586140	You mentioned a little bit about maybe a transition period as machines get smarter and outpace
1586140	1587140	humans.
1587140	1592060	There'll be a moment where it's hard to define what's human and what isn't, or are these
1592060	1595300	two very distinct forms of intelligence?
1595300	1597620	I think they're distinct forms of intelligence.
1597820	1602980	Now, of course, the digital intelligences are very good at mimicking us because they've
1602980	1606740	been trained to mimic us.
1606740	1612780	It's very hard to tell if chatGBT wrote it, or whether we wrote it.
1612780	1618100	In that sense, they look quite like us, but inside they're not working the same way.
1618100	1620020	Who is first in the room?
1620020	1622540	Hello.
1622540	1625860	My name is Hal Gregerson, and my middle name is not 9000.
1626500	1631180	I'm a faculty over in the MIT Sloan School.
1631180	1637500	Arguably asking questions is one of the most important human abilities we have.
1637500	1646620	From your perspective now in 2023, what question or two should we pay most attention to?
1646620	1653620	And is it possible for these technologies to actually help us ask better questions and
1653660	1656220	outquestion the technology?
1656220	1662740	Yes, but what I'm saying is there's many questions we should be asking, but one of
1662740	1665700	them is how do we prevent them from taking over?
1665700	1668460	How do we prevent them from getting control?
1668460	1677540	And we could ask them questions about that, but I wouldn't entirely trust their answers.
1677540	1678540	Question at the back.
1678540	1682100	And I want to get through as many as we can, so if you can keep your question as short
1682100	1683100	as possible.
1683100	1688860	Dr. Hinton, thank you so much for being here with us today.
1688860	1694860	I shall say this is the most expensive lecture I've ever paid for, but I think it was worthwhile.
1694860	1703860	I just have a question for you, because you mentioned the analogy of nuclear history,
1703860	1706300	and obviously there's a lot of comparisons.
1706300	1711860	By any chance do you remember what President Truman told Oppenheimer when he was in the
1711860	1712860	Oval Office?
1712860	1713860	No, I don't.
1713860	1718860	I know something about that, but I don't know what Truman told Oppenheimer.
1718860	1719860	Thank you.
1719860	1720860	Tell us.
1720860	1724060	We'll take it from here.
1724060	1725060	Next audience question.
1725060	1729700	Sorry, if there are people that might, let me know who's next.
1729700	1731700	Maybe you give a...
1731700	1732700	Go ahead.
1732700	1734860	Hello, Jacob Woodruff.
1734860	1740300	With the amount of data that's been required to train these large language models, would
1740300	1746740	we expect a plateau in the intelligence of these systems, and how might that slow down
1746740	1748980	or restrict the advancement?
1748980	1753980	Okay, so that is a ray of hope that maybe we've just used up all human knowledge and
1753980	1755940	they're not going to get any smarter.
1755940	1759900	But think about images and video.
1759900	1766540	So multimodal models will be much smarter than models that just train on language alone.
1766540	1770980	They'll have a much better idea of how to deal with space, for example.
1770980	1776460	And in terms of the amount of total video, we still don't have very good ways of processing
1776460	1780060	video in these models of modeling video.
1780060	1781740	We're getting better all the time.
1781740	1786860	But I think there's plenty of data and things like video that tell you how the world works.
1786860	1793180	So we're not hitting the data limits for multimodal models yet.
1793180	1795100	Next gentleman on the back.
1795460	1796940	Please do keep your question short.
1796940	1800500	Hello, Dr. Hindul Rajeev Sehwabal from PWC.
1800500	1806380	The point that I wanted to understand is that everything that AI is doing is learning from
1806380	1812220	what we are teaching them, okay, data, yes, they are fast-read learning, how one trillion
1812220	1815660	connectors can do much more than 100 trillion connectors that we have.
1815660	1820500	But every piece of human evolution has been driven by thought experiments.
1820500	1824180	Like Einstein used to do thought experiments because there was no speed of light out here
1824180	1825540	on this planet.
1825540	1830820	How can AI get to that point, if at all, and if it cannot, then how can we possibly have
1830820	1834900	an existential threat from them because they will not be self-learning, so to say.
1834900	1839260	They will be self-learning limited to the model that we tell them.
1839260	1842740	I think that's a very interesting argument.
1842740	1845460	But I think they will be able to do thought experiments.
1845460	1847100	I think they'll be able to reason.
1847100	1849260	So let me give you an analogy.
1849260	1856220	If you take alpha zero, which plays chess, it has three ingredients.
1856220	1860620	It's got something that evaluates the board position to say, is that good for me?
1860620	1864140	It's got something that looks at the board position and says, what's the sensible move
1864140	1865860	to consider?
1865860	1869380	And then it's got Monte Carlo rollout, where it does what's called calculation where you
1869380	1873300	think, if I go here and he goes there and I go here and he goes there.
1873300	1879220	Now suppose you leave out the Monte Carlo rollout and you just train it from human experts
1879380	1884340	to have a good evaluation function and a good way to choose moves to consider.
1884340	1887100	It still plays a pretty good game of chess.
1887100	1890540	And I think that's what we've got with the chatbots.
1890540	1895460	And we haven't got them doing internal reasoning, but that will come.
1895460	1899380	And once they start doing internal reasoning to check for the consistency between the different
1899380	1903260	things they believe, then they'll get much smarter and they will be able to do thought
1903260	1904500	experiments.
1904500	1911260	And one reason they haven't got this internal reasoning is because they've been trained from
1911260	1912580	inconsistent data.
1912580	1917260	And so it's very hard for them to do reasoning because they've been trained on all these inconsistent
1917260	1918260	beliefs.
1918260	1925380	And I think they're going to have to be trained so they say, if I have this ideology, then
1925380	1926380	this is true.
1926380	1928100	And if I have that ideology, then that is true.
1928100	1931420	And once they're trained like that, within an ideology, they're going to be able to try
1931420	1932420	and get consistency.
1933100	1938540	And so we're going to get a move like from a version of AlphaZero that just has something
1938540	1943700	that guesses good moves and something that evaluates positions to a version that has
1943700	1947620	long chains of Monte Carlo rollout, which is the corner of reasoning.
1947620	1950220	And it's going to get much better.
1950220	1951860	I'm going to take one in the front here.
1951860	1955260	And then if you can be quick, we'll run and squeeze one more in as well.
1955260	1958100	Louis Lamb, Jeff, I know you from a long time.
1958100	1964940	Jeff, people criticize language models because of allegedly they are lacking semantics and
1964940	1966620	grounding to the world.
1966620	1971700	And you have been trying to, as well, to explain how neural networks work for a long time.
1971700	1977140	Is the question of semantics and explainability relevant here or language models have taken
1977140	1984740	over and we are now doomed to go forward without semantics or grounding to reality?
1984780	1990660	I find it very hard to believe that they don't have semantics when they can solve problems
1990660	1993980	like, you know, how I paint the rooms, how I get all the rooms in my house to be painted
1993980	1996260	white in two years time.
1996260	2000180	I mean, whatever semantic is, it's to do with the meaning of that stuff.
2000180	2002100	And it understood the meaning.
2002100	2002980	It got it.
2002980	2009780	Now, I agree it's not grounded by being a robot, but you can make multimodal ones that
2009780	2011900	are grounded, Google's done that.
2011900	2016780	And the multimodal ones that are grounded, you can say, please close the drawer and they
2016780	2019500	reach out and grab the handle and close the drawer.
2019500	2021900	And it's very hard to say that doesn't have semantics.
2021900	2027780	In fact, in the very early days of AI, in the days of Willigrad in the 1970s, they had
2027780	2033060	just a simulated world, but they have what was called procedural semantics, where if
2033060	2039540	you said to it, put the red box in, put the red block in the green box, and it put the
2039540	2040780	red block in the green box.
2040780	2043180	She said, see, it understood the language.
2043180	2048380	And that was the criterion people used back then, but now that neural nets can do it,
2048380	2052020	they say that's not an analytical criterion.
2052020	2053020	One at the back.
2053020	2056700	Hey, Jeff, this is Ishwar Balani from SAI Group.
2056700	2061620	So clearly, the technology is advancing at an exponential pace.
2061620	2066580	I wanted to get your thoughts, if you looked at the near and medium term, say one, two,
2066580	2073300	three, or maybe five year horizon, what the social and economic implications are from
2073300	2077900	a societal perspective with job loss or maybe new jobs being created.
2077900	2082860	Just wanted to get your thoughts on how we proceed, given the state of the technology
2082860	2083860	and rate of change.
2083860	2084860	Yes.
2084860	2091620	So the alarm bell I'm ringing is to do with the existential threat of them taking control.
2091620	2095220	Lots of other people have talked about that, and I don't consider myself to be an expert
2095220	2096220	on that.
2096220	2101460	But there's some very obvious things that they're going to make a whole bunch of jobs
2101460	2103460	much more efficient.
2103460	2108460	So I know someone who answers letters of complaint to a health service, and he used to take 25
2108460	2112140	minutes writing a letter, and now it takes him five minutes because he gives it to chat
2112140	2116580	GPT and chat GPT writes the letter for him, and then he just checks it.
2116580	2122580	There'll be lots of stuff like that, which is going to cause huge increases in productivity.
2122580	2126260	There will be delays because people are very conservative about adopting new technology,
2126260	2129140	but I think there's going to be huge increases in productivity.
2129140	2133980	My worry is that those increases in productivity are going to go to putting people out of work
2133980	2137100	and making the rich richer and the poor poorer.
2137100	2142100	And as you do that, as you make that gap bigger, society gets more and more violent.
2142100	2148380	This thing called the genie index, which predicts quite well how much violence there is.
2148380	2155300	So this technology, which ought to be wonderful, even the good uses of technology for doing
2155300	2160700	helpful things ought to be wonderful, but our current political systems is going to
2160700	2164020	be used to make the rich richer and the poor poorer.
2164020	2172940	You might be able to ameliorate that by having a basic income that everybody gets, but the
2172940	2182380	technology is being developed in a society that is not designed to use it for everybody's good.
2182380	2189980	A question here from Joe Castalda of the Globe and Mail, who's in the audience.
2189980	2195980	Do you intend to hold on to your investments in Cahir and other companies, and if so, why?
2196980	2203980	Well, I could take the money and I could put it in the bank and let them profit from it.
2203980	2213980	Yes, I'm going to hold on to my investments in Cahir, partly because the people around Cahir are friends of mine.
2213980	2220980	I sort of believe these big language balls are going to be very helpful.
2221980	2229980	I think the technology should be good and it should make things work better.
2229980	2234980	It's the politics we need to fix for things like employment.
2234980	2240980	But when it comes to the existential threat, we have to think how we can keep control of the technology.
2240980	2245980	But the good news there is that we're all in the same boat, so we might be able to get cooperation.
2245980	2253980	And in speaking out, part of your thinking, as I understand it, is that you actually want to engage with the people making this technology
2253980	2260980	and change their minds or maybe make a case for, I don't really know.
2260980	2265980	We've established that we don't really know what to do, but it's about engaging rather than stepping back.
2265980	2272980	So one of the things that made me leave Google and go public with this is to say,
2272980	2281980	he used to be a junior professor, but he's now a middle rank professor who I think very highly of, who encouraged me to do this.
2281980	2284980	He said, Jeff, you need to speak out there, listen to you.
2284980	2293980	People are just blind to this danger and I think people are listening now.
2293980	2297980	Yeah, no, I think everyone in this room is listening for a start.
2297980	2304980	Just one last question and we're out of time, but do you have regrets that you're involved in making this?
2304980	2308980	Cade Met's tried very hard to get me to say I had regrets.
2308980	2310980	Cade Met's at the New York Times.
2310980	2318980	And yes, and in the end, I said, well, maybe slight regrets, which got reported as has regrets.
2318980	2322980	I don't think I made any bad decisions in doing research.
2322980	2329980	I think it was perfectly reasonable back in the 70s and 80s to do research on how to make artificial neural nets.
2329980	2331980	It wasn't really foreseeable.
2331980	2333980	This stage of it wasn't foreseeable.
2333980	2338980	And until very recently, I thought this existential crisis was a long way off.
2338980	2343980	So I don't really have any regrets about what I did.
2343980	2344980	Thank you, Jeffrey.
2344980	2346980	Thank you so much for joining us.
2352980	2354980	Thank you.
