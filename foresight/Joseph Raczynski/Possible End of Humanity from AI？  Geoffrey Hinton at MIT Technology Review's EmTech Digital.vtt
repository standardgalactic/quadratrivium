WEBVTT

00:00.000 --> 00:11.200
Hi, everyone.

00:11.200 --> 00:12.200
Welcome back.

00:12.200 --> 00:13.640
Hope you had a good lunch.

00:13.640 --> 00:19.120
My name is Ward Douglas-Heaven, senior editor for AI at MIT Technology Review, and I think

00:19.120 --> 00:23.120
we all agree there's no denying that generative AI is the thing of the moment.

00:23.120 --> 00:26.560
But innovation does not stand still, and in this chapter we're going to take a look at

00:26.560 --> 00:30.600
cutting-edge research that is already pushing ahead and asking what's next.

00:30.600 --> 00:35.200
But starting us off, I'd like to introduce a very special speaker who will be joining

00:35.200 --> 00:36.840
us virtually.

00:36.840 --> 00:42.560
Jeffrey Hinton is Professor Emeritus at University of Toronto and, until this week, an engineering

00:42.560 --> 00:43.560
fellow at Google.

00:43.560 --> 00:49.120
But on Monday he announced that after 10 years he will be stepping down.

00:49.120 --> 00:52.200
Jeffrey is one of the most important figures in modern AI.

00:52.200 --> 00:56.080
He's a pioneer of deep learning, developing some of the most fundamental techniques that

00:56.080 --> 01:00.640
underpin AI as we know it today, such as backpropagation, the algorithm that allows

01:00.640 --> 01:03.840
machines to learn.

01:03.840 --> 01:09.000
This technique is the foundation on which pretty much all of deep learning rests today.

01:09.000 --> 01:13.240
In 2018, Jeffrey received the Turing Award, which is often called the Nobel of Computer

01:13.240 --> 01:18.440
Science, alongside Jan LeCun and Yoshio Benjio.

01:18.440 --> 01:22.920
He's here with us today to talk about intelligence, what it means, and where attempts to build

01:22.920 --> 01:25.200
it into machines will take us.

01:25.320 --> 01:28.240
Jeffrey, welcome to MTech.

01:28.240 --> 01:29.240
Thank you.

01:29.240 --> 01:30.240
How's your week going?

01:30.240 --> 01:31.240
Busy few days, I imagine.

01:31.240 --> 01:35.880
Well, the last 10 minutes was horrible because my computer crashed and I had to find another

01:35.880 --> 01:38.440
computer and connect it up.

01:38.440 --> 01:39.440
And we're glad you're back.

01:39.440 --> 01:44.120
That's the kind of technical detail we're not supposed to share with the audience.

01:44.120 --> 01:45.360
It's great you're here.

01:45.360 --> 01:47.080
Very happy that you could join us.

01:47.080 --> 01:52.040
Now, I mean, it's been the news everywhere that you stepped down from Google this week.

01:52.360 --> 01:56.560
Could you start by telling us why you made that decision?

01:56.560 --> 01:58.160
Well, there were a number of reasons.

01:58.160 --> 02:00.680
There's always a bunch of reasons for a decision like that.

02:00.680 --> 02:07.320
One was that I'm 75 and I'm not as good at doing technical work as I used to be.

02:07.320 --> 02:11.440
My memory is not as good and when I program, I forget to do things.

02:11.440 --> 02:13.760
So it was time to retire.

02:13.760 --> 02:20.360
A second was, very recently, I've changed my mind a lot about the relationship between

02:20.360 --> 02:25.080
the brain and the kind of digital intelligence we're developing.

02:25.080 --> 02:30.960
So I used to think that the computer models we were developing weren't as good as the

02:30.960 --> 02:35.520
brain and the aim was to see if you could understand more about the brain by seeing

02:35.520 --> 02:39.400
what it takes to improve the computer models.

02:39.400 --> 02:44.640
Over the last few months, I've changed my mind completely and I think probably the computer

02:44.640 --> 02:47.280
models are working in a rather different way from the brain.

02:47.280 --> 02:50.600
They're using backpropagation and I think the brain's probably not.

02:50.600 --> 02:54.320
And a couple of things have led me to that conclusion, but one is the performance of

02:54.320 --> 02:56.600
things like GPT-4.

02:56.600 --> 03:02.520
So I want to get on to the performance of GPT-4 very much in a minute, but let's go back

03:02.520 --> 03:08.320
so that we all understand the argument you're making and tell us a little bit about what

03:08.320 --> 03:09.560
backpropagation is.

03:09.560 --> 03:16.040
This is an algorithm that you developed with a couple of colleagues back in the 1980s.

03:16.040 --> 03:19.720
Many different groups discovered backpropagation.

03:19.720 --> 03:26.520
The special thing we did was used it and showed that it could develop good internal representations.

03:26.520 --> 03:35.440
And curiously, we did that by implementing a tiny language model.

03:35.440 --> 03:44.040
It had embedding vectors that were only six components and the training set was 112 cases.

03:44.040 --> 03:45.040
But it was a language model.

03:45.040 --> 03:50.640
And it was trying to predict the next term in a string of symbols.

03:50.640 --> 03:56.360
And about 10 years later, Yoshua Benjo took basically the same net and used it on natural

03:56.360 --> 04:02.000
language and showed it actually worked for natural language if you made it much bigger.

04:02.000 --> 04:09.520
But the way backpropagation works, I can give you a rough explanation from it of it.

04:09.520 --> 04:15.400
People who know how it works can sit back and feel smug and laugh at the way I'm presenting

04:15.400 --> 04:16.400
it.

04:16.400 --> 04:20.440
Because I'm a bit worried about them.

04:20.440 --> 04:25.640
So imagine you wanted to detect birds in images.

04:25.640 --> 04:32.080
So an image, let's suppose it was 100 pixel by 100 pixel image, that's 10,000 pixels and

04:32.080 --> 04:38.640
each pixel is three channels, RGB, so that's 30,000 numbers, the intensity in each channel

04:38.640 --> 04:41.000
in each pixel, the represents the image.

04:41.000 --> 04:46.040
And the way to think of the computer vision problem is, how do I turn those 30,000 numbers

04:46.040 --> 04:49.440
into a decision about whether it's a bird or not?

04:49.440 --> 04:53.360
And people tried for a long time to do that and they weren't very good at it.

04:53.360 --> 04:56.240
But here's a suggestion for how you might do it.

04:56.240 --> 05:01.840
You might have a layer of feature detectors that detects very simple features in images,

05:01.840 --> 05:03.560
like for example, edges.

05:03.560 --> 05:10.640
So a feature detector might have big positive weights to a column of pixels and then big

05:10.640 --> 05:14.120
negative weights to the neighbouring column of pixels.

05:14.120 --> 05:18.040
So if both columns are bright, it won't turn on and if both columns are dim, it won't turn

05:18.040 --> 05:19.040
on.

05:19.040 --> 05:23.440
But if the column on one side is bright and the column on the other side is dim, it'll

05:23.440 --> 05:26.880
get very excited and that's an edge detector.

05:26.880 --> 05:30.920
So I just told you how to wire up an edge detector by hand by having one column of big

05:30.920 --> 05:33.280
positive weights and next to it, one column of big negative weights.

05:33.280 --> 05:37.760
And we can imagine a big layer of those detecting edges in different orientations and different

05:37.760 --> 05:39.800
scales all over the image.

05:39.800 --> 05:41.880
We'd need a rather large number of them.

05:41.880 --> 05:45.480
And edges in an image, you mean just lines, sort of edges of a shape?

05:45.480 --> 05:52.280
A place where the intensity changes from bright to dark, yeah, just that.

05:52.280 --> 05:57.360
Then we might have a layer of feature detectors above that that detects combinations of edges.

05:57.360 --> 06:04.320
So for example, we might have something that detects two edges that join at a fine angle

06:04.320 --> 06:07.280
like this.

06:07.280 --> 06:12.160
So it'll have a big positive weight to each of those two edges and if both of those edges

06:12.160 --> 06:15.120
are there at the same time, it'll get excited.

06:15.120 --> 06:18.520
And that would detect something that might be a bird's beak.

06:18.520 --> 06:20.560
It might not, but it might be a bird's beak.

06:20.560 --> 06:25.000
You might also in that layer have a feature detector that would detect a whole bunch of

06:25.000 --> 06:29.080
edges arranged in a circle.

06:29.080 --> 06:30.440
And that might be a bird's eye.

06:30.440 --> 06:31.720
It might be all sorts of other things.

06:31.720 --> 06:36.720
It might be a knob on a fridge or something.

06:36.720 --> 06:42.200
Then in a third layer, you might have a feature detector that detects this potential beak and

06:42.200 --> 06:47.480
detects a potential eye and is wired up so that it'll like a beak and an eye in the right

06:47.480 --> 06:49.040
spatial relation to one another.

06:49.040 --> 06:53.320
And if it sees that, it says, ah, this might be the head of a bird.

06:53.320 --> 06:57.240
And you can imagine if you keep wiring like that, you could eventually have something

06:57.240 --> 06:59.680
that detects a bird.

06:59.680 --> 07:04.800
But wiring all that up by hand would be very, very difficult, deciding on what should be

07:04.800 --> 07:06.680
connected to what and what the weight should be.

07:06.680 --> 07:10.520
And it would be especially difficult because you want these sort of intermediate layers

07:10.520 --> 07:16.080
to be good not just for detecting birds, but for detecting all sorts of other things.

07:16.080 --> 07:20.920
So it would be more or less impossible to wire it up by hand.

07:20.920 --> 07:23.080
So the way back propagation works is this.

07:23.080 --> 07:25.040
You start with random weights.

07:25.040 --> 07:28.720
So these feature detectors are just complete rubbish.

07:28.720 --> 07:34.200
And you put in a picture of a bird, and at the output, it says, like, 0.5, it's a bird.

07:34.200 --> 07:37.440
I suppose you only have birds on onwards.

07:37.440 --> 07:40.080
And then you ask yourself the following question.

07:40.080 --> 07:46.000
How could I change each of the weights in the network, each of the weights on connections

07:46.000 --> 07:52.920
in the network, so that instead of saying 0.5, it says 0.501, that it's a bird, and 0.499,

07:52.920 --> 07:55.120
that it's not.

07:55.120 --> 08:00.080
And you change the weights in the directions that will make it more likely to say that a

08:00.080 --> 08:05.360
bird is a bird, and less likely to say that a non-bird is a bird.

08:05.360 --> 08:07.040
And you just keep doing that.

08:07.040 --> 08:08.040
And that's back propagation.

08:08.040 --> 08:13.200
Back propagation is actually how you take the discrepancy between what you want, which

08:13.200 --> 08:17.080
is a probability of 1, that's a bird, and what it's got at present, which is probability

08:17.080 --> 08:22.000
of 0.5, that it's a bird, how you take that discrepancy and send it backwards through the

08:22.000 --> 08:28.360
network, so that you can compute for every feature detector in the network, whether you'd

08:28.360 --> 08:31.040
like it to be a bit more active or a bit less active.

08:31.040 --> 08:34.120
And once you've computed that, if you know you want a feature detector to be a bit more

08:34.120 --> 08:38.600
active, you can increase the weights coming from feature detectors in the layer below

08:38.600 --> 08:44.200
that are active, and maybe put in some negative weights to feature detectors in the layer

08:44.200 --> 08:48.600
below that are off, and now you'll have a better detector.

08:48.600 --> 08:51.640
So back propagation is just going backwards through the network to figure out for each

08:51.640 --> 08:55.640
feature detector, whether you want it a little bit more active or a little bit less active.

08:55.640 --> 08:56.640
Thank you.

08:56.640 --> 08:57.640
I can show it.

08:57.640 --> 09:03.200
There's no one in the audience here that's smiling and thinking that was a silly explanation.

09:03.200 --> 09:09.840
So let's fast forward quite a lot to, you know, that technique basically performed really

09:09.840 --> 09:15.680
well on ImageNet, and we had Joel Pino from Meta yesterday showing how far image detection

09:15.680 --> 09:21.320
had come, and it's also the technique that underpins large language models.

09:21.320 --> 09:30.120
So I want to talk now about this technique, which you initially were thinking of as almost

09:30.120 --> 09:37.280
like a poor approximation of what biological brains might do, has turned out to do things

09:37.280 --> 09:41.480
which I think have stunned you, particularly in large language models.

09:41.480 --> 09:48.600
So talk to us about why that sort of amazement that you have with today's large language

09:48.600 --> 09:54.280
models has completely sort of almost flipped your thinking of what back propagation or

09:54.280 --> 09:57.560
machine learning in general is.

09:57.560 --> 10:03.640
So if you look at these large language models, they have about a trillion connections, and

10:03.640 --> 10:07.760
things like GP24 know much more than we do.

10:07.760 --> 10:13.520
They have sort of common sense knowledge about everything, and so they probably know a thousand

10:13.520 --> 10:18.520
times as much as a person, but they've got a trillion connections and we've got a hundred

10:18.520 --> 10:20.640
trillion connections.

10:20.640 --> 10:25.200
So they're much, much better at getting a lot of knowledge into only a trillion connections

10:25.200 --> 10:31.000
than we are, and I think it's because back propagation may be a much, much better learning

10:31.000 --> 10:33.280
algorithm than what we've got.

10:33.280 --> 10:34.280
Can you define that?

10:34.280 --> 10:35.280
That's scary.

10:35.280 --> 10:37.000
Yeah, I definitely want to get onto the scary stuff.

10:37.000 --> 10:40.080
So what do you mean by better?

10:40.080 --> 10:45.560
It can pack more information into only a few connections, which are finding a trillion

10:45.560 --> 10:48.440
as only a few.

10:48.440 --> 10:57.760
So these digital computers are better at learning than humans, which itself is a huge claim,

10:57.760 --> 11:03.280
but then you also argue that that's something that we should be scared of.

11:03.280 --> 11:05.520
So could you take us through that step of the argument?

11:06.000 --> 11:14.680
Yeah, let me give you a separate piece of the argument, which is that if a computer's

11:14.680 --> 11:20.000
digital, which involves very high energy costs and very careful fabrication, you can have

11:20.000 --> 11:24.040
many copies of the same model running on different hardware that do exactly the same

11:24.040 --> 11:25.040
thing.

11:25.040 --> 11:28.040
They can look at different data, but the model is exactly the same.

11:28.040 --> 11:33.760
And what that means is, especially of 10,000 copies, they can be looking at 10,000 different

11:33.760 --> 11:35.920
subsets of the data.

11:35.920 --> 11:39.960
And whenever one of them learns anything, all the others know it.

11:39.960 --> 11:44.640
One of them figures out how to change the weights so it can deal with this data.

11:44.640 --> 11:48.040
They all communicate with each other, and they all agree to change the weights by the

11:48.040 --> 11:51.000
average of what all of them want.

11:51.000 --> 11:59.200
And now the 10,000 things are communicating very effectively with each other so that they

11:59.200 --> 12:03.240
can see 10,000 times as much data as one agent could.

12:03.240 --> 12:05.080
But people can't do that.

12:05.080 --> 12:08.840
If I learn a whole lot of stuff about quantum mechanics, and I want you to know all that

12:08.840 --> 12:13.320
stuff about quantum mechanics, it's a long, painful process of getting you to understand

12:13.320 --> 12:14.320
it.

12:14.320 --> 12:19.560
I can't just copy my weights into your brain, because your brain isn't exactly the same

12:19.560 --> 12:20.560
as mine.

12:20.560 --> 12:24.560
No, it's not.

12:24.560 --> 12:29.060
It's younger.

12:30.020 --> 12:39.100
We have digital computers that can learn more things more quickly, and they can instantly

12:39.100 --> 12:42.180
teach it to each other.

12:42.180 --> 12:46.220
People in the room here could instantly transfer what they had in their heads into mine.

12:46.220 --> 12:50.460
But why is that scary?

12:50.460 --> 12:56.860
Because they can learn so much more, and they might take an example of a doctor, and imagine

12:56.860 --> 13:03.460
you have one doctor who's seen 1,000 patients, and another doctor who's seen 100 million

13:03.460 --> 13:05.660
patients.

13:05.660 --> 13:10.340
You would expect the doctor who's seen 100 million patients, if he's not too forgetful,

13:10.340 --> 13:14.180
to have noticed all sorts of trends in the data that just aren't visible if you're only

13:14.180 --> 13:16.380
seen 1,000 patients.

13:16.380 --> 13:19.780
You may have only seen one patient with some rare disease.

13:19.780 --> 13:23.340
The other doctor who's seen 100 million will have seen, well, you can figure out how many

13:23.340 --> 13:25.980
patients, but a lot.

13:26.100 --> 13:32.860
We'll see all sorts of regularities that just aren't apparent in small data.

13:32.860 --> 13:38.140
That's why things that can get through a lot of data can probably see structure in data

13:38.140 --> 13:41.100
that we'll never see.

13:41.100 --> 13:49.100
Then take me to the point where I should be scared of this, though.

13:49.100 --> 13:53.140
If you look at GPT-4, it can already do simple reasoning.

13:53.860 --> 14:00.140
Reasoning is the area where we're still better, but I was impressed the other day at GPT-4

14:00.140 --> 14:05.180
doing a piece of common sense reasoning that I didn't think it would be able to do.

14:05.180 --> 14:10.860
I asked it, I want all the rooms in my house to be white.

14:10.860 --> 14:16.900
At present, there's some white rooms, some blue rooms, and some yellow rooms, and yellow

14:16.900 --> 14:20.100
paint fades to white within a year.

14:20.100 --> 14:25.460
What should I do if I want them all to be white in two years' time?

14:25.460 --> 14:29.300
It said you should paint the blue rooms yellow.

14:29.300 --> 14:34.180
That's not the natural solution, but it works, right?

14:34.180 --> 14:38.700
That's pretty impressive common sense reasoning of the kind that it's been very hard to get

14:38.700 --> 14:44.620
AI to do using symbolic AI, because it had to understand what fades means.

14:44.620 --> 14:50.820
It had to understood by temple stuff.

14:50.820 --> 15:00.380
They're doing sensible reasoning with an IQ of 80 or 90 or something.

15:00.380 --> 15:08.220
As a friend of mine said, it's as if some genetic engineers have said, we're going to

15:08.220 --> 15:09.980
improve grizzly bears.

15:09.980 --> 15:14.340
We've already improved them to have an IQ of 65, and they can talk English now.

15:14.340 --> 15:24.020
They're very useful for all sorts of things, but we think we can improve the IQ to 210.

15:24.020 --> 15:25.020
I certainly have.

15:25.020 --> 15:30.500
I'm sure many people have had that feeling when you're interacting with these latest

15:30.500 --> 15:35.020
chatbots, sort of hair on the back of the neck, sort of uncanny feeling.

15:35.020 --> 15:39.260
When I have that feeling and I'm uncomfortable, I just close my laptop.

15:40.260 --> 15:47.460
Yes, but these things will have learned from us by reading all the novels that ever were

15:47.460 --> 15:56.060
and everything Machiavelli ever wrote, that how to manipulate people, right?

15:56.060 --> 15:58.980
If they're much smarter than us, they'll be very good at manipulating us.

15:58.980 --> 16:00.500
You won't realize what's going on.

16:00.500 --> 16:05.060
You'll be like a two-year-old who's being asked, do you want the peas or the cauliflower

16:05.060 --> 16:08.860
and doesn't realize you don't have to have either.

16:08.860 --> 16:12.580
And you'll be that easy to manipulate.

16:12.580 --> 16:18.620
And so even if they can't directly pull levers, they can certainly get us to pull levers.

16:18.620 --> 16:23.420
It turns out if you can manipulate people, you can invade a building in Washington without

16:23.420 --> 16:28.860
ever going there yourself.

16:28.860 --> 16:29.860
Very good.

16:29.860 --> 16:30.860
Yeah.

16:30.860 --> 16:36.980
So is that, is that, I mean, if the word, okay, this is a very hypothetical world, but

16:36.980 --> 16:45.700
if there were no bad actors, you know, people with bad intentions, would we be safe?

16:45.700 --> 16:46.700
I don't know.

16:46.700 --> 16:52.500
We'd be safer in a world where people have bad intentions and where the political system

16:52.500 --> 16:59.020
is so broken that we can't even decide not to give assault rifles to teenage boys.

16:59.020 --> 17:01.980
If you can't solve that problem, how are you going to solve this problem?

17:02.980 --> 17:03.980
Well, I mean, I don't know.

17:03.980 --> 17:07.340
I was hoping that you would have some thoughts.

17:07.340 --> 17:15.060
You've, so one, I mean, unless we didn't make this clear at the beginning, I mean, you've

17:15.060 --> 17:20.700
won to speak out about this and you feel more comfortable doing that without it sort of having

17:20.700 --> 17:24.660
any blowback on Google.

17:24.660 --> 17:30.380
But you're speaking out about it, but in some sense talk is cheap if we then don't have,

17:30.380 --> 17:35.700
you know, actions or what do we do when we lots of people this week are listening to

17:35.700 --> 17:36.700
you?

17:36.700 --> 17:38.180
What should we do about it?

17:38.180 --> 17:43.340
I wish it was like climate change where you could say, if you've got half a brain, you'd

17:43.340 --> 17:46.380
stop burning carbon.

17:46.380 --> 17:51.820
It's clear what you should do about it is clear that's painful, but has to be done.

17:51.820 --> 17:55.820
I don't know of any solution like that to stop these things taking over from us.

17:55.820 --> 17:58.900
What we really want, I don't think we're going to stop developing them because they're so

17:58.900 --> 17:59.900
useful.

17:59.940 --> 18:04.220
They'll be incredibly useful in medicine and in everything else.

18:04.220 --> 18:07.060
So I don't think there's much chance of stopping development.

18:07.060 --> 18:13.060
What we want is some way of making sure that even if they're smarter than us, they're going

18:13.060 --> 18:15.100
to do things that are beneficial for us.

18:15.100 --> 18:16.940
That's called the alignment problem.

18:16.940 --> 18:21.980
But we need to try and do that in a world where there's bad actors who want to build

18:21.980 --> 18:25.100
robot soldiers that kill people.

18:25.100 --> 18:27.060
And it seems very hard to me.

18:27.060 --> 18:28.060
So I'm sorry.

18:28.060 --> 18:32.380
I'm sounding the alarm and saying, we have to worry about this, and I wish I had a nice

18:32.380 --> 18:34.380
simple solution I could push, but I don't.

18:34.380 --> 18:37.660
But I think it's very important that people get together and think hard about it and see

18:37.660 --> 18:39.020
whether there is a solution.

18:39.020 --> 18:41.180
It's not clear there is a solution.

18:41.180 --> 18:44.260
So talk to us about that.

18:44.260 --> 18:49.620
You spent your career on the technicalities of this technology.

18:49.620 --> 18:51.580
Is there no technical fix?

18:51.580 --> 18:57.020
Why can we not build in guardrails or make them worse at learning?

18:57.980 --> 19:04.100
Or restrict the way that they can communicate, if those are the two strings of your argument?

19:04.100 --> 19:07.820
We're trying to do all sorts of guardrails.

19:07.820 --> 19:11.500
But suppose they did get really smart, and these things can program, right?

19:11.500 --> 19:12.500
They can write programs.

19:12.500 --> 19:20.260
And suppose you give them the ability to execute those programs, which we'll certainly do.

19:20.260 --> 19:23.980
Smart things can outsmart us.

19:23.980 --> 19:31.620
So imagine you're two-year-old saying, my dad does things I don't like, so I'm going

19:31.620 --> 19:34.700
to make some rules for whatever my dad can do.

19:34.700 --> 19:39.300
You could probably figure out how to live with those rules and still get what you want.

19:39.300 --> 19:41.300
Yeah.

19:41.300 --> 19:50.540
But there still seems to be a step where these smart machines somehow have motivation of their

19:50.540 --> 19:51.540
own.

19:51.540 --> 19:52.540
Yes.

19:52.540 --> 19:53.540
Yes, that's a very good point.

19:54.020 --> 20:00.300
We evolved, and because we evolved, we have certain built-in goals that we find very hard

20:00.300 --> 20:07.140
to turn off, like we try not to damage our bodies, that's what pain is about.

20:07.140 --> 20:12.660
We try and get enough to eat so we feed our bodies.

20:12.660 --> 20:18.980
We try and make as many copies of ourselves as possible, maybe not deliberately that intention,

20:18.980 --> 20:24.060
but we've been wired up so there's pleasure involved in making many copies of ourselves.

20:24.060 --> 20:30.740
And that all came from evolution, and it's important that we can't turn it off.

20:30.740 --> 20:34.620
If you could turn it off, you don't do so well.

20:34.620 --> 20:37.820
There's a wonderful group called the Shakers who related to the Quakers who made beautiful

20:37.820 --> 20:46.180
furniture but didn't believe in sex, and there aren't any of them around anymore.

20:46.180 --> 20:51.140
So these digital intelligences didn't evolve.

20:51.140 --> 20:55.860
We made them, and so they don't have these built-in goals.

20:55.860 --> 21:00.860
And so the issue is, if we can put the goals in, maybe it'll all be okay.

21:00.860 --> 21:06.540
But my big worry is, sooner or later, someone will wire into them the ability to create

21:06.540 --> 21:07.540
their own sub-goals.

21:07.540 --> 21:14.980
In fact, they almost have that already, the versions of chat GPT that call chat GPT.

21:14.980 --> 21:19.820
And if you give something the ability to create your own sub-goals in order to achieve other

21:19.820 --> 21:26.220
goals, I think it'll very quickly realise that getting more control is a very good sub-goal

21:26.220 --> 21:29.700
because it helps you achieve other goals.

21:29.700 --> 21:34.100
And if these things get carried away with getting more control, we're in trouble.

21:34.100 --> 21:39.180
So what's the worst-case scenario that you think is conceivable?

21:39.820 --> 21:45.900
Oh, I think it's quite conceivable that humanity is just a passing phase in the evolution of

21:45.900 --> 21:47.260
intelligence.

21:47.260 --> 21:49.220
You couldn't directly evolve digital intelligence.

21:49.220 --> 21:53.780
It requires too much energy and too much careful fabrication.

21:53.780 --> 21:59.460
You need biological intelligence to evolve so that it can create digital intelligence.

21:59.460 --> 22:06.820
The digital intelligence can then absorb everything people ever wrote in a fairly slow way, which

22:06.900 --> 22:10.580
is what chat GPT has been doing.

22:10.580 --> 22:15.340
But then it can start getting direct experience of the world and learn much faster.

22:15.340 --> 22:22.900
And it may keep us around for a while to keep the power stations running, but after that,

22:22.900 --> 22:23.900
maybe not.

22:23.900 --> 22:29.820
So the good news is we figured out how to build beings that are immortal.

22:29.820 --> 22:34.540
So these digital intelligences, when a piece of hardware dies, they don't die.

22:34.580 --> 22:39.140
If you've got the weight stored in some medium and you can find another piece of hardware

22:39.140 --> 22:44.620
that can run the same instructions, then you can bring it to life again.

22:44.620 --> 22:49.660
So we've got immortality, but it's not for us.

22:49.660 --> 22:53.140
So Ray Kurzweil is very interested in being immortal.

22:53.140 --> 22:57.700
I think it's a very bad idea for old white men to be immortal.

22:57.700 --> 23:01.620
We've got the immortality, but it's not for Ray.

23:01.900 --> 23:07.660
No, the scary thing is that, in a way, maybe you will be, because you invented much of

23:07.660 --> 23:11.780
this technology.

23:11.780 --> 23:14.980
When I hear you say this, part of me wants to run off the stage into the street now and

23:14.980 --> 23:19.580
start unplugging computers.

23:19.580 --> 23:21.380
I'm afraid we can't do that.

23:21.380 --> 23:22.380
Why?

23:22.380 --> 23:24.300
You sound like Hal from 2001.

23:24.300 --> 23:36.820
More seriously, I know you said before that it was suggested a few months ago that there

23:36.820 --> 23:45.060
should be a moratorium on AI advancement, and I don't think that's a very good idea.

23:45.060 --> 23:48.860
But more generally, I'm curious, why?

23:48.860 --> 23:51.300
Should we not just stop?

23:51.300 --> 23:58.700
I know that you've spoken also that you're an investor of your personal wealth in some

23:58.700 --> 24:01.260
companies like Coher that are building these large language models.

24:01.260 --> 24:06.420
I'm curious about your personal sense of responsibility and each of our personal responsibility.

24:06.420 --> 24:07.420
What should we be doing?

24:07.420 --> 24:11.260
I mean, should we try and stop this, is what I'm saying.

24:11.260 --> 24:16.580
I think if you take the existential risk seriously, as I now do, I used to think it was way off,

24:16.580 --> 24:20.340
but I now think it's serious and fairly close.

24:20.340 --> 24:25.740
It might be quite sensible to just stop developing these things any further, but I think it's

24:25.740 --> 24:28.780
completely naive to think that would happen.

24:28.780 --> 24:32.180
There's no way to make that happen.

24:32.180 --> 24:36.460
I mean, if the US stops developing and the Chinese won't, they're going to be used in

24:36.460 --> 24:41.700
weapons and just for that reason alone, governments aren't going to stop developing them.

24:41.700 --> 24:47.340
So yes, I think stopping developing them might be a rational thing to do, but there's no

24:47.340 --> 24:48.340
way it's going to happen.

24:48.340 --> 24:51.460
So it's silly to sign petitions saying, please stop now.

24:51.460 --> 24:52.780
We did have a holiday.

24:52.780 --> 24:58.900
We had a holiday from about 2017 for several years because Google developed the technology

24:58.900 --> 24:59.900
first.

24:59.900 --> 25:00.900
It developed the transform.

25:00.900 --> 25:06.020
We said it also developed the fusion models, and it didn't put them out there for people

25:06.020 --> 25:07.580
to use and abuse.

25:07.580 --> 25:10.540
It was very careful with them because it didn't want to damage his reputation and he knew

25:10.540 --> 25:16.380
there could be bad consequences, but that can only happen if there's a single leader.

25:16.380 --> 25:24.820
Once open AI had built similar things using transformers and money from Microsoft, and

25:24.820 --> 25:29.740
Microsoft decided to put it out there, Google didn't have really much choice.

25:29.740 --> 25:35.660
If you're going to live in a capitalist system, you can't stop Google competing with Microsoft.

25:35.660 --> 25:38.380
So I don't think Google did anything wrong.

25:38.380 --> 25:42.660
I think it was very responsible to begin with, but I think it's just inevitable in the capitalist

25:42.660 --> 25:46.580
system or a system with competition between countries like the US and China that this

25:46.580 --> 25:49.540
stuff will be developed.

25:49.540 --> 25:54.500
My one hope is that because if we allowed it to take over, it would be bad for all of

25:54.500 --> 25:58.460
us, we could get the US and China to agree like we could with nuclear weapons, which

25:58.460 --> 26:00.100
were bad for all of us.

26:00.100 --> 26:03.580
We're all in the same boat with respect to the existential threat, so we all ought to

26:03.580 --> 26:06.460
be able to cooperate on trying to stop it.

26:06.460 --> 26:09.940
As long as we can make some money on the way.

26:09.940 --> 26:14.180
I'm going to take some audience questions from the room if you make yourself known, and

26:14.180 --> 26:17.620
while people are going around with the microphone, there's one question I was going to ask from

26:17.620 --> 26:18.740
the online audience.

26:18.740 --> 26:20.100
I'm interested.

26:20.100 --> 26:26.140
You mentioned a little bit about maybe a transition period as machines get smarter and outpace

26:26.140 --> 26:27.140
humans.

26:27.140 --> 26:32.060
There'll be a moment where it's hard to define what's human and what isn't, or are these

26:32.060 --> 26:35.300
two very distinct forms of intelligence?

26:35.300 --> 26:37.620
I think they're distinct forms of intelligence.

26:37.820 --> 26:42.980
Now, of course, the digital intelligences are very good at mimicking us because they've

26:42.980 --> 26:46.740
been trained to mimic us.

26:46.740 --> 26:52.780
It's very hard to tell if chatGBT wrote it, or whether we wrote it.

26:52.780 --> 26:58.100
In that sense, they look quite like us, but inside they're not working the same way.

26:58.100 --> 27:00.020
Who is first in the room?

27:00.020 --> 27:02.540
Hello.

27:02.540 --> 27:05.860
My name is Hal Gregerson, and my middle name is not 9000.

27:06.500 --> 27:11.180
I'm a faculty over in the MIT Sloan School.

27:11.180 --> 27:17.500
Arguably asking questions is one of the most important human abilities we have.

27:17.500 --> 27:26.620
From your perspective now in 2023, what question or two should we pay most attention to?

27:26.620 --> 27:33.620
And is it possible for these technologies to actually help us ask better questions and

27:33.660 --> 27:36.220
outquestion the technology?

27:36.220 --> 27:42.740
Yes, but what I'm saying is there's many questions we should be asking, but one of

27:42.740 --> 27:45.700
them is how do we prevent them from taking over?

27:45.700 --> 27:48.460
How do we prevent them from getting control?

27:48.460 --> 27:57.540
And we could ask them questions about that, but I wouldn't entirely trust their answers.

27:57.540 --> 27:58.540
Question at the back.

27:58.540 --> 28:02.100
And I want to get through as many as we can, so if you can keep your question as short

28:02.100 --> 28:03.100
as possible.

28:03.100 --> 28:08.860
Dr. Hinton, thank you so much for being here with us today.

28:08.860 --> 28:14.860
I shall say this is the most expensive lecture I've ever paid for, but I think it was worthwhile.

28:14.860 --> 28:23.860
I just have a question for you, because you mentioned the analogy of nuclear history,

28:23.860 --> 28:26.300
and obviously there's a lot of comparisons.

28:26.300 --> 28:31.860
By any chance do you remember what President Truman told Oppenheimer when he was in the

28:31.860 --> 28:32.860
Oval Office?

28:32.860 --> 28:33.860
No, I don't.

28:33.860 --> 28:38.860
I know something about that, but I don't know what Truman told Oppenheimer.

28:38.860 --> 28:39.860
Thank you.

28:39.860 --> 28:40.860
Tell us.

28:40.860 --> 28:44.060
We'll take it from here.

28:44.060 --> 28:45.060
Next audience question.

28:45.060 --> 28:49.700
Sorry, if there are people that might, let me know who's next.

28:49.700 --> 28:51.700
Maybe you give a...

28:51.700 --> 28:52.700
Go ahead.

28:52.700 --> 28:54.860
Hello, Jacob Woodruff.

28:54.860 --> 29:00.300
With the amount of data that's been required to train these large language models, would

29:00.300 --> 29:06.740
we expect a plateau in the intelligence of these systems, and how might that slow down

29:06.740 --> 29:08.980
or restrict the advancement?

29:08.980 --> 29:13.980
Okay, so that is a ray of hope that maybe we've just used up all human knowledge and

29:13.980 --> 29:15.940
they're not going to get any smarter.

29:15.940 --> 29:19.900
But think about images and video.

29:19.900 --> 29:26.540
So multimodal models will be much smarter than models that just train on language alone.

29:26.540 --> 29:30.980
They'll have a much better idea of how to deal with space, for example.

29:30.980 --> 29:36.460
And in terms of the amount of total video, we still don't have very good ways of processing

29:36.460 --> 29:40.060
video in these models of modeling video.

29:40.060 --> 29:41.740
We're getting better all the time.

29:41.740 --> 29:46.860
But I think there's plenty of data and things like video that tell you how the world works.

29:46.860 --> 29:53.180
So we're not hitting the data limits for multimodal models yet.

29:53.180 --> 29:55.100
Next gentleman on the back.

29:55.460 --> 29:56.940
Please do keep your question short.

29:56.940 --> 30:00.500
Hello, Dr. Hindul Rajeev Sehwabal from PWC.

30:00.500 --> 30:06.380
The point that I wanted to understand is that everything that AI is doing is learning from

30:06.380 --> 30:12.220
what we are teaching them, okay, data, yes, they are fast-read learning, how one trillion

30:12.220 --> 30:15.660
connectors can do much more than 100 trillion connectors that we have.

30:15.660 --> 30:20.500
But every piece of human evolution has been driven by thought experiments.

30:20.500 --> 30:24.180
Like Einstein used to do thought experiments because there was no speed of light out here

30:24.180 --> 30:25.540
on this planet.

30:25.540 --> 30:30.820
How can AI get to that point, if at all, and if it cannot, then how can we possibly have

30:30.820 --> 30:34.900
an existential threat from them because they will not be self-learning, so to say.

30:34.900 --> 30:39.260
They will be self-learning limited to the model that we tell them.

30:39.260 --> 30:42.740
I think that's a very interesting argument.

30:42.740 --> 30:45.460
But I think they will be able to do thought experiments.

30:45.460 --> 30:47.100
I think they'll be able to reason.

30:47.100 --> 30:49.260
So let me give you an analogy.

30:49.260 --> 30:56.220
If you take alpha zero, which plays chess, it has three ingredients.

30:56.220 --> 31:00.620
It's got something that evaluates the board position to say, is that good for me?

31:00.620 --> 31:04.140
It's got something that looks at the board position and says, what's the sensible move

31:04.140 --> 31:05.860
to consider?

31:05.860 --> 31:09.380
And then it's got Monte Carlo rollout, where it does what's called calculation where you

31:09.380 --> 31:13.300
think, if I go here and he goes there and I go here and he goes there.

31:13.300 --> 31:19.220
Now suppose you leave out the Monte Carlo rollout and you just train it from human experts

31:19.380 --> 31:24.340
to have a good evaluation function and a good way to choose moves to consider.

31:24.340 --> 31:27.100
It still plays a pretty good game of chess.

31:27.100 --> 31:30.540
And I think that's what we've got with the chatbots.

31:30.540 --> 31:35.460
And we haven't got them doing internal reasoning, but that will come.

31:35.460 --> 31:39.380
And once they start doing internal reasoning to check for the consistency between the different

31:39.380 --> 31:43.260
things they believe, then they'll get much smarter and they will be able to do thought

31:43.260 --> 31:44.500
experiments.

31:44.500 --> 31:51.260
And one reason they haven't got this internal reasoning is because they've been trained from

31:51.260 --> 31:52.580
inconsistent data.

31:52.580 --> 31:57.260
And so it's very hard for them to do reasoning because they've been trained on all these inconsistent

31:57.260 --> 31:58.260
beliefs.

31:58.260 --> 32:05.380
And I think they're going to have to be trained so they say, if I have this ideology, then

32:05.380 --> 32:06.380
this is true.

32:06.380 --> 32:08.100
And if I have that ideology, then that is true.

32:08.100 --> 32:11.420
And once they're trained like that, within an ideology, they're going to be able to try

32:11.420 --> 32:12.420
and get consistency.

32:13.100 --> 32:18.540
And so we're going to get a move like from a version of AlphaZero that just has something

32:18.540 --> 32:23.700
that guesses good moves and something that evaluates positions to a version that has

32:23.700 --> 32:27.620
long chains of Monte Carlo rollout, which is the corner of reasoning.

32:27.620 --> 32:30.220
And it's going to get much better.

32:30.220 --> 32:31.860
I'm going to take one in the front here.

32:31.860 --> 32:35.260
And then if you can be quick, we'll run and squeeze one more in as well.

32:35.260 --> 32:38.100
Louis Lamb, Jeff, I know you from a long time.

32:38.100 --> 32:44.940
Jeff, people criticize language models because of allegedly they are lacking semantics and

32:44.940 --> 32:46.620
grounding to the world.

32:46.620 --> 32:51.700
And you have been trying to, as well, to explain how neural networks work for a long time.

32:51.700 --> 32:57.140
Is the question of semantics and explainability relevant here or language models have taken

32:57.140 --> 33:04.740
over and we are now doomed to go forward without semantics or grounding to reality?

33:04.780 --> 33:10.660
I find it very hard to believe that they don't have semantics when they can solve problems

33:10.660 --> 33:13.980
like, you know, how I paint the rooms, how I get all the rooms in my house to be painted

33:13.980 --> 33:16.260
white in two years time.

33:16.260 --> 33:20.180
I mean, whatever semantic is, it's to do with the meaning of that stuff.

33:20.180 --> 33:22.100
And it understood the meaning.

33:22.100 --> 33:22.980
It got it.

33:22.980 --> 33:29.780
Now, I agree it's not grounded by being a robot, but you can make multimodal ones that

33:29.780 --> 33:31.900
are grounded, Google's done that.

33:31.900 --> 33:36.780
And the multimodal ones that are grounded, you can say, please close the drawer and they

33:36.780 --> 33:39.500
reach out and grab the handle and close the drawer.

33:39.500 --> 33:41.900
And it's very hard to say that doesn't have semantics.

33:41.900 --> 33:47.780
In fact, in the very early days of AI, in the days of Willigrad in the 1970s, they had

33:47.780 --> 33:53.060
just a simulated world, but they have what was called procedural semantics, where if

33:53.060 --> 33:59.540
you said to it, put the red box in, put the red block in the green box, and it put the

33:59.540 --> 34:00.780
red block in the green box.

34:00.780 --> 34:03.180
She said, see, it understood the language.

34:03.180 --> 34:08.380
And that was the criterion people used back then, but now that neural nets can do it,

34:08.380 --> 34:12.020
they say that's not an analytical criterion.

34:12.020 --> 34:13.020
One at the back.

34:13.020 --> 34:16.700
Hey, Jeff, this is Ishwar Balani from SAI Group.

34:16.700 --> 34:21.620
So clearly, the technology is advancing at an exponential pace.

34:21.620 --> 34:26.580
I wanted to get your thoughts, if you looked at the near and medium term, say one, two,

34:26.580 --> 34:33.300
three, or maybe five year horizon, what the social and economic implications are from

34:33.300 --> 34:37.900
a societal perspective with job loss or maybe new jobs being created.

34:37.900 --> 34:42.860
Just wanted to get your thoughts on how we proceed, given the state of the technology

34:42.860 --> 34:43.860
and rate of change.

34:43.860 --> 34:44.860
Yes.

34:44.860 --> 34:51.620
So the alarm bell I'm ringing is to do with the existential threat of them taking control.

34:51.620 --> 34:55.220
Lots of other people have talked about that, and I don't consider myself to be an expert

34:55.220 --> 34:56.220
on that.

34:56.220 --> 35:01.460
But there's some very obvious things that they're going to make a whole bunch of jobs

35:01.460 --> 35:03.460
much more efficient.

35:03.460 --> 35:08.460
So I know someone who answers letters of complaint to a health service, and he used to take 25

35:08.460 --> 35:12.140
minutes writing a letter, and now it takes him five minutes because he gives it to chat

35:12.140 --> 35:16.580
GPT and chat GPT writes the letter for him, and then he just checks it.

35:16.580 --> 35:22.580
There'll be lots of stuff like that, which is going to cause huge increases in productivity.

35:22.580 --> 35:26.260
There will be delays because people are very conservative about adopting new technology,

35:26.260 --> 35:29.140
but I think there's going to be huge increases in productivity.

35:29.140 --> 35:33.980
My worry is that those increases in productivity are going to go to putting people out of work

35:33.980 --> 35:37.100
and making the rich richer and the poor poorer.

35:37.100 --> 35:42.100
And as you do that, as you make that gap bigger, society gets more and more violent.

35:42.100 --> 35:48.380
This thing called the genie index, which predicts quite well how much violence there is.

35:48.380 --> 35:55.300
So this technology, which ought to be wonderful, even the good uses of technology for doing

35:55.300 --> 36:00.700
helpful things ought to be wonderful, but our current political systems is going to

36:00.700 --> 36:04.020
be used to make the rich richer and the poor poorer.

36:04.020 --> 36:12.940
You might be able to ameliorate that by having a basic income that everybody gets, but the

36:12.940 --> 36:22.380
technology is being developed in a society that is not designed to use it for everybody's good.

36:22.380 --> 36:29.980
A question here from Joe Castalda of the Globe and Mail, who's in the audience.

36:29.980 --> 36:35.980
Do you intend to hold on to your investments in Cahir and other companies, and if so, why?

36:36.980 --> 36:43.980
Well, I could take the money and I could put it in the bank and let them profit from it.

36:43.980 --> 36:53.980
Yes, I'm going to hold on to my investments in Cahir, partly because the people around Cahir are friends of mine.

36:53.980 --> 37:00.980
I sort of believe these big language balls are going to be very helpful.

37:01.980 --> 37:09.980
I think the technology should be good and it should make things work better.

37:09.980 --> 37:14.980
It's the politics we need to fix for things like employment.

37:14.980 --> 37:20.980
But when it comes to the existential threat, we have to think how we can keep control of the technology.

37:20.980 --> 37:25.980
But the good news there is that we're all in the same boat, so we might be able to get cooperation.

37:25.980 --> 37:33.980
And in speaking out, part of your thinking, as I understand it, is that you actually want to engage with the people making this technology

37:33.980 --> 37:40.980
and change their minds or maybe make a case for, I don't really know.

37:40.980 --> 37:45.980
We've established that we don't really know what to do, but it's about engaging rather than stepping back.

37:45.980 --> 37:52.980
So one of the things that made me leave Google and go public with this is to say,

37:52.980 --> 38:01.980
he used to be a junior professor, but he's now a middle rank professor who I think very highly of, who encouraged me to do this.

38:01.980 --> 38:04.980
He said, Jeff, you need to speak out there, listen to you.

38:04.980 --> 38:13.980
People are just blind to this danger and I think people are listening now.

38:13.980 --> 38:17.980
Yeah, no, I think everyone in this room is listening for a start.

38:17.980 --> 38:24.980
Just one last question and we're out of time, but do you have regrets that you're involved in making this?

38:24.980 --> 38:28.980
Cade Met's tried very hard to get me to say I had regrets.

38:28.980 --> 38:30.980
Cade Met's at the New York Times.

38:30.980 --> 38:38.980
And yes, and in the end, I said, well, maybe slight regrets, which got reported as has regrets.

38:38.980 --> 38:42.980
I don't think I made any bad decisions in doing research.

38:42.980 --> 38:49.980
I think it was perfectly reasonable back in the 70s and 80s to do research on how to make artificial neural nets.

38:49.980 --> 38:51.980
It wasn't really foreseeable.

38:51.980 --> 38:53.980
This stage of it wasn't foreseeable.

38:53.980 --> 38:58.980
And until very recently, I thought this existential crisis was a long way off.

38:58.980 --> 39:03.980
So I don't really have any regrets about what I did.

39:03.980 --> 39:04.980
Thank you, Jeffrey.

39:04.980 --> 39:06.980
Thank you so much for joining us.

39:12.980 --> 39:14.980
Thank you.

