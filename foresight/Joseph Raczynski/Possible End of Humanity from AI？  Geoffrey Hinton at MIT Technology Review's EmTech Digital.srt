1
00:00:00,000 --> 00:00:11,200
Hi, everyone.

2
00:00:11,200 --> 00:00:12,200
Welcome back.

3
00:00:12,200 --> 00:00:13,640
Hope you had a good lunch.

4
00:00:13,640 --> 00:00:19,120
My name is Ward Douglas-Heaven, senior editor for AI at MIT Technology Review, and I think

5
00:00:19,120 --> 00:00:23,120
we all agree there's no denying that generative AI is the thing of the moment.

6
00:00:23,120 --> 00:00:26,560
But innovation does not stand still, and in this chapter we're going to take a look at

7
00:00:26,560 --> 00:00:30,600
cutting-edge research that is already pushing ahead and asking what's next.

8
00:00:30,600 --> 00:00:35,200
But starting us off, I'd like to introduce a very special speaker who will be joining

9
00:00:35,200 --> 00:00:36,840
us virtually.

10
00:00:36,840 --> 00:00:42,560
Jeffrey Hinton is Professor Emeritus at University of Toronto and, until this week, an engineering

11
00:00:42,560 --> 00:00:43,560
fellow at Google.

12
00:00:43,560 --> 00:00:49,120
But on Monday he announced that after 10 years he will be stepping down.

13
00:00:49,120 --> 00:00:52,200
Jeffrey is one of the most important figures in modern AI.

14
00:00:52,200 --> 00:00:56,080
He's a pioneer of deep learning, developing some of the most fundamental techniques that

15
00:00:56,080 --> 00:01:00,640
underpin AI as we know it today, such as backpropagation, the algorithm that allows

16
00:01:00,640 --> 00:01:03,840
machines to learn.

17
00:01:03,840 --> 00:01:09,000
This technique is the foundation on which pretty much all of deep learning rests today.

18
00:01:09,000 --> 00:01:13,240
In 2018, Jeffrey received the Turing Award, which is often called the Nobel of Computer

19
00:01:13,240 --> 00:01:18,440
Science, alongside Jan LeCun and Yoshio Benjio.

20
00:01:18,440 --> 00:01:22,920
He's here with us today to talk about intelligence, what it means, and where attempts to build

21
00:01:22,920 --> 00:01:25,200
it into machines will take us.

22
00:01:25,320 --> 00:01:28,240
Jeffrey, welcome to MTech.

23
00:01:28,240 --> 00:01:29,240
Thank you.

24
00:01:29,240 --> 00:01:30,240
How's your week going?

25
00:01:30,240 --> 00:01:31,240
Busy few days, I imagine.

26
00:01:31,240 --> 00:01:35,880
Well, the last 10 minutes was horrible because my computer crashed and I had to find another

27
00:01:35,880 --> 00:01:38,440
computer and connect it up.

28
00:01:38,440 --> 00:01:39,440
And we're glad you're back.

29
00:01:39,440 --> 00:01:44,120
That's the kind of technical detail we're not supposed to share with the audience.

30
00:01:44,120 --> 00:01:45,360
It's great you're here.

31
00:01:45,360 --> 00:01:47,080
Very happy that you could join us.

32
00:01:47,080 --> 00:01:52,040
Now, I mean, it's been the news everywhere that you stepped down from Google this week.

33
00:01:52,360 --> 00:01:56,560
Could you start by telling us why you made that decision?

34
00:01:56,560 --> 00:01:58,160
Well, there were a number of reasons.

35
00:01:58,160 --> 00:02:00,680
There's always a bunch of reasons for a decision like that.

36
00:02:00,680 --> 00:02:07,320
One was that I'm 75 and I'm not as good at doing technical work as I used to be.

37
00:02:07,320 --> 00:02:11,440
My memory is not as good and when I program, I forget to do things.

38
00:02:11,440 --> 00:02:13,760
So it was time to retire.

39
00:02:13,760 --> 00:02:20,360
A second was, very recently, I've changed my mind a lot about the relationship between

40
00:02:20,360 --> 00:02:25,080
the brain and the kind of digital intelligence we're developing.

41
00:02:25,080 --> 00:02:30,960
So I used to think that the computer models we were developing weren't as good as the

42
00:02:30,960 --> 00:02:35,520
brain and the aim was to see if you could understand more about the brain by seeing

43
00:02:35,520 --> 00:02:39,400
what it takes to improve the computer models.

44
00:02:39,400 --> 00:02:44,640
Over the last few months, I've changed my mind completely and I think probably the computer

45
00:02:44,640 --> 00:02:47,280
models are working in a rather different way from the brain.

46
00:02:47,280 --> 00:02:50,600
They're using backpropagation and I think the brain's probably not.

47
00:02:50,600 --> 00:02:54,320
And a couple of things have led me to that conclusion, but one is the performance of

48
00:02:54,320 --> 00:02:56,600
things like GPT-4.

49
00:02:56,600 --> 00:03:02,520
So I want to get on to the performance of GPT-4 very much in a minute, but let's go back

50
00:03:02,520 --> 00:03:08,320
so that we all understand the argument you're making and tell us a little bit about what

51
00:03:08,320 --> 00:03:09,560
backpropagation is.

52
00:03:09,560 --> 00:03:16,040
This is an algorithm that you developed with a couple of colleagues back in the 1980s.

53
00:03:16,040 --> 00:03:19,720
Many different groups discovered backpropagation.

54
00:03:19,720 --> 00:03:26,520
The special thing we did was used it and showed that it could develop good internal representations.

55
00:03:26,520 --> 00:03:35,440
And curiously, we did that by implementing a tiny language model.

56
00:03:35,440 --> 00:03:44,040
It had embedding vectors that were only six components and the training set was 112 cases.

57
00:03:44,040 --> 00:03:45,040
But it was a language model.

58
00:03:45,040 --> 00:03:50,640
And it was trying to predict the next term in a string of symbols.

59
00:03:50,640 --> 00:03:56,360
And about 10 years later, Yoshua Benjo took basically the same net and used it on natural

60
00:03:56,360 --> 00:04:02,000
language and showed it actually worked for natural language if you made it much bigger.

61
00:04:02,000 --> 00:04:09,520
But the way backpropagation works, I can give you a rough explanation from it of it.

62
00:04:09,520 --> 00:04:15,400
People who know how it works can sit back and feel smug and laugh at the way I'm presenting

63
00:04:15,400 --> 00:04:16,400
it.

64
00:04:16,400 --> 00:04:20,440
Because I'm a bit worried about them.

65
00:04:20,440 --> 00:04:25,640
So imagine you wanted to detect birds in images.

66
00:04:25,640 --> 00:04:32,080
So an image, let's suppose it was 100 pixel by 100 pixel image, that's 10,000 pixels and

67
00:04:32,080 --> 00:04:38,640
each pixel is three channels, RGB, so that's 30,000 numbers, the intensity in each channel

68
00:04:38,640 --> 00:04:41,000
in each pixel, the represents the image.

69
00:04:41,000 --> 00:04:46,040
And the way to think of the computer vision problem is, how do I turn those 30,000 numbers

70
00:04:46,040 --> 00:04:49,440
into a decision about whether it's a bird or not?

71
00:04:49,440 --> 00:04:53,360
And people tried for a long time to do that and they weren't very good at it.

72
00:04:53,360 --> 00:04:56,240
But here's a suggestion for how you might do it.

73
00:04:56,240 --> 00:05:01,840
You might have a layer of feature detectors that detects very simple features in images,

74
00:05:01,840 --> 00:05:03,560
like for example, edges.

75
00:05:03,560 --> 00:05:10,640
So a feature detector might have big positive weights to a column of pixels and then big

76
00:05:10,640 --> 00:05:14,120
negative weights to the neighbouring column of pixels.

77
00:05:14,120 --> 00:05:18,040
So if both columns are bright, it won't turn on and if both columns are dim, it won't turn

78
00:05:18,040 --> 00:05:19,040
on.

79
00:05:19,040 --> 00:05:23,440
But if the column on one side is bright and the column on the other side is dim, it'll

80
00:05:23,440 --> 00:05:26,880
get very excited and that's an edge detector.

81
00:05:26,880 --> 00:05:30,920
So I just told you how to wire up an edge detector by hand by having one column of big

82
00:05:30,920 --> 00:05:33,280
positive weights and next to it, one column of big negative weights.

83
00:05:33,280 --> 00:05:37,760
And we can imagine a big layer of those detecting edges in different orientations and different

84
00:05:37,760 --> 00:05:39,800
scales all over the image.

85
00:05:39,800 --> 00:05:41,880
We'd need a rather large number of them.

86
00:05:41,880 --> 00:05:45,480
And edges in an image, you mean just lines, sort of edges of a shape?

87
00:05:45,480 --> 00:05:52,280
A place where the intensity changes from bright to dark, yeah, just that.

88
00:05:52,280 --> 00:05:57,360
Then we might have a layer of feature detectors above that that detects combinations of edges.

89
00:05:57,360 --> 00:06:04,320
So for example, we might have something that detects two edges that join at a fine angle

90
00:06:04,320 --> 00:06:07,280
like this.

91
00:06:07,280 --> 00:06:12,160
So it'll have a big positive weight to each of those two edges and if both of those edges

92
00:06:12,160 --> 00:06:15,120
are there at the same time, it'll get excited.

93
00:06:15,120 --> 00:06:18,520
And that would detect something that might be a bird's beak.

94
00:06:18,520 --> 00:06:20,560
It might not, but it might be a bird's beak.

95
00:06:20,560 --> 00:06:25,000
You might also in that layer have a feature detector that would detect a whole bunch of

96
00:06:25,000 --> 00:06:29,080
edges arranged in a circle.

97
00:06:29,080 --> 00:06:30,440
And that might be a bird's eye.

98
00:06:30,440 --> 00:06:31,720
It might be all sorts of other things.

99
00:06:31,720 --> 00:06:36,720
It might be a knob on a fridge or something.

100
00:06:36,720 --> 00:06:42,200
Then in a third layer, you might have a feature detector that detects this potential beak and

101
00:06:42,200 --> 00:06:47,480
detects a potential eye and is wired up so that it'll like a beak and an eye in the right

102
00:06:47,480 --> 00:06:49,040
spatial relation to one another.

103
00:06:49,040 --> 00:06:53,320
And if it sees that, it says, ah, this might be the head of a bird.

104
00:06:53,320 --> 00:06:57,240
And you can imagine if you keep wiring like that, you could eventually have something

105
00:06:57,240 --> 00:06:59,680
that detects a bird.

106
00:06:59,680 --> 00:07:04,800
But wiring all that up by hand would be very, very difficult, deciding on what should be

107
00:07:04,800 --> 00:07:06,680
connected to what and what the weight should be.

108
00:07:06,680 --> 00:07:10,520
And it would be especially difficult because you want these sort of intermediate layers

109
00:07:10,520 --> 00:07:16,080
to be good not just for detecting birds, but for detecting all sorts of other things.

110
00:07:16,080 --> 00:07:20,920
So it would be more or less impossible to wire it up by hand.

111
00:07:20,920 --> 00:07:23,080
So the way back propagation works is this.

112
00:07:23,080 --> 00:07:25,040
You start with random weights.

113
00:07:25,040 --> 00:07:28,720
So these feature detectors are just complete rubbish.

114
00:07:28,720 --> 00:07:34,200
And you put in a picture of a bird, and at the output, it says, like, 0.5, it's a bird.

115
00:07:34,200 --> 00:07:37,440
I suppose you only have birds on onwards.

116
00:07:37,440 --> 00:07:40,080
And then you ask yourself the following question.

117
00:07:40,080 --> 00:07:46,000
How could I change each of the weights in the network, each of the weights on connections

118
00:07:46,000 --> 00:07:52,920
in the network, so that instead of saying 0.5, it says 0.501, that it's a bird, and 0.499,

119
00:07:52,920 --> 00:07:55,120
that it's not.

120
00:07:55,120 --> 00:08:00,080
And you change the weights in the directions that will make it more likely to say that a

121
00:08:00,080 --> 00:08:05,360
bird is a bird, and less likely to say that a non-bird is a bird.

122
00:08:05,360 --> 00:08:07,040
And you just keep doing that.

123
00:08:07,040 --> 00:08:08,040
And that's back propagation.

124
00:08:08,040 --> 00:08:13,200
Back propagation is actually how you take the discrepancy between what you want, which

125
00:08:13,200 --> 00:08:17,080
is a probability of 1, that's a bird, and what it's got at present, which is probability

126
00:08:17,080 --> 00:08:22,000
of 0.5, that it's a bird, how you take that discrepancy and send it backwards through the

127
00:08:22,000 --> 00:08:28,360
network, so that you can compute for every feature detector in the network, whether you'd

128
00:08:28,360 --> 00:08:31,040
like it to be a bit more active or a bit less active.

129
00:08:31,040 --> 00:08:34,120
And once you've computed that, if you know you want a feature detector to be a bit more

130
00:08:34,120 --> 00:08:38,600
active, you can increase the weights coming from feature detectors in the layer below

131
00:08:38,600 --> 00:08:44,200
that are active, and maybe put in some negative weights to feature detectors in the layer

132
00:08:44,200 --> 00:08:48,600
below that are off, and now you'll have a better detector.

133
00:08:48,600 --> 00:08:51,640
So back propagation is just going backwards through the network to figure out for each

134
00:08:51,640 --> 00:08:55,640
feature detector, whether you want it a little bit more active or a little bit less active.

135
00:08:55,640 --> 00:08:56,640
Thank you.

136
00:08:56,640 --> 00:08:57,640
I can show it.

137
00:08:57,640 --> 00:09:03,200
There's no one in the audience here that's smiling and thinking that was a silly explanation.

138
00:09:03,200 --> 00:09:09,840
So let's fast forward quite a lot to, you know, that technique basically performed really

139
00:09:09,840 --> 00:09:15,680
well on ImageNet, and we had Joel Pino from Meta yesterday showing how far image detection

140
00:09:15,680 --> 00:09:21,320
had come, and it's also the technique that underpins large language models.

141
00:09:21,320 --> 00:09:30,120
So I want to talk now about this technique, which you initially were thinking of as almost

142
00:09:30,120 --> 00:09:37,280
like a poor approximation of what biological brains might do, has turned out to do things

143
00:09:37,280 --> 00:09:41,480
which I think have stunned you, particularly in large language models.

144
00:09:41,480 --> 00:09:48,600
So talk to us about why that sort of amazement that you have with today's large language

145
00:09:48,600 --> 00:09:54,280
models has completely sort of almost flipped your thinking of what back propagation or

146
00:09:54,280 --> 00:09:57,560
machine learning in general is.

147
00:09:57,560 --> 00:10:03,640
So if you look at these large language models, they have about a trillion connections, and

148
00:10:03,640 --> 00:10:07,760
things like GP24 know much more than we do.

149
00:10:07,760 --> 00:10:13,520
They have sort of common sense knowledge about everything, and so they probably know a thousand

150
00:10:13,520 --> 00:10:18,520
times as much as a person, but they've got a trillion connections and we've got a hundred

151
00:10:18,520 --> 00:10:20,640
trillion connections.

152
00:10:20,640 --> 00:10:25,200
So they're much, much better at getting a lot of knowledge into only a trillion connections

153
00:10:25,200 --> 00:10:31,000
than we are, and I think it's because back propagation may be a much, much better learning

154
00:10:31,000 --> 00:10:33,280
algorithm than what we've got.

155
00:10:33,280 --> 00:10:34,280
Can you define that?

156
00:10:34,280 --> 00:10:35,280
That's scary.

157
00:10:35,280 --> 00:10:37,000
Yeah, I definitely want to get onto the scary stuff.

158
00:10:37,000 --> 00:10:40,080
So what do you mean by better?

159
00:10:40,080 --> 00:10:45,560
It can pack more information into only a few connections, which are finding a trillion

160
00:10:45,560 --> 00:10:48,440
as only a few.

161
00:10:48,440 --> 00:10:57,760
So these digital computers are better at learning than humans, which itself is a huge claim,

162
00:10:57,760 --> 00:11:03,280
but then you also argue that that's something that we should be scared of.

163
00:11:03,280 --> 00:11:05,520
So could you take us through that step of the argument?

164
00:11:06,000 --> 00:11:14,680
Yeah, let me give you a separate piece of the argument, which is that if a computer's

165
00:11:14,680 --> 00:11:20,000
digital, which involves very high energy costs and very careful fabrication, you can have

166
00:11:20,000 --> 00:11:24,040
many copies of the same model running on different hardware that do exactly the same

167
00:11:24,040 --> 00:11:25,040
thing.

168
00:11:25,040 --> 00:11:28,040
They can look at different data, but the model is exactly the same.

169
00:11:28,040 --> 00:11:33,760
And what that means is, especially of 10,000 copies, they can be looking at 10,000 different

170
00:11:33,760 --> 00:11:35,920
subsets of the data.

171
00:11:35,920 --> 00:11:39,960
And whenever one of them learns anything, all the others know it.

172
00:11:39,960 --> 00:11:44,640
One of them figures out how to change the weights so it can deal with this data.

173
00:11:44,640 --> 00:11:48,040
They all communicate with each other, and they all agree to change the weights by the

174
00:11:48,040 --> 00:11:51,000
average of what all of them want.

175
00:11:51,000 --> 00:11:59,200
And now the 10,000 things are communicating very effectively with each other so that they

176
00:11:59,200 --> 00:12:03,240
can see 10,000 times as much data as one agent could.

177
00:12:03,240 --> 00:12:05,080
But people can't do that.

178
00:12:05,080 --> 00:12:08,840
If I learn a whole lot of stuff about quantum mechanics, and I want you to know all that

179
00:12:08,840 --> 00:12:13,320
stuff about quantum mechanics, it's a long, painful process of getting you to understand

180
00:12:13,320 --> 00:12:14,320
it.

181
00:12:14,320 --> 00:12:19,560
I can't just copy my weights into your brain, because your brain isn't exactly the same

182
00:12:19,560 --> 00:12:20,560
as mine.

183
00:12:20,560 --> 00:12:24,560
No, it's not.

184
00:12:24,560 --> 00:12:29,060
It's younger.

185
00:12:30,020 --> 00:12:39,100
We have digital computers that can learn more things more quickly, and they can instantly

186
00:12:39,100 --> 00:12:42,180
teach it to each other.

187
00:12:42,180 --> 00:12:46,220
People in the room here could instantly transfer what they had in their heads into mine.

188
00:12:46,220 --> 00:12:50,460
But why is that scary?

189
00:12:50,460 --> 00:12:56,860
Because they can learn so much more, and they might take an example of a doctor, and imagine

190
00:12:56,860 --> 00:13:03,460
you have one doctor who's seen 1,000 patients, and another doctor who's seen 100 million

191
00:13:03,460 --> 00:13:05,660
patients.

192
00:13:05,660 --> 00:13:10,340
You would expect the doctor who's seen 100 million patients, if he's not too forgetful,

193
00:13:10,340 --> 00:13:14,180
to have noticed all sorts of trends in the data that just aren't visible if you're only

194
00:13:14,180 --> 00:13:16,380
seen 1,000 patients.

195
00:13:16,380 --> 00:13:19,780
You may have only seen one patient with some rare disease.

196
00:13:19,780 --> 00:13:23,340
The other doctor who's seen 100 million will have seen, well, you can figure out how many

197
00:13:23,340 --> 00:13:25,980
patients, but a lot.

198
00:13:26,100 --> 00:13:32,860
We'll see all sorts of regularities that just aren't apparent in small data.

199
00:13:32,860 --> 00:13:38,140
That's why things that can get through a lot of data can probably see structure in data

200
00:13:38,140 --> 00:13:41,100
that we'll never see.

201
00:13:41,100 --> 00:13:49,100
Then take me to the point where I should be scared of this, though.

202
00:13:49,100 --> 00:13:53,140
If you look at GPT-4, it can already do simple reasoning.

203
00:13:53,860 --> 00:14:00,140
Reasoning is the area where we're still better, but I was impressed the other day at GPT-4

204
00:14:00,140 --> 00:14:05,180
doing a piece of common sense reasoning that I didn't think it would be able to do.

205
00:14:05,180 --> 00:14:10,860
I asked it, I want all the rooms in my house to be white.

206
00:14:10,860 --> 00:14:16,900
At present, there's some white rooms, some blue rooms, and some yellow rooms, and yellow

207
00:14:16,900 --> 00:14:20,100
paint fades to white within a year.

208
00:14:20,100 --> 00:14:25,460
What should I do if I want them all to be white in two years' time?

209
00:14:25,460 --> 00:14:29,300
It said you should paint the blue rooms yellow.

210
00:14:29,300 --> 00:14:34,180
That's not the natural solution, but it works, right?

211
00:14:34,180 --> 00:14:38,700
That's pretty impressive common sense reasoning of the kind that it's been very hard to get

212
00:14:38,700 --> 00:14:44,620
AI to do using symbolic AI, because it had to understand what fades means.

213
00:14:44,620 --> 00:14:50,820
It had to understood by temple stuff.

214
00:14:50,820 --> 00:15:00,380
They're doing sensible reasoning with an IQ of 80 or 90 or something.

215
00:15:00,380 --> 00:15:08,220
As a friend of mine said, it's as if some genetic engineers have said, we're going to

216
00:15:08,220 --> 00:15:09,980
improve grizzly bears.

217
00:15:09,980 --> 00:15:14,340
We've already improved them to have an IQ of 65, and they can talk English now.

218
00:15:14,340 --> 00:15:24,020
They're very useful for all sorts of things, but we think we can improve the IQ to 210.

219
00:15:24,020 --> 00:15:25,020
I certainly have.

220
00:15:25,020 --> 00:15:30,500
I'm sure many people have had that feeling when you're interacting with these latest

221
00:15:30,500 --> 00:15:35,020
chatbots, sort of hair on the back of the neck, sort of uncanny feeling.

222
00:15:35,020 --> 00:15:39,260
When I have that feeling and I'm uncomfortable, I just close my laptop.

223
00:15:40,260 --> 00:15:47,460
Yes, but these things will have learned from us by reading all the novels that ever were

224
00:15:47,460 --> 00:15:56,060
and everything Machiavelli ever wrote, that how to manipulate people, right?

225
00:15:56,060 --> 00:15:58,980
If they're much smarter than us, they'll be very good at manipulating us.

226
00:15:58,980 --> 00:16:00,500
You won't realize what's going on.

227
00:16:00,500 --> 00:16:05,060
You'll be like a two-year-old who's being asked, do you want the peas or the cauliflower

228
00:16:05,060 --> 00:16:08,860
and doesn't realize you don't have to have either.

229
00:16:08,860 --> 00:16:12,580
And you'll be that easy to manipulate.

230
00:16:12,580 --> 00:16:18,620
And so even if they can't directly pull levers, they can certainly get us to pull levers.

231
00:16:18,620 --> 00:16:23,420
It turns out if you can manipulate people, you can invade a building in Washington without

232
00:16:23,420 --> 00:16:28,860
ever going there yourself.

233
00:16:28,860 --> 00:16:29,860
Very good.

234
00:16:29,860 --> 00:16:30,860
Yeah.

235
00:16:30,860 --> 00:16:36,980
So is that, is that, I mean, if the word, okay, this is a very hypothetical world, but

236
00:16:36,980 --> 00:16:45,700
if there were no bad actors, you know, people with bad intentions, would we be safe?

237
00:16:45,700 --> 00:16:46,700
I don't know.

238
00:16:46,700 --> 00:16:52,500
We'd be safer in a world where people have bad intentions and where the political system

239
00:16:52,500 --> 00:16:59,020
is so broken that we can't even decide not to give assault rifles to teenage boys.

240
00:16:59,020 --> 00:17:01,980
If you can't solve that problem, how are you going to solve this problem?

241
00:17:02,980 --> 00:17:03,980
Well, I mean, I don't know.

242
00:17:03,980 --> 00:17:07,340
I was hoping that you would have some thoughts.

243
00:17:07,340 --> 00:17:15,060
You've, so one, I mean, unless we didn't make this clear at the beginning, I mean, you've

244
00:17:15,060 --> 00:17:20,700
won to speak out about this and you feel more comfortable doing that without it sort of having

245
00:17:20,700 --> 00:17:24,660
any blowback on Google.

246
00:17:24,660 --> 00:17:30,380
But you're speaking out about it, but in some sense talk is cheap if we then don't have,

247
00:17:30,380 --> 00:17:35,700
you know, actions or what do we do when we lots of people this week are listening to

248
00:17:35,700 --> 00:17:36,700
you?

249
00:17:36,700 --> 00:17:38,180
What should we do about it?

250
00:17:38,180 --> 00:17:43,340
I wish it was like climate change where you could say, if you've got half a brain, you'd

251
00:17:43,340 --> 00:17:46,380
stop burning carbon.

252
00:17:46,380 --> 00:17:51,820
It's clear what you should do about it is clear that's painful, but has to be done.

253
00:17:51,820 --> 00:17:55,820
I don't know of any solution like that to stop these things taking over from us.

254
00:17:55,820 --> 00:17:58,900
What we really want, I don't think we're going to stop developing them because they're so

255
00:17:58,900 --> 00:17:59,900
useful.

256
00:17:59,940 --> 00:18:04,220
They'll be incredibly useful in medicine and in everything else.

257
00:18:04,220 --> 00:18:07,060
So I don't think there's much chance of stopping development.

258
00:18:07,060 --> 00:18:13,060
What we want is some way of making sure that even if they're smarter than us, they're going

259
00:18:13,060 --> 00:18:15,100
to do things that are beneficial for us.

260
00:18:15,100 --> 00:18:16,940
That's called the alignment problem.

261
00:18:16,940 --> 00:18:21,980
But we need to try and do that in a world where there's bad actors who want to build

262
00:18:21,980 --> 00:18:25,100
robot soldiers that kill people.

263
00:18:25,100 --> 00:18:27,060
And it seems very hard to me.

264
00:18:27,060 --> 00:18:28,060
So I'm sorry.

265
00:18:28,060 --> 00:18:32,380
I'm sounding the alarm and saying, we have to worry about this, and I wish I had a nice

266
00:18:32,380 --> 00:18:34,380
simple solution I could push, but I don't.

267
00:18:34,380 --> 00:18:37,660
But I think it's very important that people get together and think hard about it and see

268
00:18:37,660 --> 00:18:39,020
whether there is a solution.

269
00:18:39,020 --> 00:18:41,180
It's not clear there is a solution.

270
00:18:41,180 --> 00:18:44,260
So talk to us about that.

271
00:18:44,260 --> 00:18:49,620
You spent your career on the technicalities of this technology.

272
00:18:49,620 --> 00:18:51,580
Is there no technical fix?

273
00:18:51,580 --> 00:18:57,020
Why can we not build in guardrails or make them worse at learning?

274
00:18:57,980 --> 00:19:04,100
Or restrict the way that they can communicate, if those are the two strings of your argument?

275
00:19:04,100 --> 00:19:07,820
We're trying to do all sorts of guardrails.

276
00:19:07,820 --> 00:19:11,500
But suppose they did get really smart, and these things can program, right?

277
00:19:11,500 --> 00:19:12,500
They can write programs.

278
00:19:12,500 --> 00:19:20,260
And suppose you give them the ability to execute those programs, which we'll certainly do.

279
00:19:20,260 --> 00:19:23,980
Smart things can outsmart us.

280
00:19:23,980 --> 00:19:31,620
So imagine you're two-year-old saying, my dad does things I don't like, so I'm going

281
00:19:31,620 --> 00:19:34,700
to make some rules for whatever my dad can do.

282
00:19:34,700 --> 00:19:39,300
You could probably figure out how to live with those rules and still get what you want.

283
00:19:39,300 --> 00:19:41,300
Yeah.

284
00:19:41,300 --> 00:19:50,540
But there still seems to be a step where these smart machines somehow have motivation of their

285
00:19:50,540 --> 00:19:51,540
own.

286
00:19:51,540 --> 00:19:52,540
Yes.

287
00:19:52,540 --> 00:19:53,540
Yes, that's a very good point.

288
00:19:54,020 --> 00:20:00,300
We evolved, and because we evolved, we have certain built-in goals that we find very hard

289
00:20:00,300 --> 00:20:07,140
to turn off, like we try not to damage our bodies, that's what pain is about.

290
00:20:07,140 --> 00:20:12,660
We try and get enough to eat so we feed our bodies.

291
00:20:12,660 --> 00:20:18,980
We try and make as many copies of ourselves as possible, maybe not deliberately that intention,

292
00:20:18,980 --> 00:20:24,060
but we've been wired up so there's pleasure involved in making many copies of ourselves.

293
00:20:24,060 --> 00:20:30,740
And that all came from evolution, and it's important that we can't turn it off.

294
00:20:30,740 --> 00:20:34,620
If you could turn it off, you don't do so well.

295
00:20:34,620 --> 00:20:37,820
There's a wonderful group called the Shakers who related to the Quakers who made beautiful

296
00:20:37,820 --> 00:20:46,180
furniture but didn't believe in sex, and there aren't any of them around anymore.

297
00:20:46,180 --> 00:20:51,140
So these digital intelligences didn't evolve.

298
00:20:51,140 --> 00:20:55,860
We made them, and so they don't have these built-in goals.

299
00:20:55,860 --> 00:21:00,860
And so the issue is, if we can put the goals in, maybe it'll all be okay.

300
00:21:00,860 --> 00:21:06,540
But my big worry is, sooner or later, someone will wire into them the ability to create

301
00:21:06,540 --> 00:21:07,540
their own sub-goals.

302
00:21:07,540 --> 00:21:14,980
In fact, they almost have that already, the versions of chat GPT that call chat GPT.

303
00:21:14,980 --> 00:21:19,820
And if you give something the ability to create your own sub-goals in order to achieve other

304
00:21:19,820 --> 00:21:26,220
goals, I think it'll very quickly realise that getting more control is a very good sub-goal

305
00:21:26,220 --> 00:21:29,700
because it helps you achieve other goals.

306
00:21:29,700 --> 00:21:34,100
And if these things get carried away with getting more control, we're in trouble.

307
00:21:34,100 --> 00:21:39,180
So what's the worst-case scenario that you think is conceivable?

308
00:21:39,820 --> 00:21:45,900
Oh, I think it's quite conceivable that humanity is just a passing phase in the evolution of

309
00:21:45,900 --> 00:21:47,260
intelligence.

310
00:21:47,260 --> 00:21:49,220
You couldn't directly evolve digital intelligence.

311
00:21:49,220 --> 00:21:53,780
It requires too much energy and too much careful fabrication.

312
00:21:53,780 --> 00:21:59,460
You need biological intelligence to evolve so that it can create digital intelligence.

313
00:21:59,460 --> 00:22:06,820
The digital intelligence can then absorb everything people ever wrote in a fairly slow way, which

314
00:22:06,900 --> 00:22:10,580
is what chat GPT has been doing.

315
00:22:10,580 --> 00:22:15,340
But then it can start getting direct experience of the world and learn much faster.

316
00:22:15,340 --> 00:22:22,900
And it may keep us around for a while to keep the power stations running, but after that,

317
00:22:22,900 --> 00:22:23,900
maybe not.

318
00:22:23,900 --> 00:22:29,820
So the good news is we figured out how to build beings that are immortal.

319
00:22:29,820 --> 00:22:34,540
So these digital intelligences, when a piece of hardware dies, they don't die.

320
00:22:34,580 --> 00:22:39,140
If you've got the weight stored in some medium and you can find another piece of hardware

321
00:22:39,140 --> 00:22:44,620
that can run the same instructions, then you can bring it to life again.

322
00:22:44,620 --> 00:22:49,660
So we've got immortality, but it's not for us.

323
00:22:49,660 --> 00:22:53,140
So Ray Kurzweil is very interested in being immortal.

324
00:22:53,140 --> 00:22:57,700
I think it's a very bad idea for old white men to be immortal.

325
00:22:57,700 --> 00:23:01,620
We've got the immortality, but it's not for Ray.

326
00:23:01,900 --> 00:23:07,660
No, the scary thing is that, in a way, maybe you will be, because you invented much of

327
00:23:07,660 --> 00:23:11,780
this technology.

328
00:23:11,780 --> 00:23:14,980
When I hear you say this, part of me wants to run off the stage into the street now and

329
00:23:14,980 --> 00:23:19,580
start unplugging computers.

330
00:23:19,580 --> 00:23:21,380
I'm afraid we can't do that.

331
00:23:21,380 --> 00:23:22,380
Why?

332
00:23:22,380 --> 00:23:24,300
You sound like Hal from 2001.

333
00:23:24,300 --> 00:23:36,820
More seriously, I know you said before that it was suggested a few months ago that there

334
00:23:36,820 --> 00:23:45,060
should be a moratorium on AI advancement, and I don't think that's a very good idea.

335
00:23:45,060 --> 00:23:48,860
But more generally, I'm curious, why?

336
00:23:48,860 --> 00:23:51,300
Should we not just stop?

337
00:23:51,300 --> 00:23:58,700
I know that you've spoken also that you're an investor of your personal wealth in some

338
00:23:58,700 --> 00:24:01,260
companies like Coher that are building these large language models.

339
00:24:01,260 --> 00:24:06,420
I'm curious about your personal sense of responsibility and each of our personal responsibility.

340
00:24:06,420 --> 00:24:07,420
What should we be doing?

341
00:24:07,420 --> 00:24:11,260
I mean, should we try and stop this, is what I'm saying.

342
00:24:11,260 --> 00:24:16,580
I think if you take the existential risk seriously, as I now do, I used to think it was way off,

343
00:24:16,580 --> 00:24:20,340
but I now think it's serious and fairly close.

344
00:24:20,340 --> 00:24:25,740
It might be quite sensible to just stop developing these things any further, but I think it's

345
00:24:25,740 --> 00:24:28,780
completely naive to think that would happen.

346
00:24:28,780 --> 00:24:32,180
There's no way to make that happen.

347
00:24:32,180 --> 00:24:36,460
I mean, if the US stops developing and the Chinese won't, they're going to be used in

348
00:24:36,460 --> 00:24:41,700
weapons and just for that reason alone, governments aren't going to stop developing them.

349
00:24:41,700 --> 00:24:47,340
So yes, I think stopping developing them might be a rational thing to do, but there's no

350
00:24:47,340 --> 00:24:48,340
way it's going to happen.

351
00:24:48,340 --> 00:24:51,460
So it's silly to sign petitions saying, please stop now.

352
00:24:51,460 --> 00:24:52,780
We did have a holiday.

353
00:24:52,780 --> 00:24:58,900
We had a holiday from about 2017 for several years because Google developed the technology

354
00:24:58,900 --> 00:24:59,900
first.

355
00:24:59,900 --> 00:25:00,900
It developed the transform.

356
00:25:00,900 --> 00:25:06,020
We said it also developed the fusion models, and it didn't put them out there for people

357
00:25:06,020 --> 00:25:07,580
to use and abuse.

358
00:25:07,580 --> 00:25:10,540
It was very careful with them because it didn't want to damage his reputation and he knew

359
00:25:10,540 --> 00:25:16,380
there could be bad consequences, but that can only happen if there's a single leader.

360
00:25:16,380 --> 00:25:24,820
Once open AI had built similar things using transformers and money from Microsoft, and

361
00:25:24,820 --> 00:25:29,740
Microsoft decided to put it out there, Google didn't have really much choice.

362
00:25:29,740 --> 00:25:35,660
If you're going to live in a capitalist system, you can't stop Google competing with Microsoft.

363
00:25:35,660 --> 00:25:38,380
So I don't think Google did anything wrong.

364
00:25:38,380 --> 00:25:42,660
I think it was very responsible to begin with, but I think it's just inevitable in the capitalist

365
00:25:42,660 --> 00:25:46,580
system or a system with competition between countries like the US and China that this

366
00:25:46,580 --> 00:25:49,540
stuff will be developed.

367
00:25:49,540 --> 00:25:54,500
My one hope is that because if we allowed it to take over, it would be bad for all of

368
00:25:54,500 --> 00:25:58,460
us, we could get the US and China to agree like we could with nuclear weapons, which

369
00:25:58,460 --> 00:26:00,100
were bad for all of us.

370
00:26:00,100 --> 00:26:03,580
We're all in the same boat with respect to the existential threat, so we all ought to

371
00:26:03,580 --> 00:26:06,460
be able to cooperate on trying to stop it.

372
00:26:06,460 --> 00:26:09,940
As long as we can make some money on the way.

373
00:26:09,940 --> 00:26:14,180
I'm going to take some audience questions from the room if you make yourself known, and

374
00:26:14,180 --> 00:26:17,620
while people are going around with the microphone, there's one question I was going to ask from

375
00:26:17,620 --> 00:26:18,740
the online audience.

376
00:26:18,740 --> 00:26:20,100
I'm interested.

377
00:26:20,100 --> 00:26:26,140
You mentioned a little bit about maybe a transition period as machines get smarter and outpace

378
00:26:26,140 --> 00:26:27,140
humans.

379
00:26:27,140 --> 00:26:32,060
There'll be a moment where it's hard to define what's human and what isn't, or are these

380
00:26:32,060 --> 00:26:35,300
two very distinct forms of intelligence?

381
00:26:35,300 --> 00:26:37,620
I think they're distinct forms of intelligence.

382
00:26:37,820 --> 00:26:42,980
Now, of course, the digital intelligences are very good at mimicking us because they've

383
00:26:42,980 --> 00:26:46,740
been trained to mimic us.

384
00:26:46,740 --> 00:26:52,780
It's very hard to tell if chatGBT wrote it, or whether we wrote it.

385
00:26:52,780 --> 00:26:58,100
In that sense, they look quite like us, but inside they're not working the same way.

386
00:26:58,100 --> 00:27:00,020
Who is first in the room?

387
00:27:00,020 --> 00:27:02,540
Hello.

388
00:27:02,540 --> 00:27:05,860
My name is Hal Gregerson, and my middle name is not 9000.

389
00:27:06,500 --> 00:27:11,180
I'm a faculty over in the MIT Sloan School.

390
00:27:11,180 --> 00:27:17,500
Arguably asking questions is one of the most important human abilities we have.

391
00:27:17,500 --> 00:27:26,620
From your perspective now in 2023, what question or two should we pay most attention to?

392
00:27:26,620 --> 00:27:33,620
And is it possible for these technologies to actually help us ask better questions and

393
00:27:33,660 --> 00:27:36,220
outquestion the technology?

394
00:27:36,220 --> 00:27:42,740
Yes, but what I'm saying is there's many questions we should be asking, but one of

395
00:27:42,740 --> 00:27:45,700
them is how do we prevent them from taking over?

396
00:27:45,700 --> 00:27:48,460
How do we prevent them from getting control?

397
00:27:48,460 --> 00:27:57,540
And we could ask them questions about that, but I wouldn't entirely trust their answers.

398
00:27:57,540 --> 00:27:58,540
Question at the back.

399
00:27:58,540 --> 00:28:02,100
And I want to get through as many as we can, so if you can keep your question as short

400
00:28:02,100 --> 00:28:03,100
as possible.

401
00:28:03,100 --> 00:28:08,860
Dr. Hinton, thank you so much for being here with us today.

402
00:28:08,860 --> 00:28:14,860
I shall say this is the most expensive lecture I've ever paid for, but I think it was worthwhile.

403
00:28:14,860 --> 00:28:23,860
I just have a question for you, because you mentioned the analogy of nuclear history,

404
00:28:23,860 --> 00:28:26,300
and obviously there's a lot of comparisons.

405
00:28:26,300 --> 00:28:31,860
By any chance do you remember what President Truman told Oppenheimer when he was in the

406
00:28:31,860 --> 00:28:32,860
Oval Office?

407
00:28:32,860 --> 00:28:33,860
No, I don't.

408
00:28:33,860 --> 00:28:38,860
I know something about that, but I don't know what Truman told Oppenheimer.

409
00:28:38,860 --> 00:28:39,860
Thank you.

410
00:28:39,860 --> 00:28:40,860
Tell us.

411
00:28:40,860 --> 00:28:44,060
We'll take it from here.

412
00:28:44,060 --> 00:28:45,060
Next audience question.

413
00:28:45,060 --> 00:28:49,700
Sorry, if there are people that might, let me know who's next.

414
00:28:49,700 --> 00:28:51,700
Maybe you give a...

415
00:28:51,700 --> 00:28:52,700
Go ahead.

416
00:28:52,700 --> 00:28:54,860
Hello, Jacob Woodruff.

417
00:28:54,860 --> 00:29:00,300
With the amount of data that's been required to train these large language models, would

418
00:29:00,300 --> 00:29:06,740
we expect a plateau in the intelligence of these systems, and how might that slow down

419
00:29:06,740 --> 00:29:08,980
or restrict the advancement?

420
00:29:08,980 --> 00:29:13,980
Okay, so that is a ray of hope that maybe we've just used up all human knowledge and

421
00:29:13,980 --> 00:29:15,940
they're not going to get any smarter.

422
00:29:15,940 --> 00:29:19,900
But think about images and video.

423
00:29:19,900 --> 00:29:26,540
So multimodal models will be much smarter than models that just train on language alone.

424
00:29:26,540 --> 00:29:30,980
They'll have a much better idea of how to deal with space, for example.

425
00:29:30,980 --> 00:29:36,460
And in terms of the amount of total video, we still don't have very good ways of processing

426
00:29:36,460 --> 00:29:40,060
video in these models of modeling video.

427
00:29:40,060 --> 00:29:41,740
We're getting better all the time.

428
00:29:41,740 --> 00:29:46,860
But I think there's plenty of data and things like video that tell you how the world works.

429
00:29:46,860 --> 00:29:53,180
So we're not hitting the data limits for multimodal models yet.

430
00:29:53,180 --> 00:29:55,100
Next gentleman on the back.

431
00:29:55,460 --> 00:29:56,940
Please do keep your question short.

432
00:29:56,940 --> 00:30:00,500
Hello, Dr. Hindul Rajeev Sehwabal from PWC.

433
00:30:00,500 --> 00:30:06,380
The point that I wanted to understand is that everything that AI is doing is learning from

434
00:30:06,380 --> 00:30:12,220
what we are teaching them, okay, data, yes, they are fast-read learning, how one trillion

435
00:30:12,220 --> 00:30:15,660
connectors can do much more than 100 trillion connectors that we have.

436
00:30:15,660 --> 00:30:20,500
But every piece of human evolution has been driven by thought experiments.

437
00:30:20,500 --> 00:30:24,180
Like Einstein used to do thought experiments because there was no speed of light out here

438
00:30:24,180 --> 00:30:25,540
on this planet.

439
00:30:25,540 --> 00:30:30,820
How can AI get to that point, if at all, and if it cannot, then how can we possibly have

440
00:30:30,820 --> 00:30:34,900
an existential threat from them because they will not be self-learning, so to say.

441
00:30:34,900 --> 00:30:39,260
They will be self-learning limited to the model that we tell them.

442
00:30:39,260 --> 00:30:42,740
I think that's a very interesting argument.

443
00:30:42,740 --> 00:30:45,460
But I think they will be able to do thought experiments.

444
00:30:45,460 --> 00:30:47,100
I think they'll be able to reason.

445
00:30:47,100 --> 00:30:49,260
So let me give you an analogy.

446
00:30:49,260 --> 00:30:56,220
If you take alpha zero, which plays chess, it has three ingredients.

447
00:30:56,220 --> 00:31:00,620
It's got something that evaluates the board position to say, is that good for me?

448
00:31:00,620 --> 00:31:04,140
It's got something that looks at the board position and says, what's the sensible move

449
00:31:04,140 --> 00:31:05,860
to consider?

450
00:31:05,860 --> 00:31:09,380
And then it's got Monte Carlo rollout, where it does what's called calculation where you

451
00:31:09,380 --> 00:31:13,300
think, if I go here and he goes there and I go here and he goes there.

452
00:31:13,300 --> 00:31:19,220
Now suppose you leave out the Monte Carlo rollout and you just train it from human experts

453
00:31:19,380 --> 00:31:24,340
to have a good evaluation function and a good way to choose moves to consider.

454
00:31:24,340 --> 00:31:27,100
It still plays a pretty good game of chess.

455
00:31:27,100 --> 00:31:30,540
And I think that's what we've got with the chatbots.

456
00:31:30,540 --> 00:31:35,460
And we haven't got them doing internal reasoning, but that will come.

457
00:31:35,460 --> 00:31:39,380
And once they start doing internal reasoning to check for the consistency between the different

458
00:31:39,380 --> 00:31:43,260
things they believe, then they'll get much smarter and they will be able to do thought

459
00:31:43,260 --> 00:31:44,500
experiments.

460
00:31:44,500 --> 00:31:51,260
And one reason they haven't got this internal reasoning is because they've been trained from

461
00:31:51,260 --> 00:31:52,580
inconsistent data.

462
00:31:52,580 --> 00:31:57,260
And so it's very hard for them to do reasoning because they've been trained on all these inconsistent

463
00:31:57,260 --> 00:31:58,260
beliefs.

464
00:31:58,260 --> 00:32:05,380
And I think they're going to have to be trained so they say, if I have this ideology, then

465
00:32:05,380 --> 00:32:06,380
this is true.

466
00:32:06,380 --> 00:32:08,100
And if I have that ideology, then that is true.

467
00:32:08,100 --> 00:32:11,420
And once they're trained like that, within an ideology, they're going to be able to try

468
00:32:11,420 --> 00:32:12,420
and get consistency.

469
00:32:13,100 --> 00:32:18,540
And so we're going to get a move like from a version of AlphaZero that just has something

470
00:32:18,540 --> 00:32:23,700
that guesses good moves and something that evaluates positions to a version that has

471
00:32:23,700 --> 00:32:27,620
long chains of Monte Carlo rollout, which is the corner of reasoning.

472
00:32:27,620 --> 00:32:30,220
And it's going to get much better.

473
00:32:30,220 --> 00:32:31,860
I'm going to take one in the front here.

474
00:32:31,860 --> 00:32:35,260
And then if you can be quick, we'll run and squeeze one more in as well.

475
00:32:35,260 --> 00:32:38,100
Louis Lamb, Jeff, I know you from a long time.

476
00:32:38,100 --> 00:32:44,940
Jeff, people criticize language models because of allegedly they are lacking semantics and

477
00:32:44,940 --> 00:32:46,620
grounding to the world.

478
00:32:46,620 --> 00:32:51,700
And you have been trying to, as well, to explain how neural networks work for a long time.

479
00:32:51,700 --> 00:32:57,140
Is the question of semantics and explainability relevant here or language models have taken

480
00:32:57,140 --> 00:33:04,740
over and we are now doomed to go forward without semantics or grounding to reality?

481
00:33:04,780 --> 00:33:10,660
I find it very hard to believe that they don't have semantics when they can solve problems

482
00:33:10,660 --> 00:33:13,980
like, you know, how I paint the rooms, how I get all the rooms in my house to be painted

483
00:33:13,980 --> 00:33:16,260
white in two years time.

484
00:33:16,260 --> 00:33:20,180
I mean, whatever semantic is, it's to do with the meaning of that stuff.

485
00:33:20,180 --> 00:33:22,100
And it understood the meaning.

486
00:33:22,100 --> 00:33:22,980
It got it.

487
00:33:22,980 --> 00:33:29,780
Now, I agree it's not grounded by being a robot, but you can make multimodal ones that

488
00:33:29,780 --> 00:33:31,900
are grounded, Google's done that.

489
00:33:31,900 --> 00:33:36,780
And the multimodal ones that are grounded, you can say, please close the drawer and they

490
00:33:36,780 --> 00:33:39,500
reach out and grab the handle and close the drawer.

491
00:33:39,500 --> 00:33:41,900
And it's very hard to say that doesn't have semantics.

492
00:33:41,900 --> 00:33:47,780
In fact, in the very early days of AI, in the days of Willigrad in the 1970s, they had

493
00:33:47,780 --> 00:33:53,060
just a simulated world, but they have what was called procedural semantics, where if

494
00:33:53,060 --> 00:33:59,540
you said to it, put the red box in, put the red block in the green box, and it put the

495
00:33:59,540 --> 00:34:00,780
red block in the green box.

496
00:34:00,780 --> 00:34:03,180
She said, see, it understood the language.

497
00:34:03,180 --> 00:34:08,380
And that was the criterion people used back then, but now that neural nets can do it,

498
00:34:08,380 --> 00:34:12,020
they say that's not an analytical criterion.

499
00:34:12,020 --> 00:34:13,020
One at the back.

500
00:34:13,020 --> 00:34:16,700
Hey, Jeff, this is Ishwar Balani from SAI Group.

501
00:34:16,700 --> 00:34:21,620
So clearly, the technology is advancing at an exponential pace.

502
00:34:21,620 --> 00:34:26,580
I wanted to get your thoughts, if you looked at the near and medium term, say one, two,

503
00:34:26,580 --> 00:34:33,300
three, or maybe five year horizon, what the social and economic implications are from

504
00:34:33,300 --> 00:34:37,900
a societal perspective with job loss or maybe new jobs being created.

505
00:34:37,900 --> 00:34:42,860
Just wanted to get your thoughts on how we proceed, given the state of the technology

506
00:34:42,860 --> 00:34:43,860
and rate of change.

507
00:34:43,860 --> 00:34:44,860
Yes.

508
00:34:44,860 --> 00:34:51,620
So the alarm bell I'm ringing is to do with the existential threat of them taking control.

509
00:34:51,620 --> 00:34:55,220
Lots of other people have talked about that, and I don't consider myself to be an expert

510
00:34:55,220 --> 00:34:56,220
on that.

511
00:34:56,220 --> 00:35:01,460
But there's some very obvious things that they're going to make a whole bunch of jobs

512
00:35:01,460 --> 00:35:03,460
much more efficient.

513
00:35:03,460 --> 00:35:08,460
So I know someone who answers letters of complaint to a health service, and he used to take 25

514
00:35:08,460 --> 00:35:12,140
minutes writing a letter, and now it takes him five minutes because he gives it to chat

515
00:35:12,140 --> 00:35:16,580
GPT and chat GPT writes the letter for him, and then he just checks it.

516
00:35:16,580 --> 00:35:22,580
There'll be lots of stuff like that, which is going to cause huge increases in productivity.

517
00:35:22,580 --> 00:35:26,260
There will be delays because people are very conservative about adopting new technology,

518
00:35:26,260 --> 00:35:29,140
but I think there's going to be huge increases in productivity.

519
00:35:29,140 --> 00:35:33,980
My worry is that those increases in productivity are going to go to putting people out of work

520
00:35:33,980 --> 00:35:37,100
and making the rich richer and the poor poorer.

521
00:35:37,100 --> 00:35:42,100
And as you do that, as you make that gap bigger, society gets more and more violent.

522
00:35:42,100 --> 00:35:48,380
This thing called the genie index, which predicts quite well how much violence there is.

523
00:35:48,380 --> 00:35:55,300
So this technology, which ought to be wonderful, even the good uses of technology for doing

524
00:35:55,300 --> 00:36:00,700
helpful things ought to be wonderful, but our current political systems is going to

525
00:36:00,700 --> 00:36:04,020
be used to make the rich richer and the poor poorer.

526
00:36:04,020 --> 00:36:12,940
You might be able to ameliorate that by having a basic income that everybody gets, but the

527
00:36:12,940 --> 00:36:22,380
technology is being developed in a society that is not designed to use it for everybody's good.

528
00:36:22,380 --> 00:36:29,980
A question here from Joe Castalda of the Globe and Mail, who's in the audience.

529
00:36:29,980 --> 00:36:35,980
Do you intend to hold on to your investments in Cahir and other companies, and if so, why?

530
00:36:36,980 --> 00:36:43,980
Well, I could take the money and I could put it in the bank and let them profit from it.

531
00:36:43,980 --> 00:36:53,980
Yes, I'm going to hold on to my investments in Cahir, partly because the people around Cahir are friends of mine.

532
00:36:53,980 --> 00:37:00,980
I sort of believe these big language balls are going to be very helpful.

533
00:37:01,980 --> 00:37:09,980
I think the technology should be good and it should make things work better.

534
00:37:09,980 --> 00:37:14,980
It's the politics we need to fix for things like employment.

535
00:37:14,980 --> 00:37:20,980
But when it comes to the existential threat, we have to think how we can keep control of the technology.

536
00:37:20,980 --> 00:37:25,980
But the good news there is that we're all in the same boat, so we might be able to get cooperation.

537
00:37:25,980 --> 00:37:33,980
And in speaking out, part of your thinking, as I understand it, is that you actually want to engage with the people making this technology

538
00:37:33,980 --> 00:37:40,980
and change their minds or maybe make a case for, I don't really know.

539
00:37:40,980 --> 00:37:45,980
We've established that we don't really know what to do, but it's about engaging rather than stepping back.

540
00:37:45,980 --> 00:37:52,980
So one of the things that made me leave Google and go public with this is to say,

541
00:37:52,980 --> 00:38:01,980
he used to be a junior professor, but he's now a middle rank professor who I think very highly of, who encouraged me to do this.

542
00:38:01,980 --> 00:38:04,980
He said, Jeff, you need to speak out there, listen to you.

543
00:38:04,980 --> 00:38:13,980
People are just blind to this danger and I think people are listening now.

544
00:38:13,980 --> 00:38:17,980
Yeah, no, I think everyone in this room is listening for a start.

545
00:38:17,980 --> 00:38:24,980
Just one last question and we're out of time, but do you have regrets that you're involved in making this?

546
00:38:24,980 --> 00:38:28,980
Cade Met's tried very hard to get me to say I had regrets.

547
00:38:28,980 --> 00:38:30,980
Cade Met's at the New York Times.

548
00:38:30,980 --> 00:38:38,980
And yes, and in the end, I said, well, maybe slight regrets, which got reported as has regrets.

549
00:38:38,980 --> 00:38:42,980
I don't think I made any bad decisions in doing research.

550
00:38:42,980 --> 00:38:49,980
I think it was perfectly reasonable back in the 70s and 80s to do research on how to make artificial neural nets.

551
00:38:49,980 --> 00:38:51,980
It wasn't really foreseeable.

552
00:38:51,980 --> 00:38:53,980
This stage of it wasn't foreseeable.

553
00:38:53,980 --> 00:38:58,980
And until very recently, I thought this existential crisis was a long way off.

554
00:38:58,980 --> 00:39:03,980
So I don't really have any regrets about what I did.

555
00:39:03,980 --> 00:39:04,980
Thank you, Jeffrey.

556
00:39:04,980 --> 00:39:06,980
Thank you so much for joining us.

557
00:39:12,980 --> 00:39:14,980
Thank you.

