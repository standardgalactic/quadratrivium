WEBVTT

00:00.000 --> 00:07.840
Hello everyone, welcome to your course, AI for Art, Aesthetics and Creativity.

00:07.840 --> 00:18.880
Today we have a very special speaker, Prof. from OpenAI, and he's going to talk about

00:18.880 --> 00:29.760
creating art and artistic work and images in general, these diffusion models and probably

00:29.760 --> 00:37.240
you have already worked with the glide collab, so he's going to walk us through that as well.

00:37.240 --> 00:45.880
So let's get us started, Prof. I always ask if please you can share with us what motivates

00:45.880 --> 00:51.880
you working in this space and also giving us a little background about yourself.

00:51.880 --> 00:57.880
For sure, thanks for having me here today, by the way this is really exciting.

00:57.880 --> 01:07.880
Yeah, so a background about me, I was an undergrad at MIT in computer science and math and then after that,

01:07.880 --> 01:13.880
I came to OpenAI to do AI research and I've been here for five years doing research on unsupervised

01:13.880 --> 01:21.880
learning, generative models, all kinds of things and what motivates me to do this research.

01:21.880 --> 01:31.880
When I was in college, I was excited by the idea of trying to understand what makes humans intelligent

01:31.880 --> 01:39.880
and I think I attended a few talks, which were really amazing and I felt like there's a lot of

01:39.880 --> 01:43.880
amazing progress happening in this field and I just wanted to part of it, see what's happening, see what

01:43.880 --> 01:49.880
I could contribute and then one thing led to another and here I am.

01:49.880 --> 01:59.880
I think so far, wouldn't say we are very close to unraveling what makes humans intelligent,

01:59.880 --> 02:07.880
but we've made a lot of progress I think in these years, so it's been pretty fun.

02:07.880 --> 02:13.880
Cool then, so I'll just get started.

02:13.880 --> 02:19.880
And anyone, feel free to just ask any question at any point, pause me if anything feels confusing,

02:19.880 --> 02:27.880
if any notation is understood. I don't see the chat window on my screen directly, so if you could just

02:27.880 --> 02:33.880
directly tell your question, that would be easier or if Ali, if you see something in the chat, just let me know.

02:33.880 --> 02:37.880
Cool, I'll get started.

02:37.880 --> 02:45.880
So I'll begin by just showing a few examples of very powerful creative ML models from the past few years.

02:45.880 --> 02:53.880
The first one you all might have already seen in news GP3, the language model from OpenAI.

02:53.880 --> 02:59.880
And one example I'm showcasing here is like these language models.

02:59.880 --> 03:10.880
They show like a very few examples of something pretty simple like here, like on the left you see examples of like poems by specific black writers.

03:10.880 --> 03:16.880
And on the right, then you can see once the model has seen examples of this kind, what it can generate.

03:16.880 --> 03:22.880
And it's getting poetry from just a few examples. This is pretty crazy.

03:22.880 --> 03:25.880
There's a second model I'll show you next.

03:25.880 --> 03:28.880
This is the audio playing.

03:31.880 --> 03:34.880
Yeah, we can hear your audio.

03:56.880 --> 04:00.880
You may bring it, and that's it.

04:00.880 --> 04:25.880
Anyhow, so that was a sample from a model for generating music called jukebox.

04:25.880 --> 04:33.880
And everything here was generated from the model, the music, the singing, how to sing it, how to pronounce the lyrics and everything.

04:33.880 --> 04:38.880
All the model was given was the lyrics and an artist and it produced all of this by itself.

04:38.880 --> 04:45.880
Does the generative model like produce the music and like the sentence separately or together?

04:45.880 --> 04:54.880
Everything together. Yeah, so because kind of how you sing something kind of has to go with the music that's provided by Swasa, right?

04:54.880 --> 04:59.880
It's kind of hard to like generate them separately from each other.

04:59.880 --> 05:01.880
What was that your question?

05:01.880 --> 05:07.880
So I can read from the chat that people are getting excited.

05:07.880 --> 05:13.880
Someone has a loud and other is saying this is freaking awesome.

05:13.880 --> 05:16.880
Thank you.

05:16.880 --> 05:21.880
Share or talk if you like.

05:21.880 --> 05:23.880
Oh yeah.

05:23.880 --> 05:28.880
Sorry, I can't see the chat window so feel free to just talk.

05:28.880 --> 05:33.880
Yeah, when we heard samples from like that from these models we were also amazed.

05:33.880 --> 05:47.880
The model I have here is the slide model you guys might have seen in papers. And here you have a model that given a text prompt is generating a visual representation of it or whatever it imagines what that the text kind of signifies.

05:47.880 --> 06:02.880
So you could see, and these are things that are go back to your point Ali you made earlier about composition. These things involve a model really having to compose a lot of different concepts together like robots meditating in a way past the retreat, but it is able to imagine this.

06:02.880 --> 06:17.880
In the last few years I think like ML models for such very hard creative tasks have become really good. And today we'll see like what are some of the kind of concepts driving this progress.

06:17.880 --> 06:33.880
And so before I even start down that route, like why are we trying to, you know, from a research perspective like trying to train models that you know create things. Well, one concept here is that, you know, this as this code by Feynman says what I cannot create I cannot.

06:33.880 --> 06:52.880
I don't understand training models that can, you know, create things be images or your video and so on. It's kind of one of the hardest tasks in those domains. And if you really care about whether models can understand images audio video so on, then one of the best ways to know

06:52.880 --> 07:01.880
if you're making progress out of these models are really learning something advanced is to see if they can create really complex things and really hard to understand things.

07:01.880 --> 07:10.880
And for people who care about representation learning or something this is one, one way you can know you're making progress on such stuff.

07:10.880 --> 07:25.880
And there has been a lot of progress in this field of, you know, trying to create things from models or what we call generative modeling. So here you see just in this very small domain of phase generation, things that GANs could create in 2014, versus things that they can create in

07:25.880 --> 07:32.880
2018, like, it's absolutely astounding how much progress has happened in the past few years.

07:32.880 --> 07:54.880
So, what is a generative model. So, you can think of what our inputs here in our data set to look like just a collection of examples, x1, y1, x2, y2, xn, yn, where x here represents, let's say an image and why some label or some other information describing this image.

07:54.880 --> 08:02.880
So you just have the sample from some natural distribution of images, p of x comma y.

08:02.880 --> 08:16.880
So you could have like images of corgis, ostriches, goldfishes and so on. You want to train a model that can learn this distribution you want to train a model that then ask for a corgi produces a corgi and ask for an ostrich produces an ostrich, or so on.

08:16.880 --> 08:28.880
You want to learn p of x given y, given some label y, corgi, ostrich, goldfish and so on. Can I generate a real image or an image that looks real from this distribution.

08:28.880 --> 08:34.880
And one such a model is trained, you can use it to generate novel samples.

08:34.880 --> 08:44.880
So you can generate corgis, ostrich, goldfishes that are actually real haven't been seen before but look like real images.

08:44.880 --> 08:54.880
One of the things I guess that matters is, is how you evaluate such models because if you if you don't have evaluation metric you can't tell you're making progress.

08:54.880 --> 09:07.880
And we won't go into too much detail here about these metrics but one of the metrics use was FID for measuring image generation and what these metrics are trying to capture is like fidelity and diversity.

09:08.880 --> 09:19.880
Fidelity would mean like how realistic or how correct an image looks versus diversity would be like how many different kinds of images such a model can generate.

09:19.880 --> 09:34.880
And so GANs were kind of like the state of the art for difficult image generation benchmarks before diffusion models came along which we will not talk about in our talk today.

09:34.880 --> 09:44.880
The progress in diffusion models has been pretty recent, it's been just like the last couple of years there's been a lot of papers and even in these people that you could see like things have been improving since 2020.

09:44.880 --> 09:52.880
And you could now generate realistic faces, you know, lots of different categories of images from ImageNet and so on from these models.

09:52.880 --> 10:08.880
So it's a pretty exciting field and these models in one of our recent papers we showed them to be actually better than GANs at generating images and so I'm pretty excited by these models and that's what I'm going to cover today.

10:08.880 --> 10:16.880
There's quick graphic here for like how things look like when diffusion models generate an image.

10:16.880 --> 10:19.880
Let me just play it again.

10:19.880 --> 10:24.880
So let's go to what these models are.

10:24.880 --> 10:36.880
As you saw in that graphic, you could see that that image was started out looking like noise and you finally got a real image out by like the slow process of noise converting to an image.

10:36.880 --> 10:51.880
And what is actually happening behind the scenes here is you have a fixed process that adds noise to a training image. So let's say you start with X0 on the left here as an image.

10:51.880 --> 10:54.880
So that's just a dog ball on the left.

10:54.880 --> 11:10.880
So a fixed process that slowly adds Gaussian noise to this image. So at each step you add a little amount of Gaussian noise. And as you go from left to right by the end, at the last time step B, you have just pure noise left.

11:10.880 --> 11:22.880
And what the model is trying to learn is to undo this process. It's trying to reverse it. It's trying to take some noise damage and be noisy a little bit, make it a little less noisy and so on.

11:22.880 --> 11:35.880
How do you obtain a generative model out of this? Well, if you train a model to reverse noise like this, then at test time when you actually want to use the model, you could start with pure noise at the end.

11:35.880 --> 11:44.880
You could start with XT. Then you could run it step by step backwards to remove noise from it and try to produce an image from it.

11:44.880 --> 11:51.880
Any questions on this diagram?

11:52.880 --> 12:00.880
Okay, I'll just check the chat to throw the question there.

12:00.880 --> 12:04.880
Okay, no questions.

12:04.880 --> 12:21.880
Okay, so let's remember the notation from your X0 is an example from the dataset, XT, capital T, XT is noise and there's intermediate steps, X little t's, we call them, where you have like some slightly

12:22.880 --> 12:38.880
noise image. And we can, to introduce more notation here, you could represent one step of the forward noising process with a distribution Q of X little t, given X little t minus 1.

12:38.880 --> 12:50.880
And right now, I'm going to use Gaussian noise as a noising process. So we're going to add a little bit of Gaussian noise to this.

12:50.880 --> 13:05.880
And the thing about here is this in this notation, there's some mean, which is centered around the original noise damage XT minus one, and there's some variance one minus alpha T here of how much noise that is being added to this image.

13:05.880 --> 13:16.880
So this is the forward process. This is the process that adds noise to images Q of XT given XT minus one. What we are learning is the reverse thing. We want to denoise this image.

13:16.880 --> 13:24.880
So we were learning P of XT minus one given XT. And what you can show is for very small

13:26.880 --> 13:35.880
noising steps where the amount of noise added is very tiny. The reverse process kind of looks like the forward process.

13:35.880 --> 13:44.880
This is kind of a set of points. Maybe I'll go in a little bit of an example here. Let's say, looking at a single pixel here, it had some real value on the real line, let's say

13:46.880 --> 13:56.880
0.8. And then you added a tiny amount of Gaussian noise to that thing. So it became 0.81 or 0.79 or so, depending on what noise you sampled.

13:56.880 --> 14:05.880
Now you're given this new value 0.79. This is the noise value. And you want to predict the distribution of what could have been the value it came from.

14:05.880 --> 14:16.880
If the amount of noise added, that you added in your step was very tiny amount of Gaussian noise, then the reverse prediction also looks kind of like a Gaussian.

14:17.880 --> 14:27.880
So it looks like, oh, it's somewhere around 0.79, some distribution that where you came from. I mean, 0.8 is pretty close to 0.79 in this situation.

14:28.880 --> 14:41.880
And so you could write that down as a model that is predicting the mean of this reverse process and the variance of this reverse process, mu theta being the mean and sigma theta here being the variance that you're trying to predict.

14:42.880 --> 14:48.880
So far, so good. Any questions at this point?

14:55.880 --> 15:10.880
Okay, so to summarize the notation again, XT minus one to XT is a process that's adding noise and the process we are trying to learn is XT to XT minus one to reverse this noise.

15:10.880 --> 15:18.880
And this looks like a Gaussian in the forward direction and predicting a new Gaussian in the reverse direction.

15:20.880 --> 15:35.880
A paper showed you could do some forms of training tricks to make this process simpler. You don't have to add noise little by little at every step. You could just directly sample an intermediate XT given your data example by just adding a lot of Gaussian noise to it.

15:36.880 --> 15:44.880
And it also showed instead of trying to predict the mean of the reverse process, you could just directly try to predict what noise was added to the image.

15:46.880 --> 15:50.880
This is possible because you could write the mean in terms of the noise that was added.

15:51.880 --> 16:11.880
Trying to predict the variance can be just simplified to just using a fixed variance or a learnt variance. We won't go into that today. All you should get from this is that to predict the reverse process where you are trying to predict the mean and the variance of the Gaussian to reverse that

16:11.880 --> 16:16.880
noising process, it's enough to try to predict what noise was added to the image.

16:18.880 --> 16:33.880
So how does this look like when you are actually training these borders? You take an image X0, you sample some random noise, you sample some Gaussian noise and you just combine these two to produce a noised XT.

16:34.880 --> 16:37.880
There's a formula here of how we've combined them.

16:38.880 --> 16:54.880
You can think of this combination as something that is kind of like interpolating between the image and the noise. So at t close to zero, you should just get the image X0 and t close to capital T, you should get complete noise.

16:55.880 --> 17:05.880
And this kind of interpolation factor, alpha t bar, kind of plotted it here, it goes from like one near t equals zero to zero near t equals capital T.

17:06.880 --> 17:11.880
This kind of controls how you interpolate between a fully denoised image versus a fully noised thing.

17:12.880 --> 17:19.880
And at training time, you're just sampling all possible combinations of mixing of noise and image and you're trying to denoise all these combinations.

17:20.880 --> 17:31.880
So what is the model trying to do now? It's trying to predict, as we said in the earlier slide, it's trying to predict what noise was added into the image.

17:32.880 --> 17:47.880
So you take in the noised XT, you take in what time step or kind of like an indication of where in the process you are, you tell the network, hey, I am at this step in the process, this is my noised image.

17:47.880 --> 17:51.880
What noise was possibly added to this image? So it's trying to predict epsilon.

17:52.880 --> 18:09.880
And what it's being used to train with is just like simple L2 loss, like just take the mean squared error of the difference between the network's prediction and the actual noise that you trained with and you try to minimise this loss so that you can train the network to predict what noise was added into the image.

18:10.880 --> 18:20.880
So intuitively, you can think of this as like, well, if I'm given a noised image, and if I can predict what noise was added to it, I can kind of like subtract that noise out, right, try to get a real image out.

18:21.880 --> 18:26.880
And this is kind of what is happening when you're training a diffusion model. It's learning to denoise images.

18:28.880 --> 18:29.880
Any questions so far?

18:29.880 --> 18:37.880
No questions.

18:38.880 --> 18:39.880
Okay.

18:41.880 --> 18:45.880
So what does the model that kind of does this denoising usually look like?

18:46.880 --> 19:05.880
Kind of models that we have in our papers usually look like these convolutional unit style models where the U kind of signifies kind of like how the shape of the model here in this picture is looking like, but to think of it as just like a model that runs a bunch of

19:06.880 --> 19:23.880
convolutional images, it kind of like down samples the image down into smaller and smaller spatial fields so that it can like learn features at different levels of granularity and then kind of samples it back into something that looks like a prediction of a noise.

19:24.880 --> 19:33.880
I don't have to go into the details of architectures, but just to give you an example of what kind of neural nets are trained to perform this task, this is how they look like.

19:35.880 --> 19:45.880
So, okay, we have a model that is trying to now denoise images, it's trying to predict the noise that was added to an image. How do we go back to actually getting a generative model out of this?

19:46.880 --> 20:00.880
As we talked about in an earlier slide, it's equivalent to predicting the mean of the reverse process, like you can write down the mean of the reverse process in terms of the noise prediction.

20:00.880 --> 20:09.880
So now that you have a network that can predict the noise that was added, you can also write down a network that can predict the mean of the reverse process.

20:09.880 --> 20:15.880
And once you have something that can tell you the mean of the reverse process, you can run the reverse process backwards.

20:15.880 --> 20:29.880
So you can start with noise x t, you could run, you could sample from this reverse process p of x t minus one given x t, you know the mean, and we have fixed the variance to something to do one step of sampling from this process.

20:30.880 --> 20:39.880
You do one more step, one more step and so on. And as we talked about what we're doing was denoising, right, we're trying to like learn a process that removes noise from images.

20:39.880 --> 20:46.880
So if you start from pure noise and you're denoising it one step at a time, by the end of the process, you would have something that looks like a real image.

20:48.880 --> 20:56.880
So, so what we've covered so far is how you train these models and how you sample from these models. Any questions so far?

21:00.880 --> 21:04.880
Okay.

21:04.880 --> 21:08.880
Um, it's okay.

21:08.880 --> 21:17.880
A bunch of theory there, but what you should remember is you train for denoising, and you can derive a sampler from it, once you've trained for denoising.

21:17.880 --> 21:23.880
What do you do next? Well, you could now make the model class conditional, you could provide labels at training time.

21:23.880 --> 21:33.880
So you could provide, you know, let's say you're training on ImageNet or something, you could have labels that say this image is a goldfincher, this image is a Corgi or so on.

21:33.880 --> 21:52.880
And you could make the model, the denoising model class conditional, you could provide these labels, the model so that, given this label, it tries to produce an image from p of x given y, like the distribution of images that are kind of represented by this label.

21:52.880 --> 22:03.880
And it's pretty simple, you just throw in a label into the model at some point, so that it now has this extra information when it's trying to denoise images.

22:03.880 --> 22:14.880
You could also do something like up sampling, you could ask the model, given this low resolution image, what would be kind of the high resolution image that could be generated from this.

22:14.880 --> 22:27.880
So again, just like throwing in a label y, you can throw in a low resolution image as extra conditioning information into the model as it tries to denoise.

22:27.880 --> 22:43.880
So, we've now talked about models that are class conditional. The thing is, if you just train models like this, where you give them a label for an image and you train them for producing the image given the label, they're not very good at doing this out of the box.

22:43.880 --> 22:58.880
They kind of produce very incoherent samples. And one of the tricks that we developed to kind of fix this was the trick of guidance, where what you do is you train a model to look at the images that are being generated.

22:58.880 --> 23:11.880
Use a classifier to classify what is the label of this image. So you kind of look at a noisy image and you're like, you know, whether you ask the model, hey, does this look like a dog or not.

23:11.880 --> 23:24.880
So you train a classifier on these noisy images, then you take a gradient of the classifier, you ask the classifier, hey, how can I increase the likelihood to make this image look like a dog.

23:24.880 --> 23:37.880
Because you can run the classifier forward, you can get a, get a probability from the classifier of it being a dog, you can also differentiate this function to get the gradient of how to change the image so that this probability increases.

23:37.880 --> 23:49.880
And then you augment your diffusion model with such a classifier to kind of guide it towards generating images that are more likely to be classified as a dog by the classifier.

23:49.880 --> 24:05.880
So how do we end up doing this in practice. Okay, so you can train a classifier on noisy images, you can just take your data set of images, noise them and train a classifier to predict the label of the noisy images.

24:05.880 --> 24:23.880
And then how do you guide now your generative diffusion model to use this classifier. Well, you run the classifier on the noisy images, you predict the probability of, you know, the class label under the classifier so whether something is a dog or not.

24:23.880 --> 24:41.880
You take the gradient of this prediction to obtain kind of direction for which the model should change its input to increase the probability of this image, you can add this direction into the mean of the reverse process that you are already going towards.

24:41.880 --> 24:59.880
So in terms of the actual formula, it just looks like adding an extra term to your mean prediction, which is the gradient of the log probability of the prediction of the label, given the noisy image.

24:59.880 --> 25:06.880
Questions on this, this is, this is important and this could be a little complicated.

25:06.880 --> 25:13.880
It seems there are two questions before this. Can you read them.

25:13.880 --> 25:16.880
Do you still need a classifier.

25:16.880 --> 25:21.880
Once the model is trained.

25:21.880 --> 25:28.880
So here by the model you mean the diffusion model, right.

25:28.880 --> 25:45.880
Yes, you still need it, because it is part of the sampling process, you're using gradients directly from the classifier in the sampling process, so you still have to keep the classifier around when you sample from the model.

25:45.880 --> 26:01.880
So if you look at the next question is the underlying representation of the classes for condition.

26:01.880 --> 26:19.880
I followed the question, Ben, could you explain.

26:19.880 --> 26:21.880
Okay.

26:21.880 --> 26:23.880
I have another question.

26:23.880 --> 26:28.880
The s in the term. Is that just a hyper parameter or

26:28.880 --> 26:35.880
we'll get to that term in the next slide but yes that is just a hyper parameter.

26:35.880 --> 26:52.880
The main thing from this slide is like we previously had a reverse process that looked like a Gaussian with some mean mu and some variance sigma. We now have a modified reverse process where we've just modified the mean mu with an extra term, which is scaled by this

26:52.880 --> 27:04.880
parameter s has the variance in it kind of for appropriate scaling as well, and it has this gradient term, which is the gradient of a classifier on noise damages.

27:04.880 --> 27:21.880
And we're kind of basically using this gradient to kind of guide the model towards directions where the classifier would predict a higher likelihood of the label being correct for the noise damage, so that the conditional model produces an image that is more correct.

27:22.880 --> 27:29.880
Another question. Why is the variance in the additional term also included.

27:29.880 --> 27:33.880
That's just how it popped out from the derivation.

27:33.880 --> 27:35.880
Oh, okay.

27:35.880 --> 27:52.880
I guess you can think of it as kind of like the step size of these things is controlled by the variance if you have a Gaussian with a very small mean in the reverse process and you don't want to take a really large step with your gradient, because you'll really, you'll pop out very far from

27:52.880 --> 28:06.880
the reverse process should have taken you to the reverse process is taking really small steps, which kind of can be thought about by its variance, and you also want to change that process only by that much amount.

28:06.880 --> 28:07.880
Does that make sense.

28:08.880 --> 28:13.880
So, I guess the variance terms like a cap in step size.

28:13.880 --> 28:22.880
Right, like to the maximum and the class very gradient is maybe somewhere between zero and one or something like that or

28:22.880 --> 28:30.880
I don't know if it has any explicit range here, but I mean it's kind of mostly just direction.

28:30.880 --> 28:34.880
Yeah, and you're kind of scaling this direction by a step size.

28:34.880 --> 28:45.880
And the extra hyper parameter is if you want to kind of like make these steps bigger or smaller than what is naturally there so that's the extra parameter s that we'll talk about in the next slide.

28:45.880 --> 28:47.880
Okay, thank you.

28:47.880 --> 28:55.880
Yeah. Okay, so the parameter s here. So what we found was if you just actually use the step size that pops out from the derivation.

28:55.880 --> 29:04.880
So as being one, no, no hyper parameter, it kind of doesn't do that much. So on the left you see the samples with as being one.

29:04.880 --> 29:11.880
They don't look like any image from any particular class, but it turned out when we added this extra hyper parameter and just bumped it up.

29:11.880 --> 29:19.880
So you have scale 10 here on the right, they actually start looking like samples from the distribution of a particular class.

29:19.880 --> 29:34.880
So you can think of this extra hyper parameter s as kind of helping the model focus on the modes of the distribution because you're kind of narrowing down the possible things that it produces, at least that's what we saw empirically.

29:34.880 --> 29:44.880
However, the trade off here is because you're narrowing it down well, they also kind of start looking similar to each other, the images that are produced.

29:44.880 --> 29:53.880
But anyhow, the way to think about the scale factor here is just that it's controlling how much guidance we're using how much is the classifier influencing the final outcome.

29:53.880 --> 30:05.880
And when you use a small value for us, it's not influencing that much when you use a large value it's influencing a lot, and the effect of their influences, you're kind of collapsing your distribution towards the modes of whatever the

30:05.880 --> 30:20.880
table that thinks is kind of the best representation of that label, they were very high scale will just collapse to the thing that the classifier is most likely to classifiers that they just not always what you want you want some kind of diversity, what you produce.

30:20.880 --> 30:27.880
There's some, there's some like intermediate value of scale that is kind of the best that you want to use.

30:27.880 --> 30:36.880
And this is kind of how the process looks like in practice here on the bottom you here you have a usual like diffusion process with scale.

30:36.880 --> 30:51.880
Let me see I can't see the image. So scale zero you're just using no guidance, and then when you turn on guidance, and now using the gradients of the classifier can nudge the process in the direction where it's more likely to produce that butterfly.

30:51.880 --> 31:02.880
So the scale up even higher you're nudging it even further out from its original reverse trajectory into this new trajectory, where it's now producing a very clear butterfly.

31:02.880 --> 31:16.880
So the scale parameter is kind of controlling how much guidance is happening and how much the model is being nudged out from its original distribution towards this new better distribution.

31:16.880 --> 31:33.880
So similar things, instead of labels you could now have text descriptions of images. So, same model class you're still conditioning on something, but this conditioning thing why instead of being a label is now a piece of text that's a robot's

31:33.880 --> 31:36.880
kind of a pass now retreat.

31:36.880 --> 31:48.880
And you could train basically the same kind of models, all you have to now change as well you don't have a classified now right, there's no classifier that is predicting a label you're that you have to predict the whole sentence, if you try to do that.

31:48.880 --> 31:53.880
So how can we do guidance in this situation.

31:53.880 --> 31:56.880
Well, first, let me go in.

31:57.880 --> 32:14.880
Oh, okay. Well, first let me go into how you can even pass in conditioning information to diffusion models, which look like text, you just, you can just simply run a transformer on the text and just attend to the representations of the text in the model.

32:14.880 --> 32:35.880
What's important is just a pictorial representation of how to deal with text being passed into these models, you can just run a transformer model on this and just have your original convolutional unit architecture attend to this model when it's trying to do the denoising.

32:35.880 --> 32:44.880
Back to guidance, how do you actually guide when you have text as the kind of label information. And one of the things you could do is use clip.

32:44.880 --> 32:51.880
I think you guys cover clip in a previous lecture Ali you said that so can.

32:51.880 --> 32:54.880
Yes, sounds sounds good.

32:54.880 --> 32:57.880
Yeah, so I'll skip clip. Okay.

32:57.880 --> 33:09.880
So I'm assuming you guys on the clip, but basically clip is a model where you have an image encoder and a text encoder, and it's trying to predict how close the representations of the image and the text.

33:09.880 --> 33:22.880
And so you can use clip for guidance, you can ask, hey, I have this noise damage, I have this text description, run the image encoder run the text encoder from clip, how close are these representations.

33:22.880 --> 33:33.880
If they're close, then you're going to get a high dot product here, you can take a gradient of this dot product and get a direction to increase this dot product.

33:33.880 --> 33:49.880
And that's the gradient you're going to use for guidance, gonna ask the model hey can you increase this dot product so the image, the not the image that you're trying to produce from the reverse process is close in representations to the representation of the text that you're provided.

33:49.880 --> 33:54.880
So this is how clip guidance works.

33:54.880 --> 34:01.880
What you can do, which we sure enough, which was short in a paper on classifier free guidance is you could skip the classifier completely.

34:01.880 --> 34:14.880
And just train a usual diffusion model for for reversing the process but train it sometimes without labels. So sometimes don't, don't tell it what was the text that described an image.

34:14.880 --> 34:22.880
And then at test time you ask the model, which direction should it go, given the label and which direction should it go without the label.

34:22.880 --> 34:43.880
And then you move your predictions in the direction of the model predictions when it was given the label so in the formula here if epsilon was the epsilon theta xt given why was the prediction of the model with the label and epsilon theta xt given

34:43.880 --> 34:53.880
the empty set five was its prediction without the labels, you're kind of taking the difference of these two and using that as your direction to kind of nudge the model in.

34:53.880 --> 35:00.880
And again, you have the scale factor s outside of this direction, which telling the model to move in the direction of the predictions with the label.

35:00.880 --> 35:06.880
And when you use as greater than one, you'll be moving a lot more in the direction of the predictions with the label.

35:06.880 --> 35:17.880
And the cool thing about this way of guiding is that you don't need a separate classifier or a clip model or anything, you're just using the diffusion model itself for guidance.

35:17.880 --> 35:29.880
You're directly just asking the model, it's own kind of prediction of which way it should go to increase the probability of the generated image being from the correct class.

35:29.880 --> 35:35.880
Any questions about classifier free guidance.

35:35.880 --> 35:54.880
What I have is, have you thought about implementing something like, okay, at this, at each stage, for instance, let's talk about the butterfly example at this stage I want to add something to this image.

35:54.880 --> 36:22.880
And so the text, you know, can gradually form the shape like for the image like, okay, I want the butterfly and then on top of it I want this flower and then this, you know, gradually giving more idea of how your butterfly want to be depicted.

36:22.880 --> 36:41.880
So you're doing this in steps for image, and then you are injecting the tokens from the clip to your, you know, your network for for image generation.

36:41.880 --> 36:50.880
And so what if gradually adding things that you want to be in that image.

36:51.880 --> 37:07.880
That's a great question, I haven't done this, like we haven't done this directly, but you can kind of do this right, you could like, you could run your reverse process to some point with with your text conditioning being just the simple thing hey it's a butterfly.

37:07.880 --> 37:18.880
Then you could continue with a new text prompt for guidance, hey, the butterfly looks like this or so on, and keep going. Maybe that works. Not sure.

37:18.880 --> 37:23.880
You could do something else where you just run the whole process, first generate a butterfly.

37:23.880 --> 37:34.880
You take the butterfly, you noise it to go back in the process, and then rerun it, but now with a different prompt, so you kind of modifying this generated butterfly in a new direction.

37:34.880 --> 37:48.880
Then, you know, noise it again and rerun it again with a slightly different prompt, you kind of be like slowly changing this generation iteratively in these like kind of like iterative modification.

37:48.880 --> 37:57.880
So in another slide later we'll show how to do this with something like in painting, but if you just wanted to do it for your direct image. This is maybe how you would do it.

37:57.880 --> 38:00.880
Does that kind of answer.

38:00.880 --> 38:11.880
Yeah, yeah, I think that's a very interesting, you know, thought and yeah I appreciate your answer.

38:11.880 --> 38:19.880
I think that in painting could be one way of thinking about it. Yeah.

38:19.880 --> 38:29.880
Yeah, but what I was trying to say there was like yeah, you could also do it without impending by like kind of modifying the full image by like renoising it and reproducing.

38:29.880 --> 38:47.880
Yeah, yeah, that also makes sense. I think that I was also referring to more like just the way that by removing the noise you are, you know, trying to somehow refine the image.

38:47.880 --> 38:53.880
This also in steps could, you know, add more context to the image.

38:53.880 --> 38:56.880
And there might be different ways of implementing it.

38:56.880 --> 38:58.880
Yeah.

38:58.880 --> 39:00.880
I think that's good.

39:00.880 --> 39:03.880
Okay, thanks.

39:03.880 --> 39:05.880
Okay.

39:05.880 --> 39:15.880
So, in our guide paper we kind of compare these two forms of like guidance for text conditional models clip guidance was classified for guidance.

39:15.880 --> 39:32.880
And here are a few samples like representative samples from the model. So on the left here is just samples without any guidance. This is just a pure conditional diffusion model there is no form of classifier clip guidance or classifier free guidance.

39:32.880 --> 39:43.880
And kind of see, you know, it's kind of getting the prompts of the pram pure steam glass window of a panda panda eating bamboo. It's kind of alright, but it's not very coherent.

39:43.880 --> 39:53.880
Then you do clip guidance with scale to start getting better with the classifier free guidance ones look the best in in all the tests we did.

39:53.880 --> 40:09.880
And I think part of this might be just that it's, it's not exactly the correct thing to use clip part of it just might be that it's kind of better inductive bias to use classifier free guidance.

40:09.880 --> 40:24.880
There could be a lot of reasons but at least empirically, this was working better in practice to generate more realistic samples from these models, and you can see guidance does make a big difference in, you know, generating more realistic things, but it also does kind of make, you know, it's

40:24.880 --> 40:34.880
kind of like more collapse effect happen all of these samples kind of start looking a little similar to each other when when you do a lot of guidance.

40:34.880 --> 40:46.880
So, what what what have we done here we've trained a model that you know given a text prompt can generate images and we've done it for this diffusion technique.

40:46.880 --> 40:59.880
And this was what the strain in the glide paper, and we then showed that this model actually was beating the older open a model, Dali, which was actually a bigger model, which was trained in a very different fashion.

40:59.880 --> 41:05.880
The strain is using an order aggressive model on these like discrete VA tokens.

41:05.880 --> 41:21.880
And it, the new diffusion model not only generated things that looked a lot more realistic it actually generated them faster and use fewer parameters. So, this new model class is actually a lot nicer to use for these tasks than the older class of models.

41:21.880 --> 41:32.880
One cool advantage of these models is also because they're not doing this thing or to aggressively they're you know just generating and hold image you can do these things that are much harder to do with these older autoregressive models can do things like in painting.

41:32.880 --> 41:38.880
So you could mask out a portion of the image, and then ask the model to kind of fill in that portion.

41:38.880 --> 41:49.880
And how would you do that well just like we passed our conditioning labels in the past you could just pass in kind of this like half filled image as extra conditioning information to the model.

41:49.880 --> 42:08.880
So you take this image and a mask on top of it, you provide this extra information to the model when it's trying to run its generative process and it's going to try to now think of this as a label hey like this is image what are the possible images that correspond to this label for this kind of

42:08.880 --> 42:28.880
a masked image, what are the things that could complete this image. And what you're providing is this kind of like image X zero with a little bit of region must and a mask and that tells what is the part of the image that has been masked.

42:28.880 --> 42:40.880
Now, to text condition and painting, you could provide an, you know, an image with a mask and you could also provide a text label to tell the model how it shouldn't paint the region.

42:40.880 --> 42:52.880
So these are examples from the paper. So on the left here, you have the text label being zebras roaming in the field and you have this image with a green mask on it. So the masked region was removed.

42:52.880 --> 43:05.880
And the model was asked to fill in this image conditioned on this product. So now it's going to try to fill in not only something that kind of completes the image correctly, like isn't there the conditional distribution but also kind of matches the product.

43:05.880 --> 43:15.880
On the right here you see something with a girl hugging a corgi on a pedestal. And it's kind of matching the style of the image very well here if you can see it kind of looks like it's like painting.

43:15.880 --> 43:26.880
Kind of nicely like blended in. So this is really cool thing which you can do very easily out of the box with diffusion models but it's kind of much harder to do with other classes of models.

43:26.880 --> 43:41.880
And you could take this idea iteratively, like you could now erase a region of image. So let's say we erased the region on the left here, and you first filled it in with, you know, a cozy living room, and you raised a different region.

43:41.880 --> 43:51.880
And you know, asked for a painting of a corgi on the wall above the couch. When you get a painting there, you raise another region, put a coffee table, put a flower vase and so on.

43:51.880 --> 44:06.880
So this is one way of kind of doing the thing you talked about Ali, but you kind of like generate things iteratively, but this is doing it from painting erasing regions, raising very specific regions, then asking the model to fill that region in with the thing you want.

44:06.880 --> 44:16.880
It doesn't cover all kind of modifications that you want to do, but it does cover things that you can represent as like adding things one by one into an image, if that makes sense.

44:16.880 --> 44:31.880
So like stuff that you maybe cannot do with this is like, you know, change the style of an image completely the full thing, because well, if you just erase the whole thing you couldn't, it wouldn't have anything to condition it can't use the style but things like this where you add things, you

44:31.880 --> 44:35.880
can do it pretty easily through iterative painting.

44:35.880 --> 44:41.880
Any questions so far on the painting side of things.

44:41.880 --> 44:48.880
So Linda is asking if the collab is available for painting I think I saw it on the website.

44:49.880 --> 44:59.880
Yes, the collab that we released in the guide repo the thing the third one is the second one that one doesn't painting.

44:59.880 --> 45:10.880
Basically, all you like to do that is you'll have to provide this extra image, and you'll have to provide a mask or like mask out a portion of the image and then provide a mask that tells what has been masked.

45:10.880 --> 45:20.880
And then you just run the guided diffusion process as usual, but now with this extra information to try to impaint this region.

45:20.880 --> 45:25.880
I'll go into kind of notebook later but yes, the notebooks there.

45:25.880 --> 45:29.880
Diego also has a question.

45:29.880 --> 45:36.880
Can you remove objects using printing. Yeah, I mean, so let me go back to the slide.

45:37.880 --> 45:51.880
I mean, I guess, technically, in the very first one we removed the thing right we just must out whatever was on the wall in the left. And here we ended up adding a painting of a colleague but you could just ask for nothing.

45:51.880 --> 45:54.880
And then it would just fill it with the wall.

45:54.880 --> 45:59.880
I don't know if there's an example here.

45:59.880 --> 46:08.880
Yeah, in all of these things we kind of change something modified something but if you just don't give any prompt is just going to try to fill it.

46:08.880 --> 46:17.880
Without any extra information is just trying to make the best possible completion, and that could be kind of like removing an object.

46:17.880 --> 46:24.880
Does that answer your question.

46:24.880 --> 46:30.880
Assuming yes, never move on.

46:30.880 --> 46:32.880
Okay.

46:32.880 --> 46:46.880
Well, you can take this idea further and you can do out painting kind of so like previously we drew a mask that was inside the image, but you could also kind of move the rectangle that the model is focusing on outside the image.

46:46.880 --> 46:57.880
So now the mask looks like a strip of things around the image that is masked, and you can ask the model to fill that thing, and you could keep moving this rectangle around to kind of expand out from an image.

46:57.880 --> 47:14.880
This is something that Holly heard, and then she like to this central image here and then she kept moving the square out kind of expand out the canvas of the model and ask you to keep filling in extra information outside of the region.

47:14.880 --> 47:23.880
At the end of the day for the model is there is just like some conditioning information some mask, and it's going to just try to fill in in that region whatever it thinks is the best possible completion.

47:23.880 --> 47:29.880
It doesn't have to be inside and be outside as well.

47:29.880 --> 47:39.880
So, one other, I guess, important thing is like, we talked about the release notebooks the breeze notebooks is kind of the released model which is the filtered glide model.

47:39.880 --> 47:56.880
So in our paper we talk about this where we, if we looked at, you know, kind of the things you could generate with the big original glide model, and there were a lot of, like, problematic things that it could generate that made it unsafe to release the full big model.

47:56.880 --> 48:12.880
So release a small smaller model on a filter data set. And it cannot generate things that look as impressive as the big model, but it's still kind of can generate realistic looking images for like some of these easier prompts.

48:12.880 --> 48:21.880
But yes, there was going to be a little bit of a performance gap between using the filtered small model that has been released versus for the best images you can see.

48:21.880 --> 48:30.880
You can still generate a lot of cool things with the small filtered model. These are some of the things I found on Twitter that people have generated with the notebooks that we released.

48:30.880 --> 48:32.880
So on the left here.

48:32.880 --> 48:43.880
I think what they did was, they kind of did the painting thing, but they just went kind of like in a panorama fashion left to right, and kind of kept asking the model to fill these landscapes.

48:44.880 --> 48:50.880
The guidance scale a lot to make it very artsy think in the right.

48:50.880 --> 48:57.880
I've kind of problem like they've done this out painting thing but

48:57.880 --> 49:11.880
I don't know how they got those structures, but I think a part of this part of the fun stuff here is kind of these prompt search or prompt tuning things where you kind of find these prompts and generate very specific and artistic styles.

49:11.880 --> 49:21.880
And if you find very cool prompts and you can now use these tricks of our painting and so on, kind of like keep expanding it out to generate these cool pieces of art.

49:21.880 --> 49:33.880
This is another thing I found on Twitter where they trained a classifier for guidance model on conceptual captions and I think this is like a flower a space flower with some space.

49:33.880 --> 49:35.880
Our team.

49:35.880 --> 49:36.880
Super cool.

49:36.880 --> 49:39.880
There is a question.

49:39.880 --> 49:41.880
You want to read it.

49:41.880 --> 49:43.880
Let's see.

49:43.880 --> 49:47.880
What did it create that was dangerous.

49:47.880 --> 49:51.880
So, or maybe comment.

49:51.880 --> 49:53.880
Oh, sorry.

49:53.880 --> 50:07.880
I think I, yeah, what did it create that was dangerous. Yeah, I guess, well, for all the days I would recommend just reading a paper. I mean, there were, and I wasn't the one who did the safety analysis here was the opinion.

50:07.880 --> 50:11.880
The people who work on safety at opening and Alex.

50:11.880 --> 50:29.880
But I think it was stuff like violence it was stuff that could be used for like, if it makes it for like misinformation and so on. But I mean these models are pretty powerful so you could generate lots of things that you don't want to be floating on the internet.

50:29.880 --> 50:44.880
I mean, the trade offs are hard here right because like, on the one hand you do want to, you know, put these powerful models out there in the hands of people to like, generate all these nice cool art and like, like lots of positive use cases right.

50:44.880 --> 50:54.880
But I think you want to also be conservative to not create a lot of like stuff that you don't want floating around on the internet that's associated with your models.

50:54.880 --> 50:57.880
You know, this is a tough trade off.

50:57.880 --> 51:11.880
I think it's, it's nice that we can still release some safe model that people can use, but making these models like fully safe when they never generate something that is like, kind of like not a, not a good thing.

51:11.880 --> 51:15.880
It's very hard problem in general.

51:15.880 --> 51:28.880
So you need to find more like detailed examples in our paper, if you're looking for like specific examples, but that was kind of our line of thinking on like releasing like the small filter budget.

51:28.880 --> 51:41.880
Okay, maybe a slight tangent but what what does the process look like of let's say calling, you know, the unsafe parts away from from the model, like how do you go about that.

51:41.880 --> 52:01.880
So calls usually I guess training these training kind of like these classifiers to filter out portions of the data set that could be like not safe, if you could, you know, train an NSFW classifier you could train a classifier for like hate symbols you could train a classifier for

52:01.880 --> 52:12.880
other things, then you, once you have labeled data on which you can train these classifiers labeled data for like I don't know real images that you consider things that you don't want to model to generate.

52:12.880 --> 52:24.880
You could run these classifiers on your training data set filter it out, then train a model on the filter data set. So hopefully the model will never go into regions where

52:24.880 --> 52:29.880
there's nothing like that because it was never part of the training data.

52:29.880 --> 52:32.880
Yeah, awesome. Thank you.

52:32.880 --> 52:43.880
Okay, so just a quick like some like look into the notebooks that we've released this. This is just like some useful.

52:43.880 --> 52:55.880
Some parameters that you will have to like, kind of like deal with when you're trying to generate stuff from the notebooks that you released. Well, there's the two scales we've talked about in a talk today the classifier free guidance scale and the clip guidance

52:55.880 --> 53:09.880
scale, the small values for these scales will generate, you know, more diverse, but not very coherent samples larger values will generate coherent things very large values will generate like very artsy looking things.

53:09.880 --> 53:18.880
The classifier free guidance scale, like, I think three might be the default but you could try five 1020 or so on to generate more artistic things.

53:18.880 --> 53:21.880
Similarly for the clip guidance scale.

53:21.880 --> 53:36.880
Time steps kind of controls like how many little steps you take in the diffusion process. I think by default we use 100. There's 100 steps of like iterative denoising that will happen so if you use a higher value to look more sharp, but you'll also spend more

53:36.880 --> 53:39.880
time creating a sample.

53:39.880 --> 53:42.880
So it was a good like trade off that we used in the thing.

53:42.880 --> 53:56.880
Finally, the further in painting notebook, you would have to provide an extra thing which is like, what is the region of a given image that you want to paint so you would have to provide, let's say a 64 by 64 image that you want to impaint and some

53:57.880 --> 54:09.880
that you've like removed that you kind of specify with a mask, which is like, I think one in places where the image is not masked and zero in the places where the image is masked.

54:09.880 --> 54:20.880
I could be wrong on the zero versus one, so you should check the notebook for which direction, basically it's a binary mask that tells this is the portion of the image that is masked, this is the portion of the image that is unmasked.

54:20.880 --> 54:26.880
And the rest is like, just just a usual image with three channels that you provide as extra information to the model.

54:26.880 --> 54:30.880
And do you just upload that as an image file.

54:30.880 --> 54:44.880
I think the way in the notebook that works is, so if this is on a co lab you'll have to have the file on drive and then you open it using pillow image dot open or something. I don't know if there's like a direct upload button.

54:44.880 --> 54:46.880
But I guess like,

54:46.880 --> 54:56.880
Oh, sorry, I guess my question was, like when you add the mask, like the mask is just like removing part of like a regular image file or like there's something more to it.

54:56.880 --> 55:07.880
Oh, yeah, that's just removing parts of the regular file. So like, I think if you want to do it programmatically just just zero out that region.

55:07.880 --> 55:09.880
Does that answer the question or.

55:09.880 --> 55:12.880
That makes sense. Yeah, thank you. Yeah.

55:12.880 --> 55:26.880
There's an example in the notebook and there's a cell in the notebook that kind of masks an image that might be more clear where you can see like, you are loading an image from the disk then you are kind of like removing a region then you are kind of writing down a

55:26.880 --> 55:33.880
mask that specifies what you removed and then you pass in all this information into the model.

55:33.880 --> 55:43.880
So I think that's it for the stuff you will need to kind of like apply this thing to the notebook.

55:43.880 --> 55:56.880
And if you want more further reading or like what we talked about today, I mean I try to focus on mostly like things you will need to understand for like kind of generating art from these things but you want to go into more detail about the

55:56.880 --> 56:06.880
design of these models. I think the best paper would probably be the denoising diffusion probabilistic models paper by Jonathan Ho, the DDPM paper.

56:06.880 --> 56:19.880
That has kind of like the basic theory of the models that we talked about today, and there was our paper on diffusion models began to imagine this is that kind of talks about the guidance trick for classifier guidance, and then the glide paper

56:19.880 --> 56:34.880
to this for text conditional models where we talk about clip guidance and classifier free guidance. There was also the paper by young song generative modeling best made ingredients of the data distribution which kind of like was before the Jonathan

56:34.880 --> 56:46.880
Ho approach this problem from a very different perspective of score matching, and in the DDPM paper Jonathan Ho and others showed how it's kind of equivalent to score matching.

56:46.880 --> 57:00.880
So if you want to understand diffusion models from a different lens, I think I would strongly recommend that paper, and these two blogs as well by Lillian and Yang on diffusion models and score matching models, basically like two sides of the same kind of understanding from both

57:00.880 --> 57:06.880
the super useful to see you know why these generated models work.

57:06.880 --> 57:11.880
And that's it.

57:11.880 --> 57:16.880
Thank you so much it was very, very interesting and fascinating.

57:16.880 --> 57:18.880
I'm very inspiring.

57:18.880 --> 57:20.880
Are there questions.

57:20.880 --> 57:21.880
Go ahead.

57:21.880 --> 57:23.880
Super cool.

57:23.880 --> 57:24.880
Thank you.

57:24.880 --> 57:40.880
Question. Did you notice if there was any relationship between say, like, if you fed it to dimensional noise, and if you were to step through the X or Y coordinates, did you notice if there's any relationship between the coordinates used for two dimensional noise and the output you can

57:40.880 --> 57:42.880
Say that again.

57:42.880 --> 57:44.880
What did you say at the end.

57:44.880 --> 57:55.880
If you use two dimensional noise is there a relationship between the coordinates that you use for the two dimensional noise map and the outputs of the model.

57:55.880 --> 58:03.880
Like is there a relationship that that that you observe between the noise that you use and the model and the outputs of the model.

58:04.880 --> 58:11.880
There's a little bit so like one way you can do this is you can like fix the noise, the sample and change the label.

58:11.880 --> 58:20.880
And you can see that the generated images for the same noise but different labels kind of have similar like perspective and spatial structure.

58:20.880 --> 58:25.880
So like, but but they look kind of like images from different classes.

58:25.880 --> 58:34.880
So this is definitely control some aspect of you know how the final output looks like and there's some kind of spatial like connection, but it's not an exact direct connection.

58:34.880 --> 58:37.880
Does that kind of answer your question.

58:37.880 --> 58:39.880
Yeah, got it.

58:39.880 --> 58:48.880
I love this in our I think diffusion models beat guns paper. I think that in the appendix we have an example where we like do this specific thing.

58:48.880 --> 58:52.880
It's actually more directly connected when you use a different sampling method.

58:52.880 --> 59:00.880
So, I showed you there is reverse process right where at each step you're doing this reverse step with the Gaussian.

59:00.880 --> 59:12.880
At each step you're adding a little bit of noise when you sample from the Gaussian, but there's a different way of reverse sampling from these models which is called ddim is another paper on that, where you just sample noise once at the start.

59:12.880 --> 59:29.880
And then you just run a deterministic reverse process to sample from the model in that case that there's kind of like this one to one correspondence between the noise and the generated image, and then it's more clear to see this.

59:29.880 --> 59:44.880
Thank you.

59:44.880 --> 59:49.880
I can ask my earlier question again, I guess if nobody else is not a question.

59:50.880 --> 01:00:07.880
So I guess, if you're using a diffusion model without clip. Right. So I guess what's being trained is the classifiers for the labels that am I understanding that correctly.

01:00:08.880 --> 01:00:10.880
For the denoising process.

01:00:10.880 --> 01:00:23.880
Yeah, I guess I'm trying to understand if without clip. How does it know what to denoise to without like some representation of the text that you're feeding it.

01:00:23.880 --> 01:00:26.880
Yeah, yeah, so.

01:00:26.880 --> 01:00:42.880
Well, yeah, if you train a model or denoising model without text labels, then it doesn't know where to go and the only way you can generate a sample for a given text distribution it would would be through like clip guidance or something, but we do have.

01:00:42.880 --> 01:00:56.880
We train these models to be text conditional diffusion model. And in the classifier free guidance case, you train it with or without the labels. So maybe I can go back to one of our slides here.

01:00:56.880 --> 01:01:08.880
The way the model, the reverse process model sees the text is through this kind of conditioning on the, the representations output by a transformer on the text.

01:01:08.880 --> 01:01:10.880
Okay, got it.

01:01:10.880 --> 01:01:14.880
So this text conditioning here is without clip.

01:01:14.880 --> 01:01:18.880
Yes, so.

01:01:18.880 --> 01:01:34.880
Can I point a glossed over in guidance was you could use guidance on top of unconditional models or conditional models. So you could have a reverse diffusion model that isn't conditioned on any labels, then it wouldn't have any way of actually like producing an image given a class.

01:01:34.880 --> 01:01:42.880
But then you could use guidance on top to get it to produce an image giving a class, but you could also use guidance on top of conditional models themselves.

01:01:42.880 --> 01:01:53.880
So you could have your original model be able to produce an image text, like we did here, but also use guidance on top to make it even better at doing this.

01:01:53.880 --> 01:01:55.880
Got it. Okay, yeah.

01:01:55.880 --> 01:01:59.880
This one.

01:01:59.880 --> 01:02:09.880
Okay, thank you.

01:02:09.880 --> 01:02:19.880
Okay, excellent. If are there more questions.

01:02:19.880 --> 01:02:24.880
Well, I think that we can wrap up the session.

01:02:24.880 --> 01:02:27.880
Again want to thank you a lot.

01:02:27.880 --> 01:02:32.880
Profile and it was, it was great. Thank you so much.

01:02:32.880 --> 01:02:41.880
Thank you. And thank you for having me and feel free to just like email me any questions later or DM me on Twitter with questions.

01:02:41.880 --> 01:02:56.880
I think there's a lot of cool stuff out happening in this so like, I would strongly recommend doing some of the like reading some of the blogs are reading just like things that you can find in other collab notebooks as well.

