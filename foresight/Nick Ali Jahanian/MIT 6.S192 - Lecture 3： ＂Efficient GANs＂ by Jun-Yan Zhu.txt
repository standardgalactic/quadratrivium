Okay, hello everyone. Welcome to this session of deep learning for art, aesthetics and creativity.
Today, we have our specialist speaker, Junion Ju, and he is an assistant professor at CMU, School of Computer Science.
He is going to talk about efficient GANS, and Junion is such a great researcher and scientist who has been working on many, many interesting generative models, including he was in peaks to peaks,
and cycle consistency scans, and Gauguin, and many other interesting and intriguing work, and I hope that we can see a gist of some of his work here today.
So, please, Junion, go ahead.
Yes, thanks for the nice introduction. I'm very happy to be here and talk about some of our recent work on how to make GANS more efficient.
Maybe some background that you probably already knew about it. Yes, GANS has been used for various content creation and creativity tasks.
For example, you can draw a sketch of a handbag or edge of a handbag and you can get an output image from the GANS.
And one recent model we have been working on is this Gauguin model, led by Thaisong Park and other people.
Here, our artist is creating a semantic label that basically says this is a mountain, this is also a log, this is the sea.
And when the mountain, he can add some sand, and we can generate an image in real time.
And he can add some more details like logs.
Yeah, and you can create a pretty nice image very quickly.
That's a very cool demonstration of the GANS and you can also, sorry about that, you can also apply a particular like a style image from a truly set and then apply it to stylize this image.
So these all look very nice and kind of perfect, right, those cool demo in front of so many people.
But if we close the door and the story here is, it's really very expensive, at least for this Gauguin project.
And to develop Gauguin project, Thaisong has about hundreds of GPUs, in the summer in turn, and for six or nine months he has access to several hundreds of GPUs, very high end, the media GPUs, which you cannot sometimes you cannot even buy it on the market,
but he has access to internal GPUs, we are not allowed to share the information.
But he has access to lots of GPUs during training, and the training also requires lots of data, like tens of thousands of data.
After almost a year's development, to learn this model, to do this demo, we actually bought a very expensive desktop, like 6,000, 7,000 laptop with actually a high end GPU, but that's still a laptop, not a desktop, so we want to carry this laptop around and do the demo, right.
So in conclusion, from this particular project I realized games are very expensive, perhaps not for everyone now at this moment.
I think it's expensive in three ways.
So for learning a game model, if you train a game model to learn the game model on a device, you need a high end GPUs, like for real time performance, otherwise it will be very laggy, you cannot have any real time demo without a high end GPU.
So in order to develop the algorithm, to develop a new algorithm, you require hundreds of GPUs to train the model.
So I think it's very, it's huge, or maybe right now only the deep end of the media can afford this kind of computation. You will for university lab, MIT or CMU, maybe you don't have so many GPUs, it's very hard to compete with big companies.
And third, to train a game model, it often requires lots of data.
And I will go through it one by one. The connection wise is very, it's very slow to learn on a CPU and mobile device, right, maybe take 10 seconds or several seconds to learn on models like CPU and even slower to learn on mobile device like a tablet or like a phone.
To train a single model for a single training session, it takes several days, maybe one or two weeks, maybe sometimes for a big game, maybe take amounts.
And, but it's not like you just train one model and publish a paper as model training might take at least a dozen of training sessions, you have hundreds of training sessions.
So each training session takes, each training session takes a month, you need lots of GPUs to parallelize your experiments.
So it's not like you have an idea, you train a model and you go through all the public paper, that's not the case.
You have an idea, you try something to work, you try something to modify the idea slightly, you have lots of iteration over the months.
So each iteration takes one week or one month on each GPUs. So if you, if you put these numbers together, if you want to actually do the research very as quickly as possible, it requires lots of GPUs.
Sometimes it's even like a top university like top lab in MIT or CMU Berkeley cannot afford it, not saying other labs.
So what people do realize is to treat this model like require tens of thousands of millions of images.
You treat a model of faces, it requires 70,000 faces.
These faces are very high quality images, and you need to align the face before you treat the model.
So there are lots of pre-processing steps to get an image, to treat an image that's model, you need labels, and you need to limit the data sets first.
So there are three things that can be used for more users, right?
If you are a content creator, if you are an artist, you may not have access to the computation, to the algorithm and data, right?
So in this talk, I would like to focus on computation and the data.
But while working on the algorithm, in this talk, we have some results on how to make data faster, how to make you can treat a game model on maybe only hundreds of images.
By that, we can maybe help more users, content creators and artists to treat and test their own game models without having access to lots of data, without having access to very high-end GPUs.
So I will talk about the first part, which is the model.
GANS are very, very computational expensive to run.
If you compare GANS, or conditional GANS, like Psychogalgan to the typical image classification model, this is the computation, right?
Maybe some people like to use Macs, some people like to use Flops, but the story is the same.
It's quite cheap to run a mobile internet for image classification purpose.
It's very relatively cheap to run a restats network for classifying caster versus dogs versus other categories.
But somehow it's very expensive to run a Psychogalgan model.
This gap is almost 500 times.
If you compare the latest generative model versus the latest classifiers.
And that's a general trend.
I think you may ask why, right?
Why is so much more expensive?
The reason for that is, like in Psychogalgan, you take an image and you produce the image with the same resolution.
So the spatial dimension of the tensor remains the same more or less through the process.
When you classify, you take an image and you produce a number.
So the spatial resolution of this tensor becomes smaller and smaller.
I think that's part of the reason, but also to generate the image, maybe you need more features.
That's the second reason.
Anyways, it's quite expensive for GANs that compare to the image classifiers.
In this work, I will briefly go through this work.
If we try to solve this problem, we propose a method called GAN compression.
GAN compression is a general purpose framework based on the distillation channel protein and neural active search.
So the idea here is, so given a teacher model, teacher model is the original model for the psychogalgan model we like to compress.
It has lots of channels.
So you take a horse image as input in psychogalgan case, you output a zebra.
And we also have a student, so the goal of this project is to try to find a student model, which with fewer filters,
and it can produce the same kind of zebra as the teacher model.
So we have a loss function to try to make sure the output, the student output zebra looks very similar to the teacher's output zebra.
That's what loss we have.
We also try to make sure that the student's intermediate feature representation is very similar to teacher's representation.
And lastly, we would like to make sure we have a GAN loss here.
We would like to make sure the student's output zebra looks like a zebra according to adversary loss.
So we have three losses.
We have the pixel loss.
We try to make sure the student zebra looks like a teacher zebra.
And we have a feature loss which makes sure the student's feature looks like student's feature looks like a teacher's feature.
We have a third loss, which is GAN loss, just like a typical GAN loss you apply to any GAN.
Yes, you want to make sure the zebra looks like a zebra.
Okay.
So this is our distillation part.
In our channel protein part we would like to, so our goal is to search for each layer, the optimal number of channels which can, we want to find a smaller number of channels,
but can still satisfy all these loss functions constraints.
So we have a bunch of channels here, for example, we have 60 channels, you can choose 32 channels, and you can choose maybe 48, 64 if you like.
And I can choose 24 if you like.
So we have channel protein.
So the idea is if you have fewer channels for each layer, your model will be smaller and faster to run.
Let's say if you reduce the channel by half for every single layer, of course the model will be faster to run.
But here, the search means we try to, we're not trying to reduce the channel uniformly, we try to maybe, for some layer we try to reduce it to 16 channels.
For some layer maybe 24 channels are necessary, or 32 channels are necessary.
So we try to, it's kind of like a search problem, the search space is just like channels per layer, you have eight layers, you have eight numbers you would like to search for.
If you have 11 layers, you have 11 numbers you would like to search for.
So the idea is we try to find this combination so that we can still reproduce a zebra which looks like teacher zebra.
Okay.
So how to do this search problem?
The search seems very hard because for each configuration you need to train the model.
And this is a combinatorial number, right?
There are lots of combinations, you can, if there are four combinations, four choices per layer, that's lots of combinations.
You don't like a naive way is you can train a model for each configuration and then compare which model works best.
But that will take lots and lots of training time.
So the idea here is we would like to have all the network to share the ways.
The idea is if you have 32 channels, share the same first 60 channels with the 60 channel model.
So by doing that, we don't have to train them all.
Each individual configuration, every single time, we can share the ways across different configurations.
So we still have only one model training.
But each time we sample a configuration, it's like this configuration, that configuration, and we try to train this configuration with the loss.
But we only have one training session.
So by each training iteration, we sample a different configuration and try to train the model with the loss function I just described.
Okay, so once we are done with the training, we can, then we can, then we can, for every single configuration, we can try to evaluate these models based on a matrix such as FID,
and then there's other metrics you have and choose the best one, and then we can fine tune that model.
Yeah, so there are a bunch of loss function, we kind of, so there are several ideas, one idea is there are several loss functions, we try to mimic the teacher model.
But it's actually such ideas, we try to share the ways across different configurations, so that we can avoid treating each configuration multiple times.
Every single time.
Okay, so that's the idea of the method.
I would like to share some results with you.
So here we are so for the house to zero, we can reduce the model size for 57.
The model size, the competition cost for 57 max to 2.7.
7 to edge to shows, 7 to Gauguin.
So we can achieve nine times to 24, 20 times compression ratio in terms of the model computational cost.
Here is a demo is you will learn the original cycle again on this mobile device, Jason Javier GPU, and we can measure the FPS.
Here is the, here's how it looks like you want to transform a house to a zebra.
And here is we learn the same more compressed model on on this hardware. So I should see four times kind of speed up and it gets real time performance on this.
This is a kind of like like on device chip you will use for like robots or cars.
But here model is out here the input image.
Got a cycle get output.
Here is a baseline.
You wish you just reduce the will cause 0.25 cycle guy.
In this setting you basically just reduce them all channels.
Per layer.
And you lose them a channel to like by by force, by four times, but we do it for every single layer so there are no, no, no, no, no search happenings.
You're very nice baseline. And you can see if you just reduce the demo channels uniformly every for every single layer, you will lose lots of details like zebras stripes which is something you would like to have.
Okay.
And here is the compressed model.
20 times compressed model with the albaster.
And you can preserve the output of the original teacher model.
But why be 20 times faster.
So recently we apply this idea to not only to cycle game but also to style get to this is ongoing submission.
So, so the idea here is, we would like to have a teacher model.
I would like to have, we will obtain a student model just like what we did before.
And during the, the idea is doing the image projection will try to try to the ideas will project this image into the latent space of the gains.
And then we add some different like directions like smiling directions like glasses directions, maybe different hair color.
And here's we can do the interacting editing, we can use this low cost model, 10 times fast model, which only takes three seconds each time you adjust the slider.
I will show a demo in a minute.
But, but when was we the user, for example, you use someone to change the hair from black to to to to white.
You can do that right and you can change this one to this one.
But once you, once you, the user is done with the entity, you can use the original model to get the final output.
The idea is that you want to like to make sure the smaller models output is consistent with the big models output through this editing process.
You will get, you learn a smaller model and you get something what you want, but, but once you press the button off, you're writing a big model.
If you get something completely different.
So this preview is not informative anymore.
I will show you a demo as an animation in this video.
So here is an import image right to project it and ought to do to we like to look, look and see it again.
And we try to find the best lately called which going to produce this input image.
And so we have what we have done that we can like move this code around maybe we find this smiling call smiling directions.
all I might have already mentioned is how you can find the directions. You can find that,
like you move here, maybe you make your face smiling and you move to a different direction,
make your hair a different color. And then the idea is we can generate an image
after you change the directions. And here is the original image.
Here we used GANs to reconstruct this image, to represent this image. This is the image
generated by GAN. This is the image of original image. And here are a few sliders.
And we would like to modify this image in various attributes.
And we'll run it on these CPUs, like just Intel CPUs. But if you click
smiling, here we click smiling, it took about three seconds to produce image with smiling face.
So you cannot drag the slider anymore, you can only click. And then you make younger or older.
It's just very laggy, right? And the idea is you would like to, if you click something or drag
the slider, it should get the results immediately. And here is our idea.
And you would like to choose our monocleidic cost GANs. And you can click something.
You only took 3.3, 3.4 seconds.
That's very, so you get very fast interactive feedback.
Okay, change it here.
And once you download the editing, you can still learn your original model
and finalize the output. So you get very fast
and interactive feedback when you are editing the photo. And you get a high quality output
where after you finalize your edits.
Here's another example.
I'll make this look older.
And we'll try to remove the gossips because sometimes you mix some more, it will add gossips.
So there's some correlation between these attributes.
But it's very interactive. But once we finalize the edits, we can
generate the final rendering, a very high quality, high resolution image.
Here's the last example.
Yeah, again, once you finalize the edits, you can click the button and learn the original model.
Yeah, so here is the one.
Yeah, so this is actually quite similar to how people is using regular, non-deplaning
content creation software either in rendering software like Blender or Maya.
You can choose to render an image with low quality preview.
It's very easy to do it in rendering algorithm. You just sample field arrays,
or maybe you have fewer bounces if you know the rendering algorithm.
It's also in a bunch of Adobe software, you can also generate a preview
of your videos and there are a bunch of ways to make rendering faster.
So that's just for your preview. And once you are done with your editing, you can export model,
export the export results to a much higher quality rendering, much higher quality
just output results. So we're trying to separate the preview and the final rendering.
We're trying to make sure the preview looks similar to final rendering.
Otherwise, the preview does not provide enough information for editing.
So far, what we have been talking about is how we can make it faster to run on CPU.
For example, maybe we instead of three seconds, we can make sure it runs
about three or four seconds. Maybe we can make sure it can run on a smartphone
maybe within one second. Next, I would like to talk about data, which also prevents many of the
users and the content creators to choose their own models. So the idea here is to choose these models.
It requires lots of selects like choosing images, right? We have seven thousand faces,
you need to be high quality faces, and people actually align the face, crop the face from
the original photo and align them. The same idea here, if you choose a big game model,
the model from DeepBand, the big game model requires an image net model,
and it requires millions of images from some of the categories.
Or you can choose a model like a bunch of people have seen the car model, the bedrooms model requires
one or two million images to choose a car model, just to learn a model about cars and bedrooms.
So it's not very easy. So if you have a new idea, oh, I would like to have a
have a model of something. It's not like you can just train the model, like it takes a lot of
time to collect the data right in the first step, before you even train the model.
Okay, so it takes months or even years to collect the data set. Some training might
require annotation if you train a big game model which condition all the class labels.
So for the here, how about I just train a model about myself, about my collaborator here is
actually professor Sohan, assistant professor at MIT. So often I just, I don't just try to
try to train a model on his portraits, right? But of course, I don't have, the thing I don't have
is I don't have 70,000 faces of professor Sohan. I don't, I probably don't have to allocate them,
but maybe I have 100 faces, maybe I have 50, maybe I have 200, but there are no way I have a million
faces. So first, I mean, maybe if you are like, celebrate your petition, maybe you have, maybe
I'm not sure if you use that case, you have a million faces. But for your friend, for your family
members, you probably have like 50, 100, so several hundred of them faces. And here,
but the ideal case is we want to train a model of my, myself, my friends, my collaborators,
but I don't have so many images. So I would like to, so you know, I hear what I would like to
train a model and try to produce some new samples. And maybe I can use this model to edit Sohan's
photo, right? Instead of using a generic face model, I would like to have a customized face model
for face editing. But in reality, if you just train a model of Stalgant 2, we get very distorted
images. If you would train a Stalgant 2 on the dataset, I just mentioned like hundreds of 50
faces. And so there are the huge gap between, if you train a model with 50 faces versus you
train a model with 70,000, a million faces. The gain is really requires a lot of data to get good
performance. And this holds for a bunch of cases, not only for a professor. Sohan's face, but also
for Obama's faces, if you train a model on 100 Obama images, it does not work very well. Okay.
If you, let's say, maybe not, maybe we have a very high standard for face, right? Maybe for other
objects, it's okay. That's not the case. If you train a model on cats, 160 cats, you get some
distorted cats. How about your dog friend? You train a model on dogs, you still get distorted dogs,
even you have 390 images. So that's highlights the issue is, if you want to use again,
for your own purpose, for your own dataset, maybe for some paintings, you don't have, if you're
training a model on paintings of a particular painter or artist, you cannot ask the artist to
produce millions of paintings in the first place, right? You would like to train a model
of 101,000 paintings of a particular style or a particular painter.
So we can, we can do this kind of experimental control setting. So we can look at CIFA-10,
which is a standard dataset for gains of competitiveness. We can measure it by FIT,
which is the lower and the better, which measures the distance between
the jerry hippies distribution versus the original training distribution. And if you
train a model on FID, sorry, you train a model on CIFA-10, you get a very low FID
for 100% training data, but you get a much higher FID if you reduce the data by five times.
And if you only train a model on 10% of CIFA images, you get a much higher FID.
That's why the gains have really rely on the number of images you are training set.
But why is that? Why, why, what happened in the CIFA-36 case,
if you only have 10% of the data, right? Something happens if you, if you, for any kind
of opportunity model, if you don't have enough data, your model might start overfitting. In this case,
we can look at the discriminator that's overfitting if you don't have enough data. So here are the
cases, the discriminator, we can, we have two plots. We have discriminators training accuracy.
Yes, how this is, how discriminator can classify real versus fake images for the training set
the model has been trained on. We can look at the discriminator's accuracy, our holdout test,
our variation training, our variation real, real images, which discriminator hasn't seen
in the training set. So you can see that for 100% of data, the model, the discriminator starts over
kind of classify the images more and more accurately. But that's not necessarily me,
it can classify the test images as accurate as training images. So that's happens a lot if you
just change the classifier, right? But the thing which makes this worse is if you only have 20%
of training images, this kind of overfitting is much more severe. If you only have 20% of training
data, the discriminator will classify the training images very accurately, very quickly,
but it will, it cannot work for the test images anymore, the test real images, it will
drop the percentage, right? Reminding this is a binary classification task, so the accuracy below
0.5 is pretty bad. And if you have 10% of data, it's even worse. It can classify this 10% of
data very quickly, real or fake, but it cannot classify any kind of test images very quickly.
And if you look at this model, this is when the gains start collapsing, the models start generating
lots of garbage images. So the issue here which we identify is that if you don't have enough images,
or if you only have 100 images, it's very easy for discriminator to just simply memorize
every single image of your training set. As a discriminator, it does not generalize well
to other real images. So if you treat a generator with overfitting discriminator,
of course, a generator cannot get all the signals about what makes Obama look like Obama,
what makes Professor Sohan look like Professor Sohan, right? If you're discriminating overfitting.
So one idea to come back, the overfitting in computer machine, machine is called data
augmentation. It's for single real image, we can create multiple versions of this image.
If I create 10 versions of images, I kind of increase my data by 10 times.
Of course, this information is redundant, but it's better than this one version, right?
So the idea is try to enlarge the data sets without collecting the new samples.
As there are a bunch of things you can do, you can, for example, this cat, you can rotate the cat,
you can flip the cat, you can maybe change the color a little bit, or you make, maybe you can
translate the cat, like make it shift left, shift right, shift up, shift down, so you can,
you can move the cat around. But in computer machine test, there's still a cat, right?
So if you train a model to classify a cat and a dog, if you move the cat around and rotate the cat
by 30 degrees, it's still a cat, right? It doesn't become a dog. So you also do the labels.
So the idea is you don't want to change the label by what we request your data.
Right? But how to, how can we apply this idea to GANs training?
How can we stay augmentation for GANs to combat the overfitting issue when we train a model on very
few images? So we have tried several ideas. And the first idea, which is very straightforward, is we
can, we can just apply the transformation or augmentation on the real images, right? Just like
we did for the training image net classifier. And we can, we can train a model on the augmented
images. That's, so that's very straightforward. But the, but the thing we're following is if you
train a model in this way, the GERA image will also has the effect of this transformation. If you,
if you, your transformation, if you change the color or you translate image around,
or if you crop some patches, your GAN will replicate these kind of artifacts, will mimic
these artifacts because your GAN does not know what is the original image look like. If you only
feed the translate, transform the images, augment the images, try to mimic the augmentation as well.
Why is that? Why, why is not an issue in the classifier? Because if you train a classifier for
cats versus dogs, your label of cat and dog is the output, right? You want to output a cat or dog,
but this label is, is, is, is the same before after augmentation. So you can still produce the cat.
But here the output of the generator is the image. If you change the output, your dataset,
your, your output of your generator will change. So you see the difference, the difference is
whether you augment the input or output. So here we had augmented output while in the classifier
case, people try to augment the input image, why keep the output the same? So if you cannot just
augment the output directly, it will mimic that augmentation. That's, we don't want to generate
these kind of images, right? And so second idea is how about we augment both real and fake images?
I hope we can cancel each other and we'll only do it for the discriminator training.
So the generator is, the training is the same. And we still train it versus the generator of
g of z. By discriminator training, we augment both the real images x and generate images g of z.
So z is a latent code, we simple from this Gaussian distribution. So, but, but the one thing
we found is since you are doing slightly different things for, for the discriminator training,
in which case you augment the x and g of z, but if you don't augment the
data for, for a generator, you will see a gap. That is your classifier discriminator works pretty
well for the transform, for, for, for, for the augmented images, t, t is augmentation,
but it does not work very well for the generating, for, for the images without augmentation,
for the g of z, which is original images. Also, which is original generate images
without augmentation. Okay. So there's a gap between the generator's objective
and discriminator's objective. And we, we found it does not work very well in practice because
of this gap. So our approach is called differential augmentation. The idea is we,
we can augment the both the, the fake images and the real images and both treating and, and,
sorry, in both generate training and discriminator training. And here I will call it differential
because if you, if you augment the data here and if you want to get gradients from the discriminator
to the generator, this transformation t needs to be differentiable. Otherwise, you will stop
the gradients from the discriminator to the generator. So we implement a bunch of
differential augmentations and apply it to here. So the single impermanence we apply
color transformation, our transient image alone, we apply something kind of cut out or kind of like
augmentation. There are three kinds of operations. And the idea is once we apply the augmentation,
we can, we can back up against the gradients from the discriminator all the way to the generator,
all the way to the generator. So that's how we can treat the generator.
And here are some results. So here's the original FID with respect to different amount of training
data. And here's our results. So we, we get slightly better for 100% training data, but we'll
get much, much better if you only have 10% or 20% of training data. So this allows us to maybe
instead of treating a model of 50,000 images, you only need 5,000 images. And here are more examples.
Yes, yes, if you only have 20% of the training data for image net, our model achieves much better
results compared to the baseline begin. We will use the same begin loss and same begin architecture,
but just add this augmentation to produce the FID by half.
And now we can try to generate some Obama or cats or dogs. And here are our results we can
produce much higher quality results when you only have 100 images.
And now we can try to generate professor's face. It's not perfect, but much better than,
than, than a baseline. And only requires 100 faces of your friends, your family member or yourself.
And compare with fun tuning methods. There are a bunch of methods which you treat a model on
a large scale data set and a fine tune it on a smaller data set.
And our method is, is, is comparable performance wise, even when we don't require these kind of
protruding images. Here we compare with transfer again, which the idea is you treat a model on
FFTQ faces and a fine tune it on one of our faces. And all methods are still slightly better than
since there are results. But of course, you can combine the best of the two words, like you can
get your model on FFTQ and fine tune it with our differentiable augmentation.
And, and you can get slightly more better results, you combine these two methods, they are complementary.
And here are one results we treat a model and we try to traverse latent, latent in the latent
space for different kind of, you can treat a model for a particular person, you can treat a model
for particular landmarks or cities or animals, just, just very download some images and, and, and
just hundreds of images you can do the job. Yeah, anyway, so, so there are not so much take home
messages, take home messages. If you only have your fortune again, do not forget
the documentation, that's the message. Okay, thank you for your attention.
Thank you so much, Junion. And it was such a great talk and a lot of interesting
directions and things to think about. I'm wondering if students have questions.
I would like to ask how some of the students can, you know, get these models and work with them and
can you please explain if what are the steps for them to get working with these models and the code?
I think you are me. Yeah, yes, yes, I try to
crit my dual monitor setup. So, yeah, yeah, I see that all the code are available on the
GitHub. I will send you the slides later. All the code are available on GitHub.
We have, we have step by step instruction on how to learn the model on
C file and how to learn the model on image data, how to learn the model on your own
data, like if you just have an image directly of hundreds of images, you can, we have a command
line and you can learn the model directly. It took like four hours to learn a model on like hundreds
of, 100 photos for a lot of your offices. It should be pretty straightforward. Yeah, yeah.
And then are there, are there tools that one could use for making some of these things more
interactive and use as a, just at the beginning you describe how it is interesting for designers
and, you know, art practitioners to use these models.
Yeah, I, I think, I think David Bao, my, my will, will talk about it. We have a bunch of tools on
visualizing and monitoring the internal, internal units of this model, if you would like to understand
it better. I'll have, I think David has a bunch of online tools he will talk about maybe tomorrow
or later. Yeah, I think in general, it's, it's also hard to use. It takes like four hours on 100
photos. So if you have 20, 80 time GPUs, yeah, so it's much faster than you know, big game model
for four months or several weeks. Yeah, I think we're also working on maybe faster training,
we are still working on that. So hopefully you can reduce the training time to several minutes
or maybe half an hour. So more people can, can, can use it, right. So oftentimes you may not have
the GPU resources, you may not have so many images. So we are working on that and try to,
yeah, but it's also good for us because we also have limited resources compared to big companies.
It's not like you're MIT and you have lots of resources, but compared to big companies,
we don't have so much resources in academia. And then also David is asking, how long does it take
to compress a model? Yeah, compress a model, it takes as long as much as time as training a model,
maybe, maybe, maybe 50 times more. It's slower because you want to,
you are training up all kinds of configuration at the same time, so it takes longer to train.
Maybe it takes 50% more time. And while we are working on
improvement of that, try to make the training faster. And it works for a bunch of models as well.
Yeah, the compressing model is slower because the idea is you can, you have a bunch of
models with different configurations. They all work pretty well. So you can use
configuration A for your CPU, you can use a different configuration for your,
for your mobile device. So you can have different models for different devices.
Once you train the model, you can, you have this kind of flexibility.
The idea is once you train the model, you only need to train once and you can deploy to 10 or
20 devices. That's, that's quite essential for critical, for practical purpose because
if you develop a product, you would like, you don't want to compress a model for
iPhone. And then next time you'll compress for older iPhone or Pixel to a Pixel 3, right?
So you would like to have this kind of versatile ability to train once and
deploy to multiple devices. Yeah. I think that this is very important because
you can put the time for the developer and then when the user wants to use it,
the hope is that they spend very much less time to get what they want, which is a very good idea.
Thank you so much, Junion. I appreciate that.
It was such a pleasure for us.
