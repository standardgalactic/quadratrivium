1
00:00:00,000 --> 00:00:08,000
Okay, hello everyone. Welcome to this session of deep learning for art, aesthetics and creativity.

2
00:00:08,000 --> 00:00:24,000
Today, we have our specialist speaker, Junion Ju, and he is an assistant professor at CMU, School of Computer Science.

3
00:00:24,000 --> 00:00:41,000
He is going to talk about efficient GANS, and Junion is such a great researcher and scientist who has been working on many, many interesting generative models, including he was in peaks to peaks,

4
00:00:41,000 --> 00:00:56,000
and cycle consistency scans, and Gauguin, and many other interesting and intriguing work, and I hope that we can see a gist of some of his work here today.

5
00:00:56,000 --> 00:01:02,000
So, please, Junion, go ahead.

6
00:01:02,000 --> 00:01:17,000
Yes, thanks for the nice introduction. I'm very happy to be here and talk about some of our recent work on how to make GANS more efficient.

7
00:01:17,000 --> 00:01:29,000
Maybe some background that you probably already knew about it. Yes, GANS has been used for various content creation and creativity tasks.

8
00:01:29,000 --> 00:01:41,000
For example, you can draw a sketch of a handbag or edge of a handbag and you can get an output image from the GANS.

9
00:01:41,000 --> 00:01:52,000
And one recent model we have been working on is this Gauguin model, led by Thaisong Park and other people.

10
00:01:52,000 --> 00:02:02,000
Here, our artist is creating a semantic label that basically says this is a mountain, this is also a log, this is the sea.

11
00:02:02,000 --> 00:02:11,000
And when the mountain, he can add some sand, and we can generate an image in real time.

12
00:02:11,000 --> 00:02:16,000
And he can add some more details like logs.

13
00:02:16,000 --> 00:02:24,000
Yeah, and you can create a pretty nice image very quickly.

14
00:02:24,000 --> 00:02:39,000
That's a very cool demonstration of the GANS and you can also, sorry about that, you can also apply a particular like a style image from a truly set and then apply it to stylize this image.

15
00:02:40,000 --> 00:02:48,000
So these all look very nice and kind of perfect, right, those cool demo in front of so many people.

16
00:02:48,000 --> 00:03:00,000
But if we close the door and the story here is, it's really very expensive, at least for this Gauguin project.

17
00:03:00,000 --> 00:03:18,000
And to develop Gauguin project, Thaisong has about hundreds of GPUs, in the summer in turn, and for six or nine months he has access to several hundreds of GPUs, very high end, the media GPUs, which you cannot sometimes you cannot even buy it on the market,

18
00:03:18,000 --> 00:03:24,000
but he has access to internal GPUs, we are not allowed to share the information.

19
00:03:24,000 --> 00:03:32,000
But he has access to lots of GPUs during training, and the training also requires lots of data, like tens of thousands of data.

20
00:03:33,000 --> 00:03:54,000
After almost a year's development, to learn this model, to do this demo, we actually bought a very expensive desktop, like 6,000, 7,000 laptop with actually a high end GPU, but that's still a laptop, not a desktop, so we want to carry this laptop around and do the demo, right.

21
00:03:54,000 --> 00:04:10,000
So in conclusion, from this particular project I realized games are very expensive, perhaps not for everyone now at this moment.

22
00:04:10,000 --> 00:04:15,000
I think it's expensive in three ways.

23
00:04:15,000 --> 00:04:35,000
So for learning a game model, if you train a game model to learn the game model on a device, you need a high end GPUs, like for real time performance, otherwise it will be very laggy, you cannot have any real time demo without a high end GPU.

24
00:04:35,000 --> 00:04:45,000
So in order to develop the algorithm, to develop a new algorithm, you require hundreds of GPUs to train the model.

25
00:04:45,000 --> 00:05:05,000
So I think it's very, it's huge, or maybe right now only the deep end of the media can afford this kind of computation. You will for university lab, MIT or CMU, maybe you don't have so many GPUs, it's very hard to compete with big companies.

26
00:05:05,000 --> 00:05:10,000
And third, to train a game model, it often requires lots of data.

27
00:05:10,000 --> 00:05:32,000
And I will go through it one by one. The connection wise is very, it's very slow to learn on a CPU and mobile device, right, maybe take 10 seconds or several seconds to learn on models like CPU and even slower to learn on mobile device like a tablet or like a phone.

28
00:05:33,000 --> 00:05:43,000
To train a single model for a single training session, it takes several days, maybe one or two weeks, maybe sometimes for a big game, maybe take amounts.

29
00:05:43,000 --> 00:05:54,000
And, but it's not like you just train one model and publish a paper as model training might take at least a dozen of training sessions, you have hundreds of training sessions.

30
00:05:55,000 --> 00:06:01,000
So each training session takes, each training session takes a month, you need lots of GPUs to parallelize your experiments.

31
00:06:01,000 --> 00:06:08,000
So it's not like you have an idea, you train a model and you go through all the public paper, that's not the case.

32
00:06:08,000 --> 00:06:17,000
You have an idea, you try something to work, you try something to modify the idea slightly, you have lots of iteration over the months.

33
00:06:17,000 --> 00:06:34,000
So each iteration takes one week or one month on each GPUs. So if you, if you put these numbers together, if you want to actually do the research very as quickly as possible, it requires lots of GPUs.

34
00:06:34,000 --> 00:06:46,000
Sometimes it's even like a top university like top lab in MIT or CMU Berkeley cannot afford it, not saying other labs.

35
00:06:46,000 --> 00:06:52,000
So what people do realize is to treat this model like require tens of thousands of millions of images.

36
00:06:52,000 --> 00:06:56,000
You treat a model of faces, it requires 70,000 faces.

37
00:06:56,000 --> 00:07:03,000
These faces are very high quality images, and you need to align the face before you treat the model.

38
00:07:03,000 --> 00:07:13,000
So there are lots of pre-processing steps to get an image, to treat an image that's model, you need labels, and you need to limit the data sets first.

39
00:07:13,000 --> 00:07:19,000
So there are three things that can be used for more users, right?

40
00:07:19,000 --> 00:07:27,000
If you are a content creator, if you are an artist, you may not have access to the computation, to the algorithm and data, right?

41
00:07:27,000 --> 00:07:32,000
So in this talk, I would like to focus on computation and the data.

42
00:07:32,000 --> 00:07:43,000
But while working on the algorithm, in this talk, we have some results on how to make data faster, how to make you can treat a game model on maybe only hundreds of images.

43
00:07:43,000 --> 00:07:58,000
By that, we can maybe help more users, content creators and artists to treat and test their own game models without having access to lots of data, without having access to very high-end GPUs.

44
00:07:58,000 --> 00:08:04,000
So I will talk about the first part, which is the model.

45
00:08:04,000 --> 00:08:09,000
GANS are very, very computational expensive to run.

46
00:08:09,000 --> 00:08:27,000
If you compare GANS, or conditional GANS, like Psychogalgan to the typical image classification model, this is the computation, right?

47
00:08:27,000 --> 00:08:33,000
Maybe some people like to use Macs, some people like to use Flops, but the story is the same.

48
00:08:33,000 --> 00:08:38,000
It's quite cheap to run a mobile internet for image classification purpose.

49
00:08:38,000 --> 00:08:47,000
It's very relatively cheap to run a restats network for classifying caster versus dogs versus other categories.

50
00:08:47,000 --> 00:08:56,000
But somehow it's very expensive to run a Psychogalgan model.

51
00:08:56,000 --> 00:08:59,000
This gap is almost 500 times.

52
00:08:59,000 --> 00:09:10,000
If you compare the latest generative model versus the latest classifiers.

53
00:09:10,000 --> 00:09:15,000
And that's a general trend.

54
00:09:15,000 --> 00:09:18,000
I think you may ask why, right?

55
00:09:18,000 --> 00:09:22,000
Why is so much more expensive?

56
00:09:22,000 --> 00:09:31,000
The reason for that is, like in Psychogalgan, you take an image and you produce the image with the same resolution.

57
00:09:31,000 --> 00:09:38,000
So the spatial dimension of the tensor remains the same more or less through the process.

58
00:09:38,000 --> 00:09:43,000
When you classify, you take an image and you produce a number.

59
00:09:43,000 --> 00:09:50,000
So the spatial resolution of this tensor becomes smaller and smaller.

60
00:09:50,000 --> 00:09:55,000
I think that's part of the reason, but also to generate the image, maybe you need more features.

61
00:09:55,000 --> 00:09:58,000
That's the second reason.

62
00:09:58,000 --> 00:10:07,000
Anyways, it's quite expensive for GANs that compare to the image classifiers.

63
00:10:07,000 --> 00:10:11,000
In this work, I will briefly go through this work.

64
00:10:11,000 --> 00:10:16,000
If we try to solve this problem, we propose a method called GAN compression.

65
00:10:16,000 --> 00:10:26,000
GAN compression is a general purpose framework based on the distillation channel protein and neural active search.

66
00:10:26,000 --> 00:10:37,000
So the idea here is, so given a teacher model, teacher model is the original model for the psychogalgan model we like to compress.

67
00:10:37,000 --> 00:10:42,000
It has lots of channels.

68
00:10:42,000 --> 00:10:50,000
So you take a horse image as input in psychogalgan case, you output a zebra.

69
00:10:50,000 --> 00:11:00,000
And we also have a student, so the goal of this project is to try to find a student model, which with fewer filters,

70
00:11:00,000 --> 00:11:05,000
and it can produce the same kind of zebra as the teacher model.

71
00:11:05,000 --> 00:11:18,000
So we have a loss function to try to make sure the output, the student output zebra looks very similar to the teacher's output zebra.

72
00:11:18,000 --> 00:11:20,000
That's what loss we have.

73
00:11:20,000 --> 00:11:29,000
We also try to make sure that the student's intermediate feature representation is very similar to teacher's representation.

74
00:11:29,000 --> 00:11:32,000
And lastly, we would like to make sure we have a GAN loss here.

75
00:11:32,000 --> 00:11:40,000
We would like to make sure the student's output zebra looks like a zebra according to adversary loss.

76
00:11:40,000 --> 00:11:42,000
So we have three losses.

77
00:11:42,000 --> 00:11:44,000
We have the pixel loss.

78
00:11:44,000 --> 00:11:51,000
We try to make sure the student zebra looks like a teacher zebra.

79
00:11:51,000 --> 00:12:01,000
And we have a feature loss which makes sure the student's feature looks like student's feature looks like a teacher's feature.

80
00:12:01,000 --> 00:12:07,000
We have a third loss, which is GAN loss, just like a typical GAN loss you apply to any GAN.

81
00:12:07,000 --> 00:12:10,000
Yes, you want to make sure the zebra looks like a zebra.

82
00:12:10,000 --> 00:12:12,000
Okay.

83
00:12:12,000 --> 00:12:14,000
So this is our distillation part.

84
00:12:14,000 --> 00:12:30,000
In our channel protein part we would like to, so our goal is to search for each layer, the optimal number of channels which can, we want to find a smaller number of channels,

85
00:12:30,000 --> 00:12:35,000
but can still satisfy all these loss functions constraints.

86
00:12:35,000 --> 00:12:47,000
So we have a bunch of channels here, for example, we have 60 channels, you can choose 32 channels, and you can choose maybe 48, 64 if you like.

87
00:12:47,000 --> 00:12:50,000
And I can choose 24 if you like.

88
00:12:50,000 --> 00:12:52,000
So we have channel protein.

89
00:12:52,000 --> 00:13:01,000
So the idea is if you have fewer channels for each layer, your model will be smaller and faster to run.

90
00:13:02,000 --> 00:13:10,000
Let's say if you reduce the channel by half for every single layer, of course the model will be faster to run.

91
00:13:10,000 --> 00:13:22,000
But here, the search means we try to, we're not trying to reduce the channel uniformly, we try to maybe, for some layer we try to reduce it to 16 channels.

92
00:13:22,000 --> 00:13:26,000
For some layer maybe 24 channels are necessary, or 32 channels are necessary.

93
00:13:26,000 --> 00:13:37,000
So we try to, it's kind of like a search problem, the search space is just like channels per layer, you have eight layers, you have eight numbers you would like to search for.

94
00:13:37,000 --> 00:13:41,000
If you have 11 layers, you have 11 numbers you would like to search for.

95
00:13:41,000 --> 00:13:51,000
So the idea is we try to find this combination so that we can still reproduce a zebra which looks like teacher zebra.

96
00:13:51,000 --> 00:13:53,000
Okay.

97
00:13:53,000 --> 00:13:55,000
So how to do this search problem?

98
00:13:55,000 --> 00:14:00,000
The search seems very hard because for each configuration you need to train the model.

99
00:14:00,000 --> 00:14:03,000
And this is a combinatorial number, right?

100
00:14:03,000 --> 00:14:10,000
There are lots of combinations, you can, if there are four combinations, four choices per layer, that's lots of combinations.

101
00:14:10,000 --> 00:14:20,000
You don't like a naive way is you can train a model for each configuration and then compare which model works best.

102
00:14:20,000 --> 00:14:23,000
But that will take lots and lots of training time.

103
00:14:23,000 --> 00:14:31,000
So the idea here is we would like to have all the network to share the ways.

104
00:14:31,000 --> 00:14:41,000
The idea is if you have 32 channels, share the same first 60 channels with the 60 channel model.

105
00:14:41,000 --> 00:14:44,000
So by doing that, we don't have to train them all.

106
00:14:44,000 --> 00:14:50,000
Each individual configuration, every single time, we can share the ways across different configurations.

107
00:14:50,000 --> 00:14:52,000
So we still have only one model training.

108
00:14:52,000 --> 00:15:01,000
But each time we sample a configuration, it's like this configuration, that configuration, and we try to train this configuration with the loss.

109
00:15:01,000 --> 00:15:04,000
But we only have one training session.

110
00:15:04,000 --> 00:15:13,000
So by each training iteration, we sample a different configuration and try to train the model with the loss function I just described.

111
00:15:13,000 --> 00:15:30,000
Okay, so once we are done with the training, we can, then we can, then we can, for every single configuration, we can try to evaluate these models based on a matrix such as FID,

112
00:15:30,000 --> 00:15:38,000
and then there's other metrics you have and choose the best one, and then we can fine tune that model.

113
00:15:38,000 --> 00:15:48,000
Yeah, so there are a bunch of loss function, we kind of, so there are several ideas, one idea is there are several loss functions, we try to mimic the teacher model.

114
00:15:48,000 --> 00:15:59,000
But it's actually such ideas, we try to share the ways across different configurations, so that we can avoid treating each configuration multiple times.

115
00:16:00,000 --> 00:16:02,000
Every single time.

116
00:16:02,000 --> 00:16:05,000
Okay, so that's the idea of the method.

117
00:16:05,000 --> 00:16:09,000
I would like to share some results with you.

118
00:16:09,000 --> 00:16:17,000
So here we are so for the house to zero, we can reduce the model size for 57.

119
00:16:17,000 --> 00:16:26,000
The model size, the competition cost for 57 max to 2.7.

120
00:16:26,000 --> 00:16:31,000
7 to edge to shows, 7 to Gauguin.

121
00:16:31,000 --> 00:16:41,000
So we can achieve nine times to 24, 20 times compression ratio in terms of the model computational cost.

122
00:16:41,000 --> 00:16:53,000
Here is a demo is you will learn the original cycle again on this mobile device, Jason Javier GPU, and we can measure the FPS.

123
00:16:53,000 --> 00:17:00,000
Here is the, here's how it looks like you want to transform a house to a zebra.

124
00:17:00,000 --> 00:17:12,000
And here is we learn the same more compressed model on on this hardware. So I should see four times kind of speed up and it gets real time performance on this.

125
00:17:12,000 --> 00:17:22,000
This is a kind of like like on device chip you will use for like robots or cars.

126
00:17:23,000 --> 00:17:26,000
But here model is out here the input image.

127
00:17:26,000 --> 00:17:31,000
Got a cycle get output.

128
00:17:31,000 --> 00:17:34,000
Here is a baseline.

129
00:17:34,000 --> 00:17:40,000
You wish you just reduce the will cause 0.25 cycle guy.

130
00:17:40,000 --> 00:17:46,000
In this setting you basically just reduce them all channels.

131
00:17:47,000 --> 00:17:50,000
Per layer.

132
00:17:50,000 --> 00:18:01,000
And you lose them a channel to like by by force, by four times, but we do it for every single layer so there are no, no, no, no, no search happenings.

133
00:18:01,000 --> 00:18:17,000
You're very nice baseline. And you can see if you just reduce the demo channels uniformly every for every single layer, you will lose lots of details like zebras stripes which is something you would like to have.

134
00:18:17,000 --> 00:18:18,000
Okay.

135
00:18:18,000 --> 00:18:21,000
And here is the compressed model.

136
00:18:21,000 --> 00:18:25,000
20 times compressed model with the albaster.

137
00:18:25,000 --> 00:18:32,000
And you can preserve the output of the original teacher model.

138
00:18:32,000 --> 00:18:40,000
But why be 20 times faster.

139
00:18:40,000 --> 00:18:50,000
So recently we apply this idea to not only to cycle game but also to style get to this is ongoing submission.

140
00:18:50,000 --> 00:18:56,000
So, so the idea here is, we would like to have a teacher model.

141
00:18:56,000 --> 00:19:02,000
I would like to have, we will obtain a student model just like what we did before.

142
00:19:03,000 --> 00:19:15,000
And during the, the idea is doing the image projection will try to try to the ideas will project this image into the latent space of the gains.

143
00:19:15,000 --> 00:19:23,000
And then we add some different like directions like smiling directions like glasses directions, maybe different hair color.

144
00:19:23,000 --> 00:19:39,000
And here's we can do the interacting editing, we can use this low cost model, 10 times fast model, which only takes three seconds each time you adjust the slider.

145
00:19:39,000 --> 00:19:41,000
I will show a demo in a minute.

146
00:19:41,000 --> 00:19:50,000
But, but when was we the user, for example, you use someone to change the hair from black to to to to white.

147
00:19:50,000 --> 00:19:54,000
You can do that right and you can change this one to this one.

148
00:19:54,000 --> 00:20:03,000
But once you, once you, the user is done with the entity, you can use the original model to get the final output.

149
00:20:03,000 --> 00:20:17,000
The idea is that you want to like to make sure the smaller models output is consistent with the big models output through this editing process.

150
00:20:17,000 --> 00:20:26,000
You will get, you learn a smaller model and you get something what you want, but, but once you press the button off, you're writing a big model.

151
00:20:26,000 --> 00:20:29,000
If you get something completely different.

152
00:20:29,000 --> 00:20:33,000
So this preview is not informative anymore.

153
00:20:33,000 --> 00:20:39,000
I will show you a demo as an animation in this video.

154
00:20:39,000 --> 00:20:50,000
So here is an import image right to project it and ought to do to we like to look, look and see it again.

155
00:20:50,000 --> 00:20:56,000
And we try to find the best lately called which going to produce this input image.

156
00:20:56,000 --> 00:21:06,000
And so we have what we have done that we can like move this code around maybe we find this smiling call smiling directions.

157
00:21:06,000 --> 00:21:14,240
all I might have already mentioned is how you can find the directions. You can find that,

158
00:21:14,240 --> 00:21:18,640
like you move here, maybe you make your face smiling and you move to a different direction,

159
00:21:18,640 --> 00:21:26,160
make your hair a different color. And then the idea is we can generate an image

160
00:21:27,760 --> 00:21:33,920
after you change the directions. And here is the original image.

161
00:21:36,320 --> 00:21:42,880
Here we used GANs to reconstruct this image, to represent this image. This is the image

162
00:21:42,880 --> 00:21:48,400
generated by GAN. This is the image of original image. And here are a few sliders.

163
00:21:51,840 --> 00:21:56,960
And we would like to modify this image in various attributes.

164
00:21:57,520 --> 00:22:08,480
And we'll run it on these CPUs, like just Intel CPUs. But if you click

165
00:22:08,480 --> 00:22:14,960
smiling, here we click smiling, it took about three seconds to produce image with smiling face.

166
00:22:17,040 --> 00:22:21,840
So you cannot drag the slider anymore, you can only click. And then you make younger or older.

167
00:22:22,080 --> 00:22:32,880
It's just very laggy, right? And the idea is you would like to, if you click something or drag

168
00:22:32,880 --> 00:22:41,360
the slider, it should get the results immediately. And here is our idea.

169
00:22:41,600 --> 00:22:51,120
And you would like to choose our monocleidic cost GANs. And you can click something.

170
00:22:51,680 --> 00:22:54,080
You only took 3.3, 3.4 seconds.

171
00:22:58,320 --> 00:23:02,400
That's very, so you get very fast interactive feedback.

172
00:23:02,960 --> 00:23:07,440
Okay, change it here.

173
00:23:12,000 --> 00:23:15,920
And once you download the editing, you can still learn your original model

174
00:23:15,920 --> 00:23:20,320
and finalize the output. So you get very fast

175
00:23:21,280 --> 00:23:28,080
and interactive feedback when you are editing the photo. And you get a high quality output

176
00:23:28,640 --> 00:23:30,240
where after you finalize your edits.

177
00:23:34,240 --> 00:23:35,280
Here's another example.

178
00:23:42,880 --> 00:23:44,960
I'll make this look older.

179
00:23:51,840 --> 00:23:56,320
And we'll try to remove the gossips because sometimes you mix some more, it will add gossips.

180
00:23:56,800 --> 00:23:59,040
So there's some correlation between these attributes.

181
00:24:00,320 --> 00:24:03,680
But it's very interactive. But once we finalize the edits, we can

182
00:24:03,680 --> 00:24:08,800
generate the final rendering, a very high quality, high resolution image.

183
00:24:14,800 --> 00:24:16,560
Here's the last example.

184
00:24:26,800 --> 00:24:36,960
Yeah, again, once you finalize the edits, you can click the button and learn the original model.

185
00:24:38,000 --> 00:24:39,280
Yeah, so here is the one.

186
00:24:42,720 --> 00:24:49,760
Yeah, so this is actually quite similar to how people is using regular, non-deplaning

187
00:24:50,720 --> 00:24:57,360
content creation software either in rendering software like Blender or Maya.

188
00:24:58,000 --> 00:25:04,480
You can choose to render an image with low quality preview.

189
00:25:05,600 --> 00:25:10,000
It's very easy to do it in rendering algorithm. You just sample field arrays,

190
00:25:10,000 --> 00:25:12,880
or maybe you have fewer bounces if you know the rendering algorithm.

191
00:25:14,320 --> 00:25:19,600
It's also in a bunch of Adobe software, you can also generate a preview

192
00:25:19,600 --> 00:25:24,640
of your videos and there are a bunch of ways to make rendering faster.

193
00:25:26,640 --> 00:25:31,840
So that's just for your preview. And once you are done with your editing, you can export model,

194
00:25:31,840 --> 00:25:37,920
export the export results to a much higher quality rendering, much higher quality

195
00:25:40,240 --> 00:25:45,360
just output results. So we're trying to separate the preview and the final rendering.

196
00:25:45,440 --> 00:25:48,160
We're trying to make sure the preview looks similar to final rendering.

197
00:25:49,040 --> 00:25:53,760
Otherwise, the preview does not provide enough information for editing.

198
00:25:57,280 --> 00:26:04,320
So far, what we have been talking about is how we can make it faster to run on CPU.

199
00:26:04,320 --> 00:26:08,320
For example, maybe we instead of three seconds, we can make sure it runs

200
00:26:08,320 --> 00:26:16,800
about three or four seconds. Maybe we can make sure it can run on a smartphone

201
00:26:18,080 --> 00:26:24,080
maybe within one second. Next, I would like to talk about data, which also prevents many of the

202
00:26:24,080 --> 00:26:32,080
users and the content creators to choose their own models. So the idea here is to choose these models.

203
00:26:32,400 --> 00:26:41,920
It requires lots of selects like choosing images, right? We have seven thousand faces,

204
00:26:41,920 --> 00:26:48,160
you need to be high quality faces, and people actually align the face, crop the face from

205
00:26:48,160 --> 00:26:54,560
the original photo and align them. The same idea here, if you choose a big game model,

206
00:26:55,520 --> 00:27:00,480
the model from DeepBand, the big game model requires an image net model,

207
00:27:01,200 --> 00:27:04,880
and it requires millions of images from some of the categories.

208
00:27:05,680 --> 00:27:11,680
Or you can choose a model like a bunch of people have seen the car model, the bedrooms model requires

209
00:27:11,680 --> 00:27:17,600
one or two million images to choose a car model, just to learn a model about cars and bedrooms.

210
00:27:18,240 --> 00:27:23,120
So it's not very easy. So if you have a new idea, oh, I would like to have a

211
00:27:24,160 --> 00:27:29,760
have a model of something. It's not like you can just train the model, like it takes a lot of

212
00:27:29,760 --> 00:27:34,960
time to collect the data right in the first step, before you even train the model.

213
00:27:35,840 --> 00:27:42,400
Okay, so it takes months or even years to collect the data set. Some training might

214
00:27:42,400 --> 00:27:48,080
require annotation if you train a big game model which condition all the class labels.

215
00:27:49,600 --> 00:27:55,680
So for the here, how about I just train a model about myself, about my collaborator here is

216
00:27:56,320 --> 00:28:02,480
actually professor Sohan, assistant professor at MIT. So often I just, I don't just try to

217
00:28:02,480 --> 00:28:08,720
try to train a model on his portraits, right? But of course, I don't have, the thing I don't have

218
00:28:08,720 --> 00:28:17,840
is I don't have 70,000 faces of professor Sohan. I don't, I probably don't have to allocate them,

219
00:28:18,560 --> 00:28:24,480
but maybe I have 100 faces, maybe I have 50, maybe I have 200, but there are no way I have a million

220
00:28:24,480 --> 00:28:34,320
faces. So first, I mean, maybe if you are like, celebrate your petition, maybe you have, maybe

221
00:28:34,880 --> 00:28:40,560
I'm not sure if you use that case, you have a million faces. But for your friend, for your family

222
00:28:40,560 --> 00:28:47,520
members, you probably have like 50, 100, so several hundred of them faces. And here,

223
00:28:48,160 --> 00:28:56,560
but the ideal case is we want to train a model of my, myself, my friends, my collaborators,

224
00:28:56,560 --> 00:29:02,880
but I don't have so many images. So I would like to, so you know, I hear what I would like to

225
00:29:02,880 --> 00:29:08,880
train a model and try to produce some new samples. And maybe I can use this model to edit Sohan's

226
00:29:08,880 --> 00:29:15,760
photo, right? Instead of using a generic face model, I would like to have a customized face model

227
00:29:16,480 --> 00:29:24,880
for face editing. But in reality, if you just train a model of Stalgant 2, we get very distorted

228
00:29:24,880 --> 00:29:30,560
images. If you would train a Stalgant 2 on the dataset, I just mentioned like hundreds of 50

229
00:29:30,960 --> 00:29:40,640
faces. And so there are the huge gap between, if you train a model with 50 faces versus you

230
00:29:40,640 --> 00:29:47,520
train a model with 70,000, a million faces. The gain is really requires a lot of data to get good

231
00:29:47,520 --> 00:29:56,320
performance. And this holds for a bunch of cases, not only for a professor. Sohan's face, but also

232
00:29:56,400 --> 00:30:03,360
for Obama's faces, if you train a model on 100 Obama images, it does not work very well. Okay.

233
00:30:03,920 --> 00:30:09,280
If you, let's say, maybe not, maybe we have a very high standard for face, right? Maybe for other

234
00:30:09,280 --> 00:30:16,880
objects, it's okay. That's not the case. If you train a model on cats, 160 cats, you get some

235
00:30:16,880 --> 00:30:22,880
distorted cats. How about your dog friend? You train a model on dogs, you still get distorted dogs,

236
00:30:22,880 --> 00:30:31,440
even you have 390 images. So that's highlights the issue is, if you want to use again,

237
00:30:32,480 --> 00:30:38,640
for your own purpose, for your own dataset, maybe for some paintings, you don't have, if you're

238
00:30:38,640 --> 00:30:45,120
training a model on paintings of a particular painter or artist, you cannot ask the artist to

239
00:30:45,120 --> 00:30:49,760
produce millions of paintings in the first place, right? You would like to train a model

240
00:30:50,480 --> 00:30:54,880
of 101,000 paintings of a particular style or a particular painter.

241
00:30:57,680 --> 00:31:04,480
So we can, we can do this kind of experimental control setting. So we can look at CIFA-10,

242
00:31:04,480 --> 00:31:12,640
which is a standard dataset for gains of competitiveness. We can measure it by FIT,

243
00:31:14,240 --> 00:31:17,120
which is the lower and the better, which measures the distance between

244
00:31:17,920 --> 00:31:23,120
the jerry hippies distribution versus the original training distribution. And if you

245
00:31:23,120 --> 00:31:28,240
train a model on FID, sorry, you train a model on CIFA-10, you get a very low FID

246
00:31:29,120 --> 00:31:35,520
for 100% training data, but you get a much higher FID if you reduce the data by five times.

247
00:31:36,560 --> 00:31:44,160
And if you only train a model on 10% of CIFA images, you get a much higher FID.

248
00:31:47,440 --> 00:31:52,240
That's why the gains have really rely on the number of images you are training set.

249
00:31:55,040 --> 00:32:01,360
But why is that? Why, why, what happened in the CIFA-36 case,

250
00:32:02,720 --> 00:32:08,160
if you only have 10% of the data, right? Something happens if you, if you, for any kind

251
00:32:08,160 --> 00:32:14,560
of opportunity model, if you don't have enough data, your model might start overfitting. In this case,

252
00:32:14,560 --> 00:32:23,040
we can look at the discriminator that's overfitting if you don't have enough data. So here are the

253
00:32:23,040 --> 00:32:30,400
cases, the discriminator, we can, we have two plots. We have discriminators training accuracy.

254
00:32:31,440 --> 00:32:41,360
Yes, how this is, how discriminator can classify real versus fake images for the training set

255
00:32:41,360 --> 00:32:48,880
the model has been trained on. We can look at the discriminator's accuracy, our holdout test,

256
00:32:48,880 --> 00:32:54,320
our variation training, our variation real, real images, which discriminator hasn't seen

257
00:32:55,120 --> 00:33:03,440
in the training set. So you can see that for 100% of data, the model, the discriminator starts over

258
00:33:03,440 --> 00:33:08,320
kind of classify the images more and more accurately. But that's not necessarily me,

259
00:33:09,040 --> 00:33:18,400
it can classify the test images as accurate as training images. So that's happens a lot if you

260
00:33:18,960 --> 00:33:25,360
just change the classifier, right? But the thing which makes this worse is if you only have 20%

261
00:33:25,360 --> 00:33:34,080
of training images, this kind of overfitting is much more severe. If you only have 20% of training

262
00:33:34,080 --> 00:33:44,400
data, the discriminator will classify the training images very accurately, very quickly,

263
00:33:45,760 --> 00:33:52,800
but it will, it cannot work for the test images anymore, the test real images, it will

264
00:33:53,600 --> 00:34:00,080
drop the percentage, right? Reminding this is a binary classification task, so the accuracy below

265
00:34:00,080 --> 00:34:10,960
0.5 is pretty bad. And if you have 10% of data, it's even worse. It can classify this 10% of

266
00:34:10,960 --> 00:34:17,760
data very quickly, real or fake, but it cannot classify any kind of test images very quickly.

267
00:34:17,760 --> 00:34:23,680
And if you look at this model, this is when the gains start collapsing, the models start generating

268
00:34:23,680 --> 00:34:32,720
lots of garbage images. So the issue here which we identify is that if you don't have enough images,

269
00:34:32,720 --> 00:34:39,920
or if you only have 100 images, it's very easy for discriminator to just simply memorize

270
00:34:40,720 --> 00:34:46,960
every single image of your training set. As a discriminator, it does not generalize well

271
00:34:47,680 --> 00:34:52,960
to other real images. So if you treat a generator with overfitting discriminator,

272
00:34:53,760 --> 00:34:58,960
of course, a generator cannot get all the signals about what makes Obama look like Obama,

273
00:34:59,680 --> 00:35:04,720
what makes Professor Sohan look like Professor Sohan, right? If you're discriminating overfitting.

274
00:35:06,160 --> 00:35:15,280
So one idea to come back, the overfitting in computer machine, machine is called data

275
00:35:15,280 --> 00:35:24,160
augmentation. It's for single real image, we can create multiple versions of this image.

276
00:35:26,560 --> 00:35:30,880
If I create 10 versions of images, I kind of increase my data by 10 times.

277
00:35:31,920 --> 00:35:37,280
Of course, this information is redundant, but it's better than this one version, right?

278
00:35:37,280 --> 00:35:44,160
So the idea is try to enlarge the data sets without collecting the new samples.

279
00:35:45,840 --> 00:35:50,160
As there are a bunch of things you can do, you can, for example, this cat, you can rotate the cat,

280
00:35:50,960 --> 00:35:58,560
you can flip the cat, you can maybe change the color a little bit, or you make, maybe you can

281
00:35:58,560 --> 00:36:05,600
translate the cat, like make it shift left, shift right, shift up, shift down, so you can,

282
00:36:05,600 --> 00:36:11,760
you can move the cat around. But in computer machine test, there's still a cat, right?

283
00:36:12,480 --> 00:36:18,240
So if you train a model to classify a cat and a dog, if you move the cat around and rotate the cat

284
00:36:18,240 --> 00:36:23,440
by 30 degrees, it's still a cat, right? It doesn't become a dog. So you also do the labels.

285
00:36:24,160 --> 00:36:28,800
So the idea is you don't want to change the label by what we request your data.

286
00:36:31,520 --> 00:36:37,280
Right? But how to, how can we apply this idea to GANs training?

287
00:36:37,520 --> 00:36:44,160
How can we stay augmentation for GANs to combat the overfitting issue when we train a model on very

288
00:36:44,160 --> 00:36:50,880
few images? So we have tried several ideas. And the first idea, which is very straightforward, is we

289
00:36:50,880 --> 00:37:04,320
can, we can just apply the transformation or augmentation on the real images, right? Just like

290
00:37:05,280 --> 00:37:13,280
we did for the training image net classifier. And we can, we can train a model on the augmented

291
00:37:13,280 --> 00:37:21,520
images. That's, so that's very straightforward. But the, but the thing we're following is if you

292
00:37:21,520 --> 00:37:31,520
train a model in this way, the GERA image will also has the effect of this transformation. If you,

293
00:37:31,520 --> 00:37:36,880
if you, your transformation, if you change the color or you translate image around,

294
00:37:38,000 --> 00:37:47,440
or if you crop some patches, your GAN will replicate these kind of artifacts, will mimic

295
00:37:47,440 --> 00:37:54,160
these artifacts because your GAN does not know what is the original image look like. If you only

296
00:37:54,160 --> 00:38:00,880
feed the translate, transform the images, augment the images, try to mimic the augmentation as well.

297
00:38:04,400 --> 00:38:10,880
Why is that? Why, why is not an issue in the classifier? Because if you train a classifier for

298
00:38:10,880 --> 00:38:18,960
cats versus dogs, your label of cat and dog is the output, right? You want to output a cat or dog,

299
00:38:18,960 --> 00:38:25,840
but this label is, is, is, is the same before after augmentation. So you can still produce the cat.

300
00:38:26,560 --> 00:38:31,920
But here the output of the generator is the image. If you change the output, your dataset,

301
00:38:32,480 --> 00:38:38,080
your, your output of your generator will change. So you see the difference, the difference is

302
00:38:38,080 --> 00:38:43,360
whether you augment the input or output. So here we had augmented output while in the classifier

303
00:38:43,840 --> 00:38:49,840
case, people try to augment the input image, why keep the output the same? So if you cannot just

304
00:38:50,480 --> 00:38:55,840
augment the output directly, it will mimic that augmentation. That's, we don't want to generate

305
00:38:55,840 --> 00:39:04,480
these kind of images, right? And so second idea is how about we augment both real and fake images?

306
00:39:05,760 --> 00:39:11,600
I hope we can cancel each other and we'll only do it for the discriminator training.

307
00:39:12,560 --> 00:39:18,640
So the generator is, the training is the same. And we still train it versus the generator of

308
00:39:18,640 --> 00:39:25,280
g of z. By discriminator training, we augment both the real images x and generate images g of z.

309
00:39:25,280 --> 00:39:33,120
So z is a latent code, we simple from this Gaussian distribution. So, but, but the one thing

310
00:39:33,120 --> 00:39:37,120
we found is since you are doing slightly different things for, for the discriminator training,

311
00:39:38,080 --> 00:39:42,320
in which case you augment the x and g of z, but if you don't augment the

312
00:39:43,520 --> 00:39:50,640
data for, for a generator, you will see a gap. That is your classifier discriminator works pretty

313
00:39:50,640 --> 00:39:59,360
well for the transform, for, for, for, for the augmented images, t, t is augmentation,

314
00:40:00,320 --> 00:40:06,080
but it does not work very well for the generating, for, for the images without augmentation,

315
00:40:06,080 --> 00:40:11,520
for the g of z, which is original images. Also, which is original generate images

316
00:40:12,160 --> 00:40:20,800
without augmentation. Okay. So there's a gap between the generator's objective

317
00:40:21,680 --> 00:40:27,280
and discriminator's objective. And we, we found it does not work very well in practice because

318
00:40:27,280 --> 00:40:33,200
of this gap. So our approach is called differential augmentation. The idea is we,

319
00:40:34,160 --> 00:40:41,360
we can augment the both the, the fake images and the real images and both treating and, and,

320
00:40:41,360 --> 00:40:47,680
sorry, in both generate training and discriminator training. And here I will call it differential

321
00:40:47,680 --> 00:40:55,440
because if you, if you augment the data here and if you want to get gradients from the discriminator

322
00:40:56,160 --> 00:41:02,960
to the generator, this transformation t needs to be differentiable. Otherwise, you will stop

323
00:41:02,960 --> 00:41:07,120
the gradients from the discriminator to the generator. So we implement a bunch of

324
00:41:07,760 --> 00:41:13,920
differential augmentations and apply it to here. So the single impermanence we apply

325
00:41:15,600 --> 00:41:21,520
color transformation, our transient image alone, we apply something kind of cut out or kind of like

326
00:41:21,520 --> 00:41:31,280
augmentation. There are three kinds of operations. And the idea is once we apply the augmentation,

327
00:41:31,280 --> 00:41:38,000
we can, we can back up against the gradients from the discriminator all the way to the generator,

328
00:41:38,000 --> 00:41:41,600
all the way to the generator. So that's how we can treat the generator.

329
00:41:43,440 --> 00:41:50,000
And here are some results. So here's the original FID with respect to different amount of training

330
00:41:50,000 --> 00:41:57,920
data. And here's our results. So we, we get slightly better for 100% training data, but we'll

331
00:41:57,920 --> 00:42:07,360
get much, much better if you only have 10% or 20% of training data. So this allows us to maybe

332
00:42:07,360 --> 00:42:14,800
instead of treating a model of 50,000 images, you only need 5,000 images. And here are more examples.

333
00:42:16,400 --> 00:42:23,120
Yes, yes, if you only have 20% of the training data for image net, our model achieves much better

334
00:42:23,120 --> 00:42:30,560
results compared to the baseline begin. We will use the same begin loss and same begin architecture,

335
00:42:30,560 --> 00:42:33,920
but just add this augmentation to produce the FID by half.

336
00:42:36,480 --> 00:42:41,760
And now we can try to generate some Obama or cats or dogs. And here are our results we can

337
00:42:42,480 --> 00:42:47,840
produce much higher quality results when you only have 100 images.

338
00:42:48,080 --> 00:42:56,720
And now we can try to generate professor's face. It's not perfect, but much better than,

339
00:42:56,720 --> 00:43:04,240
than, than a baseline. And only requires 100 faces of your friends, your family member or yourself.

340
00:43:06,720 --> 00:43:10,480
And compare with fun tuning methods. There are a bunch of methods which you treat a model on

341
00:43:11,360 --> 00:43:14,240
a large scale data set and a fine tune it on a smaller data set.

342
00:43:15,440 --> 00:43:22,320
And our method is, is, is comparable performance wise, even when we don't require these kind of

343
00:43:22,320 --> 00:43:32,480
protruding images. Here we compare with transfer again, which the idea is you treat a model on

344
00:43:32,480 --> 00:43:41,920
FFTQ faces and a fine tune it on one of our faces. And all methods are still slightly better than

345
00:43:41,920 --> 00:43:46,480
since there are results. But of course, you can combine the best of the two words, like you can

346
00:43:47,040 --> 00:43:53,200
get your model on FFTQ and fine tune it with our differentiable augmentation.

347
00:43:54,080 --> 00:44:00,240
And, and you can get slightly more better results, you combine these two methods, they are complementary.

348
00:44:00,960 --> 00:44:09,520
And here are one results we treat a model and we try to traverse latent, latent in the latent

349
00:44:09,520 --> 00:44:14,000
space for different kind of, you can treat a model for a particular person, you can treat a model

350
00:44:14,000 --> 00:44:21,200
for particular landmarks or cities or animals, just, just very download some images and, and, and

351
00:44:21,200 --> 00:44:27,360
just hundreds of images you can do the job. Yeah, anyway, so, so there are not so much take home

352
00:44:27,360 --> 00:44:33,200
messages, take home messages. If you only have your fortune again, do not forget

353
00:44:34,000 --> 00:44:36,960
the documentation, that's the message. Okay, thank you for your attention.

354
00:44:39,600 --> 00:44:46,960
Thank you so much, Junion. And it was such a great talk and a lot of interesting

355
00:44:47,600 --> 00:44:55,440
directions and things to think about. I'm wondering if students have questions.

356
00:45:05,360 --> 00:45:14,000
I would like to ask how some of the students can, you know, get these models and work with them and

357
00:45:14,080 --> 00:45:24,240
can you please explain if what are the steps for them to get working with these models and the code?

358
00:45:29,920 --> 00:45:32,960
I think you are me. Yeah, yes, yes, I try to

359
00:45:33,680 --> 00:45:39,120
crit my dual monitor setup. So, yeah, yeah, I see that all the code are available on the

360
00:45:39,120 --> 00:45:45,040
GitHub. I will send you the slides later. All the code are available on GitHub.

361
00:45:45,920 --> 00:45:50,240
We have, we have step by step instruction on how to learn the model on

362
00:45:51,360 --> 00:45:54,800
C file and how to learn the model on image data, how to learn the model on your own

363
00:45:55,840 --> 00:46:03,120
data, like if you just have an image directly of hundreds of images, you can, we have a command

364
00:46:03,360 --> 00:46:10,000
line and you can learn the model directly. It took like four hours to learn a model on like hundreds

365
00:46:10,000 --> 00:46:18,400
of, 100 photos for a lot of your offices. It should be pretty straightforward. Yeah, yeah.

366
00:46:19,440 --> 00:46:29,680
And then are there, are there tools that one could use for making some of these things more

367
00:46:29,680 --> 00:46:36,400
interactive and use as a, just at the beginning you describe how it is interesting for designers

368
00:46:37,040 --> 00:46:40,960
and, you know, art practitioners to use these models.

369
00:46:44,320 --> 00:46:51,120
Yeah, I, I think, I think David Bao, my, my will, will talk about it. We have a bunch of tools on

370
00:46:51,120 --> 00:46:58,480
visualizing and monitoring the internal, internal units of this model, if you would like to understand

371
00:46:58,480 --> 00:47:04,800
it better. I'll have, I think David has a bunch of online tools he will talk about maybe tomorrow

372
00:47:04,800 --> 00:47:14,880
or later. Yeah, I think in general, it's, it's also hard to use. It takes like four hours on 100

373
00:47:14,880 --> 00:47:24,080
photos. So if you have 20, 80 time GPUs, yeah, so it's much faster than you know, big game model

374
00:47:24,080 --> 00:47:31,520
for four months or several weeks. Yeah, I think we're also working on maybe faster training,

375
00:47:31,520 --> 00:47:36,000
we are still working on that. So hopefully you can reduce the training time to several minutes

376
00:47:36,000 --> 00:47:43,920
or maybe half an hour. So more people can, can, can use it, right. So oftentimes you may not have

377
00:47:43,920 --> 00:47:49,200
the GPU resources, you may not have so many images. So we are working on that and try to,

378
00:47:50,160 --> 00:47:54,560
yeah, but it's also good for us because we also have limited resources compared to big companies.

379
00:47:55,680 --> 00:48:00,480
It's not like you're MIT and you have lots of resources, but compared to big companies,

380
00:48:00,480 --> 00:48:07,280
we don't have so much resources in academia. And then also David is asking, how long does it take

381
00:48:07,280 --> 00:48:17,840
to compress a model? Yeah, compress a model, it takes as long as much as time as training a model,

382
00:48:17,920 --> 00:48:25,120
maybe, maybe, maybe 50 times more. It's slower because you want to,

383
00:48:27,360 --> 00:48:32,320
you are training up all kinds of configuration at the same time, so it takes longer to train.

384
00:48:32,320 --> 00:48:35,680
Maybe it takes 50% more time. And while we are working on

385
00:48:36,880 --> 00:48:44,240
improvement of that, try to make the training faster. And it works for a bunch of models as well.

386
00:48:44,560 --> 00:48:52,400
Yeah, the compressing model is slower because the idea is you can, you have a bunch of

387
00:48:52,400 --> 00:48:55,760
models with different configurations. They all work pretty well. So you can use

388
00:48:56,560 --> 00:49:00,960
configuration A for your CPU, you can use a different configuration for your,

389
00:49:02,160 --> 00:49:08,560
for your mobile device. So you can have different models for different devices.

390
00:49:08,560 --> 00:49:12,000
Once you train the model, you can, you have this kind of flexibility.

391
00:49:12,640 --> 00:49:17,120
The idea is once you train the model, you only need to train once and you can deploy to 10 or

392
00:49:17,120 --> 00:49:23,360
20 devices. That's, that's quite essential for critical, for practical purpose because

393
00:49:23,360 --> 00:49:28,400
if you develop a product, you would like, you don't want to compress a model for

394
00:49:29,760 --> 00:49:33,840
iPhone. And then next time you'll compress for older iPhone or Pixel to a Pixel 3, right?

395
00:49:33,840 --> 00:49:41,520
So you would like to have this kind of versatile ability to train once and

396
00:49:42,000 --> 00:49:49,440
deploy to multiple devices. Yeah. I think that this is very important because

397
00:49:51,040 --> 00:49:58,080
you can put the time for the developer and then when the user wants to use it,

398
00:49:59,040 --> 00:50:07,520
the hope is that they spend very much less time to get what they want, which is a very good idea.

399
00:50:08,080 --> 00:50:10,560
Thank you so much, Junion. I appreciate that.

400
00:50:13,680 --> 00:50:16,480
It was such a pleasure for us.

