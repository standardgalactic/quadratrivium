WEBVTT

00:00.000 --> 00:09.520
All right. Hello, everyone. Welcome to your course, AI for Art, Aesthetics and Design

00:09.520 --> 00:21.120
and Creativity. Today, we have a very special lecturer, AJ. He has been at MIT just like

00:21.120 --> 00:31.200
you for his undergrad. I got to know him when he was here and he's very active. I've been running

00:31.200 --> 00:39.360
at the ML groups and sometimes chatting with me about, you know, these topics of creativity and

00:40.080 --> 00:48.240
AI and art. I think that this is very exciting. He's going to tell us about his journey and

00:49.040 --> 00:58.880
and his new work. I will let him to, you know, start the discussion. AJ, one of the things that

00:58.880 --> 01:06.480
I always ask is that if you could please introduce yourself and tell us a little more about what

01:06.480 --> 01:14.000
inspires you to work in this area. Sounds good? Yeah. Yeah, absolutely. I'd be happy to.

01:14.000 --> 01:20.400
And thanks so much for having me. So today, I'm going to be talking about some work I've done,

01:20.400 --> 01:24.000
some works that's happening in the community around 3D content creation.

01:25.360 --> 01:32.400
But first about my journey. Yeah, I was at MIT for my undergrad and I was part of what is now

01:32.400 --> 01:36.080
called the AI Club. And then we called it the Machine Intelligence Community.

01:36.880 --> 01:44.400
In my undergrad, I did research in a couple of areas, but mostly actually in compilers.

01:44.400 --> 01:49.680
So a little distant from what I do now, more on the high performance computing side and

01:49.680 --> 01:54.560
performance engineering that had experienced self-driving cars and generative models for

01:54.560 --> 01:59.760
self-driving applications during undergrad and really fell in love with that topic. How do we

01:59.760 --> 02:04.800
reason about uncertainty? How do we model complex data distributions and predict the future?

02:05.760 --> 02:10.000
Like, for example, predicting the behavior of vehicles. And that led me down the path of

02:10.000 --> 02:16.320
working on generative models in my PhD. And these generative models are, these days,

02:16.320 --> 02:20.240
the state of the art generative models are parametrized by deep neural networks, which try to

02:20.240 --> 02:25.040
fit large data sets, try to estimate correlations between different variables. And these could be

02:25.040 --> 02:29.200
old types of different data modalities, like images, they could be trajectories or behaviors,

02:29.280 --> 02:36.720
like I worked on, audio, video. And today, we're going to talk a little bit about 3D objects.

02:36.720 --> 02:40.880
And so that's kind of what inspired me. At the time, I was interested in uncertainty estimation.

02:41.440 --> 02:45.840
But these days, I just really like the tangible results you can get out of generative models,

02:46.400 --> 02:52.000
novel samples, and novel designs. It's very fun. I got to look at pretty pictures all day.

02:54.640 --> 02:58.640
To my research interests, like I mentioned, around generative models, we've done some work

02:58.640 --> 03:04.400
in denoising auto encoders. How do you generate images with these denoising diffusion probabilistic

03:04.400 --> 03:08.480
models? That's purely in the 2D setting, though it's been extended to other domains.

03:10.800 --> 03:15.520
Over the past year and a half, I've also been doing a lot of work in 3D reconstruction and

03:15.520 --> 03:21.280
inverse graphics. So how do we take images and try to infer a scene from them or generate in

03:21.280 --> 03:28.720
the 3D space? Sorry, Ajay, interrupting you. It seems that some of the students want the

03:28.720 --> 03:33.280
transcription to be on. Is that okay? That's fine. Okay, excellent. Thanks.

03:36.320 --> 03:41.840
And building off of that performance engineering background I had from MIT, I also did a lot of

03:41.840 --> 03:46.240
work in the intersection of machine learning and programming languages at the start of my

03:46.240 --> 03:51.600
graduate school. And I would summarize kind of my research interest as making it easier to

03:51.600 --> 03:59.040
create creative content, especially with AI tools. And to provide some background for today,

03:59.040 --> 04:04.160
I'm going to discuss different types of scene representations. What I mean by this is how do

04:04.160 --> 04:11.760
we encode the geometry and colour of a scene in some format that we can work with digitally.

04:12.320 --> 04:17.520
And there's this very long history of this, particularly from the graphics literature.

04:18.160 --> 04:22.240
On this slide shows some different representations of geometry that you'll be familiar with some

04:22.240 --> 04:29.200
of them. 2.5D might include RGBD images, like a photo plus a depth scan. And they're point

04:29.200 --> 04:34.320
clouds, meshes. Meshes are the most common representation used in graphics applications,

04:34.320 --> 04:37.440
but they can actually be challenging to work with in a learning context.

04:37.440 --> 04:45.760
So our focus in the learning context will be on the volumetric approaches. These can be very

04:45.760 --> 04:51.520
easy to train. You can kind of think of at least a boss of greatest classifier, mapping each point

04:51.520 --> 04:58.160
in space to whether it's part of the object or not, whether it's occupied or not. More recently,

04:58.160 --> 05:03.040
there's been a significant amount of interest in neural scene representations, sometimes called

05:03.040 --> 05:08.640
implicit neural representations that define the geometry of the object with a function.

05:09.280 --> 05:13.520
That could be a distance function, so a network mapping from coordinates to the distance to the

05:13.520 --> 05:20.080
nearest surface. And these can be a lot easier to optimise. These neural scene representations

05:20.080 --> 05:25.360
can also compress the data significantly compared to explicitly representing the geometry of the

05:25.360 --> 05:32.480
scene. So we'll be focusing on that direction. And in particular, we're going to be discussing

05:32.560 --> 05:37.280
a model called neural radian fields I'll get to in a second. But they address this problem of

05:37.280 --> 05:45.200
view synthesis. So how do we take some sparsely sampled input views of a scene and then construct

05:45.200 --> 05:50.960
a representation of that scene's 3D geometry and colour in a way that allows us to render it from

05:50.960 --> 05:56.800
new perspectives? These are some example works. You can represent the scene as a multi-plane image.

05:56.880 --> 06:03.760
So instead of a flat grid of RGB values represented as multiple planes, and that allows very quick

06:04.720 --> 06:11.120
rendering from new perspectives. Neural volumes is an approach from Facebook that has an encoder

06:11.120 --> 06:16.560
decoder structure. Take some input images and encode them into a layman space, kind of a 3D

06:16.560 --> 06:23.600
layman space and decode it out to images with volume rendering. Neural radian fields have

06:23.600 --> 06:29.280
really been very popular over the last two years due to their simplicity and quality of the results

06:29.280 --> 06:36.240
they can generate. So here's an example scene that's captured on the Berkeley campus. Some photos of

06:36.240 --> 06:40.000
the scene are captured, for example, with an iPhone. I believe these are captured with an iPhone.

06:42.080 --> 06:48.160
Then poses for each photo are estimated. And this neural scene representation called the neural

06:48.160 --> 06:53.920
radian field is estimated off of those images. What's really nice is once we estimate the

06:53.920 --> 06:58.480
representation of the scene, we can render it from novel viewpoints and kind of smoothly

06:58.480 --> 07:03.840
interpolate these sparsely sampled views. The scene is only very sparsely observed from discrete

07:03.840 --> 07:07.760
points. What if you as the user want to make a photo from a new perspective?

07:09.440 --> 07:15.280
There's some interesting things to note about this rendering. Notice the specularities on the

07:15.280 --> 07:20.880
surface. They're not just modeling the diffuse light. Also modeling how the light reflected

07:20.880 --> 07:26.960
back at the user depends on the viewpoint of the camera. As you shift your head, the scene will

07:26.960 --> 07:34.720
change. This is particularly visible on very shiny surfaces like the glass or metal of the car.

07:38.000 --> 07:44.640
A neural radian field, yeah, it really is amazing and really captured the attention of a lot of

07:44.640 --> 07:51.040
people. This neural radian field has grown very quickly and there's still a lot of problems to be

07:51.040 --> 07:56.640
solved. One very interesting thing that these neural scene representations bring to mind is

07:56.640 --> 08:01.680
that we're encoding a scene in the neural network's weights. Instead of explicitly encoding the

08:01.680 --> 08:08.160
geometry of the scene via points or meshes, lists of triangles or voxel grids, it's encoded

08:08.880 --> 08:15.440
into this small multi-layer perceptron. Maybe this is a half a million parameter network,

08:15.440 --> 08:21.440
just some stacks of dense layers. It's representing a function mapping from 3D space coordinates,

08:22.160 --> 08:28.480
XYZ. This is in the scene, XYZ coordinates, and a viewing direction. What is the angle of the camera

08:28.480 --> 08:34.560
in order to model those view dependent effects? The neural network then predicts at this coordinate

08:34.560 --> 08:40.000
what is the color of the scene and then its density, sigma. There's density as something like

08:40.000 --> 08:43.600
how solid is the object and how much light will be absorbed.

08:48.480 --> 08:54.320
Rendering is done by ray tracing. This is not exactly what would be done in most graphics

08:54.320 --> 09:01.440
applications like real-time ray tracing because we've kind of encoded the light being reflected

09:01.440 --> 09:07.200
at any given point back towards the viewer into this function. So we don't have to scatter

09:07.200 --> 09:12.960
light through the scene. The viewer will cast a ray from their camera through the pixel. This is the

09:12.960 --> 09:19.280
image plane into the scene and then query the neural network along the ray. These are different

09:19.280 --> 09:26.080
3D coordinates in the scene. The color of the rendered pixel will then be some accumulation

09:26.080 --> 09:30.640
of the colors along that ray. In order to determine the color and the density along the ray,

09:30.640 --> 09:34.080
each of these coordinates is passed to the MLP as a very large batch.

09:35.920 --> 09:41.600
We get a color and density for each coordinate and then can compose them with alpha compositing

09:41.600 --> 09:47.760
into a color. There's some subtlety to this compositing. This is called the volume rendering

09:47.760 --> 09:56.480
equation because this equation is pretty simple. This is the density predicted. This is the camera

09:56.480 --> 10:02.960
origin and it's displaced some steps along the ray. The neural network will predict what is the

10:02.960 --> 10:07.760
density of the scene at that point, but it will also predict what is color. Then we're integrating

10:07.760 --> 10:12.160
this color along the ray weighted by its density, but we also have to weight it by

10:12.160 --> 10:19.520
transmittance, which is roughly speaking how much light is transmitted from the viewer

10:19.600 --> 10:26.560
to that point along the ray because once we've accumulated enough density, then objects

10:26.560 --> 10:33.040
further back in the scene will not be visible to be included. This equation for color

10:34.320 --> 10:40.160
conditioned on coordinates is differentiable with respect to the parameters of sigma and c.

10:40.160 --> 10:46.720
So sigma and c will be this neural network. Because this is fully differentiable, it's

10:46.720 --> 10:51.120
relatively easy to optimize. Instead of optimizing scene geometry, we'll optimize the

10:51.120 --> 10:58.640
weights of this neural network in order to get some desired colors. This might be the sparsely

10:58.640 --> 11:04.240
observed viewpoints, two viewpoints. Let's render the color according to the neural scene

11:04.240 --> 11:08.160
representation and then try to optimize the network so that it matches the observed views,

11:08.160 --> 11:15.360
pixel by pixel. It might take a minute to wrap your head around, but it's actually pretty simple.

11:15.360 --> 11:20.880
We have this one MLP that encodes the scene, lets us render viewpoints differentially,

11:20.880 --> 11:26.080
and we'll optimize the scene so that it matches the input views, and that's why it's inverse

11:26.080 --> 11:31.920
graphics. We're going from the 2D space to optimize for the underlying 3D representation

11:31.920 --> 11:38.320
that will reconstruct those views. Are there any questions about that?

11:39.200 --> 11:50.480
Can you talk about how the points are used for the neural net? I can't see it directly.

11:52.320 --> 12:00.080
I mean, I get the high-level idea, but could you talk about how those points are fed into the net?

12:01.040 --> 12:05.360
Yeah, so you could consider constructing an MLP with five dimensions as input,

12:05.920 --> 12:11.280
just five inputs, and then four outputs on the layers, and then intermediate features or whatever

12:11.280 --> 12:17.680
you can imagine you want. So in nerf it's 256, that would be one approach, and it does work,

12:17.680 --> 12:23.200
but then you get actually quite blurry reconstructions of the scene if you directly feed an input

12:23.200 --> 12:27.600
coordinates as these are just floating point numbers, 3D coordinates, and going that direction.

12:27.920 --> 12:34.640
But instead, what is used in this neural radiance field is assigned useoidal

12:34.640 --> 12:41.520
positional encoding, so frequency-based encoding. If you're familiar with the transformer positional

12:41.520 --> 12:49.360
encoding, this is a common approach where continuous values like coordinates or time

12:49.360 --> 12:57.360
steps are encoded using a Fourier representation. So you take sine of various scaled values of the

12:57.440 --> 13:00.400
input coordinates, and that lets the network model high-frequency detail.

13:01.680 --> 13:06.720
So instead of kind of memorizing a function from each spatial coordinate, it can model

13:09.920 --> 13:14.080
frequencies of the underlying signal if you use the sine useoidal embedding of the input.

13:14.080 --> 13:18.480
So that kind of just projects this five-dimensional input into some higher-dimensional space

13:18.480 --> 13:24.240
before feeding it to the MLP. I see. And there's no, let's say, I guess, filtering

13:24.240 --> 13:33.440
done before, I mean, applying it to the net. So it's just transforming certain, I guess,

13:33.440 --> 13:38.880
components, but not doing some, I guess, post-processing before putting it into,

13:38.880 --> 13:43.520
or let's say compression or something like that. No, not really compression. There's

13:43.600 --> 13:48.960
some, like, coordinate transform because you'll do this computation in a particular coordinate frame.

13:51.360 --> 13:57.920
There is some subsequent work which we actually build upon that does a pre-filtering of the

13:57.920 --> 14:04.800
input coordinates. So instead of encoding all the frequencies of the input coordinates,

14:04.800 --> 14:07.920
they'll be blurred depending on how far away from the camera you're querying.

14:08.880 --> 14:13.680
But that's sort of subsequent work to nerve. That reduces the aliasing.

14:15.520 --> 14:17.680
Let's see. And the network is just fully connected.

14:18.320 --> 14:21.520
Yeah, just a fully connected network. Super simple.

14:21.520 --> 14:22.080
Yeah, thank you.

14:24.080 --> 14:31.440
Yeah, one more thing that I wanted to mention here for a student is that there is a difference

14:31.520 --> 14:38.480
between how you train this model versus the models that so far you have seen for,

14:38.480 --> 14:44.720
for instance, classification. For instance, if you want to train a model for a truck,

14:46.160 --> 14:52.560
what you do is you get a lot of images of different trucks in different lightings and different,

14:54.080 --> 14:59.440
you know, models and things like that, and then fit it to your network. However,

15:00.240 --> 15:06.800
in this case, you are taking lots of images of the single truck, single scene,

15:07.360 --> 15:14.400
and you are trying to reconstruct that scene. So you said big difference between, you know,

15:15.280 --> 15:19.840
what you are used to doing and what we see in nerve.

15:22.400 --> 15:28.720
Yeah, absolutely. I kind of see it as instead of, the nerve is representing a single scene. So

15:28.720 --> 15:33.440
instead of representing explicitly or representing it with a neural net with a function,

15:34.720 --> 15:38.720
but it doesn't generalize. It interpolates these input views.

15:42.640 --> 15:47.280
And, you know, there's a catch to that, which is that in order to fit into the neural radiance

15:47.280 --> 15:53.520
field to a single scene, it generally needs a lot of data. So while these views are sampled

15:53.600 --> 15:57.520
sparsely, discreetly, and there will be larger regions of space where we don't have

15:58.480 --> 16:05.280
an image taken from that perspective, still to estimate a multi-view consistent radiance field,

16:06.000 --> 16:10.720
experiments in the paper used a large number of images per scene. That's a little bit impractical.

16:10.720 --> 16:14.320
So for these synthetic scenes, this is one synthetic scene that's rendered in blunders.

16:15.360 --> 16:19.360
They were able to get out 100 images of each scene and fit the neural radiance field on it.

16:19.360 --> 16:23.200
For those outdoor scenes, I showed earlier like that red Toyota car.

16:24.480 --> 16:29.600
I think it's a fewer, maybe 20 images, but still that's a lot to capture with a handheld camera.

16:31.600 --> 16:36.400
And in the first week of work, I'm going to talk about we improved the data efficiency of the

16:36.400 --> 16:41.760
neural radiance field training process. So instead of using, let's say 100 images on this Lego

16:42.880 --> 16:46.720
scene, we used eight photos taken from randomly sampled viewpoints.

16:49.760 --> 16:55.840
In the neural radiance field training process, we would take, we would know the pose of each image

16:55.840 --> 17:01.360
that can be estimated with a system like call map. It's really common in the 3D graphics and 3D

17:01.360 --> 17:07.840
computer vision community is given some images, estimate their camera poses with correspondence

17:07.840 --> 17:14.160
finding, then the neural radiance field loss renders an image or renders some rays from the

17:14.160 --> 17:21.200
same pose as the input, then it computes a mean squared error loss. So the pixel wise error.

17:22.880 --> 17:26.880
The reason that this loss can be used is because we know the camera pose, we're able to render out

17:26.880 --> 17:34.240
the scene from the exact same pose that the observer took the photo. If the camera poses shifted

17:34.240 --> 17:39.680
in the rendering process, then the reconstructed image and the true image won't align pixel wise

17:39.680 --> 17:44.960
and we'll learn from inconsistent geometry. And so this is done at all of the observed

17:44.960 --> 17:50.000
camera poses. And this is why the neural radiance field needs so many photos. If there's no observed

17:50.000 --> 17:53.440
photo, then it doesn't have the ability to compute a loss from a given perspective,

17:54.400 --> 18:00.720
which means that it could overfit to the input use. This representation mapping from coordinates to

18:00.720 --> 18:05.920
colors is very flexible. One possible degenerate solution would be to put a billboard in front

18:05.920 --> 18:11.200
of each camera, just a poster board, you know, off of the highway, right in front of your camera

18:11.200 --> 18:17.120
containing the image that's observed, rather than learning a consistency in geometry.

18:18.800 --> 18:25.360
And there's other ways you can get artifacts. This is described as a shape radiance ambiguity

18:25.360 --> 18:30.560
in the Nerf++ paper. Essentially, we could either reconstruct the shape correctly and then have

18:31.200 --> 18:36.640
relatively constant radiance from different cameras, or we could encode each image into the

18:36.640 --> 18:40.960
view dependent coordinate of the network. So because the network depends on the camera position,

18:40.960 --> 18:46.160
it's able to memorize potentially the photo taken from each camera.

18:48.400 --> 18:52.720
When the neural range field is trained with 100 views, it gets really crisp reconstructions. This

18:52.720 --> 18:58.640
is a hot dog scene, synthetic scene, where we render out the views in Blender. Then the neural

18:58.640 --> 19:04.880
radiance field, when it's trained with only eight views, only matches pictures close to the training

19:04.880 --> 19:10.160
data. When you move the camera further away from the observed images to try to extrapolate,

19:10.160 --> 19:15.600
then there'll be a lot of artifacts. If you regularize the neural radiance field a little bit

19:15.600 --> 19:20.560
and simplify it, it can learn more consistent geometry, but there still are a bunch of artifacts

19:20.640 --> 19:29.840
in the reconstruction. I'll skip over this. So in our work, we add an additional loss to the

19:29.840 --> 19:36.080
neural radiance field training. We keep using the Nerf mean squared error loss. It's called

19:36.080 --> 19:43.120
photometric loss on the observed views that are sparse. But then our work diet nerf adds an

19:43.120 --> 19:48.240
additional loss at unobserved positions. So because we have this neural radiance field

19:49.040 --> 19:55.920
at any iteration during training, we're able to render out novel views even before the scene has

19:55.920 --> 20:02.960
converged. It's a little silly that in Nerf, we're not able to constrain these input views,

20:03.760 --> 20:09.280
because as a person looking at, okay, let's say that our estimate of the scene's geometry

20:09.280 --> 20:14.880
gives us these renderings. This is the observed rendering. We as people can still look at these

20:14.880 --> 20:21.280
photos and derive some loss signal. Okay, the input view is a lot sharper than my current

20:21.280 --> 20:27.840
estimate of the scene. There's a little red light at the top of the truck, but there's no light on

20:27.840 --> 20:38.960
top of these reconstructions. Based on this principle that you can compare views at different camera

20:38.960 --> 20:45.680
positions as a person by comparing their semantics, like, you know, it's a bulldozer, a bulldozer is

20:45.680 --> 20:51.600
a bulldozer from any perspective. We propose to add a loss in feature space. Using some visual

20:51.600 --> 20:59.520
encoder, each of the input views is represented in a feature space. And then instead of computing

20:59.520 --> 21:07.360
the loss in pixel space, diner will compute a loss in feature space. And that allows us to regularize

21:07.360 --> 21:16.640
the scene from any perspective during training. We call this a semantic consistency loss,

21:16.640 --> 21:21.440
since we're making sure that these semantic features, things like object identity, object color,

21:22.880 --> 21:29.680
are consistent across views. And over the course of training, this improves the results.

21:30.400 --> 21:36.000
So the loss that Nerf used was this mean squared error loss, and then we're adding this semantic

21:36.000 --> 21:43.840
consistency loss where some encoder thigh, some neural network encodes rendered images, and then we

21:43.840 --> 21:53.040
compare them in a feature space. We do have to sample camera poses in order to render this, so

21:54.000 --> 22:05.600
there's just some prior distribution over camera poses. The choice of that feature thigh is really,

22:05.600 --> 22:12.400
really important for the results, because we want it to be consistent across viewpoints. So it should

22:12.400 --> 22:17.520
really encode the object's identity and properties about the object rather than low-level details,

22:17.760 --> 22:28.480
like the exact pixel colors. And motivated by that, we use a network called Clip. This is from

22:29.440 --> 22:36.320
last year. It's a representation of images and text, so a representation of an images learn,

22:37.760 --> 22:42.400
such that it has an aligned representation with an associated caption. The data that Clip is

22:42.400 --> 22:48.080
trained on is a very large data set of 400 million images that have associated captions

22:48.080 --> 22:54.800
crawled from the web. And the Clip model has really led to an explosion of work in the AI art

22:54.800 --> 22:59.680
community. It's really powerful. It's trained on such a large amount of data that we're able to

23:01.840 --> 23:06.160
prompt it with topics that you wouldn't find in a narrow data set.

23:06.480 --> 23:11.760
It also, by learning to match images to this text, we'd hope to learn some very useful features

23:11.760 --> 23:17.280
about an image. For example, in captions, you can encode classes of objects, just like image net

23:17.280 --> 23:24.080
labels. You can also encode a lot of other details, like the scene rather than just the foreground

23:24.080 --> 23:30.160
object. You can encode things about pose of the underlying object, like a sitting person,

23:30.160 --> 23:37.280
a standing person. And that should be encoded in the representation learned by the network,

23:37.280 --> 23:41.600
if it's going to be able to match images against their associated caption. So the

23:41.600 --> 23:46.080
training objective is encode a bunch of images, encode their captions, and then try to match

23:46.960 --> 23:53.680
images with their true caption. Clip was originally used for discriminative tasks,

23:53.760 --> 24:02.960
object classification in a prompting fashion. So if you want to classify photos of food,

24:04.160 --> 24:09.200
the authors of clip constructed a bunch of captions, templatized with the desired object

24:09.200 --> 24:15.200
category, a photo of guacamole, a photo of ceviche. And then the class label is given by the

24:16.080 --> 24:19.200
caption with the best match with a given image.

24:19.600 --> 24:23.680
The property we're particularly interested in in this 3D reconstruction context is whether the

24:23.680 --> 24:28.480
representations of the images are consistent across views. That's what we call semantic

24:28.480 --> 24:35.040
consistency in the work. What this plot is showing is that the cosine similarity of embeddings

24:35.040 --> 24:40.400
from the image encoder of clip within a particular scene from different camera poses is highly

24:40.400 --> 24:46.800
similar. So very high similarity in feature space within a scene across different perspectives,

24:47.040 --> 24:59.280
low similarity across scenes at different perspectives. So because images are very

24:59.280 --> 25:04.080
similar in clip's feature space, very different in pixel space, we're able to maximize feature

25:04.080 --> 25:10.880
space similarity of clip and get some useful loss. Now what you've been waiting for are the results.

25:11.600 --> 25:15.680
This is nerf trained on eight views when it's simplified. And then here is it trained with

25:15.680 --> 25:20.880
our additional semantic consistency loss. A bunch of near field artifacts in nerf,

25:20.880 --> 25:24.640
but when we add in this feature space loss, it removes a lot of those artifacts.

25:30.960 --> 25:36.240
Because those artifacts aren't plausible, they reduce the semantic consistency.

25:36.240 --> 25:46.960
Cool. I'm going to go on to the next work. Before I do, anyone have questions?

25:46.960 --> 25:56.720
I have one question, which is with regards to using clip. Are you able to access the text

25:56.720 --> 26:02.400
as well that clip generates, or are you able to decode it in some way and actually access

26:02.400 --> 26:09.120
how the clip looks at the inputs? Just in terms of explainability, I thought it could be,

26:09.120 --> 26:13.440
yeah, sounds really interesting. Yeah, that's a very good question. So in this work,

26:13.440 --> 26:19.040
we're not actually using the text encoder. We'll see that in the next work. The text encoder is

26:19.040 --> 26:24.640
just used for pre-training clip in dye and nerf. So we're only using this image encoder.

26:24.640 --> 26:29.040
Because then the motivation for that is that the neural radian students are motivated by the

26:29.120 --> 26:33.200
view synthesis problem. So there's no text caption associated with the data.

26:33.200 --> 26:36.480
They just have a couple of pictures. So we only need to use the image encoder.

26:38.800 --> 26:44.080
That said, some artists have tried to take clip and use it to create a captioning model.

26:46.480 --> 26:49.840
If you have a model that can match images against captions, can you actually synthesize

26:50.560 --> 26:56.240
captions that best match a particular image? It's a challenging discrete optimization problem

26:56.240 --> 27:00.560
because you're searching for a textual caption that will maximize some neural network's output

27:00.560 --> 27:06.160
score. And that is basically a black box optimization problem. My impression is that

27:06.160 --> 27:10.480
automatic captioning with clip doesn't work too well. It's really good at selecting an associated

27:10.480 --> 27:16.160
caption out of a list of candidates. And that's how we're able to do object classification with clip.

27:18.880 --> 27:23.200
So I think you'd be better served by learning a specific captioning model that will generate

27:23.200 --> 27:27.200
a caption condition on image rather than trying to extract captions out of clip

27:28.480 --> 27:34.480
just due to the difficulty of the optimization or the search. Thank you.

27:40.800 --> 27:47.040
So like I said, we weren't using the text encoder in diet ner. In the next work, we try to

27:47.680 --> 27:56.080
move in an even more extreme direction of generating objects without any image data.

27:56.080 --> 28:00.320
So what if we only have a caption and want to synthesize the 3D object from it?

28:02.000 --> 28:07.360
Is that possible? Can we remove this mean squared error loss entirely and only use

28:07.360 --> 28:12.880
feature space losses? And these are some examples of the results we're able to get

28:12.880 --> 28:17.360
with different captions, like a render of a Jenga tower produces this object.

28:18.960 --> 28:26.960
You can also engineer prompts, use hashtags because clip is trained on web data.

28:30.000 --> 28:32.560
Our goal is to synthesize 3D objects from just the caption.

28:34.800 --> 28:41.200
And to kind of refresh our memories, the neural radiance field is an inverse graphics approach

28:41.200 --> 28:45.680
where we have densely sampled images, optimize the shared scene representation,

28:46.480 --> 28:52.560
and then are able to render out new views. In the dream fields work, the second work in this line,

28:54.160 --> 28:59.680
we do not have any observed images, only a caption written, for example, by a human artist.

29:01.840 --> 29:07.200
We optimize something that will look fairly similar to diet ner with additional regularizers,

29:08.160 --> 29:12.160
and then are able to render out new views. And any perspective is actually a new view

29:12.160 --> 29:17.040
because we haven't observed this scene. This is an associated scene for the caption,

29:17.040 --> 29:19.440
an epic, wondrous, fantasy painting of an ocean.

29:24.080 --> 29:30.560
So the neural radius would use this mean squared error loss, and then diet ner used feature space

29:30.560 --> 29:40.080
loss where the rendered image of the scene and an observed image of the scene are encoded into

29:40.800 --> 29:51.120
feature space that is optimized. Oops. Sorry. Okay. Now in dream fields, we use the text

29:51.120 --> 29:55.200
encoder of clip. That wasn't being used before. We were just throwing it away after trading.

29:56.160 --> 30:02.880
So instead of optimizing for the feature similarity in image feature space,

30:02.880 --> 30:08.320
we now maximize similarity of image and text features. The reason we can swap between

30:09.040 --> 30:12.880
the text encoder and the image encoder is because clip has learned to align representation.

30:12.880 --> 30:17.760
It has tried to maximize the similarity of representations of images and their associated

30:17.760 --> 30:23.760
captions so those representation spaces overlap. And you can in some sense swap the encoders

30:24.480 --> 30:28.800
from text encoder to an image encoder and hopefully still have that aligned representation.

30:30.000 --> 30:32.800
But overall, the pipeline looks fairly similar. So it's randomly sample

30:33.360 --> 30:41.040
poses in the scene, render an image, and then try to make sure that its semantic features

30:41.040 --> 30:50.080
match our features of the caption. But if you apply that approach naively without any regularizer,

30:50.160 --> 30:59.920
then there are a bunch of artifacts. These are some example generations for different captions.

30:59.920 --> 31:06.000
I believe this one had something to do with liquid in a blender. This one might have been

31:06.000 --> 31:13.920
a colorful bus with graffiti on it. So without regularization, we are getting to generate scenes.

31:13.920 --> 31:18.640
And it's not surprising because there's really no data involved in this process.

31:18.720 --> 31:22.800
In Dietner, the scene was regularized by having some input views.

31:24.800 --> 31:26.960
Here, the canvas is open, wide open.

31:31.520 --> 31:38.880
So in our work, we added some regularization. The scenes are composited with randomly sample

31:38.880 --> 31:46.320
backgrounds. And we regularize the scene to be highly transparent. So this transmittance loss

31:47.040 --> 31:52.080
encourages varsity in the underlying scene. So instead of getting lots of low density

31:52.080 --> 31:58.160
wispy content, like you saw in the previous slide, with a transmittance loss and this

31:58.160 --> 32:02.080
associated background, our motivation in Dreamfields is to create more of a consistent

32:02.080 --> 32:11.520
foreground object, a single foreground object. And these are the renderings for the

32:11.520 --> 32:21.600
associated caption, washing blueberries. There's definitely a lot of room for improvement

32:21.600 --> 32:27.120
because each of these blueberries is kind of mashed together with the others. The general

32:27.120 --> 32:32.880
caption has been encoded into this scene. And there's a consistent foreground object.

32:33.680 --> 32:44.480
This is the visualization of the process of optimization. In response to the question,

32:45.200 --> 32:49.440
Leandra asked, so it's creating this from one image, there's actually no images observed.

32:49.440 --> 32:55.040
There's only a caption fed to the system. So any images that I'm showing are rendered

32:56.000 --> 32:59.040
using our neural radiance field. They're completely fictional.

33:05.280 --> 33:11.360
I mean, some intuitive explanation for this is how can we learn a scene representation such

33:11.360 --> 33:15.760
that it could be captioned with a given caption from any perspective.

33:18.800 --> 33:23.120
Maybe that's how a human sculptor went approach the problem. So given a caption, like give me,

33:24.000 --> 33:33.680
you know, a clay sculpture of a tower. Well, let's say, you know, a monocular sculptor.

33:34.720 --> 33:40.400
Good. Optimize for a clay sculpture that is a tower of many perspective.

33:43.840 --> 33:49.520
Sorry, what happens? Sorry, what happens if the caption is something vague, like just a dog?

33:49.520 --> 33:54.880
How would your optimizer know that, like, it should have the same dog even from different

33:55.440 --> 34:03.760
poses or camera poses? Yeah, excellent question. The constraint that views should

34:03.760 --> 34:07.360
represent the same object from different perspectives just comes from the shared

34:07.360 --> 34:13.840
three presentation. We're optimizing the same MLP from any perspective.

34:13.840 --> 34:22.160
Okay, thanks. We had to simplify. Well, we didn't have to. You're able to keep view dependence in

34:22.160 --> 34:26.640
the neural radiance field. So this regularizer ends up being kind of important. Like I discussed

34:26.640 --> 34:29.840
with Dietner, if you're able to learn a lot of these near field artifacts.

34:33.440 --> 34:37.120
Sharing the scene representation is important, but some of the other techniques on our paper,

34:37.120 --> 34:41.040
like the regularizer are also important for making sure you get a clean result.

34:44.800 --> 34:52.960
In this example, we experiment with different caption templates to measure the

34:52.960 --> 34:58.160
compositional generalization of the model. So the base caption template here is a teapot

34:58.160 --> 35:05.520
in the shape of a blank, a teapot imitating a blank. And then in the video, the caption beneath

35:05.520 --> 35:12.560
each object is the word that's filled into the template caption. So a teapot in the shape of

35:12.560 --> 35:18.160
an avocado produces this object. Whereas the caption of teapot in the shape of a glacier

35:18.160 --> 35:26.320
produces something more ice styled. And I'm sorry about these animations.

35:28.400 --> 35:34.560
If you switch the caption from an armchair to a teapot, you'll also notice some changes in the

35:34.560 --> 35:39.280
shape. So there's legs on this avocado chair, but when it becomes teapot, the legs are removed.

35:39.760 --> 35:46.720
There's a follow-up question about whether the Clip Library is 2D. Yes, Clip is trained only on

35:46.720 --> 35:54.480
2D images. So just on 2D views. The motivation for using Clip is that we can very scaleably

35:54.480 --> 35:59.760
acquire caption images from the internet. If you, for example, look at Wikipedia and just look at

36:00.640 --> 36:05.920
the upper right image associated with each article, it has a caption beneath it. And there's a data

36:06.000 --> 36:09.760
set out there called WikiText, which has about 11 million captioned images.

36:10.960 --> 36:16.080
The authors of Clip were able to collect even larger data set by scraping websites other than

36:16.080 --> 36:24.080
Wikipedia. But if you look at data sets with 3D objects in them, they're very small. The largest

36:24.080 --> 36:29.440
might be ShapeNet, which is entirely synthetic objects. And there's usually no caption associated.

36:29.440 --> 36:35.520
So we'd have to have a human annotate. This is a general trend in the 3D reconstruction literature

36:35.520 --> 36:44.000
that the availability of 3D data is quite limited. And so in dream fields, we're able to exploit this

36:45.840 --> 36:51.760
pre-trained 2D image encoder and text encoder, and then kind of lift it up into 3D by using

36:51.760 --> 37:01.360
a shared representation of the geometry. There's a bunch of different techniques that we use to

37:01.360 --> 37:06.800
improve the quality of the results. I won't get too much into this, but the metric is a little

37:06.800 --> 37:13.280
tricky to define because there's no reference object for each caption. We only have a data

37:13.280 --> 37:20.320
set of captions provided to us by the user, and we're one of measure how well our generations are

37:20.320 --> 37:28.880
performing. In order to do that, we use a neural metric based off of matching generated 3D objects

37:28.880 --> 37:35.120
against the input captions. This is something like precision of retrieving the correct caption,

37:35.120 --> 37:39.520
given the generator objects. Some of the most important techniques that help us here are

37:41.520 --> 37:47.600
regularizer for transmittance and data augmentations, those architecture we use for the MLP,

37:50.720 --> 37:55.280
and then later on, what model we use for clip.

37:58.880 --> 38:01.920
This is an example of the process of optimization from different iterations,

38:01.920 --> 38:06.000
so it actually can converge quite quickly, but additional detail might be added over the course

38:06.000 --> 38:12.880
of training. In order to run 20,000 iterations of optimization, it's an expensive process

38:12.880 --> 38:18.720
because we need to render out these images during training, but back of the envelope calculation

38:18.720 --> 38:26.320
is about three to four dollars to generate each model on TPU in Google Cloud at an hour.

38:26.480 --> 38:29.440
It's in the realm where an artist could afford to do this.

38:30.720 --> 38:35.520
We're working on some follow-up work, which will speed up this process and make it even less expensive.

38:41.600 --> 38:47.440
That's all I've got on these works. The broad goal here is to make content creation

38:48.240 --> 38:55.520
easier and generate assets that are useful. This 3D assets I see is particularly useful

38:55.520 --> 39:00.400
for downstream applications because they could be plugged into a game or plugged into some other

39:00.400 --> 39:07.200
system. We have code out for both of these projects. If you want to try out the text

39:07.200 --> 39:11.760
to 3D generation in your browser, you can use a Colab notebook that I put together.

39:12.400 --> 39:17.200
I've tested it on the Pro version of Colab, which has higher memory GPUs,

39:17.200 --> 39:18.880
so you might need to play with some of the parameters.

39:18.880 --> 39:28.880
Thank you so much, Eje. This is really fascinating. I have a few questions,

39:30.240 --> 39:37.600
and then before letting everyone else ask questions, the first question is that

39:39.680 --> 39:48.560
are you able to walk us through some of the Colab code today or should we do it on our time?

39:49.840 --> 39:56.560
Let me see if I have it up. Also, before going to changing your screen,

39:56.560 --> 40:05.280
can you please go back to the animations? Sorry, I have so many questions because this

40:05.280 --> 40:15.680
is really cool. Or maybe the one that is armchair. Yeah, give me one sec. Thank you so much.

40:19.360 --> 40:29.360
I think these are really cool. I think that for the students and I, this kind of inspires us to

40:29.360 --> 40:40.640
think maybe one cool thing to do is that we can generate these things and use them in some avatar

40:40.640 --> 40:47.520
or game or something, and this will be really cool. This is something for students to think about

40:48.480 --> 40:57.920
for their future projects because the goal of this course is to inspire us to think about

40:57.920 --> 41:04.400
what are the creative ways that we can use AI. This is really cool. One question that I have

41:04.400 --> 41:14.160
is that, can you share some intuition of, for instance, let's say the rubric. It looks like

41:14.240 --> 41:23.040
a rubric and it looks like a chair, but then we see that there is some, we wish there was more of the

41:24.080 --> 41:33.760
structure and it might be because clip is the objective and or assessor and thinking that,

41:33.760 --> 41:42.800
okay, as long as I have a patch of red and yellow and things like that that are appearing on rubric,

41:42.880 --> 41:48.800
I'm happy, the rest, I don't care much, or is there any better explanation of what's happening?

41:50.240 --> 41:58.800
Yeah, so the 3D structure only emerges because of the shared representation and the easiest way to

42:00.480 --> 42:05.440
satisfy clip from any perspective, having this Rubik's Cube chair from any perspective,

42:06.560 --> 42:11.440
might actually be to learn some consistent geometry. That said, there's no prior

42:12.400 --> 42:17.200
other than sparsity and some implicit regularization just in the structure of the MLP,

42:17.840 --> 42:24.560
so there's no prior on the 3D structure learned from data. That's something that I think is missing

42:24.560 --> 42:30.800
and definitely opportunity for future work is how do we learn some priors on 3D data and integrate

42:30.800 --> 42:38.240
them into the system to try to improve the plausibility of the structure. One example where

42:38.240 --> 42:45.760
this issue arises is that sometimes you'll get repeated structures on the objects,

42:46.720 --> 42:52.720
like if you optimize for a dog, maybe it will have eyes on multiple sides of its face because

42:53.680 --> 43:00.080
they're not visible. So you only see two sets of eyes from any particular viewpoint,

43:00.080 --> 43:05.920
that is all the discriminator clip ever sees are those two eyes, but the underlying geometry,

43:05.920 --> 43:09.280
there's no constraint that the dog should only have to rise.

43:11.360 --> 43:16.320
Okay, excellent. Thank you so much. Are there questions before we go to the collab?

43:21.680 --> 43:29.360
The outputs, are they like .fbx files or do they still need to be, let's say, a little bit

43:29.360 --> 43:35.520
prepared in rendering software before they can be actually readily used in the game engine?

43:35.920 --> 43:42.480
Like Unity or Unreal? They do need to be post-processed. So what you get out is a train neural net,

43:42.480 --> 43:47.200
so it's function mapping from coordinates. We don't use the v direction in these results,

43:47.200 --> 43:53.840
just XYZ coordinates mapped to color and density. So there are a bunch of ways that you could convert

43:53.840 --> 43:59.200
that. I don't know of off the shelf software that will be able to do that conversion for you,

43:59.200 --> 44:05.680
it'd have to be coded up, but you can sample the scene on some grid, for example, and then you'll

44:05.680 --> 44:10.560
get out color and RGB. You could convert that to a local voxel representation. If you want to get

44:10.560 --> 44:18.240
a mesh, there's an algorithm called marching cubes that is able to find a mesh in the scene,

44:18.240 --> 44:21.920
and there's implementations on GitHub of marching cube for neural radiance fields

44:21.920 --> 44:28.960
that we haven't integrated into our particular library. So you take a little bit of glue to

44:28.960 --> 44:34.960
grab marching cubes and then plug it in. So what do you all use to turn the neural net into these

44:34.960 --> 44:41.520
graphics? Sorry, could you repeat the question? What do you use to turn the neural net into the

44:41.520 --> 44:45.840
graphics that we see here? Oh yeah, so that's done by rendering. So you can render the neural

44:45.840 --> 44:50.880
radiance field from any perspective in the code, but that just renders out a 2D image. It doesn't

44:50.880 --> 44:56.000
give you, you know, like a mesh, versus the game engine will have its own rendering algorithm based

44:56.000 --> 45:02.560
on rasterization or ray tracing, given the underlying geometry and texture map, which might

45:02.560 --> 45:07.200
be real time. So the rendering here is not real time. You have to go evaluate the neural network

45:07.200 --> 45:11.200
at a bunch of different coordinates and accumulate its outputs into an image.

45:13.200 --> 45:19.360
So that's implemented. If you want videos, we can do that, but you'll have to DIY the conversion.

45:24.720 --> 45:27.840
Ellie had a question on strategies to reduce rendering costs.

45:28.480 --> 45:35.280
So you can render images at low resolution. And in the Colab notebook, the rendering is done at

45:35.280 --> 45:40.000
very low resolution. So experiments, you render out 168 by 168 images or higher.

45:41.520 --> 45:46.720
But Colab only gives you a single low memory GPU. So we render out 88 by 88 images.

45:47.680 --> 45:53.440
And that really significantly speeds up the process. So rendering takes maybe 300 milliseconds.

45:54.400 --> 45:57.200
So you have to do about three iterations per second.

46:02.080 --> 46:15.040
If you're using alpha transparent, okay. So Ben is asking, how do we handle with transparent objects?

46:15.760 --> 46:23.680
So the neural radiance field, the volumetric representation is really amenable to transparent

46:23.680 --> 46:31.360
objects because the density is this continuous value and we can observe objects. So accumulate

46:31.360 --> 46:36.880
color from objects behind transparent objects. In optimization, you might decrease the

46:36.880 --> 46:41.280
density of some object that should be transparent, like stay in glass windows.

46:42.240 --> 46:48.640
And the background is composited at the end. So any ray, if there is some accumulated,

46:49.440 --> 46:55.840
if the transmittance is not zero along the ray accumulated throughout the scene,

46:56.400 --> 46:58.720
then there'll be some contribution from the background image.

47:00.480 --> 47:05.040
So the reason that we've kind of encouraged coherent objects is that if the object is not

47:05.040 --> 47:09.520
coherent, then the background will leak through the translucent objects. Oh, I see what you're

47:09.520 --> 47:14.560
saying. Yeah, if you want stay in glass windows. So I mean, you would have to, the scene would

47:14.560 --> 47:18.960
probably optimize so that the transparent object is blocked from behind by something.

47:28.240 --> 47:34.160
Yeah, the next steps, I think they're exciting lots of next steps, because this is an initial work

47:34.160 --> 47:39.360
and there's things like speeding up the optimization. It's been a lot of recent work and

47:39.360 --> 47:42.800
speeding up neural radius field training for images. And I think a lot of that can be plugged in.

47:44.560 --> 47:49.200
And how do you synthesize the formable objects? How do you bring a human in the loop so they

47:49.200 --> 47:56.160
can provide feedback partway through training? All kinds of stuff to tackle in making this more

47:56.160 --> 48:07.280
of a practical system for 3D artists. And would you like me to share the collab? I guess we're at

48:07.280 --> 48:11.680
time. Yeah, please go ahead. That would be great. Thank you so much.

48:19.280 --> 48:23.040
So this is the collab notebook. You can find it from the project website.

48:25.040 --> 48:28.240
It is a compact implementation.

48:28.400 --> 48:39.680
The system will run faster on GPU than on TPU in the collab notebook. But for all of our experiments,

48:39.680 --> 48:46.320
we use TPU. Some helpers are imported from our library. So if you want to hack on some

48:46.320 --> 48:51.840
of the low level primitives, you can fork our library or kind of copy those helpers into the

48:51.840 --> 48:56.640
notebook. But the main way you'll interface with this collab notebook is by adjusting the quality

48:56.640 --> 49:06.720
settings here. So in particular, edit the query. Here I've filled in a high quality 3D

49:06.720 --> 49:13.920
render of Jenga Tower. And you can select the clip checkpoint you want to use. Clip bit B16

49:13.920 --> 49:18.800
is used in most of our experiments. There's also an internal Google model that's not available here.

49:19.920 --> 49:24.880
But you can scale down if you're running out of memory to either the B32 or ResNet 50.

49:26.880 --> 49:31.680
Choose the number optimization iterations. I think at least 1000 is necessary.

49:33.520 --> 49:37.600
But more will add more detail. Consider the rendering width and then this is the number

49:37.600 --> 49:47.200
of data augmentation. And then run training. So here's an example of the training run I've already

49:47.200 --> 49:52.480
run in the notebook for that prompt, a high quality 3D render of Jenga Tower. It won't exactly

49:52.480 --> 49:55.920
match the result that was shown in the slides because the version of the collab notebook could

49:56.000 --> 50:02.640
scale down. But over the course of 2000 iterations of optimization, you can see the different

50:02.640 --> 50:08.240
learning curves. This is the total loss that's being optimized. Clip's cosine similarity,

50:08.240 --> 50:12.720
negative cosine similarity is improving. So this means that the renderings of the object

50:12.720 --> 50:16.320
are becoming more and more consistent with the given caption over time.

50:19.040 --> 50:23.280
And the transmission is regularization here. This is showing what is the average transparency

50:23.360 --> 50:33.920
of pixels in the scene. And in this plot at the bottom, the collab notebook will ride out

50:33.920 --> 50:44.720
renderings periodically every, I believe, 100 iterations. So at the beginning, the scene is

50:44.720 --> 50:50.720
low density, essentially empty. And then over time, some content will emerge from the optimization.

50:51.520 --> 50:57.120
And then that's refined and sharpened over time. The camera's moving around. So the camera's being

50:57.120 --> 51:03.680
randomly sampled around the object. And that's why the scene is rendered from different perspectives.

51:04.800 --> 51:10.640
And then finally, the collab notebook renders out a video, 48 frames. And this is the result.

51:12.400 --> 51:18.560
On the GPU that collab gave me here a P100, the optimization I think took about six, seven minutes.

51:21.680 --> 51:25.360
So hopefully you can get some cycles in.

51:28.800 --> 51:34.160
In the run training section, it says if you run out of memory, tweak the configuration options

51:34.160 --> 51:41.040
above. What do you recommend changing? Yeah, that's a good question. So I think

51:41.040 --> 51:48.880
you can change this clip at B16. I would try to clip B32. There's also on the first import

51:48.880 --> 51:52.400
in NVIDIA SMI printout. And so you can look at how much memory is available.

51:53.040 --> 52:01.200
Sometimes it's worth retrying multiple times to get a larger GPU. This P160

52:01.200 --> 52:04.960
gigabyte I think you won't get without collab premium, which is about $10 a month.

52:06.560 --> 52:13.120
But you think you can get 15 gigabyte T4 GPUs for free. Sometimes the collab will give you an 11

52:13.120 --> 52:18.560
gigabyte GPU that might not be enough. If you can tweak the configuration parameters,

52:18.560 --> 52:24.080
I would try reducing this number of samples. So this is the number of points along each array

52:24.080 --> 52:32.320
that is sampled. And that affects the batch size. So the render width, the batch size scales

52:32.320 --> 52:36.720
quadratically with the render width because we're rendering got square images. And then the num

52:36.720 --> 52:42.720
samples the batch size to the MLP scales linearly. So you could reduce this down to 32 even at the

52:42.720 --> 52:49.600
lowest. B32 will use less memory than B16. So this relates to the patch size and the vision

52:49.600 --> 52:57.200
transformer clip encoding. And then if you want to scale down even more, you can change the number

52:57.200 --> 53:09.680
of data augmentations per iteration, maybe down to two.

53:18.000 --> 53:20.720
Oh, Ben says that you can't retry for a better GPU.

53:21.600 --> 53:29.120
That's unfortunate. I mean, I don't know whether MIT has like a shared GPU cloud, but you can

53:29.120 --> 53:35.440
also just download this IPIND and run it on your like Jupyter notebooks, post it on some MIT compute.

53:37.040 --> 53:43.200
And it will paralyze across multiple GPUs.

53:50.880 --> 53:56.640
Cool. And have you taken any more questions that you have?

53:56.640 --> 54:03.840
Excellent. Thank you so much. Maybe at this point I'll stop recording and if students have more

54:03.840 --> 54:05.200
questions, we can...

