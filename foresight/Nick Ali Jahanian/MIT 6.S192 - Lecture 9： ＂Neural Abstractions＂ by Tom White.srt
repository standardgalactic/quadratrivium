1
00:00:00,000 --> 00:00:05,240
Hello, everyone. Welcome back to your course Learning for Arts,

2
00:00:05,240 --> 00:00:09,360
Aesthetics, and Creativity. Our specialist speaker, Tom

3
00:00:09,360 --> 00:00:15,040
White, is here today to tell us about his exploration in art

4
00:00:15,040 --> 00:00:22,120
and creativity in AI. He is an artist who actually creates

5
00:00:23,080 --> 00:00:31,120
art with computers and AI, and I should let him to tell us more

6
00:00:31,120 --> 00:00:34,600
about his background. But he has done very interesting and

7
00:00:34,600 --> 00:00:38,680
fascinating work. And I think that he also has some new

8
00:00:39,080 --> 00:00:46,800
galleries about what he has created his journey of arts. So

9
00:00:46,800 --> 00:00:51,040
please, Tom, let us know what you are up to, what you're doing.

10
00:00:52,200 --> 00:00:55,560
Sure. Thank you, Ali. I really appreciate you letting me

11
00:00:55,560 --> 00:01:01,440
participate. I've often turned down talking invitations, but I'm

12
00:01:01,440 --> 00:01:05,080
an MIT alum. I really appreciate, I like your research,

13
00:01:05,080 --> 00:01:07,680
and so I appreciate your invitation. But I also really

14
00:01:07,680 --> 00:01:13,080
like IEP. When I look back at MIT, that was like the IEP courses

15
00:01:13,080 --> 00:01:15,840
that I took were really great. And so I can tell your students

16
00:01:15,840 --> 00:01:18,640
that you might not remember everything you're doing at MIT,

17
00:01:18,680 --> 00:01:20,880
but I'm pretty sure you're going to remember some of your great

18
00:01:20,880 --> 00:01:26,680
IEP courses. So my background is fairly eclectic, as you might

19
00:01:26,680 --> 00:01:30,680
imagine. I was undergraduate in math, and I went to the media

20
00:01:30,680 --> 00:01:35,560
lab about 20 years ago at MIT. And I was part of a group that

21
00:01:35,560 --> 00:01:40,440
was doing sort of incorporating graphic design into programming.

22
00:01:40,760 --> 00:01:45,120
This was sort of exploring, exploring that space. And so

23
00:01:46,600 --> 00:01:48,960
I'll talk a little bit in my talk about about some of the

24
00:01:48,960 --> 00:01:53,480
precedents that we did there. But I was coming into that sort of

25
00:01:53,520 --> 00:01:57,080
looking at ways that people could think more about coding as a

26
00:01:57,080 --> 00:02:03,720
creative discipline. There's a lot of tools now that didn't

27
00:02:03,720 --> 00:02:06,240
exist in, and there's even the idea of creative coding. And a

28
00:02:06,240 --> 00:02:12,400
lot of that came out of the work that we did in our group. After

29
00:02:12,400 --> 00:02:16,720
leaving MIT, I sort of went off into industry for many years. I

30
00:02:16,720 --> 00:02:21,840
was always interested in AI and art and the sort of intersection

31
00:02:21,840 --> 00:02:24,960
of those. But it was kind of the AI winner. And it was, you know,

32
00:02:25,200 --> 00:02:29,560
I got out of school, went into industry, but when deep learning

33
00:02:29,560 --> 00:02:32,840
started getting exciting, you know, maybe about five to 10 years

34
00:02:32,840 --> 00:02:36,000
ago, that's when I re-vectored back into space. And so I found

35
00:02:36,320 --> 00:02:41,760
I stuff back in academia and have a wearable hat. So I, you know,

36
00:02:41,760 --> 00:02:45,240
I have my own research students here, and I'll speak a little bit

37
00:02:45,280 --> 00:02:48,680
of my talk about some of the research we do. So there's like

38
00:02:48,680 --> 00:02:51,360
Rebecca was saying, there's a lot of number of practical tools

39
00:02:51,360 --> 00:02:55,240
you can build that will help other people use the medium

40
00:02:55,240 --> 00:02:58,320
creatively. And I do some of that research. But as you alluded

41
00:02:58,320 --> 00:03:02,040
to on the side, I also have my own separate arts practice. And

42
00:03:02,160 --> 00:03:04,520
most of my talk is going to be about that. Hopefully that'll

43
00:03:04,520 --> 00:03:07,240
be a good, I think everyone's art practice is different. But I

44
00:03:07,240 --> 00:03:10,360
think seeing seeing that might be hopefully inspirational for

45
00:03:10,360 --> 00:03:13,560
some students that are wondering what that's about or how they

46
00:03:13,560 --> 00:03:15,360
might one day get into something like that.

47
00:03:16,800 --> 00:03:20,080
Yeah, definitely. That would be actually very great. I think

48
00:03:20,080 --> 00:03:28,520
that maybe, yeah, I think you and also work off the work that

49
00:03:28,520 --> 00:03:33,280
Rebecca details give us some taste about what it really means

50
00:03:33,280 --> 00:03:38,160
from the art practitioners as well as, you know, the computer

51
00:03:38,160 --> 00:03:40,320
scientists. So please go ahead.

52
00:03:41,320 --> 00:03:46,320
Okay, I will try to share my slides and we'll get started. Okay,

53
00:03:46,320 --> 00:03:50,560
so I will briefly just reintroduce myself for the video. So my

54
00:03:50,560 --> 00:03:54,200
name is Tom White. You can find me online on my handle trip

55
00:03:54,200 --> 00:03:58,520
net. And there's my website trip.net. And as I said, I'm an

56
00:03:58,520 --> 00:04:02,160
artist and lecturer at Victoria University of Wellington School

57
00:04:02,480 --> 00:04:12,080
of Design. And in my work here, I teach classes on creative

58
00:04:12,080 --> 00:04:14,200
coding. So I teach primarily in the context of an art and

59
00:04:14,200 --> 00:04:18,720
design school to students that are interested in, it might be

60
00:04:18,720 --> 00:04:21,200
that they're interested in special effects, or that they're

61
00:04:21,200 --> 00:04:24,880
interested in web design, but they want to sort of incorporate

62
00:04:26,240 --> 00:04:28,880
programming into their into their tool set, because there's a

63
00:04:28,880 --> 00:04:32,680
lot of capabilities and things that have a lot. So in the past,

64
00:04:32,680 --> 00:04:35,960
where someone might, you know, learn charcoal drawing or

65
00:04:35,960 --> 00:04:39,120
search specific techniques, more and more today, students want to

66
00:04:39,120 --> 00:04:42,040
learn programming. So I teach programming, but not computer

67
00:04:42,040 --> 00:04:45,960
science angle, kind of creative coding angle. I also have

68
00:04:45,960 --> 00:04:49,000
research which I do with my graduate students. And I'll touch

69
00:04:49,000 --> 00:04:52,000
upon that briefly in the talk as well, where we make creative

70
00:04:52,000 --> 00:04:55,840
tools using some of these, these technologies and in that

71
00:04:55,840 --> 00:05:02,880
context, I also have a workshop at the NeurIPS conference

72
00:05:03,440 --> 00:05:06,160
creativity and design that I do with other co-hosts, where we

73
00:05:06,160 --> 00:05:09,400
post a lot of this research. What my talk today is about is

74
00:05:09,400 --> 00:05:12,120
about a lot of my artwork, and specifically the artwork that I've

75
00:05:12,120 --> 00:05:16,240
been doing the last three to five years. And I'm going to sort

76
00:05:16,240 --> 00:05:18,720
of go through and give you a background of myself and then

77
00:05:18,720 --> 00:05:22,800
talk about that a little bit. Let's see if I can get my first

78
00:05:23,040 --> 00:05:26,840
slide. There we go. So, so this is the outline of the talk. I'm

79
00:05:26,840 --> 00:05:29,960
going to give basically a three minute version of the talk, the

80
00:05:29,960 --> 00:05:32,920
TLDR, like what's the point of this talk, so you can sort of

81
00:05:32,960 --> 00:05:37,080
understand it all at once what this is. Then I'm going to back

82
00:05:37,080 --> 00:05:39,680
up and give you a little bit more background about where I'm

83
00:05:39,680 --> 00:05:45,000
coming from, my background in MIT, and since then, I'm going to

84
00:05:45,000 --> 00:05:47,160
spend a little bit of time talking about the precedents at

85
00:05:47,160 --> 00:05:51,880
art. So when you're doing artwork, similar to, you know, if

86
00:05:51,920 --> 00:05:54,280
you're writing a research paper, you have your references at the

87
00:05:54,280 --> 00:05:57,200
end, and that contextualizes and talks about how you're

88
00:05:57,200 --> 00:06:00,040
building on things in the past, I think it's important. In

89
00:06:00,040 --> 00:06:02,600
art, I think a lot of times it's implicit to be here, I'm

90
00:06:02,600 --> 00:06:05,400
going to make it a little bit more explicit about what my

91
00:06:05,400 --> 00:06:08,720
precedents are, and what I find inspirational, and what I'm

92
00:06:08,720 --> 00:06:14,240
trying to, what I'm using as reference points. Then I'm going

93
00:06:14,240 --> 00:06:16,360
to get into the meat of the talks. The meat of the talk is

94
00:06:16,360 --> 00:06:20,160
really, or the core of my talk here is about AI representation

95
00:06:20,160 --> 00:06:24,080
and abstraction, and specifically, my investigations

96
00:06:24,080 --> 00:06:28,240
into kind of exploring what these neural net vision systems,

97
00:06:28,400 --> 00:06:33,000
what their representations are, and how to convey that in an

98
00:06:33,000 --> 00:06:37,240
artistic context. I will talk a little bit about other AI art

99
00:06:37,240 --> 00:06:41,360
approaches, including, you know, other peers that I have that

100
00:06:41,360 --> 00:06:43,960
are in this space, and I'll touch a little bit about my

101
00:06:43,960 --> 00:06:48,400
research there. And then I'll close out by talking about the

102
00:06:48,800 --> 00:06:53,400
sort of like some of the implications of the artwork. So

103
00:06:54,000 --> 00:06:59,160
what is, what happens when the systems, when we're using these

104
00:06:59,160 --> 00:07:03,880
systems more and more as our kind of auxiliary eyes? So that's

105
00:07:03,880 --> 00:07:07,400
the outline of the talk. This is the core of it. So this is the

106
00:07:07,760 --> 00:07:11,840
sort of the summary that my ideas of our artwork is that

107
00:07:11,840 --> 00:07:16,000
machines have their own way of seeing. So they, they see things

108
00:07:16,880 --> 00:07:21,600
they're very, they're very accurate in classifiers. But I

109
00:07:21,600 --> 00:07:25,000
think they also, importantly, have slightly different ways of

110
00:07:25,000 --> 00:07:29,080
seeing the world. And so what I'm trying to do in my artwork is

111
00:07:29,080 --> 00:07:34,080
trying to expose that to a wider audience and trying to

112
00:07:34,080 --> 00:07:37,600
investigate how it is that these machines see the world and how

113
00:07:37,600 --> 00:07:41,640
it might be the same or different as us. So as a sort of a

114
00:07:41,640 --> 00:07:44,960
corollary of that, because they see things differently, I'm, I

115
00:07:45,000 --> 00:07:48,200
believe we can create or this kind of core and buy machines. In

116
00:07:48,200 --> 00:07:51,400
other words, you could use the machine perception to create

117
00:07:51,400 --> 00:07:54,400
different types of art, which actually the machines themselves

118
00:07:54,400 --> 00:08:00,240
have some opinions about. So I'll talk a little bit more about

119
00:08:00,240 --> 00:08:04,040
what that means. And so the summary there is that through art,

120
00:08:04,040 --> 00:08:07,560
we can actually appreciate ways the machines can perceive the

121
00:08:07,560 --> 00:08:12,040
world. So similar to you might encounter a new culture that you

122
00:08:12,080 --> 00:08:15,920
hadn't before, and you might explore the art of that culture.

123
00:08:16,080 --> 00:08:20,840
I'm trying to explore the art that's created when we introduce

124
00:08:20,840 --> 00:08:27,760
machine perception into into the process. So I'm very, so the

125
00:08:27,760 --> 00:08:30,600
one line version of that is I'm interested in how machines read

126
00:08:30,600 --> 00:08:34,080
images. So that's kind of the point of what I'm doing. Here's

127
00:08:34,080 --> 00:08:38,560
three prints that I've done the one on the left is an eye and

128
00:08:38,560 --> 00:08:40,840
that was created from a data set of eyes and I'll talk a little

129
00:08:40,840 --> 00:08:45,120
bit about that later. The one all the way on the right is it's

130
00:08:45,120 --> 00:08:47,600
actually a face and that was made not so much for a class

131
00:08:47,600 --> 00:08:50,480
environment for facial recognition system. The first step in

132
00:08:50,480 --> 00:08:55,720
facial recognition is a detector. And so that was made to look

133
00:08:55,720 --> 00:08:59,600
like a face to a face detection system. And the one in the

134
00:08:59,600 --> 00:09:02,120
center here, which I'll make a little bit bigger is a screen

135
00:09:02,120 --> 00:09:06,960
print of two chickens. And so this is all of these are based on

136
00:09:07,000 --> 00:09:09,920
pure images. And I'll get as you can imagine more into the

137
00:09:09,960 --> 00:09:12,880
details of how this happens as I go through the paper or go

138
00:09:12,880 --> 00:09:17,280
through the talk. But this is based on a data set of many

139
00:09:17,320 --> 00:09:21,960
actual images of chickens, the system created this this output.

140
00:09:22,520 --> 00:09:25,920
And we can actually turn it around and we can show this to

141
00:09:25,920 --> 00:09:28,560
the system again and look at its imagination of what it

142
00:09:28,560 --> 00:09:33,120
visualizes. And we get kind of this so it can actually introduce

143
00:09:33,120 --> 00:09:36,280
diagnostics into the process and say, Well, what is it that the

144
00:09:36,280 --> 00:09:39,720
computer sees when they look at this print? And that is a

145
00:09:39,720 --> 00:09:43,800
some kind of indication of the richer inner world, these neural

146
00:09:43,800 --> 00:09:49,720
networks. So that is my, I guess, you know, three minute version

147
00:09:49,720 --> 00:09:52,560
of the whole talk. And now we're going to sort of back up and go

148
00:09:52,560 --> 00:09:55,560
through that and a little bit more, a little bit more detail.

149
00:09:56,080 --> 00:09:59,360
And I'll start off talking a little bit more about myself. So

150
00:09:59,360 --> 00:10:03,920
this is made in my studio, currently, where I do my

151
00:10:03,920 --> 00:10:08,720
printing. And I'm here in Wellington, New Zealand, I'm

152
00:10:08,720 --> 00:10:11,960
going to go back and talk about not everything I've done, but

153
00:10:11,960 --> 00:10:16,080
maybe some of course, precedence along the way for getting to

154
00:10:16,240 --> 00:10:19,000
sort of just this stage of my art making.

155
00:10:20,600 --> 00:10:25,760
25 years ago at SIGGRAPH, I did in sort of using machine

156
00:10:25,760 --> 00:10:28,760
learning techniques of the time artificial evolution to make

157
00:10:28,760 --> 00:10:33,600
these real time video filters that processed video and then

158
00:10:33,600 --> 00:10:36,440
showed that on a screen. And you could also manipulate the

159
00:10:36,480 --> 00:10:39,200
evolution process to create different filters.

160
00:10:40,880 --> 00:10:46,760
In 1998, I finished my master's thesis at MIT. And so, like I

161
00:10:46,760 --> 00:10:50,520
said, I was in a group at MIT, John Midas Aesthetics and

162
00:10:50,520 --> 00:10:55,840
Computation Group. I was there with many kind of inspired

163
00:10:56,240 --> 00:11:00,320
people that were coming in from both design and computer

164
00:11:00,320 --> 00:11:04,560
programming, software engineering. And I was interested

165
00:11:04,560 --> 00:11:07,200
at the time on better human computer human interfaces. So I

166
00:11:07,200 --> 00:11:11,880
built a custom hand interface because I was interested in

167
00:11:11,880 --> 00:11:14,720
multi-touch. So multi-touch wasn't really a big idea in the

168
00:11:14,720 --> 00:11:18,760
90s. So I had to, there wasn't any interfaces that did that. So I

169
00:11:18,760 --> 00:11:22,520
built a camera optics based interface. So it's this liquid

170
00:11:22,520 --> 00:11:25,280
pad. And when you pushed on it, you would get this handprint

171
00:11:25,760 --> 00:11:28,160
that you see in the bottom right. And then on top of that, I

172
00:11:28,160 --> 00:11:32,720
built various interfaces, for example, sliding left to right

173
00:11:32,720 --> 00:11:39,640
to move, to move something, or there's various very different

174
00:11:39,640 --> 00:11:43,480
ways you could you could interact based on having more than a

175
00:11:43,480 --> 00:11:45,960
mouse. So I think this is common sort of ideas now, but this

176
00:11:45,960 --> 00:11:49,280
was something that we did in the Media Lab and I did for my

177
00:11:49,280 --> 00:11:53,280
thesis. But maybe one of the, one of the one of the other

178
00:11:53,280 --> 00:11:56,960
things at this time, as I did another art project, which is

179
00:11:56,960 --> 00:11:58,880
still pretty well known, which is called Stream of

180
00:11:58,880 --> 00:12:04,400
Consciousness, which in which I incorporated this handpad, which

181
00:12:04,400 --> 00:12:08,000
could measure sort of pressure across the whole pad into this

182
00:12:08,160 --> 00:12:12,440
interactive exhibit where words were flowing through a stream,

183
00:12:12,480 --> 00:12:14,880
and it was called Stream of Consciousness. One sort of

184
00:12:14,880 --> 00:12:18,280
interesting footnote about this is that as the words were going

185
00:12:18,280 --> 00:12:20,760
through, you could push on a word, and it would spring out

186
00:12:20,760 --> 00:12:24,760
related words. So if you were looking at the word, you know,

187
00:12:25,280 --> 00:12:27,920
university, and you pushed on it, it might say faculty or

188
00:12:27,960 --> 00:12:31,960
school, or academia or something like that. And we used

189
00:12:31,960 --> 00:12:35,520
wordnet at the time, that was part of the programming idea to

190
00:12:35,520 --> 00:12:39,200
kind of find out interesting related words, first anonyms and

191
00:12:39,200 --> 00:12:42,080
synonyms and kind of its sense that that it uses, which is the

192
00:12:42,080 --> 00:12:46,400
basis for ImageNet, which is a lot of the artwork I do today. So

193
00:12:46,400 --> 00:12:49,760
this was kind of a, again, using AI techniques at the time,

194
00:12:50,200 --> 00:12:55,480
incorporating it in these, these interactive exhibitions. But

195
00:12:55,480 --> 00:12:58,120
one of the, one of the interesting legacies, and I think

196
00:12:58,120 --> 00:13:02,320
another bit that feeds into my, my current practice is that one

197
00:13:02,320 --> 00:13:04,560
of the interesting legacies of this group is that we made a lot

198
00:13:04,560 --> 00:13:08,520
of interesting sort of core software that enabled a lot of

199
00:13:08,520 --> 00:13:11,120
people to do different things. So there was a handful of

200
00:13:11,120 --> 00:13:16,320
researchers, and I, and Ben Fry, and Jared Chiffman were

201
00:13:16,320 --> 00:13:19,200
students in the group, and we're responsible for creating and

202
00:13:19,200 --> 00:13:23,920
maintaining this core library called ACU. And the library was

203
00:13:23,920 --> 00:13:28,600
so that basically we could conduct our experiments. And it was

204
00:13:28,600 --> 00:13:33,200
constructed such that it was easy to do kind of a sketch, a

205
00:13:33,200 --> 00:13:36,440
software sketch. And the reason I bring this up is that this

206
00:13:36,440 --> 00:13:41,600
software was the basis for other systems that have come since.

207
00:13:41,600 --> 00:13:46,360
So Ben, who worked on this and Casey later adapted some of

208
00:13:46,360 --> 00:13:50,520
these ideas and made a version of this called Processing, which

209
00:13:50,560 --> 00:13:55,760
is actually quite popular still as a creative toolkit. And then

210
00:13:55,800 --> 00:13:59,240
later, another group of students took actually the code for

211
00:13:59,240 --> 00:14:04,200
this and made a new toolkit called Open Frameworks, which

212
00:14:04,360 --> 00:14:07,440
uses a lot of a lot of some of the code in this ends up in Open

213
00:14:07,440 --> 00:14:09,840
Frameworks. And then they, of course, built upon it and built

214
00:14:09,840 --> 00:14:17,240
a community around it. But many of the conventions and cliches of

215
00:14:17,240 --> 00:14:23,480
this programming style kind of live on today. And this is, it's

216
00:14:23,480 --> 00:14:25,800
interesting as it alludes to my current work, because what we

217
00:14:25,800 --> 00:14:28,160
were doing at the time, we were just trying to figure out how

218
00:14:28,160 --> 00:14:33,080
to make it easy for people to create sketches or drawings on

219
00:14:33,080 --> 00:14:36,240
the computer. And I feel that that's a lot of what I'm doing

220
00:14:36,280 --> 00:14:39,600
currently is just my audience is no longer other people, it's

221
00:14:39,600 --> 00:14:44,400
actually machine learning processes. And then I did other

222
00:14:44,400 --> 00:14:46,960
things and worked in industry, but for the purpose of this talk,

223
00:14:47,000 --> 00:14:50,320
we'll just, you know, chalk that off to the AI winner. And I'll

224
00:14:50,320 --> 00:14:53,920
come back to that in a little bit. So there's, after that, I

225
00:14:53,920 --> 00:14:58,960
graduated from school, I did some other things. I'm going to

226
00:14:58,960 --> 00:15:02,000
also open the chat over here in case other people had feedback

227
00:15:02,000 --> 00:15:05,280
that I can secretly see. Okay, so now I'm going to talk about

228
00:15:05,280 --> 00:15:11,160
precedence and art and and some of the background looking at it

229
00:15:11,160 --> 00:15:14,040
from a slightly different angle of what it is I'm trying to do.

230
00:15:14,480 --> 00:15:17,520
I'm going to go all the way back to 1927. And there's an artist

231
00:15:17,520 --> 00:15:21,600
called Stuart Davis that I admired his technique. And what he

232
00:15:21,600 --> 00:15:27,240
was doing, or what he did for kind of his central core

233
00:15:27,240 --> 00:15:30,200
inspiration for his work is that there was one year that he did

234
00:15:30,200 --> 00:15:32,840
the egg beater series. And the egg beater series was his attempt

235
00:15:32,840 --> 00:15:36,240
to look at the world in a new way. And the way he did that, his

236
00:15:36,240 --> 00:15:39,120
technique for that was he took some common everyday objects, he

237
00:15:39,120 --> 00:15:43,560
took an electric fan, a rubber glove and an egg beater. And he

238
00:15:44,280 --> 00:15:48,000
nailed those to a tabletop and basically forced himself to paint

239
00:15:48,000 --> 00:15:51,000
those things over and over and over until he thought he was

240
00:15:51,000 --> 00:15:55,720
doing something different and new. His, in his own words, what

241
00:15:55,720 --> 00:15:58,400
he said his intent was in doing this was to strip a subject

242
00:15:58,400 --> 00:16:02,440
down to the real physical source of its stimulus. So here's one

243
00:16:02,440 --> 00:16:05,880
of his egg beater works where he's, you know, taking these

244
00:16:05,880 --> 00:16:10,320
common objects and trying to find something new in them. And I

245
00:16:10,560 --> 00:16:14,200
think that that's some of the that alludes to sort of some of

246
00:16:14,200 --> 00:16:16,600
the work that I'm doing. And I'll sort of touch on that in a

247
00:16:16,600 --> 00:16:22,080
little bit. Another precedent for me was Harold Cohen, who, for

248
00:16:22,080 --> 00:16:25,600
many years, experimented with generative drawing systems, and

249
00:16:25,640 --> 00:16:29,440
is arguably the first kind of AI artist. He came at this from a

250
00:16:29,440 --> 00:16:31,800
different angle. He is a very successful visual artist and

251
00:16:31,800 --> 00:16:37,600
painter. And he decided in the early 70s that he wanted to go

252
00:16:37,680 --> 00:16:41,000
into programming into an artificial intelligence to see if he

253
00:16:41,000 --> 00:16:47,320
could codify some of his ideas on, on mark making in a, in a

254
00:16:47,320 --> 00:16:52,240
formal way. So here's an example of one of his programs and he

255
00:16:52,600 --> 00:16:55,840
initially set out to build an autonomous program. And he called

256
00:16:55,840 --> 00:16:58,520
the program Aaron and Aaron would kind of encompass all the

257
00:16:58,520 --> 00:17:05,080
ideas he had about how to generate visual works. So I

258
00:17:05,120 --> 00:17:09,040
visited Harold a couple of times and this is me visiting him.

259
00:17:09,120 --> 00:17:12,640
And when he's working on some of his later works, and I think

260
00:17:12,640 --> 00:17:17,320
it's also interesting to discuss like, even though he had these

261
00:17:17,320 --> 00:17:21,680
ideas that the the artworks were being creative autonomously

262
00:17:21,680 --> 00:17:25,360
initially, later in life, he was working more as a collaborator

263
00:17:25,360 --> 00:17:29,320
with these systems. So he was sort of working a little bit or

264
00:17:29,320 --> 00:17:32,920
finding ways where it was more of a co creation process.

265
00:17:35,440 --> 00:17:40,600
But across all of his, all of his different phases, his core

266
00:17:40,600 --> 00:17:43,080
question across this, and this is what I kind of share his

267
00:17:43,080 --> 00:17:46,840
inspiration is, what is what he his question was, what is an

268
00:17:46,840 --> 00:17:51,120
image? And the way he didn't mean that like technically what is

269
00:17:51,120 --> 00:17:53,720
an image, but he meant like, what is the minimum condition

270
00:17:53,960 --> 00:17:57,640
under which a set of marks on a page functions as an image or

271
00:17:57,640 --> 00:18:02,920
conveys meaningful information. This is a sketch he had from one

272
00:18:02,960 --> 00:18:05,720
of his papers. And I would kind of rephrase this in a machine

273
00:18:05,720 --> 00:18:10,560
learning context is saying, what is it that in what is it that

274
00:18:10,560 --> 00:18:13,800
images could do kind of central to representation and

275
00:18:13,800 --> 00:18:17,360
extraction, like what makes simplification work? How is it

276
00:18:17,360 --> 00:18:21,120
that simple drawings can be evocative and seem to stand for

277
00:18:21,360 --> 00:18:27,720
something else? So I think that this is kind of a core, a core

278
00:18:28,360 --> 00:18:33,360
concern of Harold's that that I also share. And then one last

279
00:18:33,400 --> 00:18:37,160
kind of precedent and art that I think is important for me is

280
00:18:37,160 --> 00:18:41,240
just talking a little bit about pop artists. This is not so much

281
00:18:41,240 --> 00:18:47,680
as a from the, the content side, but from the form side, this

282
00:18:47,680 --> 00:18:50,560
is actually, I share some techniques with many of the

283
00:18:50,560 --> 00:18:55,680
pop artists like Andy Warhol, Roy Lichtenstein, and how I

284
00:18:55,720 --> 00:18:58,920
execute my artwork. So for anyone that's not familiar with

285
00:18:58,920 --> 00:19:03,320
screen printing, if I'm making this chicken image over here on

286
00:19:03,320 --> 00:19:06,240
the right, that's actually printed, but it's printed from

287
00:19:06,240 --> 00:19:09,600
two layers of ink. And so these are essentially stencils that I

288
00:19:09,600 --> 00:19:14,400
have to mix inks for, and then put onto the page. So what I'm

289
00:19:14,400 --> 00:19:17,400
showing on the left are the masks that generate the

290
00:19:17,400 --> 00:19:22,320
stencils to give you the image on the right. So the stencils

291
00:19:22,320 --> 00:19:24,840
themselves get burned, transferred onto screens. These

292
00:19:24,880 --> 00:19:28,800
are physical kind of stencil, it's like a stencil, except it

293
00:19:28,800 --> 00:19:31,600
can have holes on it, there's a mesh there. And then you just

294
00:19:31,600 --> 00:19:36,160
sort of pour ink across the, across this, this screen, and

295
00:19:36,160 --> 00:19:39,200
you smash some ink through it. And that's the kind of physical

296
00:19:39,200 --> 00:19:43,520
process of, of making these, these prints. Here's a little

297
00:19:43,520 --> 00:19:45,800
slideshow I have where I'm starting off with a blank canvas.

298
00:19:45,800 --> 00:19:48,960
It's actually a blank, we base coated a color onto it, we put

299
00:19:48,960 --> 00:19:52,920
a screen down, I put some ink on top of it, I spread the ink out

300
00:19:52,920 --> 00:19:55,800
that's called the flood, then you smash the ink through and you

301
00:19:55,800 --> 00:19:58,640
get a layer. And then the more colors you want, the more layers

302
00:19:58,640 --> 00:20:01,240
you have to do that. So for the second color, you have to do

303
00:20:01,240 --> 00:20:04,920
another screen and another layer. For the third color, it's

304
00:20:04,920 --> 00:20:10,000
another screen, that's me pushing ink through that layer. And

305
00:20:10,000 --> 00:20:14,280
then there's a fourth color there. Put the ink on the screen,

306
00:20:14,280 --> 00:20:17,040
spread it out and push it through. So just kind of

307
00:20:17,080 --> 00:20:23,400
emphasizing that there's a kind of a, a specific technique I'm

308
00:20:23,400 --> 00:20:27,880
using that has, you know, precedent for getting these

309
00:20:27,880 --> 00:20:30,960
are acrylic on canvas works or acrylic on paper, but it's

310
00:20:31,000 --> 00:20:33,360
instead of a brush technique, it's a screen printing technique.

311
00:20:35,960 --> 00:20:38,400
And here's the result of some of that. So this is kind of just

312
00:20:38,400 --> 00:20:42,000
showing you what happens at the end is that I have these prints,

313
00:20:42,760 --> 00:20:46,240
and the prints end up kind of, you know, in an exhibition or in

314
00:20:46,400 --> 00:20:53,800
some type of art context. So that's kind of the precedence, the

315
00:20:53,800 --> 00:20:57,680
art precedence. I think it's useful kind of now to talk about

316
00:20:57,680 --> 00:21:01,320
more what this artwork is and what it is I'm trying to say with

317
00:21:01,320 --> 00:21:06,280
this with my art pieces. So I'm going to get into the sort of

318
00:21:06,280 --> 00:21:09,400
the main section here, which is the AI representation and

319
00:21:09,400 --> 00:21:12,280
abstraction, like how is it I actually create these, these

320
00:21:12,280 --> 00:21:15,840
works and what is it that they stand for. So just going back

321
00:21:15,880 --> 00:21:20,640
to revisiting Stuart Davis. So in my mind, Stuart Davis spent a

322
00:21:20,640 --> 00:21:24,640
year basically learning how to perceive and represent familiar

323
00:21:24,640 --> 00:21:28,600
objects in new ways. So he, you know, imagine going in every

324
00:21:28,600 --> 00:21:31,560
day and staring at these same three or four objects of

325
00:21:31,560 --> 00:21:34,840
rubber glove and electric fan until finally you're seeing

326
00:21:34,920 --> 00:21:37,000
there have been ways you hadn't before, and then you're trying

327
00:21:37,000 --> 00:21:41,360
to put that on the page. And my kind of core question is, is

328
00:21:41,360 --> 00:21:45,360
can we similarly use computer vision to introduce new ways of

329
00:21:45,360 --> 00:21:47,760
perceiving or representing familiar objects? So instead of

330
00:21:47,760 --> 00:21:51,920
going in kind of going to this meditative trance, can we kind

331
00:21:51,920 --> 00:21:55,080
of look at things through the eyes of the computer vision

332
00:21:55,080 --> 00:21:59,360
systems and see if that causes us to see familiar things in new

333
00:21:59,360 --> 00:22:04,800
ways. So appropriately enough, one of my first experiments in

334
00:22:04,800 --> 00:22:08,480
this was an electric fan. So this is an electric fan training

335
00:22:08,480 --> 00:22:11,640
set. So this is the input into the system where you have

336
00:22:12,640 --> 00:22:15,600
computer vision systems, as you probably know, you know, need

337
00:22:16,120 --> 00:22:19,320
hundreds of thousands of images. So I believe in the image net

338
00:22:19,320 --> 00:22:23,640
electric fan data, there is, you know, over 1000 images of an

339
00:22:23,640 --> 00:22:26,520
electric fan. And this is just quickly paging through some of

340
00:22:26,520 --> 00:22:29,720
those. And then what I did with that is I made a system that

341
00:22:29,720 --> 00:22:33,760
optimized perceptually for creating something that looked

342
00:22:33,760 --> 00:22:38,800
like an electric fan to multiple, multiple trained neural

343
00:22:39,040 --> 00:22:43,000
networks. And so here I'm showing the print that I made. I'm an

344
00:22:43,000 --> 00:22:46,240
electric fan, it was created using a similar technique done in

345
00:22:46,240 --> 00:22:49,960
layers. And this is on the on the right side, these are in the

346
00:22:49,960 --> 00:22:53,800
middle, I guess, is the graph showing what the opinions of

347
00:22:53,800 --> 00:22:58,080
this print are when shown to different networks like Inception

348
00:22:58,080 --> 00:23:02,480
or ResNet or VGG. And they all are very confident that they're

349
00:23:02,480 --> 00:23:06,440
looking at electric fans. So this, this is using techniques

350
00:23:06,440 --> 00:23:11,000
also from like adversarial examples. So that's some that

351
00:23:11,000 --> 00:23:14,480
these are computer search security techniques, where you try

352
00:23:14,480 --> 00:23:17,280
to essentially fool a computer into thinking it's seeing

353
00:23:17,280 --> 00:23:20,600
something. But in my instance, I don't, I'm not tricking it, I'm

354
00:23:20,600 --> 00:23:24,000
actually just trying to make a super stimulus or stimulus that

355
00:23:24,000 --> 00:23:27,800
is most evocative of that category that it knows. And so

356
00:23:27,800 --> 00:23:32,400
this is an example of me trying to see how these computer AI

357
00:23:32,480 --> 00:23:37,200
algorithms receive these common objects. Here's an example of

358
00:23:37,240 --> 00:23:40,440
the the drawing system, sort of in progress, as you can imagine,

359
00:23:40,440 --> 00:23:43,400
it's an optimization, it starts off kind of putting lines on the

360
00:23:43,400 --> 00:23:48,000
page where it thinks they're most needed. And then it's, it kind

361
00:23:48,000 --> 00:23:52,160
of anneals or optimizes over that, and makes the image look

362
00:23:52,160 --> 00:23:56,240
more and more like the target class that it's, it's trying to

363
00:23:56,240 --> 00:23:59,840
make. And one way that I contextualize this and or talk

364
00:23:59,880 --> 00:24:04,280
about this in my writing, is that for me, it kind of inverts

365
00:24:04,280 --> 00:24:07,920
the computer as a tool stereotype. So for for many,

366
00:24:07,920 --> 00:24:11,680
many years, we've seen the computer as kind of a way to

367
00:24:11,680 --> 00:24:15,960
execute someone's vision. So use it down and use Photoshop and

368
00:24:15,960 --> 00:24:19,120
you have an idea and you kind of do it, you know, use the tool

369
00:24:19,120 --> 00:24:21,960
to express your idea. What I'm trying to do here is I'm kind of

370
00:24:21,960 --> 00:24:25,160
inverting that and I've made it a tool that the computer vision

371
00:24:25,160 --> 00:24:30,800
systems themselves can use to create their own visual outputs.

372
00:24:30,960 --> 00:24:35,120
So so the I'm sort of making the tool for the perception

373
00:24:35,120 --> 00:24:41,240
systems. So after I finished some of those, I did a series of

374
00:24:41,240 --> 00:24:44,360
10 of these. And I'm going to talk a little bit about them to

375
00:24:44,360 --> 00:24:48,000
talk about how they're made, but also what machine learning

376
00:24:48,000 --> 00:24:53,280
concepts are kind of bubbling through. So this is this is a

377
00:24:53,280 --> 00:24:56,600
print called binoculars, again, using the sort of same two layer

378
00:24:56,600 --> 00:24:59,960
technique. This is the data set on the right. And then this is

379
00:24:59,960 --> 00:25:02,960
the print on the left. One interesting thing I thought about

380
00:25:02,960 --> 00:25:06,160
the way this print turned out is that it's viewing the binoculars

381
00:25:06,160 --> 00:25:09,560
in three quarter view, which is kind of an interesting angle,

382
00:25:09,560 --> 00:25:12,920
because you can see all of the features pretty easily from from

383
00:25:13,120 --> 00:25:15,600
kind of not instead of a straight on view as much of the

384
00:25:15,600 --> 00:25:20,360
data is. This is a shark specifically a hammerhead shark,

385
00:25:20,360 --> 00:25:25,240
which is one of the categories of the image net. And here it's

386
00:25:25,360 --> 00:25:29,200
done a very few number of strokes to kind of try to represent

387
00:25:29,200 --> 00:25:33,240
that the essence of the shark outlines. In fact, we can kind

388
00:25:33,240 --> 00:25:37,400
of count the strokes. I believe it's about 14 or so. So this is

389
00:25:37,400 --> 00:25:40,960
a little animation which is showing the strokes one by one

390
00:25:40,960 --> 00:25:45,720
just so you can see the number of primitives. So from, you know,

391
00:25:45,720 --> 00:25:47,520
from a programming point of view, you might think if this is

392
00:25:47,520 --> 00:25:51,880
the number of parameters in the space, but this is also kind of

393
00:25:51,960 --> 00:25:54,480
defining the complexity of the image that that it's able to

394
00:25:54,480 --> 00:25:59,360
produce. After the shark, I did a different one, which is iron

395
00:25:59,400 --> 00:26:02,520
again, kind of an interesting view it took it to this kind of

396
00:26:03,560 --> 00:26:06,680
perpendicular view, which is where you might get the most

397
00:26:06,800 --> 00:26:10,720
characteristic view of the iron. The iron uses the same number

398
00:26:10,720 --> 00:26:15,080
of strokes as the shark does. So one fun thing we can do is we

399
00:26:15,080 --> 00:26:17,640
can actually start with the shark. And then we can move the

400
00:26:17,640 --> 00:26:22,160
strokes around so that they create the iron. And I did this

401
00:26:22,200 --> 00:26:26,360
in software. And then at each step, at the end steps, I show

402
00:26:26,360 --> 00:26:28,640
this to the networks to see what they're thinking what what

403
00:26:28,640 --> 00:26:32,440
they believe they're looking at. And on one side here, you can

404
00:26:32,440 --> 00:26:34,560
see the six networks are very convinced they're looking at an

405
00:26:34,560 --> 00:26:38,160
iron. And then we move the strokes around and they're

406
00:26:38,640 --> 00:26:43,640
so they're looking at a hammerhead shark. I did this a

407
00:26:43,640 --> 00:26:46,320
couple of years ago as a demo, there's a lot of debate in the

408
00:26:46,320 --> 00:26:48,440
deep learning community how these models work. And there's a

409
00:26:49,960 --> 00:26:52,320
there's a camp, or there's a group of people that actually

410
00:26:52,520 --> 00:26:57,120
were proposing that deep learning was mainly triggering on

411
00:26:57,120 --> 00:27:00,160
textures and didn't have any global structure information. So I

412
00:27:00,160 --> 00:27:03,000
think this is a pretty compelling counter example that says,

413
00:27:03,000 --> 00:27:05,800
well, no, these deep learning systems might preference

414
00:27:05,800 --> 00:27:09,120
structure or texture or might use texture, but they do seem to

415
00:27:09,120 --> 00:27:13,600
have some global structure information in their vision

416
00:27:13,600 --> 00:27:19,560
systems. Just a couple more of these. This is a cello. Or I

417
00:27:19,560 --> 00:27:25,120
should say this is the the the label cello, which as you can

418
00:27:25,120 --> 00:27:29,320
imagine has not only cellos, but cellists and other things in

419
00:27:29,320 --> 00:27:33,560
the photos that are the used for classification. And it's

420
00:27:33,560 --> 00:27:37,560
interesting to try to not I don't have any I don't have any

421
00:27:37,560 --> 00:27:41,440
special insight into the the drawing that is produced by the

422
00:27:41,440 --> 00:27:43,800
system. So I don't know. People ask me what is it

423
00:27:43,800 --> 00:27:46,720
representing? And I don't know specifically, but I can go into

424
00:27:46,720 --> 00:27:51,280
the data and poke around and take my best guess. So to me, this

425
00:27:51,280 --> 00:27:54,360
cello image looks like this, these kind of characteristic

426
00:27:54,360 --> 00:27:57,800
photos in the data. So you might call this a mode of the data

427
00:27:57,800 --> 00:28:03,080
or a or about 25% of the data has this general shape where you

428
00:28:03,080 --> 00:28:07,120
have a cellist behind the cello playing and I'll point out a

429
00:28:07,120 --> 00:28:10,760
couple of things that I think I see that resemble these training

430
00:28:10,800 --> 00:28:13,760
examples. And I'll point out one thing that's very different. So

431
00:28:13,760 --> 00:28:16,880
you might try to figure out in the 30 seconds that it needs me

432
00:28:16,880 --> 00:28:18,960
to get there, whether you can find the thing that's different in

433
00:28:18,960 --> 00:28:21,960
those. But the things that are the same as I see this like

434
00:28:21,960 --> 00:28:25,560
light colored object kind of looming behind this darker one.

435
00:28:27,280 --> 00:28:31,720
I see that maybe fingers curled around a fretboard, which was

436
00:28:31,720 --> 00:28:35,120
kind of surprising to me. But the one thing that's very

437
00:28:35,120 --> 00:28:38,760
different about the image that it drew and all the cellists that

438
00:28:38,760 --> 00:28:41,120
I saw on the data set is all the cellists in the data set are

439
00:28:41,120 --> 00:28:43,720
right handed. And all these that I just kind of pulled out

440
00:28:43,720 --> 00:28:47,240
randomly are, but improbably the cellist it decided to draw was

441
00:28:47,240 --> 00:28:51,240
left handed. And that's relevant because, you know, I'm

442
00:28:51,240 --> 00:28:53,880
interested in how these computer vision systems see and they're

443
00:28:53,880 --> 00:28:56,160
actually because of the way they're trained, they're blind and

444
00:28:56,160 --> 00:29:00,840
left right symmetry. So because of image augmentation to offer

445
00:29:00,840 --> 00:29:04,040
the data sets, they, they cannot see or they're not aware of

446
00:29:04,040 --> 00:29:08,680
any features that are that are not that are that are dependent on

447
00:29:08,680 --> 00:29:11,480
left right symmetries. And so it chose to kind of invert this

448
00:29:11,480 --> 00:29:13,880
one and draw a left handed challenge, which I thought was

449
00:29:13,920 --> 00:29:18,280
was interesting. And then there's a there's two more. So this is

450
00:29:18,280 --> 00:29:22,160
one that's measuring cup, not too surprising here seeing the

451
00:29:22,160 --> 00:29:25,880
other ones where it's it's drawn a measuring cup that you might

452
00:29:25,880 --> 00:29:30,720
see in your kitchen. The thing that I haven't sort of so in

453
00:29:30,720 --> 00:29:33,160
addition to choosing the shapes for these drawings, it's also

454
00:29:33,200 --> 00:29:35,360
choosing the colors. And I thought it was really strange that

455
00:29:35,360 --> 00:29:37,960
it chose this bright green color, because I've never seen a

456
00:29:37,960 --> 00:29:41,480
green measuring cup. So again, I dove into the data. And I found

457
00:29:41,480 --> 00:29:45,680
that the data set actually had a large number of examples where

458
00:29:45,680 --> 00:29:48,920
there were green measuring cups. And I dug into this little bit

459
00:29:48,920 --> 00:29:52,120
more. And it turns out that there's this collectible measuring

460
00:29:52,120 --> 00:29:54,720
cup called depression glass measuring cups. This was made

461
00:29:55,520 --> 00:29:58,640
around World War One. And for whatever reason, people were

462
00:29:58,640 --> 00:30:03,080
putting uranium in the glass. And these were collectible. And

463
00:30:03,080 --> 00:30:05,000
so there's a lot of these post donal onward people are

464
00:30:05,000 --> 00:30:07,760
training them because they're collectible. And this is an

465
00:30:07,760 --> 00:30:11,880
example of sampling bias. So arguably, this measuring cup is

466
00:30:11,880 --> 00:30:15,640
not very commonly green. But because of the way this data is

467
00:30:15,640 --> 00:30:18,080
kind of farmed off of the web, there's a lot of green ones that

468
00:30:18,080 --> 00:30:21,720
end up in the training set. So to this computer vision system,

469
00:30:21,760 --> 00:30:24,640
it's actually quite likely that there would be a green measuring

470
00:30:24,640 --> 00:30:28,960
cup. So this is again, kind of showing how the machine learning

471
00:30:29,600 --> 00:30:33,480
techniques are bubbling through it in the results. And then the

472
00:30:33,480 --> 00:30:36,520
last one I'll show here is tick. So tick is one of my least

473
00:30:36,520 --> 00:30:39,840
popular, at least by sales prints that I've done. No one wants

474
00:30:39,840 --> 00:30:44,560
to put a tick in their in their home, I guess, on the wall. But

475
00:30:44,560 --> 00:30:48,400
this one I'm pointing out because in addition to sort of the

476
00:30:49,080 --> 00:30:52,440
the print itself, it actually had a really strong response

477
00:30:52,440 --> 00:30:56,840
across many networks. And so I decided to quantify that. And the

478
00:30:56,840 --> 00:31:00,440
gold standard on this is to take the validation set of the data

479
00:31:00,440 --> 00:31:04,360
set itself. And so I took the the validation set for ImageNet

480
00:31:04,360 --> 00:31:07,320
and I took a network that wasn't involved in the creation of

481
00:31:07,320 --> 00:31:11,440
this. It was inception ResNet. And I scored all of the

482
00:31:12,400 --> 00:31:16,560
validation examples. And they fell into two classes, basically

483
00:31:16,560 --> 00:31:19,880
things that were ticks and things that weren't ticks. But the

484
00:31:19,880 --> 00:31:22,440
short of it is, is that this image of a tick registers

485
00:31:22,440 --> 00:31:25,680
stronger than all of the validation examples, which is

486
00:31:25,720 --> 00:31:28,680
fairly surprising. Like the tick response here is kind of like I

487
00:31:28,680 --> 00:31:32,360
said, a super stimulus or a stronger example of a tick to

488
00:31:32,360 --> 00:31:35,040
these networks and even pictures of the tick from the

489
00:31:35,080 --> 00:31:40,480
validation set. But this has precedent in art and design as

490
00:31:40,480 --> 00:31:45,120
well. Scott McLeod in his series in his book, Understanding

491
00:31:45,120 --> 00:31:47,240
Comics, talks about amplification through

492
00:31:47,240 --> 00:31:51,040
simplification and how if you're trying to represent, say a man,

493
00:31:51,200 --> 00:31:55,160
then you're maybe not well served by using a particular photo of

494
00:31:55,160 --> 00:31:58,200
a particular man, because that's that doesn't well fit the

495
00:31:58,200 --> 00:32:03,240
concept. It's too specific to, to one particular person. And

496
00:32:03,240 --> 00:32:06,640
actually by abstracting and removing some details, you

497
00:32:06,640 --> 00:32:10,800
can come up with a drawing or sketch that better represents a

498
00:32:10,800 --> 00:32:14,440
man or a person or, you know, different levels of abstraction

499
00:32:14,440 --> 00:32:18,840
in the in concepts. And so, arguably, I think that might be

500
00:32:18,840 --> 00:32:20,960
what we're seeing here, where we're taking a real tick and

501
00:32:20,960 --> 00:32:25,280
we're removing some of the some of the some features and leaving

502
00:32:25,280 --> 00:32:29,400
the most salient ones. This is also just as a footnote, kind of

503
00:32:29,400 --> 00:32:33,080
how symbolic abstraction and writing started. So Sumerian

504
00:32:33,080 --> 00:32:37,040
writing starts if you owe someone three oxes, you write, you

505
00:32:37,040 --> 00:32:40,520
know, a picture of three oxes in your play tablet and overtime

506
00:32:40,520 --> 00:32:45,280
that evolves into kind of symbolic abstractions. So that was

507
00:32:45,280 --> 00:32:48,480
my my series, Perception Engines, which is two prints, I'm

508
00:32:48,520 --> 00:32:51,080
going to talk briefly about some other series that I've done

509
00:32:51,080 --> 00:32:54,600
since then. There's one that I did after that called synthetic

510
00:32:54,600 --> 00:32:58,520
abstractions. I've covered it here because as a as a warning,

511
00:32:58,760 --> 00:33:02,840
this is technically not safe for work imagery. So if you are

512
00:33:02,840 --> 00:33:07,120
uncomfortable looking at images that at least vision systems

513
00:33:07,120 --> 00:33:11,520
say are explicit imagery, I encourage you to pause your video

514
00:33:11,520 --> 00:33:16,440
or look away now. So here we go. These are these are these are

515
00:33:16,480 --> 00:33:20,240
abstract prints that I made, specifically looking at not so

516
00:33:20,240 --> 00:33:23,760
much systems for ImageNet, but these systems that impact us all

517
00:33:23,800 --> 00:33:26,680
online. So, you know, we have Google Safe Search, and we have

518
00:33:26,680 --> 00:33:29,960
these other systems that try to shield us from certain images.

519
00:33:29,960 --> 00:33:33,920
And I'm wondering, like, well, are there abstract versions of

520
00:33:33,920 --> 00:33:38,720
what it is that that that that seems to trigger these filters? So

521
00:33:38,720 --> 00:33:41,480
as an example of one of these, I made this print Mustard Dream,

522
00:33:41,640 --> 00:33:44,160
it's on mustard colored paper, and it's just black and white

523
00:33:44,200 --> 00:33:48,720
ink. And when you show a print, this a picture of this print

524
00:33:48,720 --> 00:33:52,640
to three division systems like Google Safe Search, it'll register

525
00:33:52,640 --> 00:33:56,160
as a as adult or racy. Similarly, Amazon thinks it's

526
00:33:56,160 --> 00:34:01,000
explicit nudity and Yahoo thinks it's not safe for work. So here

527
00:34:01,000 --> 00:34:04,240
I am exhibiting that print. I did a series of these. There was

528
00:34:04,240 --> 00:34:08,040
another one pitch stream, another one composition with red,

529
00:34:08,040 --> 00:34:10,960
blue, and yellow. That's kind of an an art joke here, because

530
00:34:10,960 --> 00:34:16,600
it's a riff on a well known other work, except my arrangement

531
00:34:16,640 --> 00:34:22,600
of inks triggers these these algorithms, these filter

532
00:34:22,600 --> 00:34:26,960
algorithms. I have done some similar canvas baked works more

533
00:34:26,960 --> 00:34:30,280
recently in Spain. So these are two newer ones where I'm

534
00:34:30,280 --> 00:34:33,400
actually trying to, you know, I don't know what it is. And the

535
00:34:33,400 --> 00:34:35,240
other one, so people ask me, Well, what is it the computer

536
00:34:35,240 --> 00:34:38,520
season? I don't know. Like I said, I don't have any kind of

537
00:34:38,840 --> 00:34:42,000
knowledge. And maybe I wouldn't want to know. I'd like that we

538
00:34:42,000 --> 00:34:44,520
don't have interpretable ML where we can ask the system what it

539
00:34:44,520 --> 00:34:47,960
sees. Here I'm actually trying to steer the data set a little bit

540
00:34:47,960 --> 00:34:53,920
more by influencing it with with influence the result by data

541
00:34:53,920 --> 00:34:56,760
set. And so this is, for example, one of those I actually

542
00:34:56,760 --> 00:35:01,240
called this one illustrated nude, which I'll talk a little bit

543
00:35:01,240 --> 00:35:05,160
later about why I changed the title of it. But that's because

544
00:35:05,160 --> 00:35:07,200
you know, when you show this to Amazon, that's what it

545
00:35:07,200 --> 00:35:10,720
classifies it at. It says, you know, it thinks it's looking at

546
00:35:10,720 --> 00:35:14,040
it in an illustrated nude and it's fairly high confidence. And

547
00:35:14,200 --> 00:35:16,920
similarly, Google safe search again, thinks it's a racy or

548
00:35:16,920 --> 00:35:21,600
adult image. So if you are searching for this, you insert

549
00:35:21,600 --> 00:35:25,040
safe search, you wouldn't see it. And if you got an email, it

550
00:35:25,040 --> 00:35:28,960
might go in your spam folder, for example. Okay, so I'm going to

551
00:35:28,960 --> 00:35:31,600
talk, I'm just going to go a few more examples just to kind of

552
00:35:31,600 --> 00:35:35,520
get a gist of how this so after that, I made sure my systems

553
00:35:35,520 --> 00:35:39,480
were working kind of in the large with these online API. So

554
00:35:39,480 --> 00:35:43,480
this is a data set for killer whale, a painting of the killer

555
00:35:43,480 --> 00:35:47,920
whale, and then the responses from Google's online vision API.

556
00:35:47,920 --> 00:35:50,640
So you can see it thinks it's a killer whale or marine mammal.

557
00:35:51,200 --> 00:35:54,480
Part of the interest here is also what the ontology or the

558
00:35:54,480 --> 00:35:58,960
labels for the these different systems are. Similarly, here's a

559
00:35:58,960 --> 00:36:02,640
penguin. So we're looking at the data set on the left, thousands

560
00:36:02,680 --> 00:36:07,480
of images of penguins, the version of a penguin that ended up

561
00:36:07,480 --> 00:36:11,400
being printed. And then you can see the Google API sees this as

562
00:36:11,400 --> 00:36:14,800
a penguin, or a flightless bird, or even particular types of

563
00:36:14,800 --> 00:36:20,720
penguins like Emperor penguin. As I mentioned before, there's

564
00:36:20,720 --> 00:36:25,040
the ability to do custom data sets. And so as a commission

565
00:36:25,360 --> 00:36:29,000
collaboration I did with yacht for their album artwork, I did a

566
00:36:29,000 --> 00:36:32,480
series of custom data sets. This is one from from that series.

567
00:36:32,680 --> 00:36:35,840
Where we had a data set of eyes, and we trained the system to

568
00:36:35,840 --> 00:36:42,040
make a synthetic eye. And as you can see, when Google looks at

569
00:36:42,040 --> 00:36:44,920
this, it thinks it's seeing a face, or a nose, or eyebrow, or

570
00:36:44,920 --> 00:36:47,920
eyelash, there's sort of eye features coming through in the

571
00:36:47,920 --> 00:36:51,880
labels. And then, and this is another one I did, similarly,

572
00:36:51,880 --> 00:36:57,800
where it's pictures of rabbits. So it's this one is a various

573
00:36:57,800 --> 00:37:01,280
kind of simple rendition of a rabbit. And again, it's kind of

574
00:37:01,320 --> 00:37:04,760
funny what the Google labels for this end up being evidently

575
00:37:04,760 --> 00:37:08,600
they have separate labels for hair, rabbit, rabbit and hairs

576
00:37:08,600 --> 00:37:11,520
and domestic rabbit, it kind of triggers all of those different

577
00:37:11,520 --> 00:37:17,920
labels in its API response. More recently, and this is kind of

578
00:37:17,960 --> 00:37:20,360
getting up to what I've been doing the last year, I've been

579
00:37:20,400 --> 00:37:24,360
exhibiting these in groups. I found that instead of showing

580
00:37:24,360 --> 00:37:29,520
one example, it actually is more enlightening to see many of

581
00:37:29,560 --> 00:37:31,680
these at once, because you can kind of get a feel for what the

582
00:37:31,680 --> 00:37:34,680
visual language or the common elements are. So these are six

583
00:37:34,680 --> 00:37:39,240
chickens and six eyes, which I exhibited about this time last

584
00:37:39,280 --> 00:37:44,080
year at Separ Gallery. This is something I did subsequently,

585
00:37:44,080 --> 00:37:46,600
where I took that kind of even a step further, and there's kind of

586
00:37:46,600 --> 00:37:50,160
this room full of images, the computer thinks are knots. And

587
00:37:50,160 --> 00:37:56,280
so we have different shape and color combinations, all all

588
00:37:56,320 --> 00:38:01,240
being different images that the computer thinks are reminding

589
00:38:01,240 --> 00:38:03,320
it of and I think knots is kind of an interesting one, because

590
00:38:03,320 --> 00:38:06,240
it's kind of this amorphous shape, but there seems to be some

591
00:38:06,240 --> 00:38:10,000
commonality visually to what what the computer thinks is a

592
00:38:10,000 --> 00:38:13,920
knot. And then this is one I have this in progress. Maybe I'll

593
00:38:13,920 --> 00:38:17,240
give you about 10 seconds to think about what these might be.

594
00:38:17,680 --> 00:38:20,480
But these are actually this looks like it might be computer

595
00:38:21,320 --> 00:38:23,840
images, but these are actually photos of canvases, which I've

596
00:38:23,840 --> 00:38:27,320
taken where I'm starting to organize these into groups. And

597
00:38:27,320 --> 00:38:30,000
these are all the canvases that are just given completed. And

598
00:38:30,000 --> 00:38:34,800
this is for an upcoming exhibition on ants. And so all of

599
00:38:34,800 --> 00:38:38,880
these are shapes that will will trigger in computer vision

600
00:38:38,880 --> 00:38:42,720
systems, to different extents, in thinking that it's looking at

601
00:38:42,760 --> 00:38:47,640
at an ant. So that's kind of a summary of my work. I'm going to

602
00:38:47,640 --> 00:38:50,480
spend the remainder of my talk talking about other approaches

603
00:38:50,480 --> 00:38:57,920
and a little bit about and a little bit about my research,

604
00:38:57,920 --> 00:39:01,360
and then I'll just briefly talk about how my my artwork impacts

605
00:39:01,360 --> 00:39:04,920
the real world, or effects it has once it gets out into the

606
00:39:04,920 --> 00:39:08,840
world. So as a lot of the other speakers have mentioned is these

607
00:39:08,840 --> 00:39:11,520
generative techniques, and I think these are very relevant, and

608
00:39:11,520 --> 00:39:15,280
they're used on other approaches, but there's also this idea that

609
00:39:15,720 --> 00:39:19,040
other AI art approaches had different narratives. So I want

610
00:39:19,080 --> 00:39:23,120
to talk a little bit about that. So my interest, so when I came

611
00:39:23,120 --> 00:39:26,200
into when I started getting interested in modern deep learning

612
00:39:26,200 --> 00:39:29,200
actually got into generative networks as well. And for a

613
00:39:29,200 --> 00:39:32,200
while, I was doing a lot of important research on this or

614
00:39:32,200 --> 00:39:35,520
research that I enjoyed digging into how these things works in

615
00:39:35,520 --> 00:39:40,520
2015 and 2016. This is an artwork I did in 2016. It was just

616
00:39:40,520 --> 00:39:44,360
large, like two meter by one meter print. Here's a zoom in

617
00:39:44,560 --> 00:39:48,160
showing this. And this was images of faces. So at the time,

618
00:39:48,160 --> 00:39:53,080
these were really high quality neural net outputs for faces,

619
00:39:53,120 --> 00:39:55,680
you know, before we had style again, and these other more

620
00:39:55,680 --> 00:39:59,280
modern networks. And I was interested in kind of the space

621
00:39:59,320 --> 00:40:05,000
of faces that could be created. And I, and I wrote a paper talking

622
00:40:05,000 --> 00:40:07,520
about some of these techniques and how you could sample these

623
00:40:07,520 --> 00:40:12,680
networks to get some of the best, the best outputs from those. I

624
00:40:12,680 --> 00:40:16,080
also, since subsequent to that have continued looking at these

625
00:40:16,080 --> 00:40:18,480
generative networks in more of a research sense. So in my

626
00:40:18,480 --> 00:40:21,960
research capacity at the university, I work, I work, for

627
00:40:21,960 --> 00:40:25,520
example, in this system with my graduate student, as Rebecca

628
00:40:25,520 --> 00:40:30,920
alluded to, it's, there's a interesting kind of getting these

629
00:40:30,920 --> 00:40:34,240
out to tools to other creators. And so one of the ideas I have

630
00:40:34,240 --> 00:40:36,840
with this graduate student was to make a spreadsheet tool where

631
00:40:36,840 --> 00:40:38,720
instead of numbers in a spreadsheet, you actually had

632
00:40:38,720 --> 00:40:42,480
samples from a generative model. And could you do use this as a

633
00:40:42,480 --> 00:40:47,480
way of giving people sort of their own create creativity tool

634
00:40:47,520 --> 00:40:51,120
through the interface of a spreadsheet. So it's looking at

635
00:40:51,120 --> 00:40:53,960
at some of these generative models. But in the context of

636
00:40:53,960 --> 00:40:57,480
this art talk, it's, it's, I mainly just wanted to say, this

637
00:40:57,480 --> 00:41:01,160
is a tool other people are using. So Helena Saren is using

638
00:41:01,160 --> 00:41:04,400
these Mario Klingerman, Helena is using these in a way that,

639
00:41:04,720 --> 00:41:07,800
again, Rebecca alluded to where she's using these small data

640
00:41:07,800 --> 00:41:10,760
sets and essentially overfitting those data sets and changing the

641
00:41:10,760 --> 00:41:14,880
data set to get the result that she's looking for. Mario

642
00:41:14,880 --> 00:41:18,840
Klingerman in this project, Neural Glitch, was training one of

643
00:41:18,840 --> 00:41:22,000
these, and then he intentionally kind of damages the network, and

644
00:41:22,000 --> 00:41:26,640
then displays how that damage comes through visually in his

645
00:41:26,640 --> 00:41:29,760
artwork. So it's the idea that there's a sort of an intentional

646
00:41:29,760 --> 00:41:33,760
glitch in there. There's two other artists that that all

647
00:41:33,760 --> 00:41:38,720
mentioned that that one is Robbie Barrett, who did this

648
00:41:38,720 --> 00:41:42,680
project a few years ago called RDCGAN, where he looked at using

649
00:41:43,280 --> 00:41:46,840
these, these generic networks to specifically create portraits.

650
00:41:47,120 --> 00:41:50,240
And there's another well known, at least in a art precedent,

651
00:41:50,240 --> 00:41:52,840
which is obvious is Edmond de Bellamy, that's well known,

652
00:41:52,840 --> 00:41:55,160
mainly because it went up for auction for a very high amount

653
00:41:55,160 --> 00:41:59,760
of money, a Sotheby's a couple of years ago. These actually, I'm

654
00:41:59,880 --> 00:42:02,880
putting up mainly because they use very, very similar techniques.

655
00:42:02,880 --> 00:42:05,960
In fact, they share some code and techniques across them, but

656
00:42:05,960 --> 00:42:08,080
they have very different narratives behind them. And so I

657
00:42:08,320 --> 00:42:11,360
wanted to kind of point out, and the narrative here is I mean,

658
00:42:11,360 --> 00:42:14,640
like what the story is behind how these are made. So when Robbie

659
00:42:14,640 --> 00:42:18,080
is, is, and when I'm, you know, showing my work, it's very much

660
00:42:18,240 --> 00:42:21,120
talking about using these things as a tool or as a collaboration.

661
00:42:21,400 --> 00:42:25,200
But there is a strong push or there's a lot of people in the

662
00:42:25,200 --> 00:42:28,520
art community that more say that, no, it's the, the computer is

663
00:42:28,520 --> 00:42:31,080
autonomous and it's making these artworks. And that was the

664
00:42:31,400 --> 00:42:34,560
kind of stance that obvious took. And it's one that resonates

665
00:42:34,600 --> 00:42:38,160
with people. I think taken to an extreme, what you get is you

666
00:42:38,160 --> 00:42:43,920
get a lot of people making work with robots. So it's common for

667
00:42:43,920 --> 00:42:46,320
people to make these robot artists. And here's just one

668
00:42:46,320 --> 00:42:51,680
example of that. But these have a long history. So there's

669
00:42:51,680 --> 00:42:55,680
actually a long history of making these drawing automatons. And

670
00:42:55,680 --> 00:43:00,680
here's one, you know, that's 200 years old, where it's basically

671
00:43:01,680 --> 00:43:05,640
being driven by gears. And so I just mentioned this is that I

672
00:43:05,640 --> 00:43:08,360
see this as a slippery slope. And so I said, kind of stay away

673
00:43:08,360 --> 00:43:12,200
from this and intentionally don't use any, at least for now,

674
00:43:12,440 --> 00:43:15,600
kind of drawing automaton, because I want to contextualize

675
00:43:15,600 --> 00:43:19,360
what I'm doing as a, as a collaboration between what I'm

676
00:43:19,360 --> 00:43:22,800
doing. So going back kind of again to, to Harold Cohen and

677
00:43:22,800 --> 00:43:27,800
his work, and most of the artists in the space. This is a what

678
00:43:28,240 --> 00:43:31,320
I'm what I'm doing is not so much me handing over full autonomy

679
00:43:31,320 --> 00:43:33,880
and saying the computer is the artist, but coming up with a

680
00:43:33,880 --> 00:43:37,880
co creation process where the there's a role for me and

681
00:43:37,880 --> 00:43:42,840
there's a role for these systems that I'm making. Okay, and in

682
00:43:42,840 --> 00:43:46,480
the last five minutes here, I'm going to spend, I'm going to

683
00:43:46,480 --> 00:43:48,760
talk a little bit about what happens when we put these things

684
00:43:48,760 --> 00:43:52,400
out in the wild, and how how these are some of my artwork, how

685
00:43:52,400 --> 00:43:55,520
it's kind of how we can understand it. So of course,

686
00:43:55,600 --> 00:43:58,560
there's, we can ask the machine what it sees, we can use

687
00:43:58,560 --> 00:44:01,120
visualization techniques, but the one I'm highlighting most

688
00:44:01,120 --> 00:44:04,080
here is these unintended consequences, like what, what

689
00:44:04,080 --> 00:44:08,160
surprises has happened. And keep in mind that for from the eyes

690
00:44:08,160 --> 00:44:11,360
of the computer perception system, this is Magritte's

691
00:44:11,520 --> 00:44:14,120
trajectory of images where he was kind of pointing out like a

692
00:44:14,120 --> 00:44:17,280
picture of a pipe is not a pipe. But that distinction often is

693
00:44:17,280 --> 00:44:20,600
lost with these perception systems, like if they see

694
00:44:20,600 --> 00:44:23,880
something in a representation of something, they more or less

695
00:44:23,920 --> 00:44:28,440
think it's that thing. So the first way you can tell or

696
00:44:28,440 --> 00:44:30,840
interpret or think about my artwork is you can ask a system

697
00:44:30,840 --> 00:44:34,280
what it sees. So if I take this picture I took of me with my

698
00:44:34,320 --> 00:44:38,440
artwork six chickens, and I feed it to the Amazon API, it will

699
00:44:38,440 --> 00:44:41,280
come back and so very diligently that it found a

700
00:44:41,280 --> 00:44:44,640
chicken and a chicken and a bird and etc. And there's a person

701
00:44:44,640 --> 00:44:46,960
over on the side. And I think that's one way to kind of

702
00:44:46,960 --> 00:44:52,480
understand how these systems are viewing or understanding these

703
00:44:52,480 --> 00:44:56,080
artworks. So the computer, the systems themselves. There was

704
00:44:56,080 --> 00:44:58,280
another way that I talked about in the beginning where you can

705
00:44:58,280 --> 00:45:01,640
actually use visualization techniques. So this is using some

706
00:45:01,640 --> 00:45:04,480
research from open AI where they have visualization techniques

707
00:45:04,480 --> 00:45:08,320
similar to deep dream, where they take, I can take one of my

708
00:45:08,320 --> 00:45:10,920
prints and feed it to one of these systems and ask it to kind

709
00:45:10,920 --> 00:45:16,120
of imagine how it relates. So here's the print to chickens.

710
00:45:16,440 --> 00:45:20,360
Here's kind of the, the inner imagination or visualization of

711
00:45:20,360 --> 00:45:23,280
what the computer sees when it sees that. And you can even take

712
00:45:23,280 --> 00:45:26,400
that a little bit further and try to visualize that in 3D. So

713
00:45:26,400 --> 00:45:29,240
this is a little slideshow where it attempts to add some fake

714
00:45:29,280 --> 00:45:35,960
depth to that to that image so that it can kind of understand

715
00:45:36,120 --> 00:45:39,400
understand it. But I think what's most interesting is how

716
00:45:39,400 --> 00:45:42,440
these things kind of accidentally kind of bump into real world

717
00:45:42,440 --> 00:45:46,000
systems. So I'm going to talk about a couple of those to close

718
00:45:46,000 --> 00:45:49,840
out. So this is an exhibition I talked about in 2018. I had

719
00:45:49,840 --> 00:45:54,360
these prints that were supposed to trigger various safe search

720
00:45:54,360 --> 00:45:56,480
filters. And this is an exhibition on the left. I have this

721
00:45:56,520 --> 00:45:59,920
not so great photo of it with a lot of glare and stuff. And I

722
00:45:59,920 --> 00:46:01,760
actually took a picture of this and was going to post it to

723
00:46:01,760 --> 00:46:05,360
Tumblr. And it didn't allow me to post it to Tumblr. The actual

724
00:46:05,360 --> 00:46:08,440
post failed and it said this post contains adult content, which

725
00:46:08,440 --> 00:46:11,440
violates our community guidelines. I was flagged as adult

726
00:46:11,440 --> 00:46:14,160
content and it was not displayed. So I think that's one

727
00:46:14,160 --> 00:46:17,400
example of it kind of bumping up against these real world

728
00:46:17,400 --> 00:46:21,360
systems. Similarly, I had another work in that series,

729
00:46:21,400 --> 00:46:23,760
Architectural Digest wanted to put a print in one of their

730
00:46:23,760 --> 00:46:27,720
magazines. I showed them some prints and they wanted the most

731
00:46:27,720 --> 00:46:31,920
colorful one, which was Lime Dream. Again, maybe not the most

732
00:46:31,920 --> 00:46:34,600
appropriate because it's supposed to be explicit imagery, but

733
00:46:34,600 --> 00:46:38,440
it they decided to go with it anyway. And so when I got a copy

734
00:46:38,440 --> 00:46:43,640
of the magazine, I took a great picture of the magazine, and I

735
00:46:43,640 --> 00:46:46,520
fit into the Amazon API and it was still convinced it was

736
00:46:46,520 --> 00:46:49,600
looking at explicit nudity. So I think this might be the first

737
00:46:49,600 --> 00:46:54,600
example of a cybernetic centerfold like a basically a photo of

738
00:46:54,600 --> 00:46:59,760
a nude made for these kind of vision systems that has appeared

739
00:46:59,760 --> 00:47:04,680
in a magazine. Similarly, like the art gets sold in weird ways.

740
00:47:04,680 --> 00:47:10,080
And so I had a print at a gallery and they sold off without my

741
00:47:10,080 --> 00:47:13,960
knowledge, one of these prints to Sloan Kettering when so if you

742
00:47:13,960 --> 00:47:16,480
go to their academic offices in New York, you can actually see

743
00:47:16,480 --> 00:47:21,400
one of these prints. Again, I don't know that I would that it

744
00:47:21,400 --> 00:47:23,760
might be the most appropriate for this environment. And in

745
00:47:23,760 --> 00:47:26,400
fact, that's one of the reasons I alluded to earlier, I gave

746
00:47:26,400 --> 00:47:30,360
these initially kind of these vague names. But now I've called

747
00:47:30,360 --> 00:47:33,240
them illustrated nudes explicitly because I want if

748
00:47:33,240 --> 00:47:36,000
someone buys or sees one of these, I want them to know what

749
00:47:36,000 --> 00:47:37,920
it's supposed to represent. And so the only way for me to

750
00:47:37,920 --> 00:47:41,480
package that is to put it in the title where it can't be missed.

751
00:47:42,480 --> 00:47:48,360
I did a print a couple of years ago that I took to NeurIPS. I

752
00:47:48,360 --> 00:47:52,440
was stretching it in my hotel room and hanging it up. And when I

753
00:47:52,440 --> 00:47:55,320
went to take a picture of it, I was very happy. This is my

754
00:47:55,320 --> 00:48:00,200
camera interface. When this little yellow rectangle appeared.

755
00:48:00,200 --> 00:48:03,040
So I think if anyone has a phone, they know this is when your

756
00:48:03,040 --> 00:48:06,200
camera's trying to focus on a face. And so just the fact that

757
00:48:06,200 --> 00:48:09,960
this was a shape intended to trigger a face and machine

758
00:48:10,000 --> 00:48:13,720
learning algorithms, kind of pulled the focus onto onto this

759
00:48:13,720 --> 00:48:17,520
because it thought there was a face in the scene. And then the

760
00:48:17,520 --> 00:48:19,880
last one of these is just getting ready for this talk. So as

761
00:48:19,880 --> 00:48:22,520
you can imagine, getting ready for this talk, I'm trying to find

762
00:48:22,520 --> 00:48:25,560
images of my artwork. And I'm pulling them on my phone. And

763
00:48:25,560 --> 00:48:28,800
lo and behold, when I pull up my phone, down at the bottom, it

764
00:48:28,800 --> 00:48:32,440
says we have several people and places in categories of things.

765
00:48:32,680 --> 00:48:35,280
And here's the categories of things that I've kind of

766
00:48:36,240 --> 00:48:38,400
partitioned for you that you might be interested in, like

767
00:48:38,440 --> 00:48:41,040
your animals and your food. So of course, these aren't real

768
00:48:41,040 --> 00:48:44,880
animals and food. If I search for banana, it thinks it's you

769
00:48:44,880 --> 00:48:47,680
know, I've done a print of banana and when I'm working in

770
00:48:47,680 --> 00:48:49,480
the print, I take a lot of pictures of it. So these have

771
00:48:49,480 --> 00:48:54,080
gotten classified as bananas or scorpions. This is a sombrero

772
00:48:54,080 --> 00:48:56,320
print. I was working on it. It's funny too, because it kind of

773
00:48:56,320 --> 00:48:59,400
mixes the real this is a picture of mine, a friend in a funny

774
00:48:59,400 --> 00:49:02,960
hat, and it ends up putting that in the picture with the artwork.

775
00:49:03,240 --> 00:49:05,760
Again, like I'm saying, the computer doesn't have a concept

776
00:49:05,760 --> 00:49:08,760
of what a lot of times is a representation of something

777
00:49:08,760 --> 00:49:12,720
versus what is the real thing. So if I do a search for syringe,

778
00:49:13,000 --> 00:49:15,440
not only do I get this weird photo that I took and I don't

779
00:49:15,440 --> 00:49:18,880
remember why I have a real syringe, but it probably I get

780
00:49:18,880 --> 00:49:22,200
this like really wacky, hypercolour syringe print that I

781
00:49:22,200 --> 00:49:24,680
was working on a few years ago. So I think it's interesting how

782
00:49:24,680 --> 00:49:29,640
these kind of get collapsed in the in these vision systems as

783
00:49:29,640 --> 00:49:34,440
being kind of the same thing. So that's it. Those just to

784
00:49:34,440 --> 00:49:37,160
recap the core ideas of my artwork is that the machines have

785
00:49:37,160 --> 00:49:42,240
their own way of seeing. We can create art for and by machines,

786
00:49:42,600 --> 00:49:45,760
which is what I'm trying to do is trying to use the the

787
00:49:46,080 --> 00:49:48,720
capabilities of machines and understand art through their

788
00:49:48,720 --> 00:49:54,720
eyes. And that and through art, we and by we, I mean, like,

789
00:49:55,920 --> 00:49:58,360
people knowledgeable about deep learning, but also people who

790
00:49:58,360 --> 00:50:01,240
might not have any background in machine learning can appreciate

791
00:50:01,240 --> 00:50:05,640
the ways that that machines perceive the world. So thanks,

792
00:50:05,640 --> 00:50:10,400
that's my online handle for Twitter and my web page. I'm

793
00:50:10,440 --> 00:50:13,120
open for taking questions that I will also say for questions that

794
00:50:13,120 --> 00:50:16,560
I'm very open for things that were part of the talk, but also

795
00:50:16,560 --> 00:50:19,200
might have been things only loosely alluded to in the talk,

796
00:50:19,200 --> 00:50:23,880
if you had questions on, you know, more about research tools or

797
00:50:23,920 --> 00:50:28,160
the community, I think that I'm happy to take any sort of a

798
00:50:28,160 --> 00:50:30,280
broad range of questions. Thank you.

799
00:50:31,480 --> 00:50:38,360
Thank you so much, Tom. It was so great. So I was wondering if

800
00:50:38,360 --> 00:50:40,000
students have questions.

801
00:50:42,760 --> 00:50:46,840
This is from very early on in the talk, but I'm wondering why you

802
00:50:46,840 --> 00:50:50,880
chose screen printing as your medium of choice.

803
00:50:56,120 --> 00:50:58,920
Great question. Sorry, I had to take a second.

804
00:50:59,040 --> 00:51:03,120
So, yeah, so there's I wanted to actually make physical work. And

805
00:51:03,120 --> 00:51:05,800
I thought that was, you know, it's actually easier for me to make

806
00:51:07,120 --> 00:51:10,800
work that is on the computer, but it takes a kind of a step

807
00:51:10,800 --> 00:51:15,040
further to create something that's printed. And that's also

808
00:51:16,320 --> 00:51:18,120
something there's two parts of the question, why make physical

809
00:51:18,120 --> 00:51:21,120
work at all? And then if you're making physical work, why have

810
00:51:21,120 --> 00:51:24,240
it be screen printed? So I think that that two reasons for

811
00:51:24,240 --> 00:51:27,040
making physical work. One is, I think that there's a lot of

812
00:51:27,080 --> 00:51:29,280
reasons for making physical work. One is, I think that people

813
00:51:29,280 --> 00:51:32,800
relate to physical work differently and when they go into

814
00:51:32,800 --> 00:51:35,680
a gallery and art setting. So for years, I did these interactive

815
00:51:35,680 --> 00:51:38,600
installations with screens and things. And I think that people

816
00:51:38,600 --> 00:51:42,800
come into those little bit of the defensive because sort of

817
00:51:42,800 --> 00:51:44,920
this technology right away, like you go in and you see the

818
00:51:44,920 --> 00:51:48,040
screen, you see, you know, this camera looking at you or

819
00:51:48,040 --> 00:51:51,800
something. And I think it sets a tone for how this is for what's

820
00:51:51,800 --> 00:51:57,000
to be expected. And so I really wanted to do a physical print

821
00:51:57,360 --> 00:52:06,480
that sort of was more more about the was more about how the

822
00:52:06,480 --> 00:52:09,720
computer sees it than about the short putting the process in

823
00:52:09,720 --> 00:52:11,840
the gallery. But the purpose for doing the screen printers, I

824
00:52:11,840 --> 00:52:15,240
wanted something that was very exact. So I could I actually have

825
00:52:15,240 --> 00:52:17,080
done some prints that there weren't part of the software I've

826
00:52:17,080 --> 00:52:20,160
experimented with other techniques, brush techniques. And

827
00:52:20,160 --> 00:52:22,360
so it's not really critical to use screen printing, but screen

828
00:52:22,400 --> 00:52:28,960
printing allows me to with some level of very precision, like

829
00:52:29,720 --> 00:52:34,440
make a traditional artwork. So and it has precedent kind of in

830
00:52:34,440 --> 00:52:37,480
the art world through through these pop artists. So I felt it

831
00:52:37,480 --> 00:52:40,480
was kind of a good, a good middle ground for kind of

832
00:52:40,480 --> 00:52:43,040
executing these I could of course print them out on a printer

833
00:52:43,040 --> 00:52:46,400
or something else. But I think that I wanted to kind of

834
00:52:46,400 --> 00:52:48,720
constrain my way, you might think in the same ways that

835
00:52:48,720 --> 00:52:51,600
artists in the past, it could strain themselves. So it makes

836
00:52:51,640 --> 00:52:55,640
it more easy to compare my works with existing artists, if I'm

837
00:52:55,640 --> 00:52:58,840
kind of operating with the same constraints are under the same

838
00:52:58,840 --> 00:53:01,880
interface, you might think of it. One thing I'll say too about

839
00:53:01,880 --> 00:53:04,920
making physical work is it makes it much more difficult to

840
00:53:04,920 --> 00:53:08,760
pull off these techniques, these kind of adversarial techniques

841
00:53:08,760 --> 00:53:11,360
because I don't know what the lighting or the angle of the

842
00:53:11,360 --> 00:53:15,880
photo is. So I try I have to make these these results work for

843
00:53:15,880 --> 00:53:21,320
a distribution of possible photos that might be taken. So if

844
00:53:21,360 --> 00:53:23,760
that's a part of you know, if you look at adversarial images

845
00:53:23,760 --> 00:53:26,440
research, there's kind of doing it on the computer and there's

846
00:53:26,440 --> 00:53:28,720
doing it in the physical world. And it's always kind of more

847
00:53:28,720 --> 00:53:31,320
difficult challenge to take these to the physical world. But

848
00:53:31,320 --> 00:53:33,720
that's kind of where I was interested in taking this.

849
00:53:38,560 --> 00:53:40,080
Cool. Thank you so much.

850
00:53:40,080 --> 00:53:56,600
Excellent. Are there are there other questions? I think that

851
00:53:56,600 --> 00:54:00,800
this is very interesting how thinking in art and you know,

852
00:54:00,800 --> 00:54:08,440
having this dual view of computers and arts can inspire

853
00:54:09,000 --> 00:54:13,120
such an interesting work. For instance, we see that, you know,

854
00:54:13,120 --> 00:54:15,720
as you said, one of the applications of this could be

855
00:54:15,720 --> 00:54:20,560
really adversarial attack and, you know, cyber security, which

856
00:54:21,240 --> 00:54:26,800
is really hard to think about it. If you only want to think about

857
00:54:27,240 --> 00:54:33,960
security and exclude this type of artistic practices, I never

858
00:54:33,960 --> 00:54:41,840
thought about it in this way, like how art can help for better

859
00:54:41,840 --> 00:54:45,720
understanding of, for instance, data bias or algorithmic

860
00:54:45,720 --> 00:54:52,120
bias, or other aspects of AI. So I thought that was very

861
00:54:52,120 --> 00:54:52,840
interesting.

862
00:54:54,320 --> 00:54:57,440
Thanks a lot. Yeah, that actually came through in my in my

863
00:54:57,440 --> 00:54:59,440
original generative work. So when I was working with

864
00:54:59,440 --> 00:55:01,960
generative networks, I found that they were actually really

865
00:55:02,000 --> 00:55:06,360
good at visualizing bias. So if you go back in my paper, I

866
00:55:06,360 --> 00:55:11,080
talked a little bit about how the celeb a data set has a label

867
00:55:11,080 --> 00:55:16,560
for for smiling. But in that data set, and I talk about ways that

868
00:55:16,560 --> 00:55:20,040
you can build dialers where you can turn up the smiling and

869
00:55:20,040 --> 00:55:23,240
down the smiling by using the labels on the data set. But

870
00:55:23,240 --> 00:55:25,680
there's bias in the data set where women are much more

871
00:55:25,680 --> 00:55:30,000
likely than men to be smiling. Just as a product of how the

872
00:55:30,000 --> 00:55:32,360
data was collected, like twice, almost twice as likely as

873
00:55:32,360 --> 00:55:35,240
like 1.5 or something like that. And so as you build these

874
00:55:35,240 --> 00:55:39,280
tools that are that are intended, or as you look at as you

875
00:55:39,280 --> 00:55:41,880
tell the network that you want to make an image smile more,

876
00:55:41,880 --> 00:55:44,320
you're actually also changing some of the masculine feminine

877
00:55:44,320 --> 00:55:48,080
characteristics of the image. And I thought it was a very

878
00:55:48,080 --> 00:55:51,280
visceral way to kind of see some of the machine learning bias.

879
00:55:52,120 --> 00:55:54,200
So that can come through in these general networks. But

880
00:55:54,200 --> 00:55:58,240
yeah, I think it also comes through in the visualization of

881
00:55:58,240 --> 00:56:03,080
these. And I'm kind of careful not to to couch my work too much

882
00:56:03,080 --> 00:56:05,400
in adversarial examples, because I think that I certainly use

883
00:56:05,400 --> 00:56:08,560
some of their techniques. But where I depart from them is that

884
00:56:08,560 --> 00:56:11,320
is they're always trying to do something imperceptible, or

885
00:56:11,320 --> 00:56:15,040
something that is, well, it's adversarial, like in the name,

886
00:56:15,040 --> 00:56:17,600
they're trying to trick the system, whereas I'm more using

887
00:56:17,600 --> 00:56:21,040
it as a visualization or kind of getting under the hood and

888
00:56:21,040 --> 00:56:25,320
kind of understanding what the stimulus is are that that might

889
00:56:25,320 --> 00:56:27,360
trigger the these in the first place.

890
00:56:28,560 --> 00:56:33,560
Yeah, also, it is very interesting how you know, doing by

891
00:56:33,560 --> 00:56:39,760
minimal strokes, you could achieve these things. And this is

892
00:56:39,760 --> 00:56:43,120
another way of thinking of compression, for instance, which

893
00:56:43,120 --> 00:56:46,000
was, as you said, inspired from art.

894
00:56:49,160 --> 00:56:51,120
Yeah, I know that's that's a really good point. Yeah. And I

895
00:56:51,120 --> 00:56:53,920
think that so there's there's a very practical reason to do that

896
00:56:53,960 --> 00:56:58,640
too is because I'm not using any really advanced techniques for

897
00:56:58,640 --> 00:57:02,840
generating these. I'm basically doing like random search or

898
00:57:03,080 --> 00:57:06,200
genetic algorithm. So it's basically doing a search over

899
00:57:06,200 --> 00:57:08,920
the space of outputs, and it just move, you know, it does a

900
00:57:08,920 --> 00:57:13,200
search estimating the gradient. So the fewer parameters you have

901
00:57:13,200 --> 00:57:17,120
the easier job you're going to have sort of hill climbing in

902
00:57:17,120 --> 00:57:20,880
that aspect. So it benefits me to have a simple representation

903
00:57:20,880 --> 00:57:23,320
for this. And there's other things about my drawing style

904
00:57:23,360 --> 00:57:26,440
when I talk about making a dry system for for these, like I

905
00:57:26,440 --> 00:57:28,560
can't have too many discontinuities and things like

906
00:57:28,560 --> 00:57:31,960
that. And the drawing styles that have but but yeah, I think

907
00:57:31,960 --> 00:57:35,080
it's also very interesting to come up with these kind of tight

908
00:57:35,400 --> 00:57:38,440
like for the shark, you know, it's 12 strokes to kind of

909
00:57:38,440 --> 00:57:44,000
represent a shape that when shown to the system still seems to

910
00:57:44,000 --> 00:57:46,840
be a strong stimulus for that category.

911
00:57:47,960 --> 00:57:52,680
Yeah, that also reminds me of the work that scientists did in

912
00:57:52,760 --> 00:57:56,560
computer vision, for instance, a work of Antonio Trouble, where

913
00:57:56,920 --> 00:58:01,000
the question is how many pixels do you need the minimum number

914
00:58:01,000 --> 00:58:06,920
of pixels to show to a computer, for instance, and in terms of

915
00:58:06,920 --> 00:58:12,440
images to see what is this object or get a gist of this

916
00:58:12,440 --> 00:58:16,880
object, you know, for instance, the experiment that set up only

917
00:58:17,520 --> 00:58:23,800
between 32 picture pixels. And they found that, you know, you

918
00:58:23,800 --> 00:58:29,560
can get a gist of what is going on in this instance, image is

919
00:58:29,600 --> 00:58:33,960
this computer vision techniques. So that is also very

920
00:58:33,960 --> 00:58:34,720
interesting.

921
00:58:35,520 --> 00:58:38,280
Yeah, it's also similar to I don't remember the researchers

922
00:58:38,280 --> 00:58:41,360
that did the psychology research where they take face images

923
00:58:41,360 --> 00:58:44,320
and they put them in very low like they basically make icons

924
00:58:44,320 --> 00:58:47,520
where they're, you know, like eight by eight pixels or 16 by

925
00:58:47,520 --> 00:58:50,840
16 pixels. And you can still represent you can still recognize

926
00:58:50,840 --> 00:58:53,920
Abraham Lincoln or these famous figures in these very low

927
00:58:53,920 --> 00:58:58,200
resolution, resolution format. So I think people also have

928
00:58:58,200 --> 00:59:02,360
this ability to decipher these very, these very low

929
00:59:02,360 --> 00:59:04,840
information images as well.

930
00:59:05,880 --> 00:59:11,760
Yeah, very interesting. Excellent. Thank you so much, Tom. It

931
00:59:11,760 --> 00:59:18,600
was great and really inspiring work. And I appreciate that.

932
00:59:19,440 --> 00:59:21,680
Thanks so much for having me. And if anyone else has any

933
00:59:21,920 --> 00:59:25,520
questions, I'm happy to, you know, if you send me an email and

934
00:59:25,560 --> 00:59:27,840
follow up from the class, I know not everyone likes asking

935
00:59:27,840 --> 00:59:32,320
questions. I'm happy to follow up on, you know, you know, every

936
00:59:32,320 --> 00:59:34,520
artist is a little bit different. And so there's no

937
00:59:34,520 --> 00:59:38,280
common. There's no common template, I think you can follow.

938
00:59:38,280 --> 00:59:41,800
But hopefully this will be a good, this is one good example

939
00:59:41,800 --> 00:59:44,160
of a path that someone else might be interested in taking.

940
00:59:44,960 --> 00:59:50,160
Certainly. And then also, Tom has been involved with the

941
00:59:50,160 --> 00:59:56,240
workshop of machine learning for art and creativity at New

942
00:59:56,240 --> 01:00:02,520
Ribs for several years. So that's also an interesting and

943
01:00:02,520 --> 01:00:07,520
valuable contribution that he's making. So if any questions

944
01:00:09,160 --> 01:00:11,760
Yeah, exactly. So I do know a lot of that research and can

945
01:00:11,760 --> 01:00:14,280
direct you to those. And I would encourage you, if you are

946
01:00:14,280 --> 01:00:17,080
interested in a space, you can, I believe, still access a lot of

947
01:00:17,080 --> 01:00:20,080
the videos, for example, from this past year's workshop, just

948
01:00:20,080 --> 01:00:23,320
to get an overview of going a little bit deeper into some of

949
01:00:23,320 --> 01:00:23,960
these talks.

950
01:00:25,640 --> 01:00:29,400
Excellent. Thank you so much, Tom. Thank you. Thank you.

951
01:00:29,400 --> 01:00:31,440
Have a good one. Thanks, everybody.

