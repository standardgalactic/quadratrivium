WEBVTT

00:00.000 --> 00:09.520
Cool. Hello everyone, welcome to your course AI for Art, Esthetic and Creativity. Today

00:09.520 --> 00:19.200
we have a very special speaker. She has an excellent background in different domains and

00:19.200 --> 00:28.880
she will tell you hopefully more about herself and her work. Sarah is a great friend and

00:29.520 --> 00:38.480
colleague of me and she kind of accepted to give us a lecture talk today. So from here I

00:41.040 --> 00:47.840
let Sarah to continue. Please go ahead. Thanks Ali. It's such a pleasure to be here. I've heard

00:47.840 --> 00:53.440
so much about this class. I don't think I have a slide about my background but I can tell you a

00:53.440 --> 00:59.920
little bit about myself. I finished my PhD in neuroscience so across the street from Seasale

00:59.920 --> 01:05.920
last year and now I'm a postdoc in the vision group and the journey throughout my PhD was

01:05.920 --> 01:12.320
a little bit of a winding path. I started thinking about explicit symbolic models for things like

01:12.320 --> 01:17.360
physics and we'll talk a little bit more about that along the way. So modeling how the mind

01:17.360 --> 01:22.480
makes inferences about things that we see but that hits a ceiling when we come up against

01:23.120 --> 01:29.120
questions of vision and types of seeing like looking at art that is really difficult to develop

01:29.120 --> 01:34.880
some kind of computational formalism for that we don't have good models for. And at the same time

01:34.880 --> 01:39.280
as I was kind of hitting that wall in my own thinking I was developing a parallel interest

01:39.840 --> 01:44.640
in visual art and doing a lot of different projects both with individual artists and with

01:44.640 --> 01:51.680
larger museum archives that I'll talk a little bit about and started to look at art as a ground for

01:52.240 --> 01:56.560
asking kind of difficult questions on the frontier of our thinking about the mind.

01:56.560 --> 02:01.600
If we look at how humans create art and in view art can we understand something about

02:02.320 --> 02:06.720
how they view the world and domains that we don't yet have good models of cognition for.

02:07.600 --> 02:12.640
So I kind of started steering my my PhD in that direction. I'll share a little bit of that work

02:12.640 --> 02:18.240
as well and as I said now I'm a postdoc with Antonio and Ellie asked me to share a little bit

02:18.240 --> 02:25.840
of my my inspiration behind that path. I don't have a good story about a specific moment I think

02:25.840 --> 02:32.720
it's been a lifelong interest for me since I was super small and reading a lot of poetry I guess

02:33.360 --> 02:38.080
thinking about kind of the the origin and nature of structure and our experience of the world.

02:38.080 --> 02:44.240
I know that's quite an abstract thing but the structure that we see in visual patterns where

02:44.320 --> 02:49.280
does that come from? Is that something that lives inherently in the brain and we imprint it on to

02:49.280 --> 02:53.680
kind of noisy and unordered stimuli or is it something that's external you know a nature

02:53.680 --> 02:59.600
nurture question and then our brains kind of evolve to reflect and I got interested in this

02:59.600 --> 03:05.920
meeting point this kind of layer between the self and the world where all the action happens so to

03:05.920 --> 03:14.000
speak and had training in applied math before I came to MIT and would think about ways to describe

03:14.000 --> 03:18.720
kind of structured inputs to processing systems and understand something about the structure of

03:18.720 --> 03:24.000
external inputs and then my neuroscience background learned a little bit how to how to

03:24.000 --> 03:28.320
think about and model the structure of a processing system right the structure of different parts of

03:28.320 --> 03:33.840
the brain and it's really been through my interest in visual art that we can start to think about

03:33.840 --> 03:39.680
and describe what happens when those two things meet and how we synthesize our world of visual

03:39.680 --> 03:45.040
experience in domains related to art and then other kind of higher level aspects of cognition

03:45.040 --> 03:51.680
like scenes or associations with moods of scenes and that kind of thing so that's that's where I am

03:51.680 --> 03:58.720
now and I think I'd like to start us off unless anybody has any leading questions about where I

03:58.720 --> 04:05.120
come from with kind of a provocation and you can you can think of this as a frame for what I'll share

04:05.120 --> 04:11.200
today but it's intended to be provocative and so the statement I'll make is that visual perception

04:11.200 --> 04:17.520
itself human perception which we attempt to mirror and model in computer vision and computer science

04:17.520 --> 04:23.280
in some cases that human perception is something that's fundamentally constructive and I say that

04:23.280 --> 04:29.360
because it solves an ill posed inverse problem like ones you've probably heard of before and

04:29.360 --> 04:36.080
doing that doing that solving requires a little bit of creativity so where am I coming from there

04:36.080 --> 04:42.000
the back of your eye as you know is is a 2d flat canvas right made up of a hierarchy of cells

04:42.000 --> 04:47.840
that were visualized in in drawing in art by Ramonica Hall hundreds of years ago and are now

04:47.840 --> 04:52.880
visualized using electromagnetic imaging and we can get actually pretty fine brain detail

04:52.880 --> 04:59.280
of the cells in the back of our eye that constitute a 2d canvas that takes in incoming image data

04:59.280 --> 05:05.760
and represents images in terms of patterns of activations via this kind of mosaic of cells

05:06.800 --> 05:12.960
yet we experience this richly 3d world so there's a setup of a problem that you've probably heard

05:12.960 --> 05:19.440
before right 2d canvas but we have 3d rich experience scenes have depth objects have 3d shape

05:19.440 --> 05:23.280
and furthermore what we see carries lots of different meanings and associations

05:23.920 --> 05:29.920
so where is all of that kind of higher level information in a 2d image classical kind of

05:29.920 --> 05:36.640
computer vision problems we look at this kind of painting by Suzanne you might not only recognize

05:36.640 --> 05:43.520
3d structure of this cottage on the mountain side right even though the image itself is 2d

05:43.520 --> 05:47.920
I might have all sorts of associations with it I might be able to say oh it's spring time

05:47.920 --> 05:52.400
think something about the time of year I might even be able to infer something about the geography

05:52.400 --> 05:58.400
by the palette used to convey what fields might be there think a little bit about the landscape

05:58.400 --> 06:05.040
I might be able to appreciate depth in pictorial space so even on this 2d plane if I put my mouse

06:05.040 --> 06:11.120
up here in the front maybe these fields are closer to me as a viewer than these ones that are far away

06:11.120 --> 06:15.120
but once again I'm just looking at a flat picture where is all of that information

06:16.320 --> 06:21.360
our brain has to solve an inverse problem like this anytime we look at a visual scene it has to get

06:21.360 --> 06:27.280
from low two-dimensional information to kind of rich 3d but there's a fundamental problem here

06:27.280 --> 06:34.480
and I pointed to that is that infinitely many 3d objects can cause the same 2d project projection

06:34.480 --> 06:39.680
that's the under constrained nature of this inverse problem that vision poses and you've

06:39.680 --> 06:45.840
experienced this quite explicitly anytime you've seen a shadow and not the object causing the

06:45.840 --> 06:50.720
shadow right and you've had to infer oh is that actually you know a monster on the wall or is

06:50.720 --> 06:56.720
that somebody's hand being projected but there are infinitely many configurations in three dimensions

06:56.720 --> 07:01.200
that could be projected downwards onto two dimensions and cause some configuration in

07:01.200 --> 07:07.920
pictorial space so how do we constrain that problem when we're solving for what we see

07:08.720 --> 07:13.920
right so this problem is ill-posed because it has as I said many infinitely many possible solutions

07:14.640 --> 07:20.560
and choosing between them requires some additional information and in the case of the brain

07:21.280 --> 07:26.240
modern neuroscience understands this as requiring the brain to construct something so that's what I

07:26.240 --> 07:32.480
mean when I say perception is fundamentally constructive or creative it requires the brain

07:32.480 --> 07:39.120
to construct a best explanation of what it's seeing of incoming information and if we call

07:39.120 --> 07:44.320
that perception then maybe you'll permit me to make a bit of a stretch and say that that makes

07:44.320 --> 07:51.920
perception itself an act of creation or an act of synthesis of a scene so one kind of popular

07:51.920 --> 07:58.160
way to solve this inference problem is by using models of the world right and we can approach

07:58.160 --> 08:02.560
that from a Bayesian lens maybe you've seen the work of Josh Tinnenbaum in the bcs department

08:04.000 --> 08:08.400
or maybe we can do that purely with deep learning it's kind of an attention that we could explore

08:08.960 --> 08:14.800
later today but I'll give you an example here and this is let me back up for a second that if we

08:14.800 --> 08:19.520
were in person this is the point where I would do kind of a live in person demo so I want you to

08:19.520 --> 08:25.040
imagine that we're all sitting kind of in a dark room or we're sitting in a studio space and out

08:25.040 --> 08:31.360
in front of you there is a table covered in black velvet and I've set some stuff on that table you

08:31.360 --> 08:38.080
don't know what it is I set it there when the lights were off and then I take a single line of red

08:38.080 --> 08:43.440
laser light and I'm going to gradually sweep it over the scene so I'm constraining the visual

08:43.440 --> 08:48.960
information you're going to receive about what's out there in the world to something kind of really

08:48.960 --> 08:54.320
low dimensional compared to what you normally get to understand kind of a garden of forms that would

08:54.320 --> 08:59.200
be sitting on the table so imagine you're there in the studio with me and you see the following

08:59.840 --> 09:04.240
you have to kind of infer what you see on the table maybe you could write it in the chat or

09:04.240 --> 09:10.080
just think to yourself when you see this give it a moment what's sitting here on the table

09:12.560 --> 09:14.880
or what kinds of things what different things

09:17.680 --> 09:24.240
maybe this would be a good use of the chat I can pull it up or you can describe features of what you see

09:34.560 --> 09:41.120
a bunch of blocks on the table great there's something cubic up now I see the corner there right

09:42.800 --> 09:48.880
multiple vases multiple forms with kind of different underlying shapes something cylindrical yep

09:49.840 --> 09:53.280
two things do you see I think there's a sphere actually there in the middle

09:56.480 --> 10:01.200
what is the experience of this light do you actually feel a physical corner when you see

10:02.080 --> 10:04.560
bent light round the corner of that cube

10:08.000 --> 10:09.680
oh oh goodness

10:13.200 --> 10:18.240
interacting with chat is a bit tough all right anyway the point I want to make here is that

10:18.240 --> 10:25.360
I can present really kind of low level information and you can if I dare open open the chat up again

10:25.920 --> 10:29.680
yeah there's a single base there's a single table and many forms sitting on top

10:29.760 --> 10:34.240
that's right so there's just a tabletop and then lots of different shapes also covered in black

10:34.240 --> 10:40.080
velvet so the light doesn't scatter and the light traces the outline of these 3d shapes

10:40.640 --> 10:46.560
and you can appreciate something about what the shapes are just by watching how light bends

10:46.560 --> 10:52.560
around their surface and the relative motion as it traverses that facade right as it moves

10:52.560 --> 10:58.000
over the surface of the sphere the light bends according to its curvature and I would argue

10:58.000 --> 11:05.760
here that because you have some notion of what a sphere is and some notion of what a cube is

11:05.760 --> 11:12.560
that is you have a relatively abstract model of these underlying shapes in your mind a mental

11:12.560 --> 11:18.240
model you can do some inference when you see light move over their surface on this way even though

11:19.120 --> 11:24.000
I choose an example like this because you've probably never seen this example before right

11:24.800 --> 11:28.320
never seen a single line of laser light move over this table surface

11:28.960 --> 11:33.760
even this this kind of setup but you can still do that inference pretty well and you did in the chat

11:35.120 --> 11:40.400
so if you'll if you'll stay with me here I'm suggesting that this is an example just your

11:40.400 --> 11:46.400
perceptually of how we can bring kind of models of the world and shapes and forms that comprise

11:46.400 --> 11:52.880
it to bear on simple visual stimuli and how we can even do that by using articulated light

11:53.440 --> 11:59.280
to isolate aspects of those stimuli and to kind of elucidate our perception to us

12:00.160 --> 12:04.560
so we do a lot of things like this in the MIT museum studio where I teach the vision and art

12:04.560 --> 12:09.280
neuroscience class which I'll talk about in a moment but that's where this this was filmed

12:09.280 --> 12:14.080
let's see if it'll let me advance even though I open the chat all right so another kind of setting

12:14.080 --> 12:19.680
in which we often hear and think about models of the world and this kind of inference is in

12:19.680 --> 12:25.600
intuitive physics and I bring this up because some of my background is also in this type of work

12:25.600 --> 12:31.680
investigating how the brain represents physical properties like mass that it uses to reason

12:31.680 --> 12:36.720
physically about the world right you would have to estimate the mass of this block that's falling

12:36.720 --> 12:41.520
and making a depression on this pillow before you would know the right amount of grip force

12:41.520 --> 12:45.840
you would need to use to to reach in and pick it up without dropping it right and this is something

12:45.840 --> 12:51.360
we do incredibly automatically and it's a skill set we develop regularly from a very early age

12:51.920 --> 12:59.440
and I found that the brain represents properties like mass with an amount of abstraction and invariance

12:59.440 --> 13:06.160
to the type of physical scene in which mass is revealed that would be necessary if mass like

13:06.160 --> 13:12.640
this were to be used as an input to an abstract generalized engine for physical simulation or

13:12.640 --> 13:18.720
what we call a physics engine in computer graphics and simulation suggesting that there is

13:18.720 --> 13:25.680
kind of some first evidence that the brain does use these kind of generalized simulation engines

13:25.680 --> 13:31.520
to solve low-level inference problems like inferring mass because we can make some hypothesis about

13:31.520 --> 13:36.320
the nature of the underlying representations it would need if it were to solve problems in this

13:36.320 --> 13:40.880
kind of way rather than by simple pattern matching or in a pixel based way where we would assume

13:40.880 --> 13:45.760
that the representation of mass would be quite different from scene to scene because the low

13:45.760 --> 13:51.200
level visual data about the scene is different but in fact that's not what we find we find representations

13:51.200 --> 13:55.840
of physical variables like mass and friction that generalize across any kind of physical scene

13:56.400 --> 14:01.120
that we test where we hold a lot of other different parameters constant right like object color

14:02.160 --> 14:06.880
and this suggests an account of physical reasoning in the brain that has been that has been studied

14:06.880 --> 14:13.040
pretty extensively computational right and that we model via probabilistic simulations of a physics

14:13.040 --> 14:17.840
engine I don't think that video is going to play for us right but this is the kind of work when I

14:17.840 --> 14:24.240
was doing when that I was doing when I was writing down like explicit models of the world that could

14:24.240 --> 14:29.280
be inverted to explain something about underlying parameters we were using for vision and in that

14:29.280 --> 14:36.720
in this case those models were physical right but what about cases like like art where it's

14:36.800 --> 14:42.320
difficult as I mentioned to develop some kind of computational formalism where we don't know the

14:42.320 --> 14:47.520
underlying model for instance how to create the Cezanne painting we saw in the beginning

14:48.240 --> 14:53.280
a priori right how do we even start what are the underlying dimensions we'd need to write down to

14:53.280 --> 15:01.120
either make sense of how we see things or how they're created so this whole area is kind of what we

15:01.120 --> 15:05.680
dive into in that vision in art and neuroscience course so this is something you're interested in

15:05.760 --> 15:10.400
it's of course an unsolved problem but we spend the fall semester every year

15:11.440 --> 15:16.480
kind of delving into it through both neuroscience literature through art practice through computation

15:16.480 --> 15:22.480
and then through studio work so kind of hands-on experimentation with principles underlying vision

15:22.480 --> 15:27.520
that we then externalize and experience ourselves and try and visualize in artistic contexts

15:29.440 --> 15:32.800
to give you a little bit of a taste of that class we would look at

15:33.360 --> 15:38.560
these examples say by by an artist in minor white and ask if we were trying to set up

15:39.360 --> 15:45.520
a typical kind of describe a model and then invert it to understand vision setting you know what is

15:45.520 --> 15:51.680
the veretical percept in either of these right if before we were considering mass of some object

15:51.680 --> 15:58.000
that the brain has to infer and then we can write down a physical law describing how mass plays into

15:58.080 --> 16:02.720
action unfolding in a scene a law describing dynamics and then invert it to think about

16:02.720 --> 16:07.120
how the brain represents mass what would the analog be here what would we write down as the

16:07.120 --> 16:12.320
veretical percept you can share some thoughts in the chat that is also an exercise you could just

16:12.320 --> 16:18.560
do yourself right maybe here you can start to get it a shadow of something outside the window

16:19.120 --> 16:24.960
I see a bike maybe a bike seat there that's kind of not the point kind of not trying to infer what

16:24.960 --> 16:31.040
caused the specific physics of this this image you're kind of getting at something different

16:31.040 --> 16:37.200
and especially here what if the artist isn't around for us to ask anymore these are actual

16:37.200 --> 16:43.440
photographs right these are photographs of something but the act of looking at it isn't about

16:43.440 --> 16:48.560
inferring the underlying cause of the image it's about inferring something else sort of aesthetic

16:48.560 --> 16:53.920
parameters that define visual experience or kind of render visual experience at a lot higher of a

16:53.920 --> 17:00.000
level how do we begin to get traction on problems like this either in seeing or in or in generation

17:00.800 --> 17:06.000
as I said you know in art we also come up against a great difficulty in that you know there are

17:06.000 --> 17:12.320
infinitely many ways to render recognizable depictions of common objects right with all

17:12.320 --> 17:18.160
sorts of idiosyncrasies illusory boundaries difficult for models to detect but we recognize

17:18.160 --> 17:24.800
a woman in these images with the dress almost instantaneously and similarly we come up against

17:24.800 --> 17:30.560
another under constrained inverse problem is in that there's infinitely many ways to render and

17:30.560 --> 17:37.280
depict kind of abstractions of commonly recognizable forms which again are difficult for

17:37.280 --> 17:42.880
current day models but they're pretty easy for us I can recognize a figure and maybe have different

17:42.880 --> 17:49.040
associations with it in each of these different images so we think a little bit about this

17:49.760 --> 17:55.280
in the course like I mentioned you can ask me a bit after this talk as well if you're if you're

17:55.280 --> 18:01.520
interested in it it's called vision and art and neuroscience all of our info is is online most

18:01.520 --> 18:08.960
of the syllabus past exhibition catalogs at vision.mit.edu it's offered through through bcs

18:09.840 --> 18:15.200
and as I said we we investigate during half the class in the seminar portion of the class

18:15.760 --> 18:19.520
kind of the underlying principles of vision and we work through a series of modules

18:20.320 --> 18:27.280
that build up visual processes from early level like v1 visual processing all the way up to kind

18:27.280 --> 18:33.840
of more rich images and we do this in parallel in a studio section during the other portion of the

18:33.840 --> 18:39.360
class where we're translating these principles of vision into the studio and building artistic

18:39.360 --> 18:45.200
contexts where we can kind of become aware of our own perceptual processing at work so examples like

18:45.200 --> 18:49.120
the one I showed you at the beginning right with the with the laser line moving over

18:49.120 --> 18:54.400
that garden of objects are examples of settings that can allow us to maybe perceive our own

18:54.400 --> 18:59.840
perception at work right or shed some light on what's going on when we look at at normal scenes

18:59.840 --> 19:04.560
right there's all these unconscious inference processes happening even when we look at corners

19:04.560 --> 19:08.800
in a room but we're not aware of them and so we ask here if we can create settings

19:09.520 --> 19:15.600
where we do become intensely aware of them and that awareness becomes kind of the art experience

19:15.600 --> 19:20.960
right and so it's the art of perceiving one's own perceptual processes at work and then over

19:20.960 --> 19:26.160
the course of the class everybody develops an individual artwork for exhibition which is super

19:26.160 --> 19:32.560
lovely and it's it's an opportunity that we don't often have in other classes at MIT so we run this

19:32.560 --> 19:38.880
for five years now had five different exhibitions and COVID be it a virtual exhibition and then

19:38.880 --> 19:46.080
this year's just opened in December and is actually still up in the MIT museum studio just off of lobby

19:46.080 --> 19:52.000
10 10 150 if anybody is on campus and wants to go check it out it's most it's open most days when

19:52.240 --> 19:59.920
staff are there but this course is the parallel to your IAP class that thinks about things more

19:59.920 --> 20:05.040
in the language of computational neuroscience than deep learning and in some aspects of the

20:05.040 --> 20:11.520
course will present deep learning or deep generative models as contexts for probing representations

20:11.520 --> 20:16.720
that might be shared by human minds and machines and we'll look at that a little bit later in this

20:16.800 --> 20:22.080
lecture but think more traditional computational neuroscience lectures readings visual art and

20:22.080 --> 20:26.880
then a studio component where you experiment with some of the stuff hands on so that's what we do

20:26.880 --> 20:33.600
envision art neuroscience we start to to probe at the richness of this art neuro and machine

20:33.600 --> 20:37.520
learning intersection there's a lot of different things we can do there and for the rest of this

20:37.520 --> 20:42.400
talk we're going to highlight a number of different projects that approach that intersection in

20:42.400 --> 20:47.280
different ways and highlight kind of different ways that you could think about engaging this

20:47.280 --> 20:52.160
material in these questions data sets and resources that we have available and kind of

20:52.160 --> 20:58.480
different ways of carving up the problem into bits so we'll start by thinking about modeling

20:58.480 --> 21:04.720
kind of the structure underlying human creativity at scale without trying to prespecify

21:05.520 --> 21:11.360
laws that you would write down for say a physics engine right can we use deep generative models

21:11.360 --> 21:18.080
to kind of approximate or appreciate or grok the structure underlying large data sets of human

21:18.080 --> 21:24.480
cultural artifacts and then use those models to experiment with cultural history on kind of a

21:24.480 --> 21:29.600
timeline that allows rapid evolution in the present so I'm speaking specifically about a project

21:29.600 --> 21:33.760
that I don't know if some of you have seen and I know Ali has seen a collaboration that I led with

21:33.760 --> 21:39.280
the Met a couple of years ago again it was fun that we were in person because we were able to

21:39.280 --> 21:44.800
actually go to the Met and see a lot of these objects but back in 2017 the Metropolitan Museum of

21:44.800 --> 21:51.600
Art was the first or one of the very first to release an open access catalog of a few hundred

21:51.600 --> 21:57.520
thousand digital images of works in the Met collection and released them into the public domain

21:57.520 --> 22:03.600
which is wonderful for for us as computer scientists and programmers and people interested in ML and

22:03.600 --> 22:08.960
art because what a rich data set that is right what a rich data set all in one place

22:08.960 --> 22:13.840
don't get me started on the issues with museum APIs but a lot of museums have followed suit in

22:13.840 --> 22:19.440
releasing their digital collections into the public domain so they're free and open for experimentation

22:20.240 --> 22:26.000
they approached us at MIT and open learning and a couple of programmers at Microsoft and asked if

22:26.000 --> 22:32.080
we might want to do a series of projects with this digital collection and so we did and we asked

22:32.080 --> 22:37.600
whether we can build deep generative models associated with archives like this of created

22:37.600 --> 22:42.800
work that are embedded in their cultural context which might ask which might allow us to ask like

22:42.800 --> 22:49.040
slightly more specific questions art historically than just you know what if you train StyleGAN on

22:49.040 --> 22:54.000
all of wiki art all at once right not conditionally so we're not appreciating any categorical

22:54.000 --> 22:59.520
differences between images but if we just showed it all of wiki art okay here we want to ask something

22:59.520 --> 23:04.880
a little more fine grained can we notice you know differences in the development of feature languages

23:04.880 --> 23:12.720
between maybe time periods or geographical regions right and can we develop ways of collaborating

23:12.720 --> 23:19.040
with those models to iterate archives forward so experimenting with chimeras between existing

23:19.040 --> 23:24.480
works and developing new works right that might sit somewhere between works that are already on a graph

23:26.080 --> 23:31.840
so one of the challenges that we faced here initially was that the data set was pretty big

23:31.840 --> 23:37.040
400,000 images but each individual category in that data set was not some might only have

23:37.040 --> 23:42.800
a couple hundred images and there's a lot of sketches and drawings and kind of uncategorized

23:42.800 --> 23:49.120
work too that makes up that 400,000 so you're in a situation where in theory you have a rich

23:49.120 --> 23:55.120
labeled data set but in practice it might be quite difficult to train anything that looks

23:55.120 --> 23:59.600
photorealistic or gives a good sense of any individual category of work because the categories

23:59.600 --> 24:06.480
themselves are not that large so at that point this was pre like style again too we started working

24:06.480 --> 24:14.800
on this in 2017-2018 um I asked whether we could instead of training a single model on say a subset

24:14.800 --> 24:20.960
of this met collection like this category of vases called yours whether we could find corresponding

24:20.960 --> 24:27.680
subspaces of what we're now referring to as foundation models like big an image net that kind

24:27.760 --> 24:33.760
of approximate our data set right so if we think about foundation models as a shared resource that

24:33.760 --> 24:38.880
ideally everybody would have access to and there were ways to think about contributing to then maybe

24:38.880 --> 24:45.680
these smaller problems become or can become a way of defining subspaces of those big models that we

24:45.680 --> 24:51.520
can interact with right rather than having to retrain a model and on our data set so we used

24:51.520 --> 24:56.240
GAN inversion here and instead of training a new model on just this category of viewers

24:56.800 --> 25:02.160
we asked whether we could embed each image that already existed into in the met collection

25:02.160 --> 25:08.560
and into the feature space of big GAN image net which happens to have a category for vases so we

25:08.560 --> 25:13.680
selected categories that were shared between image net and the met collection there are a handful

25:13.680 --> 25:19.840
about a dozen um and we maximized for each of those images the similarity between the met image

25:19.840 --> 25:24.400
and the big GAN image using a two-part loss right so we wanted them to be similar both at the pixel

25:24.400 --> 25:30.240
level and at the semantic level and we did that by looking at two different layers of a pre-trained

25:30.240 --> 25:36.000
res net as the embedding network so once we've embedded these models these images into big GAN

25:36.000 --> 25:40.720
we can then visualize the individual embeddings but we can also do something a little bit more

25:40.720 --> 25:46.640
interesting than just look at approximations of these images which might not be very good we can

25:46.640 --> 25:50.800
think about the underlying feature language that might have been learned and then look at

25:50.800 --> 25:56.720
interpolations between the existing images in the met collection I hear murmurs in the

25:56.720 --> 26:02.640
background if anybody has a question hit the chat you're super welcome to speak up um so next we

26:02.640 --> 26:08.160
look at interpolations between these existing images on the graph and we can create kind of

26:08.160 --> 26:15.040
hypothetical or dreamlike images that exist between the spaces of existing works in the collection

26:15.040 --> 26:21.360
and these are pretty interesting and beautiful and they allow us as I was mentioning to ask

26:21.360 --> 26:25.920
questions about what collaborations between geographical regions might have looked like

26:25.920 --> 26:30.720
right because we do have categorical information about where each image in the met collection

26:30.720 --> 26:36.800
came from it allows us to suggest new objects and the spaces between them so it allows us to

26:36.800 --> 26:42.400
interpolate and the other beauty of these kinds of executable models of culture is that it allows

26:42.480 --> 26:48.320
us to iterate on existing collections really rapidly um and evolve them forward and so we

26:48.320 --> 26:53.280
can kind of start to imagine archives of the future that would have embedded within them

26:53.840 --> 26:59.280
world models corresponding to the data set that exists at one point in the archive right so the

26:59.280 --> 27:04.640
archives could kind of evolve themselves forward and suggest future versions of their collections

27:04.640 --> 27:10.560
based on what's already been created and that this is again this was back in 2019 which is a

27:10.560 --> 27:16.240
long time ago in computer vision terms um but even just with with inversion into to began in the

27:16.240 --> 27:21.040
channel I was really impressed at the quality of the the images and the hypothetical objects that

27:21.040 --> 27:26.160
we could get for example here are a bunch of different generated teapots from the met latent

27:26.160 --> 27:31.760
space in the teapot category which again happened to be shared between ImageNet at that point um

27:31.760 --> 27:38.960
and the met collection and as I said we did have the the opportunity to exhibit this in the met

27:38.960 --> 27:45.760
which was absolutely wonderful um we projected a visualization of this latent space superimposed

27:45.760 --> 27:51.360
on a map of the met collection and allowed people visitors to the to the great hall to kind of step

27:51.360 --> 27:57.360
in to this latent space as projected onto the ground and explore the traversal of the spaces

27:57.360 --> 28:06.320
between works um and a projection behind them on the wall. We also made a web app version of all

28:06.400 --> 28:12.720
this that exists even though we we can't visit the met today um it's online at gen.studio if you

28:12.720 --> 28:17.680
want to go have a look after this and then all of the the code base is linked the github is linked

28:17.680 --> 28:23.600
at the bottom um if you want to check out any of that more specifically but again it places us

28:23.600 --> 28:28.960
kind of a different framing of latent space traversal than we're used to that I was interested

28:28.960 --> 28:33.840
in this project was to place us on you know we've gotten a lot further along in the video um

28:33.840 --> 28:39.200
you can go look at the at the website places us on a map between the objects when we're doing

28:39.200 --> 28:44.400
the interpolations right so we select an object to start now we land in the latent space of big

28:44.400 --> 28:50.400
gen close to that object and then we can move ourselves around on the map between objects and

28:50.400 --> 28:56.560
their embeddings in that latent space right and as we're moving physically in 2d space here online

28:56.560 --> 29:02.560
we can visualize what exists at that point in latent space and then we can find its nearest

29:02.560 --> 29:08.640
neighbor visually uh in the met collection and find what object in the existing collection

29:08.640 --> 29:13.360
is most similar to the hypothetical work that we discovered in the interstices between two

29:13.360 --> 29:20.400
existing works um so give that a look and this project lives on today um and a couple of different

29:20.400 --> 29:25.760
forms I'm still working with the artist Matthew Richie he was a collaborator uh with us on the

29:25.760 --> 29:34.160
met project um on a couple of different tendrils of of this work where we're asking all right so

29:34.880 --> 29:39.520
we can model projections of existing images in the met in the met collection by finding their

29:39.520 --> 29:47.280
embeddings in some kind of large foundation model but now in 2021 we have things like StyleGAN ADA

29:47.280 --> 29:51.760
that can can train on smaller data sets and do reasonably well in approximating data sets that

29:51.760 --> 29:56.960
would correspond to a single category in the met collection so we've done that um we've trained

29:56.960 --> 30:04.400
these models on sketches um Babylonian cuneiform tablets Japanese watercolors and some 18th century

30:04.400 --> 30:11.520
European landscapes among other things and have individual models correspond to each of these

30:11.520 --> 30:16.880
genres within the met collection and then we've been working with a friend in New York who has a

30:16.880 --> 30:23.440
robotic oil painter and can actually create layered paintings of really short walks in latent

30:23.440 --> 30:29.520
space along different dimensions in this model so think about physically visualizing some of the

30:29.520 --> 30:34.480
durability work you've looked at in this course right could we make time paintings of really

30:34.480 --> 30:41.920
short walks in latent space by superimposing robotic paintings of the visualized image kind

30:41.920 --> 30:47.360
of at different points along that walk so that's that's being exhibited right now at UNT

30:47.360 --> 30:50.880
in their contemporary art gallery let's see I've got a question in the chat room

30:53.200 --> 30:59.040
oh it's just a compliment I will take it at any point yeah I think

30:59.040 --> 31:05.120
can you please read it yes uh someone mentioned that this is a creative reason one of the most

31:05.120 --> 31:10.880
creative reasons they've seen to do latent space interpolation since it scans yeah I think that

31:10.880 --> 31:16.160
I had a slide a moment ago if you want to rewind in the recording of this suggesting that kind of

31:16.160 --> 31:21.840
part of the advent of using GANs to model kind of large databases of creative work is that they

31:21.840 --> 31:27.360
allow us to do a couple of things right that interpolation and that iteration and in cases

31:27.360 --> 31:31.760
where you can't write down a feature language underlying a set of works because you don't

31:31.760 --> 31:38.160
know our priority what it is you can imprint that or you can learn something of that in a deep

31:38.160 --> 31:45.360
generative model right and then you can collaborate with that and hypothesize what might lie on a

31:45.360 --> 31:52.080
graph of human creation if we presume that any creation artistic creation at some point in

31:52.080 --> 31:59.920
historic time is if you think about it as the manifestation of a point on kind of a sea of

31:59.920 --> 32:04.960
cultural influences and multi-generational practice iterative practice that's been shared

32:05.040 --> 32:11.040
between peoples and generations and the creation of a single work is the enactment of that process

32:11.040 --> 32:17.360
at some point in space it's natural to think of that in some sense as a model that we can capture

32:17.360 --> 32:22.960
in a latent space where we're manifesting some part of structured space at some moment but this

32:22.960 --> 32:29.280
allows us to iterate on that which I argue is similar to some historic processes of iteration

32:29.280 --> 32:36.400
and collaboration across groups of people really quickly right um so I've been trying to take this

32:36.400 --> 32:43.600
a little further now and ask all right we can make paintings of short walks in latent space we can

32:43.600 --> 32:48.720
hypothesize objects that might have existed but we don't think they ever did but we still don't

32:48.720 --> 32:55.760
know much about these models even if we train StyleGAN 2 on a set of 2,000 paintings in the

32:55.760 --> 33:01.200
net collection uh you've probably seen some of the interpretability work adjacent to what Ali

33:01.200 --> 33:06.560
has shared or David Bao's work so that style of thinking we don't know anything about this Japanese

33:06.560 --> 33:11.680
watercolor model like what do its individual neurons represent is there a neuron for trees

33:11.680 --> 33:17.840
well what is a tree here it's some brushstrokes we recognize as a tree but it's not something that

33:18.640 --> 33:24.480
BigGAN trained on image that would necessarily recognize as a tree maybe more simply kind of in

33:24.480 --> 33:30.560
the in the steerability context what do dimensions in the latent space of a model like this

33:30.560 --> 33:38.400
correspond to right sure we can find things like zoom and 3d rotation because we can name those

33:38.400 --> 33:43.120
transformations and then find directions that maximally correspond to them using that kind

33:43.120 --> 33:47.440
of steerability technique there are all sorts of other directions like the ones we're visualizing

33:47.440 --> 33:53.200
here that certainly have some affective meaning to the viewer that we don't know what they are in

33:53.200 --> 33:58.720
the models terms or in the viewer's terms so at this point in this project we're thinking about

33:58.720 --> 34:04.320
starting to name and understand dimensions underlying generative models trained on

34:04.320 --> 34:09.200
bodies of artistic work from museum digital collections not only limited to the met but

34:09.200 --> 34:15.600
around the world uh and our motivation here is to kind of create these alternate and imaginary

34:15.600 --> 34:20.800
histories of art built from unique latent walks that we can visualize in real time with this painting

34:20.800 --> 34:27.040
or computationally and then maybe understand something about aspects of picture language

34:27.040 --> 34:33.600
that might be shared across you know vastly different genres so Babylonian cuneiform tablets

34:33.600 --> 34:40.240
transformed from numeric to symbolic and image-based at a very particular point in history and can we

34:40.240 --> 34:46.880
find a dimension in style GAN trained on a very different genre of art that corresponds to a

34:46.960 --> 34:52.160
similar kind of transformation and as such can we build up kind of a picture language that would

34:52.160 --> 34:58.240
correspond to diverse forms of art making right that you might not see in any of these different

34:58.240 --> 35:03.440
categories of digital images on an archive but we might start to appreciate once we can investigate

35:03.440 --> 35:12.560
them by training deep generative models on them let's get back to great okay so when we're thinking

35:12.640 --> 35:17.680
about this intersection we've seen one example of modeling the structure underlying creativity at

35:17.680 --> 35:23.760
scale and i've done other projects and you can find many examples online both of my work and

35:23.760 --> 35:28.720
other peoples of trying to do this not for creativity at scale but for individual instances

35:28.720 --> 35:35.680
of individual artists and modeling either the style or the processes of individual art making

35:35.680 --> 35:42.400
techniques so all of these are kind of flavors of starting to imprint or grok or understand the

35:42.400 --> 35:47.680
structure underlying creativity but not symbolically right so we don't we don't know how to interpret

35:47.680 --> 35:52.480
these models even though we can visualize them and create really interesting hypothetical objects

35:52.480 --> 35:57.680
that might be indistinguishable either from existing work or from one artist's particular style

35:59.120 --> 36:04.400
we can also think about these models as a tool themselves for collaboration both in their creation

36:04.400 --> 36:10.000
and iteration with others who contribute to their models and with the models themselves

36:10.000 --> 36:15.440
which as i described represent kind of executable versions of collective cultural structure we

36:15.440 --> 36:21.200
permit permit ourselves to think about them that way or facets of kind of a global creative identity

36:22.240 --> 36:26.800
but as i mentioned now we're at a point with tools and computer vision where we can start to ask

36:27.600 --> 36:33.840
what rep representations actually underlie these models trained on artworks that are themselves

36:33.920 --> 36:37.760
executable versions of some collective cultural structure right well what is the structure what's

36:37.760 --> 36:44.480
going on under the hood do they correspond to dimensions that we find meaningful when we look

36:44.480 --> 36:50.240
at visual scenes and so in the next part of the talk i'll share a couple maybe more technical

36:50.240 --> 36:56.000
projects that explore specific ways that humans can interact with generative models

36:56.640 --> 37:01.280
in order to maybe learn something about human vision as well right so can we build

37:01.760 --> 37:07.440
shared vocabularies that help us interpret dimensions underlying these models by designing

37:07.440 --> 37:13.120
experiments that allow us to visualize and interact with images and latent walks like you've been

37:13.120 --> 37:19.040
seeing i'll pause here because i need a sip of water and i'll keep an eye on the chat in case

37:19.040 --> 37:28.480
anyone has any questions before we go on

37:33.760 --> 37:39.440
all right looks like we are question free so far five more seconds

37:42.880 --> 37:48.160
i guess i have a question yeah so this might be talked about later but i was wondering a little

37:48.240 --> 37:54.480
bit about like in your research and kind of this field how much of like human interaction is like a

37:54.480 --> 37:59.760
big part of it and kind of like the human coming in and saying uh how they think about something

37:59.760 --> 38:04.800
and see where that agrees with the computer or like kind of like where that role is played

38:06.240 --> 38:09.920
wonderful question so these kind these kinds of high-level questions that

38:09.920 --> 38:14.880
get it some experiential component or design component of the worker i think really useful

38:14.880 --> 38:19.440
ask more of them i'll tell you for different projects what that looks like and in the next

38:19.440 --> 38:25.440
section of work it's going to be really obvious because there's human annotations but for this

38:25.440 --> 38:31.920
project so the human would come in here you know we train models on datasets of art selected from

38:31.920 --> 38:36.800
the met collection and these are small and these are subsets and they were gathered by

38:37.520 --> 38:42.960
matthew richie and myself going through different genres in the digital collection of the met online

38:43.920 --> 38:49.520
and like hand selecting images from those different genres right representative images of

38:49.520 --> 38:56.160
different categories of work or maybe in a less fine grained way all images under some designation

38:56.160 --> 39:02.320
so japanese watercolors between the 17th and 19th centuries so we made that selection and

39:02.320 --> 39:07.760
tried training these models on a bunch of different such selections and decided which ended up you

39:07.760 --> 39:13.040
know with so few examples providing at least a representative sample of the kind of work that

39:13.040 --> 39:20.960
we know we saw there right and then here the selection of like walks through latent space

39:20.960 --> 39:23.920
so think of those in the same way you've been thinking about the steerability walks

39:24.800 --> 39:29.840
they were very arbitrary so that was a completely human selected so it's a kind of a different

39:29.840 --> 39:36.400
approach to interpretability where it's steered by the human eye right we're not doing it automatically

39:36.400 --> 39:42.400
and we're not doing symbolic it symbolically we don't know what these correspond to but that's

39:43.440 --> 39:47.600
trying some arbitrary walk through latent space trying many of them and then the human then

39:47.600 --> 39:52.640
selecting what to them felt like an artistic expression this is an art exhibit and then

39:52.640 --> 39:58.080
in the next step we'll ask how can we do that in a more systematic way and start to build

39:58.080 --> 40:02.480
a language corresponding to what those different walks could be a language that's shared by humans

40:02.480 --> 40:08.400
and that takes at least right now a lot of human a lot of human interaction

40:09.200 --> 40:12.240
you could think about ways to automate that we'll talk about that in a second

40:13.440 --> 40:18.960
but with any kind of human interaction it's nice to preserve the opportunity for direct

40:18.960 --> 40:23.680
engagement with models rather than intermediation by a captioner or something like that because

40:23.680 --> 40:28.880
then you could imagine using your technique on different subsets of humans right on different

40:28.880 --> 40:33.840
kinds of experiences so you might imagine getting an art historian to label and select

40:33.840 --> 40:38.000
different walks through latent space here corresponding to very nuanced changes in the

40:38.000 --> 40:43.280
development of Babylonian like cuneiform tablets right that a captioner couldn't recognize I

40:43.280 --> 40:48.320
couldn't recognize so you might want to be able to pull different kinds of humans into the loop

40:48.320 --> 40:53.680
at different times to engage in ways that kind of use their knowledge to create a unique synthesis

40:53.680 --> 40:59.920
with a generative model so that's what engagement looked like here and then with this next project

40:59.920 --> 41:06.560
it'll be super it'll be super clear and I'll make sure to speak specifically to that so thank you

41:06.560 --> 41:14.880
yeah cool so next this is probably a summary of what you've seen so far in your IAP course so

41:14.880 --> 41:20.160
there's a lot of different work on discovery of interpretable directions in the latent space

41:20.160 --> 41:25.120
of different generative models right and we can steer images along those dimensions to create

41:25.120 --> 41:30.320
interpretable transformations that allow us to interact creatively with deep generative models

41:30.320 --> 41:38.240
right here we are deep learning for creativity but a lot of these examples presume what concepts

41:38.240 --> 41:43.120
we're searching for in the latent space and in fact they do that really explicitly right we

41:43.120 --> 41:48.880
will pre-define a zoom transformation and then maximize the similarity between some transformation

41:48.880 --> 41:54.000
in the latent space and a zoom transformation as applied to some image maybe you've experimented

41:54.000 --> 42:00.000
with code for doing that but that presumes we know we're looking for zoom in the first place

42:00.000 --> 42:04.400
what if we find ourselves looking out into more you know uncharted waters so to speak

42:05.360 --> 42:11.040
here we ask how we can learn kind of a vocabulary of visual concepts maybe one that you would apply

42:11.040 --> 42:16.160
to those style gains we just saw train on the net images right we don't maybe we could look for zoom

42:16.160 --> 42:19.920
but maybe there's all sorts of more interesting transformations we could do to those images

42:19.920 --> 42:24.240
but we don't know what they are yet how can we learn a vocabulary of visual concepts rather

42:24.240 --> 42:30.000
than pre-define them or labeling them after the fact so there are a variety now of unsupervised

42:30.000 --> 42:36.240
methods for distilling these kinds of transformations in latent space that find principal components

42:36.240 --> 42:40.240
of feature space of different layers the models activation maybe you've played around with methods

42:40.240 --> 42:45.600
like GAN space that search for and rank where the largest principal components of the feature space

42:45.600 --> 42:50.240
which do provide us with interpretable transformations but they're labeled after the

42:50.240 --> 42:56.480
fact so we don't know if they're meaningful to humans kind of in their genesis but we can we

42:56.480 --> 43:00.960
can describe them right by providing labels to them another point where the human kind of comes

43:00.960 --> 43:07.520
in the loop but we want to see if we can build in human vision to the discovery process right so

43:07.520 --> 43:13.520
to supervise it but to not pre-commit to what kinds of concepts we're searching for so in this

43:13.520 --> 43:18.960
project we're trying to build or define a method for building a visual concept vocabulary for an

43:18.960 --> 43:25.360
arbitrary GAN latent space so to put it more specifically we want to learn embeddings d maybe

43:25.360 --> 43:31.200
you've called this w we want to learn some kind of walk in the latent space z if again we'll focus

43:31.200 --> 43:37.840
on big GAN here of transformations that are salient to us in visual space and we can't define

43:37.840 --> 43:44.320
an objective and optimize our d our walk to produce a transform in x in the image because we

43:44.320 --> 43:48.560
want to learn the vocabulary concepts rather than pre-commit to them and we would have to pre-commit

43:48.560 --> 43:53.360
to what that objective is right in order to optimize d so we're going to take a different

43:53.360 --> 44:01.280
approach and instead sample the space of salient or possible transformations for some given point

44:01.280 --> 44:07.680
in space for some given z and then use those sample directions as a screen so to speak onto

44:07.680 --> 44:12.640
which we can project human perceptual judgments so that's a little bit of a gratuitous metaphor but

44:12.640 --> 44:17.200
maybe a useful way of thinking about it and then we'll we'll disentangle the concepts that are

44:17.200 --> 44:22.960
projected onto that screen into a vocabulary of open-ended compositional visual concepts

44:24.560 --> 44:30.960
and what we're interested in here is the overlap between what's represented inside a model

44:30.960 --> 44:36.240
so some deep features in a model's representation and concepts meaningful to humans in visual

44:36.240 --> 44:41.280
seeing understanding we're asking how we might start to define although not completely but

44:41.280 --> 44:47.200
start to define a shared vocabulary between the two or for a given model determine what lies in

44:47.200 --> 44:53.120
that set overlap and I don't have to dwell too long on a lot of the specifics here it's all

44:54.160 --> 44:59.120
online at that URL if you want to read the paper but as I mentioned the first thing we're going to

44:59.120 --> 45:05.360
do is generate a set of sample images that produce minimal meaningful transformations in images

45:06.000 --> 45:10.000
and then humans come in the loop again we're going to ask them to label them but here we're

45:10.000 --> 45:15.280
forming the basis for the data set that we'll build our vocabulary off of and we want to keep in

45:15.280 --> 45:20.720
mind that we want a vocabulary in the end that is both diverse so corresponding to a lot of

45:20.720 --> 45:25.680
different changes that you can produce in an image and specific where a single transformation

45:25.680 --> 45:32.320
corresponds quite reliably to one visual change across viewers so we do that by defining

45:33.120 --> 45:39.120
mutually orthogonal what we call layer selective directions and these minimize change in the feature

45:39.120 --> 45:45.200
representation at some layer of big care and at some layer we'll call it layer l and this allows

45:45.200 --> 45:51.120
us to capture relatively focused changes because we hold constant how much the representation

45:51.120 --> 45:56.160
can change at some layer and we do that for different layers to capture changes at different

45:56.160 --> 46:01.760
levels of abstraction so as you can see layers closer to the image output control or fine grained

46:01.760 --> 46:06.480
aspects of the image like the color of the walls and the bedspread and as we get closer

46:06.480 --> 46:11.280
back to the latent space we're allowed to make kind of more higher level changes in things like

46:11.280 --> 46:17.760
zoom and perspective of the scene and its composition so what objects are present so here we have a

46:17.760 --> 46:22.800
base set of minimal meaningful transformations that capture changes in images at different levels

46:22.800 --> 46:29.280
of abstraction we're going to ask people to label them because we don't know what's going on visually

46:29.280 --> 46:34.560
in these scenes right so we started at a pretty small scale with just four categories in the

46:34.560 --> 46:40.560
places data set and looked at big and trained on image net and places we'll just talk about places

46:40.560 --> 46:46.880
here and visualized a handful a few thousand of these directions per category so in each of four

46:46.880 --> 46:52.720
categories looked at cottages medinas so uh street marketplaces kitchens and lakes a mix of

46:52.720 --> 46:58.560
indoor and outdoor scenes and then asked people to just simply describe the overall transition

46:58.560 --> 47:03.920
that they saw when these directions were applied to different randomly sampled starting points in

47:03.920 --> 47:09.760
the latent space right so one direction might take this cottage to this snowy cottage and change

47:09.760 --> 47:15.280
something about the sky and change the snow so these these changes are still complex we can

47:15.280 --> 47:20.560
recognize that it's the same scene and we can describe in simple language what's going on

47:20.560 --> 47:24.720
but they're they're not disentangled yet right one direction might correspond to a number of

47:24.720 --> 47:30.240
different visual changes so we did a little preprocessing to capture you know what kinds of

47:30.240 --> 47:37.280
concepts are associated with each transformation and then we decomposed those annotated directions

47:37.280 --> 47:42.480
into a visual concept vocabulary consisting of single directions labeled with single words

47:42.560 --> 47:48.720
we formulated that as a linear regression and then solved for the embeddings of individual

47:48.720 --> 47:53.200
concepts in the latent space of our begin and then we can basically read those off

47:54.560 --> 47:59.840
of our matrix e and then transform the images by manipulating them some amount along those

47:59.840 --> 48:04.560
visual concept directions happy to talk more details about that if anybody's specifically

48:04.560 --> 48:11.120
interested or you can check out the paper itself we found over 2000 concepts this way

48:11.120 --> 48:14.960
corresponding to lots of different types of visual changes so we can reproduce

48:14.960 --> 48:21.760
transformations like zoom and rotation things like color but we also get kind of a unique

48:21.760 --> 48:27.520
set of concepts corresponding to aspects of scenes like their mood for instance there's a

48:27.520 --> 48:34.080
direction in latent space of big and that makes outdoor marketplaces more festive and here we

48:34.080 --> 48:38.160
see applying that direction to an example marketplace and it rolls out a red carpet

48:38.160 --> 48:45.600
hangs some flags and brings a lot of people into that market we can visualize kind of a

48:45.600 --> 48:51.440
a sampling of these directions each applied to two different images in different categories

48:51.440 --> 48:57.920
so some directions make cottages more manicured add arches to marketplaces add shadows or make the

48:57.920 --> 49:03.200
whole scene blue and we see directions like this that generalize across all of the categories

49:03.200 --> 49:09.600
it began that we looked at you can check out the lake category we can add sunsets but also do

49:09.600 --> 49:15.440
kind of scene specific things like add reflections to water or make a lake scene foggier make a

49:15.440 --> 49:21.840
kitchen more inviting or more modern and again we didn't have to pre-specify what exactly modernity

49:21.840 --> 49:28.000
would entail when applied to a kitchen we learned that through sampling what humans associate

49:28.000 --> 49:33.840
with a transformation that was sampled randomly right uh the humans labeled that as modern and

49:33.840 --> 49:39.520
then we disentangled the specific direction in latent space corresponding to that single concept

49:39.520 --> 49:46.240
word and once it's isolated we can apply a modern transformation and know that it corresponds to

49:46.240 --> 49:53.360
what viewers found to represent modernity in a kitchen well I said we know that it corresponds to

49:53.920 --> 49:58.800
what viewers see as more modern but we don't know that for sure right we still need to ask

49:58.800 --> 50:04.800
questions like how generalizable are these directions do they compose right can we add

50:04.800 --> 50:10.400
a festive direction to eerie and get something that's both scary and festive right or could we make

50:11.040 --> 50:16.000
a kitchen both more modern and inviting so we asked those questions in a series of behavioral

50:16.000 --> 50:21.680
experiments that I left for you to check out in the paper itself so we won't in the interest of

50:21.680 --> 50:26.960
time go through those here but we do find that these directions are composable and they're generalizable

50:26.960 --> 50:32.800
across categories so there are some cases where we can even add a concept that was learned in a

50:32.800 --> 50:38.640
single category to a different category for instance making a cottage more festive right or

50:38.640 --> 50:44.000
adding snow to a marketplace even though that's not traditionally seen there we ran a set of

50:44.000 --> 50:49.120
behavioral experiments evaluating the extent to which this is successful and isolating a couple

50:49.200 --> 50:56.560
of few specific cases where it fails okay so this this wraps up this method it's at a point

50:56.560 --> 51:02.160
now where we're trying this with some of the the art models that I discussed previously right so

51:02.160 --> 51:08.480
this was still just applied to big and trained on real-world images trained on ImageNet but you

51:08.480 --> 51:14.240
can imagine using a similar similar method to find dimensions of visual interest that are also

51:14.320 --> 51:21.280
meaningful to humans in the latent space of a model trained on art images and so decompose

51:21.280 --> 51:26.720
future languages underlying different genres of art into something describable so that we

51:26.720 --> 51:32.160
can make concerted manipulations to images sampled either from foundation models that

51:32.160 --> 51:38.080
correspond to approximations of real-world images or to models trained on on archives of art images

51:38.080 --> 51:45.200
themselves I'll pause here for any questions about this there's associated code also available

51:45.200 --> 51:53.120
on that project page I linked have a quick question please um was there a reason you chose

51:54.240 --> 52:01.440
big GAN over starting with like style GAN for this type of work uh no um just a couple of

52:01.440 --> 52:06.080
different code bases that already existed um and people that had worked on big GAN for

52:06.080 --> 52:10.160
like GAN dissection we had an easy way to dissect big GAN and hypothesize what kind of

52:10.160 --> 52:14.560
things might be there uh so a lot of the GAN dissection work started with big GAN and so I

52:14.560 --> 52:19.360
was picking up where that left off and asking if we could find like style vectors that corresponded

52:19.360 --> 52:25.360
to scene level transformations instead of individual neurons um but I have extended this

52:25.360 --> 52:31.440
to style GAN outside of the paper it's just not got it it's here yeah the method's pretty I mean

52:31.440 --> 52:36.320
there are a couple of different small changes you have to make um but the method is pretty model

52:36.320 --> 52:42.000
agnostic just like defining a set of certain directions that samples the latent space in

52:42.000 --> 52:47.360
kind of minimal ways and the the method I described here is definitely not the only one you could use

52:47.360 --> 52:51.440
for that right um you could sample them by just finding the principal components of the feature

52:51.440 --> 52:55.440
space or you could sample them randomly right you could just find two points in latent space

52:55.440 --> 53:00.320
interpolate and then get people to label what's going on there um we tried a lot of these different

53:00.320 --> 53:06.000
methods uh and found that if you if you make random interpolations between two randomly

53:06.000 --> 53:10.960
sampled points then there's just so much going on in the scene that there's not a lot of inter

53:10.960 --> 53:15.440
observer agreement in how people annotate what they see there's just too much going on so we

53:15.440 --> 53:20.800
need to isolate specific changes that's why we developed that layer selective method for isolating

53:21.520 --> 53:26.880
minimal changes um but what if we used kind of the the principal component method right or used

53:26.880 --> 53:32.400
something like GAN space um there we found that the principal components of the model's feature

53:32.400 --> 53:37.520
space aren't necessarily the most interesting to humans so we might get a ton of different types of

53:37.520 --> 53:42.320
rotating the scene but not a lot of different changes of mood or changes in color up there in

53:42.320 --> 53:47.600
high-ranked principal components um so that's where that method came from but it's agnostic to the

53:47.600 --> 53:53.920
set of directions and pretty model agnostic uh the annotation is another place where humans intervene

53:53.920 --> 54:01.120
here to to tie in that last question um but you can imagine trading a captioner on a label

54:01.120 --> 54:05.200
data set like this right a little larger than the one we collected so we're thinking about doing

54:05.200 --> 54:10.720
something like that uh but preserving the human annotation does allow annotation you know in the

54:10.720 --> 54:17.280
art context by experts as I mentioned so you might want to be able to do this at scale for a brand

54:17.280 --> 54:23.040
new model and just have automatic annotations you use something like clip right for the kinds of

54:23.040 --> 54:28.640
transformations you would see inside but preserve the opportunity for experts to to annotate kind

54:28.640 --> 54:36.720
of specialized smaller trained models and there are results too from big ganttring on a couple of

54:36.720 --> 54:48.000
different data sets if you're interested um I have a question regarding like the choice of

54:48.960 --> 54:57.360
uh n in terms of annotations um so how did you arrive at this number and how are you I mean

54:58.000 --> 55:04.080
how do you know like what number is kind of sufficient yeah good question um so I assume

55:04.080 --> 55:09.040
you mean the total number of images we needed to annotate and not the total number of annotations per

55:09.040 --> 55:19.520
image which end you mean I can talk oh I see I mean either yeah well okay so at both levels uh for

55:19.520 --> 55:26.080
the directions themselves we needed to collect at least two annotations to be able to measure

55:26.080 --> 55:32.480
intersubject agreement right uh we want to see if some direction is consistently producing

55:32.480 --> 55:38.720
meaningful similar annotations across annotators we need at least two people to annotate them um

55:38.720 --> 55:44.960
so for all the directions we evaluated we had two annotators label them and measured the interanitator

55:44.960 --> 55:51.840
agreement using a couple of different metrics blue and burnt scores um but for a subset of

55:51.840 --> 55:57.360
those we had 10 annotators annotate them and just had a look at interanitator agreement across a

55:57.360 --> 56:03.120
slightly larger group uh for expense reasons we didn't do that for for all the directions because

56:03.120 --> 56:07.040
it really wasn't necessary things didn't change that much and even in that subset when we went

56:07.040 --> 56:14.400
from two to ten per uh per direction and then for the number of directions that we chose to

56:14.400 --> 56:24.240
visualize it was not a very principled decision I'm afraid um we chose I think 64 z uh per category

56:24.240 --> 56:28.560
and then a bunch of different minimal meaningful directions for them corresponding to I think

56:28.560 --> 56:33.280
the same number of principal components that we looked at in the GAN space papers so maybe the

56:33.280 --> 56:41.040
top 20 in each category so it was a bit ad hoc that decision um the the things that's going to change

56:41.040 --> 56:48.640
we can distill vocabularies using this method for any size of annotation library right uh which is

56:48.640 --> 56:52.320
one of the one of the beauties and one of the things that gives itself to to some of these more

56:52.320 --> 56:59.840
ad hoc decisions um we're doing it analytically right if we go back to this we're actually like

56:59.920 --> 57:05.840
reading off we're solving for the embedding matrix um of word embeddings in latent space

57:05.840 --> 57:10.080
of concept embeddings so we could do this with like just a couple of directions

57:11.280 --> 57:15.920
if you only had one annotation per concept it only appeared once then you're just going to get

57:16.480 --> 57:23.040
for that direction um so as you increase the vocabulary as you increase the sample size you're

57:23.040 --> 57:28.000
probably going to get a richer vocabulary but it's still possible to do on a vocabulary of this size

57:28.000 --> 57:34.240
um so we're deciding now whether it makes sense to scale this up and collect like a number of

57:34.240 --> 57:39.280
annotations where it would be possible like I said to to train a captioner on them to be able

57:39.280 --> 57:45.520
to automatically label these directions rather than have humans do it so part of it is constrained by

57:45.520 --> 57:49.680
the tractability of experiments on mechanical Turk right how many reliable annotations you

57:49.680 --> 57:59.920
can get in some period of time awesome uh thank you yeah these are really useful questions these are

57:59.920 --> 58:08.800
great um kind of along those lines more of a random question for the printer like the single words

58:08.800 --> 58:13.520
for the labels yeah was it kind of agreed upon earlier like kind of what words you'd use because

58:13.520 --> 58:18.400
like for festive maybe someone would say lively or for inviting you'd say welcoming is there like

58:18.400 --> 58:27.040
kind of a similarity score for those words or really good question um no so this is it's only

58:27.040 --> 58:32.160
preprocessed with like a little bit of limitizing so we collapse different endings people might be

58:32.160 --> 58:37.600
using our different verb conjugations onto single verbs uh but festive would have a different

58:37.600 --> 58:43.840
direction from lively uh kind of a next step in post-processing that we've talked about but

58:43.840 --> 58:49.600
haven't yet done um is to just collapse across like wordnets and sets right so you could use

58:49.600 --> 58:55.120
something like that to find synonyms of festive and then approximate one direction for lively and

58:55.120 --> 59:00.480
then be able to break it down into something maybe more fine-grained um but there were no

59:00.480 --> 59:06.160
kind of heuristics or standards for the annotators except you know they did they did a practice run

59:06.160 --> 59:11.520
and looked at a couple of different examples and were asked to describe an overall transformation

59:11.520 --> 59:16.320
that captured changes at lots of different kind of levels of abstraction we can look at the specific

59:16.320 --> 59:21.920
what did we tell her that's on here yeah how would you describe the overall transition changes in

59:21.920 --> 59:26.240
mood changes in objects or features of the scene don't mention your describing images so standard

59:26.240 --> 59:30.320
kind of turk boilerplate just address the content what you see and then they could look at some

59:30.320 --> 59:35.680
samples and then after they did a practice run they did the annotations um so any interanitator

59:35.680 --> 59:41.680
agreement is just based on their word choice which in some sense is a raw window into perception

59:41.680 --> 59:46.560
but in some sense that's bullshit and there's going to be a lot of noise there uh and we did see

59:46.560 --> 59:52.400
that reflected when we used I don't have this on these slides um but when we use blue scores to

59:52.400 --> 59:58.160
to measure interanitator agreement so when we use these layer selective directions to generate these

59:58.160 --> 01:00:04.400
kinds of transformations if we get 10 people to annotate each transformation people might use

01:00:04.400 --> 01:00:09.440
somebody might say eerie somebody might say spooky right somebody might say scary to describe the

01:00:09.440 --> 01:00:15.120
sky uh that comes up as like quite different when you look at some methods of evaluating

01:00:15.120 --> 01:00:20.400
interanitator agreement so we used first scores as well that evaluate the semantic similarity

01:00:21.440 --> 01:00:28.240
instead of just literal correspondence words and found that annotations of these kinds of

01:00:28.240 --> 01:00:32.800
directions performed a lot higher when we looked at semantic similarity of annotations as opposed to

01:00:32.800 --> 01:00:39.600
just um just word based so there's definitely reason to start trying to collapse like that when

01:00:39.600 --> 01:00:44.720
we look at the vocabulary too but we haven't yet in some sense it's it's kind of beautiful because

01:00:44.720 --> 01:00:50.960
you can see all of the different words that people used to describe changes um but you'd get a lot

01:00:50.960 --> 01:00:56.800
more power right if you could combine annotations for festive and lively and vibrant under one

01:00:56.800 --> 01:01:06.880
umbrella bit of a trailer oh yeah thank you so much yeah any other high or low level questions

01:01:06.880 --> 01:01:13.920
inter just have a maybe one or two more things not much so ask away if you do I think more more

01:01:13.920 --> 01:01:20.800
of a higher level question I remember Ali in the first lecture um right you drew you had this

01:01:20.880 --> 01:01:26.080
visualization of like two points in the latent space and you know a last function that would

01:01:26.080 --> 01:01:32.720
steer like from one or trajectory one to the other but it was like something more like a curve

01:01:32.720 --> 01:01:38.320
or something non-linear um right and you mentioned with Gannon version if you just interpolate like

01:01:39.280 --> 01:01:45.760
draw a straight line between two points you have like all sorts of things happening I was wondering

01:01:46.480 --> 01:01:55.680
if there's like a I guess almost like a like a and I guess unsupervised not a random walk but a

01:01:55.680 --> 01:02:05.520
walk that would I guess lead to less perturbations I guess in in terms of like features I mean

01:02:07.280 --> 01:02:13.600
does it make sense uh yeah we I really wanted to do that for this project um maybe Ali can speak

01:02:13.600 --> 01:02:20.000
a little bit more about about his work there maybe after we stop this recording but um linearization

01:02:20.000 --> 01:02:26.240
of this is a huge over oversimplification um and that would be one of exactly what you describe

01:02:26.240 --> 01:02:31.840
as one of the things I'm most keen to try is taking non-linear walks uh through any of these

01:02:31.840 --> 01:02:39.440
subspaces um so very on point question haven't done it you should try and do it um but describing

01:02:39.440 --> 01:02:45.200
like this the semantic structure of latent space the semantic topology if you'll permit me that

01:02:45.760 --> 01:02:52.000
is a really interesting question um because even the visual meaning corresponding to

01:02:52.000 --> 01:02:59.360
some of these adjectives some of these words is not regularized or normalized in the latent space

01:02:59.360 --> 01:03:06.320
itself so if I take five steps in the festive direction it might take me five steps to get

01:03:06.320 --> 01:03:13.200
anything that will start to register to me as festive um but the walk size for a correspondingly

01:03:13.200 --> 01:03:18.960
large visual change so to speak in a different direction could be very different um so some

01:03:18.960 --> 01:03:24.080
transformations like making an image black and white this is anecdotal but you only have to go

01:03:24.080 --> 01:03:30.080
like one step in that direction and then we'll visualize the change almost immediately uh so

01:03:30.080 --> 01:03:36.960
we're not kind of we're not walking around in like a perceptually normalized space so to speak um

01:03:36.960 --> 01:03:42.160
and there hasn't been to my knowledge a lot of work that's addressed that everything's been a

01:03:42.160 --> 01:03:50.000
little bit at hawk um so thinking about semantic topology subspaces non-linear versus linear paths

01:03:50.000 --> 01:03:54.480
and how we can think about kind of the concept mesh underlying latent space for different

01:03:54.560 --> 01:04:01.120
generative models is extremely interesting to get to be a really cool area to do some working

01:04:01.680 --> 01:04:08.960
cool thank you yeah let's ask Ali about that figure once we pull off here um I've got one more

01:04:08.960 --> 01:04:14.640
thing to show you a quick example to hopefully spark more discussion unless anybody has anything

01:04:14.640 --> 01:04:27.200
specific about this project we can always come back all right oh geez well last thing I'm going to

01:04:27.200 --> 01:04:35.200
show you uh is still a beta and uh it's very it's very early and it's even thought development

01:04:35.200 --> 01:04:40.880
but it captures something um that I think is deeply interesting uh and I think might be interesting to

01:04:40.880 --> 01:04:48.480
you all um so the former method that I showed you for building shared vocabulary between humans

01:04:48.480 --> 01:04:55.680
and models relies heavily on language right and so we get some direction and we're able to share that

01:04:56.320 --> 01:05:03.040
between people and even to repeatedly use it to steer through model space um because we've given

01:05:03.040 --> 01:05:09.200
it a label right we've used language and you might even argue that you know that's constraining the

01:05:09.200 --> 01:05:15.440
space of what people can recognize in those initial sample directions because there might be some

01:05:15.440 --> 01:05:21.600
genus or quad aspects of of images that we don't really have words for um but are still like really

01:05:21.600 --> 01:05:29.040
recognizable or perhaps the verbal you know that the words you would use to describe something are like

01:05:29.920 --> 01:05:34.400
quite complex and you wouldn't type that into an annotation like on mechanical Turk maybe you'd

01:05:34.480 --> 01:05:40.320
want to describe the sky as like the sky you saw at your grandmother's house the day she passed away

01:05:40.320 --> 01:05:46.320
or some flowers as effervescent like latte foam or a sparkling drink but you're not going to type

01:05:46.320 --> 01:05:50.560
that into mechanical Turk and there's not a single word concept to capture it so that's

01:05:50.560 --> 01:05:55.920
going to get lost in the method I described and lost in a lot of kind of standard either annotation

01:05:55.920 --> 01:06:03.840
based or uh kind of hard coded direction search so I wanted to experiment with a way to capture and

01:06:03.840 --> 01:06:09.840
learn um directions without language and this is like deeply inspired by the steerability work

01:06:10.400 --> 01:06:16.000
of all these so you'll see a method here that is is similar to that in some sense

01:06:17.360 --> 01:06:23.040
but we're allowing the human to define the transformation that they want rather than

01:06:23.040 --> 01:06:28.160
pre-defining say a zoom or rotation transform using an algorithm we're allowing humans to come

01:06:28.160 --> 01:06:34.960
into the loop and define that transformation purely visually by interacting with very very

01:06:34.960 --> 01:06:41.040
small batches of images sampled from latent space or feature space at some layer and sort them into

01:06:41.040 --> 01:06:48.480
classes corresponding to some visual feature its presence or its absence and this provides a pipeline

01:06:48.480 --> 01:06:54.480
where users can steer just like in steerability work along dimensions that they discover however

01:06:54.480 --> 01:06:59.360
that they define and they define them purely visually so labeling what happened just as a

01:06:59.360 --> 01:07:04.640
matter of convenience but they're discovered um through vision so the way to do this is really

01:07:04.640 --> 01:07:11.440
simple um take some latent space again a lot of these examples are are using big GAN you could

01:07:11.440 --> 01:07:19.120
also use style GAN um take some latent space and sample images from it right if you're using a

01:07:19.120 --> 01:07:24.160
conditional model so we pick some category here we're looking at lakes inside big GAN image or

01:07:24.160 --> 01:07:31.600
big GAN places um sample some images for a user and then that user who's determining a visual

01:07:31.600 --> 01:07:38.400
dimension of interest kind of looks over that image space and sees if anything stands out to them

01:07:39.360 --> 01:07:46.720
across that that set of images so maybe here I noticed images that seemed kind of verdant and

01:07:46.720 --> 01:07:51.760
fertile uh and maybe more more spring light but not totally seasonal you'll see where I'm going

01:07:51.760 --> 01:07:57.360
it's kind of hard to describe and these were a little dreary or more wintry but there's not snow

01:07:57.360 --> 01:08:02.240
so it's not really winter they're just kind of less fertile and vivid so that's the distinction I

01:08:02.240 --> 01:08:09.360
want to make there um and the method is very simple just like the steerability work um and a

01:08:10.080 --> 01:08:16.000
another example of work from Bolle we define a transformation just by learning a hyperplane

01:08:16.000 --> 01:08:21.440
so training a SVM and learning a hyperplane it separates those two classes of images either

01:08:21.440 --> 01:08:26.800
in the latent space or in the feature space of some layer layers activations and then when we can

01:08:27.520 --> 01:08:34.320
steer some starting image in a direction that's normal to that hyperplane and steer it across

01:08:34.320 --> 01:08:40.000
those classes right so I could take an image that starts in the kind of dreary or domain

01:08:40.720 --> 01:08:47.680
or dusky or domain and transform it normally to that hyperplane and take it into the category

01:08:47.680 --> 01:08:54.320
of things that I thought was more verdant right or more fertile but I could specify that separating

01:08:54.320 --> 01:09:02.400
hyperplane just by sorting a shockingly few number of images um so we've done a couple of more like

01:09:02.400 --> 01:09:07.920
fine-grained tests here but just for proof of concept you can discern these directions with

01:09:07.920 --> 01:09:13.520
some degree of reliability with just like five to six examples of images in each category making

01:09:13.600 --> 01:09:18.240
it really simple to interact with something like this just by dragging and sorting a few images

01:09:19.200 --> 01:09:25.360
that are sampled in the latent space okay so there's a tiny example of a demo app we have for

01:09:25.360 --> 01:09:29.680
this um and we're switching where it's hosted so it's not online at this very moment but it will

01:09:29.680 --> 01:09:35.280
be next week but it's called the latent compass it was at NeurIPS Creativity I think last year

01:09:35.280 --> 01:09:40.160
the year before um you'll see the the home interface in a second but what we do is just

01:09:40.160 --> 01:09:46.640
what I said pick some category of BigGAN here it's BigGAN places um on the bottom you see images

01:09:46.640 --> 01:09:51.360
sampled from that category and the user drags them to the right and left of the screen corresponding

01:09:51.360 --> 01:09:56.800
to two different kind of categories of concepts they want to capture uh and then once the compass

01:09:56.800 --> 01:10:01.840
calibrates and we'll see that in a second and you can drag any new image and then transform it along

01:10:01.840 --> 01:10:07.360
that dimension so here we pick the closet category I've got full closets on the right

01:10:07.360 --> 01:10:12.720
empty closets on the left and the dimension I want to capture here is something like fullness

01:10:14.000 --> 01:10:18.800
so I'm going to see if I can I can learn a direction corresponding to the visual difference

01:10:18.800 --> 01:10:25.680
between these two categories drag any new closet onto that center line and transform it along that

01:10:25.680 --> 01:10:31.680
direction filling and emptying the closets and what if we tried a different category what if I

01:10:31.680 --> 01:10:38.800
wanted to turn a medina into a full closet right what is the type of fullness that's relevant to

01:10:38.800 --> 01:10:44.080
a medina oh well it's adding people instead of adding clothes suggesting that what's been learned

01:10:44.080 --> 01:10:50.960
there that direction in latent space is abstracting generalizable enough to capture some visually

01:10:50.960 --> 01:10:56.880
recognizable dimension of fullness that's meaningful to us in different scenes right and the model's

01:10:56.880 --> 01:11:02.400
able to to generalize it in a way that's not totally dependent on the types of objects it saw

01:11:02.400 --> 01:11:08.400
in one scene so it knows in a sense that clothes make a closet full but to make a market full

01:11:08.400 --> 01:11:13.680
we're not adding clothes we're adding people and so the fullness direction is something that adds

01:11:13.680 --> 01:11:18.720
more of whatever would make that scene full um to any scene that we're selecting in the model

01:11:18.720 --> 01:11:25.200
right and trained on so few examples of course this this is really quite imperfect but it's a

01:11:25.200 --> 01:11:30.640
good proof of concept of a way that users can interact super flexibly and really visually

01:11:31.600 --> 01:11:37.600
with dimensions of interest and use that to kind of explore and surf the latent space of a model

01:11:38.480 --> 01:11:43.920
by producing replicable repeatable directions that others can explore without having to use language

01:11:44.480 --> 01:11:49.040
that's kind of a different way of carving up the puzzle of how to explore and assign meaning to

01:11:49.120 --> 01:11:55.520
directions that we that we find in latent space okay that's at latentcompass.com

01:11:55.520 --> 01:12:03.040
and we'll be back up next week I think bad timing okay so to return to our frame here

01:12:03.920 --> 01:12:10.160
we've been digging a bit into this intersection between art neuroscience and machine learning

01:12:10.160 --> 01:12:15.600
ways to explore models that have been trained on human creation right at different scales

01:12:15.600 --> 01:12:21.520
to create a new to iterate and interpolate upon archives and then also to start to understand

01:12:21.520 --> 01:12:27.200
what these models are representing and if our ways of interpreting dimensions inside models

01:12:27.200 --> 01:12:33.440
can also teach us something about human perception or allow us to start to build models of aspects of

01:12:33.440 --> 01:12:39.440
human vision that are otherwise pretty intractable because it's difficult to formalize what dimensions

01:12:39.440 --> 01:12:44.240
underlie them where I could write down what dimensions under life physical scene understanding

01:12:44.240 --> 01:12:50.720
because I know Newton's laws I couldn't write down what dimensions underlie aesthetic perception of

01:12:50.720 --> 01:12:57.520
North African marketplaces or Babylonian tablets because I don't know what a large swath of people

01:12:57.520 --> 01:13:04.160
would find perceptually interesting in a bunch of marketplaces I know from cognitive science

01:13:04.160 --> 01:13:10.960
research certain heuristics to look for but that wouldn't give us a full set of what a diversity

01:13:11.040 --> 01:13:14.880
of humans might appreciate when looking at some scene especially things like its mood

01:13:15.760 --> 01:13:20.560
so we can turn here to these kinds of large unstructured generative models that learn

01:13:21.440 --> 01:13:27.040
entirely from data entirely from images and turn to them as like a fertile ground so to speak for

01:13:27.040 --> 01:13:34.000
starting to probe and represent human perceptual experiences inside their latent space and think

01:13:34.000 --> 01:13:40.480
of latent space that way right as a screen as I said before onto which we can project human experience

01:13:40.480 --> 01:13:45.280
and then once we have those projections we can rerun them and interact with them and collaborate

01:13:45.280 --> 01:13:50.960
with them to create outputs of deep generative models that are particularly exquisite and that

01:13:50.960 --> 01:13:57.680
represents some kind of collaboration between us and models of our our creation that are operating

01:13:57.680 --> 01:14:04.880
in parallel so that's where I will leave us my emails here I'm very discoverable online

01:14:04.880 --> 01:14:12.000
but you're welcome to write me questions anytime and I will wrap here and we can have a more

01:14:12.880 --> 01:14:18.080
casual discussion unless anybody has any last questions for this part

01:14:18.480 --> 01:14:39.120
Thank you so much sir this was really interesting and inspiring with all the

01:14:39.360 --> 01:14:48.640
acidic decreasing slides and every moment of that was really full of thoughts I think that

01:14:50.560 --> 01:15:04.240
you open a window to semantically and qualitatively looking at these latent spaces and

01:15:05.200 --> 01:15:08.560
sort of our imagination and

01:15:11.360 --> 01:15:19.680
where we dream and where these models that we create dream so I really appreciate that

01:15:21.840 --> 01:15:26.480
I'm going to stop recording and then see if there are more questions

