1
00:00:00,000 --> 00:00:09,520
Cool. Hello everyone, welcome to your course AI for Art, Esthetic and Creativity. Today

2
00:00:09,520 --> 00:00:19,200
we have a very special speaker. She has an excellent background in different domains and

3
00:00:19,200 --> 00:00:28,880
she will tell you hopefully more about herself and her work. Sarah is a great friend and

4
00:00:29,520 --> 00:00:38,480
colleague of me and she kind of accepted to give us a lecture talk today. So from here I

5
00:00:41,040 --> 00:00:47,840
let Sarah to continue. Please go ahead. Thanks Ali. It's such a pleasure to be here. I've heard

6
00:00:47,840 --> 00:00:53,440
so much about this class. I don't think I have a slide about my background but I can tell you a

7
00:00:53,440 --> 00:00:59,920
little bit about myself. I finished my PhD in neuroscience so across the street from Seasale

8
00:00:59,920 --> 00:01:05,920
last year and now I'm a postdoc in the vision group and the journey throughout my PhD was

9
00:01:05,920 --> 00:01:12,320
a little bit of a winding path. I started thinking about explicit symbolic models for things like

10
00:01:12,320 --> 00:01:17,360
physics and we'll talk a little bit more about that along the way. So modeling how the mind

11
00:01:17,360 --> 00:01:22,480
makes inferences about things that we see but that hits a ceiling when we come up against

12
00:01:23,120 --> 00:01:29,120
questions of vision and types of seeing like looking at art that is really difficult to develop

13
00:01:29,120 --> 00:01:34,880
some kind of computational formalism for that we don't have good models for. And at the same time

14
00:01:34,880 --> 00:01:39,280
as I was kind of hitting that wall in my own thinking I was developing a parallel interest

15
00:01:39,840 --> 00:01:44,640
in visual art and doing a lot of different projects both with individual artists and with

16
00:01:44,640 --> 00:01:51,680
larger museum archives that I'll talk a little bit about and started to look at art as a ground for

17
00:01:52,240 --> 00:01:56,560
asking kind of difficult questions on the frontier of our thinking about the mind.

18
00:01:56,560 --> 00:02:01,600
If we look at how humans create art and in view art can we understand something about

19
00:02:02,320 --> 00:02:06,720
how they view the world and domains that we don't yet have good models of cognition for.

20
00:02:07,600 --> 00:02:12,640
So I kind of started steering my my PhD in that direction. I'll share a little bit of that work

21
00:02:12,640 --> 00:02:18,240
as well and as I said now I'm a postdoc with Antonio and Ellie asked me to share a little bit

22
00:02:18,240 --> 00:02:25,840
of my my inspiration behind that path. I don't have a good story about a specific moment I think

23
00:02:25,840 --> 00:02:32,720
it's been a lifelong interest for me since I was super small and reading a lot of poetry I guess

24
00:02:33,360 --> 00:02:38,080
thinking about kind of the the origin and nature of structure and our experience of the world.

25
00:02:38,080 --> 00:02:44,240
I know that's quite an abstract thing but the structure that we see in visual patterns where

26
00:02:44,320 --> 00:02:49,280
does that come from? Is that something that lives inherently in the brain and we imprint it on to

27
00:02:49,280 --> 00:02:53,680
kind of noisy and unordered stimuli or is it something that's external you know a nature

28
00:02:53,680 --> 00:02:59,600
nurture question and then our brains kind of evolve to reflect and I got interested in this

29
00:02:59,600 --> 00:03:05,920
meeting point this kind of layer between the self and the world where all the action happens so to

30
00:03:05,920 --> 00:03:14,000
speak and had training in applied math before I came to MIT and would think about ways to describe

31
00:03:14,000 --> 00:03:18,720
kind of structured inputs to processing systems and understand something about the structure of

32
00:03:18,720 --> 00:03:24,000
external inputs and then my neuroscience background learned a little bit how to how to

33
00:03:24,000 --> 00:03:28,320
think about and model the structure of a processing system right the structure of different parts of

34
00:03:28,320 --> 00:03:33,840
the brain and it's really been through my interest in visual art that we can start to think about

35
00:03:33,840 --> 00:03:39,680
and describe what happens when those two things meet and how we synthesize our world of visual

36
00:03:39,680 --> 00:03:45,040
experience in domains related to art and then other kind of higher level aspects of cognition

37
00:03:45,040 --> 00:03:51,680
like scenes or associations with moods of scenes and that kind of thing so that's that's where I am

38
00:03:51,680 --> 00:03:58,720
now and I think I'd like to start us off unless anybody has any leading questions about where I

39
00:03:58,720 --> 00:04:05,120
come from with kind of a provocation and you can you can think of this as a frame for what I'll share

40
00:04:05,120 --> 00:04:11,200
today but it's intended to be provocative and so the statement I'll make is that visual perception

41
00:04:11,200 --> 00:04:17,520
itself human perception which we attempt to mirror and model in computer vision and computer science

42
00:04:17,520 --> 00:04:23,280
in some cases that human perception is something that's fundamentally constructive and I say that

43
00:04:23,280 --> 00:04:29,360
because it solves an ill posed inverse problem like ones you've probably heard of before and

44
00:04:29,360 --> 00:04:36,080
doing that doing that solving requires a little bit of creativity so where am I coming from there

45
00:04:36,080 --> 00:04:42,000
the back of your eye as you know is is a 2d flat canvas right made up of a hierarchy of cells

46
00:04:42,000 --> 00:04:47,840
that were visualized in in drawing in art by Ramonica Hall hundreds of years ago and are now

47
00:04:47,840 --> 00:04:52,880
visualized using electromagnetic imaging and we can get actually pretty fine brain detail

48
00:04:52,880 --> 00:04:59,280
of the cells in the back of our eye that constitute a 2d canvas that takes in incoming image data

49
00:04:59,280 --> 00:05:05,760
and represents images in terms of patterns of activations via this kind of mosaic of cells

50
00:05:06,800 --> 00:05:12,960
yet we experience this richly 3d world so there's a setup of a problem that you've probably heard

51
00:05:12,960 --> 00:05:19,440
before right 2d canvas but we have 3d rich experience scenes have depth objects have 3d shape

52
00:05:19,440 --> 00:05:23,280
and furthermore what we see carries lots of different meanings and associations

53
00:05:23,920 --> 00:05:29,920
so where is all of that kind of higher level information in a 2d image classical kind of

54
00:05:29,920 --> 00:05:36,640
computer vision problems we look at this kind of painting by Suzanne you might not only recognize

55
00:05:36,640 --> 00:05:43,520
3d structure of this cottage on the mountain side right even though the image itself is 2d

56
00:05:43,520 --> 00:05:47,920
I might have all sorts of associations with it I might be able to say oh it's spring time

57
00:05:47,920 --> 00:05:52,400
think something about the time of year I might even be able to infer something about the geography

58
00:05:52,400 --> 00:05:58,400
by the palette used to convey what fields might be there think a little bit about the landscape

59
00:05:58,400 --> 00:06:05,040
I might be able to appreciate depth in pictorial space so even on this 2d plane if I put my mouse

60
00:06:05,040 --> 00:06:11,120
up here in the front maybe these fields are closer to me as a viewer than these ones that are far away

61
00:06:11,120 --> 00:06:15,120
but once again I'm just looking at a flat picture where is all of that information

62
00:06:16,320 --> 00:06:21,360
our brain has to solve an inverse problem like this anytime we look at a visual scene it has to get

63
00:06:21,360 --> 00:06:27,280
from low two-dimensional information to kind of rich 3d but there's a fundamental problem here

64
00:06:27,280 --> 00:06:34,480
and I pointed to that is that infinitely many 3d objects can cause the same 2d project projection

65
00:06:34,480 --> 00:06:39,680
that's the under constrained nature of this inverse problem that vision poses and you've

66
00:06:39,680 --> 00:06:45,840
experienced this quite explicitly anytime you've seen a shadow and not the object causing the

67
00:06:45,840 --> 00:06:50,720
shadow right and you've had to infer oh is that actually you know a monster on the wall or is

68
00:06:50,720 --> 00:06:56,720
that somebody's hand being projected but there are infinitely many configurations in three dimensions

69
00:06:56,720 --> 00:07:01,200
that could be projected downwards onto two dimensions and cause some configuration in

70
00:07:01,200 --> 00:07:07,920
pictorial space so how do we constrain that problem when we're solving for what we see

71
00:07:08,720 --> 00:07:13,920
right so this problem is ill-posed because it has as I said many infinitely many possible solutions

72
00:07:14,640 --> 00:07:20,560
and choosing between them requires some additional information and in the case of the brain

73
00:07:21,280 --> 00:07:26,240
modern neuroscience understands this as requiring the brain to construct something so that's what I

74
00:07:26,240 --> 00:07:32,480
mean when I say perception is fundamentally constructive or creative it requires the brain

75
00:07:32,480 --> 00:07:39,120
to construct a best explanation of what it's seeing of incoming information and if we call

76
00:07:39,120 --> 00:07:44,320
that perception then maybe you'll permit me to make a bit of a stretch and say that that makes

77
00:07:44,320 --> 00:07:51,920
perception itself an act of creation or an act of synthesis of a scene so one kind of popular

78
00:07:51,920 --> 00:07:58,160
way to solve this inference problem is by using models of the world right and we can approach

79
00:07:58,160 --> 00:08:02,560
that from a Bayesian lens maybe you've seen the work of Josh Tinnenbaum in the bcs department

80
00:08:04,000 --> 00:08:08,400
or maybe we can do that purely with deep learning it's kind of an attention that we could explore

81
00:08:08,960 --> 00:08:14,800
later today but I'll give you an example here and this is let me back up for a second that if we

82
00:08:14,800 --> 00:08:19,520
were in person this is the point where I would do kind of a live in person demo so I want you to

83
00:08:19,520 --> 00:08:25,040
imagine that we're all sitting kind of in a dark room or we're sitting in a studio space and out

84
00:08:25,040 --> 00:08:31,360
in front of you there is a table covered in black velvet and I've set some stuff on that table you

85
00:08:31,360 --> 00:08:38,080
don't know what it is I set it there when the lights were off and then I take a single line of red

86
00:08:38,080 --> 00:08:43,440
laser light and I'm going to gradually sweep it over the scene so I'm constraining the visual

87
00:08:43,440 --> 00:08:48,960
information you're going to receive about what's out there in the world to something kind of really

88
00:08:48,960 --> 00:08:54,320
low dimensional compared to what you normally get to understand kind of a garden of forms that would

89
00:08:54,320 --> 00:08:59,200
be sitting on the table so imagine you're there in the studio with me and you see the following

90
00:08:59,840 --> 00:09:04,240
you have to kind of infer what you see on the table maybe you could write it in the chat or

91
00:09:04,240 --> 00:09:10,080
just think to yourself when you see this give it a moment what's sitting here on the table

92
00:09:12,560 --> 00:09:14,880
or what kinds of things what different things

93
00:09:17,680 --> 00:09:24,240
maybe this would be a good use of the chat I can pull it up or you can describe features of what you see

94
00:09:34,560 --> 00:09:41,120
a bunch of blocks on the table great there's something cubic up now I see the corner there right

95
00:09:42,800 --> 00:09:48,880
multiple vases multiple forms with kind of different underlying shapes something cylindrical yep

96
00:09:49,840 --> 00:09:53,280
two things do you see I think there's a sphere actually there in the middle

97
00:09:56,480 --> 00:10:01,200
what is the experience of this light do you actually feel a physical corner when you see

98
00:10:02,080 --> 00:10:04,560
bent light round the corner of that cube

99
00:10:08,000 --> 00:10:09,680
oh oh goodness

100
00:10:13,200 --> 00:10:18,240
interacting with chat is a bit tough all right anyway the point I want to make here is that

101
00:10:18,240 --> 00:10:25,360
I can present really kind of low level information and you can if I dare open open the chat up again

102
00:10:25,920 --> 00:10:29,680
yeah there's a single base there's a single table and many forms sitting on top

103
00:10:29,760 --> 00:10:34,240
that's right so there's just a tabletop and then lots of different shapes also covered in black

104
00:10:34,240 --> 00:10:40,080
velvet so the light doesn't scatter and the light traces the outline of these 3d shapes

105
00:10:40,640 --> 00:10:46,560
and you can appreciate something about what the shapes are just by watching how light bends

106
00:10:46,560 --> 00:10:52,560
around their surface and the relative motion as it traverses that facade right as it moves

107
00:10:52,560 --> 00:10:58,000
over the surface of the sphere the light bends according to its curvature and I would argue

108
00:10:58,000 --> 00:11:05,760
here that because you have some notion of what a sphere is and some notion of what a cube is

109
00:11:05,760 --> 00:11:12,560
that is you have a relatively abstract model of these underlying shapes in your mind a mental

110
00:11:12,560 --> 00:11:18,240
model you can do some inference when you see light move over their surface on this way even though

111
00:11:19,120 --> 00:11:24,000
I choose an example like this because you've probably never seen this example before right

112
00:11:24,800 --> 00:11:28,320
never seen a single line of laser light move over this table surface

113
00:11:28,960 --> 00:11:33,760
even this this kind of setup but you can still do that inference pretty well and you did in the chat

114
00:11:35,120 --> 00:11:40,400
so if you'll if you'll stay with me here I'm suggesting that this is an example just your

115
00:11:40,400 --> 00:11:46,400
perceptually of how we can bring kind of models of the world and shapes and forms that comprise

116
00:11:46,400 --> 00:11:52,880
it to bear on simple visual stimuli and how we can even do that by using articulated light

117
00:11:53,440 --> 00:11:59,280
to isolate aspects of those stimuli and to kind of elucidate our perception to us

118
00:12:00,160 --> 00:12:04,560
so we do a lot of things like this in the MIT museum studio where I teach the vision and art

119
00:12:04,560 --> 00:12:09,280
neuroscience class which I'll talk about in a moment but that's where this this was filmed

120
00:12:09,280 --> 00:12:14,080
let's see if it'll let me advance even though I open the chat all right so another kind of setting

121
00:12:14,080 --> 00:12:19,680
in which we often hear and think about models of the world and this kind of inference is in

122
00:12:19,680 --> 00:12:25,600
intuitive physics and I bring this up because some of my background is also in this type of work

123
00:12:25,600 --> 00:12:31,680
investigating how the brain represents physical properties like mass that it uses to reason

124
00:12:31,680 --> 00:12:36,720
physically about the world right you would have to estimate the mass of this block that's falling

125
00:12:36,720 --> 00:12:41,520
and making a depression on this pillow before you would know the right amount of grip force

126
00:12:41,520 --> 00:12:45,840
you would need to use to to reach in and pick it up without dropping it right and this is something

127
00:12:45,840 --> 00:12:51,360
we do incredibly automatically and it's a skill set we develop regularly from a very early age

128
00:12:51,920 --> 00:12:59,440
and I found that the brain represents properties like mass with an amount of abstraction and invariance

129
00:12:59,440 --> 00:13:06,160
to the type of physical scene in which mass is revealed that would be necessary if mass like

130
00:13:06,160 --> 00:13:12,640
this were to be used as an input to an abstract generalized engine for physical simulation or

131
00:13:12,640 --> 00:13:18,720
what we call a physics engine in computer graphics and simulation suggesting that there is

132
00:13:18,720 --> 00:13:25,680
kind of some first evidence that the brain does use these kind of generalized simulation engines

133
00:13:25,680 --> 00:13:31,520
to solve low-level inference problems like inferring mass because we can make some hypothesis about

134
00:13:31,520 --> 00:13:36,320
the nature of the underlying representations it would need if it were to solve problems in this

135
00:13:36,320 --> 00:13:40,880
kind of way rather than by simple pattern matching or in a pixel based way where we would assume

136
00:13:40,880 --> 00:13:45,760
that the representation of mass would be quite different from scene to scene because the low

137
00:13:45,760 --> 00:13:51,200
level visual data about the scene is different but in fact that's not what we find we find representations

138
00:13:51,200 --> 00:13:55,840
of physical variables like mass and friction that generalize across any kind of physical scene

139
00:13:56,400 --> 00:14:01,120
that we test where we hold a lot of other different parameters constant right like object color

140
00:14:02,160 --> 00:14:06,880
and this suggests an account of physical reasoning in the brain that has been that has been studied

141
00:14:06,880 --> 00:14:13,040
pretty extensively computational right and that we model via probabilistic simulations of a physics

142
00:14:13,040 --> 00:14:17,840
engine I don't think that video is going to play for us right but this is the kind of work when I

143
00:14:17,840 --> 00:14:24,240
was doing when that I was doing when I was writing down like explicit models of the world that could

144
00:14:24,240 --> 00:14:29,280
be inverted to explain something about underlying parameters we were using for vision and in that

145
00:14:29,280 --> 00:14:36,720
in this case those models were physical right but what about cases like like art where it's

146
00:14:36,800 --> 00:14:42,320
difficult as I mentioned to develop some kind of computational formalism where we don't know the

147
00:14:42,320 --> 00:14:47,520
underlying model for instance how to create the Cezanne painting we saw in the beginning

148
00:14:48,240 --> 00:14:53,280
a priori right how do we even start what are the underlying dimensions we'd need to write down to

149
00:14:53,280 --> 00:15:01,120
either make sense of how we see things or how they're created so this whole area is kind of what we

150
00:15:01,120 --> 00:15:05,680
dive into in that vision in art and neuroscience course so this is something you're interested in

151
00:15:05,760 --> 00:15:10,400
it's of course an unsolved problem but we spend the fall semester every year

152
00:15:11,440 --> 00:15:16,480
kind of delving into it through both neuroscience literature through art practice through computation

153
00:15:16,480 --> 00:15:22,480
and then through studio work so kind of hands-on experimentation with principles underlying vision

154
00:15:22,480 --> 00:15:27,520
that we then externalize and experience ourselves and try and visualize in artistic contexts

155
00:15:29,440 --> 00:15:32,800
to give you a little bit of a taste of that class we would look at

156
00:15:33,360 --> 00:15:38,560
these examples say by by an artist in minor white and ask if we were trying to set up

157
00:15:39,360 --> 00:15:45,520
a typical kind of describe a model and then invert it to understand vision setting you know what is

158
00:15:45,520 --> 00:15:51,680
the veretical percept in either of these right if before we were considering mass of some object

159
00:15:51,680 --> 00:15:58,000
that the brain has to infer and then we can write down a physical law describing how mass plays into

160
00:15:58,080 --> 00:16:02,720
action unfolding in a scene a law describing dynamics and then invert it to think about

161
00:16:02,720 --> 00:16:07,120
how the brain represents mass what would the analog be here what would we write down as the

162
00:16:07,120 --> 00:16:12,320
veretical percept you can share some thoughts in the chat that is also an exercise you could just

163
00:16:12,320 --> 00:16:18,560
do yourself right maybe here you can start to get it a shadow of something outside the window

164
00:16:19,120 --> 00:16:24,960
I see a bike maybe a bike seat there that's kind of not the point kind of not trying to infer what

165
00:16:24,960 --> 00:16:31,040
caused the specific physics of this this image you're kind of getting at something different

166
00:16:31,040 --> 00:16:37,200
and especially here what if the artist isn't around for us to ask anymore these are actual

167
00:16:37,200 --> 00:16:43,440
photographs right these are photographs of something but the act of looking at it isn't about

168
00:16:43,440 --> 00:16:48,560
inferring the underlying cause of the image it's about inferring something else sort of aesthetic

169
00:16:48,560 --> 00:16:53,920
parameters that define visual experience or kind of render visual experience at a lot higher of a

170
00:16:53,920 --> 00:17:00,000
level how do we begin to get traction on problems like this either in seeing or in or in generation

171
00:17:00,800 --> 00:17:06,000
as I said you know in art we also come up against a great difficulty in that you know there are

172
00:17:06,000 --> 00:17:12,320
infinitely many ways to render recognizable depictions of common objects right with all

173
00:17:12,320 --> 00:17:18,160
sorts of idiosyncrasies illusory boundaries difficult for models to detect but we recognize

174
00:17:18,160 --> 00:17:24,800
a woman in these images with the dress almost instantaneously and similarly we come up against

175
00:17:24,800 --> 00:17:30,560
another under constrained inverse problem is in that there's infinitely many ways to render and

176
00:17:30,560 --> 00:17:37,280
depict kind of abstractions of commonly recognizable forms which again are difficult for

177
00:17:37,280 --> 00:17:42,880
current day models but they're pretty easy for us I can recognize a figure and maybe have different

178
00:17:42,880 --> 00:17:49,040
associations with it in each of these different images so we think a little bit about this

179
00:17:49,760 --> 00:17:55,280
in the course like I mentioned you can ask me a bit after this talk as well if you're if you're

180
00:17:55,280 --> 00:18:01,520
interested in it it's called vision and art and neuroscience all of our info is is online most

181
00:18:01,520 --> 00:18:08,960
of the syllabus past exhibition catalogs at vision.mit.edu it's offered through through bcs

182
00:18:09,840 --> 00:18:15,200
and as I said we we investigate during half the class in the seminar portion of the class

183
00:18:15,760 --> 00:18:19,520
kind of the underlying principles of vision and we work through a series of modules

184
00:18:20,320 --> 00:18:27,280
that build up visual processes from early level like v1 visual processing all the way up to kind

185
00:18:27,280 --> 00:18:33,840
of more rich images and we do this in parallel in a studio section during the other portion of the

186
00:18:33,840 --> 00:18:39,360
class where we're translating these principles of vision into the studio and building artistic

187
00:18:39,360 --> 00:18:45,200
contexts where we can kind of become aware of our own perceptual processing at work so examples like

188
00:18:45,200 --> 00:18:49,120
the one I showed you at the beginning right with the with the laser line moving over

189
00:18:49,120 --> 00:18:54,400
that garden of objects are examples of settings that can allow us to maybe perceive our own

190
00:18:54,400 --> 00:18:59,840
perception at work right or shed some light on what's going on when we look at at normal scenes

191
00:18:59,840 --> 00:19:04,560
right there's all these unconscious inference processes happening even when we look at corners

192
00:19:04,560 --> 00:19:08,800
in a room but we're not aware of them and so we ask here if we can create settings

193
00:19:09,520 --> 00:19:15,600
where we do become intensely aware of them and that awareness becomes kind of the art experience

194
00:19:15,600 --> 00:19:20,960
right and so it's the art of perceiving one's own perceptual processes at work and then over

195
00:19:20,960 --> 00:19:26,160
the course of the class everybody develops an individual artwork for exhibition which is super

196
00:19:26,160 --> 00:19:32,560
lovely and it's it's an opportunity that we don't often have in other classes at MIT so we run this

197
00:19:32,560 --> 00:19:38,880
for five years now had five different exhibitions and COVID be it a virtual exhibition and then

198
00:19:38,880 --> 00:19:46,080
this year's just opened in December and is actually still up in the MIT museum studio just off of lobby

199
00:19:46,080 --> 00:19:52,000
10 10 150 if anybody is on campus and wants to go check it out it's most it's open most days when

200
00:19:52,240 --> 00:19:59,920
staff are there but this course is the parallel to your IAP class that thinks about things more

201
00:19:59,920 --> 00:20:05,040
in the language of computational neuroscience than deep learning and in some aspects of the

202
00:20:05,040 --> 00:20:11,520
course will present deep learning or deep generative models as contexts for probing representations

203
00:20:11,520 --> 00:20:16,720
that might be shared by human minds and machines and we'll look at that a little bit later in this

204
00:20:16,800 --> 00:20:22,080
lecture but think more traditional computational neuroscience lectures readings visual art and

205
00:20:22,080 --> 00:20:26,880
then a studio component where you experiment with some of the stuff hands on so that's what we do

206
00:20:26,880 --> 00:20:33,600
envision art neuroscience we start to to probe at the richness of this art neuro and machine

207
00:20:33,600 --> 00:20:37,520
learning intersection there's a lot of different things we can do there and for the rest of this

208
00:20:37,520 --> 00:20:42,400
talk we're going to highlight a number of different projects that approach that intersection in

209
00:20:42,400 --> 00:20:47,280
different ways and highlight kind of different ways that you could think about engaging this

210
00:20:47,280 --> 00:20:52,160
material in these questions data sets and resources that we have available and kind of

211
00:20:52,160 --> 00:20:58,480
different ways of carving up the problem into bits so we'll start by thinking about modeling

212
00:20:58,480 --> 00:21:04,720
kind of the structure underlying human creativity at scale without trying to prespecify

213
00:21:05,520 --> 00:21:11,360
laws that you would write down for say a physics engine right can we use deep generative models

214
00:21:11,360 --> 00:21:18,080
to kind of approximate or appreciate or grok the structure underlying large data sets of human

215
00:21:18,080 --> 00:21:24,480
cultural artifacts and then use those models to experiment with cultural history on kind of a

216
00:21:24,480 --> 00:21:29,600
timeline that allows rapid evolution in the present so I'm speaking specifically about a project

217
00:21:29,600 --> 00:21:33,760
that I don't know if some of you have seen and I know Ali has seen a collaboration that I led with

218
00:21:33,760 --> 00:21:39,280
the Met a couple of years ago again it was fun that we were in person because we were able to

219
00:21:39,280 --> 00:21:44,800
actually go to the Met and see a lot of these objects but back in 2017 the Metropolitan Museum of

220
00:21:44,800 --> 00:21:51,600
Art was the first or one of the very first to release an open access catalog of a few hundred

221
00:21:51,600 --> 00:21:57,520
thousand digital images of works in the Met collection and released them into the public domain

222
00:21:57,520 --> 00:22:03,600
which is wonderful for for us as computer scientists and programmers and people interested in ML and

223
00:22:03,600 --> 00:22:08,960
art because what a rich data set that is right what a rich data set all in one place

224
00:22:08,960 --> 00:22:13,840
don't get me started on the issues with museum APIs but a lot of museums have followed suit in

225
00:22:13,840 --> 00:22:19,440
releasing their digital collections into the public domain so they're free and open for experimentation

226
00:22:20,240 --> 00:22:26,000
they approached us at MIT and open learning and a couple of programmers at Microsoft and asked if

227
00:22:26,000 --> 00:22:32,080
we might want to do a series of projects with this digital collection and so we did and we asked

228
00:22:32,080 --> 00:22:37,600
whether we can build deep generative models associated with archives like this of created

229
00:22:37,600 --> 00:22:42,800
work that are embedded in their cultural context which might ask which might allow us to ask like

230
00:22:42,800 --> 00:22:49,040
slightly more specific questions art historically than just you know what if you train StyleGAN on

231
00:22:49,040 --> 00:22:54,000
all of wiki art all at once right not conditionally so we're not appreciating any categorical

232
00:22:54,000 --> 00:22:59,520
differences between images but if we just showed it all of wiki art okay here we want to ask something

233
00:22:59,520 --> 00:23:04,880
a little more fine grained can we notice you know differences in the development of feature languages

234
00:23:04,880 --> 00:23:12,720
between maybe time periods or geographical regions right and can we develop ways of collaborating

235
00:23:12,720 --> 00:23:19,040
with those models to iterate archives forward so experimenting with chimeras between existing

236
00:23:19,040 --> 00:23:24,480
works and developing new works right that might sit somewhere between works that are already on a graph

237
00:23:26,080 --> 00:23:31,840
so one of the challenges that we faced here initially was that the data set was pretty big

238
00:23:31,840 --> 00:23:37,040
400,000 images but each individual category in that data set was not some might only have

239
00:23:37,040 --> 00:23:42,800
a couple hundred images and there's a lot of sketches and drawings and kind of uncategorized

240
00:23:42,800 --> 00:23:49,120
work too that makes up that 400,000 so you're in a situation where in theory you have a rich

241
00:23:49,120 --> 00:23:55,120
labeled data set but in practice it might be quite difficult to train anything that looks

242
00:23:55,120 --> 00:23:59,600
photorealistic or gives a good sense of any individual category of work because the categories

243
00:23:59,600 --> 00:24:06,480
themselves are not that large so at that point this was pre like style again too we started working

244
00:24:06,480 --> 00:24:14,800
on this in 2017-2018 um I asked whether we could instead of training a single model on say a subset

245
00:24:14,800 --> 00:24:20,960
of this met collection like this category of vases called yours whether we could find corresponding

246
00:24:20,960 --> 00:24:27,680
subspaces of what we're now referring to as foundation models like big an image net that kind

247
00:24:27,760 --> 00:24:33,760
of approximate our data set right so if we think about foundation models as a shared resource that

248
00:24:33,760 --> 00:24:38,880
ideally everybody would have access to and there were ways to think about contributing to then maybe

249
00:24:38,880 --> 00:24:45,680
these smaller problems become or can become a way of defining subspaces of those big models that we

250
00:24:45,680 --> 00:24:51,520
can interact with right rather than having to retrain a model and on our data set so we used

251
00:24:51,520 --> 00:24:56,240
GAN inversion here and instead of training a new model on just this category of viewers

252
00:24:56,800 --> 00:25:02,160
we asked whether we could embed each image that already existed into in the met collection

253
00:25:02,160 --> 00:25:08,560
and into the feature space of big GAN image net which happens to have a category for vases so we

254
00:25:08,560 --> 00:25:13,680
selected categories that were shared between image net and the met collection there are a handful

255
00:25:13,680 --> 00:25:19,840
about a dozen um and we maximized for each of those images the similarity between the met image

256
00:25:19,840 --> 00:25:24,400
and the big GAN image using a two-part loss right so we wanted them to be similar both at the pixel

257
00:25:24,400 --> 00:25:30,240
level and at the semantic level and we did that by looking at two different layers of a pre-trained

258
00:25:30,240 --> 00:25:36,000
res net as the embedding network so once we've embedded these models these images into big GAN

259
00:25:36,000 --> 00:25:40,720
we can then visualize the individual embeddings but we can also do something a little bit more

260
00:25:40,720 --> 00:25:46,640
interesting than just look at approximations of these images which might not be very good we can

261
00:25:46,640 --> 00:25:50,800
think about the underlying feature language that might have been learned and then look at

262
00:25:50,800 --> 00:25:56,720
interpolations between the existing images in the met collection I hear murmurs in the

263
00:25:56,720 --> 00:26:02,640
background if anybody has a question hit the chat you're super welcome to speak up um so next we

264
00:26:02,640 --> 00:26:08,160
look at interpolations between these existing images on the graph and we can create kind of

265
00:26:08,160 --> 00:26:15,040
hypothetical or dreamlike images that exist between the spaces of existing works in the collection

266
00:26:15,040 --> 00:26:21,360
and these are pretty interesting and beautiful and they allow us as I was mentioning to ask

267
00:26:21,360 --> 00:26:25,920
questions about what collaborations between geographical regions might have looked like

268
00:26:25,920 --> 00:26:30,720
right because we do have categorical information about where each image in the met collection

269
00:26:30,720 --> 00:26:36,800
came from it allows us to suggest new objects and the spaces between them so it allows us to

270
00:26:36,800 --> 00:26:42,400
interpolate and the other beauty of these kinds of executable models of culture is that it allows

271
00:26:42,480 --> 00:26:48,320
us to iterate on existing collections really rapidly um and evolve them forward and so we

272
00:26:48,320 --> 00:26:53,280
can kind of start to imagine archives of the future that would have embedded within them

273
00:26:53,840 --> 00:26:59,280
world models corresponding to the data set that exists at one point in the archive right so the

274
00:26:59,280 --> 00:27:04,640
archives could kind of evolve themselves forward and suggest future versions of their collections

275
00:27:04,640 --> 00:27:10,560
based on what's already been created and that this is again this was back in 2019 which is a

276
00:27:10,560 --> 00:27:16,240
long time ago in computer vision terms um but even just with with inversion into to began in the

277
00:27:16,240 --> 00:27:21,040
channel I was really impressed at the quality of the the images and the hypothetical objects that

278
00:27:21,040 --> 00:27:26,160
we could get for example here are a bunch of different generated teapots from the met latent

279
00:27:26,160 --> 00:27:31,760
space in the teapot category which again happened to be shared between ImageNet at that point um

280
00:27:31,760 --> 00:27:38,960
and the met collection and as I said we did have the the opportunity to exhibit this in the met

281
00:27:38,960 --> 00:27:45,760
which was absolutely wonderful um we projected a visualization of this latent space superimposed

282
00:27:45,760 --> 00:27:51,360
on a map of the met collection and allowed people visitors to the to the great hall to kind of step

283
00:27:51,360 --> 00:27:57,360
in to this latent space as projected onto the ground and explore the traversal of the spaces

284
00:27:57,360 --> 00:28:06,320
between works um and a projection behind them on the wall. We also made a web app version of all

285
00:28:06,400 --> 00:28:12,720
this that exists even though we we can't visit the met today um it's online at gen.studio if you

286
00:28:12,720 --> 00:28:17,680
want to go have a look after this and then all of the the code base is linked the github is linked

287
00:28:17,680 --> 00:28:23,600
at the bottom um if you want to check out any of that more specifically but again it places us

288
00:28:23,600 --> 00:28:28,960
kind of a different framing of latent space traversal than we're used to that I was interested

289
00:28:28,960 --> 00:28:33,840
in this project was to place us on you know we've gotten a lot further along in the video um

290
00:28:33,840 --> 00:28:39,200
you can go look at the at the website places us on a map between the objects when we're doing

291
00:28:39,200 --> 00:28:44,400
the interpolations right so we select an object to start now we land in the latent space of big

292
00:28:44,400 --> 00:28:50,400
gen close to that object and then we can move ourselves around on the map between objects and

293
00:28:50,400 --> 00:28:56,560
their embeddings in that latent space right and as we're moving physically in 2d space here online

294
00:28:56,560 --> 00:29:02,560
we can visualize what exists at that point in latent space and then we can find its nearest

295
00:29:02,560 --> 00:29:08,640
neighbor visually uh in the met collection and find what object in the existing collection

296
00:29:08,640 --> 00:29:13,360
is most similar to the hypothetical work that we discovered in the interstices between two

297
00:29:13,360 --> 00:29:20,400
existing works um so give that a look and this project lives on today um and a couple of different

298
00:29:20,400 --> 00:29:25,760
forms I'm still working with the artist Matthew Richie he was a collaborator uh with us on the

299
00:29:25,760 --> 00:29:34,160
met project um on a couple of different tendrils of of this work where we're asking all right so

300
00:29:34,880 --> 00:29:39,520
we can model projections of existing images in the met in the met collection by finding their

301
00:29:39,520 --> 00:29:47,280
embeddings in some kind of large foundation model but now in 2021 we have things like StyleGAN ADA

302
00:29:47,280 --> 00:29:51,760
that can can train on smaller data sets and do reasonably well in approximating data sets that

303
00:29:51,760 --> 00:29:56,960
would correspond to a single category in the met collection so we've done that um we've trained

304
00:29:56,960 --> 00:30:04,400
these models on sketches um Babylonian cuneiform tablets Japanese watercolors and some 18th century

305
00:30:04,400 --> 00:30:11,520
European landscapes among other things and have individual models correspond to each of these

306
00:30:11,520 --> 00:30:16,880
genres within the met collection and then we've been working with a friend in New York who has a

307
00:30:16,880 --> 00:30:23,440
robotic oil painter and can actually create layered paintings of really short walks in latent

308
00:30:23,440 --> 00:30:29,520
space along different dimensions in this model so think about physically visualizing some of the

309
00:30:29,520 --> 00:30:34,480
durability work you've looked at in this course right could we make time paintings of really

310
00:30:34,480 --> 00:30:41,920
short walks in latent space by superimposing robotic paintings of the visualized image kind

311
00:30:41,920 --> 00:30:47,360
of at different points along that walk so that's that's being exhibited right now at UNT

312
00:30:47,360 --> 00:30:50,880
in their contemporary art gallery let's see I've got a question in the chat room

313
00:30:53,200 --> 00:30:59,040
oh it's just a compliment I will take it at any point yeah I think

314
00:30:59,040 --> 00:31:05,120
can you please read it yes uh someone mentioned that this is a creative reason one of the most

315
00:31:05,120 --> 00:31:10,880
creative reasons they've seen to do latent space interpolation since it scans yeah I think that

316
00:31:10,880 --> 00:31:16,160
I had a slide a moment ago if you want to rewind in the recording of this suggesting that kind of

317
00:31:16,160 --> 00:31:21,840
part of the advent of using GANs to model kind of large databases of creative work is that they

318
00:31:21,840 --> 00:31:27,360
allow us to do a couple of things right that interpolation and that iteration and in cases

319
00:31:27,360 --> 00:31:31,760
where you can't write down a feature language underlying a set of works because you don't

320
00:31:31,760 --> 00:31:38,160
know our priority what it is you can imprint that or you can learn something of that in a deep

321
00:31:38,160 --> 00:31:45,360
generative model right and then you can collaborate with that and hypothesize what might lie on a

322
00:31:45,360 --> 00:31:52,080
graph of human creation if we presume that any creation artistic creation at some point in

323
00:31:52,080 --> 00:31:59,920
historic time is if you think about it as the manifestation of a point on kind of a sea of

324
00:31:59,920 --> 00:32:04,960
cultural influences and multi-generational practice iterative practice that's been shared

325
00:32:05,040 --> 00:32:11,040
between peoples and generations and the creation of a single work is the enactment of that process

326
00:32:11,040 --> 00:32:17,360
at some point in space it's natural to think of that in some sense as a model that we can capture

327
00:32:17,360 --> 00:32:22,960
in a latent space where we're manifesting some part of structured space at some moment but this

328
00:32:22,960 --> 00:32:29,280
allows us to iterate on that which I argue is similar to some historic processes of iteration

329
00:32:29,280 --> 00:32:36,400
and collaboration across groups of people really quickly right um so I've been trying to take this

330
00:32:36,400 --> 00:32:43,600
a little further now and ask all right we can make paintings of short walks in latent space we can

331
00:32:43,600 --> 00:32:48,720
hypothesize objects that might have existed but we don't think they ever did but we still don't

332
00:32:48,720 --> 00:32:55,760
know much about these models even if we train StyleGAN 2 on a set of 2,000 paintings in the

333
00:32:55,760 --> 00:33:01,200
net collection uh you've probably seen some of the interpretability work adjacent to what Ali

334
00:33:01,200 --> 00:33:06,560
has shared or David Bao's work so that style of thinking we don't know anything about this Japanese

335
00:33:06,560 --> 00:33:11,680
watercolor model like what do its individual neurons represent is there a neuron for trees

336
00:33:11,680 --> 00:33:17,840
well what is a tree here it's some brushstrokes we recognize as a tree but it's not something that

337
00:33:18,640 --> 00:33:24,480
BigGAN trained on image that would necessarily recognize as a tree maybe more simply kind of in

338
00:33:24,480 --> 00:33:30,560
the in the steerability context what do dimensions in the latent space of a model like this

339
00:33:30,560 --> 00:33:38,400
correspond to right sure we can find things like zoom and 3d rotation because we can name those

340
00:33:38,400 --> 00:33:43,120
transformations and then find directions that maximally correspond to them using that kind

341
00:33:43,120 --> 00:33:47,440
of steerability technique there are all sorts of other directions like the ones we're visualizing

342
00:33:47,440 --> 00:33:53,200
here that certainly have some affective meaning to the viewer that we don't know what they are in

343
00:33:53,200 --> 00:33:58,720
the models terms or in the viewer's terms so at this point in this project we're thinking about

344
00:33:58,720 --> 00:34:04,320
starting to name and understand dimensions underlying generative models trained on

345
00:34:04,320 --> 00:34:09,200
bodies of artistic work from museum digital collections not only limited to the met but

346
00:34:09,200 --> 00:34:15,600
around the world uh and our motivation here is to kind of create these alternate and imaginary

347
00:34:15,600 --> 00:34:20,800
histories of art built from unique latent walks that we can visualize in real time with this painting

348
00:34:20,800 --> 00:34:27,040
or computationally and then maybe understand something about aspects of picture language

349
00:34:27,040 --> 00:34:33,600
that might be shared across you know vastly different genres so Babylonian cuneiform tablets

350
00:34:33,600 --> 00:34:40,240
transformed from numeric to symbolic and image-based at a very particular point in history and can we

351
00:34:40,240 --> 00:34:46,880
find a dimension in style GAN trained on a very different genre of art that corresponds to a

352
00:34:46,960 --> 00:34:52,160
similar kind of transformation and as such can we build up kind of a picture language that would

353
00:34:52,160 --> 00:34:58,240
correspond to diverse forms of art making right that you might not see in any of these different

354
00:34:58,240 --> 00:35:03,440
categories of digital images on an archive but we might start to appreciate once we can investigate

355
00:35:03,440 --> 00:35:12,560
them by training deep generative models on them let's get back to great okay so when we're thinking

356
00:35:12,640 --> 00:35:17,680
about this intersection we've seen one example of modeling the structure underlying creativity at

357
00:35:17,680 --> 00:35:23,760
scale and i've done other projects and you can find many examples online both of my work and

358
00:35:23,760 --> 00:35:28,720
other peoples of trying to do this not for creativity at scale but for individual instances

359
00:35:28,720 --> 00:35:35,680
of individual artists and modeling either the style or the processes of individual art making

360
00:35:35,680 --> 00:35:42,400
techniques so all of these are kind of flavors of starting to imprint or grok or understand the

361
00:35:42,400 --> 00:35:47,680
structure underlying creativity but not symbolically right so we don't we don't know how to interpret

362
00:35:47,680 --> 00:35:52,480
these models even though we can visualize them and create really interesting hypothetical objects

363
00:35:52,480 --> 00:35:57,680
that might be indistinguishable either from existing work or from one artist's particular style

364
00:35:59,120 --> 00:36:04,400
we can also think about these models as a tool themselves for collaboration both in their creation

365
00:36:04,400 --> 00:36:10,000
and iteration with others who contribute to their models and with the models themselves

366
00:36:10,000 --> 00:36:15,440
which as i described represent kind of executable versions of collective cultural structure we

367
00:36:15,440 --> 00:36:21,200
permit permit ourselves to think about them that way or facets of kind of a global creative identity

368
00:36:22,240 --> 00:36:26,800
but as i mentioned now we're at a point with tools and computer vision where we can start to ask

369
00:36:27,600 --> 00:36:33,840
what rep representations actually underlie these models trained on artworks that are themselves

370
00:36:33,920 --> 00:36:37,760
executable versions of some collective cultural structure right well what is the structure what's

371
00:36:37,760 --> 00:36:44,480
going on under the hood do they correspond to dimensions that we find meaningful when we look

372
00:36:44,480 --> 00:36:50,240
at visual scenes and so in the next part of the talk i'll share a couple maybe more technical

373
00:36:50,240 --> 00:36:56,000
projects that explore specific ways that humans can interact with generative models

374
00:36:56,640 --> 00:37:01,280
in order to maybe learn something about human vision as well right so can we build

375
00:37:01,760 --> 00:37:07,440
shared vocabularies that help us interpret dimensions underlying these models by designing

376
00:37:07,440 --> 00:37:13,120
experiments that allow us to visualize and interact with images and latent walks like you've been

377
00:37:13,120 --> 00:37:19,040
seeing i'll pause here because i need a sip of water and i'll keep an eye on the chat in case

378
00:37:19,040 --> 00:37:28,480
anyone has any questions before we go on

379
00:37:33,760 --> 00:37:39,440
all right looks like we are question free so far five more seconds

380
00:37:42,880 --> 00:37:48,160
i guess i have a question yeah so this might be talked about later but i was wondering a little

381
00:37:48,240 --> 00:37:54,480
bit about like in your research and kind of this field how much of like human interaction is like a

382
00:37:54,480 --> 00:37:59,760
big part of it and kind of like the human coming in and saying uh how they think about something

383
00:37:59,760 --> 00:38:04,800
and see where that agrees with the computer or like kind of like where that role is played

384
00:38:06,240 --> 00:38:09,920
wonderful question so these kind these kinds of high-level questions that

385
00:38:09,920 --> 00:38:14,880
get it some experiential component or design component of the worker i think really useful

386
00:38:14,880 --> 00:38:19,440
ask more of them i'll tell you for different projects what that looks like and in the next

387
00:38:19,440 --> 00:38:25,440
section of work it's going to be really obvious because there's human annotations but for this

388
00:38:25,440 --> 00:38:31,920
project so the human would come in here you know we train models on datasets of art selected from

389
00:38:31,920 --> 00:38:36,800
the met collection and these are small and these are subsets and they were gathered by

390
00:38:37,520 --> 00:38:42,960
matthew richie and myself going through different genres in the digital collection of the met online

391
00:38:43,920 --> 00:38:49,520
and like hand selecting images from those different genres right representative images of

392
00:38:49,520 --> 00:38:56,160
different categories of work or maybe in a less fine grained way all images under some designation

393
00:38:56,160 --> 00:39:02,320
so japanese watercolors between the 17th and 19th centuries so we made that selection and

394
00:39:02,320 --> 00:39:07,760
tried training these models on a bunch of different such selections and decided which ended up you

395
00:39:07,760 --> 00:39:13,040
know with so few examples providing at least a representative sample of the kind of work that

396
00:39:13,040 --> 00:39:20,960
we know we saw there right and then here the selection of like walks through latent space

397
00:39:20,960 --> 00:39:23,920
so think of those in the same way you've been thinking about the steerability walks

398
00:39:24,800 --> 00:39:29,840
they were very arbitrary so that was a completely human selected so it's a kind of a different

399
00:39:29,840 --> 00:39:36,400
approach to interpretability where it's steered by the human eye right we're not doing it automatically

400
00:39:36,400 --> 00:39:42,400
and we're not doing symbolic it symbolically we don't know what these correspond to but that's

401
00:39:43,440 --> 00:39:47,600
trying some arbitrary walk through latent space trying many of them and then the human then

402
00:39:47,600 --> 00:39:52,640
selecting what to them felt like an artistic expression this is an art exhibit and then

403
00:39:52,640 --> 00:39:58,080
in the next step we'll ask how can we do that in a more systematic way and start to build

404
00:39:58,080 --> 00:40:02,480
a language corresponding to what those different walks could be a language that's shared by humans

405
00:40:02,480 --> 00:40:08,400
and that takes at least right now a lot of human a lot of human interaction

406
00:40:09,200 --> 00:40:12,240
you could think about ways to automate that we'll talk about that in a second

407
00:40:13,440 --> 00:40:18,960
but with any kind of human interaction it's nice to preserve the opportunity for direct

408
00:40:18,960 --> 00:40:23,680
engagement with models rather than intermediation by a captioner or something like that because

409
00:40:23,680 --> 00:40:28,880
then you could imagine using your technique on different subsets of humans right on different

410
00:40:28,880 --> 00:40:33,840
kinds of experiences so you might imagine getting an art historian to label and select

411
00:40:33,840 --> 00:40:38,000
different walks through latent space here corresponding to very nuanced changes in the

412
00:40:38,000 --> 00:40:43,280
development of Babylonian like cuneiform tablets right that a captioner couldn't recognize I

413
00:40:43,280 --> 00:40:48,320
couldn't recognize so you might want to be able to pull different kinds of humans into the loop

414
00:40:48,320 --> 00:40:53,680
at different times to engage in ways that kind of use their knowledge to create a unique synthesis

415
00:40:53,680 --> 00:40:59,920
with a generative model so that's what engagement looked like here and then with this next project

416
00:40:59,920 --> 00:41:06,560
it'll be super it'll be super clear and I'll make sure to speak specifically to that so thank you

417
00:41:06,560 --> 00:41:14,880
yeah cool so next this is probably a summary of what you've seen so far in your IAP course so

418
00:41:14,880 --> 00:41:20,160
there's a lot of different work on discovery of interpretable directions in the latent space

419
00:41:20,160 --> 00:41:25,120
of different generative models right and we can steer images along those dimensions to create

420
00:41:25,120 --> 00:41:30,320
interpretable transformations that allow us to interact creatively with deep generative models

421
00:41:30,320 --> 00:41:38,240
right here we are deep learning for creativity but a lot of these examples presume what concepts

422
00:41:38,240 --> 00:41:43,120
we're searching for in the latent space and in fact they do that really explicitly right we

423
00:41:43,120 --> 00:41:48,880
will pre-define a zoom transformation and then maximize the similarity between some transformation

424
00:41:48,880 --> 00:41:54,000
in the latent space and a zoom transformation as applied to some image maybe you've experimented

425
00:41:54,000 --> 00:42:00,000
with code for doing that but that presumes we know we're looking for zoom in the first place

426
00:42:00,000 --> 00:42:04,400
what if we find ourselves looking out into more you know uncharted waters so to speak

427
00:42:05,360 --> 00:42:11,040
here we ask how we can learn kind of a vocabulary of visual concepts maybe one that you would apply

428
00:42:11,040 --> 00:42:16,160
to those style gains we just saw train on the net images right we don't maybe we could look for zoom

429
00:42:16,160 --> 00:42:19,920
but maybe there's all sorts of more interesting transformations we could do to those images

430
00:42:19,920 --> 00:42:24,240
but we don't know what they are yet how can we learn a vocabulary of visual concepts rather

431
00:42:24,240 --> 00:42:30,000
than pre-define them or labeling them after the fact so there are a variety now of unsupervised

432
00:42:30,000 --> 00:42:36,240
methods for distilling these kinds of transformations in latent space that find principal components

433
00:42:36,240 --> 00:42:40,240
of feature space of different layers the models activation maybe you've played around with methods

434
00:42:40,240 --> 00:42:45,600
like GAN space that search for and rank where the largest principal components of the feature space

435
00:42:45,600 --> 00:42:50,240
which do provide us with interpretable transformations but they're labeled after the

436
00:42:50,240 --> 00:42:56,480
fact so we don't know if they're meaningful to humans kind of in their genesis but we can we

437
00:42:56,480 --> 00:43:00,960
can describe them right by providing labels to them another point where the human kind of comes

438
00:43:00,960 --> 00:43:07,520
in the loop but we want to see if we can build in human vision to the discovery process right so

439
00:43:07,520 --> 00:43:13,520
to supervise it but to not pre-commit to what kinds of concepts we're searching for so in this

440
00:43:13,520 --> 00:43:18,960
project we're trying to build or define a method for building a visual concept vocabulary for an

441
00:43:18,960 --> 00:43:25,360
arbitrary GAN latent space so to put it more specifically we want to learn embeddings d maybe

442
00:43:25,360 --> 00:43:31,200
you've called this w we want to learn some kind of walk in the latent space z if again we'll focus

443
00:43:31,200 --> 00:43:37,840
on big GAN here of transformations that are salient to us in visual space and we can't define

444
00:43:37,840 --> 00:43:44,320
an objective and optimize our d our walk to produce a transform in x in the image because we

445
00:43:44,320 --> 00:43:48,560
want to learn the vocabulary concepts rather than pre-commit to them and we would have to pre-commit

446
00:43:48,560 --> 00:43:53,360
to what that objective is right in order to optimize d so we're going to take a different

447
00:43:53,360 --> 00:44:01,280
approach and instead sample the space of salient or possible transformations for some given point

448
00:44:01,280 --> 00:44:07,680
in space for some given z and then use those sample directions as a screen so to speak onto

449
00:44:07,680 --> 00:44:12,640
which we can project human perceptual judgments so that's a little bit of a gratuitous metaphor but

450
00:44:12,640 --> 00:44:17,200
maybe a useful way of thinking about it and then we'll we'll disentangle the concepts that are

451
00:44:17,200 --> 00:44:22,960
projected onto that screen into a vocabulary of open-ended compositional visual concepts

452
00:44:24,560 --> 00:44:30,960
and what we're interested in here is the overlap between what's represented inside a model

453
00:44:30,960 --> 00:44:36,240
so some deep features in a model's representation and concepts meaningful to humans in visual

454
00:44:36,240 --> 00:44:41,280
seeing understanding we're asking how we might start to define although not completely but

455
00:44:41,280 --> 00:44:47,200
start to define a shared vocabulary between the two or for a given model determine what lies in

456
00:44:47,200 --> 00:44:53,120
that set overlap and I don't have to dwell too long on a lot of the specifics here it's all

457
00:44:54,160 --> 00:44:59,120
online at that URL if you want to read the paper but as I mentioned the first thing we're going to

458
00:44:59,120 --> 00:45:05,360
do is generate a set of sample images that produce minimal meaningful transformations in images

459
00:45:06,000 --> 00:45:10,000
and then humans come in the loop again we're going to ask them to label them but here we're

460
00:45:10,000 --> 00:45:15,280
forming the basis for the data set that we'll build our vocabulary off of and we want to keep in

461
00:45:15,280 --> 00:45:20,720
mind that we want a vocabulary in the end that is both diverse so corresponding to a lot of

462
00:45:20,720 --> 00:45:25,680
different changes that you can produce in an image and specific where a single transformation

463
00:45:25,680 --> 00:45:32,320
corresponds quite reliably to one visual change across viewers so we do that by defining

464
00:45:33,120 --> 00:45:39,120
mutually orthogonal what we call layer selective directions and these minimize change in the feature

465
00:45:39,120 --> 00:45:45,200
representation at some layer of big care and at some layer we'll call it layer l and this allows

466
00:45:45,200 --> 00:45:51,120
us to capture relatively focused changes because we hold constant how much the representation

467
00:45:51,120 --> 00:45:56,160
can change at some layer and we do that for different layers to capture changes at different

468
00:45:56,160 --> 00:46:01,760
levels of abstraction so as you can see layers closer to the image output control or fine grained

469
00:46:01,760 --> 00:46:06,480
aspects of the image like the color of the walls and the bedspread and as we get closer

470
00:46:06,480 --> 00:46:11,280
back to the latent space we're allowed to make kind of more higher level changes in things like

471
00:46:11,280 --> 00:46:17,760
zoom and perspective of the scene and its composition so what objects are present so here we have a

472
00:46:17,760 --> 00:46:22,800
base set of minimal meaningful transformations that capture changes in images at different levels

473
00:46:22,800 --> 00:46:29,280
of abstraction we're going to ask people to label them because we don't know what's going on visually

474
00:46:29,280 --> 00:46:34,560
in these scenes right so we started at a pretty small scale with just four categories in the

475
00:46:34,560 --> 00:46:40,560
places data set and looked at big and trained on image net and places we'll just talk about places

476
00:46:40,560 --> 00:46:46,880
here and visualized a handful a few thousand of these directions per category so in each of four

477
00:46:46,880 --> 00:46:52,720
categories looked at cottages medinas so uh street marketplaces kitchens and lakes a mix of

478
00:46:52,720 --> 00:46:58,560
indoor and outdoor scenes and then asked people to just simply describe the overall transition

479
00:46:58,560 --> 00:47:03,920
that they saw when these directions were applied to different randomly sampled starting points in

480
00:47:03,920 --> 00:47:09,760
the latent space right so one direction might take this cottage to this snowy cottage and change

481
00:47:09,760 --> 00:47:15,280
something about the sky and change the snow so these these changes are still complex we can

482
00:47:15,280 --> 00:47:20,560
recognize that it's the same scene and we can describe in simple language what's going on

483
00:47:20,560 --> 00:47:24,720
but they're they're not disentangled yet right one direction might correspond to a number of

484
00:47:24,720 --> 00:47:30,240
different visual changes so we did a little preprocessing to capture you know what kinds of

485
00:47:30,240 --> 00:47:37,280
concepts are associated with each transformation and then we decomposed those annotated directions

486
00:47:37,280 --> 00:47:42,480
into a visual concept vocabulary consisting of single directions labeled with single words

487
00:47:42,560 --> 00:47:48,720
we formulated that as a linear regression and then solved for the embeddings of individual

488
00:47:48,720 --> 00:47:53,200
concepts in the latent space of our begin and then we can basically read those off

489
00:47:54,560 --> 00:47:59,840
of our matrix e and then transform the images by manipulating them some amount along those

490
00:47:59,840 --> 00:48:04,560
visual concept directions happy to talk more details about that if anybody's specifically

491
00:48:04,560 --> 00:48:11,120
interested or you can check out the paper itself we found over 2000 concepts this way

492
00:48:11,120 --> 00:48:14,960
corresponding to lots of different types of visual changes so we can reproduce

493
00:48:14,960 --> 00:48:21,760
transformations like zoom and rotation things like color but we also get kind of a unique

494
00:48:21,760 --> 00:48:27,520
set of concepts corresponding to aspects of scenes like their mood for instance there's a

495
00:48:27,520 --> 00:48:34,080
direction in latent space of big and that makes outdoor marketplaces more festive and here we

496
00:48:34,080 --> 00:48:38,160
see applying that direction to an example marketplace and it rolls out a red carpet

497
00:48:38,160 --> 00:48:45,600
hangs some flags and brings a lot of people into that market we can visualize kind of a

498
00:48:45,600 --> 00:48:51,440
a sampling of these directions each applied to two different images in different categories

499
00:48:51,440 --> 00:48:57,920
so some directions make cottages more manicured add arches to marketplaces add shadows or make the

500
00:48:57,920 --> 00:49:03,200
whole scene blue and we see directions like this that generalize across all of the categories

501
00:49:03,200 --> 00:49:09,600
it began that we looked at you can check out the lake category we can add sunsets but also do

502
00:49:09,600 --> 00:49:15,440
kind of scene specific things like add reflections to water or make a lake scene foggier make a

503
00:49:15,440 --> 00:49:21,840
kitchen more inviting or more modern and again we didn't have to pre-specify what exactly modernity

504
00:49:21,840 --> 00:49:28,000
would entail when applied to a kitchen we learned that through sampling what humans associate

505
00:49:28,000 --> 00:49:33,840
with a transformation that was sampled randomly right uh the humans labeled that as modern and

506
00:49:33,840 --> 00:49:39,520
then we disentangled the specific direction in latent space corresponding to that single concept

507
00:49:39,520 --> 00:49:46,240
word and once it's isolated we can apply a modern transformation and know that it corresponds to

508
00:49:46,240 --> 00:49:53,360
what viewers found to represent modernity in a kitchen well I said we know that it corresponds to

509
00:49:53,920 --> 00:49:58,800
what viewers see as more modern but we don't know that for sure right we still need to ask

510
00:49:58,800 --> 00:50:04,800
questions like how generalizable are these directions do they compose right can we add

511
00:50:04,800 --> 00:50:10,400
a festive direction to eerie and get something that's both scary and festive right or could we make

512
00:50:11,040 --> 00:50:16,000
a kitchen both more modern and inviting so we asked those questions in a series of behavioral

513
00:50:16,000 --> 00:50:21,680
experiments that I left for you to check out in the paper itself so we won't in the interest of

514
00:50:21,680 --> 00:50:26,960
time go through those here but we do find that these directions are composable and they're generalizable

515
00:50:26,960 --> 00:50:32,800
across categories so there are some cases where we can even add a concept that was learned in a

516
00:50:32,800 --> 00:50:38,640
single category to a different category for instance making a cottage more festive right or

517
00:50:38,640 --> 00:50:44,000
adding snow to a marketplace even though that's not traditionally seen there we ran a set of

518
00:50:44,000 --> 00:50:49,120
behavioral experiments evaluating the extent to which this is successful and isolating a couple

519
00:50:49,200 --> 00:50:56,560
of few specific cases where it fails okay so this this wraps up this method it's at a point

520
00:50:56,560 --> 00:51:02,160
now where we're trying this with some of the the art models that I discussed previously right so

521
00:51:02,160 --> 00:51:08,480
this was still just applied to big and trained on real-world images trained on ImageNet but you

522
00:51:08,480 --> 00:51:14,240
can imagine using a similar similar method to find dimensions of visual interest that are also

523
00:51:14,320 --> 00:51:21,280
meaningful to humans in the latent space of a model trained on art images and so decompose

524
00:51:21,280 --> 00:51:26,720
future languages underlying different genres of art into something describable so that we

525
00:51:26,720 --> 00:51:32,160
can make concerted manipulations to images sampled either from foundation models that

526
00:51:32,160 --> 00:51:38,080
correspond to approximations of real-world images or to models trained on on archives of art images

527
00:51:38,080 --> 00:51:45,200
themselves I'll pause here for any questions about this there's associated code also available

528
00:51:45,200 --> 00:51:53,120
on that project page I linked have a quick question please um was there a reason you chose

529
00:51:54,240 --> 00:52:01,440
big GAN over starting with like style GAN for this type of work uh no um just a couple of

530
00:52:01,440 --> 00:52:06,080
different code bases that already existed um and people that had worked on big GAN for

531
00:52:06,080 --> 00:52:10,160
like GAN dissection we had an easy way to dissect big GAN and hypothesize what kind of

532
00:52:10,160 --> 00:52:14,560
things might be there uh so a lot of the GAN dissection work started with big GAN and so I

533
00:52:14,560 --> 00:52:19,360
was picking up where that left off and asking if we could find like style vectors that corresponded

534
00:52:19,360 --> 00:52:25,360
to scene level transformations instead of individual neurons um but I have extended this

535
00:52:25,360 --> 00:52:31,440
to style GAN outside of the paper it's just not got it it's here yeah the method's pretty I mean

536
00:52:31,440 --> 00:52:36,320
there are a couple of different small changes you have to make um but the method is pretty model

537
00:52:36,320 --> 00:52:42,000
agnostic just like defining a set of certain directions that samples the latent space in

538
00:52:42,000 --> 00:52:47,360
kind of minimal ways and the the method I described here is definitely not the only one you could use

539
00:52:47,360 --> 00:52:51,440
for that right um you could sample them by just finding the principal components of the feature

540
00:52:51,440 --> 00:52:55,440
space or you could sample them randomly right you could just find two points in latent space

541
00:52:55,440 --> 00:53:00,320
interpolate and then get people to label what's going on there um we tried a lot of these different

542
00:53:00,320 --> 00:53:06,000
methods uh and found that if you if you make random interpolations between two randomly

543
00:53:06,000 --> 00:53:10,960
sampled points then there's just so much going on in the scene that there's not a lot of inter

544
00:53:10,960 --> 00:53:15,440
observer agreement in how people annotate what they see there's just too much going on so we

545
00:53:15,440 --> 00:53:20,800
need to isolate specific changes that's why we developed that layer selective method for isolating

546
00:53:21,520 --> 00:53:26,880
minimal changes um but what if we used kind of the the principal component method right or used

547
00:53:26,880 --> 00:53:32,400
something like GAN space um there we found that the principal components of the model's feature

548
00:53:32,400 --> 00:53:37,520
space aren't necessarily the most interesting to humans so we might get a ton of different types of

549
00:53:37,520 --> 00:53:42,320
rotating the scene but not a lot of different changes of mood or changes in color up there in

550
00:53:42,320 --> 00:53:47,600
high-ranked principal components um so that's where that method came from but it's agnostic to the

551
00:53:47,600 --> 00:53:53,920
set of directions and pretty model agnostic uh the annotation is another place where humans intervene

552
00:53:53,920 --> 00:54:01,120
here to to tie in that last question um but you can imagine trading a captioner on a label

553
00:54:01,120 --> 00:54:05,200
data set like this right a little larger than the one we collected so we're thinking about doing

554
00:54:05,200 --> 00:54:10,720
something like that uh but preserving the human annotation does allow annotation you know in the

555
00:54:10,720 --> 00:54:17,280
art context by experts as I mentioned so you might want to be able to do this at scale for a brand

556
00:54:17,280 --> 00:54:23,040
new model and just have automatic annotations you use something like clip right for the kinds of

557
00:54:23,040 --> 00:54:28,640
transformations you would see inside but preserve the opportunity for experts to to annotate kind

558
00:54:28,640 --> 00:54:36,720
of specialized smaller trained models and there are results too from big ganttring on a couple of

559
00:54:36,720 --> 00:54:48,000
different data sets if you're interested um I have a question regarding like the choice of

560
00:54:48,960 --> 00:54:57,360
uh n in terms of annotations um so how did you arrive at this number and how are you I mean

561
00:54:58,000 --> 00:55:04,080
how do you know like what number is kind of sufficient yeah good question um so I assume

562
00:55:04,080 --> 00:55:09,040
you mean the total number of images we needed to annotate and not the total number of annotations per

563
00:55:09,040 --> 00:55:19,520
image which end you mean I can talk oh I see I mean either yeah well okay so at both levels uh for

564
00:55:19,520 --> 00:55:26,080
the directions themselves we needed to collect at least two annotations to be able to measure

565
00:55:26,080 --> 00:55:32,480
intersubject agreement right uh we want to see if some direction is consistently producing

566
00:55:32,480 --> 00:55:38,720
meaningful similar annotations across annotators we need at least two people to annotate them um

567
00:55:38,720 --> 00:55:44,960
so for all the directions we evaluated we had two annotators label them and measured the interanitator

568
00:55:44,960 --> 00:55:51,840
agreement using a couple of different metrics blue and burnt scores um but for a subset of

569
00:55:51,840 --> 00:55:57,360
those we had 10 annotators annotate them and just had a look at interanitator agreement across a

570
00:55:57,360 --> 00:56:03,120
slightly larger group uh for expense reasons we didn't do that for for all the directions because

571
00:56:03,120 --> 00:56:07,040
it really wasn't necessary things didn't change that much and even in that subset when we went

572
00:56:07,040 --> 00:56:14,400
from two to ten per uh per direction and then for the number of directions that we chose to

573
00:56:14,400 --> 00:56:24,240
visualize it was not a very principled decision I'm afraid um we chose I think 64 z uh per category

574
00:56:24,240 --> 00:56:28,560
and then a bunch of different minimal meaningful directions for them corresponding to I think

575
00:56:28,560 --> 00:56:33,280
the same number of principal components that we looked at in the GAN space papers so maybe the

576
00:56:33,280 --> 00:56:41,040
top 20 in each category so it was a bit ad hoc that decision um the the things that's going to change

577
00:56:41,040 --> 00:56:48,640
we can distill vocabularies using this method for any size of annotation library right uh which is

578
00:56:48,640 --> 00:56:52,320
one of the one of the beauties and one of the things that gives itself to to some of these more

579
00:56:52,320 --> 00:56:59,840
ad hoc decisions um we're doing it analytically right if we go back to this we're actually like

580
00:56:59,920 --> 00:57:05,840
reading off we're solving for the embedding matrix um of word embeddings in latent space

581
00:57:05,840 --> 00:57:10,080
of concept embeddings so we could do this with like just a couple of directions

582
00:57:11,280 --> 00:57:15,920
if you only had one annotation per concept it only appeared once then you're just going to get

583
00:57:16,480 --> 00:57:23,040
for that direction um so as you increase the vocabulary as you increase the sample size you're

584
00:57:23,040 --> 00:57:28,000
probably going to get a richer vocabulary but it's still possible to do on a vocabulary of this size

585
00:57:28,000 --> 00:57:34,240
um so we're deciding now whether it makes sense to scale this up and collect like a number of

586
00:57:34,240 --> 00:57:39,280
annotations where it would be possible like I said to to train a captioner on them to be able

587
00:57:39,280 --> 00:57:45,520
to automatically label these directions rather than have humans do it so part of it is constrained by

588
00:57:45,520 --> 00:57:49,680
the tractability of experiments on mechanical Turk right how many reliable annotations you

589
00:57:49,680 --> 00:57:59,920
can get in some period of time awesome uh thank you yeah these are really useful questions these are

590
00:57:59,920 --> 00:58:08,800
great um kind of along those lines more of a random question for the printer like the single words

591
00:58:08,800 --> 00:58:13,520
for the labels yeah was it kind of agreed upon earlier like kind of what words you'd use because

592
00:58:13,520 --> 00:58:18,400
like for festive maybe someone would say lively or for inviting you'd say welcoming is there like

593
00:58:18,400 --> 00:58:27,040
kind of a similarity score for those words or really good question um no so this is it's only

594
00:58:27,040 --> 00:58:32,160
preprocessed with like a little bit of limitizing so we collapse different endings people might be

595
00:58:32,160 --> 00:58:37,600
using our different verb conjugations onto single verbs uh but festive would have a different

596
00:58:37,600 --> 00:58:43,840
direction from lively uh kind of a next step in post-processing that we've talked about but

597
00:58:43,840 --> 00:58:49,600
haven't yet done um is to just collapse across like wordnets and sets right so you could use

598
00:58:49,600 --> 00:58:55,120
something like that to find synonyms of festive and then approximate one direction for lively and

599
00:58:55,120 --> 00:59:00,480
then be able to break it down into something maybe more fine-grained um but there were no

600
00:59:00,480 --> 00:59:06,160
kind of heuristics or standards for the annotators except you know they did they did a practice run

601
00:59:06,160 --> 00:59:11,520
and looked at a couple of different examples and were asked to describe an overall transformation

602
00:59:11,520 --> 00:59:16,320
that captured changes at lots of different kind of levels of abstraction we can look at the specific

603
00:59:16,320 --> 00:59:21,920
what did we tell her that's on here yeah how would you describe the overall transition changes in

604
00:59:21,920 --> 00:59:26,240
mood changes in objects or features of the scene don't mention your describing images so standard

605
00:59:26,240 --> 00:59:30,320
kind of turk boilerplate just address the content what you see and then they could look at some

606
00:59:30,320 --> 00:59:35,680
samples and then after they did a practice run they did the annotations um so any interanitator

607
00:59:35,680 --> 00:59:41,680
agreement is just based on their word choice which in some sense is a raw window into perception

608
00:59:41,680 --> 00:59:46,560
but in some sense that's bullshit and there's going to be a lot of noise there uh and we did see

609
00:59:46,560 --> 00:59:52,400
that reflected when we used I don't have this on these slides um but when we use blue scores to

610
00:59:52,400 --> 00:59:58,160
to measure interanitator agreement so when we use these layer selective directions to generate these

611
00:59:58,160 --> 01:00:04,400
kinds of transformations if we get 10 people to annotate each transformation people might use

612
01:00:04,400 --> 01:00:09,440
somebody might say eerie somebody might say spooky right somebody might say scary to describe the

613
01:00:09,440 --> 01:00:15,120
sky uh that comes up as like quite different when you look at some methods of evaluating

614
01:00:15,120 --> 01:00:20,400
interanitator agreement so we used first scores as well that evaluate the semantic similarity

615
01:00:21,440 --> 01:00:28,240
instead of just literal correspondence words and found that annotations of these kinds of

616
01:00:28,240 --> 01:00:32,800
directions performed a lot higher when we looked at semantic similarity of annotations as opposed to

617
01:00:32,800 --> 01:00:39,600
just um just word based so there's definitely reason to start trying to collapse like that when

618
01:00:39,600 --> 01:00:44,720
we look at the vocabulary too but we haven't yet in some sense it's it's kind of beautiful because

619
01:00:44,720 --> 01:00:50,960
you can see all of the different words that people used to describe changes um but you'd get a lot

620
01:00:50,960 --> 01:00:56,800
more power right if you could combine annotations for festive and lively and vibrant under one

621
01:00:56,800 --> 01:01:06,880
umbrella bit of a trailer oh yeah thank you so much yeah any other high or low level questions

622
01:01:06,880 --> 01:01:13,920
inter just have a maybe one or two more things not much so ask away if you do I think more more

623
01:01:13,920 --> 01:01:20,800
of a higher level question I remember Ali in the first lecture um right you drew you had this

624
01:01:20,880 --> 01:01:26,080
visualization of like two points in the latent space and you know a last function that would

625
01:01:26,080 --> 01:01:32,720
steer like from one or trajectory one to the other but it was like something more like a curve

626
01:01:32,720 --> 01:01:38,320
or something non-linear um right and you mentioned with Gannon version if you just interpolate like

627
01:01:39,280 --> 01:01:45,760
draw a straight line between two points you have like all sorts of things happening I was wondering

628
01:01:46,480 --> 01:01:55,680
if there's like a I guess almost like a like a and I guess unsupervised not a random walk but a

629
01:01:55,680 --> 01:02:05,520
walk that would I guess lead to less perturbations I guess in in terms of like features I mean

630
01:02:07,280 --> 01:02:13,600
does it make sense uh yeah we I really wanted to do that for this project um maybe Ali can speak

631
01:02:13,600 --> 01:02:20,000
a little bit more about about his work there maybe after we stop this recording but um linearization

632
01:02:20,000 --> 01:02:26,240
of this is a huge over oversimplification um and that would be one of exactly what you describe

633
01:02:26,240 --> 01:02:31,840
as one of the things I'm most keen to try is taking non-linear walks uh through any of these

634
01:02:31,840 --> 01:02:39,440
subspaces um so very on point question haven't done it you should try and do it um but describing

635
01:02:39,440 --> 01:02:45,200
like this the semantic structure of latent space the semantic topology if you'll permit me that

636
01:02:45,760 --> 01:02:52,000
is a really interesting question um because even the visual meaning corresponding to

637
01:02:52,000 --> 01:02:59,360
some of these adjectives some of these words is not regularized or normalized in the latent space

638
01:02:59,360 --> 01:03:06,320
itself so if I take five steps in the festive direction it might take me five steps to get

639
01:03:06,320 --> 01:03:13,200
anything that will start to register to me as festive um but the walk size for a correspondingly

640
01:03:13,200 --> 01:03:18,960
large visual change so to speak in a different direction could be very different um so some

641
01:03:18,960 --> 01:03:24,080
transformations like making an image black and white this is anecdotal but you only have to go

642
01:03:24,080 --> 01:03:30,080
like one step in that direction and then we'll visualize the change almost immediately uh so

643
01:03:30,080 --> 01:03:36,960
we're not kind of we're not walking around in like a perceptually normalized space so to speak um

644
01:03:36,960 --> 01:03:42,160
and there hasn't been to my knowledge a lot of work that's addressed that everything's been a

645
01:03:42,160 --> 01:03:50,000
little bit at hawk um so thinking about semantic topology subspaces non-linear versus linear paths

646
01:03:50,000 --> 01:03:54,480
and how we can think about kind of the concept mesh underlying latent space for different

647
01:03:54,560 --> 01:04:01,120
generative models is extremely interesting to get to be a really cool area to do some working

648
01:04:01,680 --> 01:04:08,960
cool thank you yeah let's ask Ali about that figure once we pull off here um I've got one more

649
01:04:08,960 --> 01:04:14,640
thing to show you a quick example to hopefully spark more discussion unless anybody has anything

650
01:04:14,640 --> 01:04:27,200
specific about this project we can always come back all right oh geez well last thing I'm going to

651
01:04:27,200 --> 01:04:35,200
show you uh is still a beta and uh it's very it's very early and it's even thought development

652
01:04:35,200 --> 01:04:40,880
but it captures something um that I think is deeply interesting uh and I think might be interesting to

653
01:04:40,880 --> 01:04:48,480
you all um so the former method that I showed you for building shared vocabulary between humans

654
01:04:48,480 --> 01:04:55,680
and models relies heavily on language right and so we get some direction and we're able to share that

655
01:04:56,320 --> 01:05:03,040
between people and even to repeatedly use it to steer through model space um because we've given

656
01:05:03,040 --> 01:05:09,200
it a label right we've used language and you might even argue that you know that's constraining the

657
01:05:09,200 --> 01:05:15,440
space of what people can recognize in those initial sample directions because there might be some

658
01:05:15,440 --> 01:05:21,600
genus or quad aspects of of images that we don't really have words for um but are still like really

659
01:05:21,600 --> 01:05:29,040
recognizable or perhaps the verbal you know that the words you would use to describe something are like

660
01:05:29,920 --> 01:05:34,400
quite complex and you wouldn't type that into an annotation like on mechanical Turk maybe you'd

661
01:05:34,480 --> 01:05:40,320
want to describe the sky as like the sky you saw at your grandmother's house the day she passed away

662
01:05:40,320 --> 01:05:46,320
or some flowers as effervescent like latte foam or a sparkling drink but you're not going to type

663
01:05:46,320 --> 01:05:50,560
that into mechanical Turk and there's not a single word concept to capture it so that's

664
01:05:50,560 --> 01:05:55,920
going to get lost in the method I described and lost in a lot of kind of standard either annotation

665
01:05:55,920 --> 01:06:03,840
based or uh kind of hard coded direction search so I wanted to experiment with a way to capture and

666
01:06:03,840 --> 01:06:09,840
learn um directions without language and this is like deeply inspired by the steerability work

667
01:06:10,400 --> 01:06:16,000
of all these so you'll see a method here that is is similar to that in some sense

668
01:06:17,360 --> 01:06:23,040
but we're allowing the human to define the transformation that they want rather than

669
01:06:23,040 --> 01:06:28,160
pre-defining say a zoom or rotation transform using an algorithm we're allowing humans to come

670
01:06:28,160 --> 01:06:34,960
into the loop and define that transformation purely visually by interacting with very very

671
01:06:34,960 --> 01:06:41,040
small batches of images sampled from latent space or feature space at some layer and sort them into

672
01:06:41,040 --> 01:06:48,480
classes corresponding to some visual feature its presence or its absence and this provides a pipeline

673
01:06:48,480 --> 01:06:54,480
where users can steer just like in steerability work along dimensions that they discover however

674
01:06:54,480 --> 01:06:59,360
that they define and they define them purely visually so labeling what happened just as a

675
01:06:59,360 --> 01:07:04,640
matter of convenience but they're discovered um through vision so the way to do this is really

676
01:07:04,640 --> 01:07:11,440
simple um take some latent space again a lot of these examples are are using big GAN you could

677
01:07:11,440 --> 01:07:19,120
also use style GAN um take some latent space and sample images from it right if you're using a

678
01:07:19,120 --> 01:07:24,160
conditional model so we pick some category here we're looking at lakes inside big GAN image or

679
01:07:24,160 --> 01:07:31,600
big GAN places um sample some images for a user and then that user who's determining a visual

680
01:07:31,600 --> 01:07:38,400
dimension of interest kind of looks over that image space and sees if anything stands out to them

681
01:07:39,360 --> 01:07:46,720
across that that set of images so maybe here I noticed images that seemed kind of verdant and

682
01:07:46,720 --> 01:07:51,760
fertile uh and maybe more more spring light but not totally seasonal you'll see where I'm going

683
01:07:51,760 --> 01:07:57,360
it's kind of hard to describe and these were a little dreary or more wintry but there's not snow

684
01:07:57,360 --> 01:08:02,240
so it's not really winter they're just kind of less fertile and vivid so that's the distinction I

685
01:08:02,240 --> 01:08:09,360
want to make there um and the method is very simple just like the steerability work um and a

686
01:08:10,080 --> 01:08:16,000
another example of work from Bolle we define a transformation just by learning a hyperplane

687
01:08:16,000 --> 01:08:21,440
so training a SVM and learning a hyperplane it separates those two classes of images either

688
01:08:21,440 --> 01:08:26,800
in the latent space or in the feature space of some layer layers activations and then when we can

689
01:08:27,520 --> 01:08:34,320
steer some starting image in a direction that's normal to that hyperplane and steer it across

690
01:08:34,320 --> 01:08:40,000
those classes right so I could take an image that starts in the kind of dreary or domain

691
01:08:40,720 --> 01:08:47,680
or dusky or domain and transform it normally to that hyperplane and take it into the category

692
01:08:47,680 --> 01:08:54,320
of things that I thought was more verdant right or more fertile but I could specify that separating

693
01:08:54,320 --> 01:09:02,400
hyperplane just by sorting a shockingly few number of images um so we've done a couple of more like

694
01:09:02,400 --> 01:09:07,920
fine-grained tests here but just for proof of concept you can discern these directions with

695
01:09:07,920 --> 01:09:13,520
some degree of reliability with just like five to six examples of images in each category making

696
01:09:13,600 --> 01:09:18,240
it really simple to interact with something like this just by dragging and sorting a few images

697
01:09:19,200 --> 01:09:25,360
that are sampled in the latent space okay so there's a tiny example of a demo app we have for

698
01:09:25,360 --> 01:09:29,680
this um and we're switching where it's hosted so it's not online at this very moment but it will

699
01:09:29,680 --> 01:09:35,280
be next week but it's called the latent compass it was at NeurIPS Creativity I think last year

700
01:09:35,280 --> 01:09:40,160
the year before um you'll see the the home interface in a second but what we do is just

701
01:09:40,160 --> 01:09:46,640
what I said pick some category of BigGAN here it's BigGAN places um on the bottom you see images

702
01:09:46,640 --> 01:09:51,360
sampled from that category and the user drags them to the right and left of the screen corresponding

703
01:09:51,360 --> 01:09:56,800
to two different kind of categories of concepts they want to capture uh and then once the compass

704
01:09:56,800 --> 01:10:01,840
calibrates and we'll see that in a second and you can drag any new image and then transform it along

705
01:10:01,840 --> 01:10:07,360
that dimension so here we pick the closet category I've got full closets on the right

706
01:10:07,360 --> 01:10:12,720
empty closets on the left and the dimension I want to capture here is something like fullness

707
01:10:14,000 --> 01:10:18,800
so I'm going to see if I can I can learn a direction corresponding to the visual difference

708
01:10:18,800 --> 01:10:25,680
between these two categories drag any new closet onto that center line and transform it along that

709
01:10:25,680 --> 01:10:31,680
direction filling and emptying the closets and what if we tried a different category what if I

710
01:10:31,680 --> 01:10:38,800
wanted to turn a medina into a full closet right what is the type of fullness that's relevant to

711
01:10:38,800 --> 01:10:44,080
a medina oh well it's adding people instead of adding clothes suggesting that what's been learned

712
01:10:44,080 --> 01:10:50,960
there that direction in latent space is abstracting generalizable enough to capture some visually

713
01:10:50,960 --> 01:10:56,880
recognizable dimension of fullness that's meaningful to us in different scenes right and the model's

714
01:10:56,880 --> 01:11:02,400
able to to generalize it in a way that's not totally dependent on the types of objects it saw

715
01:11:02,400 --> 01:11:08,400
in one scene so it knows in a sense that clothes make a closet full but to make a market full

716
01:11:08,400 --> 01:11:13,680
we're not adding clothes we're adding people and so the fullness direction is something that adds

717
01:11:13,680 --> 01:11:18,720
more of whatever would make that scene full um to any scene that we're selecting in the model

718
01:11:18,720 --> 01:11:25,200
right and trained on so few examples of course this this is really quite imperfect but it's a

719
01:11:25,200 --> 01:11:30,640
good proof of concept of a way that users can interact super flexibly and really visually

720
01:11:31,600 --> 01:11:37,600
with dimensions of interest and use that to kind of explore and surf the latent space of a model

721
01:11:38,480 --> 01:11:43,920
by producing replicable repeatable directions that others can explore without having to use language

722
01:11:44,480 --> 01:11:49,040
that's kind of a different way of carving up the puzzle of how to explore and assign meaning to

723
01:11:49,120 --> 01:11:55,520
directions that we that we find in latent space okay that's at latentcompass.com

724
01:11:55,520 --> 01:12:03,040
and we'll be back up next week I think bad timing okay so to return to our frame here

725
01:12:03,920 --> 01:12:10,160
we've been digging a bit into this intersection between art neuroscience and machine learning

726
01:12:10,160 --> 01:12:15,600
ways to explore models that have been trained on human creation right at different scales

727
01:12:15,600 --> 01:12:21,520
to create a new to iterate and interpolate upon archives and then also to start to understand

728
01:12:21,520 --> 01:12:27,200
what these models are representing and if our ways of interpreting dimensions inside models

729
01:12:27,200 --> 01:12:33,440
can also teach us something about human perception or allow us to start to build models of aspects of

730
01:12:33,440 --> 01:12:39,440
human vision that are otherwise pretty intractable because it's difficult to formalize what dimensions

731
01:12:39,440 --> 01:12:44,240
underlie them where I could write down what dimensions under life physical scene understanding

732
01:12:44,240 --> 01:12:50,720
because I know Newton's laws I couldn't write down what dimensions underlie aesthetic perception of

733
01:12:50,720 --> 01:12:57,520
North African marketplaces or Babylonian tablets because I don't know what a large swath of people

734
01:12:57,520 --> 01:13:04,160
would find perceptually interesting in a bunch of marketplaces I know from cognitive science

735
01:13:04,160 --> 01:13:10,960
research certain heuristics to look for but that wouldn't give us a full set of what a diversity

736
01:13:11,040 --> 01:13:14,880
of humans might appreciate when looking at some scene especially things like its mood

737
01:13:15,760 --> 01:13:20,560
so we can turn here to these kinds of large unstructured generative models that learn

738
01:13:21,440 --> 01:13:27,040
entirely from data entirely from images and turn to them as like a fertile ground so to speak for

739
01:13:27,040 --> 01:13:34,000
starting to probe and represent human perceptual experiences inside their latent space and think

740
01:13:34,000 --> 01:13:40,480
of latent space that way right as a screen as I said before onto which we can project human experience

741
01:13:40,480 --> 01:13:45,280
and then once we have those projections we can rerun them and interact with them and collaborate

742
01:13:45,280 --> 01:13:50,960
with them to create outputs of deep generative models that are particularly exquisite and that

743
01:13:50,960 --> 01:13:57,680
represents some kind of collaboration between us and models of our our creation that are operating

744
01:13:57,680 --> 01:14:04,880
in parallel so that's where I will leave us my emails here I'm very discoverable online

745
01:14:04,880 --> 01:14:12,000
but you're welcome to write me questions anytime and I will wrap here and we can have a more

746
01:14:12,880 --> 01:14:18,080
casual discussion unless anybody has any last questions for this part

747
01:14:18,480 --> 01:14:39,120
Thank you so much sir this was really interesting and inspiring with all the

748
01:14:39,360 --> 01:14:48,640
acidic decreasing slides and every moment of that was really full of thoughts I think that

749
01:14:50,560 --> 01:15:04,240
you open a window to semantically and qualitatively looking at these latent spaces and

750
01:15:05,200 --> 01:15:08,560
sort of our imagination and

751
01:15:11,360 --> 01:15:19,680
where we dream and where these models that we create dream so I really appreciate that

752
01:15:21,840 --> 01:15:26,480
I'm going to stop recording and then see if there are more questions

