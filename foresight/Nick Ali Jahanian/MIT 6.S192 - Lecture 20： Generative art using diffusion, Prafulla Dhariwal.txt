Hello everyone, welcome to your course, AI for Art, Aesthetics and Creativity.
Today we have a very special speaker, Prof. from OpenAI, and he's going to talk about
creating art and artistic work and images in general, these diffusion models and probably
you have already worked with the glide collab, so he's going to walk us through that as well.
So let's get us started, Prof. I always ask if please you can share with us what motivates
you working in this space and also giving us a little background about yourself.
For sure, thanks for having me here today, by the way this is really exciting.
Yeah, so a background about me, I was an undergrad at MIT in computer science and math and then after that,
I came to OpenAI to do AI research and I've been here for five years doing research on unsupervised
learning, generative models, all kinds of things and what motivates me to do this research.
When I was in college, I was excited by the idea of trying to understand what makes humans intelligent
and I think I attended a few talks, which were really amazing and I felt like there's a lot of
amazing progress happening in this field and I just wanted to part of it, see what's happening, see what
I could contribute and then one thing led to another and here I am.
I think so far, wouldn't say we are very close to unraveling what makes humans intelligent,
but we've made a lot of progress I think in these years, so it's been pretty fun.
Cool then, so I'll just get started.
And anyone, feel free to just ask any question at any point, pause me if anything feels confusing,
if any notation is understood. I don't see the chat window on my screen directly, so if you could just
directly tell your question, that would be easier or if Ali, if you see something in the chat, just let me know.
Cool, I'll get started.
So I'll begin by just showing a few examples of very powerful creative ML models from the past few years.
The first one you all might have already seen in news GP3, the language model from OpenAI.
And one example I'm showcasing here is like these language models.
They show like a very few examples of something pretty simple like here, like on the left you see examples of like poems by specific black writers.
And on the right, then you can see once the model has seen examples of this kind, what it can generate.
And it's getting poetry from just a few examples. This is pretty crazy.
There's a second model I'll show you next.
This is the audio playing.
Yeah, we can hear your audio.
You may bring it, and that's it.
Anyhow, so that was a sample from a model for generating music called jukebox.
And everything here was generated from the model, the music, the singing, how to sing it, how to pronounce the lyrics and everything.
All the model was given was the lyrics and an artist and it produced all of this by itself.
Does the generative model like produce the music and like the sentence separately or together?
Everything together. Yeah, so because kind of how you sing something kind of has to go with the music that's provided by Swasa, right?
It's kind of hard to like generate them separately from each other.
What was that your question?
So I can read from the chat that people are getting excited.
Someone has a loud and other is saying this is freaking awesome.
Thank you.
Share or talk if you like.
Oh yeah.
Sorry, I can't see the chat window so feel free to just talk.
Yeah, when we heard samples from like that from these models we were also amazed.
The model I have here is the slide model you guys might have seen in papers. And here you have a model that given a text prompt is generating a visual representation of it or whatever it imagines what that the text kind of signifies.
So you could see, and these are things that are go back to your point Ali you made earlier about composition. These things involve a model really having to compose a lot of different concepts together like robots meditating in a way past the retreat, but it is able to imagine this.
In the last few years I think like ML models for such very hard creative tasks have become really good. And today we'll see like what are some of the kind of concepts driving this progress.
And so before I even start down that route, like why are we trying to, you know, from a research perspective like trying to train models that you know create things. Well, one concept here is that, you know, this as this code by Feynman says what I cannot create I cannot.
I don't understand training models that can, you know, create things be images or your video and so on. It's kind of one of the hardest tasks in those domains. And if you really care about whether models can understand images audio video so on, then one of the best ways to know
if you're making progress out of these models are really learning something advanced is to see if they can create really complex things and really hard to understand things.
And for people who care about representation learning or something this is one, one way you can know you're making progress on such stuff.
And there has been a lot of progress in this field of, you know, trying to create things from models or what we call generative modeling. So here you see just in this very small domain of phase generation, things that GANs could create in 2014, versus things that they can create in
2018, like, it's absolutely astounding how much progress has happened in the past few years.
So, what is a generative model. So, you can think of what our inputs here in our data set to look like just a collection of examples, x1, y1, x2, y2, xn, yn, where x here represents, let's say an image and why some label or some other information describing this image.
So you just have the sample from some natural distribution of images, p of x comma y.
So you could have like images of corgis, ostriches, goldfishes and so on. You want to train a model that can learn this distribution you want to train a model that then ask for a corgi produces a corgi and ask for an ostrich produces an ostrich, or so on.
You want to learn p of x given y, given some label y, corgi, ostrich, goldfish and so on. Can I generate a real image or an image that looks real from this distribution.
And one such a model is trained, you can use it to generate novel samples.
So you can generate corgis, ostrich, goldfishes that are actually real haven't been seen before but look like real images.
One of the things I guess that matters is, is how you evaluate such models because if you if you don't have evaluation metric you can't tell you're making progress.
And we won't go into too much detail here about these metrics but one of the metrics use was FID for measuring image generation and what these metrics are trying to capture is like fidelity and diversity.
Fidelity would mean like how realistic or how correct an image looks versus diversity would be like how many different kinds of images such a model can generate.
And so GANs were kind of like the state of the art for difficult image generation benchmarks before diffusion models came along which we will not talk about in our talk today.
The progress in diffusion models has been pretty recent, it's been just like the last couple of years there's been a lot of papers and even in these people that you could see like things have been improving since 2020.
And you could now generate realistic faces, you know, lots of different categories of images from ImageNet and so on from these models.
So it's a pretty exciting field and these models in one of our recent papers we showed them to be actually better than GANs at generating images and so I'm pretty excited by these models and that's what I'm going to cover today.
There's quick graphic here for like how things look like when diffusion models generate an image.
Let me just play it again.
So let's go to what these models are.
As you saw in that graphic, you could see that that image was started out looking like noise and you finally got a real image out by like the slow process of noise converting to an image.
And what is actually happening behind the scenes here is you have a fixed process that adds noise to a training image. So let's say you start with X0 on the left here as an image.
So that's just a dog ball on the left.
So a fixed process that slowly adds Gaussian noise to this image. So at each step you add a little amount of Gaussian noise. And as you go from left to right by the end, at the last time step B, you have just pure noise left.
And what the model is trying to learn is to undo this process. It's trying to reverse it. It's trying to take some noise damage and be noisy a little bit, make it a little less noisy and so on.
How do you obtain a generative model out of this? Well, if you train a model to reverse noise like this, then at test time when you actually want to use the model, you could start with pure noise at the end.
You could start with XT. Then you could run it step by step backwards to remove noise from it and try to produce an image from it.
Any questions on this diagram?
Okay, I'll just check the chat to throw the question there.
Okay, no questions.
Okay, so let's remember the notation from your X0 is an example from the dataset, XT, capital T, XT is noise and there's intermediate steps, X little t's, we call them, where you have like some slightly
noise image. And we can, to introduce more notation here, you could represent one step of the forward noising process with a distribution Q of X little t, given X little t minus 1.
And right now, I'm going to use Gaussian noise as a noising process. So we're going to add a little bit of Gaussian noise to this.
And the thing about here is this in this notation, there's some mean, which is centered around the original noise damage XT minus one, and there's some variance one minus alpha T here of how much noise that is being added to this image.
So this is the forward process. This is the process that adds noise to images Q of XT given XT minus one. What we are learning is the reverse thing. We want to denoise this image.
So we were learning P of XT minus one given XT. And what you can show is for very small
noising steps where the amount of noise added is very tiny. The reverse process kind of looks like the forward process.
This is kind of a set of points. Maybe I'll go in a little bit of an example here. Let's say, looking at a single pixel here, it had some real value on the real line, let's say
0.8. And then you added a tiny amount of Gaussian noise to that thing. So it became 0.81 or 0.79 or so, depending on what noise you sampled.
Now you're given this new value 0.79. This is the noise value. And you want to predict the distribution of what could have been the value it came from.
If the amount of noise added, that you added in your step was very tiny amount of Gaussian noise, then the reverse prediction also looks kind of like a Gaussian.
So it looks like, oh, it's somewhere around 0.79, some distribution that where you came from. I mean, 0.8 is pretty close to 0.79 in this situation.
And so you could write that down as a model that is predicting the mean of this reverse process and the variance of this reverse process, mu theta being the mean and sigma theta here being the variance that you're trying to predict.
So far, so good. Any questions at this point?
Okay, so to summarize the notation again, XT minus one to XT is a process that's adding noise and the process we are trying to learn is XT to XT minus one to reverse this noise.
And this looks like a Gaussian in the forward direction and predicting a new Gaussian in the reverse direction.
A paper showed you could do some forms of training tricks to make this process simpler. You don't have to add noise little by little at every step. You could just directly sample an intermediate XT given your data example by just adding a lot of Gaussian noise to it.
And it also showed instead of trying to predict the mean of the reverse process, you could just directly try to predict what noise was added to the image.
This is possible because you could write the mean in terms of the noise that was added.
Trying to predict the variance can be just simplified to just using a fixed variance or a learnt variance. We won't go into that today. All you should get from this is that to predict the reverse process where you are trying to predict the mean and the variance of the Gaussian to reverse that
noising process, it's enough to try to predict what noise was added to the image.
So how does this look like when you are actually training these borders? You take an image X0, you sample some random noise, you sample some Gaussian noise and you just combine these two to produce a noised XT.
There's a formula here of how we've combined them.
You can think of this combination as something that is kind of like interpolating between the image and the noise. So at t close to zero, you should just get the image X0 and t close to capital T, you should get complete noise.
And this kind of interpolation factor, alpha t bar, kind of plotted it here, it goes from like one near t equals zero to zero near t equals capital T.
This kind of controls how you interpolate between a fully denoised image versus a fully noised thing.
And at training time, you're just sampling all possible combinations of mixing of noise and image and you're trying to denoise all these combinations.
So what is the model trying to do now? It's trying to predict, as we said in the earlier slide, it's trying to predict what noise was added into the image.
So you take in the noised XT, you take in what time step or kind of like an indication of where in the process you are, you tell the network, hey, I am at this step in the process, this is my noised image.
What noise was possibly added to this image? So it's trying to predict epsilon.
And what it's being used to train with is just like simple L2 loss, like just take the mean squared error of the difference between the network's prediction and the actual noise that you trained with and you try to minimise this loss so that you can train the network to predict what noise was added into the image.
So intuitively, you can think of this as like, well, if I'm given a noised image, and if I can predict what noise was added to it, I can kind of like subtract that noise out, right, try to get a real image out.
And this is kind of what is happening when you're training a diffusion model. It's learning to denoise images.
Any questions so far?
No questions.
Okay.
So what does the model that kind of does this denoising usually look like?
Kind of models that we have in our papers usually look like these convolutional unit style models where the U kind of signifies kind of like how the shape of the model here in this picture is looking like, but to think of it as just like a model that runs a bunch of
convolutional images, it kind of like down samples the image down into smaller and smaller spatial fields so that it can like learn features at different levels of granularity and then kind of samples it back into something that looks like a prediction of a noise.
I don't have to go into the details of architectures, but just to give you an example of what kind of neural nets are trained to perform this task, this is how they look like.
So, okay, we have a model that is trying to now denoise images, it's trying to predict the noise that was added to an image. How do we go back to actually getting a generative model out of this?
As we talked about in an earlier slide, it's equivalent to predicting the mean of the reverse process, like you can write down the mean of the reverse process in terms of the noise prediction.
So now that you have a network that can predict the noise that was added, you can also write down a network that can predict the mean of the reverse process.
And once you have something that can tell you the mean of the reverse process, you can run the reverse process backwards.
So you can start with noise x t, you could run, you could sample from this reverse process p of x t minus one given x t, you know the mean, and we have fixed the variance to something to do one step of sampling from this process.
You do one more step, one more step and so on. And as we talked about what we're doing was denoising, right, we're trying to like learn a process that removes noise from images.
So if you start from pure noise and you're denoising it one step at a time, by the end of the process, you would have something that looks like a real image.
So, so what we've covered so far is how you train these models and how you sample from these models. Any questions so far?
Okay.
Um, it's okay.
A bunch of theory there, but what you should remember is you train for denoising, and you can derive a sampler from it, once you've trained for denoising.
What do you do next? Well, you could now make the model class conditional, you could provide labels at training time.
So you could provide, you know, let's say you're training on ImageNet or something, you could have labels that say this image is a goldfincher, this image is a Corgi or so on.
And you could make the model, the denoising model class conditional, you could provide these labels, the model so that, given this label, it tries to produce an image from p of x given y, like the distribution of images that are kind of represented by this label.
And it's pretty simple, you just throw in a label into the model at some point, so that it now has this extra information when it's trying to denoise images.
You could also do something like up sampling, you could ask the model, given this low resolution image, what would be kind of the high resolution image that could be generated from this.
So again, just like throwing in a label y, you can throw in a low resolution image as extra conditioning information into the model as it tries to denoise.
So, we've now talked about models that are class conditional. The thing is, if you just train models like this, where you give them a label for an image and you train them for producing the image given the label, they're not very good at doing this out of the box.
They kind of produce very incoherent samples. And one of the tricks that we developed to kind of fix this was the trick of guidance, where what you do is you train a model to look at the images that are being generated.
Use a classifier to classify what is the label of this image. So you kind of look at a noisy image and you're like, you know, whether you ask the model, hey, does this look like a dog or not.
So you train a classifier on these noisy images, then you take a gradient of the classifier, you ask the classifier, hey, how can I increase the likelihood to make this image look like a dog.
Because you can run the classifier forward, you can get a, get a probability from the classifier of it being a dog, you can also differentiate this function to get the gradient of how to change the image so that this probability increases.
And then you augment your diffusion model with such a classifier to kind of guide it towards generating images that are more likely to be classified as a dog by the classifier.
So how do we end up doing this in practice. Okay, so you can train a classifier on noisy images, you can just take your data set of images, noise them and train a classifier to predict the label of the noisy images.
And then how do you guide now your generative diffusion model to use this classifier. Well, you run the classifier on the noisy images, you predict the probability of, you know, the class label under the classifier so whether something is a dog or not.
You take the gradient of this prediction to obtain kind of direction for which the model should change its input to increase the probability of this image, you can add this direction into the mean of the reverse process that you are already going towards.
So in terms of the actual formula, it just looks like adding an extra term to your mean prediction, which is the gradient of the log probability of the prediction of the label, given the noisy image.
Questions on this, this is, this is important and this could be a little complicated.
It seems there are two questions before this. Can you read them.
Do you still need a classifier.
Once the model is trained.
So here by the model you mean the diffusion model, right.
Yes, you still need it, because it is part of the sampling process, you're using gradients directly from the classifier in the sampling process, so you still have to keep the classifier around when you sample from the model.
So if you look at the next question is the underlying representation of the classes for condition.
I followed the question, Ben, could you explain.
Okay.
I have another question.
The s in the term. Is that just a hyper parameter or
we'll get to that term in the next slide but yes that is just a hyper parameter.
The main thing from this slide is like we previously had a reverse process that looked like a Gaussian with some mean mu and some variance sigma. We now have a modified reverse process where we've just modified the mean mu with an extra term, which is scaled by this
parameter s has the variance in it kind of for appropriate scaling as well, and it has this gradient term, which is the gradient of a classifier on noise damages.
And we're kind of basically using this gradient to kind of guide the model towards directions where the classifier would predict a higher likelihood of the label being correct for the noise damage, so that the conditional model produces an image that is more correct.
Another question. Why is the variance in the additional term also included.
That's just how it popped out from the derivation.
Oh, okay.
I guess you can think of it as kind of like the step size of these things is controlled by the variance if you have a Gaussian with a very small mean in the reverse process and you don't want to take a really large step with your gradient, because you'll really, you'll pop out very far from
the reverse process should have taken you to the reverse process is taking really small steps, which kind of can be thought about by its variance, and you also want to change that process only by that much amount.
Does that make sense.
So, I guess the variance terms like a cap in step size.
Right, like to the maximum and the class very gradient is maybe somewhere between zero and one or something like that or
I don't know if it has any explicit range here, but I mean it's kind of mostly just direction.
Yeah, and you're kind of scaling this direction by a step size.
And the extra hyper parameter is if you want to kind of like make these steps bigger or smaller than what is naturally there so that's the extra parameter s that we'll talk about in the next slide.
Okay, thank you.
Yeah. Okay, so the parameter s here. So what we found was if you just actually use the step size that pops out from the derivation.
So as being one, no, no hyper parameter, it kind of doesn't do that much. So on the left you see the samples with as being one.
They don't look like any image from any particular class, but it turned out when we added this extra hyper parameter and just bumped it up.
So you have scale 10 here on the right, they actually start looking like samples from the distribution of a particular class.
So you can think of this extra hyper parameter s as kind of helping the model focus on the modes of the distribution because you're kind of narrowing down the possible things that it produces, at least that's what we saw empirically.
However, the trade off here is because you're narrowing it down well, they also kind of start looking similar to each other, the images that are produced.
But anyhow, the way to think about the scale factor here is just that it's controlling how much guidance we're using how much is the classifier influencing the final outcome.
And when you use a small value for us, it's not influencing that much when you use a large value it's influencing a lot, and the effect of their influences, you're kind of collapsing your distribution towards the modes of whatever the
table that thinks is kind of the best representation of that label, they were very high scale will just collapse to the thing that the classifier is most likely to classifiers that they just not always what you want you want some kind of diversity, what you produce.
There's some, there's some like intermediate value of scale that is kind of the best that you want to use.
And this is kind of how the process looks like in practice here on the bottom you here you have a usual like diffusion process with scale.
Let me see I can't see the image. So scale zero you're just using no guidance, and then when you turn on guidance, and now using the gradients of the classifier can nudge the process in the direction where it's more likely to produce that butterfly.
So the scale up even higher you're nudging it even further out from its original reverse trajectory into this new trajectory, where it's now producing a very clear butterfly.
So the scale parameter is kind of controlling how much guidance is happening and how much the model is being nudged out from its original distribution towards this new better distribution.
So similar things, instead of labels you could now have text descriptions of images. So, same model class you're still conditioning on something, but this conditioning thing why instead of being a label is now a piece of text that's a robot's
kind of a pass now retreat.
And you could train basically the same kind of models, all you have to now change as well you don't have a classified now right, there's no classifier that is predicting a label you're that you have to predict the whole sentence, if you try to do that.
So how can we do guidance in this situation.
Well, first, let me go in.
Oh, okay. Well, first let me go into how you can even pass in conditioning information to diffusion models, which look like text, you just, you can just simply run a transformer on the text and just attend to the representations of the text in the model.
What's important is just a pictorial representation of how to deal with text being passed into these models, you can just run a transformer model on this and just have your original convolutional unit architecture attend to this model when it's trying to do the denoising.
Back to guidance, how do you actually guide when you have text as the kind of label information. And one of the things you could do is use clip.
I think you guys cover clip in a previous lecture Ali you said that so can.
Yes, sounds sounds good.
Yeah, so I'll skip clip. Okay.
So I'm assuming you guys on the clip, but basically clip is a model where you have an image encoder and a text encoder, and it's trying to predict how close the representations of the image and the text.
And so you can use clip for guidance, you can ask, hey, I have this noise damage, I have this text description, run the image encoder run the text encoder from clip, how close are these representations.
If they're close, then you're going to get a high dot product here, you can take a gradient of this dot product and get a direction to increase this dot product.
And that's the gradient you're going to use for guidance, gonna ask the model hey can you increase this dot product so the image, the not the image that you're trying to produce from the reverse process is close in representations to the representation of the text that you're provided.
So this is how clip guidance works.
What you can do, which we sure enough, which was short in a paper on classifier free guidance is you could skip the classifier completely.
And just train a usual diffusion model for for reversing the process but train it sometimes without labels. So sometimes don't, don't tell it what was the text that described an image.
And then at test time you ask the model, which direction should it go, given the label and which direction should it go without the label.
And then you move your predictions in the direction of the model predictions when it was given the label so in the formula here if epsilon was the epsilon theta xt given why was the prediction of the model with the label and epsilon theta xt given
the empty set five was its prediction without the labels, you're kind of taking the difference of these two and using that as your direction to kind of nudge the model in.
And again, you have the scale factor s outside of this direction, which telling the model to move in the direction of the predictions with the label.
And when you use as greater than one, you'll be moving a lot more in the direction of the predictions with the label.
And the cool thing about this way of guiding is that you don't need a separate classifier or a clip model or anything, you're just using the diffusion model itself for guidance.
You're directly just asking the model, it's own kind of prediction of which way it should go to increase the probability of the generated image being from the correct class.
Any questions about classifier free guidance.
What I have is, have you thought about implementing something like, okay, at this, at each stage, for instance, let's talk about the butterfly example at this stage I want to add something to this image.
And so the text, you know, can gradually form the shape like for the image like, okay, I want the butterfly and then on top of it I want this flower and then this, you know, gradually giving more idea of how your butterfly want to be depicted.
So you're doing this in steps for image, and then you are injecting the tokens from the clip to your, you know, your network for for image generation.
And so what if gradually adding things that you want to be in that image.
That's a great question, I haven't done this, like we haven't done this directly, but you can kind of do this right, you could like, you could run your reverse process to some point with with your text conditioning being just the simple thing hey it's a butterfly.
Then you could continue with a new text prompt for guidance, hey, the butterfly looks like this or so on, and keep going. Maybe that works. Not sure.
You could do something else where you just run the whole process, first generate a butterfly.
You take the butterfly, you noise it to go back in the process, and then rerun it, but now with a different prompt, so you kind of modifying this generated butterfly in a new direction.
Then, you know, noise it again and rerun it again with a slightly different prompt, you kind of be like slowly changing this generation iteratively in these like kind of like iterative modification.
So in another slide later we'll show how to do this with something like in painting, but if you just wanted to do it for your direct image. This is maybe how you would do it.
Does that kind of answer.
Yeah, yeah, I think that's a very interesting, you know, thought and yeah I appreciate your answer.
I think that in painting could be one way of thinking about it. Yeah.
Yeah, but what I was trying to say there was like yeah, you could also do it without impending by like kind of modifying the full image by like renoising it and reproducing.
Yeah, yeah, that also makes sense. I think that I was also referring to more like just the way that by removing the noise you are, you know, trying to somehow refine the image.
This also in steps could, you know, add more context to the image.
And there might be different ways of implementing it.
Yeah.
I think that's good.
Okay, thanks.
Okay.
So, in our guide paper we kind of compare these two forms of like guidance for text conditional models clip guidance was classified for guidance.
And here are a few samples like representative samples from the model. So on the left here is just samples without any guidance. This is just a pure conditional diffusion model there is no form of classifier clip guidance or classifier free guidance.
And kind of see, you know, it's kind of getting the prompts of the pram pure steam glass window of a panda panda eating bamboo. It's kind of alright, but it's not very coherent.
Then you do clip guidance with scale to start getting better with the classifier free guidance ones look the best in in all the tests we did.
And I think part of this might be just that it's, it's not exactly the correct thing to use clip part of it just might be that it's kind of better inductive bias to use classifier free guidance.
There could be a lot of reasons but at least empirically, this was working better in practice to generate more realistic samples from these models, and you can see guidance does make a big difference in, you know, generating more realistic things, but it also does kind of make, you know, it's
kind of like more collapse effect happen all of these samples kind of start looking a little similar to each other when when you do a lot of guidance.
So, what what what have we done here we've trained a model that you know given a text prompt can generate images and we've done it for this diffusion technique.
And this was what the strain in the glide paper, and we then showed that this model actually was beating the older open a model, Dali, which was actually a bigger model, which was trained in a very different fashion.
The strain is using an order aggressive model on these like discrete VA tokens.
And it, the new diffusion model not only generated things that looked a lot more realistic it actually generated them faster and use fewer parameters. So, this new model class is actually a lot nicer to use for these tasks than the older class of models.
One cool advantage of these models is also because they're not doing this thing or to aggressively they're you know just generating and hold image you can do these things that are much harder to do with these older autoregressive models can do things like in painting.
So you could mask out a portion of the image, and then ask the model to kind of fill in that portion.
And how would you do that well just like we passed our conditioning labels in the past you could just pass in kind of this like half filled image as extra conditioning information to the model.
So you take this image and a mask on top of it, you provide this extra information to the model when it's trying to run its generative process and it's going to try to now think of this as a label hey like this is image what are the possible images that correspond to this label for this kind of
a masked image, what are the things that could complete this image. And what you're providing is this kind of like image X zero with a little bit of region must and a mask and that tells what is the part of the image that has been masked.
Now, to text condition and painting, you could provide an, you know, an image with a mask and you could also provide a text label to tell the model how it shouldn't paint the region.
So these are examples from the paper. So on the left here, you have the text label being zebras roaming in the field and you have this image with a green mask on it. So the masked region was removed.
And the model was asked to fill in this image conditioned on this product. So now it's going to try to fill in not only something that kind of completes the image correctly, like isn't there the conditional distribution but also kind of matches the product.
On the right here you see something with a girl hugging a corgi on a pedestal. And it's kind of matching the style of the image very well here if you can see it kind of looks like it's like painting.
Kind of nicely like blended in. So this is really cool thing which you can do very easily out of the box with diffusion models but it's kind of much harder to do with other classes of models.
And you could take this idea iteratively, like you could now erase a region of image. So let's say we erased the region on the left here, and you first filled it in with, you know, a cozy living room, and you raised a different region.
And you know, asked for a painting of a corgi on the wall above the couch. When you get a painting there, you raise another region, put a coffee table, put a flower vase and so on.
So this is one way of kind of doing the thing you talked about Ali, but you kind of like generate things iteratively, but this is doing it from painting erasing regions, raising very specific regions, then asking the model to fill that region in with the thing you want.
It doesn't cover all kind of modifications that you want to do, but it does cover things that you can represent as like adding things one by one into an image, if that makes sense.
So like stuff that you maybe cannot do with this is like, you know, change the style of an image completely the full thing, because well, if you just erase the whole thing you couldn't, it wouldn't have anything to condition it can't use the style but things like this where you add things, you
can do it pretty easily through iterative painting.
Any questions so far on the painting side of things.
So Linda is asking if the collab is available for painting I think I saw it on the website.
Yes, the collab that we released in the guide repo the thing the third one is the second one that one doesn't painting.
Basically, all you like to do that is you'll have to provide this extra image, and you'll have to provide a mask or like mask out a portion of the image and then provide a mask that tells what has been masked.
And then you just run the guided diffusion process as usual, but now with this extra information to try to impaint this region.
I'll go into kind of notebook later but yes, the notebooks there.
Diego also has a question.
Can you remove objects using printing. Yeah, I mean, so let me go back to the slide.
I mean, I guess, technically, in the very first one we removed the thing right we just must out whatever was on the wall in the left. And here we ended up adding a painting of a colleague but you could just ask for nothing.
And then it would just fill it with the wall.
I don't know if there's an example here.
Yeah, in all of these things we kind of change something modified something but if you just don't give any prompt is just going to try to fill it.
Without any extra information is just trying to make the best possible completion, and that could be kind of like removing an object.
Does that answer your question.
Assuming yes, never move on.
Okay.
Well, you can take this idea further and you can do out painting kind of so like previously we drew a mask that was inside the image, but you could also kind of move the rectangle that the model is focusing on outside the image.
So now the mask looks like a strip of things around the image that is masked, and you can ask the model to fill that thing, and you could keep moving this rectangle around to kind of expand out from an image.
This is something that Holly heard, and then she like to this central image here and then she kept moving the square out kind of expand out the canvas of the model and ask you to keep filling in extra information outside of the region.
At the end of the day for the model is there is just like some conditioning information some mask, and it's going to just try to fill in in that region whatever it thinks is the best possible completion.
It doesn't have to be inside and be outside as well.
So, one other, I guess, important thing is like, we talked about the release notebooks the breeze notebooks is kind of the released model which is the filtered glide model.
So in our paper we talk about this where we, if we looked at, you know, kind of the things you could generate with the big original glide model, and there were a lot of, like, problematic things that it could generate that made it unsafe to release the full big model.
So release a small smaller model on a filter data set. And it cannot generate things that look as impressive as the big model, but it's still kind of can generate realistic looking images for like some of these easier prompts.
But yes, there was going to be a little bit of a performance gap between using the filtered small model that has been released versus for the best images you can see.
You can still generate a lot of cool things with the small filtered model. These are some of the things I found on Twitter that people have generated with the notebooks that we released.
So on the left here.
I think what they did was, they kind of did the painting thing, but they just went kind of like in a panorama fashion left to right, and kind of kept asking the model to fill these landscapes.
The guidance scale a lot to make it very artsy think in the right.
I've kind of problem like they've done this out painting thing but
I don't know how they got those structures, but I think a part of this part of the fun stuff here is kind of these prompt search or prompt tuning things where you kind of find these prompts and generate very specific and artistic styles.
And if you find very cool prompts and you can now use these tricks of our painting and so on, kind of like keep expanding it out to generate these cool pieces of art.
This is another thing I found on Twitter where they trained a classifier for guidance model on conceptual captions and I think this is like a flower a space flower with some space.
Our team.
Super cool.
There is a question.
You want to read it.
Let's see.
What did it create that was dangerous.
So, or maybe comment.
Oh, sorry.
I think I, yeah, what did it create that was dangerous. Yeah, I guess, well, for all the days I would recommend just reading a paper. I mean, there were, and I wasn't the one who did the safety analysis here was the opinion.
The people who work on safety at opening and Alex.
But I think it was stuff like violence it was stuff that could be used for like, if it makes it for like misinformation and so on. But I mean these models are pretty powerful so you could generate lots of things that you don't want to be floating on the internet.
I mean, the trade offs are hard here right because like, on the one hand you do want to, you know, put these powerful models out there in the hands of people to like, generate all these nice cool art and like, like lots of positive use cases right.
But I think you want to also be conservative to not create a lot of like stuff that you don't want floating around on the internet that's associated with your models.
You know, this is a tough trade off.
I think it's, it's nice that we can still release some safe model that people can use, but making these models like fully safe when they never generate something that is like, kind of like not a, not a good thing.
It's very hard problem in general.
So you need to find more like detailed examples in our paper, if you're looking for like specific examples, but that was kind of our line of thinking on like releasing like the small filter budget.
Okay, maybe a slight tangent but what what does the process look like of let's say calling, you know, the unsafe parts away from from the model, like how do you go about that.
So calls usually I guess training these training kind of like these classifiers to filter out portions of the data set that could be like not safe, if you could, you know, train an NSFW classifier you could train a classifier for like hate symbols you could train a classifier for
other things, then you, once you have labeled data on which you can train these classifiers labeled data for like I don't know real images that you consider things that you don't want to model to generate.
You could run these classifiers on your training data set filter it out, then train a model on the filter data set. So hopefully the model will never go into regions where
there's nothing like that because it was never part of the training data.
Yeah, awesome. Thank you.
Okay, so just a quick like some like look into the notebooks that we've released this. This is just like some useful.
Some parameters that you will have to like, kind of like deal with when you're trying to generate stuff from the notebooks that you released. Well, there's the two scales we've talked about in a talk today the classifier free guidance scale and the clip guidance
scale, the small values for these scales will generate, you know, more diverse, but not very coherent samples larger values will generate coherent things very large values will generate like very artsy looking things.
The classifier free guidance scale, like, I think three might be the default but you could try five 1020 or so on to generate more artistic things.
Similarly for the clip guidance scale.
Time steps kind of controls like how many little steps you take in the diffusion process. I think by default we use 100. There's 100 steps of like iterative denoising that will happen so if you use a higher value to look more sharp, but you'll also spend more
time creating a sample.
So it was a good like trade off that we used in the thing.
Finally, the further in painting notebook, you would have to provide an extra thing which is like, what is the region of a given image that you want to paint so you would have to provide, let's say a 64 by 64 image that you want to impaint and some
that you've like removed that you kind of specify with a mask, which is like, I think one in places where the image is not masked and zero in the places where the image is masked.
I could be wrong on the zero versus one, so you should check the notebook for which direction, basically it's a binary mask that tells this is the portion of the image that is masked, this is the portion of the image that is unmasked.
And the rest is like, just just a usual image with three channels that you provide as extra information to the model.
And do you just upload that as an image file.
I think the way in the notebook that works is, so if this is on a co lab you'll have to have the file on drive and then you open it using pillow image dot open or something. I don't know if there's like a direct upload button.
But I guess like,
Oh, sorry, I guess my question was, like when you add the mask, like the mask is just like removing part of like a regular image file or like there's something more to it.
Oh, yeah, that's just removing parts of the regular file. So like, I think if you want to do it programmatically just just zero out that region.
Does that answer the question or.
That makes sense. Yeah, thank you. Yeah.
There's an example in the notebook and there's a cell in the notebook that kind of masks an image that might be more clear where you can see like, you are loading an image from the disk then you are kind of like removing a region then you are kind of writing down a
mask that specifies what you removed and then you pass in all this information into the model.
So I think that's it for the stuff you will need to kind of like apply this thing to the notebook.
And if you want more further reading or like what we talked about today, I mean I try to focus on mostly like things you will need to understand for like kind of generating art from these things but you want to go into more detail about the
design of these models. I think the best paper would probably be the denoising diffusion probabilistic models paper by Jonathan Ho, the DDPM paper.
That has kind of like the basic theory of the models that we talked about today, and there was our paper on diffusion models began to imagine this is that kind of talks about the guidance trick for classifier guidance, and then the glide paper
to this for text conditional models where we talk about clip guidance and classifier free guidance. There was also the paper by young song generative modeling best made ingredients of the data distribution which kind of like was before the Jonathan
Ho approach this problem from a very different perspective of score matching, and in the DDPM paper Jonathan Ho and others showed how it's kind of equivalent to score matching.
So if you want to understand diffusion models from a different lens, I think I would strongly recommend that paper, and these two blogs as well by Lillian and Yang on diffusion models and score matching models, basically like two sides of the same kind of understanding from both
the super useful to see you know why these generated models work.
And that's it.
Thank you so much it was very, very interesting and fascinating.
I'm very inspiring.
Are there questions.
Go ahead.
Super cool.
Thank you.
Question. Did you notice if there was any relationship between say, like, if you fed it to dimensional noise, and if you were to step through the X or Y coordinates, did you notice if there's any relationship between the coordinates used for two dimensional noise and the output you can
Say that again.
What did you say at the end.
If you use two dimensional noise is there a relationship between the coordinates that you use for the two dimensional noise map and the outputs of the model.
Like is there a relationship that that that you observe between the noise that you use and the model and the outputs of the model.
There's a little bit so like one way you can do this is you can like fix the noise, the sample and change the label.
And you can see that the generated images for the same noise but different labels kind of have similar like perspective and spatial structure.
So like, but but they look kind of like images from different classes.
So this is definitely control some aspect of you know how the final output looks like and there's some kind of spatial like connection, but it's not an exact direct connection.
Does that kind of answer your question.
Yeah, got it.
I love this in our I think diffusion models beat guns paper. I think that in the appendix we have an example where we like do this specific thing.
It's actually more directly connected when you use a different sampling method.
So, I showed you there is reverse process right where at each step you're doing this reverse step with the Gaussian.
At each step you're adding a little bit of noise when you sample from the Gaussian, but there's a different way of reverse sampling from these models which is called ddim is another paper on that, where you just sample noise once at the start.
And then you just run a deterministic reverse process to sample from the model in that case that there's kind of like this one to one correspondence between the noise and the generated image, and then it's more clear to see this.
Thank you.
I can ask my earlier question again, I guess if nobody else is not a question.
So I guess, if you're using a diffusion model without clip. Right. So I guess what's being trained is the classifiers for the labels that am I understanding that correctly.
For the denoising process.
Yeah, I guess I'm trying to understand if without clip. How does it know what to denoise to without like some representation of the text that you're feeding it.
Yeah, yeah, so.
Well, yeah, if you train a model or denoising model without text labels, then it doesn't know where to go and the only way you can generate a sample for a given text distribution it would would be through like clip guidance or something, but we do have.
We train these models to be text conditional diffusion model. And in the classifier free guidance case, you train it with or without the labels. So maybe I can go back to one of our slides here.
The way the model, the reverse process model sees the text is through this kind of conditioning on the, the representations output by a transformer on the text.
Okay, got it.
So this text conditioning here is without clip.
Yes, so.
Can I point a glossed over in guidance was you could use guidance on top of unconditional models or conditional models. So you could have a reverse diffusion model that isn't conditioned on any labels, then it wouldn't have any way of actually like producing an image given a class.
But then you could use guidance on top to get it to produce an image giving a class, but you could also use guidance on top of conditional models themselves.
So you could have your original model be able to produce an image text, like we did here, but also use guidance on top to make it even better at doing this.
Got it. Okay, yeah.
This one.
Okay, thank you.
Okay, excellent. If are there more questions.
Well, I think that we can wrap up the session.
Again want to thank you a lot.
Profile and it was, it was great. Thank you so much.
Thank you. And thank you for having me and feel free to just like email me any questions later or DM me on Twitter with questions.
I think there's a lot of cool stuff out happening in this so like, I would strongly recommend doing some of the like reading some of the blogs are reading just like things that you can find in other collab notebooks as well.
