1
00:00:00,000 --> 00:00:06,760
Hello, everyone. Welcome to your course, Deep Learning for Art, Statistics and Creativity.

2
00:00:06,760 --> 00:00:13,140
Today, we have two special speakers. First, we serve as Dr. Jeff Klun, who is an associate

3
00:00:13,140 --> 00:00:20,440
professor in computer science at the University of British Columbia and also a research team

4
00:00:20,440 --> 00:00:28,640
leader at OpenAI. And he's going to talk about towards creating endlessly creative,

5
00:00:28,640 --> 00:00:35,400
open-ended innovation engines. I think this is a very exciting direction because so far

6
00:00:35,400 --> 00:00:43,000
we have talked about the interaction between art and AI. We said that how AI can help us

7
00:00:43,000 --> 00:00:50,120
to create and express ourselves and democratize the creativity in a sense. But also, the other

8
00:00:50,120 --> 00:01:02,240
direction is how our creativity can help us create better AI. For instance, how we learn

9
00:01:02,240 --> 00:01:11,840
by creating, how we define problems and find solutions for them and generalize to solve bigger

10
00:01:11,840 --> 00:01:20,640
problems and so on and so forth. So today is one of those, I would say, a realization of such a

11
00:01:20,640 --> 00:01:30,480
great idea that you will see as a gist of what Jeff has been working on. So please go ahead.

12
00:01:30,480 --> 00:01:37,840
And also, another question that we often ask in the class is that students are interested to know

13
00:01:37,840 --> 00:01:45,360
a little more about your background because they always feel inspired by seeing great scientists

14
00:01:45,360 --> 00:01:53,040
and what, for instance, got you to working on AI would be very interesting for them if you don't

15
00:01:53,040 --> 00:02:01,480
mind sharing. Great. Thank you for the introduction. Let me share my screen here and make sure that

16
00:02:01,760 --> 00:02:20,320
is working. So are you able to see my screen? Yeah. And the presentation? Yes. Okay. And can you see my

17
00:02:20,320 --> 00:02:28,960
mouse cursor? Yes. Okay. Hello, everyone. My name is Jeff Klune. And I want to talk to you today

18
00:02:28,960 --> 00:02:35,320
about trying to take on like an extremely big research challenge. I think it's a grand challenge

19
00:02:35,320 --> 00:02:43,360
of AI. And that is trying to create what we call open-ended algorithms. I wasn't planning on telling

20
00:02:43,360 --> 00:02:47,320
you a little bit about my background. I guess in brief, I started out on a quest just to understand

21
00:02:47,320 --> 00:02:53,760
two twin questions, which is how did natural evolution produce all the complexity on Earth,

22
00:02:53,760 --> 00:02:59,480
including the human brain? It's astounding. And we don't know how that process happened really. We

23
00:02:59,480 --> 00:03:04,000
don't know how to recreate it. And you'll see a lot of work towards that today. And then also, I'm

24
00:03:04,000 --> 00:03:08,360
interested in trying to figure out how does thinking happen and how can we create it in machines? And

25
00:03:08,360 --> 00:03:13,560
I think in many ways, these questions are very intertwined, as you'll see also today. So I started

26
00:03:13,560 --> 00:03:18,080
out in philosophy, actually, because I thought they had the market cornered on thinking, but really

27
00:03:18,080 --> 00:03:22,520
quickly kind of, or actually not quickly, slowly learned throughout the course of my life that the

28
00:03:22,560 --> 00:03:27,000
best way to tackle these challenges is to try to build these systems and recreate these systems

29
00:03:27,000 --> 00:03:32,000
computationally. Motivated by the wonderful quote by Richard Feynman, which is, that which I cannot

30
00:03:32,000 --> 00:03:37,480
build, I do not understand. So we understand by building. And that has certainly been true in my

31
00:03:37,480 --> 00:03:43,600
life that I understand more and more by being forced to turn speculation into code and into

32
00:03:43,600 --> 00:03:51,400
algorithms. So with that, I'm going to begin. So this talk is really going to be in two parts.

33
00:03:52,360 --> 00:03:56,920
The main part is going to be the first part. And it's about creating open-ended innovation engines.

34
00:03:57,560 --> 00:04:02,200
And if there's time, which I hope there will be, I'm going to rush through a series of work that

35
00:04:02,200 --> 00:04:06,840
we've done that I call AI neuroscience at the end. And then throughout all of this, what you're

36
00:04:06,840 --> 00:04:11,720
going to find is that this is a bit of a meandering intellectual story, because throughout my career,

37
00:04:11,720 --> 00:04:18,040
different research has kind of unintentionally produced different aesthetic artifacts of interest.

38
00:04:18,040 --> 00:04:22,360
And I kind of want to walk through some of the things and touch as many of these places where I

39
00:04:22,360 --> 00:04:26,440
think our work has produced things that are aesthetically interesting, as well as scientifically

40
00:04:26,440 --> 00:04:37,080
interesting. So the first thing I want to motivate is, you know, the idea of open-ended algorithms.

41
00:04:37,080 --> 00:04:42,200
So these are things that endlessly innovate. They just keep going forever. So if you think

42
00:04:42,200 --> 00:04:47,320
about natural evolution, look at the Tree of Life there, and think about all of the marvelous

43
00:04:47,320 --> 00:04:53,560
engineering designs that nature has brought and continues to create in an ever-going fashion,

44
00:04:53,560 --> 00:04:58,920
you know, jaguars, hawks, the human mind, everything that we know on Earth. You know,

45
00:04:58,920 --> 00:05:04,520
in most situations, we cannot rival these things with engineering. And so what's fascinating is

46
00:05:04,520 --> 00:05:09,400
that, you know, a very simple algorithm that Darwinian evolutionary algorithm, plus the context

47
00:05:09,400 --> 00:05:13,880
it's been placed in, continues to innovate for billions and billions of years. And I think it's

48
00:05:13,880 --> 00:05:18,440
really fruitful to think to yourself, you know, could you create an algorithm that you would want

49
00:05:18,440 --> 00:05:23,480
to run for billions and billions of years and come back and check whether or not it's interesting?

50
00:05:23,480 --> 00:05:28,680
Currently, as scientists, we have zero ability to produce things that are interesting even after

51
00:05:28,680 --> 00:05:34,200
a few months of running them on a computer, let alone billions and billions of years. So natural

52
00:05:34,200 --> 00:05:37,560
evolution is what we, you know, one of these open-ended algorithms. And another one is human

53
00:05:37,560 --> 00:05:42,760
culture, which just endlessly innovates and produce innovation on innovation and innovation. That's

54
00:05:42,760 --> 00:05:46,680
both true in science and technology, but it's also true in the arts, where you get, you know,

55
00:05:46,680 --> 00:05:51,400
impressionism after you get the classical paintings, and then you get, you know, post-modernism or

56
00:05:51,400 --> 00:05:57,320
Jackson Pollock or all the different kind of evolution of genres. So, you know, we started

57
00:05:57,320 --> 00:06:03,640
with the idea, when we try and wanted to try to work on this, is that natural evolution and human

58
00:06:03,640 --> 00:06:08,440
culture are what we call innovation engines. And that is that there's kind of this simple recipe

59
00:06:08,440 --> 00:06:15,000
that they follow that allows them to be creative. And that is that they start with a set of things,

60
00:06:15,000 --> 00:06:19,720
it could be an empty set, and then they generate a new thing. And then if that's interesting,

61
00:06:19,720 --> 00:06:23,720
they keep it and add it to the set. And then they take something out of that set, they change it a

62
00:06:23,720 --> 00:06:28,120
little bit, they permute it somehow, and they see if that is interesting. And if that's interesting,

63
00:06:28,120 --> 00:06:32,760
they add that to the set. And you have this growing set of things, these archives of things

64
00:06:32,760 --> 00:06:37,880
you've already produced that are interesting. And then each one of those is a stepping stone to new

65
00:06:37,880 --> 00:06:42,600
potential innovations or solutions. And if you think deeply about it, that's true both of human

66
00:06:42,600 --> 00:06:47,400
culture and natural evolution. And so the question is, with that kind of mental framework, can we

67
00:06:47,400 --> 00:06:54,120
create algorithms that do that process automatically? And so, you know, at the core, there's really kind

68
00:06:54,120 --> 00:06:58,360
of these two simple steps. The first one is you have to have something that generates new things

69
00:06:58,360 --> 00:07:03,480
based on previous things. That's the green box on the left. And then you have to evaluate whether

70
00:07:03,480 --> 00:07:07,400
not those things are interesting. And if they are, then you add them to the set and you just keep

71
00:07:07,400 --> 00:07:13,480
repeating this process. So in the long run, what we'd love to do is take, you know, humans out of

72
00:07:13,480 --> 00:07:18,200
the loop if possible, label data out of the loop, and you just have some sort of generator like a

73
00:07:18,200 --> 00:07:24,040
neural network that can generate new things like poems or codes or mathematical proofs or images,

74
00:07:24,760 --> 00:07:29,160
or, you know, technological artifacts, something, maybe another deep neural net that is trained

75
00:07:29,160 --> 00:07:34,360
to recognize what's interesting somehow, and then that process could just iterate. Now, for example,

76
00:07:34,440 --> 00:07:39,480
you could imagine that you take like the orange box here is an autoencoder. And it looks at everything

77
00:07:39,480 --> 00:07:43,560
that it's seen before, it compresses them down to a low dimensional bottleneck space, and then it has

78
00:07:43,560 --> 00:07:47,880
to uncompress them. And then if you get some new latent vector that's new that you've never seen

79
00:07:47,880 --> 00:07:51,800
before, you call that interesting. And that might be one thing that could kick off this problem.

80
00:07:52,360 --> 00:07:56,840
And if you could do that, you would have an innovations arms race in any domain, you could

81
00:07:56,840 --> 00:08:01,000
unleash this thing anywhere. And that would be amazing. And a lot of these ideas date back to

82
00:08:01,000 --> 00:08:07,320
Schmidt-Huber ideas from the early 90s. However, the problem is that when you do that, you typically

83
00:08:07,320 --> 00:08:12,040
do get new things forever, but you don't get new interesting things forever. You, for example,

84
00:08:12,040 --> 00:08:15,880
might get white noise, just an endless stream of different patterns of white noise, because those

85
00:08:15,880 --> 00:08:20,760
are uncompressible. So really, at its core, the biggest challenge in this field is kind of how

86
00:08:20,760 --> 00:08:25,800
do you avoid generating uninteresting novelty, and how do you only generate interesting novelty?

87
00:08:26,600 --> 00:08:31,400
And here's one example by a friend of mine, Josh Auerbach, who tried to basically take the same

88
00:08:31,400 --> 00:08:35,160
encoding that I'm going to tell you about later, and a similar system is trying to automatically

89
00:08:35,160 --> 00:08:40,280
generate images and try to produce new interesting images forever. And these images are interesting,

90
00:08:40,280 --> 00:08:44,120
they're pretty cool, but they're not nearly as interesting as they could be, right? They're not

91
00:08:44,120 --> 00:08:49,000
like what artists would do over the course of centuries. You would expect and hope that things

92
00:08:49,000 --> 00:08:52,440
would ultimately break out of these kind of abstract patterns. And that's because these

93
00:08:52,440 --> 00:08:59,240
things are optimized to produce information theoretic metrics like compression or mutual

94
00:08:59,240 --> 00:09:05,720
information and things like that. So what we thought in this work is that one insight you could

95
00:09:05,720 --> 00:09:11,160
have is that recognizing a new type of thing is like being able to recognize a new class of thing.

96
00:09:11,160 --> 00:09:15,960
If you've never seen a palm tree before, that's a distinct kind of trees. And if you've never

97
00:09:15,960 --> 00:09:21,480
seen a tree before, trees are distinct from dogs and roses and statues. And so

98
00:09:22,200 --> 00:09:28,440
one way to think about being able to recognize an infinite number of new classes is to approximate

99
00:09:28,440 --> 00:09:34,760
that by having a neural net just recognize a very large number of classes. And so if you could

100
00:09:34,760 --> 00:09:40,200
recognize, you know, a million classes, for example, then as the generator produces new

101
00:09:40,200 --> 00:09:44,280
instances of those classes, maybe the process could like start going out and generating each of

102
00:09:44,280 --> 00:09:49,640
these classes. And that allows us to then use supervised learning because we know how to recognize

103
00:09:49,640 --> 00:09:56,440
new classes of things. So this is an approximation to the overall goal and to try to see if this

104
00:09:56,440 --> 00:10:00,440
system can work. So the way that we wanted to approximate this, and this is all the way back

105
00:10:00,440 --> 00:10:05,800
in 2015 before image generation really worked that well, is we said, let's take a deep neural net

106
00:10:05,800 --> 00:10:11,960
that is trained on ImageNet, which is relatively new around that time. It has 1000 different classes

107
00:10:11,960 --> 00:10:15,320
and it's really good at recognizing these different classes. And then we'll have, we'll use that as

108
00:10:15,320 --> 00:10:21,080
our evaluator, which is the generator's job is to generate instances of that class. And then the

109
00:10:21,080 --> 00:10:25,800
question is, what are we going to use for this, the green box here, the generator side. So what we

110
00:10:25,800 --> 00:10:31,800
need is an algorithm that can recognize either an improvement on a current class, or when a new

111
00:10:31,800 --> 00:10:37,400
class is generated. And so we decided to use this algorithm that I'm excited to tell you about,

112
00:10:37,400 --> 00:10:40,920
because it has a lot of really interesting motivations behind it. It's called map elites.

113
00:10:41,880 --> 00:10:47,240
And it has one bin per ImageNet class. And I'll tell you what map elites is right now.

114
00:10:47,240 --> 00:10:51,800
But to tell you about map elites, I kind of want to motivate this whole field of a new kind of

115
00:10:51,800 --> 00:10:56,200
type of algorithm that my colleagues and I have been working on. And it starts with this recognition,

116
00:10:56,200 --> 00:11:00,840
which is that there's a paradox in life, which is that if you try too hard to solve a problem,

117
00:11:00,840 --> 00:11:06,920
you'll fail. However, if you ignore the objective, then you're much more likely to succeed. So imagine

118
00:11:07,000 --> 00:11:12,040
that you're in this maze here, and you're starting here, and your job is to get here. And you might

119
00:11:12,040 --> 00:11:17,160
say, well, okay, make the robot who's here, make its objective, getting as close as possible to

120
00:11:17,160 --> 00:11:21,080
the goal. Well, if you do that, and you get points here, these are all the points that get generated

121
00:11:21,080 --> 00:11:25,560
by that search algorithm, because and most of them just go straight north, because that lowers the

122
00:11:25,560 --> 00:11:30,760
distance to the goal. But then they just butt their head against that wall forever. This is a

123
00:11:30,760 --> 00:11:35,880
classic local optimus, you're familiar with these things in search. However, if you simply switch

124
00:11:35,880 --> 00:11:40,840
away from the paradigm of always try to optimize toward a goal, and you just say, let's just go

125
00:11:40,840 --> 00:11:46,200
to new places, just seek novelty. That's what you get here. And eventually this search stops

126
00:11:46,200 --> 00:11:50,760
focusing on just going north. It doesn't actually care more about north than going east. And eventually

127
00:11:50,760 --> 00:11:56,200
it winds its way around, and it solves the problem. And this right here is a metaphor for every single

128
00:11:56,200 --> 00:12:02,040
hard thing we want to do in search. If there are local optimal in space, if we need to explore

129
00:12:02,040 --> 00:12:06,760
to discover this thing, then we probably should seek novelty more than an objective. And it's

130
00:12:06,760 --> 00:12:12,520
even a metaphor for things beyond algorithmic search. It's also a metaphor for human culture

131
00:12:12,520 --> 00:12:18,360
and even natural evolution. And the idea is that almost every major scientific breakthrough,

132
00:12:19,400 --> 00:12:23,880
if you trace its lineage back, it's not a straight path to that solution. Instead,

133
00:12:23,880 --> 00:12:29,000
it's a winding, circuitous route. So for example, if you went back in time centuries and you said,

134
00:12:29,000 --> 00:12:34,680
I have this way of cooking food, and what I want is a faster way to cook food that doesn't produce

135
00:12:34,680 --> 00:12:41,800
any smoke, then you would never, if you only funded work into improved cooking technology that can

136
00:12:41,800 --> 00:12:46,760
accomplish those goals of heating things faster, you would never invent the microwave, which is a

137
00:12:46,760 --> 00:12:51,800
magical invention. Because to invent the microwave, you had to have been working on radar technology

138
00:12:51,800 --> 00:12:56,840
and recognize the chocolate bar melted in your pocket. Similarly, if you went back millennia

139
00:12:56,840 --> 00:13:02,040
to this abacus and you said, that thing does computation, I want more computation. And you

140
00:13:02,040 --> 00:13:07,080
only funded researchers who improved against the objective of producing more computation,

141
00:13:07,080 --> 00:13:12,920
you might get abacuses with like longer rods, more beads, something like that. But you would never

142
00:13:12,920 --> 00:13:17,880
invent the modern computer because to do that, you had to work on things like electricity and

143
00:13:17,880 --> 00:13:23,480
vacuum tubes, which were decidedly not produced because they improved computation, although

144
00:13:23,480 --> 00:13:28,760
they later proved instrumental to doing that. The same is true for going from this kind of energy

145
00:13:28,760 --> 00:13:33,000
to clean energy, where you have to be thinking about things like space and time that were not

146
00:13:33,000 --> 00:13:39,080
thought about because they would produce new ways of producing clean energy. So the conjecture here

147
00:13:39,080 --> 00:13:43,480
is that the only way to solve really hard problems may be to create problems while you solve them

148
00:13:43,480 --> 00:13:49,080
and goals switch between them. And so goal switching is this idea that if you're trying to solve one

149
00:13:49,080 --> 00:13:55,320
task, and you make progress on a different task, then you should also start optimizing and getting

150
00:13:55,320 --> 00:13:59,960
better on that different task. So if this robot here, this scientist here wants to make a walking

151
00:13:59,960 --> 00:14:04,600
robot, and all of a sudden during optimization, the robot starts crawling or starts balancing on

152
00:14:04,600 --> 00:14:10,840
one leg, you shouldn't throw that out as a failure because it's not helping you walk or making forward

153
00:14:10,840 --> 00:14:15,160
progress. Instead, you should start getting better at those skills to add those to the set of things

154
00:14:15,160 --> 00:14:20,120
that you work on. And ultimately, those might be stepping stones to get you to this walking robot.

155
00:14:21,080 --> 00:14:28,360
So my colleagues and I have been creating this new subfield of algorithms of AI

156
00:14:28,360 --> 00:14:32,760
called quality diversity algorithms. And this family of algorithms is trying not just to get

157
00:14:32,760 --> 00:14:36,520
the single best solution to a problem. It's trying to do something very different. It's

158
00:14:36,520 --> 00:14:43,160
trying to get a large set of diverse solutions, but where every solution is as good as possible

159
00:14:43,160 --> 00:14:49,160
for that type of solution. You want the tallest in the giraffe or the fastest ant,

160
00:14:49,160 --> 00:14:54,440
but you don't let an ant who's not that fast kind of get precluded by the fact that a cheetah is

161
00:14:54,440 --> 00:15:01,800
faster. You still want the fastest ant and the best ant you can find. So probably the most popular

162
00:15:01,800 --> 00:15:06,680
algorithm in this family at this point is this algorithm called map elites, which was invented

163
00:15:06,680 --> 00:15:12,200
by Jean-Baptiste Morel, a great colleague and friend of mine, as well as myself in 2015. And

164
00:15:12,200 --> 00:15:15,720
it's very, very simple. And the idea here is if you're going to solve a problem,

165
00:15:15,720 --> 00:15:22,120
want to first choose or learn, but we started off by choosing dimensions of interest that you find

166
00:15:22,120 --> 00:15:26,920
that you yourself like. So imagine if you're trying to make a car, for example, you might choose

167
00:15:26,920 --> 00:15:32,200
safety and fuel efficiency as two dimensions of interest. And then you just discretize these

168
00:15:33,320 --> 00:15:37,400
dimensions. And you look for the best solution, according to some criteria, like maybe it's the

169
00:15:37,400 --> 00:15:43,400
fastest car at each point in this grid. And what you want at the end of the day is not just to get

170
00:15:43,400 --> 00:15:49,640
the fastest car possible, but the fastest car for every possible tradeoff between safety and fuel

171
00:15:49,640 --> 00:15:56,120
efficiency. So here's an example problem we tried this on. This is generating soft robot morphologies,

172
00:15:56,120 --> 00:16:01,640
which is like the bodies of robots. So we gave this optimization algorithm those four materials

173
00:16:01,640 --> 00:16:06,200
there. They're kind of voxels that can pulse at different times. And some are soft and some are

174
00:16:06,200 --> 00:16:13,080
hard. And we said, you know, go fast. And, you know, first we did this without map elites,

175
00:16:13,080 --> 00:16:17,080
we just did this with a canonical optimization algorithm or a genetic algorithm in this case,

176
00:16:17,080 --> 00:16:22,280
which is just trying to optimize for speed. And what you see here is this kind of really

177
00:16:22,280 --> 00:16:26,920
interesting parade, this Noah's Ark of very different solutions and very different creatures.

178
00:16:27,800 --> 00:16:32,120
And, you know, people got really excited when we put this online and it's super fun.

179
00:16:32,120 --> 00:16:35,400
But I think one of the things that people thought really interesting about this work,

180
00:16:35,400 --> 00:16:41,000
including myself is the huge diversity of designs that you see here. You know, it starts to evoke

181
00:16:41,000 --> 00:16:45,960
nature where you see a lot of different designs. The problem is there is a trick to this. And that

182
00:16:45,960 --> 00:16:51,640
is that all of the designs that you just saw, each of those came from a different run of optimization.

183
00:16:51,640 --> 00:16:57,160
The only way you got a diversity was by starting the run again and doing a massive search to find

184
00:16:57,160 --> 00:17:01,880
one solution. But if you look within that population of creatures, they're all almost identical.

185
00:17:01,880 --> 00:17:05,880
And that's not what we want. What we want on is an algorithm that will generate a huge diversity

186
00:17:05,880 --> 00:17:09,800
of things within one run so that you can run it for billions of years and it would continue to

187
00:17:09,800 --> 00:17:14,520
produce interesting new stuff as opposed to converging to one type of solution and getting

188
00:17:14,520 --> 00:17:19,240
stuck on that kind of local optimal. So we took the map elites algorithm that I just described

189
00:17:19,240 --> 00:17:24,760
to you and we applied it to the same software last problem. And what we did there, you know,

190
00:17:24,760 --> 00:17:29,320
is we have to pick the dimensions and we chose to pick the number of voxels and then amount

191
00:17:29,400 --> 00:17:34,440
of this dark blue material because previously it hadn't been using this kind of bone-like material

192
00:17:34,440 --> 00:17:39,960
and we wanted to see it play with that resource more. And if you look at classic optimization,

193
00:17:39,960 --> 00:17:44,280
this could have been RL, but in this case it's a genetic algorithm. Any optimization,

194
00:17:44,280 --> 00:17:48,920
what you find is that it doesn't actually search the space very well. And so it has low performing

195
00:17:48,920 --> 00:17:53,720
points and it didn't do a lot of exploration. If you add diversity, which we know historically

196
00:17:53,720 --> 00:17:58,360
helps, you do get higher performing points. So you see these yellow points here, but it still

197
00:17:58,360 --> 00:18:02,520
did not explore a lot of the space, even though it's incentivized to literally explore in these

198
00:18:02,520 --> 00:18:08,440
two dimensions. Map elites is a qualitatively different algorithm. It's a sea change in terms

199
00:18:08,440 --> 00:18:12,840
of what happens within the algorithm. If you look here, you see this rich exploration where it

200
00:18:12,840 --> 00:18:18,040
fanned out and searched the entire search space and it taught you more about this search space.

201
00:18:18,040 --> 00:18:22,440
It tells you, hey, there's not very high performing points up here. There's a little bunch of optima

202
00:18:22,440 --> 00:18:26,280
over here. There's also this separate little area here that you probably would never have normally

203
00:18:26,280 --> 00:18:30,440
found, et cetera, et cetera. I'm doing these interesting points over here that you can go

204
00:18:30,440 --> 00:18:36,680
investigate. And what's interesting is it often finds a better overall high performing solution

205
00:18:36,680 --> 00:18:41,400
than if you just do direct optimization because it's doing such a better job of exploring the

206
00:18:41,400 --> 00:18:48,280
space of possibilities. So if you look at any individual final point, you can trace back its

207
00:18:48,280 --> 00:18:54,280
lineage through time to see where those solutions visited in the search space. And what you can

208
00:18:54,280 --> 00:18:59,320
see here is that they don't just kind of mine one area of the space and get better and better and

209
00:18:59,320 --> 00:19:03,400
better at that corner of the search space, that particular tradeoff between these two dimensions.

210
00:19:03,400 --> 00:19:09,240
But instead, the overall lineage takes these long, circuitous paths to their final destination.

211
00:19:09,240 --> 00:19:13,800
Just as to get a human, you had to go through an intermediate stage of being a tapeworm and then

212
00:19:13,800 --> 00:19:18,040
being like a tree dwelling. Actually, I don't know if we were a tree doubling, but kind of

213
00:19:18,040 --> 00:19:22,840
something that looked more like an ape and all sorts of intermediate steps along the way.

214
00:19:23,640 --> 00:19:29,320
So going back to the idea of an innovation engine, we now can recognize the algorithm that we're

215
00:19:29,320 --> 00:19:33,560
going to use here. There's one final thing I need to tell you about, which is how are we going to

216
00:19:33,560 --> 00:19:38,040
encode the images we're going to search for. And I'm going to tell you what I mean by the word

217
00:19:38,040 --> 00:19:41,880
encoding, because I think especially for people who are interested in aesthetics, this is one of

218
00:19:41,880 --> 00:19:47,160
the most important choices you can make. And you'll see this show up in Joel's work later as well.

219
00:19:47,160 --> 00:19:50,920
So I'm going to tell you about the encoding that we use, which is a CPPN. So first,

220
00:19:50,920 --> 00:19:54,600
I've been throwing around these terms, genetic algorithm and evolutionary algorithms. You may

221
00:19:54,600 --> 00:20:00,280
not know what they are. I'm going to very briefly explain them. If you want to search for a problem,

222
00:20:00,280 --> 00:20:03,880
this is also true in deep learning. The first choice you have to make is how to encode the

223
00:20:03,880 --> 00:20:08,280
problem. So imagine if you wanted to search for tables. Well, you could decide I'm going to store

224
00:20:08,280 --> 00:20:13,640
the length of each leg separately as a number on a parameter vector. We in evolutionary algorithms,

225
00:20:13,640 --> 00:20:17,720
we call this a genome, but in deep machine learning, it's often called a parameter vector.

226
00:20:17,720 --> 00:20:21,480
So you store the length of each leg separately and the width and the length of the surface of

227
00:20:21,480 --> 00:20:26,760
the table maybe on this string of numbers, this parameter vector. Once you've made that encoding

228
00:20:26,760 --> 00:20:31,800
choice, you then can score the population. First, you create a population at random by generating

229
00:20:31,800 --> 00:20:35,960
random strings of numbers. You score this population to see how good they are. You select

230
00:20:35,960 --> 00:20:40,680
which ones are better according to some scoring function, which could be your reward function.

231
00:20:40,680 --> 00:20:46,040
And then you just take these things here, take their parameter vectors, and you perturb them

232
00:20:46,040 --> 00:20:51,400
in a little way somehow. And then you get a new thing and then you repeat the process.

233
00:20:51,400 --> 00:20:54,200
In the gradient-based method, this is kind of like where you take the learning

234
00:20:54,200 --> 00:20:59,000
step based on the gradient of the scoring function. And then you repeat the problem.

235
00:20:59,000 --> 00:21:04,040
So when I talk about an encoding, it's this first choice, which is how do we decide what is the search

236
00:21:04,040 --> 00:21:09,720
space that we will search in the parameter vector and how does that map to the final solution?

237
00:21:09,800 --> 00:21:16,200
And that is in evolutionary language, the process of going from a genotype to a phenotype,

238
00:21:16,840 --> 00:21:20,760
or machine learning a parameter vector to a final agent or policy or artifact.

239
00:21:21,640 --> 00:21:27,480
So there is this notion of a direct encoding versus a generative encoding. And a direct encoding,

240
00:21:27,480 --> 00:21:32,520
you basically have one number on your parameter vector for every single thing in your final artifact.

241
00:21:32,520 --> 00:21:36,200
So if you're searching for the weights of a neural net, then you search separately for a

242
00:21:36,200 --> 00:21:39,960
number for each weight or for a table you search separately for the length of each leg.

243
00:21:40,760 --> 00:21:44,920
If you think about how perturbations affect these parameter vectors, though,

244
00:21:44,920 --> 00:21:51,480
they are mostly likely to produce non-regular phenotypes. So most changes are not going to

245
00:21:51,480 --> 00:21:56,920
lead to a table that has to be flat and hold your coffee. And so that makes kind of a local

246
00:21:56,920 --> 00:22:00,760
optimum between this solution and this solution. You have to go through this intermediate thing

247
00:22:00,760 --> 00:22:06,760
unless you get lucky enough to generate a regular phenotype. If you have a generative encoding,

248
00:22:06,760 --> 00:22:12,600
you reuse information in the parameter vector to produce the final thing. So you might just

249
00:22:12,600 --> 00:22:18,040
specify the length of legs once and then reuse that for these four lengths of tables. And now

250
00:22:18,040 --> 00:22:24,520
every single change to that parameter vector is going to produce a regular flat table. However,

251
00:22:24,520 --> 00:22:29,880
you've lost something. You've lost the ability to express this type of table up here. And so this

252
00:22:29,880 --> 00:22:35,640
is like a really, really essential choice when you go to produce any solution with search.

253
00:22:35,640 --> 00:22:39,960
So generative encodings, you know, my colleagues and I and many others have been focusing for a

254
00:22:39,960 --> 00:22:44,840
long time on why these types of encodings are really interesting. And some of the desirable

255
00:22:44,840 --> 00:22:49,160
properties that we want is that you can get regularity, which means you can get patterns in

256
00:22:49,160 --> 00:22:54,760
the final artifact. It might be the architecture of a neural net, or here is the hands on your,

257
00:22:54,760 --> 00:22:59,640
you know, in your body. And what you see is there's a repeating theme in your hands. That's the

258
00:22:59,640 --> 00:23:05,000
regular pattern. But it also has variation. Each of your fingers is a variation on a concept or a

259
00:23:05,000 --> 00:23:09,400
theme. And that's kind of one thing that you might want while you search. There are some others

260
00:23:09,400 --> 00:23:15,080
benefits here, but I'm not going to get into those. So this is something that I just think is really

261
00:23:15,080 --> 00:23:19,480
fascinating to think about, especially if you're interested in aesthetics. And it also ends up

262
00:23:19,480 --> 00:23:23,000
being helpful algorithmically. And it's going to factor into a lot of Joel's work, I assume,

263
00:23:23,000 --> 00:23:28,360
depending on what he talks about. And this is this question of how does nature build the

264
00:23:28,360 --> 00:23:34,600
astronomically elegant, complex creatures that you see in the natural world? Like a question

265
00:23:34,600 --> 00:23:37,640
that I'm not sure if you've ever stopped and thought about, but it's a fascinating one to think

266
00:23:37,640 --> 00:23:44,360
about is how does every cell in your body know what kind of cell to become? You have, you know,

267
00:23:44,360 --> 00:23:51,080
the same software is being run in every one of your cells, the same DNA, yet some of your cells

268
00:23:51,080 --> 00:23:56,280
turn into hair cells or spleen cells or liver cells or eye cells. How does it do that? How does

269
00:23:56,280 --> 00:24:01,640
every cell know what kind of cell to become? Well, it turns out that nature is using a generative

270
00:24:01,640 --> 00:24:07,080
encoding where it reuses information, where the cell fate, which is the type of cell, is a function

271
00:24:07,080 --> 00:24:12,520
of its geometric location in the body. It's almost as if the body wanted to know the XYZ

272
00:24:12,520 --> 00:24:17,720
GPS coordinates of each cell so that it could tell you, oh, if you're like up here, left of the

273
00:24:17,720 --> 00:24:22,200
midline, three quarters of the way up the y-axis, then become a heart cell, for example.

274
00:24:23,320 --> 00:24:28,200
So if you look through developmental biology textbooks, what you find is that these kinds

275
00:24:28,200 --> 00:24:33,640
of geometric patterns are the lingua franca of developmental biology. So here's this beautiful

276
00:24:33,640 --> 00:24:38,920
cartoon by Sean Carroll. So here's your DNA which has these genes on it. And in this developing

277
00:24:38,920 --> 00:24:44,040
embryo are currently three different chemical patterns. They're called morphogens. They're

278
00:24:44,040 --> 00:24:50,200
literally some protein that's sitting diffused inside this embryo. And if this gene here says

279
00:24:50,200 --> 00:24:56,840
that protein A is present and B and C are not present, then this gene expresses and produces

280
00:24:56,840 --> 00:25:02,280
a new protein, only where that's true. And so now you've combined these three pre-existing

281
00:25:02,280 --> 00:25:07,560
patterns to produce this fourth new pattern. And this might therefore tell the vertebra and a spine

282
00:25:07,560 --> 00:25:13,080
that they should turn into vertebra cells. You get this repeating theme down the middle, but only

283
00:25:13,480 --> 00:25:17,640
the left half of the embryo. And if you look through that, go ahead.

284
00:25:17,640 --> 00:25:22,040
Would I be able to interject real quick? Sure. My research is actually focusing on exactly this

285
00:25:22,040 --> 00:25:26,680
same kind of problem, but in mammals. And so in mammals, the morphogen model explains some stuff,

286
00:25:26,680 --> 00:25:32,200
but it's actually even more complex. It is much more complex. Everything in nature is much more

287
00:25:32,200 --> 00:25:39,240
complex than we know. So I am simplifying here because I'm flying through this material. And

288
00:25:39,320 --> 00:25:44,200
not all of the, not, it's not to say that the only thing that's happening is geometric patterning,

289
00:25:44,200 --> 00:25:51,960
but it is, basically, I think it's the backbone of the way this stuff gets built. And so by capturing

290
00:25:51,960 --> 00:25:57,560
that power and putting it into our search processes, we've gone a long way towards the power of

291
00:25:57,560 --> 00:26:03,320
developmental biology. And you could argue that you've skipped out on a lot of the extra complexity

292
00:26:03,320 --> 00:26:07,560
that would be very computationally difficult to simulate by doing these things efficiently.

293
00:26:09,320 --> 00:26:10,520
Yeah. All right. That's a good point.

294
00:26:13,240 --> 00:26:20,200
Cool. Thank you for the question. So getting to the issue I was just talking about, which is how

295
00:26:20,200 --> 00:26:26,840
can we efficiently make this sort of a process happen? So what we don't want to do computationally

296
00:26:26,840 --> 00:26:31,800
is have, like, diffusing chemicals in some chemical simulator, because that would be

297
00:26:31,800 --> 00:26:37,080
really, really expensive. And so Ken Stanley, my longtime friend and colleague figured out,

298
00:26:37,080 --> 00:26:43,080
is that you can actually abstract a lot of the power of this system without any of the underlying

299
00:26:43,960 --> 00:26:49,800
chemistry and in physics in these things that are called CPPNs or compositional pattern producing

300
00:26:49,800 --> 00:26:55,240
networks. And the idea is, is just like in nature, we're going to encode phenotypic elements as a

301
00:26:55,240 --> 00:27:01,720
function of their geometric location. So here's how it works. You take a thing that you want to

302
00:27:02,840 --> 00:27:06,920
optimize. This could be a neural network, it could be a robot morphology, it could be a

303
00:27:06,920 --> 00:27:13,640
picture. And you provide coordinates for everything in the artifact. So imagine it's easiest to think

304
00:27:13,640 --> 00:27:18,440
about pictures. So imagine you give every pixel an x, y coordinate, then you literally pass the

305
00:27:18,440 --> 00:27:24,440
number, then those numbers into this function. So first you put in one, one for this pixel,

306
00:27:24,440 --> 00:27:30,520
and then one, two, and then one, three. And you ask the genome as a function of those two numbers

307
00:27:30,520 --> 00:27:35,640
to spit out the value at that location. And if this is a random function,

308
00:27:35,640 --> 00:27:41,720
then you're going to get a random picture. But if this function here has mathematical functions

309
00:27:41,720 --> 00:27:46,840
that, you know, have regularities in them, then you're going to get a regular artifact.

310
00:27:46,840 --> 00:27:53,560
So for example, if you want left-right symmetry, you can pass the x-axis through a Gaussian here,

311
00:27:53,560 --> 00:27:57,400
and then everything downstream of that Gaussian node will have left-right symmetry.

312
00:27:58,120 --> 00:28:03,640
Similarly, you could have in the y-axis, if you wanted a repeating theme like segmentation,

313
00:28:03,640 --> 00:28:08,680
you could pass the y through a sine function, and then everything downstream of that node will be

314
00:28:09,400 --> 00:28:14,520
regular in that way. You can also add in linear things. You could say, I want to follow the sine,

315
00:28:14,520 --> 00:28:19,800
but only add in a linear component, so like shift it or warp it or bend it in certain ways. So you

316
00:28:19,800 --> 00:28:25,560
can mix and match asymmetric and symmetric and repeating themes to produce arbitrary complexity

317
00:28:25,560 --> 00:28:32,040
using these geometric functions. And kind of what was really amazing at the time,

318
00:28:32,040 --> 00:28:36,600
because image generation wasn't working very well, was the kind of images that would pop out of

319
00:28:36,600 --> 00:28:41,160
these systems. So all of these images here were produced on a website called Pickbrier, where

320
00:28:41,160 --> 00:28:46,680
humans manually choose which ones they find interesting, but the underlying encoding is a CPPN.

321
00:28:47,800 --> 00:28:51,080
And Jill's going to tell you a lot more about like a modern version of this website.

322
00:28:51,640 --> 00:28:57,720
So these images here are all encoded with CPPNs, and what you can see is very, very natural like

323
00:28:57,800 --> 00:29:03,560
shapes, like things like left-right symmetry, repeating motifs, and the lineages as you kind of

324
00:29:03,560 --> 00:29:08,200
permute and mutate these things. You go from a butterfly to a bat with these kind of beautiful

325
00:29:08,200 --> 00:29:15,240
gradations and interpolations that are nice to see. Myself and my postdoc advisor, I took the

326
00:29:15,240 --> 00:29:18,920
same exact idea and we just put it in three dimensions, and what you get are these nice

327
00:29:18,920 --> 00:29:24,360
three-dimensional shapes, which also show a lot of these regularities. And then we went off and we

328
00:29:24,360 --> 00:29:29,560
built this website called endlessforms.com, where you can go on, it's basically Pickbrier but in 3D.

329
00:29:29,560 --> 00:29:35,160
You can take an individual shape and you can say, I want to further evolve or optimize that shape.

330
00:29:35,720 --> 00:29:40,680
Let's see if this plays. Here, for example, you might take this lamp and you are presented with

331
00:29:40,680 --> 00:29:45,160
a bunch of variants on the lamp, and then you pick the one that you like and you see the next

332
00:29:45,160 --> 00:29:50,760
generation and you can kind of crawl through three-dimensional lamp space. And importantly,

333
00:29:50,760 --> 00:29:54,600
if you find one that you like, then you can publish it to the website and other people can

334
00:29:54,600 --> 00:29:59,720
pick it up and branch off of that. This is how you get that growing archive of stepping stones

335
00:30:00,520 --> 00:30:03,800
that allows us to produce kind of an interesting exploration of the space.

336
00:30:05,480 --> 00:30:10,200
Here are some of the other designs that popped out of this system, and here's kind of repeating

337
00:30:10,200 --> 00:30:16,680
segmentation, left-right symmetry, radial symmetry, and mostly a lot of the things just look really

338
00:30:16,680 --> 00:30:22,440
natural and interesting. So this is kind of a fun aesthetic space to be playing in using these CPPNs.

339
00:30:25,320 --> 00:30:30,200
Because we could, we 3D printed the objects and allowed users on the website to 3D print them,

340
00:30:30,200 --> 00:30:34,600
so it's kind of fun to hold these things in your hand, and you can therefore help people who have

341
00:30:34,600 --> 00:30:40,360
no knowledge of CAD and design to produce arbitrarily complex images and then 3D print them

342
00:30:40,360 --> 00:30:43,960
for whatever they want, like a chessboard or something. So when we put this out there,

343
00:30:43,960 --> 00:30:49,240
people really found this interesting, which I think just goes to the to the fact that if you can

344
00:30:49,240 --> 00:30:54,760
automate the design, if you can help people produce really interesting things that they're curious

345
00:30:54,760 --> 00:30:59,480
about and they find exciting, but eliminate all the technical barriers to doing so, then people

346
00:30:59,480 --> 00:31:05,480
get really excited about those tools, and Joel's website as a, you know, GAN breeder is a testament

347
00:31:05,480 --> 00:31:10,280
to that as well. So going back to the overall scientific question here, which is can we use

348
00:31:10,280 --> 00:31:15,400
this to create an open-ended algorithm? Now you know all the pieces of the puzzles. So we're

349
00:31:15,400 --> 00:31:20,120
going to have AlexNet, which is an early image net network that was quite good at the time,

350
00:31:20,120 --> 00:31:23,240
be able to recognize a thousand different classes, and then we're going to have an optimization

351
00:31:23,240 --> 00:31:27,400
algorithm that's going to generate these little tiny CPPN networks that are trying to produce

352
00:31:27,400 --> 00:31:33,000
images that light, that the DNN, the deep neural net, thinks represent, you know, are classified

353
00:31:33,000 --> 00:31:38,920
as each one of the thousand bins in image net. So the idea hopefully is that you'll get goal

354
00:31:38,920 --> 00:31:44,040
switching. So if one of the networks is the best dog we've ever seen, or particular dog,

355
00:31:44,040 --> 00:31:48,760
and then a permutation on that produces the best fish we've ever seen, then now that network can

356
00:31:48,760 --> 00:31:53,880
go to hop over to that bin and start optimizing to become a better fish. And maybe that produces a

357
00:31:53,880 --> 00:31:59,400
better stepping stone for a cat and then a bird, etc. And the hypothesis that we wanted to test

358
00:31:59,400 --> 00:32:06,600
is, is that better than separately optimizing for each one of the bins in image net? So here is

359
00:32:06,600 --> 00:32:12,360
the performance over time. Time here, training goes from bottom to top, and the category of

360
00:32:12,360 --> 00:32:17,320
thousand image net classes are along the x-axis. What you can see is that over time performance

361
00:32:17,320 --> 00:32:22,280
rises with training all the way up to one, you know, red in most places, which means that the

362
00:32:22,280 --> 00:32:27,240
deep neural net is certain that this thing is a lion, and this is a starfish, and this is a guitar.

363
00:32:27,880 --> 00:32:33,240
So my question to you is, knowing that the deep neural net thinks that each one of these things

364
00:32:33,320 --> 00:32:38,520
is in that category, you know, what do you think they look like? And if you had asked this question

365
00:32:38,520 --> 00:32:44,680
in 2015, 2016, people would have said they look like electric, you know, starfish and guitars,

366
00:32:44,680 --> 00:32:50,360
but you probably now, because you guys are, we've had the benefit of a few years, you probably are

367
00:32:50,360 --> 00:32:55,160
used to the idea that what you do, what you get is not that, but you get these things that are called

368
00:32:55,160 --> 00:33:00,840
fooling images or adversarial images, which is to say that the deep neural net is absolutely

369
00:33:00,840 --> 00:33:06,280
certain that this is a starfish, and this is a peacock, and this is a king penguin, and this is

370
00:33:06,280 --> 00:33:11,560
an electric guitar, even though they obviously are not those things. So at the time, this was a,

371
00:33:11,560 --> 00:33:16,680
this, we published this paper, deep neural nets are easily fooled, and it was a really big wake-up

372
00:33:16,680 --> 00:33:21,960
call to the community that AI sees the world differently. There are huge security concerns

373
00:33:21,960 --> 00:33:28,040
here, and this generated a tremendous amount of discussion and awareness amongst the scientific

374
00:33:28,040 --> 00:33:31,720
community, the machinery community, and also the broader public about the fact that these new

375
00:33:31,720 --> 00:33:36,040
tools that we're building have a lot of deep flaws within them that we need to worry about.

376
00:33:36,680 --> 00:33:42,120
Nowadays, everyone's very familiar with adversarial images. At the time, this was not very well known,

377
00:33:43,480 --> 00:33:49,080
and so I thought that was interesting. However, I also think from an aesthetic perspective,

378
00:33:49,080 --> 00:33:52,680
it's interesting that we were trying to generate innovation engines and generate images. We weren't

379
00:33:52,680 --> 00:33:56,600
trying to study neural nets and whether they had flaws, and then this just kind of popped out,

380
00:33:56,600 --> 00:34:01,960
so I thought that was an interesting story. But while some of the images didn't look anything

381
00:34:01,960 --> 00:34:06,280
like the categories of interest, another thing that we found interesting is that many of them

382
00:34:06,280 --> 00:34:10,280
did, and from an aesthetic perspective, this is pretty cool because now you're getting an automated

383
00:34:10,280 --> 00:34:16,440
art generator. So for example, matchstick, television, and bagel, they pretty much do look

384
00:34:16,440 --> 00:34:21,640
like those things. However, I also think from an aesthetic perspective that some of these really

385
00:34:22,200 --> 00:34:28,120
evokes an artistic interpretation of what that abstract platonic concept represented by that

386
00:34:28,120 --> 00:34:37,640
class is. For me, this image of a prison cell evokes more than just a picture of a prison cell.

387
00:34:37,640 --> 00:34:42,600
It seems to me like an artist decided to represent the bleakness but also the hope or

388
00:34:42,600 --> 00:34:47,160
something about this prison cell. And so even though there is no artist that was trying to

389
00:34:47,160 --> 00:34:51,400
capture that behind here, there's a neural network that's kind of captured the platonic

390
00:34:51,400 --> 00:34:57,720
concept of a prison cell, and that somehow leads to its own dialing in of what is central and

391
00:34:57,720 --> 00:35:03,000
essential about that concept, or at least evokes those kind of reactions in us and allows us to

392
00:35:03,000 --> 00:35:09,720
explore potentially new types of artistic and aesthetic connections to concepts. So if you

393
00:35:09,720 --> 00:35:14,520
look through the diversity of the images that were generated, I do think this kind of really hit

394
00:35:14,520 --> 00:35:19,880
the mark in terms of a quality diversity algorithm. You've got this huge set of images as all comes

395
00:35:19,880 --> 00:35:25,800
from, you know, one run. And at least I'm not, I think that they are, they might have been pulled

396
00:35:25,800 --> 00:35:30,840
from a couple of different runs in this case. But each one produces this giant, this diverse set

397
00:35:30,840 --> 00:35:34,680
of images, and many of them I think are really aesthetically interesting, like I think this

398
00:35:34,680 --> 00:35:40,040
volcano or this beacon, or this cup, I could actually imagine a coffee shop where this is this

399
00:35:40,040 --> 00:35:45,720
logo, your comments on a mask and a banana, etc. So we really, really thought it was cool to see

400
00:35:45,720 --> 00:35:51,640
kind of this pop out of an automated system back in 2015. Scientifically, we're also really

401
00:35:51,640 --> 00:35:55,640
interested in like whether or not goal switching was playing a huge role in these networks. And so

402
00:35:55,640 --> 00:36:01,080
we have, if you optimize for a single class only, like the water tower class, what we see is that

403
00:36:01,080 --> 00:36:06,280
you do indeed get stuck on a local optima. It lands on this particular pattern really early in the

404
00:36:06,280 --> 00:36:11,160
run. And then it just does minor tweets on that idea and gets stuck on it until eventually it kind

405
00:36:11,160 --> 00:36:16,760
of maxes out what you can do in that corner of the search space. In contrast with map elites,

406
00:36:16,760 --> 00:36:22,120
what you see is that early on it locks on this half dome moon image, and it does okay, but then

407
00:36:22,120 --> 00:36:26,440
it kind of gets stuck. And then from a totally different class, something that happened to have

408
00:36:26,440 --> 00:36:32,280
been produced to for the beacon class, actually ends up looking like a better water tower and

409
00:36:32,440 --> 00:36:36,920
goal switches in, it invades this class. And then with further optimization to look like a water

410
00:36:36,920 --> 00:36:42,520
tower ends up making the DNN think with 98% confidence that this is a water tower. And you

411
00:36:42,520 --> 00:36:48,680
can kind of see why. And we see this lesson over and over and over again. There's many goal switches

412
00:36:48,680 --> 00:36:54,760
happening within this population of networks. And we think that's a big reason why performance is

413
00:36:54,760 --> 00:37:01,560
much higher than when you optimize for a single class. So what's really interesting about goal

414
00:37:01,560 --> 00:37:06,440
switching is that it allows what what are what biologists call adaptive radiations. So you come

415
00:37:06,440 --> 00:37:11,640
up with a good idea like maybe a more efficient way to metabolize oxygen in one lake in Africa.

416
00:37:11,640 --> 00:37:17,000
And then that idea will spread to all of the surrounding lakes in Africa. And then on top of

417
00:37:17,000 --> 00:37:23,080
that technological foundation, those fish will respecialize to their particular niche and adapt

418
00:37:23,080 --> 00:37:27,080
that innovative incorporate that innovation. The same thing happened with Darwin's finches,

419
00:37:27,080 --> 00:37:33,160
which radiated out from one from one couple of finches to all of these diverse finches.

420
00:37:33,160 --> 00:37:37,400
And we see the same thing in technology where computers, for example, were invented for one

421
00:37:37,400 --> 00:37:42,040
purpose and then kind of spread throughout an ecosystem and are now embedded in all sorts of

422
00:37:42,040 --> 00:37:47,560
technological devices in our lives. So what's really nice is you can see these adaptive radiations

423
00:37:47,560 --> 00:37:51,880
happen in these quality diversity algorithms. So this is one of my favorite plots from all of

424
00:37:51,960 --> 00:37:57,400
the science I've done in my entire career. Inside one of these innovation engine runs,

425
00:37:57,400 --> 00:38:01,960
you've got this early innovation, which is this dome against a background, a colored background.

426
00:38:01,960 --> 00:38:08,520
And that thing, which looked up the abaya class, then radiates out and it's children because this

427
00:38:08,520 --> 00:38:13,800
is a population. So these literally are descendants of each other. It's descendants kind of produce

428
00:38:13,800 --> 00:38:19,320
a phylogenetic tree, just like we see in nature. And ultimately, this innovation turned into a

429
00:38:19,320 --> 00:38:26,600
volcano, a mosque, a water tower, a beacon, a yurt, a church, a planetarium, an obelisk, and a dome.

430
00:38:26,600 --> 00:38:30,920
And it's just awesome to see an innovation then get rid of that concept, get rift upon and kind

431
00:38:30,920 --> 00:38:36,600
of radiate out into a huge explosion of diversity. So if you study the history of biology, you'll see

432
00:38:36,600 --> 00:38:39,880
that there were many moments in the history of biology where something similar happened. We got

433
00:38:39,880 --> 00:38:45,400
like, you know, single multicellular organisms or rate or bilateral symmetry or the four-legged

434
00:38:45,400 --> 00:38:50,200
body plan. And then you see this explosion of diversity that descends from that central innovation.

435
00:38:50,200 --> 00:38:53,400
So I think it's beautiful to see that happening inside of our algorithms.

436
00:38:54,760 --> 00:39:00,120
We ended up submitting the art that was produced by this algorithm to a competition at the University

437
00:39:00,120 --> 00:39:04,360
of Wyoming where I was a professor. And every year, art students work for a year and they submit

438
00:39:04,360 --> 00:39:08,360
their best project to this competition. And then there's a judges who decide which of them get

439
00:39:08,360 --> 00:39:13,800
hung on the wall and accepted into the competition. So we did not tell them this is AI-generated art,

440
00:39:13,800 --> 00:39:18,600
we just submitted it. And not only was the art accepted, it was also given an award.

441
00:39:18,600 --> 00:39:23,000
So here you see people having wine and cheese. And I was like eavesdropping as they're discussing

442
00:39:23,000 --> 00:39:27,640
the intent of the artist behind producing all of these different images, not knowing that it was

443
00:39:27,640 --> 00:39:33,000
an AI algorithm behind it, which I thought was pretty cool. So in some sense, this passed the

444
00:39:33,000 --> 00:39:40,360
artistic turning test. Sample size one. FYI, in case you're interested, there is much more work on

445
00:39:40,360 --> 00:39:45,320
CPPNs that are more modern. So nowadays, a lot of people are playing with differentiable CPPNs

446
00:39:45,320 --> 00:39:49,880
instead of using evolution. I have to because it's so beautiful. Quickly look at the work of Alex

447
00:39:49,880 --> 00:39:55,080
here, which I highly recommend you check out. All of these things here are different CPPN

448
00:39:56,120 --> 00:40:01,000
represented networks that are doing deep visualization, which is the technique I'm

449
00:40:01,000 --> 00:40:05,240
going to tell you about later. So I encourage you to check that out. There's also, you can

450
00:40:05,240 --> 00:40:09,560
use CPPNs to encode neural networks. I did that a lot in my dissertation and now you can do that

451
00:40:09,560 --> 00:40:15,400
with Backprop. David Ha has been pushing that and there's much more work in this vein. Okay,

452
00:40:15,400 --> 00:40:20,760
so getting back to QD, I think that I hopefully have convinced you that it has all of these nice

453
00:40:20,760 --> 00:40:26,360
properties, like a diverse set of high performing solutions that it produces, it has goal switching,

454
00:40:26,360 --> 00:40:30,920
and it allows you to kind of illuminate the entire search space and learn a lot about what's possible.

455
00:40:32,040 --> 00:40:36,200
Just quickly, I want to say that these ideas really have given us a lot of leverage on hard

456
00:40:36,280 --> 00:40:41,640
technical problems. So in this paper that we had in Nature, we use these ideas to have robots that

457
00:40:41,640 --> 00:40:46,040
could adapt to damage within one to two minutes to get up and continue on with their mission,

458
00:40:46,040 --> 00:40:50,920
even if they're extremely damaged. And then we also use these ideas behind the algorithm GoExplore,

459
00:40:50,920 --> 00:40:55,720
which you may have heard of, which completely solved the Atari benchmark suite, including

460
00:40:55,720 --> 00:41:00,760
solving really hard exploration challenges like mono zoom as revenge and pitfall. You can see all

461
00:41:00,760 --> 00:41:04,920
the previous attempts to solve this heartless game, which became kind of its own grand challenge

462
00:41:04,920 --> 00:41:09,240
of the field, do not perform very well. And then this is the difference once you start adding in

463
00:41:09,240 --> 00:41:14,200
these ideas from quality diversity algorithms. Ultimately, we ended up beating the human world

464
00:41:14,200 --> 00:41:20,920
record on this game. Oh, and as a quick little teaser, this paper was also recently accepted

465
00:41:20,920 --> 00:41:24,680
to a really nice journal. I can't quite tell you which one, but if I'll share that information

466
00:41:24,680 --> 00:41:28,280
on Twitter in the next couple of weeks, if you are interested to get the final version

467
00:41:28,280 --> 00:41:33,720
and the updated version of this paper. So I think QD algorithms are really interesting.

468
00:41:33,720 --> 00:41:37,480
I think the question that we should always ask though is what's missing where, you know,

469
00:41:37,480 --> 00:41:41,880
they're not yet open-ended algorithms. So the thing that I think is missing is that while these

470
00:41:41,880 --> 00:41:46,360
things can produce a large diverse set of interesting solutions within one domain,

471
00:41:46,920 --> 00:41:50,600
ultimately, their ability to innovate is constrained because they're stuck in this one

472
00:41:50,600 --> 00:41:55,160
particular setting that we put them in. But what we really want is these open-ended algorithms that

473
00:41:55,160 --> 00:42:00,680
just keep going and kind of generating wildly different solutions as they run. So traditionally

474
00:42:00,680 --> 00:42:05,080
in ML, we pick a particular challenge like Chester, Gro or Dota or Starcraft and we bang

475
00:42:05,080 --> 00:42:09,720
away on it for a while. But the intriguing possibility that I want all of you to consider

476
00:42:09,720 --> 00:42:14,760
today is could we create an algorithm that generates its own challenges and solves them?

477
00:42:15,640 --> 00:42:21,960
Just as nature arguably created the challenge or the opportunity of leaves on the top of trees,

478
00:42:21,960 --> 00:42:26,520
and then the solution to that challenge, which is giraffes or caterpillars that can eat them.

479
00:42:27,400 --> 00:42:31,000
So, you know, this kind of a thing might produce something that's interesting

480
00:42:31,000 --> 00:42:35,720
after a billion years. So our most recent work on this is in this algorithm called Poet,

481
00:42:35,720 --> 00:42:41,160
which is the paired open-ended trailblazer. And the idea here is that we're going to try to endlessly

482
00:42:41,160 --> 00:42:45,480
generate interesting, complex and diverse learning environments and their solutions.

483
00:42:46,360 --> 00:42:52,280
So the idea is again quite simple and you'll recognize it. It's basically we want to

484
00:42:52,280 --> 00:42:57,400
generate new learning environments and we're going to add them to this set of our population of

485
00:42:57,400 --> 00:43:02,680
environments if they're not too easy and not too hard for the current population of agents.

486
00:43:02,680 --> 00:43:05,880
And if they're novel, there's something about them that's unique and different.

487
00:43:05,880 --> 00:43:09,720
And then we'll optimize agents to better solve each of these challenges and we'll allow goal

488
00:43:09,720 --> 00:43:15,880
switching between them. So the example task that we used here is obstacle courses. So this little

489
00:43:15,880 --> 00:43:21,160
creature here has to run as fast as possible without falling over. And here's the general idea.

490
00:43:21,160 --> 00:43:24,920
You start with an easy environment. So first you have to make that encoding choice. How are you

491
00:43:24,920 --> 00:43:30,760
going to encode an environment on a parameter vector? Here we have things like the number of

492
00:43:31,320 --> 00:43:35,240
whether or not there are gaps, whether or not there are stumps, the ruggedness of the terrain,

493
00:43:35,240 --> 00:43:39,800
et cetera. So you can start with an easy one of those, which is maybe just flat terrain.

494
00:43:39,800 --> 00:43:44,600
And then you start having an agent, which has its own parameter vector. This is a neural network

495
00:43:44,600 --> 00:43:49,640
and is learning via RL to solve this task. And once it gets good enough on that task,

496
00:43:49,720 --> 00:43:55,160
then we copy phi 1, the parameter vector of the environment, to make phi 2. And then we'll try

497
00:43:55,160 --> 00:44:00,680
this agent via transfer and goal switching. It goes and it starts optimizing here. Now,

498
00:44:00,680 --> 00:44:05,400
we are simultaneously continuing to optimize this parameter vector on this environment and this

499
00:44:05,400 --> 00:44:11,240
parameter vector on this environment. We keep going. Maybe eventually this environment gets

500
00:44:11,240 --> 00:44:16,920
solved well enough by this parameter vector. So we copy it and we now make phi 3 a new environment.

501
00:44:17,000 --> 00:44:23,000
Turns out that's too hard for either theta 1 or theta 2. So we throw that out. We generate,

502
00:44:23,000 --> 00:44:28,200
we try again, we get a new environment and we test this one and this one. We take the better of those

503
00:44:28,200 --> 00:44:33,640
to you on this new environment to seed training. And in this case, it was theta 2. So it goes in

504
00:44:33,640 --> 00:44:38,040
there. This does not have to be a linear chain. At any point, any one of the environments in the

505
00:44:38,040 --> 00:44:43,800
set can produce a new environment. And then we'll try all of the current agents on that environment

506
00:44:43,800 --> 00:44:49,160
to see if they're the best and if they are, they get to start. And the process can keep going like

507
00:44:49,160 --> 00:44:54,040
this. Now, imagine eventually we generate a really, really hard challenge like phi 6 here.

508
00:44:54,600 --> 00:45:00,680
And initially the best parameter vector, we try all of them on this environment was theta 5. It

509
00:45:00,680 --> 00:45:05,640
was the best stepping stone. So we start optimizing a copy of theta 5 in this environment and it gets

510
00:45:05,640 --> 00:45:09,960
better and better and better. But it maybe hits a local optimal and it can't break through and

511
00:45:09,960 --> 00:45:14,440
really, really do well on this environment. But in the meantime, we're still optimizing theta 4 on

512
00:45:14,440 --> 00:45:19,080
this environment. Maybe it has an innovation that makes it better on this environment. So it invades

513
00:45:19,080 --> 00:45:24,040
this environment, just like a species in nature could invade a new niche, kicks out that parameter

514
00:45:24,040 --> 00:45:29,480
vector. And now we start building on the back of this innovation here. And then that maybe with a

515
00:45:29,480 --> 00:45:34,040
little bit more optimization comes up with an innovation that then transfers in and becomes

516
00:45:34,040 --> 00:45:38,760
the best thing we've ever seen on phi 6. And maybe that gets us off the local optimal and solves that

517
00:45:38,760 --> 00:45:45,000
problem. So that is kind of the nature of goal switching. So here we use evolution strategies,

518
00:45:45,000 --> 00:45:52,440
but any RL algorithm would work. And you can see this little agent here. And it is traversing this

519
00:45:52,440 --> 00:45:57,960
course. And what you can see is at the beginning, all of the challenges are quite simple. They're

520
00:45:57,960 --> 00:46:04,440
a little tiny stumps, little gaps, just a little ruggedness in the terrain. But over time, the

521
00:46:04,440 --> 00:46:10,040
agent gets better and better. And the environments automatically start getting harder and harder.

522
00:46:10,040 --> 00:46:18,120
So it's kind of like a natural curriculum generation. And you can still, the algorithm

523
00:46:18,120 --> 00:46:23,400
is here is kind of still pushing in separate dimensions, like taller gaps or more ruggedness

524
00:46:23,400 --> 00:46:29,960
or wider gaps. Sorry, I didn't tell her stumps. Later in time, with more training, the algorithm

525
00:46:29,960 --> 00:46:34,680
starts to put together these challenges. Sorry, my dog is barking. So you get things like bigger

526
00:46:34,680 --> 00:46:40,120
gaps and stumps and ruggedness all put together. And ultimately, these environments get really,

527
00:46:40,120 --> 00:46:46,840
really, really difficult for this little robot to traverse. Here's another challenge that was

528
00:46:46,840 --> 00:46:53,400
invented and solved by this algorithm. So I think from an aesthetic point of view, it's kind of cool

529
00:46:53,400 --> 00:46:57,800
because you can think about each one of these robots as its own little creation. It's kind of a

530
00:46:57,800 --> 00:47:03,240
curiosity. Just like animals in the world, we love to watch nature shows and see different animals

531
00:47:03,240 --> 00:47:07,560
and how they're different and what they can accomplish and how their bodies are different,

532
00:47:07,560 --> 00:47:12,040
et cetera. So you can kind of think of the agents produced by these things as really interesting

533
00:47:12,040 --> 00:47:16,920
aesthetic artifacts. Scientifically, we wanted to see whether or not goal switching in this

534
00:47:16,920 --> 00:47:21,000
domain was paying off. And so we did direct optimization in each one of these environments

535
00:47:21,000 --> 00:47:25,880
and found that it failed miserably. That's down here. And with Poet and the goal switching,

536
00:47:25,880 --> 00:47:30,600
what you see is much, much better performance in each one of these environments. This is the only

537
00:47:30,600 --> 00:47:36,360
way that we know of to go solve these hard problems. And in the paper, there's more of a

538
00:47:36,360 --> 00:47:41,400
detailed study about that claim if you're interested. So I want to show you one anecdote of what popped

539
00:47:41,400 --> 00:47:45,480
out in the system. So I think it's so interesting. So here in the simplest possible environment,

540
00:47:45,480 --> 00:47:49,880
a flat ground, you get this agent here that is optimized for a really long time and it's got

541
00:47:49,880 --> 00:47:56,600
this knee-dragging behavior. And eventually, the system generates a permutation of this

542
00:47:56,600 --> 00:48:00,600
environment, which is a harder challenge that has little tiny stumps. And this knee-dragging

543
00:48:00,600 --> 00:48:05,720
behavior is not very good because it keeps tripping up on these little stumps. So with

544
00:48:05,720 --> 00:48:11,400
some more optimization in that environment, the agent learns to stand up and it gets faster at

545
00:48:11,400 --> 00:48:15,880
that. Now, because the algorithm is always checking any solution to see whether or not it's better

546
00:48:15,880 --> 00:48:21,560
at invading some other niche, this descendant actually goes back automatically and invades that

547
00:48:21,560 --> 00:48:27,160
flat ground, replacing this knee-drager. Now that it knows how to stand up, as you can see here,

548
00:48:27,160 --> 00:48:32,760
it gets much better performance in that new environment. And then with further optimization,

549
00:48:32,760 --> 00:48:37,160
it ends up with much better performance. Now, because this is a computational system,

550
00:48:37,160 --> 00:48:41,960
we could do the counterfactual. We went back to this original agent in the top left and we optimized

551
00:48:41,960 --> 00:48:47,400
it for an equal amount of computation in that flat ground environment. And it just never learns to

552
00:48:47,400 --> 00:48:53,160
stand up. It's just on a local optimal and it's stuck in its ways. It was only by going into a

553
00:48:53,160 --> 00:48:59,000
harder environment and coming back that it learned a better behavior and a better strategy. And this

554
00:48:59,000 --> 00:49:03,560
is why I think that it's so hard to design curricula. You would never, as a human, say that you're

555
00:49:03,560 --> 00:49:08,040
going to take something to a harder environment just to have it solve a simpler environment.

556
00:49:08,120 --> 00:49:11,240
But in this case, that's exactly what was needed to solve this problem.

557
00:49:12,520 --> 00:49:18,440
So we go through a quantifying algorithm that goal switching is essential to solve the hardest

558
00:49:18,440 --> 00:49:24,840
challenges generated by this system. So future work in this domain, I think there's all sorts of

559
00:49:24,840 --> 00:49:31,480
stuff you could do. Obviously, you could just take it into more complex rich simulators. So,

560
00:49:31,480 --> 00:49:36,360
you know, you could have more complex encodings as well. But here is like the world's from deep

561
00:49:36,360 --> 00:49:42,040
mind. But I think it's really kind of pumps my intuition is to watch, you know, what's possible,

562
00:49:42,040 --> 00:49:46,680
what will be possible in the future with more computation. Like imagine what Poet could do

563
00:49:46,680 --> 00:49:51,480
in a world this complicated, where it has to do with flying creatures and climbing and talking

564
00:49:51,480 --> 00:49:56,840
to other agents, maybe negotiating trades in a market, you know, and if you were doing all of

565
00:49:56,840 --> 00:50:01,400
this, you know, what might pop out of the system, I think it's fascinating to consider, both from

566
00:50:01,480 --> 00:50:07,320
a static perspective and from a machine learning perspective. You also could optimize the bodies

567
00:50:07,320 --> 00:50:11,400
of the creatures themselves. So in the bottom in the right, you see, you know, I showed you some

568
00:50:11,400 --> 00:50:16,680
work that we did in that vein a while back, but not with Poet. And David Ha has done that in

569
00:50:16,680 --> 00:50:21,480
particular environments that are handcrafted. But imagine if you paired body optimization with

570
00:50:21,480 --> 00:50:25,160
environment generation, then you could really get weird things like you see in nature, where you

571
00:50:25,160 --> 00:50:29,320
have a particular kind of like cave dwelling spider that's optimized to that environment,

572
00:50:29,320 --> 00:50:33,640
which is very different from birds that are flying up in the Pacific Northwest.

573
00:50:34,600 --> 00:50:37,800
So another thing that I think would be interesting would be to combine innovation

574
00:50:37,800 --> 00:50:43,400
engines with modern tools. So imagine if you took something like Dolly, which is this amazing

575
00:50:43,400 --> 00:50:49,560
new thing produced by my colleagues here at OpenAI. And not only did you have humans asking for

576
00:50:49,560 --> 00:50:54,520
particular innovations or particular images from Dolly, but you have the algorithm invent the

577
00:50:54,520 --> 00:50:58,920
challenge and the solution. So the challenge could be, you know, can you create this? Can you create

578
00:50:59,640 --> 00:51:03,640
this? Dolly would then create them. And if they're interesting, you add it to a set. And then you

579
00:51:03,640 --> 00:51:07,720
have something that looks at the set of things that are already produced and produces completely new

580
00:51:07,720 --> 00:51:12,760
types of images. That would be awesome to see. And that doesn't have to be limited to images. You

581
00:51:12,760 --> 00:51:18,040
could use then the same technology to do it in different modalities, such as videos and music

582
00:51:18,040 --> 00:51:23,240
and poetry or algorithmic space. Again, the challenge that remains is how do you detect

583
00:51:23,240 --> 00:51:27,560
what's interestingly new? I'll throw it out there that I think you probably with a lot of data could

584
00:51:27,560 --> 00:51:32,520
learn a function of what humans consider interesting. In fact, if Joel remembers, I sent him a giant

585
00:51:32,520 --> 00:51:36,200
email saying that I think we should do this with his website, GanReader. We haven't done it yet,

586
00:51:36,200 --> 00:51:41,560
but it'd be a great project for a student to take on. So I want to quickly check the time here.

587
00:51:42,600 --> 00:51:46,760
Yeah. So we started a little bit late. So I'm going to race through this because I think you'll

588
00:51:46,760 --> 00:51:51,640
find it interesting, but I won't be able to go into any detail here. But part two of the talk,

589
00:51:52,520 --> 00:51:56,280
which I'll do very quickly, is I wanted to tell you about this entire other arc of research that

590
00:51:56,280 --> 00:52:01,560
we did called AI neuroscience, which is how much we want to study. Just like neuroscientists try to

591
00:52:01,560 --> 00:52:05,480
study the human brain, we want to study how much the deep neural nets understand about the images

592
00:52:05,480 --> 00:52:12,040
that they classify. So we're all familiar with deep neural nets, but they tend to be a black box.

593
00:52:12,040 --> 00:52:17,880
We don't really know what each neuron in the deep neural net does. But one way neuroscientists probe

594
00:52:17,880 --> 00:52:23,400
this question is they literally put probes into your neurons and they look for which neurons light

595
00:52:23,400 --> 00:52:28,120
up in response to which images. For example, they found neurons that light up in response to

596
00:52:28,120 --> 00:52:33,240
Kobe Bryant or Bill Clinton, for example. And people have called these things like a Kobe Bryant

597
00:52:33,240 --> 00:52:37,480
neuron, for example, and they respond to very different modalities, such as the name Kobe Bryant,

598
00:52:37,480 --> 00:52:42,200
a line drawing him in the Lakers uniform. The question is, you don't really know just because

599
00:52:42,200 --> 00:52:46,920
the response to those images, if it's a Kobe Bryant neuron, it could be an LA Laker neuron

600
00:52:46,920 --> 00:52:52,680
instead of a Kobe Bryant image neuron, for example. So we thought the ideal task would be to synthesize

601
00:52:52,680 --> 00:52:57,560
the images that maximally activate that neuron. And if you did that and you got these images,

602
00:52:57,560 --> 00:53:02,200
then you'd know, oh, that's a Laker neuron, not a Kobe Bryant neuron. But if you got these images,

603
00:53:02,200 --> 00:53:07,400
you'd know it's a Kobe Bryant neuron. So this is actually possible with artificial neural networks,

604
00:53:07,400 --> 00:53:12,120
but you can do is you can take a neural net and then you could have like an artist, an AI artist

605
00:53:12,120 --> 00:53:18,280
that's trying to generate an image to activate this particular neuron here. And what you can do is

606
00:53:18,280 --> 00:53:22,280
you can use backprop. So the artist generates an image and then you just follow the gradient to

607
00:53:22,280 --> 00:53:26,520
increase this neuron until you get an image that lights up that neuron, and it might look like this.

608
00:53:26,520 --> 00:53:31,000
And you can do the same technique for all the intermediate neurons in the neural net.

609
00:53:31,720 --> 00:53:37,000
We call this deep visualization. Our first attempt at this actually was that same paper,

610
00:53:37,000 --> 00:53:42,760
deep neural nets are easily fooled when we did it with CPPNs here or a direct encoding on the left

611
00:53:42,760 --> 00:53:48,440
here, or with backprop on the right, we got images that did not look at all like things that they're

612
00:53:48,440 --> 00:53:54,440
supposed to, but the neural net was perfectly sure is a peacock or chimpanzee. And you know what

613
00:53:54,440 --> 00:53:59,240
happened with that paper. We then went on and started asking questions like why are these neural

614
00:53:59,240 --> 00:54:03,800
nets easily fooled? And I don't have time to get into a lot of the details here, but what we basically

615
00:54:03,800 --> 00:54:09,960
thought is that maybe deep neural nets do recognize the images they're supposed to like a lion or a

616
00:54:09,960 --> 00:54:16,040
dolphin, but maybe they recognize a whole lot of other things also as in that class unnatural images.

617
00:54:16,040 --> 00:54:21,800
So if we could stop the artist from generating unnatural images and only stay to the space of

618
00:54:21,800 --> 00:54:27,480
natural images, then we might find out what that neuron really is for and what it's been trained

619
00:54:27,480 --> 00:54:32,520
to see within the space of natural images. So skipping over some of the details here,

620
00:54:33,480 --> 00:54:37,880
the fooling work started out saying maybe these deep neural nets don't really understand at all

621
00:54:37,880 --> 00:54:41,880
what they're classifying. They're just locking on to spurious correlations like that there's a

622
00:54:41,960 --> 00:54:45,960
orange texture. If you see orange, you know, this kind of orange texture next to blue color

623
00:54:45,960 --> 00:54:50,680
of starfish, but they never learned like what a five-legged starfish is because they didn't

624
00:54:50,680 --> 00:54:54,840
need to to solve the problem. We wanted to see whether or not there is that notion of like a

625
00:54:54,840 --> 00:55:00,120
five-legged starfish in the network. So in take two, what we tried to do is we added more manual

626
00:55:00,120 --> 00:55:05,720
priors to try to constrain the image generator, the artist, to generate only natural images. And

627
00:55:05,720 --> 00:55:11,000
when we add that extra constraint, then we get images, you know, previously people had done that

628
00:55:11,000 --> 00:55:17,640
and they kind of looked like this. You start to see dumbbells and dolomations. These are the

629
00:55:17,640 --> 00:55:22,120
ones that we got with slightly better priors. And you can start to see that the network does

630
00:55:22,120 --> 00:55:27,080
actually kind of know what a flamingo is or a beetle. It's an interesting historical side note.

631
00:55:27,080 --> 00:55:32,120
These images here in this work inspire deep dream, which is also done by Alex over at Google.

632
00:55:32,120 --> 00:55:37,560
And that stuff is super cool if you haven't seen it. And then third take, we tried to add even

633
00:55:37,560 --> 00:55:43,880
better priors, manually designed priors, and what you get are these images here. And I want to stop

634
00:55:43,880 --> 00:55:48,120
for a moment and kind of reflect on this from an aesthetic perspective. We're trying to do better

635
00:55:48,120 --> 00:55:54,360
and better science. We're creating different algorithms or different hand-coded priors to

636
00:55:54,360 --> 00:55:59,400
kind of accomplish the scientific quest. But if you look at the different images, each one of them

637
00:55:59,400 --> 00:56:04,200
has a different style. And I think it's kind of interesting that like slight tweaks to algorithms

638
00:56:04,200 --> 00:56:08,680
produce wildly different artistic styles. It's like all these different artists are out there

639
00:56:08,680 --> 00:56:11,960
and you just kind of are searching through the space of artists kind of accidentally

640
00:56:11,960 --> 00:56:16,040
while you're doing your science. So this style is very different from this style. And I actually

641
00:56:16,040 --> 00:56:20,360
think this is just really beautiful. Like if I saw this in an art museum, I would think that this

642
00:56:20,360 --> 00:56:24,120
is beautiful art, even though it was produced purely for scientific reasons and we had no

643
00:56:24,120 --> 00:56:29,720
intention of producing images in this style. We then went on for one more take at this. We tried

644
00:56:29,720 --> 00:56:35,800
to say, okay, we're machine learning researchers instead of manually encoding what characterizes

645
00:56:35,800 --> 00:56:40,440
a natural image. Let's learn it. And so we start learning the natural image priors and our papers

646
00:56:40,440 --> 00:56:45,080
are full of lots of details on this. And the way that we do this, we have a generator kind of like

647
00:56:45,080 --> 00:56:50,520
the generator in a GAN. We hook it up to the target network we're interrogating. And then we try to

648
00:56:50,520 --> 00:56:54,760
search in a latent code to produce an image that activates a certain neuron in question.

649
00:56:55,400 --> 00:57:00,280
And when we did that, we got these images, which at the time were some of the most realistic images

650
00:57:00,280 --> 00:57:05,880
deep neural nets had ever produced. You were seeing realistic lawnmowers and lemons and barns

651
00:57:05,880 --> 00:57:12,360
and candles. These images are not great by modern standards, but this is 2016. Here are other images

652
00:57:12,360 --> 00:57:18,120
in this class. And for the first time ever, the images were starting to look photorealistic.

653
00:57:18,120 --> 00:57:22,200
Like these are the synthetic images for this class. And these are the real images.

654
00:57:22,200 --> 00:57:25,960
And, you know, I don't think that you would really be able to tell the difference if I had swapped

655
00:57:25,960 --> 00:57:31,640
them unless you look very carefully. So compared to the best work at the time, which is on the left,

656
00:57:31,640 --> 00:57:38,040
these images were a big step up. And they helped us confirm this hypothesis, which is basically,

657
00:57:38,040 --> 00:57:41,800
if this is the space of natural images, these networks do understand what it means to be a

658
00:57:41,800 --> 00:57:48,520
lawnmower. Like if this blue line here is the class of lawnmowers, then they do stay to, if you

659
00:57:48,520 --> 00:57:53,480
keep them in the natural, if you only generate images in the natural image space, then you do

660
00:57:53,480 --> 00:57:58,360
get a lawnmower. But if you let it generate images anywhere in the space, like all the way out here,

661
00:57:58,360 --> 00:58:03,080
then it also, the network will similarly say this garbage here is in the class of what it

662
00:58:03,080 --> 00:58:07,400
means to be a lawnmower. And so if we want for aesthetic purposes to have neural nets generate

663
00:58:07,400 --> 00:58:13,000
realistic stuff, we got to get it focused on something that both is natural and activates

664
00:58:13,000 --> 00:58:17,720
the network's classification as opposed to way out here. And GANs do this also, but they do it

665
00:58:17,720 --> 00:58:22,680
via a very different mechanism. So I told you, you could look at each individual neuron within

666
00:58:22,680 --> 00:58:27,320
the network. I don't have time to go through this now, but if you're interested, then I encourage

667
00:58:27,320 --> 00:58:31,080
you to kind of go into the paper and look, you could kind of fly around the neural net and see

668
00:58:31,080 --> 00:58:36,200
that you get things like cargill grill detectors and buckets and bird heads. And as you go up in

669
00:58:36,200 --> 00:58:40,680
the network, you get these really weird concepts like one-eyed turtles and like arches over water

670
00:58:40,680 --> 00:58:45,080
until you eventually get the class neurons where we know what they are because we've grounded them

671
00:58:45,080 --> 00:58:50,120
via our labels. The one final thing I'll mention here is that the one problem with our technique

672
00:58:50,120 --> 00:58:55,800
is that it generates very, very little diversity. So these are synthetic images produced by our

673
00:58:55,800 --> 00:59:01,560
network for this class. And they look a lot like the images that most highly activate that neuron

674
00:59:01,560 --> 00:59:06,920
from the real world from the real data set, but they don't represent the diversity of images in

675
00:59:06,920 --> 00:59:12,520
that class. And so we did a lot of work, including adding with Yashua Benji on these things called

676
00:59:12,520 --> 00:59:17,160
plug-and-play generative networks, where we wanted to add a lot more diversity. And so you could

677
00:59:17,160 --> 00:59:21,000
take the same network and you can light up a bunch of different classes that it's never even seen

678
00:59:21,000 --> 00:59:25,480
before. That's a bit of an aside like ballrooms and art galleries, but mostly we were interested in

679
00:59:25,480 --> 00:59:31,800
getting more diversity. And the takeaway message is we were able to accomplish that. So here is PPGNs,

680
00:59:31,800 --> 00:59:36,200
which is the one that has more diversity. And you can see a much more diversity in this set of

681
00:59:36,200 --> 00:59:42,440
images versus DGNAMV1, which are the images over here. And this diversity better represents

682
00:59:42,520 --> 00:59:47,720
kind of the diversity of the natural class. So with the original attempt, DGN, you got volcanoes

683
00:59:47,720 --> 00:59:52,360
that look like this. It kind of goes and finds one type of volcano, like a local optima, and it

684
00:59:52,360 --> 00:59:57,400
sits on it. But the plug-and-play generative networks are much more kind of like an open-ended

685
00:59:57,400 --> 01:00:02,200
algorithm, at least within this class, where it samples new versions of volcanoes over and over

686
01:00:02,200 --> 01:00:08,600
again. And so you get all this big diversity of volcanoes out of this new sampling technique.

687
01:00:09,320 --> 01:00:14,600
So to conclude this, the AI neuroscience part, I won't actually get into these details, but it

688
01:00:14,600 --> 01:00:19,160
taught us a lot about what neural nets, you know, what's going on inside neural nets, it taught us

689
01:00:19,160 --> 01:00:24,120
whether or not they really recognize and learn about the concepts in our world. Like we did find

690
01:00:24,120 --> 01:00:28,120
in the end that they do know what a volcano is, and you know the five-legged nature of a starshow,

691
01:00:28,120 --> 01:00:32,760
and what a lawnmower is, even though they also are susceptible to producing and recognizing

692
01:00:32,760 --> 01:00:37,800
these adversarial fooling images as being part of the class. And it was cool to see the rapid

693
01:00:37,800 --> 01:00:44,520
progress just within my own team of collaborators from 2015 to 2017. And since then, I highly

694
01:00:44,520 --> 01:00:50,200
recommend the work of Chris Ola, who's continued to push in this direction. And very, very soon,

695
01:00:50,200 --> 01:00:55,240
Gabriel Go and Chris and others have new work coming out of OpenAI that will blow your mind.

696
01:00:55,240 --> 01:01:00,200
So I encourage you to watch the OpenAI blog in the coming weeks for this new result that you

697
01:01:00,200 --> 01:01:05,560
really like. You could do all of this stuff in different modes, like speech and video, etc.

698
01:01:05,560 --> 01:01:08,760
I won't dive into this. I want to just highlight one thing. This is my future work slide all the

699
01:01:08,760 --> 01:01:13,640
way back in 2016, and I thought it would be awesome to do this with real animal brains.

700
01:01:16,680 --> 01:01:20,280
Since then, actually, somebody has done that. They took our algorithm for DGN,

701
01:01:20,280 --> 01:01:24,120
they applied it to a real monkey brain, and they synthetically are generating images that activate

702
01:01:24,120 --> 01:01:28,600
neurons within the monkey brain, specifically within the face recognition part of the monkey

703
01:01:28,600 --> 01:01:33,720
brain. And you do in fact get a synthetic monkey-looking face, which is pretty amazing.

704
01:01:33,720 --> 01:01:38,760
So to conclude my overall talk, I think innovation engines are really interesting because they kind

705
01:01:38,760 --> 01:01:44,200
of push on this question of can we automatically produce an open-ended creative process that in

706
01:01:44,200 --> 01:01:50,120
any sort of modality like art or music or invention will just endlessly generate interesting new

707
01:01:50,120 --> 01:01:54,280
things. We've got a long way to go to accomplish that goal, but my colleagues and I, like Ken Stanley

708
01:01:54,280 --> 01:01:58,360
and Joe Layman and myself are really, really focused on this goal and trying to pull that off,

709
01:01:58,360 --> 01:02:04,280
including now at OpenAI, where all those people are. And I also showed you very, very quickly some

710
01:02:04,280 --> 01:02:07,720
of our work in AI neuroscience, which we were doing for scientific reasons, but produce these

711
01:02:07,720 --> 01:02:11,800
interesting aesthetic artifacts. And I'll just leave you with one final thought, which is that I find

712
01:02:11,800 --> 01:02:16,440
it surprising how often science produces aesthetic artifacts. Almost none of the work that I was

713
01:02:16,440 --> 01:02:21,720
doing was trying to do it just for aesthetic purposes, but along the way, it produced these

714
01:02:21,720 --> 01:02:26,440
things that I think are beautiful and interesting, and could be kind of aesthetic artifacts in their

715
01:02:26,440 --> 01:02:29,960
own right. And so I think it's nice because you don't have to choose between being an artist and

716
01:02:29,960 --> 01:02:34,840
a scientist. You kind of kind of can do both nowadays, especially with the modern tools and

717
01:02:34,840 --> 01:02:39,160
machine learning. And I'm sure that's kind of a realization that is being reinforced over and

718
01:02:39,160 --> 01:02:43,480
over again with all the different lectures in this wonderful class that you are participating in.

719
01:02:43,480 --> 01:02:47,800
So with that, I want to say thank you, and I'll turn it over to either questions or Joel, depending

720
01:02:47,800 --> 01:02:55,080
on what you want to do, Ali. Thank you so much. I appreciate it. It was really interesting and

721
01:02:55,080 --> 01:03:02,840
inspiring to me, and I'm sure for many of us in this class, this comment that you also made about

722
01:03:02,840 --> 01:03:11,720
science and creating and art, I think that it also is very well aligned with some of the other

723
01:03:12,280 --> 01:03:17,560
insight that we learned from other speakers. For instance, Alyosha, of course, was mentioning that

724
01:03:17,560 --> 01:03:23,560
when I asked this question, he was mentioning that he also thinks that, you know, creativity is

725
01:03:23,560 --> 01:03:30,360
a different tier of our evolution. So that really resonated with me what you were talking today.

726
01:03:31,880 --> 01:03:42,520
And I think that this is very exciting for us. One question that I have is if students want to,

727
01:03:43,560 --> 01:03:50,280
because this is a very interesting topic, and especially that type of poet or open-endedness

728
01:03:50,840 --> 01:03:59,720
area, if a student wants to join you in this sort of mission, what do you recommend to them to work on?

729
01:04:01,320 --> 01:04:06,840
Yeah. So one thing I would recommend is we had an ICML tutorial, I think about a year ago, that

730
01:04:06,840 --> 01:04:13,160
really covered a lot of this work in more depth. It's an ICML tutorial on population-based methods.

731
01:04:13,160 --> 01:04:19,560
So then you can see, can Joel and myself kind of going through, this is Joel Lehmann, not Joel Simon,

732
01:04:19,720 --> 01:04:25,480
going through a lot of the work that we've done in this field. And I recommend reading a lot of the

733
01:04:25,480 --> 01:04:30,920
work of both Ken and Joel, as well as you can look into some of the work that we've done in this area.

734
01:04:30,920 --> 01:04:35,480
And then in terms of what I recommend you work on, there's so many things, it's like,

735
01:04:35,480 --> 01:04:40,760
there's so many options that it's fun. You could apply a lot of these algorithms in a new domain,

736
01:04:40,760 --> 01:04:47,080
for example, that you find interesting, you know, a new kind of art. You could take more modern tools

737
01:04:47,080 --> 01:04:52,120
that work really well and weave them into these ideas, or you could invent new ideas, you know,

738
01:04:52,120 --> 01:04:57,560
like I still think if people, if anyone out here can crack the question of how can you automatically

739
01:04:57,560 --> 01:05:04,520
recognize newly interesting things, that I think is like a Turing award-winning innovation that will

740
01:05:04,520 --> 01:05:11,160
catalyze and propel so much algorithmic advance, including potentially advancing our push to

741
01:05:11,160 --> 01:05:16,200
artificial general intelligence. Like that might be one of the key stepping stones that gets us there.

742
01:05:16,200 --> 01:05:19,880
So I have this paper called AI Generating Algorithms, which I recommend people check out if

743
01:05:19,880 --> 01:05:25,080
they're interested. And it basically talks about how these sorts of ideas may do the fastest path

744
01:05:25,080 --> 01:05:32,120
to produce general AI. So that's not an aesthetic quest, it's more of a scientific quest. And but

745
01:05:32,120 --> 01:05:35,800
if you're interested in that, I think that's fascinating. But I also just think, just literally

746
01:05:35,800 --> 01:05:40,840
take all these ideas and go like do Poet, but do it in some totally wild and crazy different domain,

747
01:05:40,840 --> 01:05:46,440
or do an innovation engine in, you know, like architecture or poetry and see what happens,

748
01:05:46,440 --> 01:05:52,120
you know, you can use new tools like GPT-3 or Dolly, etc. So I think there's just a lot of

749
01:05:52,120 --> 01:05:58,120
low-hanging fruit here to be explored. Certainly. And that also reminds me of what you mentioned,

750
01:05:58,120 --> 01:06:05,320
we didn't optimize to create a microwave. We explored different things and I think that

751
01:06:05,320 --> 01:06:12,440
your advice is quite in that direction. Also, Joseph has a question. Joseph, would you like to

752
01:06:13,880 --> 01:06:21,080
ask it yourself or? I wasn't able to get my mic working earlier. Let me just

753
01:06:22,280 --> 01:06:30,360
fix that. Hi, Jeff. I was just wondering if you've explored anything on Poet in multi-agent settings

754
01:06:30,360 --> 01:06:35,240
to this point. Yeah, the short answer there is that we have a lot of really exciting ideas.

755
01:06:36,120 --> 01:06:39,720
For how we want to take advantage of that. I can't share those specific ideas

756
01:06:40,760 --> 01:06:45,800
because we may or may not be working on them. But I also think in the spirit of the talk that

757
01:06:45,800 --> 01:06:51,320
the best way to make advances is to have a community of people with different ideas pushing

758
01:06:51,320 --> 01:06:55,560
different directions because you never know what's going to unlock. So I almost don't want to give you

759
01:06:55,560 --> 01:06:58,760
too many ideas either because I don't want to cause conversion thinking. I think it's almost

760
01:06:58,760 --> 01:07:02,760
better if there's so many different ways you could apply the concepts of Poet to multi-agent

761
01:07:02,760 --> 01:07:08,280
settings that I don't think you can go wrong. I think if many different people and groups push

762
01:07:08,280 --> 01:07:13,880
on that, really good things will happen. Fair. We also may or may not be working on that, right?

763
01:07:16,760 --> 01:07:21,800
The one thing I wanted to ask you specifically about that is whether you figured out one,

764
01:07:21,800 --> 01:07:27,880
maybe you can't tell me, but any way to get around the problem where in multi-agent settings,

765
01:07:27,880 --> 01:07:33,800
sometimes you don't have a single evaluation metric that correlates the environment difficulty

766
01:07:33,800 --> 01:07:38,840
with agent performance because you add more agents in, well, then the performance goes down

767
01:07:38,840 --> 01:07:42,920
because there are more of them and they're all doing smarter things. So that sort of thing has

768
01:07:42,920 --> 01:07:48,920
thrown a wrench in the whole annex measure. Yeah, that's right. So one of the things you could switch

769
01:07:48,920 --> 01:07:57,240
to is a notion of agent versus agent. Like if an agent is as opposed to doing better on that

770
01:07:57,240 --> 01:08:03,480
environment, it's that agent versus other agents or agents that have come before. Another thing

771
01:08:03,480 --> 01:08:07,960
you could do is you could switch to more of a learning progress metric, which is if they're

772
01:08:07,960 --> 01:08:16,200
getting better, are they learning? According to some measure, like does their value function,

773
01:08:17,480 --> 01:08:22,120
their prediction of how well they're going to do, is that wrong? It's because they were either

774
01:08:22,120 --> 01:08:25,880
better or worse in that situation and versus those opponents than they thought they were,

775
01:08:25,880 --> 01:08:30,440
and measures like that could really catalyze, recognizing this is still an interesting environment

776
01:08:30,440 --> 01:08:33,800
because they're learning. This is still an interesting matchup between this opponent and

777
01:08:33,800 --> 01:08:38,040
this opponent because they're learning. I mean, we've been actually trying to do something very

778
01:08:38,040 --> 01:08:44,440
similar there. It still seems to run into the same sort of issue though, right? If you can't

779
01:08:44,440 --> 01:08:48,840
measure absolute performance, it can still be difficult to then measure relative performance

780
01:08:48,840 --> 01:08:54,440
because your reward peak can be going down even if you are learning because so are all the other

781
01:08:54,440 --> 01:08:59,240
agents. Sometimes you have to run as fast as you can just to remain in the same place.

782
01:08:59,800 --> 01:09:08,920
Yeah, pretty much. That's the red queen quote from Alison Wonderland. Yeah, these are all challenges

783
01:09:08,920 --> 01:09:12,200
and it's the kind of challenge that happens once you get into the multi-agent setting.

784
01:09:12,200 --> 01:09:16,600
So I think this is just for a lot of experimentation and hard thinking has to happen. I don't think

785
01:09:16,600 --> 01:09:22,120
there's a really super, short, easy, obvious answer. It's just going to require research.

786
01:09:22,120 --> 01:09:26,280
Well, I mean, I look forward to seeing what setting it is that you're trying that out in

787
01:09:26,280 --> 01:09:29,480
whenever that gets published. Likewise, yeah, with your work.

788
01:09:31,960 --> 01:09:37,480
Thanks. Excellent. Are there questions? Any more questions?

789
01:09:45,400 --> 01:09:51,480
Guys, don't be shy. If you have questions, just go ahead. Of course, if Jeff has time.

790
01:09:52,360 --> 01:09:56,360
I have time. I just also want to be cognizant of Joel and giving him his proper time.

791
01:09:57,960 --> 01:10:07,640
Excellent. Okay, cool. Thank you. All right, then let's thank you again, Jeff. It was really,

792
01:10:07,640 --> 01:10:09,320
really interesting and inspiring.

