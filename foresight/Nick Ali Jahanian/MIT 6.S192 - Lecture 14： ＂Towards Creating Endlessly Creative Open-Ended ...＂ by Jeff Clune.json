{"text": " Hello, everyone. Welcome to your course, Deep Learning for Art, Statistics and Creativity. Today, we have two special speakers. First, we serve as Dr. Jeff Klun, who is an associate professor in computer science at the University of British Columbia and also a research team leader at OpenAI. And he's going to talk about towards creating endlessly creative, open-ended innovation engines. I think this is a very exciting direction because so far we have talked about the interaction between art and AI. We said that how AI can help us to create and express ourselves and democratize the creativity in a sense. But also, the other direction is how our creativity can help us create better AI. For instance, how we learn by creating, how we define problems and find solutions for them and generalize to solve bigger problems and so on and so forth. So today is one of those, I would say, a realization of such a great idea that you will see as a gist of what Jeff has been working on. So please go ahead. And also, another question that we often ask in the class is that students are interested to know a little more about your background because they always feel inspired by seeing great scientists and what, for instance, got you to working on AI would be very interesting for them if you don't mind sharing. Great. Thank you for the introduction. Let me share my screen here and make sure that is working. So are you able to see my screen? Yeah. And the presentation? Yes. Okay. And can you see my mouse cursor? Yes. Okay. Hello, everyone. My name is Jeff Klune. And I want to talk to you today about trying to take on like an extremely big research challenge. I think it's a grand challenge of AI. And that is trying to create what we call open-ended algorithms. I wasn't planning on telling you a little bit about my background. I guess in brief, I started out on a quest just to understand two twin questions, which is how did natural evolution produce all the complexity on Earth, including the human brain? It's astounding. And we don't know how that process happened really. We don't know how to recreate it. And you'll see a lot of work towards that today. And then also, I'm interested in trying to figure out how does thinking happen and how can we create it in machines? And I think in many ways, these questions are very intertwined, as you'll see also today. So I started out in philosophy, actually, because I thought they had the market cornered on thinking, but really quickly kind of, or actually not quickly, slowly learned throughout the course of my life that the best way to tackle these challenges is to try to build these systems and recreate these systems computationally. Motivated by the wonderful quote by Richard Feynman, which is, that which I cannot build, I do not understand. So we understand by building. And that has certainly been true in my life that I understand more and more by being forced to turn speculation into code and into algorithms. So with that, I'm going to begin. So this talk is really going to be in two parts. The main part is going to be the first part. And it's about creating open-ended innovation engines. And if there's time, which I hope there will be, I'm going to rush through a series of work that we've done that I call AI neuroscience at the end. And then throughout all of this, what you're going to find is that this is a bit of a meandering intellectual story, because throughout my career, different research has kind of unintentionally produced different aesthetic artifacts of interest. And I kind of want to walk through some of the things and touch as many of these places where I think our work has produced things that are aesthetically interesting, as well as scientifically interesting. So the first thing I want to motivate is, you know, the idea of open-ended algorithms. So these are things that endlessly innovate. They just keep going forever. So if you think about natural evolution, look at the Tree of Life there, and think about all of the marvelous engineering designs that nature has brought and continues to create in an ever-going fashion, you know, jaguars, hawks, the human mind, everything that we know on Earth. You know, in most situations, we cannot rival these things with engineering. And so what's fascinating is that, you know, a very simple algorithm that Darwinian evolutionary algorithm, plus the context it's been placed in, continues to innovate for billions and billions of years. And I think it's really fruitful to think to yourself, you know, could you create an algorithm that you would want to run for billions and billions of years and come back and check whether or not it's interesting? Currently, as scientists, we have zero ability to produce things that are interesting even after a few months of running them on a computer, let alone billions and billions of years. So natural evolution is what we, you know, one of these open-ended algorithms. And another one is human culture, which just endlessly innovates and produce innovation on innovation and innovation. That's both true in science and technology, but it's also true in the arts, where you get, you know, impressionism after you get the classical paintings, and then you get, you know, post-modernism or Jackson Pollock or all the different kind of evolution of genres. So, you know, we started with the idea, when we try and wanted to try to work on this, is that natural evolution and human culture are what we call innovation engines. And that is that there's kind of this simple recipe that they follow that allows them to be creative. And that is that they start with a set of things, it could be an empty set, and then they generate a new thing. And then if that's interesting, they keep it and add it to the set. And then they take something out of that set, they change it a little bit, they permute it somehow, and they see if that is interesting. And if that's interesting, they add that to the set. And you have this growing set of things, these archives of things you've already produced that are interesting. And then each one of those is a stepping stone to new potential innovations or solutions. And if you think deeply about it, that's true both of human culture and natural evolution. And so the question is, with that kind of mental framework, can we create algorithms that do that process automatically? And so, you know, at the core, there's really kind of these two simple steps. The first one is you have to have something that generates new things based on previous things. That's the green box on the left. And then you have to evaluate whether not those things are interesting. And if they are, then you add them to the set and you just keep repeating this process. So in the long run, what we'd love to do is take, you know, humans out of the loop if possible, label data out of the loop, and you just have some sort of generator like a neural network that can generate new things like poems or codes or mathematical proofs or images, or, you know, technological artifacts, something, maybe another deep neural net that is trained to recognize what's interesting somehow, and then that process could just iterate. Now, for example, you could imagine that you take like the orange box here is an autoencoder. And it looks at everything that it's seen before, it compresses them down to a low dimensional bottleneck space, and then it has to uncompress them. And then if you get some new latent vector that's new that you've never seen before, you call that interesting. And that might be one thing that could kick off this problem. And if you could do that, you would have an innovations arms race in any domain, you could unleash this thing anywhere. And that would be amazing. And a lot of these ideas date back to Schmidt-Huber ideas from the early 90s. However, the problem is that when you do that, you typically do get new things forever, but you don't get new interesting things forever. You, for example, might get white noise, just an endless stream of different patterns of white noise, because those are uncompressible. So really, at its core, the biggest challenge in this field is kind of how do you avoid generating uninteresting novelty, and how do you only generate interesting novelty? And here's one example by a friend of mine, Josh Auerbach, who tried to basically take the same encoding that I'm going to tell you about later, and a similar system is trying to automatically generate images and try to produce new interesting images forever. And these images are interesting, they're pretty cool, but they're not nearly as interesting as they could be, right? They're not like what artists would do over the course of centuries. You would expect and hope that things would ultimately break out of these kind of abstract patterns. And that's because these things are optimized to produce information theoretic metrics like compression or mutual information and things like that. So what we thought in this work is that one insight you could have is that recognizing a new type of thing is like being able to recognize a new class of thing. If you've never seen a palm tree before, that's a distinct kind of trees. And if you've never seen a tree before, trees are distinct from dogs and roses and statues. And so one way to think about being able to recognize an infinite number of new classes is to approximate that by having a neural net just recognize a very large number of classes. And so if you could recognize, you know, a million classes, for example, then as the generator produces new instances of those classes, maybe the process could like start going out and generating each of these classes. And that allows us to then use supervised learning because we know how to recognize new classes of things. So this is an approximation to the overall goal and to try to see if this system can work. So the way that we wanted to approximate this, and this is all the way back in 2015 before image generation really worked that well, is we said, let's take a deep neural net that is trained on ImageNet, which is relatively new around that time. It has 1000 different classes and it's really good at recognizing these different classes. And then we'll have, we'll use that as our evaluator, which is the generator's job is to generate instances of that class. And then the question is, what are we going to use for this, the green box here, the generator side. So what we need is an algorithm that can recognize either an improvement on a current class, or when a new class is generated. And so we decided to use this algorithm that I'm excited to tell you about, because it has a lot of really interesting motivations behind it. It's called map elites. And it has one bin per ImageNet class. And I'll tell you what map elites is right now. But to tell you about map elites, I kind of want to motivate this whole field of a new kind of type of algorithm that my colleagues and I have been working on. And it starts with this recognition, which is that there's a paradox in life, which is that if you try too hard to solve a problem, you'll fail. However, if you ignore the objective, then you're much more likely to succeed. So imagine that you're in this maze here, and you're starting here, and your job is to get here. And you might say, well, okay, make the robot who's here, make its objective, getting as close as possible to the goal. Well, if you do that, and you get points here, these are all the points that get generated by that search algorithm, because and most of them just go straight north, because that lowers the distance to the goal. But then they just butt their head against that wall forever. This is a classic local optimus, you're familiar with these things in search. However, if you simply switch away from the paradigm of always try to optimize toward a goal, and you just say, let's just go to new places, just seek novelty. That's what you get here. And eventually this search stops focusing on just going north. It doesn't actually care more about north than going east. And eventually it winds its way around, and it solves the problem. And this right here is a metaphor for every single hard thing we want to do in search. If there are local optimal in space, if we need to explore to discover this thing, then we probably should seek novelty more than an objective. And it's even a metaphor for things beyond algorithmic search. It's also a metaphor for human culture and even natural evolution. And the idea is that almost every major scientific breakthrough, if you trace its lineage back, it's not a straight path to that solution. Instead, it's a winding, circuitous route. So for example, if you went back in time centuries and you said, I have this way of cooking food, and what I want is a faster way to cook food that doesn't produce any smoke, then you would never, if you only funded work into improved cooking technology that can accomplish those goals of heating things faster, you would never invent the microwave, which is a magical invention. Because to invent the microwave, you had to have been working on radar technology and recognize the chocolate bar melted in your pocket. Similarly, if you went back millennia to this abacus and you said, that thing does computation, I want more computation. And you only funded researchers who improved against the objective of producing more computation, you might get abacuses with like longer rods, more beads, something like that. But you would never invent the modern computer because to do that, you had to work on things like electricity and vacuum tubes, which were decidedly not produced because they improved computation, although they later proved instrumental to doing that. The same is true for going from this kind of energy to clean energy, where you have to be thinking about things like space and time that were not thought about because they would produce new ways of producing clean energy. So the conjecture here is that the only way to solve really hard problems may be to create problems while you solve them and goals switch between them. And so goal switching is this idea that if you're trying to solve one task, and you make progress on a different task, then you should also start optimizing and getting better on that different task. So if this robot here, this scientist here wants to make a walking robot, and all of a sudden during optimization, the robot starts crawling or starts balancing on one leg, you shouldn't throw that out as a failure because it's not helping you walk or making forward progress. Instead, you should start getting better at those skills to add those to the set of things that you work on. And ultimately, those might be stepping stones to get you to this walking robot. So my colleagues and I have been creating this new subfield of algorithms of AI called quality diversity algorithms. And this family of algorithms is trying not just to get the single best solution to a problem. It's trying to do something very different. It's trying to get a large set of diverse solutions, but where every solution is as good as possible for that type of solution. You want the tallest in the giraffe or the fastest ant, but you don't let an ant who's not that fast kind of get precluded by the fact that a cheetah is faster. You still want the fastest ant and the best ant you can find. So probably the most popular algorithm in this family at this point is this algorithm called map elites, which was invented by Jean-Baptiste Morel, a great colleague and friend of mine, as well as myself in 2015. And it's very, very simple. And the idea here is if you're going to solve a problem, want to first choose or learn, but we started off by choosing dimensions of interest that you find that you yourself like. So imagine if you're trying to make a car, for example, you might choose safety and fuel efficiency as two dimensions of interest. And then you just discretize these dimensions. And you look for the best solution, according to some criteria, like maybe it's the fastest car at each point in this grid. And what you want at the end of the day is not just to get the fastest car possible, but the fastest car for every possible tradeoff between safety and fuel efficiency. So here's an example problem we tried this on. This is generating soft robot morphologies, which is like the bodies of robots. So we gave this optimization algorithm those four materials there. They're kind of voxels that can pulse at different times. And some are soft and some are hard. And we said, you know, go fast. And, you know, first we did this without map elites, we just did this with a canonical optimization algorithm or a genetic algorithm in this case, which is just trying to optimize for speed. And what you see here is this kind of really interesting parade, this Noah's Ark of very different solutions and very different creatures. And, you know, people got really excited when we put this online and it's super fun. But I think one of the things that people thought really interesting about this work, including myself is the huge diversity of designs that you see here. You know, it starts to evoke nature where you see a lot of different designs. The problem is there is a trick to this. And that is that all of the designs that you just saw, each of those came from a different run of optimization. The only way you got a diversity was by starting the run again and doing a massive search to find one solution. But if you look within that population of creatures, they're all almost identical. And that's not what we want. What we want on is an algorithm that will generate a huge diversity of things within one run so that you can run it for billions of years and it would continue to produce interesting new stuff as opposed to converging to one type of solution and getting stuck on that kind of local optimal. So we took the map elites algorithm that I just described to you and we applied it to the same software last problem. And what we did there, you know, is we have to pick the dimensions and we chose to pick the number of voxels and then amount of this dark blue material because previously it hadn't been using this kind of bone-like material and we wanted to see it play with that resource more. And if you look at classic optimization, this could have been RL, but in this case it's a genetic algorithm. Any optimization, what you find is that it doesn't actually search the space very well. And so it has low performing points and it didn't do a lot of exploration. If you add diversity, which we know historically helps, you do get higher performing points. So you see these yellow points here, but it still did not explore a lot of the space, even though it's incentivized to literally explore in these two dimensions. Map elites is a qualitatively different algorithm. It's a sea change in terms of what happens within the algorithm. If you look here, you see this rich exploration where it fanned out and searched the entire search space and it taught you more about this search space. It tells you, hey, there's not very high performing points up here. There's a little bunch of optima over here. There's also this separate little area here that you probably would never have normally found, et cetera, et cetera. I'm doing these interesting points over here that you can go investigate. And what's interesting is it often finds a better overall high performing solution than if you just do direct optimization because it's doing such a better job of exploring the space of possibilities. So if you look at any individual final point, you can trace back its lineage through time to see where those solutions visited in the search space. And what you can see here is that they don't just kind of mine one area of the space and get better and better and better at that corner of the search space, that particular tradeoff between these two dimensions. But instead, the overall lineage takes these long, circuitous paths to their final destination. Just as to get a human, you had to go through an intermediate stage of being a tapeworm and then being like a tree dwelling. Actually, I don't know if we were a tree doubling, but kind of something that looked more like an ape and all sorts of intermediate steps along the way. So going back to the idea of an innovation engine, we now can recognize the algorithm that we're going to use here. There's one final thing I need to tell you about, which is how are we going to encode the images we're going to search for. And I'm going to tell you what I mean by the word encoding, because I think especially for people who are interested in aesthetics, this is one of the most important choices you can make. And you'll see this show up in Joel's work later as well. So I'm going to tell you about the encoding that we use, which is a CPPN. So first, I've been throwing around these terms, genetic algorithm and evolutionary algorithms. You may not know what they are. I'm going to very briefly explain them. If you want to search for a problem, this is also true in deep learning. The first choice you have to make is how to encode the problem. So imagine if you wanted to search for tables. Well, you could decide I'm going to store the length of each leg separately as a number on a parameter vector. We in evolutionary algorithms, we call this a genome, but in deep machine learning, it's often called a parameter vector. So you store the length of each leg separately and the width and the length of the surface of the table maybe on this string of numbers, this parameter vector. Once you've made that encoding choice, you then can score the population. First, you create a population at random by generating random strings of numbers. You score this population to see how good they are. You select which ones are better according to some scoring function, which could be your reward function. And then you just take these things here, take their parameter vectors, and you perturb them in a little way somehow. And then you get a new thing and then you repeat the process. In the gradient-based method, this is kind of like where you take the learning step based on the gradient of the scoring function. And then you repeat the problem. So when I talk about an encoding, it's this first choice, which is how do we decide what is the search space that we will search in the parameter vector and how does that map to the final solution? And that is in evolutionary language, the process of going from a genotype to a phenotype, or machine learning a parameter vector to a final agent or policy or artifact. So there is this notion of a direct encoding versus a generative encoding. And a direct encoding, you basically have one number on your parameter vector for every single thing in your final artifact. So if you're searching for the weights of a neural net, then you search separately for a number for each weight or for a table you search separately for the length of each leg. If you think about how perturbations affect these parameter vectors, though, they are mostly likely to produce non-regular phenotypes. So most changes are not going to lead to a table that has to be flat and hold your coffee. And so that makes kind of a local optimum between this solution and this solution. You have to go through this intermediate thing unless you get lucky enough to generate a regular phenotype. If you have a generative encoding, you reuse information in the parameter vector to produce the final thing. So you might just specify the length of legs once and then reuse that for these four lengths of tables. And now every single change to that parameter vector is going to produce a regular flat table. However, you've lost something. You've lost the ability to express this type of table up here. And so this is like a really, really essential choice when you go to produce any solution with search. So generative encodings, you know, my colleagues and I and many others have been focusing for a long time on why these types of encodings are really interesting. And some of the desirable properties that we want is that you can get regularity, which means you can get patterns in the final artifact. It might be the architecture of a neural net, or here is the hands on your, you know, in your body. And what you see is there's a repeating theme in your hands. That's the regular pattern. But it also has variation. Each of your fingers is a variation on a concept or a theme. And that's kind of one thing that you might want while you search. There are some others benefits here, but I'm not going to get into those. So this is something that I just think is really fascinating to think about, especially if you're interested in aesthetics. And it also ends up being helpful algorithmically. And it's going to factor into a lot of Joel's work, I assume, depending on what he talks about. And this is this question of how does nature build the astronomically elegant, complex creatures that you see in the natural world? Like a question that I'm not sure if you've ever stopped and thought about, but it's a fascinating one to think about is how does every cell in your body know what kind of cell to become? You have, you know, the same software is being run in every one of your cells, the same DNA, yet some of your cells turn into hair cells or spleen cells or liver cells or eye cells. How does it do that? How does every cell know what kind of cell to become? Well, it turns out that nature is using a generative encoding where it reuses information, where the cell fate, which is the type of cell, is a function of its geometric location in the body. It's almost as if the body wanted to know the XYZ GPS coordinates of each cell so that it could tell you, oh, if you're like up here, left of the midline, three quarters of the way up the y-axis, then become a heart cell, for example. So if you look through developmental biology textbooks, what you find is that these kinds of geometric patterns are the lingua franca of developmental biology. So here's this beautiful cartoon by Sean Carroll. So here's your DNA which has these genes on it. And in this developing embryo are currently three different chemical patterns. They're called morphogens. They're literally some protein that's sitting diffused inside this embryo. And if this gene here says that protein A is present and B and C are not present, then this gene expresses and produces a new protein, only where that's true. And so now you've combined these three pre-existing patterns to produce this fourth new pattern. And this might therefore tell the vertebra and a spine that they should turn into vertebra cells. You get this repeating theme down the middle, but only the left half of the embryo. And if you look through that, go ahead. Would I be able to interject real quick? Sure. My research is actually focusing on exactly this same kind of problem, but in mammals. And so in mammals, the morphogen model explains some stuff, but it's actually even more complex. It is much more complex. Everything in nature is much more complex than we know. So I am simplifying here because I'm flying through this material. And not all of the, not, it's not to say that the only thing that's happening is geometric patterning, but it is, basically, I think it's the backbone of the way this stuff gets built. And so by capturing that power and putting it into our search processes, we've gone a long way towards the power of developmental biology. And you could argue that you've skipped out on a lot of the extra complexity that would be very computationally difficult to simulate by doing these things efficiently. Yeah. All right. That's a good point. Cool. Thank you for the question. So getting to the issue I was just talking about, which is how can we efficiently make this sort of a process happen? So what we don't want to do computationally is have, like, diffusing chemicals in some chemical simulator, because that would be really, really expensive. And so Ken Stanley, my longtime friend and colleague figured out, is that you can actually abstract a lot of the power of this system without any of the underlying chemistry and in physics in these things that are called CPPNs or compositional pattern producing networks. And the idea is, is just like in nature, we're going to encode phenotypic elements as a function of their geometric location. So here's how it works. You take a thing that you want to optimize. This could be a neural network, it could be a robot morphology, it could be a picture. And you provide coordinates for everything in the artifact. So imagine it's easiest to think about pictures. So imagine you give every pixel an x, y coordinate, then you literally pass the number, then those numbers into this function. So first you put in one, one for this pixel, and then one, two, and then one, three. And you ask the genome as a function of those two numbers to spit out the value at that location. And if this is a random function, then you're going to get a random picture. But if this function here has mathematical functions that, you know, have regularities in them, then you're going to get a regular artifact. So for example, if you want left-right symmetry, you can pass the x-axis through a Gaussian here, and then everything downstream of that Gaussian node will have left-right symmetry. Similarly, you could have in the y-axis, if you wanted a repeating theme like segmentation, you could pass the y through a sine function, and then everything downstream of that node will be regular in that way. You can also add in linear things. You could say, I want to follow the sine, but only add in a linear component, so like shift it or warp it or bend it in certain ways. So you can mix and match asymmetric and symmetric and repeating themes to produce arbitrary complexity using these geometric functions. And kind of what was really amazing at the time, because image generation wasn't working very well, was the kind of images that would pop out of these systems. So all of these images here were produced on a website called Pickbrier, where humans manually choose which ones they find interesting, but the underlying encoding is a CPPN. And Jill's going to tell you a lot more about like a modern version of this website. So these images here are all encoded with CPPNs, and what you can see is very, very natural like shapes, like things like left-right symmetry, repeating motifs, and the lineages as you kind of permute and mutate these things. You go from a butterfly to a bat with these kind of beautiful gradations and interpolations that are nice to see. Myself and my postdoc advisor, I took the same exact idea and we just put it in three dimensions, and what you get are these nice three-dimensional shapes, which also show a lot of these regularities. And then we went off and we built this website called endlessforms.com, where you can go on, it's basically Pickbrier but in 3D. You can take an individual shape and you can say, I want to further evolve or optimize that shape. Let's see if this plays. Here, for example, you might take this lamp and you are presented with a bunch of variants on the lamp, and then you pick the one that you like and you see the next generation and you can kind of crawl through three-dimensional lamp space. And importantly, if you find one that you like, then you can publish it to the website and other people can pick it up and branch off of that. This is how you get that growing archive of stepping stones that allows us to produce kind of an interesting exploration of the space. Here are some of the other designs that popped out of this system, and here's kind of repeating segmentation, left-right symmetry, radial symmetry, and mostly a lot of the things just look really natural and interesting. So this is kind of a fun aesthetic space to be playing in using these CPPNs. Because we could, we 3D printed the objects and allowed users on the website to 3D print them, so it's kind of fun to hold these things in your hand, and you can therefore help people who have no knowledge of CAD and design to produce arbitrarily complex images and then 3D print them for whatever they want, like a chessboard or something. So when we put this out there, people really found this interesting, which I think just goes to the to the fact that if you can automate the design, if you can help people produce really interesting things that they're curious about and they find exciting, but eliminate all the technical barriers to doing so, then people get really excited about those tools, and Joel's website as a, you know, GAN breeder is a testament to that as well. So going back to the overall scientific question here, which is can we use this to create an open-ended algorithm? Now you know all the pieces of the puzzles. So we're going to have AlexNet, which is an early image net network that was quite good at the time, be able to recognize a thousand different classes, and then we're going to have an optimization algorithm that's going to generate these little tiny CPPN networks that are trying to produce images that light, that the DNN, the deep neural net, thinks represent, you know, are classified as each one of the thousand bins in image net. So the idea hopefully is that you'll get goal switching. So if one of the networks is the best dog we've ever seen, or particular dog, and then a permutation on that produces the best fish we've ever seen, then now that network can go to hop over to that bin and start optimizing to become a better fish. And maybe that produces a better stepping stone for a cat and then a bird, etc. And the hypothesis that we wanted to test is, is that better than separately optimizing for each one of the bins in image net? So here is the performance over time. Time here, training goes from bottom to top, and the category of thousand image net classes are along the x-axis. What you can see is that over time performance rises with training all the way up to one, you know, red in most places, which means that the deep neural net is certain that this thing is a lion, and this is a starfish, and this is a guitar. So my question to you is, knowing that the deep neural net thinks that each one of these things is in that category, you know, what do you think they look like? And if you had asked this question in 2015, 2016, people would have said they look like electric, you know, starfish and guitars, but you probably now, because you guys are, we've had the benefit of a few years, you probably are used to the idea that what you do, what you get is not that, but you get these things that are called fooling images or adversarial images, which is to say that the deep neural net is absolutely certain that this is a starfish, and this is a peacock, and this is a king penguin, and this is an electric guitar, even though they obviously are not those things. So at the time, this was a, this, we published this paper, deep neural nets are easily fooled, and it was a really big wake-up call to the community that AI sees the world differently. There are huge security concerns here, and this generated a tremendous amount of discussion and awareness amongst the scientific community, the machinery community, and also the broader public about the fact that these new tools that we're building have a lot of deep flaws within them that we need to worry about. Nowadays, everyone's very familiar with adversarial images. At the time, this was not very well known, and so I thought that was interesting. However, I also think from an aesthetic perspective, it's interesting that we were trying to generate innovation engines and generate images. We weren't trying to study neural nets and whether they had flaws, and then this just kind of popped out, so I thought that was an interesting story. But while some of the images didn't look anything like the categories of interest, another thing that we found interesting is that many of them did, and from an aesthetic perspective, this is pretty cool because now you're getting an automated art generator. So for example, matchstick, television, and bagel, they pretty much do look like those things. However, I also think from an aesthetic perspective that some of these really evokes an artistic interpretation of what that abstract platonic concept represented by that class is. For me, this image of a prison cell evokes more than just a picture of a prison cell. It seems to me like an artist decided to represent the bleakness but also the hope or something about this prison cell. And so even though there is no artist that was trying to capture that behind here, there's a neural network that's kind of captured the platonic concept of a prison cell, and that somehow leads to its own dialing in of what is central and essential about that concept, or at least evokes those kind of reactions in us and allows us to explore potentially new types of artistic and aesthetic connections to concepts. So if you look through the diversity of the images that were generated, I do think this kind of really hit the mark in terms of a quality diversity algorithm. You've got this huge set of images as all comes from, you know, one run. And at least I'm not, I think that they are, they might have been pulled from a couple of different runs in this case. But each one produces this giant, this diverse set of images, and many of them I think are really aesthetically interesting, like I think this volcano or this beacon, or this cup, I could actually imagine a coffee shop where this is this logo, your comments on a mask and a banana, etc. So we really, really thought it was cool to see kind of this pop out of an automated system back in 2015. Scientifically, we're also really interested in like whether or not goal switching was playing a huge role in these networks. And so we have, if you optimize for a single class only, like the water tower class, what we see is that you do indeed get stuck on a local optima. It lands on this particular pattern really early in the run. And then it just does minor tweets on that idea and gets stuck on it until eventually it kind of maxes out what you can do in that corner of the search space. In contrast with map elites, what you see is that early on it locks on this half dome moon image, and it does okay, but then it kind of gets stuck. And then from a totally different class, something that happened to have been produced to for the beacon class, actually ends up looking like a better water tower and goal switches in, it invades this class. And then with further optimization to look like a water tower ends up making the DNN think with 98% confidence that this is a water tower. And you can kind of see why. And we see this lesson over and over and over again. There's many goal switches happening within this population of networks. And we think that's a big reason why performance is much higher than when you optimize for a single class. So what's really interesting about goal switching is that it allows what what are what biologists call adaptive radiations. So you come up with a good idea like maybe a more efficient way to metabolize oxygen in one lake in Africa. And then that idea will spread to all of the surrounding lakes in Africa. And then on top of that technological foundation, those fish will respecialize to their particular niche and adapt that innovative incorporate that innovation. The same thing happened with Darwin's finches, which radiated out from one from one couple of finches to all of these diverse finches. And we see the same thing in technology where computers, for example, were invented for one purpose and then kind of spread throughout an ecosystem and are now embedded in all sorts of technological devices in our lives. So what's really nice is you can see these adaptive radiations happen in these quality diversity algorithms. So this is one of my favorite plots from all of the science I've done in my entire career. Inside one of these innovation engine runs, you've got this early innovation, which is this dome against a background, a colored background. And that thing, which looked up the abaya class, then radiates out and it's children because this is a population. So these literally are descendants of each other. It's descendants kind of produce a phylogenetic tree, just like we see in nature. And ultimately, this innovation turned into a volcano, a mosque, a water tower, a beacon, a yurt, a church, a planetarium, an obelisk, and a dome. And it's just awesome to see an innovation then get rid of that concept, get rift upon and kind of radiate out into a huge explosion of diversity. So if you study the history of biology, you'll see that there were many moments in the history of biology where something similar happened. We got like, you know, single multicellular organisms or rate or bilateral symmetry or the four-legged body plan. And then you see this explosion of diversity that descends from that central innovation. So I think it's beautiful to see that happening inside of our algorithms. We ended up submitting the art that was produced by this algorithm to a competition at the University of Wyoming where I was a professor. And every year, art students work for a year and they submit their best project to this competition. And then there's a judges who decide which of them get hung on the wall and accepted into the competition. So we did not tell them this is AI-generated art, we just submitted it. And not only was the art accepted, it was also given an award. So here you see people having wine and cheese. And I was like eavesdropping as they're discussing the intent of the artist behind producing all of these different images, not knowing that it was an AI algorithm behind it, which I thought was pretty cool. So in some sense, this passed the artistic turning test. Sample size one. FYI, in case you're interested, there is much more work on CPPNs that are more modern. So nowadays, a lot of people are playing with differentiable CPPNs instead of using evolution. I have to because it's so beautiful. Quickly look at the work of Alex here, which I highly recommend you check out. All of these things here are different CPPN represented networks that are doing deep visualization, which is the technique I'm going to tell you about later. So I encourage you to check that out. There's also, you can use CPPNs to encode neural networks. I did that a lot in my dissertation and now you can do that with Backprop. David Ha has been pushing that and there's much more work in this vein. Okay, so getting back to QD, I think that I hopefully have convinced you that it has all of these nice properties, like a diverse set of high performing solutions that it produces, it has goal switching, and it allows you to kind of illuminate the entire search space and learn a lot about what's possible. Just quickly, I want to say that these ideas really have given us a lot of leverage on hard technical problems. So in this paper that we had in Nature, we use these ideas to have robots that could adapt to damage within one to two minutes to get up and continue on with their mission, even if they're extremely damaged. And then we also use these ideas behind the algorithm GoExplore, which you may have heard of, which completely solved the Atari benchmark suite, including solving really hard exploration challenges like mono zoom as revenge and pitfall. You can see all the previous attempts to solve this heartless game, which became kind of its own grand challenge of the field, do not perform very well. And then this is the difference once you start adding in these ideas from quality diversity algorithms. Ultimately, we ended up beating the human world record on this game. Oh, and as a quick little teaser, this paper was also recently accepted to a really nice journal. I can't quite tell you which one, but if I'll share that information on Twitter in the next couple of weeks, if you are interested to get the final version and the updated version of this paper. So I think QD algorithms are really interesting. I think the question that we should always ask though is what's missing where, you know, they're not yet open-ended algorithms. So the thing that I think is missing is that while these things can produce a large diverse set of interesting solutions within one domain, ultimately, their ability to innovate is constrained because they're stuck in this one particular setting that we put them in. But what we really want is these open-ended algorithms that just keep going and kind of generating wildly different solutions as they run. So traditionally in ML, we pick a particular challenge like Chester, Gro or Dota or Starcraft and we bang away on it for a while. But the intriguing possibility that I want all of you to consider today is could we create an algorithm that generates its own challenges and solves them? Just as nature arguably created the challenge or the opportunity of leaves on the top of trees, and then the solution to that challenge, which is giraffes or caterpillars that can eat them. So, you know, this kind of a thing might produce something that's interesting after a billion years. So our most recent work on this is in this algorithm called Poet, which is the paired open-ended trailblazer. And the idea here is that we're going to try to endlessly generate interesting, complex and diverse learning environments and their solutions. So the idea is again quite simple and you'll recognize it. It's basically we want to generate new learning environments and we're going to add them to this set of our population of environments if they're not too easy and not too hard for the current population of agents. And if they're novel, there's something about them that's unique and different. And then we'll optimize agents to better solve each of these challenges and we'll allow goal switching between them. So the example task that we used here is obstacle courses. So this little creature here has to run as fast as possible without falling over. And here's the general idea. You start with an easy environment. So first you have to make that encoding choice. How are you going to encode an environment on a parameter vector? Here we have things like the number of whether or not there are gaps, whether or not there are stumps, the ruggedness of the terrain, et cetera. So you can start with an easy one of those, which is maybe just flat terrain. And then you start having an agent, which has its own parameter vector. This is a neural network and is learning via RL to solve this task. And once it gets good enough on that task, then we copy phi 1, the parameter vector of the environment, to make phi 2. And then we'll try this agent via transfer and goal switching. It goes and it starts optimizing here. Now, we are simultaneously continuing to optimize this parameter vector on this environment and this parameter vector on this environment. We keep going. Maybe eventually this environment gets solved well enough by this parameter vector. So we copy it and we now make phi 3 a new environment. Turns out that's too hard for either theta 1 or theta 2. So we throw that out. We generate, we try again, we get a new environment and we test this one and this one. We take the better of those to you on this new environment to seed training. And in this case, it was theta 2. So it goes in there. This does not have to be a linear chain. At any point, any one of the environments in the set can produce a new environment. And then we'll try all of the current agents on that environment to see if they're the best and if they are, they get to start. And the process can keep going like this. Now, imagine eventually we generate a really, really hard challenge like phi 6 here. And initially the best parameter vector, we try all of them on this environment was theta 5. It was the best stepping stone. So we start optimizing a copy of theta 5 in this environment and it gets better and better and better. But it maybe hits a local optimal and it can't break through and really, really do well on this environment. But in the meantime, we're still optimizing theta 4 on this environment. Maybe it has an innovation that makes it better on this environment. So it invades this environment, just like a species in nature could invade a new niche, kicks out that parameter vector. And now we start building on the back of this innovation here. And then that maybe with a little bit more optimization comes up with an innovation that then transfers in and becomes the best thing we've ever seen on phi 6. And maybe that gets us off the local optimal and solves that problem. So that is kind of the nature of goal switching. So here we use evolution strategies, but any RL algorithm would work. And you can see this little agent here. And it is traversing this course. And what you can see is at the beginning, all of the challenges are quite simple. They're a little tiny stumps, little gaps, just a little ruggedness in the terrain. But over time, the agent gets better and better. And the environments automatically start getting harder and harder. So it's kind of like a natural curriculum generation. And you can still, the algorithm is here is kind of still pushing in separate dimensions, like taller gaps or more ruggedness or wider gaps. Sorry, I didn't tell her stumps. Later in time, with more training, the algorithm starts to put together these challenges. Sorry, my dog is barking. So you get things like bigger gaps and stumps and ruggedness all put together. And ultimately, these environments get really, really, really difficult for this little robot to traverse. Here's another challenge that was invented and solved by this algorithm. So I think from an aesthetic point of view, it's kind of cool because you can think about each one of these robots as its own little creation. It's kind of a curiosity. Just like animals in the world, we love to watch nature shows and see different animals and how they're different and what they can accomplish and how their bodies are different, et cetera. So you can kind of think of the agents produced by these things as really interesting aesthetic artifacts. Scientifically, we wanted to see whether or not goal switching in this domain was paying off. And so we did direct optimization in each one of these environments and found that it failed miserably. That's down here. And with Poet and the goal switching, what you see is much, much better performance in each one of these environments. This is the only way that we know of to go solve these hard problems. And in the paper, there's more of a detailed study about that claim if you're interested. So I want to show you one anecdote of what popped out in the system. So I think it's so interesting. So here in the simplest possible environment, a flat ground, you get this agent here that is optimized for a really long time and it's got this knee-dragging behavior. And eventually, the system generates a permutation of this environment, which is a harder challenge that has little tiny stumps. And this knee-dragging behavior is not very good because it keeps tripping up on these little stumps. So with some more optimization in that environment, the agent learns to stand up and it gets faster at that. Now, because the algorithm is always checking any solution to see whether or not it's better at invading some other niche, this descendant actually goes back automatically and invades that flat ground, replacing this knee-drager. Now that it knows how to stand up, as you can see here, it gets much better performance in that new environment. And then with further optimization, it ends up with much better performance. Now, because this is a computational system, we could do the counterfactual. We went back to this original agent in the top left and we optimized it for an equal amount of computation in that flat ground environment. And it just never learns to stand up. It's just on a local optimal and it's stuck in its ways. It was only by going into a harder environment and coming back that it learned a better behavior and a better strategy. And this is why I think that it's so hard to design curricula. You would never, as a human, say that you're going to take something to a harder environment just to have it solve a simpler environment. But in this case, that's exactly what was needed to solve this problem. So we go through a quantifying algorithm that goal switching is essential to solve the hardest challenges generated by this system. So future work in this domain, I think there's all sorts of stuff you could do. Obviously, you could just take it into more complex rich simulators. So, you know, you could have more complex encodings as well. But here is like the world's from deep mind. But I think it's really kind of pumps my intuition is to watch, you know, what's possible, what will be possible in the future with more computation. Like imagine what Poet could do in a world this complicated, where it has to do with flying creatures and climbing and talking to other agents, maybe negotiating trades in a market, you know, and if you were doing all of this, you know, what might pop out of the system, I think it's fascinating to consider, both from a static perspective and from a machine learning perspective. You also could optimize the bodies of the creatures themselves. So in the bottom in the right, you see, you know, I showed you some work that we did in that vein a while back, but not with Poet. And David Ha has done that in particular environments that are handcrafted. But imagine if you paired body optimization with environment generation, then you could really get weird things like you see in nature, where you have a particular kind of like cave dwelling spider that's optimized to that environment, which is very different from birds that are flying up in the Pacific Northwest. So another thing that I think would be interesting would be to combine innovation engines with modern tools. So imagine if you took something like Dolly, which is this amazing new thing produced by my colleagues here at OpenAI. And not only did you have humans asking for particular innovations or particular images from Dolly, but you have the algorithm invent the challenge and the solution. So the challenge could be, you know, can you create this? Can you create this? Dolly would then create them. And if they're interesting, you add it to a set. And then you have something that looks at the set of things that are already produced and produces completely new types of images. That would be awesome to see. And that doesn't have to be limited to images. You could use then the same technology to do it in different modalities, such as videos and music and poetry or algorithmic space. Again, the challenge that remains is how do you detect what's interestingly new? I'll throw it out there that I think you probably with a lot of data could learn a function of what humans consider interesting. In fact, if Joel remembers, I sent him a giant email saying that I think we should do this with his website, GanReader. We haven't done it yet, but it'd be a great project for a student to take on. So I want to quickly check the time here. Yeah. So we started a little bit late. So I'm going to race through this because I think you'll find it interesting, but I won't be able to go into any detail here. But part two of the talk, which I'll do very quickly, is I wanted to tell you about this entire other arc of research that we did called AI neuroscience, which is how much we want to study. Just like neuroscientists try to study the human brain, we want to study how much the deep neural nets understand about the images that they classify. So we're all familiar with deep neural nets, but they tend to be a black box. We don't really know what each neuron in the deep neural net does. But one way neuroscientists probe this question is they literally put probes into your neurons and they look for which neurons light up in response to which images. For example, they found neurons that light up in response to Kobe Bryant or Bill Clinton, for example. And people have called these things like a Kobe Bryant neuron, for example, and they respond to very different modalities, such as the name Kobe Bryant, a line drawing him in the Lakers uniform. The question is, you don't really know just because the response to those images, if it's a Kobe Bryant neuron, it could be an LA Laker neuron instead of a Kobe Bryant image neuron, for example. So we thought the ideal task would be to synthesize the images that maximally activate that neuron. And if you did that and you got these images, then you'd know, oh, that's a Laker neuron, not a Kobe Bryant neuron. But if you got these images, you'd know it's a Kobe Bryant neuron. So this is actually possible with artificial neural networks, but you can do is you can take a neural net and then you could have like an artist, an AI artist that's trying to generate an image to activate this particular neuron here. And what you can do is you can use backprop. So the artist generates an image and then you just follow the gradient to increase this neuron until you get an image that lights up that neuron, and it might look like this. And you can do the same technique for all the intermediate neurons in the neural net. We call this deep visualization. Our first attempt at this actually was that same paper, deep neural nets are easily fooled when we did it with CPPNs here or a direct encoding on the left here, or with backprop on the right, we got images that did not look at all like things that they're supposed to, but the neural net was perfectly sure is a peacock or chimpanzee. And you know what happened with that paper. We then went on and started asking questions like why are these neural nets easily fooled? And I don't have time to get into a lot of the details here, but what we basically thought is that maybe deep neural nets do recognize the images they're supposed to like a lion or a dolphin, but maybe they recognize a whole lot of other things also as in that class unnatural images. So if we could stop the artist from generating unnatural images and only stay to the space of natural images, then we might find out what that neuron really is for and what it's been trained to see within the space of natural images. So skipping over some of the details here, the fooling work started out saying maybe these deep neural nets don't really understand at all what they're classifying. They're just locking on to spurious correlations like that there's a orange texture. If you see orange, you know, this kind of orange texture next to blue color of starfish, but they never learned like what a five-legged starfish is because they didn't need to to solve the problem. We wanted to see whether or not there is that notion of like a five-legged starfish in the network. So in take two, what we tried to do is we added more manual priors to try to constrain the image generator, the artist, to generate only natural images. And when we add that extra constraint, then we get images, you know, previously people had done that and they kind of looked like this. You start to see dumbbells and dolomations. These are the ones that we got with slightly better priors. And you can start to see that the network does actually kind of know what a flamingo is or a beetle. It's an interesting historical side note. These images here in this work inspire deep dream, which is also done by Alex over at Google. And that stuff is super cool if you haven't seen it. And then third take, we tried to add even better priors, manually designed priors, and what you get are these images here. And I want to stop for a moment and kind of reflect on this from an aesthetic perspective. We're trying to do better and better science. We're creating different algorithms or different hand-coded priors to kind of accomplish the scientific quest. But if you look at the different images, each one of them has a different style. And I think it's kind of interesting that like slight tweaks to algorithms produce wildly different artistic styles. It's like all these different artists are out there and you just kind of are searching through the space of artists kind of accidentally while you're doing your science. So this style is very different from this style. And I actually think this is just really beautiful. Like if I saw this in an art museum, I would think that this is beautiful art, even though it was produced purely for scientific reasons and we had no intention of producing images in this style. We then went on for one more take at this. We tried to say, okay, we're machine learning researchers instead of manually encoding what characterizes a natural image. Let's learn it. And so we start learning the natural image priors and our papers are full of lots of details on this. And the way that we do this, we have a generator kind of like the generator in a GAN. We hook it up to the target network we're interrogating. And then we try to search in a latent code to produce an image that activates a certain neuron in question. And when we did that, we got these images, which at the time were some of the most realistic images deep neural nets had ever produced. You were seeing realistic lawnmowers and lemons and barns and candles. These images are not great by modern standards, but this is 2016. Here are other images in this class. And for the first time ever, the images were starting to look photorealistic. Like these are the synthetic images for this class. And these are the real images. And, you know, I don't think that you would really be able to tell the difference if I had swapped them unless you look very carefully. So compared to the best work at the time, which is on the left, these images were a big step up. And they helped us confirm this hypothesis, which is basically, if this is the space of natural images, these networks do understand what it means to be a lawnmower. Like if this blue line here is the class of lawnmowers, then they do stay to, if you keep them in the natural, if you only generate images in the natural image space, then you do get a lawnmower. But if you let it generate images anywhere in the space, like all the way out here, then it also, the network will similarly say this garbage here is in the class of what it means to be a lawnmower. And so if we want for aesthetic purposes to have neural nets generate realistic stuff, we got to get it focused on something that both is natural and activates the network's classification as opposed to way out here. And GANs do this also, but they do it via a very different mechanism. So I told you, you could look at each individual neuron within the network. I don't have time to go through this now, but if you're interested, then I encourage you to kind of go into the paper and look, you could kind of fly around the neural net and see that you get things like cargill grill detectors and buckets and bird heads. And as you go up in the network, you get these really weird concepts like one-eyed turtles and like arches over water until you eventually get the class neurons where we know what they are because we've grounded them via our labels. The one final thing I'll mention here is that the one problem with our technique is that it generates very, very little diversity. So these are synthetic images produced by our network for this class. And they look a lot like the images that most highly activate that neuron from the real world from the real data set, but they don't represent the diversity of images in that class. And so we did a lot of work, including adding with Yashua Benji on these things called plug-and-play generative networks, where we wanted to add a lot more diversity. And so you could take the same network and you can light up a bunch of different classes that it's never even seen before. That's a bit of an aside like ballrooms and art galleries, but mostly we were interested in getting more diversity. And the takeaway message is we were able to accomplish that. So here is PPGNs, which is the one that has more diversity. And you can see a much more diversity in this set of images versus DGNAMV1, which are the images over here. And this diversity better represents kind of the diversity of the natural class. So with the original attempt, DGN, you got volcanoes that look like this. It kind of goes and finds one type of volcano, like a local optima, and it sits on it. But the plug-and-play generative networks are much more kind of like an open-ended algorithm, at least within this class, where it samples new versions of volcanoes over and over again. And so you get all this big diversity of volcanoes out of this new sampling technique. So to conclude this, the AI neuroscience part, I won't actually get into these details, but it taught us a lot about what neural nets, you know, what's going on inside neural nets, it taught us whether or not they really recognize and learn about the concepts in our world. Like we did find in the end that they do know what a volcano is, and you know the five-legged nature of a starshow, and what a lawnmower is, even though they also are susceptible to producing and recognizing these adversarial fooling images as being part of the class. And it was cool to see the rapid progress just within my own team of collaborators from 2015 to 2017. And since then, I highly recommend the work of Chris Ola, who's continued to push in this direction. And very, very soon, Gabriel Go and Chris and others have new work coming out of OpenAI that will blow your mind. So I encourage you to watch the OpenAI blog in the coming weeks for this new result that you really like. You could do all of this stuff in different modes, like speech and video, etc. I won't dive into this. I want to just highlight one thing. This is my future work slide all the way back in 2016, and I thought it would be awesome to do this with real animal brains. Since then, actually, somebody has done that. They took our algorithm for DGN, they applied it to a real monkey brain, and they synthetically are generating images that activate neurons within the monkey brain, specifically within the face recognition part of the monkey brain. And you do in fact get a synthetic monkey-looking face, which is pretty amazing. So to conclude my overall talk, I think innovation engines are really interesting because they kind of push on this question of can we automatically produce an open-ended creative process that in any sort of modality like art or music or invention will just endlessly generate interesting new things. We've got a long way to go to accomplish that goal, but my colleagues and I, like Ken Stanley and Joe Layman and myself are really, really focused on this goal and trying to pull that off, including now at OpenAI, where all those people are. And I also showed you very, very quickly some of our work in AI neuroscience, which we were doing for scientific reasons, but produce these interesting aesthetic artifacts. And I'll just leave you with one final thought, which is that I find it surprising how often science produces aesthetic artifacts. Almost none of the work that I was doing was trying to do it just for aesthetic purposes, but along the way, it produced these things that I think are beautiful and interesting, and could be kind of aesthetic artifacts in their own right. And so I think it's nice because you don't have to choose between being an artist and a scientist. You kind of kind of can do both nowadays, especially with the modern tools and machine learning. And I'm sure that's kind of a realization that is being reinforced over and over again with all the different lectures in this wonderful class that you are participating in. So with that, I want to say thank you, and I'll turn it over to either questions or Joel, depending on what you want to do, Ali. Thank you so much. I appreciate it. It was really interesting and inspiring to me, and I'm sure for many of us in this class, this comment that you also made about science and creating and art, I think that it also is very well aligned with some of the other insight that we learned from other speakers. For instance, Alyosha, of course, was mentioning that when I asked this question, he was mentioning that he also thinks that, you know, creativity is a different tier of our evolution. So that really resonated with me what you were talking today. And I think that this is very exciting for us. One question that I have is if students want to, because this is a very interesting topic, and especially that type of poet or open-endedness area, if a student wants to join you in this sort of mission, what do you recommend to them to work on? Yeah. So one thing I would recommend is we had an ICML tutorial, I think about a year ago, that really covered a lot of this work in more depth. It's an ICML tutorial on population-based methods. So then you can see, can Joel and myself kind of going through, this is Joel Lehmann, not Joel Simon, going through a lot of the work that we've done in this field. And I recommend reading a lot of the work of both Ken and Joel, as well as you can look into some of the work that we've done in this area. And then in terms of what I recommend you work on, there's so many things, it's like, there's so many options that it's fun. You could apply a lot of these algorithms in a new domain, for example, that you find interesting, you know, a new kind of art. You could take more modern tools that work really well and weave them into these ideas, or you could invent new ideas, you know, like I still think if people, if anyone out here can crack the question of how can you automatically recognize newly interesting things, that I think is like a Turing award-winning innovation that will catalyze and propel so much algorithmic advance, including potentially advancing our push to artificial general intelligence. Like that might be one of the key stepping stones that gets us there. So I have this paper called AI Generating Algorithms, which I recommend people check out if they're interested. And it basically talks about how these sorts of ideas may do the fastest path to produce general AI. So that's not an aesthetic quest, it's more of a scientific quest. And but if you're interested in that, I think that's fascinating. But I also just think, just literally take all these ideas and go like do Poet, but do it in some totally wild and crazy different domain, or do an innovation engine in, you know, like architecture or poetry and see what happens, you know, you can use new tools like GPT-3 or Dolly, etc. So I think there's just a lot of low-hanging fruit here to be explored. Certainly. And that also reminds me of what you mentioned, we didn't optimize to create a microwave. We explored different things and I think that your advice is quite in that direction. Also, Joseph has a question. Joseph, would you like to ask it yourself or? I wasn't able to get my mic working earlier. Let me just fix that. Hi, Jeff. I was just wondering if you've explored anything on Poet in multi-agent settings to this point. Yeah, the short answer there is that we have a lot of really exciting ideas. For how we want to take advantage of that. I can't share those specific ideas because we may or may not be working on them. But I also think in the spirit of the talk that the best way to make advances is to have a community of people with different ideas pushing different directions because you never know what's going to unlock. So I almost don't want to give you too many ideas either because I don't want to cause conversion thinking. I think it's almost better if there's so many different ways you could apply the concepts of Poet to multi-agent settings that I don't think you can go wrong. I think if many different people and groups push on that, really good things will happen. Fair. We also may or may not be working on that, right? The one thing I wanted to ask you specifically about that is whether you figured out one, maybe you can't tell me, but any way to get around the problem where in multi-agent settings, sometimes you don't have a single evaluation metric that correlates the environment difficulty with agent performance because you add more agents in, well, then the performance goes down because there are more of them and they're all doing smarter things. So that sort of thing has thrown a wrench in the whole annex measure. Yeah, that's right. So one of the things you could switch to is a notion of agent versus agent. Like if an agent is as opposed to doing better on that environment, it's that agent versus other agents or agents that have come before. Another thing you could do is you could switch to more of a learning progress metric, which is if they're getting better, are they learning? According to some measure, like does their value function, their prediction of how well they're going to do, is that wrong? It's because they were either better or worse in that situation and versus those opponents than they thought they were, and measures like that could really catalyze, recognizing this is still an interesting environment because they're learning. This is still an interesting matchup between this opponent and this opponent because they're learning. I mean, we've been actually trying to do something very similar there. It still seems to run into the same sort of issue though, right? If you can't measure absolute performance, it can still be difficult to then measure relative performance because your reward peak can be going down even if you are learning because so are all the other agents. Sometimes you have to run as fast as you can just to remain in the same place. Yeah, pretty much. That's the red queen quote from Alison Wonderland. Yeah, these are all challenges and it's the kind of challenge that happens once you get into the multi-agent setting. So I think this is just for a lot of experimentation and hard thinking has to happen. I don't think there's a really super, short, easy, obvious answer. It's just going to require research. Well, I mean, I look forward to seeing what setting it is that you're trying that out in whenever that gets published. Likewise, yeah, with your work. Thanks. Excellent. Are there questions? Any more questions? Guys, don't be shy. If you have questions, just go ahead. Of course, if Jeff has time. I have time. I just also want to be cognizant of Joel and giving him his proper time. Excellent. Okay, cool. Thank you. All right, then let's thank you again, Jeff. It was really, really interesting and inspiring.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.76, "text": " Hello, everyone. Welcome to your course, Deep Learning for Art, Statistics and Creativity.", "tokens": [50364, 2425, 11, 1518, 13, 4027, 281, 428, 1164, 11, 14895, 15205, 337, 5735, 11, 49226, 293, 11972, 4253, 13, 50702], "temperature": 0.0, "avg_logprob": -0.3358898616972424, "compression_ratio": 1.4435483870967742, "no_speech_prob": 0.008650514297187328}, {"id": 1, "seek": 0, "start": 6.76, "end": 13.14, "text": " Today, we have two special speakers. First, we serve as Dr. Jeff Klun, who is an associate", "tokens": [50702, 2692, 11, 321, 362, 732, 2121, 9518, 13, 2386, 11, 321, 4596, 382, 2491, 13, 7506, 16053, 409, 11, 567, 307, 364, 14644, 51021], "temperature": 0.0, "avg_logprob": -0.3358898616972424, "compression_ratio": 1.4435483870967742, "no_speech_prob": 0.008650514297187328}, {"id": 2, "seek": 0, "start": 13.14, "end": 20.44, "text": " professor in computer science at the University of British Columbia and also a research team", "tokens": [51021, 8304, 294, 3820, 3497, 412, 264, 3535, 295, 6221, 17339, 293, 611, 257, 2132, 1469, 51386], "temperature": 0.0, "avg_logprob": -0.3358898616972424, "compression_ratio": 1.4435483870967742, "no_speech_prob": 0.008650514297187328}, {"id": 3, "seek": 0, "start": 20.44, "end": 28.64, "text": " leader at OpenAI. And he's going to talk about towards creating endlessly creative,", "tokens": [51386, 5263, 412, 7238, 48698, 13, 400, 415, 311, 516, 281, 751, 466, 3030, 4084, 44920, 5880, 11, 51796], "temperature": 0.0, "avg_logprob": -0.3358898616972424, "compression_ratio": 1.4435483870967742, "no_speech_prob": 0.008650514297187328}, {"id": 4, "seek": 2864, "start": 28.64, "end": 35.4, "text": " open-ended innovation engines. I think this is a very exciting direction because so far", "tokens": [50364, 1269, 12, 3502, 8504, 12982, 13, 286, 519, 341, 307, 257, 588, 4670, 3513, 570, 370, 1400, 50702], "temperature": 0.0, "avg_logprob": -0.1475705797710116, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.0066693928092718124}, {"id": 5, "seek": 2864, "start": 35.4, "end": 43.0, "text": " we have talked about the interaction between art and AI. We said that how AI can help us", "tokens": [50702, 321, 362, 2825, 466, 264, 9285, 1296, 1523, 293, 7318, 13, 492, 848, 300, 577, 7318, 393, 854, 505, 51082], "temperature": 0.0, "avg_logprob": -0.1475705797710116, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.0066693928092718124}, {"id": 6, "seek": 2864, "start": 43.0, "end": 50.120000000000005, "text": " to create and express ourselves and democratize the creativity in a sense. But also, the other", "tokens": [51082, 281, 1884, 293, 5109, 4175, 293, 37221, 1125, 264, 12915, 294, 257, 2020, 13, 583, 611, 11, 264, 661, 51438], "temperature": 0.0, "avg_logprob": -0.1475705797710116, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.0066693928092718124}, {"id": 7, "seek": 5012, "start": 50.12, "end": 62.239999999999995, "text": " direction is how our creativity can help us create better AI. For instance, how we learn", "tokens": [50364, 3513, 307, 577, 527, 12915, 393, 854, 505, 1884, 1101, 7318, 13, 1171, 5197, 11, 577, 321, 1466, 50970], "temperature": 0.0, "avg_logprob": -0.17894667670840309, "compression_ratio": 1.3863636363636365, "no_speech_prob": 0.015637630596756935}, {"id": 8, "seek": 5012, "start": 62.239999999999995, "end": 71.84, "text": " by creating, how we define problems and find solutions for them and generalize to solve bigger", "tokens": [50970, 538, 4084, 11, 577, 321, 6964, 2740, 293, 915, 6547, 337, 552, 293, 2674, 1125, 281, 5039, 3801, 51450], "temperature": 0.0, "avg_logprob": -0.17894667670840309, "compression_ratio": 1.3863636363636365, "no_speech_prob": 0.015637630596756935}, {"id": 9, "seek": 7184, "start": 71.84, "end": 80.64, "text": " problems and so on and so forth. So today is one of those, I would say, a realization of such a", "tokens": [50364, 2740, 293, 370, 322, 293, 370, 5220, 13, 407, 965, 307, 472, 295, 729, 11, 286, 576, 584, 11, 257, 25138, 295, 1270, 257, 50804], "temperature": 0.0, "avg_logprob": -0.1836491146603146, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.014932164922356606}, {"id": 10, "seek": 7184, "start": 80.64, "end": 90.48, "text": " great idea that you will see as a gist of what Jeff has been working on. So please go ahead.", "tokens": [50804, 869, 1558, 300, 291, 486, 536, 382, 257, 290, 468, 295, 437, 7506, 575, 668, 1364, 322, 13, 407, 1767, 352, 2286, 13, 51296], "temperature": 0.0, "avg_logprob": -0.1836491146603146, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.014932164922356606}, {"id": 11, "seek": 7184, "start": 90.48, "end": 97.84, "text": " And also, another question that we often ask in the class is that students are interested to know", "tokens": [51296, 400, 611, 11, 1071, 1168, 300, 321, 2049, 1029, 294, 264, 1508, 307, 300, 1731, 366, 3102, 281, 458, 51664], "temperature": 0.0, "avg_logprob": -0.1836491146603146, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.014932164922356606}, {"id": 12, "seek": 9784, "start": 97.84, "end": 105.36, "text": " a little more about your background because they always feel inspired by seeing great scientists", "tokens": [50364, 257, 707, 544, 466, 428, 3678, 570, 436, 1009, 841, 7547, 538, 2577, 869, 7708, 50740], "temperature": 0.0, "avg_logprob": -0.18793851679021661, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.004823874216526747}, {"id": 13, "seek": 9784, "start": 105.36, "end": 113.04, "text": " and what, for instance, got you to working on AI would be very interesting for them if you don't", "tokens": [50740, 293, 437, 11, 337, 5197, 11, 658, 291, 281, 1364, 322, 7318, 576, 312, 588, 1880, 337, 552, 498, 291, 500, 380, 51124], "temperature": 0.0, "avg_logprob": -0.18793851679021661, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.004823874216526747}, {"id": 14, "seek": 9784, "start": 113.04, "end": 121.48, "text": " mind sharing. Great. Thank you for the introduction. Let me share my screen here and make sure that", "tokens": [51124, 1575, 5414, 13, 3769, 13, 1044, 291, 337, 264, 9339, 13, 961, 385, 2073, 452, 2568, 510, 293, 652, 988, 300, 51546], "temperature": 0.0, "avg_logprob": -0.18793851679021661, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.004823874216526747}, {"id": 15, "seek": 12148, "start": 121.76, "end": 140.32, "text": " is working. So are you able to see my screen? Yeah. And the presentation? Yes. Okay. And can you see my", "tokens": [50378, 307, 1364, 13, 407, 366, 291, 1075, 281, 536, 452, 2568, 30, 865, 13, 400, 264, 5860, 30, 1079, 13, 1033, 13, 400, 393, 291, 536, 452, 51306], "temperature": 0.0, "avg_logprob": -0.26140631659556246, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.043924249708652496}, {"id": 16, "seek": 12148, "start": 140.32, "end": 148.96, "text": " mouse cursor? Yes. Okay. Hello, everyone. My name is Jeff Klune. And I want to talk to you today", "tokens": [51306, 9719, 28169, 30, 1079, 13, 1033, 13, 2425, 11, 1518, 13, 1222, 1315, 307, 7506, 16053, 2613, 13, 400, 286, 528, 281, 751, 281, 291, 965, 51738], "temperature": 0.0, "avg_logprob": -0.26140631659556246, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.043924249708652496}, {"id": 17, "seek": 14896, "start": 148.96, "end": 155.32000000000002, "text": " about trying to take on like an extremely big research challenge. I think it's a grand challenge", "tokens": [50364, 466, 1382, 281, 747, 322, 411, 364, 4664, 955, 2132, 3430, 13, 286, 519, 309, 311, 257, 2697, 3430, 50682], "temperature": 0.0, "avg_logprob": -0.13718127156351947, "compression_ratio": 1.5685483870967742, "no_speech_prob": 0.05904730409383774}, {"id": 18, "seek": 14896, "start": 155.32000000000002, "end": 163.36, "text": " of AI. And that is trying to create what we call open-ended algorithms. I wasn't planning on telling", "tokens": [50682, 295, 7318, 13, 400, 300, 307, 1382, 281, 1884, 437, 321, 818, 1269, 12, 3502, 14642, 13, 286, 2067, 380, 5038, 322, 3585, 51084], "temperature": 0.0, "avg_logprob": -0.13718127156351947, "compression_ratio": 1.5685483870967742, "no_speech_prob": 0.05904730409383774}, {"id": 19, "seek": 14896, "start": 163.36, "end": 167.32, "text": " you a little bit about my background. I guess in brief, I started out on a quest just to understand", "tokens": [51084, 291, 257, 707, 857, 466, 452, 3678, 13, 286, 2041, 294, 5353, 11, 286, 1409, 484, 322, 257, 866, 445, 281, 1223, 51282], "temperature": 0.0, "avg_logprob": -0.13718127156351947, "compression_ratio": 1.5685483870967742, "no_speech_prob": 0.05904730409383774}, {"id": 20, "seek": 14896, "start": 167.32, "end": 173.76000000000002, "text": " two twin questions, which is how did natural evolution produce all the complexity on Earth,", "tokens": [51282, 732, 18397, 1651, 11, 597, 307, 577, 630, 3303, 9303, 5258, 439, 264, 14024, 322, 4755, 11, 51604], "temperature": 0.0, "avg_logprob": -0.13718127156351947, "compression_ratio": 1.5685483870967742, "no_speech_prob": 0.05904730409383774}, {"id": 21, "seek": 17376, "start": 173.76, "end": 179.48, "text": " including the human brain? It's astounding. And we don't know how that process happened really. We", "tokens": [50364, 3009, 264, 1952, 3567, 30, 467, 311, 5357, 24625, 13, 400, 321, 500, 380, 458, 577, 300, 1399, 2011, 534, 13, 492, 50650], "temperature": 0.0, "avg_logprob": -0.12896854347652859, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.11422248929738998}, {"id": 22, "seek": 17376, "start": 179.48, "end": 184.0, "text": " don't know how to recreate it. And you'll see a lot of work towards that today. And then also, I'm", "tokens": [50650, 500, 380, 458, 577, 281, 25833, 309, 13, 400, 291, 603, 536, 257, 688, 295, 589, 3030, 300, 965, 13, 400, 550, 611, 11, 286, 478, 50876], "temperature": 0.0, "avg_logprob": -0.12896854347652859, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.11422248929738998}, {"id": 23, "seek": 17376, "start": 184.0, "end": 188.35999999999999, "text": " interested in trying to figure out how does thinking happen and how can we create it in machines? And", "tokens": [50876, 3102, 294, 1382, 281, 2573, 484, 577, 775, 1953, 1051, 293, 577, 393, 321, 1884, 309, 294, 8379, 30, 400, 51094], "temperature": 0.0, "avg_logprob": -0.12896854347652859, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.11422248929738998}, {"id": 24, "seek": 17376, "start": 188.35999999999999, "end": 193.56, "text": " I think in many ways, these questions are very intertwined, as you'll see also today. So I started", "tokens": [51094, 286, 519, 294, 867, 2098, 11, 613, 1651, 366, 588, 44400, 2001, 11, 382, 291, 603, 536, 611, 965, 13, 407, 286, 1409, 51354], "temperature": 0.0, "avg_logprob": -0.12896854347652859, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.11422248929738998}, {"id": 25, "seek": 17376, "start": 193.56, "end": 198.07999999999998, "text": " out in philosophy, actually, because I thought they had the market cornered on thinking, but really", "tokens": [51354, 484, 294, 10675, 11, 767, 11, 570, 286, 1194, 436, 632, 264, 2142, 4538, 292, 322, 1953, 11, 457, 534, 51580], "temperature": 0.0, "avg_logprob": -0.12896854347652859, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.11422248929738998}, {"id": 26, "seek": 17376, "start": 198.07999999999998, "end": 202.51999999999998, "text": " quickly kind of, or actually not quickly, slowly learned throughout the course of my life that the", "tokens": [51580, 2661, 733, 295, 11, 420, 767, 406, 2661, 11, 5692, 3264, 3710, 264, 1164, 295, 452, 993, 300, 264, 51802], "temperature": 0.0, "avg_logprob": -0.12896854347652859, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.11422248929738998}, {"id": 27, "seek": 20252, "start": 202.56, "end": 207.0, "text": " best way to tackle these challenges is to try to build these systems and recreate these systems", "tokens": [50366, 1151, 636, 281, 14896, 613, 4759, 307, 281, 853, 281, 1322, 613, 3652, 293, 25833, 613, 3652, 50588], "temperature": 0.0, "avg_logprob": -0.12397070098341557, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.0005191416712477803}, {"id": 28, "seek": 20252, "start": 207.0, "end": 212.0, "text": " computationally. Motivated by the wonderful quote by Richard Feynman, which is, that which I cannot", "tokens": [50588, 24903, 379, 13, 8956, 592, 770, 538, 264, 3715, 6513, 538, 9809, 46530, 77, 1601, 11, 597, 307, 11, 300, 597, 286, 2644, 50838], "temperature": 0.0, "avg_logprob": -0.12397070098341557, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.0005191416712477803}, {"id": 29, "seek": 20252, "start": 212.0, "end": 217.48000000000002, "text": " build, I do not understand. So we understand by building. And that has certainly been true in my", "tokens": [50838, 1322, 11, 286, 360, 406, 1223, 13, 407, 321, 1223, 538, 2390, 13, 400, 300, 575, 3297, 668, 2074, 294, 452, 51112], "temperature": 0.0, "avg_logprob": -0.12397070098341557, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.0005191416712477803}, {"id": 30, "seek": 20252, "start": 217.48000000000002, "end": 223.60000000000002, "text": " life that I understand more and more by being forced to turn speculation into code and into", "tokens": [51112, 993, 300, 286, 1223, 544, 293, 544, 538, 885, 7579, 281, 1261, 27696, 666, 3089, 293, 666, 51418], "temperature": 0.0, "avg_logprob": -0.12397070098341557, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.0005191416712477803}, {"id": 31, "seek": 20252, "start": 223.60000000000002, "end": 231.4, "text": " algorithms. So with that, I'm going to begin. So this talk is really going to be in two parts.", "tokens": [51418, 14642, 13, 407, 365, 300, 11, 286, 478, 516, 281, 1841, 13, 407, 341, 751, 307, 534, 516, 281, 312, 294, 732, 3166, 13, 51808], "temperature": 0.0, "avg_logprob": -0.12397070098341557, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.0005191416712477803}, {"id": 32, "seek": 23140, "start": 232.36, "end": 236.92000000000002, "text": " The main part is going to be the first part. And it's about creating open-ended innovation engines.", "tokens": [50412, 440, 2135, 644, 307, 516, 281, 312, 264, 700, 644, 13, 400, 309, 311, 466, 4084, 1269, 12, 3502, 8504, 12982, 13, 50640], "temperature": 0.0, "avg_logprob": -0.08024182812920932, "compression_ratio": 1.7298245614035088, "no_speech_prob": 0.0011329659027978778}, {"id": 33, "seek": 23140, "start": 237.56, "end": 242.20000000000002, "text": " And if there's time, which I hope there will be, I'm going to rush through a series of work that", "tokens": [50672, 400, 498, 456, 311, 565, 11, 597, 286, 1454, 456, 486, 312, 11, 286, 478, 516, 281, 9300, 807, 257, 2638, 295, 589, 300, 50904], "temperature": 0.0, "avg_logprob": -0.08024182812920932, "compression_ratio": 1.7298245614035088, "no_speech_prob": 0.0011329659027978778}, {"id": 34, "seek": 23140, "start": 242.20000000000002, "end": 246.84, "text": " we've done that I call AI neuroscience at the end. And then throughout all of this, what you're", "tokens": [50904, 321, 600, 1096, 300, 286, 818, 7318, 42762, 412, 264, 917, 13, 400, 550, 3710, 439, 295, 341, 11, 437, 291, 434, 51136], "temperature": 0.0, "avg_logprob": -0.08024182812920932, "compression_ratio": 1.7298245614035088, "no_speech_prob": 0.0011329659027978778}, {"id": 35, "seek": 23140, "start": 246.84, "end": 251.72, "text": " going to find is that this is a bit of a meandering intellectual story, because throughout my career,", "tokens": [51136, 516, 281, 915, 307, 300, 341, 307, 257, 857, 295, 257, 385, 474, 1794, 12576, 1657, 11, 570, 3710, 452, 3988, 11, 51380], "temperature": 0.0, "avg_logprob": -0.08024182812920932, "compression_ratio": 1.7298245614035088, "no_speech_prob": 0.0011329659027978778}, {"id": 36, "seek": 23140, "start": 251.72, "end": 258.04, "text": " different research has kind of unintentionally produced different aesthetic artifacts of interest.", "tokens": [51380, 819, 2132, 575, 733, 295, 45514, 379, 7126, 819, 20092, 24617, 295, 1179, 13, 51696], "temperature": 0.0, "avg_logprob": -0.08024182812920932, "compression_ratio": 1.7298245614035088, "no_speech_prob": 0.0011329659027978778}, {"id": 37, "seek": 25804, "start": 258.04, "end": 262.36, "text": " And I kind of want to walk through some of the things and touch as many of these places where I", "tokens": [50364, 400, 286, 733, 295, 528, 281, 1792, 807, 512, 295, 264, 721, 293, 2557, 382, 867, 295, 613, 3190, 689, 286, 50580], "temperature": 0.0, "avg_logprob": -0.08723796179535193, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0003149677941109985}, {"id": 38, "seek": 25804, "start": 262.36, "end": 266.44, "text": " think our work has produced things that are aesthetically interesting, as well as scientifically", "tokens": [50580, 519, 527, 589, 575, 7126, 721, 300, 366, 27837, 984, 1880, 11, 382, 731, 382, 39719, 50784], "temperature": 0.0, "avg_logprob": -0.08723796179535193, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0003149677941109985}, {"id": 39, "seek": 25804, "start": 266.44, "end": 277.08000000000004, "text": " interesting. So the first thing I want to motivate is, you know, the idea of open-ended algorithms.", "tokens": [50784, 1880, 13, 407, 264, 700, 551, 286, 528, 281, 28497, 307, 11, 291, 458, 11, 264, 1558, 295, 1269, 12, 3502, 14642, 13, 51316], "temperature": 0.0, "avg_logprob": -0.08723796179535193, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0003149677941109985}, {"id": 40, "seek": 25804, "start": 277.08000000000004, "end": 282.20000000000005, "text": " So these are things that endlessly innovate. They just keep going forever. So if you think", "tokens": [51316, 407, 613, 366, 721, 300, 44920, 33444, 13, 814, 445, 1066, 516, 5680, 13, 407, 498, 291, 519, 51572], "temperature": 0.0, "avg_logprob": -0.08723796179535193, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0003149677941109985}, {"id": 41, "seek": 25804, "start": 282.20000000000005, "end": 287.32000000000005, "text": " about natural evolution, look at the Tree of Life there, and think about all of the marvelous", "tokens": [51572, 466, 3303, 9303, 11, 574, 412, 264, 22291, 295, 7720, 456, 11, 293, 519, 466, 439, 295, 264, 34920, 51828], "temperature": 0.0, "avg_logprob": -0.08723796179535193, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0003149677941109985}, {"id": 42, "seek": 28732, "start": 287.32, "end": 293.56, "text": " engineering designs that nature has brought and continues to create in an ever-going fashion,", "tokens": [50364, 7043, 11347, 300, 3687, 575, 3038, 293, 6515, 281, 1884, 294, 364, 1562, 12, 8102, 6700, 11, 50676], "temperature": 0.0, "avg_logprob": -0.11403166164051402, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.0006876233383081853}, {"id": 43, "seek": 28732, "start": 293.56, "end": 298.92, "text": " you know, jaguars, hawks, the human mind, everything that we know on Earth. You know,", "tokens": [50676, 291, 458, 11, 6368, 84, 685, 11, 33634, 1694, 11, 264, 1952, 1575, 11, 1203, 300, 321, 458, 322, 4755, 13, 509, 458, 11, 50944], "temperature": 0.0, "avg_logprob": -0.11403166164051402, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.0006876233383081853}, {"id": 44, "seek": 28732, "start": 298.92, "end": 304.52, "text": " in most situations, we cannot rival these things with engineering. And so what's fascinating is", "tokens": [50944, 294, 881, 6851, 11, 321, 2644, 16286, 613, 721, 365, 7043, 13, 400, 370, 437, 311, 10343, 307, 51224], "temperature": 0.0, "avg_logprob": -0.11403166164051402, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.0006876233383081853}, {"id": 45, "seek": 28732, "start": 304.52, "end": 309.4, "text": " that, you know, a very simple algorithm that Darwinian evolutionary algorithm, plus the context", "tokens": [51224, 300, 11, 291, 458, 11, 257, 588, 2199, 9284, 300, 30233, 952, 27567, 9284, 11, 1804, 264, 4319, 51468], "temperature": 0.0, "avg_logprob": -0.11403166164051402, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.0006876233383081853}, {"id": 46, "seek": 28732, "start": 309.4, "end": 313.88, "text": " it's been placed in, continues to innovate for billions and billions of years. And I think it's", "tokens": [51468, 309, 311, 668, 7074, 294, 11, 6515, 281, 33444, 337, 17375, 293, 17375, 295, 924, 13, 400, 286, 519, 309, 311, 51692], "temperature": 0.0, "avg_logprob": -0.11403166164051402, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.0006876233383081853}, {"id": 47, "seek": 31388, "start": 313.88, "end": 318.44, "text": " really fruitful to think to yourself, you know, could you create an algorithm that you would want", "tokens": [50364, 534, 49795, 281, 519, 281, 1803, 11, 291, 458, 11, 727, 291, 1884, 364, 9284, 300, 291, 576, 528, 50592], "temperature": 0.0, "avg_logprob": -0.08298228681087494, "compression_ratio": 1.8449367088607596, "no_speech_prob": 0.0026310563553124666}, {"id": 48, "seek": 31388, "start": 318.44, "end": 323.48, "text": " to run for billions and billions of years and come back and check whether or not it's interesting?", "tokens": [50592, 281, 1190, 337, 17375, 293, 17375, 295, 924, 293, 808, 646, 293, 1520, 1968, 420, 406, 309, 311, 1880, 30, 50844], "temperature": 0.0, "avg_logprob": -0.08298228681087494, "compression_ratio": 1.8449367088607596, "no_speech_prob": 0.0026310563553124666}, {"id": 49, "seek": 31388, "start": 323.48, "end": 328.68, "text": " Currently, as scientists, we have zero ability to produce things that are interesting even after", "tokens": [50844, 19964, 11, 382, 7708, 11, 321, 362, 4018, 3485, 281, 5258, 721, 300, 366, 1880, 754, 934, 51104], "temperature": 0.0, "avg_logprob": -0.08298228681087494, "compression_ratio": 1.8449367088607596, "no_speech_prob": 0.0026310563553124666}, {"id": 50, "seek": 31388, "start": 328.68, "end": 334.2, "text": " a few months of running them on a computer, let alone billions and billions of years. So natural", "tokens": [51104, 257, 1326, 2493, 295, 2614, 552, 322, 257, 3820, 11, 718, 3312, 17375, 293, 17375, 295, 924, 13, 407, 3303, 51380], "temperature": 0.0, "avg_logprob": -0.08298228681087494, "compression_ratio": 1.8449367088607596, "no_speech_prob": 0.0026310563553124666}, {"id": 51, "seek": 31388, "start": 334.2, "end": 337.56, "text": " evolution is what we, you know, one of these open-ended algorithms. And another one is human", "tokens": [51380, 9303, 307, 437, 321, 11, 291, 458, 11, 472, 295, 613, 1269, 12, 3502, 14642, 13, 400, 1071, 472, 307, 1952, 51548], "temperature": 0.0, "avg_logprob": -0.08298228681087494, "compression_ratio": 1.8449367088607596, "no_speech_prob": 0.0026310563553124666}, {"id": 52, "seek": 31388, "start": 337.56, "end": 342.76, "text": " culture, which just endlessly innovates and produce innovation on innovation and innovation. That's", "tokens": [51548, 3713, 11, 597, 445, 44920, 5083, 1024, 293, 5258, 8504, 322, 8504, 293, 8504, 13, 663, 311, 51808], "temperature": 0.0, "avg_logprob": -0.08298228681087494, "compression_ratio": 1.8449367088607596, "no_speech_prob": 0.0026310563553124666}, {"id": 53, "seek": 34276, "start": 342.76, "end": 346.68, "text": " both true in science and technology, but it's also true in the arts, where you get, you know,", "tokens": [50364, 1293, 2074, 294, 3497, 293, 2899, 11, 457, 309, 311, 611, 2074, 294, 264, 8609, 11, 689, 291, 483, 11, 291, 458, 11, 50560], "temperature": 0.0, "avg_logprob": -0.08697281045428777, "compression_ratio": 1.7638376383763839, "no_speech_prob": 0.00011957385140703991}, {"id": 54, "seek": 34276, "start": 346.68, "end": 351.4, "text": " impressionism after you get the classical paintings, and then you get, you know, post-modernism or", "tokens": [50560, 9995, 1434, 934, 291, 483, 264, 13735, 14880, 11, 293, 550, 291, 483, 11, 291, 458, 11, 2183, 12, 42359, 1434, 420, 50796], "temperature": 0.0, "avg_logprob": -0.08697281045428777, "compression_ratio": 1.7638376383763839, "no_speech_prob": 0.00011957385140703991}, {"id": 55, "seek": 34276, "start": 351.4, "end": 357.32, "text": " Jackson Pollock or all the different kind of evolution of genres. So, you know, we started", "tokens": [50796, 10647, 31304, 1560, 420, 439, 264, 819, 733, 295, 9303, 295, 30057, 13, 407, 11, 291, 458, 11, 321, 1409, 51092], "temperature": 0.0, "avg_logprob": -0.08697281045428777, "compression_ratio": 1.7638376383763839, "no_speech_prob": 0.00011957385140703991}, {"id": 56, "seek": 34276, "start": 357.32, "end": 363.64, "text": " with the idea, when we try and wanted to try to work on this, is that natural evolution and human", "tokens": [51092, 365, 264, 1558, 11, 562, 321, 853, 293, 1415, 281, 853, 281, 589, 322, 341, 11, 307, 300, 3303, 9303, 293, 1952, 51408], "temperature": 0.0, "avg_logprob": -0.08697281045428777, "compression_ratio": 1.7638376383763839, "no_speech_prob": 0.00011957385140703991}, {"id": 57, "seek": 34276, "start": 363.64, "end": 368.44, "text": " culture are what we call innovation engines. And that is that there's kind of this simple recipe", "tokens": [51408, 3713, 366, 437, 321, 818, 8504, 12982, 13, 400, 300, 307, 300, 456, 311, 733, 295, 341, 2199, 6782, 51648], "temperature": 0.0, "avg_logprob": -0.08697281045428777, "compression_ratio": 1.7638376383763839, "no_speech_prob": 0.00011957385140703991}, {"id": 58, "seek": 36844, "start": 368.44, "end": 375.0, "text": " that they follow that allows them to be creative. And that is that they start with a set of things,", "tokens": [50364, 300, 436, 1524, 300, 4045, 552, 281, 312, 5880, 13, 400, 300, 307, 300, 436, 722, 365, 257, 992, 295, 721, 11, 50692], "temperature": 0.0, "avg_logprob": -0.06390301934603987, "compression_ratio": 2.096774193548387, "no_speech_prob": 0.010981684550642967}, {"id": 59, "seek": 36844, "start": 375.0, "end": 379.71999999999997, "text": " it could be an empty set, and then they generate a new thing. And then if that's interesting,", "tokens": [50692, 309, 727, 312, 364, 6707, 992, 11, 293, 550, 436, 8460, 257, 777, 551, 13, 400, 550, 498, 300, 311, 1880, 11, 50928], "temperature": 0.0, "avg_logprob": -0.06390301934603987, "compression_ratio": 2.096774193548387, "no_speech_prob": 0.010981684550642967}, {"id": 60, "seek": 36844, "start": 379.71999999999997, "end": 383.72, "text": " they keep it and add it to the set. And then they take something out of that set, they change it a", "tokens": [50928, 436, 1066, 309, 293, 909, 309, 281, 264, 992, 13, 400, 550, 436, 747, 746, 484, 295, 300, 992, 11, 436, 1319, 309, 257, 51128], "temperature": 0.0, "avg_logprob": -0.06390301934603987, "compression_ratio": 2.096774193548387, "no_speech_prob": 0.010981684550642967}, {"id": 61, "seek": 36844, "start": 383.72, "end": 388.12, "text": " little bit, they permute it somehow, and they see if that is interesting. And if that's interesting,", "tokens": [51128, 707, 857, 11, 436, 4784, 1169, 309, 6063, 11, 293, 436, 536, 498, 300, 307, 1880, 13, 400, 498, 300, 311, 1880, 11, 51348], "temperature": 0.0, "avg_logprob": -0.06390301934603987, "compression_ratio": 2.096774193548387, "no_speech_prob": 0.010981684550642967}, {"id": 62, "seek": 36844, "start": 388.12, "end": 392.76, "text": " they add that to the set. And you have this growing set of things, these archives of things", "tokens": [51348, 436, 909, 300, 281, 264, 992, 13, 400, 291, 362, 341, 4194, 992, 295, 721, 11, 613, 25607, 295, 721, 51580], "temperature": 0.0, "avg_logprob": -0.06390301934603987, "compression_ratio": 2.096774193548387, "no_speech_prob": 0.010981684550642967}, {"id": 63, "seek": 36844, "start": 392.76, "end": 397.88, "text": " you've already produced that are interesting. And then each one of those is a stepping stone to new", "tokens": [51580, 291, 600, 1217, 7126, 300, 366, 1880, 13, 400, 550, 1184, 472, 295, 729, 307, 257, 16821, 7581, 281, 777, 51836], "temperature": 0.0, "avg_logprob": -0.06390301934603987, "compression_ratio": 2.096774193548387, "no_speech_prob": 0.010981684550642967}, {"id": 64, "seek": 39788, "start": 397.88, "end": 402.6, "text": " potential innovations or solutions. And if you think deeply about it, that's true both of human", "tokens": [50364, 3995, 24283, 420, 6547, 13, 400, 498, 291, 519, 8760, 466, 309, 11, 300, 311, 2074, 1293, 295, 1952, 50600], "temperature": 0.0, "avg_logprob": -0.061597469079233434, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0004877554310951382}, {"id": 65, "seek": 39788, "start": 402.6, "end": 407.4, "text": " culture and natural evolution. And so the question is, with that kind of mental framework, can we", "tokens": [50600, 3713, 293, 3303, 9303, 13, 400, 370, 264, 1168, 307, 11, 365, 300, 733, 295, 4973, 8388, 11, 393, 321, 50840], "temperature": 0.0, "avg_logprob": -0.061597469079233434, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0004877554310951382}, {"id": 66, "seek": 39788, "start": 407.4, "end": 414.12, "text": " create algorithms that do that process automatically? And so, you know, at the core, there's really kind", "tokens": [50840, 1884, 14642, 300, 360, 300, 1399, 6772, 30, 400, 370, 11, 291, 458, 11, 412, 264, 4965, 11, 456, 311, 534, 733, 51176], "temperature": 0.0, "avg_logprob": -0.061597469079233434, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0004877554310951382}, {"id": 67, "seek": 39788, "start": 414.12, "end": 418.36, "text": " of these two simple steps. The first one is you have to have something that generates new things", "tokens": [51176, 295, 613, 732, 2199, 4439, 13, 440, 700, 472, 307, 291, 362, 281, 362, 746, 300, 23815, 777, 721, 51388], "temperature": 0.0, "avg_logprob": -0.061597469079233434, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0004877554310951382}, {"id": 68, "seek": 39788, "start": 418.36, "end": 423.48, "text": " based on previous things. That's the green box on the left. And then you have to evaluate whether", "tokens": [51388, 2361, 322, 3894, 721, 13, 663, 311, 264, 3092, 2424, 322, 264, 1411, 13, 400, 550, 291, 362, 281, 13059, 1968, 51644], "temperature": 0.0, "avg_logprob": -0.061597469079233434, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0004877554310951382}, {"id": 69, "seek": 39788, "start": 423.48, "end": 427.4, "text": " not those things are interesting. And if they are, then you add them to the set and you just keep", "tokens": [51644, 406, 729, 721, 366, 1880, 13, 400, 498, 436, 366, 11, 550, 291, 909, 552, 281, 264, 992, 293, 291, 445, 1066, 51840], "temperature": 0.0, "avg_logprob": -0.061597469079233434, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.0004877554310951382}, {"id": 70, "seek": 42740, "start": 427.4, "end": 433.47999999999996, "text": " repeating this process. So in the long run, what we'd love to do is take, you know, humans out of", "tokens": [50364, 18617, 341, 1399, 13, 407, 294, 264, 938, 1190, 11, 437, 321, 1116, 959, 281, 360, 307, 747, 11, 291, 458, 11, 6255, 484, 295, 50668], "temperature": 0.0, "avg_logprob": -0.1159452604210895, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.0009108340600505471}, {"id": 71, "seek": 42740, "start": 433.47999999999996, "end": 438.2, "text": " the loop if possible, label data out of the loop, and you just have some sort of generator like a", "tokens": [50668, 264, 6367, 498, 1944, 11, 7645, 1412, 484, 295, 264, 6367, 11, 293, 291, 445, 362, 512, 1333, 295, 19265, 411, 257, 50904], "temperature": 0.0, "avg_logprob": -0.1159452604210895, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.0009108340600505471}, {"id": 72, "seek": 42740, "start": 438.2, "end": 444.03999999999996, "text": " neural network that can generate new things like poems or codes or mathematical proofs or images,", "tokens": [50904, 18161, 3209, 300, 393, 8460, 777, 721, 411, 24014, 420, 14211, 420, 18894, 8177, 82, 420, 5267, 11, 51196], "temperature": 0.0, "avg_logprob": -0.1159452604210895, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.0009108340600505471}, {"id": 73, "seek": 42740, "start": 444.76, "end": 449.15999999999997, "text": " or, you know, technological artifacts, something, maybe another deep neural net that is trained", "tokens": [51232, 420, 11, 291, 458, 11, 18439, 24617, 11, 746, 11, 1310, 1071, 2452, 18161, 2533, 300, 307, 8895, 51452], "temperature": 0.0, "avg_logprob": -0.1159452604210895, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.0009108340600505471}, {"id": 74, "seek": 42740, "start": 449.15999999999997, "end": 454.35999999999996, "text": " to recognize what's interesting somehow, and then that process could just iterate. Now, for example,", "tokens": [51452, 281, 5521, 437, 311, 1880, 6063, 11, 293, 550, 300, 1399, 727, 445, 44497, 13, 823, 11, 337, 1365, 11, 51712], "temperature": 0.0, "avg_logprob": -0.1159452604210895, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.0009108340600505471}, {"id": 75, "seek": 45436, "start": 454.44, "end": 459.48, "text": " you could imagine that you take like the orange box here is an autoencoder. And it looks at everything", "tokens": [50368, 291, 727, 3811, 300, 291, 747, 411, 264, 7671, 2424, 510, 307, 364, 8399, 22660, 19866, 13, 400, 309, 1542, 412, 1203, 50620], "temperature": 0.0, "avg_logprob": -0.09357022224588597, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.00690160458907485}, {"id": 76, "seek": 45436, "start": 459.48, "end": 463.56, "text": " that it's seen before, it compresses them down to a low dimensional bottleneck space, and then it has", "tokens": [50620, 300, 309, 311, 1612, 949, 11, 309, 14778, 279, 552, 760, 281, 257, 2295, 18795, 44641, 547, 1901, 11, 293, 550, 309, 575, 50824], "temperature": 0.0, "avg_logprob": -0.09357022224588597, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.00690160458907485}, {"id": 77, "seek": 45436, "start": 463.56, "end": 467.88, "text": " to uncompress them. And then if you get some new latent vector that's new that you've never seen", "tokens": [50824, 281, 8585, 11637, 552, 13, 400, 550, 498, 291, 483, 512, 777, 48994, 8062, 300, 311, 777, 300, 291, 600, 1128, 1612, 51040], "temperature": 0.0, "avg_logprob": -0.09357022224588597, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.00690160458907485}, {"id": 78, "seek": 45436, "start": 467.88, "end": 471.8, "text": " before, you call that interesting. And that might be one thing that could kick off this problem.", "tokens": [51040, 949, 11, 291, 818, 300, 1880, 13, 400, 300, 1062, 312, 472, 551, 300, 727, 4437, 766, 341, 1154, 13, 51236], "temperature": 0.0, "avg_logprob": -0.09357022224588597, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.00690160458907485}, {"id": 79, "seek": 45436, "start": 472.36, "end": 476.84000000000003, "text": " And if you could do that, you would have an innovations arms race in any domain, you could", "tokens": [51264, 400, 498, 291, 727, 360, 300, 11, 291, 576, 362, 364, 24283, 5812, 4569, 294, 604, 9274, 11, 291, 727, 51488], "temperature": 0.0, "avg_logprob": -0.09357022224588597, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.00690160458907485}, {"id": 80, "seek": 45436, "start": 476.84000000000003, "end": 481.0, "text": " unleash this thing anywhere. And that would be amazing. And a lot of these ideas date back to", "tokens": [51488, 49814, 341, 551, 4992, 13, 400, 300, 576, 312, 2243, 13, 400, 257, 688, 295, 613, 3487, 4002, 646, 281, 51696], "temperature": 0.0, "avg_logprob": -0.09357022224588597, "compression_ratio": 1.8990228013029316, "no_speech_prob": 0.00690160458907485}, {"id": 81, "seek": 48100, "start": 481.0, "end": 487.32, "text": " Schmidt-Huber ideas from the early 90s. However, the problem is that when you do that, you typically", "tokens": [50364, 42621, 12, 39, 10261, 3487, 490, 264, 2440, 4289, 82, 13, 2908, 11, 264, 1154, 307, 300, 562, 291, 360, 300, 11, 291, 5850, 50680], "temperature": 0.0, "avg_logprob": -0.09453642995733964, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.0018098814180120826}, {"id": 82, "seek": 48100, "start": 487.32, "end": 492.04, "text": " do get new things forever, but you don't get new interesting things forever. You, for example,", "tokens": [50680, 360, 483, 777, 721, 5680, 11, 457, 291, 500, 380, 483, 777, 1880, 721, 5680, 13, 509, 11, 337, 1365, 11, 50916], "temperature": 0.0, "avg_logprob": -0.09453642995733964, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.0018098814180120826}, {"id": 83, "seek": 48100, "start": 492.04, "end": 495.88, "text": " might get white noise, just an endless stream of different patterns of white noise, because those", "tokens": [50916, 1062, 483, 2418, 5658, 11, 445, 364, 16144, 4309, 295, 819, 8294, 295, 2418, 5658, 11, 570, 729, 51108], "temperature": 0.0, "avg_logprob": -0.09453642995733964, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.0018098814180120826}, {"id": 84, "seek": 48100, "start": 495.88, "end": 500.76, "text": " are uncompressible. So really, at its core, the biggest challenge in this field is kind of how", "tokens": [51108, 366, 8585, 11637, 964, 13, 407, 534, 11, 412, 1080, 4965, 11, 264, 3880, 3430, 294, 341, 2519, 307, 733, 295, 577, 51352], "temperature": 0.0, "avg_logprob": -0.09453642995733964, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.0018098814180120826}, {"id": 85, "seek": 48100, "start": 500.76, "end": 505.8, "text": " do you avoid generating uninteresting novelty, and how do you only generate interesting novelty?", "tokens": [51352, 360, 291, 5042, 17746, 49234, 8714, 44805, 11, 293, 577, 360, 291, 787, 8460, 1880, 44805, 30, 51604], "temperature": 0.0, "avg_logprob": -0.09453642995733964, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.0018098814180120826}, {"id": 86, "seek": 50580, "start": 506.6, "end": 511.40000000000003, "text": " And here's one example by a friend of mine, Josh Auerbach, who tried to basically take the same", "tokens": [50404, 400, 510, 311, 472, 1365, 538, 257, 1277, 295, 3892, 11, 9785, 316, 5486, 32096, 11, 567, 3031, 281, 1936, 747, 264, 912, 50644], "temperature": 0.0, "avg_logprob": -0.08421239926833515, "compression_ratio": 1.76, "no_speech_prob": 0.002714456059038639}, {"id": 87, "seek": 50580, "start": 511.40000000000003, "end": 515.16, "text": " encoding that I'm going to tell you about later, and a similar system is trying to automatically", "tokens": [50644, 43430, 300, 286, 478, 516, 281, 980, 291, 466, 1780, 11, 293, 257, 2531, 1185, 307, 1382, 281, 6772, 50832], "temperature": 0.0, "avg_logprob": -0.08421239926833515, "compression_ratio": 1.76, "no_speech_prob": 0.002714456059038639}, {"id": 88, "seek": 50580, "start": 515.16, "end": 520.28, "text": " generate images and try to produce new interesting images forever. And these images are interesting,", "tokens": [50832, 8460, 5267, 293, 853, 281, 5258, 777, 1880, 5267, 5680, 13, 400, 613, 5267, 366, 1880, 11, 51088], "temperature": 0.0, "avg_logprob": -0.08421239926833515, "compression_ratio": 1.76, "no_speech_prob": 0.002714456059038639}, {"id": 89, "seek": 50580, "start": 520.28, "end": 524.12, "text": " they're pretty cool, but they're not nearly as interesting as they could be, right? They're not", "tokens": [51088, 436, 434, 1238, 1627, 11, 457, 436, 434, 406, 6217, 382, 1880, 382, 436, 727, 312, 11, 558, 30, 814, 434, 406, 51280], "temperature": 0.0, "avg_logprob": -0.08421239926833515, "compression_ratio": 1.76, "no_speech_prob": 0.002714456059038639}, {"id": 90, "seek": 50580, "start": 524.12, "end": 529.0, "text": " like what artists would do over the course of centuries. You would expect and hope that things", "tokens": [51280, 411, 437, 6910, 576, 360, 670, 264, 1164, 295, 13926, 13, 509, 576, 2066, 293, 1454, 300, 721, 51524], "temperature": 0.0, "avg_logprob": -0.08421239926833515, "compression_ratio": 1.76, "no_speech_prob": 0.002714456059038639}, {"id": 91, "seek": 50580, "start": 529.0, "end": 532.44, "text": " would ultimately break out of these kind of abstract patterns. And that's because these", "tokens": [51524, 576, 6284, 1821, 484, 295, 613, 733, 295, 12649, 8294, 13, 400, 300, 311, 570, 613, 51696], "temperature": 0.0, "avg_logprob": -0.08421239926833515, "compression_ratio": 1.76, "no_speech_prob": 0.002714456059038639}, {"id": 92, "seek": 53244, "start": 532.44, "end": 539.24, "text": " things are optimized to produce information theoretic metrics like compression or mutual", "tokens": [50364, 721, 366, 26941, 281, 5258, 1589, 14308, 299, 16367, 411, 19355, 420, 16917, 50704], "temperature": 0.0, "avg_logprob": -0.08813325791131882, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.0013667421881109476}, {"id": 93, "seek": 53244, "start": 539.24, "end": 545.72, "text": " information and things like that. So what we thought in this work is that one insight you could", "tokens": [50704, 1589, 293, 721, 411, 300, 13, 407, 437, 321, 1194, 294, 341, 589, 307, 300, 472, 11269, 291, 727, 51028], "temperature": 0.0, "avg_logprob": -0.08813325791131882, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.0013667421881109476}, {"id": 94, "seek": 53244, "start": 545.72, "end": 551.1600000000001, "text": " have is that recognizing a new type of thing is like being able to recognize a new class of thing.", "tokens": [51028, 362, 307, 300, 18538, 257, 777, 2010, 295, 551, 307, 411, 885, 1075, 281, 5521, 257, 777, 1508, 295, 551, 13, 51300], "temperature": 0.0, "avg_logprob": -0.08813325791131882, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.0013667421881109476}, {"id": 95, "seek": 53244, "start": 551.1600000000001, "end": 555.96, "text": " If you've never seen a palm tree before, that's a distinct kind of trees. And if you've never", "tokens": [51300, 759, 291, 600, 1128, 1612, 257, 17018, 4230, 949, 11, 300, 311, 257, 10644, 733, 295, 5852, 13, 400, 498, 291, 600, 1128, 51540], "temperature": 0.0, "avg_logprob": -0.08813325791131882, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.0013667421881109476}, {"id": 96, "seek": 53244, "start": 555.96, "end": 561.48, "text": " seen a tree before, trees are distinct from dogs and roses and statues. And so", "tokens": [51540, 1612, 257, 4230, 949, 11, 5852, 366, 10644, 490, 7197, 293, 28620, 293, 29480, 13, 400, 370, 51816], "temperature": 0.0, "avg_logprob": -0.08813325791131882, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.0013667421881109476}, {"id": 97, "seek": 56148, "start": 562.2, "end": 568.44, "text": " one way to think about being able to recognize an infinite number of new classes is to approximate", "tokens": [50400, 472, 636, 281, 519, 466, 885, 1075, 281, 5521, 364, 13785, 1230, 295, 777, 5359, 307, 281, 30874, 50712], "temperature": 0.0, "avg_logprob": -0.06477199943320265, "compression_ratio": 1.816793893129771, "no_speech_prob": 2.840692104655318e-05}, {"id": 98, "seek": 56148, "start": 568.44, "end": 574.76, "text": " that by having a neural net just recognize a very large number of classes. And so if you could", "tokens": [50712, 300, 538, 1419, 257, 18161, 2533, 445, 5521, 257, 588, 2416, 1230, 295, 5359, 13, 400, 370, 498, 291, 727, 51028], "temperature": 0.0, "avg_logprob": -0.06477199943320265, "compression_ratio": 1.816793893129771, "no_speech_prob": 2.840692104655318e-05}, {"id": 99, "seek": 56148, "start": 574.76, "end": 580.2, "text": " recognize, you know, a million classes, for example, then as the generator produces new", "tokens": [51028, 5521, 11, 291, 458, 11, 257, 2459, 5359, 11, 337, 1365, 11, 550, 382, 264, 19265, 14725, 777, 51300], "temperature": 0.0, "avg_logprob": -0.06477199943320265, "compression_ratio": 1.816793893129771, "no_speech_prob": 2.840692104655318e-05}, {"id": 100, "seek": 56148, "start": 580.2, "end": 584.28, "text": " instances of those classes, maybe the process could like start going out and generating each of", "tokens": [51300, 14519, 295, 729, 5359, 11, 1310, 264, 1399, 727, 411, 722, 516, 484, 293, 17746, 1184, 295, 51504], "temperature": 0.0, "avg_logprob": -0.06477199943320265, "compression_ratio": 1.816793893129771, "no_speech_prob": 2.840692104655318e-05}, {"id": 101, "seek": 56148, "start": 584.28, "end": 589.64, "text": " these classes. And that allows us to then use supervised learning because we know how to recognize", "tokens": [51504, 613, 5359, 13, 400, 300, 4045, 505, 281, 550, 764, 46533, 2539, 570, 321, 458, 577, 281, 5521, 51772], "temperature": 0.0, "avg_logprob": -0.06477199943320265, "compression_ratio": 1.816793893129771, "no_speech_prob": 2.840692104655318e-05}, {"id": 102, "seek": 58964, "start": 589.64, "end": 596.4399999999999, "text": " new classes of things. So this is an approximation to the overall goal and to try to see if this", "tokens": [50364, 777, 5359, 295, 721, 13, 407, 341, 307, 364, 28023, 281, 264, 4787, 3387, 293, 281, 853, 281, 536, 498, 341, 50704], "temperature": 0.0, "avg_logprob": -0.13007512866941273, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0007320491713471711}, {"id": 103, "seek": 58964, "start": 596.4399999999999, "end": 600.4399999999999, "text": " system can work. So the way that we wanted to approximate this, and this is all the way back", "tokens": [50704, 1185, 393, 589, 13, 407, 264, 636, 300, 321, 1415, 281, 30874, 341, 11, 293, 341, 307, 439, 264, 636, 646, 50904], "temperature": 0.0, "avg_logprob": -0.13007512866941273, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0007320491713471711}, {"id": 104, "seek": 58964, "start": 600.4399999999999, "end": 605.8, "text": " in 2015 before image generation really worked that well, is we said, let's take a deep neural net", "tokens": [50904, 294, 7546, 949, 3256, 5125, 534, 2732, 300, 731, 11, 307, 321, 848, 11, 718, 311, 747, 257, 2452, 18161, 2533, 51172], "temperature": 0.0, "avg_logprob": -0.13007512866941273, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0007320491713471711}, {"id": 105, "seek": 58964, "start": 605.8, "end": 611.96, "text": " that is trained on ImageNet, which is relatively new around that time. It has 1000 different classes", "tokens": [51172, 300, 307, 8895, 322, 29903, 31890, 11, 597, 307, 7226, 777, 926, 300, 565, 13, 467, 575, 9714, 819, 5359, 51480], "temperature": 0.0, "avg_logprob": -0.13007512866941273, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0007320491713471711}, {"id": 106, "seek": 58964, "start": 611.96, "end": 615.3199999999999, "text": " and it's really good at recognizing these different classes. And then we'll have, we'll use that as", "tokens": [51480, 293, 309, 311, 534, 665, 412, 18538, 613, 819, 5359, 13, 400, 550, 321, 603, 362, 11, 321, 603, 764, 300, 382, 51648], "temperature": 0.0, "avg_logprob": -0.13007512866941273, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0007320491713471711}, {"id": 107, "seek": 61532, "start": 615.32, "end": 621.08, "text": " our evaluator, which is the generator's job is to generate instances of that class. And then the", "tokens": [50364, 527, 6133, 1639, 11, 597, 307, 264, 19265, 311, 1691, 307, 281, 8460, 14519, 295, 300, 1508, 13, 400, 550, 264, 50652], "temperature": 0.0, "avg_logprob": -0.0983556084010912, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0016482165083289146}, {"id": 108, "seek": 61532, "start": 621.08, "end": 625.8000000000001, "text": " question is, what are we going to use for this, the green box here, the generator side. So what we", "tokens": [50652, 1168, 307, 11, 437, 366, 321, 516, 281, 764, 337, 341, 11, 264, 3092, 2424, 510, 11, 264, 19265, 1252, 13, 407, 437, 321, 50888], "temperature": 0.0, "avg_logprob": -0.0983556084010912, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0016482165083289146}, {"id": 109, "seek": 61532, "start": 625.8000000000001, "end": 631.8000000000001, "text": " need is an algorithm that can recognize either an improvement on a current class, or when a new", "tokens": [50888, 643, 307, 364, 9284, 300, 393, 5521, 2139, 364, 10444, 322, 257, 2190, 1508, 11, 420, 562, 257, 777, 51188], "temperature": 0.0, "avg_logprob": -0.0983556084010912, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0016482165083289146}, {"id": 110, "seek": 61532, "start": 631.8000000000001, "end": 637.4000000000001, "text": " class is generated. And so we decided to use this algorithm that I'm excited to tell you about,", "tokens": [51188, 1508, 307, 10833, 13, 400, 370, 321, 3047, 281, 764, 341, 9284, 300, 286, 478, 2919, 281, 980, 291, 466, 11, 51468], "temperature": 0.0, "avg_logprob": -0.0983556084010912, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0016482165083289146}, {"id": 111, "seek": 61532, "start": 637.4000000000001, "end": 640.9200000000001, "text": " because it has a lot of really interesting motivations behind it. It's called map elites.", "tokens": [51468, 570, 309, 575, 257, 688, 295, 534, 1880, 39034, 2261, 309, 13, 467, 311, 1219, 4471, 44678, 13, 51644], "temperature": 0.0, "avg_logprob": -0.0983556084010912, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0016482165083289146}, {"id": 112, "seek": 64092, "start": 641.88, "end": 647.24, "text": " And it has one bin per ImageNet class. And I'll tell you what map elites is right now.", "tokens": [50412, 400, 309, 575, 472, 5171, 680, 29903, 31890, 1508, 13, 400, 286, 603, 980, 291, 437, 4471, 44678, 307, 558, 586, 13, 50680], "temperature": 0.0, "avg_logprob": -0.06176560824034644, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.002714438596740365}, {"id": 113, "seek": 64092, "start": 647.24, "end": 651.8, "text": " But to tell you about map elites, I kind of want to motivate this whole field of a new kind of", "tokens": [50680, 583, 281, 980, 291, 466, 4471, 44678, 11, 286, 733, 295, 528, 281, 28497, 341, 1379, 2519, 295, 257, 777, 733, 295, 50908], "temperature": 0.0, "avg_logprob": -0.06176560824034644, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.002714438596740365}, {"id": 114, "seek": 64092, "start": 651.8, "end": 656.1999999999999, "text": " type of algorithm that my colleagues and I have been working on. And it starts with this recognition,", "tokens": [50908, 2010, 295, 9284, 300, 452, 7734, 293, 286, 362, 668, 1364, 322, 13, 400, 309, 3719, 365, 341, 11150, 11, 51128], "temperature": 0.0, "avg_logprob": -0.06176560824034644, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.002714438596740365}, {"id": 115, "seek": 64092, "start": 656.1999999999999, "end": 660.8399999999999, "text": " which is that there's a paradox in life, which is that if you try too hard to solve a problem,", "tokens": [51128, 597, 307, 300, 456, 311, 257, 26221, 294, 993, 11, 597, 307, 300, 498, 291, 853, 886, 1152, 281, 5039, 257, 1154, 11, 51360], "temperature": 0.0, "avg_logprob": -0.06176560824034644, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.002714438596740365}, {"id": 116, "seek": 64092, "start": 660.8399999999999, "end": 666.92, "text": " you'll fail. However, if you ignore the objective, then you're much more likely to succeed. So imagine", "tokens": [51360, 291, 603, 3061, 13, 2908, 11, 498, 291, 11200, 264, 10024, 11, 550, 291, 434, 709, 544, 3700, 281, 7754, 13, 407, 3811, 51664], "temperature": 0.0, "avg_logprob": -0.06176560824034644, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.002714438596740365}, {"id": 117, "seek": 66692, "start": 667.0, "end": 672.04, "text": " that you're in this maze here, and you're starting here, and your job is to get here. And you might", "tokens": [50368, 300, 291, 434, 294, 341, 33032, 510, 11, 293, 291, 434, 2891, 510, 11, 293, 428, 1691, 307, 281, 483, 510, 13, 400, 291, 1062, 50620], "temperature": 0.0, "avg_logprob": -0.0930630979866817, "compression_ratio": 1.8173374613003095, "no_speech_prob": 0.004608684219419956}, {"id": 118, "seek": 66692, "start": 672.04, "end": 677.16, "text": " say, well, okay, make the robot who's here, make its objective, getting as close as possible to", "tokens": [50620, 584, 11, 731, 11, 1392, 11, 652, 264, 7881, 567, 311, 510, 11, 652, 1080, 10024, 11, 1242, 382, 1998, 382, 1944, 281, 50876], "temperature": 0.0, "avg_logprob": -0.0930630979866817, "compression_ratio": 1.8173374613003095, "no_speech_prob": 0.004608684219419956}, {"id": 119, "seek": 66692, "start": 677.16, "end": 681.0799999999999, "text": " the goal. Well, if you do that, and you get points here, these are all the points that get generated", "tokens": [50876, 264, 3387, 13, 1042, 11, 498, 291, 360, 300, 11, 293, 291, 483, 2793, 510, 11, 613, 366, 439, 264, 2793, 300, 483, 10833, 51072], "temperature": 0.0, "avg_logprob": -0.0930630979866817, "compression_ratio": 1.8173374613003095, "no_speech_prob": 0.004608684219419956}, {"id": 120, "seek": 66692, "start": 681.0799999999999, "end": 685.56, "text": " by that search algorithm, because and most of them just go straight north, because that lowers the", "tokens": [51072, 538, 300, 3164, 9284, 11, 570, 293, 881, 295, 552, 445, 352, 2997, 6830, 11, 570, 300, 44936, 264, 51296], "temperature": 0.0, "avg_logprob": -0.0930630979866817, "compression_ratio": 1.8173374613003095, "no_speech_prob": 0.004608684219419956}, {"id": 121, "seek": 66692, "start": 685.56, "end": 690.76, "text": " distance to the goal. But then they just butt their head against that wall forever. This is a", "tokens": [51296, 4560, 281, 264, 3387, 13, 583, 550, 436, 445, 6660, 641, 1378, 1970, 300, 2929, 5680, 13, 639, 307, 257, 51556], "temperature": 0.0, "avg_logprob": -0.0930630979866817, "compression_ratio": 1.8173374613003095, "no_speech_prob": 0.004608684219419956}, {"id": 122, "seek": 66692, "start": 690.76, "end": 695.88, "text": " classic local optimus, you're familiar with these things in search. However, if you simply switch", "tokens": [51556, 7230, 2654, 5028, 301, 11, 291, 434, 4963, 365, 613, 721, 294, 3164, 13, 2908, 11, 498, 291, 2935, 3679, 51812], "temperature": 0.0, "avg_logprob": -0.0930630979866817, "compression_ratio": 1.8173374613003095, "no_speech_prob": 0.004608684219419956}, {"id": 123, "seek": 69588, "start": 695.88, "end": 700.84, "text": " away from the paradigm of always try to optimize toward a goal, and you just say, let's just go", "tokens": [50364, 1314, 490, 264, 24709, 295, 1009, 853, 281, 19719, 7361, 257, 3387, 11, 293, 291, 445, 584, 11, 718, 311, 445, 352, 50612], "temperature": 0.0, "avg_logprob": -0.09059387950573937, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.000519217923283577}, {"id": 124, "seek": 69588, "start": 700.84, "end": 706.2, "text": " to new places, just seek novelty. That's what you get here. And eventually this search stops", "tokens": [50612, 281, 777, 3190, 11, 445, 8075, 44805, 13, 663, 311, 437, 291, 483, 510, 13, 400, 4728, 341, 3164, 10094, 50880], "temperature": 0.0, "avg_logprob": -0.09059387950573937, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.000519217923283577}, {"id": 125, "seek": 69588, "start": 706.2, "end": 710.76, "text": " focusing on just going north. It doesn't actually care more about north than going east. And eventually", "tokens": [50880, 8416, 322, 445, 516, 6830, 13, 467, 1177, 380, 767, 1127, 544, 466, 6830, 813, 516, 10648, 13, 400, 4728, 51108], "temperature": 0.0, "avg_logprob": -0.09059387950573937, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.000519217923283577}, {"id": 126, "seek": 69588, "start": 710.76, "end": 716.2, "text": " it winds its way around, and it solves the problem. And this right here is a metaphor for every single", "tokens": [51108, 309, 17765, 1080, 636, 926, 11, 293, 309, 39890, 264, 1154, 13, 400, 341, 558, 510, 307, 257, 19157, 337, 633, 2167, 51380], "temperature": 0.0, "avg_logprob": -0.09059387950573937, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.000519217923283577}, {"id": 127, "seek": 69588, "start": 716.2, "end": 722.04, "text": " hard thing we want to do in search. If there are local optimal in space, if we need to explore", "tokens": [51380, 1152, 551, 321, 528, 281, 360, 294, 3164, 13, 759, 456, 366, 2654, 16252, 294, 1901, 11, 498, 321, 643, 281, 6839, 51672], "temperature": 0.0, "avg_logprob": -0.09059387950573937, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.000519217923283577}, {"id": 128, "seek": 72204, "start": 722.04, "end": 726.76, "text": " to discover this thing, then we probably should seek novelty more than an objective. And it's", "tokens": [50364, 281, 4411, 341, 551, 11, 550, 321, 1391, 820, 8075, 44805, 544, 813, 364, 10024, 13, 400, 309, 311, 50600], "temperature": 0.0, "avg_logprob": -0.07251645017553258, "compression_ratio": 1.6763636363636363, "no_speech_prob": 0.003171578049659729}, {"id": 129, "seek": 72204, "start": 726.76, "end": 732.52, "text": " even a metaphor for things beyond algorithmic search. It's also a metaphor for human culture", "tokens": [50600, 754, 257, 19157, 337, 721, 4399, 9284, 299, 3164, 13, 467, 311, 611, 257, 19157, 337, 1952, 3713, 50888], "temperature": 0.0, "avg_logprob": -0.07251645017553258, "compression_ratio": 1.6763636363636363, "no_speech_prob": 0.003171578049659729}, {"id": 130, "seek": 72204, "start": 732.52, "end": 738.36, "text": " and even natural evolution. And the idea is that almost every major scientific breakthrough,", "tokens": [50888, 293, 754, 3303, 9303, 13, 400, 264, 1558, 307, 300, 1920, 633, 2563, 8134, 22397, 11, 51180], "temperature": 0.0, "avg_logprob": -0.07251645017553258, "compression_ratio": 1.6763636363636363, "no_speech_prob": 0.003171578049659729}, {"id": 131, "seek": 72204, "start": 739.4, "end": 743.88, "text": " if you trace its lineage back, it's not a straight path to that solution. Instead,", "tokens": [51232, 498, 291, 13508, 1080, 38257, 646, 11, 309, 311, 406, 257, 2997, 3100, 281, 300, 3827, 13, 7156, 11, 51456], "temperature": 0.0, "avg_logprob": -0.07251645017553258, "compression_ratio": 1.6763636363636363, "no_speech_prob": 0.003171578049659729}, {"id": 132, "seek": 72204, "start": 743.88, "end": 749.0, "text": " it's a winding, circuitous route. So for example, if you went back in time centuries and you said,", "tokens": [51456, 309, 311, 257, 29775, 11, 9048, 563, 7955, 13, 407, 337, 1365, 11, 498, 291, 1437, 646, 294, 565, 13926, 293, 291, 848, 11, 51712], "temperature": 0.0, "avg_logprob": -0.07251645017553258, "compression_ratio": 1.6763636363636363, "no_speech_prob": 0.003171578049659729}, {"id": 133, "seek": 74900, "start": 749.0, "end": 754.68, "text": " I have this way of cooking food, and what I want is a faster way to cook food that doesn't produce", "tokens": [50364, 286, 362, 341, 636, 295, 6361, 1755, 11, 293, 437, 286, 528, 307, 257, 4663, 636, 281, 2543, 1755, 300, 1177, 380, 5258, 50648], "temperature": 0.0, "avg_logprob": -0.0701640182071262, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.002471956890076399}, {"id": 134, "seek": 74900, "start": 754.68, "end": 761.8, "text": " any smoke, then you would never, if you only funded work into improved cooking technology that can", "tokens": [50648, 604, 8439, 11, 550, 291, 576, 1128, 11, 498, 291, 787, 14385, 589, 666, 9689, 6361, 2899, 300, 393, 51004], "temperature": 0.0, "avg_logprob": -0.0701640182071262, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.002471956890076399}, {"id": 135, "seek": 74900, "start": 761.8, "end": 766.76, "text": " accomplish those goals of heating things faster, you would never invent the microwave, which is a", "tokens": [51004, 9021, 729, 5493, 295, 15082, 721, 4663, 11, 291, 576, 1128, 7962, 264, 19025, 11, 597, 307, 257, 51252], "temperature": 0.0, "avg_logprob": -0.0701640182071262, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.002471956890076399}, {"id": 136, "seek": 74900, "start": 766.76, "end": 771.8, "text": " magical invention. Because to invent the microwave, you had to have been working on radar technology", "tokens": [51252, 12066, 22265, 13, 1436, 281, 7962, 264, 19025, 11, 291, 632, 281, 362, 668, 1364, 322, 16544, 2899, 51504], "temperature": 0.0, "avg_logprob": -0.0701640182071262, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.002471956890076399}, {"id": 137, "seek": 74900, "start": 771.8, "end": 776.84, "text": " and recognize the chocolate bar melted in your pocket. Similarly, if you went back millennia", "tokens": [51504, 293, 5521, 264, 6215, 2159, 19057, 294, 428, 8963, 13, 13157, 11, 498, 291, 1437, 646, 21362, 654, 51756], "temperature": 0.0, "avg_logprob": -0.0701640182071262, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.002471956890076399}, {"id": 138, "seek": 77684, "start": 776.84, "end": 782.0400000000001, "text": " to this abacus and you said, that thing does computation, I want more computation. And you", "tokens": [50364, 281, 341, 410, 326, 301, 293, 291, 848, 11, 300, 551, 775, 24903, 11, 286, 528, 544, 24903, 13, 400, 291, 50624], "temperature": 0.0, "avg_logprob": -0.08115758046065227, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0028003775514662266}, {"id": 139, "seek": 77684, "start": 782.0400000000001, "end": 787.08, "text": " only funded researchers who improved against the objective of producing more computation,", "tokens": [50624, 787, 14385, 10309, 567, 9689, 1970, 264, 10024, 295, 10501, 544, 24903, 11, 50876], "temperature": 0.0, "avg_logprob": -0.08115758046065227, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0028003775514662266}, {"id": 140, "seek": 77684, "start": 787.08, "end": 792.9200000000001, "text": " you might get abacuses with like longer rods, more beads, something like that. But you would never", "tokens": [50876, 291, 1062, 483, 410, 326, 8355, 365, 411, 2854, 32761, 11, 544, 20369, 11, 746, 411, 300, 13, 583, 291, 576, 1128, 51168], "temperature": 0.0, "avg_logprob": -0.08115758046065227, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0028003775514662266}, {"id": 141, "seek": 77684, "start": 792.9200000000001, "end": 797.88, "text": " invent the modern computer because to do that, you had to work on things like electricity and", "tokens": [51168, 7962, 264, 4363, 3820, 570, 281, 360, 300, 11, 291, 632, 281, 589, 322, 721, 411, 10356, 293, 51416], "temperature": 0.0, "avg_logprob": -0.08115758046065227, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0028003775514662266}, {"id": 142, "seek": 77684, "start": 797.88, "end": 803.48, "text": " vacuum tubes, which were decidedly not produced because they improved computation, although", "tokens": [51416, 14224, 21458, 11, 597, 645, 3047, 356, 406, 7126, 570, 436, 9689, 24903, 11, 4878, 51696], "temperature": 0.0, "avg_logprob": -0.08115758046065227, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0028003775514662266}, {"id": 143, "seek": 80348, "start": 803.48, "end": 808.76, "text": " they later proved instrumental to doing that. The same is true for going from this kind of energy", "tokens": [50364, 436, 1780, 14617, 17388, 281, 884, 300, 13, 440, 912, 307, 2074, 337, 516, 490, 341, 733, 295, 2281, 50628], "temperature": 0.0, "avg_logprob": -0.051415653403745876, "compression_ratio": 1.8014705882352942, "no_speech_prob": 0.0007095670443959534}, {"id": 144, "seek": 80348, "start": 808.76, "end": 813.0, "text": " to clean energy, where you have to be thinking about things like space and time that were not", "tokens": [50628, 281, 2541, 2281, 11, 689, 291, 362, 281, 312, 1953, 466, 721, 411, 1901, 293, 565, 300, 645, 406, 50840], "temperature": 0.0, "avg_logprob": -0.051415653403745876, "compression_ratio": 1.8014705882352942, "no_speech_prob": 0.0007095670443959534}, {"id": 145, "seek": 80348, "start": 813.0, "end": 819.08, "text": " thought about because they would produce new ways of producing clean energy. So the conjecture here", "tokens": [50840, 1194, 466, 570, 436, 576, 5258, 777, 2098, 295, 10501, 2541, 2281, 13, 407, 264, 416, 1020, 540, 510, 51144], "temperature": 0.0, "avg_logprob": -0.051415653403745876, "compression_ratio": 1.8014705882352942, "no_speech_prob": 0.0007095670443959534}, {"id": 146, "seek": 80348, "start": 819.08, "end": 823.48, "text": " is that the only way to solve really hard problems may be to create problems while you solve them", "tokens": [51144, 307, 300, 264, 787, 636, 281, 5039, 534, 1152, 2740, 815, 312, 281, 1884, 2740, 1339, 291, 5039, 552, 51364], "temperature": 0.0, "avg_logprob": -0.051415653403745876, "compression_ratio": 1.8014705882352942, "no_speech_prob": 0.0007095670443959534}, {"id": 147, "seek": 80348, "start": 823.48, "end": 829.08, "text": " and goals switch between them. And so goal switching is this idea that if you're trying to solve one", "tokens": [51364, 293, 5493, 3679, 1296, 552, 13, 400, 370, 3387, 16493, 307, 341, 1558, 300, 498, 291, 434, 1382, 281, 5039, 472, 51644], "temperature": 0.0, "avg_logprob": -0.051415653403745876, "compression_ratio": 1.8014705882352942, "no_speech_prob": 0.0007095670443959534}, {"id": 148, "seek": 82908, "start": 829.08, "end": 835.32, "text": " task, and you make progress on a different task, then you should also start optimizing and getting", "tokens": [50364, 5633, 11, 293, 291, 652, 4205, 322, 257, 819, 5633, 11, 550, 291, 820, 611, 722, 40425, 293, 1242, 50676], "temperature": 0.0, "avg_logprob": -0.08431639972033801, "compression_ratio": 1.8825757575757576, "no_speech_prob": 0.005729380063712597}, {"id": 149, "seek": 82908, "start": 835.32, "end": 839.96, "text": " better on that different task. So if this robot here, this scientist here wants to make a walking", "tokens": [50676, 1101, 322, 300, 819, 5633, 13, 407, 498, 341, 7881, 510, 11, 341, 12662, 510, 2738, 281, 652, 257, 4494, 50908], "temperature": 0.0, "avg_logprob": -0.08431639972033801, "compression_ratio": 1.8825757575757576, "no_speech_prob": 0.005729380063712597}, {"id": 150, "seek": 82908, "start": 839.96, "end": 844.6, "text": " robot, and all of a sudden during optimization, the robot starts crawling or starts balancing on", "tokens": [50908, 7881, 11, 293, 439, 295, 257, 3990, 1830, 19618, 11, 264, 7881, 3719, 32979, 420, 3719, 22495, 322, 51140], "temperature": 0.0, "avg_logprob": -0.08431639972033801, "compression_ratio": 1.8825757575757576, "no_speech_prob": 0.005729380063712597}, {"id": 151, "seek": 82908, "start": 844.6, "end": 850.84, "text": " one leg, you shouldn't throw that out as a failure because it's not helping you walk or making forward", "tokens": [51140, 472, 1676, 11, 291, 4659, 380, 3507, 300, 484, 382, 257, 7763, 570, 309, 311, 406, 4315, 291, 1792, 420, 1455, 2128, 51452], "temperature": 0.0, "avg_logprob": -0.08431639972033801, "compression_ratio": 1.8825757575757576, "no_speech_prob": 0.005729380063712597}, {"id": 152, "seek": 82908, "start": 850.84, "end": 855.1600000000001, "text": " progress. Instead, you should start getting better at those skills to add those to the set of things", "tokens": [51452, 4205, 13, 7156, 11, 291, 820, 722, 1242, 1101, 412, 729, 3942, 281, 909, 729, 281, 264, 992, 295, 721, 51668], "temperature": 0.0, "avg_logprob": -0.08431639972033801, "compression_ratio": 1.8825757575757576, "no_speech_prob": 0.005729380063712597}, {"id": 153, "seek": 85516, "start": 855.16, "end": 860.12, "text": " that you work on. And ultimately, those might be stepping stones to get you to this walking robot.", "tokens": [50364, 300, 291, 589, 322, 13, 400, 6284, 11, 729, 1062, 312, 16821, 14083, 281, 483, 291, 281, 341, 4494, 7881, 13, 50612], "temperature": 0.0, "avg_logprob": -0.07067050285709714, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.0023962107952684164}, {"id": 154, "seek": 85516, "start": 861.0799999999999, "end": 868.36, "text": " So my colleagues and I have been creating this new subfield of algorithms of AI", "tokens": [50660, 407, 452, 7734, 293, 286, 362, 668, 4084, 341, 777, 1422, 7610, 295, 14642, 295, 7318, 51024], "temperature": 0.0, "avg_logprob": -0.07067050285709714, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.0023962107952684164}, {"id": 155, "seek": 85516, "start": 868.36, "end": 872.76, "text": " called quality diversity algorithms. And this family of algorithms is trying not just to get", "tokens": [51024, 1219, 3125, 8811, 14642, 13, 400, 341, 1605, 295, 14642, 307, 1382, 406, 445, 281, 483, 51244], "temperature": 0.0, "avg_logprob": -0.07067050285709714, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.0023962107952684164}, {"id": 156, "seek": 85516, "start": 872.76, "end": 876.52, "text": " the single best solution to a problem. It's trying to do something very different. It's", "tokens": [51244, 264, 2167, 1151, 3827, 281, 257, 1154, 13, 467, 311, 1382, 281, 360, 746, 588, 819, 13, 467, 311, 51432], "temperature": 0.0, "avg_logprob": -0.07067050285709714, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.0023962107952684164}, {"id": 157, "seek": 85516, "start": 876.52, "end": 883.16, "text": " trying to get a large set of diverse solutions, but where every solution is as good as possible", "tokens": [51432, 1382, 281, 483, 257, 2416, 992, 295, 9521, 6547, 11, 457, 689, 633, 3827, 307, 382, 665, 382, 1944, 51764], "temperature": 0.0, "avg_logprob": -0.07067050285709714, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.0023962107952684164}, {"id": 158, "seek": 88316, "start": 883.16, "end": 889.16, "text": " for that type of solution. You want the tallest in the giraffe or the fastest ant,", "tokens": [50364, 337, 300, 2010, 295, 3827, 13, 509, 528, 264, 42075, 294, 264, 49897, 420, 264, 14573, 2511, 11, 50664], "temperature": 0.0, "avg_logprob": -0.106178170543606, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.0008038277737796307}, {"id": 159, "seek": 88316, "start": 889.16, "end": 894.4399999999999, "text": " but you don't let an ant who's not that fast kind of get precluded by the fact that a cheetah is", "tokens": [50664, 457, 291, 500, 380, 718, 364, 2511, 567, 311, 406, 300, 2370, 733, 295, 483, 4346, 44412, 538, 264, 1186, 300, 257, 947, 47947, 307, 50928], "temperature": 0.0, "avg_logprob": -0.106178170543606, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.0008038277737796307}, {"id": 160, "seek": 88316, "start": 894.4399999999999, "end": 901.8, "text": " faster. You still want the fastest ant and the best ant you can find. So probably the most popular", "tokens": [50928, 4663, 13, 509, 920, 528, 264, 14573, 2511, 293, 264, 1151, 2511, 291, 393, 915, 13, 407, 1391, 264, 881, 3743, 51296], "temperature": 0.0, "avg_logprob": -0.106178170543606, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.0008038277737796307}, {"id": 161, "seek": 88316, "start": 901.8, "end": 906.68, "text": " algorithm in this family at this point is this algorithm called map elites, which was invented", "tokens": [51296, 9284, 294, 341, 1605, 412, 341, 935, 307, 341, 9284, 1219, 4471, 44678, 11, 597, 390, 14479, 51540], "temperature": 0.0, "avg_logprob": -0.106178170543606, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.0008038277737796307}, {"id": 162, "seek": 88316, "start": 906.68, "end": 912.1999999999999, "text": " by Jean-Baptiste Morel, a great colleague and friend of mine, as well as myself in 2015. And", "tokens": [51540, 538, 13854, 12, 33, 2796, 8375, 5048, 75, 11, 257, 869, 13532, 293, 1277, 295, 3892, 11, 382, 731, 382, 2059, 294, 7546, 13, 400, 51816], "temperature": 0.0, "avg_logprob": -0.106178170543606, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.0008038277737796307}, {"id": 163, "seek": 91220, "start": 912.2, "end": 915.72, "text": " it's very, very simple. And the idea here is if you're going to solve a problem,", "tokens": [50364, 309, 311, 588, 11, 588, 2199, 13, 400, 264, 1558, 510, 307, 498, 291, 434, 516, 281, 5039, 257, 1154, 11, 50540], "temperature": 0.0, "avg_logprob": -0.1032760035884273, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0016478549223393202}, {"id": 164, "seek": 91220, "start": 915.72, "end": 922.12, "text": " want to first choose or learn, but we started off by choosing dimensions of interest that you find", "tokens": [50540, 528, 281, 700, 2826, 420, 1466, 11, 457, 321, 1409, 766, 538, 10875, 12819, 295, 1179, 300, 291, 915, 50860], "temperature": 0.0, "avg_logprob": -0.1032760035884273, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0016478549223393202}, {"id": 165, "seek": 91220, "start": 922.12, "end": 926.9200000000001, "text": " that you yourself like. So imagine if you're trying to make a car, for example, you might choose", "tokens": [50860, 300, 291, 1803, 411, 13, 407, 3811, 498, 291, 434, 1382, 281, 652, 257, 1032, 11, 337, 1365, 11, 291, 1062, 2826, 51100], "temperature": 0.0, "avg_logprob": -0.1032760035884273, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0016478549223393202}, {"id": 166, "seek": 91220, "start": 926.9200000000001, "end": 932.2, "text": " safety and fuel efficiency as two dimensions of interest. And then you just discretize these", "tokens": [51100, 4514, 293, 6616, 10493, 382, 732, 12819, 295, 1179, 13, 400, 550, 291, 445, 25656, 1125, 613, 51364], "temperature": 0.0, "avg_logprob": -0.1032760035884273, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0016478549223393202}, {"id": 167, "seek": 91220, "start": 933.32, "end": 937.4000000000001, "text": " dimensions. And you look for the best solution, according to some criteria, like maybe it's the", "tokens": [51420, 12819, 13, 400, 291, 574, 337, 264, 1151, 3827, 11, 4650, 281, 512, 11101, 11, 411, 1310, 309, 311, 264, 51624], "temperature": 0.0, "avg_logprob": -0.1032760035884273, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0016478549223393202}, {"id": 168, "seek": 93740, "start": 937.4, "end": 943.4, "text": " fastest car at each point in this grid. And what you want at the end of the day is not just to get", "tokens": [50364, 14573, 1032, 412, 1184, 935, 294, 341, 10748, 13, 400, 437, 291, 528, 412, 264, 917, 295, 264, 786, 307, 406, 445, 281, 483, 50664], "temperature": 0.0, "avg_logprob": -0.06917700145555579, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0006262772367335856}, {"id": 169, "seek": 93740, "start": 943.4, "end": 949.64, "text": " the fastest car possible, but the fastest car for every possible tradeoff between safety and fuel", "tokens": [50664, 264, 14573, 1032, 1944, 11, 457, 264, 14573, 1032, 337, 633, 1944, 4923, 4506, 1296, 4514, 293, 6616, 50976], "temperature": 0.0, "avg_logprob": -0.06917700145555579, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0006262772367335856}, {"id": 170, "seek": 93740, "start": 949.64, "end": 956.12, "text": " efficiency. So here's an example problem we tried this on. This is generating soft robot morphologies,", "tokens": [50976, 10493, 13, 407, 510, 311, 364, 1365, 1154, 321, 3031, 341, 322, 13, 639, 307, 17746, 2787, 7881, 25778, 6204, 11, 51300], "temperature": 0.0, "avg_logprob": -0.06917700145555579, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0006262772367335856}, {"id": 171, "seek": 93740, "start": 956.12, "end": 961.64, "text": " which is like the bodies of robots. So we gave this optimization algorithm those four materials", "tokens": [51300, 597, 307, 411, 264, 7510, 295, 14733, 13, 407, 321, 2729, 341, 19618, 9284, 729, 1451, 5319, 51576], "temperature": 0.0, "avg_logprob": -0.06917700145555579, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0006262772367335856}, {"id": 172, "seek": 93740, "start": 961.64, "end": 966.1999999999999, "text": " there. They're kind of voxels that can pulse at different times. And some are soft and some are", "tokens": [51576, 456, 13, 814, 434, 733, 295, 1650, 87, 1625, 300, 393, 17709, 412, 819, 1413, 13, 400, 512, 366, 2787, 293, 512, 366, 51804], "temperature": 0.0, "avg_logprob": -0.06917700145555579, "compression_ratio": 1.7288732394366197, "no_speech_prob": 0.0006262772367335856}, {"id": 173, "seek": 96620, "start": 966.2, "end": 973.08, "text": " hard. And we said, you know, go fast. And, you know, first we did this without map elites,", "tokens": [50364, 1152, 13, 400, 321, 848, 11, 291, 458, 11, 352, 2370, 13, 400, 11, 291, 458, 11, 700, 321, 630, 341, 1553, 4471, 44678, 11, 50708], "temperature": 0.0, "avg_logprob": -0.09825711738406204, "compression_ratio": 1.8745644599303135, "no_speech_prob": 0.00039198517333716154}, {"id": 174, "seek": 96620, "start": 973.08, "end": 977.08, "text": " we just did this with a canonical optimization algorithm or a genetic algorithm in this case,", "tokens": [50708, 321, 445, 630, 341, 365, 257, 46491, 19618, 9284, 420, 257, 12462, 9284, 294, 341, 1389, 11, 50908], "temperature": 0.0, "avg_logprob": -0.09825711738406204, "compression_ratio": 1.8745644599303135, "no_speech_prob": 0.00039198517333716154}, {"id": 175, "seek": 96620, "start": 977.08, "end": 982.2800000000001, "text": " which is just trying to optimize for speed. And what you see here is this kind of really", "tokens": [50908, 597, 307, 445, 1382, 281, 19719, 337, 3073, 13, 400, 437, 291, 536, 510, 307, 341, 733, 295, 534, 51168], "temperature": 0.0, "avg_logprob": -0.09825711738406204, "compression_ratio": 1.8745644599303135, "no_speech_prob": 0.00039198517333716154}, {"id": 176, "seek": 96620, "start": 982.2800000000001, "end": 986.9200000000001, "text": " interesting parade, this Noah's Ark of very different solutions and very different creatures.", "tokens": [51168, 1880, 26128, 11, 341, 20895, 311, 16427, 295, 588, 819, 6547, 293, 588, 819, 12281, 13, 51400], "temperature": 0.0, "avg_logprob": -0.09825711738406204, "compression_ratio": 1.8745644599303135, "no_speech_prob": 0.00039198517333716154}, {"id": 177, "seek": 96620, "start": 987.8000000000001, "end": 992.12, "text": " And, you know, people got really excited when we put this online and it's super fun.", "tokens": [51444, 400, 11, 291, 458, 11, 561, 658, 534, 2919, 562, 321, 829, 341, 2950, 293, 309, 311, 1687, 1019, 13, 51660], "temperature": 0.0, "avg_logprob": -0.09825711738406204, "compression_ratio": 1.8745644599303135, "no_speech_prob": 0.00039198517333716154}, {"id": 178, "seek": 96620, "start": 992.12, "end": 995.4000000000001, "text": " But I think one of the things that people thought really interesting about this work,", "tokens": [51660, 583, 286, 519, 472, 295, 264, 721, 300, 561, 1194, 534, 1880, 466, 341, 589, 11, 51824], "temperature": 0.0, "avg_logprob": -0.09825711738406204, "compression_ratio": 1.8745644599303135, "no_speech_prob": 0.00039198517333716154}, {"id": 179, "seek": 99540, "start": 995.4, "end": 1001.0, "text": " including myself is the huge diversity of designs that you see here. You know, it starts to evoke", "tokens": [50364, 3009, 2059, 307, 264, 2603, 8811, 295, 11347, 300, 291, 536, 510, 13, 509, 458, 11, 309, 3719, 281, 1073, 2949, 50644], "temperature": 0.0, "avg_logprob": -0.07572570340386753, "compression_ratio": 1.7394366197183098, "no_speech_prob": 0.00031500982004217803}, {"id": 180, "seek": 99540, "start": 1001.0, "end": 1005.9599999999999, "text": " nature where you see a lot of different designs. The problem is there is a trick to this. And that", "tokens": [50644, 3687, 689, 291, 536, 257, 688, 295, 819, 11347, 13, 440, 1154, 307, 456, 307, 257, 4282, 281, 341, 13, 400, 300, 50892], "temperature": 0.0, "avg_logprob": -0.07572570340386753, "compression_ratio": 1.7394366197183098, "no_speech_prob": 0.00031500982004217803}, {"id": 181, "seek": 99540, "start": 1005.9599999999999, "end": 1011.64, "text": " is that all of the designs that you just saw, each of those came from a different run of optimization.", "tokens": [50892, 307, 300, 439, 295, 264, 11347, 300, 291, 445, 1866, 11, 1184, 295, 729, 1361, 490, 257, 819, 1190, 295, 19618, 13, 51176], "temperature": 0.0, "avg_logprob": -0.07572570340386753, "compression_ratio": 1.7394366197183098, "no_speech_prob": 0.00031500982004217803}, {"id": 182, "seek": 99540, "start": 1011.64, "end": 1017.16, "text": " The only way you got a diversity was by starting the run again and doing a massive search to find", "tokens": [51176, 440, 787, 636, 291, 658, 257, 8811, 390, 538, 2891, 264, 1190, 797, 293, 884, 257, 5994, 3164, 281, 915, 51452], "temperature": 0.0, "avg_logprob": -0.07572570340386753, "compression_ratio": 1.7394366197183098, "no_speech_prob": 0.00031500982004217803}, {"id": 183, "seek": 99540, "start": 1017.16, "end": 1021.88, "text": " one solution. But if you look within that population of creatures, they're all almost identical.", "tokens": [51452, 472, 3827, 13, 583, 498, 291, 574, 1951, 300, 4415, 295, 12281, 11, 436, 434, 439, 1920, 14800, 13, 51688], "temperature": 0.0, "avg_logprob": -0.07572570340386753, "compression_ratio": 1.7394366197183098, "no_speech_prob": 0.00031500982004217803}, {"id": 184, "seek": 102188, "start": 1021.88, "end": 1025.88, "text": " And that's not what we want. What we want on is an algorithm that will generate a huge diversity", "tokens": [50364, 400, 300, 311, 406, 437, 321, 528, 13, 708, 321, 528, 322, 307, 364, 9284, 300, 486, 8460, 257, 2603, 8811, 50564], "temperature": 0.0, "avg_logprob": -0.10615456373171699, "compression_ratio": 1.75625, "no_speech_prob": 0.00037993842852301896}, {"id": 185, "seek": 102188, "start": 1025.88, "end": 1029.8, "text": " of things within one run so that you can run it for billions of years and it would continue to", "tokens": [50564, 295, 721, 1951, 472, 1190, 370, 300, 291, 393, 1190, 309, 337, 17375, 295, 924, 293, 309, 576, 2354, 281, 50760], "temperature": 0.0, "avg_logprob": -0.10615456373171699, "compression_ratio": 1.75625, "no_speech_prob": 0.00037993842852301896}, {"id": 186, "seek": 102188, "start": 1029.8, "end": 1034.52, "text": " produce interesting new stuff as opposed to converging to one type of solution and getting", "tokens": [50760, 5258, 1880, 777, 1507, 382, 8851, 281, 9652, 3249, 281, 472, 2010, 295, 3827, 293, 1242, 50996], "temperature": 0.0, "avg_logprob": -0.10615456373171699, "compression_ratio": 1.75625, "no_speech_prob": 0.00037993842852301896}, {"id": 187, "seek": 102188, "start": 1034.52, "end": 1039.24, "text": " stuck on that kind of local optimal. So we took the map elites algorithm that I just described", "tokens": [50996, 5541, 322, 300, 733, 295, 2654, 16252, 13, 407, 321, 1890, 264, 4471, 44678, 9284, 300, 286, 445, 7619, 51232], "temperature": 0.0, "avg_logprob": -0.10615456373171699, "compression_ratio": 1.75625, "no_speech_prob": 0.00037993842852301896}, {"id": 188, "seek": 102188, "start": 1039.24, "end": 1044.76, "text": " to you and we applied it to the same software last problem. And what we did there, you know,", "tokens": [51232, 281, 291, 293, 321, 6456, 309, 281, 264, 912, 4722, 1036, 1154, 13, 400, 437, 321, 630, 456, 11, 291, 458, 11, 51508], "temperature": 0.0, "avg_logprob": -0.10615456373171699, "compression_ratio": 1.75625, "no_speech_prob": 0.00037993842852301896}, {"id": 189, "seek": 102188, "start": 1044.76, "end": 1049.32, "text": " is we have to pick the dimensions and we chose to pick the number of voxels and then amount", "tokens": [51508, 307, 321, 362, 281, 1888, 264, 12819, 293, 321, 5111, 281, 1888, 264, 1230, 295, 1650, 87, 1625, 293, 550, 2372, 51736], "temperature": 0.0, "avg_logprob": -0.10615456373171699, "compression_ratio": 1.75625, "no_speech_prob": 0.00037993842852301896}, {"id": 190, "seek": 104932, "start": 1049.3999999999999, "end": 1054.4399999999998, "text": " of this dark blue material because previously it hadn't been using this kind of bone-like material", "tokens": [50368, 295, 341, 2877, 3344, 2527, 570, 8046, 309, 8782, 380, 668, 1228, 341, 733, 295, 9026, 12, 4092, 2527, 50620], "temperature": 0.0, "avg_logprob": -0.106861029098283, "compression_ratio": 1.75, "no_speech_prob": 0.002115262672305107}, {"id": 191, "seek": 104932, "start": 1054.4399999999998, "end": 1059.96, "text": " and we wanted to see it play with that resource more. And if you look at classic optimization,", "tokens": [50620, 293, 321, 1415, 281, 536, 309, 862, 365, 300, 7684, 544, 13, 400, 498, 291, 574, 412, 7230, 19618, 11, 50896], "temperature": 0.0, "avg_logprob": -0.106861029098283, "compression_ratio": 1.75, "no_speech_prob": 0.002115262672305107}, {"id": 192, "seek": 104932, "start": 1059.96, "end": 1064.28, "text": " this could have been RL, but in this case it's a genetic algorithm. Any optimization,", "tokens": [50896, 341, 727, 362, 668, 497, 43, 11, 457, 294, 341, 1389, 309, 311, 257, 12462, 9284, 13, 2639, 19618, 11, 51112], "temperature": 0.0, "avg_logprob": -0.106861029098283, "compression_ratio": 1.75, "no_speech_prob": 0.002115262672305107}, {"id": 193, "seek": 104932, "start": 1064.28, "end": 1068.9199999999998, "text": " what you find is that it doesn't actually search the space very well. And so it has low performing", "tokens": [51112, 437, 291, 915, 307, 300, 309, 1177, 380, 767, 3164, 264, 1901, 588, 731, 13, 400, 370, 309, 575, 2295, 10205, 51344], "temperature": 0.0, "avg_logprob": -0.106861029098283, "compression_ratio": 1.75, "no_speech_prob": 0.002115262672305107}, {"id": 194, "seek": 104932, "start": 1068.9199999999998, "end": 1073.72, "text": " points and it didn't do a lot of exploration. If you add diversity, which we know historically", "tokens": [51344, 2793, 293, 309, 994, 380, 360, 257, 688, 295, 16197, 13, 759, 291, 909, 8811, 11, 597, 321, 458, 16180, 51584], "temperature": 0.0, "avg_logprob": -0.106861029098283, "compression_ratio": 1.75, "no_speech_prob": 0.002115262672305107}, {"id": 195, "seek": 104932, "start": 1073.72, "end": 1078.36, "text": " helps, you do get higher performing points. So you see these yellow points here, but it still", "tokens": [51584, 3665, 11, 291, 360, 483, 2946, 10205, 2793, 13, 407, 291, 536, 613, 5566, 2793, 510, 11, 457, 309, 920, 51816], "temperature": 0.0, "avg_logprob": -0.106861029098283, "compression_ratio": 1.75, "no_speech_prob": 0.002115262672305107}, {"id": 196, "seek": 107836, "start": 1078.36, "end": 1082.52, "text": " did not explore a lot of the space, even though it's incentivized to literally explore in these", "tokens": [50364, 630, 406, 6839, 257, 688, 295, 264, 1901, 11, 754, 1673, 309, 311, 35328, 1602, 281, 3736, 6839, 294, 613, 50572], "temperature": 0.0, "avg_logprob": -0.06261652752869111, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.000882946711499244}, {"id": 197, "seek": 107836, "start": 1082.52, "end": 1088.4399999999998, "text": " two dimensions. Map elites is a qualitatively different algorithm. It's a sea change in terms", "tokens": [50572, 732, 12819, 13, 22053, 44678, 307, 257, 31312, 356, 819, 9284, 13, 467, 311, 257, 4158, 1319, 294, 2115, 50868], "temperature": 0.0, "avg_logprob": -0.06261652752869111, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.000882946711499244}, {"id": 198, "seek": 107836, "start": 1088.4399999999998, "end": 1092.84, "text": " of what happens within the algorithm. If you look here, you see this rich exploration where it", "tokens": [50868, 295, 437, 2314, 1951, 264, 9284, 13, 759, 291, 574, 510, 11, 291, 536, 341, 4593, 16197, 689, 309, 51088], "temperature": 0.0, "avg_logprob": -0.06261652752869111, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.000882946711499244}, {"id": 199, "seek": 107836, "start": 1092.84, "end": 1098.04, "text": " fanned out and searched the entire search space and it taught you more about this search space.", "tokens": [51088, 283, 5943, 484, 293, 22961, 264, 2302, 3164, 1901, 293, 309, 5928, 291, 544, 466, 341, 3164, 1901, 13, 51348], "temperature": 0.0, "avg_logprob": -0.06261652752869111, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.000882946711499244}, {"id": 200, "seek": 107836, "start": 1098.04, "end": 1102.4399999999998, "text": " It tells you, hey, there's not very high performing points up here. There's a little bunch of optima", "tokens": [51348, 467, 5112, 291, 11, 4177, 11, 456, 311, 406, 588, 1090, 10205, 2793, 493, 510, 13, 821, 311, 257, 707, 3840, 295, 2427, 4775, 51568], "temperature": 0.0, "avg_logprob": -0.06261652752869111, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.000882946711499244}, {"id": 201, "seek": 107836, "start": 1102.4399999999998, "end": 1106.28, "text": " over here. There's also this separate little area here that you probably would never have normally", "tokens": [51568, 670, 510, 13, 821, 311, 611, 341, 4994, 707, 1859, 510, 300, 291, 1391, 576, 1128, 362, 5646, 51760], "temperature": 0.0, "avg_logprob": -0.06261652752869111, "compression_ratio": 1.7956656346749227, "no_speech_prob": 0.000882946711499244}, {"id": 202, "seek": 110628, "start": 1106.28, "end": 1110.44, "text": " found, et cetera, et cetera. I'm doing these interesting points over here that you can go", "tokens": [50364, 1352, 11, 1030, 11458, 11, 1030, 11458, 13, 286, 478, 884, 613, 1880, 2793, 670, 510, 300, 291, 393, 352, 50572], "temperature": 0.0, "avg_logprob": -0.08597291557534227, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.00037994986632838845}, {"id": 203, "seek": 110628, "start": 1110.44, "end": 1116.68, "text": " investigate. And what's interesting is it often finds a better overall high performing solution", "tokens": [50572, 15013, 13, 400, 437, 311, 1880, 307, 309, 2049, 10704, 257, 1101, 4787, 1090, 10205, 3827, 50884], "temperature": 0.0, "avg_logprob": -0.08597291557534227, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.00037994986632838845}, {"id": 204, "seek": 110628, "start": 1116.68, "end": 1121.3999999999999, "text": " than if you just do direct optimization because it's doing such a better job of exploring the", "tokens": [50884, 813, 498, 291, 445, 360, 2047, 19618, 570, 309, 311, 884, 1270, 257, 1101, 1691, 295, 12736, 264, 51120], "temperature": 0.0, "avg_logprob": -0.08597291557534227, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.00037994986632838845}, {"id": 205, "seek": 110628, "start": 1121.3999999999999, "end": 1128.28, "text": " space of possibilities. So if you look at any individual final point, you can trace back its", "tokens": [51120, 1901, 295, 12178, 13, 407, 498, 291, 574, 412, 604, 2609, 2572, 935, 11, 291, 393, 13508, 646, 1080, 51464], "temperature": 0.0, "avg_logprob": -0.08597291557534227, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.00037994986632838845}, {"id": 206, "seek": 110628, "start": 1128.28, "end": 1134.28, "text": " lineage through time to see where those solutions visited in the search space. And what you can", "tokens": [51464, 38257, 807, 565, 281, 536, 689, 729, 6547, 11220, 294, 264, 3164, 1901, 13, 400, 437, 291, 393, 51764], "temperature": 0.0, "avg_logprob": -0.08597291557534227, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.00037994986632838845}, {"id": 207, "seek": 113428, "start": 1134.28, "end": 1139.32, "text": " see here is that they don't just kind of mine one area of the space and get better and better and", "tokens": [50364, 536, 510, 307, 300, 436, 500, 380, 445, 733, 295, 3892, 472, 1859, 295, 264, 1901, 293, 483, 1101, 293, 1101, 293, 50616], "temperature": 0.0, "avg_logprob": -0.10618896054145985, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.0004582781984936446}, {"id": 208, "seek": 113428, "start": 1139.32, "end": 1143.3999999999999, "text": " better at that corner of the search space, that particular tradeoff between these two dimensions.", "tokens": [50616, 1101, 412, 300, 4538, 295, 264, 3164, 1901, 11, 300, 1729, 4923, 4506, 1296, 613, 732, 12819, 13, 50820], "temperature": 0.0, "avg_logprob": -0.10618896054145985, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.0004582781984936446}, {"id": 209, "seek": 113428, "start": 1143.3999999999999, "end": 1149.24, "text": " But instead, the overall lineage takes these long, circuitous paths to their final destination.", "tokens": [50820, 583, 2602, 11, 264, 4787, 38257, 2516, 613, 938, 11, 9048, 563, 14518, 281, 641, 2572, 12236, 13, 51112], "temperature": 0.0, "avg_logprob": -0.10618896054145985, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.0004582781984936446}, {"id": 210, "seek": 113428, "start": 1149.24, "end": 1153.8, "text": " Just as to get a human, you had to go through an intermediate stage of being a tapeworm and then", "tokens": [51112, 1449, 382, 281, 483, 257, 1952, 11, 291, 632, 281, 352, 807, 364, 19376, 3233, 295, 885, 257, 7314, 30917, 293, 550, 51340], "temperature": 0.0, "avg_logprob": -0.10618896054145985, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.0004582781984936446}, {"id": 211, "seek": 113428, "start": 1153.8, "end": 1158.04, "text": " being like a tree dwelling. Actually, I don't know if we were a tree doubling, but kind of", "tokens": [51340, 885, 411, 257, 4230, 41750, 13, 5135, 11, 286, 500, 380, 458, 498, 321, 645, 257, 4230, 33651, 11, 457, 733, 295, 51552], "temperature": 0.0, "avg_logprob": -0.10618896054145985, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.0004582781984936446}, {"id": 212, "seek": 113428, "start": 1158.04, "end": 1162.84, "text": " something that looked more like an ape and all sorts of intermediate steps along the way.", "tokens": [51552, 746, 300, 2956, 544, 411, 364, 44315, 293, 439, 7527, 295, 19376, 4439, 2051, 264, 636, 13, 51792], "temperature": 0.0, "avg_logprob": -0.10618896054145985, "compression_ratio": 1.8006329113924051, "no_speech_prob": 0.0004582781984936446}, {"id": 213, "seek": 116284, "start": 1163.6399999999999, "end": 1169.32, "text": " So going back to the idea of an innovation engine, we now can recognize the algorithm that we're", "tokens": [50404, 407, 516, 646, 281, 264, 1558, 295, 364, 8504, 2848, 11, 321, 586, 393, 5521, 264, 9284, 300, 321, 434, 50688], "temperature": 0.0, "avg_logprob": -0.07806564512706939, "compression_ratio": 1.7893081761006289, "no_speech_prob": 0.0003249878063797951}, {"id": 214, "seek": 116284, "start": 1169.32, "end": 1173.56, "text": " going to use here. There's one final thing I need to tell you about, which is how are we going to", "tokens": [50688, 516, 281, 764, 510, 13, 821, 311, 472, 2572, 551, 286, 643, 281, 980, 291, 466, 11, 597, 307, 577, 366, 321, 516, 281, 50900], "temperature": 0.0, "avg_logprob": -0.07806564512706939, "compression_ratio": 1.7893081761006289, "no_speech_prob": 0.0003249878063797951}, {"id": 215, "seek": 116284, "start": 1173.56, "end": 1178.04, "text": " encode the images we're going to search for. And I'm going to tell you what I mean by the word", "tokens": [50900, 2058, 1429, 264, 5267, 321, 434, 516, 281, 3164, 337, 13, 400, 286, 478, 516, 281, 980, 291, 437, 286, 914, 538, 264, 1349, 51124], "temperature": 0.0, "avg_logprob": -0.07806564512706939, "compression_ratio": 1.7893081761006289, "no_speech_prob": 0.0003249878063797951}, {"id": 216, "seek": 116284, "start": 1178.04, "end": 1181.8799999999999, "text": " encoding, because I think especially for people who are interested in aesthetics, this is one of", "tokens": [51124, 43430, 11, 570, 286, 519, 2318, 337, 561, 567, 366, 3102, 294, 35517, 11, 341, 307, 472, 295, 51316], "temperature": 0.0, "avg_logprob": -0.07806564512706939, "compression_ratio": 1.7893081761006289, "no_speech_prob": 0.0003249878063797951}, {"id": 217, "seek": 116284, "start": 1181.8799999999999, "end": 1187.1599999999999, "text": " the most important choices you can make. And you'll see this show up in Joel's work later as well.", "tokens": [51316, 264, 881, 1021, 7994, 291, 393, 652, 13, 400, 291, 603, 536, 341, 855, 493, 294, 21522, 311, 589, 1780, 382, 731, 13, 51580], "temperature": 0.0, "avg_logprob": -0.07806564512706939, "compression_ratio": 1.7893081761006289, "no_speech_prob": 0.0003249878063797951}, {"id": 218, "seek": 116284, "start": 1187.1599999999999, "end": 1190.9199999999998, "text": " So I'm going to tell you about the encoding that we use, which is a CPPN. So first,", "tokens": [51580, 407, 286, 478, 516, 281, 980, 291, 466, 264, 43430, 300, 321, 764, 11, 597, 307, 257, 383, 17755, 45, 13, 407, 700, 11, 51768], "temperature": 0.0, "avg_logprob": -0.07806564512706939, "compression_ratio": 1.7893081761006289, "no_speech_prob": 0.0003249878063797951}, {"id": 219, "seek": 119092, "start": 1190.92, "end": 1194.6000000000001, "text": " I've been throwing around these terms, genetic algorithm and evolutionary algorithms. You may", "tokens": [50364, 286, 600, 668, 10238, 926, 613, 2115, 11, 12462, 9284, 293, 27567, 14642, 13, 509, 815, 50548], "temperature": 0.0, "avg_logprob": -0.0902174290488748, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.0005702344351448119}, {"id": 220, "seek": 119092, "start": 1194.6000000000001, "end": 1200.28, "text": " not know what they are. I'm going to very briefly explain them. If you want to search for a problem,", "tokens": [50548, 406, 458, 437, 436, 366, 13, 286, 478, 516, 281, 588, 10515, 2903, 552, 13, 759, 291, 528, 281, 3164, 337, 257, 1154, 11, 50832], "temperature": 0.0, "avg_logprob": -0.0902174290488748, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.0005702344351448119}, {"id": 221, "seek": 119092, "start": 1200.28, "end": 1203.88, "text": " this is also true in deep learning. The first choice you have to make is how to encode the", "tokens": [50832, 341, 307, 611, 2074, 294, 2452, 2539, 13, 440, 700, 3922, 291, 362, 281, 652, 307, 577, 281, 2058, 1429, 264, 51012], "temperature": 0.0, "avg_logprob": -0.0902174290488748, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.0005702344351448119}, {"id": 222, "seek": 119092, "start": 1203.88, "end": 1208.28, "text": " problem. So imagine if you wanted to search for tables. Well, you could decide I'm going to store", "tokens": [51012, 1154, 13, 407, 3811, 498, 291, 1415, 281, 3164, 337, 8020, 13, 1042, 11, 291, 727, 4536, 286, 478, 516, 281, 3531, 51232], "temperature": 0.0, "avg_logprob": -0.0902174290488748, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.0005702344351448119}, {"id": 223, "seek": 119092, "start": 1208.28, "end": 1213.64, "text": " the length of each leg separately as a number on a parameter vector. We in evolutionary algorithms,", "tokens": [51232, 264, 4641, 295, 1184, 1676, 14759, 382, 257, 1230, 322, 257, 13075, 8062, 13, 492, 294, 27567, 14642, 11, 51500], "temperature": 0.0, "avg_logprob": -0.0902174290488748, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.0005702344351448119}, {"id": 224, "seek": 119092, "start": 1213.64, "end": 1217.72, "text": " we call this a genome, but in deep machine learning, it's often called a parameter vector.", "tokens": [51500, 321, 818, 341, 257, 21953, 11, 457, 294, 2452, 3479, 2539, 11, 309, 311, 2049, 1219, 257, 13075, 8062, 13, 51704], "temperature": 0.0, "avg_logprob": -0.0902174290488748, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.0005702344351448119}, {"id": 225, "seek": 121772, "start": 1217.72, "end": 1221.48, "text": " So you store the length of each leg separately and the width and the length of the surface of", "tokens": [50364, 407, 291, 3531, 264, 4641, 295, 1184, 1676, 14759, 293, 264, 11402, 293, 264, 4641, 295, 264, 3753, 295, 50552], "temperature": 0.0, "avg_logprob": -0.0745212031948951, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.00023048628645483404}, {"id": 226, "seek": 121772, "start": 1221.48, "end": 1226.76, "text": " the table maybe on this string of numbers, this parameter vector. Once you've made that encoding", "tokens": [50552, 264, 3199, 1310, 322, 341, 6798, 295, 3547, 11, 341, 13075, 8062, 13, 3443, 291, 600, 1027, 300, 43430, 50816], "temperature": 0.0, "avg_logprob": -0.0745212031948951, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.00023048628645483404}, {"id": 227, "seek": 121772, "start": 1226.76, "end": 1231.8, "text": " choice, you then can score the population. First, you create a population at random by generating", "tokens": [50816, 3922, 11, 291, 550, 393, 6175, 264, 4415, 13, 2386, 11, 291, 1884, 257, 4415, 412, 4974, 538, 17746, 51068], "temperature": 0.0, "avg_logprob": -0.0745212031948951, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.00023048628645483404}, {"id": 228, "seek": 121772, "start": 1231.8, "end": 1235.96, "text": " random strings of numbers. You score this population to see how good they are. You select", "tokens": [51068, 4974, 13985, 295, 3547, 13, 509, 6175, 341, 4415, 281, 536, 577, 665, 436, 366, 13, 509, 3048, 51276], "temperature": 0.0, "avg_logprob": -0.0745212031948951, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.00023048628645483404}, {"id": 229, "seek": 121772, "start": 1235.96, "end": 1240.68, "text": " which ones are better according to some scoring function, which could be your reward function.", "tokens": [51276, 597, 2306, 366, 1101, 4650, 281, 512, 22358, 2445, 11, 597, 727, 312, 428, 7782, 2445, 13, 51512], "temperature": 0.0, "avg_logprob": -0.0745212031948951, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.00023048628645483404}, {"id": 230, "seek": 121772, "start": 1240.68, "end": 1246.04, "text": " And then you just take these things here, take their parameter vectors, and you perturb them", "tokens": [51512, 400, 550, 291, 445, 747, 613, 721, 510, 11, 747, 641, 13075, 18875, 11, 293, 291, 40468, 552, 51780], "temperature": 0.0, "avg_logprob": -0.0745212031948951, "compression_ratio": 1.8929765886287626, "no_speech_prob": 0.00023048628645483404}, {"id": 231, "seek": 124604, "start": 1246.04, "end": 1251.3999999999999, "text": " in a little way somehow. And then you get a new thing and then you repeat the process.", "tokens": [50364, 294, 257, 707, 636, 6063, 13, 400, 550, 291, 483, 257, 777, 551, 293, 550, 291, 7149, 264, 1399, 13, 50632], "temperature": 0.0, "avg_logprob": -0.11316659909869553, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.00019714001973625273}, {"id": 232, "seek": 124604, "start": 1251.3999999999999, "end": 1254.2, "text": " In the gradient-based method, this is kind of like where you take the learning", "tokens": [50632, 682, 264, 16235, 12, 6032, 3170, 11, 341, 307, 733, 295, 411, 689, 291, 747, 264, 2539, 50772], "temperature": 0.0, "avg_logprob": -0.11316659909869553, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.00019714001973625273}, {"id": 233, "seek": 124604, "start": 1254.2, "end": 1259.0, "text": " step based on the gradient of the scoring function. And then you repeat the problem.", "tokens": [50772, 1823, 2361, 322, 264, 16235, 295, 264, 22358, 2445, 13, 400, 550, 291, 7149, 264, 1154, 13, 51012], "temperature": 0.0, "avg_logprob": -0.11316659909869553, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.00019714001973625273}, {"id": 234, "seek": 124604, "start": 1259.0, "end": 1264.04, "text": " So when I talk about an encoding, it's this first choice, which is how do we decide what is the search", "tokens": [51012, 407, 562, 286, 751, 466, 364, 43430, 11, 309, 311, 341, 700, 3922, 11, 597, 307, 577, 360, 321, 4536, 437, 307, 264, 3164, 51264], "temperature": 0.0, "avg_logprob": -0.11316659909869553, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.00019714001973625273}, {"id": 235, "seek": 124604, "start": 1264.04, "end": 1269.72, "text": " space that we will search in the parameter vector and how does that map to the final solution?", "tokens": [51264, 1901, 300, 321, 486, 3164, 294, 264, 13075, 8062, 293, 577, 775, 300, 4471, 281, 264, 2572, 3827, 30, 51548], "temperature": 0.0, "avg_logprob": -0.11316659909869553, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.00019714001973625273}, {"id": 236, "seek": 126972, "start": 1269.8, "end": 1276.2, "text": " And that is in evolutionary language, the process of going from a genotype to a phenotype,", "tokens": [50368, 400, 300, 307, 294, 27567, 2856, 11, 264, 1399, 295, 516, 490, 257, 1049, 13108, 281, 257, 7279, 13108, 11, 50688], "temperature": 0.0, "avg_logprob": -0.10347029796013466, "compression_ratio": 1.7890625, "no_speech_prob": 0.0010003704810515046}, {"id": 237, "seek": 126972, "start": 1276.84, "end": 1280.76, "text": " or machine learning a parameter vector to a final agent or policy or artifact.", "tokens": [50720, 420, 3479, 2539, 257, 13075, 8062, 281, 257, 2572, 9461, 420, 3897, 420, 34806, 13, 50916], "temperature": 0.0, "avg_logprob": -0.10347029796013466, "compression_ratio": 1.7890625, "no_speech_prob": 0.0010003704810515046}, {"id": 238, "seek": 126972, "start": 1281.64, "end": 1287.48, "text": " So there is this notion of a direct encoding versus a generative encoding. And a direct encoding,", "tokens": [50960, 407, 456, 307, 341, 10710, 295, 257, 2047, 43430, 5717, 257, 1337, 1166, 43430, 13, 400, 257, 2047, 43430, 11, 51252], "temperature": 0.0, "avg_logprob": -0.10347029796013466, "compression_ratio": 1.7890625, "no_speech_prob": 0.0010003704810515046}, {"id": 239, "seek": 126972, "start": 1287.48, "end": 1292.52, "text": " you basically have one number on your parameter vector for every single thing in your final artifact.", "tokens": [51252, 291, 1936, 362, 472, 1230, 322, 428, 13075, 8062, 337, 633, 2167, 551, 294, 428, 2572, 34806, 13, 51504], "temperature": 0.0, "avg_logprob": -0.10347029796013466, "compression_ratio": 1.7890625, "no_speech_prob": 0.0010003704810515046}, {"id": 240, "seek": 126972, "start": 1292.52, "end": 1296.2, "text": " So if you're searching for the weights of a neural net, then you search separately for a", "tokens": [51504, 407, 498, 291, 434, 10808, 337, 264, 17443, 295, 257, 18161, 2533, 11, 550, 291, 3164, 14759, 337, 257, 51688], "temperature": 0.0, "avg_logprob": -0.10347029796013466, "compression_ratio": 1.7890625, "no_speech_prob": 0.0010003704810515046}, {"id": 241, "seek": 129620, "start": 1296.2, "end": 1299.96, "text": " number for each weight or for a table you search separately for the length of each leg.", "tokens": [50364, 1230, 337, 1184, 3364, 420, 337, 257, 3199, 291, 3164, 14759, 337, 264, 4641, 295, 1184, 1676, 13, 50552], "temperature": 0.0, "avg_logprob": -0.08828670199554746, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.00019715665257535875}, {"id": 242, "seek": 129620, "start": 1300.76, "end": 1304.92, "text": " If you think about how perturbations affect these parameter vectors, though,", "tokens": [50592, 759, 291, 519, 466, 577, 40468, 763, 3345, 613, 13075, 18875, 11, 1673, 11, 50800], "temperature": 0.0, "avg_logprob": -0.08828670199554746, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.00019715665257535875}, {"id": 243, "seek": 129620, "start": 1304.92, "end": 1311.48, "text": " they are mostly likely to produce non-regular phenotypes. So most changes are not going to", "tokens": [50800, 436, 366, 5240, 3700, 281, 5258, 2107, 12, 26713, 7279, 19477, 13, 407, 881, 2962, 366, 406, 516, 281, 51128], "temperature": 0.0, "avg_logprob": -0.08828670199554746, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.00019715665257535875}, {"id": 244, "seek": 129620, "start": 1311.48, "end": 1316.92, "text": " lead to a table that has to be flat and hold your coffee. And so that makes kind of a local", "tokens": [51128, 1477, 281, 257, 3199, 300, 575, 281, 312, 4962, 293, 1797, 428, 4982, 13, 400, 370, 300, 1669, 733, 295, 257, 2654, 51400], "temperature": 0.0, "avg_logprob": -0.08828670199554746, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.00019715665257535875}, {"id": 245, "seek": 129620, "start": 1316.92, "end": 1320.76, "text": " optimum between this solution and this solution. You have to go through this intermediate thing", "tokens": [51400, 39326, 1296, 341, 3827, 293, 341, 3827, 13, 509, 362, 281, 352, 807, 341, 19376, 551, 51592], "temperature": 0.0, "avg_logprob": -0.08828670199554746, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.00019715665257535875}, {"id": 246, "seek": 132076, "start": 1320.76, "end": 1326.76, "text": " unless you get lucky enough to generate a regular phenotype. If you have a generative encoding,", "tokens": [50364, 5969, 291, 483, 6356, 1547, 281, 8460, 257, 3890, 7279, 13108, 13, 759, 291, 362, 257, 1337, 1166, 43430, 11, 50664], "temperature": 0.0, "avg_logprob": -0.04514711432986789, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.00037996273022145033}, {"id": 247, "seek": 132076, "start": 1326.76, "end": 1332.6, "text": " you reuse information in the parameter vector to produce the final thing. So you might just", "tokens": [50664, 291, 26225, 1589, 294, 264, 13075, 8062, 281, 5258, 264, 2572, 551, 13, 407, 291, 1062, 445, 50956], "temperature": 0.0, "avg_logprob": -0.04514711432986789, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.00037996273022145033}, {"id": 248, "seek": 132076, "start": 1332.6, "end": 1338.04, "text": " specify the length of legs once and then reuse that for these four lengths of tables. And now", "tokens": [50956, 16500, 264, 4641, 295, 5668, 1564, 293, 550, 26225, 300, 337, 613, 1451, 26329, 295, 8020, 13, 400, 586, 51228], "temperature": 0.0, "avg_logprob": -0.04514711432986789, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.00037996273022145033}, {"id": 249, "seek": 132076, "start": 1338.04, "end": 1344.52, "text": " every single change to that parameter vector is going to produce a regular flat table. However,", "tokens": [51228, 633, 2167, 1319, 281, 300, 13075, 8062, 307, 516, 281, 5258, 257, 3890, 4962, 3199, 13, 2908, 11, 51552], "temperature": 0.0, "avg_logprob": -0.04514711432986789, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.00037996273022145033}, {"id": 250, "seek": 132076, "start": 1344.52, "end": 1349.8799999999999, "text": " you've lost something. You've lost the ability to express this type of table up here. And so this", "tokens": [51552, 291, 600, 2731, 746, 13, 509, 600, 2731, 264, 3485, 281, 5109, 341, 2010, 295, 3199, 493, 510, 13, 400, 370, 341, 51820], "temperature": 0.0, "avg_logprob": -0.04514711432986789, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.00037996273022145033}, {"id": 251, "seek": 134988, "start": 1349.88, "end": 1355.64, "text": " is like a really, really essential choice when you go to produce any solution with search.", "tokens": [50364, 307, 411, 257, 534, 11, 534, 7115, 3922, 562, 291, 352, 281, 5258, 604, 3827, 365, 3164, 13, 50652], "temperature": 0.0, "avg_logprob": -0.07555745329175677, "compression_ratio": 1.6702508960573477, "no_speech_prob": 7.966643897816539e-05}, {"id": 252, "seek": 134988, "start": 1355.64, "end": 1359.96, "text": " So generative encodings, you know, my colleagues and I and many others have been focusing for a", "tokens": [50652, 407, 1337, 1166, 2058, 378, 1109, 11, 291, 458, 11, 452, 7734, 293, 286, 293, 867, 2357, 362, 668, 8416, 337, 257, 50868], "temperature": 0.0, "avg_logprob": -0.07555745329175677, "compression_ratio": 1.6702508960573477, "no_speech_prob": 7.966643897816539e-05}, {"id": 253, "seek": 134988, "start": 1359.96, "end": 1364.8400000000001, "text": " long time on why these types of encodings are really interesting. And some of the desirable", "tokens": [50868, 938, 565, 322, 983, 613, 3467, 295, 2058, 378, 1109, 366, 534, 1880, 13, 400, 512, 295, 264, 30533, 51112], "temperature": 0.0, "avg_logprob": -0.07555745329175677, "compression_ratio": 1.6702508960573477, "no_speech_prob": 7.966643897816539e-05}, {"id": 254, "seek": 134988, "start": 1364.8400000000001, "end": 1369.16, "text": " properties that we want is that you can get regularity, which means you can get patterns in", "tokens": [51112, 7221, 300, 321, 528, 307, 300, 291, 393, 483, 3890, 507, 11, 597, 1355, 291, 393, 483, 8294, 294, 51328], "temperature": 0.0, "avg_logprob": -0.07555745329175677, "compression_ratio": 1.6702508960573477, "no_speech_prob": 7.966643897816539e-05}, {"id": 255, "seek": 134988, "start": 1369.16, "end": 1374.7600000000002, "text": " the final artifact. It might be the architecture of a neural net, or here is the hands on your,", "tokens": [51328, 264, 2572, 34806, 13, 467, 1062, 312, 264, 9482, 295, 257, 18161, 2533, 11, 420, 510, 307, 264, 2377, 322, 428, 11, 51608], "temperature": 0.0, "avg_logprob": -0.07555745329175677, "compression_ratio": 1.6702508960573477, "no_speech_prob": 7.966643897816539e-05}, {"id": 256, "seek": 137476, "start": 1374.76, "end": 1379.64, "text": " you know, in your body. And what you see is there's a repeating theme in your hands. That's the", "tokens": [50364, 291, 458, 11, 294, 428, 1772, 13, 400, 437, 291, 536, 307, 456, 311, 257, 18617, 6314, 294, 428, 2377, 13, 663, 311, 264, 50608], "temperature": 0.0, "avg_logprob": -0.07145260097263577, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0023964117281138897}, {"id": 257, "seek": 137476, "start": 1379.64, "end": 1385.0, "text": " regular pattern. But it also has variation. Each of your fingers is a variation on a concept or a", "tokens": [50608, 3890, 5102, 13, 583, 309, 611, 575, 12990, 13, 6947, 295, 428, 7350, 307, 257, 12990, 322, 257, 3410, 420, 257, 50876], "temperature": 0.0, "avg_logprob": -0.07145260097263577, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0023964117281138897}, {"id": 258, "seek": 137476, "start": 1385.0, "end": 1389.4, "text": " theme. And that's kind of one thing that you might want while you search. There are some others", "tokens": [50876, 6314, 13, 400, 300, 311, 733, 295, 472, 551, 300, 291, 1062, 528, 1339, 291, 3164, 13, 821, 366, 512, 2357, 51096], "temperature": 0.0, "avg_logprob": -0.07145260097263577, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0023964117281138897}, {"id": 259, "seek": 137476, "start": 1389.4, "end": 1395.08, "text": " benefits here, but I'm not going to get into those. So this is something that I just think is really", "tokens": [51096, 5311, 510, 11, 457, 286, 478, 406, 516, 281, 483, 666, 729, 13, 407, 341, 307, 746, 300, 286, 445, 519, 307, 534, 51380], "temperature": 0.0, "avg_logprob": -0.07145260097263577, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0023964117281138897}, {"id": 260, "seek": 137476, "start": 1395.08, "end": 1399.48, "text": " fascinating to think about, especially if you're interested in aesthetics. And it also ends up", "tokens": [51380, 10343, 281, 519, 466, 11, 2318, 498, 291, 434, 3102, 294, 35517, 13, 400, 309, 611, 5314, 493, 51600], "temperature": 0.0, "avg_logprob": -0.07145260097263577, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0023964117281138897}, {"id": 261, "seek": 137476, "start": 1399.48, "end": 1403.0, "text": " being helpful algorithmically. And it's going to factor into a lot of Joel's work, I assume,", "tokens": [51600, 885, 4961, 9284, 984, 13, 400, 309, 311, 516, 281, 5952, 666, 257, 688, 295, 21522, 311, 589, 11, 286, 6552, 11, 51776], "temperature": 0.0, "avg_logprob": -0.07145260097263577, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.0023964117281138897}, {"id": 262, "seek": 140300, "start": 1403.0, "end": 1408.36, "text": " depending on what he talks about. And this is this question of how does nature build the", "tokens": [50364, 5413, 322, 437, 415, 6686, 466, 13, 400, 341, 307, 341, 1168, 295, 577, 775, 3687, 1322, 264, 50632], "temperature": 0.0, "avg_logprob": -0.069664620516593, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0008829259313642979}, {"id": 263, "seek": 140300, "start": 1408.36, "end": 1414.6, "text": " astronomically elegant, complex creatures that you see in the natural world? Like a question", "tokens": [50632, 26302, 984, 21117, 11, 3997, 12281, 300, 291, 536, 294, 264, 3303, 1002, 30, 1743, 257, 1168, 50944], "temperature": 0.0, "avg_logprob": -0.069664620516593, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0008829259313642979}, {"id": 264, "seek": 140300, "start": 1414.6, "end": 1417.64, "text": " that I'm not sure if you've ever stopped and thought about, but it's a fascinating one to think", "tokens": [50944, 300, 286, 478, 406, 988, 498, 291, 600, 1562, 5936, 293, 1194, 466, 11, 457, 309, 311, 257, 10343, 472, 281, 519, 51096], "temperature": 0.0, "avg_logprob": -0.069664620516593, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0008829259313642979}, {"id": 265, "seek": 140300, "start": 1417.64, "end": 1424.36, "text": " about is how does every cell in your body know what kind of cell to become? You have, you know,", "tokens": [51096, 466, 307, 577, 775, 633, 2815, 294, 428, 1772, 458, 437, 733, 295, 2815, 281, 1813, 30, 509, 362, 11, 291, 458, 11, 51432], "temperature": 0.0, "avg_logprob": -0.069664620516593, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0008829259313642979}, {"id": 266, "seek": 140300, "start": 1424.36, "end": 1431.08, "text": " the same software is being run in every one of your cells, the same DNA, yet some of your cells", "tokens": [51432, 264, 912, 4722, 307, 885, 1190, 294, 633, 472, 295, 428, 5438, 11, 264, 912, 8272, 11, 1939, 512, 295, 428, 5438, 51768], "temperature": 0.0, "avg_logprob": -0.069664620516593, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0008829259313642979}, {"id": 267, "seek": 143108, "start": 1431.08, "end": 1436.28, "text": " turn into hair cells or spleen cells or liver cells or eye cells. How does it do that? How does", "tokens": [50364, 1261, 666, 2578, 5438, 420, 637, 28238, 5438, 420, 15019, 5438, 420, 3313, 5438, 13, 1012, 775, 309, 360, 300, 30, 1012, 775, 50624], "temperature": 0.0, "avg_logprob": -0.0667803825870637, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.0010003139032050967}, {"id": 268, "seek": 143108, "start": 1436.28, "end": 1441.6399999999999, "text": " every cell know what kind of cell to become? Well, it turns out that nature is using a generative", "tokens": [50624, 633, 2815, 458, 437, 733, 295, 2815, 281, 1813, 30, 1042, 11, 309, 4523, 484, 300, 3687, 307, 1228, 257, 1337, 1166, 50892], "temperature": 0.0, "avg_logprob": -0.0667803825870637, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.0010003139032050967}, {"id": 269, "seek": 143108, "start": 1441.6399999999999, "end": 1447.08, "text": " encoding where it reuses information, where the cell fate, which is the type of cell, is a function", "tokens": [50892, 43430, 689, 309, 319, 8355, 1589, 11, 689, 264, 2815, 12738, 11, 597, 307, 264, 2010, 295, 2815, 11, 307, 257, 2445, 51164], "temperature": 0.0, "avg_logprob": -0.0667803825870637, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.0010003139032050967}, {"id": 270, "seek": 143108, "start": 1447.08, "end": 1452.52, "text": " of its geometric location in the body. It's almost as if the body wanted to know the XYZ", "tokens": [51164, 295, 1080, 33246, 4914, 294, 264, 1772, 13, 467, 311, 1920, 382, 498, 264, 1772, 1415, 281, 458, 264, 48826, 57, 51436], "temperature": 0.0, "avg_logprob": -0.0667803825870637, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.0010003139032050967}, {"id": 271, "seek": 143108, "start": 1452.52, "end": 1457.72, "text": " GPS coordinates of each cell so that it could tell you, oh, if you're like up here, left of the", "tokens": [51436, 19462, 21056, 295, 1184, 2815, 370, 300, 309, 727, 980, 291, 11, 1954, 11, 498, 291, 434, 411, 493, 510, 11, 1411, 295, 264, 51696], "temperature": 0.0, "avg_logprob": -0.0667803825870637, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.0010003139032050967}, {"id": 272, "seek": 145772, "start": 1457.72, "end": 1462.2, "text": " midline, three quarters of the way up the y-axis, then become a heart cell, for example.", "tokens": [50364, 2062, 1889, 11, 1045, 20612, 295, 264, 636, 493, 264, 288, 12, 24633, 11, 550, 1813, 257, 1917, 2815, 11, 337, 1365, 13, 50588], "temperature": 0.0, "avg_logprob": -0.0933376829200816, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.0004172577755525708}, {"id": 273, "seek": 145772, "start": 1463.32, "end": 1468.2, "text": " So if you look through developmental biology textbooks, what you find is that these kinds", "tokens": [50644, 407, 498, 291, 574, 807, 30160, 14956, 33587, 11, 437, 291, 915, 307, 300, 613, 3685, 50888], "temperature": 0.0, "avg_logprob": -0.0933376829200816, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.0004172577755525708}, {"id": 274, "seek": 145772, "start": 1468.2, "end": 1473.64, "text": " of geometric patterns are the lingua franca of developmental biology. So here's this beautiful", "tokens": [50888, 295, 33246, 8294, 366, 264, 22949, 4398, 431, 40835, 295, 30160, 14956, 13, 407, 510, 311, 341, 2238, 51160], "temperature": 0.0, "avg_logprob": -0.0933376829200816, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.0004172577755525708}, {"id": 275, "seek": 145772, "start": 1473.64, "end": 1478.92, "text": " cartoon by Sean Carroll. So here's your DNA which has these genes on it. And in this developing", "tokens": [51160, 18569, 538, 14839, 48456, 13, 407, 510, 311, 428, 8272, 597, 575, 613, 14424, 322, 309, 13, 400, 294, 341, 6416, 51424], "temperature": 0.0, "avg_logprob": -0.0933376829200816, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.0004172577755525708}, {"id": 276, "seek": 145772, "start": 1478.92, "end": 1484.04, "text": " embryo are currently three different chemical patterns. They're called morphogens. They're", "tokens": [51424, 31588, 78, 366, 4362, 1045, 819, 7313, 8294, 13, 814, 434, 1219, 25778, 35325, 13, 814, 434, 51680], "temperature": 0.0, "avg_logprob": -0.0933376829200816, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.0004172577755525708}, {"id": 277, "seek": 148404, "start": 1484.04, "end": 1490.2, "text": " literally some protein that's sitting diffused inside this embryo. And if this gene here says", "tokens": [50364, 3736, 512, 7944, 300, 311, 3798, 7593, 4717, 1854, 341, 31588, 78, 13, 400, 498, 341, 12186, 510, 1619, 50672], "temperature": 0.0, "avg_logprob": -0.08589130748401988, "compression_ratio": 1.7857142857142858, "no_speech_prob": 7.966870907694101e-05}, {"id": 278, "seek": 148404, "start": 1490.2, "end": 1496.84, "text": " that protein A is present and B and C are not present, then this gene expresses and produces", "tokens": [50672, 300, 7944, 316, 307, 1974, 293, 363, 293, 383, 366, 406, 1974, 11, 550, 341, 12186, 39204, 293, 14725, 51004], "temperature": 0.0, "avg_logprob": -0.08589130748401988, "compression_ratio": 1.7857142857142858, "no_speech_prob": 7.966870907694101e-05}, {"id": 279, "seek": 148404, "start": 1496.84, "end": 1502.28, "text": " a new protein, only where that's true. And so now you've combined these three pre-existing", "tokens": [51004, 257, 777, 7944, 11, 787, 689, 300, 311, 2074, 13, 400, 370, 586, 291, 600, 9354, 613, 1045, 659, 12, 36447, 51276], "temperature": 0.0, "avg_logprob": -0.08589130748401988, "compression_ratio": 1.7857142857142858, "no_speech_prob": 7.966870907694101e-05}, {"id": 280, "seek": 148404, "start": 1502.28, "end": 1507.56, "text": " patterns to produce this fourth new pattern. And this might therefore tell the vertebra and a spine", "tokens": [51276, 8294, 281, 5258, 341, 6409, 777, 5102, 13, 400, 341, 1062, 4412, 980, 264, 16167, 6198, 293, 257, 15395, 51540], "temperature": 0.0, "avg_logprob": -0.08589130748401988, "compression_ratio": 1.7857142857142858, "no_speech_prob": 7.966870907694101e-05}, {"id": 281, "seek": 148404, "start": 1507.56, "end": 1513.08, "text": " that they should turn into vertebra cells. You get this repeating theme down the middle, but only", "tokens": [51540, 300, 436, 820, 1261, 666, 16167, 6198, 5438, 13, 509, 483, 341, 18617, 6314, 760, 264, 2808, 11, 457, 787, 51816], "temperature": 0.0, "avg_logprob": -0.08589130748401988, "compression_ratio": 1.7857142857142858, "no_speech_prob": 7.966870907694101e-05}, {"id": 282, "seek": 151308, "start": 1513.48, "end": 1517.6399999999999, "text": " the left half of the embryo. And if you look through that, go ahead.", "tokens": [50384, 264, 1411, 1922, 295, 264, 31588, 78, 13, 400, 498, 291, 574, 807, 300, 11, 352, 2286, 13, 50592], "temperature": 0.0, "avg_logprob": -0.12007797516144074, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.00120641163084656}, {"id": 283, "seek": 151308, "start": 1517.6399999999999, "end": 1522.04, "text": " Would I be able to interject real quick? Sure. My research is actually focusing on exactly this", "tokens": [50592, 6068, 286, 312, 1075, 281, 46787, 957, 1702, 30, 4894, 13, 1222, 2132, 307, 767, 8416, 322, 2293, 341, 50812], "temperature": 0.0, "avg_logprob": -0.12007797516144074, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.00120641163084656}, {"id": 284, "seek": 151308, "start": 1522.04, "end": 1526.6799999999998, "text": " same kind of problem, but in mammals. And so in mammals, the morphogen model explains some stuff,", "tokens": [50812, 912, 733, 295, 1154, 11, 457, 294, 35408, 13, 400, 370, 294, 35408, 11, 264, 25778, 8799, 2316, 13948, 512, 1507, 11, 51044], "temperature": 0.0, "avg_logprob": -0.12007797516144074, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.00120641163084656}, {"id": 285, "seek": 151308, "start": 1526.6799999999998, "end": 1532.1999999999998, "text": " but it's actually even more complex. It is much more complex. Everything in nature is much more", "tokens": [51044, 457, 309, 311, 767, 754, 544, 3997, 13, 467, 307, 709, 544, 3997, 13, 5471, 294, 3687, 307, 709, 544, 51320], "temperature": 0.0, "avg_logprob": -0.12007797516144074, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.00120641163084656}, {"id": 286, "seek": 151308, "start": 1532.1999999999998, "end": 1539.24, "text": " complex than we know. So I am simplifying here because I'm flying through this material. And", "tokens": [51320, 3997, 813, 321, 458, 13, 407, 286, 669, 6883, 5489, 510, 570, 286, 478, 7137, 807, 341, 2527, 13, 400, 51672], "temperature": 0.0, "avg_logprob": -0.12007797516144074, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.00120641163084656}, {"id": 287, "seek": 153924, "start": 1539.32, "end": 1544.2, "text": " not all of the, not, it's not to say that the only thing that's happening is geometric patterning,", "tokens": [50368, 406, 439, 295, 264, 11, 406, 11, 309, 311, 406, 281, 584, 300, 264, 787, 551, 300, 311, 2737, 307, 33246, 3829, 773, 11, 50612], "temperature": 0.0, "avg_logprob": -0.10459205323615961, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.00029589663608931005}, {"id": 288, "seek": 153924, "start": 1544.2, "end": 1551.96, "text": " but it is, basically, I think it's the backbone of the way this stuff gets built. And so by capturing", "tokens": [50612, 457, 309, 307, 11, 1936, 11, 286, 519, 309, 311, 264, 34889, 295, 264, 636, 341, 1507, 2170, 3094, 13, 400, 370, 538, 23384, 51000], "temperature": 0.0, "avg_logprob": -0.10459205323615961, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.00029589663608931005}, {"id": 289, "seek": 153924, "start": 1551.96, "end": 1557.56, "text": " that power and putting it into our search processes, we've gone a long way towards the power of", "tokens": [51000, 300, 1347, 293, 3372, 309, 666, 527, 3164, 7555, 11, 321, 600, 2780, 257, 938, 636, 3030, 264, 1347, 295, 51280], "temperature": 0.0, "avg_logprob": -0.10459205323615961, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.00029589663608931005}, {"id": 290, "seek": 153924, "start": 1557.56, "end": 1563.32, "text": " developmental biology. And you could argue that you've skipped out on a lot of the extra complexity", "tokens": [51280, 30160, 14956, 13, 400, 291, 727, 9695, 300, 291, 600, 30193, 484, 322, 257, 688, 295, 264, 2857, 14024, 51568], "temperature": 0.0, "avg_logprob": -0.10459205323615961, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.00029589663608931005}, {"id": 291, "seek": 153924, "start": 1563.32, "end": 1567.56, "text": " that would be very computationally difficult to simulate by doing these things efficiently.", "tokens": [51568, 300, 576, 312, 588, 24903, 379, 2252, 281, 27817, 538, 884, 613, 721, 19621, 13, 51780], "temperature": 0.0, "avg_logprob": -0.10459205323615961, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.00029589663608931005}, {"id": 292, "seek": 156924, "start": 1569.32, "end": 1570.52, "text": " Yeah. All right. That's a good point.", "tokens": [50368, 865, 13, 1057, 558, 13, 663, 311, 257, 665, 935, 13, 50428], "temperature": 0.0, "avg_logprob": -0.11857330208957785, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00027796588256023824}, {"id": 293, "seek": 156924, "start": 1573.24, "end": 1580.2, "text": " Cool. Thank you for the question. So getting to the issue I was just talking about, which is how", "tokens": [50564, 8561, 13, 1044, 291, 337, 264, 1168, 13, 407, 1242, 281, 264, 2734, 286, 390, 445, 1417, 466, 11, 597, 307, 577, 50912], "temperature": 0.0, "avg_logprob": -0.11857330208957785, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00027796588256023824}, {"id": 294, "seek": 156924, "start": 1580.2, "end": 1586.84, "text": " can we efficiently make this sort of a process happen? So what we don't want to do computationally", "tokens": [50912, 393, 321, 19621, 652, 341, 1333, 295, 257, 1399, 1051, 30, 407, 437, 321, 500, 380, 528, 281, 360, 24903, 379, 51244], "temperature": 0.0, "avg_logprob": -0.11857330208957785, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00027796588256023824}, {"id": 295, "seek": 156924, "start": 1586.84, "end": 1591.8, "text": " is have, like, diffusing chemicals in some chemical simulator, because that would be", "tokens": [51244, 307, 362, 11, 411, 11, 7593, 7981, 16152, 294, 512, 7313, 32974, 11, 570, 300, 576, 312, 51492], "temperature": 0.0, "avg_logprob": -0.11857330208957785, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00027796588256023824}, {"id": 296, "seek": 156924, "start": 1591.8, "end": 1597.08, "text": " really, really expensive. And so Ken Stanley, my longtime friend and colleague figured out,", "tokens": [51492, 534, 11, 534, 5124, 13, 400, 370, 8273, 28329, 11, 452, 44363, 1277, 293, 13532, 8932, 484, 11, 51756], "temperature": 0.0, "avg_logprob": -0.11857330208957785, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00027796588256023824}, {"id": 297, "seek": 159708, "start": 1597.08, "end": 1603.08, "text": " is that you can actually abstract a lot of the power of this system without any of the underlying", "tokens": [50364, 307, 300, 291, 393, 767, 12649, 257, 688, 295, 264, 1347, 295, 341, 1185, 1553, 604, 295, 264, 14217, 50664], "temperature": 0.0, "avg_logprob": -0.10034819545908871, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.0007095010369084775}, {"id": 298, "seek": 159708, "start": 1603.96, "end": 1609.8, "text": " chemistry and in physics in these things that are called CPPNs or compositional pattern producing", "tokens": [50708, 12558, 293, 294, 10649, 294, 613, 721, 300, 366, 1219, 383, 17755, 45, 82, 420, 10199, 2628, 5102, 10501, 51000], "temperature": 0.0, "avg_logprob": -0.10034819545908871, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.0007095010369084775}, {"id": 299, "seek": 159708, "start": 1609.8, "end": 1615.24, "text": " networks. And the idea is, is just like in nature, we're going to encode phenotypic elements as a", "tokens": [51000, 9590, 13, 400, 264, 1558, 307, 11, 307, 445, 411, 294, 3687, 11, 321, 434, 516, 281, 2058, 1429, 7279, 6737, 37509, 4959, 382, 257, 51272], "temperature": 0.0, "avg_logprob": -0.10034819545908871, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.0007095010369084775}, {"id": 300, "seek": 159708, "start": 1615.24, "end": 1621.72, "text": " function of their geometric location. So here's how it works. You take a thing that you want to", "tokens": [51272, 2445, 295, 641, 33246, 4914, 13, 407, 510, 311, 577, 309, 1985, 13, 509, 747, 257, 551, 300, 291, 528, 281, 51596], "temperature": 0.0, "avg_logprob": -0.10034819545908871, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.0007095010369084775}, {"id": 301, "seek": 159708, "start": 1622.84, "end": 1626.9199999999998, "text": " optimize. This could be a neural network, it could be a robot morphology, it could be a", "tokens": [51652, 19719, 13, 639, 727, 312, 257, 18161, 3209, 11, 309, 727, 312, 257, 7881, 25778, 1793, 11, 309, 727, 312, 257, 51856], "temperature": 0.0, "avg_logprob": -0.10034819545908871, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.0007095010369084775}, {"id": 302, "seek": 162692, "start": 1626.92, "end": 1633.64, "text": " picture. And you provide coordinates for everything in the artifact. So imagine it's easiest to think", "tokens": [50364, 3036, 13, 400, 291, 2893, 21056, 337, 1203, 294, 264, 34806, 13, 407, 3811, 309, 311, 12889, 281, 519, 50700], "temperature": 0.0, "avg_logprob": -0.13171920945159102, "compression_ratio": 1.8816326530612244, "no_speech_prob": 0.0007320932927541435}, {"id": 303, "seek": 162692, "start": 1633.64, "end": 1638.44, "text": " about pictures. So imagine you give every pixel an x, y coordinate, then you literally pass the", "tokens": [50700, 466, 5242, 13, 407, 3811, 291, 976, 633, 19261, 364, 2031, 11, 288, 15670, 11, 550, 291, 3736, 1320, 264, 50940], "temperature": 0.0, "avg_logprob": -0.13171920945159102, "compression_ratio": 1.8816326530612244, "no_speech_prob": 0.0007320932927541435}, {"id": 304, "seek": 162692, "start": 1638.44, "end": 1644.44, "text": " number, then those numbers into this function. So first you put in one, one for this pixel,", "tokens": [50940, 1230, 11, 550, 729, 3547, 666, 341, 2445, 13, 407, 700, 291, 829, 294, 472, 11, 472, 337, 341, 19261, 11, 51240], "temperature": 0.0, "avg_logprob": -0.13171920945159102, "compression_ratio": 1.8816326530612244, "no_speech_prob": 0.0007320932927541435}, {"id": 305, "seek": 162692, "start": 1644.44, "end": 1650.52, "text": " and then one, two, and then one, three. And you ask the genome as a function of those two numbers", "tokens": [51240, 293, 550, 472, 11, 732, 11, 293, 550, 472, 11, 1045, 13, 400, 291, 1029, 264, 21953, 382, 257, 2445, 295, 729, 732, 3547, 51544], "temperature": 0.0, "avg_logprob": -0.13171920945159102, "compression_ratio": 1.8816326530612244, "no_speech_prob": 0.0007320932927541435}, {"id": 306, "seek": 162692, "start": 1650.52, "end": 1655.64, "text": " to spit out the value at that location. And if this is a random function,", "tokens": [51544, 281, 22127, 484, 264, 2158, 412, 300, 4914, 13, 400, 498, 341, 307, 257, 4974, 2445, 11, 51800], "temperature": 0.0, "avg_logprob": -0.13171920945159102, "compression_ratio": 1.8816326530612244, "no_speech_prob": 0.0007320932927541435}, {"id": 307, "seek": 165564, "start": 1655.64, "end": 1661.72, "text": " then you're going to get a random picture. But if this function here has mathematical functions", "tokens": [50364, 550, 291, 434, 516, 281, 483, 257, 4974, 3036, 13, 583, 498, 341, 2445, 510, 575, 18894, 6828, 50668], "temperature": 0.0, "avg_logprob": -0.07894700711911863, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.0002453422057442367}, {"id": 308, "seek": 165564, "start": 1661.72, "end": 1666.8400000000001, "text": " that, you know, have regularities in them, then you're going to get a regular artifact.", "tokens": [50668, 300, 11, 291, 458, 11, 362, 3890, 1088, 294, 552, 11, 550, 291, 434, 516, 281, 483, 257, 3890, 34806, 13, 50924], "temperature": 0.0, "avg_logprob": -0.07894700711911863, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.0002453422057442367}, {"id": 309, "seek": 165564, "start": 1666.8400000000001, "end": 1673.5600000000002, "text": " So for example, if you want left-right symmetry, you can pass the x-axis through a Gaussian here,", "tokens": [50924, 407, 337, 1365, 11, 498, 291, 528, 1411, 12, 1938, 25440, 11, 291, 393, 1320, 264, 2031, 12, 24633, 807, 257, 39148, 510, 11, 51260], "temperature": 0.0, "avg_logprob": -0.07894700711911863, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.0002453422057442367}, {"id": 310, "seek": 165564, "start": 1673.5600000000002, "end": 1677.4, "text": " and then everything downstream of that Gaussian node will have left-right symmetry.", "tokens": [51260, 293, 550, 1203, 30621, 295, 300, 39148, 9984, 486, 362, 1411, 12, 1938, 25440, 13, 51452], "temperature": 0.0, "avg_logprob": -0.07894700711911863, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.0002453422057442367}, {"id": 311, "seek": 165564, "start": 1678.1200000000001, "end": 1683.64, "text": " Similarly, you could have in the y-axis, if you wanted a repeating theme like segmentation,", "tokens": [51488, 13157, 11, 291, 727, 362, 294, 264, 288, 12, 24633, 11, 498, 291, 1415, 257, 18617, 6314, 411, 9469, 399, 11, 51764], "temperature": 0.0, "avg_logprob": -0.07894700711911863, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.0002453422057442367}, {"id": 312, "seek": 168364, "start": 1683.64, "end": 1688.68, "text": " you could pass the y through a sine function, and then everything downstream of that node will be", "tokens": [50364, 291, 727, 1320, 264, 288, 807, 257, 18609, 2445, 11, 293, 550, 1203, 30621, 295, 300, 9984, 486, 312, 50616], "temperature": 0.0, "avg_logprob": -0.08580912676724521, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0002453446504659951}, {"id": 313, "seek": 168364, "start": 1689.4, "end": 1694.5200000000002, "text": " regular in that way. You can also add in linear things. You could say, I want to follow the sine,", "tokens": [50652, 3890, 294, 300, 636, 13, 509, 393, 611, 909, 294, 8213, 721, 13, 509, 727, 584, 11, 286, 528, 281, 1524, 264, 18609, 11, 50908], "temperature": 0.0, "avg_logprob": -0.08580912676724521, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0002453446504659951}, {"id": 314, "seek": 168364, "start": 1694.5200000000002, "end": 1699.8000000000002, "text": " but only add in a linear component, so like shift it or warp it or bend it in certain ways. So you", "tokens": [50908, 457, 787, 909, 294, 257, 8213, 6542, 11, 370, 411, 5513, 309, 420, 36030, 309, 420, 11229, 309, 294, 1629, 2098, 13, 407, 291, 51172], "temperature": 0.0, "avg_logprob": -0.08580912676724521, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0002453446504659951}, {"id": 315, "seek": 168364, "start": 1699.8000000000002, "end": 1705.5600000000002, "text": " can mix and match asymmetric and symmetric and repeating themes to produce arbitrary complexity", "tokens": [51172, 393, 2890, 293, 2995, 37277, 17475, 293, 32330, 293, 18617, 13544, 281, 5258, 23211, 14024, 51460], "temperature": 0.0, "avg_logprob": -0.08580912676724521, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0002453446504659951}, {"id": 316, "seek": 168364, "start": 1705.5600000000002, "end": 1712.0400000000002, "text": " using these geometric functions. And kind of what was really amazing at the time,", "tokens": [51460, 1228, 613, 33246, 6828, 13, 400, 733, 295, 437, 390, 534, 2243, 412, 264, 565, 11, 51784], "temperature": 0.0, "avg_logprob": -0.08580912676724521, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0002453446504659951}, {"id": 317, "seek": 171204, "start": 1712.04, "end": 1716.6, "text": " because image generation wasn't working very well, was the kind of images that would pop out of", "tokens": [50364, 570, 3256, 5125, 2067, 380, 1364, 588, 731, 11, 390, 264, 733, 295, 5267, 300, 576, 1665, 484, 295, 50592], "temperature": 0.0, "avg_logprob": -0.14821474175704152, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.002979930490255356}, {"id": 318, "seek": 171204, "start": 1716.6, "end": 1721.1599999999999, "text": " these systems. So all of these images here were produced on a website called Pickbrier, where", "tokens": [50592, 613, 3652, 13, 407, 439, 295, 613, 5267, 510, 645, 7126, 322, 257, 3144, 1219, 14129, 65, 7326, 11, 689, 50820], "temperature": 0.0, "avg_logprob": -0.14821474175704152, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.002979930490255356}, {"id": 319, "seek": 171204, "start": 1721.1599999999999, "end": 1726.68, "text": " humans manually choose which ones they find interesting, but the underlying encoding is a CPPN.", "tokens": [50820, 6255, 16945, 2826, 597, 2306, 436, 915, 1880, 11, 457, 264, 14217, 43430, 307, 257, 383, 17755, 45, 13, 51096], "temperature": 0.0, "avg_logprob": -0.14821474175704152, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.002979930490255356}, {"id": 320, "seek": 171204, "start": 1727.8, "end": 1731.08, "text": " And Jill's going to tell you a lot more about like a modern version of this website.", "tokens": [51152, 400, 24690, 311, 516, 281, 980, 291, 257, 688, 544, 466, 411, 257, 4363, 3037, 295, 341, 3144, 13, 51316], "temperature": 0.0, "avg_logprob": -0.14821474175704152, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.002979930490255356}, {"id": 321, "seek": 171204, "start": 1731.6399999999999, "end": 1737.72, "text": " So these images here are all encoded with CPPNs, and what you can see is very, very natural like", "tokens": [51344, 407, 613, 5267, 510, 366, 439, 2058, 12340, 365, 383, 17755, 45, 82, 11, 293, 437, 291, 393, 536, 307, 588, 11, 588, 3303, 411, 51648], "temperature": 0.0, "avg_logprob": -0.14821474175704152, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.002979930490255356}, {"id": 322, "seek": 173772, "start": 1737.8, "end": 1743.56, "text": " shapes, like things like left-right symmetry, repeating motifs, and the lineages as you kind of", "tokens": [50368, 10854, 11, 411, 721, 411, 1411, 12, 1938, 25440, 11, 18617, 2184, 18290, 11, 293, 264, 1622, 1660, 382, 291, 733, 295, 50656], "temperature": 0.0, "avg_logprob": -0.09625562570862851, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.0012064578477293253}, {"id": 323, "seek": 173772, "start": 1743.56, "end": 1748.2, "text": " permute and mutate these things. You go from a butterfly to a bat with these kind of beautiful", "tokens": [50656, 4784, 1169, 293, 5839, 473, 613, 721, 13, 509, 352, 490, 257, 22140, 281, 257, 7362, 365, 613, 733, 295, 2238, 50888], "temperature": 0.0, "avg_logprob": -0.09625562570862851, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.0012064578477293253}, {"id": 324, "seek": 173772, "start": 1748.2, "end": 1755.24, "text": " gradations and interpolations that are nice to see. Myself and my postdoc advisor, I took the", "tokens": [50888, 2771, 763, 293, 44902, 763, 300, 366, 1481, 281, 536, 13, 37795, 1967, 293, 452, 2183, 39966, 19161, 11, 286, 1890, 264, 51240], "temperature": 0.0, "avg_logprob": -0.09625562570862851, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.0012064578477293253}, {"id": 325, "seek": 173772, "start": 1755.24, "end": 1758.92, "text": " same exact idea and we just put it in three dimensions, and what you get are these nice", "tokens": [51240, 912, 1900, 1558, 293, 321, 445, 829, 309, 294, 1045, 12819, 11, 293, 437, 291, 483, 366, 613, 1481, 51424], "temperature": 0.0, "avg_logprob": -0.09625562570862851, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.0012064578477293253}, {"id": 326, "seek": 173772, "start": 1758.92, "end": 1764.3600000000001, "text": " three-dimensional shapes, which also show a lot of these regularities. And then we went off and we", "tokens": [51424, 1045, 12, 18759, 10854, 11, 597, 611, 855, 257, 688, 295, 613, 3890, 1088, 13, 400, 550, 321, 1437, 766, 293, 321, 51696], "temperature": 0.0, "avg_logprob": -0.09625562570862851, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.0012064578477293253}, {"id": 327, "seek": 176436, "start": 1764.36, "end": 1769.56, "text": " built this website called endlessforms.com, where you can go on, it's basically Pickbrier but in 3D.", "tokens": [50364, 3094, 341, 3144, 1219, 16144, 837, 82, 13, 1112, 11, 689, 291, 393, 352, 322, 11, 309, 311, 1936, 14129, 65, 7326, 457, 294, 805, 35, 13, 50624], "temperature": 0.0, "avg_logprob": -0.08285486205550265, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.005729278549551964}, {"id": 328, "seek": 176436, "start": 1769.56, "end": 1775.1599999999999, "text": " You can take an individual shape and you can say, I want to further evolve or optimize that shape.", "tokens": [50624, 509, 393, 747, 364, 2609, 3909, 293, 291, 393, 584, 11, 286, 528, 281, 3052, 16693, 420, 19719, 300, 3909, 13, 50904], "temperature": 0.0, "avg_logprob": -0.08285486205550265, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.005729278549551964}, {"id": 329, "seek": 176436, "start": 1775.7199999999998, "end": 1780.6799999999998, "text": " Let's see if this plays. Here, for example, you might take this lamp and you are presented with", "tokens": [50932, 961, 311, 536, 498, 341, 5749, 13, 1692, 11, 337, 1365, 11, 291, 1062, 747, 341, 12684, 293, 291, 366, 8212, 365, 51180], "temperature": 0.0, "avg_logprob": -0.08285486205550265, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.005729278549551964}, {"id": 330, "seek": 176436, "start": 1780.6799999999998, "end": 1785.1599999999999, "text": " a bunch of variants on the lamp, and then you pick the one that you like and you see the next", "tokens": [51180, 257, 3840, 295, 21669, 322, 264, 12684, 11, 293, 550, 291, 1888, 264, 472, 300, 291, 411, 293, 291, 536, 264, 958, 51404], "temperature": 0.0, "avg_logprob": -0.08285486205550265, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.005729278549551964}, {"id": 331, "seek": 176436, "start": 1785.1599999999999, "end": 1790.76, "text": " generation and you can kind of crawl through three-dimensional lamp space. And importantly,", "tokens": [51404, 5125, 293, 291, 393, 733, 295, 24767, 807, 1045, 12, 18759, 12684, 1901, 13, 400, 8906, 11, 51684], "temperature": 0.0, "avg_logprob": -0.08285486205550265, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.005729278549551964}, {"id": 332, "seek": 179076, "start": 1790.76, "end": 1794.6, "text": " if you find one that you like, then you can publish it to the website and other people can", "tokens": [50364, 498, 291, 915, 472, 300, 291, 411, 11, 550, 291, 393, 11374, 309, 281, 264, 3144, 293, 661, 561, 393, 50556], "temperature": 0.0, "avg_logprob": -0.10154148351366275, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.000185211596544832}, {"id": 333, "seek": 179076, "start": 1794.6, "end": 1799.72, "text": " pick it up and branch off of that. This is how you get that growing archive of stepping stones", "tokens": [50556, 1888, 309, 493, 293, 9819, 766, 295, 300, 13, 639, 307, 577, 291, 483, 300, 4194, 23507, 295, 16821, 14083, 50812], "temperature": 0.0, "avg_logprob": -0.10154148351366275, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.000185211596544832}, {"id": 334, "seek": 179076, "start": 1800.52, "end": 1803.8, "text": " that allows us to produce kind of an interesting exploration of the space.", "tokens": [50852, 300, 4045, 505, 281, 5258, 733, 295, 364, 1880, 16197, 295, 264, 1901, 13, 51016], "temperature": 0.0, "avg_logprob": -0.10154148351366275, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.000185211596544832}, {"id": 335, "seek": 179076, "start": 1805.48, "end": 1810.2, "text": " Here are some of the other designs that popped out of this system, and here's kind of repeating", "tokens": [51100, 1692, 366, 512, 295, 264, 661, 11347, 300, 21545, 484, 295, 341, 1185, 11, 293, 510, 311, 733, 295, 18617, 51336], "temperature": 0.0, "avg_logprob": -0.10154148351366275, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.000185211596544832}, {"id": 336, "seek": 179076, "start": 1810.2, "end": 1816.68, "text": " segmentation, left-right symmetry, radial symmetry, and mostly a lot of the things just look really", "tokens": [51336, 9469, 399, 11, 1411, 12, 1938, 25440, 11, 38783, 25440, 11, 293, 5240, 257, 688, 295, 264, 721, 445, 574, 534, 51660], "temperature": 0.0, "avg_logprob": -0.10154148351366275, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.000185211596544832}, {"id": 337, "seek": 181668, "start": 1816.68, "end": 1822.44, "text": " natural and interesting. So this is kind of a fun aesthetic space to be playing in using these CPPNs.", "tokens": [50364, 3303, 293, 1880, 13, 407, 341, 307, 733, 295, 257, 1019, 20092, 1901, 281, 312, 2433, 294, 1228, 613, 383, 17755, 45, 82, 13, 50652], "temperature": 0.0, "avg_logprob": -0.09724848957385047, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0016480664489790797}, {"id": 338, "seek": 181668, "start": 1825.3200000000002, "end": 1830.2, "text": " Because we could, we 3D printed the objects and allowed users on the website to 3D print them,", "tokens": [50796, 1436, 321, 727, 11, 321, 805, 35, 13567, 264, 6565, 293, 4350, 5022, 322, 264, 3144, 281, 805, 35, 4482, 552, 11, 51040], "temperature": 0.0, "avg_logprob": -0.09724848957385047, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0016480664489790797}, {"id": 339, "seek": 181668, "start": 1830.2, "end": 1834.6000000000001, "text": " so it's kind of fun to hold these things in your hand, and you can therefore help people who have", "tokens": [51040, 370, 309, 311, 733, 295, 1019, 281, 1797, 613, 721, 294, 428, 1011, 11, 293, 291, 393, 4412, 854, 561, 567, 362, 51260], "temperature": 0.0, "avg_logprob": -0.09724848957385047, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0016480664489790797}, {"id": 340, "seek": 181668, "start": 1834.6000000000001, "end": 1840.3600000000001, "text": " no knowledge of CAD and design to produce arbitrarily complex images and then 3D print them", "tokens": [51260, 572, 3601, 295, 41143, 293, 1715, 281, 5258, 19071, 3289, 3997, 5267, 293, 550, 805, 35, 4482, 552, 51548], "temperature": 0.0, "avg_logprob": -0.09724848957385047, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0016480664489790797}, {"id": 341, "seek": 181668, "start": 1840.3600000000001, "end": 1843.96, "text": " for whatever they want, like a chessboard or something. So when we put this out there,", "tokens": [51548, 337, 2035, 436, 528, 11, 411, 257, 24122, 3787, 420, 746, 13, 407, 562, 321, 829, 341, 484, 456, 11, 51728], "temperature": 0.0, "avg_logprob": -0.09724848957385047, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0016480664489790797}, {"id": 342, "seek": 184396, "start": 1843.96, "end": 1849.24, "text": " people really found this interesting, which I think just goes to the to the fact that if you can", "tokens": [50364, 561, 534, 1352, 341, 1880, 11, 597, 286, 519, 445, 1709, 281, 264, 281, 264, 1186, 300, 498, 291, 393, 50628], "temperature": 0.0, "avg_logprob": -0.10897047025663359, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.000646097760181874}, {"id": 343, "seek": 184396, "start": 1849.24, "end": 1854.76, "text": " automate the design, if you can help people produce really interesting things that they're curious", "tokens": [50628, 31605, 264, 1715, 11, 498, 291, 393, 854, 561, 5258, 534, 1880, 721, 300, 436, 434, 6369, 50904], "temperature": 0.0, "avg_logprob": -0.10897047025663359, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.000646097760181874}, {"id": 344, "seek": 184396, "start": 1854.76, "end": 1859.48, "text": " about and they find exciting, but eliminate all the technical barriers to doing so, then people", "tokens": [50904, 466, 293, 436, 915, 4670, 11, 457, 13819, 439, 264, 6191, 13565, 281, 884, 370, 11, 550, 561, 51140], "temperature": 0.0, "avg_logprob": -0.10897047025663359, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.000646097760181874}, {"id": 345, "seek": 184396, "start": 1859.48, "end": 1865.48, "text": " get really excited about those tools, and Joel's website as a, you know, GAN breeder is a testament", "tokens": [51140, 483, 534, 2919, 466, 729, 3873, 11, 293, 21522, 311, 3144, 382, 257, 11, 291, 458, 11, 460, 1770, 1403, 10020, 307, 257, 35499, 51440], "temperature": 0.0, "avg_logprob": -0.10897047025663359, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.000646097760181874}, {"id": 346, "seek": 184396, "start": 1865.48, "end": 1870.28, "text": " to that as well. So going back to the overall scientific question here, which is can we use", "tokens": [51440, 281, 300, 382, 731, 13, 407, 516, 646, 281, 264, 4787, 8134, 1168, 510, 11, 597, 307, 393, 321, 764, 51680], "temperature": 0.0, "avg_logprob": -0.10897047025663359, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.000646097760181874}, {"id": 347, "seek": 187028, "start": 1870.28, "end": 1875.3999999999999, "text": " this to create an open-ended algorithm? Now you know all the pieces of the puzzles. So we're", "tokens": [50364, 341, 281, 1884, 364, 1269, 12, 3502, 9284, 30, 823, 291, 458, 439, 264, 3755, 295, 264, 24138, 13, 407, 321, 434, 50620], "temperature": 0.0, "avg_logprob": -0.13152668055366068, "compression_ratio": 1.7680250783699059, "no_speech_prob": 0.008059369400143623}, {"id": 348, "seek": 187028, "start": 1875.3999999999999, "end": 1880.12, "text": " going to have AlexNet, which is an early image net network that was quite good at the time,", "tokens": [50620, 516, 281, 362, 5202, 31890, 11, 597, 307, 364, 2440, 3256, 2533, 3209, 300, 390, 1596, 665, 412, 264, 565, 11, 50856], "temperature": 0.0, "avg_logprob": -0.13152668055366068, "compression_ratio": 1.7680250783699059, "no_speech_prob": 0.008059369400143623}, {"id": 349, "seek": 187028, "start": 1880.12, "end": 1883.24, "text": " be able to recognize a thousand different classes, and then we're going to have an optimization", "tokens": [50856, 312, 1075, 281, 5521, 257, 4714, 819, 5359, 11, 293, 550, 321, 434, 516, 281, 362, 364, 19618, 51012], "temperature": 0.0, "avg_logprob": -0.13152668055366068, "compression_ratio": 1.7680250783699059, "no_speech_prob": 0.008059369400143623}, {"id": 350, "seek": 187028, "start": 1883.24, "end": 1887.3999999999999, "text": " algorithm that's going to generate these little tiny CPPN networks that are trying to produce", "tokens": [51012, 9284, 300, 311, 516, 281, 8460, 613, 707, 5870, 383, 17755, 45, 9590, 300, 366, 1382, 281, 5258, 51220], "temperature": 0.0, "avg_logprob": -0.13152668055366068, "compression_ratio": 1.7680250783699059, "no_speech_prob": 0.008059369400143623}, {"id": 351, "seek": 187028, "start": 1887.3999999999999, "end": 1893.0, "text": " images that light, that the DNN, the deep neural net, thinks represent, you know, are classified", "tokens": [51220, 5267, 300, 1442, 11, 300, 264, 21500, 45, 11, 264, 2452, 18161, 2533, 11, 7309, 2906, 11, 291, 458, 11, 366, 20627, 51500], "temperature": 0.0, "avg_logprob": -0.13152668055366068, "compression_ratio": 1.7680250783699059, "no_speech_prob": 0.008059369400143623}, {"id": 352, "seek": 187028, "start": 1893.0, "end": 1898.92, "text": " as each one of the thousand bins in image net. So the idea hopefully is that you'll get goal", "tokens": [51500, 382, 1184, 472, 295, 264, 4714, 41275, 294, 3256, 2533, 13, 407, 264, 1558, 4696, 307, 300, 291, 603, 483, 3387, 51796], "temperature": 0.0, "avg_logprob": -0.13152668055366068, "compression_ratio": 1.7680250783699059, "no_speech_prob": 0.008059369400143623}, {"id": 353, "seek": 189892, "start": 1898.92, "end": 1904.04, "text": " switching. So if one of the networks is the best dog we've ever seen, or particular dog,", "tokens": [50364, 16493, 13, 407, 498, 472, 295, 264, 9590, 307, 264, 1151, 3000, 321, 600, 1562, 1612, 11, 420, 1729, 3000, 11, 50620], "temperature": 0.0, "avg_logprob": -0.07891949221619174, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.005219114478677511}, {"id": 354, "seek": 189892, "start": 1904.04, "end": 1908.76, "text": " and then a permutation on that produces the best fish we've ever seen, then now that network can", "tokens": [50620, 293, 550, 257, 4784, 11380, 322, 300, 14725, 264, 1151, 3506, 321, 600, 1562, 1612, 11, 550, 586, 300, 3209, 393, 50856], "temperature": 0.0, "avg_logprob": -0.07891949221619174, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.005219114478677511}, {"id": 355, "seek": 189892, "start": 1908.76, "end": 1913.88, "text": " go to hop over to that bin and start optimizing to become a better fish. And maybe that produces a", "tokens": [50856, 352, 281, 3818, 670, 281, 300, 5171, 293, 722, 40425, 281, 1813, 257, 1101, 3506, 13, 400, 1310, 300, 14725, 257, 51112], "temperature": 0.0, "avg_logprob": -0.07891949221619174, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.005219114478677511}, {"id": 356, "seek": 189892, "start": 1913.88, "end": 1919.4, "text": " better stepping stone for a cat and then a bird, etc. And the hypothesis that we wanted to test", "tokens": [51112, 1101, 16821, 7581, 337, 257, 3857, 293, 550, 257, 5255, 11, 5183, 13, 400, 264, 17291, 300, 321, 1415, 281, 1500, 51388], "temperature": 0.0, "avg_logprob": -0.07891949221619174, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.005219114478677511}, {"id": 357, "seek": 189892, "start": 1919.4, "end": 1926.6000000000001, "text": " is, is that better than separately optimizing for each one of the bins in image net? So here is", "tokens": [51388, 307, 11, 307, 300, 1101, 813, 14759, 40425, 337, 1184, 472, 295, 264, 41275, 294, 3256, 2533, 30, 407, 510, 307, 51748], "temperature": 0.0, "avg_logprob": -0.07891949221619174, "compression_ratio": 1.8521400778210118, "no_speech_prob": 0.005219114478677511}, {"id": 358, "seek": 192660, "start": 1926.6, "end": 1932.36, "text": " the performance over time. Time here, training goes from bottom to top, and the category of", "tokens": [50364, 264, 3389, 670, 565, 13, 6161, 510, 11, 3097, 1709, 490, 2767, 281, 1192, 11, 293, 264, 7719, 295, 50652], "temperature": 0.0, "avg_logprob": -0.09622255422301211, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0010985772823914886}, {"id": 359, "seek": 192660, "start": 1932.36, "end": 1937.32, "text": " thousand image net classes are along the x-axis. What you can see is that over time performance", "tokens": [50652, 4714, 3256, 2533, 5359, 366, 2051, 264, 2031, 12, 24633, 13, 708, 291, 393, 536, 307, 300, 670, 565, 3389, 50900], "temperature": 0.0, "avg_logprob": -0.09622255422301211, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0010985772823914886}, {"id": 360, "seek": 192660, "start": 1937.32, "end": 1942.28, "text": " rises with training all the way up to one, you know, red in most places, which means that the", "tokens": [50900, 21373, 365, 3097, 439, 264, 636, 493, 281, 472, 11, 291, 458, 11, 2182, 294, 881, 3190, 11, 597, 1355, 300, 264, 51148], "temperature": 0.0, "avg_logprob": -0.09622255422301211, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0010985772823914886}, {"id": 361, "seek": 192660, "start": 1942.28, "end": 1947.24, "text": " deep neural net is certain that this thing is a lion, and this is a starfish, and this is a guitar.", "tokens": [51148, 2452, 18161, 2533, 307, 1629, 300, 341, 551, 307, 257, 17226, 11, 293, 341, 307, 257, 3543, 11608, 11, 293, 341, 307, 257, 7531, 13, 51396], "temperature": 0.0, "avg_logprob": -0.09622255422301211, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0010985772823914886}, {"id": 362, "seek": 192660, "start": 1947.8799999999999, "end": 1953.24, "text": " So my question to you is, knowing that the deep neural net thinks that each one of these things", "tokens": [51428, 407, 452, 1168, 281, 291, 307, 11, 5276, 300, 264, 2452, 18161, 2533, 7309, 300, 1184, 472, 295, 613, 721, 51696], "temperature": 0.0, "avg_logprob": -0.09622255422301211, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0010985772823914886}, {"id": 363, "seek": 195324, "start": 1953.32, "end": 1958.52, "text": " is in that category, you know, what do you think they look like? And if you had asked this question", "tokens": [50368, 307, 294, 300, 7719, 11, 291, 458, 11, 437, 360, 291, 519, 436, 574, 411, 30, 400, 498, 291, 632, 2351, 341, 1168, 50628], "temperature": 0.0, "avg_logprob": -0.08451581970462954, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.0025503356009721756}, {"id": 364, "seek": 195324, "start": 1958.52, "end": 1964.68, "text": " in 2015, 2016, people would have said they look like electric, you know, starfish and guitars,", "tokens": [50628, 294, 7546, 11, 6549, 11, 561, 576, 362, 848, 436, 574, 411, 5210, 11, 291, 458, 11, 3543, 11608, 293, 36809, 11, 50936], "temperature": 0.0, "avg_logprob": -0.08451581970462954, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.0025503356009721756}, {"id": 365, "seek": 195324, "start": 1964.68, "end": 1970.36, "text": " but you probably now, because you guys are, we've had the benefit of a few years, you probably are", "tokens": [50936, 457, 291, 1391, 586, 11, 570, 291, 1074, 366, 11, 321, 600, 632, 264, 5121, 295, 257, 1326, 924, 11, 291, 1391, 366, 51220], "temperature": 0.0, "avg_logprob": -0.08451581970462954, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.0025503356009721756}, {"id": 366, "seek": 195324, "start": 1970.36, "end": 1975.16, "text": " used to the idea that what you do, what you get is not that, but you get these things that are called", "tokens": [51220, 1143, 281, 264, 1558, 300, 437, 291, 360, 11, 437, 291, 483, 307, 406, 300, 11, 457, 291, 483, 613, 721, 300, 366, 1219, 51460], "temperature": 0.0, "avg_logprob": -0.08451581970462954, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.0025503356009721756}, {"id": 367, "seek": 195324, "start": 1975.16, "end": 1980.84, "text": " fooling images or adversarial images, which is to say that the deep neural net is absolutely", "tokens": [51460, 7979, 278, 5267, 420, 17641, 44745, 5267, 11, 597, 307, 281, 584, 300, 264, 2452, 18161, 2533, 307, 3122, 51744], "temperature": 0.0, "avg_logprob": -0.08451581970462954, "compression_ratio": 1.7875457875457876, "no_speech_prob": 0.0025503356009721756}, {"id": 368, "seek": 198084, "start": 1980.84, "end": 1986.28, "text": " certain that this is a starfish, and this is a peacock, and this is a king penguin, and this is", "tokens": [50364, 1629, 300, 341, 307, 257, 3543, 11608, 11, 293, 341, 307, 257, 43370, 1560, 11, 293, 341, 307, 257, 4867, 45752, 11, 293, 341, 307, 50636], "temperature": 0.0, "avg_logprob": -0.09942520614218923, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.004196913912892342}, {"id": 369, "seek": 198084, "start": 1986.28, "end": 1991.56, "text": " an electric guitar, even though they obviously are not those things. So at the time, this was a,", "tokens": [50636, 364, 5210, 7531, 11, 754, 1673, 436, 2745, 366, 406, 729, 721, 13, 407, 412, 264, 565, 11, 341, 390, 257, 11, 50900], "temperature": 0.0, "avg_logprob": -0.09942520614218923, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.004196913912892342}, {"id": 370, "seek": 198084, "start": 1991.56, "end": 1996.6799999999998, "text": " this, we published this paper, deep neural nets are easily fooled, and it was a really big wake-up", "tokens": [50900, 341, 11, 321, 6572, 341, 3035, 11, 2452, 18161, 36170, 366, 3612, 33372, 11, 293, 309, 390, 257, 534, 955, 6634, 12, 1010, 51156], "temperature": 0.0, "avg_logprob": -0.09942520614218923, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.004196913912892342}, {"id": 371, "seek": 198084, "start": 1996.6799999999998, "end": 2001.9599999999998, "text": " call to the community that AI sees the world differently. There are huge security concerns", "tokens": [51156, 818, 281, 264, 1768, 300, 7318, 8194, 264, 1002, 7614, 13, 821, 366, 2603, 3825, 7389, 51420], "temperature": 0.0, "avg_logprob": -0.09942520614218923, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.004196913912892342}, {"id": 372, "seek": 198084, "start": 2001.9599999999998, "end": 2008.04, "text": " here, and this generated a tremendous amount of discussion and awareness amongst the scientific", "tokens": [51420, 510, 11, 293, 341, 10833, 257, 10048, 2372, 295, 5017, 293, 8888, 12918, 264, 8134, 51724], "temperature": 0.0, "avg_logprob": -0.09942520614218923, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.004196913912892342}, {"id": 373, "seek": 200804, "start": 2008.04, "end": 2011.72, "text": " community, the machinery community, and also the broader public about the fact that these new", "tokens": [50364, 1768, 11, 264, 27302, 1768, 11, 293, 611, 264, 13227, 1908, 466, 264, 1186, 300, 613, 777, 50548], "temperature": 0.0, "avg_logprob": -0.10283485678739326, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.003171181306242943}, {"id": 374, "seek": 200804, "start": 2011.72, "end": 2016.04, "text": " tools that we're building have a lot of deep flaws within them that we need to worry about.", "tokens": [50548, 3873, 300, 321, 434, 2390, 362, 257, 688, 295, 2452, 27108, 1951, 552, 300, 321, 643, 281, 3292, 466, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10283485678739326, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.003171181306242943}, {"id": 375, "seek": 200804, "start": 2016.68, "end": 2022.12, "text": " Nowadays, everyone's very familiar with adversarial images. At the time, this was not very well known,", "tokens": [50796, 28908, 11, 1518, 311, 588, 4963, 365, 17641, 44745, 5267, 13, 1711, 264, 565, 11, 341, 390, 406, 588, 731, 2570, 11, 51068], "temperature": 0.0, "avg_logprob": -0.10283485678739326, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.003171181306242943}, {"id": 376, "seek": 200804, "start": 2023.48, "end": 2029.08, "text": " and so I thought that was interesting. However, I also think from an aesthetic perspective,", "tokens": [51136, 293, 370, 286, 1194, 300, 390, 1880, 13, 2908, 11, 286, 611, 519, 490, 364, 20092, 4585, 11, 51416], "temperature": 0.0, "avg_logprob": -0.10283485678739326, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.003171181306242943}, {"id": 377, "seek": 200804, "start": 2029.08, "end": 2032.68, "text": " it's interesting that we were trying to generate innovation engines and generate images. We weren't", "tokens": [51416, 309, 311, 1880, 300, 321, 645, 1382, 281, 8460, 8504, 12982, 293, 8460, 5267, 13, 492, 4999, 380, 51596], "temperature": 0.0, "avg_logprob": -0.10283485678739326, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.003171181306242943}, {"id": 378, "seek": 200804, "start": 2032.68, "end": 2036.6, "text": " trying to study neural nets and whether they had flaws, and then this just kind of popped out,", "tokens": [51596, 1382, 281, 2979, 18161, 36170, 293, 1968, 436, 632, 27108, 11, 293, 550, 341, 445, 733, 295, 21545, 484, 11, 51792], "temperature": 0.0, "avg_logprob": -0.10283485678739326, "compression_ratio": 1.7801857585139318, "no_speech_prob": 0.003171181306242943}, {"id": 379, "seek": 203660, "start": 2036.6, "end": 2041.9599999999998, "text": " so I thought that was an interesting story. But while some of the images didn't look anything", "tokens": [50364, 370, 286, 1194, 300, 390, 364, 1880, 1657, 13, 583, 1339, 512, 295, 264, 5267, 994, 380, 574, 1340, 50632], "temperature": 0.0, "avg_logprob": -0.06559853733710523, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00033528852509334683}, {"id": 380, "seek": 203660, "start": 2041.9599999999998, "end": 2046.28, "text": " like the categories of interest, another thing that we found interesting is that many of them", "tokens": [50632, 411, 264, 10479, 295, 1179, 11, 1071, 551, 300, 321, 1352, 1880, 307, 300, 867, 295, 552, 50848], "temperature": 0.0, "avg_logprob": -0.06559853733710523, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00033528852509334683}, {"id": 381, "seek": 203660, "start": 2046.28, "end": 2050.2799999999997, "text": " did, and from an aesthetic perspective, this is pretty cool because now you're getting an automated", "tokens": [50848, 630, 11, 293, 490, 364, 20092, 4585, 11, 341, 307, 1238, 1627, 570, 586, 291, 434, 1242, 364, 18473, 51048], "temperature": 0.0, "avg_logprob": -0.06559853733710523, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00033528852509334683}, {"id": 382, "seek": 203660, "start": 2050.2799999999997, "end": 2056.44, "text": " art generator. So for example, matchstick, television, and bagel, they pretty much do look", "tokens": [51048, 1523, 19265, 13, 407, 337, 1365, 11, 2995, 11881, 11, 8815, 11, 293, 3411, 338, 11, 436, 1238, 709, 360, 574, 51356], "temperature": 0.0, "avg_logprob": -0.06559853733710523, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00033528852509334683}, {"id": 383, "seek": 203660, "start": 2056.44, "end": 2061.64, "text": " like those things. However, I also think from an aesthetic perspective that some of these really", "tokens": [51356, 411, 729, 721, 13, 2908, 11, 286, 611, 519, 490, 364, 20092, 4585, 300, 512, 295, 613, 534, 51616], "temperature": 0.0, "avg_logprob": -0.06559853733710523, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00033528852509334683}, {"id": 384, "seek": 206164, "start": 2062.2, "end": 2068.12, "text": " evokes an artistic interpretation of what that abstract platonic concept represented by that", "tokens": [50392, 1073, 8606, 364, 17090, 14174, 295, 437, 300, 12649, 3403, 11630, 3410, 10379, 538, 300, 50688], "temperature": 0.0, "avg_logprob": -0.09546044815418332, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.004903925117105246}, {"id": 385, "seek": 206164, "start": 2068.12, "end": 2077.64, "text": " class is. For me, this image of a prison cell evokes more than just a picture of a prison cell.", "tokens": [50688, 1508, 307, 13, 1171, 385, 11, 341, 3256, 295, 257, 6168, 2815, 1073, 8606, 544, 813, 445, 257, 3036, 295, 257, 6168, 2815, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09546044815418332, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.004903925117105246}, {"id": 386, "seek": 206164, "start": 2077.64, "end": 2082.6, "text": " It seems to me like an artist decided to represent the bleakness but also the hope or", "tokens": [51164, 467, 2544, 281, 385, 411, 364, 5748, 3047, 281, 2906, 264, 5408, 514, 1287, 457, 611, 264, 1454, 420, 51412], "temperature": 0.0, "avg_logprob": -0.09546044815418332, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.004903925117105246}, {"id": 387, "seek": 206164, "start": 2082.6, "end": 2087.16, "text": " something about this prison cell. And so even though there is no artist that was trying to", "tokens": [51412, 746, 466, 341, 6168, 2815, 13, 400, 370, 754, 1673, 456, 307, 572, 5748, 300, 390, 1382, 281, 51640], "temperature": 0.0, "avg_logprob": -0.09546044815418332, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.004903925117105246}, {"id": 388, "seek": 208716, "start": 2087.16, "end": 2091.3999999999996, "text": " capture that behind here, there's a neural network that's kind of captured the platonic", "tokens": [50364, 7983, 300, 2261, 510, 11, 456, 311, 257, 18161, 3209, 300, 311, 733, 295, 11828, 264, 3403, 11630, 50576], "temperature": 0.0, "avg_logprob": -0.09355648585728236, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.0015483357710763812}, {"id": 389, "seek": 208716, "start": 2091.3999999999996, "end": 2097.72, "text": " concept of a prison cell, and that somehow leads to its own dialing in of what is central and", "tokens": [50576, 3410, 295, 257, 6168, 2815, 11, 293, 300, 6063, 6689, 281, 1080, 1065, 5502, 278, 294, 295, 437, 307, 5777, 293, 50892], "temperature": 0.0, "avg_logprob": -0.09355648585728236, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.0015483357710763812}, {"id": 390, "seek": 208716, "start": 2097.72, "end": 2103.0, "text": " essential about that concept, or at least evokes those kind of reactions in us and allows us to", "tokens": [50892, 7115, 466, 300, 3410, 11, 420, 412, 1935, 1073, 8606, 729, 733, 295, 12215, 294, 505, 293, 4045, 505, 281, 51156], "temperature": 0.0, "avg_logprob": -0.09355648585728236, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.0015483357710763812}, {"id": 391, "seek": 208716, "start": 2103.0, "end": 2109.72, "text": " explore potentially new types of artistic and aesthetic connections to concepts. So if you", "tokens": [51156, 6839, 7263, 777, 3467, 295, 17090, 293, 20092, 9271, 281, 10392, 13, 407, 498, 291, 51492], "temperature": 0.0, "avg_logprob": -0.09355648585728236, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.0015483357710763812}, {"id": 392, "seek": 208716, "start": 2109.72, "end": 2114.52, "text": " look through the diversity of the images that were generated, I do think this kind of really hit", "tokens": [51492, 574, 807, 264, 8811, 295, 264, 5267, 300, 645, 10833, 11, 286, 360, 519, 341, 733, 295, 534, 2045, 51732], "temperature": 0.0, "avg_logprob": -0.09355648585728236, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.0015483357710763812}, {"id": 393, "seek": 211452, "start": 2114.52, "end": 2119.88, "text": " the mark in terms of a quality diversity algorithm. You've got this huge set of images as all comes", "tokens": [50364, 264, 1491, 294, 2115, 295, 257, 3125, 8811, 9284, 13, 509, 600, 658, 341, 2603, 992, 295, 5267, 382, 439, 1487, 50632], "temperature": 0.0, "avg_logprob": -0.1208358780812409, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.011857366189360619}, {"id": 394, "seek": 211452, "start": 2119.88, "end": 2125.8, "text": " from, you know, one run. And at least I'm not, I think that they are, they might have been pulled", "tokens": [50632, 490, 11, 291, 458, 11, 472, 1190, 13, 400, 412, 1935, 286, 478, 406, 11, 286, 519, 300, 436, 366, 11, 436, 1062, 362, 668, 7373, 50928], "temperature": 0.0, "avg_logprob": -0.1208358780812409, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.011857366189360619}, {"id": 395, "seek": 211452, "start": 2125.8, "end": 2130.84, "text": " from a couple of different runs in this case. But each one produces this giant, this diverse set", "tokens": [50928, 490, 257, 1916, 295, 819, 6676, 294, 341, 1389, 13, 583, 1184, 472, 14725, 341, 7410, 11, 341, 9521, 992, 51180], "temperature": 0.0, "avg_logprob": -0.1208358780812409, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.011857366189360619}, {"id": 396, "seek": 211452, "start": 2130.84, "end": 2134.68, "text": " of images, and many of them I think are really aesthetically interesting, like I think this", "tokens": [51180, 295, 5267, 11, 293, 867, 295, 552, 286, 519, 366, 534, 27837, 984, 1880, 11, 411, 286, 519, 341, 51372], "temperature": 0.0, "avg_logprob": -0.1208358780812409, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.011857366189360619}, {"id": 397, "seek": 211452, "start": 2134.68, "end": 2140.04, "text": " volcano or this beacon, or this cup, I could actually imagine a coffee shop where this is this", "tokens": [51372, 21979, 420, 341, 41669, 11, 420, 341, 4414, 11, 286, 727, 767, 3811, 257, 4982, 3945, 689, 341, 307, 341, 51640], "temperature": 0.0, "avg_logprob": -0.1208358780812409, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.011857366189360619}, {"id": 398, "seek": 214004, "start": 2140.04, "end": 2145.72, "text": " logo, your comments on a mask and a banana, etc. So we really, really thought it was cool to see", "tokens": [50364, 9699, 11, 428, 3053, 322, 257, 6094, 293, 257, 14194, 11, 5183, 13, 407, 321, 534, 11, 534, 1194, 309, 390, 1627, 281, 536, 50648], "temperature": 0.0, "avg_logprob": -0.10130953588405577, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0031716213561594486}, {"id": 399, "seek": 214004, "start": 2145.72, "end": 2151.64, "text": " kind of this pop out of an automated system back in 2015. Scientifically, we're also really", "tokens": [50648, 733, 295, 341, 1665, 484, 295, 364, 18473, 1185, 646, 294, 7546, 13, 18944, 4278, 11, 321, 434, 611, 534, 50944], "temperature": 0.0, "avg_logprob": -0.10130953588405577, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0031716213561594486}, {"id": 400, "seek": 214004, "start": 2151.64, "end": 2155.64, "text": " interested in like whether or not goal switching was playing a huge role in these networks. And so", "tokens": [50944, 3102, 294, 411, 1968, 420, 406, 3387, 16493, 390, 2433, 257, 2603, 3090, 294, 613, 9590, 13, 400, 370, 51144], "temperature": 0.0, "avg_logprob": -0.10130953588405577, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0031716213561594486}, {"id": 401, "seek": 214004, "start": 2155.64, "end": 2161.08, "text": " we have, if you optimize for a single class only, like the water tower class, what we see is that", "tokens": [51144, 321, 362, 11, 498, 291, 19719, 337, 257, 2167, 1508, 787, 11, 411, 264, 1281, 10567, 1508, 11, 437, 321, 536, 307, 300, 51416], "temperature": 0.0, "avg_logprob": -0.10130953588405577, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0031716213561594486}, {"id": 402, "seek": 214004, "start": 2161.08, "end": 2166.2799999999997, "text": " you do indeed get stuck on a local optima. It lands on this particular pattern really early in the", "tokens": [51416, 291, 360, 6451, 483, 5541, 322, 257, 2654, 2427, 4775, 13, 467, 5949, 322, 341, 1729, 5102, 534, 2440, 294, 264, 51676], "temperature": 0.0, "avg_logprob": -0.10130953588405577, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0031716213561594486}, {"id": 403, "seek": 216628, "start": 2166.28, "end": 2171.1600000000003, "text": " run. And then it just does minor tweets on that idea and gets stuck on it until eventually it kind", "tokens": [50364, 1190, 13, 400, 550, 309, 445, 775, 6696, 25671, 322, 300, 1558, 293, 2170, 5541, 322, 309, 1826, 4728, 309, 733, 50608], "temperature": 0.0, "avg_logprob": -0.11096003988514776, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.004467919934540987}, {"id": 404, "seek": 216628, "start": 2171.1600000000003, "end": 2176.76, "text": " of maxes out what you can do in that corner of the search space. In contrast with map elites,", "tokens": [50608, 295, 11469, 279, 484, 437, 291, 393, 360, 294, 300, 4538, 295, 264, 3164, 1901, 13, 682, 8712, 365, 4471, 44678, 11, 50888], "temperature": 0.0, "avg_logprob": -0.11096003988514776, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.004467919934540987}, {"id": 405, "seek": 216628, "start": 2176.76, "end": 2182.1200000000003, "text": " what you see is that early on it locks on this half dome moon image, and it does okay, but then", "tokens": [50888, 437, 291, 536, 307, 300, 2440, 322, 309, 20703, 322, 341, 1922, 27191, 7135, 3256, 11, 293, 309, 775, 1392, 11, 457, 550, 51156], "temperature": 0.0, "avg_logprob": -0.11096003988514776, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.004467919934540987}, {"id": 406, "seek": 216628, "start": 2182.1200000000003, "end": 2186.44, "text": " it kind of gets stuck. And then from a totally different class, something that happened to have", "tokens": [51156, 309, 733, 295, 2170, 5541, 13, 400, 550, 490, 257, 3879, 819, 1508, 11, 746, 300, 2011, 281, 362, 51372], "temperature": 0.0, "avg_logprob": -0.11096003988514776, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.004467919934540987}, {"id": 407, "seek": 216628, "start": 2186.44, "end": 2192.28, "text": " been produced to for the beacon class, actually ends up looking like a better water tower and", "tokens": [51372, 668, 7126, 281, 337, 264, 41669, 1508, 11, 767, 5314, 493, 1237, 411, 257, 1101, 1281, 10567, 293, 51664], "temperature": 0.0, "avg_logprob": -0.11096003988514776, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.004467919934540987}, {"id": 408, "seek": 219228, "start": 2192.44, "end": 2196.92, "text": " goal switches in, it invades this class. And then with further optimization to look like a water", "tokens": [50372, 3387, 19458, 294, 11, 309, 1048, 2977, 341, 1508, 13, 400, 550, 365, 3052, 19618, 281, 574, 411, 257, 1281, 50596], "temperature": 0.0, "avg_logprob": -0.08570753676550728, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0004172867629677057}, {"id": 409, "seek": 219228, "start": 2196.92, "end": 2202.52, "text": " tower ends up making the DNN think with 98% confidence that this is a water tower. And you", "tokens": [50596, 10567, 5314, 493, 1455, 264, 21500, 45, 519, 365, 20860, 4, 6687, 300, 341, 307, 257, 1281, 10567, 13, 400, 291, 50876], "temperature": 0.0, "avg_logprob": -0.08570753676550728, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0004172867629677057}, {"id": 410, "seek": 219228, "start": 2202.52, "end": 2208.6800000000003, "text": " can kind of see why. And we see this lesson over and over and over again. There's many goal switches", "tokens": [50876, 393, 733, 295, 536, 983, 13, 400, 321, 536, 341, 6898, 670, 293, 670, 293, 670, 797, 13, 821, 311, 867, 3387, 19458, 51184], "temperature": 0.0, "avg_logprob": -0.08570753676550728, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0004172867629677057}, {"id": 411, "seek": 219228, "start": 2208.6800000000003, "end": 2214.76, "text": " happening within this population of networks. And we think that's a big reason why performance is", "tokens": [51184, 2737, 1951, 341, 4415, 295, 9590, 13, 400, 321, 519, 300, 311, 257, 955, 1778, 983, 3389, 307, 51488], "temperature": 0.0, "avg_logprob": -0.08570753676550728, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0004172867629677057}, {"id": 412, "seek": 219228, "start": 2214.76, "end": 2221.5600000000004, "text": " much higher than when you optimize for a single class. So what's really interesting about goal", "tokens": [51488, 709, 2946, 813, 562, 291, 19719, 337, 257, 2167, 1508, 13, 407, 437, 311, 534, 1880, 466, 3387, 51828], "temperature": 0.0, "avg_logprob": -0.08570753676550728, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0004172867629677057}, {"id": 413, "seek": 222156, "start": 2221.56, "end": 2226.44, "text": " switching is that it allows what what are what biologists call adaptive radiations. So you come", "tokens": [50364, 16493, 307, 300, 309, 4045, 437, 437, 366, 437, 3228, 12256, 818, 27912, 16335, 763, 13, 407, 291, 808, 50608], "temperature": 0.0, "avg_logprob": -0.11550054096040271, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0033755393233150244}, {"id": 414, "seek": 222156, "start": 2226.44, "end": 2231.64, "text": " up with a good idea like maybe a more efficient way to metabolize oxygen in one lake in Africa.", "tokens": [50608, 493, 365, 257, 665, 1558, 411, 1310, 257, 544, 7148, 636, 281, 19110, 1125, 9169, 294, 472, 11001, 294, 7349, 13, 50868], "temperature": 0.0, "avg_logprob": -0.11550054096040271, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0033755393233150244}, {"id": 415, "seek": 222156, "start": 2231.64, "end": 2237.0, "text": " And then that idea will spread to all of the surrounding lakes in Africa. And then on top of", "tokens": [50868, 400, 550, 300, 1558, 486, 3974, 281, 439, 295, 264, 11498, 25595, 294, 7349, 13, 400, 550, 322, 1192, 295, 51136], "temperature": 0.0, "avg_logprob": -0.11550054096040271, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0033755393233150244}, {"id": 416, "seek": 222156, "start": 2237.0, "end": 2243.08, "text": " that technological foundation, those fish will respecialize to their particular niche and adapt", "tokens": [51136, 300, 18439, 7030, 11, 729, 3506, 486, 725, 494, 1013, 1125, 281, 641, 1729, 19956, 293, 6231, 51440], "temperature": 0.0, "avg_logprob": -0.11550054096040271, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0033755393233150244}, {"id": 417, "seek": 222156, "start": 2243.08, "end": 2247.08, "text": " that innovative incorporate that innovation. The same thing happened with Darwin's finches,", "tokens": [51440, 300, 12999, 16091, 300, 8504, 13, 440, 912, 551, 2011, 365, 30233, 311, 962, 3781, 11, 51640], "temperature": 0.0, "avg_logprob": -0.11550054096040271, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0033755393233150244}, {"id": 418, "seek": 224708, "start": 2247.08, "end": 2253.16, "text": " which radiated out from one from one couple of finches to all of these diverse finches.", "tokens": [50364, 597, 16335, 770, 484, 490, 472, 490, 472, 1916, 295, 962, 3781, 281, 439, 295, 613, 9521, 962, 3781, 13, 50668], "temperature": 0.0, "avg_logprob": -0.07865547216855563, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0013668430037796497}, {"id": 419, "seek": 224708, "start": 2253.16, "end": 2257.4, "text": " And we see the same thing in technology where computers, for example, were invented for one", "tokens": [50668, 400, 321, 536, 264, 912, 551, 294, 2899, 689, 10807, 11, 337, 1365, 11, 645, 14479, 337, 472, 50880], "temperature": 0.0, "avg_logprob": -0.07865547216855563, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0013668430037796497}, {"id": 420, "seek": 224708, "start": 2257.4, "end": 2262.04, "text": " purpose and then kind of spread throughout an ecosystem and are now embedded in all sorts of", "tokens": [50880, 4334, 293, 550, 733, 295, 3974, 3710, 364, 11311, 293, 366, 586, 16741, 294, 439, 7527, 295, 51112], "temperature": 0.0, "avg_logprob": -0.07865547216855563, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0013668430037796497}, {"id": 421, "seek": 224708, "start": 2262.04, "end": 2267.56, "text": " technological devices in our lives. So what's really nice is you can see these adaptive radiations", "tokens": [51112, 18439, 5759, 294, 527, 2909, 13, 407, 437, 311, 534, 1481, 307, 291, 393, 536, 613, 27912, 16335, 763, 51388], "temperature": 0.0, "avg_logprob": -0.07865547216855563, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0013668430037796497}, {"id": 422, "seek": 224708, "start": 2267.56, "end": 2271.88, "text": " happen in these quality diversity algorithms. So this is one of my favorite plots from all of", "tokens": [51388, 1051, 294, 613, 3125, 8811, 14642, 13, 407, 341, 307, 472, 295, 452, 2954, 28609, 490, 439, 295, 51604], "temperature": 0.0, "avg_logprob": -0.07865547216855563, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0013668430037796497}, {"id": 423, "seek": 227188, "start": 2271.96, "end": 2277.4, "text": " the science I've done in my entire career. Inside one of these innovation engine runs,", "tokens": [50368, 264, 3497, 286, 600, 1096, 294, 452, 2302, 3988, 13, 15123, 472, 295, 613, 8504, 2848, 6676, 11, 50640], "temperature": 0.0, "avg_logprob": -0.1321109332869538, "compression_ratio": 1.7, "no_speech_prob": 0.003375941887497902}, {"id": 424, "seek": 227188, "start": 2277.4, "end": 2281.96, "text": " you've got this early innovation, which is this dome against a background, a colored background.", "tokens": [50640, 291, 600, 658, 341, 2440, 8504, 11, 597, 307, 341, 27191, 1970, 257, 3678, 11, 257, 14332, 3678, 13, 50868], "temperature": 0.0, "avg_logprob": -0.1321109332869538, "compression_ratio": 1.7, "no_speech_prob": 0.003375941887497902}, {"id": 425, "seek": 227188, "start": 2281.96, "end": 2288.52, "text": " And that thing, which looked up the abaya class, then radiates out and it's children because this", "tokens": [50868, 400, 300, 551, 11, 597, 2956, 493, 264, 410, 4427, 1508, 11, 550, 16335, 1024, 484, 293, 309, 311, 2227, 570, 341, 51196], "temperature": 0.0, "avg_logprob": -0.1321109332869538, "compression_ratio": 1.7, "no_speech_prob": 0.003375941887497902}, {"id": 426, "seek": 227188, "start": 2288.52, "end": 2293.8, "text": " is a population. So these literally are descendants of each other. It's descendants kind of produce", "tokens": [51196, 307, 257, 4415, 13, 407, 613, 3736, 366, 31693, 295, 1184, 661, 13, 467, 311, 31693, 733, 295, 5258, 51460], "temperature": 0.0, "avg_logprob": -0.1321109332869538, "compression_ratio": 1.7, "no_speech_prob": 0.003375941887497902}, {"id": 427, "seek": 227188, "start": 2293.8, "end": 2299.32, "text": " a phylogenetic tree, just like we see in nature. And ultimately, this innovation turned into a", "tokens": [51460, 257, 903, 88, 4987, 268, 3532, 4230, 11, 445, 411, 321, 536, 294, 3687, 13, 400, 6284, 11, 341, 8504, 3574, 666, 257, 51736], "temperature": 0.0, "avg_logprob": -0.1321109332869538, "compression_ratio": 1.7, "no_speech_prob": 0.003375941887497902}, {"id": 428, "seek": 229932, "start": 2299.32, "end": 2306.6000000000004, "text": " volcano, a mosque, a water tower, a beacon, a yurt, a church, a planetarium, an obelisk, and a dome.", "tokens": [50364, 21979, 11, 257, 31501, 11, 257, 1281, 10567, 11, 257, 41669, 11, 257, 288, 6224, 11, 257, 4128, 11, 257, 5054, 19612, 11, 364, 1111, 338, 7797, 11, 293, 257, 27191, 13, 50728], "temperature": 0.0, "avg_logprob": -0.09928898513317108, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.00036822998663410544}, {"id": 429, "seek": 229932, "start": 2306.6000000000004, "end": 2310.92, "text": " And it's just awesome to see an innovation then get rid of that concept, get rift upon and kind", "tokens": [50728, 400, 309, 311, 445, 3476, 281, 536, 364, 8504, 550, 483, 3973, 295, 300, 3410, 11, 483, 367, 2008, 3564, 293, 733, 50944], "temperature": 0.0, "avg_logprob": -0.09928898513317108, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.00036822998663410544}, {"id": 430, "seek": 229932, "start": 2310.92, "end": 2316.6000000000004, "text": " of radiate out into a huge explosion of diversity. So if you study the history of biology, you'll see", "tokens": [50944, 295, 2843, 13024, 484, 666, 257, 2603, 15673, 295, 8811, 13, 407, 498, 291, 2979, 264, 2503, 295, 14956, 11, 291, 603, 536, 51228], "temperature": 0.0, "avg_logprob": -0.09928898513317108, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.00036822998663410544}, {"id": 431, "seek": 229932, "start": 2316.6000000000004, "end": 2319.88, "text": " that there were many moments in the history of biology where something similar happened. We got", "tokens": [51228, 300, 456, 645, 867, 6065, 294, 264, 2503, 295, 14956, 689, 746, 2531, 2011, 13, 492, 658, 51392], "temperature": 0.0, "avg_logprob": -0.09928898513317108, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.00036822998663410544}, {"id": 432, "seek": 229932, "start": 2319.88, "end": 2325.4, "text": " like, you know, single multicellular organisms or rate or bilateral symmetry or the four-legged", "tokens": [51392, 411, 11, 291, 458, 11, 2167, 2120, 573, 285, 1040, 22110, 420, 3314, 420, 38772, 25440, 420, 264, 1451, 12, 306, 12244, 51668], "temperature": 0.0, "avg_logprob": -0.09928898513317108, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.00036822998663410544}, {"id": 433, "seek": 232540, "start": 2325.4, "end": 2330.2000000000003, "text": " body plan. And then you see this explosion of diversity that descends from that central innovation.", "tokens": [50364, 1772, 1393, 13, 400, 550, 291, 536, 341, 15673, 295, 8811, 300, 7471, 2581, 490, 300, 5777, 8504, 13, 50604], "temperature": 0.0, "avg_logprob": -0.09116191130418044, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.0015483942115679383}, {"id": 434, "seek": 232540, "start": 2330.2000000000003, "end": 2333.4, "text": " So I think it's beautiful to see that happening inside of our algorithms.", "tokens": [50604, 407, 286, 519, 309, 311, 2238, 281, 536, 300, 2737, 1854, 295, 527, 14642, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09116191130418044, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.0015483942115679383}, {"id": 435, "seek": 232540, "start": 2334.76, "end": 2340.12, "text": " We ended up submitting the art that was produced by this algorithm to a competition at the University", "tokens": [50832, 492, 4590, 493, 31836, 264, 1523, 300, 390, 7126, 538, 341, 9284, 281, 257, 6211, 412, 264, 3535, 51100], "temperature": 0.0, "avg_logprob": -0.09116191130418044, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.0015483942115679383}, {"id": 436, "seek": 232540, "start": 2340.12, "end": 2344.36, "text": " of Wyoming where I was a professor. And every year, art students work for a year and they submit", "tokens": [51100, 295, 30810, 689, 286, 390, 257, 8304, 13, 400, 633, 1064, 11, 1523, 1731, 589, 337, 257, 1064, 293, 436, 10315, 51312], "temperature": 0.0, "avg_logprob": -0.09116191130418044, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.0015483942115679383}, {"id": 437, "seek": 232540, "start": 2344.36, "end": 2348.36, "text": " their best project to this competition. And then there's a judges who decide which of them get", "tokens": [51312, 641, 1151, 1716, 281, 341, 6211, 13, 400, 550, 456, 311, 257, 14449, 567, 4536, 597, 295, 552, 483, 51512], "temperature": 0.0, "avg_logprob": -0.09116191130418044, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.0015483942115679383}, {"id": 438, "seek": 232540, "start": 2348.36, "end": 2353.8, "text": " hung on the wall and accepted into the competition. So we did not tell them this is AI-generated art,", "tokens": [51512, 5753, 322, 264, 2929, 293, 9035, 666, 264, 6211, 13, 407, 321, 630, 406, 980, 552, 341, 307, 7318, 12, 21848, 770, 1523, 11, 51784], "temperature": 0.0, "avg_logprob": -0.09116191130418044, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.0015483942115679383}, {"id": 439, "seek": 235380, "start": 2353.8, "end": 2358.6000000000004, "text": " we just submitted it. And not only was the art accepted, it was also given an award.", "tokens": [50364, 321, 445, 14405, 309, 13, 400, 406, 787, 390, 264, 1523, 9035, 11, 309, 390, 611, 2212, 364, 7130, 13, 50604], "temperature": 0.0, "avg_logprob": -0.08315740895067525, "compression_ratio": 1.6445993031358885, "no_speech_prob": 0.0005702333874069154}, {"id": 440, "seek": 235380, "start": 2358.6000000000004, "end": 2363.0, "text": " So here you see people having wine and cheese. And I was like eavesdropping as they're discussing", "tokens": [50604, 407, 510, 291, 536, 561, 1419, 7209, 293, 5399, 13, 400, 286, 390, 411, 308, 5423, 45869, 3759, 382, 436, 434, 10850, 50824], "temperature": 0.0, "avg_logprob": -0.08315740895067525, "compression_ratio": 1.6445993031358885, "no_speech_prob": 0.0005702333874069154}, {"id": 441, "seek": 235380, "start": 2363.0, "end": 2367.6400000000003, "text": " the intent of the artist behind producing all of these different images, not knowing that it was", "tokens": [50824, 264, 8446, 295, 264, 5748, 2261, 10501, 439, 295, 613, 819, 5267, 11, 406, 5276, 300, 309, 390, 51056], "temperature": 0.0, "avg_logprob": -0.08315740895067525, "compression_ratio": 1.6445993031358885, "no_speech_prob": 0.0005702333874069154}, {"id": 442, "seek": 235380, "start": 2367.6400000000003, "end": 2373.0, "text": " an AI algorithm behind it, which I thought was pretty cool. So in some sense, this passed the", "tokens": [51056, 364, 7318, 9284, 2261, 309, 11, 597, 286, 1194, 390, 1238, 1627, 13, 407, 294, 512, 2020, 11, 341, 4678, 264, 51324], "temperature": 0.0, "avg_logprob": -0.08315740895067525, "compression_ratio": 1.6445993031358885, "no_speech_prob": 0.0005702333874069154}, {"id": 443, "seek": 235380, "start": 2373.0, "end": 2380.36, "text": " artistic turning test. Sample size one. FYI, in case you're interested, there is much more work on", "tokens": [51324, 17090, 6246, 1500, 13, 4832, 781, 2744, 472, 13, 42730, 40, 11, 294, 1389, 291, 434, 3102, 11, 456, 307, 709, 544, 589, 322, 51692], "temperature": 0.0, "avg_logprob": -0.08315740895067525, "compression_ratio": 1.6445993031358885, "no_speech_prob": 0.0005702333874069154}, {"id": 444, "seek": 238036, "start": 2380.36, "end": 2385.32, "text": " CPPNs that are more modern. So nowadays, a lot of people are playing with differentiable CPPNs", "tokens": [50364, 383, 17755, 45, 82, 300, 366, 544, 4363, 13, 407, 13434, 11, 257, 688, 295, 561, 366, 2433, 365, 819, 9364, 383, 17755, 45, 82, 50612], "temperature": 0.0, "avg_logprob": -0.09509425467633187, "compression_ratio": 1.7120743034055728, "no_speech_prob": 0.012049240991473198}, {"id": 445, "seek": 238036, "start": 2385.32, "end": 2389.88, "text": " instead of using evolution. I have to because it's so beautiful. Quickly look at the work of Alex", "tokens": [50612, 2602, 295, 1228, 9303, 13, 286, 362, 281, 570, 309, 311, 370, 2238, 13, 31800, 574, 412, 264, 589, 295, 5202, 50840], "temperature": 0.0, "avg_logprob": -0.09509425467633187, "compression_ratio": 1.7120743034055728, "no_speech_prob": 0.012049240991473198}, {"id": 446, "seek": 238036, "start": 2389.88, "end": 2395.08, "text": " here, which I highly recommend you check out. All of these things here are different CPPN", "tokens": [50840, 510, 11, 597, 286, 5405, 2748, 291, 1520, 484, 13, 1057, 295, 613, 721, 510, 366, 819, 383, 17755, 45, 51100], "temperature": 0.0, "avg_logprob": -0.09509425467633187, "compression_ratio": 1.7120743034055728, "no_speech_prob": 0.012049240991473198}, {"id": 447, "seek": 238036, "start": 2396.1200000000003, "end": 2401.0, "text": " represented networks that are doing deep visualization, which is the technique I'm", "tokens": [51152, 10379, 9590, 300, 366, 884, 2452, 25801, 11, 597, 307, 264, 6532, 286, 478, 51396], "temperature": 0.0, "avg_logprob": -0.09509425467633187, "compression_ratio": 1.7120743034055728, "no_speech_prob": 0.012049240991473198}, {"id": 448, "seek": 238036, "start": 2401.0, "end": 2405.2400000000002, "text": " going to tell you about later. So I encourage you to check that out. There's also, you can", "tokens": [51396, 516, 281, 980, 291, 466, 1780, 13, 407, 286, 5373, 291, 281, 1520, 300, 484, 13, 821, 311, 611, 11, 291, 393, 51608], "temperature": 0.0, "avg_logprob": -0.09509425467633187, "compression_ratio": 1.7120743034055728, "no_speech_prob": 0.012049240991473198}, {"id": 449, "seek": 238036, "start": 2405.2400000000002, "end": 2409.56, "text": " use CPPNs to encode neural networks. I did that a lot in my dissertation and now you can do that", "tokens": [51608, 764, 383, 17755, 45, 82, 281, 2058, 1429, 18161, 9590, 13, 286, 630, 300, 257, 688, 294, 452, 39555, 293, 586, 291, 393, 360, 300, 51824], "temperature": 0.0, "avg_logprob": -0.09509425467633187, "compression_ratio": 1.7120743034055728, "no_speech_prob": 0.012049240991473198}, {"id": 450, "seek": 240956, "start": 2409.56, "end": 2415.4, "text": " with Backprop. David Ha has been pushing that and there's much more work in this vein. Okay,", "tokens": [50364, 365, 5833, 79, 1513, 13, 4389, 4064, 575, 668, 7380, 300, 293, 456, 311, 709, 544, 589, 294, 341, 30669, 13, 1033, 11, 50656], "temperature": 0.0, "avg_logprob": -0.11251224905757581, "compression_ratio": 1.632996632996633, "no_speech_prob": 0.0008293818100355566}, {"id": 451, "seek": 240956, "start": 2415.4, "end": 2420.7599999999998, "text": " so getting back to QD, I think that I hopefully have convinced you that it has all of these nice", "tokens": [50656, 370, 1242, 646, 281, 1249, 35, 11, 286, 519, 300, 286, 4696, 362, 12561, 291, 300, 309, 575, 439, 295, 613, 1481, 50924], "temperature": 0.0, "avg_logprob": -0.11251224905757581, "compression_ratio": 1.632996632996633, "no_speech_prob": 0.0008293818100355566}, {"id": 452, "seek": 240956, "start": 2420.7599999999998, "end": 2426.36, "text": " properties, like a diverse set of high performing solutions that it produces, it has goal switching,", "tokens": [50924, 7221, 11, 411, 257, 9521, 992, 295, 1090, 10205, 6547, 300, 309, 14725, 11, 309, 575, 3387, 16493, 11, 51204], "temperature": 0.0, "avg_logprob": -0.11251224905757581, "compression_ratio": 1.632996632996633, "no_speech_prob": 0.0008293818100355566}, {"id": 453, "seek": 240956, "start": 2426.36, "end": 2430.92, "text": " and it allows you to kind of illuminate the entire search space and learn a lot about what's possible.", "tokens": [51204, 293, 309, 4045, 291, 281, 733, 295, 28593, 473, 264, 2302, 3164, 1901, 293, 1466, 257, 688, 466, 437, 311, 1944, 13, 51432], "temperature": 0.0, "avg_logprob": -0.11251224905757581, "compression_ratio": 1.632996632996633, "no_speech_prob": 0.0008293818100355566}, {"id": 454, "seek": 240956, "start": 2432.04, "end": 2436.2, "text": " Just quickly, I want to say that these ideas really have given us a lot of leverage on hard", "tokens": [51488, 1449, 2661, 11, 286, 528, 281, 584, 300, 613, 3487, 534, 362, 2212, 505, 257, 688, 295, 13982, 322, 1152, 51696], "temperature": 0.0, "avg_logprob": -0.11251224905757581, "compression_ratio": 1.632996632996633, "no_speech_prob": 0.0008293818100355566}, {"id": 455, "seek": 243620, "start": 2436.2799999999997, "end": 2441.64, "text": " technical problems. So in this paper that we had in Nature, we use these ideas to have robots that", "tokens": [50368, 6191, 2740, 13, 407, 294, 341, 3035, 300, 321, 632, 294, 20159, 11, 321, 764, 613, 3487, 281, 362, 14733, 300, 50636], "temperature": 0.0, "avg_logprob": -0.11619333177804947, "compression_ratio": 1.7275449101796407, "no_speech_prob": 0.013630496338009834}, {"id": 456, "seek": 243620, "start": 2441.64, "end": 2446.04, "text": " could adapt to damage within one to two minutes to get up and continue on with their mission,", "tokens": [50636, 727, 6231, 281, 4344, 1951, 472, 281, 732, 2077, 281, 483, 493, 293, 2354, 322, 365, 641, 4447, 11, 50856], "temperature": 0.0, "avg_logprob": -0.11619333177804947, "compression_ratio": 1.7275449101796407, "no_speech_prob": 0.013630496338009834}, {"id": 457, "seek": 243620, "start": 2446.04, "end": 2450.9199999999996, "text": " even if they're extremely damaged. And then we also use these ideas behind the algorithm GoExplore,", "tokens": [50856, 754, 498, 436, 434, 4664, 14080, 13, 400, 550, 321, 611, 764, 613, 3487, 2261, 264, 9284, 1037, 11149, 564, 418, 11, 51100], "temperature": 0.0, "avg_logprob": -0.11619333177804947, "compression_ratio": 1.7275449101796407, "no_speech_prob": 0.013630496338009834}, {"id": 458, "seek": 243620, "start": 2450.9199999999996, "end": 2455.72, "text": " which you may have heard of, which completely solved the Atari benchmark suite, including", "tokens": [51100, 597, 291, 815, 362, 2198, 295, 11, 597, 2584, 13041, 264, 41381, 18927, 14205, 11, 3009, 51340], "temperature": 0.0, "avg_logprob": -0.11619333177804947, "compression_ratio": 1.7275449101796407, "no_speech_prob": 0.013630496338009834}, {"id": 459, "seek": 243620, "start": 2455.72, "end": 2460.7599999999998, "text": " solving really hard exploration challenges like mono zoom as revenge and pitfall. You can see all", "tokens": [51340, 12606, 534, 1152, 16197, 4759, 411, 35624, 8863, 382, 16711, 293, 10147, 6691, 13, 509, 393, 536, 439, 51592], "temperature": 0.0, "avg_logprob": -0.11619333177804947, "compression_ratio": 1.7275449101796407, "no_speech_prob": 0.013630496338009834}, {"id": 460, "seek": 243620, "start": 2460.7599999999998, "end": 2464.9199999999996, "text": " the previous attempts to solve this heartless game, which became kind of its own grand challenge", "tokens": [51592, 264, 3894, 15257, 281, 5039, 341, 1917, 1832, 1216, 11, 597, 3062, 733, 295, 1080, 1065, 2697, 3430, 51800], "temperature": 0.0, "avg_logprob": -0.11619333177804947, "compression_ratio": 1.7275449101796407, "no_speech_prob": 0.013630496338009834}, {"id": 461, "seek": 246492, "start": 2464.92, "end": 2469.2400000000002, "text": " of the field, do not perform very well. And then this is the difference once you start adding in", "tokens": [50364, 295, 264, 2519, 11, 360, 406, 2042, 588, 731, 13, 400, 550, 341, 307, 264, 2649, 1564, 291, 722, 5127, 294, 50580], "temperature": 0.0, "avg_logprob": -0.07428331153337346, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.0007791501120664179}, {"id": 462, "seek": 246492, "start": 2469.2400000000002, "end": 2474.2000000000003, "text": " these ideas from quality diversity algorithms. Ultimately, we ended up beating the human world", "tokens": [50580, 613, 3487, 490, 3125, 8811, 14642, 13, 23921, 11, 321, 4590, 493, 13497, 264, 1952, 1002, 50828], "temperature": 0.0, "avg_logprob": -0.07428331153337346, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.0007791501120664179}, {"id": 463, "seek": 246492, "start": 2474.2000000000003, "end": 2480.92, "text": " record on this game. Oh, and as a quick little teaser, this paper was also recently accepted", "tokens": [50828, 2136, 322, 341, 1216, 13, 876, 11, 293, 382, 257, 1702, 707, 35326, 11, 341, 3035, 390, 611, 3938, 9035, 51164], "temperature": 0.0, "avg_logprob": -0.07428331153337346, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.0007791501120664179}, {"id": 464, "seek": 246492, "start": 2480.92, "end": 2484.6800000000003, "text": " to a really nice journal. I can't quite tell you which one, but if I'll share that information", "tokens": [51164, 281, 257, 534, 1481, 6708, 13, 286, 393, 380, 1596, 980, 291, 597, 472, 11, 457, 498, 286, 603, 2073, 300, 1589, 51352], "temperature": 0.0, "avg_logprob": -0.07428331153337346, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.0007791501120664179}, {"id": 465, "seek": 246492, "start": 2484.6800000000003, "end": 2488.28, "text": " on Twitter in the next couple of weeks, if you are interested to get the final version", "tokens": [51352, 322, 5794, 294, 264, 958, 1916, 295, 3259, 11, 498, 291, 366, 3102, 281, 483, 264, 2572, 3037, 51532], "temperature": 0.0, "avg_logprob": -0.07428331153337346, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.0007791501120664179}, {"id": 466, "seek": 246492, "start": 2488.28, "end": 2493.7200000000003, "text": " and the updated version of this paper. So I think QD algorithms are really interesting.", "tokens": [51532, 293, 264, 10588, 3037, 295, 341, 3035, 13, 407, 286, 519, 1249, 35, 14642, 366, 534, 1880, 13, 51804], "temperature": 0.0, "avg_logprob": -0.07428331153337346, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.0007791501120664179}, {"id": 467, "seek": 249372, "start": 2493.72, "end": 2497.48, "text": " I think the question that we should always ask though is what's missing where, you know,", "tokens": [50364, 286, 519, 264, 1168, 300, 321, 820, 1009, 1029, 1673, 307, 437, 311, 5361, 689, 11, 291, 458, 11, 50552], "temperature": 0.0, "avg_logprob": -0.08425343938234474, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0013246882008388638}, {"id": 468, "seek": 249372, "start": 2497.48, "end": 2501.8799999999997, "text": " they're not yet open-ended algorithms. So the thing that I think is missing is that while these", "tokens": [50552, 436, 434, 406, 1939, 1269, 12, 3502, 14642, 13, 407, 264, 551, 300, 286, 519, 307, 5361, 307, 300, 1339, 613, 50772], "temperature": 0.0, "avg_logprob": -0.08425343938234474, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0013246882008388638}, {"id": 469, "seek": 249372, "start": 2501.8799999999997, "end": 2506.3599999999997, "text": " things can produce a large diverse set of interesting solutions within one domain,", "tokens": [50772, 721, 393, 5258, 257, 2416, 9521, 992, 295, 1880, 6547, 1951, 472, 9274, 11, 50996], "temperature": 0.0, "avg_logprob": -0.08425343938234474, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0013246882008388638}, {"id": 470, "seek": 249372, "start": 2506.9199999999996, "end": 2510.6, "text": " ultimately, their ability to innovate is constrained because they're stuck in this one", "tokens": [51024, 6284, 11, 641, 3485, 281, 33444, 307, 38901, 570, 436, 434, 5541, 294, 341, 472, 51208], "temperature": 0.0, "avg_logprob": -0.08425343938234474, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0013246882008388638}, {"id": 471, "seek": 249372, "start": 2510.6, "end": 2515.16, "text": " particular setting that we put them in. But what we really want is these open-ended algorithms that", "tokens": [51208, 1729, 3287, 300, 321, 829, 552, 294, 13, 583, 437, 321, 534, 528, 307, 613, 1269, 12, 3502, 14642, 300, 51436], "temperature": 0.0, "avg_logprob": -0.08425343938234474, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0013246882008388638}, {"id": 472, "seek": 249372, "start": 2515.16, "end": 2520.68, "text": " just keep going and kind of generating wildly different solutions as they run. So traditionally", "tokens": [51436, 445, 1066, 516, 293, 733, 295, 17746, 34731, 819, 6547, 382, 436, 1190, 13, 407, 19067, 51712], "temperature": 0.0, "avg_logprob": -0.08425343938234474, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0013246882008388638}, {"id": 473, "seek": 252068, "start": 2520.68, "end": 2525.08, "text": " in ML, we pick a particular challenge like Chester, Gro or Dota or Starcraft and we bang", "tokens": [50364, 294, 21601, 11, 321, 1888, 257, 1729, 3430, 411, 761, 3011, 11, 12981, 420, 413, 5377, 420, 5705, 5611, 293, 321, 8550, 50584], "temperature": 0.0, "avg_logprob": -0.1092677422619741, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.0023226491175591946}, {"id": 474, "seek": 252068, "start": 2525.08, "end": 2529.72, "text": " away on it for a while. But the intriguing possibility that I want all of you to consider", "tokens": [50584, 1314, 322, 309, 337, 257, 1339, 13, 583, 264, 32503, 7959, 300, 286, 528, 439, 295, 291, 281, 1949, 50816], "temperature": 0.0, "avg_logprob": -0.1092677422619741, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.0023226491175591946}, {"id": 475, "seek": 252068, "start": 2529.72, "end": 2534.7599999999998, "text": " today is could we create an algorithm that generates its own challenges and solves them?", "tokens": [50816, 965, 307, 727, 321, 1884, 364, 9284, 300, 23815, 1080, 1065, 4759, 293, 39890, 552, 30, 51068], "temperature": 0.0, "avg_logprob": -0.1092677422619741, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.0023226491175591946}, {"id": 476, "seek": 252068, "start": 2535.64, "end": 2541.96, "text": " Just as nature arguably created the challenge or the opportunity of leaves on the top of trees,", "tokens": [51112, 1449, 382, 3687, 26771, 2942, 264, 3430, 420, 264, 2650, 295, 5510, 322, 264, 1192, 295, 5852, 11, 51428], "temperature": 0.0, "avg_logprob": -0.1092677422619741, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.0023226491175591946}, {"id": 477, "seek": 252068, "start": 2541.96, "end": 2546.52, "text": " and then the solution to that challenge, which is giraffes or caterpillars that can eat them.", "tokens": [51428, 293, 550, 264, 3827, 281, 300, 3430, 11, 597, 307, 14703, 2518, 279, 420, 44982, 373, 685, 300, 393, 1862, 552, 13, 51656], "temperature": 0.0, "avg_logprob": -0.1092677422619741, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.0023226491175591946}, {"id": 478, "seek": 254652, "start": 2547.4, "end": 2551.0, "text": " So, you know, this kind of a thing might produce something that's interesting", "tokens": [50408, 407, 11, 291, 458, 11, 341, 733, 295, 257, 551, 1062, 5258, 746, 300, 311, 1880, 50588], "temperature": 0.0, "avg_logprob": -0.08894979606554346, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.00023048814909998327}, {"id": 479, "seek": 254652, "start": 2551.0, "end": 2555.72, "text": " after a billion years. So our most recent work on this is in this algorithm called Poet,", "tokens": [50588, 934, 257, 5218, 924, 13, 407, 527, 881, 5162, 589, 322, 341, 307, 294, 341, 9284, 1219, 6165, 302, 11, 50824], "temperature": 0.0, "avg_logprob": -0.08894979606554346, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.00023048814909998327}, {"id": 480, "seek": 254652, "start": 2555.72, "end": 2561.16, "text": " which is the paired open-ended trailblazer. And the idea here is that we're going to try to endlessly", "tokens": [50824, 597, 307, 264, 25699, 1269, 12, 3502, 9924, 36138, 4527, 13, 400, 264, 1558, 510, 307, 300, 321, 434, 516, 281, 853, 281, 44920, 51096], "temperature": 0.0, "avg_logprob": -0.08894979606554346, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.00023048814909998327}, {"id": 481, "seek": 254652, "start": 2561.16, "end": 2565.48, "text": " generate interesting, complex and diverse learning environments and their solutions.", "tokens": [51096, 8460, 1880, 11, 3997, 293, 9521, 2539, 12388, 293, 641, 6547, 13, 51312], "temperature": 0.0, "avg_logprob": -0.08894979606554346, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.00023048814909998327}, {"id": 482, "seek": 254652, "start": 2566.36, "end": 2572.28, "text": " So the idea is again quite simple and you'll recognize it. It's basically we want to", "tokens": [51356, 407, 264, 1558, 307, 797, 1596, 2199, 293, 291, 603, 5521, 309, 13, 467, 311, 1936, 321, 528, 281, 51652], "temperature": 0.0, "avg_logprob": -0.08894979606554346, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.00023048814909998327}, {"id": 483, "seek": 257228, "start": 2572.28, "end": 2577.4, "text": " generate new learning environments and we're going to add them to this set of our population of", "tokens": [50364, 8460, 777, 2539, 12388, 293, 321, 434, 516, 281, 909, 552, 281, 341, 992, 295, 527, 4415, 295, 50620], "temperature": 0.0, "avg_logprob": -0.07368001630229334, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.000882928550709039}, {"id": 484, "seek": 257228, "start": 2577.4, "end": 2582.6800000000003, "text": " environments if they're not too easy and not too hard for the current population of agents.", "tokens": [50620, 12388, 498, 436, 434, 406, 886, 1858, 293, 406, 886, 1152, 337, 264, 2190, 4415, 295, 12554, 13, 50884], "temperature": 0.0, "avg_logprob": -0.07368001630229334, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.000882928550709039}, {"id": 485, "seek": 257228, "start": 2582.6800000000003, "end": 2585.88, "text": " And if they're novel, there's something about them that's unique and different.", "tokens": [50884, 400, 498, 436, 434, 7613, 11, 456, 311, 746, 466, 552, 300, 311, 3845, 293, 819, 13, 51044], "temperature": 0.0, "avg_logprob": -0.07368001630229334, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.000882928550709039}, {"id": 486, "seek": 257228, "start": 2585.88, "end": 2589.7200000000003, "text": " And then we'll optimize agents to better solve each of these challenges and we'll allow goal", "tokens": [51044, 400, 550, 321, 603, 19719, 12554, 281, 1101, 5039, 1184, 295, 613, 4759, 293, 321, 603, 2089, 3387, 51236], "temperature": 0.0, "avg_logprob": -0.07368001630229334, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.000882928550709039}, {"id": 487, "seek": 257228, "start": 2589.7200000000003, "end": 2595.88, "text": " switching between them. So the example task that we used here is obstacle courses. So this little", "tokens": [51236, 16493, 1296, 552, 13, 407, 264, 1365, 5633, 300, 321, 1143, 510, 307, 23112, 7712, 13, 407, 341, 707, 51544], "temperature": 0.0, "avg_logprob": -0.07368001630229334, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.000882928550709039}, {"id": 488, "seek": 257228, "start": 2595.88, "end": 2601.1600000000003, "text": " creature here has to run as fast as possible without falling over. And here's the general idea.", "tokens": [51544, 12797, 510, 575, 281, 1190, 382, 2370, 382, 1944, 1553, 7440, 670, 13, 400, 510, 311, 264, 2674, 1558, 13, 51808], "temperature": 0.0, "avg_logprob": -0.07368001630229334, "compression_ratio": 1.8104575163398693, "no_speech_prob": 0.000882928550709039}, {"id": 489, "seek": 260116, "start": 2601.16, "end": 2604.92, "text": " You start with an easy environment. So first you have to make that encoding choice. How are you", "tokens": [50364, 509, 722, 365, 364, 1858, 2823, 13, 407, 700, 291, 362, 281, 652, 300, 43430, 3922, 13, 1012, 366, 291, 50552], "temperature": 0.0, "avg_logprob": -0.08485689650486855, "compression_ratio": 1.819672131147541, "no_speech_prob": 0.000646081636659801}, {"id": 490, "seek": 260116, "start": 2604.92, "end": 2610.7599999999998, "text": " going to encode an environment on a parameter vector? Here we have things like the number of", "tokens": [50552, 516, 281, 2058, 1429, 364, 2823, 322, 257, 13075, 8062, 30, 1692, 321, 362, 721, 411, 264, 1230, 295, 50844], "temperature": 0.0, "avg_logprob": -0.08485689650486855, "compression_ratio": 1.819672131147541, "no_speech_prob": 0.000646081636659801}, {"id": 491, "seek": 260116, "start": 2611.3199999999997, "end": 2615.24, "text": " whether or not there are gaps, whether or not there are stumps, the ruggedness of the terrain,", "tokens": [50872, 1968, 420, 406, 456, 366, 15031, 11, 1968, 420, 406, 456, 366, 342, 16951, 11, 264, 42662, 1287, 295, 264, 17674, 11, 51068], "temperature": 0.0, "avg_logprob": -0.08485689650486855, "compression_ratio": 1.819672131147541, "no_speech_prob": 0.000646081636659801}, {"id": 492, "seek": 260116, "start": 2615.24, "end": 2619.7999999999997, "text": " et cetera. So you can start with an easy one of those, which is maybe just flat terrain.", "tokens": [51068, 1030, 11458, 13, 407, 291, 393, 722, 365, 364, 1858, 472, 295, 729, 11, 597, 307, 1310, 445, 4962, 17674, 13, 51296], "temperature": 0.0, "avg_logprob": -0.08485689650486855, "compression_ratio": 1.819672131147541, "no_speech_prob": 0.000646081636659801}, {"id": 493, "seek": 260116, "start": 2619.7999999999997, "end": 2624.6, "text": " And then you start having an agent, which has its own parameter vector. This is a neural network", "tokens": [51296, 400, 550, 291, 722, 1419, 364, 9461, 11, 597, 575, 1080, 1065, 13075, 8062, 13, 639, 307, 257, 18161, 3209, 51536], "temperature": 0.0, "avg_logprob": -0.08485689650486855, "compression_ratio": 1.819672131147541, "no_speech_prob": 0.000646081636659801}, {"id": 494, "seek": 260116, "start": 2624.6, "end": 2629.64, "text": " and is learning via RL to solve this task. And once it gets good enough on that task,", "tokens": [51536, 293, 307, 2539, 5766, 497, 43, 281, 5039, 341, 5633, 13, 400, 1564, 309, 2170, 665, 1547, 322, 300, 5633, 11, 51788], "temperature": 0.0, "avg_logprob": -0.08485689650486855, "compression_ratio": 1.819672131147541, "no_speech_prob": 0.000646081636659801}, {"id": 495, "seek": 262964, "start": 2629.72, "end": 2635.16, "text": " then we copy phi 1, the parameter vector of the environment, to make phi 2. And then we'll try", "tokens": [50368, 550, 321, 5055, 13107, 502, 11, 264, 13075, 8062, 295, 264, 2823, 11, 281, 652, 13107, 568, 13, 400, 550, 321, 603, 853, 50640], "temperature": 0.0, "avg_logprob": -0.11354602300203763, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.0006461451412178576}, {"id": 496, "seek": 262964, "start": 2635.16, "end": 2640.68, "text": " this agent via transfer and goal switching. It goes and it starts optimizing here. Now,", "tokens": [50640, 341, 9461, 5766, 5003, 293, 3387, 16493, 13, 467, 1709, 293, 309, 3719, 40425, 510, 13, 823, 11, 50916], "temperature": 0.0, "avg_logprob": -0.11354602300203763, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.0006461451412178576}, {"id": 497, "seek": 262964, "start": 2640.68, "end": 2645.4, "text": " we are simultaneously continuing to optimize this parameter vector on this environment and this", "tokens": [50916, 321, 366, 16561, 9289, 281, 19719, 341, 13075, 8062, 322, 341, 2823, 293, 341, 51152], "temperature": 0.0, "avg_logprob": -0.11354602300203763, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.0006461451412178576}, {"id": 498, "seek": 262964, "start": 2645.4, "end": 2651.24, "text": " parameter vector on this environment. We keep going. Maybe eventually this environment gets", "tokens": [51152, 13075, 8062, 322, 341, 2823, 13, 492, 1066, 516, 13, 2704, 4728, 341, 2823, 2170, 51444], "temperature": 0.0, "avg_logprob": -0.11354602300203763, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.0006461451412178576}, {"id": 499, "seek": 262964, "start": 2651.24, "end": 2656.92, "text": " solved well enough by this parameter vector. So we copy it and we now make phi 3 a new environment.", "tokens": [51444, 13041, 731, 1547, 538, 341, 13075, 8062, 13, 407, 321, 5055, 309, 293, 321, 586, 652, 13107, 805, 257, 777, 2823, 13, 51728], "temperature": 0.0, "avg_logprob": -0.11354602300203763, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.0006461451412178576}, {"id": 500, "seek": 265692, "start": 2657.0, "end": 2663.0, "text": " Turns out that's too hard for either theta 1 or theta 2. So we throw that out. We generate,", "tokens": [50368, 29524, 484, 300, 311, 886, 1152, 337, 2139, 9725, 502, 420, 9725, 568, 13, 407, 321, 3507, 300, 484, 13, 492, 8460, 11, 50668], "temperature": 0.0, "avg_logprob": -0.08628465258885944, "compression_ratio": 1.894941634241245, "no_speech_prob": 0.0003250088484492153}, {"id": 501, "seek": 265692, "start": 2663.0, "end": 2668.2000000000003, "text": " we try again, we get a new environment and we test this one and this one. We take the better of those", "tokens": [50668, 321, 853, 797, 11, 321, 483, 257, 777, 2823, 293, 321, 1500, 341, 472, 293, 341, 472, 13, 492, 747, 264, 1101, 295, 729, 50928], "temperature": 0.0, "avg_logprob": -0.08628465258885944, "compression_ratio": 1.894941634241245, "no_speech_prob": 0.0003250088484492153}, {"id": 502, "seek": 265692, "start": 2668.2000000000003, "end": 2673.64, "text": " to you on this new environment to seed training. And in this case, it was theta 2. So it goes in", "tokens": [50928, 281, 291, 322, 341, 777, 2823, 281, 8871, 3097, 13, 400, 294, 341, 1389, 11, 309, 390, 9725, 568, 13, 407, 309, 1709, 294, 51200], "temperature": 0.0, "avg_logprob": -0.08628465258885944, "compression_ratio": 1.894941634241245, "no_speech_prob": 0.0003250088484492153}, {"id": 503, "seek": 265692, "start": 2673.64, "end": 2678.04, "text": " there. This does not have to be a linear chain. At any point, any one of the environments in the", "tokens": [51200, 456, 13, 639, 775, 406, 362, 281, 312, 257, 8213, 5021, 13, 1711, 604, 935, 11, 604, 472, 295, 264, 12388, 294, 264, 51420], "temperature": 0.0, "avg_logprob": -0.08628465258885944, "compression_ratio": 1.894941634241245, "no_speech_prob": 0.0003250088484492153}, {"id": 504, "seek": 265692, "start": 2678.04, "end": 2683.8, "text": " set can produce a new environment. And then we'll try all of the current agents on that environment", "tokens": [51420, 992, 393, 5258, 257, 777, 2823, 13, 400, 550, 321, 603, 853, 439, 295, 264, 2190, 12554, 322, 300, 2823, 51708], "temperature": 0.0, "avg_logprob": -0.08628465258885944, "compression_ratio": 1.894941634241245, "no_speech_prob": 0.0003250088484492153}, {"id": 505, "seek": 268380, "start": 2683.8, "end": 2689.1600000000003, "text": " to see if they're the best and if they are, they get to start. And the process can keep going like", "tokens": [50364, 281, 536, 498, 436, 434, 264, 1151, 293, 498, 436, 366, 11, 436, 483, 281, 722, 13, 400, 264, 1399, 393, 1066, 516, 411, 50632], "temperature": 0.0, "avg_logprob": -0.12331528499208648, "compression_ratio": 1.7985074626865671, "no_speech_prob": 0.0002531369391363114}, {"id": 506, "seek": 268380, "start": 2689.1600000000003, "end": 2694.04, "text": " this. Now, imagine eventually we generate a really, really hard challenge like phi 6 here.", "tokens": [50632, 341, 13, 823, 11, 3811, 4728, 321, 8460, 257, 534, 11, 534, 1152, 3430, 411, 13107, 1386, 510, 13, 50876], "temperature": 0.0, "avg_logprob": -0.12331528499208648, "compression_ratio": 1.7985074626865671, "no_speech_prob": 0.0002531369391363114}, {"id": 507, "seek": 268380, "start": 2694.6000000000004, "end": 2700.6800000000003, "text": " And initially the best parameter vector, we try all of them on this environment was theta 5. It", "tokens": [50904, 400, 9105, 264, 1151, 13075, 8062, 11, 321, 853, 439, 295, 552, 322, 341, 2823, 390, 9725, 1025, 13, 467, 51208], "temperature": 0.0, "avg_logprob": -0.12331528499208648, "compression_ratio": 1.7985074626865671, "no_speech_prob": 0.0002531369391363114}, {"id": 508, "seek": 268380, "start": 2700.6800000000003, "end": 2705.6400000000003, "text": " was the best stepping stone. So we start optimizing a copy of theta 5 in this environment and it gets", "tokens": [51208, 390, 264, 1151, 16821, 7581, 13, 407, 321, 722, 40425, 257, 5055, 295, 9725, 1025, 294, 341, 2823, 293, 309, 2170, 51456], "temperature": 0.0, "avg_logprob": -0.12331528499208648, "compression_ratio": 1.7985074626865671, "no_speech_prob": 0.0002531369391363114}, {"id": 509, "seek": 268380, "start": 2705.6400000000003, "end": 2709.96, "text": " better and better and better. But it maybe hits a local optimal and it can't break through and", "tokens": [51456, 1101, 293, 1101, 293, 1101, 13, 583, 309, 1310, 8664, 257, 2654, 16252, 293, 309, 393, 380, 1821, 807, 293, 51672], "temperature": 0.0, "avg_logprob": -0.12331528499208648, "compression_ratio": 1.7985074626865671, "no_speech_prob": 0.0002531369391363114}, {"id": 510, "seek": 270996, "start": 2709.96, "end": 2714.44, "text": " really, really do well on this environment. But in the meantime, we're still optimizing theta 4 on", "tokens": [50364, 534, 11, 534, 360, 731, 322, 341, 2823, 13, 583, 294, 264, 14991, 11, 321, 434, 920, 40425, 9725, 1017, 322, 50588], "temperature": 0.0, "avg_logprob": -0.08096873491330255, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.00041726906783878803}, {"id": 511, "seek": 270996, "start": 2714.44, "end": 2719.08, "text": " this environment. Maybe it has an innovation that makes it better on this environment. So it invades", "tokens": [50588, 341, 2823, 13, 2704, 309, 575, 364, 8504, 300, 1669, 309, 1101, 322, 341, 2823, 13, 407, 309, 1048, 2977, 50820], "temperature": 0.0, "avg_logprob": -0.08096873491330255, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.00041726906783878803}, {"id": 512, "seek": 270996, "start": 2719.08, "end": 2724.04, "text": " this environment, just like a species in nature could invade a new niche, kicks out that parameter", "tokens": [50820, 341, 2823, 11, 445, 411, 257, 6172, 294, 3687, 727, 39171, 257, 777, 19956, 11, 21293, 484, 300, 13075, 51068], "temperature": 0.0, "avg_logprob": -0.08096873491330255, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.00041726906783878803}, {"id": 513, "seek": 270996, "start": 2724.04, "end": 2729.48, "text": " vector. And now we start building on the back of this innovation here. And then that maybe with a", "tokens": [51068, 8062, 13, 400, 586, 321, 722, 2390, 322, 264, 646, 295, 341, 8504, 510, 13, 400, 550, 300, 1310, 365, 257, 51340], "temperature": 0.0, "avg_logprob": -0.08096873491330255, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.00041726906783878803}, {"id": 514, "seek": 270996, "start": 2729.48, "end": 2734.04, "text": " little bit more optimization comes up with an innovation that then transfers in and becomes", "tokens": [51340, 707, 857, 544, 19618, 1487, 493, 365, 364, 8504, 300, 550, 29137, 294, 293, 3643, 51568], "temperature": 0.0, "avg_logprob": -0.08096873491330255, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.00041726906783878803}, {"id": 515, "seek": 270996, "start": 2734.04, "end": 2738.76, "text": " the best thing we've ever seen on phi 6. And maybe that gets us off the local optimal and solves that", "tokens": [51568, 264, 1151, 551, 321, 600, 1562, 1612, 322, 13107, 1386, 13, 400, 1310, 300, 2170, 505, 766, 264, 2654, 16252, 293, 39890, 300, 51804], "temperature": 0.0, "avg_logprob": -0.08096873491330255, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.00041726906783878803}, {"id": 516, "seek": 273876, "start": 2738.76, "end": 2745.0, "text": " problem. So that is kind of the nature of goal switching. So here we use evolution strategies,", "tokens": [50364, 1154, 13, 407, 300, 307, 733, 295, 264, 3687, 295, 3387, 16493, 13, 407, 510, 321, 764, 9303, 9029, 11, 50676], "temperature": 0.0, "avg_logprob": -0.06509302100356744, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0002531290228944272}, {"id": 517, "seek": 273876, "start": 2745.0, "end": 2752.44, "text": " but any RL algorithm would work. And you can see this little agent here. And it is traversing this", "tokens": [50676, 457, 604, 497, 43, 9284, 576, 589, 13, 400, 291, 393, 536, 341, 707, 9461, 510, 13, 400, 309, 307, 23149, 278, 341, 51048], "temperature": 0.0, "avg_logprob": -0.06509302100356744, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0002531290228944272}, {"id": 518, "seek": 273876, "start": 2752.44, "end": 2757.96, "text": " course. And what you can see is at the beginning, all of the challenges are quite simple. They're", "tokens": [51048, 1164, 13, 400, 437, 291, 393, 536, 307, 412, 264, 2863, 11, 439, 295, 264, 4759, 366, 1596, 2199, 13, 814, 434, 51324], "temperature": 0.0, "avg_logprob": -0.06509302100356744, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0002531290228944272}, {"id": 519, "seek": 273876, "start": 2757.96, "end": 2764.44, "text": " a little tiny stumps, little gaps, just a little ruggedness in the terrain. But over time, the", "tokens": [51324, 257, 707, 5870, 342, 16951, 11, 707, 15031, 11, 445, 257, 707, 42662, 1287, 294, 264, 17674, 13, 583, 670, 565, 11, 264, 51648], "temperature": 0.0, "avg_logprob": -0.06509302100356744, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0002531290228944272}, {"id": 520, "seek": 276444, "start": 2764.44, "end": 2770.04, "text": " agent gets better and better. And the environments automatically start getting harder and harder.", "tokens": [50364, 9461, 2170, 1101, 293, 1101, 13, 400, 264, 12388, 6772, 722, 1242, 6081, 293, 6081, 13, 50644], "temperature": 0.0, "avg_logprob": -0.17802421436753385, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.00016345658514183015}, {"id": 521, "seek": 276444, "start": 2770.04, "end": 2778.12, "text": " So it's kind of like a natural curriculum generation. And you can still, the algorithm", "tokens": [50644, 407, 309, 311, 733, 295, 411, 257, 3303, 14302, 5125, 13, 400, 291, 393, 920, 11, 264, 9284, 51048], "temperature": 0.0, "avg_logprob": -0.17802421436753385, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.00016345658514183015}, {"id": 522, "seek": 276444, "start": 2778.12, "end": 2783.4, "text": " is here is kind of still pushing in separate dimensions, like taller gaps or more ruggedness", "tokens": [51048, 307, 510, 307, 733, 295, 920, 7380, 294, 4994, 12819, 11, 411, 22406, 15031, 420, 544, 42662, 1287, 51312], "temperature": 0.0, "avg_logprob": -0.17802421436753385, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.00016345658514183015}, {"id": 523, "seek": 276444, "start": 2783.4, "end": 2789.96, "text": " or wider gaps. Sorry, I didn't tell her stumps. Later in time, with more training, the algorithm", "tokens": [51312, 420, 11842, 15031, 13, 4919, 11, 286, 994, 380, 980, 720, 342, 16951, 13, 11965, 294, 565, 11, 365, 544, 3097, 11, 264, 9284, 51640], "temperature": 0.0, "avg_logprob": -0.17802421436753385, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.00016345658514183015}, {"id": 524, "seek": 278996, "start": 2789.96, "end": 2794.68, "text": " starts to put together these challenges. Sorry, my dog is barking. So you get things like bigger", "tokens": [50364, 3719, 281, 829, 1214, 613, 4759, 13, 4919, 11, 452, 3000, 307, 32995, 13, 407, 291, 483, 721, 411, 3801, 50600], "temperature": 0.0, "avg_logprob": -0.09504423914728938, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.001244695857167244}, {"id": 525, "seek": 278996, "start": 2794.68, "end": 2800.12, "text": " gaps and stumps and ruggedness all put together. And ultimately, these environments get really,", "tokens": [50600, 15031, 293, 342, 16951, 293, 42662, 1287, 439, 829, 1214, 13, 400, 6284, 11, 613, 12388, 483, 534, 11, 50872], "temperature": 0.0, "avg_logprob": -0.09504423914728938, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.001244695857167244}, {"id": 526, "seek": 278996, "start": 2800.12, "end": 2806.84, "text": " really, really difficult for this little robot to traverse. Here's another challenge that was", "tokens": [50872, 534, 11, 534, 2252, 337, 341, 707, 7881, 281, 45674, 13, 1692, 311, 1071, 3430, 300, 390, 51208], "temperature": 0.0, "avg_logprob": -0.09504423914728938, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.001244695857167244}, {"id": 527, "seek": 278996, "start": 2806.84, "end": 2813.4, "text": " invented and solved by this algorithm. So I think from an aesthetic point of view, it's kind of cool", "tokens": [51208, 14479, 293, 13041, 538, 341, 9284, 13, 407, 286, 519, 490, 364, 20092, 935, 295, 1910, 11, 309, 311, 733, 295, 1627, 51536], "temperature": 0.0, "avg_logprob": -0.09504423914728938, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.001244695857167244}, {"id": 528, "seek": 278996, "start": 2813.4, "end": 2817.8, "text": " because you can think about each one of these robots as its own little creation. It's kind of a", "tokens": [51536, 570, 291, 393, 519, 466, 1184, 472, 295, 613, 14733, 382, 1080, 1065, 707, 8016, 13, 467, 311, 733, 295, 257, 51756], "temperature": 0.0, "avg_logprob": -0.09504423914728938, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.001244695857167244}, {"id": 529, "seek": 281780, "start": 2817.8, "end": 2823.2400000000002, "text": " curiosity. Just like animals in the world, we love to watch nature shows and see different animals", "tokens": [50364, 18769, 13, 1449, 411, 4882, 294, 264, 1002, 11, 321, 959, 281, 1159, 3687, 3110, 293, 536, 819, 4882, 50636], "temperature": 0.0, "avg_logprob": -0.08651171875, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.0005526881432160735}, {"id": 530, "seek": 281780, "start": 2823.2400000000002, "end": 2827.5600000000004, "text": " and how they're different and what they can accomplish and how their bodies are different,", "tokens": [50636, 293, 577, 436, 434, 819, 293, 437, 436, 393, 9021, 293, 577, 641, 7510, 366, 819, 11, 50852], "temperature": 0.0, "avg_logprob": -0.08651171875, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.0005526881432160735}, {"id": 531, "seek": 281780, "start": 2827.5600000000004, "end": 2832.04, "text": " et cetera. So you can kind of think of the agents produced by these things as really interesting", "tokens": [50852, 1030, 11458, 13, 407, 291, 393, 733, 295, 519, 295, 264, 12554, 7126, 538, 613, 721, 382, 534, 1880, 51076], "temperature": 0.0, "avg_logprob": -0.08651171875, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.0005526881432160735}, {"id": 532, "seek": 281780, "start": 2832.04, "end": 2836.92, "text": " aesthetic artifacts. Scientifically, we wanted to see whether or not goal switching in this", "tokens": [51076, 20092, 24617, 13, 18944, 4278, 11, 321, 1415, 281, 536, 1968, 420, 406, 3387, 16493, 294, 341, 51320], "temperature": 0.0, "avg_logprob": -0.08651171875, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.0005526881432160735}, {"id": 533, "seek": 281780, "start": 2836.92, "end": 2841.0, "text": " domain was paying off. And so we did direct optimization in each one of these environments", "tokens": [51320, 9274, 390, 6229, 766, 13, 400, 370, 321, 630, 2047, 19618, 294, 1184, 472, 295, 613, 12388, 51524], "temperature": 0.0, "avg_logprob": -0.08651171875, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.0005526881432160735}, {"id": 534, "seek": 281780, "start": 2841.0, "end": 2845.88, "text": " and found that it failed miserably. That's down here. And with Poet and the goal switching,", "tokens": [51524, 293, 1352, 300, 309, 7612, 17725, 1188, 13, 663, 311, 760, 510, 13, 400, 365, 6165, 302, 293, 264, 3387, 16493, 11, 51768], "temperature": 0.0, "avg_logprob": -0.08651171875, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.0005526881432160735}, {"id": 535, "seek": 284588, "start": 2845.88, "end": 2850.6, "text": " what you see is much, much better performance in each one of these environments. This is the only", "tokens": [50364, 437, 291, 536, 307, 709, 11, 709, 1101, 3389, 294, 1184, 472, 295, 613, 12388, 13, 639, 307, 264, 787, 50600], "temperature": 0.0, "avg_logprob": -0.07639664870042068, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.0004582471738103777}, {"id": 536, "seek": 284588, "start": 2850.6, "end": 2856.36, "text": " way that we know of to go solve these hard problems. And in the paper, there's more of a", "tokens": [50600, 636, 300, 321, 458, 295, 281, 352, 5039, 613, 1152, 2740, 13, 400, 294, 264, 3035, 11, 456, 311, 544, 295, 257, 50888], "temperature": 0.0, "avg_logprob": -0.07639664870042068, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.0004582471738103777}, {"id": 537, "seek": 284588, "start": 2856.36, "end": 2861.4, "text": " detailed study about that claim if you're interested. So I want to show you one anecdote of what popped", "tokens": [50888, 9942, 2979, 466, 300, 3932, 498, 291, 434, 3102, 13, 407, 286, 528, 281, 855, 291, 472, 49845, 295, 437, 21545, 51140], "temperature": 0.0, "avg_logprob": -0.07639664870042068, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.0004582471738103777}, {"id": 538, "seek": 284588, "start": 2861.4, "end": 2865.48, "text": " out in the system. So I think it's so interesting. So here in the simplest possible environment,", "tokens": [51140, 484, 294, 264, 1185, 13, 407, 286, 519, 309, 311, 370, 1880, 13, 407, 510, 294, 264, 22811, 1944, 2823, 11, 51344], "temperature": 0.0, "avg_logprob": -0.07639664870042068, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.0004582471738103777}, {"id": 539, "seek": 284588, "start": 2865.48, "end": 2869.88, "text": " a flat ground, you get this agent here that is optimized for a really long time and it's got", "tokens": [51344, 257, 4962, 2727, 11, 291, 483, 341, 9461, 510, 300, 307, 26941, 337, 257, 534, 938, 565, 293, 309, 311, 658, 51564], "temperature": 0.0, "avg_logprob": -0.07639664870042068, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.0004582471738103777}, {"id": 540, "seek": 286988, "start": 2869.88, "end": 2876.6, "text": " this knee-dragging behavior. And eventually, the system generates a permutation of this", "tokens": [50364, 341, 9434, 12, 67, 424, 10877, 5223, 13, 400, 4728, 11, 264, 1185, 23815, 257, 4784, 11380, 295, 341, 50700], "temperature": 0.0, "avg_logprob": -0.07866624079713035, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0025506699457764626}, {"id": 541, "seek": 286988, "start": 2876.6, "end": 2880.6, "text": " environment, which is a harder challenge that has little tiny stumps. And this knee-dragging", "tokens": [50700, 2823, 11, 597, 307, 257, 6081, 3430, 300, 575, 707, 5870, 342, 16951, 13, 400, 341, 9434, 12, 67, 424, 10877, 50900], "temperature": 0.0, "avg_logprob": -0.07866624079713035, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0025506699457764626}, {"id": 542, "seek": 286988, "start": 2880.6, "end": 2885.7200000000003, "text": " behavior is not very good because it keeps tripping up on these little stumps. So with", "tokens": [50900, 5223, 307, 406, 588, 665, 570, 309, 5965, 1376, 3759, 493, 322, 613, 707, 342, 16951, 13, 407, 365, 51156], "temperature": 0.0, "avg_logprob": -0.07866624079713035, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0025506699457764626}, {"id": 543, "seek": 286988, "start": 2885.7200000000003, "end": 2891.4, "text": " some more optimization in that environment, the agent learns to stand up and it gets faster at", "tokens": [51156, 512, 544, 19618, 294, 300, 2823, 11, 264, 9461, 27152, 281, 1463, 493, 293, 309, 2170, 4663, 412, 51440], "temperature": 0.0, "avg_logprob": -0.07866624079713035, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0025506699457764626}, {"id": 544, "seek": 286988, "start": 2891.4, "end": 2895.88, "text": " that. Now, because the algorithm is always checking any solution to see whether or not it's better", "tokens": [51440, 300, 13, 823, 11, 570, 264, 9284, 307, 1009, 8568, 604, 3827, 281, 536, 1968, 420, 406, 309, 311, 1101, 51664], "temperature": 0.0, "avg_logprob": -0.07866624079713035, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0025506699457764626}, {"id": 545, "seek": 289588, "start": 2895.88, "end": 2901.56, "text": " at invading some other niche, this descendant actually goes back automatically and invades that", "tokens": [50364, 412, 1048, 8166, 512, 661, 19956, 11, 341, 16333, 394, 767, 1709, 646, 6772, 293, 1048, 2977, 300, 50648], "temperature": 0.0, "avg_logprob": -0.06587914733199386, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.0008040261454880238}, {"id": 546, "seek": 289588, "start": 2901.56, "end": 2907.1600000000003, "text": " flat ground, replacing this knee-drager. Now that it knows how to stand up, as you can see here,", "tokens": [50648, 4962, 2727, 11, 19139, 341, 9434, 12, 67, 424, 1321, 13, 823, 300, 309, 3255, 577, 281, 1463, 493, 11, 382, 291, 393, 536, 510, 11, 50928], "temperature": 0.0, "avg_logprob": -0.06587914733199386, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.0008040261454880238}, {"id": 547, "seek": 289588, "start": 2907.1600000000003, "end": 2912.76, "text": " it gets much better performance in that new environment. And then with further optimization,", "tokens": [50928, 309, 2170, 709, 1101, 3389, 294, 300, 777, 2823, 13, 400, 550, 365, 3052, 19618, 11, 51208], "temperature": 0.0, "avg_logprob": -0.06587914733199386, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.0008040261454880238}, {"id": 548, "seek": 289588, "start": 2912.76, "end": 2917.1600000000003, "text": " it ends up with much better performance. Now, because this is a computational system,", "tokens": [51208, 309, 5314, 493, 365, 709, 1101, 3389, 13, 823, 11, 570, 341, 307, 257, 28270, 1185, 11, 51428], "temperature": 0.0, "avg_logprob": -0.06587914733199386, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.0008040261454880238}, {"id": 549, "seek": 289588, "start": 2917.1600000000003, "end": 2921.96, "text": " we could do the counterfactual. We went back to this original agent in the top left and we optimized", "tokens": [51428, 321, 727, 360, 264, 5682, 44919, 901, 13, 492, 1437, 646, 281, 341, 3380, 9461, 294, 264, 1192, 1411, 293, 321, 26941, 51668], "temperature": 0.0, "avg_logprob": -0.06587914733199386, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.0008040261454880238}, {"id": 550, "seek": 292196, "start": 2921.96, "end": 2927.4, "text": " it for an equal amount of computation in that flat ground environment. And it just never learns to", "tokens": [50364, 309, 337, 364, 2681, 2372, 295, 24903, 294, 300, 4962, 2727, 2823, 13, 400, 309, 445, 1128, 27152, 281, 50636], "temperature": 0.0, "avg_logprob": -0.07955980708456448, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.002396378666162491}, {"id": 551, "seek": 292196, "start": 2927.4, "end": 2933.16, "text": " stand up. It's just on a local optimal and it's stuck in its ways. It was only by going into a", "tokens": [50636, 1463, 493, 13, 467, 311, 445, 322, 257, 2654, 16252, 293, 309, 311, 5541, 294, 1080, 2098, 13, 467, 390, 787, 538, 516, 666, 257, 50924], "temperature": 0.0, "avg_logprob": -0.07955980708456448, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.002396378666162491}, {"id": 552, "seek": 292196, "start": 2933.16, "end": 2939.0, "text": " harder environment and coming back that it learned a better behavior and a better strategy. And this", "tokens": [50924, 6081, 2823, 293, 1348, 646, 300, 309, 3264, 257, 1101, 5223, 293, 257, 1101, 5206, 13, 400, 341, 51216], "temperature": 0.0, "avg_logprob": -0.07955980708456448, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.002396378666162491}, {"id": 553, "seek": 292196, "start": 2939.0, "end": 2943.56, "text": " is why I think that it's so hard to design curricula. You would never, as a human, say that you're", "tokens": [51216, 307, 983, 286, 519, 300, 309, 311, 370, 1152, 281, 1715, 13179, 3780, 13, 509, 576, 1128, 11, 382, 257, 1952, 11, 584, 300, 291, 434, 51444], "temperature": 0.0, "avg_logprob": -0.07955980708456448, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.002396378666162491}, {"id": 554, "seek": 292196, "start": 2943.56, "end": 2948.04, "text": " going to take something to a harder environment just to have it solve a simpler environment.", "tokens": [51444, 516, 281, 747, 746, 281, 257, 6081, 2823, 445, 281, 362, 309, 5039, 257, 18587, 2823, 13, 51668], "temperature": 0.0, "avg_logprob": -0.07955980708456448, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.002396378666162491}, {"id": 555, "seek": 294804, "start": 2948.12, "end": 2951.24, "text": " But in this case, that's exactly what was needed to solve this problem.", "tokens": [50368, 583, 294, 341, 1389, 11, 300, 311, 2293, 437, 390, 2978, 281, 5039, 341, 1154, 13, 50524], "temperature": 0.0, "avg_logprob": -0.15203388459091888, "compression_ratio": 1.6556776556776556, "no_speech_prob": 0.0007792576216161251}, {"id": 556, "seek": 294804, "start": 2952.52, "end": 2958.44, "text": " So we go through a quantifying algorithm that goal switching is essential to solve the hardest", "tokens": [50588, 407, 321, 352, 807, 257, 4426, 5489, 9284, 300, 3387, 16493, 307, 7115, 281, 5039, 264, 13158, 50884], "temperature": 0.0, "avg_logprob": -0.15203388459091888, "compression_ratio": 1.6556776556776556, "no_speech_prob": 0.0007792576216161251}, {"id": 557, "seek": 294804, "start": 2958.44, "end": 2964.84, "text": " challenges generated by this system. So future work in this domain, I think there's all sorts of", "tokens": [50884, 4759, 10833, 538, 341, 1185, 13, 407, 2027, 589, 294, 341, 9274, 11, 286, 519, 456, 311, 439, 7527, 295, 51204], "temperature": 0.0, "avg_logprob": -0.15203388459091888, "compression_ratio": 1.6556776556776556, "no_speech_prob": 0.0007792576216161251}, {"id": 558, "seek": 294804, "start": 2964.84, "end": 2971.48, "text": " stuff you could do. Obviously, you could just take it into more complex rich simulators. So,", "tokens": [51204, 1507, 291, 727, 360, 13, 7580, 11, 291, 727, 445, 747, 309, 666, 544, 3997, 4593, 1034, 39265, 13, 407, 11, 51536], "temperature": 0.0, "avg_logprob": -0.15203388459091888, "compression_ratio": 1.6556776556776556, "no_speech_prob": 0.0007792576216161251}, {"id": 559, "seek": 294804, "start": 2971.48, "end": 2976.36, "text": " you know, you could have more complex encodings as well. But here is like the world's from deep", "tokens": [51536, 291, 458, 11, 291, 727, 362, 544, 3997, 2058, 378, 1109, 382, 731, 13, 583, 510, 307, 411, 264, 1002, 311, 490, 2452, 51780], "temperature": 0.0, "avg_logprob": -0.15203388459091888, "compression_ratio": 1.6556776556776556, "no_speech_prob": 0.0007792576216161251}, {"id": 560, "seek": 297636, "start": 2976.36, "end": 2982.04, "text": " mind. But I think it's really kind of pumps my intuition is to watch, you know, what's possible,", "tokens": [50364, 1575, 13, 583, 286, 519, 309, 311, 534, 733, 295, 27648, 452, 24002, 307, 281, 1159, 11, 291, 458, 11, 437, 311, 1944, 11, 50648], "temperature": 0.0, "avg_logprob": -0.12137759965041588, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00419760262593627}, {"id": 561, "seek": 297636, "start": 2982.04, "end": 2986.6800000000003, "text": " what will be possible in the future with more computation. Like imagine what Poet could do", "tokens": [50648, 437, 486, 312, 1944, 294, 264, 2027, 365, 544, 24903, 13, 1743, 3811, 437, 6165, 302, 727, 360, 50880], "temperature": 0.0, "avg_logprob": -0.12137759965041588, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00419760262593627}, {"id": 562, "seek": 297636, "start": 2986.6800000000003, "end": 2991.48, "text": " in a world this complicated, where it has to do with flying creatures and climbing and talking", "tokens": [50880, 294, 257, 1002, 341, 6179, 11, 689, 309, 575, 281, 360, 365, 7137, 12281, 293, 14780, 293, 1417, 51120], "temperature": 0.0, "avg_logprob": -0.12137759965041588, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00419760262593627}, {"id": 563, "seek": 297636, "start": 2991.48, "end": 2996.84, "text": " to other agents, maybe negotiating trades in a market, you know, and if you were doing all of", "tokens": [51120, 281, 661, 12554, 11, 1310, 30396, 21287, 294, 257, 2142, 11, 291, 458, 11, 293, 498, 291, 645, 884, 439, 295, 51388], "temperature": 0.0, "avg_logprob": -0.12137759965041588, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00419760262593627}, {"id": 564, "seek": 297636, "start": 2996.84, "end": 3001.4, "text": " this, you know, what might pop out of the system, I think it's fascinating to consider, both from", "tokens": [51388, 341, 11, 291, 458, 11, 437, 1062, 1665, 484, 295, 264, 1185, 11, 286, 519, 309, 311, 10343, 281, 1949, 11, 1293, 490, 51616], "temperature": 0.0, "avg_logprob": -0.12137759965041588, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00419760262593627}, {"id": 565, "seek": 300140, "start": 3001.48, "end": 3007.32, "text": " a static perspective and from a machine learning perspective. You also could optimize the bodies", "tokens": [50368, 257, 13437, 4585, 293, 490, 257, 3479, 2539, 4585, 13, 509, 611, 727, 19719, 264, 7510, 50660], "temperature": 0.0, "avg_logprob": -0.08291077801561732, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.000779275200329721}, {"id": 566, "seek": 300140, "start": 3007.32, "end": 3011.4, "text": " of the creatures themselves. So in the bottom in the right, you see, you know, I showed you some", "tokens": [50660, 295, 264, 12281, 2969, 13, 407, 294, 264, 2767, 294, 264, 558, 11, 291, 536, 11, 291, 458, 11, 286, 4712, 291, 512, 50864], "temperature": 0.0, "avg_logprob": -0.08291077801561732, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.000779275200329721}, {"id": 567, "seek": 300140, "start": 3011.4, "end": 3016.6800000000003, "text": " work that we did in that vein a while back, but not with Poet. And David Ha has done that in", "tokens": [50864, 589, 300, 321, 630, 294, 300, 30669, 257, 1339, 646, 11, 457, 406, 365, 6165, 302, 13, 400, 4389, 4064, 575, 1096, 300, 294, 51128], "temperature": 0.0, "avg_logprob": -0.08291077801561732, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.000779275200329721}, {"id": 568, "seek": 300140, "start": 3016.6800000000003, "end": 3021.48, "text": " particular environments that are handcrafted. But imagine if you paired body optimization with", "tokens": [51128, 1729, 12388, 300, 366, 1011, 5611, 292, 13, 583, 3811, 498, 291, 25699, 1772, 19618, 365, 51368], "temperature": 0.0, "avg_logprob": -0.08291077801561732, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.000779275200329721}, {"id": 569, "seek": 300140, "start": 3021.48, "end": 3025.1600000000003, "text": " environment generation, then you could really get weird things like you see in nature, where you", "tokens": [51368, 2823, 5125, 11, 550, 291, 727, 534, 483, 3657, 721, 411, 291, 536, 294, 3687, 11, 689, 291, 51552], "temperature": 0.0, "avg_logprob": -0.08291077801561732, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.000779275200329721}, {"id": 570, "seek": 300140, "start": 3025.1600000000003, "end": 3029.32, "text": " have a particular kind of like cave dwelling spider that's optimized to that environment,", "tokens": [51552, 362, 257, 1729, 733, 295, 411, 11730, 41750, 17614, 300, 311, 26941, 281, 300, 2823, 11, 51760], "temperature": 0.0, "avg_logprob": -0.08291077801561732, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.000779275200329721}, {"id": 571, "seek": 302932, "start": 3029.32, "end": 3033.6400000000003, "text": " which is very different from birds that are flying up in the Pacific Northwest.", "tokens": [50364, 597, 307, 588, 819, 490, 9009, 300, 366, 7137, 493, 294, 264, 13335, 26068, 13, 50580], "temperature": 0.0, "avg_logprob": -0.07284711201985677, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.00035690670483745635}, {"id": 572, "seek": 302932, "start": 3034.6000000000004, "end": 3037.8, "text": " So another thing that I think would be interesting would be to combine innovation", "tokens": [50628, 407, 1071, 551, 300, 286, 519, 576, 312, 1880, 576, 312, 281, 10432, 8504, 50788], "temperature": 0.0, "avg_logprob": -0.07284711201985677, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.00035690670483745635}, {"id": 573, "seek": 302932, "start": 3037.8, "end": 3043.4, "text": " engines with modern tools. So imagine if you took something like Dolly, which is this amazing", "tokens": [50788, 12982, 365, 4363, 3873, 13, 407, 3811, 498, 291, 1890, 746, 411, 1144, 13020, 11, 597, 307, 341, 2243, 51068], "temperature": 0.0, "avg_logprob": -0.07284711201985677, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.00035690670483745635}, {"id": 574, "seek": 302932, "start": 3043.4, "end": 3049.56, "text": " new thing produced by my colleagues here at OpenAI. And not only did you have humans asking for", "tokens": [51068, 777, 551, 7126, 538, 452, 7734, 510, 412, 7238, 48698, 13, 400, 406, 787, 630, 291, 362, 6255, 3365, 337, 51376], "temperature": 0.0, "avg_logprob": -0.07284711201985677, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.00035690670483745635}, {"id": 575, "seek": 302932, "start": 3049.56, "end": 3054.52, "text": " particular innovations or particular images from Dolly, but you have the algorithm invent the", "tokens": [51376, 1729, 24283, 420, 1729, 5267, 490, 1144, 13020, 11, 457, 291, 362, 264, 9284, 7962, 264, 51624], "temperature": 0.0, "avg_logprob": -0.07284711201985677, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.00035690670483745635}, {"id": 576, "seek": 302932, "start": 3054.52, "end": 3058.92, "text": " challenge and the solution. So the challenge could be, you know, can you create this? Can you create", "tokens": [51624, 3430, 293, 264, 3827, 13, 407, 264, 3430, 727, 312, 11, 291, 458, 11, 393, 291, 1884, 341, 30, 1664, 291, 1884, 51844], "temperature": 0.0, "avg_logprob": -0.07284711201985677, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.00035690670483745635}, {"id": 577, "seek": 305892, "start": 3059.64, "end": 3063.64, "text": " this? Dolly would then create them. And if they're interesting, you add it to a set. And then you", "tokens": [50400, 341, 30, 1144, 13020, 576, 550, 1884, 552, 13, 400, 498, 436, 434, 1880, 11, 291, 909, 309, 281, 257, 992, 13, 400, 550, 291, 50600], "temperature": 0.0, "avg_logprob": -0.07716458776722783, "compression_ratio": 1.7439759036144578, "no_speech_prob": 0.0009396090754307806}, {"id": 578, "seek": 305892, "start": 3063.64, "end": 3067.7200000000003, "text": " have something that looks at the set of things that are already produced and produces completely new", "tokens": [50600, 362, 746, 300, 1542, 412, 264, 992, 295, 721, 300, 366, 1217, 7126, 293, 14725, 2584, 777, 50804], "temperature": 0.0, "avg_logprob": -0.07716458776722783, "compression_ratio": 1.7439759036144578, "no_speech_prob": 0.0009396090754307806}, {"id": 579, "seek": 305892, "start": 3067.7200000000003, "end": 3072.76, "text": " types of images. That would be awesome to see. And that doesn't have to be limited to images. You", "tokens": [50804, 3467, 295, 5267, 13, 663, 576, 312, 3476, 281, 536, 13, 400, 300, 1177, 380, 362, 281, 312, 5567, 281, 5267, 13, 509, 51056], "temperature": 0.0, "avg_logprob": -0.07716458776722783, "compression_ratio": 1.7439759036144578, "no_speech_prob": 0.0009396090754307806}, {"id": 580, "seek": 305892, "start": 3072.76, "end": 3078.04, "text": " could use then the same technology to do it in different modalities, such as videos and music", "tokens": [51056, 727, 764, 550, 264, 912, 2899, 281, 360, 309, 294, 819, 1072, 16110, 11, 1270, 382, 2145, 293, 1318, 51320], "temperature": 0.0, "avg_logprob": -0.07716458776722783, "compression_ratio": 1.7439759036144578, "no_speech_prob": 0.0009396090754307806}, {"id": 581, "seek": 305892, "start": 3078.04, "end": 3083.2400000000002, "text": " and poetry or algorithmic space. Again, the challenge that remains is how do you detect", "tokens": [51320, 293, 15155, 420, 9284, 299, 1901, 13, 3764, 11, 264, 3430, 300, 7023, 307, 577, 360, 291, 5531, 51580], "temperature": 0.0, "avg_logprob": -0.07716458776722783, "compression_ratio": 1.7439759036144578, "no_speech_prob": 0.0009396090754307806}, {"id": 582, "seek": 305892, "start": 3083.2400000000002, "end": 3087.56, "text": " what's interestingly new? I'll throw it out there that I think you probably with a lot of data could", "tokens": [51580, 437, 311, 25873, 777, 30, 286, 603, 3507, 309, 484, 456, 300, 286, 519, 291, 1391, 365, 257, 688, 295, 1412, 727, 51796], "temperature": 0.0, "avg_logprob": -0.07716458776722783, "compression_ratio": 1.7439759036144578, "no_speech_prob": 0.0009396090754307806}, {"id": 583, "seek": 308756, "start": 3087.56, "end": 3092.52, "text": " learn a function of what humans consider interesting. In fact, if Joel remembers, I sent him a giant", "tokens": [50364, 1466, 257, 2445, 295, 437, 6255, 1949, 1880, 13, 682, 1186, 11, 498, 21522, 26228, 11, 286, 2279, 796, 257, 7410, 50612], "temperature": 0.0, "avg_logprob": -0.09264931457721634, "compression_ratio": 1.6505681818181819, "no_speech_prob": 0.0034820172004401684}, {"id": 584, "seek": 308756, "start": 3092.52, "end": 3096.2, "text": " email saying that I think we should do this with his website, GanReader. We haven't done it yet,", "tokens": [50612, 3796, 1566, 300, 286, 519, 321, 820, 360, 341, 365, 702, 3144, 11, 19461, 40702, 260, 13, 492, 2378, 380, 1096, 309, 1939, 11, 50796], "temperature": 0.0, "avg_logprob": -0.09264931457721634, "compression_ratio": 1.6505681818181819, "no_speech_prob": 0.0034820172004401684}, {"id": 585, "seek": 308756, "start": 3096.2, "end": 3101.56, "text": " but it'd be a great project for a student to take on. So I want to quickly check the time here.", "tokens": [50796, 457, 309, 1116, 312, 257, 869, 1716, 337, 257, 3107, 281, 747, 322, 13, 407, 286, 528, 281, 2661, 1520, 264, 565, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09264931457721634, "compression_ratio": 1.6505681818181819, "no_speech_prob": 0.0034820172004401684}, {"id": 586, "seek": 308756, "start": 3102.6, "end": 3106.7599999999998, "text": " Yeah. So we started a little bit late. So I'm going to race through this because I think you'll", "tokens": [51116, 865, 13, 407, 321, 1409, 257, 707, 857, 3469, 13, 407, 286, 478, 516, 281, 4569, 807, 341, 570, 286, 519, 291, 603, 51324], "temperature": 0.0, "avg_logprob": -0.09264931457721634, "compression_ratio": 1.6505681818181819, "no_speech_prob": 0.0034820172004401684}, {"id": 587, "seek": 308756, "start": 3106.7599999999998, "end": 3111.64, "text": " find it interesting, but I won't be able to go into any detail here. But part two of the talk,", "tokens": [51324, 915, 309, 1880, 11, 457, 286, 1582, 380, 312, 1075, 281, 352, 666, 604, 2607, 510, 13, 583, 644, 732, 295, 264, 751, 11, 51568], "temperature": 0.0, "avg_logprob": -0.09264931457721634, "compression_ratio": 1.6505681818181819, "no_speech_prob": 0.0034820172004401684}, {"id": 588, "seek": 308756, "start": 3112.52, "end": 3116.2799999999997, "text": " which I'll do very quickly, is I wanted to tell you about this entire other arc of research that", "tokens": [51612, 597, 286, 603, 360, 588, 2661, 11, 307, 286, 1415, 281, 980, 291, 466, 341, 2302, 661, 10346, 295, 2132, 300, 51800], "temperature": 0.0, "avg_logprob": -0.09264931457721634, "compression_ratio": 1.6505681818181819, "no_speech_prob": 0.0034820172004401684}, {"id": 589, "seek": 311628, "start": 3116.28, "end": 3121.5600000000004, "text": " we did called AI neuroscience, which is how much we want to study. Just like neuroscientists try to", "tokens": [50364, 321, 630, 1219, 7318, 42762, 11, 597, 307, 577, 709, 321, 528, 281, 2979, 13, 1449, 411, 28813, 5412, 1751, 853, 281, 50628], "temperature": 0.0, "avg_logprob": -0.07957393547584271, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0006261662347242236}, {"id": 590, "seek": 311628, "start": 3121.5600000000004, "end": 3125.48, "text": " study the human brain, we want to study how much the deep neural nets understand about the images", "tokens": [50628, 2979, 264, 1952, 3567, 11, 321, 528, 281, 2979, 577, 709, 264, 2452, 18161, 36170, 1223, 466, 264, 5267, 50824], "temperature": 0.0, "avg_logprob": -0.07957393547584271, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0006261662347242236}, {"id": 591, "seek": 311628, "start": 3125.48, "end": 3132.0400000000004, "text": " that they classify. So we're all familiar with deep neural nets, but they tend to be a black box.", "tokens": [50824, 300, 436, 33872, 13, 407, 321, 434, 439, 4963, 365, 2452, 18161, 36170, 11, 457, 436, 3928, 281, 312, 257, 2211, 2424, 13, 51152], "temperature": 0.0, "avg_logprob": -0.07957393547584271, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0006261662347242236}, {"id": 592, "seek": 311628, "start": 3132.0400000000004, "end": 3137.88, "text": " We don't really know what each neuron in the deep neural net does. But one way neuroscientists probe", "tokens": [51152, 492, 500, 380, 534, 458, 437, 1184, 34090, 294, 264, 2452, 18161, 2533, 775, 13, 583, 472, 636, 28813, 5412, 1751, 22715, 51444], "temperature": 0.0, "avg_logprob": -0.07957393547584271, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0006261662347242236}, {"id": 593, "seek": 311628, "start": 3137.88, "end": 3143.4, "text": " this question is they literally put probes into your neurons and they look for which neurons light", "tokens": [51444, 341, 1168, 307, 436, 3736, 829, 1239, 279, 666, 428, 22027, 293, 436, 574, 337, 597, 22027, 1442, 51720], "temperature": 0.0, "avg_logprob": -0.07957393547584271, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0006261662347242236}, {"id": 594, "seek": 314340, "start": 3143.4, "end": 3148.12, "text": " up in response to which images. For example, they found neurons that light up in response to", "tokens": [50364, 493, 294, 4134, 281, 597, 5267, 13, 1171, 1365, 11, 436, 1352, 22027, 300, 1442, 493, 294, 4134, 281, 50600], "temperature": 0.0, "avg_logprob": -0.08370252277540124, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0037059460300952196}, {"id": 595, "seek": 314340, "start": 3148.12, "end": 3153.2400000000002, "text": " Kobe Bryant or Bill Clinton, for example. And people have called these things like a Kobe Bryant", "tokens": [50600, 46296, 46466, 420, 5477, 15445, 11, 337, 1365, 13, 400, 561, 362, 1219, 613, 721, 411, 257, 46296, 46466, 50856], "temperature": 0.0, "avg_logprob": -0.08370252277540124, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0037059460300952196}, {"id": 596, "seek": 314340, "start": 3153.2400000000002, "end": 3157.48, "text": " neuron, for example, and they respond to very different modalities, such as the name Kobe Bryant,", "tokens": [50856, 34090, 11, 337, 1365, 11, 293, 436, 4196, 281, 588, 819, 1072, 16110, 11, 1270, 382, 264, 1315, 46296, 46466, 11, 51068], "temperature": 0.0, "avg_logprob": -0.08370252277540124, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0037059460300952196}, {"id": 597, "seek": 314340, "start": 3157.48, "end": 3162.2000000000003, "text": " a line drawing him in the Lakers uniform. The question is, you don't really know just because", "tokens": [51068, 257, 1622, 6316, 796, 294, 264, 441, 19552, 9452, 13, 440, 1168, 307, 11, 291, 500, 380, 534, 458, 445, 570, 51304], "temperature": 0.0, "avg_logprob": -0.08370252277540124, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0037059460300952196}, {"id": 598, "seek": 314340, "start": 3162.2000000000003, "end": 3166.92, "text": " the response to those images, if it's a Kobe Bryant neuron, it could be an LA Laker neuron", "tokens": [51304, 264, 4134, 281, 729, 5267, 11, 498, 309, 311, 257, 46296, 46466, 34090, 11, 309, 727, 312, 364, 9855, 441, 4003, 34090, 51540], "temperature": 0.0, "avg_logprob": -0.08370252277540124, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0037059460300952196}, {"id": 599, "seek": 314340, "start": 3166.92, "end": 3172.6800000000003, "text": " instead of a Kobe Bryant image neuron, for example. So we thought the ideal task would be to synthesize", "tokens": [51540, 2602, 295, 257, 46296, 46466, 3256, 34090, 11, 337, 1365, 13, 407, 321, 1194, 264, 7157, 5633, 576, 312, 281, 26617, 1125, 51828], "temperature": 0.0, "avg_logprob": -0.08370252277540124, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0037059460300952196}, {"id": 600, "seek": 317268, "start": 3172.68, "end": 3177.56, "text": " the images that maximally activate that neuron. And if you did that and you got these images,", "tokens": [50364, 264, 5267, 300, 5138, 379, 13615, 300, 34090, 13, 400, 498, 291, 630, 300, 293, 291, 658, 613, 5267, 11, 50608], "temperature": 0.0, "avg_logprob": -0.08178432727682179, "compression_ratio": 2.049122807017544, "no_speech_prob": 0.00023778266040608287}, {"id": 601, "seek": 317268, "start": 3177.56, "end": 3182.2, "text": " then you'd know, oh, that's a Laker neuron, not a Kobe Bryant neuron. But if you got these images,", "tokens": [50608, 550, 291, 1116, 458, 11, 1954, 11, 300, 311, 257, 441, 4003, 34090, 11, 406, 257, 46296, 46466, 34090, 13, 583, 498, 291, 658, 613, 5267, 11, 50840], "temperature": 0.0, "avg_logprob": -0.08178432727682179, "compression_ratio": 2.049122807017544, "no_speech_prob": 0.00023778266040608287}, {"id": 602, "seek": 317268, "start": 3182.2, "end": 3187.3999999999996, "text": " you'd know it's a Kobe Bryant neuron. So this is actually possible with artificial neural networks,", "tokens": [50840, 291, 1116, 458, 309, 311, 257, 46296, 46466, 34090, 13, 407, 341, 307, 767, 1944, 365, 11677, 18161, 9590, 11, 51100], "temperature": 0.0, "avg_logprob": -0.08178432727682179, "compression_ratio": 2.049122807017544, "no_speech_prob": 0.00023778266040608287}, {"id": 603, "seek": 317268, "start": 3187.3999999999996, "end": 3192.12, "text": " but you can do is you can take a neural net and then you could have like an artist, an AI artist", "tokens": [51100, 457, 291, 393, 360, 307, 291, 393, 747, 257, 18161, 2533, 293, 550, 291, 727, 362, 411, 364, 5748, 11, 364, 7318, 5748, 51336], "temperature": 0.0, "avg_logprob": -0.08178432727682179, "compression_ratio": 2.049122807017544, "no_speech_prob": 0.00023778266040608287}, {"id": 604, "seek": 317268, "start": 3192.12, "end": 3198.2799999999997, "text": " that's trying to generate an image to activate this particular neuron here. And what you can do is", "tokens": [51336, 300, 311, 1382, 281, 8460, 364, 3256, 281, 13615, 341, 1729, 34090, 510, 13, 400, 437, 291, 393, 360, 307, 51644], "temperature": 0.0, "avg_logprob": -0.08178432727682179, "compression_ratio": 2.049122807017544, "no_speech_prob": 0.00023778266040608287}, {"id": 605, "seek": 317268, "start": 3198.2799999999997, "end": 3202.2799999999997, "text": " you can use backprop. So the artist generates an image and then you just follow the gradient to", "tokens": [51644, 291, 393, 764, 646, 79, 1513, 13, 407, 264, 5748, 23815, 364, 3256, 293, 550, 291, 445, 1524, 264, 16235, 281, 51844], "temperature": 0.0, "avg_logprob": -0.08178432727682179, "compression_ratio": 2.049122807017544, "no_speech_prob": 0.00023778266040608287}, {"id": 606, "seek": 320228, "start": 3202.28, "end": 3206.52, "text": " increase this neuron until you get an image that lights up that neuron, and it might look like this.", "tokens": [50364, 3488, 341, 34090, 1826, 291, 483, 364, 3256, 300, 5811, 493, 300, 34090, 11, 293, 309, 1062, 574, 411, 341, 13, 50576], "temperature": 0.0, "avg_logprob": -0.11015434265136718, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00033530910150147974}, {"id": 607, "seek": 320228, "start": 3206.52, "end": 3211.0, "text": " And you can do the same technique for all the intermediate neurons in the neural net.", "tokens": [50576, 400, 291, 393, 360, 264, 912, 6532, 337, 439, 264, 19376, 22027, 294, 264, 18161, 2533, 13, 50800], "temperature": 0.0, "avg_logprob": -0.11015434265136718, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00033530910150147974}, {"id": 608, "seek": 320228, "start": 3211.7200000000003, "end": 3217.0, "text": " We call this deep visualization. Our first attempt at this actually was that same paper,", "tokens": [50836, 492, 818, 341, 2452, 25801, 13, 2621, 700, 5217, 412, 341, 767, 390, 300, 912, 3035, 11, 51100], "temperature": 0.0, "avg_logprob": -0.11015434265136718, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00033530910150147974}, {"id": 609, "seek": 320228, "start": 3217.0, "end": 3222.76, "text": " deep neural nets are easily fooled when we did it with CPPNs here or a direct encoding on the left", "tokens": [51100, 2452, 18161, 36170, 366, 3612, 33372, 562, 321, 630, 309, 365, 383, 17755, 45, 82, 510, 420, 257, 2047, 43430, 322, 264, 1411, 51388], "temperature": 0.0, "avg_logprob": -0.11015434265136718, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00033530910150147974}, {"id": 610, "seek": 320228, "start": 3222.76, "end": 3228.44, "text": " here, or with backprop on the right, we got images that did not look at all like things that they're", "tokens": [51388, 510, 11, 420, 365, 646, 79, 1513, 322, 264, 558, 11, 321, 658, 5267, 300, 630, 406, 574, 412, 439, 411, 721, 300, 436, 434, 51672], "temperature": 0.0, "avg_logprob": -0.11015434265136718, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.00033530910150147974}, {"id": 611, "seek": 322844, "start": 3228.44, "end": 3234.44, "text": " supposed to, but the neural net was perfectly sure is a peacock or chimpanzee. And you know what", "tokens": [50364, 3442, 281, 11, 457, 264, 18161, 2533, 390, 6239, 988, 307, 257, 43370, 1560, 420, 18375, 48410, 68, 13, 400, 291, 458, 437, 50664], "temperature": 0.0, "avg_logprob": -0.0876675304184612, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.0019264442380517721}, {"id": 612, "seek": 322844, "start": 3234.44, "end": 3239.2400000000002, "text": " happened with that paper. We then went on and started asking questions like why are these neural", "tokens": [50664, 2011, 365, 300, 3035, 13, 492, 550, 1437, 322, 293, 1409, 3365, 1651, 411, 983, 366, 613, 18161, 50904], "temperature": 0.0, "avg_logprob": -0.0876675304184612, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.0019264442380517721}, {"id": 613, "seek": 322844, "start": 3239.2400000000002, "end": 3243.8, "text": " nets easily fooled? And I don't have time to get into a lot of the details here, but what we basically", "tokens": [50904, 36170, 3612, 33372, 30, 400, 286, 500, 380, 362, 565, 281, 483, 666, 257, 688, 295, 264, 4365, 510, 11, 457, 437, 321, 1936, 51132], "temperature": 0.0, "avg_logprob": -0.0876675304184612, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.0019264442380517721}, {"id": 614, "seek": 322844, "start": 3243.8, "end": 3249.96, "text": " thought is that maybe deep neural nets do recognize the images they're supposed to like a lion or a", "tokens": [51132, 1194, 307, 300, 1310, 2452, 18161, 36170, 360, 5521, 264, 5267, 436, 434, 3442, 281, 411, 257, 17226, 420, 257, 51440], "temperature": 0.0, "avg_logprob": -0.0876675304184612, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.0019264442380517721}, {"id": 615, "seek": 322844, "start": 3249.96, "end": 3256.04, "text": " dolphin, but maybe they recognize a whole lot of other things also as in that class unnatural images.", "tokens": [51440, 46759, 11, 457, 1310, 436, 5521, 257, 1379, 688, 295, 661, 721, 611, 382, 294, 300, 1508, 43470, 5267, 13, 51744], "temperature": 0.0, "avg_logprob": -0.0876675304184612, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.0019264442380517721}, {"id": 616, "seek": 325604, "start": 3256.04, "end": 3261.8, "text": " So if we could stop the artist from generating unnatural images and only stay to the space of", "tokens": [50364, 407, 498, 321, 727, 1590, 264, 5748, 490, 17746, 43470, 5267, 293, 787, 1754, 281, 264, 1901, 295, 50652], "temperature": 0.0, "avg_logprob": -0.10290907030907746, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0001022873111651279}, {"id": 617, "seek": 325604, "start": 3261.8, "end": 3267.48, "text": " natural images, then we might find out what that neuron really is for and what it's been trained", "tokens": [50652, 3303, 5267, 11, 550, 321, 1062, 915, 484, 437, 300, 34090, 534, 307, 337, 293, 437, 309, 311, 668, 8895, 50936], "temperature": 0.0, "avg_logprob": -0.10290907030907746, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0001022873111651279}, {"id": 618, "seek": 325604, "start": 3267.48, "end": 3272.52, "text": " to see within the space of natural images. So skipping over some of the details here,", "tokens": [50936, 281, 536, 1951, 264, 1901, 295, 3303, 5267, 13, 407, 31533, 670, 512, 295, 264, 4365, 510, 11, 51188], "temperature": 0.0, "avg_logprob": -0.10290907030907746, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0001022873111651279}, {"id": 619, "seek": 325604, "start": 3273.48, "end": 3277.88, "text": " the fooling work started out saying maybe these deep neural nets don't really understand at all", "tokens": [51236, 264, 7979, 278, 589, 1409, 484, 1566, 1310, 613, 2452, 18161, 36170, 500, 380, 534, 1223, 412, 439, 51456], "temperature": 0.0, "avg_logprob": -0.10290907030907746, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0001022873111651279}, {"id": 620, "seek": 325604, "start": 3277.88, "end": 3281.88, "text": " what they're classifying. They're just locking on to spurious correlations like that there's a", "tokens": [51456, 437, 436, 434, 1508, 5489, 13, 814, 434, 445, 23954, 322, 281, 637, 24274, 13983, 763, 411, 300, 456, 311, 257, 51656], "temperature": 0.0, "avg_logprob": -0.10290907030907746, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0001022873111651279}, {"id": 621, "seek": 328188, "start": 3281.96, "end": 3285.96, "text": " orange texture. If you see orange, you know, this kind of orange texture next to blue color", "tokens": [50368, 7671, 8091, 13, 759, 291, 536, 7671, 11, 291, 458, 11, 341, 733, 295, 7671, 8091, 958, 281, 3344, 2017, 50568], "temperature": 0.0, "avg_logprob": -0.08602924613685875, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.0012064110487699509}, {"id": 622, "seek": 328188, "start": 3285.96, "end": 3290.6800000000003, "text": " of starfish, but they never learned like what a five-legged starfish is because they didn't", "tokens": [50568, 295, 3543, 11608, 11, 457, 436, 1128, 3264, 411, 437, 257, 1732, 12, 306, 12244, 3543, 11608, 307, 570, 436, 994, 380, 50804], "temperature": 0.0, "avg_logprob": -0.08602924613685875, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.0012064110487699509}, {"id": 623, "seek": 328188, "start": 3290.6800000000003, "end": 3294.84, "text": " need to to solve the problem. We wanted to see whether or not there is that notion of like a", "tokens": [50804, 643, 281, 281, 5039, 264, 1154, 13, 492, 1415, 281, 536, 1968, 420, 406, 456, 307, 300, 10710, 295, 411, 257, 51012], "temperature": 0.0, "avg_logprob": -0.08602924613685875, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.0012064110487699509}, {"id": 624, "seek": 328188, "start": 3294.84, "end": 3300.12, "text": " five-legged starfish in the network. So in take two, what we tried to do is we added more manual", "tokens": [51012, 1732, 12, 306, 12244, 3543, 11608, 294, 264, 3209, 13, 407, 294, 747, 732, 11, 437, 321, 3031, 281, 360, 307, 321, 3869, 544, 9688, 51276], "temperature": 0.0, "avg_logprob": -0.08602924613685875, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.0012064110487699509}, {"id": 625, "seek": 328188, "start": 3300.12, "end": 3305.7200000000003, "text": " priors to try to constrain the image generator, the artist, to generate only natural images. And", "tokens": [51276, 1790, 830, 281, 853, 281, 1817, 7146, 264, 3256, 19265, 11, 264, 5748, 11, 281, 8460, 787, 3303, 5267, 13, 400, 51556], "temperature": 0.0, "avg_logprob": -0.08602924613685875, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.0012064110487699509}, {"id": 626, "seek": 328188, "start": 3305.7200000000003, "end": 3311.0, "text": " when we add that extra constraint, then we get images, you know, previously people had done that", "tokens": [51556, 562, 321, 909, 300, 2857, 25534, 11, 550, 321, 483, 5267, 11, 291, 458, 11, 8046, 561, 632, 1096, 300, 51820], "temperature": 0.0, "avg_logprob": -0.08602924613685875, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.0012064110487699509}, {"id": 627, "seek": 331100, "start": 3311.0, "end": 3317.64, "text": " and they kind of looked like this. You start to see dumbbells and dolomations. These are the", "tokens": [50364, 293, 436, 733, 295, 2956, 411, 341, 13, 509, 722, 281, 536, 39316, 82, 293, 17858, 298, 763, 13, 1981, 366, 264, 50696], "temperature": 0.0, "avg_logprob": -0.13651584366620598, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.00033530365908518434}, {"id": 628, "seek": 331100, "start": 3317.64, "end": 3322.12, "text": " ones that we got with slightly better priors. And you can start to see that the network does", "tokens": [50696, 2306, 300, 321, 658, 365, 4748, 1101, 1790, 830, 13, 400, 291, 393, 722, 281, 536, 300, 264, 3209, 775, 50920], "temperature": 0.0, "avg_logprob": -0.13651584366620598, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.00033530365908518434}, {"id": 629, "seek": 331100, "start": 3322.12, "end": 3327.08, "text": " actually kind of know what a flamingo is or a beetle. It's an interesting historical side note.", "tokens": [50920, 767, 733, 295, 458, 437, 257, 45718, 78, 307, 420, 257, 49735, 13, 467, 311, 364, 1880, 8584, 1252, 3637, 13, 51168], "temperature": 0.0, "avg_logprob": -0.13651584366620598, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.00033530365908518434}, {"id": 630, "seek": 331100, "start": 3327.08, "end": 3332.12, "text": " These images here in this work inspire deep dream, which is also done by Alex over at Google.", "tokens": [51168, 1981, 5267, 510, 294, 341, 589, 15638, 2452, 3055, 11, 597, 307, 611, 1096, 538, 5202, 670, 412, 3329, 13, 51420], "temperature": 0.0, "avg_logprob": -0.13651584366620598, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.00033530365908518434}, {"id": 631, "seek": 331100, "start": 3332.12, "end": 3337.56, "text": " And that stuff is super cool if you haven't seen it. And then third take, we tried to add even", "tokens": [51420, 400, 300, 1507, 307, 1687, 1627, 498, 291, 2378, 380, 1612, 309, 13, 400, 550, 2636, 747, 11, 321, 3031, 281, 909, 754, 51692], "temperature": 0.0, "avg_logprob": -0.13651584366620598, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.00033530365908518434}, {"id": 632, "seek": 333756, "start": 3337.56, "end": 3343.88, "text": " better priors, manually designed priors, and what you get are these images here. And I want to stop", "tokens": [50364, 1101, 1790, 830, 11, 16945, 4761, 1790, 830, 11, 293, 437, 291, 483, 366, 613, 5267, 510, 13, 400, 286, 528, 281, 1590, 50680], "temperature": 0.0, "avg_logprob": -0.08748453855514526, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0012841217685490847}, {"id": 633, "seek": 333756, "start": 3343.88, "end": 3348.12, "text": " for a moment and kind of reflect on this from an aesthetic perspective. We're trying to do better", "tokens": [50680, 337, 257, 1623, 293, 733, 295, 5031, 322, 341, 490, 364, 20092, 4585, 13, 492, 434, 1382, 281, 360, 1101, 50892], "temperature": 0.0, "avg_logprob": -0.08748453855514526, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0012841217685490847}, {"id": 634, "seek": 333756, "start": 3348.12, "end": 3354.36, "text": " and better science. We're creating different algorithms or different hand-coded priors to", "tokens": [50892, 293, 1101, 3497, 13, 492, 434, 4084, 819, 14642, 420, 819, 1011, 12, 66, 12340, 1790, 830, 281, 51204], "temperature": 0.0, "avg_logprob": -0.08748453855514526, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0012841217685490847}, {"id": 635, "seek": 333756, "start": 3354.36, "end": 3359.4, "text": " kind of accomplish the scientific quest. But if you look at the different images, each one of them", "tokens": [51204, 733, 295, 9021, 264, 8134, 866, 13, 583, 498, 291, 574, 412, 264, 819, 5267, 11, 1184, 472, 295, 552, 51456], "temperature": 0.0, "avg_logprob": -0.08748453855514526, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0012841217685490847}, {"id": 636, "seek": 333756, "start": 3359.4, "end": 3364.2, "text": " has a different style. And I think it's kind of interesting that like slight tweaks to algorithms", "tokens": [51456, 575, 257, 819, 3758, 13, 400, 286, 519, 309, 311, 733, 295, 1880, 300, 411, 4036, 46664, 281, 14642, 51696], "temperature": 0.0, "avg_logprob": -0.08748453855514526, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0012841217685490847}, {"id": 637, "seek": 336420, "start": 3364.2, "end": 3368.68, "text": " produce wildly different artistic styles. It's like all these different artists are out there", "tokens": [50364, 5258, 34731, 819, 17090, 13273, 13, 467, 311, 411, 439, 613, 819, 6910, 366, 484, 456, 50588], "temperature": 0.0, "avg_logprob": -0.06428239440917968, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.001206292537972331}, {"id": 638, "seek": 336420, "start": 3368.68, "end": 3371.96, "text": " and you just kind of are searching through the space of artists kind of accidentally", "tokens": [50588, 293, 291, 445, 733, 295, 366, 10808, 807, 264, 1901, 295, 6910, 733, 295, 15715, 50752], "temperature": 0.0, "avg_logprob": -0.06428239440917968, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.001206292537972331}, {"id": 639, "seek": 336420, "start": 3371.96, "end": 3376.04, "text": " while you're doing your science. So this style is very different from this style. And I actually", "tokens": [50752, 1339, 291, 434, 884, 428, 3497, 13, 407, 341, 3758, 307, 588, 819, 490, 341, 3758, 13, 400, 286, 767, 50956], "temperature": 0.0, "avg_logprob": -0.06428239440917968, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.001206292537972331}, {"id": 640, "seek": 336420, "start": 3376.04, "end": 3380.3599999999997, "text": " think this is just really beautiful. Like if I saw this in an art museum, I would think that this", "tokens": [50956, 519, 341, 307, 445, 534, 2238, 13, 1743, 498, 286, 1866, 341, 294, 364, 1523, 8441, 11, 286, 576, 519, 300, 341, 51172], "temperature": 0.0, "avg_logprob": -0.06428239440917968, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.001206292537972331}, {"id": 641, "seek": 336420, "start": 3380.3599999999997, "end": 3384.12, "text": " is beautiful art, even though it was produced purely for scientific reasons and we had no", "tokens": [51172, 307, 2238, 1523, 11, 754, 1673, 309, 390, 7126, 17491, 337, 8134, 4112, 293, 321, 632, 572, 51360], "temperature": 0.0, "avg_logprob": -0.06428239440917968, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.001206292537972331}, {"id": 642, "seek": 336420, "start": 3384.12, "end": 3389.72, "text": " intention of producing images in this style. We then went on for one more take at this. We tried", "tokens": [51360, 7789, 295, 10501, 5267, 294, 341, 3758, 13, 492, 550, 1437, 322, 337, 472, 544, 747, 412, 341, 13, 492, 3031, 51640], "temperature": 0.0, "avg_logprob": -0.06428239440917968, "compression_ratio": 1.8543046357615893, "no_speech_prob": 0.001206292537972331}, {"id": 643, "seek": 338972, "start": 3389.72, "end": 3395.7999999999997, "text": " to say, okay, we're machine learning researchers instead of manually encoding what characterizes", "tokens": [50364, 281, 584, 11, 1392, 11, 321, 434, 3479, 2539, 10309, 2602, 295, 16945, 43430, 437, 2517, 5660, 50668], "temperature": 0.0, "avg_logprob": -0.10698700766278128, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.002251218305900693}, {"id": 644, "seek": 338972, "start": 3395.7999999999997, "end": 3400.4399999999996, "text": " a natural image. Let's learn it. And so we start learning the natural image priors and our papers", "tokens": [50668, 257, 3303, 3256, 13, 961, 311, 1466, 309, 13, 400, 370, 321, 722, 2539, 264, 3303, 3256, 1790, 830, 293, 527, 10577, 50900], "temperature": 0.0, "avg_logprob": -0.10698700766278128, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.002251218305900693}, {"id": 645, "seek": 338972, "start": 3400.4399999999996, "end": 3405.08, "text": " are full of lots of details on this. And the way that we do this, we have a generator kind of like", "tokens": [50900, 366, 1577, 295, 3195, 295, 4365, 322, 341, 13, 400, 264, 636, 300, 321, 360, 341, 11, 321, 362, 257, 19265, 733, 295, 411, 51132], "temperature": 0.0, "avg_logprob": -0.10698700766278128, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.002251218305900693}, {"id": 646, "seek": 338972, "start": 3405.08, "end": 3410.52, "text": " the generator in a GAN. We hook it up to the target network we're interrogating. And then we try to", "tokens": [51132, 264, 19265, 294, 257, 460, 1770, 13, 492, 6328, 309, 493, 281, 264, 3779, 3209, 321, 434, 24871, 990, 13, 400, 550, 321, 853, 281, 51404], "temperature": 0.0, "avg_logprob": -0.10698700766278128, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.002251218305900693}, {"id": 647, "seek": 338972, "start": 3410.52, "end": 3414.7599999999998, "text": " search in a latent code to produce an image that activates a certain neuron in question.", "tokens": [51404, 3164, 294, 257, 48994, 3089, 281, 5258, 364, 3256, 300, 43869, 257, 1629, 34090, 294, 1168, 13, 51616], "temperature": 0.0, "avg_logprob": -0.10698700766278128, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.002251218305900693}, {"id": 648, "seek": 341476, "start": 3415.4, "end": 3420.28, "text": " And when we did that, we got these images, which at the time were some of the most realistic images", "tokens": [50396, 400, 562, 321, 630, 300, 11, 321, 658, 613, 5267, 11, 597, 412, 264, 565, 645, 512, 295, 264, 881, 12465, 5267, 50640], "temperature": 0.0, "avg_logprob": -0.0838516370385094, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.019712604582309723}, {"id": 649, "seek": 341476, "start": 3420.28, "end": 3425.88, "text": " deep neural nets had ever produced. You were seeing realistic lawnmowers and lemons and barns", "tokens": [50640, 2452, 18161, 36170, 632, 1562, 7126, 13, 509, 645, 2577, 12465, 19915, 76, 23054, 293, 47098, 293, 18492, 82, 50920], "temperature": 0.0, "avg_logprob": -0.0838516370385094, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.019712604582309723}, {"id": 650, "seek": 341476, "start": 3425.88, "end": 3432.36, "text": " and candles. These images are not great by modern standards, but this is 2016. Here are other images", "tokens": [50920, 293, 23774, 13, 1981, 5267, 366, 406, 869, 538, 4363, 7787, 11, 457, 341, 307, 6549, 13, 1692, 366, 661, 5267, 51244], "temperature": 0.0, "avg_logprob": -0.0838516370385094, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.019712604582309723}, {"id": 651, "seek": 341476, "start": 3432.36, "end": 3438.1200000000003, "text": " in this class. And for the first time ever, the images were starting to look photorealistic.", "tokens": [51244, 294, 341, 1508, 13, 400, 337, 264, 700, 565, 1562, 11, 264, 5267, 645, 2891, 281, 574, 2409, 418, 304, 3142, 13, 51532], "temperature": 0.0, "avg_logprob": -0.0838516370385094, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.019712604582309723}, {"id": 652, "seek": 341476, "start": 3438.1200000000003, "end": 3442.2000000000003, "text": " Like these are the synthetic images for this class. And these are the real images.", "tokens": [51532, 1743, 613, 366, 264, 23420, 5267, 337, 341, 1508, 13, 400, 613, 366, 264, 957, 5267, 13, 51736], "temperature": 0.0, "avg_logprob": -0.0838516370385094, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.019712604582309723}, {"id": 653, "seek": 344220, "start": 3442.2, "end": 3445.96, "text": " And, you know, I don't think that you would really be able to tell the difference if I had swapped", "tokens": [50364, 400, 11, 291, 458, 11, 286, 500, 380, 519, 300, 291, 576, 534, 312, 1075, 281, 980, 264, 2649, 498, 286, 632, 50011, 50552], "temperature": 0.0, "avg_logprob": -0.06622854868570964, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.0005191898089833558}, {"id": 654, "seek": 344220, "start": 3445.96, "end": 3451.64, "text": " them unless you look very carefully. So compared to the best work at the time, which is on the left,", "tokens": [50552, 552, 5969, 291, 574, 588, 7500, 13, 407, 5347, 281, 264, 1151, 589, 412, 264, 565, 11, 597, 307, 322, 264, 1411, 11, 50836], "temperature": 0.0, "avg_logprob": -0.06622854868570964, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.0005191898089833558}, {"id": 655, "seek": 344220, "start": 3451.64, "end": 3458.04, "text": " these images were a big step up. And they helped us confirm this hypothesis, which is basically,", "tokens": [50836, 613, 5267, 645, 257, 955, 1823, 493, 13, 400, 436, 4254, 505, 9064, 341, 17291, 11, 597, 307, 1936, 11, 51156], "temperature": 0.0, "avg_logprob": -0.06622854868570964, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.0005191898089833558}, {"id": 656, "seek": 344220, "start": 3458.04, "end": 3461.7999999999997, "text": " if this is the space of natural images, these networks do understand what it means to be a", "tokens": [51156, 498, 341, 307, 264, 1901, 295, 3303, 5267, 11, 613, 9590, 360, 1223, 437, 309, 1355, 281, 312, 257, 51344], "temperature": 0.0, "avg_logprob": -0.06622854868570964, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.0005191898089833558}, {"id": 657, "seek": 344220, "start": 3461.7999999999997, "end": 3468.52, "text": " lawnmower. Like if this blue line here is the class of lawnmowers, then they do stay to, if you", "tokens": [51344, 19915, 76, 968, 13, 1743, 498, 341, 3344, 1622, 510, 307, 264, 1508, 295, 19915, 76, 23054, 11, 550, 436, 360, 1754, 281, 11, 498, 291, 51680], "temperature": 0.0, "avg_logprob": -0.06622854868570964, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.0005191898089833558}, {"id": 658, "seek": 346852, "start": 3468.52, "end": 3473.48, "text": " keep them in the natural, if you only generate images in the natural image space, then you do", "tokens": [50364, 1066, 552, 294, 264, 3303, 11, 498, 291, 787, 8460, 5267, 294, 264, 3303, 3256, 1901, 11, 550, 291, 360, 50612], "temperature": 0.0, "avg_logprob": -0.0512508900045491, "compression_ratio": 1.898989898989899, "no_speech_prob": 0.0006877682753838599}, {"id": 659, "seek": 346852, "start": 3473.48, "end": 3478.36, "text": " get a lawnmower. But if you let it generate images anywhere in the space, like all the way out here,", "tokens": [50612, 483, 257, 19915, 76, 968, 13, 583, 498, 291, 718, 309, 8460, 5267, 4992, 294, 264, 1901, 11, 411, 439, 264, 636, 484, 510, 11, 50856], "temperature": 0.0, "avg_logprob": -0.0512508900045491, "compression_ratio": 1.898989898989899, "no_speech_prob": 0.0006877682753838599}, {"id": 660, "seek": 346852, "start": 3478.36, "end": 3483.08, "text": " then it also, the network will similarly say this garbage here is in the class of what it", "tokens": [50856, 550, 309, 611, 11, 264, 3209, 486, 14138, 584, 341, 14150, 510, 307, 294, 264, 1508, 295, 437, 309, 51092], "temperature": 0.0, "avg_logprob": -0.0512508900045491, "compression_ratio": 1.898989898989899, "no_speech_prob": 0.0006877682753838599}, {"id": 661, "seek": 346852, "start": 3483.08, "end": 3487.4, "text": " means to be a lawnmower. And so if we want for aesthetic purposes to have neural nets generate", "tokens": [51092, 1355, 281, 312, 257, 19915, 76, 968, 13, 400, 370, 498, 321, 528, 337, 20092, 9932, 281, 362, 18161, 36170, 8460, 51308], "temperature": 0.0, "avg_logprob": -0.0512508900045491, "compression_ratio": 1.898989898989899, "no_speech_prob": 0.0006877682753838599}, {"id": 662, "seek": 346852, "start": 3487.4, "end": 3493.0, "text": " realistic stuff, we got to get it focused on something that both is natural and activates", "tokens": [51308, 12465, 1507, 11, 321, 658, 281, 483, 309, 5178, 322, 746, 300, 1293, 307, 3303, 293, 43869, 51588], "temperature": 0.0, "avg_logprob": -0.0512508900045491, "compression_ratio": 1.898989898989899, "no_speech_prob": 0.0006877682753838599}, {"id": 663, "seek": 346852, "start": 3493.0, "end": 3497.72, "text": " the network's classification as opposed to way out here. And GANs do this also, but they do it", "tokens": [51588, 264, 3209, 311, 21538, 382, 8851, 281, 636, 484, 510, 13, 400, 460, 1770, 82, 360, 341, 611, 11, 457, 436, 360, 309, 51824], "temperature": 0.0, "avg_logprob": -0.0512508900045491, "compression_ratio": 1.898989898989899, "no_speech_prob": 0.0006877682753838599}, {"id": 664, "seek": 349772, "start": 3497.72, "end": 3502.68, "text": " via a very different mechanism. So I told you, you could look at each individual neuron within", "tokens": [50364, 5766, 257, 588, 819, 7513, 13, 407, 286, 1907, 291, 11, 291, 727, 574, 412, 1184, 2609, 34090, 1951, 50612], "temperature": 0.0, "avg_logprob": -0.07766054678654326, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0012063226895406842}, {"id": 665, "seek": 349772, "start": 3502.68, "end": 3507.3199999999997, "text": " the network. I don't have time to go through this now, but if you're interested, then I encourage", "tokens": [50612, 264, 3209, 13, 286, 500, 380, 362, 565, 281, 352, 807, 341, 586, 11, 457, 498, 291, 434, 3102, 11, 550, 286, 5373, 50844], "temperature": 0.0, "avg_logprob": -0.07766054678654326, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0012063226895406842}, {"id": 666, "seek": 349772, "start": 3507.3199999999997, "end": 3511.08, "text": " you to kind of go into the paper and look, you could kind of fly around the neural net and see", "tokens": [50844, 291, 281, 733, 295, 352, 666, 264, 3035, 293, 574, 11, 291, 727, 733, 295, 3603, 926, 264, 18161, 2533, 293, 536, 51032], "temperature": 0.0, "avg_logprob": -0.07766054678654326, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0012063226895406842}, {"id": 667, "seek": 349772, "start": 3511.08, "end": 3516.2, "text": " that you get things like cargill grill detectors and buckets and bird heads. And as you go up in", "tokens": [51032, 300, 291, 483, 721, 411, 1032, 70, 373, 16492, 46866, 293, 32191, 293, 5255, 8050, 13, 400, 382, 291, 352, 493, 294, 51288], "temperature": 0.0, "avg_logprob": -0.07766054678654326, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0012063226895406842}, {"id": 668, "seek": 349772, "start": 3516.2, "end": 3520.68, "text": " the network, you get these really weird concepts like one-eyed turtles and like arches over water", "tokens": [51288, 264, 3209, 11, 291, 483, 613, 534, 3657, 10392, 411, 472, 12, 37860, 32422, 293, 411, 594, 3781, 670, 1281, 51512], "temperature": 0.0, "avg_logprob": -0.07766054678654326, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0012063226895406842}, {"id": 669, "seek": 349772, "start": 3520.68, "end": 3525.08, "text": " until you eventually get the class neurons where we know what they are because we've grounded them", "tokens": [51512, 1826, 291, 4728, 483, 264, 1508, 22027, 689, 321, 458, 437, 436, 366, 570, 321, 600, 23535, 552, 51732], "temperature": 0.0, "avg_logprob": -0.07766054678654326, "compression_ratio": 1.7822085889570551, "no_speech_prob": 0.0012063226895406842}, {"id": 670, "seek": 352508, "start": 3525.08, "end": 3530.12, "text": " via our labels. The one final thing I'll mention here is that the one problem with our technique", "tokens": [50364, 5766, 527, 16949, 13, 440, 472, 2572, 551, 286, 603, 2152, 510, 307, 300, 264, 472, 1154, 365, 527, 6532, 50616], "temperature": 0.0, "avg_logprob": -0.08160473815107767, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0005357167101465166}, {"id": 671, "seek": 352508, "start": 3530.12, "end": 3535.7999999999997, "text": " is that it generates very, very little diversity. So these are synthetic images produced by our", "tokens": [50616, 307, 300, 309, 23815, 588, 11, 588, 707, 8811, 13, 407, 613, 366, 23420, 5267, 7126, 538, 527, 50900], "temperature": 0.0, "avg_logprob": -0.08160473815107767, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0005357167101465166}, {"id": 672, "seek": 352508, "start": 3535.7999999999997, "end": 3541.56, "text": " network for this class. And they look a lot like the images that most highly activate that neuron", "tokens": [50900, 3209, 337, 341, 1508, 13, 400, 436, 574, 257, 688, 411, 264, 5267, 300, 881, 5405, 13615, 300, 34090, 51188], "temperature": 0.0, "avg_logprob": -0.08160473815107767, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0005357167101465166}, {"id": 673, "seek": 352508, "start": 3541.56, "end": 3546.92, "text": " from the real world from the real data set, but they don't represent the diversity of images in", "tokens": [51188, 490, 264, 957, 1002, 490, 264, 957, 1412, 992, 11, 457, 436, 500, 380, 2906, 264, 8811, 295, 5267, 294, 51456], "temperature": 0.0, "avg_logprob": -0.08160473815107767, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0005357167101465166}, {"id": 674, "seek": 352508, "start": 3546.92, "end": 3552.52, "text": " that class. And so we did a lot of work, including adding with Yashua Benji on these things called", "tokens": [51456, 300, 1508, 13, 400, 370, 321, 630, 257, 688, 295, 589, 11, 3009, 5127, 365, 398, 1299, 4398, 3964, 4013, 322, 613, 721, 1219, 51736], "temperature": 0.0, "avg_logprob": -0.08160473815107767, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0005357167101465166}, {"id": 675, "seek": 355252, "start": 3552.52, "end": 3557.16, "text": " plug-and-play generative networks, where we wanted to add a lot more diversity. And so you could", "tokens": [50364, 5452, 12, 474, 12, 2858, 1337, 1166, 9590, 11, 689, 321, 1415, 281, 909, 257, 688, 544, 8811, 13, 400, 370, 291, 727, 50596], "temperature": 0.0, "avg_logprob": -0.10556860298945986, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.0008040062966756523}, {"id": 676, "seek": 355252, "start": 3557.16, "end": 3561.0, "text": " take the same network and you can light up a bunch of different classes that it's never even seen", "tokens": [50596, 747, 264, 912, 3209, 293, 291, 393, 1442, 493, 257, 3840, 295, 819, 5359, 300, 309, 311, 1128, 754, 1612, 50788], "temperature": 0.0, "avg_logprob": -0.10556860298945986, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.0008040062966756523}, {"id": 677, "seek": 355252, "start": 3561.0, "end": 3565.48, "text": " before. That's a bit of an aside like ballrooms and art galleries, but mostly we were interested in", "tokens": [50788, 949, 13, 663, 311, 257, 857, 295, 364, 7359, 411, 2594, 32346, 293, 1523, 40141, 11, 457, 5240, 321, 645, 3102, 294, 51012], "temperature": 0.0, "avg_logprob": -0.10556860298945986, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.0008040062966756523}, {"id": 678, "seek": 355252, "start": 3565.48, "end": 3571.8, "text": " getting more diversity. And the takeaway message is we were able to accomplish that. So here is PPGNs,", "tokens": [51012, 1242, 544, 8811, 13, 400, 264, 30681, 3636, 307, 321, 645, 1075, 281, 9021, 300, 13, 407, 510, 307, 37369, 38, 45, 82, 11, 51328], "temperature": 0.0, "avg_logprob": -0.10556860298945986, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.0008040062966756523}, {"id": 679, "seek": 355252, "start": 3571.8, "end": 3576.2, "text": " which is the one that has more diversity. And you can see a much more diversity in this set of", "tokens": [51328, 597, 307, 264, 472, 300, 575, 544, 8811, 13, 400, 291, 393, 536, 257, 709, 544, 8811, 294, 341, 992, 295, 51548], "temperature": 0.0, "avg_logprob": -0.10556860298945986, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.0008040062966756523}, {"id": 680, "seek": 355252, "start": 3576.2, "end": 3582.44, "text": " images versus DGNAMV1, which are the images over here. And this diversity better represents", "tokens": [51548, 5267, 5717, 413, 38, 45, 2865, 53, 16, 11, 597, 366, 264, 5267, 670, 510, 13, 400, 341, 8811, 1101, 8855, 51860], "temperature": 0.0, "avg_logprob": -0.10556860298945986, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.0008040062966756523}, {"id": 681, "seek": 358244, "start": 3582.52, "end": 3587.7200000000003, "text": " kind of the diversity of the natural class. So with the original attempt, DGN, you got volcanoes", "tokens": [50368, 733, 295, 264, 8811, 295, 264, 3303, 1508, 13, 407, 365, 264, 3380, 5217, 11, 413, 38, 45, 11, 291, 658, 48221, 50628], "temperature": 0.0, "avg_logprob": -0.06747614435788964, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.00040440610609948635}, {"id": 682, "seek": 358244, "start": 3587.7200000000003, "end": 3592.36, "text": " that look like this. It kind of goes and finds one type of volcano, like a local optima, and it", "tokens": [50628, 300, 574, 411, 341, 13, 467, 733, 295, 1709, 293, 10704, 472, 2010, 295, 21979, 11, 411, 257, 2654, 2427, 4775, 11, 293, 309, 50860], "temperature": 0.0, "avg_logprob": -0.06747614435788964, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.00040440610609948635}, {"id": 683, "seek": 358244, "start": 3592.36, "end": 3597.4, "text": " sits on it. But the plug-and-play generative networks are much more kind of like an open-ended", "tokens": [50860, 12696, 322, 309, 13, 583, 264, 5452, 12, 474, 12, 2858, 1337, 1166, 9590, 366, 709, 544, 733, 295, 411, 364, 1269, 12, 3502, 51112], "temperature": 0.0, "avg_logprob": -0.06747614435788964, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.00040440610609948635}, {"id": 684, "seek": 358244, "start": 3597.4, "end": 3602.2000000000003, "text": " algorithm, at least within this class, where it samples new versions of volcanoes over and over", "tokens": [51112, 9284, 11, 412, 1935, 1951, 341, 1508, 11, 689, 309, 10938, 777, 9606, 295, 48221, 670, 293, 670, 51352], "temperature": 0.0, "avg_logprob": -0.06747614435788964, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.00040440610609948635}, {"id": 685, "seek": 358244, "start": 3602.2000000000003, "end": 3608.6, "text": " again. And so you get all this big diversity of volcanoes out of this new sampling technique.", "tokens": [51352, 797, 13, 400, 370, 291, 483, 439, 341, 955, 8811, 295, 48221, 484, 295, 341, 777, 21179, 6532, 13, 51672], "temperature": 0.0, "avg_logprob": -0.06747614435788964, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.00040440610609948635}, {"id": 686, "seek": 360860, "start": 3609.3199999999997, "end": 3614.6, "text": " So to conclude this, the AI neuroscience part, I won't actually get into these details, but it", "tokens": [50400, 407, 281, 16886, 341, 11, 264, 7318, 42762, 644, 11, 286, 1582, 380, 767, 483, 666, 613, 4365, 11, 457, 309, 50664], "temperature": 0.0, "avg_logprob": -0.10884147966411752, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.0010003132047131658}, {"id": 687, "seek": 360860, "start": 3614.6, "end": 3619.16, "text": " taught us a lot about what neural nets, you know, what's going on inside neural nets, it taught us", "tokens": [50664, 5928, 505, 257, 688, 466, 437, 18161, 36170, 11, 291, 458, 11, 437, 311, 516, 322, 1854, 18161, 36170, 11, 309, 5928, 505, 50892], "temperature": 0.0, "avg_logprob": -0.10884147966411752, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.0010003132047131658}, {"id": 688, "seek": 360860, "start": 3619.16, "end": 3624.12, "text": " whether or not they really recognize and learn about the concepts in our world. Like we did find", "tokens": [50892, 1968, 420, 406, 436, 534, 5521, 293, 1466, 466, 264, 10392, 294, 527, 1002, 13, 1743, 321, 630, 915, 51140], "temperature": 0.0, "avg_logprob": -0.10884147966411752, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.0010003132047131658}, {"id": 689, "seek": 360860, "start": 3624.12, "end": 3628.12, "text": " in the end that they do know what a volcano is, and you know the five-legged nature of a starshow,", "tokens": [51140, 294, 264, 917, 300, 436, 360, 458, 437, 257, 21979, 307, 11, 293, 291, 458, 264, 1732, 12, 306, 12244, 3687, 295, 257, 6105, 4286, 11, 51340], "temperature": 0.0, "avg_logprob": -0.10884147966411752, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.0010003132047131658}, {"id": 690, "seek": 360860, "start": 3628.12, "end": 3632.7599999999998, "text": " and what a lawnmower is, even though they also are susceptible to producing and recognizing", "tokens": [51340, 293, 437, 257, 19915, 76, 968, 307, 11, 754, 1673, 436, 611, 366, 31249, 281, 10501, 293, 18538, 51572], "temperature": 0.0, "avg_logprob": -0.10884147966411752, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.0010003132047131658}, {"id": 691, "seek": 360860, "start": 3632.7599999999998, "end": 3637.7999999999997, "text": " these adversarial fooling images as being part of the class. And it was cool to see the rapid", "tokens": [51572, 613, 17641, 44745, 7979, 278, 5267, 382, 885, 644, 295, 264, 1508, 13, 400, 309, 390, 1627, 281, 536, 264, 7558, 51824], "temperature": 0.0, "avg_logprob": -0.10884147966411752, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.0010003132047131658}, {"id": 692, "seek": 363780, "start": 3637.8, "end": 3644.52, "text": " progress just within my own team of collaborators from 2015 to 2017. And since then, I highly", "tokens": [50364, 4205, 445, 1951, 452, 1065, 1469, 295, 39789, 490, 7546, 281, 6591, 13, 400, 1670, 550, 11, 286, 5405, 50700], "temperature": 0.0, "avg_logprob": -0.11310936471690303, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.005552617367357016}, {"id": 693, "seek": 363780, "start": 3644.52, "end": 3650.2000000000003, "text": " recommend the work of Chris Ola, who's continued to push in this direction. And very, very soon,", "tokens": [50700, 2748, 264, 589, 295, 6688, 422, 875, 11, 567, 311, 7014, 281, 2944, 294, 341, 3513, 13, 400, 588, 11, 588, 2321, 11, 50984], "temperature": 0.0, "avg_logprob": -0.11310936471690303, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.005552617367357016}, {"id": 694, "seek": 363780, "start": 3650.2000000000003, "end": 3655.2400000000002, "text": " Gabriel Go and Chris and others have new work coming out of OpenAI that will blow your mind.", "tokens": [50984, 20985, 1037, 293, 6688, 293, 2357, 362, 777, 589, 1348, 484, 295, 7238, 48698, 300, 486, 6327, 428, 1575, 13, 51236], "temperature": 0.0, "avg_logprob": -0.11310936471690303, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.005552617367357016}, {"id": 695, "seek": 363780, "start": 3655.2400000000002, "end": 3660.2000000000003, "text": " So I encourage you to watch the OpenAI blog in the coming weeks for this new result that you", "tokens": [51236, 407, 286, 5373, 291, 281, 1159, 264, 7238, 48698, 6968, 294, 264, 1348, 3259, 337, 341, 777, 1874, 300, 291, 51484], "temperature": 0.0, "avg_logprob": -0.11310936471690303, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.005552617367357016}, {"id": 696, "seek": 363780, "start": 3660.2000000000003, "end": 3665.5600000000004, "text": " really like. You could do all of this stuff in different modes, like speech and video, etc.", "tokens": [51484, 534, 411, 13, 509, 727, 360, 439, 295, 341, 1507, 294, 819, 14068, 11, 411, 6218, 293, 960, 11, 5183, 13, 51752], "temperature": 0.0, "avg_logprob": -0.11310936471690303, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.005552617367357016}, {"id": 697, "seek": 366556, "start": 3665.56, "end": 3668.7599999999998, "text": " I won't dive into this. I want to just highlight one thing. This is my future work slide all the", "tokens": [50364, 286, 1582, 380, 9192, 666, 341, 13, 286, 528, 281, 445, 5078, 472, 551, 13, 639, 307, 452, 2027, 589, 4137, 439, 264, 50524], "temperature": 0.0, "avg_logprob": -0.16094534526499668, "compression_ratio": 1.686335403726708, "no_speech_prob": 0.0244128555059433}, {"id": 698, "seek": 366556, "start": 3668.7599999999998, "end": 3673.64, "text": " way back in 2016, and I thought it would be awesome to do this with real animal brains.", "tokens": [50524, 636, 646, 294, 6549, 11, 293, 286, 1194, 309, 576, 312, 3476, 281, 360, 341, 365, 957, 5496, 15442, 13, 50768], "temperature": 0.0, "avg_logprob": -0.16094534526499668, "compression_ratio": 1.686335403726708, "no_speech_prob": 0.0244128555059433}, {"id": 699, "seek": 366556, "start": 3676.68, "end": 3680.2799999999997, "text": " Since then, actually, somebody has done that. They took our algorithm for DGN,", "tokens": [50920, 4162, 550, 11, 767, 11, 2618, 575, 1096, 300, 13, 814, 1890, 527, 9284, 337, 413, 38, 45, 11, 51100], "temperature": 0.0, "avg_logprob": -0.16094534526499668, "compression_ratio": 1.686335403726708, "no_speech_prob": 0.0244128555059433}, {"id": 700, "seek": 366556, "start": 3680.2799999999997, "end": 3684.12, "text": " they applied it to a real monkey brain, and they synthetically are generating images that activate", "tokens": [51100, 436, 6456, 309, 281, 257, 957, 17847, 3567, 11, 293, 436, 10657, 22652, 366, 17746, 5267, 300, 13615, 51292], "temperature": 0.0, "avg_logprob": -0.16094534526499668, "compression_ratio": 1.686335403726708, "no_speech_prob": 0.0244128555059433}, {"id": 701, "seek": 366556, "start": 3684.12, "end": 3688.6, "text": " neurons within the monkey brain, specifically within the face recognition part of the monkey", "tokens": [51292, 22027, 1951, 264, 17847, 3567, 11, 4682, 1951, 264, 1851, 11150, 644, 295, 264, 17847, 51516], "temperature": 0.0, "avg_logprob": -0.16094534526499668, "compression_ratio": 1.686335403726708, "no_speech_prob": 0.0244128555059433}, {"id": 702, "seek": 366556, "start": 3688.6, "end": 3693.72, "text": " brain. And you do in fact get a synthetic monkey-looking face, which is pretty amazing.", "tokens": [51516, 3567, 13, 400, 291, 360, 294, 1186, 483, 257, 23420, 17847, 12, 16129, 1851, 11, 597, 307, 1238, 2243, 13, 51772], "temperature": 0.0, "avg_logprob": -0.16094534526499668, "compression_ratio": 1.686335403726708, "no_speech_prob": 0.0244128555059433}, {"id": 703, "seek": 369372, "start": 3693.72, "end": 3698.7599999999998, "text": " So to conclude my overall talk, I think innovation engines are really interesting because they kind", "tokens": [50364, 407, 281, 16886, 452, 4787, 751, 11, 286, 519, 8504, 12982, 366, 534, 1880, 570, 436, 733, 50616], "temperature": 0.0, "avg_logprob": -0.12075184463360987, "compression_ratio": 1.652027027027027, "no_speech_prob": 0.0015969809610396624}, {"id": 704, "seek": 369372, "start": 3698.7599999999998, "end": 3704.2, "text": " of push on this question of can we automatically produce an open-ended creative process that in", "tokens": [50616, 295, 2944, 322, 341, 1168, 295, 393, 321, 6772, 5258, 364, 1269, 12, 3502, 5880, 1399, 300, 294, 50888], "temperature": 0.0, "avg_logprob": -0.12075184463360987, "compression_ratio": 1.652027027027027, "no_speech_prob": 0.0015969809610396624}, {"id": 705, "seek": 369372, "start": 3704.2, "end": 3710.12, "text": " any sort of modality like art or music or invention will just endlessly generate interesting new", "tokens": [50888, 604, 1333, 295, 1072, 1860, 411, 1523, 420, 1318, 420, 22265, 486, 445, 44920, 8460, 1880, 777, 51184], "temperature": 0.0, "avg_logprob": -0.12075184463360987, "compression_ratio": 1.652027027027027, "no_speech_prob": 0.0015969809610396624}, {"id": 706, "seek": 369372, "start": 3710.12, "end": 3714.2799999999997, "text": " things. We've got a long way to go to accomplish that goal, but my colleagues and I, like Ken Stanley", "tokens": [51184, 721, 13, 492, 600, 658, 257, 938, 636, 281, 352, 281, 9021, 300, 3387, 11, 457, 452, 7734, 293, 286, 11, 411, 8273, 28329, 51392], "temperature": 0.0, "avg_logprob": -0.12075184463360987, "compression_ratio": 1.652027027027027, "no_speech_prob": 0.0015969809610396624}, {"id": 707, "seek": 369372, "start": 3714.2799999999997, "end": 3718.3599999999997, "text": " and Joe Layman and myself are really, really focused on this goal and trying to pull that off,", "tokens": [51392, 293, 6807, 20084, 1601, 293, 2059, 366, 534, 11, 534, 5178, 322, 341, 3387, 293, 1382, 281, 2235, 300, 766, 11, 51596], "temperature": 0.0, "avg_logprob": -0.12075184463360987, "compression_ratio": 1.652027027027027, "no_speech_prob": 0.0015969809610396624}, {"id": 708, "seek": 371836, "start": 3718.36, "end": 3724.28, "text": " including now at OpenAI, where all those people are. And I also showed you very, very quickly some", "tokens": [50364, 3009, 586, 412, 7238, 48698, 11, 689, 439, 729, 561, 366, 13, 400, 286, 611, 4712, 291, 588, 11, 588, 2661, 512, 50660], "temperature": 0.0, "avg_logprob": -0.10496166348457336, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.017436577007174492}, {"id": 709, "seek": 371836, "start": 3724.28, "end": 3727.7200000000003, "text": " of our work in AI neuroscience, which we were doing for scientific reasons, but produce these", "tokens": [50660, 295, 527, 589, 294, 7318, 42762, 11, 597, 321, 645, 884, 337, 8134, 4112, 11, 457, 5258, 613, 50832], "temperature": 0.0, "avg_logprob": -0.10496166348457336, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.017436577007174492}, {"id": 710, "seek": 371836, "start": 3727.7200000000003, "end": 3731.8, "text": " interesting aesthetic artifacts. And I'll just leave you with one final thought, which is that I find", "tokens": [50832, 1880, 20092, 24617, 13, 400, 286, 603, 445, 1856, 291, 365, 472, 2572, 1194, 11, 597, 307, 300, 286, 915, 51036], "temperature": 0.0, "avg_logprob": -0.10496166348457336, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.017436577007174492}, {"id": 711, "seek": 371836, "start": 3731.8, "end": 3736.44, "text": " it surprising how often science produces aesthetic artifacts. Almost none of the work that I was", "tokens": [51036, 309, 8830, 577, 2049, 3497, 14725, 20092, 24617, 13, 12627, 6022, 295, 264, 589, 300, 286, 390, 51268], "temperature": 0.0, "avg_logprob": -0.10496166348457336, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.017436577007174492}, {"id": 712, "seek": 371836, "start": 3736.44, "end": 3741.7200000000003, "text": " doing was trying to do it just for aesthetic purposes, but along the way, it produced these", "tokens": [51268, 884, 390, 1382, 281, 360, 309, 445, 337, 20092, 9932, 11, 457, 2051, 264, 636, 11, 309, 7126, 613, 51532], "temperature": 0.0, "avg_logprob": -0.10496166348457336, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.017436577007174492}, {"id": 713, "seek": 371836, "start": 3741.7200000000003, "end": 3746.44, "text": " things that I think are beautiful and interesting, and could be kind of aesthetic artifacts in their", "tokens": [51532, 721, 300, 286, 519, 366, 2238, 293, 1880, 11, 293, 727, 312, 733, 295, 20092, 24617, 294, 641, 51768], "temperature": 0.0, "avg_logprob": -0.10496166348457336, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.017436577007174492}, {"id": 714, "seek": 374644, "start": 3746.44, "end": 3749.96, "text": " own right. And so I think it's nice because you don't have to choose between being an artist and", "tokens": [50364, 1065, 558, 13, 400, 370, 286, 519, 309, 311, 1481, 570, 291, 500, 380, 362, 281, 2826, 1296, 885, 364, 5748, 293, 50540], "temperature": 0.0, "avg_logprob": -0.08625617925671564, "compression_ratio": 1.721556886227545, "no_speech_prob": 0.0029707595240324736}, {"id": 715, "seek": 374644, "start": 3749.96, "end": 3754.84, "text": " a scientist. You kind of kind of can do both nowadays, especially with the modern tools and", "tokens": [50540, 257, 12662, 13, 509, 733, 295, 733, 295, 393, 360, 1293, 13434, 11, 2318, 365, 264, 4363, 3873, 293, 50784], "temperature": 0.0, "avg_logprob": -0.08625617925671564, "compression_ratio": 1.721556886227545, "no_speech_prob": 0.0029707595240324736}, {"id": 716, "seek": 374644, "start": 3754.84, "end": 3759.16, "text": " machine learning. And I'm sure that's kind of a realization that is being reinforced over and", "tokens": [50784, 3479, 2539, 13, 400, 286, 478, 988, 300, 311, 733, 295, 257, 25138, 300, 307, 885, 31365, 670, 293, 51000], "temperature": 0.0, "avg_logprob": -0.08625617925671564, "compression_ratio": 1.721556886227545, "no_speech_prob": 0.0029707595240324736}, {"id": 717, "seek": 374644, "start": 3759.16, "end": 3763.48, "text": " over again with all the different lectures in this wonderful class that you are participating in.", "tokens": [51000, 670, 797, 365, 439, 264, 819, 16564, 294, 341, 3715, 1508, 300, 291, 366, 13950, 294, 13, 51216], "temperature": 0.0, "avg_logprob": -0.08625617925671564, "compression_ratio": 1.721556886227545, "no_speech_prob": 0.0029707595240324736}, {"id": 718, "seek": 374644, "start": 3763.48, "end": 3767.8, "text": " So with that, I want to say thank you, and I'll turn it over to either questions or Joel, depending", "tokens": [51216, 407, 365, 300, 11, 286, 528, 281, 584, 1309, 291, 11, 293, 286, 603, 1261, 309, 670, 281, 2139, 1651, 420, 21522, 11, 5413, 51432], "temperature": 0.0, "avg_logprob": -0.08625617925671564, "compression_ratio": 1.721556886227545, "no_speech_prob": 0.0029707595240324736}, {"id": 719, "seek": 374644, "start": 3767.8, "end": 3775.08, "text": " on what you want to do, Ali. Thank you so much. I appreciate it. It was really interesting and", "tokens": [51432, 322, 437, 291, 528, 281, 360, 11, 12020, 13, 1044, 291, 370, 709, 13, 286, 4449, 309, 13, 467, 390, 534, 1880, 293, 51796], "temperature": 0.0, "avg_logprob": -0.08625617925671564, "compression_ratio": 1.721556886227545, "no_speech_prob": 0.0029707595240324736}, {"id": 720, "seek": 377508, "start": 3775.08, "end": 3782.84, "text": " inspiring to me, and I'm sure for many of us in this class, this comment that you also made about", "tokens": [50364, 15883, 281, 385, 11, 293, 286, 478, 988, 337, 867, 295, 505, 294, 341, 1508, 11, 341, 2871, 300, 291, 611, 1027, 466, 50752], "temperature": 0.0, "avg_logprob": -0.11867002730673933, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.004598247818648815}, {"id": 721, "seek": 377508, "start": 3782.84, "end": 3791.72, "text": " science and creating and art, I think that it also is very well aligned with some of the other", "tokens": [50752, 3497, 293, 4084, 293, 1523, 11, 286, 519, 300, 309, 611, 307, 588, 731, 17962, 365, 512, 295, 264, 661, 51196], "temperature": 0.0, "avg_logprob": -0.11867002730673933, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.004598247818648815}, {"id": 722, "seek": 377508, "start": 3792.2799999999997, "end": 3797.56, "text": " insight that we learned from other speakers. For instance, Alyosha, of course, was mentioning that", "tokens": [51224, 11269, 300, 321, 3264, 490, 661, 9518, 13, 1171, 5197, 11, 27008, 329, 1641, 11, 295, 1164, 11, 390, 18315, 300, 51488], "temperature": 0.0, "avg_logprob": -0.11867002730673933, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.004598247818648815}, {"id": 723, "seek": 377508, "start": 3797.56, "end": 3803.56, "text": " when I asked this question, he was mentioning that he also thinks that, you know, creativity is", "tokens": [51488, 562, 286, 2351, 341, 1168, 11, 415, 390, 18315, 300, 415, 611, 7309, 300, 11, 291, 458, 11, 12915, 307, 51788], "temperature": 0.0, "avg_logprob": -0.11867002730673933, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.004598247818648815}, {"id": 724, "seek": 380356, "start": 3803.56, "end": 3810.36, "text": " a different tier of our evolution. So that really resonated with me what you were talking today.", "tokens": [50364, 257, 819, 12362, 295, 527, 9303, 13, 407, 300, 534, 47957, 365, 385, 437, 291, 645, 1417, 965, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1778428975273581, "compression_ratio": 1.5, "no_speech_prob": 0.0021111848764121532}, {"id": 725, "seek": 380356, "start": 3811.88, "end": 3822.52, "text": " And I think that this is very exciting for us. One question that I have is if students want to,", "tokens": [50780, 400, 286, 519, 300, 341, 307, 588, 4670, 337, 505, 13, 1485, 1168, 300, 286, 362, 307, 498, 1731, 528, 281, 11, 51312], "temperature": 0.0, "avg_logprob": -0.1778428975273581, "compression_ratio": 1.5, "no_speech_prob": 0.0021111848764121532}, {"id": 726, "seek": 380356, "start": 3823.56, "end": 3830.2799999999997, "text": " because this is a very interesting topic, and especially that type of poet or open-endedness", "tokens": [51364, 570, 341, 307, 257, 588, 1880, 4829, 11, 293, 2318, 300, 2010, 295, 20874, 420, 1269, 12, 3502, 1287, 51700], "temperature": 0.0, "avg_logprob": -0.1778428975273581, "compression_ratio": 1.5, "no_speech_prob": 0.0021111848764121532}, {"id": 727, "seek": 383028, "start": 3830.84, "end": 3839.7200000000003, "text": " area, if a student wants to join you in this sort of mission, what do you recommend to them to work on?", "tokens": [50392, 1859, 11, 498, 257, 3107, 2738, 281, 3917, 291, 294, 341, 1333, 295, 4447, 11, 437, 360, 291, 2748, 281, 552, 281, 589, 322, 30, 50836], "temperature": 0.0, "avg_logprob": -0.13088118588482892, "compression_ratio": 1.5912698412698412, "no_speech_prob": 0.0014543005963787436}, {"id": 728, "seek": 383028, "start": 3841.32, "end": 3846.84, "text": " Yeah. So one thing I would recommend is we had an ICML tutorial, I think about a year ago, that", "tokens": [50916, 865, 13, 407, 472, 551, 286, 576, 2748, 307, 321, 632, 364, 14360, 12683, 7073, 11, 286, 519, 466, 257, 1064, 2057, 11, 300, 51192], "temperature": 0.0, "avg_logprob": -0.13088118588482892, "compression_ratio": 1.5912698412698412, "no_speech_prob": 0.0014543005963787436}, {"id": 729, "seek": 383028, "start": 3846.84, "end": 3853.1600000000003, "text": " really covered a lot of this work in more depth. It's an ICML tutorial on population-based methods.", "tokens": [51192, 534, 5343, 257, 688, 295, 341, 589, 294, 544, 7161, 13, 467, 311, 364, 14360, 12683, 7073, 322, 4415, 12, 6032, 7150, 13, 51508], "temperature": 0.0, "avg_logprob": -0.13088118588482892, "compression_ratio": 1.5912698412698412, "no_speech_prob": 0.0014543005963787436}, {"id": 730, "seek": 383028, "start": 3853.1600000000003, "end": 3859.5600000000004, "text": " So then you can see, can Joel and myself kind of going through, this is Joel Lehmann, not Joel Simon,", "tokens": [51508, 407, 550, 291, 393, 536, 11, 393, 21522, 293, 2059, 733, 295, 516, 807, 11, 341, 307, 21522, 1456, 8587, 969, 11, 406, 21522, 13193, 11, 51828], "temperature": 0.0, "avg_logprob": -0.13088118588482892, "compression_ratio": 1.5912698412698412, "no_speech_prob": 0.0014543005963787436}, {"id": 731, "seek": 385956, "start": 3859.72, "end": 3865.48, "text": " going through a lot of the work that we've done in this field. And I recommend reading a lot of the", "tokens": [50372, 516, 807, 257, 688, 295, 264, 589, 300, 321, 600, 1096, 294, 341, 2519, 13, 400, 286, 2748, 3760, 257, 688, 295, 264, 50660], "temperature": 0.0, "avg_logprob": -0.09138029675151027, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.0007549220463261008}, {"id": 732, "seek": 385956, "start": 3865.48, "end": 3870.92, "text": " work of both Ken and Joel, as well as you can look into some of the work that we've done in this area.", "tokens": [50660, 589, 295, 1293, 8273, 293, 21522, 11, 382, 731, 382, 291, 393, 574, 666, 512, 295, 264, 589, 300, 321, 600, 1096, 294, 341, 1859, 13, 50932], "temperature": 0.0, "avg_logprob": -0.09138029675151027, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.0007549220463261008}, {"id": 733, "seek": 385956, "start": 3870.92, "end": 3875.48, "text": " And then in terms of what I recommend you work on, there's so many things, it's like,", "tokens": [50932, 400, 550, 294, 2115, 295, 437, 286, 2748, 291, 589, 322, 11, 456, 311, 370, 867, 721, 11, 309, 311, 411, 11, 51160], "temperature": 0.0, "avg_logprob": -0.09138029675151027, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.0007549220463261008}, {"id": 734, "seek": 385956, "start": 3875.48, "end": 3880.7599999999998, "text": " there's so many options that it's fun. You could apply a lot of these algorithms in a new domain,", "tokens": [51160, 456, 311, 370, 867, 3956, 300, 309, 311, 1019, 13, 509, 727, 3079, 257, 688, 295, 613, 14642, 294, 257, 777, 9274, 11, 51424], "temperature": 0.0, "avg_logprob": -0.09138029675151027, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.0007549220463261008}, {"id": 735, "seek": 385956, "start": 3880.7599999999998, "end": 3887.08, "text": " for example, that you find interesting, you know, a new kind of art. You could take more modern tools", "tokens": [51424, 337, 1365, 11, 300, 291, 915, 1880, 11, 291, 458, 11, 257, 777, 733, 295, 1523, 13, 509, 727, 747, 544, 4363, 3873, 51740], "temperature": 0.0, "avg_logprob": -0.09138029675151027, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.0007549220463261008}, {"id": 736, "seek": 388708, "start": 3887.08, "end": 3892.12, "text": " that work really well and weave them into these ideas, or you could invent new ideas, you know,", "tokens": [50364, 300, 589, 534, 731, 293, 29145, 552, 666, 613, 3487, 11, 420, 291, 727, 7962, 777, 3487, 11, 291, 458, 11, 50616], "temperature": 0.0, "avg_logprob": -0.09023840493018474, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.0015482757007703185}, {"id": 737, "seek": 388708, "start": 3892.12, "end": 3897.56, "text": " like I still think if people, if anyone out here can crack the question of how can you automatically", "tokens": [50616, 411, 286, 920, 519, 498, 561, 11, 498, 2878, 484, 510, 393, 6226, 264, 1168, 295, 577, 393, 291, 6772, 50888], "temperature": 0.0, "avg_logprob": -0.09023840493018474, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.0015482757007703185}, {"id": 738, "seek": 388708, "start": 3897.56, "end": 3904.52, "text": " recognize newly interesting things, that I think is like a Turing award-winning innovation that will", "tokens": [50888, 5521, 15109, 1880, 721, 11, 300, 286, 519, 307, 411, 257, 314, 1345, 7130, 12, 32960, 8504, 300, 486, 51236], "temperature": 0.0, "avg_logprob": -0.09023840493018474, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.0015482757007703185}, {"id": 739, "seek": 388708, "start": 3904.52, "end": 3911.16, "text": " catalyze and propel so much algorithmic advance, including potentially advancing our push to", "tokens": [51236, 3857, 5222, 1381, 293, 2365, 338, 370, 709, 9284, 299, 7295, 11, 3009, 7263, 27267, 527, 2944, 281, 51568], "temperature": 0.0, "avg_logprob": -0.09023840493018474, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.0015482757007703185}, {"id": 740, "seek": 388708, "start": 3911.16, "end": 3916.2, "text": " artificial general intelligence. Like that might be one of the key stepping stones that gets us there.", "tokens": [51568, 11677, 2674, 7599, 13, 1743, 300, 1062, 312, 472, 295, 264, 2141, 16821, 14083, 300, 2170, 505, 456, 13, 51820], "temperature": 0.0, "avg_logprob": -0.09023840493018474, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.0015482757007703185}, {"id": 741, "seek": 391620, "start": 3916.2, "end": 3919.8799999999997, "text": " So I have this paper called AI Generating Algorithms, which I recommend people check out if", "tokens": [50364, 407, 286, 362, 341, 3035, 1219, 7318, 15409, 990, 35014, 6819, 2592, 11, 597, 286, 2748, 561, 1520, 484, 498, 50548], "temperature": 0.0, "avg_logprob": -0.10705723196773206, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0002611223317217082}, {"id": 742, "seek": 391620, "start": 3919.8799999999997, "end": 3925.08, "text": " they're interested. And it basically talks about how these sorts of ideas may do the fastest path", "tokens": [50548, 436, 434, 3102, 13, 400, 309, 1936, 6686, 466, 577, 613, 7527, 295, 3487, 815, 360, 264, 14573, 3100, 50808], "temperature": 0.0, "avg_logprob": -0.10705723196773206, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0002611223317217082}, {"id": 743, "seek": 391620, "start": 3925.08, "end": 3932.12, "text": " to produce general AI. So that's not an aesthetic quest, it's more of a scientific quest. And but", "tokens": [50808, 281, 5258, 2674, 7318, 13, 407, 300, 311, 406, 364, 20092, 866, 11, 309, 311, 544, 295, 257, 8134, 866, 13, 400, 457, 51160], "temperature": 0.0, "avg_logprob": -0.10705723196773206, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0002611223317217082}, {"id": 744, "seek": 391620, "start": 3932.12, "end": 3935.7999999999997, "text": " if you're interested in that, I think that's fascinating. But I also just think, just literally", "tokens": [51160, 498, 291, 434, 3102, 294, 300, 11, 286, 519, 300, 311, 10343, 13, 583, 286, 611, 445, 519, 11, 445, 3736, 51344], "temperature": 0.0, "avg_logprob": -0.10705723196773206, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0002611223317217082}, {"id": 745, "seek": 391620, "start": 3935.7999999999997, "end": 3940.8399999999997, "text": " take all these ideas and go like do Poet, but do it in some totally wild and crazy different domain,", "tokens": [51344, 747, 439, 613, 3487, 293, 352, 411, 360, 6165, 302, 11, 457, 360, 309, 294, 512, 3879, 4868, 293, 3219, 819, 9274, 11, 51596], "temperature": 0.0, "avg_logprob": -0.10705723196773206, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0002611223317217082}, {"id": 746, "seek": 394084, "start": 3940.84, "end": 3946.44, "text": " or do an innovation engine in, you know, like architecture or poetry and see what happens,", "tokens": [50364, 420, 360, 364, 8504, 2848, 294, 11, 291, 458, 11, 411, 9482, 420, 15155, 293, 536, 437, 2314, 11, 50644], "temperature": 0.0, "avg_logprob": -0.16694102684656778, "compression_ratio": 1.5165289256198347, "no_speech_prob": 0.0013455055886879563}, {"id": 747, "seek": 394084, "start": 3946.44, "end": 3952.1200000000003, "text": " you know, you can use new tools like GPT-3 or Dolly, etc. So I think there's just a lot of", "tokens": [50644, 291, 458, 11, 291, 393, 764, 777, 3873, 411, 26039, 51, 12, 18, 420, 1144, 13020, 11, 5183, 13, 407, 286, 519, 456, 311, 445, 257, 688, 295, 50928], "temperature": 0.0, "avg_logprob": -0.16694102684656778, "compression_ratio": 1.5165289256198347, "no_speech_prob": 0.0013455055886879563}, {"id": 748, "seek": 394084, "start": 3952.1200000000003, "end": 3958.1200000000003, "text": " low-hanging fruit here to be explored. Certainly. And that also reminds me of what you mentioned,", "tokens": [50928, 2295, 12, 71, 9741, 6773, 510, 281, 312, 24016, 13, 16628, 13, 400, 300, 611, 12025, 385, 295, 437, 291, 2835, 11, 51228], "temperature": 0.0, "avg_logprob": -0.16694102684656778, "compression_ratio": 1.5165289256198347, "no_speech_prob": 0.0013455055886879563}, {"id": 749, "seek": 394084, "start": 3958.1200000000003, "end": 3965.32, "text": " we didn't optimize to create a microwave. We explored different things and I think that", "tokens": [51228, 321, 994, 380, 19719, 281, 1884, 257, 19025, 13, 492, 24016, 819, 721, 293, 286, 519, 300, 51588], "temperature": 0.0, "avg_logprob": -0.16694102684656778, "compression_ratio": 1.5165289256198347, "no_speech_prob": 0.0013455055886879563}, {"id": 750, "seek": 396532, "start": 3965.32, "end": 3972.44, "text": " your advice is quite in that direction. Also, Joseph has a question. Joseph, would you like to", "tokens": [50364, 428, 5192, 307, 1596, 294, 300, 3513, 13, 2743, 11, 11170, 575, 257, 1168, 13, 11170, 11, 576, 291, 411, 281, 50720], "temperature": 0.0, "avg_logprob": -0.15047130584716797, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.002888054819777608}, {"id": 751, "seek": 396532, "start": 3973.88, "end": 3981.0800000000004, "text": " ask it yourself or? I wasn't able to get my mic working earlier. Let me just", "tokens": [50792, 1029, 309, 1803, 420, 30, 286, 2067, 380, 1075, 281, 483, 452, 3123, 1364, 3071, 13, 961, 385, 445, 51152], "temperature": 0.0, "avg_logprob": -0.15047130584716797, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.002888054819777608}, {"id": 752, "seek": 396532, "start": 3982.28, "end": 3990.36, "text": " fix that. Hi, Jeff. I was just wondering if you've explored anything on Poet in multi-agent settings", "tokens": [51212, 3191, 300, 13, 2421, 11, 7506, 13, 286, 390, 445, 6359, 498, 291, 600, 24016, 1340, 322, 6165, 302, 294, 4825, 12, 559, 317, 6257, 51616], "temperature": 0.0, "avg_logprob": -0.15047130584716797, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.002888054819777608}, {"id": 753, "seek": 396532, "start": 3990.36, "end": 3995.2400000000002, "text": " to this point. Yeah, the short answer there is that we have a lot of really exciting ideas.", "tokens": [51616, 281, 341, 935, 13, 865, 11, 264, 2099, 1867, 456, 307, 300, 321, 362, 257, 688, 295, 534, 4670, 3487, 13, 51860], "temperature": 0.0, "avg_logprob": -0.15047130584716797, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.002888054819777608}, {"id": 754, "seek": 399532, "start": 3996.1200000000003, "end": 3999.7200000000003, "text": " For how we want to take advantage of that. I can't share those specific ideas", "tokens": [50404, 1171, 577, 321, 528, 281, 747, 5002, 295, 300, 13, 286, 393, 380, 2073, 729, 2685, 3487, 50584], "temperature": 0.0, "avg_logprob": -0.08109048304666999, "compression_ratio": 1.7922077922077921, "no_speech_prob": 0.0010001094778999686}, {"id": 755, "seek": 399532, "start": 4000.76, "end": 4005.8, "text": " because we may or may not be working on them. But I also think in the spirit of the talk that", "tokens": [50636, 570, 321, 815, 420, 815, 406, 312, 1364, 322, 552, 13, 583, 286, 611, 519, 294, 264, 3797, 295, 264, 751, 300, 50888], "temperature": 0.0, "avg_logprob": -0.08109048304666999, "compression_ratio": 1.7922077922077921, "no_speech_prob": 0.0010001094778999686}, {"id": 756, "seek": 399532, "start": 4005.8, "end": 4011.32, "text": " the best way to make advances is to have a community of people with different ideas pushing", "tokens": [50888, 264, 1151, 636, 281, 652, 25297, 307, 281, 362, 257, 1768, 295, 561, 365, 819, 3487, 7380, 51164], "temperature": 0.0, "avg_logprob": -0.08109048304666999, "compression_ratio": 1.7922077922077921, "no_speech_prob": 0.0010001094778999686}, {"id": 757, "seek": 399532, "start": 4011.32, "end": 4015.56, "text": " different directions because you never know what's going to unlock. So I almost don't want to give you", "tokens": [51164, 819, 11095, 570, 291, 1128, 458, 437, 311, 516, 281, 11634, 13, 407, 286, 1920, 500, 380, 528, 281, 976, 291, 51376], "temperature": 0.0, "avg_logprob": -0.08109048304666999, "compression_ratio": 1.7922077922077921, "no_speech_prob": 0.0010001094778999686}, {"id": 758, "seek": 399532, "start": 4015.56, "end": 4018.76, "text": " too many ideas either because I don't want to cause conversion thinking. I think it's almost", "tokens": [51376, 886, 867, 3487, 2139, 570, 286, 500, 380, 528, 281, 3082, 14298, 1953, 13, 286, 519, 309, 311, 1920, 51536], "temperature": 0.0, "avg_logprob": -0.08109048304666999, "compression_ratio": 1.7922077922077921, "no_speech_prob": 0.0010001094778999686}, {"id": 759, "seek": 399532, "start": 4018.76, "end": 4022.76, "text": " better if there's so many different ways you could apply the concepts of Poet to multi-agent", "tokens": [51536, 1101, 498, 456, 311, 370, 867, 819, 2098, 291, 727, 3079, 264, 10392, 295, 6165, 302, 281, 4825, 12, 559, 317, 51736], "temperature": 0.0, "avg_logprob": -0.08109048304666999, "compression_ratio": 1.7922077922077921, "no_speech_prob": 0.0010001094778999686}, {"id": 760, "seek": 402276, "start": 4022.76, "end": 4028.28, "text": " settings that I don't think you can go wrong. I think if many different people and groups push", "tokens": [50364, 6257, 300, 286, 500, 380, 519, 291, 393, 352, 2085, 13, 286, 519, 498, 867, 819, 561, 293, 3935, 2944, 50640], "temperature": 0.0, "avg_logprob": -0.11010635777523643, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.001409805379807949}, {"id": 761, "seek": 402276, "start": 4028.28, "end": 4033.88, "text": " on that, really good things will happen. Fair. We also may or may not be working on that, right?", "tokens": [50640, 322, 300, 11, 534, 665, 721, 486, 1051, 13, 12157, 13, 492, 611, 815, 420, 815, 406, 312, 1364, 322, 300, 11, 558, 30, 50920], "temperature": 0.0, "avg_logprob": -0.11010635777523643, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.001409805379807949}, {"id": 762, "seek": 402276, "start": 4036.76, "end": 4041.8, "text": " The one thing I wanted to ask you specifically about that is whether you figured out one,", "tokens": [51064, 440, 472, 551, 286, 1415, 281, 1029, 291, 4682, 466, 300, 307, 1968, 291, 8932, 484, 472, 11, 51316], "temperature": 0.0, "avg_logprob": -0.11010635777523643, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.001409805379807949}, {"id": 763, "seek": 402276, "start": 4041.8, "end": 4047.88, "text": " maybe you can't tell me, but any way to get around the problem where in multi-agent settings,", "tokens": [51316, 1310, 291, 393, 380, 980, 385, 11, 457, 604, 636, 281, 483, 926, 264, 1154, 689, 294, 4825, 12, 559, 317, 6257, 11, 51620], "temperature": 0.0, "avg_logprob": -0.11010635777523643, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.001409805379807949}, {"id": 764, "seek": 404788, "start": 4047.88, "end": 4053.8, "text": " sometimes you don't have a single evaluation metric that correlates the environment difficulty", "tokens": [50364, 2171, 291, 500, 380, 362, 257, 2167, 13344, 20678, 300, 13983, 1024, 264, 2823, 10360, 50660], "temperature": 0.0, "avg_logprob": -0.0998842716217041, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.002714078640565276}, {"id": 765, "seek": 404788, "start": 4053.8, "end": 4058.84, "text": " with agent performance because you add more agents in, well, then the performance goes down", "tokens": [50660, 365, 9461, 3389, 570, 291, 909, 544, 12554, 294, 11, 731, 11, 550, 264, 3389, 1709, 760, 50912], "temperature": 0.0, "avg_logprob": -0.0998842716217041, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.002714078640565276}, {"id": 766, "seek": 404788, "start": 4058.84, "end": 4062.92, "text": " because there are more of them and they're all doing smarter things. So that sort of thing has", "tokens": [50912, 570, 456, 366, 544, 295, 552, 293, 436, 434, 439, 884, 20294, 721, 13, 407, 300, 1333, 295, 551, 575, 51116], "temperature": 0.0, "avg_logprob": -0.0998842716217041, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.002714078640565276}, {"id": 767, "seek": 404788, "start": 4062.92, "end": 4068.92, "text": " thrown a wrench in the whole annex measure. Yeah, that's right. So one of the things you could switch", "tokens": [51116, 11732, 257, 25406, 294, 264, 1379, 41012, 3481, 13, 865, 11, 300, 311, 558, 13, 407, 472, 295, 264, 721, 291, 727, 3679, 51416], "temperature": 0.0, "avg_logprob": -0.0998842716217041, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.002714078640565276}, {"id": 768, "seek": 404788, "start": 4068.92, "end": 4077.2400000000002, "text": " to is a notion of agent versus agent. Like if an agent is as opposed to doing better on that", "tokens": [51416, 281, 307, 257, 10710, 295, 9461, 5717, 9461, 13, 1743, 498, 364, 9461, 307, 382, 8851, 281, 884, 1101, 322, 300, 51832], "temperature": 0.0, "avg_logprob": -0.0998842716217041, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.002714078640565276}, {"id": 769, "seek": 407724, "start": 4077.24, "end": 4083.4799999999996, "text": " environment, it's that agent versus other agents or agents that have come before. Another thing", "tokens": [50364, 2823, 11, 309, 311, 300, 9461, 5717, 661, 12554, 420, 12554, 300, 362, 808, 949, 13, 3996, 551, 50676], "temperature": 0.0, "avg_logprob": -0.16546561831519718, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.0011692371917888522}, {"id": 770, "seek": 407724, "start": 4083.4799999999996, "end": 4087.9599999999996, "text": " you could do is you could switch to more of a learning progress metric, which is if they're", "tokens": [50676, 291, 727, 360, 307, 291, 727, 3679, 281, 544, 295, 257, 2539, 4205, 20678, 11, 597, 307, 498, 436, 434, 50900], "temperature": 0.0, "avg_logprob": -0.16546561831519718, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.0011692371917888522}, {"id": 771, "seek": 407724, "start": 4087.9599999999996, "end": 4096.2, "text": " getting better, are they learning? According to some measure, like does their value function,", "tokens": [50900, 1242, 1101, 11, 366, 436, 2539, 30, 7328, 281, 512, 3481, 11, 411, 775, 641, 2158, 2445, 11, 51312], "temperature": 0.0, "avg_logprob": -0.16546561831519718, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.0011692371917888522}, {"id": 772, "seek": 407724, "start": 4097.48, "end": 4102.12, "text": " their prediction of how well they're going to do, is that wrong? It's because they were either", "tokens": [51376, 641, 17630, 295, 577, 731, 436, 434, 516, 281, 360, 11, 307, 300, 2085, 30, 467, 311, 570, 436, 645, 2139, 51608], "temperature": 0.0, "avg_logprob": -0.16546561831519718, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.0011692371917888522}, {"id": 773, "seek": 407724, "start": 4102.12, "end": 4105.88, "text": " better or worse in that situation and versus those opponents than they thought they were,", "tokens": [51608, 1101, 420, 5324, 294, 300, 2590, 293, 5717, 729, 19001, 813, 436, 1194, 436, 645, 11, 51796], "temperature": 0.0, "avg_logprob": -0.16546561831519718, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.0011692371917888522}, {"id": 774, "seek": 410588, "start": 4105.88, "end": 4110.4400000000005, "text": " and measures like that could really catalyze, recognizing this is still an interesting environment", "tokens": [50364, 293, 8000, 411, 300, 727, 534, 3857, 5222, 1381, 11, 18538, 341, 307, 920, 364, 1880, 2823, 50592], "temperature": 0.0, "avg_logprob": -0.08264831669074445, "compression_ratio": 1.9057239057239057, "no_speech_prob": 0.0003149677941109985}, {"id": 775, "seek": 410588, "start": 4110.4400000000005, "end": 4113.8, "text": " because they're learning. This is still an interesting matchup between this opponent and", "tokens": [50592, 570, 436, 434, 2539, 13, 639, 307, 920, 364, 1880, 2995, 1010, 1296, 341, 10620, 293, 50760], "temperature": 0.0, "avg_logprob": -0.08264831669074445, "compression_ratio": 1.9057239057239057, "no_speech_prob": 0.0003149677941109985}, {"id": 776, "seek": 410588, "start": 4113.8, "end": 4118.04, "text": " this opponent because they're learning. I mean, we've been actually trying to do something very", "tokens": [50760, 341, 10620, 570, 436, 434, 2539, 13, 286, 914, 11, 321, 600, 668, 767, 1382, 281, 360, 746, 588, 50972], "temperature": 0.0, "avg_logprob": -0.08264831669074445, "compression_ratio": 1.9057239057239057, "no_speech_prob": 0.0003149677941109985}, {"id": 777, "seek": 410588, "start": 4118.04, "end": 4124.4400000000005, "text": " similar there. It still seems to run into the same sort of issue though, right? If you can't", "tokens": [50972, 2531, 456, 13, 467, 920, 2544, 281, 1190, 666, 264, 912, 1333, 295, 2734, 1673, 11, 558, 30, 759, 291, 393, 380, 51292], "temperature": 0.0, "avg_logprob": -0.08264831669074445, "compression_ratio": 1.9057239057239057, "no_speech_prob": 0.0003149677941109985}, {"id": 778, "seek": 410588, "start": 4124.4400000000005, "end": 4128.84, "text": " measure absolute performance, it can still be difficult to then measure relative performance", "tokens": [51292, 3481, 8236, 3389, 11, 309, 393, 920, 312, 2252, 281, 550, 3481, 4972, 3389, 51512], "temperature": 0.0, "avg_logprob": -0.08264831669074445, "compression_ratio": 1.9057239057239057, "no_speech_prob": 0.0003149677941109985}, {"id": 779, "seek": 410588, "start": 4128.84, "end": 4134.4400000000005, "text": " because your reward peak can be going down even if you are learning because so are all the other", "tokens": [51512, 570, 428, 7782, 10651, 393, 312, 516, 760, 754, 498, 291, 366, 2539, 570, 370, 366, 439, 264, 661, 51792], "temperature": 0.0, "avg_logprob": -0.08264831669074445, "compression_ratio": 1.9057239057239057, "no_speech_prob": 0.0003149677941109985}, {"id": 780, "seek": 413444, "start": 4134.44, "end": 4139.24, "text": " agents. Sometimes you have to run as fast as you can just to remain in the same place.", "tokens": [50364, 12554, 13, 4803, 291, 362, 281, 1190, 382, 2370, 382, 291, 393, 445, 281, 6222, 294, 264, 912, 1081, 13, 50604], "temperature": 0.0, "avg_logprob": -0.13135107969626403, "compression_ratio": 1.6453900709219857, "no_speech_prob": 0.0009696512133814394}, {"id": 781, "seek": 413444, "start": 4139.799999999999, "end": 4148.919999999999, "text": " Yeah, pretty much. That's the red queen quote from Alison Wonderland. Yeah, these are all challenges", "tokens": [50632, 865, 11, 1238, 709, 13, 663, 311, 264, 2182, 12206, 6513, 490, 41001, 13224, 1661, 13, 865, 11, 613, 366, 439, 4759, 51088], "temperature": 0.0, "avg_logprob": -0.13135107969626403, "compression_ratio": 1.6453900709219857, "no_speech_prob": 0.0009696512133814394}, {"id": 782, "seek": 413444, "start": 4148.919999999999, "end": 4152.2, "text": " and it's the kind of challenge that happens once you get into the multi-agent setting.", "tokens": [51088, 293, 309, 311, 264, 733, 295, 3430, 300, 2314, 1564, 291, 483, 666, 264, 4825, 12, 559, 317, 3287, 13, 51252], "temperature": 0.0, "avg_logprob": -0.13135107969626403, "compression_ratio": 1.6453900709219857, "no_speech_prob": 0.0009696512133814394}, {"id": 783, "seek": 413444, "start": 4152.2, "end": 4156.599999999999, "text": " So I think this is just for a lot of experimentation and hard thinking has to happen. I don't think", "tokens": [51252, 407, 286, 519, 341, 307, 445, 337, 257, 688, 295, 37142, 293, 1152, 1953, 575, 281, 1051, 13, 286, 500, 380, 519, 51472], "temperature": 0.0, "avg_logprob": -0.13135107969626403, "compression_ratio": 1.6453900709219857, "no_speech_prob": 0.0009696512133814394}, {"id": 784, "seek": 413444, "start": 4156.599999999999, "end": 4162.12, "text": " there's a really super, short, easy, obvious answer. It's just going to require research.", "tokens": [51472, 456, 311, 257, 534, 1687, 11, 2099, 11, 1858, 11, 6322, 1867, 13, 467, 311, 445, 516, 281, 3651, 2132, 13, 51748], "temperature": 0.0, "avg_logprob": -0.13135107969626403, "compression_ratio": 1.6453900709219857, "no_speech_prob": 0.0009696512133814394}, {"id": 785, "seek": 416212, "start": 4162.12, "end": 4166.28, "text": " Well, I mean, I look forward to seeing what setting it is that you're trying that out in", "tokens": [50364, 1042, 11, 286, 914, 11, 286, 574, 2128, 281, 2577, 437, 3287, 309, 307, 300, 291, 434, 1382, 300, 484, 294, 50572], "temperature": 0.0, "avg_logprob": -0.1458888234971445, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.0009839489357545972}, {"id": 786, "seek": 416212, "start": 4166.28, "end": 4169.48, "text": " whenever that gets published. Likewise, yeah, with your work.", "tokens": [50572, 5699, 300, 2170, 6572, 13, 30269, 11, 1338, 11, 365, 428, 589, 13, 50732], "temperature": 0.0, "avg_logprob": -0.1458888234971445, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.0009839489357545972}, {"id": 787, "seek": 416212, "start": 4171.96, "end": 4177.48, "text": " Thanks. Excellent. Are there questions? Any more questions?", "tokens": [50856, 2561, 13, 16723, 13, 2014, 456, 1651, 30, 2639, 544, 1651, 30, 51132], "temperature": 0.0, "avg_logprob": -0.1458888234971445, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.0009839489357545972}, {"id": 788, "seek": 416212, "start": 4185.4, "end": 4191.48, "text": " Guys, don't be shy. If you have questions, just go ahead. Of course, if Jeff has time.", "tokens": [51528, 7855, 11, 500, 380, 312, 12685, 13, 759, 291, 362, 1651, 11, 445, 352, 2286, 13, 2720, 1164, 11, 498, 7506, 575, 565, 13, 51832], "temperature": 0.0, "avg_logprob": -0.1458888234971445, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.0009839489357545972}, {"id": 789, "seek": 419212, "start": 4192.36, "end": 4196.36, "text": " I have time. I just also want to be cognizant of Joel and giving him his proper time.", "tokens": [50376, 286, 362, 565, 13, 286, 445, 611, 528, 281, 312, 11786, 590, 394, 295, 21522, 293, 2902, 796, 702, 2296, 565, 13, 50576], "temperature": 0.0, "avg_logprob": -0.20576696880793166, "compression_ratio": 1.3396226415094339, "no_speech_prob": 0.0010299840942025185}, {"id": 790, "seek": 419212, "start": 4197.96, "end": 4207.64, "text": " Excellent. Okay, cool. Thank you. All right, then let's thank you again, Jeff. It was really,", "tokens": [50656, 16723, 13, 1033, 11, 1627, 13, 1044, 291, 13, 1057, 558, 11, 550, 718, 311, 1309, 291, 797, 11, 7506, 13, 467, 390, 534, 11, 51140], "temperature": 0.0, "avg_logprob": -0.20576696880793166, "compression_ratio": 1.3396226415094339, "no_speech_prob": 0.0010299840942025185}, {"id": 791, "seek": 419212, "start": 4207.64, "end": 4209.32, "text": " really interesting and inspiring.", "tokens": [51140, 534, 1880, 293, 15883, 13, 51224], "temperature": 0.0, "avg_logprob": -0.20576696880793166, "compression_ratio": 1.3396226415094339, "no_speech_prob": 0.0010299840942025185}], "language": "en"}