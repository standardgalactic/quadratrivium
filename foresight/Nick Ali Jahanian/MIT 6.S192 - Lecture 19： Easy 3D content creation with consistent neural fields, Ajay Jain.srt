1
00:00:00,000 --> 00:00:09,520
All right. Hello, everyone. Welcome to your course, AI for Art, Aesthetics and Design

2
00:00:09,520 --> 00:00:21,120
and Creativity. Today, we have a very special lecturer, AJ. He has been at MIT just like

3
00:00:21,120 --> 00:00:31,200
you for his undergrad. I got to know him when he was here and he's very active. I've been running

4
00:00:31,200 --> 00:00:39,360
at the ML groups and sometimes chatting with me about, you know, these topics of creativity and

5
00:00:40,080 --> 00:00:48,240
AI and art. I think that this is very exciting. He's going to tell us about his journey and

6
00:00:49,040 --> 00:00:58,880
and his new work. I will let him to, you know, start the discussion. AJ, one of the things that

7
00:00:58,880 --> 00:01:06,480
I always ask is that if you could please introduce yourself and tell us a little more about what

8
00:01:06,480 --> 00:01:14,000
inspires you to work in this area. Sounds good? Yeah. Yeah, absolutely. I'd be happy to.

9
00:01:14,000 --> 00:01:20,400
And thanks so much for having me. So today, I'm going to be talking about some work I've done,

10
00:01:20,400 --> 00:01:24,000
some works that's happening in the community around 3D content creation.

11
00:01:25,360 --> 00:01:32,400
But first about my journey. Yeah, I was at MIT for my undergrad and I was part of what is now

12
00:01:32,400 --> 00:01:36,080
called the AI Club. And then we called it the Machine Intelligence Community.

13
00:01:36,880 --> 00:01:44,400
In my undergrad, I did research in a couple of areas, but mostly actually in compilers.

14
00:01:44,400 --> 00:01:49,680
So a little distant from what I do now, more on the high performance computing side and

15
00:01:49,680 --> 00:01:54,560
performance engineering that had experienced self-driving cars and generative models for

16
00:01:54,560 --> 00:01:59,760
self-driving applications during undergrad and really fell in love with that topic. How do we

17
00:01:59,760 --> 00:02:04,800
reason about uncertainty? How do we model complex data distributions and predict the future?

18
00:02:05,760 --> 00:02:10,000
Like, for example, predicting the behavior of vehicles. And that led me down the path of

19
00:02:10,000 --> 00:02:16,320
working on generative models in my PhD. And these generative models are, these days,

20
00:02:16,320 --> 00:02:20,240
the state of the art generative models are parametrized by deep neural networks, which try to

21
00:02:20,240 --> 00:02:25,040
fit large data sets, try to estimate correlations between different variables. And these could be

22
00:02:25,040 --> 00:02:29,200
old types of different data modalities, like images, they could be trajectories or behaviors,

23
00:02:29,280 --> 00:02:36,720
like I worked on, audio, video. And today, we're going to talk a little bit about 3D objects.

24
00:02:36,720 --> 00:02:40,880
And so that's kind of what inspired me. At the time, I was interested in uncertainty estimation.

25
00:02:41,440 --> 00:02:45,840
But these days, I just really like the tangible results you can get out of generative models,

26
00:02:46,400 --> 00:02:52,000
novel samples, and novel designs. It's very fun. I got to look at pretty pictures all day.

27
00:02:54,640 --> 00:02:58,640
To my research interests, like I mentioned, around generative models, we've done some work

28
00:02:58,640 --> 00:03:04,400
in denoising auto encoders. How do you generate images with these denoising diffusion probabilistic

29
00:03:04,400 --> 00:03:08,480
models? That's purely in the 2D setting, though it's been extended to other domains.

30
00:03:10,800 --> 00:03:15,520
Over the past year and a half, I've also been doing a lot of work in 3D reconstruction and

31
00:03:15,520 --> 00:03:21,280
inverse graphics. So how do we take images and try to infer a scene from them or generate in

32
00:03:21,280 --> 00:03:28,720
the 3D space? Sorry, Ajay, interrupting you. It seems that some of the students want the

33
00:03:28,720 --> 00:03:33,280
transcription to be on. Is that okay? That's fine. Okay, excellent. Thanks.

34
00:03:36,320 --> 00:03:41,840
And building off of that performance engineering background I had from MIT, I also did a lot of

35
00:03:41,840 --> 00:03:46,240
work in the intersection of machine learning and programming languages at the start of my

36
00:03:46,240 --> 00:03:51,600
graduate school. And I would summarize kind of my research interest as making it easier to

37
00:03:51,600 --> 00:03:59,040
create creative content, especially with AI tools. And to provide some background for today,

38
00:03:59,040 --> 00:04:04,160
I'm going to discuss different types of scene representations. What I mean by this is how do

39
00:04:04,160 --> 00:04:11,760
we encode the geometry and colour of a scene in some format that we can work with digitally.

40
00:04:12,320 --> 00:04:17,520
And there's this very long history of this, particularly from the graphics literature.

41
00:04:18,160 --> 00:04:22,240
On this slide shows some different representations of geometry that you'll be familiar with some

42
00:04:22,240 --> 00:04:29,200
of them. 2.5D might include RGBD images, like a photo plus a depth scan. And they're point

43
00:04:29,200 --> 00:04:34,320
clouds, meshes. Meshes are the most common representation used in graphics applications,

44
00:04:34,320 --> 00:04:37,440
but they can actually be challenging to work with in a learning context.

45
00:04:37,440 --> 00:04:45,760
So our focus in the learning context will be on the volumetric approaches. These can be very

46
00:04:45,760 --> 00:04:51,520
easy to train. You can kind of think of at least a boss of greatest classifier, mapping each point

47
00:04:51,520 --> 00:04:58,160
in space to whether it's part of the object or not, whether it's occupied or not. More recently,

48
00:04:58,160 --> 00:05:03,040
there's been a significant amount of interest in neural scene representations, sometimes called

49
00:05:03,040 --> 00:05:08,640
implicit neural representations that define the geometry of the object with a function.

50
00:05:09,280 --> 00:05:13,520
That could be a distance function, so a network mapping from coordinates to the distance to the

51
00:05:13,520 --> 00:05:20,080
nearest surface. And these can be a lot easier to optimise. These neural scene representations

52
00:05:20,080 --> 00:05:25,360
can also compress the data significantly compared to explicitly representing the geometry of the

53
00:05:25,360 --> 00:05:32,480
scene. So we'll be focusing on that direction. And in particular, we're going to be discussing

54
00:05:32,560 --> 00:05:37,280
a model called neural radian fields I'll get to in a second. But they address this problem of

55
00:05:37,280 --> 00:05:45,200
view synthesis. So how do we take some sparsely sampled input views of a scene and then construct

56
00:05:45,200 --> 00:05:50,960
a representation of that scene's 3D geometry and colour in a way that allows us to render it from

57
00:05:50,960 --> 00:05:56,800
new perspectives? These are some example works. You can represent the scene as a multi-plane image.

58
00:05:56,880 --> 00:06:03,760
So instead of a flat grid of RGB values represented as multiple planes, and that allows very quick

59
00:06:04,720 --> 00:06:11,120
rendering from new perspectives. Neural volumes is an approach from Facebook that has an encoder

60
00:06:11,120 --> 00:06:16,560
decoder structure. Take some input images and encode them into a layman space, kind of a 3D

61
00:06:16,560 --> 00:06:23,600
layman space and decode it out to images with volume rendering. Neural radian fields have

62
00:06:23,600 --> 00:06:29,280
really been very popular over the last two years due to their simplicity and quality of the results

63
00:06:29,280 --> 00:06:36,240
they can generate. So here's an example scene that's captured on the Berkeley campus. Some photos of

64
00:06:36,240 --> 00:06:40,000
the scene are captured, for example, with an iPhone. I believe these are captured with an iPhone.

65
00:06:42,080 --> 00:06:48,160
Then poses for each photo are estimated. And this neural scene representation called the neural

66
00:06:48,160 --> 00:06:53,920
radian field is estimated off of those images. What's really nice is once we estimate the

67
00:06:53,920 --> 00:06:58,480
representation of the scene, we can render it from novel viewpoints and kind of smoothly

68
00:06:58,480 --> 00:07:03,840
interpolate these sparsely sampled views. The scene is only very sparsely observed from discrete

69
00:07:03,840 --> 00:07:07,760
points. What if you as the user want to make a photo from a new perspective?

70
00:07:09,440 --> 00:07:15,280
There's some interesting things to note about this rendering. Notice the specularities on the

71
00:07:15,280 --> 00:07:20,880
surface. They're not just modeling the diffuse light. Also modeling how the light reflected

72
00:07:20,880 --> 00:07:26,960
back at the user depends on the viewpoint of the camera. As you shift your head, the scene will

73
00:07:26,960 --> 00:07:34,720
change. This is particularly visible on very shiny surfaces like the glass or metal of the car.

74
00:07:38,000 --> 00:07:44,640
A neural radian field, yeah, it really is amazing and really captured the attention of a lot of

75
00:07:44,640 --> 00:07:51,040
people. This neural radian field has grown very quickly and there's still a lot of problems to be

76
00:07:51,040 --> 00:07:56,640
solved. One very interesting thing that these neural scene representations bring to mind is

77
00:07:56,640 --> 00:08:01,680
that we're encoding a scene in the neural network's weights. Instead of explicitly encoding the

78
00:08:01,680 --> 00:08:08,160
geometry of the scene via points or meshes, lists of triangles or voxel grids, it's encoded

79
00:08:08,880 --> 00:08:15,440
into this small multi-layer perceptron. Maybe this is a half a million parameter network,

80
00:08:15,440 --> 00:08:21,440
just some stacks of dense layers. It's representing a function mapping from 3D space coordinates,

81
00:08:22,160 --> 00:08:28,480
XYZ. This is in the scene, XYZ coordinates, and a viewing direction. What is the angle of the camera

82
00:08:28,480 --> 00:08:34,560
in order to model those view dependent effects? The neural network then predicts at this coordinate

83
00:08:34,560 --> 00:08:40,000
what is the color of the scene and then its density, sigma. There's density as something like

84
00:08:40,000 --> 00:08:43,600
how solid is the object and how much light will be absorbed.

85
00:08:48,480 --> 00:08:54,320
Rendering is done by ray tracing. This is not exactly what would be done in most graphics

86
00:08:54,320 --> 00:09:01,440
applications like real-time ray tracing because we've kind of encoded the light being reflected

87
00:09:01,440 --> 00:09:07,200
at any given point back towards the viewer into this function. So we don't have to scatter

88
00:09:07,200 --> 00:09:12,960
light through the scene. The viewer will cast a ray from their camera through the pixel. This is the

89
00:09:12,960 --> 00:09:19,280
image plane into the scene and then query the neural network along the ray. These are different

90
00:09:19,280 --> 00:09:26,080
3D coordinates in the scene. The color of the rendered pixel will then be some accumulation

91
00:09:26,080 --> 00:09:30,640
of the colors along that ray. In order to determine the color and the density along the ray,

92
00:09:30,640 --> 00:09:34,080
each of these coordinates is passed to the MLP as a very large batch.

93
00:09:35,920 --> 00:09:41,600
We get a color and density for each coordinate and then can compose them with alpha compositing

94
00:09:41,600 --> 00:09:47,760
into a color. There's some subtlety to this compositing. This is called the volume rendering

95
00:09:47,760 --> 00:09:56,480
equation because this equation is pretty simple. This is the density predicted. This is the camera

96
00:09:56,480 --> 00:10:02,960
origin and it's displaced some steps along the ray. The neural network will predict what is the

97
00:10:02,960 --> 00:10:07,760
density of the scene at that point, but it will also predict what is color. Then we're integrating

98
00:10:07,760 --> 00:10:12,160
this color along the ray weighted by its density, but we also have to weight it by

99
00:10:12,160 --> 00:10:19,520
transmittance, which is roughly speaking how much light is transmitted from the viewer

100
00:10:19,600 --> 00:10:26,560
to that point along the ray because once we've accumulated enough density, then objects

101
00:10:26,560 --> 00:10:33,040
further back in the scene will not be visible to be included. This equation for color

102
00:10:34,320 --> 00:10:40,160
conditioned on coordinates is differentiable with respect to the parameters of sigma and c.

103
00:10:40,160 --> 00:10:46,720
So sigma and c will be this neural network. Because this is fully differentiable, it's

104
00:10:46,720 --> 00:10:51,120
relatively easy to optimize. Instead of optimizing scene geometry, we'll optimize the

105
00:10:51,120 --> 00:10:58,640
weights of this neural network in order to get some desired colors. This might be the sparsely

106
00:10:58,640 --> 00:11:04,240
observed viewpoints, two viewpoints. Let's render the color according to the neural scene

107
00:11:04,240 --> 00:11:08,160
representation and then try to optimize the network so that it matches the observed views,

108
00:11:08,160 --> 00:11:15,360
pixel by pixel. It might take a minute to wrap your head around, but it's actually pretty simple.

109
00:11:15,360 --> 00:11:20,880
We have this one MLP that encodes the scene, lets us render viewpoints differentially,

110
00:11:20,880 --> 00:11:26,080
and we'll optimize the scene so that it matches the input views, and that's why it's inverse

111
00:11:26,080 --> 00:11:31,920
graphics. We're going from the 2D space to optimize for the underlying 3D representation

112
00:11:31,920 --> 00:11:38,320
that will reconstruct those views. Are there any questions about that?

113
00:11:39,200 --> 00:11:50,480
Can you talk about how the points are used for the neural net? I can't see it directly.

114
00:11:52,320 --> 00:12:00,080
I mean, I get the high-level idea, but could you talk about how those points are fed into the net?

115
00:12:01,040 --> 00:12:05,360
Yeah, so you could consider constructing an MLP with five dimensions as input,

116
00:12:05,920 --> 00:12:11,280
just five inputs, and then four outputs on the layers, and then intermediate features or whatever

117
00:12:11,280 --> 00:12:17,680
you can imagine you want. So in nerf it's 256, that would be one approach, and it does work,

118
00:12:17,680 --> 00:12:23,200
but then you get actually quite blurry reconstructions of the scene if you directly feed an input

119
00:12:23,200 --> 00:12:27,600
coordinates as these are just floating point numbers, 3D coordinates, and going that direction.

120
00:12:27,920 --> 00:12:34,640
But instead, what is used in this neural radiance field is assigned useoidal

121
00:12:34,640 --> 00:12:41,520
positional encoding, so frequency-based encoding. If you're familiar with the transformer positional

122
00:12:41,520 --> 00:12:49,360
encoding, this is a common approach where continuous values like coordinates or time

123
00:12:49,360 --> 00:12:57,360
steps are encoded using a Fourier representation. So you take sine of various scaled values of the

124
00:12:57,440 --> 00:13:00,400
input coordinates, and that lets the network model high-frequency detail.

125
00:13:01,680 --> 00:13:06,720
So instead of kind of memorizing a function from each spatial coordinate, it can model

126
00:13:09,920 --> 00:13:14,080
frequencies of the underlying signal if you use the sine useoidal embedding of the input.

127
00:13:14,080 --> 00:13:18,480
So that kind of just projects this five-dimensional input into some higher-dimensional space

128
00:13:18,480 --> 00:13:24,240
before feeding it to the MLP. I see. And there's no, let's say, I guess, filtering

129
00:13:24,240 --> 00:13:33,440
done before, I mean, applying it to the net. So it's just transforming certain, I guess,

130
00:13:33,440 --> 00:13:38,880
components, but not doing some, I guess, post-processing before putting it into,

131
00:13:38,880 --> 00:13:43,520
or let's say compression or something like that. No, not really compression. There's

132
00:13:43,600 --> 00:13:48,960
some, like, coordinate transform because you'll do this computation in a particular coordinate frame.

133
00:13:51,360 --> 00:13:57,920
There is some subsequent work which we actually build upon that does a pre-filtering of the

134
00:13:57,920 --> 00:14:04,800
input coordinates. So instead of encoding all the frequencies of the input coordinates,

135
00:14:04,800 --> 00:14:07,920
they'll be blurred depending on how far away from the camera you're querying.

136
00:14:08,880 --> 00:14:13,680
But that's sort of subsequent work to nerve. That reduces the aliasing.

137
00:14:15,520 --> 00:14:17,680
Let's see. And the network is just fully connected.

138
00:14:18,320 --> 00:14:21,520
Yeah, just a fully connected network. Super simple.

139
00:14:21,520 --> 00:14:22,080
Yeah, thank you.

140
00:14:24,080 --> 00:14:31,440
Yeah, one more thing that I wanted to mention here for a student is that there is a difference

141
00:14:31,520 --> 00:14:38,480
between how you train this model versus the models that so far you have seen for,

142
00:14:38,480 --> 00:14:44,720
for instance, classification. For instance, if you want to train a model for a truck,

143
00:14:46,160 --> 00:14:52,560
what you do is you get a lot of images of different trucks in different lightings and different,

144
00:14:54,080 --> 00:14:59,440
you know, models and things like that, and then fit it to your network. However,

145
00:15:00,240 --> 00:15:06,800
in this case, you are taking lots of images of the single truck, single scene,

146
00:15:07,360 --> 00:15:14,400
and you are trying to reconstruct that scene. So you said big difference between, you know,

147
00:15:15,280 --> 00:15:19,840
what you are used to doing and what we see in nerve.

148
00:15:22,400 --> 00:15:28,720
Yeah, absolutely. I kind of see it as instead of, the nerve is representing a single scene. So

149
00:15:28,720 --> 00:15:33,440
instead of representing explicitly or representing it with a neural net with a function,

150
00:15:34,720 --> 00:15:38,720
but it doesn't generalize. It interpolates these input views.

151
00:15:42,640 --> 00:15:47,280
And, you know, there's a catch to that, which is that in order to fit into the neural radiance

152
00:15:47,280 --> 00:15:53,520
field to a single scene, it generally needs a lot of data. So while these views are sampled

153
00:15:53,600 --> 00:15:57,520
sparsely, discreetly, and there will be larger regions of space where we don't have

154
00:15:58,480 --> 00:16:05,280
an image taken from that perspective, still to estimate a multi-view consistent radiance field,

155
00:16:06,000 --> 00:16:10,720
experiments in the paper used a large number of images per scene. That's a little bit impractical.

156
00:16:10,720 --> 00:16:14,320
So for these synthetic scenes, this is one synthetic scene that's rendered in blunders.

157
00:16:15,360 --> 00:16:19,360
They were able to get out 100 images of each scene and fit the neural radiance field on it.

158
00:16:19,360 --> 00:16:23,200
For those outdoor scenes, I showed earlier like that red Toyota car.

159
00:16:24,480 --> 00:16:29,600
I think it's a fewer, maybe 20 images, but still that's a lot to capture with a handheld camera.

160
00:16:31,600 --> 00:16:36,400
And in the first week of work, I'm going to talk about we improved the data efficiency of the

161
00:16:36,400 --> 00:16:41,760
neural radiance field training process. So instead of using, let's say 100 images on this Lego

162
00:16:42,880 --> 00:16:46,720
scene, we used eight photos taken from randomly sampled viewpoints.

163
00:16:49,760 --> 00:16:55,840
In the neural radiance field training process, we would take, we would know the pose of each image

164
00:16:55,840 --> 00:17:01,360
that can be estimated with a system like call map. It's really common in the 3D graphics and 3D

165
00:17:01,360 --> 00:17:07,840
computer vision community is given some images, estimate their camera poses with correspondence

166
00:17:07,840 --> 00:17:14,160
finding, then the neural radiance field loss renders an image or renders some rays from the

167
00:17:14,160 --> 00:17:21,200
same pose as the input, then it computes a mean squared error loss. So the pixel wise error.

168
00:17:22,880 --> 00:17:26,880
The reason that this loss can be used is because we know the camera pose, we're able to render out

169
00:17:26,880 --> 00:17:34,240
the scene from the exact same pose that the observer took the photo. If the camera poses shifted

170
00:17:34,240 --> 00:17:39,680
in the rendering process, then the reconstructed image and the true image won't align pixel wise

171
00:17:39,680 --> 00:17:44,960
and we'll learn from inconsistent geometry. And so this is done at all of the observed

172
00:17:44,960 --> 00:17:50,000
camera poses. And this is why the neural radiance field needs so many photos. If there's no observed

173
00:17:50,000 --> 00:17:53,440
photo, then it doesn't have the ability to compute a loss from a given perspective,

174
00:17:54,400 --> 00:18:00,720
which means that it could overfit to the input use. This representation mapping from coordinates to

175
00:18:00,720 --> 00:18:05,920
colors is very flexible. One possible degenerate solution would be to put a billboard in front

176
00:18:05,920 --> 00:18:11,200
of each camera, just a poster board, you know, off of the highway, right in front of your camera

177
00:18:11,200 --> 00:18:17,120
containing the image that's observed, rather than learning a consistency in geometry.

178
00:18:18,800 --> 00:18:25,360
And there's other ways you can get artifacts. This is described as a shape radiance ambiguity

179
00:18:25,360 --> 00:18:30,560
in the Nerf++ paper. Essentially, we could either reconstruct the shape correctly and then have

180
00:18:31,200 --> 00:18:36,640
relatively constant radiance from different cameras, or we could encode each image into the

181
00:18:36,640 --> 00:18:40,960
view dependent coordinate of the network. So because the network depends on the camera position,

182
00:18:40,960 --> 00:18:46,160
it's able to memorize potentially the photo taken from each camera.

183
00:18:48,400 --> 00:18:52,720
When the neural range field is trained with 100 views, it gets really crisp reconstructions. This

184
00:18:52,720 --> 00:18:58,640
is a hot dog scene, synthetic scene, where we render out the views in Blender. Then the neural

185
00:18:58,640 --> 00:19:04,880
radiance field, when it's trained with only eight views, only matches pictures close to the training

186
00:19:04,880 --> 00:19:10,160
data. When you move the camera further away from the observed images to try to extrapolate,

187
00:19:10,160 --> 00:19:15,600
then there'll be a lot of artifacts. If you regularize the neural radiance field a little bit

188
00:19:15,600 --> 00:19:20,560
and simplify it, it can learn more consistent geometry, but there still are a bunch of artifacts

189
00:19:20,640 --> 00:19:29,840
in the reconstruction. I'll skip over this. So in our work, we add an additional loss to the

190
00:19:29,840 --> 00:19:36,080
neural radiance field training. We keep using the Nerf mean squared error loss. It's called

191
00:19:36,080 --> 00:19:43,120
photometric loss on the observed views that are sparse. But then our work diet nerf adds an

192
00:19:43,120 --> 00:19:48,240
additional loss at unobserved positions. So because we have this neural radiance field

193
00:19:49,040 --> 00:19:55,920
at any iteration during training, we're able to render out novel views even before the scene has

194
00:19:55,920 --> 00:20:02,960
converged. It's a little silly that in Nerf, we're not able to constrain these input views,

195
00:20:03,760 --> 00:20:09,280
because as a person looking at, okay, let's say that our estimate of the scene's geometry

196
00:20:09,280 --> 00:20:14,880
gives us these renderings. This is the observed rendering. We as people can still look at these

197
00:20:14,880 --> 00:20:21,280
photos and derive some loss signal. Okay, the input view is a lot sharper than my current

198
00:20:21,280 --> 00:20:27,840
estimate of the scene. There's a little red light at the top of the truck, but there's no light on

199
00:20:27,840 --> 00:20:38,960
top of these reconstructions. Based on this principle that you can compare views at different camera

200
00:20:38,960 --> 00:20:45,680
positions as a person by comparing their semantics, like, you know, it's a bulldozer, a bulldozer is

201
00:20:45,680 --> 00:20:51,600
a bulldozer from any perspective. We propose to add a loss in feature space. Using some visual

202
00:20:51,600 --> 00:20:59,520
encoder, each of the input views is represented in a feature space. And then instead of computing

203
00:20:59,520 --> 00:21:07,360
the loss in pixel space, diner will compute a loss in feature space. And that allows us to regularize

204
00:21:07,360 --> 00:21:16,640
the scene from any perspective during training. We call this a semantic consistency loss,

205
00:21:16,640 --> 00:21:21,440
since we're making sure that these semantic features, things like object identity, object color,

206
00:21:22,880 --> 00:21:29,680
are consistent across views. And over the course of training, this improves the results.

207
00:21:30,400 --> 00:21:36,000
So the loss that Nerf used was this mean squared error loss, and then we're adding this semantic

208
00:21:36,000 --> 00:21:43,840
consistency loss where some encoder thigh, some neural network encodes rendered images, and then we

209
00:21:43,840 --> 00:21:53,040
compare them in a feature space. We do have to sample camera poses in order to render this, so

210
00:21:54,000 --> 00:22:05,600
there's just some prior distribution over camera poses. The choice of that feature thigh is really,

211
00:22:05,600 --> 00:22:12,400
really important for the results, because we want it to be consistent across viewpoints. So it should

212
00:22:12,400 --> 00:22:17,520
really encode the object's identity and properties about the object rather than low-level details,

213
00:22:17,760 --> 00:22:28,480
like the exact pixel colors. And motivated by that, we use a network called Clip. This is from

214
00:22:29,440 --> 00:22:36,320
last year. It's a representation of images and text, so a representation of an images learn,

215
00:22:37,760 --> 00:22:42,400
such that it has an aligned representation with an associated caption. The data that Clip is

216
00:22:42,400 --> 00:22:48,080
trained on is a very large data set of 400 million images that have associated captions

217
00:22:48,080 --> 00:22:54,800
crawled from the web. And the Clip model has really led to an explosion of work in the AI art

218
00:22:54,800 --> 00:22:59,680
community. It's really powerful. It's trained on such a large amount of data that we're able to

219
00:23:01,840 --> 00:23:06,160
prompt it with topics that you wouldn't find in a narrow data set.

220
00:23:06,480 --> 00:23:11,760
It also, by learning to match images to this text, we'd hope to learn some very useful features

221
00:23:11,760 --> 00:23:17,280
about an image. For example, in captions, you can encode classes of objects, just like image net

222
00:23:17,280 --> 00:23:24,080
labels. You can also encode a lot of other details, like the scene rather than just the foreground

223
00:23:24,080 --> 00:23:30,160
object. You can encode things about pose of the underlying object, like a sitting person,

224
00:23:30,160 --> 00:23:37,280
a standing person. And that should be encoded in the representation learned by the network,

225
00:23:37,280 --> 00:23:41,600
if it's going to be able to match images against their associated caption. So the

226
00:23:41,600 --> 00:23:46,080
training objective is encode a bunch of images, encode their captions, and then try to match

227
00:23:46,960 --> 00:23:53,680
images with their true caption. Clip was originally used for discriminative tasks,

228
00:23:53,760 --> 00:24:02,960
object classification in a prompting fashion. So if you want to classify photos of food,

229
00:24:04,160 --> 00:24:09,200
the authors of clip constructed a bunch of captions, templatized with the desired object

230
00:24:09,200 --> 00:24:15,200
category, a photo of guacamole, a photo of ceviche. And then the class label is given by the

231
00:24:16,080 --> 00:24:19,200
caption with the best match with a given image.

232
00:24:19,600 --> 00:24:23,680
The property we're particularly interested in in this 3D reconstruction context is whether the

233
00:24:23,680 --> 00:24:28,480
representations of the images are consistent across views. That's what we call semantic

234
00:24:28,480 --> 00:24:35,040
consistency in the work. What this plot is showing is that the cosine similarity of embeddings

235
00:24:35,040 --> 00:24:40,400
from the image encoder of clip within a particular scene from different camera poses is highly

236
00:24:40,400 --> 00:24:46,800
similar. So very high similarity in feature space within a scene across different perspectives,

237
00:24:47,040 --> 00:24:59,280
low similarity across scenes at different perspectives. So because images are very

238
00:24:59,280 --> 00:25:04,080
similar in clip's feature space, very different in pixel space, we're able to maximize feature

239
00:25:04,080 --> 00:25:10,880
space similarity of clip and get some useful loss. Now what you've been waiting for are the results.

240
00:25:11,600 --> 00:25:15,680
This is nerf trained on eight views when it's simplified. And then here is it trained with

241
00:25:15,680 --> 00:25:20,880
our additional semantic consistency loss. A bunch of near field artifacts in nerf,

242
00:25:20,880 --> 00:25:24,640
but when we add in this feature space loss, it removes a lot of those artifacts.

243
00:25:30,960 --> 00:25:36,240
Because those artifacts aren't plausible, they reduce the semantic consistency.

244
00:25:36,240 --> 00:25:46,960
Cool. I'm going to go on to the next work. Before I do, anyone have questions?

245
00:25:46,960 --> 00:25:56,720
I have one question, which is with regards to using clip. Are you able to access the text

246
00:25:56,720 --> 00:26:02,400
as well that clip generates, or are you able to decode it in some way and actually access

247
00:26:02,400 --> 00:26:09,120
how the clip looks at the inputs? Just in terms of explainability, I thought it could be,

248
00:26:09,120 --> 00:26:13,440
yeah, sounds really interesting. Yeah, that's a very good question. So in this work,

249
00:26:13,440 --> 00:26:19,040
we're not actually using the text encoder. We'll see that in the next work. The text encoder is

250
00:26:19,040 --> 00:26:24,640
just used for pre-training clip in dye and nerf. So we're only using this image encoder.

251
00:26:24,640 --> 00:26:29,040
Because then the motivation for that is that the neural radian students are motivated by the

252
00:26:29,120 --> 00:26:33,200
view synthesis problem. So there's no text caption associated with the data.

253
00:26:33,200 --> 00:26:36,480
They just have a couple of pictures. So we only need to use the image encoder.

254
00:26:38,800 --> 00:26:44,080
That said, some artists have tried to take clip and use it to create a captioning model.

255
00:26:46,480 --> 00:26:49,840
If you have a model that can match images against captions, can you actually synthesize

256
00:26:50,560 --> 00:26:56,240
captions that best match a particular image? It's a challenging discrete optimization problem

257
00:26:56,240 --> 00:27:00,560
because you're searching for a textual caption that will maximize some neural network's output

258
00:27:00,560 --> 00:27:06,160
score. And that is basically a black box optimization problem. My impression is that

259
00:27:06,160 --> 00:27:10,480
automatic captioning with clip doesn't work too well. It's really good at selecting an associated

260
00:27:10,480 --> 00:27:16,160
caption out of a list of candidates. And that's how we're able to do object classification with clip.

261
00:27:18,880 --> 00:27:23,200
So I think you'd be better served by learning a specific captioning model that will generate

262
00:27:23,200 --> 00:27:27,200
a caption condition on image rather than trying to extract captions out of clip

263
00:27:28,480 --> 00:27:34,480
just due to the difficulty of the optimization or the search. Thank you.

264
00:27:40,800 --> 00:27:47,040
So like I said, we weren't using the text encoder in diet ner. In the next work, we try to

265
00:27:47,680 --> 00:27:56,080
move in an even more extreme direction of generating objects without any image data.

266
00:27:56,080 --> 00:28:00,320
So what if we only have a caption and want to synthesize the 3D object from it?

267
00:28:02,000 --> 00:28:07,360
Is that possible? Can we remove this mean squared error loss entirely and only use

268
00:28:07,360 --> 00:28:12,880
feature space losses? And these are some examples of the results we're able to get

269
00:28:12,880 --> 00:28:17,360
with different captions, like a render of a Jenga tower produces this object.

270
00:28:18,960 --> 00:28:26,960
You can also engineer prompts, use hashtags because clip is trained on web data.

271
00:28:30,000 --> 00:28:32,560
Our goal is to synthesize 3D objects from just the caption.

272
00:28:34,800 --> 00:28:41,200
And to kind of refresh our memories, the neural radiance field is an inverse graphics approach

273
00:28:41,200 --> 00:28:45,680
where we have densely sampled images, optimize the shared scene representation,

274
00:28:46,480 --> 00:28:52,560
and then are able to render out new views. In the dream fields work, the second work in this line,

275
00:28:54,160 --> 00:28:59,680
we do not have any observed images, only a caption written, for example, by a human artist.

276
00:29:01,840 --> 00:29:07,200
We optimize something that will look fairly similar to diet ner with additional regularizers,

277
00:29:08,160 --> 00:29:12,160
and then are able to render out new views. And any perspective is actually a new view

278
00:29:12,160 --> 00:29:17,040
because we haven't observed this scene. This is an associated scene for the caption,

279
00:29:17,040 --> 00:29:19,440
an epic, wondrous, fantasy painting of an ocean.

280
00:29:24,080 --> 00:29:30,560
So the neural radius would use this mean squared error loss, and then diet ner used feature space

281
00:29:30,560 --> 00:29:40,080
loss where the rendered image of the scene and an observed image of the scene are encoded into

282
00:29:40,800 --> 00:29:51,120
feature space that is optimized. Oops. Sorry. Okay. Now in dream fields, we use the text

283
00:29:51,120 --> 00:29:55,200
encoder of clip. That wasn't being used before. We were just throwing it away after trading.

284
00:29:56,160 --> 00:30:02,880
So instead of optimizing for the feature similarity in image feature space,

285
00:30:02,880 --> 00:30:08,320
we now maximize similarity of image and text features. The reason we can swap between

286
00:30:09,040 --> 00:30:12,880
the text encoder and the image encoder is because clip has learned to align representation.

287
00:30:12,880 --> 00:30:17,760
It has tried to maximize the similarity of representations of images and their associated

288
00:30:17,760 --> 00:30:23,760
captions so those representation spaces overlap. And you can in some sense swap the encoders

289
00:30:24,480 --> 00:30:28,800
from text encoder to an image encoder and hopefully still have that aligned representation.

290
00:30:30,000 --> 00:30:32,800
But overall, the pipeline looks fairly similar. So it's randomly sample

291
00:30:33,360 --> 00:30:41,040
poses in the scene, render an image, and then try to make sure that its semantic features

292
00:30:41,040 --> 00:30:50,080
match our features of the caption. But if you apply that approach naively without any regularizer,

293
00:30:50,160 --> 00:30:59,920
then there are a bunch of artifacts. These are some example generations for different captions.

294
00:30:59,920 --> 00:31:06,000
I believe this one had something to do with liquid in a blender. This one might have been

295
00:31:06,000 --> 00:31:13,920
a colorful bus with graffiti on it. So without regularization, we are getting to generate scenes.

296
00:31:13,920 --> 00:31:18,640
And it's not surprising because there's really no data involved in this process.

297
00:31:18,720 --> 00:31:22,800
In Dietner, the scene was regularized by having some input views.

298
00:31:24,800 --> 00:31:26,960
Here, the canvas is open, wide open.

299
00:31:31,520 --> 00:31:38,880
So in our work, we added some regularization. The scenes are composited with randomly sample

300
00:31:38,880 --> 00:31:46,320
backgrounds. And we regularize the scene to be highly transparent. So this transmittance loss

301
00:31:47,040 --> 00:31:52,080
encourages varsity in the underlying scene. So instead of getting lots of low density

302
00:31:52,080 --> 00:31:58,160
wispy content, like you saw in the previous slide, with a transmittance loss and this

303
00:31:58,160 --> 00:32:02,080
associated background, our motivation in Dreamfields is to create more of a consistent

304
00:32:02,080 --> 00:32:11,520
foreground object, a single foreground object. And these are the renderings for the

305
00:32:11,520 --> 00:32:21,600
associated caption, washing blueberries. There's definitely a lot of room for improvement

306
00:32:21,600 --> 00:32:27,120
because each of these blueberries is kind of mashed together with the others. The general

307
00:32:27,120 --> 00:32:32,880
caption has been encoded into this scene. And there's a consistent foreground object.

308
00:32:33,680 --> 00:32:44,480
This is the visualization of the process of optimization. In response to the question,

309
00:32:45,200 --> 00:32:49,440
Leandra asked, so it's creating this from one image, there's actually no images observed.

310
00:32:49,440 --> 00:32:55,040
There's only a caption fed to the system. So any images that I'm showing are rendered

311
00:32:56,000 --> 00:32:59,040
using our neural radiance field. They're completely fictional.

312
00:33:05,280 --> 00:33:11,360
I mean, some intuitive explanation for this is how can we learn a scene representation such

313
00:33:11,360 --> 00:33:15,760
that it could be captioned with a given caption from any perspective.

314
00:33:18,800 --> 00:33:23,120
Maybe that's how a human sculptor went approach the problem. So given a caption, like give me,

315
00:33:24,000 --> 00:33:33,680
you know, a clay sculpture of a tower. Well, let's say, you know, a monocular sculptor.

316
00:33:34,720 --> 00:33:40,400
Good. Optimize for a clay sculpture that is a tower of many perspective.

317
00:33:43,840 --> 00:33:49,520
Sorry, what happens? Sorry, what happens if the caption is something vague, like just a dog?

318
00:33:49,520 --> 00:33:54,880
How would your optimizer know that, like, it should have the same dog even from different

319
00:33:55,440 --> 00:34:03,760
poses or camera poses? Yeah, excellent question. The constraint that views should

320
00:34:03,760 --> 00:34:07,360
represent the same object from different perspectives just comes from the shared

321
00:34:07,360 --> 00:34:13,840
three presentation. We're optimizing the same MLP from any perspective.

322
00:34:13,840 --> 00:34:22,160
Okay, thanks. We had to simplify. Well, we didn't have to. You're able to keep view dependence in

323
00:34:22,160 --> 00:34:26,640
the neural radiance field. So this regularizer ends up being kind of important. Like I discussed

324
00:34:26,640 --> 00:34:29,840
with Dietner, if you're able to learn a lot of these near field artifacts.

325
00:34:33,440 --> 00:34:37,120
Sharing the scene representation is important, but some of the other techniques on our paper,

326
00:34:37,120 --> 00:34:41,040
like the regularizer are also important for making sure you get a clean result.

327
00:34:44,800 --> 00:34:52,960
In this example, we experiment with different caption templates to measure the

328
00:34:52,960 --> 00:34:58,160
compositional generalization of the model. So the base caption template here is a teapot

329
00:34:58,160 --> 00:35:05,520
in the shape of a blank, a teapot imitating a blank. And then in the video, the caption beneath

330
00:35:05,520 --> 00:35:12,560
each object is the word that's filled into the template caption. So a teapot in the shape of

331
00:35:12,560 --> 00:35:18,160
an avocado produces this object. Whereas the caption of teapot in the shape of a glacier

332
00:35:18,160 --> 00:35:26,320
produces something more ice styled. And I'm sorry about these animations.

333
00:35:28,400 --> 00:35:34,560
If you switch the caption from an armchair to a teapot, you'll also notice some changes in the

334
00:35:34,560 --> 00:35:39,280
shape. So there's legs on this avocado chair, but when it becomes teapot, the legs are removed.

335
00:35:39,760 --> 00:35:46,720
There's a follow-up question about whether the Clip Library is 2D. Yes, Clip is trained only on

336
00:35:46,720 --> 00:35:54,480
2D images. So just on 2D views. The motivation for using Clip is that we can very scaleably

337
00:35:54,480 --> 00:35:59,760
acquire caption images from the internet. If you, for example, look at Wikipedia and just look at

338
00:36:00,640 --> 00:36:05,920
the upper right image associated with each article, it has a caption beneath it. And there's a data

339
00:36:06,000 --> 00:36:09,760
set out there called WikiText, which has about 11 million captioned images.

340
00:36:10,960 --> 00:36:16,080
The authors of Clip were able to collect even larger data set by scraping websites other than

341
00:36:16,080 --> 00:36:24,080
Wikipedia. But if you look at data sets with 3D objects in them, they're very small. The largest

342
00:36:24,080 --> 00:36:29,440
might be ShapeNet, which is entirely synthetic objects. And there's usually no caption associated.

343
00:36:29,440 --> 00:36:35,520
So we'd have to have a human annotate. This is a general trend in the 3D reconstruction literature

344
00:36:35,520 --> 00:36:44,000
that the availability of 3D data is quite limited. And so in dream fields, we're able to exploit this

345
00:36:45,840 --> 00:36:51,760
pre-trained 2D image encoder and text encoder, and then kind of lift it up into 3D by using

346
00:36:51,760 --> 00:37:01,360
a shared representation of the geometry. There's a bunch of different techniques that we use to

347
00:37:01,360 --> 00:37:06,800
improve the quality of the results. I won't get too much into this, but the metric is a little

348
00:37:06,800 --> 00:37:13,280
tricky to define because there's no reference object for each caption. We only have a data

349
00:37:13,280 --> 00:37:20,320
set of captions provided to us by the user, and we're one of measure how well our generations are

350
00:37:20,320 --> 00:37:28,880
performing. In order to do that, we use a neural metric based off of matching generated 3D objects

351
00:37:28,880 --> 00:37:35,120
against the input captions. This is something like precision of retrieving the correct caption,

352
00:37:35,120 --> 00:37:39,520
given the generator objects. Some of the most important techniques that help us here are

353
00:37:41,520 --> 00:37:47,600
regularizer for transmittance and data augmentations, those architecture we use for the MLP,

354
00:37:50,720 --> 00:37:55,280
and then later on, what model we use for clip.

355
00:37:58,880 --> 00:38:01,920
This is an example of the process of optimization from different iterations,

356
00:38:01,920 --> 00:38:06,000
so it actually can converge quite quickly, but additional detail might be added over the course

357
00:38:06,000 --> 00:38:12,880
of training. In order to run 20,000 iterations of optimization, it's an expensive process

358
00:38:12,880 --> 00:38:18,720
because we need to render out these images during training, but back of the envelope calculation

359
00:38:18,720 --> 00:38:26,320
is about three to four dollars to generate each model on TPU in Google Cloud at an hour.

360
00:38:26,480 --> 00:38:29,440
It's in the realm where an artist could afford to do this.

361
00:38:30,720 --> 00:38:35,520
We're working on some follow-up work, which will speed up this process and make it even less expensive.

362
00:38:41,600 --> 00:38:47,440
That's all I've got on these works. The broad goal here is to make content creation

363
00:38:48,240 --> 00:38:55,520
easier and generate assets that are useful. This 3D assets I see is particularly useful

364
00:38:55,520 --> 00:39:00,400
for downstream applications because they could be plugged into a game or plugged into some other

365
00:39:00,400 --> 00:39:07,200
system. We have code out for both of these projects. If you want to try out the text

366
00:39:07,200 --> 00:39:11,760
to 3D generation in your browser, you can use a Colab notebook that I put together.

367
00:39:12,400 --> 00:39:17,200
I've tested it on the Pro version of Colab, which has higher memory GPUs,

368
00:39:17,200 --> 00:39:18,880
so you might need to play with some of the parameters.

369
00:39:18,880 --> 00:39:28,880
Thank you so much, Eje. This is really fascinating. I have a few questions,

370
00:39:30,240 --> 00:39:37,600
and then before letting everyone else ask questions, the first question is that

371
00:39:39,680 --> 00:39:48,560
are you able to walk us through some of the Colab code today or should we do it on our time?

372
00:39:49,840 --> 00:39:56,560
Let me see if I have it up. Also, before going to changing your screen,

373
00:39:56,560 --> 00:40:05,280
can you please go back to the animations? Sorry, I have so many questions because this

374
00:40:05,280 --> 00:40:15,680
is really cool. Or maybe the one that is armchair. Yeah, give me one sec. Thank you so much.

375
00:40:19,360 --> 00:40:29,360
I think these are really cool. I think that for the students and I, this kind of inspires us to

376
00:40:29,360 --> 00:40:40,640
think maybe one cool thing to do is that we can generate these things and use them in some avatar

377
00:40:40,640 --> 00:40:47,520
or game or something, and this will be really cool. This is something for students to think about

378
00:40:48,480 --> 00:40:57,920
for their future projects because the goal of this course is to inspire us to think about

379
00:40:57,920 --> 00:41:04,400
what are the creative ways that we can use AI. This is really cool. One question that I have

380
00:41:04,400 --> 00:41:14,160
is that, can you share some intuition of, for instance, let's say the rubric. It looks like

381
00:41:14,240 --> 00:41:23,040
a rubric and it looks like a chair, but then we see that there is some, we wish there was more of the

382
00:41:24,080 --> 00:41:33,760
structure and it might be because clip is the objective and or assessor and thinking that,

383
00:41:33,760 --> 00:41:42,800
okay, as long as I have a patch of red and yellow and things like that that are appearing on rubric,

384
00:41:42,880 --> 00:41:48,800
I'm happy, the rest, I don't care much, or is there any better explanation of what's happening?

385
00:41:50,240 --> 00:41:58,800
Yeah, so the 3D structure only emerges because of the shared representation and the easiest way to

386
00:42:00,480 --> 00:42:05,440
satisfy clip from any perspective, having this Rubik's Cube chair from any perspective,

387
00:42:06,560 --> 00:42:11,440
might actually be to learn some consistent geometry. That said, there's no prior

388
00:42:12,400 --> 00:42:17,200
other than sparsity and some implicit regularization just in the structure of the MLP,

389
00:42:17,840 --> 00:42:24,560
so there's no prior on the 3D structure learned from data. That's something that I think is missing

390
00:42:24,560 --> 00:42:30,800
and definitely opportunity for future work is how do we learn some priors on 3D data and integrate

391
00:42:30,800 --> 00:42:38,240
them into the system to try to improve the plausibility of the structure. One example where

392
00:42:38,240 --> 00:42:45,760
this issue arises is that sometimes you'll get repeated structures on the objects,

393
00:42:46,720 --> 00:42:52,720
like if you optimize for a dog, maybe it will have eyes on multiple sides of its face because

394
00:42:53,680 --> 00:43:00,080
they're not visible. So you only see two sets of eyes from any particular viewpoint,

395
00:43:00,080 --> 00:43:05,920
that is all the discriminator clip ever sees are those two eyes, but the underlying geometry,

396
00:43:05,920 --> 00:43:09,280
there's no constraint that the dog should only have to rise.

397
00:43:11,360 --> 00:43:16,320
Okay, excellent. Thank you so much. Are there questions before we go to the collab?

398
00:43:21,680 --> 00:43:29,360
The outputs, are they like .fbx files or do they still need to be, let's say, a little bit

399
00:43:29,360 --> 00:43:35,520
prepared in rendering software before they can be actually readily used in the game engine?

400
00:43:35,920 --> 00:43:42,480
Like Unity or Unreal? They do need to be post-processed. So what you get out is a train neural net,

401
00:43:42,480 --> 00:43:47,200
so it's function mapping from coordinates. We don't use the v direction in these results,

402
00:43:47,200 --> 00:43:53,840
just XYZ coordinates mapped to color and density. So there are a bunch of ways that you could convert

403
00:43:53,840 --> 00:43:59,200
that. I don't know of off the shelf software that will be able to do that conversion for you,

404
00:43:59,200 --> 00:44:05,680
it'd have to be coded up, but you can sample the scene on some grid, for example, and then you'll

405
00:44:05,680 --> 00:44:10,560
get out color and RGB. You could convert that to a local voxel representation. If you want to get

406
00:44:10,560 --> 00:44:18,240
a mesh, there's an algorithm called marching cubes that is able to find a mesh in the scene,

407
00:44:18,240 --> 00:44:21,920
and there's implementations on GitHub of marching cube for neural radiance fields

408
00:44:21,920 --> 00:44:28,960
that we haven't integrated into our particular library. So you take a little bit of glue to

409
00:44:28,960 --> 00:44:34,960
grab marching cubes and then plug it in. So what do you all use to turn the neural net into these

410
00:44:34,960 --> 00:44:41,520
graphics? Sorry, could you repeat the question? What do you use to turn the neural net into the

411
00:44:41,520 --> 00:44:45,840
graphics that we see here? Oh yeah, so that's done by rendering. So you can render the neural

412
00:44:45,840 --> 00:44:50,880
radiance field from any perspective in the code, but that just renders out a 2D image. It doesn't

413
00:44:50,880 --> 00:44:56,000
give you, you know, like a mesh, versus the game engine will have its own rendering algorithm based

414
00:44:56,000 --> 00:45:02,560
on rasterization or ray tracing, given the underlying geometry and texture map, which might

415
00:45:02,560 --> 00:45:07,200
be real time. So the rendering here is not real time. You have to go evaluate the neural network

416
00:45:07,200 --> 00:45:11,200
at a bunch of different coordinates and accumulate its outputs into an image.

417
00:45:13,200 --> 00:45:19,360
So that's implemented. If you want videos, we can do that, but you'll have to DIY the conversion.

418
00:45:24,720 --> 00:45:27,840
Ellie had a question on strategies to reduce rendering costs.

419
00:45:28,480 --> 00:45:35,280
So you can render images at low resolution. And in the Colab notebook, the rendering is done at

420
00:45:35,280 --> 00:45:40,000
very low resolution. So experiments, you render out 168 by 168 images or higher.

421
00:45:41,520 --> 00:45:46,720
But Colab only gives you a single low memory GPU. So we render out 88 by 88 images.

422
00:45:47,680 --> 00:45:53,440
And that really significantly speeds up the process. So rendering takes maybe 300 milliseconds.

423
00:45:54,400 --> 00:45:57,200
So you have to do about three iterations per second.

424
00:46:02,080 --> 00:46:15,040
If you're using alpha transparent, okay. So Ben is asking, how do we handle with transparent objects?

425
00:46:15,760 --> 00:46:23,680
So the neural radiance field, the volumetric representation is really amenable to transparent

426
00:46:23,680 --> 00:46:31,360
objects because the density is this continuous value and we can observe objects. So accumulate

427
00:46:31,360 --> 00:46:36,880
color from objects behind transparent objects. In optimization, you might decrease the

428
00:46:36,880 --> 00:46:41,280
density of some object that should be transparent, like stay in glass windows.

429
00:46:42,240 --> 00:46:48,640
And the background is composited at the end. So any ray, if there is some accumulated,

430
00:46:49,440 --> 00:46:55,840
if the transmittance is not zero along the ray accumulated throughout the scene,

431
00:46:56,400 --> 00:46:58,720
then there'll be some contribution from the background image.

432
00:47:00,480 --> 00:47:05,040
So the reason that we've kind of encouraged coherent objects is that if the object is not

433
00:47:05,040 --> 00:47:09,520
coherent, then the background will leak through the translucent objects. Oh, I see what you're

434
00:47:09,520 --> 00:47:14,560
saying. Yeah, if you want stay in glass windows. So I mean, you would have to, the scene would

435
00:47:14,560 --> 00:47:18,960
probably optimize so that the transparent object is blocked from behind by something.

436
00:47:28,240 --> 00:47:34,160
Yeah, the next steps, I think they're exciting lots of next steps, because this is an initial work

437
00:47:34,160 --> 00:47:39,360
and there's things like speeding up the optimization. It's been a lot of recent work and

438
00:47:39,360 --> 00:47:42,800
speeding up neural radius field training for images. And I think a lot of that can be plugged in.

439
00:47:44,560 --> 00:47:49,200
And how do you synthesize the formable objects? How do you bring a human in the loop so they

440
00:47:49,200 --> 00:47:56,160
can provide feedback partway through training? All kinds of stuff to tackle in making this more

441
00:47:56,160 --> 00:48:07,280
of a practical system for 3D artists. And would you like me to share the collab? I guess we're at

442
00:48:07,280 --> 00:48:11,680
time. Yeah, please go ahead. That would be great. Thank you so much.

443
00:48:19,280 --> 00:48:23,040
So this is the collab notebook. You can find it from the project website.

444
00:48:25,040 --> 00:48:28,240
It is a compact implementation.

445
00:48:28,400 --> 00:48:39,680
The system will run faster on GPU than on TPU in the collab notebook. But for all of our experiments,

446
00:48:39,680 --> 00:48:46,320
we use TPU. Some helpers are imported from our library. So if you want to hack on some

447
00:48:46,320 --> 00:48:51,840
of the low level primitives, you can fork our library or kind of copy those helpers into the

448
00:48:51,840 --> 00:48:56,640
notebook. But the main way you'll interface with this collab notebook is by adjusting the quality

449
00:48:56,640 --> 00:49:06,720
settings here. So in particular, edit the query. Here I've filled in a high quality 3D

450
00:49:06,720 --> 00:49:13,920
render of Jenga Tower. And you can select the clip checkpoint you want to use. Clip bit B16

451
00:49:13,920 --> 00:49:18,800
is used in most of our experiments. There's also an internal Google model that's not available here.

452
00:49:19,920 --> 00:49:24,880
But you can scale down if you're running out of memory to either the B32 or ResNet 50.

453
00:49:26,880 --> 00:49:31,680
Choose the number optimization iterations. I think at least 1000 is necessary.

454
00:49:33,520 --> 00:49:37,600
But more will add more detail. Consider the rendering width and then this is the number

455
00:49:37,600 --> 00:49:47,200
of data augmentation. And then run training. So here's an example of the training run I've already

456
00:49:47,200 --> 00:49:52,480
run in the notebook for that prompt, a high quality 3D render of Jenga Tower. It won't exactly

457
00:49:52,480 --> 00:49:55,920
match the result that was shown in the slides because the version of the collab notebook could

458
00:49:56,000 --> 00:50:02,640
scale down. But over the course of 2000 iterations of optimization, you can see the different

459
00:50:02,640 --> 00:50:08,240
learning curves. This is the total loss that's being optimized. Clip's cosine similarity,

460
00:50:08,240 --> 00:50:12,720
negative cosine similarity is improving. So this means that the renderings of the object

461
00:50:12,720 --> 00:50:16,320
are becoming more and more consistent with the given caption over time.

462
00:50:19,040 --> 00:50:23,280
And the transmission is regularization here. This is showing what is the average transparency

463
00:50:23,360 --> 00:50:33,920
of pixels in the scene. And in this plot at the bottom, the collab notebook will ride out

464
00:50:33,920 --> 00:50:44,720
renderings periodically every, I believe, 100 iterations. So at the beginning, the scene is

465
00:50:44,720 --> 00:50:50,720
low density, essentially empty. And then over time, some content will emerge from the optimization.

466
00:50:51,520 --> 00:50:57,120
And then that's refined and sharpened over time. The camera's moving around. So the camera's being

467
00:50:57,120 --> 00:51:03,680
randomly sampled around the object. And that's why the scene is rendered from different perspectives.

468
00:51:04,800 --> 00:51:10,640
And then finally, the collab notebook renders out a video, 48 frames. And this is the result.

469
00:51:12,400 --> 00:51:18,560
On the GPU that collab gave me here a P100, the optimization I think took about six, seven minutes.

470
00:51:21,680 --> 00:51:25,360
So hopefully you can get some cycles in.

471
00:51:28,800 --> 00:51:34,160
In the run training section, it says if you run out of memory, tweak the configuration options

472
00:51:34,160 --> 00:51:41,040
above. What do you recommend changing? Yeah, that's a good question. So I think

473
00:51:41,040 --> 00:51:48,880
you can change this clip at B16. I would try to clip B32. There's also on the first import

474
00:51:48,880 --> 00:51:52,400
in NVIDIA SMI printout. And so you can look at how much memory is available.

475
00:51:53,040 --> 00:52:01,200
Sometimes it's worth retrying multiple times to get a larger GPU. This P160

476
00:52:01,200 --> 00:52:04,960
gigabyte I think you won't get without collab premium, which is about $10 a month.

477
00:52:06,560 --> 00:52:13,120
But you think you can get 15 gigabyte T4 GPUs for free. Sometimes the collab will give you an 11

478
00:52:13,120 --> 00:52:18,560
gigabyte GPU that might not be enough. If you can tweak the configuration parameters,

479
00:52:18,560 --> 00:52:24,080
I would try reducing this number of samples. So this is the number of points along each array

480
00:52:24,080 --> 00:52:32,320
that is sampled. And that affects the batch size. So the render width, the batch size scales

481
00:52:32,320 --> 00:52:36,720
quadratically with the render width because we're rendering got square images. And then the num

482
00:52:36,720 --> 00:52:42,720
samples the batch size to the MLP scales linearly. So you could reduce this down to 32 even at the

483
00:52:42,720 --> 00:52:49,600
lowest. B32 will use less memory than B16. So this relates to the patch size and the vision

484
00:52:49,600 --> 00:52:57,200
transformer clip encoding. And then if you want to scale down even more, you can change the number

485
00:52:57,200 --> 00:53:09,680
of data augmentations per iteration, maybe down to two.

486
00:53:18,000 --> 00:53:20,720
Oh, Ben says that you can't retry for a better GPU.

487
00:53:21,600 --> 00:53:29,120
That's unfortunate. I mean, I don't know whether MIT has like a shared GPU cloud, but you can

488
00:53:29,120 --> 00:53:35,440
also just download this IPIND and run it on your like Jupyter notebooks, post it on some MIT compute.

489
00:53:37,040 --> 00:53:43,200
And it will paralyze across multiple GPUs.

490
00:53:50,880 --> 00:53:56,640
Cool. And have you taken any more questions that you have?

491
00:53:56,640 --> 00:54:03,840
Excellent. Thank you so much. Maybe at this point I'll stop recording and if students have more

492
00:54:03,840 --> 00:54:05,200
questions, we can...

