start	end	text
0	4880	Hello, everyone, welcome back to our course, a deep learning
4880	12840	for art, acetic and creativity. Today, it is our pleasure to
12840	18280	have very a specialist speaker, David Bao, and I just let him
18280	22640	to introduce him a little more, because I think it's very
22640	29400	inspiring for many students, the path that he has come to
29400	35040	this point and for future. Please go ahead. So I was, I
35040	43920	want to give a little background since I am a post
44120	46760	industry academic, I spent a bunch of years as a software
46760	50760	engineer at Google before coming back to MIT. And I want to
51000	56720	give a little bit of insight in my thinking there. So, you know,
56720	58320	the reason it's really interesting to be in computer
58320	61600	science right now is because the field is changing. The dream
61640	66000	of having self programmed computers is one of the oldest
66000	72200	dreams in computer science, but it's never been a reality. Even
72200	75640	though we've studied machine learning for a long time, I think
75640	79120	that until just a few years ago, machine learning was really
79120	82920	more accurately called, it would have been more accurately
82920	87520	called the art of accurate counting. You know, statistics,
89160	93160	you know, understanding the statistics of, you know, how
93160	97080	frequent words are and by grams or, you know, certain image
97080	101760	statistics or something like that. And, and, and if you if you
101760	105600	understand statistics well, then, then, then, then, you know,
105600	110920	you could do some nice tricks. But I think that until recently
110960	115760	really calling these things sort of self programmed systems
116400	118680	would have been an overstatement. But I don't think it's
118680	121960	really an overstatement anymore. I think that these machine
121960	127640	learning models are really learning non trivial things. And
127680	130600	it leads to all sorts of questions about, you know,
130680	134840	what should we be doing as programmers? What does it mean
134840	137280	to do software engineering? And so I thought it was very
137280	140840	interesting time to come back to academia. That's, that's why
140880	144920	I'm here. And I actually think that that's one of the choices
144960	148200	you face when you're trying to decide between industry and
148200	153400	academia. And I think in industry, you will have lots of
153400	157920	resources to make things work to make the next widget or the
157920	162160	application. And, you know, there are great places, Google is a
162160	164840	great place, we can really push state of the art in that and
164840	169480	do really neat stuff. I think that there's less of a push in
169480	173800	industry to ask the question, Why? You know, why do things
173800	177440	work? Why are we doing what we're doing? Where is it going to
177440	180040	lead in either unintended consequences and things like
180040	182800	that? You know, we, we tend not to ask those questions too much
182800	186160	industry, because there's so much to emphasize on, you know, the
186160	192640	how of how to how to get it to work. And so, and so, so I
192640	197020	thought it was a time to, to switch tracks and start asking
197020	199200	why because the field is changing so dramatically. And I
199200	203280	think that, you know, I'd encourage people who have an
203280	208840	interest in these type of questions to, to realize you
208840	211480	can really make a real contribution by taking the
211480	216480	academic track as well. So okay, so let me introduce my talk.
216480	219760	So it's about painting with neurons of general adversarial
219760	225720	networks. It comes out of work from asking why, you know, why
225760	233600	do these networks do what they do? And so, so let me, let me
233600	238320	advance here. Am I in full screen? So do you see the, do you
238320	241440	see the like the full screen slideshow I can't see what I'm
241440	244080	projecting? Or do you see like all my notes and all that stuff?
244200	248120	Yeah, I can see it. But also maybe a student can tell us.
249400	250000	Yeah, okay.
250360	251000	That's fine.
251400	256480	Is everything okay? Yeah, it's a full screen slide. Hopefully,
256520	261680	it's okay. So, so, okay. So the main problem that we're looking
261680	265520	at here, and I'm not sure why the, the images are overlapped in
265520	269400	the right way. Hopefully, the layout will get fixed. So we're
269400	273920	going to next slides. But the, the, the, the, the main problem
274160	279160	surrounding my talk is image generation. And so, for the last
279200	281560	few years, there's been this question, how do you make a
281560	286960	state of the art program to generate realistic images? And,
287320	289560	you know, the general process is first you want to collect a
289560	292640	data set of real images, like these pictures of buildings on
292640	299160	the right. And, and then you want to, you know, train some sort
299160	302200	of program, some sort of generator network to generate
302760	306040	those programs. And so, so, you know, it's been a puzzle. There's
306040	308920	a lot of different ways you could imagine doing this. And so
309760	312680	people have been puzzling, how do you train such a thing? How do
312680	315240	you even supervise it? You know, what should the, what should
315240	319560	the inputs and the outputs of the network be? And, and, and the
319560	322200	thing that has really been working the best in recent
322200	324640	years is, you know, in architecture, you guys have all
324640	328760	heard of called GANs, generative adversarial networks. And the
328760	332400	trick for GANs is to reduce it down to a simpler problem that
332400	336240	we know what we're doing. And so the simpler problem that
336240	340160	they're recognized when designing GANs was that
340440	342880	generating images, we don't really know how to do, but
342880	346320	classifying images, gosh, that is an easy problem. We can
346320	351240	classify images. And so, so what we could do is we could train a
351240	355960	classifier on this really easy task, which is given two sets
356000	361040	of pixels, which image is real, and which image is not a real
361040	365480	photograph. And it turns out that for most arrangements of
365480	368560	pixels, this is a very easy task to train a discriminator on
368560	371560	it gets very good, you know, very quickly, we'll start getting
371560	376520	100% accurately on that. And so, so but the neat thing is that
376520	378320	once we have a discriminator that can tell the difference
378320	382360	between a fake image and a real image, then we can hook it up
382360	385600	to our generator, and we can say, All right, we didn't know how
385600	389560	to tell you, generator, how to make a real image. But you know
389560	392480	what this discriminator can tell you, because all you have to do
392480	396520	is generate patterns of pixels that fool the discriminator, if
396520	400520	you can make the discriminator think it's real, then it must be
400800	406960	better than random. Now, the problem is that, even though the
406960	409920	discriminator can get very accurate at telling what's real,
411200	414720	they, the generator will also be very good at learning how to
414720	417800	fool the discriminator without working very hard, it'll realize
418000	420960	that aha, the only thing I need to do to make the discriminator
420960	424360	think is real is put some blue sky in there and put some texture
424360	427440	that kind of looks like, you know, building texture. And, and
427440	429640	the discriminator will say, Well, that totally looks real,
429640	433120	there's a sky, you know, there's, there's the right, the right
433120	436120	colors for buildings and some vertical lines and things. Ah,
436120	438760	that's totally real. But as a human, we look at that, we think,
438760	441880	Oh, that's not a very realistic image at all. So the trick is to
441880	444560	iterate this process to go back and forth after the generator
444560	449000	can generate sort of halfway looking real images, then have
449000	452680	the discriminator say, Ah, well, that's actually fake. And
452680	455120	we're going to tell the difference between those new fakes,
455120	458200	those better fakes, and actual real photographs, and the
458200	461120	discriminator has to now work harder at getting better. And so
461120	465000	if you, if you alternate these processes, then, then you end up
465000	468280	very conversion to very, very good generators that can generate
468280	475600	very realistic images. And they, you know, the typical learning
475600	479440	process is actually just to do only one step of iteration
480040	483240	between the discriminator and generator and just alternate that.
483240	486360	So by the time you're done, you've played this game, you
486360	488520	know, millions and millions of times back and forth between
488520	491200	the generator and the discriminator. But the new thing
491240	494360	that's happening here is that it can generate these images that
494360	500800	look very realistic in the end. But let's see. So Oh, here's
500800	505000	another picture. So we'll get this images out that look very
505000	508400	realistic in the end. And we'll get this generator, which is
508400	512440	just a deterministic function that takes actually the input of
512440	516480	the generator is actually just a random vector. So we'll take
516480	519680	these relatively small random vectors like 512 dimensional
519680	522480	random vector, and we'll put it into this thing. And it's been
522480	525760	trained so that no matter what it outputs, it will look very
525760	528720	realistic, like this example image here. Or if I change a
528720	531680	vector, I'll get a different image out and it will again look
531680	534440	very realistic, even if it looks completely different. And so
534480	537080	it's just a deterministic function that really wants to
537080	541920	make realistic images. And, and so here's like a sample of like
542320	545640	output from a generator. And you can see that after millions of
545640	549920	these sort of generative training steps, where it's
549920	552800	pitted against a discriminator, it actually gets to be pretty
552800	556840	good. And so this is a style game v2. It's a model that was
556840	562040	published last year. And, and it's, you know, currently the
562040	566240	state of the art in generating realistic images of certain
566800	571000	certain types of image distributions. And, and so when
571040	573840	when you look at a collection of images like this, you might
573840	576880	think, actually, the first time I looked at the output of some of
576880	582840	these state of the organs, I was confused between the training
582840	586360	set, and the generated output, this is not the training set,
586360	591280	this is actually what the generator is producing. And so, so
591280	595600	you see all sorts of interesting effects here. And so the one of
595600	600200	the questions to ask is, what the heck is the model doing
600200	603280	inside? Can we understand the underlying algorithm? And what
603280	606760	the characteristics of that algorithm is like, why does this
606760	613040	work? And so one of the funny things that you'll notice is
613480	616680	that some of the images have these strange artifacts, like
616680	622440	take a look at this one here. So this, this scan is pretty good.
622560	628360	It's this generator is so good that it actually has noticed that
628360	633720	the training distribution that is imitating has some percentage
633720	639040	of images that were stolen off of shutter stock. And they still
639040	643600	have the watermark on them. And, and, and, and the generator
643600	646660	says, well, if I want to make things look realistic, I better
647140	651580	put watermarks on some percentage of my images too. It
651580	655740	learns it's got to protect its own copyright. So, so it, it
655740	661900	does that. And so something like 6% of the output images from
662300	665140	state of the art style, again, will have these kind of
665140	669620	artifacts that show the same type of watermarks that were on
669620	674180	the training set. This is the Elson Church training set. And
674220	677780	so, so yeah, this kind of watermarks like this. But the
677780	682260	reason I thought this was cool was that it, it's this very
682260	685740	clear thing that the image generator does, but it doesn't
685740	689580	always do it. Like most of the time when it generates images,
689860	693260	it generates images without a watermark, but sometimes you get
693260	697900	these watermarks. And so, and so it's, it's almost like this
697900	704020	binary decision. It's like, there must be a switch that the
704020	707940	network has at some point where it decides whether it's going to
707940	711300	put a watermark on an image or not. And so we can kind of ask
711300	713900	the question, where's that switch? Is there a neuron
713900	718420	somewhere in this network, which is, which is controlling the
718420	722700	watermarkness. And so, so I went on a hunt for this, this,
722700	725140	this particular network has about 30 million parameters,
725140	729260	which sounds like a lot, but it's just a deterministic computer
729260	734300	program in the end. And, and it's not that hard to go hunting
734340	737780	for things like this, you just, you can make an algorithm that
738380	741020	has a heuristic that determines whether it's a watermark or not
741020	745340	and just go hunting for, for things that correlate with that.
745900	752060	And so I'll show you what I found. So at layer five, I found
752060	757140	this very interesting neuron that did correlate with watermarks
757140	762180	a lot. It was activating whenever images look like this in
762180	766340	the end. And, and not only that, but because it's at layer five,
766980	771340	it has a has a location for where the image activates. And I'll
771340	775340	show you where, where it's activating. So, so this neuron
775340	778220	is activating, you know, at these middle parts of images,
778220	780820	whenever the image is showing a watermark. And there are other
780820	783540	neurons that have similar behavior, like so for example,
783540	788900	there's this neuron 234 at the same layer. And it activates in
788900	791580	regions like this, both in the middle watermark and the bottom
792020	796820	bar that shows up. And there's about, if you hunt through the
796820	801620	neural network, you find about 30 neurons that are similar and
801660	808220	behave like this. And so that's, that's pretty cool. So then the
808220	811780	question is, well, do these things really act like a switch?
811820	815260	What if we've removed these neurons from the network? What if
815260	819060	we force them all off? What if we turn, what if we force these
819060	823220	neurons to be off all the time? That will happen. So normally,
823220	827420	we think of these neural networks as completely opaque
827420	831980	systems. We train them end to end, they're just, you know,
832060	835420	these big black box functions. And we normally think of the
835420	838780	functions as computing things where everything depends on
838780	840820	everything. And so if you randomly rip through the
840820	845180	function and remove some of its operations, then maybe you
845180	848980	expect to get total nonsense out just garbage or noise. But we
848980	851580	found these particular neurons that really correlate to this
851580	854100	thing. So let's see what happens when we turn them off. Do we
854100	858180	get anything intelligible at all? So this is what the network
858180	861780	generated before these are the watermark images I showed you
861780	865140	before. And I'll show you what happens if I turn off these 30
865700	871060	watermark neurons. So I'm going to give the network the same
871060	875020	input. But turn off these neurons during its computation,
875020	877060	and you can see what the output looks like. So you can see
877500	881020	before chain, you know, forcing these neurons off and after
881020	885660	forcing those neurons off. The images are still very
885780	890340	intelligible, they look realistic still. But now the
890380	894620	watermarks are gone. So I thought I was when I when I saw
894620	898100	this, I was pretty excited, because it's like, Oh, there are
898100	900980	switches inside the networks. And these networks are doing all
900980	903940	sorts of amazing things, not just like showing watermarks. You
903940	906380	know, so when I first found this, it was on Progressive GAN,
906380	909500	which is a year earlier than a couple years earlier, the images
909500	912980	didn't look quite as good. But but still in Progressive GAN,
912980	917460	they do all sorts of amazing things, like they will arrange a
917460	921700	scene with a river and trees and grass and, you know, building
921700	925140	architectures with all sorts of different features. And you can
925140	928460	ask, you know, is there a switch to turn on and off clouds in
928460	932540	the skies or switch to turn on and off trees or windows and
932540	940500	buildings? And, and so I went hunting for that. And, and, and
940500	944220	the way I went hunting is I tested every neuron one at a time
944220	946780	I inverted the test. So basically, I look at each neuron,
947140	951020	and I say, Where is it activating? And, and I asked a
951020	955420	question, is it activating an interesting part of different
955420	959020	images? So for example, if I took this one neuron here, and I
959020	962100	see where it's activating when it's generating this image, you
962100	966340	can see it's very hot on the right and on the left, but not
966340	971580	much up in the sky. And on this very same neuron, when we
971580	976060	generate a different image with a different input, this very same
976060	979140	neuron is not activating very much anywhere in this this image.
979980	983460	But if we generate another image, then it will activate in a
983460	988140	specific area here, mostly on the lower left part of this image.
988140	990500	And you can see what's on the lower left. There's a, there's a
990500	994340	tree there. And so it kind of gives you the hypothesis that
994340	997700	maybe this neuron is correlated with trees somehow. So
997700	1000980	obviously, we can, we can do this, we can collect this
1001220	1005860	information over thousands of examples of generated images by
1005860	1010660	looking at where the neuron is activating, we can ask what what
1010660	1014300	kind of thing is in the image, what kind of objects, what are
1014300	1018180	the semantics of the image in the location that the the neurons
1018180	1023740	are in. And we can just repeat that test process, you know,
1023780	1028340	thousands of times to see if the neuron is agreeing with any
1028340	1032700	particular kind of semantics that are in the images. So if, if
1032700	1035740	the, if the neurons are showing up where the trees are all the
1035740	1039780	time, we can just count and see if if that's if that's true in
1039780	1043820	general. And we can also look for correlations with other
1043820	1047580	things. So what I did is I, I searched for correlations with
1047580	1050340	thousands of different, you know, hundreds of different,
1051140	1054340	different types of semantics and object classes, different parts
1054340	1057660	of buildings or, or objects or other things that can show up
1057660	1061100	in a scene. And so what do we find? Well, we do find, you
1061100	1063260	know, there's a neuron that correlates with trees, just like
1063260	1066300	the one I was showing you. There's actually a few that are
1066300	1069380	like that. And there's also neurons that correlate with
1069380	1073540	other things like domes, or, or other building parts like
1073540	1077980	windows and doors. And, and if you change the model to look at
1077980	1081220	other things, then you can find neurons that correlate with
1081220	1086780	things like windows, or chairs, or other things that they show
1086780	1090420	up in, in the scene. And so this is actually pretty neat,
1090420	1097260	because this model was trained unsupervised by any labels. All
1097260	1102300	we did is we told it, generate realistic looking scenes,
1102660	1108700	realistic looking photos. And, and, and we did not train it
1108700	1113460	with any labels, we didn't tell it that these are photos of
1115380	1119100	scenes that have big windows, and these are photos of scenes
1119100	1121980	that have little windows or anything like that. Or, or, or
1121980	1125300	here's where the windows are. But what happened was, the
1125300	1129220	network discovered that it had to, you know, learn a
1129220	1135900	representation, where windows are represented differently from
1135900	1142260	the way chairs are represented. But somehow, even though, you
1142260	1144500	know, windows can look very different from each other and
1144500	1146860	chairs can look very different from each other, that the
1146860	1149380	network has this represent this this component of this
1149380	1156580	representation, this neuron that activates on all these chairs,
1156820	1159660	despite the amazing amount of diversity that it shows, like
1159700	1163700	none of these chairs really look similar to each other, they
1163700	1166060	have different colors and different textures, and they're
1166060	1170140	oriented in different ways. And yet, the same neuron is
1170140	1173260	activating on all of them, the same thing goes for other
1173260	1177100	things that show up in these images. So does anybody have
1177100	1180060	any questions about, about, about this? I'd love this to be a
1180060	1181860	little bit more interactive than the way I'm doing the talk.
1181860	1184860	So let me open the floor for a question for a minute.
1187420	1192140	Has anybody tried playing with the internals of GANs yet? I'd
1192140	1198820	love to see if has anybody like generated images using a GAN
1198820	1199340	before?
1201900	1208700	No, but I do have a question. Yes. So what was like the end goal
1208700	1213460	or the larger reason behind finding all of these neurons
1213900	1218820	that correspond to different objects or features?
1219540	1224300	Well, when I was originally looking at it, my original goal
1224300	1230340	was just to understand how these models did their computation.
1230540	1236180	So asking the question why. But the neat thing is that after I
1236180	1239740	found this structure, then it became clear that there are
1239740	1244780	new applications that you can build on top of it. And I think
1244780	1246780	that's one of the cool things that comes out of this sort of
1246780	1250660	academic style inquiry is, you know, originally, I was just
1250660	1252980	looking to make catalogs like this. This is a catalog of all
1252980	1255540	the different types of correlations that I found with
1255540	1258220	neurons inside a model for generating kitchens, and the
1258220	1261860	kinds of, you know, the patterns you see. And, and, you know,
1261860	1264500	I've done this before for classifiers. And, you know, when
1264500	1266700	you do for generators, you get different patterns. And so I was
1266740	1269980	just really interested in making these maps of seeing what is
1269980	1274460	computed at what layer, you know, where and how accurately. So
1274460	1279020	this is, you know, the progressive gain has, depending
1279020	1282500	on the resolution has about 15 layers. And if you sort of
1282500	1285740	chart what you see in different layers, you can see this this
1285740	1288460	really interesting thing phenomenon where it's in the
1288460	1292940	middle layers that you get these highly semantic correlated
1292940	1296300	neurons. But then as you get to the later layers, then they
1296340	1299260	tend to be more physical. And there's not as many semantic
1299260	1302380	objects. So it's like in layer five, we have things that really
1302380	1305820	correlate with ovens and chairs and windows and doors, even
1305820	1309860	though like a window kind of looks like an oven. The model
1310220	1313700	clearly has different neurons that correlate with windows from
1313700	1318700	ones that look like ovens. And so so that so that's that's that
1318700	1322500	so I was originally interested in just mapping things out. But
1323460	1327420	the correlations were so striking that it leads to these
1327420	1330580	interesting applications that you can build. And I can I'll show
1330580	1336220	you some in the next step. Let me before I do that, let me see
1336220	1338220	if anybody else has a question as well.
1341340	1345420	Yeah, David, I was hoping that you could also show us the
1345420	1349740	application at some point, which I think these are very good to
1349780	1354980	see why you asked this question. I mean, yes, it's more
1355780	1358620	That's great. Let me let me zoom out to the application. So
1360060	1363220	so the the neat thing is that just like we could turn off
1363220	1367860	watermarks, we can turn on and off things in image generation.
1367860	1370060	So for example, if I find all the neurons that correlate with
1370060	1373740	trees, and I turn them off, you can see what happens. I'm going
1373740	1377900	to turn them off sort of one at a time here. And so originally,
1378340	1381460	the image will just be generated this. But if I turn off some
1381460	1385980	tree neurons, you can see that we can actually remove the trees
1385980	1389460	from the scene. And the cool thing is that this is different
1389460	1393420	from Photoshop. If you went and you tried to erase trees from an
1393420	1397180	image, then you'd have this puzzle of what would happen
1397220	1400300	about stuff that was occluded by the trees like what's going on
1400300	1404660	behind there. And so this image generator is actually it's got
1404700	1407940	this latent model that has an understanding of what the scene
1407940	1411340	is. And so, and even has an understanding of things that is
1411340	1415100	not explicitly drawing. So if you remove the trees from the scene,
1415380	1419220	then it'll come up with a reasonable looking, you know,
1419460	1424340	image to draw what was behind the trees, or you can do the
1424340	1429340	opposite. Which is you can take neurons that were not originally
1429340	1433780	on in a generated scene and turn them on. So if I take a set of
1433780	1436660	neurons that correlate with doors, and I turn them on in a
1436660	1439540	certain location, and you can see what happens in the generated
1439540	1442380	image, you know, I'll get this door in the scene, not only
1442380	1445900	will it just be a door, but it'll be it'll have like an
1445900	1451220	appropriate size and your orientation and style for for
1451220	1453940	the building that it's in. So if I take exactly the same neurons,
1454460	1457820	and I activate them in a different location like here in
1457820	1461140	this building, then even though it's exactly the same neurons
1461140	1464740	exactly the same activation that I've done, I get a different
1464740	1468100	door that is like a much smaller has a different style and so on.
1468100	1471660	It's appropriate to the building that it's in. If I if I try to
1471660	1475140	put a door in a place that would make sense, like by turning on
1475140	1479820	neurons up in the sky, then it like will like not do anything.
1480860	1484300	This is this is the actual output of what happens if I turn on
1484300	1488180	the exact same neurons up in this location. So there's a lot of
1488220	1492340	interesting context sensitivity that you can measure. But one of
1492340	1494380	the cool things that you can do is you can actually hook this up
1494380	1499900	to a paintbrush user interface, like I can find neurons that
1499900	1503700	correlate with domes or doors or things. And if I want to add
1503700	1506300	doors to a building, I can just sort of paint them on. And the
1506300	1509380	doors will show up and you can see the orientation of the doors
1509380	1512980	is appropriate to the wall that you put them in. If I just say I
1512980	1516780	want trees, it'll put trunks and leaves, you know, in the right
1516780	1519100	place in the trees or plants with a plant, plant them on the
1519100	1522700	ground. If I take grass and I can turn the grass neurons off and
1522700	1526180	remove grass from the scene and it'll come up with what the
1526180	1529260	scene should look like instead. And so I can kind of do these
1530220	1533900	semantic manipulations directly. Oh, here we're turning on domes
1533900	1540220	and you can see it will turn the top of the the church from a
1540220	1544140	spiral to a dome, but it also sort of stitched the dome into
1544140	1547740	place to make it look good here. I'm removing grass again. We
1547740	1553020	can like put a door in the scene. And if I if I put you know,
1553020	1556540	sort of put a door in the wall, then it'll it'll come up with
1556540	1559900	like the appropriate location and style orientation for the door
1559900	1564940	even if I draw very roughly. So when I'm drawing, every time I
1564940	1567900	touch the surface here, what I'm really doing is I'm just turning
1567900	1573140	on a few neurons. And and I'm letting the the math of the
1573140	1579780	GAN generator deal with all of the the details of how to arrange
1579780	1582180	the actual pixel. So does that does that sort of give you a
1582180	1585260	sense? Does that answer your question for like, you know,
1585260	1588340	what kinds of things you can do with this by understanding what's
1588340	1591300	going on in the interior of the model? Maybe now I should stop
1591300	1596060	it. Oh, yes, go ahead. Are these different neurons for like
1596060	1601060	doors in different areas? No, no. So when I when you click on the
1601060	1607780	door button on the left, I am picking 20 neurons that are
1607780	1611540	the door neurons. So by doing the statistical analysis ahead of
1611540	1615340	time that I showed you earlier, I've identified 20 neurons that
1615340	1619860	correlate very strongly with the presence of doors. And when you
1620020	1624020	click on the button on the left, I am picking those neurons. Now,
1624220	1626540	it's a convolutional network. So there's this translation
1626540	1631460	and depends those neurons appear at every pixel. And so what you
1631460	1634660	can do is you can just turn on those neurons in random pixels
1634660	1635460	that you touch.
1636980	1639180	Changing where the neurons are, that was what I didn't
1639180	1641860	understand. Does that make sense? So, so because it's a
1641860	1644900	convolutional network, so it's actually it's it's like the neural
1644900	1649020	network is cloned at every location. It's the same neural
1649020	1652820	network that's being used to process every, you know, patch
1652820	1657700	or patch of pixels in the image. And, and so if I asked for a
1657700	1660300	door in a place that wouldn't really make sense, then it
1660300	1663180	won't put a door there. If I asked for a door in a place that
1663180	1665420	makes sense, it'll make a big intervention, it'll stick a big
1665420	1668500	door there, which you can see. So I could be very rough about
1668500	1673660	where I put a door and it'll like put it in the right place. So
1673660	1679100	that's that's the idea. So let me let me zoom around here, I'll
1679100	1684220	show you a couple other things that you can do. So now there's
1684220	1687340	some limitations to this. And I'll just show you some of the
1687340	1689300	techniques that you can use to get around the limitation. So one
1689300	1692100	of the problems is that, you know, we can do all this cool
1692100	1695900	editing, but we can do this editing of a randomly generated
1695900	1701220	image. And, and so, so when I posted this demo on on the web,
1701260	1703660	you know, an artist called me and said, Hey, you know, I love
1703660	1707060	how you can edit images, I can edit this image of a kitchen
1707060	1710780	here. But that's not the kitchen I want to edit, I want to edit
1710780	1716100	my own kitchen, right? Like here's a photo of my kitchen. And I
1716100	1721740	want to edit that one. And I had to explain to them, you know,
1721740	1725660	they said, Oh, can you just load into your demo? My my kitchen
1725660	1729340	instead of yours. And I had to explain, no, no, no, that's not
1729340	1733460	how GANs work. They're unconditional generators. You
1733460	1739540	know, you give it a random vector of 512 numbers. And it
1739540	1743900	decides what image to make. And then once it decides what image
1743900	1746780	to make, then you can edit it. And so I'm sorry, I can't edit
1746780	1749980	your kitchen. And so they were very disappointed by that
1749980	1752140	because they had all sorts of ideas of things they wanted to
1752140	1758260	do. And so now the problem is that, you know, the problem
1758260	1762380	could be solved if we could find the random vector, some random
1762420	1766420	vector that that output the kitchen image or the specific
1766420	1771380	real photo that I wanted. The problem is how do I find it 512
1771380	1776860	dimensional vectors as pretty big vector space. And and so I
1776860	1780700	don't know if my GAN can actually generate this image or not. So
1780700	1784260	one of the things you can do is you can just treat this as a as
1784260	1787500	an inversion problem. You can take the neural network and you
1787500	1791900	can learn how to run it backwards. Basically, you know,
1792300	1795340	think of the neural network as a function G, and you want to
1795340	1798300	learn G inverse. So you can treat that as another training
1798300	1800580	problem. And there's a bunch of tricks and I won't go into all
1800580	1804740	the tricks here. But but basically, the idea is that you
1804740	1808140	can actually find a Z that comes closest to generating your image
1808140	1813580	by by training and doing a couple other tricks. And you can
1813580	1818660	actually get a Z that will generate your image pretty
1818660	1823460	closely. But the thing that's a little bit sad is it also
1823460	1827860	reveals things that the network cannot do. So so this network
1828740	1831660	is capable of generating this image that I'm showing you here.
1832220	1835340	But the original kitchen that I started with look like this. So
1835340	1837420	you can see what the differences are. I've lost a lot of
1837420	1842700	stuff. Right. So, you know, I can use the GAN to edit this
1842700	1846940	image. But this image is not exactly what I started with. And
1846940	1853980	so. So one of the pieces of science that that I did is I
1853980	1858260	asked a question, you know, is there some way that we can
1858260	1861500	actually make this work? Can we actually, you know, get the
1861500	1866620	network to output a real photo that that the user gave us? We
1866620	1870060	get the network to output this sort of simplified version of
1870060	1875060	it. It turns out that if I modify the weights of the
1875060	1881460	network, I can actually fine tune the network to get it so that
1881460	1884900	a very, very nearby network with weights that are almost the
1884900	1891060	same as the original actually hits this target image. Exactly.
1891820	1896900	And so so there's a bunch of details in the right way of
1896900	1899620	doing this. But it turns out that, you know, you don't actually
1899620	1902780	have to change much if you change the fine grained weights
1902900	1907300	of a network. You can you can change a lot of the details of
1908340	1912500	what images actually get generated. And and and if you
1912540	1917180	are given a target image to get you can actually tweak tweak any
1917180	1921740	network to generate exactly that target image if we want. And
1921740	1922100	so
1924100	1927700	so you know, so yeah, we can get all the objects back. But the
1927700	1929820	new thing is we haven't really changed the network much. So we
1929820	1933300	can still do editing. So like if we take the window correlated
1933300	1936340	neurons, we can take our modified network, we can turn them
1936340	1942180	on. And and now we can like add a window. Let's see if we show
1942180	1944380	that. Yeah, so this here's outlook. So we get this nice
1944380	1948900	window here. And the scene is began is doing its cool tricks of
1948900	1951860	orienting the window properly, doing some reasonable things. And
1951860	1955620	it has some really interesting effects that are non trivial
1955620	1958820	here. Some of them are good and some are bad. So for example,
1958860	1961740	all I did was turn on the neurons in this location saying I
1961740	1967460	want windows. And it did it. But look what else it did. It also
1967460	1970020	added these reflections right here on the counter. And so this
1970340	1974140	this kitchen guy does this a lot like adds adds non local
1974140	1976660	reflections where it thinks that there's a shiny table. And so
1976660	1979300	the cool thing here is that after I did all the inversion and
1979300	1982340	stuff, this guy actually thinks that there's a shiny table here
1982340	1984860	and it's right. And it thinks that if I add a window here,
1984860	1987340	they should add reflection. That's right. Also, but look
1987340	1990860	what else happened up here. See this lamp up here. When I first
1990860	1992980	lifted this in low resolution, I thought, Oh, maybe it turned off
1992980	1995060	the lamp because once you have windows, you don't need the light
1995060	1998700	on. But no, it didn't do that. It just messed up the lamp. It's
1998700	2002820	just total it took this whole area up here and just and just
2002820	2007580	distorted it badly. And so so that that's a little dissatisfying.
2007580	2012020	It means that this fine tuning thing, where we get again to,
2013020	2017660	you know, target a specific user image, when I do when I try to
2017660	2020580	teach it all the details, I'm not really teaching it what the lamp
2020580	2024620	was, I was just sort of showing it how to arrange the pixels. And
2024620	2029540	again, made its best guess on how to generalize how the image
2029540	2032180	should look differently. If I change something like out of
2032180	2035340	window, but with only one example of a lamp that looks like
2035340	2037860	this, it generalized wrong, it has no idea what should happen to
2037860	2040940	that lamp when I when I add a window. So this is this question
2040980	2045340	of like how to make changes in a network with with with
2045340	2050260	achieving good generalization is, which is a good question. And
2050300	2053820	it was, there was something that puzzled me for a year after
2053820	2057380	doing this work. But but the work is still pretty cool, you can
2057380	2061460	still use it for modifying real photos. So here's like a photo of
2062020	2066700	I got off of Wikipedia of like some real locations. And you can
2066740	2069860	you can edit them, I can add grass, I can add doors, I can add
2069860	2073860	domes, you know, just like, just like the the the other
2073860	2076380	campaign app, except I can actually start with a real photo
2076380	2078980	that you give me. And I can invert that photo through the
2078980	2081820	network, get a good starting image, fine tune the network to
2081860	2085540	make it make it output, you know, the target image and edit that
2085540	2088540	image, add bigger domes, and it'll sort of match the
2088540	2092780	architectural style. And, and, and, you know, do different things
2092780	2095700	like that, I can add domes, remove domes, add doors, you
2095700	2098900	know, things like that. Let me see if I can get this video here
2098940	2104140	to show. So this is the status center. Let's add some doors
2104140	2108380	here. So you get the idea, I'm doing exactly the same
2108380	2111820	intervention that I did before. And it's it's opinion just like
2111820	2115260	before, it will not add doors in places that it doesn't think are
2115300	2117900	not good places for a door, it has some opinions about where
2117900	2120740	doors are allowed, it likes to put them in brick walls. It
2120740	2125780	thinks it's okay to put a door in a tower, like that architectural
2125780	2128880	detail. Oh, I put domes here. It's happy to put domes on top of
2128880	2131560	buildings. It's not happy to put a dome like in the middle of the
2131560	2135520	sky. It's not happy to put a door in the middle of the sky. But
2135520	2140760	you know, it put trees in different places. And, and so
2140760	2142160	there are things that it understands, there are things
2142160	2144320	that it doesn't understand very well, it's sort of making a
2144320	2147160	guess of what the structure of the image is. It doesn't know what
2147160	2150480	to make of my advisor, you know, sort of planting grass in front
2150480	2153920	of him. And that's not very realistic. But you kind of get
2153920	2157280	a feel for what the structure and knowledge of the model is by
2157280	2159560	doing these kind of interventions. So this was really
2159560	2162840	cool. I think it got a lot of people's attention. Adobe
2162840	2167080	noticed this stuff, and has been busy trying to make different
2167080	2172280	painting applications using, you know, GAN technology that are
2172320	2177160	I think partially inspired by by by this kind of discovery. So
2177440	2182920	David, I have a question. Yep. This is really cool. Question is,
2183280	2188240	when you modify, for instance, churches, I assume you have
2188600	2193800	trained your GAN on a church data set. Yes, that's correct.
2193840	2197680	What about when you do it on the real images, for instance, in
2197680	2202920	this case, you know, your advisor? Yes. So actually, both of
2202920	2205880	these are using the church data set as well. So the church
2205880	2206400	data set,
2206440	2210760	interesting that even you have trained again on church, you
2210760	2212480	can depict a person.
2213280	2218320	Yes. So this is so the GAN. Now, you have to keep in mind that
2218320	2222480	what I've done here is I fine tuned the GAN. So you can
2222480	2225280	actually, you know, you can actually get you can actually
2225280	2231160	get a GAN to do a lot of things by fine tuning it. So I've I've
2231160	2236240	told the GAN, please basically overfit on this target image. So
2236240	2243880	the GAN, you know, has 30 million parameters. And, and you
2243880	2247200	know, an image only has, you know, 10,000 pixels, and it has
2247200	2251520	plenty of excess capacity to memorize the details that I
2251520	2256320	might want to do. And so what I've done is this as I've taken
2256320	2260400	the image, I've asked again, through my inversion techniques,
2260440	2264880	what is the closest church image that you can generate that
2264880	2268000	looks like my thing. And you get a different image. I don't
2268000	2271240	have the image to show you here, but you get an image that looks
2271240	2273800	kind of more church like it's a little bit, it'll be
2273800	2276800	architectural have the right kind of shape, the kind of right
2276800	2280280	textures. But you know, it won't show my advisor here and
2280280	2283720	things like that. It'll be, it'll be this rough approximation
2284240	2290040	for that my my image, but that is in the domain of what the GAN
2290040	2294280	can actually generate. Then I say, Okay, that's not what I
2294280	2297560	want to do. I want to actually edit this photo. So let's fine
2297560	2304680	tune that network so that so that given that same Z instead of
2304680	2307400	generating the church that you would normally generate, I want
2307400	2311560	you to generate this image, change the weight slightly, get it
2311560	2316520	so that that Z targets this. And, and so that's what I've done
2316520	2319320	here. But I've tried I've done that in a way where I try not to
2319320	2322240	change the weights too much. I just try to change the weights. I
2322280	2324800	change the fine grained layers. And I don't change the coarse
2324800	2328320	grain layers. And I, and I have a regularizer to make sure the
2328320	2333760	weights don't change too much. And that you are changing the
2333800	2339440	pre trained weights, or you are putting some extra weights, and
2339440	2343960	then you place them. Oh, here, I'm actually changing the pre
2343960	2348640	trained weights. So the network has 15 layers. I'm actually
2348640	2352480	going and I'm changing some of those layers. I'm not adding
2352480	2356920	anything new to the network. I'm just changing the weights in
2356920	2361480	the network itself. Now, now what I've done here is I've
2361480	2365720	overfit the network to this one image. The network is not
2365800	2370040	generalizing this knowledge. So for example, you can draw
2370040	2374720	Antonio in this one image. But if I look in the network, if I
2374720	2377840	probe it a lot and see, can it ever generate Antonio in a
2377840	2381960	different setting in a different image? It cannot. In
2381960	2386640	fact, you know, as much as we probe things, it really doesn't
2386640	2389160	look like we've changed the output of the network in any
2389160	2393720	meaningful way for any image, except for this one. It's almost
2393720	2397480	like, you know, the network generates this really complicated
2397480	2401520	manifold of realistic images. And we've told we've picked up one
2401520	2404680	point of the manifold, and we've dragged it over to pass to
2404680	2408160	this point. But we've done it in a very local way. So it's
2408160	2411280	really not affected any other points of what the GAN is
2411280	2418200	generating. And so so but but for the purposes of doing this
2418200	2420120	kind of application, it doesn't matter that it's not
2420120	2422600	generalizing because the user doesn't care about a different
2422600	2425000	photo, they just care about their own photo. So it's a pretty
2425000	2429040	cool. It's pretty cool technique anyway, even though it's sort
2429040	2432920	of not the classical goal of machine learning. Does that make
2432960	2437840	sense? Yeah, it does. And I wonder if the user has more
2437840	2443760	images of themselves with that over time, and make the network
2443800	2445640	even better in generation?
2447440	2450480	Yes, this is the big question. And I played with this for many
2450480	2452720	months, and I haven't got it to work. And if anybody can figure
2452720	2454760	out how to get to work, I feel like it's one of the holy grails
2455160	2458160	of like how to add a new thing to a generator. So like, the
2458160	2461120	generator knows about all these things that knows about trees
2461120	2466400	and knows about all these architectural pieces, you know. But
2466400	2468880	what if I came along with something new? What if I was
2470680	2474360	what if my what if I what if I work for GM and I want to sell
2474360	2477040	Cadillacs, then I then I might come to one of these models and
2477040	2479640	say, you know what, you should draw cars. In fact, I want you to
2479640	2482680	draw specific cars. I want you to draw Cadillacs in front of all
2482680	2487760	these buildings. How would I add Cadillacs to my model or add
2487760	2491120	Antonio to my model or something like that? And we don't know how
2491120	2493400	to do that yet. Although I'm going to show you a little bit of
2493400	2498200	work, where we can do something that's very similar. And if I
2498200	2501120	don't know if I have time to, to go over this, but I'm going to
2501160	2503440	I'm going to zoom through this because I'm so excited by this
2503440	2510680	work. So, so, so it's motivated by this, this sort of question,
2510720	2515640	which is, you know, we have a model of like drawing towers,
2515640	2519400	let's say, right? But there are things in the world that we might
2519400	2523400	want to model that we don't have a data set for. For example, you
2523400	2527080	know, in in in Decatur County, Illinois, there's this courthouse
2527080	2529880	that has a tree growing out the top of the tower. It started
2529880	2532800	growing out there by accident, but the people in the town love
2532800	2536240	it. And so but it's but there's no so like if I want to get a
2536240	2540360	generative model to draw trees growing out of tops of towers, I
2540360	2544240	can't do that in a classical way because I can't create a big
2544240	2547840	data set of a million buildings that have trees growing on the
2547840	2550520	top of the towers, because they don't exist. It's just this one.
2551360	2556640	And so now if if the point is I want to generate images of this
2556640	2560840	type, you know, well, I could use a regular image editor, I
2560840	2563200	can take any building of a tower, and of course, I can stick a
2563200	2567000	tree on it, right? I could use my, you know, again, painting
2567000	2570080	method to, you know, activate tree neurons or something like
2570080	2573280	that. But no, no, that's not what I'm asking. I'm asking this
2573280	2577240	other question of like, how can we stick tree towers into my
2577240	2581160	model? How do I modify the model to have this new concept in it?
2581480	2583880	Like I start with this model that has all these weights that
2583920	2586800	encode all these rules for how buildings look and things like
2586800	2591080	that. And I want to create a new model that has new weights that
2591080	2594120	encode new rules. So for example, the old model could generate
2594120	2597200	all these buildings that of towers that look normal have
2597200	2600880	spires, you know, pointy tops. And I want to make a new model
2600880	2604560	that has weights, they encode a different rule, so that like,
2604680	2608200	they have trees growing out the top, right, or any rule that I
2608200	2612800	choose, right? And it turns out that this is actually possible.
2612800	2616160	So this is different from the technique that I showed you
2616160	2619080	before, because in this technique, it's actually
2619080	2623240	generalizing. This is, you know, if you use this technique, not
2623240	2628120	only you change the output of one image of the GAN to have like
2628120	2633480	some effect, but we can actually change the outputs for a whole
2633480	2638360	class of, you know, a large subset of the outputs of the GANs to
2638360	2644000	follow a different rule, like any pointy tower output will have
2644040	2648000	trees instead of pointy towers. And so so I'll just show you a
2648000	2650360	little bit of like the interaction here of what it
2650360	2655280	looks like when you get our method into an application. So I
2656200	2658680	let's see if I can get this to work. So here, what I'm showing
2658680	2662840	you is the output of a style GAN be to generating churches, you
2662840	2666680	can kind of, and there are three parts of this UI, there's an
2666680	2670920	image viewer, then what you do is you can select a rule that you
2670920	2673440	want to change, and then you can specify how you want to change
2673440	2675800	your rule. So there's three parts of this little user
2675800	2679040	interface. And I'll just show you sort of how how the effect
2679040	2683520	looks by showing you one of the interactions. So you can kind of
2683520	2688560	use the image viewer to scroll through lots of examples of of
2688560	2692400	what the the generator is capable of generating. And then we
2692400	2694800	can go to these examples and we can say, Hey, you know what I'm
2694800	2698760	really interested in? I'm interested in this rule of how
2698760	2702200	to generate pointy towers. And so I can select a few pointy
2702200	2706320	towers. And you can think of this as what I'm looking for is
2706320	2709360	the neurons that are responsible for the shape. And so I can
2709360	2713920	select a few examples and I can say, Hey, what other, what other
2713920	2717600	outputs of the GAN share the same representation? And, and it'll
2717600	2720120	show me, Oh, yes, the GAN is generalizing this way, these
2720120	2723520	other pointy towers are represented the same way as the
2723520	2727040	ones that you chose. And then I can go and I can say, All right,
2727040	2733400	I want to redefine how these pointy towers are rendered by
2733400	2736640	this generator, I want them to be rendered like this tree here.
2736640	2741240	So I can copy the tree from one output of the generator, and I
2741240	2745680	can paste it into where I would like that tree to show up. I
2745680	2749440	wanted to show up instead of pointy towers. And then I can
2749440	2754000	say, Okay, now insert this new rule into the model, compute
2754000	2757840	what the right changes to change the model. And then after I do
2757840	2762240	that, that takes about a second to do the math to figure out how
2762240	2766080	to change a rule. And then after I do that, then I get the GAN to
2766120	2770480	generate new images. And, and they look like this, you know,
2770520	2774760	like the tops of the towers, now have trees on them instead. So
2774760	2777600	you can see how that looks. And it's not just affecting that
2777600	2782680	one image, it's affecting all the pointy tower images. I can do a
2782680	2786400	little search for more pointy tower images. And, and do I have
2786400	2789760	that here in my thing? Yeah, so here's a search for more pointy
2789760	2792120	tower images. And you can see they, you know, they all have
2792120	2796040	gotten these trees sprouting out the top of it, like some sort
2796040	2801880	of dystopian tree world where vegetation is taking over the
2801880	2806600	planet. And, and so you can do this in a bunch of things, I'm
2806600	2810880	gonna skip over some of the technical things here, or some
2810880	2813640	of the other examples of what you can do here. You can edit
2813640	2816600	reflections and things like that. I've got other videos that you
2816600	2819400	can look for on the internet. But I wanted to show you a sense
2819400	2822800	for what we're doing inside when we do this kind of thing. So
2822880	2827280	like I showed you before that again, has is like, got all these
2827280	2831920	convolutional layers stacked up, it's about 15 layers. And what
2831920	2836120	what, what, what the discovery was that led to this application
2836640	2840920	was that each one of those layers can be thought of as
2840920	2846000	solving a very simple, separate problem from the other layers.
2846360	2849800	And what is that simple problem? It, it can be treated like a
2849800	2854840	memory, where the layer is solving this problem of matching
2854840	2859680	key value pairs that it's memorized. So every location
2859760	2864320	has a feature vector that you can think of as a key. And what
2864360	2866760	and the key each key like, you know, represents a certain
2866760	2869360	type of context, like, you know, the middles of towers or the
2869360	2872640	tops of towers or something like that. And what you can think
2872680	2880680	of the map as as as storing is what should be what is like the
2880680	2885000	pattern of features that should be rendered whenever that
2885000	2888120	context comes up. Right. So you can think of it as just
2888120	2894760	basically key value store. And and so so this whole idea of
2894800	2897880	using a matrix as a key value stores and it's like the oldest
2897880	2903560	idea in neural networks. People observe back in the 1970s,
2904080	2908360	that if you have a single layer neural network, you can treat
2908360	2912320	it as a as an approximate key value store that remembers keys
2912320	2918320	with minimal error. And and so if you had a set of keys and a
2918320	2921320	set of values you want to store, and you ask what is the
2921320	2924480	optimal single layer neural network that you'd use to store
2924480	2928840	it. It's actually, you know, classical linear algebra, it's
2928840	2931840	like the solution to a least squares problem. So what we can
2931840	2936880	hypothesize is that in these very, very fancy, you know, 2020,
2936920	2942680	you know, 50 years later, deep neural networks, actually, each
2942680	2945920	layer is just acting as one of these. Now, which keys are being
2945920	2949600	stored and what values were being stored? We don't know. But
2949600	2952240	we could hypothesize that there is some set of things that are
2952240	2956680	being memorized, some set of keys and values. And so that that
2956680	2959560	maybe this weight matrix that we have is the solution to the
2959560	2963200	assembly squares problem. So the cool thing that we can do is we
2963200	2965880	can say we can ask the question, what would the weight matrix
2965880	2970120	look like if we changed one of the rules? What if we had one new
2970120	2974280	key value pair that we wanted to change? Then what would the
2974280	2978120	weight matrix be? Instead, we want all the other things that
2978120	2981680	the network has memorized to still be memorized with minimal
2981720	2985000	error, just as before, except we're going to give this new
2985000	2988360	constraint, we want to write a new key value pair into it. And it
2988360	2990320	turns out that that's also least squares problems and
2990320	2991800	constrained least squares problem, we can write down the
2991800	2995320	solution in this form. And the cool thing about these two, the
2995320	2998640	squares problems is that they cancel each other out. Most of
2998640	3003080	the terms are the same. And, and, and we can actually ask the
3003080	3007360	question, how would the weights have to change if we add a new
3007360	3011480	key value pair, without knowing which values were written into
3011480	3015360	the network before, we don't actually have to know what the
3015360	3020600	old key value pairs were, we can just assume that the network
3020600	3025320	was optimal as storing all these key value pairs. And, and the
3025320	3029640	math for like how to write a new key value pair comes out the
3029640	3032120	same anyway. So, so that's there's there's a little bit of a
3032120	3037320	mathematical insight and trick here. But what it allows us to do
3037320	3042440	is it allows us to find exactly what we want to do to change one
3042440	3044720	thing that the network is memorized, you do this rank one
3044720	3050600	update in a specific direction. And, and you can take a key and
3050600	3054200	change it to any value you want. And that will, you know, the
3054200	3059760	same form will minimize error for, for other keys, regardless of
3059760	3063400	what value we write, it's almost like it really is a form of
3063440	3067320	memory, that we're changing. So our method is basically you
3067320	3073080	find a key by asking the user to select a few contexts that look
3073080	3076040	the same, we average them to get a good key. Then we ask for a
3076040	3079240	copy paste example to get a goal. That's the new value that we
3079240	3081840	want to write into the key of the memory. And then we do this
3081840	3088000	math to, to find how to change w in the direction of the key
3088000	3093520	only, we find a rank one update that does this. And so, and so
3093520	3095480	that avoids changing other rules. So we can do this on a bunch
3095480	3099120	of different GAN models. And, and so you can see, like, you
3099120	3104520	know, people like to change people's expressions here. So
3104520	3107280	what we're doing is a little different from what you normally
3107280	3109160	do to change people's expressions. And again, what
3109160	3111440	we're doing is we're actually going to rewrite the GAN. So it
3111440	3113520	only outputs people who are smiling, we're going to take all
3113520	3115880	the frowns, we're saying, okay, there's, there's a rule for
3115880	3118040	frowns, we're going to change that to a rule for smiles by
3118040	3121680	showing an example. And so by patching frowns to smiles, now
3121680	3124880	we have a model that just outputs people who are smiling. Now
3124880	3128440	we live in a happy world. So that's, that's, that's pretty
3128440	3131080	cool. And now, of course, we could have done that by, you
3131080	3135080	know, changing the training set by collecting only training
3135080	3137480	data of people who are smiling. But the neat thing is that you
3137480	3140560	can also do this for things where you don't have a training
3140560	3143080	set that looks like it. So for example, there's a, there's a
3143080	3147160	rule in the model for how eyebrows should look on kids. So
3147160	3151080	you can see that kids have these very wispy light eyebrows that
3151080	3154480	don't have much hair. So we can find that rule by identifying a
3154480	3157400	few examples that gives us a rank one direction in the weight
3157400	3161000	matrix. And then we can redefine it, we can write a new thing
3161000	3163280	into it and say, you know what, we want the eyebrows to look
3163280	3167000	like this, like that's very bushy much sash. And, you know,
3167000	3171000	paste into one example, do the math. And then now we can change
3171040	3174000	weights in a way that generalizes. So now all the kids had
3174000	3178440	these very bushy, you know, eyebrows. And it's something
3178440	3181520	that we wouldn't have been able to get by collecting training
3181520	3183800	set because we don't have kids that look like this in the real
3183800	3187280	world. It's something that just comes out of our imagination. So
3187280	3191720	this is kind of the thing. I kind of feel like this is the big
3191720	3195720	reason why, why, why be interested in how these models
3195720	3200640	are working inside. And the reason to be so interested in it is
3200640	3204560	because as long as we don't look inside our models, then we're
3204560	3207960	really constrained. Because the only thing that our models can
3207960	3212480	really do is imitate the real world. We can collect huge
3212480	3216040	amounts of data. And the models that we create, we'll just get
3216040	3219680	better and better at imitating the way that the data is the way
3219680	3223480	the world is today. And I kind of feel like it goes a little bit
3223480	3226600	against why I was interested in computer science years ago when I
3226600	3229200	entered it in the first place. Because the amazing thing about
3229200	3233360	computer science is that you can use it to create algorithms
3233360	3236480	that represent things in the world that don't exist yet,
3236520	3239520	things that you can only imagine. And so machine learning is
3239520	3242400	sort of on this path right now, where we're getting very, very
3242400	3246640	good at replicating the way the world is. And we're going to be
3246640	3250280	confronted with this question of how do we use these techniques
3250440	3254080	to actually create new worlds that don't exist yet that are the
3254080	3258400	way that we want them to be. And I think that this really
3258400	3263880	going to require us to not just get models that are just really
3263880	3267720	good at imitating, but also models that are understandable to
3267720	3273160	people so that we can change their rules inside, and then use
3273160	3276480	them to create things that are based on our imagination instead
3276480	3284320	of just the training data. And so here's a fun thing here, I
3284360	3290560	think, if I want to be fair to the horses, you notice that none
3290560	3295080	of the horses in this horse generating GAN get to wear hats
3295120	3298560	even though all the people get to wear hats. So we can change
3298560	3303280	that by taking a hat from a person and inserting it into our
3303280	3306440	GAN's model of what a horse's head should look like. And now
3306440	3311680	horses get to wear hats, right? And so, so let's build a better
3311680	3318000	world. And, and allow people to change the rules of the world by
3318000	3322360	making the rules more visible and and manipulatable by humans.
3322760	3328640	That's that's sort of the goal of the whole thing. So any
3328640	3329960	questions? Any questions?
3330280	3331960	I have a question. Yes.
3332080	3335920	Does this method work with multiple different models? Or is
3335920	3339600	it only successful when like, taking a hat from within this
3339600	3342080	model and put it on a horse?
3342160	3349000	So right now, this, this method is only able to take it, it's
3349240	3353120	it's only able to rewire one model. So I can take one part of a
3353120	3356720	model and rewire it to a different model, you're sort of
3356720	3359200	asking the transplant question. So I'm sort of at the point
3359200	3363520	where, you know, it's like a surgeon, I can like connect one
3363520	3365720	blood vessel to another blood vessel in the same human, right?
3365720	3367480	And you're sort of asking the question, well, can I do a heart
3367480	3370240	transplant? Can I take a heart out of one person put another one?
3370560	3375720	And I cannot do that yet. It turns out to be harder. And, but I
3375760	3380680	but it is a it is an obvious goal. And I, and I feel confident
3380680	3385200	that if we understand well enough, all the things that make
3385200	3388400	these computations work, what is needed for the care and
3388400	3391600	feeding of a computational module? What is a computational
3391600	3395200	module inside a big learning system? Then we should, you
3395240	3398640	know, it should be a goal to be able to move a piece of
3398640	3403160	computation from one neural network to another one. Does that
3403160	3403640	make sense?
3405480	3406040	Yes, thank you.
3406400	3409840	Yep. That's a really great question, by the way. I think
3409840	3415120	it's, I think it's fundamental. Any other questions?
3419280	3424520	This is not too well articulated question. I was just
3424520	3428440	curious what you, what are your thoughts about this? I think
3428440	3434000	this is this like neural nets have tendency to like avoid the
3434000	3436760	responsibility of the results, like everything is done in the
3436800	3439920	hidden layers and sort of shrug off shrug off the
3439920	3443600	responsibility about the results. And I thought it was like
3443600	3448160	interesting how you set the objective towards something as
3448200	3453400	abstract as realistic. And here, like how you define the concept
3453400	3457600	of being realistic is based on the big data you collected from
3457600	3462680	the web, but but oftentimes some like fake images sometimes
3462680	3468080	look even more realistic than real images. And I don't know,
3468080	3471280	like tree growing on top of the building may look fairly
3471280	3476680	realistic for some people, but maybe for plant experts, maybe
3476720	3482480	it would not. Right. So I don't know, like, I think this might
3482640	3487280	result in like the blurring between the it's making us hard
3487280	3490040	to distinguish between the real and the fake or something like
3490040	3490560	that. I don't know.
3491120	3497400	Yes, yes. No, I think that there are so so the we're we're
3497400	3502600	unaccustomed to making it easy for making programs that make
3502600	3506040	such realistic renderings of the world. And it's actually a
3506040	3510680	concern. I think that, you know, people have misused this
3510720	3513720	technology already that we you know, we use we you know, there's
3513720	3518960	the whole deep fakes phenomenon. But even without like faking
3518960	3526040	videos, people people have you know, used face generators to
3526040	3529160	make lots of fake Facebook profiles and things like that,
3529160	3532240	you know, pretending there are millions of people that exist
3532240	3536240	that don't actually exist and things like that. So so even
3536280	3538640	before you sort of do manipulations of the world, I
3538640	3543720	think that there's already this problem of of of, you know,
3543720	3548200	pretending that there's a lot of data that there actually isn't
3548240	3551120	by using these generator models. And so I think that there's
3553640	3558000	you know, the whole the whole question of fakes is a very
3558000	3565160	serious question, like how do we how do we function society if
3565160	3569680	we don't know what's real and what's fake. Now, it's not a
3569680	3575720	totally new issue. You don't need a state of the art deep
3575760	3579760	learning model to make fake, you know, people have made fake
3579760	3583760	photoshopped by hand forever, people write can write text that
3583760	3587640	has all sorts of lies forever. In fact, that's probably more
3587640	3591560	effective than you know, trying to train a deep learning model
3591560	3595040	and, you know, sort of make it work. But I think it's I think
3595040	3597560	it's a, you know, it's still an important question because the
3597560	3602000	easier we make it to make fakes, you start to get issues like a
3602000	3606160	scalable fakes, where it's not just one, one photo that is a lie
3606160	3609920	or one article is lie, you could generate millions. And I think
3609920	3612560	that there are serious issues with that. So there's some pretty
3612560	3617360	interesting work in forensics for detecting fakes, and things
3617360	3621480	like that. That I think is important to invest in as well
3621480	3625240	as as we as we advance the state of the art and this kind of
3625240	3631040	thing. So I so so I don't want to minimize the implications of
3631040	3634120	this type of thing. I think that for the type of work that I'm
3634120	3636520	doing, I think that you observe that the tree kind of looks
3636520	3638960	realistic, it's not super realistic. You know, if you're a
3638960	3645000	plant expert, it's just sort of, you know, sort of there. I
3645040	3647400	think the same thing with hats, they don't really super look
3647400	3650120	like hats. And so I think that we're, we're sort of the stage
3650120	3654000	where they're really exciting where the implication of what
3654000	3660480	I've done here, I think is the idea that the, you know, learning
3660840	3664440	how these models are working inside, by understanding what
3664440	3667720	the internal structure of the models is, is really the, that
3667960	3671880	the the exciting part that that it's starting to give a little
3671880	3677240	insight on how we might untangle and disassemble what the
3677240	3681560	internal logic is, that is being learned by these, these deep
3681560	3686400	networks. And, and I'm actually, I feel like this is, I feel
3686400	3688400	like there's a different issue other than fakes, which is
3688400	3692640	actually has some ethical implications, which is transparency
3692640	3695200	of deep networks. Because one thing that they're not really
3695200	3698680	good at doing is when you have a deep network do something
3698680	3701960	amazing, they're really not good at answering the question, why?
3702320	3705800	Why did you do that? Why did you choose to render it this way?
3705800	3709680	Why did you choose to pick these objects to put in the scene?
3709680	3713560	Or why did you choose to deny me some credit or to, you know, to
3713560	3716920	make some other decision that we were at, you know, depending on
3716920	3720600	neural networks to do. And I think that if we can understand
3721320	3725560	how to disassemble the rules that are being applied inside the
3725560	3728440	network for it to make its decision, then I think that we'll
3728480	3733240	will be, we'll have a way of asking why. And by looking at
3733240	3736320	the computation directly. So that's my, that's one of my
3736320	3738240	goals and one of my hopes in doing this kind of work.
3741720	3746520	Definitely, I can see some of the worst of you about like,
3747960	3751240	about the transparency of the neural network, especially when
3751240	3756120	you, when you show the example where you detected a single
3756120	3760120	neuron that contributes to the watermark thing, I think that
3761360	3762400	it was really interesting.
3763720	3767440	Yeah, I think so too. I was surprised that it worked because
3767440	3770280	we normally think of neural X is very, very, very opaque.
3774160	3779320	I also have a small question regarding artifacts. So I think
3779560	3784160	in the beginning, you talked about how you segmented the
3784240	3790600	network with like masks that were classified before by mapping
3790720	3795720	neurons and beginning layers, which create things. But could
3795760	3800360	like, can that be also used to figure out where artifacts or
3800360	3804400	anomalies are generated to make gains better?
3804960	3810880	Yeah, actually, I don't have a picture of it here. But in my
3810880	3814640	work where I was looking for neurons, originally, it's called
3814920	3817400	the paper is called GAN dissection, you can you can
3817400	3822760	Google for it. And, and I showed that in that paper, we
3822760	3825800	analyze some of the pre trained GANs that came from a previous
3826000	3830280	work from NVIDIA called progressive GAN, we analyzed
3830280	3832880	some of the pre trained models, and we found that they actually
3832880	3836560	are neurons that correlate with bad looking artifacts in a
3836560	3840240	scene. And if you turn those neurons off, you can actually
3840640	3845080	not only improve the quality of the output of the GAN, just
3845080	3848760	qualitatively like you can get these artifacts to not show up
3848760	3853760	but using standard measures of GAN, you know, statistical
3853760	3857720	measures of GAN image fidelity at large scale. By removing these
3857720	3861280	neurons, you can actually improve the what we call the FID
3861280	3865120	scores of these GANs when we tested on like 50,000 images. And
3865120	3869320	so, so that's actually very weird to me, that's, it was a big
3869320	3875760	surprise. Because, because we, we train these things using, you
3875760	3879720	know, powerful optimization techniques, using, you know,
3879920	3882600	billions of floating point operations, you know, training
3882600	3886040	these things on big expensive GPUs for a long period of time. And
3886040	3890000	the idea that a human can come along, and do a simple looking
3890120	3893040	visualization, pick out a few neurons based on things that
3893160	3897120	don't look good. And improve the model by turning those neurons
3897120	3901920	off. It was like it shouldn't be possible, right? If it was so
3901920	3905360	easy to improve the model that way, why couldn't the optimizer
3905920	3910520	find it? And so, so I think that was, that was, that was pretty
3910520	3917240	interesting. I have not repeated that experiment on the latest
3917280	3921280	GANs, which are actually much better the style GANs. To
3921280	3925280	architecture, they went back and they analyzed a bunch of the
3925280	3929920	artifacts that show up in this, this family of GANs. And they,
3929960	3931920	they found that there are certain learning methods that they
3931920	3935400	can do to remove the artifacts or reduce them somewhat. And so I
3935400	3940400	don't know if a human can still beat the current generation of
3940400	3943520	GANs, it'd be worth going back and seeing that phenomenon is
3943520	3944000	still there.
3944880	3947000	That's pretty cool. Thank you. Yep.
3951000	3955000	Okay, excellent. Thank you so much, David. It was really
3955240	3962400	fascinating topic and talk and more interesting to me, asking
3962400	3966600	the right questions, asking questions and learning to ask
3966600	3970160	the right questions. It's really interesting. And I think that
3970280	3973280	it opened paths to many of us.
3974040	3977400	Excellent. Hey, thank you for the opportunity to talk to the
3977400	3983600	group here today. I always enjoy the, the chance to interact
3983600	3987200	with folks about this. If anybody wants to send other questions
3987200	3992360	about it, of course, you can always send me a note. And, you
3992360	3993520	know, I love this stuff.
3993800	3998280	Yeah, definitely. I think that it would be great to follow
3998640	4003960	follow your work on your GitHub and your website, and
4003960	4007160	especially for students who play with the tools that you have,
4007160	4010520	so they have them get an understanding of how these two
4010520	4013360	work and make them curious about the work.
4013880	4014360	Cool.
4015080	4016560	Excellent. Thank you so much.
4017320	4018840	Thank you, Ali. Thank you, everybody.
4019440	4021480	Thank you. Bye now.
