start	end	text
0	9520	Cool. Hello everyone, welcome to your course AI for Art, Esthetic and Creativity. Today
9520	19200	we have a very special speaker. She has an excellent background in different domains and
19200	28880	she will tell you hopefully more about herself and her work. Sarah is a great friend and
29520	38480	colleague of me and she kind of accepted to give us a lecture talk today. So from here I
41040	47840	let Sarah to continue. Please go ahead. Thanks Ali. It's such a pleasure to be here. I've heard
47840	53440	so much about this class. I don't think I have a slide about my background but I can tell you a
53440	59920	little bit about myself. I finished my PhD in neuroscience so across the street from Seasale
59920	65920	last year and now I'm a postdoc in the vision group and the journey throughout my PhD was
65920	72320	a little bit of a winding path. I started thinking about explicit symbolic models for things like
72320	77360	physics and we'll talk a little bit more about that along the way. So modeling how the mind
77360	82480	makes inferences about things that we see but that hits a ceiling when we come up against
83120	89120	questions of vision and types of seeing like looking at art that is really difficult to develop
89120	94880	some kind of computational formalism for that we don't have good models for. And at the same time
94880	99280	as I was kind of hitting that wall in my own thinking I was developing a parallel interest
99840	104640	in visual art and doing a lot of different projects both with individual artists and with
104640	111680	larger museum archives that I'll talk a little bit about and started to look at art as a ground for
112240	116560	asking kind of difficult questions on the frontier of our thinking about the mind.
116560	121600	If we look at how humans create art and in view art can we understand something about
122320	126720	how they view the world and domains that we don't yet have good models of cognition for.
127600	132640	So I kind of started steering my my PhD in that direction. I'll share a little bit of that work
132640	138240	as well and as I said now I'm a postdoc with Antonio and Ellie asked me to share a little bit
138240	145840	of my my inspiration behind that path. I don't have a good story about a specific moment I think
145840	152720	it's been a lifelong interest for me since I was super small and reading a lot of poetry I guess
153360	158080	thinking about kind of the the origin and nature of structure and our experience of the world.
158080	164240	I know that's quite an abstract thing but the structure that we see in visual patterns where
164320	169280	does that come from? Is that something that lives inherently in the brain and we imprint it on to
169280	173680	kind of noisy and unordered stimuli or is it something that's external you know a nature
173680	179600	nurture question and then our brains kind of evolve to reflect and I got interested in this
179600	185920	meeting point this kind of layer between the self and the world where all the action happens so to
185920	194000	speak and had training in applied math before I came to MIT and would think about ways to describe
194000	198720	kind of structured inputs to processing systems and understand something about the structure of
198720	204000	external inputs and then my neuroscience background learned a little bit how to how to
204000	208320	think about and model the structure of a processing system right the structure of different parts of
208320	213840	the brain and it's really been through my interest in visual art that we can start to think about
213840	219680	and describe what happens when those two things meet and how we synthesize our world of visual
219680	225040	experience in domains related to art and then other kind of higher level aspects of cognition
225040	231680	like scenes or associations with moods of scenes and that kind of thing so that's that's where I am
231680	238720	now and I think I'd like to start us off unless anybody has any leading questions about where I
238720	245120	come from with kind of a provocation and you can you can think of this as a frame for what I'll share
245120	251200	today but it's intended to be provocative and so the statement I'll make is that visual perception
251200	257520	itself human perception which we attempt to mirror and model in computer vision and computer science
257520	263280	in some cases that human perception is something that's fundamentally constructive and I say that
263280	269360	because it solves an ill posed inverse problem like ones you've probably heard of before and
269360	276080	doing that doing that solving requires a little bit of creativity so where am I coming from there
276080	282000	the back of your eye as you know is is a 2d flat canvas right made up of a hierarchy of cells
282000	287840	that were visualized in in drawing in art by Ramonica Hall hundreds of years ago and are now
287840	292880	visualized using electromagnetic imaging and we can get actually pretty fine brain detail
292880	299280	of the cells in the back of our eye that constitute a 2d canvas that takes in incoming image data
299280	305760	and represents images in terms of patterns of activations via this kind of mosaic of cells
306800	312960	yet we experience this richly 3d world so there's a setup of a problem that you've probably heard
312960	319440	before right 2d canvas but we have 3d rich experience scenes have depth objects have 3d shape
319440	323280	and furthermore what we see carries lots of different meanings and associations
323920	329920	so where is all of that kind of higher level information in a 2d image classical kind of
329920	336640	computer vision problems we look at this kind of painting by Suzanne you might not only recognize
336640	343520	3d structure of this cottage on the mountain side right even though the image itself is 2d
343520	347920	I might have all sorts of associations with it I might be able to say oh it's spring time
347920	352400	think something about the time of year I might even be able to infer something about the geography
352400	358400	by the palette used to convey what fields might be there think a little bit about the landscape
358400	365040	I might be able to appreciate depth in pictorial space so even on this 2d plane if I put my mouse
365040	371120	up here in the front maybe these fields are closer to me as a viewer than these ones that are far away
371120	375120	but once again I'm just looking at a flat picture where is all of that information
376320	381360	our brain has to solve an inverse problem like this anytime we look at a visual scene it has to get
381360	387280	from low two-dimensional information to kind of rich 3d but there's a fundamental problem here
387280	394480	and I pointed to that is that infinitely many 3d objects can cause the same 2d project projection
394480	399680	that's the under constrained nature of this inverse problem that vision poses and you've
399680	405840	experienced this quite explicitly anytime you've seen a shadow and not the object causing the
405840	410720	shadow right and you've had to infer oh is that actually you know a monster on the wall or is
410720	416720	that somebody's hand being projected but there are infinitely many configurations in three dimensions
416720	421200	that could be projected downwards onto two dimensions and cause some configuration in
421200	427920	pictorial space so how do we constrain that problem when we're solving for what we see
428720	433920	right so this problem is ill-posed because it has as I said many infinitely many possible solutions
434640	440560	and choosing between them requires some additional information and in the case of the brain
441280	446240	modern neuroscience understands this as requiring the brain to construct something so that's what I
446240	452480	mean when I say perception is fundamentally constructive or creative it requires the brain
452480	459120	to construct a best explanation of what it's seeing of incoming information and if we call
459120	464320	that perception then maybe you'll permit me to make a bit of a stretch and say that that makes
464320	471920	perception itself an act of creation or an act of synthesis of a scene so one kind of popular
471920	478160	way to solve this inference problem is by using models of the world right and we can approach
478160	482560	that from a Bayesian lens maybe you've seen the work of Josh Tinnenbaum in the bcs department
484000	488400	or maybe we can do that purely with deep learning it's kind of an attention that we could explore
488960	494800	later today but I'll give you an example here and this is let me back up for a second that if we
494800	499520	were in person this is the point where I would do kind of a live in person demo so I want you to
499520	505040	imagine that we're all sitting kind of in a dark room or we're sitting in a studio space and out
505040	511360	in front of you there is a table covered in black velvet and I've set some stuff on that table you
511360	518080	don't know what it is I set it there when the lights were off and then I take a single line of red
518080	523440	laser light and I'm going to gradually sweep it over the scene so I'm constraining the visual
523440	528960	information you're going to receive about what's out there in the world to something kind of really
528960	534320	low dimensional compared to what you normally get to understand kind of a garden of forms that would
534320	539200	be sitting on the table so imagine you're there in the studio with me and you see the following
539840	544240	you have to kind of infer what you see on the table maybe you could write it in the chat or
544240	550080	just think to yourself when you see this give it a moment what's sitting here on the table
552560	554880	or what kinds of things what different things
557680	564240	maybe this would be a good use of the chat I can pull it up or you can describe features of what you see
574560	581120	a bunch of blocks on the table great there's something cubic up now I see the corner there right
582800	588880	multiple vases multiple forms with kind of different underlying shapes something cylindrical yep
589840	593280	two things do you see I think there's a sphere actually there in the middle
596480	601200	what is the experience of this light do you actually feel a physical corner when you see
602080	604560	bent light round the corner of that cube
608000	609680	oh oh goodness
613200	618240	interacting with chat is a bit tough all right anyway the point I want to make here is that
618240	625360	I can present really kind of low level information and you can if I dare open open the chat up again
625920	629680	yeah there's a single base there's a single table and many forms sitting on top
629760	634240	that's right so there's just a tabletop and then lots of different shapes also covered in black
634240	640080	velvet so the light doesn't scatter and the light traces the outline of these 3d shapes
640640	646560	and you can appreciate something about what the shapes are just by watching how light bends
646560	652560	around their surface and the relative motion as it traverses that facade right as it moves
652560	658000	over the surface of the sphere the light bends according to its curvature and I would argue
658000	665760	here that because you have some notion of what a sphere is and some notion of what a cube is
665760	672560	that is you have a relatively abstract model of these underlying shapes in your mind a mental
672560	678240	model you can do some inference when you see light move over their surface on this way even though
679120	684000	I choose an example like this because you've probably never seen this example before right
684800	688320	never seen a single line of laser light move over this table surface
688960	693760	even this this kind of setup but you can still do that inference pretty well and you did in the chat
695120	700400	so if you'll if you'll stay with me here I'm suggesting that this is an example just your
700400	706400	perceptually of how we can bring kind of models of the world and shapes and forms that comprise
706400	712880	it to bear on simple visual stimuli and how we can even do that by using articulated light
713440	719280	to isolate aspects of those stimuli and to kind of elucidate our perception to us
720160	724560	so we do a lot of things like this in the MIT museum studio where I teach the vision and art
724560	729280	neuroscience class which I'll talk about in a moment but that's where this this was filmed
729280	734080	let's see if it'll let me advance even though I open the chat all right so another kind of setting
734080	739680	in which we often hear and think about models of the world and this kind of inference is in
739680	745600	intuitive physics and I bring this up because some of my background is also in this type of work
745600	751680	investigating how the brain represents physical properties like mass that it uses to reason
751680	756720	physically about the world right you would have to estimate the mass of this block that's falling
756720	761520	and making a depression on this pillow before you would know the right amount of grip force
761520	765840	you would need to use to to reach in and pick it up without dropping it right and this is something
765840	771360	we do incredibly automatically and it's a skill set we develop regularly from a very early age
771920	779440	and I found that the brain represents properties like mass with an amount of abstraction and invariance
779440	786160	to the type of physical scene in which mass is revealed that would be necessary if mass like
786160	792640	this were to be used as an input to an abstract generalized engine for physical simulation or
792640	798720	what we call a physics engine in computer graphics and simulation suggesting that there is
798720	805680	kind of some first evidence that the brain does use these kind of generalized simulation engines
805680	811520	to solve low-level inference problems like inferring mass because we can make some hypothesis about
811520	816320	the nature of the underlying representations it would need if it were to solve problems in this
816320	820880	kind of way rather than by simple pattern matching or in a pixel based way where we would assume
820880	825760	that the representation of mass would be quite different from scene to scene because the low
825760	831200	level visual data about the scene is different but in fact that's not what we find we find representations
831200	835840	of physical variables like mass and friction that generalize across any kind of physical scene
836400	841120	that we test where we hold a lot of other different parameters constant right like object color
842160	846880	and this suggests an account of physical reasoning in the brain that has been that has been studied
846880	853040	pretty extensively computational right and that we model via probabilistic simulations of a physics
853040	857840	engine I don't think that video is going to play for us right but this is the kind of work when I
857840	864240	was doing when that I was doing when I was writing down like explicit models of the world that could
864240	869280	be inverted to explain something about underlying parameters we were using for vision and in that
869280	876720	in this case those models were physical right but what about cases like like art where it's
876800	882320	difficult as I mentioned to develop some kind of computational formalism where we don't know the
882320	887520	underlying model for instance how to create the Cezanne painting we saw in the beginning
888240	893280	a priori right how do we even start what are the underlying dimensions we'd need to write down to
893280	901120	either make sense of how we see things or how they're created so this whole area is kind of what we
901120	905680	dive into in that vision in art and neuroscience course so this is something you're interested in
905760	910400	it's of course an unsolved problem but we spend the fall semester every year
911440	916480	kind of delving into it through both neuroscience literature through art practice through computation
916480	922480	and then through studio work so kind of hands-on experimentation with principles underlying vision
922480	927520	that we then externalize and experience ourselves and try and visualize in artistic contexts
929440	932800	to give you a little bit of a taste of that class we would look at
933360	938560	these examples say by by an artist in minor white and ask if we were trying to set up
939360	945520	a typical kind of describe a model and then invert it to understand vision setting you know what is
945520	951680	the veretical percept in either of these right if before we were considering mass of some object
951680	958000	that the brain has to infer and then we can write down a physical law describing how mass plays into
958080	962720	action unfolding in a scene a law describing dynamics and then invert it to think about
962720	967120	how the brain represents mass what would the analog be here what would we write down as the
967120	972320	veretical percept you can share some thoughts in the chat that is also an exercise you could just
972320	978560	do yourself right maybe here you can start to get it a shadow of something outside the window
979120	984960	I see a bike maybe a bike seat there that's kind of not the point kind of not trying to infer what
984960	991040	caused the specific physics of this this image you're kind of getting at something different
991040	997200	and especially here what if the artist isn't around for us to ask anymore these are actual
997200	1003440	photographs right these are photographs of something but the act of looking at it isn't about
1003440	1008560	inferring the underlying cause of the image it's about inferring something else sort of aesthetic
1008560	1013920	parameters that define visual experience or kind of render visual experience at a lot higher of a
1013920	1020000	level how do we begin to get traction on problems like this either in seeing or in or in generation
1020800	1026000	as I said you know in art we also come up against a great difficulty in that you know there are
1026000	1032320	infinitely many ways to render recognizable depictions of common objects right with all
1032320	1038160	sorts of idiosyncrasies illusory boundaries difficult for models to detect but we recognize
1038160	1044800	a woman in these images with the dress almost instantaneously and similarly we come up against
1044800	1050560	another under constrained inverse problem is in that there's infinitely many ways to render and
1050560	1057280	depict kind of abstractions of commonly recognizable forms which again are difficult for
1057280	1062880	current day models but they're pretty easy for us I can recognize a figure and maybe have different
1062880	1069040	associations with it in each of these different images so we think a little bit about this
1069760	1075280	in the course like I mentioned you can ask me a bit after this talk as well if you're if you're
1075280	1081520	interested in it it's called vision and art and neuroscience all of our info is is online most
1081520	1088960	of the syllabus past exhibition catalogs at vision.mit.edu it's offered through through bcs
1089840	1095200	and as I said we we investigate during half the class in the seminar portion of the class
1095760	1099520	kind of the underlying principles of vision and we work through a series of modules
1100320	1107280	that build up visual processes from early level like v1 visual processing all the way up to kind
1107280	1113840	of more rich images and we do this in parallel in a studio section during the other portion of the
1113840	1119360	class where we're translating these principles of vision into the studio and building artistic
1119360	1125200	contexts where we can kind of become aware of our own perceptual processing at work so examples like
1125200	1129120	the one I showed you at the beginning right with the with the laser line moving over
1129120	1134400	that garden of objects are examples of settings that can allow us to maybe perceive our own
1134400	1139840	perception at work right or shed some light on what's going on when we look at at normal scenes
1139840	1144560	right there's all these unconscious inference processes happening even when we look at corners
1144560	1148800	in a room but we're not aware of them and so we ask here if we can create settings
1149520	1155600	where we do become intensely aware of them and that awareness becomes kind of the art experience
1155600	1160960	right and so it's the art of perceiving one's own perceptual processes at work and then over
1160960	1166160	the course of the class everybody develops an individual artwork for exhibition which is super
1166160	1172560	lovely and it's it's an opportunity that we don't often have in other classes at MIT so we run this
1172560	1178880	for five years now had five different exhibitions and COVID be it a virtual exhibition and then
1178880	1186080	this year's just opened in December and is actually still up in the MIT museum studio just off of lobby
1186080	1192000	10 10 150 if anybody is on campus and wants to go check it out it's most it's open most days when
1192240	1199920	staff are there but this course is the parallel to your IAP class that thinks about things more
1199920	1205040	in the language of computational neuroscience than deep learning and in some aspects of the
1205040	1211520	course will present deep learning or deep generative models as contexts for probing representations
1211520	1216720	that might be shared by human minds and machines and we'll look at that a little bit later in this
1216800	1222080	lecture but think more traditional computational neuroscience lectures readings visual art and
1222080	1226880	then a studio component where you experiment with some of the stuff hands on so that's what we do
1226880	1233600	envision art neuroscience we start to to probe at the richness of this art neuro and machine
1233600	1237520	learning intersection there's a lot of different things we can do there and for the rest of this
1237520	1242400	talk we're going to highlight a number of different projects that approach that intersection in
1242400	1247280	different ways and highlight kind of different ways that you could think about engaging this
1247280	1252160	material in these questions data sets and resources that we have available and kind of
1252160	1258480	different ways of carving up the problem into bits so we'll start by thinking about modeling
1258480	1264720	kind of the structure underlying human creativity at scale without trying to prespecify
1265520	1271360	laws that you would write down for say a physics engine right can we use deep generative models
1271360	1278080	to kind of approximate or appreciate or grok the structure underlying large data sets of human
1278080	1284480	cultural artifacts and then use those models to experiment with cultural history on kind of a
1284480	1289600	timeline that allows rapid evolution in the present so I'm speaking specifically about a project
1289600	1293760	that I don't know if some of you have seen and I know Ali has seen a collaboration that I led with
1293760	1299280	the Met a couple of years ago again it was fun that we were in person because we were able to
1299280	1304800	actually go to the Met and see a lot of these objects but back in 2017 the Metropolitan Museum of
1304800	1311600	Art was the first or one of the very first to release an open access catalog of a few hundred
1311600	1317520	thousand digital images of works in the Met collection and released them into the public domain
1317520	1323600	which is wonderful for for us as computer scientists and programmers and people interested in ML and
1323600	1328960	art because what a rich data set that is right what a rich data set all in one place
1328960	1333840	don't get me started on the issues with museum APIs but a lot of museums have followed suit in
1333840	1339440	releasing their digital collections into the public domain so they're free and open for experimentation
1340240	1346000	they approached us at MIT and open learning and a couple of programmers at Microsoft and asked if
1346000	1352080	we might want to do a series of projects with this digital collection and so we did and we asked
1352080	1357600	whether we can build deep generative models associated with archives like this of created
1357600	1362800	work that are embedded in their cultural context which might ask which might allow us to ask like
1362800	1369040	slightly more specific questions art historically than just you know what if you train StyleGAN on
1369040	1374000	all of wiki art all at once right not conditionally so we're not appreciating any categorical
1374000	1379520	differences between images but if we just showed it all of wiki art okay here we want to ask something
1379520	1384880	a little more fine grained can we notice you know differences in the development of feature languages
1384880	1392720	between maybe time periods or geographical regions right and can we develop ways of collaborating
1392720	1399040	with those models to iterate archives forward so experimenting with chimeras between existing
1399040	1404480	works and developing new works right that might sit somewhere between works that are already on a graph
1406080	1411840	so one of the challenges that we faced here initially was that the data set was pretty big
1411840	1417040	400,000 images but each individual category in that data set was not some might only have
1417040	1422800	a couple hundred images and there's a lot of sketches and drawings and kind of uncategorized
1422800	1429120	work too that makes up that 400,000 so you're in a situation where in theory you have a rich
1429120	1435120	labeled data set but in practice it might be quite difficult to train anything that looks
1435120	1439600	photorealistic or gives a good sense of any individual category of work because the categories
1439600	1446480	themselves are not that large so at that point this was pre like style again too we started working
1446480	1454800	on this in 2017-2018 um I asked whether we could instead of training a single model on say a subset
1454800	1460960	of this met collection like this category of vases called yours whether we could find corresponding
1460960	1467680	subspaces of what we're now referring to as foundation models like big an image net that kind
1467760	1473760	of approximate our data set right so if we think about foundation models as a shared resource that
1473760	1478880	ideally everybody would have access to and there were ways to think about contributing to then maybe
1478880	1485680	these smaller problems become or can become a way of defining subspaces of those big models that we
1485680	1491520	can interact with right rather than having to retrain a model and on our data set so we used
1491520	1496240	GAN inversion here and instead of training a new model on just this category of viewers
1496800	1502160	we asked whether we could embed each image that already existed into in the met collection
1502160	1508560	and into the feature space of big GAN image net which happens to have a category for vases so we
1508560	1513680	selected categories that were shared between image net and the met collection there are a handful
1513680	1519840	about a dozen um and we maximized for each of those images the similarity between the met image
1519840	1524400	and the big GAN image using a two-part loss right so we wanted them to be similar both at the pixel
1524400	1530240	level and at the semantic level and we did that by looking at two different layers of a pre-trained
1530240	1536000	res net as the embedding network so once we've embedded these models these images into big GAN
1536000	1540720	we can then visualize the individual embeddings but we can also do something a little bit more
1540720	1546640	interesting than just look at approximations of these images which might not be very good we can
1546640	1550800	think about the underlying feature language that might have been learned and then look at
1550800	1556720	interpolations between the existing images in the met collection I hear murmurs in the
1556720	1562640	background if anybody has a question hit the chat you're super welcome to speak up um so next we
1562640	1568160	look at interpolations between these existing images on the graph and we can create kind of
1568160	1575040	hypothetical or dreamlike images that exist between the spaces of existing works in the collection
1575040	1581360	and these are pretty interesting and beautiful and they allow us as I was mentioning to ask
1581360	1585920	questions about what collaborations between geographical regions might have looked like
1585920	1590720	right because we do have categorical information about where each image in the met collection
1590720	1596800	came from it allows us to suggest new objects and the spaces between them so it allows us to
1596800	1602400	interpolate and the other beauty of these kinds of executable models of culture is that it allows
1602480	1608320	us to iterate on existing collections really rapidly um and evolve them forward and so we
1608320	1613280	can kind of start to imagine archives of the future that would have embedded within them
1613840	1619280	world models corresponding to the data set that exists at one point in the archive right so the
1619280	1624640	archives could kind of evolve themselves forward and suggest future versions of their collections
1624640	1630560	based on what's already been created and that this is again this was back in 2019 which is a
1630560	1636240	long time ago in computer vision terms um but even just with with inversion into to began in the
1636240	1641040	channel I was really impressed at the quality of the the images and the hypothetical objects that
1641040	1646160	we could get for example here are a bunch of different generated teapots from the met latent
1646160	1651760	space in the teapot category which again happened to be shared between ImageNet at that point um
1651760	1658960	and the met collection and as I said we did have the the opportunity to exhibit this in the met
1658960	1665760	which was absolutely wonderful um we projected a visualization of this latent space superimposed
1665760	1671360	on a map of the met collection and allowed people visitors to the to the great hall to kind of step
1671360	1677360	in to this latent space as projected onto the ground and explore the traversal of the spaces
1677360	1686320	between works um and a projection behind them on the wall. We also made a web app version of all
1686400	1692720	this that exists even though we we can't visit the met today um it's online at gen.studio if you
1692720	1697680	want to go have a look after this and then all of the the code base is linked the github is linked
1697680	1703600	at the bottom um if you want to check out any of that more specifically but again it places us
1703600	1708960	kind of a different framing of latent space traversal than we're used to that I was interested
1708960	1713840	in this project was to place us on you know we've gotten a lot further along in the video um
1713840	1719200	you can go look at the at the website places us on a map between the objects when we're doing
1719200	1724400	the interpolations right so we select an object to start now we land in the latent space of big
1724400	1730400	gen close to that object and then we can move ourselves around on the map between objects and
1730400	1736560	their embeddings in that latent space right and as we're moving physically in 2d space here online
1736560	1742560	we can visualize what exists at that point in latent space and then we can find its nearest
1742560	1748640	neighbor visually uh in the met collection and find what object in the existing collection
1748640	1753360	is most similar to the hypothetical work that we discovered in the interstices between two
1753360	1760400	existing works um so give that a look and this project lives on today um and a couple of different
1760400	1765760	forms I'm still working with the artist Matthew Richie he was a collaborator uh with us on the
1765760	1774160	met project um on a couple of different tendrils of of this work where we're asking all right so
1774880	1779520	we can model projections of existing images in the met in the met collection by finding their
1779520	1787280	embeddings in some kind of large foundation model but now in 2021 we have things like StyleGAN ADA
1787280	1791760	that can can train on smaller data sets and do reasonably well in approximating data sets that
1791760	1796960	would correspond to a single category in the met collection so we've done that um we've trained
1796960	1804400	these models on sketches um Babylonian cuneiform tablets Japanese watercolors and some 18th century
1804400	1811520	European landscapes among other things and have individual models correspond to each of these
1811520	1816880	genres within the met collection and then we've been working with a friend in New York who has a
1816880	1823440	robotic oil painter and can actually create layered paintings of really short walks in latent
1823440	1829520	space along different dimensions in this model so think about physically visualizing some of the
1829520	1834480	durability work you've looked at in this course right could we make time paintings of really
1834480	1841920	short walks in latent space by superimposing robotic paintings of the visualized image kind
1841920	1847360	of at different points along that walk so that's that's being exhibited right now at UNT
1847360	1850880	in their contemporary art gallery let's see I've got a question in the chat room
1853200	1859040	oh it's just a compliment I will take it at any point yeah I think
1859040	1865120	can you please read it yes uh someone mentioned that this is a creative reason one of the most
1865120	1870880	creative reasons they've seen to do latent space interpolation since it scans yeah I think that
1870880	1876160	I had a slide a moment ago if you want to rewind in the recording of this suggesting that kind of
1876160	1881840	part of the advent of using GANs to model kind of large databases of creative work is that they
1881840	1887360	allow us to do a couple of things right that interpolation and that iteration and in cases
1887360	1891760	where you can't write down a feature language underlying a set of works because you don't
1891760	1898160	know our priority what it is you can imprint that or you can learn something of that in a deep
1898160	1905360	generative model right and then you can collaborate with that and hypothesize what might lie on a
1905360	1912080	graph of human creation if we presume that any creation artistic creation at some point in
1912080	1919920	historic time is if you think about it as the manifestation of a point on kind of a sea of
1919920	1924960	cultural influences and multi-generational practice iterative practice that's been shared
1925040	1931040	between peoples and generations and the creation of a single work is the enactment of that process
1931040	1937360	at some point in space it's natural to think of that in some sense as a model that we can capture
1937360	1942960	in a latent space where we're manifesting some part of structured space at some moment but this
1942960	1949280	allows us to iterate on that which I argue is similar to some historic processes of iteration
1949280	1956400	and collaboration across groups of people really quickly right um so I've been trying to take this
1956400	1963600	a little further now and ask all right we can make paintings of short walks in latent space we can
1963600	1968720	hypothesize objects that might have existed but we don't think they ever did but we still don't
1968720	1975760	know much about these models even if we train StyleGAN 2 on a set of 2,000 paintings in the
1975760	1981200	net collection uh you've probably seen some of the interpretability work adjacent to what Ali
1981200	1986560	has shared or David Bao's work so that style of thinking we don't know anything about this Japanese
1986560	1991680	watercolor model like what do its individual neurons represent is there a neuron for trees
1991680	1997840	well what is a tree here it's some brushstrokes we recognize as a tree but it's not something that
1998640	2004480	BigGAN trained on image that would necessarily recognize as a tree maybe more simply kind of in
2004480	2010560	the in the steerability context what do dimensions in the latent space of a model like this
2010560	2018400	correspond to right sure we can find things like zoom and 3d rotation because we can name those
2018400	2023120	transformations and then find directions that maximally correspond to them using that kind
2023120	2027440	of steerability technique there are all sorts of other directions like the ones we're visualizing
2027440	2033200	here that certainly have some affective meaning to the viewer that we don't know what they are in
2033200	2038720	the models terms or in the viewer's terms so at this point in this project we're thinking about
2038720	2044320	starting to name and understand dimensions underlying generative models trained on
2044320	2049200	bodies of artistic work from museum digital collections not only limited to the met but
2049200	2055600	around the world uh and our motivation here is to kind of create these alternate and imaginary
2055600	2060800	histories of art built from unique latent walks that we can visualize in real time with this painting
2060800	2067040	or computationally and then maybe understand something about aspects of picture language
2067040	2073600	that might be shared across you know vastly different genres so Babylonian cuneiform tablets
2073600	2080240	transformed from numeric to symbolic and image-based at a very particular point in history and can we
2080240	2086880	find a dimension in style GAN trained on a very different genre of art that corresponds to a
2086960	2092160	similar kind of transformation and as such can we build up kind of a picture language that would
2092160	2098240	correspond to diverse forms of art making right that you might not see in any of these different
2098240	2103440	categories of digital images on an archive but we might start to appreciate once we can investigate
2103440	2112560	them by training deep generative models on them let's get back to great okay so when we're thinking
2112640	2117680	about this intersection we've seen one example of modeling the structure underlying creativity at
2117680	2123760	scale and i've done other projects and you can find many examples online both of my work and
2123760	2128720	other peoples of trying to do this not for creativity at scale but for individual instances
2128720	2135680	of individual artists and modeling either the style or the processes of individual art making
2135680	2142400	techniques so all of these are kind of flavors of starting to imprint or grok or understand the
2142400	2147680	structure underlying creativity but not symbolically right so we don't we don't know how to interpret
2147680	2152480	these models even though we can visualize them and create really interesting hypothetical objects
2152480	2157680	that might be indistinguishable either from existing work or from one artist's particular style
2159120	2164400	we can also think about these models as a tool themselves for collaboration both in their creation
2164400	2170000	and iteration with others who contribute to their models and with the models themselves
2170000	2175440	which as i described represent kind of executable versions of collective cultural structure we
2175440	2181200	permit permit ourselves to think about them that way or facets of kind of a global creative identity
2182240	2186800	but as i mentioned now we're at a point with tools and computer vision where we can start to ask
2187600	2193840	what rep representations actually underlie these models trained on artworks that are themselves
2193920	2197760	executable versions of some collective cultural structure right well what is the structure what's
2197760	2204480	going on under the hood do they correspond to dimensions that we find meaningful when we look
2204480	2210240	at visual scenes and so in the next part of the talk i'll share a couple maybe more technical
2210240	2216000	projects that explore specific ways that humans can interact with generative models
2216640	2221280	in order to maybe learn something about human vision as well right so can we build
2221760	2227440	shared vocabularies that help us interpret dimensions underlying these models by designing
2227440	2233120	experiments that allow us to visualize and interact with images and latent walks like you've been
2233120	2239040	seeing i'll pause here because i need a sip of water and i'll keep an eye on the chat in case
2239040	2248480	anyone has any questions before we go on
2253760	2259440	all right looks like we are question free so far five more seconds
2262880	2268160	i guess i have a question yeah so this might be talked about later but i was wondering a little
2268240	2274480	bit about like in your research and kind of this field how much of like human interaction is like a
2274480	2279760	big part of it and kind of like the human coming in and saying uh how they think about something
2279760	2284800	and see where that agrees with the computer or like kind of like where that role is played
2286240	2289920	wonderful question so these kind these kinds of high-level questions that
2289920	2294880	get it some experiential component or design component of the worker i think really useful
2294880	2299440	ask more of them i'll tell you for different projects what that looks like and in the next
2299440	2305440	section of work it's going to be really obvious because there's human annotations but for this
2305440	2311920	project so the human would come in here you know we train models on datasets of art selected from
2311920	2316800	the met collection and these are small and these are subsets and they were gathered by
2317520	2322960	matthew richie and myself going through different genres in the digital collection of the met online
2323920	2329520	and like hand selecting images from those different genres right representative images of
2329520	2336160	different categories of work or maybe in a less fine grained way all images under some designation
2336160	2342320	so japanese watercolors between the 17th and 19th centuries so we made that selection and
2342320	2347760	tried training these models on a bunch of different such selections and decided which ended up you
2347760	2353040	know with so few examples providing at least a representative sample of the kind of work that
2353040	2360960	we know we saw there right and then here the selection of like walks through latent space
2360960	2363920	so think of those in the same way you've been thinking about the steerability walks
2364800	2369840	they were very arbitrary so that was a completely human selected so it's a kind of a different
2369840	2376400	approach to interpretability where it's steered by the human eye right we're not doing it automatically
2376400	2382400	and we're not doing symbolic it symbolically we don't know what these correspond to but that's
2383440	2387600	trying some arbitrary walk through latent space trying many of them and then the human then
2387600	2392640	selecting what to them felt like an artistic expression this is an art exhibit and then
2392640	2398080	in the next step we'll ask how can we do that in a more systematic way and start to build
2398080	2402480	a language corresponding to what those different walks could be a language that's shared by humans
2402480	2408400	and that takes at least right now a lot of human a lot of human interaction
2409200	2412240	you could think about ways to automate that we'll talk about that in a second
2413440	2418960	but with any kind of human interaction it's nice to preserve the opportunity for direct
2418960	2423680	engagement with models rather than intermediation by a captioner or something like that because
2423680	2428880	then you could imagine using your technique on different subsets of humans right on different
2428880	2433840	kinds of experiences so you might imagine getting an art historian to label and select
2433840	2438000	different walks through latent space here corresponding to very nuanced changes in the
2438000	2443280	development of Babylonian like cuneiform tablets right that a captioner couldn't recognize I
2443280	2448320	couldn't recognize so you might want to be able to pull different kinds of humans into the loop
2448320	2453680	at different times to engage in ways that kind of use their knowledge to create a unique synthesis
2453680	2459920	with a generative model so that's what engagement looked like here and then with this next project
2459920	2466560	it'll be super it'll be super clear and I'll make sure to speak specifically to that so thank you
2466560	2474880	yeah cool so next this is probably a summary of what you've seen so far in your IAP course so
2474880	2480160	there's a lot of different work on discovery of interpretable directions in the latent space
2480160	2485120	of different generative models right and we can steer images along those dimensions to create
2485120	2490320	interpretable transformations that allow us to interact creatively with deep generative models
2490320	2498240	right here we are deep learning for creativity but a lot of these examples presume what concepts
2498240	2503120	we're searching for in the latent space and in fact they do that really explicitly right we
2503120	2508880	will pre-define a zoom transformation and then maximize the similarity between some transformation
2508880	2514000	in the latent space and a zoom transformation as applied to some image maybe you've experimented
2514000	2520000	with code for doing that but that presumes we know we're looking for zoom in the first place
2520000	2524400	what if we find ourselves looking out into more you know uncharted waters so to speak
2525360	2531040	here we ask how we can learn kind of a vocabulary of visual concepts maybe one that you would apply
2531040	2536160	to those style gains we just saw train on the net images right we don't maybe we could look for zoom
2536160	2539920	but maybe there's all sorts of more interesting transformations we could do to those images
2539920	2544240	but we don't know what they are yet how can we learn a vocabulary of visual concepts rather
2544240	2550000	than pre-define them or labeling them after the fact so there are a variety now of unsupervised
2550000	2556240	methods for distilling these kinds of transformations in latent space that find principal components
2556240	2560240	of feature space of different layers the models activation maybe you've played around with methods
2560240	2565600	like GAN space that search for and rank where the largest principal components of the feature space
2565600	2570240	which do provide us with interpretable transformations but they're labeled after the
2570240	2576480	fact so we don't know if they're meaningful to humans kind of in their genesis but we can we
2576480	2580960	can describe them right by providing labels to them another point where the human kind of comes
2580960	2587520	in the loop but we want to see if we can build in human vision to the discovery process right so
2587520	2593520	to supervise it but to not pre-commit to what kinds of concepts we're searching for so in this
2593520	2598960	project we're trying to build or define a method for building a visual concept vocabulary for an
2598960	2605360	arbitrary GAN latent space so to put it more specifically we want to learn embeddings d maybe
2605360	2611200	you've called this w we want to learn some kind of walk in the latent space z if again we'll focus
2611200	2617840	on big GAN here of transformations that are salient to us in visual space and we can't define
2617840	2624320	an objective and optimize our d our walk to produce a transform in x in the image because we
2624320	2628560	want to learn the vocabulary concepts rather than pre-commit to them and we would have to pre-commit
2628560	2633360	to what that objective is right in order to optimize d so we're going to take a different
2633360	2641280	approach and instead sample the space of salient or possible transformations for some given point
2641280	2647680	in space for some given z and then use those sample directions as a screen so to speak onto
2647680	2652640	which we can project human perceptual judgments so that's a little bit of a gratuitous metaphor but
2652640	2657200	maybe a useful way of thinking about it and then we'll we'll disentangle the concepts that are
2657200	2662960	projected onto that screen into a vocabulary of open-ended compositional visual concepts
2664560	2670960	and what we're interested in here is the overlap between what's represented inside a model
2670960	2676240	so some deep features in a model's representation and concepts meaningful to humans in visual
2676240	2681280	seeing understanding we're asking how we might start to define although not completely but
2681280	2687200	start to define a shared vocabulary between the two or for a given model determine what lies in
2687200	2693120	that set overlap and I don't have to dwell too long on a lot of the specifics here it's all
2694160	2699120	online at that URL if you want to read the paper but as I mentioned the first thing we're going to
2699120	2705360	do is generate a set of sample images that produce minimal meaningful transformations in images
2706000	2710000	and then humans come in the loop again we're going to ask them to label them but here we're
2710000	2715280	forming the basis for the data set that we'll build our vocabulary off of and we want to keep in
2715280	2720720	mind that we want a vocabulary in the end that is both diverse so corresponding to a lot of
2720720	2725680	different changes that you can produce in an image and specific where a single transformation
2725680	2732320	corresponds quite reliably to one visual change across viewers so we do that by defining
2733120	2739120	mutually orthogonal what we call layer selective directions and these minimize change in the feature
2739120	2745200	representation at some layer of big care and at some layer we'll call it layer l and this allows
2745200	2751120	us to capture relatively focused changes because we hold constant how much the representation
2751120	2756160	can change at some layer and we do that for different layers to capture changes at different
2756160	2761760	levels of abstraction so as you can see layers closer to the image output control or fine grained
2761760	2766480	aspects of the image like the color of the walls and the bedspread and as we get closer
2766480	2771280	back to the latent space we're allowed to make kind of more higher level changes in things like
2771280	2777760	zoom and perspective of the scene and its composition so what objects are present so here we have a
2777760	2782800	base set of minimal meaningful transformations that capture changes in images at different levels
2782800	2789280	of abstraction we're going to ask people to label them because we don't know what's going on visually
2789280	2794560	in these scenes right so we started at a pretty small scale with just four categories in the
2794560	2800560	places data set and looked at big and trained on image net and places we'll just talk about places
2800560	2806880	here and visualized a handful a few thousand of these directions per category so in each of four
2806880	2812720	categories looked at cottages medinas so uh street marketplaces kitchens and lakes a mix of
2812720	2818560	indoor and outdoor scenes and then asked people to just simply describe the overall transition
2818560	2823920	that they saw when these directions were applied to different randomly sampled starting points in
2823920	2829760	the latent space right so one direction might take this cottage to this snowy cottage and change
2829760	2835280	something about the sky and change the snow so these these changes are still complex we can
2835280	2840560	recognize that it's the same scene and we can describe in simple language what's going on
2840560	2844720	but they're they're not disentangled yet right one direction might correspond to a number of
2844720	2850240	different visual changes so we did a little preprocessing to capture you know what kinds of
2850240	2857280	concepts are associated with each transformation and then we decomposed those annotated directions
2857280	2862480	into a visual concept vocabulary consisting of single directions labeled with single words
2862560	2868720	we formulated that as a linear regression and then solved for the embeddings of individual
2868720	2873200	concepts in the latent space of our begin and then we can basically read those off
2874560	2879840	of our matrix e and then transform the images by manipulating them some amount along those
2879840	2884560	visual concept directions happy to talk more details about that if anybody's specifically
2884560	2891120	interested or you can check out the paper itself we found over 2000 concepts this way
2891120	2894960	corresponding to lots of different types of visual changes so we can reproduce
2894960	2901760	transformations like zoom and rotation things like color but we also get kind of a unique
2901760	2907520	set of concepts corresponding to aspects of scenes like their mood for instance there's a
2907520	2914080	direction in latent space of big and that makes outdoor marketplaces more festive and here we
2914080	2918160	see applying that direction to an example marketplace and it rolls out a red carpet
2918160	2925600	hangs some flags and brings a lot of people into that market we can visualize kind of a
2925600	2931440	a sampling of these directions each applied to two different images in different categories
2931440	2937920	so some directions make cottages more manicured add arches to marketplaces add shadows or make the
2937920	2943200	whole scene blue and we see directions like this that generalize across all of the categories
2943200	2949600	it began that we looked at you can check out the lake category we can add sunsets but also do
2949600	2955440	kind of scene specific things like add reflections to water or make a lake scene foggier make a
2955440	2961840	kitchen more inviting or more modern and again we didn't have to pre-specify what exactly modernity
2961840	2968000	would entail when applied to a kitchen we learned that through sampling what humans associate
2968000	2973840	with a transformation that was sampled randomly right uh the humans labeled that as modern and
2973840	2979520	then we disentangled the specific direction in latent space corresponding to that single concept
2979520	2986240	word and once it's isolated we can apply a modern transformation and know that it corresponds to
2986240	2993360	what viewers found to represent modernity in a kitchen well I said we know that it corresponds to
2993920	2998800	what viewers see as more modern but we don't know that for sure right we still need to ask
2998800	3004800	questions like how generalizable are these directions do they compose right can we add
3004800	3010400	a festive direction to eerie and get something that's both scary and festive right or could we make
3011040	3016000	a kitchen both more modern and inviting so we asked those questions in a series of behavioral
3016000	3021680	experiments that I left for you to check out in the paper itself so we won't in the interest of
3021680	3026960	time go through those here but we do find that these directions are composable and they're generalizable
3026960	3032800	across categories so there are some cases where we can even add a concept that was learned in a
3032800	3038640	single category to a different category for instance making a cottage more festive right or
3038640	3044000	adding snow to a marketplace even though that's not traditionally seen there we ran a set of
3044000	3049120	behavioral experiments evaluating the extent to which this is successful and isolating a couple
3049200	3056560	of few specific cases where it fails okay so this this wraps up this method it's at a point
3056560	3062160	now where we're trying this with some of the the art models that I discussed previously right so
3062160	3068480	this was still just applied to big and trained on real-world images trained on ImageNet but you
3068480	3074240	can imagine using a similar similar method to find dimensions of visual interest that are also
3074320	3081280	meaningful to humans in the latent space of a model trained on art images and so decompose
3081280	3086720	future languages underlying different genres of art into something describable so that we
3086720	3092160	can make concerted manipulations to images sampled either from foundation models that
3092160	3098080	correspond to approximations of real-world images or to models trained on on archives of art images
3098080	3105200	themselves I'll pause here for any questions about this there's associated code also available
3105200	3113120	on that project page I linked have a quick question please um was there a reason you chose
3114240	3121440	big GAN over starting with like style GAN for this type of work uh no um just a couple of
3121440	3126080	different code bases that already existed um and people that had worked on big GAN for
3126080	3130160	like GAN dissection we had an easy way to dissect big GAN and hypothesize what kind of
3130160	3134560	things might be there uh so a lot of the GAN dissection work started with big GAN and so I
3134560	3139360	was picking up where that left off and asking if we could find like style vectors that corresponded
3139360	3145360	to scene level transformations instead of individual neurons um but I have extended this
3145360	3151440	to style GAN outside of the paper it's just not got it it's here yeah the method's pretty I mean
3151440	3156320	there are a couple of different small changes you have to make um but the method is pretty model
3156320	3162000	agnostic just like defining a set of certain directions that samples the latent space in
3162000	3167360	kind of minimal ways and the the method I described here is definitely not the only one you could use
3167360	3171440	for that right um you could sample them by just finding the principal components of the feature
3171440	3175440	space or you could sample them randomly right you could just find two points in latent space
3175440	3180320	interpolate and then get people to label what's going on there um we tried a lot of these different
3180320	3186000	methods uh and found that if you if you make random interpolations between two randomly
3186000	3190960	sampled points then there's just so much going on in the scene that there's not a lot of inter
3190960	3195440	observer agreement in how people annotate what they see there's just too much going on so we
3195440	3200800	need to isolate specific changes that's why we developed that layer selective method for isolating
3201520	3206880	minimal changes um but what if we used kind of the the principal component method right or used
3206880	3212400	something like GAN space um there we found that the principal components of the model's feature
3212400	3217520	space aren't necessarily the most interesting to humans so we might get a ton of different types of
3217520	3222320	rotating the scene but not a lot of different changes of mood or changes in color up there in
3222320	3227600	high-ranked principal components um so that's where that method came from but it's agnostic to the
3227600	3233920	set of directions and pretty model agnostic uh the annotation is another place where humans intervene
3233920	3241120	here to to tie in that last question um but you can imagine trading a captioner on a label
3241120	3245200	data set like this right a little larger than the one we collected so we're thinking about doing
3245200	3250720	something like that uh but preserving the human annotation does allow annotation you know in the
3250720	3257280	art context by experts as I mentioned so you might want to be able to do this at scale for a brand
3257280	3263040	new model and just have automatic annotations you use something like clip right for the kinds of
3263040	3268640	transformations you would see inside but preserve the opportunity for experts to to annotate kind
3268640	3276720	of specialized smaller trained models and there are results too from big ganttring on a couple of
3276720	3288000	different data sets if you're interested um I have a question regarding like the choice of
3288960	3297360	uh n in terms of annotations um so how did you arrive at this number and how are you I mean
3298000	3304080	how do you know like what number is kind of sufficient yeah good question um so I assume
3304080	3309040	you mean the total number of images we needed to annotate and not the total number of annotations per
3309040	3319520	image which end you mean I can talk oh I see I mean either yeah well okay so at both levels uh for
3319520	3326080	the directions themselves we needed to collect at least two annotations to be able to measure
3326080	3332480	intersubject agreement right uh we want to see if some direction is consistently producing
3332480	3338720	meaningful similar annotations across annotators we need at least two people to annotate them um
3338720	3344960	so for all the directions we evaluated we had two annotators label them and measured the interanitator
3344960	3351840	agreement using a couple of different metrics blue and burnt scores um but for a subset of
3351840	3357360	those we had 10 annotators annotate them and just had a look at interanitator agreement across a
3357360	3363120	slightly larger group uh for expense reasons we didn't do that for for all the directions because
3363120	3367040	it really wasn't necessary things didn't change that much and even in that subset when we went
3367040	3374400	from two to ten per uh per direction and then for the number of directions that we chose to
3374400	3384240	visualize it was not a very principled decision I'm afraid um we chose I think 64 z uh per category
3384240	3388560	and then a bunch of different minimal meaningful directions for them corresponding to I think
3388560	3393280	the same number of principal components that we looked at in the GAN space papers so maybe the
3393280	3401040	top 20 in each category so it was a bit ad hoc that decision um the the things that's going to change
3401040	3408640	we can distill vocabularies using this method for any size of annotation library right uh which is
3408640	3412320	one of the one of the beauties and one of the things that gives itself to to some of these more
3412320	3419840	ad hoc decisions um we're doing it analytically right if we go back to this we're actually like
3419920	3425840	reading off we're solving for the embedding matrix um of word embeddings in latent space
3425840	3430080	of concept embeddings so we could do this with like just a couple of directions
3431280	3435920	if you only had one annotation per concept it only appeared once then you're just going to get
3436480	3443040	for that direction um so as you increase the vocabulary as you increase the sample size you're
3443040	3448000	probably going to get a richer vocabulary but it's still possible to do on a vocabulary of this size
3448000	3454240	um so we're deciding now whether it makes sense to scale this up and collect like a number of
3454240	3459280	annotations where it would be possible like I said to to train a captioner on them to be able
3459280	3465520	to automatically label these directions rather than have humans do it so part of it is constrained by
3465520	3469680	the tractability of experiments on mechanical Turk right how many reliable annotations you
3469680	3479920	can get in some period of time awesome uh thank you yeah these are really useful questions these are
3479920	3488800	great um kind of along those lines more of a random question for the printer like the single words
3488800	3493520	for the labels yeah was it kind of agreed upon earlier like kind of what words you'd use because
3493520	3498400	like for festive maybe someone would say lively or for inviting you'd say welcoming is there like
3498400	3507040	kind of a similarity score for those words or really good question um no so this is it's only
3507040	3512160	preprocessed with like a little bit of limitizing so we collapse different endings people might be
3512160	3517600	using our different verb conjugations onto single verbs uh but festive would have a different
3517600	3523840	direction from lively uh kind of a next step in post-processing that we've talked about but
3523840	3529600	haven't yet done um is to just collapse across like wordnets and sets right so you could use
3529600	3535120	something like that to find synonyms of festive and then approximate one direction for lively and
3535120	3540480	then be able to break it down into something maybe more fine-grained um but there were no
3540480	3546160	kind of heuristics or standards for the annotators except you know they did they did a practice run
3546160	3551520	and looked at a couple of different examples and were asked to describe an overall transformation
3551520	3556320	that captured changes at lots of different kind of levels of abstraction we can look at the specific
3556320	3561920	what did we tell her that's on here yeah how would you describe the overall transition changes in
3561920	3566240	mood changes in objects or features of the scene don't mention your describing images so standard
3566240	3570320	kind of turk boilerplate just address the content what you see and then they could look at some
3570320	3575680	samples and then after they did a practice run they did the annotations um so any interanitator
3575680	3581680	agreement is just based on their word choice which in some sense is a raw window into perception
3581680	3586560	but in some sense that's bullshit and there's going to be a lot of noise there uh and we did see
3586560	3592400	that reflected when we used I don't have this on these slides um but when we use blue scores to
3592400	3598160	to measure interanitator agreement so when we use these layer selective directions to generate these
3598160	3604400	kinds of transformations if we get 10 people to annotate each transformation people might use
3604400	3609440	somebody might say eerie somebody might say spooky right somebody might say scary to describe the
3609440	3615120	sky uh that comes up as like quite different when you look at some methods of evaluating
3615120	3620400	interanitator agreement so we used first scores as well that evaluate the semantic similarity
3621440	3628240	instead of just literal correspondence words and found that annotations of these kinds of
3628240	3632800	directions performed a lot higher when we looked at semantic similarity of annotations as opposed to
3632800	3639600	just um just word based so there's definitely reason to start trying to collapse like that when
3639600	3644720	we look at the vocabulary too but we haven't yet in some sense it's it's kind of beautiful because
3644720	3650960	you can see all of the different words that people used to describe changes um but you'd get a lot
3650960	3656800	more power right if you could combine annotations for festive and lively and vibrant under one
3656800	3666880	umbrella bit of a trailer oh yeah thank you so much yeah any other high or low level questions
3666880	3673920	inter just have a maybe one or two more things not much so ask away if you do I think more more
3673920	3680800	of a higher level question I remember Ali in the first lecture um right you drew you had this
3680880	3686080	visualization of like two points in the latent space and you know a last function that would
3686080	3692720	steer like from one or trajectory one to the other but it was like something more like a curve
3692720	3698320	or something non-linear um right and you mentioned with Gannon version if you just interpolate like
3699280	3705760	draw a straight line between two points you have like all sorts of things happening I was wondering
3706480	3715680	if there's like a I guess almost like a like a and I guess unsupervised not a random walk but a
3715680	3725520	walk that would I guess lead to less perturbations I guess in in terms of like features I mean
3727280	3733600	does it make sense uh yeah we I really wanted to do that for this project um maybe Ali can speak
3733600	3740000	a little bit more about about his work there maybe after we stop this recording but um linearization
3740000	3746240	of this is a huge over oversimplification um and that would be one of exactly what you describe
3746240	3751840	as one of the things I'm most keen to try is taking non-linear walks uh through any of these
3751840	3759440	subspaces um so very on point question haven't done it you should try and do it um but describing
3759440	3765200	like this the semantic structure of latent space the semantic topology if you'll permit me that
3765760	3772000	is a really interesting question um because even the visual meaning corresponding to
3772000	3779360	some of these adjectives some of these words is not regularized or normalized in the latent space
3779360	3786320	itself so if I take five steps in the festive direction it might take me five steps to get
3786320	3793200	anything that will start to register to me as festive um but the walk size for a correspondingly
3793200	3798960	large visual change so to speak in a different direction could be very different um so some
3798960	3804080	transformations like making an image black and white this is anecdotal but you only have to go
3804080	3810080	like one step in that direction and then we'll visualize the change almost immediately uh so
3810080	3816960	we're not kind of we're not walking around in like a perceptually normalized space so to speak um
3816960	3822160	and there hasn't been to my knowledge a lot of work that's addressed that everything's been a
3822160	3830000	little bit at hawk um so thinking about semantic topology subspaces non-linear versus linear paths
3830000	3834480	and how we can think about kind of the concept mesh underlying latent space for different
3834560	3841120	generative models is extremely interesting to get to be a really cool area to do some working
3841680	3848960	cool thank you yeah let's ask Ali about that figure once we pull off here um I've got one more
3848960	3854640	thing to show you a quick example to hopefully spark more discussion unless anybody has anything
3854640	3867200	specific about this project we can always come back all right oh geez well last thing I'm going to
3867200	3875200	show you uh is still a beta and uh it's very it's very early and it's even thought development
3875200	3880880	but it captures something um that I think is deeply interesting uh and I think might be interesting to
3880880	3888480	you all um so the former method that I showed you for building shared vocabulary between humans
3888480	3895680	and models relies heavily on language right and so we get some direction and we're able to share that
3896320	3903040	between people and even to repeatedly use it to steer through model space um because we've given
3903040	3909200	it a label right we've used language and you might even argue that you know that's constraining the
3909200	3915440	space of what people can recognize in those initial sample directions because there might be some
3915440	3921600	genus or quad aspects of of images that we don't really have words for um but are still like really
3921600	3929040	recognizable or perhaps the verbal you know that the words you would use to describe something are like
3929920	3934400	quite complex and you wouldn't type that into an annotation like on mechanical Turk maybe you'd
3934480	3940320	want to describe the sky as like the sky you saw at your grandmother's house the day she passed away
3940320	3946320	or some flowers as effervescent like latte foam or a sparkling drink but you're not going to type
3946320	3950560	that into mechanical Turk and there's not a single word concept to capture it so that's
3950560	3955920	going to get lost in the method I described and lost in a lot of kind of standard either annotation
3955920	3963840	based or uh kind of hard coded direction search so I wanted to experiment with a way to capture and
3963840	3969840	learn um directions without language and this is like deeply inspired by the steerability work
3970400	3976000	of all these so you'll see a method here that is is similar to that in some sense
3977360	3983040	but we're allowing the human to define the transformation that they want rather than
3983040	3988160	pre-defining say a zoom or rotation transform using an algorithm we're allowing humans to come
3988160	3994960	into the loop and define that transformation purely visually by interacting with very very
3994960	4001040	small batches of images sampled from latent space or feature space at some layer and sort them into
4001040	4008480	classes corresponding to some visual feature its presence or its absence and this provides a pipeline
4008480	4014480	where users can steer just like in steerability work along dimensions that they discover however
4014480	4019360	that they define and they define them purely visually so labeling what happened just as a
4019360	4024640	matter of convenience but they're discovered um through vision so the way to do this is really
4024640	4031440	simple um take some latent space again a lot of these examples are are using big GAN you could
4031440	4039120	also use style GAN um take some latent space and sample images from it right if you're using a
4039120	4044160	conditional model so we pick some category here we're looking at lakes inside big GAN image or
4044160	4051600	big GAN places um sample some images for a user and then that user who's determining a visual
4051600	4058400	dimension of interest kind of looks over that image space and sees if anything stands out to them
4059360	4066720	across that that set of images so maybe here I noticed images that seemed kind of verdant and
4066720	4071760	fertile uh and maybe more more spring light but not totally seasonal you'll see where I'm going
4071760	4077360	it's kind of hard to describe and these were a little dreary or more wintry but there's not snow
4077360	4082240	so it's not really winter they're just kind of less fertile and vivid so that's the distinction I
4082240	4089360	want to make there um and the method is very simple just like the steerability work um and a
4090080	4096000	another example of work from Bolle we define a transformation just by learning a hyperplane
4096000	4101440	so training a SVM and learning a hyperplane it separates those two classes of images either
4101440	4106800	in the latent space or in the feature space of some layer layers activations and then when we can
4107520	4114320	steer some starting image in a direction that's normal to that hyperplane and steer it across
4114320	4120000	those classes right so I could take an image that starts in the kind of dreary or domain
4120720	4127680	or dusky or domain and transform it normally to that hyperplane and take it into the category
4127680	4134320	of things that I thought was more verdant right or more fertile but I could specify that separating
4134320	4142400	hyperplane just by sorting a shockingly few number of images um so we've done a couple of more like
4142400	4147920	fine-grained tests here but just for proof of concept you can discern these directions with
4147920	4153520	some degree of reliability with just like five to six examples of images in each category making
4153600	4158240	it really simple to interact with something like this just by dragging and sorting a few images
4159200	4165360	that are sampled in the latent space okay so there's a tiny example of a demo app we have for
4165360	4169680	this um and we're switching where it's hosted so it's not online at this very moment but it will
4169680	4175280	be next week but it's called the latent compass it was at NeurIPS Creativity I think last year
4175280	4180160	the year before um you'll see the the home interface in a second but what we do is just
4180160	4186640	what I said pick some category of BigGAN here it's BigGAN places um on the bottom you see images
4186640	4191360	sampled from that category and the user drags them to the right and left of the screen corresponding
4191360	4196800	to two different kind of categories of concepts they want to capture uh and then once the compass
4196800	4201840	calibrates and we'll see that in a second and you can drag any new image and then transform it along
4201840	4207360	that dimension so here we pick the closet category I've got full closets on the right
4207360	4212720	empty closets on the left and the dimension I want to capture here is something like fullness
4214000	4218800	so I'm going to see if I can I can learn a direction corresponding to the visual difference
4218800	4225680	between these two categories drag any new closet onto that center line and transform it along that
4225680	4231680	direction filling and emptying the closets and what if we tried a different category what if I
4231680	4238800	wanted to turn a medina into a full closet right what is the type of fullness that's relevant to
4238800	4244080	a medina oh well it's adding people instead of adding clothes suggesting that what's been learned
4244080	4250960	there that direction in latent space is abstracting generalizable enough to capture some visually
4250960	4256880	recognizable dimension of fullness that's meaningful to us in different scenes right and the model's
4256880	4262400	able to to generalize it in a way that's not totally dependent on the types of objects it saw
4262400	4268400	in one scene so it knows in a sense that clothes make a closet full but to make a market full
4268400	4273680	we're not adding clothes we're adding people and so the fullness direction is something that adds
4273680	4278720	more of whatever would make that scene full um to any scene that we're selecting in the model
4278720	4285200	right and trained on so few examples of course this this is really quite imperfect but it's a
4285200	4290640	good proof of concept of a way that users can interact super flexibly and really visually
4291600	4297600	with dimensions of interest and use that to kind of explore and surf the latent space of a model
4298480	4303920	by producing replicable repeatable directions that others can explore without having to use language
4304480	4309040	that's kind of a different way of carving up the puzzle of how to explore and assign meaning to
4309120	4315520	directions that we that we find in latent space okay that's at latentcompass.com
4315520	4323040	and we'll be back up next week I think bad timing okay so to return to our frame here
4323920	4330160	we've been digging a bit into this intersection between art neuroscience and machine learning
4330160	4335600	ways to explore models that have been trained on human creation right at different scales
4335600	4341520	to create a new to iterate and interpolate upon archives and then also to start to understand
4341520	4347200	what these models are representing and if our ways of interpreting dimensions inside models
4347200	4353440	can also teach us something about human perception or allow us to start to build models of aspects of
4353440	4359440	human vision that are otherwise pretty intractable because it's difficult to formalize what dimensions
4359440	4364240	underlie them where I could write down what dimensions under life physical scene understanding
4364240	4370720	because I know Newton's laws I couldn't write down what dimensions underlie aesthetic perception of
4370720	4377520	North African marketplaces or Babylonian tablets because I don't know what a large swath of people
4377520	4384160	would find perceptually interesting in a bunch of marketplaces I know from cognitive science
4384160	4390960	research certain heuristics to look for but that wouldn't give us a full set of what a diversity
4391040	4394880	of humans might appreciate when looking at some scene especially things like its mood
4395760	4400560	so we can turn here to these kinds of large unstructured generative models that learn
4401440	4407040	entirely from data entirely from images and turn to them as like a fertile ground so to speak for
4407040	4414000	starting to probe and represent human perceptual experiences inside their latent space and think
4414000	4420480	of latent space that way right as a screen as I said before onto which we can project human experience
4420480	4425280	and then once we have those projections we can rerun them and interact with them and collaborate
4425280	4430960	with them to create outputs of deep generative models that are particularly exquisite and that
4430960	4437680	represents some kind of collaboration between us and models of our our creation that are operating
4437680	4444880	in parallel so that's where I will leave us my emails here I'm very discoverable online
4444880	4452000	but you're welcome to write me questions anytime and I will wrap here and we can have a more
4452880	4458080	casual discussion unless anybody has any last questions for this part
4458480	4479120	Thank you so much sir this was really interesting and inspiring with all the
4479360	4488640	acidic decreasing slides and every moment of that was really full of thoughts I think that
4490560	4504240	you open a window to semantically and qualitatively looking at these latent spaces and
4505200	4508560	sort of our imagination and
4511360	4519680	where we dream and where these models that we create dream so I really appreciate that
4521840	4526480	I'm going to stop recording and then see if there are more questions
