{"text": " Cool. Hello everyone, welcome to your course AI for Art, Esthetic and Creativity. Today we have a very special speaker. She has an excellent background in different domains and she will tell you hopefully more about herself and her work. Sarah is a great friend and colleague of me and she kind of accepted to give us a lecture talk today. So from here I let Sarah to continue. Please go ahead. Thanks Ali. It's such a pleasure to be here. I've heard so much about this class. I don't think I have a slide about my background but I can tell you a little bit about myself. I finished my PhD in neuroscience so across the street from Seasale last year and now I'm a postdoc in the vision group and the journey throughout my PhD was a little bit of a winding path. I started thinking about explicit symbolic models for things like physics and we'll talk a little bit more about that along the way. So modeling how the mind makes inferences about things that we see but that hits a ceiling when we come up against questions of vision and types of seeing like looking at art that is really difficult to develop some kind of computational formalism for that we don't have good models for. And at the same time as I was kind of hitting that wall in my own thinking I was developing a parallel interest in visual art and doing a lot of different projects both with individual artists and with larger museum archives that I'll talk a little bit about and started to look at art as a ground for asking kind of difficult questions on the frontier of our thinking about the mind. If we look at how humans create art and in view art can we understand something about how they view the world and domains that we don't yet have good models of cognition for. So I kind of started steering my my PhD in that direction. I'll share a little bit of that work as well and as I said now I'm a postdoc with Antonio and Ellie asked me to share a little bit of my my inspiration behind that path. I don't have a good story about a specific moment I think it's been a lifelong interest for me since I was super small and reading a lot of poetry I guess thinking about kind of the the origin and nature of structure and our experience of the world. I know that's quite an abstract thing but the structure that we see in visual patterns where does that come from? Is that something that lives inherently in the brain and we imprint it on to kind of noisy and unordered stimuli or is it something that's external you know a nature nurture question and then our brains kind of evolve to reflect and I got interested in this meeting point this kind of layer between the self and the world where all the action happens so to speak and had training in applied math before I came to MIT and would think about ways to describe kind of structured inputs to processing systems and understand something about the structure of external inputs and then my neuroscience background learned a little bit how to how to think about and model the structure of a processing system right the structure of different parts of the brain and it's really been through my interest in visual art that we can start to think about and describe what happens when those two things meet and how we synthesize our world of visual experience in domains related to art and then other kind of higher level aspects of cognition like scenes or associations with moods of scenes and that kind of thing so that's that's where I am now and I think I'd like to start us off unless anybody has any leading questions about where I come from with kind of a provocation and you can you can think of this as a frame for what I'll share today but it's intended to be provocative and so the statement I'll make is that visual perception itself human perception which we attempt to mirror and model in computer vision and computer science in some cases that human perception is something that's fundamentally constructive and I say that because it solves an ill posed inverse problem like ones you've probably heard of before and doing that doing that solving requires a little bit of creativity so where am I coming from there the back of your eye as you know is is a 2d flat canvas right made up of a hierarchy of cells that were visualized in in drawing in art by Ramonica Hall hundreds of years ago and are now visualized using electromagnetic imaging and we can get actually pretty fine brain detail of the cells in the back of our eye that constitute a 2d canvas that takes in incoming image data and represents images in terms of patterns of activations via this kind of mosaic of cells yet we experience this richly 3d world so there's a setup of a problem that you've probably heard before right 2d canvas but we have 3d rich experience scenes have depth objects have 3d shape and furthermore what we see carries lots of different meanings and associations so where is all of that kind of higher level information in a 2d image classical kind of computer vision problems we look at this kind of painting by Suzanne you might not only recognize 3d structure of this cottage on the mountain side right even though the image itself is 2d I might have all sorts of associations with it I might be able to say oh it's spring time think something about the time of year I might even be able to infer something about the geography by the palette used to convey what fields might be there think a little bit about the landscape I might be able to appreciate depth in pictorial space so even on this 2d plane if I put my mouse up here in the front maybe these fields are closer to me as a viewer than these ones that are far away but once again I'm just looking at a flat picture where is all of that information our brain has to solve an inverse problem like this anytime we look at a visual scene it has to get from low two-dimensional information to kind of rich 3d but there's a fundamental problem here and I pointed to that is that infinitely many 3d objects can cause the same 2d project projection that's the under constrained nature of this inverse problem that vision poses and you've experienced this quite explicitly anytime you've seen a shadow and not the object causing the shadow right and you've had to infer oh is that actually you know a monster on the wall or is that somebody's hand being projected but there are infinitely many configurations in three dimensions that could be projected downwards onto two dimensions and cause some configuration in pictorial space so how do we constrain that problem when we're solving for what we see right so this problem is ill-posed because it has as I said many infinitely many possible solutions and choosing between them requires some additional information and in the case of the brain modern neuroscience understands this as requiring the brain to construct something so that's what I mean when I say perception is fundamentally constructive or creative it requires the brain to construct a best explanation of what it's seeing of incoming information and if we call that perception then maybe you'll permit me to make a bit of a stretch and say that that makes perception itself an act of creation or an act of synthesis of a scene so one kind of popular way to solve this inference problem is by using models of the world right and we can approach that from a Bayesian lens maybe you've seen the work of Josh Tinnenbaum in the bcs department or maybe we can do that purely with deep learning it's kind of an attention that we could explore later today but I'll give you an example here and this is let me back up for a second that if we were in person this is the point where I would do kind of a live in person demo so I want you to imagine that we're all sitting kind of in a dark room or we're sitting in a studio space and out in front of you there is a table covered in black velvet and I've set some stuff on that table you don't know what it is I set it there when the lights were off and then I take a single line of red laser light and I'm going to gradually sweep it over the scene so I'm constraining the visual information you're going to receive about what's out there in the world to something kind of really low dimensional compared to what you normally get to understand kind of a garden of forms that would be sitting on the table so imagine you're there in the studio with me and you see the following you have to kind of infer what you see on the table maybe you could write it in the chat or just think to yourself when you see this give it a moment what's sitting here on the table or what kinds of things what different things maybe this would be a good use of the chat I can pull it up or you can describe features of what you see a bunch of blocks on the table great there's something cubic up now I see the corner there right multiple vases multiple forms with kind of different underlying shapes something cylindrical yep two things do you see I think there's a sphere actually there in the middle what is the experience of this light do you actually feel a physical corner when you see bent light round the corner of that cube oh oh goodness interacting with chat is a bit tough all right anyway the point I want to make here is that I can present really kind of low level information and you can if I dare open open the chat up again yeah there's a single base there's a single table and many forms sitting on top that's right so there's just a tabletop and then lots of different shapes also covered in black velvet so the light doesn't scatter and the light traces the outline of these 3d shapes and you can appreciate something about what the shapes are just by watching how light bends around their surface and the relative motion as it traverses that facade right as it moves over the surface of the sphere the light bends according to its curvature and I would argue here that because you have some notion of what a sphere is and some notion of what a cube is that is you have a relatively abstract model of these underlying shapes in your mind a mental model you can do some inference when you see light move over their surface on this way even though I choose an example like this because you've probably never seen this example before right never seen a single line of laser light move over this table surface even this this kind of setup but you can still do that inference pretty well and you did in the chat so if you'll if you'll stay with me here I'm suggesting that this is an example just your perceptually of how we can bring kind of models of the world and shapes and forms that comprise it to bear on simple visual stimuli and how we can even do that by using articulated light to isolate aspects of those stimuli and to kind of elucidate our perception to us so we do a lot of things like this in the MIT museum studio where I teach the vision and art neuroscience class which I'll talk about in a moment but that's where this this was filmed let's see if it'll let me advance even though I open the chat all right so another kind of setting in which we often hear and think about models of the world and this kind of inference is in intuitive physics and I bring this up because some of my background is also in this type of work investigating how the brain represents physical properties like mass that it uses to reason physically about the world right you would have to estimate the mass of this block that's falling and making a depression on this pillow before you would know the right amount of grip force you would need to use to to reach in and pick it up without dropping it right and this is something we do incredibly automatically and it's a skill set we develop regularly from a very early age and I found that the brain represents properties like mass with an amount of abstraction and invariance to the type of physical scene in which mass is revealed that would be necessary if mass like this were to be used as an input to an abstract generalized engine for physical simulation or what we call a physics engine in computer graphics and simulation suggesting that there is kind of some first evidence that the brain does use these kind of generalized simulation engines to solve low-level inference problems like inferring mass because we can make some hypothesis about the nature of the underlying representations it would need if it were to solve problems in this kind of way rather than by simple pattern matching or in a pixel based way where we would assume that the representation of mass would be quite different from scene to scene because the low level visual data about the scene is different but in fact that's not what we find we find representations of physical variables like mass and friction that generalize across any kind of physical scene that we test where we hold a lot of other different parameters constant right like object color and this suggests an account of physical reasoning in the brain that has been that has been studied pretty extensively computational right and that we model via probabilistic simulations of a physics engine I don't think that video is going to play for us right but this is the kind of work when I was doing when that I was doing when I was writing down like explicit models of the world that could be inverted to explain something about underlying parameters we were using for vision and in that in this case those models were physical right but what about cases like like art where it's difficult as I mentioned to develop some kind of computational formalism where we don't know the underlying model for instance how to create the Cezanne painting we saw in the beginning a priori right how do we even start what are the underlying dimensions we'd need to write down to either make sense of how we see things or how they're created so this whole area is kind of what we dive into in that vision in art and neuroscience course so this is something you're interested in it's of course an unsolved problem but we spend the fall semester every year kind of delving into it through both neuroscience literature through art practice through computation and then through studio work so kind of hands-on experimentation with principles underlying vision that we then externalize and experience ourselves and try and visualize in artistic contexts to give you a little bit of a taste of that class we would look at these examples say by by an artist in minor white and ask if we were trying to set up a typical kind of describe a model and then invert it to understand vision setting you know what is the veretical percept in either of these right if before we were considering mass of some object that the brain has to infer and then we can write down a physical law describing how mass plays into action unfolding in a scene a law describing dynamics and then invert it to think about how the brain represents mass what would the analog be here what would we write down as the veretical percept you can share some thoughts in the chat that is also an exercise you could just do yourself right maybe here you can start to get it a shadow of something outside the window I see a bike maybe a bike seat there that's kind of not the point kind of not trying to infer what caused the specific physics of this this image you're kind of getting at something different and especially here what if the artist isn't around for us to ask anymore these are actual photographs right these are photographs of something but the act of looking at it isn't about inferring the underlying cause of the image it's about inferring something else sort of aesthetic parameters that define visual experience or kind of render visual experience at a lot higher of a level how do we begin to get traction on problems like this either in seeing or in or in generation as I said you know in art we also come up against a great difficulty in that you know there are infinitely many ways to render recognizable depictions of common objects right with all sorts of idiosyncrasies illusory boundaries difficult for models to detect but we recognize a woman in these images with the dress almost instantaneously and similarly we come up against another under constrained inverse problem is in that there's infinitely many ways to render and depict kind of abstractions of commonly recognizable forms which again are difficult for current day models but they're pretty easy for us I can recognize a figure and maybe have different associations with it in each of these different images so we think a little bit about this in the course like I mentioned you can ask me a bit after this talk as well if you're if you're interested in it it's called vision and art and neuroscience all of our info is is online most of the syllabus past exhibition catalogs at vision.mit.edu it's offered through through bcs and as I said we we investigate during half the class in the seminar portion of the class kind of the underlying principles of vision and we work through a series of modules that build up visual processes from early level like v1 visual processing all the way up to kind of more rich images and we do this in parallel in a studio section during the other portion of the class where we're translating these principles of vision into the studio and building artistic contexts where we can kind of become aware of our own perceptual processing at work so examples like the one I showed you at the beginning right with the with the laser line moving over that garden of objects are examples of settings that can allow us to maybe perceive our own perception at work right or shed some light on what's going on when we look at at normal scenes right there's all these unconscious inference processes happening even when we look at corners in a room but we're not aware of them and so we ask here if we can create settings where we do become intensely aware of them and that awareness becomes kind of the art experience right and so it's the art of perceiving one's own perceptual processes at work and then over the course of the class everybody develops an individual artwork for exhibition which is super lovely and it's it's an opportunity that we don't often have in other classes at MIT so we run this for five years now had five different exhibitions and COVID be it a virtual exhibition and then this year's just opened in December and is actually still up in the MIT museum studio just off of lobby 10 10 150 if anybody is on campus and wants to go check it out it's most it's open most days when staff are there but this course is the parallel to your IAP class that thinks about things more in the language of computational neuroscience than deep learning and in some aspects of the course will present deep learning or deep generative models as contexts for probing representations that might be shared by human minds and machines and we'll look at that a little bit later in this lecture but think more traditional computational neuroscience lectures readings visual art and then a studio component where you experiment with some of the stuff hands on so that's what we do envision art neuroscience we start to to probe at the richness of this art neuro and machine learning intersection there's a lot of different things we can do there and for the rest of this talk we're going to highlight a number of different projects that approach that intersection in different ways and highlight kind of different ways that you could think about engaging this material in these questions data sets and resources that we have available and kind of different ways of carving up the problem into bits so we'll start by thinking about modeling kind of the structure underlying human creativity at scale without trying to prespecify laws that you would write down for say a physics engine right can we use deep generative models to kind of approximate or appreciate or grok the structure underlying large data sets of human cultural artifacts and then use those models to experiment with cultural history on kind of a timeline that allows rapid evolution in the present so I'm speaking specifically about a project that I don't know if some of you have seen and I know Ali has seen a collaboration that I led with the Met a couple of years ago again it was fun that we were in person because we were able to actually go to the Met and see a lot of these objects but back in 2017 the Metropolitan Museum of Art was the first or one of the very first to release an open access catalog of a few hundred thousand digital images of works in the Met collection and released them into the public domain which is wonderful for for us as computer scientists and programmers and people interested in ML and art because what a rich data set that is right what a rich data set all in one place don't get me started on the issues with museum APIs but a lot of museums have followed suit in releasing their digital collections into the public domain so they're free and open for experimentation they approached us at MIT and open learning and a couple of programmers at Microsoft and asked if we might want to do a series of projects with this digital collection and so we did and we asked whether we can build deep generative models associated with archives like this of created work that are embedded in their cultural context which might ask which might allow us to ask like slightly more specific questions art historically than just you know what if you train StyleGAN on all of wiki art all at once right not conditionally so we're not appreciating any categorical differences between images but if we just showed it all of wiki art okay here we want to ask something a little more fine grained can we notice you know differences in the development of feature languages between maybe time periods or geographical regions right and can we develop ways of collaborating with those models to iterate archives forward so experimenting with chimeras between existing works and developing new works right that might sit somewhere between works that are already on a graph so one of the challenges that we faced here initially was that the data set was pretty big 400,000 images but each individual category in that data set was not some might only have a couple hundred images and there's a lot of sketches and drawings and kind of uncategorized work too that makes up that 400,000 so you're in a situation where in theory you have a rich labeled data set but in practice it might be quite difficult to train anything that looks photorealistic or gives a good sense of any individual category of work because the categories themselves are not that large so at that point this was pre like style again too we started working on this in 2017-2018 um I asked whether we could instead of training a single model on say a subset of this met collection like this category of vases called yours whether we could find corresponding subspaces of what we're now referring to as foundation models like big an image net that kind of approximate our data set right so if we think about foundation models as a shared resource that ideally everybody would have access to and there were ways to think about contributing to then maybe these smaller problems become or can become a way of defining subspaces of those big models that we can interact with right rather than having to retrain a model and on our data set so we used GAN inversion here and instead of training a new model on just this category of viewers we asked whether we could embed each image that already existed into in the met collection and into the feature space of big GAN image net which happens to have a category for vases so we selected categories that were shared between image net and the met collection there are a handful about a dozen um and we maximized for each of those images the similarity between the met image and the big GAN image using a two-part loss right so we wanted them to be similar both at the pixel level and at the semantic level and we did that by looking at two different layers of a pre-trained res net as the embedding network so once we've embedded these models these images into big GAN we can then visualize the individual embeddings but we can also do something a little bit more interesting than just look at approximations of these images which might not be very good we can think about the underlying feature language that might have been learned and then look at interpolations between the existing images in the met collection I hear murmurs in the background if anybody has a question hit the chat you're super welcome to speak up um so next we look at interpolations between these existing images on the graph and we can create kind of hypothetical or dreamlike images that exist between the spaces of existing works in the collection and these are pretty interesting and beautiful and they allow us as I was mentioning to ask questions about what collaborations between geographical regions might have looked like right because we do have categorical information about where each image in the met collection came from it allows us to suggest new objects and the spaces between them so it allows us to interpolate and the other beauty of these kinds of executable models of culture is that it allows us to iterate on existing collections really rapidly um and evolve them forward and so we can kind of start to imagine archives of the future that would have embedded within them world models corresponding to the data set that exists at one point in the archive right so the archives could kind of evolve themselves forward and suggest future versions of their collections based on what's already been created and that this is again this was back in 2019 which is a long time ago in computer vision terms um but even just with with inversion into to began in the channel I was really impressed at the quality of the the images and the hypothetical objects that we could get for example here are a bunch of different generated teapots from the met latent space in the teapot category which again happened to be shared between ImageNet at that point um and the met collection and as I said we did have the the opportunity to exhibit this in the met which was absolutely wonderful um we projected a visualization of this latent space superimposed on a map of the met collection and allowed people visitors to the to the great hall to kind of step in to this latent space as projected onto the ground and explore the traversal of the spaces between works um and a projection behind them on the wall. We also made a web app version of all this that exists even though we we can't visit the met today um it's online at gen.studio if you want to go have a look after this and then all of the the code base is linked the github is linked at the bottom um if you want to check out any of that more specifically but again it places us kind of a different framing of latent space traversal than we're used to that I was interested in this project was to place us on you know we've gotten a lot further along in the video um you can go look at the at the website places us on a map between the objects when we're doing the interpolations right so we select an object to start now we land in the latent space of big gen close to that object and then we can move ourselves around on the map between objects and their embeddings in that latent space right and as we're moving physically in 2d space here online we can visualize what exists at that point in latent space and then we can find its nearest neighbor visually uh in the met collection and find what object in the existing collection is most similar to the hypothetical work that we discovered in the interstices between two existing works um so give that a look and this project lives on today um and a couple of different forms I'm still working with the artist Matthew Richie he was a collaborator uh with us on the met project um on a couple of different tendrils of of this work where we're asking all right so we can model projections of existing images in the met in the met collection by finding their embeddings in some kind of large foundation model but now in 2021 we have things like StyleGAN ADA that can can train on smaller data sets and do reasonably well in approximating data sets that would correspond to a single category in the met collection so we've done that um we've trained these models on sketches um Babylonian cuneiform tablets Japanese watercolors and some 18th century European landscapes among other things and have individual models correspond to each of these genres within the met collection and then we've been working with a friend in New York who has a robotic oil painter and can actually create layered paintings of really short walks in latent space along different dimensions in this model so think about physically visualizing some of the durability work you've looked at in this course right could we make time paintings of really short walks in latent space by superimposing robotic paintings of the visualized image kind of at different points along that walk so that's that's being exhibited right now at UNT in their contemporary art gallery let's see I've got a question in the chat room oh it's just a compliment I will take it at any point yeah I think can you please read it yes uh someone mentioned that this is a creative reason one of the most creative reasons they've seen to do latent space interpolation since it scans yeah I think that I had a slide a moment ago if you want to rewind in the recording of this suggesting that kind of part of the advent of using GANs to model kind of large databases of creative work is that they allow us to do a couple of things right that interpolation and that iteration and in cases where you can't write down a feature language underlying a set of works because you don't know our priority what it is you can imprint that or you can learn something of that in a deep generative model right and then you can collaborate with that and hypothesize what might lie on a graph of human creation if we presume that any creation artistic creation at some point in historic time is if you think about it as the manifestation of a point on kind of a sea of cultural influences and multi-generational practice iterative practice that's been shared between peoples and generations and the creation of a single work is the enactment of that process at some point in space it's natural to think of that in some sense as a model that we can capture in a latent space where we're manifesting some part of structured space at some moment but this allows us to iterate on that which I argue is similar to some historic processes of iteration and collaboration across groups of people really quickly right um so I've been trying to take this a little further now and ask all right we can make paintings of short walks in latent space we can hypothesize objects that might have existed but we don't think they ever did but we still don't know much about these models even if we train StyleGAN 2 on a set of 2,000 paintings in the net collection uh you've probably seen some of the interpretability work adjacent to what Ali has shared or David Bao's work so that style of thinking we don't know anything about this Japanese watercolor model like what do its individual neurons represent is there a neuron for trees well what is a tree here it's some brushstrokes we recognize as a tree but it's not something that BigGAN trained on image that would necessarily recognize as a tree maybe more simply kind of in the in the steerability context what do dimensions in the latent space of a model like this correspond to right sure we can find things like zoom and 3d rotation because we can name those transformations and then find directions that maximally correspond to them using that kind of steerability technique there are all sorts of other directions like the ones we're visualizing here that certainly have some affective meaning to the viewer that we don't know what they are in the models terms or in the viewer's terms so at this point in this project we're thinking about starting to name and understand dimensions underlying generative models trained on bodies of artistic work from museum digital collections not only limited to the met but around the world uh and our motivation here is to kind of create these alternate and imaginary histories of art built from unique latent walks that we can visualize in real time with this painting or computationally and then maybe understand something about aspects of picture language that might be shared across you know vastly different genres so Babylonian cuneiform tablets transformed from numeric to symbolic and image-based at a very particular point in history and can we find a dimension in style GAN trained on a very different genre of art that corresponds to a similar kind of transformation and as such can we build up kind of a picture language that would correspond to diverse forms of art making right that you might not see in any of these different categories of digital images on an archive but we might start to appreciate once we can investigate them by training deep generative models on them let's get back to great okay so when we're thinking about this intersection we've seen one example of modeling the structure underlying creativity at scale and i've done other projects and you can find many examples online both of my work and other peoples of trying to do this not for creativity at scale but for individual instances of individual artists and modeling either the style or the processes of individual art making techniques so all of these are kind of flavors of starting to imprint or grok or understand the structure underlying creativity but not symbolically right so we don't we don't know how to interpret these models even though we can visualize them and create really interesting hypothetical objects that might be indistinguishable either from existing work or from one artist's particular style we can also think about these models as a tool themselves for collaboration both in their creation and iteration with others who contribute to their models and with the models themselves which as i described represent kind of executable versions of collective cultural structure we permit permit ourselves to think about them that way or facets of kind of a global creative identity but as i mentioned now we're at a point with tools and computer vision where we can start to ask what rep representations actually underlie these models trained on artworks that are themselves executable versions of some collective cultural structure right well what is the structure what's going on under the hood do they correspond to dimensions that we find meaningful when we look at visual scenes and so in the next part of the talk i'll share a couple maybe more technical projects that explore specific ways that humans can interact with generative models in order to maybe learn something about human vision as well right so can we build shared vocabularies that help us interpret dimensions underlying these models by designing experiments that allow us to visualize and interact with images and latent walks like you've been seeing i'll pause here because i need a sip of water and i'll keep an eye on the chat in case anyone has any questions before we go on all right looks like we are question free so far five more seconds i guess i have a question yeah so this might be talked about later but i was wondering a little bit about like in your research and kind of this field how much of like human interaction is like a big part of it and kind of like the human coming in and saying uh how they think about something and see where that agrees with the computer or like kind of like where that role is played wonderful question so these kind these kinds of high-level questions that get it some experiential component or design component of the worker i think really useful ask more of them i'll tell you for different projects what that looks like and in the next section of work it's going to be really obvious because there's human annotations but for this project so the human would come in here you know we train models on datasets of art selected from the met collection and these are small and these are subsets and they were gathered by matthew richie and myself going through different genres in the digital collection of the met online and like hand selecting images from those different genres right representative images of different categories of work or maybe in a less fine grained way all images under some designation so japanese watercolors between the 17th and 19th centuries so we made that selection and tried training these models on a bunch of different such selections and decided which ended up you know with so few examples providing at least a representative sample of the kind of work that we know we saw there right and then here the selection of like walks through latent space so think of those in the same way you've been thinking about the steerability walks they were very arbitrary so that was a completely human selected so it's a kind of a different approach to interpretability where it's steered by the human eye right we're not doing it automatically and we're not doing symbolic it symbolically we don't know what these correspond to but that's trying some arbitrary walk through latent space trying many of them and then the human then selecting what to them felt like an artistic expression this is an art exhibit and then in the next step we'll ask how can we do that in a more systematic way and start to build a language corresponding to what those different walks could be a language that's shared by humans and that takes at least right now a lot of human a lot of human interaction you could think about ways to automate that we'll talk about that in a second but with any kind of human interaction it's nice to preserve the opportunity for direct engagement with models rather than intermediation by a captioner or something like that because then you could imagine using your technique on different subsets of humans right on different kinds of experiences so you might imagine getting an art historian to label and select different walks through latent space here corresponding to very nuanced changes in the development of Babylonian like cuneiform tablets right that a captioner couldn't recognize I couldn't recognize so you might want to be able to pull different kinds of humans into the loop at different times to engage in ways that kind of use their knowledge to create a unique synthesis with a generative model so that's what engagement looked like here and then with this next project it'll be super it'll be super clear and I'll make sure to speak specifically to that so thank you yeah cool so next this is probably a summary of what you've seen so far in your IAP course so there's a lot of different work on discovery of interpretable directions in the latent space of different generative models right and we can steer images along those dimensions to create interpretable transformations that allow us to interact creatively with deep generative models right here we are deep learning for creativity but a lot of these examples presume what concepts we're searching for in the latent space and in fact they do that really explicitly right we will pre-define a zoom transformation and then maximize the similarity between some transformation in the latent space and a zoom transformation as applied to some image maybe you've experimented with code for doing that but that presumes we know we're looking for zoom in the first place what if we find ourselves looking out into more you know uncharted waters so to speak here we ask how we can learn kind of a vocabulary of visual concepts maybe one that you would apply to those style gains we just saw train on the net images right we don't maybe we could look for zoom but maybe there's all sorts of more interesting transformations we could do to those images but we don't know what they are yet how can we learn a vocabulary of visual concepts rather than pre-define them or labeling them after the fact so there are a variety now of unsupervised methods for distilling these kinds of transformations in latent space that find principal components of feature space of different layers the models activation maybe you've played around with methods like GAN space that search for and rank where the largest principal components of the feature space which do provide us with interpretable transformations but they're labeled after the fact so we don't know if they're meaningful to humans kind of in their genesis but we can we can describe them right by providing labels to them another point where the human kind of comes in the loop but we want to see if we can build in human vision to the discovery process right so to supervise it but to not pre-commit to what kinds of concepts we're searching for so in this project we're trying to build or define a method for building a visual concept vocabulary for an arbitrary GAN latent space so to put it more specifically we want to learn embeddings d maybe you've called this w we want to learn some kind of walk in the latent space z if again we'll focus on big GAN here of transformations that are salient to us in visual space and we can't define an objective and optimize our d our walk to produce a transform in x in the image because we want to learn the vocabulary concepts rather than pre-commit to them and we would have to pre-commit to what that objective is right in order to optimize d so we're going to take a different approach and instead sample the space of salient or possible transformations for some given point in space for some given z and then use those sample directions as a screen so to speak onto which we can project human perceptual judgments so that's a little bit of a gratuitous metaphor but maybe a useful way of thinking about it and then we'll we'll disentangle the concepts that are projected onto that screen into a vocabulary of open-ended compositional visual concepts and what we're interested in here is the overlap between what's represented inside a model so some deep features in a model's representation and concepts meaningful to humans in visual seeing understanding we're asking how we might start to define although not completely but start to define a shared vocabulary between the two or for a given model determine what lies in that set overlap and I don't have to dwell too long on a lot of the specifics here it's all online at that URL if you want to read the paper but as I mentioned the first thing we're going to do is generate a set of sample images that produce minimal meaningful transformations in images and then humans come in the loop again we're going to ask them to label them but here we're forming the basis for the data set that we'll build our vocabulary off of and we want to keep in mind that we want a vocabulary in the end that is both diverse so corresponding to a lot of different changes that you can produce in an image and specific where a single transformation corresponds quite reliably to one visual change across viewers so we do that by defining mutually orthogonal what we call layer selective directions and these minimize change in the feature representation at some layer of big care and at some layer we'll call it layer l and this allows us to capture relatively focused changes because we hold constant how much the representation can change at some layer and we do that for different layers to capture changes at different levels of abstraction so as you can see layers closer to the image output control or fine grained aspects of the image like the color of the walls and the bedspread and as we get closer back to the latent space we're allowed to make kind of more higher level changes in things like zoom and perspective of the scene and its composition so what objects are present so here we have a base set of minimal meaningful transformations that capture changes in images at different levels of abstraction we're going to ask people to label them because we don't know what's going on visually in these scenes right so we started at a pretty small scale with just four categories in the places data set and looked at big and trained on image net and places we'll just talk about places here and visualized a handful a few thousand of these directions per category so in each of four categories looked at cottages medinas so uh street marketplaces kitchens and lakes a mix of indoor and outdoor scenes and then asked people to just simply describe the overall transition that they saw when these directions were applied to different randomly sampled starting points in the latent space right so one direction might take this cottage to this snowy cottage and change something about the sky and change the snow so these these changes are still complex we can recognize that it's the same scene and we can describe in simple language what's going on but they're they're not disentangled yet right one direction might correspond to a number of different visual changes so we did a little preprocessing to capture you know what kinds of concepts are associated with each transformation and then we decomposed those annotated directions into a visual concept vocabulary consisting of single directions labeled with single words we formulated that as a linear regression and then solved for the embeddings of individual concepts in the latent space of our begin and then we can basically read those off of our matrix e and then transform the images by manipulating them some amount along those visual concept directions happy to talk more details about that if anybody's specifically interested or you can check out the paper itself we found over 2000 concepts this way corresponding to lots of different types of visual changes so we can reproduce transformations like zoom and rotation things like color but we also get kind of a unique set of concepts corresponding to aspects of scenes like their mood for instance there's a direction in latent space of big and that makes outdoor marketplaces more festive and here we see applying that direction to an example marketplace and it rolls out a red carpet hangs some flags and brings a lot of people into that market we can visualize kind of a a sampling of these directions each applied to two different images in different categories so some directions make cottages more manicured add arches to marketplaces add shadows or make the whole scene blue and we see directions like this that generalize across all of the categories it began that we looked at you can check out the lake category we can add sunsets but also do kind of scene specific things like add reflections to water or make a lake scene foggier make a kitchen more inviting or more modern and again we didn't have to pre-specify what exactly modernity would entail when applied to a kitchen we learned that through sampling what humans associate with a transformation that was sampled randomly right uh the humans labeled that as modern and then we disentangled the specific direction in latent space corresponding to that single concept word and once it's isolated we can apply a modern transformation and know that it corresponds to what viewers found to represent modernity in a kitchen well I said we know that it corresponds to what viewers see as more modern but we don't know that for sure right we still need to ask questions like how generalizable are these directions do they compose right can we add a festive direction to eerie and get something that's both scary and festive right or could we make a kitchen both more modern and inviting so we asked those questions in a series of behavioral experiments that I left for you to check out in the paper itself so we won't in the interest of time go through those here but we do find that these directions are composable and they're generalizable across categories so there are some cases where we can even add a concept that was learned in a single category to a different category for instance making a cottage more festive right or adding snow to a marketplace even though that's not traditionally seen there we ran a set of behavioral experiments evaluating the extent to which this is successful and isolating a couple of few specific cases where it fails okay so this this wraps up this method it's at a point now where we're trying this with some of the the art models that I discussed previously right so this was still just applied to big and trained on real-world images trained on ImageNet but you can imagine using a similar similar method to find dimensions of visual interest that are also meaningful to humans in the latent space of a model trained on art images and so decompose future languages underlying different genres of art into something describable so that we can make concerted manipulations to images sampled either from foundation models that correspond to approximations of real-world images or to models trained on on archives of art images themselves I'll pause here for any questions about this there's associated code also available on that project page I linked have a quick question please um was there a reason you chose big GAN over starting with like style GAN for this type of work uh no um just a couple of different code bases that already existed um and people that had worked on big GAN for like GAN dissection we had an easy way to dissect big GAN and hypothesize what kind of things might be there uh so a lot of the GAN dissection work started with big GAN and so I was picking up where that left off and asking if we could find like style vectors that corresponded to scene level transformations instead of individual neurons um but I have extended this to style GAN outside of the paper it's just not got it it's here yeah the method's pretty I mean there are a couple of different small changes you have to make um but the method is pretty model agnostic just like defining a set of certain directions that samples the latent space in kind of minimal ways and the the method I described here is definitely not the only one you could use for that right um you could sample them by just finding the principal components of the feature space or you could sample them randomly right you could just find two points in latent space interpolate and then get people to label what's going on there um we tried a lot of these different methods uh and found that if you if you make random interpolations between two randomly sampled points then there's just so much going on in the scene that there's not a lot of inter observer agreement in how people annotate what they see there's just too much going on so we need to isolate specific changes that's why we developed that layer selective method for isolating minimal changes um but what if we used kind of the the principal component method right or used something like GAN space um there we found that the principal components of the model's feature space aren't necessarily the most interesting to humans so we might get a ton of different types of rotating the scene but not a lot of different changes of mood or changes in color up there in high-ranked principal components um so that's where that method came from but it's agnostic to the set of directions and pretty model agnostic uh the annotation is another place where humans intervene here to to tie in that last question um but you can imagine trading a captioner on a label data set like this right a little larger than the one we collected so we're thinking about doing something like that uh but preserving the human annotation does allow annotation you know in the art context by experts as I mentioned so you might want to be able to do this at scale for a brand new model and just have automatic annotations you use something like clip right for the kinds of transformations you would see inside but preserve the opportunity for experts to to annotate kind of specialized smaller trained models and there are results too from big ganttring on a couple of different data sets if you're interested um I have a question regarding like the choice of uh n in terms of annotations um so how did you arrive at this number and how are you I mean how do you know like what number is kind of sufficient yeah good question um so I assume you mean the total number of images we needed to annotate and not the total number of annotations per image which end you mean I can talk oh I see I mean either yeah well okay so at both levels uh for the directions themselves we needed to collect at least two annotations to be able to measure intersubject agreement right uh we want to see if some direction is consistently producing meaningful similar annotations across annotators we need at least two people to annotate them um so for all the directions we evaluated we had two annotators label them and measured the interanitator agreement using a couple of different metrics blue and burnt scores um but for a subset of those we had 10 annotators annotate them and just had a look at interanitator agreement across a slightly larger group uh for expense reasons we didn't do that for for all the directions because it really wasn't necessary things didn't change that much and even in that subset when we went from two to ten per uh per direction and then for the number of directions that we chose to visualize it was not a very principled decision I'm afraid um we chose I think 64 z uh per category and then a bunch of different minimal meaningful directions for them corresponding to I think the same number of principal components that we looked at in the GAN space papers so maybe the top 20 in each category so it was a bit ad hoc that decision um the the things that's going to change we can distill vocabularies using this method for any size of annotation library right uh which is one of the one of the beauties and one of the things that gives itself to to some of these more ad hoc decisions um we're doing it analytically right if we go back to this we're actually like reading off we're solving for the embedding matrix um of word embeddings in latent space of concept embeddings so we could do this with like just a couple of directions if you only had one annotation per concept it only appeared once then you're just going to get for that direction um so as you increase the vocabulary as you increase the sample size you're probably going to get a richer vocabulary but it's still possible to do on a vocabulary of this size um so we're deciding now whether it makes sense to scale this up and collect like a number of annotations where it would be possible like I said to to train a captioner on them to be able to automatically label these directions rather than have humans do it so part of it is constrained by the tractability of experiments on mechanical Turk right how many reliable annotations you can get in some period of time awesome uh thank you yeah these are really useful questions these are great um kind of along those lines more of a random question for the printer like the single words for the labels yeah was it kind of agreed upon earlier like kind of what words you'd use because like for festive maybe someone would say lively or for inviting you'd say welcoming is there like kind of a similarity score for those words or really good question um no so this is it's only preprocessed with like a little bit of limitizing so we collapse different endings people might be using our different verb conjugations onto single verbs uh but festive would have a different direction from lively uh kind of a next step in post-processing that we've talked about but haven't yet done um is to just collapse across like wordnets and sets right so you could use something like that to find synonyms of festive and then approximate one direction for lively and then be able to break it down into something maybe more fine-grained um but there were no kind of heuristics or standards for the annotators except you know they did they did a practice run and looked at a couple of different examples and were asked to describe an overall transformation that captured changes at lots of different kind of levels of abstraction we can look at the specific what did we tell her that's on here yeah how would you describe the overall transition changes in mood changes in objects or features of the scene don't mention your describing images so standard kind of turk boilerplate just address the content what you see and then they could look at some samples and then after they did a practice run they did the annotations um so any interanitator agreement is just based on their word choice which in some sense is a raw window into perception but in some sense that's bullshit and there's going to be a lot of noise there uh and we did see that reflected when we used I don't have this on these slides um but when we use blue scores to to measure interanitator agreement so when we use these layer selective directions to generate these kinds of transformations if we get 10 people to annotate each transformation people might use somebody might say eerie somebody might say spooky right somebody might say scary to describe the sky uh that comes up as like quite different when you look at some methods of evaluating interanitator agreement so we used first scores as well that evaluate the semantic similarity instead of just literal correspondence words and found that annotations of these kinds of directions performed a lot higher when we looked at semantic similarity of annotations as opposed to just um just word based so there's definitely reason to start trying to collapse like that when we look at the vocabulary too but we haven't yet in some sense it's it's kind of beautiful because you can see all of the different words that people used to describe changes um but you'd get a lot more power right if you could combine annotations for festive and lively and vibrant under one umbrella bit of a trailer oh yeah thank you so much yeah any other high or low level questions inter just have a maybe one or two more things not much so ask away if you do I think more more of a higher level question I remember Ali in the first lecture um right you drew you had this visualization of like two points in the latent space and you know a last function that would steer like from one or trajectory one to the other but it was like something more like a curve or something non-linear um right and you mentioned with Gannon version if you just interpolate like draw a straight line between two points you have like all sorts of things happening I was wondering if there's like a I guess almost like a like a and I guess unsupervised not a random walk but a walk that would I guess lead to less perturbations I guess in in terms of like features I mean does it make sense uh yeah we I really wanted to do that for this project um maybe Ali can speak a little bit more about about his work there maybe after we stop this recording but um linearization of this is a huge over oversimplification um and that would be one of exactly what you describe as one of the things I'm most keen to try is taking non-linear walks uh through any of these subspaces um so very on point question haven't done it you should try and do it um but describing like this the semantic structure of latent space the semantic topology if you'll permit me that is a really interesting question um because even the visual meaning corresponding to some of these adjectives some of these words is not regularized or normalized in the latent space itself so if I take five steps in the festive direction it might take me five steps to get anything that will start to register to me as festive um but the walk size for a correspondingly large visual change so to speak in a different direction could be very different um so some transformations like making an image black and white this is anecdotal but you only have to go like one step in that direction and then we'll visualize the change almost immediately uh so we're not kind of we're not walking around in like a perceptually normalized space so to speak um and there hasn't been to my knowledge a lot of work that's addressed that everything's been a little bit at hawk um so thinking about semantic topology subspaces non-linear versus linear paths and how we can think about kind of the concept mesh underlying latent space for different generative models is extremely interesting to get to be a really cool area to do some working cool thank you yeah let's ask Ali about that figure once we pull off here um I've got one more thing to show you a quick example to hopefully spark more discussion unless anybody has anything specific about this project we can always come back all right oh geez well last thing I'm going to show you uh is still a beta and uh it's very it's very early and it's even thought development but it captures something um that I think is deeply interesting uh and I think might be interesting to you all um so the former method that I showed you for building shared vocabulary between humans and models relies heavily on language right and so we get some direction and we're able to share that between people and even to repeatedly use it to steer through model space um because we've given it a label right we've used language and you might even argue that you know that's constraining the space of what people can recognize in those initial sample directions because there might be some genus or quad aspects of of images that we don't really have words for um but are still like really recognizable or perhaps the verbal you know that the words you would use to describe something are like quite complex and you wouldn't type that into an annotation like on mechanical Turk maybe you'd want to describe the sky as like the sky you saw at your grandmother's house the day she passed away or some flowers as effervescent like latte foam or a sparkling drink but you're not going to type that into mechanical Turk and there's not a single word concept to capture it so that's going to get lost in the method I described and lost in a lot of kind of standard either annotation based or uh kind of hard coded direction search so I wanted to experiment with a way to capture and learn um directions without language and this is like deeply inspired by the steerability work of all these so you'll see a method here that is is similar to that in some sense but we're allowing the human to define the transformation that they want rather than pre-defining say a zoom or rotation transform using an algorithm we're allowing humans to come into the loop and define that transformation purely visually by interacting with very very small batches of images sampled from latent space or feature space at some layer and sort them into classes corresponding to some visual feature its presence or its absence and this provides a pipeline where users can steer just like in steerability work along dimensions that they discover however that they define and they define them purely visually so labeling what happened just as a matter of convenience but they're discovered um through vision so the way to do this is really simple um take some latent space again a lot of these examples are are using big GAN you could also use style GAN um take some latent space and sample images from it right if you're using a conditional model so we pick some category here we're looking at lakes inside big GAN image or big GAN places um sample some images for a user and then that user who's determining a visual dimension of interest kind of looks over that image space and sees if anything stands out to them across that that set of images so maybe here I noticed images that seemed kind of verdant and fertile uh and maybe more more spring light but not totally seasonal you'll see where I'm going it's kind of hard to describe and these were a little dreary or more wintry but there's not snow so it's not really winter they're just kind of less fertile and vivid so that's the distinction I want to make there um and the method is very simple just like the steerability work um and a another example of work from Bolle we define a transformation just by learning a hyperplane so training a SVM and learning a hyperplane it separates those two classes of images either in the latent space or in the feature space of some layer layers activations and then when we can steer some starting image in a direction that's normal to that hyperplane and steer it across those classes right so I could take an image that starts in the kind of dreary or domain or dusky or domain and transform it normally to that hyperplane and take it into the category of things that I thought was more verdant right or more fertile but I could specify that separating hyperplane just by sorting a shockingly few number of images um so we've done a couple of more like fine-grained tests here but just for proof of concept you can discern these directions with some degree of reliability with just like five to six examples of images in each category making it really simple to interact with something like this just by dragging and sorting a few images that are sampled in the latent space okay so there's a tiny example of a demo app we have for this um and we're switching where it's hosted so it's not online at this very moment but it will be next week but it's called the latent compass it was at NeurIPS Creativity I think last year the year before um you'll see the the home interface in a second but what we do is just what I said pick some category of BigGAN here it's BigGAN places um on the bottom you see images sampled from that category and the user drags them to the right and left of the screen corresponding to two different kind of categories of concepts they want to capture uh and then once the compass calibrates and we'll see that in a second and you can drag any new image and then transform it along that dimension so here we pick the closet category I've got full closets on the right empty closets on the left and the dimension I want to capture here is something like fullness so I'm going to see if I can I can learn a direction corresponding to the visual difference between these two categories drag any new closet onto that center line and transform it along that direction filling and emptying the closets and what if we tried a different category what if I wanted to turn a medina into a full closet right what is the type of fullness that's relevant to a medina oh well it's adding people instead of adding clothes suggesting that what's been learned there that direction in latent space is abstracting generalizable enough to capture some visually recognizable dimension of fullness that's meaningful to us in different scenes right and the model's able to to generalize it in a way that's not totally dependent on the types of objects it saw in one scene so it knows in a sense that clothes make a closet full but to make a market full we're not adding clothes we're adding people and so the fullness direction is something that adds more of whatever would make that scene full um to any scene that we're selecting in the model right and trained on so few examples of course this this is really quite imperfect but it's a good proof of concept of a way that users can interact super flexibly and really visually with dimensions of interest and use that to kind of explore and surf the latent space of a model by producing replicable repeatable directions that others can explore without having to use language that's kind of a different way of carving up the puzzle of how to explore and assign meaning to directions that we that we find in latent space okay that's at latentcompass.com and we'll be back up next week I think bad timing okay so to return to our frame here we've been digging a bit into this intersection between art neuroscience and machine learning ways to explore models that have been trained on human creation right at different scales to create a new to iterate and interpolate upon archives and then also to start to understand what these models are representing and if our ways of interpreting dimensions inside models can also teach us something about human perception or allow us to start to build models of aspects of human vision that are otherwise pretty intractable because it's difficult to formalize what dimensions underlie them where I could write down what dimensions under life physical scene understanding because I know Newton's laws I couldn't write down what dimensions underlie aesthetic perception of North African marketplaces or Babylonian tablets because I don't know what a large swath of people would find perceptually interesting in a bunch of marketplaces I know from cognitive science research certain heuristics to look for but that wouldn't give us a full set of what a diversity of humans might appreciate when looking at some scene especially things like its mood so we can turn here to these kinds of large unstructured generative models that learn entirely from data entirely from images and turn to them as like a fertile ground so to speak for starting to probe and represent human perceptual experiences inside their latent space and think of latent space that way right as a screen as I said before onto which we can project human experience and then once we have those projections we can rerun them and interact with them and collaborate with them to create outputs of deep generative models that are particularly exquisite and that represents some kind of collaboration between us and models of our our creation that are operating in parallel so that's where I will leave us my emails here I'm very discoverable online but you're welcome to write me questions anytime and I will wrap here and we can have a more casual discussion unless anybody has any last questions for this part Thank you so much sir this was really interesting and inspiring with all the acidic decreasing slides and every moment of that was really full of thoughts I think that you open a window to semantically and qualitatively looking at these latent spaces and sort of our imagination and where we dream and where these models that we create dream so I really appreciate that I'm going to stop recording and then see if there are more questions", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.52, "text": " Cool. Hello everyone, welcome to your course AI for Art, Esthetic and Creativity. Today", "tokens": [50364, 8561, 13, 2425, 1518, 11, 2928, 281, 428, 1164, 7318, 337, 5735, 11, 4410, 17151, 293, 11972, 4253, 13, 2692, 50840], "temperature": 0.0, "avg_logprob": -0.27392150509741997, "compression_ratio": 1.4171122994652405, "no_speech_prob": 0.027431461960077286}, {"id": 1, "seek": 0, "start": 9.52, "end": 19.2, "text": " we have a very special speaker. She has an excellent background in different domains and", "tokens": [50840, 321, 362, 257, 588, 2121, 8145, 13, 1240, 575, 364, 7103, 3678, 294, 819, 25514, 293, 51324], "temperature": 0.0, "avg_logprob": -0.27392150509741997, "compression_ratio": 1.4171122994652405, "no_speech_prob": 0.027431461960077286}, {"id": 2, "seek": 0, "start": 19.2, "end": 28.88, "text": " she will tell you hopefully more about herself and her work. Sarah is a great friend and", "tokens": [51324, 750, 486, 980, 291, 4696, 544, 466, 7530, 293, 720, 589, 13, 9519, 307, 257, 869, 1277, 293, 51808], "temperature": 0.0, "avg_logprob": -0.27392150509741997, "compression_ratio": 1.4171122994652405, "no_speech_prob": 0.027431461960077286}, {"id": 3, "seek": 2888, "start": 29.52, "end": 38.48, "text": " colleague of me and she kind of accepted to give us a lecture talk today. So from here I", "tokens": [50396, 13532, 295, 385, 293, 750, 733, 295, 9035, 281, 976, 505, 257, 7991, 751, 965, 13, 407, 490, 510, 286, 50844], "temperature": 0.0, "avg_logprob": -0.12355856577555338, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0030671770218759775}, {"id": 4, "seek": 2888, "start": 41.04, "end": 47.84, "text": " let Sarah to continue. Please go ahead. Thanks Ali. It's such a pleasure to be here. I've heard", "tokens": [50972, 718, 9519, 281, 2354, 13, 2555, 352, 2286, 13, 2561, 12020, 13, 467, 311, 1270, 257, 6834, 281, 312, 510, 13, 286, 600, 2198, 51312], "temperature": 0.0, "avg_logprob": -0.12355856577555338, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0030671770218759775}, {"id": 5, "seek": 2888, "start": 47.84, "end": 53.44, "text": " so much about this class. I don't think I have a slide about my background but I can tell you a", "tokens": [51312, 370, 709, 466, 341, 1508, 13, 286, 500, 380, 519, 286, 362, 257, 4137, 466, 452, 3678, 457, 286, 393, 980, 291, 257, 51592], "temperature": 0.0, "avg_logprob": -0.12355856577555338, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0030671770218759775}, {"id": 6, "seek": 5344, "start": 53.44, "end": 59.92, "text": " little bit about myself. I finished my PhD in neuroscience so across the street from Seasale", "tokens": [50364, 707, 857, 466, 2059, 13, 286, 4335, 452, 14476, 294, 42762, 370, 2108, 264, 4838, 490, 1100, 296, 1220, 50688], "temperature": 0.0, "avg_logprob": -0.09974936012909791, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.04078899323940277}, {"id": 7, "seek": 5344, "start": 59.92, "end": 65.92, "text": " last year and now I'm a postdoc in the vision group and the journey throughout my PhD was", "tokens": [50688, 1036, 1064, 293, 586, 286, 478, 257, 2183, 39966, 294, 264, 5201, 1594, 293, 264, 4671, 3710, 452, 14476, 390, 50988], "temperature": 0.0, "avg_logprob": -0.09974936012909791, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.04078899323940277}, {"id": 8, "seek": 5344, "start": 65.92, "end": 72.32, "text": " a little bit of a winding path. I started thinking about explicit symbolic models for things like", "tokens": [50988, 257, 707, 857, 295, 257, 29775, 3100, 13, 286, 1409, 1953, 466, 13691, 25755, 5245, 337, 721, 411, 51308], "temperature": 0.0, "avg_logprob": -0.09974936012909791, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.04078899323940277}, {"id": 9, "seek": 5344, "start": 72.32, "end": 77.36, "text": " physics and we'll talk a little bit more about that along the way. So modeling how the mind", "tokens": [51308, 10649, 293, 321, 603, 751, 257, 707, 857, 544, 466, 300, 2051, 264, 636, 13, 407, 15983, 577, 264, 1575, 51560], "temperature": 0.0, "avg_logprob": -0.09974936012909791, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.04078899323940277}, {"id": 10, "seek": 5344, "start": 77.36, "end": 82.47999999999999, "text": " makes inferences about things that we see but that hits a ceiling when we come up against", "tokens": [51560, 1669, 13596, 2667, 466, 721, 300, 321, 536, 457, 300, 8664, 257, 13655, 562, 321, 808, 493, 1970, 51816], "temperature": 0.0, "avg_logprob": -0.09974936012909791, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.04078899323940277}, {"id": 11, "seek": 8248, "start": 83.12, "end": 89.12, "text": " questions of vision and types of seeing like looking at art that is really difficult to develop", "tokens": [50396, 1651, 295, 5201, 293, 3467, 295, 2577, 411, 1237, 412, 1523, 300, 307, 534, 2252, 281, 1499, 50696], "temperature": 0.0, "avg_logprob": -0.08558301471528552, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.001808962901122868}, {"id": 12, "seek": 8248, "start": 89.12, "end": 94.88000000000001, "text": " some kind of computational formalism for that we don't have good models for. And at the same time", "tokens": [50696, 512, 733, 295, 28270, 9860, 1434, 337, 300, 321, 500, 380, 362, 665, 5245, 337, 13, 400, 412, 264, 912, 565, 50984], "temperature": 0.0, "avg_logprob": -0.08558301471528552, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.001808962901122868}, {"id": 13, "seek": 8248, "start": 94.88000000000001, "end": 99.28, "text": " as I was kind of hitting that wall in my own thinking I was developing a parallel interest", "tokens": [50984, 382, 286, 390, 733, 295, 8850, 300, 2929, 294, 452, 1065, 1953, 286, 390, 6416, 257, 8952, 1179, 51204], "temperature": 0.0, "avg_logprob": -0.08558301471528552, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.001808962901122868}, {"id": 14, "seek": 8248, "start": 99.84, "end": 104.64, "text": " in visual art and doing a lot of different projects both with individual artists and with", "tokens": [51232, 294, 5056, 1523, 293, 884, 257, 688, 295, 819, 4455, 1293, 365, 2609, 6910, 293, 365, 51472], "temperature": 0.0, "avg_logprob": -0.08558301471528552, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.001808962901122868}, {"id": 15, "seek": 8248, "start": 104.64, "end": 111.68, "text": " larger museum archives that I'll talk a little bit about and started to look at art as a ground for", "tokens": [51472, 4833, 8441, 25607, 300, 286, 603, 751, 257, 707, 857, 466, 293, 1409, 281, 574, 412, 1523, 382, 257, 2727, 337, 51824], "temperature": 0.0, "avg_logprob": -0.08558301471528552, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.001808962901122868}, {"id": 16, "seek": 11168, "start": 112.24000000000001, "end": 116.56, "text": " asking kind of difficult questions on the frontier of our thinking about the mind.", "tokens": [50392, 3365, 733, 295, 2252, 1651, 322, 264, 35853, 295, 527, 1953, 466, 264, 1575, 13, 50608], "temperature": 0.0, "avg_logprob": -0.07088236897080033, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.00024523670435883105}, {"id": 17, "seek": 11168, "start": 116.56, "end": 121.60000000000001, "text": " If we look at how humans create art and in view art can we understand something about", "tokens": [50608, 759, 321, 574, 412, 577, 6255, 1884, 1523, 293, 294, 1910, 1523, 393, 321, 1223, 746, 466, 50860], "temperature": 0.0, "avg_logprob": -0.07088236897080033, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.00024523670435883105}, {"id": 18, "seek": 11168, "start": 122.32000000000001, "end": 126.72000000000001, "text": " how they view the world and domains that we don't yet have good models of cognition for.", "tokens": [50896, 577, 436, 1910, 264, 1002, 293, 25514, 300, 321, 500, 380, 1939, 362, 665, 5245, 295, 46905, 337, 13, 51116], "temperature": 0.0, "avg_logprob": -0.07088236897080033, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.00024523670435883105}, {"id": 19, "seek": 11168, "start": 127.60000000000001, "end": 132.64000000000001, "text": " So I kind of started steering my my PhD in that direction. I'll share a little bit of that work", "tokens": [51160, 407, 286, 733, 295, 1409, 14823, 452, 452, 14476, 294, 300, 3513, 13, 286, 603, 2073, 257, 707, 857, 295, 300, 589, 51412], "temperature": 0.0, "avg_logprob": -0.07088236897080033, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.00024523670435883105}, {"id": 20, "seek": 11168, "start": 132.64000000000001, "end": 138.24, "text": " as well and as I said now I'm a postdoc with Antonio and Ellie asked me to share a little bit", "tokens": [51412, 382, 731, 293, 382, 286, 848, 586, 286, 478, 257, 2183, 39966, 365, 22527, 293, 27151, 2351, 385, 281, 2073, 257, 707, 857, 51692], "temperature": 0.0, "avg_logprob": -0.07088236897080033, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.00024523670435883105}, {"id": 21, "seek": 13824, "start": 138.24, "end": 145.84, "text": " of my my inspiration behind that path. I don't have a good story about a specific moment I think", "tokens": [50364, 295, 452, 452, 10249, 2261, 300, 3100, 13, 286, 500, 380, 362, 257, 665, 1657, 466, 257, 2685, 1623, 286, 519, 50744], "temperature": 0.0, "avg_logprob": -0.06812941486185248, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00020334951113909483}, {"id": 22, "seek": 13824, "start": 145.84, "end": 152.72, "text": " it's been a lifelong interest for me since I was super small and reading a lot of poetry I guess", "tokens": [50744, 309, 311, 668, 257, 27232, 1179, 337, 385, 1670, 286, 390, 1687, 1359, 293, 3760, 257, 688, 295, 15155, 286, 2041, 51088], "temperature": 0.0, "avg_logprob": -0.06812941486185248, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00020334951113909483}, {"id": 23, "seek": 13824, "start": 153.36, "end": 158.08, "text": " thinking about kind of the the origin and nature of structure and our experience of the world.", "tokens": [51120, 1953, 466, 733, 295, 264, 264, 4957, 293, 3687, 295, 3877, 293, 527, 1752, 295, 264, 1002, 13, 51356], "temperature": 0.0, "avg_logprob": -0.06812941486185248, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00020334951113909483}, {"id": 24, "seek": 13824, "start": 158.08, "end": 164.24, "text": " I know that's quite an abstract thing but the structure that we see in visual patterns where", "tokens": [51356, 286, 458, 300, 311, 1596, 364, 12649, 551, 457, 264, 3877, 300, 321, 536, 294, 5056, 8294, 689, 51664], "temperature": 0.0, "avg_logprob": -0.06812941486185248, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00020334951113909483}, {"id": 25, "seek": 16424, "start": 164.32000000000002, "end": 169.28, "text": " does that come from? Is that something that lives inherently in the brain and we imprint it on to", "tokens": [50368, 775, 300, 808, 490, 30, 1119, 300, 746, 300, 2909, 27993, 294, 264, 3567, 293, 321, 44615, 309, 322, 281, 50616], "temperature": 0.0, "avg_logprob": -0.09376031947585772, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.000570009695366025}, {"id": 26, "seek": 16424, "start": 169.28, "end": 173.68, "text": " kind of noisy and unordered stimuli or is it something that's external you know a nature", "tokens": [50616, 733, 295, 24518, 293, 517, 765, 4073, 47752, 420, 307, 309, 746, 300, 311, 8320, 291, 458, 257, 3687, 50836], "temperature": 0.0, "avg_logprob": -0.09376031947585772, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.000570009695366025}, {"id": 27, "seek": 16424, "start": 173.68, "end": 179.60000000000002, "text": " nurture question and then our brains kind of evolve to reflect and I got interested in this", "tokens": [50836, 41451, 1168, 293, 550, 527, 15442, 733, 295, 16693, 281, 5031, 293, 286, 658, 3102, 294, 341, 51132], "temperature": 0.0, "avg_logprob": -0.09376031947585772, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.000570009695366025}, {"id": 28, "seek": 16424, "start": 179.60000000000002, "end": 185.92000000000002, "text": " meeting point this kind of layer between the self and the world where all the action happens so to", "tokens": [51132, 3440, 935, 341, 733, 295, 4583, 1296, 264, 2698, 293, 264, 1002, 689, 439, 264, 3069, 2314, 370, 281, 51448], "temperature": 0.0, "avg_logprob": -0.09376031947585772, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.000570009695366025}, {"id": 29, "seek": 16424, "start": 185.92000000000002, "end": 194.0, "text": " speak and had training in applied math before I came to MIT and would think about ways to describe", "tokens": [51448, 1710, 293, 632, 3097, 294, 6456, 5221, 949, 286, 1361, 281, 13100, 293, 576, 519, 466, 2098, 281, 6786, 51852], "temperature": 0.0, "avg_logprob": -0.09376031947585772, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.000570009695366025}, {"id": 30, "seek": 19400, "start": 194.0, "end": 198.72, "text": " kind of structured inputs to processing systems and understand something about the structure of", "tokens": [50364, 733, 295, 18519, 15743, 281, 9007, 3652, 293, 1223, 746, 466, 264, 3877, 295, 50600], "temperature": 0.0, "avg_logprob": -0.04851357638835907, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.00019104436796624213}, {"id": 31, "seek": 19400, "start": 198.72, "end": 204.0, "text": " external inputs and then my neuroscience background learned a little bit how to how to", "tokens": [50600, 8320, 15743, 293, 550, 452, 42762, 3678, 3264, 257, 707, 857, 577, 281, 577, 281, 50864], "temperature": 0.0, "avg_logprob": -0.04851357638835907, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.00019104436796624213}, {"id": 32, "seek": 19400, "start": 204.0, "end": 208.32, "text": " think about and model the structure of a processing system right the structure of different parts of", "tokens": [50864, 519, 466, 293, 2316, 264, 3877, 295, 257, 9007, 1185, 558, 264, 3877, 295, 819, 3166, 295, 51080], "temperature": 0.0, "avg_logprob": -0.04851357638835907, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.00019104436796624213}, {"id": 33, "seek": 19400, "start": 208.32, "end": 213.84, "text": " the brain and it's really been through my interest in visual art that we can start to think about", "tokens": [51080, 264, 3567, 293, 309, 311, 534, 668, 807, 452, 1179, 294, 5056, 1523, 300, 321, 393, 722, 281, 519, 466, 51356], "temperature": 0.0, "avg_logprob": -0.04851357638835907, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.00019104436796624213}, {"id": 34, "seek": 19400, "start": 213.84, "end": 219.68, "text": " and describe what happens when those two things meet and how we synthesize our world of visual", "tokens": [51356, 293, 6786, 437, 2314, 562, 729, 732, 721, 1677, 293, 577, 321, 26617, 1125, 527, 1002, 295, 5056, 51648], "temperature": 0.0, "avg_logprob": -0.04851357638835907, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.00019104436796624213}, {"id": 35, "seek": 21968, "start": 219.68, "end": 225.04000000000002, "text": " experience in domains related to art and then other kind of higher level aspects of cognition", "tokens": [50364, 1752, 294, 25514, 4077, 281, 1523, 293, 550, 661, 733, 295, 2946, 1496, 7270, 295, 46905, 50632], "temperature": 0.0, "avg_logprob": -0.056133975153384, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0010001109912991524}, {"id": 36, "seek": 21968, "start": 225.04000000000002, "end": 231.68, "text": " like scenes or associations with moods of scenes and that kind of thing so that's that's where I am", "tokens": [50632, 411, 8026, 420, 26597, 365, 9268, 82, 295, 8026, 293, 300, 733, 295, 551, 370, 300, 311, 300, 311, 689, 286, 669, 50964], "temperature": 0.0, "avg_logprob": -0.056133975153384, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0010001109912991524}, {"id": 37, "seek": 21968, "start": 231.68, "end": 238.72, "text": " now and I think I'd like to start us off unless anybody has any leading questions about where I", "tokens": [50964, 586, 293, 286, 519, 286, 1116, 411, 281, 722, 505, 766, 5969, 4472, 575, 604, 5775, 1651, 466, 689, 286, 51316], "temperature": 0.0, "avg_logprob": -0.056133975153384, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0010001109912991524}, {"id": 38, "seek": 21968, "start": 238.72, "end": 245.12, "text": " come from with kind of a provocation and you can you can think of this as a frame for what I'll share", "tokens": [51316, 808, 490, 365, 733, 295, 257, 24568, 399, 293, 291, 393, 291, 393, 519, 295, 341, 382, 257, 3920, 337, 437, 286, 603, 2073, 51636], "temperature": 0.0, "avg_logprob": -0.056133975153384, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0010001109912991524}, {"id": 39, "seek": 24512, "start": 245.12, "end": 251.20000000000002, "text": " today but it's intended to be provocative and so the statement I'll make is that visual perception", "tokens": [50364, 965, 457, 309, 311, 10226, 281, 312, 47663, 293, 370, 264, 5629, 286, 603, 652, 307, 300, 5056, 12860, 50668], "temperature": 0.0, "avg_logprob": -0.06796705417143993, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.007573334034532309}, {"id": 40, "seek": 24512, "start": 251.20000000000002, "end": 257.52, "text": " itself human perception which we attempt to mirror and model in computer vision and computer science", "tokens": [50668, 2564, 1952, 12860, 597, 321, 5217, 281, 8013, 293, 2316, 294, 3820, 5201, 293, 3820, 3497, 50984], "temperature": 0.0, "avg_logprob": -0.06796705417143993, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.007573334034532309}, {"id": 41, "seek": 24512, "start": 257.52, "end": 263.28000000000003, "text": " in some cases that human perception is something that's fundamentally constructive and I say that", "tokens": [50984, 294, 512, 3331, 300, 1952, 12860, 307, 746, 300, 311, 17879, 30223, 293, 286, 584, 300, 51272], "temperature": 0.0, "avg_logprob": -0.06796705417143993, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.007573334034532309}, {"id": 42, "seek": 24512, "start": 263.28000000000003, "end": 269.36, "text": " because it solves an ill posed inverse problem like ones you've probably heard of before and", "tokens": [51272, 570, 309, 39890, 364, 3171, 31399, 17340, 1154, 411, 2306, 291, 600, 1391, 2198, 295, 949, 293, 51576], "temperature": 0.0, "avg_logprob": -0.06796705417143993, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.007573334034532309}, {"id": 43, "seek": 26936, "start": 269.36, "end": 276.08000000000004, "text": " doing that doing that solving requires a little bit of creativity so where am I coming from there", "tokens": [50364, 884, 300, 884, 300, 12606, 7029, 257, 707, 857, 295, 12915, 370, 689, 669, 286, 1348, 490, 456, 50700], "temperature": 0.0, "avg_logprob": -0.07815176412599896, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.0018096667481586337}, {"id": 44, "seek": 26936, "start": 276.08000000000004, "end": 282.0, "text": " the back of your eye as you know is is a 2d flat canvas right made up of a hierarchy of cells", "tokens": [50700, 264, 646, 295, 428, 3313, 382, 291, 458, 307, 307, 257, 568, 67, 4962, 16267, 558, 1027, 493, 295, 257, 22333, 295, 5438, 50996], "temperature": 0.0, "avg_logprob": -0.07815176412599896, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.0018096667481586337}, {"id": 45, "seek": 26936, "start": 282.0, "end": 287.84000000000003, "text": " that were visualized in in drawing in art by Ramonica Hall hundreds of years ago and are now", "tokens": [50996, 300, 645, 5056, 1602, 294, 294, 6316, 294, 1523, 538, 9078, 266, 2262, 5434, 6779, 295, 924, 2057, 293, 366, 586, 51288], "temperature": 0.0, "avg_logprob": -0.07815176412599896, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.0018096667481586337}, {"id": 46, "seek": 26936, "start": 287.84000000000003, "end": 292.88, "text": " visualized using electromagnetic imaging and we can get actually pretty fine brain detail", "tokens": [51288, 5056, 1602, 1228, 32214, 25036, 293, 321, 393, 483, 767, 1238, 2489, 3567, 2607, 51540], "temperature": 0.0, "avg_logprob": -0.07815176412599896, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.0018096667481586337}, {"id": 47, "seek": 26936, "start": 292.88, "end": 299.28000000000003, "text": " of the cells in the back of our eye that constitute a 2d canvas that takes in incoming image data", "tokens": [51540, 295, 264, 5438, 294, 264, 646, 295, 527, 3313, 300, 41658, 257, 568, 67, 16267, 300, 2516, 294, 22341, 3256, 1412, 51860], "temperature": 0.0, "avg_logprob": -0.07815176412599896, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.0018096667481586337}, {"id": 48, "seek": 29928, "start": 299.28, "end": 305.76, "text": " and represents images in terms of patterns of activations via this kind of mosaic of cells", "tokens": [50364, 293, 8855, 5267, 294, 2115, 295, 8294, 295, 2430, 763, 5766, 341, 733, 295, 275, 42261, 295, 5438, 50688], "temperature": 0.0, "avg_logprob": -0.06794901353767119, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.0004043664375785738}, {"id": 49, "seek": 29928, "start": 306.79999999999995, "end": 312.96, "text": " yet we experience this richly 3d world so there's a setup of a problem that you've probably heard", "tokens": [50740, 1939, 321, 1752, 341, 4593, 356, 805, 67, 1002, 370, 456, 311, 257, 8657, 295, 257, 1154, 300, 291, 600, 1391, 2198, 51048], "temperature": 0.0, "avg_logprob": -0.06794901353767119, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.0004043664375785738}, {"id": 50, "seek": 29928, "start": 312.96, "end": 319.44, "text": " before right 2d canvas but we have 3d rich experience scenes have depth objects have 3d shape", "tokens": [51048, 949, 558, 568, 67, 16267, 457, 321, 362, 805, 67, 4593, 1752, 8026, 362, 7161, 6565, 362, 805, 67, 3909, 51372], "temperature": 0.0, "avg_logprob": -0.06794901353767119, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.0004043664375785738}, {"id": 51, "seek": 29928, "start": 319.44, "end": 323.28, "text": " and furthermore what we see carries lots of different meanings and associations", "tokens": [51372, 293, 3052, 3138, 437, 321, 536, 16402, 3195, 295, 819, 28138, 293, 26597, 51564], "temperature": 0.0, "avg_logprob": -0.06794901353767119, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.0004043664375785738}, {"id": 52, "seek": 32328, "start": 323.91999999999996, "end": 329.91999999999996, "text": " so where is all of that kind of higher level information in a 2d image classical kind of", "tokens": [50396, 370, 689, 307, 439, 295, 300, 733, 295, 2946, 1496, 1589, 294, 257, 568, 67, 3256, 13735, 733, 295, 50696], "temperature": 0.0, "avg_logprob": -0.08165239606584822, "compression_ratio": 1.7786259541984732, "no_speech_prob": 0.0064865476451814175}, {"id": 53, "seek": 32328, "start": 329.91999999999996, "end": 336.64, "text": " computer vision problems we look at this kind of painting by Suzanne you might not only recognize", "tokens": [50696, 3820, 5201, 2740, 321, 574, 412, 341, 733, 295, 5370, 538, 48901, 291, 1062, 406, 787, 5521, 51032], "temperature": 0.0, "avg_logprob": -0.08165239606584822, "compression_ratio": 1.7786259541984732, "no_speech_prob": 0.0064865476451814175}, {"id": 54, "seek": 32328, "start": 336.64, "end": 343.52, "text": " 3d structure of this cottage on the mountain side right even though the image itself is 2d", "tokens": [51032, 805, 67, 3877, 295, 341, 37209, 322, 264, 6937, 1252, 558, 754, 1673, 264, 3256, 2564, 307, 568, 67, 51376], "temperature": 0.0, "avg_logprob": -0.08165239606584822, "compression_ratio": 1.7786259541984732, "no_speech_prob": 0.0064865476451814175}, {"id": 55, "seek": 32328, "start": 343.52, "end": 347.91999999999996, "text": " I might have all sorts of associations with it I might be able to say oh it's spring time", "tokens": [51376, 286, 1062, 362, 439, 7527, 295, 26597, 365, 309, 286, 1062, 312, 1075, 281, 584, 1954, 309, 311, 5587, 565, 51596], "temperature": 0.0, "avg_logprob": -0.08165239606584822, "compression_ratio": 1.7786259541984732, "no_speech_prob": 0.0064865476451814175}, {"id": 56, "seek": 32328, "start": 347.91999999999996, "end": 352.4, "text": " think something about the time of year I might even be able to infer something about the geography", "tokens": [51596, 519, 746, 466, 264, 565, 295, 1064, 286, 1062, 754, 312, 1075, 281, 13596, 746, 466, 264, 26695, 51820], "temperature": 0.0, "avg_logprob": -0.08165239606584822, "compression_ratio": 1.7786259541984732, "no_speech_prob": 0.0064865476451814175}, {"id": 57, "seek": 35240, "start": 352.4, "end": 358.4, "text": " by the palette used to convey what fields might be there think a little bit about the landscape", "tokens": [50364, 538, 264, 15851, 1143, 281, 16965, 437, 7909, 1062, 312, 456, 519, 257, 707, 857, 466, 264, 9661, 50664], "temperature": 0.0, "avg_logprob": -0.05981050761400071, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.00048779131611809134}, {"id": 58, "seek": 35240, "start": 358.4, "end": 365.03999999999996, "text": " I might be able to appreciate depth in pictorial space so even on this 2d plane if I put my mouse", "tokens": [50664, 286, 1062, 312, 1075, 281, 4449, 7161, 294, 2317, 5181, 1901, 370, 754, 322, 341, 568, 67, 5720, 498, 286, 829, 452, 9719, 50996], "temperature": 0.0, "avg_logprob": -0.05981050761400071, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.00048779131611809134}, {"id": 59, "seek": 35240, "start": 365.03999999999996, "end": 371.12, "text": " up here in the front maybe these fields are closer to me as a viewer than these ones that are far away", "tokens": [50996, 493, 510, 294, 264, 1868, 1310, 613, 7909, 366, 4966, 281, 385, 382, 257, 16767, 813, 613, 2306, 300, 366, 1400, 1314, 51300], "temperature": 0.0, "avg_logprob": -0.05981050761400071, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.00048779131611809134}, {"id": 60, "seek": 35240, "start": 371.12, "end": 375.12, "text": " but once again I'm just looking at a flat picture where is all of that information", "tokens": [51300, 457, 1564, 797, 286, 478, 445, 1237, 412, 257, 4962, 3036, 689, 307, 439, 295, 300, 1589, 51500], "temperature": 0.0, "avg_logprob": -0.05981050761400071, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.00048779131611809134}, {"id": 61, "seek": 35240, "start": 376.32, "end": 381.35999999999996, "text": " our brain has to solve an inverse problem like this anytime we look at a visual scene it has to get", "tokens": [51560, 527, 3567, 575, 281, 5039, 364, 17340, 1154, 411, 341, 13038, 321, 574, 412, 257, 5056, 4145, 309, 575, 281, 483, 51812], "temperature": 0.0, "avg_logprob": -0.05981050761400071, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.00048779131611809134}, {"id": 62, "seek": 38136, "start": 381.36, "end": 387.28000000000003, "text": " from low two-dimensional information to kind of rich 3d but there's a fundamental problem here", "tokens": [50364, 490, 2295, 732, 12, 18759, 1589, 281, 733, 295, 4593, 805, 67, 457, 456, 311, 257, 8088, 1154, 510, 50660], "temperature": 0.0, "avg_logprob": -0.05421308370736929, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0011692114640027285}, {"id": 63, "seek": 38136, "start": 387.28000000000003, "end": 394.48, "text": " and I pointed to that is that infinitely many 3d objects can cause the same 2d project projection", "tokens": [50660, 293, 286, 10932, 281, 300, 307, 300, 36227, 867, 805, 67, 6565, 393, 3082, 264, 912, 568, 67, 1716, 22743, 51020], "temperature": 0.0, "avg_logprob": -0.05421308370736929, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0011692114640027285}, {"id": 64, "seek": 38136, "start": 394.48, "end": 399.68, "text": " that's the under constrained nature of this inverse problem that vision poses and you've", "tokens": [51020, 300, 311, 264, 833, 38901, 3687, 295, 341, 17340, 1154, 300, 5201, 26059, 293, 291, 600, 51280], "temperature": 0.0, "avg_logprob": -0.05421308370736929, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0011692114640027285}, {"id": 65, "seek": 38136, "start": 399.68, "end": 405.84000000000003, "text": " experienced this quite explicitly anytime you've seen a shadow and not the object causing the", "tokens": [51280, 6751, 341, 1596, 20803, 13038, 291, 600, 1612, 257, 8576, 293, 406, 264, 2657, 9853, 264, 51588], "temperature": 0.0, "avg_logprob": -0.05421308370736929, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0011692114640027285}, {"id": 66, "seek": 38136, "start": 405.84000000000003, "end": 410.72, "text": " shadow right and you've had to infer oh is that actually you know a monster on the wall or is", "tokens": [51588, 8576, 558, 293, 291, 600, 632, 281, 13596, 1954, 307, 300, 767, 291, 458, 257, 10090, 322, 264, 2929, 420, 307, 51832], "temperature": 0.0, "avg_logprob": -0.05421308370736929, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0011692114640027285}, {"id": 67, "seek": 41072, "start": 410.72, "end": 416.72, "text": " that somebody's hand being projected but there are infinitely many configurations in three dimensions", "tokens": [50364, 300, 2618, 311, 1011, 885, 26231, 457, 456, 366, 36227, 867, 31493, 294, 1045, 12819, 50664], "temperature": 0.0, "avg_logprob": -0.077000549861363, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.0001313332177232951}, {"id": 68, "seek": 41072, "start": 416.72, "end": 421.20000000000005, "text": " that could be projected downwards onto two dimensions and cause some configuration in", "tokens": [50664, 300, 727, 312, 26231, 39880, 3911, 732, 12819, 293, 3082, 512, 11694, 294, 50888], "temperature": 0.0, "avg_logprob": -0.077000549861363, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.0001313332177232951}, {"id": 69, "seek": 41072, "start": 421.20000000000005, "end": 427.92, "text": " pictorial space so how do we constrain that problem when we're solving for what we see", "tokens": [50888, 2317, 5181, 1901, 370, 577, 360, 321, 1817, 7146, 300, 1154, 562, 321, 434, 12606, 337, 437, 321, 536, 51224], "temperature": 0.0, "avg_logprob": -0.077000549861363, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.0001313332177232951}, {"id": 70, "seek": 41072, "start": 428.72, "end": 433.92, "text": " right so this problem is ill-posed because it has as I said many infinitely many possible solutions", "tokens": [51264, 558, 370, 341, 1154, 307, 3171, 12, 79, 1744, 570, 309, 575, 382, 286, 848, 867, 36227, 867, 1944, 6547, 51524], "temperature": 0.0, "avg_logprob": -0.077000549861363, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.0001313332177232951}, {"id": 71, "seek": 43392, "start": 434.64000000000004, "end": 440.56, "text": " and choosing between them requires some additional information and in the case of the brain", "tokens": [50400, 293, 10875, 1296, 552, 7029, 512, 4497, 1589, 293, 294, 264, 1389, 295, 264, 3567, 50696], "temperature": 0.0, "avg_logprob": -0.052559859222835965, "compression_ratio": 1.8374384236453203, "no_speech_prob": 0.024784518405795097}, {"id": 72, "seek": 43392, "start": 441.28000000000003, "end": 446.24, "text": " modern neuroscience understands this as requiring the brain to construct something so that's what I", "tokens": [50732, 4363, 42762, 15146, 341, 382, 24165, 264, 3567, 281, 7690, 746, 370, 300, 311, 437, 286, 50980], "temperature": 0.0, "avg_logprob": -0.052559859222835965, "compression_ratio": 1.8374384236453203, "no_speech_prob": 0.024784518405795097}, {"id": 73, "seek": 43392, "start": 446.24, "end": 452.48, "text": " mean when I say perception is fundamentally constructive or creative it requires the brain", "tokens": [50980, 914, 562, 286, 584, 12860, 307, 17879, 30223, 420, 5880, 309, 7029, 264, 3567, 51292], "temperature": 0.0, "avg_logprob": -0.052559859222835965, "compression_ratio": 1.8374384236453203, "no_speech_prob": 0.024784518405795097}, {"id": 74, "seek": 43392, "start": 452.48, "end": 459.12, "text": " to construct a best explanation of what it's seeing of incoming information and if we call", "tokens": [51292, 281, 7690, 257, 1151, 10835, 295, 437, 309, 311, 2577, 295, 22341, 1589, 293, 498, 321, 818, 51624], "temperature": 0.0, "avg_logprob": -0.052559859222835965, "compression_ratio": 1.8374384236453203, "no_speech_prob": 0.024784518405795097}, {"id": 75, "seek": 45912, "start": 459.12, "end": 464.32, "text": " that perception then maybe you'll permit me to make a bit of a stretch and say that that makes", "tokens": [50364, 300, 12860, 550, 1310, 291, 603, 13423, 385, 281, 652, 257, 857, 295, 257, 5985, 293, 584, 300, 300, 1669, 50624], "temperature": 0.0, "avg_logprob": -0.10406374716543937, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0015725635457783937}, {"id": 76, "seek": 45912, "start": 464.32, "end": 471.92, "text": " perception itself an act of creation or an act of synthesis of a scene so one kind of popular", "tokens": [50624, 12860, 2564, 364, 605, 295, 8016, 420, 364, 605, 295, 30252, 295, 257, 4145, 370, 472, 733, 295, 3743, 51004], "temperature": 0.0, "avg_logprob": -0.10406374716543937, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0015725635457783937}, {"id": 77, "seek": 45912, "start": 471.92, "end": 478.16, "text": " way to solve this inference problem is by using models of the world right and we can approach", "tokens": [51004, 636, 281, 5039, 341, 38253, 1154, 307, 538, 1228, 5245, 295, 264, 1002, 558, 293, 321, 393, 3109, 51316], "temperature": 0.0, "avg_logprob": -0.10406374716543937, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0015725635457783937}, {"id": 78, "seek": 45912, "start": 478.16, "end": 482.56, "text": " that from a Bayesian lens maybe you've seen the work of Josh Tinnenbaum in the bcs department", "tokens": [51316, 300, 490, 257, 7840, 42434, 6765, 1310, 291, 600, 1612, 264, 589, 295, 9785, 314, 11399, 46641, 294, 264, 272, 14368, 5882, 51536], "temperature": 0.0, "avg_logprob": -0.10406374716543937, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0015725635457783937}, {"id": 79, "seek": 45912, "start": 484.0, "end": 488.4, "text": " or maybe we can do that purely with deep learning it's kind of an attention that we could explore", "tokens": [51608, 420, 1310, 321, 393, 360, 300, 17491, 365, 2452, 2539, 309, 311, 733, 295, 364, 3202, 300, 321, 727, 6839, 51828], "temperature": 0.0, "avg_logprob": -0.10406374716543937, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0015725635457783937}, {"id": 80, "seek": 48840, "start": 488.96, "end": 494.79999999999995, "text": " later today but I'll give you an example here and this is let me back up for a second that if we", "tokens": [50392, 1780, 965, 457, 286, 603, 976, 291, 364, 1365, 510, 293, 341, 307, 718, 385, 646, 493, 337, 257, 1150, 300, 498, 321, 50684], "temperature": 0.0, "avg_logprob": -0.05697354816255115, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0012838194379583001}, {"id": 81, "seek": 48840, "start": 494.79999999999995, "end": 499.52, "text": " were in person this is the point where I would do kind of a live in person demo so I want you to", "tokens": [50684, 645, 294, 954, 341, 307, 264, 935, 689, 286, 576, 360, 733, 295, 257, 1621, 294, 954, 10723, 370, 286, 528, 291, 281, 50920], "temperature": 0.0, "avg_logprob": -0.05697354816255115, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0012838194379583001}, {"id": 82, "seek": 48840, "start": 499.52, "end": 505.03999999999996, "text": " imagine that we're all sitting kind of in a dark room or we're sitting in a studio space and out", "tokens": [50920, 3811, 300, 321, 434, 439, 3798, 733, 295, 294, 257, 2877, 1808, 420, 321, 434, 3798, 294, 257, 6811, 1901, 293, 484, 51196], "temperature": 0.0, "avg_logprob": -0.05697354816255115, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0012838194379583001}, {"id": 83, "seek": 48840, "start": 505.03999999999996, "end": 511.35999999999996, "text": " in front of you there is a table covered in black velvet and I've set some stuff on that table you", "tokens": [51196, 294, 1868, 295, 291, 456, 307, 257, 3199, 5343, 294, 2211, 41905, 293, 286, 600, 992, 512, 1507, 322, 300, 3199, 291, 51512], "temperature": 0.0, "avg_logprob": -0.05697354816255115, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0012838194379583001}, {"id": 84, "seek": 48840, "start": 511.35999999999996, "end": 518.0799999999999, "text": " don't know what it is I set it there when the lights were off and then I take a single line of red", "tokens": [51512, 500, 380, 458, 437, 309, 307, 286, 992, 309, 456, 562, 264, 5811, 645, 766, 293, 550, 286, 747, 257, 2167, 1622, 295, 2182, 51848], "temperature": 0.0, "avg_logprob": -0.05697354816255115, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0012838194379583001}, {"id": 85, "seek": 51808, "start": 518.08, "end": 523.44, "text": " laser light and I'm going to gradually sweep it over the scene so I'm constraining the visual", "tokens": [50364, 12530, 1442, 293, 286, 478, 516, 281, 13145, 22169, 309, 670, 264, 4145, 370, 286, 478, 11525, 1760, 264, 5056, 50632], "temperature": 0.0, "avg_logprob": -0.03693209561434659, "compression_ratio": 1.8396946564885497, "no_speech_prob": 0.0002164924517273903}, {"id": 86, "seek": 51808, "start": 523.44, "end": 528.96, "text": " information you're going to receive about what's out there in the world to something kind of really", "tokens": [50632, 1589, 291, 434, 516, 281, 4774, 466, 437, 311, 484, 456, 294, 264, 1002, 281, 746, 733, 295, 534, 50908], "temperature": 0.0, "avg_logprob": -0.03693209561434659, "compression_ratio": 1.8396946564885497, "no_speech_prob": 0.0002164924517273903}, {"id": 87, "seek": 51808, "start": 528.96, "end": 534.32, "text": " low dimensional compared to what you normally get to understand kind of a garden of forms that would", "tokens": [50908, 2295, 18795, 5347, 281, 437, 291, 5646, 483, 281, 1223, 733, 295, 257, 7431, 295, 6422, 300, 576, 51176], "temperature": 0.0, "avg_logprob": -0.03693209561434659, "compression_ratio": 1.8396946564885497, "no_speech_prob": 0.0002164924517273903}, {"id": 88, "seek": 51808, "start": 534.32, "end": 539.2, "text": " be sitting on the table so imagine you're there in the studio with me and you see the following", "tokens": [51176, 312, 3798, 322, 264, 3199, 370, 3811, 291, 434, 456, 294, 264, 6811, 365, 385, 293, 291, 536, 264, 3480, 51420], "temperature": 0.0, "avg_logprob": -0.03693209561434659, "compression_ratio": 1.8396946564885497, "no_speech_prob": 0.0002164924517273903}, {"id": 89, "seek": 51808, "start": 539.84, "end": 544.24, "text": " you have to kind of infer what you see on the table maybe you could write it in the chat or", "tokens": [51452, 291, 362, 281, 733, 295, 13596, 437, 291, 536, 322, 264, 3199, 1310, 291, 727, 2464, 309, 294, 264, 5081, 420, 51672], "temperature": 0.0, "avg_logprob": -0.03693209561434659, "compression_ratio": 1.8396946564885497, "no_speech_prob": 0.0002164924517273903}, {"id": 90, "seek": 54424, "start": 544.24, "end": 550.08, "text": " just think to yourself when you see this give it a moment what's sitting here on the table", "tokens": [50364, 445, 519, 281, 1803, 562, 291, 536, 341, 976, 309, 257, 1623, 437, 311, 3798, 510, 322, 264, 3199, 50656], "temperature": 0.0, "avg_logprob": -0.09590333905713312, "compression_ratio": 1.596026490066225, "no_speech_prob": 9.914026304613799e-05}, {"id": 91, "seek": 54424, "start": 552.5600000000001, "end": 554.88, "text": " or what kinds of things what different things", "tokens": [50780, 420, 437, 3685, 295, 721, 437, 819, 721, 50896], "temperature": 0.0, "avg_logprob": -0.09590333905713312, "compression_ratio": 1.596026490066225, "no_speech_prob": 9.914026304613799e-05}, {"id": 92, "seek": 54424, "start": 557.6800000000001, "end": 564.24, "text": " maybe this would be a good use of the chat I can pull it up or you can describe features of what you see", "tokens": [51036, 1310, 341, 576, 312, 257, 665, 764, 295, 264, 5081, 286, 393, 2235, 309, 493, 420, 291, 393, 6786, 4122, 295, 437, 291, 536, 51364], "temperature": 0.0, "avg_logprob": -0.09590333905713312, "compression_ratio": 1.596026490066225, "no_speech_prob": 9.914026304613799e-05}, {"id": 93, "seek": 57424, "start": 574.5600000000001, "end": 581.12, "text": " a bunch of blocks on the table great there's something cubic up now I see the corner there right", "tokens": [50380, 257, 3840, 295, 8474, 322, 264, 3199, 869, 456, 311, 746, 28733, 493, 586, 286, 536, 264, 4538, 456, 558, 50708], "temperature": 0.0, "avg_logprob": -0.10720254213382037, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.0010320832952857018}, {"id": 94, "seek": 57424, "start": 582.8, "end": 588.88, "text": " multiple vases multiple forms with kind of different underlying shapes something cylindrical yep", "tokens": [50792, 3866, 371, 1957, 3866, 6422, 365, 733, 295, 819, 14217, 10854, 746, 28044, 15888, 18633, 51096], "temperature": 0.0, "avg_logprob": -0.10720254213382037, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.0010320832952857018}, {"id": 95, "seek": 57424, "start": 589.84, "end": 593.28, "text": " two things do you see I think there's a sphere actually there in the middle", "tokens": [51144, 732, 721, 360, 291, 536, 286, 519, 456, 311, 257, 16687, 767, 456, 294, 264, 2808, 51316], "temperature": 0.0, "avg_logprob": -0.10720254213382037, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.0010320832952857018}, {"id": 96, "seek": 57424, "start": 596.48, "end": 601.2, "text": " what is the experience of this light do you actually feel a physical corner when you see", "tokens": [51476, 437, 307, 264, 1752, 295, 341, 1442, 360, 291, 767, 841, 257, 4001, 4538, 562, 291, 536, 51712], "temperature": 0.0, "avg_logprob": -0.10720254213382037, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.0010320832952857018}, {"id": 97, "seek": 60120, "start": 602.08, "end": 604.5600000000001, "text": " bent light round the corner of that cube", "tokens": [50408, 14075, 1442, 3098, 264, 4538, 295, 300, 13728, 50532], "temperature": 0.0, "avg_logprob": -0.10161683559417725, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00040441678720526397}, {"id": 98, "seek": 60120, "start": 608.0, "end": 609.6800000000001, "text": " oh oh goodness", "tokens": [50704, 1954, 1954, 8387, 50788], "temperature": 0.0, "avg_logprob": -0.10161683559417725, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00040441678720526397}, {"id": 99, "seek": 60120, "start": 613.2, "end": 618.24, "text": " interacting with chat is a bit tough all right anyway the point I want to make here is that", "tokens": [50964, 18017, 365, 5081, 307, 257, 857, 4930, 439, 558, 4033, 264, 935, 286, 528, 281, 652, 510, 307, 300, 51216], "temperature": 0.0, "avg_logprob": -0.10161683559417725, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00040441678720526397}, {"id": 100, "seek": 60120, "start": 618.24, "end": 625.36, "text": " I can present really kind of low level information and you can if I dare open open the chat up again", "tokens": [51216, 286, 393, 1974, 534, 733, 295, 2295, 1496, 1589, 293, 291, 393, 498, 286, 8955, 1269, 1269, 264, 5081, 493, 797, 51572], "temperature": 0.0, "avg_logprob": -0.10161683559417725, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00040441678720526397}, {"id": 101, "seek": 60120, "start": 625.9200000000001, "end": 629.6800000000001, "text": " yeah there's a single base there's a single table and many forms sitting on top", "tokens": [51600, 1338, 456, 311, 257, 2167, 3096, 456, 311, 257, 2167, 3199, 293, 867, 6422, 3798, 322, 1192, 51788], "temperature": 0.0, "avg_logprob": -0.10161683559417725, "compression_ratio": 1.6565656565656566, "no_speech_prob": 0.00040441678720526397}, {"id": 102, "seek": 62968, "start": 629.76, "end": 634.2399999999999, "text": " that's right so there's just a tabletop and then lots of different shapes also covered in black", "tokens": [50368, 300, 311, 558, 370, 456, 311, 445, 257, 14136, 404, 293, 550, 3195, 295, 819, 10854, 611, 5343, 294, 2211, 50592], "temperature": 0.0, "avg_logprob": -0.05482997417449951, "compression_ratio": 1.8247011952191234, "no_speech_prob": 0.00037989349220879376}, {"id": 103, "seek": 62968, "start": 634.2399999999999, "end": 640.0799999999999, "text": " velvet so the light doesn't scatter and the light traces the outline of these 3d shapes", "tokens": [50592, 41905, 370, 264, 1442, 1177, 380, 34951, 293, 264, 1442, 26076, 264, 16387, 295, 613, 805, 67, 10854, 50884], "temperature": 0.0, "avg_logprob": -0.05482997417449951, "compression_ratio": 1.8247011952191234, "no_speech_prob": 0.00037989349220879376}, {"id": 104, "seek": 62968, "start": 640.64, "end": 646.56, "text": " and you can appreciate something about what the shapes are just by watching how light bends", "tokens": [50912, 293, 291, 393, 4449, 746, 466, 437, 264, 10854, 366, 445, 538, 1976, 577, 1442, 42990, 51208], "temperature": 0.0, "avg_logprob": -0.05482997417449951, "compression_ratio": 1.8247011952191234, "no_speech_prob": 0.00037989349220879376}, {"id": 105, "seek": 62968, "start": 646.56, "end": 652.56, "text": " around their surface and the relative motion as it traverses that facade right as it moves", "tokens": [51208, 926, 641, 3753, 293, 264, 4972, 5394, 382, 309, 23149, 279, 300, 46261, 558, 382, 309, 6067, 51508], "temperature": 0.0, "avg_logprob": -0.05482997417449951, "compression_ratio": 1.8247011952191234, "no_speech_prob": 0.00037989349220879376}, {"id": 106, "seek": 62968, "start": 652.56, "end": 658.0, "text": " over the surface of the sphere the light bends according to its curvature and I would argue", "tokens": [51508, 670, 264, 3753, 295, 264, 16687, 264, 1442, 42990, 4650, 281, 1080, 37638, 293, 286, 576, 9695, 51780], "temperature": 0.0, "avg_logprob": -0.05482997417449951, "compression_ratio": 1.8247011952191234, "no_speech_prob": 0.00037989349220879376}, {"id": 107, "seek": 65800, "start": 658.0, "end": 665.76, "text": " here that because you have some notion of what a sphere is and some notion of what a cube is", "tokens": [50364, 510, 300, 570, 291, 362, 512, 10710, 295, 437, 257, 16687, 307, 293, 512, 10710, 295, 437, 257, 13728, 307, 50752], "temperature": 0.0, "avg_logprob": -0.05460665865642268, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.00116929248906672}, {"id": 108, "seek": 65800, "start": 665.76, "end": 672.56, "text": " that is you have a relatively abstract model of these underlying shapes in your mind a mental", "tokens": [50752, 300, 307, 291, 362, 257, 7226, 12649, 2316, 295, 613, 14217, 10854, 294, 428, 1575, 257, 4973, 51092], "temperature": 0.0, "avg_logprob": -0.05460665865642268, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.00116929248906672}, {"id": 109, "seek": 65800, "start": 672.56, "end": 678.24, "text": " model you can do some inference when you see light move over their surface on this way even though", "tokens": [51092, 2316, 291, 393, 360, 512, 38253, 562, 291, 536, 1442, 1286, 670, 641, 3753, 322, 341, 636, 754, 1673, 51376], "temperature": 0.0, "avg_logprob": -0.05460665865642268, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.00116929248906672}, {"id": 110, "seek": 65800, "start": 679.12, "end": 684.0, "text": " I choose an example like this because you've probably never seen this example before right", "tokens": [51420, 286, 2826, 364, 1365, 411, 341, 570, 291, 600, 1391, 1128, 1612, 341, 1365, 949, 558, 51664], "temperature": 0.0, "avg_logprob": -0.05460665865642268, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.00116929248906672}, {"id": 111, "seek": 68400, "start": 684.8, "end": 688.32, "text": " never seen a single line of laser light move over this table surface", "tokens": [50404, 1128, 1612, 257, 2167, 1622, 295, 12530, 1442, 1286, 670, 341, 3199, 3753, 50580], "temperature": 0.0, "avg_logprob": -0.0644442013331822, "compression_ratio": 1.7421875, "no_speech_prob": 0.0004877869796473533}, {"id": 112, "seek": 68400, "start": 688.96, "end": 693.76, "text": " even this this kind of setup but you can still do that inference pretty well and you did in the chat", "tokens": [50612, 754, 341, 341, 733, 295, 8657, 457, 291, 393, 920, 360, 300, 38253, 1238, 731, 293, 291, 630, 294, 264, 5081, 50852], "temperature": 0.0, "avg_logprob": -0.0644442013331822, "compression_ratio": 1.7421875, "no_speech_prob": 0.0004877869796473533}, {"id": 113, "seek": 68400, "start": 695.12, "end": 700.4, "text": " so if you'll if you'll stay with me here I'm suggesting that this is an example just your", "tokens": [50920, 370, 498, 291, 603, 498, 291, 603, 1754, 365, 385, 510, 286, 478, 18094, 300, 341, 307, 364, 1365, 445, 428, 51184], "temperature": 0.0, "avg_logprob": -0.0644442013331822, "compression_ratio": 1.7421875, "no_speech_prob": 0.0004877869796473533}, {"id": 114, "seek": 68400, "start": 700.4, "end": 706.4, "text": " perceptually of how we can bring kind of models of the world and shapes and forms that comprise", "tokens": [51184, 43276, 671, 295, 577, 321, 393, 1565, 733, 295, 5245, 295, 264, 1002, 293, 10854, 293, 6422, 300, 16802, 908, 51484], "temperature": 0.0, "avg_logprob": -0.0644442013331822, "compression_ratio": 1.7421875, "no_speech_prob": 0.0004877869796473533}, {"id": 115, "seek": 68400, "start": 706.4, "end": 712.88, "text": " it to bear on simple visual stimuli and how we can even do that by using articulated light", "tokens": [51484, 309, 281, 6155, 322, 2199, 5056, 47752, 293, 577, 321, 393, 754, 360, 300, 538, 1228, 43322, 1442, 51808], "temperature": 0.0, "avg_logprob": -0.0644442013331822, "compression_ratio": 1.7421875, "no_speech_prob": 0.0004877869796473533}, {"id": 116, "seek": 71288, "start": 713.4399999999999, "end": 719.28, "text": " to isolate aspects of those stimuli and to kind of elucidate our perception to us", "tokens": [50392, 281, 25660, 7270, 295, 729, 47752, 293, 281, 733, 295, 806, 1311, 327, 473, 527, 12860, 281, 505, 50684], "temperature": 0.0, "avg_logprob": -0.06613292260603472, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.0006876421975903213}, {"id": 117, "seek": 71288, "start": 720.16, "end": 724.56, "text": " so we do a lot of things like this in the MIT museum studio where I teach the vision and art", "tokens": [50728, 370, 321, 360, 257, 688, 295, 721, 411, 341, 294, 264, 13100, 8441, 6811, 689, 286, 2924, 264, 5201, 293, 1523, 50948], "temperature": 0.0, "avg_logprob": -0.06613292260603472, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.0006876421975903213}, {"id": 118, "seek": 71288, "start": 724.56, "end": 729.28, "text": " neuroscience class which I'll talk about in a moment but that's where this this was filmed", "tokens": [50948, 42762, 1508, 597, 286, 603, 751, 466, 294, 257, 1623, 457, 300, 311, 689, 341, 341, 390, 15133, 51184], "temperature": 0.0, "avg_logprob": -0.06613292260603472, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.0006876421975903213}, {"id": 119, "seek": 71288, "start": 729.28, "end": 734.08, "text": " let's see if it'll let me advance even though I open the chat all right so another kind of setting", "tokens": [51184, 718, 311, 536, 498, 309, 603, 718, 385, 7295, 754, 1673, 286, 1269, 264, 5081, 439, 558, 370, 1071, 733, 295, 3287, 51424], "temperature": 0.0, "avg_logprob": -0.06613292260603472, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.0006876421975903213}, {"id": 120, "seek": 71288, "start": 734.08, "end": 739.68, "text": " in which we often hear and think about models of the world and this kind of inference is in", "tokens": [51424, 294, 597, 321, 2049, 1568, 293, 519, 466, 5245, 295, 264, 1002, 293, 341, 733, 295, 38253, 307, 294, 51704], "temperature": 0.0, "avg_logprob": -0.06613292260603472, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.0006876421975903213}, {"id": 121, "seek": 73968, "start": 739.68, "end": 745.5999999999999, "text": " intuitive physics and I bring this up because some of my background is also in this type of work", "tokens": [50364, 21769, 10649, 293, 286, 1565, 341, 493, 570, 512, 295, 452, 3678, 307, 611, 294, 341, 2010, 295, 589, 50660], "temperature": 0.0, "avg_logprob": -0.03866991902341937, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.005908884108066559}, {"id": 122, "seek": 73968, "start": 745.5999999999999, "end": 751.68, "text": " investigating how the brain represents physical properties like mass that it uses to reason", "tokens": [50660, 22858, 577, 264, 3567, 8855, 4001, 7221, 411, 2758, 300, 309, 4960, 281, 1778, 50964], "temperature": 0.0, "avg_logprob": -0.03866991902341937, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.005908884108066559}, {"id": 123, "seek": 73968, "start": 751.68, "end": 756.7199999999999, "text": " physically about the world right you would have to estimate the mass of this block that's falling", "tokens": [50964, 9762, 466, 264, 1002, 558, 291, 576, 362, 281, 12539, 264, 2758, 295, 341, 3461, 300, 311, 7440, 51216], "temperature": 0.0, "avg_logprob": -0.03866991902341937, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.005908884108066559}, {"id": 124, "seek": 73968, "start": 756.7199999999999, "end": 761.52, "text": " and making a depression on this pillow before you would know the right amount of grip force", "tokens": [51216, 293, 1455, 257, 10799, 322, 341, 18581, 949, 291, 576, 458, 264, 558, 2372, 295, 12007, 3464, 51456], "temperature": 0.0, "avg_logprob": -0.03866991902341937, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.005908884108066559}, {"id": 125, "seek": 73968, "start": 761.52, "end": 765.8399999999999, "text": " you would need to use to to reach in and pick it up without dropping it right and this is something", "tokens": [51456, 291, 576, 643, 281, 764, 281, 281, 2524, 294, 293, 1888, 309, 493, 1553, 13601, 309, 558, 293, 341, 307, 746, 51672], "temperature": 0.0, "avg_logprob": -0.03866991902341937, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.005908884108066559}, {"id": 126, "seek": 76584, "start": 765.84, "end": 771.36, "text": " we do incredibly automatically and it's a skill set we develop regularly from a very early age", "tokens": [50364, 321, 360, 6252, 6772, 293, 309, 311, 257, 5389, 992, 321, 1499, 11672, 490, 257, 588, 2440, 3205, 50640], "temperature": 0.0, "avg_logprob": -0.053594212473174675, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0029801346827298403}, {"id": 127, "seek": 76584, "start": 771.9200000000001, "end": 779.44, "text": " and I found that the brain represents properties like mass with an amount of abstraction and invariance", "tokens": [50668, 293, 286, 1352, 300, 264, 3567, 8855, 7221, 411, 2758, 365, 364, 2372, 295, 37765, 293, 33270, 719, 51044], "temperature": 0.0, "avg_logprob": -0.053594212473174675, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0029801346827298403}, {"id": 128, "seek": 76584, "start": 779.44, "end": 786.1600000000001, "text": " to the type of physical scene in which mass is revealed that would be necessary if mass like", "tokens": [51044, 281, 264, 2010, 295, 4001, 4145, 294, 597, 2758, 307, 9599, 300, 576, 312, 4818, 498, 2758, 411, 51380], "temperature": 0.0, "avg_logprob": -0.053594212473174675, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0029801346827298403}, {"id": 129, "seek": 76584, "start": 786.1600000000001, "end": 792.64, "text": " this were to be used as an input to an abstract generalized engine for physical simulation or", "tokens": [51380, 341, 645, 281, 312, 1143, 382, 364, 4846, 281, 364, 12649, 44498, 2848, 337, 4001, 16575, 420, 51704], "temperature": 0.0, "avg_logprob": -0.053594212473174675, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0029801346827298403}, {"id": 130, "seek": 79264, "start": 792.64, "end": 798.72, "text": " what we call a physics engine in computer graphics and simulation suggesting that there is", "tokens": [50364, 437, 321, 818, 257, 10649, 2848, 294, 3820, 11837, 293, 16575, 18094, 300, 456, 307, 50668], "temperature": 0.0, "avg_logprob": -0.0539832655916509, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.0003459297295194119}, {"id": 131, "seek": 79264, "start": 798.72, "end": 805.68, "text": " kind of some first evidence that the brain does use these kind of generalized simulation engines", "tokens": [50668, 733, 295, 512, 700, 4467, 300, 264, 3567, 775, 764, 613, 733, 295, 44498, 16575, 12982, 51016], "temperature": 0.0, "avg_logprob": -0.0539832655916509, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.0003459297295194119}, {"id": 132, "seek": 79264, "start": 805.68, "end": 811.52, "text": " to solve low-level inference problems like inferring mass because we can make some hypothesis about", "tokens": [51016, 281, 5039, 2295, 12, 12418, 38253, 2740, 411, 13596, 2937, 2758, 570, 321, 393, 652, 512, 17291, 466, 51308], "temperature": 0.0, "avg_logprob": -0.0539832655916509, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.0003459297295194119}, {"id": 133, "seek": 79264, "start": 811.52, "end": 816.3199999999999, "text": " the nature of the underlying representations it would need if it were to solve problems in this", "tokens": [51308, 264, 3687, 295, 264, 14217, 33358, 309, 576, 643, 498, 309, 645, 281, 5039, 2740, 294, 341, 51548], "temperature": 0.0, "avg_logprob": -0.0539832655916509, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.0003459297295194119}, {"id": 134, "seek": 79264, "start": 816.3199999999999, "end": 820.88, "text": " kind of way rather than by simple pattern matching or in a pixel based way where we would assume", "tokens": [51548, 733, 295, 636, 2831, 813, 538, 2199, 5102, 14324, 420, 294, 257, 19261, 2361, 636, 689, 321, 576, 6552, 51776], "temperature": 0.0, "avg_logprob": -0.0539832655916509, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.0003459297295194119}, {"id": 135, "seek": 82088, "start": 820.88, "end": 825.76, "text": " that the representation of mass would be quite different from scene to scene because the low", "tokens": [50364, 300, 264, 10290, 295, 2758, 576, 312, 1596, 819, 490, 4145, 281, 4145, 570, 264, 2295, 50608], "temperature": 0.0, "avg_logprob": -0.06701931327280372, "compression_ratio": 1.906614785992218, "no_speech_prob": 0.00029590693884529173}, {"id": 136, "seek": 82088, "start": 825.76, "end": 831.2, "text": " level visual data about the scene is different but in fact that's not what we find we find representations", "tokens": [50608, 1496, 5056, 1412, 466, 264, 4145, 307, 819, 457, 294, 1186, 300, 311, 406, 437, 321, 915, 321, 915, 33358, 50880], "temperature": 0.0, "avg_logprob": -0.06701931327280372, "compression_ratio": 1.906614785992218, "no_speech_prob": 0.00029590693884529173}, {"id": 137, "seek": 82088, "start": 831.2, "end": 835.84, "text": " of physical variables like mass and friction that generalize across any kind of physical scene", "tokens": [50880, 295, 4001, 9102, 411, 2758, 293, 17710, 300, 2674, 1125, 2108, 604, 733, 295, 4001, 4145, 51112], "temperature": 0.0, "avg_logprob": -0.06701931327280372, "compression_ratio": 1.906614785992218, "no_speech_prob": 0.00029590693884529173}, {"id": 138, "seek": 82088, "start": 836.4, "end": 841.12, "text": " that we test where we hold a lot of other different parameters constant right like object color", "tokens": [51140, 300, 321, 1500, 689, 321, 1797, 257, 688, 295, 661, 819, 9834, 5754, 558, 411, 2657, 2017, 51376], "temperature": 0.0, "avg_logprob": -0.06701931327280372, "compression_ratio": 1.906614785992218, "no_speech_prob": 0.00029590693884529173}, {"id": 139, "seek": 82088, "start": 842.16, "end": 846.88, "text": " and this suggests an account of physical reasoning in the brain that has been that has been studied", "tokens": [51428, 293, 341, 13409, 364, 2696, 295, 4001, 21577, 294, 264, 3567, 300, 575, 668, 300, 575, 668, 9454, 51664], "temperature": 0.0, "avg_logprob": -0.06701931327280372, "compression_ratio": 1.906614785992218, "no_speech_prob": 0.00029590693884529173}, {"id": 140, "seek": 84688, "start": 846.88, "end": 853.04, "text": " pretty extensively computational right and that we model via probabilistic simulations of a physics", "tokens": [50364, 1238, 32636, 28270, 558, 293, 300, 321, 2316, 5766, 31959, 3142, 35138, 295, 257, 10649, 50672], "temperature": 0.0, "avg_logprob": -0.08107404525463398, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.0015008302871137857}, {"id": 141, "seek": 84688, "start": 853.04, "end": 857.84, "text": " engine I don't think that video is going to play for us right but this is the kind of work when I", "tokens": [50672, 2848, 286, 500, 380, 519, 300, 960, 307, 516, 281, 862, 337, 505, 558, 457, 341, 307, 264, 733, 295, 589, 562, 286, 50912], "temperature": 0.0, "avg_logprob": -0.08107404525463398, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.0015008302871137857}, {"id": 142, "seek": 84688, "start": 857.84, "end": 864.24, "text": " was doing when that I was doing when I was writing down like explicit models of the world that could", "tokens": [50912, 390, 884, 562, 300, 286, 390, 884, 562, 286, 390, 3579, 760, 411, 13691, 5245, 295, 264, 1002, 300, 727, 51232], "temperature": 0.0, "avg_logprob": -0.08107404525463398, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.0015008302871137857}, {"id": 143, "seek": 84688, "start": 864.24, "end": 869.28, "text": " be inverted to explain something about underlying parameters we were using for vision and in that", "tokens": [51232, 312, 38969, 281, 2903, 746, 466, 14217, 9834, 321, 645, 1228, 337, 5201, 293, 294, 300, 51484], "temperature": 0.0, "avg_logprob": -0.08107404525463398, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.0015008302871137857}, {"id": 144, "seek": 84688, "start": 869.28, "end": 876.72, "text": " in this case those models were physical right but what about cases like like art where it's", "tokens": [51484, 294, 341, 1389, 729, 5245, 645, 4001, 558, 457, 437, 466, 3331, 411, 411, 1523, 689, 309, 311, 51856], "temperature": 0.0, "avg_logprob": -0.08107404525463398, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.0015008302871137857}, {"id": 145, "seek": 87672, "start": 876.8000000000001, "end": 882.32, "text": " difficult as I mentioned to develop some kind of computational formalism where we don't know the", "tokens": [50368, 2252, 382, 286, 2835, 281, 1499, 512, 733, 295, 28270, 9860, 1434, 689, 321, 500, 380, 458, 264, 50644], "temperature": 0.0, "avg_logprob": -0.08412190719887062, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.0007317220442928374}, {"id": 146, "seek": 87672, "start": 882.32, "end": 887.52, "text": " underlying model for instance how to create the Cezanne painting we saw in the beginning", "tokens": [50644, 14217, 2316, 337, 5197, 577, 281, 1884, 264, 383, 4371, 12674, 5370, 321, 1866, 294, 264, 2863, 50904], "temperature": 0.0, "avg_logprob": -0.08412190719887062, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.0007317220442928374}, {"id": 147, "seek": 87672, "start": 888.24, "end": 893.28, "text": " a priori right how do we even start what are the underlying dimensions we'd need to write down to", "tokens": [50940, 257, 4059, 72, 558, 577, 360, 321, 754, 722, 437, 366, 264, 14217, 12819, 321, 1116, 643, 281, 2464, 760, 281, 51192], "temperature": 0.0, "avg_logprob": -0.08412190719887062, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.0007317220442928374}, {"id": 148, "seek": 87672, "start": 893.28, "end": 901.12, "text": " either make sense of how we see things or how they're created so this whole area is kind of what we", "tokens": [51192, 2139, 652, 2020, 295, 577, 321, 536, 721, 420, 577, 436, 434, 2942, 370, 341, 1379, 1859, 307, 733, 295, 437, 321, 51584], "temperature": 0.0, "avg_logprob": -0.08412190719887062, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.0007317220442928374}, {"id": 149, "seek": 87672, "start": 901.12, "end": 905.6800000000001, "text": " dive into in that vision in art and neuroscience course so this is something you're interested in", "tokens": [51584, 9192, 666, 294, 300, 5201, 294, 1523, 293, 42762, 1164, 370, 341, 307, 746, 291, 434, 3102, 294, 51812], "temperature": 0.0, "avg_logprob": -0.08412190719887062, "compression_ratio": 1.7554744525547445, "no_speech_prob": 0.0007317220442928374}, {"id": 150, "seek": 90568, "start": 905.76, "end": 910.4, "text": " it's of course an unsolved problem but we spend the fall semester every year", "tokens": [50368, 309, 311, 295, 1164, 364, 2693, 29110, 1154, 457, 321, 3496, 264, 2100, 11894, 633, 1064, 50600], "temperature": 0.0, "avg_logprob": -0.059840414259168834, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.0003918906149920076}, {"id": 151, "seek": 90568, "start": 911.4399999999999, "end": 916.4799999999999, "text": " kind of delving into it through both neuroscience literature through art practice through computation", "tokens": [50652, 733, 295, 1103, 798, 666, 309, 807, 1293, 42762, 10394, 807, 1523, 3124, 807, 24903, 50904], "temperature": 0.0, "avg_logprob": -0.059840414259168834, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.0003918906149920076}, {"id": 152, "seek": 90568, "start": 916.4799999999999, "end": 922.4799999999999, "text": " and then through studio work so kind of hands-on experimentation with principles underlying vision", "tokens": [50904, 293, 550, 807, 6811, 589, 370, 733, 295, 2377, 12, 266, 37142, 365, 9156, 14217, 5201, 51204], "temperature": 0.0, "avg_logprob": -0.059840414259168834, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.0003918906149920076}, {"id": 153, "seek": 90568, "start": 922.4799999999999, "end": 927.52, "text": " that we then externalize and experience ourselves and try and visualize in artistic contexts", "tokens": [51204, 300, 321, 550, 8320, 1125, 293, 1752, 4175, 293, 853, 293, 23273, 294, 17090, 30628, 51456], "temperature": 0.0, "avg_logprob": -0.059840414259168834, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.0003918906149920076}, {"id": 154, "seek": 90568, "start": 929.4399999999999, "end": 932.8, "text": " to give you a little bit of a taste of that class we would look at", "tokens": [51552, 281, 976, 291, 257, 707, 857, 295, 257, 3939, 295, 300, 1508, 321, 576, 574, 412, 51720], "temperature": 0.0, "avg_logprob": -0.059840414259168834, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.0003918906149920076}, {"id": 155, "seek": 93280, "start": 933.3599999999999, "end": 938.56, "text": " these examples say by by an artist in minor white and ask if we were trying to set up", "tokens": [50392, 613, 5110, 584, 538, 538, 364, 5748, 294, 6696, 2418, 293, 1029, 498, 321, 645, 1382, 281, 992, 493, 50652], "temperature": 0.0, "avg_logprob": -0.07655734239622604, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.001283856458030641}, {"id": 156, "seek": 93280, "start": 939.3599999999999, "end": 945.52, "text": " a typical kind of describe a model and then invert it to understand vision setting you know what is", "tokens": [50692, 257, 7476, 733, 295, 6786, 257, 2316, 293, 550, 33966, 309, 281, 1223, 5201, 3287, 291, 458, 437, 307, 51000], "temperature": 0.0, "avg_logprob": -0.07655734239622604, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.001283856458030641}, {"id": 157, "seek": 93280, "start": 945.52, "end": 951.68, "text": " the veretical percept in either of these right if before we were considering mass of some object", "tokens": [51000, 264, 1306, 27800, 43276, 294, 2139, 295, 613, 558, 498, 949, 321, 645, 8079, 2758, 295, 512, 2657, 51308], "temperature": 0.0, "avg_logprob": -0.07655734239622604, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.001283856458030641}, {"id": 158, "seek": 93280, "start": 951.68, "end": 958.0, "text": " that the brain has to infer and then we can write down a physical law describing how mass plays into", "tokens": [51308, 300, 264, 3567, 575, 281, 13596, 293, 550, 321, 393, 2464, 760, 257, 4001, 2101, 16141, 577, 2758, 5749, 666, 51624], "temperature": 0.0, "avg_logprob": -0.07655734239622604, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.001283856458030641}, {"id": 159, "seek": 95800, "start": 958.08, "end": 962.72, "text": " action unfolding in a scene a law describing dynamics and then invert it to think about", "tokens": [50368, 3069, 44586, 294, 257, 4145, 257, 2101, 16141, 15679, 293, 550, 33966, 309, 281, 519, 466, 50600], "temperature": 0.0, "avg_logprob": -0.057533273157083765, "compression_ratio": 1.8007662835249043, "no_speech_prob": 0.0013668450992554426}, {"id": 160, "seek": 95800, "start": 962.72, "end": 967.12, "text": " how the brain represents mass what would the analog be here what would we write down as the", "tokens": [50600, 577, 264, 3567, 8855, 2758, 437, 576, 264, 16660, 312, 510, 437, 576, 321, 2464, 760, 382, 264, 50820], "temperature": 0.0, "avg_logprob": -0.057533273157083765, "compression_ratio": 1.8007662835249043, "no_speech_prob": 0.0013668450992554426}, {"id": 161, "seek": 95800, "start": 967.12, "end": 972.32, "text": " veretical percept you can share some thoughts in the chat that is also an exercise you could just", "tokens": [50820, 1306, 27800, 43276, 291, 393, 2073, 512, 4598, 294, 264, 5081, 300, 307, 611, 364, 5380, 291, 727, 445, 51080], "temperature": 0.0, "avg_logprob": -0.057533273157083765, "compression_ratio": 1.8007662835249043, "no_speech_prob": 0.0013668450992554426}, {"id": 162, "seek": 95800, "start": 972.32, "end": 978.56, "text": " do yourself right maybe here you can start to get it a shadow of something outside the window", "tokens": [51080, 360, 1803, 558, 1310, 510, 291, 393, 722, 281, 483, 309, 257, 8576, 295, 746, 2380, 264, 4910, 51392], "temperature": 0.0, "avg_logprob": -0.057533273157083765, "compression_ratio": 1.8007662835249043, "no_speech_prob": 0.0013668450992554426}, {"id": 163, "seek": 95800, "start": 979.12, "end": 984.96, "text": " I see a bike maybe a bike seat there that's kind of not the point kind of not trying to infer what", "tokens": [51420, 286, 536, 257, 5656, 1310, 257, 5656, 6121, 456, 300, 311, 733, 295, 406, 264, 935, 733, 295, 406, 1382, 281, 13596, 437, 51712], "temperature": 0.0, "avg_logprob": -0.057533273157083765, "compression_ratio": 1.8007662835249043, "no_speech_prob": 0.0013668450992554426}, {"id": 164, "seek": 98496, "start": 984.96, "end": 991.0400000000001, "text": " caused the specific physics of this this image you're kind of getting at something different", "tokens": [50364, 7008, 264, 2685, 10649, 295, 341, 341, 3256, 291, 434, 733, 295, 1242, 412, 746, 819, 50668], "temperature": 0.0, "avg_logprob": -0.050761821318645865, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0004877827304881066}, {"id": 165, "seek": 98496, "start": 991.0400000000001, "end": 997.2, "text": " and especially here what if the artist isn't around for us to ask anymore these are actual", "tokens": [50668, 293, 2318, 510, 437, 498, 264, 5748, 1943, 380, 926, 337, 505, 281, 1029, 3602, 613, 366, 3539, 50976], "temperature": 0.0, "avg_logprob": -0.050761821318645865, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0004877827304881066}, {"id": 166, "seek": 98496, "start": 997.2, "end": 1003.44, "text": " photographs right these are photographs of something but the act of looking at it isn't about", "tokens": [50976, 17649, 558, 613, 366, 17649, 295, 746, 457, 264, 605, 295, 1237, 412, 309, 1943, 380, 466, 51288], "temperature": 0.0, "avg_logprob": -0.050761821318645865, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0004877827304881066}, {"id": 167, "seek": 98496, "start": 1003.44, "end": 1008.5600000000001, "text": " inferring the underlying cause of the image it's about inferring something else sort of aesthetic", "tokens": [51288, 13596, 2937, 264, 14217, 3082, 295, 264, 3256, 309, 311, 466, 13596, 2937, 746, 1646, 1333, 295, 20092, 51544], "temperature": 0.0, "avg_logprob": -0.050761821318645865, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0004877827304881066}, {"id": 168, "seek": 98496, "start": 1008.5600000000001, "end": 1013.9200000000001, "text": " parameters that define visual experience or kind of render visual experience at a lot higher of a", "tokens": [51544, 9834, 300, 6964, 5056, 1752, 420, 733, 295, 15529, 5056, 1752, 412, 257, 688, 2946, 295, 257, 51812], "temperature": 0.0, "avg_logprob": -0.050761821318645865, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0004877827304881066}, {"id": 169, "seek": 101392, "start": 1013.92, "end": 1020.0, "text": " level how do we begin to get traction on problems like this either in seeing or in or in generation", "tokens": [50364, 1496, 577, 360, 321, 1841, 281, 483, 23558, 322, 2740, 411, 341, 2139, 294, 2577, 420, 294, 420, 294, 5125, 50668], "temperature": 0.0, "avg_logprob": -0.06830350557963054, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000755179557017982}, {"id": 170, "seek": 101392, "start": 1020.8, "end": 1026.0, "text": " as I said you know in art we also come up against a great difficulty in that you know there are", "tokens": [50708, 382, 286, 848, 291, 458, 294, 1523, 321, 611, 808, 493, 1970, 257, 869, 10360, 294, 300, 291, 458, 456, 366, 50968], "temperature": 0.0, "avg_logprob": -0.06830350557963054, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000755179557017982}, {"id": 171, "seek": 101392, "start": 1026.0, "end": 1032.32, "text": " infinitely many ways to render recognizable depictions of common objects right with all", "tokens": [50968, 36227, 867, 2098, 281, 15529, 40757, 1367, 15607, 295, 2689, 6565, 558, 365, 439, 51284], "temperature": 0.0, "avg_logprob": -0.06830350557963054, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000755179557017982}, {"id": 172, "seek": 101392, "start": 1032.32, "end": 1038.1599999999999, "text": " sorts of idiosyncrasies illusory boundaries difficult for models to detect but we recognize", "tokens": [51284, 7527, 295, 4496, 2717, 34015, 3906, 530, 3171, 301, 827, 13180, 2252, 337, 5245, 281, 5531, 457, 321, 5521, 51576], "temperature": 0.0, "avg_logprob": -0.06830350557963054, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.000755179557017982}, {"id": 173, "seek": 103816, "start": 1038.16, "end": 1044.8000000000002, "text": " a woman in these images with the dress almost instantaneously and similarly we come up against", "tokens": [50364, 257, 3059, 294, 613, 5267, 365, 264, 5231, 1920, 9836, 13131, 293, 14138, 321, 808, 493, 1970, 50696], "temperature": 0.0, "avg_logprob": -0.0684261071054559, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0024714451283216476}, {"id": 174, "seek": 103816, "start": 1044.8000000000002, "end": 1050.5600000000002, "text": " another under constrained inverse problem is in that there's infinitely many ways to render and", "tokens": [50696, 1071, 833, 38901, 17340, 1154, 307, 294, 300, 456, 311, 36227, 867, 2098, 281, 15529, 293, 50984], "temperature": 0.0, "avg_logprob": -0.0684261071054559, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0024714451283216476}, {"id": 175, "seek": 103816, "start": 1050.5600000000002, "end": 1057.28, "text": " depict kind of abstractions of commonly recognizable forms which again are difficult for", "tokens": [50984, 31553, 733, 295, 12649, 626, 295, 12719, 40757, 6422, 597, 797, 366, 2252, 337, 51320], "temperature": 0.0, "avg_logprob": -0.0684261071054559, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0024714451283216476}, {"id": 176, "seek": 103816, "start": 1057.28, "end": 1062.88, "text": " current day models but they're pretty easy for us I can recognize a figure and maybe have different", "tokens": [51320, 2190, 786, 5245, 457, 436, 434, 1238, 1858, 337, 505, 286, 393, 5521, 257, 2573, 293, 1310, 362, 819, 51600], "temperature": 0.0, "avg_logprob": -0.0684261071054559, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0024714451283216476}, {"id": 177, "seek": 106288, "start": 1062.88, "end": 1069.0400000000002, "text": " associations with it in each of these different images so we think a little bit about this", "tokens": [50364, 26597, 365, 309, 294, 1184, 295, 613, 819, 5267, 370, 321, 519, 257, 707, 857, 466, 341, 50672], "temperature": 0.0, "avg_logprob": -0.08644421716754357, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.0030736015178263187}, {"id": 178, "seek": 106288, "start": 1069.7600000000002, "end": 1075.2800000000002, "text": " in the course like I mentioned you can ask me a bit after this talk as well if you're if you're", "tokens": [50708, 294, 264, 1164, 411, 286, 2835, 291, 393, 1029, 385, 257, 857, 934, 341, 751, 382, 731, 498, 291, 434, 498, 291, 434, 50984], "temperature": 0.0, "avg_logprob": -0.08644421716754357, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.0030736015178263187}, {"id": 179, "seek": 106288, "start": 1075.2800000000002, "end": 1081.5200000000002, "text": " interested in it it's called vision and art and neuroscience all of our info is is online most", "tokens": [50984, 3102, 294, 309, 309, 311, 1219, 5201, 293, 1523, 293, 42762, 439, 295, 527, 13614, 307, 307, 2950, 881, 51296], "temperature": 0.0, "avg_logprob": -0.08644421716754357, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.0030736015178263187}, {"id": 180, "seek": 106288, "start": 1081.5200000000002, "end": 1088.96, "text": " of the syllabus past exhibition catalogs at vision.mit.edu it's offered through through bcs", "tokens": [51296, 295, 264, 48077, 1791, 14414, 19746, 82, 412, 5201, 13, 3508, 13, 22938, 309, 311, 8059, 807, 807, 272, 14368, 51668], "temperature": 0.0, "avg_logprob": -0.08644421716754357, "compression_ratio": 1.6651785714285714, "no_speech_prob": 0.0030736015178263187}, {"id": 181, "seek": 108896, "start": 1089.8400000000001, "end": 1095.2, "text": " and as I said we we investigate during half the class in the seminar portion of the class", "tokens": [50408, 293, 382, 286, 848, 321, 321, 15013, 1830, 1922, 264, 1508, 294, 264, 29235, 8044, 295, 264, 1508, 50676], "temperature": 0.0, "avg_logprob": -0.06536313382590689, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.004195835907012224}, {"id": 182, "seek": 108896, "start": 1095.76, "end": 1099.52, "text": " kind of the underlying principles of vision and we work through a series of modules", "tokens": [50704, 733, 295, 264, 14217, 9156, 295, 5201, 293, 321, 589, 807, 257, 2638, 295, 16679, 50892], "temperature": 0.0, "avg_logprob": -0.06536313382590689, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.004195835907012224}, {"id": 183, "seek": 108896, "start": 1100.32, "end": 1107.28, "text": " that build up visual processes from early level like v1 visual processing all the way up to kind", "tokens": [50932, 300, 1322, 493, 5056, 7555, 490, 2440, 1496, 411, 371, 16, 5056, 9007, 439, 264, 636, 493, 281, 733, 51280], "temperature": 0.0, "avg_logprob": -0.06536313382590689, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.004195835907012224}, {"id": 184, "seek": 108896, "start": 1107.28, "end": 1113.8400000000001, "text": " of more rich images and we do this in parallel in a studio section during the other portion of the", "tokens": [51280, 295, 544, 4593, 5267, 293, 321, 360, 341, 294, 8952, 294, 257, 6811, 3541, 1830, 264, 661, 8044, 295, 264, 51608], "temperature": 0.0, "avg_logprob": -0.06536313382590689, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.004195835907012224}, {"id": 185, "seek": 111384, "start": 1113.84, "end": 1119.36, "text": " class where we're translating these principles of vision into the studio and building artistic", "tokens": [50364, 1508, 689, 321, 434, 35030, 613, 9156, 295, 5201, 666, 264, 6811, 293, 2390, 17090, 50640], "temperature": 0.0, "avg_logprob": -0.03293539524078369, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.008572911843657494}, {"id": 186, "seek": 111384, "start": 1119.36, "end": 1125.1999999999998, "text": " contexts where we can kind of become aware of our own perceptual processing at work so examples like", "tokens": [50640, 30628, 689, 321, 393, 733, 295, 1813, 3650, 295, 527, 1065, 43276, 901, 9007, 412, 589, 370, 5110, 411, 50932], "temperature": 0.0, "avg_logprob": -0.03293539524078369, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.008572911843657494}, {"id": 187, "seek": 111384, "start": 1125.1999999999998, "end": 1129.12, "text": " the one I showed you at the beginning right with the with the laser line moving over", "tokens": [50932, 264, 472, 286, 4712, 291, 412, 264, 2863, 558, 365, 264, 365, 264, 12530, 1622, 2684, 670, 51128], "temperature": 0.0, "avg_logprob": -0.03293539524078369, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.008572911843657494}, {"id": 188, "seek": 111384, "start": 1129.12, "end": 1134.3999999999999, "text": " that garden of objects are examples of settings that can allow us to maybe perceive our own", "tokens": [51128, 300, 7431, 295, 6565, 366, 5110, 295, 6257, 300, 393, 2089, 505, 281, 1310, 20281, 527, 1065, 51392], "temperature": 0.0, "avg_logprob": -0.03293539524078369, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.008572911843657494}, {"id": 189, "seek": 111384, "start": 1134.3999999999999, "end": 1139.84, "text": " perception at work right or shed some light on what's going on when we look at at normal scenes", "tokens": [51392, 12860, 412, 589, 558, 420, 14951, 512, 1442, 322, 437, 311, 516, 322, 562, 321, 574, 412, 412, 2710, 8026, 51664], "temperature": 0.0, "avg_logprob": -0.03293539524078369, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.008572911843657494}, {"id": 190, "seek": 113984, "start": 1139.84, "end": 1144.56, "text": " right there's all these unconscious inference processes happening even when we look at corners", "tokens": [50364, 558, 456, 311, 439, 613, 18900, 38253, 7555, 2737, 754, 562, 321, 574, 412, 12413, 50600], "temperature": 0.0, "avg_logprob": -0.0467130184173584, "compression_ratio": 1.811764705882353, "no_speech_prob": 0.0019870446994900703}, {"id": 191, "seek": 113984, "start": 1144.56, "end": 1148.8, "text": " in a room but we're not aware of them and so we ask here if we can create settings", "tokens": [50600, 294, 257, 1808, 457, 321, 434, 406, 3650, 295, 552, 293, 370, 321, 1029, 510, 498, 321, 393, 1884, 6257, 50812], "temperature": 0.0, "avg_logprob": -0.0467130184173584, "compression_ratio": 1.811764705882353, "no_speech_prob": 0.0019870446994900703}, {"id": 192, "seek": 113984, "start": 1149.52, "end": 1155.6, "text": " where we do become intensely aware of them and that awareness becomes kind of the art experience", "tokens": [50848, 689, 321, 360, 1813, 43235, 3650, 295, 552, 293, 300, 8888, 3643, 733, 295, 264, 1523, 1752, 51152], "temperature": 0.0, "avg_logprob": -0.0467130184173584, "compression_ratio": 1.811764705882353, "no_speech_prob": 0.0019870446994900703}, {"id": 193, "seek": 113984, "start": 1155.6, "end": 1160.9599999999998, "text": " right and so it's the art of perceiving one's own perceptual processes at work and then over", "tokens": [51152, 558, 293, 370, 309, 311, 264, 1523, 295, 9016, 2123, 472, 311, 1065, 43276, 901, 7555, 412, 589, 293, 550, 670, 51420], "temperature": 0.0, "avg_logprob": -0.0467130184173584, "compression_ratio": 1.811764705882353, "no_speech_prob": 0.0019870446994900703}, {"id": 194, "seek": 113984, "start": 1160.9599999999998, "end": 1166.1599999999999, "text": " the course of the class everybody develops an individual artwork for exhibition which is super", "tokens": [51420, 264, 1164, 295, 264, 1508, 2201, 25453, 364, 2609, 15829, 337, 14414, 597, 307, 1687, 51680], "temperature": 0.0, "avg_logprob": -0.0467130184173584, "compression_ratio": 1.811764705882353, "no_speech_prob": 0.0019870446994900703}, {"id": 195, "seek": 116616, "start": 1166.16, "end": 1172.5600000000002, "text": " lovely and it's it's an opportunity that we don't often have in other classes at MIT so we run this", "tokens": [50364, 7496, 293, 309, 311, 309, 311, 364, 2650, 300, 321, 500, 380, 2049, 362, 294, 661, 5359, 412, 13100, 370, 321, 1190, 341, 50684], "temperature": 0.0, "avg_logprob": -0.12118301391601563, "compression_ratio": 1.588, "no_speech_prob": 0.0027990604285150766}, {"id": 196, "seek": 116616, "start": 1172.5600000000002, "end": 1178.88, "text": " for five years now had five different exhibitions and COVID be it a virtual exhibition and then", "tokens": [50684, 337, 1732, 924, 586, 632, 1732, 819, 41522, 293, 4566, 312, 309, 257, 6374, 14414, 293, 550, 51000], "temperature": 0.0, "avg_logprob": -0.12118301391601563, "compression_ratio": 1.588, "no_speech_prob": 0.0027990604285150766}, {"id": 197, "seek": 116616, "start": 1178.88, "end": 1186.0800000000002, "text": " this year's just opened in December and is actually still up in the MIT museum studio just off of lobby", "tokens": [51000, 341, 1064, 311, 445, 5625, 294, 7687, 293, 307, 767, 920, 493, 294, 264, 13100, 8441, 6811, 445, 766, 295, 21067, 51360], "temperature": 0.0, "avg_logprob": -0.12118301391601563, "compression_ratio": 1.588, "no_speech_prob": 0.0027990604285150766}, {"id": 198, "seek": 116616, "start": 1186.0800000000002, "end": 1192.0, "text": " 10 10 150 if anybody is on campus and wants to go check it out it's most it's open most days when", "tokens": [51360, 1266, 1266, 8451, 498, 4472, 307, 322, 4828, 293, 2738, 281, 352, 1520, 309, 484, 309, 311, 881, 309, 311, 1269, 881, 1708, 562, 51656], "temperature": 0.0, "avg_logprob": -0.12118301391601563, "compression_ratio": 1.588, "no_speech_prob": 0.0027990604285150766}, {"id": 199, "seek": 119200, "start": 1192.24, "end": 1199.92, "text": " staff are there but this course is the parallel to your IAP class that thinks about things more", "tokens": [50376, 3525, 366, 456, 457, 341, 1164, 307, 264, 8952, 281, 428, 286, 4715, 1508, 300, 7309, 466, 721, 544, 50760], "temperature": 0.0, "avg_logprob": -0.07227729279317974, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0016734205419197679}, {"id": 200, "seek": 119200, "start": 1199.92, "end": 1205.04, "text": " in the language of computational neuroscience than deep learning and in some aspects of the", "tokens": [50760, 294, 264, 2856, 295, 28270, 42762, 813, 2452, 2539, 293, 294, 512, 7270, 295, 264, 51016], "temperature": 0.0, "avg_logprob": -0.07227729279317974, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0016734205419197679}, {"id": 201, "seek": 119200, "start": 1205.04, "end": 1211.52, "text": " course will present deep learning or deep generative models as contexts for probing representations", "tokens": [51016, 1164, 486, 1974, 2452, 2539, 420, 2452, 1337, 1166, 5245, 382, 30628, 337, 1239, 278, 33358, 51340], "temperature": 0.0, "avg_logprob": -0.07227729279317974, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0016734205419197679}, {"id": 202, "seek": 119200, "start": 1211.52, "end": 1216.72, "text": " that might be shared by human minds and machines and we'll look at that a little bit later in this", "tokens": [51340, 300, 1062, 312, 5507, 538, 1952, 9634, 293, 8379, 293, 321, 603, 574, 412, 300, 257, 707, 857, 1780, 294, 341, 51600], "temperature": 0.0, "avg_logprob": -0.07227729279317974, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0016734205419197679}, {"id": 203, "seek": 121672, "start": 1216.8, "end": 1222.08, "text": " lecture but think more traditional computational neuroscience lectures readings visual art and", "tokens": [50368, 7991, 457, 519, 544, 5164, 28270, 42762, 16564, 27319, 5056, 1523, 293, 50632], "temperature": 0.0, "avg_logprob": -0.08719918131828308, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.011323481798171997}, {"id": 204, "seek": 121672, "start": 1222.08, "end": 1226.88, "text": " then a studio component where you experiment with some of the stuff hands on so that's what we do", "tokens": [50632, 550, 257, 6811, 6542, 689, 291, 5120, 365, 512, 295, 264, 1507, 2377, 322, 370, 300, 311, 437, 321, 360, 50872], "temperature": 0.0, "avg_logprob": -0.08719918131828308, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.011323481798171997}, {"id": 205, "seek": 121672, "start": 1226.88, "end": 1233.6000000000001, "text": " envision art neuroscience we start to to probe at the richness of this art neuro and machine", "tokens": [50872, 24739, 1523, 42762, 321, 722, 281, 281, 22715, 412, 264, 44506, 295, 341, 1523, 16499, 293, 3479, 51208], "temperature": 0.0, "avg_logprob": -0.08719918131828308, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.011323481798171997}, {"id": 206, "seek": 121672, "start": 1233.6000000000001, "end": 1237.52, "text": " learning intersection there's a lot of different things we can do there and for the rest of this", "tokens": [51208, 2539, 15236, 456, 311, 257, 688, 295, 819, 721, 321, 393, 360, 456, 293, 337, 264, 1472, 295, 341, 51404], "temperature": 0.0, "avg_logprob": -0.08719918131828308, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.011323481798171997}, {"id": 207, "seek": 121672, "start": 1237.52, "end": 1242.4, "text": " talk we're going to highlight a number of different projects that approach that intersection in", "tokens": [51404, 751, 321, 434, 516, 281, 5078, 257, 1230, 295, 819, 4455, 300, 3109, 300, 15236, 294, 51648], "temperature": 0.0, "avg_logprob": -0.08719918131828308, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.011323481798171997}, {"id": 208, "seek": 124240, "start": 1242.4, "end": 1247.2800000000002, "text": " different ways and highlight kind of different ways that you could think about engaging this", "tokens": [50364, 819, 2098, 293, 5078, 733, 295, 819, 2098, 300, 291, 727, 519, 466, 11268, 341, 50608], "temperature": 0.0, "avg_logprob": -0.06778319845808313, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.001500581158325076}, {"id": 209, "seek": 124240, "start": 1247.2800000000002, "end": 1252.16, "text": " material in these questions data sets and resources that we have available and kind of", "tokens": [50608, 2527, 294, 613, 1651, 1412, 6352, 293, 3593, 300, 321, 362, 2435, 293, 733, 295, 50852], "temperature": 0.0, "avg_logprob": -0.06778319845808313, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.001500581158325076}, {"id": 210, "seek": 124240, "start": 1252.16, "end": 1258.48, "text": " different ways of carving up the problem into bits so we'll start by thinking about modeling", "tokens": [50852, 819, 2098, 295, 31872, 493, 264, 1154, 666, 9239, 370, 321, 603, 722, 538, 1953, 466, 15983, 51168], "temperature": 0.0, "avg_logprob": -0.06778319845808313, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.001500581158325076}, {"id": 211, "seek": 124240, "start": 1258.48, "end": 1264.72, "text": " kind of the structure underlying human creativity at scale without trying to prespecify", "tokens": [51168, 733, 295, 264, 3877, 14217, 1952, 12915, 412, 4373, 1553, 1382, 281, 1183, 494, 66, 2505, 51480], "temperature": 0.0, "avg_logprob": -0.06778319845808313, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.001500581158325076}, {"id": 212, "seek": 124240, "start": 1265.52, "end": 1271.3600000000001, "text": " laws that you would write down for say a physics engine right can we use deep generative models", "tokens": [51520, 6064, 300, 291, 576, 2464, 760, 337, 584, 257, 10649, 2848, 558, 393, 321, 764, 2452, 1337, 1166, 5245, 51812], "temperature": 0.0, "avg_logprob": -0.06778319845808313, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.001500581158325076}, {"id": 213, "seek": 127136, "start": 1271.36, "end": 1278.08, "text": " to kind of approximate or appreciate or grok the structure underlying large data sets of human", "tokens": [50364, 281, 733, 295, 30874, 420, 4449, 420, 4634, 74, 264, 3877, 14217, 2416, 1412, 6352, 295, 1952, 50700], "temperature": 0.0, "avg_logprob": -0.09069153921944755, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.00041064832475967705}, {"id": 214, "seek": 127136, "start": 1278.08, "end": 1284.4799999999998, "text": " cultural artifacts and then use those models to experiment with cultural history on kind of a", "tokens": [50700, 6988, 24617, 293, 550, 764, 729, 5245, 281, 5120, 365, 6988, 2503, 322, 733, 295, 257, 51020], "temperature": 0.0, "avg_logprob": -0.09069153921944755, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.00041064832475967705}, {"id": 215, "seek": 127136, "start": 1284.4799999999998, "end": 1289.6, "text": " timeline that allows rapid evolution in the present so I'm speaking specifically about a project", "tokens": [51020, 12933, 300, 4045, 7558, 9303, 294, 264, 1974, 370, 286, 478, 4124, 4682, 466, 257, 1716, 51276], "temperature": 0.0, "avg_logprob": -0.09069153921944755, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.00041064832475967705}, {"id": 216, "seek": 127136, "start": 1289.6, "end": 1293.76, "text": " that I don't know if some of you have seen and I know Ali has seen a collaboration that I led with", "tokens": [51276, 300, 286, 500, 380, 458, 498, 512, 295, 291, 362, 1612, 293, 286, 458, 12020, 575, 1612, 257, 9363, 300, 286, 4684, 365, 51484], "temperature": 0.0, "avg_logprob": -0.09069153921944755, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.00041064832475967705}, {"id": 217, "seek": 127136, "start": 1293.76, "end": 1299.28, "text": " the Met a couple of years ago again it was fun that we were in person because we were able to", "tokens": [51484, 264, 6377, 257, 1916, 295, 924, 2057, 797, 309, 390, 1019, 300, 321, 645, 294, 954, 570, 321, 645, 1075, 281, 51760], "temperature": 0.0, "avg_logprob": -0.09069153921944755, "compression_ratio": 1.7132616487455197, "no_speech_prob": 0.00041064832475967705}, {"id": 218, "seek": 129928, "start": 1299.28, "end": 1304.8, "text": " actually go to the Met and see a lot of these objects but back in 2017 the Metropolitan Museum of", "tokens": [50364, 767, 352, 281, 264, 6377, 293, 536, 257, 688, 295, 613, 6565, 457, 646, 294, 6591, 264, 45489, 10967, 295, 50640], "temperature": 0.0, "avg_logprob": -0.05982057406352116, "compression_ratio": 1.72, "no_speech_prob": 0.00036815760540775955}, {"id": 219, "seek": 129928, "start": 1304.8, "end": 1311.6, "text": " Art was the first or one of the very first to release an open access catalog of a few hundred", "tokens": [50640, 5735, 390, 264, 700, 420, 472, 295, 264, 588, 700, 281, 4374, 364, 1269, 2105, 19746, 295, 257, 1326, 3262, 50980], "temperature": 0.0, "avg_logprob": -0.05982057406352116, "compression_ratio": 1.72, "no_speech_prob": 0.00036815760540775955}, {"id": 220, "seek": 129928, "start": 1311.6, "end": 1317.52, "text": " thousand digital images of works in the Met collection and released them into the public domain", "tokens": [50980, 4714, 4562, 5267, 295, 1985, 294, 264, 6377, 5765, 293, 4736, 552, 666, 264, 1908, 9274, 51276], "temperature": 0.0, "avg_logprob": -0.05982057406352116, "compression_ratio": 1.72, "no_speech_prob": 0.00036815760540775955}, {"id": 221, "seek": 129928, "start": 1317.52, "end": 1323.6, "text": " which is wonderful for for us as computer scientists and programmers and people interested in ML and", "tokens": [51276, 597, 307, 3715, 337, 337, 505, 382, 3820, 7708, 293, 41504, 293, 561, 3102, 294, 21601, 293, 51580], "temperature": 0.0, "avg_logprob": -0.05982057406352116, "compression_ratio": 1.72, "no_speech_prob": 0.00036815760540775955}, {"id": 222, "seek": 129928, "start": 1323.6, "end": 1328.96, "text": " art because what a rich data set that is right what a rich data set all in one place", "tokens": [51580, 1523, 570, 437, 257, 4593, 1412, 992, 300, 307, 558, 437, 257, 4593, 1412, 992, 439, 294, 472, 1081, 51848], "temperature": 0.0, "avg_logprob": -0.05982057406352116, "compression_ratio": 1.72, "no_speech_prob": 0.00036815760540775955}, {"id": 223, "seek": 132896, "start": 1328.96, "end": 1333.8400000000001, "text": " don't get me started on the issues with museum APIs but a lot of museums have followed suit in", "tokens": [50364, 500, 380, 483, 385, 1409, 322, 264, 2663, 365, 8441, 21445, 457, 257, 688, 295, 23248, 362, 6263, 5722, 294, 50608], "temperature": 0.0, "avg_logprob": -0.06516302222072488, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.0006260620430111885}, {"id": 224, "seek": 132896, "start": 1333.8400000000001, "end": 1339.44, "text": " releasing their digital collections into the public domain so they're free and open for experimentation", "tokens": [50608, 16327, 641, 4562, 16641, 666, 264, 1908, 9274, 370, 436, 434, 1737, 293, 1269, 337, 37142, 50888], "temperature": 0.0, "avg_logprob": -0.06516302222072488, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.0006260620430111885}, {"id": 225, "seek": 132896, "start": 1340.24, "end": 1346.0, "text": " they approached us at MIT and open learning and a couple of programmers at Microsoft and asked if", "tokens": [50928, 436, 17247, 505, 412, 13100, 293, 1269, 2539, 293, 257, 1916, 295, 41504, 412, 8116, 293, 2351, 498, 51216], "temperature": 0.0, "avg_logprob": -0.06516302222072488, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.0006260620430111885}, {"id": 226, "seek": 132896, "start": 1346.0, "end": 1352.08, "text": " we might want to do a series of projects with this digital collection and so we did and we asked", "tokens": [51216, 321, 1062, 528, 281, 360, 257, 2638, 295, 4455, 365, 341, 4562, 5765, 293, 370, 321, 630, 293, 321, 2351, 51520], "temperature": 0.0, "avg_logprob": -0.06516302222072488, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.0006260620430111885}, {"id": 227, "seek": 132896, "start": 1352.08, "end": 1357.6000000000001, "text": " whether we can build deep generative models associated with archives like this of created", "tokens": [51520, 1968, 321, 393, 1322, 2452, 1337, 1166, 5245, 6615, 365, 25607, 411, 341, 295, 2942, 51796], "temperature": 0.0, "avg_logprob": -0.06516302222072488, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.0006260620430111885}, {"id": 228, "seek": 135760, "start": 1357.6, "end": 1362.8, "text": " work that are embedded in their cultural context which might ask which might allow us to ask like", "tokens": [50364, 589, 300, 366, 16741, 294, 641, 6988, 4319, 597, 1062, 1029, 597, 1062, 2089, 505, 281, 1029, 411, 50624], "temperature": 0.0, "avg_logprob": -0.08629281052919192, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0019555934704840183}, {"id": 229, "seek": 135760, "start": 1362.8, "end": 1369.04, "text": " slightly more specific questions art historically than just you know what if you train StyleGAN on", "tokens": [50624, 4748, 544, 2685, 1651, 1523, 16180, 813, 445, 291, 458, 437, 498, 291, 3847, 27004, 27699, 322, 50936], "temperature": 0.0, "avg_logprob": -0.08629281052919192, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0019555934704840183}, {"id": 230, "seek": 135760, "start": 1369.04, "end": 1374.0, "text": " all of wiki art all at once right not conditionally so we're not appreciating any categorical", "tokens": [50936, 439, 295, 261, 9850, 1523, 439, 412, 1564, 558, 406, 4188, 379, 370, 321, 434, 406, 3616, 990, 604, 19250, 804, 51184], "temperature": 0.0, "avg_logprob": -0.08629281052919192, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0019555934704840183}, {"id": 231, "seek": 135760, "start": 1374.0, "end": 1379.52, "text": " differences between images but if we just showed it all of wiki art okay here we want to ask something", "tokens": [51184, 7300, 1296, 5267, 457, 498, 321, 445, 4712, 309, 439, 295, 261, 9850, 1523, 1392, 510, 321, 528, 281, 1029, 746, 51460], "temperature": 0.0, "avg_logprob": -0.08629281052919192, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0019555934704840183}, {"id": 232, "seek": 135760, "start": 1379.52, "end": 1384.8799999999999, "text": " a little more fine grained can we notice you know differences in the development of feature languages", "tokens": [51460, 257, 707, 544, 2489, 1295, 2001, 393, 321, 3449, 291, 458, 7300, 294, 264, 3250, 295, 4111, 8650, 51728], "temperature": 0.0, "avg_logprob": -0.08629281052919192, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0019555934704840183}, {"id": 233, "seek": 138488, "start": 1384.88, "end": 1392.72, "text": " between maybe time periods or geographical regions right and can we develop ways of collaborating", "tokens": [50364, 1296, 1310, 565, 13804, 420, 39872, 10682, 558, 293, 393, 321, 1499, 2098, 295, 30188, 50756], "temperature": 0.0, "avg_logprob": -0.05558406194051107, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.00019107741536572576}, {"id": 234, "seek": 138488, "start": 1392.72, "end": 1399.0400000000002, "text": " with those models to iterate archives forward so experimenting with chimeras between existing", "tokens": [50756, 365, 729, 5245, 281, 44497, 25607, 2128, 370, 29070, 365, 18375, 6985, 1296, 6741, 51072], "temperature": 0.0, "avg_logprob": -0.05558406194051107, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.00019107741536572576}, {"id": 235, "seek": 138488, "start": 1399.0400000000002, "end": 1404.48, "text": " works and developing new works right that might sit somewhere between works that are already on a graph", "tokens": [51072, 1985, 293, 6416, 777, 1985, 558, 300, 1062, 1394, 4079, 1296, 1985, 300, 366, 1217, 322, 257, 4295, 51344], "temperature": 0.0, "avg_logprob": -0.05558406194051107, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.00019107741536572576}, {"id": 236, "seek": 138488, "start": 1406.0800000000002, "end": 1411.8400000000001, "text": " so one of the challenges that we faced here initially was that the data set was pretty big", "tokens": [51424, 370, 472, 295, 264, 4759, 300, 321, 11446, 510, 9105, 390, 300, 264, 1412, 992, 390, 1238, 955, 51712], "temperature": 0.0, "avg_logprob": -0.05558406194051107, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.00019107741536572576}, {"id": 237, "seek": 141184, "start": 1411.84, "end": 1417.04, "text": " 400,000 images but each individual category in that data set was not some might only have", "tokens": [50364, 8423, 11, 1360, 5267, 457, 1184, 2609, 7719, 294, 300, 1412, 992, 390, 406, 512, 1062, 787, 362, 50624], "temperature": 0.0, "avg_logprob": -0.04849374069357818, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.000779228750616312}, {"id": 238, "seek": 141184, "start": 1417.04, "end": 1422.8, "text": " a couple hundred images and there's a lot of sketches and drawings and kind of uncategorized", "tokens": [50624, 257, 1916, 3262, 5267, 293, 456, 311, 257, 688, 295, 34547, 293, 18618, 293, 733, 295, 6219, 2968, 284, 1602, 50912], "temperature": 0.0, "avg_logprob": -0.04849374069357818, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.000779228750616312}, {"id": 239, "seek": 141184, "start": 1422.8, "end": 1429.12, "text": " work too that makes up that 400,000 so you're in a situation where in theory you have a rich", "tokens": [50912, 589, 886, 300, 1669, 493, 300, 8423, 11, 1360, 370, 291, 434, 294, 257, 2590, 689, 294, 5261, 291, 362, 257, 4593, 51228], "temperature": 0.0, "avg_logprob": -0.04849374069357818, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.000779228750616312}, {"id": 240, "seek": 141184, "start": 1429.12, "end": 1435.12, "text": " labeled data set but in practice it might be quite difficult to train anything that looks", "tokens": [51228, 21335, 1412, 992, 457, 294, 3124, 309, 1062, 312, 1596, 2252, 281, 3847, 1340, 300, 1542, 51528], "temperature": 0.0, "avg_logprob": -0.04849374069357818, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.000779228750616312}, {"id": 241, "seek": 141184, "start": 1435.12, "end": 1439.6, "text": " photorealistic or gives a good sense of any individual category of work because the categories", "tokens": [51528, 2409, 418, 304, 3142, 420, 2709, 257, 665, 2020, 295, 604, 2609, 7719, 295, 589, 570, 264, 10479, 51752], "temperature": 0.0, "avg_logprob": -0.04849374069357818, "compression_ratio": 1.7624521072796935, "no_speech_prob": 0.000779228750616312}, {"id": 242, "seek": 143960, "start": 1439.6, "end": 1446.48, "text": " themselves are not that large so at that point this was pre like style again too we started working", "tokens": [50364, 2969, 366, 406, 300, 2416, 370, 412, 300, 935, 341, 390, 659, 411, 3758, 797, 886, 321, 1409, 1364, 50708], "temperature": 0.0, "avg_logprob": -0.16714635762301358, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0007792479009367526}, {"id": 243, "seek": 143960, "start": 1446.48, "end": 1454.8, "text": " on this in 2017-2018 um I asked whether we could instead of training a single model on say a subset", "tokens": [50708, 322, 341, 294, 6591, 12, 35023, 1105, 286, 2351, 1968, 321, 727, 2602, 295, 3097, 257, 2167, 2316, 322, 584, 257, 25993, 51124], "temperature": 0.0, "avg_logprob": -0.16714635762301358, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0007792479009367526}, {"id": 244, "seek": 143960, "start": 1454.8, "end": 1460.9599999999998, "text": " of this met collection like this category of vases called yours whether we could find corresponding", "tokens": [51124, 295, 341, 1131, 5765, 411, 341, 7719, 295, 371, 1957, 1219, 6342, 1968, 321, 727, 915, 11760, 51432], "temperature": 0.0, "avg_logprob": -0.16714635762301358, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0007792479009367526}, {"id": 245, "seek": 143960, "start": 1460.9599999999998, "end": 1467.6799999999998, "text": " subspaces of what we're now referring to as foundation models like big an image net that kind", "tokens": [51432, 2090, 79, 2116, 295, 437, 321, 434, 586, 13761, 281, 382, 7030, 5245, 411, 955, 364, 3256, 2533, 300, 733, 51768], "temperature": 0.0, "avg_logprob": -0.16714635762301358, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0007792479009367526}, {"id": 246, "seek": 146768, "start": 1467.76, "end": 1473.76, "text": " of approximate our data set right so if we think about foundation models as a shared resource that", "tokens": [50368, 295, 30874, 527, 1412, 992, 558, 370, 498, 321, 519, 466, 7030, 5245, 382, 257, 5507, 7684, 300, 50668], "temperature": 0.0, "avg_logprob": -0.06237391325143667, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.0003249934525229037}, {"id": 247, "seek": 146768, "start": 1473.76, "end": 1478.88, "text": " ideally everybody would have access to and there were ways to think about contributing to then maybe", "tokens": [50668, 22915, 2201, 576, 362, 2105, 281, 293, 456, 645, 2098, 281, 519, 466, 19270, 281, 550, 1310, 50924], "temperature": 0.0, "avg_logprob": -0.06237391325143667, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.0003249934525229037}, {"id": 248, "seek": 146768, "start": 1478.88, "end": 1485.68, "text": " these smaller problems become or can become a way of defining subspaces of those big models that we", "tokens": [50924, 613, 4356, 2740, 1813, 420, 393, 1813, 257, 636, 295, 17827, 2090, 79, 2116, 295, 729, 955, 5245, 300, 321, 51264], "temperature": 0.0, "avg_logprob": -0.06237391325143667, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.0003249934525229037}, {"id": 249, "seek": 146768, "start": 1485.68, "end": 1491.52, "text": " can interact with right rather than having to retrain a model and on our data set so we used", "tokens": [51264, 393, 4648, 365, 558, 2831, 813, 1419, 281, 1533, 7146, 257, 2316, 293, 322, 527, 1412, 992, 370, 321, 1143, 51556], "temperature": 0.0, "avg_logprob": -0.06237391325143667, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.0003249934525229037}, {"id": 250, "seek": 146768, "start": 1491.52, "end": 1496.24, "text": " GAN inversion here and instead of training a new model on just this category of viewers", "tokens": [51556, 460, 1770, 43576, 510, 293, 2602, 295, 3097, 257, 777, 2316, 322, 445, 341, 7719, 295, 8499, 51792], "temperature": 0.0, "avg_logprob": -0.06237391325143667, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.0003249934525229037}, {"id": 251, "seek": 149624, "start": 1496.8, "end": 1502.16, "text": " we asked whether we could embed each image that already existed into in the met collection", "tokens": [50392, 321, 2351, 1968, 321, 727, 12240, 1184, 3256, 300, 1217, 13135, 666, 294, 264, 1131, 5765, 50660], "temperature": 0.0, "avg_logprob": -0.0978403878868173, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.0003353087813593447}, {"id": 252, "seek": 149624, "start": 1502.16, "end": 1508.56, "text": " and into the feature space of big GAN image net which happens to have a category for vases so we", "tokens": [50660, 293, 666, 264, 4111, 1901, 295, 955, 460, 1770, 3256, 2533, 597, 2314, 281, 362, 257, 7719, 337, 371, 1957, 370, 321, 50980], "temperature": 0.0, "avg_logprob": -0.0978403878868173, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.0003353087813593447}, {"id": 253, "seek": 149624, "start": 1508.56, "end": 1513.68, "text": " selected categories that were shared between image net and the met collection there are a handful", "tokens": [50980, 8209, 10479, 300, 645, 5507, 1296, 3256, 2533, 293, 264, 1131, 5765, 456, 366, 257, 16458, 51236], "temperature": 0.0, "avg_logprob": -0.0978403878868173, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.0003353087813593447}, {"id": 254, "seek": 149624, "start": 1513.68, "end": 1519.84, "text": " about a dozen um and we maximized for each of those images the similarity between the met image", "tokens": [51236, 466, 257, 16654, 1105, 293, 321, 5138, 1602, 337, 1184, 295, 729, 5267, 264, 32194, 1296, 264, 1131, 3256, 51544], "temperature": 0.0, "avg_logprob": -0.0978403878868173, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.0003353087813593447}, {"id": 255, "seek": 149624, "start": 1519.84, "end": 1524.4, "text": " and the big GAN image using a two-part loss right so we wanted them to be similar both at the pixel", "tokens": [51544, 293, 264, 955, 460, 1770, 3256, 1228, 257, 732, 12, 6971, 4470, 558, 370, 321, 1415, 552, 281, 312, 2531, 1293, 412, 264, 19261, 51772], "temperature": 0.0, "avg_logprob": -0.0978403878868173, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.0003353087813593447}, {"id": 256, "seek": 152440, "start": 1524.4, "end": 1530.24, "text": " level and at the semantic level and we did that by looking at two different layers of a pre-trained", "tokens": [50364, 1496, 293, 412, 264, 47982, 1496, 293, 321, 630, 300, 538, 1237, 412, 732, 819, 7914, 295, 257, 659, 12, 17227, 2001, 50656], "temperature": 0.0, "avg_logprob": -0.05540032159714472, "compression_ratio": 1.75, "no_speech_prob": 0.00015353791241068393}, {"id": 257, "seek": 152440, "start": 1530.24, "end": 1536.0, "text": " res net as the embedding network so once we've embedded these models these images into big GAN", "tokens": [50656, 725, 2533, 382, 264, 12240, 3584, 3209, 370, 1564, 321, 600, 16741, 613, 5245, 613, 5267, 666, 955, 460, 1770, 50944], "temperature": 0.0, "avg_logprob": -0.05540032159714472, "compression_ratio": 1.75, "no_speech_prob": 0.00015353791241068393}, {"id": 258, "seek": 152440, "start": 1536.0, "end": 1540.72, "text": " we can then visualize the individual embeddings but we can also do something a little bit more", "tokens": [50944, 321, 393, 550, 23273, 264, 2609, 12240, 29432, 457, 321, 393, 611, 360, 746, 257, 707, 857, 544, 51180], "temperature": 0.0, "avg_logprob": -0.05540032159714472, "compression_ratio": 1.75, "no_speech_prob": 0.00015353791241068393}, {"id": 259, "seek": 152440, "start": 1540.72, "end": 1546.64, "text": " interesting than just look at approximations of these images which might not be very good we can", "tokens": [51180, 1880, 813, 445, 574, 412, 8542, 763, 295, 613, 5267, 597, 1062, 406, 312, 588, 665, 321, 393, 51476], "temperature": 0.0, "avg_logprob": -0.05540032159714472, "compression_ratio": 1.75, "no_speech_prob": 0.00015353791241068393}, {"id": 260, "seek": 152440, "start": 1546.64, "end": 1550.8000000000002, "text": " think about the underlying feature language that might have been learned and then look at", "tokens": [51476, 519, 466, 264, 14217, 4111, 2856, 300, 1062, 362, 668, 3264, 293, 550, 574, 412, 51684], "temperature": 0.0, "avg_logprob": -0.05540032159714472, "compression_ratio": 1.75, "no_speech_prob": 0.00015353791241068393}, {"id": 261, "seek": 155080, "start": 1550.8, "end": 1556.72, "text": " interpolations between the existing images in the met collection I hear murmurs in the", "tokens": [50364, 44902, 763, 1296, 264, 6741, 5267, 294, 264, 1131, 5765, 286, 1568, 39729, 2156, 294, 264, 50660], "temperature": 0.0, "avg_logprob": -0.06784058824370179, "compression_ratio": 1.8514851485148516, "no_speech_prob": 0.0008293759892694652}, {"id": 262, "seek": 155080, "start": 1556.72, "end": 1562.6399999999999, "text": " background if anybody has a question hit the chat you're super welcome to speak up um so next we", "tokens": [50660, 3678, 498, 4472, 575, 257, 1168, 2045, 264, 5081, 291, 434, 1687, 2928, 281, 1710, 493, 1105, 370, 958, 321, 50956], "temperature": 0.0, "avg_logprob": -0.06784058824370179, "compression_ratio": 1.8514851485148516, "no_speech_prob": 0.0008293759892694652}, {"id": 263, "seek": 155080, "start": 1562.6399999999999, "end": 1568.1599999999999, "text": " look at interpolations between these existing images on the graph and we can create kind of", "tokens": [50956, 574, 412, 44902, 763, 1296, 613, 6741, 5267, 322, 264, 4295, 293, 321, 393, 1884, 733, 295, 51232], "temperature": 0.0, "avg_logprob": -0.06784058824370179, "compression_ratio": 1.8514851485148516, "no_speech_prob": 0.0008293759892694652}, {"id": 264, "seek": 155080, "start": 1568.1599999999999, "end": 1575.04, "text": " hypothetical or dreamlike images that exist between the spaces of existing works in the collection", "tokens": [51232, 33053, 420, 3055, 4092, 5267, 300, 2514, 1296, 264, 7673, 295, 6741, 1985, 294, 264, 5765, 51576], "temperature": 0.0, "avg_logprob": -0.06784058824370179, "compression_ratio": 1.8514851485148516, "no_speech_prob": 0.0008293759892694652}, {"id": 265, "seek": 157504, "start": 1575.04, "end": 1581.36, "text": " and these are pretty interesting and beautiful and they allow us as I was mentioning to ask", "tokens": [50364, 293, 613, 366, 1238, 1880, 293, 2238, 293, 436, 2089, 505, 382, 286, 390, 18315, 281, 1029, 50680], "temperature": 0.0, "avg_logprob": -0.06747872778709899, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0017002889653667808}, {"id": 266, "seek": 157504, "start": 1581.36, "end": 1585.92, "text": " questions about what collaborations between geographical regions might have looked like", "tokens": [50680, 1651, 466, 437, 36908, 1296, 39872, 10682, 1062, 362, 2956, 411, 50908], "temperature": 0.0, "avg_logprob": -0.06747872778709899, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0017002889653667808}, {"id": 267, "seek": 157504, "start": 1585.92, "end": 1590.72, "text": " right because we do have categorical information about where each image in the met collection", "tokens": [50908, 558, 570, 321, 360, 362, 19250, 804, 1589, 466, 689, 1184, 3256, 294, 264, 1131, 5765, 51148], "temperature": 0.0, "avg_logprob": -0.06747872778709899, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0017002889653667808}, {"id": 268, "seek": 157504, "start": 1590.72, "end": 1596.8, "text": " came from it allows us to suggest new objects and the spaces between them so it allows us to", "tokens": [51148, 1361, 490, 309, 4045, 505, 281, 3402, 777, 6565, 293, 264, 7673, 1296, 552, 370, 309, 4045, 505, 281, 51452], "temperature": 0.0, "avg_logprob": -0.06747872778709899, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0017002889653667808}, {"id": 269, "seek": 157504, "start": 1596.8, "end": 1602.3999999999999, "text": " interpolate and the other beauty of these kinds of executable models of culture is that it allows", "tokens": [51452, 44902, 473, 293, 264, 661, 6643, 295, 613, 3685, 295, 7568, 712, 5245, 295, 3713, 307, 300, 309, 4045, 51732], "temperature": 0.0, "avg_logprob": -0.06747872778709899, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0017002889653667808}, {"id": 270, "seek": 160240, "start": 1602.48, "end": 1608.3200000000002, "text": " us to iterate on existing collections really rapidly um and evolve them forward and so we", "tokens": [50368, 505, 281, 44497, 322, 6741, 16641, 534, 12910, 1105, 293, 16693, 552, 2128, 293, 370, 321, 50660], "temperature": 0.0, "avg_logprob": -0.06301723420619965, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0010646256851032376}, {"id": 271, "seek": 160240, "start": 1608.3200000000002, "end": 1613.2800000000002, "text": " can kind of start to imagine archives of the future that would have embedded within them", "tokens": [50660, 393, 733, 295, 722, 281, 3811, 25607, 295, 264, 2027, 300, 576, 362, 16741, 1951, 552, 50908], "temperature": 0.0, "avg_logprob": -0.06301723420619965, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0010646256851032376}, {"id": 272, "seek": 160240, "start": 1613.8400000000001, "end": 1619.2800000000002, "text": " world models corresponding to the data set that exists at one point in the archive right so the", "tokens": [50936, 1002, 5245, 11760, 281, 264, 1412, 992, 300, 8198, 412, 472, 935, 294, 264, 23507, 558, 370, 264, 51208], "temperature": 0.0, "avg_logprob": -0.06301723420619965, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0010646256851032376}, {"id": 273, "seek": 160240, "start": 1619.2800000000002, "end": 1624.64, "text": " archives could kind of evolve themselves forward and suggest future versions of their collections", "tokens": [51208, 25607, 727, 733, 295, 16693, 2969, 2128, 293, 3402, 2027, 9606, 295, 641, 16641, 51476], "temperature": 0.0, "avg_logprob": -0.06301723420619965, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0010646256851032376}, {"id": 274, "seek": 160240, "start": 1624.64, "end": 1630.5600000000002, "text": " based on what's already been created and that this is again this was back in 2019 which is a", "tokens": [51476, 2361, 322, 437, 311, 1217, 668, 2942, 293, 300, 341, 307, 797, 341, 390, 646, 294, 6071, 597, 307, 257, 51772], "temperature": 0.0, "avg_logprob": -0.06301723420619965, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0010646256851032376}, {"id": 275, "seek": 163056, "start": 1630.56, "end": 1636.24, "text": " long time ago in computer vision terms um but even just with with inversion into to began in the", "tokens": [50364, 938, 565, 2057, 294, 3820, 5201, 2115, 1105, 457, 754, 445, 365, 365, 43576, 666, 281, 4283, 294, 264, 50648], "temperature": 0.0, "avg_logprob": -0.12275290266375675, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.00031990662682801485}, {"id": 276, "seek": 163056, "start": 1636.24, "end": 1641.04, "text": " channel I was really impressed at the quality of the the images and the hypothetical objects that", "tokens": [50648, 2269, 286, 390, 534, 11679, 412, 264, 3125, 295, 264, 264, 5267, 293, 264, 33053, 6565, 300, 50888], "temperature": 0.0, "avg_logprob": -0.12275290266375675, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.00031990662682801485}, {"id": 277, "seek": 163056, "start": 1641.04, "end": 1646.1599999999999, "text": " we could get for example here are a bunch of different generated teapots from the met latent", "tokens": [50888, 321, 727, 483, 337, 1365, 510, 366, 257, 3840, 295, 819, 10833, 535, 569, 1971, 490, 264, 1131, 48994, 51144], "temperature": 0.0, "avg_logprob": -0.12275290266375675, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.00031990662682801485}, {"id": 278, "seek": 163056, "start": 1646.1599999999999, "end": 1651.76, "text": " space in the teapot category which again happened to be shared between ImageNet at that point um", "tokens": [51144, 1901, 294, 264, 535, 569, 310, 7719, 597, 797, 2011, 281, 312, 5507, 1296, 29903, 31890, 412, 300, 935, 1105, 51424], "temperature": 0.0, "avg_logprob": -0.12275290266375675, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.00031990662682801485}, {"id": 279, "seek": 163056, "start": 1651.76, "end": 1658.96, "text": " and the met collection and as I said we did have the the opportunity to exhibit this in the met", "tokens": [51424, 293, 264, 1131, 5765, 293, 382, 286, 848, 321, 630, 362, 264, 264, 2650, 281, 20487, 341, 294, 264, 1131, 51784], "temperature": 0.0, "avg_logprob": -0.12275290266375675, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.00031990662682801485}, {"id": 280, "seek": 165896, "start": 1658.96, "end": 1665.76, "text": " which was absolutely wonderful um we projected a visualization of this latent space superimposed", "tokens": [50364, 597, 390, 3122, 3715, 1105, 321, 26231, 257, 25801, 295, 341, 48994, 1901, 1687, 8814, 1744, 50704], "temperature": 0.0, "avg_logprob": -0.07195063524468001, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.000709409941919148}, {"id": 281, "seek": 165896, "start": 1665.76, "end": 1671.3600000000001, "text": " on a map of the met collection and allowed people visitors to the to the great hall to kind of step", "tokens": [50704, 322, 257, 4471, 295, 264, 1131, 5765, 293, 4350, 561, 14315, 281, 264, 281, 264, 869, 6500, 281, 733, 295, 1823, 50984], "temperature": 0.0, "avg_logprob": -0.07195063524468001, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.000709409941919148}, {"id": 282, "seek": 165896, "start": 1671.3600000000001, "end": 1677.3600000000001, "text": " in to this latent space as projected onto the ground and explore the traversal of the spaces", "tokens": [50984, 294, 281, 341, 48994, 1901, 382, 26231, 3911, 264, 2727, 293, 6839, 264, 23149, 304, 295, 264, 7673, 51284], "temperature": 0.0, "avg_logprob": -0.07195063524468001, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.000709409941919148}, {"id": 283, "seek": 165896, "start": 1677.3600000000001, "end": 1686.32, "text": " between works um and a projection behind them on the wall. We also made a web app version of all", "tokens": [51284, 1296, 1985, 1105, 293, 257, 22743, 2261, 552, 322, 264, 2929, 13, 492, 611, 1027, 257, 3670, 724, 3037, 295, 439, 51732], "temperature": 0.0, "avg_logprob": -0.07195063524468001, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.000709409941919148}, {"id": 284, "seek": 168632, "start": 1686.3999999999999, "end": 1692.72, "text": " this that exists even though we we can't visit the met today um it's online at gen.studio if you", "tokens": [50368, 341, 300, 8198, 754, 1673, 321, 321, 393, 380, 3441, 264, 1131, 965, 1105, 309, 311, 2950, 412, 1049, 13, 28349, 1004, 498, 291, 50684], "temperature": 0.0, "avg_logprob": -0.07352581024169921, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.001866979873739183}, {"id": 285, "seek": 168632, "start": 1692.72, "end": 1697.6799999999998, "text": " want to go have a look after this and then all of the the code base is linked the github is linked", "tokens": [50684, 528, 281, 352, 362, 257, 574, 934, 341, 293, 550, 439, 295, 264, 264, 3089, 3096, 307, 9408, 264, 290, 355, 836, 307, 9408, 50932], "temperature": 0.0, "avg_logprob": -0.07352581024169921, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.001866979873739183}, {"id": 286, "seek": 168632, "start": 1697.6799999999998, "end": 1703.6, "text": " at the bottom um if you want to check out any of that more specifically but again it places us", "tokens": [50932, 412, 264, 2767, 1105, 498, 291, 528, 281, 1520, 484, 604, 295, 300, 544, 4682, 457, 797, 309, 3190, 505, 51228], "temperature": 0.0, "avg_logprob": -0.07352581024169921, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.001866979873739183}, {"id": 287, "seek": 168632, "start": 1703.6, "end": 1708.96, "text": " kind of a different framing of latent space traversal than we're used to that I was interested", "tokens": [51228, 733, 295, 257, 819, 28971, 295, 48994, 1901, 23149, 304, 813, 321, 434, 1143, 281, 300, 286, 390, 3102, 51496], "temperature": 0.0, "avg_logprob": -0.07352581024169921, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.001866979873739183}, {"id": 288, "seek": 168632, "start": 1708.96, "end": 1713.84, "text": " in this project was to place us on you know we've gotten a lot further along in the video um", "tokens": [51496, 294, 341, 1716, 390, 281, 1081, 505, 322, 291, 458, 321, 600, 5768, 257, 688, 3052, 2051, 294, 264, 960, 1105, 51740], "temperature": 0.0, "avg_logprob": -0.07352581024169921, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.001866979873739183}, {"id": 289, "seek": 171384, "start": 1713.84, "end": 1719.1999999999998, "text": " you can go look at the at the website places us on a map between the objects when we're doing", "tokens": [50364, 291, 393, 352, 574, 412, 264, 412, 264, 3144, 3190, 505, 322, 257, 4471, 1296, 264, 6565, 562, 321, 434, 884, 50632], "temperature": 0.0, "avg_logprob": -0.07054704263669635, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.0014547070022672415}, {"id": 290, "seek": 171384, "start": 1719.1999999999998, "end": 1724.3999999999999, "text": " the interpolations right so we select an object to start now we land in the latent space of big", "tokens": [50632, 264, 44902, 763, 558, 370, 321, 3048, 364, 2657, 281, 722, 586, 321, 2117, 294, 264, 48994, 1901, 295, 955, 50892], "temperature": 0.0, "avg_logprob": -0.07054704263669635, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.0014547070022672415}, {"id": 291, "seek": 171384, "start": 1724.3999999999999, "end": 1730.3999999999999, "text": " gen close to that object and then we can move ourselves around on the map between objects and", "tokens": [50892, 1049, 1998, 281, 300, 2657, 293, 550, 321, 393, 1286, 4175, 926, 322, 264, 4471, 1296, 6565, 293, 51192], "temperature": 0.0, "avg_logprob": -0.07054704263669635, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.0014547070022672415}, {"id": 292, "seek": 171384, "start": 1730.3999999999999, "end": 1736.56, "text": " their embeddings in that latent space right and as we're moving physically in 2d space here online", "tokens": [51192, 641, 12240, 29432, 294, 300, 48994, 1901, 558, 293, 382, 321, 434, 2684, 9762, 294, 568, 67, 1901, 510, 2950, 51500], "temperature": 0.0, "avg_logprob": -0.07054704263669635, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.0014547070022672415}, {"id": 293, "seek": 171384, "start": 1736.56, "end": 1742.56, "text": " we can visualize what exists at that point in latent space and then we can find its nearest", "tokens": [51500, 321, 393, 23273, 437, 8198, 412, 300, 935, 294, 48994, 1901, 293, 550, 321, 393, 915, 1080, 23831, 51800], "temperature": 0.0, "avg_logprob": -0.07054704263669635, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.0014547070022672415}, {"id": 294, "seek": 174256, "start": 1742.56, "end": 1748.6399999999999, "text": " neighbor visually uh in the met collection and find what object in the existing collection", "tokens": [50364, 5987, 19622, 2232, 294, 264, 1131, 5765, 293, 915, 437, 2657, 294, 264, 6741, 5765, 50668], "temperature": 0.0, "avg_logprob": -0.09690454781773579, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.0002867693256121129}, {"id": 295, "seek": 174256, "start": 1748.6399999999999, "end": 1753.36, "text": " is most similar to the hypothetical work that we discovered in the interstices between two", "tokens": [50668, 307, 881, 2531, 281, 264, 33053, 589, 300, 321, 6941, 294, 264, 728, 372, 1473, 1296, 732, 50904], "temperature": 0.0, "avg_logprob": -0.09690454781773579, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.0002867693256121129}, {"id": 296, "seek": 174256, "start": 1753.36, "end": 1760.3999999999999, "text": " existing works um so give that a look and this project lives on today um and a couple of different", "tokens": [50904, 6741, 1985, 1105, 370, 976, 300, 257, 574, 293, 341, 1716, 2909, 322, 965, 1105, 293, 257, 1916, 295, 819, 51256], "temperature": 0.0, "avg_logprob": -0.09690454781773579, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.0002867693256121129}, {"id": 297, "seek": 174256, "start": 1760.3999999999999, "end": 1765.76, "text": " forms I'm still working with the artist Matthew Richie he was a collaborator uh with us on the", "tokens": [51256, 6422, 286, 478, 920, 1364, 365, 264, 5748, 12434, 6781, 414, 415, 390, 257, 5091, 1639, 2232, 365, 505, 322, 264, 51524], "temperature": 0.0, "avg_logprob": -0.09690454781773579, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.0002867693256121129}, {"id": 298, "seek": 176576, "start": 1765.76, "end": 1774.16, "text": " met project um on a couple of different tendrils of of this work where we're asking all right so", "tokens": [50364, 1131, 1716, 1105, 322, 257, 1916, 295, 819, 3928, 48789, 295, 295, 341, 589, 689, 321, 434, 3365, 439, 558, 370, 50784], "temperature": 0.0, "avg_logprob": -0.10684375984724177, "compression_ratio": 1.6, "no_speech_prob": 0.005059502087533474}, {"id": 299, "seek": 176576, "start": 1774.8799999999999, "end": 1779.52, "text": " we can model projections of existing images in the met in the met collection by finding their", "tokens": [50820, 321, 393, 2316, 32371, 295, 6741, 5267, 294, 264, 1131, 294, 264, 1131, 5765, 538, 5006, 641, 51052], "temperature": 0.0, "avg_logprob": -0.10684375984724177, "compression_ratio": 1.6, "no_speech_prob": 0.005059502087533474}, {"id": 300, "seek": 176576, "start": 1779.52, "end": 1787.28, "text": " embeddings in some kind of large foundation model but now in 2021 we have things like StyleGAN ADA", "tokens": [51052, 12240, 29432, 294, 512, 733, 295, 2416, 7030, 2316, 457, 586, 294, 7201, 321, 362, 721, 411, 27004, 27699, 39354, 51440], "temperature": 0.0, "avg_logprob": -0.10684375984724177, "compression_ratio": 1.6, "no_speech_prob": 0.005059502087533474}, {"id": 301, "seek": 176576, "start": 1787.28, "end": 1791.76, "text": " that can can train on smaller data sets and do reasonably well in approximating data sets that", "tokens": [51440, 300, 393, 393, 3847, 322, 4356, 1412, 6352, 293, 360, 23551, 731, 294, 8542, 990, 1412, 6352, 300, 51664], "temperature": 0.0, "avg_logprob": -0.10684375984724177, "compression_ratio": 1.6, "no_speech_prob": 0.005059502087533474}, {"id": 302, "seek": 179176, "start": 1791.76, "end": 1796.96, "text": " would correspond to a single category in the met collection so we've done that um we've trained", "tokens": [50364, 576, 6805, 281, 257, 2167, 7719, 294, 264, 1131, 5765, 370, 321, 600, 1096, 300, 1105, 321, 600, 8895, 50624], "temperature": 0.0, "avg_logprob": -0.09740997222532709, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0006458943826146424}, {"id": 303, "seek": 179176, "start": 1796.96, "end": 1804.4, "text": " these models on sketches um Babylonian cuneiform tablets Japanese watercolors and some 18th century", "tokens": [50624, 613, 5245, 322, 34547, 1105, 30278, 952, 269, 2613, 8629, 27622, 5433, 1281, 8768, 830, 293, 512, 2443, 392, 4901, 50996], "temperature": 0.0, "avg_logprob": -0.09740997222532709, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0006458943826146424}, {"id": 304, "seek": 179176, "start": 1804.4, "end": 1811.52, "text": " European landscapes among other things and have individual models correspond to each of these", "tokens": [50996, 6473, 29822, 3654, 661, 721, 293, 362, 2609, 5245, 6805, 281, 1184, 295, 613, 51352], "temperature": 0.0, "avg_logprob": -0.09740997222532709, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0006458943826146424}, {"id": 305, "seek": 179176, "start": 1811.52, "end": 1816.8799999999999, "text": " genres within the met collection and then we've been working with a friend in New York who has a", "tokens": [51352, 30057, 1951, 264, 1131, 5765, 293, 550, 321, 600, 668, 1364, 365, 257, 1277, 294, 1873, 3609, 567, 575, 257, 51620], "temperature": 0.0, "avg_logprob": -0.09740997222532709, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0006458943826146424}, {"id": 306, "seek": 181688, "start": 1816.88, "end": 1823.44, "text": " robotic oil painter and can actually create layered paintings of really short walks in latent", "tokens": [50364, 30468, 3184, 26619, 293, 393, 767, 1884, 34666, 14880, 295, 534, 2099, 12896, 294, 48994, 50692], "temperature": 0.0, "avg_logprob": -0.07817225138346354, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.006095757707953453}, {"id": 307, "seek": 181688, "start": 1823.44, "end": 1829.5200000000002, "text": " space along different dimensions in this model so think about physically visualizing some of the", "tokens": [50692, 1901, 2051, 819, 12819, 294, 341, 2316, 370, 519, 466, 9762, 5056, 3319, 512, 295, 264, 50996], "temperature": 0.0, "avg_logprob": -0.07817225138346354, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.006095757707953453}, {"id": 308, "seek": 181688, "start": 1829.5200000000002, "end": 1834.48, "text": " durability work you've looked at in this course right could we make time paintings of really", "tokens": [50996, 33664, 589, 291, 600, 2956, 412, 294, 341, 1164, 558, 727, 321, 652, 565, 14880, 295, 534, 51244], "temperature": 0.0, "avg_logprob": -0.07817225138346354, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.006095757707953453}, {"id": 309, "seek": 181688, "start": 1834.48, "end": 1841.92, "text": " short walks in latent space by superimposing robotic paintings of the visualized image kind", "tokens": [51244, 2099, 12896, 294, 48994, 1901, 538, 1687, 8814, 6110, 30468, 14880, 295, 264, 5056, 1602, 3256, 733, 51616], "temperature": 0.0, "avg_logprob": -0.07817225138346354, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.006095757707953453}, {"id": 310, "seek": 184192, "start": 1841.92, "end": 1847.3600000000001, "text": " of at different points along that walk so that's that's being exhibited right now at UNT", "tokens": [50364, 295, 412, 819, 2793, 2051, 300, 1792, 370, 300, 311, 300, 311, 885, 49446, 558, 586, 412, 8229, 51, 50636], "temperature": 0.0, "avg_logprob": -0.1352927047427338, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.006092223804444075}, {"id": 311, "seek": 184192, "start": 1847.3600000000001, "end": 1850.88, "text": " in their contemporary art gallery let's see I've got a question in the chat room", "tokens": [50636, 294, 641, 14878, 1523, 18378, 718, 311, 536, 286, 600, 658, 257, 1168, 294, 264, 5081, 1808, 50812], "temperature": 0.0, "avg_logprob": -0.1352927047427338, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.006092223804444075}, {"id": 312, "seek": 184192, "start": 1853.2, "end": 1859.04, "text": " oh it's just a compliment I will take it at any point yeah I think", "tokens": [50928, 1954, 309, 311, 445, 257, 16250, 286, 486, 747, 309, 412, 604, 935, 1338, 286, 519, 51220], "temperature": 0.0, "avg_logprob": -0.1352927047427338, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.006092223804444075}, {"id": 313, "seek": 184192, "start": 1859.04, "end": 1865.1200000000001, "text": " can you please read it yes uh someone mentioned that this is a creative reason one of the most", "tokens": [51220, 393, 291, 1767, 1401, 309, 2086, 2232, 1580, 2835, 300, 341, 307, 257, 5880, 1778, 472, 295, 264, 881, 51524], "temperature": 0.0, "avg_logprob": -0.1352927047427338, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.006092223804444075}, {"id": 314, "seek": 184192, "start": 1865.1200000000001, "end": 1870.88, "text": " creative reasons they've seen to do latent space interpolation since it scans yeah I think that", "tokens": [51524, 5880, 4112, 436, 600, 1612, 281, 360, 48994, 1901, 44902, 399, 1670, 309, 35116, 1338, 286, 519, 300, 51812], "temperature": 0.0, "avg_logprob": -0.1352927047427338, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.006092223804444075}, {"id": 315, "seek": 187088, "start": 1870.88, "end": 1876.16, "text": " I had a slide a moment ago if you want to rewind in the recording of this suggesting that kind of", "tokens": [50364, 286, 632, 257, 4137, 257, 1623, 2057, 498, 291, 528, 281, 41458, 294, 264, 6613, 295, 341, 18094, 300, 733, 295, 50628], "temperature": 0.0, "avg_logprob": -0.0499873854897239, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0006259377114474773}, {"id": 316, "seek": 187088, "start": 1876.16, "end": 1881.8400000000001, "text": " part of the advent of using GANs to model kind of large databases of creative work is that they", "tokens": [50628, 644, 295, 264, 7045, 295, 1228, 460, 1770, 82, 281, 2316, 733, 295, 2416, 22380, 295, 5880, 589, 307, 300, 436, 50912], "temperature": 0.0, "avg_logprob": -0.0499873854897239, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0006259377114474773}, {"id": 317, "seek": 187088, "start": 1881.8400000000001, "end": 1887.3600000000001, "text": " allow us to do a couple of things right that interpolation and that iteration and in cases", "tokens": [50912, 2089, 505, 281, 360, 257, 1916, 295, 721, 558, 300, 44902, 399, 293, 300, 24784, 293, 294, 3331, 51188], "temperature": 0.0, "avg_logprob": -0.0499873854897239, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0006259377114474773}, {"id": 318, "seek": 187088, "start": 1887.3600000000001, "end": 1891.7600000000002, "text": " where you can't write down a feature language underlying a set of works because you don't", "tokens": [51188, 689, 291, 393, 380, 2464, 760, 257, 4111, 2856, 14217, 257, 992, 295, 1985, 570, 291, 500, 380, 51408], "temperature": 0.0, "avg_logprob": -0.0499873854897239, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0006259377114474773}, {"id": 319, "seek": 187088, "start": 1891.7600000000002, "end": 1898.16, "text": " know our priority what it is you can imprint that or you can learn something of that in a deep", "tokens": [51408, 458, 527, 9365, 437, 309, 307, 291, 393, 44615, 300, 420, 291, 393, 1466, 746, 295, 300, 294, 257, 2452, 51728], "temperature": 0.0, "avg_logprob": -0.0499873854897239, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0006259377114474773}, {"id": 320, "seek": 189816, "start": 1898.16, "end": 1905.3600000000001, "text": " generative model right and then you can collaborate with that and hypothesize what might lie on a", "tokens": [50364, 1337, 1166, 2316, 558, 293, 550, 291, 393, 18338, 365, 300, 293, 14276, 1125, 437, 1062, 4544, 322, 257, 50724], "temperature": 0.0, "avg_logprob": -0.038153886795043945, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.003073605243116617}, {"id": 321, "seek": 189816, "start": 1905.3600000000001, "end": 1912.0800000000002, "text": " graph of human creation if we presume that any creation artistic creation at some point in", "tokens": [50724, 4295, 295, 1952, 8016, 498, 321, 43283, 300, 604, 8016, 17090, 8016, 412, 512, 935, 294, 51060], "temperature": 0.0, "avg_logprob": -0.038153886795043945, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.003073605243116617}, {"id": 322, "seek": 189816, "start": 1912.0800000000002, "end": 1919.92, "text": " historic time is if you think about it as the manifestation of a point on kind of a sea of", "tokens": [51060, 13236, 565, 307, 498, 291, 519, 466, 309, 382, 264, 29550, 295, 257, 935, 322, 733, 295, 257, 4158, 295, 51452], "temperature": 0.0, "avg_logprob": -0.038153886795043945, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.003073605243116617}, {"id": 323, "seek": 189816, "start": 1919.92, "end": 1924.96, "text": " cultural influences and multi-generational practice iterative practice that's been shared", "tokens": [51452, 6988, 21222, 293, 4825, 12, 21848, 1478, 3124, 17138, 1166, 3124, 300, 311, 668, 5507, 51704], "temperature": 0.0, "avg_logprob": -0.038153886795043945, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.003073605243116617}, {"id": 324, "seek": 192496, "start": 1925.04, "end": 1931.04, "text": " between peoples and generations and the creation of a single work is the enactment of that process", "tokens": [50368, 1296, 16915, 293, 10593, 293, 264, 8016, 295, 257, 2167, 589, 307, 264, 25909, 518, 295, 300, 1399, 50668], "temperature": 0.0, "avg_logprob": -0.037500827811485116, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.003592483466491103}, {"id": 325, "seek": 192496, "start": 1931.04, "end": 1937.3600000000001, "text": " at some point in space it's natural to think of that in some sense as a model that we can capture", "tokens": [50668, 412, 512, 935, 294, 1901, 309, 311, 3303, 281, 519, 295, 300, 294, 512, 2020, 382, 257, 2316, 300, 321, 393, 7983, 50984], "temperature": 0.0, "avg_logprob": -0.037500827811485116, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.003592483466491103}, {"id": 326, "seek": 192496, "start": 1937.3600000000001, "end": 1942.96, "text": " in a latent space where we're manifesting some part of structured space at some moment but this", "tokens": [50984, 294, 257, 48994, 1901, 689, 321, 434, 8173, 8714, 512, 644, 295, 18519, 1901, 412, 512, 1623, 457, 341, 51264], "temperature": 0.0, "avg_logprob": -0.037500827811485116, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.003592483466491103}, {"id": 327, "seek": 192496, "start": 1942.96, "end": 1949.28, "text": " allows us to iterate on that which I argue is similar to some historic processes of iteration", "tokens": [51264, 4045, 505, 281, 44497, 322, 300, 597, 286, 9695, 307, 2531, 281, 512, 13236, 7555, 295, 24784, 51580], "temperature": 0.0, "avg_logprob": -0.037500827811485116, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.003592483466491103}, {"id": 328, "seek": 194928, "start": 1949.28, "end": 1956.3999999999999, "text": " and collaboration across groups of people really quickly right um so I've been trying to take this", "tokens": [50364, 293, 9363, 2108, 3935, 295, 561, 534, 2661, 558, 1105, 370, 286, 600, 668, 1382, 281, 747, 341, 50720], "temperature": 0.0, "avg_logprob": -0.08147754139370389, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.004607643000781536}, {"id": 329, "seek": 194928, "start": 1956.3999999999999, "end": 1963.6, "text": " a little further now and ask all right we can make paintings of short walks in latent space we can", "tokens": [50720, 257, 707, 3052, 586, 293, 1029, 439, 558, 321, 393, 652, 14880, 295, 2099, 12896, 294, 48994, 1901, 321, 393, 51080], "temperature": 0.0, "avg_logprob": -0.08147754139370389, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.004607643000781536}, {"id": 330, "seek": 194928, "start": 1963.6, "end": 1968.72, "text": " hypothesize objects that might have existed but we don't think they ever did but we still don't", "tokens": [51080, 14276, 1125, 6565, 300, 1062, 362, 13135, 457, 321, 500, 380, 519, 436, 1562, 630, 457, 321, 920, 500, 380, 51336], "temperature": 0.0, "avg_logprob": -0.08147754139370389, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.004607643000781536}, {"id": 331, "seek": 194928, "start": 1968.72, "end": 1975.76, "text": " know much about these models even if we train StyleGAN 2 on a set of 2,000 paintings in the", "tokens": [51336, 458, 709, 466, 613, 5245, 754, 498, 321, 3847, 27004, 27699, 568, 322, 257, 992, 295, 568, 11, 1360, 14880, 294, 264, 51688], "temperature": 0.0, "avg_logprob": -0.08147754139370389, "compression_ratio": 1.5524193548387097, "no_speech_prob": 0.004607643000781536}, {"id": 332, "seek": 197576, "start": 1975.76, "end": 1981.2, "text": " net collection uh you've probably seen some of the interpretability work adjacent to what Ali", "tokens": [50364, 2533, 5765, 2232, 291, 600, 1391, 1612, 512, 295, 264, 7302, 2310, 589, 24441, 281, 437, 12020, 50636], "temperature": 0.0, "avg_logprob": -0.1279818336918669, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.005057128146290779}, {"id": 333, "seek": 197576, "start": 1981.2, "end": 1986.56, "text": " has shared or David Bao's work so that style of thinking we don't know anything about this Japanese", "tokens": [50636, 575, 5507, 420, 4389, 6777, 78, 311, 589, 370, 300, 3758, 295, 1953, 321, 500, 380, 458, 1340, 466, 341, 5433, 50904], "temperature": 0.0, "avg_logprob": -0.1279818336918669, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.005057128146290779}, {"id": 334, "seek": 197576, "start": 1986.56, "end": 1991.68, "text": " watercolor model like what do its individual neurons represent is there a neuron for trees", "tokens": [50904, 31727, 2316, 411, 437, 360, 1080, 2609, 22027, 2906, 307, 456, 257, 34090, 337, 5852, 51160], "temperature": 0.0, "avg_logprob": -0.1279818336918669, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.005057128146290779}, {"id": 335, "seek": 197576, "start": 1991.68, "end": 1997.84, "text": " well what is a tree here it's some brushstrokes we recognize as a tree but it's not something that", "tokens": [51160, 731, 437, 307, 257, 4230, 510, 309, 311, 512, 5287, 27616, 5993, 321, 5521, 382, 257, 4230, 457, 309, 311, 406, 746, 300, 51468], "temperature": 0.0, "avg_logprob": -0.1279818336918669, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.005057128146290779}, {"id": 336, "seek": 197576, "start": 1998.64, "end": 2004.48, "text": " BigGAN trained on image that would necessarily recognize as a tree maybe more simply kind of in", "tokens": [51508, 5429, 27699, 8895, 322, 3256, 300, 576, 4725, 5521, 382, 257, 4230, 1310, 544, 2935, 733, 295, 294, 51800], "temperature": 0.0, "avg_logprob": -0.1279818336918669, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.005057128146290779}, {"id": 337, "seek": 200448, "start": 2004.48, "end": 2010.56, "text": " the in the steerability context what do dimensions in the latent space of a model like this", "tokens": [50364, 264, 294, 264, 30814, 2310, 4319, 437, 360, 12819, 294, 264, 48994, 1901, 295, 257, 2316, 411, 341, 50668], "temperature": 0.0, "avg_logprob": -0.06166395810571047, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.00036818129592575133}, {"id": 338, "seek": 200448, "start": 2010.56, "end": 2018.4, "text": " correspond to right sure we can find things like zoom and 3d rotation because we can name those", "tokens": [50668, 6805, 281, 558, 988, 321, 393, 915, 721, 411, 8863, 293, 805, 67, 12447, 570, 321, 393, 1315, 729, 51060], "temperature": 0.0, "avg_logprob": -0.06166395810571047, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.00036818129592575133}, {"id": 339, "seek": 200448, "start": 2018.4, "end": 2023.1200000000001, "text": " transformations and then find directions that maximally correspond to them using that kind", "tokens": [51060, 34852, 293, 550, 915, 11095, 300, 5138, 379, 6805, 281, 552, 1228, 300, 733, 51296], "temperature": 0.0, "avg_logprob": -0.06166395810571047, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.00036818129592575133}, {"id": 340, "seek": 200448, "start": 2023.1200000000001, "end": 2027.44, "text": " of steerability technique there are all sorts of other directions like the ones we're visualizing", "tokens": [51296, 295, 30814, 2310, 6532, 456, 366, 439, 7527, 295, 661, 11095, 411, 264, 2306, 321, 434, 5056, 3319, 51512], "temperature": 0.0, "avg_logprob": -0.06166395810571047, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.00036818129592575133}, {"id": 341, "seek": 200448, "start": 2027.44, "end": 2033.2, "text": " here that certainly have some affective meaning to the viewer that we don't know what they are in", "tokens": [51512, 510, 300, 3297, 362, 512, 3345, 488, 3620, 281, 264, 16767, 300, 321, 500, 380, 458, 437, 436, 366, 294, 51800], "temperature": 0.0, "avg_logprob": -0.06166395810571047, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.00036818129592575133}, {"id": 342, "seek": 203320, "start": 2033.2, "end": 2038.72, "text": " the models terms or in the viewer's terms so at this point in this project we're thinking about", "tokens": [50364, 264, 5245, 2115, 420, 294, 264, 16767, 311, 2115, 370, 412, 341, 935, 294, 341, 1716, 321, 434, 1953, 466, 50640], "temperature": 0.0, "avg_logprob": -0.08369381377037535, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0003458527207840234}, {"id": 343, "seek": 203320, "start": 2038.72, "end": 2044.32, "text": " starting to name and understand dimensions underlying generative models trained on", "tokens": [50640, 2891, 281, 1315, 293, 1223, 12819, 14217, 1337, 1166, 5245, 8895, 322, 50920], "temperature": 0.0, "avg_logprob": -0.08369381377037535, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0003458527207840234}, {"id": 344, "seek": 203320, "start": 2044.32, "end": 2049.2, "text": " bodies of artistic work from museum digital collections not only limited to the met but", "tokens": [50920, 7510, 295, 17090, 589, 490, 8441, 4562, 16641, 406, 787, 5567, 281, 264, 1131, 457, 51164], "temperature": 0.0, "avg_logprob": -0.08369381377037535, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0003458527207840234}, {"id": 345, "seek": 203320, "start": 2049.2, "end": 2055.6, "text": " around the world uh and our motivation here is to kind of create these alternate and imaginary", "tokens": [51164, 926, 264, 1002, 2232, 293, 527, 12335, 510, 307, 281, 733, 295, 1884, 613, 18873, 293, 26164, 51484], "temperature": 0.0, "avg_logprob": -0.08369381377037535, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0003458527207840234}, {"id": 346, "seek": 203320, "start": 2055.6, "end": 2060.8, "text": " histories of art built from unique latent walks that we can visualize in real time with this painting", "tokens": [51484, 30631, 295, 1523, 3094, 490, 3845, 48994, 12896, 300, 321, 393, 23273, 294, 957, 565, 365, 341, 5370, 51744], "temperature": 0.0, "avg_logprob": -0.08369381377037535, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.0003458527207840234}, {"id": 347, "seek": 206080, "start": 2060.8, "end": 2067.04, "text": " or computationally and then maybe understand something about aspects of picture language", "tokens": [50364, 420, 24903, 379, 293, 550, 1310, 1223, 746, 466, 7270, 295, 3036, 2856, 50676], "temperature": 0.0, "avg_logprob": -0.08972694903989381, "compression_ratio": 1.6, "no_speech_prob": 0.0021146268118172884}, {"id": 348, "seek": 206080, "start": 2067.04, "end": 2073.6000000000004, "text": " that might be shared across you know vastly different genres so Babylonian cuneiform tablets", "tokens": [50676, 300, 1062, 312, 5507, 2108, 291, 458, 41426, 819, 30057, 370, 30278, 952, 269, 2613, 8629, 27622, 51004], "temperature": 0.0, "avg_logprob": -0.08972694903989381, "compression_ratio": 1.6, "no_speech_prob": 0.0021146268118172884}, {"id": 349, "seek": 206080, "start": 2073.6000000000004, "end": 2080.2400000000002, "text": " transformed from numeric to symbolic and image-based at a very particular point in history and can we", "tokens": [51004, 16894, 490, 7866, 299, 281, 25755, 293, 3256, 12, 6032, 412, 257, 588, 1729, 935, 294, 2503, 293, 393, 321, 51336], "temperature": 0.0, "avg_logprob": -0.08972694903989381, "compression_ratio": 1.6, "no_speech_prob": 0.0021146268118172884}, {"id": 350, "seek": 206080, "start": 2080.2400000000002, "end": 2086.88, "text": " find a dimension in style GAN trained on a very different genre of art that corresponds to a", "tokens": [51336, 915, 257, 10139, 294, 3758, 460, 1770, 8895, 322, 257, 588, 819, 11022, 295, 1523, 300, 23249, 281, 257, 51668], "temperature": 0.0, "avg_logprob": -0.08972694903989381, "compression_ratio": 1.6, "no_speech_prob": 0.0021146268118172884}, {"id": 351, "seek": 208688, "start": 2086.96, "end": 2092.1600000000003, "text": " similar kind of transformation and as such can we build up kind of a picture language that would", "tokens": [50368, 2531, 733, 295, 9887, 293, 382, 1270, 393, 321, 1322, 493, 733, 295, 257, 3036, 2856, 300, 576, 50628], "temperature": 0.0, "avg_logprob": -0.06401242528642927, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.00059753458481282}, {"id": 352, "seek": 208688, "start": 2092.1600000000003, "end": 2098.2400000000002, "text": " correspond to diverse forms of art making right that you might not see in any of these different", "tokens": [50628, 6805, 281, 9521, 6422, 295, 1523, 1455, 558, 300, 291, 1062, 406, 536, 294, 604, 295, 613, 819, 50932], "temperature": 0.0, "avg_logprob": -0.06401242528642927, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.00059753458481282}, {"id": 353, "seek": 208688, "start": 2098.2400000000002, "end": 2103.44, "text": " categories of digital images on an archive but we might start to appreciate once we can investigate", "tokens": [50932, 10479, 295, 4562, 5267, 322, 364, 23507, 457, 321, 1062, 722, 281, 4449, 1564, 321, 393, 15013, 51192], "temperature": 0.0, "avg_logprob": -0.06401242528642927, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.00059753458481282}, {"id": 354, "seek": 208688, "start": 2103.44, "end": 2112.56, "text": " them by training deep generative models on them let's get back to great okay so when we're thinking", "tokens": [51192, 552, 538, 3097, 2452, 1337, 1166, 5245, 322, 552, 718, 311, 483, 646, 281, 869, 1392, 370, 562, 321, 434, 1953, 51648], "temperature": 0.0, "avg_logprob": -0.06401242528642927, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.00059753458481282}, {"id": 355, "seek": 211256, "start": 2112.64, "end": 2117.68, "text": " about this intersection we've seen one example of modeling the structure underlying creativity at", "tokens": [50368, 466, 341, 15236, 321, 600, 1612, 472, 1365, 295, 15983, 264, 3877, 14217, 12915, 412, 50620], "temperature": 0.0, "avg_logprob": -0.07564866383870443, "compression_ratio": 1.9086294416243654, "no_speech_prob": 0.021261774003505707}, {"id": 356, "seek": 211256, "start": 2117.68, "end": 2123.7599999999998, "text": " scale and i've done other projects and you can find many examples online both of my work and", "tokens": [50620, 4373, 293, 741, 600, 1096, 661, 4455, 293, 291, 393, 915, 867, 5110, 2950, 1293, 295, 452, 589, 293, 50924], "temperature": 0.0, "avg_logprob": -0.07564866383870443, "compression_ratio": 1.9086294416243654, "no_speech_prob": 0.021261774003505707}, {"id": 357, "seek": 211256, "start": 2123.7599999999998, "end": 2128.72, "text": " other peoples of trying to do this not for creativity at scale but for individual instances", "tokens": [50924, 661, 16915, 295, 1382, 281, 360, 341, 406, 337, 12915, 412, 4373, 457, 337, 2609, 14519, 51172], "temperature": 0.0, "avg_logprob": -0.07564866383870443, "compression_ratio": 1.9086294416243654, "no_speech_prob": 0.021261774003505707}, {"id": 358, "seek": 211256, "start": 2128.72, "end": 2135.68, "text": " of individual artists and modeling either the style or the processes of individual art making", "tokens": [51172, 295, 2609, 6910, 293, 15983, 2139, 264, 3758, 420, 264, 7555, 295, 2609, 1523, 1455, 51520], "temperature": 0.0, "avg_logprob": -0.07564866383870443, "compression_ratio": 1.9086294416243654, "no_speech_prob": 0.021261774003505707}, {"id": 359, "seek": 213568, "start": 2135.68, "end": 2142.3999999999996, "text": " techniques so all of these are kind of flavors of starting to imprint or grok or understand the", "tokens": [50364, 7512, 370, 439, 295, 613, 366, 733, 295, 16303, 295, 2891, 281, 44615, 420, 4634, 74, 420, 1223, 264, 50700], "temperature": 0.0, "avg_logprob": -0.04309310436248779, "compression_ratio": 1.75, "no_speech_prob": 0.009406614117324352}, {"id": 360, "seek": 213568, "start": 2142.3999999999996, "end": 2147.68, "text": " structure underlying creativity but not symbolically right so we don't we don't know how to interpret", "tokens": [50700, 3877, 14217, 12915, 457, 406, 5986, 984, 558, 370, 321, 500, 380, 321, 500, 380, 458, 577, 281, 7302, 50964], "temperature": 0.0, "avg_logprob": -0.04309310436248779, "compression_ratio": 1.75, "no_speech_prob": 0.009406614117324352}, {"id": 361, "seek": 213568, "start": 2147.68, "end": 2152.48, "text": " these models even though we can visualize them and create really interesting hypothetical objects", "tokens": [50964, 613, 5245, 754, 1673, 321, 393, 23273, 552, 293, 1884, 534, 1880, 33053, 6565, 51204], "temperature": 0.0, "avg_logprob": -0.04309310436248779, "compression_ratio": 1.75, "no_speech_prob": 0.009406614117324352}, {"id": 362, "seek": 213568, "start": 2152.48, "end": 2157.68, "text": " that might be indistinguishable either from existing work or from one artist's particular style", "tokens": [51204, 300, 1062, 312, 1016, 468, 7050, 742, 712, 2139, 490, 6741, 589, 420, 490, 472, 5748, 311, 1729, 3758, 51464], "temperature": 0.0, "avg_logprob": -0.04309310436248779, "compression_ratio": 1.75, "no_speech_prob": 0.009406614117324352}, {"id": 363, "seek": 213568, "start": 2159.12, "end": 2164.3999999999996, "text": " we can also think about these models as a tool themselves for collaboration both in their creation", "tokens": [51536, 321, 393, 611, 519, 466, 613, 5245, 382, 257, 2290, 2969, 337, 9363, 1293, 294, 641, 8016, 51800], "temperature": 0.0, "avg_logprob": -0.04309310436248779, "compression_ratio": 1.75, "no_speech_prob": 0.009406614117324352}, {"id": 364, "seek": 216440, "start": 2164.4, "end": 2170.0, "text": " and iteration with others who contribute to their models and with the models themselves", "tokens": [50364, 293, 24784, 365, 2357, 567, 10586, 281, 641, 5245, 293, 365, 264, 5245, 2969, 50644], "temperature": 0.0, "avg_logprob": -0.11600617860492907, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.00035683787427842617}, {"id": 365, "seek": 216440, "start": 2170.0, "end": 2175.44, "text": " which as i described represent kind of executable versions of collective cultural structure we", "tokens": [50644, 597, 382, 741, 7619, 2906, 733, 295, 7568, 712, 9606, 295, 12590, 6988, 3877, 321, 50916], "temperature": 0.0, "avg_logprob": -0.11600617860492907, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.00035683787427842617}, {"id": 366, "seek": 216440, "start": 2175.44, "end": 2181.2000000000003, "text": " permit permit ourselves to think about them that way or facets of kind of a global creative identity", "tokens": [50916, 13423, 13423, 4175, 281, 519, 466, 552, 300, 636, 420, 49752, 295, 733, 295, 257, 4338, 5880, 6575, 51204], "temperature": 0.0, "avg_logprob": -0.11600617860492907, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.00035683787427842617}, {"id": 367, "seek": 216440, "start": 2182.2400000000002, "end": 2186.8, "text": " but as i mentioned now we're at a point with tools and computer vision where we can start to ask", "tokens": [51256, 457, 382, 741, 2835, 586, 321, 434, 412, 257, 935, 365, 3873, 293, 3820, 5201, 689, 321, 393, 722, 281, 1029, 51484], "temperature": 0.0, "avg_logprob": -0.11600617860492907, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.00035683787427842617}, {"id": 368, "seek": 216440, "start": 2187.6, "end": 2193.84, "text": " what rep representations actually underlie these models trained on artworks that are themselves", "tokens": [51524, 437, 1085, 33358, 767, 833, 6302, 613, 5245, 8895, 322, 15829, 82, 300, 366, 2969, 51836], "temperature": 0.0, "avg_logprob": -0.11600617860492907, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.00035683787427842617}, {"id": 369, "seek": 219384, "start": 2193.92, "end": 2197.76, "text": " executable versions of some collective cultural structure right well what is the structure what's", "tokens": [50368, 7568, 712, 9606, 295, 512, 12590, 6988, 3877, 558, 731, 437, 307, 264, 3877, 437, 311, 50560], "temperature": 0.0, "avg_logprob": -0.04166363908889446, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.0009842057479545474}, {"id": 370, "seek": 219384, "start": 2197.76, "end": 2204.48, "text": " going on under the hood do they correspond to dimensions that we find meaningful when we look", "tokens": [50560, 516, 322, 833, 264, 13376, 360, 436, 6805, 281, 12819, 300, 321, 915, 10995, 562, 321, 574, 50896], "temperature": 0.0, "avg_logprob": -0.04166363908889446, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.0009842057479545474}, {"id": 371, "seek": 219384, "start": 2204.48, "end": 2210.2400000000002, "text": " at visual scenes and so in the next part of the talk i'll share a couple maybe more technical", "tokens": [50896, 412, 5056, 8026, 293, 370, 294, 264, 958, 644, 295, 264, 751, 741, 603, 2073, 257, 1916, 1310, 544, 6191, 51184], "temperature": 0.0, "avg_logprob": -0.04166363908889446, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.0009842057479545474}, {"id": 372, "seek": 219384, "start": 2210.2400000000002, "end": 2216.0, "text": " projects that explore specific ways that humans can interact with generative models", "tokens": [51184, 4455, 300, 6839, 2685, 2098, 300, 6255, 393, 4648, 365, 1337, 1166, 5245, 51472], "temperature": 0.0, "avg_logprob": -0.04166363908889446, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.0009842057479545474}, {"id": 373, "seek": 219384, "start": 2216.6400000000003, "end": 2221.28, "text": " in order to maybe learn something about human vision as well right so can we build", "tokens": [51504, 294, 1668, 281, 1310, 1466, 746, 466, 1952, 5201, 382, 731, 558, 370, 393, 321, 1322, 51736], "temperature": 0.0, "avg_logprob": -0.04166363908889446, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.0009842057479545474}, {"id": 374, "seek": 222128, "start": 2221.76, "end": 2227.44, "text": " shared vocabularies that help us interpret dimensions underlying these models by designing", "tokens": [50388, 5507, 2329, 455, 1040, 530, 300, 854, 505, 7302, 12819, 14217, 613, 5245, 538, 14685, 50672], "temperature": 0.0, "avg_logprob": -0.07298234152415442, "compression_ratio": 1.5754189944134078, "no_speech_prob": 0.005379314534366131}, {"id": 375, "seek": 222128, "start": 2227.44, "end": 2233.1200000000003, "text": " experiments that allow us to visualize and interact with images and latent walks like you've been", "tokens": [50672, 12050, 300, 2089, 505, 281, 23273, 293, 4648, 365, 5267, 293, 48994, 12896, 411, 291, 600, 668, 50956], "temperature": 0.0, "avg_logprob": -0.07298234152415442, "compression_ratio": 1.5754189944134078, "no_speech_prob": 0.005379314534366131}, {"id": 376, "seek": 222128, "start": 2233.1200000000003, "end": 2239.0400000000004, "text": " seeing i'll pause here because i need a sip of water and i'll keep an eye on the chat in case", "tokens": [50956, 2577, 741, 603, 10465, 510, 570, 741, 643, 257, 29668, 295, 1281, 293, 741, 603, 1066, 364, 3313, 322, 264, 5081, 294, 1389, 51252], "temperature": 0.0, "avg_logprob": -0.07298234152415442, "compression_ratio": 1.5754189944134078, "no_speech_prob": 0.005379314534366131}, {"id": 377, "seek": 223904, "start": 2239.04, "end": 2248.48, "text": " anyone has any questions before we go on", "tokens": [50364, 2878, 575, 604, 1651, 949, 321, 352, 322, 50836], "temperature": 0.0, "avg_logprob": -0.14726502554757254, "compression_ratio": 1.5149253731343284, "no_speech_prob": 0.025547409430146217}, {"id": 378, "seek": 223904, "start": 2253.7599999999998, "end": 2259.44, "text": " all right looks like we are question free so far five more seconds", "tokens": [51100, 439, 558, 1542, 411, 321, 366, 1168, 1737, 370, 1400, 1732, 544, 3949, 51384], "temperature": 0.0, "avg_logprob": -0.14726502554757254, "compression_ratio": 1.5149253731343284, "no_speech_prob": 0.025547409430146217}, {"id": 379, "seek": 223904, "start": 2262.88, "end": 2268.16, "text": " i guess i have a question yeah so this might be talked about later but i was wondering a little", "tokens": [51556, 741, 2041, 741, 362, 257, 1168, 1338, 370, 341, 1062, 312, 2825, 466, 1780, 457, 741, 390, 6359, 257, 707, 51820], "temperature": 0.0, "avg_logprob": -0.14726502554757254, "compression_ratio": 1.5149253731343284, "no_speech_prob": 0.025547409430146217}, {"id": 380, "seek": 226816, "start": 2268.24, "end": 2274.48, "text": " bit about like in your research and kind of this field how much of like human interaction is like a", "tokens": [50368, 857, 466, 411, 294, 428, 2132, 293, 733, 295, 341, 2519, 577, 709, 295, 411, 1952, 9285, 307, 411, 257, 50680], "temperature": 0.0, "avg_logprob": -0.08113607734140724, "compression_ratio": 1.9152542372881356, "no_speech_prob": 0.002114600734785199}, {"id": 381, "seek": 226816, "start": 2274.48, "end": 2279.7599999999998, "text": " big part of it and kind of like the human coming in and saying uh how they think about something", "tokens": [50680, 955, 644, 295, 309, 293, 733, 295, 411, 264, 1952, 1348, 294, 293, 1566, 2232, 577, 436, 519, 466, 746, 50944], "temperature": 0.0, "avg_logprob": -0.08113607734140724, "compression_ratio": 1.9152542372881356, "no_speech_prob": 0.002114600734785199}, {"id": 382, "seek": 226816, "start": 2279.7599999999998, "end": 2284.7999999999997, "text": " and see where that agrees with the computer or like kind of like where that role is played", "tokens": [50944, 293, 536, 689, 300, 26383, 365, 264, 3820, 420, 411, 733, 295, 411, 689, 300, 3090, 307, 3737, 51196], "temperature": 0.0, "avg_logprob": -0.08113607734140724, "compression_ratio": 1.9152542372881356, "no_speech_prob": 0.002114600734785199}, {"id": 383, "seek": 226816, "start": 2286.24, "end": 2289.92, "text": " wonderful question so these kind these kinds of high-level questions that", "tokens": [51268, 3715, 1168, 370, 613, 733, 613, 3685, 295, 1090, 12, 12418, 1651, 300, 51452], "temperature": 0.0, "avg_logprob": -0.08113607734140724, "compression_ratio": 1.9152542372881356, "no_speech_prob": 0.002114600734785199}, {"id": 384, "seek": 226816, "start": 2289.92, "end": 2294.8799999999997, "text": " get it some experiential component or design component of the worker i think really useful", "tokens": [51452, 483, 309, 512, 49611, 831, 6542, 420, 1715, 6542, 295, 264, 11346, 741, 519, 534, 4420, 51700], "temperature": 0.0, "avg_logprob": -0.08113607734140724, "compression_ratio": 1.9152542372881356, "no_speech_prob": 0.002114600734785199}, {"id": 385, "seek": 229488, "start": 2294.88, "end": 2299.44, "text": " ask more of them i'll tell you for different projects what that looks like and in the next", "tokens": [50364, 1029, 544, 295, 552, 741, 603, 980, 291, 337, 819, 4455, 437, 300, 1542, 411, 293, 294, 264, 958, 50592], "temperature": 0.0, "avg_logprob": -0.11058078947521391, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.0014544755686074495}, {"id": 386, "seek": 229488, "start": 2299.44, "end": 2305.44, "text": " section of work it's going to be really obvious because there's human annotations but for this", "tokens": [50592, 3541, 295, 589, 309, 311, 516, 281, 312, 534, 6322, 570, 456, 311, 1952, 25339, 763, 457, 337, 341, 50892], "temperature": 0.0, "avg_logprob": -0.11058078947521391, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.0014544755686074495}, {"id": 387, "seek": 229488, "start": 2305.44, "end": 2311.92, "text": " project so the human would come in here you know we train models on datasets of art selected from", "tokens": [50892, 1716, 370, 264, 1952, 576, 808, 294, 510, 291, 458, 321, 3847, 5245, 322, 42856, 295, 1523, 8209, 490, 51216], "temperature": 0.0, "avg_logprob": -0.11058078947521391, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.0014544755686074495}, {"id": 388, "seek": 229488, "start": 2311.92, "end": 2316.8, "text": " the met collection and these are small and these are subsets and they were gathered by", "tokens": [51216, 264, 1131, 5765, 293, 613, 366, 1359, 293, 613, 366, 2090, 1385, 293, 436, 645, 13032, 538, 51460], "temperature": 0.0, "avg_logprob": -0.11058078947521391, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.0014544755686074495}, {"id": 389, "seek": 229488, "start": 2317.52, "end": 2322.96, "text": " matthew richie and myself going through different genres in the digital collection of the met online", "tokens": [51496, 3803, 392, 1023, 4593, 414, 293, 2059, 516, 807, 819, 30057, 294, 264, 4562, 5765, 295, 264, 1131, 2950, 51768], "temperature": 0.0, "avg_logprob": -0.11058078947521391, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.0014544755686074495}, {"id": 390, "seek": 232296, "start": 2323.92, "end": 2329.52, "text": " and like hand selecting images from those different genres right representative images of", "tokens": [50412, 293, 411, 1011, 18182, 5267, 490, 729, 819, 30057, 558, 12424, 5267, 295, 50692], "temperature": 0.0, "avg_logprob": -0.08511420039387492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0008827080018818378}, {"id": 391, "seek": 232296, "start": 2329.52, "end": 2336.16, "text": " different categories of work or maybe in a less fine grained way all images under some designation", "tokens": [50692, 819, 10479, 295, 589, 420, 1310, 294, 257, 1570, 2489, 1295, 2001, 636, 439, 5267, 833, 512, 40838, 51024], "temperature": 0.0, "avg_logprob": -0.08511420039387492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0008827080018818378}, {"id": 392, "seek": 232296, "start": 2336.16, "end": 2342.32, "text": " so japanese watercolors between the 17th and 19th centuries so we made that selection and", "tokens": [51024, 370, 49508, 1281, 8768, 830, 1296, 264, 3282, 392, 293, 1294, 392, 13926, 370, 321, 1027, 300, 9450, 293, 51332], "temperature": 0.0, "avg_logprob": -0.08511420039387492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0008827080018818378}, {"id": 393, "seek": 232296, "start": 2342.32, "end": 2347.76, "text": " tried training these models on a bunch of different such selections and decided which ended up you", "tokens": [51332, 3031, 3097, 613, 5245, 322, 257, 3840, 295, 819, 1270, 47829, 293, 3047, 597, 4590, 493, 291, 51604], "temperature": 0.0, "avg_logprob": -0.08511420039387492, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0008827080018818378}, {"id": 394, "seek": 234776, "start": 2347.76, "end": 2353.0400000000004, "text": " know with so few examples providing at least a representative sample of the kind of work that", "tokens": [50364, 458, 365, 370, 1326, 5110, 6530, 412, 1935, 257, 12424, 6889, 295, 264, 733, 295, 589, 300, 50628], "temperature": 0.0, "avg_logprob": -0.055033365885416664, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.001809751964174211}, {"id": 395, "seek": 234776, "start": 2353.0400000000004, "end": 2360.96, "text": " we know we saw there right and then here the selection of like walks through latent space", "tokens": [50628, 321, 458, 321, 1866, 456, 558, 293, 550, 510, 264, 9450, 295, 411, 12896, 807, 48994, 1901, 51024], "temperature": 0.0, "avg_logprob": -0.055033365885416664, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.001809751964174211}, {"id": 396, "seek": 234776, "start": 2360.96, "end": 2363.92, "text": " so think of those in the same way you've been thinking about the steerability walks", "tokens": [51024, 370, 519, 295, 729, 294, 264, 912, 636, 291, 600, 668, 1953, 466, 264, 30814, 2310, 12896, 51172], "temperature": 0.0, "avg_logprob": -0.055033365885416664, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.001809751964174211}, {"id": 397, "seek": 234776, "start": 2364.8, "end": 2369.84, "text": " they were very arbitrary so that was a completely human selected so it's a kind of a different", "tokens": [51216, 436, 645, 588, 23211, 370, 300, 390, 257, 2584, 1952, 8209, 370, 309, 311, 257, 733, 295, 257, 819, 51468], "temperature": 0.0, "avg_logprob": -0.055033365885416664, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.001809751964174211}, {"id": 398, "seek": 234776, "start": 2369.84, "end": 2376.4, "text": " approach to interpretability where it's steered by the human eye right we're not doing it automatically", "tokens": [51468, 3109, 281, 7302, 2310, 689, 309, 311, 2126, 4073, 538, 264, 1952, 3313, 558, 321, 434, 406, 884, 309, 6772, 51796], "temperature": 0.0, "avg_logprob": -0.055033365885416664, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.001809751964174211}, {"id": 399, "seek": 237640, "start": 2376.4, "end": 2382.4, "text": " and we're not doing symbolic it symbolically we don't know what these correspond to but that's", "tokens": [50364, 293, 321, 434, 406, 884, 25755, 309, 5986, 984, 321, 500, 380, 458, 437, 613, 6805, 281, 457, 300, 311, 50664], "temperature": 0.0, "avg_logprob": -0.06011601990344478, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0012443984160199761}, {"id": 400, "seek": 237640, "start": 2383.44, "end": 2387.6, "text": " trying some arbitrary walk through latent space trying many of them and then the human then", "tokens": [50716, 1382, 512, 23211, 1792, 807, 48994, 1901, 1382, 867, 295, 552, 293, 550, 264, 1952, 550, 50924], "temperature": 0.0, "avg_logprob": -0.06011601990344478, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0012443984160199761}, {"id": 401, "seek": 237640, "start": 2387.6, "end": 2392.64, "text": " selecting what to them felt like an artistic expression this is an art exhibit and then", "tokens": [50924, 18182, 437, 281, 552, 2762, 411, 364, 17090, 6114, 341, 307, 364, 1523, 20487, 293, 550, 51176], "temperature": 0.0, "avg_logprob": -0.06011601990344478, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0012443984160199761}, {"id": 402, "seek": 237640, "start": 2392.64, "end": 2398.08, "text": " in the next step we'll ask how can we do that in a more systematic way and start to build", "tokens": [51176, 294, 264, 958, 1823, 321, 603, 1029, 577, 393, 321, 360, 300, 294, 257, 544, 27249, 636, 293, 722, 281, 1322, 51448], "temperature": 0.0, "avg_logprob": -0.06011601990344478, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0012443984160199761}, {"id": 403, "seek": 237640, "start": 2398.08, "end": 2402.48, "text": " a language corresponding to what those different walks could be a language that's shared by humans", "tokens": [51448, 257, 2856, 11760, 281, 437, 729, 819, 12896, 727, 312, 257, 2856, 300, 311, 5507, 538, 6255, 51668], "temperature": 0.0, "avg_logprob": -0.06011601990344478, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0012443984160199761}, {"id": 404, "seek": 240248, "start": 2402.48, "end": 2408.4, "text": " and that takes at least right now a lot of human a lot of human interaction", "tokens": [50364, 293, 300, 2516, 412, 1935, 558, 586, 257, 688, 295, 1952, 257, 688, 295, 1952, 9285, 50660], "temperature": 0.0, "avg_logprob": -0.06613964101542598, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0003352486528456211}, {"id": 405, "seek": 240248, "start": 2409.2, "end": 2412.2400000000002, "text": " you could think about ways to automate that we'll talk about that in a second", "tokens": [50700, 291, 727, 519, 466, 2098, 281, 31605, 300, 321, 603, 751, 466, 300, 294, 257, 1150, 50852], "temperature": 0.0, "avg_logprob": -0.06613964101542598, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0003352486528456211}, {"id": 406, "seek": 240248, "start": 2413.44, "end": 2418.96, "text": " but with any kind of human interaction it's nice to preserve the opportunity for direct", "tokens": [50912, 457, 365, 604, 733, 295, 1952, 9285, 309, 311, 1481, 281, 15665, 264, 2650, 337, 2047, 51188], "temperature": 0.0, "avg_logprob": -0.06613964101542598, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0003352486528456211}, {"id": 407, "seek": 240248, "start": 2418.96, "end": 2423.68, "text": " engagement with models rather than intermediation by a captioner or something like that because", "tokens": [51188, 8742, 365, 5245, 2831, 813, 15184, 399, 538, 257, 31974, 260, 420, 746, 411, 300, 570, 51424], "temperature": 0.0, "avg_logprob": -0.06613964101542598, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0003352486528456211}, {"id": 408, "seek": 240248, "start": 2423.68, "end": 2428.88, "text": " then you could imagine using your technique on different subsets of humans right on different", "tokens": [51424, 550, 291, 727, 3811, 1228, 428, 6532, 322, 819, 2090, 1385, 295, 6255, 558, 322, 819, 51684], "temperature": 0.0, "avg_logprob": -0.06613964101542598, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0003352486528456211}, {"id": 409, "seek": 242888, "start": 2428.88, "end": 2433.84, "text": " kinds of experiences so you might imagine getting an art historian to label and select", "tokens": [50364, 3685, 295, 5235, 370, 291, 1062, 3811, 1242, 364, 1523, 25139, 281, 7645, 293, 3048, 50612], "temperature": 0.0, "avg_logprob": -0.05704227748670076, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.003373776562511921}, {"id": 410, "seek": 242888, "start": 2433.84, "end": 2438.0, "text": " different walks through latent space here corresponding to very nuanced changes in the", "tokens": [50612, 819, 12896, 807, 48994, 1901, 510, 11760, 281, 588, 45115, 2962, 294, 264, 50820], "temperature": 0.0, "avg_logprob": -0.05704227748670076, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.003373776562511921}, {"id": 411, "seek": 242888, "start": 2438.0, "end": 2443.28, "text": " development of Babylonian like cuneiform tablets right that a captioner couldn't recognize I", "tokens": [50820, 3250, 295, 30278, 952, 411, 269, 2613, 8629, 27622, 558, 300, 257, 31974, 260, 2809, 380, 5521, 286, 51084], "temperature": 0.0, "avg_logprob": -0.05704227748670076, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.003373776562511921}, {"id": 412, "seek": 242888, "start": 2443.28, "end": 2448.32, "text": " couldn't recognize so you might want to be able to pull different kinds of humans into the loop", "tokens": [51084, 2809, 380, 5521, 370, 291, 1062, 528, 281, 312, 1075, 281, 2235, 819, 3685, 295, 6255, 666, 264, 6367, 51336], "temperature": 0.0, "avg_logprob": -0.05704227748670076, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.003373776562511921}, {"id": 413, "seek": 242888, "start": 2448.32, "end": 2453.6800000000003, "text": " at different times to engage in ways that kind of use their knowledge to create a unique synthesis", "tokens": [51336, 412, 819, 1413, 281, 4683, 294, 2098, 300, 733, 295, 764, 641, 3601, 281, 1884, 257, 3845, 30252, 51604], "temperature": 0.0, "avg_logprob": -0.05704227748670076, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.003373776562511921}, {"id": 414, "seek": 245368, "start": 2453.68, "end": 2459.9199999999996, "text": " with a generative model so that's what engagement looked like here and then with this next project", "tokens": [50364, 365, 257, 1337, 1166, 2316, 370, 300, 311, 437, 8742, 2956, 411, 510, 293, 550, 365, 341, 958, 1716, 50676], "temperature": 0.0, "avg_logprob": -0.07278969552781847, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0051362779922783375}, {"id": 415, "seek": 245368, "start": 2459.9199999999996, "end": 2466.56, "text": " it'll be super it'll be super clear and I'll make sure to speak specifically to that so thank you", "tokens": [50676, 309, 603, 312, 1687, 309, 603, 312, 1687, 1850, 293, 286, 603, 652, 988, 281, 1710, 4682, 281, 300, 370, 1309, 291, 51008], "temperature": 0.0, "avg_logprob": -0.07278969552781847, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0051362779922783375}, {"id": 416, "seek": 245368, "start": 2466.56, "end": 2474.8799999999997, "text": " yeah cool so next this is probably a summary of what you've seen so far in your IAP course so", "tokens": [51008, 1338, 1627, 370, 958, 341, 307, 1391, 257, 12691, 295, 437, 291, 600, 1612, 370, 1400, 294, 428, 286, 4715, 1164, 370, 51424], "temperature": 0.0, "avg_logprob": -0.07278969552781847, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0051362779922783375}, {"id": 417, "seek": 245368, "start": 2474.8799999999997, "end": 2480.16, "text": " there's a lot of different work on discovery of interpretable directions in the latent space", "tokens": [51424, 456, 311, 257, 688, 295, 819, 589, 322, 12114, 295, 7302, 712, 11095, 294, 264, 48994, 1901, 51688], "temperature": 0.0, "avg_logprob": -0.07278969552781847, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0051362779922783375}, {"id": 418, "seek": 248016, "start": 2480.16, "end": 2485.12, "text": " of different generative models right and we can steer images along those dimensions to create", "tokens": [50364, 295, 819, 1337, 1166, 5245, 558, 293, 321, 393, 30814, 5267, 2051, 729, 12819, 281, 1884, 50612], "temperature": 0.0, "avg_logprob": -0.05517974976570376, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0021816103253513575}, {"id": 419, "seek": 248016, "start": 2485.12, "end": 2490.3199999999997, "text": " interpretable transformations that allow us to interact creatively with deep generative models", "tokens": [50612, 7302, 712, 34852, 300, 2089, 505, 281, 4648, 43750, 365, 2452, 1337, 1166, 5245, 50872], "temperature": 0.0, "avg_logprob": -0.05517974976570376, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0021816103253513575}, {"id": 420, "seek": 248016, "start": 2490.3199999999997, "end": 2498.24, "text": " right here we are deep learning for creativity but a lot of these examples presume what concepts", "tokens": [50872, 558, 510, 321, 366, 2452, 2539, 337, 12915, 457, 257, 688, 295, 613, 5110, 43283, 437, 10392, 51268], "temperature": 0.0, "avg_logprob": -0.05517974976570376, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0021816103253513575}, {"id": 421, "seek": 248016, "start": 2498.24, "end": 2503.12, "text": " we're searching for in the latent space and in fact they do that really explicitly right we", "tokens": [51268, 321, 434, 10808, 337, 294, 264, 48994, 1901, 293, 294, 1186, 436, 360, 300, 534, 20803, 558, 321, 51512], "temperature": 0.0, "avg_logprob": -0.05517974976570376, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0021816103253513575}, {"id": 422, "seek": 248016, "start": 2503.12, "end": 2508.8799999999997, "text": " will pre-define a zoom transformation and then maximize the similarity between some transformation", "tokens": [51512, 486, 659, 12, 20595, 533, 257, 8863, 9887, 293, 550, 19874, 264, 32194, 1296, 512, 9887, 51800], "temperature": 0.0, "avg_logprob": -0.05517974976570376, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0021816103253513575}, {"id": 423, "seek": 250888, "start": 2508.88, "end": 2514.0, "text": " in the latent space and a zoom transformation as applied to some image maybe you've experimented", "tokens": [50364, 294, 264, 48994, 1901, 293, 257, 8863, 9887, 382, 6456, 281, 512, 3256, 1310, 291, 600, 5120, 292, 50620], "temperature": 0.0, "avg_logprob": -0.052966743434241056, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.00044409860856831074}, {"id": 424, "seek": 250888, "start": 2514.0, "end": 2520.0, "text": " with code for doing that but that presumes we know we're looking for zoom in the first place", "tokens": [50620, 365, 3089, 337, 884, 300, 457, 300, 1183, 10018, 321, 458, 321, 434, 1237, 337, 8863, 294, 264, 700, 1081, 50920], "temperature": 0.0, "avg_logprob": -0.052966743434241056, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.00044409860856831074}, {"id": 425, "seek": 250888, "start": 2520.0, "end": 2524.4, "text": " what if we find ourselves looking out into more you know uncharted waters so to speak", "tokens": [50920, 437, 498, 321, 915, 4175, 1237, 484, 666, 544, 291, 458, 33686, 47350, 12975, 370, 281, 1710, 51140], "temperature": 0.0, "avg_logprob": -0.052966743434241056, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.00044409860856831074}, {"id": 426, "seek": 250888, "start": 2525.36, "end": 2531.04, "text": " here we ask how we can learn kind of a vocabulary of visual concepts maybe one that you would apply", "tokens": [51188, 510, 321, 1029, 577, 321, 393, 1466, 733, 295, 257, 19864, 295, 5056, 10392, 1310, 472, 300, 291, 576, 3079, 51472], "temperature": 0.0, "avg_logprob": -0.052966743434241056, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.00044409860856831074}, {"id": 427, "seek": 250888, "start": 2531.04, "end": 2536.1600000000003, "text": " to those style gains we just saw train on the net images right we don't maybe we could look for zoom", "tokens": [51472, 281, 729, 3758, 16823, 321, 445, 1866, 3847, 322, 264, 2533, 5267, 558, 321, 500, 380, 1310, 321, 727, 574, 337, 8863, 51728], "temperature": 0.0, "avg_logprob": -0.052966743434241056, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.00044409860856831074}, {"id": 428, "seek": 253616, "start": 2536.16, "end": 2539.92, "text": " but maybe there's all sorts of more interesting transformations we could do to those images", "tokens": [50364, 457, 1310, 456, 311, 439, 7527, 295, 544, 1880, 34852, 321, 727, 360, 281, 729, 5267, 50552], "temperature": 0.0, "avg_logprob": -0.05578544537226359, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.0009108066442422569}, {"id": 429, "seek": 253616, "start": 2539.92, "end": 2544.24, "text": " but we don't know what they are yet how can we learn a vocabulary of visual concepts rather", "tokens": [50552, 457, 321, 500, 380, 458, 437, 436, 366, 1939, 577, 393, 321, 1466, 257, 19864, 295, 5056, 10392, 2831, 50768], "temperature": 0.0, "avg_logprob": -0.05578544537226359, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.0009108066442422569}, {"id": 430, "seek": 253616, "start": 2544.24, "end": 2550.0, "text": " than pre-define them or labeling them after the fact so there are a variety now of unsupervised", "tokens": [50768, 813, 659, 12, 20595, 533, 552, 420, 40244, 552, 934, 264, 1186, 370, 456, 366, 257, 5673, 586, 295, 2693, 12879, 24420, 51056], "temperature": 0.0, "avg_logprob": -0.05578544537226359, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.0009108066442422569}, {"id": 431, "seek": 253616, "start": 2550.0, "end": 2556.24, "text": " methods for distilling these kinds of transformations in latent space that find principal components", "tokens": [51056, 7150, 337, 1483, 7345, 613, 3685, 295, 34852, 294, 48994, 1901, 300, 915, 9716, 6677, 51368], "temperature": 0.0, "avg_logprob": -0.05578544537226359, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.0009108066442422569}, {"id": 432, "seek": 253616, "start": 2556.24, "end": 2560.24, "text": " of feature space of different layers the models activation maybe you've played around with methods", "tokens": [51368, 295, 4111, 1901, 295, 819, 7914, 264, 5245, 24433, 1310, 291, 600, 3737, 926, 365, 7150, 51568], "temperature": 0.0, "avg_logprob": -0.05578544537226359, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.0009108066442422569}, {"id": 433, "seek": 253616, "start": 2560.24, "end": 2565.6, "text": " like GAN space that search for and rank where the largest principal components of the feature space", "tokens": [51568, 411, 460, 1770, 1901, 300, 3164, 337, 293, 6181, 689, 264, 6443, 9716, 6677, 295, 264, 4111, 1901, 51836], "temperature": 0.0, "avg_logprob": -0.05578544537226359, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.0009108066442422569}, {"id": 434, "seek": 256560, "start": 2565.6, "end": 2570.24, "text": " which do provide us with interpretable transformations but they're labeled after the", "tokens": [50364, 597, 360, 2893, 505, 365, 7302, 712, 34852, 457, 436, 434, 21335, 934, 264, 50596], "temperature": 0.0, "avg_logprob": -0.046999283886830744, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.00028673477936536074}, {"id": 435, "seek": 256560, "start": 2570.24, "end": 2576.48, "text": " fact so we don't know if they're meaningful to humans kind of in their genesis but we can we", "tokens": [50596, 1186, 370, 321, 500, 380, 458, 498, 436, 434, 10995, 281, 6255, 733, 295, 294, 641, 1049, 9374, 457, 321, 393, 321, 50908], "temperature": 0.0, "avg_logprob": -0.046999283886830744, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.00028673477936536074}, {"id": 436, "seek": 256560, "start": 2576.48, "end": 2580.96, "text": " can describe them right by providing labels to them another point where the human kind of comes", "tokens": [50908, 393, 6786, 552, 558, 538, 6530, 16949, 281, 552, 1071, 935, 689, 264, 1952, 733, 295, 1487, 51132], "temperature": 0.0, "avg_logprob": -0.046999283886830744, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.00028673477936536074}, {"id": 437, "seek": 256560, "start": 2580.96, "end": 2587.52, "text": " in the loop but we want to see if we can build in human vision to the discovery process right so", "tokens": [51132, 294, 264, 6367, 457, 321, 528, 281, 536, 498, 321, 393, 1322, 294, 1952, 5201, 281, 264, 12114, 1399, 558, 370, 51460], "temperature": 0.0, "avg_logprob": -0.046999283886830744, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.00028673477936536074}, {"id": 438, "seek": 256560, "start": 2587.52, "end": 2593.52, "text": " to supervise it but to not pre-commit to what kinds of concepts we're searching for so in this", "tokens": [51460, 281, 37971, 908, 309, 457, 281, 406, 659, 12, 1112, 3508, 281, 437, 3685, 295, 10392, 321, 434, 10808, 337, 370, 294, 341, 51760], "temperature": 0.0, "avg_logprob": -0.046999283886830744, "compression_ratio": 1.8093385214007782, "no_speech_prob": 0.00028673477936536074}, {"id": 439, "seek": 259352, "start": 2593.52, "end": 2598.96, "text": " project we're trying to build or define a method for building a visual concept vocabulary for an", "tokens": [50364, 1716, 321, 434, 1382, 281, 1322, 420, 6964, 257, 3170, 337, 2390, 257, 5056, 3410, 19864, 337, 364, 50636], "temperature": 0.0, "avg_logprob": -0.06931493570516398, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00021651023416779935}, {"id": 440, "seek": 259352, "start": 2598.96, "end": 2605.36, "text": " arbitrary GAN latent space so to put it more specifically we want to learn embeddings d maybe", "tokens": [50636, 23211, 460, 1770, 48994, 1901, 370, 281, 829, 309, 544, 4682, 321, 528, 281, 1466, 12240, 29432, 274, 1310, 50956], "temperature": 0.0, "avg_logprob": -0.06931493570516398, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00021651023416779935}, {"id": 441, "seek": 259352, "start": 2605.36, "end": 2611.2, "text": " you've called this w we want to learn some kind of walk in the latent space z if again we'll focus", "tokens": [50956, 291, 600, 1219, 341, 261, 321, 528, 281, 1466, 512, 733, 295, 1792, 294, 264, 48994, 1901, 710, 498, 797, 321, 603, 1879, 51248], "temperature": 0.0, "avg_logprob": -0.06931493570516398, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00021651023416779935}, {"id": 442, "seek": 259352, "start": 2611.2, "end": 2617.84, "text": " on big GAN here of transformations that are salient to us in visual space and we can't define", "tokens": [51248, 322, 955, 460, 1770, 510, 295, 34852, 300, 366, 1845, 1196, 281, 505, 294, 5056, 1901, 293, 321, 393, 380, 6964, 51580], "temperature": 0.0, "avg_logprob": -0.06931493570516398, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00021651023416779935}, {"id": 443, "seek": 261784, "start": 2617.84, "end": 2624.32, "text": " an objective and optimize our d our walk to produce a transform in x in the image because we", "tokens": [50364, 364, 10024, 293, 19719, 527, 274, 527, 1792, 281, 5258, 257, 4088, 294, 2031, 294, 264, 3256, 570, 321, 50688], "temperature": 0.0, "avg_logprob": -0.07326493318053498, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.005218127742409706}, {"id": 444, "seek": 261784, "start": 2624.32, "end": 2628.56, "text": " want to learn the vocabulary concepts rather than pre-commit to them and we would have to pre-commit", "tokens": [50688, 528, 281, 1466, 264, 19864, 10392, 2831, 813, 659, 12, 1112, 3508, 281, 552, 293, 321, 576, 362, 281, 659, 12, 1112, 3508, 50900], "temperature": 0.0, "avg_logprob": -0.07326493318053498, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.005218127742409706}, {"id": 445, "seek": 261784, "start": 2628.56, "end": 2633.36, "text": " to what that objective is right in order to optimize d so we're going to take a different", "tokens": [50900, 281, 437, 300, 10024, 307, 558, 294, 1668, 281, 19719, 274, 370, 321, 434, 516, 281, 747, 257, 819, 51140], "temperature": 0.0, "avg_logprob": -0.07326493318053498, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.005218127742409706}, {"id": 446, "seek": 261784, "start": 2633.36, "end": 2641.28, "text": " approach and instead sample the space of salient or possible transformations for some given point", "tokens": [51140, 3109, 293, 2602, 6889, 264, 1901, 295, 1845, 1196, 420, 1944, 34852, 337, 512, 2212, 935, 51536], "temperature": 0.0, "avg_logprob": -0.07326493318053498, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.005218127742409706}, {"id": 447, "seek": 264128, "start": 2641.28, "end": 2647.6800000000003, "text": " in space for some given z and then use those sample directions as a screen so to speak onto", "tokens": [50364, 294, 1901, 337, 512, 2212, 710, 293, 550, 764, 729, 6889, 11095, 382, 257, 2568, 370, 281, 1710, 3911, 50684], "temperature": 0.0, "avg_logprob": -0.055566334030003224, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.052568867802619934}, {"id": 448, "seek": 264128, "start": 2647.6800000000003, "end": 2652.6400000000003, "text": " which we can project human perceptual judgments so that's a little bit of a gratuitous metaphor but", "tokens": [50684, 597, 321, 393, 1716, 1952, 43276, 901, 40337, 370, 300, 311, 257, 707, 857, 295, 257, 38342, 563, 19157, 457, 50932], "temperature": 0.0, "avg_logprob": -0.055566334030003224, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.052568867802619934}, {"id": 449, "seek": 264128, "start": 2652.6400000000003, "end": 2657.2000000000003, "text": " maybe a useful way of thinking about it and then we'll we'll disentangle the concepts that are", "tokens": [50932, 1310, 257, 4420, 636, 295, 1953, 466, 309, 293, 550, 321, 603, 321, 603, 37313, 7846, 264, 10392, 300, 366, 51160], "temperature": 0.0, "avg_logprob": -0.055566334030003224, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.052568867802619934}, {"id": 450, "seek": 264128, "start": 2657.2000000000003, "end": 2662.96, "text": " projected onto that screen into a vocabulary of open-ended compositional visual concepts", "tokens": [51160, 26231, 3911, 300, 2568, 666, 257, 19864, 295, 1269, 12, 3502, 10199, 2628, 5056, 10392, 51448], "temperature": 0.0, "avg_logprob": -0.055566334030003224, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.052568867802619934}, {"id": 451, "seek": 264128, "start": 2664.5600000000004, "end": 2670.96, "text": " and what we're interested in here is the overlap between what's represented inside a model", "tokens": [51528, 293, 437, 321, 434, 3102, 294, 510, 307, 264, 19959, 1296, 437, 311, 10379, 1854, 257, 2316, 51848], "temperature": 0.0, "avg_logprob": -0.055566334030003224, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.052568867802619934}, {"id": 452, "seek": 267096, "start": 2670.96, "end": 2676.2400000000002, "text": " so some deep features in a model's representation and concepts meaningful to humans in visual", "tokens": [50364, 370, 512, 2452, 4122, 294, 257, 2316, 311, 10290, 293, 10392, 10995, 281, 6255, 294, 5056, 50628], "temperature": 0.0, "avg_logprob": -0.08014998663039434, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.00020978867541998625}, {"id": 453, "seek": 267096, "start": 2676.2400000000002, "end": 2681.28, "text": " seeing understanding we're asking how we might start to define although not completely but", "tokens": [50628, 2577, 3701, 321, 434, 3365, 577, 321, 1062, 722, 281, 6964, 4878, 406, 2584, 457, 50880], "temperature": 0.0, "avg_logprob": -0.08014998663039434, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.00020978867541998625}, {"id": 454, "seek": 267096, "start": 2681.28, "end": 2687.2, "text": " start to define a shared vocabulary between the two or for a given model determine what lies in", "tokens": [50880, 722, 281, 6964, 257, 5507, 19864, 1296, 264, 732, 420, 337, 257, 2212, 2316, 6997, 437, 9134, 294, 51176], "temperature": 0.0, "avg_logprob": -0.08014998663039434, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.00020978867541998625}, {"id": 455, "seek": 267096, "start": 2687.2, "end": 2693.12, "text": " that set overlap and I don't have to dwell too long on a lot of the specifics here it's all", "tokens": [51176, 300, 992, 19959, 293, 286, 500, 380, 362, 281, 24355, 886, 938, 322, 257, 688, 295, 264, 28454, 510, 309, 311, 439, 51472], "temperature": 0.0, "avg_logprob": -0.08014998663039434, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.00020978867541998625}, {"id": 456, "seek": 267096, "start": 2694.16, "end": 2699.12, "text": " online at that URL if you want to read the paper but as I mentioned the first thing we're going to", "tokens": [51524, 2950, 412, 300, 12905, 498, 291, 528, 281, 1401, 264, 3035, 457, 382, 286, 2835, 264, 700, 551, 321, 434, 516, 281, 51772], "temperature": 0.0, "avg_logprob": -0.08014998663039434, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.00020978867541998625}, {"id": 457, "seek": 269912, "start": 2699.12, "end": 2705.3599999999997, "text": " do is generate a set of sample images that produce minimal meaningful transformations in images", "tokens": [50364, 360, 307, 8460, 257, 992, 295, 6889, 5267, 300, 5258, 13206, 10995, 34852, 294, 5267, 50676], "temperature": 0.0, "avg_logprob": -0.06243950649372582, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0008555187378078699}, {"id": 458, "seek": 269912, "start": 2706.0, "end": 2710.0, "text": " and then humans come in the loop again we're going to ask them to label them but here we're", "tokens": [50708, 293, 550, 6255, 808, 294, 264, 6367, 797, 321, 434, 516, 281, 1029, 552, 281, 7645, 552, 457, 510, 321, 434, 50908], "temperature": 0.0, "avg_logprob": -0.06243950649372582, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0008555187378078699}, {"id": 459, "seek": 269912, "start": 2710.0, "end": 2715.2799999999997, "text": " forming the basis for the data set that we'll build our vocabulary off of and we want to keep in", "tokens": [50908, 15745, 264, 5143, 337, 264, 1412, 992, 300, 321, 603, 1322, 527, 19864, 766, 295, 293, 321, 528, 281, 1066, 294, 51172], "temperature": 0.0, "avg_logprob": -0.06243950649372582, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0008555187378078699}, {"id": 460, "seek": 269912, "start": 2715.2799999999997, "end": 2720.72, "text": " mind that we want a vocabulary in the end that is both diverse so corresponding to a lot of", "tokens": [51172, 1575, 300, 321, 528, 257, 19864, 294, 264, 917, 300, 307, 1293, 9521, 370, 11760, 281, 257, 688, 295, 51444], "temperature": 0.0, "avg_logprob": -0.06243950649372582, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0008555187378078699}, {"id": 461, "seek": 269912, "start": 2720.72, "end": 2725.68, "text": " different changes that you can produce in an image and specific where a single transformation", "tokens": [51444, 819, 2962, 300, 291, 393, 5258, 294, 364, 3256, 293, 2685, 689, 257, 2167, 9887, 51692], "temperature": 0.0, "avg_logprob": -0.06243950649372582, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0008555187378078699}, {"id": 462, "seek": 272568, "start": 2725.68, "end": 2732.3199999999997, "text": " corresponds quite reliably to one visual change across viewers so we do that by defining", "tokens": [50364, 23249, 1596, 49927, 281, 472, 5056, 1319, 2108, 8499, 370, 321, 360, 300, 538, 17827, 50696], "temperature": 0.0, "avg_logprob": -0.10558622592204325, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001000200747512281}, {"id": 463, "seek": 272568, "start": 2733.12, "end": 2739.12, "text": " mutually orthogonal what we call layer selective directions and these minimize change in the feature", "tokens": [50736, 39144, 41488, 437, 321, 818, 4583, 33930, 11095, 293, 613, 17522, 1319, 294, 264, 4111, 51036], "temperature": 0.0, "avg_logprob": -0.10558622592204325, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001000200747512281}, {"id": 464, "seek": 272568, "start": 2739.12, "end": 2745.2, "text": " representation at some layer of big care and at some layer we'll call it layer l and this allows", "tokens": [51036, 10290, 412, 512, 4583, 295, 955, 1127, 293, 412, 512, 4583, 321, 603, 818, 309, 4583, 287, 293, 341, 4045, 51340], "temperature": 0.0, "avg_logprob": -0.10558622592204325, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001000200747512281}, {"id": 465, "seek": 272568, "start": 2745.2, "end": 2751.12, "text": " us to capture relatively focused changes because we hold constant how much the representation", "tokens": [51340, 505, 281, 7983, 7226, 5178, 2962, 570, 321, 1797, 5754, 577, 709, 264, 10290, 51636], "temperature": 0.0, "avg_logprob": -0.10558622592204325, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001000200747512281}, {"id": 466, "seek": 275112, "start": 2751.12, "end": 2756.16, "text": " can change at some layer and we do that for different layers to capture changes at different", "tokens": [50364, 393, 1319, 412, 512, 4583, 293, 321, 360, 300, 337, 819, 7914, 281, 7983, 2962, 412, 819, 50616], "temperature": 0.0, "avg_logprob": -0.06271235448009563, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.013626991771161556}, {"id": 467, "seek": 275112, "start": 2756.16, "end": 2761.7599999999998, "text": " levels of abstraction so as you can see layers closer to the image output control or fine grained", "tokens": [50616, 4358, 295, 37765, 370, 382, 291, 393, 536, 7914, 4966, 281, 264, 3256, 5598, 1969, 420, 2489, 1295, 2001, 50896], "temperature": 0.0, "avg_logprob": -0.06271235448009563, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.013626991771161556}, {"id": 468, "seek": 275112, "start": 2761.7599999999998, "end": 2766.48, "text": " aspects of the image like the color of the walls and the bedspread and as we get closer", "tokens": [50896, 7270, 295, 264, 3256, 411, 264, 2017, 295, 264, 7920, 293, 264, 2901, 4952, 2538, 293, 382, 321, 483, 4966, 51132], "temperature": 0.0, "avg_logprob": -0.06271235448009563, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.013626991771161556}, {"id": 469, "seek": 275112, "start": 2766.48, "end": 2771.2799999999997, "text": " back to the latent space we're allowed to make kind of more higher level changes in things like", "tokens": [51132, 646, 281, 264, 48994, 1901, 321, 434, 4350, 281, 652, 733, 295, 544, 2946, 1496, 2962, 294, 721, 411, 51372], "temperature": 0.0, "avg_logprob": -0.06271235448009563, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.013626991771161556}, {"id": 470, "seek": 275112, "start": 2771.2799999999997, "end": 2777.7599999999998, "text": " zoom and perspective of the scene and its composition so what objects are present so here we have a", "tokens": [51372, 8863, 293, 4585, 295, 264, 4145, 293, 1080, 12686, 370, 437, 6565, 366, 1974, 370, 510, 321, 362, 257, 51696], "temperature": 0.0, "avg_logprob": -0.06271235448009563, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.013626991771161556}, {"id": 471, "seek": 277776, "start": 2777.76, "end": 2782.8, "text": " base set of minimal meaningful transformations that capture changes in images at different levels", "tokens": [50364, 3096, 992, 295, 13206, 10995, 34852, 300, 7983, 2962, 294, 5267, 412, 819, 4358, 50616], "temperature": 0.0, "avg_logprob": -0.0720652571091285, "compression_ratio": 1.7617328519855595, "no_speech_prob": 0.0010001796763390303}, {"id": 472, "seek": 277776, "start": 2782.8, "end": 2789.28, "text": " of abstraction we're going to ask people to label them because we don't know what's going on visually", "tokens": [50616, 295, 37765, 321, 434, 516, 281, 1029, 561, 281, 7645, 552, 570, 321, 500, 380, 458, 437, 311, 516, 322, 19622, 50940], "temperature": 0.0, "avg_logprob": -0.0720652571091285, "compression_ratio": 1.7617328519855595, "no_speech_prob": 0.0010001796763390303}, {"id": 473, "seek": 277776, "start": 2789.28, "end": 2794.5600000000004, "text": " in these scenes right so we started at a pretty small scale with just four categories in the", "tokens": [50940, 294, 613, 8026, 558, 370, 321, 1409, 412, 257, 1238, 1359, 4373, 365, 445, 1451, 10479, 294, 264, 51204], "temperature": 0.0, "avg_logprob": -0.0720652571091285, "compression_ratio": 1.7617328519855595, "no_speech_prob": 0.0010001796763390303}, {"id": 474, "seek": 277776, "start": 2794.5600000000004, "end": 2800.5600000000004, "text": " places data set and looked at big and trained on image net and places we'll just talk about places", "tokens": [51204, 3190, 1412, 992, 293, 2956, 412, 955, 293, 8895, 322, 3256, 2533, 293, 3190, 321, 603, 445, 751, 466, 3190, 51504], "temperature": 0.0, "avg_logprob": -0.0720652571091285, "compression_ratio": 1.7617328519855595, "no_speech_prob": 0.0010001796763390303}, {"id": 475, "seek": 277776, "start": 2800.5600000000004, "end": 2806.88, "text": " here and visualized a handful a few thousand of these directions per category so in each of four", "tokens": [51504, 510, 293, 5056, 1602, 257, 16458, 257, 1326, 4714, 295, 613, 11095, 680, 7719, 370, 294, 1184, 295, 1451, 51820], "temperature": 0.0, "avg_logprob": -0.0720652571091285, "compression_ratio": 1.7617328519855595, "no_speech_prob": 0.0010001796763390303}, {"id": 476, "seek": 280688, "start": 2806.88, "end": 2812.7200000000003, "text": " categories looked at cottages medinas so uh street marketplaces kitchens and lakes a mix of", "tokens": [50364, 10479, 2956, 412, 11550, 1660, 1205, 17094, 370, 2232, 4838, 2142, 34840, 350, 47314, 293, 25595, 257, 2890, 295, 50656], "temperature": 0.0, "avg_logprob": -0.05944445698531633, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.00045818588114343584}, {"id": 477, "seek": 280688, "start": 2812.7200000000003, "end": 2818.56, "text": " indoor and outdoor scenes and then asked people to just simply describe the overall transition", "tokens": [50656, 24029, 293, 15942, 8026, 293, 550, 2351, 561, 281, 445, 2935, 6786, 264, 4787, 6034, 50948], "temperature": 0.0, "avg_logprob": -0.05944445698531633, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.00045818588114343584}, {"id": 478, "seek": 280688, "start": 2818.56, "end": 2823.92, "text": " that they saw when these directions were applied to different randomly sampled starting points in", "tokens": [50948, 300, 436, 1866, 562, 613, 11095, 645, 6456, 281, 819, 16979, 3247, 15551, 2891, 2793, 294, 51216], "temperature": 0.0, "avg_logprob": -0.05944445698531633, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.00045818588114343584}, {"id": 479, "seek": 280688, "start": 2823.92, "end": 2829.76, "text": " the latent space right so one direction might take this cottage to this snowy cottage and change", "tokens": [51216, 264, 48994, 1901, 558, 370, 472, 3513, 1062, 747, 341, 37209, 281, 341, 5756, 88, 37209, 293, 1319, 51508], "temperature": 0.0, "avg_logprob": -0.05944445698531633, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.00045818588114343584}, {"id": 480, "seek": 280688, "start": 2829.76, "end": 2835.28, "text": " something about the sky and change the snow so these these changes are still complex we can", "tokens": [51508, 746, 466, 264, 5443, 293, 1319, 264, 5756, 370, 613, 613, 2962, 366, 920, 3997, 321, 393, 51784], "temperature": 0.0, "avg_logprob": -0.05944445698531633, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.00045818588114343584}, {"id": 481, "seek": 283528, "start": 2835.28, "end": 2840.5600000000004, "text": " recognize that it's the same scene and we can describe in simple language what's going on", "tokens": [50364, 5521, 300, 309, 311, 264, 912, 4145, 293, 321, 393, 6786, 294, 2199, 2856, 437, 311, 516, 322, 50628], "temperature": 0.0, "avg_logprob": -0.05431509017944336, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.0003459049912635237}, {"id": 482, "seek": 283528, "start": 2840.5600000000004, "end": 2844.7200000000003, "text": " but they're they're not disentangled yet right one direction might correspond to a number of", "tokens": [50628, 457, 436, 434, 436, 434, 406, 37313, 39101, 1939, 558, 472, 3513, 1062, 6805, 281, 257, 1230, 295, 50836], "temperature": 0.0, "avg_logprob": -0.05431509017944336, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.0003459049912635237}, {"id": 483, "seek": 283528, "start": 2844.7200000000003, "end": 2850.2400000000002, "text": " different visual changes so we did a little preprocessing to capture you know what kinds of", "tokens": [50836, 819, 5056, 2962, 370, 321, 630, 257, 707, 2666, 340, 780, 278, 281, 7983, 291, 458, 437, 3685, 295, 51112], "temperature": 0.0, "avg_logprob": -0.05431509017944336, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.0003459049912635237}, {"id": 484, "seek": 283528, "start": 2850.2400000000002, "end": 2857.28, "text": " concepts are associated with each transformation and then we decomposed those annotated directions", "tokens": [51112, 10392, 366, 6615, 365, 1184, 9887, 293, 550, 321, 22867, 1744, 729, 25339, 770, 11095, 51464], "temperature": 0.0, "avg_logprob": -0.05431509017944336, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.0003459049912635237}, {"id": 485, "seek": 283528, "start": 2857.28, "end": 2862.48, "text": " into a visual concept vocabulary consisting of single directions labeled with single words", "tokens": [51464, 666, 257, 5056, 3410, 19864, 33921, 295, 2167, 11095, 21335, 365, 2167, 2283, 51724], "temperature": 0.0, "avg_logprob": -0.05431509017944336, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.0003459049912635237}, {"id": 486, "seek": 286248, "start": 2862.56, "end": 2868.72, "text": " we formulated that as a linear regression and then solved for the embeddings of individual", "tokens": [50368, 321, 48936, 300, 382, 257, 8213, 24590, 293, 550, 13041, 337, 264, 12240, 29432, 295, 2609, 50676], "temperature": 0.0, "avg_logprob": -0.0862291653951009, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.001597374677658081}, {"id": 487, "seek": 286248, "start": 2868.72, "end": 2873.2, "text": " concepts in the latent space of our begin and then we can basically read those off", "tokens": [50676, 10392, 294, 264, 48994, 1901, 295, 527, 1841, 293, 550, 321, 393, 1936, 1401, 729, 766, 50900], "temperature": 0.0, "avg_logprob": -0.0862291653951009, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.001597374677658081}, {"id": 488, "seek": 286248, "start": 2874.56, "end": 2879.84, "text": " of our matrix e and then transform the images by manipulating them some amount along those", "tokens": [50968, 295, 527, 8141, 308, 293, 550, 4088, 264, 5267, 538, 40805, 552, 512, 2372, 2051, 729, 51232], "temperature": 0.0, "avg_logprob": -0.0862291653951009, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.001597374677658081}, {"id": 489, "seek": 286248, "start": 2879.84, "end": 2884.56, "text": " visual concept directions happy to talk more details about that if anybody's specifically", "tokens": [51232, 5056, 3410, 11095, 2055, 281, 751, 544, 4365, 466, 300, 498, 4472, 311, 4682, 51468], "temperature": 0.0, "avg_logprob": -0.0862291653951009, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.001597374677658081}, {"id": 490, "seek": 286248, "start": 2884.56, "end": 2891.12, "text": " interested or you can check out the paper itself we found over 2000 concepts this way", "tokens": [51468, 3102, 420, 291, 393, 1520, 484, 264, 3035, 2564, 321, 1352, 670, 8132, 10392, 341, 636, 51796], "temperature": 0.0, "avg_logprob": -0.0862291653951009, "compression_ratio": 1.7254901960784315, "no_speech_prob": 0.001597374677658081}, {"id": 491, "seek": 289112, "start": 2891.12, "end": 2894.96, "text": " corresponding to lots of different types of visual changes so we can reproduce", "tokens": [50364, 11760, 281, 3195, 295, 819, 3467, 295, 5056, 2962, 370, 321, 393, 29501, 50556], "temperature": 0.0, "avg_logprob": -0.0612807059555911, "compression_ratio": 1.7651821862348178, "no_speech_prob": 0.0007791122188791633}, {"id": 492, "seek": 289112, "start": 2894.96, "end": 2901.7599999999998, "text": " transformations like zoom and rotation things like color but we also get kind of a unique", "tokens": [50556, 34852, 411, 8863, 293, 12447, 721, 411, 2017, 457, 321, 611, 483, 733, 295, 257, 3845, 50896], "temperature": 0.0, "avg_logprob": -0.0612807059555911, "compression_ratio": 1.7651821862348178, "no_speech_prob": 0.0007791122188791633}, {"id": 493, "seek": 289112, "start": 2901.7599999999998, "end": 2907.52, "text": " set of concepts corresponding to aspects of scenes like their mood for instance there's a", "tokens": [50896, 992, 295, 10392, 11760, 281, 7270, 295, 8026, 411, 641, 9268, 337, 5197, 456, 311, 257, 51184], "temperature": 0.0, "avg_logprob": -0.0612807059555911, "compression_ratio": 1.7651821862348178, "no_speech_prob": 0.0007791122188791633}, {"id": 494, "seek": 289112, "start": 2907.52, "end": 2914.08, "text": " direction in latent space of big and that makes outdoor marketplaces more festive and here we", "tokens": [51184, 3513, 294, 48994, 1901, 295, 955, 293, 300, 1669, 15942, 2142, 34840, 544, 42729, 293, 510, 321, 51512], "temperature": 0.0, "avg_logprob": -0.0612807059555911, "compression_ratio": 1.7651821862348178, "no_speech_prob": 0.0007791122188791633}, {"id": 495, "seek": 289112, "start": 2914.08, "end": 2918.16, "text": " see applying that direction to an example marketplace and it rolls out a red carpet", "tokens": [51512, 536, 9275, 300, 3513, 281, 364, 1365, 19455, 293, 309, 15767, 484, 257, 2182, 18119, 51716], "temperature": 0.0, "avg_logprob": -0.0612807059555911, "compression_ratio": 1.7651821862348178, "no_speech_prob": 0.0007791122188791633}, {"id": 496, "seek": 291816, "start": 2918.16, "end": 2925.6, "text": " hangs some flags and brings a lot of people into that market we can visualize kind of a", "tokens": [50364, 35947, 512, 23265, 293, 5607, 257, 688, 295, 561, 666, 300, 2142, 321, 393, 23273, 733, 295, 257, 50736], "temperature": 0.0, "avg_logprob": -0.08320032192181938, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0011692066909745336}, {"id": 497, "seek": 291816, "start": 2925.6, "end": 2931.44, "text": " a sampling of these directions each applied to two different images in different categories", "tokens": [50736, 257, 21179, 295, 613, 11095, 1184, 6456, 281, 732, 819, 5267, 294, 819, 10479, 51028], "temperature": 0.0, "avg_logprob": -0.08320032192181938, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0011692066909745336}, {"id": 498, "seek": 291816, "start": 2931.44, "end": 2937.92, "text": " so some directions make cottages more manicured add arches to marketplaces add shadows or make the", "tokens": [51028, 370, 512, 11095, 652, 11550, 1660, 544, 48139, 3831, 909, 594, 3781, 281, 2142, 34840, 909, 14740, 420, 652, 264, 51352], "temperature": 0.0, "avg_logprob": -0.08320032192181938, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0011692066909745336}, {"id": 499, "seek": 291816, "start": 2937.92, "end": 2943.2, "text": " whole scene blue and we see directions like this that generalize across all of the categories", "tokens": [51352, 1379, 4145, 3344, 293, 321, 536, 11095, 411, 341, 300, 2674, 1125, 2108, 439, 295, 264, 10479, 51616], "temperature": 0.0, "avg_logprob": -0.08320032192181938, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.0011692066909745336}, {"id": 500, "seek": 294320, "start": 2943.2, "end": 2949.6, "text": " it began that we looked at you can check out the lake category we can add sunsets but also do", "tokens": [50364, 309, 4283, 300, 321, 2956, 412, 291, 393, 1520, 484, 264, 11001, 7719, 321, 393, 909, 3295, 82, 1385, 457, 611, 360, 50684], "temperature": 0.0, "avg_logprob": -0.07081848250495063, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.005383435171097517}, {"id": 501, "seek": 294320, "start": 2949.6, "end": 2955.4399999999996, "text": " kind of scene specific things like add reflections to water or make a lake scene foggier make a", "tokens": [50684, 733, 295, 4145, 2685, 721, 411, 909, 30679, 281, 1281, 420, 652, 257, 11001, 4145, 13648, 70, 811, 652, 257, 50976], "temperature": 0.0, "avg_logprob": -0.07081848250495063, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.005383435171097517}, {"id": 502, "seek": 294320, "start": 2955.4399999999996, "end": 2961.8399999999997, "text": " kitchen more inviting or more modern and again we didn't have to pre-specify what exactly modernity", "tokens": [50976, 6525, 544, 18202, 420, 544, 4363, 293, 797, 321, 994, 380, 362, 281, 659, 12, 7053, 66, 2505, 437, 2293, 4363, 507, 51296], "temperature": 0.0, "avg_logprob": -0.07081848250495063, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.005383435171097517}, {"id": 503, "seek": 294320, "start": 2961.8399999999997, "end": 2968.0, "text": " would entail when applied to a kitchen we learned that through sampling what humans associate", "tokens": [51296, 576, 948, 864, 562, 6456, 281, 257, 6525, 321, 3264, 300, 807, 21179, 437, 6255, 14644, 51604], "temperature": 0.0, "avg_logprob": -0.07081848250495063, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.005383435171097517}, {"id": 504, "seek": 296800, "start": 2968.0, "end": 2973.84, "text": " with a transformation that was sampled randomly right uh the humans labeled that as modern and", "tokens": [50364, 365, 257, 9887, 300, 390, 3247, 15551, 16979, 558, 2232, 264, 6255, 21335, 300, 382, 4363, 293, 50656], "temperature": 0.0, "avg_logprob": -0.09236231333092798, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.0030744958203285933}, {"id": 505, "seek": 296800, "start": 2973.84, "end": 2979.52, "text": " then we disentangled the specific direction in latent space corresponding to that single concept", "tokens": [50656, 550, 321, 37313, 39101, 264, 2685, 3513, 294, 48994, 1901, 11760, 281, 300, 2167, 3410, 50940], "temperature": 0.0, "avg_logprob": -0.09236231333092798, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.0030744958203285933}, {"id": 506, "seek": 296800, "start": 2979.52, "end": 2986.24, "text": " word and once it's isolated we can apply a modern transformation and know that it corresponds to", "tokens": [50940, 1349, 293, 1564, 309, 311, 14621, 321, 393, 3079, 257, 4363, 9887, 293, 458, 300, 309, 23249, 281, 51276], "temperature": 0.0, "avg_logprob": -0.09236231333092798, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.0030744958203285933}, {"id": 507, "seek": 296800, "start": 2986.24, "end": 2993.36, "text": " what viewers found to represent modernity in a kitchen well I said we know that it corresponds to", "tokens": [51276, 437, 8499, 1352, 281, 2906, 4363, 507, 294, 257, 6525, 731, 286, 848, 321, 458, 300, 309, 23249, 281, 51632], "temperature": 0.0, "avg_logprob": -0.09236231333092798, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.0030744958203285933}, {"id": 508, "seek": 299336, "start": 2993.92, "end": 2998.8, "text": " what viewers see as more modern but we don't know that for sure right we still need to ask", "tokens": [50392, 437, 8499, 536, 382, 544, 4363, 457, 321, 500, 380, 458, 300, 337, 988, 558, 321, 920, 643, 281, 1029, 50636], "temperature": 0.0, "avg_logprob": -0.07212157476515997, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.009404812939465046}, {"id": 509, "seek": 299336, "start": 2998.8, "end": 3004.8, "text": " questions like how generalizable are these directions do they compose right can we add", "tokens": [50636, 1651, 411, 577, 2674, 22395, 366, 613, 11095, 360, 436, 35925, 558, 393, 321, 909, 50936], "temperature": 0.0, "avg_logprob": -0.07212157476515997, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.009404812939465046}, {"id": 510, "seek": 299336, "start": 3004.8, "end": 3010.4, "text": " a festive direction to eerie and get something that's both scary and festive right or could we make", "tokens": [50936, 257, 42729, 3513, 281, 308, 17487, 293, 483, 746, 300, 311, 1293, 6958, 293, 42729, 558, 420, 727, 321, 652, 51216], "temperature": 0.0, "avg_logprob": -0.07212157476515997, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.009404812939465046}, {"id": 511, "seek": 299336, "start": 3011.04, "end": 3016.0, "text": " a kitchen both more modern and inviting so we asked those questions in a series of behavioral", "tokens": [51248, 257, 6525, 1293, 544, 4363, 293, 18202, 370, 321, 2351, 729, 1651, 294, 257, 2638, 295, 19124, 51496], "temperature": 0.0, "avg_logprob": -0.07212157476515997, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.009404812939465046}, {"id": 512, "seek": 299336, "start": 3016.0, "end": 3021.6800000000003, "text": " experiments that I left for you to check out in the paper itself so we won't in the interest of", "tokens": [51496, 12050, 300, 286, 1411, 337, 291, 281, 1520, 484, 294, 264, 3035, 2564, 370, 321, 1582, 380, 294, 264, 1179, 295, 51780], "temperature": 0.0, "avg_logprob": -0.07212157476515997, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.009404812939465046}, {"id": 513, "seek": 302168, "start": 3021.68, "end": 3026.96, "text": " time go through those here but we do find that these directions are composable and they're generalizable", "tokens": [50364, 565, 352, 807, 729, 510, 457, 321, 360, 915, 300, 613, 11095, 366, 10199, 712, 293, 436, 434, 2674, 22395, 50628], "temperature": 0.0, "avg_logprob": -0.04395197362315898, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.0005191676900722086}, {"id": 514, "seek": 302168, "start": 3026.96, "end": 3032.7999999999997, "text": " across categories so there are some cases where we can even add a concept that was learned in a", "tokens": [50628, 2108, 10479, 370, 456, 366, 512, 3331, 689, 321, 393, 754, 909, 257, 3410, 300, 390, 3264, 294, 257, 50920], "temperature": 0.0, "avg_logprob": -0.04395197362315898, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.0005191676900722086}, {"id": 515, "seek": 302168, "start": 3032.7999999999997, "end": 3038.64, "text": " single category to a different category for instance making a cottage more festive right or", "tokens": [50920, 2167, 7719, 281, 257, 819, 7719, 337, 5197, 1455, 257, 37209, 544, 42729, 558, 420, 51212], "temperature": 0.0, "avg_logprob": -0.04395197362315898, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.0005191676900722086}, {"id": 516, "seek": 302168, "start": 3038.64, "end": 3044.0, "text": " adding snow to a marketplace even though that's not traditionally seen there we ran a set of", "tokens": [51212, 5127, 5756, 281, 257, 19455, 754, 1673, 300, 311, 406, 19067, 1612, 456, 321, 5872, 257, 992, 295, 51480], "temperature": 0.0, "avg_logprob": -0.04395197362315898, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.0005191676900722086}, {"id": 517, "seek": 302168, "start": 3044.0, "end": 3049.12, "text": " behavioral experiments evaluating the extent to which this is successful and isolating a couple", "tokens": [51480, 19124, 12050, 27479, 264, 8396, 281, 597, 341, 307, 4406, 293, 48912, 257, 1916, 51736], "temperature": 0.0, "avg_logprob": -0.04395197362315898, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.0005191676900722086}, {"id": 518, "seek": 304912, "start": 3049.2, "end": 3056.56, "text": " of few specific cases where it fails okay so this this wraps up this method it's at a point", "tokens": [50368, 295, 1326, 2685, 3331, 689, 309, 18199, 1392, 370, 341, 341, 25831, 493, 341, 3170, 309, 311, 412, 257, 935, 50736], "temperature": 0.0, "avg_logprob": -0.112854284398696, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.001866250648163259}, {"id": 519, "seek": 304912, "start": 3056.56, "end": 3062.16, "text": " now where we're trying this with some of the the art models that I discussed previously right so", "tokens": [50736, 586, 689, 321, 434, 1382, 341, 365, 512, 295, 264, 264, 1523, 5245, 300, 286, 7152, 8046, 558, 370, 51016], "temperature": 0.0, "avg_logprob": -0.112854284398696, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.001866250648163259}, {"id": 520, "seek": 304912, "start": 3062.16, "end": 3068.48, "text": " this was still just applied to big and trained on real-world images trained on ImageNet but you", "tokens": [51016, 341, 390, 920, 445, 6456, 281, 955, 293, 8895, 322, 957, 12, 13217, 5267, 8895, 322, 29903, 31890, 457, 291, 51332], "temperature": 0.0, "avg_logprob": -0.112854284398696, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.001866250648163259}, {"id": 521, "seek": 304912, "start": 3068.48, "end": 3074.24, "text": " can imagine using a similar similar method to find dimensions of visual interest that are also", "tokens": [51332, 393, 3811, 1228, 257, 2531, 2531, 3170, 281, 915, 12819, 295, 5056, 1179, 300, 366, 611, 51620], "temperature": 0.0, "avg_logprob": -0.112854284398696, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.001866250648163259}, {"id": 522, "seek": 307424, "start": 3074.3199999999997, "end": 3081.2799999999997, "text": " meaningful to humans in the latent space of a model trained on art images and so decompose", "tokens": [50368, 10995, 281, 6255, 294, 264, 48994, 1901, 295, 257, 2316, 8895, 322, 1523, 5267, 293, 370, 22867, 541, 50716], "temperature": 0.0, "avg_logprob": -0.06029088775832932, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.004197327420115471}, {"id": 523, "seek": 307424, "start": 3081.2799999999997, "end": 3086.72, "text": " future languages underlying different genres of art into something describable so that we", "tokens": [50716, 2027, 8650, 14217, 819, 30057, 295, 1523, 666, 746, 2189, 65, 712, 370, 300, 321, 50988], "temperature": 0.0, "avg_logprob": -0.06029088775832932, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.004197327420115471}, {"id": 524, "seek": 307424, "start": 3086.72, "end": 3092.16, "text": " can make concerted manipulations to images sampled either from foundation models that", "tokens": [50988, 393, 652, 8543, 292, 9258, 4136, 281, 5267, 3247, 15551, 2139, 490, 7030, 5245, 300, 51260], "temperature": 0.0, "avg_logprob": -0.06029088775832932, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.004197327420115471}, {"id": 525, "seek": 307424, "start": 3092.16, "end": 3098.08, "text": " correspond to approximations of real-world images or to models trained on on archives of art images", "tokens": [51260, 6805, 281, 8542, 763, 295, 957, 12, 13217, 5267, 420, 281, 5245, 8895, 322, 322, 25607, 295, 1523, 5267, 51556], "temperature": 0.0, "avg_logprob": -0.06029088775832932, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.004197327420115471}, {"id": 526, "seek": 309808, "start": 3098.08, "end": 3105.2, "text": " themselves I'll pause here for any questions about this there's associated code also available", "tokens": [50364, 2969, 286, 603, 10465, 510, 337, 604, 1651, 466, 341, 456, 311, 6615, 3089, 611, 2435, 50720], "temperature": 0.0, "avg_logprob": -0.15635226146284356, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.04021182283759117}, {"id": 527, "seek": 309808, "start": 3105.2, "end": 3113.12, "text": " on that project page I linked have a quick question please um was there a reason you chose", "tokens": [50720, 322, 300, 1716, 3028, 286, 9408, 362, 257, 1702, 1168, 1767, 1105, 390, 456, 257, 1778, 291, 5111, 51116], "temperature": 0.0, "avg_logprob": -0.15635226146284356, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.04021182283759117}, {"id": 528, "seek": 309808, "start": 3114.24, "end": 3121.44, "text": " big GAN over starting with like style GAN for this type of work uh no um just a couple of", "tokens": [51172, 955, 460, 1770, 670, 2891, 365, 411, 3758, 460, 1770, 337, 341, 2010, 295, 589, 2232, 572, 1105, 445, 257, 1916, 295, 51532], "temperature": 0.0, "avg_logprob": -0.15635226146284356, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.04021182283759117}, {"id": 529, "seek": 309808, "start": 3121.44, "end": 3126.08, "text": " different code bases that already existed um and people that had worked on big GAN for", "tokens": [51532, 819, 3089, 17949, 300, 1217, 13135, 1105, 293, 561, 300, 632, 2732, 322, 955, 460, 1770, 337, 51764], "temperature": 0.0, "avg_logprob": -0.15635226146284356, "compression_ratio": 1.587719298245614, "no_speech_prob": 0.04021182283759117}, {"id": 530, "seek": 312608, "start": 3126.08, "end": 3130.16, "text": " like GAN dissection we had an easy way to dissect big GAN and hypothesize what kind of", "tokens": [50364, 411, 460, 1770, 717, 11963, 321, 632, 364, 1858, 636, 281, 48332, 955, 460, 1770, 293, 14276, 1125, 437, 733, 295, 50568], "temperature": 0.0, "avg_logprob": -0.0622877019696531, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.002181932795792818}, {"id": 531, "seek": 312608, "start": 3130.16, "end": 3134.56, "text": " things might be there uh so a lot of the GAN dissection work started with big GAN and so I", "tokens": [50568, 721, 1062, 312, 456, 2232, 370, 257, 688, 295, 264, 460, 1770, 717, 11963, 589, 1409, 365, 955, 460, 1770, 293, 370, 286, 50788], "temperature": 0.0, "avg_logprob": -0.0622877019696531, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.002181932795792818}, {"id": 532, "seek": 312608, "start": 3134.56, "end": 3139.36, "text": " was picking up where that left off and asking if we could find like style vectors that corresponded", "tokens": [50788, 390, 8867, 493, 689, 300, 1411, 766, 293, 3365, 498, 321, 727, 915, 411, 3758, 18875, 300, 6805, 292, 51028], "temperature": 0.0, "avg_logprob": -0.0622877019696531, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.002181932795792818}, {"id": 533, "seek": 312608, "start": 3139.36, "end": 3145.36, "text": " to scene level transformations instead of individual neurons um but I have extended this", "tokens": [51028, 281, 4145, 1496, 34852, 2602, 295, 2609, 22027, 1105, 457, 286, 362, 10913, 341, 51328], "temperature": 0.0, "avg_logprob": -0.0622877019696531, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.002181932795792818}, {"id": 534, "seek": 312608, "start": 3145.36, "end": 3151.44, "text": " to style GAN outside of the paper it's just not got it it's here yeah the method's pretty I mean", "tokens": [51328, 281, 3758, 460, 1770, 2380, 295, 264, 3035, 309, 311, 445, 406, 658, 309, 309, 311, 510, 1338, 264, 3170, 311, 1238, 286, 914, 51632], "temperature": 0.0, "avg_logprob": -0.0622877019696531, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.002181932795792818}, {"id": 535, "seek": 315144, "start": 3151.44, "end": 3156.32, "text": " there are a couple of different small changes you have to make um but the method is pretty model", "tokens": [50364, 456, 366, 257, 1916, 295, 819, 1359, 2962, 291, 362, 281, 652, 1105, 457, 264, 3170, 307, 1238, 2316, 50608], "temperature": 0.0, "avg_logprob": -0.06971954345703125, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.00041726333438418806}, {"id": 536, "seek": 315144, "start": 3156.32, "end": 3162.0, "text": " agnostic just like defining a set of certain directions that samples the latent space in", "tokens": [50608, 623, 77, 19634, 445, 411, 17827, 257, 992, 295, 1629, 11095, 300, 10938, 264, 48994, 1901, 294, 50892], "temperature": 0.0, "avg_logprob": -0.06971954345703125, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.00041726333438418806}, {"id": 537, "seek": 315144, "start": 3162.0, "end": 3167.36, "text": " kind of minimal ways and the the method I described here is definitely not the only one you could use", "tokens": [50892, 733, 295, 13206, 2098, 293, 264, 264, 3170, 286, 7619, 510, 307, 2138, 406, 264, 787, 472, 291, 727, 764, 51160], "temperature": 0.0, "avg_logprob": -0.06971954345703125, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.00041726333438418806}, {"id": 538, "seek": 315144, "start": 3167.36, "end": 3171.44, "text": " for that right um you could sample them by just finding the principal components of the feature", "tokens": [51160, 337, 300, 558, 1105, 291, 727, 6889, 552, 538, 445, 5006, 264, 9716, 6677, 295, 264, 4111, 51364], "temperature": 0.0, "avg_logprob": -0.06971954345703125, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.00041726333438418806}, {"id": 539, "seek": 315144, "start": 3171.44, "end": 3175.44, "text": " space or you could sample them randomly right you could just find two points in latent space", "tokens": [51364, 1901, 420, 291, 727, 6889, 552, 16979, 558, 291, 727, 445, 915, 732, 2793, 294, 48994, 1901, 51564], "temperature": 0.0, "avg_logprob": -0.06971954345703125, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.00041726333438418806}, {"id": 540, "seek": 315144, "start": 3175.44, "end": 3180.32, "text": " interpolate and then get people to label what's going on there um we tried a lot of these different", "tokens": [51564, 44902, 473, 293, 550, 483, 561, 281, 7645, 437, 311, 516, 322, 456, 1105, 321, 3031, 257, 688, 295, 613, 819, 51808], "temperature": 0.0, "avg_logprob": -0.06971954345703125, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.00041726333438418806}, {"id": 541, "seek": 318032, "start": 3180.32, "end": 3186.0, "text": " methods uh and found that if you if you make random interpolations between two randomly", "tokens": [50364, 7150, 2232, 293, 1352, 300, 498, 291, 498, 291, 652, 4974, 44902, 763, 1296, 732, 16979, 50648], "temperature": 0.0, "avg_logprob": -0.0734232791419168, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.00035692634992301464}, {"id": 542, "seek": 318032, "start": 3186.0, "end": 3190.96, "text": " sampled points then there's just so much going on in the scene that there's not a lot of inter", "tokens": [50648, 3247, 15551, 2793, 550, 456, 311, 445, 370, 709, 516, 322, 294, 264, 4145, 300, 456, 311, 406, 257, 688, 295, 728, 50896], "temperature": 0.0, "avg_logprob": -0.0734232791419168, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.00035692634992301464}, {"id": 543, "seek": 318032, "start": 3190.96, "end": 3195.44, "text": " observer agreement in how people annotate what they see there's just too much going on so we", "tokens": [50896, 27878, 8106, 294, 577, 561, 25339, 473, 437, 436, 536, 456, 311, 445, 886, 709, 516, 322, 370, 321, 51120], "temperature": 0.0, "avg_logprob": -0.0734232791419168, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.00035692634992301464}, {"id": 544, "seek": 318032, "start": 3195.44, "end": 3200.8, "text": " need to isolate specific changes that's why we developed that layer selective method for isolating", "tokens": [51120, 643, 281, 25660, 2685, 2962, 300, 311, 983, 321, 4743, 300, 4583, 33930, 3170, 337, 48912, 51388], "temperature": 0.0, "avg_logprob": -0.0734232791419168, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.00035692634992301464}, {"id": 545, "seek": 318032, "start": 3201.52, "end": 3206.88, "text": " minimal changes um but what if we used kind of the the principal component method right or used", "tokens": [51424, 13206, 2962, 1105, 457, 437, 498, 321, 1143, 733, 295, 264, 264, 9716, 6542, 3170, 558, 420, 1143, 51692], "temperature": 0.0, "avg_logprob": -0.0734232791419168, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.00035692634992301464}, {"id": 546, "seek": 320688, "start": 3206.88, "end": 3212.4, "text": " something like GAN space um there we found that the principal components of the model's feature", "tokens": [50364, 746, 411, 460, 1770, 1901, 1105, 456, 321, 1352, 300, 264, 9716, 6677, 295, 264, 2316, 311, 4111, 50640], "temperature": 0.0, "avg_logprob": -0.06059115523592048, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00027799600502476096}, {"id": 547, "seek": 320688, "start": 3212.4, "end": 3217.52, "text": " space aren't necessarily the most interesting to humans so we might get a ton of different types of", "tokens": [50640, 1901, 3212, 380, 4725, 264, 881, 1880, 281, 6255, 370, 321, 1062, 483, 257, 2952, 295, 819, 3467, 295, 50896], "temperature": 0.0, "avg_logprob": -0.06059115523592048, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00027799600502476096}, {"id": 548, "seek": 320688, "start": 3217.52, "end": 3222.32, "text": " rotating the scene but not a lot of different changes of mood or changes in color up there in", "tokens": [50896, 19627, 264, 4145, 457, 406, 257, 688, 295, 819, 2962, 295, 9268, 420, 2962, 294, 2017, 493, 456, 294, 51136], "temperature": 0.0, "avg_logprob": -0.06059115523592048, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00027799600502476096}, {"id": 549, "seek": 320688, "start": 3222.32, "end": 3227.6, "text": " high-ranked principal components um so that's where that method came from but it's agnostic to the", "tokens": [51136, 1090, 12, 20479, 292, 9716, 6677, 1105, 370, 300, 311, 689, 300, 3170, 1361, 490, 457, 309, 311, 623, 77, 19634, 281, 264, 51400], "temperature": 0.0, "avg_logprob": -0.06059115523592048, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00027799600502476096}, {"id": 550, "seek": 320688, "start": 3227.6, "end": 3233.92, "text": " set of directions and pretty model agnostic uh the annotation is another place where humans intervene", "tokens": [51400, 992, 295, 11095, 293, 1238, 2316, 623, 77, 19634, 2232, 264, 48654, 307, 1071, 1081, 689, 6255, 30407, 51716], "temperature": 0.0, "avg_logprob": -0.06059115523592048, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00027799600502476096}, {"id": 551, "seek": 323392, "start": 3233.92, "end": 3241.12, "text": " here to to tie in that last question um but you can imagine trading a captioner on a label", "tokens": [50364, 510, 281, 281, 7582, 294, 300, 1036, 1168, 1105, 457, 291, 393, 3811, 9529, 257, 31974, 260, 322, 257, 7645, 50724], "temperature": 0.0, "avg_logprob": -0.08642724295642888, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.0027143454644829035}, {"id": 552, "seek": 323392, "start": 3241.12, "end": 3245.2000000000003, "text": " data set like this right a little larger than the one we collected so we're thinking about doing", "tokens": [50724, 1412, 992, 411, 341, 558, 257, 707, 4833, 813, 264, 472, 321, 11087, 370, 321, 434, 1953, 466, 884, 50928], "temperature": 0.0, "avg_logprob": -0.08642724295642888, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.0027143454644829035}, {"id": 553, "seek": 323392, "start": 3245.2000000000003, "end": 3250.7200000000003, "text": " something like that uh but preserving the human annotation does allow annotation you know in the", "tokens": [50928, 746, 411, 300, 2232, 457, 33173, 264, 1952, 48654, 775, 2089, 48654, 291, 458, 294, 264, 51204], "temperature": 0.0, "avg_logprob": -0.08642724295642888, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.0027143454644829035}, {"id": 554, "seek": 323392, "start": 3250.7200000000003, "end": 3257.28, "text": " art context by experts as I mentioned so you might want to be able to do this at scale for a brand", "tokens": [51204, 1523, 4319, 538, 8572, 382, 286, 2835, 370, 291, 1062, 528, 281, 312, 1075, 281, 360, 341, 412, 4373, 337, 257, 3360, 51532], "temperature": 0.0, "avg_logprob": -0.08642724295642888, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.0027143454644829035}, {"id": 555, "seek": 323392, "start": 3257.28, "end": 3263.04, "text": " new model and just have automatic annotations you use something like clip right for the kinds of", "tokens": [51532, 777, 2316, 293, 445, 362, 12509, 25339, 763, 291, 764, 746, 411, 7353, 558, 337, 264, 3685, 295, 51820], "temperature": 0.0, "avg_logprob": -0.08642724295642888, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.0027143454644829035}, {"id": 556, "seek": 326304, "start": 3263.04, "end": 3268.64, "text": " transformations you would see inside but preserve the opportunity for experts to to annotate kind", "tokens": [50364, 34852, 291, 576, 536, 1854, 457, 15665, 264, 2650, 337, 8572, 281, 281, 25339, 473, 733, 50644], "temperature": 0.0, "avg_logprob": -0.1431674644595287, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0001851717970566824}, {"id": 557, "seek": 326304, "start": 3268.64, "end": 3276.72, "text": " of specialized smaller trained models and there are results too from big ganttring on a couple of", "tokens": [50644, 295, 19813, 4356, 8895, 5245, 293, 456, 366, 3542, 886, 490, 955, 290, 394, 83, 2937, 322, 257, 1916, 295, 51048], "temperature": 0.0, "avg_logprob": -0.1431674644595287, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0001851717970566824}, {"id": 558, "seek": 326304, "start": 3276.72, "end": 3288.0, "text": " different data sets if you're interested um I have a question regarding like the choice of", "tokens": [51048, 819, 1412, 6352, 498, 291, 434, 3102, 1105, 286, 362, 257, 1168, 8595, 411, 264, 3922, 295, 51612], "temperature": 0.0, "avg_logprob": -0.1431674644595287, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0001851717970566824}, {"id": 559, "seek": 328800, "start": 3288.96, "end": 3297.36, "text": " uh n in terms of annotations um so how did you arrive at this number and how are you I mean", "tokens": [50412, 2232, 297, 294, 2115, 295, 25339, 763, 1105, 370, 577, 630, 291, 8881, 412, 341, 1230, 293, 577, 366, 291, 286, 914, 50832], "temperature": 0.0, "avg_logprob": -0.09848044575124547, "compression_ratio": 1.7515527950310559, "no_speech_prob": 0.003373901592567563}, {"id": 560, "seek": 328800, "start": 3298.0, "end": 3304.08, "text": " how do you know like what number is kind of sufficient yeah good question um so I assume", "tokens": [50864, 577, 360, 291, 458, 411, 437, 1230, 307, 733, 295, 11563, 1338, 665, 1168, 1105, 370, 286, 6552, 51168], "temperature": 0.0, "avg_logprob": -0.09848044575124547, "compression_ratio": 1.7515527950310559, "no_speech_prob": 0.003373901592567563}, {"id": 561, "seek": 328800, "start": 3304.08, "end": 3309.04, "text": " you mean the total number of images we needed to annotate and not the total number of annotations per", "tokens": [51168, 291, 914, 264, 3217, 1230, 295, 5267, 321, 2978, 281, 25339, 473, 293, 406, 264, 3217, 1230, 295, 25339, 763, 680, 51416], "temperature": 0.0, "avg_logprob": -0.09848044575124547, "compression_ratio": 1.7515527950310559, "no_speech_prob": 0.003373901592567563}, {"id": 562, "seek": 330904, "start": 3309.04, "end": 3319.52, "text": " image which end you mean I can talk oh I see I mean either yeah well okay so at both levels uh for", "tokens": [50364, 3256, 597, 917, 291, 914, 286, 393, 751, 1954, 286, 536, 286, 914, 2139, 1338, 731, 1392, 370, 412, 1293, 4358, 2232, 337, 50888], "temperature": 0.0, "avg_logprob": -0.08057055586860293, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.015181813389062881}, {"id": 563, "seek": 330904, "start": 3319.52, "end": 3326.08, "text": " the directions themselves we needed to collect at least two annotations to be able to measure", "tokens": [50888, 264, 11095, 2969, 321, 2978, 281, 2500, 412, 1935, 732, 25339, 763, 281, 312, 1075, 281, 3481, 51216], "temperature": 0.0, "avg_logprob": -0.08057055586860293, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.015181813389062881}, {"id": 564, "seek": 330904, "start": 3326.08, "end": 3332.48, "text": " intersubject agreement right uh we want to see if some direction is consistently producing", "tokens": [51216, 728, 30131, 1020, 8106, 558, 2232, 321, 528, 281, 536, 498, 512, 3513, 307, 14961, 10501, 51536], "temperature": 0.0, "avg_logprob": -0.08057055586860293, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.015181813389062881}, {"id": 565, "seek": 330904, "start": 3332.48, "end": 3338.72, "text": " meaningful similar annotations across annotators we need at least two people to annotate them um", "tokens": [51536, 10995, 2531, 25339, 763, 2108, 25339, 3391, 321, 643, 412, 1935, 732, 561, 281, 25339, 473, 552, 1105, 51848], "temperature": 0.0, "avg_logprob": -0.08057055586860293, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.015181813389062881}, {"id": 566, "seek": 333872, "start": 3338.72, "end": 3344.9599999999996, "text": " so for all the directions we evaluated we had two annotators label them and measured the interanitator", "tokens": [50364, 370, 337, 439, 264, 11095, 321, 25509, 321, 632, 732, 25339, 3391, 7645, 552, 293, 12690, 264, 728, 282, 270, 1639, 50676], "temperature": 0.0, "avg_logprob": -0.07084181549352243, "compression_ratio": 1.9243027888446216, "no_speech_prob": 0.00014424178516492248}, {"id": 567, "seek": 333872, "start": 3344.9599999999996, "end": 3351.8399999999997, "text": " agreement using a couple of different metrics blue and burnt scores um but for a subset of", "tokens": [50676, 8106, 1228, 257, 1916, 295, 819, 16367, 3344, 293, 18901, 13444, 1105, 457, 337, 257, 25993, 295, 51020], "temperature": 0.0, "avg_logprob": -0.07084181549352243, "compression_ratio": 1.9243027888446216, "no_speech_prob": 0.00014424178516492248}, {"id": 568, "seek": 333872, "start": 3351.8399999999997, "end": 3357.3599999999997, "text": " those we had 10 annotators annotate them and just had a look at interanitator agreement across a", "tokens": [51020, 729, 321, 632, 1266, 25339, 3391, 25339, 473, 552, 293, 445, 632, 257, 574, 412, 728, 282, 270, 1639, 8106, 2108, 257, 51296], "temperature": 0.0, "avg_logprob": -0.07084181549352243, "compression_ratio": 1.9243027888446216, "no_speech_prob": 0.00014424178516492248}, {"id": 569, "seek": 333872, "start": 3357.3599999999997, "end": 3363.12, "text": " slightly larger group uh for expense reasons we didn't do that for for all the directions because", "tokens": [51296, 4748, 4833, 1594, 2232, 337, 18406, 4112, 321, 994, 380, 360, 300, 337, 337, 439, 264, 11095, 570, 51584], "temperature": 0.0, "avg_logprob": -0.07084181549352243, "compression_ratio": 1.9243027888446216, "no_speech_prob": 0.00014424178516492248}, {"id": 570, "seek": 333872, "start": 3363.12, "end": 3367.04, "text": " it really wasn't necessary things didn't change that much and even in that subset when we went", "tokens": [51584, 309, 534, 2067, 380, 4818, 721, 994, 380, 1319, 300, 709, 293, 754, 294, 300, 25993, 562, 321, 1437, 51780], "temperature": 0.0, "avg_logprob": -0.07084181549352243, "compression_ratio": 1.9243027888446216, "no_speech_prob": 0.00014424178516492248}, {"id": 571, "seek": 336704, "start": 3367.04, "end": 3374.4, "text": " from two to ten per uh per direction and then for the number of directions that we chose to", "tokens": [50364, 490, 732, 281, 2064, 680, 2232, 680, 3513, 293, 550, 337, 264, 1230, 295, 11095, 300, 321, 5111, 281, 50732], "temperature": 0.0, "avg_logprob": -0.07962528116562788, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0027999188750982285}, {"id": 572, "seek": 336704, "start": 3374.4, "end": 3384.24, "text": " visualize it was not a very principled decision I'm afraid um we chose I think 64 z uh per category", "tokens": [50732, 23273, 309, 390, 406, 257, 588, 3681, 15551, 3537, 286, 478, 4638, 1105, 321, 5111, 286, 519, 12145, 710, 2232, 680, 7719, 51224], "temperature": 0.0, "avg_logprob": -0.07962528116562788, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0027999188750982285}, {"id": 573, "seek": 336704, "start": 3384.24, "end": 3388.56, "text": " and then a bunch of different minimal meaningful directions for them corresponding to I think", "tokens": [51224, 293, 550, 257, 3840, 295, 819, 13206, 10995, 11095, 337, 552, 11760, 281, 286, 519, 51440], "temperature": 0.0, "avg_logprob": -0.07962528116562788, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0027999188750982285}, {"id": 574, "seek": 336704, "start": 3388.56, "end": 3393.2799999999997, "text": " the same number of principal components that we looked at in the GAN space papers so maybe the", "tokens": [51440, 264, 912, 1230, 295, 9716, 6677, 300, 321, 2956, 412, 294, 264, 460, 1770, 1901, 10577, 370, 1310, 264, 51676], "temperature": 0.0, "avg_logprob": -0.07962528116562788, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0027999188750982285}, {"id": 575, "seek": 339328, "start": 3393.28, "end": 3401.0400000000004, "text": " top 20 in each category so it was a bit ad hoc that decision um the the things that's going to change", "tokens": [50364, 1192, 945, 294, 1184, 7719, 370, 309, 390, 257, 857, 614, 16708, 300, 3537, 1105, 264, 264, 721, 300, 311, 516, 281, 1319, 50752], "temperature": 0.0, "avg_logprob": -0.07899134854475658, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.0019263862632215023}, {"id": 576, "seek": 339328, "start": 3401.0400000000004, "end": 3408.6400000000003, "text": " we can distill vocabularies using this method for any size of annotation library right uh which is", "tokens": [50752, 321, 393, 42923, 2329, 455, 1040, 530, 1228, 341, 3170, 337, 604, 2744, 295, 48654, 6405, 558, 2232, 597, 307, 51132], "temperature": 0.0, "avg_logprob": -0.07899134854475658, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.0019263862632215023}, {"id": 577, "seek": 339328, "start": 3408.6400000000003, "end": 3412.32, "text": " one of the one of the beauties and one of the things that gives itself to to some of these more", "tokens": [51132, 472, 295, 264, 472, 295, 264, 1869, 530, 293, 472, 295, 264, 721, 300, 2709, 2564, 281, 281, 512, 295, 613, 544, 51316], "temperature": 0.0, "avg_logprob": -0.07899134854475658, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.0019263862632215023}, {"id": 578, "seek": 339328, "start": 3412.32, "end": 3419.84, "text": " ad hoc decisions um we're doing it analytically right if we go back to this we're actually like", "tokens": [51316, 614, 16708, 5327, 1105, 321, 434, 884, 309, 10783, 984, 558, 498, 321, 352, 646, 281, 341, 321, 434, 767, 411, 51692], "temperature": 0.0, "avg_logprob": -0.07899134854475658, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.0019263862632215023}, {"id": 579, "seek": 341984, "start": 3419.92, "end": 3425.84, "text": " reading off we're solving for the embedding matrix um of word embeddings in latent space", "tokens": [50368, 3760, 766, 321, 434, 12606, 337, 264, 12240, 3584, 8141, 1105, 295, 1349, 12240, 29432, 294, 48994, 1901, 50664], "temperature": 0.0, "avg_logprob": -0.08116046201835558, "compression_ratio": 1.9125, "no_speech_prob": 0.0019874214194715023}, {"id": 580, "seek": 341984, "start": 3425.84, "end": 3430.08, "text": " of concept embeddings so we could do this with like just a couple of directions", "tokens": [50664, 295, 3410, 12240, 29432, 370, 321, 727, 360, 341, 365, 411, 445, 257, 1916, 295, 11095, 50876], "temperature": 0.0, "avg_logprob": -0.08116046201835558, "compression_ratio": 1.9125, "no_speech_prob": 0.0019874214194715023}, {"id": 581, "seek": 341984, "start": 3431.28, "end": 3435.92, "text": " if you only had one annotation per concept it only appeared once then you're just going to get", "tokens": [50936, 498, 291, 787, 632, 472, 48654, 680, 3410, 309, 787, 8516, 1564, 550, 291, 434, 445, 516, 281, 483, 51168], "temperature": 0.0, "avg_logprob": -0.08116046201835558, "compression_ratio": 1.9125, "no_speech_prob": 0.0019874214194715023}, {"id": 582, "seek": 341984, "start": 3436.48, "end": 3443.04, "text": " for that direction um so as you increase the vocabulary as you increase the sample size you're", "tokens": [51196, 337, 300, 3513, 1105, 370, 382, 291, 3488, 264, 19864, 382, 291, 3488, 264, 6889, 2744, 291, 434, 51524], "temperature": 0.0, "avg_logprob": -0.08116046201835558, "compression_ratio": 1.9125, "no_speech_prob": 0.0019874214194715023}, {"id": 583, "seek": 341984, "start": 3443.04, "end": 3448.0, "text": " probably going to get a richer vocabulary but it's still possible to do on a vocabulary of this size", "tokens": [51524, 1391, 516, 281, 483, 257, 29021, 19864, 457, 309, 311, 920, 1944, 281, 360, 322, 257, 19864, 295, 341, 2744, 51772], "temperature": 0.0, "avg_logprob": -0.08116046201835558, "compression_ratio": 1.9125, "no_speech_prob": 0.0019874214194715023}, {"id": 584, "seek": 344800, "start": 3448.0, "end": 3454.24, "text": " um so we're deciding now whether it makes sense to scale this up and collect like a number of", "tokens": [50364, 1105, 370, 321, 434, 17990, 586, 1968, 309, 1669, 2020, 281, 4373, 341, 493, 293, 2500, 411, 257, 1230, 295, 50676], "temperature": 0.0, "avg_logprob": -0.11151665732974098, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0007094538304954767}, {"id": 585, "seek": 344800, "start": 3454.24, "end": 3459.28, "text": " annotations where it would be possible like I said to to train a captioner on them to be able", "tokens": [50676, 25339, 763, 689, 309, 576, 312, 1944, 411, 286, 848, 281, 281, 3847, 257, 31974, 260, 322, 552, 281, 312, 1075, 50928], "temperature": 0.0, "avg_logprob": -0.11151665732974098, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0007094538304954767}, {"id": 586, "seek": 344800, "start": 3459.28, "end": 3465.52, "text": " to automatically label these directions rather than have humans do it so part of it is constrained by", "tokens": [50928, 281, 6772, 7645, 613, 11095, 2831, 813, 362, 6255, 360, 309, 370, 644, 295, 309, 307, 38901, 538, 51240], "temperature": 0.0, "avg_logprob": -0.11151665732974098, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0007094538304954767}, {"id": 587, "seek": 344800, "start": 3465.52, "end": 3469.68, "text": " the tractability of experiments on mechanical Turk right how many reliable annotations you", "tokens": [51240, 264, 24207, 2310, 295, 12050, 322, 12070, 15714, 558, 577, 867, 12924, 25339, 763, 291, 51448], "temperature": 0.0, "avg_logprob": -0.11151665732974098, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0007094538304954767}, {"id": 588, "seek": 346968, "start": 3469.68, "end": 3479.9199999999996, "text": " can get in some period of time awesome uh thank you yeah these are really useful questions these are", "tokens": [50364, 393, 483, 294, 512, 2896, 295, 565, 3476, 2232, 1309, 291, 1338, 613, 366, 534, 4420, 1651, 613, 366, 50876], "temperature": 0.0, "avg_logprob": -0.080295346504034, "compression_ratio": 1.7990867579908676, "no_speech_prob": 0.01825583353638649}, {"id": 589, "seek": 346968, "start": 3479.9199999999996, "end": 3488.7999999999997, "text": " great um kind of along those lines more of a random question for the printer like the single words", "tokens": [50876, 869, 1105, 733, 295, 2051, 729, 3876, 544, 295, 257, 4974, 1168, 337, 264, 16671, 411, 264, 2167, 2283, 51320], "temperature": 0.0, "avg_logprob": -0.080295346504034, "compression_ratio": 1.7990867579908676, "no_speech_prob": 0.01825583353638649}, {"id": 590, "seek": 346968, "start": 3488.7999999999997, "end": 3493.52, "text": " for the labels yeah was it kind of agreed upon earlier like kind of what words you'd use because", "tokens": [51320, 337, 264, 16949, 1338, 390, 309, 733, 295, 9166, 3564, 3071, 411, 733, 295, 437, 2283, 291, 1116, 764, 570, 51556], "temperature": 0.0, "avg_logprob": -0.080295346504034, "compression_ratio": 1.7990867579908676, "no_speech_prob": 0.01825583353638649}, {"id": 591, "seek": 346968, "start": 3493.52, "end": 3498.3999999999996, "text": " like for festive maybe someone would say lively or for inviting you'd say welcoming is there like", "tokens": [51556, 411, 337, 42729, 1310, 1580, 576, 584, 30866, 420, 337, 18202, 291, 1116, 584, 17378, 307, 456, 411, 51800], "temperature": 0.0, "avg_logprob": -0.080295346504034, "compression_ratio": 1.7990867579908676, "no_speech_prob": 0.01825583353638649}, {"id": 592, "seek": 349840, "start": 3498.4, "end": 3507.04, "text": " kind of a similarity score for those words or really good question um no so this is it's only", "tokens": [50364, 733, 295, 257, 32194, 6175, 337, 729, 2283, 420, 534, 665, 1168, 1105, 572, 370, 341, 307, 309, 311, 787, 50796], "temperature": 0.0, "avg_logprob": -0.06771402580793513, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00041725902701728046}, {"id": 593, "seek": 349840, "start": 3507.04, "end": 3512.1600000000003, "text": " preprocessed with like a little bit of limitizing so we collapse different endings people might be", "tokens": [50796, 2666, 340, 780, 292, 365, 411, 257, 707, 857, 295, 4948, 3319, 370, 321, 15584, 819, 42474, 561, 1062, 312, 51052], "temperature": 0.0, "avg_logprob": -0.06771402580793513, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00041725902701728046}, {"id": 594, "seek": 349840, "start": 3512.1600000000003, "end": 3517.6, "text": " using our different verb conjugations onto single verbs uh but festive would have a different", "tokens": [51052, 1228, 527, 819, 9595, 29456, 763, 3911, 2167, 30051, 2232, 457, 42729, 576, 362, 257, 819, 51324], "temperature": 0.0, "avg_logprob": -0.06771402580793513, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00041725902701728046}, {"id": 595, "seek": 349840, "start": 3517.6, "end": 3523.84, "text": " direction from lively uh kind of a next step in post-processing that we've talked about but", "tokens": [51324, 3513, 490, 30866, 2232, 733, 295, 257, 958, 1823, 294, 2183, 12, 41075, 278, 300, 321, 600, 2825, 466, 457, 51636], "temperature": 0.0, "avg_logprob": -0.06771402580793513, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.00041725902701728046}, {"id": 596, "seek": 352384, "start": 3523.84, "end": 3529.6000000000004, "text": " haven't yet done um is to just collapse across like wordnets and sets right so you could use", "tokens": [50364, 2378, 380, 1939, 1096, 1105, 307, 281, 445, 15584, 2108, 411, 1349, 77, 1385, 293, 6352, 558, 370, 291, 727, 764, 50652], "temperature": 0.0, "avg_logprob": -0.06451087969320791, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.004467501770704985}, {"id": 597, "seek": 352384, "start": 3529.6000000000004, "end": 3535.1200000000003, "text": " something like that to find synonyms of festive and then approximate one direction for lively and", "tokens": [50652, 746, 411, 300, 281, 915, 5451, 2526, 2592, 295, 42729, 293, 550, 30874, 472, 3513, 337, 30866, 293, 50928], "temperature": 0.0, "avg_logprob": -0.06451087969320791, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.004467501770704985}, {"id": 598, "seek": 352384, "start": 3535.1200000000003, "end": 3540.48, "text": " then be able to break it down into something maybe more fine-grained um but there were no", "tokens": [50928, 550, 312, 1075, 281, 1821, 309, 760, 666, 746, 1310, 544, 2489, 12, 20735, 2001, 1105, 457, 456, 645, 572, 51196], "temperature": 0.0, "avg_logprob": -0.06451087969320791, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.004467501770704985}, {"id": 599, "seek": 352384, "start": 3540.48, "end": 3546.1600000000003, "text": " kind of heuristics or standards for the annotators except you know they did they did a practice run", "tokens": [51196, 733, 295, 415, 374, 6006, 420, 7787, 337, 264, 25339, 3391, 3993, 291, 458, 436, 630, 436, 630, 257, 3124, 1190, 51480], "temperature": 0.0, "avg_logprob": -0.06451087969320791, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.004467501770704985}, {"id": 600, "seek": 352384, "start": 3546.1600000000003, "end": 3551.52, "text": " and looked at a couple of different examples and were asked to describe an overall transformation", "tokens": [51480, 293, 2956, 412, 257, 1916, 295, 819, 5110, 293, 645, 2351, 281, 6786, 364, 4787, 9887, 51748], "temperature": 0.0, "avg_logprob": -0.06451087969320791, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.004467501770704985}, {"id": 601, "seek": 355152, "start": 3551.52, "end": 3556.32, "text": " that captured changes at lots of different kind of levels of abstraction we can look at the specific", "tokens": [50364, 300, 11828, 2962, 412, 3195, 295, 819, 733, 295, 4358, 295, 37765, 321, 393, 574, 412, 264, 2685, 50604], "temperature": 0.0, "avg_logprob": -0.10322718753992954, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.0026310128159821033}, {"id": 602, "seek": 355152, "start": 3556.32, "end": 3561.92, "text": " what did we tell her that's on here yeah how would you describe the overall transition changes in", "tokens": [50604, 437, 630, 321, 980, 720, 300, 311, 322, 510, 1338, 577, 576, 291, 6786, 264, 4787, 6034, 2962, 294, 50884], "temperature": 0.0, "avg_logprob": -0.10322718753992954, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.0026310128159821033}, {"id": 603, "seek": 355152, "start": 3561.92, "end": 3566.24, "text": " mood changes in objects or features of the scene don't mention your describing images so standard", "tokens": [50884, 9268, 2962, 294, 6565, 420, 4122, 295, 264, 4145, 500, 380, 2152, 428, 16141, 5267, 370, 3832, 51100], "temperature": 0.0, "avg_logprob": -0.10322718753992954, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.0026310128159821033}, {"id": 604, "seek": 355152, "start": 3566.24, "end": 3570.32, "text": " kind of turk boilerplate just address the content what you see and then they could look at some", "tokens": [51100, 733, 295, 3243, 74, 39228, 37008, 445, 2985, 264, 2701, 437, 291, 536, 293, 550, 436, 727, 574, 412, 512, 51304], "temperature": 0.0, "avg_logprob": -0.10322718753992954, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.0026310128159821033}, {"id": 605, "seek": 355152, "start": 3570.32, "end": 3575.68, "text": " samples and then after they did a practice run they did the annotations um so any interanitator", "tokens": [51304, 10938, 293, 550, 934, 436, 630, 257, 3124, 1190, 436, 630, 264, 25339, 763, 1105, 370, 604, 728, 282, 270, 1639, 51572], "temperature": 0.0, "avg_logprob": -0.10322718753992954, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.0026310128159821033}, {"id": 606, "seek": 357568, "start": 3575.68, "end": 3581.68, "text": " agreement is just based on their word choice which in some sense is a raw window into perception", "tokens": [50364, 8106, 307, 445, 2361, 322, 641, 1349, 3922, 597, 294, 512, 2020, 307, 257, 8936, 4910, 666, 12860, 50664], "temperature": 0.0, "avg_logprob": -0.05365471486692076, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.021608730778098106}, {"id": 607, "seek": 357568, "start": 3581.68, "end": 3586.56, "text": " but in some sense that's bullshit and there's going to be a lot of noise there uh and we did see", "tokens": [50664, 457, 294, 512, 2020, 300, 311, 22676, 293, 456, 311, 516, 281, 312, 257, 688, 295, 5658, 456, 2232, 293, 321, 630, 536, 50908], "temperature": 0.0, "avg_logprob": -0.05365471486692076, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.021608730778098106}, {"id": 608, "seek": 357568, "start": 3586.56, "end": 3592.3999999999996, "text": " that reflected when we used I don't have this on these slides um but when we use blue scores to", "tokens": [50908, 300, 15502, 562, 321, 1143, 286, 500, 380, 362, 341, 322, 613, 9788, 1105, 457, 562, 321, 764, 3344, 13444, 281, 51200], "temperature": 0.0, "avg_logprob": -0.05365471486692076, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.021608730778098106}, {"id": 609, "seek": 357568, "start": 3592.3999999999996, "end": 3598.16, "text": " to measure interanitator agreement so when we use these layer selective directions to generate these", "tokens": [51200, 281, 3481, 728, 282, 270, 1639, 8106, 370, 562, 321, 764, 613, 4583, 33930, 11095, 281, 8460, 613, 51488], "temperature": 0.0, "avg_logprob": -0.05365471486692076, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.021608730778098106}, {"id": 610, "seek": 357568, "start": 3598.16, "end": 3604.3999999999996, "text": " kinds of transformations if we get 10 people to annotate each transformation people might use", "tokens": [51488, 3685, 295, 34852, 498, 321, 483, 1266, 561, 281, 25339, 473, 1184, 9887, 561, 1062, 764, 51800], "temperature": 0.0, "avg_logprob": -0.05365471486692076, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.021608730778098106}, {"id": 611, "seek": 360440, "start": 3604.4, "end": 3609.44, "text": " somebody might say eerie somebody might say spooky right somebody might say scary to describe the", "tokens": [50364, 2618, 1062, 584, 308, 17487, 2618, 1062, 584, 30510, 558, 2618, 1062, 584, 6958, 281, 6786, 264, 50616], "temperature": 0.0, "avg_logprob": -0.08297298832943564, "compression_ratio": 1.884, "no_speech_prob": 0.0006262221722863615}, {"id": 612, "seek": 360440, "start": 3609.44, "end": 3615.12, "text": " sky uh that comes up as like quite different when you look at some methods of evaluating", "tokens": [50616, 5443, 2232, 300, 1487, 493, 382, 411, 1596, 819, 562, 291, 574, 412, 512, 7150, 295, 27479, 50900], "temperature": 0.0, "avg_logprob": -0.08297298832943564, "compression_ratio": 1.884, "no_speech_prob": 0.0006262221722863615}, {"id": 613, "seek": 360440, "start": 3615.12, "end": 3620.4, "text": " interanitator agreement so we used first scores as well that evaluate the semantic similarity", "tokens": [50900, 728, 282, 270, 1639, 8106, 370, 321, 1143, 700, 13444, 382, 731, 300, 13059, 264, 47982, 32194, 51164], "temperature": 0.0, "avg_logprob": -0.08297298832943564, "compression_ratio": 1.884, "no_speech_prob": 0.0006262221722863615}, {"id": 614, "seek": 360440, "start": 3621.44, "end": 3628.2400000000002, "text": " instead of just literal correspondence words and found that annotations of these kinds of", "tokens": [51216, 2602, 295, 445, 20411, 38135, 2283, 293, 1352, 300, 25339, 763, 295, 613, 3685, 295, 51556], "temperature": 0.0, "avg_logprob": -0.08297298832943564, "compression_ratio": 1.884, "no_speech_prob": 0.0006262221722863615}, {"id": 615, "seek": 360440, "start": 3628.2400000000002, "end": 3632.8, "text": " directions performed a lot higher when we looked at semantic similarity of annotations as opposed to", "tokens": [51556, 11095, 10332, 257, 688, 2946, 562, 321, 2956, 412, 47982, 32194, 295, 25339, 763, 382, 8851, 281, 51784], "temperature": 0.0, "avg_logprob": -0.08297298832943564, "compression_ratio": 1.884, "no_speech_prob": 0.0006262221722863615}, {"id": 616, "seek": 363280, "start": 3632.8, "end": 3639.6000000000004, "text": " just um just word based so there's definitely reason to start trying to collapse like that when", "tokens": [50364, 445, 1105, 445, 1349, 2361, 370, 456, 311, 2138, 1778, 281, 722, 1382, 281, 15584, 411, 300, 562, 50704], "temperature": 0.0, "avg_logprob": -0.0665719834241, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.00040438384166918695}, {"id": 617, "seek": 363280, "start": 3639.6000000000004, "end": 3644.7200000000003, "text": " we look at the vocabulary too but we haven't yet in some sense it's it's kind of beautiful because", "tokens": [50704, 321, 574, 412, 264, 19864, 886, 457, 321, 2378, 380, 1939, 294, 512, 2020, 309, 311, 309, 311, 733, 295, 2238, 570, 50960], "temperature": 0.0, "avg_logprob": -0.0665719834241, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.00040438384166918695}, {"id": 618, "seek": 363280, "start": 3644.7200000000003, "end": 3650.96, "text": " you can see all of the different words that people used to describe changes um but you'd get a lot", "tokens": [50960, 291, 393, 536, 439, 295, 264, 819, 2283, 300, 561, 1143, 281, 6786, 2962, 1105, 457, 291, 1116, 483, 257, 688, 51272], "temperature": 0.0, "avg_logprob": -0.0665719834241, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.00040438384166918695}, {"id": 619, "seek": 363280, "start": 3650.96, "end": 3656.8, "text": " more power right if you could combine annotations for festive and lively and vibrant under one", "tokens": [51272, 544, 1347, 558, 498, 291, 727, 10432, 25339, 763, 337, 42729, 293, 30866, 293, 21571, 833, 472, 51564], "temperature": 0.0, "avg_logprob": -0.0665719834241, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.00040438384166918695}, {"id": 620, "seek": 365680, "start": 3656.8, "end": 3666.88, "text": " umbrella bit of a trailer oh yeah thank you so much yeah any other high or low level questions", "tokens": [50364, 21925, 857, 295, 257, 11724, 1954, 1338, 1309, 291, 370, 709, 1338, 604, 661, 1090, 420, 2295, 1496, 1651, 50868], "temperature": 0.0, "avg_logprob": -0.1944548662971048, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.003881579264998436}, {"id": 621, "seek": 365680, "start": 3666.88, "end": 3673.92, "text": " inter just have a maybe one or two more things not much so ask away if you do I think more more", "tokens": [50868, 728, 445, 362, 257, 1310, 472, 420, 732, 544, 721, 406, 709, 370, 1029, 1314, 498, 291, 360, 286, 519, 544, 544, 51220], "temperature": 0.0, "avg_logprob": -0.1944548662971048, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.003881579264998436}, {"id": 622, "seek": 365680, "start": 3673.92, "end": 3680.8, "text": " of a higher level question I remember Ali in the first lecture um right you drew you had this", "tokens": [51220, 295, 257, 2946, 1496, 1168, 286, 1604, 12020, 294, 264, 700, 7991, 1105, 558, 291, 12804, 291, 632, 341, 51564], "temperature": 0.0, "avg_logprob": -0.1944548662971048, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.003881579264998436}, {"id": 623, "seek": 368080, "start": 3680.88, "end": 3686.0800000000004, "text": " visualization of like two points in the latent space and you know a last function that would", "tokens": [50368, 25801, 295, 411, 732, 2793, 294, 264, 48994, 1901, 293, 291, 458, 257, 1036, 2445, 300, 576, 50628], "temperature": 0.0, "avg_logprob": -0.12623327119009836, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.013834425248205662}, {"id": 624, "seek": 368080, "start": 3686.0800000000004, "end": 3692.7200000000003, "text": " steer like from one or trajectory one to the other but it was like something more like a curve", "tokens": [50628, 30814, 411, 490, 472, 420, 21512, 472, 281, 264, 661, 457, 309, 390, 411, 746, 544, 411, 257, 7605, 50960], "temperature": 0.0, "avg_logprob": -0.12623327119009836, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.013834425248205662}, {"id": 625, "seek": 368080, "start": 3692.7200000000003, "end": 3698.32, "text": " or something non-linear um right and you mentioned with Gannon version if you just interpolate like", "tokens": [50960, 420, 746, 2107, 12, 28263, 1105, 558, 293, 291, 2835, 365, 460, 16138, 3037, 498, 291, 445, 44902, 473, 411, 51240], "temperature": 0.0, "avg_logprob": -0.12623327119009836, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.013834425248205662}, {"id": 626, "seek": 368080, "start": 3699.28, "end": 3705.76, "text": " draw a straight line between two points you have like all sorts of things happening I was wondering", "tokens": [51288, 2642, 257, 2997, 1622, 1296, 732, 2793, 291, 362, 411, 439, 7527, 295, 721, 2737, 286, 390, 6359, 51612], "temperature": 0.0, "avg_logprob": -0.12623327119009836, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.013834425248205662}, {"id": 627, "seek": 370576, "start": 3706.48, "end": 3715.6800000000003, "text": " if there's like a I guess almost like a like a and I guess unsupervised not a random walk but a", "tokens": [50400, 498, 456, 311, 411, 257, 286, 2041, 1920, 411, 257, 411, 257, 293, 286, 2041, 2693, 12879, 24420, 406, 257, 4974, 1792, 457, 257, 50860], "temperature": 0.0, "avg_logprob": -0.15730209873147208, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.009250158444046974}, {"id": 628, "seek": 370576, "start": 3715.6800000000003, "end": 3725.5200000000004, "text": " walk that would I guess lead to less perturbations I guess in in terms of like features I mean", "tokens": [50860, 1792, 300, 576, 286, 2041, 1477, 281, 1570, 40468, 763, 286, 2041, 294, 294, 2115, 295, 411, 4122, 286, 914, 51352], "temperature": 0.0, "avg_logprob": -0.15730209873147208, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.009250158444046974}, {"id": 629, "seek": 370576, "start": 3727.28, "end": 3733.6000000000004, "text": " does it make sense uh yeah we I really wanted to do that for this project um maybe Ali can speak", "tokens": [51440, 775, 309, 652, 2020, 2232, 1338, 321, 286, 534, 1415, 281, 360, 300, 337, 341, 1716, 1105, 1310, 12020, 393, 1710, 51756], "temperature": 0.0, "avg_logprob": -0.15730209873147208, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.009250158444046974}, {"id": 630, "seek": 373360, "start": 3733.6, "end": 3740.0, "text": " a little bit more about about his work there maybe after we stop this recording but um linearization", "tokens": [50364, 257, 707, 857, 544, 466, 466, 702, 589, 456, 1310, 934, 321, 1590, 341, 6613, 457, 1105, 8213, 2144, 50684], "temperature": 0.0, "avg_logprob": -0.06425788047465872, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.001454541110433638}, {"id": 631, "seek": 373360, "start": 3740.0, "end": 3746.24, "text": " of this is a huge over oversimplification um and that would be one of exactly what you describe", "tokens": [50684, 295, 341, 307, 257, 2603, 670, 15488, 332, 564, 3774, 1105, 293, 300, 576, 312, 472, 295, 2293, 437, 291, 6786, 50996], "temperature": 0.0, "avg_logprob": -0.06425788047465872, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.001454541110433638}, {"id": 632, "seek": 373360, "start": 3746.24, "end": 3751.8399999999997, "text": " as one of the things I'm most keen to try is taking non-linear walks uh through any of these", "tokens": [50996, 382, 472, 295, 264, 721, 286, 478, 881, 20297, 281, 853, 307, 1940, 2107, 12, 28263, 12896, 2232, 807, 604, 295, 613, 51276], "temperature": 0.0, "avg_logprob": -0.06425788047465872, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.001454541110433638}, {"id": 633, "seek": 373360, "start": 3751.8399999999997, "end": 3759.44, "text": " subspaces um so very on point question haven't done it you should try and do it um but describing", "tokens": [51276, 2090, 79, 2116, 1105, 370, 588, 322, 935, 1168, 2378, 380, 1096, 309, 291, 820, 853, 293, 360, 309, 1105, 457, 16141, 51656], "temperature": 0.0, "avg_logprob": -0.06425788047465872, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.001454541110433638}, {"id": 634, "seek": 375944, "start": 3759.44, "end": 3765.2000000000003, "text": " like this the semantic structure of latent space the semantic topology if you'll permit me that", "tokens": [50364, 411, 341, 264, 47982, 3877, 295, 48994, 1901, 264, 47982, 1192, 1793, 498, 291, 603, 13423, 385, 300, 50652], "temperature": 0.0, "avg_logprob": -0.05964919585215894, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.004903154447674751}, {"id": 635, "seek": 375944, "start": 3765.76, "end": 3772.0, "text": " is a really interesting question um because even the visual meaning corresponding to", "tokens": [50680, 307, 257, 534, 1880, 1168, 1105, 570, 754, 264, 5056, 3620, 11760, 281, 50992], "temperature": 0.0, "avg_logprob": -0.05964919585215894, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.004903154447674751}, {"id": 636, "seek": 375944, "start": 3772.0, "end": 3779.36, "text": " some of these adjectives some of these words is not regularized or normalized in the latent space", "tokens": [50992, 512, 295, 613, 29378, 1539, 512, 295, 613, 2283, 307, 406, 3890, 1602, 420, 48704, 294, 264, 48994, 1901, 51360], "temperature": 0.0, "avg_logprob": -0.05964919585215894, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.004903154447674751}, {"id": 637, "seek": 375944, "start": 3779.36, "end": 3786.32, "text": " itself so if I take five steps in the festive direction it might take me five steps to get", "tokens": [51360, 2564, 370, 498, 286, 747, 1732, 4439, 294, 264, 42729, 3513, 309, 1062, 747, 385, 1732, 4439, 281, 483, 51708], "temperature": 0.0, "avg_logprob": -0.05964919585215894, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.004903154447674751}, {"id": 638, "seek": 378632, "start": 3786.32, "end": 3793.2000000000003, "text": " anything that will start to register to me as festive um but the walk size for a correspondingly", "tokens": [50364, 1340, 300, 486, 722, 281, 7280, 281, 385, 382, 42729, 1105, 457, 264, 1792, 2744, 337, 257, 11760, 356, 50708], "temperature": 0.0, "avg_logprob": -0.04931703908943835, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.0010984304826706648}, {"id": 639, "seek": 378632, "start": 3793.2000000000003, "end": 3798.96, "text": " large visual change so to speak in a different direction could be very different um so some", "tokens": [50708, 2416, 5056, 1319, 370, 281, 1710, 294, 257, 819, 3513, 727, 312, 588, 819, 1105, 370, 512, 50996], "temperature": 0.0, "avg_logprob": -0.04931703908943835, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.0010984304826706648}, {"id": 640, "seek": 378632, "start": 3798.96, "end": 3804.0800000000004, "text": " transformations like making an image black and white this is anecdotal but you only have to go", "tokens": [50996, 34852, 411, 1455, 364, 3256, 2211, 293, 2418, 341, 307, 26652, 38180, 457, 291, 787, 362, 281, 352, 51252], "temperature": 0.0, "avg_logprob": -0.04931703908943835, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.0010984304826706648}, {"id": 641, "seek": 378632, "start": 3804.0800000000004, "end": 3810.0800000000004, "text": " like one step in that direction and then we'll visualize the change almost immediately uh so", "tokens": [51252, 411, 472, 1823, 294, 300, 3513, 293, 550, 321, 603, 23273, 264, 1319, 1920, 4258, 2232, 370, 51552], "temperature": 0.0, "avg_logprob": -0.04931703908943835, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.0010984304826706648}, {"id": 642, "seek": 381008, "start": 3810.08, "end": 3816.96, "text": " we're not kind of we're not walking around in like a perceptually normalized space so to speak um", "tokens": [50364, 321, 434, 406, 733, 295, 321, 434, 406, 4494, 926, 294, 411, 257, 43276, 671, 48704, 1901, 370, 281, 1710, 1105, 50708], "temperature": 0.0, "avg_logprob": -0.06198864633386785, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0003249991568736732}, {"id": 643, "seek": 381008, "start": 3816.96, "end": 3822.16, "text": " and there hasn't been to my knowledge a lot of work that's addressed that everything's been a", "tokens": [50708, 293, 456, 6132, 380, 668, 281, 452, 3601, 257, 688, 295, 589, 300, 311, 13847, 300, 1203, 311, 668, 257, 50968], "temperature": 0.0, "avg_logprob": -0.06198864633386785, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0003249991568736732}, {"id": 644, "seek": 381008, "start": 3822.16, "end": 3830.0, "text": " little bit at hawk um so thinking about semantic topology subspaces non-linear versus linear paths", "tokens": [50968, 707, 857, 412, 33634, 74, 1105, 370, 1953, 466, 47982, 1192, 1793, 2090, 79, 2116, 2107, 12, 28263, 5717, 8213, 14518, 51360], "temperature": 0.0, "avg_logprob": -0.06198864633386785, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0003249991568736732}, {"id": 645, "seek": 381008, "start": 3830.0, "end": 3834.48, "text": " and how we can think about kind of the concept mesh underlying latent space for different", "tokens": [51360, 293, 577, 321, 393, 519, 466, 733, 295, 264, 3410, 17407, 14217, 48994, 1901, 337, 819, 51584], "temperature": 0.0, "avg_logprob": -0.06198864633386785, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0003249991568736732}, {"id": 646, "seek": 383448, "start": 3834.56, "end": 3841.12, "text": " generative models is extremely interesting to get to be a really cool area to do some working", "tokens": [50368, 1337, 1166, 5245, 307, 4664, 1880, 281, 483, 281, 312, 257, 534, 1627, 1859, 281, 360, 512, 1364, 50696], "temperature": 0.0, "avg_logprob": -0.10911088436841965, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.004679915960878134}, {"id": 647, "seek": 383448, "start": 3841.68, "end": 3848.96, "text": " cool thank you yeah let's ask Ali about that figure once we pull off here um I've got one more", "tokens": [50724, 1627, 1309, 291, 1338, 718, 311, 1029, 12020, 466, 300, 2573, 1564, 321, 2235, 766, 510, 1105, 286, 600, 658, 472, 544, 51088], "temperature": 0.0, "avg_logprob": -0.10911088436841965, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.004679915960878134}, {"id": 648, "seek": 383448, "start": 3848.96, "end": 3854.64, "text": " thing to show you a quick example to hopefully spark more discussion unless anybody has anything", "tokens": [51088, 551, 281, 855, 291, 257, 1702, 1365, 281, 4696, 9908, 544, 5017, 5969, 4472, 575, 1340, 51372], "temperature": 0.0, "avg_logprob": -0.10911088436841965, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.004679915960878134}, {"id": 649, "seek": 385464, "start": 3854.64, "end": 3867.2, "text": " specific about this project we can always come back all right oh geez well last thing I'm going to", "tokens": [50364, 2685, 466, 341, 1716, 321, 393, 1009, 808, 646, 439, 558, 1954, 46108, 731, 1036, 551, 286, 478, 516, 281, 50992], "temperature": 0.0, "avg_logprob": -0.08083350416542827, "compression_ratio": 1.6536312849162011, "no_speech_prob": 0.005468000657856464}, {"id": 650, "seek": 385464, "start": 3867.2, "end": 3875.2, "text": " show you uh is still a beta and uh it's very it's very early and it's even thought development", "tokens": [50992, 855, 291, 2232, 307, 920, 257, 9861, 293, 2232, 309, 311, 588, 309, 311, 588, 2440, 293, 309, 311, 754, 1194, 3250, 51392], "temperature": 0.0, "avg_logprob": -0.08083350416542827, "compression_ratio": 1.6536312849162011, "no_speech_prob": 0.005468000657856464}, {"id": 651, "seek": 385464, "start": 3875.2, "end": 3880.8799999999997, "text": " but it captures something um that I think is deeply interesting uh and I think might be interesting to", "tokens": [51392, 457, 309, 27986, 746, 1105, 300, 286, 519, 307, 8760, 1880, 2232, 293, 286, 519, 1062, 312, 1880, 281, 51676], "temperature": 0.0, "avg_logprob": -0.08083350416542827, "compression_ratio": 1.6536312849162011, "no_speech_prob": 0.005468000657856464}, {"id": 652, "seek": 388088, "start": 3880.88, "end": 3888.48, "text": " you all um so the former method that I showed you for building shared vocabulary between humans", "tokens": [50364, 291, 439, 1105, 370, 264, 5819, 3170, 300, 286, 4712, 291, 337, 2390, 5507, 19864, 1296, 6255, 50744], "temperature": 0.0, "avg_logprob": -0.038559883139854256, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.004466123413294554}, {"id": 653, "seek": 388088, "start": 3888.48, "end": 3895.6800000000003, "text": " and models relies heavily on language right and so we get some direction and we're able to share that", "tokens": [50744, 293, 5245, 30910, 10950, 322, 2856, 558, 293, 370, 321, 483, 512, 3513, 293, 321, 434, 1075, 281, 2073, 300, 51104], "temperature": 0.0, "avg_logprob": -0.038559883139854256, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.004466123413294554}, {"id": 654, "seek": 388088, "start": 3896.32, "end": 3903.04, "text": " between people and even to repeatedly use it to steer through model space um because we've given", "tokens": [51136, 1296, 561, 293, 754, 281, 18227, 764, 309, 281, 30814, 807, 2316, 1901, 1105, 570, 321, 600, 2212, 51472], "temperature": 0.0, "avg_logprob": -0.038559883139854256, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.004466123413294554}, {"id": 655, "seek": 388088, "start": 3903.04, "end": 3909.2000000000003, "text": " it a label right we've used language and you might even argue that you know that's constraining the", "tokens": [51472, 309, 257, 7645, 558, 321, 600, 1143, 2856, 293, 291, 1062, 754, 9695, 300, 291, 458, 300, 311, 11525, 1760, 264, 51780], "temperature": 0.0, "avg_logprob": -0.038559883139854256, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.004466123413294554}, {"id": 656, "seek": 390920, "start": 3909.2, "end": 3915.4399999999996, "text": " space of what people can recognize in those initial sample directions because there might be some", "tokens": [50364, 1901, 295, 437, 561, 393, 5521, 294, 729, 5883, 6889, 11095, 570, 456, 1062, 312, 512, 50676], "temperature": 0.0, "avg_logprob": -0.12744919459025064, "compression_ratio": 1.6610878661087867, "no_speech_prob": 0.002471477957442403}, {"id": 657, "seek": 390920, "start": 3915.4399999999996, "end": 3921.6, "text": " genus or quad aspects of of images that we don't really have words for um but are still like really", "tokens": [50676, 1049, 301, 420, 10787, 7270, 295, 295, 5267, 300, 321, 500, 380, 534, 362, 2283, 337, 1105, 457, 366, 920, 411, 534, 50984], "temperature": 0.0, "avg_logprob": -0.12744919459025064, "compression_ratio": 1.6610878661087867, "no_speech_prob": 0.002471477957442403}, {"id": 658, "seek": 390920, "start": 3921.6, "end": 3929.04, "text": " recognizable or perhaps the verbal you know that the words you would use to describe something are like", "tokens": [50984, 40757, 420, 4317, 264, 24781, 291, 458, 300, 264, 2283, 291, 576, 764, 281, 6786, 746, 366, 411, 51356], "temperature": 0.0, "avg_logprob": -0.12744919459025064, "compression_ratio": 1.6610878661087867, "no_speech_prob": 0.002471477957442403}, {"id": 659, "seek": 390920, "start": 3929.9199999999996, "end": 3934.3999999999996, "text": " quite complex and you wouldn't type that into an annotation like on mechanical Turk maybe you'd", "tokens": [51400, 1596, 3997, 293, 291, 2759, 380, 2010, 300, 666, 364, 48654, 411, 322, 12070, 15714, 1310, 291, 1116, 51624], "temperature": 0.0, "avg_logprob": -0.12744919459025064, "compression_ratio": 1.6610878661087867, "no_speech_prob": 0.002471477957442403}, {"id": 660, "seek": 393440, "start": 3934.48, "end": 3940.32, "text": " want to describe the sky as like the sky you saw at your grandmother's house the day she passed away", "tokens": [50368, 528, 281, 6786, 264, 5443, 382, 411, 264, 5443, 291, 1866, 412, 428, 14317, 311, 1782, 264, 786, 750, 4678, 1314, 50660], "temperature": 0.0, "avg_logprob": -0.05825638345309666, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0024721091613173485}, {"id": 661, "seek": 393440, "start": 3940.32, "end": 3946.32, "text": " or some flowers as effervescent like latte foam or a sparkling drink but you're not going to type", "tokens": [50660, 420, 512, 8085, 382, 1244, 9054, 2207, 411, 37854, 12958, 420, 257, 39967, 2822, 457, 291, 434, 406, 516, 281, 2010, 50960], "temperature": 0.0, "avg_logprob": -0.05825638345309666, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0024721091613173485}, {"id": 662, "seek": 393440, "start": 3946.32, "end": 3950.56, "text": " that into mechanical Turk and there's not a single word concept to capture it so that's", "tokens": [50960, 300, 666, 12070, 15714, 293, 456, 311, 406, 257, 2167, 1349, 3410, 281, 7983, 309, 370, 300, 311, 51172], "temperature": 0.0, "avg_logprob": -0.05825638345309666, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0024721091613173485}, {"id": 663, "seek": 393440, "start": 3950.56, "end": 3955.92, "text": " going to get lost in the method I described and lost in a lot of kind of standard either annotation", "tokens": [51172, 516, 281, 483, 2731, 294, 264, 3170, 286, 7619, 293, 2731, 294, 257, 688, 295, 733, 295, 3832, 2139, 48654, 51440], "temperature": 0.0, "avg_logprob": -0.05825638345309666, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0024721091613173485}, {"id": 664, "seek": 393440, "start": 3955.92, "end": 3963.84, "text": " based or uh kind of hard coded direction search so I wanted to experiment with a way to capture and", "tokens": [51440, 2361, 420, 2232, 733, 295, 1152, 34874, 3513, 3164, 370, 286, 1415, 281, 5120, 365, 257, 636, 281, 7983, 293, 51836], "temperature": 0.0, "avg_logprob": -0.05825638345309666, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0024721091613173485}, {"id": 665, "seek": 396384, "start": 3963.84, "end": 3969.84, "text": " learn um directions without language and this is like deeply inspired by the steerability work", "tokens": [50364, 1466, 1105, 11095, 1553, 2856, 293, 341, 307, 411, 8760, 7547, 538, 264, 30814, 2310, 589, 50664], "temperature": 0.0, "avg_logprob": -0.06992591181887856, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.0017263902118429542}, {"id": 666, "seek": 396384, "start": 3970.4, "end": 3976.0, "text": " of all these so you'll see a method here that is is similar to that in some sense", "tokens": [50692, 295, 439, 613, 370, 291, 603, 536, 257, 3170, 510, 300, 307, 307, 2531, 281, 300, 294, 512, 2020, 50972], "temperature": 0.0, "avg_logprob": -0.06992591181887856, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.0017263902118429542}, {"id": 667, "seek": 396384, "start": 3977.36, "end": 3983.04, "text": " but we're allowing the human to define the transformation that they want rather than", "tokens": [51040, 457, 321, 434, 8293, 264, 1952, 281, 6964, 264, 9887, 300, 436, 528, 2831, 813, 51324], "temperature": 0.0, "avg_logprob": -0.06992591181887856, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.0017263902118429542}, {"id": 668, "seek": 396384, "start": 3983.04, "end": 3988.1600000000003, "text": " pre-defining say a zoom or rotation transform using an algorithm we're allowing humans to come", "tokens": [51324, 659, 12, 20595, 1760, 584, 257, 8863, 420, 12447, 4088, 1228, 364, 9284, 321, 434, 8293, 6255, 281, 808, 51580], "temperature": 0.0, "avg_logprob": -0.06992591181887856, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.0017263902118429542}, {"id": 669, "seek": 398816, "start": 3988.16, "end": 3994.96, "text": " into the loop and define that transformation purely visually by interacting with very very", "tokens": [50364, 666, 264, 6367, 293, 6964, 300, 9887, 17491, 19622, 538, 18017, 365, 588, 588, 50704], "temperature": 0.0, "avg_logprob": -0.0524795369098061, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.004005513619631529}, {"id": 670, "seek": 398816, "start": 3994.96, "end": 4001.04, "text": " small batches of images sampled from latent space or feature space at some layer and sort them into", "tokens": [50704, 1359, 15245, 279, 295, 5267, 3247, 15551, 490, 48994, 1901, 420, 4111, 1901, 412, 512, 4583, 293, 1333, 552, 666, 51008], "temperature": 0.0, "avg_logprob": -0.0524795369098061, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.004005513619631529}, {"id": 671, "seek": 398816, "start": 4001.04, "end": 4008.48, "text": " classes corresponding to some visual feature its presence or its absence and this provides a pipeline", "tokens": [51008, 5359, 11760, 281, 512, 5056, 4111, 1080, 6814, 420, 1080, 17145, 293, 341, 6417, 257, 15517, 51380], "temperature": 0.0, "avg_logprob": -0.0524795369098061, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.004005513619631529}, {"id": 672, "seek": 398816, "start": 4008.48, "end": 4014.48, "text": " where users can steer just like in steerability work along dimensions that they discover however", "tokens": [51380, 689, 5022, 393, 30814, 445, 411, 294, 30814, 2310, 589, 2051, 12819, 300, 436, 4411, 4461, 51680], "temperature": 0.0, "avg_logprob": -0.0524795369098061, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.004005513619631529}, {"id": 673, "seek": 401448, "start": 4014.48, "end": 4019.36, "text": " that they define and they define them purely visually so labeling what happened just as a", "tokens": [50364, 300, 436, 6964, 293, 436, 6964, 552, 17491, 19622, 370, 40244, 437, 2011, 445, 382, 257, 50608], "temperature": 0.0, "avg_logprob": -0.07882528484992261, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.0007095253095030785}, {"id": 674, "seek": 401448, "start": 4019.36, "end": 4024.64, "text": " matter of convenience but they're discovered um through vision so the way to do this is really", "tokens": [50608, 1871, 295, 19283, 457, 436, 434, 6941, 1105, 807, 5201, 370, 264, 636, 281, 360, 341, 307, 534, 50872], "temperature": 0.0, "avg_logprob": -0.07882528484992261, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.0007095253095030785}, {"id": 675, "seek": 401448, "start": 4024.64, "end": 4031.44, "text": " simple um take some latent space again a lot of these examples are are using big GAN you could", "tokens": [50872, 2199, 1105, 747, 512, 48994, 1901, 797, 257, 688, 295, 613, 5110, 366, 366, 1228, 955, 460, 1770, 291, 727, 51212], "temperature": 0.0, "avg_logprob": -0.07882528484992261, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.0007095253095030785}, {"id": 676, "seek": 401448, "start": 4031.44, "end": 4039.12, "text": " also use style GAN um take some latent space and sample images from it right if you're using a", "tokens": [51212, 611, 764, 3758, 460, 1770, 1105, 747, 512, 48994, 1901, 293, 6889, 5267, 490, 309, 558, 498, 291, 434, 1228, 257, 51596], "temperature": 0.0, "avg_logprob": -0.07882528484992261, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.0007095253095030785}, {"id": 677, "seek": 401448, "start": 4039.12, "end": 4044.16, "text": " conditional model so we pick some category here we're looking at lakes inside big GAN image or", "tokens": [51596, 27708, 2316, 370, 321, 1888, 512, 7719, 510, 321, 434, 1237, 412, 25595, 1854, 955, 460, 1770, 3256, 420, 51848], "temperature": 0.0, "avg_logprob": -0.07882528484992261, "compression_ratio": 1.756554307116105, "no_speech_prob": 0.0007095253095030785}, {"id": 678, "seek": 404416, "start": 4044.16, "end": 4051.6, "text": " big GAN places um sample some images for a user and then that user who's determining a visual", "tokens": [50364, 955, 460, 1770, 3190, 1105, 6889, 512, 5267, 337, 257, 4195, 293, 550, 300, 4195, 567, 311, 23751, 257, 5056, 50736], "temperature": 0.0, "avg_logprob": -0.06347816489463629, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.00028679435490630567}, {"id": 679, "seek": 404416, "start": 4051.6, "end": 4058.3999999999996, "text": " dimension of interest kind of looks over that image space and sees if anything stands out to them", "tokens": [50736, 10139, 295, 1179, 733, 295, 1542, 670, 300, 3256, 1901, 293, 8194, 498, 1340, 7382, 484, 281, 552, 51076], "temperature": 0.0, "avg_logprob": -0.06347816489463629, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.00028679435490630567}, {"id": 680, "seek": 404416, "start": 4059.3599999999997, "end": 4066.72, "text": " across that that set of images so maybe here I noticed images that seemed kind of verdant and", "tokens": [51124, 2108, 300, 300, 992, 295, 5267, 370, 1310, 510, 286, 5694, 5267, 300, 6576, 733, 295, 6387, 394, 293, 51492], "temperature": 0.0, "avg_logprob": -0.06347816489463629, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.00028679435490630567}, {"id": 681, "seek": 404416, "start": 4066.72, "end": 4071.7599999999998, "text": " fertile uh and maybe more more spring light but not totally seasonal you'll see where I'm going", "tokens": [51492, 43509, 2232, 293, 1310, 544, 544, 5587, 1442, 457, 406, 3879, 27421, 291, 603, 536, 689, 286, 478, 516, 51744], "temperature": 0.0, "avg_logprob": -0.06347816489463629, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.00028679435490630567}, {"id": 682, "seek": 407176, "start": 4071.76, "end": 4077.36, "text": " it's kind of hard to describe and these were a little dreary or more wintry but there's not snow", "tokens": [50364, 309, 311, 733, 295, 1152, 281, 6786, 293, 613, 645, 257, 707, 22540, 822, 420, 544, 261, 686, 627, 457, 456, 311, 406, 5756, 50644], "temperature": 0.0, "avg_logprob": -0.10727443526276445, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.00041721074376255274}, {"id": 683, "seek": 407176, "start": 4077.36, "end": 4082.2400000000002, "text": " so it's not really winter they're just kind of less fertile and vivid so that's the distinction I", "tokens": [50644, 370, 309, 311, 406, 534, 6355, 436, 434, 445, 733, 295, 1570, 43509, 293, 23603, 370, 300, 311, 264, 16844, 286, 50888], "temperature": 0.0, "avg_logprob": -0.10727443526276445, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.00041721074376255274}, {"id": 684, "seek": 407176, "start": 4082.2400000000002, "end": 4089.36, "text": " want to make there um and the method is very simple just like the steerability work um and a", "tokens": [50888, 528, 281, 652, 456, 1105, 293, 264, 3170, 307, 588, 2199, 445, 411, 264, 30814, 2310, 589, 1105, 293, 257, 51244], "temperature": 0.0, "avg_logprob": -0.10727443526276445, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.00041721074376255274}, {"id": 685, "seek": 407176, "start": 4090.0800000000004, "end": 4096.0, "text": " another example of work from Bolle we define a transformation just by learning a hyperplane", "tokens": [51280, 1071, 1365, 295, 589, 490, 363, 1833, 68, 321, 6964, 257, 9887, 445, 538, 2539, 257, 9848, 36390, 51576], "temperature": 0.0, "avg_logprob": -0.10727443526276445, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.00041721074376255274}, {"id": 686, "seek": 407176, "start": 4096.0, "end": 4101.4400000000005, "text": " so training a SVM and learning a hyperplane it separates those two classes of images either", "tokens": [51576, 370, 3097, 257, 31910, 44, 293, 2539, 257, 9848, 36390, 309, 34149, 729, 732, 5359, 295, 5267, 2139, 51848], "temperature": 0.0, "avg_logprob": -0.10727443526276445, "compression_ratio": 1.757462686567164, "no_speech_prob": 0.00041721074376255274}, {"id": 687, "seek": 410144, "start": 4101.44, "end": 4106.799999999999, "text": " in the latent space or in the feature space of some layer layers activations and then when we can", "tokens": [50364, 294, 264, 48994, 1901, 420, 294, 264, 4111, 1901, 295, 512, 4583, 7914, 2430, 763, 293, 550, 562, 321, 393, 50632], "temperature": 0.0, "avg_logprob": -0.061250969421031864, "compression_ratio": 1.87, "no_speech_prob": 0.00013979972572997212}, {"id": 688, "seek": 410144, "start": 4107.5199999999995, "end": 4114.32, "text": " steer some starting image in a direction that's normal to that hyperplane and steer it across", "tokens": [50668, 30814, 512, 2891, 3256, 294, 257, 3513, 300, 311, 2710, 281, 300, 9848, 36390, 293, 30814, 309, 2108, 51008], "temperature": 0.0, "avg_logprob": -0.061250969421031864, "compression_ratio": 1.87, "no_speech_prob": 0.00013979972572997212}, {"id": 689, "seek": 410144, "start": 4114.32, "end": 4120.0, "text": " those classes right so I could take an image that starts in the kind of dreary or domain", "tokens": [51008, 729, 5359, 558, 370, 286, 727, 747, 364, 3256, 300, 3719, 294, 264, 733, 295, 22540, 822, 420, 9274, 51292], "temperature": 0.0, "avg_logprob": -0.061250969421031864, "compression_ratio": 1.87, "no_speech_prob": 0.00013979972572997212}, {"id": 690, "seek": 410144, "start": 4120.719999999999, "end": 4127.679999999999, "text": " or dusky or domain and transform it normally to that hyperplane and take it into the category", "tokens": [51328, 420, 14284, 4133, 420, 9274, 293, 4088, 309, 5646, 281, 300, 9848, 36390, 293, 747, 309, 666, 264, 7719, 51676], "temperature": 0.0, "avg_logprob": -0.061250969421031864, "compression_ratio": 1.87, "no_speech_prob": 0.00013979972572997212}, {"id": 691, "seek": 412768, "start": 4127.68, "end": 4134.320000000001, "text": " of things that I thought was more verdant right or more fertile but I could specify that separating", "tokens": [50364, 295, 721, 300, 286, 1194, 390, 544, 6387, 394, 558, 420, 544, 43509, 457, 286, 727, 16500, 300, 29279, 50696], "temperature": 0.0, "avg_logprob": -0.04940245872320131, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0040689436718821526}, {"id": 692, "seek": 412768, "start": 4134.320000000001, "end": 4142.400000000001, "text": " hyperplane just by sorting a shockingly few number of images um so we've done a couple of more like", "tokens": [50696, 9848, 36390, 445, 538, 32411, 257, 5588, 12163, 1326, 1230, 295, 5267, 1105, 370, 321, 600, 1096, 257, 1916, 295, 544, 411, 51100], "temperature": 0.0, "avg_logprob": -0.04940245872320131, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0040689436718821526}, {"id": 693, "seek": 412768, "start": 4142.400000000001, "end": 4147.92, "text": " fine-grained tests here but just for proof of concept you can discern these directions with", "tokens": [51100, 2489, 12, 20735, 2001, 6921, 510, 457, 445, 337, 8177, 295, 3410, 291, 393, 30868, 613, 11095, 365, 51376], "temperature": 0.0, "avg_logprob": -0.04940245872320131, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0040689436718821526}, {"id": 694, "seek": 412768, "start": 4147.92, "end": 4153.52, "text": " some degree of reliability with just like five to six examples of images in each category making", "tokens": [51376, 512, 4314, 295, 24550, 365, 445, 411, 1732, 281, 2309, 5110, 295, 5267, 294, 1184, 7719, 1455, 51656], "temperature": 0.0, "avg_logprob": -0.04940245872320131, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0040689436718821526}, {"id": 695, "seek": 415352, "start": 4153.6, "end": 4158.240000000001, "text": " it really simple to interact with something like this just by dragging and sorting a few images", "tokens": [50368, 309, 534, 2199, 281, 4648, 365, 746, 411, 341, 445, 538, 24385, 293, 32411, 257, 1326, 5267, 50600], "temperature": 0.0, "avg_logprob": -0.10240570430097909, "compression_ratio": 1.675, "no_speech_prob": 0.001597430557012558}, {"id": 696, "seek": 415352, "start": 4159.200000000001, "end": 4165.360000000001, "text": " that are sampled in the latent space okay so there's a tiny example of a demo app we have for", "tokens": [50648, 300, 366, 3247, 15551, 294, 264, 48994, 1901, 1392, 370, 456, 311, 257, 5870, 1365, 295, 257, 10723, 724, 321, 362, 337, 50956], "temperature": 0.0, "avg_logprob": -0.10240570430097909, "compression_ratio": 1.675, "no_speech_prob": 0.001597430557012558}, {"id": 697, "seek": 415352, "start": 4165.360000000001, "end": 4169.68, "text": " this um and we're switching where it's hosted so it's not online at this very moment but it will", "tokens": [50956, 341, 1105, 293, 321, 434, 16493, 689, 309, 311, 19204, 370, 309, 311, 406, 2950, 412, 341, 588, 1623, 457, 309, 486, 51172], "temperature": 0.0, "avg_logprob": -0.10240570430097909, "compression_ratio": 1.675, "no_speech_prob": 0.001597430557012558}, {"id": 698, "seek": 415352, "start": 4169.68, "end": 4175.280000000001, "text": " be next week but it's called the latent compass it was at NeurIPS Creativity I think last year", "tokens": [51172, 312, 958, 1243, 457, 309, 311, 1219, 264, 48994, 10707, 309, 390, 412, 1734, 374, 40, 6273, 11972, 4253, 286, 519, 1036, 1064, 51452], "temperature": 0.0, "avg_logprob": -0.10240570430097909, "compression_ratio": 1.675, "no_speech_prob": 0.001597430557012558}, {"id": 699, "seek": 415352, "start": 4175.280000000001, "end": 4180.160000000001, "text": " the year before um you'll see the the home interface in a second but what we do is just", "tokens": [51452, 264, 1064, 949, 1105, 291, 603, 536, 264, 264, 1280, 9226, 294, 257, 1150, 457, 437, 321, 360, 307, 445, 51696], "temperature": 0.0, "avg_logprob": -0.10240570430097909, "compression_ratio": 1.675, "no_speech_prob": 0.001597430557012558}, {"id": 700, "seek": 418016, "start": 4180.16, "end": 4186.639999999999, "text": " what I said pick some category of BigGAN here it's BigGAN places um on the bottom you see images", "tokens": [50364, 437, 286, 848, 1888, 512, 7719, 295, 5429, 27699, 510, 309, 311, 5429, 27699, 3190, 1105, 322, 264, 2767, 291, 536, 5267, 50688], "temperature": 0.0, "avg_logprob": -0.07751235286746405, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0002378027274971828}, {"id": 701, "seek": 418016, "start": 4186.639999999999, "end": 4191.36, "text": " sampled from that category and the user drags them to the right and left of the screen corresponding", "tokens": [50688, 3247, 15551, 490, 300, 7719, 293, 264, 4195, 5286, 82, 552, 281, 264, 558, 293, 1411, 295, 264, 2568, 11760, 50924], "temperature": 0.0, "avg_logprob": -0.07751235286746405, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0002378027274971828}, {"id": 702, "seek": 418016, "start": 4191.36, "end": 4196.8, "text": " to two different kind of categories of concepts they want to capture uh and then once the compass", "tokens": [50924, 281, 732, 819, 733, 295, 10479, 295, 10392, 436, 528, 281, 7983, 2232, 293, 550, 1564, 264, 10707, 51196], "temperature": 0.0, "avg_logprob": -0.07751235286746405, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0002378027274971828}, {"id": 703, "seek": 418016, "start": 4196.8, "end": 4201.84, "text": " calibrates and we'll see that in a second and you can drag any new image and then transform it along", "tokens": [51196, 2104, 6414, 1024, 293, 321, 603, 536, 300, 294, 257, 1150, 293, 291, 393, 5286, 604, 777, 3256, 293, 550, 4088, 309, 2051, 51448], "temperature": 0.0, "avg_logprob": -0.07751235286746405, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0002378027274971828}, {"id": 704, "seek": 418016, "start": 4201.84, "end": 4207.36, "text": " that dimension so here we pick the closet category I've got full closets on the right", "tokens": [51448, 300, 10139, 370, 510, 321, 1888, 264, 16669, 7719, 286, 600, 658, 1577, 2611, 1385, 322, 264, 558, 51724], "temperature": 0.0, "avg_logprob": -0.07751235286746405, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0002378027274971828}, {"id": 705, "seek": 420736, "start": 4207.36, "end": 4212.719999999999, "text": " empty closets on the left and the dimension I want to capture here is something like fullness", "tokens": [50364, 6707, 2611, 1385, 322, 264, 1411, 293, 264, 10139, 286, 528, 281, 7983, 510, 307, 746, 411, 45262, 50632], "temperature": 0.0, "avg_logprob": -0.06482664361057511, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.0006262219976633787}, {"id": 706, "seek": 420736, "start": 4214.0, "end": 4218.799999999999, "text": " so I'm going to see if I can I can learn a direction corresponding to the visual difference", "tokens": [50696, 370, 286, 478, 516, 281, 536, 498, 286, 393, 286, 393, 1466, 257, 3513, 11760, 281, 264, 5056, 2649, 50936], "temperature": 0.0, "avg_logprob": -0.06482664361057511, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.0006262219976633787}, {"id": 707, "seek": 420736, "start": 4218.799999999999, "end": 4225.679999999999, "text": " between these two categories drag any new closet onto that center line and transform it along that", "tokens": [50936, 1296, 613, 732, 10479, 5286, 604, 777, 16669, 3911, 300, 3056, 1622, 293, 4088, 309, 2051, 300, 51280], "temperature": 0.0, "avg_logprob": -0.06482664361057511, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.0006262219976633787}, {"id": 708, "seek": 420736, "start": 4225.679999999999, "end": 4231.679999999999, "text": " direction filling and emptying the closets and what if we tried a different category what if I", "tokens": [51280, 3513, 10623, 293, 6113, 1840, 264, 2611, 1385, 293, 437, 498, 321, 3031, 257, 819, 7719, 437, 498, 286, 51580], "temperature": 0.0, "avg_logprob": -0.06482664361057511, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.0006262219976633787}, {"id": 709, "seek": 423168, "start": 4231.68, "end": 4238.8, "text": " wanted to turn a medina into a full closet right what is the type of fullness that's relevant to", "tokens": [50364, 1415, 281, 1261, 257, 1205, 1426, 666, 257, 1577, 16669, 558, 437, 307, 264, 2010, 295, 45262, 300, 311, 7340, 281, 50720], "temperature": 0.0, "avg_logprob": -0.0689643147480057, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.005553227849304676}, {"id": 710, "seek": 423168, "start": 4238.8, "end": 4244.08, "text": " a medina oh well it's adding people instead of adding clothes suggesting that what's been learned", "tokens": [50720, 257, 1205, 1426, 1954, 731, 309, 311, 5127, 561, 2602, 295, 5127, 5534, 18094, 300, 437, 311, 668, 3264, 50984], "temperature": 0.0, "avg_logprob": -0.0689643147480057, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.005553227849304676}, {"id": 711, "seek": 423168, "start": 4244.08, "end": 4250.96, "text": " there that direction in latent space is abstracting generalizable enough to capture some visually", "tokens": [50984, 456, 300, 3513, 294, 48994, 1901, 307, 12649, 278, 2674, 22395, 1547, 281, 7983, 512, 19622, 51328], "temperature": 0.0, "avg_logprob": -0.0689643147480057, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.005553227849304676}, {"id": 712, "seek": 423168, "start": 4250.96, "end": 4256.88, "text": " recognizable dimension of fullness that's meaningful to us in different scenes right and the model's", "tokens": [51328, 40757, 10139, 295, 45262, 300, 311, 10995, 281, 505, 294, 819, 8026, 558, 293, 264, 2316, 311, 51624], "temperature": 0.0, "avg_logprob": -0.0689643147480057, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.005553227849304676}, {"id": 713, "seek": 425688, "start": 4256.88, "end": 4262.400000000001, "text": " able to to generalize it in a way that's not totally dependent on the types of objects it saw", "tokens": [50364, 1075, 281, 281, 2674, 1125, 309, 294, 257, 636, 300, 311, 406, 3879, 12334, 322, 264, 3467, 295, 6565, 309, 1866, 50640], "temperature": 0.0, "avg_logprob": -0.040896250984885474, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.0019261115230619907}, {"id": 714, "seek": 425688, "start": 4262.400000000001, "end": 4268.400000000001, "text": " in one scene so it knows in a sense that clothes make a closet full but to make a market full", "tokens": [50640, 294, 472, 4145, 370, 309, 3255, 294, 257, 2020, 300, 5534, 652, 257, 16669, 1577, 457, 281, 652, 257, 2142, 1577, 50940], "temperature": 0.0, "avg_logprob": -0.040896250984885474, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.0019261115230619907}, {"id": 715, "seek": 425688, "start": 4268.400000000001, "end": 4273.68, "text": " we're not adding clothes we're adding people and so the fullness direction is something that adds", "tokens": [50940, 321, 434, 406, 5127, 5534, 321, 434, 5127, 561, 293, 370, 264, 45262, 3513, 307, 746, 300, 10860, 51204], "temperature": 0.0, "avg_logprob": -0.040896250984885474, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.0019261115230619907}, {"id": 716, "seek": 425688, "start": 4273.68, "end": 4278.72, "text": " more of whatever would make that scene full um to any scene that we're selecting in the model", "tokens": [51204, 544, 295, 2035, 576, 652, 300, 4145, 1577, 1105, 281, 604, 4145, 300, 321, 434, 18182, 294, 264, 2316, 51456], "temperature": 0.0, "avg_logprob": -0.040896250984885474, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.0019261115230619907}, {"id": 717, "seek": 425688, "start": 4278.72, "end": 4285.2, "text": " right and trained on so few examples of course this this is really quite imperfect but it's a", "tokens": [51456, 558, 293, 8895, 322, 370, 1326, 5110, 295, 1164, 341, 341, 307, 534, 1596, 26714, 457, 309, 311, 257, 51780], "temperature": 0.0, "avg_logprob": -0.040896250984885474, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.0019261115230619907}, {"id": 718, "seek": 428520, "start": 4285.2, "end": 4290.639999999999, "text": " good proof of concept of a way that users can interact super flexibly and really visually", "tokens": [50364, 665, 8177, 295, 3410, 295, 257, 636, 300, 5022, 393, 4648, 1687, 5896, 3545, 293, 534, 19622, 50636], "temperature": 0.0, "avg_logprob": -0.08920799813619475, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.0002958796685561538}, {"id": 719, "seek": 428520, "start": 4291.599999999999, "end": 4297.599999999999, "text": " with dimensions of interest and use that to kind of explore and surf the latent space of a model", "tokens": [50684, 365, 12819, 295, 1179, 293, 764, 300, 281, 733, 295, 6839, 293, 9684, 264, 48994, 1901, 295, 257, 2316, 50984], "temperature": 0.0, "avg_logprob": -0.08920799813619475, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.0002958796685561538}, {"id": 720, "seek": 428520, "start": 4298.48, "end": 4303.92, "text": " by producing replicable repeatable directions that others can explore without having to use language", "tokens": [51028, 538, 10501, 3248, 43023, 7149, 712, 11095, 300, 2357, 393, 6839, 1553, 1419, 281, 764, 2856, 51300], "temperature": 0.0, "avg_logprob": -0.08920799813619475, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.0002958796685561538}, {"id": 721, "seek": 428520, "start": 4304.48, "end": 4309.04, "text": " that's kind of a different way of carving up the puzzle of how to explore and assign meaning to", "tokens": [51328, 300, 311, 733, 295, 257, 819, 636, 295, 31872, 493, 264, 12805, 295, 577, 281, 6839, 293, 6269, 3620, 281, 51556], "temperature": 0.0, "avg_logprob": -0.08920799813619475, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.0002958796685561538}, {"id": 722, "seek": 430904, "start": 4309.12, "end": 4315.5199999999995, "text": " directions that we that we find in latent space okay that's at latentcompass.com", "tokens": [50368, 11095, 300, 321, 300, 321, 915, 294, 48994, 1901, 1392, 300, 311, 412, 4465, 317, 25052, 13, 1112, 50688], "temperature": 0.0, "avg_logprob": -0.1027680421486879, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.0013456394663080573}, {"id": 723, "seek": 430904, "start": 4315.5199999999995, "end": 4323.04, "text": " and we'll be back up next week I think bad timing okay so to return to our frame here", "tokens": [50688, 293, 321, 603, 312, 646, 493, 958, 1243, 286, 519, 1578, 10822, 1392, 370, 281, 2736, 281, 527, 3920, 510, 51064], "temperature": 0.0, "avg_logprob": -0.1027680421486879, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.0013456394663080573}, {"id": 724, "seek": 430904, "start": 4323.92, "end": 4330.16, "text": " we've been digging a bit into this intersection between art neuroscience and machine learning", "tokens": [51108, 321, 600, 668, 17343, 257, 857, 666, 341, 15236, 1296, 1523, 42762, 293, 3479, 2539, 51420], "temperature": 0.0, "avg_logprob": -0.1027680421486879, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.0013456394663080573}, {"id": 725, "seek": 430904, "start": 4330.16, "end": 4335.6, "text": " ways to explore models that have been trained on human creation right at different scales", "tokens": [51420, 2098, 281, 6839, 5245, 300, 362, 668, 8895, 322, 1952, 8016, 558, 412, 819, 17408, 51692], "temperature": 0.0, "avg_logprob": -0.1027680421486879, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.0013456394663080573}, {"id": 726, "seek": 433560, "start": 4335.6, "end": 4341.52, "text": " to create a new to iterate and interpolate upon archives and then also to start to understand", "tokens": [50364, 281, 1884, 257, 777, 281, 44497, 293, 44902, 473, 3564, 25607, 293, 550, 611, 281, 722, 281, 1223, 50660], "temperature": 0.0, "avg_logprob": -0.06491324802239735, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0016994685865938663}, {"id": 727, "seek": 433560, "start": 4341.52, "end": 4347.200000000001, "text": " what these models are representing and if our ways of interpreting dimensions inside models", "tokens": [50660, 437, 613, 5245, 366, 13460, 293, 498, 527, 2098, 295, 37395, 12819, 1854, 5245, 50944], "temperature": 0.0, "avg_logprob": -0.06491324802239735, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0016994685865938663}, {"id": 728, "seek": 433560, "start": 4347.200000000001, "end": 4353.4400000000005, "text": " can also teach us something about human perception or allow us to start to build models of aspects of", "tokens": [50944, 393, 611, 2924, 505, 746, 466, 1952, 12860, 420, 2089, 505, 281, 722, 281, 1322, 5245, 295, 7270, 295, 51256], "temperature": 0.0, "avg_logprob": -0.06491324802239735, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0016994685865938663}, {"id": 729, "seek": 433560, "start": 4353.4400000000005, "end": 4359.4400000000005, "text": " human vision that are otherwise pretty intractable because it's difficult to formalize what dimensions", "tokens": [51256, 1952, 5201, 300, 366, 5911, 1238, 560, 1897, 712, 570, 309, 311, 2252, 281, 9860, 1125, 437, 12819, 51556], "temperature": 0.0, "avg_logprob": -0.06491324802239735, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0016994685865938663}, {"id": 730, "seek": 433560, "start": 4359.4400000000005, "end": 4364.240000000001, "text": " underlie them where I could write down what dimensions under life physical scene understanding", "tokens": [51556, 833, 6302, 552, 689, 286, 727, 2464, 760, 437, 12819, 833, 993, 4001, 4145, 3701, 51796], "temperature": 0.0, "avg_logprob": -0.06491324802239735, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0016994685865938663}, {"id": 731, "seek": 436424, "start": 4364.24, "end": 4370.719999999999, "text": " because I know Newton's laws I couldn't write down what dimensions underlie aesthetic perception of", "tokens": [50364, 570, 286, 458, 19541, 311, 6064, 286, 2809, 380, 2464, 760, 437, 12819, 833, 6302, 20092, 12860, 295, 50688], "temperature": 0.0, "avg_logprob": -0.05044606713687672, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.0008555968524888158}, {"id": 732, "seek": 436424, "start": 4370.719999999999, "end": 4377.5199999999995, "text": " North African marketplaces or Babylonian tablets because I don't know what a large swath of people", "tokens": [50688, 4067, 7312, 2142, 34840, 420, 30278, 952, 27622, 570, 286, 500, 380, 458, 437, 257, 2416, 1693, 998, 295, 561, 51028], "temperature": 0.0, "avg_logprob": -0.05044606713687672, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.0008555968524888158}, {"id": 733, "seek": 436424, "start": 4377.5199999999995, "end": 4384.16, "text": " would find perceptually interesting in a bunch of marketplaces I know from cognitive science", "tokens": [51028, 576, 915, 43276, 671, 1880, 294, 257, 3840, 295, 2142, 34840, 286, 458, 490, 15605, 3497, 51360], "temperature": 0.0, "avg_logprob": -0.05044606713687672, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.0008555968524888158}, {"id": 734, "seek": 436424, "start": 4384.16, "end": 4390.96, "text": " research certain heuristics to look for but that wouldn't give us a full set of what a diversity", "tokens": [51360, 2132, 1629, 415, 374, 6006, 281, 574, 337, 457, 300, 2759, 380, 976, 505, 257, 1577, 992, 295, 437, 257, 8811, 51700], "temperature": 0.0, "avg_logprob": -0.05044606713687672, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.0008555968524888158}, {"id": 735, "seek": 439096, "start": 4391.04, "end": 4394.88, "text": " of humans might appreciate when looking at some scene especially things like its mood", "tokens": [50368, 295, 6255, 1062, 4449, 562, 1237, 412, 512, 4145, 2318, 721, 411, 1080, 9268, 50560], "temperature": 0.0, "avg_logprob": -0.06033007140012132, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.005726581439375877}, {"id": 736, "seek": 439096, "start": 4395.76, "end": 4400.56, "text": " so we can turn here to these kinds of large unstructured generative models that learn", "tokens": [50604, 370, 321, 393, 1261, 510, 281, 613, 3685, 295, 2416, 18799, 46847, 1337, 1166, 5245, 300, 1466, 50844], "temperature": 0.0, "avg_logprob": -0.06033007140012132, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.005726581439375877}, {"id": 737, "seek": 439096, "start": 4401.44, "end": 4407.04, "text": " entirely from data entirely from images and turn to them as like a fertile ground so to speak for", "tokens": [50888, 7696, 490, 1412, 7696, 490, 5267, 293, 1261, 281, 552, 382, 411, 257, 43509, 2727, 370, 281, 1710, 337, 51168], "temperature": 0.0, "avg_logprob": -0.06033007140012132, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.005726581439375877}, {"id": 738, "seek": 439096, "start": 4407.04, "end": 4414.0, "text": " starting to probe and represent human perceptual experiences inside their latent space and think", "tokens": [51168, 2891, 281, 22715, 293, 2906, 1952, 43276, 901, 5235, 1854, 641, 48994, 1901, 293, 519, 51516], "temperature": 0.0, "avg_logprob": -0.06033007140012132, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.005726581439375877}, {"id": 739, "seek": 439096, "start": 4414.0, "end": 4420.4800000000005, "text": " of latent space that way right as a screen as I said before onto which we can project human experience", "tokens": [51516, 295, 48994, 1901, 300, 636, 558, 382, 257, 2568, 382, 286, 848, 949, 3911, 597, 321, 393, 1716, 1952, 1752, 51840], "temperature": 0.0, "avg_logprob": -0.06033007140012132, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.005726581439375877}, {"id": 740, "seek": 442048, "start": 4420.48, "end": 4425.28, "text": " and then once we have those projections we can rerun them and interact with them and collaborate", "tokens": [50364, 293, 550, 1564, 321, 362, 729, 32371, 321, 393, 43819, 409, 552, 293, 4648, 365, 552, 293, 18338, 50604], "temperature": 0.0, "avg_logprob": -0.057420337200164796, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.00041064422111958265}, {"id": 741, "seek": 442048, "start": 4425.28, "end": 4430.959999999999, "text": " with them to create outputs of deep generative models that are particularly exquisite and that", "tokens": [50604, 365, 552, 281, 1884, 23930, 295, 2452, 1337, 1166, 5245, 300, 366, 4098, 454, 34152, 293, 300, 50888], "temperature": 0.0, "avg_logprob": -0.057420337200164796, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.00041064422111958265}, {"id": 742, "seek": 442048, "start": 4430.959999999999, "end": 4437.679999999999, "text": " represents some kind of collaboration between us and models of our our creation that are operating", "tokens": [50888, 8855, 512, 733, 295, 9363, 1296, 505, 293, 5245, 295, 527, 527, 8016, 300, 366, 7447, 51224], "temperature": 0.0, "avg_logprob": -0.057420337200164796, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.00041064422111958265}, {"id": 743, "seek": 442048, "start": 4437.679999999999, "end": 4444.879999999999, "text": " in parallel so that's where I will leave us my emails here I'm very discoverable online", "tokens": [51224, 294, 8952, 370, 300, 311, 689, 286, 486, 1856, 505, 452, 12524, 510, 286, 478, 588, 4411, 712, 2950, 51584], "temperature": 0.0, "avg_logprob": -0.057420337200164796, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.00041064422111958265}, {"id": 744, "seek": 444488, "start": 4444.88, "end": 4452.0, "text": " but you're welcome to write me questions anytime and I will wrap here and we can have a more", "tokens": [50364, 457, 291, 434, 2928, 281, 2464, 385, 1651, 13038, 293, 286, 486, 7019, 510, 293, 321, 393, 362, 257, 544, 50720], "temperature": 0.0, "avg_logprob": -0.23245656812513196, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.008283581584692001}, {"id": 745, "seek": 444488, "start": 4452.88, "end": 4458.08, "text": " casual discussion unless anybody has any last questions for this part", "tokens": [50764, 13052, 5017, 5969, 4472, 575, 604, 1036, 1651, 337, 341, 644, 51024], "temperature": 0.0, "avg_logprob": -0.23245656812513196, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.008283581584692001}, {"id": 746, "seek": 445808, "start": 4458.48, "end": 4479.12, "text": " Thank you so much sir this was really interesting and inspiring with all the", "tokens": [50384, 1044, 291, 370, 709, 4735, 341, 390, 534, 1880, 293, 15883, 365, 439, 264, 51416], "temperature": 0.0, "avg_logprob": -0.2628504965040419, "compression_ratio": 1.0555555555555556, "no_speech_prob": 0.005264850798994303}, {"id": 747, "seek": 447912, "start": 4479.36, "end": 4488.64, "text": " acidic decreasing slides and every moment of that was really full of thoughts I think that", "tokens": [50376, 39514, 23223, 9788, 293, 633, 1623, 295, 300, 390, 534, 1577, 295, 4598, 286, 519, 300, 50840], "temperature": 0.0, "avg_logprob": -0.19687342643737793, "compression_ratio": 1.4274193548387097, "no_speech_prob": 0.005889656953513622}, {"id": 748, "seek": 447912, "start": 4490.5599999999995, "end": 4504.24, "text": " you open a window to semantically and qualitatively looking at these latent spaces and", "tokens": [50936, 291, 1269, 257, 4910, 281, 4361, 49505, 293, 31312, 356, 1237, 412, 613, 48994, 7673, 293, 51620], "temperature": 0.0, "avg_logprob": -0.19687342643737793, "compression_ratio": 1.4274193548387097, "no_speech_prob": 0.005889656953513622}, {"id": 749, "seek": 450424, "start": 4505.2, "end": 4508.5599999999995, "text": " sort of our imagination and", "tokens": [50412, 1333, 295, 527, 12938, 293, 50580], "temperature": 0.0, "avg_logprob": -0.1353139423188709, "compression_ratio": 1.464, "no_speech_prob": 0.0016002451302483678}, {"id": 750, "seek": 450424, "start": 4511.36, "end": 4519.679999999999, "text": " where we dream and where these models that we create dream so I really appreciate that", "tokens": [50720, 689, 321, 3055, 293, 689, 613, 5245, 300, 321, 1884, 3055, 370, 286, 534, 4449, 300, 51136], "temperature": 0.0, "avg_logprob": -0.1353139423188709, "compression_ratio": 1.464, "no_speech_prob": 0.0016002451302483678}, {"id": 751, "seek": 450424, "start": 4521.84, "end": 4526.48, "text": " I'm going to stop recording and then see if there are more questions", "tokens": [51244, 286, 478, 516, 281, 1590, 6613, 293, 550, 536, 498, 456, 366, 544, 1651, 51476], "temperature": 0.0, "avg_logprob": -0.1353139423188709, "compression_ratio": 1.464, "no_speech_prob": 0.0016002451302483678}], "language": "en"}