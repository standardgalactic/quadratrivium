WEBVTT

00:00.000 --> 00:08.000
Okay, hello everyone. Welcome to this session of deep learning for art, aesthetics and creativity.

00:08.000 --> 00:24.000
Today, we have our specialist speaker, Junion Ju, and he is an assistant professor at CMU, School of Computer Science.

00:24.000 --> 00:41.000
He is going to talk about efficient GANS, and Junion is such a great researcher and scientist who has been working on many, many interesting generative models, including he was in peaks to peaks,

00:41.000 --> 00:56.000
and cycle consistency scans, and Gauguin, and many other interesting and intriguing work, and I hope that we can see a gist of some of his work here today.

00:56.000 --> 01:02.000
So, please, Junion, go ahead.

01:02.000 --> 01:17.000
Yes, thanks for the nice introduction. I'm very happy to be here and talk about some of our recent work on how to make GANS more efficient.

01:17.000 --> 01:29.000
Maybe some background that you probably already knew about it. Yes, GANS has been used for various content creation and creativity tasks.

01:29.000 --> 01:41.000
For example, you can draw a sketch of a handbag or edge of a handbag and you can get an output image from the GANS.

01:41.000 --> 01:52.000
And one recent model we have been working on is this Gauguin model, led by Thaisong Park and other people.

01:52.000 --> 02:02.000
Here, our artist is creating a semantic label that basically says this is a mountain, this is also a log, this is the sea.

02:02.000 --> 02:11.000
And when the mountain, he can add some sand, and we can generate an image in real time.

02:11.000 --> 02:16.000
And he can add some more details like logs.

02:16.000 --> 02:24.000
Yeah, and you can create a pretty nice image very quickly.

02:24.000 --> 02:39.000
That's a very cool demonstration of the GANS and you can also, sorry about that, you can also apply a particular like a style image from a truly set and then apply it to stylize this image.

02:40.000 --> 02:48.000
So these all look very nice and kind of perfect, right, those cool demo in front of so many people.

02:48.000 --> 03:00.000
But if we close the door and the story here is, it's really very expensive, at least for this Gauguin project.

03:00.000 --> 03:18.000
And to develop Gauguin project, Thaisong has about hundreds of GPUs, in the summer in turn, and for six or nine months he has access to several hundreds of GPUs, very high end, the media GPUs, which you cannot sometimes you cannot even buy it on the market,

03:18.000 --> 03:24.000
but he has access to internal GPUs, we are not allowed to share the information.

03:24.000 --> 03:32.000
But he has access to lots of GPUs during training, and the training also requires lots of data, like tens of thousands of data.

03:33.000 --> 03:54.000
After almost a year's development, to learn this model, to do this demo, we actually bought a very expensive desktop, like 6,000, 7,000 laptop with actually a high end GPU, but that's still a laptop, not a desktop, so we want to carry this laptop around and do the demo, right.

03:54.000 --> 04:10.000
So in conclusion, from this particular project I realized games are very expensive, perhaps not for everyone now at this moment.

04:10.000 --> 04:15.000
I think it's expensive in three ways.

04:15.000 --> 04:35.000
So for learning a game model, if you train a game model to learn the game model on a device, you need a high end GPUs, like for real time performance, otherwise it will be very laggy, you cannot have any real time demo without a high end GPU.

04:35.000 --> 04:45.000
So in order to develop the algorithm, to develop a new algorithm, you require hundreds of GPUs to train the model.

04:45.000 --> 05:05.000
So I think it's very, it's huge, or maybe right now only the deep end of the media can afford this kind of computation. You will for university lab, MIT or CMU, maybe you don't have so many GPUs, it's very hard to compete with big companies.

05:05.000 --> 05:10.000
And third, to train a game model, it often requires lots of data.

05:10.000 --> 05:32.000
And I will go through it one by one. The connection wise is very, it's very slow to learn on a CPU and mobile device, right, maybe take 10 seconds or several seconds to learn on models like CPU and even slower to learn on mobile device like a tablet or like a phone.

05:33.000 --> 05:43.000
To train a single model for a single training session, it takes several days, maybe one or two weeks, maybe sometimes for a big game, maybe take amounts.

05:43.000 --> 05:54.000
And, but it's not like you just train one model and publish a paper as model training might take at least a dozen of training sessions, you have hundreds of training sessions.

05:55.000 --> 06:01.000
So each training session takes, each training session takes a month, you need lots of GPUs to parallelize your experiments.

06:01.000 --> 06:08.000
So it's not like you have an idea, you train a model and you go through all the public paper, that's not the case.

06:08.000 --> 06:17.000
You have an idea, you try something to work, you try something to modify the idea slightly, you have lots of iteration over the months.

06:17.000 --> 06:34.000
So each iteration takes one week or one month on each GPUs. So if you, if you put these numbers together, if you want to actually do the research very as quickly as possible, it requires lots of GPUs.

06:34.000 --> 06:46.000
Sometimes it's even like a top university like top lab in MIT or CMU Berkeley cannot afford it, not saying other labs.

06:46.000 --> 06:52.000
So what people do realize is to treat this model like require tens of thousands of millions of images.

06:52.000 --> 06:56.000
You treat a model of faces, it requires 70,000 faces.

06:56.000 --> 07:03.000
These faces are very high quality images, and you need to align the face before you treat the model.

07:03.000 --> 07:13.000
So there are lots of pre-processing steps to get an image, to treat an image that's model, you need labels, and you need to limit the data sets first.

07:13.000 --> 07:19.000
So there are three things that can be used for more users, right?

07:19.000 --> 07:27.000
If you are a content creator, if you are an artist, you may not have access to the computation, to the algorithm and data, right?

07:27.000 --> 07:32.000
So in this talk, I would like to focus on computation and the data.

07:32.000 --> 07:43.000
But while working on the algorithm, in this talk, we have some results on how to make data faster, how to make you can treat a game model on maybe only hundreds of images.

07:43.000 --> 07:58.000
By that, we can maybe help more users, content creators and artists to treat and test their own game models without having access to lots of data, without having access to very high-end GPUs.

07:58.000 --> 08:04.000
So I will talk about the first part, which is the model.

08:04.000 --> 08:09.000
GANS are very, very computational expensive to run.

08:09.000 --> 08:27.000
If you compare GANS, or conditional GANS, like Psychogalgan to the typical image classification model, this is the computation, right?

08:27.000 --> 08:33.000
Maybe some people like to use Macs, some people like to use Flops, but the story is the same.

08:33.000 --> 08:38.000
It's quite cheap to run a mobile internet for image classification purpose.

08:38.000 --> 08:47.000
It's very relatively cheap to run a restats network for classifying caster versus dogs versus other categories.

08:47.000 --> 08:56.000
But somehow it's very expensive to run a Psychogalgan model.

08:56.000 --> 08:59.000
This gap is almost 500 times.

08:59.000 --> 09:10.000
If you compare the latest generative model versus the latest classifiers.

09:10.000 --> 09:15.000
And that's a general trend.

09:15.000 --> 09:18.000
I think you may ask why, right?

09:18.000 --> 09:22.000
Why is so much more expensive?

09:22.000 --> 09:31.000
The reason for that is, like in Psychogalgan, you take an image and you produce the image with the same resolution.

09:31.000 --> 09:38.000
So the spatial dimension of the tensor remains the same more or less through the process.

09:38.000 --> 09:43.000
When you classify, you take an image and you produce a number.

09:43.000 --> 09:50.000
So the spatial resolution of this tensor becomes smaller and smaller.

09:50.000 --> 09:55.000
I think that's part of the reason, but also to generate the image, maybe you need more features.

09:55.000 --> 09:58.000
That's the second reason.

09:58.000 --> 10:07.000
Anyways, it's quite expensive for GANs that compare to the image classifiers.

10:07.000 --> 10:11.000
In this work, I will briefly go through this work.

10:11.000 --> 10:16.000
If we try to solve this problem, we propose a method called GAN compression.

10:16.000 --> 10:26.000
GAN compression is a general purpose framework based on the distillation channel protein and neural active search.

10:26.000 --> 10:37.000
So the idea here is, so given a teacher model, teacher model is the original model for the psychogalgan model we like to compress.

10:37.000 --> 10:42.000
It has lots of channels.

10:42.000 --> 10:50.000
So you take a horse image as input in psychogalgan case, you output a zebra.

10:50.000 --> 11:00.000
And we also have a student, so the goal of this project is to try to find a student model, which with fewer filters,

11:00.000 --> 11:05.000
and it can produce the same kind of zebra as the teacher model.

11:05.000 --> 11:18.000
So we have a loss function to try to make sure the output, the student output zebra looks very similar to the teacher's output zebra.

11:18.000 --> 11:20.000
That's what loss we have.

11:20.000 --> 11:29.000
We also try to make sure that the student's intermediate feature representation is very similar to teacher's representation.

11:29.000 --> 11:32.000
And lastly, we would like to make sure we have a GAN loss here.

11:32.000 --> 11:40.000
We would like to make sure the student's output zebra looks like a zebra according to adversary loss.

11:40.000 --> 11:42.000
So we have three losses.

11:42.000 --> 11:44.000
We have the pixel loss.

11:44.000 --> 11:51.000
We try to make sure the student zebra looks like a teacher zebra.

11:51.000 --> 12:01.000
And we have a feature loss which makes sure the student's feature looks like student's feature looks like a teacher's feature.

12:01.000 --> 12:07.000
We have a third loss, which is GAN loss, just like a typical GAN loss you apply to any GAN.

12:07.000 --> 12:10.000
Yes, you want to make sure the zebra looks like a zebra.

12:10.000 --> 12:12.000
Okay.

12:12.000 --> 12:14.000
So this is our distillation part.

12:14.000 --> 12:30.000
In our channel protein part we would like to, so our goal is to search for each layer, the optimal number of channels which can, we want to find a smaller number of channels,

12:30.000 --> 12:35.000
but can still satisfy all these loss functions constraints.

12:35.000 --> 12:47.000
So we have a bunch of channels here, for example, we have 60 channels, you can choose 32 channels, and you can choose maybe 48, 64 if you like.

12:47.000 --> 12:50.000
And I can choose 24 if you like.

12:50.000 --> 12:52.000
So we have channel protein.

12:52.000 --> 13:01.000
So the idea is if you have fewer channels for each layer, your model will be smaller and faster to run.

13:02.000 --> 13:10.000
Let's say if you reduce the channel by half for every single layer, of course the model will be faster to run.

13:10.000 --> 13:22.000
But here, the search means we try to, we're not trying to reduce the channel uniformly, we try to maybe, for some layer we try to reduce it to 16 channels.

13:22.000 --> 13:26.000
For some layer maybe 24 channels are necessary, or 32 channels are necessary.

13:26.000 --> 13:37.000
So we try to, it's kind of like a search problem, the search space is just like channels per layer, you have eight layers, you have eight numbers you would like to search for.

13:37.000 --> 13:41.000
If you have 11 layers, you have 11 numbers you would like to search for.

13:41.000 --> 13:51.000
So the idea is we try to find this combination so that we can still reproduce a zebra which looks like teacher zebra.

13:51.000 --> 13:53.000
Okay.

13:53.000 --> 13:55.000
So how to do this search problem?

13:55.000 --> 14:00.000
The search seems very hard because for each configuration you need to train the model.

14:00.000 --> 14:03.000
And this is a combinatorial number, right?

14:03.000 --> 14:10.000
There are lots of combinations, you can, if there are four combinations, four choices per layer, that's lots of combinations.

14:10.000 --> 14:20.000
You don't like a naive way is you can train a model for each configuration and then compare which model works best.

14:20.000 --> 14:23.000
But that will take lots and lots of training time.

14:23.000 --> 14:31.000
So the idea here is we would like to have all the network to share the ways.

14:31.000 --> 14:41.000
The idea is if you have 32 channels, share the same first 60 channels with the 60 channel model.

14:41.000 --> 14:44.000
So by doing that, we don't have to train them all.

14:44.000 --> 14:50.000
Each individual configuration, every single time, we can share the ways across different configurations.

14:50.000 --> 14:52.000
So we still have only one model training.

14:52.000 --> 15:01.000
But each time we sample a configuration, it's like this configuration, that configuration, and we try to train this configuration with the loss.

15:01.000 --> 15:04.000
But we only have one training session.

15:04.000 --> 15:13.000
So by each training iteration, we sample a different configuration and try to train the model with the loss function I just described.

15:13.000 --> 15:30.000
Okay, so once we are done with the training, we can, then we can, then we can, for every single configuration, we can try to evaluate these models based on a matrix such as FID,

15:30.000 --> 15:38.000
and then there's other metrics you have and choose the best one, and then we can fine tune that model.

15:38.000 --> 15:48.000
Yeah, so there are a bunch of loss function, we kind of, so there are several ideas, one idea is there are several loss functions, we try to mimic the teacher model.

15:48.000 --> 15:59.000
But it's actually such ideas, we try to share the ways across different configurations, so that we can avoid treating each configuration multiple times.

16:00.000 --> 16:02.000
Every single time.

16:02.000 --> 16:05.000
Okay, so that's the idea of the method.

16:05.000 --> 16:09.000
I would like to share some results with you.

16:09.000 --> 16:17.000
So here we are so for the house to zero, we can reduce the model size for 57.

16:17.000 --> 16:26.000
The model size, the competition cost for 57 max to 2.7.

16:26.000 --> 16:31.000
7 to edge to shows, 7 to Gauguin.

16:31.000 --> 16:41.000
So we can achieve nine times to 24, 20 times compression ratio in terms of the model computational cost.

16:41.000 --> 16:53.000
Here is a demo is you will learn the original cycle again on this mobile device, Jason Javier GPU, and we can measure the FPS.

16:53.000 --> 17:00.000
Here is the, here's how it looks like you want to transform a house to a zebra.

17:00.000 --> 17:12.000
And here is we learn the same more compressed model on on this hardware. So I should see four times kind of speed up and it gets real time performance on this.

17:12.000 --> 17:22.000
This is a kind of like like on device chip you will use for like robots or cars.

17:23.000 --> 17:26.000
But here model is out here the input image.

17:26.000 --> 17:31.000
Got a cycle get output.

17:31.000 --> 17:34.000
Here is a baseline.

17:34.000 --> 17:40.000
You wish you just reduce the will cause 0.25 cycle guy.

17:40.000 --> 17:46.000
In this setting you basically just reduce them all channels.

17:47.000 --> 17:50.000
Per layer.

17:50.000 --> 18:01.000
And you lose them a channel to like by by force, by four times, but we do it for every single layer so there are no, no, no, no, no search happenings.

18:01.000 --> 18:17.000
You're very nice baseline. And you can see if you just reduce the demo channels uniformly every for every single layer, you will lose lots of details like zebras stripes which is something you would like to have.

18:17.000 --> 18:18.000
Okay.

18:18.000 --> 18:21.000
And here is the compressed model.

18:21.000 --> 18:25.000
20 times compressed model with the albaster.

18:25.000 --> 18:32.000
And you can preserve the output of the original teacher model.

18:32.000 --> 18:40.000
But why be 20 times faster.

18:40.000 --> 18:50.000
So recently we apply this idea to not only to cycle game but also to style get to this is ongoing submission.

18:50.000 --> 18:56.000
So, so the idea here is, we would like to have a teacher model.

18:56.000 --> 19:02.000
I would like to have, we will obtain a student model just like what we did before.

19:03.000 --> 19:15.000
And during the, the idea is doing the image projection will try to try to the ideas will project this image into the latent space of the gains.

19:15.000 --> 19:23.000
And then we add some different like directions like smiling directions like glasses directions, maybe different hair color.

19:23.000 --> 19:39.000
And here's we can do the interacting editing, we can use this low cost model, 10 times fast model, which only takes three seconds each time you adjust the slider.

19:39.000 --> 19:41.000
I will show a demo in a minute.

19:41.000 --> 19:50.000
But, but when was we the user, for example, you use someone to change the hair from black to to to to white.

19:50.000 --> 19:54.000
You can do that right and you can change this one to this one.

19:54.000 --> 20:03.000
But once you, once you, the user is done with the entity, you can use the original model to get the final output.

20:03.000 --> 20:17.000
The idea is that you want to like to make sure the smaller models output is consistent with the big models output through this editing process.

20:17.000 --> 20:26.000
You will get, you learn a smaller model and you get something what you want, but, but once you press the button off, you're writing a big model.

20:26.000 --> 20:29.000
If you get something completely different.

20:29.000 --> 20:33.000
So this preview is not informative anymore.

20:33.000 --> 20:39.000
I will show you a demo as an animation in this video.

20:39.000 --> 20:50.000
So here is an import image right to project it and ought to do to we like to look, look and see it again.

20:50.000 --> 20:56.000
And we try to find the best lately called which going to produce this input image.

20:56.000 --> 21:06.000
And so we have what we have done that we can like move this code around maybe we find this smiling call smiling directions.

21:06.000 --> 21:14.240
all I might have already mentioned is how you can find the directions. You can find that,

21:14.240 --> 21:18.640
like you move here, maybe you make your face smiling and you move to a different direction,

21:18.640 --> 21:26.160
make your hair a different color. And then the idea is we can generate an image

21:27.760 --> 21:33.920
after you change the directions. And here is the original image.

21:36.320 --> 21:42.880
Here we used GANs to reconstruct this image, to represent this image. This is the image

21:42.880 --> 21:48.400
generated by GAN. This is the image of original image. And here are a few sliders.

21:51.840 --> 21:56.960
And we would like to modify this image in various attributes.

21:57.520 --> 22:08.480
And we'll run it on these CPUs, like just Intel CPUs. But if you click

22:08.480 --> 22:14.960
smiling, here we click smiling, it took about three seconds to produce image with smiling face.

22:17.040 --> 22:21.840
So you cannot drag the slider anymore, you can only click. And then you make younger or older.

22:22.080 --> 22:32.880
It's just very laggy, right? And the idea is you would like to, if you click something or drag

22:32.880 --> 22:41.360
the slider, it should get the results immediately. And here is our idea.

22:41.600 --> 22:51.120
And you would like to choose our monocleidic cost GANs. And you can click something.

22:51.680 --> 22:54.080
You only took 3.3, 3.4 seconds.

22:58.320 --> 23:02.400
That's very, so you get very fast interactive feedback.

23:02.960 --> 23:07.440
Okay, change it here.

23:12.000 --> 23:15.920
And once you download the editing, you can still learn your original model

23:15.920 --> 23:20.320
and finalize the output. So you get very fast

23:21.280 --> 23:28.080
and interactive feedback when you are editing the photo. And you get a high quality output

23:28.640 --> 23:30.240
where after you finalize your edits.

23:34.240 --> 23:35.280
Here's another example.

23:42.880 --> 23:44.960
I'll make this look older.

23:51.840 --> 23:56.320
And we'll try to remove the gossips because sometimes you mix some more, it will add gossips.

23:56.800 --> 23:59.040
So there's some correlation between these attributes.

24:00.320 --> 24:03.680
But it's very interactive. But once we finalize the edits, we can

24:03.680 --> 24:08.800
generate the final rendering, a very high quality, high resolution image.

24:14.800 --> 24:16.560
Here's the last example.

24:26.800 --> 24:36.960
Yeah, again, once you finalize the edits, you can click the button and learn the original model.

24:38.000 --> 24:39.280
Yeah, so here is the one.

24:42.720 --> 24:49.760
Yeah, so this is actually quite similar to how people is using regular, non-deplaning

24:50.720 --> 24:57.360
content creation software either in rendering software like Blender or Maya.

24:58.000 --> 25:04.480
You can choose to render an image with low quality preview.

25:05.600 --> 25:10.000
It's very easy to do it in rendering algorithm. You just sample field arrays,

25:10.000 --> 25:12.880
or maybe you have fewer bounces if you know the rendering algorithm.

25:14.320 --> 25:19.600
It's also in a bunch of Adobe software, you can also generate a preview

25:19.600 --> 25:24.640
of your videos and there are a bunch of ways to make rendering faster.

25:26.640 --> 25:31.840
So that's just for your preview. And once you are done with your editing, you can export model,

25:31.840 --> 25:37.920
export the export results to a much higher quality rendering, much higher quality

25:40.240 --> 25:45.360
just output results. So we're trying to separate the preview and the final rendering.

25:45.440 --> 25:48.160
We're trying to make sure the preview looks similar to final rendering.

25:49.040 --> 25:53.760
Otherwise, the preview does not provide enough information for editing.

25:57.280 --> 26:04.320
So far, what we have been talking about is how we can make it faster to run on CPU.

26:04.320 --> 26:08.320
For example, maybe we instead of three seconds, we can make sure it runs

26:08.320 --> 26:16.800
about three or four seconds. Maybe we can make sure it can run on a smartphone

26:18.080 --> 26:24.080
maybe within one second. Next, I would like to talk about data, which also prevents many of the

26:24.080 --> 26:32.080
users and the content creators to choose their own models. So the idea here is to choose these models.

26:32.400 --> 26:41.920
It requires lots of selects like choosing images, right? We have seven thousand faces,

26:41.920 --> 26:48.160
you need to be high quality faces, and people actually align the face, crop the face from

26:48.160 --> 26:54.560
the original photo and align them. The same idea here, if you choose a big game model,

26:55.520 --> 27:00.480
the model from DeepBand, the big game model requires an image net model,

27:01.200 --> 27:04.880
and it requires millions of images from some of the categories.

27:05.680 --> 27:11.680
Or you can choose a model like a bunch of people have seen the car model, the bedrooms model requires

27:11.680 --> 27:17.600
one or two million images to choose a car model, just to learn a model about cars and bedrooms.

27:18.240 --> 27:23.120
So it's not very easy. So if you have a new idea, oh, I would like to have a

27:24.160 --> 27:29.760
have a model of something. It's not like you can just train the model, like it takes a lot of

27:29.760 --> 27:34.960
time to collect the data right in the first step, before you even train the model.

27:35.840 --> 27:42.400
Okay, so it takes months or even years to collect the data set. Some training might

27:42.400 --> 27:48.080
require annotation if you train a big game model which condition all the class labels.

27:49.600 --> 27:55.680
So for the here, how about I just train a model about myself, about my collaborator here is

27:56.320 --> 28:02.480
actually professor Sohan, assistant professor at MIT. So often I just, I don't just try to

28:02.480 --> 28:08.720
try to train a model on his portraits, right? But of course, I don't have, the thing I don't have

28:08.720 --> 28:17.840
is I don't have 70,000 faces of professor Sohan. I don't, I probably don't have to allocate them,

28:18.560 --> 28:24.480
but maybe I have 100 faces, maybe I have 50, maybe I have 200, but there are no way I have a million

28:24.480 --> 28:34.320
faces. So first, I mean, maybe if you are like, celebrate your petition, maybe you have, maybe

28:34.880 --> 28:40.560
I'm not sure if you use that case, you have a million faces. But for your friend, for your family

28:40.560 --> 28:47.520
members, you probably have like 50, 100, so several hundred of them faces. And here,

28:48.160 --> 28:56.560
but the ideal case is we want to train a model of my, myself, my friends, my collaborators,

28:56.560 --> 29:02.880
but I don't have so many images. So I would like to, so you know, I hear what I would like to

29:02.880 --> 29:08.880
train a model and try to produce some new samples. And maybe I can use this model to edit Sohan's

29:08.880 --> 29:15.760
photo, right? Instead of using a generic face model, I would like to have a customized face model

29:16.480 --> 29:24.880
for face editing. But in reality, if you just train a model of Stalgant 2, we get very distorted

29:24.880 --> 29:30.560
images. If you would train a Stalgant 2 on the dataset, I just mentioned like hundreds of 50

29:30.960 --> 29:40.640
faces. And so there are the huge gap between, if you train a model with 50 faces versus you

29:40.640 --> 29:47.520
train a model with 70,000, a million faces. The gain is really requires a lot of data to get good

29:47.520 --> 29:56.320
performance. And this holds for a bunch of cases, not only for a professor. Sohan's face, but also

29:56.400 --> 30:03.360
for Obama's faces, if you train a model on 100 Obama images, it does not work very well. Okay.

30:03.920 --> 30:09.280
If you, let's say, maybe not, maybe we have a very high standard for face, right? Maybe for other

30:09.280 --> 30:16.880
objects, it's okay. That's not the case. If you train a model on cats, 160 cats, you get some

30:16.880 --> 30:22.880
distorted cats. How about your dog friend? You train a model on dogs, you still get distorted dogs,

30:22.880 --> 30:31.440
even you have 390 images. So that's highlights the issue is, if you want to use again,

30:32.480 --> 30:38.640
for your own purpose, for your own dataset, maybe for some paintings, you don't have, if you're

30:38.640 --> 30:45.120
training a model on paintings of a particular painter or artist, you cannot ask the artist to

30:45.120 --> 30:49.760
produce millions of paintings in the first place, right? You would like to train a model

30:50.480 --> 30:54.880
of 101,000 paintings of a particular style or a particular painter.

30:57.680 --> 31:04.480
So we can, we can do this kind of experimental control setting. So we can look at CIFA-10,

31:04.480 --> 31:12.640
which is a standard dataset for gains of competitiveness. We can measure it by FIT,

31:14.240 --> 31:17.120
which is the lower and the better, which measures the distance between

31:17.920 --> 31:23.120
the jerry hippies distribution versus the original training distribution. And if you

31:23.120 --> 31:28.240
train a model on FID, sorry, you train a model on CIFA-10, you get a very low FID

31:29.120 --> 31:35.520
for 100% training data, but you get a much higher FID if you reduce the data by five times.

31:36.560 --> 31:44.160
And if you only train a model on 10% of CIFA images, you get a much higher FID.

31:47.440 --> 31:52.240
That's why the gains have really rely on the number of images you are training set.

31:55.040 --> 32:01.360
But why is that? Why, why, what happened in the CIFA-36 case,

32:02.720 --> 32:08.160
if you only have 10% of the data, right? Something happens if you, if you, for any kind

32:08.160 --> 32:14.560
of opportunity model, if you don't have enough data, your model might start overfitting. In this case,

32:14.560 --> 32:23.040
we can look at the discriminator that's overfitting if you don't have enough data. So here are the

32:23.040 --> 32:30.400
cases, the discriminator, we can, we have two plots. We have discriminators training accuracy.

32:31.440 --> 32:41.360
Yes, how this is, how discriminator can classify real versus fake images for the training set

32:41.360 --> 32:48.880
the model has been trained on. We can look at the discriminator's accuracy, our holdout test,

32:48.880 --> 32:54.320
our variation training, our variation real, real images, which discriminator hasn't seen

32:55.120 --> 33:03.440
in the training set. So you can see that for 100% of data, the model, the discriminator starts over

33:03.440 --> 33:08.320
kind of classify the images more and more accurately. But that's not necessarily me,

33:09.040 --> 33:18.400
it can classify the test images as accurate as training images. So that's happens a lot if you

33:18.960 --> 33:25.360
just change the classifier, right? But the thing which makes this worse is if you only have 20%

33:25.360 --> 33:34.080
of training images, this kind of overfitting is much more severe. If you only have 20% of training

33:34.080 --> 33:44.400
data, the discriminator will classify the training images very accurately, very quickly,

33:45.760 --> 33:52.800
but it will, it cannot work for the test images anymore, the test real images, it will

33:53.600 --> 34:00.080
drop the percentage, right? Reminding this is a binary classification task, so the accuracy below

34:00.080 --> 34:10.960
0.5 is pretty bad. And if you have 10% of data, it's even worse. It can classify this 10% of

34:10.960 --> 34:17.760
data very quickly, real or fake, but it cannot classify any kind of test images very quickly.

34:17.760 --> 34:23.680
And if you look at this model, this is when the gains start collapsing, the models start generating

34:23.680 --> 34:32.720
lots of garbage images. So the issue here which we identify is that if you don't have enough images,

34:32.720 --> 34:39.920
or if you only have 100 images, it's very easy for discriminator to just simply memorize

34:40.720 --> 34:46.960
every single image of your training set. As a discriminator, it does not generalize well

34:47.680 --> 34:52.960
to other real images. So if you treat a generator with overfitting discriminator,

34:53.760 --> 34:58.960
of course, a generator cannot get all the signals about what makes Obama look like Obama,

34:59.680 --> 35:04.720
what makes Professor Sohan look like Professor Sohan, right? If you're discriminating overfitting.

35:06.160 --> 35:15.280
So one idea to come back, the overfitting in computer machine, machine is called data

35:15.280 --> 35:24.160
augmentation. It's for single real image, we can create multiple versions of this image.

35:26.560 --> 35:30.880
If I create 10 versions of images, I kind of increase my data by 10 times.

35:31.920 --> 35:37.280
Of course, this information is redundant, but it's better than this one version, right?

35:37.280 --> 35:44.160
So the idea is try to enlarge the data sets without collecting the new samples.

35:45.840 --> 35:50.160
As there are a bunch of things you can do, you can, for example, this cat, you can rotate the cat,

35:50.960 --> 35:58.560
you can flip the cat, you can maybe change the color a little bit, or you make, maybe you can

35:58.560 --> 36:05.600
translate the cat, like make it shift left, shift right, shift up, shift down, so you can,

36:05.600 --> 36:11.760
you can move the cat around. But in computer machine test, there's still a cat, right?

36:12.480 --> 36:18.240
So if you train a model to classify a cat and a dog, if you move the cat around and rotate the cat

36:18.240 --> 36:23.440
by 30 degrees, it's still a cat, right? It doesn't become a dog. So you also do the labels.

36:24.160 --> 36:28.800
So the idea is you don't want to change the label by what we request your data.

36:31.520 --> 36:37.280
Right? But how to, how can we apply this idea to GANs training?

36:37.520 --> 36:44.160
How can we stay augmentation for GANs to combat the overfitting issue when we train a model on very

36:44.160 --> 36:50.880
few images? So we have tried several ideas. And the first idea, which is very straightforward, is we

36:50.880 --> 37:04.320
can, we can just apply the transformation or augmentation on the real images, right? Just like

37:05.280 --> 37:13.280
we did for the training image net classifier. And we can, we can train a model on the augmented

37:13.280 --> 37:21.520
images. That's, so that's very straightforward. But the, but the thing we're following is if you

37:21.520 --> 37:31.520
train a model in this way, the GERA image will also has the effect of this transformation. If you,

37:31.520 --> 37:36.880
if you, your transformation, if you change the color or you translate image around,

37:38.000 --> 37:47.440
or if you crop some patches, your GAN will replicate these kind of artifacts, will mimic

37:47.440 --> 37:54.160
these artifacts because your GAN does not know what is the original image look like. If you only

37:54.160 --> 38:00.880
feed the translate, transform the images, augment the images, try to mimic the augmentation as well.

38:04.400 --> 38:10.880
Why is that? Why, why is not an issue in the classifier? Because if you train a classifier for

38:10.880 --> 38:18.960
cats versus dogs, your label of cat and dog is the output, right? You want to output a cat or dog,

38:18.960 --> 38:25.840
but this label is, is, is, is the same before after augmentation. So you can still produce the cat.

38:26.560 --> 38:31.920
But here the output of the generator is the image. If you change the output, your dataset,

38:32.480 --> 38:38.080
your, your output of your generator will change. So you see the difference, the difference is

38:38.080 --> 38:43.360
whether you augment the input or output. So here we had augmented output while in the classifier

38:43.840 --> 38:49.840
case, people try to augment the input image, why keep the output the same? So if you cannot just

38:50.480 --> 38:55.840
augment the output directly, it will mimic that augmentation. That's, we don't want to generate

38:55.840 --> 39:04.480
these kind of images, right? And so second idea is how about we augment both real and fake images?

39:05.760 --> 39:11.600
I hope we can cancel each other and we'll only do it for the discriminator training.

39:12.560 --> 39:18.640
So the generator is, the training is the same. And we still train it versus the generator of

39:18.640 --> 39:25.280
g of z. By discriminator training, we augment both the real images x and generate images g of z.

39:25.280 --> 39:33.120
So z is a latent code, we simple from this Gaussian distribution. So, but, but the one thing

39:33.120 --> 39:37.120
we found is since you are doing slightly different things for, for the discriminator training,

39:38.080 --> 39:42.320
in which case you augment the x and g of z, but if you don't augment the

39:43.520 --> 39:50.640
data for, for a generator, you will see a gap. That is your classifier discriminator works pretty

39:50.640 --> 39:59.360
well for the transform, for, for, for, for the augmented images, t, t is augmentation,

40:00.320 --> 40:06.080
but it does not work very well for the generating, for, for the images without augmentation,

40:06.080 --> 40:11.520
for the g of z, which is original images. Also, which is original generate images

40:12.160 --> 40:20.800
without augmentation. Okay. So there's a gap between the generator's objective

40:21.680 --> 40:27.280
and discriminator's objective. And we, we found it does not work very well in practice because

40:27.280 --> 40:33.200
of this gap. So our approach is called differential augmentation. The idea is we,

40:34.160 --> 40:41.360
we can augment the both the, the fake images and the real images and both treating and, and,

40:41.360 --> 40:47.680
sorry, in both generate training and discriminator training. And here I will call it differential

40:47.680 --> 40:55.440
because if you, if you augment the data here and if you want to get gradients from the discriminator

40:56.160 --> 41:02.960
to the generator, this transformation t needs to be differentiable. Otherwise, you will stop

41:02.960 --> 41:07.120
the gradients from the discriminator to the generator. So we implement a bunch of

41:07.760 --> 41:13.920
differential augmentations and apply it to here. So the single impermanence we apply

41:15.600 --> 41:21.520
color transformation, our transient image alone, we apply something kind of cut out or kind of like

41:21.520 --> 41:31.280
augmentation. There are three kinds of operations. And the idea is once we apply the augmentation,

41:31.280 --> 41:38.000
we can, we can back up against the gradients from the discriminator all the way to the generator,

41:38.000 --> 41:41.600
all the way to the generator. So that's how we can treat the generator.

41:43.440 --> 41:50.000
And here are some results. So here's the original FID with respect to different amount of training

41:50.000 --> 41:57.920
data. And here's our results. So we, we get slightly better for 100% training data, but we'll

41:57.920 --> 42:07.360
get much, much better if you only have 10% or 20% of training data. So this allows us to maybe

42:07.360 --> 42:14.800
instead of treating a model of 50,000 images, you only need 5,000 images. And here are more examples.

42:16.400 --> 42:23.120
Yes, yes, if you only have 20% of the training data for image net, our model achieves much better

42:23.120 --> 42:30.560
results compared to the baseline begin. We will use the same begin loss and same begin architecture,

42:30.560 --> 42:33.920
but just add this augmentation to produce the FID by half.

42:36.480 --> 42:41.760
And now we can try to generate some Obama or cats or dogs. And here are our results we can

42:42.480 --> 42:47.840
produce much higher quality results when you only have 100 images.

42:48.080 --> 42:56.720
And now we can try to generate professor's face. It's not perfect, but much better than,

42:56.720 --> 43:04.240
than, than a baseline. And only requires 100 faces of your friends, your family member or yourself.

43:06.720 --> 43:10.480
And compare with fun tuning methods. There are a bunch of methods which you treat a model on

43:11.360 --> 43:14.240
a large scale data set and a fine tune it on a smaller data set.

43:15.440 --> 43:22.320
And our method is, is, is comparable performance wise, even when we don't require these kind of

43:22.320 --> 43:32.480
protruding images. Here we compare with transfer again, which the idea is you treat a model on

43:32.480 --> 43:41.920
FFTQ faces and a fine tune it on one of our faces. And all methods are still slightly better than

43:41.920 --> 43:46.480
since there are results. But of course, you can combine the best of the two words, like you can

43:47.040 --> 43:53.200
get your model on FFTQ and fine tune it with our differentiable augmentation.

43:54.080 --> 44:00.240
And, and you can get slightly more better results, you combine these two methods, they are complementary.

44:00.960 --> 44:09.520
And here are one results we treat a model and we try to traverse latent, latent in the latent

44:09.520 --> 44:14.000
space for different kind of, you can treat a model for a particular person, you can treat a model

44:14.000 --> 44:21.200
for particular landmarks or cities or animals, just, just very download some images and, and, and

44:21.200 --> 44:27.360
just hundreds of images you can do the job. Yeah, anyway, so, so there are not so much take home

44:27.360 --> 44:33.200
messages, take home messages. If you only have your fortune again, do not forget

44:34.000 --> 44:36.960
the documentation, that's the message. Okay, thank you for your attention.

44:39.600 --> 44:46.960
Thank you so much, Junion. And it was such a great talk and a lot of interesting

44:47.600 --> 44:55.440
directions and things to think about. I'm wondering if students have questions.

45:05.360 --> 45:14.000
I would like to ask how some of the students can, you know, get these models and work with them and

45:14.080 --> 45:24.240
can you please explain if what are the steps for them to get working with these models and the code?

45:29.920 --> 45:32.960
I think you are me. Yeah, yes, yes, I try to

45:33.680 --> 45:39.120
crit my dual monitor setup. So, yeah, yeah, I see that all the code are available on the

45:39.120 --> 45:45.040
GitHub. I will send you the slides later. All the code are available on GitHub.

45:45.920 --> 45:50.240
We have, we have step by step instruction on how to learn the model on

45:51.360 --> 45:54.800
C file and how to learn the model on image data, how to learn the model on your own

45:55.840 --> 46:03.120
data, like if you just have an image directly of hundreds of images, you can, we have a command

46:03.360 --> 46:10.000
line and you can learn the model directly. It took like four hours to learn a model on like hundreds

46:10.000 --> 46:18.400
of, 100 photos for a lot of your offices. It should be pretty straightforward. Yeah, yeah.

46:19.440 --> 46:29.680
And then are there, are there tools that one could use for making some of these things more

46:29.680 --> 46:36.400
interactive and use as a, just at the beginning you describe how it is interesting for designers

46:37.040 --> 46:40.960
and, you know, art practitioners to use these models.

46:44.320 --> 46:51.120
Yeah, I, I think, I think David Bao, my, my will, will talk about it. We have a bunch of tools on

46:51.120 --> 46:58.480
visualizing and monitoring the internal, internal units of this model, if you would like to understand

46:58.480 --> 47:04.800
it better. I'll have, I think David has a bunch of online tools he will talk about maybe tomorrow

47:04.800 --> 47:14.880
or later. Yeah, I think in general, it's, it's also hard to use. It takes like four hours on 100

47:14.880 --> 47:24.080
photos. So if you have 20, 80 time GPUs, yeah, so it's much faster than you know, big game model

47:24.080 --> 47:31.520
for four months or several weeks. Yeah, I think we're also working on maybe faster training,

47:31.520 --> 47:36.000
we are still working on that. So hopefully you can reduce the training time to several minutes

47:36.000 --> 47:43.920
or maybe half an hour. So more people can, can, can use it, right. So oftentimes you may not have

47:43.920 --> 47:49.200
the GPU resources, you may not have so many images. So we are working on that and try to,

47:50.160 --> 47:54.560
yeah, but it's also good for us because we also have limited resources compared to big companies.

47:55.680 --> 48:00.480
It's not like you're MIT and you have lots of resources, but compared to big companies,

48:00.480 --> 48:07.280
we don't have so much resources in academia. And then also David is asking, how long does it take

48:07.280 --> 48:17.840
to compress a model? Yeah, compress a model, it takes as long as much as time as training a model,

48:17.920 --> 48:25.120
maybe, maybe, maybe 50 times more. It's slower because you want to,

48:27.360 --> 48:32.320
you are training up all kinds of configuration at the same time, so it takes longer to train.

48:32.320 --> 48:35.680
Maybe it takes 50% more time. And while we are working on

48:36.880 --> 48:44.240
improvement of that, try to make the training faster. And it works for a bunch of models as well.

48:44.560 --> 48:52.400
Yeah, the compressing model is slower because the idea is you can, you have a bunch of

48:52.400 --> 48:55.760
models with different configurations. They all work pretty well. So you can use

48:56.560 --> 49:00.960
configuration A for your CPU, you can use a different configuration for your,

49:02.160 --> 49:08.560
for your mobile device. So you can have different models for different devices.

49:08.560 --> 49:12.000
Once you train the model, you can, you have this kind of flexibility.

49:12.640 --> 49:17.120
The idea is once you train the model, you only need to train once and you can deploy to 10 or

49:17.120 --> 49:23.360
20 devices. That's, that's quite essential for critical, for practical purpose because

49:23.360 --> 49:28.400
if you develop a product, you would like, you don't want to compress a model for

49:29.760 --> 49:33.840
iPhone. And then next time you'll compress for older iPhone or Pixel to a Pixel 3, right?

49:33.840 --> 49:41.520
So you would like to have this kind of versatile ability to train once and

49:42.000 --> 49:49.440
deploy to multiple devices. Yeah. I think that this is very important because

49:51.040 --> 49:58.080
you can put the time for the developer and then when the user wants to use it,

49:59.040 --> 50:07.520
the hope is that they spend very much less time to get what they want, which is a very good idea.

50:08.080 --> 50:10.560
Thank you so much, Junion. I appreciate that.

50:13.680 --> 50:16.480
It was such a pleasure for us.

