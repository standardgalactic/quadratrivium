1
00:00:00,000 --> 00:00:07,840
Hello everyone, welcome to your course, AI for Art, Aesthetics and Creativity.

2
00:00:07,840 --> 00:00:18,880
Today we have a very special speaker, Prof. from OpenAI, and he's going to talk about

3
00:00:18,880 --> 00:00:29,760
creating art and artistic work and images in general, these diffusion models and probably

4
00:00:29,760 --> 00:00:37,240
you have already worked with the glide collab, so he's going to walk us through that as well.

5
00:00:37,240 --> 00:00:45,880
So let's get us started, Prof. I always ask if please you can share with us what motivates

6
00:00:45,880 --> 00:00:51,880
you working in this space and also giving us a little background about yourself.

7
00:00:51,880 --> 00:00:57,880
For sure, thanks for having me here today, by the way this is really exciting.

8
00:00:57,880 --> 00:01:07,880
Yeah, so a background about me, I was an undergrad at MIT in computer science and math and then after that,

9
00:01:07,880 --> 00:01:13,880
I came to OpenAI to do AI research and I've been here for five years doing research on unsupervised

10
00:01:13,880 --> 00:01:21,880
learning, generative models, all kinds of things and what motivates me to do this research.

11
00:01:21,880 --> 00:01:31,880
When I was in college, I was excited by the idea of trying to understand what makes humans intelligent

12
00:01:31,880 --> 00:01:39,880
and I think I attended a few talks, which were really amazing and I felt like there's a lot of

13
00:01:39,880 --> 00:01:43,880
amazing progress happening in this field and I just wanted to part of it, see what's happening, see what

14
00:01:43,880 --> 00:01:49,880
I could contribute and then one thing led to another and here I am.

15
00:01:49,880 --> 00:01:59,880
I think so far, wouldn't say we are very close to unraveling what makes humans intelligent,

16
00:01:59,880 --> 00:02:07,880
but we've made a lot of progress I think in these years, so it's been pretty fun.

17
00:02:07,880 --> 00:02:13,880
Cool then, so I'll just get started.

18
00:02:13,880 --> 00:02:19,880
And anyone, feel free to just ask any question at any point, pause me if anything feels confusing,

19
00:02:19,880 --> 00:02:27,880
if any notation is understood. I don't see the chat window on my screen directly, so if you could just

20
00:02:27,880 --> 00:02:33,880
directly tell your question, that would be easier or if Ali, if you see something in the chat, just let me know.

21
00:02:33,880 --> 00:02:37,880
Cool, I'll get started.

22
00:02:37,880 --> 00:02:45,880
So I'll begin by just showing a few examples of very powerful creative ML models from the past few years.

23
00:02:45,880 --> 00:02:53,880
The first one you all might have already seen in news GP3, the language model from OpenAI.

24
00:02:53,880 --> 00:02:59,880
And one example I'm showcasing here is like these language models.

25
00:02:59,880 --> 00:03:10,880
They show like a very few examples of something pretty simple like here, like on the left you see examples of like poems by specific black writers.

26
00:03:10,880 --> 00:03:16,880
And on the right, then you can see once the model has seen examples of this kind, what it can generate.

27
00:03:16,880 --> 00:03:22,880
And it's getting poetry from just a few examples. This is pretty crazy.

28
00:03:22,880 --> 00:03:25,880
There's a second model I'll show you next.

29
00:03:25,880 --> 00:03:28,880
This is the audio playing.

30
00:03:31,880 --> 00:03:34,880
Yeah, we can hear your audio.

31
00:03:56,880 --> 00:04:00,880
You may bring it, and that's it.

32
00:04:00,880 --> 00:04:25,880
Anyhow, so that was a sample from a model for generating music called jukebox.

33
00:04:25,880 --> 00:04:33,880
And everything here was generated from the model, the music, the singing, how to sing it, how to pronounce the lyrics and everything.

34
00:04:33,880 --> 00:04:38,880
All the model was given was the lyrics and an artist and it produced all of this by itself.

35
00:04:38,880 --> 00:04:45,880
Does the generative model like produce the music and like the sentence separately or together?

36
00:04:45,880 --> 00:04:54,880
Everything together. Yeah, so because kind of how you sing something kind of has to go with the music that's provided by Swasa, right?

37
00:04:54,880 --> 00:04:59,880
It's kind of hard to like generate them separately from each other.

38
00:04:59,880 --> 00:05:01,880
What was that your question?

39
00:05:01,880 --> 00:05:07,880
So I can read from the chat that people are getting excited.

40
00:05:07,880 --> 00:05:13,880
Someone has a loud and other is saying this is freaking awesome.

41
00:05:13,880 --> 00:05:16,880
Thank you.

42
00:05:16,880 --> 00:05:21,880
Share or talk if you like.

43
00:05:21,880 --> 00:05:23,880
Oh yeah.

44
00:05:23,880 --> 00:05:28,880
Sorry, I can't see the chat window so feel free to just talk.

45
00:05:28,880 --> 00:05:33,880
Yeah, when we heard samples from like that from these models we were also amazed.

46
00:05:33,880 --> 00:05:47,880
The model I have here is the slide model you guys might have seen in papers. And here you have a model that given a text prompt is generating a visual representation of it or whatever it imagines what that the text kind of signifies.

47
00:05:47,880 --> 00:06:02,880
So you could see, and these are things that are go back to your point Ali you made earlier about composition. These things involve a model really having to compose a lot of different concepts together like robots meditating in a way past the retreat, but it is able to imagine this.

48
00:06:02,880 --> 00:06:17,880
In the last few years I think like ML models for such very hard creative tasks have become really good. And today we'll see like what are some of the kind of concepts driving this progress.

49
00:06:17,880 --> 00:06:33,880
And so before I even start down that route, like why are we trying to, you know, from a research perspective like trying to train models that you know create things. Well, one concept here is that, you know, this as this code by Feynman says what I cannot create I cannot.

50
00:06:33,880 --> 00:06:52,880
I don't understand training models that can, you know, create things be images or your video and so on. It's kind of one of the hardest tasks in those domains. And if you really care about whether models can understand images audio video so on, then one of the best ways to know

51
00:06:52,880 --> 00:07:01,880
if you're making progress out of these models are really learning something advanced is to see if they can create really complex things and really hard to understand things.

52
00:07:01,880 --> 00:07:10,880
And for people who care about representation learning or something this is one, one way you can know you're making progress on such stuff.

53
00:07:10,880 --> 00:07:25,880
And there has been a lot of progress in this field of, you know, trying to create things from models or what we call generative modeling. So here you see just in this very small domain of phase generation, things that GANs could create in 2014, versus things that they can create in

54
00:07:25,880 --> 00:07:32,880
2018, like, it's absolutely astounding how much progress has happened in the past few years.

55
00:07:32,880 --> 00:07:54,880
So, what is a generative model. So, you can think of what our inputs here in our data set to look like just a collection of examples, x1, y1, x2, y2, xn, yn, where x here represents, let's say an image and why some label or some other information describing this image.

56
00:07:54,880 --> 00:08:02,880
So you just have the sample from some natural distribution of images, p of x comma y.

57
00:08:02,880 --> 00:08:16,880
So you could have like images of corgis, ostriches, goldfishes and so on. You want to train a model that can learn this distribution you want to train a model that then ask for a corgi produces a corgi and ask for an ostrich produces an ostrich, or so on.

58
00:08:16,880 --> 00:08:28,880
You want to learn p of x given y, given some label y, corgi, ostrich, goldfish and so on. Can I generate a real image or an image that looks real from this distribution.

59
00:08:28,880 --> 00:08:34,880
And one such a model is trained, you can use it to generate novel samples.

60
00:08:34,880 --> 00:08:44,880
So you can generate corgis, ostrich, goldfishes that are actually real haven't been seen before but look like real images.

61
00:08:44,880 --> 00:08:54,880
One of the things I guess that matters is, is how you evaluate such models because if you if you don't have evaluation metric you can't tell you're making progress.

62
00:08:54,880 --> 00:09:07,880
And we won't go into too much detail here about these metrics but one of the metrics use was FID for measuring image generation and what these metrics are trying to capture is like fidelity and diversity.

63
00:09:08,880 --> 00:09:19,880
Fidelity would mean like how realistic or how correct an image looks versus diversity would be like how many different kinds of images such a model can generate.

64
00:09:19,880 --> 00:09:34,880
And so GANs were kind of like the state of the art for difficult image generation benchmarks before diffusion models came along which we will not talk about in our talk today.

65
00:09:34,880 --> 00:09:44,880
The progress in diffusion models has been pretty recent, it's been just like the last couple of years there's been a lot of papers and even in these people that you could see like things have been improving since 2020.

66
00:09:44,880 --> 00:09:52,880
And you could now generate realistic faces, you know, lots of different categories of images from ImageNet and so on from these models.

67
00:09:52,880 --> 00:10:08,880
So it's a pretty exciting field and these models in one of our recent papers we showed them to be actually better than GANs at generating images and so I'm pretty excited by these models and that's what I'm going to cover today.

68
00:10:08,880 --> 00:10:16,880
There's quick graphic here for like how things look like when diffusion models generate an image.

69
00:10:16,880 --> 00:10:19,880
Let me just play it again.

70
00:10:19,880 --> 00:10:24,880
So let's go to what these models are.

71
00:10:24,880 --> 00:10:36,880
As you saw in that graphic, you could see that that image was started out looking like noise and you finally got a real image out by like the slow process of noise converting to an image.

72
00:10:36,880 --> 00:10:51,880
And what is actually happening behind the scenes here is you have a fixed process that adds noise to a training image. So let's say you start with X0 on the left here as an image.

73
00:10:51,880 --> 00:10:54,880
So that's just a dog ball on the left.

74
00:10:54,880 --> 00:11:10,880
So a fixed process that slowly adds Gaussian noise to this image. So at each step you add a little amount of Gaussian noise. And as you go from left to right by the end, at the last time step B, you have just pure noise left.

75
00:11:10,880 --> 00:11:22,880
And what the model is trying to learn is to undo this process. It's trying to reverse it. It's trying to take some noise damage and be noisy a little bit, make it a little less noisy and so on.

76
00:11:22,880 --> 00:11:35,880
How do you obtain a generative model out of this? Well, if you train a model to reverse noise like this, then at test time when you actually want to use the model, you could start with pure noise at the end.

77
00:11:35,880 --> 00:11:44,880
You could start with XT. Then you could run it step by step backwards to remove noise from it and try to produce an image from it.

78
00:11:44,880 --> 00:11:51,880
Any questions on this diagram?

79
00:11:52,880 --> 00:12:00,880
Okay, I'll just check the chat to throw the question there.

80
00:12:00,880 --> 00:12:04,880
Okay, no questions.

81
00:12:04,880 --> 00:12:21,880
Okay, so let's remember the notation from your X0 is an example from the dataset, XT, capital T, XT is noise and there's intermediate steps, X little t's, we call them, where you have like some slightly

82
00:12:22,880 --> 00:12:38,880
noise image. And we can, to introduce more notation here, you could represent one step of the forward noising process with a distribution Q of X little t, given X little t minus 1.

83
00:12:38,880 --> 00:12:50,880
And right now, I'm going to use Gaussian noise as a noising process. So we're going to add a little bit of Gaussian noise to this.

84
00:12:50,880 --> 00:13:05,880
And the thing about here is this in this notation, there's some mean, which is centered around the original noise damage XT minus one, and there's some variance one minus alpha T here of how much noise that is being added to this image.

85
00:13:05,880 --> 00:13:16,880
So this is the forward process. This is the process that adds noise to images Q of XT given XT minus one. What we are learning is the reverse thing. We want to denoise this image.

86
00:13:16,880 --> 00:13:24,880
So we were learning P of XT minus one given XT. And what you can show is for very small

87
00:13:26,880 --> 00:13:35,880
noising steps where the amount of noise added is very tiny. The reverse process kind of looks like the forward process.

88
00:13:35,880 --> 00:13:44,880
This is kind of a set of points. Maybe I'll go in a little bit of an example here. Let's say, looking at a single pixel here, it had some real value on the real line, let's say

89
00:13:46,880 --> 00:13:56,880
0.8. And then you added a tiny amount of Gaussian noise to that thing. So it became 0.81 or 0.79 or so, depending on what noise you sampled.

90
00:13:56,880 --> 00:14:05,880
Now you're given this new value 0.79. This is the noise value. And you want to predict the distribution of what could have been the value it came from.

91
00:14:05,880 --> 00:14:16,880
If the amount of noise added, that you added in your step was very tiny amount of Gaussian noise, then the reverse prediction also looks kind of like a Gaussian.

92
00:14:17,880 --> 00:14:27,880
So it looks like, oh, it's somewhere around 0.79, some distribution that where you came from. I mean, 0.8 is pretty close to 0.79 in this situation.

93
00:14:28,880 --> 00:14:41,880
And so you could write that down as a model that is predicting the mean of this reverse process and the variance of this reverse process, mu theta being the mean and sigma theta here being the variance that you're trying to predict.

94
00:14:42,880 --> 00:14:48,880
So far, so good. Any questions at this point?

95
00:14:55,880 --> 00:15:10,880
Okay, so to summarize the notation again, XT minus one to XT is a process that's adding noise and the process we are trying to learn is XT to XT minus one to reverse this noise.

96
00:15:10,880 --> 00:15:18,880
And this looks like a Gaussian in the forward direction and predicting a new Gaussian in the reverse direction.

97
00:15:20,880 --> 00:15:35,880
A paper showed you could do some forms of training tricks to make this process simpler. You don't have to add noise little by little at every step. You could just directly sample an intermediate XT given your data example by just adding a lot of Gaussian noise to it.

98
00:15:36,880 --> 00:15:44,880
And it also showed instead of trying to predict the mean of the reverse process, you could just directly try to predict what noise was added to the image.

99
00:15:46,880 --> 00:15:50,880
This is possible because you could write the mean in terms of the noise that was added.

100
00:15:51,880 --> 00:16:11,880
Trying to predict the variance can be just simplified to just using a fixed variance or a learnt variance. We won't go into that today. All you should get from this is that to predict the reverse process where you are trying to predict the mean and the variance of the Gaussian to reverse that

101
00:16:11,880 --> 00:16:16,880
noising process, it's enough to try to predict what noise was added to the image.

102
00:16:18,880 --> 00:16:33,880
So how does this look like when you are actually training these borders? You take an image X0, you sample some random noise, you sample some Gaussian noise and you just combine these two to produce a noised XT.

103
00:16:34,880 --> 00:16:37,880
There's a formula here of how we've combined them.

104
00:16:38,880 --> 00:16:54,880
You can think of this combination as something that is kind of like interpolating between the image and the noise. So at t close to zero, you should just get the image X0 and t close to capital T, you should get complete noise.

105
00:16:55,880 --> 00:17:05,880
And this kind of interpolation factor, alpha t bar, kind of plotted it here, it goes from like one near t equals zero to zero near t equals capital T.

106
00:17:06,880 --> 00:17:11,880
This kind of controls how you interpolate between a fully denoised image versus a fully noised thing.

107
00:17:12,880 --> 00:17:19,880
And at training time, you're just sampling all possible combinations of mixing of noise and image and you're trying to denoise all these combinations.

108
00:17:20,880 --> 00:17:31,880
So what is the model trying to do now? It's trying to predict, as we said in the earlier slide, it's trying to predict what noise was added into the image.

109
00:17:32,880 --> 00:17:47,880
So you take in the noised XT, you take in what time step or kind of like an indication of where in the process you are, you tell the network, hey, I am at this step in the process, this is my noised image.

110
00:17:47,880 --> 00:17:51,880
What noise was possibly added to this image? So it's trying to predict epsilon.

111
00:17:52,880 --> 00:18:09,880
And what it's being used to train with is just like simple L2 loss, like just take the mean squared error of the difference between the network's prediction and the actual noise that you trained with and you try to minimise this loss so that you can train the network to predict what noise was added into the image.

112
00:18:10,880 --> 00:18:20,880
So intuitively, you can think of this as like, well, if I'm given a noised image, and if I can predict what noise was added to it, I can kind of like subtract that noise out, right, try to get a real image out.

113
00:18:21,880 --> 00:18:26,880
And this is kind of what is happening when you're training a diffusion model. It's learning to denoise images.

114
00:18:28,880 --> 00:18:29,880
Any questions so far?

115
00:18:29,880 --> 00:18:37,880
No questions.

116
00:18:38,880 --> 00:18:39,880
Okay.

117
00:18:41,880 --> 00:18:45,880
So what does the model that kind of does this denoising usually look like?

118
00:18:46,880 --> 00:19:05,880
Kind of models that we have in our papers usually look like these convolutional unit style models where the U kind of signifies kind of like how the shape of the model here in this picture is looking like, but to think of it as just like a model that runs a bunch of

119
00:19:06,880 --> 00:19:23,880
convolutional images, it kind of like down samples the image down into smaller and smaller spatial fields so that it can like learn features at different levels of granularity and then kind of samples it back into something that looks like a prediction of a noise.

120
00:19:24,880 --> 00:19:33,880
I don't have to go into the details of architectures, but just to give you an example of what kind of neural nets are trained to perform this task, this is how they look like.

121
00:19:35,880 --> 00:19:45,880
So, okay, we have a model that is trying to now denoise images, it's trying to predict the noise that was added to an image. How do we go back to actually getting a generative model out of this?

122
00:19:46,880 --> 00:20:00,880
As we talked about in an earlier slide, it's equivalent to predicting the mean of the reverse process, like you can write down the mean of the reverse process in terms of the noise prediction.

123
00:20:00,880 --> 00:20:09,880
So now that you have a network that can predict the noise that was added, you can also write down a network that can predict the mean of the reverse process.

124
00:20:09,880 --> 00:20:15,880
And once you have something that can tell you the mean of the reverse process, you can run the reverse process backwards.

125
00:20:15,880 --> 00:20:29,880
So you can start with noise x t, you could run, you could sample from this reverse process p of x t minus one given x t, you know the mean, and we have fixed the variance to something to do one step of sampling from this process.

126
00:20:30,880 --> 00:20:39,880
You do one more step, one more step and so on. And as we talked about what we're doing was denoising, right, we're trying to like learn a process that removes noise from images.

127
00:20:39,880 --> 00:20:46,880
So if you start from pure noise and you're denoising it one step at a time, by the end of the process, you would have something that looks like a real image.

128
00:20:48,880 --> 00:20:56,880
So, so what we've covered so far is how you train these models and how you sample from these models. Any questions so far?

129
00:21:00,880 --> 00:21:04,880
Okay.

130
00:21:04,880 --> 00:21:08,880
Um, it's okay.

131
00:21:08,880 --> 00:21:17,880
A bunch of theory there, but what you should remember is you train for denoising, and you can derive a sampler from it, once you've trained for denoising.

132
00:21:17,880 --> 00:21:23,880
What do you do next? Well, you could now make the model class conditional, you could provide labels at training time.

133
00:21:23,880 --> 00:21:33,880
So you could provide, you know, let's say you're training on ImageNet or something, you could have labels that say this image is a goldfincher, this image is a Corgi or so on.

134
00:21:33,880 --> 00:21:52,880
And you could make the model, the denoising model class conditional, you could provide these labels, the model so that, given this label, it tries to produce an image from p of x given y, like the distribution of images that are kind of represented by this label.

135
00:21:52,880 --> 00:22:03,880
And it's pretty simple, you just throw in a label into the model at some point, so that it now has this extra information when it's trying to denoise images.

136
00:22:03,880 --> 00:22:14,880
You could also do something like up sampling, you could ask the model, given this low resolution image, what would be kind of the high resolution image that could be generated from this.

137
00:22:14,880 --> 00:22:27,880
So again, just like throwing in a label y, you can throw in a low resolution image as extra conditioning information into the model as it tries to denoise.

138
00:22:27,880 --> 00:22:43,880
So, we've now talked about models that are class conditional. The thing is, if you just train models like this, where you give them a label for an image and you train them for producing the image given the label, they're not very good at doing this out of the box.

139
00:22:43,880 --> 00:22:58,880
They kind of produce very incoherent samples. And one of the tricks that we developed to kind of fix this was the trick of guidance, where what you do is you train a model to look at the images that are being generated.

140
00:22:58,880 --> 00:23:11,880
Use a classifier to classify what is the label of this image. So you kind of look at a noisy image and you're like, you know, whether you ask the model, hey, does this look like a dog or not.

141
00:23:11,880 --> 00:23:24,880
So you train a classifier on these noisy images, then you take a gradient of the classifier, you ask the classifier, hey, how can I increase the likelihood to make this image look like a dog.

142
00:23:24,880 --> 00:23:37,880
Because you can run the classifier forward, you can get a, get a probability from the classifier of it being a dog, you can also differentiate this function to get the gradient of how to change the image so that this probability increases.

143
00:23:37,880 --> 00:23:49,880
And then you augment your diffusion model with such a classifier to kind of guide it towards generating images that are more likely to be classified as a dog by the classifier.

144
00:23:49,880 --> 00:24:05,880
So how do we end up doing this in practice. Okay, so you can train a classifier on noisy images, you can just take your data set of images, noise them and train a classifier to predict the label of the noisy images.

145
00:24:05,880 --> 00:24:23,880
And then how do you guide now your generative diffusion model to use this classifier. Well, you run the classifier on the noisy images, you predict the probability of, you know, the class label under the classifier so whether something is a dog or not.

146
00:24:23,880 --> 00:24:41,880
You take the gradient of this prediction to obtain kind of direction for which the model should change its input to increase the probability of this image, you can add this direction into the mean of the reverse process that you are already going towards.

147
00:24:41,880 --> 00:24:59,880
So in terms of the actual formula, it just looks like adding an extra term to your mean prediction, which is the gradient of the log probability of the prediction of the label, given the noisy image.

148
00:24:59,880 --> 00:25:06,880
Questions on this, this is, this is important and this could be a little complicated.

149
00:25:06,880 --> 00:25:13,880
It seems there are two questions before this. Can you read them.

150
00:25:13,880 --> 00:25:16,880
Do you still need a classifier.

151
00:25:16,880 --> 00:25:21,880
Once the model is trained.

152
00:25:21,880 --> 00:25:28,880
So here by the model you mean the diffusion model, right.

153
00:25:28,880 --> 00:25:45,880
Yes, you still need it, because it is part of the sampling process, you're using gradients directly from the classifier in the sampling process, so you still have to keep the classifier around when you sample from the model.

154
00:25:45,880 --> 00:26:01,880
So if you look at the next question is the underlying representation of the classes for condition.

155
00:26:01,880 --> 00:26:19,880
I followed the question, Ben, could you explain.

156
00:26:19,880 --> 00:26:21,880
Okay.

157
00:26:21,880 --> 00:26:23,880
I have another question.

158
00:26:23,880 --> 00:26:28,880
The s in the term. Is that just a hyper parameter or

159
00:26:28,880 --> 00:26:35,880
we'll get to that term in the next slide but yes that is just a hyper parameter.

160
00:26:35,880 --> 00:26:52,880
The main thing from this slide is like we previously had a reverse process that looked like a Gaussian with some mean mu and some variance sigma. We now have a modified reverse process where we've just modified the mean mu with an extra term, which is scaled by this

161
00:26:52,880 --> 00:27:04,880
parameter s has the variance in it kind of for appropriate scaling as well, and it has this gradient term, which is the gradient of a classifier on noise damages.

162
00:27:04,880 --> 00:27:21,880
And we're kind of basically using this gradient to kind of guide the model towards directions where the classifier would predict a higher likelihood of the label being correct for the noise damage, so that the conditional model produces an image that is more correct.

163
00:27:22,880 --> 00:27:29,880
Another question. Why is the variance in the additional term also included.

164
00:27:29,880 --> 00:27:33,880
That's just how it popped out from the derivation.

165
00:27:33,880 --> 00:27:35,880
Oh, okay.

166
00:27:35,880 --> 00:27:52,880
I guess you can think of it as kind of like the step size of these things is controlled by the variance if you have a Gaussian with a very small mean in the reverse process and you don't want to take a really large step with your gradient, because you'll really, you'll pop out very far from

167
00:27:52,880 --> 00:28:06,880
the reverse process should have taken you to the reverse process is taking really small steps, which kind of can be thought about by its variance, and you also want to change that process only by that much amount.

168
00:28:06,880 --> 00:28:07,880
Does that make sense.

169
00:28:08,880 --> 00:28:13,880
So, I guess the variance terms like a cap in step size.

170
00:28:13,880 --> 00:28:22,880
Right, like to the maximum and the class very gradient is maybe somewhere between zero and one or something like that or

171
00:28:22,880 --> 00:28:30,880
I don't know if it has any explicit range here, but I mean it's kind of mostly just direction.

172
00:28:30,880 --> 00:28:34,880
Yeah, and you're kind of scaling this direction by a step size.

173
00:28:34,880 --> 00:28:45,880
And the extra hyper parameter is if you want to kind of like make these steps bigger or smaller than what is naturally there so that's the extra parameter s that we'll talk about in the next slide.

174
00:28:45,880 --> 00:28:47,880
Okay, thank you.

175
00:28:47,880 --> 00:28:55,880
Yeah. Okay, so the parameter s here. So what we found was if you just actually use the step size that pops out from the derivation.

176
00:28:55,880 --> 00:29:04,880
So as being one, no, no hyper parameter, it kind of doesn't do that much. So on the left you see the samples with as being one.

177
00:29:04,880 --> 00:29:11,880
They don't look like any image from any particular class, but it turned out when we added this extra hyper parameter and just bumped it up.

178
00:29:11,880 --> 00:29:19,880
So you have scale 10 here on the right, they actually start looking like samples from the distribution of a particular class.

179
00:29:19,880 --> 00:29:34,880
So you can think of this extra hyper parameter s as kind of helping the model focus on the modes of the distribution because you're kind of narrowing down the possible things that it produces, at least that's what we saw empirically.

180
00:29:34,880 --> 00:29:44,880
However, the trade off here is because you're narrowing it down well, they also kind of start looking similar to each other, the images that are produced.

181
00:29:44,880 --> 00:29:53,880
But anyhow, the way to think about the scale factor here is just that it's controlling how much guidance we're using how much is the classifier influencing the final outcome.

182
00:29:53,880 --> 00:30:05,880
And when you use a small value for us, it's not influencing that much when you use a large value it's influencing a lot, and the effect of their influences, you're kind of collapsing your distribution towards the modes of whatever the

183
00:30:05,880 --> 00:30:20,880
table that thinks is kind of the best representation of that label, they were very high scale will just collapse to the thing that the classifier is most likely to classifiers that they just not always what you want you want some kind of diversity, what you produce.

184
00:30:20,880 --> 00:30:27,880
There's some, there's some like intermediate value of scale that is kind of the best that you want to use.

185
00:30:27,880 --> 00:30:36,880
And this is kind of how the process looks like in practice here on the bottom you here you have a usual like diffusion process with scale.

186
00:30:36,880 --> 00:30:51,880
Let me see I can't see the image. So scale zero you're just using no guidance, and then when you turn on guidance, and now using the gradients of the classifier can nudge the process in the direction where it's more likely to produce that butterfly.

187
00:30:51,880 --> 00:31:02,880
So the scale up even higher you're nudging it even further out from its original reverse trajectory into this new trajectory, where it's now producing a very clear butterfly.

188
00:31:02,880 --> 00:31:16,880
So the scale parameter is kind of controlling how much guidance is happening and how much the model is being nudged out from its original distribution towards this new better distribution.

189
00:31:16,880 --> 00:31:33,880
So similar things, instead of labels you could now have text descriptions of images. So, same model class you're still conditioning on something, but this conditioning thing why instead of being a label is now a piece of text that's a robot's

190
00:31:33,880 --> 00:31:36,880
kind of a pass now retreat.

191
00:31:36,880 --> 00:31:48,880
And you could train basically the same kind of models, all you have to now change as well you don't have a classified now right, there's no classifier that is predicting a label you're that you have to predict the whole sentence, if you try to do that.

192
00:31:48,880 --> 00:31:53,880
So how can we do guidance in this situation.

193
00:31:53,880 --> 00:31:56,880
Well, first, let me go in.

194
00:31:57,880 --> 00:32:14,880
Oh, okay. Well, first let me go into how you can even pass in conditioning information to diffusion models, which look like text, you just, you can just simply run a transformer on the text and just attend to the representations of the text in the model.

195
00:32:14,880 --> 00:32:35,880
What's important is just a pictorial representation of how to deal with text being passed into these models, you can just run a transformer model on this and just have your original convolutional unit architecture attend to this model when it's trying to do the denoising.

196
00:32:35,880 --> 00:32:44,880
Back to guidance, how do you actually guide when you have text as the kind of label information. And one of the things you could do is use clip.

197
00:32:44,880 --> 00:32:51,880
I think you guys cover clip in a previous lecture Ali you said that so can.

198
00:32:51,880 --> 00:32:54,880
Yes, sounds sounds good.

199
00:32:54,880 --> 00:32:57,880
Yeah, so I'll skip clip. Okay.

200
00:32:57,880 --> 00:33:09,880
So I'm assuming you guys on the clip, but basically clip is a model where you have an image encoder and a text encoder, and it's trying to predict how close the representations of the image and the text.

201
00:33:09,880 --> 00:33:22,880
And so you can use clip for guidance, you can ask, hey, I have this noise damage, I have this text description, run the image encoder run the text encoder from clip, how close are these representations.

202
00:33:22,880 --> 00:33:33,880
If they're close, then you're going to get a high dot product here, you can take a gradient of this dot product and get a direction to increase this dot product.

203
00:33:33,880 --> 00:33:49,880
And that's the gradient you're going to use for guidance, gonna ask the model hey can you increase this dot product so the image, the not the image that you're trying to produce from the reverse process is close in representations to the representation of the text that you're provided.

204
00:33:49,880 --> 00:33:54,880
So this is how clip guidance works.

205
00:33:54,880 --> 00:34:01,880
What you can do, which we sure enough, which was short in a paper on classifier free guidance is you could skip the classifier completely.

206
00:34:01,880 --> 00:34:14,880
And just train a usual diffusion model for for reversing the process but train it sometimes without labels. So sometimes don't, don't tell it what was the text that described an image.

207
00:34:14,880 --> 00:34:22,880
And then at test time you ask the model, which direction should it go, given the label and which direction should it go without the label.

208
00:34:22,880 --> 00:34:43,880
And then you move your predictions in the direction of the model predictions when it was given the label so in the formula here if epsilon was the epsilon theta xt given why was the prediction of the model with the label and epsilon theta xt given

209
00:34:43,880 --> 00:34:53,880
the empty set five was its prediction without the labels, you're kind of taking the difference of these two and using that as your direction to kind of nudge the model in.

210
00:34:53,880 --> 00:35:00,880
And again, you have the scale factor s outside of this direction, which telling the model to move in the direction of the predictions with the label.

211
00:35:00,880 --> 00:35:06,880
And when you use as greater than one, you'll be moving a lot more in the direction of the predictions with the label.

212
00:35:06,880 --> 00:35:17,880
And the cool thing about this way of guiding is that you don't need a separate classifier or a clip model or anything, you're just using the diffusion model itself for guidance.

213
00:35:17,880 --> 00:35:29,880
You're directly just asking the model, it's own kind of prediction of which way it should go to increase the probability of the generated image being from the correct class.

214
00:35:29,880 --> 00:35:35,880
Any questions about classifier free guidance.

215
00:35:35,880 --> 00:35:54,880
What I have is, have you thought about implementing something like, okay, at this, at each stage, for instance, let's talk about the butterfly example at this stage I want to add something to this image.

216
00:35:54,880 --> 00:36:22,880
And so the text, you know, can gradually form the shape like for the image like, okay, I want the butterfly and then on top of it I want this flower and then this, you know, gradually giving more idea of how your butterfly want to be depicted.

217
00:36:22,880 --> 00:36:41,880
So you're doing this in steps for image, and then you are injecting the tokens from the clip to your, you know, your network for for image generation.

218
00:36:41,880 --> 00:36:50,880
And so what if gradually adding things that you want to be in that image.

219
00:36:51,880 --> 00:37:07,880
That's a great question, I haven't done this, like we haven't done this directly, but you can kind of do this right, you could like, you could run your reverse process to some point with with your text conditioning being just the simple thing hey it's a butterfly.

220
00:37:07,880 --> 00:37:18,880
Then you could continue with a new text prompt for guidance, hey, the butterfly looks like this or so on, and keep going. Maybe that works. Not sure.

221
00:37:18,880 --> 00:37:23,880
You could do something else where you just run the whole process, first generate a butterfly.

222
00:37:23,880 --> 00:37:34,880
You take the butterfly, you noise it to go back in the process, and then rerun it, but now with a different prompt, so you kind of modifying this generated butterfly in a new direction.

223
00:37:34,880 --> 00:37:48,880
Then, you know, noise it again and rerun it again with a slightly different prompt, you kind of be like slowly changing this generation iteratively in these like kind of like iterative modification.

224
00:37:48,880 --> 00:37:57,880
So in another slide later we'll show how to do this with something like in painting, but if you just wanted to do it for your direct image. This is maybe how you would do it.

225
00:37:57,880 --> 00:38:00,880
Does that kind of answer.

226
00:38:00,880 --> 00:38:11,880
Yeah, yeah, I think that's a very interesting, you know, thought and yeah I appreciate your answer.

227
00:38:11,880 --> 00:38:19,880
I think that in painting could be one way of thinking about it. Yeah.

228
00:38:19,880 --> 00:38:29,880
Yeah, but what I was trying to say there was like yeah, you could also do it without impending by like kind of modifying the full image by like renoising it and reproducing.

229
00:38:29,880 --> 00:38:47,880
Yeah, yeah, that also makes sense. I think that I was also referring to more like just the way that by removing the noise you are, you know, trying to somehow refine the image.

230
00:38:47,880 --> 00:38:53,880
This also in steps could, you know, add more context to the image.

231
00:38:53,880 --> 00:38:56,880
And there might be different ways of implementing it.

232
00:38:56,880 --> 00:38:58,880
Yeah.

233
00:38:58,880 --> 00:39:00,880
I think that's good.

234
00:39:00,880 --> 00:39:03,880
Okay, thanks.

235
00:39:03,880 --> 00:39:05,880
Okay.

236
00:39:05,880 --> 00:39:15,880
So, in our guide paper we kind of compare these two forms of like guidance for text conditional models clip guidance was classified for guidance.

237
00:39:15,880 --> 00:39:32,880
And here are a few samples like representative samples from the model. So on the left here is just samples without any guidance. This is just a pure conditional diffusion model there is no form of classifier clip guidance or classifier free guidance.

238
00:39:32,880 --> 00:39:43,880
And kind of see, you know, it's kind of getting the prompts of the pram pure steam glass window of a panda panda eating bamboo. It's kind of alright, but it's not very coherent.

239
00:39:43,880 --> 00:39:53,880
Then you do clip guidance with scale to start getting better with the classifier free guidance ones look the best in in all the tests we did.

240
00:39:53,880 --> 00:40:09,880
And I think part of this might be just that it's, it's not exactly the correct thing to use clip part of it just might be that it's kind of better inductive bias to use classifier free guidance.

241
00:40:09,880 --> 00:40:24,880
There could be a lot of reasons but at least empirically, this was working better in practice to generate more realistic samples from these models, and you can see guidance does make a big difference in, you know, generating more realistic things, but it also does kind of make, you know, it's

242
00:40:24,880 --> 00:40:34,880
kind of like more collapse effect happen all of these samples kind of start looking a little similar to each other when when you do a lot of guidance.

243
00:40:34,880 --> 00:40:46,880
So, what what what have we done here we've trained a model that you know given a text prompt can generate images and we've done it for this diffusion technique.

244
00:40:46,880 --> 00:40:59,880
And this was what the strain in the glide paper, and we then showed that this model actually was beating the older open a model, Dali, which was actually a bigger model, which was trained in a very different fashion.

245
00:40:59,880 --> 00:41:05,880
The strain is using an order aggressive model on these like discrete VA tokens.

246
00:41:05,880 --> 00:41:21,880
And it, the new diffusion model not only generated things that looked a lot more realistic it actually generated them faster and use fewer parameters. So, this new model class is actually a lot nicer to use for these tasks than the older class of models.

247
00:41:21,880 --> 00:41:32,880
One cool advantage of these models is also because they're not doing this thing or to aggressively they're you know just generating and hold image you can do these things that are much harder to do with these older autoregressive models can do things like in painting.

248
00:41:32,880 --> 00:41:38,880
So you could mask out a portion of the image, and then ask the model to kind of fill in that portion.

249
00:41:38,880 --> 00:41:49,880
And how would you do that well just like we passed our conditioning labels in the past you could just pass in kind of this like half filled image as extra conditioning information to the model.

250
00:41:49,880 --> 00:42:08,880
So you take this image and a mask on top of it, you provide this extra information to the model when it's trying to run its generative process and it's going to try to now think of this as a label hey like this is image what are the possible images that correspond to this label for this kind of

251
00:42:08,880 --> 00:42:28,880
a masked image, what are the things that could complete this image. And what you're providing is this kind of like image X zero with a little bit of region must and a mask and that tells what is the part of the image that has been masked.

252
00:42:28,880 --> 00:42:40,880
Now, to text condition and painting, you could provide an, you know, an image with a mask and you could also provide a text label to tell the model how it shouldn't paint the region.

253
00:42:40,880 --> 00:42:52,880
So these are examples from the paper. So on the left here, you have the text label being zebras roaming in the field and you have this image with a green mask on it. So the masked region was removed.

254
00:42:52,880 --> 00:43:05,880
And the model was asked to fill in this image conditioned on this product. So now it's going to try to fill in not only something that kind of completes the image correctly, like isn't there the conditional distribution but also kind of matches the product.

255
00:43:05,880 --> 00:43:15,880
On the right here you see something with a girl hugging a corgi on a pedestal. And it's kind of matching the style of the image very well here if you can see it kind of looks like it's like painting.

256
00:43:15,880 --> 00:43:26,880
Kind of nicely like blended in. So this is really cool thing which you can do very easily out of the box with diffusion models but it's kind of much harder to do with other classes of models.

257
00:43:26,880 --> 00:43:41,880
And you could take this idea iteratively, like you could now erase a region of image. So let's say we erased the region on the left here, and you first filled it in with, you know, a cozy living room, and you raised a different region.

258
00:43:41,880 --> 00:43:51,880
And you know, asked for a painting of a corgi on the wall above the couch. When you get a painting there, you raise another region, put a coffee table, put a flower vase and so on.

259
00:43:51,880 --> 00:44:06,880
So this is one way of kind of doing the thing you talked about Ali, but you kind of like generate things iteratively, but this is doing it from painting erasing regions, raising very specific regions, then asking the model to fill that region in with the thing you want.

260
00:44:06,880 --> 00:44:16,880
It doesn't cover all kind of modifications that you want to do, but it does cover things that you can represent as like adding things one by one into an image, if that makes sense.

261
00:44:16,880 --> 00:44:31,880
So like stuff that you maybe cannot do with this is like, you know, change the style of an image completely the full thing, because well, if you just erase the whole thing you couldn't, it wouldn't have anything to condition it can't use the style but things like this where you add things, you

262
00:44:31,880 --> 00:44:35,880
can do it pretty easily through iterative painting.

263
00:44:35,880 --> 00:44:41,880
Any questions so far on the painting side of things.

264
00:44:41,880 --> 00:44:48,880
So Linda is asking if the collab is available for painting I think I saw it on the website.

265
00:44:49,880 --> 00:44:59,880
Yes, the collab that we released in the guide repo the thing the third one is the second one that one doesn't painting.

266
00:44:59,880 --> 00:45:10,880
Basically, all you like to do that is you'll have to provide this extra image, and you'll have to provide a mask or like mask out a portion of the image and then provide a mask that tells what has been masked.

267
00:45:10,880 --> 00:45:20,880
And then you just run the guided diffusion process as usual, but now with this extra information to try to impaint this region.

268
00:45:20,880 --> 00:45:25,880
I'll go into kind of notebook later but yes, the notebooks there.

269
00:45:25,880 --> 00:45:29,880
Diego also has a question.

270
00:45:29,880 --> 00:45:36,880
Can you remove objects using printing. Yeah, I mean, so let me go back to the slide.

271
00:45:37,880 --> 00:45:51,880
I mean, I guess, technically, in the very first one we removed the thing right we just must out whatever was on the wall in the left. And here we ended up adding a painting of a colleague but you could just ask for nothing.

272
00:45:51,880 --> 00:45:54,880
And then it would just fill it with the wall.

273
00:45:54,880 --> 00:45:59,880
I don't know if there's an example here.

274
00:45:59,880 --> 00:46:08,880
Yeah, in all of these things we kind of change something modified something but if you just don't give any prompt is just going to try to fill it.

275
00:46:08,880 --> 00:46:17,880
Without any extra information is just trying to make the best possible completion, and that could be kind of like removing an object.

276
00:46:17,880 --> 00:46:24,880
Does that answer your question.

277
00:46:24,880 --> 00:46:30,880
Assuming yes, never move on.

278
00:46:30,880 --> 00:46:32,880
Okay.

279
00:46:32,880 --> 00:46:46,880
Well, you can take this idea further and you can do out painting kind of so like previously we drew a mask that was inside the image, but you could also kind of move the rectangle that the model is focusing on outside the image.

280
00:46:46,880 --> 00:46:57,880
So now the mask looks like a strip of things around the image that is masked, and you can ask the model to fill that thing, and you could keep moving this rectangle around to kind of expand out from an image.

281
00:46:57,880 --> 00:47:14,880
This is something that Holly heard, and then she like to this central image here and then she kept moving the square out kind of expand out the canvas of the model and ask you to keep filling in extra information outside of the region.

282
00:47:14,880 --> 00:47:23,880
At the end of the day for the model is there is just like some conditioning information some mask, and it's going to just try to fill in in that region whatever it thinks is the best possible completion.

283
00:47:23,880 --> 00:47:29,880
It doesn't have to be inside and be outside as well.

284
00:47:29,880 --> 00:47:39,880
So, one other, I guess, important thing is like, we talked about the release notebooks the breeze notebooks is kind of the released model which is the filtered glide model.

285
00:47:39,880 --> 00:47:56,880
So in our paper we talk about this where we, if we looked at, you know, kind of the things you could generate with the big original glide model, and there were a lot of, like, problematic things that it could generate that made it unsafe to release the full big model.

286
00:47:56,880 --> 00:48:12,880
So release a small smaller model on a filter data set. And it cannot generate things that look as impressive as the big model, but it's still kind of can generate realistic looking images for like some of these easier prompts.

287
00:48:12,880 --> 00:48:21,880
But yes, there was going to be a little bit of a performance gap between using the filtered small model that has been released versus for the best images you can see.

288
00:48:21,880 --> 00:48:30,880
You can still generate a lot of cool things with the small filtered model. These are some of the things I found on Twitter that people have generated with the notebooks that we released.

289
00:48:30,880 --> 00:48:32,880
So on the left here.

290
00:48:32,880 --> 00:48:43,880
I think what they did was, they kind of did the painting thing, but they just went kind of like in a panorama fashion left to right, and kind of kept asking the model to fill these landscapes.

291
00:48:44,880 --> 00:48:50,880
The guidance scale a lot to make it very artsy think in the right.

292
00:48:50,880 --> 00:48:57,880
I've kind of problem like they've done this out painting thing but

293
00:48:57,880 --> 00:49:11,880
I don't know how they got those structures, but I think a part of this part of the fun stuff here is kind of these prompt search or prompt tuning things where you kind of find these prompts and generate very specific and artistic styles.

294
00:49:11,880 --> 00:49:21,880
And if you find very cool prompts and you can now use these tricks of our painting and so on, kind of like keep expanding it out to generate these cool pieces of art.

295
00:49:21,880 --> 00:49:33,880
This is another thing I found on Twitter where they trained a classifier for guidance model on conceptual captions and I think this is like a flower a space flower with some space.

296
00:49:33,880 --> 00:49:35,880
Our team.

297
00:49:35,880 --> 00:49:36,880
Super cool.

298
00:49:36,880 --> 00:49:39,880
There is a question.

299
00:49:39,880 --> 00:49:41,880
You want to read it.

300
00:49:41,880 --> 00:49:43,880
Let's see.

301
00:49:43,880 --> 00:49:47,880
What did it create that was dangerous.

302
00:49:47,880 --> 00:49:51,880
So, or maybe comment.

303
00:49:51,880 --> 00:49:53,880
Oh, sorry.

304
00:49:53,880 --> 00:50:07,880
I think I, yeah, what did it create that was dangerous. Yeah, I guess, well, for all the days I would recommend just reading a paper. I mean, there were, and I wasn't the one who did the safety analysis here was the opinion.

305
00:50:07,880 --> 00:50:11,880
The people who work on safety at opening and Alex.

306
00:50:11,880 --> 00:50:29,880
But I think it was stuff like violence it was stuff that could be used for like, if it makes it for like misinformation and so on. But I mean these models are pretty powerful so you could generate lots of things that you don't want to be floating on the internet.

307
00:50:29,880 --> 00:50:44,880
I mean, the trade offs are hard here right because like, on the one hand you do want to, you know, put these powerful models out there in the hands of people to like, generate all these nice cool art and like, like lots of positive use cases right.

308
00:50:44,880 --> 00:50:54,880
But I think you want to also be conservative to not create a lot of like stuff that you don't want floating around on the internet that's associated with your models.

309
00:50:54,880 --> 00:50:57,880
You know, this is a tough trade off.

310
00:50:57,880 --> 00:51:11,880
I think it's, it's nice that we can still release some safe model that people can use, but making these models like fully safe when they never generate something that is like, kind of like not a, not a good thing.

311
00:51:11,880 --> 00:51:15,880
It's very hard problem in general.

312
00:51:15,880 --> 00:51:28,880
So you need to find more like detailed examples in our paper, if you're looking for like specific examples, but that was kind of our line of thinking on like releasing like the small filter budget.

313
00:51:28,880 --> 00:51:41,880
Okay, maybe a slight tangent but what what does the process look like of let's say calling, you know, the unsafe parts away from from the model, like how do you go about that.

314
00:51:41,880 --> 00:52:01,880
So calls usually I guess training these training kind of like these classifiers to filter out portions of the data set that could be like not safe, if you could, you know, train an NSFW classifier you could train a classifier for like hate symbols you could train a classifier for

315
00:52:01,880 --> 00:52:12,880
other things, then you, once you have labeled data on which you can train these classifiers labeled data for like I don't know real images that you consider things that you don't want to model to generate.

316
00:52:12,880 --> 00:52:24,880
You could run these classifiers on your training data set filter it out, then train a model on the filter data set. So hopefully the model will never go into regions where

317
00:52:24,880 --> 00:52:29,880
there's nothing like that because it was never part of the training data.

318
00:52:29,880 --> 00:52:32,880
Yeah, awesome. Thank you.

319
00:52:32,880 --> 00:52:43,880
Okay, so just a quick like some like look into the notebooks that we've released this. This is just like some useful.

320
00:52:43,880 --> 00:52:55,880
Some parameters that you will have to like, kind of like deal with when you're trying to generate stuff from the notebooks that you released. Well, there's the two scales we've talked about in a talk today the classifier free guidance scale and the clip guidance

321
00:52:55,880 --> 00:53:09,880
scale, the small values for these scales will generate, you know, more diverse, but not very coherent samples larger values will generate coherent things very large values will generate like very artsy looking things.

322
00:53:09,880 --> 00:53:18,880
The classifier free guidance scale, like, I think three might be the default but you could try five 1020 or so on to generate more artistic things.

323
00:53:18,880 --> 00:53:21,880
Similarly for the clip guidance scale.

324
00:53:21,880 --> 00:53:36,880
Time steps kind of controls like how many little steps you take in the diffusion process. I think by default we use 100. There's 100 steps of like iterative denoising that will happen so if you use a higher value to look more sharp, but you'll also spend more

325
00:53:36,880 --> 00:53:39,880
time creating a sample.

326
00:53:39,880 --> 00:53:42,880
So it was a good like trade off that we used in the thing.

327
00:53:42,880 --> 00:53:56,880
Finally, the further in painting notebook, you would have to provide an extra thing which is like, what is the region of a given image that you want to paint so you would have to provide, let's say a 64 by 64 image that you want to impaint and some

328
00:53:57,880 --> 00:54:09,880
that you've like removed that you kind of specify with a mask, which is like, I think one in places where the image is not masked and zero in the places where the image is masked.

329
00:54:09,880 --> 00:54:20,880
I could be wrong on the zero versus one, so you should check the notebook for which direction, basically it's a binary mask that tells this is the portion of the image that is masked, this is the portion of the image that is unmasked.

330
00:54:20,880 --> 00:54:26,880
And the rest is like, just just a usual image with three channels that you provide as extra information to the model.

331
00:54:26,880 --> 00:54:30,880
And do you just upload that as an image file.

332
00:54:30,880 --> 00:54:44,880
I think the way in the notebook that works is, so if this is on a co lab you'll have to have the file on drive and then you open it using pillow image dot open or something. I don't know if there's like a direct upload button.

333
00:54:44,880 --> 00:54:46,880
But I guess like,

334
00:54:46,880 --> 00:54:56,880
Oh, sorry, I guess my question was, like when you add the mask, like the mask is just like removing part of like a regular image file or like there's something more to it.

335
00:54:56,880 --> 00:55:07,880
Oh, yeah, that's just removing parts of the regular file. So like, I think if you want to do it programmatically just just zero out that region.

336
00:55:07,880 --> 00:55:09,880
Does that answer the question or.

337
00:55:09,880 --> 00:55:12,880
That makes sense. Yeah, thank you. Yeah.

338
00:55:12,880 --> 00:55:26,880
There's an example in the notebook and there's a cell in the notebook that kind of masks an image that might be more clear where you can see like, you are loading an image from the disk then you are kind of like removing a region then you are kind of writing down a

339
00:55:26,880 --> 00:55:33,880
mask that specifies what you removed and then you pass in all this information into the model.

340
00:55:33,880 --> 00:55:43,880
So I think that's it for the stuff you will need to kind of like apply this thing to the notebook.

341
00:55:43,880 --> 00:55:56,880
And if you want more further reading or like what we talked about today, I mean I try to focus on mostly like things you will need to understand for like kind of generating art from these things but you want to go into more detail about the

342
00:55:56,880 --> 00:56:06,880
design of these models. I think the best paper would probably be the denoising diffusion probabilistic models paper by Jonathan Ho, the DDPM paper.

343
00:56:06,880 --> 00:56:19,880
That has kind of like the basic theory of the models that we talked about today, and there was our paper on diffusion models began to imagine this is that kind of talks about the guidance trick for classifier guidance, and then the glide paper

344
00:56:19,880 --> 00:56:34,880
to this for text conditional models where we talk about clip guidance and classifier free guidance. There was also the paper by young song generative modeling best made ingredients of the data distribution which kind of like was before the Jonathan

345
00:56:34,880 --> 00:56:46,880
Ho approach this problem from a very different perspective of score matching, and in the DDPM paper Jonathan Ho and others showed how it's kind of equivalent to score matching.

346
00:56:46,880 --> 00:57:00,880
So if you want to understand diffusion models from a different lens, I think I would strongly recommend that paper, and these two blogs as well by Lillian and Yang on diffusion models and score matching models, basically like two sides of the same kind of understanding from both

347
00:57:00,880 --> 00:57:06,880
the super useful to see you know why these generated models work.

348
00:57:06,880 --> 00:57:11,880
And that's it.

349
00:57:11,880 --> 00:57:16,880
Thank you so much it was very, very interesting and fascinating.

350
00:57:16,880 --> 00:57:18,880
I'm very inspiring.

351
00:57:18,880 --> 00:57:20,880
Are there questions.

352
00:57:20,880 --> 00:57:21,880
Go ahead.

353
00:57:21,880 --> 00:57:23,880
Super cool.

354
00:57:23,880 --> 00:57:24,880
Thank you.

355
00:57:24,880 --> 00:57:40,880
Question. Did you notice if there was any relationship between say, like, if you fed it to dimensional noise, and if you were to step through the X or Y coordinates, did you notice if there's any relationship between the coordinates used for two dimensional noise and the output you can

356
00:57:40,880 --> 00:57:42,880
Say that again.

357
00:57:42,880 --> 00:57:44,880
What did you say at the end.

358
00:57:44,880 --> 00:57:55,880
If you use two dimensional noise is there a relationship between the coordinates that you use for the two dimensional noise map and the outputs of the model.

359
00:57:55,880 --> 00:58:03,880
Like is there a relationship that that that you observe between the noise that you use and the model and the outputs of the model.

360
00:58:04,880 --> 00:58:11,880
There's a little bit so like one way you can do this is you can like fix the noise, the sample and change the label.

361
00:58:11,880 --> 00:58:20,880
And you can see that the generated images for the same noise but different labels kind of have similar like perspective and spatial structure.

362
00:58:20,880 --> 00:58:25,880
So like, but but they look kind of like images from different classes.

363
00:58:25,880 --> 00:58:34,880
So this is definitely control some aspect of you know how the final output looks like and there's some kind of spatial like connection, but it's not an exact direct connection.

364
00:58:34,880 --> 00:58:37,880
Does that kind of answer your question.

365
00:58:37,880 --> 00:58:39,880
Yeah, got it.

366
00:58:39,880 --> 00:58:48,880
I love this in our I think diffusion models beat guns paper. I think that in the appendix we have an example where we like do this specific thing.

367
00:58:48,880 --> 00:58:52,880
It's actually more directly connected when you use a different sampling method.

368
00:58:52,880 --> 00:59:00,880
So, I showed you there is reverse process right where at each step you're doing this reverse step with the Gaussian.

369
00:59:00,880 --> 00:59:12,880
At each step you're adding a little bit of noise when you sample from the Gaussian, but there's a different way of reverse sampling from these models which is called ddim is another paper on that, where you just sample noise once at the start.

370
00:59:12,880 --> 00:59:29,880
And then you just run a deterministic reverse process to sample from the model in that case that there's kind of like this one to one correspondence between the noise and the generated image, and then it's more clear to see this.

371
00:59:29,880 --> 00:59:44,880
Thank you.

372
00:59:44,880 --> 00:59:49,880
I can ask my earlier question again, I guess if nobody else is not a question.

373
00:59:50,880 --> 01:00:07,880
So I guess, if you're using a diffusion model without clip. Right. So I guess what's being trained is the classifiers for the labels that am I understanding that correctly.

374
01:00:08,880 --> 01:00:10,880
For the denoising process.

375
01:00:10,880 --> 01:00:23,880
Yeah, I guess I'm trying to understand if without clip. How does it know what to denoise to without like some representation of the text that you're feeding it.

376
01:00:23,880 --> 01:00:26,880
Yeah, yeah, so.

377
01:00:26,880 --> 01:00:42,880
Well, yeah, if you train a model or denoising model without text labels, then it doesn't know where to go and the only way you can generate a sample for a given text distribution it would would be through like clip guidance or something, but we do have.

378
01:00:42,880 --> 01:00:56,880
We train these models to be text conditional diffusion model. And in the classifier free guidance case, you train it with or without the labels. So maybe I can go back to one of our slides here.

379
01:00:56,880 --> 01:01:08,880
The way the model, the reverse process model sees the text is through this kind of conditioning on the, the representations output by a transformer on the text.

380
01:01:08,880 --> 01:01:10,880
Okay, got it.

381
01:01:10,880 --> 01:01:14,880
So this text conditioning here is without clip.

382
01:01:14,880 --> 01:01:18,880
Yes, so.

383
01:01:18,880 --> 01:01:34,880
Can I point a glossed over in guidance was you could use guidance on top of unconditional models or conditional models. So you could have a reverse diffusion model that isn't conditioned on any labels, then it wouldn't have any way of actually like producing an image given a class.

384
01:01:34,880 --> 01:01:42,880
But then you could use guidance on top to get it to produce an image giving a class, but you could also use guidance on top of conditional models themselves.

385
01:01:42,880 --> 01:01:53,880
So you could have your original model be able to produce an image text, like we did here, but also use guidance on top to make it even better at doing this.

386
01:01:53,880 --> 01:01:55,880
Got it. Okay, yeah.

387
01:01:55,880 --> 01:01:59,880
This one.

388
01:01:59,880 --> 01:02:09,880
Okay, thank you.

389
01:02:09,880 --> 01:02:19,880
Okay, excellent. If are there more questions.

390
01:02:19,880 --> 01:02:24,880
Well, I think that we can wrap up the session.

391
01:02:24,880 --> 01:02:27,880
Again want to thank you a lot.

392
01:02:27,880 --> 01:02:32,880
Profile and it was, it was great. Thank you so much.

393
01:02:32,880 --> 01:02:41,880
Thank you. And thank you for having me and feel free to just like email me any questions later or DM me on Twitter with questions.

394
01:02:41,880 --> 01:02:56,880
I think there's a lot of cool stuff out happening in this so like, I would strongly recommend doing some of the like reading some of the blogs are reading just like things that you can find in other collab notebooks as well.

