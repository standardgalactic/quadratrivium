{"text": " All right. Hello, everyone. Welcome to your course, AI for Art, Aesthetics and Design and Creativity. Today, we have a very special lecturer, AJ. He has been at MIT just like you for his undergrad. I got to know him when he was here and he's very active. I've been running at the ML groups and sometimes chatting with me about, you know, these topics of creativity and AI and art. I think that this is very exciting. He's going to tell us about his journey and and his new work. I will let him to, you know, start the discussion. AJ, one of the things that I always ask is that if you could please introduce yourself and tell us a little more about what inspires you to work in this area. Sounds good? Yeah. Yeah, absolutely. I'd be happy to. And thanks so much for having me. So today, I'm going to be talking about some work I've done, some works that's happening in the community around 3D content creation. But first about my journey. Yeah, I was at MIT for my undergrad and I was part of what is now called the AI Club. And then we called it the Machine Intelligence Community. In my undergrad, I did research in a couple of areas, but mostly actually in compilers. So a little distant from what I do now, more on the high performance computing side and performance engineering that had experienced self-driving cars and generative models for self-driving applications during undergrad and really fell in love with that topic. How do we reason about uncertainty? How do we model complex data distributions and predict the future? Like, for example, predicting the behavior of vehicles. And that led me down the path of working on generative models in my PhD. And these generative models are, these days, the state of the art generative models are parametrized by deep neural networks, which try to fit large data sets, try to estimate correlations between different variables. And these could be old types of different data modalities, like images, they could be trajectories or behaviors, like I worked on, audio, video. And today, we're going to talk a little bit about 3D objects. And so that's kind of what inspired me. At the time, I was interested in uncertainty estimation. But these days, I just really like the tangible results you can get out of generative models, novel samples, and novel designs. It's very fun. I got to look at pretty pictures all day. To my research interests, like I mentioned, around generative models, we've done some work in denoising auto encoders. How do you generate images with these denoising diffusion probabilistic models? That's purely in the 2D setting, though it's been extended to other domains. Over the past year and a half, I've also been doing a lot of work in 3D reconstruction and inverse graphics. So how do we take images and try to infer a scene from them or generate in the 3D space? Sorry, Ajay, interrupting you. It seems that some of the students want the transcription to be on. Is that okay? That's fine. Okay, excellent. Thanks. And building off of that performance engineering background I had from MIT, I also did a lot of work in the intersection of machine learning and programming languages at the start of my graduate school. And I would summarize kind of my research interest as making it easier to create creative content, especially with AI tools. And to provide some background for today, I'm going to discuss different types of scene representations. What I mean by this is how do we encode the geometry and colour of a scene in some format that we can work with digitally. And there's this very long history of this, particularly from the graphics literature. On this slide shows some different representations of geometry that you'll be familiar with some of them. 2.5D might include RGBD images, like a photo plus a depth scan. And they're point clouds, meshes. Meshes are the most common representation used in graphics applications, but they can actually be challenging to work with in a learning context. So our focus in the learning context will be on the volumetric approaches. These can be very easy to train. You can kind of think of at least a boss of greatest classifier, mapping each point in space to whether it's part of the object or not, whether it's occupied or not. More recently, there's been a significant amount of interest in neural scene representations, sometimes called implicit neural representations that define the geometry of the object with a function. That could be a distance function, so a network mapping from coordinates to the distance to the nearest surface. And these can be a lot easier to optimise. These neural scene representations can also compress the data significantly compared to explicitly representing the geometry of the scene. So we'll be focusing on that direction. And in particular, we're going to be discussing a model called neural radian fields I'll get to in a second. But they address this problem of view synthesis. So how do we take some sparsely sampled input views of a scene and then construct a representation of that scene's 3D geometry and colour in a way that allows us to render it from new perspectives? These are some example works. You can represent the scene as a multi-plane image. So instead of a flat grid of RGB values represented as multiple planes, and that allows very quick rendering from new perspectives. Neural volumes is an approach from Facebook that has an encoder decoder structure. Take some input images and encode them into a layman space, kind of a 3D layman space and decode it out to images with volume rendering. Neural radian fields have really been very popular over the last two years due to their simplicity and quality of the results they can generate. So here's an example scene that's captured on the Berkeley campus. Some photos of the scene are captured, for example, with an iPhone. I believe these are captured with an iPhone. Then poses for each photo are estimated. And this neural scene representation called the neural radian field is estimated off of those images. What's really nice is once we estimate the representation of the scene, we can render it from novel viewpoints and kind of smoothly interpolate these sparsely sampled views. The scene is only very sparsely observed from discrete points. What if you as the user want to make a photo from a new perspective? There's some interesting things to note about this rendering. Notice the specularities on the surface. They're not just modeling the diffuse light. Also modeling how the light reflected back at the user depends on the viewpoint of the camera. As you shift your head, the scene will change. This is particularly visible on very shiny surfaces like the glass or metal of the car. A neural radian field, yeah, it really is amazing and really captured the attention of a lot of people. This neural radian field has grown very quickly and there's still a lot of problems to be solved. One very interesting thing that these neural scene representations bring to mind is that we're encoding a scene in the neural network's weights. Instead of explicitly encoding the geometry of the scene via points or meshes, lists of triangles or voxel grids, it's encoded into this small multi-layer perceptron. Maybe this is a half a million parameter network, just some stacks of dense layers. It's representing a function mapping from 3D space coordinates, XYZ. This is in the scene, XYZ coordinates, and a viewing direction. What is the angle of the camera in order to model those view dependent effects? The neural network then predicts at this coordinate what is the color of the scene and then its density, sigma. There's density as something like how solid is the object and how much light will be absorbed. Rendering is done by ray tracing. This is not exactly what would be done in most graphics applications like real-time ray tracing because we've kind of encoded the light being reflected at any given point back towards the viewer into this function. So we don't have to scatter light through the scene. The viewer will cast a ray from their camera through the pixel. This is the image plane into the scene and then query the neural network along the ray. These are different 3D coordinates in the scene. The color of the rendered pixel will then be some accumulation of the colors along that ray. In order to determine the color and the density along the ray, each of these coordinates is passed to the MLP as a very large batch. We get a color and density for each coordinate and then can compose them with alpha compositing into a color. There's some subtlety to this compositing. This is called the volume rendering equation because this equation is pretty simple. This is the density predicted. This is the camera origin and it's displaced some steps along the ray. The neural network will predict what is the density of the scene at that point, but it will also predict what is color. Then we're integrating this color along the ray weighted by its density, but we also have to weight it by transmittance, which is roughly speaking how much light is transmitted from the viewer to that point along the ray because once we've accumulated enough density, then objects further back in the scene will not be visible to be included. This equation for color conditioned on coordinates is differentiable with respect to the parameters of sigma and c. So sigma and c will be this neural network. Because this is fully differentiable, it's relatively easy to optimize. Instead of optimizing scene geometry, we'll optimize the weights of this neural network in order to get some desired colors. This might be the sparsely observed viewpoints, two viewpoints. Let's render the color according to the neural scene representation and then try to optimize the network so that it matches the observed views, pixel by pixel. It might take a minute to wrap your head around, but it's actually pretty simple. We have this one MLP that encodes the scene, lets us render viewpoints differentially, and we'll optimize the scene so that it matches the input views, and that's why it's inverse graphics. We're going from the 2D space to optimize for the underlying 3D representation that will reconstruct those views. Are there any questions about that? Can you talk about how the points are used for the neural net? I can't see it directly. I mean, I get the high-level idea, but could you talk about how those points are fed into the net? Yeah, so you could consider constructing an MLP with five dimensions as input, just five inputs, and then four outputs on the layers, and then intermediate features or whatever you can imagine you want. So in nerf it's 256, that would be one approach, and it does work, but then you get actually quite blurry reconstructions of the scene if you directly feed an input coordinates as these are just floating point numbers, 3D coordinates, and going that direction. But instead, what is used in this neural radiance field is assigned useoidal positional encoding, so frequency-based encoding. If you're familiar with the transformer positional encoding, this is a common approach where continuous values like coordinates or time steps are encoded using a Fourier representation. So you take sine of various scaled values of the input coordinates, and that lets the network model high-frequency detail. So instead of kind of memorizing a function from each spatial coordinate, it can model frequencies of the underlying signal if you use the sine useoidal embedding of the input. So that kind of just projects this five-dimensional input into some higher-dimensional space before feeding it to the MLP. I see. And there's no, let's say, I guess, filtering done before, I mean, applying it to the net. So it's just transforming certain, I guess, components, but not doing some, I guess, post-processing before putting it into, or let's say compression or something like that. No, not really compression. There's some, like, coordinate transform because you'll do this computation in a particular coordinate frame. There is some subsequent work which we actually build upon that does a pre-filtering of the input coordinates. So instead of encoding all the frequencies of the input coordinates, they'll be blurred depending on how far away from the camera you're querying. But that's sort of subsequent work to nerve. That reduces the aliasing. Let's see. And the network is just fully connected. Yeah, just a fully connected network. Super simple. Yeah, thank you. Yeah, one more thing that I wanted to mention here for a student is that there is a difference between how you train this model versus the models that so far you have seen for, for instance, classification. For instance, if you want to train a model for a truck, what you do is you get a lot of images of different trucks in different lightings and different, you know, models and things like that, and then fit it to your network. However, in this case, you are taking lots of images of the single truck, single scene, and you are trying to reconstruct that scene. So you said big difference between, you know, what you are used to doing and what we see in nerve. Yeah, absolutely. I kind of see it as instead of, the nerve is representing a single scene. So instead of representing explicitly or representing it with a neural net with a function, but it doesn't generalize. It interpolates these input views. And, you know, there's a catch to that, which is that in order to fit into the neural radiance field to a single scene, it generally needs a lot of data. So while these views are sampled sparsely, discreetly, and there will be larger regions of space where we don't have an image taken from that perspective, still to estimate a multi-view consistent radiance field, experiments in the paper used a large number of images per scene. That's a little bit impractical. So for these synthetic scenes, this is one synthetic scene that's rendered in blunders. They were able to get out 100 images of each scene and fit the neural radiance field on it. For those outdoor scenes, I showed earlier like that red Toyota car. I think it's a fewer, maybe 20 images, but still that's a lot to capture with a handheld camera. And in the first week of work, I'm going to talk about we improved the data efficiency of the neural radiance field training process. So instead of using, let's say 100 images on this Lego scene, we used eight photos taken from randomly sampled viewpoints. In the neural radiance field training process, we would take, we would know the pose of each image that can be estimated with a system like call map. It's really common in the 3D graphics and 3D computer vision community is given some images, estimate their camera poses with correspondence finding, then the neural radiance field loss renders an image or renders some rays from the same pose as the input, then it computes a mean squared error loss. So the pixel wise error. The reason that this loss can be used is because we know the camera pose, we're able to render out the scene from the exact same pose that the observer took the photo. If the camera poses shifted in the rendering process, then the reconstructed image and the true image won't align pixel wise and we'll learn from inconsistent geometry. And so this is done at all of the observed camera poses. And this is why the neural radiance field needs so many photos. If there's no observed photo, then it doesn't have the ability to compute a loss from a given perspective, which means that it could overfit to the input use. This representation mapping from coordinates to colors is very flexible. One possible degenerate solution would be to put a billboard in front of each camera, just a poster board, you know, off of the highway, right in front of your camera containing the image that's observed, rather than learning a consistency in geometry. And there's other ways you can get artifacts. This is described as a shape radiance ambiguity in the Nerf++ paper. Essentially, we could either reconstruct the shape correctly and then have relatively constant radiance from different cameras, or we could encode each image into the view dependent coordinate of the network. So because the network depends on the camera position, it's able to memorize potentially the photo taken from each camera. When the neural range field is trained with 100 views, it gets really crisp reconstructions. This is a hot dog scene, synthetic scene, where we render out the views in Blender. Then the neural radiance field, when it's trained with only eight views, only matches pictures close to the training data. When you move the camera further away from the observed images to try to extrapolate, then there'll be a lot of artifacts. If you regularize the neural radiance field a little bit and simplify it, it can learn more consistent geometry, but there still are a bunch of artifacts in the reconstruction. I'll skip over this. So in our work, we add an additional loss to the neural radiance field training. We keep using the Nerf mean squared error loss. It's called photometric loss on the observed views that are sparse. But then our work diet nerf adds an additional loss at unobserved positions. So because we have this neural radiance field at any iteration during training, we're able to render out novel views even before the scene has converged. It's a little silly that in Nerf, we're not able to constrain these input views, because as a person looking at, okay, let's say that our estimate of the scene's geometry gives us these renderings. This is the observed rendering. We as people can still look at these photos and derive some loss signal. Okay, the input view is a lot sharper than my current estimate of the scene. There's a little red light at the top of the truck, but there's no light on top of these reconstructions. Based on this principle that you can compare views at different camera positions as a person by comparing their semantics, like, you know, it's a bulldozer, a bulldozer is a bulldozer from any perspective. We propose to add a loss in feature space. Using some visual encoder, each of the input views is represented in a feature space. And then instead of computing the loss in pixel space, diner will compute a loss in feature space. And that allows us to regularize the scene from any perspective during training. We call this a semantic consistency loss, since we're making sure that these semantic features, things like object identity, object color, are consistent across views. And over the course of training, this improves the results. So the loss that Nerf used was this mean squared error loss, and then we're adding this semantic consistency loss where some encoder thigh, some neural network encodes rendered images, and then we compare them in a feature space. We do have to sample camera poses in order to render this, so there's just some prior distribution over camera poses. The choice of that feature thigh is really, really important for the results, because we want it to be consistent across viewpoints. So it should really encode the object's identity and properties about the object rather than low-level details, like the exact pixel colors. And motivated by that, we use a network called Clip. This is from last year. It's a representation of images and text, so a representation of an images learn, such that it has an aligned representation with an associated caption. The data that Clip is trained on is a very large data set of 400 million images that have associated captions crawled from the web. And the Clip model has really led to an explosion of work in the AI art community. It's really powerful. It's trained on such a large amount of data that we're able to prompt it with topics that you wouldn't find in a narrow data set. It also, by learning to match images to this text, we'd hope to learn some very useful features about an image. For example, in captions, you can encode classes of objects, just like image net labels. You can also encode a lot of other details, like the scene rather than just the foreground object. You can encode things about pose of the underlying object, like a sitting person, a standing person. And that should be encoded in the representation learned by the network, if it's going to be able to match images against their associated caption. So the training objective is encode a bunch of images, encode their captions, and then try to match images with their true caption. Clip was originally used for discriminative tasks, object classification in a prompting fashion. So if you want to classify photos of food, the authors of clip constructed a bunch of captions, templatized with the desired object category, a photo of guacamole, a photo of ceviche. And then the class label is given by the caption with the best match with a given image. The property we're particularly interested in in this 3D reconstruction context is whether the representations of the images are consistent across views. That's what we call semantic consistency in the work. What this plot is showing is that the cosine similarity of embeddings from the image encoder of clip within a particular scene from different camera poses is highly similar. So very high similarity in feature space within a scene across different perspectives, low similarity across scenes at different perspectives. So because images are very similar in clip's feature space, very different in pixel space, we're able to maximize feature space similarity of clip and get some useful loss. Now what you've been waiting for are the results. This is nerf trained on eight views when it's simplified. And then here is it trained with our additional semantic consistency loss. A bunch of near field artifacts in nerf, but when we add in this feature space loss, it removes a lot of those artifacts. Because those artifacts aren't plausible, they reduce the semantic consistency. Cool. I'm going to go on to the next work. Before I do, anyone have questions? I have one question, which is with regards to using clip. Are you able to access the text as well that clip generates, or are you able to decode it in some way and actually access how the clip looks at the inputs? Just in terms of explainability, I thought it could be, yeah, sounds really interesting. Yeah, that's a very good question. So in this work, we're not actually using the text encoder. We'll see that in the next work. The text encoder is just used for pre-training clip in dye and nerf. So we're only using this image encoder. Because then the motivation for that is that the neural radian students are motivated by the view synthesis problem. So there's no text caption associated with the data. They just have a couple of pictures. So we only need to use the image encoder. That said, some artists have tried to take clip and use it to create a captioning model. If you have a model that can match images against captions, can you actually synthesize captions that best match a particular image? It's a challenging discrete optimization problem because you're searching for a textual caption that will maximize some neural network's output score. And that is basically a black box optimization problem. My impression is that automatic captioning with clip doesn't work too well. It's really good at selecting an associated caption out of a list of candidates. And that's how we're able to do object classification with clip. So I think you'd be better served by learning a specific captioning model that will generate a caption condition on image rather than trying to extract captions out of clip just due to the difficulty of the optimization or the search. Thank you. So like I said, we weren't using the text encoder in diet ner. In the next work, we try to move in an even more extreme direction of generating objects without any image data. So what if we only have a caption and want to synthesize the 3D object from it? Is that possible? Can we remove this mean squared error loss entirely and only use feature space losses? And these are some examples of the results we're able to get with different captions, like a render of a Jenga tower produces this object. You can also engineer prompts, use hashtags because clip is trained on web data. Our goal is to synthesize 3D objects from just the caption. And to kind of refresh our memories, the neural radiance field is an inverse graphics approach where we have densely sampled images, optimize the shared scene representation, and then are able to render out new views. In the dream fields work, the second work in this line, we do not have any observed images, only a caption written, for example, by a human artist. We optimize something that will look fairly similar to diet ner with additional regularizers, and then are able to render out new views. And any perspective is actually a new view because we haven't observed this scene. This is an associated scene for the caption, an epic, wondrous, fantasy painting of an ocean. So the neural radius would use this mean squared error loss, and then diet ner used feature space loss where the rendered image of the scene and an observed image of the scene are encoded into feature space that is optimized. Oops. Sorry. Okay. Now in dream fields, we use the text encoder of clip. That wasn't being used before. We were just throwing it away after trading. So instead of optimizing for the feature similarity in image feature space, we now maximize similarity of image and text features. The reason we can swap between the text encoder and the image encoder is because clip has learned to align representation. It has tried to maximize the similarity of representations of images and their associated captions so those representation spaces overlap. And you can in some sense swap the encoders from text encoder to an image encoder and hopefully still have that aligned representation. But overall, the pipeline looks fairly similar. So it's randomly sample poses in the scene, render an image, and then try to make sure that its semantic features match our features of the caption. But if you apply that approach naively without any regularizer, then there are a bunch of artifacts. These are some example generations for different captions. I believe this one had something to do with liquid in a blender. This one might have been a colorful bus with graffiti on it. So without regularization, we are getting to generate scenes. And it's not surprising because there's really no data involved in this process. In Dietner, the scene was regularized by having some input views. Here, the canvas is open, wide open. So in our work, we added some regularization. The scenes are composited with randomly sample backgrounds. And we regularize the scene to be highly transparent. So this transmittance loss encourages varsity in the underlying scene. So instead of getting lots of low density wispy content, like you saw in the previous slide, with a transmittance loss and this associated background, our motivation in Dreamfields is to create more of a consistent foreground object, a single foreground object. And these are the renderings for the associated caption, washing blueberries. There's definitely a lot of room for improvement because each of these blueberries is kind of mashed together with the others. The general caption has been encoded into this scene. And there's a consistent foreground object. This is the visualization of the process of optimization. In response to the question, Leandra asked, so it's creating this from one image, there's actually no images observed. There's only a caption fed to the system. So any images that I'm showing are rendered using our neural radiance field. They're completely fictional. I mean, some intuitive explanation for this is how can we learn a scene representation such that it could be captioned with a given caption from any perspective. Maybe that's how a human sculptor went approach the problem. So given a caption, like give me, you know, a clay sculpture of a tower. Well, let's say, you know, a monocular sculptor. Good. Optimize for a clay sculpture that is a tower of many perspective. Sorry, what happens? Sorry, what happens if the caption is something vague, like just a dog? How would your optimizer know that, like, it should have the same dog even from different poses or camera poses? Yeah, excellent question. The constraint that views should represent the same object from different perspectives just comes from the shared three presentation. We're optimizing the same MLP from any perspective. Okay, thanks. We had to simplify. Well, we didn't have to. You're able to keep view dependence in the neural radiance field. So this regularizer ends up being kind of important. Like I discussed with Dietner, if you're able to learn a lot of these near field artifacts. Sharing the scene representation is important, but some of the other techniques on our paper, like the regularizer are also important for making sure you get a clean result. In this example, we experiment with different caption templates to measure the compositional generalization of the model. So the base caption template here is a teapot in the shape of a blank, a teapot imitating a blank. And then in the video, the caption beneath each object is the word that's filled into the template caption. So a teapot in the shape of an avocado produces this object. Whereas the caption of teapot in the shape of a glacier produces something more ice styled. And I'm sorry about these animations. If you switch the caption from an armchair to a teapot, you'll also notice some changes in the shape. So there's legs on this avocado chair, but when it becomes teapot, the legs are removed. There's a follow-up question about whether the Clip Library is 2D. Yes, Clip is trained only on 2D images. So just on 2D views. The motivation for using Clip is that we can very scaleably acquire caption images from the internet. If you, for example, look at Wikipedia and just look at the upper right image associated with each article, it has a caption beneath it. And there's a data set out there called WikiText, which has about 11 million captioned images. The authors of Clip were able to collect even larger data set by scraping websites other than Wikipedia. But if you look at data sets with 3D objects in them, they're very small. The largest might be ShapeNet, which is entirely synthetic objects. And there's usually no caption associated. So we'd have to have a human annotate. This is a general trend in the 3D reconstruction literature that the availability of 3D data is quite limited. And so in dream fields, we're able to exploit this pre-trained 2D image encoder and text encoder, and then kind of lift it up into 3D by using a shared representation of the geometry. There's a bunch of different techniques that we use to improve the quality of the results. I won't get too much into this, but the metric is a little tricky to define because there's no reference object for each caption. We only have a data set of captions provided to us by the user, and we're one of measure how well our generations are performing. In order to do that, we use a neural metric based off of matching generated 3D objects against the input captions. This is something like precision of retrieving the correct caption, given the generator objects. Some of the most important techniques that help us here are regularizer for transmittance and data augmentations, those architecture we use for the MLP, and then later on, what model we use for clip. This is an example of the process of optimization from different iterations, so it actually can converge quite quickly, but additional detail might be added over the course of training. In order to run 20,000 iterations of optimization, it's an expensive process because we need to render out these images during training, but back of the envelope calculation is about three to four dollars to generate each model on TPU in Google Cloud at an hour. It's in the realm where an artist could afford to do this. We're working on some follow-up work, which will speed up this process and make it even less expensive. That's all I've got on these works. The broad goal here is to make content creation easier and generate assets that are useful. This 3D assets I see is particularly useful for downstream applications because they could be plugged into a game or plugged into some other system. We have code out for both of these projects. If you want to try out the text to 3D generation in your browser, you can use a Colab notebook that I put together. I've tested it on the Pro version of Colab, which has higher memory GPUs, so you might need to play with some of the parameters. Thank you so much, Eje. This is really fascinating. I have a few questions, and then before letting everyone else ask questions, the first question is that are you able to walk us through some of the Colab code today or should we do it on our time? Let me see if I have it up. Also, before going to changing your screen, can you please go back to the animations? Sorry, I have so many questions because this is really cool. Or maybe the one that is armchair. Yeah, give me one sec. Thank you so much. I think these are really cool. I think that for the students and I, this kind of inspires us to think maybe one cool thing to do is that we can generate these things and use them in some avatar or game or something, and this will be really cool. This is something for students to think about for their future projects because the goal of this course is to inspire us to think about what are the creative ways that we can use AI. This is really cool. One question that I have is that, can you share some intuition of, for instance, let's say the rubric. It looks like a rubric and it looks like a chair, but then we see that there is some, we wish there was more of the structure and it might be because clip is the objective and or assessor and thinking that, okay, as long as I have a patch of red and yellow and things like that that are appearing on rubric, I'm happy, the rest, I don't care much, or is there any better explanation of what's happening? Yeah, so the 3D structure only emerges because of the shared representation and the easiest way to satisfy clip from any perspective, having this Rubik's Cube chair from any perspective, might actually be to learn some consistent geometry. That said, there's no prior other than sparsity and some implicit regularization just in the structure of the MLP, so there's no prior on the 3D structure learned from data. That's something that I think is missing and definitely opportunity for future work is how do we learn some priors on 3D data and integrate them into the system to try to improve the plausibility of the structure. One example where this issue arises is that sometimes you'll get repeated structures on the objects, like if you optimize for a dog, maybe it will have eyes on multiple sides of its face because they're not visible. So you only see two sets of eyes from any particular viewpoint, that is all the discriminator clip ever sees are those two eyes, but the underlying geometry, there's no constraint that the dog should only have to rise. Okay, excellent. Thank you so much. Are there questions before we go to the collab? The outputs, are they like .fbx files or do they still need to be, let's say, a little bit prepared in rendering software before they can be actually readily used in the game engine? Like Unity or Unreal? They do need to be post-processed. So what you get out is a train neural net, so it's function mapping from coordinates. We don't use the v direction in these results, just XYZ coordinates mapped to color and density. So there are a bunch of ways that you could convert that. I don't know of off the shelf software that will be able to do that conversion for you, it'd have to be coded up, but you can sample the scene on some grid, for example, and then you'll get out color and RGB. You could convert that to a local voxel representation. If you want to get a mesh, there's an algorithm called marching cubes that is able to find a mesh in the scene, and there's implementations on GitHub of marching cube for neural radiance fields that we haven't integrated into our particular library. So you take a little bit of glue to grab marching cubes and then plug it in. So what do you all use to turn the neural net into these graphics? Sorry, could you repeat the question? What do you use to turn the neural net into the graphics that we see here? Oh yeah, so that's done by rendering. So you can render the neural radiance field from any perspective in the code, but that just renders out a 2D image. It doesn't give you, you know, like a mesh, versus the game engine will have its own rendering algorithm based on rasterization or ray tracing, given the underlying geometry and texture map, which might be real time. So the rendering here is not real time. You have to go evaluate the neural network at a bunch of different coordinates and accumulate its outputs into an image. So that's implemented. If you want videos, we can do that, but you'll have to DIY the conversion. Ellie had a question on strategies to reduce rendering costs. So you can render images at low resolution. And in the Colab notebook, the rendering is done at very low resolution. So experiments, you render out 168 by 168 images or higher. But Colab only gives you a single low memory GPU. So we render out 88 by 88 images. And that really significantly speeds up the process. So rendering takes maybe 300 milliseconds. So you have to do about three iterations per second. If you're using alpha transparent, okay. So Ben is asking, how do we handle with transparent objects? So the neural radiance field, the volumetric representation is really amenable to transparent objects because the density is this continuous value and we can observe objects. So accumulate color from objects behind transparent objects. In optimization, you might decrease the density of some object that should be transparent, like stay in glass windows. And the background is composited at the end. So any ray, if there is some accumulated, if the transmittance is not zero along the ray accumulated throughout the scene, then there'll be some contribution from the background image. So the reason that we've kind of encouraged coherent objects is that if the object is not coherent, then the background will leak through the translucent objects. Oh, I see what you're saying. Yeah, if you want stay in glass windows. So I mean, you would have to, the scene would probably optimize so that the transparent object is blocked from behind by something. Yeah, the next steps, I think they're exciting lots of next steps, because this is an initial work and there's things like speeding up the optimization. It's been a lot of recent work and speeding up neural radius field training for images. And I think a lot of that can be plugged in. And how do you synthesize the formable objects? How do you bring a human in the loop so they can provide feedback partway through training? All kinds of stuff to tackle in making this more of a practical system for 3D artists. And would you like me to share the collab? I guess we're at time. Yeah, please go ahead. That would be great. Thank you so much. So this is the collab notebook. You can find it from the project website. It is a compact implementation. The system will run faster on GPU than on TPU in the collab notebook. But for all of our experiments, we use TPU. Some helpers are imported from our library. So if you want to hack on some of the low level primitives, you can fork our library or kind of copy those helpers into the notebook. But the main way you'll interface with this collab notebook is by adjusting the quality settings here. So in particular, edit the query. Here I've filled in a high quality 3D render of Jenga Tower. And you can select the clip checkpoint you want to use. Clip bit B16 is used in most of our experiments. There's also an internal Google model that's not available here. But you can scale down if you're running out of memory to either the B32 or ResNet 50. Choose the number optimization iterations. I think at least 1000 is necessary. But more will add more detail. Consider the rendering width and then this is the number of data augmentation. And then run training. So here's an example of the training run I've already run in the notebook for that prompt, a high quality 3D render of Jenga Tower. It won't exactly match the result that was shown in the slides because the version of the collab notebook could scale down. But over the course of 2000 iterations of optimization, you can see the different learning curves. This is the total loss that's being optimized. Clip's cosine similarity, negative cosine similarity is improving. So this means that the renderings of the object are becoming more and more consistent with the given caption over time. And the transmission is regularization here. This is showing what is the average transparency of pixels in the scene. And in this plot at the bottom, the collab notebook will ride out renderings periodically every, I believe, 100 iterations. So at the beginning, the scene is low density, essentially empty. And then over time, some content will emerge from the optimization. And then that's refined and sharpened over time. The camera's moving around. So the camera's being randomly sampled around the object. And that's why the scene is rendered from different perspectives. And then finally, the collab notebook renders out a video, 48 frames. And this is the result. On the GPU that collab gave me here a P100, the optimization I think took about six, seven minutes. So hopefully you can get some cycles in. In the run training section, it says if you run out of memory, tweak the configuration options above. What do you recommend changing? Yeah, that's a good question. So I think you can change this clip at B16. I would try to clip B32. There's also on the first import in NVIDIA SMI printout. And so you can look at how much memory is available. Sometimes it's worth retrying multiple times to get a larger GPU. This P160 gigabyte I think you won't get without collab premium, which is about $10 a month. But you think you can get 15 gigabyte T4 GPUs for free. Sometimes the collab will give you an 11 gigabyte GPU that might not be enough. If you can tweak the configuration parameters, I would try reducing this number of samples. So this is the number of points along each array that is sampled. And that affects the batch size. So the render width, the batch size scales quadratically with the render width because we're rendering got square images. And then the num samples the batch size to the MLP scales linearly. So you could reduce this down to 32 even at the lowest. B32 will use less memory than B16. So this relates to the patch size and the vision transformer clip encoding. And then if you want to scale down even more, you can change the number of data augmentations per iteration, maybe down to two. Oh, Ben says that you can't retry for a better GPU. That's unfortunate. I mean, I don't know whether MIT has like a shared GPU cloud, but you can also just download this IPIND and run it on your like Jupyter notebooks, post it on some MIT compute. And it will paralyze across multiple GPUs. Cool. And have you taken any more questions that you have? Excellent. Thank you so much. Maybe at this point I'll stop recording and if students have more questions, we can...", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.52, "text": " All right. Hello, everyone. Welcome to your course, AI for Art, Aesthetics and Design", "tokens": [50364, 1057, 558, 13, 2425, 11, 1518, 13, 4027, 281, 428, 1164, 11, 7318, 337, 5735, 11, 316, 377, 9092, 1167, 293, 12748, 50840], "temperature": 0.0, "avg_logprob": -0.2473844337463379, "compression_ratio": 1.2428571428571429, "no_speech_prob": 0.018925825133919716}, {"id": 1, "seek": 0, "start": 9.52, "end": 21.12, "text": " and Creativity. Today, we have a very special lecturer, AJ. He has been at MIT just like", "tokens": [50840, 293, 11972, 4253, 13, 2692, 11, 321, 362, 257, 588, 2121, 49881, 11, 32759, 13, 634, 575, 668, 412, 13100, 445, 411, 51420], "temperature": 0.0, "avg_logprob": -0.2473844337463379, "compression_ratio": 1.2428571428571429, "no_speech_prob": 0.018925825133919716}, {"id": 2, "seek": 2112, "start": 21.12, "end": 31.200000000000003, "text": " you for his undergrad. I got to know him when he was here and he's very active. I've been running", "tokens": [50364, 291, 337, 702, 14295, 13, 286, 658, 281, 458, 796, 562, 415, 390, 510, 293, 415, 311, 588, 4967, 13, 286, 600, 668, 2614, 50868], "temperature": 0.0, "avg_logprob": -0.13083734904250052, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.22360751032829285}, {"id": 3, "seek": 2112, "start": 31.200000000000003, "end": 39.36, "text": " at the ML groups and sometimes chatting with me about, you know, these topics of creativity and", "tokens": [50868, 412, 264, 21601, 3935, 293, 2171, 24654, 365, 385, 466, 11, 291, 458, 11, 613, 8378, 295, 12915, 293, 51276], "temperature": 0.0, "avg_logprob": -0.13083734904250052, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.22360751032829285}, {"id": 4, "seek": 2112, "start": 40.08, "end": 48.24, "text": " AI and art. I think that this is very exciting. He's going to tell us about his journey and", "tokens": [51312, 7318, 293, 1523, 13, 286, 519, 300, 341, 307, 588, 4670, 13, 634, 311, 516, 281, 980, 505, 466, 702, 4671, 293, 51720], "temperature": 0.0, "avg_logprob": -0.13083734904250052, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.22360751032829285}, {"id": 5, "seek": 4824, "start": 49.04, "end": 58.88, "text": " and his new work. I will let him to, you know, start the discussion. AJ, one of the things that", "tokens": [50404, 293, 702, 777, 589, 13, 286, 486, 718, 796, 281, 11, 291, 458, 11, 722, 264, 5017, 13, 32759, 11, 472, 295, 264, 721, 300, 50896], "temperature": 0.0, "avg_logprob": -0.12496772766113282, "compression_ratio": 1.455958549222798, "no_speech_prob": 0.0027111696545034647}, {"id": 6, "seek": 4824, "start": 58.88, "end": 66.48, "text": " I always ask is that if you could please introduce yourself and tell us a little more about what", "tokens": [50896, 286, 1009, 1029, 307, 300, 498, 291, 727, 1767, 5366, 1803, 293, 980, 505, 257, 707, 544, 466, 437, 51276], "temperature": 0.0, "avg_logprob": -0.12496772766113282, "compression_ratio": 1.455958549222798, "no_speech_prob": 0.0027111696545034647}, {"id": 7, "seek": 4824, "start": 66.48, "end": 74.0, "text": " inspires you to work in this area. Sounds good? Yeah. Yeah, absolutely. I'd be happy to.", "tokens": [51276, 32566, 291, 281, 589, 294, 341, 1859, 13, 14576, 665, 30, 865, 13, 865, 11, 3122, 13, 286, 1116, 312, 2055, 281, 13, 51652], "temperature": 0.0, "avg_logprob": -0.12496772766113282, "compression_ratio": 1.455958549222798, "no_speech_prob": 0.0027111696545034647}, {"id": 8, "seek": 7400, "start": 74.0, "end": 80.4, "text": " And thanks so much for having me. So today, I'm going to be talking about some work I've done,", "tokens": [50364, 400, 3231, 370, 709, 337, 1419, 385, 13, 407, 965, 11, 286, 478, 516, 281, 312, 1417, 466, 512, 589, 286, 600, 1096, 11, 50684], "temperature": 0.0, "avg_logprob": -0.15411658619725427, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.001031995750963688}, {"id": 9, "seek": 7400, "start": 80.4, "end": 84.0, "text": " some works that's happening in the community around 3D content creation.", "tokens": [50684, 512, 1985, 300, 311, 2737, 294, 264, 1768, 926, 805, 35, 2701, 8016, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15411658619725427, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.001031995750963688}, {"id": 10, "seek": 7400, "start": 85.36, "end": 92.4, "text": " But first about my journey. Yeah, I was at MIT for my undergrad and I was part of what is now", "tokens": [50932, 583, 700, 466, 452, 4671, 13, 865, 11, 286, 390, 412, 13100, 337, 452, 14295, 293, 286, 390, 644, 295, 437, 307, 586, 51284], "temperature": 0.0, "avg_logprob": -0.15411658619725427, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.001031995750963688}, {"id": 11, "seek": 7400, "start": 92.4, "end": 96.08, "text": " called the AI Club. And then we called it the Machine Intelligence Community.", "tokens": [51284, 1219, 264, 7318, 11288, 13, 400, 550, 321, 1219, 309, 264, 22155, 27274, 10421, 13, 51468], "temperature": 0.0, "avg_logprob": -0.15411658619725427, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.001031995750963688}, {"id": 12, "seek": 9608, "start": 96.88, "end": 104.4, "text": " In my undergrad, I did research in a couple of areas, but mostly actually in compilers.", "tokens": [50404, 682, 452, 14295, 11, 286, 630, 2132, 294, 257, 1916, 295, 3179, 11, 457, 5240, 767, 294, 715, 388, 433, 13, 50780], "temperature": 0.0, "avg_logprob": -0.11668731227065578, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.003944747149944305}, {"id": 13, "seek": 9608, "start": 104.4, "end": 109.67999999999999, "text": " So a little distant from what I do now, more on the high performance computing side and", "tokens": [50780, 407, 257, 707, 17275, 490, 437, 286, 360, 586, 11, 544, 322, 264, 1090, 3389, 15866, 1252, 293, 51044], "temperature": 0.0, "avg_logprob": -0.11668731227065578, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.003944747149944305}, {"id": 14, "seek": 9608, "start": 109.67999999999999, "end": 114.56, "text": " performance engineering that had experienced self-driving cars and generative models for", "tokens": [51044, 3389, 7043, 300, 632, 6751, 2698, 12, 47094, 5163, 293, 1337, 1166, 5245, 337, 51288], "temperature": 0.0, "avg_logprob": -0.11668731227065578, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.003944747149944305}, {"id": 15, "seek": 9608, "start": 114.56, "end": 119.75999999999999, "text": " self-driving applications during undergrad and really fell in love with that topic. How do we", "tokens": [51288, 2698, 12, 47094, 5821, 1830, 14295, 293, 534, 5696, 294, 959, 365, 300, 4829, 13, 1012, 360, 321, 51548], "temperature": 0.0, "avg_logprob": -0.11668731227065578, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.003944747149944305}, {"id": 16, "seek": 9608, "start": 119.75999999999999, "end": 124.8, "text": " reason about uncertainty? How do we model complex data distributions and predict the future?", "tokens": [51548, 1778, 466, 15697, 30, 1012, 360, 321, 2316, 3997, 1412, 37870, 293, 6069, 264, 2027, 30, 51800], "temperature": 0.0, "avg_logprob": -0.11668731227065578, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.003944747149944305}, {"id": 17, "seek": 12480, "start": 125.75999999999999, "end": 130.0, "text": " Like, for example, predicting the behavior of vehicles. And that led me down the path of", "tokens": [50412, 1743, 11, 337, 1365, 11, 32884, 264, 5223, 295, 8948, 13, 400, 300, 4684, 385, 760, 264, 3100, 295, 50624], "temperature": 0.0, "avg_logprob": -0.13154289457533094, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0013667108723893762}, {"id": 18, "seek": 12480, "start": 130.0, "end": 136.32, "text": " working on generative models in my PhD. And these generative models are, these days,", "tokens": [50624, 1364, 322, 1337, 1166, 5245, 294, 452, 14476, 13, 400, 613, 1337, 1166, 5245, 366, 11, 613, 1708, 11, 50940], "temperature": 0.0, "avg_logprob": -0.13154289457533094, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0013667108723893762}, {"id": 19, "seek": 12480, "start": 136.32, "end": 140.24, "text": " the state of the art generative models are parametrized by deep neural networks, which try to", "tokens": [50940, 264, 1785, 295, 264, 1523, 1337, 1166, 5245, 366, 6220, 302, 470, 11312, 538, 2452, 18161, 9590, 11, 597, 853, 281, 51136], "temperature": 0.0, "avg_logprob": -0.13154289457533094, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0013667108723893762}, {"id": 20, "seek": 12480, "start": 140.24, "end": 145.04, "text": " fit large data sets, try to estimate correlations between different variables. And these could be", "tokens": [51136, 3318, 2416, 1412, 6352, 11, 853, 281, 12539, 13983, 763, 1296, 819, 9102, 13, 400, 613, 727, 312, 51376], "temperature": 0.0, "avg_logprob": -0.13154289457533094, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0013667108723893762}, {"id": 21, "seek": 12480, "start": 145.04, "end": 149.2, "text": " old types of different data modalities, like images, they could be trajectories or behaviors,", "tokens": [51376, 1331, 3467, 295, 819, 1412, 1072, 16110, 11, 411, 5267, 11, 436, 727, 312, 18257, 2083, 420, 15501, 11, 51584], "temperature": 0.0, "avg_logprob": -0.13154289457533094, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0013667108723893762}, {"id": 22, "seek": 14920, "start": 149.28, "end": 156.72, "text": " like I worked on, audio, video. And today, we're going to talk a little bit about 3D objects.", "tokens": [50368, 411, 286, 2732, 322, 11, 6278, 11, 960, 13, 400, 965, 11, 321, 434, 516, 281, 751, 257, 707, 857, 466, 805, 35, 6565, 13, 50740], "temperature": 0.0, "avg_logprob": -0.11652058513224625, "compression_ratio": 1.6236933797909407, "no_speech_prob": 0.007343908306211233}, {"id": 23, "seek": 14920, "start": 156.72, "end": 160.88, "text": " And so that's kind of what inspired me. At the time, I was interested in uncertainty estimation.", "tokens": [50740, 400, 370, 300, 311, 733, 295, 437, 7547, 385, 13, 1711, 264, 565, 11, 286, 390, 3102, 294, 15697, 35701, 13, 50948], "temperature": 0.0, "avg_logprob": -0.11652058513224625, "compression_ratio": 1.6236933797909407, "no_speech_prob": 0.007343908306211233}, {"id": 24, "seek": 14920, "start": 161.44, "end": 165.83999999999997, "text": " But these days, I just really like the tangible results you can get out of generative models,", "tokens": [50976, 583, 613, 1708, 11, 286, 445, 534, 411, 264, 27094, 3542, 291, 393, 483, 484, 295, 1337, 1166, 5245, 11, 51196], "temperature": 0.0, "avg_logprob": -0.11652058513224625, "compression_ratio": 1.6236933797909407, "no_speech_prob": 0.007343908306211233}, {"id": 25, "seek": 14920, "start": 166.39999999999998, "end": 172.0, "text": " novel samples, and novel designs. It's very fun. I got to look at pretty pictures all day.", "tokens": [51224, 7613, 10938, 11, 293, 7613, 11347, 13, 467, 311, 588, 1019, 13, 286, 658, 281, 574, 412, 1238, 5242, 439, 786, 13, 51504], "temperature": 0.0, "avg_logprob": -0.11652058513224625, "compression_ratio": 1.6236933797909407, "no_speech_prob": 0.007343908306211233}, {"id": 26, "seek": 14920, "start": 174.64, "end": 178.64, "text": " To my research interests, like I mentioned, around generative models, we've done some work", "tokens": [51636, 1407, 452, 2132, 8847, 11, 411, 286, 2835, 11, 926, 1337, 1166, 5245, 11, 321, 600, 1096, 512, 589, 51836], "temperature": 0.0, "avg_logprob": -0.11652058513224625, "compression_ratio": 1.6236933797909407, "no_speech_prob": 0.007343908306211233}, {"id": 27, "seek": 17864, "start": 178.64, "end": 184.39999999999998, "text": " in denoising auto encoders. How do you generate images with these denoising diffusion probabilistic", "tokens": [50364, 294, 1441, 78, 3436, 8399, 2058, 378, 433, 13, 1012, 360, 291, 8460, 5267, 365, 613, 1441, 78, 3436, 25242, 31959, 3142, 50652], "temperature": 0.0, "avg_logprob": -0.09253124480551862, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.00018226685642730445}, {"id": 28, "seek": 17864, "start": 184.39999999999998, "end": 188.48, "text": " models? That's purely in the 2D setting, though it's been extended to other domains.", "tokens": [50652, 5245, 30, 663, 311, 17491, 294, 264, 568, 35, 3287, 11, 1673, 309, 311, 668, 10913, 281, 661, 25514, 13, 50856], "temperature": 0.0, "avg_logprob": -0.09253124480551862, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.00018226685642730445}, {"id": 29, "seek": 17864, "start": 190.79999999999998, "end": 195.51999999999998, "text": " Over the past year and a half, I've also been doing a lot of work in 3D reconstruction and", "tokens": [50972, 4886, 264, 1791, 1064, 293, 257, 1922, 11, 286, 600, 611, 668, 884, 257, 688, 295, 589, 294, 805, 35, 31565, 293, 51208], "temperature": 0.0, "avg_logprob": -0.09253124480551862, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.00018226685642730445}, {"id": 30, "seek": 17864, "start": 195.51999999999998, "end": 201.27999999999997, "text": " inverse graphics. So how do we take images and try to infer a scene from them or generate in", "tokens": [51208, 17340, 11837, 13, 407, 577, 360, 321, 747, 5267, 293, 853, 281, 13596, 257, 4145, 490, 552, 420, 8460, 294, 51496], "temperature": 0.0, "avg_logprob": -0.09253124480551862, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.00018226685642730445}, {"id": 31, "seek": 20128, "start": 201.28, "end": 208.72, "text": " the 3D space? Sorry, Ajay, interrupting you. It seems that some of the students want the", "tokens": [50364, 264, 805, 35, 1901, 30, 4919, 11, 25862, 320, 11, 49455, 291, 13, 467, 2544, 300, 512, 295, 264, 1731, 528, 264, 50736], "temperature": 0.0, "avg_logprob": -0.12323826060575598, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.0022160515654832125}, {"id": 32, "seek": 20128, "start": 208.72, "end": 213.28, "text": " transcription to be on. Is that okay? That's fine. Okay, excellent. Thanks.", "tokens": [50736, 35288, 281, 312, 322, 13, 1119, 300, 1392, 30, 663, 311, 2489, 13, 1033, 11, 7103, 13, 2561, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12323826060575598, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.0022160515654832125}, {"id": 33, "seek": 20128, "start": 216.32, "end": 221.84, "text": " And building off of that performance engineering background I had from MIT, I also did a lot of", "tokens": [51116, 400, 2390, 766, 295, 300, 3389, 7043, 3678, 286, 632, 490, 13100, 11, 286, 611, 630, 257, 688, 295, 51392], "temperature": 0.0, "avg_logprob": -0.12323826060575598, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.0022160515654832125}, {"id": 34, "seek": 20128, "start": 221.84, "end": 226.24, "text": " work in the intersection of machine learning and programming languages at the start of my", "tokens": [51392, 589, 294, 264, 15236, 295, 3479, 2539, 293, 9410, 8650, 412, 264, 722, 295, 452, 51612], "temperature": 0.0, "avg_logprob": -0.12323826060575598, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.0022160515654832125}, {"id": 35, "seek": 22624, "start": 226.24, "end": 231.60000000000002, "text": " graduate school. And I would summarize kind of my research interest as making it easier to", "tokens": [50364, 8080, 1395, 13, 400, 286, 576, 20858, 733, 295, 452, 2132, 1179, 382, 1455, 309, 3571, 281, 50632], "temperature": 0.0, "avg_logprob": -0.1117229576570442, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.00057914515491575}, {"id": 36, "seek": 22624, "start": 231.60000000000002, "end": 239.04000000000002, "text": " create creative content, especially with AI tools. And to provide some background for today,", "tokens": [50632, 1884, 5880, 2701, 11, 2318, 365, 7318, 3873, 13, 400, 281, 2893, 512, 3678, 337, 965, 11, 51004], "temperature": 0.0, "avg_logprob": -0.1117229576570442, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.00057914515491575}, {"id": 37, "seek": 22624, "start": 239.04000000000002, "end": 244.16000000000003, "text": " I'm going to discuss different types of scene representations. What I mean by this is how do", "tokens": [51004, 286, 478, 516, 281, 2248, 819, 3467, 295, 4145, 33358, 13, 708, 286, 914, 538, 341, 307, 577, 360, 51260], "temperature": 0.0, "avg_logprob": -0.1117229576570442, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.00057914515491575}, {"id": 38, "seek": 22624, "start": 244.16000000000003, "end": 251.76000000000002, "text": " we encode the geometry and colour of a scene in some format that we can work with digitally.", "tokens": [51260, 321, 2058, 1429, 264, 18426, 293, 8267, 295, 257, 4145, 294, 512, 7877, 300, 321, 393, 589, 365, 36938, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1117229576570442, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.00057914515491575}, {"id": 39, "seek": 25176, "start": 252.32, "end": 257.52, "text": " And there's this very long history of this, particularly from the graphics literature.", "tokens": [50392, 400, 456, 311, 341, 588, 938, 2503, 295, 341, 11, 4098, 490, 264, 11837, 10394, 13, 50652], "temperature": 0.0, "avg_logprob": -0.12004581451416016, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.0008827254641801119}, {"id": 40, "seek": 25176, "start": 258.15999999999997, "end": 262.24, "text": " On this slide shows some different representations of geometry that you'll be familiar with some", "tokens": [50684, 1282, 341, 4137, 3110, 512, 819, 33358, 295, 18426, 300, 291, 603, 312, 4963, 365, 512, 50888], "temperature": 0.0, "avg_logprob": -0.12004581451416016, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.0008827254641801119}, {"id": 41, "seek": 25176, "start": 262.24, "end": 269.2, "text": " of them. 2.5D might include RGBD images, like a photo plus a depth scan. And they're point", "tokens": [50888, 295, 552, 13, 568, 13, 20, 35, 1062, 4090, 31231, 35, 5267, 11, 411, 257, 5052, 1804, 257, 7161, 11049, 13, 400, 436, 434, 935, 51236], "temperature": 0.0, "avg_logprob": -0.12004581451416016, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.0008827254641801119}, {"id": 42, "seek": 25176, "start": 269.2, "end": 274.32, "text": " clouds, meshes. Meshes are the most common representation used in graphics applications,", "tokens": [51236, 12193, 11, 3813, 8076, 13, 17485, 8076, 366, 264, 881, 2689, 10290, 1143, 294, 11837, 5821, 11, 51492], "temperature": 0.0, "avg_logprob": -0.12004581451416016, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.0008827254641801119}, {"id": 43, "seek": 25176, "start": 274.32, "end": 277.44, "text": " but they can actually be challenging to work with in a learning context.", "tokens": [51492, 457, 436, 393, 767, 312, 7595, 281, 589, 365, 294, 257, 2539, 4319, 13, 51648], "temperature": 0.0, "avg_logprob": -0.12004581451416016, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.0008827254641801119}, {"id": 44, "seek": 27744, "start": 277.44, "end": 285.76, "text": " So our focus in the learning context will be on the volumetric approaches. These can be very", "tokens": [50364, 407, 527, 1879, 294, 264, 2539, 4319, 486, 312, 322, 264, 1996, 449, 17475, 11587, 13, 1981, 393, 312, 588, 50780], "temperature": 0.0, "avg_logprob": -0.1544769393073188, "compression_ratio": 1.6, "no_speech_prob": 0.0004299360734876245}, {"id": 45, "seek": 27744, "start": 285.76, "end": 291.52, "text": " easy to train. You can kind of think of at least a boss of greatest classifier, mapping each point", "tokens": [50780, 1858, 281, 3847, 13, 509, 393, 733, 295, 519, 295, 412, 1935, 257, 5741, 295, 6636, 1508, 9902, 11, 18350, 1184, 935, 51068], "temperature": 0.0, "avg_logprob": -0.1544769393073188, "compression_ratio": 1.6, "no_speech_prob": 0.0004299360734876245}, {"id": 46, "seek": 27744, "start": 291.52, "end": 298.15999999999997, "text": " in space to whether it's part of the object or not, whether it's occupied or not. More recently,", "tokens": [51068, 294, 1901, 281, 1968, 309, 311, 644, 295, 264, 2657, 420, 406, 11, 1968, 309, 311, 19629, 420, 406, 13, 5048, 3938, 11, 51400], "temperature": 0.0, "avg_logprob": -0.1544769393073188, "compression_ratio": 1.6, "no_speech_prob": 0.0004299360734876245}, {"id": 47, "seek": 27744, "start": 298.15999999999997, "end": 303.04, "text": " there's been a significant amount of interest in neural scene representations, sometimes called", "tokens": [51400, 456, 311, 668, 257, 4776, 2372, 295, 1179, 294, 18161, 4145, 33358, 11, 2171, 1219, 51644], "temperature": 0.0, "avg_logprob": -0.1544769393073188, "compression_ratio": 1.6, "no_speech_prob": 0.0004299360734876245}, {"id": 48, "seek": 30304, "start": 303.04, "end": 308.64000000000004, "text": " implicit neural representations that define the geometry of the object with a function.", "tokens": [50364, 26947, 18161, 33358, 300, 6964, 264, 18426, 295, 264, 2657, 365, 257, 2445, 13, 50644], "temperature": 0.0, "avg_logprob": -0.08018727646660559, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0006876222905702889}, {"id": 49, "seek": 30304, "start": 309.28000000000003, "end": 313.52000000000004, "text": " That could be a distance function, so a network mapping from coordinates to the distance to the", "tokens": [50676, 663, 727, 312, 257, 4560, 2445, 11, 370, 257, 3209, 18350, 490, 21056, 281, 264, 4560, 281, 264, 50888], "temperature": 0.0, "avg_logprob": -0.08018727646660559, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0006876222905702889}, {"id": 50, "seek": 30304, "start": 313.52000000000004, "end": 320.08000000000004, "text": " nearest surface. And these can be a lot easier to optimise. These neural scene representations", "tokens": [50888, 23831, 3753, 13, 400, 613, 393, 312, 257, 688, 3571, 281, 5028, 908, 13, 1981, 18161, 4145, 33358, 51216], "temperature": 0.0, "avg_logprob": -0.08018727646660559, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0006876222905702889}, {"id": 51, "seek": 30304, "start": 320.08000000000004, "end": 325.36, "text": " can also compress the data significantly compared to explicitly representing the geometry of the", "tokens": [51216, 393, 611, 14778, 264, 1412, 10591, 5347, 281, 20803, 13460, 264, 18426, 295, 264, 51480], "temperature": 0.0, "avg_logprob": -0.08018727646660559, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0006876222905702889}, {"id": 52, "seek": 30304, "start": 325.36, "end": 332.48, "text": " scene. So we'll be focusing on that direction. And in particular, we're going to be discussing", "tokens": [51480, 4145, 13, 407, 321, 603, 312, 8416, 322, 300, 3513, 13, 400, 294, 1729, 11, 321, 434, 516, 281, 312, 10850, 51836], "temperature": 0.0, "avg_logprob": -0.08018727646660559, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0006876222905702889}, {"id": 53, "seek": 33248, "start": 332.56, "end": 337.28000000000003, "text": " a model called neural radian fields I'll get to in a second. But they address this problem of", "tokens": [50368, 257, 2316, 1219, 18161, 2843, 952, 7909, 286, 603, 483, 281, 294, 257, 1150, 13, 583, 436, 2985, 341, 1154, 295, 50604], "temperature": 0.0, "avg_logprob": -0.10953981499922903, "compression_ratio": 1.549800796812749, "no_speech_prob": 0.000687499821651727}, {"id": 54, "seek": 33248, "start": 337.28000000000003, "end": 345.20000000000005, "text": " view synthesis. So how do we take some sparsely sampled input views of a scene and then construct", "tokens": [50604, 1910, 30252, 13, 407, 577, 360, 321, 747, 512, 637, 685, 736, 3247, 15551, 4846, 6809, 295, 257, 4145, 293, 550, 7690, 51000], "temperature": 0.0, "avg_logprob": -0.10953981499922903, "compression_ratio": 1.549800796812749, "no_speech_prob": 0.000687499821651727}, {"id": 55, "seek": 33248, "start": 345.20000000000005, "end": 350.96000000000004, "text": " a representation of that scene's 3D geometry and colour in a way that allows us to render it from", "tokens": [51000, 257, 10290, 295, 300, 4145, 311, 805, 35, 18426, 293, 8267, 294, 257, 636, 300, 4045, 505, 281, 15529, 309, 490, 51288], "temperature": 0.0, "avg_logprob": -0.10953981499922903, "compression_ratio": 1.549800796812749, "no_speech_prob": 0.000687499821651727}, {"id": 56, "seek": 33248, "start": 350.96000000000004, "end": 356.8, "text": " new perspectives? These are some example works. You can represent the scene as a multi-plane image.", "tokens": [51288, 777, 16766, 30, 1981, 366, 512, 1365, 1985, 13, 509, 393, 2906, 264, 4145, 382, 257, 4825, 12, 36390, 3256, 13, 51580], "temperature": 0.0, "avg_logprob": -0.10953981499922903, "compression_ratio": 1.549800796812749, "no_speech_prob": 0.000687499821651727}, {"id": 57, "seek": 35680, "start": 356.88, "end": 363.76, "text": " So instead of a flat grid of RGB values represented as multiple planes, and that allows very quick", "tokens": [50368, 407, 2602, 295, 257, 4962, 10748, 295, 31231, 4190, 10379, 382, 3866, 14952, 11, 293, 300, 4045, 588, 1702, 50712], "temperature": 0.0, "avg_logprob": -0.1445657836066352, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.007812968455255032}, {"id": 58, "seek": 35680, "start": 364.72, "end": 371.12, "text": " rendering from new perspectives. Neural volumes is an approach from Facebook that has an encoder", "tokens": [50760, 22407, 490, 777, 16766, 13, 1734, 1807, 22219, 307, 364, 3109, 490, 4384, 300, 575, 364, 2058, 19866, 51080], "temperature": 0.0, "avg_logprob": -0.1445657836066352, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.007812968455255032}, {"id": 59, "seek": 35680, "start": 371.12, "end": 376.56, "text": " decoder structure. Take some input images and encode them into a layman space, kind of a 3D", "tokens": [51080, 979, 19866, 3877, 13, 3664, 512, 4846, 5267, 293, 2058, 1429, 552, 666, 257, 2360, 1601, 1901, 11, 733, 295, 257, 805, 35, 51352], "temperature": 0.0, "avg_logprob": -0.1445657836066352, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.007812968455255032}, {"id": 60, "seek": 35680, "start": 376.56, "end": 383.6, "text": " layman space and decode it out to images with volume rendering. Neural radian fields have", "tokens": [51352, 2360, 1601, 1901, 293, 979, 1429, 309, 484, 281, 5267, 365, 5523, 22407, 13, 1734, 1807, 2843, 952, 7909, 362, 51704], "temperature": 0.0, "avg_logprob": -0.1445657836066352, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.007812968455255032}, {"id": 61, "seek": 38360, "start": 383.6, "end": 389.28000000000003, "text": " really been very popular over the last two years due to their simplicity and quality of the results", "tokens": [50364, 534, 668, 588, 3743, 670, 264, 1036, 732, 924, 3462, 281, 641, 25632, 293, 3125, 295, 264, 3542, 50648], "temperature": 0.0, "avg_logprob": -0.10733677620111509, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0007094325264915824}, {"id": 62, "seek": 38360, "start": 389.28000000000003, "end": 396.24, "text": " they can generate. So here's an example scene that's captured on the Berkeley campus. Some photos of", "tokens": [50648, 436, 393, 8460, 13, 407, 510, 311, 364, 1365, 4145, 300, 311, 11828, 322, 264, 23684, 4828, 13, 2188, 5787, 295, 50996], "temperature": 0.0, "avg_logprob": -0.10733677620111509, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0007094325264915824}, {"id": 63, "seek": 38360, "start": 396.24, "end": 400.0, "text": " the scene are captured, for example, with an iPhone. I believe these are captured with an iPhone.", "tokens": [50996, 264, 4145, 366, 11828, 11, 337, 1365, 11, 365, 364, 7252, 13, 286, 1697, 613, 366, 11828, 365, 364, 7252, 13, 51184], "temperature": 0.0, "avg_logprob": -0.10733677620111509, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0007094325264915824}, {"id": 64, "seek": 38360, "start": 402.08000000000004, "end": 408.16, "text": " Then poses for each photo are estimated. And this neural scene representation called the neural", "tokens": [51288, 1396, 26059, 337, 1184, 5052, 366, 14109, 13, 400, 341, 18161, 4145, 10290, 1219, 264, 18161, 51592], "temperature": 0.0, "avg_logprob": -0.10733677620111509, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.0007094325264915824}, {"id": 65, "seek": 40816, "start": 408.16, "end": 413.92, "text": " radian field is estimated off of those images. What's really nice is once we estimate the", "tokens": [50364, 2843, 952, 2519, 307, 14109, 766, 295, 729, 5267, 13, 708, 311, 534, 1481, 307, 1564, 321, 12539, 264, 50652], "temperature": 0.0, "avg_logprob": -0.10752083914620536, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0006261736270971596}, {"id": 66, "seek": 40816, "start": 413.92, "end": 418.48, "text": " representation of the scene, we can render it from novel viewpoints and kind of smoothly", "tokens": [50652, 10290, 295, 264, 4145, 11, 321, 393, 15529, 309, 490, 7613, 1910, 20552, 293, 733, 295, 19565, 50880], "temperature": 0.0, "avg_logprob": -0.10752083914620536, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0006261736270971596}, {"id": 67, "seek": 40816, "start": 418.48, "end": 423.84000000000003, "text": " interpolate these sparsely sampled views. The scene is only very sparsely observed from discrete", "tokens": [50880, 44902, 473, 613, 637, 685, 736, 3247, 15551, 6809, 13, 440, 4145, 307, 787, 588, 637, 685, 736, 13095, 490, 27706, 51148], "temperature": 0.0, "avg_logprob": -0.10752083914620536, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0006261736270971596}, {"id": 68, "seek": 40816, "start": 423.84000000000003, "end": 427.76000000000005, "text": " points. What if you as the user want to make a photo from a new perspective?", "tokens": [51148, 2793, 13, 708, 498, 291, 382, 264, 4195, 528, 281, 652, 257, 5052, 490, 257, 777, 4585, 30, 51344], "temperature": 0.0, "avg_logprob": -0.10752083914620536, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0006261736270971596}, {"id": 69, "seek": 40816, "start": 429.44000000000005, "end": 435.28000000000003, "text": " There's some interesting things to note about this rendering. Notice the specularities on the", "tokens": [51428, 821, 311, 512, 1880, 721, 281, 3637, 466, 341, 22407, 13, 13428, 264, 1608, 1040, 1088, 322, 264, 51720], "temperature": 0.0, "avg_logprob": -0.10752083914620536, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.0006261736270971596}, {"id": 70, "seek": 43528, "start": 435.28, "end": 440.88, "text": " surface. They're not just modeling the diffuse light. Also modeling how the light reflected", "tokens": [50364, 3753, 13, 814, 434, 406, 445, 15983, 264, 42165, 1442, 13, 2743, 15983, 577, 264, 1442, 15502, 50644], "temperature": 0.0, "avg_logprob": -0.12965294447812167, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0001794946874724701}, {"id": 71, "seek": 43528, "start": 440.88, "end": 446.96, "text": " back at the user depends on the viewpoint of the camera. As you shift your head, the scene will", "tokens": [50644, 646, 412, 264, 4195, 5946, 322, 264, 35248, 295, 264, 2799, 13, 1018, 291, 5513, 428, 1378, 11, 264, 4145, 486, 50948], "temperature": 0.0, "avg_logprob": -0.12965294447812167, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0001794946874724701}, {"id": 72, "seek": 43528, "start": 446.96, "end": 454.71999999999997, "text": " change. This is particularly visible on very shiny surfaces like the glass or metal of the car.", "tokens": [50948, 1319, 13, 639, 307, 4098, 8974, 322, 588, 16997, 16130, 411, 264, 4276, 420, 5760, 295, 264, 1032, 13, 51336], "temperature": 0.0, "avg_logprob": -0.12965294447812167, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0001794946874724701}, {"id": 73, "seek": 43528, "start": 458.0, "end": 464.64, "text": " A neural radian field, yeah, it really is amazing and really captured the attention of a lot of", "tokens": [51500, 316, 18161, 2843, 952, 2519, 11, 1338, 11, 309, 534, 307, 2243, 293, 534, 11828, 264, 3202, 295, 257, 688, 295, 51832], "temperature": 0.0, "avg_logprob": -0.12965294447812167, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0001794946874724701}, {"id": 74, "seek": 46464, "start": 464.64, "end": 471.03999999999996, "text": " people. This neural radian field has grown very quickly and there's still a lot of problems to be", "tokens": [50364, 561, 13, 639, 18161, 2843, 952, 2519, 575, 7709, 588, 2661, 293, 456, 311, 920, 257, 688, 295, 2740, 281, 312, 50684], "temperature": 0.0, "avg_logprob": -0.1445740352977406, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00015829790208954364}, {"id": 75, "seek": 46464, "start": 471.03999999999996, "end": 476.64, "text": " solved. One very interesting thing that these neural scene representations bring to mind is", "tokens": [50684, 13041, 13, 1485, 588, 1880, 551, 300, 613, 18161, 4145, 33358, 1565, 281, 1575, 307, 50964], "temperature": 0.0, "avg_logprob": -0.1445740352977406, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00015829790208954364}, {"id": 76, "seek": 46464, "start": 476.64, "end": 481.68, "text": " that we're encoding a scene in the neural network's weights. Instead of explicitly encoding the", "tokens": [50964, 300, 321, 434, 43430, 257, 4145, 294, 264, 18161, 3209, 311, 17443, 13, 7156, 295, 20803, 43430, 264, 51216], "temperature": 0.0, "avg_logprob": -0.1445740352977406, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00015829790208954364}, {"id": 77, "seek": 46464, "start": 481.68, "end": 488.15999999999997, "text": " geometry of the scene via points or meshes, lists of triangles or voxel grids, it's encoded", "tokens": [51216, 18426, 295, 264, 4145, 5766, 2793, 420, 3813, 8076, 11, 14511, 295, 29896, 420, 1650, 87, 338, 677, 3742, 11, 309, 311, 2058, 12340, 51540], "temperature": 0.0, "avg_logprob": -0.1445740352977406, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00015829790208954364}, {"id": 78, "seek": 48816, "start": 488.88000000000005, "end": 495.44, "text": " into this small multi-layer perceptron. Maybe this is a half a million parameter network,", "tokens": [50400, 666, 341, 1359, 4825, 12, 8376, 260, 43276, 2044, 13, 2704, 341, 307, 257, 1922, 257, 2459, 13075, 3209, 11, 50728], "temperature": 0.0, "avg_logprob": -0.1585448824841043, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.00263059139251709}, {"id": 79, "seek": 48816, "start": 495.44, "end": 501.44000000000005, "text": " just some stacks of dense layers. It's representing a function mapping from 3D space coordinates,", "tokens": [50728, 445, 512, 30792, 295, 18011, 7914, 13, 467, 311, 13460, 257, 2445, 18350, 490, 805, 35, 1901, 21056, 11, 51028], "temperature": 0.0, "avg_logprob": -0.1585448824841043, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.00263059139251709}, {"id": 80, "seek": 48816, "start": 502.16, "end": 508.48, "text": " XYZ. This is in the scene, XYZ coordinates, and a viewing direction. What is the angle of the camera", "tokens": [51064, 48826, 57, 13, 639, 307, 294, 264, 4145, 11, 48826, 57, 21056, 11, 293, 257, 17480, 3513, 13, 708, 307, 264, 5802, 295, 264, 2799, 51380], "temperature": 0.0, "avg_logprob": -0.1585448824841043, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.00263059139251709}, {"id": 81, "seek": 48816, "start": 508.48, "end": 514.5600000000001, "text": " in order to model those view dependent effects? The neural network then predicts at this coordinate", "tokens": [51380, 294, 1668, 281, 2316, 729, 1910, 12334, 5065, 30, 440, 18161, 3209, 550, 6069, 82, 412, 341, 15670, 51684], "temperature": 0.0, "avg_logprob": -0.1585448824841043, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.00263059139251709}, {"id": 82, "seek": 51456, "start": 514.56, "end": 520.0, "text": " what is the color of the scene and then its density, sigma. There's density as something like", "tokens": [50364, 437, 307, 264, 2017, 295, 264, 4145, 293, 550, 1080, 10305, 11, 12771, 13, 821, 311, 10305, 382, 746, 411, 50636], "temperature": 0.0, "avg_logprob": -0.1151310920715332, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0001233805378433317}, {"id": 83, "seek": 51456, "start": 520.0, "end": 523.5999999999999, "text": " how solid is the object and how much light will be absorbed.", "tokens": [50636, 577, 5100, 307, 264, 2657, 293, 577, 709, 1442, 486, 312, 20799, 13, 50816], "temperature": 0.0, "avg_logprob": -0.1151310920715332, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0001233805378433317}, {"id": 84, "seek": 51456, "start": 528.4799999999999, "end": 534.3199999999999, "text": " Rendering is done by ray tracing. This is not exactly what would be done in most graphics", "tokens": [51060, 48174, 1794, 307, 1096, 538, 18592, 25262, 13, 639, 307, 406, 2293, 437, 576, 312, 1096, 294, 881, 11837, 51352], "temperature": 0.0, "avg_logprob": -0.1151310920715332, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0001233805378433317}, {"id": 85, "seek": 51456, "start": 534.3199999999999, "end": 541.4399999999999, "text": " applications like real-time ray tracing because we've kind of encoded the light being reflected", "tokens": [51352, 5821, 411, 957, 12, 3766, 18592, 25262, 570, 321, 600, 733, 295, 2058, 12340, 264, 1442, 885, 15502, 51708], "temperature": 0.0, "avg_logprob": -0.1151310920715332, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0001233805378433317}, {"id": 86, "seek": 54144, "start": 541.44, "end": 547.2, "text": " at any given point back towards the viewer into this function. So we don't have to scatter", "tokens": [50364, 412, 604, 2212, 935, 646, 3030, 264, 16767, 666, 341, 2445, 13, 407, 321, 500, 380, 362, 281, 34951, 50652], "temperature": 0.0, "avg_logprob": -0.07616329193115234, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0007319921860471368}, {"id": 87, "seek": 54144, "start": 547.2, "end": 552.96, "text": " light through the scene. The viewer will cast a ray from their camera through the pixel. This is the", "tokens": [50652, 1442, 807, 264, 4145, 13, 440, 16767, 486, 4193, 257, 18592, 490, 641, 2799, 807, 264, 19261, 13, 639, 307, 264, 50940], "temperature": 0.0, "avg_logprob": -0.07616329193115234, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0007319921860471368}, {"id": 88, "seek": 54144, "start": 552.96, "end": 559.2800000000001, "text": " image plane into the scene and then query the neural network along the ray. These are different", "tokens": [50940, 3256, 5720, 666, 264, 4145, 293, 550, 14581, 264, 18161, 3209, 2051, 264, 18592, 13, 1981, 366, 819, 51256], "temperature": 0.0, "avg_logprob": -0.07616329193115234, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0007319921860471368}, {"id": 89, "seek": 54144, "start": 559.2800000000001, "end": 566.08, "text": " 3D coordinates in the scene. The color of the rendered pixel will then be some accumulation", "tokens": [51256, 805, 35, 21056, 294, 264, 4145, 13, 440, 2017, 295, 264, 28748, 19261, 486, 550, 312, 512, 35647, 51596], "temperature": 0.0, "avg_logprob": -0.07616329193115234, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0007319921860471368}, {"id": 90, "seek": 54144, "start": 566.08, "end": 570.6400000000001, "text": " of the colors along that ray. In order to determine the color and the density along the ray,", "tokens": [51596, 295, 264, 4577, 2051, 300, 18592, 13, 682, 1668, 281, 6997, 264, 2017, 293, 264, 10305, 2051, 264, 18592, 11, 51824], "temperature": 0.0, "avg_logprob": -0.07616329193115234, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0007319921860471368}, {"id": 91, "seek": 57064, "start": 570.64, "end": 574.08, "text": " each of these coordinates is passed to the MLP as a very large batch.", "tokens": [50364, 1184, 295, 613, 21056, 307, 4678, 281, 264, 21601, 47, 382, 257, 588, 2416, 15245, 13, 50536], "temperature": 0.0, "avg_logprob": -0.12760852905641118, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.00018808369350153953}, {"id": 92, "seek": 57064, "start": 575.92, "end": 581.6, "text": " We get a color and density for each coordinate and then can compose them with alpha compositing", "tokens": [50628, 492, 483, 257, 2017, 293, 10305, 337, 1184, 15670, 293, 550, 393, 35925, 552, 365, 8961, 10199, 1748, 50912], "temperature": 0.0, "avg_logprob": -0.12760852905641118, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.00018808369350153953}, {"id": 93, "seek": 57064, "start": 581.6, "end": 587.76, "text": " into a color. There's some subtlety to this compositing. This is called the volume rendering", "tokens": [50912, 666, 257, 2017, 13, 821, 311, 512, 7257, 75, 2210, 281, 341, 10199, 1748, 13, 639, 307, 1219, 264, 5523, 22407, 51220], "temperature": 0.0, "avg_logprob": -0.12760852905641118, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.00018808369350153953}, {"id": 94, "seek": 57064, "start": 587.76, "end": 596.48, "text": " equation because this equation is pretty simple. This is the density predicted. This is the camera", "tokens": [51220, 5367, 570, 341, 5367, 307, 1238, 2199, 13, 639, 307, 264, 10305, 19147, 13, 639, 307, 264, 2799, 51656], "temperature": 0.0, "avg_logprob": -0.12760852905641118, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.00018808369350153953}, {"id": 95, "seek": 59648, "start": 596.48, "end": 602.96, "text": " origin and it's displaced some steps along the ray. The neural network will predict what is the", "tokens": [50364, 4957, 293, 309, 311, 33692, 512, 4439, 2051, 264, 18592, 13, 440, 18161, 3209, 486, 6069, 437, 307, 264, 50688], "temperature": 0.0, "avg_logprob": -0.17010877245948428, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0009695433545857668}, {"id": 96, "seek": 59648, "start": 602.96, "end": 607.76, "text": " density of the scene at that point, but it will also predict what is color. Then we're integrating", "tokens": [50688, 10305, 295, 264, 4145, 412, 300, 935, 11, 457, 309, 486, 611, 6069, 437, 307, 2017, 13, 1396, 321, 434, 26889, 50928], "temperature": 0.0, "avg_logprob": -0.17010877245948428, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0009695433545857668}, {"id": 97, "seek": 59648, "start": 607.76, "end": 612.16, "text": " this color along the ray weighted by its density, but we also have to weight it by", "tokens": [50928, 341, 2017, 2051, 264, 18592, 32807, 538, 1080, 10305, 11, 457, 321, 611, 362, 281, 3364, 309, 538, 51148], "temperature": 0.0, "avg_logprob": -0.17010877245948428, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0009695433545857668}, {"id": 98, "seek": 59648, "start": 612.16, "end": 619.52, "text": " transmittance, which is roughly speaking how much light is transmitted from the viewer", "tokens": [51148, 7715, 593, 719, 11, 597, 307, 9810, 4124, 577, 709, 1442, 307, 25355, 490, 264, 16767, 51516], "temperature": 0.0, "avg_logprob": -0.17010877245948428, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0009695433545857668}, {"id": 99, "seek": 61952, "start": 619.6, "end": 626.56, "text": " to that point along the ray because once we've accumulated enough density, then objects", "tokens": [50368, 281, 300, 935, 2051, 264, 18592, 570, 1564, 321, 600, 31346, 1547, 10305, 11, 550, 6565, 50716], "temperature": 0.0, "avg_logprob": -0.12889829048743615, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.0017001718515530229}, {"id": 100, "seek": 61952, "start": 626.56, "end": 633.04, "text": " further back in the scene will not be visible to be included. This equation for color", "tokens": [50716, 3052, 646, 294, 264, 4145, 486, 406, 312, 8974, 281, 312, 5556, 13, 639, 5367, 337, 2017, 51040], "temperature": 0.0, "avg_logprob": -0.12889829048743615, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.0017001718515530229}, {"id": 101, "seek": 61952, "start": 634.3199999999999, "end": 640.16, "text": " conditioned on coordinates is differentiable with respect to the parameters of sigma and c.", "tokens": [51104, 35833, 322, 21056, 307, 819, 9364, 365, 3104, 281, 264, 9834, 295, 12771, 293, 269, 13, 51396], "temperature": 0.0, "avg_logprob": -0.12889829048743615, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.0017001718515530229}, {"id": 102, "seek": 61952, "start": 640.16, "end": 646.72, "text": " So sigma and c will be this neural network. Because this is fully differentiable, it's", "tokens": [51396, 407, 12771, 293, 269, 486, 312, 341, 18161, 3209, 13, 1436, 341, 307, 4498, 819, 9364, 11, 309, 311, 51724], "temperature": 0.0, "avg_logprob": -0.12889829048743615, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.0017001718515530229}, {"id": 103, "seek": 64672, "start": 646.72, "end": 651.12, "text": " relatively easy to optimize. Instead of optimizing scene geometry, we'll optimize the", "tokens": [50364, 7226, 1858, 281, 19719, 13, 7156, 295, 40425, 4145, 18426, 11, 321, 603, 19719, 264, 50584], "temperature": 0.0, "avg_logprob": -0.14140645740101637, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.000368207081919536}, {"id": 104, "seek": 64672, "start": 651.12, "end": 658.64, "text": " weights of this neural network in order to get some desired colors. This might be the sparsely", "tokens": [50584, 17443, 295, 341, 18161, 3209, 294, 1668, 281, 483, 512, 14721, 4577, 13, 639, 1062, 312, 264, 637, 685, 736, 50960], "temperature": 0.0, "avg_logprob": -0.14140645740101637, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.000368207081919536}, {"id": 105, "seek": 64672, "start": 658.64, "end": 664.24, "text": " observed viewpoints, two viewpoints. Let's render the color according to the neural scene", "tokens": [50960, 13095, 1910, 20552, 11, 732, 1910, 20552, 13, 961, 311, 15529, 264, 2017, 4650, 281, 264, 18161, 4145, 51240], "temperature": 0.0, "avg_logprob": -0.14140645740101637, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.000368207081919536}, {"id": 106, "seek": 64672, "start": 664.24, "end": 668.1600000000001, "text": " representation and then try to optimize the network so that it matches the observed views,", "tokens": [51240, 10290, 293, 550, 853, 281, 19719, 264, 3209, 370, 300, 309, 10676, 264, 13095, 6809, 11, 51436], "temperature": 0.0, "avg_logprob": -0.14140645740101637, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.000368207081919536}, {"id": 107, "seek": 64672, "start": 668.1600000000001, "end": 675.36, "text": " pixel by pixel. It might take a minute to wrap your head around, but it's actually pretty simple.", "tokens": [51436, 19261, 538, 19261, 13, 467, 1062, 747, 257, 3456, 281, 7019, 428, 1378, 926, 11, 457, 309, 311, 767, 1238, 2199, 13, 51796], "temperature": 0.0, "avg_logprob": -0.14140645740101637, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.000368207081919536}, {"id": 108, "seek": 67536, "start": 675.36, "end": 680.88, "text": " We have this one MLP that encodes the scene, lets us render viewpoints differentially,", "tokens": [50364, 492, 362, 341, 472, 21601, 47, 300, 2058, 4789, 264, 4145, 11, 6653, 505, 15529, 1910, 20552, 819, 2270, 11, 50640], "temperature": 0.0, "avg_logprob": -0.1530201842145222, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.0006261400412768126}, {"id": 109, "seek": 67536, "start": 680.88, "end": 686.08, "text": " and we'll optimize the scene so that it matches the input views, and that's why it's inverse", "tokens": [50640, 293, 321, 603, 19719, 264, 4145, 370, 300, 309, 10676, 264, 4846, 6809, 11, 293, 300, 311, 983, 309, 311, 17340, 50900], "temperature": 0.0, "avg_logprob": -0.1530201842145222, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.0006261400412768126}, {"id": 110, "seek": 67536, "start": 686.08, "end": 691.92, "text": " graphics. We're going from the 2D space to optimize for the underlying 3D representation", "tokens": [50900, 11837, 13, 492, 434, 516, 490, 264, 568, 35, 1901, 281, 19719, 337, 264, 14217, 805, 35, 10290, 51192], "temperature": 0.0, "avg_logprob": -0.1530201842145222, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.0006261400412768126}, {"id": 111, "seek": 67536, "start": 691.92, "end": 698.32, "text": " that will reconstruct those views. Are there any questions about that?", "tokens": [51192, 300, 486, 31499, 729, 6809, 13, 2014, 456, 604, 1651, 466, 300, 30, 51512], "temperature": 0.0, "avg_logprob": -0.1530201842145222, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.0006261400412768126}, {"id": 112, "seek": 69832, "start": 699.2, "end": 710.48, "text": " Can you talk about how the points are used for the neural net? I can't see it directly.", "tokens": [50408, 1664, 291, 751, 466, 577, 264, 2793, 366, 1143, 337, 264, 18161, 2533, 30, 286, 393, 380, 536, 309, 3838, 13, 50972], "temperature": 0.0, "avg_logprob": -0.20579347610473633, "compression_ratio": 1.5317919075144508, "no_speech_prob": 0.03249550238251686}, {"id": 113, "seek": 69832, "start": 712.32, "end": 720.08, "text": " I mean, I get the high-level idea, but could you talk about how those points are fed into the net?", "tokens": [51064, 286, 914, 11, 286, 483, 264, 1090, 12, 12418, 1558, 11, 457, 727, 291, 751, 466, 577, 729, 2793, 366, 4636, 666, 264, 2533, 30, 51452], "temperature": 0.0, "avg_logprob": -0.20579347610473633, "compression_ratio": 1.5317919075144508, "no_speech_prob": 0.03249550238251686}, {"id": 114, "seek": 69832, "start": 721.0400000000001, "end": 725.36, "text": " Yeah, so you could consider constructing an MLP with five dimensions as input,", "tokens": [51500, 865, 11, 370, 291, 727, 1949, 39969, 364, 21601, 47, 365, 1732, 12819, 382, 4846, 11, 51716], "temperature": 0.0, "avg_logprob": -0.20579347610473633, "compression_ratio": 1.5317919075144508, "no_speech_prob": 0.03249550238251686}, {"id": 115, "seek": 72536, "start": 725.92, "end": 731.28, "text": " just five inputs, and then four outputs on the layers, and then intermediate features or whatever", "tokens": [50392, 445, 1732, 15743, 11, 293, 550, 1451, 23930, 322, 264, 7914, 11, 293, 550, 19376, 4122, 420, 2035, 50660], "temperature": 0.0, "avg_logprob": -0.1995169607441077, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.001133274519816041}, {"id": 116, "seek": 72536, "start": 731.28, "end": 737.6800000000001, "text": " you can imagine you want. So in nerf it's 256, that would be one approach, and it does work,", "tokens": [50660, 291, 393, 3811, 291, 528, 13, 407, 294, 18219, 69, 309, 311, 38882, 11, 300, 576, 312, 472, 3109, 11, 293, 309, 775, 589, 11, 50980], "temperature": 0.0, "avg_logprob": -0.1995169607441077, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.001133274519816041}, {"id": 117, "seek": 72536, "start": 737.6800000000001, "end": 743.2, "text": " but then you get actually quite blurry reconstructions of the scene if you directly feed an input", "tokens": [50980, 457, 550, 291, 483, 767, 1596, 37644, 31499, 626, 295, 264, 4145, 498, 291, 3838, 3154, 364, 4846, 51256], "temperature": 0.0, "avg_logprob": -0.1995169607441077, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.001133274519816041}, {"id": 118, "seek": 72536, "start": 743.2, "end": 747.6, "text": " coordinates as these are just floating point numbers, 3D coordinates, and going that direction.", "tokens": [51256, 21056, 382, 613, 366, 445, 12607, 935, 3547, 11, 805, 35, 21056, 11, 293, 516, 300, 3513, 13, 51476], "temperature": 0.0, "avg_logprob": -0.1995169607441077, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.001133274519816041}, {"id": 119, "seek": 74760, "start": 747.9200000000001, "end": 754.64, "text": " But instead, what is used in this neural radiance field is assigned useoidal", "tokens": [50380, 583, 2602, 11, 437, 307, 1143, 294, 341, 18161, 2843, 6276, 2519, 307, 13279, 764, 17079, 304, 50716], "temperature": 0.0, "avg_logprob": -0.22017375125160701, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0015974603593349457}, {"id": 120, "seek": 74760, "start": 754.64, "end": 761.52, "text": " positional encoding, so frequency-based encoding. If you're familiar with the transformer positional", "tokens": [50716, 2535, 304, 43430, 11, 370, 7893, 12, 6032, 43430, 13, 759, 291, 434, 4963, 365, 264, 31782, 2535, 304, 51060], "temperature": 0.0, "avg_logprob": -0.22017375125160701, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0015974603593349457}, {"id": 121, "seek": 74760, "start": 761.52, "end": 769.36, "text": " encoding, this is a common approach where continuous values like coordinates or time", "tokens": [51060, 43430, 11, 341, 307, 257, 2689, 3109, 689, 10957, 4190, 411, 21056, 420, 565, 51452], "temperature": 0.0, "avg_logprob": -0.22017375125160701, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0015974603593349457}, {"id": 122, "seek": 74760, "start": 769.36, "end": 777.36, "text": " steps are encoded using a Fourier representation. So you take sine of various scaled values of the", "tokens": [51452, 4439, 366, 2058, 12340, 1228, 257, 36810, 10290, 13, 407, 291, 747, 18609, 295, 3683, 36039, 4190, 295, 264, 51852], "temperature": 0.0, "avg_logprob": -0.22017375125160701, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0015974603593349457}, {"id": 123, "seek": 77736, "start": 777.44, "end": 780.4, "text": " input coordinates, and that lets the network model high-frequency detail.", "tokens": [50368, 4846, 21056, 11, 293, 300, 6653, 264, 3209, 2316, 1090, 12, 19325, 48154, 2607, 13, 50516], "temperature": 0.0, "avg_logprob": -0.1573810699658516, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.0003796546079684049}, {"id": 124, "seek": 77736, "start": 781.6800000000001, "end": 786.72, "text": " So instead of kind of memorizing a function from each spatial coordinate, it can model", "tokens": [50580, 407, 2602, 295, 733, 295, 10560, 3319, 257, 2445, 490, 1184, 23598, 15670, 11, 309, 393, 2316, 50832], "temperature": 0.0, "avg_logprob": -0.1573810699658516, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.0003796546079684049}, {"id": 125, "seek": 77736, "start": 789.92, "end": 794.08, "text": " frequencies of the underlying signal if you use the sine useoidal embedding of the input.", "tokens": [50992, 20250, 295, 264, 14217, 6358, 498, 291, 764, 264, 18609, 764, 17079, 304, 12240, 3584, 295, 264, 4846, 13, 51200], "temperature": 0.0, "avg_logprob": -0.1573810699658516, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.0003796546079684049}, {"id": 126, "seek": 77736, "start": 794.08, "end": 798.48, "text": " So that kind of just projects this five-dimensional input into some higher-dimensional space", "tokens": [51200, 407, 300, 733, 295, 445, 4455, 341, 1732, 12, 18759, 4846, 666, 512, 2946, 12, 18759, 1901, 51420], "temperature": 0.0, "avg_logprob": -0.1573810699658516, "compression_ratio": 1.6650485436893203, "no_speech_prob": 0.0003796546079684049}, {"id": 127, "seek": 79848, "start": 798.48, "end": 804.24, "text": " before feeding it to the MLP. I see. And there's no, let's say, I guess, filtering", "tokens": [50364, 949, 12919, 309, 281, 264, 21601, 47, 13, 286, 536, 13, 400, 456, 311, 572, 11, 718, 311, 584, 11, 286, 2041, 11, 30822, 50652], "temperature": 0.0, "avg_logprob": -0.21067252210391466, "compression_ratio": 1.685, "no_speech_prob": 0.04668084532022476}, {"id": 128, "seek": 79848, "start": 804.24, "end": 813.44, "text": " done before, I mean, applying it to the net. So it's just transforming certain, I guess,", "tokens": [50652, 1096, 949, 11, 286, 914, 11, 9275, 309, 281, 264, 2533, 13, 407, 309, 311, 445, 27210, 1629, 11, 286, 2041, 11, 51112], "temperature": 0.0, "avg_logprob": -0.21067252210391466, "compression_ratio": 1.685, "no_speech_prob": 0.04668084532022476}, {"id": 129, "seek": 79848, "start": 813.44, "end": 818.88, "text": " components, but not doing some, I guess, post-processing before putting it into,", "tokens": [51112, 6677, 11, 457, 406, 884, 512, 11, 286, 2041, 11, 2183, 12, 41075, 278, 949, 3372, 309, 666, 11, 51384], "temperature": 0.0, "avg_logprob": -0.21067252210391466, "compression_ratio": 1.685, "no_speech_prob": 0.04668084532022476}, {"id": 130, "seek": 79848, "start": 818.88, "end": 823.52, "text": " or let's say compression or something like that. No, not really compression. There's", "tokens": [51384, 420, 718, 311, 584, 19355, 420, 746, 411, 300, 13, 883, 11, 406, 534, 19355, 13, 821, 311, 51616], "temperature": 0.0, "avg_logprob": -0.21067252210391466, "compression_ratio": 1.685, "no_speech_prob": 0.04668084532022476}, {"id": 131, "seek": 82352, "start": 823.6, "end": 828.96, "text": " some, like, coordinate transform because you'll do this computation in a particular coordinate frame.", "tokens": [50368, 512, 11, 411, 11, 15670, 4088, 570, 291, 603, 360, 341, 24903, 294, 257, 1729, 15670, 3920, 13, 50636], "temperature": 0.0, "avg_logprob": -0.1293871795074849, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00012728651927318424}, {"id": 132, "seek": 82352, "start": 831.36, "end": 837.92, "text": " There is some subsequent work which we actually build upon that does a pre-filtering of the", "tokens": [50756, 821, 307, 512, 19962, 589, 597, 321, 767, 1322, 3564, 300, 775, 257, 659, 12, 19776, 34200, 295, 264, 51084], "temperature": 0.0, "avg_logprob": -0.1293871795074849, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00012728651927318424}, {"id": 133, "seek": 82352, "start": 837.92, "end": 844.8, "text": " input coordinates. So instead of encoding all the frequencies of the input coordinates,", "tokens": [51084, 4846, 21056, 13, 407, 2602, 295, 43430, 439, 264, 20250, 295, 264, 4846, 21056, 11, 51428], "temperature": 0.0, "avg_logprob": -0.1293871795074849, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00012728651927318424}, {"id": 134, "seek": 82352, "start": 844.8, "end": 847.92, "text": " they'll be blurred depending on how far away from the camera you're querying.", "tokens": [51428, 436, 603, 312, 43525, 5413, 322, 577, 1400, 1314, 490, 264, 2799, 291, 434, 7083, 1840, 13, 51584], "temperature": 0.0, "avg_logprob": -0.1293871795074849, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00012728651927318424}, {"id": 135, "seek": 84792, "start": 848.88, "end": 853.68, "text": " But that's sort of subsequent work to nerve. That reduces the aliasing.", "tokens": [50412, 583, 300, 311, 1333, 295, 19962, 589, 281, 16355, 13, 663, 18081, 264, 10198, 3349, 13, 50652], "temperature": 0.0, "avg_logprob": -0.34073101846795334, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.0009108106605708599}, {"id": 136, "seek": 84792, "start": 855.52, "end": 857.68, "text": " Let's see. And the network is just fully connected.", "tokens": [50744, 961, 311, 536, 13, 400, 264, 3209, 307, 445, 4498, 4582, 13, 50852], "temperature": 0.0, "avg_logprob": -0.34073101846795334, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.0009108106605708599}, {"id": 137, "seek": 84792, "start": 858.3199999999999, "end": 861.52, "text": " Yeah, just a fully connected network. Super simple.", "tokens": [50884, 865, 11, 445, 257, 4498, 4582, 3209, 13, 4548, 2199, 13, 51044], "temperature": 0.0, "avg_logprob": -0.34073101846795334, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.0009108106605708599}, {"id": 138, "seek": 84792, "start": 861.52, "end": 862.0799999999999, "text": " Yeah, thank you.", "tokens": [51044, 865, 11, 1309, 291, 13, 51072], "temperature": 0.0, "avg_logprob": -0.34073101846795334, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.0009108106605708599}, {"id": 139, "seek": 84792, "start": 864.0799999999999, "end": 871.4399999999999, "text": " Yeah, one more thing that I wanted to mention here for a student is that there is a difference", "tokens": [51172, 865, 11, 472, 544, 551, 300, 286, 1415, 281, 2152, 510, 337, 257, 3107, 307, 300, 456, 307, 257, 2649, 51540], "temperature": 0.0, "avg_logprob": -0.34073101846795334, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.0009108106605708599}, {"id": 140, "seek": 87144, "start": 871.5200000000001, "end": 878.48, "text": " between how you train this model versus the models that so far you have seen for,", "tokens": [50368, 1296, 577, 291, 3847, 341, 2316, 5717, 264, 5245, 300, 370, 1400, 291, 362, 1612, 337, 11, 50716], "temperature": 0.0, "avg_logprob": -0.11050023471607881, "compression_ratio": 1.751269035532995, "no_speech_prob": 0.000868422444909811}, {"id": 141, "seek": 87144, "start": 878.48, "end": 884.72, "text": " for instance, classification. For instance, if you want to train a model for a truck,", "tokens": [50716, 337, 5197, 11, 21538, 13, 1171, 5197, 11, 498, 291, 528, 281, 3847, 257, 2316, 337, 257, 5898, 11, 51028], "temperature": 0.0, "avg_logprob": -0.11050023471607881, "compression_ratio": 1.751269035532995, "no_speech_prob": 0.000868422444909811}, {"id": 142, "seek": 87144, "start": 886.1600000000001, "end": 892.5600000000001, "text": " what you do is you get a lot of images of different trucks in different lightings and different,", "tokens": [51100, 437, 291, 360, 307, 291, 483, 257, 688, 295, 5267, 295, 819, 16156, 294, 819, 1442, 1109, 293, 819, 11, 51420], "temperature": 0.0, "avg_logprob": -0.11050023471607881, "compression_ratio": 1.751269035532995, "no_speech_prob": 0.000868422444909811}, {"id": 143, "seek": 87144, "start": 894.08, "end": 899.44, "text": " you know, models and things like that, and then fit it to your network. However,", "tokens": [51496, 291, 458, 11, 5245, 293, 721, 411, 300, 11, 293, 550, 3318, 309, 281, 428, 3209, 13, 2908, 11, 51764], "temperature": 0.0, "avg_logprob": -0.11050023471607881, "compression_ratio": 1.751269035532995, "no_speech_prob": 0.000868422444909811}, {"id": 144, "seek": 89944, "start": 900.24, "end": 906.8000000000001, "text": " in this case, you are taking lots of images of the single truck, single scene,", "tokens": [50404, 294, 341, 1389, 11, 291, 366, 1940, 3195, 295, 5267, 295, 264, 2167, 5898, 11, 2167, 4145, 11, 50732], "temperature": 0.0, "avg_logprob": -0.1456170430997523, "compression_ratio": 1.65625, "no_speech_prob": 0.00011957963579334319}, {"id": 145, "seek": 89944, "start": 907.36, "end": 914.4000000000001, "text": " and you are trying to reconstruct that scene. So you said big difference between, you know,", "tokens": [50760, 293, 291, 366, 1382, 281, 31499, 300, 4145, 13, 407, 291, 848, 955, 2649, 1296, 11, 291, 458, 11, 51112], "temperature": 0.0, "avg_logprob": -0.1456170430997523, "compression_ratio": 1.65625, "no_speech_prob": 0.00011957963579334319}, {"id": 146, "seek": 89944, "start": 915.2800000000001, "end": 919.84, "text": " what you are used to doing and what we see in nerve.", "tokens": [51156, 437, 291, 366, 1143, 281, 884, 293, 437, 321, 536, 294, 16355, 13, 51384], "temperature": 0.0, "avg_logprob": -0.1456170430997523, "compression_ratio": 1.65625, "no_speech_prob": 0.00011957963579334319}, {"id": 147, "seek": 89944, "start": 922.4000000000001, "end": 928.72, "text": " Yeah, absolutely. I kind of see it as instead of, the nerve is representing a single scene. So", "tokens": [51512, 865, 11, 3122, 13, 286, 733, 295, 536, 309, 382, 2602, 295, 11, 264, 16355, 307, 13460, 257, 2167, 4145, 13, 407, 51828], "temperature": 0.0, "avg_logprob": -0.1456170430997523, "compression_ratio": 1.65625, "no_speech_prob": 0.00011957963579334319}, {"id": 148, "seek": 92872, "start": 928.72, "end": 933.44, "text": " instead of representing explicitly or representing it with a neural net with a function,", "tokens": [50364, 2602, 295, 13460, 20803, 420, 13460, 309, 365, 257, 18161, 2533, 365, 257, 2445, 11, 50600], "temperature": 0.0, "avg_logprob": -0.14104461669921875, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0001794904819689691}, {"id": 149, "seek": 92872, "start": 934.72, "end": 938.72, "text": " but it doesn't generalize. It interpolates these input views.", "tokens": [50664, 457, 309, 1177, 380, 2674, 1125, 13, 467, 44902, 1024, 613, 4846, 6809, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14104461669921875, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0001794904819689691}, {"id": 150, "seek": 92872, "start": 942.64, "end": 947.28, "text": " And, you know, there's a catch to that, which is that in order to fit into the neural radiance", "tokens": [51060, 400, 11, 291, 458, 11, 456, 311, 257, 3745, 281, 300, 11, 597, 307, 300, 294, 1668, 281, 3318, 666, 264, 18161, 2843, 6276, 51292], "temperature": 0.0, "avg_logprob": -0.14104461669921875, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0001794904819689691}, {"id": 151, "seek": 92872, "start": 947.28, "end": 953.52, "text": " field to a single scene, it generally needs a lot of data. So while these views are sampled", "tokens": [51292, 2519, 281, 257, 2167, 4145, 11, 309, 5101, 2203, 257, 688, 295, 1412, 13, 407, 1339, 613, 6809, 366, 3247, 15551, 51604], "temperature": 0.0, "avg_logprob": -0.14104461669921875, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0001794904819689691}, {"id": 152, "seek": 95352, "start": 953.6, "end": 957.52, "text": " sparsely, discreetly, and there will be larger regions of space where we don't have", "tokens": [50368, 637, 685, 736, 11, 2983, 4751, 356, 11, 293, 456, 486, 312, 4833, 10682, 295, 1901, 689, 321, 500, 380, 362, 50564], "temperature": 0.0, "avg_logprob": -0.14124065533018948, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.0015009042108431458}, {"id": 153, "seek": 95352, "start": 958.48, "end": 965.28, "text": " an image taken from that perspective, still to estimate a multi-view consistent radiance field,", "tokens": [50612, 364, 3256, 2726, 490, 300, 4585, 11, 920, 281, 12539, 257, 4825, 12, 1759, 8398, 2843, 6276, 2519, 11, 50952], "temperature": 0.0, "avg_logprob": -0.14124065533018948, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.0015009042108431458}, {"id": 154, "seek": 95352, "start": 966.0, "end": 970.72, "text": " experiments in the paper used a large number of images per scene. That's a little bit impractical.", "tokens": [50988, 12050, 294, 264, 3035, 1143, 257, 2416, 1230, 295, 5267, 680, 4145, 13, 663, 311, 257, 707, 857, 704, 1897, 804, 13, 51224], "temperature": 0.0, "avg_logprob": -0.14124065533018948, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.0015009042108431458}, {"id": 155, "seek": 95352, "start": 970.72, "end": 974.3199999999999, "text": " So for these synthetic scenes, this is one synthetic scene that's rendered in blunders.", "tokens": [51224, 407, 337, 613, 23420, 8026, 11, 341, 307, 472, 23420, 4145, 300, 311, 28748, 294, 888, 997, 433, 13, 51404], "temperature": 0.0, "avg_logprob": -0.14124065533018948, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.0015009042108431458}, {"id": 156, "seek": 95352, "start": 975.36, "end": 979.36, "text": " They were able to get out 100 images of each scene and fit the neural radiance field on it.", "tokens": [51456, 814, 645, 1075, 281, 483, 484, 2319, 5267, 295, 1184, 4145, 293, 3318, 264, 18161, 2843, 6276, 2519, 322, 309, 13, 51656], "temperature": 0.0, "avg_logprob": -0.14124065533018948, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.0015009042108431458}, {"id": 157, "seek": 97936, "start": 979.36, "end": 983.2, "text": " For those outdoor scenes, I showed earlier like that red Toyota car.", "tokens": [50364, 1171, 729, 15942, 8026, 11, 286, 4712, 3071, 411, 300, 2182, 22926, 1032, 13, 50556], "temperature": 0.0, "avg_logprob": -0.15083739390740028, "compression_ratio": 1.5289855072463767, "no_speech_prob": 0.00010887545795412734}, {"id": 158, "seek": 97936, "start": 984.48, "end": 989.6, "text": " I think it's a fewer, maybe 20 images, but still that's a lot to capture with a handheld camera.", "tokens": [50620, 286, 519, 309, 311, 257, 13366, 11, 1310, 945, 5267, 11, 457, 920, 300, 311, 257, 688, 281, 7983, 365, 257, 37634, 2799, 13, 50876], "temperature": 0.0, "avg_logprob": -0.15083739390740028, "compression_ratio": 1.5289855072463767, "no_speech_prob": 0.00010887545795412734}, {"id": 159, "seek": 97936, "start": 991.6, "end": 996.4, "text": " And in the first week of work, I'm going to talk about we improved the data efficiency of the", "tokens": [50976, 400, 294, 264, 700, 1243, 295, 589, 11, 286, 478, 516, 281, 751, 466, 321, 9689, 264, 1412, 10493, 295, 264, 51216], "temperature": 0.0, "avg_logprob": -0.15083739390740028, "compression_ratio": 1.5289855072463767, "no_speech_prob": 0.00010887545795412734}, {"id": 160, "seek": 97936, "start": 996.4, "end": 1001.76, "text": " neural radiance field training process. So instead of using, let's say 100 images on this Lego", "tokens": [51216, 18161, 2843, 6276, 2519, 3097, 1399, 13, 407, 2602, 295, 1228, 11, 718, 311, 584, 2319, 5267, 322, 341, 28761, 51484], "temperature": 0.0, "avg_logprob": -0.15083739390740028, "compression_ratio": 1.5289855072463767, "no_speech_prob": 0.00010887545795412734}, {"id": 161, "seek": 97936, "start": 1002.88, "end": 1006.72, "text": " scene, we used eight photos taken from randomly sampled viewpoints.", "tokens": [51540, 4145, 11, 321, 1143, 3180, 5787, 2726, 490, 16979, 3247, 15551, 1910, 20552, 13, 51732], "temperature": 0.0, "avg_logprob": -0.15083739390740028, "compression_ratio": 1.5289855072463767, "no_speech_prob": 0.00010887545795412734}, {"id": 162, "seek": 100936, "start": 1009.76, "end": 1015.84, "text": " In the neural radiance field training process, we would take, we would know the pose of each image", "tokens": [50384, 682, 264, 18161, 2843, 6276, 2519, 3097, 1399, 11, 321, 576, 747, 11, 321, 576, 458, 264, 10774, 295, 1184, 3256, 50688], "temperature": 0.0, "avg_logprob": -0.14460940794511276, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.00010228317842120305}, {"id": 163, "seek": 100936, "start": 1015.84, "end": 1021.36, "text": " that can be estimated with a system like call map. It's really common in the 3D graphics and 3D", "tokens": [50688, 300, 393, 312, 14109, 365, 257, 1185, 411, 818, 4471, 13, 467, 311, 534, 2689, 294, 264, 805, 35, 11837, 293, 805, 35, 50964], "temperature": 0.0, "avg_logprob": -0.14460940794511276, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.00010228317842120305}, {"id": 164, "seek": 100936, "start": 1021.36, "end": 1027.84, "text": " computer vision community is given some images, estimate their camera poses with correspondence", "tokens": [50964, 3820, 5201, 1768, 307, 2212, 512, 5267, 11, 12539, 641, 2799, 26059, 365, 38135, 51288], "temperature": 0.0, "avg_logprob": -0.14460940794511276, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.00010228317842120305}, {"id": 165, "seek": 100936, "start": 1027.84, "end": 1034.16, "text": " finding, then the neural radiance field loss renders an image or renders some rays from the", "tokens": [51288, 5006, 11, 550, 264, 18161, 2843, 6276, 2519, 4470, 6125, 433, 364, 3256, 420, 6125, 433, 512, 24417, 490, 264, 51604], "temperature": 0.0, "avg_logprob": -0.14460940794511276, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.00010228317842120305}, {"id": 166, "seek": 103416, "start": 1034.16, "end": 1041.2, "text": " same pose as the input, then it computes a mean squared error loss. So the pixel wise error.", "tokens": [50364, 912, 10774, 382, 264, 4846, 11, 550, 309, 715, 1819, 257, 914, 8889, 6713, 4470, 13, 407, 264, 19261, 10829, 6713, 13, 50716], "temperature": 0.0, "avg_logprob": -0.13071350429369055, "compression_ratio": 1.75, "no_speech_prob": 0.0003053283435292542}, {"id": 167, "seek": 103416, "start": 1042.88, "end": 1046.88, "text": " The reason that this loss can be used is because we know the camera pose, we're able to render out", "tokens": [50800, 440, 1778, 300, 341, 4470, 393, 312, 1143, 307, 570, 321, 458, 264, 2799, 10774, 11, 321, 434, 1075, 281, 15529, 484, 51000], "temperature": 0.0, "avg_logprob": -0.13071350429369055, "compression_ratio": 1.75, "no_speech_prob": 0.0003053283435292542}, {"id": 168, "seek": 103416, "start": 1046.88, "end": 1054.24, "text": " the scene from the exact same pose that the observer took the photo. If the camera poses shifted", "tokens": [51000, 264, 4145, 490, 264, 1900, 912, 10774, 300, 264, 27878, 1890, 264, 5052, 13, 759, 264, 2799, 26059, 18892, 51368], "temperature": 0.0, "avg_logprob": -0.13071350429369055, "compression_ratio": 1.75, "no_speech_prob": 0.0003053283435292542}, {"id": 169, "seek": 103416, "start": 1054.24, "end": 1059.68, "text": " in the rendering process, then the reconstructed image and the true image won't align pixel wise", "tokens": [51368, 294, 264, 22407, 1399, 11, 550, 264, 31499, 292, 3256, 293, 264, 2074, 3256, 1582, 380, 7975, 19261, 10829, 51640], "temperature": 0.0, "avg_logprob": -0.13071350429369055, "compression_ratio": 1.75, "no_speech_prob": 0.0003053283435292542}, {"id": 170, "seek": 105968, "start": 1059.68, "end": 1064.96, "text": " and we'll learn from inconsistent geometry. And so this is done at all of the observed", "tokens": [50364, 293, 321, 603, 1466, 490, 36891, 18426, 13, 400, 370, 341, 307, 1096, 412, 439, 295, 264, 13095, 50628], "temperature": 0.0, "avg_logprob": -0.11268564919445002, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.0005192095995880663}, {"id": 171, "seek": 105968, "start": 1064.96, "end": 1070.0, "text": " camera poses. And this is why the neural radiance field needs so many photos. If there's no observed", "tokens": [50628, 2799, 26059, 13, 400, 341, 307, 983, 264, 18161, 2843, 6276, 2519, 2203, 370, 867, 5787, 13, 759, 456, 311, 572, 13095, 50880], "temperature": 0.0, "avg_logprob": -0.11268564919445002, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.0005192095995880663}, {"id": 172, "seek": 105968, "start": 1070.0, "end": 1073.44, "text": " photo, then it doesn't have the ability to compute a loss from a given perspective,", "tokens": [50880, 5052, 11, 550, 309, 1177, 380, 362, 264, 3485, 281, 14722, 257, 4470, 490, 257, 2212, 4585, 11, 51052], "temperature": 0.0, "avg_logprob": -0.11268564919445002, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.0005192095995880663}, {"id": 173, "seek": 105968, "start": 1074.4, "end": 1080.72, "text": " which means that it could overfit to the input use. This representation mapping from coordinates to", "tokens": [51100, 597, 1355, 300, 309, 727, 670, 6845, 281, 264, 4846, 764, 13, 639, 10290, 18350, 490, 21056, 281, 51416], "temperature": 0.0, "avg_logprob": -0.11268564919445002, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.0005192095995880663}, {"id": 174, "seek": 105968, "start": 1080.72, "end": 1085.92, "text": " colors is very flexible. One possible degenerate solution would be to put a billboard in front", "tokens": [51416, 4577, 307, 588, 11358, 13, 1485, 1944, 40520, 473, 3827, 576, 312, 281, 829, 257, 2961, 3787, 294, 1868, 51676], "temperature": 0.0, "avg_logprob": -0.11268564919445002, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.0005192095995880663}, {"id": 175, "seek": 108592, "start": 1085.92, "end": 1091.2, "text": " of each camera, just a poster board, you know, off of the highway, right in front of your camera", "tokens": [50364, 295, 1184, 2799, 11, 445, 257, 17171, 3150, 11, 291, 458, 11, 766, 295, 264, 17205, 11, 558, 294, 1868, 295, 428, 2799, 50628], "temperature": 0.0, "avg_logprob": -0.1514162906380587, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0004441536439117044}, {"id": 176, "seek": 108592, "start": 1091.2, "end": 1097.1200000000001, "text": " containing the image that's observed, rather than learning a consistency in geometry.", "tokens": [50628, 19273, 264, 3256, 300, 311, 13095, 11, 2831, 813, 2539, 257, 14416, 294, 18426, 13, 50924], "temperature": 0.0, "avg_logprob": -0.1514162906380587, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0004441536439117044}, {"id": 177, "seek": 108592, "start": 1098.8000000000002, "end": 1105.3600000000001, "text": " And there's other ways you can get artifacts. This is described as a shape radiance ambiguity", "tokens": [51008, 400, 456, 311, 661, 2098, 291, 393, 483, 24617, 13, 639, 307, 7619, 382, 257, 3909, 2843, 6276, 46519, 51336], "temperature": 0.0, "avg_logprob": -0.1514162906380587, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0004441536439117044}, {"id": 178, "seek": 108592, "start": 1105.3600000000001, "end": 1110.5600000000002, "text": " in the Nerf++ paper. Essentially, we could either reconstruct the shape correctly and then have", "tokens": [51336, 294, 264, 36536, 69, 25472, 3035, 13, 23596, 11, 321, 727, 2139, 31499, 264, 3909, 8944, 293, 550, 362, 51596], "temperature": 0.0, "avg_logprob": -0.1514162906380587, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0004441536439117044}, {"id": 179, "seek": 111056, "start": 1111.2, "end": 1116.6399999999999, "text": " relatively constant radiance from different cameras, or we could encode each image into the", "tokens": [50396, 7226, 5754, 2843, 6276, 490, 819, 8622, 11, 420, 321, 727, 2058, 1429, 1184, 3256, 666, 264, 50668], "temperature": 0.0, "avg_logprob": -0.11546339988708496, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.00025312782963737845}, {"id": 180, "seek": 111056, "start": 1116.6399999999999, "end": 1120.96, "text": " view dependent coordinate of the network. So because the network depends on the camera position,", "tokens": [50668, 1910, 12334, 15670, 295, 264, 3209, 13, 407, 570, 264, 3209, 5946, 322, 264, 2799, 2535, 11, 50884], "temperature": 0.0, "avg_logprob": -0.11546339988708496, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.00025312782963737845}, {"id": 181, "seek": 111056, "start": 1120.96, "end": 1126.1599999999999, "text": " it's able to memorize potentially the photo taken from each camera.", "tokens": [50884, 309, 311, 1075, 281, 27478, 7263, 264, 5052, 2726, 490, 1184, 2799, 13, 51144], "temperature": 0.0, "avg_logprob": -0.11546339988708496, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.00025312782963737845}, {"id": 182, "seek": 111056, "start": 1128.3999999999999, "end": 1132.72, "text": " When the neural range field is trained with 100 views, it gets really crisp reconstructions. This", "tokens": [51256, 1133, 264, 18161, 3613, 2519, 307, 8895, 365, 2319, 6809, 11, 309, 2170, 534, 22952, 31499, 626, 13, 639, 51472], "temperature": 0.0, "avg_logprob": -0.11546339988708496, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.00025312782963737845}, {"id": 183, "seek": 111056, "start": 1132.72, "end": 1138.6399999999999, "text": " is a hot dog scene, synthetic scene, where we render out the views in Blender. Then the neural", "tokens": [51472, 307, 257, 2368, 3000, 4145, 11, 23420, 4145, 11, 689, 321, 15529, 484, 264, 6809, 294, 2177, 3216, 13, 1396, 264, 18161, 51768], "temperature": 0.0, "avg_logprob": -0.11546339988708496, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.00025312782963737845}, {"id": 184, "seek": 113864, "start": 1138.64, "end": 1144.88, "text": " radiance field, when it's trained with only eight views, only matches pictures close to the training", "tokens": [50364, 2843, 6276, 2519, 11, 562, 309, 311, 8895, 365, 787, 3180, 6809, 11, 787, 10676, 5242, 1998, 281, 264, 3097, 50676], "temperature": 0.0, "avg_logprob": -0.07571987087806958, "compression_ratio": 1.6297872340425532, "no_speech_prob": 4.9081365432357416e-05}, {"id": 185, "seek": 113864, "start": 1144.88, "end": 1150.16, "text": " data. When you move the camera further away from the observed images to try to extrapolate,", "tokens": [50676, 1412, 13, 1133, 291, 1286, 264, 2799, 3052, 1314, 490, 264, 13095, 5267, 281, 853, 281, 48224, 473, 11, 50940], "temperature": 0.0, "avg_logprob": -0.07571987087806958, "compression_ratio": 1.6297872340425532, "no_speech_prob": 4.9081365432357416e-05}, {"id": 186, "seek": 113864, "start": 1150.16, "end": 1155.6000000000001, "text": " then there'll be a lot of artifacts. If you regularize the neural radiance field a little bit", "tokens": [50940, 550, 456, 603, 312, 257, 688, 295, 24617, 13, 759, 291, 3890, 1125, 264, 18161, 2843, 6276, 2519, 257, 707, 857, 51212], "temperature": 0.0, "avg_logprob": -0.07571987087806958, "compression_ratio": 1.6297872340425532, "no_speech_prob": 4.9081365432357416e-05}, {"id": 187, "seek": 113864, "start": 1155.6000000000001, "end": 1160.5600000000002, "text": " and simplify it, it can learn more consistent geometry, but there still are a bunch of artifacts", "tokens": [51212, 293, 20460, 309, 11, 309, 393, 1466, 544, 8398, 18426, 11, 457, 456, 920, 366, 257, 3840, 295, 24617, 51460], "temperature": 0.0, "avg_logprob": -0.07571987087806958, "compression_ratio": 1.6297872340425532, "no_speech_prob": 4.9081365432357416e-05}, {"id": 188, "seek": 116056, "start": 1160.6399999999999, "end": 1169.84, "text": " in the reconstruction. I'll skip over this. So in our work, we add an additional loss to the", "tokens": [50368, 294, 264, 31565, 13, 286, 603, 10023, 670, 341, 13, 407, 294, 527, 589, 11, 321, 909, 364, 4497, 4470, 281, 264, 50828], "temperature": 0.0, "avg_logprob": -0.1423722267150879, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.0027570906095206738}, {"id": 189, "seek": 116056, "start": 1169.84, "end": 1176.08, "text": " neural radiance field training. We keep using the Nerf mean squared error loss. It's called", "tokens": [50828, 18161, 2843, 6276, 2519, 3097, 13, 492, 1066, 1228, 264, 36536, 69, 914, 8889, 6713, 4470, 13, 467, 311, 1219, 51140], "temperature": 0.0, "avg_logprob": -0.1423722267150879, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.0027570906095206738}, {"id": 190, "seek": 116056, "start": 1176.08, "end": 1183.12, "text": " photometric loss on the observed views that are sparse. But then our work diet nerf adds an", "tokens": [51140, 2409, 29470, 4470, 322, 264, 13095, 6809, 300, 366, 637, 11668, 13, 583, 550, 527, 589, 6339, 18219, 69, 10860, 364, 51492], "temperature": 0.0, "avg_logprob": -0.1423722267150879, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.0027570906095206738}, {"id": 191, "seek": 116056, "start": 1183.12, "end": 1188.24, "text": " additional loss at unobserved positions. So because we have this neural radiance field", "tokens": [51492, 4497, 4470, 412, 8526, 929, 6913, 8432, 13, 407, 570, 321, 362, 341, 18161, 2843, 6276, 2519, 51748], "temperature": 0.0, "avg_logprob": -0.1423722267150879, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.0027570906095206738}, {"id": 192, "seek": 118824, "start": 1189.04, "end": 1195.92, "text": " at any iteration during training, we're able to render out novel views even before the scene has", "tokens": [50404, 412, 604, 24784, 1830, 3097, 11, 321, 434, 1075, 281, 15529, 484, 7613, 6809, 754, 949, 264, 4145, 575, 50748], "temperature": 0.0, "avg_logprob": -0.09114621275214739, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.00027799911913461983}, {"id": 193, "seek": 118824, "start": 1195.92, "end": 1202.96, "text": " converged. It's a little silly that in Nerf, we're not able to constrain these input views,", "tokens": [50748, 9652, 3004, 13, 467, 311, 257, 707, 11774, 300, 294, 36536, 69, 11, 321, 434, 406, 1075, 281, 1817, 7146, 613, 4846, 6809, 11, 51100], "temperature": 0.0, "avg_logprob": -0.09114621275214739, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.00027799911913461983}, {"id": 194, "seek": 118824, "start": 1203.76, "end": 1209.28, "text": " because as a person looking at, okay, let's say that our estimate of the scene's geometry", "tokens": [51140, 570, 382, 257, 954, 1237, 412, 11, 1392, 11, 718, 311, 584, 300, 527, 12539, 295, 264, 4145, 311, 18426, 51416], "temperature": 0.0, "avg_logprob": -0.09114621275214739, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.00027799911913461983}, {"id": 195, "seek": 118824, "start": 1209.28, "end": 1214.88, "text": " gives us these renderings. This is the observed rendering. We as people can still look at these", "tokens": [51416, 2709, 505, 613, 15529, 1109, 13, 639, 307, 264, 13095, 22407, 13, 492, 382, 561, 393, 920, 574, 412, 613, 51696], "temperature": 0.0, "avg_logprob": -0.09114621275214739, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.00027799911913461983}, {"id": 196, "seek": 121488, "start": 1214.88, "end": 1221.2800000000002, "text": " photos and derive some loss signal. Okay, the input view is a lot sharper than my current", "tokens": [50364, 5787, 293, 28446, 512, 4470, 6358, 13, 1033, 11, 264, 4846, 1910, 307, 257, 688, 44670, 813, 452, 2190, 50684], "temperature": 0.0, "avg_logprob": -0.08257330327794171, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0005702318158000708}, {"id": 197, "seek": 121488, "start": 1221.2800000000002, "end": 1227.8400000000001, "text": " estimate of the scene. There's a little red light at the top of the truck, but there's no light on", "tokens": [50684, 12539, 295, 264, 4145, 13, 821, 311, 257, 707, 2182, 1442, 412, 264, 1192, 295, 264, 5898, 11, 457, 456, 311, 572, 1442, 322, 51012], "temperature": 0.0, "avg_logprob": -0.08257330327794171, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0005702318158000708}, {"id": 198, "seek": 121488, "start": 1227.8400000000001, "end": 1238.96, "text": " top of these reconstructions. Based on this principle that you can compare views at different camera", "tokens": [51012, 1192, 295, 613, 31499, 626, 13, 18785, 322, 341, 8665, 300, 291, 393, 6794, 6809, 412, 819, 2799, 51568], "temperature": 0.0, "avg_logprob": -0.08257330327794171, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0005702318158000708}, {"id": 199, "seek": 123896, "start": 1238.96, "end": 1245.68, "text": " positions as a person by comparing their semantics, like, you know, it's a bulldozer, a bulldozer is", "tokens": [50364, 8432, 382, 257, 954, 538, 15763, 641, 4361, 45298, 11, 411, 11, 291, 458, 11, 309, 311, 257, 4693, 2595, 4527, 11, 257, 4693, 2595, 4527, 307, 50700], "temperature": 0.0, "avg_logprob": -0.12266542415807744, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.010983282700181007}, {"id": 200, "seek": 123896, "start": 1245.68, "end": 1251.6000000000001, "text": " a bulldozer from any perspective. We propose to add a loss in feature space. Using some visual", "tokens": [50700, 257, 4693, 2595, 4527, 490, 604, 4585, 13, 492, 17421, 281, 909, 257, 4470, 294, 4111, 1901, 13, 11142, 512, 5056, 50996], "temperature": 0.0, "avg_logprob": -0.12266542415807744, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.010983282700181007}, {"id": 201, "seek": 123896, "start": 1251.6000000000001, "end": 1259.52, "text": " encoder, each of the input views is represented in a feature space. And then instead of computing", "tokens": [50996, 2058, 19866, 11, 1184, 295, 264, 4846, 6809, 307, 10379, 294, 257, 4111, 1901, 13, 400, 550, 2602, 295, 15866, 51392], "temperature": 0.0, "avg_logprob": -0.12266542415807744, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.010983282700181007}, {"id": 202, "seek": 123896, "start": 1259.52, "end": 1267.3600000000001, "text": " the loss in pixel space, diner will compute a loss in feature space. And that allows us to regularize", "tokens": [51392, 264, 4470, 294, 19261, 1901, 11, 3791, 260, 486, 14722, 257, 4470, 294, 4111, 1901, 13, 400, 300, 4045, 505, 281, 3890, 1125, 51784], "temperature": 0.0, "avg_logprob": -0.12266542415807744, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.010983282700181007}, {"id": 203, "seek": 126736, "start": 1267.36, "end": 1276.6399999999999, "text": " the scene from any perspective during training. We call this a semantic consistency loss,", "tokens": [50364, 264, 4145, 490, 604, 4585, 1830, 3097, 13, 492, 818, 341, 257, 47982, 14416, 4470, 11, 50828], "temperature": 0.0, "avg_logprob": -0.09307359436811027, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.00013550935545936227}, {"id": 204, "seek": 126736, "start": 1276.6399999999999, "end": 1281.4399999999998, "text": " since we're making sure that these semantic features, things like object identity, object color,", "tokens": [50828, 1670, 321, 434, 1455, 988, 300, 613, 47982, 4122, 11, 721, 411, 2657, 6575, 11, 2657, 2017, 11, 51068], "temperature": 0.0, "avg_logprob": -0.09307359436811027, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.00013550935545936227}, {"id": 205, "seek": 126736, "start": 1282.8799999999999, "end": 1289.6799999999998, "text": " are consistent across views. And over the course of training, this improves the results.", "tokens": [51140, 366, 8398, 2108, 6809, 13, 400, 670, 264, 1164, 295, 3097, 11, 341, 24771, 264, 3542, 13, 51480], "temperature": 0.0, "avg_logprob": -0.09307359436811027, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.00013550935545936227}, {"id": 206, "seek": 128968, "start": 1290.4, "end": 1296.0, "text": " So the loss that Nerf used was this mean squared error loss, and then we're adding this semantic", "tokens": [50400, 407, 264, 4470, 300, 36536, 69, 1143, 390, 341, 914, 8889, 6713, 4470, 11, 293, 550, 321, 434, 5127, 341, 47982, 50680], "temperature": 0.0, "avg_logprob": -0.23131659411001895, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.00023049127776175737}, {"id": 207, "seek": 128968, "start": 1296.0, "end": 1303.8400000000001, "text": " consistency loss where some encoder thigh, some neural network encodes rendered images, and then we", "tokens": [50680, 14416, 4470, 689, 512, 2058, 19866, 27871, 11, 512, 18161, 3209, 2058, 4789, 28748, 5267, 11, 293, 550, 321, 51072], "temperature": 0.0, "avg_logprob": -0.23131659411001895, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.00023049127776175737}, {"id": 208, "seek": 128968, "start": 1303.8400000000001, "end": 1313.04, "text": " compare them in a feature space. We do have to sample camera poses in order to render this, so", "tokens": [51072, 6794, 552, 294, 257, 4111, 1901, 13, 492, 360, 362, 281, 6889, 2799, 26059, 294, 1668, 281, 15529, 341, 11, 370, 51532], "temperature": 0.0, "avg_logprob": -0.23131659411001895, "compression_ratio": 1.598901098901099, "no_speech_prob": 0.00023049127776175737}, {"id": 209, "seek": 131304, "start": 1314.0, "end": 1325.6, "text": " there's just some prior distribution over camera poses. The choice of that feature thigh is really,", "tokens": [50412, 456, 311, 445, 512, 4059, 7316, 670, 2799, 26059, 13, 440, 3922, 295, 300, 4111, 27871, 307, 534, 11, 50992], "temperature": 0.0, "avg_logprob": -0.22829659779866537, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0006460999138653278}, {"id": 210, "seek": 131304, "start": 1325.6, "end": 1332.3999999999999, "text": " really important for the results, because we want it to be consistent across viewpoints. So it should", "tokens": [50992, 534, 1021, 337, 264, 3542, 11, 570, 321, 528, 309, 281, 312, 8398, 2108, 1910, 20552, 13, 407, 309, 820, 51332], "temperature": 0.0, "avg_logprob": -0.22829659779866537, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0006460999138653278}, {"id": 211, "seek": 131304, "start": 1332.3999999999999, "end": 1337.52, "text": " really encode the object's identity and properties about the object rather than low-level details,", "tokens": [51332, 534, 2058, 1429, 264, 2657, 311, 6575, 293, 7221, 466, 264, 2657, 2831, 813, 2295, 12, 12418, 4365, 11, 51588], "temperature": 0.0, "avg_logprob": -0.22829659779866537, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.0006460999138653278}, {"id": 212, "seek": 133752, "start": 1337.76, "end": 1348.48, "text": " like the exact pixel colors. And motivated by that, we use a network called Clip. This is from", "tokens": [50376, 411, 264, 1900, 19261, 4577, 13, 400, 14515, 538, 300, 11, 321, 764, 257, 3209, 1219, 2033, 647, 13, 639, 307, 490, 50912], "temperature": 0.0, "avg_logprob": -0.1767449939952177, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.002888342132791877}, {"id": 213, "seek": 133752, "start": 1349.44, "end": 1356.32, "text": " last year. It's a representation of images and text, so a representation of an images learn,", "tokens": [50960, 1036, 1064, 13, 467, 311, 257, 10290, 295, 5267, 293, 2487, 11, 370, 257, 10290, 295, 364, 5267, 1466, 11, 51304], "temperature": 0.0, "avg_logprob": -0.1767449939952177, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.002888342132791877}, {"id": 214, "seek": 133752, "start": 1357.76, "end": 1362.4, "text": " such that it has an aligned representation with an associated caption. The data that Clip is", "tokens": [51376, 1270, 300, 309, 575, 364, 17962, 10290, 365, 364, 6615, 31974, 13, 440, 1412, 300, 2033, 647, 307, 51608], "temperature": 0.0, "avg_logprob": -0.1767449939952177, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.002888342132791877}, {"id": 215, "seek": 136240, "start": 1362.4, "end": 1368.0800000000002, "text": " trained on is a very large data set of 400 million images that have associated captions", "tokens": [50364, 8895, 322, 307, 257, 588, 2416, 1412, 992, 295, 8423, 2459, 5267, 300, 362, 6615, 44832, 50648], "temperature": 0.0, "avg_logprob": -0.09570920190145803, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0013041346101090312}, {"id": 216, "seek": 136240, "start": 1368.0800000000002, "end": 1374.8000000000002, "text": " crawled from the web. And the Clip model has really led to an explosion of work in the AI art", "tokens": [50648, 13999, 1493, 490, 264, 3670, 13, 400, 264, 2033, 647, 2316, 575, 534, 4684, 281, 364, 15673, 295, 589, 294, 264, 7318, 1523, 50984], "temperature": 0.0, "avg_logprob": -0.09570920190145803, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0013041346101090312}, {"id": 217, "seek": 136240, "start": 1374.8000000000002, "end": 1379.68, "text": " community. It's really powerful. It's trained on such a large amount of data that we're able to", "tokens": [50984, 1768, 13, 467, 311, 534, 4005, 13, 467, 311, 8895, 322, 1270, 257, 2416, 2372, 295, 1412, 300, 321, 434, 1075, 281, 51228], "temperature": 0.0, "avg_logprob": -0.09570920190145803, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0013041346101090312}, {"id": 218, "seek": 136240, "start": 1381.8400000000001, "end": 1386.16, "text": " prompt it with topics that you wouldn't find in a narrow data set.", "tokens": [51336, 12391, 309, 365, 8378, 300, 291, 2759, 380, 915, 294, 257, 9432, 1412, 992, 13, 51552], "temperature": 0.0, "avg_logprob": -0.09570920190145803, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0013041346101090312}, {"id": 219, "seek": 138616, "start": 1386.48, "end": 1391.76, "text": " It also, by learning to match images to this text, we'd hope to learn some very useful features", "tokens": [50380, 467, 611, 11, 538, 2539, 281, 2995, 5267, 281, 341, 2487, 11, 321, 1116, 1454, 281, 1466, 512, 588, 4420, 4122, 50644], "temperature": 0.0, "avg_logprob": -0.18082259803689937, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0013666747836396098}, {"id": 220, "seek": 138616, "start": 1391.76, "end": 1397.28, "text": " about an image. For example, in captions, you can encode classes of objects, just like image net", "tokens": [50644, 466, 364, 3256, 13, 1171, 1365, 11, 294, 44832, 11, 291, 393, 2058, 1429, 5359, 295, 6565, 11, 445, 411, 3256, 2533, 50920], "temperature": 0.0, "avg_logprob": -0.18082259803689937, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0013666747836396098}, {"id": 221, "seek": 138616, "start": 1397.28, "end": 1404.0800000000002, "text": " labels. You can also encode a lot of other details, like the scene rather than just the foreground", "tokens": [50920, 16949, 13, 509, 393, 611, 2058, 1429, 257, 688, 295, 661, 4365, 11, 411, 264, 4145, 2831, 813, 445, 264, 32058, 51260], "temperature": 0.0, "avg_logprob": -0.18082259803689937, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0013666747836396098}, {"id": 222, "seek": 138616, "start": 1404.0800000000002, "end": 1410.16, "text": " object. You can encode things about pose of the underlying object, like a sitting person,", "tokens": [51260, 2657, 13, 509, 393, 2058, 1429, 721, 466, 10774, 295, 264, 14217, 2657, 11, 411, 257, 3798, 954, 11, 51564], "temperature": 0.0, "avg_logprob": -0.18082259803689937, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0013666747836396098}, {"id": 223, "seek": 141016, "start": 1410.16, "end": 1417.28, "text": " a standing person. And that should be encoded in the representation learned by the network,", "tokens": [50364, 257, 4877, 954, 13, 400, 300, 820, 312, 2058, 12340, 294, 264, 10290, 3264, 538, 264, 3209, 11, 50720], "temperature": 0.0, "avg_logprob": -0.10847523771686318, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.00023045807029120624}, {"id": 224, "seek": 141016, "start": 1417.28, "end": 1421.6000000000001, "text": " if it's going to be able to match images against their associated caption. So the", "tokens": [50720, 498, 309, 311, 516, 281, 312, 1075, 281, 2995, 5267, 1970, 641, 6615, 31974, 13, 407, 264, 50936], "temperature": 0.0, "avg_logprob": -0.10847523771686318, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.00023045807029120624}, {"id": 225, "seek": 141016, "start": 1421.6000000000001, "end": 1426.0800000000002, "text": " training objective is encode a bunch of images, encode their captions, and then try to match", "tokens": [50936, 3097, 10024, 307, 2058, 1429, 257, 3840, 295, 5267, 11, 2058, 1429, 641, 44832, 11, 293, 550, 853, 281, 2995, 51160], "temperature": 0.0, "avg_logprob": -0.10847523771686318, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.00023045807029120624}, {"id": 226, "seek": 141016, "start": 1426.96, "end": 1433.68, "text": " images with their true caption. Clip was originally used for discriminative tasks,", "tokens": [51204, 5267, 365, 641, 2074, 31974, 13, 2033, 647, 390, 7993, 1143, 337, 20828, 1166, 9608, 11, 51540], "temperature": 0.0, "avg_logprob": -0.10847523771686318, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.00023045807029120624}, {"id": 227, "seek": 143368, "start": 1433.76, "end": 1442.96, "text": " object classification in a prompting fashion. So if you want to classify photos of food,", "tokens": [50368, 2657, 21538, 294, 257, 12391, 278, 6700, 13, 407, 498, 291, 528, 281, 33872, 5787, 295, 1755, 11, 50828], "temperature": 0.0, "avg_logprob": -0.11378544191770916, "compression_ratio": 1.6391752577319587, "no_speech_prob": 0.005134893115609884}, {"id": 228, "seek": 143368, "start": 1444.16, "end": 1449.2, "text": " the authors of clip constructed a bunch of captions, templatized with the desired object", "tokens": [50888, 264, 16552, 295, 7353, 17083, 257, 3840, 295, 44832, 11, 9100, 267, 1602, 365, 264, 14721, 2657, 51140], "temperature": 0.0, "avg_logprob": -0.11378544191770916, "compression_ratio": 1.6391752577319587, "no_speech_prob": 0.005134893115609884}, {"id": 229, "seek": 143368, "start": 1449.2, "end": 1455.2, "text": " category, a photo of guacamole, a photo of ceviche. And then the class label is given by the", "tokens": [51140, 7719, 11, 257, 5052, 295, 695, 47190, 4812, 11, 257, 5052, 295, 43266, 9304, 13, 400, 550, 264, 1508, 7645, 307, 2212, 538, 264, 51440], "temperature": 0.0, "avg_logprob": -0.11378544191770916, "compression_ratio": 1.6391752577319587, "no_speech_prob": 0.005134893115609884}, {"id": 230, "seek": 143368, "start": 1456.0800000000002, "end": 1459.2, "text": " caption with the best match with a given image.", "tokens": [51484, 31974, 365, 264, 1151, 2995, 365, 257, 2212, 3256, 13, 51640], "temperature": 0.0, "avg_logprob": -0.11378544191770916, "compression_ratio": 1.6391752577319587, "no_speech_prob": 0.005134893115609884}, {"id": 231, "seek": 145920, "start": 1459.6000000000001, "end": 1463.68, "text": " The property we're particularly interested in in this 3D reconstruction context is whether the", "tokens": [50384, 440, 4707, 321, 434, 4098, 3102, 294, 294, 341, 805, 35, 31565, 4319, 307, 1968, 264, 50588], "temperature": 0.0, "avg_logprob": -0.1847299819296979, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.00031495123403146863}, {"id": 232, "seek": 145920, "start": 1463.68, "end": 1468.48, "text": " representations of the images are consistent across views. That's what we call semantic", "tokens": [50588, 33358, 295, 264, 5267, 366, 8398, 2108, 6809, 13, 663, 311, 437, 321, 818, 47982, 50828], "temperature": 0.0, "avg_logprob": -0.1847299819296979, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.00031495123403146863}, {"id": 233, "seek": 145920, "start": 1468.48, "end": 1475.04, "text": " consistency in the work. What this plot is showing is that the cosine similarity of embeddings", "tokens": [50828, 14416, 294, 264, 589, 13, 708, 341, 7542, 307, 4099, 307, 300, 264, 23565, 32194, 295, 12240, 29432, 51156], "temperature": 0.0, "avg_logprob": -0.1847299819296979, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.00031495123403146863}, {"id": 234, "seek": 145920, "start": 1475.04, "end": 1480.4, "text": " from the image encoder of clip within a particular scene from different camera poses is highly", "tokens": [51156, 490, 264, 3256, 2058, 19866, 295, 7353, 1951, 257, 1729, 4145, 490, 819, 2799, 26059, 307, 5405, 51424], "temperature": 0.0, "avg_logprob": -0.1847299819296979, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.00031495123403146863}, {"id": 235, "seek": 145920, "start": 1480.4, "end": 1486.8, "text": " similar. So very high similarity in feature space within a scene across different perspectives,", "tokens": [51424, 2531, 13, 407, 588, 1090, 32194, 294, 4111, 1901, 1951, 257, 4145, 2108, 819, 16766, 11, 51744], "temperature": 0.0, "avg_logprob": -0.1847299819296979, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.00031495123403146863}, {"id": 236, "seek": 148680, "start": 1487.04, "end": 1499.28, "text": " low similarity across scenes at different perspectives. So because images are very", "tokens": [50376, 2295, 32194, 2108, 8026, 412, 819, 16766, 13, 407, 570, 5267, 366, 588, 50988], "temperature": 0.0, "avg_logprob": -0.14876604654702796, "compression_ratio": 1.64, "no_speech_prob": 0.00015354058996308595}, {"id": 237, "seek": 148680, "start": 1499.28, "end": 1504.08, "text": " similar in clip's feature space, very different in pixel space, we're able to maximize feature", "tokens": [50988, 2531, 294, 7353, 311, 4111, 1901, 11, 588, 819, 294, 19261, 1901, 11, 321, 434, 1075, 281, 19874, 4111, 51228], "temperature": 0.0, "avg_logprob": -0.14876604654702796, "compression_ratio": 1.64, "no_speech_prob": 0.00015354058996308595}, {"id": 238, "seek": 148680, "start": 1504.08, "end": 1510.8799999999999, "text": " space similarity of clip and get some useful loss. Now what you've been waiting for are the results.", "tokens": [51228, 1901, 32194, 295, 7353, 293, 483, 512, 4420, 4470, 13, 823, 437, 291, 600, 668, 3806, 337, 366, 264, 3542, 13, 51568], "temperature": 0.0, "avg_logprob": -0.14876604654702796, "compression_ratio": 1.64, "no_speech_prob": 0.00015354058996308595}, {"id": 239, "seek": 148680, "start": 1511.6, "end": 1515.68, "text": " This is nerf trained on eight views when it's simplified. And then here is it trained with", "tokens": [51604, 639, 307, 18219, 69, 8895, 322, 3180, 6809, 562, 309, 311, 26335, 13, 400, 550, 510, 307, 309, 8895, 365, 51808], "temperature": 0.0, "avg_logprob": -0.14876604654702796, "compression_ratio": 1.64, "no_speech_prob": 0.00015354058996308595}, {"id": 240, "seek": 151568, "start": 1515.68, "end": 1520.88, "text": " our additional semantic consistency loss. A bunch of near field artifacts in nerf,", "tokens": [50364, 527, 4497, 47982, 14416, 4470, 13, 316, 3840, 295, 2651, 2519, 24617, 294, 18219, 69, 11, 50624], "temperature": 0.0, "avg_logprob": -0.12203957817771217, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0002694114809855819}, {"id": 241, "seek": 151568, "start": 1520.88, "end": 1524.64, "text": " but when we add in this feature space loss, it removes a lot of those artifacts.", "tokens": [50624, 457, 562, 321, 909, 294, 341, 4111, 1901, 4470, 11, 309, 30445, 257, 688, 295, 729, 24617, 13, 50812], "temperature": 0.0, "avg_logprob": -0.12203957817771217, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0002694114809855819}, {"id": 242, "seek": 151568, "start": 1530.96, "end": 1536.24, "text": " Because those artifacts aren't plausible, they reduce the semantic consistency.", "tokens": [51128, 1436, 729, 24617, 3212, 380, 39925, 11, 436, 5407, 264, 47982, 14416, 13, 51392], "temperature": 0.0, "avg_logprob": -0.12203957817771217, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0002694114809855819}, {"id": 243, "seek": 153624, "start": 1536.24, "end": 1546.96, "text": " Cool. I'm going to go on to the next work. Before I do, anyone have questions?", "tokens": [50364, 8561, 13, 286, 478, 516, 281, 352, 322, 281, 264, 958, 589, 13, 4546, 286, 360, 11, 2878, 362, 1651, 30, 50900], "temperature": 0.0, "avg_logprob": -0.18008632936339447, "compression_ratio": 1.5, "no_speech_prob": 0.001673518680036068}, {"id": 244, "seek": 153624, "start": 1546.96, "end": 1556.72, "text": " I have one question, which is with regards to using clip. Are you able to access the text", "tokens": [50900, 286, 362, 472, 1168, 11, 597, 307, 365, 14258, 281, 1228, 7353, 13, 2014, 291, 1075, 281, 2105, 264, 2487, 51388], "temperature": 0.0, "avg_logprob": -0.18008632936339447, "compression_ratio": 1.5, "no_speech_prob": 0.001673518680036068}, {"id": 245, "seek": 153624, "start": 1556.72, "end": 1562.4, "text": " as well that clip generates, or are you able to decode it in some way and actually access", "tokens": [51388, 382, 731, 300, 7353, 23815, 11, 420, 366, 291, 1075, 281, 979, 1429, 309, 294, 512, 636, 293, 767, 2105, 51672], "temperature": 0.0, "avg_logprob": -0.18008632936339447, "compression_ratio": 1.5, "no_speech_prob": 0.001673518680036068}, {"id": 246, "seek": 156240, "start": 1562.4, "end": 1569.1200000000001, "text": " how the clip looks at the inputs? Just in terms of explainability, I thought it could be,", "tokens": [50364, 577, 264, 7353, 1542, 412, 264, 15743, 30, 1449, 294, 2115, 295, 2903, 2310, 11, 286, 1194, 309, 727, 312, 11, 50700], "temperature": 0.0, "avg_logprob": -0.13023810467477573, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.00020981677516829222}, {"id": 247, "seek": 156240, "start": 1569.1200000000001, "end": 1573.44, "text": " yeah, sounds really interesting. Yeah, that's a very good question. So in this work,", "tokens": [50700, 1338, 11, 3263, 534, 1880, 13, 865, 11, 300, 311, 257, 588, 665, 1168, 13, 407, 294, 341, 589, 11, 50916], "temperature": 0.0, "avg_logprob": -0.13023810467477573, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.00020981677516829222}, {"id": 248, "seek": 156240, "start": 1573.44, "end": 1579.0400000000002, "text": " we're not actually using the text encoder. We'll see that in the next work. The text encoder is", "tokens": [50916, 321, 434, 406, 767, 1228, 264, 2487, 2058, 19866, 13, 492, 603, 536, 300, 294, 264, 958, 589, 13, 440, 2487, 2058, 19866, 307, 51196], "temperature": 0.0, "avg_logprob": -0.13023810467477573, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.00020981677516829222}, {"id": 249, "seek": 156240, "start": 1579.0400000000002, "end": 1584.64, "text": " just used for pre-training clip in dye and nerf. So we're only using this image encoder.", "tokens": [51196, 445, 1143, 337, 659, 12, 17227, 1760, 7353, 294, 20179, 293, 18219, 69, 13, 407, 321, 434, 787, 1228, 341, 3256, 2058, 19866, 13, 51476], "temperature": 0.0, "avg_logprob": -0.13023810467477573, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.00020981677516829222}, {"id": 250, "seek": 156240, "start": 1584.64, "end": 1589.0400000000002, "text": " Because then the motivation for that is that the neural radian students are motivated by the", "tokens": [51476, 1436, 550, 264, 12335, 337, 300, 307, 300, 264, 18161, 2843, 952, 1731, 366, 14515, 538, 264, 51696], "temperature": 0.0, "avg_logprob": -0.13023810467477573, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.00020981677516829222}, {"id": 251, "seek": 158904, "start": 1589.12, "end": 1593.2, "text": " view synthesis problem. So there's no text caption associated with the data.", "tokens": [50368, 1910, 30252, 1154, 13, 407, 456, 311, 572, 2487, 31974, 6615, 365, 264, 1412, 13, 50572], "temperature": 0.0, "avg_logprob": -0.07585801883619658, "compression_ratio": 1.697211155378486, "no_speech_prob": 0.002322300337255001}, {"id": 252, "seek": 158904, "start": 1593.2, "end": 1596.48, "text": " They just have a couple of pictures. So we only need to use the image encoder.", "tokens": [50572, 814, 445, 362, 257, 1916, 295, 5242, 13, 407, 321, 787, 643, 281, 764, 264, 3256, 2058, 19866, 13, 50736], "temperature": 0.0, "avg_logprob": -0.07585801883619658, "compression_ratio": 1.697211155378486, "no_speech_prob": 0.002322300337255001}, {"id": 253, "seek": 158904, "start": 1598.8, "end": 1604.08, "text": " That said, some artists have tried to take clip and use it to create a captioning model.", "tokens": [50852, 663, 848, 11, 512, 6910, 362, 3031, 281, 747, 7353, 293, 764, 309, 281, 1884, 257, 31974, 278, 2316, 13, 51116], "temperature": 0.0, "avg_logprob": -0.07585801883619658, "compression_ratio": 1.697211155378486, "no_speech_prob": 0.002322300337255001}, {"id": 254, "seek": 158904, "start": 1606.48, "end": 1609.84, "text": " If you have a model that can match images against captions, can you actually synthesize", "tokens": [51236, 759, 291, 362, 257, 2316, 300, 393, 2995, 5267, 1970, 44832, 11, 393, 291, 767, 26617, 1125, 51404], "temperature": 0.0, "avg_logprob": -0.07585801883619658, "compression_ratio": 1.697211155378486, "no_speech_prob": 0.002322300337255001}, {"id": 255, "seek": 158904, "start": 1610.56, "end": 1616.24, "text": " captions that best match a particular image? It's a challenging discrete optimization problem", "tokens": [51440, 44832, 300, 1151, 2995, 257, 1729, 3256, 30, 467, 311, 257, 7595, 27706, 19618, 1154, 51724], "temperature": 0.0, "avg_logprob": -0.07585801883619658, "compression_ratio": 1.697211155378486, "no_speech_prob": 0.002322300337255001}, {"id": 256, "seek": 161624, "start": 1616.24, "end": 1620.56, "text": " because you're searching for a textual caption that will maximize some neural network's output", "tokens": [50364, 570, 291, 434, 10808, 337, 257, 2487, 901, 31974, 300, 486, 19874, 512, 18161, 3209, 311, 5598, 50580], "temperature": 0.0, "avg_logprob": -0.08463341456193191, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.0002868047740776092}, {"id": 257, "seek": 161624, "start": 1620.56, "end": 1626.16, "text": " score. And that is basically a black box optimization problem. My impression is that", "tokens": [50580, 6175, 13, 400, 300, 307, 1936, 257, 2211, 2424, 19618, 1154, 13, 1222, 9995, 307, 300, 50860], "temperature": 0.0, "avg_logprob": -0.08463341456193191, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.0002868047740776092}, {"id": 258, "seek": 161624, "start": 1626.16, "end": 1630.48, "text": " automatic captioning with clip doesn't work too well. It's really good at selecting an associated", "tokens": [50860, 12509, 31974, 278, 365, 7353, 1177, 380, 589, 886, 731, 13, 467, 311, 534, 665, 412, 18182, 364, 6615, 51076], "temperature": 0.0, "avg_logprob": -0.08463341456193191, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.0002868047740776092}, {"id": 259, "seek": 161624, "start": 1630.48, "end": 1636.16, "text": " caption out of a list of candidates. And that's how we're able to do object classification with clip.", "tokens": [51076, 31974, 484, 295, 257, 1329, 295, 11255, 13, 400, 300, 311, 577, 321, 434, 1075, 281, 360, 2657, 21538, 365, 7353, 13, 51360], "temperature": 0.0, "avg_logprob": -0.08463341456193191, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.0002868047740776092}, {"id": 260, "seek": 161624, "start": 1638.88, "end": 1643.2, "text": " So I think you'd be better served by learning a specific captioning model that will generate", "tokens": [51496, 407, 286, 519, 291, 1116, 312, 1101, 7584, 538, 2539, 257, 2685, 31974, 278, 2316, 300, 486, 8460, 51712], "temperature": 0.0, "avg_logprob": -0.08463341456193191, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.0002868047740776092}, {"id": 261, "seek": 164320, "start": 1643.2, "end": 1647.2, "text": " a caption condition on image rather than trying to extract captions out of clip", "tokens": [50364, 257, 31974, 4188, 322, 3256, 2831, 813, 1382, 281, 8947, 44832, 484, 295, 7353, 50564], "temperature": 0.0, "avg_logprob": -0.20621427412955992, "compression_ratio": 1.4550898203592815, "no_speech_prob": 0.00031496273004449904}, {"id": 262, "seek": 164320, "start": 1648.48, "end": 1654.48, "text": " just due to the difficulty of the optimization or the search. Thank you.", "tokens": [50628, 445, 3462, 281, 264, 10360, 295, 264, 19618, 420, 264, 3164, 13, 1044, 291, 13, 50928], "temperature": 0.0, "avg_logprob": -0.20621427412955992, "compression_ratio": 1.4550898203592815, "no_speech_prob": 0.00031496273004449904}, {"id": 263, "seek": 164320, "start": 1660.8, "end": 1667.04, "text": " So like I said, we weren't using the text encoder in diet ner. In the next work, we try to", "tokens": [51244, 407, 411, 286, 848, 11, 321, 4999, 380, 1228, 264, 2487, 2058, 19866, 294, 6339, 18219, 13, 682, 264, 958, 589, 11, 321, 853, 281, 51556], "temperature": 0.0, "avg_logprob": -0.20621427412955992, "compression_ratio": 1.4550898203592815, "no_speech_prob": 0.00031496273004449904}, {"id": 264, "seek": 166704, "start": 1667.68, "end": 1676.08, "text": " move in an even more extreme direction of generating objects without any image data.", "tokens": [50396, 1286, 294, 364, 754, 544, 8084, 3513, 295, 17746, 6565, 1553, 604, 3256, 1412, 13, 50816], "temperature": 0.0, "avg_logprob": -0.12576930950849485, "compression_ratio": 1.5, "no_speech_prob": 0.0017541993875056505}, {"id": 265, "seek": 166704, "start": 1676.08, "end": 1680.32, "text": " So what if we only have a caption and want to synthesize the 3D object from it?", "tokens": [50816, 407, 437, 498, 321, 787, 362, 257, 31974, 293, 528, 281, 26617, 1125, 264, 805, 35, 2657, 490, 309, 30, 51028], "temperature": 0.0, "avg_logprob": -0.12576930950849485, "compression_ratio": 1.5, "no_speech_prob": 0.0017541993875056505}, {"id": 266, "seek": 166704, "start": 1682.0, "end": 1687.36, "text": " Is that possible? Can we remove this mean squared error loss entirely and only use", "tokens": [51112, 1119, 300, 1944, 30, 1664, 321, 4159, 341, 914, 8889, 6713, 4470, 7696, 293, 787, 764, 51380], "temperature": 0.0, "avg_logprob": -0.12576930950849485, "compression_ratio": 1.5, "no_speech_prob": 0.0017541993875056505}, {"id": 267, "seek": 166704, "start": 1687.36, "end": 1692.8799999999999, "text": " feature space losses? And these are some examples of the results we're able to get", "tokens": [51380, 4111, 1901, 15352, 30, 400, 613, 366, 512, 5110, 295, 264, 3542, 321, 434, 1075, 281, 483, 51656], "temperature": 0.0, "avg_logprob": -0.12576930950849485, "compression_ratio": 1.5, "no_speech_prob": 0.0017541993875056505}, {"id": 268, "seek": 169288, "start": 1692.88, "end": 1697.3600000000001, "text": " with different captions, like a render of a Jenga tower produces this object.", "tokens": [50364, 365, 819, 44832, 11, 411, 257, 15529, 295, 257, 508, 31494, 10567, 14725, 341, 2657, 13, 50588], "temperature": 0.0, "avg_logprob": -0.19890566129942197, "compression_ratio": 1.4694835680751173, "no_speech_prob": 0.0007552375318482518}, {"id": 269, "seek": 169288, "start": 1698.96, "end": 1706.96, "text": " You can also engineer prompts, use hashtags because clip is trained on web data.", "tokens": [50668, 509, 393, 611, 11403, 41095, 11, 764, 50016, 570, 7353, 307, 8895, 322, 3670, 1412, 13, 51068], "temperature": 0.0, "avg_logprob": -0.19890566129942197, "compression_ratio": 1.4694835680751173, "no_speech_prob": 0.0007552375318482518}, {"id": 270, "seek": 169288, "start": 1710.0, "end": 1712.5600000000002, "text": " Our goal is to synthesize 3D objects from just the caption.", "tokens": [51220, 2621, 3387, 307, 281, 26617, 1125, 805, 35, 6565, 490, 445, 264, 31974, 13, 51348], "temperature": 0.0, "avg_logprob": -0.19890566129942197, "compression_ratio": 1.4694835680751173, "no_speech_prob": 0.0007552375318482518}, {"id": 271, "seek": 169288, "start": 1714.8000000000002, "end": 1721.2, "text": " And to kind of refresh our memories, the neural radiance field is an inverse graphics approach", "tokens": [51460, 400, 281, 733, 295, 15134, 527, 8495, 11, 264, 18161, 2843, 6276, 2519, 307, 364, 17340, 11837, 3109, 51780], "temperature": 0.0, "avg_logprob": -0.19890566129942197, "compression_ratio": 1.4694835680751173, "no_speech_prob": 0.0007552375318482518}, {"id": 272, "seek": 172120, "start": 1721.2, "end": 1725.68, "text": " where we have densely sampled images, optimize the shared scene representation,", "tokens": [50364, 689, 321, 362, 24505, 736, 3247, 15551, 5267, 11, 19719, 264, 5507, 4145, 10290, 11, 50588], "temperature": 0.0, "avg_logprob": -0.1394959618063534, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.00045107046025805175}, {"id": 273, "seek": 172120, "start": 1726.48, "end": 1732.56, "text": " and then are able to render out new views. In the dream fields work, the second work in this line,", "tokens": [50628, 293, 550, 366, 1075, 281, 15529, 484, 777, 6809, 13, 682, 264, 3055, 7909, 589, 11, 264, 1150, 589, 294, 341, 1622, 11, 50932], "temperature": 0.0, "avg_logprob": -0.1394959618063534, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.00045107046025805175}, {"id": 274, "seek": 172120, "start": 1734.16, "end": 1739.68, "text": " we do not have any observed images, only a caption written, for example, by a human artist.", "tokens": [51012, 321, 360, 406, 362, 604, 13095, 5267, 11, 787, 257, 31974, 3720, 11, 337, 1365, 11, 538, 257, 1952, 5748, 13, 51288], "temperature": 0.0, "avg_logprob": -0.1394959618063534, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.00045107046025805175}, {"id": 275, "seek": 172120, "start": 1741.8400000000001, "end": 1747.2, "text": " We optimize something that will look fairly similar to diet ner with additional regularizers,", "tokens": [51396, 492, 19719, 746, 300, 486, 574, 6457, 2531, 281, 6339, 18219, 365, 4497, 3890, 22525, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1394959618063534, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.00045107046025805175}, {"id": 276, "seek": 174720, "start": 1748.16, "end": 1752.16, "text": " and then are able to render out new views. And any perspective is actually a new view", "tokens": [50412, 293, 550, 366, 1075, 281, 15529, 484, 777, 6809, 13, 400, 604, 4585, 307, 767, 257, 777, 1910, 50612], "temperature": 0.0, "avg_logprob": -0.2152992047761616, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.00011590761278057471}, {"id": 277, "seek": 174720, "start": 1752.16, "end": 1757.04, "text": " because we haven't observed this scene. This is an associated scene for the caption,", "tokens": [50612, 570, 321, 2378, 380, 13095, 341, 4145, 13, 639, 307, 364, 6615, 4145, 337, 264, 31974, 11, 50856], "temperature": 0.0, "avg_logprob": -0.2152992047761616, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.00011590761278057471}, {"id": 278, "seek": 174720, "start": 1757.04, "end": 1759.44, "text": " an epic, wondrous, fantasy painting of an ocean.", "tokens": [50856, 364, 13581, 11, 2046, 21189, 11, 13861, 5370, 295, 364, 7810, 13, 50976], "temperature": 0.0, "avg_logprob": -0.2152992047761616, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.00011590761278057471}, {"id": 279, "seek": 174720, "start": 1764.0800000000002, "end": 1770.56, "text": " So the neural radius would use this mean squared error loss, and then diet ner used feature space", "tokens": [51208, 407, 264, 18161, 15845, 576, 764, 341, 914, 8889, 6713, 4470, 11, 293, 550, 6339, 18219, 1143, 4111, 1901, 51532], "temperature": 0.0, "avg_logprob": -0.2152992047761616, "compression_ratio": 1.5463414634146342, "no_speech_prob": 0.00011590761278057471}, {"id": 280, "seek": 177056, "start": 1770.56, "end": 1780.08, "text": " loss where the rendered image of the scene and an observed image of the scene are encoded into", "tokens": [50364, 4470, 689, 264, 28748, 3256, 295, 264, 4145, 293, 364, 13095, 3256, 295, 264, 4145, 366, 2058, 12340, 666, 50840], "temperature": 0.0, "avg_logprob": -0.16072530331818954, "compression_ratio": 1.5248618784530388, "no_speech_prob": 0.0012064040638506413}, {"id": 281, "seek": 177056, "start": 1780.8, "end": 1791.12, "text": " feature space that is optimized. Oops. Sorry. Okay. Now in dream fields, we use the text", "tokens": [50876, 4111, 1901, 300, 307, 26941, 13, 21726, 13, 4919, 13, 1033, 13, 823, 294, 3055, 7909, 11, 321, 764, 264, 2487, 51392], "temperature": 0.0, "avg_logprob": -0.16072530331818954, "compression_ratio": 1.5248618784530388, "no_speech_prob": 0.0012064040638506413}, {"id": 282, "seek": 177056, "start": 1791.12, "end": 1795.2, "text": " encoder of clip. That wasn't being used before. We were just throwing it away after trading.", "tokens": [51392, 2058, 19866, 295, 7353, 13, 663, 2067, 380, 885, 1143, 949, 13, 492, 645, 445, 10238, 309, 1314, 934, 9529, 13, 51596], "temperature": 0.0, "avg_logprob": -0.16072530331818954, "compression_ratio": 1.5248618784530388, "no_speech_prob": 0.0012064040638506413}, {"id": 283, "seek": 179520, "start": 1796.16, "end": 1802.88, "text": " So instead of optimizing for the feature similarity in image feature space,", "tokens": [50412, 407, 2602, 295, 40425, 337, 264, 4111, 32194, 294, 3256, 4111, 1901, 11, 50748], "temperature": 0.0, "avg_logprob": -0.1303581824669471, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.00048778249765746295}, {"id": 284, "seek": 179520, "start": 1802.88, "end": 1808.32, "text": " we now maximize similarity of image and text features. The reason we can swap between", "tokens": [50748, 321, 586, 19874, 32194, 295, 3256, 293, 2487, 4122, 13, 440, 1778, 321, 393, 18135, 1296, 51020], "temperature": 0.0, "avg_logprob": -0.1303581824669471, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.00048778249765746295}, {"id": 285, "seek": 179520, "start": 1809.04, "end": 1812.88, "text": " the text encoder and the image encoder is because clip has learned to align representation.", "tokens": [51056, 264, 2487, 2058, 19866, 293, 264, 3256, 2058, 19866, 307, 570, 7353, 575, 3264, 281, 7975, 10290, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1303581824669471, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.00048778249765746295}, {"id": 286, "seek": 179520, "start": 1812.88, "end": 1817.76, "text": " It has tried to maximize the similarity of representations of images and their associated", "tokens": [51248, 467, 575, 3031, 281, 19874, 264, 32194, 295, 33358, 295, 5267, 293, 641, 6615, 51492], "temperature": 0.0, "avg_logprob": -0.1303581824669471, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.00048778249765746295}, {"id": 287, "seek": 179520, "start": 1817.76, "end": 1823.76, "text": " captions so those representation spaces overlap. And you can in some sense swap the encoders", "tokens": [51492, 44832, 370, 729, 10290, 7673, 19959, 13, 400, 291, 393, 294, 512, 2020, 18135, 264, 2058, 378, 433, 51792], "temperature": 0.0, "avg_logprob": -0.1303581824669471, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.00048778249765746295}, {"id": 288, "seek": 182376, "start": 1824.48, "end": 1828.8, "text": " from text encoder to an image encoder and hopefully still have that aligned representation.", "tokens": [50400, 490, 2487, 2058, 19866, 281, 364, 3256, 2058, 19866, 293, 4696, 920, 362, 300, 17962, 10290, 13, 50616], "temperature": 0.0, "avg_logprob": -0.12712012529373168, "compression_ratio": 1.6221198156682028, "no_speech_prob": 0.00011590818030526862}, {"id": 289, "seek": 182376, "start": 1830.0, "end": 1832.8, "text": " But overall, the pipeline looks fairly similar. So it's randomly sample", "tokens": [50676, 583, 4787, 11, 264, 15517, 1542, 6457, 2531, 13, 407, 309, 311, 16979, 6889, 50816], "temperature": 0.0, "avg_logprob": -0.12712012529373168, "compression_ratio": 1.6221198156682028, "no_speech_prob": 0.00011590818030526862}, {"id": 290, "seek": 182376, "start": 1833.36, "end": 1841.04, "text": " poses in the scene, render an image, and then try to make sure that its semantic features", "tokens": [50844, 26059, 294, 264, 4145, 11, 15529, 364, 3256, 11, 293, 550, 853, 281, 652, 988, 300, 1080, 47982, 4122, 51228], "temperature": 0.0, "avg_logprob": -0.12712012529373168, "compression_ratio": 1.6221198156682028, "no_speech_prob": 0.00011590818030526862}, {"id": 291, "seek": 182376, "start": 1841.04, "end": 1850.08, "text": " match our features of the caption. But if you apply that approach naively without any regularizer,", "tokens": [51228, 2995, 527, 4122, 295, 264, 31974, 13, 583, 498, 291, 3079, 300, 3109, 1667, 3413, 1553, 604, 3890, 6545, 11, 51680], "temperature": 0.0, "avg_logprob": -0.12712012529373168, "compression_ratio": 1.6221198156682028, "no_speech_prob": 0.00011590818030526862}, {"id": 292, "seek": 185008, "start": 1850.1599999999999, "end": 1859.9199999999998, "text": " then there are a bunch of artifacts. These are some example generations for different captions.", "tokens": [50368, 550, 456, 366, 257, 3840, 295, 24617, 13, 1981, 366, 512, 1365, 10593, 337, 819, 44832, 13, 50856], "temperature": 0.0, "avg_logprob": -0.1199361173118033, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.0009398013935424387}, {"id": 293, "seek": 185008, "start": 1859.9199999999998, "end": 1866.0, "text": " I believe this one had something to do with liquid in a blender. This one might have been", "tokens": [50856, 286, 1697, 341, 472, 632, 746, 281, 360, 365, 6553, 294, 257, 24564, 13, 639, 472, 1062, 362, 668, 51160], "temperature": 0.0, "avg_logprob": -0.1199361173118033, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.0009398013935424387}, {"id": 294, "seek": 185008, "start": 1866.0, "end": 1873.9199999999998, "text": " a colorful bus with graffiti on it. So without regularization, we are getting to generate scenes.", "tokens": [51160, 257, 18506, 1255, 365, 40531, 322, 309, 13, 407, 1553, 3890, 2144, 11, 321, 366, 1242, 281, 8460, 8026, 13, 51556], "temperature": 0.0, "avg_logprob": -0.1199361173118033, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.0009398013935424387}, {"id": 295, "seek": 185008, "start": 1873.9199999999998, "end": 1878.6399999999999, "text": " And it's not surprising because there's really no data involved in this process.", "tokens": [51556, 400, 309, 311, 406, 8830, 570, 456, 311, 534, 572, 1412, 3288, 294, 341, 1399, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1199361173118033, "compression_ratio": 1.5895196506550218, "no_speech_prob": 0.0009398013935424387}, {"id": 296, "seek": 187864, "start": 1878.72, "end": 1882.8000000000002, "text": " In Dietner, the scene was regularized by having some input views.", "tokens": [50368, 682, 29606, 1193, 11, 264, 4145, 390, 3890, 1602, 538, 1419, 512, 4846, 6809, 13, 50572], "temperature": 0.0, "avg_logprob": -0.16277642446021512, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0002233662671642378}, {"id": 297, "seek": 187864, "start": 1884.8000000000002, "end": 1886.96, "text": " Here, the canvas is open, wide open.", "tokens": [50672, 1692, 11, 264, 16267, 307, 1269, 11, 4874, 1269, 13, 50780], "temperature": 0.0, "avg_logprob": -0.16277642446021512, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0002233662671642378}, {"id": 298, "seek": 187864, "start": 1891.5200000000002, "end": 1898.88, "text": " So in our work, we added some regularization. The scenes are composited with randomly sample", "tokens": [51008, 407, 294, 527, 589, 11, 321, 3869, 512, 3890, 2144, 13, 440, 8026, 366, 10199, 1226, 365, 16979, 6889, 51376], "temperature": 0.0, "avg_logprob": -0.16277642446021512, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0002233662671642378}, {"id": 299, "seek": 187864, "start": 1898.88, "end": 1906.3200000000002, "text": " backgrounds. And we regularize the scene to be highly transparent. So this transmittance loss", "tokens": [51376, 17336, 13, 400, 321, 3890, 1125, 264, 4145, 281, 312, 5405, 12737, 13, 407, 341, 7715, 593, 719, 4470, 51748], "temperature": 0.0, "avg_logprob": -0.16277642446021512, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0002233662671642378}, {"id": 300, "seek": 190632, "start": 1907.04, "end": 1912.08, "text": " encourages varsity in the underlying scene. So instead of getting lots of low density", "tokens": [50400, 28071, 46130, 507, 294, 264, 14217, 4145, 13, 407, 2602, 295, 1242, 3195, 295, 2295, 10305, 50652], "temperature": 0.0, "avg_logprob": -0.14696549765671355, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00034591477015055716}, {"id": 301, "seek": 190632, "start": 1912.08, "end": 1918.1599999999999, "text": " wispy content, like you saw in the previous slide, with a transmittance loss and this", "tokens": [50652, 261, 7631, 88, 2701, 11, 411, 291, 1866, 294, 264, 3894, 4137, 11, 365, 257, 7715, 593, 719, 4470, 293, 341, 50956], "temperature": 0.0, "avg_logprob": -0.14696549765671355, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00034591477015055716}, {"id": 302, "seek": 190632, "start": 1918.1599999999999, "end": 1922.08, "text": " associated background, our motivation in Dreamfields is to create more of a consistent", "tokens": [50956, 6615, 3678, 11, 527, 12335, 294, 12105, 7610, 82, 307, 281, 1884, 544, 295, 257, 8398, 51152], "temperature": 0.0, "avg_logprob": -0.14696549765671355, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00034591477015055716}, {"id": 303, "seek": 190632, "start": 1922.08, "end": 1931.52, "text": " foreground object, a single foreground object. And these are the renderings for the", "tokens": [51152, 32058, 2657, 11, 257, 2167, 32058, 2657, 13, 400, 613, 366, 264, 15529, 1109, 337, 264, 51624], "temperature": 0.0, "avg_logprob": -0.14696549765671355, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00034591477015055716}, {"id": 304, "seek": 193152, "start": 1931.52, "end": 1941.6, "text": " associated caption, washing blueberries. There's definitely a lot of room for improvement", "tokens": [50364, 6615, 31974, 11, 13836, 43722, 13, 821, 311, 2138, 257, 688, 295, 1808, 337, 10444, 50868], "temperature": 0.0, "avg_logprob": -0.16614270210266113, "compression_ratio": 1.5142857142857142, "no_speech_prob": 0.0010001566261053085}, {"id": 305, "seek": 193152, "start": 1941.6, "end": 1947.12, "text": " because each of these blueberries is kind of mashed together with the others. The general", "tokens": [50868, 570, 1184, 295, 613, 43722, 307, 733, 295, 38964, 1214, 365, 264, 2357, 13, 440, 2674, 51144], "temperature": 0.0, "avg_logprob": -0.16614270210266113, "compression_ratio": 1.5142857142857142, "no_speech_prob": 0.0010001566261053085}, {"id": 306, "seek": 193152, "start": 1947.12, "end": 1952.8799999999999, "text": " caption has been encoded into this scene. And there's a consistent foreground object.", "tokens": [51144, 31974, 575, 668, 2058, 12340, 666, 341, 4145, 13, 400, 456, 311, 257, 8398, 32058, 2657, 13, 51432], "temperature": 0.0, "avg_logprob": -0.16614270210266113, "compression_ratio": 1.5142857142857142, "no_speech_prob": 0.0010001566261053085}, {"id": 307, "seek": 195288, "start": 1953.68, "end": 1964.48, "text": " This is the visualization of the process of optimization. In response to the question,", "tokens": [50404, 639, 307, 264, 25801, 295, 264, 1399, 295, 19618, 13, 682, 4134, 281, 264, 1168, 11, 50944], "temperature": 0.0, "avg_logprob": -0.1378944336421906, "compression_ratio": 1.497142857142857, "no_speech_prob": 0.0004877507162746042}, {"id": 308, "seek": 195288, "start": 1965.2, "end": 1969.44, "text": " Leandra asked, so it's creating this from one image, there's actually no images observed.", "tokens": [50980, 1456, 18401, 2351, 11, 370, 309, 311, 4084, 341, 490, 472, 3256, 11, 456, 311, 767, 572, 5267, 13095, 13, 51192], "temperature": 0.0, "avg_logprob": -0.1378944336421906, "compression_ratio": 1.497142857142857, "no_speech_prob": 0.0004877507162746042}, {"id": 309, "seek": 195288, "start": 1969.44, "end": 1975.0400000000002, "text": " There's only a caption fed to the system. So any images that I'm showing are rendered", "tokens": [51192, 821, 311, 787, 257, 31974, 4636, 281, 264, 1185, 13, 407, 604, 5267, 300, 286, 478, 4099, 366, 28748, 51472], "temperature": 0.0, "avg_logprob": -0.1378944336421906, "compression_ratio": 1.497142857142857, "no_speech_prob": 0.0004877507162746042}, {"id": 310, "seek": 197504, "start": 1976.0, "end": 1979.04, "text": " using our neural radiance field. They're completely fictional.", "tokens": [50412, 1228, 527, 18161, 2843, 6276, 2519, 13, 814, 434, 2584, 28911, 13, 50564], "temperature": 0.0, "avg_logprob": -0.21748270670572917, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.002550322562456131}, {"id": 311, "seek": 197504, "start": 1985.28, "end": 1991.36, "text": " I mean, some intuitive explanation for this is how can we learn a scene representation such", "tokens": [50876, 286, 914, 11, 512, 21769, 10835, 337, 341, 307, 577, 393, 321, 1466, 257, 4145, 10290, 1270, 51180], "temperature": 0.0, "avg_logprob": -0.21748270670572917, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.002550322562456131}, {"id": 312, "seek": 197504, "start": 1991.36, "end": 1995.76, "text": " that it could be captioned with a given caption from any perspective.", "tokens": [51180, 300, 309, 727, 312, 31974, 292, 365, 257, 2212, 31974, 490, 604, 4585, 13, 51400], "temperature": 0.0, "avg_logprob": -0.21748270670572917, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.002550322562456131}, {"id": 313, "seek": 197504, "start": 1998.8, "end": 2003.12, "text": " Maybe that's how a human sculptor went approach the problem. So given a caption, like give me,", "tokens": [51552, 2704, 300, 311, 577, 257, 1952, 12613, 284, 1437, 3109, 264, 1154, 13, 407, 2212, 257, 31974, 11, 411, 976, 385, 11, 51768], "temperature": 0.0, "avg_logprob": -0.21748270670572917, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.002550322562456131}, {"id": 314, "seek": 200312, "start": 2004.0, "end": 2013.6799999999998, "text": " you know, a clay sculpture of a tower. Well, let's say, you know, a monocular sculptor.", "tokens": [50408, 291, 458, 11, 257, 13517, 22972, 295, 257, 10567, 13, 1042, 11, 718, 311, 584, 11, 291, 458, 11, 257, 1108, 905, 1040, 12613, 284, 13, 50892], "temperature": 0.0, "avg_logprob": -0.23082077671104753, "compression_ratio": 1.5521472392638036, "no_speech_prob": 0.0004582572146318853}, {"id": 315, "seek": 200312, "start": 2014.7199999999998, "end": 2020.3999999999999, "text": " Good. Optimize for a clay sculpture that is a tower of many perspective.", "tokens": [50944, 2205, 13, 35013, 1125, 337, 257, 13517, 22972, 300, 307, 257, 10567, 295, 867, 4585, 13, 51228], "temperature": 0.0, "avg_logprob": -0.23082077671104753, "compression_ratio": 1.5521472392638036, "no_speech_prob": 0.0004582572146318853}, {"id": 316, "seek": 200312, "start": 2023.84, "end": 2029.52, "text": " Sorry, what happens? Sorry, what happens if the caption is something vague, like just a dog?", "tokens": [51400, 4919, 11, 437, 2314, 30, 4919, 11, 437, 2314, 498, 264, 31974, 307, 746, 24247, 11, 411, 445, 257, 3000, 30, 51684], "temperature": 0.0, "avg_logprob": -0.23082077671104753, "compression_ratio": 1.5521472392638036, "no_speech_prob": 0.0004582572146318853}, {"id": 317, "seek": 202952, "start": 2029.52, "end": 2034.8799999999999, "text": " How would your optimizer know that, like, it should have the same dog even from different", "tokens": [50364, 1012, 576, 428, 5028, 6545, 458, 300, 11, 411, 11, 309, 820, 362, 264, 912, 3000, 754, 490, 819, 50632], "temperature": 0.0, "avg_logprob": -0.1578984260559082, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.00022338055714499205}, {"id": 318, "seek": 202952, "start": 2035.44, "end": 2043.76, "text": " poses or camera poses? Yeah, excellent question. The constraint that views should", "tokens": [50660, 26059, 420, 2799, 26059, 30, 865, 11, 7103, 1168, 13, 440, 25534, 300, 6809, 820, 51076], "temperature": 0.0, "avg_logprob": -0.1578984260559082, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.00022338055714499205}, {"id": 319, "seek": 202952, "start": 2043.76, "end": 2047.36, "text": " represent the same object from different perspectives just comes from the shared", "tokens": [51076, 2906, 264, 912, 2657, 490, 819, 16766, 445, 1487, 490, 264, 5507, 51256], "temperature": 0.0, "avg_logprob": -0.1578984260559082, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.00022338055714499205}, {"id": 320, "seek": 202952, "start": 2047.36, "end": 2053.84, "text": " three presentation. We're optimizing the same MLP from any perspective.", "tokens": [51256, 1045, 5860, 13, 492, 434, 40425, 264, 912, 21601, 47, 490, 604, 4585, 13, 51580], "temperature": 0.0, "avg_logprob": -0.1578984260559082, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.00022338055714499205}, {"id": 321, "seek": 205384, "start": 2053.84, "end": 2062.1600000000003, "text": " Okay, thanks. We had to simplify. Well, we didn't have to. You're able to keep view dependence in", "tokens": [50364, 1033, 11, 3231, 13, 492, 632, 281, 20460, 13, 1042, 11, 321, 994, 380, 362, 281, 13, 509, 434, 1075, 281, 1066, 1910, 31704, 294, 50780], "temperature": 0.0, "avg_logprob": -0.20931773319422642, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0015974995912984014}, {"id": 322, "seek": 205384, "start": 2062.1600000000003, "end": 2066.6400000000003, "text": " the neural radiance field. So this regularizer ends up being kind of important. Like I discussed", "tokens": [50780, 264, 18161, 2843, 6276, 2519, 13, 407, 341, 3890, 6545, 5314, 493, 885, 733, 295, 1021, 13, 1743, 286, 7152, 51004], "temperature": 0.0, "avg_logprob": -0.20931773319422642, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0015974995912984014}, {"id": 323, "seek": 205384, "start": 2066.6400000000003, "end": 2069.84, "text": " with Dietner, if you're able to learn a lot of these near field artifacts.", "tokens": [51004, 365, 29606, 1193, 11, 498, 291, 434, 1075, 281, 1466, 257, 688, 295, 613, 2651, 2519, 24617, 13, 51164], "temperature": 0.0, "avg_logprob": -0.20931773319422642, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0015974995912984014}, {"id": 324, "seek": 205384, "start": 2073.44, "end": 2077.1200000000003, "text": " Sharing the scene representation is important, but some of the other techniques on our paper,", "tokens": [51344, 49060, 264, 4145, 10290, 307, 1021, 11, 457, 512, 295, 264, 661, 7512, 322, 527, 3035, 11, 51528], "temperature": 0.0, "avg_logprob": -0.20931773319422642, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0015974995912984014}, {"id": 325, "seek": 205384, "start": 2077.1200000000003, "end": 2081.04, "text": " like the regularizer are also important for making sure you get a clean result.", "tokens": [51528, 411, 264, 3890, 6545, 366, 611, 1021, 337, 1455, 988, 291, 483, 257, 2541, 1874, 13, 51724], "temperature": 0.0, "avg_logprob": -0.20931773319422642, "compression_ratio": 1.6227106227106227, "no_speech_prob": 0.0015974995912984014}, {"id": 326, "seek": 208384, "start": 2084.8, "end": 2092.96, "text": " In this example, we experiment with different caption templates to measure the", "tokens": [50412, 682, 341, 1365, 11, 321, 5120, 365, 819, 31974, 21165, 281, 3481, 264, 50820], "temperature": 0.0, "avg_logprob": -0.12490043211519049, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.0002530973288230598}, {"id": 327, "seek": 208384, "start": 2092.96, "end": 2098.1600000000003, "text": " compositional generalization of the model. So the base caption template here is a teapot", "tokens": [50820, 10199, 2628, 2674, 2144, 295, 264, 2316, 13, 407, 264, 3096, 31974, 12379, 510, 307, 257, 535, 569, 310, 51080], "temperature": 0.0, "avg_logprob": -0.12490043211519049, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.0002530973288230598}, {"id": 328, "seek": 208384, "start": 2098.1600000000003, "end": 2105.52, "text": " in the shape of a blank, a teapot imitating a blank. And then in the video, the caption beneath", "tokens": [51080, 294, 264, 3909, 295, 257, 8247, 11, 257, 535, 569, 310, 566, 16350, 257, 8247, 13, 400, 550, 294, 264, 960, 11, 264, 31974, 17149, 51448], "temperature": 0.0, "avg_logprob": -0.12490043211519049, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.0002530973288230598}, {"id": 329, "seek": 208384, "start": 2105.52, "end": 2112.56, "text": " each object is the word that's filled into the template caption. So a teapot in the shape of", "tokens": [51448, 1184, 2657, 307, 264, 1349, 300, 311, 6412, 666, 264, 12379, 31974, 13, 407, 257, 535, 569, 310, 294, 264, 3909, 295, 51800], "temperature": 0.0, "avg_logprob": -0.12490043211519049, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.0002530973288230598}, {"id": 330, "seek": 211256, "start": 2112.56, "end": 2118.16, "text": " an avocado produces this object. Whereas the caption of teapot in the shape of a glacier", "tokens": [50364, 364, 27041, 14725, 341, 2657, 13, 13813, 264, 31974, 295, 535, 569, 310, 294, 264, 3909, 295, 257, 48021, 50644], "temperature": 0.0, "avg_logprob": -0.13414047576568938, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.0003682243695948273}, {"id": 331, "seek": 211256, "start": 2118.16, "end": 2126.32, "text": " produces something more ice styled. And I'm sorry about these animations.", "tokens": [50644, 14725, 746, 544, 4435, 7952, 1493, 13, 400, 286, 478, 2597, 466, 613, 22868, 13, 51052], "temperature": 0.0, "avg_logprob": -0.13414047576568938, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.0003682243695948273}, {"id": 332, "seek": 211256, "start": 2128.4, "end": 2134.56, "text": " If you switch the caption from an armchair to a teapot, you'll also notice some changes in the", "tokens": [51156, 759, 291, 3679, 264, 31974, 490, 364, 3726, 17892, 281, 257, 535, 569, 310, 11, 291, 603, 611, 3449, 512, 2962, 294, 264, 51464], "temperature": 0.0, "avg_logprob": -0.13414047576568938, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.0003682243695948273}, {"id": 333, "seek": 211256, "start": 2134.56, "end": 2139.2799999999997, "text": " shape. So there's legs on this avocado chair, but when it becomes teapot, the legs are removed.", "tokens": [51464, 3909, 13, 407, 456, 311, 5668, 322, 341, 27041, 6090, 11, 457, 562, 309, 3643, 535, 569, 310, 11, 264, 5668, 366, 7261, 13, 51700], "temperature": 0.0, "avg_logprob": -0.13414047576568938, "compression_ratio": 1.6342592592592593, "no_speech_prob": 0.0003682243695948273}, {"id": 334, "seek": 213928, "start": 2139.76, "end": 2146.7200000000003, "text": " There's a follow-up question about whether the Clip Library is 2D. Yes, Clip is trained only on", "tokens": [50388, 821, 311, 257, 1524, 12, 1010, 1168, 466, 1968, 264, 2033, 647, 12806, 307, 568, 35, 13, 1079, 11, 2033, 647, 307, 8895, 787, 322, 50736], "temperature": 0.0, "avg_logprob": -0.1533823776245117, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.00024533155374228954}, {"id": 335, "seek": 213928, "start": 2146.7200000000003, "end": 2154.48, "text": " 2D images. So just on 2D views. The motivation for using Clip is that we can very scaleably", "tokens": [50736, 568, 35, 5267, 13, 407, 445, 322, 568, 35, 6809, 13, 440, 12335, 337, 1228, 2033, 647, 307, 300, 321, 393, 588, 4373, 1188, 51124], "temperature": 0.0, "avg_logprob": -0.1533823776245117, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.00024533155374228954}, {"id": 336, "seek": 213928, "start": 2154.48, "end": 2159.76, "text": " acquire caption images from the internet. If you, for example, look at Wikipedia and just look at", "tokens": [51124, 20001, 31974, 5267, 490, 264, 4705, 13, 759, 291, 11, 337, 1365, 11, 574, 412, 28999, 293, 445, 574, 412, 51388], "temperature": 0.0, "avg_logprob": -0.1533823776245117, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.00024533155374228954}, {"id": 337, "seek": 213928, "start": 2160.6400000000003, "end": 2165.92, "text": " the upper right image associated with each article, it has a caption beneath it. And there's a data", "tokens": [51432, 264, 6597, 558, 3256, 6615, 365, 1184, 7222, 11, 309, 575, 257, 31974, 17149, 309, 13, 400, 456, 311, 257, 1412, 51696], "temperature": 0.0, "avg_logprob": -0.1533823776245117, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.00024533155374228954}, {"id": 338, "seek": 216592, "start": 2166.0, "end": 2169.76, "text": " set out there called WikiText, which has about 11 million captioned images.", "tokens": [50368, 992, 484, 456, 1219, 35892, 50198, 11, 597, 575, 466, 2975, 2459, 31974, 292, 5267, 13, 50556], "temperature": 0.0, "avg_logprob": -0.0996575962413441, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.0010984971886500716}, {"id": 339, "seek": 216592, "start": 2170.96, "end": 2176.08, "text": " The authors of Clip were able to collect even larger data set by scraping websites other than", "tokens": [50616, 440, 16552, 295, 2033, 647, 645, 1075, 281, 2500, 754, 4833, 1412, 992, 538, 43738, 12891, 661, 813, 50872], "temperature": 0.0, "avg_logprob": -0.0996575962413441, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.0010984971886500716}, {"id": 340, "seek": 216592, "start": 2176.08, "end": 2184.08, "text": " Wikipedia. But if you look at data sets with 3D objects in them, they're very small. The largest", "tokens": [50872, 28999, 13, 583, 498, 291, 574, 412, 1412, 6352, 365, 805, 35, 6565, 294, 552, 11, 436, 434, 588, 1359, 13, 440, 6443, 51272], "temperature": 0.0, "avg_logprob": -0.0996575962413441, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.0010984971886500716}, {"id": 341, "seek": 216592, "start": 2184.08, "end": 2189.44, "text": " might be ShapeNet, which is entirely synthetic objects. And there's usually no caption associated.", "tokens": [51272, 1062, 312, 49148, 31890, 11, 597, 307, 7696, 23420, 6565, 13, 400, 456, 311, 2673, 572, 31974, 6615, 13, 51540], "temperature": 0.0, "avg_logprob": -0.0996575962413441, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.0010984971886500716}, {"id": 342, "seek": 216592, "start": 2189.44, "end": 2195.52, "text": " So we'd have to have a human annotate. This is a general trend in the 3D reconstruction literature", "tokens": [51540, 407, 321, 1116, 362, 281, 362, 257, 1952, 25339, 473, 13, 639, 307, 257, 2674, 6028, 294, 264, 805, 35, 31565, 10394, 51844], "temperature": 0.0, "avg_logprob": -0.0996575962413441, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.0010984971886500716}, {"id": 343, "seek": 219552, "start": 2195.52, "end": 2204.0, "text": " that the availability of 3D data is quite limited. And so in dream fields, we're able to exploit this", "tokens": [50364, 300, 264, 17945, 295, 805, 35, 1412, 307, 1596, 5567, 13, 400, 370, 294, 3055, 7909, 11, 321, 434, 1075, 281, 25924, 341, 50788], "temperature": 0.0, "avg_logprob": -0.08331337610880533, "compression_ratio": 1.4306930693069306, "no_speech_prob": 0.00011771519348258153}, {"id": 344, "seek": 219552, "start": 2205.84, "end": 2211.7599999999998, "text": " pre-trained 2D image encoder and text encoder, and then kind of lift it up into 3D by using", "tokens": [50880, 659, 12, 17227, 2001, 568, 35, 3256, 2058, 19866, 293, 2487, 2058, 19866, 11, 293, 550, 733, 295, 5533, 309, 493, 666, 805, 35, 538, 1228, 51176], "temperature": 0.0, "avg_logprob": -0.08331337610880533, "compression_ratio": 1.4306930693069306, "no_speech_prob": 0.00011771519348258153}, {"id": 345, "seek": 219552, "start": 2211.7599999999998, "end": 2221.36, "text": " a shared representation of the geometry. There's a bunch of different techniques that we use to", "tokens": [51176, 257, 5507, 10290, 295, 264, 18426, 13, 821, 311, 257, 3840, 295, 819, 7512, 300, 321, 764, 281, 51656], "temperature": 0.0, "avg_logprob": -0.08331337610880533, "compression_ratio": 1.4306930693069306, "no_speech_prob": 0.00011771519348258153}, {"id": 346, "seek": 222136, "start": 2221.36, "end": 2226.8, "text": " improve the quality of the results. I won't get too much into this, but the metric is a little", "tokens": [50364, 3470, 264, 3125, 295, 264, 3542, 13, 286, 1582, 380, 483, 886, 709, 666, 341, 11, 457, 264, 20678, 307, 257, 707, 50636], "temperature": 0.0, "avg_logprob": -0.07104910456615945, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0017002306412905455}, {"id": 347, "seek": 222136, "start": 2226.8, "end": 2233.28, "text": " tricky to define because there's no reference object for each caption. We only have a data", "tokens": [50636, 12414, 281, 6964, 570, 456, 311, 572, 6408, 2657, 337, 1184, 31974, 13, 492, 787, 362, 257, 1412, 50960], "temperature": 0.0, "avg_logprob": -0.07104910456615945, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0017002306412905455}, {"id": 348, "seek": 222136, "start": 2233.28, "end": 2240.32, "text": " set of captions provided to us by the user, and we're one of measure how well our generations are", "tokens": [50960, 992, 295, 44832, 5649, 281, 505, 538, 264, 4195, 11, 293, 321, 434, 472, 295, 3481, 577, 731, 527, 10593, 366, 51312], "temperature": 0.0, "avg_logprob": -0.07104910456615945, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0017002306412905455}, {"id": 349, "seek": 222136, "start": 2240.32, "end": 2248.88, "text": " performing. In order to do that, we use a neural metric based off of matching generated 3D objects", "tokens": [51312, 10205, 13, 682, 1668, 281, 360, 300, 11, 321, 764, 257, 18161, 20678, 2361, 766, 295, 14324, 10833, 805, 35, 6565, 51740], "temperature": 0.0, "avg_logprob": -0.07104910456615945, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0017002306412905455}, {"id": 350, "seek": 224888, "start": 2248.88, "end": 2255.12, "text": " against the input captions. This is something like precision of retrieving the correct caption,", "tokens": [50364, 1970, 264, 4846, 44832, 13, 639, 307, 746, 411, 18356, 295, 19817, 798, 264, 3006, 31974, 11, 50676], "temperature": 0.0, "avg_logprob": -0.15344278232471362, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0001739629078656435}, {"id": 351, "seek": 224888, "start": 2255.12, "end": 2259.52, "text": " given the generator objects. Some of the most important techniques that help us here are", "tokens": [50676, 2212, 264, 19265, 6565, 13, 2188, 295, 264, 881, 1021, 7512, 300, 854, 505, 510, 366, 50896], "temperature": 0.0, "avg_logprob": -0.15344278232471362, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0001739629078656435}, {"id": 352, "seek": 224888, "start": 2261.52, "end": 2267.6, "text": " regularizer for transmittance and data augmentations, those architecture we use for the MLP,", "tokens": [50996, 3890, 6545, 337, 7715, 593, 719, 293, 1412, 29919, 763, 11, 729, 9482, 321, 764, 337, 264, 21601, 47, 11, 51300], "temperature": 0.0, "avg_logprob": -0.15344278232471362, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0001739629078656435}, {"id": 353, "seek": 224888, "start": 2270.7200000000003, "end": 2275.28, "text": " and then later on, what model we use for clip.", "tokens": [51456, 293, 550, 1780, 322, 11, 437, 2316, 321, 764, 337, 7353, 13, 51684], "temperature": 0.0, "avg_logprob": -0.15344278232471362, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0001739629078656435}, {"id": 354, "seek": 227888, "start": 2278.88, "end": 2281.92, "text": " This is an example of the process of optimization from different iterations,", "tokens": [50364, 639, 307, 364, 1365, 295, 264, 1399, 295, 19618, 490, 819, 36540, 11, 50516], "temperature": 0.0, "avg_logprob": -0.12401930166750538, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0007552065071649849}, {"id": 355, "seek": 227888, "start": 2281.92, "end": 2286.0, "text": " so it actually can converge quite quickly, but additional detail might be added over the course", "tokens": [50516, 370, 309, 767, 393, 41881, 1596, 2661, 11, 457, 4497, 2607, 1062, 312, 3869, 670, 264, 1164, 50720], "temperature": 0.0, "avg_logprob": -0.12401930166750538, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0007552065071649849}, {"id": 356, "seek": 227888, "start": 2286.0, "end": 2292.88, "text": " of training. In order to run 20,000 iterations of optimization, it's an expensive process", "tokens": [50720, 295, 3097, 13, 682, 1668, 281, 1190, 945, 11, 1360, 36540, 295, 19618, 11, 309, 311, 364, 5124, 1399, 51064], "temperature": 0.0, "avg_logprob": -0.12401930166750538, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0007552065071649849}, {"id": 357, "seek": 227888, "start": 2292.88, "end": 2298.7200000000003, "text": " because we need to render out these images during training, but back of the envelope calculation", "tokens": [51064, 570, 321, 643, 281, 15529, 484, 613, 5267, 1830, 3097, 11, 457, 646, 295, 264, 19989, 17108, 51356], "temperature": 0.0, "avg_logprob": -0.12401930166750538, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0007552065071649849}, {"id": 358, "seek": 227888, "start": 2298.7200000000003, "end": 2306.32, "text": " is about three to four dollars to generate each model on TPU in Google Cloud at an hour.", "tokens": [51356, 307, 466, 1045, 281, 1451, 3808, 281, 8460, 1184, 2316, 322, 314, 8115, 294, 3329, 8061, 412, 364, 1773, 13, 51736], "temperature": 0.0, "avg_logprob": -0.12401930166750538, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0007552065071649849}, {"id": 359, "seek": 230632, "start": 2306.48, "end": 2309.44, "text": " It's in the realm where an artist could afford to do this.", "tokens": [50372, 467, 311, 294, 264, 15355, 689, 364, 5748, 727, 6157, 281, 360, 341, 13, 50520], "temperature": 0.0, "avg_logprob": -0.12391387985413332, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.00020338993635959923}, {"id": 360, "seek": 230632, "start": 2310.7200000000003, "end": 2315.52, "text": " We're working on some follow-up work, which will speed up this process and make it even less expensive.", "tokens": [50584, 492, 434, 1364, 322, 512, 1524, 12, 1010, 589, 11, 597, 486, 3073, 493, 341, 1399, 293, 652, 309, 754, 1570, 5124, 13, 50824], "temperature": 0.0, "avg_logprob": -0.12391387985413332, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.00020338993635959923}, {"id": 361, "seek": 230632, "start": 2321.6000000000004, "end": 2327.44, "text": " That's all I've got on these works. The broad goal here is to make content creation", "tokens": [51128, 663, 311, 439, 286, 600, 658, 322, 613, 1985, 13, 440, 4152, 3387, 510, 307, 281, 652, 2701, 8016, 51420], "temperature": 0.0, "avg_logprob": -0.12391387985413332, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.00020338993635959923}, {"id": 362, "seek": 230632, "start": 2328.2400000000002, "end": 2335.52, "text": " easier and generate assets that are useful. This 3D assets I see is particularly useful", "tokens": [51460, 3571, 293, 8460, 9769, 300, 366, 4420, 13, 639, 805, 35, 9769, 286, 536, 307, 4098, 4420, 51824], "temperature": 0.0, "avg_logprob": -0.12391387985413332, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.00020338993635959923}, {"id": 363, "seek": 233552, "start": 2335.52, "end": 2340.4, "text": " for downstream applications because they could be plugged into a game or plugged into some other", "tokens": [50364, 337, 30621, 5821, 570, 436, 727, 312, 25679, 666, 257, 1216, 420, 25679, 666, 512, 661, 50608], "temperature": 0.0, "avg_logprob": -0.10289993671455769, "compression_ratio": 1.576, "no_speech_prob": 0.0009996300796046853}, {"id": 364, "seek": 233552, "start": 2340.4, "end": 2347.2, "text": " system. We have code out for both of these projects. If you want to try out the text", "tokens": [50608, 1185, 13, 492, 362, 3089, 484, 337, 1293, 295, 613, 4455, 13, 759, 291, 528, 281, 853, 484, 264, 2487, 50948], "temperature": 0.0, "avg_logprob": -0.10289993671455769, "compression_ratio": 1.576, "no_speech_prob": 0.0009996300796046853}, {"id": 365, "seek": 233552, "start": 2347.2, "end": 2351.7599999999998, "text": " to 3D generation in your browser, you can use a Colab notebook that I put together.", "tokens": [50948, 281, 805, 35, 5125, 294, 428, 11185, 11, 291, 393, 764, 257, 4004, 455, 21060, 300, 286, 829, 1214, 13, 51176], "temperature": 0.0, "avg_logprob": -0.10289993671455769, "compression_ratio": 1.576, "no_speech_prob": 0.0009996300796046853}, {"id": 366, "seek": 233552, "start": 2352.4, "end": 2357.2, "text": " I've tested it on the Pro version of Colab, which has higher memory GPUs,", "tokens": [51208, 286, 600, 8246, 309, 322, 264, 1705, 3037, 295, 4004, 455, 11, 597, 575, 2946, 4675, 18407, 82, 11, 51448], "temperature": 0.0, "avg_logprob": -0.10289993671455769, "compression_ratio": 1.576, "no_speech_prob": 0.0009996300796046853}, {"id": 367, "seek": 233552, "start": 2357.2, "end": 2358.88, "text": " so you might need to play with some of the parameters.", "tokens": [51448, 370, 291, 1062, 643, 281, 862, 365, 512, 295, 264, 9834, 13, 51532], "temperature": 0.0, "avg_logprob": -0.10289993671455769, "compression_ratio": 1.576, "no_speech_prob": 0.0009996300796046853}, {"id": 368, "seek": 235888, "start": 2358.88, "end": 2368.88, "text": " Thank you so much, Eje. This is really fascinating. I have a few questions,", "tokens": [50364, 1044, 291, 370, 709, 11, 462, 2884, 13, 639, 307, 534, 10343, 13, 286, 362, 257, 1326, 1651, 11, 50864], "temperature": 0.0, "avg_logprob": -0.19365015484037854, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.0012632649159058928}, {"id": 369, "seek": 235888, "start": 2370.2400000000002, "end": 2377.6, "text": " and then before letting everyone else ask questions, the first question is that", "tokens": [50932, 293, 550, 949, 8295, 1518, 1646, 1029, 1651, 11, 264, 700, 1168, 307, 300, 51300], "temperature": 0.0, "avg_logprob": -0.19365015484037854, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.0012632649159058928}, {"id": 370, "seek": 235888, "start": 2379.6800000000003, "end": 2388.56, "text": " are you able to walk us through some of the Colab code today or should we do it on our time?", "tokens": [51404, 366, 291, 1075, 281, 1792, 505, 807, 512, 295, 264, 4004, 455, 3089, 965, 420, 820, 321, 360, 309, 322, 527, 565, 30, 51848], "temperature": 0.0, "avg_logprob": -0.19365015484037854, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.0012632649159058928}, {"id": 371, "seek": 238888, "start": 2389.84, "end": 2396.56, "text": " Let me see if I have it up. Also, before going to changing your screen,", "tokens": [50412, 961, 385, 536, 498, 286, 362, 309, 493, 13, 2743, 11, 949, 516, 281, 4473, 428, 2568, 11, 50748], "temperature": 0.0, "avg_logprob": -0.18547920619740205, "compression_ratio": 1.4342857142857144, "no_speech_prob": 0.001168732764199376}, {"id": 372, "seek": 238888, "start": 2396.56, "end": 2405.28, "text": " can you please go back to the animations? Sorry, I have so many questions because this", "tokens": [50748, 393, 291, 1767, 352, 646, 281, 264, 22868, 30, 4919, 11, 286, 362, 370, 867, 1651, 570, 341, 51184], "temperature": 0.0, "avg_logprob": -0.18547920619740205, "compression_ratio": 1.4342857142857144, "no_speech_prob": 0.001168732764199376}, {"id": 373, "seek": 238888, "start": 2405.28, "end": 2415.6800000000003, "text": " is really cool. Or maybe the one that is armchair. Yeah, give me one sec. Thank you so much.", "tokens": [51184, 307, 534, 1627, 13, 1610, 1310, 264, 472, 300, 307, 3726, 17892, 13, 865, 11, 976, 385, 472, 907, 13, 1044, 291, 370, 709, 13, 51704], "temperature": 0.0, "avg_logprob": -0.18547920619740205, "compression_ratio": 1.4342857142857144, "no_speech_prob": 0.001168732764199376}, {"id": 374, "seek": 241888, "start": 2419.36, "end": 2429.36, "text": " I think these are really cool. I think that for the students and I, this kind of inspires us to", "tokens": [50388, 286, 519, 613, 366, 534, 1627, 13, 286, 519, 300, 337, 264, 1731, 293, 286, 11, 341, 733, 295, 32566, 505, 281, 50888], "temperature": 0.0, "avg_logprob": -0.13800149645124163, "compression_ratio": 1.774390243902439, "no_speech_prob": 0.001150523778051138}, {"id": 375, "seek": 241888, "start": 2429.36, "end": 2440.6400000000003, "text": " think maybe one cool thing to do is that we can generate these things and use them in some avatar", "tokens": [50888, 519, 1310, 472, 1627, 551, 281, 360, 307, 300, 321, 393, 8460, 613, 721, 293, 764, 552, 294, 512, 36205, 51452], "temperature": 0.0, "avg_logprob": -0.13800149645124163, "compression_ratio": 1.774390243902439, "no_speech_prob": 0.001150523778051138}, {"id": 376, "seek": 241888, "start": 2440.6400000000003, "end": 2447.52, "text": " or game or something, and this will be really cool. This is something for students to think about", "tokens": [51452, 420, 1216, 420, 746, 11, 293, 341, 486, 312, 534, 1627, 13, 639, 307, 746, 337, 1731, 281, 519, 466, 51796], "temperature": 0.0, "avg_logprob": -0.13800149645124163, "compression_ratio": 1.774390243902439, "no_speech_prob": 0.001150523778051138}, {"id": 377, "seek": 244752, "start": 2448.48, "end": 2457.92, "text": " for their future projects because the goal of this course is to inspire us to think about", "tokens": [50412, 337, 641, 2027, 4455, 570, 264, 3387, 295, 341, 1164, 307, 281, 15638, 505, 281, 519, 466, 50884], "temperature": 0.0, "avg_logprob": -0.15059219581493433, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.0013837326550856233}, {"id": 378, "seek": 244752, "start": 2457.92, "end": 2464.4, "text": " what are the creative ways that we can use AI. This is really cool. One question that I have", "tokens": [50884, 437, 366, 264, 5880, 2098, 300, 321, 393, 764, 7318, 13, 639, 307, 534, 1627, 13, 1485, 1168, 300, 286, 362, 51208], "temperature": 0.0, "avg_logprob": -0.15059219581493433, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.0013837326550856233}, {"id": 379, "seek": 244752, "start": 2464.4, "end": 2474.16, "text": " is that, can you share some intuition of, for instance, let's say the rubric. It looks like", "tokens": [51208, 307, 300, 11, 393, 291, 2073, 512, 24002, 295, 11, 337, 5197, 11, 718, 311, 584, 264, 5915, 1341, 13, 467, 1542, 411, 51696], "temperature": 0.0, "avg_logprob": -0.15059219581493433, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.0013837326550856233}, {"id": 380, "seek": 247416, "start": 2474.24, "end": 2483.04, "text": " a rubric and it looks like a chair, but then we see that there is some, we wish there was more of the", "tokens": [50368, 257, 5915, 1341, 293, 309, 1542, 411, 257, 6090, 11, 457, 550, 321, 536, 300, 456, 307, 512, 11, 321, 3172, 456, 390, 544, 295, 264, 50808], "temperature": 0.0, "avg_logprob": -0.1440033285241378, "compression_ratio": 1.655367231638418, "no_speech_prob": 0.003119133645668626}, {"id": 381, "seek": 247416, "start": 2484.08, "end": 2493.7599999999998, "text": " structure and it might be because clip is the objective and or assessor and thinking that,", "tokens": [50860, 3877, 293, 309, 1062, 312, 570, 7353, 307, 264, 10024, 293, 420, 5877, 284, 293, 1953, 300, 11, 51344], "temperature": 0.0, "avg_logprob": -0.1440033285241378, "compression_ratio": 1.655367231638418, "no_speech_prob": 0.003119133645668626}, {"id": 382, "seek": 247416, "start": 2493.7599999999998, "end": 2502.7999999999997, "text": " okay, as long as I have a patch of red and yellow and things like that that are appearing on rubric,", "tokens": [51344, 1392, 11, 382, 938, 382, 286, 362, 257, 9972, 295, 2182, 293, 5566, 293, 721, 411, 300, 300, 366, 19870, 322, 5915, 1341, 11, 51796], "temperature": 0.0, "avg_logprob": -0.1440033285241378, "compression_ratio": 1.655367231638418, "no_speech_prob": 0.003119133645668626}, {"id": 383, "seek": 250280, "start": 2502.88, "end": 2508.8, "text": " I'm happy, the rest, I don't care much, or is there any better explanation of what's happening?", "tokens": [50368, 286, 478, 2055, 11, 264, 1472, 11, 286, 500, 380, 1127, 709, 11, 420, 307, 456, 604, 1101, 10835, 295, 437, 311, 2737, 30, 50664], "temperature": 0.0, "avg_logprob": -0.13837670170983604, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.00032495276536792517}, {"id": 384, "seek": 250280, "start": 2510.2400000000002, "end": 2518.8, "text": " Yeah, so the 3D structure only emerges because of the shared representation and the easiest way to", "tokens": [50736, 865, 11, 370, 264, 805, 35, 3877, 787, 38965, 570, 295, 264, 5507, 10290, 293, 264, 12889, 636, 281, 51164], "temperature": 0.0, "avg_logprob": -0.13837670170983604, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.00032495276536792517}, {"id": 385, "seek": 250280, "start": 2520.48, "end": 2525.44, "text": " satisfy clip from any perspective, having this Rubik's Cube chair from any perspective,", "tokens": [51248, 19319, 7353, 490, 604, 4585, 11, 1419, 341, 10518, 1035, 311, 33003, 6090, 490, 604, 4585, 11, 51496], "temperature": 0.0, "avg_logprob": -0.13837670170983604, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.00032495276536792517}, {"id": 386, "seek": 250280, "start": 2526.5600000000004, "end": 2531.44, "text": " might actually be to learn some consistent geometry. That said, there's no prior", "tokens": [51552, 1062, 767, 312, 281, 1466, 512, 8398, 18426, 13, 663, 848, 11, 456, 311, 572, 4059, 51796], "temperature": 0.0, "avg_logprob": -0.13837670170983604, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.00032495276536792517}, {"id": 387, "seek": 253144, "start": 2532.4, "end": 2537.2000000000003, "text": " other than sparsity and some implicit regularization just in the structure of the MLP,", "tokens": [50412, 661, 813, 637, 685, 507, 293, 512, 26947, 3890, 2144, 445, 294, 264, 3877, 295, 264, 21601, 47, 11, 50652], "temperature": 0.0, "avg_logprob": -0.1270974620004718, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0006460219738073647}, {"id": 388, "seek": 253144, "start": 2537.84, "end": 2544.56, "text": " so there's no prior on the 3D structure learned from data. That's something that I think is missing", "tokens": [50684, 370, 456, 311, 572, 4059, 322, 264, 805, 35, 3877, 3264, 490, 1412, 13, 663, 311, 746, 300, 286, 519, 307, 5361, 51020], "temperature": 0.0, "avg_logprob": -0.1270974620004718, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0006460219738073647}, {"id": 389, "seek": 253144, "start": 2544.56, "end": 2550.8, "text": " and definitely opportunity for future work is how do we learn some priors on 3D data and integrate", "tokens": [51020, 293, 2138, 2650, 337, 2027, 589, 307, 577, 360, 321, 1466, 512, 1790, 830, 322, 805, 35, 1412, 293, 13365, 51332], "temperature": 0.0, "avg_logprob": -0.1270974620004718, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0006460219738073647}, {"id": 390, "seek": 253144, "start": 2550.8, "end": 2558.2400000000002, "text": " them into the system to try to improve the plausibility of the structure. One example where", "tokens": [51332, 552, 666, 264, 1185, 281, 853, 281, 3470, 264, 34946, 2841, 295, 264, 3877, 13, 1485, 1365, 689, 51704], "temperature": 0.0, "avg_logprob": -0.1270974620004718, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0006460219738073647}, {"id": 391, "seek": 255824, "start": 2558.24, "end": 2565.7599999999998, "text": " this issue arises is that sometimes you'll get repeated structures on the objects,", "tokens": [50364, 341, 2734, 27388, 307, 300, 2171, 291, 603, 483, 10477, 9227, 322, 264, 6565, 11, 50740], "temperature": 0.0, "avg_logprob": -0.08683766388311619, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.00017950424808077514}, {"id": 392, "seek": 255824, "start": 2566.72, "end": 2572.72, "text": " like if you optimize for a dog, maybe it will have eyes on multiple sides of its face because", "tokens": [50788, 411, 498, 291, 19719, 337, 257, 3000, 11, 1310, 309, 486, 362, 2575, 322, 3866, 4881, 295, 1080, 1851, 570, 51088], "temperature": 0.0, "avg_logprob": -0.08683766388311619, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.00017950424808077514}, {"id": 393, "seek": 255824, "start": 2573.68, "end": 2580.08, "text": " they're not visible. So you only see two sets of eyes from any particular viewpoint,", "tokens": [51136, 436, 434, 406, 8974, 13, 407, 291, 787, 536, 732, 6352, 295, 2575, 490, 604, 1729, 35248, 11, 51456], "temperature": 0.0, "avg_logprob": -0.08683766388311619, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.00017950424808077514}, {"id": 394, "seek": 255824, "start": 2580.08, "end": 2585.9199999999996, "text": " that is all the discriminator clip ever sees are those two eyes, but the underlying geometry,", "tokens": [51456, 300, 307, 439, 264, 20828, 1639, 7353, 1562, 8194, 366, 729, 732, 2575, 11, 457, 264, 14217, 18426, 11, 51748], "temperature": 0.0, "avg_logprob": -0.08683766388311619, "compression_ratio": 1.5707964601769913, "no_speech_prob": 0.00017950424808077514}, {"id": 395, "seek": 258592, "start": 2585.92, "end": 2589.28, "text": " there's no constraint that the dog should only have to rise.", "tokens": [50364, 456, 311, 572, 25534, 300, 264, 3000, 820, 787, 362, 281, 6272, 13, 50532], "temperature": 0.0, "avg_logprob": -0.2138105875038239, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.0005581927835009992}, {"id": 396, "seek": 258592, "start": 2591.36, "end": 2596.32, "text": " Okay, excellent. Thank you so much. Are there questions before we go to the collab?", "tokens": [50636, 1033, 11, 7103, 13, 1044, 291, 370, 709, 13, 2014, 456, 1651, 949, 321, 352, 281, 264, 44228, 30, 50884], "temperature": 0.0, "avg_logprob": -0.2138105875038239, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.0005581927835009992}, {"id": 397, "seek": 258592, "start": 2601.6800000000003, "end": 2609.36, "text": " The outputs, are they like .fbx files or do they still need to be, let's say, a little bit", "tokens": [51152, 440, 23930, 11, 366, 436, 411, 2411, 69, 65, 87, 7098, 420, 360, 436, 920, 643, 281, 312, 11, 718, 311, 584, 11, 257, 707, 857, 51536], "temperature": 0.0, "avg_logprob": -0.2138105875038239, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.0005581927835009992}, {"id": 398, "seek": 258592, "start": 2609.36, "end": 2615.52, "text": " prepared in rendering software before they can be actually readily used in the game engine?", "tokens": [51536, 4927, 294, 22407, 4722, 949, 436, 393, 312, 767, 26336, 1143, 294, 264, 1216, 2848, 30, 51844], "temperature": 0.0, "avg_logprob": -0.2138105875038239, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.0005581927835009992}, {"id": 399, "seek": 261592, "start": 2615.92, "end": 2622.48, "text": " Like Unity or Unreal? They do need to be post-processed. So what you get out is a train neural net,", "tokens": [50364, 1743, 27913, 420, 34464, 30, 814, 360, 643, 281, 312, 2183, 12, 41075, 292, 13, 407, 437, 291, 483, 484, 307, 257, 3847, 18161, 2533, 11, 50692], "temperature": 0.0, "avg_logprob": -0.16703498840332032, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0003300123498775065}, {"id": 400, "seek": 261592, "start": 2622.48, "end": 2627.2000000000003, "text": " so it's function mapping from coordinates. We don't use the v direction in these results,", "tokens": [50692, 370, 309, 311, 2445, 18350, 490, 21056, 13, 492, 500, 380, 764, 264, 371, 3513, 294, 613, 3542, 11, 50928], "temperature": 0.0, "avg_logprob": -0.16703498840332032, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0003300123498775065}, {"id": 401, "seek": 261592, "start": 2627.2000000000003, "end": 2633.84, "text": " just XYZ coordinates mapped to color and density. So there are a bunch of ways that you could convert", "tokens": [50928, 445, 48826, 57, 21056, 33318, 281, 2017, 293, 10305, 13, 407, 456, 366, 257, 3840, 295, 2098, 300, 291, 727, 7620, 51260], "temperature": 0.0, "avg_logprob": -0.16703498840332032, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0003300123498775065}, {"id": 402, "seek": 261592, "start": 2633.84, "end": 2639.2000000000003, "text": " that. I don't know of off the shelf software that will be able to do that conversion for you,", "tokens": [51260, 300, 13, 286, 500, 380, 458, 295, 766, 264, 15222, 4722, 300, 486, 312, 1075, 281, 360, 300, 14298, 337, 291, 11, 51528], "temperature": 0.0, "avg_logprob": -0.16703498840332032, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0003300123498775065}, {"id": 403, "seek": 261592, "start": 2639.2000000000003, "end": 2645.6800000000003, "text": " it'd have to be coded up, but you can sample the scene on some grid, for example, and then you'll", "tokens": [51528, 309, 1116, 362, 281, 312, 34874, 493, 11, 457, 291, 393, 6889, 264, 4145, 322, 512, 10748, 11, 337, 1365, 11, 293, 550, 291, 603, 51852], "temperature": 0.0, "avg_logprob": -0.16703498840332032, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0003300123498775065}, {"id": 404, "seek": 264568, "start": 2645.68, "end": 2650.56, "text": " get out color and RGB. You could convert that to a local voxel representation. If you want to get", "tokens": [50364, 483, 484, 2017, 293, 31231, 13, 509, 727, 7620, 300, 281, 257, 2654, 1650, 87, 338, 10290, 13, 759, 291, 528, 281, 483, 50608], "temperature": 0.0, "avg_logprob": -0.11718520096370152, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0001970914745470509}, {"id": 405, "seek": 264568, "start": 2650.56, "end": 2658.24, "text": " a mesh, there's an algorithm called marching cubes that is able to find a mesh in the scene,", "tokens": [50608, 257, 17407, 11, 456, 311, 364, 9284, 1219, 30523, 25415, 300, 307, 1075, 281, 915, 257, 17407, 294, 264, 4145, 11, 50992], "temperature": 0.0, "avg_logprob": -0.11718520096370152, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0001970914745470509}, {"id": 406, "seek": 264568, "start": 2658.24, "end": 2661.9199999999996, "text": " and there's implementations on GitHub of marching cube for neural radiance fields", "tokens": [50992, 293, 456, 311, 4445, 763, 322, 23331, 295, 30523, 13728, 337, 18161, 2843, 6276, 7909, 51176], "temperature": 0.0, "avg_logprob": -0.11718520096370152, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0001970914745470509}, {"id": 407, "seek": 264568, "start": 2661.9199999999996, "end": 2668.96, "text": " that we haven't integrated into our particular library. So you take a little bit of glue to", "tokens": [51176, 300, 321, 2378, 380, 10919, 666, 527, 1729, 6405, 13, 407, 291, 747, 257, 707, 857, 295, 8998, 281, 51528], "temperature": 0.0, "avg_logprob": -0.11718520096370152, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0001970914745470509}, {"id": 408, "seek": 264568, "start": 2668.96, "end": 2674.96, "text": " grab marching cubes and then plug it in. So what do you all use to turn the neural net into these", "tokens": [51528, 4444, 30523, 25415, 293, 550, 5452, 309, 294, 13, 407, 437, 360, 291, 439, 764, 281, 1261, 264, 18161, 2533, 666, 613, 51828], "temperature": 0.0, "avg_logprob": -0.11718520096370152, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.0001970914745470509}, {"id": 409, "seek": 267496, "start": 2674.96, "end": 2681.52, "text": " graphics? Sorry, could you repeat the question? What do you use to turn the neural net into the", "tokens": [50364, 11837, 30, 4919, 11, 727, 291, 7149, 264, 1168, 30, 708, 360, 291, 764, 281, 1261, 264, 18161, 2533, 666, 264, 50692], "temperature": 0.0, "avg_logprob": -0.10534925784094859, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.00039196701254695654}, {"id": 410, "seek": 267496, "start": 2681.52, "end": 2685.84, "text": " graphics that we see here? Oh yeah, so that's done by rendering. So you can render the neural", "tokens": [50692, 11837, 300, 321, 536, 510, 30, 876, 1338, 11, 370, 300, 311, 1096, 538, 22407, 13, 407, 291, 393, 15529, 264, 18161, 50908], "temperature": 0.0, "avg_logprob": -0.10534925784094859, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.00039196701254695654}, {"id": 411, "seek": 267496, "start": 2685.84, "end": 2690.88, "text": " radiance field from any perspective in the code, but that just renders out a 2D image. It doesn't", "tokens": [50908, 2843, 6276, 2519, 490, 604, 4585, 294, 264, 3089, 11, 457, 300, 445, 6125, 433, 484, 257, 568, 35, 3256, 13, 467, 1177, 380, 51160], "temperature": 0.0, "avg_logprob": -0.10534925784094859, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.00039196701254695654}, {"id": 412, "seek": 267496, "start": 2690.88, "end": 2696.0, "text": " give you, you know, like a mesh, versus the game engine will have its own rendering algorithm based", "tokens": [51160, 976, 291, 11, 291, 458, 11, 411, 257, 17407, 11, 5717, 264, 1216, 2848, 486, 362, 1080, 1065, 22407, 9284, 2361, 51416], "temperature": 0.0, "avg_logprob": -0.10534925784094859, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.00039196701254695654}, {"id": 413, "seek": 267496, "start": 2696.0, "end": 2702.56, "text": " on rasterization or ray tracing, given the underlying geometry and texture map, which might", "tokens": [51416, 322, 367, 1727, 2144, 420, 18592, 25262, 11, 2212, 264, 14217, 18426, 293, 8091, 4471, 11, 597, 1062, 51744], "temperature": 0.0, "avg_logprob": -0.10534925784094859, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.00039196701254695654}, {"id": 414, "seek": 270256, "start": 2702.56, "end": 2707.2, "text": " be real time. So the rendering here is not real time. You have to go evaluate the neural network", "tokens": [50364, 312, 957, 565, 13, 407, 264, 22407, 510, 307, 406, 957, 565, 13, 509, 362, 281, 352, 13059, 264, 18161, 3209, 50596], "temperature": 0.0, "avg_logprob": -0.09691181778907776, "compression_ratio": 1.5534883720930233, "no_speech_prob": 0.0006069891387596726}, {"id": 415, "seek": 270256, "start": 2707.2, "end": 2711.2, "text": " at a bunch of different coordinates and accumulate its outputs into an image.", "tokens": [50596, 412, 257, 3840, 295, 819, 21056, 293, 33384, 1080, 23930, 666, 364, 3256, 13, 50796], "temperature": 0.0, "avg_logprob": -0.09691181778907776, "compression_ratio": 1.5534883720930233, "no_speech_prob": 0.0006069891387596726}, {"id": 416, "seek": 270256, "start": 2713.2, "end": 2719.36, "text": " So that's implemented. If you want videos, we can do that, but you'll have to DIY the conversion.", "tokens": [50896, 407, 300, 311, 12270, 13, 759, 291, 528, 2145, 11, 321, 393, 360, 300, 11, 457, 291, 603, 362, 281, 22194, 264, 14298, 13, 51204], "temperature": 0.0, "avg_logprob": -0.09691181778907776, "compression_ratio": 1.5534883720930233, "no_speech_prob": 0.0006069891387596726}, {"id": 417, "seek": 270256, "start": 2724.72, "end": 2727.84, "text": " Ellie had a question on strategies to reduce rendering costs.", "tokens": [51472, 27151, 632, 257, 1168, 322, 9029, 281, 5407, 22407, 5497, 13, 51628], "temperature": 0.0, "avg_logprob": -0.09691181778907776, "compression_ratio": 1.5534883720930233, "no_speech_prob": 0.0006069891387596726}, {"id": 418, "seek": 272784, "start": 2728.48, "end": 2735.28, "text": " So you can render images at low resolution. And in the Colab notebook, the rendering is done at", "tokens": [50396, 407, 291, 393, 15529, 5267, 412, 2295, 8669, 13, 400, 294, 264, 4004, 455, 21060, 11, 264, 22407, 307, 1096, 412, 50736], "temperature": 0.0, "avg_logprob": -0.16943808807723823, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0004172602202743292}, {"id": 419, "seek": 272784, "start": 2735.28, "end": 2740.0, "text": " very low resolution. So experiments, you render out 168 by 168 images or higher.", "tokens": [50736, 588, 2295, 8669, 13, 407, 12050, 11, 291, 15529, 484, 3165, 23, 538, 3165, 23, 5267, 420, 2946, 13, 50972], "temperature": 0.0, "avg_logprob": -0.16943808807723823, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0004172602202743292}, {"id": 420, "seek": 272784, "start": 2741.52, "end": 2746.7200000000003, "text": " But Colab only gives you a single low memory GPU. So we render out 88 by 88 images.", "tokens": [51048, 583, 4004, 455, 787, 2709, 291, 257, 2167, 2295, 4675, 18407, 13, 407, 321, 15529, 484, 24587, 538, 24587, 5267, 13, 51308], "temperature": 0.0, "avg_logprob": -0.16943808807723823, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0004172602202743292}, {"id": 421, "seek": 272784, "start": 2747.6800000000003, "end": 2753.44, "text": " And that really significantly speeds up the process. So rendering takes maybe 300 milliseconds.", "tokens": [51356, 400, 300, 534, 10591, 16411, 493, 264, 1399, 13, 407, 22407, 2516, 1310, 6641, 34184, 13, 51644], "temperature": 0.0, "avg_logprob": -0.16943808807723823, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0004172602202743292}, {"id": 422, "seek": 275344, "start": 2754.4, "end": 2757.2000000000003, "text": " So you have to do about three iterations per second.", "tokens": [50412, 407, 291, 362, 281, 360, 466, 1045, 36540, 680, 1150, 13, 50552], "temperature": 0.0, "avg_logprob": -0.3973278387998923, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.0007095197797752917}, {"id": 423, "seek": 275344, "start": 2762.08, "end": 2775.04, "text": " If you're using alpha transparent, okay. So Ben is asking, how do we handle with transparent objects?", "tokens": [50796, 759, 291, 434, 1228, 8961, 12737, 11, 1392, 13, 407, 3964, 307, 3365, 11, 577, 360, 321, 4813, 365, 12737, 6565, 30, 51444], "temperature": 0.0, "avg_logprob": -0.3973278387998923, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.0007095197797752917}, {"id": 424, "seek": 277504, "start": 2775.7599999999998, "end": 2783.68, "text": " So the neural radiance field, the volumetric representation is really amenable to transparent", "tokens": [50400, 407, 264, 18161, 2843, 6276, 2519, 11, 264, 1996, 449, 17475, 10290, 307, 534, 18497, 712, 281, 12737, 50796], "temperature": 0.0, "avg_logprob": -0.1851857982269705, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.0009695470216684043}, {"id": 425, "seek": 277504, "start": 2783.68, "end": 2791.36, "text": " objects because the density is this continuous value and we can observe objects. So accumulate", "tokens": [50796, 6565, 570, 264, 10305, 307, 341, 10957, 2158, 293, 321, 393, 11441, 6565, 13, 407, 33384, 51180], "temperature": 0.0, "avg_logprob": -0.1851857982269705, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.0009695470216684043}, {"id": 426, "seek": 277504, "start": 2791.36, "end": 2796.88, "text": " color from objects behind transparent objects. In optimization, you might decrease the", "tokens": [51180, 2017, 490, 6565, 2261, 12737, 6565, 13, 682, 19618, 11, 291, 1062, 11514, 264, 51456], "temperature": 0.0, "avg_logprob": -0.1851857982269705, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.0009695470216684043}, {"id": 427, "seek": 277504, "start": 2796.88, "end": 2801.2799999999997, "text": " density of some object that should be transparent, like stay in glass windows.", "tokens": [51456, 10305, 295, 512, 2657, 300, 820, 312, 12737, 11, 411, 1754, 294, 4276, 9309, 13, 51676], "temperature": 0.0, "avg_logprob": -0.1851857982269705, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.0009695470216684043}, {"id": 428, "seek": 280128, "start": 2802.2400000000002, "end": 2808.6400000000003, "text": " And the background is composited at the end. So any ray, if there is some accumulated,", "tokens": [50412, 400, 264, 3678, 307, 10199, 1226, 412, 264, 917, 13, 407, 604, 18592, 11, 498, 456, 307, 512, 31346, 11, 50732], "temperature": 0.0, "avg_logprob": -0.1433970034122467, "compression_ratio": 1.8237885462555066, "no_speech_prob": 0.0004582109395414591}, {"id": 429, "seek": 280128, "start": 2809.44, "end": 2815.84, "text": " if the transmittance is not zero along the ray accumulated throughout the scene,", "tokens": [50772, 498, 264, 7715, 593, 719, 307, 406, 4018, 2051, 264, 18592, 31346, 3710, 264, 4145, 11, 51092], "temperature": 0.0, "avg_logprob": -0.1433970034122467, "compression_ratio": 1.8237885462555066, "no_speech_prob": 0.0004582109395414591}, {"id": 430, "seek": 280128, "start": 2816.4, "end": 2818.7200000000003, "text": " then there'll be some contribution from the background image.", "tokens": [51120, 550, 456, 603, 312, 512, 13150, 490, 264, 3678, 3256, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1433970034122467, "compression_ratio": 1.8237885462555066, "no_speech_prob": 0.0004582109395414591}, {"id": 431, "seek": 280128, "start": 2820.48, "end": 2825.0400000000004, "text": " So the reason that we've kind of encouraged coherent objects is that if the object is not", "tokens": [51324, 407, 264, 1778, 300, 321, 600, 733, 295, 14658, 36239, 6565, 307, 300, 498, 264, 2657, 307, 406, 51552], "temperature": 0.0, "avg_logprob": -0.1433970034122467, "compression_ratio": 1.8237885462555066, "no_speech_prob": 0.0004582109395414591}, {"id": 432, "seek": 280128, "start": 2825.0400000000004, "end": 2829.52, "text": " coherent, then the background will leak through the translucent objects. Oh, I see what you're", "tokens": [51552, 36239, 11, 550, 264, 3678, 486, 17143, 807, 264, 48236, 6565, 13, 876, 11, 286, 536, 437, 291, 434, 51776], "temperature": 0.0, "avg_logprob": -0.1433970034122467, "compression_ratio": 1.8237885462555066, "no_speech_prob": 0.0004582109395414591}, {"id": 433, "seek": 282952, "start": 2829.52, "end": 2834.56, "text": " saying. Yeah, if you want stay in glass windows. So I mean, you would have to, the scene would", "tokens": [50364, 1566, 13, 865, 11, 498, 291, 528, 1754, 294, 4276, 9309, 13, 407, 286, 914, 11, 291, 576, 362, 281, 11, 264, 4145, 576, 50616], "temperature": 0.0, "avg_logprob": -0.1983542067281316, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0013040212215855718}, {"id": 434, "seek": 282952, "start": 2834.56, "end": 2838.96, "text": " probably optimize so that the transparent object is blocked from behind by something.", "tokens": [50616, 1391, 19719, 370, 300, 264, 12737, 2657, 307, 15470, 490, 2261, 538, 746, 13, 50836], "temperature": 0.0, "avg_logprob": -0.1983542067281316, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0013040212215855718}, {"id": 435, "seek": 282952, "start": 2848.24, "end": 2854.16, "text": " Yeah, the next steps, I think they're exciting lots of next steps, because this is an initial work", "tokens": [51300, 865, 11, 264, 958, 4439, 11, 286, 519, 436, 434, 4670, 3195, 295, 958, 4439, 11, 570, 341, 307, 364, 5883, 589, 51596], "temperature": 0.0, "avg_logprob": -0.1983542067281316, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0013040212215855718}, {"id": 436, "seek": 282952, "start": 2854.16, "end": 2859.36, "text": " and there's things like speeding up the optimization. It's been a lot of recent work and", "tokens": [51596, 293, 456, 311, 721, 411, 35593, 493, 264, 19618, 13, 467, 311, 668, 257, 688, 295, 5162, 589, 293, 51856], "temperature": 0.0, "avg_logprob": -0.1983542067281316, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0013040212215855718}, {"id": 437, "seek": 285936, "start": 2859.36, "end": 2862.8, "text": " speeding up neural radius field training for images. And I think a lot of that can be plugged in.", "tokens": [50364, 35593, 493, 18161, 15845, 2519, 3097, 337, 5267, 13, 400, 286, 519, 257, 688, 295, 300, 393, 312, 25679, 294, 13, 50536], "temperature": 0.0, "avg_logprob": -0.149641930429559, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0003099787572864443}, {"id": 438, "seek": 285936, "start": 2864.56, "end": 2869.2000000000003, "text": " And how do you synthesize the formable objects? How do you bring a human in the loop so they", "tokens": [50624, 400, 577, 360, 291, 26617, 1125, 264, 1254, 712, 6565, 30, 1012, 360, 291, 1565, 257, 1952, 294, 264, 6367, 370, 436, 50856], "temperature": 0.0, "avg_logprob": -0.149641930429559, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0003099787572864443}, {"id": 439, "seek": 285936, "start": 2869.2000000000003, "end": 2876.1600000000003, "text": " can provide feedback partway through training? All kinds of stuff to tackle in making this more", "tokens": [50856, 393, 2893, 5824, 644, 676, 807, 3097, 30, 1057, 3685, 295, 1507, 281, 14896, 294, 1455, 341, 544, 51204], "temperature": 0.0, "avg_logprob": -0.149641930429559, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0003099787572864443}, {"id": 440, "seek": 285936, "start": 2876.1600000000003, "end": 2887.28, "text": " of a practical system for 3D artists. And would you like me to share the collab? I guess we're at", "tokens": [51204, 295, 257, 8496, 1185, 337, 805, 35, 6910, 13, 400, 576, 291, 411, 385, 281, 2073, 264, 44228, 30, 286, 2041, 321, 434, 412, 51760], "temperature": 0.0, "avg_logprob": -0.149641930429559, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0003099787572864443}, {"id": 441, "seek": 288728, "start": 2887.28, "end": 2891.6800000000003, "text": " time. Yeah, please go ahead. That would be great. Thank you so much.", "tokens": [50364, 565, 13, 865, 11, 1767, 352, 2286, 13, 663, 576, 312, 869, 13, 1044, 291, 370, 709, 13, 50584], "temperature": 0.0, "avg_logprob": -0.132693221171697, "compression_ratio": 1.2985074626865671, "no_speech_prob": 0.0008163191960193217}, {"id": 442, "seek": 288728, "start": 2899.28, "end": 2903.0400000000004, "text": " So this is the collab notebook. You can find it from the project website.", "tokens": [50964, 407, 341, 307, 264, 44228, 21060, 13, 509, 393, 915, 309, 490, 264, 1716, 3144, 13, 51152], "temperature": 0.0, "avg_logprob": -0.132693221171697, "compression_ratio": 1.2985074626865671, "no_speech_prob": 0.0008163191960193217}, {"id": 443, "seek": 288728, "start": 2905.0400000000004, "end": 2908.2400000000002, "text": " It is a compact implementation.", "tokens": [51252, 467, 307, 257, 14679, 11420, 13, 51412], "temperature": 0.0, "avg_logprob": -0.132693221171697, "compression_ratio": 1.2985074626865671, "no_speech_prob": 0.0008163191960193217}, {"id": 444, "seek": 290824, "start": 2908.3999999999996, "end": 2919.68, "text": " The system will run faster on GPU than on TPU in the collab notebook. But for all of our experiments,", "tokens": [50372, 440, 1185, 486, 1190, 4663, 322, 18407, 813, 322, 314, 8115, 294, 264, 44228, 21060, 13, 583, 337, 439, 295, 527, 12050, 11, 50936], "temperature": 0.0, "avg_logprob": -0.11497817140944461, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.003763483837246895}, {"id": 445, "seek": 290824, "start": 2919.68, "end": 2926.3199999999997, "text": " we use TPU. Some helpers are imported from our library. So if you want to hack on some", "tokens": [50936, 321, 764, 314, 8115, 13, 2188, 854, 433, 366, 25524, 490, 527, 6405, 13, 407, 498, 291, 528, 281, 10339, 322, 512, 51268], "temperature": 0.0, "avg_logprob": -0.11497817140944461, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.003763483837246895}, {"id": 446, "seek": 290824, "start": 2926.3199999999997, "end": 2931.8399999999997, "text": " of the low level primitives, you can fork our library or kind of copy those helpers into the", "tokens": [51268, 295, 264, 2295, 1496, 2886, 38970, 11, 291, 393, 17716, 527, 6405, 420, 733, 295, 5055, 729, 854, 433, 666, 264, 51544], "temperature": 0.0, "avg_logprob": -0.11497817140944461, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.003763483837246895}, {"id": 447, "seek": 290824, "start": 2931.8399999999997, "end": 2936.64, "text": " notebook. But the main way you'll interface with this collab notebook is by adjusting the quality", "tokens": [51544, 21060, 13, 583, 264, 2135, 636, 291, 603, 9226, 365, 341, 44228, 21060, 307, 538, 23559, 264, 3125, 51784], "temperature": 0.0, "avg_logprob": -0.11497817140944461, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.003763483837246895}, {"id": 448, "seek": 293664, "start": 2936.64, "end": 2946.72, "text": " settings here. So in particular, edit the query. Here I've filled in a high quality 3D", "tokens": [50364, 6257, 510, 13, 407, 294, 1729, 11, 8129, 264, 14581, 13, 1692, 286, 600, 6412, 294, 257, 1090, 3125, 805, 35, 50868], "temperature": 0.0, "avg_logprob": -0.1398054858048757, "compression_ratio": 1.464, "no_speech_prob": 0.00043045301572419703}, {"id": 449, "seek": 293664, "start": 2946.72, "end": 2953.92, "text": " render of Jenga Tower. And you can select the clip checkpoint you want to use. Clip bit B16", "tokens": [50868, 15529, 295, 508, 31494, 17877, 13, 400, 291, 393, 3048, 264, 7353, 42269, 291, 528, 281, 764, 13, 2033, 647, 857, 363, 6866, 51228], "temperature": 0.0, "avg_logprob": -0.1398054858048757, "compression_ratio": 1.464, "no_speech_prob": 0.00043045301572419703}, {"id": 450, "seek": 293664, "start": 2953.92, "end": 2958.7999999999997, "text": " is used in most of our experiments. There's also an internal Google model that's not available here.", "tokens": [51228, 307, 1143, 294, 881, 295, 527, 12050, 13, 821, 311, 611, 364, 6920, 3329, 2316, 300, 311, 406, 2435, 510, 13, 51472], "temperature": 0.0, "avg_logprob": -0.1398054858048757, "compression_ratio": 1.464, "no_speech_prob": 0.00043045301572419703}, {"id": 451, "seek": 293664, "start": 2959.92, "end": 2964.8799999999997, "text": " But you can scale down if you're running out of memory to either the B32 or ResNet 50.", "tokens": [51528, 583, 291, 393, 4373, 760, 498, 291, 434, 2614, 484, 295, 4675, 281, 2139, 264, 363, 11440, 420, 5015, 31890, 2625, 13, 51776], "temperature": 0.0, "avg_logprob": -0.1398054858048757, "compression_ratio": 1.464, "no_speech_prob": 0.00043045301572419703}, {"id": 452, "seek": 296664, "start": 2966.8799999999997, "end": 2971.68, "text": " Choose the number optimization iterations. I think at least 1000 is necessary.", "tokens": [50376, 21661, 264, 1230, 19618, 36540, 13, 286, 519, 412, 1935, 9714, 307, 4818, 13, 50616], "temperature": 0.0, "avg_logprob": -0.15472499302455356, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.000261145323747769}, {"id": 453, "seek": 296664, "start": 2973.52, "end": 2977.6, "text": " But more will add more detail. Consider the rendering width and then this is the number", "tokens": [50708, 583, 544, 486, 909, 544, 2607, 13, 17416, 264, 22407, 11402, 293, 550, 341, 307, 264, 1230, 50912], "temperature": 0.0, "avg_logprob": -0.15472499302455356, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.000261145323747769}, {"id": 454, "seek": 296664, "start": 2977.6, "end": 2987.2, "text": " of data augmentation. And then run training. So here's an example of the training run I've already", "tokens": [50912, 295, 1412, 14501, 19631, 13, 400, 550, 1190, 3097, 13, 407, 510, 311, 364, 1365, 295, 264, 3097, 1190, 286, 600, 1217, 51392], "temperature": 0.0, "avg_logprob": -0.15472499302455356, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.000261145323747769}, {"id": 455, "seek": 296664, "start": 2987.2, "end": 2992.48, "text": " run in the notebook for that prompt, a high quality 3D render of Jenga Tower. It won't exactly", "tokens": [51392, 1190, 294, 264, 21060, 337, 300, 12391, 11, 257, 1090, 3125, 805, 35, 15529, 295, 508, 31494, 17877, 13, 467, 1582, 380, 2293, 51656], "temperature": 0.0, "avg_logprob": -0.15472499302455356, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.000261145323747769}, {"id": 456, "seek": 296664, "start": 2992.48, "end": 2995.92, "text": " match the result that was shown in the slides because the version of the collab notebook could", "tokens": [51656, 2995, 264, 1874, 300, 390, 4898, 294, 264, 9788, 570, 264, 3037, 295, 264, 44228, 21060, 727, 51828], "temperature": 0.0, "avg_logprob": -0.15472499302455356, "compression_ratio": 1.6425992779783394, "no_speech_prob": 0.000261145323747769}, {"id": 457, "seek": 299592, "start": 2996.0, "end": 3002.64, "text": " scale down. But over the course of 2000 iterations of optimization, you can see the different", "tokens": [50368, 4373, 760, 13, 583, 670, 264, 1164, 295, 8132, 36540, 295, 19618, 11, 291, 393, 536, 264, 819, 50700], "temperature": 0.0, "avg_logprob": -0.12760808620047062, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.00017947590094991028}, {"id": 458, "seek": 299592, "start": 3002.64, "end": 3008.2400000000002, "text": " learning curves. This is the total loss that's being optimized. Clip's cosine similarity,", "tokens": [50700, 2539, 19490, 13, 639, 307, 264, 3217, 4470, 300, 311, 885, 26941, 13, 2033, 647, 311, 23565, 32194, 11, 50980], "temperature": 0.0, "avg_logprob": -0.12760808620047062, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.00017947590094991028}, {"id": 459, "seek": 299592, "start": 3008.2400000000002, "end": 3012.7200000000003, "text": " negative cosine similarity is improving. So this means that the renderings of the object", "tokens": [50980, 3671, 23565, 32194, 307, 11470, 13, 407, 341, 1355, 300, 264, 15529, 1109, 295, 264, 2657, 51204], "temperature": 0.0, "avg_logprob": -0.12760808620047062, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.00017947590094991028}, {"id": 460, "seek": 299592, "start": 3012.7200000000003, "end": 3016.32, "text": " are becoming more and more consistent with the given caption over time.", "tokens": [51204, 366, 5617, 544, 293, 544, 8398, 365, 264, 2212, 31974, 670, 565, 13, 51384], "temperature": 0.0, "avg_logprob": -0.12760808620047062, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.00017947590094991028}, {"id": 461, "seek": 299592, "start": 3019.04, "end": 3023.28, "text": " And the transmission is regularization here. This is showing what is the average transparency", "tokens": [51520, 400, 264, 11574, 307, 3890, 2144, 510, 13, 639, 307, 4099, 437, 307, 264, 4274, 17131, 51732], "temperature": 0.0, "avg_logprob": -0.12760808620047062, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.00017947590094991028}, {"id": 462, "seek": 302328, "start": 3023.36, "end": 3033.92, "text": " of pixels in the scene. And in this plot at the bottom, the collab notebook will ride out", "tokens": [50368, 295, 18668, 294, 264, 4145, 13, 400, 294, 341, 7542, 412, 264, 2767, 11, 264, 44228, 21060, 486, 5077, 484, 50896], "temperature": 0.0, "avg_logprob": -0.1821068272446141, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.0003005384060088545}, {"id": 463, "seek": 302328, "start": 3033.92, "end": 3044.7200000000003, "text": " renderings periodically every, I believe, 100 iterations. So at the beginning, the scene is", "tokens": [50896, 15529, 1109, 38916, 633, 11, 286, 1697, 11, 2319, 36540, 13, 407, 412, 264, 2863, 11, 264, 4145, 307, 51436], "temperature": 0.0, "avg_logprob": -0.1821068272446141, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.0003005384060088545}, {"id": 464, "seek": 302328, "start": 3044.7200000000003, "end": 3050.7200000000003, "text": " low density, essentially empty. And then over time, some content will emerge from the optimization.", "tokens": [51436, 2295, 10305, 11, 4476, 6707, 13, 400, 550, 670, 565, 11, 512, 2701, 486, 21511, 490, 264, 19618, 13, 51736], "temperature": 0.0, "avg_logprob": -0.1821068272446141, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.0003005384060088545}, {"id": 465, "seek": 305072, "start": 3051.52, "end": 3057.12, "text": " And then that's refined and sharpened over time. The camera's moving around. So the camera's being", "tokens": [50404, 400, 550, 300, 311, 26201, 293, 31570, 292, 670, 565, 13, 440, 2799, 311, 2684, 926, 13, 407, 264, 2799, 311, 885, 50684], "temperature": 0.0, "avg_logprob": -0.1369241112156918, "compression_ratio": 1.576, "no_speech_prob": 0.00016090174904093146}, {"id": 466, "seek": 305072, "start": 3057.12, "end": 3063.68, "text": " randomly sampled around the object. And that's why the scene is rendered from different perspectives.", "tokens": [50684, 16979, 3247, 15551, 926, 264, 2657, 13, 400, 300, 311, 983, 264, 4145, 307, 28748, 490, 819, 16766, 13, 51012], "temperature": 0.0, "avg_logprob": -0.1369241112156918, "compression_ratio": 1.576, "no_speech_prob": 0.00016090174904093146}, {"id": 467, "seek": 305072, "start": 3064.7999999999997, "end": 3070.64, "text": " And then finally, the collab notebook renders out a video, 48 frames. And this is the result.", "tokens": [51068, 400, 550, 2721, 11, 264, 44228, 21060, 6125, 433, 484, 257, 960, 11, 11174, 12083, 13, 400, 341, 307, 264, 1874, 13, 51360], "temperature": 0.0, "avg_logprob": -0.1369241112156918, "compression_ratio": 1.576, "no_speech_prob": 0.00016090174904093146}, {"id": 468, "seek": 305072, "start": 3072.3999999999996, "end": 3078.56, "text": " On the GPU that collab gave me here a P100, the optimization I think took about six, seven minutes.", "tokens": [51448, 1282, 264, 18407, 300, 44228, 2729, 385, 510, 257, 430, 6879, 11, 264, 19618, 286, 519, 1890, 466, 2309, 11, 3407, 2077, 13, 51756], "temperature": 0.0, "avg_logprob": -0.1369241112156918, "compression_ratio": 1.576, "no_speech_prob": 0.00016090174904093146}, {"id": 469, "seek": 308072, "start": 3081.68, "end": 3085.3599999999997, "text": " So hopefully you can get some cycles in.", "tokens": [50412, 407, 4696, 291, 393, 483, 512, 17796, 294, 13, 50596], "temperature": 0.0, "avg_logprob": -0.1332710642873505, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.00013763409515377134}, {"id": 470, "seek": 308072, "start": 3088.7999999999997, "end": 3094.16, "text": " In the run training section, it says if you run out of memory, tweak the configuration options", "tokens": [50768, 682, 264, 1190, 3097, 3541, 11, 309, 1619, 498, 291, 1190, 484, 295, 4675, 11, 29879, 264, 11694, 3956, 51036], "temperature": 0.0, "avg_logprob": -0.1332710642873505, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.00013763409515377134}, {"id": 471, "seek": 308072, "start": 3094.16, "end": 3101.04, "text": " above. What do you recommend changing? Yeah, that's a good question. So I think", "tokens": [51036, 3673, 13, 708, 360, 291, 2748, 4473, 30, 865, 11, 300, 311, 257, 665, 1168, 13, 407, 286, 519, 51380], "temperature": 0.0, "avg_logprob": -0.1332710642873505, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.00013763409515377134}, {"id": 472, "seek": 308072, "start": 3101.04, "end": 3108.8799999999997, "text": " you can change this clip at B16. I would try to clip B32. There's also on the first import", "tokens": [51380, 291, 393, 1319, 341, 7353, 412, 363, 6866, 13, 286, 576, 853, 281, 7353, 363, 11440, 13, 821, 311, 611, 322, 264, 700, 974, 51772], "temperature": 0.0, "avg_logprob": -0.1332710642873505, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.00013763409515377134}, {"id": 473, "seek": 310888, "start": 3108.88, "end": 3112.4, "text": " in NVIDIA SMI printout. And so you can look at how much memory is available.", "tokens": [50364, 294, 426, 3958, 6914, 13115, 40, 4482, 346, 13, 400, 370, 291, 393, 574, 412, 577, 709, 4675, 307, 2435, 13, 50540], "temperature": 0.0, "avg_logprob": -0.14850259042000985, "compression_ratio": 1.5597014925373134, "no_speech_prob": 0.0024717047344893217}, {"id": 474, "seek": 310888, "start": 3113.04, "end": 3121.2000000000003, "text": " Sometimes it's worth retrying multiple times to get a larger GPU. This P160", "tokens": [50572, 4803, 309, 311, 3163, 1533, 19076, 3866, 1413, 281, 483, 257, 4833, 18407, 13, 639, 430, 44158, 50980], "temperature": 0.0, "avg_logprob": -0.14850259042000985, "compression_ratio": 1.5597014925373134, "no_speech_prob": 0.0024717047344893217}, {"id": 475, "seek": 310888, "start": 3121.2000000000003, "end": 3124.96, "text": " gigabyte I think you won't get without collab premium, which is about $10 a month.", "tokens": [50980, 8741, 34529, 286, 519, 291, 1582, 380, 483, 1553, 44228, 12049, 11, 597, 307, 466, 1848, 3279, 257, 1618, 13, 51168], "temperature": 0.0, "avg_logprob": -0.14850259042000985, "compression_ratio": 1.5597014925373134, "no_speech_prob": 0.0024717047344893217}, {"id": 476, "seek": 310888, "start": 3126.56, "end": 3133.12, "text": " But you think you can get 15 gigabyte T4 GPUs for free. Sometimes the collab will give you an 11", "tokens": [51248, 583, 291, 519, 291, 393, 483, 2119, 8741, 34529, 314, 19, 18407, 82, 337, 1737, 13, 4803, 264, 44228, 486, 976, 291, 364, 2975, 51576], "temperature": 0.0, "avg_logprob": -0.14850259042000985, "compression_ratio": 1.5597014925373134, "no_speech_prob": 0.0024717047344893217}, {"id": 477, "seek": 310888, "start": 3133.12, "end": 3138.56, "text": " gigabyte GPU that might not be enough. If you can tweak the configuration parameters,", "tokens": [51576, 8741, 34529, 18407, 300, 1062, 406, 312, 1547, 13, 759, 291, 393, 29879, 264, 11694, 9834, 11, 51848], "temperature": 0.0, "avg_logprob": -0.14850259042000985, "compression_ratio": 1.5597014925373134, "no_speech_prob": 0.0024717047344893217}, {"id": 478, "seek": 313856, "start": 3138.56, "end": 3144.08, "text": " I would try reducing this number of samples. So this is the number of points along each array", "tokens": [50364, 286, 576, 853, 12245, 341, 1230, 295, 10938, 13, 407, 341, 307, 264, 1230, 295, 2793, 2051, 1184, 10225, 50640], "temperature": 0.0, "avg_logprob": -0.15038705134129787, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0003352683561388403}, {"id": 479, "seek": 313856, "start": 3144.08, "end": 3152.32, "text": " that is sampled. And that affects the batch size. So the render width, the batch size scales", "tokens": [50640, 300, 307, 3247, 15551, 13, 400, 300, 11807, 264, 15245, 2744, 13, 407, 264, 15529, 11402, 11, 264, 15245, 2744, 17408, 51052], "temperature": 0.0, "avg_logprob": -0.15038705134129787, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0003352683561388403}, {"id": 480, "seek": 313856, "start": 3152.32, "end": 3156.72, "text": " quadratically with the render width because we're rendering got square images. And then the num", "tokens": [51052, 10787, 4481, 984, 365, 264, 15529, 11402, 570, 321, 434, 22407, 658, 3732, 5267, 13, 400, 550, 264, 1031, 51272], "temperature": 0.0, "avg_logprob": -0.15038705134129787, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0003352683561388403}, {"id": 481, "seek": 313856, "start": 3156.72, "end": 3162.72, "text": " samples the batch size to the MLP scales linearly. So you could reduce this down to 32 even at the", "tokens": [51272, 10938, 264, 15245, 2744, 281, 264, 21601, 47, 17408, 43586, 13, 407, 291, 727, 5407, 341, 760, 281, 8858, 754, 412, 264, 51572], "temperature": 0.0, "avg_logprob": -0.15038705134129787, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0003352683561388403}, {"id": 482, "seek": 316272, "start": 3162.72, "end": 3169.6, "text": " lowest. B32 will use less memory than B16. So this relates to the patch size and the vision", "tokens": [50364, 12437, 13, 363, 11440, 486, 764, 1570, 4675, 813, 363, 6866, 13, 407, 341, 16155, 281, 264, 9972, 2744, 293, 264, 5201, 50708], "temperature": 0.0, "avg_logprob": -0.1149834394454956, "compression_ratio": 1.3194444444444444, "no_speech_prob": 0.0006165786180645227}, {"id": 483, "seek": 316272, "start": 3169.6, "end": 3177.2, "text": " transformer clip encoding. And then if you want to scale down even more, you can change the number", "tokens": [50708, 31782, 7353, 43430, 13, 400, 550, 498, 291, 528, 281, 4373, 760, 754, 544, 11, 291, 393, 1319, 264, 1230, 51088], "temperature": 0.0, "avg_logprob": -0.1149834394454956, "compression_ratio": 1.3194444444444444, "no_speech_prob": 0.0006165786180645227}, {"id": 484, "seek": 317720, "start": 3177.2, "end": 3189.68, "text": " of data augmentations per iteration, maybe down to two.", "tokens": [50364, 295, 1412, 29919, 763, 680, 24784, 11, 1310, 760, 281, 732, 13, 50988], "temperature": 0.0, "avg_logprob": -0.20397027333577475, "compression_ratio": 1.1030927835051547, "no_speech_prob": 0.006096694152802229}, {"id": 485, "seek": 317720, "start": 3198.0, "end": 3200.72, "text": " Oh, Ben says that you can't retry for a better GPU.", "tokens": [51404, 876, 11, 3964, 1619, 300, 291, 393, 380, 1533, 627, 337, 257, 1101, 18407, 13, 51540], "temperature": 0.0, "avg_logprob": -0.20397027333577475, "compression_ratio": 1.1030927835051547, "no_speech_prob": 0.006096694152802229}, {"id": 486, "seek": 320072, "start": 3201.6, "end": 3209.12, "text": " That's unfortunate. I mean, I don't know whether MIT has like a shared GPU cloud, but you can", "tokens": [50408, 663, 311, 17843, 13, 286, 914, 11, 286, 500, 380, 458, 1968, 13100, 575, 411, 257, 5507, 18407, 4588, 11, 457, 291, 393, 50784], "temperature": 0.0, "avg_logprob": -0.26683299724872295, "compression_ratio": 1.3222222222222222, "no_speech_prob": 0.008845279924571514}, {"id": 487, "seek": 320072, "start": 3209.12, "end": 3215.4399999999996, "text": " also just download this IPIND and run it on your like Jupyter notebooks, post it on some MIT compute.", "tokens": [50784, 611, 445, 5484, 341, 8671, 1464, 35, 293, 1190, 309, 322, 428, 411, 22125, 88, 391, 43782, 11, 2183, 309, 322, 512, 13100, 14722, 13, 51100], "temperature": 0.0, "avg_logprob": -0.26683299724872295, "compression_ratio": 1.3222222222222222, "no_speech_prob": 0.008845279924571514}, {"id": 488, "seek": 320072, "start": 3217.04, "end": 3223.2, "text": " And it will paralyze across multiple GPUs.", "tokens": [51180, 400, 309, 486, 32645, 1381, 2108, 3866, 18407, 82, 13, 51488], "temperature": 0.0, "avg_logprob": -0.26683299724872295, "compression_ratio": 1.3222222222222222, "no_speech_prob": 0.008845279924571514}, {"id": 489, "seek": 323072, "start": 3230.8799999999997, "end": 3236.64, "text": " Cool. And have you taken any more questions that you have?", "tokens": [50372, 8561, 13, 400, 362, 291, 2726, 604, 544, 1651, 300, 291, 362, 30, 50660], "temperature": 0.0, "avg_logprob": -0.30010113186306425, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.0015716900816187263}, {"id": 490, "seek": 323072, "start": 3236.64, "end": 3243.8399999999997, "text": " Excellent. Thank you so much. Maybe at this point I'll stop recording and if students have more", "tokens": [50660, 16723, 13, 1044, 291, 370, 709, 13, 2704, 412, 341, 935, 286, 603, 1590, 6613, 293, 498, 1731, 362, 544, 51020], "temperature": 0.0, "avg_logprob": -0.30010113186306425, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.0015716900816187263}, {"id": 491, "seek": 323072, "start": 3243.8399999999997, "end": 3245.2, "text": " questions, we can...", "tokens": [51020, 1651, 11, 321, 393, 485, 51088], "temperature": 0.0, "avg_logprob": -0.30010113186306425, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.0015716900816187263}], "language": "en"}