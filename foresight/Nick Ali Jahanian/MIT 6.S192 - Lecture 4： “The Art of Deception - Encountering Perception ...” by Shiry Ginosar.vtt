WEBVTT

00:00.000 --> 00:08.360
Yeah, and we have our another specialist speaker today.

00:08.360 --> 00:16.320
This is such a pleasure for me to have Shiri Ghinosar.

00:16.320 --> 00:22.920
She's a computing innovation postdoctoral fellow at UC Berkeley.

00:22.920 --> 00:25.520
And she has done so many interesting work

00:25.520 --> 00:34.120
and very creative these models of current ARs.

00:34.120 --> 00:37.480
Many interesting projects that she will hopefully

00:37.480 --> 00:39.800
share a gist of that with us.

00:39.800 --> 00:45.520
I can tell you about understanding

00:45.520 --> 00:48.920
the evolution of her images that was very striking to me,

00:48.920 --> 00:52.200
as well as the dance project that she did,

00:52.200 --> 00:54.520
and other interesting things that she

00:54.520 --> 00:59.720
tries to go beyond the only pixels.

00:59.720 --> 01:06.000
So Shiri, go ahead and please start your talk.

01:06.000 --> 01:07.840
Oh, thank you so much.

01:07.840 --> 01:10.880
So hi, I'm Shiri.

01:10.880 --> 01:15.880
And I'd like to talk to you today about the art of deception,

01:15.880 --> 01:20.880
or basically using perception as a creative material.

01:20.880 --> 01:24.680
And I have a lot of material, but I want this.

01:24.680 --> 01:27.600
I know it's kind of strange to do this over Zoom.

01:27.600 --> 01:32.280
People are kind of shy to budge in.

01:32.280 --> 01:35.160
But it can really be a conversation.

01:35.160 --> 01:38.240
We can take it this anywhere that you want.

01:38.240 --> 01:40.320
It could be kind of like a fireside chat.

01:40.320 --> 01:42.960
It could be, you know, we can make it this into different things.

01:42.960 --> 01:45.680
But so I want to encourage you, if you have questions,

01:45.680 --> 01:49.840
you could just start talking, because I don't have the chat on,

01:49.840 --> 01:51.360
and I can't man all these things.

01:51.360 --> 01:55.360
But just feel free to just stop me at any point.

01:55.360 --> 01:58.440
And I want to say that, because I know that it's, you know,

01:58.440 --> 02:02.080
this whole remote thing is kind of, you know, strange.

02:02.080 --> 02:03.800
So OK, so let's start talking.

02:03.800 --> 02:05.800
And I want to get you into the mood.

02:05.800 --> 02:09.200
So I want us to look at a video together to start with.

02:09.200 --> 02:15.360
And this is a really nice music video by Michel Gondry

02:15.360 --> 02:17.720
that he made for the Chemical Brothers.

02:17.720 --> 02:19.880
And I want you to really notice what is going on.

02:19.880 --> 02:24.160
I want you to listen and I want you to look at what is going

02:24.160 --> 02:26.040
at what is going on in this video.

03:47.720 --> 04:01.040
OK, so if you zoned out a little bit in the beginning there,

04:01.040 --> 04:02.720
because, you know, you look at this thing

04:02.720 --> 04:05.560
and you think to yourself, OK, I'm looking out the window,

04:05.560 --> 04:08.800
I'm a train, I've been there, it's kind of nice and relaxing,

04:08.800 --> 04:10.280
but it's also a little bit mind-numbing.

04:10.280 --> 04:13.320
So you kind of zone out and you're like, OK.

04:13.320 --> 04:16.440
But then suddenly it hits you that there's a surprise here.

04:16.480 --> 04:20.720
So actually all the visuals are, yes.

04:20.720 --> 04:25.480
I think that did you want the student to tell you what they think

04:25.480 --> 04:29.400
or you just want to tell them?

04:29.400 --> 04:33.320
They can tell me what I, I can't really see you guys.

04:33.320 --> 04:34.920
I can only see you.

04:34.920 --> 04:37.640
OK, so if you want to say something, just like say it.

04:37.640 --> 04:40.960
Yeah, if you please, if your students want to say something,

04:40.960 --> 04:42.680
just unmute and say it.

04:42.680 --> 04:46.200
I can also actually, before you do that, now that we've stopped,

04:46.200 --> 04:50.240
I want to ask the audio, is it reasonable?

04:50.240 --> 04:50.960
Is it too loud?

04:50.960 --> 04:53.880
Is it too low of the of the videos?

04:53.880 --> 04:55.440
It's a good time to fix this

04:55.440 --> 04:58.160
because we're going to have a lot of audio and videos.

04:58.160 --> 05:04.440
I think it was reasonable, maybe a little, maybe a little less like.

05:04.440 --> 05:06.400
But it was good.

05:09.000 --> 05:11.120
So OK, we made a little bit less loud.

05:11.120 --> 05:13.000
If it's not going forward, complain.

05:13.000 --> 05:14.760
Yes, excellent.

05:14.760 --> 05:20.080
So if anyone wants to say what they think or should I read it?

05:20.080 --> 05:26.280
So Ben says, it's repeating scene in sync to the music sections,

05:26.280 --> 05:31.240
moving on to the next loop of the scene

05:31.240 --> 05:34.440
as the track moves to the next section.

05:34.440 --> 05:39.200
Simon says, the camera feels like it is bouncing in rhythm.

05:41.320 --> 05:43.560
That is, that is, that is wonderful.

05:43.640 --> 05:45.160
Those are wonderful observations.

05:45.160 --> 05:49.080
But like, guys, just turn on your microphones and just talk.

05:49.080 --> 05:51.480
OK, don't make Ali read all of your all of your things.

05:52.040 --> 05:55.360
So it will be more just more interactive, you know.

05:57.120 --> 05:59.720
OK, so yes, that's exactly what's going on.

05:59.920 --> 06:03.880
Everything is being repeated and is synced to the music and sync to the beat.

06:04.160 --> 06:06.040
And Michel Goudre is very good at this.

06:06.040 --> 06:09.040
He did this a couple of times and amazingly, he did this by hand.

06:09.040 --> 06:12.480
So there's a very interesting YouTube video.

06:12.520 --> 06:16.440
It's kind of like a documentary about the making of this video.

06:16.440 --> 06:21.120
And you see how they charted out the entire score of the music

06:21.360 --> 06:23.840
and planned out exactly what they're going to show.

06:25.840 --> 06:29.360
Individuals and literally, literally compose this by hand.

06:29.360 --> 06:31.200
There's no, there's no AI here or anything like that.

06:31.800 --> 06:36.840
So the interesting thing is that this kind of combination

06:36.840 --> 06:41.440
between your different senses plays a trick on you and gives you this surprise,

06:41.440 --> 06:43.000
which makes it interesting.

06:43.000 --> 06:46.360
But if you didn't notice what was going on, you would think, oh, yeah, you know,

06:46.360 --> 06:48.280
it looks like it's real, but it's not.

06:48.280 --> 06:49.560
There's nothing real about this.

06:49.560 --> 06:51.160
It's completely composed.

06:52.160 --> 06:55.840
OK, so so let's take a step back and talk a little about

06:55.840 --> 06:59.800
but put ourselves in context and look at the history of what people did

06:59.800 --> 07:04.040
in order to depict the world from the very beginning of art.

07:04.960 --> 07:08.840
The main goal of art was to, you know,

07:09.280 --> 07:12.120
capture the world realistically.

07:12.120 --> 07:15.960
So they started with with cave drawings of prehistoric people

07:15.960 --> 07:20.080
who saw these big cows with the big big horns

07:20.080 --> 07:24.880
and wanted to wanted to capture them with coal on the on the

07:25.920 --> 07:28.600
walls of their caves.

07:28.600 --> 07:32.320
And going forward, many, many years, people got more sophisticated.

07:33.040 --> 07:38.080
This is a very nice mosaic from the Byzantine time.

07:38.080 --> 07:42.760
And there's already a lot of dress here and different people and a lot of details.

07:43.880 --> 07:47.800
But at this time, all of the depiction of people was very, very flat.

07:47.920 --> 07:52.360
So there's almost no depth in the image and everybody is kind of frontal

07:52.600 --> 07:57.320
and has a very just nondescript

07:58.680 --> 08:04.160
facial emotions that everybody's kind of like severe going forward.

08:04.280 --> 08:06.480
You know, this is the beginning of the Renaissance.

08:06.480 --> 08:07.720
So this is Giotto.

08:07.720 --> 08:11.520
And this is one of the first times that depth appears in an image.

08:11.840 --> 08:14.480
And the way he does it is kind of it's a nice trick.

08:14.480 --> 08:18.680
He puts this person in green in front of Christ.

08:19.040 --> 08:23.000
And it kind of draws your attention inwards into the image.

08:23.240 --> 08:25.960
And you can notice, you know, people are not frontal here anymore.

08:25.960 --> 08:28.920
This is kind of diverging

08:31.040 --> 08:33.520
style from from the Byzantine style.

08:33.800 --> 08:36.080
And you can see motion in their faces.

08:36.080 --> 08:38.440
So it's becoming more and more realistic.

08:39.680 --> 08:42.040
Things get even better in the Renaissance.

08:42.040 --> 08:45.440
Here we already have linear perspective.

08:45.440 --> 08:49.160
We have the little people in the back, you know, are smaller.

08:49.160 --> 08:51.480
And we have people in the front, which are bigger.

08:51.480 --> 08:55.920
And it kind of makes you feel like everything is going into the image.

08:55.920 --> 08:58.160
And you can kind of walk into the scene.

08:58.960 --> 09:03.640
And, of course, going a little bit forward, you know, we almost achieved

09:04.560 --> 09:06.400
perfection in painting.

09:06.400 --> 09:08.320
So this is Van Eyck.

09:08.760 --> 09:14.360
And this is a beautiful painting where he has here geometric

09:14.600 --> 09:16.480
orthogonal perspective.

09:16.480 --> 09:20.600
And there's, you know, you can really walk into this image.

09:20.800 --> 09:25.240
There's a lot of detail if you blow up, for example, the chandelier.

09:25.240 --> 09:28.280
You can see how the light is bouncing on the metal.

09:28.640 --> 09:34.640
The little flames of the candles are very, very nicely painted.

09:34.640 --> 09:37.800
Everything is very realistic and even better.

09:37.800 --> 09:42.280
He puts a mirror behind those those two people who are getting married here.

09:42.600 --> 09:45.840
And you can get even a double depth in the image.

09:45.840 --> 09:49.520
So you can walk into the image and you can even see the backs of the people

09:49.520 --> 09:52.600
and the people who are looking at the people who are getting married.

09:53.760 --> 09:55.560
So this is is really wonderful.

09:55.760 --> 10:01.440
And even you can see the the little beads, the glass, the glass beads

10:01.440 --> 10:02.760
that are hanging up.

10:02.760 --> 10:05.320
It just is is almost perfect.

10:07.960 --> 10:10.520
Of course, all of this this perfection

10:11.840 --> 10:13.960
basically ended with the invention of the camera

10:14.280 --> 10:17.480
because then you could just walk out or look out of your window

10:17.480 --> 10:19.520
and take a snapshot and you're done, right?

10:19.520 --> 10:21.040
You know, this is like, there you go.

10:21.040 --> 10:24.000
This is the world, the world, you know, it is this is it.

10:24.400 --> 10:27.400
Anything you want, you could take a picture of and it kind of

10:28.800 --> 10:32.520
made this this break in the art world because suddenly,

10:33.040 --> 10:36.400
you know, the people who would do your your portrait for a lot of money,

10:36.400 --> 10:38.760
we're out of we're out of we're out of work in a way.

10:39.680 --> 10:42.280
Of course, this raises different questions.

10:42.280 --> 10:47.200
So once you have an image that you can take and the pixels are real,

10:47.400 --> 10:51.640
then the question can become about whether whether is this a real picture?

10:51.640 --> 10:55.040
Is this fake? You know, is this kiss that happened in Paris?

10:55.040 --> 10:57.560
Was it was it really did it really happen for real?

10:57.560 --> 11:00.080
Or was it a stage? This one was staged, by the way.

11:01.640 --> 11:05.080
Are aliens actually descending on New York or is this a movie?

11:05.080 --> 11:07.120
You know, is this a conspiracy theory?

11:07.120 --> 11:09.240
Are the people actually land on the moon?

11:09.240 --> 11:13.000
Or, you know, can I believe the pixels of the camera or not?

11:15.360 --> 11:20.360
And interestingly and interestingly for us,

11:21.240 --> 11:25.880
artists didn't actually give up after the invention of the camera.

11:26.440 --> 11:28.760
They actually became more sophisticated.

11:28.760 --> 11:32.560
So here there is a nice example from Monet.

11:32.560 --> 11:36.640
It's an impressionist painting and you can see a parade in Paris.

11:36.640 --> 11:40.280
There's a lot of people going and walking in the street.

11:40.600 --> 11:44.640
And, you know, let's try let's try to get you talking a little bit.

11:44.640 --> 11:48.320
So like, what do you notice in this image in this painting?

11:51.360 --> 11:54.520
Anybody? What's interesting about it?

11:54.520 --> 11:56.520
There's a lot of flags.

11:56.520 --> 11:58.520
There's a lot of flags, yes.

11:58.520 --> 12:01.520
And what's happening to the flags?

12:01.520 --> 12:03.040
Oh, they're waving.

12:03.040 --> 12:07.120
So it's like depicting motion and they're kind of blown in the wind.

12:08.200 --> 12:09.880
Why do you think they're blowing in the wind?

12:09.880 --> 12:11.880
What makes you think that?

12:13.280 --> 12:15.480
They're blurry.

12:15.480 --> 12:17.480
That's true. They are blurry.

12:18.480 --> 12:20.360
But are they really blurry?

12:20.360 --> 12:23.680
Like, is it is it fuzzy, like a Gaussian filter on top of them?

12:24.280 --> 12:26.520
We're really like blow up a section here.

12:29.720 --> 12:33.760
They're not like blurry in the in the normal sense,

12:33.760 --> 12:36.680
but they're not rigidly defined.

12:36.680 --> 12:39.920
So like the looseness of the brushstrokes

12:40.280 --> 12:45.720
and the way that they all kind of have that implies the wind and the motion.

12:46.520 --> 12:48.160
Exactly, exactly.

12:48.160 --> 12:55.040
So these flags are basically built out of strips of blue and yellow and red,

12:55.040 --> 13:00.160
which is the French plan, but they're not it's spatially imprecise.

13:00.160 --> 13:03.000
They're kind of jumbled strokes of paint,

13:03.000 --> 13:05.400
even though each stroke of paint is not blurry,

13:05.800 --> 13:08.800
the juxtaposition of them is kind of all over the place.

13:09.440 --> 13:13.960
And the interesting thing about this is that

13:14.960 --> 13:20.600
this kind of matches really nicely to our peripheral vision.

13:20.600 --> 13:28.200
So in our peripheral vision, we see things kind of in a spatially imprecise way.

13:28.200 --> 13:30.200
It's not that we see things blurry.

13:30.200 --> 13:33.960
It's just that we don't really care where exactly they are in the image.

13:33.960 --> 13:39.440
So if you kind of look at this painting in a glance,

13:39.440 --> 13:42.000
you kind of think to yourself, oh, everything is fine.

13:43.000 --> 13:47.200
But when you actually look and take a very close look at it,

13:47.200 --> 13:50.560
you see that everything is really, really jumbled because you're using your

13:51.440 --> 13:57.200
your actual, you know, attentive vision in the middle of your of your of your viewing field.

13:57.200 --> 14:01.920
And that is more takes more into account the spatial arrangement of stuff.

14:02.240 --> 14:05.520
And so really to get a sense of this painting, you have to kind of look

14:05.680 --> 14:10.440
in every time you take a good look at it, you get a different sense of what's going on

14:10.440 --> 14:14.560
because your periphery is giving you a different a different sensation.

14:14.680 --> 14:17.600
And that's what kind of gives this this vibrant motion.

14:17.960 --> 14:21.600
So in a way, you know, you get an impression.

14:21.600 --> 14:24.760
That's why they call it impressionist because you get an impression of what's going on

14:24.760 --> 14:26.920
at every glance, but it's not really correct.

14:27.280 --> 14:30.000
And this is how Monet is playing with your visual system

14:30.000 --> 14:32.880
and making you feel like there's something real here like motion,

14:32.880 --> 14:35.040
which is really not in the pixels themselves.

14:36.640 --> 14:39.920
OK, so, you know, fast forward many, many years.

14:39.920 --> 14:41.880
And we have computer graphics.

14:44.400 --> 14:47.000
Can I interrupt and be a little perfectionist?

14:47.000 --> 14:50.240
I think that I really love these slides.

14:50.400 --> 14:53.280
There is this panel

14:53.520 --> 14:58.800
built order from, you know, that maybe you want to close it.

14:59.280 --> 15:03.200
Oh, no. Oh, no, thank you.

15:03.960 --> 15:06.640
You know, this happened to me before and I should know this already.

15:07.640 --> 15:11.280
OK, OK, thank you, thank you.

15:12.000 --> 15:13.920
Oh, why don't you say something before?

15:13.920 --> 15:18.200
OK, good to make a postman always, always close the build order.

15:20.400 --> 15:23.760
OK, so we went to computer graphics, right?

15:24.080 --> 15:27.200
And suddenly, oh, no, no, I don't have my little timer.

15:27.200 --> 15:30.840
OK, I'm going to have to look at the clock and suddenly.

15:32.520 --> 15:35.880
You could model the world physically, right?

15:35.880 --> 15:41.000
You can make a physical model of the light and the and the objects

15:41.000 --> 15:43.520
and the scene and you can render them perfectly.

15:44.360 --> 15:48.600
But you could almost render them too perfect.

15:49.320 --> 15:53.160
This is, you know, what you get out of of graphics a lot of the time

15:53.480 --> 15:55.800
is super, super real.

15:55.800 --> 16:01.280
But it kind of makes you feel and gives you a strange feeling.

16:01.280 --> 16:02.960
It's kind of too realistic.

16:02.960 --> 16:05.440
You know, everything is a little bit too clean.

16:06.080 --> 16:08.080
Everything is a little bit too robotic.

16:09.000 --> 16:11.000
Do you know why that is?

16:11.000 --> 16:13.640
Why does it make you feel that way?

16:18.880 --> 16:21.400
OK, so I'll give you my idea.

16:21.400 --> 16:25.320
My idea is that what's missing is the noise.

16:25.480 --> 16:29.920
What's missing is the junk, is the dirt, is the cracks in the sidewalk

16:29.920 --> 16:35.040
that are kind of random, the suit on the buildings, you know,

16:35.040 --> 16:38.440
all the all the all the noise, all the junk, basically,

16:39.280 --> 16:44.720
where our visual system is really used to seeing all this noise

16:44.720 --> 16:49.080
in in the outside world, and there's a lot of beauty in this complexity

16:49.440 --> 16:52.880
that when you take it away because you can't model it physically,

16:52.880 --> 16:57.000
it's too complex to model, you know, our visual system just jumps in

16:57.000 --> 16:59.520
and is like, oh, something is really wrong with this image.

17:00.760 --> 17:02.760
And so this is very important perceptually.

17:02.760 --> 17:08.840
And a lot of what I do is try to capture this complexity of the of the visual world.

17:10.120 --> 17:12.160
OK, so we talked a little bit of our perception.

17:12.880 --> 17:17.240
I want to make one more note about it, is that a lot of perception

17:17.240 --> 17:20.080
is really in your head. It's not really what you see.

17:20.400 --> 17:22.520
OK, so here's an example.

17:22.520 --> 17:28.040
And if you look at this image and you look at the at the thing

17:28.040 --> 17:31.040
that's that's in a red box, what is what do you see?

17:33.760 --> 17:42.920
A soccer player, a player, yes, soccer player, yes, great soccer player.

17:43.360 --> 17:45.800
But do you really think that there's a soccer player there?

17:45.800 --> 17:48.800
Like if I take him and I magnify him.

17:49.760 --> 17:51.960
Eh, it's just a bunch of pixels, right?

17:51.960 --> 17:54.000
It's like there's white pixels, there's black pixels.

17:54.000 --> 17:55.320
You can't really see the head.

17:55.320 --> 17:57.760
There's a little bit of legs there, but there's really, you know,

17:58.160 --> 18:00.400
there's there's really no player here, right?

18:00.400 --> 18:01.680
Do you agree?

18:01.680 --> 18:07.680
Like if you look at this and this, it's like, so a lot of what makes you think

18:07.680 --> 18:09.880
there's a soccer player here is actually the context.

18:09.880 --> 18:13.160
You know, there's a soccer field, it's in situation.

18:13.160 --> 18:15.280
You're kind of like filling in the details.

18:16.000 --> 18:17.160
Here's another example.

18:17.160 --> 18:20.520
This is an image from a video that Antonio shot.

18:21.280 --> 18:26.320
And it's kind of blurry, but but your brain can fill in the details.

18:26.320 --> 18:28.720
What are you? What do you see here?

18:31.400 --> 18:37.200
A man sitting at a computer with like a phone to his ear.

18:37.720 --> 18:38.800
Exactly.

18:38.800 --> 18:43.960
But if I show you the details, you suddenly see that everything is wrong.

18:43.960 --> 18:45.160
White, he's not talking on the phone.

18:45.160 --> 18:47.800
He's talking in the shoe and he's not looking at a computer.

18:47.800 --> 18:50.000
He's looking at a at a garbage can.

18:50.000 --> 18:53.400
OK, and the mouse is actually a stapler and there's a toaster there.

18:53.400 --> 18:58.120
So everything is wrong, but the spatial configuration of the scene is fine.

18:58.280 --> 19:01.160
And if you blur it out, your brain is just like, oh, this is fine.

19:01.160 --> 19:02.920
You know, I'm just going to fill in the details.

19:03.840 --> 19:07.520
So there are these loopholes of perception.

19:08.000 --> 19:10.840
And this is a great opportunity for design.

19:11.520 --> 19:17.160
And this is how we're going to weave in things that are not real,

19:17.440 --> 19:21.280
but kind of make you think that they are as long as we're careful

19:21.280 --> 19:22.840
to guard the important bits.

19:22.840 --> 19:26.520
There are some there are some anchors of perception that we should care about.

19:26.960 --> 19:31.000
And we have to make sure that they're there and your brain is going to do the rest.

19:31.560 --> 19:40.560
OK, so what I'm going to cover today is I'm going to look at multiple examples of this.

19:40.560 --> 19:43.280
So we're going to look a little bit at work that

19:44.720 --> 19:47.400
looks at different senses and putting them together.

19:47.400 --> 19:50.880
So here we're going to talk about audio and motion, so vision and hearing.

19:51.600 --> 19:54.280
We're going to talk about modeling these complexities.

19:54.280 --> 19:58.600
So the very, very fine details of individual appearance.

19:59.120 --> 20:04.400
And we're going to talk about using time or using the passion,

20:04.400 --> 20:09.720
the passing of time as a creative material.

20:10.400 --> 20:14.040
I'll stop here and ask if there's any questions or complaints, and then I will continue.

20:16.240 --> 20:20.760
OK, so let's talk first about audio and motion.

20:21.760 --> 20:26.160
We're going to talk about how people move when they speak.

20:26.160 --> 20:30.120
And it's the the what I'm talking about is going to look something like this.

20:34.320 --> 20:36.120
I know how was it the personality travels?

20:36.120 --> 20:38.560
Well, maybe there'll be some sort of physical explanation for it.

20:38.560 --> 20:44.360
So people move when they speak and these are called conversational gestures.

20:44.360 --> 20:47.440
OK, there's the kind of stuff that we do when when we speak.

20:47.440 --> 20:50.440
And we never do basically when we don't when we don't talk.

20:51.080 --> 20:56.080
And this type of gesture is not the only form of communicative gesture.

20:56.080 --> 21:00.080
There's a continuum from language that accompanies speech, sorry,

21:00.080 --> 21:05.000
language that accompanies motion that accompanies language to motion

21:05.000 --> 21:06.240
that replaces language.

21:06.240 --> 21:09.600
So, for example, sign language is a language of its own.

21:09.840 --> 21:12.760
It doesn't need speech to go with it.

21:12.760 --> 21:17.280
And blends are like Italian, like there's all kinds of things that Italians do,

21:17.280 --> 21:20.400
which kind of have meanings that people agree upon.

21:20.400 --> 21:23.000
So it's almost like a language.

21:23.000 --> 21:24.840
But we're not going to talk about these things.

21:24.840 --> 21:29.400
We're going to talk about the motion that accompanies speech when you do talk.

21:32.640 --> 21:37.120
So what we want here is you want to learn about how people use gestures

21:37.120 --> 21:42.840
when they speak and to do that, we're going to take in a raw audio signal of speech.

21:42.840 --> 21:44.640
So literally the waveform.

21:44.640 --> 21:48.160
And from that, we want to directly predict hand and arm gestures.

21:48.200 --> 21:49.960
So it's going to look something like this.

21:55.280 --> 21:57.400
So this is a really, really hard question.

21:57.400 --> 21:59.720
OK, it's an ill-defined problem.

21:59.960 --> 22:03.760
There's not a one to one correspondence between the audio and the motion

22:03.760 --> 22:06.640
because I can say something today and move in a particular way

22:06.640 --> 22:08.840
and do it a completely different tomorrow.

22:09.920 --> 22:11.920
It's not synchronous.

22:11.920 --> 22:16.640
The motion is often not synchronous to the related utterance.

22:17.120 --> 22:21.760
And it's also a task that would be really hard for people to do or even to annotate for you.

22:21.760 --> 22:25.600
So getting supervised learning in this setup is really, really hard.

22:27.480 --> 22:33.480
And what we want to do is we want to, you know, learn about this in kind of in the wild setting.

22:33.680 --> 22:39.040
So what we did is we went and collected a large data set of people who are speaking.

22:39.480 --> 22:46.160
And for each frame, we annotated it automatically using an out of the box

22:47.040 --> 22:48.640
2D pose detection.

22:48.640 --> 22:52.960
So it kind of finds the pose of the arms of the hand and the hands of the people.

22:53.320 --> 22:55.120
And the data looks kind of like this.

23:01.000 --> 23:03.200
Waiting outside. Why are you telling me all this?

23:03.200 --> 23:05.160
And you're not going to believe what they said they want to do.

23:06.240 --> 23:08.640
Isn't that disgusting? It's 2012.

23:08.840 --> 23:11.200
We're still not on the.

23:11.200 --> 23:14.040
And then report it to the police.

23:14.040 --> 23:18.640
Even Lauer's conversations light more photons per second.

23:19.320 --> 23:22.840
Still none of the two young to be vaccinated.

23:22.840 --> 23:25.120
And why would you choose not to do that?

23:25.120 --> 23:29.320
So you can already see that people are very different in how they gesticulate.

23:29.600 --> 23:32.400
But within a person, there's a lot of repetition.

23:32.400 --> 23:37.640
And here I'm showing clusters in in the rows of clusters of gestures.

23:38.720 --> 23:42.720
And this is because people just tend to perform the same motions over and over again,

23:42.840 --> 23:45.000
because they have their typical style.

23:45.560 --> 23:48.040
And that's great because it gives us a learning signal.

23:48.040 --> 23:51.640
And so what we're going to do here is we're going to model each person individually.

23:52.520 --> 23:56.160
And the way we're going to predict gestures from audio is we're literally going to take

23:56.160 --> 23:58.200
the raw audio as input.

23:58.200 --> 23:59.680
We're going to treat it like an image.

23:59.680 --> 24:02.080
So we're going to think about it as a spectrogram.

24:02.120 --> 24:07.200
We're going to stick it into a neural network and we're going to output a temporal series of poses.

24:07.640 --> 24:11.400
And what each one of these really is, is just a vector of numbers.

24:11.560 --> 24:15.640
But they represent the pose of the arms and hands of a person.

24:17.240 --> 24:18.680
And the result looks like this.

24:23.080 --> 24:25.320
So, you know, try and separate the two.

24:25.960 --> 24:32.280
OK. Now the good news is the one thing to notice is, you know, for this given audio,

24:32.480 --> 24:36.720
we predict a stack of these poses and we have two kinds of losses.

24:36.760 --> 24:40.880
One is the regression loss to this pseudo ground truth of 2D poses that we have.

24:41.480 --> 24:47.880
But what we really want is we want to generate motion according to the style of this particular person.

24:48.720 --> 24:58.080
And so we add another adversarial loss that will tell us whether the motion is real or not with respect

24:58.080 --> 25:04.360
to this person. And this makes a really big difference perceptually, because I'm going to show you here

25:04.360 --> 25:09.560
on the left, you're going to see the result when you only have a regression.

25:09.560 --> 25:14.600
And what happens when you do that is that you kind of get something that's very close to the mean.

25:14.600 --> 25:16.560
So the motion is just very, very slow.

25:16.560 --> 25:18.520
It's kind of like you're going through honey.

25:19.120 --> 25:25.040
And when you add this adversarial loss, you kind of snap to one mode of the output because, you know,

25:25.040 --> 25:29.640
I could have predicted different gestures, but I'm going to pick just one and make it look real.

25:29.800 --> 25:31.680
OK. So that's going to be on the right hand side.

25:32.640 --> 25:39.240
So, you know, it makes us feel a little bit better about what we're seeing.

25:39.240 --> 25:42.080
And here, by the way, I'm pasting in the face.

25:42.080 --> 25:46.640
I'm not predicting the face in this work, but I'm pasting in the ground truth face to give you more

25:46.640 --> 25:51.040
perceptual context to see what, to kind of understand what you're seeing.

25:52.040 --> 25:58.240
OK. So we can also look at prediction results for different people and just look at what it looks like.

25:58.800 --> 26:03.960
Talking about the instantaneous rate, the rate, just when the concentration and I'm putting the ground truth

26:03.960 --> 26:10.400
video on the bottom right, even though, you know, I could really have predicted any realistic motion.

26:10.400 --> 26:13.880
I don't really want to predict the real one, but it's just for reference.

26:13.880 --> 26:20.880
Energy to boil. Higher kinetic energy, higher temperature.

26:21.880 --> 26:25.880
Logo in order to make it more modern. Yeah.

26:28.880 --> 26:30.880
Yeah. Appropriate noise for that. Thank you.

26:30.880 --> 26:35.880
That is where this method fails. So we're only taking audio as input.

26:35.880 --> 26:40.880
There's no, like, there's no, you know, text, there's no semantics.

26:40.880 --> 26:50.880
So the main limitation here is that even though we're using hours of data for training, we really, there's really not enough data to capture.

26:50.880 --> 26:56.880
The cement, you know, fine grain semantics in order to predict metaphoric testers.

26:56.880 --> 26:58.880
So I'm going to show you a video here.

26:58.880 --> 27:01.880
And I want you to notice what happens when he says the word random.

27:02.880 --> 27:09.880
So we, we don't get the circular motion that goes with random. We just can't predict that.

27:09.880 --> 27:18.880
But we do predict the beat motion, which is these up and down repetitive motion that kind of comes with random motion.

27:18.880 --> 27:21.880
So we don't get the circular motion that goes with random.

27:22.880 --> 27:39.880
We just can't predict that. But we do predict the beat motion, which is these up and down repetitive motion that kind of cut the sentence temporally and make us feel like the person is actually moving while they're talking, which is, which is nice.

27:40.880 --> 27:51.880
Another thing that's, that's interesting about this is that it's a little bit hard to do this kind of work the way that I described it.

27:51.880 --> 27:59.880
And that is because the most important part of the body when you're speaking with your hands basically is, is the hands, right.

27:59.880 --> 28:16.880
And the hands are very, very small in an image. And they're very articulated the fingers. And we're just not there yet in terms of computer vision 3D reconstruction techniques to get really good hand estimations.

28:16.880 --> 28:35.880
And so our entire everything we did depends on the fact that we have good, good, you know, detections of body pose and that's not the case. So here's an example, just a randomly picked example from Ellen, Ellen's videos and you can see what happens if you just use an image based system to get the hand reconstructions.

28:36.880 --> 28:48.880
To a CBS for one item. And the receipt was so long that I couldn't even believe it. I called it outrageous. I called it mind boggling. I called it long.

28:48.880 --> 28:56.880
Okay, so this is complete junk, right, like if you try to request to this and everything would with fail miserably. So what do you do.

28:56.880 --> 29:07.880
One thing that we noticed that is actually super interesting is that there is a really high correlation between the motion of the arms and the shape of the hand.

29:07.880 --> 29:29.880
So we're going to do a trick and we're going to take as input not only the audio. If we want to get better hands but you can look at just taking the arms as input, and only from the arms, the arm motion, you can predict a pretty good shape of the hands which is which is almost seems magical.

29:29.880 --> 29:38.880
And so if you do this body to hand thing, what you get looks like this.

29:38.880 --> 29:40.880
Electrons around it.

29:40.880 --> 29:55.880
The atomic oxygen would have six. So it has two more than it normally has. So there's like there's no pixel going in there's no audio going into here there's literally just the motion of the arms and it's it's amazing that it even works.

29:56.880 --> 30:04.880
Again, there's not enough information right so it's not enough to capture metaphoric so if you see here.

30:04.880 --> 30:08.880
She's got this.

30:09.880 --> 30:15.880
And this is the ground truth video okay.

30:15.880 --> 30:28.880
To a CBS for one item. And if you just take the arms, you're not going to get that it looks like this.

30:29.880 --> 30:43.880
And so you can do an extra trick and you can say okay well I can take in the body but I can also just look at the input images because that's what image based reconstruction does anyways.

30:43.880 --> 30:48.880
And together with this body prior, I can get a much better hand reconstruction.

30:48.880 --> 31:00.880
So here on the left is the is the body only input no audio no pixels. And on the right, I have the body and the image together.

31:00.880 --> 31:09.880
To a CBS for one item. And the receipt was so long that I couldn't even believe it I called it outrageous I called it mind boggling I called it long.

31:09.880 --> 31:17.880
So this is already better this time like, you know, this is stuff that we can work with already and it looks much more realistic.

31:17.880 --> 31:23.880
Are there any basic restrictions according to just like human anatomy that are also used within the model.

31:23.880 --> 31:25.880
Sorry saying it.

31:25.880 --> 31:35.880
Are there any just base restrictions on what movement is possible according to human anatomy like you can't completely I don't know. Yeah.

31:35.880 --> 31:38.880
And not.

31:38.880 --> 31:44.880
Sorry, I'm speaking to you from the garage so there's there's exciting background noise.

31:44.880 --> 31:53.880
And not in my model. Okay, but there is in those.

31:53.880 --> 32:03.880
So when we use, when we use ground truth that's coming from 2D reconstruction of key points or 3D reconstruction of hands in this case.

32:03.880 --> 32:10.880
Those models have a lot of, you know, human pose priors built into them.

32:10.880 --> 32:28.880
But we don't. So we're at we're kind of the goal of the, of the body to hands angle is to add an additional fire but one that is not based on, you know, physics that you might calculate from human bodies but it is based on a data driven prior.

32:28.880 --> 32:34.880
So if you've seen enough bodies, you can infer this automatically. Does that answer your question.

32:34.880 --> 32:35.880
Yes, thank you.

32:35.880 --> 32:44.880
Cool. All right. So, let's see what time is it when do we stop at on the hour right.

32:44.880 --> 32:52.880
Yeah, I mean we could, we could go a little further if you want. No, no, no, I'm just like I can I can I have a lot of stuff but I'm just going to cut accordingly.

32:52.880 --> 33:00.880
So I'm going to show you the more interesting. I think that we said 2pm for students but it's. Yeah, that's fine. Okay.

33:00.880 --> 33:14.880
So, okay, so in this work, we really only care to predict 2D motion because we that's what we were, we were interested in. And these stick figures are a nice output representation.

33:14.880 --> 33:24.880
But they don't really provide you enough perceptual context as a viewer to actually see that what the result looks like if we do a good job. Okay.

33:24.880 --> 33:37.880
So instead we can synthesize a video, an actual video the speaker so we what are we going to do we're going to take a real video of the speaker and

33:37.880 --> 33:54.880
right, we can do the same trick where we get a 2d pose detection. And our goal is to learn the mapping between these 2d pose skeletons back to the real frame of the person and this is based on picks to picks you know you probably

33:54.880 --> 33:59.880
you know we stole us so this is this is based on his work, but we're going to do this for a video.

33:59.880 --> 34:13.880
So if you do that, what you get is something like this, where again, I'm pasting in the ground truth face key points because I'm not predicting that but I need them to make a video right so it's going to look like this.

34:13.880 --> 34:30.880
Try and separate the two. Now, now the good news is, these days, very few people will say they are completely and I just want to like focus you on this. This is a completely fake video. Okay, it's completely synthesized there is like, there's nothing real about it.

34:30.880 --> 34:49.880
And it's actually being predicted from raw audio with this 2d, you know system that gives us only the pose. And, and that's kind of amazing. It's, it's the coolest thing about this is that not only can we synthesize a realistically looking

34:50.880 --> 34:57.880
video, but we also managed to capture a very convincing motion of the person.

34:57.880 --> 35:15.880
Mostly what what makes you think that it's good is those beat gestures it's like okay I'm talking and I'm chopping up my sentence and there's kind of going with the same rhythm as my voice and not as convincing enough for people to think that that this is real.

35:15.880 --> 35:18.880
Okay.

35:18.880 --> 35:31.880
We're making fake videos, basically. Okay, so it's interesting to think about what can we do in order to decide whether a video was faked or not.

35:31.880 --> 35:44.880
And it turns out that the same kind of idea can be applied to forensics as well. So I'm going to show you an example let's look at Obama and like look at this is from his address to the nation which he used to do every week.

35:44.880 --> 35:52.880
And I want you to say to see what he does when he says you know hello everybody.

35:52.880 --> 36:13.880
Hi everybody, hi everybody, hi everybody, hi everybody, hi everybody, hi everybody, hi everybody, hi everybody, hi everybody, he has this like upward motion. And basically, if you have the right words, and you have the right motion together, then it's Obama.

36:13.880 --> 36:29.880
But if you try to do some deep fake of him and you try to change his lips and you know make him say something different, you wouldn't get the right motion to go with this. And this is like a great signal to see that something is fake.

36:29.880 --> 36:49.880
And this is a really nice line of work that shorty did you see broccoli with honey for a while they look at exactly this kind of thing so it uses the, you know the whole of the person all of the details in multiple modalities to detect things that are fake.

36:50.880 --> 37:00.880
All right, I'm going to move a little bit to something else if there's any question this is a good time.

37:00.880 --> 37:09.880
Okay, so let's talk a little bit more about this idea of learning on the little details of a person, a person's appearance.

37:09.880 --> 37:18.880
We're going to consider a very special style of gesture which is dance. And it's an art for it's been around since the beginning of time.

37:18.880 --> 37:34.880
But frankly, nobody's really figured out how to capture the little subtleties of it. And what we're going to do is we're going to start we're just going to start directly by looking at a demo. Okay, so look at what what we can do.

37:40.880 --> 37:42.880
Oh

37:45.880 --> 37:47.880
Oh

37:48.880 --> 37:50.880
To me

37:51.880 --> 37:53.880
Close

37:54.880 --> 38:01.880
If I tell my heart I'll still feel pain Whatever I do still feels the same

38:02.880 --> 38:04.880
Nothing but tell me now

38:07.880 --> 38:10.880
Everything we had well it's gone to waste

38:13.880 --> 38:22.880
What we're doing here is we're taking this source video, we're detecting its pose and we're puppeting our target person to dance in the same way.

38:22.880 --> 38:35.880
And it looks really nice, but I'm actually using a trick on you I'm using a perceptual loophole and and this is really interesting we usually don't show this but it turns out that the music is very helpful.

38:35.880 --> 38:41.880
Okay, I'm going to show you exactly the same result without the audio and I want you to notice if it looks different to you.

38:52.880 --> 38:54.880
Okay.

39:18.880 --> 39:20.880
Okay, anyone

39:20.880 --> 39:26.880
Mostly just look super unnatural where before it looked like it actually like was dancing.

39:26.880 --> 39:42.880
It's awful right it's like everything is moving it's kind of wobbly and like, you know, just, just not right right. And it's really interesting it's exactly the same pixels okay I just turned off the audio, but when I show it to you with audio.

39:43.880 --> 39:50.880
Your perceptual system is like whoa there's like music and there's dancing and it's together and you're filling in the holes.

39:50.880 --> 40:04.880
You really are. And it's very important it turns out when you're showing a demo if you guys go and do things and design or and even an AI to kind of set it up in a way that that makes people be in their happy places.

40:04.880 --> 40:19.880
And use multiple senses when they're when they're looking at something and it actually makes it look better than that it really is. And our goal here is actually not to make a perfect video I don't really want to be in the business of making people do things that they

40:19.880 --> 40:30.880
didn't do our goal is to study you know the statistics and whatever, but that it is very helpful to use this different senses. Okay, so what are we doing here.

40:30.880 --> 40:45.880
In a way, it turns out that we can use the same technology we talked about before to transfer dance motion from one person who knows how to dance to a different person who is a terrible dancer, like my co author here, think way.

40:46.880 --> 40:52.880
And this is basically motion that's not conditioned on audio it's conditioned on different people.

40:52.880 --> 40:58.880
And training is the same we train a cycle from you to your stick figure and back to you.

40:58.880 --> 41:09.880
And you can think about it as trying to go from you to full reconstruction of you through this tiny little bottleneck, which is these 2d poses that are not learned.

41:09.880 --> 41:18.880
And the idea here is to get learned a really good mapping from this stick figure back to the appearance of the person.

41:18.880 --> 41:30.880
Because I really want to model what he looks like. I want to capture all the little minute details of his body and the way that he kind of, you know, looks when he's moving.

41:30.880 --> 41:44.880
And I want to get the stuff that you can river really annotate and want to get all the complexity this this beauty in the details. This is what I want. So the goal here is not to start from a single image of a person and make them dance.

41:44.880 --> 41:52.880
The goal is to actually model this person and for that I need a bunch of training data of this person from different poses.

41:52.880 --> 42:00.880
And at test time, we unwrap the cycle, and we put a different person on the other hand.

42:00.880 --> 42:06.880
So here test and training are not the same in purpose and this is different from a lot of other methods.

42:06.880 --> 42:20.880
And these stick figures are a nicer presentation in the middle, because you're kind of agnostic for appearance like you could be, you know, bigger or smaller and width, but but it doesn't really matter for for your skeletal structure.

42:20.880 --> 42:31.880
So if we managed to learn a good model of our target guy, we should be able to sample any new pose from this ballerina and synthesize a new image of the way.

42:31.880 --> 42:38.880
And this is good because a good model of appearance to generalize to new poses.

42:38.880 --> 42:49.880
So I'm using video synthesis kind of like at school it's like show your work, you know the generated video is as is a test of whether our modeling or perception of this guy really worked.

42:49.880 --> 43:09.880
Okay, so we do some tricks to make this not you know frame by frame and achieve a temporal coherence which is which is important perceptually, but I actually want to talk about something else I want to talk about the fact that the face is very important perceptually.

43:09.880 --> 43:26.880
Okay, and we didn't affect this this is, this is Renoir, and there's something really interesting about this painting, which is basically that there's different resolution of painting between different parts of the image right.

43:26.880 --> 43:37.880
The face is very, very sharp, and the eyes are extremely sharp you can even see like the the glint of the of the lighting reflecting on her pupil.

43:37.880 --> 43:43.880
But everything else is fuzzy, and it's kind of like the flags we saw before it's like not really in place.

43:43.880 --> 44:04.880
This is something very, very interesting, it makes you draw your gaze to her eyes, which are the most expressive part of the face, it gives you an emotional reaction, and it makes you ignore everything else because everything else is in your periphery which is not really

44:04.880 --> 44:13.880
attuned to spatial relationships anyways, and it kind of mimics our own perceptual experience.

44:13.880 --> 44:23.880
When we gaze into someone else's eyes like somebody that we really love or somebody that we really want to listen to it's always you know everything else about their body is in the periphery.

44:23.880 --> 44:39.880
So, so this is kind of the trick that Renoir is doing is playing on you, and we're going to play the same trick. Okay, so, so, you know this is this was done before amazing gans were around and so we didn't have the technology to make everything look perfect.

44:39.880 --> 44:53.880
Okay, the face is important. If we get the face right, people will say oh this looks nice. So we actually devote a special again just to the face region, in order to correct it and more realistic.

44:53.880 --> 45:09.880
It makes a big difference. So, if this is the baseline, this is after temporal swathing, this is what happens when we add an additional again for the face itself and it kind of looks almost like Caroline, who used to be my undergrad and did this work.

45:09.880 --> 45:15.880
And now she's, she's a full grown PhD with you guys at MIT.

45:15.880 --> 45:43.880
And, and you can see her dancing in all of these videos, like this one.

45:43.880 --> 45:54.880
Okay, and we can also do the same thing we take a motion one person and we can apply it to many people. And there, when it's small of course the face matters less.

46:13.880 --> 46:34.880
And we turn it into a controllable interactive application that got a lot of attention to the field of image and video synthesis. It appeared in popular press it exhibited and museums, it was incorporated into stage performances.

46:34.880 --> 46:45.880
And now my co author, the same guy who doesn't know how to dance, even turn this into an app that you can download for free from the app store and you can dance and tick tock and whatever you want with it.

46:46.880 --> 47:04.880
And this is interesting. But another direction that we kind of have been discussing a word to take this technology is to provide a platform for capturing performance as a form of intellectual property for for choreography.

47:04.880 --> 47:12.880
Because it turns out that unlike musical score that can be copyrighted, there's no way to copyright dance.

47:12.880 --> 47:27.880
In fact, there wasn't even a way to capture dance in the West until the 20th century. So most of the belays that we know of like Swan Lake and you know, all of these even the famous ones, haven't really survived in their original form.

47:27.880 --> 47:41.880
Until this day there isn't an agreed upon notation of dance and this is just one example which is called the monetization, but all of the, all of the forms of notation.

47:41.880 --> 47:54.880
They don't have a way to accurately capture all the small details. They're not parametric they're not scalable every time you come up with a different move you have to come up with a different notation.

47:54.880 --> 48:08.880
And our, our idea is that capturing things the way that we do can can try and get to these to these issues and maybe offer a new solution.

48:08.880 --> 48:13.880
So we talked a little bit about the fact that there is artifacts but

48:14.880 --> 48:29.880
Actually, it turns out that that is the interesting part about this technology to a lot of artists. So here's, here's one example where the same team plays little company had a gig where they made this music video.

48:29.880 --> 48:37.880
It's really celebrating the problems in this technology and making them into art.

48:37.880 --> 49:06.880
And here's another example that I really love. This isn't actually using our work it's using that to bid but it's very, very similar so you can ignore the differences and just focus on what this person is doing with this.

49:37.880 --> 49:54.880
Thank you.

50:07.880 --> 50:08.880
Thank you.

50:37.880 --> 51:01.880
Thank you.

51:01.880 --> 51:26.880
Perfect. But Jake here really loves those and he's actually pushing it like to the extreme extreme so there's all this hair and the feathers which are really, really hard to capture for guns, but that he's just like let's throw it all in and just let it, you know, be artistic and and and this is kind of what he's been looking for in using these technologies.

51:26.880 --> 51:47.880
Okay, but, but there's also a limitation on how you can use this kind of stuff for art, because essentially what we're doing when we generate images or we generate video or motion using again is that we're always using a training set which is real.

51:47.880 --> 52:06.880
Okay, and we're trying to teach AI how to make us more of this real thing. Okay, so, so we want to, you know, maybe generalize out of the distribution of the training set but not by much we still want to keep things realistic that's our training signal.

52:06.880 --> 52:20.880
And this is this becomes a limitation when you want to do something like this so so these are visuals from New York's tabula bassa music video.

52:20.880 --> 52:29.880
And this actually goes with a with a large performance and show that she had a year ago.

52:29.880 --> 52:42.880
And what they wanted here and they actually came to us to ask for help in order to do this and we couldn't help them what they wanted to do is they wanted to have this marriage between a human and an orchid.

52:42.880 --> 52:54.880
So, so they want the motion and they use mocap for this but they want the motion to come from a human but the visuals and kind of the embellishment to come from a flower.

52:54.880 --> 53:14.880
And there's nothing that we can do to help them, because we don't yet have this ability to do this compositionality between different realistic things to make something to make a new kind of life for him if you may.

53:14.880 --> 53:24.880
We just can't do there's we don't know how to do this. So this is something that is that is a limitation of training to do realism.

53:24.880 --> 53:38.880
And it's a very interesting future direction from work if you're looking for something really cool to do this this kind of idea can give you can give you a nice direction that we don't know how to solve yet.

53:38.880 --> 53:44.880
So how did they do that. How did they did it by hand they did it by hand so.

53:44.880 --> 53:56.880
So they, this is a really cool artist. He's also a professor I think in Hong Kong or one something like that. And they did motion capture on the people.

53:56.880 --> 54:18.880
They have the motion signatures, but everything else is hand designed, you know, 3D models and add motion that is that is basically put together with with its dressed, you know, it's, it's dressed on top of the human motion.

54:19.880 --> 54:32.880
What they wanted would have been similar to people who are here in the class. The other day there was some AI that could make pictures out of like the texture of the skin of an elephant or something.

54:32.880 --> 54:36.880
Like the texture of noodles would have been similar to that.

54:36.880 --> 54:45.880
Like you had a face and then you draw it with the texture of say noodles and then you change the noodles to make it look like it's talking.

54:45.880 --> 54:57.880
So that's one. So you mean like the Geiger paper, right. The counterfactual paper. And there's also, I know you guys looked yesterday at the Lee. So there's different.

54:57.880 --> 55:11.880
Yeah, that's one approach to do that. It's a little bit right now that technology is not yet.

55:12.880 --> 55:28.880
But if you take those noodles and you make them in the shape of a dog or whatever, it's still noodles, you know, and you just cut them into dog. There's nothing that really takes the texture and like kind of applies it onto the 3D form.

55:28.880 --> 55:36.880
So you see here if you look at the at the texture of the flower, it really changes with the articulation of the flower, right.

55:37.880 --> 55:52.880
If you just take noodles and you, you know, you make the right mask for them, it's still it's not yet realistic, right. And this is again something we don't know how to do we don't know if you look at the cycle again results for example.

55:52.880 --> 56:10.880
It doesn't conform to the 3D shape of the object. It doesn't actually respect that. So, yes, that's in the right direction, but it's not yet there. All of this is open problems.

56:10.880 --> 56:13.880
Answer your question.

56:13.880 --> 56:17.880
Yeah, yeah, just stuff to think about it's neat neat stuff.

56:17.880 --> 56:32.880
This is really cool. I don't know how to solve this but it's this is a cool, this is a cool, cool idea, cool, cool direction. Okay, I literally have three minutes so I am going to have more stuff so

56:32.880 --> 56:45.880
the rest of the stuff I wanted to talk about would have been kind of designing with with time using time was as an interesting medium.

56:45.880 --> 56:59.880
And I guess what I can do is just just tell you two different highlights. Okay, so I'm not going to really walk you through all of the story here, but

56:59.880 --> 57:10.880
let's see. If we think about if we think about time and we think about images not from video but like large collections of images.

57:10.880 --> 57:22.880
There's something really interesting about them and the interesting things is that a lot of the time when you think about historical data you think about text, but but you don't really write everything down.

57:22.880 --> 57:35.880
And, and those are things that are really captured in images, and we're lucky we're very, very lucky that we have now basically more than 100 years of historical visual record.

57:35.880 --> 57:50.880
And the things that you get from that are things that are you know you can look at this image and kind of think to yourself you know if 100 years from now somebody wanted to explain in a history book, what are hipsters, it would be really

57:50.880 --> 58:04.880
really hard, because, you know, here you can see the difference between nerds and hipsters and you know what is it that makes these guys look cool is it is it hats, is it the scarves that the Instagram filter that was slapped on the image like is you know what is.

58:05.880 --> 58:08.880
It's really hard to say in words.

58:08.880 --> 58:14.880
And in any case we don't really bother to talk about these things when we write stuff down.

58:14.880 --> 58:19.880
And so historical images kind of captured this for us.

58:19.880 --> 58:31.880
But of course then we kind of you know if you took a collection of historical images you kind of end up with you know a bunch of historical images that you need to sort through and you end up selling in the garage sale because it's too much work.

58:31.880 --> 58:48.880
So if there is a way to automatically get, you know, how do things change over time from images that would be really cool. And that's stuff that we've done a couple of times here this is work with historical yearbooks high school

58:48.880 --> 58:58.880
yearbooks, which is a really nice source of data because there's spaces and they always stay the same but what changes is fashions and social norms.

58:58.880 --> 59:15.880
And what you can do with with a lot of data like this that has kind of a consistent subject matter but changes over time is something like Jason Sullivan here who is an artist has done with his graduating class versus his mother so he graduated

59:15.880 --> 59:31.880
in 1988 from Fort Worth in Texas. And on the left you see an average image of all the people in his classroom the women and the men versus on the right the ones that came from his mother's class of 67 and you can already see that there's big differences

59:31.880 --> 59:48.880
right people used to look different these to stress a little bit different. They used to treat the camera a little bit differently. And we did the same thing with our data, where we took, you know, 100 years of photographs and we looked at averages of men versus

59:48.880 --> 01:00:01.880
women over time and you can notice that people, you know, look a little bit different that the hair is different they smile more than they used to, you can quantify this kind of thing.

01:00:01.880 --> 01:00:17.880
You can look for other characteristic elements for different decades like different hairstyles that are very distinctive.

01:00:17.880 --> 01:00:28.880
And this gives you tools for analysis of creativity and fashion, but we're not going to look at this but instead what we're going to say is, okay.

01:00:28.880 --> 01:00:44.880
This is interesting you can do this with spaces everything is very online you can look at fashion but what happens if you want to think about time and in the real world in the outdoor world how can you use a time as a, as a, as a creative medium.

01:00:44.880 --> 01:01:03.880
And so one thing we did afterwards is we went and looked at a lot of data coming from street view images, and we looked at whether we can say okay let's say I want to travel in time I'm stuck in a pandemic and I want to go visit New York but I want to make sure that I did it on a particular

01:01:03.880 --> 01:01:16.880
Sunday afternoon in 2011 okay how can I do this is so maybe I can use flicker images if I want to go to Columbus Circle, but if I want to go to some random corner there's just not.

01:01:16.880 --> 01:01:19.880
There's just not the people just don't take pictures there.

01:01:19.880 --> 01:01:38.880
So what we did is we went and looked at the Google time machine, which is basically your normal street view interface but they actually keep historical images of the previous runs of the of the cars through the city, and that gives you a single location with

01:01:38.880 --> 01:01:50.880
different riding conditions and different weather conditions. And this is really cool because you can collect this at a really large scale, like, you know, all of New York or basically the entire world.

01:01:50.880 --> 01:02:05.880
And then for each location you have multiple snapshots of that place, only there's still very sparse, and to go to this place in a particular day and time you have to learn how to fill in the gaps.

01:02:05.880 --> 01:02:12.880
Okay, because you know the buildings stay the same, but the weather conditions might change.

01:02:12.880 --> 01:02:20.880
So the travel and time you want to take a particular image and you want to be able to change the lighting and the, and the weather.

01:02:20.880 --> 01:02:28.880
So basically what you want to do is you need to disentangle or learn to disentangle the things that are varying temporally versus the things that are permanent.

01:02:28.880 --> 01:02:41.880
And if you can do that, then you can use time basically to synthesize new things that you know you never really captured they might have existed but you don't know because you weren't there.

01:02:41.880 --> 01:02:50.880
And so for a particular scene, you can do things like you can rotate the sun around and this is completely synthetic right this is a result of what we do.

01:02:50.880 --> 01:03:07.880
Or you can copy and paste buildings, for example, so you can modify the permanent factor, and that would look something like this right here's here's an inserted building it looks perfect but but it's completely fake.

01:03:07.880 --> 01:03:18.880
And I'm not going I'm going to, you know, just not going to go into the technical detail of how it's done but basically the thing that helps us is that we've seen the same place over and over again.

01:03:18.880 --> 01:03:23.880
The one thing I do want to talk about is that the nugget.

01:03:23.880 --> 01:03:39.880
The technical nugget that we use here is that we can use a decomposition of the scene into two things that graphics tells us that are, you know,

01:03:39.880 --> 01:03:50.880
the correct, which is the difference between shading and reflectance where shading kind of captures the shadows and the effects of the illumination on the scene.

01:03:50.880 --> 01:04:03.880
And the reflectance is the actual color so I'm wearing an actual blue sweater, that would be the reflectance of the sweater and then there's the effects of the light on it that puts in the shadow.

01:04:03.880 --> 01:04:19.880
The thing about about these this this shadow representations this the shading representation is that we're we in our mind we think about the fact that maybe shadows are gray is like if you think about them as grayscale but they're actually not.

01:04:19.880 --> 01:04:22.880
Okay, and this this is the interesting thing for sexually here.

01:04:22.880 --> 01:04:35.880
And you can see this in this nice painting by Monet. Okay, so, so Monet is painting a grain stack that is sitting in a set of snow on the ground so the snow should be white the ground is white.

01:04:35.880 --> 01:04:39.880
But there's actually two colors for the illumination.

01:04:39.880 --> 01:04:42.880
There's blue from the sky.

01:04:42.880 --> 01:04:46.880
And that is kind of an indirect diffuse light.

01:04:46.880 --> 01:04:50.880
And there is a direct light that is yellow from the sun.

01:04:50.880 --> 01:04:56.880
And if you look at the shadow that's being cast on the snow.

01:04:56.880 --> 01:05:08.880
In the shadow, it's blue, because the direct light doesn't hit doesn't hit the snow and so you mostly get the indirect illumination from the sky.

01:05:08.880 --> 01:05:23.880
So the blue then the surrounding light, which has the yellow mixed into it. And when I was doing a trick here where he's actually coloring with the yellow to complement the blue just in the border to make it even more clear to your, your, you know, your center

01:05:23.880 --> 01:05:37.880
surround cells and your in your eyes that this is what's happening. Okay, so, so the trick here to get everything to look realistic was to say okay, people before I have kind of thought about shading as a grayscale thing.

01:05:38.880 --> 01:05:52.880
And most of the color in their models have gone to the reflectance images, but we actually use a two toned shading where we take separately into account the blue and the yellow.

01:05:52.880 --> 01:06:05.880
And we are trying to really capture a lot of the blue of the sky in the shading model and not in the reflectance model. And that kind of makes everything come more together and look more realistic.

01:06:05.880 --> 01:06:22.880
And then we can say okay and you know we generalize we take an image from a completely unseen place like Paris we've never been to Paris we didn't train on Paris this is like a one image example and we can relate it and make it look like you've been there and whenever you want, basically.

01:06:22.880 --> 01:06:34.880
Okay, so now I am very much over time. So I am going to stop here we've looked at audio motion we've looked at details we've looked at visual patterns over time.

01:06:35.880 --> 01:06:46.880
There's there's some food for thought you can take out of here like, you know, for example, AI and perception we can use it to create tools for art design.

01:06:46.880 --> 01:06:53.880
There's good and bad implications you have to think about what happens when you make synthetic or fake content.

01:06:53.880 --> 01:07:00.880
And we've talked about modeling all kinds of complex things and multimodal stuff.

01:07:00.880 --> 01:07:04.880
But there's a lot of stuff that's left to be done.

01:07:04.880 --> 01:07:20.880
For example, the example of the person in the orchid and compositionality. And there's also the question of how do you not only provide tools that are creative, but also create creative machines and I think you're going to learn about that more later this week.

01:07:20.880 --> 01:07:29.880
One final note is, if you want to learn more about perception and art. This is a great book by Margaret Livingston from Harvard.

01:07:29.880 --> 01:07:34.880
And these are collaborators, thank you all collaborators and that's about it.

01:07:34.880 --> 01:07:37.880
Thank you.

01:07:37.880 --> 01:07:44.880
Thank you so much that was very, very interesting and intriguing and inspiring.

01:07:44.880 --> 01:07:47.880
Oh, I'm sorry I went a little bit over time.

01:07:47.880 --> 01:07:49.880
I was trying to like, lower the details.

01:07:49.880 --> 01:08:03.880
I'm sure that there are many interesting things I personally learned and also students hopefully inspired their thoughts and the future work.

01:08:03.880 --> 01:08:09.880
Is there any question from students.

01:08:09.880 --> 01:08:14.880
I think that most of them are thanking you and I see that in the chat.

01:08:14.880 --> 01:08:17.880
Oh chat. Okay.

01:08:17.880 --> 01:08:20.880
Yeah, I think that.

01:08:20.880 --> 01:08:24.880
Okay, cool.

01:08:24.880 --> 01:08:26.880
Excellent.

01:08:26.880 --> 01:08:29.880
Thank you again. It was a great talk.

01:08:29.880 --> 01:08:30.880
Thank you.

01:08:30.880 --> 01:08:39.880
I'm very excited to, you know, put this online so everyone can benefit from it.

01:08:39.880 --> 01:08:41.880
Cool.

01:08:41.880 --> 01:08:43.880
Thank you so much.

01:08:43.880 --> 01:08:44.880
Bye.

01:08:44.880 --> 01:08:45.880
Bye now.

