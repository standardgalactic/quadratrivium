Hello, everyone, welcome back to our course, a deep learning
for art, acetic and creativity. Today, it is our pleasure to
have very a specialist speaker, David Bao, and I just let him
to introduce him a little more, because I think it's very
inspiring for many students, the path that he has come to
this point and for future. Please go ahead. So I was, I
want to give a little background since I am a post
industry academic, I spent a bunch of years as a software
engineer at Google before coming back to MIT. And I want to
give a little bit of insight in my thinking there. So, you know,
the reason it's really interesting to be in computer
science right now is because the field is changing. The dream
of having self programmed computers is one of the oldest
dreams in computer science, but it's never been a reality. Even
though we've studied machine learning for a long time, I think
that until just a few years ago, machine learning was really
more accurately called, it would have been more accurately
called the art of accurate counting. You know, statistics,
you know, understanding the statistics of, you know, how
frequent words are and by grams or, you know, certain image
statistics or something like that. And, and, and if you if you
understand statistics well, then, then, then, then, you know,
you could do some nice tricks. But I think that until recently
really calling these things sort of self programmed systems
would have been an overstatement. But I don't think it's
really an overstatement anymore. I think that these machine
learning models are really learning non trivial things. And
it leads to all sorts of questions about, you know,
what should we be doing as programmers? What does it mean
to do software engineering? And so I thought it was very
interesting time to come back to academia. That's, that's why
I'm here. And I actually think that that's one of the choices
you face when you're trying to decide between industry and
academia. And I think in industry, you will have lots of
resources to make things work to make the next widget or the
application. And, you know, there are great places, Google is a
great place, we can really push state of the art in that and
do really neat stuff. I think that there's less of a push in
industry to ask the question, Why? You know, why do things
work? Why are we doing what we're doing? Where is it going to
lead in either unintended consequences and things like
that? You know, we, we tend not to ask those questions too much
industry, because there's so much to emphasize on, you know, the
how of how to how to get it to work. And so, and so, so I
thought it was a time to, to switch tracks and start asking
why because the field is changing so dramatically. And I
think that, you know, I'd encourage people who have an
interest in these type of questions to, to realize you
can really make a real contribution by taking the
academic track as well. So okay, so let me introduce my talk.
So it's about painting with neurons of general adversarial
networks. It comes out of work from asking why, you know, why
do these networks do what they do? And so, so let me, let me
advance here. Am I in full screen? So do you see the, do you
see the like the full screen slideshow I can't see what I'm
projecting? Or do you see like all my notes and all that stuff?
Yeah, I can see it. But also maybe a student can tell us.
Yeah, okay.
That's fine.
Is everything okay? Yeah, it's a full screen slide. Hopefully,
it's okay. So, so, okay. So the main problem that we're looking
at here, and I'm not sure why the, the images are overlapped in
the right way. Hopefully, the layout will get fixed. So we're
going to next slides. But the, the, the, the, the main problem
surrounding my talk is image generation. And so, for the last
few years, there's been this question, how do you make a
state of the art program to generate realistic images? And,
you know, the general process is first you want to collect a
data set of real images, like these pictures of buildings on
the right. And, and then you want to, you know, train some sort
of program, some sort of generator network to generate
those programs. And so, so, you know, it's been a puzzle. There's
a lot of different ways you could imagine doing this. And so
people have been puzzling, how do you train such a thing? How do
you even supervise it? You know, what should the, what should
the inputs and the outputs of the network be? And, and, and the
thing that has really been working the best in recent
years is, you know, in architecture, you guys have all
heard of called GANs, generative adversarial networks. And the
trick for GANs is to reduce it down to a simpler problem that
we know what we're doing. And so the simpler problem that
they're recognized when designing GANs was that
generating images, we don't really know how to do, but
classifying images, gosh, that is an easy problem. We can
classify images. And so, so what we could do is we could train a
classifier on this really easy task, which is given two sets
of pixels, which image is real, and which image is not a real
photograph. And it turns out that for most arrangements of
pixels, this is a very easy task to train a discriminator on
it gets very good, you know, very quickly, we'll start getting
100% accurately on that. And so, so but the neat thing is that
once we have a discriminator that can tell the difference
between a fake image and a real image, then we can hook it up
to our generator, and we can say, All right, we didn't know how
to tell you, generator, how to make a real image. But you know
what this discriminator can tell you, because all you have to do
is generate patterns of pixels that fool the discriminator, if
you can make the discriminator think it's real, then it must be
better than random. Now, the problem is that, even though the
discriminator can get very accurate at telling what's real,
they, the generator will also be very good at learning how to
fool the discriminator without working very hard, it'll realize
that aha, the only thing I need to do to make the discriminator
think is real is put some blue sky in there and put some texture
that kind of looks like, you know, building texture. And, and
the discriminator will say, Well, that totally looks real,
there's a sky, you know, there's, there's the right, the right
colors for buildings and some vertical lines and things. Ah,
that's totally real. But as a human, we look at that, we think,
Oh, that's not a very realistic image at all. So the trick is to
iterate this process to go back and forth after the generator
can generate sort of halfway looking real images, then have
the discriminator say, Ah, well, that's actually fake. And
we're going to tell the difference between those new fakes,
those better fakes, and actual real photographs, and the
discriminator has to now work harder at getting better. And so
if you, if you alternate these processes, then, then you end up
very conversion to very, very good generators that can generate
very realistic images. And they, you know, the typical learning
process is actually just to do only one step of iteration
between the discriminator and generator and just alternate that.
So by the time you're done, you've played this game, you
know, millions and millions of times back and forth between
the generator and the discriminator. But the new thing
that's happening here is that it can generate these images that
look very realistic in the end. But let's see. So Oh, here's
another picture. So we'll get this images out that look very
realistic in the end. And we'll get this generator, which is
just a deterministic function that takes actually the input of
the generator is actually just a random vector. So we'll take
these relatively small random vectors like 512 dimensional
random vector, and we'll put it into this thing. And it's been
trained so that no matter what it outputs, it will look very
realistic, like this example image here. Or if I change a
vector, I'll get a different image out and it will again look
very realistic, even if it looks completely different. And so
it's just a deterministic function that really wants to
make realistic images. And, and so here's like a sample of like
output from a generator. And you can see that after millions of
these sort of generative training steps, where it's
pitted against a discriminator, it actually gets to be pretty
good. And so this is a style game v2. It's a model that was
published last year. And, and it's, you know, currently the
state of the art in generating realistic images of certain
certain types of image distributions. And, and so when
when you look at a collection of images like this, you might
think, actually, the first time I looked at the output of some of
these state of the organs, I was confused between the training
set, and the generated output, this is not the training set,
this is actually what the generator is producing. And so, so
you see all sorts of interesting effects here. And so the one of
the questions to ask is, what the heck is the model doing
inside? Can we understand the underlying algorithm? And what
the characteristics of that algorithm is like, why does this
work? And so one of the funny things that you'll notice is
that some of the images have these strange artifacts, like
take a look at this one here. So this, this scan is pretty good.
It's this generator is so good that it actually has noticed that
the training distribution that is imitating has some percentage
of images that were stolen off of shutter stock. And they still
have the watermark on them. And, and, and, and the generator
says, well, if I want to make things look realistic, I better
put watermarks on some percentage of my images too. It
learns it's got to protect its own copyright. So, so it, it
does that. And so something like 6% of the output images from
state of the art style, again, will have these kind of
artifacts that show the same type of watermarks that were on
the training set. This is the Elson Church training set. And
so, so yeah, this kind of watermarks like this. But the
reason I thought this was cool was that it, it's this very
clear thing that the image generator does, but it doesn't
always do it. Like most of the time when it generates images,
it generates images without a watermark, but sometimes you get
these watermarks. And so, and so it's, it's almost like this
binary decision. It's like, there must be a switch that the
network has at some point where it decides whether it's going to
put a watermark on an image or not. And so we can kind of ask
the question, where's that switch? Is there a neuron
somewhere in this network, which is, which is controlling the
watermarkness. And so, so I went on a hunt for this, this,
this particular network has about 30 million parameters,
which sounds like a lot, but it's just a deterministic computer
program in the end. And, and it's not that hard to go hunting
for things like this, you just, you can make an algorithm that
has a heuristic that determines whether it's a watermark or not
and just go hunting for, for things that correlate with that.
And so I'll show you what I found. So at layer five, I found
this very interesting neuron that did correlate with watermarks
a lot. It was activating whenever images look like this in
the end. And, and not only that, but because it's at layer five,
it has a has a location for where the image activates. And I'll
show you where, where it's activating. So, so this neuron
is activating, you know, at these middle parts of images,
whenever the image is showing a watermark. And there are other
neurons that have similar behavior, like so for example,
there's this neuron 234 at the same layer. And it activates in
regions like this, both in the middle watermark and the bottom
bar that shows up. And there's about, if you hunt through the
neural network, you find about 30 neurons that are similar and
behave like this. And so that's, that's pretty cool. So then the
question is, well, do these things really act like a switch?
What if we've removed these neurons from the network? What if
we force them all off? What if we turn, what if we force these
neurons to be off all the time? That will happen. So normally,
we think of these neural networks as completely opaque
systems. We train them end to end, they're just, you know,
these big black box functions. And we normally think of the
functions as computing things where everything depends on
everything. And so if you randomly rip through the
function and remove some of its operations, then maybe you
expect to get total nonsense out just garbage or noise. But we
found these particular neurons that really correlate to this
thing. So let's see what happens when we turn them off. Do we
get anything intelligible at all? So this is what the network
generated before these are the watermark images I showed you
before. And I'll show you what happens if I turn off these 30
watermark neurons. So I'm going to give the network the same
input. But turn off these neurons during its computation,
and you can see what the output looks like. So you can see
before chain, you know, forcing these neurons off and after
forcing those neurons off. The images are still very
intelligible, they look realistic still. But now the
watermarks are gone. So I thought I was when I when I saw
this, I was pretty excited, because it's like, Oh, there are
switches inside the networks. And these networks are doing all
sorts of amazing things, not just like showing watermarks. You
know, so when I first found this, it was on Progressive GAN,
which is a year earlier than a couple years earlier, the images
didn't look quite as good. But but still in Progressive GAN,
they do all sorts of amazing things, like they will arrange a
scene with a river and trees and grass and, you know, building
architectures with all sorts of different features. And you can
ask, you know, is there a switch to turn on and off clouds in
the skies or switch to turn on and off trees or windows and
buildings? And, and so I went hunting for that. And, and, and
the way I went hunting is I tested every neuron one at a time
I inverted the test. So basically, I look at each neuron,
and I say, Where is it activating? And, and I asked a
question, is it activating an interesting part of different
images? So for example, if I took this one neuron here, and I
see where it's activating when it's generating this image, you
can see it's very hot on the right and on the left, but not
much up in the sky. And on this very same neuron, when we
generate a different image with a different input, this very same
neuron is not activating very much anywhere in this this image.
But if we generate another image, then it will activate in a
specific area here, mostly on the lower left part of this image.
And you can see what's on the lower left. There's a, there's a
tree there. And so it kind of gives you the hypothesis that
maybe this neuron is correlated with trees somehow. So
obviously, we can, we can do this, we can collect this
information over thousands of examples of generated images by
looking at where the neuron is activating, we can ask what what
kind of thing is in the image, what kind of objects, what are
the semantics of the image in the location that the the neurons
are in. And we can just repeat that test process, you know,
thousands of times to see if the neuron is agreeing with any
particular kind of semantics that are in the images. So if, if
the, if the neurons are showing up where the trees are all the
time, we can just count and see if if that's if that's true in
general. And we can also look for correlations with other
things. So what I did is I, I searched for correlations with
thousands of different, you know, hundreds of different,
different types of semantics and object classes, different parts
of buildings or, or objects or other things that can show up
in a scene. And so what do we find? Well, we do find, you
know, there's a neuron that correlates with trees, just like
the one I was showing you. There's actually a few that are
like that. And there's also neurons that correlate with
other things like domes, or, or other building parts like
windows and doors. And, and if you change the model to look at
other things, then you can find neurons that correlate with
things like windows, or chairs, or other things that they show
up in, in the scene. And so this is actually pretty neat,
because this model was trained unsupervised by any labels. All
we did is we told it, generate realistic looking scenes,
realistic looking photos. And, and, and we did not train it
with any labels, we didn't tell it that these are photos of
scenes that have big windows, and these are photos of scenes
that have little windows or anything like that. Or, or, or
here's where the windows are. But what happened was, the
network discovered that it had to, you know, learn a
representation, where windows are represented differently from
the way chairs are represented. But somehow, even though, you
know, windows can look very different from each other and
chairs can look very different from each other, that the
network has this represent this this component of this
representation, this neuron that activates on all these chairs,
despite the amazing amount of diversity that it shows, like
none of these chairs really look similar to each other, they
have different colors and different textures, and they're
oriented in different ways. And yet, the same neuron is
activating on all of them, the same thing goes for other
things that show up in these images. So does anybody have
any questions about, about, about this? I'd love this to be a
little bit more interactive than the way I'm doing the talk.
So let me open the floor for a question for a minute.
Has anybody tried playing with the internals of GANs yet? I'd
love to see if has anybody like generated images using a GAN
before?
No, but I do have a question. Yes. So what was like the end goal
or the larger reason behind finding all of these neurons
that correspond to different objects or features?
Well, when I was originally looking at it, my original goal
was just to understand how these models did their computation.
So asking the question why. But the neat thing is that after I
found this structure, then it became clear that there are
new applications that you can build on top of it. And I think
that's one of the cool things that comes out of this sort of
academic style inquiry is, you know, originally, I was just
looking to make catalogs like this. This is a catalog of all
the different types of correlations that I found with
neurons inside a model for generating kitchens, and the
kinds of, you know, the patterns you see. And, and, you know,
I've done this before for classifiers. And, you know, when
you do for generators, you get different patterns. And so I was
just really interested in making these maps of seeing what is
computed at what layer, you know, where and how accurately. So
this is, you know, the progressive gain has, depending
on the resolution has about 15 layers. And if you sort of
chart what you see in different layers, you can see this this
really interesting thing phenomenon where it's in the
middle layers that you get these highly semantic correlated
neurons. But then as you get to the later layers, then they
tend to be more physical. And there's not as many semantic
objects. So it's like in layer five, we have things that really
correlate with ovens and chairs and windows and doors, even
though like a window kind of looks like an oven. The model
clearly has different neurons that correlate with windows from
ones that look like ovens. And so so that so that's that's that
so I was originally interested in just mapping things out. But
the correlations were so striking that it leads to these
interesting applications that you can build. And I can I'll show
you some in the next step. Let me before I do that, let me see
if anybody else has a question as well.
Yeah, David, I was hoping that you could also show us the
application at some point, which I think these are very good to
see why you asked this question. I mean, yes, it's more
That's great. Let me let me zoom out to the application. So
so the the neat thing is that just like we could turn off
watermarks, we can turn on and off things in image generation.
So for example, if I find all the neurons that correlate with
trees, and I turn them off, you can see what happens. I'm going
to turn them off sort of one at a time here. And so originally,
the image will just be generated this. But if I turn off some
tree neurons, you can see that we can actually remove the trees
from the scene. And the cool thing is that this is different
from Photoshop. If you went and you tried to erase trees from an
image, then you'd have this puzzle of what would happen
about stuff that was occluded by the trees like what's going on
behind there. And so this image generator is actually it's got
this latent model that has an understanding of what the scene
is. And so, and even has an understanding of things that is
not explicitly drawing. So if you remove the trees from the scene,
then it'll come up with a reasonable looking, you know,
image to draw what was behind the trees, or you can do the
opposite. Which is you can take neurons that were not originally
on in a generated scene and turn them on. So if I take a set of
neurons that correlate with doors, and I turn them on in a
certain location, and you can see what happens in the generated
image, you know, I'll get this door in the scene, not only
will it just be a door, but it'll be it'll have like an
appropriate size and your orientation and style for for
the building that it's in. So if I take exactly the same neurons,
and I activate them in a different location like here in
this building, then even though it's exactly the same neurons
exactly the same activation that I've done, I get a different
door that is like a much smaller has a different style and so on.
It's appropriate to the building that it's in. If I if I try to
put a door in a place that would make sense, like by turning on
neurons up in the sky, then it like will like not do anything.
This is this is the actual output of what happens if I turn on
the exact same neurons up in this location. So there's a lot of
interesting context sensitivity that you can measure. But one of
the cool things that you can do is you can actually hook this up
to a paintbrush user interface, like I can find neurons that
correlate with domes or doors or things. And if I want to add
doors to a building, I can just sort of paint them on. And the
doors will show up and you can see the orientation of the doors
is appropriate to the wall that you put them in. If I just say I
want trees, it'll put trunks and leaves, you know, in the right
place in the trees or plants with a plant, plant them on the
ground. If I take grass and I can turn the grass neurons off and
remove grass from the scene and it'll come up with what the
scene should look like instead. And so I can kind of do these
semantic manipulations directly. Oh, here we're turning on domes
and you can see it will turn the top of the the church from a
spiral to a dome, but it also sort of stitched the dome into
place to make it look good here. I'm removing grass again. We
can like put a door in the scene. And if I if I put you know,
sort of put a door in the wall, then it'll it'll come up with
like the appropriate location and style orientation for the door
even if I draw very roughly. So when I'm drawing, every time I
touch the surface here, what I'm really doing is I'm just turning
on a few neurons. And and I'm letting the the math of the
GAN generator deal with all of the the details of how to arrange
the actual pixel. So does that does that sort of give you a
sense? Does that answer your question for like, you know,
what kinds of things you can do with this by understanding what's
going on in the interior of the model? Maybe now I should stop
it. Oh, yes, go ahead. Are these different neurons for like
doors in different areas? No, no. So when I when you click on the
door button on the left, I am picking 20 neurons that are
the door neurons. So by doing the statistical analysis ahead of
time that I showed you earlier, I've identified 20 neurons that
correlate very strongly with the presence of doors. And when you
click on the button on the left, I am picking those neurons. Now,
it's a convolutional network. So there's this translation
and depends those neurons appear at every pixel. And so what you
can do is you can just turn on those neurons in random pixels
that you touch.
Changing where the neurons are, that was what I didn't
understand. Does that make sense? So, so because it's a
convolutional network, so it's actually it's it's like the neural
network is cloned at every location. It's the same neural
network that's being used to process every, you know, patch
or patch of pixels in the image. And, and so if I asked for a
door in a place that wouldn't really make sense, then it
won't put a door there. If I asked for a door in a place that
makes sense, it'll make a big intervention, it'll stick a big
door there, which you can see. So I could be very rough about
where I put a door and it'll like put it in the right place. So
that's that's the idea. So let me let me zoom around here, I'll
show you a couple other things that you can do. So now there's
some limitations to this. And I'll just show you some of the
techniques that you can use to get around the limitation. So one
of the problems is that, you know, we can do all this cool
editing, but we can do this editing of a randomly generated
image. And, and so, so when I posted this demo on on the web,
you know, an artist called me and said, Hey, you know, I love
how you can edit images, I can edit this image of a kitchen
here. But that's not the kitchen I want to edit, I want to edit
my own kitchen, right? Like here's a photo of my kitchen. And I
want to edit that one. And I had to explain to them, you know,
they said, Oh, can you just load into your demo? My my kitchen
instead of yours. And I had to explain, no, no, no, that's not
how GANs work. They're unconditional generators. You
know, you give it a random vector of 512 numbers. And it
decides what image to make. And then once it decides what image
to make, then you can edit it. And so I'm sorry, I can't edit
your kitchen. And so they were very disappointed by that
because they had all sorts of ideas of things they wanted to
do. And so now the problem is that, you know, the problem
could be solved if we could find the random vector, some random
vector that that output the kitchen image or the specific
real photo that I wanted. The problem is how do I find it 512
dimensional vectors as pretty big vector space. And and so I
don't know if my GAN can actually generate this image or not. So
one of the things you can do is you can just treat this as a as
an inversion problem. You can take the neural network and you
can learn how to run it backwards. Basically, you know,
think of the neural network as a function G, and you want to
learn G inverse. So you can treat that as another training
problem. And there's a bunch of tricks and I won't go into all
the tricks here. But but basically, the idea is that you
can actually find a Z that comes closest to generating your image
by by training and doing a couple other tricks. And you can
actually get a Z that will generate your image pretty
closely. But the thing that's a little bit sad is it also
reveals things that the network cannot do. So so this network
is capable of generating this image that I'm showing you here.
But the original kitchen that I started with look like this. So
you can see what the differences are. I've lost a lot of
stuff. Right. So, you know, I can use the GAN to edit this
image. But this image is not exactly what I started with. And
so. So one of the pieces of science that that I did is I
asked a question, you know, is there some way that we can
actually make this work? Can we actually, you know, get the
network to output a real photo that that the user gave us? We
get the network to output this sort of simplified version of
it. It turns out that if I modify the weights of the
network, I can actually fine tune the network to get it so that
a very, very nearby network with weights that are almost the
same as the original actually hits this target image. Exactly.
And so so there's a bunch of details in the right way of
doing this. But it turns out that, you know, you don't actually
have to change much if you change the fine grained weights
of a network. You can you can change a lot of the details of
what images actually get generated. And and and if you
are given a target image to get you can actually tweak tweak any
network to generate exactly that target image if we want. And
so
so you know, so yeah, we can get all the objects back. But the
new thing is we haven't really changed the network much. So we
can still do editing. So like if we take the window correlated
neurons, we can take our modified network, we can turn them
on. And and now we can like add a window. Let's see if we show
that. Yeah, so this here's outlook. So we get this nice
window here. And the scene is began is doing its cool tricks of
orienting the window properly, doing some reasonable things. And
it has some really interesting effects that are non trivial
here. Some of them are good and some are bad. So for example,
all I did was turn on the neurons in this location saying I
want windows. And it did it. But look what else it did. It also
added these reflections right here on the counter. And so this
this kitchen guy does this a lot like adds adds non local
reflections where it thinks that there's a shiny table. And so
the cool thing here is that after I did all the inversion and
stuff, this guy actually thinks that there's a shiny table here
and it's right. And it thinks that if I add a window here,
they should add reflection. That's right. Also, but look
what else happened up here. See this lamp up here. When I first
lifted this in low resolution, I thought, Oh, maybe it turned off
the lamp because once you have windows, you don't need the light
on. But no, it didn't do that. It just messed up the lamp. It's
just total it took this whole area up here and just and just
distorted it badly. And so so that that's a little dissatisfying.
It means that this fine tuning thing, where we get again to,
you know, target a specific user image, when I do when I try to
teach it all the details, I'm not really teaching it what the lamp
was, I was just sort of showing it how to arrange the pixels. And
again, made its best guess on how to generalize how the image
should look differently. If I change something like out of
window, but with only one example of a lamp that looks like
this, it generalized wrong, it has no idea what should happen to
that lamp when I when I add a window. So this is this question
of like how to make changes in a network with with with
achieving good generalization is, which is a good question. And
it was, there was something that puzzled me for a year after
doing this work. But but the work is still pretty cool, you can
still use it for modifying real photos. So here's like a photo of
I got off of Wikipedia of like some real locations. And you can
you can edit them, I can add grass, I can add doors, I can add
domes, you know, just like, just like the the the other
campaign app, except I can actually start with a real photo
that you give me. And I can invert that photo through the
network, get a good starting image, fine tune the network to
make it make it output, you know, the target image and edit that
image, add bigger domes, and it'll sort of match the
architectural style. And, and, and, you know, do different things
like that, I can add domes, remove domes, add doors, you
know, things like that. Let me see if I can get this video here
to show. So this is the status center. Let's add some doors
here. So you get the idea, I'm doing exactly the same
intervention that I did before. And it's it's opinion just like
before, it will not add doors in places that it doesn't think are
not good places for a door, it has some opinions about where
doors are allowed, it likes to put them in brick walls. It
thinks it's okay to put a door in a tower, like that architectural
detail. Oh, I put domes here. It's happy to put domes on top of
buildings. It's not happy to put a dome like in the middle of the
sky. It's not happy to put a door in the middle of the sky. But
you know, it put trees in different places. And, and so
there are things that it understands, there are things
that it doesn't understand very well, it's sort of making a
guess of what the structure of the image is. It doesn't know what
to make of my advisor, you know, sort of planting grass in front
of him. And that's not very realistic. But you kind of get
a feel for what the structure and knowledge of the model is by
doing these kind of interventions. So this was really
cool. I think it got a lot of people's attention. Adobe
noticed this stuff, and has been busy trying to make different
painting applications using, you know, GAN technology that are
I think partially inspired by by by this kind of discovery. So
David, I have a question. Yep. This is really cool. Question is,
when you modify, for instance, churches, I assume you have
trained your GAN on a church data set. Yes, that's correct.
What about when you do it on the real images, for instance, in
this case, you know, your advisor? Yes. So actually, both of
these are using the church data set as well. So the church
data set,
interesting that even you have trained again on church, you
can depict a person.
Yes. So this is so the GAN. Now, you have to keep in mind that
what I've done here is I fine tuned the GAN. So you can
actually, you know, you can actually get you can actually
get a GAN to do a lot of things by fine tuning it. So I've I've
told the GAN, please basically overfit on this target image. So
the GAN, you know, has 30 million parameters. And, and you
know, an image only has, you know, 10,000 pixels, and it has
plenty of excess capacity to memorize the details that I
might want to do. And so what I've done is this as I've taken
the image, I've asked again, through my inversion techniques,
what is the closest church image that you can generate that
looks like my thing. And you get a different image. I don't
have the image to show you here, but you get an image that looks
kind of more church like it's a little bit, it'll be
architectural have the right kind of shape, the kind of right
textures. But you know, it won't show my advisor here and
things like that. It'll be, it'll be this rough approximation
for that my my image, but that is in the domain of what the GAN
can actually generate. Then I say, Okay, that's not what I
want to do. I want to actually edit this photo. So let's fine
tune that network so that so that given that same Z instead of
generating the church that you would normally generate, I want
you to generate this image, change the weight slightly, get it
so that that Z targets this. And, and so that's what I've done
here. But I've tried I've done that in a way where I try not to
change the weights too much. I just try to change the weights. I
change the fine grained layers. And I don't change the coarse
grain layers. And I, and I have a regularizer to make sure the
weights don't change too much. And that you are changing the
pre trained weights, or you are putting some extra weights, and
then you place them. Oh, here, I'm actually changing the pre
trained weights. So the network has 15 layers. I'm actually
going and I'm changing some of those layers. I'm not adding
anything new to the network. I'm just changing the weights in
the network itself. Now, now what I've done here is I've
overfit the network to this one image. The network is not
generalizing this knowledge. So for example, you can draw
Antonio in this one image. But if I look in the network, if I
probe it a lot and see, can it ever generate Antonio in a
different setting in a different image? It cannot. In
fact, you know, as much as we probe things, it really doesn't
look like we've changed the output of the network in any
meaningful way for any image, except for this one. It's almost
like, you know, the network generates this really complicated
manifold of realistic images. And we've told we've picked up one
point of the manifold, and we've dragged it over to pass to
this point. But we've done it in a very local way. So it's
really not affected any other points of what the GAN is
generating. And so so but but for the purposes of doing this
kind of application, it doesn't matter that it's not
generalizing because the user doesn't care about a different
photo, they just care about their own photo. So it's a pretty
cool. It's pretty cool technique anyway, even though it's sort
of not the classical goal of machine learning. Does that make
sense? Yeah, it does. And I wonder if the user has more
images of themselves with that over time, and make the network
even better in generation?
Yes, this is the big question. And I played with this for many
months, and I haven't got it to work. And if anybody can figure
out how to get to work, I feel like it's one of the holy grails
of like how to add a new thing to a generator. So like, the
generator knows about all these things that knows about trees
and knows about all these architectural pieces, you know. But
what if I came along with something new? What if I was
what if my what if I what if I work for GM and I want to sell
Cadillacs, then I then I might come to one of these models and
say, you know what, you should draw cars. In fact, I want you to
draw specific cars. I want you to draw Cadillacs in front of all
these buildings. How would I add Cadillacs to my model or add
Antonio to my model or something like that? And we don't know how
to do that yet. Although I'm going to show you a little bit of
work, where we can do something that's very similar. And if I
don't know if I have time to, to go over this, but I'm going to
I'm going to zoom through this because I'm so excited by this
work. So, so, so it's motivated by this, this sort of question,
which is, you know, we have a model of like drawing towers,
let's say, right? But there are things in the world that we might
want to model that we don't have a data set for. For example, you
know, in in in Decatur County, Illinois, there's this courthouse
that has a tree growing out the top of the tower. It started
growing out there by accident, but the people in the town love
it. And so but it's but there's no so like if I want to get a
generative model to draw trees growing out of tops of towers, I
can't do that in a classical way because I can't create a big
data set of a million buildings that have trees growing on the
top of the towers, because they don't exist. It's just this one.
And so now if if the point is I want to generate images of this
type, you know, well, I could use a regular image editor, I
can take any building of a tower, and of course, I can stick a
tree on it, right? I could use my, you know, again, painting
method to, you know, activate tree neurons or something like
that. But no, no, that's not what I'm asking. I'm asking this
other question of like, how can we stick tree towers into my
model? How do I modify the model to have this new concept in it?
Like I start with this model that has all these weights that
encode all these rules for how buildings look and things like
that. And I want to create a new model that has new weights that
encode new rules. So for example, the old model could generate
all these buildings that of towers that look normal have
spires, you know, pointy tops. And I want to make a new model
that has weights, they encode a different rule, so that like,
they have trees growing out the top, right, or any rule that I
choose, right? And it turns out that this is actually possible.
So this is different from the technique that I showed you
before, because in this technique, it's actually
generalizing. This is, you know, if you use this technique, not
only you change the output of one image of the GAN to have like
some effect, but we can actually change the outputs for a whole
class of, you know, a large subset of the outputs of the GANs to
follow a different rule, like any pointy tower output will have
trees instead of pointy towers. And so so I'll just show you a
little bit of like the interaction here of what it
looks like when you get our method into an application. So I
let's see if I can get this to work. So here, what I'm showing
you is the output of a style GAN be to generating churches, you
can kind of, and there are three parts of this UI, there's an
image viewer, then what you do is you can select a rule that you
want to change, and then you can specify how you want to change
your rule. So there's three parts of this little user
interface. And I'll just show you sort of how how the effect
looks by showing you one of the interactions. So you can kind of
use the image viewer to scroll through lots of examples of of
what the the generator is capable of generating. And then we
can go to these examples and we can say, Hey, you know what I'm
really interested in? I'm interested in this rule of how
to generate pointy towers. And so I can select a few pointy
towers. And you can think of this as what I'm looking for is
the neurons that are responsible for the shape. And so I can
select a few examples and I can say, Hey, what other, what other
outputs of the GAN share the same representation? And, and it'll
show me, Oh, yes, the GAN is generalizing this way, these
other pointy towers are represented the same way as the
ones that you chose. And then I can go and I can say, All right,
I want to redefine how these pointy towers are rendered by
this generator, I want them to be rendered like this tree here.
So I can copy the tree from one output of the generator, and I
can paste it into where I would like that tree to show up. I
wanted to show up instead of pointy towers. And then I can
say, Okay, now insert this new rule into the model, compute
what the right changes to change the model. And then after I do
that, that takes about a second to do the math to figure out how
to change a rule. And then after I do that, then I get the GAN to
generate new images. And, and they look like this, you know,
like the tops of the towers, now have trees on them instead. So
you can see how that looks. And it's not just affecting that
one image, it's affecting all the pointy tower images. I can do a
little search for more pointy tower images. And, and do I have
that here in my thing? Yeah, so here's a search for more pointy
tower images. And you can see they, you know, they all have
gotten these trees sprouting out the top of it, like some sort
of dystopian tree world where vegetation is taking over the
planet. And, and so you can do this in a bunch of things, I'm
gonna skip over some of the technical things here, or some
of the other examples of what you can do here. You can edit
reflections and things like that. I've got other videos that you
can look for on the internet. But I wanted to show you a sense
for what we're doing inside when we do this kind of thing. So
like I showed you before that again, has is like, got all these
convolutional layers stacked up, it's about 15 layers. And what
what, what, what the discovery was that led to this application
was that each one of those layers can be thought of as
solving a very simple, separate problem from the other layers.
And what is that simple problem? It, it can be treated like a
memory, where the layer is solving this problem of matching
key value pairs that it's memorized. So every location
has a feature vector that you can think of as a key. And what
and the key each key like, you know, represents a certain
type of context, like, you know, the middles of towers or the
tops of towers or something like that. And what you can think
of the map as as as storing is what should be what is like the
pattern of features that should be rendered whenever that
context comes up. Right. So you can think of it as just
basically key value store. And and so so this whole idea of
using a matrix as a key value stores and it's like the oldest
idea in neural networks. People observe back in the 1970s,
that if you have a single layer neural network, you can treat
it as a as an approximate key value store that remembers keys
with minimal error. And and so if you had a set of keys and a
set of values you want to store, and you ask what is the
optimal single layer neural network that you'd use to store
it. It's actually, you know, classical linear algebra, it's
like the solution to a least squares problem. So what we can
hypothesize is that in these very, very fancy, you know, 2020,
you know, 50 years later, deep neural networks, actually, each
layer is just acting as one of these. Now, which keys are being
stored and what values were being stored? We don't know. But
we could hypothesize that there is some set of things that are
being memorized, some set of keys and values. And so that that
maybe this weight matrix that we have is the solution to the
assembly squares problem. So the cool thing that we can do is we
can say we can ask the question, what would the weight matrix
look like if we changed one of the rules? What if we had one new
key value pair that we wanted to change? Then what would the
weight matrix be? Instead, we want all the other things that
the network has memorized to still be memorized with minimal
error, just as before, except we're going to give this new
constraint, we want to write a new key value pair into it. And it
turns out that that's also least squares problems and
constrained least squares problem, we can write down the
solution in this form. And the cool thing about these two, the
squares problems is that they cancel each other out. Most of
the terms are the same. And, and, and we can actually ask the
question, how would the weights have to change if we add a new
key value pair, without knowing which values were written into
the network before, we don't actually have to know what the
old key value pairs were, we can just assume that the network
was optimal as storing all these key value pairs. And, and the
math for like how to write a new key value pair comes out the
same anyway. So, so that's there's there's a little bit of a
mathematical insight and trick here. But what it allows us to do
is it allows us to find exactly what we want to do to change one
thing that the network is memorized, you do this rank one
update in a specific direction. And, and you can take a key and
change it to any value you want. And that will, you know, the
same form will minimize error for, for other keys, regardless of
what value we write, it's almost like it really is a form of
memory, that we're changing. So our method is basically you
find a key by asking the user to select a few contexts that look
the same, we average them to get a good key. Then we ask for a
copy paste example to get a goal. That's the new value that we
want to write into the key of the memory. And then we do this
math to, to find how to change w in the direction of the key
only, we find a rank one update that does this. And so, and so
that avoids changing other rules. So we can do this on a bunch
of different GAN models. And, and so you can see, like, you
know, people like to change people's expressions here. So
what we're doing is a little different from what you normally
do to change people's expressions. And again, what
we're doing is we're actually going to rewrite the GAN. So it
only outputs people who are smiling, we're going to take all
the frowns, we're saying, okay, there's, there's a rule for
frowns, we're going to change that to a rule for smiles by
showing an example. And so by patching frowns to smiles, now
we have a model that just outputs people who are smiling. Now
we live in a happy world. So that's, that's, that's pretty
cool. And now, of course, we could have done that by, you
know, changing the training set by collecting only training
data of people who are smiling. But the neat thing is that you
can also do this for things where you don't have a training
set that looks like it. So for example, there's a, there's a
rule in the model for how eyebrows should look on kids. So
you can see that kids have these very wispy light eyebrows that
don't have much hair. So we can find that rule by identifying a
few examples that gives us a rank one direction in the weight
matrix. And then we can redefine it, we can write a new thing
into it and say, you know what, we want the eyebrows to look
like this, like that's very bushy much sash. And, you know,
paste into one example, do the math. And then now we can change
weights in a way that generalizes. So now all the kids had
these very bushy, you know, eyebrows. And it's something
that we wouldn't have been able to get by collecting training
set because we don't have kids that look like this in the real
world. It's something that just comes out of our imagination. So
this is kind of the thing. I kind of feel like this is the big
reason why, why, why be interested in how these models
are working inside. And the reason to be so interested in it is
because as long as we don't look inside our models, then we're
really constrained. Because the only thing that our models can
really do is imitate the real world. We can collect huge
amounts of data. And the models that we create, we'll just get
better and better at imitating the way that the data is the way
the world is today. And I kind of feel like it goes a little bit
against why I was interested in computer science years ago when I
entered it in the first place. Because the amazing thing about
computer science is that you can use it to create algorithms
that represent things in the world that don't exist yet,
things that you can only imagine. And so machine learning is
sort of on this path right now, where we're getting very, very
good at replicating the way the world is. And we're going to be
confronted with this question of how do we use these techniques
to actually create new worlds that don't exist yet that are the
way that we want them to be. And I think that this really
going to require us to not just get models that are just really
good at imitating, but also models that are understandable to
people so that we can change their rules inside, and then use
them to create things that are based on our imagination instead
of just the training data. And so here's a fun thing here, I
think, if I want to be fair to the horses, you notice that none
of the horses in this horse generating GAN get to wear hats
even though all the people get to wear hats. So we can change
that by taking a hat from a person and inserting it into our
GAN's model of what a horse's head should look like. And now
horses get to wear hats, right? And so, so let's build a better
world. And, and allow people to change the rules of the world by
making the rules more visible and and manipulatable by humans.
That's that's sort of the goal of the whole thing. So any
questions? Any questions?
I have a question. Yes.
Does this method work with multiple different models? Or is
it only successful when like, taking a hat from within this
model and put it on a horse?
So right now, this, this method is only able to take it, it's
it's only able to rewire one model. So I can take one part of a
model and rewire it to a different model, you're sort of
asking the transplant question. So I'm sort of at the point
where, you know, it's like a surgeon, I can like connect one
blood vessel to another blood vessel in the same human, right?
And you're sort of asking the question, well, can I do a heart
transplant? Can I take a heart out of one person put another one?
And I cannot do that yet. It turns out to be harder. And, but I
but it is a it is an obvious goal. And I, and I feel confident
that if we understand well enough, all the things that make
these computations work, what is needed for the care and
feeding of a computational module? What is a computational
module inside a big learning system? Then we should, you
know, it should be a goal to be able to move a piece of
computation from one neural network to another one. Does that
make sense?
Yes, thank you.
Yep. That's a really great question, by the way. I think
it's, I think it's fundamental. Any other questions?
This is not too well articulated question. I was just
curious what you, what are your thoughts about this? I think
this is this like neural nets have tendency to like avoid the
responsibility of the results, like everything is done in the
hidden layers and sort of shrug off shrug off the
responsibility about the results. And I thought it was like
interesting how you set the objective towards something as
abstract as realistic. And here, like how you define the concept
of being realistic is based on the big data you collected from
the web, but but oftentimes some like fake images sometimes
look even more realistic than real images. And I don't know,
like tree growing on top of the building may look fairly
realistic for some people, but maybe for plant experts, maybe
it would not. Right. So I don't know, like, I think this might
result in like the blurring between the it's making us hard
to distinguish between the real and the fake or something like
that. I don't know.
Yes, yes. No, I think that there are so so the we're we're
unaccustomed to making it easy for making programs that make
such realistic renderings of the world. And it's actually a
concern. I think that, you know, people have misused this
technology already that we you know, we use we you know, there's
the whole deep fakes phenomenon. But even without like faking
videos, people people have you know, used face generators to
make lots of fake Facebook profiles and things like that,
you know, pretending there are millions of people that exist
that don't actually exist and things like that. So so even
before you sort of do manipulations of the world, I
think that there's already this problem of of of, you know,
pretending that there's a lot of data that there actually isn't
by using these generator models. And so I think that there's
you know, the whole the whole question of fakes is a very
serious question, like how do we how do we function society if
we don't know what's real and what's fake. Now, it's not a
totally new issue. You don't need a state of the art deep
learning model to make fake, you know, people have made fake
photoshopped by hand forever, people write can write text that
has all sorts of lies forever. In fact, that's probably more
effective than you know, trying to train a deep learning model
and, you know, sort of make it work. But I think it's I think
it's a, you know, it's still an important question because the
easier we make it to make fakes, you start to get issues like a
scalable fakes, where it's not just one, one photo that is a lie
or one article is lie, you could generate millions. And I think
that there are serious issues with that. So there's some pretty
interesting work in forensics for detecting fakes, and things
like that. That I think is important to invest in as well
as as we as we advance the state of the art and this kind of
thing. So I so so I don't want to minimize the implications of
this type of thing. I think that for the type of work that I'm
doing, I think that you observe that the tree kind of looks
realistic, it's not super realistic. You know, if you're a
plant expert, it's just sort of, you know, sort of there. I
think the same thing with hats, they don't really super look
like hats. And so I think that we're, we're sort of the stage
where they're really exciting where the implication of what
I've done here, I think is the idea that the, you know, learning
how these models are working inside, by understanding what
the internal structure of the models is, is really the, that
the the exciting part that that it's starting to give a little
insight on how we might untangle and disassemble what the
internal logic is, that is being learned by these, these deep
networks. And, and I'm actually, I feel like this is, I feel
like there's a different issue other than fakes, which is
actually has some ethical implications, which is transparency
of deep networks. Because one thing that they're not really
good at doing is when you have a deep network do something
amazing, they're really not good at answering the question, why?
Why did you do that? Why did you choose to render it this way?
Why did you choose to pick these objects to put in the scene?
Or why did you choose to deny me some credit or to, you know, to
make some other decision that we were at, you know, depending on
neural networks to do. And I think that if we can understand
how to disassemble the rules that are being applied inside the
network for it to make its decision, then I think that we'll
will be, we'll have a way of asking why. And by looking at
the computation directly. So that's my, that's one of my
goals and one of my hopes in doing this kind of work.
Definitely, I can see some of the worst of you about like,
about the transparency of the neural network, especially when
you, when you show the example where you detected a single
neuron that contributes to the watermark thing, I think that
it was really interesting.
Yeah, I think so too. I was surprised that it worked because
we normally think of neural X is very, very, very opaque.
I also have a small question regarding artifacts. So I think
in the beginning, you talked about how you segmented the
network with like masks that were classified before by mapping
neurons and beginning layers, which create things. But could
like, can that be also used to figure out where artifacts or
anomalies are generated to make gains better?
Yeah, actually, I don't have a picture of it here. But in my
work where I was looking for neurons, originally, it's called
the paper is called GAN dissection, you can you can
Google for it. And, and I showed that in that paper, we
analyze some of the pre trained GANs that came from a previous
work from NVIDIA called progressive GAN, we analyzed
some of the pre trained models, and we found that they actually
are neurons that correlate with bad looking artifacts in a
scene. And if you turn those neurons off, you can actually
not only improve the quality of the output of the GAN, just
qualitatively like you can get these artifacts to not show up
but using standard measures of GAN, you know, statistical
measures of GAN image fidelity at large scale. By removing these
neurons, you can actually improve the what we call the FID
scores of these GANs when we tested on like 50,000 images. And
so, so that's actually very weird to me, that's, it was a big
surprise. Because, because we, we train these things using, you
know, powerful optimization techniques, using, you know,
billions of floating point operations, you know, training
these things on big expensive GPUs for a long period of time. And
the idea that a human can come along, and do a simple looking
visualization, pick out a few neurons based on things that
don't look good. And improve the model by turning those neurons
off. It was like it shouldn't be possible, right? If it was so
easy to improve the model that way, why couldn't the optimizer
find it? And so, so I think that was, that was, that was pretty
interesting. I have not repeated that experiment on the latest
GANs, which are actually much better the style GANs. To
architecture, they went back and they analyzed a bunch of the
artifacts that show up in this, this family of GANs. And they,
they found that there are certain learning methods that they
can do to remove the artifacts or reduce them somewhat. And so I
don't know if a human can still beat the current generation of
GANs, it'd be worth going back and seeing that phenomenon is
still there.
That's pretty cool. Thank you. Yep.
Okay, excellent. Thank you so much, David. It was really
fascinating topic and talk and more interesting to me, asking
the right questions, asking questions and learning to ask
the right questions. It's really interesting. And I think that
it opened paths to many of us.
Excellent. Hey, thank you for the opportunity to talk to the
group here today. I always enjoy the, the chance to interact
with folks about this. If anybody wants to send other questions
about it, of course, you can always send me a note. And, you
know, I love this stuff.
Yeah, definitely. I think that it would be great to follow
follow your work on your GitHub and your website, and
especially for students who play with the tools that you have,
so they have them get an understanding of how these two
work and make them curious about the work.
Cool.
Excellent. Thank you so much.
Thank you, Ali. Thank you, everybody.
Thank you. Bye now.
