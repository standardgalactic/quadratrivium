Hello, everyone. Welcome back to your course Learning for Arts,
Aesthetics, and Creativity. Our specialist speaker, Tom
White, is here today to tell us about his exploration in art
and creativity in AI. He is an artist who actually creates
art with computers and AI, and I should let him to tell us more
about his background. But he has done very interesting and
fascinating work. And I think that he also has some new
galleries about what he has created his journey of arts. So
please, Tom, let us know what you are up to, what you're doing.
Sure. Thank you, Ali. I really appreciate you letting me
participate. I've often turned down talking invitations, but I'm
an MIT alum. I really appreciate, I like your research,
and so I appreciate your invitation. But I also really
like IEP. When I look back at MIT, that was like the IEP courses
that I took were really great. And so I can tell your students
that you might not remember everything you're doing at MIT,
but I'm pretty sure you're going to remember some of your great
IEP courses. So my background is fairly eclectic, as you might
imagine. I was undergraduate in math, and I went to the media
lab about 20 years ago at MIT. And I was part of a group that
was doing sort of incorporating graphic design into programming.
This was sort of exploring, exploring that space. And so
I'll talk a little bit in my talk about about some of the
precedents that we did there. But I was coming into that sort of
looking at ways that people could think more about coding as a
creative discipline. There's a lot of tools now that didn't
exist in, and there's even the idea of creative coding. And a
lot of that came out of the work that we did in our group. After
leaving MIT, I sort of went off into industry for many years. I
was always interested in AI and art and the sort of intersection
of those. But it was kind of the AI winner. And it was, you know,
I got out of school, went into industry, but when deep learning
started getting exciting, you know, maybe about five to 10 years
ago, that's when I re-vectored back into space. And so I found
I stuff back in academia and have a wearable hat. So I, you know,
I have my own research students here, and I'll speak a little bit
of my talk about some of the research we do. So there's like
Rebecca was saying, there's a lot of number of practical tools
you can build that will help other people use the medium
creatively. And I do some of that research. But as you alluded
to on the side, I also have my own separate arts practice. And
most of my talk is going to be about that. Hopefully that'll
be a good, I think everyone's art practice is different. But I
think seeing seeing that might be hopefully inspirational for
some students that are wondering what that's about or how they
might one day get into something like that.
Yeah, definitely. That would be actually very great. I think
that maybe, yeah, I think you and also work off the work that
Rebecca details give us some taste about what it really means
from the art practitioners as well as, you know, the computer
scientists. So please go ahead.
Okay, I will try to share my slides and we'll get started. Okay,
so I will briefly just reintroduce myself for the video. So my
name is Tom White. You can find me online on my handle trip
net. And there's my website trip.net. And as I said, I'm an
artist and lecturer at Victoria University of Wellington School
of Design. And in my work here, I teach classes on creative
coding. So I teach primarily in the context of an art and
design school to students that are interested in, it might be
that they're interested in special effects, or that they're
interested in web design, but they want to sort of incorporate
programming into their into their tool set, because there's a
lot of capabilities and things that have a lot. So in the past,
where someone might, you know, learn charcoal drawing or
search specific techniques, more and more today, students want to
learn programming. So I teach programming, but not computer
science angle, kind of creative coding angle. I also have
research which I do with my graduate students. And I'll touch
upon that briefly in the talk as well, where we make creative
tools using some of these, these technologies and in that
context, I also have a workshop at the NeurIPS conference
creativity and design that I do with other co-hosts, where we
post a lot of this research. What my talk today is about is
about a lot of my artwork, and specifically the artwork that I've
been doing the last three to five years. And I'm going to sort
of go through and give you a background of myself and then
talk about that a little bit. Let's see if I can get my first
slide. There we go. So, so this is the outline of the talk. I'm
going to give basically a three minute version of the talk, the
TLDR, like what's the point of this talk, so you can sort of
understand it all at once what this is. Then I'm going to back
up and give you a little bit more background about where I'm
coming from, my background in MIT, and since then, I'm going to
spend a little bit of time talking about the precedents at
art. So when you're doing artwork, similar to, you know, if
you're writing a research paper, you have your references at the
end, and that contextualizes and talks about how you're
building on things in the past, I think it's important. In
art, I think a lot of times it's implicit to be here, I'm
going to make it a little bit more explicit about what my
precedents are, and what I find inspirational, and what I'm
trying to, what I'm using as reference points. Then I'm going
to get into the meat of the talks. The meat of the talk is
really, or the core of my talk here is about AI representation
and abstraction, and specifically, my investigations
into kind of exploring what these neural net vision systems,
what their representations are, and how to convey that in an
artistic context. I will talk a little bit about other AI art
approaches, including, you know, other peers that I have that
are in this space, and I'll touch a little bit about my
research there. And then I'll close out by talking about the
sort of like some of the implications of the artwork. So
what is, what happens when the systems, when we're using these
systems more and more as our kind of auxiliary eyes? So that's
the outline of the talk. This is the core of it. So this is the
sort of the summary that my ideas of our artwork is that
machines have their own way of seeing. So they, they see things
they're very, they're very accurate in classifiers. But I
think they also, importantly, have slightly different ways of
seeing the world. And so what I'm trying to do in my artwork is
trying to expose that to a wider audience and trying to
investigate how it is that these machines see the world and how
it might be the same or different as us. So as a sort of a
corollary of that, because they see things differently, I'm, I
believe we can create or this kind of core and buy machines. In
other words, you could use the machine perception to create
different types of art, which actually the machines themselves
have some opinions about. So I'll talk a little bit more about
what that means. And so the summary there is that through art,
we can actually appreciate ways the machines can perceive the
world. So similar to you might encounter a new culture that you
hadn't before, and you might explore the art of that culture.
I'm trying to explore the art that's created when we introduce
machine perception into into the process. So I'm very, so the
one line version of that is I'm interested in how machines read
images. So that's kind of the point of what I'm doing. Here's
three prints that I've done the one on the left is an eye and
that was created from a data set of eyes and I'll talk a little
bit about that later. The one all the way on the right is it's
actually a face and that was made not so much for a class
environment for facial recognition system. The first step in
facial recognition is a detector. And so that was made to look
like a face to a face detection system. And the one in the
center here, which I'll make a little bit bigger is a screen
print of two chickens. And so this is all of these are based on
pure images. And I'll get as you can imagine more into the
details of how this happens as I go through the paper or go
through the talk. But this is based on a data set of many
actual images of chickens, the system created this this output.
And we can actually turn it around and we can show this to
the system again and look at its imagination of what it
visualizes. And we get kind of this so it can actually introduce
diagnostics into the process and say, Well, what is it that the
computer sees when they look at this print? And that is a
some kind of indication of the richer inner world, these neural
networks. So that is my, I guess, you know, three minute version
of the whole talk. And now we're going to sort of back up and go
through that and a little bit more, a little bit more detail.
And I'll start off talking a little bit more about myself. So
this is made in my studio, currently, where I do my
printing. And I'm here in Wellington, New Zealand, I'm
going to go back and talk about not everything I've done, but
maybe some of course, precedence along the way for getting to
sort of just this stage of my art making.
25 years ago at SIGGRAPH, I did in sort of using machine
learning techniques of the time artificial evolution to make
these real time video filters that processed video and then
showed that on a screen. And you could also manipulate the
evolution process to create different filters.
In 1998, I finished my master's thesis at MIT. And so, like I
said, I was in a group at MIT, John Midas Aesthetics and
Computation Group. I was there with many kind of inspired
people that were coming in from both design and computer
programming, software engineering. And I was interested
at the time on better human computer human interfaces. So I
built a custom hand interface because I was interested in
multi-touch. So multi-touch wasn't really a big idea in the
90s. So I had to, there wasn't any interfaces that did that. So I
built a camera optics based interface. So it's this liquid
pad. And when you pushed on it, you would get this handprint
that you see in the bottom right. And then on top of that, I
built various interfaces, for example, sliding left to right
to move, to move something, or there's various very different
ways you could you could interact based on having more than a
mouse. So I think this is common sort of ideas now, but this
was something that we did in the Media Lab and I did for my
thesis. But maybe one of the, one of the one of the other
things at this time, as I did another art project, which is
still pretty well known, which is called Stream of
Consciousness, which in which I incorporated this handpad, which
could measure sort of pressure across the whole pad into this
interactive exhibit where words were flowing through a stream,
and it was called Stream of Consciousness. One sort of
interesting footnote about this is that as the words were going
through, you could push on a word, and it would spring out
related words. So if you were looking at the word, you know,
university, and you pushed on it, it might say faculty or
school, or academia or something like that. And we used
wordnet at the time, that was part of the programming idea to
kind of find out interesting related words, first anonyms and
synonyms and kind of its sense that that it uses, which is the
basis for ImageNet, which is a lot of the artwork I do today. So
this was kind of a, again, using AI techniques at the time,
incorporating it in these, these interactive exhibitions. But
one of the, one of the interesting legacies, and I think
another bit that feeds into my, my current practice is that one
of the interesting legacies of this group is that we made a lot
of interesting sort of core software that enabled a lot of
people to do different things. So there was a handful of
researchers, and I, and Ben Fry, and Jared Chiffman were
students in the group, and we're responsible for creating and
maintaining this core library called ACU. And the library was
so that basically we could conduct our experiments. And it was
constructed such that it was easy to do kind of a sketch, a
software sketch. And the reason I bring this up is that this
software was the basis for other systems that have come since.
So Ben, who worked on this and Casey later adapted some of
these ideas and made a version of this called Processing, which
is actually quite popular still as a creative toolkit. And then
later, another group of students took actually the code for
this and made a new toolkit called Open Frameworks, which
uses a lot of a lot of some of the code in this ends up in Open
Frameworks. And then they, of course, built upon it and built
a community around it. But many of the conventions and cliches of
this programming style kind of live on today. And this is, it's
interesting as it alludes to my current work, because what we
were doing at the time, we were just trying to figure out how
to make it easy for people to create sketches or drawings on
the computer. And I feel that that's a lot of what I'm doing
currently is just my audience is no longer other people, it's
actually machine learning processes. And then I did other
things and worked in industry, but for the purpose of this talk,
we'll just, you know, chalk that off to the AI winner. And I'll
come back to that in a little bit. So there's, after that, I
graduated from school, I did some other things. I'm going to
also open the chat over here in case other people had feedback
that I can secretly see. Okay, so now I'm going to talk about
precedence and art and and some of the background looking at it
from a slightly different angle of what it is I'm trying to do.
I'm going to go all the way back to 1927. And there's an artist
called Stuart Davis that I admired his technique. And what he
was doing, or what he did for kind of his central core
inspiration for his work is that there was one year that he did
the egg beater series. And the egg beater series was his attempt
to look at the world in a new way. And the way he did that, his
technique for that was he took some common everyday objects, he
took an electric fan, a rubber glove and an egg beater. And he
nailed those to a tabletop and basically forced himself to paint
those things over and over and over until he thought he was
doing something different and new. His, in his own words, what
he said his intent was in doing this was to strip a subject
down to the real physical source of its stimulus. So here's one
of his egg beater works where he's, you know, taking these
common objects and trying to find something new in them. And I
think that that's some of the that alludes to sort of some of
the work that I'm doing. And I'll sort of touch on that in a
little bit. Another precedent for me was Harold Cohen, who, for
many years, experimented with generative drawing systems, and
is arguably the first kind of AI artist. He came at this from a
different angle. He is a very successful visual artist and
painter. And he decided in the early 70s that he wanted to go
into programming into an artificial intelligence to see if he
could codify some of his ideas on, on mark making in a, in a
formal way. So here's an example of one of his programs and he
initially set out to build an autonomous program. And he called
the program Aaron and Aaron would kind of encompass all the
ideas he had about how to generate visual works. So I
visited Harold a couple of times and this is me visiting him.
And when he's working on some of his later works, and I think
it's also interesting to discuss like, even though he had these
ideas that the the artworks were being creative autonomously
initially, later in life, he was working more as a collaborator
with these systems. So he was sort of working a little bit or
finding ways where it was more of a co creation process.
But across all of his, all of his different phases, his core
question across this, and this is what I kind of share his
inspiration is, what is what he his question was, what is an
image? And the way he didn't mean that like technically what is
an image, but he meant like, what is the minimum condition
under which a set of marks on a page functions as an image or
conveys meaningful information. This is a sketch he had from one
of his papers. And I would kind of rephrase this in a machine
learning context is saying, what is it that in what is it that
images could do kind of central to representation and
extraction, like what makes simplification work? How is it
that simple drawings can be evocative and seem to stand for
something else? So I think that this is kind of a core, a core
concern of Harold's that that I also share. And then one last
kind of precedent and art that I think is important for me is
just talking a little bit about pop artists. This is not so much
as a from the, the content side, but from the form side, this
is actually, I share some techniques with many of the
pop artists like Andy Warhol, Roy Lichtenstein, and how I
execute my artwork. So for anyone that's not familiar with
screen printing, if I'm making this chicken image over here on
the right, that's actually printed, but it's printed from
two layers of ink. And so these are essentially stencils that I
have to mix inks for, and then put onto the page. So what I'm
showing on the left are the masks that generate the
stencils to give you the image on the right. So the stencils
themselves get burned, transferred onto screens. These
are physical kind of stencil, it's like a stencil, except it
can have holes on it, there's a mesh there. And then you just
sort of pour ink across the, across this, this screen, and
you smash some ink through it. And that's the kind of physical
process of, of making these, these prints. Here's a little
slideshow I have where I'm starting off with a blank canvas.
It's actually a blank, we base coated a color onto it, we put
a screen down, I put some ink on top of it, I spread the ink out
that's called the flood, then you smash the ink through and you
get a layer. And then the more colors you want, the more layers
you have to do that. So for the second color, you have to do
another screen and another layer. For the third color, it's
another screen, that's me pushing ink through that layer. And
then there's a fourth color there. Put the ink on the screen,
spread it out and push it through. So just kind of
emphasizing that there's a kind of a, a specific technique I'm
using that has, you know, precedent for getting these
are acrylic on canvas works or acrylic on paper, but it's
instead of a brush technique, it's a screen printing technique.
And here's the result of some of that. So this is kind of just
showing you what happens at the end is that I have these prints,
and the prints end up kind of, you know, in an exhibition or in
some type of art context. So that's kind of the precedence, the
art precedence. I think it's useful kind of now to talk about
more what this artwork is and what it is I'm trying to say with
this with my art pieces. So I'm going to get into the sort of
the main section here, which is the AI representation and
abstraction, like how is it I actually create these, these
works and what is it that they stand for. So just going back
to revisiting Stuart Davis. So in my mind, Stuart Davis spent a
year basically learning how to perceive and represent familiar
objects in new ways. So he, you know, imagine going in every
day and staring at these same three or four objects of
rubber glove and electric fan until finally you're seeing
there have been ways you hadn't before, and then you're trying
to put that on the page. And my kind of core question is, is
can we similarly use computer vision to introduce new ways of
perceiving or representing familiar objects? So instead of
going in kind of going to this meditative trance, can we kind
of look at things through the eyes of the computer vision
systems and see if that causes us to see familiar things in new
ways. So appropriately enough, one of my first experiments in
this was an electric fan. So this is an electric fan training
set. So this is the input into the system where you have
computer vision systems, as you probably know, you know, need
hundreds of thousands of images. So I believe in the image net
electric fan data, there is, you know, over 1000 images of an
electric fan. And this is just quickly paging through some of
those. And then what I did with that is I made a system that
optimized perceptually for creating something that looked
like an electric fan to multiple, multiple trained neural
networks. And so here I'm showing the print that I made. I'm an
electric fan, it was created using a similar technique done in
layers. And this is on the on the right side, these are in the
middle, I guess, is the graph showing what the opinions of
this print are when shown to different networks like Inception
or ResNet or VGG. And they all are very confident that they're
looking at electric fans. So this, this is using techniques
also from like adversarial examples. So that's some that
these are computer search security techniques, where you try
to essentially fool a computer into thinking it's seeing
something. But in my instance, I don't, I'm not tricking it, I'm
actually just trying to make a super stimulus or stimulus that
is most evocative of that category that it knows. And so
this is an example of me trying to see how these computer AI
algorithms receive these common objects. Here's an example of
the the drawing system, sort of in progress, as you can imagine,
it's an optimization, it starts off kind of putting lines on the
page where it thinks they're most needed. And then it's, it kind
of anneals or optimizes over that, and makes the image look
more and more like the target class that it's, it's trying to
make. And one way that I contextualize this and or talk
about this in my writing, is that for me, it kind of inverts
the computer as a tool stereotype. So for for many,
many years, we've seen the computer as kind of a way to
execute someone's vision. So use it down and use Photoshop and
you have an idea and you kind of do it, you know, use the tool
to express your idea. What I'm trying to do here is I'm kind of
inverting that and I've made it a tool that the computer vision
systems themselves can use to create their own visual outputs.
So so the I'm sort of making the tool for the perception
systems. So after I finished some of those, I did a series of
10 of these. And I'm going to talk a little bit about them to
talk about how they're made, but also what machine learning
concepts are kind of bubbling through. So this is this is a
print called binoculars, again, using the sort of same two layer
technique. This is the data set on the right. And then this is
the print on the left. One interesting thing I thought about
the way this print turned out is that it's viewing the binoculars
in three quarter view, which is kind of an interesting angle,
because you can see all of the features pretty easily from from
kind of not instead of a straight on view as much of the
data is. This is a shark specifically a hammerhead shark,
which is one of the categories of the image net. And here it's
done a very few number of strokes to kind of try to represent
that the essence of the shark outlines. In fact, we can kind
of count the strokes. I believe it's about 14 or so. So this is
a little animation which is showing the strokes one by one
just so you can see the number of primitives. So from, you know,
from a programming point of view, you might think if this is
the number of parameters in the space, but this is also kind of
defining the complexity of the image that that it's able to
produce. After the shark, I did a different one, which is iron
again, kind of an interesting view it took it to this kind of
perpendicular view, which is where you might get the most
characteristic view of the iron. The iron uses the same number
of strokes as the shark does. So one fun thing we can do is we
can actually start with the shark. And then we can move the
strokes around so that they create the iron. And I did this
in software. And then at each step, at the end steps, I show
this to the networks to see what they're thinking what what
they believe they're looking at. And on one side here, you can
see the six networks are very convinced they're looking at an
iron. And then we move the strokes around and they're
so they're looking at a hammerhead shark. I did this a
couple of years ago as a demo, there's a lot of debate in the
deep learning community how these models work. And there's a
there's a camp, or there's a group of people that actually
were proposing that deep learning was mainly triggering on
textures and didn't have any global structure information. So I
think this is a pretty compelling counter example that says,
well, no, these deep learning systems might preference
structure or texture or might use texture, but they do seem to
have some global structure information in their vision
systems. Just a couple more of these. This is a cello. Or I
should say this is the the the label cello, which as you can
imagine has not only cellos, but cellists and other things in
the photos that are the used for classification. And it's
interesting to try to not I don't have any I don't have any
special insight into the the drawing that is produced by the
system. So I don't know. People ask me what is it
representing? And I don't know specifically, but I can go into
the data and poke around and take my best guess. So to me, this
cello image looks like this, these kind of characteristic
photos in the data. So you might call this a mode of the data
or a or about 25% of the data has this general shape where you
have a cellist behind the cello playing and I'll point out a
couple of things that I think I see that resemble these training
examples. And I'll point out one thing that's very different. So
you might try to figure out in the 30 seconds that it needs me
to get there, whether you can find the thing that's different in
those. But the things that are the same as I see this like
light colored object kind of looming behind this darker one.
I see that maybe fingers curled around a fretboard, which was
kind of surprising to me. But the one thing that's very
different about the image that it drew and all the cellists that
I saw on the data set is all the cellists in the data set are
right handed. And all these that I just kind of pulled out
randomly are, but improbably the cellist it decided to draw was
left handed. And that's relevant because, you know, I'm
interested in how these computer vision systems see and they're
actually because of the way they're trained, they're blind and
left right symmetry. So because of image augmentation to offer
the data sets, they, they cannot see or they're not aware of
any features that are that are not that are that are dependent on
left right symmetries. And so it chose to kind of invert this
one and draw a left handed challenge, which I thought was
was interesting. And then there's a there's two more. So this is
one that's measuring cup, not too surprising here seeing the
other ones where it's it's drawn a measuring cup that you might
see in your kitchen. The thing that I haven't sort of so in
addition to choosing the shapes for these drawings, it's also
choosing the colors. And I thought it was really strange that
it chose this bright green color, because I've never seen a
green measuring cup. So again, I dove into the data. And I found
that the data set actually had a large number of examples where
there were green measuring cups. And I dug into this little bit
more. And it turns out that there's this collectible measuring
cup called depression glass measuring cups. This was made
around World War One. And for whatever reason, people were
putting uranium in the glass. And these were collectible. And
so there's a lot of these post donal onward people are
training them because they're collectible. And this is an
example of sampling bias. So arguably, this measuring cup is
not very commonly green. But because of the way this data is
kind of farmed off of the web, there's a lot of green ones that
end up in the training set. So to this computer vision system,
it's actually quite likely that there would be a green measuring
cup. So this is again, kind of showing how the machine learning
techniques are bubbling through it in the results. And then the
last one I'll show here is tick. So tick is one of my least
popular, at least by sales prints that I've done. No one wants
to put a tick in their in their home, I guess, on the wall. But
this one I'm pointing out because in addition to sort of the
the print itself, it actually had a really strong response
across many networks. And so I decided to quantify that. And the
gold standard on this is to take the validation set of the data
set itself. And so I took the the validation set for ImageNet
and I took a network that wasn't involved in the creation of
this. It was inception ResNet. And I scored all of the
validation examples. And they fell into two classes, basically
things that were ticks and things that weren't ticks. But the
short of it is, is that this image of a tick registers
stronger than all of the validation examples, which is
fairly surprising. Like the tick response here is kind of like I
said, a super stimulus or a stronger example of a tick to
these networks and even pictures of the tick from the
validation set. But this has precedent in art and design as
well. Scott McLeod in his series in his book, Understanding
Comics, talks about amplification through
simplification and how if you're trying to represent, say a man,
then you're maybe not well served by using a particular photo of
a particular man, because that's that doesn't well fit the
concept. It's too specific to, to one particular person. And
actually by abstracting and removing some details, you
can come up with a drawing or sketch that better represents a
man or a person or, you know, different levels of abstraction
in the in concepts. And so, arguably, I think that might be
what we're seeing here, where we're taking a real tick and
we're removing some of the some of the some features and leaving
the most salient ones. This is also just as a footnote, kind of
how symbolic abstraction and writing started. So Sumerian
writing starts if you owe someone three oxes, you write, you
know, a picture of three oxes in your play tablet and overtime
that evolves into kind of symbolic abstractions. So that was
my my series, Perception Engines, which is two prints, I'm
going to talk briefly about some other series that I've done
since then. There's one that I did after that called synthetic
abstractions. I've covered it here because as a as a warning,
this is technically not safe for work imagery. So if you are
uncomfortable looking at images that at least vision systems
say are explicit imagery, I encourage you to pause your video
or look away now. So here we go. These are these are these are
abstract prints that I made, specifically looking at not so
much systems for ImageNet, but these systems that impact us all
online. So, you know, we have Google Safe Search, and we have
these other systems that try to shield us from certain images.
And I'm wondering, like, well, are there abstract versions of
what it is that that that that seems to trigger these filters? So
as an example of one of these, I made this print Mustard Dream,
it's on mustard colored paper, and it's just black and white
ink. And when you show a print, this a picture of this print
to three division systems like Google Safe Search, it'll register
as a as adult or racy. Similarly, Amazon thinks it's
explicit nudity and Yahoo thinks it's not safe for work. So here
I am exhibiting that print. I did a series of these. There was
another one pitch stream, another one composition with red,
blue, and yellow. That's kind of an an art joke here, because
it's a riff on a well known other work, except my arrangement
of inks triggers these these algorithms, these filter
algorithms. I have done some similar canvas baked works more
recently in Spain. So these are two newer ones where I'm
actually trying to, you know, I don't know what it is. And the
other one, so people ask me, Well, what is it the computer
season? I don't know. Like I said, I don't have any kind of
knowledge. And maybe I wouldn't want to know. I'd like that we
don't have interpretable ML where we can ask the system what it
sees. Here I'm actually trying to steer the data set a little bit
more by influencing it with with influence the result by data
set. And so this is, for example, one of those I actually
called this one illustrated nude, which I'll talk a little bit
later about why I changed the title of it. But that's because
you know, when you show this to Amazon, that's what it
classifies it at. It says, you know, it thinks it's looking at
it in an illustrated nude and it's fairly high confidence. And
similarly, Google safe search again, thinks it's a racy or
adult image. So if you are searching for this, you insert
safe search, you wouldn't see it. And if you got an email, it
might go in your spam folder, for example. Okay, so I'm going to
talk, I'm just going to go a few more examples just to kind of
get a gist of how this so after that, I made sure my systems
were working kind of in the large with these online API. So
this is a data set for killer whale, a painting of the killer
whale, and then the responses from Google's online vision API.
So you can see it thinks it's a killer whale or marine mammal.
Part of the interest here is also what the ontology or the
labels for the these different systems are. Similarly, here's a
penguin. So we're looking at the data set on the left, thousands
of images of penguins, the version of a penguin that ended up
being printed. And then you can see the Google API sees this as
a penguin, or a flightless bird, or even particular types of
penguins like Emperor penguin. As I mentioned before, there's
the ability to do custom data sets. And so as a commission
collaboration I did with yacht for their album artwork, I did a
series of custom data sets. This is one from from that series.
Where we had a data set of eyes, and we trained the system to
make a synthetic eye. And as you can see, when Google looks at
this, it thinks it's seeing a face, or a nose, or eyebrow, or
eyelash, there's sort of eye features coming through in the
labels. And then, and this is another one I did, similarly,
where it's pictures of rabbits. So it's this one is a various
kind of simple rendition of a rabbit. And again, it's kind of
funny what the Google labels for this end up being evidently
they have separate labels for hair, rabbit, rabbit and hairs
and domestic rabbit, it kind of triggers all of those different
labels in its API response. More recently, and this is kind of
getting up to what I've been doing the last year, I've been
exhibiting these in groups. I found that instead of showing
one example, it actually is more enlightening to see many of
these at once, because you can kind of get a feel for what the
visual language or the common elements are. So these are six
chickens and six eyes, which I exhibited about this time last
year at Separ Gallery. This is something I did subsequently,
where I took that kind of even a step further, and there's kind of
this room full of images, the computer thinks are knots. And
so we have different shape and color combinations, all all
being different images that the computer thinks are reminding
it of and I think knots is kind of an interesting one, because
it's kind of this amorphous shape, but there seems to be some
commonality visually to what what the computer thinks is a
knot. And then this is one I have this in progress. Maybe I'll
give you about 10 seconds to think about what these might be.
But these are actually this looks like it might be computer
images, but these are actually photos of canvases, which I've
taken where I'm starting to organize these into groups. And
these are all the canvases that are just given completed. And
this is for an upcoming exhibition on ants. And so all of
these are shapes that will will trigger in computer vision
systems, to different extents, in thinking that it's looking at
at an ant. So that's kind of a summary of my work. I'm going to
spend the remainder of my talk talking about other approaches
and a little bit about and a little bit about my research,
and then I'll just briefly talk about how my my artwork impacts
the real world, or effects it has once it gets out into the
world. So as a lot of the other speakers have mentioned is these
generative techniques, and I think these are very relevant, and
they're used on other approaches, but there's also this idea that
other AI art approaches had different narratives. So I want
to talk a little bit about that. So my interest, so when I came
into when I started getting interested in modern deep learning
actually got into generative networks as well. And for a
while, I was doing a lot of important research on this or
research that I enjoyed digging into how these things works in
2015 and 2016. This is an artwork I did in 2016. It was just
large, like two meter by one meter print. Here's a zoom in
showing this. And this was images of faces. So at the time,
these were really high quality neural net outputs for faces,
you know, before we had style again, and these other more
modern networks. And I was interested in kind of the space
of faces that could be created. And I, and I wrote a paper talking
about some of these techniques and how you could sample these
networks to get some of the best, the best outputs from those. I
also, since subsequent to that have continued looking at these
generative networks in more of a research sense. So in my
research capacity at the university, I work, I work, for
example, in this system with my graduate student, as Rebecca
alluded to, it's, there's a interesting kind of getting these
out to tools to other creators. And so one of the ideas I have
with this graduate student was to make a spreadsheet tool where
instead of numbers in a spreadsheet, you actually had
samples from a generative model. And could you do use this as a
way of giving people sort of their own create creativity tool
through the interface of a spreadsheet. So it's looking at
at some of these generative models. But in the context of
this art talk, it's, it's, I mainly just wanted to say, this
is a tool other people are using. So Helena Saren is using
these Mario Klingerman, Helena is using these in a way that,
again, Rebecca alluded to where she's using these small data
sets and essentially overfitting those data sets and changing the
data set to get the result that she's looking for. Mario
Klingerman in this project, Neural Glitch, was training one of
these, and then he intentionally kind of damages the network, and
then displays how that damage comes through visually in his
artwork. So it's the idea that there's a sort of an intentional
glitch in there. There's two other artists that that all
mentioned that that one is Robbie Barrett, who did this
project a few years ago called RDCGAN, where he looked at using
these, these generic networks to specifically create portraits.
And there's another well known, at least in a art precedent,
which is obvious is Edmond de Bellamy, that's well known,
mainly because it went up for auction for a very high amount
of money, a Sotheby's a couple of years ago. These actually, I'm
putting up mainly because they use very, very similar techniques.
In fact, they share some code and techniques across them, but
they have very different narratives behind them. And so I
wanted to kind of point out, and the narrative here is I mean,
like what the story is behind how these are made. So when Robbie
is, is, and when I'm, you know, showing my work, it's very much
talking about using these things as a tool or as a collaboration.
But there is a strong push or there's a lot of people in the
art community that more say that, no, it's the, the computer is
autonomous and it's making these artworks. And that was the
kind of stance that obvious took. And it's one that resonates
with people. I think taken to an extreme, what you get is you
get a lot of people making work with robots. So it's common for
people to make these robot artists. And here's just one
example of that. But these have a long history. So there's
actually a long history of making these drawing automatons. And
here's one, you know, that's 200 years old, where it's basically
being driven by gears. And so I just mentioned this is that I
see this as a slippery slope. And so I said, kind of stay away
from this and intentionally don't use any, at least for now,
kind of drawing automaton, because I want to contextualize
what I'm doing as a, as a collaboration between what I'm
doing. So going back kind of again to, to Harold Cohen and
his work, and most of the artists in the space. This is a what
I'm what I'm doing is not so much me handing over full autonomy
and saying the computer is the artist, but coming up with a
co creation process where the there's a role for me and
there's a role for these systems that I'm making. Okay, and in
the last five minutes here, I'm going to spend, I'm going to
talk a little bit about what happens when we put these things
out in the wild, and how how these are some of my artwork, how
it's kind of how we can understand it. So of course,
there's, we can ask the machine what it sees, we can use
visualization techniques, but the one I'm highlighting most
here is these unintended consequences, like what, what
surprises has happened. And keep in mind that for from the eyes
of the computer perception system, this is Magritte's
trajectory of images where he was kind of pointing out like a
picture of a pipe is not a pipe. But that distinction often is
lost with these perception systems, like if they see
something in a representation of something, they more or less
think it's that thing. So the first way you can tell or
interpret or think about my artwork is you can ask a system
what it sees. So if I take this picture I took of me with my
artwork six chickens, and I feed it to the Amazon API, it will
come back and so very diligently that it found a
chicken and a chicken and a bird and etc. And there's a person
over on the side. And I think that's one way to kind of
understand how these systems are viewing or understanding these
artworks. So the computer, the systems themselves. There was
another way that I talked about in the beginning where you can
actually use visualization techniques. So this is using some
research from open AI where they have visualization techniques
similar to deep dream, where they take, I can take one of my
prints and feed it to one of these systems and ask it to kind
of imagine how it relates. So here's the print to chickens.
Here's kind of the, the inner imagination or visualization of
what the computer sees when it sees that. And you can even take
that a little bit further and try to visualize that in 3D. So
this is a little slideshow where it attempts to add some fake
depth to that to that image so that it can kind of understand
understand it. But I think what's most interesting is how
these things kind of accidentally kind of bump into real world
systems. So I'm going to talk about a couple of those to close
out. So this is an exhibition I talked about in 2018. I had
these prints that were supposed to trigger various safe search
filters. And this is an exhibition on the left. I have this
not so great photo of it with a lot of glare and stuff. And I
actually took a picture of this and was going to post it to
Tumblr. And it didn't allow me to post it to Tumblr. The actual
post failed and it said this post contains adult content, which
violates our community guidelines. I was flagged as adult
content and it was not displayed. So I think that's one
example of it kind of bumping up against these real world
systems. Similarly, I had another work in that series,
Architectural Digest wanted to put a print in one of their
magazines. I showed them some prints and they wanted the most
colorful one, which was Lime Dream. Again, maybe not the most
appropriate because it's supposed to be explicit imagery, but
it they decided to go with it anyway. And so when I got a copy
of the magazine, I took a great picture of the magazine, and I
fit into the Amazon API and it was still convinced it was
looking at explicit nudity. So I think this might be the first
example of a cybernetic centerfold like a basically a photo of
a nude made for these kind of vision systems that has appeared
in a magazine. Similarly, like the art gets sold in weird ways.
And so I had a print at a gallery and they sold off without my
knowledge, one of these prints to Sloan Kettering when so if you
go to their academic offices in New York, you can actually see
one of these prints. Again, I don't know that I would that it
might be the most appropriate for this environment. And in
fact, that's one of the reasons I alluded to earlier, I gave
these initially kind of these vague names. But now I've called
them illustrated nudes explicitly because I want if
someone buys or sees one of these, I want them to know what
it's supposed to represent. And so the only way for me to
package that is to put it in the title where it can't be missed.
I did a print a couple of years ago that I took to NeurIPS. I
was stretching it in my hotel room and hanging it up. And when I
went to take a picture of it, I was very happy. This is my
camera interface. When this little yellow rectangle appeared.
So I think if anyone has a phone, they know this is when your
camera's trying to focus on a face. And so just the fact that
this was a shape intended to trigger a face and machine
learning algorithms, kind of pulled the focus onto onto this
because it thought there was a face in the scene. And then the
last one of these is just getting ready for this talk. So as
you can imagine, getting ready for this talk, I'm trying to find
images of my artwork. And I'm pulling them on my phone. And
lo and behold, when I pull up my phone, down at the bottom, it
says we have several people and places in categories of things.
And here's the categories of things that I've kind of
partitioned for you that you might be interested in, like
your animals and your food. So of course, these aren't real
animals and food. If I search for banana, it thinks it's you
know, I've done a print of banana and when I'm working in
the print, I take a lot of pictures of it. So these have
gotten classified as bananas or scorpions. This is a sombrero
print. I was working on it. It's funny too, because it kind of
mixes the real this is a picture of mine, a friend in a funny
hat, and it ends up putting that in the picture with the artwork.
Again, like I'm saying, the computer doesn't have a concept
of what a lot of times is a representation of something
versus what is the real thing. So if I do a search for syringe,
not only do I get this weird photo that I took and I don't
remember why I have a real syringe, but it probably I get
this like really wacky, hypercolour syringe print that I
was working on a few years ago. So I think it's interesting how
these kind of get collapsed in the in these vision systems as
being kind of the same thing. So that's it. Those just to
recap the core ideas of my artwork is that the machines have
their own way of seeing. We can create art for and by machines,
which is what I'm trying to do is trying to use the the
capabilities of machines and understand art through their
eyes. And that and through art, we and by we, I mean, like,
people knowledgeable about deep learning, but also people who
might not have any background in machine learning can appreciate
the ways that that machines perceive the world. So thanks,
that's my online handle for Twitter and my web page. I'm
open for taking questions that I will also say for questions that
I'm very open for things that were part of the talk, but also
might have been things only loosely alluded to in the talk,
if you had questions on, you know, more about research tools or
the community, I think that I'm happy to take any sort of a
broad range of questions. Thank you.
Thank you so much, Tom. It was so great. So I was wondering if
students have questions.
This is from very early on in the talk, but I'm wondering why you
chose screen printing as your medium of choice.
Great question. Sorry, I had to take a second.
So, yeah, so there's I wanted to actually make physical work. And
I thought that was, you know, it's actually easier for me to make
work that is on the computer, but it takes a kind of a step
further to create something that's printed. And that's also
something there's two parts of the question, why make physical
work at all? And then if you're making physical work, why have
it be screen printed? So I think that that two reasons for
making physical work. One is, I think that there's a lot of
reasons for making physical work. One is, I think that people
relate to physical work differently and when they go into
a gallery and art setting. So for years, I did these interactive
installations with screens and things. And I think that people
come into those little bit of the defensive because sort of
this technology right away, like you go in and you see the
screen, you see, you know, this camera looking at you or
something. And I think it sets a tone for how this is for what's
to be expected. And so I really wanted to do a physical print
that sort of was more more about the was more about how the
computer sees it than about the short putting the process in
the gallery. But the purpose for doing the screen printers, I
wanted something that was very exact. So I could I actually have
done some prints that there weren't part of the software I've
experimented with other techniques, brush techniques. And
so it's not really critical to use screen printing, but screen
printing allows me to with some level of very precision, like
make a traditional artwork. So and it has precedent kind of in
the art world through through these pop artists. So I felt it
was kind of a good, a good middle ground for kind of
executing these I could of course print them out on a printer
or something else. But I think that I wanted to kind of
constrain my way, you might think in the same ways that
artists in the past, it could strain themselves. So it makes
it more easy to compare my works with existing artists, if I'm
kind of operating with the same constraints are under the same
interface, you might think of it. One thing I'll say too about
making physical work is it makes it much more difficult to
pull off these techniques, these kind of adversarial techniques
because I don't know what the lighting or the angle of the
photo is. So I try I have to make these these results work for
a distribution of possible photos that might be taken. So if
that's a part of you know, if you look at adversarial images
research, there's kind of doing it on the computer and there's
doing it in the physical world. And it's always kind of more
difficult challenge to take these to the physical world. But
that's kind of where I was interested in taking this.
Cool. Thank you so much.
Excellent. Are there are there other questions? I think that
this is very interesting how thinking in art and you know,
having this dual view of computers and arts can inspire
such an interesting work. For instance, we see that, you know,
as you said, one of the applications of this could be
really adversarial attack and, you know, cyber security, which
is really hard to think about it. If you only want to think about
security and exclude this type of artistic practices, I never
thought about it in this way, like how art can help for better
understanding of, for instance, data bias or algorithmic
bias, or other aspects of AI. So I thought that was very
interesting.
Thanks a lot. Yeah, that actually came through in my in my
original generative work. So when I was working with
generative networks, I found that they were actually really
good at visualizing bias. So if you go back in my paper, I
talked a little bit about how the celeb a data set has a label
for for smiling. But in that data set, and I talk about ways that
you can build dialers where you can turn up the smiling and
down the smiling by using the labels on the data set. But
there's bias in the data set where women are much more
likely than men to be smiling. Just as a product of how the
data was collected, like twice, almost twice as likely as
like 1.5 or something like that. And so as you build these
tools that are that are intended, or as you look at as you
tell the network that you want to make an image smile more,
you're actually also changing some of the masculine feminine
characteristics of the image. And I thought it was a very
visceral way to kind of see some of the machine learning bias.
So that can come through in these general networks. But
yeah, I think it also comes through in the visualization of
these. And I'm kind of careful not to to couch my work too much
in adversarial examples, because I think that I certainly use
some of their techniques. But where I depart from them is that
is they're always trying to do something imperceptible, or
something that is, well, it's adversarial, like in the name,
they're trying to trick the system, whereas I'm more using
it as a visualization or kind of getting under the hood and
kind of understanding what the stimulus is are that that might
trigger the these in the first place.
Yeah, also, it is very interesting how you know, doing by
minimal strokes, you could achieve these things. And this is
another way of thinking of compression, for instance, which
was, as you said, inspired from art.
Yeah, I know that's that's a really good point. Yeah. And I
think that so there's there's a very practical reason to do that
too is because I'm not using any really advanced techniques for
generating these. I'm basically doing like random search or
genetic algorithm. So it's basically doing a search over
the space of outputs, and it just move, you know, it does a
search estimating the gradient. So the fewer parameters you have
the easier job you're going to have sort of hill climbing in
that aspect. So it benefits me to have a simple representation
for this. And there's other things about my drawing style
when I talk about making a dry system for for these, like I
can't have too many discontinuities and things like
that. And the drawing styles that have but but yeah, I think
it's also very interesting to come up with these kind of tight
like for the shark, you know, it's 12 strokes to kind of
represent a shape that when shown to the system still seems to
be a strong stimulus for that category.
Yeah, that also reminds me of the work that scientists did in
computer vision, for instance, a work of Antonio Trouble, where
the question is how many pixels do you need the minimum number
of pixels to show to a computer, for instance, and in terms of
images to see what is this object or get a gist of this
object, you know, for instance, the experiment that set up only
between 32 picture pixels. And they found that, you know, you
can get a gist of what is going on in this instance, image is
this computer vision techniques. So that is also very
interesting.
Yeah, it's also similar to I don't remember the researchers
that did the psychology research where they take face images
and they put them in very low like they basically make icons
where they're, you know, like eight by eight pixels or 16 by
16 pixels. And you can still represent you can still recognize
Abraham Lincoln or these famous figures in these very low
resolution, resolution format. So I think people also have
this ability to decipher these very, these very low
information images as well.
Yeah, very interesting. Excellent. Thank you so much, Tom. It
was great and really inspiring work. And I appreciate that.
Thanks so much for having me. And if anyone else has any
questions, I'm happy to, you know, if you send me an email and
follow up from the class, I know not everyone likes asking
questions. I'm happy to follow up on, you know, you know, every
artist is a little bit different. And so there's no
common. There's no common template, I think you can follow.
But hopefully this will be a good, this is one good example
of a path that someone else might be interested in taking.
Certainly. And then also, Tom has been involved with the
workshop of machine learning for art and creativity at New
Ribs for several years. So that's also an interesting and
valuable contribution that he's making. So if any questions
Yeah, exactly. So I do know a lot of that research and can
direct you to those. And I would encourage you, if you are
interested in a space, you can, I believe, still access a lot of
the videos, for example, from this past year's workshop, just
to get an overview of going a little bit deeper into some of
these talks.
Excellent. Thank you so much, Tom. Thank you. Thank you.
Have a good one. Thanks, everybody.
