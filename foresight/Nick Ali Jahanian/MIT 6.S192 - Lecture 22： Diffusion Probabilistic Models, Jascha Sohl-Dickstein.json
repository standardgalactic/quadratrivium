{"text": " This meeting is being recorded. Hello, everyone. Welcome to your course, AI for Art, Aesthetics, and Creativity. Today we have a very special speaker, Joshua Soldiksten, who is a Senior Research Scientist at Google Brain, and he is one of the pioneers of diffusion models and deep learning. At least in the, as far as I can see in the literature for the area of deep learning, but he can tell me more and correct, tell us more and correct us. So let's ask Joshua to, if you would like to share a little more about his interest and what he inspires him, and then start from there. Great. Yeah, so I wasn't prepared for the interest question, but I've done a lot of different things. I mean, most in the science, but I worked on the Mars rovers after graduation, and then I went into the PhD in biophysics, and then I did computational neuroscience, and then I worked in the computational neuroscience lab, and then I started doing machine learning. And so I don't know if there's like any single coherent thread, except that I'm chasing things that I think are really, really cool. And I will say I get like an amazing sense of satisfaction from figuring things out that like no one has ever figured out before. And I think that might be one of my driving motivations. And so I think creating something new is maybe one of the most satisfying parts I feel about science, and maybe something else, something that you also feel if you're like creating art. Cool. So thanks for the invitation for being here. I hope this will be a fun talk. I'm going to tell you about diffusion probabilistic models, especially I'm going to tell you about content from these two papers in the lower right. Ali already said this a few times, but pretty please interrupt me with questions through the talk. It's like so good to get feedback from the audience from giving a remote talk. I also actually left time in the talk for questions. So if you don't ask any questions, then we're going to end up comfortably early. Cool. So before I dive in at all, I just want to start by calling out my collaborators, especially Eric and Eru and Surya on the 2015 paper and Abhishek, Ben, Dirk, Stefano and Yang on the 2021 paper. Maybe a particular call out to Yang Song, who was the first author on the most recent ICLR paper and did like an absolutely incredible job, as well as to maybe Ben Pool, who was Yang's primary mentor, and also deserves an outsize share of the credit. I'm going to spend most of this talk diving into the nuts and bolts of this class of models. Since this is an AI plus creativity class though I wanted to just start by sharing some of the ways this kind of model is already being used to create art. Maybe the first of these is there is this group of artists on Twitter, and I suppose probably in real life too, using classic techniques called guided diffusion to generate amazing images conditioned on textual pumps. And so here we see two different examples of this. Here we see the prompt is a surreal album cover depicting a boost of eternal dread, hashtag pixel art. And actually putting things in pixel art in the prompt tends to make it the model produced images that might be more likely to appear with that hashtag, and so that's actually part of the prompt. And you get some images that could be interpreted as a boost of eternal dread, or on the right you prompt is a snowstorm in Los Angeles, and you are able to produce some images that are like the models imagining what a snowstorm might look like in Los Angeles. And at least to my eyes I'm surprised and impressed by the quality of the results they can get in this way. The second of these maybe more creative uses that I'm aware of is work that uses diffusion to turn very rough sketches into high quality images. So here in the top you can see some rough sketches of a few different scenes, and then in the bottom you can see the fusion process running to turn those rough sketches into detailed imagined scenes. So I also think that this is a pretty cool example of the potential for creative use of this class of models. Okay, so shared some creative uses of diffusion models, but structure for the rest of the talk is going to look something like this. I am going to provide some physical intuition for what we're going to be doing. Then I'm going to work to make that physical intuition more mathematically precise. Then I'll show how we can generate samples from the model, and then I'll tell you about some surprising connections between diffusion processes and neural ODS, which you may or may not have already heard about. And then I'll show how we can sample and evaluate conditional distributions. At a super high level, we're going to use diffusion to destroy any structure in our training data. Then we're going to carefully characterize this destruction of structure, and we're going to reverse time, and we're going to run the destructive process backwards to build a generative model of the data. Training a model to reverse time maybe sounds a little bit crazy, but there may be two observations we can make that highlight its causability. The first of these observations is that we can use diffusion to destroy the structure in our data distribution. Here I want you to imagine, and this is going to be very much trying to ground this on my physical intuition, which if you have a physics background you might like, otherwise I apologize. Here I want you to imagine that the density of dye molecules represents a probability density, and our goal here is to learn this probability density. This is typically a very challenging thing to do. But even if we can't build a model of the structure in our data distribution directly, what we can do is we can map our data distribution to a much simpler distribution that we can model. So in this physical example, if we allow diffusion to continue long enough, then eventually the dye molecule would be evenly distributed in the jar, and we'd have a uniform distribution. It's maybe not immediately clear that this is helpful, but what if we could reverse time and run this process backwards? What if we could start at a uniform distribution and generate the data distribution? In physics, this is overwhelmingly unlikely to happen spontaneously. Liquids don't spontaneously unmix any more than a shattered glass will spontaneously reassemble, but we can maybe use machine learning to do it. And to see our first clue as to why this might be possible, let's zoom in on a small volume of fluid here. So Q super exciting keynote zoom effect. So the first clue that time reversal might be possible is that although there's this concept of macroscopic irreversibility where the reverse trajectory is overwhelmingly less probable than the forward trajectory from the microscopic perspective. The picture is completely different and is symmetric. So here we've zoomed in and now each bright spec corresponds to a single dye molecule undergoing diffusion. And if we were in person, I'd ask you whether this video was like being played forwards or backwards, but I'm just going to tell you that this video is being played backwards in time. And here I'll flip it around. And now it's the same video, but it's being played forwards in time. And you can see that despite like flipping the arrow of time, the motion of the dye molecules, the behavior of the dye molecules looks looks completely identical and make that more concrete. The diffusion kernel has the same functional form both both forwards and backwards in in time. So at every time step, the next time molecule position is drawn from a very small Gaussian centered around its current position and more generally depending on situation might also be be a drift run. This is really great for us, because it means that if our forward diffusion process was a sequence of small Gaussians, then our reverse diffusion process is also going to be a sequence of small Gaussians. And I'm showing this in continuous state space. I'm not going to talk about it today, but you can do the same thing for like diffusion over binary variables. All right, this is probably a good time to pause for questions for a question and also barking dog, which you may or may not be able to hear. Yes, you can definitely hear the dog. Cool. So just to maybe do a summary slide away, I told you we're going to use the fusion process to destroy all the structure in the data. And then we're going to learn the reversal of this diffusion process. And learning the reversal of this fusion process is going to end up being requiring estimating a function for the mean and covariance of each step of the process. And that reverse diffusion process is going to form our model of the data. So maybe just to illustrate what this means for real data, we might start with data points corresponding to images on the left. And running diffusion on these data points will correspond to mixing in more and more and more random noise to the images. And after some amount of time will be left, we'll erase all the structure and we'll just be left with a color of noise. In the right image, we see a 1D example. Here the x-axis is time and we are diffusing a 1D bimodal distribution into a 1D Gaussian. And we're going to learn the time reversal of this process. We'll be able to generate data samples by running the diffusion process that starts at noise and ends at the data distribution. And here we see this go through the images where a noise sample corresponds just a color noise image and ends up as a sampled image. And on the right you can see this for the simple 1D distribution. Okay, so I've just tried to provide intuition for what is going to happen and I'm about to dive into a bunch of math. So this is an excellent time to pause and poll for questions if anyone has anything they want to ask about. Maybe I can ask a question. If you could please go to the previous slide. In order to understand a little more of this slide. So you are saying you are sort of introducing noise in 1D. Does it mean that you are showing the noise that over time you're adding to each pixel? Yeah, so maybe something that's implicit in the slide which I should describe explicitly. So here on the right pane we're showing the evolution of samples in like a 1D distribution. So in the left in the right pane just like in the left pane there are four samples and they're evolving in 1D. If you take an image you can think of an image as being like a really long vector of length like number of pixels times number of colors. So if you have like a thousand by a thousand image with three colors then it will be like a vector of length like three million. And so then what we're doing is we're doing the same thing we're doing on the right but we're doing it in this like three million dimensional space instead of this like one dimensional space. So like you can think of that image as being a point in this three million dimensional space and we're just diffusing the image like through the space. And if you look at what that looks like that looks like noise being mixed into the image as you like take the image and like diffuse each of its coordinates by this process. Perfect, thank you so much. Thank you for the question. Yeah, so so making this a little bit more mathematical. We're going to start with samples from a data distribution Q of X zero. So Q of X zero might for instance be like the distribution over natural images so it might be many many many examples of images. And for every forward diffusion step, we are going to decay the slide sample slightly towards the origin and add a small amount of Gaussian noise. And this this corresponds to diffusion in a quadratic well or harmonic well. And if you run this for enough steps, then at the end we're going to end up with an identity covariance Gaussian distribution centered at at the origin. So this is going to be our forward diffusion process, which is going to take your sample X zero and destroy all destruction your sample X zero until by X capital T, you just have like a random noise vector. And here once again illustrating this we're going to start with a whole bunch of points. And here you've seen a one day example and like a three million D example and here's like a 2D example. Here we're taking a bunch of points that originally have some like structure and we're mixing diffusing them until they have no structure left. Now for the reverse process, we're going to start at the identity covariance Gaussian. And because we know that the reverse process is the same functional form as the board process. We know that the reverse distribution of trajectories can match the forward distribution. If we also make it a sequence of small Gaussians. And so here we have to do is we have to learn the mean and the covariance of these Gaussians. And so if we find the right functions f mu and sigma and you make our step size small enough. Then then after running like big T steps of the reverse diffusion process will end up back at our data distribution. F mu and sigma here are going to be like super complicated functions. These are these are like the outputs of like state of the art neural networks. But what's nice about this is we've like transformed the problem of building a density model into the problem of learning functions for the mean and the covariance of a sequence of Gaussians. You can see in a second that this is basically going to be a supervised regression problem. You can see this this illustrated. So you start with a noise sample and your general process is going to like run a diffusion process which which turns that noise sample into into samples of the data. Here's just illustrating the same thing with like a cartoon panel. Ah, the well that we ended up at I called it a quadratic well. That's that's to respond to a to a question in the chat. Or or in physics sometimes they call it a harmonic well. So basically if you run if you run diffusion diffusion and energy landscape, which is a quadratic, then then the particles rather than just like drifting away to infinity will like will like, you know, diffuse and drift, but they'll kind of stay roughly around the origin because the quadratic energy landscape likes pulse them back in. Okay, so this is going to be the mathematical high water part our watermark probably. So how do we train these things. We're going to do it using using a variational bound that's essentially identical to that in in hierarchical days. So the probability that the generative model assigns to a data point can be found by integrating over all trajectories that end at that data point. This integral is intractable to compute, but we can borrow a technique called called an important sampling to to multiply and divide by the probability of the four diffusion trajectory, which which is Q of x one the capital T given X zero P X zero is now an expectation over the ratio of the four in reverse trajectory probabilities averaged over for trajectories. We want to train our model by maximizing the log likelihood of the data under the model. And this this corresponds to taking the average over the data distribution Q X zero. This up after the data distribution Q X zero of the log of P X zero and here I just substituted in this form up here into the log of P X zero. Intervals inside logs are pain in the ass. So we use Jensen's inequality to lower bound the log likelihood and and bring the integral outside the log. If the forward and reverse distributions over trajectories exactly overlap. So if the P and Q distributions describe exactly the same trajectory, then distribution of trajectories in this lower bound becomes becomes type. This everything we've just done is also equivalent to writing down the variational bound for like a very, very, very deep variational encoder. We're each time step here corresponds to a layer of the variational encoder. And and where the inference distribution Q is fixed and we're only learning the general distribution P. Yeah, Q is the the, let me go back a second. Q is the forward diffusion process Q is the distribution over over X at every time step that starts from your data and injects a little bit of Gaussian noise at every time step until until you get to the model. I'm sorry until you get to the prior until you get to like the isotropic Gaussian. Okay, where worry. So, if we do a little bit more algebra on this. You can rearrange this into a some over kale divergences between the, the posterior from the forward trajectory, which is that first term inside the kale. And the reverse trajectory, which is that second term inside inside the kale. And this is just a sum over this across across every time step. And the beautiful thing about this is that both of these distributions are Gaussian. The second one is Gaussian because like the entire forward diffusion process conditioned on the data sample is Gaussian. And so this is just a conditional distribution of that big joint Gaussian distribution, which is also a Gaussian. And the second one is Gaussian, because we know it has the same functional form, the reverse process of the same functional form and supported process. And just as reminder, this is the functional form of the reverse diffusion process. So we can write down our training objective. And our training objective is to minimize an expectation over training data and over time steps of this kale divergence between the forward posterior and reverse distribution for a single for a single step. And the kale divergence between two oceans has a super simple functional form, which basically just reduces to to regression. So, so we've transformed our like unstructured unsupervised learning problem into into a supervised regression problem. And, and we know how to, we know how to solve those. So this is the hardest part in the whole talk. So I'm going to pause here for like, like 15 seconds. Could you give a short refresher on the kale distance. The kale distance between two distributions is a information theoretic measurement of how similar the two distributions are to each other. It has some some nice properties. One of those properties is So one interpretation of the kale between like Q and P is it tells you how many bits it would, how many, how inefficient, how many bits you would lose if you tried to describe distribution P, but thought it was distribution Q. So it tells you like by how many bits your model is like an efficient inefficient. It also is like closely connected to log likelihood in that if you take the kale divergence between the data distribution, the model distribution. This is equal to a constant minus the log likelihood of the model. But I think just in general you should think of KL as being a measure of distance between two probability distributions. So you mentioned the, I guess, beautiful property of the kale distance of two Gaussians can simplify the problem. So what if, you know, there'll be let's say a different type of noise, then I guess it wouldn't necessarily simplify. Yeah, yeah, if you had if you had different types of noise, you would, depending on type of noise you very likely would not be able to just like analytically write down the form, you would still be able to optimize it. You would just have to use the thing that you can do analytically here is we can marginalize over over X of t minus one in in this expression and this this I haven't I haven't written down the order of this but but the reason that the calibrating two Gaussians is really nice is because you've been just like marginalized out the X t minus one and that gives you a much lower variance estimate of the loss in the radium. If you use a different form for the noise, then you would probably have to sample X of t minus one, and that would probably be a much higher variance estimate for for your learning signal, but but you could probably still do it. And something like heavy tail noise might be might be really interesting. There's a question for why we do KLQP instead of KLPQ. So, in general, we tend to do KL from the data to the model. And the reason for that is probably because of the connection between KL divergence and log likelihood. What we're often interested in is the log probability of the data points under model, and that means you have to take an expectation over over the data. So you want to like average over your training data of the difference between the two. I think also, it can be very difficult to compute the other way around, because we don't know what the log like we're trying to fit the log probability of the training data. And if you flip the KL around, then you have terms that look like samples from the model of the log of the data distribution, and we don't know how to evaluate those in general, which is another reason that we usually go from from Q to P using our formulation here. Yeah, so this optimization process, or at least the specific loss that we get depends on us having defined the noise like we chose the noise right like we we chose that the noise is is a bunch of like small Gaussian perturbations. And, and that's what makes the specific form possible is if you use this for your noise than the entire forward trajectory. So like Q of X of one to X capital T given X zero is just one big joint Gaussian, and a lot of things become easier once once your entire forward trajectory is one big joint Gaussian. Cool. Alright, so now I'm going to connect this to stochastic differential equations. So I just presented this in in discrete time. But if we take the step sizes to be smaller and smaller and smaller, then we can turn this discrete diffusion process into a limiting stochastic differential equation. It turns out to be extremely useful and to have some some very nice properties as as before, we're going to gradually mix noise into our data distribution until it turns into Gaussian. And, and as I just said, this is the continuous time limit of the discrete diffusion process I saw the moment ago. So here our data distribution is P sub zero. This is a slight change in notation, not a big one. And P sub T is the intermediate distribution from running this stochastic process for a time interval T, and then P capital T is the final final distribution which should look like just noise. So and little T here is between zero and big T. And so an SDE is a generalization of ordinary differential equations. And so we can write it down like this. And here the green term is what you would normally have in an ODE. And it governs the deterministic properties of the stochastic process select the drift. And the red term is the noise that the red term controls like the stochastic fluctuations of the process. So here you should think of DW as being infinitesimal Gaussian noise or like Brownian motion. So we've now replaced the forward process with its continuous time limit, which is stochastic differential equation. And we can do the same thing with the reverse process. One really surprising aspect of stochastic differential equations is that given their equation forward of time and given the marginal distribution PT at time T, the time reversal of the SDE has a very simple analytic form. In reverse SDE, here DT is going to be an infinitesimal negative time step. And DT is going to be an infinitesimal negative time step. And DW is still Brownian motion or DW is still Brownian motion or like little Gaussian perturbations. And if we know the gradient of log PFT with respect to X, then that's all the information we need to like define the reverse SDE, the time reversal of the SDE. And here the gradient of log PFT is the quantity which is often known as the score function. And so we can train an approximation S of theta to this score function. And the way in which you train this is using the continuous time limit of the same variational down to the log likelihood I showed like in the previous section. So basically you can train this thing exactly the same way that you train the discrete time version. There's also connection to denoising score matching, but I don't think it really matters here. You can train this using the same way. So okay, so this is just the continuous time limit of the discrete time diffusion for that version before. And maybe the thing that's really neat about it is we can now link these like drift terms that we were estimating to this thing called a score function, which is like a simple and known property of the distribution PFT. Okay, and so now we've defined the model two different ways. And we've talked about how to train the model. So let's talk about how to sample. And then let's look at some some pretty samples. So after training our model, I mean the sweet timer score base, I'm going to show you a score base here. So score base, we generate samples by, by just numerically integrating the reverse SD. So, so mind you, we've approximated grading of log PFT with our function s. And you can use any off the shelf SD integrator to solve this. The simplest of these is, is called the Euler-Mariama solver. And this is just the, the discrete time discretization of this SD, which actually maps us back to the discrete time diffusion process, where at every, at every time step, you like change X, you change your sample by that looks like this term times a finite changing T. And then you add a little noise with variance proportional to delta T. And you just run this over and over and over again, until you get back a sample. So this is, this is the, maybe the most naive discretization of the SD. Okay. If you maybe went at the sample generation procedure the right way, then, then we're actually training and generating samples with a neural network, which is like thousands of layers deep, where each layer is like a time step into the diffusion process. And, and so this can be interpreted as like an extraordinarily deep general model. If you squint that at a slightly different length, then we're proposing a general model, which has like thousands of times the compute cost of most general models, which is maybe not quite as exciting as saying thousands of layers deep. But, but it will turn out there are ways to make, to make the sampling process more efficient. And I'm going to show you one of them in the talk. Okay, so, so what can you do with this thing? Well, here are some example samples. These are samples from a diffusion model that we built trained on celebe HQ. These are 1024 by 1024 images. I don't know how high resolution they are after after some to me, these are like in this English full from from real human beings. So, so, so we've like crossed over the uncanny valley. So, numerically, this class of models currently beats autoregressive models. In terms of log likelihood where autoregressive models were the winners. This is not my work. This is a paper of dirt King was in Tim Solomon's. They also began in terms of like at by the inception score on some data sets. So, for instance, image that five full by five fall, again, not my work. So these things seem to be remarkably good generative models of images. They also enable you to do some cool things that you can't do with with other other techniques. Let me actually just to time this. Should I expect to end sharply attend or should I expect to take them extra like. Yeah, take your time. You know, as much as you would like because typically we end this course, even after you are done with your lecture, we may stay a little more and chat about things. Okay, cool. I will not take forever, but I will I will not try to do to the last last slides in four minutes as well. I'm actually it's probably really good time for me to pause for a second and see if there are any questions. So, Okay, I'm going to keep on going. I'm going to tell you about these stuff. One really cool thing is that any SD can be transformed into a corresponding ordinary differential equations non stochastic differential equation. Without changing the marginal distributions pt of x, that is, there is an ODE, which has the same distribution over x at all times t. And so this corresponding ODE would allow us to sample from the same distribution starting from the same prior distribution. But by solving ODE instead of an SD, this is this is here is what the ODE looks like. We show the SDH trajectories in red, and the ODE trajectories in, in white, and they both the SD and the ODE are starting from the same points. And you can see that the SD is a stochastic trajectory that converts the starting distribution final distribution easy the ODE. Similarly, trace has the same marginal distribution at every time point, so it starts from the same distribution, and then it ends up the same distribution, but it does this in a deterministic way. Given the SDE, the corresponding ODE, this is just the general general relationship between SDEs and ODE's. Given the SDE, the corresponding ODE, like once again only depends on the score function of pt, for which we already learned, we've already learned the estimator for the score functions. So, so this is just S of, S of theta. So, if we want to generate samples, we can generate samples by integrating this ODE instead of by integrating the SDE. This kind of blew my, so Yang is the one who realized you could do this, and this kind of completely blew my mind when he shared that we could do this. So, so I hope, I hope at least some fraction of you are similarly like scandalized that you can turn SDEs and ODE's like this. So, the question is, are there benefits to modeling with an SDE if there is an ODE equivalent, and the, the two parts to, to the answer. Part number one is the SDE formulation is what allows us to train it. So, so we have to at least like conceptually go through, through SDE space in order to train it. Part number two is that we're training this function S theta to match the score function, but in practice, this function S of theta probably does not correspond to the gradient of any well defined like log probability distribution. Like if we were to train S of theta perfectly, then we would have S of theta is equal to like gradient of X of log P of t, but in actuality S of theta, it's just like a vector. And it probably is not a vector that actually corresponds to the gradient of the log probability. And, and because of that inconsistency in the definition of S of theta, you actually get slightly different distributions if you integrate the ODE and integrate the SDE. And visually, if you integrate the SDE, the samples tend to look just a tiny bit better. But, but I think we don't, we don't fully understand why, but the, the reason distributions are a little bit different is because we're violating this assumption that like S of theta is actually the, the gradient of, of the log marginal density. Okay, so we have this ODE. If you've heard of neural ODE, you can think of this as being like a specific example of a neural ODE, because like S of theta is a super complicated neural network. What's really cool is once you have an ODE, you can use just like off the shelf ODE solvers to, to generate samples and like ODE solvers are like really remarkably good. I didn't, I didn't realize quite how good they were in that they can like just off the shelf ones can just like generate like a few million dimensional like samples from a super high dimensional ODE. So here, for instance, is the samples you get from running the an adaptive ODE sampler with a different allowed number of samples. And you can see that after maybe like 86 this samples from the ODE solvers, so allowing the ODE solver to evaluate the, the ODE equation at like 86 time points. You get what's a pretty high quality image, whereas it takes like thousands for, for the SDE. The other thing you can do with ODE is you can compute an exact log probability for four data points, which means that you can get exactly like this. We can, I can show you a table of performance. Our numbers are bold. This table is now like almost a year old because our paper is now almost a year old. But, but take home message is this class of techniques works like surprising the well both in terms of log likelihood and in terms of measures of perceptual performance like like FID. And I want to tell you about one more thing before, before I break, but maybe this is another good place to pause for a second. If there are any questions about the ODE. Okay, cool. Let me tell you about one more thing, which may actually be one of the most relevant things if you want to use this class of models for, for like creative applications, which is that there is a very well motivated way to control generation under under this class of models. So, just to back up a second. At training time, we return our data sample X zero by running our SDE and we get a noise sample. We want to perform control generation at test time. So we want to be given, we're going to be given a control signal, which I'm going to know is why here. So, for instance, why might be a class label. And so the forward diffusion process will then perturb a conditional data sample X zero given why to complete noise. And by reversing this procedure. We should be able to start from isotropic Gaussian noise ball and obtain a sample X zero given why the reverse time procedure condition and why can be given by the following conditional reverse time SDE. So here, all we've done is we've just replaced the score function of PT with the score function of PT of X, given why. So this is nice. But at first sight, this seems like you have to train a whole bunch of like an entirely new model, because the conditional distribution is function of T is unknown. But what we can do is we can apply basis rule to this. So the first term is just the unconditional score function. And is what we like already spent the rest of the talk talking about how to train exactly the same as what we were training before. The second term can be trained completely separately. From the score base model, or even sometimes can just be like written down in in close form using using the main knowledge. And so, and so the product of these two terms plus the constant is equal to to the log P of T of X given why. And I'm sorry, because these are logs, I should have said that some of these two terms. This is just this. This is just base rule applied to PT of X given why. And so, this is a particularly cool capability, because it's not something you can do at test time for like gams or VES or autoregressive models or any of these. Here we can train a ginormous like unsupervised model images, and then we can train a little classifier like PT of Y given X later. And we can use that little classifier to like guide our image generation. So one example of this is here we have a here we're making PT we're making why the actual class. And so we can use this to do like post hoc sampling of CFR 10 images that come from the class bird on the left or come from the class deer on the right. You can also do this for in painting. So here, why is the part of the image that you know, and you want to generate the entire image like conditioned on the part of the image that you actually know. And so here are the first column is the true image, the ground truth image. The second column, we've thrown away all but the part of the image that you can actually see. So you've thrown away all we've thrown away the surround or the center. And then in the remaining columns, we're showing independent samples of in painting all the missing content in in these images. And you can see that an off the shelf diffusion model not trained to do in painting can can still do a good job in painting. You can also see that there's like diversity in the images that it generates. It doesn't generate same bedroom over and over and over again it generates like a sequence of plausible in paintings of missing information. We can do this for colorization. So we can take an image we can make grayscale and then we can infer infer the colors. Cool. This also let me just actually jump way back to being talking again. This is also the same technique essentially that's used to generate these these art examples. So they're rather than using some pt of y given X as a guiding signal. They're using the output of like the clip classifier as the guiding signal and they're like multiply by some scalar. But but they are guiding the diffusion generation in the same way as as I just showed. And then using it to create novel artistic creations. Cool. Okay, so to summarize. I have shown you a general model based on diffusion processes. We first corrupt data to a known noise distribution using diffusion. And then we learn the time reversal of this diffusion process in either discrete or continuous time. And we can then generate samples by drawing a random noise vector and simulating the reverse diffusion process. There are some advantages of our framework. First, image quality is super duper high. Second, there's equivalence to neural OVs or flow models, which allows us to do things like like exact likelihood computation. I didn't get a chance to talk about this, but but I actually talked about it very briefly before before the talk proper, but our encoding is also uniquely identifiable, meaning that every well trained model will have identical latent codes for identical input data points. This is either a positive or negative depending on how you look at it, but it is a unique property of this class of models. And finally, we can do controllable generation without without retraining the model. Examples like include like class conditional generation. Including some of LA clip guided diffusion measure at the beginning. Yeah. Okay, that's what I got. Thank you so much for listening. Thank you so much. This was awesome. Really helpful. Seems that this controllable generation is really cool because once you have the sort of probability of X and then you can. So that is sort of task agnostic in a way and then whatever task you want, learn it and then use that sort of backbone that you have already trained will learn from the data. This is, this is really cool. And then I think that this fact that you can identify encoding or do the reverse process is also very cool because there is a great deal of how to, for instance, take my image and then map it to the latent space of again, so that I can modify it. Yeah. People have been trying to train inverters. They are getting better and better, but it seems that in this case, the inverter comes for free. Yeah, yeah. That's very cool. So are there questions? I think more on a high level. I mean, what, I guess, having a physics background clearly probably helped come with an idea that is, let's say, kind of, I mean, yeah, based on physical intuition, but it's so different from I guess the GAN architecture. So like, in terms of more like human creativity, like how do we have to look more into the nature to find more inspiration for those, maybe even other models that or other paradigms or what is your suggestion for people who are interested in this field? Yeah, you mean like in machine learning, how do you come up with creative different ideas? Yeah. Yeah, I mean, so definitely, I mean, I'm biased on my own background, so my answer is going to be be like me, which is not really a good answer. But no, one thing I do think is really actually good, though, is I think it's good to have, I think it's good to have a background, which is not the straight machine learning background. I think having exposure to ideas and having a novel perspective like definitely helps. I think probably even more important than that is like talking with and collaborating with people with different ideas than you. Like whatever your background is, if you can like work on a team and work closely and talk closely with people that have a very different background, then you're going to come up with ideas that no one else is going to come up with. And so I think, yeah. Very cool. I think there are questions on the chat. Yeah, I just say, yeah, I think I just saw that as well. Okay, so one question is like just out of curiosity, what happens if the initial input X zero is out of distribution. And so before the fusion process, the process that takes X zero and turns it in the noise is a is a fixed process. So it will take an action. Maybe let me open up for a second. So, um, Yeah, so just to be the Good thing. Okay, so the forward process is is a fixed process. So any sample X zero on the left here is going to get turned into a sample from an unimole calcium. And I should I should one subtle to cure which is pretty important is that every sample on the left gets mapped to the entire distribution on the right. So if you were to start from the same same sample on the left over and over and over again and run the diffusion process again, like every time you ran the diffusion process, you would get a different trajectory, and you would get a different sample on the right. So the forward process maps every single like possible input sample to the entire like PT like like isotropic calcium prior sample. And the reverse is also true. If you start with a sample on the right and you run the SD then you will get a sample from your model of the distribution. But if you run if you were to start from the same sample on the right over and over and over again. Every time you did that you get a different sample from your distribution. And so every sample from the prior is actually also mapped to the entire distribution. And so it's not like there, unless you're using OD formalism, there's not like the one to one correspondence between the image space and the latent space. And so and so you would turn X zero into the same sample from the same latent distribution and then when you came back to the image, you wouldn't know anything X zero anymore. There's another question here. Is it possible to work with multiple classes within one diffusion probabilistic model. And I mean the answer is, so you can train your diffusion model on any distribution that you want to train on, I guess, so so I think the answer is yes, I think, I think the more precise answer would depend on exactly what you wanted to do. But, but there's no reason that multiple classes should be harder than one class. Okay, great. Maybe I can stop there recording here and if there are more questions, you can ask.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.0, "text": " This meeting is being recorded.", "tokens": [50364, 639, 3440, 307, 885, 8287, 13, 50514], "temperature": 0.0, "avg_logprob": -0.349317985668517, "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.06459549814462662}, {"id": 1, "seek": 0, "start": 3.0, "end": 11.0, "text": " Hello, everyone. Welcome to your course, AI for Art, Aesthetics, and Creativity.", "tokens": [50514, 2425, 11, 1518, 13, 4027, 281, 428, 1164, 11, 7318, 337, 5735, 11, 316, 377, 9092, 1167, 11, 293, 11972, 4253, 13, 50914], "temperature": 0.0, "avg_logprob": -0.349317985668517, "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.06459549814462662}, {"id": 2, "seek": 0, "start": 11.0, "end": 22.0, "text": " Today we have a very special speaker, Joshua Soldiksten, who is a Senior Research Scientist", "tokens": [50914, 2692, 321, 362, 257, 588, 2121, 8145, 11, 24005, 7026, 67, 1035, 6266, 11, 567, 307, 257, 18370, 10303, 18944, 468, 51464], "temperature": 0.0, "avg_logprob": -0.349317985668517, "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.06459549814462662}, {"id": 3, "seek": 2200, "start": 22.0, "end": 34.0, "text": " at Google Brain, and he is one of the pioneers of diffusion models and deep learning.", "tokens": [50364, 412, 3329, 29783, 11, 293, 415, 307, 472, 295, 264, 47381, 295, 25242, 5245, 293, 2452, 2539, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19281929258316283, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.27840349078178406}, {"id": 4, "seek": 2200, "start": 34.0, "end": 43.0, "text": " At least in the, as far as I can see in the literature for the area of deep learning,", "tokens": [50964, 1711, 1935, 294, 264, 11, 382, 1400, 382, 286, 393, 536, 294, 264, 10394, 337, 264, 1859, 295, 2452, 2539, 11, 51414], "temperature": 0.0, "avg_logprob": -0.19281929258316283, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.27840349078178406}, {"id": 5, "seek": 2200, "start": 43.0, "end": 49.0, "text": " but he can tell me more and correct, tell us more and correct us.", "tokens": [51414, 457, 415, 393, 980, 385, 544, 293, 3006, 11, 980, 505, 544, 293, 3006, 505, 13, 51714], "temperature": 0.0, "avg_logprob": -0.19281929258316283, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.27840349078178406}, {"id": 6, "seek": 4900, "start": 49.0, "end": 64.0, "text": " So let's ask Joshua to, if you would like to share a little more about his interest and what he inspires him, and then start from there.", "tokens": [50364, 407, 718, 311, 1029, 24005, 281, 11, 498, 291, 576, 411, 281, 2073, 257, 707, 544, 466, 702, 1179, 293, 437, 415, 32566, 796, 11, 293, 550, 722, 490, 456, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1625235510654137, "compression_ratio": 1.4251497005988023, "no_speech_prob": 0.0195888951420784}, {"id": 7, "seek": 4900, "start": 64.0, "end": 73.0, "text": " Great. Yeah, so I wasn't prepared for the interest question, but I've done a lot of different things.", "tokens": [51114, 3769, 13, 865, 11, 370, 286, 2067, 380, 4927, 337, 264, 1179, 1168, 11, 457, 286, 600, 1096, 257, 688, 295, 819, 721, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1625235510654137, "compression_ratio": 1.4251497005988023, "no_speech_prob": 0.0195888951420784}, {"id": 8, "seek": 7300, "start": 73.0, "end": 82.0, "text": " I mean, most in the science, but I worked on the Mars rovers after graduation, and then I went into the PhD in biophysics, and then I did computational neuroscience,", "tokens": [50364, 286, 914, 11, 881, 294, 264, 3497, 11, 457, 286, 2732, 322, 264, 9692, 744, 840, 934, 15652, 11, 293, 550, 286, 1437, 666, 264, 14476, 294, 3228, 5317, 41732, 11, 293, 550, 286, 630, 28270, 42762, 11, 50814], "temperature": 0.0, "avg_logprob": -0.15680936042298663, "compression_ratio": 1.768888888888889, "no_speech_prob": 0.44489747285842896}, {"id": 9, "seek": 7300, "start": 82.0, "end": 87.0, "text": " and then I worked in the computational neuroscience lab, and then I started doing machine learning.", "tokens": [50814, 293, 550, 286, 2732, 294, 264, 28270, 42762, 2715, 11, 293, 550, 286, 1409, 884, 3479, 2539, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15680936042298663, "compression_ratio": 1.768888888888889, "no_speech_prob": 0.44489747285842896}, {"id": 10, "seek": 7300, "start": 87.0, "end": 96.0, "text": " And so I don't know if there's like any single coherent thread, except that I'm chasing things that I think are really, really cool.", "tokens": [51064, 400, 370, 286, 500, 380, 458, 498, 456, 311, 411, 604, 2167, 36239, 7207, 11, 3993, 300, 286, 478, 17876, 721, 300, 286, 519, 366, 534, 11, 534, 1627, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15680936042298663, "compression_ratio": 1.768888888888889, "no_speech_prob": 0.44489747285842896}, {"id": 11, "seek": 9600, "start": 96.0, "end": 104.0, "text": " And I will say I get like an amazing sense of satisfaction from figuring things out that like no one has ever figured out before.", "tokens": [50364, 400, 286, 486, 584, 286, 483, 411, 364, 2243, 2020, 295, 18715, 490, 15213, 721, 484, 300, 411, 572, 472, 575, 1562, 8932, 484, 949, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09251884667270155, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.34470468759536743}, {"id": 12, "seek": 9600, "start": 104.0, "end": 110.0, "text": " And I think that might be one of my driving motivations.", "tokens": [50764, 400, 286, 519, 300, 1062, 312, 472, 295, 452, 4840, 39034, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09251884667270155, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.34470468759536743}, {"id": 13, "seek": 9600, "start": 110.0, "end": 125.0, "text": " And so I think creating something new is maybe one of the most satisfying parts I feel about science, and maybe something else, something that you also feel if you're like creating art.", "tokens": [51064, 400, 370, 286, 519, 4084, 746, 777, 307, 1310, 472, 295, 264, 881, 18348, 3166, 286, 841, 466, 3497, 11, 293, 1310, 746, 1646, 11, 746, 300, 291, 611, 841, 498, 291, 434, 411, 4084, 1523, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09251884667270155, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.34470468759536743}, {"id": 14, "seek": 12500, "start": 125.0, "end": 131.0, "text": " Cool. So thanks for the invitation for being here. I hope this will be a fun talk.", "tokens": [50364, 8561, 13, 407, 3231, 337, 264, 17890, 337, 885, 510, 13, 286, 1454, 341, 486, 312, 257, 1019, 751, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1391133653356674, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.02673276886343956}, {"id": 15, "seek": 12500, "start": 131.0, "end": 143.0, "text": " I'm going to tell you about diffusion probabilistic models, especially I'm going to tell you about content from these two papers in the lower right.", "tokens": [50664, 286, 478, 516, 281, 980, 291, 466, 25242, 31959, 3142, 5245, 11, 2318, 286, 478, 516, 281, 980, 291, 466, 2701, 490, 613, 732, 10577, 294, 264, 3126, 558, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1391133653356674, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.02673276886343956}, {"id": 16, "seek": 12500, "start": 143.0, "end": 152.0, "text": " Ali already said this a few times, but pretty please interrupt me with questions through the talk. It's like so good to get feedback from the audience from giving a remote talk.", "tokens": [51264, 12020, 1217, 848, 341, 257, 1326, 1413, 11, 457, 1238, 1767, 12729, 385, 365, 1651, 807, 264, 751, 13, 467, 311, 411, 370, 665, 281, 483, 5824, 490, 264, 4034, 490, 2902, 257, 8607, 751, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1391133653356674, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.02673276886343956}, {"id": 17, "seek": 15200, "start": 152.0, "end": 161.0, "text": " I also actually left time in the talk for questions. So if you don't ask any questions, then we're going to end up comfortably early.", "tokens": [50364, 286, 611, 767, 1411, 565, 294, 264, 751, 337, 1651, 13, 407, 498, 291, 500, 380, 1029, 604, 1651, 11, 550, 321, 434, 516, 281, 917, 493, 25101, 2440, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1375887895885267, "compression_ratio": 1.2410714285714286, "no_speech_prob": 0.01589895598590374}, {"id": 18, "seek": 15200, "start": 161.0, "end": 166.0, "text": " Cool.", "tokens": [50814, 8561, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1375887895885267, "compression_ratio": 1.2410714285714286, "no_speech_prob": 0.01589895598590374}, {"id": 19, "seek": 16600, "start": 166.0, "end": 186.0, "text": " So before I dive in at all, I just want to start by calling out my collaborators, especially Eric and Eru and Surya on the 2015 paper and Abhishek, Ben, Dirk, Stefano and Yang on the 2021 paper.", "tokens": [50364, 407, 949, 286, 9192, 294, 412, 439, 11, 286, 445, 528, 281, 722, 538, 5141, 484, 452, 39789, 11, 2318, 9336, 293, 462, 894, 293, 318, 2598, 64, 322, 264, 7546, 3035, 293, 2847, 18300, 675, 74, 11, 3964, 11, 413, 18610, 11, 43421, 3730, 293, 11978, 322, 264, 7201, 3035, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2322946275983538, "compression_ratio": 1.3287671232876712, "no_speech_prob": 0.2593071460723877}, {"id": 20, "seek": 18600, "start": 186.0, "end": 208.0, "text": " Maybe a particular call out to Yang Song, who was the first author on the most recent ICLR paper and did like an absolutely incredible job, as well as to maybe Ben Pool, who was Yang's primary mentor, and also deserves an outsize share of the credit.", "tokens": [50364, 2704, 257, 1729, 818, 484, 281, 11978, 11862, 11, 567, 390, 264, 700, 3793, 322, 264, 881, 5162, 14360, 31722, 3035, 293, 630, 411, 364, 3122, 4651, 1691, 11, 382, 731, 382, 281, 1310, 3964, 46188, 11, 567, 390, 11978, 311, 6194, 14478, 11, 293, 611, 17037, 364, 14758, 1125, 2073, 295, 264, 5397, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14723835961293366, "compression_ratio": 1.3966480446927374, "no_speech_prob": 0.3842112123966217}, {"id": 21, "seek": 20800, "start": 208.0, "end": 216.0, "text": " I'm going to spend most of this talk diving into the nuts and bolts of this class of models.", "tokens": [50364, 286, 478, 516, 281, 3496, 881, 295, 341, 751, 20241, 666, 264, 10483, 293, 18127, 295, 341, 1508, 295, 5245, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06076892755799374, "compression_ratio": 1.559748427672956, "no_speech_prob": 0.27138009667396545}, {"id": 22, "seek": 20800, "start": 216.0, "end": 227.0, "text": " Since this is an AI plus creativity class though I wanted to just start by sharing some of the ways this kind of model is already being used to create art.", "tokens": [50764, 4162, 341, 307, 364, 7318, 1804, 12915, 1508, 1673, 286, 1415, 281, 445, 722, 538, 5414, 512, 295, 264, 2098, 341, 733, 295, 2316, 307, 1217, 885, 1143, 281, 1884, 1523, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06076892755799374, "compression_ratio": 1.559748427672956, "no_speech_prob": 0.27138009667396545}, {"id": 23, "seek": 22700, "start": 227.0, "end": 247.0, "text": " Maybe the first of these is there is this group of artists on Twitter, and I suppose probably in real life too, using classic techniques called guided diffusion to generate amazing images conditioned on textual pumps.", "tokens": [50364, 2704, 264, 700, 295, 613, 307, 456, 307, 341, 1594, 295, 6910, 322, 5794, 11, 293, 286, 7297, 1391, 294, 957, 993, 886, 11, 1228, 7230, 7512, 1219, 19663, 25242, 281, 8460, 2243, 5267, 35833, 322, 2487, 901, 27648, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1516017480330034, "compression_ratio": 1.4, "no_speech_prob": 0.8050605058670044}, {"id": 24, "seek": 24700, "start": 247.0, "end": 257.0, "text": " And so here we see two different examples of this. Here we see the prompt is a surreal album cover depicting a boost of eternal dread, hashtag pixel art.", "tokens": [50364, 400, 370, 510, 321, 536, 732, 819, 5110, 295, 341, 13, 1692, 321, 536, 264, 12391, 307, 257, 32513, 6030, 2060, 1367, 21490, 257, 9194, 295, 14503, 22236, 11, 20379, 19261, 1523, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13448491463294396, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.7789549827575684}, {"id": 25, "seek": 24700, "start": 257.0, "end": 267.0, "text": " And actually putting things in pixel art in the prompt tends to make it the model produced images that might be more likely to appear with that hashtag, and so that's actually part of the prompt.", "tokens": [50864, 400, 767, 3372, 721, 294, 19261, 1523, 294, 264, 12391, 12258, 281, 652, 309, 264, 2316, 7126, 5267, 300, 1062, 312, 544, 3700, 281, 4204, 365, 300, 20379, 11, 293, 370, 300, 311, 767, 644, 295, 264, 12391, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13448491463294396, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.7789549827575684}, {"id": 26, "seek": 26700, "start": 267.0, "end": 287.0, "text": " And you get some images that could be interpreted as a boost of eternal dread, or on the right you prompt is a snowstorm in Los Angeles, and you are able to produce some images that are like the models imagining what a snowstorm might look like in Los Angeles.", "tokens": [50364, 400, 291, 483, 512, 5267, 300, 727, 312, 26749, 382, 257, 9194, 295, 14503, 22236, 11, 420, 322, 264, 558, 291, 12391, 307, 257, 5756, 17367, 294, 7632, 12292, 11, 293, 291, 366, 1075, 281, 5258, 512, 5267, 300, 366, 411, 264, 5245, 27798, 437, 257, 5756, 17367, 1062, 574, 411, 294, 7632, 12292, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10059608039209399, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.5227922201156616}, {"id": 27, "seek": 28700, "start": 287.0, "end": 301.0, "text": " And at least to my eyes I'm surprised and impressed by the quality of the results they can get in this way.", "tokens": [50364, 400, 412, 1935, 281, 452, 2575, 286, 478, 6100, 293, 11679, 538, 264, 3125, 295, 264, 3542, 436, 393, 483, 294, 341, 636, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10766562100114493, "compression_ratio": 1.4911242603550297, "no_speech_prob": 0.421761155128479}, {"id": 28, "seek": 28700, "start": 301.0, "end": 311.0, "text": " The second of these maybe more creative uses that I'm aware of is work that uses diffusion to turn very rough sketches into high quality images.", "tokens": [51064, 440, 1150, 295, 613, 1310, 544, 5880, 4960, 300, 286, 478, 3650, 295, 307, 589, 300, 4960, 25242, 281, 1261, 588, 5903, 34547, 666, 1090, 3125, 5267, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10766562100114493, "compression_ratio": 1.4911242603550297, "no_speech_prob": 0.421761155128479}, {"id": 29, "seek": 31100, "start": 311.0, "end": 331.0, "text": " So here in the top you can see some rough sketches of a few different scenes, and then in the bottom you can see the fusion process running to turn those rough sketches into detailed imagined scenes.", "tokens": [50364, 407, 510, 294, 264, 1192, 291, 393, 536, 512, 5903, 34547, 295, 257, 1326, 819, 8026, 11, 293, 550, 294, 264, 2767, 291, 393, 536, 264, 23100, 1399, 2614, 281, 1261, 729, 5903, 34547, 666, 9942, 16590, 8026, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07003384967182957, "compression_ratio": 1.5426356589147288, "no_speech_prob": 0.138322114944458}, {"id": 30, "seek": 33100, "start": 331.0, "end": 349.0, "text": " So I also think that this is a pretty cool example of the potential for creative use of this class of models.", "tokens": [50364, 407, 286, 611, 519, 300, 341, 307, 257, 1238, 1627, 1365, 295, 264, 3995, 337, 5880, 764, 295, 341, 1508, 295, 5245, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06530465020073785, "compression_ratio": 1.1978021978021978, "no_speech_prob": 0.14975903928279877}, {"id": 31, "seek": 34900, "start": 349.0, "end": 362.0, "text": " Okay, so shared some creative uses of diffusion models, but structure for the rest of the talk is going to look something like this.", "tokens": [50364, 1033, 11, 370, 5507, 512, 5880, 4960, 295, 25242, 5245, 11, 457, 3877, 337, 264, 1472, 295, 264, 751, 307, 516, 281, 574, 746, 411, 341, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16664785385131836, "compression_ratio": 1.4421768707482994, "no_speech_prob": 0.3101078271865845}, {"id": 32, "seek": 34900, "start": 362.0, "end": 370.0, "text": " I am going to provide some physical intuition for what we're going to be doing.", "tokens": [51014, 286, 669, 516, 281, 2893, 512, 4001, 24002, 337, 437, 321, 434, 516, 281, 312, 884, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16664785385131836, "compression_ratio": 1.4421768707482994, "no_speech_prob": 0.3101078271865845}, {"id": 33, "seek": 37000, "start": 370.0, "end": 374.0, "text": " Then I'm going to work to make that physical intuition more mathematically precise.", "tokens": [50364, 1396, 286, 478, 516, 281, 589, 281, 652, 300, 4001, 24002, 544, 44003, 13600, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12279135064233708, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.5806089043617249}, {"id": 34, "seek": 37000, "start": 374.0, "end": 386.0, "text": " Then I'll show how we can generate samples from the model, and then I'll tell you about some surprising connections between diffusion processes and neural ODS, which you may or may not have already heard about.", "tokens": [50564, 1396, 286, 603, 855, 577, 321, 393, 8460, 10938, 490, 264, 2316, 11, 293, 550, 286, 603, 980, 291, 466, 512, 8830, 9271, 1296, 25242, 7555, 293, 18161, 422, 11844, 11, 597, 291, 815, 420, 815, 406, 362, 1217, 2198, 466, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12279135064233708, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.5806089043617249}, {"id": 35, "seek": 37000, "start": 386.0, "end": 393.0, "text": " And then I'll show how we can sample and evaluate conditional distributions.", "tokens": [51164, 400, 550, 286, 603, 855, 577, 321, 393, 6889, 293, 13059, 27708, 37870, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12279135064233708, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.5806089043617249}, {"id": 36, "seek": 39300, "start": 394.0, "end": 402.0, "text": " At a super high level, we're going to use diffusion to destroy any structure in our training data.", "tokens": [50414, 1711, 257, 1687, 1090, 1496, 11, 321, 434, 516, 281, 764, 25242, 281, 5293, 604, 3877, 294, 527, 3097, 1412, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07132750126852919, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.14022113382816315}, {"id": 37, "seek": 39300, "start": 402.0, "end": 415.0, "text": " Then we're going to carefully characterize this destruction of structure, and we're going to reverse time, and we're going to run the destructive process backwards to build a generative model of the data.", "tokens": [50814, 1396, 321, 434, 516, 281, 7500, 38463, 341, 13563, 295, 3877, 11, 293, 321, 434, 516, 281, 9943, 565, 11, 293, 321, 434, 516, 281, 1190, 264, 26960, 1399, 12204, 281, 1322, 257, 1337, 1166, 2316, 295, 264, 1412, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07132750126852919, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.14022113382816315}, {"id": 38, "seek": 41500, "start": 415.0, "end": 428.0, "text": " Training a model to reverse time maybe sounds a little bit crazy, but there may be two observations we can make that highlight its causability.", "tokens": [50364, 20620, 257, 2316, 281, 9943, 565, 1310, 3263, 257, 707, 857, 3219, 11, 457, 456, 815, 312, 732, 18163, 321, 393, 652, 300, 5078, 1080, 3302, 2310, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07920019714920609, "compression_ratio": 1.526946107784431, "no_speech_prob": 0.14022956788539886}, {"id": 39, "seek": 41500, "start": 428.0, "end": 439.0, "text": " The first of these observations is that we can use diffusion to destroy the structure in our data distribution.", "tokens": [51014, 440, 700, 295, 613, 18163, 307, 300, 321, 393, 764, 25242, 281, 5293, 264, 3877, 294, 527, 1412, 7316, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07920019714920609, "compression_ratio": 1.526946107784431, "no_speech_prob": 0.14022956788539886}, {"id": 40, "seek": 43900, "start": 439.0, "end": 448.0, "text": " Here I want you to imagine, and this is going to be very much trying to ground this on my physical intuition, which if you have a physics background you might like, otherwise I apologize.", "tokens": [50364, 1692, 286, 528, 291, 281, 3811, 11, 293, 341, 307, 516, 281, 312, 588, 709, 1382, 281, 2727, 341, 322, 452, 4001, 24002, 11, 597, 498, 291, 362, 257, 10649, 3678, 291, 1062, 411, 11, 5911, 286, 12328, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11917638217701632, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.011682474985718727}, {"id": 41, "seek": 43900, "start": 448.0, "end": 458.0, "text": " Here I want you to imagine that the density of dye molecules represents a probability density, and our goal here is to learn this probability density.", "tokens": [50814, 1692, 286, 528, 291, 281, 3811, 300, 264, 10305, 295, 20179, 13093, 8855, 257, 8482, 10305, 11, 293, 527, 3387, 510, 307, 281, 1466, 341, 8482, 10305, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11917638217701632, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.011682474985718727}, {"id": 42, "seek": 43900, "start": 458.0, "end": 463.0, "text": " This is typically a very challenging thing to do.", "tokens": [51314, 639, 307, 5850, 257, 588, 7595, 551, 281, 360, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11917638217701632, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.011682474985718727}, {"id": 43, "seek": 46300, "start": 463.0, "end": 476.0, "text": " But even if we can't build a model of the structure in our data distribution directly, what we can do is we can map our data distribution to a much simpler distribution that we can model.", "tokens": [50364, 583, 754, 498, 321, 393, 380, 1322, 257, 2316, 295, 264, 3877, 294, 527, 1412, 7316, 3838, 11, 437, 321, 393, 360, 307, 321, 393, 4471, 527, 1412, 7316, 281, 257, 709, 18587, 7316, 300, 321, 393, 2316, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1108097971221547, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.00831321906298399}, {"id": 44, "seek": 46300, "start": 476.0, "end": 492.0, "text": " So in this physical example, if we allow diffusion to continue long enough, then eventually the dye molecule would be evenly distributed in the jar, and we'd have a uniform distribution.", "tokens": [51014, 407, 294, 341, 4001, 1365, 11, 498, 321, 2089, 25242, 281, 2354, 938, 1547, 11, 550, 4728, 264, 20179, 15582, 576, 312, 17658, 12631, 294, 264, 15181, 11, 293, 321, 1116, 362, 257, 9452, 7316, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1108097971221547, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.00831321906298399}, {"id": 45, "seek": 49200, "start": 492.0, "end": 509.0, "text": " It's maybe not immediately clear that this is helpful, but what if we could reverse time and run this process backwards? What if we could start at a uniform distribution and generate the data distribution?", "tokens": [50364, 467, 311, 1310, 406, 4258, 1850, 300, 341, 307, 4961, 11, 457, 437, 498, 321, 727, 9943, 565, 293, 1190, 341, 1399, 12204, 30, 708, 498, 321, 727, 722, 412, 257, 9452, 7316, 293, 8460, 264, 1412, 7316, 30, 51214], "temperature": 0.0, "avg_logprob": -0.15541450367417448, "compression_ratio": 1.4642857142857142, "no_speech_prob": 0.010009477846324444}, {"id": 46, "seek": 50900, "start": 509.0, "end": 516.0, "text": " In physics, this is overwhelmingly unlikely to happen spontaneously.", "tokens": [50364, 682, 10649, 11, 341, 307, 42926, 17518, 281, 1051, 47632, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14054911032966946, "compression_ratio": 1.4489795918367347, "no_speech_prob": 0.4105372726917267}, {"id": 47, "seek": 50900, "start": 516.0, "end": 528.0, "text": " Liquids don't spontaneously unmix any more than a shattered glass will spontaneously reassemble, but we can maybe use machine learning to do it.", "tokens": [50714, 32331, 3742, 500, 380, 47632, 19334, 970, 604, 544, 813, 257, 35209, 4276, 486, 47632, 319, 37319, 11, 457, 321, 393, 1310, 764, 3479, 2539, 281, 360, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14054911032966946, "compression_ratio": 1.4489795918367347, "no_speech_prob": 0.4105372726917267}, {"id": 48, "seek": 52800, "start": 528.0, "end": 540.0, "text": " And to see our first clue as to why this might be possible, let's zoom in on a small volume of fluid here.", "tokens": [50364, 400, 281, 536, 527, 700, 13602, 382, 281, 983, 341, 1062, 312, 1944, 11, 718, 311, 8863, 294, 322, 257, 1359, 5523, 295, 9113, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1008720874786377, "compression_ratio": 1.2352941176470589, "no_speech_prob": 0.07468687742948532}, {"id": 49, "seek": 52800, "start": 540.0, "end": 552.0, "text": " So Q super exciting keynote zoom effect.", "tokens": [50964, 407, 1249, 1687, 4670, 33896, 8863, 1802, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1008720874786377, "compression_ratio": 1.2352941176470589, "no_speech_prob": 0.07468687742948532}, {"id": 50, "seek": 55200, "start": 552.0, "end": 574.0, "text": " So the first clue that time reversal might be possible is that although there's this concept of macroscopic irreversibility where the reverse trajectory is overwhelmingly less probable than the forward trajectory from the microscopic perspective.", "tokens": [50364, 407, 264, 700, 13602, 300, 565, 42778, 1062, 312, 1944, 307, 300, 4878, 456, 311, 341, 3410, 295, 7912, 38006, 299, 16014, 840, 2841, 689, 264, 9943, 21512, 307, 42926, 1570, 21759, 813, 264, 2128, 21512, 490, 264, 47897, 4585, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09192158494676862, "compression_ratio": 1.6042780748663101, "no_speech_prob": 0.013841400854289532}, {"id": 51, "seek": 55200, "start": 574.0, "end": 579.0, "text": " The picture is completely different and is symmetric.", "tokens": [51464, 440, 3036, 307, 2584, 819, 293, 307, 32330, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09192158494676862, "compression_ratio": 1.6042780748663101, "no_speech_prob": 0.013841400854289532}, {"id": 52, "seek": 57900, "start": 579.0, "end": 587.0, "text": " So here we've zoomed in and now each bright spec corresponds to a single dye molecule undergoing diffusion.", "tokens": [50364, 407, 510, 321, 600, 8863, 292, 294, 293, 586, 1184, 4730, 1608, 23249, 281, 257, 2167, 20179, 15582, 40033, 25242, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09421372413635254, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.07574990391731262}, {"id": 53, "seek": 57900, "start": 587.0, "end": 597.0, "text": " And if we were in person, I'd ask you whether this video was like being played forwards or backwards, but I'm just going to tell you that this video is being played backwards in time.", "tokens": [50764, 400, 498, 321, 645, 294, 954, 11, 286, 1116, 1029, 291, 1968, 341, 960, 390, 411, 885, 3737, 30126, 420, 12204, 11, 457, 286, 478, 445, 516, 281, 980, 291, 300, 341, 960, 307, 885, 3737, 12204, 294, 565, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09421372413635254, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.07574990391731262}, {"id": 54, "seek": 57900, "start": 597.0, "end": 599.0, "text": " And here I'll flip it around.", "tokens": [51264, 400, 510, 286, 603, 7929, 309, 926, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09421372413635254, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.07574990391731262}, {"id": 55, "seek": 57900, "start": 599.0, "end": 604.0, "text": " And now it's the same video, but it's being played forwards in time.", "tokens": [51364, 400, 586, 309, 311, 264, 912, 960, 11, 457, 309, 311, 885, 3737, 30126, 294, 565, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09421372413635254, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.07574990391731262}, {"id": 56, "seek": 60400, "start": 604.0, "end": 619.0, "text": " And you can see that despite like flipping the arrow of time, the motion of the dye molecules, the behavior of the dye molecules looks looks completely identical and make that more concrete.", "tokens": [50364, 400, 291, 393, 536, 300, 7228, 411, 26886, 264, 11610, 295, 565, 11, 264, 5394, 295, 264, 20179, 13093, 11, 264, 5223, 295, 264, 20179, 13093, 1542, 1542, 2584, 14800, 293, 652, 300, 544, 9859, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1689375214657541, "compression_ratio": 1.6193181818181819, "no_speech_prob": 0.02593570575118065}, {"id": 57, "seek": 60400, "start": 619.0, "end": 625.0, "text": " The diffusion kernel has the same functional form both both forwards and backwards in in time.", "tokens": [51114, 440, 25242, 28256, 575, 264, 912, 11745, 1254, 1293, 1293, 30126, 293, 12204, 294, 294, 565, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1689375214657541, "compression_ratio": 1.6193181818181819, "no_speech_prob": 0.02593570575118065}, {"id": 58, "seek": 62500, "start": 625.0, "end": 641.0, "text": " So at every time step, the next time molecule position is drawn from a very small Gaussian centered around its current position and more generally depending on situation might also be be a drift run.", "tokens": [50364, 407, 412, 633, 565, 1823, 11, 264, 958, 565, 15582, 2535, 307, 10117, 490, 257, 588, 1359, 39148, 18988, 926, 1080, 2190, 2535, 293, 544, 5101, 5413, 322, 2590, 1062, 611, 312, 312, 257, 19699, 1190, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17653151256282154, "compression_ratio": 1.4214285714285715, "no_speech_prob": 0.12076839059591293}, {"id": 59, "seek": 64100, "start": 642.0, "end": 663.0, "text": " This is really great for us, because it means that if our forward diffusion process was a sequence of small Gaussians, then our reverse diffusion process is also going to be a sequence of small Gaussians.", "tokens": [50414, 639, 307, 534, 869, 337, 505, 11, 570, 309, 1355, 300, 498, 527, 2128, 25242, 1399, 390, 257, 8310, 295, 1359, 10384, 2023, 2567, 11, 550, 527, 9943, 25242, 1399, 307, 611, 516, 281, 312, 257, 8310, 295, 1359, 10384, 2023, 2567, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09574347353996114, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.48377299308776855}, {"id": 60, "seek": 66300, "start": 663.0, "end": 677.0, "text": " And I'm showing this in continuous state space. I'm not going to talk about it today, but you can do the same thing for like diffusion over binary variables.", "tokens": [50364, 400, 286, 478, 4099, 341, 294, 10957, 1785, 1901, 13, 286, 478, 406, 516, 281, 751, 466, 309, 965, 11, 457, 291, 393, 360, 264, 912, 551, 337, 411, 25242, 670, 17434, 9102, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1802971495522393, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.3553082346916199}, {"id": 61, "seek": 66300, "start": 677.0, "end": 689.0, "text": " All right, this is probably a good time to pause for questions for a question and also barking dog, which you may or may not be able to hear.", "tokens": [51064, 1057, 558, 11, 341, 307, 1391, 257, 665, 565, 281, 10465, 337, 1651, 337, 257, 1168, 293, 611, 32995, 3000, 11, 597, 291, 815, 420, 815, 406, 312, 1075, 281, 1568, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1802971495522393, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.3553082346916199}, {"id": 62, "seek": 68900, "start": 689.0, "end": 694.0, "text": " Yes, you can definitely hear the dog.", "tokens": [50364, 1079, 11, 291, 393, 2138, 1568, 264, 3000, 13, 50614], "temperature": 0.0, "avg_logprob": -0.19594019286486566, "compression_ratio": 1.3134328358208955, "no_speech_prob": 0.04669921472668648}, {"id": 63, "seek": 68900, "start": 694.0, "end": 700.0, "text": " Cool.", "tokens": [50614, 8561, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19594019286486566, "compression_ratio": 1.3134328358208955, "no_speech_prob": 0.04669921472668648}, {"id": 64, "seek": 68900, "start": 700.0, "end": 707.0, "text": " So just to maybe do a summary slide away, I told you we're going to use the fusion process to destroy all the structure in the data.", "tokens": [50914, 407, 445, 281, 1310, 360, 257, 12691, 4137, 1314, 11, 286, 1907, 291, 321, 434, 516, 281, 764, 264, 23100, 1399, 281, 5293, 439, 264, 3877, 294, 264, 1412, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19594019286486566, "compression_ratio": 1.3134328358208955, "no_speech_prob": 0.04669921472668648}, {"id": 65, "seek": 70700, "start": 707.0, "end": 713.0, "text": " And then we're going to learn the reversal of this diffusion process.", "tokens": [50364, 400, 550, 321, 434, 516, 281, 1466, 264, 42778, 295, 341, 25242, 1399, 13, 50664], "temperature": 0.0, "avg_logprob": -0.18569150337806115, "compression_ratio": 1.6934306569343065, "no_speech_prob": 0.020638462156057358}, {"id": 66, "seek": 70700, "start": 713.0, "end": 726.0, "text": " And learning the reversal of this fusion process is going to end up being requiring estimating a function for the mean and covariance of each step of the process.", "tokens": [50664, 400, 2539, 264, 42778, 295, 341, 23100, 1399, 307, 516, 281, 917, 493, 885, 24165, 8017, 990, 257, 2445, 337, 264, 914, 293, 49851, 719, 295, 1184, 1823, 295, 264, 1399, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18569150337806115, "compression_ratio": 1.6934306569343065, "no_speech_prob": 0.020638462156057358}, {"id": 67, "seek": 72600, "start": 726.0, "end": 747.0, "text": " And that reverse diffusion process is going to form our model of the data.", "tokens": [50364, 400, 300, 9943, 25242, 1399, 307, 516, 281, 1254, 527, 2316, 295, 264, 1412, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10271960810611122, "compression_ratio": 1.0571428571428572, "no_speech_prob": 0.008710119873285294}, {"id": 68, "seek": 74700, "start": 747.0, "end": 757.0, "text": " So maybe just to illustrate what this means for real data, we might start with data points corresponding to images on the left.", "tokens": [50364, 407, 1310, 445, 281, 23221, 437, 341, 1355, 337, 957, 1412, 11, 321, 1062, 722, 365, 1412, 2793, 11760, 281, 5267, 322, 264, 1411, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13813191368466332, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.47977232933044434}, {"id": 69, "seek": 74700, "start": 757.0, "end": 764.0, "text": " And running diffusion on these data points will correspond to mixing in more and more and more random noise to the images.", "tokens": [50864, 400, 2614, 25242, 322, 613, 1412, 2793, 486, 6805, 281, 11983, 294, 544, 293, 544, 293, 544, 4974, 5658, 281, 264, 5267, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13813191368466332, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.47977232933044434}, {"id": 70, "seek": 74700, "start": 764.0, "end": 773.0, "text": " And after some amount of time will be left, we'll erase all the structure and we'll just be left with a color of noise.", "tokens": [51214, 400, 934, 512, 2372, 295, 565, 486, 312, 1411, 11, 321, 603, 23525, 439, 264, 3877, 293, 321, 603, 445, 312, 1411, 365, 257, 2017, 295, 5658, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13813191368466332, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.47977232933044434}, {"id": 71, "seek": 77300, "start": 773.0, "end": 796.0, "text": " In the right image, we see a 1D example. Here the x-axis is time and we are diffusing a 1D bimodal distribution into a 1D Gaussian.", "tokens": [50364, 682, 264, 558, 3256, 11, 321, 536, 257, 502, 35, 1365, 13, 1692, 264, 2031, 12, 24633, 307, 565, 293, 321, 366, 7593, 7981, 257, 502, 35, 272, 332, 378, 304, 7316, 666, 257, 502, 35, 39148, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2106826873052688, "compression_ratio": 1.2242990654205608, "no_speech_prob": 0.014057868160307407}, {"id": 72, "seek": 79600, "start": 796.0, "end": 807.0, "text": " And we're going to learn the time reversal of this process. We'll be able to generate data samples by running the diffusion process that starts at noise and ends at the data distribution.", "tokens": [50364, 400, 321, 434, 516, 281, 1466, 264, 565, 42778, 295, 341, 1399, 13, 492, 603, 312, 1075, 281, 8460, 1412, 10938, 538, 2614, 264, 25242, 1399, 300, 3719, 412, 5658, 293, 5314, 412, 264, 1412, 7316, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06873862336321575, "compression_ratio": 1.406015037593985, "no_speech_prob": 0.04956332594156265}, {"id": 73, "seek": 80700, "start": 807.0, "end": 816.0, "text": " And here we see this go through the images where a noise sample corresponds just a color noise image and ends up as a sampled image.", "tokens": [50364, 400, 510, 321, 536, 341, 352, 807, 264, 5267, 689, 257, 5658, 6889, 23249, 445, 257, 2017, 5658, 3256, 293, 5314, 493, 382, 257, 3247, 15551, 3256, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1600024651507942, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.20668983459472656}, {"id": 74, "seek": 80700, "start": 816.0, "end": 827.0, "text": " And on the right you can see this for the simple 1D distribution.", "tokens": [50814, 400, 322, 264, 558, 291, 393, 536, 341, 337, 264, 2199, 502, 35, 7316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1600024651507942, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.20668983459472656}, {"id": 75, "seek": 82700, "start": 827.0, "end": 840.0, "text": " Okay, so I've just tried to provide intuition for what is going to happen and I'm about to dive into a bunch of math.", "tokens": [50364, 1033, 11, 370, 286, 600, 445, 3031, 281, 2893, 24002, 337, 437, 307, 516, 281, 1051, 293, 286, 478, 466, 281, 9192, 666, 257, 3840, 295, 5221, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14671622216701508, "compression_ratio": 1.17, "no_speech_prob": 0.20374251902103424}, {"id": 76, "seek": 84000, "start": 840.0, "end": 864.0, "text": " So this is an excellent time to pause and poll for questions if anyone has anything they want to ask about.", "tokens": [50364, 407, 341, 307, 364, 7103, 565, 281, 10465, 293, 6418, 337, 1651, 498, 2878, 575, 1340, 436, 528, 281, 1029, 466, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12087622055640587, "compression_ratio": 1.202247191011236, "no_speech_prob": 0.04668831825256348}, {"id": 77, "seek": 86400, "start": 864.0, "end": 870.0, "text": " Maybe I can ask a question. If you could please go to the previous slide.", "tokens": [50364, 2704, 286, 393, 1029, 257, 1168, 13, 759, 291, 727, 1767, 352, 281, 264, 3894, 4137, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1398971187534617, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.4161510467529297}, {"id": 78, "seek": 86400, "start": 870.0, "end": 891.0, "text": " In order to understand a little more of this slide. So you are saying you are sort of introducing noise in 1D. Does it mean that you are showing the noise that over time you're adding to each pixel?", "tokens": [50664, 682, 1668, 281, 1223, 257, 707, 544, 295, 341, 4137, 13, 407, 291, 366, 1566, 291, 366, 1333, 295, 15424, 5658, 294, 502, 35, 13, 4402, 309, 914, 300, 291, 366, 4099, 264, 5658, 300, 670, 565, 291, 434, 5127, 281, 1184, 19261, 30, 51714], "temperature": 0.0, "avg_logprob": -0.1398971187534617, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.4161510467529297}, {"id": 79, "seek": 89100, "start": 891.0, "end": 901.0, "text": " Yeah, so maybe something that's implicit in the slide which I should describe explicitly.", "tokens": [50364, 865, 11, 370, 1310, 746, 300, 311, 26947, 294, 264, 4137, 597, 286, 820, 6786, 20803, 13, 50864], "temperature": 0.0, "avg_logprob": -0.23148013270178505, "compression_ratio": 1.3407407407407408, "no_speech_prob": 0.07578805834054947}, {"id": 80, "seek": 89100, "start": 901.0, "end": 912.0, "text": " So here on the right pane we're showing the evolution of samples in like a 1D distribution.", "tokens": [50864, 407, 510, 322, 264, 558, 32605, 321, 434, 4099, 264, 9303, 295, 10938, 294, 411, 257, 502, 35, 7316, 13, 51414], "temperature": 0.0, "avg_logprob": -0.23148013270178505, "compression_ratio": 1.3407407407407408, "no_speech_prob": 0.07578805834054947}, {"id": 81, "seek": 91200, "start": 912.0, "end": 919.0, "text": " So in the left in the right pane just like in the left pane there are four samples and they're evolving in 1D.", "tokens": [50364, 407, 294, 264, 1411, 294, 264, 558, 32605, 445, 411, 294, 264, 1411, 32605, 456, 366, 1451, 10938, 293, 436, 434, 21085, 294, 502, 35, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12086003621419271, "compression_ratio": 1.8125, "no_speech_prob": 0.6180760860443115}, {"id": 82, "seek": 91200, "start": 919.0, "end": 931.0, "text": " If you take an image you can think of an image as being like a really long vector of length like number of pixels times number of colors.", "tokens": [50714, 759, 291, 747, 364, 3256, 291, 393, 519, 295, 364, 3256, 382, 885, 411, 257, 534, 938, 8062, 295, 4641, 411, 1230, 295, 18668, 1413, 1230, 295, 4577, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12086003621419271, "compression_ratio": 1.8125, "no_speech_prob": 0.6180760860443115}, {"id": 83, "seek": 91200, "start": 931.0, "end": 940.0, "text": " So if you have like a thousand by a thousand image with three colors then it will be like a vector of length like three million.", "tokens": [51314, 407, 498, 291, 362, 411, 257, 4714, 538, 257, 4714, 3256, 365, 1045, 4577, 550, 309, 486, 312, 411, 257, 8062, 295, 4641, 411, 1045, 2459, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12086003621419271, "compression_ratio": 1.8125, "no_speech_prob": 0.6180760860443115}, {"id": 84, "seek": 94000, "start": 940.0, "end": 949.0, "text": " And so then what we're doing is we're doing the same thing we're doing on the right but we're doing it in this like three million dimensional space instead of this like one dimensional space.", "tokens": [50364, 400, 370, 550, 437, 321, 434, 884, 307, 321, 434, 884, 264, 912, 551, 321, 434, 884, 322, 264, 558, 457, 321, 434, 884, 309, 294, 341, 411, 1045, 2459, 18795, 1901, 2602, 295, 341, 411, 472, 18795, 1901, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06874978077876104, "compression_ratio": 2.029585798816568, "no_speech_prob": 0.06181490793824196}, {"id": 85, "seek": 94000, "start": 949.0, "end": 958.0, "text": " So like you can think of that image as being a point in this three million dimensional space and we're just diffusing the image like through the space.", "tokens": [50814, 407, 411, 291, 393, 519, 295, 300, 3256, 382, 885, 257, 935, 294, 341, 1045, 2459, 18795, 1901, 293, 321, 434, 445, 7593, 7981, 264, 3256, 411, 807, 264, 1901, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06874978077876104, "compression_ratio": 2.029585798816568, "no_speech_prob": 0.06181490793824196}, {"id": 86, "seek": 95800, "start": 958.0, "end": 973.0, "text": " And if you look at what that looks like that looks like noise being mixed into the image as you like take the image and like diffuse each of its coordinates by this process.", "tokens": [50364, 400, 498, 291, 574, 412, 437, 300, 1542, 411, 300, 1542, 411, 5658, 885, 7467, 666, 264, 3256, 382, 291, 411, 747, 264, 3256, 293, 411, 42165, 1184, 295, 1080, 21056, 538, 341, 1399, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11892910514559064, "compression_ratio": 1.5472972972972974, "no_speech_prob": 0.04529525712132454}, {"id": 87, "seek": 95800, "start": 973.0, "end": 976.0, "text": " Perfect, thank you so much.", "tokens": [51114, 10246, 11, 1309, 291, 370, 709, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11892910514559064, "compression_ratio": 1.5472972972972974, "no_speech_prob": 0.04529525712132454}, {"id": 88, "seek": 95800, "start": 976.0, "end": 984.0, "text": " Thank you for the question.", "tokens": [51264, 1044, 291, 337, 264, 1168, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11892910514559064, "compression_ratio": 1.5472972972972974, "no_speech_prob": 0.04529525712132454}, {"id": 89, "seek": 98400, "start": 984.0, "end": 990.0, "text": " Yeah, so so making this a little bit more mathematical.", "tokens": [50364, 865, 11, 370, 370, 1455, 341, 257, 707, 857, 544, 18894, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17545367825415828, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.04531112685799599}, {"id": 90, "seek": 98400, "start": 990.0, "end": 996.0, "text": " We're going to start with samples from a data distribution Q of X zero.", "tokens": [50664, 492, 434, 516, 281, 722, 365, 10938, 490, 257, 1412, 7316, 1249, 295, 1783, 4018, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17545367825415828, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.04531112685799599}, {"id": 91, "seek": 98400, "start": 996.0, "end": 1008.0, "text": " So Q of X zero might for instance be like the distribution over natural images so it might be many many many examples of images.", "tokens": [50964, 407, 1249, 295, 1783, 4018, 1062, 337, 5197, 312, 411, 264, 7316, 670, 3303, 5267, 370, 309, 1062, 312, 867, 867, 867, 5110, 295, 5267, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17545367825415828, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.04531112685799599}, {"id": 92, "seek": 100800, "start": 1008.0, "end": 1022.0, "text": " And for every forward diffusion step, we are going to decay the slide sample slightly towards the origin and add a small amount of Gaussian noise.", "tokens": [50364, 400, 337, 633, 2128, 25242, 1823, 11, 321, 366, 516, 281, 21039, 264, 4137, 6889, 4748, 3030, 264, 4957, 293, 909, 257, 1359, 2372, 295, 39148, 5658, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17706656455993652, "compression_ratio": 1.280701754385965, "no_speech_prob": 0.29717761278152466}, {"id": 93, "seek": 102200, "start": 1022.0, "end": 1041.0, "text": " And this this corresponds to diffusion in a quadratic well or harmonic well. And if you run this for enough steps, then at the end we're going to end up with an identity covariance Gaussian distribution centered at at the origin.", "tokens": [50364, 400, 341, 341, 23249, 281, 25242, 294, 257, 37262, 731, 420, 32270, 731, 13, 400, 498, 291, 1190, 341, 337, 1547, 4439, 11, 550, 412, 264, 917, 321, 434, 516, 281, 917, 493, 365, 364, 6575, 49851, 719, 39148, 7316, 18988, 412, 412, 264, 4957, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13283705711364746, "compression_ratio": 1.4679487179487178, "no_speech_prob": 0.20930565893650055}, {"id": 94, "seek": 104100, "start": 1041.0, "end": 1058.0, "text": " So this is going to be our forward diffusion process, which is going to take your sample X zero and destroy all destruction your sample X zero until by X capital T, you just have like a random noise vector.", "tokens": [50364, 407, 341, 307, 516, 281, 312, 527, 2128, 25242, 1399, 11, 597, 307, 516, 281, 747, 428, 6889, 1783, 4018, 293, 5293, 439, 13563, 428, 6889, 1783, 4018, 1826, 538, 1783, 4238, 314, 11, 291, 445, 362, 411, 257, 4974, 5658, 8062, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15748657718781503, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.04533647000789642}, {"id": 95, "seek": 104100, "start": 1058.0, "end": 1067.0, "text": " And here once again illustrating this we're going to start with a whole bunch of points. And here you've seen a one day example and like a three million D example and here's like a 2D example.", "tokens": [51214, 400, 510, 1564, 797, 8490, 8754, 341, 321, 434, 516, 281, 722, 365, 257, 1379, 3840, 295, 2793, 13, 400, 510, 291, 600, 1612, 257, 472, 786, 1365, 293, 411, 257, 1045, 2459, 413, 1365, 293, 510, 311, 411, 257, 568, 35, 1365, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15748657718781503, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.04533647000789642}, {"id": 96, "seek": 106700, "start": 1067.0, "end": 1077.0, "text": " Here we're taking a bunch of points that originally have some like structure and we're mixing diffusing them until they have no structure left.", "tokens": [50364, 1692, 321, 434, 1940, 257, 3840, 295, 2793, 300, 7993, 362, 512, 411, 3877, 293, 321, 434, 11983, 7593, 7981, 552, 1826, 436, 362, 572, 3877, 1411, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08782446384429932, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.022278618067502975}, {"id": 97, "seek": 106700, "start": 1077.0, "end": 1084.0, "text": " Now for the reverse process, we're going to start at the identity covariance Gaussian.", "tokens": [50864, 823, 337, 264, 9943, 1399, 11, 321, 434, 516, 281, 722, 412, 264, 6575, 49851, 719, 39148, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08782446384429932, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.022278618067502975}, {"id": 98, "seek": 106700, "start": 1084.0, "end": 1090.0, "text": " And because we know that the reverse process is the same functional form as the board process.", "tokens": [51214, 400, 570, 321, 458, 300, 264, 9943, 1399, 307, 264, 912, 11745, 1254, 382, 264, 3150, 1399, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08782446384429932, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.022278618067502975}, {"id": 99, "seek": 109000, "start": 1091.0, "end": 1104.0, "text": " We know that the reverse distribution of trajectories can match the forward distribution. If we also make it a sequence of small Gaussians.", "tokens": [50414, 492, 458, 300, 264, 9943, 7316, 295, 18257, 2083, 393, 2995, 264, 2128, 7316, 13, 759, 321, 611, 652, 309, 257, 8310, 295, 1359, 10384, 2023, 2567, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14692791577043204, "compression_ratio": 1.5430463576158941, "no_speech_prob": 0.08385308086872101}, {"id": 100, "seek": 109000, "start": 1104.0, "end": 1115.0, "text": " And so here we have to do is we have to learn the mean and the covariance of these Gaussians.", "tokens": [51064, 400, 370, 510, 321, 362, 281, 360, 307, 321, 362, 281, 1466, 264, 914, 293, 264, 49851, 719, 295, 613, 10384, 2023, 2567, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14692791577043204, "compression_ratio": 1.5430463576158941, "no_speech_prob": 0.08385308086872101}, {"id": 101, "seek": 111500, "start": 1115.0, "end": 1122.0, "text": " And so if we find the right functions f mu and sigma and you make our step size small enough.", "tokens": [50364, 400, 370, 498, 321, 915, 264, 558, 6828, 283, 2992, 293, 12771, 293, 291, 652, 527, 1823, 2744, 1359, 1547, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1992746469925861, "compression_ratio": 1.4189189189189189, "no_speech_prob": 0.0049050236120820045}, {"id": 102, "seek": 111500, "start": 1122.0, "end": 1132.0, "text": " Then then after running like big T steps of the reverse diffusion process will end up back at our data distribution.", "tokens": [50714, 1396, 550, 934, 2614, 411, 955, 314, 4439, 295, 264, 9943, 25242, 1399, 486, 917, 493, 646, 412, 527, 1412, 7316, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1992746469925861, "compression_ratio": 1.4189189189189189, "no_speech_prob": 0.0049050236120820045}, {"id": 103, "seek": 113200, "start": 1133.0, "end": 1143.0, "text": " F mu and sigma here are going to be like super complicated functions. These are these are like the outputs of like state of the art neural networks.", "tokens": [50414, 479, 2992, 293, 12771, 510, 366, 516, 281, 312, 411, 1687, 6179, 6828, 13, 1981, 366, 613, 366, 411, 264, 23930, 295, 411, 1785, 295, 264, 1523, 18161, 9590, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12573712666829426, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.03307764604687691}, {"id": 104, "seek": 113200, "start": 1143.0, "end": 1155.0, "text": " But what's nice about this is we've like transformed the problem of building a density model into the problem of learning functions for the mean and the covariance of a sequence of Gaussians.", "tokens": [50914, 583, 437, 311, 1481, 466, 341, 307, 321, 600, 411, 16894, 264, 1154, 295, 2390, 257, 10305, 2316, 666, 264, 1154, 295, 2539, 6828, 337, 264, 914, 293, 264, 49851, 719, 295, 257, 8310, 295, 10384, 2023, 2567, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12573712666829426, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.03307764604687691}, {"id": 105, "seek": 115500, "start": 1155.0, "end": 1163.0, "text": " You can see in a second that this is basically going to be a supervised regression problem.", "tokens": [50364, 509, 393, 536, 294, 257, 1150, 300, 341, 307, 1936, 516, 281, 312, 257, 46533, 24590, 1154, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17014504992772664, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.09942394495010376}, {"id": 106, "seek": 115500, "start": 1163.0, "end": 1177.0, "text": " You can see this this illustrated. So you start with a noise sample and your general process is going to like run a diffusion process which which turns that noise sample into into samples of the data.", "tokens": [50764, 509, 393, 536, 341, 341, 33875, 13, 407, 291, 722, 365, 257, 5658, 6889, 293, 428, 2674, 1399, 307, 516, 281, 411, 1190, 257, 25242, 1399, 597, 597, 4523, 300, 5658, 6889, 666, 666, 10938, 295, 264, 1412, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17014504992772664, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.09942394495010376}, {"id": 107, "seek": 117700, "start": 1177.0, "end": 1185.0, "text": " Here's just illustrating the same thing with like a cartoon panel.", "tokens": [50364, 1692, 311, 445, 8490, 8754, 264, 912, 551, 365, 411, 257, 18569, 4831, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2810127845177284, "compression_ratio": 1.5, "no_speech_prob": 0.012622294947504997}, {"id": 108, "seek": 117700, "start": 1185.0, "end": 1195.0, "text": " Ah, the well that we ended up at I called it a quadratic well. That's that's to respond to a to a question in the chat.", "tokens": [50764, 2438, 11, 264, 731, 300, 321, 4590, 493, 412, 286, 1219, 309, 257, 37262, 731, 13, 663, 311, 300, 311, 281, 4196, 281, 257, 281, 257, 1168, 294, 264, 5081, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2810127845177284, "compression_ratio": 1.5, "no_speech_prob": 0.012622294947504997}, {"id": 109, "seek": 117700, "start": 1195.0, "end": 1200.0, "text": " Or or in physics sometimes they call it a harmonic well.", "tokens": [51264, 1610, 420, 294, 10649, 2171, 436, 818, 309, 257, 32270, 731, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2810127845177284, "compression_ratio": 1.5, "no_speech_prob": 0.012622294947504997}, {"id": 110, "seek": 120000, "start": 1200.0, "end": 1219.0, "text": " So basically if you run if you run diffusion diffusion and energy landscape, which is a quadratic, then then the particles rather than just like drifting away to infinity will like will like, you know, diffuse and drift, but they'll kind of stay roughly around the origin because the quadratic energy landscape likes", "tokens": [50364, 407, 1936, 498, 291, 1190, 498, 291, 1190, 25242, 25242, 293, 2281, 9661, 11, 597, 307, 257, 37262, 11, 550, 550, 264, 10007, 2831, 813, 445, 411, 37973, 1314, 281, 13202, 486, 411, 486, 411, 11, 291, 458, 11, 42165, 293, 19699, 11, 457, 436, 603, 733, 295, 1754, 9810, 926, 264, 4957, 570, 264, 37262, 2281, 9661, 5902, 51314], "temperature": 0.0, "avg_logprob": -0.19808762868245441, "compression_ratio": 1.7656903765690377, "no_speech_prob": 0.0016483761137351394}, {"id": 111, "seek": 120000, "start": 1219.0, "end": 1223.0, "text": " pulse them back in.", "tokens": [51314, 17709, 552, 646, 294, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19808762868245441, "compression_ratio": 1.7656903765690377, "no_speech_prob": 0.0016483761137351394}, {"id": 112, "seek": 120000, "start": 1223.0, "end": 1229.0, "text": " Okay, so this is going to be the mathematical high water part our watermark probably.", "tokens": [51514, 1033, 11, 370, 341, 307, 516, 281, 312, 264, 18894, 1090, 1281, 644, 527, 1281, 5638, 1391, 13, 51814], "temperature": 0.0, "avg_logprob": -0.19808762868245441, "compression_ratio": 1.7656903765690377, "no_speech_prob": 0.0016483761137351394}, {"id": 113, "seek": 122900, "start": 1229.0, "end": 1241.0, "text": " So how do we train these things. We're going to do it using using a variational bound that's essentially identical to that in in hierarchical days.", "tokens": [50364, 407, 577, 360, 321, 3847, 613, 721, 13, 492, 434, 516, 281, 360, 309, 1228, 1228, 257, 3034, 1478, 5472, 300, 311, 4476, 14800, 281, 300, 294, 294, 35250, 804, 1708, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11562812328338623, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.0015975666465237737}, {"id": 114, "seek": 122900, "start": 1241.0, "end": 1253.0, "text": " So the probability that the generative model assigns to a data point can be found by integrating over all trajectories that end at that data point.", "tokens": [50964, 407, 264, 8482, 300, 264, 1337, 1166, 2316, 6269, 82, 281, 257, 1412, 935, 393, 312, 1352, 538, 26889, 670, 439, 18257, 2083, 300, 917, 412, 300, 1412, 935, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11562812328338623, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.0015975666465237737}, {"id": 115, "seek": 125300, "start": 1253.0, "end": 1264.0, "text": " This integral is intractable to compute, but we can borrow a technique called called an important sampling to to", "tokens": [50364, 639, 11573, 307, 560, 1897, 712, 281, 14722, 11, 457, 321, 393, 11172, 257, 6532, 1219, 1219, 364, 1021, 21179, 281, 281, 50914], "temperature": 0.0, "avg_logprob": -0.2245829960085311, "compression_ratio": 1.4337349397590362, "no_speech_prob": 0.07154212146997452}, {"id": 116, "seek": 125300, "start": 1264.0, "end": 1277.0, "text": " multiply and divide by the probability of the four diffusion trajectory, which which is Q of x one the capital T given X zero", "tokens": [50914, 12972, 293, 9845, 538, 264, 8482, 295, 264, 1451, 25242, 21512, 11, 597, 597, 307, 1249, 295, 2031, 472, 264, 4238, 314, 2212, 1783, 4018, 51564], "temperature": 0.0, "avg_logprob": -0.2245829960085311, "compression_ratio": 1.4337349397590362, "no_speech_prob": 0.07154212146997452}, {"id": 117, "seek": 127700, "start": 1277.0, "end": 1288.0, "text": " P X zero is now an expectation over the ratio of the four in reverse trajectory probabilities averaged over for trajectories.", "tokens": [50364, 430, 1783, 4018, 307, 586, 364, 14334, 670, 264, 8509, 295, 264, 1451, 294, 9943, 21512, 33783, 18247, 2980, 670, 337, 18257, 2083, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1840514425021499, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.007575963623821735}, {"id": 118, "seek": 127700, "start": 1288.0, "end": 1297.0, "text": " We want to train our model by maximizing the log likelihood of the data under the model.", "tokens": [50914, 492, 528, 281, 3847, 527, 2316, 538, 5138, 3319, 264, 3565, 22119, 295, 264, 1412, 833, 264, 2316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1840514425021499, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.007575963623821735}, {"id": 119, "seek": 127700, "start": 1297.0, "end": 1303.0, "text": " And this this corresponds to taking the average over the data distribution Q X zero.", "tokens": [51364, 400, 341, 341, 23249, 281, 1940, 264, 4274, 670, 264, 1412, 7316, 1249, 1783, 4018, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1840514425021499, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.007575963623821735}, {"id": 120, "seek": 130300, "start": 1304.0, "end": 1316.0, "text": " This up after the data distribution Q X zero of the log of P X zero and here I just substituted in this form up here into the log of P X zero.", "tokens": [50414, 639, 493, 934, 264, 1412, 7316, 1249, 1783, 4018, 295, 264, 3565, 295, 430, 1783, 4018, 293, 510, 286, 445, 26441, 4866, 294, 341, 1254, 493, 510, 666, 264, 3565, 295, 430, 1783, 4018, 13, 51014], "temperature": 0.0, "avg_logprob": -0.31538110620835247, "compression_ratio": 1.4566929133858268, "no_speech_prob": 0.03620605543255806}, {"id": 121, "seek": 130300, "start": 1316.0, "end": 1320.0, "text": " Intervals inside logs are pain in the ass.", "tokens": [51014, 5751, 19778, 1854, 20820, 366, 1822, 294, 264, 1256, 13, 51214], "temperature": 0.0, "avg_logprob": -0.31538110620835247, "compression_ratio": 1.4566929133858268, "no_speech_prob": 0.03620605543255806}, {"id": 122, "seek": 132000, "start": 1320.0, "end": 1333.0, "text": " So we use Jensen's inequality to lower bound the log likelihood and and bring the integral outside the log.", "tokens": [50364, 407, 321, 764, 508, 32934, 311, 16970, 281, 3126, 5472, 264, 3565, 22119, 293, 293, 1565, 264, 11573, 2380, 264, 3565, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14128631703993855, "compression_ratio": 1.7593582887700534, "no_speech_prob": 0.0060961986891925335}, {"id": 123, "seek": 132000, "start": 1333.0, "end": 1338.0, "text": " If the forward and reverse distributions over trajectories exactly overlap.", "tokens": [51014, 759, 264, 2128, 293, 9943, 37870, 670, 18257, 2083, 2293, 19959, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14128631703993855, "compression_ratio": 1.7593582887700534, "no_speech_prob": 0.0060961986891925335}, {"id": 124, "seek": 132000, "start": 1338.0, "end": 1348.0, "text": " So if the P and Q distributions describe exactly the same trajectory, then distribution of trajectories in this lower bound becomes becomes type.", "tokens": [51264, 407, 498, 264, 430, 293, 1249, 37870, 6786, 2293, 264, 912, 21512, 11, 550, 7316, 295, 18257, 2083, 294, 341, 3126, 5472, 3643, 3643, 2010, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14128631703993855, "compression_ratio": 1.7593582887700534, "no_speech_prob": 0.0060961986891925335}, {"id": 125, "seek": 134800, "start": 1348.0, "end": 1359.0, "text": " This everything we've just done is also equivalent to writing down the variational bound for like a very, very, very deep variational encoder.", "tokens": [50364, 639, 1203, 321, 600, 445, 1096, 307, 611, 10344, 281, 3579, 760, 264, 3034, 1478, 5472, 337, 411, 257, 588, 11, 588, 11, 588, 2452, 3034, 1478, 2058, 19866, 13, 50914], "temperature": 0.0, "avg_logprob": -0.179596978264886, "compression_ratio": 1.6649484536082475, "no_speech_prob": 0.004133050795644522}, {"id": 126, "seek": 134800, "start": 1359.0, "end": 1365.0, "text": " We're each time step here corresponds to a layer of the variational encoder.", "tokens": [50914, 492, 434, 1184, 565, 1823, 510, 23249, 281, 257, 4583, 295, 264, 3034, 1478, 2058, 19866, 13, 51214], "temperature": 0.0, "avg_logprob": -0.179596978264886, "compression_ratio": 1.6649484536082475, "no_speech_prob": 0.004133050795644522}, {"id": 127, "seek": 134800, "start": 1365.0, "end": 1374.0, "text": " And and where the inference distribution Q is fixed and we're only learning the general distribution P.", "tokens": [51214, 400, 293, 689, 264, 38253, 7316, 1249, 307, 6806, 293, 321, 434, 787, 2539, 264, 2674, 7316, 430, 13, 51664], "temperature": 0.0, "avg_logprob": -0.179596978264886, "compression_ratio": 1.6649484536082475, "no_speech_prob": 0.004133050795644522}, {"id": 128, "seek": 137400, "start": 1374.0, "end": 1381.0, "text": " Yeah, Q is the the, let me go back a second.", "tokens": [50364, 865, 11, 1249, 307, 264, 264, 11, 718, 385, 352, 646, 257, 1150, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14842325543600415, "compression_ratio": 1.5644171779141105, "no_speech_prob": 0.012238111346960068}, {"id": 129, "seek": 137400, "start": 1381.0, "end": 1398.0, "text": " Q is the forward diffusion process Q is the distribution over over X at every time step that starts from your data and injects a little bit of Gaussian noise at every time step until until you get to the model.", "tokens": [50714, 1249, 307, 264, 2128, 25242, 1399, 1249, 307, 264, 7316, 670, 670, 1783, 412, 633, 565, 1823, 300, 3719, 490, 428, 1412, 293, 10711, 82, 257, 707, 857, 295, 39148, 5658, 412, 633, 565, 1823, 1826, 1826, 291, 483, 281, 264, 2316, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14842325543600415, "compression_ratio": 1.5644171779141105, "no_speech_prob": 0.012238111346960068}, {"id": 130, "seek": 139800, "start": 1398.0, "end": 1413.0, "text": " I'm sorry until you get to the prior until you get to like the isotropic Gaussian.", "tokens": [50364, 286, 478, 2597, 1826, 291, 483, 281, 264, 4059, 1826, 291, 483, 281, 411, 264, 38018, 39173, 39148, 13, 51114], "temperature": 0.0, "avg_logprob": -0.25616861426311993, "compression_ratio": 1.1549295774647887, "no_speech_prob": 0.050311874598264694}, {"id": 131, "seek": 141300, "start": 1413.0, "end": 1417.0, "text": " Okay, where worry.", "tokens": [50364, 1033, 11, 689, 3292, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17581322318629214, "compression_ratio": 1.4258064516129032, "no_speech_prob": 0.06949710100889206}, {"id": 132, "seek": 141300, "start": 1417.0, "end": 1421.0, "text": " So, if we do a little bit more algebra on this.", "tokens": [50564, 407, 11, 498, 321, 360, 257, 707, 857, 544, 21989, 322, 341, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17581322318629214, "compression_ratio": 1.4258064516129032, "no_speech_prob": 0.06949710100889206}, {"id": 133, "seek": 141300, "start": 1421.0, "end": 1438.0, "text": " You can rearrange this into a some over kale divergences between the, the posterior from the forward trajectory, which is that first term inside the kale.", "tokens": [50764, 509, 393, 39568, 341, 666, 257, 512, 670, 34699, 18558, 1766, 887, 1296, 264, 11, 264, 33529, 490, 264, 2128, 21512, 11, 597, 307, 300, 700, 1433, 1854, 264, 34699, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17581322318629214, "compression_ratio": 1.4258064516129032, "no_speech_prob": 0.06949710100889206}, {"id": 134, "seek": 143800, "start": 1438.0, "end": 1449.0, "text": " And the reverse trajectory, which is that second term inside inside the kale.", "tokens": [50364, 400, 264, 9943, 21512, 11, 597, 307, 300, 1150, 1433, 1854, 1854, 264, 34699, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13229528427124024, "compression_ratio": 1.5374149659863945, "no_speech_prob": 0.009266423061490059}, {"id": 135, "seek": 143800, "start": 1449.0, "end": 1459.0, "text": " And this is just a sum over this across across every time step. And the beautiful thing about this is that both of these distributions are Gaussian.", "tokens": [50914, 400, 341, 307, 445, 257, 2408, 670, 341, 2108, 2108, 633, 565, 1823, 13, 400, 264, 2238, 551, 466, 341, 307, 300, 1293, 295, 613, 37870, 366, 39148, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13229528427124024, "compression_ratio": 1.5374149659863945, "no_speech_prob": 0.009266423061490059}, {"id": 136, "seek": 145900, "start": 1459.0, "end": 1472.0, "text": " The second one is Gaussian because like the entire forward diffusion process conditioned on the data sample is Gaussian. And so this is just a conditional distribution of that big joint Gaussian distribution, which is also a Gaussian.", "tokens": [50364, 440, 1150, 472, 307, 39148, 570, 411, 264, 2302, 2128, 25242, 1399, 35833, 322, 264, 1412, 6889, 307, 39148, 13, 400, 370, 341, 307, 445, 257, 27708, 7316, 295, 300, 955, 7225, 39148, 7316, 11, 597, 307, 611, 257, 39148, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16333518558078342, "compression_ratio": 1.6137931034482758, "no_speech_prob": 0.5769684910774231}, {"id": 137, "seek": 147200, "start": 1472.0, "end": 1490.0, "text": " And the second one is Gaussian, because we know it has the same functional form, the reverse process of the same functional form and supported process. And just as reminder, this is the functional form of the reverse diffusion process.", "tokens": [50364, 400, 264, 1150, 472, 307, 39148, 11, 570, 321, 458, 309, 575, 264, 912, 11745, 1254, 11, 264, 9943, 1399, 295, 264, 912, 11745, 1254, 293, 8104, 1399, 13, 400, 445, 382, 13548, 11, 341, 307, 264, 11745, 1254, 295, 264, 9943, 25242, 1399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1683407802971042, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.03160830959677696}, {"id": 138, "seek": 149000, "start": 1490.0, "end": 1515.0, "text": " So we can write down our training objective. And our training objective is to minimize an expectation over training data and over time steps of this kale divergence between the forward posterior and reverse distribution for a single for a single step.", "tokens": [50364, 407, 321, 393, 2464, 760, 527, 3097, 10024, 13, 400, 527, 3097, 10024, 307, 281, 17522, 364, 14334, 670, 3097, 1412, 293, 670, 565, 4439, 295, 341, 34699, 47387, 1296, 264, 2128, 33529, 293, 9943, 7316, 337, 257, 2167, 337, 257, 2167, 1823, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2206700841585795, "compression_ratio": 1.6622516556291391, "no_speech_prob": 0.23339776694774628}, {"id": 139, "seek": 151500, "start": 1515.0, "end": 1526.0, "text": " And the kale divergence between two oceans has a super simple functional form, which basically just reduces to to regression.", "tokens": [50364, 400, 264, 34699, 47387, 1296, 732, 25004, 575, 257, 1687, 2199, 11745, 1254, 11, 597, 1936, 445, 18081, 281, 281, 24590, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16329113886906552, "compression_ratio": 1.5602094240837696, "no_speech_prob": 0.04600975289940834}, {"id": 140, "seek": 151500, "start": 1526.0, "end": 1543.0, "text": " So, so we've transformed our like unstructured unsupervised learning problem into into a supervised regression problem. And, and we know how to, we know how to solve those.", "tokens": [50914, 407, 11, 370, 321, 600, 16894, 527, 411, 18799, 46847, 2693, 12879, 24420, 2539, 1154, 666, 666, 257, 46533, 24590, 1154, 13, 400, 11, 293, 321, 458, 577, 281, 11, 321, 458, 577, 281, 5039, 729, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16329113886906552, "compression_ratio": 1.5602094240837696, "no_speech_prob": 0.04600975289940834}, {"id": 141, "seek": 154300, "start": 1543.0, "end": 1561.0, "text": " So this is the hardest part in the whole talk. So I'm going to pause here for like, like 15 seconds.", "tokens": [50364, 407, 341, 307, 264, 13158, 644, 294, 264, 1379, 751, 13, 407, 286, 478, 516, 281, 10465, 510, 337, 411, 11, 411, 2119, 3949, 13, 51264], "temperature": 0.0, "avg_logprob": -0.30283597458240596, "compression_ratio": 1.2301587301587302, "no_speech_prob": 0.12729573249816895}, {"id": 142, "seek": 154300, "start": 1561.0, "end": 1566.0, "text": " Could you give a short refresher on the kale distance.", "tokens": [51264, 7497, 291, 976, 257, 2099, 17368, 511, 322, 264, 34699, 4560, 13, 51514], "temperature": 0.0, "avg_logprob": -0.30283597458240596, "compression_ratio": 1.2301587301587302, "no_speech_prob": 0.12729573249816895}, {"id": 143, "seek": 156600, "start": 1567.0, "end": 1580.0, "text": " The kale distance between two distributions is a information theoretic measurement of how similar the two distributions are to each other.", "tokens": [50414, 440, 34699, 4560, 1296, 732, 37870, 307, 257, 1589, 14308, 299, 13160, 295, 577, 2531, 264, 732, 37870, 366, 281, 1184, 661, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18546184273653252, "compression_ratio": 1.5307692307692307, "no_speech_prob": 0.1622697114944458}, {"id": 144, "seek": 156600, "start": 1580.0, "end": 1585.0, "text": " It has some some nice properties.", "tokens": [51064, 467, 575, 512, 512, 1481, 7221, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18546184273653252, "compression_ratio": 1.5307692307692307, "no_speech_prob": 0.1622697114944458}, {"id": 145, "seek": 156600, "start": 1585.0, "end": 1588.0, "text": " One of those properties is", "tokens": [51314, 1485, 295, 729, 7221, 307, 51464], "temperature": 0.0, "avg_logprob": -0.18546184273653252, "compression_ratio": 1.5307692307692307, "no_speech_prob": 0.1622697114944458}, {"id": 146, "seek": 158800, "start": 1589.0, "end": 1616.0, "text": " So one interpretation of the kale between like Q and P is it tells you how many bits it would, how many, how inefficient, how many bits you would lose if you tried to describe distribution P, but thought it was distribution Q.", "tokens": [50414, 407, 472, 14174, 295, 264, 34699, 1296, 411, 1249, 293, 430, 307, 309, 5112, 291, 577, 867, 9239, 309, 576, 11, 577, 867, 11, 577, 43495, 11, 577, 867, 9239, 291, 576, 3624, 498, 291, 3031, 281, 6786, 7316, 430, 11, 457, 1194, 309, 390, 7316, 1249, 13, 51764], "temperature": 0.0, "avg_logprob": -0.22974863419165978, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.03066512942314148}, {"id": 147, "seek": 161600, "start": 1616.0, "end": 1624.0, "text": " So it tells you like by how many bits your model is like an efficient inefficient.", "tokens": [50364, 407, 309, 5112, 291, 411, 538, 577, 867, 9239, 428, 2316, 307, 411, 364, 7148, 43495, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19293325864351712, "compression_ratio": 1.6914285714285715, "no_speech_prob": 0.02516353130340576}, {"id": 148, "seek": 161600, "start": 1624.0, "end": 1638.0, "text": " It also is like closely connected to log likelihood in that if you take the kale divergence between the data distribution, the model distribution. This is equal to", "tokens": [50764, 467, 611, 307, 411, 8185, 4582, 281, 3565, 22119, 294, 300, 498, 291, 747, 264, 34699, 47387, 1296, 264, 1412, 7316, 11, 264, 2316, 7316, 13, 639, 307, 2681, 281, 51464], "temperature": 0.0, "avg_logprob": -0.19293325864351712, "compression_ratio": 1.6914285714285715, "no_speech_prob": 0.02516353130340576}, {"id": 149, "seek": 161600, "start": 1638.0, "end": 1644.0, "text": " a constant minus the log likelihood of the model.", "tokens": [51464, 257, 5754, 3175, 264, 3565, 22119, 295, 264, 2316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.19293325864351712, "compression_ratio": 1.6914285714285715, "no_speech_prob": 0.02516353130340576}, {"id": 150, "seek": 164400, "start": 1644.0, "end": 1655.0, "text": " But I think just in general you should think of KL as being a measure of distance between two probability distributions.", "tokens": [50364, 583, 286, 519, 445, 294, 2674, 291, 820, 519, 295, 47991, 382, 885, 257, 3481, 295, 4560, 1296, 732, 8482, 37870, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14757570853600135, "compression_ratio": 1.2, "no_speech_prob": 0.004324169829487801}, {"id": 151, "seek": 165500, "start": 1655.0, "end": 1674.0, "text": " So you mentioned the, I guess, beautiful property of the kale distance of two Gaussians can simplify the problem. So what if, you know, there'll be let's say a different type of noise, then I guess it wouldn't necessarily simplify.", "tokens": [50364, 407, 291, 2835, 264, 11, 286, 2041, 11, 2238, 4707, 295, 264, 34699, 4560, 295, 732, 10384, 2023, 2567, 393, 20460, 264, 1154, 13, 407, 437, 498, 11, 291, 458, 11, 456, 603, 312, 718, 311, 584, 257, 819, 2010, 295, 5658, 11, 550, 286, 2041, 309, 2759, 380, 4725, 20460, 13, 51314], "temperature": 0.0, "avg_logprob": -0.25093681471688406, "compression_ratio": 1.4620253164556962, "no_speech_prob": 0.07358147203922272}, {"id": 152, "seek": 167400, "start": 1674.0, "end": 1688.0, "text": " Yeah, yeah, if you had if you had different types of noise, you would, depending on type of noise you very likely would not be able to just like analytically write down the form, you would still be able to optimize it.", "tokens": [50364, 865, 11, 1338, 11, 498, 291, 632, 498, 291, 632, 819, 3467, 295, 5658, 11, 291, 576, 11, 5413, 322, 2010, 295, 5658, 291, 588, 3700, 576, 406, 312, 1075, 281, 445, 411, 10783, 984, 2464, 760, 264, 1254, 11, 291, 576, 920, 312, 1075, 281, 19719, 309, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14570575463967245, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.41066691279411316}, {"id": 153, "seek": 167400, "start": 1688.0, "end": 1695.0, "text": " You would just have to use", "tokens": [51064, 509, 576, 445, 362, 281, 764, 51414], "temperature": 0.0, "avg_logprob": -0.14570575463967245, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.41066691279411316}, {"id": 154, "seek": 169500, "start": 1695.0, "end": 1703.0, "text": " the thing that you can do analytically here is we can marginalize over over X of t minus one", "tokens": [50364, 264, 551, 300, 291, 393, 360, 10783, 984, 510, 307, 321, 393, 16885, 1125, 670, 670, 1783, 295, 256, 3175, 472, 50764], "temperature": 0.0, "avg_logprob": -0.2504333047305836, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.03619660809636116}, {"id": 155, "seek": 169500, "start": 1703.0, "end": 1717.0, "text": " in in this expression and this this I haven't I haven't written down the order of this but but the reason that the calibrating two Gaussians is really nice is because you've been just like marginalized out the X t minus one and that gives you a much lower variance estimate", "tokens": [50764, 294, 294, 341, 6114, 293, 341, 341, 286, 2378, 380, 286, 2378, 380, 3720, 760, 264, 1668, 295, 341, 457, 457, 264, 1778, 300, 264, 2104, 6414, 990, 732, 10384, 2023, 2567, 307, 534, 1481, 307, 570, 291, 600, 668, 445, 411, 32522, 484, 264, 1783, 256, 3175, 472, 293, 300, 2709, 291, 257, 709, 3126, 21977, 12539, 51464], "temperature": 0.0, "avg_logprob": -0.2504333047305836, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.03619660809636116}, {"id": 156, "seek": 171700, "start": 1717.0, "end": 1735.0, "text": " of the loss in the radium. If you use a different form for the noise, then you would probably have to sample X of t minus one, and that would probably be a much higher variance estimate for for your learning signal, but but you could probably", "tokens": [50364, 295, 264, 4470, 294, 264, 2843, 2197, 13, 759, 291, 764, 257, 819, 1254, 337, 264, 5658, 11, 550, 291, 576, 1391, 362, 281, 6889, 1783, 295, 256, 3175, 472, 11, 293, 300, 576, 1391, 312, 257, 709, 2946, 21977, 12539, 337, 337, 428, 2539, 6358, 11, 457, 457, 291, 727, 1391, 51264], "temperature": 0.0, "avg_logprob": -0.13578031909081242, "compression_ratio": 1.6037735849056605, "no_speech_prob": 0.2252586930990219}, {"id": 157, "seek": 171700, "start": 1735.0, "end": 1737.0, "text": " still do it.", "tokens": [51264, 920, 360, 309, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13578031909081242, "compression_ratio": 1.6037735849056605, "no_speech_prob": 0.2252586930990219}, {"id": 158, "seek": 173700, "start": 1737.0, "end": 1741.0, "text": " And something like heavy tail noise might be might be really interesting.", "tokens": [50364, 400, 746, 411, 4676, 6838, 5658, 1062, 312, 1062, 312, 534, 1880, 13, 50564], "temperature": 0.0, "avg_logprob": -0.20084365002520674, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.22794604301452637}, {"id": 159, "seek": 173700, "start": 1741.0, "end": 1747.0, "text": " There's a question for why we do KLQP instead of KLPQ.", "tokens": [50564, 821, 311, 257, 1168, 337, 983, 321, 360, 47991, 48, 47, 2602, 295, 47991, 47, 48, 13, 50864], "temperature": 0.0, "avg_logprob": -0.20084365002520674, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.22794604301452637}, {"id": 160, "seek": 173700, "start": 1747.0, "end": 1751.0, "text": " So,", "tokens": [50864, 407, 11, 51064], "temperature": 0.0, "avg_logprob": -0.20084365002520674, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.22794604301452637}, {"id": 161, "seek": 173700, "start": 1751.0, "end": 1756.0, "text": " in general, we tend to do KL from the data to the model.", "tokens": [51064, 294, 2674, 11, 321, 3928, 281, 360, 47991, 490, 264, 1412, 281, 264, 2316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20084365002520674, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.22794604301452637}, {"id": 162, "seek": 173700, "start": 1756.0, "end": 1763.0, "text": " And the reason for that is probably because of the connection between KL divergence and log likelihood.", "tokens": [51314, 400, 264, 1778, 337, 300, 307, 1391, 570, 295, 264, 4984, 1296, 47991, 47387, 293, 3565, 22119, 13, 51664], "temperature": 0.0, "avg_logprob": -0.20084365002520674, "compression_ratio": 1.4873096446700507, "no_speech_prob": 0.22794604301452637}, {"id": 163, "seek": 176300, "start": 1763.0, "end": 1783.0, "text": " What we're often interested in is the log probability of the data points under model, and that means you have to take an expectation over over the data. So you want to like average over your training data of the difference between the two.", "tokens": [50364, 708, 321, 434, 2049, 3102, 294, 307, 264, 3565, 8482, 295, 264, 1412, 2793, 833, 2316, 11, 293, 300, 1355, 291, 362, 281, 747, 364, 14334, 670, 670, 264, 1412, 13, 407, 291, 528, 281, 411, 4274, 670, 428, 3097, 1412, 295, 264, 2649, 1296, 264, 732, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08863043785095215, "compression_ratio": 1.5126582278481013, "no_speech_prob": 0.02296825498342514}, {"id": 164, "seek": 178300, "start": 1783.0, "end": 1794.0, "text": " I think also, it can be very difficult to compute the other way around, because we don't know what the log like we're trying to fit the log probability of the training data.", "tokens": [50364, 286, 519, 611, 11, 309, 393, 312, 588, 2252, 281, 14722, 264, 661, 636, 926, 11, 570, 321, 500, 380, 458, 437, 264, 3565, 411, 321, 434, 1382, 281, 3318, 264, 3565, 8482, 295, 264, 3097, 1412, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11243648721714213, "compression_ratio": 1.6718146718146718, "no_speech_prob": 0.09530147165060043}, {"id": 165, "seek": 178300, "start": 1794.0, "end": 1811.0, "text": " And if you flip the KL around, then you have terms that look like samples from the model of the log of the data distribution, and we don't know how to evaluate those in general, which is another reason that we usually go from from Q to P using our formulation", "tokens": [50914, 400, 498, 291, 7929, 264, 47991, 926, 11, 550, 291, 362, 2115, 300, 574, 411, 10938, 490, 264, 2316, 295, 264, 3565, 295, 264, 1412, 7316, 11, 293, 321, 500, 380, 458, 577, 281, 13059, 729, 294, 2674, 11, 597, 307, 1071, 1778, 300, 321, 2673, 352, 490, 490, 1249, 281, 430, 1228, 527, 37642, 51764], "temperature": 0.0, "avg_logprob": -0.11243648721714213, "compression_ratio": 1.6718146718146718, "no_speech_prob": 0.09530147165060043}, {"id": 166, "seek": 181100, "start": 1811.0, "end": 1814.0, "text": " here.", "tokens": [50364, 510, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16670559002802923, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.06553103029727936}, {"id": 167, "seek": 181100, "start": 1814.0, "end": 1830.0, "text": " Yeah, so this optimization process, or at least the specific loss that we get depends on us having defined the noise like we chose the noise right like we we chose that the noise is is a bunch of like small Gaussian", "tokens": [50514, 865, 11, 370, 341, 19618, 1399, 11, 420, 412, 1935, 264, 2685, 4470, 300, 321, 483, 5946, 322, 505, 1419, 7642, 264, 5658, 411, 321, 5111, 264, 5658, 558, 411, 321, 321, 5111, 300, 264, 5658, 307, 307, 257, 3840, 295, 411, 1359, 39148, 51314], "temperature": 0.0, "avg_logprob": -0.16670559002802923, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.06553103029727936}, {"id": 168, "seek": 183000, "start": 1830.0, "end": 1851.0, "text": " perturbations. And, and that's what makes the specific form possible is if you use this for your noise than the entire forward trajectory. So like Q of X of one to X capital T given X zero is just one big joint Gaussian, and a lot of things become easier once once your entire forward", "tokens": [50364, 40468, 763, 13, 400, 11, 293, 300, 311, 437, 1669, 264, 2685, 1254, 1944, 307, 498, 291, 764, 341, 337, 428, 5658, 813, 264, 2302, 2128, 21512, 13, 407, 411, 1249, 295, 1783, 295, 472, 281, 1783, 4238, 314, 2212, 1783, 4018, 307, 445, 472, 955, 7225, 39148, 11, 293, 257, 688, 295, 721, 1813, 3571, 1564, 1564, 428, 2302, 2128, 51414], "temperature": 0.0, "avg_logprob": -0.25134042593149036, "compression_ratio": 1.5026455026455026, "no_speech_prob": 0.5424327254295349}, {"id": 169, "seek": 185100, "start": 1851.0, "end": 1864.0, "text": " trajectory is one big joint Gaussian.", "tokens": [50364, 21512, 307, 472, 955, 7225, 39148, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12569104135036469, "compression_ratio": 1.1495327102803738, "no_speech_prob": 0.17988674342632294}, {"id": 170, "seek": 185100, "start": 1864.0, "end": 1871.0, "text": " Cool. Alright, so now I'm going to connect this to stochastic differential equations.", "tokens": [51014, 8561, 13, 2798, 11, 370, 586, 286, 478, 516, 281, 1745, 341, 281, 342, 8997, 2750, 15756, 11787, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12569104135036469, "compression_ratio": 1.1495327102803738, "no_speech_prob": 0.17988674342632294}, {"id": 171, "seek": 187100, "start": 1871.0, "end": 1876.0, "text": " So I just presented this in in discrete time.", "tokens": [50364, 407, 286, 445, 8212, 341, 294, 294, 27706, 565, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08104746043682098, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.03620534390211105}, {"id": 172, "seek": 187100, "start": 1876.0, "end": 1890.0, "text": " But if we take the step sizes to be smaller and smaller and smaller, then we can turn this discrete diffusion process into a limiting stochastic differential equation.", "tokens": [50614, 583, 498, 321, 747, 264, 1823, 11602, 281, 312, 4356, 293, 4356, 293, 4356, 11, 550, 321, 393, 1261, 341, 27706, 25242, 1399, 666, 257, 22083, 342, 8997, 2750, 15756, 5367, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08104746043682098, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.03620534390211105}, {"id": 173, "seek": 189000, "start": 1890.0, "end": 1905.0, "text": " It turns out to be extremely useful and to have some some very nice properties as as before, we're going to gradually mix noise into our data distribution until it turns into Gaussian.", "tokens": [50364, 467, 4523, 484, 281, 312, 4664, 4420, 293, 281, 362, 512, 512, 588, 1481, 7221, 382, 382, 949, 11, 321, 434, 516, 281, 13145, 2890, 5658, 666, 527, 1412, 7316, 1826, 309, 4523, 666, 39148, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15925316312419835, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.09531044960021973}, {"id": 174, "seek": 189000, "start": 1905.0, "end": 1913.0, "text": " And, and as I just said, this is the continuous time limit of the discrete diffusion process I saw the moment ago.", "tokens": [51114, 400, 11, 293, 382, 286, 445, 848, 11, 341, 307, 264, 10957, 565, 4948, 295, 264, 27706, 25242, 1399, 286, 1866, 264, 1623, 2057, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15925316312419835, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.09531044960021973}, {"id": 175, "seek": 191300, "start": 1913.0, "end": 1921.0, "text": " So here our data distribution is P sub zero. This is a slight change in notation, not a big one.", "tokens": [50364, 407, 510, 527, 1412, 7316, 307, 430, 1422, 4018, 13, 639, 307, 257, 4036, 1319, 294, 24657, 11, 406, 257, 955, 472, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15289256821817426, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.026751095429062843}, {"id": 176, "seek": 191300, "start": 1921.0, "end": 1939.0, "text": " And P sub T is the intermediate distribution from running this stochastic process for a time interval T, and then P capital T is the final final distribution which should look like just noise.", "tokens": [50764, 400, 430, 1422, 314, 307, 264, 19376, 7316, 490, 2614, 341, 342, 8997, 2750, 1399, 337, 257, 565, 15035, 314, 11, 293, 550, 430, 4238, 314, 307, 264, 2572, 2572, 7316, 597, 820, 574, 411, 445, 5658, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15289256821817426, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.026751095429062843}, {"id": 177, "seek": 193900, "start": 1939.0, "end": 1951.0, "text": " So and little T here is between zero and big T. And so an SDE is a generalization of ordinary differential equations.", "tokens": [50364, 407, 293, 707, 314, 510, 307, 1296, 4018, 293, 955, 314, 13, 400, 370, 364, 14638, 36, 307, 257, 2674, 2144, 295, 10547, 15756, 11787, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19620379423483825, "compression_ratio": 1.555, "no_speech_prob": 0.017705043777823448}, {"id": 178, "seek": 193900, "start": 1951.0, "end": 1959.0, "text": " And so we can write it down like this. And here the green term is what you would normally have in an ODE.", "tokens": [50964, 400, 370, 321, 393, 2464, 309, 760, 411, 341, 13, 400, 510, 264, 3092, 1433, 307, 437, 291, 576, 5646, 362, 294, 364, 422, 22296, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19620379423483825, "compression_ratio": 1.555, "no_speech_prob": 0.017705043777823448}, {"id": 179, "seek": 193900, "start": 1959.0, "end": 1964.0, "text": " And it governs the deterministic properties of the stochastic process select the drift.", "tokens": [51364, 400, 309, 1980, 82, 264, 15957, 3142, 7221, 295, 264, 342, 8997, 2750, 1399, 3048, 264, 19699, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19620379423483825, "compression_ratio": 1.555, "no_speech_prob": 0.017705043777823448}, {"id": 180, "seek": 196400, "start": 1964.0, "end": 1973.0, "text": " And the red term is the noise that the red term controls like the stochastic fluctuations of the process.", "tokens": [50364, 400, 264, 2182, 1433, 307, 264, 5658, 300, 264, 2182, 1433, 9003, 411, 264, 342, 8997, 2750, 45276, 295, 264, 1399, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09595554404788548, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.016399869695305824}, {"id": 181, "seek": 196400, "start": 1973.0, "end": 1984.0, "text": " So here you should think of DW as being infinitesimal Gaussian noise or like Brownian motion.", "tokens": [50814, 407, 510, 291, 820, 519, 295, 45318, 382, 885, 7193, 3324, 10650, 39148, 5658, 420, 411, 8030, 952, 5394, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09595554404788548, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.016399869695305824}, {"id": 182, "seek": 196400, "start": 1984.0, "end": 1992.0, "text": " So we've now replaced the forward process with its continuous time limit, which is stochastic differential equation.", "tokens": [51364, 407, 321, 600, 586, 10772, 264, 2128, 1399, 365, 1080, 10957, 565, 4948, 11, 597, 307, 342, 8997, 2750, 15756, 5367, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09595554404788548, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.016399869695305824}, {"id": 183, "seek": 199200, "start": 1992.0, "end": 1996.0, "text": " And we can do the same thing with the reverse process.", "tokens": [50364, 400, 321, 393, 360, 264, 912, 551, 365, 264, 9943, 1399, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1204721208602663, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.016146143898367882}, {"id": 184, "seek": 199200, "start": 1996.0, "end": 2010.0, "text": " One really surprising aspect of stochastic differential equations is that given their equation forward of time and given the marginal distribution PT at time T,", "tokens": [50564, 1485, 534, 8830, 4171, 295, 342, 8997, 2750, 15756, 11787, 307, 300, 2212, 641, 5367, 2128, 295, 565, 293, 2212, 264, 16885, 7316, 430, 51, 412, 565, 314, 11, 51264], "temperature": 0.0, "avg_logprob": -0.1204721208602663, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.016146143898367882}, {"id": 185, "seek": 199200, "start": 2010.0, "end": 2017.0, "text": " the time reversal of the SDE has a very simple analytic form.", "tokens": [51264, 264, 565, 42778, 295, 264, 14638, 36, 575, 257, 588, 2199, 40358, 1254, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1204721208602663, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.016146143898367882}, {"id": 186, "seek": 201700, "start": 2017.0, "end": 2026.0, "text": " In reverse SDE, here DT is going to be an infinitesimal negative time step.", "tokens": [50364, 682, 9943, 14638, 36, 11, 510, 413, 51, 307, 516, 281, 312, 364, 7193, 3324, 10650, 3671, 565, 1823, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1700257807970047, "compression_ratio": 1.888, "no_speech_prob": 0.03731602802872658}, {"id": 187, "seek": 201700, "start": 2026.0, "end": 2031.0, "text": " And DT is going to be an infinitesimal negative time step.", "tokens": [50814, 400, 413, 51, 307, 516, 281, 312, 364, 7193, 3324, 10650, 3671, 565, 1823, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1700257807970047, "compression_ratio": 1.888, "no_speech_prob": 0.03731602802872658}, {"id": 188, "seek": 201700, "start": 2031.0, "end": 2041.0, "text": " And DW is still Brownian motion or DW is still Brownian motion or like little Gaussian perturbations.", "tokens": [51064, 400, 45318, 307, 920, 8030, 952, 5394, 420, 45318, 307, 920, 8030, 952, 5394, 420, 411, 707, 39148, 40468, 763, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1700257807970047, "compression_ratio": 1.888, "no_speech_prob": 0.03731602802872658}, {"id": 189, "seek": 204100, "start": 2041.0, "end": 2056.0, "text": " And if we know the gradient of log PFT with respect to X, then that's all the information we need to like define the reverse SDE, the time reversal of the SDE.", "tokens": [50364, 400, 498, 321, 458, 264, 16235, 295, 3565, 430, 25469, 365, 3104, 281, 1783, 11, 550, 300, 311, 439, 264, 1589, 321, 643, 281, 411, 6964, 264, 9943, 14638, 36, 11, 264, 565, 42778, 295, 264, 14638, 36, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11107785151554989, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.008709324523806572}, {"id": 190, "seek": 204100, "start": 2056.0, "end": 2065.0, "text": " And here the gradient of log PFT is the quantity which is often known as the score function.", "tokens": [51114, 400, 510, 264, 16235, 295, 3565, 430, 25469, 307, 264, 11275, 597, 307, 2049, 2570, 382, 264, 6175, 2445, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11107785151554989, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.008709324523806572}, {"id": 191, "seek": 206500, "start": 2065.0, "end": 2074.0, "text": " And so we can train an approximation S of theta to this score function.", "tokens": [50364, 400, 370, 321, 393, 3847, 364, 28023, 318, 295, 9725, 281, 341, 6175, 2445, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15868546827784125, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.007458992302417755}, {"id": 192, "seek": 206500, "start": 2074.0, "end": 2087.0, "text": " And the way in which you train this is using the continuous time limit of the same variational down to the log likelihood I showed like in the previous section.", "tokens": [50814, 400, 264, 636, 294, 597, 291, 3847, 341, 307, 1228, 264, 10957, 565, 4948, 295, 264, 912, 3034, 1478, 760, 281, 264, 3565, 22119, 286, 4712, 411, 294, 264, 3894, 3541, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15868546827784125, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.007458992302417755}, {"id": 193, "seek": 208700, "start": 2087.0, "end": 2097.0, "text": " So basically you can train this thing exactly the same way that you train the discrete time version.", "tokens": [50364, 407, 1936, 291, 393, 3847, 341, 551, 2293, 264, 912, 636, 300, 291, 3847, 264, 27706, 565, 3037, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15360738520036665, "compression_ratio": 1.570469798657718, "no_speech_prob": 0.09398229420185089}, {"id": 194, "seek": 208700, "start": 2097.0, "end": 2103.0, "text": " There's also connection to denoising score matching, but I don't think it really matters here.", "tokens": [50864, 821, 311, 611, 4984, 281, 1441, 78, 3436, 6175, 14324, 11, 457, 286, 500, 380, 519, 309, 534, 7001, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15360738520036665, "compression_ratio": 1.570469798657718, "no_speech_prob": 0.09398229420185089}, {"id": 195, "seek": 208700, "start": 2103.0, "end": 2113.0, "text": " You can train this using the same way.", "tokens": [51164, 509, 393, 3847, 341, 1228, 264, 912, 636, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15360738520036665, "compression_ratio": 1.570469798657718, "no_speech_prob": 0.09398229420185089}, {"id": 196, "seek": 211300, "start": 2113.0, "end": 2122.0, "text": " So okay, so this is just the continuous time limit of the discrete time diffusion for that version before.", "tokens": [50364, 407, 1392, 11, 370, 341, 307, 445, 264, 10957, 565, 4948, 295, 264, 27706, 565, 25242, 337, 300, 3037, 949, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1556631508520094, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.019711904227733612}, {"id": 197, "seek": 211300, "start": 2122.0, "end": 2136.0, "text": " And maybe the thing that's really neat about it is we can now link these like drift terms that we were estimating to this thing called a score function,", "tokens": [50814, 400, 1310, 264, 551, 300, 311, 534, 10654, 466, 309, 307, 321, 393, 586, 2113, 613, 411, 19699, 2115, 300, 321, 645, 8017, 990, 281, 341, 551, 1219, 257, 6175, 2445, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1556631508520094, "compression_ratio": 1.505813953488372, "no_speech_prob": 0.019711904227733612}, {"id": 198, "seek": 213600, "start": 2136.0, "end": 2146.0, "text": " which is like a simple and known property of the distribution PFT.", "tokens": [50364, 597, 307, 411, 257, 2199, 293, 2570, 4707, 295, 264, 7316, 430, 25469, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11272339082100022, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.12246747314929962}, {"id": 199, "seek": 213600, "start": 2146.0, "end": 2151.0, "text": " Okay, and so now we've defined the model two different ways.", "tokens": [50864, 1033, 11, 293, 370, 586, 321, 600, 7642, 264, 2316, 732, 819, 2098, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11272339082100022, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.12246747314929962}, {"id": 200, "seek": 213600, "start": 2151.0, "end": 2153.0, "text": " And we've talked about how to train the model.", "tokens": [51114, 400, 321, 600, 2825, 466, 577, 281, 3847, 264, 2316, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11272339082100022, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.12246747314929962}, {"id": 201, "seek": 213600, "start": 2153.0, "end": 2156.0, "text": " So let's talk about how to sample.", "tokens": [51214, 407, 718, 311, 751, 466, 577, 281, 6889, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11272339082100022, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.12246747314929962}, {"id": 202, "seek": 213600, "start": 2156.0, "end": 2164.0, "text": " And then let's look at some some pretty samples.", "tokens": [51364, 400, 550, 718, 311, 574, 412, 512, 512, 1238, 10938, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11272339082100022, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.12246747314929962}, {"id": 203, "seek": 216400, "start": 2164.0, "end": 2170.0, "text": " So after training our model, I mean the sweet timer score base, I'm going to show you a score base here.", "tokens": [50364, 407, 934, 3097, 527, 2316, 11, 286, 914, 264, 3844, 19247, 6175, 3096, 11, 286, 478, 516, 281, 855, 291, 257, 6175, 3096, 510, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3329176298329528, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.003123011440038681}, {"id": 204, "seek": 216400, "start": 2170.0, "end": 2178.0, "text": " So score base, we generate samples by, by just numerically integrating the reverse SD.", "tokens": [50664, 407, 6175, 3096, 11, 321, 8460, 10938, 538, 11, 538, 445, 7866, 984, 26889, 264, 9943, 14638, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3329176298329528, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.003123011440038681}, {"id": 205, "seek": 216400, "start": 2178.0, "end": 2186.0, "text": " So, so mind you, we've approximated grading of log PFT with our function s.", "tokens": [51064, 407, 11, 370, 1575, 291, 11, 321, 600, 8542, 770, 35540, 295, 3565, 430, 25469, 365, 527, 2445, 262, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3329176298329528, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.003123011440038681}, {"id": 206, "seek": 218600, "start": 2186.0, "end": 2192.0, "text": " And you can use any off the shelf SD integrator to solve this.", "tokens": [50364, 400, 291, 393, 764, 604, 766, 264, 15222, 14638, 3572, 1639, 281, 5039, 341, 13, 50664], "temperature": 0.0, "avg_logprob": -0.18994604839998133, "compression_ratio": 1.4971098265895955, "no_speech_prob": 0.12247376143932343}, {"id": 207, "seek": 218600, "start": 2192.0, "end": 2200.0, "text": " The simplest of these is, is called the Euler-Mariama solver.", "tokens": [50664, 440, 22811, 295, 613, 307, 11, 307, 1219, 264, 462, 26318, 12, 44, 3504, 2404, 1404, 331, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18994604839998133, "compression_ratio": 1.4971098265895955, "no_speech_prob": 0.12247376143932343}, {"id": 208, "seek": 218600, "start": 2200.0, "end": 2212.0, "text": " And this is just the, the discrete time discretization of this SD, which actually maps us back to the discrete time diffusion process,", "tokens": [51064, 400, 341, 307, 445, 264, 11, 264, 27706, 565, 25656, 2144, 295, 341, 14638, 11, 597, 767, 11317, 505, 646, 281, 264, 27706, 565, 25242, 1399, 11, 51664], "temperature": 0.0, "avg_logprob": -0.18994604839998133, "compression_ratio": 1.4971098265895955, "no_speech_prob": 0.12247376143932343}, {"id": 209, "seek": 221200, "start": 2212.0, "end": 2227.0, "text": " where at every, at every time step, you like change X, you change your sample by that looks like this term times a finite changing T.", "tokens": [50364, 689, 412, 633, 11, 412, 633, 565, 1823, 11, 291, 411, 1319, 1783, 11, 291, 1319, 428, 6889, 538, 300, 1542, 411, 341, 1433, 1413, 257, 19362, 4473, 314, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2026611328125, "compression_ratio": 1.4676258992805755, "no_speech_prob": 0.02297259494662285}, {"id": 210, "seek": 221200, "start": 2227.0, "end": 2235.0, "text": " And then you add a little noise with variance proportional to delta T.", "tokens": [51114, 400, 550, 291, 909, 257, 707, 5658, 365, 21977, 24969, 281, 8289, 314, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2026611328125, "compression_ratio": 1.4676258992805755, "no_speech_prob": 0.02297259494662285}, {"id": 211, "seek": 223500, "start": 2235.0, "end": 2244.0, "text": " And you just run this over and over and over again, until you get back a sample.", "tokens": [50364, 400, 291, 445, 1190, 341, 670, 293, 670, 293, 670, 797, 11, 1826, 291, 483, 646, 257, 6889, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09906457332854575, "compression_ratio": 1.3277310924369747, "no_speech_prob": 0.02095937356352806}, {"id": 212, "seek": 223500, "start": 2244.0, "end": 2253.0, "text": " So this is, this is the, maybe the most naive discretization of the SD.", "tokens": [50814, 407, 341, 307, 11, 341, 307, 264, 11, 1310, 264, 881, 29052, 25656, 2144, 295, 264, 14638, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09906457332854575, "compression_ratio": 1.3277310924369747, "no_speech_prob": 0.02095937356352806}, {"id": 213, "seek": 223500, "start": 2253.0, "end": 2257.0, "text": " Okay.", "tokens": [51264, 1033, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09906457332854575, "compression_ratio": 1.3277310924369747, "no_speech_prob": 0.02095937356352806}, {"id": 214, "seek": 225700, "start": 2258.0, "end": 2274.0, "text": " If you maybe went at the sample generation procedure the right way, then, then we're actually training and generating samples with a neural network, which is like thousands of layers deep, where each layer is like a time step into the diffusion process.", "tokens": [50414, 759, 291, 1310, 1437, 412, 264, 6889, 5125, 10747, 264, 558, 636, 11, 550, 11, 550, 321, 434, 767, 3097, 293, 17746, 10938, 365, 257, 18161, 3209, 11, 597, 307, 411, 5383, 295, 7914, 2452, 11, 689, 1184, 4583, 307, 411, 257, 565, 1823, 666, 264, 25242, 1399, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1440688388448366, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.01854201778769493}, {"id": 215, "seek": 225700, "start": 2274.0, "end": 2281.0, "text": " And, and so this can be interpreted as like an extraordinarily deep general model.", "tokens": [51214, 400, 11, 293, 370, 341, 393, 312, 26749, 382, 411, 364, 34557, 2452, 2674, 2316, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1440688388448366, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.01854201778769493}, {"id": 216, "seek": 228100, "start": 2281.0, "end": 2295.0, "text": " If you squint that at a slightly different length, then we're proposing a general model, which has like thousands of times the compute cost of most general models, which is maybe not quite as exciting as saying thousands of layers deep.", "tokens": [50364, 759, 291, 2339, 686, 300, 412, 257, 4748, 819, 4641, 11, 550, 321, 434, 29939, 257, 2674, 2316, 11, 597, 575, 411, 5383, 295, 1413, 264, 14722, 2063, 295, 881, 2674, 5245, 11, 597, 307, 1310, 406, 1596, 382, 4670, 382, 1566, 5383, 295, 7914, 2452, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15872297741117933, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.027580169960856438}, {"id": 217, "seek": 228100, "start": 2295.0, "end": 2302.0, "text": " But, but it will turn out there are ways to make, to make the sampling process more efficient.", "tokens": [51064, 583, 11, 457, 309, 486, 1261, 484, 456, 366, 2098, 281, 652, 11, 281, 652, 264, 21179, 1399, 544, 7148, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15872297741117933, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.027580169960856438}, {"id": 218, "seek": 228100, "start": 2302.0, "end": 2306.0, "text": " And I'm going to show you one of them in the talk.", "tokens": [51414, 400, 286, 478, 516, 281, 855, 291, 472, 295, 552, 294, 264, 751, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15872297741117933, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.027580169960856438}, {"id": 219, "seek": 228100, "start": 2306.0, "end": 2310.0, "text": " Okay, so, so what can you do with this thing?", "tokens": [51614, 1033, 11, 370, 11, 370, 437, 393, 291, 360, 365, 341, 551, 30, 51814], "temperature": 0.0, "avg_logprob": -0.15872297741117933, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.027580169960856438}, {"id": 220, "seek": 231000, "start": 2310.0, "end": 2314.0, "text": " Well, here are some example samples.", "tokens": [50364, 1042, 11, 510, 366, 512, 1365, 10938, 13, 50564], "temperature": 0.0, "avg_logprob": -0.18767068602822043, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0011333986185491085}, {"id": 221, "seek": 231000, "start": 2314.0, "end": 2321.0, "text": " These are samples from a diffusion model that we built trained on celebe HQ.", "tokens": [50564, 1981, 366, 10938, 490, 257, 25242, 2316, 300, 321, 3094, 8895, 322, 43165, 650, 43209, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18767068602822043, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0011333986185491085}, {"id": 222, "seek": 231000, "start": 2321.0, "end": 2325.0, "text": " These are 1024 by 1024 images.", "tokens": [50914, 1981, 366, 1266, 7911, 538, 1266, 7911, 5267, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18767068602822043, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0011333986185491085}, {"id": 223, "seek": 231000, "start": 2325.0, "end": 2332.0, "text": " I don't know how high resolution they are after after some to me, these are like in this English full from from real human beings.", "tokens": [51114, 286, 500, 380, 458, 577, 1090, 8669, 436, 366, 934, 934, 512, 281, 385, 11, 613, 366, 411, 294, 341, 3669, 1577, 490, 490, 957, 1952, 8958, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18767068602822043, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0011333986185491085}, {"id": 224, "seek": 231000, "start": 2332.0, "end": 2339.0, "text": " So, so, so we've like crossed over the uncanny valley.", "tokens": [51464, 407, 11, 370, 11, 370, 321, 600, 411, 14622, 670, 264, 6219, 11612, 17636, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18767068602822043, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.0011333986185491085}, {"id": 225, "seek": 233900, "start": 2339.0, "end": 2348.0, "text": " So, numerically, this class of models currently beats autoregressive models.", "tokens": [50364, 407, 11, 7866, 984, 11, 341, 1508, 295, 5245, 4362, 16447, 1476, 418, 3091, 488, 5245, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3279464320132607, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.004330032039433718}, {"id": 226, "seek": 233900, "start": 2348.0, "end": 2357.0, "text": " In terms of log likelihood where autoregressive models were the winners. This is not my work. This is a paper of dirt King was in Tim Solomon's.", "tokens": [50814, 682, 2115, 295, 3565, 22119, 689, 1476, 418, 3091, 488, 5245, 645, 264, 17193, 13, 639, 307, 406, 452, 589, 13, 639, 307, 257, 3035, 295, 11483, 3819, 390, 294, 7172, 32209, 311, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3279464320132607, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.004330032039433718}, {"id": 227, "seek": 233900, "start": 2357.0, "end": 2368.0, "text": " They also began in terms of like at by the inception score on some data sets. So, for instance, image that five full by five fall, again, not my work.", "tokens": [51264, 814, 611, 4283, 294, 2115, 295, 411, 412, 538, 264, 49834, 6175, 322, 512, 1412, 6352, 13, 407, 11, 337, 5197, 11, 3256, 300, 1732, 1577, 538, 1732, 2100, 11, 797, 11, 406, 452, 589, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3279464320132607, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.004330032039433718}, {"id": 228, "seek": 236800, "start": 2368.0, "end": 2377.0, "text": " So these things seem to be remarkably good generative models of images.", "tokens": [50364, 407, 613, 721, 1643, 281, 312, 37381, 665, 1337, 1166, 5245, 295, 5267, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14856235504150392, "compression_ratio": 1.4428571428571428, "no_speech_prob": 0.008313843980431557}, {"id": 229, "seek": 236800, "start": 2377.0, "end": 2386.0, "text": " They also enable you to do some cool things that you can't do with with other other techniques.", "tokens": [50814, 814, 611, 9528, 291, 281, 360, 512, 1627, 721, 300, 291, 393, 380, 360, 365, 365, 661, 661, 7512, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14856235504150392, "compression_ratio": 1.4428571428571428, "no_speech_prob": 0.008313843980431557}, {"id": 230, "seek": 236800, "start": 2386.0, "end": 2389.0, "text": " Let me actually just to time this.", "tokens": [51264, 961, 385, 767, 445, 281, 565, 341, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14856235504150392, "compression_ratio": 1.4428571428571428, "no_speech_prob": 0.008313843980431557}, {"id": 231, "seek": 238900, "start": 2389.0, "end": 2397.0, "text": " Should I expect to end sharply attend or should I expect to take them extra like.", "tokens": [50364, 6454, 286, 2066, 281, 917, 42893, 6888, 420, 820, 286, 2066, 281, 747, 552, 2857, 411, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22247170690280288, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.20410007238388062}, {"id": 232, "seek": 238900, "start": 2397.0, "end": 2399.0, "text": " Yeah, take your time.", "tokens": [50764, 865, 11, 747, 428, 565, 13, 50864], "temperature": 0.0, "avg_logprob": -0.22247170690280288, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.20410007238388062}, {"id": 233, "seek": 238900, "start": 2399.0, "end": 2414.0, "text": " You know, as much as you would like because typically we end this course, even after you are done with your lecture, we may stay a little more and chat about things.", "tokens": [50864, 509, 458, 11, 382, 709, 382, 291, 576, 411, 570, 5850, 321, 917, 341, 1164, 11, 754, 934, 291, 366, 1096, 365, 428, 7991, 11, 321, 815, 1754, 257, 707, 544, 293, 5081, 466, 721, 13, 51614], "temperature": 0.0, "avg_logprob": -0.22247170690280288, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.20410007238388062}, {"id": 234, "seek": 241400, "start": 2415.0, "end": 2424.0, "text": " Okay, cool. I will not take forever, but I will I will not try to do to the last last slides in four minutes as well.", "tokens": [50414, 1033, 11, 1627, 13, 286, 486, 406, 747, 5680, 11, 457, 286, 486, 286, 486, 406, 853, 281, 360, 281, 264, 1036, 1036, 9788, 294, 1451, 2077, 382, 731, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2138085057658534, "compression_ratio": 1.4935064935064934, "no_speech_prob": 0.12413424998521805}, {"id": 235, "seek": 241400, "start": 2424.0, "end": 2441.0, "text": " I'm actually it's probably really good time for me to pause for a second and see if there are any questions. So,", "tokens": [50864, 286, 478, 767, 309, 311, 1391, 534, 665, 565, 337, 385, 281, 10465, 337, 257, 1150, 293, 536, 498, 456, 366, 604, 1651, 13, 407, 11, 51714], "temperature": 0.0, "avg_logprob": -0.2138085057658534, "compression_ratio": 1.4935064935064934, "no_speech_prob": 0.12413424998521805}, {"id": 236, "seek": 244100, "start": 2441.0, "end": 2449.0, "text": " Okay, I'm going to keep on going. I'm going to tell you about these stuff.", "tokens": [50364, 1033, 11, 286, 478, 516, 281, 1066, 322, 516, 13, 286, 478, 516, 281, 980, 291, 466, 613, 1507, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2338204002380371, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.013839327730238438}, {"id": 237, "seek": 244100, "start": 2449.0, "end": 2464.0, "text": " One really cool thing is that any SD can be transformed into a corresponding ordinary differential equations non stochastic differential equation.", "tokens": [50764, 1485, 534, 1627, 551, 307, 300, 604, 14638, 393, 312, 16894, 666, 257, 11760, 10547, 15756, 11787, 2107, 342, 8997, 2750, 15756, 5367, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2338204002380371, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.013839327730238438}, {"id": 238, "seek": 246400, "start": 2464.0, "end": 2477.0, "text": " Without changing the marginal distributions pt of x, that is, there is an ODE, which has the same distribution over x at all times t.", "tokens": [50364, 9129, 4473, 264, 16885, 37870, 280, 83, 295, 2031, 11, 300, 307, 11, 456, 307, 364, 422, 22296, 11, 597, 575, 264, 912, 7316, 670, 2031, 412, 439, 1413, 256, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2457393872535835, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.1558685153722763}, {"id": 239, "seek": 246400, "start": 2477.0, "end": 2485.0, "text": " And so this corresponding ODE would allow us to sample from the same distribution starting from the same prior distribution.", "tokens": [51014, 400, 370, 341, 11760, 422, 22296, 576, 2089, 505, 281, 6889, 490, 264, 912, 7316, 2891, 490, 264, 912, 4059, 7316, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2457393872535835, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.1558685153722763}, {"id": 240, "seek": 248500, "start": 2485.0, "end": 2492.0, "text": " But by solving ODE instead of an SD, this is this is here is what the ODE looks like.", "tokens": [50364, 583, 538, 12606, 422, 22296, 2602, 295, 364, 14638, 11, 341, 307, 341, 307, 510, 307, 437, 264, 422, 22296, 1542, 411, 13, 50714], "temperature": 0.0, "avg_logprob": -0.23028526884136777, "compression_ratio": 1.5369127516778522, "no_speech_prob": 0.3168865740299225}, {"id": 241, "seek": 248500, "start": 2492.0, "end": 2503.0, "text": " We show the SDH trajectories in red, and the ODE trajectories in, in white, and they both the SD and the ODE are starting from the same points.", "tokens": [50714, 492, 855, 264, 14638, 39, 18257, 2083, 294, 2182, 11, 293, 264, 422, 22296, 18257, 2083, 294, 11, 294, 2418, 11, 293, 436, 1293, 264, 14638, 293, 264, 422, 22296, 366, 2891, 490, 264, 912, 2793, 13, 51264], "temperature": 0.0, "avg_logprob": -0.23028526884136777, "compression_ratio": 1.5369127516778522, "no_speech_prob": 0.3168865740299225}, {"id": 242, "seek": 250300, "start": 2503.0, "end": 2515.0, "text": " And you can see that the SD is a stochastic trajectory that converts the starting distribution final distribution easy the ODE.", "tokens": [50364, 400, 291, 393, 536, 300, 264, 14638, 307, 257, 342, 8997, 2750, 21512, 300, 38874, 264, 2891, 7316, 2572, 7316, 1858, 264, 422, 22296, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12851513607401244, "compression_ratio": 1.7379679144385027, "no_speech_prob": 0.12926320731639862}, {"id": 243, "seek": 250300, "start": 2515.0, "end": 2532.0, "text": " Similarly, trace has the same marginal distribution at every time point, so it starts from the same distribution, and then it ends up the same distribution, but it does this in a deterministic way.", "tokens": [50964, 13157, 11, 13508, 575, 264, 912, 16885, 7316, 412, 633, 565, 935, 11, 370, 309, 3719, 490, 264, 912, 7316, 11, 293, 550, 309, 5314, 493, 264, 912, 7316, 11, 457, 309, 775, 341, 294, 257, 15957, 3142, 636, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12851513607401244, "compression_ratio": 1.7379679144385027, "no_speech_prob": 0.12926320731639862}, {"id": 244, "seek": 253200, "start": 2532.0, "end": 2542.0, "text": " Given the SDE, the corresponding ODE, this is just the general general relationship between SDEs and ODE's.", "tokens": [50364, 18600, 264, 14638, 36, 11, 264, 11760, 422, 22296, 11, 341, 307, 445, 264, 2674, 2674, 2480, 1296, 14638, 20442, 293, 422, 22296, 311, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17977151605818006, "compression_ratio": 1.7604790419161678, "no_speech_prob": 0.023306749761104584}, {"id": 245, "seek": 253200, "start": 2542.0, "end": 2557.0, "text": " Given the SDE, the corresponding ODE, like once again only depends on the score function of pt, for which we already learned, we've already learned the estimator for the score functions.", "tokens": [50864, 18600, 264, 14638, 36, 11, 264, 11760, 422, 22296, 11, 411, 1564, 797, 787, 5946, 322, 264, 6175, 2445, 295, 280, 83, 11, 337, 597, 321, 1217, 3264, 11, 321, 600, 1217, 3264, 264, 8017, 1639, 337, 264, 6175, 6828, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17977151605818006, "compression_ratio": 1.7604790419161678, "no_speech_prob": 0.023306749761104584}, {"id": 246, "seek": 255700, "start": 2557.0, "end": 2578.0, "text": " So, so this is just S of, S of theta. So, if we want to generate samples, we can generate samples by integrating this ODE instead of by integrating the SDE.", "tokens": [50364, 407, 11, 370, 341, 307, 445, 318, 295, 11, 318, 295, 9725, 13, 407, 11, 498, 321, 528, 281, 8460, 10938, 11, 321, 393, 8460, 10938, 538, 26889, 341, 422, 22296, 2602, 295, 538, 26889, 264, 14638, 36, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1725150041801985, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.10208400338888168}, {"id": 247, "seek": 257800, "start": 2578.0, "end": 2586.0, "text": " This kind of blew my, so Yang is the one who realized you could do this, and this kind of completely blew my mind when he shared that we could do this.", "tokens": [50364, 639, 733, 295, 19075, 452, 11, 370, 11978, 307, 264, 472, 567, 5334, 291, 727, 360, 341, 11, 293, 341, 733, 295, 2584, 19075, 452, 1575, 562, 415, 5507, 300, 321, 727, 360, 341, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16919452196931187, "compression_ratio": 1.5617977528089888, "no_speech_prob": 0.4834507703781128}, {"id": 248, "seek": 257800, "start": 2586.0, "end": 2607.0, "text": " So, so I hope, I hope at least some fraction of you are similarly like scandalized that you can turn SDEs and ODE's like this.", "tokens": [50764, 407, 11, 370, 286, 1454, 11, 286, 1454, 412, 1935, 512, 14135, 295, 291, 366, 14138, 411, 27922, 1602, 300, 291, 393, 1261, 14638, 20442, 293, 422, 22296, 311, 411, 341, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16919452196931187, "compression_ratio": 1.5617977528089888, "no_speech_prob": 0.4834507703781128}, {"id": 249, "seek": 260700, "start": 2607.0, "end": 2626.0, "text": " So, the question is, are there benefits to modeling with an SDE if there is an ODE equivalent, and the, the two parts to, to the answer.", "tokens": [50364, 407, 11, 264, 1168, 307, 11, 366, 456, 5311, 281, 15983, 365, 364, 14638, 36, 498, 456, 307, 364, 422, 22296, 10344, 11, 293, 264, 11, 264, 732, 3166, 281, 11, 281, 264, 1867, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1580047974219689, "compression_ratio": 1.2710280373831775, "no_speech_prob": 0.030179549008607864}, {"id": 250, "seek": 262600, "start": 2627.0, "end": 2639.0, "text": " Part number one is the SDE formulation is what allows us to train it. So, so we have to at least like conceptually go through, through SDE space in order to train it.", "tokens": [50414, 4100, 1230, 472, 307, 264, 14638, 36, 37642, 307, 437, 4045, 505, 281, 3847, 309, 13, 407, 11, 370, 321, 362, 281, 412, 1935, 411, 3410, 671, 352, 807, 11, 807, 14638, 36, 1901, 294, 1668, 281, 3847, 309, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10288811813701283, "compression_ratio": 1.3387096774193548, "no_speech_prob": 0.1643795520067215}, {"id": 251, "seek": 263900, "start": 2639.0, "end": 2657.0, "text": " Part number two is that we're training this function S theta to match the score function, but in practice, this function S of theta probably does not correspond to the gradient of any well defined like log probability distribution.", "tokens": [50364, 4100, 1230, 732, 307, 300, 321, 434, 3097, 341, 2445, 318, 9725, 281, 2995, 264, 6175, 2445, 11, 457, 294, 3124, 11, 341, 2445, 318, 295, 9725, 1391, 775, 406, 6805, 281, 264, 16235, 295, 604, 731, 7642, 411, 3565, 8482, 7316, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10562677586332281, "compression_ratio": 1.5298013245033113, "no_speech_prob": 0.025173312053084373}, {"id": 252, "seek": 265700, "start": 2657.0, "end": 2668.0, "text": " Like if we were to train S of theta perfectly, then we would have S of theta is equal to like gradient of X of log P of t, but in actuality S of theta, it's just like a vector.", "tokens": [50364, 1743, 498, 321, 645, 281, 3847, 318, 295, 9725, 6239, 11, 550, 321, 576, 362, 318, 295, 9725, 307, 2681, 281, 411, 16235, 295, 1783, 295, 3565, 430, 295, 256, 11, 457, 294, 3539, 507, 318, 295, 9725, 11, 309, 311, 445, 411, 257, 8062, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15415341513497488, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.0330786295235157}, {"id": 253, "seek": 265700, "start": 2668.0, "end": 2675.0, "text": " And it probably is not a vector that actually corresponds to the gradient of the log probability.", "tokens": [50914, 400, 309, 1391, 307, 406, 257, 8062, 300, 767, 23249, 281, 264, 16235, 295, 264, 3565, 8482, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15415341513497488, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.0330786295235157}, {"id": 254, "seek": 267500, "start": 2676.0, "end": 2692.0, "text": " And, and because of that inconsistency in the definition of S of theta, you actually get slightly different distributions if you integrate the ODE and integrate the SDE.", "tokens": [50414, 400, 11, 293, 570, 295, 300, 22039, 468, 3020, 294, 264, 7123, 295, 318, 295, 9725, 11, 291, 767, 483, 4748, 819, 37870, 498, 291, 13365, 264, 422, 22296, 293, 13365, 264, 14638, 36, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1359729766845703, "compression_ratio": 1.3629032258064515, "no_speech_prob": 0.0574745312333107}, {"id": 255, "seek": 269200, "start": 2692.0, "end": 2698.0, "text": " And visually, if you integrate the SDE, the samples tend to look just a tiny bit better.", "tokens": [50364, 400, 19622, 11, 498, 291, 13365, 264, 14638, 36, 11, 264, 10938, 3928, 281, 574, 445, 257, 5870, 857, 1101, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10812734048577803, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.002115527866408229}, {"id": 256, "seek": 269200, "start": 2699.0, "end": 2715.0, "text": " But, but I think we don't, we don't fully understand why, but the, the reason distributions are a little bit different is because we're violating this assumption that like S of theta is actually the, the gradient of, of the log marginal density.", "tokens": [50714, 583, 11, 457, 286, 519, 321, 500, 380, 11, 321, 500, 380, 4498, 1223, 983, 11, 457, 264, 11, 264, 1778, 37870, 366, 257, 707, 857, 819, 307, 570, 321, 434, 42201, 341, 15302, 300, 411, 318, 295, 9725, 307, 767, 264, 11, 264, 16235, 295, 11, 295, 264, 3565, 16885, 10305, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10812734048577803, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.002115527866408229}, {"id": 257, "seek": 272200, "start": 2722.0, "end": 2736.0, "text": " Okay, so we have this ODE. If you've heard of neural ODE, you can think of this as being like a specific example of a neural ODE, because like S of theta is a super complicated neural network.", "tokens": [50364, 1033, 11, 370, 321, 362, 341, 422, 22296, 13, 759, 291, 600, 2198, 295, 18161, 422, 22296, 11, 291, 393, 519, 295, 341, 382, 885, 411, 257, 2685, 1365, 295, 257, 18161, 422, 22296, 11, 570, 411, 318, 295, 9725, 307, 257, 1687, 6179, 18161, 3209, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1505738894144694, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.012050843797624111}, {"id": 258, "seek": 273600, "start": 2737.0, "end": 2750.0, "text": " What's really cool is once you have an ODE, you can use just like off the shelf ODE solvers to, to generate samples and like ODE solvers are like really remarkably good.", "tokens": [50414, 708, 311, 534, 1627, 307, 1564, 291, 362, 364, 422, 22296, 11, 291, 393, 764, 445, 411, 766, 264, 15222, 422, 22296, 1404, 840, 281, 11, 281, 8460, 10938, 293, 411, 422, 22296, 1404, 840, 366, 411, 534, 37381, 665, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09320248378796524, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.05338886007666588}, {"id": 259, "seek": 273600, "start": 2750.0, "end": 2763.0, "text": " I didn't, I didn't realize quite how good they were in that they can like just off the shelf ones can just like generate like a few million dimensional like samples from a super high dimensional ODE.", "tokens": [51064, 286, 994, 380, 11, 286, 994, 380, 4325, 1596, 577, 665, 436, 645, 294, 300, 436, 393, 411, 445, 766, 264, 15222, 2306, 393, 445, 411, 8460, 411, 257, 1326, 2459, 18795, 411, 10938, 490, 257, 1687, 1090, 18795, 422, 22296, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09320248378796524, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.05338886007666588}, {"id": 260, "seek": 276300, "start": 2764.0, "end": 2776.0, "text": " So here, for instance, is the samples you get from running the an adaptive ODE sampler with a different allowed number of samples.", "tokens": [50414, 407, 510, 11, 337, 5197, 11, 307, 264, 10938, 291, 483, 490, 2614, 264, 364, 27912, 422, 22296, 3247, 22732, 365, 257, 819, 4350, 1230, 295, 10938, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12172695249319077, "compression_ratio": 1.2264150943396226, "no_speech_prob": 0.020325081422924995}, {"id": 261, "seek": 277600, "start": 2776.0, "end": 2792.0, "text": " And you can see that after maybe like 86 this samples from the ODE solvers, so allowing the ODE solver to evaluate the, the ODE equation at like 86 time points.", "tokens": [50364, 400, 291, 393, 536, 300, 934, 1310, 411, 26687, 341, 10938, 490, 264, 422, 22296, 1404, 840, 11, 370, 8293, 264, 422, 22296, 1404, 331, 281, 13059, 264, 11, 264, 422, 22296, 5367, 412, 411, 26687, 565, 2793, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18103200997879257, "compression_ratio": 1.4682080924855492, "no_speech_prob": 0.03903292492032051}, {"id": 262, "seek": 277600, "start": 2792.0, "end": 2798.0, "text": " You get what's a pretty high quality image, whereas it takes like thousands for, for the SDE.", "tokens": [51164, 509, 483, 437, 311, 257, 1238, 1090, 3125, 3256, 11, 9735, 309, 2516, 411, 5383, 337, 11, 337, 264, 318, 22296, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18103200997879257, "compression_ratio": 1.4682080924855492, "no_speech_prob": 0.03903292492032051}, {"id": 263, "seek": 279800, "start": 2798.0, "end": 2814.0, "text": " The other thing you can do with ODE is you can compute an exact log probability for four data points, which means that you can get exactly like this.", "tokens": [50364, 440, 661, 551, 291, 393, 360, 365, 422, 22296, 307, 291, 393, 14722, 364, 1900, 3565, 8482, 337, 1451, 1412, 2793, 11, 597, 1355, 300, 291, 393, 483, 2293, 411, 341, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2151689910888672, "compression_ratio": 1.3517241379310345, "no_speech_prob": 0.09133965522050858}, {"id": 264, "seek": 279800, "start": 2814.0, "end": 2817.0, "text": " We can, I can show you a table of performance.", "tokens": [51164, 492, 393, 11, 286, 393, 855, 291, 257, 3199, 295, 3389, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2151689910888672, "compression_ratio": 1.3517241379310345, "no_speech_prob": 0.09133965522050858}, {"id": 265, "seek": 281700, "start": 2818.0, "end": 2823.0, "text": " Our numbers are bold. This table is now like almost a year old because our paper is now almost a year old.", "tokens": [50414, 2621, 3547, 366, 11928, 13, 639, 3199, 307, 586, 411, 1920, 257, 1064, 1331, 570, 527, 3035, 307, 586, 1920, 257, 1064, 1331, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2005326242157907, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.24210643768310547}, {"id": 266, "seek": 281700, "start": 2823.0, "end": 2836.0, "text": " But, but take home message is this class of techniques works like surprising the well both in terms of log likelihood and in terms of measures of perceptual performance like like FID.", "tokens": [50664, 583, 11, 457, 747, 1280, 3636, 307, 341, 1508, 295, 7512, 1985, 411, 8830, 264, 731, 1293, 294, 2115, 295, 3565, 22119, 293, 294, 2115, 295, 8000, 295, 43276, 901, 3389, 411, 411, 479, 2777, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2005326242157907, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.24210643768310547}, {"id": 267, "seek": 283600, "start": 2836.0, "end": 2845.0, "text": " And I want to tell you about one more thing before, before I break, but maybe this is another good place to pause for a second.", "tokens": [50364, 400, 286, 528, 281, 980, 291, 466, 472, 544, 551, 949, 11, 949, 286, 1821, 11, 457, 1310, 341, 307, 1071, 665, 1081, 281, 10465, 337, 257, 1150, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15118487675984701, "compression_ratio": 1.3203125, "no_speech_prob": 0.018261346966028214}, {"id": 268, "seek": 283600, "start": 2845.0, "end": 2847.0, "text": " If there are any questions about the ODE.", "tokens": [50814, 759, 456, 366, 604, 1651, 466, 264, 422, 22296, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15118487675984701, "compression_ratio": 1.3203125, "no_speech_prob": 0.018261346966028214}, {"id": 269, "seek": 284700, "start": 2847.0, "end": 2876.0, "text": " Okay, cool. Let me tell you about one more thing, which may actually be one of the most relevant things if you want to use this class of models for, for like creative applications, which is that there is a very well motivated way to control generation under under this class of models.", "tokens": [50364, 1033, 11, 1627, 13, 961, 385, 980, 291, 466, 472, 544, 551, 11, 597, 815, 767, 312, 472, 295, 264, 881, 7340, 721, 498, 291, 528, 281, 764, 341, 1508, 295, 5245, 337, 11, 337, 411, 5880, 5821, 11, 597, 307, 300, 456, 307, 257, 588, 731, 14515, 636, 281, 1969, 5125, 833, 833, 341, 1508, 295, 5245, 13, 51814], "temperature": 0.0, "avg_logprob": -0.21243990621259135, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.03621279075741768}, {"id": 270, "seek": 287700, "start": 2878.0, "end": 2885.0, "text": " So, just to back up a second.", "tokens": [50414, 407, 11, 445, 281, 646, 493, 257, 1150, 13, 50764], "temperature": 0.0, "avg_logprob": -0.24811336517333984, "compression_ratio": 1.3185185185185184, "no_speech_prob": 0.02226981334388256}, {"id": 271, "seek": 287700, "start": 2885.0, "end": 2895.0, "text": " At training time, we return our data sample X zero by running our SDE and we get a noise sample.", "tokens": [50764, 1711, 3097, 565, 11, 321, 2736, 527, 1412, 6889, 1783, 4018, 538, 2614, 527, 318, 22296, 293, 321, 483, 257, 5658, 6889, 13, 51264], "temperature": 0.0, "avg_logprob": -0.24811336517333984, "compression_ratio": 1.3185185185185184, "no_speech_prob": 0.02226981334388256}, {"id": 272, "seek": 287700, "start": 2895.0, "end": 2900.0, "text": " We want to perform control generation at test time.", "tokens": [51264, 492, 528, 281, 2042, 1969, 5125, 412, 1500, 565, 13, 51514], "temperature": 0.0, "avg_logprob": -0.24811336517333984, "compression_ratio": 1.3185185185185184, "no_speech_prob": 0.02226981334388256}, {"id": 273, "seek": 290000, "start": 2900.0, "end": 2907.0, "text": " So we want to be given, we're going to be given a control signal, which I'm going to know is why here.", "tokens": [50364, 407, 321, 528, 281, 312, 2212, 11, 321, 434, 516, 281, 312, 2212, 257, 1969, 6358, 11, 597, 286, 478, 516, 281, 458, 307, 983, 510, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18732609468347886, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.009411152452230453}, {"id": 274, "seek": 290000, "start": 2907.0, "end": 2913.0, "text": " So, for instance, why might be a class label.", "tokens": [50714, 407, 11, 337, 5197, 11, 983, 1062, 312, 257, 1508, 7645, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18732609468347886, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.009411152452230453}, {"id": 275, "seek": 290000, "start": 2913.0, "end": 2923.0, "text": " And so the forward diffusion process will then perturb a conditional data sample X zero given why to complete noise.", "tokens": [51014, 400, 370, 264, 2128, 25242, 1399, 486, 550, 40468, 257, 27708, 1412, 6889, 1783, 4018, 2212, 983, 281, 3566, 5658, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18732609468347886, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.009411152452230453}, {"id": 276, "seek": 292300, "start": 2924.0, "end": 2928.0, "text": " And by reversing this procedure.", "tokens": [50414, 400, 538, 14582, 278, 341, 10747, 13, 50614], "temperature": 0.0, "avg_logprob": -0.15715773289020246, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.009857222437858582}, {"id": 277, "seek": 292300, "start": 2928.0, "end": 2949.0, "text": " We should be able to start from isotropic Gaussian noise ball and obtain a sample X zero given why the reverse time procedure condition and why can be given by the following conditional reverse time SDE.", "tokens": [50614, 492, 820, 312, 1075, 281, 722, 490, 38018, 39173, 39148, 5658, 2594, 293, 12701, 257, 6889, 1783, 4018, 2212, 983, 264, 9943, 565, 10747, 4188, 293, 983, 393, 312, 2212, 538, 264, 3480, 27708, 9943, 565, 318, 22296, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15715773289020246, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.009857222437858582}, {"id": 278, "seek": 294900, "start": 2949.0, "end": 2960.0, "text": " So here, all we've done is we've just replaced the score function of PT with the score function of PT of X, given why.", "tokens": [50364, 407, 510, 11, 439, 321, 600, 1096, 307, 321, 600, 445, 10772, 264, 6175, 2445, 295, 35460, 365, 264, 6175, 2445, 295, 35460, 295, 1783, 11, 2212, 983, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1275031226021903, "compression_ratio": 1.6210045662100456, "no_speech_prob": 0.007119076792150736}, {"id": 279, "seek": 294900, "start": 2960.0, "end": 2962.0, "text": " So this is nice.", "tokens": [50914, 407, 341, 307, 1481, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1275031226021903, "compression_ratio": 1.6210045662100456, "no_speech_prob": 0.007119076792150736}, {"id": 280, "seek": 294900, "start": 2962.0, "end": 2973.0, "text": " But at first sight, this seems like you have to train a whole bunch of like an entirely new model, because the conditional distribution is function of T is unknown.", "tokens": [51014, 583, 412, 700, 7860, 11, 341, 2544, 411, 291, 362, 281, 3847, 257, 1379, 3840, 295, 411, 364, 7696, 777, 2316, 11, 570, 264, 27708, 7316, 307, 2445, 295, 314, 307, 9841, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1275031226021903, "compression_ratio": 1.6210045662100456, "no_speech_prob": 0.007119076792150736}, {"id": 281, "seek": 294900, "start": 2973.0, "end": 2978.0, "text": " But what we can do is we can apply basis rule to this.", "tokens": [51564, 583, 437, 321, 393, 360, 307, 321, 393, 3079, 5143, 4978, 281, 341, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1275031226021903, "compression_ratio": 1.6210045662100456, "no_speech_prob": 0.007119076792150736}, {"id": 282, "seek": 297800, "start": 2978.0, "end": 2985.0, "text": " So the first term is just the unconditional score function.", "tokens": [50364, 407, 264, 700, 1433, 307, 445, 264, 47916, 6175, 2445, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13735815395008433, "compression_ratio": 1.5408805031446542, "no_speech_prob": 0.001809753943234682}, {"id": 283, "seek": 297800, "start": 2985.0, "end": 2996.0, "text": " And is what we like already spent the rest of the talk talking about how to train exactly the same as what we were training before.", "tokens": [50714, 400, 307, 437, 321, 411, 1217, 4418, 264, 1472, 295, 264, 751, 1417, 466, 577, 281, 3847, 2293, 264, 912, 382, 437, 321, 645, 3097, 949, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13735815395008433, "compression_ratio": 1.5408805031446542, "no_speech_prob": 0.001809753943234682}, {"id": 284, "seek": 297800, "start": 2996.0, "end": 3002.0, "text": " The second term can be trained completely separately.", "tokens": [51264, 440, 1150, 1433, 393, 312, 8895, 2584, 14759, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13735815395008433, "compression_ratio": 1.5408805031446542, "no_speech_prob": 0.001809753943234682}, {"id": 285, "seek": 300200, "start": 3002.0, "end": 3009.0, "text": " From the score base model, or even sometimes can just be like written down in in close form using using the main knowledge.", "tokens": [50364, 3358, 264, 6175, 3096, 2316, 11, 420, 754, 2171, 393, 445, 312, 411, 3720, 760, 294, 294, 1998, 1254, 1228, 1228, 264, 2135, 3601, 13, 50714], "temperature": 0.0, "avg_logprob": -0.22705975974478373, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.021281208842992783}, {"id": 286, "seek": 300200, "start": 3009.0, "end": 3021.0, "text": " And so, and so the product of these two terms plus the constant is equal to to the log P of T of X given why.", "tokens": [50714, 400, 370, 11, 293, 370, 264, 1674, 295, 613, 732, 2115, 1804, 264, 5754, 307, 2681, 281, 281, 264, 3565, 430, 295, 314, 295, 1783, 2212, 983, 13, 51314], "temperature": 0.0, "avg_logprob": -0.22705975974478373, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.021281208842992783}, {"id": 287, "seek": 300200, "start": 3021.0, "end": 3026.0, "text": " And I'm sorry, because these are logs, I should have said that some of these two terms.", "tokens": [51314, 400, 286, 478, 2597, 11, 570, 613, 366, 20820, 11, 286, 820, 362, 848, 300, 512, 295, 613, 732, 2115, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22705975974478373, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.021281208842992783}, {"id": 288, "seek": 302600, "start": 3026.0, "end": 3033.0, "text": " This is just this. This is just base rule applied to PT of X given why.", "tokens": [50364, 639, 307, 445, 341, 13, 639, 307, 445, 3096, 4978, 6456, 281, 35460, 295, 1783, 2212, 983, 13, 50714], "temperature": 0.0, "avg_logprob": -0.26434568374875994, "compression_ratio": 1.3869047619047619, "no_speech_prob": 0.005468467716127634}, {"id": 289, "seek": 302600, "start": 3033.0, "end": 3046.0, "text": " And so, this is a particularly cool capability, because it's not something you can do at test time for like gams or VES or autoregressive models or any of these.", "tokens": [50714, 400, 370, 11, 341, 307, 257, 4098, 1627, 13759, 11, 570, 309, 311, 406, 746, 291, 393, 360, 412, 1500, 565, 337, 411, 290, 4070, 420, 691, 2358, 420, 1476, 418, 3091, 488, 5245, 420, 604, 295, 613, 13, 51364], "temperature": 0.0, "avg_logprob": -0.26434568374875994, "compression_ratio": 1.3869047619047619, "no_speech_prob": 0.005468467716127634}, {"id": 290, "seek": 304600, "start": 3046.0, "end": 3055.0, "text": " Here we can train a ginormous like unsupervised model images, and then we can train a little classifier like PT of Y given X later.", "tokens": [50364, 1692, 321, 393, 3847, 257, 36604, 687, 563, 411, 2693, 12879, 24420, 2316, 5267, 11, 293, 550, 321, 393, 3847, 257, 707, 1508, 9902, 411, 35460, 295, 398, 2212, 1783, 1780, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14005448268010065, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.21983928978443146}, {"id": 291, "seek": 304600, "start": 3055.0, "end": 3061.0, "text": " And we can use that little classifier to like guide our image generation.", "tokens": [50814, 400, 321, 393, 764, 300, 707, 1508, 9902, 281, 411, 5934, 527, 3256, 5125, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14005448268010065, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.21983928978443146}, {"id": 292, "seek": 304600, "start": 3061.0, "end": 3072.0, "text": " So one example of this is here we have a here we're making PT we're making why the actual class.", "tokens": [51114, 407, 472, 1365, 295, 341, 307, 510, 321, 362, 257, 510, 321, 434, 1455, 35460, 321, 434, 1455, 983, 264, 3539, 1508, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14005448268010065, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.21983928978443146}, {"id": 293, "seek": 307200, "start": 3072.0, "end": 3084.0, "text": " And so we can use this to do like post hoc sampling of CFR 10 images that come from the class bird on the left or come from the class deer on the right.", "tokens": [50364, 400, 370, 321, 393, 764, 341, 281, 360, 411, 2183, 16708, 21179, 295, 21792, 49, 1266, 5267, 300, 808, 490, 264, 1508, 5255, 322, 264, 1411, 420, 808, 490, 264, 1508, 17120, 322, 264, 558, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1180954087864269, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.021603291854262352}, {"id": 294, "seek": 307200, "start": 3084.0, "end": 3086.0, "text": " You can also do this for in painting.", "tokens": [50964, 509, 393, 611, 360, 341, 337, 294, 5370, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1180954087864269, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.021603291854262352}, {"id": 295, "seek": 307200, "start": 3086.0, "end": 3099.0, "text": " So here, why is the part of the image that you know, and you want to generate the entire image like conditioned on the part of the image that you actually know.", "tokens": [51064, 407, 510, 11, 983, 307, 264, 644, 295, 264, 3256, 300, 291, 458, 11, 293, 291, 528, 281, 8460, 264, 2302, 3256, 411, 35833, 322, 264, 644, 295, 264, 3256, 300, 291, 767, 458, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1180954087864269, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.021603291854262352}, {"id": 296, "seek": 309900, "start": 3099.0, "end": 3103.0, "text": " And so here are the first column is the true image, the ground truth image.", "tokens": [50364, 400, 370, 510, 366, 264, 700, 7738, 307, 264, 2074, 3256, 11, 264, 2727, 3494, 3256, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15434793992476029, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.007343909237533808}, {"id": 297, "seek": 309900, "start": 3103.0, "end": 3110.0, "text": " The second column, we've thrown away all but the part of the image that you can actually see.", "tokens": [50564, 440, 1150, 7738, 11, 321, 600, 11732, 1314, 439, 457, 264, 644, 295, 264, 3256, 300, 291, 393, 767, 536, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15434793992476029, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.007343909237533808}, {"id": 298, "seek": 309900, "start": 3110.0, "end": 3115.0, "text": " So you've thrown away all we've thrown away the surround or the center.", "tokens": [50914, 407, 291, 600, 11732, 1314, 439, 321, 600, 11732, 1314, 264, 6262, 420, 264, 3056, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15434793992476029, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.007343909237533808}, {"id": 299, "seek": 309900, "start": 3115.0, "end": 3124.0, "text": " And then in the remaining columns, we're showing independent samples of in painting all the missing content in in these images.", "tokens": [51164, 400, 550, 294, 264, 8877, 13766, 11, 321, 434, 4099, 6695, 10938, 295, 294, 5370, 439, 264, 5361, 2701, 294, 294, 613, 5267, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15434793992476029, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.007343909237533808}, {"id": 300, "seek": 312400, "start": 3124.0, "end": 3135.0, "text": " And you can see that an off the shelf diffusion model not trained to do in painting can can still do a good job in painting.", "tokens": [50364, 400, 291, 393, 536, 300, 364, 766, 264, 15222, 25242, 2316, 406, 8895, 281, 360, 294, 5370, 393, 393, 920, 360, 257, 665, 1691, 294, 5370, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10781818942019814, "compression_ratio": 1.75, "no_speech_prob": 0.015899453312158585}, {"id": 301, "seek": 312400, "start": 3135.0, "end": 3139.0, "text": " You can also see that there's like diversity in the images that it generates.", "tokens": [50914, 509, 393, 611, 536, 300, 456, 311, 411, 8811, 294, 264, 5267, 300, 309, 23815, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10781818942019814, "compression_ratio": 1.75, "no_speech_prob": 0.015899453312158585}, {"id": 302, "seek": 312400, "start": 3139.0, "end": 3153.0, "text": " It doesn't generate same bedroom over and over and over again it generates like a sequence of plausible in paintings of missing information.", "tokens": [51114, 467, 1177, 380, 8460, 912, 11211, 670, 293, 670, 293, 670, 797, 309, 23815, 411, 257, 8310, 295, 39925, 294, 14880, 295, 5361, 1589, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10781818942019814, "compression_ratio": 1.75, "no_speech_prob": 0.015899453312158585}, {"id": 303, "seek": 315300, "start": 3154.0, "end": 3160.0, "text": " We can do this for colorization.", "tokens": [50414, 492, 393, 360, 341, 337, 2017, 2144, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2067967749930717, "compression_ratio": 1.5856353591160222, "no_speech_prob": 0.003592311404645443}, {"id": 304, "seek": 315300, "start": 3160.0, "end": 3166.0, "text": " So we can take an image we can make grayscale and then we can infer infer the colors.", "tokens": [50714, 407, 321, 393, 747, 364, 3256, 321, 393, 652, 677, 3772, 37088, 293, 550, 321, 393, 13596, 13596, 264, 4577, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2067967749930717, "compression_ratio": 1.5856353591160222, "no_speech_prob": 0.003592311404645443}, {"id": 305, "seek": 315300, "start": 3166.0, "end": 3170.0, "text": " Cool.", "tokens": [51014, 8561, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2067967749930717, "compression_ratio": 1.5856353591160222, "no_speech_prob": 0.003592311404645443}, {"id": 306, "seek": 315300, "start": 3170.0, "end": 3175.0, "text": " This also let me just actually jump way back to being talking again.", "tokens": [51214, 639, 611, 718, 385, 445, 767, 3012, 636, 646, 281, 885, 1417, 797, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2067967749930717, "compression_ratio": 1.5856353591160222, "no_speech_prob": 0.003592311404645443}, {"id": 307, "seek": 315300, "start": 3175.0, "end": 3181.0, "text": " This is also the same technique essentially that's used to generate these these art examples.", "tokens": [51464, 639, 307, 611, 264, 912, 6532, 4476, 300, 311, 1143, 281, 8460, 613, 613, 1523, 5110, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2067967749930717, "compression_ratio": 1.5856353591160222, "no_speech_prob": 0.003592311404645443}, {"id": 308, "seek": 318100, "start": 3181.0, "end": 3188.0, "text": " So they're rather than using some pt of y given X as a guiding signal.", "tokens": [50364, 407, 436, 434, 2831, 813, 1228, 512, 280, 83, 295, 288, 2212, 1783, 382, 257, 25061, 6358, 13, 50714], "temperature": 0.0, "avg_logprob": -0.23464682102203369, "compression_ratio": 1.64, "no_speech_prob": 0.0017001795349642634}, {"id": 309, "seek": 318100, "start": 3188.0, "end": 3194.0, "text": " They're using the output of like the clip classifier as the guiding signal and they're like multiply by some scalar.", "tokens": [50714, 814, 434, 1228, 264, 5598, 295, 411, 264, 7353, 1508, 9902, 382, 264, 25061, 6358, 293, 436, 434, 411, 12972, 538, 512, 39684, 13, 51014], "temperature": 0.0, "avg_logprob": -0.23464682102203369, "compression_ratio": 1.64, "no_speech_prob": 0.0017001795349642634}, {"id": 310, "seek": 318100, "start": 3194.0, "end": 3203.0, "text": " But but they are guiding the diffusion generation in the same way as as I just showed.", "tokens": [51014, 583, 457, 436, 366, 25061, 264, 25242, 5125, 294, 264, 912, 636, 382, 382, 286, 445, 4712, 13, 51464], "temperature": 0.0, "avg_logprob": -0.23464682102203369, "compression_ratio": 1.64, "no_speech_prob": 0.0017001795349642634}, {"id": 311, "seek": 318100, "start": 3203.0, "end": 3210.0, "text": " And then using it to create novel artistic creations.", "tokens": [51464, 400, 550, 1228, 309, 281, 1884, 7613, 17090, 37836, 13, 51814], "temperature": 0.0, "avg_logprob": -0.23464682102203369, "compression_ratio": 1.64, "no_speech_prob": 0.0017001795349642634}, {"id": 312, "seek": 321000, "start": 3210.0, "end": 3216.0, "text": " Cool. Okay, so to summarize.", "tokens": [50364, 8561, 13, 1033, 11, 370, 281, 20858, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1345584915905464, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.0005440980894491076}, {"id": 313, "seek": 321000, "start": 3216.0, "end": 3220.0, "text": " I have shown you a general model based on diffusion processes.", "tokens": [50664, 286, 362, 4898, 291, 257, 2674, 2316, 2361, 322, 25242, 7555, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1345584915905464, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.0005440980894491076}, {"id": 314, "seek": 321000, "start": 3220.0, "end": 3224.0, "text": " We first corrupt data to a known noise distribution using diffusion.", "tokens": [50864, 492, 700, 17366, 1412, 281, 257, 2570, 5658, 7316, 1228, 25242, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1345584915905464, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.0005440980894491076}, {"id": 315, "seek": 321000, "start": 3224.0, "end": 3231.0, "text": " And then we learn the time reversal of this diffusion process in either discrete or continuous time.", "tokens": [51064, 400, 550, 321, 1466, 264, 565, 42778, 295, 341, 25242, 1399, 294, 2139, 27706, 420, 10957, 565, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1345584915905464, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.0005440980894491076}, {"id": 316, "seek": 321000, "start": 3231.0, "end": 3239.0, "text": " And we can then generate samples by drawing a random noise vector and simulating the reverse diffusion process.", "tokens": [51414, 400, 321, 393, 550, 8460, 10938, 538, 6316, 257, 4974, 5658, 8062, 293, 1034, 12162, 264, 9943, 25242, 1399, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1345584915905464, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.0005440980894491076}, {"id": 317, "seek": 323900, "start": 3239.0, "end": 3243.0, "text": " There are some advantages of our framework.", "tokens": [50364, 821, 366, 512, 14906, 295, 527, 8388, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17503989615091464, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.004680385813117027}, {"id": 318, "seek": 323900, "start": 3243.0, "end": 3248.0, "text": " First, image quality is super duper high.", "tokens": [50564, 2386, 11, 3256, 3125, 307, 1687, 1581, 610, 1090, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17503989615091464, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.004680385813117027}, {"id": 319, "seek": 323900, "start": 3248.0, "end": 3259.0, "text": " Second, there's equivalence to neural OVs or flow models, which allows us to do things like like exact likelihood computation.", "tokens": [50814, 5736, 11, 456, 311, 9052, 655, 281, 18161, 422, 53, 82, 420, 3095, 5245, 11, 597, 4045, 505, 281, 360, 721, 411, 411, 1900, 22119, 24903, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17503989615091464, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.004680385813117027}, {"id": 320, "seek": 323900, "start": 3259.0, "end": 3267.0, "text": " I didn't get a chance to talk about this, but but I actually talked about it very briefly before before the talk proper, but", "tokens": [51364, 286, 994, 380, 483, 257, 2931, 281, 751, 466, 341, 11, 457, 457, 286, 767, 2825, 466, 309, 588, 10515, 949, 949, 264, 751, 2296, 11, 457, 51764], "temperature": 0.0, "avg_logprob": -0.17503989615091464, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.004680385813117027}, {"id": 321, "seek": 326700, "start": 3267.0, "end": 3275.0, "text": " our encoding is also uniquely identifiable, meaning that every well trained model will have identical latent codes for identical input data points.", "tokens": [50364, 527, 43430, 307, 611, 31474, 2473, 30876, 11, 3620, 300, 633, 731, 8895, 2316, 486, 362, 14800, 48994, 14211, 337, 14800, 4846, 1412, 2793, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1520662973093432, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.0080600930377841}, {"id": 322, "seek": 326700, "start": 3275.0, "end": 3281.0, "text": " This is either a positive or negative depending on how you look at it, but it is a unique property of this class of models.", "tokens": [50764, 639, 307, 2139, 257, 3353, 420, 3671, 5413, 322, 577, 291, 574, 412, 309, 11, 457, 309, 307, 257, 3845, 4707, 295, 341, 1508, 295, 5245, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1520662973093432, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.0080600930377841}, {"id": 323, "seek": 326700, "start": 3281.0, "end": 3287.0, "text": " And finally, we can do controllable generation without without retraining the model.", "tokens": [51064, 400, 2721, 11, 321, 393, 360, 45159, 712, 5125, 1553, 1553, 49356, 1760, 264, 2316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1520662973093432, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.0080600930377841}, {"id": 324, "seek": 326700, "start": 3287.0, "end": 3291.0, "text": " Examples like include like class conditional generation.", "tokens": [51364, 48591, 411, 4090, 411, 1508, 27708, 5125, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1520662973093432, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.0080600930377841}, {"id": 325, "seek": 329100, "start": 3291.0, "end": 3299.0, "text": " Including some of LA clip guided diffusion measure at the beginning.", "tokens": [50364, 27137, 512, 295, 9855, 7353, 19663, 25242, 3481, 412, 264, 2863, 13, 50764], "temperature": 0.0, "avg_logprob": -0.26395830118431235, "compression_ratio": 1.3405797101449275, "no_speech_prob": 0.013200130313634872}, {"id": 326, "seek": 329100, "start": 3299.0, "end": 3301.0, "text": " Yeah.", "tokens": [50764, 865, 13, 50864], "temperature": 0.0, "avg_logprob": -0.26395830118431235, "compression_ratio": 1.3405797101449275, "no_speech_prob": 0.013200130313634872}, {"id": 327, "seek": 329100, "start": 3301.0, "end": 3305.0, "text": " Okay, that's what I got. Thank you so much for listening.", "tokens": [50864, 1033, 11, 300, 311, 437, 286, 658, 13, 1044, 291, 370, 709, 337, 4764, 13, 51064], "temperature": 0.0, "avg_logprob": -0.26395830118431235, "compression_ratio": 1.3405797101449275, "no_speech_prob": 0.013200130313634872}, {"id": 328, "seek": 329100, "start": 3305.0, "end": 3308.0, "text": " Thank you so much. This was awesome.", "tokens": [51064, 1044, 291, 370, 709, 13, 639, 390, 3476, 13, 51214], "temperature": 0.0, "avg_logprob": -0.26395830118431235, "compression_ratio": 1.3405797101449275, "no_speech_prob": 0.013200130313634872}, {"id": 329, "seek": 329100, "start": 3308.0, "end": 3312.0, "text": " Really helpful.", "tokens": [51214, 4083, 4961, 13, 51414], "temperature": 0.0, "avg_logprob": -0.26395830118431235, "compression_ratio": 1.3405797101449275, "no_speech_prob": 0.013200130313634872}, {"id": 330, "seek": 331200, "start": 3312.0, "end": 3327.0, "text": " Seems that this controllable generation is really cool because once you have the sort of probability of X and then you can.", "tokens": [50364, 22524, 300, 341, 45159, 712, 5125, 307, 534, 1627, 570, 1564, 291, 362, 264, 1333, 295, 8482, 295, 1783, 293, 550, 291, 393, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1533165659223284, "compression_ratio": 1.2178217821782178, "no_speech_prob": 0.06631376594305038}, {"id": 331, "seek": 332700, "start": 3327.0, "end": 3344.0, "text": " So that is sort of task agnostic in a way and then whatever task you want, learn it and then use that sort of backbone that you have already trained will learn from the data.", "tokens": [50364, 407, 300, 307, 1333, 295, 5633, 623, 77, 19634, 294, 257, 636, 293, 550, 2035, 5633, 291, 528, 11, 1466, 309, 293, 550, 764, 300, 1333, 295, 34889, 300, 291, 362, 1217, 8895, 486, 1466, 490, 264, 1412, 13, 51214], "temperature": 0.0, "avg_logprob": -0.21441754754984155, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.012787636369466782}, {"id": 332, "seek": 332700, "start": 3344.0, "end": 3346.0, "text": " This is, this is really cool.", "tokens": [51214, 639, 307, 11, 341, 307, 534, 1627, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21441754754984155, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.012787636369466782}, {"id": 333, "seek": 334600, "start": 3346.0, "end": 3374.0, "text": " And then I think that this fact that you can identify encoding or do the reverse process is also very cool because there is a great deal of how to, for instance, take my image and then map it to the latent space of again, so that I can modify it.", "tokens": [50364, 400, 550, 286, 519, 300, 341, 1186, 300, 291, 393, 5876, 43430, 420, 360, 264, 9943, 1399, 307, 611, 588, 1627, 570, 456, 307, 257, 869, 2028, 295, 577, 281, 11, 337, 5197, 11, 747, 452, 3256, 293, 550, 4471, 309, 281, 264, 48994, 1901, 295, 797, 11, 370, 300, 286, 393, 16927, 309, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16414834685244803, "compression_ratio": 1.5, "no_speech_prob": 0.1794968694448471}, {"id": 334, "seek": 337400, "start": 3374.0, "end": 3377.0, "text": " Yeah.", "tokens": [50364, 865, 13, 50514], "temperature": 0.0, "avg_logprob": -0.17732965535130993, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.04863596335053444}, {"id": 335, "seek": 337400, "start": 3377.0, "end": 3383.0, "text": " People have been trying to train inverters.", "tokens": [50514, 3432, 362, 668, 1382, 281, 3847, 28653, 1559, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17732965535130993, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.04863596335053444}, {"id": 336, "seek": 337400, "start": 3383.0, "end": 3391.0, "text": " They are getting better and better, but it seems that in this case, the inverter comes for free.", "tokens": [50814, 814, 366, 1242, 1101, 293, 1101, 11, 457, 309, 2544, 300, 294, 341, 1389, 11, 264, 47201, 1487, 337, 1737, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17732965535130993, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.04863596335053444}, {"id": 337, "seek": 337400, "start": 3391.0, "end": 3394.0, "text": " Yeah, yeah.", "tokens": [51214, 865, 11, 1338, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17732965535130993, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.04863596335053444}, {"id": 338, "seek": 337400, "start": 3394.0, "end": 3401.0, "text": " That's very cool. So are there questions?", "tokens": [51364, 663, 311, 588, 1627, 13, 407, 366, 456, 1651, 30, 51714], "temperature": 0.0, "avg_logprob": -0.17732965535130993, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.04863596335053444}, {"id": 339, "seek": 340100, "start": 3401.0, "end": 3425.0, "text": " I think more on a high level. I mean, what, I guess, having a physics background clearly probably helped come with an idea that is, let's say, kind of, I mean, yeah, based on physical intuition, but it's so different from I guess the GAN architecture.", "tokens": [50364, 286, 519, 544, 322, 257, 1090, 1496, 13, 286, 914, 11, 437, 11, 286, 2041, 11, 1419, 257, 10649, 3678, 4448, 1391, 4254, 808, 365, 364, 1558, 300, 307, 11, 718, 311, 584, 11, 733, 295, 11, 286, 914, 11, 1338, 11, 2361, 322, 4001, 24002, 11, 457, 309, 311, 370, 819, 490, 286, 2041, 264, 460, 1770, 9482, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20803186297416687, "compression_ratio": 1.4180790960451977, "no_speech_prob": 0.08132098615169525}, {"id": 340, "seek": 342500, "start": 3425.0, "end": 3448.0, "text": " So like, in terms of more like human creativity, like how do we have to look more into the nature to find more inspiration for those, maybe even other models that or other paradigms or what is your suggestion for people who are interested in this field?", "tokens": [50364, 407, 411, 11, 294, 2115, 295, 544, 411, 1952, 12915, 11, 411, 577, 360, 321, 362, 281, 574, 544, 666, 264, 3687, 281, 915, 544, 10249, 337, 729, 11, 1310, 754, 661, 5245, 300, 420, 661, 13480, 328, 2592, 420, 437, 307, 428, 16541, 337, 561, 567, 366, 3102, 294, 341, 2519, 30, 51514], "temperature": 0.0, "avg_logprob": -0.13595248431694218, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.1537199467420578}, {"id": 341, "seek": 342500, "start": 3448.0, "end": 3453.0, "text": " Yeah, you mean like in machine learning, how do you come up with creative different ideas?", "tokens": [51514, 865, 11, 291, 914, 411, 294, 3479, 2539, 11, 577, 360, 291, 808, 493, 365, 5880, 819, 3487, 30, 51764], "temperature": 0.0, "avg_logprob": -0.13595248431694218, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.1537199467420578}, {"id": 342, "seek": 342500, "start": 3453.0, "end": 3454.0, "text": " Yeah.", "tokens": [51764, 865, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13595248431694218, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.1537199467420578}, {"id": 343, "seek": 345400, "start": 3454.0, "end": 3475.0, "text": " Yeah, I mean, so definitely, I mean, I'm biased on my own background, so my answer is going to be be like me, which is not really a good answer. But no, one thing I do think is really actually good, though, is I think it's good to have, I think it's good to have a background, which is not the straight machine learning background.", "tokens": [50364, 865, 11, 286, 914, 11, 370, 2138, 11, 286, 914, 11, 286, 478, 28035, 322, 452, 1065, 3678, 11, 370, 452, 1867, 307, 516, 281, 312, 312, 411, 385, 11, 597, 307, 406, 534, 257, 665, 1867, 13, 583, 572, 11, 472, 551, 286, 360, 519, 307, 534, 767, 665, 11, 1673, 11, 307, 286, 519, 309, 311, 665, 281, 362, 11, 286, 519, 309, 311, 665, 281, 362, 257, 3678, 11, 597, 307, 406, 264, 2997, 3479, 2539, 3678, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13893643547506893, "compression_ratio": 1.8495575221238938, "no_speech_prob": 0.022267965599894524}, {"id": 344, "seek": 345400, "start": 3475.0, "end": 3482.0, "text": " I think having exposure to ideas and having a novel perspective like definitely helps.", "tokens": [51414, 286, 519, 1419, 10420, 281, 3487, 293, 1419, 257, 7613, 4585, 411, 2138, 3665, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13893643547506893, "compression_ratio": 1.8495575221238938, "no_speech_prob": 0.022267965599894524}, {"id": 345, "seek": 348200, "start": 3482.0, "end": 3504.0, "text": " I think probably even more important than that is like talking with and collaborating with people with different ideas than you. Like whatever your background is, if you can like work on a team and work closely and talk closely with people that have a very different background, then you're going to come up with ideas that no one else is going to come up with.", "tokens": [50364, 286, 519, 1391, 754, 544, 1021, 813, 300, 307, 411, 1417, 365, 293, 30188, 365, 561, 365, 819, 3487, 813, 291, 13, 1743, 2035, 428, 3678, 307, 11, 498, 291, 393, 411, 589, 322, 257, 1469, 293, 589, 8185, 293, 751, 8185, 365, 561, 300, 362, 257, 588, 819, 3678, 11, 550, 291, 434, 516, 281, 808, 493, 365, 3487, 300, 572, 472, 1646, 307, 516, 281, 808, 493, 365, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09338200092315674, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.25895464420318604}, {"id": 346, "seek": 348200, "start": 3504.0, "end": 3511.0, "text": " And so I think, yeah.", "tokens": [51464, 400, 370, 286, 519, 11, 1338, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09338200092315674, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.25895464420318604}, {"id": 347, "seek": 351100, "start": 3511.0, "end": 3514.0, "text": " Very cool. I think there are questions on the chat.", "tokens": [50364, 4372, 1627, 13, 286, 519, 456, 366, 1651, 322, 264, 5081, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15928622809323398, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.030634930357336998}, {"id": 348, "seek": 351100, "start": 3514.0, "end": 3518.0, "text": " Yeah, I just say, yeah, I think I just saw that as well.", "tokens": [50514, 865, 11, 286, 445, 584, 11, 1338, 11, 286, 519, 286, 445, 1866, 300, 382, 731, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15928622809323398, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.030634930357336998}, {"id": 349, "seek": 351100, "start": 3518.0, "end": 3536.0, "text": " Okay, so one question is like just out of curiosity, what happens if the initial input X zero is out of distribution. And so before the fusion process, the process that takes X zero and turns it in the noise is a is a fixed process.", "tokens": [50714, 1033, 11, 370, 472, 1168, 307, 411, 445, 484, 295, 18769, 11, 437, 2314, 498, 264, 5883, 4846, 1783, 4018, 307, 484, 295, 7316, 13, 400, 370, 949, 264, 23100, 1399, 11, 264, 1399, 300, 2516, 1783, 4018, 293, 4523, 309, 294, 264, 5658, 307, 257, 307, 257, 6806, 1399, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15928622809323398, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.030634930357336998}, {"id": 350, "seek": 353600, "start": 3536.0, "end": 3542.0, "text": " So it will take an action. Maybe let me open up for a second.", "tokens": [50364, 407, 309, 486, 747, 364, 3069, 13, 2704, 718, 385, 1269, 493, 337, 257, 1150, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2777841329574585, "compression_ratio": 1.105263157894737, "no_speech_prob": 0.0502997525036335}, {"id": 351, "seek": 353600, "start": 3542.0, "end": 3547.0, "text": " So, um,", "tokens": [50664, 407, 11, 1105, 11, 50914], "temperature": 0.0, "avg_logprob": -0.2777841329574585, "compression_ratio": 1.105263157894737, "no_speech_prob": 0.0502997525036335}, {"id": 352, "seek": 353600, "start": 3547.0, "end": 3552.0, "text": " Yeah, so just to be the", "tokens": [50914, 865, 11, 370, 445, 281, 312, 264, 51164], "temperature": 0.0, "avg_logprob": -0.2777841329574585, "compression_ratio": 1.105263157894737, "no_speech_prob": 0.0502997525036335}, {"id": 353, "seek": 353600, "start": 3552.0, "end": 3555.0, "text": " Good thing.", "tokens": [51164, 2205, 551, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2777841329574585, "compression_ratio": 1.105263157894737, "no_speech_prob": 0.0502997525036335}, {"id": 354, "seek": 355500, "start": 3555.0, "end": 3567.0, "text": " Okay, so the forward process is is a fixed process. So any sample X zero on the left here is going to get turned into a sample from an unimole calcium.", "tokens": [50364, 1033, 11, 370, 264, 2128, 1399, 307, 307, 257, 6806, 1399, 13, 407, 604, 6889, 1783, 4018, 322, 264, 1411, 510, 307, 516, 281, 483, 3574, 666, 257, 6889, 490, 364, 517, 332, 4812, 20918, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19617019759284127, "compression_ratio": 1.5670103092783505, "no_speech_prob": 0.032075803726911545}, {"id": 355, "seek": 355500, "start": 3567.0, "end": 3576.0, "text": " And I should I should one subtle to cure which is pretty important is that every sample on the left gets mapped to the entire distribution on the right.", "tokens": [50964, 400, 286, 820, 286, 820, 472, 13743, 281, 13698, 597, 307, 1238, 1021, 307, 300, 633, 6889, 322, 264, 1411, 2170, 33318, 281, 264, 2302, 7316, 322, 264, 558, 13, 51414], "temperature": 0.0, "avg_logprob": -0.19617019759284127, "compression_ratio": 1.5670103092783505, "no_speech_prob": 0.032075803726911545}, {"id": 356, "seek": 357600, "start": 3576.0, "end": 3586.0, "text": " So if you were to start from the same same sample on the left over and over and over again and run the diffusion process again, like every time you ran the diffusion process, you would get a different trajectory, and you would get a different sample on the right.", "tokens": [50364, 407, 498, 291, 645, 281, 722, 490, 264, 912, 912, 6889, 322, 264, 1411, 670, 293, 670, 293, 670, 797, 293, 1190, 264, 25242, 1399, 797, 11, 411, 633, 565, 291, 5872, 264, 25242, 1399, 11, 291, 576, 483, 257, 819, 21512, 11, 293, 291, 576, 483, 257, 819, 6889, 322, 264, 558, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1585774589987362, "compression_ratio": 1.994949494949495, "no_speech_prob": 0.10663069039583206}, {"id": 357, "seek": 357600, "start": 3586.0, "end": 3598.0, "text": " So the forward process maps every single like possible input sample to the entire like PT like like isotropic calcium prior sample.", "tokens": [50864, 407, 264, 2128, 1399, 11317, 633, 2167, 411, 1944, 4846, 6889, 281, 264, 2302, 411, 35460, 411, 411, 307, 310, 39173, 20918, 4059, 6889, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1585774589987362, "compression_ratio": 1.994949494949495, "no_speech_prob": 0.10663069039583206}, {"id": 358, "seek": 359800, "start": 3598.0, "end": 3609.0, "text": " And the reverse is also true. If you start with a sample on the right and you run the SD then you will get a sample from your model of the distribution.", "tokens": [50364, 400, 264, 9943, 307, 611, 2074, 13, 759, 291, 722, 365, 257, 6889, 322, 264, 558, 293, 291, 1190, 264, 14638, 550, 291, 486, 483, 257, 6889, 490, 428, 2316, 295, 264, 7316, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10412675957930716, "compression_ratio": 1.9714285714285715, "no_speech_prob": 0.1066443994641304}, {"id": 359, "seek": 359800, "start": 3609.0, "end": 3617.0, "text": " But if you run if you were to start from the same sample on the right over and over and over again. Every time you did that you get a different sample from your distribution.", "tokens": [50914, 583, 498, 291, 1190, 498, 291, 645, 281, 722, 490, 264, 912, 6889, 322, 264, 558, 670, 293, 670, 293, 670, 797, 13, 2048, 565, 291, 630, 300, 291, 483, 257, 819, 6889, 490, 428, 7316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10412675957930716, "compression_ratio": 1.9714285714285715, "no_speech_prob": 0.1066443994641304}, {"id": 360, "seek": 359800, "start": 3617.0, "end": 3624.0, "text": " And so every sample from the prior is actually also mapped to the entire distribution.", "tokens": [51314, 400, 370, 633, 6889, 490, 264, 4059, 307, 767, 611, 33318, 281, 264, 2302, 7316, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10412675957930716, "compression_ratio": 1.9714285714285715, "no_speech_prob": 0.1066443994641304}, {"id": 361, "seek": 362400, "start": 3624.0, "end": 3636.0, "text": " And so it's not like there, unless you're using OD formalism, there's not like the one to one correspondence between the image space and the latent space.", "tokens": [50364, 400, 370, 309, 311, 406, 411, 456, 11, 5969, 291, 434, 1228, 48447, 9860, 1434, 11, 456, 311, 406, 411, 264, 472, 281, 472, 38135, 1296, 264, 3256, 1901, 293, 264, 48994, 1901, 13, 50964], "temperature": 0.0, "avg_logprob": -0.21642184567141842, "compression_ratio": 1.7098445595854923, "no_speech_prob": 0.029294030740857124}, {"id": 362, "seek": 362400, "start": 3636.0, "end": 3648.0, "text": " And so and so you would turn X zero into the same sample from the same latent distribution and then when you came back to the image, you wouldn't know anything X zero anymore.", "tokens": [50964, 400, 370, 293, 370, 291, 576, 1261, 1783, 4018, 666, 264, 912, 6889, 490, 264, 912, 48994, 7316, 293, 550, 562, 291, 1361, 646, 281, 264, 3256, 11, 291, 2759, 380, 458, 1340, 1783, 4018, 3602, 13, 51564], "temperature": 0.0, "avg_logprob": -0.21642184567141842, "compression_ratio": 1.7098445595854923, "no_speech_prob": 0.029294030740857124}, {"id": 363, "seek": 364800, "start": 3648.0, "end": 3656.0, "text": " There's another question here. Is it possible to work with multiple classes within one diffusion probabilistic model.", "tokens": [50364, 821, 311, 1071, 1168, 510, 13, 1119, 309, 1944, 281, 589, 365, 3866, 5359, 1951, 472, 25242, 31959, 3142, 2316, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16529337952776654, "compression_ratio": 1.6966824644549763, "no_speech_prob": 0.03408253937959671}, {"id": 364, "seek": 364800, "start": 3656.0, "end": 3672.0, "text": " And I mean the answer is, so you can train your diffusion model on any distribution that you want to train on, I guess, so so I think the answer is yes, I think, I think the more precise answer would depend on exactly what you wanted to do.", "tokens": [50764, 400, 286, 914, 264, 1867, 307, 11, 370, 291, 393, 3847, 428, 25242, 2316, 322, 604, 7316, 300, 291, 528, 281, 3847, 322, 11, 286, 2041, 11, 370, 370, 286, 519, 264, 1867, 307, 2086, 11, 286, 519, 11, 286, 519, 264, 544, 13600, 1867, 576, 5672, 322, 2293, 437, 291, 1415, 281, 360, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16529337952776654, "compression_ratio": 1.6966824644549763, "no_speech_prob": 0.03408253937959671}, {"id": 365, "seek": 367200, "start": 3672.0, "end": 3683.0, "text": " But, but there's no reason that multiple classes should be harder than one class.", "tokens": [50364, 583, 11, 457, 456, 311, 572, 1778, 300, 3866, 5359, 820, 312, 6081, 813, 472, 1508, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14002786742316353, "compression_ratio": 1.0804597701149425, "no_speech_prob": 0.05178045481443405}, {"id": 366, "seek": 367200, "start": 3683.0, "end": 3685.0, "text": " Okay, great.", "tokens": [50914, 1033, 11, 869, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14002786742316353, "compression_ratio": 1.0804597701149425, "no_speech_prob": 0.05178045481443405}, {"id": 367, "seek": 368500, "start": 3685.0, "end": 3692.0, "text": " Maybe I can stop there recording here and if there are more questions, you can ask.", "tokens": [50364, 2704, 286, 393, 1590, 456, 6613, 510, 293, 498, 456, 366, 544, 1651, 11, 291, 393, 1029, 13, 50714], "temperature": 0.0, "avg_logprob": -0.20586145491827101, "compression_ratio": 1.077922077922078, "no_speech_prob": 0.34422093629837036}], "language": "en"}