WEBVTT

00:00.000 --> 00:05.240
Hello, everyone. Welcome back to your course Learning for Arts,

00:05.240 --> 00:09.360
Aesthetics, and Creativity. Our specialist speaker, Tom

00:09.360 --> 00:15.040
White, is here today to tell us about his exploration in art

00:15.040 --> 00:22.120
and creativity in AI. He is an artist who actually creates

00:23.080 --> 00:31.120
art with computers and AI, and I should let him to tell us more

00:31.120 --> 00:34.600
about his background. But he has done very interesting and

00:34.600 --> 00:38.680
fascinating work. And I think that he also has some new

00:39.080 --> 00:46.800
galleries about what he has created his journey of arts. So

00:46.800 --> 00:51.040
please, Tom, let us know what you are up to, what you're doing.

00:52.200 --> 00:55.560
Sure. Thank you, Ali. I really appreciate you letting me

00:55.560 --> 01:01.440
participate. I've often turned down talking invitations, but I'm

01:01.440 --> 01:05.080
an MIT alum. I really appreciate, I like your research,

01:05.080 --> 01:07.680
and so I appreciate your invitation. But I also really

01:07.680 --> 01:13.080
like IEP. When I look back at MIT, that was like the IEP courses

01:13.080 --> 01:15.840
that I took were really great. And so I can tell your students

01:15.840 --> 01:18.640
that you might not remember everything you're doing at MIT,

01:18.680 --> 01:20.880
but I'm pretty sure you're going to remember some of your great

01:20.880 --> 01:26.680
IEP courses. So my background is fairly eclectic, as you might

01:26.680 --> 01:30.680
imagine. I was undergraduate in math, and I went to the media

01:30.680 --> 01:35.560
lab about 20 years ago at MIT. And I was part of a group that

01:35.560 --> 01:40.440
was doing sort of incorporating graphic design into programming.

01:40.760 --> 01:45.120
This was sort of exploring, exploring that space. And so

01:46.600 --> 01:48.960
I'll talk a little bit in my talk about about some of the

01:48.960 --> 01:53.480
precedents that we did there. But I was coming into that sort of

01:53.520 --> 01:57.080
looking at ways that people could think more about coding as a

01:57.080 --> 02:03.720
creative discipline. There's a lot of tools now that didn't

02:03.720 --> 02:06.240
exist in, and there's even the idea of creative coding. And a

02:06.240 --> 02:12.400
lot of that came out of the work that we did in our group. After

02:12.400 --> 02:16.720
leaving MIT, I sort of went off into industry for many years. I

02:16.720 --> 02:21.840
was always interested in AI and art and the sort of intersection

02:21.840 --> 02:24.960
of those. But it was kind of the AI winner. And it was, you know,

02:25.200 --> 02:29.560
I got out of school, went into industry, but when deep learning

02:29.560 --> 02:32.840
started getting exciting, you know, maybe about five to 10 years

02:32.840 --> 02:36.000
ago, that's when I re-vectored back into space. And so I found

02:36.320 --> 02:41.760
I stuff back in academia and have a wearable hat. So I, you know,

02:41.760 --> 02:45.240
I have my own research students here, and I'll speak a little bit

02:45.280 --> 02:48.680
of my talk about some of the research we do. So there's like

02:48.680 --> 02:51.360
Rebecca was saying, there's a lot of number of practical tools

02:51.360 --> 02:55.240
you can build that will help other people use the medium

02:55.240 --> 02:58.320
creatively. And I do some of that research. But as you alluded

02:58.320 --> 03:02.040
to on the side, I also have my own separate arts practice. And

03:02.160 --> 03:04.520
most of my talk is going to be about that. Hopefully that'll

03:04.520 --> 03:07.240
be a good, I think everyone's art practice is different. But I

03:07.240 --> 03:10.360
think seeing seeing that might be hopefully inspirational for

03:10.360 --> 03:13.560
some students that are wondering what that's about or how they

03:13.560 --> 03:15.360
might one day get into something like that.

03:16.800 --> 03:20.080
Yeah, definitely. That would be actually very great. I think

03:20.080 --> 03:28.520
that maybe, yeah, I think you and also work off the work that

03:28.520 --> 03:33.280
Rebecca details give us some taste about what it really means

03:33.280 --> 03:38.160
from the art practitioners as well as, you know, the computer

03:38.160 --> 03:40.320
scientists. So please go ahead.

03:41.320 --> 03:46.320
Okay, I will try to share my slides and we'll get started. Okay,

03:46.320 --> 03:50.560
so I will briefly just reintroduce myself for the video. So my

03:50.560 --> 03:54.200
name is Tom White. You can find me online on my handle trip

03:54.200 --> 03:58.520
net. And there's my website trip.net. And as I said, I'm an

03:58.520 --> 04:02.160
artist and lecturer at Victoria University of Wellington School

04:02.480 --> 04:12.080
of Design. And in my work here, I teach classes on creative

04:12.080 --> 04:14.200
coding. So I teach primarily in the context of an art and

04:14.200 --> 04:18.720
design school to students that are interested in, it might be

04:18.720 --> 04:21.200
that they're interested in special effects, or that they're

04:21.200 --> 04:24.880
interested in web design, but they want to sort of incorporate

04:26.240 --> 04:28.880
programming into their into their tool set, because there's a

04:28.880 --> 04:32.680
lot of capabilities and things that have a lot. So in the past,

04:32.680 --> 04:35.960
where someone might, you know, learn charcoal drawing or

04:35.960 --> 04:39.120
search specific techniques, more and more today, students want to

04:39.120 --> 04:42.040
learn programming. So I teach programming, but not computer

04:42.040 --> 04:45.960
science angle, kind of creative coding angle. I also have

04:45.960 --> 04:49.000
research which I do with my graduate students. And I'll touch

04:49.000 --> 04:52.000
upon that briefly in the talk as well, where we make creative

04:52.000 --> 04:55.840
tools using some of these, these technologies and in that

04:55.840 --> 05:02.880
context, I also have a workshop at the NeurIPS conference

05:03.440 --> 05:06.160
creativity and design that I do with other co-hosts, where we

05:06.160 --> 05:09.400
post a lot of this research. What my talk today is about is

05:09.400 --> 05:12.120
about a lot of my artwork, and specifically the artwork that I've

05:12.120 --> 05:16.240
been doing the last three to five years. And I'm going to sort

05:16.240 --> 05:18.720
of go through and give you a background of myself and then

05:18.720 --> 05:22.800
talk about that a little bit. Let's see if I can get my first

05:23.040 --> 05:26.840
slide. There we go. So, so this is the outline of the talk. I'm

05:26.840 --> 05:29.960
going to give basically a three minute version of the talk, the

05:29.960 --> 05:32.920
TLDR, like what's the point of this talk, so you can sort of

05:32.960 --> 05:37.080
understand it all at once what this is. Then I'm going to back

05:37.080 --> 05:39.680
up and give you a little bit more background about where I'm

05:39.680 --> 05:45.000
coming from, my background in MIT, and since then, I'm going to

05:45.000 --> 05:47.160
spend a little bit of time talking about the precedents at

05:47.160 --> 05:51.880
art. So when you're doing artwork, similar to, you know, if

05:51.920 --> 05:54.280
you're writing a research paper, you have your references at the

05:54.280 --> 05:57.200
end, and that contextualizes and talks about how you're

05:57.200 --> 06:00.040
building on things in the past, I think it's important. In

06:00.040 --> 06:02.600
art, I think a lot of times it's implicit to be here, I'm

06:02.600 --> 06:05.400
going to make it a little bit more explicit about what my

06:05.400 --> 06:08.720
precedents are, and what I find inspirational, and what I'm

06:08.720 --> 06:14.240
trying to, what I'm using as reference points. Then I'm going

06:14.240 --> 06:16.360
to get into the meat of the talks. The meat of the talk is

06:16.360 --> 06:20.160
really, or the core of my talk here is about AI representation

06:20.160 --> 06:24.080
and abstraction, and specifically, my investigations

06:24.080 --> 06:28.240
into kind of exploring what these neural net vision systems,

06:28.400 --> 06:33.000
what their representations are, and how to convey that in an

06:33.000 --> 06:37.240
artistic context. I will talk a little bit about other AI art

06:37.240 --> 06:41.360
approaches, including, you know, other peers that I have that

06:41.360 --> 06:43.960
are in this space, and I'll touch a little bit about my

06:43.960 --> 06:48.400
research there. And then I'll close out by talking about the

06:48.800 --> 06:53.400
sort of like some of the implications of the artwork. So

06:54.000 --> 06:59.160
what is, what happens when the systems, when we're using these

06:59.160 --> 07:03.880
systems more and more as our kind of auxiliary eyes? So that's

07:03.880 --> 07:07.400
the outline of the talk. This is the core of it. So this is the

07:07.760 --> 07:11.840
sort of the summary that my ideas of our artwork is that

07:11.840 --> 07:16.000
machines have their own way of seeing. So they, they see things

07:16.880 --> 07:21.600
they're very, they're very accurate in classifiers. But I

07:21.600 --> 07:25.000
think they also, importantly, have slightly different ways of

07:25.000 --> 07:29.080
seeing the world. And so what I'm trying to do in my artwork is

07:29.080 --> 07:34.080
trying to expose that to a wider audience and trying to

07:34.080 --> 07:37.600
investigate how it is that these machines see the world and how

07:37.600 --> 07:41.640
it might be the same or different as us. So as a sort of a

07:41.640 --> 07:44.960
corollary of that, because they see things differently, I'm, I

07:45.000 --> 07:48.200
believe we can create or this kind of core and buy machines. In

07:48.200 --> 07:51.400
other words, you could use the machine perception to create

07:51.400 --> 07:54.400
different types of art, which actually the machines themselves

07:54.400 --> 08:00.240
have some opinions about. So I'll talk a little bit more about

08:00.240 --> 08:04.040
what that means. And so the summary there is that through art,

08:04.040 --> 08:07.560
we can actually appreciate ways the machines can perceive the

08:07.560 --> 08:12.040
world. So similar to you might encounter a new culture that you

08:12.080 --> 08:15.920
hadn't before, and you might explore the art of that culture.

08:16.080 --> 08:20.840
I'm trying to explore the art that's created when we introduce

08:20.840 --> 08:27.760
machine perception into into the process. So I'm very, so the

08:27.760 --> 08:30.600
one line version of that is I'm interested in how machines read

08:30.600 --> 08:34.080
images. So that's kind of the point of what I'm doing. Here's

08:34.080 --> 08:38.560
three prints that I've done the one on the left is an eye and

08:38.560 --> 08:40.840
that was created from a data set of eyes and I'll talk a little

08:40.840 --> 08:45.120
bit about that later. The one all the way on the right is it's

08:45.120 --> 08:47.600
actually a face and that was made not so much for a class

08:47.600 --> 08:50.480
environment for facial recognition system. The first step in

08:50.480 --> 08:55.720
facial recognition is a detector. And so that was made to look

08:55.720 --> 08:59.600
like a face to a face detection system. And the one in the

08:59.600 --> 09:02.120
center here, which I'll make a little bit bigger is a screen

09:02.120 --> 09:06.960
print of two chickens. And so this is all of these are based on

09:07.000 --> 09:09.920
pure images. And I'll get as you can imagine more into the

09:09.960 --> 09:12.880
details of how this happens as I go through the paper or go

09:12.880 --> 09:17.280
through the talk. But this is based on a data set of many

09:17.320 --> 09:21.960
actual images of chickens, the system created this this output.

09:22.520 --> 09:25.920
And we can actually turn it around and we can show this to

09:25.920 --> 09:28.560
the system again and look at its imagination of what it

09:28.560 --> 09:33.120
visualizes. And we get kind of this so it can actually introduce

09:33.120 --> 09:36.280
diagnostics into the process and say, Well, what is it that the

09:36.280 --> 09:39.720
computer sees when they look at this print? And that is a

09:39.720 --> 09:43.800
some kind of indication of the richer inner world, these neural

09:43.800 --> 09:49.720
networks. So that is my, I guess, you know, three minute version

09:49.720 --> 09:52.560
of the whole talk. And now we're going to sort of back up and go

09:52.560 --> 09:55.560
through that and a little bit more, a little bit more detail.

09:56.080 --> 09:59.360
And I'll start off talking a little bit more about myself. So

09:59.360 --> 10:03.920
this is made in my studio, currently, where I do my

10:03.920 --> 10:08.720
printing. And I'm here in Wellington, New Zealand, I'm

10:08.720 --> 10:11.960
going to go back and talk about not everything I've done, but

10:11.960 --> 10:16.080
maybe some of course, precedence along the way for getting to

10:16.240 --> 10:19.000
sort of just this stage of my art making.

10:20.600 --> 10:25.760
25 years ago at SIGGRAPH, I did in sort of using machine

10:25.760 --> 10:28.760
learning techniques of the time artificial evolution to make

10:28.760 --> 10:33.600
these real time video filters that processed video and then

10:33.600 --> 10:36.440
showed that on a screen. And you could also manipulate the

10:36.480 --> 10:39.200
evolution process to create different filters.

10:40.880 --> 10:46.760
In 1998, I finished my master's thesis at MIT. And so, like I

10:46.760 --> 10:50.520
said, I was in a group at MIT, John Midas Aesthetics and

10:50.520 --> 10:55.840
Computation Group. I was there with many kind of inspired

10:56.240 --> 11:00.320
people that were coming in from both design and computer

11:00.320 --> 11:04.560
programming, software engineering. And I was interested

11:04.560 --> 11:07.200
at the time on better human computer human interfaces. So I

11:07.200 --> 11:11.880
built a custom hand interface because I was interested in

11:11.880 --> 11:14.720
multi-touch. So multi-touch wasn't really a big idea in the

11:14.720 --> 11:18.760
90s. So I had to, there wasn't any interfaces that did that. So I

11:18.760 --> 11:22.520
built a camera optics based interface. So it's this liquid

11:22.520 --> 11:25.280
pad. And when you pushed on it, you would get this handprint

11:25.760 --> 11:28.160
that you see in the bottom right. And then on top of that, I

11:28.160 --> 11:32.720
built various interfaces, for example, sliding left to right

11:32.720 --> 11:39.640
to move, to move something, or there's various very different

11:39.640 --> 11:43.480
ways you could you could interact based on having more than a

11:43.480 --> 11:45.960
mouse. So I think this is common sort of ideas now, but this

11:45.960 --> 11:49.280
was something that we did in the Media Lab and I did for my

11:49.280 --> 11:53.280
thesis. But maybe one of the, one of the one of the other

11:53.280 --> 11:56.960
things at this time, as I did another art project, which is

11:56.960 --> 11:58.880
still pretty well known, which is called Stream of

11:58.880 --> 12:04.400
Consciousness, which in which I incorporated this handpad, which

12:04.400 --> 12:08.000
could measure sort of pressure across the whole pad into this

12:08.160 --> 12:12.440
interactive exhibit where words were flowing through a stream,

12:12.480 --> 12:14.880
and it was called Stream of Consciousness. One sort of

12:14.880 --> 12:18.280
interesting footnote about this is that as the words were going

12:18.280 --> 12:20.760
through, you could push on a word, and it would spring out

12:20.760 --> 12:24.760
related words. So if you were looking at the word, you know,

12:25.280 --> 12:27.920
university, and you pushed on it, it might say faculty or

12:27.960 --> 12:31.960
school, or academia or something like that. And we used

12:31.960 --> 12:35.520
wordnet at the time, that was part of the programming idea to

12:35.520 --> 12:39.200
kind of find out interesting related words, first anonyms and

12:39.200 --> 12:42.080
synonyms and kind of its sense that that it uses, which is the

12:42.080 --> 12:46.400
basis for ImageNet, which is a lot of the artwork I do today. So

12:46.400 --> 12:49.760
this was kind of a, again, using AI techniques at the time,

12:50.200 --> 12:55.480
incorporating it in these, these interactive exhibitions. But

12:55.480 --> 12:58.120
one of the, one of the interesting legacies, and I think

12:58.120 --> 13:02.320
another bit that feeds into my, my current practice is that one

13:02.320 --> 13:04.560
of the interesting legacies of this group is that we made a lot

13:04.560 --> 13:08.520
of interesting sort of core software that enabled a lot of

13:08.520 --> 13:11.120
people to do different things. So there was a handful of

13:11.120 --> 13:16.320
researchers, and I, and Ben Fry, and Jared Chiffman were

13:16.320 --> 13:19.200
students in the group, and we're responsible for creating and

13:19.200 --> 13:23.920
maintaining this core library called ACU. And the library was

13:23.920 --> 13:28.600
so that basically we could conduct our experiments. And it was

13:28.600 --> 13:33.200
constructed such that it was easy to do kind of a sketch, a

13:33.200 --> 13:36.440
software sketch. And the reason I bring this up is that this

13:36.440 --> 13:41.600
software was the basis for other systems that have come since.

13:41.600 --> 13:46.360
So Ben, who worked on this and Casey later adapted some of

13:46.360 --> 13:50.520
these ideas and made a version of this called Processing, which

13:50.560 --> 13:55.760
is actually quite popular still as a creative toolkit. And then

13:55.800 --> 13:59.240
later, another group of students took actually the code for

13:59.240 --> 14:04.200
this and made a new toolkit called Open Frameworks, which

14:04.360 --> 14:07.440
uses a lot of a lot of some of the code in this ends up in Open

14:07.440 --> 14:09.840
Frameworks. And then they, of course, built upon it and built

14:09.840 --> 14:17.240
a community around it. But many of the conventions and cliches of

14:17.240 --> 14:23.480
this programming style kind of live on today. And this is, it's

14:23.480 --> 14:25.800
interesting as it alludes to my current work, because what we

14:25.800 --> 14:28.160
were doing at the time, we were just trying to figure out how

14:28.160 --> 14:33.080
to make it easy for people to create sketches or drawings on

14:33.080 --> 14:36.240
the computer. And I feel that that's a lot of what I'm doing

14:36.280 --> 14:39.600
currently is just my audience is no longer other people, it's

14:39.600 --> 14:44.400
actually machine learning processes. And then I did other

14:44.400 --> 14:46.960
things and worked in industry, but for the purpose of this talk,

14:47.000 --> 14:50.320
we'll just, you know, chalk that off to the AI winner. And I'll

14:50.320 --> 14:53.920
come back to that in a little bit. So there's, after that, I

14:53.920 --> 14:58.960
graduated from school, I did some other things. I'm going to

14:58.960 --> 15:02.000
also open the chat over here in case other people had feedback

15:02.000 --> 15:05.280
that I can secretly see. Okay, so now I'm going to talk about

15:05.280 --> 15:11.160
precedence and art and and some of the background looking at it

15:11.160 --> 15:14.040
from a slightly different angle of what it is I'm trying to do.

15:14.480 --> 15:17.520
I'm going to go all the way back to 1927. And there's an artist

15:17.520 --> 15:21.600
called Stuart Davis that I admired his technique. And what he

15:21.600 --> 15:27.240
was doing, or what he did for kind of his central core

15:27.240 --> 15:30.200
inspiration for his work is that there was one year that he did

15:30.200 --> 15:32.840
the egg beater series. And the egg beater series was his attempt

15:32.840 --> 15:36.240
to look at the world in a new way. And the way he did that, his

15:36.240 --> 15:39.120
technique for that was he took some common everyday objects, he

15:39.120 --> 15:43.560
took an electric fan, a rubber glove and an egg beater. And he

15:44.280 --> 15:48.000
nailed those to a tabletop and basically forced himself to paint

15:48.000 --> 15:51.000
those things over and over and over until he thought he was

15:51.000 --> 15:55.720
doing something different and new. His, in his own words, what

15:55.720 --> 15:58.400
he said his intent was in doing this was to strip a subject

15:58.400 --> 16:02.440
down to the real physical source of its stimulus. So here's one

16:02.440 --> 16:05.880
of his egg beater works where he's, you know, taking these

16:05.880 --> 16:10.320
common objects and trying to find something new in them. And I

16:10.560 --> 16:14.200
think that that's some of the that alludes to sort of some of

16:14.200 --> 16:16.600
the work that I'm doing. And I'll sort of touch on that in a

16:16.600 --> 16:22.080
little bit. Another precedent for me was Harold Cohen, who, for

16:22.080 --> 16:25.600
many years, experimented with generative drawing systems, and

16:25.640 --> 16:29.440
is arguably the first kind of AI artist. He came at this from a

16:29.440 --> 16:31.800
different angle. He is a very successful visual artist and

16:31.800 --> 16:37.600
painter. And he decided in the early 70s that he wanted to go

16:37.680 --> 16:41.000
into programming into an artificial intelligence to see if he

16:41.000 --> 16:47.320
could codify some of his ideas on, on mark making in a, in a

16:47.320 --> 16:52.240
formal way. So here's an example of one of his programs and he

16:52.600 --> 16:55.840
initially set out to build an autonomous program. And he called

16:55.840 --> 16:58.520
the program Aaron and Aaron would kind of encompass all the

16:58.520 --> 17:05.080
ideas he had about how to generate visual works. So I

17:05.120 --> 17:09.040
visited Harold a couple of times and this is me visiting him.

17:09.120 --> 17:12.640
And when he's working on some of his later works, and I think

17:12.640 --> 17:17.320
it's also interesting to discuss like, even though he had these

17:17.320 --> 17:21.680
ideas that the the artworks were being creative autonomously

17:21.680 --> 17:25.360
initially, later in life, he was working more as a collaborator

17:25.360 --> 17:29.320
with these systems. So he was sort of working a little bit or

17:29.320 --> 17:32.920
finding ways where it was more of a co creation process.

17:35.440 --> 17:40.600
But across all of his, all of his different phases, his core

17:40.600 --> 17:43.080
question across this, and this is what I kind of share his

17:43.080 --> 17:46.840
inspiration is, what is what he his question was, what is an

17:46.840 --> 17:51.120
image? And the way he didn't mean that like technically what is

17:51.120 --> 17:53.720
an image, but he meant like, what is the minimum condition

17:53.960 --> 17:57.640
under which a set of marks on a page functions as an image or

17:57.640 --> 18:02.920
conveys meaningful information. This is a sketch he had from one

18:02.960 --> 18:05.720
of his papers. And I would kind of rephrase this in a machine

18:05.720 --> 18:10.560
learning context is saying, what is it that in what is it that

18:10.560 --> 18:13.800
images could do kind of central to representation and

18:13.800 --> 18:17.360
extraction, like what makes simplification work? How is it

18:17.360 --> 18:21.120
that simple drawings can be evocative and seem to stand for

18:21.360 --> 18:27.720
something else? So I think that this is kind of a core, a core

18:28.360 --> 18:33.360
concern of Harold's that that I also share. And then one last

18:33.400 --> 18:37.160
kind of precedent and art that I think is important for me is

18:37.160 --> 18:41.240
just talking a little bit about pop artists. This is not so much

18:41.240 --> 18:47.680
as a from the, the content side, but from the form side, this

18:47.680 --> 18:50.560
is actually, I share some techniques with many of the

18:50.560 --> 18:55.680
pop artists like Andy Warhol, Roy Lichtenstein, and how I

18:55.720 --> 18:58.920
execute my artwork. So for anyone that's not familiar with

18:58.920 --> 19:03.320
screen printing, if I'm making this chicken image over here on

19:03.320 --> 19:06.240
the right, that's actually printed, but it's printed from

19:06.240 --> 19:09.600
two layers of ink. And so these are essentially stencils that I

19:09.600 --> 19:14.400
have to mix inks for, and then put onto the page. So what I'm

19:14.400 --> 19:17.400
showing on the left are the masks that generate the

19:17.400 --> 19:22.320
stencils to give you the image on the right. So the stencils

19:22.320 --> 19:24.840
themselves get burned, transferred onto screens. These

19:24.880 --> 19:28.800
are physical kind of stencil, it's like a stencil, except it

19:28.800 --> 19:31.600
can have holes on it, there's a mesh there. And then you just

19:31.600 --> 19:36.160
sort of pour ink across the, across this, this screen, and

19:36.160 --> 19:39.200
you smash some ink through it. And that's the kind of physical

19:39.200 --> 19:43.520
process of, of making these, these prints. Here's a little

19:43.520 --> 19:45.800
slideshow I have where I'm starting off with a blank canvas.

19:45.800 --> 19:48.960
It's actually a blank, we base coated a color onto it, we put

19:48.960 --> 19:52.920
a screen down, I put some ink on top of it, I spread the ink out

19:52.920 --> 19:55.800
that's called the flood, then you smash the ink through and you

19:55.800 --> 19:58.640
get a layer. And then the more colors you want, the more layers

19:58.640 --> 20:01.240
you have to do that. So for the second color, you have to do

20:01.240 --> 20:04.920
another screen and another layer. For the third color, it's

20:04.920 --> 20:10.000
another screen, that's me pushing ink through that layer. And

20:10.000 --> 20:14.280
then there's a fourth color there. Put the ink on the screen,

20:14.280 --> 20:17.040
spread it out and push it through. So just kind of

20:17.080 --> 20:23.400
emphasizing that there's a kind of a, a specific technique I'm

20:23.400 --> 20:27.880
using that has, you know, precedent for getting these

20:27.880 --> 20:30.960
are acrylic on canvas works or acrylic on paper, but it's

20:31.000 --> 20:33.360
instead of a brush technique, it's a screen printing technique.

20:35.960 --> 20:38.400
And here's the result of some of that. So this is kind of just

20:38.400 --> 20:42.000
showing you what happens at the end is that I have these prints,

20:42.760 --> 20:46.240
and the prints end up kind of, you know, in an exhibition or in

20:46.400 --> 20:53.800
some type of art context. So that's kind of the precedence, the

20:53.800 --> 20:57.680
art precedence. I think it's useful kind of now to talk about

20:57.680 --> 21:01.320
more what this artwork is and what it is I'm trying to say with

21:01.320 --> 21:06.280
this with my art pieces. So I'm going to get into the sort of

21:06.280 --> 21:09.400
the main section here, which is the AI representation and

21:09.400 --> 21:12.280
abstraction, like how is it I actually create these, these

21:12.280 --> 21:15.840
works and what is it that they stand for. So just going back

21:15.880 --> 21:20.640
to revisiting Stuart Davis. So in my mind, Stuart Davis spent a

21:20.640 --> 21:24.640
year basically learning how to perceive and represent familiar

21:24.640 --> 21:28.600
objects in new ways. So he, you know, imagine going in every

21:28.600 --> 21:31.560
day and staring at these same three or four objects of

21:31.560 --> 21:34.840
rubber glove and electric fan until finally you're seeing

21:34.920 --> 21:37.000
there have been ways you hadn't before, and then you're trying

21:37.000 --> 21:41.360
to put that on the page. And my kind of core question is, is

21:41.360 --> 21:45.360
can we similarly use computer vision to introduce new ways of

21:45.360 --> 21:47.760
perceiving or representing familiar objects? So instead of

21:47.760 --> 21:51.920
going in kind of going to this meditative trance, can we kind

21:51.920 --> 21:55.080
of look at things through the eyes of the computer vision

21:55.080 --> 21:59.360
systems and see if that causes us to see familiar things in new

21:59.360 --> 22:04.800
ways. So appropriately enough, one of my first experiments in

22:04.800 --> 22:08.480
this was an electric fan. So this is an electric fan training

22:08.480 --> 22:11.640
set. So this is the input into the system where you have

22:12.640 --> 22:15.600
computer vision systems, as you probably know, you know, need

22:16.120 --> 22:19.320
hundreds of thousands of images. So I believe in the image net

22:19.320 --> 22:23.640
electric fan data, there is, you know, over 1000 images of an

22:23.640 --> 22:26.520
electric fan. And this is just quickly paging through some of

22:26.520 --> 22:29.720
those. And then what I did with that is I made a system that

22:29.720 --> 22:33.760
optimized perceptually for creating something that looked

22:33.760 --> 22:38.800
like an electric fan to multiple, multiple trained neural

22:39.040 --> 22:43.000
networks. And so here I'm showing the print that I made. I'm an

22:43.000 --> 22:46.240
electric fan, it was created using a similar technique done in

22:46.240 --> 22:49.960
layers. And this is on the on the right side, these are in the

22:49.960 --> 22:53.800
middle, I guess, is the graph showing what the opinions of

22:53.800 --> 22:58.080
this print are when shown to different networks like Inception

22:58.080 --> 23:02.480
or ResNet or VGG. And they all are very confident that they're

23:02.480 --> 23:06.440
looking at electric fans. So this, this is using techniques

23:06.440 --> 23:11.000
also from like adversarial examples. So that's some that

23:11.000 --> 23:14.480
these are computer search security techniques, where you try

23:14.480 --> 23:17.280
to essentially fool a computer into thinking it's seeing

23:17.280 --> 23:20.600
something. But in my instance, I don't, I'm not tricking it, I'm

23:20.600 --> 23:24.000
actually just trying to make a super stimulus or stimulus that

23:24.000 --> 23:27.800
is most evocative of that category that it knows. And so

23:27.800 --> 23:32.400
this is an example of me trying to see how these computer AI

23:32.480 --> 23:37.200
algorithms receive these common objects. Here's an example of

23:37.240 --> 23:40.440
the the drawing system, sort of in progress, as you can imagine,

23:40.440 --> 23:43.400
it's an optimization, it starts off kind of putting lines on the

23:43.400 --> 23:48.000
page where it thinks they're most needed. And then it's, it kind

23:48.000 --> 23:52.160
of anneals or optimizes over that, and makes the image look

23:52.160 --> 23:56.240
more and more like the target class that it's, it's trying to

23:56.240 --> 23:59.840
make. And one way that I contextualize this and or talk

23:59.880 --> 24:04.280
about this in my writing, is that for me, it kind of inverts

24:04.280 --> 24:07.920
the computer as a tool stereotype. So for for many,

24:07.920 --> 24:11.680
many years, we've seen the computer as kind of a way to

24:11.680 --> 24:15.960
execute someone's vision. So use it down and use Photoshop and

24:15.960 --> 24:19.120
you have an idea and you kind of do it, you know, use the tool

24:19.120 --> 24:21.960
to express your idea. What I'm trying to do here is I'm kind of

24:21.960 --> 24:25.160
inverting that and I've made it a tool that the computer vision

24:25.160 --> 24:30.800
systems themselves can use to create their own visual outputs.

24:30.960 --> 24:35.120
So so the I'm sort of making the tool for the perception

24:35.120 --> 24:41.240
systems. So after I finished some of those, I did a series of

24:41.240 --> 24:44.360
10 of these. And I'm going to talk a little bit about them to

24:44.360 --> 24:48.000
talk about how they're made, but also what machine learning

24:48.000 --> 24:53.280
concepts are kind of bubbling through. So this is this is a

24:53.280 --> 24:56.600
print called binoculars, again, using the sort of same two layer

24:56.600 --> 24:59.960
technique. This is the data set on the right. And then this is

24:59.960 --> 25:02.960
the print on the left. One interesting thing I thought about

25:02.960 --> 25:06.160
the way this print turned out is that it's viewing the binoculars

25:06.160 --> 25:09.560
in three quarter view, which is kind of an interesting angle,

25:09.560 --> 25:12.920
because you can see all of the features pretty easily from from

25:13.120 --> 25:15.600
kind of not instead of a straight on view as much of the

25:15.600 --> 25:20.360
data is. This is a shark specifically a hammerhead shark,

25:20.360 --> 25:25.240
which is one of the categories of the image net. And here it's

25:25.360 --> 25:29.200
done a very few number of strokes to kind of try to represent

25:29.200 --> 25:33.240
that the essence of the shark outlines. In fact, we can kind

25:33.240 --> 25:37.400
of count the strokes. I believe it's about 14 or so. So this is

25:37.400 --> 25:40.960
a little animation which is showing the strokes one by one

25:40.960 --> 25:45.720
just so you can see the number of primitives. So from, you know,

25:45.720 --> 25:47.520
from a programming point of view, you might think if this is

25:47.520 --> 25:51.880
the number of parameters in the space, but this is also kind of

25:51.960 --> 25:54.480
defining the complexity of the image that that it's able to

25:54.480 --> 25:59.360
produce. After the shark, I did a different one, which is iron

25:59.400 --> 26:02.520
again, kind of an interesting view it took it to this kind of

26:03.560 --> 26:06.680
perpendicular view, which is where you might get the most

26:06.800 --> 26:10.720
characteristic view of the iron. The iron uses the same number

26:10.720 --> 26:15.080
of strokes as the shark does. So one fun thing we can do is we

26:15.080 --> 26:17.640
can actually start with the shark. And then we can move the

26:17.640 --> 26:22.160
strokes around so that they create the iron. And I did this

26:22.200 --> 26:26.360
in software. And then at each step, at the end steps, I show

26:26.360 --> 26:28.640
this to the networks to see what they're thinking what what

26:28.640 --> 26:32.440
they believe they're looking at. And on one side here, you can

26:32.440 --> 26:34.560
see the six networks are very convinced they're looking at an

26:34.560 --> 26:38.160
iron. And then we move the strokes around and they're

26:38.640 --> 26:43.640
so they're looking at a hammerhead shark. I did this a

26:43.640 --> 26:46.320
couple of years ago as a demo, there's a lot of debate in the

26:46.320 --> 26:48.440
deep learning community how these models work. And there's a

26:49.960 --> 26:52.320
there's a camp, or there's a group of people that actually

26:52.520 --> 26:57.120
were proposing that deep learning was mainly triggering on

26:57.120 --> 27:00.160
textures and didn't have any global structure information. So I

27:00.160 --> 27:03.000
think this is a pretty compelling counter example that says,

27:03.000 --> 27:05.800
well, no, these deep learning systems might preference

27:05.800 --> 27:09.120
structure or texture or might use texture, but they do seem to

27:09.120 --> 27:13.600
have some global structure information in their vision

27:13.600 --> 27:19.560
systems. Just a couple more of these. This is a cello. Or I

27:19.560 --> 27:25.120
should say this is the the the label cello, which as you can

27:25.120 --> 27:29.320
imagine has not only cellos, but cellists and other things in

27:29.320 --> 27:33.560
the photos that are the used for classification. And it's

27:33.560 --> 27:37.560
interesting to try to not I don't have any I don't have any

27:37.560 --> 27:41.440
special insight into the the drawing that is produced by the

27:41.440 --> 27:43.800
system. So I don't know. People ask me what is it

27:43.800 --> 27:46.720
representing? And I don't know specifically, but I can go into

27:46.720 --> 27:51.280
the data and poke around and take my best guess. So to me, this

27:51.280 --> 27:54.360
cello image looks like this, these kind of characteristic

27:54.360 --> 27:57.800
photos in the data. So you might call this a mode of the data

27:57.800 --> 28:03.080
or a or about 25% of the data has this general shape where you

28:03.080 --> 28:07.120
have a cellist behind the cello playing and I'll point out a

28:07.120 --> 28:10.760
couple of things that I think I see that resemble these training

28:10.800 --> 28:13.760
examples. And I'll point out one thing that's very different. So

28:13.760 --> 28:16.880
you might try to figure out in the 30 seconds that it needs me

28:16.880 --> 28:18.960
to get there, whether you can find the thing that's different in

28:18.960 --> 28:21.960
those. But the things that are the same as I see this like

28:21.960 --> 28:25.560
light colored object kind of looming behind this darker one.

28:27.280 --> 28:31.720
I see that maybe fingers curled around a fretboard, which was

28:31.720 --> 28:35.120
kind of surprising to me. But the one thing that's very

28:35.120 --> 28:38.760
different about the image that it drew and all the cellists that

28:38.760 --> 28:41.120
I saw on the data set is all the cellists in the data set are

28:41.120 --> 28:43.720
right handed. And all these that I just kind of pulled out

28:43.720 --> 28:47.240
randomly are, but improbably the cellist it decided to draw was

28:47.240 --> 28:51.240
left handed. And that's relevant because, you know, I'm

28:51.240 --> 28:53.880
interested in how these computer vision systems see and they're

28:53.880 --> 28:56.160
actually because of the way they're trained, they're blind and

28:56.160 --> 29:00.840
left right symmetry. So because of image augmentation to offer

29:00.840 --> 29:04.040
the data sets, they, they cannot see or they're not aware of

29:04.040 --> 29:08.680
any features that are that are not that are that are dependent on

29:08.680 --> 29:11.480
left right symmetries. And so it chose to kind of invert this

29:11.480 --> 29:13.880
one and draw a left handed challenge, which I thought was

29:13.920 --> 29:18.280
was interesting. And then there's a there's two more. So this is

29:18.280 --> 29:22.160
one that's measuring cup, not too surprising here seeing the

29:22.160 --> 29:25.880
other ones where it's it's drawn a measuring cup that you might

29:25.880 --> 29:30.720
see in your kitchen. The thing that I haven't sort of so in

29:30.720 --> 29:33.160
addition to choosing the shapes for these drawings, it's also

29:33.200 --> 29:35.360
choosing the colors. And I thought it was really strange that

29:35.360 --> 29:37.960
it chose this bright green color, because I've never seen a

29:37.960 --> 29:41.480
green measuring cup. So again, I dove into the data. And I found

29:41.480 --> 29:45.680
that the data set actually had a large number of examples where

29:45.680 --> 29:48.920
there were green measuring cups. And I dug into this little bit

29:48.920 --> 29:52.120
more. And it turns out that there's this collectible measuring

29:52.120 --> 29:54.720
cup called depression glass measuring cups. This was made

29:55.520 --> 29:58.640
around World War One. And for whatever reason, people were

29:58.640 --> 30:03.080
putting uranium in the glass. And these were collectible. And

30:03.080 --> 30:05.000
so there's a lot of these post donal onward people are

30:05.000 --> 30:07.760
training them because they're collectible. And this is an

30:07.760 --> 30:11.880
example of sampling bias. So arguably, this measuring cup is

30:11.880 --> 30:15.640
not very commonly green. But because of the way this data is

30:15.640 --> 30:18.080
kind of farmed off of the web, there's a lot of green ones that

30:18.080 --> 30:21.720
end up in the training set. So to this computer vision system,

30:21.760 --> 30:24.640
it's actually quite likely that there would be a green measuring

30:24.640 --> 30:28.960
cup. So this is again, kind of showing how the machine learning

30:29.600 --> 30:33.480
techniques are bubbling through it in the results. And then the

30:33.480 --> 30:36.520
last one I'll show here is tick. So tick is one of my least

30:36.520 --> 30:39.840
popular, at least by sales prints that I've done. No one wants

30:39.840 --> 30:44.560
to put a tick in their in their home, I guess, on the wall. But

30:44.560 --> 30:48.400
this one I'm pointing out because in addition to sort of the

30:49.080 --> 30:52.440
the print itself, it actually had a really strong response

30:52.440 --> 30:56.840
across many networks. And so I decided to quantify that. And the

30:56.840 --> 31:00.440
gold standard on this is to take the validation set of the data

31:00.440 --> 31:04.360
set itself. And so I took the the validation set for ImageNet

31:04.360 --> 31:07.320
and I took a network that wasn't involved in the creation of

31:07.320 --> 31:11.440
this. It was inception ResNet. And I scored all of the

31:12.400 --> 31:16.560
validation examples. And they fell into two classes, basically

31:16.560 --> 31:19.880
things that were ticks and things that weren't ticks. But the

31:19.880 --> 31:22.440
short of it is, is that this image of a tick registers

31:22.440 --> 31:25.680
stronger than all of the validation examples, which is

31:25.720 --> 31:28.680
fairly surprising. Like the tick response here is kind of like I

31:28.680 --> 31:32.360
said, a super stimulus or a stronger example of a tick to

31:32.360 --> 31:35.040
these networks and even pictures of the tick from the

31:35.080 --> 31:40.480
validation set. But this has precedent in art and design as

31:40.480 --> 31:45.120
well. Scott McLeod in his series in his book, Understanding

31:45.120 --> 31:47.240
Comics, talks about amplification through

31:47.240 --> 31:51.040
simplification and how if you're trying to represent, say a man,

31:51.200 --> 31:55.160
then you're maybe not well served by using a particular photo of

31:55.160 --> 31:58.200
a particular man, because that's that doesn't well fit the

31:58.200 --> 32:03.240
concept. It's too specific to, to one particular person. And

32:03.240 --> 32:06.640
actually by abstracting and removing some details, you

32:06.640 --> 32:10.800
can come up with a drawing or sketch that better represents a

32:10.800 --> 32:14.440
man or a person or, you know, different levels of abstraction

32:14.440 --> 32:18.840
in the in concepts. And so, arguably, I think that might be

32:18.840 --> 32:20.960
what we're seeing here, where we're taking a real tick and

32:20.960 --> 32:25.280
we're removing some of the some of the some features and leaving

32:25.280 --> 32:29.400
the most salient ones. This is also just as a footnote, kind of

32:29.400 --> 32:33.080
how symbolic abstraction and writing started. So Sumerian

32:33.080 --> 32:37.040
writing starts if you owe someone three oxes, you write, you

32:37.040 --> 32:40.520
know, a picture of three oxes in your play tablet and overtime

32:40.520 --> 32:45.280
that evolves into kind of symbolic abstractions. So that was

32:45.280 --> 32:48.480
my my series, Perception Engines, which is two prints, I'm

32:48.520 --> 32:51.080
going to talk briefly about some other series that I've done

32:51.080 --> 32:54.600
since then. There's one that I did after that called synthetic

32:54.600 --> 32:58.520
abstractions. I've covered it here because as a as a warning,

32:58.760 --> 33:02.840
this is technically not safe for work imagery. So if you are

33:02.840 --> 33:07.120
uncomfortable looking at images that at least vision systems

33:07.120 --> 33:11.520
say are explicit imagery, I encourage you to pause your video

33:11.520 --> 33:16.440
or look away now. So here we go. These are these are these are

33:16.480 --> 33:20.240
abstract prints that I made, specifically looking at not so

33:20.240 --> 33:23.760
much systems for ImageNet, but these systems that impact us all

33:23.800 --> 33:26.680
online. So, you know, we have Google Safe Search, and we have

33:26.680 --> 33:29.960
these other systems that try to shield us from certain images.

33:29.960 --> 33:33.920
And I'm wondering, like, well, are there abstract versions of

33:33.920 --> 33:38.720
what it is that that that that seems to trigger these filters? So

33:38.720 --> 33:41.480
as an example of one of these, I made this print Mustard Dream,

33:41.640 --> 33:44.160
it's on mustard colored paper, and it's just black and white

33:44.200 --> 33:48.720
ink. And when you show a print, this a picture of this print

33:48.720 --> 33:52.640
to three division systems like Google Safe Search, it'll register

33:52.640 --> 33:56.160
as a as adult or racy. Similarly, Amazon thinks it's

33:56.160 --> 34:01.000
explicit nudity and Yahoo thinks it's not safe for work. So here

34:01.000 --> 34:04.240
I am exhibiting that print. I did a series of these. There was

34:04.240 --> 34:08.040
another one pitch stream, another one composition with red,

34:08.040 --> 34:10.960
blue, and yellow. That's kind of an an art joke here, because

34:10.960 --> 34:16.600
it's a riff on a well known other work, except my arrangement

34:16.640 --> 34:22.600
of inks triggers these these algorithms, these filter

34:22.600 --> 34:26.960
algorithms. I have done some similar canvas baked works more

34:26.960 --> 34:30.280
recently in Spain. So these are two newer ones where I'm

34:30.280 --> 34:33.400
actually trying to, you know, I don't know what it is. And the

34:33.400 --> 34:35.240
other one, so people ask me, Well, what is it the computer

34:35.240 --> 34:38.520
season? I don't know. Like I said, I don't have any kind of

34:38.840 --> 34:42.000
knowledge. And maybe I wouldn't want to know. I'd like that we

34:42.000 --> 34:44.520
don't have interpretable ML where we can ask the system what it

34:44.520 --> 34:47.960
sees. Here I'm actually trying to steer the data set a little bit

34:47.960 --> 34:53.920
more by influencing it with with influence the result by data

34:53.920 --> 34:56.760
set. And so this is, for example, one of those I actually

34:56.760 --> 35:01.240
called this one illustrated nude, which I'll talk a little bit

35:01.240 --> 35:05.160
later about why I changed the title of it. But that's because

35:05.160 --> 35:07.200
you know, when you show this to Amazon, that's what it

35:07.200 --> 35:10.720
classifies it at. It says, you know, it thinks it's looking at

35:10.720 --> 35:14.040
it in an illustrated nude and it's fairly high confidence. And

35:14.200 --> 35:16.920
similarly, Google safe search again, thinks it's a racy or

35:16.920 --> 35:21.600
adult image. So if you are searching for this, you insert

35:21.600 --> 35:25.040
safe search, you wouldn't see it. And if you got an email, it

35:25.040 --> 35:28.960
might go in your spam folder, for example. Okay, so I'm going to

35:28.960 --> 35:31.600
talk, I'm just going to go a few more examples just to kind of

35:31.600 --> 35:35.520
get a gist of how this so after that, I made sure my systems

35:35.520 --> 35:39.480
were working kind of in the large with these online API. So

35:39.480 --> 35:43.480
this is a data set for killer whale, a painting of the killer

35:43.480 --> 35:47.920
whale, and then the responses from Google's online vision API.

35:47.920 --> 35:50.640
So you can see it thinks it's a killer whale or marine mammal.

35:51.200 --> 35:54.480
Part of the interest here is also what the ontology or the

35:54.480 --> 35:58.960
labels for the these different systems are. Similarly, here's a

35:58.960 --> 36:02.640
penguin. So we're looking at the data set on the left, thousands

36:02.680 --> 36:07.480
of images of penguins, the version of a penguin that ended up

36:07.480 --> 36:11.400
being printed. And then you can see the Google API sees this as

36:11.400 --> 36:14.800
a penguin, or a flightless bird, or even particular types of

36:14.800 --> 36:20.720
penguins like Emperor penguin. As I mentioned before, there's

36:20.720 --> 36:25.040
the ability to do custom data sets. And so as a commission

36:25.360 --> 36:29.000
collaboration I did with yacht for their album artwork, I did a

36:29.000 --> 36:32.480
series of custom data sets. This is one from from that series.

36:32.680 --> 36:35.840
Where we had a data set of eyes, and we trained the system to

36:35.840 --> 36:42.040
make a synthetic eye. And as you can see, when Google looks at

36:42.040 --> 36:44.920
this, it thinks it's seeing a face, or a nose, or eyebrow, or

36:44.920 --> 36:47.920
eyelash, there's sort of eye features coming through in the

36:47.920 --> 36:51.880
labels. And then, and this is another one I did, similarly,

36:51.880 --> 36:57.800
where it's pictures of rabbits. So it's this one is a various

36:57.800 --> 37:01.280
kind of simple rendition of a rabbit. And again, it's kind of

37:01.320 --> 37:04.760
funny what the Google labels for this end up being evidently

37:04.760 --> 37:08.600
they have separate labels for hair, rabbit, rabbit and hairs

37:08.600 --> 37:11.520
and domestic rabbit, it kind of triggers all of those different

37:11.520 --> 37:17.920
labels in its API response. More recently, and this is kind of

37:17.960 --> 37:20.360
getting up to what I've been doing the last year, I've been

37:20.400 --> 37:24.360
exhibiting these in groups. I found that instead of showing

37:24.360 --> 37:29.520
one example, it actually is more enlightening to see many of

37:29.560 --> 37:31.680
these at once, because you can kind of get a feel for what the

37:31.680 --> 37:34.680
visual language or the common elements are. So these are six

37:34.680 --> 37:39.240
chickens and six eyes, which I exhibited about this time last

37:39.280 --> 37:44.080
year at Separ Gallery. This is something I did subsequently,

37:44.080 --> 37:46.600
where I took that kind of even a step further, and there's kind of

37:46.600 --> 37:50.160
this room full of images, the computer thinks are knots. And

37:50.160 --> 37:56.280
so we have different shape and color combinations, all all

37:56.320 --> 38:01.240
being different images that the computer thinks are reminding

38:01.240 --> 38:03.320
it of and I think knots is kind of an interesting one, because

38:03.320 --> 38:06.240
it's kind of this amorphous shape, but there seems to be some

38:06.240 --> 38:10.000
commonality visually to what what the computer thinks is a

38:10.000 --> 38:13.920
knot. And then this is one I have this in progress. Maybe I'll

38:13.920 --> 38:17.240
give you about 10 seconds to think about what these might be.

38:17.680 --> 38:20.480
But these are actually this looks like it might be computer

38:21.320 --> 38:23.840
images, but these are actually photos of canvases, which I've

38:23.840 --> 38:27.320
taken where I'm starting to organize these into groups. And

38:27.320 --> 38:30.000
these are all the canvases that are just given completed. And

38:30.000 --> 38:34.800
this is for an upcoming exhibition on ants. And so all of

38:34.800 --> 38:38.880
these are shapes that will will trigger in computer vision

38:38.880 --> 38:42.720
systems, to different extents, in thinking that it's looking at

38:42.760 --> 38:47.640
at an ant. So that's kind of a summary of my work. I'm going to

38:47.640 --> 38:50.480
spend the remainder of my talk talking about other approaches

38:50.480 --> 38:57.920
and a little bit about and a little bit about my research,

38:57.920 --> 39:01.360
and then I'll just briefly talk about how my my artwork impacts

39:01.360 --> 39:04.920
the real world, or effects it has once it gets out into the

39:04.920 --> 39:08.840
world. So as a lot of the other speakers have mentioned is these

39:08.840 --> 39:11.520
generative techniques, and I think these are very relevant, and

39:11.520 --> 39:15.280
they're used on other approaches, but there's also this idea that

39:15.720 --> 39:19.040
other AI art approaches had different narratives. So I want

39:19.080 --> 39:23.120
to talk a little bit about that. So my interest, so when I came

39:23.120 --> 39:26.200
into when I started getting interested in modern deep learning

39:26.200 --> 39:29.200
actually got into generative networks as well. And for a

39:29.200 --> 39:32.200
while, I was doing a lot of important research on this or

39:32.200 --> 39:35.520
research that I enjoyed digging into how these things works in

39:35.520 --> 39:40.520
2015 and 2016. This is an artwork I did in 2016. It was just

39:40.520 --> 39:44.360
large, like two meter by one meter print. Here's a zoom in

39:44.560 --> 39:48.160
showing this. And this was images of faces. So at the time,

39:48.160 --> 39:53.080
these were really high quality neural net outputs for faces,

39:53.120 --> 39:55.680
you know, before we had style again, and these other more

39:55.680 --> 39:59.280
modern networks. And I was interested in kind of the space

39:59.320 --> 40:05.000
of faces that could be created. And I, and I wrote a paper talking

40:05.000 --> 40:07.520
about some of these techniques and how you could sample these

40:07.520 --> 40:12.680
networks to get some of the best, the best outputs from those. I

40:12.680 --> 40:16.080
also, since subsequent to that have continued looking at these

40:16.080 --> 40:18.480
generative networks in more of a research sense. So in my

40:18.480 --> 40:21.960
research capacity at the university, I work, I work, for

40:21.960 --> 40:25.520
example, in this system with my graduate student, as Rebecca

40:25.520 --> 40:30.920
alluded to, it's, there's a interesting kind of getting these

40:30.920 --> 40:34.240
out to tools to other creators. And so one of the ideas I have

40:34.240 --> 40:36.840
with this graduate student was to make a spreadsheet tool where

40:36.840 --> 40:38.720
instead of numbers in a spreadsheet, you actually had

40:38.720 --> 40:42.480
samples from a generative model. And could you do use this as a

40:42.480 --> 40:47.480
way of giving people sort of their own create creativity tool

40:47.520 --> 40:51.120
through the interface of a spreadsheet. So it's looking at

40:51.120 --> 40:53.960
at some of these generative models. But in the context of

40:53.960 --> 40:57.480
this art talk, it's, it's, I mainly just wanted to say, this

40:57.480 --> 41:01.160
is a tool other people are using. So Helena Saren is using

41:01.160 --> 41:04.400
these Mario Klingerman, Helena is using these in a way that,

41:04.720 --> 41:07.800
again, Rebecca alluded to where she's using these small data

41:07.800 --> 41:10.760
sets and essentially overfitting those data sets and changing the

41:10.760 --> 41:14.880
data set to get the result that she's looking for. Mario

41:14.880 --> 41:18.840
Klingerman in this project, Neural Glitch, was training one of

41:18.840 --> 41:22.000
these, and then he intentionally kind of damages the network, and

41:22.000 --> 41:26.640
then displays how that damage comes through visually in his

41:26.640 --> 41:29.760
artwork. So it's the idea that there's a sort of an intentional

41:29.760 --> 41:33.760
glitch in there. There's two other artists that that all

41:33.760 --> 41:38.720
mentioned that that one is Robbie Barrett, who did this

41:38.720 --> 41:42.680
project a few years ago called RDCGAN, where he looked at using

41:43.280 --> 41:46.840
these, these generic networks to specifically create portraits.

41:47.120 --> 41:50.240
And there's another well known, at least in a art precedent,

41:50.240 --> 41:52.840
which is obvious is Edmond de Bellamy, that's well known,

41:52.840 --> 41:55.160
mainly because it went up for auction for a very high amount

41:55.160 --> 41:59.760
of money, a Sotheby's a couple of years ago. These actually, I'm

41:59.880 --> 42:02.880
putting up mainly because they use very, very similar techniques.

42:02.880 --> 42:05.960
In fact, they share some code and techniques across them, but

42:05.960 --> 42:08.080
they have very different narratives behind them. And so I

42:08.320 --> 42:11.360
wanted to kind of point out, and the narrative here is I mean,

42:11.360 --> 42:14.640
like what the story is behind how these are made. So when Robbie

42:14.640 --> 42:18.080
is, is, and when I'm, you know, showing my work, it's very much

42:18.240 --> 42:21.120
talking about using these things as a tool or as a collaboration.

42:21.400 --> 42:25.200
But there is a strong push or there's a lot of people in the

42:25.200 --> 42:28.520
art community that more say that, no, it's the, the computer is

42:28.520 --> 42:31.080
autonomous and it's making these artworks. And that was the

42:31.400 --> 42:34.560
kind of stance that obvious took. And it's one that resonates

42:34.600 --> 42:38.160
with people. I think taken to an extreme, what you get is you

42:38.160 --> 42:43.920
get a lot of people making work with robots. So it's common for

42:43.920 --> 42:46.320
people to make these robot artists. And here's just one

42:46.320 --> 42:51.680
example of that. But these have a long history. So there's

42:51.680 --> 42:55.680
actually a long history of making these drawing automatons. And

42:55.680 --> 43:00.680
here's one, you know, that's 200 years old, where it's basically

43:01.680 --> 43:05.640
being driven by gears. And so I just mentioned this is that I

43:05.640 --> 43:08.360
see this as a slippery slope. And so I said, kind of stay away

43:08.360 --> 43:12.200
from this and intentionally don't use any, at least for now,

43:12.440 --> 43:15.600
kind of drawing automaton, because I want to contextualize

43:15.600 --> 43:19.360
what I'm doing as a, as a collaboration between what I'm

43:19.360 --> 43:22.800
doing. So going back kind of again to, to Harold Cohen and

43:22.800 --> 43:27.800
his work, and most of the artists in the space. This is a what

43:28.240 --> 43:31.320
I'm what I'm doing is not so much me handing over full autonomy

43:31.320 --> 43:33.880
and saying the computer is the artist, but coming up with a

43:33.880 --> 43:37.880
co creation process where the there's a role for me and

43:37.880 --> 43:42.840
there's a role for these systems that I'm making. Okay, and in

43:42.840 --> 43:46.480
the last five minutes here, I'm going to spend, I'm going to

43:46.480 --> 43:48.760
talk a little bit about what happens when we put these things

43:48.760 --> 43:52.400
out in the wild, and how how these are some of my artwork, how

43:52.400 --> 43:55.520
it's kind of how we can understand it. So of course,

43:55.600 --> 43:58.560
there's, we can ask the machine what it sees, we can use

43:58.560 --> 44:01.120
visualization techniques, but the one I'm highlighting most

44:01.120 --> 44:04.080
here is these unintended consequences, like what, what

44:04.080 --> 44:08.160
surprises has happened. And keep in mind that for from the eyes

44:08.160 --> 44:11.360
of the computer perception system, this is Magritte's

44:11.520 --> 44:14.120
trajectory of images where he was kind of pointing out like a

44:14.120 --> 44:17.280
picture of a pipe is not a pipe. But that distinction often is

44:17.280 --> 44:20.600
lost with these perception systems, like if they see

44:20.600 --> 44:23.880
something in a representation of something, they more or less

44:23.920 --> 44:28.440
think it's that thing. So the first way you can tell or

44:28.440 --> 44:30.840
interpret or think about my artwork is you can ask a system

44:30.840 --> 44:34.280
what it sees. So if I take this picture I took of me with my

44:34.320 --> 44:38.440
artwork six chickens, and I feed it to the Amazon API, it will

44:38.440 --> 44:41.280
come back and so very diligently that it found a

44:41.280 --> 44:44.640
chicken and a chicken and a bird and etc. And there's a person

44:44.640 --> 44:46.960
over on the side. And I think that's one way to kind of

44:46.960 --> 44:52.480
understand how these systems are viewing or understanding these

44:52.480 --> 44:56.080
artworks. So the computer, the systems themselves. There was

44:56.080 --> 44:58.280
another way that I talked about in the beginning where you can

44:58.280 --> 45:01.640
actually use visualization techniques. So this is using some

45:01.640 --> 45:04.480
research from open AI where they have visualization techniques

45:04.480 --> 45:08.320
similar to deep dream, where they take, I can take one of my

45:08.320 --> 45:10.920
prints and feed it to one of these systems and ask it to kind

45:10.920 --> 45:16.120
of imagine how it relates. So here's the print to chickens.

45:16.440 --> 45:20.360
Here's kind of the, the inner imagination or visualization of

45:20.360 --> 45:23.280
what the computer sees when it sees that. And you can even take

45:23.280 --> 45:26.400
that a little bit further and try to visualize that in 3D. So

45:26.400 --> 45:29.240
this is a little slideshow where it attempts to add some fake

45:29.280 --> 45:35.960
depth to that to that image so that it can kind of understand

45:36.120 --> 45:39.400
understand it. But I think what's most interesting is how

45:39.400 --> 45:42.440
these things kind of accidentally kind of bump into real world

45:42.440 --> 45:46.000
systems. So I'm going to talk about a couple of those to close

45:46.000 --> 45:49.840
out. So this is an exhibition I talked about in 2018. I had

45:49.840 --> 45:54.360
these prints that were supposed to trigger various safe search

45:54.360 --> 45:56.480
filters. And this is an exhibition on the left. I have this

45:56.520 --> 45:59.920
not so great photo of it with a lot of glare and stuff. And I

45:59.920 --> 46:01.760
actually took a picture of this and was going to post it to

46:01.760 --> 46:05.360
Tumblr. And it didn't allow me to post it to Tumblr. The actual

46:05.360 --> 46:08.440
post failed and it said this post contains adult content, which

46:08.440 --> 46:11.440
violates our community guidelines. I was flagged as adult

46:11.440 --> 46:14.160
content and it was not displayed. So I think that's one

46:14.160 --> 46:17.400
example of it kind of bumping up against these real world

46:17.400 --> 46:21.360
systems. Similarly, I had another work in that series,

46:21.400 --> 46:23.760
Architectural Digest wanted to put a print in one of their

46:23.760 --> 46:27.720
magazines. I showed them some prints and they wanted the most

46:27.720 --> 46:31.920
colorful one, which was Lime Dream. Again, maybe not the most

46:31.920 --> 46:34.600
appropriate because it's supposed to be explicit imagery, but

46:34.600 --> 46:38.440
it they decided to go with it anyway. And so when I got a copy

46:38.440 --> 46:43.640
of the magazine, I took a great picture of the magazine, and I

46:43.640 --> 46:46.520
fit into the Amazon API and it was still convinced it was

46:46.520 --> 46:49.600
looking at explicit nudity. So I think this might be the first

46:49.600 --> 46:54.600
example of a cybernetic centerfold like a basically a photo of

46:54.600 --> 46:59.760
a nude made for these kind of vision systems that has appeared

46:59.760 --> 47:04.680
in a magazine. Similarly, like the art gets sold in weird ways.

47:04.680 --> 47:10.080
And so I had a print at a gallery and they sold off without my

47:10.080 --> 47:13.960
knowledge, one of these prints to Sloan Kettering when so if you

47:13.960 --> 47:16.480
go to their academic offices in New York, you can actually see

47:16.480 --> 47:21.400
one of these prints. Again, I don't know that I would that it

47:21.400 --> 47:23.760
might be the most appropriate for this environment. And in

47:23.760 --> 47:26.400
fact, that's one of the reasons I alluded to earlier, I gave

47:26.400 --> 47:30.360
these initially kind of these vague names. But now I've called

47:30.360 --> 47:33.240
them illustrated nudes explicitly because I want if

47:33.240 --> 47:36.000
someone buys or sees one of these, I want them to know what

47:36.000 --> 47:37.920
it's supposed to represent. And so the only way for me to

47:37.920 --> 47:41.480
package that is to put it in the title where it can't be missed.

47:42.480 --> 47:48.360
I did a print a couple of years ago that I took to NeurIPS. I

47:48.360 --> 47:52.440
was stretching it in my hotel room and hanging it up. And when I

47:52.440 --> 47:55.320
went to take a picture of it, I was very happy. This is my

47:55.320 --> 48:00.200
camera interface. When this little yellow rectangle appeared.

48:00.200 --> 48:03.040
So I think if anyone has a phone, they know this is when your

48:03.040 --> 48:06.200
camera's trying to focus on a face. And so just the fact that

48:06.200 --> 48:09.960
this was a shape intended to trigger a face and machine

48:10.000 --> 48:13.720
learning algorithms, kind of pulled the focus onto onto this

48:13.720 --> 48:17.520
because it thought there was a face in the scene. And then the

48:17.520 --> 48:19.880
last one of these is just getting ready for this talk. So as

48:19.880 --> 48:22.520
you can imagine, getting ready for this talk, I'm trying to find

48:22.520 --> 48:25.560
images of my artwork. And I'm pulling them on my phone. And

48:25.560 --> 48:28.800
lo and behold, when I pull up my phone, down at the bottom, it

48:28.800 --> 48:32.440
says we have several people and places in categories of things.

48:32.680 --> 48:35.280
And here's the categories of things that I've kind of

48:36.240 --> 48:38.400
partitioned for you that you might be interested in, like

48:38.440 --> 48:41.040
your animals and your food. So of course, these aren't real

48:41.040 --> 48:44.880
animals and food. If I search for banana, it thinks it's you

48:44.880 --> 48:47.680
know, I've done a print of banana and when I'm working in

48:47.680 --> 48:49.480
the print, I take a lot of pictures of it. So these have

48:49.480 --> 48:54.080
gotten classified as bananas or scorpions. This is a sombrero

48:54.080 --> 48:56.320
print. I was working on it. It's funny too, because it kind of

48:56.320 --> 48:59.400
mixes the real this is a picture of mine, a friend in a funny

48:59.400 --> 49:02.960
hat, and it ends up putting that in the picture with the artwork.

49:03.240 --> 49:05.760
Again, like I'm saying, the computer doesn't have a concept

49:05.760 --> 49:08.760
of what a lot of times is a representation of something

49:08.760 --> 49:12.720
versus what is the real thing. So if I do a search for syringe,

49:13.000 --> 49:15.440
not only do I get this weird photo that I took and I don't

49:15.440 --> 49:18.880
remember why I have a real syringe, but it probably I get

49:18.880 --> 49:22.200
this like really wacky, hypercolour syringe print that I

49:22.200 --> 49:24.680
was working on a few years ago. So I think it's interesting how

49:24.680 --> 49:29.640
these kind of get collapsed in the in these vision systems as

49:29.640 --> 49:34.440
being kind of the same thing. So that's it. Those just to

49:34.440 --> 49:37.160
recap the core ideas of my artwork is that the machines have

49:37.160 --> 49:42.240
their own way of seeing. We can create art for and by machines,

49:42.600 --> 49:45.760
which is what I'm trying to do is trying to use the the

49:46.080 --> 49:48.720
capabilities of machines and understand art through their

49:48.720 --> 49:54.720
eyes. And that and through art, we and by we, I mean, like,

49:55.920 --> 49:58.360
people knowledgeable about deep learning, but also people who

49:58.360 --> 50:01.240
might not have any background in machine learning can appreciate

50:01.240 --> 50:05.640
the ways that that machines perceive the world. So thanks,

50:05.640 --> 50:10.400
that's my online handle for Twitter and my web page. I'm

50:10.440 --> 50:13.120
open for taking questions that I will also say for questions that

50:13.120 --> 50:16.560
I'm very open for things that were part of the talk, but also

50:16.560 --> 50:19.200
might have been things only loosely alluded to in the talk,

50:19.200 --> 50:23.880
if you had questions on, you know, more about research tools or

50:23.920 --> 50:28.160
the community, I think that I'm happy to take any sort of a

50:28.160 --> 50:30.280
broad range of questions. Thank you.

50:31.480 --> 50:38.360
Thank you so much, Tom. It was so great. So I was wondering if

50:38.360 --> 50:40.000
students have questions.

50:42.760 --> 50:46.840
This is from very early on in the talk, but I'm wondering why you

50:46.840 --> 50:50.880
chose screen printing as your medium of choice.

50:56.120 --> 50:58.920
Great question. Sorry, I had to take a second.

50:59.040 --> 51:03.120
So, yeah, so there's I wanted to actually make physical work. And

51:03.120 --> 51:05.800
I thought that was, you know, it's actually easier for me to make

51:07.120 --> 51:10.800
work that is on the computer, but it takes a kind of a step

51:10.800 --> 51:15.040
further to create something that's printed. And that's also

51:16.320 --> 51:18.120
something there's two parts of the question, why make physical

51:18.120 --> 51:21.120
work at all? And then if you're making physical work, why have

51:21.120 --> 51:24.240
it be screen printed? So I think that that two reasons for

51:24.240 --> 51:27.040
making physical work. One is, I think that there's a lot of

51:27.080 --> 51:29.280
reasons for making physical work. One is, I think that people

51:29.280 --> 51:32.800
relate to physical work differently and when they go into

51:32.800 --> 51:35.680
a gallery and art setting. So for years, I did these interactive

51:35.680 --> 51:38.600
installations with screens and things. And I think that people

51:38.600 --> 51:42.800
come into those little bit of the defensive because sort of

51:42.800 --> 51:44.920
this technology right away, like you go in and you see the

51:44.920 --> 51:48.040
screen, you see, you know, this camera looking at you or

51:48.040 --> 51:51.800
something. And I think it sets a tone for how this is for what's

51:51.800 --> 51:57.000
to be expected. And so I really wanted to do a physical print

51:57.360 --> 52:06.480
that sort of was more more about the was more about how the

52:06.480 --> 52:09.720
computer sees it than about the short putting the process in

52:09.720 --> 52:11.840
the gallery. But the purpose for doing the screen printers, I

52:11.840 --> 52:15.240
wanted something that was very exact. So I could I actually have

52:15.240 --> 52:17.080
done some prints that there weren't part of the software I've

52:17.080 --> 52:20.160
experimented with other techniques, brush techniques. And

52:20.160 --> 52:22.360
so it's not really critical to use screen printing, but screen

52:22.400 --> 52:28.960
printing allows me to with some level of very precision, like

52:29.720 --> 52:34.440
make a traditional artwork. So and it has precedent kind of in

52:34.440 --> 52:37.480
the art world through through these pop artists. So I felt it

52:37.480 --> 52:40.480
was kind of a good, a good middle ground for kind of

52:40.480 --> 52:43.040
executing these I could of course print them out on a printer

52:43.040 --> 52:46.400
or something else. But I think that I wanted to kind of

52:46.400 --> 52:48.720
constrain my way, you might think in the same ways that

52:48.720 --> 52:51.600
artists in the past, it could strain themselves. So it makes

52:51.640 --> 52:55.640
it more easy to compare my works with existing artists, if I'm

52:55.640 --> 52:58.840
kind of operating with the same constraints are under the same

52:58.840 --> 53:01.880
interface, you might think of it. One thing I'll say too about

53:01.880 --> 53:04.920
making physical work is it makes it much more difficult to

53:04.920 --> 53:08.760
pull off these techniques, these kind of adversarial techniques

53:08.760 --> 53:11.360
because I don't know what the lighting or the angle of the

53:11.360 --> 53:15.880
photo is. So I try I have to make these these results work for

53:15.880 --> 53:21.320
a distribution of possible photos that might be taken. So if

53:21.360 --> 53:23.760
that's a part of you know, if you look at adversarial images

53:23.760 --> 53:26.440
research, there's kind of doing it on the computer and there's

53:26.440 --> 53:28.720
doing it in the physical world. And it's always kind of more

53:28.720 --> 53:31.320
difficult challenge to take these to the physical world. But

53:31.320 --> 53:33.720
that's kind of where I was interested in taking this.

53:38.560 --> 53:40.080
Cool. Thank you so much.

53:40.080 --> 53:56.600
Excellent. Are there are there other questions? I think that

53:56.600 --> 54:00.800
this is very interesting how thinking in art and you know,

54:00.800 --> 54:08.440
having this dual view of computers and arts can inspire

54:09.000 --> 54:13.120
such an interesting work. For instance, we see that, you know,

54:13.120 --> 54:15.720
as you said, one of the applications of this could be

54:15.720 --> 54:20.560
really adversarial attack and, you know, cyber security, which

54:21.240 --> 54:26.800
is really hard to think about it. If you only want to think about

54:27.240 --> 54:33.960
security and exclude this type of artistic practices, I never

54:33.960 --> 54:41.840
thought about it in this way, like how art can help for better

54:41.840 --> 54:45.720
understanding of, for instance, data bias or algorithmic

54:45.720 --> 54:52.120
bias, or other aspects of AI. So I thought that was very

54:52.120 --> 54:52.840
interesting.

54:54.320 --> 54:57.440
Thanks a lot. Yeah, that actually came through in my in my

54:57.440 --> 54:59.440
original generative work. So when I was working with

54:59.440 --> 55:01.960
generative networks, I found that they were actually really

55:02.000 --> 55:06.360
good at visualizing bias. So if you go back in my paper, I

55:06.360 --> 55:11.080
talked a little bit about how the celeb a data set has a label

55:11.080 --> 55:16.560
for for smiling. But in that data set, and I talk about ways that

55:16.560 --> 55:20.040
you can build dialers where you can turn up the smiling and

55:20.040 --> 55:23.240
down the smiling by using the labels on the data set. But

55:23.240 --> 55:25.680
there's bias in the data set where women are much more

55:25.680 --> 55:30.000
likely than men to be smiling. Just as a product of how the

55:30.000 --> 55:32.360
data was collected, like twice, almost twice as likely as

55:32.360 --> 55:35.240
like 1.5 or something like that. And so as you build these

55:35.240 --> 55:39.280
tools that are that are intended, or as you look at as you

55:39.280 --> 55:41.880
tell the network that you want to make an image smile more,

55:41.880 --> 55:44.320
you're actually also changing some of the masculine feminine

55:44.320 --> 55:48.080
characteristics of the image. And I thought it was a very

55:48.080 --> 55:51.280
visceral way to kind of see some of the machine learning bias.

55:52.120 --> 55:54.200
So that can come through in these general networks. But

55:54.200 --> 55:58.240
yeah, I think it also comes through in the visualization of

55:58.240 --> 56:03.080
these. And I'm kind of careful not to to couch my work too much

56:03.080 --> 56:05.400
in adversarial examples, because I think that I certainly use

56:05.400 --> 56:08.560
some of their techniques. But where I depart from them is that

56:08.560 --> 56:11.320
is they're always trying to do something imperceptible, or

56:11.320 --> 56:15.040
something that is, well, it's adversarial, like in the name,

56:15.040 --> 56:17.600
they're trying to trick the system, whereas I'm more using

56:17.600 --> 56:21.040
it as a visualization or kind of getting under the hood and

56:21.040 --> 56:25.320
kind of understanding what the stimulus is are that that might

56:25.320 --> 56:27.360
trigger the these in the first place.

56:28.560 --> 56:33.560
Yeah, also, it is very interesting how you know, doing by

56:33.560 --> 56:39.760
minimal strokes, you could achieve these things. And this is

56:39.760 --> 56:43.120
another way of thinking of compression, for instance, which

56:43.120 --> 56:46.000
was, as you said, inspired from art.

56:49.160 --> 56:51.120
Yeah, I know that's that's a really good point. Yeah. And I

56:51.120 --> 56:53.920
think that so there's there's a very practical reason to do that

56:53.960 --> 56:58.640
too is because I'm not using any really advanced techniques for

56:58.640 --> 57:02.840
generating these. I'm basically doing like random search or

57:03.080 --> 57:06.200
genetic algorithm. So it's basically doing a search over

57:06.200 --> 57:08.920
the space of outputs, and it just move, you know, it does a

57:08.920 --> 57:13.200
search estimating the gradient. So the fewer parameters you have

57:13.200 --> 57:17.120
the easier job you're going to have sort of hill climbing in

57:17.120 --> 57:20.880
that aspect. So it benefits me to have a simple representation

57:20.880 --> 57:23.320
for this. And there's other things about my drawing style

57:23.360 --> 57:26.440
when I talk about making a dry system for for these, like I

57:26.440 --> 57:28.560
can't have too many discontinuities and things like

57:28.560 --> 57:31.960
that. And the drawing styles that have but but yeah, I think

57:31.960 --> 57:35.080
it's also very interesting to come up with these kind of tight

57:35.400 --> 57:38.440
like for the shark, you know, it's 12 strokes to kind of

57:38.440 --> 57:44.000
represent a shape that when shown to the system still seems to

57:44.000 --> 57:46.840
be a strong stimulus for that category.

57:47.960 --> 57:52.680
Yeah, that also reminds me of the work that scientists did in

57:52.760 --> 57:56.560
computer vision, for instance, a work of Antonio Trouble, where

57:56.920 --> 58:01.000
the question is how many pixels do you need the minimum number

58:01.000 --> 58:06.920
of pixels to show to a computer, for instance, and in terms of

58:06.920 --> 58:12.440
images to see what is this object or get a gist of this

58:12.440 --> 58:16.880
object, you know, for instance, the experiment that set up only

58:17.520 --> 58:23.800
between 32 picture pixels. And they found that, you know, you

58:23.800 --> 58:29.560
can get a gist of what is going on in this instance, image is

58:29.600 --> 58:33.960
this computer vision techniques. So that is also very

58:33.960 --> 58:34.720
interesting.

58:35.520 --> 58:38.280
Yeah, it's also similar to I don't remember the researchers

58:38.280 --> 58:41.360
that did the psychology research where they take face images

58:41.360 --> 58:44.320
and they put them in very low like they basically make icons

58:44.320 --> 58:47.520
where they're, you know, like eight by eight pixels or 16 by

58:47.520 --> 58:50.840
16 pixels. And you can still represent you can still recognize

58:50.840 --> 58:53.920
Abraham Lincoln or these famous figures in these very low

58:53.920 --> 58:58.200
resolution, resolution format. So I think people also have

58:58.200 --> 59:02.360
this ability to decipher these very, these very low

59:02.360 --> 59:04.840
information images as well.

59:05.880 --> 59:11.760
Yeah, very interesting. Excellent. Thank you so much, Tom. It

59:11.760 --> 59:18.600
was great and really inspiring work. And I appreciate that.

59:19.440 --> 59:21.680
Thanks so much for having me. And if anyone else has any

59:21.920 --> 59:25.520
questions, I'm happy to, you know, if you send me an email and

59:25.560 --> 59:27.840
follow up from the class, I know not everyone likes asking

59:27.840 --> 59:32.320
questions. I'm happy to follow up on, you know, you know, every

59:32.320 --> 59:34.520
artist is a little bit different. And so there's no

59:34.520 --> 59:38.280
common. There's no common template, I think you can follow.

59:38.280 --> 59:41.800
But hopefully this will be a good, this is one good example

59:41.800 --> 59:44.160
of a path that someone else might be interested in taking.

59:44.960 --> 59:50.160
Certainly. And then also, Tom has been involved with the

59:50.160 --> 59:56.240
workshop of machine learning for art and creativity at New

59:56.240 --> 01:00:02.520
Ribs for several years. So that's also an interesting and

01:00:02.520 --> 01:00:07.520
valuable contribution that he's making. So if any questions

01:00:09.160 --> 01:00:11.760
Yeah, exactly. So I do know a lot of that research and can

01:00:11.760 --> 01:00:14.280
direct you to those. And I would encourage you, if you are

01:00:14.280 --> 01:00:17.080
interested in a space, you can, I believe, still access a lot of

01:00:17.080 --> 01:00:20.080
the videos, for example, from this past year's workshop, just

01:00:20.080 --> 01:00:23.320
to get an overview of going a little bit deeper into some of

01:00:23.320 --> 01:00:23.960
these talks.

01:00:25.640 --> 01:00:29.400
Excellent. Thank you so much, Tom. Thank you. Thank you.

01:00:29.400 --> 01:00:31.440
Have a good one. Thanks, everybody.

