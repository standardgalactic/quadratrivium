Yeah, and we have our another specialist speaker today.
This is such a pleasure for me to have Shiri Ghinosar.
She's a computing innovation postdoctoral fellow at UC Berkeley.
And she has done so many interesting work
and very creative these models of current ARs.
Many interesting projects that she will hopefully
share a gist of that with us.
I can tell you about understanding
the evolution of her images that was very striking to me,
as well as the dance project that she did,
and other interesting things that she
tries to go beyond the only pixels.
So Shiri, go ahead and please start your talk.
Oh, thank you so much.
So hi, I'm Shiri.
And I'd like to talk to you today about the art of deception,
or basically using perception as a creative material.
And I have a lot of material, but I want this.
I know it's kind of strange to do this over Zoom.
People are kind of shy to budge in.
But it can really be a conversation.
We can take it this anywhere that you want.
It could be kind of like a fireside chat.
It could be, you know, we can make it this into different things.
But so I want to encourage you, if you have questions,
you could just start talking, because I don't have the chat on,
and I can't man all these things.
But just feel free to just stop me at any point.
And I want to say that, because I know that it's, you know,
this whole remote thing is kind of, you know, strange.
So OK, so let's start talking.
And I want to get you into the mood.
So I want us to look at a video together to start with.
And this is a really nice music video by Michel Gondry
that he made for the Chemical Brothers.
And I want you to really notice what is going on.
I want you to listen and I want you to look at what is going
at what is going on in this video.
OK, so if you zoned out a little bit in the beginning there,
because, you know, you look at this thing
and you think to yourself, OK, I'm looking out the window,
I'm a train, I've been there, it's kind of nice and relaxing,
but it's also a little bit mind-numbing.
So you kind of zone out and you're like, OK.
But then suddenly it hits you that there's a surprise here.
So actually all the visuals are, yes.
I think that did you want the student to tell you what they think
or you just want to tell them?
They can tell me what I, I can't really see you guys.
I can only see you.
OK, so if you want to say something, just like say it.
Yeah, if you please, if your students want to say something,
just unmute and say it.
I can also actually, before you do that, now that we've stopped,
I want to ask the audio, is it reasonable?
Is it too loud?
Is it too low of the of the videos?
It's a good time to fix this
because we're going to have a lot of audio and videos.
I think it was reasonable, maybe a little, maybe a little less like.
But it was good.
So OK, we made a little bit less loud.
If it's not going forward, complain.
Yes, excellent.
So if anyone wants to say what they think or should I read it?
So Ben says, it's repeating scene in sync to the music sections,
moving on to the next loop of the scene
as the track moves to the next section.
Simon says, the camera feels like it is bouncing in rhythm.
That is, that is, that is wonderful.
Those are wonderful observations.
But like, guys, just turn on your microphones and just talk.
OK, don't make Ali read all of your all of your things.
So it will be more just more interactive, you know.
OK, so yes, that's exactly what's going on.
Everything is being repeated and is synced to the music and sync to the beat.
And Michel Goudre is very good at this.
He did this a couple of times and amazingly, he did this by hand.
So there's a very interesting YouTube video.
It's kind of like a documentary about the making of this video.
And you see how they charted out the entire score of the music
and planned out exactly what they're going to show.
Individuals and literally, literally compose this by hand.
There's no, there's no AI here or anything like that.
So the interesting thing is that this kind of combination
between your different senses plays a trick on you and gives you this surprise,
which makes it interesting.
But if you didn't notice what was going on, you would think, oh, yeah, you know,
it looks like it's real, but it's not.
There's nothing real about this.
It's completely composed.
OK, so so let's take a step back and talk a little about
but put ourselves in context and look at the history of what people did
in order to depict the world from the very beginning of art.
The main goal of art was to, you know,
capture the world realistically.
So they started with with cave drawings of prehistoric people
who saw these big cows with the big big horns
and wanted to wanted to capture them with coal on the on the
walls of their caves.
And going forward, many, many years, people got more sophisticated.
This is a very nice mosaic from the Byzantine time.
And there's already a lot of dress here and different people and a lot of details.
But at this time, all of the depiction of people was very, very flat.
So there's almost no depth in the image and everybody is kind of frontal
and has a very just nondescript
facial emotions that everybody's kind of like severe going forward.
You know, this is the beginning of the Renaissance.
So this is Giotto.
And this is one of the first times that depth appears in an image.
And the way he does it is kind of it's a nice trick.
He puts this person in green in front of Christ.
And it kind of draws your attention inwards into the image.
And you can notice, you know, people are not frontal here anymore.
This is kind of diverging
style from from the Byzantine style.
And you can see motion in their faces.
So it's becoming more and more realistic.
Things get even better in the Renaissance.
Here we already have linear perspective.
We have the little people in the back, you know, are smaller.
And we have people in the front, which are bigger.
And it kind of makes you feel like everything is going into the image.
And you can kind of walk into the scene.
And, of course, going a little bit forward, you know, we almost achieved
perfection in painting.
So this is Van Eyck.
And this is a beautiful painting where he has here geometric
orthogonal perspective.
And there's, you know, you can really walk into this image.
There's a lot of detail if you blow up, for example, the chandelier.
You can see how the light is bouncing on the metal.
The little flames of the candles are very, very nicely painted.
Everything is very realistic and even better.
He puts a mirror behind those those two people who are getting married here.
And you can get even a double depth in the image.
So you can walk into the image and you can even see the backs of the people
and the people who are looking at the people who are getting married.
So this is is really wonderful.
And even you can see the the little beads, the glass, the glass beads
that are hanging up.
It just is is almost perfect.
Of course, all of this this perfection
basically ended with the invention of the camera
because then you could just walk out or look out of your window
and take a snapshot and you're done, right?
You know, this is like, there you go.
This is the world, the world, you know, it is this is it.
Anything you want, you could take a picture of and it kind of
made this this break in the art world because suddenly,
you know, the people who would do your your portrait for a lot of money,
we're out of we're out of we're out of work in a way.
Of course, this raises different questions.
So once you have an image that you can take and the pixels are real,
then the question can become about whether whether is this a real picture?
Is this fake? You know, is this kiss that happened in Paris?
Was it was it really did it really happen for real?
Or was it a stage? This one was staged, by the way.
Are aliens actually descending on New York or is this a movie?
You know, is this a conspiracy theory?
Are the people actually land on the moon?
Or, you know, can I believe the pixels of the camera or not?
And interestingly and interestingly for us,
artists didn't actually give up after the invention of the camera.
They actually became more sophisticated.
So here there is a nice example from Monet.
It's an impressionist painting and you can see a parade in Paris.
There's a lot of people going and walking in the street.
And, you know, let's try let's try to get you talking a little bit.
So like, what do you notice in this image in this painting?
Anybody? What's interesting about it?
There's a lot of flags.
There's a lot of flags, yes.
And what's happening to the flags?
Oh, they're waving.
So it's like depicting motion and they're kind of blown in the wind.
Why do you think they're blowing in the wind?
What makes you think that?
They're blurry.
That's true. They are blurry.
But are they really blurry?
Like, is it is it fuzzy, like a Gaussian filter on top of them?
We're really like blow up a section here.
They're not like blurry in the in the normal sense,
but they're not rigidly defined.
So like the looseness of the brushstrokes
and the way that they all kind of have that implies the wind and the motion.
Exactly, exactly.
So these flags are basically built out of strips of blue and yellow and red,
which is the French plan, but they're not it's spatially imprecise.
They're kind of jumbled strokes of paint,
even though each stroke of paint is not blurry,
the juxtaposition of them is kind of all over the place.
And the interesting thing about this is that
this kind of matches really nicely to our peripheral vision.
So in our peripheral vision, we see things kind of in a spatially imprecise way.
It's not that we see things blurry.
It's just that we don't really care where exactly they are in the image.
So if you kind of look at this painting in a glance,
you kind of think to yourself, oh, everything is fine.
But when you actually look and take a very close look at it,
you see that everything is really, really jumbled because you're using your
your actual, you know, attentive vision in the middle of your of your of your viewing field.
And that is more takes more into account the spatial arrangement of stuff.
And so really to get a sense of this painting, you have to kind of look
in every time you take a good look at it, you get a different sense of what's going on
because your periphery is giving you a different a different sensation.
And that's what kind of gives this this vibrant motion.
So in a way, you know, you get an impression.
That's why they call it impressionist because you get an impression of what's going on
at every glance, but it's not really correct.
And this is how Monet is playing with your visual system
and making you feel like there's something real here like motion,
which is really not in the pixels themselves.
OK, so, you know, fast forward many, many years.
And we have computer graphics.
Can I interrupt and be a little perfectionist?
I think that I really love these slides.
There is this panel
built order from, you know, that maybe you want to close it.
Oh, no. Oh, no, thank you.
You know, this happened to me before and I should know this already.
OK, OK, thank you, thank you.
Oh, why don't you say something before?
OK, good to make a postman always, always close the build order.
OK, so we went to computer graphics, right?
And suddenly, oh, no, no, I don't have my little timer.
OK, I'm going to have to look at the clock and suddenly.
You could model the world physically, right?
You can make a physical model of the light and the and the objects
and the scene and you can render them perfectly.
But you could almost render them too perfect.
This is, you know, what you get out of of graphics a lot of the time
is super, super real.
But it kind of makes you feel and gives you a strange feeling.
It's kind of too realistic.
You know, everything is a little bit too clean.
Everything is a little bit too robotic.
Do you know why that is?
Why does it make you feel that way?
OK, so I'll give you my idea.
My idea is that what's missing is the noise.
What's missing is the junk, is the dirt, is the cracks in the sidewalk
that are kind of random, the suit on the buildings, you know,
all the all the all the noise, all the junk, basically,
where our visual system is really used to seeing all this noise
in in the outside world, and there's a lot of beauty in this complexity
that when you take it away because you can't model it physically,
it's too complex to model, you know, our visual system just jumps in
and is like, oh, something is really wrong with this image.
And so this is very important perceptually.
And a lot of what I do is try to capture this complexity of the of the visual world.
OK, so we talked a little bit of our perception.
I want to make one more note about it, is that a lot of perception
is really in your head. It's not really what you see.
OK, so here's an example.
And if you look at this image and you look at the at the thing
that's that's in a red box, what is what do you see?
A soccer player, a player, yes, soccer player, yes, great soccer player.
But do you really think that there's a soccer player there?
Like if I take him and I magnify him.
Eh, it's just a bunch of pixels, right?
It's like there's white pixels, there's black pixels.
You can't really see the head.
There's a little bit of legs there, but there's really, you know,
there's there's really no player here, right?
Do you agree?
Like if you look at this and this, it's like, so a lot of what makes you think
there's a soccer player here is actually the context.
You know, there's a soccer field, it's in situation.
You're kind of like filling in the details.
Here's another example.
This is an image from a video that Antonio shot.
And it's kind of blurry, but but your brain can fill in the details.
What are you? What do you see here?
A man sitting at a computer with like a phone to his ear.
Exactly.
But if I show you the details, you suddenly see that everything is wrong.
White, he's not talking on the phone.
He's talking in the shoe and he's not looking at a computer.
He's looking at a at a garbage can.
OK, and the mouse is actually a stapler and there's a toaster there.
So everything is wrong, but the spatial configuration of the scene is fine.
And if you blur it out, your brain is just like, oh, this is fine.
You know, I'm just going to fill in the details.
So there are these loopholes of perception.
And this is a great opportunity for design.
And this is how we're going to weave in things that are not real,
but kind of make you think that they are as long as we're careful
to guard the important bits.
There are some there are some anchors of perception that we should care about.
And we have to make sure that they're there and your brain is going to do the rest.
OK, so what I'm going to cover today is I'm going to look at multiple examples of this.
So we're going to look a little bit at work that
looks at different senses and putting them together.
So here we're going to talk about audio and motion, so vision and hearing.
We're going to talk about modeling these complexities.
So the very, very fine details of individual appearance.
And we're going to talk about using time or using the passion,
the passing of time as a creative material.
I'll stop here and ask if there's any questions or complaints, and then I will continue.
OK, so let's talk first about audio and motion.
We're going to talk about how people move when they speak.
And it's the the what I'm talking about is going to look something like this.
I know how was it the personality travels?
Well, maybe there'll be some sort of physical explanation for it.
So people move when they speak and these are called conversational gestures.
OK, there's the kind of stuff that we do when when we speak.
And we never do basically when we don't when we don't talk.
And this type of gesture is not the only form of communicative gesture.
There's a continuum from language that accompanies speech, sorry,
language that accompanies motion that accompanies language to motion
that replaces language.
So, for example, sign language is a language of its own.
It doesn't need speech to go with it.
And blends are like Italian, like there's all kinds of things that Italians do,
which kind of have meanings that people agree upon.
So it's almost like a language.
But we're not going to talk about these things.
We're going to talk about the motion that accompanies speech when you do talk.
So what we want here is you want to learn about how people use gestures
when they speak and to do that, we're going to take in a raw audio signal of speech.
So literally the waveform.
And from that, we want to directly predict hand and arm gestures.
So it's going to look something like this.
So this is a really, really hard question.
OK, it's an ill-defined problem.
There's not a one to one correspondence between the audio and the motion
because I can say something today and move in a particular way
and do it a completely different tomorrow.
It's not synchronous.
The motion is often not synchronous to the related utterance.
And it's also a task that would be really hard for people to do or even to annotate for you.
So getting supervised learning in this setup is really, really hard.
And what we want to do is we want to, you know, learn about this in kind of in the wild setting.
So what we did is we went and collected a large data set of people who are speaking.
And for each frame, we annotated it automatically using an out of the box
2D pose detection.
So it kind of finds the pose of the arms of the hand and the hands of the people.
And the data looks kind of like this.
Waiting outside. Why are you telling me all this?
And you're not going to believe what they said they want to do.
Isn't that disgusting? It's 2012.
We're still not on the.
And then report it to the police.
Even Lauer's conversations light more photons per second.
Still none of the two young to be vaccinated.
And why would you choose not to do that?
So you can already see that people are very different in how they gesticulate.
But within a person, there's a lot of repetition.
And here I'm showing clusters in in the rows of clusters of gestures.
And this is because people just tend to perform the same motions over and over again,
because they have their typical style.
And that's great because it gives us a learning signal.
And so what we're going to do here is we're going to model each person individually.
And the way we're going to predict gestures from audio is we're literally going to take
the raw audio as input.
We're going to treat it like an image.
So we're going to think about it as a spectrogram.
We're going to stick it into a neural network and we're going to output a temporal series of poses.
And what each one of these really is, is just a vector of numbers.
But they represent the pose of the arms and hands of a person.
And the result looks like this.
So, you know, try and separate the two.
OK. Now the good news is the one thing to notice is, you know, for this given audio,
we predict a stack of these poses and we have two kinds of losses.
One is the regression loss to this pseudo ground truth of 2D poses that we have.
But what we really want is we want to generate motion according to the style of this particular person.
And so we add another adversarial loss that will tell us whether the motion is real or not with respect
to this person. And this makes a really big difference perceptually, because I'm going to show you here
on the left, you're going to see the result when you only have a regression.
And what happens when you do that is that you kind of get something that's very close to the mean.
So the motion is just very, very slow.
It's kind of like you're going through honey.
And when you add this adversarial loss, you kind of snap to one mode of the output because, you know,
I could have predicted different gestures, but I'm going to pick just one and make it look real.
OK. So that's going to be on the right hand side.
So, you know, it makes us feel a little bit better about what we're seeing.
And here, by the way, I'm pasting in the face.
I'm not predicting the face in this work, but I'm pasting in the ground truth face to give you more
perceptual context to see what, to kind of understand what you're seeing.
OK. So we can also look at prediction results for different people and just look at what it looks like.
Talking about the instantaneous rate, the rate, just when the concentration and I'm putting the ground truth
video on the bottom right, even though, you know, I could really have predicted any realistic motion.
I don't really want to predict the real one, but it's just for reference.
Energy to boil. Higher kinetic energy, higher temperature.
Logo in order to make it more modern. Yeah.
Yeah. Appropriate noise for that. Thank you.
That is where this method fails. So we're only taking audio as input.
There's no, like, there's no, you know, text, there's no semantics.
So the main limitation here is that even though we're using hours of data for training, we really, there's really not enough data to capture.
The cement, you know, fine grain semantics in order to predict metaphoric testers.
So I'm going to show you a video here.
And I want you to notice what happens when he says the word random.
So we, we don't get the circular motion that goes with random. We just can't predict that.
But we do predict the beat motion, which is these up and down repetitive motion that kind of comes with random motion.
So we don't get the circular motion that goes with random.
We just can't predict that. But we do predict the beat motion, which is these up and down repetitive motion that kind of cut the sentence temporally and make us feel like the person is actually moving while they're talking, which is, which is nice.
Another thing that's, that's interesting about this is that it's a little bit hard to do this kind of work the way that I described it.
And that is because the most important part of the body when you're speaking with your hands basically is, is the hands, right.
And the hands are very, very small in an image. And they're very articulated the fingers. And we're just not there yet in terms of computer vision 3D reconstruction techniques to get really good hand estimations.
And so our entire everything we did depends on the fact that we have good, good, you know, detections of body pose and that's not the case. So here's an example, just a randomly picked example from Ellen, Ellen's videos and you can see what happens if you just use an image based system to get the hand reconstructions.
To a CBS for one item. And the receipt was so long that I couldn't even believe it. I called it outrageous. I called it mind boggling. I called it long.
Okay, so this is complete junk, right, like if you try to request to this and everything would with fail miserably. So what do you do.
One thing that we noticed that is actually super interesting is that there is a really high correlation between the motion of the arms and the shape of the hand.
So we're going to do a trick and we're going to take as input not only the audio. If we want to get better hands but you can look at just taking the arms as input, and only from the arms, the arm motion, you can predict a pretty good shape of the hands which is which is almost seems magical.
And so if you do this body to hand thing, what you get looks like this.
Electrons around it.
The atomic oxygen would have six. So it has two more than it normally has. So there's like there's no pixel going in there's no audio going into here there's literally just the motion of the arms and it's it's amazing that it even works.
Again, there's not enough information right so it's not enough to capture metaphoric so if you see here.
She's got this.
And this is the ground truth video okay.
To a CBS for one item. And if you just take the arms, you're not going to get that it looks like this.
And so you can do an extra trick and you can say okay well I can take in the body but I can also just look at the input images because that's what image based reconstruction does anyways.
And together with this body prior, I can get a much better hand reconstruction.
So here on the left is the is the body only input no audio no pixels. And on the right, I have the body and the image together.
To a CBS for one item. And the receipt was so long that I couldn't even believe it I called it outrageous I called it mind boggling I called it long.
So this is already better this time like, you know, this is stuff that we can work with already and it looks much more realistic.
Are there any basic restrictions according to just like human anatomy that are also used within the model.
Sorry saying it.
Are there any just base restrictions on what movement is possible according to human anatomy like you can't completely I don't know. Yeah.
And not.
Sorry, I'm speaking to you from the garage so there's there's exciting background noise.
And not in my model. Okay, but there is in those.
So when we use, when we use ground truth that's coming from 2D reconstruction of key points or 3D reconstruction of hands in this case.
Those models have a lot of, you know, human pose priors built into them.
But we don't. So we're at we're kind of the goal of the, of the body to hands angle is to add an additional fire but one that is not based on, you know, physics that you might calculate from human bodies but it is based on a data driven prior.
So if you've seen enough bodies, you can infer this automatically. Does that answer your question.
Yes, thank you.
Cool. All right. So, let's see what time is it when do we stop at on the hour right.
Yeah, I mean we could, we could go a little further if you want. No, no, no, I'm just like I can I can I have a lot of stuff but I'm just going to cut accordingly.
So I'm going to show you the more interesting. I think that we said 2pm for students but it's. Yeah, that's fine. Okay.
So, okay, so in this work, we really only care to predict 2D motion because we that's what we were, we were interested in. And these stick figures are a nice output representation.
But they don't really provide you enough perceptual context as a viewer to actually see that what the result looks like if we do a good job. Okay.
So instead we can synthesize a video, an actual video the speaker so we what are we going to do we're going to take a real video of the speaker and
right, we can do the same trick where we get a 2d pose detection. And our goal is to learn the mapping between these 2d pose skeletons back to the real frame of the person and this is based on picks to picks you know you probably
you know we stole us so this is this is based on his work, but we're going to do this for a video.
So if you do that, what you get is something like this, where again, I'm pasting in the ground truth face key points because I'm not predicting that but I need them to make a video right so it's going to look like this.
Try and separate the two. Now, now the good news is, these days, very few people will say they are completely and I just want to like focus you on this. This is a completely fake video. Okay, it's completely synthesized there is like, there's nothing real about it.
And it's actually being predicted from raw audio with this 2d, you know system that gives us only the pose. And, and that's kind of amazing. It's, it's the coolest thing about this is that not only can we synthesize a realistically looking
video, but we also managed to capture a very convincing motion of the person.
Mostly what what makes you think that it's good is those beat gestures it's like okay I'm talking and I'm chopping up my sentence and there's kind of going with the same rhythm as my voice and not as convincing enough for people to think that that this is real.
Okay.
We're making fake videos, basically. Okay, so it's interesting to think about what can we do in order to decide whether a video was faked or not.
And it turns out that the same kind of idea can be applied to forensics as well. So I'm going to show you an example let's look at Obama and like look at this is from his address to the nation which he used to do every week.
And I want you to say to see what he does when he says you know hello everybody.
Hi everybody, hi everybody, hi everybody, hi everybody, hi everybody, hi everybody, hi everybody, hi everybody, hi everybody, he has this like upward motion. And basically, if you have the right words, and you have the right motion together, then it's Obama.
But if you try to do some deep fake of him and you try to change his lips and you know make him say something different, you wouldn't get the right motion to go with this. And this is like a great signal to see that something is fake.
And this is a really nice line of work that shorty did you see broccoli with honey for a while they look at exactly this kind of thing so it uses the, you know the whole of the person all of the details in multiple modalities to detect things that are fake.
All right, I'm going to move a little bit to something else if there's any question this is a good time.
Okay, so let's talk a little bit more about this idea of learning on the little details of a person, a person's appearance.
We're going to consider a very special style of gesture which is dance. And it's an art for it's been around since the beginning of time.
But frankly, nobody's really figured out how to capture the little subtleties of it. And what we're going to do is we're going to start we're just going to start directly by looking at a demo. Okay, so look at what what we can do.
Oh
Oh
To me
Close
If I tell my heart I'll still feel pain Whatever I do still feels the same
Nothing but tell me now
Everything we had well it's gone to waste
What we're doing here is we're taking this source video, we're detecting its pose and we're puppeting our target person to dance in the same way.
And it looks really nice, but I'm actually using a trick on you I'm using a perceptual loophole and and this is really interesting we usually don't show this but it turns out that the music is very helpful.
Okay, I'm going to show you exactly the same result without the audio and I want you to notice if it looks different to you.
Okay.
Okay, anyone
Mostly just look super unnatural where before it looked like it actually like was dancing.
It's awful right it's like everything is moving it's kind of wobbly and like, you know, just, just not right right. And it's really interesting it's exactly the same pixels okay I just turned off the audio, but when I show it to you with audio.
Your perceptual system is like whoa there's like music and there's dancing and it's together and you're filling in the holes.
You really are. And it's very important it turns out when you're showing a demo if you guys go and do things and design or and even an AI to kind of set it up in a way that that makes people be in their happy places.
And use multiple senses when they're when they're looking at something and it actually makes it look better than that it really is. And our goal here is actually not to make a perfect video I don't really want to be in the business of making people do things that they
didn't do our goal is to study you know the statistics and whatever, but that it is very helpful to use this different senses. Okay, so what are we doing here.
In a way, it turns out that we can use the same technology we talked about before to transfer dance motion from one person who knows how to dance to a different person who is a terrible dancer, like my co author here, think way.
And this is basically motion that's not conditioned on audio it's conditioned on different people.
And training is the same we train a cycle from you to your stick figure and back to you.
And you can think about it as trying to go from you to full reconstruction of you through this tiny little bottleneck, which is these 2d poses that are not learned.
And the idea here is to get learned a really good mapping from this stick figure back to the appearance of the person.
Because I really want to model what he looks like. I want to capture all the little minute details of his body and the way that he kind of, you know, looks when he's moving.
And I want to get the stuff that you can river really annotate and want to get all the complexity this this beauty in the details. This is what I want. So the goal here is not to start from a single image of a person and make them dance.
The goal is to actually model this person and for that I need a bunch of training data of this person from different poses.
And at test time, we unwrap the cycle, and we put a different person on the other hand.
So here test and training are not the same in purpose and this is different from a lot of other methods.
And these stick figures are a nicer presentation in the middle, because you're kind of agnostic for appearance like you could be, you know, bigger or smaller and width, but but it doesn't really matter for for your skeletal structure.
So if we managed to learn a good model of our target guy, we should be able to sample any new pose from this ballerina and synthesize a new image of the way.
And this is good because a good model of appearance to generalize to new poses.
So I'm using video synthesis kind of like at school it's like show your work, you know the generated video is as is a test of whether our modeling or perception of this guy really worked.
Okay, so we do some tricks to make this not you know frame by frame and achieve a temporal coherence which is which is important perceptually, but I actually want to talk about something else I want to talk about the fact that the face is very important perceptually.
Okay, and we didn't affect this this is, this is Renoir, and there's something really interesting about this painting, which is basically that there's different resolution of painting between different parts of the image right.
The face is very, very sharp, and the eyes are extremely sharp you can even see like the the glint of the of the lighting reflecting on her pupil.
But everything else is fuzzy, and it's kind of like the flags we saw before it's like not really in place.
This is something very, very interesting, it makes you draw your gaze to her eyes, which are the most expressive part of the face, it gives you an emotional reaction, and it makes you ignore everything else because everything else is in your periphery which is not really
attuned to spatial relationships anyways, and it kind of mimics our own perceptual experience.
When we gaze into someone else's eyes like somebody that we really love or somebody that we really want to listen to it's always you know everything else about their body is in the periphery.
So, so this is kind of the trick that Renoir is doing is playing on you, and we're going to play the same trick. Okay, so, so, you know this is this was done before amazing gans were around and so we didn't have the technology to make everything look perfect.
Okay, the face is important. If we get the face right, people will say oh this looks nice. So we actually devote a special again just to the face region, in order to correct it and more realistic.
It makes a big difference. So, if this is the baseline, this is after temporal swathing, this is what happens when we add an additional again for the face itself and it kind of looks almost like Caroline, who used to be my undergrad and did this work.
And now she's, she's a full grown PhD with you guys at MIT.
And, and you can see her dancing in all of these videos, like this one.
Okay, and we can also do the same thing we take a motion one person and we can apply it to many people. And there, when it's small of course the face matters less.
And we turn it into a controllable interactive application that got a lot of attention to the field of image and video synthesis. It appeared in popular press it exhibited and museums, it was incorporated into stage performances.
And now my co author, the same guy who doesn't know how to dance, even turn this into an app that you can download for free from the app store and you can dance and tick tock and whatever you want with it.
And this is interesting. But another direction that we kind of have been discussing a word to take this technology is to provide a platform for capturing performance as a form of intellectual property for for choreography.
Because it turns out that unlike musical score that can be copyrighted, there's no way to copyright dance.
In fact, there wasn't even a way to capture dance in the West until the 20th century. So most of the belays that we know of like Swan Lake and you know, all of these even the famous ones, haven't really survived in their original form.
Until this day there isn't an agreed upon notation of dance and this is just one example which is called the monetization, but all of the, all of the forms of notation.
They don't have a way to accurately capture all the small details. They're not parametric they're not scalable every time you come up with a different move you have to come up with a different notation.
And our, our idea is that capturing things the way that we do can can try and get to these to these issues and maybe offer a new solution.
So we talked a little bit about the fact that there is artifacts but
Actually, it turns out that that is the interesting part about this technology to a lot of artists. So here's, here's one example where the same team plays little company had a gig where they made this music video.
It's really celebrating the problems in this technology and making them into art.
And here's another example that I really love. This isn't actually using our work it's using that to bid but it's very, very similar so you can ignore the differences and just focus on what this person is doing with this.
Thank you.
Thank you.
Thank you.
Perfect. But Jake here really loves those and he's actually pushing it like to the extreme extreme so there's all this hair and the feathers which are really, really hard to capture for guns, but that he's just like let's throw it all in and just let it, you know, be artistic and and and this is kind of what he's been looking for in using these technologies.
Okay, but, but there's also a limitation on how you can use this kind of stuff for art, because essentially what we're doing when we generate images or we generate video or motion using again is that we're always using a training set which is real.
Okay, and we're trying to teach AI how to make us more of this real thing. Okay, so, so we want to, you know, maybe generalize out of the distribution of the training set but not by much we still want to keep things realistic that's our training signal.
And this is this becomes a limitation when you want to do something like this so so these are visuals from New York's tabula bassa music video.
And this actually goes with a with a large performance and show that she had a year ago.
And what they wanted here and they actually came to us to ask for help in order to do this and we couldn't help them what they wanted to do is they wanted to have this marriage between a human and an orchid.
So, so they want the motion and they use mocap for this but they want the motion to come from a human but the visuals and kind of the embellishment to come from a flower.
And there's nothing that we can do to help them, because we don't yet have this ability to do this compositionality between different realistic things to make something to make a new kind of life for him if you may.
We just can't do there's we don't know how to do this. So this is something that is that is a limitation of training to do realism.
And it's a very interesting future direction from work if you're looking for something really cool to do this this kind of idea can give you can give you a nice direction that we don't know how to solve yet.
So how did they do that. How did they did it by hand they did it by hand so.
So they, this is a really cool artist. He's also a professor I think in Hong Kong or one something like that. And they did motion capture on the people.
They have the motion signatures, but everything else is hand designed, you know, 3D models and add motion that is that is basically put together with with its dressed, you know, it's, it's dressed on top of the human motion.
What they wanted would have been similar to people who are here in the class. The other day there was some AI that could make pictures out of like the texture of the skin of an elephant or something.
Like the texture of noodles would have been similar to that.
Like you had a face and then you draw it with the texture of say noodles and then you change the noodles to make it look like it's talking.
So that's one. So you mean like the Geiger paper, right. The counterfactual paper. And there's also, I know you guys looked yesterday at the Lee. So there's different.
Yeah, that's one approach to do that. It's a little bit right now that technology is not yet.
But if you take those noodles and you make them in the shape of a dog or whatever, it's still noodles, you know, and you just cut them into dog. There's nothing that really takes the texture and like kind of applies it onto the 3D form.
So you see here if you look at the at the texture of the flower, it really changes with the articulation of the flower, right.
If you just take noodles and you, you know, you make the right mask for them, it's still it's not yet realistic, right. And this is again something we don't know how to do we don't know if you look at the cycle again results for example.
It doesn't conform to the 3D shape of the object. It doesn't actually respect that. So, yes, that's in the right direction, but it's not yet there. All of this is open problems.
Answer your question.
Yeah, yeah, just stuff to think about it's neat neat stuff.
This is really cool. I don't know how to solve this but it's this is a cool, this is a cool, cool idea, cool, cool direction. Okay, I literally have three minutes so I am going to have more stuff so
the rest of the stuff I wanted to talk about would have been kind of designing with with time using time was as an interesting medium.
And I guess what I can do is just just tell you two different highlights. Okay, so I'm not going to really walk you through all of the story here, but
let's see. If we think about if we think about time and we think about images not from video but like large collections of images.
There's something really interesting about them and the interesting things is that a lot of the time when you think about historical data you think about text, but but you don't really write everything down.
And, and those are things that are really captured in images, and we're lucky we're very, very lucky that we have now basically more than 100 years of historical visual record.
And the things that you get from that are things that are you know you can look at this image and kind of think to yourself you know if 100 years from now somebody wanted to explain in a history book, what are hipsters, it would be really
really hard, because, you know, here you can see the difference between nerds and hipsters and you know what is it that makes these guys look cool is it is it hats, is it the scarves that the Instagram filter that was slapped on the image like is you know what is.
It's really hard to say in words.
And in any case we don't really bother to talk about these things when we write stuff down.
And so historical images kind of captured this for us.
But of course then we kind of you know if you took a collection of historical images you kind of end up with you know a bunch of historical images that you need to sort through and you end up selling in the garage sale because it's too much work.
So if there is a way to automatically get, you know, how do things change over time from images that would be really cool. And that's stuff that we've done a couple of times here this is work with historical yearbooks high school
yearbooks, which is a really nice source of data because there's spaces and they always stay the same but what changes is fashions and social norms.
And what you can do with with a lot of data like this that has kind of a consistent subject matter but changes over time is something like Jason Sullivan here who is an artist has done with his graduating class versus his mother so he graduated
in 1988 from Fort Worth in Texas. And on the left you see an average image of all the people in his classroom the women and the men versus on the right the ones that came from his mother's class of 67 and you can already see that there's big differences
right people used to look different these to stress a little bit different. They used to treat the camera a little bit differently. And we did the same thing with our data, where we took, you know, 100 years of photographs and we looked at averages of men versus
women over time and you can notice that people, you know, look a little bit different that the hair is different they smile more than they used to, you can quantify this kind of thing.
You can look for other characteristic elements for different decades like different hairstyles that are very distinctive.
And this gives you tools for analysis of creativity and fashion, but we're not going to look at this but instead what we're going to say is, okay.
This is interesting you can do this with spaces everything is very online you can look at fashion but what happens if you want to think about time and in the real world in the outdoor world how can you use a time as a, as a, as a creative medium.
And so one thing we did afterwards is we went and looked at a lot of data coming from street view images, and we looked at whether we can say okay let's say I want to travel in time I'm stuck in a pandemic and I want to go visit New York but I want to make sure that I did it on a particular
Sunday afternoon in 2011 okay how can I do this is so maybe I can use flicker images if I want to go to Columbus Circle, but if I want to go to some random corner there's just not.
There's just not the people just don't take pictures there.
So what we did is we went and looked at the Google time machine, which is basically your normal street view interface but they actually keep historical images of the previous runs of the of the cars through the city, and that gives you a single location with
different riding conditions and different weather conditions. And this is really cool because you can collect this at a really large scale, like, you know, all of New York or basically the entire world.
And then for each location you have multiple snapshots of that place, only there's still very sparse, and to go to this place in a particular day and time you have to learn how to fill in the gaps.
Okay, because you know the buildings stay the same, but the weather conditions might change.
So the travel and time you want to take a particular image and you want to be able to change the lighting and the, and the weather.
So basically what you want to do is you need to disentangle or learn to disentangle the things that are varying temporally versus the things that are permanent.
And if you can do that, then you can use time basically to synthesize new things that you know you never really captured they might have existed but you don't know because you weren't there.
And so for a particular scene, you can do things like you can rotate the sun around and this is completely synthetic right this is a result of what we do.
Or you can copy and paste buildings, for example, so you can modify the permanent factor, and that would look something like this right here's here's an inserted building it looks perfect but but it's completely fake.
And I'm not going I'm going to, you know, just not going to go into the technical detail of how it's done but basically the thing that helps us is that we've seen the same place over and over again.
The one thing I do want to talk about is that the nugget.
The technical nugget that we use here is that we can use a decomposition of the scene into two things that graphics tells us that are, you know,
the correct, which is the difference between shading and reflectance where shading kind of captures the shadows and the effects of the illumination on the scene.
And the reflectance is the actual color so I'm wearing an actual blue sweater, that would be the reflectance of the sweater and then there's the effects of the light on it that puts in the shadow.
The thing about about these this this shadow representations this the shading representation is that we're we in our mind we think about the fact that maybe shadows are gray is like if you think about them as grayscale but they're actually not.
Okay, and this this is the interesting thing for sexually here.
And you can see this in this nice painting by Monet. Okay, so, so Monet is painting a grain stack that is sitting in a set of snow on the ground so the snow should be white the ground is white.
But there's actually two colors for the illumination.
There's blue from the sky.
And that is kind of an indirect diffuse light.
And there is a direct light that is yellow from the sun.
And if you look at the shadow that's being cast on the snow.
In the shadow, it's blue, because the direct light doesn't hit doesn't hit the snow and so you mostly get the indirect illumination from the sky.
So the blue then the surrounding light, which has the yellow mixed into it. And when I was doing a trick here where he's actually coloring with the yellow to complement the blue just in the border to make it even more clear to your, your, you know, your center
surround cells and your in your eyes that this is what's happening. Okay, so, so the trick here to get everything to look realistic was to say okay, people before I have kind of thought about shading as a grayscale thing.
And most of the color in their models have gone to the reflectance images, but we actually use a two toned shading where we take separately into account the blue and the yellow.
And we are trying to really capture a lot of the blue of the sky in the shading model and not in the reflectance model. And that kind of makes everything come more together and look more realistic.
And then we can say okay and you know we generalize we take an image from a completely unseen place like Paris we've never been to Paris we didn't train on Paris this is like a one image example and we can relate it and make it look like you've been there and whenever you want, basically.
Okay, so now I am very much over time. So I am going to stop here we've looked at audio motion we've looked at details we've looked at visual patterns over time.
There's there's some food for thought you can take out of here like, you know, for example, AI and perception we can use it to create tools for art design.
There's good and bad implications you have to think about what happens when you make synthetic or fake content.
And we've talked about modeling all kinds of complex things and multimodal stuff.
But there's a lot of stuff that's left to be done.
For example, the example of the person in the orchid and compositionality. And there's also the question of how do you not only provide tools that are creative, but also create creative machines and I think you're going to learn about that more later this week.
One final note is, if you want to learn more about perception and art. This is a great book by Margaret Livingston from Harvard.
And these are collaborators, thank you all collaborators and that's about it.
Thank you.
Thank you so much that was very, very interesting and intriguing and inspiring.
Oh, I'm sorry I went a little bit over time.
I was trying to like, lower the details.
I'm sure that there are many interesting things I personally learned and also students hopefully inspired their thoughts and the future work.
Is there any question from students.
I think that most of them are thanking you and I see that in the chat.
Oh chat. Okay.
Yeah, I think that.
Okay, cool.
Excellent.
Thank you again. It was a great talk.
Thank you.
I'm very excited to, you know, put this online so everyone can benefit from it.
Cool.
Thank you so much.
Bye.
Bye now.
