start	end	text
0	9520	All right. Hello, everyone. Welcome to your course, AI for Art, Aesthetics and Design
9520	21120	and Creativity. Today, we have a very special lecturer, AJ. He has been at MIT just like
21120	31200	you for his undergrad. I got to know him when he was here and he's very active. I've been running
31200	39360	at the ML groups and sometimes chatting with me about, you know, these topics of creativity and
40080	48240	AI and art. I think that this is very exciting. He's going to tell us about his journey and
49040	58880	and his new work. I will let him to, you know, start the discussion. AJ, one of the things that
58880	66480	I always ask is that if you could please introduce yourself and tell us a little more about what
66480	74000	inspires you to work in this area. Sounds good? Yeah. Yeah, absolutely. I'd be happy to.
74000	80400	And thanks so much for having me. So today, I'm going to be talking about some work I've done,
80400	84000	some works that's happening in the community around 3D content creation.
85360	92400	But first about my journey. Yeah, I was at MIT for my undergrad and I was part of what is now
92400	96080	called the AI Club. And then we called it the Machine Intelligence Community.
96880	104400	In my undergrad, I did research in a couple of areas, but mostly actually in compilers.
104400	109680	So a little distant from what I do now, more on the high performance computing side and
109680	114560	performance engineering that had experienced self-driving cars and generative models for
114560	119760	self-driving applications during undergrad and really fell in love with that topic. How do we
119760	124800	reason about uncertainty? How do we model complex data distributions and predict the future?
125760	130000	Like, for example, predicting the behavior of vehicles. And that led me down the path of
130000	136320	working on generative models in my PhD. And these generative models are, these days,
136320	140240	the state of the art generative models are parametrized by deep neural networks, which try to
140240	145040	fit large data sets, try to estimate correlations between different variables. And these could be
145040	149200	old types of different data modalities, like images, they could be trajectories or behaviors,
149280	156720	like I worked on, audio, video. And today, we're going to talk a little bit about 3D objects.
156720	160880	And so that's kind of what inspired me. At the time, I was interested in uncertainty estimation.
161440	165840	But these days, I just really like the tangible results you can get out of generative models,
166400	172000	novel samples, and novel designs. It's very fun. I got to look at pretty pictures all day.
174640	178640	To my research interests, like I mentioned, around generative models, we've done some work
178640	184400	in denoising auto encoders. How do you generate images with these denoising diffusion probabilistic
184400	188480	models? That's purely in the 2D setting, though it's been extended to other domains.
190800	195520	Over the past year and a half, I've also been doing a lot of work in 3D reconstruction and
195520	201280	inverse graphics. So how do we take images and try to infer a scene from them or generate in
201280	208720	the 3D space? Sorry, Ajay, interrupting you. It seems that some of the students want the
208720	213280	transcription to be on. Is that okay? That's fine. Okay, excellent. Thanks.
216320	221840	And building off of that performance engineering background I had from MIT, I also did a lot of
221840	226240	work in the intersection of machine learning and programming languages at the start of my
226240	231600	graduate school. And I would summarize kind of my research interest as making it easier to
231600	239040	create creative content, especially with AI tools. And to provide some background for today,
239040	244160	I'm going to discuss different types of scene representations. What I mean by this is how do
244160	251760	we encode the geometry and colour of a scene in some format that we can work with digitally.
252320	257520	And there's this very long history of this, particularly from the graphics literature.
258160	262240	On this slide shows some different representations of geometry that you'll be familiar with some
262240	269200	of them. 2.5D might include RGBD images, like a photo plus a depth scan. And they're point
269200	274320	clouds, meshes. Meshes are the most common representation used in graphics applications,
274320	277440	but they can actually be challenging to work with in a learning context.
277440	285760	So our focus in the learning context will be on the volumetric approaches. These can be very
285760	291520	easy to train. You can kind of think of at least a boss of greatest classifier, mapping each point
291520	298160	in space to whether it's part of the object or not, whether it's occupied or not. More recently,
298160	303040	there's been a significant amount of interest in neural scene representations, sometimes called
303040	308640	implicit neural representations that define the geometry of the object with a function.
309280	313520	That could be a distance function, so a network mapping from coordinates to the distance to the
313520	320080	nearest surface. And these can be a lot easier to optimise. These neural scene representations
320080	325360	can also compress the data significantly compared to explicitly representing the geometry of the
325360	332480	scene. So we'll be focusing on that direction. And in particular, we're going to be discussing
332560	337280	a model called neural radian fields I'll get to in a second. But they address this problem of
337280	345200	view synthesis. So how do we take some sparsely sampled input views of a scene and then construct
345200	350960	a representation of that scene's 3D geometry and colour in a way that allows us to render it from
350960	356800	new perspectives? These are some example works. You can represent the scene as a multi-plane image.
356880	363760	So instead of a flat grid of RGB values represented as multiple planes, and that allows very quick
364720	371120	rendering from new perspectives. Neural volumes is an approach from Facebook that has an encoder
371120	376560	decoder structure. Take some input images and encode them into a layman space, kind of a 3D
376560	383600	layman space and decode it out to images with volume rendering. Neural radian fields have
383600	389280	really been very popular over the last two years due to their simplicity and quality of the results
389280	396240	they can generate. So here's an example scene that's captured on the Berkeley campus. Some photos of
396240	400000	the scene are captured, for example, with an iPhone. I believe these are captured with an iPhone.
402080	408160	Then poses for each photo are estimated. And this neural scene representation called the neural
408160	413920	radian field is estimated off of those images. What's really nice is once we estimate the
413920	418480	representation of the scene, we can render it from novel viewpoints and kind of smoothly
418480	423840	interpolate these sparsely sampled views. The scene is only very sparsely observed from discrete
423840	427760	points. What if you as the user want to make a photo from a new perspective?
429440	435280	There's some interesting things to note about this rendering. Notice the specularities on the
435280	440880	surface. They're not just modeling the diffuse light. Also modeling how the light reflected
440880	446960	back at the user depends on the viewpoint of the camera. As you shift your head, the scene will
446960	454720	change. This is particularly visible on very shiny surfaces like the glass or metal of the car.
458000	464640	A neural radian field, yeah, it really is amazing and really captured the attention of a lot of
464640	471040	people. This neural radian field has grown very quickly and there's still a lot of problems to be
471040	476640	solved. One very interesting thing that these neural scene representations bring to mind is
476640	481680	that we're encoding a scene in the neural network's weights. Instead of explicitly encoding the
481680	488160	geometry of the scene via points or meshes, lists of triangles or voxel grids, it's encoded
488880	495440	into this small multi-layer perceptron. Maybe this is a half a million parameter network,
495440	501440	just some stacks of dense layers. It's representing a function mapping from 3D space coordinates,
502160	508480	XYZ. This is in the scene, XYZ coordinates, and a viewing direction. What is the angle of the camera
508480	514560	in order to model those view dependent effects? The neural network then predicts at this coordinate
514560	520000	what is the color of the scene and then its density, sigma. There's density as something like
520000	523600	how solid is the object and how much light will be absorbed.
528480	534320	Rendering is done by ray tracing. This is not exactly what would be done in most graphics
534320	541440	applications like real-time ray tracing because we've kind of encoded the light being reflected
541440	547200	at any given point back towards the viewer into this function. So we don't have to scatter
547200	552960	light through the scene. The viewer will cast a ray from their camera through the pixel. This is the
552960	559280	image plane into the scene and then query the neural network along the ray. These are different
559280	566080	3D coordinates in the scene. The color of the rendered pixel will then be some accumulation
566080	570640	of the colors along that ray. In order to determine the color and the density along the ray,
570640	574080	each of these coordinates is passed to the MLP as a very large batch.
575920	581600	We get a color and density for each coordinate and then can compose them with alpha compositing
581600	587760	into a color. There's some subtlety to this compositing. This is called the volume rendering
587760	596480	equation because this equation is pretty simple. This is the density predicted. This is the camera
596480	602960	origin and it's displaced some steps along the ray. The neural network will predict what is the
602960	607760	density of the scene at that point, but it will also predict what is color. Then we're integrating
607760	612160	this color along the ray weighted by its density, but we also have to weight it by
612160	619520	transmittance, which is roughly speaking how much light is transmitted from the viewer
619600	626560	to that point along the ray because once we've accumulated enough density, then objects
626560	633040	further back in the scene will not be visible to be included. This equation for color
634320	640160	conditioned on coordinates is differentiable with respect to the parameters of sigma and c.
640160	646720	So sigma and c will be this neural network. Because this is fully differentiable, it's
646720	651120	relatively easy to optimize. Instead of optimizing scene geometry, we'll optimize the
651120	658640	weights of this neural network in order to get some desired colors. This might be the sparsely
658640	664240	observed viewpoints, two viewpoints. Let's render the color according to the neural scene
664240	668160	representation and then try to optimize the network so that it matches the observed views,
668160	675360	pixel by pixel. It might take a minute to wrap your head around, but it's actually pretty simple.
675360	680880	We have this one MLP that encodes the scene, lets us render viewpoints differentially,
680880	686080	and we'll optimize the scene so that it matches the input views, and that's why it's inverse
686080	691920	graphics. We're going from the 2D space to optimize for the underlying 3D representation
691920	698320	that will reconstruct those views. Are there any questions about that?
699200	710480	Can you talk about how the points are used for the neural net? I can't see it directly.
712320	720080	I mean, I get the high-level idea, but could you talk about how those points are fed into the net?
721040	725360	Yeah, so you could consider constructing an MLP with five dimensions as input,
725920	731280	just five inputs, and then four outputs on the layers, and then intermediate features or whatever
731280	737680	you can imagine you want. So in nerf it's 256, that would be one approach, and it does work,
737680	743200	but then you get actually quite blurry reconstructions of the scene if you directly feed an input
743200	747600	coordinates as these are just floating point numbers, 3D coordinates, and going that direction.
747920	754640	But instead, what is used in this neural radiance field is assigned useoidal
754640	761520	positional encoding, so frequency-based encoding. If you're familiar with the transformer positional
761520	769360	encoding, this is a common approach where continuous values like coordinates or time
769360	777360	steps are encoded using a Fourier representation. So you take sine of various scaled values of the
777440	780400	input coordinates, and that lets the network model high-frequency detail.
781680	786720	So instead of kind of memorizing a function from each spatial coordinate, it can model
789920	794080	frequencies of the underlying signal if you use the sine useoidal embedding of the input.
794080	798480	So that kind of just projects this five-dimensional input into some higher-dimensional space
798480	804240	before feeding it to the MLP. I see. And there's no, let's say, I guess, filtering
804240	813440	done before, I mean, applying it to the net. So it's just transforming certain, I guess,
813440	818880	components, but not doing some, I guess, post-processing before putting it into,
818880	823520	or let's say compression or something like that. No, not really compression. There's
823600	828960	some, like, coordinate transform because you'll do this computation in a particular coordinate frame.
831360	837920	There is some subsequent work which we actually build upon that does a pre-filtering of the
837920	844800	input coordinates. So instead of encoding all the frequencies of the input coordinates,
844800	847920	they'll be blurred depending on how far away from the camera you're querying.
848880	853680	But that's sort of subsequent work to nerve. That reduces the aliasing.
855520	857680	Let's see. And the network is just fully connected.
858320	861520	Yeah, just a fully connected network. Super simple.
861520	862080	Yeah, thank you.
864080	871440	Yeah, one more thing that I wanted to mention here for a student is that there is a difference
871520	878480	between how you train this model versus the models that so far you have seen for,
878480	884720	for instance, classification. For instance, if you want to train a model for a truck,
886160	892560	what you do is you get a lot of images of different trucks in different lightings and different,
894080	899440	you know, models and things like that, and then fit it to your network. However,
900240	906800	in this case, you are taking lots of images of the single truck, single scene,
907360	914400	and you are trying to reconstruct that scene. So you said big difference between, you know,
915280	919840	what you are used to doing and what we see in nerve.
922400	928720	Yeah, absolutely. I kind of see it as instead of, the nerve is representing a single scene. So
928720	933440	instead of representing explicitly or representing it with a neural net with a function,
934720	938720	but it doesn't generalize. It interpolates these input views.
942640	947280	And, you know, there's a catch to that, which is that in order to fit into the neural radiance
947280	953520	field to a single scene, it generally needs a lot of data. So while these views are sampled
953600	957520	sparsely, discreetly, and there will be larger regions of space where we don't have
958480	965280	an image taken from that perspective, still to estimate a multi-view consistent radiance field,
966000	970720	experiments in the paper used a large number of images per scene. That's a little bit impractical.
970720	974320	So for these synthetic scenes, this is one synthetic scene that's rendered in blunders.
975360	979360	They were able to get out 100 images of each scene and fit the neural radiance field on it.
979360	983200	For those outdoor scenes, I showed earlier like that red Toyota car.
984480	989600	I think it's a fewer, maybe 20 images, but still that's a lot to capture with a handheld camera.
991600	996400	And in the first week of work, I'm going to talk about we improved the data efficiency of the
996400	1001760	neural radiance field training process. So instead of using, let's say 100 images on this Lego
1002880	1006720	scene, we used eight photos taken from randomly sampled viewpoints.
1009760	1015840	In the neural radiance field training process, we would take, we would know the pose of each image
1015840	1021360	that can be estimated with a system like call map. It's really common in the 3D graphics and 3D
1021360	1027840	computer vision community is given some images, estimate their camera poses with correspondence
1027840	1034160	finding, then the neural radiance field loss renders an image or renders some rays from the
1034160	1041200	same pose as the input, then it computes a mean squared error loss. So the pixel wise error.
1042880	1046880	The reason that this loss can be used is because we know the camera pose, we're able to render out
1046880	1054240	the scene from the exact same pose that the observer took the photo. If the camera poses shifted
1054240	1059680	in the rendering process, then the reconstructed image and the true image won't align pixel wise
1059680	1064960	and we'll learn from inconsistent geometry. And so this is done at all of the observed
1064960	1070000	camera poses. And this is why the neural radiance field needs so many photos. If there's no observed
1070000	1073440	photo, then it doesn't have the ability to compute a loss from a given perspective,
1074400	1080720	which means that it could overfit to the input use. This representation mapping from coordinates to
1080720	1085920	colors is very flexible. One possible degenerate solution would be to put a billboard in front
1085920	1091200	of each camera, just a poster board, you know, off of the highway, right in front of your camera
1091200	1097120	containing the image that's observed, rather than learning a consistency in geometry.
1098800	1105360	And there's other ways you can get artifacts. This is described as a shape radiance ambiguity
1105360	1110560	in the Nerf++ paper. Essentially, we could either reconstruct the shape correctly and then have
1111200	1116640	relatively constant radiance from different cameras, or we could encode each image into the
1116640	1120960	view dependent coordinate of the network. So because the network depends on the camera position,
1120960	1126160	it's able to memorize potentially the photo taken from each camera.
1128400	1132720	When the neural range field is trained with 100 views, it gets really crisp reconstructions. This
1132720	1138640	is a hot dog scene, synthetic scene, where we render out the views in Blender. Then the neural
1138640	1144880	radiance field, when it's trained with only eight views, only matches pictures close to the training
1144880	1150160	data. When you move the camera further away from the observed images to try to extrapolate,
1150160	1155600	then there'll be a lot of artifacts. If you regularize the neural radiance field a little bit
1155600	1160560	and simplify it, it can learn more consistent geometry, but there still are a bunch of artifacts
1160640	1169840	in the reconstruction. I'll skip over this. So in our work, we add an additional loss to the
1169840	1176080	neural radiance field training. We keep using the Nerf mean squared error loss. It's called
1176080	1183120	photometric loss on the observed views that are sparse. But then our work diet nerf adds an
1183120	1188240	additional loss at unobserved positions. So because we have this neural radiance field
1189040	1195920	at any iteration during training, we're able to render out novel views even before the scene has
1195920	1202960	converged. It's a little silly that in Nerf, we're not able to constrain these input views,
1203760	1209280	because as a person looking at, okay, let's say that our estimate of the scene's geometry
1209280	1214880	gives us these renderings. This is the observed rendering. We as people can still look at these
1214880	1221280	photos and derive some loss signal. Okay, the input view is a lot sharper than my current
1221280	1227840	estimate of the scene. There's a little red light at the top of the truck, but there's no light on
1227840	1238960	top of these reconstructions. Based on this principle that you can compare views at different camera
1238960	1245680	positions as a person by comparing their semantics, like, you know, it's a bulldozer, a bulldozer is
1245680	1251600	a bulldozer from any perspective. We propose to add a loss in feature space. Using some visual
1251600	1259520	encoder, each of the input views is represented in a feature space. And then instead of computing
1259520	1267360	the loss in pixel space, diner will compute a loss in feature space. And that allows us to regularize
1267360	1276640	the scene from any perspective during training. We call this a semantic consistency loss,
1276640	1281440	since we're making sure that these semantic features, things like object identity, object color,
1282880	1289680	are consistent across views. And over the course of training, this improves the results.
1290400	1296000	So the loss that Nerf used was this mean squared error loss, and then we're adding this semantic
1296000	1303840	consistency loss where some encoder thigh, some neural network encodes rendered images, and then we
1303840	1313040	compare them in a feature space. We do have to sample camera poses in order to render this, so
1314000	1325600	there's just some prior distribution over camera poses. The choice of that feature thigh is really,
1325600	1332400	really important for the results, because we want it to be consistent across viewpoints. So it should
1332400	1337520	really encode the object's identity and properties about the object rather than low-level details,
1337760	1348480	like the exact pixel colors. And motivated by that, we use a network called Clip. This is from
1349440	1356320	last year. It's a representation of images and text, so a representation of an images learn,
1357760	1362400	such that it has an aligned representation with an associated caption. The data that Clip is
1362400	1368080	trained on is a very large data set of 400 million images that have associated captions
1368080	1374800	crawled from the web. And the Clip model has really led to an explosion of work in the AI art
1374800	1379680	community. It's really powerful. It's trained on such a large amount of data that we're able to
1381840	1386160	prompt it with topics that you wouldn't find in a narrow data set.
1386480	1391760	It also, by learning to match images to this text, we'd hope to learn some very useful features
1391760	1397280	about an image. For example, in captions, you can encode classes of objects, just like image net
1397280	1404080	labels. You can also encode a lot of other details, like the scene rather than just the foreground
1404080	1410160	object. You can encode things about pose of the underlying object, like a sitting person,
1410160	1417280	a standing person. And that should be encoded in the representation learned by the network,
1417280	1421600	if it's going to be able to match images against their associated caption. So the
1421600	1426080	training objective is encode a bunch of images, encode their captions, and then try to match
1426960	1433680	images with their true caption. Clip was originally used for discriminative tasks,
1433760	1442960	object classification in a prompting fashion. So if you want to classify photos of food,
1444160	1449200	the authors of clip constructed a bunch of captions, templatized with the desired object
1449200	1455200	category, a photo of guacamole, a photo of ceviche. And then the class label is given by the
1456080	1459200	caption with the best match with a given image.
1459600	1463680	The property we're particularly interested in in this 3D reconstruction context is whether the
1463680	1468480	representations of the images are consistent across views. That's what we call semantic
1468480	1475040	consistency in the work. What this plot is showing is that the cosine similarity of embeddings
1475040	1480400	from the image encoder of clip within a particular scene from different camera poses is highly
1480400	1486800	similar. So very high similarity in feature space within a scene across different perspectives,
1487040	1499280	low similarity across scenes at different perspectives. So because images are very
1499280	1504080	similar in clip's feature space, very different in pixel space, we're able to maximize feature
1504080	1510880	space similarity of clip and get some useful loss. Now what you've been waiting for are the results.
1511600	1515680	This is nerf trained on eight views when it's simplified. And then here is it trained with
1515680	1520880	our additional semantic consistency loss. A bunch of near field artifacts in nerf,
1520880	1524640	but when we add in this feature space loss, it removes a lot of those artifacts.
1530960	1536240	Because those artifacts aren't plausible, they reduce the semantic consistency.
1536240	1546960	Cool. I'm going to go on to the next work. Before I do, anyone have questions?
1546960	1556720	I have one question, which is with regards to using clip. Are you able to access the text
1556720	1562400	as well that clip generates, or are you able to decode it in some way and actually access
1562400	1569120	how the clip looks at the inputs? Just in terms of explainability, I thought it could be,
1569120	1573440	yeah, sounds really interesting. Yeah, that's a very good question. So in this work,
1573440	1579040	we're not actually using the text encoder. We'll see that in the next work. The text encoder is
1579040	1584640	just used for pre-training clip in dye and nerf. So we're only using this image encoder.
1584640	1589040	Because then the motivation for that is that the neural radian students are motivated by the
1589120	1593200	view synthesis problem. So there's no text caption associated with the data.
1593200	1596480	They just have a couple of pictures. So we only need to use the image encoder.
1598800	1604080	That said, some artists have tried to take clip and use it to create a captioning model.
1606480	1609840	If you have a model that can match images against captions, can you actually synthesize
1610560	1616240	captions that best match a particular image? It's a challenging discrete optimization problem
1616240	1620560	because you're searching for a textual caption that will maximize some neural network's output
1620560	1626160	score. And that is basically a black box optimization problem. My impression is that
1626160	1630480	automatic captioning with clip doesn't work too well. It's really good at selecting an associated
1630480	1636160	caption out of a list of candidates. And that's how we're able to do object classification with clip.
1638880	1643200	So I think you'd be better served by learning a specific captioning model that will generate
1643200	1647200	a caption condition on image rather than trying to extract captions out of clip
1648480	1654480	just due to the difficulty of the optimization or the search. Thank you.
1660800	1667040	So like I said, we weren't using the text encoder in diet ner. In the next work, we try to
1667680	1676080	move in an even more extreme direction of generating objects without any image data.
1676080	1680320	So what if we only have a caption and want to synthesize the 3D object from it?
1682000	1687360	Is that possible? Can we remove this mean squared error loss entirely and only use
1687360	1692880	feature space losses? And these are some examples of the results we're able to get
1692880	1697360	with different captions, like a render of a Jenga tower produces this object.
1698960	1706960	You can also engineer prompts, use hashtags because clip is trained on web data.
1710000	1712560	Our goal is to synthesize 3D objects from just the caption.
1714800	1721200	And to kind of refresh our memories, the neural radiance field is an inverse graphics approach
1721200	1725680	where we have densely sampled images, optimize the shared scene representation,
1726480	1732560	and then are able to render out new views. In the dream fields work, the second work in this line,
1734160	1739680	we do not have any observed images, only a caption written, for example, by a human artist.
1741840	1747200	We optimize something that will look fairly similar to diet ner with additional regularizers,
1748160	1752160	and then are able to render out new views. And any perspective is actually a new view
1752160	1757040	because we haven't observed this scene. This is an associated scene for the caption,
1757040	1759440	an epic, wondrous, fantasy painting of an ocean.
1764080	1770560	So the neural radius would use this mean squared error loss, and then diet ner used feature space
1770560	1780080	loss where the rendered image of the scene and an observed image of the scene are encoded into
1780800	1791120	feature space that is optimized. Oops. Sorry. Okay. Now in dream fields, we use the text
1791120	1795200	encoder of clip. That wasn't being used before. We were just throwing it away after trading.
1796160	1802880	So instead of optimizing for the feature similarity in image feature space,
1802880	1808320	we now maximize similarity of image and text features. The reason we can swap between
1809040	1812880	the text encoder and the image encoder is because clip has learned to align representation.
1812880	1817760	It has tried to maximize the similarity of representations of images and their associated
1817760	1823760	captions so those representation spaces overlap. And you can in some sense swap the encoders
1824480	1828800	from text encoder to an image encoder and hopefully still have that aligned representation.
1830000	1832800	But overall, the pipeline looks fairly similar. So it's randomly sample
1833360	1841040	poses in the scene, render an image, and then try to make sure that its semantic features
1841040	1850080	match our features of the caption. But if you apply that approach naively without any regularizer,
1850160	1859920	then there are a bunch of artifacts. These are some example generations for different captions.
1859920	1866000	I believe this one had something to do with liquid in a blender. This one might have been
1866000	1873920	a colorful bus with graffiti on it. So without regularization, we are getting to generate scenes.
1873920	1878640	And it's not surprising because there's really no data involved in this process.
1878720	1882800	In Dietner, the scene was regularized by having some input views.
1884800	1886960	Here, the canvas is open, wide open.
1891520	1898880	So in our work, we added some regularization. The scenes are composited with randomly sample
1898880	1906320	backgrounds. And we regularize the scene to be highly transparent. So this transmittance loss
1907040	1912080	encourages varsity in the underlying scene. So instead of getting lots of low density
1912080	1918160	wispy content, like you saw in the previous slide, with a transmittance loss and this
1918160	1922080	associated background, our motivation in Dreamfields is to create more of a consistent
1922080	1931520	foreground object, a single foreground object. And these are the renderings for the
1931520	1941600	associated caption, washing blueberries. There's definitely a lot of room for improvement
1941600	1947120	because each of these blueberries is kind of mashed together with the others. The general
1947120	1952880	caption has been encoded into this scene. And there's a consistent foreground object.
1953680	1964480	This is the visualization of the process of optimization. In response to the question,
1965200	1969440	Leandra asked, so it's creating this from one image, there's actually no images observed.
1969440	1975040	There's only a caption fed to the system. So any images that I'm showing are rendered
1976000	1979040	using our neural radiance field. They're completely fictional.
1985280	1991360	I mean, some intuitive explanation for this is how can we learn a scene representation such
1991360	1995760	that it could be captioned with a given caption from any perspective.
1998800	2003120	Maybe that's how a human sculptor went approach the problem. So given a caption, like give me,
2004000	2013680	you know, a clay sculpture of a tower. Well, let's say, you know, a monocular sculptor.
2014720	2020400	Good. Optimize for a clay sculpture that is a tower of many perspective.
2023840	2029520	Sorry, what happens? Sorry, what happens if the caption is something vague, like just a dog?
2029520	2034880	How would your optimizer know that, like, it should have the same dog even from different
2035440	2043760	poses or camera poses? Yeah, excellent question. The constraint that views should
2043760	2047360	represent the same object from different perspectives just comes from the shared
2047360	2053840	three presentation. We're optimizing the same MLP from any perspective.
2053840	2062160	Okay, thanks. We had to simplify. Well, we didn't have to. You're able to keep view dependence in
2062160	2066640	the neural radiance field. So this regularizer ends up being kind of important. Like I discussed
2066640	2069840	with Dietner, if you're able to learn a lot of these near field artifacts.
2073440	2077120	Sharing the scene representation is important, but some of the other techniques on our paper,
2077120	2081040	like the regularizer are also important for making sure you get a clean result.
2084800	2092960	In this example, we experiment with different caption templates to measure the
2092960	2098160	compositional generalization of the model. So the base caption template here is a teapot
2098160	2105520	in the shape of a blank, a teapot imitating a blank. And then in the video, the caption beneath
2105520	2112560	each object is the word that's filled into the template caption. So a teapot in the shape of
2112560	2118160	an avocado produces this object. Whereas the caption of teapot in the shape of a glacier
2118160	2126320	produces something more ice styled. And I'm sorry about these animations.
2128400	2134560	If you switch the caption from an armchair to a teapot, you'll also notice some changes in the
2134560	2139280	shape. So there's legs on this avocado chair, but when it becomes teapot, the legs are removed.
2139760	2146720	There's a follow-up question about whether the Clip Library is 2D. Yes, Clip is trained only on
2146720	2154480	2D images. So just on 2D views. The motivation for using Clip is that we can very scaleably
2154480	2159760	acquire caption images from the internet. If you, for example, look at Wikipedia and just look at
2160640	2165920	the upper right image associated with each article, it has a caption beneath it. And there's a data
2166000	2169760	set out there called WikiText, which has about 11 million captioned images.
2170960	2176080	The authors of Clip were able to collect even larger data set by scraping websites other than
2176080	2184080	Wikipedia. But if you look at data sets with 3D objects in them, they're very small. The largest
2184080	2189440	might be ShapeNet, which is entirely synthetic objects. And there's usually no caption associated.
2189440	2195520	So we'd have to have a human annotate. This is a general trend in the 3D reconstruction literature
2195520	2204000	that the availability of 3D data is quite limited. And so in dream fields, we're able to exploit this
2205840	2211760	pre-trained 2D image encoder and text encoder, and then kind of lift it up into 3D by using
2211760	2221360	a shared representation of the geometry. There's a bunch of different techniques that we use to
2221360	2226800	improve the quality of the results. I won't get too much into this, but the metric is a little
2226800	2233280	tricky to define because there's no reference object for each caption. We only have a data
2233280	2240320	set of captions provided to us by the user, and we're one of measure how well our generations are
2240320	2248880	performing. In order to do that, we use a neural metric based off of matching generated 3D objects
2248880	2255120	against the input captions. This is something like precision of retrieving the correct caption,
2255120	2259520	given the generator objects. Some of the most important techniques that help us here are
2261520	2267600	regularizer for transmittance and data augmentations, those architecture we use for the MLP,
2270720	2275280	and then later on, what model we use for clip.
2278880	2281920	This is an example of the process of optimization from different iterations,
2281920	2286000	so it actually can converge quite quickly, but additional detail might be added over the course
2286000	2292880	of training. In order to run 20,000 iterations of optimization, it's an expensive process
2292880	2298720	because we need to render out these images during training, but back of the envelope calculation
2298720	2306320	is about three to four dollars to generate each model on TPU in Google Cloud at an hour.
2306480	2309440	It's in the realm where an artist could afford to do this.
2310720	2315520	We're working on some follow-up work, which will speed up this process and make it even less expensive.
2321600	2327440	That's all I've got on these works. The broad goal here is to make content creation
2328240	2335520	easier and generate assets that are useful. This 3D assets I see is particularly useful
2335520	2340400	for downstream applications because they could be plugged into a game or plugged into some other
2340400	2347200	system. We have code out for both of these projects. If you want to try out the text
2347200	2351760	to 3D generation in your browser, you can use a Colab notebook that I put together.
2352400	2357200	I've tested it on the Pro version of Colab, which has higher memory GPUs,
2357200	2358880	so you might need to play with some of the parameters.
2358880	2368880	Thank you so much, Eje. This is really fascinating. I have a few questions,
2370240	2377600	and then before letting everyone else ask questions, the first question is that
2379680	2388560	are you able to walk us through some of the Colab code today or should we do it on our time?
2389840	2396560	Let me see if I have it up. Also, before going to changing your screen,
2396560	2405280	can you please go back to the animations? Sorry, I have so many questions because this
2405280	2415680	is really cool. Or maybe the one that is armchair. Yeah, give me one sec. Thank you so much.
2419360	2429360	I think these are really cool. I think that for the students and I, this kind of inspires us to
2429360	2440640	think maybe one cool thing to do is that we can generate these things and use them in some avatar
2440640	2447520	or game or something, and this will be really cool. This is something for students to think about
2448480	2457920	for their future projects because the goal of this course is to inspire us to think about
2457920	2464400	what are the creative ways that we can use AI. This is really cool. One question that I have
2464400	2474160	is that, can you share some intuition of, for instance, let's say the rubric. It looks like
2474240	2483040	a rubric and it looks like a chair, but then we see that there is some, we wish there was more of the
2484080	2493760	structure and it might be because clip is the objective and or assessor and thinking that,
2493760	2502800	okay, as long as I have a patch of red and yellow and things like that that are appearing on rubric,
2502880	2508800	I'm happy, the rest, I don't care much, or is there any better explanation of what's happening?
2510240	2518800	Yeah, so the 3D structure only emerges because of the shared representation and the easiest way to
2520480	2525440	satisfy clip from any perspective, having this Rubik's Cube chair from any perspective,
2526560	2531440	might actually be to learn some consistent geometry. That said, there's no prior
2532400	2537200	other than sparsity and some implicit regularization just in the structure of the MLP,
2537840	2544560	so there's no prior on the 3D structure learned from data. That's something that I think is missing
2544560	2550800	and definitely opportunity for future work is how do we learn some priors on 3D data and integrate
2550800	2558240	them into the system to try to improve the plausibility of the structure. One example where
2558240	2565760	this issue arises is that sometimes you'll get repeated structures on the objects,
2566720	2572720	like if you optimize for a dog, maybe it will have eyes on multiple sides of its face because
2573680	2580080	they're not visible. So you only see two sets of eyes from any particular viewpoint,
2580080	2585920	that is all the discriminator clip ever sees are those two eyes, but the underlying geometry,
2585920	2589280	there's no constraint that the dog should only have to rise.
2591360	2596320	Okay, excellent. Thank you so much. Are there questions before we go to the collab?
2601680	2609360	The outputs, are they like .fbx files or do they still need to be, let's say, a little bit
2609360	2615520	prepared in rendering software before they can be actually readily used in the game engine?
2615920	2622480	Like Unity or Unreal? They do need to be post-processed. So what you get out is a train neural net,
2622480	2627200	so it's function mapping from coordinates. We don't use the v direction in these results,
2627200	2633840	just XYZ coordinates mapped to color and density. So there are a bunch of ways that you could convert
2633840	2639200	that. I don't know of off the shelf software that will be able to do that conversion for you,
2639200	2645680	it'd have to be coded up, but you can sample the scene on some grid, for example, and then you'll
2645680	2650560	get out color and RGB. You could convert that to a local voxel representation. If you want to get
2650560	2658240	a mesh, there's an algorithm called marching cubes that is able to find a mesh in the scene,
2658240	2661920	and there's implementations on GitHub of marching cube for neural radiance fields
2661920	2668960	that we haven't integrated into our particular library. So you take a little bit of glue to
2668960	2674960	grab marching cubes and then plug it in. So what do you all use to turn the neural net into these
2674960	2681520	graphics? Sorry, could you repeat the question? What do you use to turn the neural net into the
2681520	2685840	graphics that we see here? Oh yeah, so that's done by rendering. So you can render the neural
2685840	2690880	radiance field from any perspective in the code, but that just renders out a 2D image. It doesn't
2690880	2696000	give you, you know, like a mesh, versus the game engine will have its own rendering algorithm based
2696000	2702560	on rasterization or ray tracing, given the underlying geometry and texture map, which might
2702560	2707200	be real time. So the rendering here is not real time. You have to go evaluate the neural network
2707200	2711200	at a bunch of different coordinates and accumulate its outputs into an image.
2713200	2719360	So that's implemented. If you want videos, we can do that, but you'll have to DIY the conversion.
2724720	2727840	Ellie had a question on strategies to reduce rendering costs.
2728480	2735280	So you can render images at low resolution. And in the Colab notebook, the rendering is done at
2735280	2740000	very low resolution. So experiments, you render out 168 by 168 images or higher.
2741520	2746720	But Colab only gives you a single low memory GPU. So we render out 88 by 88 images.
2747680	2753440	And that really significantly speeds up the process. So rendering takes maybe 300 milliseconds.
2754400	2757200	So you have to do about three iterations per second.
2762080	2775040	If you're using alpha transparent, okay. So Ben is asking, how do we handle with transparent objects?
2775760	2783680	So the neural radiance field, the volumetric representation is really amenable to transparent
2783680	2791360	objects because the density is this continuous value and we can observe objects. So accumulate
2791360	2796880	color from objects behind transparent objects. In optimization, you might decrease the
2796880	2801280	density of some object that should be transparent, like stay in glass windows.
2802240	2808640	And the background is composited at the end. So any ray, if there is some accumulated,
2809440	2815840	if the transmittance is not zero along the ray accumulated throughout the scene,
2816400	2818720	then there'll be some contribution from the background image.
2820480	2825040	So the reason that we've kind of encouraged coherent objects is that if the object is not
2825040	2829520	coherent, then the background will leak through the translucent objects. Oh, I see what you're
2829520	2834560	saying. Yeah, if you want stay in glass windows. So I mean, you would have to, the scene would
2834560	2838960	probably optimize so that the transparent object is blocked from behind by something.
2848240	2854160	Yeah, the next steps, I think they're exciting lots of next steps, because this is an initial work
2854160	2859360	and there's things like speeding up the optimization. It's been a lot of recent work and
2859360	2862800	speeding up neural radius field training for images. And I think a lot of that can be plugged in.
2864560	2869200	And how do you synthesize the formable objects? How do you bring a human in the loop so they
2869200	2876160	can provide feedback partway through training? All kinds of stuff to tackle in making this more
2876160	2887280	of a practical system for 3D artists. And would you like me to share the collab? I guess we're at
2887280	2891680	time. Yeah, please go ahead. That would be great. Thank you so much.
2899280	2903040	So this is the collab notebook. You can find it from the project website.
2905040	2908240	It is a compact implementation.
2908400	2919680	The system will run faster on GPU than on TPU in the collab notebook. But for all of our experiments,
2919680	2926320	we use TPU. Some helpers are imported from our library. So if you want to hack on some
2926320	2931840	of the low level primitives, you can fork our library or kind of copy those helpers into the
2931840	2936640	notebook. But the main way you'll interface with this collab notebook is by adjusting the quality
2936640	2946720	settings here. So in particular, edit the query. Here I've filled in a high quality 3D
2946720	2953920	render of Jenga Tower. And you can select the clip checkpoint you want to use. Clip bit B16
2953920	2958800	is used in most of our experiments. There's also an internal Google model that's not available here.
2959920	2964880	But you can scale down if you're running out of memory to either the B32 or ResNet 50.
2966880	2971680	Choose the number optimization iterations. I think at least 1000 is necessary.
2973520	2977600	But more will add more detail. Consider the rendering width and then this is the number
2977600	2987200	of data augmentation. And then run training. So here's an example of the training run I've already
2987200	2992480	run in the notebook for that prompt, a high quality 3D render of Jenga Tower. It won't exactly
2992480	2995920	match the result that was shown in the slides because the version of the collab notebook could
2996000	3002640	scale down. But over the course of 2000 iterations of optimization, you can see the different
3002640	3008240	learning curves. This is the total loss that's being optimized. Clip's cosine similarity,
3008240	3012720	negative cosine similarity is improving. So this means that the renderings of the object
3012720	3016320	are becoming more and more consistent with the given caption over time.
3019040	3023280	And the transmission is regularization here. This is showing what is the average transparency
3023360	3033920	of pixels in the scene. And in this plot at the bottom, the collab notebook will ride out
3033920	3044720	renderings periodically every, I believe, 100 iterations. So at the beginning, the scene is
3044720	3050720	low density, essentially empty. And then over time, some content will emerge from the optimization.
3051520	3057120	And then that's refined and sharpened over time. The camera's moving around. So the camera's being
3057120	3063680	randomly sampled around the object. And that's why the scene is rendered from different perspectives.
3064800	3070640	And then finally, the collab notebook renders out a video, 48 frames. And this is the result.
3072400	3078560	On the GPU that collab gave me here a P100, the optimization I think took about six, seven minutes.
3081680	3085360	So hopefully you can get some cycles in.
3088800	3094160	In the run training section, it says if you run out of memory, tweak the configuration options
3094160	3101040	above. What do you recommend changing? Yeah, that's a good question. So I think
3101040	3108880	you can change this clip at B16. I would try to clip B32. There's also on the first import
3108880	3112400	in NVIDIA SMI printout. And so you can look at how much memory is available.
3113040	3121200	Sometimes it's worth retrying multiple times to get a larger GPU. This P160
3121200	3124960	gigabyte I think you won't get without collab premium, which is about $10 a month.
3126560	3133120	But you think you can get 15 gigabyte T4 GPUs for free. Sometimes the collab will give you an 11
3133120	3138560	gigabyte GPU that might not be enough. If you can tweak the configuration parameters,
3138560	3144080	I would try reducing this number of samples. So this is the number of points along each array
3144080	3152320	that is sampled. And that affects the batch size. So the render width, the batch size scales
3152320	3156720	quadratically with the render width because we're rendering got square images. And then the num
3156720	3162720	samples the batch size to the MLP scales linearly. So you could reduce this down to 32 even at the
3162720	3169600	lowest. B32 will use less memory than B16. So this relates to the patch size and the vision
3169600	3177200	transformer clip encoding. And then if you want to scale down even more, you can change the number
3177200	3189680	of data augmentations per iteration, maybe down to two.
3198000	3200720	Oh, Ben says that you can't retry for a better GPU.
3201600	3209120	That's unfortunate. I mean, I don't know whether MIT has like a shared GPU cloud, but you can
3209120	3215440	also just download this IPIND and run it on your like Jupyter notebooks, post it on some MIT compute.
3217040	3223200	And it will paralyze across multiple GPUs.
3230880	3236640	Cool. And have you taken any more questions that you have?
3236640	3243840	Excellent. Thank you so much. Maybe at this point I'll stop recording and if students have more
3243840	3245200	questions, we can...
