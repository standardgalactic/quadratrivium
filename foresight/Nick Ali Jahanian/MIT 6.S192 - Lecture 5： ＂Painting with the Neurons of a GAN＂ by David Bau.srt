1
00:00:00,000 --> 00:00:04,880
Hello, everyone, welcome back to our course, a deep learning

2
00:00:04,880 --> 00:00:12,840
for art, acetic and creativity. Today, it is our pleasure to

3
00:00:12,840 --> 00:00:18,280
have very a specialist speaker, David Bao, and I just let him

4
00:00:18,280 --> 00:00:22,640
to introduce him a little more, because I think it's very

5
00:00:22,640 --> 00:00:29,400
inspiring for many students, the path that he has come to

6
00:00:29,400 --> 00:00:35,040
this point and for future. Please go ahead. So I was, I

7
00:00:35,040 --> 00:00:43,920
want to give a little background since I am a post

8
00:00:44,120 --> 00:00:46,760
industry academic, I spent a bunch of years as a software

9
00:00:46,760 --> 00:00:50,760
engineer at Google before coming back to MIT. And I want to

10
00:00:51,000 --> 00:00:56,720
give a little bit of insight in my thinking there. So, you know,

11
00:00:56,720 --> 00:00:58,320
the reason it's really interesting to be in computer

12
00:00:58,320 --> 00:01:01,600
science right now is because the field is changing. The dream

13
00:01:01,640 --> 00:01:06,000
of having self programmed computers is one of the oldest

14
00:01:06,000 --> 00:01:12,200
dreams in computer science, but it's never been a reality. Even

15
00:01:12,200 --> 00:01:15,640
though we've studied machine learning for a long time, I think

16
00:01:15,640 --> 00:01:19,120
that until just a few years ago, machine learning was really

17
00:01:19,120 --> 00:01:22,920
more accurately called, it would have been more accurately

18
00:01:22,920 --> 00:01:27,520
called the art of accurate counting. You know, statistics,

19
00:01:29,160 --> 00:01:33,160
you know, understanding the statistics of, you know, how

20
00:01:33,160 --> 00:01:37,080
frequent words are and by grams or, you know, certain image

21
00:01:37,080 --> 00:01:41,760
statistics or something like that. And, and, and if you if you

22
00:01:41,760 --> 00:01:45,600
understand statistics well, then, then, then, then, you know,

23
00:01:45,600 --> 00:01:50,920
you could do some nice tricks. But I think that until recently

24
00:01:50,960 --> 00:01:55,760
really calling these things sort of self programmed systems

25
00:01:56,400 --> 00:01:58,680
would have been an overstatement. But I don't think it's

26
00:01:58,680 --> 00:02:01,960
really an overstatement anymore. I think that these machine

27
00:02:01,960 --> 00:02:07,640
learning models are really learning non trivial things. And

28
00:02:07,680 --> 00:02:10,600
it leads to all sorts of questions about, you know,

29
00:02:10,680 --> 00:02:14,840
what should we be doing as programmers? What does it mean

30
00:02:14,840 --> 00:02:17,280
to do software engineering? And so I thought it was very

31
00:02:17,280 --> 00:02:20,840
interesting time to come back to academia. That's, that's why

32
00:02:20,880 --> 00:02:24,920
I'm here. And I actually think that that's one of the choices

33
00:02:24,960 --> 00:02:28,200
you face when you're trying to decide between industry and

34
00:02:28,200 --> 00:02:33,400
academia. And I think in industry, you will have lots of

35
00:02:33,400 --> 00:02:37,920
resources to make things work to make the next widget or the

36
00:02:37,920 --> 00:02:42,160
application. And, you know, there are great places, Google is a

37
00:02:42,160 --> 00:02:44,840
great place, we can really push state of the art in that and

38
00:02:44,840 --> 00:02:49,480
do really neat stuff. I think that there's less of a push in

39
00:02:49,480 --> 00:02:53,800
industry to ask the question, Why? You know, why do things

40
00:02:53,800 --> 00:02:57,440
work? Why are we doing what we're doing? Where is it going to

41
00:02:57,440 --> 00:03:00,040
lead in either unintended consequences and things like

42
00:03:00,040 --> 00:03:02,800
that? You know, we, we tend not to ask those questions too much

43
00:03:02,800 --> 00:03:06,160
industry, because there's so much to emphasize on, you know, the

44
00:03:06,160 --> 00:03:12,640
how of how to how to get it to work. And so, and so, so I

45
00:03:12,640 --> 00:03:17,020
thought it was a time to, to switch tracks and start asking

46
00:03:17,020 --> 00:03:19,200
why because the field is changing so dramatically. And I

47
00:03:19,200 --> 00:03:23,280
think that, you know, I'd encourage people who have an

48
00:03:23,280 --> 00:03:28,840
interest in these type of questions to, to realize you

49
00:03:28,840 --> 00:03:31,480
can really make a real contribution by taking the

50
00:03:31,480 --> 00:03:36,480
academic track as well. So okay, so let me introduce my talk.

51
00:03:36,480 --> 00:03:39,760
So it's about painting with neurons of general adversarial

52
00:03:39,760 --> 00:03:45,720
networks. It comes out of work from asking why, you know, why

53
00:03:45,760 --> 00:03:53,600
do these networks do what they do? And so, so let me, let me

54
00:03:53,600 --> 00:03:58,320
advance here. Am I in full screen? So do you see the, do you

55
00:03:58,320 --> 00:04:01,440
see the like the full screen slideshow I can't see what I'm

56
00:04:01,440 --> 00:04:04,080
projecting? Or do you see like all my notes and all that stuff?

57
00:04:04,200 --> 00:04:08,120
Yeah, I can see it. But also maybe a student can tell us.

58
00:04:09,400 --> 00:04:10,000
Yeah, okay.

59
00:04:10,360 --> 00:04:11,000
That's fine.

60
00:04:11,400 --> 00:04:16,480
Is everything okay? Yeah, it's a full screen slide. Hopefully,

61
00:04:16,520 --> 00:04:21,680
it's okay. So, so, okay. So the main problem that we're looking

62
00:04:21,680 --> 00:04:25,520
at here, and I'm not sure why the, the images are overlapped in

63
00:04:25,520 --> 00:04:29,400
the right way. Hopefully, the layout will get fixed. So we're

64
00:04:29,400 --> 00:04:33,920
going to next slides. But the, the, the, the, the main problem

65
00:04:34,160 --> 00:04:39,160
surrounding my talk is image generation. And so, for the last

66
00:04:39,200 --> 00:04:41,560
few years, there's been this question, how do you make a

67
00:04:41,560 --> 00:04:46,960
state of the art program to generate realistic images? And,

68
00:04:47,320 --> 00:04:49,560
you know, the general process is first you want to collect a

69
00:04:49,560 --> 00:04:52,640
data set of real images, like these pictures of buildings on

70
00:04:52,640 --> 00:04:59,160
the right. And, and then you want to, you know, train some sort

71
00:04:59,160 --> 00:05:02,200
of program, some sort of generator network to generate

72
00:05:02,760 --> 00:05:06,040
those programs. And so, so, you know, it's been a puzzle. There's

73
00:05:06,040 --> 00:05:08,920
a lot of different ways you could imagine doing this. And so

74
00:05:09,760 --> 00:05:12,680
people have been puzzling, how do you train such a thing? How do

75
00:05:12,680 --> 00:05:15,240
you even supervise it? You know, what should the, what should

76
00:05:15,240 --> 00:05:19,560
the inputs and the outputs of the network be? And, and, and the

77
00:05:19,560 --> 00:05:22,200
thing that has really been working the best in recent

78
00:05:22,200 --> 00:05:24,640
years is, you know, in architecture, you guys have all

79
00:05:24,640 --> 00:05:28,760
heard of called GANs, generative adversarial networks. And the

80
00:05:28,760 --> 00:05:32,400
trick for GANs is to reduce it down to a simpler problem that

81
00:05:32,400 --> 00:05:36,240
we know what we're doing. And so the simpler problem that

82
00:05:36,240 --> 00:05:40,160
they're recognized when designing GANs was that

83
00:05:40,440 --> 00:05:42,880
generating images, we don't really know how to do, but

84
00:05:42,880 --> 00:05:46,320
classifying images, gosh, that is an easy problem. We can

85
00:05:46,320 --> 00:05:51,240
classify images. And so, so what we could do is we could train a

86
00:05:51,240 --> 00:05:55,960
classifier on this really easy task, which is given two sets

87
00:05:56,000 --> 00:06:01,040
of pixels, which image is real, and which image is not a real

88
00:06:01,040 --> 00:06:05,480
photograph. And it turns out that for most arrangements of

89
00:06:05,480 --> 00:06:08,560
pixels, this is a very easy task to train a discriminator on

90
00:06:08,560 --> 00:06:11,560
it gets very good, you know, very quickly, we'll start getting

91
00:06:11,560 --> 00:06:16,520
100% accurately on that. And so, so but the neat thing is that

92
00:06:16,520 --> 00:06:18,320
once we have a discriminator that can tell the difference

93
00:06:18,320 --> 00:06:22,360
between a fake image and a real image, then we can hook it up

94
00:06:22,360 --> 00:06:25,600
to our generator, and we can say, All right, we didn't know how

95
00:06:25,600 --> 00:06:29,560
to tell you, generator, how to make a real image. But you know

96
00:06:29,560 --> 00:06:32,480
what this discriminator can tell you, because all you have to do

97
00:06:32,480 --> 00:06:36,520
is generate patterns of pixels that fool the discriminator, if

98
00:06:36,520 --> 00:06:40,520
you can make the discriminator think it's real, then it must be

99
00:06:40,800 --> 00:06:46,960
better than random. Now, the problem is that, even though the

100
00:06:46,960 --> 00:06:49,920
discriminator can get very accurate at telling what's real,

101
00:06:51,200 --> 00:06:54,720
they, the generator will also be very good at learning how to

102
00:06:54,720 --> 00:06:57,800
fool the discriminator without working very hard, it'll realize

103
00:06:58,000 --> 00:07:00,960
that aha, the only thing I need to do to make the discriminator

104
00:07:00,960 --> 00:07:04,360
think is real is put some blue sky in there and put some texture

105
00:07:04,360 --> 00:07:07,440
that kind of looks like, you know, building texture. And, and

106
00:07:07,440 --> 00:07:09,640
the discriminator will say, Well, that totally looks real,

107
00:07:09,640 --> 00:07:13,120
there's a sky, you know, there's, there's the right, the right

108
00:07:13,120 --> 00:07:16,120
colors for buildings and some vertical lines and things. Ah,

109
00:07:16,120 --> 00:07:18,760
that's totally real. But as a human, we look at that, we think,

110
00:07:18,760 --> 00:07:21,880
Oh, that's not a very realistic image at all. So the trick is to

111
00:07:21,880 --> 00:07:24,560
iterate this process to go back and forth after the generator

112
00:07:24,560 --> 00:07:29,000
can generate sort of halfway looking real images, then have

113
00:07:29,000 --> 00:07:32,680
the discriminator say, Ah, well, that's actually fake. And

114
00:07:32,680 --> 00:07:35,120
we're going to tell the difference between those new fakes,

115
00:07:35,120 --> 00:07:38,200
those better fakes, and actual real photographs, and the

116
00:07:38,200 --> 00:07:41,120
discriminator has to now work harder at getting better. And so

117
00:07:41,120 --> 00:07:45,000
if you, if you alternate these processes, then, then you end up

118
00:07:45,000 --> 00:07:48,280
very conversion to very, very good generators that can generate

119
00:07:48,280 --> 00:07:55,600
very realistic images. And they, you know, the typical learning

120
00:07:55,600 --> 00:07:59,440
process is actually just to do only one step of iteration

121
00:08:00,040 --> 00:08:03,240
between the discriminator and generator and just alternate that.

122
00:08:03,240 --> 00:08:06,360
So by the time you're done, you've played this game, you

123
00:08:06,360 --> 00:08:08,520
know, millions and millions of times back and forth between

124
00:08:08,520 --> 00:08:11,200
the generator and the discriminator. But the new thing

125
00:08:11,240 --> 00:08:14,360
that's happening here is that it can generate these images that

126
00:08:14,360 --> 00:08:20,800
look very realistic in the end. But let's see. So Oh, here's

127
00:08:20,800 --> 00:08:25,000
another picture. So we'll get this images out that look very

128
00:08:25,000 --> 00:08:28,400
realistic in the end. And we'll get this generator, which is

129
00:08:28,400 --> 00:08:32,440
just a deterministic function that takes actually the input of

130
00:08:32,440 --> 00:08:36,480
the generator is actually just a random vector. So we'll take

131
00:08:36,480 --> 00:08:39,680
these relatively small random vectors like 512 dimensional

132
00:08:39,680 --> 00:08:42,480
random vector, and we'll put it into this thing. And it's been

133
00:08:42,480 --> 00:08:45,760
trained so that no matter what it outputs, it will look very

134
00:08:45,760 --> 00:08:48,720
realistic, like this example image here. Or if I change a

135
00:08:48,720 --> 00:08:51,680
vector, I'll get a different image out and it will again look

136
00:08:51,680 --> 00:08:54,440
very realistic, even if it looks completely different. And so

137
00:08:54,480 --> 00:08:57,080
it's just a deterministic function that really wants to

138
00:08:57,080 --> 00:09:01,920
make realistic images. And, and so here's like a sample of like

139
00:09:02,320 --> 00:09:05,640
output from a generator. And you can see that after millions of

140
00:09:05,640 --> 00:09:09,920
these sort of generative training steps, where it's

141
00:09:09,920 --> 00:09:12,800
pitted against a discriminator, it actually gets to be pretty

142
00:09:12,800 --> 00:09:16,840
good. And so this is a style game v2. It's a model that was

143
00:09:16,840 --> 00:09:22,040
published last year. And, and it's, you know, currently the

144
00:09:22,040 --> 00:09:26,240
state of the art in generating realistic images of certain

145
00:09:26,800 --> 00:09:31,000
certain types of image distributions. And, and so when

146
00:09:31,040 --> 00:09:33,840
when you look at a collection of images like this, you might

147
00:09:33,840 --> 00:09:36,880
think, actually, the first time I looked at the output of some of

148
00:09:36,880 --> 00:09:42,840
these state of the organs, I was confused between the training

149
00:09:42,840 --> 00:09:46,360
set, and the generated output, this is not the training set,

150
00:09:46,360 --> 00:09:51,280
this is actually what the generator is producing. And so, so

151
00:09:51,280 --> 00:09:55,600
you see all sorts of interesting effects here. And so the one of

152
00:09:55,600 --> 00:10:00,200
the questions to ask is, what the heck is the model doing

153
00:10:00,200 --> 00:10:03,280
inside? Can we understand the underlying algorithm? And what

154
00:10:03,280 --> 00:10:06,760
the characteristics of that algorithm is like, why does this

155
00:10:06,760 --> 00:10:13,040
work? And so one of the funny things that you'll notice is

156
00:10:13,480 --> 00:10:16,680
that some of the images have these strange artifacts, like

157
00:10:16,680 --> 00:10:22,440
take a look at this one here. So this, this scan is pretty good.

158
00:10:22,560 --> 00:10:28,360
It's this generator is so good that it actually has noticed that

159
00:10:28,360 --> 00:10:33,720
the training distribution that is imitating has some percentage

160
00:10:33,720 --> 00:10:39,040
of images that were stolen off of shutter stock. And they still

161
00:10:39,040 --> 00:10:43,600
have the watermark on them. And, and, and, and the generator

162
00:10:43,600 --> 00:10:46,660
says, well, if I want to make things look realistic, I better

163
00:10:47,140 --> 00:10:51,580
put watermarks on some percentage of my images too. It

164
00:10:51,580 --> 00:10:55,740
learns it's got to protect its own copyright. So, so it, it

165
00:10:55,740 --> 00:11:01,900
does that. And so something like 6% of the output images from

166
00:11:02,300 --> 00:11:05,140
state of the art style, again, will have these kind of

167
00:11:05,140 --> 00:11:09,620
artifacts that show the same type of watermarks that were on

168
00:11:09,620 --> 00:11:14,180
the training set. This is the Elson Church training set. And

169
00:11:14,220 --> 00:11:17,780
so, so yeah, this kind of watermarks like this. But the

170
00:11:17,780 --> 00:11:22,260
reason I thought this was cool was that it, it's this very

171
00:11:22,260 --> 00:11:25,740
clear thing that the image generator does, but it doesn't

172
00:11:25,740 --> 00:11:29,580
always do it. Like most of the time when it generates images,

173
00:11:29,860 --> 00:11:33,260
it generates images without a watermark, but sometimes you get

174
00:11:33,260 --> 00:11:37,900
these watermarks. And so, and so it's, it's almost like this

175
00:11:37,900 --> 00:11:44,020
binary decision. It's like, there must be a switch that the

176
00:11:44,020 --> 00:11:47,940
network has at some point where it decides whether it's going to

177
00:11:47,940 --> 00:11:51,300
put a watermark on an image or not. And so we can kind of ask

178
00:11:51,300 --> 00:11:53,900
the question, where's that switch? Is there a neuron

179
00:11:53,900 --> 00:11:58,420
somewhere in this network, which is, which is controlling the

180
00:11:58,420 --> 00:12:02,700
watermarkness. And so, so I went on a hunt for this, this,

181
00:12:02,700 --> 00:12:05,140
this particular network has about 30 million parameters,

182
00:12:05,140 --> 00:12:09,260
which sounds like a lot, but it's just a deterministic computer

183
00:12:09,260 --> 00:12:14,300
program in the end. And, and it's not that hard to go hunting

184
00:12:14,340 --> 00:12:17,780
for things like this, you just, you can make an algorithm that

185
00:12:18,380 --> 00:12:21,020
has a heuristic that determines whether it's a watermark or not

186
00:12:21,020 --> 00:12:25,340
and just go hunting for, for things that correlate with that.

187
00:12:25,900 --> 00:12:32,060
And so I'll show you what I found. So at layer five, I found

188
00:12:32,060 --> 00:12:37,140
this very interesting neuron that did correlate with watermarks

189
00:12:37,140 --> 00:12:42,180
a lot. It was activating whenever images look like this in

190
00:12:42,180 --> 00:12:46,340
the end. And, and not only that, but because it's at layer five,

191
00:12:46,980 --> 00:12:51,340
it has a has a location for where the image activates. And I'll

192
00:12:51,340 --> 00:12:55,340
show you where, where it's activating. So, so this neuron

193
00:12:55,340 --> 00:12:58,220
is activating, you know, at these middle parts of images,

194
00:12:58,220 --> 00:13:00,820
whenever the image is showing a watermark. And there are other

195
00:13:00,820 --> 00:13:03,540
neurons that have similar behavior, like so for example,

196
00:13:03,540 --> 00:13:08,900
there's this neuron 234 at the same layer. And it activates in

197
00:13:08,900 --> 00:13:11,580
regions like this, both in the middle watermark and the bottom

198
00:13:12,020 --> 00:13:16,820
bar that shows up. And there's about, if you hunt through the

199
00:13:16,820 --> 00:13:21,620
neural network, you find about 30 neurons that are similar and

200
00:13:21,660 --> 00:13:28,220
behave like this. And so that's, that's pretty cool. So then the

201
00:13:28,220 --> 00:13:31,780
question is, well, do these things really act like a switch?

202
00:13:31,820 --> 00:13:35,260
What if we've removed these neurons from the network? What if

203
00:13:35,260 --> 00:13:39,060
we force them all off? What if we turn, what if we force these

204
00:13:39,060 --> 00:13:43,220
neurons to be off all the time? That will happen. So normally,

205
00:13:43,220 --> 00:13:47,420
we think of these neural networks as completely opaque

206
00:13:47,420 --> 00:13:51,980
systems. We train them end to end, they're just, you know,

207
00:13:52,060 --> 00:13:55,420
these big black box functions. And we normally think of the

208
00:13:55,420 --> 00:13:58,780
functions as computing things where everything depends on

209
00:13:58,780 --> 00:14:00,820
everything. And so if you randomly rip through the

210
00:14:00,820 --> 00:14:05,180
function and remove some of its operations, then maybe you

211
00:14:05,180 --> 00:14:08,980
expect to get total nonsense out just garbage or noise. But we

212
00:14:08,980 --> 00:14:11,580
found these particular neurons that really correlate to this

213
00:14:11,580 --> 00:14:14,100
thing. So let's see what happens when we turn them off. Do we

214
00:14:14,100 --> 00:14:18,180
get anything intelligible at all? So this is what the network

215
00:14:18,180 --> 00:14:21,780
generated before these are the watermark images I showed you

216
00:14:21,780 --> 00:14:25,140
before. And I'll show you what happens if I turn off these 30

217
00:14:25,700 --> 00:14:31,060
watermark neurons. So I'm going to give the network the same

218
00:14:31,060 --> 00:14:35,020
input. But turn off these neurons during its computation,

219
00:14:35,020 --> 00:14:37,060
and you can see what the output looks like. So you can see

220
00:14:37,500 --> 00:14:41,020
before chain, you know, forcing these neurons off and after

221
00:14:41,020 --> 00:14:45,660
forcing those neurons off. The images are still very

222
00:14:45,780 --> 00:14:50,340
intelligible, they look realistic still. But now the

223
00:14:50,380 --> 00:14:54,620
watermarks are gone. So I thought I was when I when I saw

224
00:14:54,620 --> 00:14:58,100
this, I was pretty excited, because it's like, Oh, there are

225
00:14:58,100 --> 00:15:00,980
switches inside the networks. And these networks are doing all

226
00:15:00,980 --> 00:15:03,940
sorts of amazing things, not just like showing watermarks. You

227
00:15:03,940 --> 00:15:06,380
know, so when I first found this, it was on Progressive GAN,

228
00:15:06,380 --> 00:15:09,500
which is a year earlier than a couple years earlier, the images

229
00:15:09,500 --> 00:15:12,980
didn't look quite as good. But but still in Progressive GAN,

230
00:15:12,980 --> 00:15:17,460
they do all sorts of amazing things, like they will arrange a

231
00:15:17,460 --> 00:15:21,700
scene with a river and trees and grass and, you know, building

232
00:15:21,700 --> 00:15:25,140
architectures with all sorts of different features. And you can

233
00:15:25,140 --> 00:15:28,460
ask, you know, is there a switch to turn on and off clouds in

234
00:15:28,460 --> 00:15:32,540
the skies or switch to turn on and off trees or windows and

235
00:15:32,540 --> 00:15:40,500
buildings? And, and so I went hunting for that. And, and, and

236
00:15:40,500 --> 00:15:44,220
the way I went hunting is I tested every neuron one at a time

237
00:15:44,220 --> 00:15:46,780
I inverted the test. So basically, I look at each neuron,

238
00:15:47,140 --> 00:15:51,020
and I say, Where is it activating? And, and I asked a

239
00:15:51,020 --> 00:15:55,420
question, is it activating an interesting part of different

240
00:15:55,420 --> 00:15:59,020
images? So for example, if I took this one neuron here, and I

241
00:15:59,020 --> 00:16:02,100
see where it's activating when it's generating this image, you

242
00:16:02,100 --> 00:16:06,340
can see it's very hot on the right and on the left, but not

243
00:16:06,340 --> 00:16:11,580
much up in the sky. And on this very same neuron, when we

244
00:16:11,580 --> 00:16:16,060
generate a different image with a different input, this very same

245
00:16:16,060 --> 00:16:19,140
neuron is not activating very much anywhere in this this image.

246
00:16:19,980 --> 00:16:23,460
But if we generate another image, then it will activate in a

247
00:16:23,460 --> 00:16:28,140
specific area here, mostly on the lower left part of this image.

248
00:16:28,140 --> 00:16:30,500
And you can see what's on the lower left. There's a, there's a

249
00:16:30,500 --> 00:16:34,340
tree there. And so it kind of gives you the hypothesis that

250
00:16:34,340 --> 00:16:37,700
maybe this neuron is correlated with trees somehow. So

251
00:16:37,700 --> 00:16:40,980
obviously, we can, we can do this, we can collect this

252
00:16:41,220 --> 00:16:45,860
information over thousands of examples of generated images by

253
00:16:45,860 --> 00:16:50,660
looking at where the neuron is activating, we can ask what what

254
00:16:50,660 --> 00:16:54,300
kind of thing is in the image, what kind of objects, what are

255
00:16:54,300 --> 00:16:58,180
the semantics of the image in the location that the the neurons

256
00:16:58,180 --> 00:17:03,740
are in. And we can just repeat that test process, you know,

257
00:17:03,780 --> 00:17:08,340
thousands of times to see if the neuron is agreeing with any

258
00:17:08,340 --> 00:17:12,700
particular kind of semantics that are in the images. So if, if

259
00:17:12,700 --> 00:17:15,740
the, if the neurons are showing up where the trees are all the

260
00:17:15,740 --> 00:17:19,780
time, we can just count and see if if that's if that's true in

261
00:17:19,780 --> 00:17:23,820
general. And we can also look for correlations with other

262
00:17:23,820 --> 00:17:27,580
things. So what I did is I, I searched for correlations with

263
00:17:27,580 --> 00:17:30,340
thousands of different, you know, hundreds of different,

264
00:17:31,140 --> 00:17:34,340
different types of semantics and object classes, different parts

265
00:17:34,340 --> 00:17:37,660
of buildings or, or objects or other things that can show up

266
00:17:37,660 --> 00:17:41,100
in a scene. And so what do we find? Well, we do find, you

267
00:17:41,100 --> 00:17:43,260
know, there's a neuron that correlates with trees, just like

268
00:17:43,260 --> 00:17:46,300
the one I was showing you. There's actually a few that are

269
00:17:46,300 --> 00:17:49,380
like that. And there's also neurons that correlate with

270
00:17:49,380 --> 00:17:53,540
other things like domes, or, or other building parts like

271
00:17:53,540 --> 00:17:57,980
windows and doors. And, and if you change the model to look at

272
00:17:57,980 --> 00:18:01,220
other things, then you can find neurons that correlate with

273
00:18:01,220 --> 00:18:06,780
things like windows, or chairs, or other things that they show

274
00:18:06,780 --> 00:18:10,420
up in, in the scene. And so this is actually pretty neat,

275
00:18:10,420 --> 00:18:17,260
because this model was trained unsupervised by any labels. All

276
00:18:17,260 --> 00:18:22,300
we did is we told it, generate realistic looking scenes,

277
00:18:22,660 --> 00:18:28,700
realistic looking photos. And, and, and we did not train it

278
00:18:28,700 --> 00:18:33,460
with any labels, we didn't tell it that these are photos of

279
00:18:35,380 --> 00:18:39,100
scenes that have big windows, and these are photos of scenes

280
00:18:39,100 --> 00:18:41,980
that have little windows or anything like that. Or, or, or

281
00:18:41,980 --> 00:18:45,300
here's where the windows are. But what happened was, the

282
00:18:45,300 --> 00:18:49,220
network discovered that it had to, you know, learn a

283
00:18:49,220 --> 00:18:55,900
representation, where windows are represented differently from

284
00:18:55,900 --> 00:19:02,260
the way chairs are represented. But somehow, even though, you

285
00:19:02,260 --> 00:19:04,500
know, windows can look very different from each other and

286
00:19:04,500 --> 00:19:06,860
chairs can look very different from each other, that the

287
00:19:06,860 --> 00:19:09,380
network has this represent this this component of this

288
00:19:09,380 --> 00:19:16,580
representation, this neuron that activates on all these chairs,

289
00:19:16,820 --> 00:19:19,660
despite the amazing amount of diversity that it shows, like

290
00:19:19,700 --> 00:19:23,700
none of these chairs really look similar to each other, they

291
00:19:23,700 --> 00:19:26,060
have different colors and different textures, and they're

292
00:19:26,060 --> 00:19:30,140
oriented in different ways. And yet, the same neuron is

293
00:19:30,140 --> 00:19:33,260
activating on all of them, the same thing goes for other

294
00:19:33,260 --> 00:19:37,100
things that show up in these images. So does anybody have

295
00:19:37,100 --> 00:19:40,060
any questions about, about, about this? I'd love this to be a

296
00:19:40,060 --> 00:19:41,860
little bit more interactive than the way I'm doing the talk.

297
00:19:41,860 --> 00:19:44,860
So let me open the floor for a question for a minute.

298
00:19:47,420 --> 00:19:52,140
Has anybody tried playing with the internals of GANs yet? I'd

299
00:19:52,140 --> 00:19:58,820
love to see if has anybody like generated images using a GAN

300
00:19:58,820 --> 00:19:59,340
before?

301
00:20:01,900 --> 00:20:08,700
No, but I do have a question. Yes. So what was like the end goal

302
00:20:08,700 --> 00:20:13,460
or the larger reason behind finding all of these neurons

303
00:20:13,900 --> 00:20:18,820
that correspond to different objects or features?

304
00:20:19,540 --> 00:20:24,300
Well, when I was originally looking at it, my original goal

305
00:20:24,300 --> 00:20:30,340
was just to understand how these models did their computation.

306
00:20:30,540 --> 00:20:36,180
So asking the question why. But the neat thing is that after I

307
00:20:36,180 --> 00:20:39,740
found this structure, then it became clear that there are

308
00:20:39,740 --> 00:20:44,780
new applications that you can build on top of it. And I think

309
00:20:44,780 --> 00:20:46,780
that's one of the cool things that comes out of this sort of

310
00:20:46,780 --> 00:20:50,660
academic style inquiry is, you know, originally, I was just

311
00:20:50,660 --> 00:20:52,980
looking to make catalogs like this. This is a catalog of all

312
00:20:52,980 --> 00:20:55,540
the different types of correlations that I found with

313
00:20:55,540 --> 00:20:58,220
neurons inside a model for generating kitchens, and the

314
00:20:58,220 --> 00:21:01,860
kinds of, you know, the patterns you see. And, and, you know,

315
00:21:01,860 --> 00:21:04,500
I've done this before for classifiers. And, you know, when

316
00:21:04,500 --> 00:21:06,700
you do for generators, you get different patterns. And so I was

317
00:21:06,740 --> 00:21:09,980
just really interested in making these maps of seeing what is

318
00:21:09,980 --> 00:21:14,460
computed at what layer, you know, where and how accurately. So

319
00:21:14,460 --> 00:21:19,020
this is, you know, the progressive gain has, depending

320
00:21:19,020 --> 00:21:22,500
on the resolution has about 15 layers. And if you sort of

321
00:21:22,500 --> 00:21:25,740
chart what you see in different layers, you can see this this

322
00:21:25,740 --> 00:21:28,460
really interesting thing phenomenon where it's in the

323
00:21:28,460 --> 00:21:32,940
middle layers that you get these highly semantic correlated

324
00:21:32,940 --> 00:21:36,300
neurons. But then as you get to the later layers, then they

325
00:21:36,340 --> 00:21:39,260
tend to be more physical. And there's not as many semantic

326
00:21:39,260 --> 00:21:42,380
objects. So it's like in layer five, we have things that really

327
00:21:42,380 --> 00:21:45,820
correlate with ovens and chairs and windows and doors, even

328
00:21:45,820 --> 00:21:49,860
though like a window kind of looks like an oven. The model

329
00:21:50,220 --> 00:21:53,700
clearly has different neurons that correlate with windows from

330
00:21:53,700 --> 00:21:58,700
ones that look like ovens. And so so that so that's that's that

331
00:21:58,700 --> 00:22:02,500
so I was originally interested in just mapping things out. But

332
00:22:03,460 --> 00:22:07,420
the correlations were so striking that it leads to these

333
00:22:07,420 --> 00:22:10,580
interesting applications that you can build. And I can I'll show

334
00:22:10,580 --> 00:22:16,220
you some in the next step. Let me before I do that, let me see

335
00:22:16,220 --> 00:22:18,220
if anybody else has a question as well.

336
00:22:21,340 --> 00:22:25,420
Yeah, David, I was hoping that you could also show us the

337
00:22:25,420 --> 00:22:29,740
application at some point, which I think these are very good to

338
00:22:29,780 --> 00:22:34,980
see why you asked this question. I mean, yes, it's more

339
00:22:35,780 --> 00:22:38,620
That's great. Let me let me zoom out to the application. So

340
00:22:40,060 --> 00:22:43,220
so the the neat thing is that just like we could turn off

341
00:22:43,220 --> 00:22:47,860
watermarks, we can turn on and off things in image generation.

342
00:22:47,860 --> 00:22:50,060
So for example, if I find all the neurons that correlate with

343
00:22:50,060 --> 00:22:53,740
trees, and I turn them off, you can see what happens. I'm going

344
00:22:53,740 --> 00:22:57,900
to turn them off sort of one at a time here. And so originally,

345
00:22:58,340 --> 00:23:01,460
the image will just be generated this. But if I turn off some

346
00:23:01,460 --> 00:23:05,980
tree neurons, you can see that we can actually remove the trees

347
00:23:05,980 --> 00:23:09,460
from the scene. And the cool thing is that this is different

348
00:23:09,460 --> 00:23:13,420
from Photoshop. If you went and you tried to erase trees from an

349
00:23:13,420 --> 00:23:17,180
image, then you'd have this puzzle of what would happen

350
00:23:17,220 --> 00:23:20,300
about stuff that was occluded by the trees like what's going on

351
00:23:20,300 --> 00:23:24,660
behind there. And so this image generator is actually it's got

352
00:23:24,700 --> 00:23:27,940
this latent model that has an understanding of what the scene

353
00:23:27,940 --> 00:23:31,340
is. And so, and even has an understanding of things that is

354
00:23:31,340 --> 00:23:35,100
not explicitly drawing. So if you remove the trees from the scene,

355
00:23:35,380 --> 00:23:39,220
then it'll come up with a reasonable looking, you know,

356
00:23:39,460 --> 00:23:44,340
image to draw what was behind the trees, or you can do the

357
00:23:44,340 --> 00:23:49,340
opposite. Which is you can take neurons that were not originally

358
00:23:49,340 --> 00:23:53,780
on in a generated scene and turn them on. So if I take a set of

359
00:23:53,780 --> 00:23:56,660
neurons that correlate with doors, and I turn them on in a

360
00:23:56,660 --> 00:23:59,540
certain location, and you can see what happens in the generated

361
00:23:59,540 --> 00:24:02,380
image, you know, I'll get this door in the scene, not only

362
00:24:02,380 --> 00:24:05,900
will it just be a door, but it'll be it'll have like an

363
00:24:05,900 --> 00:24:11,220
appropriate size and your orientation and style for for

364
00:24:11,220 --> 00:24:13,940
the building that it's in. So if I take exactly the same neurons,

365
00:24:14,460 --> 00:24:17,820
and I activate them in a different location like here in

366
00:24:17,820 --> 00:24:21,140
this building, then even though it's exactly the same neurons

367
00:24:21,140 --> 00:24:24,740
exactly the same activation that I've done, I get a different

368
00:24:24,740 --> 00:24:28,100
door that is like a much smaller has a different style and so on.

369
00:24:28,100 --> 00:24:31,660
It's appropriate to the building that it's in. If I if I try to

370
00:24:31,660 --> 00:24:35,140
put a door in a place that would make sense, like by turning on

371
00:24:35,140 --> 00:24:39,820
neurons up in the sky, then it like will like not do anything.

372
00:24:40,860 --> 00:24:44,300
This is this is the actual output of what happens if I turn on

373
00:24:44,300 --> 00:24:48,180
the exact same neurons up in this location. So there's a lot of

374
00:24:48,220 --> 00:24:52,340
interesting context sensitivity that you can measure. But one of

375
00:24:52,340 --> 00:24:54,380
the cool things that you can do is you can actually hook this up

376
00:24:54,380 --> 00:24:59,900
to a paintbrush user interface, like I can find neurons that

377
00:24:59,900 --> 00:25:03,700
correlate with domes or doors or things. And if I want to add

378
00:25:03,700 --> 00:25:06,300
doors to a building, I can just sort of paint them on. And the

379
00:25:06,300 --> 00:25:09,380
doors will show up and you can see the orientation of the doors

380
00:25:09,380 --> 00:25:12,980
is appropriate to the wall that you put them in. If I just say I

381
00:25:12,980 --> 00:25:16,780
want trees, it'll put trunks and leaves, you know, in the right

382
00:25:16,780 --> 00:25:19,100
place in the trees or plants with a plant, plant them on the

383
00:25:19,100 --> 00:25:22,700
ground. If I take grass and I can turn the grass neurons off and

384
00:25:22,700 --> 00:25:26,180
remove grass from the scene and it'll come up with what the

385
00:25:26,180 --> 00:25:29,260
scene should look like instead. And so I can kind of do these

386
00:25:30,220 --> 00:25:33,900
semantic manipulations directly. Oh, here we're turning on domes

387
00:25:33,900 --> 00:25:40,220
and you can see it will turn the top of the the church from a

388
00:25:40,220 --> 00:25:44,140
spiral to a dome, but it also sort of stitched the dome into

389
00:25:44,140 --> 00:25:47,740
place to make it look good here. I'm removing grass again. We

390
00:25:47,740 --> 00:25:53,020
can like put a door in the scene. And if I if I put you know,

391
00:25:53,020 --> 00:25:56,540
sort of put a door in the wall, then it'll it'll come up with

392
00:25:56,540 --> 00:25:59,900
like the appropriate location and style orientation for the door

393
00:25:59,900 --> 00:26:04,940
even if I draw very roughly. So when I'm drawing, every time I

394
00:26:04,940 --> 00:26:07,900
touch the surface here, what I'm really doing is I'm just turning

395
00:26:07,900 --> 00:26:13,140
on a few neurons. And and I'm letting the the math of the

396
00:26:13,140 --> 00:26:19,780
GAN generator deal with all of the the details of how to arrange

397
00:26:19,780 --> 00:26:22,180
the actual pixel. So does that does that sort of give you a

398
00:26:22,180 --> 00:26:25,260
sense? Does that answer your question for like, you know,

399
00:26:25,260 --> 00:26:28,340
what kinds of things you can do with this by understanding what's

400
00:26:28,340 --> 00:26:31,300
going on in the interior of the model? Maybe now I should stop

401
00:26:31,300 --> 00:26:36,060
it. Oh, yes, go ahead. Are these different neurons for like

402
00:26:36,060 --> 00:26:41,060
doors in different areas? No, no. So when I when you click on the

403
00:26:41,060 --> 00:26:47,780
door button on the left, I am picking 20 neurons that are

404
00:26:47,780 --> 00:26:51,540
the door neurons. So by doing the statistical analysis ahead of

405
00:26:51,540 --> 00:26:55,340
time that I showed you earlier, I've identified 20 neurons that

406
00:26:55,340 --> 00:26:59,860
correlate very strongly with the presence of doors. And when you

407
00:27:00,020 --> 00:27:04,020
click on the button on the left, I am picking those neurons. Now,

408
00:27:04,220 --> 00:27:06,540
it's a convolutional network. So there's this translation

409
00:27:06,540 --> 00:27:11,460
and depends those neurons appear at every pixel. And so what you

410
00:27:11,460 --> 00:27:14,660
can do is you can just turn on those neurons in random pixels

411
00:27:14,660 --> 00:27:15,460
that you touch.

412
00:27:16,980 --> 00:27:19,180
Changing where the neurons are, that was what I didn't

413
00:27:19,180 --> 00:27:21,860
understand. Does that make sense? So, so because it's a

414
00:27:21,860 --> 00:27:24,900
convolutional network, so it's actually it's it's like the neural

415
00:27:24,900 --> 00:27:29,020
network is cloned at every location. It's the same neural

416
00:27:29,020 --> 00:27:32,820
network that's being used to process every, you know, patch

417
00:27:32,820 --> 00:27:37,700
or patch of pixels in the image. And, and so if I asked for a

418
00:27:37,700 --> 00:27:40,300
door in a place that wouldn't really make sense, then it

419
00:27:40,300 --> 00:27:43,180
won't put a door there. If I asked for a door in a place that

420
00:27:43,180 --> 00:27:45,420
makes sense, it'll make a big intervention, it'll stick a big

421
00:27:45,420 --> 00:27:48,500
door there, which you can see. So I could be very rough about

422
00:27:48,500 --> 00:27:53,660
where I put a door and it'll like put it in the right place. So

423
00:27:53,660 --> 00:27:59,100
that's that's the idea. So let me let me zoom around here, I'll

424
00:27:59,100 --> 00:28:04,220
show you a couple other things that you can do. So now there's

425
00:28:04,220 --> 00:28:07,340
some limitations to this. And I'll just show you some of the

426
00:28:07,340 --> 00:28:09,300
techniques that you can use to get around the limitation. So one

427
00:28:09,300 --> 00:28:12,100
of the problems is that, you know, we can do all this cool

428
00:28:12,100 --> 00:28:15,900
editing, but we can do this editing of a randomly generated

429
00:28:15,900 --> 00:28:21,220
image. And, and so, so when I posted this demo on on the web,

430
00:28:21,260 --> 00:28:23,660
you know, an artist called me and said, Hey, you know, I love

431
00:28:23,660 --> 00:28:27,060
how you can edit images, I can edit this image of a kitchen

432
00:28:27,060 --> 00:28:30,780
here. But that's not the kitchen I want to edit, I want to edit

433
00:28:30,780 --> 00:28:36,100
my own kitchen, right? Like here's a photo of my kitchen. And I

434
00:28:36,100 --> 00:28:41,740
want to edit that one. And I had to explain to them, you know,

435
00:28:41,740 --> 00:28:45,660
they said, Oh, can you just load into your demo? My my kitchen

436
00:28:45,660 --> 00:28:49,340
instead of yours. And I had to explain, no, no, no, that's not

437
00:28:49,340 --> 00:28:53,460
how GANs work. They're unconditional generators. You

438
00:28:53,460 --> 00:28:59,540
know, you give it a random vector of 512 numbers. And it

439
00:28:59,540 --> 00:29:03,900
decides what image to make. And then once it decides what image

440
00:29:03,900 --> 00:29:06,780
to make, then you can edit it. And so I'm sorry, I can't edit

441
00:29:06,780 --> 00:29:09,980
your kitchen. And so they were very disappointed by that

442
00:29:09,980 --> 00:29:12,140
because they had all sorts of ideas of things they wanted to

443
00:29:12,140 --> 00:29:18,260
do. And so now the problem is that, you know, the problem

444
00:29:18,260 --> 00:29:22,380
could be solved if we could find the random vector, some random

445
00:29:22,420 --> 00:29:26,420
vector that that output the kitchen image or the specific

446
00:29:26,420 --> 00:29:31,380
real photo that I wanted. The problem is how do I find it 512

447
00:29:31,380 --> 00:29:36,860
dimensional vectors as pretty big vector space. And and so I

448
00:29:36,860 --> 00:29:40,700
don't know if my GAN can actually generate this image or not. So

449
00:29:40,700 --> 00:29:44,260
one of the things you can do is you can just treat this as a as

450
00:29:44,260 --> 00:29:47,500
an inversion problem. You can take the neural network and you

451
00:29:47,500 --> 00:29:51,900
can learn how to run it backwards. Basically, you know,

452
00:29:52,300 --> 00:29:55,340
think of the neural network as a function G, and you want to

453
00:29:55,340 --> 00:29:58,300
learn G inverse. So you can treat that as another training

454
00:29:58,300 --> 00:30:00,580
problem. And there's a bunch of tricks and I won't go into all

455
00:30:00,580 --> 00:30:04,740
the tricks here. But but basically, the idea is that you

456
00:30:04,740 --> 00:30:08,140
can actually find a Z that comes closest to generating your image

457
00:30:08,140 --> 00:30:13,580
by by training and doing a couple other tricks. And you can

458
00:30:13,580 --> 00:30:18,660
actually get a Z that will generate your image pretty

459
00:30:18,660 --> 00:30:23,460
closely. But the thing that's a little bit sad is it also

460
00:30:23,460 --> 00:30:27,860
reveals things that the network cannot do. So so this network

461
00:30:28,740 --> 00:30:31,660
is capable of generating this image that I'm showing you here.

462
00:30:32,220 --> 00:30:35,340
But the original kitchen that I started with look like this. So

463
00:30:35,340 --> 00:30:37,420
you can see what the differences are. I've lost a lot of

464
00:30:37,420 --> 00:30:42,700
stuff. Right. So, you know, I can use the GAN to edit this

465
00:30:42,700 --> 00:30:46,940
image. But this image is not exactly what I started with. And

466
00:30:46,940 --> 00:30:53,980
so. So one of the pieces of science that that I did is I

467
00:30:53,980 --> 00:30:58,260
asked a question, you know, is there some way that we can

468
00:30:58,260 --> 00:31:01,500
actually make this work? Can we actually, you know, get the

469
00:31:01,500 --> 00:31:06,620
network to output a real photo that that the user gave us? We

470
00:31:06,620 --> 00:31:10,060
get the network to output this sort of simplified version of

471
00:31:10,060 --> 00:31:15,060
it. It turns out that if I modify the weights of the

472
00:31:15,060 --> 00:31:21,460
network, I can actually fine tune the network to get it so that

473
00:31:21,460 --> 00:31:24,900
a very, very nearby network with weights that are almost the

474
00:31:24,900 --> 00:31:31,060
same as the original actually hits this target image. Exactly.

475
00:31:31,820 --> 00:31:36,900
And so so there's a bunch of details in the right way of

476
00:31:36,900 --> 00:31:39,620
doing this. But it turns out that, you know, you don't actually

477
00:31:39,620 --> 00:31:42,780
have to change much if you change the fine grained weights

478
00:31:42,900 --> 00:31:47,300
of a network. You can you can change a lot of the details of

479
00:31:48,340 --> 00:31:52,500
what images actually get generated. And and and if you

480
00:31:52,540 --> 00:31:57,180
are given a target image to get you can actually tweak tweak any

481
00:31:57,180 --> 00:32:01,740
network to generate exactly that target image if we want. And

482
00:32:01,740 --> 00:32:02,100
so

483
00:32:04,100 --> 00:32:07,700
so you know, so yeah, we can get all the objects back. But the

484
00:32:07,700 --> 00:32:09,820
new thing is we haven't really changed the network much. So we

485
00:32:09,820 --> 00:32:13,300
can still do editing. So like if we take the window correlated

486
00:32:13,300 --> 00:32:16,340
neurons, we can take our modified network, we can turn them

487
00:32:16,340 --> 00:32:22,180
on. And and now we can like add a window. Let's see if we show

488
00:32:22,180 --> 00:32:24,380
that. Yeah, so this here's outlook. So we get this nice

489
00:32:24,380 --> 00:32:28,900
window here. And the scene is began is doing its cool tricks of

490
00:32:28,900 --> 00:32:31,860
orienting the window properly, doing some reasonable things. And

491
00:32:31,860 --> 00:32:35,620
it has some really interesting effects that are non trivial

492
00:32:35,620 --> 00:32:38,820
here. Some of them are good and some are bad. So for example,

493
00:32:38,860 --> 00:32:41,740
all I did was turn on the neurons in this location saying I

494
00:32:41,740 --> 00:32:47,460
want windows. And it did it. But look what else it did. It also

495
00:32:47,460 --> 00:32:50,020
added these reflections right here on the counter. And so this

496
00:32:50,340 --> 00:32:54,140
this kitchen guy does this a lot like adds adds non local

497
00:32:54,140 --> 00:32:56,660
reflections where it thinks that there's a shiny table. And so

498
00:32:56,660 --> 00:32:59,300
the cool thing here is that after I did all the inversion and

499
00:32:59,300 --> 00:33:02,340
stuff, this guy actually thinks that there's a shiny table here

500
00:33:02,340 --> 00:33:04,860
and it's right. And it thinks that if I add a window here,

501
00:33:04,860 --> 00:33:07,340
they should add reflection. That's right. Also, but look

502
00:33:07,340 --> 00:33:10,860
what else happened up here. See this lamp up here. When I first

503
00:33:10,860 --> 00:33:12,980
lifted this in low resolution, I thought, Oh, maybe it turned off

504
00:33:12,980 --> 00:33:15,060
the lamp because once you have windows, you don't need the light

505
00:33:15,060 --> 00:33:18,700
on. But no, it didn't do that. It just messed up the lamp. It's

506
00:33:18,700 --> 00:33:22,820
just total it took this whole area up here and just and just

507
00:33:22,820 --> 00:33:27,580
distorted it badly. And so so that that's a little dissatisfying.

508
00:33:27,580 --> 00:33:32,020
It means that this fine tuning thing, where we get again to,

509
00:33:33,020 --> 00:33:37,660
you know, target a specific user image, when I do when I try to

510
00:33:37,660 --> 00:33:40,580
teach it all the details, I'm not really teaching it what the lamp

511
00:33:40,580 --> 00:33:44,620
was, I was just sort of showing it how to arrange the pixels. And

512
00:33:44,620 --> 00:33:49,540
again, made its best guess on how to generalize how the image

513
00:33:49,540 --> 00:33:52,180
should look differently. If I change something like out of

514
00:33:52,180 --> 00:33:55,340
window, but with only one example of a lamp that looks like

515
00:33:55,340 --> 00:33:57,860
this, it generalized wrong, it has no idea what should happen to

516
00:33:57,860 --> 00:34:00,940
that lamp when I when I add a window. So this is this question

517
00:34:00,980 --> 00:34:05,340
of like how to make changes in a network with with with

518
00:34:05,340 --> 00:34:10,260
achieving good generalization is, which is a good question. And

519
00:34:10,300 --> 00:34:13,820
it was, there was something that puzzled me for a year after

520
00:34:13,820 --> 00:34:17,380
doing this work. But but the work is still pretty cool, you can

521
00:34:17,380 --> 00:34:21,460
still use it for modifying real photos. So here's like a photo of

522
00:34:22,020 --> 00:34:26,700
I got off of Wikipedia of like some real locations. And you can

523
00:34:26,740 --> 00:34:29,860
you can edit them, I can add grass, I can add doors, I can add

524
00:34:29,860 --> 00:34:33,860
domes, you know, just like, just like the the the other

525
00:34:33,860 --> 00:34:36,380
campaign app, except I can actually start with a real photo

526
00:34:36,380 --> 00:34:38,980
that you give me. And I can invert that photo through the

527
00:34:38,980 --> 00:34:41,820
network, get a good starting image, fine tune the network to

528
00:34:41,860 --> 00:34:45,540
make it make it output, you know, the target image and edit that

529
00:34:45,540 --> 00:34:48,540
image, add bigger domes, and it'll sort of match the

530
00:34:48,540 --> 00:34:52,780
architectural style. And, and, and, you know, do different things

531
00:34:52,780 --> 00:34:55,700
like that, I can add domes, remove domes, add doors, you

532
00:34:55,700 --> 00:34:58,900
know, things like that. Let me see if I can get this video here

533
00:34:58,940 --> 00:35:04,140
to show. So this is the status center. Let's add some doors

534
00:35:04,140 --> 00:35:08,380
here. So you get the idea, I'm doing exactly the same

535
00:35:08,380 --> 00:35:11,820
intervention that I did before. And it's it's opinion just like

536
00:35:11,820 --> 00:35:15,260
before, it will not add doors in places that it doesn't think are

537
00:35:15,300 --> 00:35:17,900
not good places for a door, it has some opinions about where

538
00:35:17,900 --> 00:35:20,740
doors are allowed, it likes to put them in brick walls. It

539
00:35:20,740 --> 00:35:25,780
thinks it's okay to put a door in a tower, like that architectural

540
00:35:25,780 --> 00:35:28,880
detail. Oh, I put domes here. It's happy to put domes on top of

541
00:35:28,880 --> 00:35:31,560
buildings. It's not happy to put a dome like in the middle of the

542
00:35:31,560 --> 00:35:35,520
sky. It's not happy to put a door in the middle of the sky. But

543
00:35:35,520 --> 00:35:40,760
you know, it put trees in different places. And, and so

544
00:35:40,760 --> 00:35:42,160
there are things that it understands, there are things

545
00:35:42,160 --> 00:35:44,320
that it doesn't understand very well, it's sort of making a

546
00:35:44,320 --> 00:35:47,160
guess of what the structure of the image is. It doesn't know what

547
00:35:47,160 --> 00:35:50,480
to make of my advisor, you know, sort of planting grass in front

548
00:35:50,480 --> 00:35:53,920
of him. And that's not very realistic. But you kind of get

549
00:35:53,920 --> 00:35:57,280
a feel for what the structure and knowledge of the model is by

550
00:35:57,280 --> 00:35:59,560
doing these kind of interventions. So this was really

551
00:35:59,560 --> 00:36:02,840
cool. I think it got a lot of people's attention. Adobe

552
00:36:02,840 --> 00:36:07,080
noticed this stuff, and has been busy trying to make different

553
00:36:07,080 --> 00:36:12,280
painting applications using, you know, GAN technology that are

554
00:36:12,320 --> 00:36:17,160
I think partially inspired by by by this kind of discovery. So

555
00:36:17,440 --> 00:36:22,920
David, I have a question. Yep. This is really cool. Question is,

556
00:36:23,280 --> 00:36:28,240
when you modify, for instance, churches, I assume you have

557
00:36:28,600 --> 00:36:33,800
trained your GAN on a church data set. Yes, that's correct.

558
00:36:33,840 --> 00:36:37,680
What about when you do it on the real images, for instance, in

559
00:36:37,680 --> 00:36:42,920
this case, you know, your advisor? Yes. So actually, both of

560
00:36:42,920 --> 00:36:45,880
these are using the church data set as well. So the church

561
00:36:45,880 --> 00:36:46,400
data set,

562
00:36:46,440 --> 00:36:50,760
interesting that even you have trained again on church, you

563
00:36:50,760 --> 00:36:52,480
can depict a person.

564
00:36:53,280 --> 00:36:58,320
Yes. So this is so the GAN. Now, you have to keep in mind that

565
00:36:58,320 --> 00:37:02,480
what I've done here is I fine tuned the GAN. So you can

566
00:37:02,480 --> 00:37:05,280
actually, you know, you can actually get you can actually

567
00:37:05,280 --> 00:37:11,160
get a GAN to do a lot of things by fine tuning it. So I've I've

568
00:37:11,160 --> 00:37:16,240
told the GAN, please basically overfit on this target image. So

569
00:37:16,240 --> 00:37:23,880
the GAN, you know, has 30 million parameters. And, and you

570
00:37:23,880 --> 00:37:27,200
know, an image only has, you know, 10,000 pixels, and it has

571
00:37:27,200 --> 00:37:31,520
plenty of excess capacity to memorize the details that I

572
00:37:31,520 --> 00:37:36,320
might want to do. And so what I've done is this as I've taken

573
00:37:36,320 --> 00:37:40,400
the image, I've asked again, through my inversion techniques,

574
00:37:40,440 --> 00:37:44,880
what is the closest church image that you can generate that

575
00:37:44,880 --> 00:37:48,000
looks like my thing. And you get a different image. I don't

576
00:37:48,000 --> 00:37:51,240
have the image to show you here, but you get an image that looks

577
00:37:51,240 --> 00:37:53,800
kind of more church like it's a little bit, it'll be

578
00:37:53,800 --> 00:37:56,800
architectural have the right kind of shape, the kind of right

579
00:37:56,800 --> 00:38:00,280
textures. But you know, it won't show my advisor here and

580
00:38:00,280 --> 00:38:03,720
things like that. It'll be, it'll be this rough approximation

581
00:38:04,240 --> 00:38:10,040
for that my my image, but that is in the domain of what the GAN

582
00:38:10,040 --> 00:38:14,280
can actually generate. Then I say, Okay, that's not what I

583
00:38:14,280 --> 00:38:17,560
want to do. I want to actually edit this photo. So let's fine

584
00:38:17,560 --> 00:38:24,680
tune that network so that so that given that same Z instead of

585
00:38:24,680 --> 00:38:27,400
generating the church that you would normally generate, I want

586
00:38:27,400 --> 00:38:31,560
you to generate this image, change the weight slightly, get it

587
00:38:31,560 --> 00:38:36,520
so that that Z targets this. And, and so that's what I've done

588
00:38:36,520 --> 00:38:39,320
here. But I've tried I've done that in a way where I try not to

589
00:38:39,320 --> 00:38:42,240
change the weights too much. I just try to change the weights. I

590
00:38:42,280 --> 00:38:44,800
change the fine grained layers. And I don't change the coarse

591
00:38:44,800 --> 00:38:48,320
grain layers. And I, and I have a regularizer to make sure the

592
00:38:48,320 --> 00:38:53,760
weights don't change too much. And that you are changing the

593
00:38:53,800 --> 00:38:59,440
pre trained weights, or you are putting some extra weights, and

594
00:38:59,440 --> 00:39:03,960
then you place them. Oh, here, I'm actually changing the pre

595
00:39:03,960 --> 00:39:08,640
trained weights. So the network has 15 layers. I'm actually

596
00:39:08,640 --> 00:39:12,480
going and I'm changing some of those layers. I'm not adding

597
00:39:12,480 --> 00:39:16,920
anything new to the network. I'm just changing the weights in

598
00:39:16,920 --> 00:39:21,480
the network itself. Now, now what I've done here is I've

599
00:39:21,480 --> 00:39:25,720
overfit the network to this one image. The network is not

600
00:39:25,800 --> 00:39:30,040
generalizing this knowledge. So for example, you can draw

601
00:39:30,040 --> 00:39:34,720
Antonio in this one image. But if I look in the network, if I

602
00:39:34,720 --> 00:39:37,840
probe it a lot and see, can it ever generate Antonio in a

603
00:39:37,840 --> 00:39:41,960
different setting in a different image? It cannot. In

604
00:39:41,960 --> 00:39:46,640
fact, you know, as much as we probe things, it really doesn't

605
00:39:46,640 --> 00:39:49,160
look like we've changed the output of the network in any

606
00:39:49,160 --> 00:39:53,720
meaningful way for any image, except for this one. It's almost

607
00:39:53,720 --> 00:39:57,480
like, you know, the network generates this really complicated

608
00:39:57,480 --> 00:40:01,520
manifold of realistic images. And we've told we've picked up one

609
00:40:01,520 --> 00:40:04,680
point of the manifold, and we've dragged it over to pass to

610
00:40:04,680 --> 00:40:08,160
this point. But we've done it in a very local way. So it's

611
00:40:08,160 --> 00:40:11,280
really not affected any other points of what the GAN is

612
00:40:11,280 --> 00:40:18,200
generating. And so so but but for the purposes of doing this

613
00:40:18,200 --> 00:40:20,120
kind of application, it doesn't matter that it's not

614
00:40:20,120 --> 00:40:22,600
generalizing because the user doesn't care about a different

615
00:40:22,600 --> 00:40:25,000
photo, they just care about their own photo. So it's a pretty

616
00:40:25,000 --> 00:40:29,040
cool. It's pretty cool technique anyway, even though it's sort

617
00:40:29,040 --> 00:40:32,920
of not the classical goal of machine learning. Does that make

618
00:40:32,960 --> 00:40:37,840
sense? Yeah, it does. And I wonder if the user has more

619
00:40:37,840 --> 00:40:43,760
images of themselves with that over time, and make the network

620
00:40:43,800 --> 00:40:45,640
even better in generation?

621
00:40:47,440 --> 00:40:50,480
Yes, this is the big question. And I played with this for many

622
00:40:50,480 --> 00:40:52,720
months, and I haven't got it to work. And if anybody can figure

623
00:40:52,720 --> 00:40:54,760
out how to get to work, I feel like it's one of the holy grails

624
00:40:55,160 --> 00:40:58,160
of like how to add a new thing to a generator. So like, the

625
00:40:58,160 --> 00:41:01,120
generator knows about all these things that knows about trees

626
00:41:01,120 --> 00:41:06,400
and knows about all these architectural pieces, you know. But

627
00:41:06,400 --> 00:41:08,880
what if I came along with something new? What if I was

628
00:41:10,680 --> 00:41:14,360
what if my what if I what if I work for GM and I want to sell

629
00:41:14,360 --> 00:41:17,040
Cadillacs, then I then I might come to one of these models and

630
00:41:17,040 --> 00:41:19,640
say, you know what, you should draw cars. In fact, I want you to

631
00:41:19,640 --> 00:41:22,680
draw specific cars. I want you to draw Cadillacs in front of all

632
00:41:22,680 --> 00:41:27,760
these buildings. How would I add Cadillacs to my model or add

633
00:41:27,760 --> 00:41:31,120
Antonio to my model or something like that? And we don't know how

634
00:41:31,120 --> 00:41:33,400
to do that yet. Although I'm going to show you a little bit of

635
00:41:33,400 --> 00:41:38,200
work, where we can do something that's very similar. And if I

636
00:41:38,200 --> 00:41:41,120
don't know if I have time to, to go over this, but I'm going to

637
00:41:41,160 --> 00:41:43,440
I'm going to zoom through this because I'm so excited by this

638
00:41:43,440 --> 00:41:50,680
work. So, so, so it's motivated by this, this sort of question,

639
00:41:50,720 --> 00:41:55,640
which is, you know, we have a model of like drawing towers,

640
00:41:55,640 --> 00:41:59,400
let's say, right? But there are things in the world that we might

641
00:41:59,400 --> 00:42:03,400
want to model that we don't have a data set for. For example, you

642
00:42:03,400 --> 00:42:07,080
know, in in in Decatur County, Illinois, there's this courthouse

643
00:42:07,080 --> 00:42:09,880
that has a tree growing out the top of the tower. It started

644
00:42:09,880 --> 00:42:12,800
growing out there by accident, but the people in the town love

645
00:42:12,800 --> 00:42:16,240
it. And so but it's but there's no so like if I want to get a

646
00:42:16,240 --> 00:42:20,360
generative model to draw trees growing out of tops of towers, I

647
00:42:20,360 --> 00:42:24,240
can't do that in a classical way because I can't create a big

648
00:42:24,240 --> 00:42:27,840
data set of a million buildings that have trees growing on the

649
00:42:27,840 --> 00:42:30,520
top of the towers, because they don't exist. It's just this one.

650
00:42:31,360 --> 00:42:36,640
And so now if if the point is I want to generate images of this

651
00:42:36,640 --> 00:42:40,840
type, you know, well, I could use a regular image editor, I

652
00:42:40,840 --> 00:42:43,200
can take any building of a tower, and of course, I can stick a

653
00:42:43,200 --> 00:42:47,000
tree on it, right? I could use my, you know, again, painting

654
00:42:47,000 --> 00:42:50,080
method to, you know, activate tree neurons or something like

655
00:42:50,080 --> 00:42:53,280
that. But no, no, that's not what I'm asking. I'm asking this

656
00:42:53,280 --> 00:42:57,240
other question of like, how can we stick tree towers into my

657
00:42:57,240 --> 00:43:01,160
model? How do I modify the model to have this new concept in it?

658
00:43:01,480 --> 00:43:03,880
Like I start with this model that has all these weights that

659
00:43:03,920 --> 00:43:06,800
encode all these rules for how buildings look and things like

660
00:43:06,800 --> 00:43:11,080
that. And I want to create a new model that has new weights that

661
00:43:11,080 --> 00:43:14,120
encode new rules. So for example, the old model could generate

662
00:43:14,120 --> 00:43:17,200
all these buildings that of towers that look normal have

663
00:43:17,200 --> 00:43:20,880
spires, you know, pointy tops. And I want to make a new model

664
00:43:20,880 --> 00:43:24,560
that has weights, they encode a different rule, so that like,

665
00:43:24,680 --> 00:43:28,200
they have trees growing out the top, right, or any rule that I

666
00:43:28,200 --> 00:43:32,800
choose, right? And it turns out that this is actually possible.

667
00:43:32,800 --> 00:43:36,160
So this is different from the technique that I showed you

668
00:43:36,160 --> 00:43:39,080
before, because in this technique, it's actually

669
00:43:39,080 --> 00:43:43,240
generalizing. This is, you know, if you use this technique, not

670
00:43:43,240 --> 00:43:48,120
only you change the output of one image of the GAN to have like

671
00:43:48,120 --> 00:43:53,480
some effect, but we can actually change the outputs for a whole

672
00:43:53,480 --> 00:43:58,360
class of, you know, a large subset of the outputs of the GANs to

673
00:43:58,360 --> 00:44:04,000
follow a different rule, like any pointy tower output will have

674
00:44:04,040 --> 00:44:08,000
trees instead of pointy towers. And so so I'll just show you a

675
00:44:08,000 --> 00:44:10,360
little bit of like the interaction here of what it

676
00:44:10,360 --> 00:44:15,280
looks like when you get our method into an application. So I

677
00:44:16,200 --> 00:44:18,680
let's see if I can get this to work. So here, what I'm showing

678
00:44:18,680 --> 00:44:22,840
you is the output of a style GAN be to generating churches, you

679
00:44:22,840 --> 00:44:26,680
can kind of, and there are three parts of this UI, there's an

680
00:44:26,680 --> 00:44:30,920
image viewer, then what you do is you can select a rule that you

681
00:44:30,920 --> 00:44:33,440
want to change, and then you can specify how you want to change

682
00:44:33,440 --> 00:44:35,800
your rule. So there's three parts of this little user

683
00:44:35,800 --> 00:44:39,040
interface. And I'll just show you sort of how how the effect

684
00:44:39,040 --> 00:44:43,520
looks by showing you one of the interactions. So you can kind of

685
00:44:43,520 --> 00:44:48,560
use the image viewer to scroll through lots of examples of of

686
00:44:48,560 --> 00:44:52,400
what the the generator is capable of generating. And then we

687
00:44:52,400 --> 00:44:54,800
can go to these examples and we can say, Hey, you know what I'm

688
00:44:54,800 --> 00:44:58,760
really interested in? I'm interested in this rule of how

689
00:44:58,760 --> 00:45:02,200
to generate pointy towers. And so I can select a few pointy

690
00:45:02,200 --> 00:45:06,320
towers. And you can think of this as what I'm looking for is

691
00:45:06,320 --> 00:45:09,360
the neurons that are responsible for the shape. And so I can

692
00:45:09,360 --> 00:45:13,920
select a few examples and I can say, Hey, what other, what other

693
00:45:13,920 --> 00:45:17,600
outputs of the GAN share the same representation? And, and it'll

694
00:45:17,600 --> 00:45:20,120
show me, Oh, yes, the GAN is generalizing this way, these

695
00:45:20,120 --> 00:45:23,520
other pointy towers are represented the same way as the

696
00:45:23,520 --> 00:45:27,040
ones that you chose. And then I can go and I can say, All right,

697
00:45:27,040 --> 00:45:33,400
I want to redefine how these pointy towers are rendered by

698
00:45:33,400 --> 00:45:36,640
this generator, I want them to be rendered like this tree here.

699
00:45:36,640 --> 00:45:41,240
So I can copy the tree from one output of the generator, and I

700
00:45:41,240 --> 00:45:45,680
can paste it into where I would like that tree to show up. I

701
00:45:45,680 --> 00:45:49,440
wanted to show up instead of pointy towers. And then I can

702
00:45:49,440 --> 00:45:54,000
say, Okay, now insert this new rule into the model, compute

703
00:45:54,000 --> 00:45:57,840
what the right changes to change the model. And then after I do

704
00:45:57,840 --> 00:46:02,240
that, that takes about a second to do the math to figure out how

705
00:46:02,240 --> 00:46:06,080
to change a rule. And then after I do that, then I get the GAN to

706
00:46:06,120 --> 00:46:10,480
generate new images. And, and they look like this, you know,

707
00:46:10,520 --> 00:46:14,760
like the tops of the towers, now have trees on them instead. So

708
00:46:14,760 --> 00:46:17,600
you can see how that looks. And it's not just affecting that

709
00:46:17,600 --> 00:46:22,680
one image, it's affecting all the pointy tower images. I can do a

710
00:46:22,680 --> 00:46:26,400
little search for more pointy tower images. And, and do I have

711
00:46:26,400 --> 00:46:29,760
that here in my thing? Yeah, so here's a search for more pointy

712
00:46:29,760 --> 00:46:32,120
tower images. And you can see they, you know, they all have

713
00:46:32,120 --> 00:46:36,040
gotten these trees sprouting out the top of it, like some sort

714
00:46:36,040 --> 00:46:41,880
of dystopian tree world where vegetation is taking over the

715
00:46:41,880 --> 00:46:46,600
planet. And, and so you can do this in a bunch of things, I'm

716
00:46:46,600 --> 00:46:50,880
gonna skip over some of the technical things here, or some

717
00:46:50,880 --> 00:46:53,640
of the other examples of what you can do here. You can edit

718
00:46:53,640 --> 00:46:56,600
reflections and things like that. I've got other videos that you

719
00:46:56,600 --> 00:46:59,400
can look for on the internet. But I wanted to show you a sense

720
00:46:59,400 --> 00:47:02,800
for what we're doing inside when we do this kind of thing. So

721
00:47:02,880 --> 00:47:07,280
like I showed you before that again, has is like, got all these

722
00:47:07,280 --> 00:47:11,920
convolutional layers stacked up, it's about 15 layers. And what

723
00:47:11,920 --> 00:47:16,120
what, what, what the discovery was that led to this application

724
00:47:16,640 --> 00:47:20,920
was that each one of those layers can be thought of as

725
00:47:20,920 --> 00:47:26,000
solving a very simple, separate problem from the other layers.

726
00:47:26,360 --> 00:47:29,800
And what is that simple problem? It, it can be treated like a

727
00:47:29,800 --> 00:47:34,840
memory, where the layer is solving this problem of matching

728
00:47:34,840 --> 00:47:39,680
key value pairs that it's memorized. So every location

729
00:47:39,760 --> 00:47:44,320
has a feature vector that you can think of as a key. And what

730
00:47:44,360 --> 00:47:46,760
and the key each key like, you know, represents a certain

731
00:47:46,760 --> 00:47:49,360
type of context, like, you know, the middles of towers or the

732
00:47:49,360 --> 00:47:52,640
tops of towers or something like that. And what you can think

733
00:47:52,680 --> 00:48:00,680
of the map as as as storing is what should be what is like the

734
00:48:00,680 --> 00:48:05,000
pattern of features that should be rendered whenever that

735
00:48:05,000 --> 00:48:08,120
context comes up. Right. So you can think of it as just

736
00:48:08,120 --> 00:48:14,760
basically key value store. And and so so this whole idea of

737
00:48:14,800 --> 00:48:17,880
using a matrix as a key value stores and it's like the oldest

738
00:48:17,880 --> 00:48:23,560
idea in neural networks. People observe back in the 1970s,

739
00:48:24,080 --> 00:48:28,360
that if you have a single layer neural network, you can treat

740
00:48:28,360 --> 00:48:32,320
it as a as an approximate key value store that remembers keys

741
00:48:32,320 --> 00:48:38,320
with minimal error. And and so if you had a set of keys and a

742
00:48:38,320 --> 00:48:41,320
set of values you want to store, and you ask what is the

743
00:48:41,320 --> 00:48:44,480
optimal single layer neural network that you'd use to store

744
00:48:44,480 --> 00:48:48,840
it. It's actually, you know, classical linear algebra, it's

745
00:48:48,840 --> 00:48:51,840
like the solution to a least squares problem. So what we can

746
00:48:51,840 --> 00:48:56,880
hypothesize is that in these very, very fancy, you know, 2020,

747
00:48:56,920 --> 00:49:02,680
you know, 50 years later, deep neural networks, actually, each

748
00:49:02,680 --> 00:49:05,920
layer is just acting as one of these. Now, which keys are being

749
00:49:05,920 --> 00:49:09,600
stored and what values were being stored? We don't know. But

750
00:49:09,600 --> 00:49:12,240
we could hypothesize that there is some set of things that are

751
00:49:12,240 --> 00:49:16,680
being memorized, some set of keys and values. And so that that

752
00:49:16,680 --> 00:49:19,560
maybe this weight matrix that we have is the solution to the

753
00:49:19,560 --> 00:49:23,200
assembly squares problem. So the cool thing that we can do is we

754
00:49:23,200 --> 00:49:25,880
can say we can ask the question, what would the weight matrix

755
00:49:25,880 --> 00:49:30,120
look like if we changed one of the rules? What if we had one new

756
00:49:30,120 --> 00:49:34,280
key value pair that we wanted to change? Then what would the

757
00:49:34,280 --> 00:49:38,120
weight matrix be? Instead, we want all the other things that

758
00:49:38,120 --> 00:49:41,680
the network has memorized to still be memorized with minimal

759
00:49:41,720 --> 00:49:45,000
error, just as before, except we're going to give this new

760
00:49:45,000 --> 00:49:48,360
constraint, we want to write a new key value pair into it. And it

761
00:49:48,360 --> 00:49:50,320
turns out that that's also least squares problems and

762
00:49:50,320 --> 00:49:51,800
constrained least squares problem, we can write down the

763
00:49:51,800 --> 00:49:55,320
solution in this form. And the cool thing about these two, the

764
00:49:55,320 --> 00:49:58,640
squares problems is that they cancel each other out. Most of

765
00:49:58,640 --> 00:50:03,080
the terms are the same. And, and, and we can actually ask the

766
00:50:03,080 --> 00:50:07,360
question, how would the weights have to change if we add a new

767
00:50:07,360 --> 00:50:11,480
key value pair, without knowing which values were written into

768
00:50:11,480 --> 00:50:15,360
the network before, we don't actually have to know what the

769
00:50:15,360 --> 00:50:20,600
old key value pairs were, we can just assume that the network

770
00:50:20,600 --> 00:50:25,320
was optimal as storing all these key value pairs. And, and the

771
00:50:25,320 --> 00:50:29,640
math for like how to write a new key value pair comes out the

772
00:50:29,640 --> 00:50:32,120
same anyway. So, so that's there's there's a little bit of a

773
00:50:32,120 --> 00:50:37,320
mathematical insight and trick here. But what it allows us to do

774
00:50:37,320 --> 00:50:42,440
is it allows us to find exactly what we want to do to change one

775
00:50:42,440 --> 00:50:44,720
thing that the network is memorized, you do this rank one

776
00:50:44,720 --> 00:50:50,600
update in a specific direction. And, and you can take a key and

777
00:50:50,600 --> 00:50:54,200
change it to any value you want. And that will, you know, the

778
00:50:54,200 --> 00:50:59,760
same form will minimize error for, for other keys, regardless of

779
00:50:59,760 --> 00:51:03,400
what value we write, it's almost like it really is a form of

780
00:51:03,440 --> 00:51:07,320
memory, that we're changing. So our method is basically you

781
00:51:07,320 --> 00:51:13,080
find a key by asking the user to select a few contexts that look

782
00:51:13,080 --> 00:51:16,040
the same, we average them to get a good key. Then we ask for a

783
00:51:16,040 --> 00:51:19,240
copy paste example to get a goal. That's the new value that we

784
00:51:19,240 --> 00:51:21,840
want to write into the key of the memory. And then we do this

785
00:51:21,840 --> 00:51:28,000
math to, to find how to change w in the direction of the key

786
00:51:28,000 --> 00:51:33,520
only, we find a rank one update that does this. And so, and so

787
00:51:33,520 --> 00:51:35,480
that avoids changing other rules. So we can do this on a bunch

788
00:51:35,480 --> 00:51:39,120
of different GAN models. And, and so you can see, like, you

789
00:51:39,120 --> 00:51:44,520
know, people like to change people's expressions here. So

790
00:51:44,520 --> 00:51:47,280
what we're doing is a little different from what you normally

791
00:51:47,280 --> 00:51:49,160
do to change people's expressions. And again, what

792
00:51:49,160 --> 00:51:51,440
we're doing is we're actually going to rewrite the GAN. So it

793
00:51:51,440 --> 00:51:53,520
only outputs people who are smiling, we're going to take all

794
00:51:53,520 --> 00:51:55,880
the frowns, we're saying, okay, there's, there's a rule for

795
00:51:55,880 --> 00:51:58,040
frowns, we're going to change that to a rule for smiles by

796
00:51:58,040 --> 00:52:01,680
showing an example. And so by patching frowns to smiles, now

797
00:52:01,680 --> 00:52:04,880
we have a model that just outputs people who are smiling. Now

798
00:52:04,880 --> 00:52:08,440
we live in a happy world. So that's, that's, that's pretty

799
00:52:08,440 --> 00:52:11,080
cool. And now, of course, we could have done that by, you

800
00:52:11,080 --> 00:52:15,080
know, changing the training set by collecting only training

801
00:52:15,080 --> 00:52:17,480
data of people who are smiling. But the neat thing is that you

802
00:52:17,480 --> 00:52:20,560
can also do this for things where you don't have a training

803
00:52:20,560 --> 00:52:23,080
set that looks like it. So for example, there's a, there's a

804
00:52:23,080 --> 00:52:27,160
rule in the model for how eyebrows should look on kids. So

805
00:52:27,160 --> 00:52:31,080
you can see that kids have these very wispy light eyebrows that

806
00:52:31,080 --> 00:52:34,480
don't have much hair. So we can find that rule by identifying a

807
00:52:34,480 --> 00:52:37,400
few examples that gives us a rank one direction in the weight

808
00:52:37,400 --> 00:52:41,000
matrix. And then we can redefine it, we can write a new thing

809
00:52:41,000 --> 00:52:43,280
into it and say, you know what, we want the eyebrows to look

810
00:52:43,280 --> 00:52:47,000
like this, like that's very bushy much sash. And, you know,

811
00:52:47,000 --> 00:52:51,000
paste into one example, do the math. And then now we can change

812
00:52:51,040 --> 00:52:54,000
weights in a way that generalizes. So now all the kids had

813
00:52:54,000 --> 00:52:58,440
these very bushy, you know, eyebrows. And it's something

814
00:52:58,440 --> 00:53:01,520
that we wouldn't have been able to get by collecting training

815
00:53:01,520 --> 00:53:03,800
set because we don't have kids that look like this in the real

816
00:53:03,800 --> 00:53:07,280
world. It's something that just comes out of our imagination. So

817
00:53:07,280 --> 00:53:11,720
this is kind of the thing. I kind of feel like this is the big

818
00:53:11,720 --> 00:53:15,720
reason why, why, why be interested in how these models

819
00:53:15,720 --> 00:53:20,640
are working inside. And the reason to be so interested in it is

820
00:53:20,640 --> 00:53:24,560
because as long as we don't look inside our models, then we're

821
00:53:24,560 --> 00:53:27,960
really constrained. Because the only thing that our models can

822
00:53:27,960 --> 00:53:32,480
really do is imitate the real world. We can collect huge

823
00:53:32,480 --> 00:53:36,040
amounts of data. And the models that we create, we'll just get

824
00:53:36,040 --> 00:53:39,680
better and better at imitating the way that the data is the way

825
00:53:39,680 --> 00:53:43,480
the world is today. And I kind of feel like it goes a little bit

826
00:53:43,480 --> 00:53:46,600
against why I was interested in computer science years ago when I

827
00:53:46,600 --> 00:53:49,200
entered it in the first place. Because the amazing thing about

828
00:53:49,200 --> 00:53:53,360
computer science is that you can use it to create algorithms

829
00:53:53,360 --> 00:53:56,480
that represent things in the world that don't exist yet,

830
00:53:56,520 --> 00:53:59,520
things that you can only imagine. And so machine learning is

831
00:53:59,520 --> 00:54:02,400
sort of on this path right now, where we're getting very, very

832
00:54:02,400 --> 00:54:06,640
good at replicating the way the world is. And we're going to be

833
00:54:06,640 --> 00:54:10,280
confronted with this question of how do we use these techniques

834
00:54:10,440 --> 00:54:14,080
to actually create new worlds that don't exist yet that are the

835
00:54:14,080 --> 00:54:18,400
way that we want them to be. And I think that this really

836
00:54:18,400 --> 00:54:23,880
going to require us to not just get models that are just really

837
00:54:23,880 --> 00:54:27,720
good at imitating, but also models that are understandable to

838
00:54:27,720 --> 00:54:33,160
people so that we can change their rules inside, and then use

839
00:54:33,160 --> 00:54:36,480
them to create things that are based on our imagination instead

840
00:54:36,480 --> 00:54:44,320
of just the training data. And so here's a fun thing here, I

841
00:54:44,360 --> 00:54:50,560
think, if I want to be fair to the horses, you notice that none

842
00:54:50,560 --> 00:54:55,080
of the horses in this horse generating GAN get to wear hats

843
00:54:55,120 --> 00:54:58,560
even though all the people get to wear hats. So we can change

844
00:54:58,560 --> 00:55:03,280
that by taking a hat from a person and inserting it into our

845
00:55:03,280 --> 00:55:06,440
GAN's model of what a horse's head should look like. And now

846
00:55:06,440 --> 00:55:11,680
horses get to wear hats, right? And so, so let's build a better

847
00:55:11,680 --> 00:55:18,000
world. And, and allow people to change the rules of the world by

848
00:55:18,000 --> 00:55:22,360
making the rules more visible and and manipulatable by humans.

849
00:55:22,760 --> 00:55:28,640
That's that's sort of the goal of the whole thing. So any

850
00:55:28,640 --> 00:55:29,960
questions? Any questions?

851
00:55:30,280 --> 00:55:31,960
I have a question. Yes.

852
00:55:32,080 --> 00:55:35,920
Does this method work with multiple different models? Or is

853
00:55:35,920 --> 00:55:39,600
it only successful when like, taking a hat from within this

854
00:55:39,600 --> 00:55:42,080
model and put it on a horse?

855
00:55:42,160 --> 00:55:49,000
So right now, this, this method is only able to take it, it's

856
00:55:49,240 --> 00:55:53,120
it's only able to rewire one model. So I can take one part of a

857
00:55:53,120 --> 00:55:56,720
model and rewire it to a different model, you're sort of

858
00:55:56,720 --> 00:55:59,200
asking the transplant question. So I'm sort of at the point

859
00:55:59,200 --> 00:56:03,520
where, you know, it's like a surgeon, I can like connect one

860
00:56:03,520 --> 00:56:05,720
blood vessel to another blood vessel in the same human, right?

861
00:56:05,720 --> 00:56:07,480
And you're sort of asking the question, well, can I do a heart

862
00:56:07,480 --> 00:56:10,240
transplant? Can I take a heart out of one person put another one?

863
00:56:10,560 --> 00:56:15,720
And I cannot do that yet. It turns out to be harder. And, but I

864
00:56:15,760 --> 00:56:20,680
but it is a it is an obvious goal. And I, and I feel confident

865
00:56:20,680 --> 00:56:25,200
that if we understand well enough, all the things that make

866
00:56:25,200 --> 00:56:28,400
these computations work, what is needed for the care and

867
00:56:28,400 --> 00:56:31,600
feeding of a computational module? What is a computational

868
00:56:31,600 --> 00:56:35,200
module inside a big learning system? Then we should, you

869
00:56:35,240 --> 00:56:38,640
know, it should be a goal to be able to move a piece of

870
00:56:38,640 --> 00:56:43,160
computation from one neural network to another one. Does that

871
00:56:43,160 --> 00:56:43,640
make sense?

872
00:56:45,480 --> 00:56:46,040
Yes, thank you.

873
00:56:46,400 --> 00:56:49,840
Yep. That's a really great question, by the way. I think

874
00:56:49,840 --> 00:56:55,120
it's, I think it's fundamental. Any other questions?

875
00:56:59,280 --> 00:57:04,520
This is not too well articulated question. I was just

876
00:57:04,520 --> 00:57:08,440
curious what you, what are your thoughts about this? I think

877
00:57:08,440 --> 00:57:14,000
this is this like neural nets have tendency to like avoid the

878
00:57:14,000 --> 00:57:16,760
responsibility of the results, like everything is done in the

879
00:57:16,800 --> 00:57:19,920
hidden layers and sort of shrug off shrug off the

880
00:57:19,920 --> 00:57:23,600
responsibility about the results. And I thought it was like

881
00:57:23,600 --> 00:57:28,160
interesting how you set the objective towards something as

882
00:57:28,200 --> 00:57:33,400
abstract as realistic. And here, like how you define the concept

883
00:57:33,400 --> 00:57:37,600
of being realistic is based on the big data you collected from

884
00:57:37,600 --> 00:57:42,680
the web, but but oftentimes some like fake images sometimes

885
00:57:42,680 --> 00:57:48,080
look even more realistic than real images. And I don't know,

886
00:57:48,080 --> 00:57:51,280
like tree growing on top of the building may look fairly

887
00:57:51,280 --> 00:57:56,680
realistic for some people, but maybe for plant experts, maybe

888
00:57:56,720 --> 00:58:02,480
it would not. Right. So I don't know, like, I think this might

889
00:58:02,640 --> 00:58:07,280
result in like the blurring between the it's making us hard

890
00:58:07,280 --> 00:58:10,040
to distinguish between the real and the fake or something like

891
00:58:10,040 --> 00:58:10,560
that. I don't know.

892
00:58:11,120 --> 00:58:17,400
Yes, yes. No, I think that there are so so the we're we're

893
00:58:17,400 --> 00:58:22,600
unaccustomed to making it easy for making programs that make

894
00:58:22,600 --> 00:58:26,040
such realistic renderings of the world. And it's actually a

895
00:58:26,040 --> 00:58:30,680
concern. I think that, you know, people have misused this

896
00:58:30,720 --> 00:58:33,720
technology already that we you know, we use we you know, there's

897
00:58:33,720 --> 00:58:38,960
the whole deep fakes phenomenon. But even without like faking

898
00:58:38,960 --> 00:58:46,040
videos, people people have you know, used face generators to

899
00:58:46,040 --> 00:58:49,160
make lots of fake Facebook profiles and things like that,

900
00:58:49,160 --> 00:58:52,240
you know, pretending there are millions of people that exist

901
00:58:52,240 --> 00:58:56,240
that don't actually exist and things like that. So so even

902
00:58:56,280 --> 00:58:58,640
before you sort of do manipulations of the world, I

903
00:58:58,640 --> 00:59:03,720
think that there's already this problem of of of, you know,

904
00:59:03,720 --> 00:59:08,200
pretending that there's a lot of data that there actually isn't

905
00:59:08,240 --> 00:59:11,120
by using these generator models. And so I think that there's

906
00:59:13,640 --> 00:59:18,000
you know, the whole the whole question of fakes is a very

907
00:59:18,000 --> 00:59:25,160
serious question, like how do we how do we function society if

908
00:59:25,160 --> 00:59:29,680
we don't know what's real and what's fake. Now, it's not a

909
00:59:29,680 --> 00:59:35,720
totally new issue. You don't need a state of the art deep

910
00:59:35,760 --> 00:59:39,760
learning model to make fake, you know, people have made fake

911
00:59:39,760 --> 00:59:43,760
photoshopped by hand forever, people write can write text that

912
00:59:43,760 --> 00:59:47,640
has all sorts of lies forever. In fact, that's probably more

913
00:59:47,640 --> 00:59:51,560
effective than you know, trying to train a deep learning model

914
00:59:51,560 --> 00:59:55,040
and, you know, sort of make it work. But I think it's I think

915
00:59:55,040 --> 00:59:57,560
it's a, you know, it's still an important question because the

916
00:59:57,560 --> 01:00:02,000
easier we make it to make fakes, you start to get issues like a

917
01:00:02,000 --> 01:00:06,160
scalable fakes, where it's not just one, one photo that is a lie

918
01:00:06,160 --> 01:00:09,920
or one article is lie, you could generate millions. And I think

919
01:00:09,920 --> 01:00:12,560
that there are serious issues with that. So there's some pretty

920
01:00:12,560 --> 01:00:17,360
interesting work in forensics for detecting fakes, and things

921
01:00:17,360 --> 01:00:21,480
like that. That I think is important to invest in as well

922
01:00:21,480 --> 01:00:25,240
as as we as we advance the state of the art and this kind of

923
01:00:25,240 --> 01:00:31,040
thing. So I so so I don't want to minimize the implications of

924
01:00:31,040 --> 01:00:34,120
this type of thing. I think that for the type of work that I'm

925
01:00:34,120 --> 01:00:36,520
doing, I think that you observe that the tree kind of looks

926
01:00:36,520 --> 01:00:38,960
realistic, it's not super realistic. You know, if you're a

927
01:00:38,960 --> 01:00:45,000
plant expert, it's just sort of, you know, sort of there. I

928
01:00:45,040 --> 01:00:47,400
think the same thing with hats, they don't really super look

929
01:00:47,400 --> 01:00:50,120
like hats. And so I think that we're, we're sort of the stage

930
01:00:50,120 --> 01:00:54,000
where they're really exciting where the implication of what

931
01:00:54,000 --> 01:01:00,480
I've done here, I think is the idea that the, you know, learning

932
01:01:00,840 --> 01:01:04,440
how these models are working inside, by understanding what

933
01:01:04,440 --> 01:01:07,720
the internal structure of the models is, is really the, that

934
01:01:07,960 --> 01:01:11,880
the the exciting part that that it's starting to give a little

935
01:01:11,880 --> 01:01:17,240
insight on how we might untangle and disassemble what the

936
01:01:17,240 --> 01:01:21,560
internal logic is, that is being learned by these, these deep

937
01:01:21,560 --> 01:01:26,400
networks. And, and I'm actually, I feel like this is, I feel

938
01:01:26,400 --> 01:01:28,400
like there's a different issue other than fakes, which is

939
01:01:28,400 --> 01:01:32,640
actually has some ethical implications, which is transparency

940
01:01:32,640 --> 01:01:35,200
of deep networks. Because one thing that they're not really

941
01:01:35,200 --> 01:01:38,680
good at doing is when you have a deep network do something

942
01:01:38,680 --> 01:01:41,960
amazing, they're really not good at answering the question, why?

943
01:01:42,320 --> 01:01:45,800
Why did you do that? Why did you choose to render it this way?

944
01:01:45,800 --> 01:01:49,680
Why did you choose to pick these objects to put in the scene?

945
01:01:49,680 --> 01:01:53,560
Or why did you choose to deny me some credit or to, you know, to

946
01:01:53,560 --> 01:01:56,920
make some other decision that we were at, you know, depending on

947
01:01:56,920 --> 01:02:00,600
neural networks to do. And I think that if we can understand

948
01:02:01,320 --> 01:02:05,560
how to disassemble the rules that are being applied inside the

949
01:02:05,560 --> 01:02:08,440
network for it to make its decision, then I think that we'll

950
01:02:08,480 --> 01:02:13,240
will be, we'll have a way of asking why. And by looking at

951
01:02:13,240 --> 01:02:16,320
the computation directly. So that's my, that's one of my

952
01:02:16,320 --> 01:02:18,240
goals and one of my hopes in doing this kind of work.

953
01:02:21,720 --> 01:02:26,520
Definitely, I can see some of the worst of you about like,

954
01:02:27,960 --> 01:02:31,240
about the transparency of the neural network, especially when

955
01:02:31,240 --> 01:02:36,120
you, when you show the example where you detected a single

956
01:02:36,120 --> 01:02:40,120
neuron that contributes to the watermark thing, I think that

957
01:02:41,360 --> 01:02:42,400
it was really interesting.

958
01:02:43,720 --> 01:02:47,440
Yeah, I think so too. I was surprised that it worked because

959
01:02:47,440 --> 01:02:50,280
we normally think of neural X is very, very, very opaque.

960
01:02:54,160 --> 01:02:59,320
I also have a small question regarding artifacts. So I think

961
01:02:59,560 --> 01:03:04,160
in the beginning, you talked about how you segmented the

962
01:03:04,240 --> 01:03:10,600
network with like masks that were classified before by mapping

963
01:03:10,720 --> 01:03:15,720
neurons and beginning layers, which create things. But could

964
01:03:15,760 --> 01:03:20,360
like, can that be also used to figure out where artifacts or

965
01:03:20,360 --> 01:03:24,400
anomalies are generated to make gains better?

966
01:03:24,960 --> 01:03:30,880
Yeah, actually, I don't have a picture of it here. But in my

967
01:03:30,880 --> 01:03:34,640
work where I was looking for neurons, originally, it's called

968
01:03:34,920 --> 01:03:37,400
the paper is called GAN dissection, you can you can

969
01:03:37,400 --> 01:03:42,760
Google for it. And, and I showed that in that paper, we

970
01:03:42,760 --> 01:03:45,800
analyze some of the pre trained GANs that came from a previous

971
01:03:46,000 --> 01:03:50,280
work from NVIDIA called progressive GAN, we analyzed

972
01:03:50,280 --> 01:03:52,880
some of the pre trained models, and we found that they actually

973
01:03:52,880 --> 01:03:56,560
are neurons that correlate with bad looking artifacts in a

974
01:03:56,560 --> 01:04:00,240
scene. And if you turn those neurons off, you can actually

975
01:04:00,640 --> 01:04:05,080
not only improve the quality of the output of the GAN, just

976
01:04:05,080 --> 01:04:08,760
qualitatively like you can get these artifacts to not show up

977
01:04:08,760 --> 01:04:13,760
but using standard measures of GAN, you know, statistical

978
01:04:13,760 --> 01:04:17,720
measures of GAN image fidelity at large scale. By removing these

979
01:04:17,720 --> 01:04:21,280
neurons, you can actually improve the what we call the FID

980
01:04:21,280 --> 01:04:25,120
scores of these GANs when we tested on like 50,000 images. And

981
01:04:25,120 --> 01:04:29,320
so, so that's actually very weird to me, that's, it was a big

982
01:04:29,320 --> 01:04:35,760
surprise. Because, because we, we train these things using, you

983
01:04:35,760 --> 01:04:39,720
know, powerful optimization techniques, using, you know,

984
01:04:39,920 --> 01:04:42,600
billions of floating point operations, you know, training

985
01:04:42,600 --> 01:04:46,040
these things on big expensive GPUs for a long period of time. And

986
01:04:46,040 --> 01:04:50,000
the idea that a human can come along, and do a simple looking

987
01:04:50,120 --> 01:04:53,040
visualization, pick out a few neurons based on things that

988
01:04:53,160 --> 01:04:57,120
don't look good. And improve the model by turning those neurons

989
01:04:57,120 --> 01:05:01,920
off. It was like it shouldn't be possible, right? If it was so

990
01:05:01,920 --> 01:05:05,360
easy to improve the model that way, why couldn't the optimizer

991
01:05:05,920 --> 01:05:10,520
find it? And so, so I think that was, that was, that was pretty

992
01:05:10,520 --> 01:05:17,240
interesting. I have not repeated that experiment on the latest

993
01:05:17,280 --> 01:05:21,280
GANs, which are actually much better the style GANs. To

994
01:05:21,280 --> 01:05:25,280
architecture, they went back and they analyzed a bunch of the

995
01:05:25,280 --> 01:05:29,920
artifacts that show up in this, this family of GANs. And they,

996
01:05:29,960 --> 01:05:31,920
they found that there are certain learning methods that they

997
01:05:31,920 --> 01:05:35,400
can do to remove the artifacts or reduce them somewhat. And so I

998
01:05:35,400 --> 01:05:40,400
don't know if a human can still beat the current generation of

999
01:05:40,400 --> 01:05:43,520
GANs, it'd be worth going back and seeing that phenomenon is

1000
01:05:43,520 --> 01:05:44,000
still there.

1001
01:05:44,880 --> 01:05:47,000
That's pretty cool. Thank you. Yep.

1002
01:05:51,000 --> 01:05:55,000
Okay, excellent. Thank you so much, David. It was really

1003
01:05:55,240 --> 01:06:02,400
fascinating topic and talk and more interesting to me, asking

1004
01:06:02,400 --> 01:06:06,600
the right questions, asking questions and learning to ask

1005
01:06:06,600 --> 01:06:10,160
the right questions. It's really interesting. And I think that

1006
01:06:10,280 --> 01:06:13,280
it opened paths to many of us.

1007
01:06:14,040 --> 01:06:17,400
Excellent. Hey, thank you for the opportunity to talk to the

1008
01:06:17,400 --> 01:06:23,600
group here today. I always enjoy the, the chance to interact

1009
01:06:23,600 --> 01:06:27,200
with folks about this. If anybody wants to send other questions

1010
01:06:27,200 --> 01:06:32,360
about it, of course, you can always send me a note. And, you

1011
01:06:32,360 --> 01:06:33,520
know, I love this stuff.

1012
01:06:33,800 --> 01:06:38,280
Yeah, definitely. I think that it would be great to follow

1013
01:06:38,640 --> 01:06:43,960
follow your work on your GitHub and your website, and

1014
01:06:43,960 --> 01:06:47,160
especially for students who play with the tools that you have,

1015
01:06:47,160 --> 01:06:50,520
so they have them get an understanding of how these two

1016
01:06:50,520 --> 01:06:53,360
work and make them curious about the work.

1017
01:06:53,880 --> 01:06:54,360
Cool.

1018
01:06:55,080 --> 01:06:56,560
Excellent. Thank you so much.

1019
01:06:57,320 --> 01:06:58,840
Thank you, Ali. Thank you, everybody.

1020
01:06:59,440 --> 01:07:01,480
Thank you. Bye now.

